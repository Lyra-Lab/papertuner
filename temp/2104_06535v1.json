{
  "id": "http://arxiv.org/abs/2104.06535v1",
  "title": "NPE: An FPGA-based Overlay Processor for Natural Language Processing",
  "authors": [
    "Hamza Khan",
    "Asma Khan",
    "Zainab Khan",
    "Lun Bin Huang",
    "Kun Wang",
    "Lei He"
  ],
  "abstract": "In recent years, transformer-based models have shown state-of-the-art results\nfor Natural Language Processing (NLP). In particular, the introduction of the\nBERT language model brought with it breakthroughs in tasks such as question\nanswering and natural language inference, advancing applications that allow\nhumans to interact naturally with embedded devices. FPGA-based overlay\nprocessors have been shown as effective solutions for edge image and video\nprocessing applications, which mostly rely on low precision linear matrix\noperations. In contrast, transformer-based NLP techniques employ a variety of\nhigher precision nonlinear operations with significantly higher frequency. We\npresent NPE, an FPGA-based overlay processor that can efficiently execute a\nvariety of NLP models. NPE offers software-like programmability to the end user\nand, unlike FPGA designs that implement specialized accelerators for each\nnonlinear function, can be upgraded for future NLP models without requiring\nreconfiguration. We demonstrate that NPE can meet real-time conversational AI\nlatency targets for the BERT language model with $4\\times$ lower power than\nCPUs and $6\\times$ lower power than GPUs. We also show NPE uses $3\\times$ fewer\nFPGA resources relative to comparable BERT network-specific accelerators in the\nliterature. NPE provides a cost-effective and power-efficient FPGA-based\nsolution for Natural Language Processing at the edge.",
  "text": "NPE: An FPGA-based Overlay Processor for Natural Language\nProcessing\nHamza Khan\nUC Los Angeles\nAsma Khan\nUC Irvine\nZainab Khan\nStanford University\nLun Bin Huang\nIndependent Researcher\nKun Wang\nUC Los Angeles\nLei He\nUC Los Angeles\nABSTRACT\nIn recent years, transformer-based models have shown state-of-the-\nart results for Natural Language Processing (NLP). In particular, the\nintroduction of the BERT language model brought with it break-\nthroughs in tasks such as question answering and natural language\ninference, advancing applications that allow humans to interact nat-\nurally with embedded devices. FPGA-based overlay processors have\nbeen shown as effective solutions for edge image and video process-\ning applications, which mostly rely on low precision linear matrix\noperations. In contrast, transformer-based NLP techniques employ\na variety of higher precision nonlinear operations with significantly\nhigher frequency. We present NPE, an FPGA-based overlay proces-\nsor that can efficiently execute a variety of NLP models. NPE offers\nsoftware-like programmability to the end user and, unlike FPGA\ndesigns that implement specialized accelerators for each nonlinear\nfunction, can be upgraded for future NLP models without requir-\ning reconfiguration. We demonstrate that NPE can meet real-time\nconversational AI latency targets for the BERT language model\nwith 4Ã— lower power than CPUs and 6Ã— lower power than GPUs.\nWe also show NPE uses 3Ã— fewer FPGA resources relative to com-\nparable BERT network-specific accelerators in the literature. NPE\nprovides a cost-effective and power-efficient FPGA-based solution\nfor Natural Language Processing at the edge.\nKEYWORDS\nNLP, FPGA, Overlay, Processor, Accelerator, Nonlinear, BERT, Ma-\nchine Learning\n1\nINTRODUCTION\nOne of the most common Natural Language Processing (NLP) tasks\nis sequence transduction, translating an input sequence to an out-\nput sequence. Traditionally, convolutional neural networks (CNNs)\nand recurrent neural networks (RNNs) have been used for this task\n[9, 10]. The Transformer architecture [22] removes all recurrent\nand convolutional components and instead relies on self-attention\nmechanisms, typically showing better computation time and per-\nformance. A transformer is made up of two parts, encoders and de-\ncoders. The Bidirectional Encoder Representations from Transform-\ners (BERT) model [6] incorporates the encoders from transformers\nto generate a state-of-the-art language representation model. When\nit was first introduced, BERT broke records on eleven different\nNLP tasks. Since then, variations of BERT such as RoBERTa [18]\nFPGA â€™21, February 28-March 21, 2021, Virtual\n2021. ACM ISBN 978-1-4503-8218-2/21/02...$15.00\nhttps://doi.org/10.1145/3431920.3439477\nand DistilBERT [20] have shown even better performance and ac-\ncuracy. Implementing efficient accelerators for inference of these\ntransformer models has proven a challenging task.\nFPGA-based overlay processors have provided effective solutions\nfor CNN and other network inference on the edge, allowing for\nflexibility across networks without reconfiguring the FPGA [24â€“26].\nCNNs consist mostly of linear matrix operations like convolution\nand pooling, and have consistently shown resilience to extreme\nquantization. However, BERT and its successors [13, 15, 18, 20] can-\nnot be efficiently accelerated using existing CNN FPGA accelerators.\nAlthough they compute many matrix multiplications, transformer\nmodels also introduce complex nonlinear operations that require\nhigher precision, and are called with higher frequency. For instance,\nsoftmax and layer normalization [2] are performed several times in\neach encoder for BERT, and block subsequent computation until\nthey are finished processing. As such, it is essential to calculate\nthem efficiently while maintaining high throughput. These non-\nlinearities must be computed on-device for performance-sensitive\napplications, as sending to a CPU causes significant latency over-\nhead and is not practical on the edge.\nMost existing accelerators [11, 19] include specialized units for\ncomputing each type of nonlinearity. For instance, FTRANS [17],\nthe only previously published FPGA accelerator for transformers,\nincludes separate softmax and layer normalization modules. Since\nNLP is a constantly-evolving field that may introduce different types\nof nonlinearities, this specialized approach means that an FPGA de-\nsign may need reconfiguration for additional NLP networks. It also\nleads to unnecessary area overhead and under-utilized resources\nacross nonlinear operations.\nIn this paper we propose NPE, an FPGA-based overlay processor\nfor NLP model inference at the edge. As shown in Figure 1, unlike\nmost other accelerators, NPE employs a common method for ap-\nproximating different nonlinear functions efficiently and accurately\nwithout added overhead. The main contributions of our work are\nas follows:\nâ€¢ We design a software-programmable domain-specific over-\nlay processor with a matrix multiply unit and a multi-precision\nvector unit for NLP processing.\nâ€¢ We employ a unified piecewise polynomial approach for\nnonlinear function approximation to allow extensibility to\nfuture nonlinear functions that may be required.\nâ€¢ We demonstrate that our proposed accelerator can meet the\nreal-time latency constraints for conversational AI while\nmaintaining 4Ã— and 6Ã— lower power than GPUs and CPUs\nrespectively. Our design utilizes 3Ã— fewer FPGA resources\narXiv:2104.06535v1  [cs.AR]  13 Apr 2021\nFigure 1: BERT network structure, showing how each opera-\ntion would be mapped onto NPE compared to a conventional\nnetwork-specialized accelerator.\nrelative to a comparable network-specific transformer FPGA\naccelerator.\n2\nRELATED WORK\nWhile BERT has been highly optimized at the software level for\nCPU and GPU, little work has been published related to custom\nhardware acceleration of any transformer-based networks, particu-\nlarly on FPGAs. Two ASICs have been recently proposed, OPTIMUS\n[19] and ğ´3 [11], that each accelerate different parts of transformer\ninference. OPTIMUS optimizes matrix multiplication for transform-\ners by exploiting sparsity. It has dedicated exponential, divider, and\nsquare root components for nonlinearities, leading to wasted area\nsince each is only used a small fraction of the time. ğ´3 accelerates\nattention mechanisms using various approximation methods. It has\na very deep pipeline that is specialized for attention, which is inef-\nficient from an overall design point of view because the multipliers\nand function approximation units cannot be reused even for other\nmatrix multiplications. In particular, ğ´3 would need to be paired\nwith a conventional matrix multiply accelerator to implement BERT.\nAt any one time, either the ğ´3 unit or the main accelerator would be\ncomputing, leading to many idle resources and wasted performance\npotential.\nTo the best of our knowledge, FTRANS [17] is the only currently\npublished FPGA accelerator for BERT and related transformer net-\nworks. FTRANS takes a very specialized approach for implementing\ntransformers, in which it has dedicated encoder and decoder mod-\nules. Each of these modules has many specialized components. For\nexample, the attention head module contains components for soft-\nmax and layer normalization, as well as five unique PE banks to\nperform each matrix multiply subroutine required (see Table 1 for\nthe computation in an attention layer). While it is sufficient to im-\nplement transformers, FTRANS is not flexible enough to handle\nany non-transformer network. As the NLP state-of-the-art evolves\nand new network variants emerge, the FTRANS architecture may\nhave to be extensively redesigned to adapt.\n3\nBACKGROUND\n3.1\nConversational AI\nLow power and low latency NLP is a prerequisite for conversa-\ntional AI at the network edge. Conversational AI allows people to\ninteract naturally with machines in a dialogue. For instance, a user\nmay ask a question to a smart speaker and expect a human-like\nresponse almost immediately. For this response to seem natural,\nit must be returned within 300 milliseconds. In these 300 ms, the\ndevice must perform several complex steps including speech-to-\ntext, user authentication, and natural language processing. As a\nresult, any single modelâ€™s inference should be complete within 10-\n15 milliseconds. Recently, many efforts have been made by the GPU\ncommunity to optimize GPU implementations of BERT to reach\nthis threshold.\n3.2\nThe BERT Model\nBERT adopts the structure of the encoders from transformers. While\nthere are many BERT variants, the particular structure can be de-\nscribed by three parameters: number of encoders ğ¿, number of\nattention heads ğ´, and hidden layer size ğ». We focus on BERTBASE,\nwhich is composed of 12 encoders, each with 12 attention heads\nand a hidden size of 768 (ğ¿= 12, ğ´= 12, ğ»= 768). Figure 1 shows\nthe general structure for BERT.\nThe model starts with an embedding layer that converts each\ninput language sequence into features. For instance, an input se-\nquence with 512 tokens in BERTBASE would be converted to a\n512 Ã— 768 matrix, where each token is replaced by a 768-length\nfeature vector. Here, a token refers to a few adjacent characters,\nwhere a word is made up of one or more tokens. The embedding\nstep has negligible computation but requires lots of memory. There-\nfore, we assume this initial step is performed off-chip and focus on\naccelerating the computationally-intensive encoders.\nEmbedding is followed by ğ¿= 12 encoders. Each encoder per-\nforms four back-to-back operations: multi-headed self-attention,\nlayer normalization, feed-forward layers, then layer normalization\nagain. The encoder calculation can be decomposed into matrix-\nmatrix multiplies followed by one of three primitives: softmax, layer\nnormalization, and the GELU activation [12]. Table 1 describes the\ncomputation in further detail.\nThe BERT model is typically trained to handle sequences of up\nto 512 tokens. For any particular task we pick a sequence length\nğ‘ ğ‘’ğ‘_ğ‘™ğ‘’ğ‘›â‰¤512, padding any input sequences less than ğ‘ ğ‘’ğ‘_ğ‘™ğ‘’ğ‘›\nand truncating any sequences larger than ğ‘ ğ‘’ğ‘_ğ‘™ğ‘’ğ‘›. This parameter\ndetermines the complexity of the operations performed and the\ninference speed. Sequence lengths less than 32 are usually too small\nfor practical applications. Typical benchmarks use sequence lengths\nof 64 or 128.\n3.3\nBERT Nonlinear Operations\n3.3.1\nGELU Activation. The GELU activation is defined by the\nfollowing equation:\nGELU(ğ‘¥) = ğ‘¥ğ‘ƒ(ğ‘‹â‰¤ğ‘¥) = ğ‘¥Â· 1\n2 [1 + erf (ğ‘¥/\nâˆš\n2)]\n(1)\nIt is commonly approximated using the tanh function as in Equa-\ntion 2 and can also be approximated directly using a lookup table.\nTable 1: Computation for BERT with A attention heads. ğ‘‹\nrepresents the input to an encoder. ğ‘‹1-ğ‘‹5 represent subse-\nquent outputs after each operation. The computation is bro-\nken down into matrix-matrix multiplies, softmax, layer nor-\nmalization, and GELU.\nEmbedding\nğ‘‹= Embedding(input_sequence)\nMulti-Headed Self-Attention\nfor (i = 0 to A - 1)\nğ‘„ğ‘–= ğ‘‹ğ‘Šğ‘„ğ‘–;\nğ¾ğ‘–= ğ‘‹ğ‘Šğ¾ğ‘–;\nğ‘‰ğ‘–= ğ‘‹ğ‘Šğ‘‰ğ‘–\nğ‘ğ‘–= softmax(ğ‘„ğ‘–ğ¾ğ‘–ğ‘‡\nğ‘˜\n)ğ‘‰ğ‘–\nwhere ğ‘˜is a known constant\nğ‘‹1 = [ğ‘0, ..., ğ‘ğ´âˆ’1] ğ‘Šğ‘‚\nLayer Normalization A\nğ‘‹2 = LayerNorm(ğ‘‹+ ğ‘‹1)\nFeed-Forward\nğ‘‹3 = GELU(ğ‘‹1 ğ‘Š1 + ğ‘1)\nğ‘‹4 = ğ‘‹3 ğ‘Š2 + ğ‘2\nLayer Normalization B\nğ‘‹5 = LayerNorm(ğ‘‹2 + ğ‘‹4)\nGELU(ğ‘¥) â‰ˆ0.5ğ‘¥(1 + tanh [\nâˆšï¸\n2/ğœ‹(ğ‘¥+ 0.044715ğ‘¥3)])\n(2)\n3.3.2\nLayer Normalization. Layer normalization first requires\ncomputing the mean and variance of a matrix across rows. Given a\nmatrix ğ‘¥of dimension ğ‘Ã— ğ¾, we compute the mean and variance\nfor row ğ‘–.\nğœ‡ğ‘–= 1\nğ¾\nğ¾\nâˆ‘ï¸\nğ‘˜=1\nğ‘¥ğ‘–,ğ‘˜;\nğœ2\nğ‘–= 1\nğ¾\nğ¾\nâˆ‘ï¸\nğ‘˜=1\n(ğ‘¥ğ‘–,ğ‘˜âˆ’ğœ‡ğ‘–)2\n(3)\nThen, the mean and variance are applied to each element using\nEquation 4 to get the normalized output Ë†ğ‘¥. Finally, each Ë†ğ‘¥ğ‘–is scaled\nand shifted by trained vectorsğ›¾and ğ›½to get the layer normalization\noutput ğ‘¦, as shown in Equation 5.\nË†ğ‘¥ğ‘–,ğ‘˜= ğ‘¥ğ‘–,ğ‘˜âˆ’ğœ‡ğ‘˜\nâˆšï¸ƒ\nğœ2\nğ‘˜+ ğœ–\n(4)\nğ‘¦ğ‘–,ğ‘˜= Ë†ğ‘¥ğ‘–,ğ‘˜Â· ğ›¾ğ‘˜+ ğ›½ğ‘˜\n(5)\n3.3.3\nSoftmax. The definition of softmax is shown in Equation\n6. Softmax can be difficult to implement in hardware because of\nthe exponential and division operations. It can be directly realized\nusing dedicated exponential and division units, at the cost of under-\nutilized resources and extra area overhead.\nsoftmax(ğ‘¥ğ‘—) =\nğ‘’ğ‘¥ğ‘—\nÃ\nğ‘–ğ‘’ğ‘¥ğ‘–\n(6)\n3.4\nThroughput Requirements of Nonlinear\nOperations\nSince matrix multiply operations depend on the results from pre-\nceding nonlinear operations, nonlinear processing needs to have\nhigh enough throughput to not add significant latency overhead.\nThe throughput requirement for nonlinear operations can be de-\ntermined by the number of elements we need to process and the\ncycles available for processing. We define the cycle budget for a\nnonlinear operation as the number of cycles that the preceding\nmatrix multiply takes to process, given the matrix multiply dimen-\nsions and the number of multiplies per cycle. For BERTBASE, given\n2048 multiplies per cycle and a sequence length of 512, we show the\nthroughput requirements for each nonlinearity in Table 2. The layer\nnormalization after attention and the one after GELU are shown\nseparately, since they have different throughput requirements.\nTable 2: Throughput requirements (elements per cycle) for\ndifferent nonlinearities in BERTBASE with sequence length\nof 512 and 2048 multiplies per cycle. The matrix to be pro-\ncessed has dimensions ğ‘Ã— ğ‘€.\nNonlinearity\nN\nM\nCycle Budget\nThroughput\n% of Overall Cycles\nSoftmax\n512\n512\n8,192\n32\n5\nLayer Norm A\n512\n768\n147,456\n2.7\n7.5\nGELU\n512\n3072\n589,824\n2.7\n30\nLayer Norm B\n512\n768\n589,824\n0.7\n30\nThe final column of Table 2 indicates percentage of overall cycles\nthat depend on each nonlinear computation. Specifically, this value\nshows the percentage of overall matrix multiply cycles that are\nfollowed by the nonlinearity. For instance, we see that 30% of the\noverall cycle time is spent computing the matrix multiply inputs to\nGELU operations.\nFrom Table 2, we see that Layer Normalization and GELU both\nrequire a throughput average of less than three elements per cycle,\nwhile softmax requires 32 elements per cycle to throughput-match\nthe matrix multiply operations. To put this in perspective, this\nmeans that without additional optimizations we would need to\nperform softmax on a vector of 512 elements in just 16 cycles.\n4\nNONLINEARITY PROCESSING\n4.1\nNonlinear Function Approximation\nThere are dozens of ways to approximate nonlinear functions. We\nfirst discuss several specialized approaches that are commonly used\nfor different nonlinear functions. We then discuss our efficient\nuniform approach to approximate several types of nonlinearities.\n4.1.1\nSpecialized Approaches. Approximating complex nonlin-\near functions has been a field of interest for many decades. Common\nfunctions like softmax and square roots have been implemented\nin many different ways over the years using mathematical mod-\nels with varying computational complexities and accuracies. The\nsquare root function, for example, can be approximated using a\nTaylor Series Expansion [14], the Chebyshev approximation [21],\nthe Newton-Raphson algorithm [5], and the CORDIC algorithm\n[1, 23]. It can also be approximated directly using a lookup table\n(LUT). Softmax is also often implemented using one or more LUTs\nfor exponential calculation [7, 27].\nFor reasonable accuracy with the lookup-based approaches, the\nLUTs typically are large and require a lot of memory. In contrast,\npiecewise linear approaches have shown good accuracy while main-\ntaining very small lookup tables [8].\n4.1.2\nUnified Nonlinearity Processing. As previously shown,\neach type of nonlinearity has several ways it can be approximated.\nHowever, taking a separate approach for each function can lead to\nmany times more area and much more underutilized resources. For\ninstance, most transformer accelerators have dedicated exponential\nunits and dividers for softmax and dedicated square root units for\nlayer normalization.\nWe take a more unified approach that uses only piecewise polyno-\nmial approximation along with simple vector operations to process\nvarious nonlinearities. Some functions, like GELU, can directly be\napproximated using piecewise approximation. Others, like soft-\nmax, may use piecewise approximation for intermediate functions\nlike exponentials or square roots and then use adders, multipli-\ners, etc. for the remainder of the computation. To implement this\nunified approach, our hardware is optimized for piecewise func-\ntion approximation as well as other operations like multiplication,\naddition/subtraction, and vector reduction (e.g., max, sum).\n4.1.3\nMulti-precision Computation. Unlike matrix multiplies\nand other linear operations, nonlinear computation cannot be fully\nquantized to low-precision data types (such as 8-bit fixed point\nnumbers) while maintaining network accuracy. Instead, different\nsteps of the nonlinear computation require varying levels of preci-\nsion. For instance, layer normalization may take in a 16-bit fixed\npoint number as an input but will need to process the variance cal-\nculations using 32 or even 64-bit fixed point. As such, all supported\noperations (arithmetic, reduction, etc.) need to be flexible enough\nto operate on several different data types.\n4.2\nPiecewise Function Approximation\n4.2.1\nGeneral Piecewise Approximations. Piecewise function\napproximation involves breaking up a nonlinear function of interest\ninto smaller regions along a chosen interval, each approximated\nby a simpler function such as a line or polynomial. Continuous\nPiecewise Linear (CPWL) approximations, in particular, use lines\nfor approximation where the end point of each region is the same as\nthe start point of the next region. Figure 2 shows an approximation\nof ğ‘£(ğ‘¥) = âˆšğ‘¥with just three segments. The starting ğ‘¥and ğ‘£(ğ‘¥)\nvalues for each segment are called knot samples and nodal values\nrespectively.\nTypically, approximation regions are chosen to be either uni-\nform width or non-uniform width. Uniform width segments tend\nto be easier to evaluate but require significantly more segments.\nNon-uniform width segments can better approximate functions\nwith fewer segments but require more complex evaluation. The\nadvantage of non-uniform segments in terms of total number of\nsegments required becomes even more significant when dealing\nwith functions that have large mostly-linear regions. In particular,\nconsider functions like GELU(x) and âˆšğ‘¥which are nearly linear\nFigure 2: Piecewise linear approximation of âˆšğ‘¥on the inter-\nval [0, 2) with three segments.\nexcept for a very small nonlinear region near zero. Uniform seg-\nmentation would require orders of magnitude more segments than\nnon-uniform to approximate the linear regions, leading to signifi-\ncantly more memory usage for the same maximum approximation\nerror.\nWe can also use a piecewise polynomial approach to approximate\nsome functions, which takes more cycles to compute but gives\nhigher accuracy. For most functions, including those needed for\nBERT, piecewise linear is enough. We discuss our non-uniform\ncontinuous piecewise linear segmentation in the following section.\n4.2.2\nPiecewise Linear Approach. The main considerations for\npiecewise linear approximations are the number of segments, seg-\nment widths, and the best approximation for each segment. Frenzen\net al. [8] explored the number of segments needed for piecewise\nlinear approximation of various common functions using differ-\nent segmentation techniques. Based on their results, we see that\nit is possible to maintain high accuracy with very few segments\n(even less than 10, depending on accuracy constraints). We use a\nsegmentation approach based on [3], which describes one method\nfor finding an optimal partition with non-uniform segmentation.\nWe find that even sub-optimal segmentation can result in no accu-\nracy loss for BERT inference on the test set. Algorithm 1 gives the\ngeneral computation for evaluating a continuous piecewise linear\nfunction.\nAlgorithm 1 Continuous Piecewise Linear Approximation of a\nfunction ğ‘£(ğ‘¥) based on [3]. Here, the knot samples are ğ‘¥0, ..., ğ‘¥ğ‘\nand the corresponding nodal values are ğ‘£(ğ‘¥0), ..., ğ‘£(ğ‘¥ğ‘).\n1) Find the sub-interval containing the input value ğ‘¥given that:\nğ‘¥ğ‘–âˆ’1 â‰¤ğ‘¥< ğ‘¥ğ‘–\n2) Find the fractional distance ğ›¿from ğ‘¥ğ‘–âˆ’1:\nğ›¿= (ğ‘¥âˆ’ğ‘¥ğ‘–âˆ’1)/(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘–âˆ’1)\n3) Determine the final piecewise linear approximation:\nğ‘£(ğ‘¥) â‰ˆ(1 âˆ’ğ›¿) ğ‘£(ğ‘¥ğ‘–âˆ’1) + ğ›¿ğ‘£(ğ‘¥ğ‘–)\nFinding the sub-interval from Step 1 of Algorithm 1 adds ad-\nditional complexity. If using uniform segmentation, the segment\nFigure 3: Overall architecture of NPE. The memory read unit (MRU) reads from external memory and writes to the MMU input\nbuffer (MIB). The MMU performs matrix multiplication and writes results to its scratchpad memory. The NVU reads from the\nMMU scratchpad memory and writes intermediate results to the MMU input buffers. The memory write unit (MWU) transfers\nfinal results from the NVU scratchpad to external memory.\nnumber can be found simply by using the upper bits of the input.\nSince we use non-uniform segmentation, we implement more com-\nplex segment address calculation like that used in [16]. In software,\nthis segment address calculation could be performed using Algo-\nrithm 2. On a CPU or GPU, these calculations would take tens of\ninstructions. Meanwhile, with specialized hardware, this task can\neasily be done on an FPGA or ASIC within a single clock cycle,\nsuch as with a priority encoder.\nAlgorithm 2 Software evaluation of non-uniform piecewise linear\nsegment address for input ğ‘¥with knot samples ğ‘¥0, ..., ğ‘¥ğ‘.\nfor ğ‘–â†ğ‘, 0 do\nif ğ‘¥â‰¥ğ‘¥ğ‘–then\nreturn i\nend if\nend for\nBy itself, piecewise linear approximation is not always accurate\nenough to be used without a large number of segments. However,\nwith normalization and range limiting of the fixed point input and\nsubsequent denormalization of the output, this approximation can\nmaintain high accuracy with only a few segments.\n5\nACCELERATOR ARCHITECTURE\nIn this section, we present the overall architecture of NPE, our\nFPGA overlay processor for NLP. NPE adopts several components\nfrom OPU [24] including the matrix multiply unit (MMU). Figure 3\nshows the architecture of the accelerator.\n5.1\nAccelerator Modules\nThe NPE architecture consists of the following: instruction control\nunit (ICU), memory read unit (MRU), memory write unit (MWU),\nmatrix multiply unit (MMU), and the nonlinear vector unit (NVU).\nThe ICU sends instructions to each of the modules. Data is ex-\nchanged between the functional units through a series of memory\nbuffers.\n5.2\nData Flow\nThe memory read unit (MRU) reads data from external memory\nand writes it to the MMUâ€™s input buffer (MIB). The MMU reads\nfrom the MIB and uses its scratchpad memory (MMEM) both for\nintermediate computations and for its final output. The NVU uses\nits scratchpad memory (NMEM) for its intermediate operations.\nThe NVU accesses MMEM for its input data and deposits the final\nresults either to the MIB (MMU input buffer) or to the NMEM (NVU\nscratchpad) for subsequent retrieval by the MWU, which writes\nthese results to the external memory. The operations of the MRU,\nMWU, MMU and the NVU are pipelined to hide off-chip memory\nlatency and to allow for maximum concurrency between these\nunits.\n5.3\nMatrix Multiply Unit (MMU)\nThe MMU computation consists of five stages: data selection, in-\nner product, adder tree reduction, accumulation, and quantization.\nData selection loads the necessary matrix operands from the input\nbuffers and rearranges them as needed. The matrix multiplication\nis implemented using an array of PEs, where each PE performs an\ninner product using an array of multipliers followed by an adder\ntree. Then, the PE outputs can be further summed up by another\nadder tree, with the number of final outputs dependent on the ma-\ntrix multiplication dimensions. Finally, the inner product outputs\nare accumulated and then quantized.\nThe MMU implementation has 128 PEs with 16 multiply accu-\nmulate units each (for a total of 2048 multipliers). These multiply\naccumulate units map to the FPGAâ€™s DSP slices in our implemen-\ntation. There are two versions of the NPE design, one supporting\n8-bit matrix multiplies and the other supporting 16-bit matrix mul-\ntiplies. The 16-bit version uses each DSP slice for a single element\nmultiply, for a total throughput of 2048 multiplies per cycle. The\n8-bit version decomposes each DSP slice into two 8-bit multipliers\nwith one input in common (due to DSP slice resource constraints).\nOn the same board, we can get a throughput of 4096 8-bit multiplies\nper cycle with 2048 DSP slices.\nFigure 4: Overall architecture of the NVU. The LSU transfers data between the VRF and either NMEM, MMEM, or the MIB. The\nvector compute unit (VCU) fetches operands from either the VRF or the SRF and can deposit results back to either the VRF\nor the SRF. The scalar compute unit fetches operands from the SRF and writes results back to the SRF. The microprogram\ncontroller (MPC) controls the operations of the NVU on a cycle-by-cycle basis.\n5.4\nMatrix Multiply Quantization\nThe matrix multiply for transformer networks can be quantized\nto 16-bit fixed point with no perceptible loss in accuracy. Several\nworks [4, 28] have also shown the feasibility of 8-bit quantization\nof transformers. As shown in [28], BERT can be implemented with\n8-bit matrix multiplies with minimal accuracy loss. For this reason,\nwhile we support both 16 and 8-bit matrix multiplies, we plan to use\n8-bit matrix multiply for our implementation. For both the 16-bit\nand the 8-bit matrix multiplies, the output of the MMU is written\nout to the MMEM (MMU scratchpad memory) as 16-bit fixed point\nvalues. Consequently, the NVU always consumes 16-bit fixed point\nvalues and generates either 8-bit or 16-bit results for the subsequent\nmatrix multiplies.\n5.5\nSoftware Simulation\nWe simulate the architecture in software to validate the accuracy\nconstraints of end-to-end BERT model inference. In order to model\nthe overall accuracy loss, our simulations take into account the NPE\nmodules used as well as the data quantization at each intermediate\nstep. In particular, we model the fixed-point quantization effects\nof matrix multiplication and the approximation errors from our\nunified nonlinear processing approach, including piecewise linear\napproximations for various nonlinear operations.\n6\nNONLINEAR VECTOR UNIT (NVU)\nARCHITECTURE\nThe key novel component of NPEâ€™s design is the nonlinear vec-\ntor unit (NVU) that handles high-throughput nonlinear function\ncomputation with minimal resource overhead. The NVU is a data-\nparallel vector load/store architecture that performs arithmetic and\nlogical operations on multiple elements in each clock cycle. The\narithmetic performance of the NVU is defined by the width of the\nvector registers and the degree of parallelism in the arithmetic\ndatapath.\nThe NVU, shown in Figure 4, comprises a vector load/store unit\n(LSU), a vector register file (VRF), a vector compute unit (VCU), a\nscalar register file (SRF), a scalar compute unit (SCU), and a mi-\ncroprogram controller (MPC). The VRF, the VCU, and the LSU all\noperate on fixed-length vectors. The LSU accesses vectors resident\nin either the NVU memory (NMEM), the MMU memory (MMEM),\nor the MMU input buffer (MIB). The vector compute unit (VCU)\nand scalar compute unit (SCU) are designed to handle 16, 32 and\n64-bit operations to allow for higher bit precision in intermediate\ncalculations when required.\n6.1\nMicroprogram Controller (MPC)\nThe instruction control unit (ICU) of NPE sends instructions to\nall NPE functional units, including the MMU and the NVU. These\ninstructions are designed to operate over many clock cycles. The\nNVU interprets these high-level instructions to implement nonlin-\near functions like softmax, layer normalization, GELU, etc.\nThe microprogram controller (MPC) of the NVU is responsible\nfor breaking down ICU instructions into a sequence of VLIW in-\nstruction bundles, which are used to control the LSU, the VCU, and\nthe SCU. Since the VCU can execute up to three operations concur-\nrently, as many as five instructions can be dispatched by the MPC\nin each micro-instruction cycle. The microprogram corresponding\nto each instruction is stored in the microprogram memory.\n6.2\nVector Register File (VRF)\nThe vector register file (VRF) provides a high-throughput scratch-\npad for the vector compute unit (VCU). It allows the VCU to exploit\nlocality of data across operations. The NVU requires a register file\nwith 8 logical read and write ports in order to maintain concur-\nrency of various VCU functional units. However, we implement the\nVRF using dual-port (1R1W) BRAMS with a combination of time\nsharing and data duplication. The resulting register file operates at\nthe desired frequency of 200 MHz while requiring less than 5% of\noverall BRAM resources.\n6.3\nNVU Memory (NMEM)\nThe NVU memory subsystem (NMEM) serves primarily as scratch-\npad for the NVU. The memory write unit (MWU) can also read\nresults from the NMEM through a dedicated read port. The NMEM\nuses multiple banks of single-port memories, which are designed to\nsupport loads and stores of entire vectors in a single clock cycle. The\nNMEM can be implemented efficiently using single-port BRAMs\non the FPGA. The NMEM also has arbitration logic to allow access\nfrom both the NVU and the MWU. The NMEM also contains logic\nfor data permutation, which is required for strided and indexed\naccesses.\n6.4\nVector Load/Store Unit (LSU)\nThe vector load/store unit performs fixed-length vector transfers\nbetween the vector register file and either the NMEM, the MMEM,\nor the MMU input buffer (MIB). The LSU supports non-strided,\nstrided, and indexed loads and stores from the NMEM.\n6.5\nVector Compute Unit (VCU)\nThe vector compute unit (VCU) supports a full complement of\nvector and intra-vector arithmetic (add, subtract, multiply, shift,\nsum, dot product, etc.), logical, compare (<, â‰¥, =, etc.), min/max\nand permute operations. In addition, specialized capabilities for\npiecewise polynomial approximation have been added, allowing\nthe NVU to approximate nonlinear functions more than 10Ã— faster\nthan traditional SIMD processors. The VCU reads vectors from\nthe VRF or the scalar register file (SRF) and, depending on the\noperation, writes results back to the VRF or SRF. Vector reduction\nresults are written to the SRF, and vector-scalar operations fetch\nscalar operands from the SRF.\nThe VCU implements multi-precision arithmetic while sharing\nresources between 8, 16, 32, and 64-bit fixed point data types. While\nthe MMU only needs to handle a single data type (either 8 or 16-bit\nfixed point), NVU operations typically involve mixed precision in-\ncluding 32 and 64-bit representations for intermediate calculations.\nThe VCU can execute up to three operations concurrently.\n6.6\nScalar Compute Unit (SCU)\nThe NVU also includes a scalar compute unit (SCU), which oper-\nates out of a scalar register file (SRF). Like the VCU, the SCU can\nhandle 8, 16, 32, and 64-bit operations. The concurrent operation\nof vector and scalar functional units allow for the computation of\nnonlinear functions of vector reduce operations. For example, the\nNVU is capable of performing an inner product followed by the\n1\nâˆšğ‘¥operation for layer normalization variance calculations while\nmaintaining full throughput.\n6.7\nScalable Architecture of the NVU\nAll operations of the NVU are performed on fixed bit-width vec-\ntor registers which make up the VRF. There are 32 vector reg-\nisters in the VRF. All NVU micro-instructions reference source\nand destination vector registers. The overall performance of the\nNVU can be described by a single parameter, i.e. the vector register\nwidth (ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»). The number of elements processed per micro-\ninstruction depends on the element size (8, 16, 32, or 64 bits). For\nexample, a ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»of 256 can hold 32 elements of 8 bits, 16 ele-\nments of 16 bits, etc. The NVUâ€™s area and performance depend on\nthe number of elements that can be processed per micro-instruction.\n7\nTHROUGHPUT ANALYSIS\nFor initial analysis of NPEâ€™s architecture, we examine the through-\nput requirements for BERT on our architecture. While the MMU\nhas both 8-bit and 16-bit variations, we focus on NPE with 16-bit\nMMU and pair it with NVUs of different ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ». For the re-\nmainder of this work, we describe the NVU variants based on the\nvector register width ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ». NVU-ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»refers to the\nNVU with ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»-bit vector registers. For instance, NVU-256\nmeans that a vector register is 256 bits. We focus on four NVU sizes\n(NVU-256, NVU-512, NVU-1024, and NVU-2048), comparing the\nthroughput of each NVU variant to the required BERT throughputs.\n7.1\nNVU Throughput\nTable 3 shows the individual NVU performance results on each\nnonlinear function required for BERT. To normalize the results\nacross NVU vector register widths, we give the number of cycles\nneeded to process a 512 length array of 16-bit elements and the\ncorresponding throughput in elements per cycle.\nTable 3: Throughput (elements per cycle) of NVU with differ-\nent ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»on BERTâ€™s nonlinear functions; cycle count\nto process a 512-element vector is shown in parentheses.\nNVU Width\nSoftmax\nLayer Norm\nGELU\nNVU-256\n1.64 (312)\n0.64 (804)\n4\n(128)\nNVU-512\n3.05 (168)\n1.29 (396)\n8\n(64)\nNVU-1024\n4.74 (108)\n2.42 (212)\n16 (32)\nNVU-2048\n6.40 (80)\n4.13 (124)\n32 (16)\n7.2\nBERT Throughput Requirements\nWe analyze the effective throughput requirement for each nonlin-\nearity in BERT. This builds on the analysis in Table 2, where we\nestablished that the worst-case throughput requirement for soft-\nmax is 32 elements per cycle to keep up with the MMU. However,\nhere we demonstrate that we can relax the worst case requirement\nby taking into account the optimization of overlapping indepen-\ndent computations. Then, we show the final requirements after\noptimization.\n7.2.1\nOverlapping Computation. In most cases, each stage of\nthe transformer network computation is dependent on the results\nof the preceding stage. For instance, the feed-forward layer has a\nmatrix multiply with GELU followed by a matrix multiply with\nLayer Normalization. The GELU computation must be finished\nbefore the next matrix multiply is started. This holds true for all\nLayer Normalization and GELU operations in BERT. This means\nthat GELU and Layer Normalization must be rate matched with the\nMMU in order to avoid stalling the MMU.\nFortunately, this is not the case with softmax and parts of the at-\ntention mechanism. We can reduce the throughput requirements of\nsoftmax by overlapping it with independent matrix computations in\nmulti-headed self-attention. For example, the computation in Table\n1 of softmax((ğ‘„ğ‘–ğ¾ğ‘–ğ‘‡)/ğ‘˜) can be overlapped with the matrix multi-\nplication ğ‘‰ğ‘–= ğ‘‹ğ‘Šğ‘‰ğ‘–. Since computation for each attention head is\nindependent, we can also overlap softmax for head ğ‘–with some part\nof the computation for the next head ğ‘–+ 1. In this way,the through-\nput requirements of the softmax computation can be relaxed by\nmore than 4Ã—.\n7.2.2\nOptimized BERT Throughput Requirements. Taking\noverlapping computations into account, we see the throughput\nrequirements shown in Table 4. In general, the throughput require-\nments of matrix multiplies in BERT do not depend on BERT network\nsequence length. However, for some of the attention computation,\nthere is a dependence on sequence length. This only affects nonlin-\nearity throughput when we overlap independent matrix multiplies\nwith softmax, which is why we see a throughput dependence for\nsoftmax on sequence length in Table 4.\nAlthough softmax tends to have very high throughput require-\nments for higher sequence lengths, it only accounts for a small per-\ncentage of overall computation (see Table 2). Layer Normalization\nand GELU are needed for approximately two thirds of the total com-\nputation time but have relatively lower throughput requirements.\nIf the NVUâ€™s softmax computation cannot match MMU throughput,\nwe may still only get a small inference time overhead. Meanwhile,\nif layer normalization or GELU cannot be throughput-matched, we\nwould expect a more noticeable inference time overhead.\nTable 4: Throughput (elements per cycle) required for differ-\nent BERT input sequence lengths on nonlinear functions for\nNPE (16-bit MMU); Layer Norm A refers to normalization af-\nter attention and Layer Norm B refers to normalization after\nGELU.\nSequence Length\nSoftmax\nLayer Norm A\nLayer Norm B\nGELU\n64\n0.92\n2.6\n0.6\n2.6\n128\n1.79\n2.6\n0.6\n2.6\n256\n3.39\n2.6\n0.6\n2.6\n512\n6.29\n2.6\n0.6\n2.6\nBy comparing results from Tables 3 and 4, we see that NVU-2048\nis more than capable of keeping up with the 16-bit MMU. In fact,\nNVU-1024 can approach or exceed most of the requirements except\nsoftmax with sequence length 512. Given that softmax only takes\nup a few percentage of computation, and a sequence length of 512\nis not needed for most applications, it is evident that there would\nonly be marginal benefit of using NVU-2048 over NVU-1024. For\nthis reason, we only analyze NVU-2048 for its inference time, as\na comparison metric indicating ideal NVU performance (where\nMMU never stalls). Similarly, with the 8-bit MMU, the NVU-2048\nalso nearly matches the matrix multiplier throughput and can be\nused as a reference point.\n8\nEVALUATION\nWe implement NPE at 200 MHz on the Xilinx Zynq Z-7100 FPGA,\nwhich has 2,020 DSP slices, 26.5 Mb RAM, and 277k LUTs. We\nlook at several NVU variants (NVU-256, NVU-512, and NVU-1024),\neach of which can be paired with either the 8-bit or 16-bit MMU.\nWe examine FPGA utilization for each NVU variant separately,\nthen show overall FPGA utilization of each of the six resulting\nNPE configurations. We calculate software-simulated inference\ntimes for BERT for these six configurations and compare them to\nthe corresponding NVU-2048 reference inference time. Finally, we\nevaluate NPEâ€™s performance on BERT inference relative to other\nimplementationsâ€™.\n8.1\nFPGA Utilization\nIn Table 5, we individually show the FPGA utilization results for\nseveral components of the NVU: the NVU memory (NMEM), the\nvector register file (VRF), and the compute units (VCU and SCU).\nThen, in Table 6, we give the cumulative FPGA resource utilization\nfor NPE using each NVU variant, both for 8-bit and 16-bit NPE.\nFrom these results, we see that all the NVU variants are small\nrelative to the overall NPE design. Even NVU-1024 uses less than\nthree percent of overall flip-flop, DSP slice, and BRAM resources\neach. The larger NVUs do use 7-15% of the overall LUT resources,\nmuch of which is due to the muxes required for shifting. Despite\nthis, the overall design still has many LUTs left over.\n8.2\nInference Time\nThe system simulation gives a cycle count estimate for a single\ninference of BERTBASE, which can be used to determine inference\ntime given the operating clock speed. The relative inference times\nFigure 5: BERT inference time percent overhead using NPE\nwith 16-bit MMU for different sequence lengths and NVU\nvariants. Overhead is relative to the minimum time using\nNVU-2048.\nTable 5: FPGA Resource Utilization for components of NVU-256, NVU-512, and NVU-1024 on Zynq Z-7100.\nModule\nğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»\nLUT\nFF\nDSP Slices\nBRAM\nF7 Mux\nF8 Mux\nNMEM\nNVU-256\n776 (0.28%)\n1234 (0.22%)\n0\n4 (0.53%)\n0\n0\nVRF\nNVU-256\n156 (0.06%)\n513 (0.09%)\n0\n4 (0.53%)\n0\n0\nVCU+SCU\nNVU-256\n10328 (3.72%)\n1753 (0.32%)\n8 (0.4%)\n0\n3 (<0.01%)\n0\nTotal\nNVU-256\n11260 (4.06%)\n3500 (0.63%)\n8 (0.4%)\n8 (1.06%)\n3 (<0.01%)\n0\nNMEM\nNVU-512\n1330 (0.48%)\n2268 (0.41%)\n0\n8 (1.06%)\n0\n0\nVRF\nNVU-512\n306 (0.11%)\n1025 (0.18%)\n0\n8 (1.06%)\n0\n0\nVCU+SCU\nNVU-512\n19549 (7.05%)\n3441 (0.62%)\n16 (0.79%)\n0\n12 (<0.01%)\n5 (<0.01%)\nTotal\nNVU-512\n21185 (7.64%)\n6734 (1.21%)\n16 (0.79%)\n16 (2.1%)\n12 (<0.01%)\n5 (<0.01%)\nNMEM\nNVU-1024\n2902 (1.05%)\n4377 (0.79%)\n0\n16 (2.1%)\n350 (0.25%)\n0\nVRF\nNVU-1024\n607 (0.22%)\n2049 (0.37%)\n0\n16 (2.1%)\n0\n0\nVCU/SCU\nNVU-1024\n34423 (12.41%)\n6984 (1.26%)\n32 (1.58%)\n0\n37 (0.03%)\n5 (<0.01%)\nTotal\nNVU-1024\n37932 (13.67%)\n13410 (2.42%)\n32 (1.58%)\n32 (4.2%)\n387 (0.28%)\n5 (<0.01%)\nTable 6: Overall FPGA Resource Utilization on Zynq Z-7100\nfor NPE with 8-bit and 16-bit MMU with NVU-256, NVU-512,\nand NVU-1024.\nMMU\nğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»\nLUT\nFF\nDSP Slices\nBRAM\n8-bit\nNVU-256\n165776 (59.76%)\n341151 (61.49%)\n1994 (98.71%)\n345 (45.70%)\n8-bit\nNVU-512\n175701 (63.33%)\n344385 (62.07%)\n2002 (99.10%)\n353 (46.75%)\n8-bit\nNVU-1024\n192448 (69.37%)\n351061 (63.28%)\n2018 (99.90%)\n369 (48.87%)\n16-bit\nNVU-256\n129231 (46.59%)\n250738 (45.19%)\n1995 (98.76%)\n502.5 (66.56%)\n16-bit\nNVU-512\n139156 (50.16%)\n253972 (45.78%)\n2003 (99.16%)\n510.5 (67.61%)\n16-bit\nNVU-1024\n155903 (56.20%)\n260648 (46.98%)\n2019 (99.95%)\n526.5 (69.73%)\nof NPE with 16-bit MMU and NVU-256, NVU-512, and NVU-1024\nare compared to inference time with NVU-2048. For NPE with 16-bit\nMMU, NVU-2048 gives the ideal inference time because it always\nexceeds the MMU throughput.\nFigure 5 shows the percent inference time overhead of NVUs\nof different ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»for NPE with 16-bit MMU. We see that in\nall cases GELU does not add latency overhead for any sequence\nlength. Overall, NVU-1024 has very little overhead compared to the\nbaseline case. The small difference is because layer normalization\nthroughput is slightly lower than that which is needed to match\nthe MMU. For smaller sequence lengths, NVU-1024 adds less than\n1% latency overhead, NVU-512 adds around 10%, and NVU-256\nadds about 30%. Depending on the use case, these overheads may\nbe acceptable given the reduced area costs. For higher sequence\nlengths, NVU-256 begins to show very large overheads of 53% and\n97%.\nNote that inference time overhead alone is not the only criteria\nthat should be used to evaluate these options. Even larger overheads\nmay be acceptable, as long as the overall inference time including\noverhead is within the target for conversational AI. For this reason,\nthe actual inference time is compared below.\nThe BERT inference time for NPE with 16-bit and 8-bit MMUs\nwith each ğ‘‰ğ‘…ğ‘Šğ¼ğ·ğ‘‡ğ»is shown in Figure 6. We see that NPE with\n8-bit MMU can achieve sub-10 ms inference time with sequence\nlength of 64 even with NVU-512, but that the inference time in-\ncreases proportionally as sequence length increases. For typical\napplications, sequence length of 64 is sufficient. For conversational\nAI, we require within 10-15 ms inference time, which we can clearly\nsurpass with NVU-512 and NVU-1024 for both 8 and 16-bit.\n8.3\nComparison with CPU, GPU, and FPGA\nThe authors of the FTRANS transformer FPGA accelerator [17]\nprovide inference benchmarks by running RoBERTa, an optimized\nversion of BERT with the same model architecture but trained more\nthoroughly. Since BERT and RoBERTa have the same architecture,\nwe can compare our BERT acceleratorâ€™s inference times with their\nRoBERTa benchmarks. We compare with our NPE with 16-bit and\n8-bit MMUs with NVU-1024 on the Zynq Z-7100. The devices used\nin the benchmark are an i7-8700k CPU, an RTX 5000 GPU, and an\nUltrascale+ VCU118 FPGA (for FTRANS). The RTX 5000 has 1.52Ã—\nmore compute units than our Zynq FPGA and runs at 8.1Ã— higher\nclock frequency. The VCU118 has 6,840 DSP slices and 2,586k logic\ncells (3.39Ã— the DSP slices and 5.82Ã— the logic cells on our board).\nThe inference times and relative latencies are shown in Table 7. We\nalso give the approximate power consumption of each device.\nTable 7: Throughput (inference/sec) of NPE with NVU-1024\ncompared with CPU (i7-8700k), GPU (RTX 5000), and FPGA\n(VCU118). We also give relative throughput compared to\nFTRANS, throughput per DSP slice relative to FTRANS (for\nFPGA implementations), and approximate power consump-\ntion.\ni7-8700k\nRTX 5000\nFTRANS\nNPE (16-bit)\nNPE (8-bit)\nThroughput\n3.76\n57.46\n101.79\n73.69\n135.14\nRelative Speedup\n0.037Ã—\n0.56Ã—\n1Ã— (baseline)\n0.72Ã—\n1.33Ã—\nDSP Slices Utilized\n-\n-\n6,840\n2,020\n2,020\nThroughput per DSP\n-\n-\n0.0148 (1Ã—)\n0.0365 (2.5Ã—)\n0.0669 (4.5Ã—)\nApproximate Power (W)\n80\n120\n25\n20\n20\nFrom the results, we see that the CPU is far too slow for con-\nversational AI. While the RTX 5000 GPU gets close, it does not\nmeet the conversational AI latency targets. However, with a larger\nor more optimized GPU it could meet these requirements, albeit\nwith much higher power consumption. Both FTRANS and NPE\nimplementations stay within the range needed for conversational\nAI.\nFigure 6: BERT inference time (in ms) with different NVU widths and sequence lengths. Results are shown separately for NPEs\nwith 8-bit and 16-bit matrix multiplies.\n8.4\nBenchmarks Discussion\nThe biggest benefit of an FPGA implementation of BERT over CPU\nand GPU is with power consumption. From Table 7, we see about\na 4Ã— power benefit over CPU and 6Ã— over GPU. This difference\nin power consumption is especially important for NLP processing\non edge devices. While FTRANS and NPE both have comparable\nperformance and power, FTRANS uses over 3Ã— more resources than\nNPE since it uses a much larger FPGA. We attribute some of the\ndifference in resource consumption to the fact that FTRANS uses\nspecialized modules for each transformer and each nonlinearity,\nwhich leads to additional area and under-utilized components.\n9\nCONCLUSION\nIn this paper we propose NPE, an FPGA-based overlay processor\nthat is domain-specialized for Natural Language Processing. NPE\noffers software-like programmability and provides a unified frame-\nwork to process arbitrarily complex nonlinear functions. If a new\nstate-of-the-art NLP model were to surpass transformers in the\ncoming years, NPE is most likely flexible enough to adapt to it\nwithout requiring reconfiguring the FPGA accelerator or adding\nspecialized processing modules. NPE can also meet the inference\nlatency requirements for conversational AI for the BERT language\nmodel. Relative to CPU and GPU, NPE has 4Ã— and 6Ã— lower power\nconsumption respectively. Our accelerator shows comparable per-\nformance to a transformer model specialized FPGA accelerator, but\nNPE uses 3Ã— lower FPGA resources. Overall, we find that NPE is\na promising solution for low-cost and low-power NLP network\ninference at the edge.\nREFERENCES\n[1] Ray Andraka. 1998. A survey of CORDIC algorithms for FPGA based computers.\nIn Proceedings of the 1998 ACM/SIGDA sixth international symposium on Field\nprogrammable gate arrays. 191â€“200.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv preprint arXiv:1607.06450 (2016).\n[3] Daniel BerjÃ³n, Guillermo Gallego, Carlos Cuevas, Francisco MorÃ¡n, and Narciso\nGarcÃ­a. 2015. Optimal piecewise linear function approximation for GPU-based\napplications. IEEE transactions on cybernetics 46, 11 (2015), 2584â€“2595.\n[4] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon, Sun\nChoi, Kushal Datta, and Vikram Saletore. 2019. Efficient 8-bit quantization\nof transformer neural machine language translation model.\narXiv preprint\narXiv:1906.00532 (2019).\n[5] Marius A Cornea-Hasegan, Roger A Golliver, and Peter Markstein. 1999. Correct-\nness proofs outline for Newton-Raphson based floating-point divide and square\nroot algorithms. In Proceedings 14th IEEE Symposium on Computer Arithmetic\n(Cat. No. 99CB36336). IEEE, 96â€“105.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[7] Gaoming Du, Chao Tian, Zhenmin Li, Duoli Zhang, Yongsheng Yin, and Yim-\ning Ouyang. 2019. Efficient Softmax Hardware Architecture for Deep Neural\nNetworks. In Proceedings of the 2019 on Great Lakes Symposium on VLSI. 75â€“80.\n[8] Christopher L Frenzen, Tsutomu Sasao, and Jon T Butler. 2010. On the number of\nsegments needed in a piecewise linear approximation. Journal of Computational\nand Applied mathematics 234, 2 (2010), 437â€“446.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N\nDauphin. 2017. Convolutional sequence to sequence learning. arXiv preprint\narXiv:1705.03122 (2017).\n[10] Alex Graves. 2012. Sequence transduction with recurrent neural networks. arXiv\npreprint arXiv:1211.3711 (2012).\n[11] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H Oh, Yeonhong Park, Yoonho\nSong, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W Lee, et al. 2020. AË† 3:\nAccelerating Attention Mechanisms in Neural Networks with Approximation. In\n2020 IEEE International Symposium on High Performance Computer Architecture\n(HPCA). IEEE, 328â€“341.\n[12] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 (2016).\n[13] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\narXiv preprint arXiv:1909.10351 (2019).\n[14] Taek-Jun Kwon and Jeffrey Draper. 2009. Floating-point division and square root\nusing a Taylor-series expansion algorithm. Microelectronics Journal 40, 11 (2009),\n1601â€“1605.\n[15] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning\nof language representations. arXiv preprint arXiv:1909.11942 (2019).\n[16] Dong-U Lee, Wayne Luk, John Villasenor, and Peter YK Cheung. 2003. Non-\nuniform segmentation for hardware function evaluation. In International Confer-\nence on Field Programmable Logic and Applications. Springer, 796â€“807.\n[17] Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang Chen,\nMimi Xie, Lipeng Wan, Hang Liu, and Caiwen Ding. 2020. FTRANS: energy-\nefficient acceleration of transformers using FPGA. In Proceedings of the ACM/IEEE\nInternational Symposium on Low Power Electronics and Design. 175â€“180.\n[18] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[19] Junki Park, Hyunsung Yoon, Daehyun Ahn, Jungwook Choi, and Jae-Joon Kim.\n2020. OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neu-\nral network accelerator. In Proceedings of Machine Learning and Systems, I. Dhillon,\nD. Papailiopoulos, and V. Sze (Eds.). Vol. 2. 363â€“378. https://proceedings.mlsys.\norg/paper/2020/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf\n[20] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 (2019).\n[21] Jun Sawada and Ruben Gamboa. 2002. Mechanical verification of a square root\nalgorithm using Taylorâ€™s theorem. In International Conference on Formal Methods\nin Computer-Aided Design. Springer, 274â€“291.\n[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems. 5998â€“6008.\n[23] Jack Volder. 1959. The CORDIC computing technique. In Papers presented at the\nthe March 3-5, 1959, western joint computer conference. 257â€“261.\n[24] Yunxuan Yu, Chen Wu, Tiandong Zhao, Kun Wang, and Lei He. 2019. Opu: An\nfpga-based overlay processor for convolutional neural networks. IEEE Transac-\ntions on Very Large Scale Integration (VLSI) Systems 28, 1 (2019), 35â€“47.\n[25] Yunxuan Yu, Tiandong Zhao, Kun Wang, and Lei He. 2020. Light-OPU: An\nFPGA-based Overlay Processor for Lightweight Convolutional Neural Networks.\nIn The 2020 ACM/SIGDA International Symposium on Field-Programmable Gate\nArrays. 122â€“132.\n[26] Yunxuan Yu, Tiandong Zhao, Mingyu Wang, Kun Wang, and Lei He. 2020. Uni-\nOPU: An FPGA-Based Uniform Accelerator for Convolutional and Transposed\nConvolutional Networks. IEEE Transactions on Very Large Scale Integration (VLSI)\nSystems (2020).\n[27] Bo Yuan. 2016. Efficient hardware architecture of softmax layer in deep neural\nnetwork. In 2016 29th IEEE International System-on-Chip Conference (SOCC). IEEE,\n323â€“326.\n[28] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\nQuantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\n",
  "categories": [
    "cs.AR"
  ],
  "published": "2021-04-13",
  "updated": "2021-04-13"
}