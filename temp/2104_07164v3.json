{
  "id": "http://arxiv.org/abs/2104.07164v3",
  "title": "Unsupervised Continual Learning Via Pseudo Labels",
  "authors": [
    "Jiangpeng He",
    "Fengqing Zhu"
  ],
  "abstract": "Continual learning aims to learn new tasks incrementally using less\ncomputation and memory resources instead of retraining the model from scratch\nwhenever new task arrives. However, existing approaches are designed in\nsupervised fashion assuming all data from new tasks have been manually\nannotated, which are not practical for many real-life applications. In this\nwork, we propose to use pseudo label instead of the ground truth to make\ncontinual learning feasible in unsupervised mode. The pseudo labels of new data\nare obtained by applying global clustering algorithm and we propose to use the\nmodel updated from last incremental step as the feature extractor. Due to the\nscarcity of existing work, we introduce a new benchmark experimental protocol\nfor unsupervised continual learning of image classification task under\nclass-incremental setting where no class label is provided for each incremental\nlearning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC)\ndatasets by incorporating the pseudo label with various existing supervised\napproaches and show promising results in unsupervised scenario.",
  "text": "Unsupervised Continual Learning Via Pseudo Labels\nJiangpeng He , Fengqing Zhu\nSchool of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA\n{he416, zhu0}@purdue.edu\nAbstract\nContinual learning aims to learn new tasks incre-\nmentally using less computation and memory re-\nsources instead of retraining the model from scratch\nwhenever new task arrives. However, existing ap-\nproaches are designed in supervised fashion as-\nsuming all data from new tasks have been manu-\nally annotated, which are not practical for many\nreal-life applications.\nIn this work, we propose\nto use pseudo label instead of the ground truth to\nmake continual learning feasible in unsupervised\nmode.\nThe pseudo labels of new data are ob-\ntained by applying global clustering algorithm and\nwe propose to use the model updated from last\nincremental step as the feature extractor. Due to\nthe scarcity of existing work, we introduce a new\nbenchmark experimental protocol for unsupervised\ncontinual learning of image classiﬁcation task un-\nder class-incremental setting where no class la-\nbel is provided for each incremental learning step.\nOur method is evaluated on the CIFAR-100 and\nImageNet (ILSVRC) datasets by incorporating the\npseudo label with various existing supervised ap-\nproaches and show promising results in unsuper-\nvised scenario.\n1\nIntroduction\nThe success of many deep learning techniques rely on the\nfollowing two assumptions: 1) training data is identically and\nindependently distributed (i.i.d.), which rarely happens if new\ndata and tasks arrive sequentially over time, 2) labels for the\ntraining data are available, which requires additional data an-\nnotation by human effort, and can be noisy as well. Con-\ntinual learning has been proposed to tackle issue #1, which\naims at learning new tasks incrementally without forgetting\nthe knowledge on all tasks seen so far. Unsupervised learn-\ning focuses on addressing issue #2 to learn visual represen-\ntations used for downstream tasks directly from unlabeled\ndata. However, unsupervised continual learning, which is ex-\npected to tackle both issues mentioned above, has not been\nwell studied [Masana et al., 2020]. Therefore, we introduce a\nsimple yet effective method in this work that can be adapted\nFigure 1: Supervised vs. unsupervised continual learning for the\nnew task i. h refers to the model in different incremental steps. The\nsupervised and our proposed pseudo label based unsupervised con-\ntinual learning are illustrated by green and red arrows respectively.\nby existing supervised continual learning approaches in un-\nsupervised setting where no class label is required during the\nlearning phase. We focuses on image classiﬁcation task un-\nder the class-incremental setting [Hsu et al., 2018] and the\nobjective is to learn from unlabeled data for each incremen-\ntal step while providing semantic meaningful clusters on all\nclasses seen so far during inference. Figure 1 illustrates the\ndifference between the typical supervised and proposed un-\nsupervised continual learning scenarios to learn a new task i.\nCurrent continual learning approaches can be generally\nsummarized into three categories including (1) Regulariza-\ntion based, (2) Bias-correction based and (3) Rehearsal\nbased. Our proposed method can be directly embedded into\nexisting supervised approaches in category (1) and (2) with an\nadditional step to extract features of unlabeled data and per-\nform clustering to obtain pseudo label. However, for meth-\nods in (3), selecting exemplars from learned tasks when class\nlabel is not provided in unsupervised scenario is still an un-\nsolved and challenging step. In this work, we tackle this is-\nsue by sampling the unlabeled data from the centroid of each\ngenerated cluster as exemplars to incorporate with Rehearsal\nbased approaches.\nPseudo label [Lee and others, 2013] is widely applied in\nboth semi-supervised and unsupervised learning scenarios to\nhandle unlabeled data for downstream tasks, which is effec-\ntive due to its simplicity, generality and ease of implementa-\ntion. However, whether it is feasible for continual learning\narXiv:2104.07164v3  [cs.CV]  30 Jul 2021\nto rely on pseudo labels instead of human annotations is not\nwell unexplored yet, which is more challenging as we also\nneed to address catastrophic forgetting [McCloskey and Co-\nhen, 1989] in addition to learning new knowledge from unla-\nbeled data.\nIn this work, we adopt K-means [Lloyd, 1982] as our\nglobal clustering algorithm for illustration purpose and we\npropose to use the continual learning model (except the last\nfully connected layers) at every incremental step for feature\nextraction of unlabeled data to obtain pseudo label. The ex-\nemplars used for Rehearsal based approaches are selected af-\nter applying k-means from each generated cluster based on\nthe distance to cluster centroid without requiring the class la-\nbels. Note that we are not proposing new approach to ad-\ndress catastrophic forgetting for continual learning in this\nwork, but instead we test the effectiveness of using pseudo\nlabels to make existing supervised methods feasible in unsu-\npervised setting. Therefore, we incorporate our method with\nexisting representative supervised approaches from all three\ncategories mentioned above including LWF [Li and Hoiem,\n2017], ICARL [Rebufﬁet al., 2017], EEIL [Castro et al.,\n2018], LUCIR [Hou et al., 2019], WA [Zhao et al., 2020] and\nILIO [He et al., 2020]. We show promising performance in\nunsupervised scenario on both CIFAR-100 [Krizhevsky et al.,\n2009] and ImageNet (ILSVRC) [Russakovsky et al., 2015]\ndatasets compared with results in supervised case that do re-\nquire the ground truth for continual learning. The main con-\ntributions of this paper are summarized as follows.\n• We explore a novel problem for continual learning us-\ning pseudo labels instead of human annotations, which\nis both challenging and meaningful for real-life applica-\ntions.\n• Our proposed method can be easily adapted by existing\nsupervised continual learning techniques and we achieve\ncompetitive performance on both CIFAR-100 and Ima-\ngeNet in unsupervised scenario.\n• A new benchmark evaluation protocol is introduced for\nfuture research work and extensive experiments are con-\nducted to analyze the effectiveness of each component\nin our proposed method.\n2\nRelated Work\n2.1\nContinual Learning\nThe major challenge for continual learning is catastrophic\nforgetting [McCloskey and Cohen, 1989] where the model\nquickly forgets already learned knowledge due to the un-\navailability of old data during the learning phase of new\ntasks. Many effective techniques have been proposed to ad-\ndress catastrophic forgetting in supervised scenario, which\ncan be divided into three main categories: (1) Regulariza-\ntion based methods aim to retain old knowledge by con-\nstraining the change of parameters that are important for old\ntasks. Knowledge distillation loss [Hinton et al., 2015] is\none of the representatives, which was ﬁrst applied in [Li and\nHoiem, 2017] to transfer knowledge using soft target dis-\ntribution from teacher model to student model.\nLater the\nvariants of distillation loss proposed in [Hou et al., 2019;\nHe et al., 2020] are shown to be more effective by using\nstronger constraints. (2) Bias-correction based strategy aims\nto maintain the model performance by correcting the bi-\nased parameters towards new tasks in the classiﬁer. Wu et\nal. [Wu et al., 2019] proposed to apply an additional lin-\near layer with a validation sets after each incremental step.\nWeight Aligning (WA) is proposed in [Zhao et al., 2020] to\ndirectly correct the biased weights in the FC layer, which\ndoes not require extra parameters compared with previous\none.\n(3) Rehearsal based methods [Rebufﬁet al., 2017;\nCastro et al., 2018] use partial data from old tasks to periodi-\ncally remind model of already learned knowledge to mitigate\nforgetting.\nHowever, all these methods require class label for the con-\ntinual learning process, which limits their applications in real\nworld. Therefore, in this work we propose to use pseudo la-\nbel obtained from cluster assignments to make existing super-\nvised approaches feasible in unsupervised mode.\n2.2\nUnsupervised Representation Learning\nMany approaches have been proposed to learn visual repre-\nsentation using deep models with no supervision. Cluster-\ning is one type of unsupervised learning methods that has\nbeen extensively studied in computer vision problems [Caron\net al., 2018; Zhan et al., 2020], which requires little do-\nmain knowledge from unlabeled data compared with self-\nsupervised learning [Jing and Tian, 2020].\nCaron et\nal. [Caron et al., 2018] proposed to iteratively cluster features\nand update model with subsequently assigned pseudo labels\nobtained by applying standard clustering algorithm such as\nK-means [Lloyd, 1982]. The most recent work [Zhan et al.,\n2020] propose to perform clustering and model update simul-\ntaneously to address the model’s instability during training\nphase. However, all these existing methods only work on\nstatic datasets and are not capable of learning new knowledge\nincrementally. In addition, the idea of using pseudo label is\nalso rarely explored under continual learning context where\nthe learning environment changes a lot since we need to ad-\ndress catastrophic forgetting as well besides learning visual\nrepresentation from unlabeled data. In this work, we propose\nto use the ﬁxed pseudo label for unsupervised continual learn-\ning which is described in Section 4. We also show in Sec-\ntion 5.5 that iteratively perform clustering to update pseudo\nlabels will result in performance degradation under continual\nlearning context.\n3\nProblem Setup\nContinual learning aims to learn knowledge from a sequence\nof new tasks.\nBroadly speaking, it can be divided into\n(1) task-incremental, (2) domain-incremental, and (3) class-\nincremental as discussed in [Hsu et al., 2018].\nMethods\ndesigned for task-incremental problems use a multi-head\nclassiﬁer for each independent task and domain-incremental\nmethods aim to learn the label shift instead of new classes.\nIn this work, we study the unsupervised scenario under\nclass-incremental setting, which is also known as Single-\nIncremental-Task [Maltoni and Lomonaco, 2019] using a\nsingle-head classiﬁer for inference. Speciﬁcally, the class-\nincremental learning problem T can be formulated as learn-\ning a sequence of N tasks {T 1, ..., T N} corresponding to\nN −1 incremental steps since the learning of the ﬁrst task T 1\nis not related to incremental regime as no previous knowledge\nneed to maintain. Each task T i ∈T for i ∈{1, ...N} con-\ntains M i non-overlapped new classes to learn. In this work,\nwe study class-incremental learning in unsupervised scenario\nstarting from task T 2 for incremental steps only where we\nassume the initial model is supervisedly trained on T 1. Let\n{D1, ..., DN} denotes the training data corresponds to N\ntasks, where Di indicates the data belonging to the task i.\nIn supervised case, Di = {(xi\n1, yi\n1)...(xi\nni, yi\nni)} where x and\ny represent the data and the label respectively, and ni refers\nto the number of total training data in Di. In unsupervised\ncase, we assume the labels of data are unknown for each in-\ncremental step so Di = {xi\n1...xi\nni} for i ∈{2, 3, ...N}. The\nobjective is to learn from unlabeled data for each incremental\nstep while providing semantic meaningful clusters after each\nstep on test data belonging to all classes seen so far.\nFixed step size: As shown above M i refers to the number\nof added new classes for task T i ∈T , which is also deﬁned\nas incremental step size . Existing benchmark protocols [Re-\nbufﬁet al., 2017; He et al., 2020; Hou et al., 2019] for super-\nvised continual learning use a ﬁxed step size M over all tasks\nwhere M i = M for i ∈1, ...N and the continual learning\nunder variable step size is not well studied yet even in su-\npervised case. Therefore, we also assume that the number of\nnew added classes for each task remain unchanged over the\nentire continual learning process, i.e. the ﬁxed step size M\nis known in advance in our unsupervised setting while class\nlabels of data in each incremental step are not provided.\nOnline and ofﬂine implementation: Based on training re-\nstriction, continual learning methods can be implemented as\neither online or ofﬂine where the former methods [He et al.,\n2020] use each data only once to update the model and the\ndata can be used for multiple times in the ofﬂine case. In\ngeneral, the online scenario is more closer to real life set-\nting but is also more challenging to realize. In this work, our\nproposed method is implemented in both online and ofﬂine\nscenarios for unsupervised continual learning. Note that for\nour implementation in online case, we assume that we have\naccess to all training data {xi\n1...xi\nni} ∈Di before the learning\nof each new task i but we use each data only once to update\nthe model.\n4\nOur Method\nIn this work, we propose a simple yet effective method for\nunsupervised continual learning using pseudo label obtained\nbased on cluster assignments. The overall procedure to learn\na sequence of new tasks {T 1, ..., T N} are illustrated in Algo-\nrithm 1. The updated model after learning each task is eval-\nuated to provide semantic meaningful clusters on all classes\nseen so far.\nFor illustration purpose, we adopt k-means as our global\nclustering algorithm to generate cluster assignments and ob-\ntain pseudo label, which will be illustrated in Section 4.1.\nThen, we demonstrate how to easily incorporate our method\nwith existing supervised approaches in Section 4.2.\nAlgorithm 1 Unsupervised Continual Learning\nInput: a sequence of N tasks {T 1, ..., T N}\nInput: An initial model h0\nRequire: Clustering algorithm Θ\nOutput: Updated model hN\n1: M 1 ←|T 1|class {Added classes in ﬁrst task}\n2: h1 ←Learning(T 1, h0) {Learning ﬁrst task}\n3: for i = 2, ..., N do\n4:\nM i ←|T i|class{number of new added classes}\n5:\nDi ←{x1, ..., xni} {Unlabeled training data in T i}\n6:\nhfe ←hi−1 {Feature extractor}\n7:\n{˜a1,.. ˜ani} ←Θ(hfe(Di)) {Cluster assignments}\n8:\n˜Y i ←{\n˜yk\nk=1,...ni\n= ˜ak +\ni−1\nP\nj=1\nM j} {Pseudo label}\n9:\nhi ←Continual Learning(Di, ˜Y i, hi−1)\n10: end for\n11: return hN\n4.1\nClustering: Obtain Pseudo Label\nClustering is one of the most common methods for unsu-\npervised learning, which requires little domain knowledge\ncompared with self-supervised techniques. We focus on us-\ning a general clustering method such as K-means [Lloyd,\n1982], while we also provide the experimental results using\nother clustering methods as illustrated in Appendix, which\nindicates that the choice is not critical for continual learn-\ning performance in our setting.\nSpeciﬁcally, K-means al-\ngorithm learns a centroid matrix C together with cluster as-\nsignments ˜ak for each input data xk by iteratively minimiz-\ning\n1\nN\nPN\nk=1 ||hfe(xk) −C˜ak||2\n2, where hfe refers to the\nfeature extractor.\nLet m and n represent the number of\nlearned classes and new added classes respectively, then we\nhave ˜ak ∈{1, 2, ..., n} and the pseudo label ˜Y for contin-\nual learning is obtained by {˜yk = ˜ak + m|k = 1, 2, ..} and\n˜yk ∈{m + 1, m + 2, ..., m + n}.\nLearning visual representation from unlabeled data using\npseudo label is proposed in [Caron et al., 2018], which itera-\ntively performs clustering and updating the feature extractor.\nHowever, they are not capable of learning new classes incre-\nmentally and the learning environment changes under con-\ntinual learning context as we need to maintain the learned\nknowledge as well as learning from new tasks. Therefore, in\nthis work we propose to apply the model, hfe = hi−1, ob-\ntained after incremental step i −1 (except the last fully con-\nnected layer) as the feature extractor for incremental step i to\nextract feature embeddings on all unlabeled data belonging to\nthe new task. Next, we apply k-means based on extracted fea-\ntures to generate cluster assignments and use the ﬁxed pseudo\nlabel ˜Y to learn from new task during the entire incremental\nlearning step i. We show in our experiments later that alter-\nnatively performing clustering and use pseudo label to update\nthe model as in [Caron et al., 2018] will result in performance\ndegradation which is discussed in Section 5.5. Note that we\nassume h1 is obtained from T 1 in supervised mode as illus-\ntrated in Section 3, so in this work we mainly focus on how\nto incrementally learn new classes from unlabeled data while\nmaintaining performance on all old classes seen so far.\n4.2\nIncorporating into Supervised Approaches\nThe obtained pseudo label ˜Y can be easily incorporated with\nRegularization-based methods using knowledge distillation\nloss or its variants.\nThe distillation loss is formulated by\nEquation 1\nLD = 1\nN\nN\nX\nk=1\nm\nX\nr=1\n−ˆp(r)\nT (xk)log[p(r)\nT (xk)]\n(1)\nˆp(r)\nT\n=\nexp (ˆo(r)/T)\nPm\nj=1 exp (ˆo(j)/T) , p(r)\nT\n=\nexp (o(r)/T)\nPm\nj=1 exp (o(j)/T)\nwhere ˆom×1 and om×1 denote the output logits of student and\nteacher models respectively for the m learned classes. T is\nthe temperature scalar used to soften the probability distribu-\ntion. The cross entropy loss to learn the added n new classes\ncan be expressed as\nLC = 1\nN\nN\nX\nk=1\nn+m\nX\nr=1\n−˜y(r)\nk log[p(r)(xk)]\n(2)\nwhere ˜yk ∈˜Y is the obtained pseudo label for data xk instead\nof the ground truth labels in supervised case. Then the cross-\ndistillation loss combining cross entropy LC and distillation\nLD is formulated in Equation 3 with a hyper-parameter α =\nm\nm+n to tune the inﬂuence between two terms.\nLCD(x) = αLD(x) + (1 −α)LC(x)\n(3)\nHerding dynamic algorithm [Welling, 2009] is widely ap-\nplied for Rehearsal based methods to select exemplars based\non class mean in supervised case. However, since no class la-\nbel is provided in unsupervised scenario, we instead propose\nto select exemplars based on cluster mean. Algorithm 2 de-\nscribes exemplar selection step for task T i. The exemplar set\nQ stores the data and pseudo label pair denoted as (xk, ˜yk).\nThe incorporation with Bias-correction based methods is\nthe most straightforward. BIC [Wu et al., 2019] applies an\nadditional linear model for bias correction after each incre-\nmental step using a small validation set containing balanced\nold and new class data. In our unsupervised scenario, both\nthe training and validation set used to estimate bias can be\nconstructed using obtained pseudo label instead of the ground\ntruth. The most recent work WA [Zhao et al., 2020] calculates\nthe norms of weights vectors in FC layer for old and new class\nrespectively and use the ratio to correct bias without requiring\nextra parameters. Thus our method can be directly embedded\nwith it by an addition step to obtain pseudo label as illustrated\nin Section 4.1.\nWe emphasize that we are not introducing new method\nto address catastrophic forgetting, but rather investigating\nwhether it is possible to use pseudo labels instead of ground\ntruth labels for continual learning. We show in Section 5\nthat our proposed method works effectively with existing ap-\nproaches from all categories mentioned above.\nAlgorithm 2 Unsupervised Exemplar Selection\nInput: image set Di = {x1, ..., xni} from task T i\nInput: q target exemplars per class\nRequire: clustering algorithm Θ\nRequire: feature extractor hfe = hi\nOutput: Exemplar set Q\n1: M i ←|T i|class{number of new added classes}\n2: {˜a1,.. ˜ani} ←Θ(hfe(Di)) {Cluster assignments}\n3: for j = 1, 2,..., Mi do\n4:\nXj ←{xn|˜an = j}\n5:\nµj ←\n1\n|Xj|\nP\nx∈Xj hfe(x) {Cluster mean}\n6:\nfor k = 1, 2,..., q do\n7:\nek ←argmin\nx∈Xj\n{µj −1\nk[hfe(x) + Pk−1\nl=1 hfe(el)]}\n{herding selection [Welling, 2009] within cluster}\n8:\nend for\n9:\nQ ←Q ∪{e1, ..., eq}\n10: end for\n11: return Q\n5\nExperimental Results\nIn this section, we evaluate our proposed method from two\nperspectives.\n1) We incorporate with existing approaches\nand compare results obtained in unsupervised and supervised\ncases to show the ability of using pseudo labels for unsuper-\nvised continual learning to provide semantic meaningful clus-\nters for all classes seen so far. 2) We analyze the effectiveness\nof each component in our proposed method including the ex-\nemplar selection and the choice of feature extractor in unsu-\npervised scenario. These experimental results are presented\nand discussed in Sections 5.4 and 5.5, respectively. (Addi-\ntional results are available in Appendix)\n5.1\nBenchmark Experimental Protocol\nAlthough different benchmark experimental protocols are\nused in supervised case [Rebufﬁet al., 2017; Hou et al., 2019;\nHe et al., 2020], there is no agreed protocol for evaluation of\nunsupervised continual learning methods. In addition, var-\nious learning environments may happen when class label is\nnot available so it is impossible to use one protocol to evalu-\nate upon all potential scenarios. Thus, our proposed new pro-\ntocol focuses on class-incremental learning setting and aims\nto evaluate the ability of unsupervised methods to learn from\nunlabeled data while maintaining the learned knowledge dur-\ning continual learning. Speciﬁcally, the following assump-\ntions are made: (1) all the new data belong to new class, (2)\nthe number of new added class (step size) is ﬁxed and known\nbeforehand, (3) no class label is provided for learning (except\nfor the initial step) and (4) the updated model should be able\nto provide semantic meaningful clusters for all classes seen so\nfar during inference. Our protocol is introduced based on cur-\nrent research progress for supervised class-incremental learn-\ning and three benchmark datasets are considered including (i)\nCIFAR-100 [Krizhevsky et al., 2009] with step size 5, 10, 20,\n50 (ii) ImageNet-1000 (ILSVRC) [Russakovsky et al., 2015]\nwith step size 100 and (iii) ImageNet-100 (100 classes subset\nof ImageNet-1000) with step size 10. Top-1 and Top-5 ACC\nDatasets\nCIFAR-100\nImageNet\nStep size\n5\n10\n20\n50\n10\n100\nACC\nAvg\nLast\nAvg\nLast\nAvg\nLast\nAvg\nLast\nAvg\nLast\nAvg\nLast\nLWF (w/)\n0.299\n0.155\n0.393\n0.240\n0.465\n0.352\n0.512\n0.512\n0.602\n0.391\n0.528\n0.374\nLWF+Ours (w/o, ∆)\n-0.071\n-0.029\n-0.091\n-0.025\n-0.086\n-0.062\n-0.095\n-0.095\n-0.033\n-0.053\n-0.211\n-0.174\nICARL (w/)\n0.606\n0.461\n0.626\n0.518\n0.641\n0.565\n0.607\n0.607\n0.821\n0.644\n0.608\n0.440\nICARL+Ours (w/o, ∆)\n-0.084\n-0.045\n-0.135\n-0.142\n-0.158\n-0.174\n-0.108\n-0.108\n-0.043\n-0.047\n-0.197\n-0.015\nEEIL (w/)\n0.643\n0.482\n0.638\n0.517\n0.637\n0.565\n0.603\n0.603\n0.893\n0.805\n0.696\n0.520\nEEIL+Ours (w/o, ∆)\n-0.071\n-0.043\n-0.131\n-0.121\n-0.131\n-0.148\n-0.088\n-0.088\n-0.040\n-0.064\n-0.199\n-0.154\nLUCIR (w/)\n0.623\n0.478\n0.631\n0.521\n0.647\n0.589\n0.642\n0.642\n0.898\n0.835\n0.834\n0.751\nLUCIR+Ours (w/o, ∆)\n-0.015\n-0.003\n-0.104\n-0.106\n-0.131\n-0.152\n-0.111\n-0.111\n-0.037\n-0.083\n-0.293\n-0.342\nWA (w/)\n0.643\n0.496\n0.649\n0.535\n0.669\n0.592\n0.655\n0.655\n0.905\n0.841\n0.859\n0.811\nWA+Ours (w/o, ∆)\n-0.034\n-0.014\n-0.110\n-0.106\n-0.121\n-0.136\n-0.092\n-0.092\n-0.037\n-0.056\n-0.295\n-0.376\nILIO (w/)\n0.664\n0.515\n0.676\n0.564\n0.681\n0.621\n0.652\n0.652\n0.903\n0.845\n0.696\n0.601\nILIO+Ours (w/o, ∆)\n-0.123\n-0.194\n-0.140\n-0.175\n-0.134\n-0.157\n-0.106\n-0.106\n-0.057\n-0.118\n-0.178\n-0.212\nTable 1: Summary of unsupervised results and the comparison with supervised case. The average ACC (Avg) over all incremental steps\nand the last step ACC (Last) are reported. w/ and w/o denote with or without label for continual learning, i.e. supervised or unsupervised.\n{∆= w/ −w/o} shows the performance difference. Spotlight results (|∆| < 0.05) for Avg accuracy are marked in bold.\n(a)\n(b)\n(c)\n(d)\nFigure 2: Results on CIFAR-100 with step size 5, 10, 20, and 50 by incorporating our method with existing approaches to realize continual\nlearning in unsupervised scenario. (Best viewed in color)\nare used for CIFAR-100 and ImageNet, respectively.\n5.2\nEvaluation Metrics\nWe evaluate our method using cluster accuracy (ACC), which\nis widely applied in unsupervised setting [Caron et al., 2018;\nVan Gansbeke et al., 2020] when class label is not provided.\nWe ﬁrst ﬁnd the most represented class label for each cluster\nusing Hungarian matching algorithm [Kuhn, 1955], and then\ncalculate the accuracy as Nc\nN where N is the total number of\ndata and Nc is the number of correctly classiﬁed data. Note\nthat the classiﬁcation accuracy used in supervised setting is\nconsistent with cluster accuracy and is widely used for per-\nformance comparison in unsupervised case as in [Van Gans-\nbeke et al., 2020]. In this work, ACC is used to evaluate the\nmodel’s ability to provide semantic meaningful clusters.\n5.3\nImplementation Detail\nOur implementation is based on Pytorch [Paszke et al., 2017]\nand we use ResNet-32 for CIFAR-100 and ResNet-18 for Im-\nageNet. The ResNet implementation follows the setting as\nsuggested in [He et al., 2016]. The setting of incorporated\nexisting approaches follows their own repositories. We select\nq = 20 exemplars per cluster to construct exemplar set and ar-\nrange classes using identical random seed (1993) with bench-\nmark supervised experiment protocol [Rebufﬁet al., 2017].\nWe ran ﬁve times for each experiment and the average perfor-\nmance is reported.\n5.4\nIncorporating with Supervised Approaches\nIn this part, our method is evaluated when incorporated\ninto existing supervised approaches including LWF [Li and\nHoiem, 2017], ICARL [Rebufﬁet al., 2017], EEIL [Cas-\ntro et al., 2018], LUCIR [Hou et al., 2019], WA [Zhao et\nal., 2020] and ILIO [He et al., 2020], which are representa-\ntive methods from all Regularization based, Bias-correction\nbased and Rehearsal based categories as described in Sec-\ntion 2. Note that ILIO is implemented in online scenario\nwhere each data is used only once to update model while oth-\ners are implemented in ofﬂine. We embed the pseudo label as\nillustrated in Section 4 to evaluate the performance of selected\napproaches in unsupervised mode. E.g. ICARL + Ours de-\nnotes the implementation of ICARL in unsupervised mode\nby incorporating with our proposed method. Table 1 sum-\nmarizes results in terms of last step ACC (Last) and average\nACC (Avg) calculated by averaging ACC for all incremental\nsteps, which shows overall performance for the entire con-\ntinual learning procedure. We also report the performance\ndifference ∆= w/ −w/o and observe only small degrada-\ntion by comparing unsupervised results with supervised re-\nsults. In addition, we calculate the average accuracy drop\nby Avg(∆) = Avg(w/) −Avg(w/o) for each incremen-\ntal step corresponds to each method. The Avg(∆) ranges\nfrom [0.015, 0.295] with an average of 0.114. Our method\ncan work well with but not limited to these selected repre-\nDatasets\nCIFAR-100\nImageNet\nStep size\n5\n10\n20\n50\n10\n100\nACC\nAvg\nLast\nAvg\nLast\nAvg\nLast\nAvg\nLast\nAvg\nLast\nAvg\nLast\nScratch\n0.106\n0.038\n0.095\n0.015\n0.122\n0.038\n0.226\n0.226\n0.282\n0.158\n0.069\n0.023\nPCA\n0.156\n0.085\n0.143\n0.061\n0.171\n0.083\n0.287\n0.287\n0.308\n0.175\n/\n/\nFFE\n0.459\n0.338\n0.399\n0.281\n0.401\n0.323\n0.392\n0.392\n0.757\n0.620\n0.405\n0.275\nUPL-10\n0.498\n0.376\n0.415\n0.293\n0.430\n0.320\n0.401\n0.401\n0.797\n0.653\n0.446\n0.294\nUPL-20\n0.523\n0.394\n0.422\n0.296\n0.445\n0.339\n0.413\n0.413\n0.816\n0.699\n0.458\n0.311\nUPL-30\n0.513\n0.383\n0.435\n0.324\n0.459\n0.364\n0.433\n0.433\n0.832\n0.705\n0.460\n0.332\nOurs\n0.558\n0.426\n0.482\n0.368\n0.486\n0.397\n0.495\n0.495\n0.849\n0.722\n0.471\n0.342\nTable 2: Ablation study for different approaches to obtain pseudo labels on CIFAR-100 and ImageNet in terms of average ACC (Avg)\nand last step ACC (Last). The best results are marked in bold.\nsentative methods and we achieve competitive performance\nin unsupervised scenario without requiring human annotated\nlabels during continual learning phase. Figure 2 shows clus-\nter accuracy for each incremental step on CIFAR-100. (More\nresults and discussion are available in the Appendix)\n5.5\nAblation Study\nWe conduct extensive experiments to 1) analyze the unsuper-\nvised exemplar selection step as described in Section 4.2 by\nvarying the number of exemplars per class and compare the\nresults with random selection. 2) Study the impacts of dif-\nferent methods that can be used to extract feature for clus-\ntering to obtain pseudo label during continual learning. For\nboth experiments, we ﬁrst construct our baseline method de-\nnoted as Ours, which uses distillation loss as described in\nEquation 3 and exemplars from learned tasks as described in\nAlgorithm 2. (see implementation detail in Appendix)\nFor part 1), we vary the target number of exemplars per\nclass q ∈{10, 20, 50, 100} and compare the results with ran-\ndom exemplar selection from each generated cluster, denoted\nas Random. The results on CIFAR-100 are shown in Fig-\nure 3. We observe that the overall performance will be im-\nproved by increasing q even using randomly selected exem-\nplars. In addition, our proposed method, which selects ex-\nemplars based on cluster mean, outperforms Random by a\nlarger margin when q becomes larger.\nFor part 2), we compare our method using the updated\nmodel from last incremental step as feature extractor as illus-\ntrated in Algorithm 1 with i) Scratch: apply a scratch model\nwith the same network architecture as feature extractor, ii)\nPCA: directly apply PCA algorithm [Wold et al., 1987] on\ninput images to obtain feature embeddings for clustering, iii)\nFixed Feature Extractor (FFE): use model h1 as described\nin Section 4.1 as the ﬁxed feature extractor for the entire con-\ntinual learning process, iv) Updated Pseudo Label (UPL-\nK): iteratively update model and perform clustering within\neach incremental step as proposed in [Caron et al., 2018],\nwhere K indicates how frequently we update the pseudo la-\nbel e.g. UPL - 10 means we update pseudo label for every\n10 epochs. All these variants are modiﬁed based on our base-\nline method. Results are summarized in Table 2. The scratch\nmethod provides lower bound performance and FFE outper-\nforms PCA by a large margin, showing the advanced abil-\nity of using deep models to extract more discriminative fea-\nture for clustering. Note that we did not perform PCA on\nImageNet-1000 as it takes quite a long time for computation.\nFigure 3: Results on CIFAR-100 by varying target exemplar size\nq ∈{10, 20, 50, 100} and comparison with random selection.\nComparing UPL-K with K = 0, 10, 20, 30 (K = 0 is Ours),\nwe observe that if the updating frequency increases (K de-\ncreases), the overall performance degrades. As discussed in\nSection 4.1, different from unsupervised representation learn-\ning that uses a model from scratch, in continual learning we\nalso need to preserve the learned knowledge for all classes\nseen so far and update pseudo label repeatedly will accelerate\nthe catastrophic forgetting, resulting in the performance drop.\n6\nConclusion\nIn summary, we explore a novel problem of unsupervised\ncontinual learning under class-incremental setting where the\nobjective is to learn new classes incrementally while provid-\ning semantic meaningful clusters on all classes seen so far.\nWe proposed a simple yet effective method using pseudo la-\nbels obtained based on cluster assignments to learn from un-\nlabeled data for each incremental step. We introduced a new\nexperimental protocol and evaluate our method on benchmark\nimage classiﬁcation datasets including CIFAR-100 and Ima-\ngeNet (ILSVRC). We demonstrate that our method can be\neasily embedded with various existing supervised approaches\nimplemented under both online and ofﬂine modes to achieve\ncompetitive performance in unsupervised scenario. Finally,\nwe show that our proposed exemplar selection method works\neffectively without requiring ground truth and iteratively up-\ndating pseudo labels will cause performance degradation un-\nder continual learning context.\nReferences\n[Caron et al., 2018] Mathilde Caron, Piotr Bojanowski, Ar-\nmand Joulin, and Matthijs Douze. Deep clustering for un-\nsupervised learning of visual features. Proceedings of the\nEuropean Conference on Computer Vision, 2018.\n[Castro et al., 2018] Francisco M. Castro, Manuel J. Marin-\nJimenez, Nicolas Guil, Cordelia Schmid, and Karteek Ala-\nhari. End-to-end incremental learning. Proceedings of the\nEuropean Conference on Computer Vision, 2018.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 770–778, 2016.\n[He et al., 2020] Jiangpeng He, Runyu Mao, Zeman Shao,\nand Fengqing Zhu.\nIncremental learning in online sce-\nnario. Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 13926–13935,\n2020.\n[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, and\nJeffrey Dean. Distilling the knowledge in a neural net-\nwork. Proceedings of the NIPS Deep Learning and Repre-\nsentation Learning Workshop, 2015.\n[Hou et al., 2019] Saihui Hou, Xinyu Pan, Chen Change\nLoy, Zilei Wang, and Dahua Lin. Learning a uniﬁed clas-\nsiﬁer incrementally via rebalancing. Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 831–839, 2019.\n[Hsu et al., 2018] Yen-Chang Hsu, Yen-Cheng Liu, Anita\nRamasamy, and Zsolt Kira. Re-evaluating continual learn-\ning scenarios: A categorization and case for strong base-\nlines. arXiv preprint arXiv:1810.12488, 2018.\n[Jing and Tian, 2020] Longlong Jing and Yingli Tian. Self-\nsupervised visual feature learning with deep neural net-\nworks: A survey. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2020.\n[Krizhevsky et al., 2009] Alex Krizhevsky, Geoffrey Hinton,\net al. Learning multiple layers of features from tiny im-\nages. 2009.\n[Kuhn, 1955] Harold W Kuhn. The hungarian method for the\nassignment problem. Naval research logistics quarterly,\n2(1-2):83–97, 1955.\n[Lee and others, 2013] Dong-Hyun Lee et al. Pseudo-label:\nThe simple and efﬁcient semi-supervised learning method\nfor deep neural networks. Workshop on challenges in rep-\nresentation learning, ICML, 2013.\n[Li and Hoiem, 2017] Zhizhong\nLi\nand\nDerek\nHoiem.\nLearning without forgetting. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 40(12):2935–\n2947, 2017.\n[Lloyd, 1982] Stuart Lloyd.\nLeast squares quantization\nin pcm.\nIEEE transactions on information theory,\n28(2):129–137, 1982.\n[Maltoni and Lomonaco, 2019] Davide Maltoni and Vin-\ncenzo Lomonaco.\nContinuous learning in single-\nincremental-task scenarios. Neural Networks, 2019.\n[Masana et al., 2020] Marc Masana, Xialei Liu, Bartlomiej\nTwardowski, Mikel Menta, Andrew D Bagdanov, and\nJoost van de Weijer.\nClass-incremental learning:\nsurvey and performance evaluation.\narXiv preprint\narXiv:2010.15277, 2020.\n[McCloskey and Cohen, 1989] Michael\nMcCloskey\nand\nNeal J Cohen. Catastrophic interference in connectionist\nnetworks: The sequential learning problem. In Psychology\nof Learning and Motivation, volume 24, pages 109–165.\nElsevier, 1989.\n[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith\nChintala, Gregory Chanan, Edward Yang, Zachary De-\nVito, Zeming Lin, Alban Desmaison, Luca Antiga, and\nAdam Lerer. Automatic differentiation in PyTorch. Pro-\nceedings of the Advances Neural Information Processing\nSystems Workshop, 2017.\n[Rebufﬁet al., 2017] Sylvestre-Alvise Rebufﬁ,\nAlexander\nKolesnikov, Georg Sperl, and Christoph H. Lampert.\niCaRL: Incremental classiﬁer and representation learning.\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, July 2017.\n[Russakovsky et al., 2015] Olga Russakovsky,\nJia Deng,\nHao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael\nBernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet\nLarge Scale Visual Recognition Challenge. International\nJournal of Computer Vision, 115(3):211–252, 2015.\n[Van Gansbeke et al., 2020] Wouter Van Gansbeke, Simon\nVandenhende, Stamatios Georgoulis, Marc Proesmans,\nand Luc Van Gool.\nScan: Learning to classify images\nwithout labels. European Conference on Computer Vision,\npages 268–285, 2020.\n[Welling, 2009] Max Welling. Herding dynamical weights\nto learn. Proceedings of the International Conference on\nMachine Learning, pages 1121–1128, 2009.\n[Wold et al., 1987] Svante Wold, Kim Esbensen, and Paul\nGeladi. Principal component analysis. Chemometrics and\nintelligent laboratory systems, 2(1-3):37–52, 1987.\n[Wu et al., 2019] Yue Wu, Yinpeng Chen, Lijuan Wang,\nYuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.\nLarge scale incremental learning. Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\nJune 2019.\n[Zhan et al., 2020] Xiaohang Zhan, Jiahao Xie, Ziwei Liu,\nYew-Soon Ong, and Chen Change Loy. Online deep clus-\ntering for unsupervised representation learning.\npages\n6688–6697, 2020.\n[Zhao et al., 2020] Bowen Zhao, Xi Xiao, Guojun Gan, Bin\nZhang, and Shu-Tao Xia. Maintaining discrimination and\nfairness in class incremental learning. Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 13208–13217, 2020.\nFigure 4: Overview of our baseline solution to learn the new task\ni. h refers to the model in different steps and m denotes the number\nof learned classes so far after task i −1. Firstly, we apply hi−1\n(except the last fully connected layer) to extract feature embeddings\nused for K-means clustering where the number 1, 2, 3 denote the\ncorresponding cluster assignments. In step 2 we obtain the pseudo\nlabel 1+m, 2+m, 3+m respectively. Finally in step 3, the unlabeled\ndata with pseudo label is used together for continual learning.\nA\nImplementation Detail For Our Baseline\nMethod\nIn the paper, we conduct extensive experiments in Section\n5.51 using a baseline solution denoted as Ours. The overview\nof our baseline method is shown in Figure 4, which incorpo-\nrates pseudo labels introduced in Section 4.1 with knowledge\ndistillation loss as in Equation 3 and exemplar replay as de-\nscribed in Algorithm 2. Thus, the difference between our\nbaseline solution and EEIL is that we exclude data augmen-\ntation and balanced ﬁne-tuning steps.\nWe apply ResNet-32 for CIFAR and ResNet-18 for Ima-\ngeNet, which keeps same with in Section 5.3. We use batch\nsize of 128 with initial learning rate of 0.1. SGD optimizer is\napplied with weight decay of 0.00001. We train 120 epochs\nfor each incremental step and the learning rate is decreased\nby 1/10 for every 30 epochs. We perform each experiment\n5 times and the average results are reported in Table 2 and\nFigure 3.\nB\nAdditional Experimental Results\nIn this section, we show additional experiment results for (1)\ncluster accuracy for each incremental step on ImageNet. (2)\nAnalysis of performance drop compared with supervised re-\nsults corresponds to Table 1. (3) Impact of different cluster-\ning algorithms by comparing K-means clustering with Gaus-\nsian Mixture Models (GMM). (4) Results in terms of other\nevaluation metrics including Normalized Mutual Information\n(NMI) and Adjusted Rand Index (ARI). Both experiments are\nimplemented by using our baseline solution as described in\nSection A on test data of CIFAR-100 with step size 5, 10, 20,\n50.\n1Section, Table and Figure references marked in bold can be\nfound in the submitted paper\nFigure 5: Results on ImageNet with step size (a) 10 on ImageNet-\n100 and (b) 100 on ImageNet-1000.(Best viewed in color)\nDatasets\nCIFAR-100\nImageNet\nStep size\n5\n10\n20\n50\n10\n100\nLWF\n-0.071\n-0.091\n-0.086\n-0.095\n-0.033\n-0.211\nICARL\n-0.084\n-0.132\n-0.158\n-0.108\n-0.043\n-0.197\nEEIL\n-0.071\n-0.131\n-0.131\n-0.088\n-0.040\n-0.199\nLUCIR\n-0.015\n-0.104\n-0.131\n-0.111\n-0.037\n-0.293\nWA\n-0.034\n-0.110\n-0.121\n-0.092\n-0.037\n-0.295\nILIO\n-0.123\n-0.140\n-0.134\n-0.106\n-0.057\n-0.178\nTable 3:\nSummary of performance degradation Avg(∆) =\nAvg(w/) −Avg(w/o) in terms of average accuracy Avg.\nB.1\nResults on ImageNet\nThe cluster accuracy evaluated after each incremental step on\nCIFAR-100 with different step sizes are shown in Figure 2\nand in this part we provide the results on ImageNet with step\nsize 10 and 100 as shown in Figure 5.\nB.2\nAnalysis of Performance Drop\nIn Section 5.4, we incorporate our method with existing su-\npervised approaches and results are shown in Table 1. In this\npart, we further investigate the performance degradation in\nunsupervised scenario. Speciﬁcally, we calculate the average\naccuracy drop by Avg(∆) = Avg(w/)−Avg(w/o) for each\nincremental step corresponds to each method. The results are\nshown in Table 3 where Avg(∆) ranges from [0.015, 0.295]\nwith an average of 0.114. We notice that the performance\ndegradation for each incremental step do not vary a lot for\ndifferent approaches. Therefore, the methods with higher ac-\ncuracy in supervised case are more likely to achieve higher\nperformance in unsupervised scenario by incorporating with\nour pseudo labels. In addition, the performance degradation\nwill increase in online scenario (ILIO) as well as for very\nlarge incremental step size (100), which are both challeng-\ning cases even in supervised continual learning with human\nannotations.\nB.3\nK-means VS. GMM\nAs illustrated in Section 4, our proposed method use K-\nmeans clustering for illustration purpose to obtain the pseudo\nlabels and to sample exemplars. In this part, we show the\nresults in terms of cluster accuracy (ACC) for each incre-\nmental step by comparing K-means with GMMs, which es-\ntimates the parameters of each Gaussian distribution through\nexpectation-maximization algorithm.\nFigure 6 shows the\nACC results on CIFAR-100 with step size 5, 10, 20 and 50.\n(a)\n(b)\n(c)\n(d)\nFigure 6: Results of test data on CIFAR-100 for comparing K-means and GMM with incremental step size (a) 5, (b) 10, (c) 20 and (d) 50.\nThe Accuracy refers to cluster accuracy (ACC). (Best viewed in color)\n(a)\n(b)\n(c)\n(d)\nFigure 7: Ofﬂine results in terms of NMI and ARI of test data on CIFAR-100 with incremental step size (a) 5, (b) 10, (c) 20 and (d) 50. The\nAccuracy refers to cluster accuracy (ACC). Note that only unsupervised incremental results (starting from task 2) are reported in this part.\n(Best viewed in color)\nWe observe only small performance difference for all incre-\nmental steps, which shows that the choice of clustering meth-\nods are not crucial for our proposed method.\nB.4\nResults Evaluated By NMI and ARI\nIn the paper we use cluster accuracy (ACC) to evaluate the\nmodel’s ability to provide semantic meaningful cluster on test\ndata as illustrated in Section 5.2. In this part, in addition to\nACC, we also provide results in terms of NMI and ARI to\nmeasure the quality of obtained semantic meaningful clusters.\nLet A and B refer to ground truth labels and generated cluster\nassignments.\nNMI measures the shared information between two clus-\ntering assignments A and B by\nNMI(A, B) =\nI(A, B)\np\nH(A)H(B)\nwhere H(•) and I(•) denote entropy and mutual information,\nrespectively.\nARI is deﬁned by\nARI(A, B) =\nP\ni,j\n\u0012\nNi,j\n2\n\u0013\n−\nP\ni\n Ni\n2\n!\nP\nj\n Nj\n2\n!\n N\n2\n!\n1\n2[P\ni\n\u0012\nNi\n2\n\u0013\n+ P\nj\n\u0012\nNj\n2\n\u0013\n] −\nP\ni\n Ni\n2\n!\nP\nj\n Nj\n2\n!\n N\n2\n!\nwhere\n\u0012\nN\n2\n\u0013\nrefers to binomial coefﬁcients and N is the to-\ntal number of data in the cluster. Ni and Nj denote number\nof data with cluster assignment Ci in B and the number of\ndata with class label C⋆\nj in A, respectively. Ni,j is the num-\nber of data with the class label C⋆\nj ∈A assigned to cluster\nassignment Ci ∈B.\nNMI and ARI ranges from [0, 1] and 1 means a perfect\nmatch. Figure 7 shows the results on CIFAR-100 with in-\ncremental step size 5, 10, 20, and 50 using our baseline\nmethod. We observe consistent performance compared with\nusing cluster accuracy (ACC) as metric as shown in Figure 2\nin paper. Note that both results are reported starting from the\nsecond task (ﬁrst incremental step).\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-04-14",
  "updated": "2021-07-30"
}