{
  "id": "http://arxiv.org/abs/2104.07642v2",
  "title": "Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining",
  "authors": [
    "Chih-chan Tien",
    "Shane Steinert-Threlkeld"
  ],
  "abstract": "This work presents methods for learning cross-lingual sentence\nrepresentations using paired or unpaired bilingual texts. We hypothesize that\nthe cross-lingual alignment strategy is transferable, and therefore a model\ntrained to align only two languages can encode multilingually more aligned\nrepresentations. We thus introduce dual-pivot transfer: training on one\nlanguage pair and evaluating on other pairs. To study this theory, we design\nunsupervised models trained on unpaired sentences and single-pair supervised\nmodels trained on bitexts, both based on the unsupervised language model XLM-R\nwith its parameters frozen. The experiments evaluate the models as universal\nsentence encoders on the task of unsupervised bitext mining on two datasets,\nwhere the unsupervised model reaches the state of the art of unsupervised\nretrieval, and the alternative single-pair supervised model approaches the\nperformance of multilingually supervised models. The results suggest that\nbilingual training techniques as proposed can be applied to get sentence\nrepresentations with multilingual alignment.",
  "text": "Bilingual alignment transfers to multilingual alignment for unsupervised\nparallel text mining\nChih-chan Tien ∗\nUniversity of Chicago\ncctien@uchicago.edu\nShane Steinert-Threlkeld\nUniversity of Washington\nshanest@uw.edu\nAbstract\nThis work presents methods for learning cross-\nlingual sentence representations using paired\nor unpaired bilingual texts. We hypothesize\nthat the cross-lingual alignment strategy is\ntransferable, and therefore a model trained to\nalign only two languages can encode multilin-\ngually more aligned representations. We thus\nintroduce dual-pivot transfer: training on one\nlanguage pair and evaluating on other pairs.\nTo study this theory, we design unsupervised\nmodels trained on unpaired sentences and\nsingle-pair supervised models trained on bi-\ntexts, both based on the unsupervised language\nmodel XLM-R with its parameters frozen. The\nexperiments evaluate the models as univer-\nsal sentence encoders on the task of unsuper-\nvised bitext mining on two datasets, where the\nunsupervised model reaches the state of the\nart of unsupervised retrieval, and the alterna-\ntive single-pair supervised model approaches\nthe performance of multilingually supervised\nmodels.\nThe results suggest that bilingual\ntraining techniques as proposed can be applied\nto get sentence representations with multilin-\ngual alignment.\n1\nIntroduction\nCross-lingual alignment as evaluated by retrieval\ntasks has been shown to be present in the repre-\nsentations of recent massive multilingual models\nwhich are not trained on bitexts (Pires et al., 2019;\nConneau et al., 2020b). Other studies further show\nthat sentence representations with higher cross-\nlingual comparability can be achieved by training a\ncross-lingual mapping (Aldarmaki and Diab, 2019)\nor ﬁne-tuning (Cao et al., 2020) for every pair of\nlanguages. These two lines of research show that,\non the one hand, multilingual alignment arises from\ntraining using monolingual corpora alone, and, on\nthe other, bilingual alignment can be enhanced by\ntraining on bitexts of speciﬁc language pairs.\n∗Work done while at the Department of Linguistics at the\nUniversity of Washington.\nCombining these insights yields a question: can\ntraining with bilingual corpora help improve multi-\nlingual alignment? Given a language model encod-\ning texts in different languages with some shared\nstructure already, we can expect that the model fur-\nther trained to align a pair of languages will take\nadvantage of the shared structure and will there-\nfore generalize the alignment strategy to other lan-\nguage pairs. From a practical point of view, bitexts\nfor some pairs of languages are more abundant\nthan others, and it is therefore efﬁcient to lever-\nage data from resource-rich pairs for the alignment\nof resource-poor pairs in training multilingual lan-\nguage models.\nTo better understand the cross-lingual structure\nfrom the unsupervised models, we also ask the fol-\nlowing question: how can multilingual alignment\ninformation be extracted from the unsupervised\nlanguage models? Unsupervised multilingual mod-\nels out-of-the-box as sentence encoders fall short\nof their supervised counterparts such as LASER\n(Artetxe and Schwenk, 2019b) in the task of bi-\ntext mining (Hu et al., 2020). The discovery of\ncross-lingual structure in the hidden states in the\nunsupervised model (Pires et al., 2019; Conneau\net al., 2020b), however, raises the possibility that\nwith relatively light post-training for better extrac-\ntion of deep features, the unsupervised models can\ngenerate much more multilingually aligned repre-\nsentations.\nIn this paper, we address both questions with\nthe design of dual-pivot transfer, where a model\nis trained for bilingual alignment but tested for\nmultilingual alignment. And we hypothesize that\ntraining to encourage similarity between sentence\nrepresentations from two languages, the dual pivots,\ncan help generate more aligned representations not\nonly for the pivot pair, but also for other pairs.\nIn particular, we design and study a simple ex-\ntraction module on top of the pretrained multi-\nlingual language model XLM-R (Conneau et al.,\narXiv:2104.07642v2  [cs.CL]  15 Mar 2022\n2020a). To harness different training signals, we\npropose two training architectures. In the case of\ntraining on unpaired sentences, the model is encour-\naged by adversarial training to encode sentences\nfrom the two languages with similar distributions.\nIn the other case where bitexts of the two pivot\nlanguages are used, the model is encouraged to en-\ncode encountered parallel sentences similarly. Both\nmodels are then transferred to language pairs other\nthan the dual pivots. This enables our model to\nbe used for unsupervised bitext mining, or bitext\nmining where the model is trained only on parallel\nsentences from a single language pair.\nThe experiments show that both training strate-\ngies are effective, where the unsupervised model\nreaches the state of the art on completely un-\nsupervised bitext mining, and the one-pair su-\npervised model approaching the state-of-the-art\nmultilingually-supervised language models in one\nbitext mining task.\nOur contributions are fourfold:\n• This study proposes effective methods of bilin-\ngual training using paired or unpaired sen-\ntences for sentence representation with multi-\nlingual alignment. The strategies can be incor-\nporated in language model training for greater\nefﬁciency in the future.\n• The work demonstrates that the alignment in-\nformation in unsupervised multilingual lan-\nguage models is extractable by simple bilin-\ngual training of a light extraction module\n(without ﬁne-tuning) with performance com-\nparable to fully supervised models and reach-\ning the state of the art of unsupervised models.\n• The models are tested using a new experi-\nmental design—dual-pivot transfer—to eval-\nuate the generalizability of a bilingually-\nsupervised sentence encoder to the task of\ntext mining for other language pairs on which\nit is not trained.\n• This study shows that unsupervised bitext min-\ning has strong performance which is compa-\nrable to bitext mining by a fully supervised\nmodel, so the proposed techniques can be ap-\nplied to augment bilingual corpora for data-\nscarce language pairs in the future.\n2\nRelated work\nAlignment with adversarial nets\nThis work fol-\nlows the line of previous studies which use adver-\nsarial networks (GANs) (Goodfellow et al., 2014)\nto align cross-domain distributions of embeddings\nwithout supervision of paired samples, in some\ncases in tandem with cycle consistency (Zhu et al.,\n2017), which encourages representations “trans-\nlated” to another language then “translated” back\nto be similar to the starting representations. Con-\nneau et al. (2018)’s MUSE project trains a linear\nmap from the word-embedding space of one lan-\nguage to that of another using GANs, the method of\nwhich is later applied to an unsupervised machine\ntranslation model (Lample et al., 2018a). Cycle\nconsistency in complement to adversarial training\nhas been shown to be effective in helping to learn\ncross-lingual lexicon induction (Zhang et al., 2017;\nXu et al., 2018; Mohiuddin and Joty, 2020). Our\nwork is the ﬁrst to our knowledge to apply such\nstrategy of adversarial training and cycle consis-\ntency to the task of bitext mining.\nAlignment with pretrained LMs\nWe adopt the\ntraining strategy aforementioned on top of pre-\ntrained multilingual language models, the ex-\ntractability of multilingual information from which\nhas been studied in several ways. Pires et al. (2019)\nﬁnd multilingual alignment in the multilingual\nBERT (mBERT) model (Devlin et al., 2019) pre-\ntrained on monolingual corpora only, while Con-\nneau et al. (2020b) identify shared multilingual\nstructure in monolingual BERT models. Other\nwork studies the pretrained models dynamically\nby either ﬁne-tuning the pretrained model for cross-\nlingual alignment (Cao et al., 2020) or learning\ncross-lingual transformation (Aldarmaki and Diab,\n2019) with supervision from aligned texts. Re-\ncently, Yang et al. (2020) use multitask training\nto train multilingual encoders focusing on the per-\nformance on retrieval, and Reimers and Gurevych\n(2020) use bitexts to tune multilingual language\nmodels and to distill knowledge from a teacher\nmodel which has been tuned on paraphrase pairs.\nAlso, Chi et al. (2021a) pretrain an alternative\nXLM-R on a cross-lingual contrastive objective.\nOur work falls in the line of exploring multilingual-\nity of pretrained models with a distinct emphasis on\ninvestigating the multilingual structure induced by\nbilingual training without ﬁne-tuning or alternative\npretraining.\nXLM-R\nxi\ns\nLinear \ncombination\nLinear map\nyi\ns\nXLM-R\nxj\nt\nLinear \ncombination\nLinear map\nyj\nt\nF\nG\nCycle consistency loss Lcycle\nDiscriminator\nAdversarial loss Ladv\nSource sentence\nTarget sentence\nFigure 1:\nSchematic depiction of the unsupervised\nmodel with adversarial and cycle consistency losses.\nUnsupervised parallel sentence mining\nThe\nevaluation task of our work is bitext mining with-\nout supervision from any bitexts or from bitexts of\nthe pair of languages of the mining task. Such ex-\nperiments have been explored previously. Hangya\net al. (2018) show that unsupervised bilingual word\nembeddings are effective on bitext mining, and\nHangya and Fraser (2019) further improve the\nsystem with a word-alignment algorithm. Kiros\n(2020) trains a lensing module over mBERT for\nthe task of natural language inference (NLI) and\ntransfers the model to bitext mining. Keung et al.\n(2020)’s system uses bootstrapped bitexts to ﬁne-\ntune mBERT, while Kvapilíková et al. (2020)’s\nsystem uses synthetic bitexts from an unsupervised\nmachine translation system to ﬁne-tune XLM (Con-\nneau and Lample, 2019). Results from the three\naforementioned studies are included in Section 5\nfor comparisons. Methodologically, our approach\ndiffers from the above in that our system is based on\nanother pretrained model XLM-R (Conneau et al.,\n2020a) without ﬁne-tuning, for one of the goals of\nthe study is to understand the extractability of the\nalignment information from the pretrained model;\nand our model receives training signals from exist-\ning monolingual corpora or bitexts, instead of from\nNLI, bootstrapped, or synthesized data.\n3\nModel\n3.1\nA linear combination and a linear map\nThe model as an encoder generates ﬁxed-length\nvectors as sentence representations from the the hid-\nden states of the pretrained multilingual language\nmodel XLM-R (Conneau et al., 2020a). Formally,\ngiven a sentence πi,γ in language γ ∈{s, t}, with\nthe pretrained language model producing features\nxγ\ni of l layers, sequence length q, and embedding\nsize d, the extraction module f(·) generates a sen-\ntence embedding yγ\ni of ﬁxed size d based on the\nfeatures xγ\ni , or\nf(xγ\ni ) = yγ\ni ,\nxγ\ni ∈Rl×q×d and yγ\ni ∈Rd .\nWith the parameters of XLM-R frozen, within\nthe extraction module f(·) are only two trainable\ncomponents. The ﬁrst is an ELMo-style trainable\nsoftmax-normalized weighted linear combination\nmodule (Peters et al., 2018), and the second being\na trainable linear map. The linear combination\nmodule learns to weight the hidden states of every\nlayer l of the pretrained language model and output\na weighted average, on which a sum-pooling layer\nis then applied to q embeddings. And then the\nlinear map takes this bag-of-word representation\nand produces the ﬁnal sentence representation yγ\ni\nof the model.\n3.2\nAdversarial learning with unpaired texts\nMonolingual corpora in different languages with\nlanguage labels provide the signal for alignment\nif the semantic contents of the utterances share\nsimilar distributions across corpora. In order to\nexploit this information, we introduce to the model\nadversarial networks (Goodfellow et al., 2014) with\ncycle consistency (Zhu et al., 2017), to promote\nsimilarity in the distribution of representations.\nAs is usual in GANs, there is a discriminator\nmodule d(·), which in this model consumes the\nrepresentation yγ\ni and outputs continuous scores\nfor the language identity γ of the sentence πi,γ.\nFollowing Romanov et al. (2019), as inspired by\nWasserstein-GAN (Arjovsky et al., 2017), the loss\nof the discriminator Ldisc is the difference between\nthe unnormalized scores instead of the usual cross-\nentropy loss, or\nLdisc = d(ys\ni ) −d(yt\nj) .\nAnd the adversarial loss Ladv = −Ldisc updates\nthe parameters of the extraction module f(·) to\nencourage it to generate encodings abstract from\nlanguage-speciﬁc information.\nAdversarial training helps learning aligned en-\ncodings across languages at the distributional level.\nAt the individual level, however, the model is not\nconstrained to generate encodings which are both\naligned and discriminative (Zhu et al., 2017). In\nparticular, a degenerate encoder can produce pure\nnoise which is distributively identical across lan-\nguages. A cycle consistency module inspired by\nZhu et al. (2017) is therefore used to constrain the\nmodel to encode with individual-discerning align-\nment. Cycle consistency is also reminiscent of the\ntechnique of using back-translation for unsuper-\nvised translation systems (Lample et al., 2018b).\nIn this model, a trainable linear map F(·) maps\nelements from the encoding space of one language\nto the space of the other, and another linear map\nG(·) operates in the reverse direction. The cycle\nloss so deﬁned is used to update parameters for\nboth of the cycle mappings and the encoder:\nLcycle = h(ys\ni , G(F(ys\ni ))) + h(F(G(yt\nj)), yt\nj)\nwhere h is the triplet ranking loss function which\nsums the hinge costs in both directions:\nh(a, b) =\nX\nn\nmax(0, α −sim(a, b) + sim(an, b))\n+ max(0, α −sim(a, b) + sim(a, bn)) ,\nwhere the margin α and the number of negative\nsamples n are hyperparameters, and sim(·) is co-\nsine similarity. The loss function h encourages the\nmodel to encode similar representations between\npositive pairs (a, b) and dissimilar representations\nbetween negative pairs (an, b) and (a, bn), where\nan and bn are sampled from the embeddings in the\nmini-batch. Based on the ﬁndings that the hard neg-\natives, or non-translation pairs of high similarity\nbetween them, are more effective than the sum of\nnegatives in the ranking loss (Faghri et al., 2018),\nour system always includes in the summands the\ncosts from the hardest negatives in the mini-batch\nalong with the costs from any other randomly sam-\npled ones.\nThe full loss of the unsupervised model is\nLunsup = Ladv + λLcycle ,\nwith a hyperparameter λ. This unsupervised model\nis presented schematically with Figure 1.\nHyperparameter\nvalues\noutput dimension\nd\n{1024}\n# negative samples\nn\n{1, 2, 4}\nmargin value\nα\n{0, 0.2, 0.4}\nweight for cycle loss\nλ\n{1, 5, 10}\ndiscriminator step times\nκ\n{1, 2}\nTable 1: Hyperparameters and experimented values.\n3.3\nLearning alignment with bitexts\nIn addition to the completely unsupervised model,\nwe also experiment with a model which is super-\nvised with bitext from one pair of languages and\nthen transferred to other pairs. In this set-up, in-\nstead of using cyclical mappings, bitexts provide\nthe alignment signal through the ranking loss di-\nrectly, so the loss for the supervised model is\nLsup = h(ys\ni , yt\ni) ,\nwhere ys\ni and yt\ni are representations of parallel sen-\ntences.\n4\nTraining\nThe model is trained with the Adam optimizer\n(Kingma and Ba, 2014) and learning rate 0.001\nwith the parameters in XLM-R frozen. Our train-\ning program is built upon AllenNLP (Gardner et al.,\n2018), HuggingFace Transformers (Wolf et al.,\n2020), and PyTorch (Paszke et al., 2019). The\ncode for this study is released publicly.1\nFor adversarial training, the discriminator is up-\ndated κ times for every step of backpropagation\nto the encoder. Other hyperparameters include the\ndimension of the output representations d, number\nof negative samples n, margin value α, and weight\nof the cycle loss λ. The hyperparameters and the\nthe values which are experimented with are sum-\nmarized in Table 1. We empirically determine the\nhyperparameters among experimented values, and\nreport their values in speciﬁc evaluation sections.\nThe bilingual corpora used to train the encoder is\ntaken from OPUS (Tiedemann, 2012) as produced\nfor the training of XLM (Conneau and Lample,\n2019).2 We experimented with two language pairs\nfor training the model—Arabic-English (ar-en) and\n1The repository at https://github.com/cctien/\nbimultialign.\n2We\nuse\nthe\nscript\nhttps://github.\ncom/facebookresearch/XLM/blob/main/\nget-data-para.sh\nto\nget\nthe\ncorpora\nMultiUN\n(Eisele and Chen, 2010) and EUbookshop, where each\ntraining corpus we use is of 9 million sentences.\nModel\nF1 score (%)\nxx↔en\nAverage de\nfr\nru\nzh\nUnsupervised\nModel (ar-en unsup.)\n81.8\n84.1 77.9 87.9 77.1\nModel (de-en unsup.)\n82.4\n91.4 75.6 86.1 76.5\nXLM-R L16-boe\n68.7\n75.4 65.0 75.6 59.0\nKvapilíková et al. (2020)\n75.8\n80.1 78.8 77.2 67.0\nKeung et al. (2020)\n69.5\n74.9 73.0 69.9 60.1\nKiros (2020)\n51.7\n59.0 59.5 47.1 41.1\nOne-pair supervised\nModel (ar-en bitexts sup.) 89.1\n91.7 89.2 90.1 85.6\nModel (de-en bitexts sup.) 89.6\n92.5 89.6 90.3 85.8\nFully Supervised\nLASER\n92.8\n95.4 92.4 92.3 91.2\nLaBSE\n93.5\n95.9 92.5 92.4 93.0\nXLM-R+SBERT\n88.6\n90.8 87.1 88.6 87.8\nTable 2:\nF1 scores on the BUCC bitext mining text.\nSimple average of scores from 4 tasks reported in the\nsecond column.\nHighest scores in their groups are\nbolded.\nGerman-English (de-en)—to explore potential ef-\nfects of the choice of the dual-pivots. After being\ntrained, the encoder is evaluated on two tasks of bi-\ntext mining between texts in English and in another\nlanguage. Additionally, we train the models with\nthe pivot pair of Arabic-German (ar-de), which\ndoes not include English, to be evaluated on the\nsecond task.\n5\nEvaluations\nFour models, two unsupervised and two one-pair\nsupervised trained on either of the two language\npairs, are evaluated on two bitext mining or re-\ntrieval tasks of the BUCC corpus (Zweigenbaum\net al., 2018) and of the Tatoeba corpus (Artetxe and\nSchwenk, 2019a).\n5.1\nBaselines and comparisons\nUnsupervised baselines\nThe XLM-R (Conneau\net al., 2020a) bag-of-embedding (boe) represen-\ntations out-of-the-box serve as the unsupervised\nbaseline. We identify the best-performing among\nthe layers of orders of multiples of 4, or layer\nL ∈{0, 4, 8, 12, 16, 20, 24}, as the baseline. In\nthe case of BUCC mining task, for example, the\nbest-performing baseline model is of layer 16 and\ndenoted by XLM-R L16-boe.\nResults from Kiros (2020), Keung et al. (2020),\nand Kvapilíková et al. (2020), as state-of-the-art\nmodels for unsupervised bitext mining from pre-\ntrained language models, are included for compari-\nson (see Section 2 for a description of them).\nFully supervised models\nLASER (Artetxe and\nSchwenk, 2019b) and LaBSE (Feng et al., 2020),\nboth fully supervised with multilingual bitexts, are\nincluded for comparisons. LASER is an LSTM-\nbased encoder and translation model trained on par-\nallel corpora of 93 languages, and is the earlier lead-\ning system on the two mining tasks. LaBSE on the\nother hand is a transformer-based multilingual sen-\ntence encoder supervised with parallel sentences\nfrom 109 languages using the additive margin soft-\nmax (Wang et al., 2018) for the translation language\nmodeling objective, and has state-of-the-art perfor-\nmance on the two mining tasks. Finally, XLM-\nR+SBERT from Reimers and Gurevych, 2020 is\nXLM-R ﬁne-tuned to align representations of bi-\ntexts of 50 language pairs and to distill knowledge\nfrom SBERT (Reimers and Gurevych, 2019) ﬁne-\ntuned on English paraphrase pairs.\n5.2\nBUCC\nThe BUCC corpora (Zweigenbaum et al., 2018),\nconsist of 95k to 460k sentences in each of 4\nlanguages—German, French, Russian, and Man-\ndarin Chinese—with around 3% of such sentences\nbeing English-aligned. The task is to mine for the\ntranslation pairs.\nMargin-based retrieval\nThe retrieval is based\non the margin-based similarity scores (Artetxe and\nSchwenk, 2019a) related to CSLS (Conneau et al.,\n2018),\nscore(ys, yt) = margin(sim(ys, yt), scale(ys, yt))\nscale(ys, yt) =\nX\nz∈NNk(ys)\nsim(ys, z)\n2k\n+\nX\nz∈NNk(yt)\nsim(yt, z)\n2k\n,\nwhere NNk(y) denotes the k nearest neighbors of\ny in the other language. Here we use k = 4 and the\nratio margin function, or margin(a, b) = a/b, fol-\nlowing the literature (Artetxe and Schwenk, 2019b).\nBy scaling up the similarity associated with more\nisolated embeddings, margin-based retrieval helps\nalleviate the hubness problem (Radovanovic et al.,\n2010), where some embeddings or hubs are near-\nest neighbors of many other embeddings with high\nprobability.\nFollowing Hu et al. (2020), our model is eval-\nuated on the training split of the BUCC corpora,\nand the threshold of the similarity score cutting\noff translations from non-translations is optimized\nfor each language pair. While Kvapilíková et al.\n(2020) and Kiros (2020) optimize for the language-\nspeciﬁc mining thresholds as we do here, Keung\net al. (2020) use a prior probability to infer the\nthresholds. And different from all other baselines\nor models for comparisons presented here, Kva-\npilíková et al. (2020)’s model is evaluated upon the\nundisclosed test split of the BUCC corpus.\nResults\nF1 scores on the BUCC dataset pre-\nsented in Table 2 demonstrate that bilingual align-\nment learned by the model is transferable to other\npairs of languages. The hyperparameter values of\nthe unsupervised model presented in the table are\nn = 1, α = 0, λ = 5, κ = 2, and those of the\nsupervised model are n = 1, α = 0.\nThe adversarially-trained unsupervised model\noutperforms the unsupervised baselines and near-\ning the state of the art, and is thus effective\nin extracting sentence representations which are\nsharable across languages. The choice of pivot\npairs shows effects on the unsupervised models,\nwith the model trained on the de-en texts perform-\ning better than that on the ar-en texts at mining for\nparallel sentences between English and German\nby 7 points. The results suggest that while align-\nment is transferable, the unsupervised model can\nbe further improved for multilingual alignment by\nbeing trained on multilingual texts of more than\ntwo pivots.\nThe one-pair supervised model trained with bi-\ntexts of one pair of languages, on the other hand,\nperforms within a 6-point range of the fully super-\nvised systems, which shows that much alignment\ninformation from unsupervised pretrained models\nis recoverable by the simple extraction module.\nNoticeably, the model supervised with ar-en bi-\ntexts but not from the four pairs of the task sees\na 20-point increase from the plain XLM-R, and\nthe choice of dual pivots does not have signiﬁcant\neffects on the supervised model.\n5.3\nTatoeba\nWe also measure the parallel sentence matching\naccuracy over the Tatoeba dataset (Artetxe and\nSchwenk, 2019b). This dataset consists of 100\nto 1,000 English-aligned sentence pairs for 112 lan-\nguages, and the task is to retrieve the translation\nin one of the target languages given a sentence in\nEnglish using absolute similarity scores without\nmargin-scaling.\nModel\nAverage accuracy (%)\n36\n41\n112\nUnsupervised\nModel (ar-en unsup.)\n73.4\n70.8\n55.3\nModel (ar-de unsup.)\n71.5\n68.9\n53.3\nModel (de-en unsup.)\n74.2 72.0 56.0\nXLM-R L12-boe\n54.3\n53.1\n39.6\nKvapilíková et al. (2020)\n−−\n50.2\n−−\nOne-pair supervised\nModel (ar-en bitexts sup.)\n79.6\n78.3\n61.1\nModel (ar-de bitexts sup.)\n74.1\n71.8\n56.0\nModel (de-en bitexts sup.) 80.4 78.9 62.4\nSupervised with 50+ pairs\nLASER\n85.4\n79.1\n66.9\nLaBSE\n95.0 −−\n83.7\nXLM-R+SBERT\n86.2\n84.3\n67.1\nTable 3:\nAverage accuracy scores on the Tatoeba\ndataset in three average groups. Highest scores in their\ngroups are bolded.\nResults\nMatching accuracy for the retrieval task\nof the Tatoeba dataset are presented in Table 3.\nFollowing Feng et al., 2020, average scores from\ndifferent groups are presented to compare differ-\nent models. The 36 languages are those selected\nby Xtreme (Hu et al., 2020), and the 41 languages\nare those for which results are presented in Kva-\npilíková et al., 2020. The hyperparameter values of\nthe unsupervised model presented in the table are\nn = 2, α = 0.2, λ = 10, κ = 2, and those of the\nsupervised model are n = 1, α = 0.\nThe unsupervised model outperforms the base-\nlines by roughly 20 points, and the one-pair super-\nvised model performs close to the the supervised\nmodel LASER but falls shorts by around 10 to\n20 points to the other supervised model LaBSE.\nWhen one of the two pivot languages is English,\nthe choice of the pivot does not show much differ-\nence on this task on average. While the models\ntrained on ar-de (where neither pivot language is\nEnglish) still exhibits strong transfer performance,\nthere is a drop of around 2 to 6 points from the\nmodels where English is one of the pivot languages\n(ar-en and de-en).\n6\nAnalysis\nTo understand the factors affecting the performance\nof the model, we consider several variants. All\nmodels presented in this section are trained with the\nsame hyperparameters presented in the evaluation\nsection above using either de-en corpora or corpora\nof multiple language pairs (Section 6.3).\nModel\nTatoeba 36\nUnsupervised\nXLM-R average-boe\n54.9\nXLM-R L12-boe\n54.3\nL = Ladv\nlinear combination\n34.7\nlinear map\n0.1\nlinear combination + linear map\n2.1\nL = Lcycle\nlinear combination\n55.4\nlinear map\n69.8\nlinear combination + linear map\n68.0\nL = Ladv + λLcycle\nlinear combination\n47.4\nlinear map\n70.5\nlinear combination + linear map\n74.2\nOne-pair supervised\nL = Lsup\nlinear combination\n67.5\nlinear map\n80.1\nlinear combination + linear map\n80.4\nTable 4:\nAblation results of the accuracy (%) on the\nTatoeba averaged across 36 languages.\n6.1\nAblation\nWe\nablate\nfrom\nthe\nmodel\nthe\ntrainable\ncomponents—the\nweighted\nlinear\ncombina-\ntion and the linear map—as well as the two\ntraining losses, Ladv and Lcycle, of the unsu-\npervised model.\nWhen the weighted linear\ncombination is ablated, we use the unweighted\naverage of embeddings across layers.\nWe evaluate on the average accuracy over the 36\nlanguages of the Tatoeba corpus. The results in Ta-\nble 4 show a few interesting trends. First, the cycle\nconsistency loss is essential for the unsupervised\nmodel, as can be seen by the very low performance\nwhen only Ladv is used. Secondly, the linear map\nplays a larger role than the linear combination in ex-\ntracting alignment information in the unsupervised\nmodel: in both conditions with cycle consistency\nloss, the linear map alone outperforms the linear\ncombination alone, and in the condition with only\ncycle consistency loss, the linear map alone does\nbest. Finally, in the one-pair supervised model, the\nlinear combination module alone shows gains of\n13 points from the baseline but does not produce\ngains when trained along with a linear map.\n0\n4\n8\n12\n16\n20\n24\nLayer\n20\n40\n60\n80\nAverage accuracy (%)\nModel\none-pair sup.\nunsup.\nbaseline/boe\nFigure 2:\nAverage accuracies of 36 languages of the\nTatoeba task where the model is trained with represen-\ntations from XLM-R layers of orders which are multi-\nples of 4.\nModel\nModel with # of pairs\n1\n2\n4\n8\n16\nUnsupervised\n74.2\n74.9\n75.2\n77.5\n77.1\nSupervised\n80.4\n80.0\n80.0\n83.3\n85.7\nTable 5: Models with multiple pairs with accuracy (%)\non Tatoeba averaged across 36 languages.\n6.2\nSingle-layer representations\nPrevious studies show that representations from\ndifferent layers differ in their cross-lingual align-\nment (Pires et al., 2019; Conneau et al., 2020b).\nTo understand this phenomenon in the present set-\nting, we take layers whose orders are multiples of 4\nand train the model with representations from one\nsingle layer without combining embeddings from\ndifferent layers.\nThe average accuracy scores over 36 languages\nin Tatoeba summarized in Figure 2 show that the\nalignment information is most extractable deep\nin the middle layers, corroborating the ﬁndings\nfrom the previous work (Kvapilíková et al., 2020;\nLitschko et al., 2021; Chi et al., 2021b). The model\ntrained with the best-performing layer shows simi-\nlar or higher scores than the full model with learned\nlinear combination, which is consistent with the\nﬁndings from the ablation that the learned linear\ncombination is not essential for extracting align-\nment information.\n6.3\nTraining with multiple language pairs\nIt is possible that a model trained on texts from\nmore pairs of languages may improve upon the\nmultilingual alignment so far demonstrated. To test\nthis, we trained the model with the same hyper-\nModel\nF1 score (%)\nxx↔en\nde\nfr\nru\nzh\nUnsupervised\nOptimized thresholds\n91.4\n75.6\n86.1\n76.5\nDual-pivot threshold\n91.4\n73.5\n84.9\n75.8\nOne-pair supervised\nOptimized thresholds\n92.5\n89.6\n90.3\n85.8\nDual-pivot threshold\n92.5\n89.6\n90.2\n85.2\nTable 6:\nF1 scores on the BUCC training split with\nthe model trained with de-en texts. The system with\noptimized thresholds tunes the threshold for each pair,\nas in Table 2; the dual-pivot system uses the threshold\nfrom the de-en pair for all four pairs.\nparameters as in the previous section but on texts\nfrom multiple languages,3 where each multi-pair\nmodel is trained on 16 million total sentences. The\nresults are in Table 5. There is an aggregate 3 point\nimprovement for the unsupervised model and 5\npoint for the supervised model. The results sug-\ngest that with our models, one bilingual pivot is\ncapable of extracting much transferable multilin-\ngual representations from XLM-R, but using more\npivots can still improve the transferability of the\nrepresentations to some extent.\n6.4\nThreshold transfer\nPrevious work observes that in the BUCC min-\ning task, the thresholds optimized for different lan-\nguage pairs are close to one another, suggesting\nthat one can tune the threshold on high-resource\npairs and use the system to mine other language\npairs (Kiros, 2020). We examine the mining per-\nformance on the BUCC dataset of two threshold\nschemes: optimized thresholds, where thresholds\nare optimized for each language pair, and the dual-\npivot threshold, where the threshold optimized for\nthe pivot pair, in this case German-English, is used\nto mine all languages.\nThe scores from these two schemes on BUCC\nare summarized in Table 6. The results show that\nthe thresholds optimized for the pivot pair trans-\nfer to other pairs with at most a 2-point decrease\nin the F1 score, and that the results of the two\nschemes are almost identical to the one-pair super-\nvised model. These experiments corroborate the\n3The 16 pairs are those between en and ar, bg, de,\nel, es, fa, fr, hi, id, ru, sw, th, tr,\nur, vi, zh.\nModel\nar-ar\nen-en\nes-es\nen-ar\nen-es\nen-tr\nUnsup.\nXLM-R\n47.2\n58.5\n53.0\n31.2\n17.3\n28.1\nModel\n51.7\n74.5\n63.8\n42.0\n41.9\n37.9\nOne-pair sup.\nModel\n54.7\n67.5\n65.0\n43.7\n39.8\n44.0\nOther systems\nLASER\n68.9\n77.6\n79.7\n66.5\n57.9\n72.0\nLaBSE\n69.1\n79.4\n80.8\n74.5\n65.5\n72.0\nSBERT\n79.6\n88.8\n86.3\n82.3\n83.1\n80.9\nTable 7:\nSpearman correlation (%) on multilingual\nSTS 2017 (Cer et al., 2017) of the unsupervised and\nsupervised models trained on the de-en pair.\nprevious observation and demonstrate yet another\ncase for leveraging texts from resource-rich pairs\nfor unsupervised mining of other language pairs.\n6.5\nMultilingual semantic textual similarity\nTo test whether our method of training language-\nagnostic sentence encoders encourages meaning-\nbased representations, we evaluate the models on\nthe multilingual semantic textual similarity (STS)\n2017 (Cer et al., 2017) with Spearman correlation\nreported in Table 7. All evaluation pairs on average\nsee about 10 percentage-point increases from base-\nline (XLM-R L12-boe) for both models. Yet the\ngaps between our models and the fully supervised\nsystems suggest that supervision with more lan-\nguage pairs and more trainable parameters likely\nencourages sentence representations to be closer to\nwhat humans see as meaning.\n7\nConclusion\nThis work shows that training for bilingual align-\nment beneﬁts multilingual alignment for unsuper-\nvised bitext mining.\nThe unsupervised model\nshows the effectiveness of adversarial training with\ncycle consistency for building multilingual lan-\nguage models, and reaches the state of the art of\nunsupervised bitext mining. Both unsupervised\nand one-pair supervised models show that signif-\nicant multilingual alignment in an unsupervised\nlanguage model can be recovered by a linear map-\nping, and that combining monolingual and bilin-\ngual training data can be a data-efﬁcient method\nfor promoting multilingual alignment. Future work\nmay combine both the supervised and the unsu-\npervised techniques to attain sentence embeddings\nwith stronger multilingual alignment through the\ntransferability of bilingual alignment demonstrated\nin this work, and such work will beneﬁt tasks in-\nvolving languages of low resources in bitexts.\nReferences\nHanan Aldarmaki and Mona Diab. 2019.\nContext-\naware cross-lingual mapping. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3906–3911, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMartin Arjovsky, Soumith Chintala, and Léon Bottou.\n2017. Wasserstein generative adversarial networks.\nIn Proceedings of Machine Learning Research, vol-\nume 70, pages 214–223, International Convention\nCentre, Sydney, Australia. PMLR.\nMikel Artetxe and Holger Schwenk. 2019a. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3197–3203, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019b.\nMas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond.\nTransac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In International Conference on Learning Rep-\nresentations.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021a. Infoxlm: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training.\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma,\nSaksham Singhal, Payal Bajaj, Xia Song, and\nFuru Wei. 2021b. Xlm-e: Cross-lingual language\nmodel pre-training via electra.\narXiv preprint\narXiv:2106.16138.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining.\nIn H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 7059–\n7069. Curran Associates, Inc.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b.\nEmerging\ncross-lingual structure in pretrained language mod-\nels.\nIn Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 6022–6034, Online. Association for Compu-\ntational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAndreas Eisele and Yu Chen. 2010.\nMultiUN: A\nmultilingual corpus from united nation documents.\nIn Proceedings of the Seventh International Con-\nference on Language Resources and Evaluation\n(LREC’10), Valletta, Malta. European Language Re-\nsources Association (ELRA).\nFartash Faghri, David J. Fleet, Jamie Ryan Kiros,\nand Sanja Fidler. 2018. Vse++: Improving visual-\nsemantic embeddings with hard negatives. In Pro-\nceedings of the British Machine Vision Conference\n(BMVC).\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020.\nLanguage-\nagnostic BERT sentence embedding.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1–\n6, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014.\nGenerative\nadversarial nets.\nIn Z. Ghahramani, M. Welling,\nC. Cortes, N. D. Lawrence, and K. Q. Weinberger,\neditors, Advances in Neural Information Processing\nSystems 27, pages 2672–2680. Curran Associates,\nInc.\nViktor\nHangya,\nFabienne\nBraune,\nYuliya\nKala-\nsouskaya, and Alexander Fraser. 2018.\nUnsuper-\nvised parallel sentence extraction from comparable\ncorpora. In International Workshop on Spoken Lan-\nguage Translation.\nViktor Hangya and Alexander Fraser. 2019. Unsuper-\nvised parallel sentence extraction with parallel seg-\nment detection helps machine translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1224–\n1234, Florence, Italy. Association for Computational\nLinguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalization.\nPhillip Keung, Julian Salazar, Yichao Lu, and Noah A.\nSmith. 2020. Unsupervised bitext mining and trans-\nlation via self-trained contextual embeddings.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nJamie Kiros. 2020. Contextual lensing of universal sen-\ntence representations.\nIvana Kvapilíková, Mikel Artetxe, Gorka Labaka,\nEneko Agirre, and Ondˇrej Bojar. 2020.\nUnsuper-\nvised multilingual sentence embeddings for parallel\ncorpus mining. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: Student Research Workshop, pages 255–\n262, Online. Association for Computational Linguis-\ntics.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations (ICLR).\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5039–5049, Brussels, Belgium. Association\nfor Computational Linguistics.\nRobert Litschko, Ivan Vuli´c, Simone Paolo Ponzetto,\nand Goran Glavaš. 2021.\nEvaluating multilin-\ngual text encoders for unsupervised cross-lingual re-\ntrieval. In Proceedings of ECIR.\nTasnim Mohiuddin and Shaﬁq Joty. 2020.\nUnsuper-\nvised word translation with adversarial autoencoder.\nComputational Linguistics, 46(2):257–288.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019.\nPy-\ntorch: An imperative style, high-performance deep\nlearning library.\nIn H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8026–8037. Curran Asso-\nciates, Inc.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations.\nIn Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT?\nIn Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nMilos Radovanovic, Alexandros Nanopoulos, and Mir-\njana Ivanovic. 2010. Hubs in space: Popular nearest\nneighbors in high-dimensional data. Journal of Ma-\nchine Learning Research, 11(86):2487–2531.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2020.\nMaking\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation.\nIn Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4512–4525,\nOnline. Association for Computational Linguistics.\nAlexey Romanov, Anna Rumshisky, Anna Rogers, and\nDavid Donahue. 2019. Adversarial decomposition\nof text representation. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 815–825, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS.\nIn Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nF. Wang, J. Cheng, W. Liu, and H. Liu. 2018. Additive\nmargin softmax for face veriﬁcation. IEEE Signal\nProcessing Letters, 25(7):926–930.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing.\nRuochen Xu, Yiming Yang, Naoki Otani, and Yuexin\nWu. 2018.\nUnsupervised cross-lingual transfer of\nword embedding spaces. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2465–2474, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYinfei Yang,\nDaniel Cer,\nAmin Ahmad,\nMandy\nGuo, Jax Law, Noah Constant, Gustavo Hernan-\ndez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung,\nBrian Strope, and Ray Kurzweil. 2020. Multilingual\nuniversal sentence encoder for semantic retrieval.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 87–94, Online. Association\nfor Computational Linguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017.\nAdversarial training for unsupervised\nbilingual lexicon induction. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1:\nLong Papers),\npages 1959–1970, Vancouver, Canada. Association\nfor Computational Linguistics.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.\nEfros. 2017. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In Pro-\nceedings of the IEEE International Conference on\nComputer Vision (ICCV).\nPierre Zweigenbaum, Serge Sharoff, and Reinhard\nRapp. 2018. A multilingual dataset for evaluating\nparallel sentence extraction from comparable cor-\npora. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-04-15",
  "updated": "2022-03-15"
}