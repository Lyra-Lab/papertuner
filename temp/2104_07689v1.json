{
  "id": "http://arxiv.org/abs/2104.07689v1",
  "title": "Dual Contrastive Learning for Unsupervised Image-to-Image Translation",
  "authors": [
    "Junlin Han",
    "Mehrdad Shoeiby",
    "Lars Petersson",
    "Mohammad Ali Armin"
  ],
  "abstract": "Unsupervised image-to-image translation tasks aim to find a mapping between a\nsource domain X and a target domain Y from unpaired training data. Contrastive\nlearning for Unpaired image-to-image Translation (CUT) yields state-of-the-art\nresults in modeling unsupervised image-to-image translation by maximizing\nmutual information between input and output patches using only one encoder for\nboth domains. In this paper, we propose a novel method based on contrastive\nlearning and a dual learning setting (exploiting two encoders) to infer an\nefficient mapping between unpaired data. Additionally, while CUT suffers from\nmode collapse, a variant of our method efficiently addresses this issue. We\nfurther demonstrate the advantage of our approach through extensive ablation\nstudies demonstrating superior performance comparing to recent approaches in\nmultiple challenging image translation tasks. Lastly, we demonstrate that the\ngap between unsupervised methods and supervised methods can be efficiently\nclosed.",
  "text": "Dual Contrastive Learning for Unsupervised Image-to-Image Translation\nJunlin Han1,2\nMehrdad Shoeiby1\nLars Petersson1\nMohammad Ali Armin1\n1DATA61-CSIRO, 2Australian National University\n{junlin.han, mehrdad.shoeiby, lars.petersson, ali.armin}@data61.csiro.au\nAbstract\nUnsupervised image-to-image translation tasks aim to\nﬁnd a mapping between a source domain X and a target\ndomain Y from unpaired training data. Contrastive learn-\ning for Unpaired image-to-image Translation (CUT) yields\nstate-of-the-art results in modeling unsupervised image-to-\nimage translation by maximizing mutual information be-\ntween input and output patches using only one encoder for\nboth domains. In this paper, we propose a novel method\nbased on contrastive learning and a dual learning setting\n(exploiting two encoders) to infer an efﬁcient mapping be-\ntween unpaired data. Additionally, while CUT suffers from\nmode collapse, a variant of our method efﬁciently addresses\nthis issue. We further demonstrate the advantage of our\napproach through extensive ablation studies demonstrating\nsuperior performance comparing to recent approaches in\nmultiple challenging image translation tasks.\nLastly, we\ndemonstrate that the gap between unsupervised methods\nand supervised methods can be efﬁciently closed.\n1. Introduction\nThe image-to-image translation task aims to convert im-\nages from one domain to another domain, e.g., horse to\nzebra, low-resolution images to high-resolution images,\nimage to label, photography to painting, and vice versa.\nImage-to-image translation has drawn considerable atten-\ntion due to its wide range of applications including style-\ntransfer [47, 20, 25, 35, 1], image in-painting [37], colouri-\nsation [45], super-resolution [22, 44], dehazing [28], under-\nwater image restoration [14], and denoising [2].\nIn unsupervised image-to-image translation without\npaired data, the main problem is that the adversarial\nloss [12] is signiﬁcantly under-constrained, that is, there\nexist multiple possible mappings between the two domains\nwhich make the training unstable and, hence, the transla-\ntion unsuccessful. To restrict the mapping, the contempo-\nrary approaches CycleGAN [47], DiscoGAN [23], and Du-\nalGAN [43] use a similar idea, the assumption of cycle-\nconsistency [47] which learns the reverse mapping from\nthe target domain back to the source domain and measures\nwhether the reconstruction image is identical to the input\nimage. The cycle-consistency [47] assumption ensures that\nthe translated images have similar texture information to the\ntarget domain, failing to perform geometry changes. Also,\nthe cycle-consistency [47] assumption forces the relation-\nship between the two domains to be a bijection [27]. This\nis usually not ideal. For example, in the horse to zebra im-\nage translation task, the reconstruction is constrained via a\nﬁdelity loss, compromising image diversity.\nTo address this constraint, recently contrastive learning\nbetween multiple views of the data has achieved state-of-\nthe-art performance [16, 5, 18, 34] in the ﬁeld of self-\nsupervised representation learning. This was followed by\nCUT [35] introducing contrastive learning for unpaired\nimage-to-image translation with a patch-based, multi-layer\nPatchNCE loss to maximize the mutual information be-\ntween corresponding patches of input and output images.\nWhile CUT [35] demonstrated the efﬁciency of con-\ntrastive learning, we believe certain design choices are lim-\niting its performance. For example, one embedding was\nused for two distinct domains which may not efﬁciently\ncapture the domain gap.\nTo further leverage contrastive\nlearning and avoid the drawbacks of cycle-consistency [47],\nwe propose our dual contrastive learning approach which is\nreferred to as DCLGAN.\nDCLGAN aims to maximize mutual information by\nlearning the correspondence between input and output im-\nage patches using separate embeddings.\nBy employing\ndifferent encoders and projection heads for different do-\nmains, we learn suitable embeddings to maximize agree-\nment. The dual learning setting [43] also helps to stabilize\ntraining. Besides, we revisit some design choices and ﬁnd\nthat removing RGB pixels representing small patches, in\nthe PatchNCE loss, can be beneﬁcial. We show that cycle-\nconsistency [47] is unnecessary and in fact counter-intuitive\nwhen there is no strict constraint on geometrical structure.\nLastly, a variant of DCLGAN, referred to as SimDCL, sig-\nniﬁcantly avoids mode collapse.\nThis paper presents a novel framework and its’ variant\nthat can break the limitations of CycleGAN [47] (limited\n1\narXiv:2104.07689v1  [cs.CV]  15 Apr 2021\nFigure 1. Overall architecture of DCLGAN: By dually learning two mappings G : X →Y and F : Y →X, we successfully enable\nunpaired image-to-image translation without cycle-consistency. We deﬁne the encode half of G and F to be Genc, Fenc. We use Genc\nand HX as embedding X and Fenc and HY as embedding Y . We depict here the GAN loss (green line), the patch-based multiplayer\nPatchNCE loss (purple line). We omit the identical loss here. For variant SimDCL, we add the similarity loss between real images and\nfake images belonging to the same domain (dashed orange line). PatchNCE loss helps the generated fake image red patch to be similar to\nits real input image yellow patch while dissimilar to other blue patches.\nperformance in geometry changes) and CUT [35] (suffering\nmode collapse and a few inferior results). Through exten-\nsive experiments, we demonstrate the quantitative and qual-\nitative superiority of our method compared to several state-\nof-the-art methods on various popular tasks. Additionally,\nwe show that our method successfully closes the gap be-\ntween unsupervised and supervised methods, as contrastive\nlearning has done in the ﬁeld of self-supervised learning.\nA comprehensive ablation study demonstrates the effective-\nness of DCLGAN. Our code is available at GitHub.\n2. Related Work\nImage-to-image Translation.\nGANs [12] have been ap-\nplied to a multitude of image applications, especially in\nimage-to-image translation.\nThe key to the success of\nGANs is the idea of adversarial loss [12], which forces\nthe generated image to be indistinguishable in principle\nfrom the real image. Generally, image-to-image transla-\ntion can be categorized into two groups: a paired setting\n(supervised) [42, 21, 36] and an unpaired setting (unsuper-\nvised) [47, 23, 43]. Paired setting means the training set is\npaired, every image from domain X has a corresponding\nimage from domain Y .\nSupervised methods.\nIn this line, Pix2Pix [21] ﬁrst\nachieved task-agnostic image translation supporting multi-\nple image-to-image translation tasks using only a general\nmethod. It has then been extended to Pix2PixHD [42] en-\nabling synthesizing high-resolution photo-realistic images.\nSPADE [36] introduces the spatially-adaptive normalization\nlayer to further improve the quality of generated images.\nThese supervised approaches require paired data for train-\ning, which imposes a limitation on their usage.\nUnsupervised methods. In unsupervised settings, the\ncurrent methods [47, 20, 35, 25, 7, 43, 1, 23, 10, 30, 26, 43,\n4, 46] are mainly developed based on two assumptions: a\nshared latent space [30] and a cycle-consistency assump-\ntion [47]. UNIT [30] proposes a shared latent space as-\nsumption which assumes a pair of corresponding images in\ndifferent domains can be mapped to the same latent repre-\nsentation in a shared-latent space. Recent works [20, 25, 7,\n26, 6] further enable multi-modal and multi-domain synthe-\nsis to bring diversity in the translated outputs. MUNIT [20]\ndisentangles domain-speciﬁc features by splitting the latent\nspace into style code and content code. DRIT [25, 26] em-\nbeds images onto two spaces including a domain-speciﬁc\nattribute space and a content space capturing shared infor-\nmation. StarGAN [6, 7] employs a uniﬁed model architec-\nture to translate images across multiple domains.\nBreak the cycle. CycleGAN [47] learns two mappings\nsimultaneously via translating an image to the target do-\nmain and back preserving the ﬁdelity of the input and the\nreconstructed image. This leads it to be too restrictive. Re-\ncently, a few methods [35, 33, 1, 10] have tried to break\n2\nthe cycle to alleviate the problem of cycle-consistency [47].\nCouncilGAN [33] uses more than two generators and dis-\ncriminators along with the council loss. DistanceGAN [1]\nand GCGAN [10] enable one-way translation. They employ\ndifferent constraints from different aspects. We take the ad-\nvantages of both CycleGAN [47] and CUT [35], employing\nthe idea of mutual information maximization to enable two-\nsided unsupervised image-to-image translation based on the\narchitectures of CycleGAN [47].\nContrastive learning.\nIn the ﬁeld of unsupervised\nrepresentation learning [5, 16, 18, 34], contrastive learn-\ning aims to learn an embedding where associated signals\nare pulled together while other samples in the dataset are\npushed away. Signals may vary depending on speciﬁc tasks.\nIn general, the objective is to discriminate its transformed\nversion against other samples. To get the transformed ver-\nsion, data augmentation shows the most successful results\n[5, 16]. However, natural transformations can draw com-\nparable results [18, 15] if ideal natural sources such as\nthose arising audio and optical ﬂow in videos are avail-\nable.\nFor image-to-image translations, patches are ideal\nnatural sources for instance discrimination since they are\neasy to track and use [18, 34, 35].\nCUT [35] ﬁrst ap-\nplies noise contrastive estimation to image-to-image trans-\nlation tasks by learning the correspondence between in-\nput image patches and the corresponding generated image\npatches, achieving a performance superior to those based on\ncycle-consistency [47]. We further rethink several design\nchoices of leveraging contrastive learning, making it more\nbeneﬁcial to employ contrastive learning for unsupervised\nimage-to-image translation. We extend one-sided mapping\nto two-sided, performing better in learning embeddings and\nthus achieving new state-of-the-art results. We additionally\naddress the mode collapse problem that previous methods\nbased on mutual information maximization can not handle.\n3. Method\nGiven two domains X ⊂RH×W ×C and Y ⊂RH×W ×3\nand a dataset of unpaired instances X containing some im-\nages x and Y containing some images y. We denote them\nX = {x ∈X} and Y = {y ∈Y}. We aim to learn two\nmappings G : X →Y and F : Y →X.\nDCLGAN has two generators G, F as well as two dis-\ncriminators DX, DY . G enables the mapping from domain\nX to domain Y and F enables the reverse mapping. DX\nand DY ensure that the translated images belong to the cor-\nrect image domain. The ﬁrst half of the generators are de-\nﬁned as encoder while the second half are decoders and pre-\nsented as Genc and Fenc followed by Gdec and Fdec respec-\ntively.\nFor each mapping, we extract features of images from\nfour layers of the encoder and send them to a two-layer\nMLP projection head (HX and HY ). Such a projection head\nlearns to project the extracted features from the encoder to a\nstack of features. Note that we use Genc and HX as the em-\nbedding for domain X and use Fenc and HY as the embed-\nding for domain Y . If two domains share common semantic\ninformation such as horse and zebra, using one encoder can\nprovide reasonable results. However, this may fail to cap-\nture the variability in two distinctive domains with a large\ngap. Additionally, we introduce four light networks to cap-\nture the common information within one domain and form\na similarity loss.\nFigure 1 shows the overall architecture of DCLGAN and\nSimDCL. DCLGAN combines three losses including ad-\nversarial loss [12], PatchNCE loss, and identity loss [47]\nwhereas SimDCL has one additional similarity loss to ad-\ndress mode collapse. The details of our objective are de-\nscribed below.\n3.1. Adversarial loss\nAn adversarial loss [12] is employed to encourage the\ngenerator to generate visually similar images to images\nfrom the target domain, for the mapping G : X →Y with\ndiscriminator DY , the GAN loss is calculated by:\nLGAN (G, DY , X, Y ) = Ey∼Y [log DY (y)]\n+ Ex∼X [log (1 −DY (G(x))] ,\n(1)\nwhere G tries to generate images G(x) that look similar\nto images from domain Y , while DY aims to distinguish\nbetween translated samples G(x) and real samples y. A\nsimilar adversarial loss for the mapping F : Y →X and its\ndiscriminator DX is introduced as:\nLGAN (F, DX, X, Y ) = Ex∼X [log DX(x)]\n+ Ey∼Y [log (1 −DX(G(y))] .\n(2)\n3.2. Patch-based multi-layer contrastive learning\nMutual information maximization.\nOur goal is to\nmaximize the mutual information between corresponding\npatches of the input and the output. For instance, for a patch\nshowing the eye of a generated dog (top-right of Figure 1),\nwe should be able to more strongly associate it with the eye\nof the input real cat other than the rest of the patches of the\ncat.\nFollowing the setting of CUT [35], we employ a noisy\ncontrastive estimation framework [34] to maximize the mu-\ntual information between inputs and outputs. The idea be-\nhind contrastive learning is to correlate two signals, i.e., the\n“query” and its’ “positive” example, in contrast to other ex-\namples in the dataset (referred to as “negatives”).\nWe map query, positive, and N negatives to K-\ndimensional vectors and denote them v, v+ ∈RK and\nv−∈RN×K respectively. Note that v−\nn ∈RK denotes the\n3\nn-th negative. We normalize vectors with L2-normalization\nthen set up an (N +1)-way classiﬁcation problem and com-\npute the probability that a “positive” is selected over “neg-\natives”. Mathematically, this can be expressed as a cross-\nentropy loss [13] which is computed by:\nℓ\n\u0000v, v+, v−\u0001\n= −log(\nexp (sim(v, v+)/τ)\nexp (sim(v, v+)/τ) + PN\nn=1 exp\n\u0000sim(v, v−\nn )/τ\n\u0001),\n(3)\nwhere sim(u, v) = u⊤v/∥u∥∥v∥denotes the cosine simi-\nlarity between u and v. τ denotes a temperature parameter\nto scale the distance between the query and other examples,\nwe use 0.07 as default.\nPatchNCE loss. We use Genc and HX to extract fea-\ntures from domain X and use Fenc and HY to extract fea-\ntures from domain Y . We do not share weights in order\nto learn better embeddings and capture variability in two\ndistinct domains. We select L layers from Genc(X) and\nsend it to HX, embedding one image to a stack of features\n{zl}L =\n\b\nHl\nX\n\u0000Gl\nenc(x)\n\u0001\t\nL, where Gl\nenc represents the\noutput of l-th selected layers.\nNow we consider the patches. After having a stack of\nfeatures, each feature actually represents one patch from the\nimage. We take advantage of that and denote the spatial lo-\ncations in each selected layer as s ∈{1, ..., Sl}, where Sl\nis the number of spatial locations in each layer. We select\na query each time, refer the corresponding feature (“posi-\ntive”) as zs\nl ∈RCl and all other features “negatives”) as\nzS\\s\nl\n∈R(Sl−1)×Cl, where Cl is the number of channels in\neach layer. For the generated fake image G(x) belonging to\ndomain Y , we exploit the advantages of dual learning and\nuse a different embedding of domain Y . Similarly, we get\nanother stack of features {ˆzl}L =\n\b\nHl\nY\n\u0000F l\nenc(G(x))\n\u0001\t\nL.\nWe aim to match the corresponding patches of input\nand output images. The patch-based, multi-layer PatchNCE\nloss [35] for mapping G : X →Y can be expressed as:\nLPatchNCEX(G, HX, HY , X) =\nEx∼X\nL\nX\nl=1\nSl\nX\ns=1\nℓ\n\u0010\nˆzs\nl , zs\nl , zS\\s\nl\n\u0011\n.\n(4)\nConsider the reverse mapping F : Y →X, we introduce a\nsimilar loss as well,\nLPatchNCEY(F, HX, HY , Y ) =\nEy∼Y\nL\nX\nl=1\nSl\nX\ns=1\nℓ\n\u0010\nˆzs\nl , zs\nl , zS\\s\nl\n\u0011\n,\n(5)\nwhere {zl}L\n=\n\b\nHl\nY\n\u0000F l\nenc(y)\n\u0001\t\nL and {ˆzl}L\n=\n\b\nHl\nX\n\u0000Gl\nenc(F(y))\n\u0001\t\nL are different from G : X →Y .\n3.3. Similarity loss\nIntuitively, images from the same domain should have\nsome similarities. Their semantics are different but they\nshare a common style.\nIn the dual learning setting, we\nhave one real and one fake image belonging to the same do-\nmain in each iteration. After getting four stacks of features,\nwe use four light networks (Hxr, Hxf, Hyr, Hyf) to project\nthem to 64-dim vectors, where x, y, r, f refers to images\nwithin domain X, images within domain Y, real, fake cor-\nrespondingly. These 64-dim vectors belonging to the same\ndomain can be measured by a similarity loss, such a loss\ncan be formalised as:\nLsim(G, F, HX, HY , Hxr, Hxf, Hyr, Hyf)\n= [∥Hxr(HX(Genc(x))) −Hxf(HX(Genc(F(y))))∥sum\n1\n]\n+ [∥Hyr(HY (Fenc(y))) −Hyf(HY (Fenc(G(x))))∥sum\n1\n] ,\n(6)\nwhere sum means we sum them up together. Implementing\na similarity loss on the deep features forces the generated\nimages to be realistic, as opposed to mode collapsed out-\nputs, by encouraging the deep features of the generated and\nreal images to be similar.\n3.4. Identity loss\nIn order to prevent generators from unnecessary changes,\nwe add an identity loss [47].\nUnlike CUT [35], We do\nnot employ PatchNCE loss as identity loss due to training\nspeed.\nLidentity(G, F) = Ex∼X [∥F(x) −x∥1]\n+ Ey∼Y [∥G(y) −y∥1] .\n(7)\nSuch an identity loss can encourage the mappings to pre-\nserve color composition between the input and output.\n3.5. General objective\nDCLGAN.\nThe generated image should be realistic and\npatches in the input and output images should share same\ncorrespondence. We employ identity loss [47] in the default\nsetting. The full objective is estimated by:\nL(G, F, DX, DY , HX, HY )\n= λGAN(LGAN(G, DY , X, Y ) + LGAN(F, DX, X, Y ))\n+ λNCELPatchNCEX(G, HX, HY , X)\n+ λNCELPatchNCEY(F, HX, HY , Y )\n+ λidtLidentity(G, F).\n(8)\nWe set λGAN = 1, λNCE = 2 and λidt = 1. DCLGAN\nachieves superior performance to existing methods.\nSimDCL. We introduce SimDCL since methods based\non mutual information maximization suffer from mode col-\nlapse in some speciﬁc tasks. We add similarity loss to the\n4\nfull objective of DCLGAN, and name it SimDCL, where\nsim is short for similarity and DCL stands for dual con-\ntrastive learning. The full objective of this variant is:\nL(G, F, DX, DY , HX, HY )\n= λGAN(LGAN(G, DY , X, Y ) + LGAN(F, DX, X, Y ))\n+ λNCELPatchNCEX(G, HX, HY , X)\n+ λNCELPatchNCEY(F, HX, HY , Y )\n+ λsimLsim(G, F, HX, HY , H1, H2, H3, H4)\n+ λidtLidentity(G, F).\n(9)\nWe set λGAN = 1, λNCE = 2, λSIM = 10 and λidt = 1. This\nvariant runs slower than DCLGAN, we recommend using it\nfor Photo →Label, semantic segmentation and similar tasks\nto avoid mode collapse. SimDCL achieves equal or slightly\nworse performance compared to DCLGAN.\n4. Experiments\nThe training details, datasets, and our evaluation proto-\ncol along with all baselines are described as follows.\n4.1. Training details\nWe mostly follow the setting of CUT [35] to train our\nproposed model. We use Hinge GAN loss [29] instead of\nLSGAN loss [32]. More speciﬁcally, we use the Adam op-\ntimiser [24] with β1 = 0.5 and β2 = 0.999. DCLGAN is\ntrained for 400 epochs with a learning rate of 0.0001 while\nSimDCL is trained for 200 epochs with a learning rate of\n0.0002 unless speciﬁed. The learning rate starts to decay\nlinearly after half of the total epochs. We use a ResNet-\nbased [17] generator with PatchGAN [21] as discriminator.\nWe use a batch size of 1 and instance normalization [40].\nAll training images are loaded in 286 × 286 then cropped\nto 256 × 256 patches. More details on the training and the\narchitecture are provided in the supplementary material.\n4.2. Datasets\nWe evaluated our proposed method and baselines on six\ndifferent datasets with nine tasks.\nHorse ↔Zebra was introduced in CycleGAN, it con-\ntains 1067 horse images, 1344 zebra images as the training\nset and 260 test images all collected from ImageNet [9].\nCat ↔Dog contains 5000 training images and 500\ntest images for each domain.\nIt was introduced in Star-\nGAN2 [7]. DCLGAN is trained for 200 epochs only for\nthis dataset.\nCityScapes [8] contains 2975 training and 500 valida-\ntion images for each domain. One domain is city scenes\nfrom German cities and the other is semantic segmentation\nlabels. We focus on Label →City. We also leverage labels\nto measure how well methods discover correspondences.\nVan Gogh →Photo contains 400 Van Gogh paintings\nand 6287 photographs from Flickr. It was collected in Cy-\ncleGAN [47]. DCLGAN is trained for 200 epochs only for\nthis task. We reuse the training set of Van Gogh paintings\nas the test set.\nLabel ↔Facade is similar to CityScapes, it contains\n400 paired training images and 106 paired test images from\nthe CMP Facade Database [39].\nOrange →Apple is also from ImageNet [9]. It contains\n1019 orange images and 995 apple images in the training\nset. For testing, we use 248 orange images.\n4.3. Evaluation\nMetrics We mainly use Fr´echet Inception Distance\n(FID) [19] to measure the quality of generated images.\nFID [19] shows high correspondence with human percep-\ntion, it is based on the Inception Score (IS) [38]. Lower\nFID [19] means lower Fr´echet distance between real and\ngenerated images. That is to say, lower FID [19] means gen-\nerated images are more realistic. For cityscapes, following\nPix2Pix [21], we use the pre-trained semantic segmentation\nnetwork FCN-8 [31] and compute three metrics. They are\nmean class Intersection over Union (IoU), pixel-wise accu-\nracy (pixAcc), and average class accuracy (classAcc).\nBaselines. We perform qualitative and quantitative com-\nparison between our proposed method and recent state-of-\nthe-art unsupervised methods including CUT [35], Fast-\nCUT [35], CycleGAN [47], MUNIT [20], DRIT [25], Dis-\ntanceGAN [1], SelfDistance [1] and GCGAN [10]. MU-\nNIT [20] and DRIT [25] are able to generate diverse results\nfor only one input image and the others only produce one\nresult. Among them, CUT, FastCUT [35], DistanceGAN,\nSelfDistance [1] and GcGAN [10] are one-sided methods.\nThe rest are two-sided methods.\n5. Results\nHere, we compare our algorithms (DCLGAN and\nSimDCL) with all baselines on different datasets.\nFur-\nther, we compare DCLGAN to supervised methods on the\nCityScapes dataset using the FCN [31] score, showing that\nthe performance of our method is on par with supervised\nmethods. Lastly, we show that SimDCL avoids mode col-\nlapse.\n5.1. Comparison of different methods\nTable 1 shows a comparison of the quantitative results\nof DCLGAN and SimDCL with several baselines on three\nchallenging tasks, including CityScapes, Cat →Dog, and\nHorse →Zebra. We only use the FID [19] score as our\nquantitative metric. It is evident that our algorithms perform\nstronger than all the baseline. Figure 2 presents the corre-\nsponding randomly selected qualitative results. DCLGAN\n5\nInput\nDCLGAN\nSimDCL\nCUT\nFastCUT\nCycleGAN\nMUNIT\nDRIT\nSelfDis\nDistance\nGCGAN\nFigure 2. Comparison to all baselines on the Horse→Zebra, Cat→Dog, and CityScapes tasks. DCLGAN and SimDCL show visual\nsatisfactory results. The last row is a failure case, our methods are unable to identify unusual pose and rare background. They fail to\ndistinguish foreground and background, adding zebra textures to the cloud.\nCityScapes\nCat→Dog\nHorse →Zebra\nOverall\nMethod\nFID↓\nFID↓\nFID↓\nsec/iter↓\nRanking\nCycleGAN [47]\n68.6\n85.9\n66.8\n0.40\n4\nMUNIT [20]\n91.4\n104.4\n133.8\n0.39\n9\nDRIT [23]\n155.3\n123.4\n140.0\n0.70\n10\nDistance [1]\n85.8\n155.3\n72.0\n0.15\n6\nSelfDistance [1]\n78.8\n144.4\n80.8\n0.16\n6\nGCGAN [10]\n105.2\n96.6\n86.7\n0.62\n8\nCUT [35]\n56.4\n76.2\n45.5\n0.24\n3\nFastCUT [35]\n68.8\n94.0\n73.4\n0.15\n5\nDCLGAN (ours)\n49.4\n60.7\n43.2\n0.41\n1\nSimDCL (ours)\n51.3\n65.5\n47.1\n0.47\n2\nTable 1. Comparison to all baselines on the Horse→Zebra,\nCat→Dog, and CityScapes tasks. DCLGAN denotes our model\nwithout Similarity loss and SimDCL denotes our model with Sim-\nilarity loss. We show FID [19] score for all tasks. The overall\nranking is based on the FID score among all tasks. DCLGAN\ngenerates better images with acceptable speed, runs a bit slower\nthan CycleGAN [47]. Our variant SimDCL also shows competi-\ntive results.\nperforms both geometry changes and texture changes with\nnegligible artifacts, this is especially successful in Cat →\nDog while other methods can not generate realistic images.\nIt is worth mentioning that models generating multiple out-\nputs perform the worst.\nWe select the top four methods from Table 1 and set a\nsecond comparison among them by testing them in 5 more\ntasks: Zebra →Horse, Van Gogh →Photo, Dog →Cat,\nLabel →Facade and Orange →Dog. We show quantita-\ntive results in Table 2 and randomly picked qualitative re-\nsults in Figure 3. The results suggest that DCLGAN keeps\nsuperior performance comparing to other methods among\nvarious tasks.\nMethods under the cycle-consistency as-\nsumption [47] usually fail to perform geometric changes\nwhile methods based on mutual information maximization\nsuccessfully enable both geometric changes and texture\nchanges. This is explicitly shown in Dog →Cat tasks.\n5.2. Comparison to supervised methods\nHere we compare our DCLGAN method with three\npopular supervised methods, Pix2Pix [21], photo-realistic\nimage synthesis system CRN [3] and discriminative re-\ngion proposal adversarial network DRPAN [41] on the\nCityScapes dataset. We follow the setting in Pix2Pix [21]\nand use a pre-trained semantic segmentation network FCN-\n8 [31] to compute the FCN score. Quantitative results are\nshown in Table 3. DCLGAN performs best in pixACC and\nsigniﬁcantly closes the gap between unsupervised methods\nand supervised methods. On average, our method performs\non par with supervised methods.\n6\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nFigure 3. Comparison between the best four methods on ﬁve more tasks including Zebra →Horse, Van Gogh →Photo, Dog →Cat, Label\n→Facade, and Orange →Apple. We randomly pick two samples for each task. Our DCLGAN performs both geometry changes and\ntexture changes. The last row show two typical failure cases, the ﬁrst one fails to translate the image since input is only a small part of\nzebra, the last one fails to keep the structure of humans, translating them to Yosemite.\nZebra→Horse\nVan Gogh →Photo\nDog →Cat\nLabel →Facade\nOrange →Apple\nModel\nMethod\nFID↓\nFID↓\nOverall Runtime↓\nFID↓\nFID↓\nFID↓\nParameters\nCycleGAN [47]\n154.3\n103.0\n106hr\n107.7\n127.5\n117.7\n28.286M\nCUT [35]\n170.5\n96.9\n125hr\n26.8\n119.7\n127.0\n14.406M\nDCLGAN (ours)\n139.5\n93.7\n108hr\n22.2\n119.2\n124.9\n28.812M\nSimDCL (ours)\n152.5\n93.5\n124hr\n22.8\n132.3\n134.4\n28.852M\nTable 2. Comparison between the best four methods on Zebra →Horse, Van Gogh →Photo, Dog →Cat, and Label →Facade tasks.\nDCLGAN still outperforms other methods in most tasks. The overall runtimes are provided for Dog →Cat task, in hours. Note CUT\nis trained for 400 epochs while the rest for 200 epochs only. The overall ranking circumstances among methods compared in here are\nidentical to the ﬁrst comparison (Table 1) except for a tie with CycleGAN [47] and CUT [35].\nCityScapes\nMethod\npixAcc↑\nclassAcc↑\nIoU↑\nDCLGAN(ours)\n0.74\n0.22\n0.17\nPix2Pix [21]\n0.66\n0.23\n0.17\nCRN [3]\n0.69\n0.21\n0.20\nDRPAN [41]\n0.73\n0.24\n0.19\nGround Truth\n0.80\n0.26\n0.21\nTable 3. Comparison between unsupervised DCLGAN and super-\nvised Pix2Pix [21], CRN [3], DRPAN [41] on CityScapes dataset.\nWe follow the setting of Pix2Pix [21] to compute the FCN [31]\nscore. DCLGAN outperforms supervised methods in pixAcc, sug-\ngesting the gap between unsupervised methods and supervised\nmethods is closing.\n5.3. Addressing mode collapse via similarity loss.\nOur ﬁnal comparison is a stress test on mode collapse.\nMode collapse in generation tasks means the outputs lack\ndiversity, and usually, the outputs are not realistic.\nWe\nﬁnd that methods based on mutual information maximiza-\ntion (CUT and DCLGAN) can not prevent mode collapse\nin Photo →Label and similar tasks. To address this issue,\nwe design SimDCL. We test the best four methods on the\nFacade →Label task and show the randomly picked vi-\nsual results in Figure 4. No matter what the input is, the\noutputs of both CUT and DCLGAN are almost identical\nwhile SimDCL generates reasonable outputs for different\ninputs. SimDCL is more robust to the mode collapse issue\ncompared with other methods based on mutual information\n7\nInput\nDCLGAN SimDCL\nCUT\nCycleGAN Label\nFigure 4. Comparison between the best four methods on Facade →\nLabel task. Methods based on mutual information maximization\nsuffer from mode collapse in this task. We address this by in-\ntroducing SimDCL to prevent mode collapse. SimDCL also cap-\nture more correspondence between facade and label than Cycle-\nGAN [47].\nmaximization.\n6. Ablation study\nDCLGAN shows superior performance compared to all\nbaselines. We explore what is making contrastive learn-\ning effective. We analyze DCLGAN by studying each of\nour contributions in isolation via conducting several experi-\nments, summarized in Table 4. We use three tasks including\nHorse →Zebra, Zebra →Horse, and CityScapes in our ab-\nlation study.\nWe show the results of: (I) Adding the ﬁrst RGB pix-\nels back. (II) Drawing external negatives. (III) Using the\nsame encoder and MLP for one mapping instead of two.\n(IV) Adding cycle-consistency loss. (V) Removing the dual\nsetting.\nHorse →Zebra\nZebra →Horse\nCityScapes\nAblation\nFID↓\nFID↓\nFID↓\nI\n49.7\n156.7\n50.3\nII\n41.7\n149.2\n49.6\nIII\n44.0\n153.4\n52.2\nIV\n44.6\n140.6\n55.4\nV\n47.0\n151.3\n91.5\nDCLGAN\n43.2\n139.5\n49.4\nTable 4. Quantitative results for ablations.\n(I) CUT [35] uses features from ﬁve layers in total in-\ncluding the ﬁrst RGB pixels in PatchNCE loss (l = 5 in\nEquations 4 and 5). Layers and spatial locations within the\nfeature stack represent patches of the input image. Deeper\nlayers correspond to bigger patches. However, RGB pixels\nrepresents the smallest possible patch size (1 × 1), provid-\ning misleading information. We ﬁnd that not including the\nRGB layer encourages convergence. In fact, if we adopt the\nstrategy in CUT [35] (l = 5), the results deteriorate in all\nthree tasks as demonstrated in Table 4.\n(II) Effect of drawing external negatives. CUT [35]\nstates that internal negatives (patches from an input image\nonly) are more effective than external negatives (patches\nfrom other images). CUT [35] adds negatives using a mo-\nmentum encoder [16]. We explore this in a different ap-\nproach, by taking the advantage of the dual setting. DCL-\nGAN produces four different stacks of features at each iter-\nation. Concatenating two stacks of features belonging to the\nsame domain provides more negatives (255 internal and 256\nexternal) for one query while the default DCLGAN uses\n255 internal negatives. We obverse better quantitative re-\nsults in Horse →Zebra and very close results in CityScapes\nfor this variant. Although the gap of FID score between the\ndefault DCLGAN and this variant is small, the visual qual-\nity is not as good as that of the default DCLGAN, that is,\nobjects in the generated image tend to be merged together.\n(III) Effect of using separate embeddings for each do-\nmain. While CUT [35] uses the same embedding for both\ndomains, we use two separate embeddings, one for each do-\nmain. Adopting the CUT [35] strategy in our network we\nﬁnd that the results will deteriorate, as demonstrated in Ta-\nble 4. One embedding fails to capture the variability in two\ndistinct domains, for instance, Photo →Label.\n(IV) Effect of Cycle-consistency loss.\nTo test if the\ncycle-consistency loss can improve the results, we add\ncycle-consistency [47] loss to our objective. We did not\nobserve any improvements (Table 4).\nAlthough cycle-\nconsistency and mutual information maximization share\nsome commonalities, DCLGAN is much less restrictive.\nDCLGAN focuses on both texture and geometry changes\nwhile CycleGAN [47] mostly focuses on texture only. We\ntested this variant in two tasks requiring geometry changes,\nCat →Dog and Dog →Cat, the FID scores are 71.1 and\n35.5 respectively, all worse than the original DCLGAN. We\nconclude that when strict limitations on geometry are not\ncrucial, cycle-consistency [47] loss is better to be avoided.\n(V) Dual settings stabilize the training. We remove\nthe dual setting to demonstrate its’ effect. We keep other\nsettings the same as DCLGAN. The results are worse than\nDCLGAN, which shows the dual setting can learn better\nembeddings for different domains and stabilize the training.\n7. Conclusion\nWe show that a dual setting can better leverage con-\ntrastive learning in unsupervised unpaired image-to-image\ntranslation. We also revise some signiﬁcant designs to ren-\nder contrastive learning more effective. In addition, a vari-\nant of DCLGAN, SimDCL mitigates mode collapse. Fi-\nnally, we show that our method can hugely close the gap\nbetween unsupervised and supervised methods in challeng-\ning datasets such as CityScape, just as contrastive learning\nin the ﬁeld of self-supervised representation learning.\n8\nReferences\n[1] Sagie Benaim and Lior Wolf. One-sided unsupervised do-\nmain mapping. In Advances in neural information process-\ning systems (NIPS), pages 752–762, 2017.\n[2] Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming\nYang. Image blind denoising with generative adversarial net-\nwork based noise modeling. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 3155–\n3164, 2018.\n[3] Qifeng Chen and Vladlen Koltun. Photographic image syn-\nthesis with cascaded reﬁnement networks. In IEEE interna-\ntional conference on computer vision (ICCV), pages 1511–\n1520, 2017.\n[4] Runfa Chen, Wenbing Huang, Binghui Huang, Fuchun Sun,\nand Bin Fang. Reusing discriminators for encoding: Towards\nunsupervised image-to-image translation. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 8168–8177, 2020.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learn-\ning of visual representations. In International Conference on\nMachine Learning (ICML), 2020.\n[6] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,\nSunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-\ntive adversarial networks for multi-domain image-to-image\ntranslation. In IEEE conference on computer vision and pat-\ntern recognition (CVPR), pages 8789–8797, 2018.\n[7] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\nStargan v2: Diverse image synthesis for multiple domains. In\nIEEE Conference on Computer vision and pattern recognitio\n(CVPR), pages 8188–8197, 2020.\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding.\nIn IEEE\nconference on computer vision and pattern recognition\n(CVPR), pages 3213–3223, 2016.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE conference on computer vision and pat-\ntern recognition (CVPR), pages 248–255. Ieee, 2009.\n[10] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, Kun Zhang, and Dacheng Tao.\nGeometry-\nconsistent generative adversarial networks for one-sided un-\nsupervised domain mapping. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2427–\n2436, 2019.\n[11] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiﬁ-\ncial intelligence and statistics, pages 249–256, 2010.\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems (NIPS), pages 2672–\n2680, 2014.\n[13] Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive\nestimation: A new estimation principle for unnormalized\nstatistical models. In Proceedings of the Thirteenth Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics,\npages 297–304, 2010.\n[14] Junlin Han, Mehrdad Shoeiby, Tim Malthus, Elizabeth\nBotha, Janet Anstee, Saeed Anwar, Ran Wei, Lars Pe-\ntersson, and Mohammad Ali Armin.\nSingle underwa-\nter image restoration by contrastive learning.\nIn IEEE\nInternational Geoscience and Remote Sensing Symposium\n(IGARSS), 2021.\n[15] Tengda Han, Weidi Xie, and Andrew Zisserman.\nSelf-\nsupervised co-training for video representation learning. In\nAdvances in neural information processing systems (NIPS),\n2020.\n[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual repre-\nsentation learning. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 9729–9738, 2020.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In IEEE Con-\nference on Computer vision and pattern recognitio (CVPR),\npages 770–778, 2016.\n[18] Olivier J H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali\nRazavi, Carl Doersch, SM Eslami, and Aaron van den Oord.\nData-efﬁcient image recognition with contrastive predictive\ncoding. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in neural information processing systems\n(NIPS), pages 6626–6637, 2017.\n[20] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.\nMultimodal unsupervised image-to-image translation. In Eu-\nropean Conference on Computer Vision (ECCV), pages 172–\n189, 2018.\n[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017.\n[22] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate\nimage super-resolution using very deep convolutional net-\nworks. In IEEE conference on computer vision and pattern\nrecognition (CVPR), pages 1646–1654, 2016.\n[23] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,\nand Jiwon Kim. Learning to discover cross-domain relations\nwith generative adversarial networks. In International Con-\nference on Machine Learning (ICML), 2017.\n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. International Conference on Learn-\ning Representations (ICLR), 2014.\n[25] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh\nSingh, and Ming-Hsuan Yang.\nDiverse image-to-image\ntranslation via disentangled representations.\nIn European\nconference on computer vision (ECCV), pages 35–51, 2018.\n[26] Hsin-Ying Lee, Hung-Yu Tseng, Qi Mao, Jia-Bin Huang,\nYu-Ding Lu, Maneesh Singh, and Ming-Hsuan Yang.\n9\nDrit++: Diverse image-to-image translation via disentangled\nrepresentations. International Journal of Computer Vision\n(IJCV), pages 1–16, 2020.\n[27] Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun\nChen, Ricardo Henao, and Lawrence Carin.\nAlice: To-\nwards understanding adversarial learning for joint distribu-\ntion matching. In Advances in Neural Information Process-\ning Systems (NIPS), pages 5495–5503, 2017.\n[28] Runde Li, Jinshan Pan, Zechao Li, and Jinhui Tang. Single\nimage dehazing via conditional generative adversarial net-\nwork. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8202–8211, 2018.\n[29] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv\npreprint arXiv:1705.02894, 2017.\n[30] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised\nimage-to-image translation networks. In Advances in neu-\nral information processing systems (NIPS), pages 700–708,\n2017.\n[31] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation.\nIn\nIEEE conference on computer vision and pattern recognition\n(CVPR), pages 3431–3440, 2015.\n[32] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen\nWang, and Stephen Paul Smolley. Least squares generative\nadversarial networks. In IEEE international conference on\ncomputer vision (ICCV), pages 2794–2802, 2017.\n[33] Ori Nizan and Ayellet Tal. Breaking the cycle - colleagues\nare all you need. In IEEE conference on computer vision and\npattern recognition (CVPR), 2020.\n[34] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018.\n[35] Taesung Park, Alexei A. Efros, Richard Zhang, and Jun-\nYan Zhu. Contrastive learning for unpaired image-to-image\ntranslation.\nIn European Conference on Computer Vision\n(ECCV), 2020.\n[36] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019.\n[37] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros.\nContext encoders: Feature\nlearning by inpainting.\nIn IEEE conference on computer\nvision and pattern recognition(CVPR), pages 2536–2544,\n2016.\n[38] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. In Advances in neural information pro-\ncessing systems (NIPS), pages 2234–2242, 2016.\n[39] Radim Tyleˇcek and Radim ˇS´ara. Spatial pattern templates\nfor recognition of objects with regular structure.\nIn Ger-\nman Conference on Pattern Recognition, pages 364–374.\nSpringer, 2013.\n[40] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-\nstance normalization: The missing ingredient for fast styliza-\ntion. arXiv preprint arXiv:1607.08022, 2016.\n[41] Chao Wang, Haiyong Zheng, Zhibin Yu, Ziqiang Zheng,\nZhaorui Gu, and Bing Zheng. Discriminative region proposal\nadversarial networks for high-quality image-to-image trans-\nlation. In European conference on computer vision (ECCV),\npages 770–785, 2018.\n[42] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\nJan Kautz, and Bryan Catanzaro. High-resolution image syn-\nthesis and semantic manipulation with conditional gans. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018.\n[43] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dual-\ngan: Unsupervised dual learning for image-to-image transla-\ntion. In IEEE International Conference on Computer Vision\n(ICCV), pages 2849–2857, 2017.\n[44] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,\nChao Dong, and Liang Lin.\nUnsupervised image super-\nresolution using cycle-in-cycle generative adversarial net-\nworks. In IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), pages 701–710, 2018.\n[45] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization.\nIn European conference on computer\nvision (ECCV), pages 649–666. Springer, 2016.\n[46] Yihao Zhao, Ruihai Wu, and Hao Dong. Unpaired image-\nto-image translation using adversarial consistency loss. In\nEuropean conference on computer vision (ECCV), 2020.\n[47] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks. In IEEE International Con-\nference on Computer Vision (ICCV), 2017.\n10\nA. Appendix\nA.1. Implementation Details\nA.1.1\nArchitecture of Generator and layers used for\nPatchNCE loss\nOur generator architecture is based on CycleGAN [47]\nand CUT [35].\nWe only use ResNet-based [17] gener-\nator with 9 residual blocks for training.\nIt contains 2\ndownsampling blocks, 9 residual blocks, and 2 upsampling\nblocks.\nEach downsampling and upsampling block fol-\nlows two-stride convolution/deconvolution, normalization,\nReLU. Each residual block contains convolution, normal-\nization, ReLU, convolution, normalization, and residual\nconnection.\nWe deﬁne the ﬁrst half of generators G and F as encoder\nwhich is represented as Genc and Fenc. The patch-based\nmulti-layer PatchNCE loss is computed using features from\nfour layers of the encoder (the ﬁrst and second downsam-\npling convolution, and the ﬁrst and the ﬁfth residual block).\nThe patch sizes extracted from these four layers are 9×9,\n15×15, 35×35, and 99×99 resolution respectively. Follow-\ning CUT [35], for each layer’s features, we sample 256 ran-\ndom locations and apply the 2-layer MLP (projection head\nHX, HY ) to infer 256-dim ﬁnal features.\nA.1.2\nArchitecture of Discriminator\nWe use the same PatchGAN discriminator architecture as\nCycleGAN [47] and Pix2Pix [21] which uses local patches\nof sizes 70x70 and assigns every patch a result. This is\nequivalent to manually crop one image into 70x70 overlap-\nping patches, run a regular discriminator over each patch,\nand average the results.\nFor instance, the discriminator\ntakes an image from either domain X or domain Y , passes it\nthrough ﬁve downsampling Convolutional-Normalization-\nLeakeyReLU layers, and outputs a result matrix of 30x30.\nEach element corresponds to the classiﬁcation result of one\npatch. Following CycleGAN [47] and Pix2Pix [21], in or-\nder to improve the stability of adversarial training, we use a\nbuffer to store 50 previously generated images.\nA.1.3\nArchitecture of four light networks\nFor\nSimDCL,\nwe\nuse\nfour\nlight\nnetworks\n(Hxr, Hxf, Hyr, Hyf).\nThese\nnetworks\nproject\nthe\n256-dim features to 64-dim vectors. Each network contains\none convolutional layer followed by ReLU, average pool-\ning, linear transformation (64-dim to 64-dim), ReLU, and\nlinear transformation (64-dim to 64-dim).\nA.1.4\nAdditional training details\nWe presented most training details in the main paper,\nhere, we depict some additional training details.\nFor\nSimDCL, we use the Adam optimiser [24] with β1\n= 0.5 and β2 = 0.999.\nWe update the weights of\nHX, HY , Hxr, Hxf, Hyr, Hyf together with learning rate\n0.0002.\nFor both DCLGAN and SimDCL, we initialize weights\nusing xavier initialization [11].\nWe load all images in\n286x286 resolution and randomly crop them into 256x256\npatches during training and we load test images in 256x256\nresolution. All images from the test set are used for evalua-\ntion. For all tasks, we train our method and other baselines\nwith a Tesla P100-PCIE-16GB GPU. The GPU driver ver-\nsion is 440.64.00 and the CUDA version is 10.2.\nA.1.5\nAdditional evaluation details\nWe list the evaluation details of Fr´echet Inception Distance\n(FID) [19] and Fully convolutional Network (FCN) [31]\nscore.\nFor FID [19] score, we use the ofﬁcial Py-\nTorch implementation with the default setting to match\nthe evaluation protocol of CUT [35].\nThe link is\nhttps://github.com/mseitzer/pytorch-ﬁd.\nFor FCN [31] score, we use the ofﬁcial PyTorch im-\nplementation of CycleGAN [47] and Pix2Pix [21].\nThe\nlink is https://github.com/junyanz/pytorch-CycleGAN-and-\npix2pix. The FCN [31] score is a well-known semantic seg-\nmentation metric on the CityScapes dataset. It measures\nhow the algorithm ﬁnds correspondences between labels\nand images. The FCN [31] score is computed using a pre-\ntrained FCN-8 [31] network that predicts a label map for\na photo. We input the generated photos to the pre-trained\nnetwork and measure the predicted labels with ground truth\nusing three semantic segmentation metrics including mean\nclass Intersection over Union (IoU), pixel-wise accuracy\n(pixAcc), and average class accuracy (classAcc).\nA.2. Qualitative results of ablations\nFor different ablations including (I) Adding the ﬁrst\nRGB pixels back, (II) Drawing external negatives, (III) Us-\ning the same encoder and MLP for one mapping instead of\ntwo, (IV) Adding cycle-consistency loss, and (V) Remov-\ning the dual setting. We show the randomly picked qualita-\ntive results in Figure 5 and Figure 6. The qualitative results\nsuggest that DCLGAN generates more realistic images than\nother variants, while each of our contribution has shown its\nefﬁciency.\nA.3. Additional Results\nWe evaluated our proposed method and baselines among\nnine tasks.\nWe choose the best four methods and show\nmore qualitative results among all tasks except for Facade\n11\n→Label. This is an extension of Figure 2 and Figure 3 in\nthe main paper. Figure 7 shows some qualitative results of\nHorse ↔Zebra, Figure 8 shows the results of Cat ↔Dog.\nThe results of CityScapes and Label →Facade are shown in\nFigure 9, Figure 10 shows the results of Van Gogh →Photo\nand Orange →Apple. From Figure 7, we can observe that\nDCLGAN does not show perfect inference results in Horse\n↔Zebra tasks. This is mainly due to the limitation of the\ndataset, where horse images are collected from ImageNet\nusing the keyword wild horse. Although DCLGAN per-\nforms better than all other state-of-the-art methods among\nmultiple challenging tasks, similar to most recent methods,\nit sometimes fails to distinguish the foreground and back-\nground.\nCat ↔Dog task requires geometry changes to match the\ndistribution. As shown in Figure 8, DCLGAN performs the\nbest in geometry changes and generates realistic cats/dogs\nwith reasonable structure while CycleGAN [47] fails to per-\nform any geometry change. For texture changes, DCLGAN\nconsistently outperforms all other methods on the whole.\nThis is shown on the third row of Figure 10 when CUT [35]\nfails to modify the color of whole oranges.\nB. Discussions\nB.1. Similarity loss and mode collapse\nThe degenerated solution for similarity loss is avoided\nby the constraints from other losses. The features sent to\nHxr, Hxf or Hyr, Hyf are also different at each iteration,\nwhere the features do not represent patches with the same\nlocation.\nSimilarity loss prevents mode collapse, this is due to the\nmode collapse outputs not only lack diversities but also tend\nto be unrealistic. However, the diverse real images are al-\nways of good quality. Thus, when there is a potential mode\ncollapse issue, the similarity loss increases, and behaves\nlike a regularization term, to avoid mode collapse.\nB.2. External negatives\nWe present the effect of drawing external negatives in\nsection 6, ablation (II). We show that drawing external neg-\natives in the same manner as SimCLR [5] may have a pos-\nitive inﬂuence on certain tasks quantitatively, but external\nnegatives usually negatively affect qualitative results. This\nis shown in ﬁgure 5, where the generated pedestrians & cars\ntend to be merged together. We agree with [35] that internal\nnegatives are more powerful than external negatives, how-\never, we hypothesize that drawing external negatives in dif-\nferent ways may lead to different conclusions which can be\ninvestigated in the future.\n12\nInput\nI\nII\nIII\nIV\nV\nDCLGAN\nFigure 5. Qualitative results of ablations on CityScapes task.\n13\nInput\nI\nII\nIII\nIV\nV\nDCLGAN\nFigure 6. Qualitative results of ablations on Horse →Zebra and Zebra →Horse tasks.\n14\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nFigure 7. Additional results of Horse ↔Zebra.\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nFigure 8. Additional results of Cat ↔Dog.\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nFigure 9. Additional results of CityScapes and Label →Facade.\n15\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nInput\nDCLGAN\nSimDCL\nCUT\nCycleGAN\nFigure 10. Additional results of Van Gogh →Photo and Orange →Apple.\n16\n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2021-04-15",
  "updated": "2021-04-15"
}