{
  "id": "http://arxiv.org/abs/2104.10068v2",
  "title": "Contrastive Learning for Sports Video: Unsupervised Player Classification",
  "authors": [
    "Maria Koshkina",
    "Hemanth Pidaparthy",
    "James H. Elder"
  ],
  "abstract": "We address the problem of unsupervised classification of players in a team\nsport according to their team affiliation, when jersey colours and design are\nnot known a priori. We adopt a contrastive learning approach in which an\nembedding network learns to maximize the distance between representations of\nplayers on different teams relative to players on the same team, in a purely\nunsupervised fashion, without any labelled data. We evaluate the approach using\na new hockey dataset and find that it outperforms prior unsupervised approaches\nby a substantial margin, particularly for real-time application when only a\nsmall number of frames are available for unsupervised learning before team\nassignments must be made. Remarkably, we show that our contrastive method\nachieves 94% accuracy after unsupervised training on only a single frame, with\naccuracy rising to 97% within 500 frames (17 seconds of game time). We further\ndemonstrate how accurate team classification allows accurate team-conditional\nheat maps of player positioning to be computed.",
  "text": "Contrastive Learning for Sports Video: Unsupervised Player Classiﬁcation\nMaria Koshkina\nYork University\nToronto, Canada\nkoshkina@yorku.ca\nHemanth Pidaparthy\nYork University\nToronto, Canada\nphemanth@yorku.ca\nJames H. Elder\nYork University\nToronto, Canada\njelder@yorku.ca\nAbstract\nWe address the problem of unsupervised classiﬁcation of\nplayers in a team sport according to their team afﬁliation,\nwhen jersey colours and design are not known a priori. We\nadopt a contrastive learning approach in which an embed-\nding network learns to maximize the distance between rep-\nresentations of players on different teams relative to players\non the same team, in a purely unsupervised fashion, without\nany labelled data. We evaluate the approach using a new\nhockey dataset and ﬁnd that it outperforms prior unsuper-\nvised approaches by a substantial margin, particularly for\nreal-time application when only a small number of frames\nare available for unsupervised learning before team assign-\nments must be made. Remarkably, we show that our con-\ntrastive method achieves 94% accuracy after unsupervised\ntraining on only a single frame, with accuracy rising to 97%\nwithin 500 frames (17 seconds of game time). We further\ndemonstrate how accurate team classiﬁcation allows accu-\nrate team-conditional heat maps of player positioning to be\ncomputed.\n1. Introduction\nTeam membership classiﬁcation (i.e. labelling each per-\nson on a playing surface as a member of team A, team B or\na referee) is a critical task in sports video analytics: most in-\nferences and statistics depend upon knowing which player\nare on each team, including attempts on goal, offsides, and\nplayer conﬁgurations. Accurate team afﬁliation labels can\nalso improve player tracking. The problem can be challeng-\ning due to the extreme variations in player pose, occlusions,\nmotion blur and uneven illumination.\nPrior work (e.g., [17, 13]) has framed the problem as a\nsupervised learning task in which labelled data (e.g., bound-\ning boxes with team identiﬁers) are used to learn a classiﬁer.\nEarly supervised methods employed hand-crafted colour-\nbased features [18, 16], while more recent approaches train\nconvolutional neural networks (CNNs) on labelled datasets\nto perform player segmentation [13] and classiﬁcation [17].\nUnfortunately, the supervised player classiﬁcation ap-\nproach [17] has limited application, since it requires ﬁne-\ntuning on every new game for optimal classiﬁer perfor-\nmance . The team segmentation approach [13] has been\nfound to generalize better but does not provide player in-\nstance segmentation and requires expensive pixel-wise an-\nnotation to train the system. For all of these reasons, an\nunsupervised approach is preferred.\nTo date, unsupervised approaches [21, 14, 7, 4, 26] rely\nsolely on colour-based features such as colour histograms.\nWhile these are simple and lightweight, typically many\nframes are needed from each new game in order to learn\nthe colour distributions, and these methods fail when the\ntwo teams are wearing similar colours.\nOur goal in this paper is to understand whether a more\npowerful representation, that may include both colour and\nconﬁgural information, can be learned in a fully unsuper-\nvised manner, and whether such a representation can reduce\nthe number of frames needed for training and improve gen-\nerallization to novel teams, jerseys, lighting and camera pa-\nrameters.\nTo achieve this, we employ unsupervised contrastive\nlearning to train a CNN to cluster players into two teams.\nWe demonstrate our system’s performance on a new hockey\ndataset and compare it to previously proposed unsupervised\nteam afﬁliation learning approaches. Figure 1 demonstrates\noverall system design. The dataset and code are available at\nhttps://github.com/mkoshkina/teamId.\nOur main contributions are:\n1. We introduce what is, to our knowledge, the ﬁrst un-\nsupervised deep learning approach for team classiﬁ-\ncation. This novel contrastive learning approach al-\nlows us to generalize to novel games, teams and jerseys\nwithout labelled data.\n2. We introduce a new annotated hockey dataset that can\nbe used to evaluate player detection and team classiﬁ-\ncation algorithms.\n3. We show that our novel unsupervised algorithm out-\nperforms prior unsupervised approaches by a large\narXiv:2104.10068v2  [cs.CV]  3 May 2021\nmargin, especially when only a small number of\nframes are available for unsupervised learning before\nteam assignments must be made. This limits the burn-\nin time for real-time streaming applications and allows\nthe system to adapt quickly to changes in lighting or\ncamera parameters.\n4. We show how our system for team classiﬁcation can be\nused to produce accurate team-conditioned heat maps\nof player positioning, useful for coaching and strategic\nanalysis.\n2. Related Work\n2.1. Player Classiﬁcation\nAutomatic labelling of players according to team is criti-\ncal for sport video understanding, including player tracking\n[18, 16, 26, 3], player conﬁguration analysis, activity recog-\nnition [4] and detection of game events [7].\nEarly work relied on colour histograms [21, 3, 14, 7, 18,\n16, 4] and ‘bag of words’ representations of colour features\n[26]. These approaches are lightweight, however the exclu-\nsive reliance on colour features make them more sensitive to\nillumination changes and could lead to lower performance\nwhen teams are wearing similar colours.\nIn recent years, supervised deep learning based meth-\nods for player detection and player labelling have been pro-\nposed [17, 13]. These methods perform well but require\nlabelled data for training.\nIn [13], a CNN is trained to\nsegment players and generate team pixel-wise descriptors,\nwhere pixels of players from the same team have descrip-\ntors that are close in embedding space.\nPixels are then\nclustered to identify the players on the two teams. This\nmethod requires pixel-level team labelling to train the net-\nwork and per-image pixel-level clustering at the inference\nstage. Moreover, it does not provide instance-level segmen-\ntation so would not be suitable for use in player location\nheatmap generation.\nLu et al. [17] also take a supervised approach, employ-\ning a cascaded CNN to learn team membership classiﬁca-\ntion (team A, team B and others) from labelled data. This\nmethod has good results but does not generalize well and\nthus requires ﬁne-tuning on labelled samples from a new\ngame in order to be used for that game.\nClearly, both simple colour-based unsupervised ap-\nproaches and more sophisticated CNN-based supervised ap-\nproaches have limitations. Here we explore whether mod-\nern deep unsupervised learning methods can be used to\novercome these limitations.\n2.2. Contrastive Learning and Deep Clustering\nContrastive learning [9] is a self-supervised representa-\ntion learning approach that aims to map similar objects to\nbe close in embedding space and dissimilar objects further\napart, and has been shown to produce excellent results on a\nnumber of tasks [10, 6]. In our work we use a simple CNN\ntrained with triplet loss [12] to learn a feature space that best\nseparates players into two teams.\nRecent work in contrastive learning [10, 6] shows excel-\nlent results in unsupervised representation learning on large\ndatasets such as ImageNet[24] or COCO[15]. These meth-\nods are based on noise contrastive estimation and involve\nusing an anchor (typically an augmented version of an orig-\ninal image), one positive (another augmented version of the\nsame image) and a large number of negatives, randomly\npicked from the training set. This setup works well for a\ndataset with a large number of categories, where randomly-\npicked images are unlikely to contain many positives. In our\nsetting however, although we have a large number of images\nwe have a relatively small number of categories (unique jer-\nsey designs). More precisely, ImageNet contains 1000 cat-\negories and our training dataset has 10. As a consequence,\nin our setting using random images as negatives results in a\n10% of false negatives. This adversely affects training. For\nthis reason, a simple triplet loss works much better in our\nsetting.\nOur work is inspired in part by deep clustering ap-\nproaches [27, 28] in which CNNs are used to jointly learn\nfeature representations and cluster centres in an unsuper-\nvised fashion. In our approach we use pseudo-labels from\nan initial k-means clustering as a supervision signal to train\nour contrastive learning CNN. The main divergence from\nprior methods is that we are only interested in learning fea-\nture space that will lead to good data separation - cluster\ncentres can be quite different in each new game.\nOnce\ntrained, the network is only used to extract features from\nplayer images.\n3. Method\n3.1. Overview\nOur general goal is to develop automatic sports analy-\nsis tools that provide valuable visualizations, statistics and\nanalyses for coaches and players. Our current work is fo-\ncused on hockey, but can easily be adapted to other team\nsports such as soccer, basketball and football. In this paper\nwe design and evaluate a system that automatically detects\nplayers, classiﬁes them into teams and returns a heatmap of\nthe distribution of players for each of the two teams.\nWe employ video from a stationary 4K camera that cap-\ntures the whole playing surface, and use the Mask R-CNN\nnetwork [11]) to detect and segment all people on the ice,\nincluding the players from the two teams and the referees.\nSince the referee uniform is consistent across games, we\nﬁrst train a CNN to perform referee classiﬁcation based\nupon labelled data (referee, non-referee).\nMask R-CNN\nReferee and \nplayer \nlabelling\nTeam \nposition \nheatmap \ngeneration\nTeam B\nTeam A\nReferee \nclassifier\nPlayer \nembedding\nnetwork\nClustering\nRefs\nTeam A\nTeam B\nFigure 1. Overview of the proposed system. Mask R-CNN is ﬁrst used to detect and segment each person on the playing surface. A\npre-trained CNN is then used to classify referees, whlie remaining players are passed to our embedding network for clustering into teams.\nThis allows production of heat maps showing the distribution of the two teams over the playing surface.\nIn order to classify players we employ an embedding\nCNN trained with triplet loss to extract a learned feature\nvector for each player image. We then use k-means to es-\ntimate cluster centres for the two teams from one or more\ninitial frames of the video. On all subsequent frames we as-\nsign each player to a team based on the closest cluster cen-\ntre in feature space. Using a learned homography, we geo-\nlocate each detected player on the ice surface and use kernel\ndensity estimation (KDE) to construct a heatmap represent-\ning the distribution of players across the playing surface for\neach team. Figure 1 shows the pipeline for our system.\n3.2. Dataset\nWe introduce a new labelled hockey video dataset that\nwill be made public on acceptance of this paper. Despite\nthe variety of available sport video datasets [1, 8], to the best\nof our knowledge our new hockey dataset will be the only\npublicly-available sport video dataset that contains team af-\nﬁliation labels.\nThe dataset is drawn from 15 different hockey games\ncaptured over two seasons. Seven of the games (season 1)\nare captured with a wide-ﬁeld stationary 4K (3840 × 2160\npixel) 30 fps camera that captures nearly the whole rink. In\norder to better capture the whole rink, season 2 games are\ncaptured by two 4K cameras with 75 degree horizontal dis-\nplacement, together capturing the whole rink with modest\noverlap. We deﬁned a virtual camera with intrinsic parame-\nters matching the two real cameras and extrinsic parameters\nequal to the mean of the two real cameras. Each of the two\ncamera images was rectiﬁed to the virtual camera through\na homography with the ice surface. The two virtual images\nwere then smoothly blended. The resulting season 2 videos\nhave resolution of 5930 × 1080 pixels.\nWe manually close-cropped the videos to the 3840×900\n(season 1 games) and 5680×904 (season 2 games) rectan-\ngle bounding the rink (Fig. 1). From each game we ran-\ndomly extracted a video clip of roughly 850 frames (28 sec).\nEach game contains a unique combination of player uni-\nforms, and since play is active in each clip there is consid-\nerable variation in player pose, motion blur and occlusions\nbetween players. Players were automatically detected using\nMask R-CNN (see below). To eliminate coaches and bench\nplayers, we applied a heuristic to exclude detections close\nto the bottom of the frame that had bounding box height less\nthan twice the width.\nFor evaluation only, we manually annotated every 10th\nframe of each game clip, thus obtaining between 80-90 la-\nbelled frames per game. Annotations consist of:\n1. Mask R-CNN detections\n• Class label (Team A, Team B, Referee)\n2. Manual detections (including players not detected by\nMask R-CNN)\n• Class label (Team A, Team B)\n• Estimated image projections of points of contact\nwith playing surface (skates on the ice)\nTo label the Mask R-CNN detections, extracted player\nimages with segmentation masks applied were manually in-\nspected one-by-one. Only the images that could be iden-\ntiﬁed by visual inspection to belong to a player (Team A,\nTeam B ) or referee were labelled, the rest were marked as\nfalse positives. If there were multiple players within one\nextracted image the player with the most pixels in the mask\nwas selected.\nWe use the Mask-RCNN labels of detected players to\nevaluate the accuracy of team classiﬁcation algorithms and\nuse the manual detection annotations to evaluate the accu-\nracy of our team positioning heatmaps.\nThe 15-game dataset was divided into training, valida-\ntion and test sets with a 9-2-4 split. Both training and test\nset contains a mix of season 1 and season 2 games. One\nlimitation of the dataset is that even though each game has a\nunique combinations of teams playing, some teams appear\nmultiple times through the dataset. We have ensured that\nthe test set includes a game with previously unseen teams.\n3.3. Player Detection and Segmentation\nWe employ Mask R-CNN [11] trained on MS COCO\n[15] to detect and segment all people on the playing sur-\nface. To adapt to the different resolution, aspect ratio and\nexpected size of people in our video relative to MS COCO,\nwe partitioned each frame into left and right images with a\n40 pixel central overlap, running Mask R-CNN on each in-\ndividually before merging results. Bounding boxes detected\nin the left image that overlap boxes detected in the right im-\nage by 45% or more are merged by selecting the larger of\nthe two boxes. We deﬁne the estimated image location of\neach player as the mid-point of the lower boundary of the\nR-CNN bounding box.\n3.4. Referee Classiﬁer\nSince referee uniforms are consistent across games, a su-\npervised approach is appropriate. We use the referee/non-\nreferee labels from our Mask R-CNN detections to train and\nevaluate a simple CNN classiﬁer. This is the only way that\nlabelled data is used in our system, aside from evaluation.\nOur CNN classiﬁer takes as input R-CNN detection images\nwith segmentation masks applied and classiﬁes them as ref-\neree or non-referee. We employ a small CNN with 3 con-\nvolutional layers (16, 32, and 64 output channels) and 3x3\nkernels followed by 2 fully connected layers. We train the\nnetwork with a binary cross entropy loss function, employ-\ning the Adam optimizer.\n3.5. Unsupervised Team Assignment:\nFeature\nLearning and Clustering\nAn ideal team labelling algorithm will be unsupervised,\ngeneralizing to new games without needing any labelled\ndata, and will require minimal frames (burn-in time) from\nthe beginning of the game to determine accurate labels for\neach player on the team.\nPrevious unsupervised approaches used colour features\nsuch as histograms and bag-of-colours. These approaches\ncan be effective but since they do not consider spatial fea-\ntures, performance may suffer when teams are wearing jer-\nseys with similar colour proﬁles, or when illumination vari-\nations render colour features unreliable. Here we explore\nwhether an embedding CNN trained by contrastive learning\ncan produce a more powerful representation that, by incor-\nporating both colour and spatial features, can learn a reliable\nfeature representation from fewer frames, and thus have a\nshorter burn-in time.\nWe employ a CNN with 3 convolutional layers (16, 32,\nand 64 channels) and 3x3 kernels, each followed by a pool-\ning layer, and two fully connected layers. The last layer\nreturns a feature vector of length 1024. We train our net-\nwork using the Adam optimizer on a training set of games\nusing a triplet loss [12]. Input is a triplet of extracted images\nwith the R-CNN mask applied: an anchor image, a positive\nimage and a negative image. The positive image is an image\nof a player believed to be from the same team as the anchor\nimage, while the negative image is a player believed to be\non the other team. The triplet loss function, when back-\npropagated, drives the network to decrease the distance in\nthe embedding space between the anchor and positive im-\nages, while increasing the distance between the anchor and\nnegative images. In order to ensure that the learned repre-\nsentation does not exclusively rely on colour, we randomly\nconvert 50% of training triplets to grayscale.\nUnsupervised training of the embedding network re-\nquires a method for estimating whether two input images\nhave the same or different labels.\nWe seed this process\nwith a simple colour-based distance measure, representing\neach image as a normalized RGB histogram with 8 bins per\ncolour channel and then using k-means to cluster players\ninto two teams.\nTo form the triplets, we ﬁrst rank the player images xi in\nterms of their team assignment conﬁdence scores pij, using\na standard ‘soft k-means’ measure:\npi1\n=\n∥xi −c2∥\n∥xi −c1∥+ ∥xi −c2∥\n(1)\npi2\n=\n∥xi −c1∥\n∥xi −c1∥+ ∥xi −c2∥\n(2)\nwhere cj is the centre of cluster j and pij is the conﬁdence\nwith which image i is assigned to cluster j.\nWe consider only high-conﬁdence samples (pij > 0.9))\nfor training to limit the label noise. We then randomly form\ntriplets by sampling the anchor and positive images from\none cluster (anchor and positive example) and the negative\nimage from the other.\nAs the training proceeds we regenerate these pseudo-\nlabels and training triplets, but replacing the histogram\nrepresentation with the evolving embedded representation\nlearned by the network. We train until convergence (no im-\nprovement on the validation data for 3 epochs) or a maxi-\nmum of 30 epochs on the initial colour histogram pseudo-\nlabels and then generate new pseudo-labels from the evolv-\ning embedded representation every 10 epochs (or until con-\nvergence). We ﬁnd that the proportion of high-conﬁdence\nsamples grows over time, indicating that the network is\nlearning a representation that improves data separation. Fig-\nure 2 illustrates this training process.\nOnce unsupervised training of the embedding network\non the training set is complete, we apply the network to\nnovel games with unseen teams and uniforms. We use the\nﬁrst nburn frames of the unseen game as input to k-means\nto determine the two cluster centres for this new game in\nthe pre-learned embedding space. Once the cluster centres\nare identiﬁed, we associate detected players in subsequent\nframes with the nearest cluster centre, and evaluate on anno-\ntated frames. Figure 3 illustrates our use of data for training\nand evaluation purposes.\n3.6. Team Positioning Heatmaps\nOne of the many useful applications of player detection\nand labelling is the generation of team positioning heatmaps\nthat can help coaches and players understand how their\nplayers and the players on the opposing team tend to be\ndistributed throughout a game or portion of the game.\nTo generate these heatmaps we use a learned homog-\nraphy to transfer the image coordinates of each detected\nplayer (midpoint of the bottom of each bounding box) to\nthe corresponding point on a model of the playing sur-\nface. The homography was computed from 19 correspond-\ning pairs of points in one video frame and in a template\nmodel of the ice rink (Illustration showing keypoints and\nbackprojected player positions is included in supplementary\nmaterial). We then used the standard least-squares repro-\njection method [19] to estimate the homography mapping\nimage pixels to points on the ice surface. Based on these\nplayer positions and the team afﬁliations estimated using\nour unsupervised contrastive learning algorithm over mul-\ntiple frames, we compute a rectiﬁed map of player density\n(players per square metre per frame) using Gaussian kernel\ndensity estimation (KDE) [23, 22]. Figure 5 shows exam-\nples of these automatically-generated maps.\n4. Evaluation\n4.1. Implementation Details\nOur system is implemented in Python 3 with Pytorch\nand Sklearn. We use the publicly-available Mask R-CNN\nnetwork and weights [2] with a conﬁdence threshold of\n0.6. Both referee and embedding networks take as input\nplayer images with segmentation masks applied, resized to\n62×128 pixels, roughly the average size of a player im-\nage. To reduce the impact of illumination variations we\napplied an afﬁne transform I′\ni(x, y) = aI(x, y) + b to the\nintensities Ii of all three channels i ∈{R, G, B} such that\nminx,y,i I′\ni = 0 and maxx,y,i I′\ni = 255.\nK-means computation of cluster centres entails 10 ran-\ndom initializations: The solution that minimizes the mean\nsquared deviation from cluster centres is selected.\n4.2. Comparison with Other Unsupervised Ap-\nproaches\nWe compare the performance of our unsupervised team\nafﬁliation algorithm against the two main previously pro-\nposed unsupervised team labelling approaches: colour his-\ntograms [21, 14, 7, 18, 16, 4] and bag-of-words representa-\ntions of colour features [26]. Since the code and datasets for\nthese previous approaches are not available, we performed a\nhyperparameter search using k-fold cross-validation to de-\ntermine the optimal parameters and use k-means cluster-\ning to determine cluster centres. These optimal parameters\nwere the number of bins per channel for the histogram al-\ngorithm and number of words for the bag of colours algo-\nrithm. In addition, we evaluated whether to use the entire\nsegmented player or just the upper half, since the lower half\nof the uniform is fairly consistent across teams, and also\nexperimented with multiple colour spaces (see below).\nWe also experiment with replacing features learned by\nour contrastive learning network with features learned with\nconvolutional autoencoder (see Section 4.2.3).\nComparison with previously used supervised approaches\n[17, 13] is not feasible as the code and datasets are not avail-\nable.\n4.2.1\nColour Histogram Algorithm\nOur colour histogram method simply histograms the\ncolours within the segmented player, normalizing by the\nnumber of pixels. We experimented with RGB, LAB and\nHSV colour spaces, and also tried eliminating the luma or\nvalue channel (i.e., two-dimensional AB and HV spaces) to\nreduce sensitivity to illumination, but found optimal perfor-\nmance with RGB coding.\nCluster centres are then found using k-means, using Eu-\nclidean distance in the colour histogram space. The single\nhyperparameter is the number n of bins per channel: k-fold\ncross-validation revealed that n = 8 produces best results\nfor our dataset. We also found that performance was slightly\nbetter if only the upper half of the segmented player was\nconsidered as the player jerseys are most distinct between\nteams.\nStep 1: Cluster images \nfrom each game based \non color features. \nGenerate initial input \ntriplets.\nK-Means\nInput Triplets\nCNN\nCNN\nCNN\nTriplet \nLoss\nStep 2: Train embedding \nnetwork for several \niterations\nK-Means\nStep 3: Cluster images \nbased on newly-learned \nfeatures. Regenerate \ninput triplets based on \nnew clustering results.\nRepeat Step 2 & Step 3 \nuntil network convergence\nFigure 2. Self-supervised training of embedding network.\n(a) Network training\nvideo frames\nReferee CNN\nRepresentation \nlearning \n10th\n20th\n(b) Evaluation: novel video\nCluster\nEmbedding\nCNN\nnburn\nEvaluate predictions\nEmbedding\nCNN\ncluster centres\nunlabeled\nlabeled\nCluster centres\nlearning\nEvaluation\nUnlabeled\nframe\nLabeled\nframe\nUnlabeled \nframe used for \ncluster learning\nFigure 3. Data usage for training and evaluation. a) Labelled frames are used to train the referee classiﬁer, but the embedding network\nrepresentation is learned in unsupervised fashion, without reference to labels. b) To perform inference on a novel video, we use the ﬁrst\nnburn frames to ﬁnd cluster centres in the learned embedded representation. Labelled frames are used only for evaluation.\n4.2.2\nBag-of-Colours Algorithm\nIn our bag-of-colours method, we employ the expectation\nmaximization algorithm to ﬁt a Gaussian mixture model\n(GMM) with n components to the normalized colours of the\nplayers in the initial training partition of the novel game.\nThese components then form the words of a dictionary\nwith which to encode players in subsequent frames. K-fold\ncross-validation reveals that n = 35 components yields op-\ntimal results. We use k-means clustering to ﬁnd each team’s\ncluster centres in this 35-dimensional space and assign play-\ners to the closest cluster. We again ﬁnd that considering\nonly the top half of the segmented player yields superior\nresults. We also consider a variation of this approach, pre-\ntrainied bag-of-colours, where the dictionary of colours is\nlearned on training set games.\n4.2.3\nAutoencoder\nFor additional comparison, we use small convolutional au-\ntoencoder [20] trained on image reconstruction. The en-\ncoder network architecture is kept the same as our embed-\nding network and decoder mirrors the encoder. After train-\ning on the images in our training set, we use encoder por-\ntion to extract 1024 feature vector for each test image. We\nthen use these features in the same setting as our embedding\nfeatures to ﬁrst learn cluster centres on the burn-in frames\nand then assign players to closest centre for the rest of the\nframes.\n4.3. Evaluation Methodology\nWe evaluate team afﬁliation labelling on players detected\nby mask R-CNN. These include false positives and imper-\nfect segmentations. In addition, since the referee classiﬁer\nis also imperfect, some referees will be incorrect classiﬁed\nas players and will add noise to the contrastive learning pro-\ncess. We test both our supervised referee classiﬁer and our\nunsupervised embedding network team classiﬁer on the test\nset consisting of 4 games.\nAccuracy is evaluated over the 30 annotated frames im-\nmediately following the burn-in interval. Since every tenth\nframe is annotated, this represents roughly 10 sec of video\nat 30 fps.\nWe assess effects of noise in initial pseudo-labels on em-\nbedding network performance by considering different team\nassignment conﬁdence scores pij thresholds. Higher conﬁ-\ndence threshold leads to better clustering performance. We\ninclude this evaluation in supplementary materials .\n4.4. Referee Classiﬁcation\nFor each game we have 80-90 frames annotated frames\nand there are 3-4 referees on the rink, so for 9 training\ngames we have 2000 referees in our training set, augment-\ning these by left/right reﬂections yields a total of 4000 train-\ning vectors.\nEmploying a softmax threshold of 0.5, we\nachieve a mean accuracy of 98% with 93%, precision, 96%\nrecall.\nPrecision-recall curve for referee classiﬁer is in-\ncluded in supplementary materials.\n4.5. Team Classiﬁcation\nTable 1 shows the mean accuracy of team classiﬁca-\ntion for the all algorithms under evaluation. Results de-\npend upon the number nburn of frames available for learn-\ning cluster centres prior to inference. When nburn is large\n(512 in this case), two colour-based and our methods per-\nform fairly well, with colour-based methods rivalling our\nCNN approach. However, when nburn is small (1 in this\ncase), performance of the colour-only methods drops dra-\nmatically, while our embedding CNN approach still per-\nforms very well.\nThis behaviour is shown in more detail in Fig. 4. We see\nthat the simpler colour-based approaches and autoencoder\napproach improve continuously as the number of training\nframes increases, while our embedding CNN approach per-\nforms well even with only one training frame, improving\nonly modestly thereafter. At least 512 burn-in frames are re-\nquired before the pure colour approaches begin to rival our\nMethod\nnburn = 1\nnburn = 512\nColour Histogram\n0.87 ± 0.031\n0.97 ± 0.012\nBag-of-colours\n0.76 ± 0.032\n0.97 ± 0.018\nPretrained Bag-of-colours\n0.86 ± 0.099\n0.89 ± 0.189\nAutoencoder\n0.70 ± 0.076\n0.92 ± 0.099\nEmbedding CNN\n0.94 ± 0.009\n0.97 ± 0.011\nTable 1. Team classiﬁcation accuracy as a function of the number\nnburn of frames available for learning cluster centres prior to in-\nference. We show mean and standard error of the accuracy over 4\ntest games.\nFigure 4. Error rate as a function of the number nburn of initial\nframes used to learn cluster centres. We show mean error of the\naccuracy over 4 test games\nembedding CNN algorithm. Autoencoder method is lag-\nging behind even with 512 burn-in frames.\nWe believe that the advantage of our embedding CNN\napproach derives from the ability of our unsupervised con-\ntrastive learning network to learn from the training games\nan embedding space that is more effective for discriminat-\ning teams than colour histograms. This more discrimina-\ntive space then allows well-separated cluster centres to be\nlearned very quickly from the novel game.\n4.6. Team Position Heatmaps Results\nOne useful application of player detection and team clas-\nsiﬁcation is to allow visualization of team positioning over\nthe course of a game or a portion of a game. We demonstrate\nthis by generating heatmaps for each game based upon the\n800-900 frames used for each game in our experiments. A\nlearned homography is employed to back-project the image\nlocation (midpoint of the bottom boundary of the bounding\nbox) to the playing surface. We also back-project our man-\nual detections (Section 3.2) to form a ground-truth heatmap.\nGaussian kernel density estimation [23, 22] is then used\nto estimated the player density (players per metre squared\nper frame) for both estimated and ground-truth heatmaps.\nHistogram\nGround \nTruth\nTeam A\nTeam B\nBofC\nCNN\n(Ours)\nPosition map\nPosition map\nError map\nError map\nPosition probability\nSquared error\nFigure 5. Team positioning heatmaps for a test game.\nMethod\nMean KL-divergence\nColour histogram\n0.072\nBag-of-colours\n0.069\nEmbedding CNN (Ours)\n0.047\nTable 2. KL-divergence of automatically-generated player posi-\ntioning heatmaps from ground truth.\nThe Gaussian bandwidth for KDE is calculated using Sil-\nverman’s rule of thumb [25], and is roughly 30 pixels for all\nimages (template rink image size is 496x240 pixels).\nFig. 5 shows example results from one test game for the\nthree team classiﬁcation methods using nburn = 1 frames\nto learn cluster centres. We see that our embedding CNN\napproach more consistently represents the true player den-\nsities than the pure-colour histogram or bag of colours ap-\nproaches.\nFor quantitative evaluation, we scale the maps to inte-\ngrate to one and then compute the KL-divergence between\nestimated and ground truth densities over our test set (Ta-\nble 2).\nWhile the bag-of-colours algorithm outperforms\na simple colour histogram, our embedding CNN approach\nsubstantially outperforms both pure-colour methods.\n4.6.1\nRuntime\nOur experiments are conducted on 3.6GHz Intel Core i9\nCPU x 16 with 64 GB RAM and an Nvidia GeForce RTX\n2080 GPU. Our method runs in real time on segmented\nplayer images. It takes 21 miliseconds to learn team ap-\npearances for the game from a single frame and 11 milisec-\nonds per frame for inference on subsequent frames. For\nconvenience, we employed the widely-available but non-\nreal-time Mask R-CNN network [11] for player detection\nand segmentation, which runs at roughly 5fps. If replaced\nwith a real-time segmentation network, such as Yolact [5],\nour whole system will run in real-time. We leave this for\nfuture work.\n5. Conclusions & Future Work\nOur results demonstrate that a learned representation that\ncan incorporate both colour and spatial features can produce\nsuperior results for team classiﬁcation than a pure-colour\napproach. We also demonstrate that such a representation\ncan be learned in unsupervised fashion, using contrastive\nlearning with a triplet loss. A major beneﬁt is that unsu-\npervised pre-learning of the representation allows for ultra\nrapid learning of cluster centres from novel games, which\nlimits the burn-in period, allowing online inference. We\nalso show how this approach to team classiﬁcation can be\nused to produce accurate team-conditional player position-\ning maps that can be useful for coaching and game analysis.\nImprovements could be made by integrating with player\ntracking: While team classiﬁcation will aid tracking, the\nconverse is also true: Tracking can potentially eliminate oc-\ncasional errors in team classiﬁcation.\nAcknowledgements.\nWe\nacknowledge\nthe\nsup-\nport of the York VISTA (vista.info.yorku.ca) and Re-\nsearch Chair programs.\nWe thank Canlan Ice Sports\n(www.canlansports.com) and York University Athletics for\nproviding the video data for this project.\nReferences\n[1] APIDIS\nbasketball\ndataset.\nhttps : / / sites .\nuclouvain . be / ispgroup / index . php /\nSoftwares/APIDIS.\n[2] Waleed Abdulla. Mask R-CNN for object detection and in-\nstance segmentation on Keras and TensorFlow.\nhttps:\n//github.com/matterport/Mask_RCNN, 2017.\n[3] H. Ben Shitrit, J. Berclaz, F. Fleuret, and P. Fua. Tracking\nmultiple people under global appearance constraints. In 2011\nInternational Conference on Computer Vision, pages 137–\n144, 2011.\n[4] Alina Bialkowski, Patrick Lucey, Peter Carr, Sridha Sridha-\nran, and Iain Matthews. Representing team behaviours from\nnoisy data using player role. In Computer Vision in Sports,\npages 247–269. Springer, 2014.\n[5] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\nYOLACT: Real-time instance segmentation. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 9157–9166, 2019.\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020.\n[7] Tiziana D’Orazio, Marco Leo, Paolo Spagnolo, Pier Luigi\nMazzeo, Nicola Mosca, Massimiliano Nitti, and Arcangelo\nDistante. An investigation into the feasibility of real-time\nsoccer offside detection from a multiple camera system.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 19(12):1804–1818, 2009.\n[8] Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and\nBernard Ghanem. Soccernet: A scalable dataset for action\nspotting in soccer videos. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) Workshops,\nJune 2018.\n[9] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional-\nity reduction by learning an invariant mapping. In 2006 IEEE\nComputer Society Conference on Computer Vision and Pat-\ntern Recognition, volume 2, pages 1735–1742. IEEE, 2006.\n[10] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual repre-\nsentation learning. arXiv preprint arXiv:1911.05722, 2019.\n[11] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick. Mask R-\nCNN. In 2017 IEEE International Conference on Computer\nVision, pages 2980–2988, 2017.\n[12] Elad Hoffer and Nir Ailon. Deep metric learning using triplet\nnetwork.\nIn International Workshop on Similarity-Based\nPattern Recognition, pages 84–92. Springer, 2015.\n[13] Maxime\nIstasse,\nJulien\nMoreau,\nand\nChristophe\nDe Vleeschouwer.\nAssociative embedding for team\ndiscrimination. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition Workshops, pages\n0–0, 2019.\n[14] Zdravko Ivankovic, Milos Rackovic, and Miodrag Ivkovic.\nAutomatic player position detection in basketball games.\nMultimedia\nTools\nand\nApplications,\n72(3):2741–2767,\n2014.\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean Conference on Computer Vision, pages 740–755.\nSpringer, 2014.\n[16] Jingchen Liu and Peter Carr. Detecting and tracking sports\nplayers with random forests and context-conditioned mo-\ntion models. In Computer Vision in Sports, pages 113–132.\nSpringer, 2014.\n[17] Keyu Lu, Jianhui Chen, James J Little, and Hangen He.\nLightweight convolutional neural networks for player detec-\ntion and classiﬁcation. Computer Vision and Image Under-\nstanding, 172:77–87, 2018.\n[18] Wei-Lwun Lu, Jo-Anne Ting, James J Little, and Kevin P\nMurphy. Learning to track and identify players from broad-\ncast sports videos. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 35(7):1704–1716, 2013.\n[19] Quan-Tuan Luong and Olivier D Faugeras. The fundamental\nmatrix: Theory, algorithms, and stability analysis. Interna-\ntional Journal of Computer Vision, 17(1):43–75, 1996.\n[20] Jonathan Masci, Ueli Meier, Dan Cires¸an, and J¨urgen\nSchmidhuber. Stacked convolutional auto-encoders for hi-\nerarchical feature extraction. In International Conference on\nArtiﬁcial Neural Networks, pages 52–59. Springer, 2011.\n[21] Pier Luigi Mazzeo, Paolo Spagnolo, Marco Leo, and Tiziana\nD’Orazio. Football players classiﬁcation in a multi-camera\nenvironment.\nIn International Conference on Advanced\nConcepts for Intelligent Vision Systems, pages 143–154.\nSpringer, 2010.\n[22] Emanuel Parzen.\nOn estimation of a probability density\nfunction and mode.\nAnnals of Mathematical Statistics,\n33(3):1065–1076, 09 1962.\n[23] Murray Rosenblatt. Remarks on some nonparametric esti-\nmates of a density function. Annals of Mathematical Statis-\ntics, 27(3):832–837, 09 1956.\n[24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\nlenge.\nInternational Journal of Computer Vision (IJCV),\n115(3):211–252, 2015.\n[25] Bernard W Silverman. Density estimation for statistics and\ndata analysis, volume 26. CRC press, 1986.\n[26] Xiaofeng Tong, Jia Liu, Tao Wang, and Yimin Zhang. Au-\ntomatic player labeling, tracking and ﬁeld registration and\ntrajectory mapping in broadcast soccer video. ACM Trans-\nactions on Intelligent Systems and Technology, 2(2):1–32,\n2011.\n[27] Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi\nHong. Towards k-means-friendly spaces: Simultaneous deep\nlearning and clustering. In International Conference on Ma-\nchine Learning, pages 3861–3870. PMLR, 2017.\n[28] Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsuper-\nvised learning of deep representations and image clusters.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5147–5156, 2016.\nSupplementary Materials\nContrastive Learning for Sports Video: Unsupervised Player Classiﬁcation\n1. Effect of Noise in Pseudo-Labels\nTo evaluate effects of noise in initial pseudo-labels\non embedding network performance we consider different\nteam assignment conﬁdence scores pij thresholds. Setting\nhigher threshold results in selecting fewer samples in the\ninitial training set, while using lower threshold results in\nhaving a larger but noisier sample set. Table 1 shows that\nselecting higher conﬁdence threshold leads to better cluster-\ning performance.\nThreshold\nError Rate\nTrain Set Size\npij > 0.5\n0.134\n77%\npij > 0.7\n0.037\n72%\npij > 0.9\n0.031\n60%\nTable 1. Team classiﬁcation error as a function of initial pseudo-\nlabels conﬁdence threshold.\nWe indicate pecentage of training\nsamples that satisﬁed the threshold and were included in the initial\ntraining set.\n2. Homography\nIn order to generate team position heatmaps, we use a\nlearned homography to transfer the image coordinates of\neach detected player (midpoint of the bottom of each bound-\ning box) to the corresponding point on a model of the play-\ning surface. The homography was computed from 19 cor-\nresponding pairs of points in one video frame and in a tem-\nplate model of the ice rink Figure 1 shows keypoints and\nbackprojected player positions.\n3. Referee Classiﬁer\nFigure 2 shows precision-recall curve for referee classi-\nﬁer.\nFigure 2. Precision-Recall curve of referee classiﬁer.\n1\narXiv:2104.10068v2  [cs.CV]  3 May 2021\n(a) Rink key points\n(b) Player positions\nFigure 1. Homography mapping video pixels to ice coordinates. (a) Keypoint pairs. (b) Backprojected player positions.\n2\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-04-15",
  "updated": "2021-05-03"
}