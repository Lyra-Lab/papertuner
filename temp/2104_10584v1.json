{
  "id": "http://arxiv.org/abs/2104.10584v1",
  "title": "Deep Learning for Click-Through Rate Estimation",
  "authors": [
    "Weinan Zhang",
    "Jiarui Qin",
    "Wei Guo",
    "Ruiming Tang",
    "Xiuqiang He"
  ],
  "abstract": "Click-through rate (CTR) estimation plays as a core function module in\nvarious personalized online services, including online advertising, recommender\nsystems, and web search etc. From 2015, the success of deep learning started to\nbenefit CTR estimation performance and now deep CTR models have been widely\napplied in many industrial platforms. In this survey, we provide a\ncomprehensive review of deep learning models for CTR estimation tasks. First,\nwe take a review of the transfer from shallow to deep CTR models and explain\nwhy going deep is a necessary trend of development. Second, we concentrate on\nexplicit feature interaction learning modules of deep CTR models. Then, as an\nimportant perspective on large platforms with abundant user histories, deep\nbehavior models are discussed. Moreover, the recently emerged automated methods\nfor deep CTR architecture design are presented. Finally, we summarize the\nsurvey and discuss the future prospects of this field.",
  "text": "Deep Learning for Click-Through Rate Estimation\nWeinan Zhang†, Jiarui Qin†, Wei Guo‡, Ruiming Tang‡, Xiuqiang He‡\n†Shanghai Jiao Tong University\n‡Huawei Noah’s Ark Lab\n{wnzhang, qinjr}@apex.sjtu.edu.cn, {guowei67, tangruiming, hexiuqiang1}@huawei.com\nAbstract\nClick-through rate (CTR) estimation plays as a core\nfunction module in various personalized online ser-\nvices, including online advertising, recommender\nsystems, and web search etc. From 2015, the suc-\ncess of deep learning started to beneﬁt CTR estima-\ntion performance and now deep CTR models have\nbeen widely applied in many industrial platforms.\nIn this survey, we provide a comprehensive review\nof deep learning models for CTR estimation tasks.\nFirst, we take a review of the transfer from shallow\nto deep CTR models and explain why going deep\nis a necessary trend of development. Second, we\nconcentrate on explicit feature interaction learning\nmodules of deep CTR models. Then, as an impor-\ntant perspective on large platforms with abundant\nuser histories, deep behavior models are discussed.\nMoreover, the recently emerged automated meth-\nods for deep CTR architecture design are presented.\nFinally, we summarize the survey and discuss the\nfuture prospects of this ﬁeld.\n1\nBackground\nPersonalized services have become very important in various\nonline information systems [Fan and Poole, 2006], such as\nitem recommendation in e-commerce, auction in online adver-\ntising, page ranking in web search, to name a few. Regarding\nthe click as the representative behavior of user preference,\nclick-through rate (CTR) estimation based on learning over\nthe logged behavior data plays as a core function module in\nthese personalization services [Agarwal et al., 2014].\nUSER ID\nCITY\nOCCUPATION ITEM ID\nCATEGORY\nBRAND\nCLICK\nU1\nLA\nstudent\nI1\nT-shirt\nB1\n1\nU2\nNYC\nstudent\nI1\nT-shirt\nB1\n1\nU1\nLA\nstudent\nI2\ncell phone\nB2\n0\nU3\nLondon\nmanager\nI3\ncell phone\nB3\n0\nFigure 1: Multi-ﬁeld categorical data for CTR estimation.\nFor CTR estimation tasks, the dataset is commonly repre-\nsented as a table. In the training pipeline of the CTR esti-\nmator, the historical instance set with a pre-deﬁned size will\nbe fetched, e.g., the previous 7-day logs. Then the feature\nengineering module will be executed, including feature nor-\nmalization, discretization of continuous feature value, feature\nhashing and indexing etc. [Juan et al., 2016]. Finally, each\ninstance is in a multi-ﬁeld categorical format while the corre-\nsponding label is binary (1 for click and 0 for non-click), with\nan example shown in Fig. 1.1\nAs for the model training, the CTR estimation task is formu-\nlated as a binary classiﬁcation problem, with the cross entropy\nloss function as\nl(x, y, Θ) = −y log σ(fΘ(x)) −(1 −y) log(1 −σ(fΘ(x))) ,\n(1)\nwhere σ(z) = 1/(1 + exp(−z)) is the sigmoid function, fΘ\ndenotes the logit value function, parameterized by Θ, (x, y)\ndenotes the instance with feature vector x and label y.\nWe pick some representative models to demonstrate the\ndevelopment trend of CTR estimation models as illustrated in\nFig. 2. The development of the models could be summarized\ninto two aspects which are feature engineering complexity and\nmodel capacity. For early CTR models, constrained by com-\nputing power, the major efforts have been made on designing\nbetter features by human with adopting simple models. Later,\nmore complicated models (with deep architectures and better\nmodeling capacity) are introduced to liberate the complexity\nof human efforts for feature engineering. A more recent trend\nfocuses again on feature engineering using some learnable\nmethods, because it has come to a bottleneck of performance\nby solely designing more complicated deep models. Combin-\ning both complex models and learnable feature engineering is\nthe new development direction.\n2\nFrom Shallow to Deep CTR Models\nDealing with such a binary classiﬁcation task as in Eq. (1),\nlogistic regression (LR) [Richardson et al., 2007] is the most\nbasic model with the advantage of high efﬁciency and ease for\nfast deployment. The logit value of LR is calculated as\nf LR\nΘ (x) = θ0 +\nm\nX\ni=1\nxiθi ,\n(2)\nwhere Θ = {θi}m\ni=0 and m denotes the feature size. How-\never, many discriminative patterns for click prediction are\n1Note that the scope of data format in this paper is focused on\nmulti-ﬁeld categorical data as shown in Fig. 1. Other content data\nsuch as text and image is not considered.\narXiv:2104.10584v1  [cs.IR]  21 Apr 2021\nModel Capacity \nFeature Engineering Requirement\nLR('07)\nPOLY2\n('10)\nFMs('10)\nDeepFM('17)\nDIEN('19)\nAMER('20)\nUBR('20)\nSIM('20)\nAutoFIS('20)\nshallow/early deep\ndeep interaction \nbehavior modeling\nautoml \nW&D('16)\nDeveloping\nFigure 2: Development trend of CTR estimation models.\ncombining features (or called cross features), e.g., user\nis a student (OCCUPATION:STUDENT) and location is LA\n(CITY:LA) will uplift the predicted CTR for a Disneyland\nad (AD:DISNEYLAND), which corresponds to a third-order\ncombining feature. Therefore, a straightforward way is to\nmanually select and design many useful combining features,\nwhich requires enormous human efforts, as presented in Fig. 2.\nPOLY2 [Chang et al., 2010] assigns a weight to every\nsecond-order combining feature, which requires O(m2) pa-\nrameter space. Unfortunately, the performance of POLY2\nmight be poor when the data is sparse. The key problem is\nthat the parameters related to feature interactions cannot be\ncorrectly estimated in the sparse data where many features\nhave never or seldom occurred together.\nAnother way to improve LR is to utilize the capacity of fea-\nture selection and combination of Gradient Boosting Decision\nTree (GBDT) for learning feature interactions automatically\n[Cheng et al., 2014]. However, GBDT is hard to be trained in\nparallel and can only exploit a small fraction of possible fea-\nture interactions, which limit its performance and application\nin large-scale scenarios.\nFactorization machine (FM) [Rendle, 2010] assigns a k-\ndimensional learnable embedding vector vi to each feature\ni, which enables the model to explore the effectiveness of\ncombining features in a ﬂexible manner as\nf FM\nΘ (x) = θ0 +\nm\nX\ni=1\nxiθi +\nm\nX\ni=1\nm\nX\nj=i+1\nxixjv⊤\ni vj ,\n(3)\nwhere Θ = (θ, v). It is intuitive to see that if the combin-\ning feature (i, j) is positively (negatively) correlated with\nP(y = 1), then the embedding inner product v⊤\ni vj will be\nlearned as positive (negative) automatically. Further exten-\nsions of FM have been proposed. The most notable extension\nis Field-aware FM (FFM) [Juan et al., 2016], which assigns\nmultiple independent embeddings for a feature to explicitly\nmodel feature interactions with different ﬁelds. Despite the\nsigniﬁcant improvement over FM, FFM suffers from heavy\nparameter settings. Besides, Gradient Boosting FM (GBFM)\n[Cheng et al., 2014], Higher-Order FM (HOFM) [Blondel et\nal., 2016], and Field-weighted FM (FwFM) [Pan et al., 2018]\nare also proposed to improve FM. By directly enumerating all\nthe possible feature interactions, FM based models reduce the\nhuman involvement in feature engineering.\nFrom 2015, the success of deep learning started to beneﬁt\nCTR estimation performance via transferring the classic archi-\ntectures or developing new ones. The universal approximation\nproperty of neural networks [Cybenko, 1989] and deep learn-\ning programming libraries with GPU computing stack make\nit possible to train deep neural network models to capture\nhigh-order feature interaction patterns and achieve better per-\nformance in CTR estimation. With the vector representation of\neach sparse (or categorical) feature, the instance dense vector\ncan be built by concatenating these vectors, which can be sim-\nply fed into a multi-layer perceptron (MLP) with a sigmoid\noutput. In industry, such an architecture is called DNN or\nsparse neural network (SNN). Zhang et al. [2016] studied the\nparameter initialization of DNN for CTR estimation and found\nthat the embedding vectors initialized by pre-training via a\nfactorization machine (FNN) or a stacked auto-encoder can\nimprove the prediction performance of DNN. Wide & Deep\nnetwork [Cheng et al., 2016] is one of the earliest published\ndeep models on CTR estimation, which adds the logit value of\nLR (wide part) and DNN (deep part) before feeding into the\nﬁnal sigmoid function as\nf W&D\nΘ\n(x) = θ0 +\nm\nX\ni=1\nxiθi + MLPφ([v1, v2, . . . , vm]) , (4)\nwhere Θ = (θ, v, φ). Notice that manually designed cross\nfeatures can also be included in the feature vector. Thus the\nDNN part can be regarded as learning a residual signal of LR\nto further approach the label. Moreover, DeepCross [Shan et\nal., 2016] extends residual network [He et al., 2016] for per-\nforming automatic feature interactions learning in an implicit\nway. Such early deep CTR models alleviate human efforts in\nfeature engineering by incorporating simple MLPs.\nAlthough these early MLP-based deep models do achieve\nperformance improvement, it may take a considerable effort to\ntrain a good model. Qu et al. [2018] pointed out the insensitive\ngradient issue of DNN to capture discrete feature interactions\nand empirically showed that DNN with various sizes cannot\nwell ﬁt a POLY2 function. Rendle et al. [2020] found that it is\nmore difﬁcult for an MLP to effectively learn the high-order\ncombining feature patterns in CTR estimation tasks than a dot\nproduct in FM.\nAbove ﬁndings suggest the explicit design of feature in-\nteraction learning in deep architectures as will be discussed\nin Section 3. Later we will introduce the deep user behavior\nmodels in Section 4 and the automated architecture search\nmethods in Section 5. One can see that (i) feature interaction\nlearning mainly focuses on efﬁcient pattern mining within an\ninstance, (ii) user behavior modeling explores the dependency\nbetween multiple instances of a user, and (iii) automated ar-\nchitecture search methods aim to design above two kinds of\ndeep models in a hands-free manner, which form the structure\nof the remaining part of this paper.\n3\nFeature Interaction\nAs shown in Figure 3, the input multi-ﬁeld categorical data\nis often represented as high-dimensional one-hot vectors. An\nembedding layer is usually employed to compress these one-\nhot features into low-dimensional real-value vectors. Feature\n...\nStudent Beijing\nT-shirt\nNike\n0\n0\n1 ...\n0\n0\n1 ...\n1\n0\n0 ...\n0\n0\n1\nRaw Input\nOne-hot Vector\n...\n...\n...\n...\n...\n...\n...\nEmbedding Layer\nFeature Interaction Layer\nPredicted CTR\nDeep Network\n...\nStudent Beijing\nT-shirt\nNike\n0\n0\n1 ...\n0\n0\n1 ...\n1\n0\n0 ...\n0\n0\n1\nRaw Input\nOne-hot Vector\n...\n...\n...\n...\n...\n...\n...\nEmbedding Layer\nFeature Interaction Layer\nPredicted CTR\nDeep Network\n(a) Single Tower Network\n(b) Dual Tower Network\nFigure 3: Illustration of single tower (left) and dual tower (right) deep CTR architectures.\ninteraction modeling, which indicates the combination rela-\ntionships of multiple features, is the key to build a predictive\nmodel. With the great power of feature representation learning,\nDNN is potential in automatic feature interactions modeling.\nHowever, as pointed in Section 2, it is difﬁcult for a single\nDNN to learn high-order feature interactions effectively. As\na result, many works that incorporate explicitly feature inter-\nactions with DNN are proposed in recent years. The feature\ninteraction learning of existing models can be formulated as:\nf FIL\nΘ (x) = fψ([v1, v2, . . . , vm])+MLPφ([v1, . . . , vm]) , (5)\nwhere Θ = (φ, ψ) is the parameter set, fψ is the explicitly\nfeature interaction learning function and MLPφ is an optional\nfunction that only dual tower models have. We ﬁrst give a\ndetailed description of some existing operators for explicitly\nfeature interaction learning, then we discuss the role of the\nDNN part in model architecture.\n3.1\nFeature Interaction Operators\nThere are multiple operators developed for explicitly feature\ninteraction learning, which can be mainly classiﬁed into three\ncategories, i.e., product operators, convolutional operators and\nattention operators.\nProduct Operators. To explicitly model the second-order\nfeature interactions, Product-based Neural Network (PNN)\n[Qu et al., 2016] introduces a product layer between the em-\nbedding layer and deep network to model the feature interac-\ntions between different ﬁelds. Specially, two variants are pro-\nposed, namely Inner Product-based Neural Network (IPNN)\nand Outer Product-based Neural Network (OPNN), which\nutilize an inner product layer or an outer product layer for\nfeature interaction modeling, respectively. Figure 4(a) depicts\nthe architecture of the inner product operation. Experimen-\ntal results show that adding product operations yields better\nconvergence and improves the network’s prediction ability.\nNeural Factorization machines (NFM) [He and Chua, 2017]\nﬁnds that existing deep structures have difﬁculty in network\ntraining. Thus it proposes a bi-interaction layer between the\nembedding layer and deep network to model pairwise feature\ninteractions. By combining the linear feature interactions of\nthe bi-interaction layer and the non-linear feature interactions\nof DNN, NFM is much easier to train than existing deep mod-\nels. As the ﬁnal function learned by DNN can be arbitrary,\nthere is no theoretical guarantee on whether each order of\nfeature interaction is modeled or not. Cross Network [Wang et\nal., 2017] solves this problem using a cross network to apply\nfeature crossing at each layer explicitly. Thus the order in-\ncreases at each layer and is determined by layer depth. Notice\nthat Cross Network utilizes vector multiplication for feature\ncrossing, which has a limited expressiveness, Cross Network\nV2 [Wang et al., 2020] further replaces the cross vector in\nCross Network into a cross matrix to make it more expressive.\nInspired by Cross Network, Compressed Interaction Network\n(CIN) [Lian et al., 2018], a more effective model, is proposed\nto capture the feature interactions of bounded orders. To learn\nfeature interactions better, Kernel Product Neural Network\n(KPNN) and Product-network In Network (PIN) utilize a ker-\nnel product and a micro-net architecture respectively to model\nfeature interactions [Qu et al., 2018].\nConvolutional Operators. Besides product operation for fea-\nture interaction modeling, Convolutional Neural Networks\n(CNN) and Graph Convolutional Networks (GCN) are also\nexplored for feature interaction modeling. Convolutional Click\nPrediction Model (CCPM) [Liu et al., 2015] performs convolu-\ntion, pooling and non-linear activation repeatedly to generate\narbitrary-order feature interactions, which is shown in Figure\n4(b). However, CCPM can only learn part of feature inter-\nactions between adjacent features since it is sensitive to the\nﬁeld order. Feature Generation Convolutional Neural Network\n(FGCNN) [Liu et al., 2019a] improves CCPM by introducing\na recombination layer to model non-adjacent features. It then\ncombines the new features generated by CNN with raw fea-\ntures for ﬁnal prediction. FGCNN veriﬁes that the generated\nfeatures by CNN can augment the original feature space and\nreduce the optimization difﬁculties of existing deep structures.\nFeature Interaction Graph Neural Networks (FiGNN) [Li et\nal., 2019] argues that existing deep models that use a simple\nunstructured combination of feature ﬁelds have a limited ca-\n...\n...\n...\n...\n...\nEmbeddings\n(a) Product Operator\n...\n...\n...\n...\n...\nFeature Interactions\n...\n...\n...\n...\n...\n(c) Attention Operator\nAttention Net\nFeature Interactions\nConvolution\nFeature Maps\nPooling/Fully \nConnected Layer\n(b) Convolutional Operator\nEmbeddings\nFigure 4: Illustration of three typical interaction operators.\npacity to model sophisticated feature interactions. Inspired by\nthe success of GCN, it treats the multi-ﬁeld categorical data as\na fully connected graph where different ﬁelds as graph nodes\nand interactions between different ﬁelds as graph edges, and\nthen models feature interactions via graph propagation.\nAttention Operators. Some works have tried to exploit the\nattention mechanism for feature interaction modeling in CTR\nestimation. Attentional Factorization Machines (AFM) [Xiao\net al., 2017] improves FM by utilizing an additional attention\nnetwork to enable feature interactions to contribute differently\nto the prediction. The pairwise interacted vector of two fea-\ntures is fed into the attention network to calculate an attention\nscore for this interacted vector at each step, as shown in Fig-\nure 4(c). Then a softmax function is applied to normalize\nthese attention scores. Finally, these attention scores are mul-\ntiplied with the interacted vectors for ﬁnal prediction. Feature\nImportance and Bilinear feature Interaction NETwork (FiB-\niNET) [Huang et al., 2019] extends the Squeeze-Excitation\nnetwork (SENET) [Hu et al., 2018] which has made great\nsuccess in computer vision to learn the feature importance. It\nthen uses a bilinear function to learn the feature interactions.\nInspired by the success of self-attention [Vaswani et al., 2017]\nin natural language processing, AutoInt [Song et al., 2019] uti-\nlizes a multi-head self-attentive neural network with residual\nconnections to explicitly model the feature interactions with\ndifferent orders and can also provide explainable prediction via\nattention weights. Interpretable CTR prediction model with\nHierarchical Attention (InterHAt) [Li et al., 2020] combines\na transformer network with multiple attentional aggregation\nlayers for feature interactions learning, which achieves high\ntraining efﬁciency with comparable performance and can ex-\nplain the importance of different feature interactions.\n3.2\nThe Role of DNN in Deep CTR Models\nThe feature interaction operators in Section 3.1 can be utilized\nto construct single tower networks or dual tower networks\naccording to the relative positions of DNN and feature inter-\naction layer in the architecture. Single tower models place\nfeature interaction and deep network successively in the ar-\nchitecture, as illustrated in Fig. 3(a). These models could\neffectively capture the high-order feature interactions but the\nsignals of low-order feature interactions may vanish in the fol-\nlowing DNN. To better capture low-order feature interactions,\ndual tower networks are proposed. As shown in Fig. 3(b), it\nplaces the feature interaction layer and DNN parallelly. The\nfeature interaction layer is responsible for explicitly capturing\nlow-order interactions while the high-order ones are captured\nimplicitly by the DNN. Outputs of both modules are used to\ngenerate the ﬁnal prediction. Single tower models like NFM\n[He and Chua, 2017] and PIN [Qu et al., 2018] have a stronger\nmodeling capacity with a more sophisticated network struc-\nture. But they usually suffer from the bad local minima and\nheavily rely on parameter initialization. Wide & Deep [Cheng\net al., 2016], DeepFM [Guo et al., 2017], DCN [Wang et al.,\n2017], DCN V2 [Wang et al., 2020], xDeepFM [Lian et al.,\n2018] and Autoint+ [Song et al., 2019] are dual tower models.\nThe DNN part can always be regarded as a supplementary to\nlearn the residual signal of the feature interaction layer to ap-\nproach the label, which yields stable training and the improved\nperformance.\n4\nUser Behavior Modeling\nUser behaviors contain crucial patterns of user interests. Mod-\neling user behaviors is becoming an essential topic of the CTR\nestimation task in recent years. In this section, we will give an\nelaborate review of the user behavior modeling literature.\ntime\nStudent\n...\nT-shirt\ny\nBehavioral Feature\nBehavior Modeling\nUser Behavior Modeling\nprediction\nloss\nfetch behaviors\nuser behaviors\n...\naggregation\n1) most recent consecutive behaviors\n2) retrieve the most relevant behaviors\nFigure 5: General framework of user behavior modeling.\nThe user behaviors are usually organized as a multi-value\ncategorical feature, each value is a behavior identiﬁer. The be-\nhavioral feature is usually sequential, sorted with the temporal\norder. The item IDs and the corresponding item features (if\nany) form a behavior sequence. The overall framework of user\nbehavior modeling is illustrated in Fig. 5. In general, it could\nbe summarized as Eq. (6), where xm = {b1, b2, . . . , bT } is\nthe multi-value behavioral feature, T is the sequence length,\nΘ = (φ, ψ) is the parameter set and Uψ is the user behavior\nmodeling function. Then the logit value is calculated as\nf UBM\nΘ\n(x) = MLPφ([v1, v2, . . . , vm−1, Uψ(xm)]) .\n(6)\nThe key point is to design an effective Uψ function to learn a\ndense representation of the behavioral feature. After getting\nthe behavioral representation, it is aggregated (concatenation)\nwith other feature embeddings, and the aggregated features\nwill be fed into an MLP to get the ﬁnal prediction. The user\nbehavior modeling algorithms could be categorized into three\nclasses, i.e., attention based models, memory network based\nmodels and retrieval based models. There are also many user\nbehavior modeling methods [Fang et al., 2020] in sequential\nrecommendation task, but they are not included in this section\nbecause our paper concentrates on CTR estimation task.\n4.1\nAttention based Models\nDeep Interest Network (DIN) [Zhou et al., 2018] is the ﬁrst\nmodel that introduces attention mechanism in user behavior\nmodeling of CTR estimation task. It utilizes an attention\nmechanism to attribute different historical behaviors with dif-\nferent weights according to the relatedness with the target\nitem. As the user’s latent interests evolve over time dynam-\nically, it is vital to capture the temporal patterns inside the\nbehavior sequence, which motivates the design of Deep In-\nterest Evolution Network (DIEN) [Zhou et al., 2019]. DIEN\nis a two-layer GRU (Gate Recurrent Unit) structure with the\nattention mechanism. The ﬁrst GRU layer is responsible for\ninterest extraction, and the second layer is used for modeling\nthe interest evolving. AUGRU (GRU with attentional update\ngate) is proposed for the interest evolving layer, which uses the\nattention weights to update the states of GRU. DIEN further\nincorporates an auxiliary loss that uses consecutive behavior\nto supervise the learning of hidden states at each step, making\nhidden states expressive enough to represent latent interest.\nAs self-attention [Vaswani et al., 2017] has shown promising\nperformance gain in many applications, it is also used in user\nbehavior modeling. Behavior Sequence Transformer (BST)\n[Chen et al., 2019a] directly incorporates multi-head atten-\ntion layer as the sequential feature extractor to capture the\ndependency among behaviors. Deep Session Interest Network\n(DSIN) [Feng et al., 2019] splits user behaviors into multiple\nsessions. It uses self-attention with bias encoding to get accu-\nrate behavior representation inside each session and employs\nBi-LSTM to capture the sequential relationship among histor-\nical sessions. The above models have shown the efﬁcacy of\nattention mechanism in user behavior modeling.\n4.2\nMemory Network based Models\nAs there is a massive amount of user behavior data accumu-\nlated on large e-commerce platforms [Ren et al., 2019], it is\nof more importance to handle the very long behavior sequence\nand mine the patterns in more distant user history. Models\nlike DIN or DIEN is limited by time complexity, and they\nare not feasible in modeling very long sequence. Thus mem-\nory network based models are proposed with proper system\ndesigns. Ren et al. [2019] proposed Hierarchical Periodic\nMemory Network (HPMN) and the lifelong sequential mod-\neling framework. It uses a lifelong personalized memory to\nstore the user interest representations, and HPMN is respon-\nsible for updating the memory states incrementally. HPMN\nis a multi-layer GRU architecture with different update fre-\nquencies at each layer. The upper layers update less frequently\nthan the lower layers, thus HPMN could capture long-term\nyet multi-scale temporal patterns. With similar motivations,\nPi et al. [2019] designed a User Interest Center (UIC) module\nwhich decouples the time-consuming user behavior model-\ning procedure with the real-time prediction process. At the\ninference time, the user behavioral representation is directly\nobtained from UIC, which is calculated ofﬂine. UIC proposes\na Multi-channel user Interest Memory Network (MIMN) to\ncalculate the user behavioral representations as the user inter-\nests are usually multi-channeled. Compared with HPMN, UIC\nprovides a more systematic solution to the long sequence mod-\neling from the industrial perspective, while HPMN is the ﬁrst\nto incorporate the memory concept to handle long sequence\nmodeling in the CTR estimation task.\n4.3\nRetrieval based Models\nDespite the tremendous success of the current user behavior\nmodeling solutions, there are still some drawbacks. All the\nabove models use the most recent consecutive behaviors and\ntry to incorporate complex architectures to capture sequential\npatterns in longer sequences, which bring heavy burdens on\nthe system overhead. User Behavior Retrieval (UBR4CTR)\n[Qin et al., 2020] proposes a new framework that uses search\nengine techniques to retrieve the most relevant behaviors from\nthe entire user behavior sequence. Instead of using a large\nnumber of consecutive behaviors, only a small number of the\nmost relevant behaviors will be retrieved at each inference time.\nThis retrieval approach not only solves the time complexity\nproblem of handling very long sequences but also alleviates\nthe noisy signals in long consecutive sequences because only\nthe most relevant behaviors are input to the model.\nUBR4CTR uses search engine techniques to retrieve the top-\nk relevant historical behaviors. The features of the inference\ntarget are selected to generate the query. The query generation\nprocedure is parametric. Thus, it makes the retrieval process\nlearnable, and it is optimized using REINFORCE algorithm.\nSearch-based Interest Model (SIM) [Qi et al., 2020] is another\nretrieval based model which proposes hard search and soft\nsearch approaches. For the hard search, it uses predeﬁned IDs\nsuch as user ID and category ID to build the index. As for\nthe soft search, SIM retrieves relevant behaviors using their\nembeddings by local sensitive hashing (LSH).\nMost of the user behavior models mentioned in this section\nare deployed in real-world online systems. These user behav-\nior modeling algorithms and systems contribute signiﬁcantly\nto the development of CTR estimation techniques in terms of\nboth economic returns and academic values.\n5\nAutomated Architecture Search\nAs some automatically designed networks achieve comparable\nperformance to the human-developed models in computer\nvision, researchers of recommender systems also propose to\ndevise automated methods for deep CTR architecture design.\nDepending on the different parts that such methods focus\non, they can be categorized into three classes: (1) automatic\ndesign of ﬂexible and adaptable embedding dimensions for\nindividual features; (2) automatic selection or generation of\neffective feature interactions; (3) automatic design of network\narchitectures.\n5.1\nEmbedding Dimension Search\nEmbedding representation is a key factor to deep CTR models,\nas it is the major component of model parameters, which natu-\nrally has a high impact on the model performance. Therefore,\nsome works optimize the embeddings by searching embedding\ndimensions for different features adaptively and automatically.\nNIS [Joglekar et al., 2020] and ESAPN [Liu et al., 2020c]\nperform reinforcement learning (RL) to search for mixed fea-\nture embedding dimensions automatically. NIS [Joglekar et\nal., 2020] ﬁrst divides the universal dimension space into sev-\neral blocks predeﬁned by human experts and then applies RL\nto generate decision sequences on selecting such dimension\nblocks for different features. The reward function of the RL\nalgorithm considers both the accuracy of the recommendation\nmodel and memory cost. ESAPN [Liu et al., 2020c] also pre-\ndeﬁnes a set of candidate embedding dimensions for different\nfeatures. A policy network is utilized for each ﬁeld to search\nthe embedding sizes for different features dynamically. The\npolicy network takes the frequency and current embedding\nsize of a feature as the input, and outputs a decision of whether\nto enlarge the current dimension of this feature.\nThe above two methods select feature dimensions from\na small discrete set of candidate dimensions, whereas\nDNIS [Cheng et al., 2020] and PEP [Liu et al., 2021] make\nthe candidate dimensions continuous.\nMore speciﬁcally,\nDNIS [Cheng et al., 2020] introduces a binary indicator matrix\nfor each feature block (features are grouped into feature blocks\naccording to their frequency to reduce the search space) to\nindicate the existence of the corresponding dimension. A soft\nselection layer is then proposed to relax the search space of the\nbinary indicator matrix to be continuous. Then a predeﬁned\nthreshold value is utilized to ﬁlter unimportant dimensions in\nthe soft selection layer. However, this threshold value is hard\nto tune in practice and thus may lead to suboptimal model\nperformance. Motivated by this observation, PEP [Liu et al.,\n2021] prunes embedding parameters where the threshold val-\nues can be adaptively learned from data.\nThe works mentioned above perform a hard selection in the\nsearch space, i.e., only selecting one embedding dimension\nfor each feature. On the contrary, soft selection strategy is pro-\nposed in AutoEmb [Zhao et al., 2020c] and AutoDim [Zhao\net al., 2020b]. The soft selection strategy sums over the em-\nbeddings of the candidate dimensions with learnable weights,\nwhere such weights are trained via differentiable search algo-\nrithms (e.g., DARTS). AutoDim allocates different embedding\nsizes for different feature ﬁelds, while AutoEmb searches\ndifferent embedding sizes for individual features.\n5.2\nFeature Interaction Search\nAs stated earlier, feature interactions modeling is crucial in\ndeep CTR models, thus automatically devising effective fea-\nture interactions is of high potential value.\nAutoFIS [Liu et al., 2020b] automatically identiﬁes and then\nselects important feature interactions for factorization models.\nAutoFIS enumerates all the feature interactions and utilizes\na set of architecture parameters to indicate the importance of\nindividual feature interactions. These architecture parameters\nare optimized by gradient descent (inspired by DARTS [Liu et\nal., 2019b]) with GRDA optimizer [Chao and Cheng, 2019] to\nget a sparse solution, such that those unimportant feature inter-\nactions (i.e., with architecture parameters of zero values) will\nbe ﬁltered away automatically. However, AutoFIS restricts the\ninteraction function to be the inner product.\nAs stated in PIN [Qu et al., 2018], a micro-network could\nbe more effective than the inner product as the interaction\nfunction. Inspired by this observation, besides identifying im-\nportant feature interactions, determining suitable interaction\nfunctions is also beneﬁcial, which motivates SIF [Yao et al.,\n2020] and AutoFeature [Khawar et al., 2020]. SIF [Yao et\nal., 2020] automatically devises suitable interaction functions\nfor matrix factorization (considering only user and item iden-\ntiﬁer, without any other features) in different datasets. The\nsearch space of the interaction functions consists of micro\nspace and macro space, where micro space refers to element-\nwise MLP, and macro space includes ﬁve linear algebra opera-\ntions. AutoFeature [Khawar et al., 2020] proposes to utilize a\nmicro-network with different architectures to model feature\ninteractions between each pair of ﬁelds. The architecture of\neach micro-network is searched from a search space with ﬁve\npre-deﬁned operations. The search process is implemented by\nan evolutionary algorithm with the Naive Bayes tree.\nHowever, neither AutoFIS nor AutoFeature can model high-\norder feature interactions, as they need to enumerate all the\npossible feature interactions beforehand. To avoid such inef-\nﬁcient enumeration, AutoGroup [Liu et al., 2020a] proposes\nto generate some groups of features, such that their interac-\ntions of a given order are effective. Each feature has an initial\nprobability of 0.5 to be selected in any of such groups. Such\nprobability values are parameterized and learned by Gumbel-\nSoftmax trick [Jang et al., 2017], from the supervised signal\nof the CTR estimation task.\nAll the works mentioned above select or generate effec-\ntive feature interactions for feature ﬁelds. BP-FIS [Chen et\nal., 2019b] is the ﬁrst work to identify important feature in-\nteractions for different users by Bayesian variable selection,\nproviding ﬁner granularity of feature interaction selection than\nprevious works. Speciﬁcally, a Bayesian generative model is\nbuilt with a derived lower bound, which can be optimized by\nan efﬁcient stochastic gradient variational Bayes method to\nlearn the parameters.\n5.3\nWhole Architecture Search\nThe last category of works studies searching the whole archi-\ntecture of deep CTR models.\nAutoCTR [Song et al., 2020] designs a two-level hierar-\nchical search space by abstracting representative structures in\nstate-of-the-art CTR estimation architectures (namely, MLP,\ndot product and factorization machine) into virtual blocks,\nwhere such blocks are connected as a directed acyclic graph\n(DAG) in a similar way as in DARTS. The outer space consists\nof the connections among blocks, while the inner space is\ncomposed of the detailed hyperparameters in different blocks.\nThe evolutionary algorithm is utilized as the search algorithm,\nwith some optimizations on the efﬁciency.\nAMER [Zhao et al., 2020a] searches architectures to extract\nsequential representation from sequential features (i.e., user\nbehaviors) and explores different feature interactions among\nnon-sequential features automatically and simultaneously. On\none hand, the search space of behavior modeling includes nor-\nmalization, activation, and layer selection (e.g., convolution,\nrecurrent, pooling, attention layers). Several architectures are\nrandomly sampled for evaluation on the validation set, and\nonly the top ones are selected for further evaluation. On the\nother hand, effective feature interactions are searched by pro-\ngressively increasing the order of interactions. The candidate\nfeature interactions are initialized by single non-sequential\nfeatures and updated by interacting with all possible non-\nsequential features and keeping the best interactions with the\nhighest validation performance.\n6\nSummary and Future Prospects\nThis paper provides a brief review of the development of\ndeep learning models for CTR estimation tasks. With feature-\ninteraction operators, deep models are more capable of cap-\nturing high-order combining feature patterns in the multi-ﬁeld\ncategorical data and yield better prediction performance. With\nattention mechanism, memory networks or retrieval-based ap-\nproaches, the representation of a user behavior history can\nbe effectively learned, which further improves the prediction\nperformance. As the deep architectures for CTR estimation\ncan be various, some trials have been performed to automati-\ncally search the deep architectures to make the whole process\nhands-free. All the above progresses are achieved within the\nlast ﬁve years.\nDespite the fast development and great success of deep\nlearning on CTR estimation, we still note that there are some\nsigniﬁcant challenges in this ﬁeld to be addressed.\n• Deep learning theory. There are plenty of works on de-\nsigning CTR models, but seldom works have focused on\nthe deep learning theory of these models, including the\nsample complexity analysis, learning behaviors of feature\ninteraction layers, gradient analysis etc.\n• Representation learning. Just like other discrete data such\nas text and graph, it is reasonable to expect that the repre-\nsentation learning (or called pretraining) of the multi-ﬁeld\ncategorical data will largely improve the CTR prediction\nperformance. However, to our knowledge, there has been\nrare existing work on this perspective so far.\n• Learning over multi-modal data. In modern information\nsystems, there are various multimedia elements of items\nor browsing environments. Thus it is of high potential\nto design CTR estimation models to deal with the feature\ninteractions over multi-modal data.\n• Strategical data processing. The recent progress of strate-\ngies of user historical behavior fetching, e.g., user behavior\nretrieval and ranking, suggests the great potential of explor-\ning the combining way of data processing with deep model\ndesign. Making such data processing learnable would be of\nhigh value for research.\nAcknowledgement\nRuiming Tang is the corresponding author. Weinan Zhang\nis supported by “New Generation of AI 2030” Major Project\n(2018AAA0100900) and National Natural Science Foundation\nof China (61772333, 61632017). The work is also sponsored\nby Huawei Innovation Research Program.\nReferences\n[Agarwal et al., 2014] Deepak Agarwal, Bo Long, Jonathan\nTraupman, Doris Xin, and Liang Zhang. Laser: A scalable\nresponse prediction platform for online advertising. In\nWSDM, pages 173–182, 2014.\n[Blondel et al., 2016] Mathieu Blondel,\nAkinori Fujino,\nNaonori Ueda, and Masakazu Ishihata. Higher-order fac-\ntorization machines. In NIPS, 2016.\n[Chang et al., 2010] Yin-Wen Chang, Cho-Jui Hsieh, Kai-\nWei Chang, Michael Ringgaard, and Chih-Jen Lin. Training\nand testing low-degree polynomial data mappings via linear\nsvm. Journal of Machine Learning Research, 11(4), 2010.\n[Chao and Cheng, 2019] Shih-Kang Chao and Guang Cheng.\nA generalization of regularized dual averaging and its dy-\nnamics. CoRR, abs/1909.10072, 2019.\n[Chen et al., 2019a] Qiwei Chen, Huan Zhao, Wei Li, Pipei\nHuang, and Wenwu Ou. Behavior sequence transformer for\ne-commerce recommendation in alibaba. In 1st DLP-KDD\nWorkshop, pages 1–4, 2019.\n[Chen et al., 2019b] Yifan Chen, Pengjie Ren, Yang Wang,\nand Maarten de Rijke. Bayesian personalized feature in-\nteraction selection for factorization machines. In SIGIR,\npages 665–674, 2019.\n[Cheng et al., 2014] Chen Cheng, Fen Xia, Tong Zhang, Ir-\nwin King, and Michael R Lyu. Gradient boosting factoriza-\ntion machines. In RecSys, pages 265–272. ACM, 2014.\n[Cheng et al., 2016] Heng-Tze Cheng, Levent Koc, Jeremiah\nHarmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,\nGlen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir,\net al. Wide & deep learning for recommender systems. In\n1st DLRS workshop, pages 7–10, 2016.\n[Cheng et al., 2020] Weiyu Cheng, Yanyan Shen, and Lin-\npeng Huang. Differentiable neural input search for recom-\nmender systems. CoRR, abs/2006.04466, 2020.\n[Cybenko, 1989] George Cybenko. Approximation by super-\npositions of a sigmoidal function. Mathematics of control,\nsignals and systems, 2(4):303–314, 1989.\n[Fan and Poole, 2006] Haiyan Fan and Marshall Scott Poole.\nWhat is personalization? perspectives on the design and\nimplementation of personalization in information systems.\nJournal of Organizational Computing and Electronic Com-\nmerce, 16(3-4):179–202, 2006.\n[Fang et al., 2020] Hui Fang, Danning Zhang, Yiheng Shu,\nand Guibing Guo. Deep learning for sequential recom-\nmendation: Algorithms, inﬂuential factors, and evaluations.\nTOIS, 39(1):1–42, 2020.\n[Feng et al., 2019] Yufei Feng, Fuyu Lv, Weichen Shen,\nMenghan Wang, Fei Sun, Yu Zhu, and Keping Yang. Deep\nsession interest network for click-through rate prediction.\nIn IJCAI, 2019.\n[Guo et al., 2017] Huifeng Guo, Ruiming Tang, Yunming Ye,\nZhenguo Li, and Xiuqiang He. Deepfm: a factorization-\nmachine based neural network for ctr prediction. IJCAI,\n2017.\n[He and Chua, 2017] Xiangnan He and Tat-Seng Chua. Neu-\nral factorization machines for sparse predictive analytics.\nIn SIGIR, pages 355–364, 2017.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, pages 770–778, 2016.\n[Hu et al., 2018] Jie Hu, Li Shen, and Gang Sun. Squeeze-\nand-excitation networks. In CVPR, pages 7132–7141, 2018.\n[Huang et al., 2019] Tongwen Huang, Zhiqi Zhang, and Jun-\nlin Zhang. Fibinet: combining feature importance and\nbilinear feature interaction for click-through rate prediction.\nIn RecSys, pages 169–177, 2019.\n[Jang et al., 2017] Eric Jang, Shixiang Gu, and Ben Poole.\nCategorical reparameterization with gumbel-softmax. In\nICLR, 2017.\n[Joglekar et al., 2020] Manas R. Joglekar, Cong Li, Mei\nChen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav\nKhaitan, Jiahui Liu, and Quoc V. Le. Neural input search\nfor large scale recommendation models. In KDD, 2020.\n[Juan et al., 2016] Yuchin Juan, Yong Zhuang, Wei-Sheng\nChin, and Chih-Jen Lin. Field-aware factorization machines\nfor ctr prediction. In RecSys, pages 43–50, 2016.\n[Khawar et al., 2020] Farhan Khawar, Xu Hang, Ruiming\nTang, Bin Liu, Zhenguo Li, and Xiuqiang He. Autofeature:\nSearching for feature interactions and their architectures\nfor click-through rate prediction. In CIKM, 2020.\n[Li et al., 2019] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang,\nand Liang Wang. Fi-gnn: Modeling feature interactions via\ngraph neural networks for ctr prediction. In CIKM, 2019.\n[Li et al., 2020] Zeyu Li, Wei Cheng, Yang Chen, Haifeng\nChen, and Wei Wang.\nInterpretable click-through rate\nprediction through hierarchical attention. In WSDM, 2020.\n[Lian et al., 2018] Jianxun Lian, Xiaohuan Zhou, Fuzheng\nZhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun.\nxdeepfm: Combining explicit and implicit feature interac-\ntions for recommender systems. In KDD, 2018.\n[Liu et al., 2015] Qiang Liu, Feng Yu, Shu Wu, and Liang\nWang. A convolutional click prediction model. In CIKM,\n2015.\n[Liu et al., 2019a] Bin Liu, Ruiming Tang, Yingzhi Chen,\nJinkai Yu, Huifeng Guo, and Yuzhou Zhang. Feature gen-\neration by convolutional neural network for click-through\nrate prediction. In WWW, pages 1119–1129, 2019.\n[Liu et al., 2019b] Hanxiao Liu, Karen Simonyan, and Yim-\ning Yang. DARTS: differentiable architecture search. In\nICLR, 2019.\n[Liu et al., 2020a] Bin Liu, Niannan Xue, Huifeng Guo,\nRuiming Tang, Stefanos Zafeiriou, Xiuqiang He, and Zhen-\nguo Li. Autogroup: Automatic feature grouping for mod-\nelling explicit high-order feature interactions in ctr predic-\ntion. In SIGIR, pages 199–208, 2020.\n[Liu et al., 2020b] Bin Liu, Chenxu Zhu, Guilin Li, Weinan\nZhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo\nLi, and Yong Yu. Autoﬁs: Automatic feature interaction\nselection in factorization models for click-through rate pre-\ndiction. In KDD, pages 2636–2645, 2020.\n[Liu et al., 2020c] Haochen Liu, Xiangyu Zhao, Chong\nWang, Xiaobing Liu, and Jiliang Tang. Automated em-\nbedding size search in deep recommender systems. In\nSIGIR, pages 2307–2316, 2020.\n[Liu et al., 2021] Siyi Liu, Chen Gao, Yihong Chen, Depeng\nJin, and Yong Li. Learnable embedding sizes for recom-\nmender systems. CoRR, abs/2101.07577, 2021.\n[Pan et al., 2018] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz,\nWenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. Field-\nweighted factorization machines for click-through rate pre-\ndiction in display advertising. In WWW, 2018.\n[Pi et al., 2019] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang\nZhu, and Kun Gai. Practice on long sequential user behavior\nmodeling for click-through rate prediction. In KDD, 2019.\n[Qi et al., 2020] Pi Qi, Xiaoqiang Zhu, Guorui Zhou, Yujing\nZhang, Zhe Wang, Lejian Ren, Ying Fan, and Kun Gai.\nSearch-based user interest modeling with lifelong sequen-\ntial behavior data for click-through rate prediction. In KDD,\n2020.\n[Qin et al., 2020] Jiarui Qin, W. Zhang, Xin Wu, Jiarui Jin,\nYuchen Fang, and Y. Yu. User behavior retrieval for click-\nthrough rate prediction. In SIGIR, 2020.\n[Qu et al., 2016] Yanru Qu, Han Cai, Kan Ren, Weinan\nZhang, Yong Yu, Ying Wen, and Jun Wang. Product-based\nneural networks for user response prediction. In ICDM,\npages 1149–1154, 2016.\n[Qu et al., 2018] Yanru Qu, Bohui Fang, Weinan Zhang,\nRuiming Tang, Minzhe Niu, Huifeng Guo, Yong Yu, and\nXiuqiang He. Product-based neural networks for user re-\nsponse prediction over multi-ﬁeld categorical data. TOIS,\n37(1):1–35, 2018.\n[Ren et al., 2019] Kan Ren, Jiarui Qin, Yuchen Fang, Weinan\nZhang, Lei Zheng, Weijie Bian, Guorui Zhou, Jian Xu,\nYong Yu, Xiaoqiang Zhu, et al. Lifelong sequential mod-\neling with personalized memorization for user response\nprediction. In SIGIR, 2019.\n[Rendle et al., 2020] Steffen\nRendle,\nWalid\nKrichene,\nLi Zhang, and John Anderson.\nNeural collaborative\nﬁltering vs. matrix factorization revisited. In RecSys, 2020.\n[Rendle, 2010] Steffen Rendle. Factorization machines. In\nICDM, 2010.\n[Richardson et al., 2007] Matthew Richardson, Ewa Domi-\nnowska, and Robert Ragno. Predicting clicks: estimating\nthe click-through rate for new ads. In WWW, 2007.\n[Shan et al., 2016] Ying Shan, T Ryan Hoens, Jian Jiao, Hai-\njing Wang, Dong Yu, and JC Mao. Deep crossing: Web-\nscale modeling without manually crafted combinatorial\nfeatures. In KDD, pages 255–262, 2016.\n[Song et al., 2019] Weiping Song, Chence Shi, Zhiping Xiao,\nZhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang.\nAutoint: Automatic feature interaction learning via self-\nattentive neural networks. In CIKM, 2019.\n[Song et al., 2020] Qingquan Song, Dehua Cheng, Hanning\nZhou, Jiyan Yang, Yuandong Tian, and Xia Hu. Towards\nautomated neural interaction discovery for click-through\nrate prediction. In KDD, pages 945–955, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998–6008, 2017.\n[Wang et al., 2017] Ruoxi Wang, Bin Fu, Gang Fu, and Min-\ngliang Wang. Deep & cross network for ad click predictions.\nIn ADKDD, pages 1–7. 2017.\n[Wang et al., 2020] Ruoxi Wang, Rakesh Shivanna, Derek Z\nCheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H\nChi. Dcn-m: Improved deep & cross network for feature\ncross learning in web-scale learning to rank systems. arXiv\npreprint arXiv:2008.13535, 2020.\n[Xiao et al., 2017] Jun Xiao, Hao Ye, Xiangnan He, Han-\nwang Zhang, Fei Wu, and Tat-Seng Chua. Attentional\nfactorization machines: Learning the weight of feature in-\nteractions via attention networks. IJCAI, 2017.\n[Yao et al., 2020] Quanming Yao, Xiangning Chen, James T.\nKwok, Yong Li, and Cho-Jui Hsieh. Efﬁcient neural inter-\naction function search for collaborative ﬁltering. In WWW,\npages 1660–1670, 2020.\n[Zhang et al., 2016] Weinan Zhang, Tianming Du, and Jun\nWang. Deep learning over multi-ﬁeld categorical data: A\ncase study on user response prediction. ECIR, 2016.\n[Zhao et al., 2020a] Pengyu Zhao, Kecheng Xiao, Yuanxing\nZhang, Kaigui Bian, and Wei Yan. AMER: automatic\nbehavior modeling and interaction exploration in recom-\nmender system. CoRR, abs/2006.05933, 2020.\n[Zhao et al., 2020b] Xiangyu Zhao, Haochen Liu, Hui Liu,\nJiliang Tang, Weiwei Guo, Jun Shi, Sida Wang, Huiji Gao,\nand Bo Long. Memory-efﬁcient embedding for recommen-\ndations. CoRR, abs/2006.14827, 2020.\n[Zhao et al., 2020c] Xiangyu Zhao, Chong Wang, Ming\nChen, Xudong Zheng, Xiaobing Liu, and Jiliang Tang.\nAutoemb: Automated embedding dimensionality search\nin streaming recommendations. CoRR, abs/2002.11252,\n2020.\n[Zhou et al., 2018] Guorui Zhou, Xiaoqiang Zhu, Chenru\nSong, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi\nJin, Han Li, and Kun Gai. Deep interest network for click-\nthrough rate prediction. In KDD, 2018.\n[Zhou et al., 2019] Guorui Zhou, Na Mou, Ying Fan, Qi Pi,\nWeijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai.\nDeep interest evolution network for click-through rate pre-\ndiction. In AAAI, volume 33, pages 5941–5948, 2019.\n",
  "categories": [
    "cs.IR",
    "cs.LG"
  ],
  "published": "2021-04-21",
  "updated": "2021-04-21"
}