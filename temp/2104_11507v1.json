{
  "id": "http://arxiv.org/abs/2104.11507v1",
  "title": "DeepfakeUCL: Deepfake Detection via Unsupervised Contrastive Learning",
  "authors": [
    "Sheldon Fung",
    "Xuequan Lu",
    "Chao Zhang",
    "Chang-Tsun Li"
  ],
  "abstract": "Face deepfake detection has seen impressive results recently. Nearly all\nexisting deep learning techniques for face deepfake detection are fully\nsupervised and require labels during training. In this paper, we design a novel\ndeepfake detection method via unsupervised contrastive learning. We first\ngenerate two different transformed versions of an image and feed them into two\nsequential sub-networks, i.e., an encoder and a projection head. The\nunsupervised training is achieved by maximizing the correspondence degree of\nthe outputs of the projection head. To evaluate the detection performance of\nour unsupervised method, we further use the unsupervised features to train an\nefficient linear classification network. Extensive experiments show that our\nunsupervised learning method enables comparable detection performance to\nstate-of-the-art supervised techniques, in both the intra- and inter-dataset\nsettings. We also conduct ablation studies for our method.",
  "text": "DeepfakeUCL: Deepfake Detection via\nUnsupervised Contrastive Learning\n1st Sheldon Fung\nSchool of Information Technology\nDeakin University\nGeelong, Australia\nsheldonvon@outlook.com\n2nd Xuequan Lu*\nSchool of Information Technology\nDeakin University\nGeelong, Australia\nxuequan.lu@deakin.edu.au\n3rd Chao Zhang\nFaculty of Engineering\nUniversity of Fukui\nFukui, Japan\nzhang@u-fukui.ac.jp\n4th Chang-Tsun Li\nSchool of Information Technology\nDeakin University\nGeelong, Australia\nchangtsun.li@deakin.edu.au\n©2021 IEEE Copyright.\nAbstract—Face deepfake detection has seen impressive results\nrecently. Nearly all existing deep learning techniques for face\ndeepfake detection are fully supervised and require labels during\ntraining. In this paper, we design a novel deepfake detection\nmethod via unsupervised contrastive learning. We ﬁrst gener-\nate two different transformed versions of an image and feed\nthem into two sequential sub-networks, i.e., an encoder and\na projection head. The unsupervised training is achieved by\nmaximizing the correspondence degree of the outputs of the\nprojection head. To evaluate the detection performance of our\nunsupervised method, we further use the unsupervised features\nto train an efﬁcient linear classiﬁcation network. Extensive\nexperiments show that our unsupervised learning method enables\ncomparable detection performance to state-of-the-art supervised\ntechniques, in both the intra- and inter-dataset settings. We also\nconduct ablation studies for our method.\nI. INTRODUCTION\nRealistic face synthesis or manipulation has led to a rapid\nincrease of deepfake images and videos which pose security\nand privacy threats to our society. In light of this, researchers\nhave proposed countermeasures against face deepfake, i.e., the\ndetection or recognition of face deepfake. Note that there are\ndifferent deepfakes, and in this work, we simply use deepfake\nfor face deepfake unless stated otherwise.\nIt is challenging to detect manipulated faces due to the\nevolution of deep learning techniques for face manipulation.\nEarly deepfake detection methods focused on the use of cues\nleft by the face manipulation techniques, for example, Li et\nal. [9] determined whether or not an image is manipulated by\nanalyzing the frequency of eye blinking. These methods are\nfragile and tend to fail if those cues are removed or missing.\nDeep learning methods were therefore proposed to detect face\nmanipulation, and have achieved promising outcomes due to\nsupervised learning. With technical evolution, those deepfake\ncontents are becoming increasingly realistic and the well-\ntrained models will thus become obsolete and require retrain-\ning on new data. Nevertheless, supervised learning has to “see”\n*Corresponding author.\nthe labels, and labeling is time-consuming and tedious. To\nour knowledge, unsupervised learning for deepfake detection\nhas rarely been studied so far. It is more challenging than\nsupervised learning since labels are not known during training.\nIn this paper, we design a novel unsupervised learning\napproach for face manipulation detection. The core idea is\nto ﬁrstly generate two transformed versions of a face image\nusing two different transformations, and then maximize their\nagreement after going through an encoder network and a\nprojection head network. This is inspired by a contrastive\nframework [30]. The model trained without supervision will\nbe used to produce features (i.e., output of the encoder) to be\ntaken as the input of a linear classiﬁer network for deepfake\nevaluation. We use the output of the encoder since it has more\neffective features than the output of the projection head (see\nSection IV-D).\nExtensive experiments on three publicly available datasets\nvalidate our unsupervised contrastive learning method. It\nyields comparable performance to state-of-the-art supervised\nlearning methods and non-deep-learning methods in terms of\nboth the intra-dataset and inter-dataset settings. We also per-\nform ablation studies for our method. Our main contributions\nare as follows.\n• We propose an unsupervised contrastive learning ap-\nproach for deepfake detection.\n• We conduct a variety of experiments to test our method\nand compare it with state-of-the-art deepfake detection\ntechniques.\nThe rest of the paper is organized as follows. Section II\nreviews previous research work on face manipulation and\ndeepfake detection. Section III presents the proposed approach.\nWe explain the experimental results and analyze the results in\nSection IV. Section V concludes this work.\nII. RELATED WORK\nIn this section, we will ﬁrstly review previous face synthesis\nor manipulation methods. Then we will look back upon recent\narXiv:2104.11507v1  [cs.CV]  23 Apr 2021\nworks on the detection of face forgery.\nFace manipulation approaches. There have been methods\nfor face image manipulation before the emergence of deep\nlearning based methods. Dale et al. [2] introduced a face-\nswapping method based on a 3D multi-linear model for face\ntracking and warping. A similar approach was presented by\nJustus et al. [3], which tracks the facial expression using\na dense photometric consistency measure. Zhmoginov et al.\n[4] introduced deep learning techniques into the task of\nface manipulation. Concretely, they used neural networks to\neffectively invert low-dimensional face embeddings while pro-\nducing realistically looking consistent images, which was later\nfurther implemented into a famous phone app called FaceAPP\n[5]. Since then, many deep learning based face manipulation\nmethods arise. For example, Thies et al. [6] introduced a\nnew image synthesis approach that takes advantage of the\ntraditional graphics pipeline and learnable components, which\nwas later used to generate manipulated faces in FaceForen-\nsics++ [7]. An even more intriguing and similar technique\nwas introduced by Wei et al. [8], which aimed at modifying\na face image according to a given attribute value. Shao et\nal. [37] proposed to explicitly transfer expressions by directly\nmapping two unpaired images to two synthesized images with\nswapped expressions.\nDetection methods. The privacy and security impact of\nface manipulation on individuals as well as society drives\nresearchers to develop detection techniques for face manipula-\ntion. Methods for detecting manipulated faces can be classiﬁed\ninto two types. One is to utilize the visual cues of the\nimperfections of face manipulation methods, e.g., detecting the\nfrequency of eye blinking [9], the abnormality of head pose\n[10] and other visual features [11]. However, these methods\nare usually vulnerable and can easily become invalid once the\nmanipulation methods are reﬁned by removing those cues. For\nexample, Li et al. [12] put forwarded a detection method based\non the face warping artifacts. This method achieved the AUC\nof 80.1% when testing on UADFV (Published by [12]) but\ndropped signiﬁcantly to 56.9% when confronting with Celeb-\nDF [13]. In addition to focusing on the visual cues, Li et al.\n[14] introduced a detection method for morphed face images\nbased on PRNU [15]–[17].\nThe other category of methods resorts to the deep learning\napproaches. Darius et al. [18] presented two networks with a\nfew layers to focus on the mesoscopic properties of images.\nZhou et al. [19] proposed a method using the two-stream\nGoogLeNet InceptionV3 model [20] for the task and achieved\nstate-of-the-art performance. Hsu et al. [21] also resorted\nto a similar solution and used a two-stream DenseNet with\ncontrastive loss. A triplet loss [23] integrated with three-\nstream Xception [22] also reached state-of-the-art results,\nas presented by Feng et al. [24]. R¨ossler et al. [25] also\npresented high performance forensics results using Xception\n[22]. The robustness of the Xception [22] network allows it\nto be a strong candidate for the backbone network in some\nother methods, e.g., the approaches introduced by Dang et\nal. [26] and Tolosana et al. [27]. The rapid development of\nface manipulation also enables videos to be the manipulated\ncontent. G¨uera et al. [28] used recurrent neural network (RNN)\nin conjunction with convolutional neural network (CNN) to\ndetect videos containing manipulated faces. While [28] used\ntheir private dataset, Sabir et al. [29] used a similar approach\nto train and evaluate on FaceForensics++ [25].\nIII. OUR APPROACH\nOur approach takes three steps: data preprocessing, unsuper-\nvised training, and follow-up classiﬁcation. We ﬁrst preprocess\nan image into a face-centered image to fully utilize the data\nof the face area. We then perform unsupervised contrastive\ntraining with the paired transformed images of each prepro-\ncessed image and the contrastive loss. This step enables the\nunsupervised learning of separable features, which can be used\nto further train a classiﬁer for the evaluation (step 3). Figure\n1 shows the overview of our method.\nA. Data Preprocessing\nMost available face manipulation datasets are stored in the\nformat of video. Moreover, faces in those videos usually take\nup a small proportion of area and thus might raise the difﬁculty\nwhen learning features. Therefore, following [25], we use Dlib\n[31] for locating the bounding box of the face in each frame\nand crop the image with the maximum edge of the bounding\nbox, generating a squared image with the face at the center\n(see Figure 1).\nB. Unsupervised Contrastive Learning\nAt ﬁrst, we transform the image of interest into its two\ndifferent versions via two different augmentations (or trans-\nformations). Then we encourage the backbone network to\nlearn the features of one image by maximizing the similarity\nbetween the two augmented versions of this image.\nTransformed versions generation. To synthesize two dif-\nferent versions of each image in the dataset, we process the\nimage of interest with several augmentation methods: random\ncrop, random ﬂip, random color jittering, and grayscale (Figure\n2).\nWe denote the image as x and the two augmented views\nas xi and xj, which will be fed into the encoder network for\nunsupervised contrastive learning.\nEncoder network. Many advanced convolutional neural\nnetworks (CNN) have proven to be feasible for detecting\nmanipulated faces in recent research, e.g., InceptionV3 [19],\nDenseNet [21], VGG16 [26], Xception [25]. These CNNs are\ntrained in a fully-supervised manner. Among these CNNs, we\nemploy Xception as our encoder, which shows the capability\nof learning contrastive features [24]. We denote the output of\nthe Xception network (denote as XNet(·)) as fi = XNet(xi).\nProjection head network. To raise the efﬁciency of per-\nforming the loss function, a stack of linear layers (projection\nhead, denoted as g(·)) is concatenated with the Xception\nnetwork. We thus obtain\nzi = g(fi) = W2σ(W1(fi)),\n(1)\nFig. 1. Overview of our proposed method. It ﬁrst preprocesses an image into a face-centred image (Section III-A) which is then transformed into two versions\nusing augmentation. The pair of the transformed images are further fed sequentially through the network consisting of an encoder (Xception as backbone)\nand projection head (a stack of linear layers) for maximizing agreement. After unsupervised learning, a simple linear classiﬁer is trained on the features to\nclassify images of interest as either “real” or “fake”. Left: unsupervised contrastive learning, right: linear classiﬁcation.\nFig. 2.\nThe basic augmentation we used when training the unsupervised\nlearning network. Note that all augmentations are applied randomly.\nwhere W1 and W2 are two linear layers with 2, 048 and\n64 neurons, respectively. A ReLu layer σ is placed between\nthem. Notice that g(·) is only used while training the feature\nextraction network.\nLoss function. We use the cosine similarity to measure the\nsimilarity of two samples.\nsim(zi, zj) =\nzi · zj\nmax(∥zi∥2 · ∥zj∥2, ϵ),\n(2)\nwhere ϵ is set to 1e−8. Suppose we have N samples. With the\npair augmentation, the mini-batch size will become 2N. With\nregard to the loss, we simply apply cross-entropy after softmax\nregression to the similarity within a mini-batch of samples.\nLoss(i, j) = −log σsoftmax(zi, zj) −log σsoftmax(zj, zi),\n(3)\nσsoftmax(zi, zj) =\nexp(sim(zi, zj)\nτ\n)\nP2N\nk=1 exp(sim(zi, zk)\nτ\n)\n,\n(4)\nWhere τ is the temperature of the contrastive loss.\nC. Classiﬁcation\nFor evaluation, a variety of classiﬁers, such as SVM and\nBayes classiﬁer, can be trained on the features extracted\nby the unsupervised contrastive learning step. In this work,\nwe simply choose a linear classiﬁcation network, with the\nstructure illustrated in Table I. It should be noted that the\noutput of the encoder is used as input for supervised classiﬁer\ntraining. This is because the output of the projection head\ninvolves less effective information than the output of the\nencoder, which is evidenced in Section IV-D.\nTABLE I\nTHE STRUCTURE OF THE CLASSIFICATION NETWORK.\nLayer Number\nClassiﬁcation network\n1\nFully connected layer, neurons = 2048\n2\nFully connected layer, neurons = 4096\n3\nFully connected layer, neurons = 2048\n4\nFully connected layer, neurons = 256\n5\nLeaky ReLu, negative slope = 0.4\n6\nSoftMax Layer\nIV. EXPERIMENTAL RESULTS\nIn this section, we will ﬁrst describe the used datasets and\nthen explain the experimental settings. Then we will show the\nexperimental results and compare the proposed method with\nthe state-of-the-art techniques. In the end, we will provide\nablation studies.\nA. Datasets\nWe use three commonly used datasets for evaluation: Face-\nForensics++ [25], UADFV [12] and Celeb-DF [13]. Each\ndataset has its own characteristics. FaceForensics++ contains\nvideos involving faces manipulated by a variety of methods\n(i.e., DeepFake [32], Face2Face [33], FaceSwap [34] and\nNeuralTexture [6]). It has a total of 3,700 videos, including\n1,000 pristine videos and 2,700 manipulated videos. UADFV\nis a relatively small yet commonly-used dataset. It consists of\n98 videos, including 49 pristine videos and 49 manipulated\nvideos. Celeb-DF is a large face forgery dataset containing\nfaces manipulated with algorithms that are able to circumvent\nthe common artifacts such as temporal ﬂickering frames and\ncolor inconsistency present in the other two datasets. It con-\ntains 6,529 videos, involving 890 pristine videos and 5,639\nmanipulated videos.\nFor each dataset, we ﬁrst perform data preprocessing as\nillustrated in Section III-A and extract a maximum of 400\nimages for each video. Note that some videos in the dataset\ncontain frames less than 400, and in this case, we extract all\nthe frames for such videos. We randomly select 15% of those\nimages to form the test set and the rest are used as the training\nset, as suggested by Feng et al. [24].\nTABLE II\nIMAGE NUMBERS IN THE SPLIT SETS USED IN OUR EXPERIMENTS.\nDatasets\nTrain (real)\nTrain (fake)\nTest (real)\nTest (fake)\nFaceForensics++\n115556\n108935\n20393\n20473\nUADFV\n10100\n9761\n1783\n1723\nCeleb-DF\n172187\n165884\n30386\n29259\nTable II shows the details of the datasets we used in our\nexperiments. Notice that the size of UADFV is less than 10%\nof FaceForensics++ and Celeb-DF, which might affect the\nperformance in the cross-dataset setting when the network is\ntrained on UADFV.\nFig. 3.\nFace images after data preprocessing (face locating, cropping and\nresizing) from three datasets (FaceForensics++, UADFV and Celeb-DF).\nWe show some of the images in three datasets after prepro-\ncessing in Figure 3. We observe that most images from those\nthree datasets share similar visual quality (image resolution,\nbrightness, etc.). However, it is obvious for human eyes that\nthe fake face images in FaceForensics++ are distinguishable\nbecause of the color inconsistency and the unnatural facial\nfeatures.\nB. Experimental Settings\nOur framework is implemented on PyTorch with a desktop\nPC equipped with an Intel Core i9-9820X CPU (3.30GHz,\n48GB memory) and a GeForce RTX 2080Ti GPU (11GB\nmemory, CUDA 10.0).\nFor training the networks, we resort to two-step learning\nillustrated in Figure 1.\n• For training the unsupervised network, the learning rate\nis set to 5e−4 with the scheduler of step size 6 and 50%\ndescending rate. The batch size is a key factor and is set\nto 40 for most experiments unless otherwise speciﬁed.\nThe temperature parameter τ is set to 0.5 throughout all\nexperiments.\n• For the classiﬁcation task, the learning rate is set to 3e-1\nwith the scheduler of step size 400 and 80% descending\nrate. The batch size is set to 6,000.\nWe employ SGD as an optimizer for both networks (unsuper-\nvised learning and classiﬁcation learning) and train them for\n20 epochs and 5,000 epochs, respectively.\nC. Comparisons\nIn this section, we provide intra-dataset and cross-dataset\nresults, which allow us to compare our method with the state-\nof-the-art methods.\nTABLE III\nAUC(%) ON FACEFORENSICS++ (FF++), UADFV AND CELEB-DF (SEE\nSUBSECTION III-A). BEST RESULTS IN INTRA-DATASET SETTING ARE\nUNDERLINED, AND BEST RESULTS FOR THE CROSS-DATASET SETTING ARE\nIN BOLD.\nMethods\nTrain data\nFF++\nUADFV\nCeleb-DF\nTwo-stream [19]\nPrivate\n70.1\n85.1\n53.8\nMeso4 [18]\nPrivate\n84.7\n84.3\n54.8\nMesoInception4 [18]\nPrivate\n83.0\n82.1\n53.6\nVA-MLP [11]\nPrivate\n66.4\n70.2\n55.0\nVA-LogReg [11]\nPrivate\n78.0\n54.0\n55.1\nMulti-task [35]\nFF\n76.3\n65.8\n54.3\nXception [25]\nFF++\n99.7\n80.4\n48.2\nCapsule [36]\nFF++\n96.6\n61.3\n57.5\nXception+Tri. [24]\nFF++\n99.9\n74.3\n61.7\nXception [13]\nUADFV\n-\n96.8\n52.2\nXception+Reg. [26]\nUADFV\n-\n98.4\n57.1\nXception+Tri. [24]\nUADFV\n61.3\n99.9\n60.0\nHeadPose [10]\nUADFV\n47.3\n89.0\n54.6\nFWA [12]\nUADFV\n80.1\n97.4\n56.9\nXception+Tri. [24]\nCeleb-DF\n60.2\n88.9\n99.9\nDeepfakeUCL (Ours)\nFF++\n93.0\n67.5\n56.8\nDeepfakeUCL (Ours)\nUADFV\n56.2\n98.9\n64.8\nDeepfakeUCL (Ours)\nCeleb-DF\n58.9\n85.6\n90.5\nWe show the area under the curve (AUC) results in our\nexperiments in Table III. Despite that our method is unsu-\npervised, it still achieves fairly close performance and even\noutperforms some supervised learning methods. For results\ntrained on FaceForensics++, our method is 3.6% weaker than\nCapsule and approximately 6.8% weaker than Xception and\nXception+Tri. when testing on the same dataset. However,\nit outperforms Capsule and Xception by 6.2% and 8.2%\nwhen testing on UADFV and Celeb-DF, respectively. For\nresults with the networks trained on UADFV, except for\nXception+Tri., which is 1% stronger, our method outperforms\nall other methods when testing on the same dataset. It even\nachieves the top results when testing on Celeb-DF, reaching\n64.8% in AUC, which is 4.8% higher than the second-best\nmethod, Xception+Tri.. It also beats HeadPose by 8.9% when\ntesting on FaceForensics++, which is mere 47.3% in AUC.\nWhen training on the Celeb-DF dataset, there are insufﬁcient\nresults for comparison. However, we can observe that the re-\nsults of our method are quite close to Xception+Tri.. Although\nFig. 4. Receiver Operating Characteristic (ROC) curves of the models trained\nand tested on the same and different datasets. Note that, F, U, and C in the\nﬁgure represent FaceForensics++, UADFV, and Celeb-DF, respectively. The\nﬁrst pair of brackets specify the datasets that the model is trained and tested\non. For instance, (F, U) indicates that the model is trained on FaceForensics++\nand tested on UADFV.\nour method is 9.4% weaker than Xception+Tri., it is merely\n1.3% and 3.3% weaker than Xception+Tri. when testing on\nFaceForensics++ and UADFV, respectively.\nWe also report the ROC curves of the corresponding AUCs\n(in Table III) in Figure 4, where the 45-degree curve is the\nbaseline which is achieved by a random classiﬁer. Therefore,\nthe closer to the baseline the curve is, the less reliable the\nmodel is. This also reveals that the inter-dataset setting is more\nchallenging than the intra-dataset setting such that the trained\nmodels perform better at the intra-dataset setting.\nD. Ablation Study\nIn this section, we provide three ablation studies: unsu-\npervised versus supervised contrastive learning, data augmen-\ntation and encoder versus projection head. The ﬁrst study\nis to demonstrate if our unsupervised contrastive learning\nmechanism is effective. The second study is to test the effects\nof different combinations of data augmentation schemes. The\nthird ablation study is to show that the output of the encoder\nis better than that of the projection head for the downstream\nclassiﬁcation evaluation.\nUnsupervised contrastive learning versus supervised\nlearning. We utilize the Xception network [22] as the back-\nbone of our unsupervised contrastive learning framework,\nwhich learns feature points by comparing two different views\nof a single image. Thus, it is useful to compare the supervised\nXception networks with our method.\nFig. 5. The data of the above three plots are based on Table III. From left to right, the networks in each plot are trained on FaceForensics++ (denoted as\nFF++), UADFV and Celeb-DF, respectively. In each plot, from left to right, the results in each section are tested on FaceForensics++ (also denoted as FF++),\nUADFV, and Celeb-DF, respectively.\nTABLE IV\nAUC(%) TRAINED ON CELEB-DF AND TESTED ON FACEFORENSICS++ (FF++), UADFV AND CELEB-DF WITH DIFFERENT DATA AUGMENTATION\nMETHODS.\nAugmentation\nRandom Crop\nRandom Flip\nRandom Color Jittering\nRandom Gray Scale\nFF++\nUADFV\nCeleb-DF\n1\n✓\n×\n×\n×\n51.8\n65.8\n93.3\n2\n✓\n✓\n×\n×\n53.8\n64.8\n93.2\n3\n✓\n✓\n✓\n✓\n58.9\n85.6\n90.5\nIn Figure 5, we report the comparison of three cases by\ntaking Xception as backbone: original Xception (supervised),\nthe combination of Xception and triplet loss (supervised),\nand our unsupervised contrastive learning method (Deep-\nFakeUCL). Surprisingly, the results of our method outperform\nthe results by training Xception on UADFV, which are 2.1%\nand 12.6% higher when testing on UADFV and Celeb-DF,\nrespectively. Although our method is 6.7% and 12.9% weaker\nthan Xception when training on FaceForensics++ and testing\non FaceForensics++ and UADFV respectively, it boosts up to\n56.8% when testing on Celeb-DF, which is 8.6% higher than\nthe results trained by Xception. For the supervised contrastive\nlearning method, which is driven by the Triplet network, our\nmethod still outperforms it by 4.8%, reaching 64.8% when\ntraining on UADFV and testing on Celeb-DF. Notice that\nUADFV is a relatively small dataset while Celeb-DF is about\n17 times larger than UADFV (see Table II). Therefore, it is\nconsidered to be a more challenging dataset when used for\ntraining purposes.\nData augmentation choices. To generate two different\nviews of a single image, we utilize some common data\naugmentation methods to manipulate the images. However,\nwith different data augmentation combinations, we observe\nsome noticeably different outcomes. The results are shown\nin Table IV. In the experiments, we utilize random image\ncropping as the fundamental data augmentation, which does\nnot alter the content of the image but the position of the content\nin the image. Two other combinations are also used. Note that\nthose results are all trained on Celeb-DF with the conﬁguration\nillustrated in Section IV-B.\nWe can observe from the results that other than the test\nresults of Celeb-DF itself, the AUC generally increases with\nrespect to the complexity of the data augmentation. In other\nwords, the more complicated the data augmentation is, the\nmore robust the model is. When all four manipulation schemes\nare applied to the data augmentation process (i.e., Augmenta-\ntion 3), the results for FaceForensics++ raise dramatically to\nabout 59% from around 52%. Moreover, the result for UADFV\nboosts enormously up to 85.6%, which is over 20% higher than\nusing random cropping only. The result of testing on Celeb-DF\nitself drops by around 3% when four forms of manipulation are\napplied, whereas this drawback seems insigniﬁcant compared\nto the aforementioned improvements.\nEncoder versus projection head. We further evaluate the\nfeatures learned by the projection head, by taking its output\nfeatures as input for the linear classiﬁcation. From Figure 6,\nwe observe the classiﬁcation results using the features output\nby the encoder are remarkably better than using the output of\nthe features by the projection head, suggesting that the encoder\nlearns more useful unsupervised features.\nFig. 6. Encoder versus projection head. Networks are trained on the UADFV\ndataset.\nV. CONCLUSION\nWe have presented an unsupervised contrastive learning\nmethod for deepfake detection. Compared to most existing\ndeepfake detection techniques which are fully supervised, our\nmethod learns separable features in an unsupervised manner.\nExperiments demonstrate the effectiveness of our method and\nshow the comparable performance of our method to state-of-\nthe-art deepfake detection techniques in both intra- and inter-\ndataset settings. We also conducted ablation studies for the\nproposed method. As future work, it would be interesting to\nincorporate temporal information within the proposed frame-\nwork to achieve more robust results.\nREFERENCES\n[1] Hsu, C.C., Zhuang, Y.X. and Lee, C.Y., 2020. Deep fake image detection\nbased on pairwise learning. Applied Sciences, 10(1), p.370.\n[2] Dale, K., Sunkavalli, K., Johnson, M.K., Vlasic, D., Matusik, W. and\nPﬁster, H., 2011, December. Video face replacement. In Proceedings of\nthe 2011 SIGGRAPH Asia Conference (pp. 1-10).\n[3] Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C. and Nießner, M.,\n2016. Face2face: Real-time face capture and reenactment of rgb videos.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition (pp. 2387-2395).\n[4] Zhmoginov, A. and Sandler, M., 2016. Inverting face embeddings with\nconvolutional neural networks. arXiv preprint arXiv:1606.04189.\n[5] Faceapp, https://www.faceapp.com/, (Accessed on 26/11/2020)\n[6] Thies, J., Zollh¨ofer, M. and Nießner, M., 2019. Deferred neural ren-\ndering: Image synthesis using neural textures. ACM Transactions on\nGraphics (TOG), 38(4), pp.1-12.\n[7] Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J. and\nNießner, M., 2019. Faceforensics++: Learning to detect manipulated\nfacial images. In Proceedings of the IEEE International Conference on\nComputer Vision (pp. 1-11).\n[8] Shen, W. and Liu, R., 2017. Learning residual images for face attribute\nmanipulation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition (pp. 4030-4038).\n[9] Li, Y., Chang, M.C. and Lyu, S., 2018, December. In ictu oculi:\nExposing ai created fake videos by detecting eye blinking. In 2018 IEEE\nInternational Workshop on Information Forensics and Security (WIFS)\n(pp. 1-7). IEEE.\n[10] Yang, X., Li, Y. and Lyu, S., 2019, May. Exposing deep fakes using\ninconsistent head poses. In ICASSP 2019-2019 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) (pp.\n8261-8265). IEEE.\n[11] Matern, F., Riess, C. and Stamminger, M., 2019, January. Exploiting\nvisual artifacts to expose deepfakes and face manipulations. In 2019\nIEEE Winter Applications of Computer Vision Workshops (WACVW)\n(pp. 83-92). IEEE.\n[12] Yuezun Li and Siwei Lyu, 2019, June. Exposing deepfake videos by\ndetecting face warping artifacts. In 2019 IEEE Conference on Computer\nVision and Pattern Recognition Work- shops (CVPRW).\n[13] Li, Y., Yang, X., Sun, P., Qi, H. and Lyu, S., 2020. Celeb-DF: A Large-\nscale Challenging Dataset for DeepFake Forensics. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(pp. 3207-3216).\n[14] Debiasi, L., Scherhag, U., Rathgeb, C., Uhl, A. and Busch, C., 2018,\nJune. PRNU-based detection of morphed face images. In 2018 Interna-\ntional Workshop on Biometrics and Forensics (IWBF) (pp. 1-7). IEEE.\n[15] Chen, M., Fridrich, J., Goljan, M. and Luk´as, J., 2008. Determining\nimage origin and integrity using sensor noise. IEEE Transactions on\ninformation forensics and security, 3(1), pp.74-90.\n[16] Li, C.T. and Li, Y., 2011. Color-decoupled photo response non-\nuniformity for digital image forensics. IEEE Transactions on Circuits\nand Systems for Video Technology, 22(2), pp.260-271.\n[17] Lin, X. and Li, C.T., 2020. PRNU-Based Content Forgery Localization\nAugmented With Image Segmentation. IEEE Access, 8, pp.222645-\n222659.\n[18] Afchar, D., Nozick, V., Yamagishi, J. and Echizen, I., 2018, December.\nMesonet: a compact facial video forgery detection network. In 2018\nIEEE International Workshop on Information Forensics and Security\n(WIFS) (pp. 1-7). IEEE.\n[19] Zhou, P., Han, X., Morariu, V.I. and Davis, L.S., 2017, July. Two-stream\nneural networks for tampered face detection. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition Workshops (CVPRW) (pp.\n1831-1839). IEEE.\n[20] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D.,\nErhan, D., Vanhoucke, V. and Rabinovich, A., 2015. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on computer vision\nand pattern recognition (pp. 1-9).\n[21] Hsu, C.C., Zhuang, Y.X. and Lee, C.Y., 2020. Deep fake image detection\nbased on pairwise learning. Applied Sciences, 10(1), p.370.\n[22] Chollet, F., 2017. Xception: Deep learning with depthwise separable\nconvolutions. In Proceedings of the IEEE conference on computer vision\nand pattern recognition (pp. 1251-1258).\n[23] Hoffer, E. and Ailon, N., 2015, October. Deep metric learning using\ntriplet network. In International Workshop on Similarity-Based Pattern\nRecognition (pp. 84-92). Springer, Cham.\n[24] Feng, D., Lu, X. and Lin, X., 2020, November. Deep Detection for\nFace Manipulation. In International Conference on Neural Information\nProcessing (pp. 316-323). Springer, Cham.\n[25] Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J. and\nNießner, M., 2019. Faceforensics++: Learning to detect manipulated\nfacial images. In Proceedings of the IEEE International Conference on\nComputer Vision (pp. 1-11).\n[26] Dang, H., Liu, F., Stehouwer, J., Liu, X. and Jain, A.K., 2020. On the\ndetection of digital face manipulation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 5781-\n5790).\n[27] Tolosana, R., Romero-Tapiador, S., Fierrez, J. and Vera-Rodriguez,\nR., 2020. DeepFakes Evolution: Analysis of Facial Regions and Fake\nDetection Performance. arXiv preprint arXiv:2004.07532.\n[28] G¨uera, D. and Delp, E.J., 2018, November. Deepfake video detection\nusing recurrent neural networks. In 2018 15th IEEE International Con-\nference on Advanced Video and Signal Based Surveillance (AVSS) (pp.\n1-6). IEEE.\n[29] Sabir, E., Cheng, J., Jaiswal, A., AbdAlmageed, W., Masi, I. and Natara-\njan, P., 2019. Recurrent convolutional strategies for face manipulation\ndetection in videos. Interfaces (GUI), 3(1).\n[30] Chen,\nT.,\nKornblith,\nS.,\nNorouzi,\nM.\nand\nHinton,\nG.,\n2020.\nA Simple Framework\nfor Contrastive\nLearning of\nVisual\nRep-\nresentations.\nProceedings\nof\nthe\n37th\nInternational\nConference\non Machine Learning, in PMLR 119:1597-1607 Available from\nhttp://proceedings.mlr.press/v119/chen20j.html.\n[31] King, D.E., 2009. Dlib-ml: A machine learning toolkit. The Journal of\nMachine Learning Research, 10, pp.1755-1758.\n[32] Deepfakes github, https://github.com/deepfakes/faceswap, (Accessed on\n1/08/2021)\n[33] Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C. and Nießner, M.,\n2016. Face2face: Real-time face capture and reenactment of rgb videos.\nIn Proceedings of the IEEE conference on computer vision and pattern\nrecognition (pp. 2387-2395).\n[34] Faceswap github, https://github.com/MarekKowalski/FaceSwap/, (Ac-\ncessed on 1/08/2021)\n[35] H. H. Nguyen, F. Fang and J. Yamagishi, 2019. Multi-task learning for\ndetecting and segmenting manipulated facial images and videos. CoRR\nabs/1906.06876.\n[36] Nguyen, H.H., Yamagishi, J. and Echizen, I., 2019, May. Capsule-\nforensics: Using capsule networks to detect forged images and videos.\nIn ICASSP 2019-2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) (pp. 2307-2311). IEEE.\n[37] Shao, Z., Zhu, H., Tang, J., Lu, X., Ma, L., 2019. Explicit facial\nexpression transfer via ﬁne-grained semantic representations. arXiv\npreprint arXiv:1909.02967.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2021-04-23",
  "updated": "2021-04-23"
}