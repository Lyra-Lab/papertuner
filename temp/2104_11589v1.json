{
  "id": "http://arxiv.org/abs/2104.11589v1",
  "title": "SBNet: Segmentation-based Network for Natural Language-based Vehicle Search",
  "authors": [
    "Sangrok Lee",
    "Taekang Woo",
    "Sang Hun Lee"
  ],
  "abstract": "Natural language-based vehicle retrieval is a task to find a target vehicle\nwithin a given image based on a natural language description as a query. This\ntechnology can be applied to various areas including police searching for a\nsuspect vehicle. However, it is challenging due to the ambiguity of language\ndescriptions and the difficulty of processing multi-modal data. To tackle this\nproblem, we propose a deep neural network called SBNet that performs natural\nlanguage-based segmentation for vehicle retrieval. We also propose two\ntask-specific modules to improve performance: a substitution module that helps\nfeatures from different domains to be embedded in the same space and a future\nprediction module that learns temporal information. SBnet has been trained\nusing the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three\nunique natural language descriptions each and tested 530 unique vehicle tracks\nand their corresponding query sets. SBNet achieved a significant improvement\nover the baseline in the natural language-based vehicle tracking track in the\nAI City Challenge 2021.",
  "text": "arXiv:2104.11589v1  [cs.CV]  22 Apr 2021\nSBNet: Segmentation-based Network for Natural Language-based Vehicle\nSearch\nSangrok Lee\nMODULABS\nsrl@modulabs.ai\nTaekang Woo\nNAVER Corporation\nt.k.woo@navercorp.com\nSang Hun Lee*\nKookmin University\nshlee@kookmin.ac.kr\nAbstract\nNatural language-based vehicle retrieval is a task to\nﬁnd a target vehicle within a given image based on a nat-\nural language description as a query.\nThis technology\ncan be applied to various areas including police search-\ning for a suspect vehicle. However, it is challenging due\nto the ambiguity of language descriptions and the difﬁ-\nculty of processing multi-modal data. To tackle this prob-\nlem, we propose a deep neural network called SBNet that\nperforms natural language-based segmentation for vehi-\ncle retrieval. We also propose two task-speciﬁc modules\nto improve performance: a substitution module that helps\nfeatures from different domains to be embedded in the\nsame space and a future prediction module that learns\ntemporal information. SBnet has been trained using the\nCityFlow-NL dataset that contains 2,498 tracks of vehicles\nwith three unique natural language descriptions each and\ntested 530 unique vehicle tracks and their corresponding\nquery sets. SBNet achieved a signiﬁcant improvement over\nthe baseline in the natural language-based vehicle track-\ning track in the AI City Challenge 2021.\nSource Code:\nhttps://github.com/lsrock1/nlp_search\n1. Introduction\nSearching for a vehicle in an image database with natu-\nral language (NL) descriptions is a challenging problem in\ncomputer vision[4, 15]. It can be widely applied in video\nsurveillance and trafﬁc analysis. Currently, many surveil-\nlance cameras are installed on roads and highways and gen-\nerate a huge amount of video data per second. Manually\nsearching for a criminal suspect vehicle in such video data\ncan be time consuming. Therefore, the development of an\nautomatic vehicle search function is vital and urgent.\nVehicle retrieval using image-based queries is called ve-\nhicle re-identiﬁcation in computer vision[11, 23, 13, 14].\nGiven a query image, the algorithm obtains afﬁnities be-\n*Corresponding author\nFigure 1. Examples of proposed SBNet output. With natural lan-\nguage and an image, it ﬁnds the corresponding area in the image\nand shows high activation in that area. The left column is the nat-\nural language description, the center column is the model’s output\nactivation mask, and the right column is the input image.\ntween the query and the vehicle images in the database and\nretrieves the most similar vehicles. However, in many crim-\ninal cases, there are no images of the suspected vehicle, only\nverbal descriptions. Therefore, this method’s ability to ﬁnd\nthe vehicle is signiﬁcantly limited in these cases.\nTo overcome this limitation, searching for vehicles with\nNL descriptions has been proposed.\nIt is not necessary\nto provide a vehicle photo as in the image-based query\nmethod. NL also can precisely describe the details of the ve-\nhicle appearance and does not require labelers to go through\nthe entire list of attributes.\nIn this work, the CityFlow-NL dataset [4] is used as\nthe benchmark dataset. According to our observations, the\nmain problems in this task are multi-modal question an-\nswering and relational reasoning. The task can be assumed\nas answering a visual question where the answer is yes or\nno.\nWe adopt the attention mechanism to deal with the\nmulti-modal dataset and the channel modulation method\nproposed in [19]. In addition to this module, we propose\nfuture prediction and substitution modules to improve per-\nformance. The future prediction module is for embedding\nvehicle movement information, while the substitution mod-\nule helps to describe two different types of domain data in\nthe same embedding space. Figure 1 illustrates our model’s\n1\nactivation results for given sets of image and NL descrip-\ntion. Our contributions are summarized below.\n• We propose a new segmentation-based network model\ncalled SBNet to perform NL-based vehicle retrieval.\n• We introduce two speciﬁc modules to improve perfor-\nmance: the future prediction and substitution modules.\n• Our proposed SBNet outperforms the current baseline\nmodel without post-processing.\n2. Related Work\nNatural language-based vehicle retrieval is a multi-target\nand multi-camera task that uses multi-modal data for im-\nages and NL descriptions. Other tasks like object track-\ning via descriptions[2, 3, 16], video retrieval[24, 7], video\nlocalization[5, 10, 22] shares some similar points with the\nNL-based vehicle retrieval.\nIn tracking tasks, they em-\nploy detection models and leverage language features from\nthe hidden states of the recurrent neural network(RNN)[2].\nLiu et al. [16] used RNN and convolutional neural net-\nwork(CNN) models to extract embedding features and also\nutilized the attention mechanism and dynamic ﬁlter genera-\ntion method. In video retrieval and localization, Zhang et al.\n[24] introduced a video embedding model and graph repre-\nsentation. Hendricks et al. [7] exploited the context network\nand distance loss to embed different modal data to the same\nfeature space. Gavrilyuk et al. [5] employed CNN architec-\ntures for the language and video backbone model to predict\nthe segmentation mask of the target object.\nNL-based person re-identiﬁcation (re-id) is also a very\nsimilar task with NL-based vehicle one [15] except for dif-\nferent targets just as personal re-id is very close to vehi-\ncle re-id. However, in NL-based person re-id, only cropped\nperson images are given, and thus understanding the back-\nground context is not required unlike in NL-based vehicle\nre-id. The spatial-temporal localization by NL description\ntask was ﬁrst introduced by Yamaguchi et al. [22, 4]. The\nActivityNet dataset is annotated with NL descriptions to\nfacilitate the training and evaluation of the proposed task.\nHowever, the temporal retrieval in [22] entails retrieving the\ntarget video clip from a set of video clips. On the contrary,\nthe goal of the vehicle retrieval task is to temporally localize\nthe target object within one sequence of video. Addition-\nally, the targets in the ActivityNet-NL take up most of the\nframe and cannot serve as a tracking benchmark.\nFeng et al. [4] extended the widely adopted CityFlow\nBenchmark with NL descriptions for vehicle targets and\nintroduce the CityFlow-NL Benchmark.\nThe CityFlow-\nNL contains more than 5,000 unique and precise NL de-\nscriptions of vehicle targets, making it the ﬁrst multi-target\nmulti-camera tracking with NL descriptions dataset. More-\nover, the dataset facilitates research at the intersection of\nmulti-object tracking, retrieval by NL descriptions, and\ntemporal localization of events. They focus on two foun-\ndational tasks: the Vehicle Retrieval by NL task and the Ve-\nhicle Tracking by NL task, which take advantage of the pro-\nposed CityFlow-NL benchmark and provide a strong basis\nfor future re- search on the multi-target multi-camera track-\ning by NL description task.\nWe introduced the CityFlow-NL Benchmark to develop\nSBNet, an NL-based vehicle retrieval network. Our ap-\nproach proposed in this paper is close to image localization\nmethods. However, we also consider the temporal infor-\nmation using the future prediction module and employ the\nco-attention mechanism and channel modulation to embed\nrelation features.\n3. Problem Deﬁnition\nIn the AI City Challenge, the goal of the NL-based ve-\nhicle retrieval task is to ﬁnd a vehicle described in English\nfrom images. Vehicles are presented as objects with bound-\ning boxes for each image in the scene video, not as cropped\nobjects. The image can have multiple vehicles, and the tar-\nget vehicle is speciﬁed by a bounding box. Three different\nNL descriptions are provided for one target vehicle.\nThe goal of this task is to segment the area where the\ncorresponding vehicle is located when the query is given.\nFor this task, we prepared a train dataset Dtrain and a set of\nthe ground truth segmentation labels G:\nDtrain = {(i1, n1\n1, n2\n1, n3\n1), (i2, n1\n2, n2\n2, n3\n2), . . . ,\n(it, n1\nt, n2\nt, n3\nt)},\n(1)\nG = {(g1, g2, g3, . . . , gt), }, g ∈Rh×w,\n(2)\nwhere Dtrain has t sets of data, each of which has\nimage(ij), and three NL description(nk\nj).\nG is ground\ntruth segmentation labels for each train data. We introduce\nbounding box segmentation, which assigns a class to every\npixel in the region of bounding box of the target in a given\nimage, and apply it throughout the work.\n4. Proposed Method\nTo tackle the NL-based vehicle retrieval task, we propose\na multi-modal localization model.\nIn the following sec-\ntions, we ﬁrst illustrate our overall architecture in Section\n4.1. Then, we describe the NL module (NLM) and image\nprocessing module (IPM) in Section 4.2 and Section 4.3, re-\nspectively. Finally, we introduce the multi-modal module,\nwhich handles the NL feature and image feature simultane-\nously and searches for the target vehicle using the language\nfeature, in Section 4.6.\n2\nFigure 2. Overall architecture of SBNet.\n4.1. Overall architecture\nAs illustrated in Fig. 2, our proposed method has three\nmain feature modules: the NLM, IPM, and multi-modal\nmodule. First, the raw image and NL descriptions are em-\nbeded using the IPM and NLM. We adopt ResNet50[6] for\nthe IPM and ELECTRA[1] for the NLM. ResNet50 is a\nwidely used backbone network for image processing, while\nELECTRA is a well-known pretrained model for NL. In\ngeneral, the pretrained NL models, like BERT and ELEC-\nTRA, outperform the previous ones and show reasonable\nperformance on subtasks that have few datasets.\nIn our\ntask, we adopt ELECTRA because ELECTRA outperforms\nthe transformer models that are not pretrained[21]. In ad-\ndition, the multi-modal module is one of the most impor-\ntant parts of our network, which was inspired by relational\nreasoning[19, 20]. To combine two types of information,\nimage and NL, co-attention and channel mixture are used.\nThe three modules, IPM, NLM, and multi-modal mod-\nule, are the backbone of our network. The other modules\nsuch as future prediction, substitution, and classiﬁcation\nmodules were introduced additionally for boosting perfor-\nmance. The details are described in the following sections.\n4.2. NL module\nThe description type is English NL. To embed this\ndata, we use the pretrained language model ELECTRA[1].\nELECTRA consists of transformer modules and a self-\nsupervised language representation learning model and has\ntwo parts: a generator and a discriminator.\nIn the self-\nsupervised phase, the masked language is fed to the model.\nThe generator predicts adequate words for the mask and\nthe discriminator distinguishes the generated words.\nIn\nour model, we use ELECTRA’s small discriminator as the\nNLM. The NLM process is as follows:\nFN = NLM(n), FN ∈Rl×e,\n(3)\nwhere FN is the embedded language feature, l is the length\nof the sentence, and e is the embedding dimension size. In\nour work, we set l to 30 in training and e to 2048. It is note\nthat ELECTRA small output channel is 256, therefore we\nuse another 256 × 2048 linear layer.\n4.3. Image processing module\nWe exploit ResNet50 as IPM to embed the scene image.\nThe image feature is leveraged for segmenting the vehicle\nbox area. To maintain resolution, we set the stride of the\nlast ﬁve stages of ResNet to 1 rather than 2.\nFI = IPM(i), FI ∈Rc×h∗×w∗,\n(4)\nwhere FI is an image feature, c is the embedding channel,\nand h∗and w∗are feature height and width respectively. c\nis 2048 in our work.\n4.4. Classiﬁcation module\nIn addition to the NLM and IPM, we attach simple clas-\nsiﬁcation modules to each module. With a rule-based al-\ngorithm, we can extract the vehicle color and type. Using\nthese labels, we can attach classiﬁcation modules to NLM\nand IPM modules. In ELECTRA, CLS token is used for\nthe classiﬁcation task. Following this, we use the CLS to-\nken position to classify color and type in the NLM. In the\nIPM, we pool a vector via the bounding box and classify the\ncolor and type with this vector.\nCn, Tn = Ln(FNcls),\n(5)\nFIcls = 1\n|b|\nh∗\nX\ni=0\nw∗\nX\nj=0\nFI ∗b, b ∈R1×h∗×w∗, FIcls ∈Rc,\n(6)\nCi, Ti = MLPi(FIcls),\n(7)\nwhere Cn and Tn are the predicted color and type from the\nNL feature. FNcls is the CLS token feature and Ln is one\nlinear layer. FIcls is the masked pooled vector in the spatial\ndimension. Via FIcls, we predict Ci and Ti, which are the\ncolor and type of the vehicle in the image. MLPi has two\nlinear layers with a ReLU function.\n3\n4.5. Substitution module\nThis task assumes a one-to-one relation between each\nimage and language set. In other words, an image and NL\ndescriptions are semantically same. Inspired by this idea,\nwe devise and add a substitution module. This module gen-\nerates an image feature from an NL feature and an NL fea-\nture from an image feature. In learning, we try to train these\ntwo features so that they are exchangeable. This module’s\ntarget features that should be generated are as follows:\nFIgt =\n1\nh∗× w∗\nh∗\nX\ni=0\nw∗\nX\nj=0\nFI(i)(j), FIgt ∈Rc,\n(8)\nFNgt = 1\nl\nl\nX\nk=0\nFN (k), FNgt ∈Re,\n(9)\nwhere FIgt is the target image feature that is the spatial\nmean of FI, FI(i)(j) is a vector on the i, j point in the\nspatial dimension, FNgt is the target NL feature that is the\nmean of the sentence feature, FN (k) is a vector of the kth\nword feature. The generated features are summarized as\nfollows:\nFIg = MLP(FN), FIg ∈Rc,\n(10)\nFNg = CNN(FI, b), FNg ∈Re,\n(11)\nThe MLP is two stacked linear layers with one leaky ReLU\nactivation function. The CNN consists of two convolu-\ntional layers with one ReLU activation function, and b is\nthe target vehicle’s bounding box.\n4.6. Multi-modal module\nMulti-modal module is for interpreting two different\ndomain features:\nimage and NL. First, we apply co-\nattention[8] with the image and NL feature. Using an atten-\ntion matrix, we enhance the NL and image feature informa-\ntion from a different domain. Using channel modulation[9],\nwe inject the relation information of the query description.\nFinally, we exploit the self-attention method for relation in-\nformation in the spatial domain. The attention between two\nfeatures FN and FI is deﬁned as follows:\nA = al(FI) ⊗an(FN),\n(12)\nA = σ( A\n√c), A ∈Rl×h∗w∗,\n(13)\nA is the resulting attention, al is a linear projection layer,\nand an is a 1 × 1 convolutional layer. ⊗is matrix multipli-\ncation. σ is a softmax function. We leverage A to enhance\nthe FN and FI. The enhancement is conducted as follows:\nFNe = FN + A ⊗FI,\n(14)\nFIe = FI + A ⊗FN\n(15)\nThe language description has relation information in the\nscene situation such as the following:\nWhite SUV keeps straight behind a line of ve-\nhicles.\nLeveraging relation reasoning method we modulate chan-\nnel activation to inject relation information from language\nfeature. The modulating process is as follows:\nFNm = FNe ∗MLP(FIe),\n(16)\nThe MLP has two linear layers, with ReLU activation as\nthe intermediate layer and the sigmoid function as the end\nlayer. The result of MLP is in the c× 1 × 1 dimension. We\ncompute element-wise multiplication.\n4.7. Mask prediction module\nFinally, to conduct main task, vehicle area segmen-\ntaion, we adopt multiple convolutional layers to segment\nthe mask. The segmentation layers are follows:\nM = G(FNm), M ∈R1×h∗×w∗,\n(17)\nwhere M is the mask and G is the convolutional layers (i.e.,\nthree non-linear layers and one convolution layer). The non-\nlinear layers have convolution, batch normalization, and\nReLU functions. All convolutional layers have a 3 × 3 ker-\nnel size and a 1 × 1 padding size. The output channels are\n1024, 512, 256, and 1 in order.\n4.8. Future prediction module\nThe image is part of a video. Thus, the description in-\nvolves temporal information such as the following:\nA gray small car is turning left.\nTo embed this vehicle action, we add the future frame pre-\ndiction module. This simply predicts the next frame using\nFI. The prediction is conducted as follows:\nU = B(FI), U ∈R3×h∗×w∗,\n(18)\nwhere U is the predicted future frame, and B is the multiple\nconvolutional layers.\n4.9. Probability computation\nTo perform the task, we compute the probability of\nmatching between the NL description and vehicle. As illus-\ntrated in Fig. 4.9, The probability has three parts: mask pre-\ndiction ratio (MPR), substitution similarity (SS), and color\nand type matching probability (CTM).\nMPR =\nP M ∗b\nP b\n,\n(19)\nSS = cs(FIgt, FIg) + cs(FNgt, FNg),\n(20)\nwhere cs is the cosine similarity, and\n4\nFigure 3. The probability computation process between the NL\ndescription and image.\nCT M = Ci[Cn] + Ti[Tn],\n(21)\nwhere Ci[Cn] is the output of the softmax value of Ci of the\nCn color element, and Ti[Tn] is the output of the softmax\nvalue of Ti of the Tn type element.\nThe ﬁnal probability between a NL description and an\nimage is computed as follows:\nProb = MPR + SS + λ ∗CT M\n(22)\nwhere λ is a weight and is set to 0.5.\n5. Loss Function\nThe total loss function is deﬁned as follows:\nLtotal = Lseg + λ1 × Lcls + Lsub + λ2 × Lfut\n(23)\nwhere Lseg is vehicle segmentation loss, Lcls is vehicle\nclassiﬁcation loss, Lsub is substitution loss, Lfut is future\nprediction loss, and λ1 and λ2 are weights and are set to 0.2\nand 0.2 respectively. The losses are described in the follow-\ning sections.\n5.1. Vehicle segmentation loss\nVehicle segmentation loss is binary mask prediction loss\nwhich is calculated using the binary cross entropy formula\nas follows:\nLseg = −\nX\nb × log(M),\n(24)\nwhere b is the ground truth segmentation mask. It is note-\nworthy that we use the bounding box area as the segmenta-\ntion mask because we do not have a precise vehicle mask.\n5.2. Vehicle classiﬁcation loss\nVehicle classiﬁcation loss is cross entropy loss for Cn,\nTn, Ci, and Ti.\nWe deﬁne the prediction set as P\n=\n{Cn, Ci, Tn, Ti} and the corresponding ground truth set as\nPgt:\nLcls = −\nX\nZ∈P\nPgt[Z] × log(Z),\n(25)\nwhere Pgt[Z] is the ground truth label of Z.\n5.3. Substitution loss\nThe goal of this loss function is to train the FI and FN\nso that they are semantically exchangeable.\nLsub = 2 −cs(FIgt, FIg) + cs(FNgt, FNg),\n(26)\nwhere cs is the cosine similarity. This loss makes the gen-\nerated features and ground truth features closer.\n5.4. Future prediction loss\nThis loss is aimed for training the future prediction mod-\nule so that it forecasts next frame more precisely. The mean\nsquared error is calculated for each pixel and summed for\nall pixels.\nLfut =\n1\n3 × h × w\nX\n(U −I)2\n(27)\n6. Dataset\n6.1. Cityﬂow-NL\nOur model has been developed using the Cityﬂow-NL\ndataset.\nThis dataset consists of 666 targets vehicles in\n3,028 (single-view) tracks from 40 calibrated cameras, and\n5,289 unique NL descriptions. Each track has a sequence\nof scene images and three NL descriptions typically. The\naverage number of frames a target vehicle is 75.85.\n6.2. Denoising the descriptions\nTo depress the noise in the learning process, we perform\npre-processing. Vehicle color and type are ambiguous in\nperception. Therefore, in some cases, the three descriptions\nare not match in terms of color and type. We use the extrac-\ntion and voting method to clean the color and type. First,\nwe choose 12 colors and 10 types that exist in this dataset\nas labels. Via a matching algorithm, we can extract the type\nand color from the sentence. Because sentences are simple\nand mostly have only one color and type speciﬁer, it can be\neasily conducted. One image has three NL descriptions, and\neach has a color and type. To unify the color and type, we\nchoose the most likely type and color. Finally, we replace\nthe other colors and types with the chosen color and type to\ndenoise the data.\n7. Experiment\n7.1. Implementation detail\nPreprocessing resizes all images to 384×384 pixels and\napplies random translation effects. We use the Adam [12]\n5\nFigure 4. Examples of ﬁve highest matching images for each NL\ndescription. The NL description is on the left, and the images are\nsorted in descending order according to the matching probability\non the right.\noptimizer with a weight decay of 3e-5 and a momentum of\n0.9. The proposed model is trained with a batch size of 64, a\ntraining epoch of 10, and an initial learning rate of 0.00003,\ndivided by 10 at 5 and 8 epochs. Label smoothing is also\napplied to avoid overﬁtting in classiﬁcation loss. Training\nrequired 24 h on the Cityﬂow-NL datasets, using 4 NVIDIA\nRTX 1080 GPU system. The training code was written in\nPyTorch [18].\nFigure 7.1 shows the examples of the SBNet’s output for\nﬁve highest matching images for each NL description. NL\ndescriptions are on the left while the images sorted in de-\nscending order according to the matching probability are on\nthe right.\n7.2. Evaluation metric and results\nTo evaluate, we use recall and Mean Reciprocal Rank\n(MRR) metric, which are standard metrics for retrieval\ntasks[17].\nWe use the baseline model for Cityﬂow-NL\nBenchmark presented in [4].\n7.2.1\nEffects of modules\nTo evaluate the effects of each module, we conduct ablation\nstudies with and without the modules. Table 1 shows that\nthe performance gain depends on additional modules. With\neach modules, its corresponding probability computation\nare also added. The classiﬁcation module brings about 0.5%\nimprovement in baseline (i.e., without any additional mod-\nules). The model with the substitution module shows an 1%\nimprovement, and the future prediction module achieves a\n0.7% performance improvement.\n7.2.2\nCityﬂow-NL performance\nOur competition performance is shown in Table 2.\nWe\nachieve 10th place on the ﬁnal leaderboard. We also make\na comparison with the baseline proposed in [4].\nSBNet\nModule Name\nIncluded\nClassiﬁcation\nNo\nYes\nYes\nYes\nSubstitution\nNo\nNo\nYes\nYes\nFuture Prediction\nNo\nNo\nNo\nYes\nMRR\n0.0977\n0.1025\n0.1124\n0.1195\nTable 1. Model performance on with and without modules from\nthe leaderboard. Without the three modules, the model consists of\nthe IPM, NLM, multi-modal module, and mask prediction module.\nachieves a signiﬁcant performance improvement from the\nbaseline as shown in Table 3.\nRank\nName\nMRR\n1\nAlibaba-UTS\n0.1869\n2\nTimeLab\n0.1613\n3\nSBUK\n0.1594\n4\nSNLP\n0.1571\n5\nHUST\n0.1564\n6\nHCMUS\n0.1560\n7\nVCA\n0.1548\n8\naiem2021\n0.1364\n9\nEnablers\n0.1314\n10\nModulabs (ours)\n0.1195\nTable 2. Leaderboard of the Track 5 in the AI City Challenge 2021.\nModel\nMRR\nSBNet\n0.1195\nSiamese baseline[4]\n0.0269\nTable 3. Performance comparison between the SBNet and baseline\nmodels.\n8. Conclusion\nTo tackle the NL-based vehicle retrieval task, we pro-\nposed a segmentation-based network model called SBNet.\nIt consists of the IPM, NLM, and multi-modal module to\nhandle NL descriptions and images simultaneously.\nWe\nalso introduce the substitution module and future predic-\ntion module, which improve the performance. The match-\ning probability between the description and image is com-\nputed with each module’s output. We achieved a signiﬁcant\nimprovement over the baseline and ranked 10th in the nat-\nural language-based vehicle tracking track in the AI City\nChallenge 2021.\nAcknowledgement\nThis work was supported by the National Research\nFoundation of Korea (NRF) grant funded by the Korea gov-\nernment (MEST) (No.2020R1A2C1102767).\n6\nReferences\n[1] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christo-\npher D. Manning. Electra: Pre-training text encoders as dis-\ncriminators rather than generators. arXiv, abs/2003.10555,\n2020.\n[2] Qi Feng, Vitaly Ablavsky, Qinxun Bai, Guorong Li, and S.\nSclaroff. Real-time visual object tracking with natural lan-\nguage description. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 700–\n709, 2020.\n[3] Qi Feng, Vitaly Ablavsky, Qinxun Bai, and S. Sclaroff. Ro-\nbust visual object tracking with natural language region pro-\nposal network. arXiv, abs/1912.02048, 2019.\n[4] Qi Feng, Vitaly Ablavsky, and Stan Sclaroff. Cityﬂow-nl:\nTracking and retrieval of vehicles at city scale by natural lan-\nguage descriptions. arXiv, abs/2101.04741, 2021.\n[5] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees\nG. M. Snoek. Actor and action video segmentation from a\nsentence. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 5958–5966,\n2018.\n[6] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 770–778, 2016.\n[7] Lisa Anne Hendricks, O. Wang, E. Shechtman, Josef Sivic,\nTrevor Darrell, and Bryan C. Russell. Localizing moments\nin video with natural language. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), pages\n5804–5813, 2017.\n[8] Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-\nLuh Liu. One-shot object detection with co-attention and\nco-excitation. arXiv, abs/1911.12529, 2019.\n[9] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\nSqueeze-and-excitation networks.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence, 42:2011–2023,\n2020.\n[10] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-\nmentation from natural language expressions. In Proceed-\nings of the European Conference on Computer Vision, pages\n108–124, 2016.\n[11] Sultan Daud Khan and Habib Ullah. A survey of advances in\nvision-based vehicle re-identiﬁcation. Computer Vision and\nImage Understanding, 182:50–63, 2019.\n[12] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv, abs/1412.6980, 2014.\n[13] Sangrok Lee, Eunsoo Park, Hongsuk Yi, and Sang Hun Lee.\nStrdan: Synthetic-to-real domain adaptation network for ve-\nhicle re-identiﬁcation.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 608–609, 2020.\n[14] Sangrok Lee, Taekang Woo, and Sang Hun Lee.\nMulti-\nattention-based soft partition network for vehicle re-\nidentiﬁcation. arXiv, 2021.\n[15] Shaomeng Li, Tonglin Xiao, Hongsheng Li, B. Zhou, Dayu\nYue, and Xiaogang Wang. Person search with natural lan-\nguage description. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5187–5196, 2017.\n[16] Zhenyang Li, Ran Tao, Efstratios Gavves, Cees G. M. Snoek,\nand Arnold W.M. Smeulders. Tracking by natural language\nspeciﬁcation.\nIn Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n7350–7358, 2017.\n[17] Christopher D. Manning, P. Raghavan, and Hinrich Sch¨utze.\nIntroduction to information retrieval. Cambridge University\nPress, 2008.\n[18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K¨opf, Edward Yang, Zach DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. Pytorch: An imper-\native style, high-performance deep learning library. arXiv,\nabs/1912.01703, 2019.\n[19] Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, 2018.\n[20] Adam Santoro, David Raposo, David G. T. Barrett, Mateusz\nMalinowski, Razvan Pascanu, Peter Battaglia, and Timon-\nthy Lillicrap. A simple neural network module for relational\nreasoning. arXiv, abs/1706.01427, 2017.\n[21] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\narXiv,\nabs/1706.03762, 2017.\n[22] Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, and\nTatsuya Harada. Spatio-temporal person retrieval via natural\nlanguage queries. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), pages 1462–1471,\n2017.\n[23] Zakria,\nJianhua Deng,\nMuhammad Saddam Khokhar,\nMuhammad Umar Aftab, Jingye Cai1, Rajesh Kumar,\nand Jay Kumar.\nTrends in vehicle re-identiﬁcation past,\npresent, and future:\nA comprehensive reviews.\narXiv,\nabs/2102.09744, 2021.\n[24] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and\nLarry S. Davis. Man: Moment alignment network for natural\nlanguage moment retrieval via iterative graph adjustment. In\nProceedings of IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 1247–1257, 2019.\n7\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "I.2.10; I.5.1; I.4.8"
  ],
  "published": "2021-04-22",
  "updated": "2021-04-22"
}