{
  "id": "http://arxiv.org/abs/2104.13725v1",
  "title": "Preserving Semantic Consistency in Unsupervised Domain Adaptation Using Generative Adversarial Networks",
  "authors": [
    "Mohammad Mahfujur Rahman",
    "Clinton Fookes",
    "Sridha Sridharan"
  ],
  "abstract": "Unsupervised domain adaptation seeks to mitigate the distribution discrepancy\nbetween source and target domains, given labeled samples of the source domain\nand unlabeled samples of the target domain. Generative adversarial networks\n(GANs) have demonstrated significant improvement in domain adaptation by\nproducing images which are domain specific for training. However, most of the\nexisting GAN based techniques for unsupervised domain adaptation do not\nconsider semantic information during domain matching, hence these methods\ndegrade the performance when the source and target domain data are semantically\ndifferent. In this paper, we propose an end-to-end novel semantic consistent\ngenerative adversarial network (SCGAN). This network can achieve source to\ntarget domain matching by capturing semantic information at the feature level\nand producing images for unsupervised domain adaptation from both the source\nand the target domains. We demonstrate the robustness of our proposed method\nwhich exceeds the state-of-the-art performance in unsupervised domain\nadaptation settings by performing experiments on digit and object\nclassification tasks.",
  "text": "Preserving Semantic Consistency in Unsupervised\nDomain Adaptation Using Generative Adversarial\nNetworks\nMohammad Mahfujur Rahman∗, Clinton Fookes, Sridha Sridharan\nSignal Processing, Artiﬁcial Intelligence and Vision Technologies (SAIVT), Queensland\nUniversity of Technology (QUT), Brisbane, QLD 4000, Australia\nAbstract\nUnsupervised domain adaptation seeks to mitigate the distribution discrepancy\nbetween source and target domains, given labeled samples of the source domain\nand unlabeled samples of the target domain. Generative adversarial networks\n(GANs) have demonstrated signiﬁcant improvement in domain adaptation by\nproducing images which are domain speciﬁc for training.\nHowever, most of\nthe existing GAN based techniques for unsupervised domain adaptation do not\nconsider semantic information during domain matching, hence these methods\ndegrade the performance when the source and target domain data are semanti-\ncally diﬀerent. In this paper, we propose an end-to-end novel semantic consis-\ntent generative adversarial network (SCGAN). This network can achieve source\nto target domain matching by capturing semantic information at the feature\nlevel and producing images for unsupervised domain adaptation from both the\nsource and the target domains. We demonstrate the robustness of our proposed\nmethod which exceeds the state-of-the-art performance in unsupervised domain\nadaptation settings by performing experiments on digit and object classiﬁcation\ntasks.\nKeywords:\nDomain adaptation, dataset bias, domain discrepancy, deep neural\nnetworks, generative adversarial networks, object classiﬁcation.\n∗Corresponding author: Tel.: +61416453032;\nEmail address: m27.rahman@qut.edu.au (Mohammad Mahfujur Rahman )\nPreprint submitted to Pattern Recognition Letters\nApril 29, 2021\narXiv:2104.13725v1  [cs.CV]  28 Apr 2021\n1. Introduction\nA common requirement for the success of deep learning approaches is the\navailability of large amounts of labeled training data. These methods are gener-\nally trained on massive amounts of labeled training data and tested on the data\nthat have the same distribution as the training data. When the distributions\nof the training and test data are diﬀerent, the performance of the deep learning\ntechniques degrades. To improve the performance in such situations, one needs\nto collect large amounts of labeled training data with the same distribution as\nthe test data and this is a time consuming and expensive procedure. To address\nthis problem one can use transfer learning where a learned model for one task\nis reused as the starting point for another task. We can reduce the expense of\naccumulating large sets of labeled data for training by transferring the infor-\nmation obtained from diverse but similar domains. Domain adaptation (DA) is\na special type of transfer learning where the source data (training) and target\ndata (testing) have a diﬀerent distribution but share the same tasks.\nEarly DA techniques used hand-crafted features and were known as shallow\nDA methods [1]. Deep neural networks extract more transferable and domain-\ninvariant features.\nThus, later eﬀorts were motivated to extend shallow DA\napproaches to deep neural network based DA approaches [2, 3, 4, 5]. Recently,\ngenerative adversarial networks (GANs) introduced by [6] have been shown to\nbe successful in the area of unsupervised domain adaptation (UDA). The GAN\nis used for DA for transforming the images from one domain to another. In\n[7], image translation and extracting domain invariant information is achieved\nusing only a single encoder and a conditioned domain indicator code. In [8],\ndomain alteration is achieved by using two encoders and two discriminators. In\n[9, 10] the network produces source like images from both the source and target\ndomains to minimize the discrepancy. However, the existing GAN based DA\napproaches [7, 9, 8] fail to preserve the semantic information across the source\nand target domains. As a result, the performance is compromised when the\n2\n \n \n \n \n \nxs \nxt \nE \nC \nz \nK \nG \n \n \n \n \nReal/\nfake \nClass \nlabels \nD1 \nD2 \nReal/\nfake \nClass \nlabels \nSLoss \nSLoss \nE \nE \nE \nE \nxs          s         \nxt         s          \nxs        t           \nxt         t          \nFigure 1: Illustration of our proposed approach. The proposed architecture comprises four\nmain sections: an encoder E, a generator G, a classiﬁer C and two discriminators, D1 for the\nsource domain and D2 for the target domain. The encoder embeds the images either from\nthe source or target domain to extract latent embeddings. The generator decodes the latent\nembeddings from the encoder with a conditioned domain key. The generated images are also\nfed into the encoder to capture semantic information. The discriminators diﬀerentiate the\nreal and generated images for both source and target domains.\nsource and target domains are vastly diﬀerent.\nTo overcome the above-mentioned problem, we propose a semantic consis-\ntent generative adversarial network (SCGAN) for UDA to preserve the semantic\ninformation for the source and target domains. Although Hoﬀman et al. [11]\nproposed semantic consistency loss in DA task, the formulation of our pro-\nposed semantic consistency loss is signiﬁcantly diﬀerent. In [11], the labels of\nthe source data are used as it formulates the semantic consistency loss using\ncross-entropy loss; however, we formulate the semantic consistent loss using L1\ndistance without using labels of the source data. Furthermore, [11] is not an\nend-to-end method for image classiﬁcation task. In one step, it generates im-\nages and in another step, the generated images are used in the DA network\nto classify the images. An end-to-end deep domain adaptation strategy is able\nto tackle the domain bias through the incorporation of feature extraction and\ndomain alignment into a united architecture. Since the feature extraction mod-\nule receives the feedback from the domain alignment module, an end-to-end\n3\ndeep domain adaptation strategy will perform better when compared to the not\nend-to-end domain adaption approaches. We show that our proposed approach,\nwhich preserves semantic consistency in domain transfer, has superior perfor-\nmance over the approach proposed by Hoﬀman et al 2018 which uses a semantic\nconstancy loss. Our approach also exceeds the performance of other state of the\nart approaches for DA.\nOur proposed method is an end-to-end DA approach for the image classiﬁca-\ntion tasks. We adopt pseudo labeling of unlabeled target samples that attempt\nto minimise the domain discrepancy. To achieve highly conﬁdent pseudo-labeled\ntarget data which is mostly correct, the model is pre-trained with labeled source\ndata. The source and target data are passed through a common encoder during\nthe training phase to obtain a latent representation. The generator then de-\ncodes the latent representation with the help of a domain key that ensures the\ndomain alteration. The generated images are supposed to look like source or\ntarget styled images, hence we use two discriminators to identify the real or fake\nimages from the source and target domains. Moreover, the discriminators act\nas a multi-class classiﬁer to assure the gradient signals backpropagated by the\ndiscriminator for the target data corresponding to the respective classes. The\ngenerated images are passed through the encoder again to obtain the semantic\nrepresentation of the source and target domains. The classiﬁer which is used to\npredict the labels of the target data is developed on the latent representation\nextracted from the encoder. Our proposed approach achieves state-of-the-art\nperformance on digit and object classiﬁcation tasks in the UDA setting. The\ncontributions of this work are summarised as follows:\n• A semantic consistent generative adversarial network (SCGAN) is pro-\nposed for UDA where the labeled source data and unlabeled target data\nare used during training, and the unlabeled data is used during testing.\n• The proposed SCGAN preserves the semantic information across the source\nand target domains to improve the performance when domains are vastly\ndiﬀerent.\n4\n• SCGAN achieves superior results on object and digit classiﬁcation tasks\ncompared to the state-of-the-art.\n2. Related Works\nAdversarial learning is promisingly utilized in domain adaptation. Almost all\nthe adversarial learning based domain adaptation approaches [3, 12, 13] follow\nthe concept from a generative adversarial network (GAN) [6]. A discriminator\nis trained in such methods to determine whether the sampled feature derives\nfrom the source domain or target domain. By contrast, the feature extractor\nis trained to deceive the discriminator. Adversarial DA approaches address the\ndomain shift problem in the feature-space whereas GAN based DA approaches\n[14, 11, 9, 15, 7, 16, 17, 18, 19, 20, 21, 22, 23, 24] reduce the domain shift between\nthe source and target data in the pixel-space by altering the source data to the\nstyle of a target domain.\nLiu et al. [14] developed coupled generative adversarial networks (CoGAN)\nto learn a shared distribution of the source and target domains utilizing two\nclassiﬁers for two domains. In this approach, the predicting functions are ad-\njusted to enable the source classiﬁer to identify the target data accurately. An-\nother GAN-based DA approach is introduced in [9] where the network generates\nsource-styled images from the source and target embeddings. In [15], a condi-\ntional GAN is proposed for UDA in the context of a segmentation task. In [7], a\nduplex discriminator based GAN is used to mitigate the domain shift problem.\nGenerative domain adaptation networks (G-DAN) is proposed in [16] which\nis capable to generate new domains by modifying the source and target distribu-\ntions. Cycada [11] performed domain adaptation by translating source images\ninto target styled images inspiring from the image-to-image translating method\n[25]. Fengmao et al. [24] investigated the target data with their corresponding\nlabels by disentangling the class code from the latent variables of the generator\nfor target through the cooperation of the high mutual information and weight\nsharing process. Murez et al. [22] combined cycle consistency, domain speciﬁc\nreconstruction and domain invariant feature extraction to transfer the source\n5\nimages into the target images and vice versa. Sankaranarayanan et al. [21]\nproposed a generative network aiming to transfer the target images into source\nlike images. In [19], two generators are used for image translation from the\nsource domain to target domain and vice-versa. However, the existing GAN\nbased DA approaches fail to capture eﬀectively the semantic information across\nthe domains during domain translation. To mitigate this problem, the gener-\nated images are fed into an encoder to capture the semantic information in our\nproposed approach.\n3. Proposed Method\n3.1. Problem Setup\nIn this section, we provide a brief illustration of the proposed SCGAN on\nUDA. We commence with a conventional representation of the domain adapta-\ntion problem. A domain D is denoted by a joint distribution P(X, Y ) deﬁned on\nX × Y where X represents the feature space and Y represents the label space.\nWe assume that we have access to labeled data Xs = {xi\ns, yi\ns} from the source\ndomain DS, and unlabeled data Xt = {xi\nt} from the target domain DT . The\nsource and target domain follow a diﬀerent distribution; however, both domains\nshare the same set of classes or categories. The aim of UDA is to seek a clas-\nsifying function F : X →Y which is able to classify Xt to the corresponding\nlabels Yt given Xs = {xi\ns, yi\ns} and Xt = {xi\nt} during training as the input.\n3.2. Semantic Consistent Generative Adversarial Network (SCGAN)\nThe aim of our proposed SCGAN which is composed of one encoder, one\ngenerator, one classiﬁer, and two discriminators is to extract a domain invari-\nant feature representation and provide domain alteration. Figure 1 shows an\noverview of our proposed approach.\nThe encoder denoted as E learns a la-\ntent embedding E : X →Z from the input images from either the source or\ntarget domain; furthermore, the classiﬁcation network, denoted as C, learns a\nprediction function C : Z →Y . As the target data is unlabeled, the classifying\nnetwork has access to the labels of the source data only. The encoder learns\nthe domain shift between DS and DT during extracting latent representation\n6\nfrom the target data during training. The generator G generates source and\ntarget styled images by decoding the extracted latent representation by the en-\ncoder with conditioned by a domain key using a reconstruction loss. As the\nreconstruction loss is used in the pixel-level, it fails to capture semantic features\nshared across the source and target domain which leads to a drop of performance\nwhen the source and target domains are vastly diﬀerent.\nTo capture the semantic features of the source and target domain, all the\ngenerated images are passed through an encoder and the semantic information\nis preserved by a semantic consistent loss. The tasks of the discriminators is\nto diﬀerentiate the real images from generated images and predict the class\nof the real images which compel the latent features to be domain speciﬁc and\ncategorical knowledge is preserved. The labels of both source-styled from the\nsource domain and target-styled from the source domain are available which\ncan be directly used in the discriminators. However, the label of the source-\nstyled from the target domain and target-styled from the target domain are not\nable to be obtained, so the pseudo categorical labels anticipated and assigned\nfrom the classiﬁer are utilized by training the classiﬁer with the source data.\nAfter achieving the convergence, the learned network provides annotation of the\ngenerated source-styled from target data and target-styled from target data.\nThe discriminators (D1) and (D2) map the real image or the generated image\ninto two distributions: the probability of the input being real, which is modeled\nas a binary classiﬁer and the class probability distribution of the input, which\nis modeled as a Nc-way classiﬁer.\nNow, we will brieﬂy formalize the illustration above. At ﬁrst, the encoder\nreceives an input image xi either from the source domain or target domain and\nextracts a latent representation, z = E(xi). The classiﬁer is built on the latent\nrepresentation taking as input the latent representation which is generated by\nE and predicts a multi-class distribution. The latent representation from the\nsource and target domain can be denoted as zs and zt respectively. The latent\nrepresentation z is supposed to be domain speciﬁc. The generator G takes the\nlatent space representation as input from the feature extractor and produces\n7\nimages with a condition and G can be represented as,\nxk = G(z, k), z ∈zs ∪zt,\n(1)\nwhere the domain key k ∈{ks, kt} is a one-hot vector used to determine the\ndomain.\nThe generator G generates four types of images from two input domains.\nThe generated images appear as either source-styled or target-styled images.\nFor example, the source-styled image generated from the source data can be\nexpressed as,\nxs→s = G(E(xs), ks) = G(zs, ks),\n(2)\nwhere xs→s is the source-styled image generated from source data.\nSimilarly, the target-styled image (xs→t) can be generated from source data,\nthe source-styled image (xt→s) can be generated from target data and the target-\nstyled image (xt→t) can be generated from the target data. It is noted that the\nencoder and generator can be any type of deep learning network. When the\nsource domain image xs is translated into source domain styled image xs→s, it\nis expected that the generated image should be the same as itself xs. Similarly,\nwhen the target domain image xt is translated into target styled image, the\ngenerated image should be the same as xt. Besides, the target-styled image\nfrom the source domain xs→t should look like the real target domain and the\nsource-styled image generated from the target domain xt→s should look like real\nsource domain.\nDomain alteration can be achieved by the reconstruction loss and domain\nkey. However, the reconstruction loss fails to capture the semantic information\nacross the source and target domains.\nTo capture the semantic information\nacross the domains at the feature level, the generated images are passed through\nthe encoder. For example, the encoder produces the latent representation from\nthe source-styled generated images from source data as follows,\nzs→s = E(xs→s),\n(3)\n8\nSimilarly, the encoder produces the latent representation from the other\nsource and target-styled images that are generated from the source and target\ndata as zs→t, zt→s and zt→t.\nFinally, the objective function of the encoder and generator can be repre-\nsented as,\nLG = min\nE,G\n\u0000X\nxs∈,DS\n(J(D2(xs→t), ys→t) + αl1(xs→s, xs) + βl1(zs→s, zs→t)+\nX\nxt∈DT\n(J(D1(xt→s), yt→s) + αl1(xt→t, xt) + βl1(zt→t, zt→s)\n\u0001\n, (4)\nwhere J(., .) is the cross entropy loss, D1 and D2 are two discriminators, α and β\nare the hyper parameters for the reconstructed loss and semantic consistent loss\nrespectively. l1 is the Manhattan distance between the latent representations of\nthe generated images.\nThe discriminators not only diﬀerentiate the real and generated images but\nalso categorise the real source and target images based on the true labels of the\nsource data and pseudo labels of the target data. The discriminator D1 for the\nsource domain aims to diﬀerentiate the real source image xs and the generated\ntarget styled image from source xt→s.\nMoreover, D1 categorize the xs and\nxt→s using true labels for source data and pseudo labels for source styled image\nfrom target domain. Similarly, D2 for target domain aims to diﬀerentiate real\nand generated images as well as categorize the source-styled image from target\ndomain and target-styled image from source domain. As the target-styled image\nfrom the source domain remains the same category as real source data, we use\nthe true labels for the target-styled image from source domain and pseudo labels\nfor the target-styled image from target data due to unavailability of the labels\nof target data.\nIn summary, the objective function of the two discriminators D1 and D2 can\n9\nbe formulated as,\nLD = min\nD1,D2\n\u0000X\nxs∈,DS\n(J(D1(xs), ys) + J(D2(xs→t), ys→t)+\nX\nxt∈,DT\n(J(D1(xt→s), yt→s) + J(D2(xt), yt)\n\u0001\n.\n(5)\n3.3. Classiﬁer\nA classiﬁer C is used on the latent embeddings that are extracted from the\nencoder for the classiﬁcation network. The objective function of the classiﬁer\nfor the source and target data can be represented as follows,\nLc = min\nC\n\u0000 X\nxs∈DS\nJ(zs, ys) +\nX\nxt∈DT\nJ(zt, yt)\n\u0001\n,\n(6)\nwhere J(.,.) is the cross entropy loss, ys is the label of source data and yt is the\npseudo label of target data. The classiﬁer C is pre-trained with the supervised\nsource data for obtaining the high conﬁdent pseudo labeled target data. It is\nnotable that C can be any type of deep learning network. Furthermore, the\nclassiﬁer assists to achieve the domain invariant latent representation as the\ndomain-speciﬁc portion in latent space.\n3.4. Overall Objective\nThe overall objective function of our proposed SCGAN can be formulated\nas follows,\nLtotal =\nmin\nE,C,G,D(LG + LD + γLc),\n(7)\nwhere γ is a hyper parameter of the classiﬁer network. To preserve the category\ninformation of the source and target data, the generator and the discriminators\nare optimized in an adversarial way.\n4. Experiments and Results\n4.1. Datasets\nWe evaluate our proposed SCGAN on digit and object classiﬁcation tasks.\nMNIST [33] consists of 10 classes which ranges 0 to 9. USPS [34] dataset is\n10\n(a) Source domain \n(b)Target domain \nFigure 2: Examples of generated images during Amazon (A) →Webcam (W) transfer task.\nThe ﬁrst and fourth columns represent the source and target domain’s real images respectively.\nThe second and third columns represent the generated images from source domain with source\nand target domains style respectively. The ﬁfth and sixth columns represent the generated\nimages from target domain with target and source domains style respectively.\n11\nTable 1: Recognition accuracies for cross-domain digit classiﬁcation tasks. We use the con-\nventional protocol for UDA where source data are labeled, but target data are unlabeled.\nMNIST →USPS indicates MNIST is the source and USPS is the target domain.\nMethod\nMNIST →USPS\nUSPS →MNIST\nSVHN →MNIST\nMNIST →SVHN\nMNIST →MNIST-M\nSource Only\n86.2\n75.3\n61.9\n32.3\n69.7\nCORAL [26]\n81.7\n-\n63.1\n-\n76.9\nMMD [27]\n81.1\n-\n71.1\n-\n76.9\nDANN [3]\n85.1\n74.2\n73.9\n35.7\n77.4\nADDA [12]\n89.4\n90.1\n76.0\n-\n-\nATT [28]\n-\n-\n86.2\n52.8\n94.2\nCoGAN [14]\n91.2\n89.1\n-\n-\n62.0\nUNIT [29]\n95.9\n93.5\n90.5\n-\n-\nSBADA-GAN [8]\n97.6\n95.0\n76.1\n61.1\n99.4\nGTA [9]\n92.8\n90.8\n92.4\n-\n-\nCyCADA [11]\n94.8\n95.7\n88.3\n-\n-\nDupGAN [7]\n96.0\n98.8\n92.5\n62.7\n-\nI2I [22]\n98.8\n97.6\n90.1\n-\n-\nSCGAN(Ours)\n97.9\n98.7\n94.5\n65.8\n99.2\nTable 2: Image classiﬁcation accuracies on the Oﬃce-31 dataset for deep domain adaptation.\nFor UDA, we use the standard protocol where source data is labeled while target data is\nunlabeled. A →W shows A (Amazon) is the source and W (Webcam) is the target domain.\nMethods\nA →W\nD →W\nD →A\nW →A\nW →D\nA →D\nAvg.\nResnet-Source Only\n68.4\n96.7\n62.5\n60.7\n99.3\n68.9\n76.1\nDAN [2]\n80.5\n97.1\n63.6\n62.8\n99.6\n78.6\n80.4\nRTN [30]\n84.5\n96.8\n66.2\n64.8\n99.4\n77.5\n81.6\nDANN [3]\n82.0\n96.9\n68.2\n67.4\n99.1\n79.7\n82.2\nADDA [12]\n86.2\n96.2\n69.5\n68.9\n98.4\n77.8\n82.9\nJAN [31]\n85.4\n97.4\n68.6\n70.0\n99.8\n84.7\n84.3\nMADA [13]\n90.0\n97.4\n70.3\n66.4\n99.6\n87.8\n85.2\nSimNet [32]\n88.6\n98.2\n73.4\n71.6\n99.7\n85.3\n86.2\nGTA [9]\n89.5\n97.9\n72.8\n71.4\n99.8\n87.7\n86.5\nSCGAN (Ours)\n91.5\n98.1\n74.5\n73.6\n99.8\n90.3\n88.0\ncreated by taking the digit images which are automatically scanned from United\nStates Postal Service’s envelopes. It contains 9,298 images with 10 classes ranges\nfrom 0 to 9. SVHN [35] is extracted from the street view house number, com-\nprising of more than 600,000 colored images. MNIST-M [3] dataset comprises\nMNIST digits where the background is blended with random color patches. For\nMNIST →USPS, USPS →MNIST, SVHN →MNIST, MNIST →SVHN and\nMNIST →MNIST-M transfer tasks, we use the same protocol as [7, 9]. Oﬃce-\n31 [36] dataset consists of three domains: Amazon (A), Webcam (W) and DSLR\n12\n(D). It has 31 diﬀerent classes. We adopt the same protocol as [9, 32] for A →\nW, D →W, D →A, W →A, W →D and A →D transfer tasks.\nTable 3: Ablation study of our proposed approach on unsupervised domain adaptation.\nMethod\nSVHN →MNIST\nMNIST →SVHN\nA →D\nD →A\nSCGAN (without semantic loss)\n92.8\n63.2\n88.1\n72.9\nSCGAN (with semantic loss)\n94.5\n65.8\n90.3\n74.5\n5. Visualization\n(a) \n(b) \n(c) \nFigure 3: t-SNE visualization of the activations of (a) Resnet without adaptation (b) SCGAN-\nwithout semantic consistent loss and (c) SCGAN.\n5.1. Experiments\nWe evaluate our proposed SCGAN in the context of image classiﬁcation tasks\nwhere labeled source data and unlabeled target data is used during training.\nThe performance is reported in terms of classiﬁcation accuracy. We compare\nour SCGAN approach for UDA with some state-of-the-art methods. For digit\nclassiﬁcation tasks, we compare our approach with CORAL [26], MMD [27],\nDANN [3], ADDA [12], ATT [28], CoGAN [14], UNIT [29], SBADA-GAN [8],\nGTA [9], CyCADA [11], DupGAN [7] and I2I [22]. For object classiﬁcation tasks\non Oﬃce-31 dataset, we compare our approach with DAN [2], RTN [30], DANN\n[3], ADDA [12], JAN [31], MADA [13], SimNet [32] and GTA [9]. For more\nin-depth comparison we also compare the results with a source only method\nwhere the same architecture is used with the labeled source data.\n13\n5.2. Implementation Details\nIn the experiments of digit classiﬁcation tasks, we employ the architecture of\nconvolutional neural network used in [28] and [29]. For MNIST →USPS, USPS\n→MNIST, SVHN →MNIST transfer tasks, we follow the same architecture of\n[29] and for MNIST →SVHN, MNIST →MNISTM transfer tasks we follow the\nsame architecture of [28]. The impact of pseudo-labels in a neural network was\nstudied in [37]. They claimed that training a classiﬁer with pseudo-labels has the\nsame eﬀect as entropy regularization, resulting in low-density class separation.\nInspired by [37], the classiﬁer C is pre-trained with only source domain images\nto achieve mostly accurate and high-conﬁdent pseudo-labeled target domain\nsamples. The pseudo label with softmax score greater than a threshold is chosen\nto train the predicting model. In MNIST →SVHN, we set the pseudo labeling\nthreshold value to 0.95. We picked it to 0.9 in in other transfer tasks. For\noptimization, we use Momentum SGD and set the momentum to 0.9.\nThe\nlearning rate is calculated on validation splits and uses either 0.01 or 0.05. For\nMNIST →USPS, USPS →MNIST, and SVHN →MNIST transfer tasks, the\nvalue of α, β and γ are set as 10.0, 1.0 and 0.2 respectively, and for MNIST\n→SVHN, MNIST →MNIST-M transfer tasks and all other object recognition\ntasks, the value of α, β and γ are set as 1.0, 1.0 and 1.0 respectively.\n5.3. Results\nIn this section, we evaluate the eﬃcacy of the proposed method on two\nclassiﬁcation tasks: digit classiﬁcation and image classiﬁcation. The proposed\nmethod is compared with state-of-the-art methods as shown in Table 1 and\nTable 2.\nTable 1 shows the recognition accuracies for digit experiments on four bench-\nmark datasets: MNIST, USPS, SVHN and MNIST-M. For fair comparison, we\nfollow the same settings as followed by the state-of-the-art methods. We adopt\nﬁve UDA settings: MNIST →USPS, USPS →MNIST, SVHN →MNIST,\nMNIST →SVHN and MNIST →MNIST-M to evaluate the proposed SCGAN.\nIn MNIST →USPS, SVHN →MNIST, and MNIST →SVHN settings, the\n14\nprevious state-of-the-art approach DUPGAN achieved 96.0%, 92.5% and 62.7%\nrecognition accuracies, however, proposed SCGAN achieves a 1.9%, 2.0% and\n3.1% increase over DUPGAN, indicating a substantial improvement in perfor-\nmance with preserving semantic features across domains. Although in USPS →\nMNIST and MNIST →MNIST-M experiments, DUPGAN and SBADA-GAN\noutperforms our method by 0.1% and 0.2% recognition accuraices, which is a\nslight increase in performance. In particular, we achieve good performance im-\nprovement, i.e., up to +3 % points in one of the most challenging tasks, MNIST\n→SVHN. To further evaluate the proposed method, our deep network is trained\nwith the label information of the source domain only, referred to as Source Only\nwhen the target domain is not available.\nFor object classiﬁcation, Oﬃce-31 dataset is used to evaluate our SCGAN.\nWe evaluate six transfer tasks A →W, D →W, D →A, W →A, W →D and\nA →D on this dataset. We adopt the conventional unsupervised protocol for\nDA where labeled source and unlabeled target data are used. Our SCGAN is\ncompared with state-of-the-art methods in Table 2. Our method achieves 88.0%\naverage accuracy and the previous state-of-the-art method [9] acheived 86.5%.\nAs shown in Table 2, proposed SCGAN outperforms all the compared methods,\nespecially in all challenging transfer tasks: A →W, D →A, W →A, and A →\nD.\n5.4. Ablation Study\nWe study the eﬀect of adding semantic consistent loss to clarify the role\ncapturing semantic information of our proposed method. We train the network\nwithout semantic consistent loss and with semantic consistent loss for SVHN\n→MNIST, MNIST →SVHN, A→D and D→A transfer tasks as shown in\nTable 3 to justify the eﬀectiveness of our proposed model. According to Table\n3, for SVHN →MNIST and MNIST →SVHN, without semantic consistent\nloss, the performance drops by 1.7% and 2.6% respectively than SCGAN with\nsemantic consistent loss. For object classiﬁcation task, we perform experiment\non A→D and D→A transfer tasks without the semantic loss. However, the\n15\nperformance has 2.2% and 1.6% decrease than considering the model with the\nsemantic consistent loss.\nWe evaluate the eﬀectiveness of our proposed approach and perform a t-SNE\nvisualization of the learned embeddings. Figure 3 shows a t-SNE visualization\nof the learned embeddings on the A →W transfer task. Here we have consid-\nered 7 classes for better visualization. We observe that the clusters formed by\nSCGAN separate classes while mixing source and target domains much more\neﬀectively than the method without domain adaptation and SCGAN-without\nsemantic loss. Clusters generated by our proposed model are observed to be\nable to distinguish classes far more eﬀectively than SCGAN without seman-\ntic consistent loss and without domain adaptation approach. Thus we can say\nthat SCGAN and SCGAN without semantic consistent loss reduce the discrep-\nancy between the source and target data in the latent embedding space more\nthan the method without domain adaptation. From Figure 3, we can observe\nthat SCGAN less wrongly clustered points in the red clusters (class: monitor)\nthan SCGAN without semantic consistent loss and without domain adaptation\napproaches.\n6. Conclusion\nIn this paper, we propose an end-to-end semantic consistent generative ad-\nversarial network for unsupervised domain adaptation. We aim to capture the\nsemantic information across the source and target domains in the feature level\nduring the domain alteration for minimizing the discrepancy among domains\nand utilize pseudo labels assigned to target data for boosting the performance.\nThe domain alteration is achieved with one encoder, one generator, and two\ndiscriminators for the source and target domains. A classiﬁer which is assem-\nbled on the encoder is utilized to predict the categories of the source and target\ndata. We demonstrate the performance of our approach for unsupervised do-\nmain adaptation on digit and object detection tasks, and show that it oﬀers\nsuperior or competitive performance compared to the state-of-the-art methods\nthat have been reported to date.\n16\nAcknowledgements\nThe research presented in this paper was supported by Australian Research\nCouncil (ARC) Discovery Project Grant DP170100632.\nReferences\n[1] B. Gong, Y. Shi, F. Sha, K. Grauman, Geodesic ﬂow kernel for unsupervised\ndomain adaptation, in: CVPR, 2012.\n[2] M. Long, Y. Cao, J. Wang, M. I. Jordan, Learning transferable features\nwith deep adaptation networks, in: ICML, 2015.\n[3] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-\nlette, M. Marchand, V. S. Lempitsky, Domain-adversarial training of neural\nnetworks, JMLR 17.\n[4] M. M. Rahman, C. Fookes, M. Baktashmotlagh, S. Sridharan, On Mini-\nmum Discrepancy Estimation for Deep Domain Adaptation, Springer In-\nternational Publishing, Cham, 2020, pp. 81–94.\n[5] M. M. Rahman, C. Fookes, M. Baktashmotlagh, S. Sridharan, Correlation-\naware adversarial domain adaptation and generalization, PR 100.\n[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in: NIPS,\n2014.\n[7] L. Hu, M. Kan, S. Shan, X. Chen, Duplex generative adversarial network\nfor unsupervised domain adaptation, in: CVPR, 2018.\n[8] P. Russo, F. M. Carlucci, T. Tommasi, B. Caputo, From source to target\nand back: symmetric bi-directional adaptive gan, in: CVPR, 2018.\n[9] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, R. Chellappa, Generate to\nadapt: Aligning domains using generative adversarial networks, in: CVPR,\n2018.\n17\n[10] M. M. Rahman, C. Fookes, M. Baktashmotlagh, S. Sridharan, Multi-\ncomponent image translation for deep domain generalization, in:\n2019\nIEEE Winter Conference on Applications of Computer Vision (WACV),\nIEEE, 2019, pp. 579–588.\n[11] J. Hoﬀman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,\nT. Darrell, CyCADA: Cycle-consistent adversarial domain adaptation, in:\nICML, 2018.\n[12] E. Tzeng, J. Hoﬀman, K. Saenko, T. Darrell, Adversarial discriminative\ndomain adaptation, in: CVPR, 2017.\n[13] Z. Pei, Z. Cao, M. Long, J. Wang, Multi-adversarial domain adaptation,\nin: AAAI, 2018.\n[14] M.-Y. Liu, O. Tuzel, Coupled generative adversarial networks, in: NIPS,\n2016.\n[15] W. Hong, Z. Wang, M. Yang, J. Yuan, Conditional generative adversarial\nnetwork for structured domain adaptation, in: CVPR, 2018.\n[16] M. Gong, K. Zhang, B. Huang, C. Glymour, D. Tao, K. Batmanghe-\nlich,\nCausal generative domain adaptation networks,\narXiv preprint\narXiv:1804.04333.\n[17] R. Gong, W. Li, Y. Chen, L. V. Gool, Dlow: Domain ﬂow for adaptation\nand generalization, in: CVPR, 2019.\n[18] L. Tran, K. Sohn, X. Yu, X. Liu, M. Chandraker, Gotta adapt’em all: Joint\npixel and feature-level domain adaptation for recognition in the wild, in:\nCVPR, 2019.\n[19] P. Russo, F. M. Carlucci, T. Tommasi, B. Caputo, From source to target\nand back: symmetric bi-directional adaptive gan, in: CVPR, 2018.\n[20] L. Hu, M. Kan, S. Shan, X. Chen, Duplex generative adversarial network\nfor unsupervised domain adaptation, in: CVPR, 2018.\n18\n[21] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, R. Chellappa, Generate to\nadapt: Aligning domains using generative adversarial networks, in: CVPR,\n2018.\n[22] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, K. Kim, Image to\nimage translation for domain adaptation, in: CVPR, 2018.\n[23] W. Hong, Z. Wang, M. Yang, J. Yuan, Conditional generative adversarial\nnetwork for structured domain adaptation, in: CVPR, 2018.\n[24] F. Lv, J. Zhu, G. Yang, L. Duan, Targan: Generating target data with class\nlabels for unsupervised domain adaptation, Knowledge-Based Systems 172\n(2019) 123–129.\n[25] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image trans-\nlation using cycle-consistent adversarial networks, in: ICCV, 2017.\n[26] B. Sun, J. Feng, K. Saenko, Return of frustratingly easy domain adaptation,\nin: AAAI, 2016.\n[27] E. Tzeng, J. Hoﬀman, T. Darrell, K. Saenko, Simultaneous deep transfer\nacross domains and tasks, in: ICCV, 2015.\n[28] K. Saito, Y. Ushiku, T. Harada, Asymmetric tri-training for unsupervised\ndomain adaptation, in: ICML, 2017.\n[29] M.-Y. Liu, T. Breuel, J. Kautz, Unsupervised image-to-image translation\nnetworks, in: NIPS, 2017.\n[30] M. Long, H. Zhu, J. Wang, M. I. Jordan, Unsupervised domain adaptation\nwith residual transfer networks, in: NIPS, 2016.\n[31] M. Long, H. Zhu, J. Wang, M. I. Jordan, Deep transfer learning with joint\nadaptation networks, in: ICML, 2017.\n[32] Y. Zhang, H. Tang, K. Jia, M. Tan, Domain-symmetric networks for ad-\nversarial domain adaptation, in: CVPR, 2019.\n19\n[33] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning ap-\nplied to document recognition, in: Proceedings of the IEEE, 1998.\n[34] T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning,\nSpringer Series in Statistics, 2001.\n[35] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Y Ng, Reading\ndigits in natural images with unsupervised feature learning, in: NIPS, 2011.\n[36] K. Saenko, B. Kulis, M. Fritz, T. Darrell, Adapting visual category models\nto new domains, in: ECCV, 2010.\n[37] D.-H. Lee, Pseudo-label: The simple and eﬃcient semi-supervised learning\nmethod for deep neural networks, in: ICML, 2013.\n20\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-04-28",
  "updated": "2021-04-28"
}