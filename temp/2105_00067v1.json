{
  "id": "http://arxiv.org/abs/2105.00067v1",
  "title": "Unsupervised Discriminative Embedding for Sub-Action Learning in Complex Activities",
  "authors": [
    "Sirnam Swetha",
    "Hilde Kuehne",
    "Yogesh S Rawat",
    "Mubarak Shah"
  ],
  "abstract": "Action recognition and detection in the context of long untrimmed video\nsequences has seen an increased attention from the research community. However,\nannotation of complex activities is usually time consuming and challenging in\npractice. Therefore, recent works started to tackle the problem of unsupervised\nlearning of sub-actions in complex activities. This paper proposes a novel\napproach for unsupervised sub-action learning in complex activities. The\nproposed method maps both visual and temporal representations to a latent space\nwhere the sub-actions are learnt discriminatively in an end-to-end fashion. To\nthis end, we propose to learn sub-actions as latent concepts and a novel\ndiscriminative latent concept learning (DLCL) module aids in learning\nsub-actions. The proposed DLCL module lends on the idea of latent concepts to\nlearn compact representations in the latent embedding space in an unsupervised\nway. The result is a set of latent vectors that can be interpreted as cluster\ncenters in the embedding space. The latent space itself is formed by a joint\nvisual and temporal embedding capturing the visual similarity and temporal\nordering of the data. Our joint learning with discriminative latent concept\nmodule is novel which eliminates the need for explicit clustering. We validate\nour approach on three benchmark datasets and show that the proposed combination\nof visual-temporal embedding and discriminative latent concepts allow to learn\nrobust action representations in an unsupervised setting.",
  "text": "Unsupervised Discriminative Embedding for Sub-Action Learning in Complex\nActivities\nSirnam Swetha⋆, Hilde Kuehne†, Yogesh S Rawat⋆, Mubarak Shah⋆\n⋆Center for Research in Computer Vision, University of Central Florida, Orlando, FL\n†MIT-IBM Watson Lab, Cambridge, MA\nAbstract\nAction recognition and detection in the context of long\nuntrimmed video sequences has seen an increased attention\nfrom the research community. However, annotation of com-\nplex activities is usually time consuming and challenging\nin practice. Therefore, recent works started to tackle the\nproblem of unsupervised learning of sub-actions in com-\nplex activities. This paper proposes a novel approach for\nunsupervised sub-action learning in complex activities. The\nproposed method maps both visual and temporal represen-\ntations to a latent space where the sub-actions are learnt\ndiscriminatively in an end-to-end fashion. To this end, we\npropose to learn sub-actions as latent concepts and a novel\ndiscriminative latent concept learning (DLCL) module aids\nin learning sub-actions. The proposed DLCL module lends\non the idea of latent concepts to learn compact represen-\ntations in the latent embedding space in an unsupervised\nway. The result is a set of latent vectors that can be in-\nterpreted as cluster centers in the embedding space. The\nlatent space itself is formed by a joint visual and temporal\nembedding capturing the visual similarity and temporal or-\ndering of the data. Our joint learning with discriminative\nlatent concept module is novel which eliminates the need\nfor explicit clustering. We validate our approach on three\nbenchmark datasets and show that the proposed combina-\ntion of visual-temporal embedding and discriminative latent\nconcepts allow to learn robust action representations in an\nunsupervised setting.\n1. Introduction\nRecent years have seen a great progress in video activ-\nity analysis. However, most of this research is focused on\nthe classiﬁcation of short video clips with atomic or short-\nrange actions [1, 2, 3]. This is a relatively easier task when\ncompared with analysis of untrimmed and complex video\nsequences [4, 5, 6, 7, 8, 9, 10, 11, 12]. In untrimmed video\nanalysis, the focus is either on the problem of temporal ac-\ntion localization [13, 14, 15, 16], where only a set of key\nactions is considered in untrimmed videos; or on the task of\nFigure 1: Overview of the proposed approach. Given videos of a\ncomplex activity, the proposed model learns sub-actions as latent\nconcepts in an end-to-end manner. The latent concept assignment\nfor each input video segment feature forms sub-action prediction\nshown as ‘Initial Predictions’, which is then reﬁned using Viterbi\nto generate ‘Final Predictions’. Sample results for activity ‘Make\nChocolate Milk’, it can be seen that the latent concepts are able\nto group sub-actions. The sub-action ‘pour-milk’ includes lifting\nbottle and pouring milk; the jitter can be associated to the confu-\nsion when either a chocolate/milk bottle is lifted.\ntemporal action segmentation [9, 10, 17, 18, 19, 20], where\neach frame of the video is associated with a respective sub-\naction class as it requires to identify sub-actions and also\ntemporally localize them.\nExisting works on temporal action segmentation mainly\nexplore supervised approaches where frame-level annota-\ntions are required for all the sub-actions\n[21, 8, 12, 9,\n22, 11, 23]. However, complex activities are usually long-\nranged and obtaining frame-level annotation is arduous and\nexpensive. A new line of research focuses on learning these\nsub-actions from videos of a complex activity in an unsu-\npervised setting [4, 5, 19, 10, 24] . In the unsupervised set-\nting, the problem is even more challenging as it requires (i)\nbreaking down a complex activity video into semantically\nmeaningful sub-actions; and (ii) capturing the temporal re-\nlationship between the sub-actions. Most approaches tackle\nthis problem in two stages, where during the ﬁrst stage an\n1\narXiv:2105.00067v1  [cs.CV]  30 Apr 2021\nembedding based on visual and/or temporal information is\nlearned, and in the second stage clustering is applied on this\nembedding space. This limits the learning ability by pre-\nventing the embedding to actually learn from clustering. At\nthe same time, performing explicit clustering which is inde-\npendent of embedding learning makes the model less efﬁ-\ncient and does not utilize end-to-end learning.\nTo address this problem, we propose an end-to-end ap-\nproach where sub-actions are learned by combining embed-\nding and latent concepts.\nHere, the embedding space is\ntrained jointly with the latent concepts leading to an effec-\ntive sub-action discovery as shown in Figure 1. To allow\nfor such a joint training, we propose a novel discriminative\nlatent concept learning (DLCL) module which combines la-\ntent concept learning with a contrastive loss to ensure that\nthe sub-actions learnt in the latent embedding are distant.\nThe resulting latent concept vectors can be interpreted as\ncluster centers, removing the need for explicit clustering at\na later stage.\nAs the sub-actions are softly bound to the temporal po-\nsition of each activity, incorporating temporal ordering is\ncrucial. Recent works [10, 19] incorporated temporal em-\nbedding either by predicting the discrete temporal entities\nor by learning continuous temporal embedding with shallow\nMLP architectures. In those cases, the temporal information\nis only given by a discrete or continuous scalar value and\nthe joint embedding space is constructed by predicting this\nvalue from the input. To learn better spatio-temporal rep-\nresentations, we propose to use temporal position encod-\ning [25] instead of scalar values and learn the respective\nembedding space by jointly reconstructing both visual and\ntemporal representations. This embedding is further trained\njointly with constrastive loss of the latent concept module,\nso that the embedding is also guided by and contributes to\noverall clustering.\nWe evaluate our method on three benchmark datasets:\nBreakfast [8], 50Salads [26] and YouTube Instructions [5].\nFor the evaluation at test time, we follow the protocol from\n[19] and employ the Viterbi algorithm to decode the initial\nsub-action predictions into coherent segments based on the\nordered clustering of the sub-action latent concepts. A de-\ntailed analysis shows the impact of the proposed elements,\nthe reconstruction and well as the latent concept learning.\nIn summary, we propose a novel end-to-end unsuper-\nvised approach for sub-action learning in complex activi-\nties. We make the following contributions in this work:\n• We propose an unsupervised end-to-end approach for\nsub-action learning in complex activities by jointly\nlearning an embedding which simultaneously incorpo-\nrates visual and temporal information.\n• We learn discriminative latent concepts using con-\ntrastive loss, thus integrating clustering as part of latent\nembedding learning.\n• Our method improves the state-of-the-art on three\nbenchmark datasets.\n2. Related Work\nRecently, there has been a lot of interest in learning with\nless supervision. This is essential for both action [3, 1, 2]\nand complex activity understanding [12, 27, 11], as super-\nvised approaches require a large number of frames to be\nannotated in videos, which is expensive, tedious and cannot\nbe scaled to large datasets. Weakly supervised approaches\nuse a video and readily available information like accompa-\nnying text narration or audio. Some works [28, 29] use as-\nsociated text narrations or scripts for learning actions in the\nvideo. Another line of work with weak-supervision include\nthe works where it is assumed that the order of sub-actions\nis known [17, 30, 31, 32], however the per-frame annota-\ntions between video and sub-actions are not known during\ntraining. Authors in [33] propose to use combination of au-\ndio, text and video to identify steps in instructional videos\nin kitchen setting. The performance of the above methods\nis highly dependent on both the availability and quality of\nthe text/audio alignment to video, which is not guaranteed\nand heavily limit their application.\nThere have been some works, in which the assump-\ntion of weak supervision have been removed in learning\nof action classes. One of the ﬁrst works with no super-\nvision addressed the problem of human motion segmenta-\ntion [34] based on sensory-motor data, and proposed an\napplication of a parallel synchronous grammar, to learn\nsimple action representations similar to words in language.\nLater, a Bayesian non-parametric approach to concurrently\nmodel multiple sets of time series data was proposed in\n[35].\nHowever, this work only focuses on motion cap-\nture data.\n[36, 37] beneﬁt from the temporal structure of\nvideos to ﬁne-tune networks without any labels. Addition-\nally, [38, 39, 40, 41] also leverage the temporal structure of\nvideos to learn feature representation to learn actions.\nRecently, unsupervised approaches have been proposed\nto learn sub-actions in complex activity. [10, 19, 24] pro-\npose unsupervised approaches for temporal segmentation\nof complex activity into sub-actions. While [4] proposes\nto solve a variant of the problem where the goal is to de-\ntect event boundaries, i.e. event boundary segmentation for\ncomplex activities. This does not focus on identifying sub-\nactions, instead it learns to identify boundaries across multi-\nple sub-actions in long videos. A self-supervised predictive\nlearning framework is proposed to solve by utilizing the dif-\nference between observed and predicted frame features to\ndetermine event boundaries in complex activities.\nIn this work, we focus on solving the temporal seg-\nmentation of complex activity into sub-actions as shown\nin [10, 19, 24]. In [10], an iterative approach is proposed\nthat alternates between discriminative learning and gener-\n2\nFigure 2: Overview of the proposed model. Given videos for a complex activity, we extract visual features (Xnm) and compute positional\nencoding vectors (ρnm) which are fed to the encoder to map them to a joint latent embedding for learning sub-action clusters. To learn\nthese sub-action clusters as latent concepts bY , an attention block (D) is used which takes in randomly initialized vectors (Y ) along with\nφnm and learns the latent concepts. We use contrastive loss to learn bY discriminatively in B. Here α, znm and bYk represent attention,\nlatent vector for input Xnm and kth latent concept respectively.\native modeling. For discriminative learning, they map the\nvisual features into latent space using a ‘linear’ transforma-\ntion and compute the weight matrix which minimizes the\npairwise ranking loss. For temporal ordering they use Gen-\neralized Mallows Model which models distributions over\npermutations as they formulate complex activity as a se-\nquence of permutable sub-actions. In [19], the model in-\ncorporates the continuous temporal ordering of frames in\na joint embedding space. This is achieved by training a\nregressor to predict the timestamp of frames in a video.\nThe hidden layer representations are used as the embedding\nfor clustering and the clusters are ordered with respect to\ntheir time stamp. We refer to this model as CTE (Continu-\nous Temporal Embedding). In [24], two-stage embedding\npipeline is proposed where a next frame prediction U-Net\nmodel in stage one is combined with with temporal discrim-\ninator in stage two followed by clustering. The temporal\nembedding model employed is similar to [19].\nLatent embedding learning is crucial for unsupervised\nlearning, recently [7] formulated learning graph based la-\ntent embedding using latent concepts for supervised classi-\nﬁcation of complex activities. The intuition was to model\nlong range videos using latent concepts as graphical nodes\nfor complex activity recognition. Inspired by their ideology\nof latent concept learning to model latent space, we propose\nDLCL as an unsupervised latent learning module with joint\nembedding learning to model sub-actions.\nMost of the above works in unsupervised learning in-\nvolve two stage process which does not utilize end-to-end\nlearning making them less efﬁcient as clustering is inde-\npendent of embedding learning. In this work, we present an\nend-to-end model where clustering is incorporated in em-\nbedding learning using a constrastive loss. To incorporate\ntemporal ordering we propose to use positional encodings\nand we also propose an effective way to unify visual and\ntemporal representations to learn a visual-temporal embed-\nding by jointly reconstructing visual and temporal represen-\ntations. The proposed latent embedding is not only better at\ncapturing visual & temporal representations but also clus-\ntering friendly. We demonstrate later in this paper the use-\nfulness of the proposed model both qualitatively and quan-\ntitatively.\n3. Proposed Model\n3.1. Overview\nGiven a set of N videos, {Vn}N\nn=1, for a complex ac-\ntivity, we divide each video into segments and for each\nsegment we extract I3D features [1] , and compute posi-\ntional encoding vectors [25] as described in Section 3.2.\nEach video is represented by Mn features where Xnm rep-\nresent the mth feature in the nth video, and its correspond-\ning positional encoding is represented by ρnm. The task\nis to learn the sub-actions and their ordering for each ac-\ntivity, i.e., by predicting sequence of a sub-action labels\nlnm ∈{1, 2, ..., K} for each feature Xnm for each video.\nThe number of sub-actions labels K for each activity is the\nmaximum number of possible sub-actions as they occur in\nthat activity.\nOverview of our proposed model is shown in Figure 2.\n3\nFirst we learn an encoded representation of Xnm and ρnm\nshown as φnm, which is passed as input feature to the\n‘Attention Block’ (shown as D in Figure 2) to learn the\nlatent concepts/clusters which are representative of sub-\nactions.\nThe attention block learns the latent concepts\nbY ∈{bY1, bY2, ..., bYK} discriminatively, where each input\nfeature (φnm) is assigned to only one latent concept. We\nuse a combination of reconstruction loss and constrastive\nloss to learn the embedding(shown as B in Figure 2). We\nbelieve that using a combination of both visual and temporal\ninformation in conjunction with latent concept learning to\nlearn a latent embedding (shown as block B in ﬁg. 2) makes\nour model more robust. We evaluate the performance of our\nmodel based on latent concept assignments for each input\nfeature which forms ‘Initial Predictions’. Then, we model\nthe sub-action transitions and perform Viterbi decoding to\nestimate optimal sub-action ordering.\nNote that unlike previous works [19], we do not per-\nform explicit clustering, instead our model learns to cluster\nfeatures in latent space as discussed in Section 3.2 & 3.3.\nThus eliminating the need for all the data to be available at\nonce, our model can learn the latent concepts incrementally.\nThe resulting sub-action latent concepts are temporally or-\ndered and then each video is decoded w.r.t the above order-\ning given initial sub-action probability assignments for each\nclip to each latent concept as described in Section 3.5.\n3.2. Joint Visual and Temporal Latent Embedding\nIn unsupervised learning, the approach to learn clusters\nin latent space plays a critical role in learning semantically\nmeaningful clusters. We employ an encoder-decoder model\nto obtain the latent representation. Skip-connections are in-\ncluded between encoder and decoder (shown as C in Fig-\nure 2), as they help to preserve commonality of an action\nand reduce redundant information like background in latent\nrepresentation.\nFor incorporating temporal ordering in our model, we\nemploy the positional encodings inspired by [25]. We di-\nvide the video segment sequence into g equal groups and\nthen use the ordering index to compute positional encod-\ning vectors. Quantizing temporal index of the video clip\nand using a positional encoding not only captures relative\npositioning but also makes it easy to generalize for highly\nvarying video lengths.\nThe idea of learning a mapping\nfrom features to joint visual and temporal embedding with\nan encoder-decoder aids in grouping clips into sub-actions\nin the latent space. The reconstruction loss for the auto-\nencoder is composed of visual features and positional en-\ncoding as shown below,\nLossr = L(Xnm, X′\nnm) + β ∗L(ρnm, ρ′\nnm),\n(1)\nwhere, Xnm, X′\nnm respectively represent input and recon-\nstructed visual feature; ρnm, ρ′\nnm respectively represent in-\nput and reconstructed positional encoding; β is a hyperpa-\nrameter and L is a loss function penalizing X′\nnm and ρ′\nnm for\nbeing dissimilar from Xnm and ρnm respectively, namely\nmean squared error. A combination of latent visual feature\nrepresentation and the positional encodings becomes input\nto the ‘Attention Block’. In order to ensure that the learnt\nclusters are representative of sub-actions, the clusters have\nto be distant in the latent space, which is described in the\nnext section.\n3.3. Discriminative Latent Concept Learning\nThe idea behind having this module is to learn the sub-\naction clusters discriminatively in the latent space in an end-\nto-end fashion, eliminating the need for explicit clustering.\nThe attention block is inspired by [7], which takes an input\nfeature (φnm) and randomly initialized latent vectors (Y )\nwhich is analogous to cluster center initialization as shown\nin Figure 2. The latent concepts (bY ) are learnt using an\nMLP with weight (w) and bias (b) i.e., it transforms the\nrandom latent vector initializations (Y ) to latent concepts\n(bY ) as bY = w ∗Y + b. Though latent vector initializa-\ntion (Y ) is ﬁxed, w & b are learnable parameters making\nthe latent concepts (bY ) learnable in the latent space. These\nlatent concepts which represent cluster centers are learned\nby minimizing the contrastive loss by moving features in\nthe latent space closer to the latent concepts. The similar-\nity between input feature (φnm) and latent concepts (bY ) is\nmeasured with the dot product ⊗. Then, activation func-\ntion σ is applied on the similarities to compute activation\nvalues α i.e., α = σ(φnm ∗bY T ). Finally, the attended la-\ntent vector representation is computed as Znm = α ⊙bY ,\nwhich captures how much each latent concept is related to\nthe given input feature. However, these latent concepts tend\nto learn similar/overlapping concepts, which is not what we\nintend to learn. Our objective in learning the latent con-\ncepts is to cluster the latent representations discriminatively.\nWe achieve this with a contrastive loss, where the similar-\nity between latent vectors of the same sub-action with the\nmaximum conﬁdent latent concept is maximized, while the\nsimilarity w.r.t other latent concepts is minimized as shown\nin Eq 2.\nLossd(Znm, bY ) = −log\nesim(Znm,bYk∗)\nP\nk̸=k∗esim(Znm,bYk)\n(2)\nwhere, bYk represents the latent concept associated with kth\nsub-action, sim denotes cosine similarity and k∗represents\nthe latent concept with maximum conﬁdence probability for\nZnm as shown below\nk∗= argmax\nk\nP(k|Znm)\n(3)\nwhere P(k|Znm) = σ(sim(Znm, bYk)) represents the conﬁ-\ndence probability of latent vector Znm for the latent concept\nbYk, σ is softmax activation.\n4\nFigure 3:\nQualitative comparison of initial predictions (w/o\nviterbi) and after Viterbi predictions of our approach for activity\n‘Make Tea’. It can be seen that our model (‘Init’) is able to group\nsub-actions and also learn the ordering of sub-actions for an ac-\ntivity. The jitter in sub-action predictions occurs during transition\nfrom one sub-action to next, which is expected during transition.\nFinally, using transition modeling Viterbi decoding smoothness\nthe jitter between sub-action transitions.\n3.4. Overall Loss.\nTotal loss for learning the proposed embedding is com-\nposed of losses from Section 3.2 and 3.3 as Loss = λ ∗\nLossr + γ ∗Lossd\n3.5. Temporal Segmentation\nInitial Predictions At test time, we ﬁrst assign each feature\nin video to its respective closest latent concept vector using\nEq 3. This gives initial predictions directly based on the\nembedding (shown as predictions in Figure 1). For ease of\nunderstanding, we refer to those as latent sets, analogous to\nclusters, from here on.\nTransition modeling and Viterbi decoding\nFigure 4 represents a brief outline for transition modeling\nand Viterbi decoding. To allow for a temporal decoding,\nthe global ordering of the latent sets needs to be estimated.\nWe follow here the protocol proposed by [19] and compute\nthe mean timestamp for each set (shown as T in Figure 4)\nand sort them in ascending order. The last set in the sorted\nordering becomes the terminal state and using this ordering\nthe sub-action state transition probabilities from sub-action\ni to j are deﬁned as P(j|i) given:\nP(j|i) =\n\n\n\n\n\n\n\n\n\n0.5,\nif j immediately follows i\n0.5,\nif i = j\n1.0,\nif i = j & j is terminal state\n0,\notherwise\n(4)\nDecoding\nFinally, we use the ordering and transition\nprobabilities to compute the best path for the set ordering\ngiven the input features Xnm and ρnm. Using Eq. 3 we com-\npute the probability of each embedded input feature (Znm)\nbelonging to the latent set k. We maximize the probability\nof the input sequence following the order deﬁned by Eq. 4\nFigure 4: Brief overview of transition modeling and Viterbi de-\ncoding. Each latent concept is color coded (best viewed in color).\nThe latent concepts are ordered w.r.t the mean time (shown as T)\nand each video is decoded into coherent segments using Viterbi\nalgorithm based on the ordered sub-action latent concepts.\nto get consistent latent set assignments in a video by maxi-\nmizing,\n¯lMn\n1\n= argmax\nl1,...,lMn\nMn\nY\nm=1\nP(lm|lm−1) ∗P(lm|Znm),\n(5)\nwhere l1, ..., lm ∈{1, 2, ..., K} represent the set label se-\nquence for nth video, P(lm = k|Znm) is the probability\nthat Znm belongs to the kth latent set (as described in Sec-\ntion 3.3), ¯lMn\n1\nis the set label sequence for the maximum\nlikelihood for nth video.\n4. Experiments\nFor our experiments, we deﬁne a segment as a sequence\nof 8 frames. The video segment sequence is divided into\n128 equal groups and then the ordering index is used to\ncompute positional encoding [25] for each segment. We ex-\ntract I3D features (layer ‘mixed 5c’) which is fed to the en-\ncoder. Our embedding dimension is 1024. We use a 3-layer\nencoder-decoder with Adam optimizer and the learning rate\nis set to 1 × 10−4. We evaluate our approach on 3 datasets.\nBreakfast Dataset comprises of 10 complex activities of\nhumans performing common kitchen activities. There are\na total of 48 sub-activities in 1, 712 videos with varying\nlengths based on activity and preparation style with vari-\nations in sub-action orderings.\n50Salads Dataset contains videos of duration 4.5 hours for\na single complex activity ‘making mixed salad’. It is a mul-\ntimodal dataset, as it includes RGB frames, depth maps and\naccelerometer data. However, we only use RGB frames. The\nvideos in this dataset are much longer with average frame\nlength of 10k frames and provides annotations at multiple\ngranularity levels.\nYouTube Instructions Dataset has 5 activities and 150\nvideos with 47 sub-actions. These videos are taken from\n5\n(a) Make Cereals\n(b) Make Chocolate Milk\nFigure 5: Illustrative comparison with state-of-the-art. By com-\nparing with CTE (Init) and Ours (Init), we show that our approach\nlearns to model sub-actions with very few intermittent sub-action\ntransitions leading to effective grouping of sub-actions.\nThen,\nViterbi decoding helps to smoothen the intermittent jitters in pre-\ndictions. We show that our method provides coherent sub-action\npredictions and is able to capture the orderings for sub-actions.\nMethod\nF1-score\nMoF\nWeakly Supervised\nRNN-FC [42]\n33.3%\nTCFPN [17]\n38.4%\nNN-Vit [32]\n43%\nD3TW [43]\n45.7%\nCDFL [44]\n50.2%\nUnsupervised\nMallow [10]\n-\n34.6%\nCTE [19]\n26.4%\n41.8%\nJVT [24]\n29.9%\n48.1%\nOurs\n31.9%\n47.4%\nLSTM-AL [4]*\n-\n42.9%*\nOurs*\n-\n74.6%*\nTable 1: Comparison of the proposed method to state-of-the-art\non Breakfast dataset. Here, * denotes results with video-based\nHungarian matching for the task event boundary segmentation.\nYouTube directly and have background segments where\nthere is no sub-action. The frequency and spread of back-\nground varies based on activity as well as on the person\nperforming the task. Hence, the background segments nei-\nther have similar appearance nor have a temporal ordering.\nTherefore the background segments would be assigned to\nthe latent concepts with very less conﬁdence probability.\nFollowing protocol in [19], we consider τ percent of clips\nwith least conﬁdence as background. Only the foreground\nlabeled segments along with latent concepts assignments\nform our initial predictions.\nWe report results for back-\nground ratio of 60%.\nMethod\nMoC\nMoC\nw/o Viterbi\nw Viterbi\nCTE [19] with FV\n20.9%\n40.1%\nCTE [19] with I3D\n24.8%\n36.8%\nOurs with I3D\n37.5%\n46.9%\nTable 2: Comparison of MoC (Mean over class) of all activities\non Breakfast dataset before and after applying Viterbi. FV repre-\nsents Fisher Vectors.\nMetrics Our model predicts a sequence of cluster labels\n∈{1, 2, ..., K} for each video without any correspondence\nto the K ground-truth class labels. To map ground-truth and\nprediction label correspondences, inline with [5, 10, 19],\nfor each activity we use the Hungarian method to ﬁnd a one-\nto-one mapping for each cluster to exactly one sub-action\nand report performance after this mapping. In this work, we\nuse Mean over Frames (MoF) as used by [10, 19] as well\nas F1-score used by [5]. In addition, we report Mean over\nclass (MoC) accuracy, as it averages the accuracy for each\nactivity class, therefore giving equal weights to all classes\nirrespective of the underlying data distribution. MoF is the\npercentage of correct predictions computed across all activ-\nity classes together, which can be affected by the underlying\nactivity classes distribution and biased towards dominant\nactivity class. For F1-score, similar to previous methods,\nwe report the mean score over all activities. For state-of-\nthe-art comparisons, we also evaluate our method for the\ntask of event boundary segmentation following the proto-\ncol in [4] and compare our method to [4] - indicated as\nvideo-based Hungarian matching.\n4.1. Comparison to state-of-the-art\nHere, we compare the proposed method to state-of-the-\nart approaches. We present the accuracy comparison with\nrecent works on Breakfast dataset in Table 1 and present the\nperformance on new metric MoC in Table 2. Our approach\nachieves 47.4% MoF and 31.9% F1-score which is 2% gain\nover state-of-the-art as shown in table 1. We show qualita-\ntive evaluation of the proposed approach in Figure 3 & 5. In\nFigure 5, we show that our approach models the sub-actions\ncoherently with very less intermittent sub-action transitions\nalong with learning ordering of sub-actions for complex ac-\ntivity. For example, in Figure 3 our model predicts ‘stir-tea’\nwith intermittent transitions after ‘pour-water’, this occurs\nwhen the person dips the tea bag in water which closely\nresembles to the sub-action ‘stir-tea’ (as shown in last im-\nage in Figure 3) and then it correctly predicts background\nonce the dip action ends (there is no annotation for ‘dip-\nping tea-bag’ in ground truth) indicating the goodness of the\nproposed sub-action learning. The intermittent transitions\nindicate that the model confuses to assign latent concept\n6\nMethod\nF1-score\nMoF\nCTE [19]\n-\n35.5%\nJVT [24]\n-\n30.6%\nOurs\n34.4%\n42.2%\nLSTM-AL [4]*\n-\n60.6*%\nOurs*\n-\n70.2%*\nTable 3: Comparison of the proposed method to state-of-the-art\nunsupervised approaches on 50Salads dataset at granularity ‘eval’.\nHere, * denotes results with video-based Hungarian matching for\nthe task event boundary segmentation.\nbased on single feature and Viterbi aids in generating more\ncoherent sub-action segments for the sequence as shown.\nAdditionally, we evaluate our method for the task of event\nboundary segmentation and compare with the state-of-the-\nart approaches. Our approach out-performs the state-of-the-\nart MoF by a margin of 31% on Breakfast dataset indicating\nthe effectiveness of the proposed method to temporally seg-\nment meaningful sub-actions.\nFor 50Salads dataset, we perform evaluation on granu-\nlarity level ‘eval’ and provide state-of-the-art comparison in\nTable 3. Our method out-performs [19] by 6.67% and [24]\nby 11.6% with an F1-score of 34.37%. We further eval-\nuate our method for the task of event boundary segmen-\ntation and perform state-of-the-art comparison in Table 3.\nWe show 10% gain over state-of-the-art [4] MoF, indicating\nour method is effective in sub-action learning for complex\nevents.\nFor YouTube Instructions dataset, we follow protocol in\n[5, 10, 19] and report the performance of our approach with-\nout considering the background frames. We achieve 42%\nMoC & 43.8% MoF (as shown in Table 4). This is a 4.8%\ngain in MoF over state-of-the-art method with comparable\nF1-score. Note that [4] reported F1-score with background\nframes included on YouTube Instructions Dataset. We fol-\nlow the same procedure and compare our method to [4] in\nTable 4 (indicated with *). It can be seen that our method\noutperforms the state-of-the-art for event boundary segmen-\ntation task showing the sub-action learning capability to\nidentify better event boundaries.\n4.2. Evaluation of the Embedding.\nTo demonstrate the impact of the proposed embedding,\nwe compare our Joint Embedding with Continuous Tempo-\nral Embedding in [19] in Table 2. From Table 2 (MoC w/o\nviterbi), it can be seen that the proposed joint embedding\noutperforms the continuous temporal embedding by a huge\nmargin of 16.6%. It can be seen that our ‘MoC w/o viterbi’\nis closer to the CTE ‘MoC w Viterbi’ suggesting that our\nembedding is very effective. To emphasize that our gain in\nMethod\nF1-score\nMoF\nFrank-Wolfe [5]\n24.4%\n34.6%\nMallow [10]\n27.0%\n27.8%\nCTE [19]\n28.3%\n39.0%\nJVT [24]\n29.9%\n28.2%\nOurs\n29.6%\n43.8%\nLSTM-AL [4]*\n39.7%*\n-\nOurs*\n45.4%*\n-\nTable 4: Comparison of the proposed method to state-of-the-art\nunsupervised methods on YouTube Instructions dataset. Here, *\ndenotes results with video-based Hungarian matching for the task\nevent boundary segmentation.\nperformance is due to the effectiveness of the approach and\nnot with using I3D features, we train [19] using I3D fea-\ntures by keeping the embedding dimension same as ours and\ncompare the performance. As shown in Table 2, the MoC\nw/o Viterbi improves by 4% by using I3D features on CTE,\nwhile the MoC with Viterbi drops by 3% with 1% increase\nin F1-score. However, our approach still outperforms the\nbaseline (with same embedding dimension) by huge margin\nindicating our approach effectiveness.\nBesides dataset level comparisons, we also show activity\nlevel comparison with CTE [19]. Figure 6 (a) shows that our\njoint embedding outperforms CTE on all activities indicat-\ning the signiﬁcance of our joint embedding. We see a drop\nin performance for activity ‘making cereals’ after Viterbi\ndecoding (from Figure 6(b)), this can be attributed to the\nordering of the sub-actions ‘take-bowl’ and ‘pour-cereals’.\nFor many samples in ‘making cereals’, the sub-action ‘take-\nbowl’ does not occur impacting the ordering of both sub-\nactions leading to drop in performance.\n4.3. Ablation Experiments\nWe perform the below ablation studies on the breakfast\ndataset.\nEffect of Loss Components. To begin with, we ﬁrst exam-\nine the inﬂuence of Lossr and Lossd on our model and the\nperformances are presented in Table 5. It can be seen that\nhaving all loss components leads to best performance.\nEffect of Discriminative learning. The use of constrastive\nloss (Lossd) helps the clusters to move apart in the latent\nspace. This helps in obtaining more discrete boundaries in\nthe latent space. As shown in Table 5, the accuracy drasti-\ncally reduces to 35.8% (11% ↓) indicating the importance\nof discriminative learning.\nEffect of Positional Encoding. Positional Encoding plays\na crucial role in our model. It helps to temporally group\nthe video clips in the latent space. As sub-actions are softly\nbound to the temporal position for each activity, removing\n7\n(a) MoF w/o Viterbi\n(b) Final MoF\nFigure 6: Activity level MoF comparison on Breakfast dataset\nwith CTE [19]. Last column represents the average (MoC) for all\nactivities. (a) represents MoF for each activity without Viterbi i.e,\nthe MoF is computed based on the learnt cluster assignments. Our\nmethod outperforms the baseline on all activities. (b) represents\nMoF for each activity after applying Viterbi.\nLossr\nLossd\nMoC\nLf\nLp\n\u0013\n-\n-\n25.7%\n-\n\u0013\n-\n33.6%\n\u0013\n\u0013\n-\n35.8%\n\u0013\n-\n\u0013\n40.2%\n-\n\u0013\n\u0013\n40.1%\n\u0013\n\u0013\n\u0013\n46.9%\nTable 5: Ablation experiments for the loss components are per-\nformed on the Breakfast dataset. Lossr and Lossd represents\nreconstruction loss and contrastive loss respectively. Lf and Lp\ndenote the reconstruction loss for feature and positional encoding\nrespectively.\nreconstruction loss for positional encoding is expected to\ndeteriorate the model performance. We observe the simi-\nlar trend in Table 5. Additionally, we perform an ablation\nby removing the PE component branch and train our model\nend-to-end. As expected, there is a signiﬁcant reduction in\naccuracy and F1-score (as shown in Table 6) indicating the\nsigniﬁcance of using positional encoding.\nEffect of Skip-Connections. To assess the effectiveness\nof skip-connections, we report performance by removing\nthe skip-connections and train model end-to-end. We re-\nFigure 7: MoF vs. #sub-actions for all activities in Breakfast\ndataset. k represents the number of sub-actions from ground-truth;\nwe vary the sub-actions for each activity and report MoF.\nport the performance in Table 6, it can be seen that w/o\nskip-connections, the accuracy drops considerably indicat-\ning that the skip-connections help in learning better repre-\nsentations.\nw/o PE\nw/o SC\nfull\nMoC\n40.9%\n35.7%\n46.9%\nF1-score\n20.3%\n28.7%\n31.9%\nTable 6: Ablations experiments to evaluate the effect of PE and\nSC on Breakfast dataset (w/o: without, PE: Positional Encoding,\nSC: skip-connections).\nEffect of Sub-actions Cluster Size. For all the above eval-\nuations, the sub-action cluster size (K) is deﬁned as men-\ntioned in Section 3.1. To analyze the impact of sub-action\ncluster size, we vary the number of sub-actions from K −2\nto K+2 where K is the number of sub-actions as per ground\ntruth and evaluate performance. Figure 7 shows the MoF vs\nnumber of sub-actions for each activity in Breakfast dataset.\nFor 6 out of 10 activities we see that having K sub-actions\nleads to best performance.\n5. Conclusion\nIn this work we proposed an end-to-end approach for\nunsupervised learning of sub-actions in complex activities.\nThe main motivation behind this approach is to design a la-\ntent space to incorporate visual as well as positional encod-\ning together. This latent space is learned via jointly train-\ning this embedding space in conjunction with a contrastive\nlearning for clustering. We show that this allows for a ro-\nbust learning that on it’s own already results in a reason-\nable clustering of sub-actions. We then predict optimal sub-\naction sequence by employing the Viterbi algorithm which\noutperforms all the other methods. Our evaluation shows\nthe impact of the proposed ideas and how they are able to\nimprove the performance on this task compared to existing\nmethods.\n8\nReferences\n[1] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\n2017.\n[2] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nICCV, 2019.\n[3] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos.\nIn\nNeurIPS, 2014.\n[4] Sathyanarayanan N Aakur and Sudeep Sarkar.\nA percep-\ntual prediction framework for self supervised event segmen-\ntation. In CVPR, 2019.\n[5] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,\nIvan Laptev, Josef Sivic, and Simon Lacoste-Julien. Unsu-\npervised learning from narrated instruction videos. In CVPR,\n2016.\n[6] Noureldien Hussein, Efstratios Gavves, and Arnold WM\nSmeulders. Timeception for complex action recognition. In\nCVPR, 2019.\n[7] Noureldien Hussein, Efstratios Gavves, and Arnold WM\nSmeulders. Videograph: Recognizing minutes-long human\nactivities in videos. arXiv, 2019.\n[8] H. Kuehne, A. B. Arslan, and T. Serre. The language of ac-\ntions: Recovering the syntax and semantics of goal-directed\nhuman activities. In CVPR, 2014.\n[9] Hilde Kuehne, Juergen Gall, and Thomas Serre. An end-to-\nend generative framework for video segmentation and recog-\nnition. In WACV, 2016.\n[10] Fadime Sener and Angela Yao. Unsupervised learning and\nsegmentation of complex activities from video. In CVPR,\n2018.\n[11] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki\nMiyazawa, and Shih-Fu Chang.\nCdc: Convolutional-de-\nconvolutional networks for precise temporal action localiza-\ntion in untrimmed videos. In CVPR, 2017.\n[12] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-\nFei.\nEnd-to-end learning of action detection from frame\nglimpses in videos. In CVPR, 2016.\n[13] Peihao Chen, Chuang Gan, Guangyao Shen, Wenbing\nHuang, Runhao Zeng, and Mingkui Tan. Relation attention\nfor temporal action localization. IEEE Transactions on Mul-\ntimedia, 2019.\n[14] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo\nLuo, and Tao Mei. Gaussian temporal awareness networks\nfor action localization. In CVPR, 2019.\n[15] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and\nBernard Ghanem. G-tad: Sub-graph localization for tempo-\nral action detection. arXiv, 2019.\n[16] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong,\nPeilin Zhao, Junzhou Huang, and Chuang Gan. Graph con-\nvolutional networks for temporal action localization.\nIn\nICCV, 2019.\n[17] Li Ding and Chenliang Xu. Weakly-supervised action seg-\nmentation with iterative soft boundary assignment. In CVPR,\n2018.\n[18] Alexander Richard, Hilde Kuehne, and Juergen Gall. Action\nsets: Weakly supervised action segmentation without order-\ning constraints. In CVPR, 2018.\n[19] Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen\nGall. Unsupervised learning of action classes with contin-\nuous temporal embedding. In CVPR, 2019.\n[20] Yazan Abu Farha and Jurgen Gall.\nMs-tcn: Multi-stage\ntemporal convolutional network for action segmentation. In\nCVPR, 2019.\n[21] Yazan Abu Farha and Jurgen Gall.\nMs-tcn: Multi-stage\ntemporal convolutional network for action segmentation. In\nCVPR, 2019.\n[22] Hilde Kuehne, Alexander Richard, and Juergen Gall. A hy-\nbrid rnn-hmm approach for weakly supervised temporal ac-\ntion segmentation.\nIEEE transactions on pattern analysis\nand machine intelligence, 2018.\n[23] Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, and\nGregory D. Hager. Temporal convolutional networks for ac-\ntion segmentation and detection. In CVPR, 2017.\n[24] Rosaura G VidalMata, Walter J Scheirer, Anna Kukleva,\nDavid Cox, and Hilde Kuehne. Joint visual-temporal em-\nbedding for unsupervised learning of actions in untrimmed\nsequences. In WACV, 2020.\n[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[26] Sebastian Stein and Stephen J McKenna. Combining em-\nbedded accelerometers with computer vision for recognizing\nfood preparation activities. In UBICOMP, 2013.\n[27] De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Con-\nnectionist temporal modeling for weakly supervised action\nlabeling. In ECCV, 2016.\n[28] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Ben-\njamin Rozenfeld.\nLearning realistic human actions from\nmovies. In CVPR, 2008.\n[29] Ozan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh\nSaxena. Unsupervised semantic parsing of video collections.\nIn ICCV, 2015.\n[30] Hilde Kuehne, Alexander Richard, and Juergen Gall. Weakly\nsupervised learning of actions from transcripts. CVIU, 2017.\n[31] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly\nsupervised action learning with rnn based ﬁne-to-coarse\nmodeling. In CVPR, 2017.\n[32] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juer-\ngen Gall. Neuralnetwork-viterbi: A framework for weakly\nsupervised video learning. In CVPR, 2018.\n[33] Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick\nJohnston, Andrew Rabinovich, and Kevin Murphy. What’s\ncookin’? interpreting cooking videos using text, speech and\nvision. arXiv, 2015.\n9\n[34] Gutemberg Guerra-Filho and Yiannis Aloimonos.\nA lan-\nguage for human action. Computer, 2007.\n[35] Emily B Fox, Michael C Hughes, Erik B Sudderth, Michael I\nJordan, et al. Joint modeling of multiple time series via the\nbeta process with application to motion capture segmenta-\ntion. The Annals of Applied Statistics, 2014.\n[36] Xiaolong Wang and Abhinav Gupta. Unsupervised learning\nof visual representations using videos. In ICCV, 2015.\n[37] Biagio Brattoli, Uta Buchler, Anna-Sophia Wahl, Martin E\nSchwab, and Bjorn Ommer. Lstm self-supervision for de-\ntailed behavior analysis. In CVPR, 2017.\n[38] Vignesh Ramanathan, Kevin Tang, Greg Mori, and Li Fei-\nFei. Learning temporal embeddings for complex video anal-\nysis. In ICCV, 2015.\n[39] Basura Fernando, Efstratios Gavves, Jose M Oramas, Amir\nGhodrati, and Tinne Tuytelaars. Modeling video evolution\nfor action recognition. In CVPR, 2015.\n[40] Anoop Cherian, Basura Fernando, Mehrtash Harandi, and\nStephen Gould. Generalized rank pooling for activity recog-\nnition. In CVPR, 2017.\n[41] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-\nHsuan Yang. Unsupervised representation learning by sort-\ning sequences. In ICCV, 2017.\n[42] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly\nsupervised action learning with rnn based ﬁne-to-coarse\nmodeling. In CVPR, 2017.\n[43] Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and\nJuan Carlos Niebles. D3tw: Discriminative differentiable dy-\nnamic time warping for weakly supervised action alignment\nand segmentation. In CVPR, 2019.\n[44] Jun Li, Peng Lei, and Sinisa Todorovic. Weakly supervised\nenergy-based learning for action segmentation.\nIn ICCV,\n2019.\n10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-04-30",
  "updated": "2021-04-30"
}