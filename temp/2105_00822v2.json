{
  "id": "http://arxiv.org/abs/2105.00822v2",
  "title": "Generative Adversarial Reward Learning for Generalized Behavior Tendency Inference",
  "authors": [
    "Xiaocong Chen",
    "Lina Yao",
    "Xianzhi Wang",
    "Aixin Sun",
    "Wenjie Zhang",
    "Quan Z. Sheng"
  ],
  "abstract": "Recent advances in reinforcement learning have inspired increasing interest\nin learning user modeling adaptively through dynamic interactions, e.g., in\nreinforcement learning based recommender systems. Reward function is crucial\nfor most of reinforcement learning applications as it can provide the guideline\nabout the optimization. However, current reinforcement-learning-based methods\nrely on manually-defined reward functions, which cannot adapt to dynamic and\nnoisy environments. Besides, they generally use task-specific reward functions\nthat sacrifice generalization ability. We propose a generative inverse\nreinforcement learning for user behavioral preference modelling, to address the\nabove issues. Instead of using predefined reward functions, our model can\nautomatically learn the rewards from user's actions based on discriminative\nactor-critic network and Wasserstein GAN. Our model provides a general way of\ncharacterizing and explaining underlying behavioral tendencies, and our\nexperiments show our method outperforms state-of-the-art methods in a variety\nof scenarios, namely traffic signal control, online recommender systems, and\nscanpath prediction.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nGenerative Adversarial Reward Learning for\nGeneralized Behavior Tendency Inference\nXiaocong Chen, Lina Yao, Member, IEEE, Xianzhi Wang, Member, IEEE, Aixin Sun, Member, IEEE,\nWenjie Zhang, Member, IEEE, Quan Z. Sheng, Member, IEEE\nAbstract—Recent advances in reinforcement learning have inspired increasing interest in learning user modeling adaptively through\ndynamic interactions, e.g., in reinforcement learning based recommender systems. Reward function is crucial for most of reinforcement\nlearning applications as it can provide the guideline about the optimization. However, current reinforcement-learning-based methods\nrely on manually-deﬁned reward functions, which cannot adapt to dynamic and noisy environments. Besides, they generally use\ntask-speciﬁc reward functions that sacriﬁce generalization ability. We propose a generative inverse reinforcement learning for user\nbehavioral preference modelling, to address the above issues. Instead of using predeﬁned reward functions, our model can\nautomatically learn the rewards from user’s actions based on discriminative actor-critic network and Wasserstein GAN. Our model\nprovides a general way of characterizing and explaining underlying behavioral tendencies, and our experiments show our method\noutperforms state-of-the-art methods in a variety of scenarios, namely trafﬁc signal control, online recommender systems, and\nscanpath prediction.\nIndex Terms—Inverse Reinforcement Learning, Behavioral Tendency Modeling, Adversarial Training, Generative Model\n!\n1\nINTRODUCTION\nB\nEHAVIOR modeling provides a footprint about user’s\nbehaviors and preferences. It is a cornerstone of di-\nverse downstream applications that support personalized\nservices and predictive decision-making, such as human-\nrobot interactions, recommender systems, and intelligent\ntransportation systems. Recommender systems generally\nuse users’ past activities to predict their future interest\n[1], [2], [3]; past studies integrate demographic information\nwith user’s long-term interest on personalized tasks [4],\n[5], [6], [7]. In human-robot interaction, a robot learns\nfrom user behaviors to predict user’s activities and provide\nnecessary support [8]. Multimodal probabilistic models [9]\nand teacher-student network [10] are often used to predict\nuser’s intention for trafﬁc prediction or object segmentation.\nTravel behavior analysis ; it is a typical task in smart-city\napplications [11], [12].\nTraditional methods learn static behavioral tendencies\nvia modeling user’s historical activities with items as a\nfeature space [13] or a user-item matrix [14]. In contrast,\nreinforcement learning shows advantages in learning user’s\npreference or behavioral tendency through dynamic interac-\ntions between agent and environment. It has attracted lots of\nresearch interests in recommendation systems [6], intention\nprediction [15], trafﬁc control [16], and human-robot inter-\naction domains [17]. Reinforcement learning covers several\ncategories of methods, such as value-based methods, policy-\n•\nX. Chen, L. Yao and W. Zhang are with the School of Computer Science\nand Engineering, University of New South Wales, Sydney, NSW, 2052,\nAustralia.\nE-mail: {xiaocong.chen, lina.yao, wenjie.zhang}@unsw.edu.au\n•\nX. Wang is with School of Computer Science, University of Technology\nSydney, Sydney, NSW, 2007, Australia.\n•\nA. Sun is with Nanyang Technological University, Singapore.\n•\nQ. Sheng is with Department of Computing, Macquarie University,\nSydney, NSW, 2109, Australia.\nbased methods, and hybrid methods. All these methods\nuse the accumulated reward during a long term to indicate\nuser’s activities. The reward function is manually deﬁned\nand requires extensive effort to contemplate potential fac-\ntors.\nIn general, user’s activities are noisy, occasionally con-\ntaminated by imperfect user behaviors and thus may not\nalways reveal user’s interest or intention. For example, in\nonline shopping, a user may follow a clear logic to buy items\nand randomly add additional items because of promotion\nor discounts. This makes it difﬁcult to deﬁne an accurate\nreward function because the noises also affect the fulﬁllment\nof task goals in reinforcement learning. Another challenge\nlies in the common practice of adding task-speciﬁc terms\nto the reward function to cope with different tasks. Cur-\nrent studies usually require manually adjusting the reward\nfunction to model user’s proﬁles [2], [18], [19]. Manual\nadjustment tends to produce imperfect results because it is\nunrealistic to consider all reward function possibilities, not\nto mention designing reward functions for new tasks.\nA better way to determine the reward function is to learn\nit automatically through dynamic agent-environment in-\nteractions. Inverse reinforcement learning recently emerges\nas an appealing solution, which learns reward function\nlearning from demonstrations in a few scenarios [20]. How-\never, inverse reinforcement learning faces two challenges\nfor user behavior modeling. First, it requires a repeated,\ncomputational expensive reinforcement learning process to\napply a learned reward function [21]; second, given an\nexpert policy, there could be countless reward functions\nfor choice, making the selection of reward function difﬁcult\nand the optimization computationally expensive. The only\nrecommendation model [22] that adopts improved inverse\nreinforcement learning simply skips the repeated reinforce-\nment learning process; thus, it is hard to converge as it\narXiv:2105.00822v2  [cs.LG]  5 May 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nlacks sampling efﬁciency and training stability; furthermore,\nthe model only works for recommender systems and lacks\ngeneralization ability.\nWith such existing challenges, manually designed re-\nward function has less feasibility and generalizability. More-\nover, [22] employs inverse reinforcement learning to learn\nthe reward from demonstration which still suffers the un-\ndeﬁned problem due to the nature of the logarithm. To\nrelieve this, we manipulate the function by adding an extra\nlearnable term to avoid such a problem. In addition, existing\nworks do not consider the absorbing state problem such\nthat agents will stop learning once the absorbing states are\nreached. The major reason why is that agent will receive\nzero rewards in absorbing states and may lead to a sub-\noptimal policy.\nIn this paper, we aim to construct user models directly\nfrom an array of various demonstrations efﬁciently and\nadaptively, based on a generalized inverse reinforcement\nlearning method. Learning from demonstrations not only\navoids the need for inferring a reward function but also\nreduces computational complexity. To this end, we propose\na new model that employs a generative adversarial strategy\nto generate candidate reward functions and to approximate\nthe true reward. We use the new model as a general way of\ncharacterizing and explaining tendencies in user behaviors.\nIn summary, we make the following contributions:\n•\nWe propose a new inverse reinforcement-learning-\nbased method to capture user’s behavioral tenden-\ncies. To the best of our knowledge, this is the ﬁrst\nwork to formulate user’s behavioral tendency using\ninverse reinforcement learning.\n•\nWe design a novel stabilized sample-efﬁcient dis-\ncriminative actor-critic network with Wasserstein\nGAN to implement the proposed framework. Our\nframework is off-policy and can reduce interactions\nbetween system and environment to improve efﬁ-\nciency. Besides, we integrate a learnable term into\nour reward function to increase the capability of our\nmethod.\n•\nOur extensive experiments demonstrate the general-\nization ability and feasibility of our approach in three\ndifferent scenarios. We use visualization to show the\nexplainability of our method.\n2\nPROBLEM FORMULATION AND PRELIMINARY\nBehavioral tendency refers to user’s preferences at a certain\ntimestamp and is usually hard to be evaluated directly. The\ncommon way to evaluate behavioral tendencies is to exam-\nine how well the actions taken out of the learned behavioral\ntendencies match the real actions taken by the user. It is\nsimilar to reinforcement learning’s decision-making process,\nwhere the agent ﬁgures out an optimal policy π such that\neach action of it could achieve a good reward.\nIn this work, we deﬁne behavioral tendencies modeling\nas an optimal policy ﬁnding problem. Given a set of users\nU = {u0, u1, · · · , un}, a set of items O = {o0, o1, · · · , om}\nand user’s demographic information D = {d0, d1, · · · , dn}.\nWe ﬁrst deﬁne the Markov Decision Process (MDP) as a\ntuple (S, A, P, R, γ), where S is the state space (i.e., the\ncombination of the subset of O, subset of U and its cor-\nresponding D). A is the action space, which includes all\npossible agent’s decisions, R is a set of rewards received for\neach action a ∈A, P is a set of state transition probability,\nand γ is the discount factor used to balance the future\nreward and the current reward. The policy can be deﬁned\nas π : S →A—given a state s ∈S, π will return an\naction a ∈A so as to maximize the reward. However, it\nis unrealistic to ﬁnd a universal reward function for user be-\nhavioral tendency, which is highly task-dependent. Hence,\nwe employ Inverse reinforcement learning (IRL) to learn\na policy π from the demonstration from expert policy πE,\nwhich always results in user’s true behavior. We formulate\nthe IRL process using a uniform cost function c(s, a) [20]:\nminimize\nπ\nmax\nc∈C Eπ[c(s, a)] −EπE[c(s, a)]\n(1)\nThe cost function class C is restricted to convex sets de-\nﬁned by the linear combination of a few basis functions\n{f1, f2, · · · , fk}. Hence, given a state-action pair (s, a), the\ncorresponding feature vector can be represented as f(s, a) =\n[f1(s, a), f2(s, a), · · · , fk(s, a)]. Eπ[c(s, a)] is deﬁned as (on\nγ-discounted inﬁnite horizon):\nEπ[c(s, a)] = E[\n∞\nX\nt=0\nγtc(st, at)]\n(2)\nAccording to Eq.(1), the cost function class C is convex\nsets, which have two different formats: linear format [23]\nand convex format [24], respectively:\nCl =\nn X\ni\nwifi : ∥w∥2 ≤1\no\n(3)\nCc =\nn X\ni\nwifi :\nX\ni\nwi = 1, ∀i s.t. wi ≥0\no\n(4)\nThe corresponding objective functions are as follows:\n∥Eπ[f(s, a)] −EπE[f(s, a)]∥2\n(5)\nEπ[fj(s, a)] −EπE[fj(s, a)]\n(6)\nEq.(5) is known as feature expectation matching [23],\nwhich aims to minimize the l2 distance between the state-\naction pairs that are generated by learned policy π and ex-\npert policy πE. Eq.(6) aims to minimize the function fj such\nthat the worst-case should achieve a higher value [25]. Since\nEq.(1 suffers the feature ambiguity problem, we introduce\nγ-discounted causal entropy [26] (shown below) to relieve\nthe problem:\nH(π) ≜Eπ[−log π(a|s)] = Est,at∼π\n\u0014\n−\n∞\nX\nt=0\nγt log π(at|st)\n\u0015\n(7)\nAs such, Eq.(1) can be written by using the γ-discounted\ncausal entropy as:\nminimize\nπ\n−H(π) −EπE[c(s, a)] + max\nc∈C Eπ[c(s, a)]\n(8)\nSuppose Π is the policy set. We deﬁne the loss func-\ntion c(s, a) to ensure the expert policy receives the lowest\ncost while all the other learned policies get higher costs.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nFC\nGAE\nGAE\n...\n...\nExpected State-Action Pairs\nLearned State-Action Pairs\n...\n...\nReal World Tasks\nExpert Action  \n...\n...\n...\nOur Model\n1\n2\n3\n4\n5\n6\n7\n8\nFC\nFC\nFC\nFC\nSoftmax\nFC\nFC\nFC\nFC\nSoftmax\nPolicy\nAction\nState\nValue\nFully-Connected\nLayer\nUrban Mobility\nManagement\nRecommender System\nScanpath Prediction\nEnvironment-System Interaction\nLegend\nPPO Update\nFig. 1: Overall structure of the proposed framework. The left-hand side provides three example environments from top to\nbottom: urban mobility management, recommender system and scanpath prediction. The proposed model will interact with\nthe environment to achieve the corresponding state representations for current task. The expert actions will be achieved\nsimultaneously and feed into our model to participate the training procedure of the discriminator.\nReferring to Eq.(8), the maximum causal entropy inverse\nreinforcement learning [27] works as follows:\nmaximize\nc∈C\n(min\nπ∈Π −H(π) + Eπ[c(s, a)]) −EπE[c(s, a)]\n(9)\nThen, the policy set Π can be obtained via policy genera-\ntion. Policy generation is the problem of matching two occu-\npancy measures and can be solved by training a Generative\nAdversarial Network (GAN) [28]. The occupancy measure\nρ for policy π can be deﬁned as:\nρπ(s, a) = π(s|a)\n∞\nX\nt=0\nγtP(st = s|π)\n(10)\nWe adopts GAIL [21] and make an analogy from the occu-\npancy matching to distribution matching to bridge inverse\nreinforcement learning and GAN. A GA regularizer is de-\nsigned to restrict the entropy function:\nψGA(c(s, a)) =\n(\nEπE[−c(s, a) −log(1 −exp(c(s, a)))]\nc < 0\n∞\nc ≥0\n(11)\nThe GA regularizer enables us to measure the difference\nbetween the π and πE directly without the reward function:\nψGA(ρπ −ρπE) =\nmax\nD∈(0,1)S×A Eπ[log D(s, a)]\n+EπE[log(1 −D(s, a))]\n(12)\nThe loss function from the discriminator D is deﬁned\nas c(s, a) in Eq.(9); it uses negative log loss (commonly used\nfor binary classiﬁcation) to distinguish the policies π and πE\nvia state-action pairs. The optimal of Eq.(12) is equivalence\nto the Jensen-Shannon divergence [29]:\nDJS(ρπ, ρπE) = DKL(ρπ∥(ρπ + ρπE)/2)+\nDKL(ρπE∥(ρπ + ρπE)/2)\n(13)\nFinally, we rewrite inverse reinforcement learning by\nsubstituting the GA regularizer into Eq.(8):\nminimize\nπ\n−λH(π) + ψGA(ρπ −ρπE)\n|\n{z\n}\nDJS(ρπ,ρπE )\n(14)\nwhere λ is a factor with λ ≥0. Eq.(14) has the same goal\nas the GAN, i.e., ﬁnding the squared metric between distri-\nbutions. Eq.(14) can be further extended into the following,\nwhich serves as the objective function for GAIL:\nminimize\nπ\n−λH(π) + ψGA(ρπ −ρπE) ≡min\nπ max\nD LD\nLD = Eπ[log D(s, a)] + EπE[log(1 −D(s, a))] −λH(π)\n(15)\nWe summarized all the notations used in this paper in\nTable 1.\n3\nMETHODOLOGY\nThe overall structure of our proposed method (shown in\nFig. 1) consists of three components: policy and reward\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nTABLE 1: Main notations\nSymbols\nMeaning\nU\nSet of users\nO\nSet of items\nR\nSet of rewards received\nD\nSet of demographic information\n| · |\nNumber of unique elements in ·\nγ\nDiscount Factor\nH(π)\nγ-discounted casual entropy\nE\nExpectation\nρ\nOccupancy Measure\nSt\nState space at timestamp t\nat\nAction space at timestamp t\nπ\nPolicy\nπE\nExpert Policy\nD\nDiscriminator\nDKL\nKullback–Leibler divergence\nDJS\nJensen-Shannon divergence\n·∥·\nDivergence\nlearning, stabilized sample efﬁcient discriminative actor-\ncritic network, and its optimization. Policy and reward\nlearning aims to solve the reward bias and the absorbing\nstate problem by introducing a learnable reward function\nand environment feedback. The stabilized actor-critic net-\nwork aims to improve the training stability and sample\nefﬁciency for the ex sting methods. Optimization refers to\nthe method to optimize the policy and the algorithms to\ntrain the overall approach.\n3.1\nPolicy and Reward Learning\nWe consider behavioral tendencies inference as an agent\npolicy learning problem and an agent policy as the ab-\nstraction of user’s behavioral tendencies. Policy learning\naims to make the learned policy π and expert policy πE.\nWe deﬁne the occupancy measure ρ in Eq.(10) and solve\npolicy learning as a occupancy measure based distribution\nmatching problem [23]. To this end, we deﬁne a reward\nfunction below to determine the performance in existing\nmethods:\nr(s, a) = log(D(s, a)) −log(1 −D(s, a))\n(16)\n[30] design a dynamic robust disentangled reward function\nfor the approximation by introducing the future state s′.\nr′(s, a) = log(D(s, a, s′)) −log(1 −D(s, a, s′))\n(17)\nThe reward function deﬁned in Eq.(16) is not robust\nfor dynamic environments. Although Eq.(17) improves it\nby assigning positive and negative rewards for each time\nstep to empower the agent to ﬁt into different scenarios,\nboth Eq.(16) and Eq.(17) have the absorbing state problem,\ni.e., the agent will receive no reward at the end of each\nepisode, leading to sub-optimal policies [31]. Speciﬁcally, in-\nstead of exploring more policies, the reward function r(s, a)\nwill assign a negative reward bias for the discriminator to\ndistinguish samples from the generated policies and expert\npolicies at the beginning of the learning process. Since the\nagent aims to avoid the negative penalty, the zero reward\nmay lead to early stops.\nMoreover, the above two reward functions are more\nsuitable for survival or exploration tasks rather than the goal\nof this study. For survival tasks, the reward used on GAIL\nis log D(s, a), which is always negative because D(s, a)\n(∈[0, 1]) encourages agent to end current episode to stop\nmore negative rewards. For exploration tasks, the reward\nfunction is −log(1 −D(s, a)), it is always positive and may\nresult in the agent to loop in the environment to collect more\nrewards.\nWe add a bias term to the reward function r(s, a), as\ndeﬁned by either Eq.(16) or Eq.(17) to overcome the reward\nbias. In addition, we introduce a new reward given by\nenvironment re for reward shaping. Finally, we have the\nfollowing:\nrn(s, a) = λi\n \nr(s, a) +\n∞\nX\nt=T +1\nγt−T r(sa, ·)\n!\n+ re\n(18)\nwhere r(sa, ·) is a learnable reward function, which is train-\nable during the training process. We also add a dimension\nto indicate whether the current state is an absorbing state\nor not (denoted by 1 or 0, respectively). Besides, we simply\nsample the reward from the replay buffer, considering the\nbias term is unstable in practice.\n3.2\nStabilized Sample Efﬁcient Discriminative Actor-\nCritic Network\nThe stabilized sample efﬁcient discriminative actor-critic\nnetwork aims to enable the agent to learn the policy ef-\nﬁciently. We take a variant of the actor-critic network,\nadvantage actor-critic network [32], as the backbone of our\napproach. In this network, the actor uses policy gradient\nand the critic’s feedback to update the policy, and the\ncritic uses Q-learning to evaluate the policy and provides\nfeedback [33].\nGiven the state space at timestamp t, the environment\ndetermines a state st, which contains user’s recent interest\nand demographic information embedded, via the actor-\nnetwork [34], [35]. The actor-network feeds the state st to\na network that has four fully-connected layers with ReLU\nas the activation function. The ﬁnal layer of the network\noutputs a policy function π, which is parameterized by θ.\nThen, the critic network takes two inputs: the trajectory\n(st, at), and the current policy πθt from the actor-network.\nWe concatenate the state-action pair (st, at) and feed it into\na network with four fully-connected layers (with ReLU as\nthe activation function) and a softmax layer. The output of\nthe critic-network is a value V (st, at) ∈IR to be used for\noptimization (to be introduced later).\nThe discriminator D is the key component of our ap-\nproach. To build an end-to-end model that better approxi-\nmates the expert policy πE, we parameterize the policy with\nπθ and clip the discriminator’s output so that D : S × A →\n(0, 1) with weight w. The loss function of D is denoted by\nLD. Besides, we use Adam [36] to optimize weight w (the\noptimization for θ will be introduced later). We consider\nthe discriminator D as a local cost function provider to\nguide the policy update. During the minimization of the\nloss function LD, i.e., ﬁnding a point (π, D) for it, the policy\nwill move toward expect-like regions (divided by D) in the\nlatent space.\nLike many other networks, Actor-critic network also\nsuffers the sample inefﬁciency problem [37], i.e., the agent\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nhas to conduct sampling from the expert policy distribu-\ntion, given the signiﬁcant number of agent-environment\ninteractions needed to learn the expert policy during the\ntraining process. In this regard, we use an off-policy re-\ninforcement learning algorithm (instead of on-policy rein-\nforcement learning algorithms) to reduce interactions with\nthe environment. In particular, we introduce a replay buffer\nR to store previous state-action pairs; when training dis-\ncriminator we sample the transition from the replay buffer\nR in off-policy learning (instead of sampling trajectories\nfrom a policy directly). We thereby deﬁne the loss function\nas follows:\nLD = ER[log D(s, a)] + EπE[log(1 −D(s, a))] −λH(π)\n(19)\nEq.(19) matches the occupancy measures between the\nexpert and the distribution induced by R. Instead of com-\nparing the latest trained policy π and expert policy πE, it\ncomprises a mixture of all policy distributions that appeared\nduring training. Considering off-policy learning has differ-\nent expectation from on-policy learning, we use importance\nsampling on the replay buffer to balance it.\nLD = ER\n\u0014ρπθ(s, a)\nρR(s, a) log D(s, a)\n\u0015\n+ EπE[log(1 −D(s, a))] −λH(π)\n(20)\nConsidering GAN has the training instability prob-\nlem [38], we employ the Wasserstein GAN [39] to improve\nthe discriminator’s performance. While a normal GAN min-\nimizes JS-Divergence cannot measure the distance between\ntwo distributions, Wasserstein GANs uses the EM-distance\nand Kantorovich-Rubinstein duality to resolve the prob-\nlem [40].\nEπ[log D(s, a)] −EπE[log(D(s, a))]\n+ EπE[(∥∇D(s, a)∥−1)2]\n(21)\nWe further use gradient penalty to improve the training\nfor Wasserstein GANs [39], given the gradient penalty can\nimprove training stability for JS-Divergence-based GANs\n[41]. We thereby obtain the ﬁnal loss function as follows:\nLD = ER\n\u0014ρπθ(s, a)\nρR(s, a) log D(s, a)\n\u0015\n+ EπE[log(1 −D(s, a))]\n−λH(π) + EπE[(∥∇D(s, a)∥−1)2]\n(22)\n3.3\nOptimization\nWe conduct a joint training process on the policy network\n(i.e., the actor-critic network) and the discriminator. We pa-\nrameterize the policy network with policy parameter θ and\nupdate it using trust region policy optimization (TRPO) [42]\nbased on the discriminator. TRPO introduces a trust region\nby restricting the agent’s step size to ensure a new policy is\nbetter than the old one. We formulate the TRPO problem as\nfollows:\nmax\nθ\n1\nT\nT\nX\nt=0\n\u0014 πθ(at|st)\nπθold(at|st)At\n\u0015\nsubject to Dθold\nKL (πθold, πθ) ≤η\n(23)\nwhere An is the advantage function calculated by General-\nized Advantage Estimation (GAE) [43]. GAE is described\nas follows:\nAt =\n∞\nX\nl=0\n(γλg)lδV\nt+l\nwhere δV\nt+l = −V (st) +\n∞\nX\nl=0\nγlrt+l\n(24)\nwhere rt+l is the test reward for l-step’s at timestamp t, as\ndeﬁned on Eq.(18). Considering the high computation load\nof updating TRPO via optimizing Eq.(23), we update the\npolicy using a simpler optimization method called Proximal\nPolicy Optimization (PPO) [44], which has an objective\nfunction below:\nE\nτ∼πold\nh\nT\nX\nt=0\nmin\n\u0010 πθ(at|st)\nπθold(at|st)At,\nclip\n\u0010 πθ(at|st)\nπθold(at|st), 1 −ϵ, 1 + ϵ\n\u0011\nAt\n\u0011i\n(25)\nwhere ϵ is the clipping parameter representing the max-\nimum percentage of change that can be made by each\nupdate.\nThe overall training procedure is illustrated in Algorithm\n1, which involves the training of both the discriminator and\nthe actor-critic network. For the discriminator, we use Adma\nas the optimizer to ﬁnd the gradient for Eq.(22) for weight\nw at step i:\nEπ[∇w log(Dw(s, a))] + EπE[∇w log(1 −Dw(s, a))]\n+EπE[(∥∇wD(s, a)∥−1)2\n(26)\n4\nEXPERIMENTS\nWe evaluate the proposed framework and demonstrate\nits generalization capability by conducting experiments in\nthree different environments: Trafﬁc Control, Recommenda-\ntion System, and Scanpath Prediction. Our model is imple-\nmented in Pytorch [45] and all experiments are conducted\non a server with 6 NVIDIA TITAN X Pascal GPUs, 2\nNVIDIA TITAN RTX with 768 GB memory.\n4.1\nUrban Mobility Management\nIn the trafﬁc control scenario, the agent is required to\ncontrol cars to conduct a certain task. The objective is to\nminimize the total waiting time in the trip.\n4.1.1\nSimulation of Urban Mobility\nTrafﬁc\nsignal\ncontrol\nis\ncritical\nto\neffective\nmobility\nmanagement in modern cities. To apply our model to\nthis context, we use the Simulation of Urban MObility\n(SUMO)\n[46]\nlibrary,\na\nmicroscopic,\nspace-continuous,\nand time-discrete trafﬁc ﬂow simulation tool, to test the\nmethod’s performance. The agent controls trafﬁc signals,\nand a car may take three actions facing trafﬁc lights:\ngo straight, turn left, or turn right, depending on user’s\npreference. We design a simple two-way road network\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\n1\n5\n10\n15\n20\nTime Step ('000 s)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nTotal Waiting Time (s)\nOurs\nexpert\nQ-Learning\nDeep Q-Learning\nSARSA\nA2C\n(a) Total waiting time with 95% conﬁ-\ndence interval\n1\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCTR\nExpert\nOurs\nIRecGAN\nKGRL\nGAUM\nPGCR\n(b) CTR with 95% conﬁdence interval\n0\n1\n2\n3\n4\n5\n6\nNumber of Fixations Made to Target\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Probability\nOurs\nHuman\nIRL\nBC_LSTM\nBC_CNN\nFix\nDetector\n(c) Cumulative probability comparison\nfor selected baseline methods\nFig. 2: Overall results comparison. From left to right are represented to (a) Trafﬁc Control, (b) Recommendation System\nand (c) Scanpath Prediction.\nthat contains eight trafﬁc lights for testing. We employ\nan open-sourced library sumo-rl\n1 to enable our agent\ncan interact with the simulation environment (including\nreceiving the reward) directly. The number of cars available\nin the environment is unlimited; the environment keeps\ngenerating cars until this simulation step ends or the road\nreaches its full capacity.\n4.1.2\nExpert Policy Acquisition\nSince there is no ofﬁcial expert policy available for our\ncustomized road network, we use the same strategy as\nintroduced by\n[47] to collect a set of imperfect expert\npolicies from a pre-trained policy network. This policy\nnetwork is built upon actor-critic network, which is trained\nby using Deep Deterministic Policy Gradients (DDPG) [48].\nExpert policies are stored via state-action pairs, which\nconcatenate observed states and expert actions.\n4.1.3\nBaseline Methods\nWe evaluate our model against several traditional reinforce-\nment learning methods in this scenario.\n•\nQ-Learning: An off-policy reinforcement learning\nmethod that ﬁnds the best action given the current\nstate.\n•\nDeep Q-learning: A deep Q-learning method that\nemploys the neural network to extract features.\n•\nSARSA\n[49]:\nState–action–reward–state–action\n(SARSA)\nis\nan\nimproved\nQ-learning\nmethod\ncommonly used for trafﬁc signal control.\n•\nAdvantage Actor-Critic Network (A2C) [32]: An\nasynchronous method built on an actor-critic net-\nwork for deep reinforcement learning.\nExperiments are conducted in exactly the same environment\nto ensure a fair comparison. All the baseline methods are\nimplemented by using PyTorch and are publicly available\n1. https://github.com/LucasAlegre/sumo-rl\n2. The reward provided by environment for each simulation\nstep can deﬁned as:\nr =\nNts\nX\nn=0\ns ∗Ncp\n(27)\nwhere Nts is the number of trafﬁc signals available in the\nenvironment, s is the average car speed in this simulation\nstep, and Ncp is number of cars passed this trafﬁc signals at\nthe end of this simulation step. The evaluate metric is the\ntotal waiting time deﬁned below:\nt =\n1000\nX\ni=0\nNc\nX\nc=0\nti\nc\n(28)\nwhere ti\nc is the time that the car c waits at trafﬁc light i, and\n1, 000 is the duration for one simulation step. If car c does\nnot meet trafﬁc light i, we set ti\nc = 0.\n4.1.4\nHyper-parameters Setting and Results\nDDPG parameters for the pre-trained model include γ =\n0.95, τ = 0.001, the size of the hidden layer 128, the size of\nthe reply buffer 1, 000, and the number of episode 20, 000.\nParameters for Ornstein-Uhlenbeck Noise include the scale\n0.1, µ = 0, θ = 0.15, σ = 0.2. For our method, we set\nthe number of time steps to 20, 000, the hidden size of the\nadvantage actor-critic network to 256, the hidden size for\ndiscriminator to 128, the learning rate to 0.003, factor λ to\n10−3, mini batch size to 5, and the epoch of PPO to 4. For the\ngeneralized advantage estimation, we set the discount factor\nγ to 0.995, λg = 0.97, and ϵ = 0.2. We also set λi = 1 for\nreward shaping and λ = 1 for H(π). The results in Fig. 2 (a)\nshow our method generally outperforms than all baseline\nmethods.\n4.2\nRecommendation System\nIn the recommendation scenario, the agent aims to interact\nwith a dynamic environment to mine user’s interests and\nmake recommendations to users.\n2. https://github.com/hill-a/stable-baselines\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\nAlgorithm 1: Training algorithm for our model\ninput: Expert replay buffer RE, Initialize Policy\nReplay Buffer R,Initialize policy parameter\nθ0, clipping parameter ϵ\n1 Function Absorbing(τ) is\n2\nif sT is a absorbing state then\n3\n{sT , aT , ·, s′\nT } ←{sT , aT , ·, sa};\n4\nτ ←τ ∪{sa, ·, ·, sa};\n5\nend\n6\nreturn τ;\n7 end\n8 for τ = {st, at, ·, s′\nt}T\nt=1 ∈RE do\n9\nτ ←Absorbing(τ);\n10 end\n11 R ←∅;\n12 for i = 1, 2, · · · do\n13\nSampling trajectories τ = {st, at, ·, s′\nt}T\nt=1 ∼πθi;\n14\nR ←R∪Absorbing(τ) ;\n15\nfor j = 1, · · · , |τ| do\n16\n{st, at, ·, ·}B\nt=1 ∼R, {s′\nt, a′\nt, ·, ·}B\nt=1 ∼RE ;\n17\nUpdate the parameter wi by gradient on\nEq.(26) ;\n18\nend\n19\nfor j = 1, · · · , |τ| do\n20\n{st, at, ·, ·}B\nt=1 ∼R;\n21\nfor b = 1, · · · , B do\n22\nr = log(Dwi(sb, ab))−log(1−Dwi(sb, ab))\n;\n23\nCalculate the reshape reward r′ by Eq.(18)\n;\n24\n(sb, ab, ·, s′\nb) ←(sb, ab, r′, s′\nb) ;\n25\nend\n26\nfor k = 0, 1, · · · do\n27\nGet the trajectories (s, a) on policy\nπθ = π(θk) ;\n28\nEstimate advantage At using Eq.(24);\n29\nCompute the Policy Update\nθk+1 = arg max\nθ\nEq.(25)\nBy taking K step of minibatch SGD (via\nAdma)\n30\nend\n31\nθi ←θK;\n32\nend\n33 end\n4.2.1\nVirtualTB\nWe use an open-source online recommendation platform,\nVirtualTB [50], to test the performance of the proposed\nmethods in a recommendation system. VirtualTB is a dy-\nnamic environment built on OpenAI Gym3 to test our\nmethod’s feasibility on recommendation tasks. VirtualTB\nemploys a customized agent to interact with it and achieves\nthe corresponding rewards. It can also generate several\ncustomers with different preferences during the agent-\n3. https://gym.openai.com/\nAlgorithm 2: PPO Update\ninput: Initialize policy parameter θ0, clipping\nparameter ϵ\n1 for k = 0, 1, · · · do\n2\nGet the trajectories (s, a) on policy πθ = π(θk) ;\n3\nEstimate advantage At using Eq.(24);\n4\nCompute the Policy Update\nθk+1 = arg max\nθ\nEq.(25)\nBy taking K step of minibatch SGD (via Adma)\n5 end\n6 θi ←θK;\nenvironment interaction. In VirtualTB, each customer has\n11 static attributes encoded into an 88-dimensional space\nwith binary values as the demographic information. The\ncustomers have multiple dynamic interests, which are en-\ncoded into a 3-dimensional space and may change over\nthe interaction process. Each item has several attributes\n(e.g., price and sales volume), which are encoded into a 27-\ndimensional space. We use CTR as the evaluation metric\nbecause the gym environment can only provide rewards as\nfeedback. CTR is deﬁned as follows:\nCTR =\nrepisode\n10 ∗Nstep\n(29)\nwhere repisode is the reward that the agent receives at each\nepisode. At each episode, the agent may take Nstep steps\nand receive a maximum reward of 10 per step.\n4.2.2\nBaseline Methods\nWe evaluate our model against four state-of-the-art methods\ncovering methods based on deep Q-learning, policy gradi-\nent, and actor-critic networks.\n•\nIRecGAN [51]: An online recommendation method\nthat employs reinforcement learning and GAN.\n•\nPGCR [52]: A policy-Gradient-based method for con-\ntextual recommendation.\n•\nGAUM [53]: A deep Q-learning based method that\nemploys GAN and cascade Q-learning for recom-\nmendation.\n•\nKGRL [34]: An Actor-Critic-based method for inter-\nactive recommendation, a variant of online recom-\nmendation.\nNote that GAUM and PGCR are not designed for online\nrecommendation, and KGRL requires a knowledge graph—\nwhich is unavailable to the gym environment—as the side\ninformation, . Hence, we only keep the network structure\nof those networks when testing them on the VirtualTB\nplatform.\n4.2.3\nHyper-parameters Setting and Results\nThe hyper-parameters are set in a similar way as in the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\ntrafﬁc signals control. We set the number of episodes to\n200, 000 for both the pre-trained policy network and our\nmethod. To ease comparison, we conﬁgure each iteration\nto contain 100 episodes. The results in Fig. 2 (b) show\nour method outperform all state-of-the-art methods. KGRL’s\npoor performance may be partially caused by its reliance on\nknowledge graph, which is unavailable in our experiments.\n4.3\nScanpath Prediction\nScanpath prediction is a type of goal-directed human\nintention prediction problem [15]. Take the last task in Fig. 1\nfor example. Given a few objects, a user may ﬁrst look at\nitem 1, then follows the item numbers annotated in the\nﬁgure, and ﬁnally reaches item 8. The task aims to predict\nuser’s intention (i.e., item 8), given the start item (i.e., item\n1).\n4.3.1\nExperimental Setup\nWe follow the same experimental setup as [15] and con-\nduct all experiments on a public COCO-18 Search dataset4.\nWe replace the fully-connected layer in actor-network with\nCNN to achieve the best performance of our method on\nimages. The critic-network has a new structure with two\nCNN layers followed by two fully-connected layers. The\ndiscriminator contains all CNN layers with a softmax layer\nas output. We also resize the input image from the original\nsize of 1680 × 1050 into 320 × 512 and construct the state\nby using the contextual beliefs calculated from a Panoptic-\nFPN with a backbone network (ResNet-50-FPN) pretrained\non COCO2017.\n4.3.2\nBaseline Methods\nWe compare our method with several baseline methods,\nincluding simple CNN based methods, behavior-cloning\nbased methods, and inverse reinforcement-learning-based\nmethod.\n•\nDetector: A simple CNN to predict the location of a\ntarget item.\n•\nFixation heuristics [15]: A method similar to Detector\nbut using the ﬁxation to predict the location of a\ntarget item.\n•\nBC-CNN [54]: A behavior-cloning method that uses\nCNN as the basic layer structure.\n•\nBC-LSTM [55]: A behavior-cloning method that uses\nLSTM as the basic layer structure.\n•\nIRL [15]: A state-of-the-art inverse reinforcement-\nlearning-based method for scanpath prediction.\nExperiments are conducted under the same conditions to\nensure fairness.\n4. https://saliency.tuebingen.ai/datasets/COCO-\nSearch18/index new.html\n4.3.3\nPerformance Comparison\nThe hyper-parameters settings are the same as those used\nfor the recommendation task. We also use the same evalu-\nation metrics as used in [15] to evaluate the performance:\ncumulative probability, probability mismatch, and scanpath\nratio. The results in Fig. 2(c) show the cumulative probabil-\nity of the gaze landing on the target after ﬁrst six ﬁxations.\nWe report the probability mismatch and scanpath ratio in\ntable 2.\nTABLE 2: Results comparison for selected methods on prob-\nability mismatch and scanpath ratio\nProbability Mismatch ↓\nScanpath Ratio↑\nHuman\nn.a.\n0.862\nDetector\n1.166\n0.687\nBC-CNN\n1.328\n0.706\nBC-LSTM\n3.497\n0.406\nFixation\n3.046\n0.545\nIRL\n0.987\n0.862\nOurs\n0.961\n0.881\n4.4\nEvaluation on Explainability\nExplainability plays a crucial role on the understanding\nof the decision-making process. By visualizing the learned\nreward map, we show in this experiment our model can\nprovide a certain level of interpretability. We evaluate the\nexplanability for our model in the scanpath prediction sce-\nnario. Fig. 4 shows that the reward maps recovered by the\nour model depend heavily on the category of the search\ntarget. In the ﬁrst image, the highest reward is assigned to\nthe piazza when drinking beers. Similarly, the searching of\nroad signal on the road, the stop signal get almost all of the\nreward while the car get only a few.\n4.5\nAblation Study\nWe test using three different optimization strategies (DDPG,\nAdaptive KL Penalty Coefﬁcient and Twin Delayed DDPG)\nto update the policy parameter θ. (TD3) [56]. The Adaptive\nKL Penalty Coefﬁcient is deﬁned as:\nL(θ) = Et\nhπθ′(at|st)\nπθ(at|st) At −βKL[πθ(·|st), πθ′(·|st)]\ni\n(30)\nwhere the β will be adjust dynamically by the following,\n(\nβ ←β/2\nd < dtarget ∗1.5\nβ ←β ∗2\nd >= dtarget ∗1.5\n(31)\nwhere d = Et[KL[πθold(·|st), πθ(·|st)]]\nWe empirically choose coefﬁcient 1.5 and 2 and select total\nwaiting time, CTR, and cumulative probability as the eval-\nuation metrics to compare the three optimization strategies\nfor trafﬁc signal control, recommendation system, and scan-\npath prediction, respectively. The results (shown in Fig. 3)\nshow our optimization method achieve a similar result as\nTD3 on all the three tasks but is better than TD3 on the\nrecommendation task. Hence, we want to conduct a further\nstep about the parameter selection about the PPO and GAE\nwhich can be found on Table 3.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\n1\n6\n11\n16\nTime Step ('000s)\n0\n1000\n2000\n3000\n4000\n5000\n6000\nTotal Waiting Time (s)\nOurs\nTD3\nAdaptive KL Penalty\nDDPG\n(a) Trafﬁc\n1\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nCTR\nOurs\nDDPG\nAdaptive KL Penalty\nTD3\n(b) Recommendation\n0\n1\n2\n3\n4\n5\n6\nNumber of Fixations Made to Target\n0.0\n0.2\n0.4\n0.6\n0.8\nCumulative Probability\nOurs\nTD3\nAdaptive KL Penalty\nDDPG\n(c) Scanpath Prediction\nFig. 3: Results of ablation study for those three selected environments.\nTABLE 3: CTR for Different Parameter Settings for GAE and PPO with 95% Conﬁdence Interval\nGAE: λg\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\nPPO: ϵ\n0.05\n0.630 ± 0.063\n0.632 ± 0.064\n0.633 ± 0.062\n0.630 ± 0.059\n0.626 ± 0.060\n0.629 ± 0.059\n0.10\n0.632 ± 0.062\n0.635 ± 0.060\n0.636 ± 0.061\n0.636 ± 0.058\n0.634 ± 0.061\n0.633 ± 0.060\n0.15\n0.633 ± 0.060\n0.635 ± 0.061\n0.639 ± 0.061\n0.640 ± 0.057\n0.639 ± 0.059\n0.638 ± 0.061\n0.20\n0.634 ± 0.060\n0.636 ± 0.060\n0.641 ± 0.063\n0.643 ± 0.061\n0.643 ± 0.063\n0.641 ± 0.058\n0.25\n0.631 ± 0.061\n0.635 ± 0.059\n0.636 ± 0.060\n0.637 ± 0.060\n0.636 ± 0.061\n0.634 ± 0.059\n0.30\n0.630 ± 0.059\n0.631 ± 0.061\n0.632 ± 0.060\n0.630 ± 0.059\n0.630 ± 0.058\n0.629 ± 0.050\n1\n2\n(a) Piazza\n1\n2\n3\n4\n(b) Stop\n1\n2\n3\n4\n5\n6\n(c) Piano\nFig. 4: Reward maps learned by the our model for three\ndifferent search targets which are piazza, stop signal and\npiano respectively in the context of Scanpath Prediction. The\nnumber means user’s vision trajectory for searching target\nitem which is the largest number refers to. For example, in\n(a) 2 represents piazza, in (b) 4 represents to stop sign and\nin (c), 6 represents to the piano. In addition, the hotmap\nrepresent the highest reward area which will be awarded to\nagent.\n5\nRELATED WORK\nUser behavior tendency modeling has been an active topic\nin research, and most previous efforts have been focusing\non feature engineering rather than an end-to-end learning\nstructure. Kim et al. [7] considers long-term interest as a\nreasonable representation of general interest and acknowl-\nedges its importance for personalization services. On this\nbasis, Liu et al. [57] propose a framework that considers\nboth long-term and short-term interest for user behavior\nmodeling. Rather than establishing static models, Chung et\nal. [58] models long-term and short-term user proﬁle scores\nto model user behaviors incrementally. Recently, Song et\nal. [59] propose to jointly model long-term and short-term\nuser interest for recommendation based on deep learning\nmethods. Pi et al. [60] further propose a MIMN model for se-\nquential user behavior modeling. Despite good performance\non their respective tasks, all the above methods are task-\nspeciﬁc and lack generalization ability.\nReinforcement learning is widely used for user behav-\nior modeling in recommendation systems. Zheng et al. [2]\nadopt deep Q-learning to build up user proﬁle during the\ninteraction process in a music recommendation system. Zou\net al. [61] improve the Q-learning structure to stabilized\nthe reward function and make the recommendation robust.\n[34], [62], [63] apply reinforcement learning for extracting\nuser’s interest from a knowledge graph. Liu et al. [35]\nembed user’s information into a latent space and conduct\nrecommendation via deep reinforcement learning. Different\nfrom those mentioned works, Pan et al. [52] applies the pol-\nicy gradient directly to optimize the recommendation policy.\nChen et al. [53] integrates the GAN into the reinforcement\nlearning framework so that user’s side information to enrich\nthe latent space to improve the recommendation accuracy.\nShang et al. [64] considers the environment co-founder fac-\ntors and propose a multi-agent based reinforcement learning\nmethod for recommendation. All the above studies require\ndeﬁning accurate reward functions, which are hard to obtain\nin the real world.\nInverse reinforcement learning emerges where reward\nfunctions cannot be deﬁned [20]. Lee et al. [65] ﬁrstly use\nthe inverse reinforcement learning to learn user’s behavior\nstyles. However, general inverse reinforcement learning is\ncomputationally expensive. Ho et al. [21] propose a genera-\ntive reinforcement learning approach to improve efﬁciency.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\nFu et al. [30] further extend the idea to a general form to\nobtain a more stable reward function. Kostrikov et al. [31]\nﬁnd a generative method may suffer instability in training,\nwhich can be relieved by using EM-distance instead of\nJS-divergence. Yang et al. [15] ﬁrst introduces the inverse\nreinforcement learning into the scanpath prediction and\ndemonstrate the superior performance. IRL demonstrates\nthe huge potential and widely used in robot learning as\nit can empower agent to learn from the demonstration in\ndifferent environments and tasks without dramatical explo-\nration about the environment or familiar with the tasks.\nChen et al. [22] expands this idea into recommender system\nand shows the feasibility of IRL in recommendation task.\n6\nCONCLUSION AND FUTURE WORK\nIn this paper, we propose a new method based on advantage\nactor-critic network with inverse reinforcement learning for\nuser behavior modeling, to overcome the adverse impact\ncaused by an inaccurate reward function. In particular,\nwe use the wasserstein GAN instead of GAN to increase\ntraining stability and a reply buffer for off-policy learning\nto increase sample efﬁciency.\nComparison of our method with several state-of-the-art\nmethods in three different scenarios (namely trafﬁc signal\ncontrol, recommendation system, and scanpath prediction)\ndemonstrate our method’s feasibility in those scenarios and\nsuperior performance to baseline methods.\nExperience replay can boost the sample efﬁciency by\nswitching the sampling process from the environment to\nreplay buffer. However, it is not ideal as some tasks may\nintroduce a giant state and action spaces such as recommen-\ndation. Sampling from such giant state and action spaces\nare not efﬁcient. Moreover, not every experience are useful\neven it comes from the demonstration. The major reason is\nthat expert demonstrations are sampled from replay buffer\nrandomly and may orthogonal with the current state and\nlead to opposed actions. The possible solutions including,\nthe state-aware experience replay method or prioritized\nexperience replay based methods [66]. Another potential\nimprovement leads by the Wasserstein GAN as the Lips-\nchitz constraint is hard to enforce and may lead to model\nconverge issue.\nREFERENCES\n[1]\nJ. Hong, E.-H. Suh, J. Kim, and S. Kim, “Context-aware system for\nproactive personalized service based on context history,” Expert\nSystems with Applications, vol. 36, no. 4, pp. 7448–7457, 2009.\n[2]\nG. Zheng, F. Zhang, Z. Zheng, Y. Xiang, N. J. Yuan, X. Xie,\nand Z. Li, “Drn: A deep reinforcement learning framework for\nnews recommendation,” in Proceedings of the 2018 World Wide Web\nConference, 2018, pp. 167–176.\n[3]\nS. Zhang, L. Yao, A. Sun, and Y. Tay, “Deep learning based\nrecommender system: A survey and new perspectives,” ACM\nComputing Surveys (CSUR), vol. 52, no. 1, pp. 1–38, 2019.\n[4]\nL. Hu, L. Cao, S. Wang, G. Xu, J. Cao, and Z. Gu, “Diversify-\ning personalized recommendation with user-session context.” in\nIJCAI, 2017, pp. 1858–1864.\n[5]\nX. Xu, F. Dong, Y. Li, S. He, and X. Li, “Contextual-bandit based\npersonalized recommendation with time-varying user interests.”\nin AAAI, 2020, pp. 6518–6525.\n[6]\nX. Wang, Y. Wang, D. Hsu, and Y. Wang, “Exploration in in-\nteractive personalized music recommendation: a reinforcement\nlearning approach,” ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM), vol. 11, no. 1, pp. 1–22,\n2014.\n[7]\nH. R. Kim and P. K. Chan, “Learning implicit user interest hi-\nerarchy for context in personalization,” in Proceedings of the 8th\ninternational conference on Intelligent user interfaces, 2003, pp. 101–\n108.\n[8]\nT. B. Sheridan, “Human–robot interaction: status and challenges,”\nHuman factors, vol. 58, no. 4, pp. 525–532, 2016.\n[9]\nE. Schmerling, K. Leung, W. Vollprecht, and M. Pavone, “Mul-\ntimodal probabilistic model-based planning for human-robot in-\nteraction,” in 2018 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2018, pp. 1–9.\n[10] M. Siam, C. Jiang, S. Lu, L. Petrich, M. Gamal, M. Elhoseiny, and\nM. Jagersand, “Video object segmentation using teacher-student\nadaptation in a human robot interaction (hri) setting,” in 2019\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2019, pp. 50–56.\n[11] C. Badii, P. Bellini, D. Cenni, A. Diﬁno, M. Paolucci, and P. Nesi,\n“User engagement engine for smart city strategies,” in 2017\nIEEE International Conference on Smart Computing (SMARTCOMP).\nIEEE, 2017, pp. 1–7.\n[12] P. Bellini, D. Cenni, P. Nesi, and I. Paoli, “Wi-ﬁbased city users’\nbehaviour analysis for smart city,” Journal of Visual Languages &\nComputing, vol. 42, pp. 31–45, 2017.\n[13] S. Seko, T. Yagi, M. Motegi, and S. Muto, “Group recommendation\nusing feature space representing behavioral tendency and power\nbalance among members,” in Proceedings of the ﬁfth ACM conference\non Recommender systems, 2011, pp. 101–108.\n[14] Y. Shi, M. Larson, and A. Hanjalic, “Collaborative ﬁltering beyond\nthe user-item matrix: A survey of the state of the art and future\nchallenges,” ACM Computing Surveys (CSUR), vol. 47, no. 1, pp.\n1–45, 2014.\n[15] Z. Yang, L. Huang, Y. Chen, Z. Wei, S. Ahn, G. Zelinsky, D. Sama-\nras, and M. Hoai, “Predicting goal-directed human attention using\ninverse reinforcement learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n193–202.\n[16] A. L. Bazzan, “Opportunities for multiagent systems and multia-\ngent reinforcement learning in trafﬁc control,” Autonomous Agents\nand Multi-Agent Systems, vol. 18, no. 3, p. 342, 2009.\n[17] H. Liu, Y. Zhang, W. Si, X. Xie, Y. Zhu, and S.-C. Zhu, “Interactive\nrobot knowledge patching using augmented reality,” in 2018 IEEE\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2018, pp. 1947–1954.\n[18] S.-Y. Chen, Y. Yu, Q. Da, J. Tan, H.-K. Huang, and H.-H. Tang,\n“Stabilizing reinforcement learning in dynamic environment with\napplication to online recommendation,” in Proceedings of the 24th\nACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, 2018, pp. 1187–1196.\n[19] H. Chen, X. Dai, H. Cai, W. Zhang, X. Wang, R. Tang, Y. Zhang,\nand Y. Yu, “Large-scale interactive recommendation with tree-\nstructured policy gradient,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, vol. 33, 2019, pp. 3312–3320.\n[20] A. Y. Ng, S. J. Russell et al., “Algorithms for inverse reinforcement\nlearning.” in Icml, vol. 1, 2000, p. 2.\n[21] J. Ho and S. Ermon, “Generative adversarial imitation learning,”\nin Advances in neural information processing systems, 2016, pp. 4565–\n4573.\n[22] X. Chen, L. Yao, A. Sun, X. Wang, X. Xu, and L. Zhu, “Generative\ninverse deep reinforcement learning for online recommendation,”\narXiv preprint arXiv:2011.02248, 2020.\n[23] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse re-\ninforcement learning,” in Proceedings of the twenty-ﬁrst international\nconference on Machine learning.\nACM, 2004, p. 1.\n[24] U. Syed, M. Bowling, and R. E. Schapire, “Apprenticeship learning\nusing linear programming,” in Proceedings of the 25th international\nconference on Machine learning, 2008, pp. 1032–1039.\n[25] U. Syed and R. E. Schapire, “A game-theoretic approach to ap-\nprenticeship learning,” in Advances in neural information processing\nsystems, 2008, pp. 1449–1456.\n[26] M. Bloem and N. Bambos, “Inﬁnite time horizon maximum causal\nentropy inverse reinforcement learning,” in 53rd IEEE Conference\non Decision and Control.\nIEEE, 2014, pp. 4911–4916.\n[27] B. D. Ziebart, J. A. Bagnell, and A. K. Dey, “Modeling interaction\nvia the principle of maximum causal entropy,” 2010.\n[28] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial\nnets,” in Advances in neural information processing systems, 2014, pp.\n2672–2680.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\n[29] X. Nguyen, M. J. Wainwright, M. I. Jordan et al., “On surrogate\nloss functions and f-divergences,” The Annals of Statistics, vol. 37,\nno. 2, pp. 876–904, 2009.\n[30] J.\nFu,\nK.\nLuo,\nand\nS.\nLevine,\n“Learning\nrobust\nrewards\nwith adverserial inverse reinforcement learning,” in International\nConference on Learning Representations, 2018. [Online]. Available:\nhttps://openreview.net/forum?id=rkHywl-A-\n[31] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson,\n“Discriminator-actor-critic: Addressing sample inefﬁciency and\nreward bias in adversarial imitation learning,” in International\nConference on Learning Representations, 2019. [Online]. Available:\nhttps://openreview.net/forum?id=Hk4fpoA5Km\n[32] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in International conference on machine learn-\ning, 2016, pp. 1928–1937.\n[33] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algorithms,” in\nAdvances in neural information processing systems, 2000, pp. 1008–\n1014.\n[34] X. Chen, C. Huang, L. Yao, X. Wang, W. Liu, and W. Zhang,\n“Knowledge-guided deep reinforcement learning for interactive\nrecommendation,” arXiv preprint arXiv:2004.08068, 2020.\n[35] F. Liu, H. Guo, X. Li, R. Tang, Y. Ye, and X. He, “End-to-end deep\nreinforcement learning based recommendation with supervised\nembedding,” in Proceedings of the 13th International Conference on\nWeb Search and Data Mining, 2020, pp. 384–392.\n[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” arXiv preprint arXiv:1412.6980, 2014.\n[37] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu,\nand N. de Freitas, “Sample efﬁcient actor-critic with experience\nreplay,” arXiv preprint arXiv:1611.01224, 2016.\n[38] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\npreprint arXiv:1701.07875, 2017.\n[39] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.\nCourville, “Improved training of wasserstein gans,” in Advances\nin neural information processing systems, 2017, pp. 5767–5777.\n[40] C. Villani, Optimal transport: old and new.\nSpringer Science &\nBusiness Media, 2008, vol. 338.\n[41] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet,\n“Are gans created equal? a large-scale study,” in Advances in neural\ninformation processing systems, 2018, pp. 700–709.\n[42] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International conference on machine\nlearning, 2015, pp. 1889–1897.\n[43] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-\ndimensional continuous control using generalized advantage esti-\nmation,” arXiv preprint arXiv:1506.02438, 2015.\n[44] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal\npolicy\noptimization\nalgorithms,”\narXiv\npreprint\narXiv:1707.06347, 2017.\n[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch:\nAn imperative style, high-performance deep learning library,” in\nAdvances in neural information processing systems, 2019, pp. 8026–\n8037.\n[46] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P.\nFl¨otter¨od, R. Hilbrich, L. L¨ucken, J. Rummel, P. Wagner, and\nE. Wießner, “Microscopic trafﬁc simulation using sumo,” in\nThe 21st IEEE International Conference on Intelligent Transportation\nSystems.\nIEEE, 2018. [Online]. Available: https://elib.dlr.de/\n124092/\n[47] Y. Gao, H. Xu, J. Lin, F. Yu, S. Levine, and T. Darrell,\n“Reinforcement learning from imperfect demonstrations,” 2018.\n[Online]. Available: https://openreview.net/forum?id=BJJ9bz-0-\n[48] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep rein-\nforcement learning,” arXiv preprint arXiv:1509.02971, 2015.\n[49] G. A. Rummery and M. Niranjan, On-line Q-learning using con-\nnectionist systems.\nUniversity of Cambridge, Department of\nEngineering Cambridge, UK, 1994, vol. 37.\n[50] J.-C. Shi, Y. Yu, Q. Da, S.-Y. Chen, and A.-X. Zeng, “Virtual-taobao:\nVirtualizing real-world online retail environment for reinforce-\nment learning,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 33, 2019, pp. 4902–4909.\n[51] X. Bai, J. Guan, and H. Wang, “A model-based reinforcement\nlearning with adversarial training for online recommendation,” in\nAdvances in Neural Information Processing Systems, 2019, pp. 10 735–\n10 746.\n[52] F. Pan, Q. Cai, P. Tang, F. Zhuang, and Q. He, “Policy gradients for\ncontextual recommendations,” in The World Wide Web Conference,\n2019, pp. 1421–1431.\n[53] X. Chen, S. Li, H. Li, S. Jiang, Y. Qi, and L. Song, “Generative\nadversarial user model for reinforcement learning based recom-\nmendation system,” in International Conference on Machine Learning,\n2019, pp. 1052–1061.\n[54] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learn-\ning affordance for direct perception in autonomous driving,” in\nProceedings of the IEEE International Conference on Computer Vision,\n2015, pp. 2722–2730.\n[55] N. Ballas, L. Yao, C. Pal, and A. Courville, “Delving deeper into\nconvolutional networks for learning video representations,” arXiv\npreprint arXiv:1511.06432, 2015.\n[56] S. Fujimoto, H. Van Hoof, and D. Meger, “Addressing func-\ntion approximation error in actor-critic methods,” arXiv preprint\narXiv:1802.09477, 2018.\n[57] H. Liu and M. Zamanian, “Framework for selecting and delivering\nadvertisements over a network based on combined short-term and\nlong-term user behavioral interests,” Mar. 15 2007, uS Patent App.\n11/225,238.\n[58] C. Y. Chung, A. Gupta, J. M. Koran, L.-J. Lin, and H. Yin, “Incre-\nmental update of long-term and short-term user proﬁle scores in\na behavioral targeting system,” Mar. 8 2011, uS Patent 7,904,448.\n[59] Y. Song, A. M. Elkahky, and X. He, “Multi-rate deep learning for\ntemporal recommendation,” in Proceedings of the 39th International\nACM SIGIR conference on Research and Development in Information\nRetrieval, 2016, pp. 909–912.\n[60] Q. Pi, W. Bian, G. Zhou, X. Zhu, and K. Gai, “Practice on\nlong sequential user behavior modeling for click-through rate\nprediction,” in Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 2019, pp. 2671–\n2679.\n[61] L. Zou, L. Xia, P. Du, Z. Zhang, T. Bai, W. Liu, J.-Y. Nie, and D. Yin,\n“Pseudo dyna-q: A reinforcement learning framework for inter-\nactive recommendation,” in Proceedings of the 13th International\nConference on Web Search and Data Mining, 2020, pp. 816–824.\n[62] K. Zhao, X. Wang, Y. Zhang, L. Zhao, Z. Liu, C. Xing, and X. Xie,\n“Leveraging demonstrations for reinforcement recommendation\nreasoning over knowledge graphs,” in Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development\nin Information Retrieval, 2020, pp. 239–248.\n[63] P. Wang, Y. Fan, L. Xia, W. X. Zhao, S. Niu, and J. Huang, “Kerl:\nA knowledge-guided reinforcement learning model for sequential\nrecommendation,” in Proceedings of the 43rd International ACM SI-\nGIR Conference on Research and Development in Information Retrieval,\n2020, pp. 209–218.\n[64] W. Shang, Y. Yu, Q. Li, Z. Qin, Y. Meng, and J. Ye, “Environ-\nment reconstruction with hidden confounders for reinforcement\nlearning based recommendation,” in Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining, 2019, pp. 566–576.\n[65] S. J. Lee and Z. Popovi´c, “Learning behavior styles with inverse re-\ninforcement learning,” ACM transactions on graphics (TOG), vol. 29,\nno. 4, pp. 1–7, 2010.\n[66] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized\nexperience replay,” arXiv preprint arXiv:1511.05952, 2015.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.IR"
  ],
  "published": "2021-05-03",
  "updated": "2021-05-05"
}