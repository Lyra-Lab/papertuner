{
  "id": "http://arxiv.org/abs/2105.01735v1",
  "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish",
  "authors": [
    "Robert Mroczkowski",
    "Piotr Rybak",
    "Alina Wróblewska",
    "Ireneusz Gawlik"
  ],
  "abstract": "BERT-based models are currently used for solving nearly all Natural Language\nProcessing (NLP) tasks and most often achieve state-of-the-art results.\nTherefore, the NLP community conducts extensive research on understanding these\nmodels, but above all on designing effective and efficient training procedures.\nSeveral ablation studies investigating how to train BERT-like models have been\ncarried out, but the vast majority of them concerned only the English language.\nA training procedure designed for English does not have to be universal and\napplicable to other especially typologically different languages. Therefore,\nthis paper presents the first ablation study focused on Polish, which, unlike\nthe isolating English language, is a fusional language. We design and\nthoroughly evaluate a pretraining procedure of transferring knowledge from\nmultilingual to monolingual BERT-based models. In addition to multilingual\nmodel initialization, other factors that possibly influence pretraining are\nalso explored, i.e. training objective, corpus size, BPE-Dropout, and\npretraining length. Based on the proposed procedure, a Polish BERT-based\nlanguage model -- HerBERT -- is trained. This model achieves state-of-the-art\nresults on multiple downstream tasks.",
  "text": "HerBERT: Efﬁciently Pretrained Transformer-based\nLanguage Model for Polish\nRobert Mroczkowski1 Piotr Rybak1 Alina Wróblewska2 Ireneusz Gawlik1,3\n1ML Research at Allegro.pl\n2Institute of Computer Science, Polish Academy of Sciences\n3Department of Computer Science, AGH University of Science and Technology\n{firstname.lastname}@allegro.pl, alina@ipipan.waw.pl\nAbstract\nBERT-based models are currently used for\nsolving nearly all Natural Language Process-\ning (NLP) tasks and most often achieve state-\nof-the-art results.\nTherefore, the NLP com-\nmunity conducts extensive research on under-\nstanding these models, but above all on design-\ning effective and efﬁcient training procedures.\nSeveral ablation studies investigating how to\ntrain BERT-like models have been carried out,\nbut the vast majority of them concerned only\nthe English language. A training procedure de-\nsigned for English does not have to be univer-\nsal and applicable to other especially typolog-\nically different languages. Therefore, this pa-\nper presents the ﬁrst ablation study focused on\nPolish, which, unlike the isolating English lan-\nguage, is a fusional language. We design and\nthoroughly evaluate a pretraining procedure of\ntransferring knowledge from multilingual to\nmonolingual BERT-based models. In addition\nto multilingual model initialization, other fac-\ntors that possibly inﬂuence pretraining are also\nexplored, i.e. training objective, corpus size,\nBPE-Dropout, and pretraining length. Based\non the proposed procedure, a Polish BERT-\nbased language model – HerBERT – is trained.\nThis model achieves state-of-the-art results on\nmultiple downstream tasks.\n1\nIntroduction\nRecent advancements in self-supervised pretrain-\ning techniques drastically changed the way we de-\nsign Natural Language Processing (NLP) systems.\nEven though, pretraining has been present in NLP\nfor many years (Mikolov et al., 2013; Pennington\net al., 2014; Bojanowski et al., 2017), only recently\nwe observed a shift from task-speciﬁc to general-\npurpose models. In particular, the BERT model\n(Devlin et al., 2019) proved to be a dominant ar-\nchitecture and obtained state-of-the-art results for\na variety of NLP tasks.\nWhile most of the research related to analyzing\nand improving BERT-based models focuses on En-\nglish, there is an increasing body of work aimed\nat training and evaluation of models for other lan-\nguages, including Polish. Thus far, a handful of\nmodels speciﬁc for Polish has been released, e.g.\nPolbert1, ﬁrst version of HerBERT (Rybak et al.,\n2020), and Polish RoBERTa (Dadas et al., 2020).\nAforementioned works lack ablation studies,\nmaking it difﬁcult to attribute hyperparameters\nchoices to models performance. In this work, we\nﬁll this gap by conducting an extensive set of exper-\niments and developing an efﬁcient BERT training\nprocedure. As a result, we were able to train and\nrelease a new BERT-based model for Polish lan-\nguage understanding. Our model establishes a new\nstate-of-the-art on the variety of downstream tasks\nincluding semantic relatedness, question answer-\ning, sentiment analysis and part-of-speech tagging.\nTo summarize, our contributions are:\n1. development and evaluation of an efﬁcient\npretraining procedure for transferring knowl-\nedge from multilingual to monolingual lan-\nguage models based on work by Arkhipov\net al. (2019),\n2. detailed analysis and an ablation study chal-\nlenging the effectiveness of Sentence Struc-\ntural Objective (SSO, Wang et al., 2020), and\nByte Pair Encoding Dropout (BPE-Dropout,\nProvilkov et al., 2020),\n3. release of HerBERT2 – a BERT-based model\nfor Polish language understanding, which\nachieves state-of-the-art results on KLEJ\nBenchmark (Rybak et al., 2020) and POS tag-\nging task (Wróblewska, 2020).\n1https://github.com/kldarek/polbert\n2https://huggingface.co/allegro/\nherbert-large-cased\narXiv:2105.01735v1  [cs.CL]  4 May 2021\nThe rest of the paper is organized as follows. In\nSection 2, we provide an overview of related work.\nAfter that, Section 3 introduces the BERT-based\nlanguage model and experimental setup used in this\nwork. In Section 4, we conduct a thorough ablation\nstudy to investigate the impact of several design\nchoices on the performance of downstream tasks.\nNext, in Section 5 we apply drawn conclusions\nand describe the training of HerBERT model. In\nSection 6, we evaluate HerBERT on a set of eleven\ntasks and compare its performance to other state-\nof-the-art models. Finally, we conclude our work\nin Section 7.\n2\nRelated Work\nThe ﬁrst signiﬁcant ablation study of BERT-based\nlanguage pretraining was described by Liu et al.\n(2019). Authors demonstrated the ineffectiveness\nof Next Sentence Prediction (NSP) objective, the\nimportance of dynamic token masking, and gains\nfrom using both large batch size and large training\ndataset. Further large-scale studies analyzed the\nrelation between the model and the training dataset\nsizes (Kaplan et al., 2020), the amount of compute\nused for training (Brown et al., 2020) and training\nstrategies and objectives (Raffel et al., 2019).\nOther work focused on studying and improv-\ning BERT training objectives. As mentioned be-\nfore, the NSP objective was either removed (Liu\net al., 2019) or enhanced either by predicting the\ncorrect order of sentences (Sentence Order Pre-\ndiction (SOP), Lan et al., 2020) or discriminat-\ning between previous, next and random sentence\n(Sentence Structural Objective (SSO), Wang et al.,\n2020). Similarly, the Masked Language Modelling\n(MLM) objective was extended to either predict\nspans of tokens (Joshi et al., 2019), re-order shuf-\nﬂed tokens (Word Structural Objective (WSO),\nWang et al., 2020) or replaced altogether with a\nbinary classiﬁcation problem using mask genera-\ntion (Clark et al., 2020).\nFor tokenization, the Byte Pair Encoding algo-\nrithm (BPE, Sennrich et al., 2016) is commonly\nused. The original BERT model used WordPiece\nimplementation (Schuster and Nakajima, 2012),\nwhich was later replaced by SentencePiece (Kudo\nand Richardson, 2018). Gong et al. (2018) dis-\ncovered that rare words lack semantic meaning.\nProvilkov et al. (2020) proposed a BPE-Dropout\ntechnique to solve this issue.\nAll of the above work was conducted for En-\nglish language understanding. There was little re-\nsearch into understanding how different pretrain-\ning techniques affect BERT-based models for other\nlanguages. The main research focus was to train\nBERT-based models and report their performance\non downstream tasks. The ﬁrst such models were\nreleased for German3 and Chinese (Devlin et al.,\n2019), recently followed by Finnish (Virtanen et al.,\n2019), French (Martin et al., 2020; Le et al., 2020),\nPolish (Rybak et al., 2020; Dadas et al., 2020),\nRussian (Kuratov and Arkhipov, 2019), and many\nother languages4. Research on developing and in-\nvestigating an efﬁcient procedure of pretraining\nBERT-based models was rather neglected in these\nlanguages.\nLanguage understanding for low-resource lan-\nguages has also been addressed by training jointly\nfor several languages at the same time. That ap-\nproach improves performance for moderate and\nlow-resource languages as showed by Conneau and\nLample (2019). The ﬁrst model of this kind was the\nmultilingual BERT trained for 104 languages (De-\nvlin et al., 2019) followed by Conneau and Lample\n(2019) and Conneau et al. (2020).\n3\nExperimental Setup\nIn this section, we describe the experimental setup\nused in the ablation study. First, we introduce the\ncorpora we used to train models. Then, we give an\noverview of the language model architecture and\ntraining procedure. In particular, we describe the\nmethod of transferring knowledge from multilin-\ngual to monolingual BERT-based models. Finally,\nwe present the evaluation tasks.\n3.1\nTraining Data\nWe gathered six corpora to create two datasets\non which we trained HerBERT. The ﬁrst dataset\n(henceforth called Small) consists of corpora of\nthe highest quality, i.e.\nNKJP, Wikipedia, and\nWolne Lektury. The second dataset (Large) is over\nﬁve times larger as it additionally contains texts of\nlower quality (CCNet and Open Subtitles). Below,\nwe present a short description of each corpus. Ad-\nditionally, we include the basic corpora statistics in\nTable 1.\nNKJP\n(Narodowy Korpus J˛ezyka Polskiego, eng.\nNational Corpus of Polish) (Przepiórkowski, 2012)\nis a well balanced collection of Polish texts. It\n3https://deepset.ai/german-bert\n4https://huggingface.co/models\nCorpus\nTokens Documents Avg len\nSource Corpora\nNKJP\n1357M\n3.9M\n347\nWikipedia\n260M\n1.4M\n190\nWolne Lektury\n41M\n5.5k\n7447\nCCNet Head\n2641M\n7.0M\n379\nCCNet Middle\n3243M\n7.9M\n409\nOpen Subtitles\n1056M\n1.1M\n961\nFinal Corpora\nSmall\n1658M\n5.3M\n313\nLarge\n8599M\n21.3M\n404\nTable 1: Overview of all data sources used to train Her-\nBERT. We combine them into two corpora. The Small\ncorpus consists of the highest quality text resources:\nNKJP, Wikipedia, and Wolne Lektury. The Large cor-\npus consists of all sources. Avg len is the average num-\nber of tokens per document in each corpus.\nconsists of texts from many different sources, such\nas classic literature, books, newspapers, journals,\ntranscripts of conversations, and texts crawled from\nthe internet.\nWikipedia\nis an online encyclopedia created by\nthe community of Internet users. Even though it is\ncrowd-sourced, it is recognized as a high-quality\ncollection of articles.\nWolne Lektury\n(eng. Free Readings)5 is a col-\nlection of over ﬁve thousand books and poems,\nmostly from 19th and 20th century, which have\nalready fallen in the public domain.\nCCNet\n(Wenzek et al., 2020) is a clean mono-\nlingual corpus extracted from Common Crawl6\ndataset of crawled websites.\nOpen Subtitles\nis a popular website offering\nmovie and TV subtitles, which was used by Lison\nand Tiedemann (2016) to curate and release a mul-\ntilingual parallel corpus from which we extracted\nits monolingual Polish part.\n3.2\nLanguage Model\nTokenizer\nWe used Byte-Pair Encoding (BPE)\ntokenizer (Sennrich et al., 2016) with the vocabu-\nlary size of 50k tokens and trained it on the most\n5https://wolnelektury.pl\n6http://commoncrawl.org/\nrepresentative parts of our corpus, i.e annotated\nsubset of the NKJP, and the Wikipedia.\nSubword regularization is supposed to empha-\nsize the semantic meaning of tokens (Gong et al.,\n2018; Provilkov et al., 2020). To verify its im-\npact on training language model we used a BPE-\nDropout (Provilkov et al., 2020) with a probability\nof dropping a merge equal to 10%.\nArchitecture\nWe followed the original BERT\n(Devlin et al., 2019) architectures for both BASE\n(12 layers, 12 attention heads and hidden dimen-\nsion of 768) and LARGE (24 layers, 16 attention\nheads and hidden dimension of 1024) variants.\nInitialization\nWe initialized models either ran-\ndomly or by using weights from XLM-RoBERTa\n(Conneau et al., 2020). In the latter case, the pa-\nrameters for all layers except word embeddings and\ntoken type embeddings were copied directly from\nthe source model. Since XLM-RoBERTa does not\nuse the NSP objective and does not have the token\ntype embeddings, we took them from the original\nBERT model.\nTo overcome the difference in tokenizers vocabu-\nlaries we used a method similar to (Arkhipov et al.,\n2019). If a token from the target model vocabulary\nwas present in the source model vocabulary then\nwe directly copied its weights.\nOtherwise, it was split into smaller units and the\nembedding was obtained by averaging sub-tokens\nembeddings.\nTraining Objectives\nWe trained all models with\nan updated version of the MLM objective (Joshi\net al., 2019; Martin et al., 2020), masking ranges\nof subsequent tokens belonging to single words in-\nstead of individual (possibly subword) tokens. We\nreplaced the NSP objective with SSO. The other\nparameters were kept the same as in the original\nBERT paper. Training objective is deﬁned in Equa-\ntion 1.\nL = LMLM(θ) + α · LSSO(θ)\n(1)\nwhere α is the SSO weight.\n3.3\nTasks\nKLEJ Benchmark\nThe standard method for\nevaluating pretrained language models is to use\na diverse collection of tasks grouped into a sin-\ngle benchmark. Such benchmarks exist in many\nlanguages, e.g.\nEnglish (GLUE, Wang et al.,\n2019), Chinese (CLUE, Xu et al., 2020), and Polish\n(KLEJ, Rybak et al., 2020).\nFollowing this paradigm we ﬁrst veriﬁed the\nquality of assessed models with KLEJ. It consists\nof nine tasks: name entity classiﬁcation (NKJP-\nNER, Przepiórkowski, 2012), semantic relatedness\n(CDSC-R, Wróblewska and Krasnowska-Kiera´s,\n2017), natural language inference (CDSC-E,\nWróblewska and Krasnowska-Kiera´s, 2017), cyber-\nbullying detection (CBD, Ptaszynski et al., 2019),\nsentiment analysis (PolEmo2.0-IN, PolEmo2.0-\nOUT, Koco´n et al., 2019, AR, Rybak et al., 2020),\nquestion answering (Czy wiesz?, Marcinczuk et al.,\n2013), and text similarity (PSC, Ogrodniczuk and\nKope´c, 2014).\nPOS Tagging and Dependency Parsing\nAll of\nthe KLEJ Benchmark tasks belong to the classiﬁ-\ncation or regression type. It is therefore difﬁcult to\nassess the quality of individual token embeddings.\nTo address this issue, we further evaluated Her-\nBERT on part-of-speech tagging and dependency\nparsing tasks.\nFor tagging, we used the manually annotated sub-\nset of NKJP (Degórski and Przepiórkowski, 2012),\nconverted to the CoNLL-U format by Wróblewska\n(2020). We evaluated models performance on a test\nset using accuracy and F1-Score.\nFor dependency parsing, we applied Polish De-\npendency Bank (Wróblewska, 2018) from the Uni-\nversal Dependencies repository (release 2.5, Zeman\net al., 2019).\nIn addition to three Transformer-based models,\nwe also included models trained with static em-\nbeddings.\nThe ﬁrst one did not use pretrained\nembeddings while the latter utilized fastText (Bo-\njanowski et al., 2017) embeddings trained on Com-\nmon Crawl.\nThe models are evaluated with the standard met-\nrics: UAS (unlabeled attachment score) and LAS\n(labelled attachment score). The gold-standard seg-\nmentation was preserved. We report the results on\nthe test set.\n4\nAblation Study\nIn this section, we analyze the impact of several\ndesign choices on downstream task performance of\nPolish BERT-based models. In particular, we focus\non initialization, corpus size, training objective,\nBPE-Dropout, and the length of pretraining.\n4.1\nExperimental Design\nHyperparameters\nUnless stated otherwise, in\nall experiments we trained BERTBASE model ini-\ntialized with XLM-RoBERTa weights for 10k iter-\nations using a linear decay schedule of the learning\nrate with a peak value of 7 · 10−4 and a warm-up\nof 500 iterations. We used a batch size of 2560.\nEvaluation\nDifferent experimental setups were\ncompared using the average score on the KLEJ\nBenchmark. The validation sets are used for evalu-\nation and we report the results corresponding to the\nmedian values of the ﬁve runs. Since only six tasks\nin KLEJ Benchmark have validation sets the scores\nare not directly comparable to those reported in\nSection 6. We used Welch’s t-test (Welch, 1947)\nwith a p-value of 0.01 to test for statistical differ-\nences between experimental variants.\n4.2\nResults\nInitialization\nOne of the main goals of this work\nis to propose an efﬁcient strategy to train a monolin-\ngual language model. We began with investigating\nthe impact of pretraining the language model itself.\nFor this purpose, the following experiments were\ndesigned.\nInit\nPretraining\nBPE\nScore\nAblation Models\nRandom\nNo\n-\n58.15 ± 0.33\nXLM-R\nNo\n-\n83.15 ± 1.22\nRandom\nYes\nNo\n85.65 ± 0.43\nXLM-R\nYes\nNo\n88.80 ± 0.15\nRandom\nYes\nYes\n85.78 ± 0.23\nXLM-R\nYes\nYes\n89.10 ± 0.19\nOriginal Models\nXLM-R\n-\n-\n88.82 ± 0.15\nTable 2: Average scores on KLEJ Benchmark depend-\ning on the initialization scheme: Random – initializa-\ntion with random weights, XLM-R – initialization with\nXLM-RoBERTa weights. We used BERTBASE model\ntrained for 10k iterations with the SSO weight equal to\n1.0 on the Large corpus. The best score within each\ngroup is underlined, the best overall is bold.\nFirst, we ﬁne-tuned randomly initialized BERT\nmodel on KLEJ Benchmark tasks. Note that this\nmodel is not pretrained in any way. As expected,\nthe results on the KLEJ Benchmark are really poor\nwith the average score equal to 58.15.\nNext, we evaluated the BERT model initialized\nwith XLM-RoBERTa weights (see Table 2). It\nachieved much better average score than the ran-\ndomly initialized model (83.15 vs 58.15), but it was\nstill not as good as the original XLM-RoBERTa\nmodel (88.82%). The difference in the performance\ncan be explained by the transfer efﬁciency. The\nmethod of transferring token embeddings between\ndifferent tokenizers proves to retain most informa-\ntion, but not all of it.\nTo measure the impact of initialization on pre-\ntraining optimization, we trained the aforemen-\ntioned models for 10k iterations. Beside MLM\nobjective, we used SSO loss with α = 1.0 and\nconducted experiments with both enabled and dis-\nabled BPE-Dropout. Models initialized with XLM-\nRoBERTa achieve signiﬁcantly higher results than\nmodels initialized randomly, 89.10 vs 85.78 and\n88.80 vs 85.65 for pretraining with and without\nBPE-Dropout respectively.\nModels\ninitialized\nwith\nXLM-RoBERTa\nachieved similar results to the original XLM-\nRoBERTa (the differences are not statistically\nsigniﬁcant). It proves that it is possible to quickly\nrecover from the performance drop caused by a\ntokenizer conversion procedure and obtain a much\nbetter model than the one initialized randomly.\nCorpus Size\nAs mentioned in Section 2, previ-\nous research show that pretraining on a larger cor-\npus is beneﬁcial for downstream task performance\n(Kaplan et al., 2020; Brown et al., 2020). We in-\nvestigated this by pretraining BERTBASE model on\nboth Small and Large corpora (see Section 3.1). To\nmitigate a possible impact of confounding variable,\nwe also vary the weight of SSO loss and usage of\nBPE-Dropout (see Table 3).\nAs expected, the model pretrained on a Large\ncorpus performs better on downstream tasks. How-\never, the difference is statistically signiﬁcant only\nfor the experiment with SSO weight equal to 1.0\nand BPE-Dropout enabled. Therefore it’s not obvi-\nous whether a larger corpus is actually beneﬁcial.\nSentence Structural Objective\nSubsequently,\nwe tested SSO, i.e. the recently introduced replace-\nment for the NSP objective, which proved to be in-\neffective. We compared models trained with three\nvalues of SSO weight α (see Section 3.2): 0.0 (no\nSSO), 0.1 (small impact of SSO), and 1.0 (SSO\nCorpus\nSSO\nBPE\nScore\nSmall\n1.0\nYes\n88.73 ± 0.08\nLarge\n1.0\nYes\n89.10 ± 0.19\nSmall\n0.1\nYes\n88.90 ± 0.24\nLarge\n0.1\nYes\n89.37 ± 0.25\nSmall\n0.0\nYes\n89.18 ± 0.15\nLarge\n0.0\nYes\n89.25 ± 0.21\nSmall\n0.0\nNo\n89.12 ± 0.29\nLarge\n0.0\nNo\n89.28 ± 0.26\nTable 3: Average scores on KLEJ Benchmark depend-\ning on a corpus size. We used BERTBASE model trained\nfor 10k iterations with or without BPE-Dropout and\nwith various SSO weights. The best score within each\ngroup is underlined, the best overall is bold.\nequally important as MLM objective) (see Table\n4).\nThe experiment showed, that SSO actually hurts\ndownstream task performance. The differences\nbetween enabled and disabled SSO are statistically\nsigniﬁcant for two out of three experimental setups.\nThe only scenario for which the negative effect\nof SSO is not statistically signiﬁcant is using the\nLarge corpus and BPE-dropout. Overall, the best\nresults are achieved using a small SSO weight but\nthe differences are not signiﬁcantly different from\ndisabling SSO.\nSSO\nCorpus\nBPE\nScore\n0.0\nSmall\nYes\n89.18 ± 0.15\n0.1\nSmall\nYes\n88.90 ± 0.24\n1.0\nSmall\nYes\n88.73 ± 0.08\n0.0\nLarge\nYes\n89.25 ± 0.21\n0.1\nLarge\nYes\n89.37 ± 0.25\n1.0\nLarge\nYes\n89.10 ± 0.19\n0.0\nLarge\nNo\n89.28 ± 0.26\n0.1\nLarge\nNo\n89.45 ± 0.18\n1.0\nLarge\nNo\n88.80 ± 0.15\nTable 4: Average scores on KLEJ Benchmark depend-\ning on a SSO weight.\nWe used BERTBASE model\ntrained for 10k iterations with BPE-Dropout. The best\nscore within each group is underlined, the best overall\nis bold.\nBPE-Dropout\nThe BPE-Dropout could be ben-\neﬁcial for downstream task performance, but its\nimpact is difﬁcult to assess due to many confound-\ning variables.\nThe model initialization with XLM-RoBERTa\nweights means that token embedding is already\nsemantically meaningful even without additional\npretraining. However, for both random and XLM-\nRoBERTa initialization the BPE-Dropout is beneﬁ-\ncial.\nBPE\nInit\nSSO\nCorpus\nScore\nNo\nRandom\n1.0\nLarge\n85.65 ± 0.43\nYes\nRandom\n1.0\nLarge\n85.78 ± 0.23\nNo\nXLM-R\n1.0\nLarge\n88.80 ± 0.15\nYes\nXLM-R\n1.0\nLarge\n89.10 ± 0.19\nNo\nXLM-R\n0.0\nLarge\n89.28 ± 0.26\nYes\nXLM-R\n0.0\nLarge\n89.25 ± 0.21\nNo\nXLM-R\n0.0\nSmall\n89.12 ± 0.29\nYes\nXLM-R\n0.0\nSmall\n89.18 ± 0.15\nTable 5: Average scores on KLEJ Benchmark depend-\ning on usage of BPE-Dropout.\nWe used BERTBASE\nmodel trained for 10k iterations on a large corpus. The\nbest score within each group is underlined, the best\noverall is bold.\nAccording to the results (see Table 5), none of\nthe reported differences is statistically signiﬁcant\nand we can only conclude that BPE-Dropout does\nnot inﬂuence the model performance.\nLength of Pretraining\nThe length of pretraining\nin terms of the number of iterations is commonly\nconsidered an important factor of the ﬁnal quality\nof the model (Kaplan et al., 2020). Even though it\nseems straightforward to validate this hypothesis\nin practice it is not so trivial.\nWhen pretraining Transformer-based models lin-\near decaying learning rate is typically used. There-\nfore, increasing the number of training iterations\nchanges the learning rate schedule and impacts the\ntraining. In our initial experiments usage of the\nsame learning rate caused the longer training to\ncollapse. Instead, we chose the learning rate for\nwhich the value of loss function after 10k steps was\nsimilar. We found that the learning rate equal to\n3 · 10−4 worked best for training in 50k steps.\nUsing the presented experiment setup, we tested\nthe impact of pretraining length for two values\n# Iter\nLR\nSSO\nScore\n10k\n7 · 10−4\n1.0\n89.10 ± 0.19\n50k\n3 · 10−4\n1.0\n89.43 ± 0.10\n10k\n7 · 10−4\n0.1\n89.37 ± 0.25\n50k\n3 · 10−4\n0.1\n89.87 ± 0.22\nTable 6: Average scores on KLEJ Benchmark depend-\ning training length. We used BERTBASE model trained\non a large corpus with BPE-Dropout. The best score\nwithin each group is underlined, the best overall is bold.\nof SSO weight: 1.0 and 0.1. In both cases, the\nmodel pretrained with more iterations achieves\nonly slightly better but statistically signiﬁcant re-\nsults (see Table 6).\n5\nHerBERT\nIn this section, we apply conclusions drawn from\nthe ablation study (see Section 4) and describe the\nﬁnal pretraining procedure used to train HerBERT\nmodel.\nPretraining Procedure\nHerBERT was trained\non the Large corpus. We used Dropout-BPE in\ntokenizer with a probability of a drop equals to 10%.\nFinally, HerBERT models were initialized with\nweights from XLM-RoBERTa and were trained\nwith the objective deﬁned in Equation 1 with SSO\nweight equal to 0.1.\nOptimization\nWe trained HerBERTBASE using\nAdam optimizer (Kingma and Ba, 2014) with pa-\nrameters: β1 = 0.9, β2 = 0.999, ϵ = 10−8 and\na linear decay learning rate schedule with a peak\nvalue of 3 · 10−4. Due to the initial transfer of\nweights from the already trained model, the warm-\nup stage was set to a relatively small number of 500\niterations. The whole training took 50k iterations.\nTraining of HerBERTLARGE was longer (60k iter-\nations) and had a more complex learning rate sched-\nule. For the ﬁrst 15k we linearly decayed the learn-\ning rate from 3 · 10−4 to 2.5 · 10−4. We observed\nthe saturation of evaluation metrics and decided to\ndrop the learning rate to 1 · 10−4. After training\nfor another 25k steps and reaching the learning rate\nof 7 · 10−5 we again reached the plateau of eval-\nuation metrics. In the last phase of training, we\ndropped the learning rate to 3 · 10−5 and trained\nfor 20k steps until it reached zero. Additionally,\nduring the last phase of training, we disabled both\nModel\nAVG\nNKJP-NER\nCDSC-E\nCDSC-R\nCBD\nPolEmo2.0-IN\nPolEmo2.0-OUT\nCzy wiesz?\nPSC\nAR\nBase Models\nXLM-RoBERTa\n84.7 ± 0.29\n91.7\n93.3\n93.4\n66.4\n90.9\n77.1\n64.3\n97.6\n87.3\nPolish RoBERTa\n85.6 ± 0.29\n94.0\n94.2\n94.2\n63.6\n90.3\n76.9\n71.6\n98.6\n87.4\nHerBERT\n86.3 ± 0.36\n94.5\n94.5\n94.0\n67.4\n90.9\n80.4\n68.1\n98.9\n87.7\nLarge Models\nXLM-RoBERTa\n86.8 ± 0.30\n94.2\n94.7\n93.9\n67.6\n92.1\n81.6\n70.0\n98.3\n88.5\nPolish RoBERTa\n87.5 ± 0.29\n94.9\n93.4\n94.7\n69.3\n92.2\n81.4\n74.1\n99.1\n88.6\nHerBERT\n88.4 ± 0.19\n96.4\n94.1\n94.9\n72.0\n92.2\n81.8\n75.8\n98.9\n89.1\nTable 7: Evaluation results on KLEJ Benchmark. AVG is the average score across all tasks. Scores are reported for\ntest set and correspond to median values across ﬁve runs. The best scores within each group are underlined, the\nbest overall are in bold.\nBPE-Dropout and dropout within the Transformer\nitself as suggested by Lan et al. (2020).\nBoth HerBERTBASE and HerBERTLARGE mod-\nels were trained with a batch size of 2560.\n6\nEvaluation\nIn this section, we introduce other top-performing\nmodels for Polish language understanding and com-\npare their performance on evaluation tasks (see\nSection 3.3) to HerBERT.\n6.1\nModels\nAccording to the KLEJ Benchmark leaderboard7\nthe three top-performing models of Polish language\nunderstanding are XLM-RoBERTa-NKJP8, Polish\nRoBERTa, and XLM-RoBERTa. These are also the\nonly three models available in LARGE architecture\nvariant.\nUnfortunately,\nthe\nXLM-RoBERTa-NKJP\nmodel is not publicly available, so we cannot use\nit for our evaluation. However, it has the same\naverage score as the runner-up (Polish RoBERTa)\nwhich we compare HerBERT with.\n7https://klejbenchmark.com/\nleaderboard/\n8XLM-RoBERTa-NKJP is XLM-RoBERTa model addi-\ntionally ﬁne-tuned on NKJP corpus.\n6.2\nResults\nKLEJ Benchmark\nBoth variants of HerBERT\nachieved the best average performance, signiﬁ-\ncantly outperforming Polish RoBERTa and XLM-\nRoBERTa (see Table 7). Regarding BASE models,\nHerBERTBASE improves the state-of-the-art result\nby 0.7pp and for HerBERTLARGE the improvement\nis even bigger (0.9pp). In particular, HerBERTBASE\nscores best on eight out of nine tasks (tying on\nPolEmo2.0-OUT, performing slightly worse on\nCDSC-R) and HerBERTLARGE in seven out of nine\ntasks (tying in PolEmo2.0-IN, performing worse\nin CDSC-E and PSC). Surprisingly, HerBERTBASE\nis better than HerBERTLARGE in CDSC-E. The\nsame behaviour is noticeable for Polish RoBERTa,\nbut not for XLM-RoBERTa. For other tasks, the\nLARGE models are always better. Summing up,\nHerBERTLARGE is the new state-of-the-art Polish\nlanguage model based on the results of the KLEJ\nBenchmark.\nIt is worth emphasizing that the proposed proce-\ndure for efﬁcient pretraining by transferring knowl-\nedge from multilingual to monolingual language\nallowed HerBERT to achieve better results than Pol-\nish RoBERTa even though it was optimized with\naround ten times shorter training.\nPOS Tagging\nHerBERT achieves overall better\nresults in terms of both accuracy and F1-Score.\nModel\nAccuracy\nF1-Score\nBase Models\nXLM-RoBERTa\n95.97 ± 0.04\n95.79 ± 0.05\nPolish RoBERTa\n96.13 ± 0.03\n95.92 ± 0.03\nHerBERT\n96.46 ± 0.04\n96.27 ± 0.04\nLarge Models\nXLM-RoBERTa\n97.07 ± 0.05\n96.93 ± 0.05\nPolish RoBERTa\n97.21 ± 0.02\n97.05 ± 0.03\nHerBERT\n97.30 ± 0.02\n97.17 ± 0.02\nTable 8:\nPart-of-speech tagging results on NKJP\ndataset. Scores are reported for the test set and are me-\ndian values across ﬁve runs. Best scores within each\ngroup are underlined, best overall are bold.\nHerBERTBASE beats the second-best model (i.e.\nPolish RoBERTa) by a margin of 0.33pp (F1-Score\nby 0.35pp) and HerBERTLARGE by a margin of\n0.09pp (F1-Score by 0.12pp). It should be em-\nphasized that while the improvements may appear\nto be minor, they are statistically signiﬁcant. All\nresults are presented in Table 8.\nDependency Parsing\nThe dependency parsing\nresults are much more ambiguous than in other\ntasks. As expected, the models with static fast-\nText embeddings performed much worse than\nTransformer-based models (around 3pp differ-\nence for UAS, and 4pp for LAS). In the case of\nTransformer-based models, the differences are less\nnoticeable. As expected, the LARGE models out-\nperform the BASE models. The best performing\nmodel is Polish RoBERTa. HerBERT models per-\nformance is the worst across Transformer-based\nmodels except for the UAS score which is slightly\nbetter than XLM-RoBERTa for BASE models. All\nresults are presented in Table 9.\n7\nConclusion\nIn this work, we conducted a thorough ablation\nstudy regarding training BERT-based models for\nPolish language.\nWe evaluated several design\nchoices for pretraining BERT outside of English\nlanguage. Contrary to Wang et al. (2020), our ex-\nperiments demonstrated that SSO is not beneﬁcial\nfor the downstream task performance. It also turned\nout that BPE-Dropout does not increase the quality\nof a pretrained language model.\nModel\nUAS\nLAS\nStatic Embeddings\nPlain\n90.58 ± 0.07\n87.35 ± 0.12\nFastText\n92.20 ± 0.14\n89.57 ± 0.13\nBase Models\nXLM-RoBERTa\n95.14 ± 0.07\n93.25 ± 0.12\nPolish RoBERTa\n95.41 ± 0.24\n93.65 ± 0.34\nHerBERT\n95.18 ± 0.22\n93.24 ± 0.23\nLarge Models\nXLM-RoBERTa\n95.38 ± 0.02\n93.66 ± 0.07\nPolish RoBERTa\n95.60 ± 0.18\n93.90 ± 0.21\nHerBERT\n95.11 ± 0.04\n93.32 ± 0.02\nTable 9: Dependency parsing results on Polish Depen-\ndency Bank dataset. Scores are reported for the test set\nand are median values across three runs. Best scores\nwithin each group are underlined, best overall are bold.\nAs a result of our studies we developed and eval-\nuated an efﬁcient pretraining procedure for transfer-\nring knowledge from multilingual to monolingual\nBERT-based models. We used it to train and release\nHerBERT – a Transformer-based language model\nfor Polish. It was trained on a diverse multi-source\ncorpus. The conducted experiments conﬁrmed its\nhigh performance on a set of eleven diverse linguis-\ntic tasks, as HerBERT turned out to be the best on\neight of them. In particular, it is the best model\nfor Polish language understanding according to the\nKLEJ Benchmark.\nIt is worth emphasizing that the quality of the\nobtained language model was even more impres-\nsive considering its short training time. Due to\nmultilingual initialization, HerBERTBASE outper-\nformed Polish RoBERTaBASE even though it was\ntrained with a smaller batch size (2560 vs 8000) for\na fewer number of steps (50k vs 125k). The same\nbehaviour is also visible for HerBERTLARGE. Ad-\nditionally, we conducted a separate ablation study\nto conﬁrm that the success of HerBERT is caused\nby the described initialization scheme. It showed\nthat in fact, it was the most important factor to\nimproved the quality of HerBERT.\nWe believe that the proposed training procedure\nand detailed experiments will encourage NLP re-\nsearchers to cost-effectively train language models\nfor other languages.\nReferences\nMikhail Arkhipov, Maria Troﬁmova, Yuri Kuratov, and\nAlexey Sorokin. 2019.\nTuning multilingual trans-\nformers for language-speciﬁc named entity recogni-\ntion. In Proceedings of the 7th Workshop on Balto-\nSlavic Natural Language Processing, pages 89–93,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal et al. 2020.\nLanguage models are few-shot learners.\nCoRR,\nabs/2005.14165.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020.\nELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators.\nIn 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán et al. 2020. Unsupervised cross-lingual rep-\nresentation learning at scale. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 8440–8451, Online. As-\nsociation for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining.\nIn H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 7059–\n7069. Curran Associates, Inc.\nSławomir Dadas, Michał Perełkiewicz, and Rafał\nPo´swiata. 2020.\nPre-training polish transformer-\nbased language models at scale.\nŁukasz Degórski and Adam Przepiórkowski. 2012.\nRecznie znakowany milionowy podkorpus NKJP,\npages 51–58.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: Frequency-agnostic\nword representation.\nIn S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information\nProcessing Systems 31, pages 1334–1345. Curran\nAssociates, Inc.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\nSpanbert:\nImproving\npre-training\nby\nrepre-\nsenting and predicting spans.\narXiv preprint\narXiv:1907.10529.\nJared Kaplan,\nSam McCandlish,\nTom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child et\nal. 2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nDiederik P. Kingma and Jimmy Ba. 2014.\nAdam:\nA method for stochastic optimization.\nCite\narxiv:1412.6980Comment: Published as a confer-\nence paper at the 3rd International Conference for\nLearning Representations, San Diego, 2015.\nJan Koco´n, Piotr Miłkowski, and Monika Za´sko-\nZieli´nska. 2019. Multi-level sentiment analysis of\nPolEmo 2.0: Extended corpus of multi-domain con-\nsumer reviews. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 980–991, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation\nof deep bidirectional multilingual transformers for\nrussian language.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020.\nALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne,\nMaximin Coavoux,\nBenjamin Lecouteux et al.\n2020.\nFlauBERT: Unsupervised language model\npre-training for French. In Proceedings of the 12th\nLanguage Resources and Evaluation Conference,\npages 2479–2490, Marseille, France. European Lan-\nguage Resources Association.\nPierre Lison and Jörg Tiedemann. 2016.\nOpensub-\ntitles2016: Extracting large parallel corpora from\nmovie and tv subtitles. In Proceedings of the Tenth\nInternational Conference on Language Resources\nand Evaluation (LREC 2016), Paris, France. Euro-\npean Language Resources Association (ELRA).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen et al. 2019. Roberta: A ro-\nbustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nMichał Marcinczuk, Marcin Ptak, Adam Radziszewski,\nand Maciej Piasecki. 2013.\nOpen dataset for de-\nvelopment of polish question answering systems.\nIn Proceedings of the 6th Language & Technology\nConference: Human Language Technologies as a\nChallenge for Computer Science and Linguistics,\nWydawnictwo Poznanskie, Fundacja Uniwersytetu\nim. Adama Mickiewicza.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie et al. 2020. CamemBERT: a tasty\nFrench language model. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 7203–7219, Online. Asso-\nciation for Computational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nMaciej Ogrodniczuk and Mateusz Kope´c. 2014. The\nPolish Summaries Corpus.\nIn Proceedings of the\nNinth International Conference on Language Re-\nsources and Evaluation, LREC 2014.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1532–1543, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nIvan Provilkov, Dmitrii Emelianenko, and Elena Voita.\n2020. BPE-dropout: Simple and effective subword\nregularization. pages 1882–1892.\nAdam Przepiórkowski. 2012. Narodowy korpus j˛ezyka\npolskiego. Naukowe PWN.\nMichal Ptaszynski, Agata Pieciukiewicz, and Paweł\nDybała. 2019. Results of the poleval 2019 shared\ntask 6: First dataset and open shared task for auto-\nmatic cyberbullying detection in polish twitter. Pro-\nceedings of the PolEval 2019 Workshop, page 89.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena et al.\n2019.\nExploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer.\nCoRR,\nabs/1910.10683.\nPiotr Rybak, Robert Mroczkowski, Janusz Tracz, and\nIreneusz Gawlik. 2020.\nKLEJ: Comprehensive\nbenchmark for polish language understanding.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1191–\n1201, Online. Association for Computational Lin-\nguistics.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search.\nIn 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152. IEEE.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units.\nIn Association for Computational\nLinguistics (ACL), pages 1715–1725.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski et al. 2019. Mul-\ntilingual is not enough: Bert for ﬁnnish.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia,\nZuyi Bao et al. 2020. Structbert: Incorporating lan-\nguage structures into pre-training for deep language\nunderstanding. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nB. L. Welch. 1947.\nTHE GENERALIZATION OF\n‘STUDENT’S’ PROBLEM WHEN SEVERAL DIF-\nFERENT POPULATION VARLANCES ARE IN-\nVOLVED. Biometrika, 34(1-2):28–35.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin et al. 2020. CCNet: Extracting high\nquality monolingual datasets from web crawl data.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4003–4012, Mar-\nseille, France. European Language Resources Asso-\nciation.\nAlina Wróblewska. 2018. Extended and enhanced pol-\nish dependency bank in universal dependencies for-\nmat. In Proceedings of the Second Workshop on Uni-\nversal Dependencies (UDW 2018), pages 173–182.\nAssociation for Computational Linguistics.\nAlina Wróblewska. 2020. Towards the conversion of\nNational Corpus of Polish to Universal Dependen-\ncies.\nIn Proceedings of the 12th Language Re-\nsources and Evaluation Conference, pages 5308–\n5315, Marseille, France. European Language Re-\nsources Association.\nAlina Wróblewska and Katarzyna Krasnowska-Kiera´s.\n2017. Polish evaluation dataset for compositional\ndistributional semantics models.\nIn Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 784–792.\nLiang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chenjie\nCao, Weitang Liu et al. 2020. Clue: A chinese lan-\nguage understanding evaluation benchmark.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Noëmi\nAepli, Željko Agi´c, Lars Ahrenberg et al. 2019. Uni-\nversal dependencies 2.5.\nLINDAT/CLARIAH-CZ\ndigital library at the Institute of Formal and Ap-\nplied Linguistics (ÚFAL), Faculty of Mathematics\nand Physics, Charles University.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-05-04",
  "updated": "2021-05-04"
}