{
  "id": "http://arxiv.org/abs/2105.01755v1",
  "title": "Reinforcement Learning for Scalable Logic Optimization with Graph Neural Networks",
  "authors": [
    "Xavier Timoneda",
    "Lukas Cavigelli"
  ],
  "abstract": "Logic optimization is an NP-hard problem commonly approached through\nhand-engineered heuristics. We propose to combine graph convolutional networks\nwith reinforcement learning and a novel, scalable node embedding method to\nlearn which local transforms should be applied to the logic graph. We show that\nthis method achieves a similar size reduction as ABC on smaller circuits and\noutperforms it by 1.5-1.75x on larger random graphs.",
  "text": "Reinforcement Learning for Scalable Logic\nOptimization with Graph Neural Networks\nXavier Timoneda, Lukas Cavigelli\nHuawei Technologies, Zurich Research Center, Switzerland\nAbstract—Logic optimization is an NP-hard problem com-\nmonly approached through hand-engineered heuristics. We pro-\npose to combine graph convolutional networks with reinforce-\nment learning and a novel, scalable node embedding method to\nlearn which local transforms should be applied to the logic graph.\nWe show that this method achieves a similar size reduction as\nABC on smaller circuits and outperforms it by 1.5–1.75× on\nlarger random graphs.\nI. INTRODUCTION & RELATED WORK\nLogic synthesis is a crucial optimization step in the ﬂow\nfrom the RTL description to the ﬁnal GDS data. Minimiz-\ning the gate count of a logic circuit, even without timing\nconstraints, is known to be NP-hard. With today’s available\ncompute power, logic functions with only a few inputs have\ncome within reach of optimal synthesis (e.g., using SAT\nsolvers). The synthesis of larger circuits, however, remains in\nthe hand of algorithms relying on heuristics.\nDeep learning has revolutionized the ﬁeld of computer\nvision and many others by replacing engineered heuristics\nwith multi-layered learned heuristics, leading to dramatic\nleaps in the quality of results (QoR). In the following, we\ninvestigate how the game of logic optimization can be taught\nto graph convolutional neural networks (GCNs) [1] operating\non a graph representation of logic circuits using reinforcement\nlearning (RL).\nTwo previous works have made ﬁrst steps in this direction.\nDRiLLS [2] has applied RL at a coarse-grained level to select\nwhich commands/heuristics of UC Berkeley’s ABC logic syn-\nthesis tool [3] should be applied, showing clear improvements\nin their QoR. Haaswijk et al. [4] have investigated the direct\napplication of RL to GCNs for logic optimization based\non majority-inverter graphs (MIGs), showing that they can\nachieve optimality for small logic functions by comparing the\nresults against synthesis based on SAT solvers. This method\nhas the key limitations that 1) its embedding method does not\nscale to larger circuits, requiring a larger embedding dimension\nas the circuit size increases, 2) it has to be trained on a graph\nwith an identical number of nodes as the one to be optimized,\nand 3) it only applies a single action at one node of the graph,\nresulting in O(N 2) compute time.\nThis preprint has been accepted for publication at DAC’21. © 2021 IEEE.\nPersonal use of this material is permitted. Permission from IEEE must\nbe obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes,\ncreating new collective works, for resale or redistribution to servers or lists,\nor reuse of any copyrighted component of this work in other works.\nGCN\nCurrent MIG\nAgent\nInitial MIG\nClone MIG\nEpisode\nend\nMIG Generator\nLogits\nNetwork\nActions\nSample\nStep\nReward\nEpisode start\nEnvironment\nApply actions\nUpdate\nparams\nFig. 1. Schema of the DRL system for logic optimization.\nIn this work, we combine RL with GCNs based on MIG\nlogic graphs and focus on overcoming these limitations of\n[4]. Speciﬁcally, we contribute 1) a novel embedding method\nthat is independent of the graph size while demonstrating its\ncapability to achieve a high QoR; 2) a compact RL-based\nframework resulting in a GCN with only a few trainable\nparameters that enables inference on arbitrarily large graphs\nregardless of the graph size used during training; 3) a GCN\nthat learns an optimized set of atomic actions for each node,\napplying local modiﬁcations at many nodes within the graph\nwith each inference step; 4) an evaluation of our method that\nincludes 500-node random logic circuits.\nII. PROPOSED METHOD\nOur method, shown in Fig. 1, performs logic synthesis on\ncircuits composed by 3-input majority and inverter gates rep-\nresented in our model as MIGs, whose algebra allows deﬁning\na sound and complete move set [5]. At every time step, the\nsystem applies the instructions provided by an RL agent which\nlearns autonomously which transformations are best to apply\nto each circuit conﬁguration. The training process is organized\nby episodes with a ﬁxed number of steps. At episode start,\nan initial graph is generated and a sequence with sets of\ntransformations is applied until episode end. The RL agent\nlearns its policy by interacting with the environment. This is\nresponsible for consistently applying the agent’s chosen legal\ntransformations while blocking illegal transformation attempts.\nThe latter includes handling collisions of simultaneous actions\nfrom distinct nodes. To help the agent explore the space more\nefﬁciently, we provide additional inference rules which ease\napplicability, mitigating the reward sparsity problem. In this\nsetup, it is no longer needed to commute each node inputs\nin speciﬁc ways to allow action application. The environment\narXiv:2105.01755v1  [cs.LG]  4 May 2021\nTABLE I\nLOCAL MAJORITY-INVERTER GRAPH TRANSFORMATIONS\nΩ· C\nCommutativity M(x, y, z) = M(y, x, z) = M(z, y, x)\nΩ· A\nAssociativity\nM(x, u, M(y, u, z) = M(z, u, M(y, u, x))\nΩ· Ac Compl. Assoc. M(x, u, M(y, u′, z) = M(x, u, M(y, x, z))\nΩ· D Distributivity\nM(x, y, M(u, v, z) = M(M(x, y, z), M(x, y, v), z)\nΩ· I\nInverter Prop. M′(x, y, z) = M(x′, y′, z′)\nΛ · M Majority\nM(x, x, z) = x and M(x, x′, z) = z\nΛ · R\nRedundancy\n∀u, v ∈nodes: if in(u)=in(v), replace all u with v\ncomputes and sends to the agent the reward (size difference\nbetween initial and ﬁnal graphs) achieved in an episode. Thus,\nall actions taken at episodes with size reduction are positively\nreinforced.\nFor a given node ni of a MIG and a maximum adjacency\ndepth dadj, we deﬁne the neighborhood Ni of ni as the set of\nnodes nj which are reachable from ni traversing no more than\ndadj edges. We deﬁne a state space S where s ∈S represents\nany subgraph that can be observed by the GCN in Ni. We\ncall action any graph transformation that keeps intact the truth\ntable of the MIG (legal move). These actions are selected by\nthe agent (Ωactions), or are always applied after each step (Λ\nactions), cf. Table I. Additionally, the identity action Ω· I is\nalways applicable.\nAt every step of an episode, the agent observes the state\nof each node ni in the MIG by performing a GCN forward\npass. At each hidden layer, the network collects for each\nni features vectors (FVs) from adjacent nodes nk. The FVs\nof its 3 inputs are concatenated with 3 aggregated FVs for\nthe output. The nodes connected to the output are binned\nbased on its input port (0, 1, or 2) of the connection, then\nthe FVs in each of the bins are summed up to produce the\naggregated FVs. The resulting FVs are passed through linear\nlayers and non-linearities, while the process is repeated for\nevery layer. In this layout, the network explores for each ni\na broader Ni at each layer. Finally, the network outputs a\ndistribution with action probabilities for each node given its\nstate P(a | s), and the agent samples an action for each\nnode. In accordance, the environment applies the legal selected\nmoves and this process is repeated until the episode ends,\nwhen the reward is computed and used to update the model\nparameters. We train our model with Policy Gradient, as the\nsharp changes on estimation policy require the explicit state\nexploration from on-policy algorithms. They also break ties\non equivalent sequences by exploiting one of them.\nModel embeddings: A major obstacle of GCN-based mod-\nels is the need to embed each node with the necessary\ninformation. Common options are learned embeddings based\non the node type, which lacks relevant information, and\nnode IDs, which are non-scalable as they depend on the\ngraph size. We propose an alternative, scalable embedding\nmethod in which every node can just “see” whether a node\nin its neighborhood is “itself” or “not itself”. This implies\nthat every node neighborhood is initialized with its node-\nspeciﬁc embedding before a forward pass. In the following, we\nScalable embeddings\nNode-ID embeddings\nM\nM\nM\nM\nM\nM\nM\nM\nM\nM\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\n0\n1\n2\nFig. 2.\nRepresentation of the pattern seen by the model within a node\nneighborhood for (1) traditional Node-ID and (2) our proposed scalable\nembeddings. For (1), the model explores neighborhoods by comparing feature\nvectors through paths in all directions, while the latter unravels sequences\nof edges through closed loops of length up to dadj. Below, the unraveled\ngathered sequences of features are shown, which encode the inverters and\ngates traversed by the path.\ndemonstrate that this is sufﬁcient to learn the required features\nfor logic optimization. Fig. 2 depicts the feature collection\nprocess.\nThe proposed method nevertheless has some limitations. 1)\nThe multi-action schema makes it impractical to use actions\ninvolving an arbitrary number of nodes, such as substitution or\nrelevance. These have been proven to be effective in heuristic\noptimization approaches. However, the system can still get to\nany state without these actions in a few more steps [5]. 2)\nThe speciﬁc embeddings for each node have a memory and\ncompute time footprint depending on the graph’s connectivity.\nHowever, this is not a scalability issue as it does not depend\non graph size.\nIII. RESULTS\nWe train our model with 3-input and 4-input decompositions\nobtained through sum-of-products (SOP) and compare the\nmean size reduction obtained with the maximum achievable,\ncomputed with cirkit’s exact synthesis [6]. We also train with\ntwo sets of 1000 random graphs with 100 inputs, 2 outputs and\nsizes (50, 500). Finally, we train our model with C1355 and\nC880 benchmarks [7]. Size reductions achieved with Random\ngraphs and benchmarks are compared with the ones obtained\nwith ABC [3]. The following table shows the average size\nreduction achieved with each dataset and method, and their\nrelative improvement with respect to the baseline.\nDataset\nbaselinea\n—— DLLOA [4] ——\n—— ours ——\nMSRb\nMSRb\nrelMSRc\nMSRb\nrelMSRc\nSOP3\n8.65\n8.65\n100%\n7.38\n85%\nSOP4\n24.59\n20.41\n83%\n24.57\n100%\nRand 50\n25.54\n–\n–\n44.92\n175%\nRand 500\n271.24\n–\n–\n413.68\n152%\nC880[7]\n39\n–\n–\n17\n44%\nC1355[7]\n114\n98\n86%\n106\n93%\na SOP datasets: optimal synthesis (cirkit [6]); all others: ABC [3].\nb Mean Size Reduction: Average difference between initial and ﬁnal graphs.\nc Relative Mean Size Reduction: Average size reduction relative to baseline.\nThe model provides competitive results for the benchmarks\nand SOP datasets while outperforming ABC for randomly\ngenerated graphs. This difference in performance is inﬂuenced\nby the fact that ABC operates with AIGs, which resembles\nthe original structure of benchmarks and SOPs, while our LO\noperates with MIGs, used for generating random graphs.\nIV. CONCLUSION\nWe have proposed to combine GCNs with RL with a novel\nnode embedding to perform logic optimization on MIGs.\nWe show promising results on several benchmark graphs,\noutperforming ABC with a size reduction of up to 1.75× on\nlarger, random graphs.\nREFERENCES\n[1] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” in Proc. ICLR, 2017.\n[2] A. Hosny et al., “DRiLLS: Deep Reinforcement Learning for Logic\nSynthesis,” (ASP-DAC), 2020.\n[3] “ABC: System for Sequential Logic Synthesis and Formal Veriﬁcation.\nhttps://github.com/berkeley-abc/abc.”\n[4] W. Haaswijk et al., “Deep Learning for Logic Optimization Algorithms,”\nin Proc. IEEE ISCAS, 2018.\n[5] L. Amar´u et al., “Majority-Inverter Graph: A New Paradigm for Logic\nOptimization,” IEEE TCAD, vol. 35, no. 5, pp. 806–819, 2016.\n[6] “Cirkit: A circuit toolkit. https://github.com/msoeken/cirkit.”\n[7] “ISCAS85 benchs: http://www.pld.ttu.ee/∼maksim/benchmarks/iscas85.”\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-05-04",
  "updated": "2021-05-04"
}