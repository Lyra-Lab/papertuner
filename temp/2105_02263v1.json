{
  "id": "http://arxiv.org/abs/2105.02263v1",
  "title": "ADAM: A Sandbox for Implementing Language Learning",
  "authors": [
    "Ryan Gabbard",
    "Deniz Beser",
    "Jacob Lichtefeld",
    "Joe Cecil",
    "Mitch Marcus",
    "Sarah Payne",
    "Charles Yang",
    "Marjorie Freedman"
  ],
  "abstract": "We present ADAM, a software system for designing and running child language\nlearning experiments in Python. The system uses a virtual world to simulate a\ngrounded language acquisition process in which the language learner utilizes\ncognitively plausible learning algorithms to form perceptual and linguistic\nrepresentations of the observed world. The modular nature of ADAM makes it easy\nto design and test different language learning curricula as well as learning\nalgorithms. In this report, we describe the architecture of the ADAM system in\ndetail, and illustrate its components with examples. We provide our code.",
  "text": "ADAM: A Sandbox for Implementing Language Learning\nRyan Gabbard*1, Deniz Beser†1, Jacob Lichtefeld‡1, Joe Cecil§1, Mitch Marcus2, Sarah Payne2,3, Charles Yang2,3\nand Marjorie Freedman1\n1Information Sciences Institute, University of Southern California\n2Department of Computer and Information Science, University of Pennsylvania\n3Department of Linguistics, University of Pennsylvania\nAbstract\nWe present ADAM, a software system for designing and run-\nning child language learning experiments in Python. The sys-\ntem uses a virtual world to simulate a grounded language ac-\nquisition process in which the language learner utilizes cog-\nnitively plausible learning algorithms to form perceptual and\nlinguistic representations of the observed world. The modu-\nlar nature of ADAM makes it easy to design and test different\nlanguage learning curricula as well as learning algorithms. In\nthis report, we describe the architecture of the ADAM system\nin detail, and illustrate its components with examples. We pro-\nvide our code.1\nKeywords: Language Acquisition, Cognitive Modeling\n1\nIntroduction\nADAM is a software “sandbox” for experiments in child lan-\nguage learning. It enables an experimenter with a modest\ndegree of Python programming experience to quickly and\ncompactly specify a curriculum consisting of a sequence\nof situations in a virtual world. It then projects these situa-\ntions into customizable linguistic representations and non-\nlinguistic perceptual representations which can be con-\nsumed by learning algorithms. It also provides a framework\nfor evaluating learner performance both during and after the\nlearning phase. This approach provides several advantages:\n• Learning from language together with concrete situations\ncaptures the fact that humans learn language not from lan-\nguage alone but rather from language grounded in particu-\nlar situations.\n• Using a virtual world for our grounding makes it easy for a\nresearcher to design custom curricula for their experiments\n(see section 3.2).\n• Using a virtual world for grounding provides the learner\nwith a realistic model of a pre-linguistic infant’s perception\nof the world (see section 3.4) as grounding, rather than, for\nexample, raw pixels.\n*ryan.gabbard@gmail.com. All work done while at ISI, prior to\njoining Amazon.\n†beser@isi.edu\n‡jacobl@isi.edu\n§cecil@isi.edu\n1Our code is available at https://github.com/isi-vista/adam\n• Generating language from situations enables performing\nexperiments across multiple languages by only swapping\nout one system module (the language generator; see sec-\ntion 3.3).\n• Generating the curriculum and perceptions in Python code\nmakes it easy to perform variations on experiments, includ-\ning adding irrelevant situations, adding noise to the percep-\ntual representations, and altering word distributions.\nIn the main body of this report, we ﬁrst review relevant\nbackground in the language acquisition literature. Then, we\ndescribe the ADAM system architecture at a high level and\nthen provide some examples of what it learns from a simple\nsample curriculum. We then describe related work and direc-\ntions for future work. Finally, a series of appendices provide\ndetailed descriptions of system components.\n2\nBackground\nThe current literature consists of several proposals for chil-\ndren’s word learning process which fall into two broad\ngroups. ‘Global’ approaches (e.g, Fazly, Alishahi, & Steven-\nson, 2010; Siskind, 1996; Yu & Smith, 2007a; Goodman,\nDale, & Li, 2008; Smith, Smith, & Blythe, 2011) resolve un-\ncertainty by aggregating situational data over time to iden-\ntify the best supported word-referent associations. By con-\ntrast, ‘local’ approaches (e.g, Medina, Snedeker, Trueswell,\n& Gleitman, 2011; Spiegel & Halberda, 2011; Trueswell,\nMedina, Hafri, & Gleitman, 2013; Aravind et al., 2018) at-\ntempt to resolve uncertainty immediately: in the simplest\nform, the learner hypothesizes a word meaning, maintains it\nwhen the word is heard if it is conﬁrmed (e.g. is present in\nthe observed situation) but replaces it with a new meaning if\ndisconﬁrmed. Additionally, there are models that combine\naspects of both approaches (Stevens, Gleitman, Trueswell,\n& Yang, 2017), where only actively hypothesized meanings\nare maintained (as in the local approach), whose associa-\ntions with the word change probabilistically in response to\nthe cumulative effect of learning instances (as in the global\napproach).\nMost previous computational research has focused on the\nmodeling of behavioral results from word learning experi-\nments or the quantitative assessment of the models’ effec-\n1\narXiv:2105.02263v1  [cs.CL]  5 May 2021\nFigure 1: A High-level architecture diagram, which happens in a loop.\ntiveness on (small) annotated child-directed input corpora.\nLarger-scale simulation studies (e.g, Fazly et al., 2010; Vogt,\n2012; Blythe, Smith, & Smith, 2016), by contrast, are eval-\nuated on arbitrary word-reference mappings that bear little\nresemblance to the linguistic, cognitive, and perceptual infor-\nmation available to the child learner.\nMoreover, the words used in previous studies are most\noften basic-level terms (dog), sometimes reﬂecting different\nlevels of carefully constructed conceptual taxonomy (‘husky’,\n‘dog’, ‘animal’) (e.g, Xu & Tenenbaum, 2007; Spencer, Per-\none, Smith, & Samuelson, 2011). The meanings of the words\nare almost always atomic. In the real world, however, word\nmeanings can only be learned by making subtle and incre-\nmental reﬁnements. For instance, the word chair is used for\nreferents with highly variable attributes (size, color, number\nof legs, etc.).\nADAM aims to provide a computational platform that en-\nables systematic studies of how the structure of the learning\ndata, the perceptual complexity of the learning environment,\nand the cognitive resources available to the learner collec-\ntively impact the effectiveness of the language acquisition\nmodels. In the following sections, we describe the ADAM\nsystem in detail.\n3\nHigh-level System Architecture\nAt the highest level, the ADAM system is a loop where a lan-\nguage learner repeatedly perceives situations (states of the\nworld or transitions between such states) paired with linguis-\ntic utterances (Figure 1). After each observation, the learner\nupdates its internal state. The sequence of situations observed\nby the learner is called a curriculum. At any time, the learner\ncan perceive a new situation and be asked to generate a rele-\nvant and appropriate utterance.\nFigure 2 displays the full architecture, which has three no-\ntable additions. First, the learner has no direct access to the\nsituations in the curriculum. Instead, it receives only a pairing\nof a linguistic representation (section 3.3) and a perceptual\nrepresentation (section 3.4) which are both generated from a\nsituation representation (section 3.1) by the language gen-\nerator (section 3.3) and perception generator (section 3.4),\nrespectively. In the current implementation, these representa-\ntions are meant to model an 18-month-old’s perception of the\nworld around them. Second, we provide situation templates,\nwhich are a compact way for an experimenter to specify a\nlarge curriculum (section 3.2). Third, an evaluator allows for\ntracking learning progress and evaluating experiments (sec-\ntion 3.6).\nBelow we describe the components of the system in more\ndetail.\n3.1\nSituations and Curricula\nA situation is an abstract description of either a state or action\nin the world. A curriculum is a sequence of situations. Sit-\nuations are never accessed directly by the learner but rather\nonly through the mediation of a derived perception (section\n3.4).\nA situation consists of a set of objects which may have\nproperties, a set of relations between these objects, and a set\nof actions which occur. Object locations are not encoded via\ncoordinates but only in terms of regions which are deﬁned\nwith respect to other objects; the way these regions are de-\nﬁned is largely drawn from Landau and Jackendoff (1993).\nObjects, object properties, and relations are drawn from a\nuser-deﬁned ontology; this ontology is for the convenience of\nthe curriculum designer only and is not accessible to the lan-\nguage learner. Certain aspects of a situation may be marked\nas salient, meaning they will be remarked upon in linguistic\nutterances derived from this situation.\nMore detail on the representation of situations (appendix\nB) and ontologies (appendix A) may be found in the appen-\ndices.\n3.2\nSituation Templates\nDeﬁning every situation in a realistic curriculum con-\nsisting\nof\nthousands\nor\nmillions\nof\ninstances\nwould\nbe\nimpractical.\nInstead,\nwe\nallow\nresearchers\nto\nuse a simple Python-based domain-speciﬁc language to\nspecify situation templates (Figure 3).\nFor exam-\nple,\na\nsituation\ntemplate\nlike\nx[+ANIMATE, +HUMAN]\n∧y[+ INANIMATE] ∧MANIPULATION_ACTION(x, y) ∧\nRELATIVE_SIZE(SMALLER, y, x) could be instantiated by\nPICK_UP(PERSON, CUP) or ROLL(DOG, BALL).\nSituation templates have a similar structure to situations,\nbut where situations contain objects, properties, relations, and\nactions situation templates can instead contain template vari-\nables which range over elements of the user-deﬁned hierar-\nchical ontology. Restrictions can be placed on how these vari-\nables may be ﬁlled. A situation generator then reads these\ntemplates and instantiates the templates using either exhaus-\ntive enumeration or conﬁgurable sampling strategies.\nBecause situation templates are deﬁned using Python code,\nthe creation of templates can itself be automated. This can be\nuseful, for example, to easily test all possible combinations\nof multiple experimental parameters.\nFigure 2: A Mid-level architecture diagram, with additional detail added to the processing loop.\nFigure 3: A simple situation template which will generate many concrete situations where an object is sitting on some ﬂat\nsurface.\n3.3\nLinguistic Representation\nAny sequence of discrete symbols can in principle work as a\nlinguistic representation for ADAM (phonemes, morphemes,\netc.). In the default implementation, we use English ortho-\ngraphic words2 and Yale romanization for Chinese.\nLinguistic representations are not directly speciﬁed as part\nof a curriculum but instead are generated from situation rep-\nresentations by a language generator. This results in a key\nstrength of ADAM: experiments can be performed in multi-\nple languages from the same curriculum simply by swapping\nthe language generator component. The initial language gen-\nerator implementations for English and Chinese are described\nin Appendix E.\n3.4\nPerceptual Representation\nThe language learner is not permitted to observe situations\ndirectly. Instead, a perception generator module ﬁrst trans-\nlates them to perceptual representations. The default imple-\nmentation uses a perceptual representation designed to mimic\nthose of a roughly 18-month-old child.\nThis default representation includes object shape and\nstructure representations derived from (Marr, 1982) and\n(Biederman, 1987); region and path information derived from\n(Landau & Jackendoff, 1993); a small set of object prop-\nerties, such as liquid and self-moving; the ability of\n2The plural marker is split off as its own token; learning mor-\nphonology could be done in an extended version of the default\nADAM implementation\nthe language learner to recognize particular familiar indi-\nviduals; information on what objects are gazed at by the\nspeaker (Conboy, Rivera-Gaxiola, Silva-Pereyra, & Kuhl,\n2008); very coarse grained relative size and possession re-\nlationships; which objects are the speaker and addressee;\nand proto-roles such as stationary, causes-change, and\nvolitionally-involved derived from (Dowty, 1991).\nThe perception generator component could be swapped for\na different implementation to run experiments with a different\nperceptual representation. It is also possible to parameterize\nthe current implementation to allow for the addition, exclu-\nsion, or unreliable perception of aspects of the representation.\n3.5\nLanguage Learner\nIn ADAM, a learner is any object which can\n• observe pairs of perceptual and linguistic representation\nand update its internal state in response.\n• observe a perceptual representation and produce a linguis-\ntic representation in response.\nWe intend ADAM as a “sandbox” where many different\nlearning algorithms can be implemented. By default, the sys-\ntem includes four learning algorithms: subset learning, Pur-\nsuit (Stevens et al., 2017), Cross-Situational Learning (Yu\n& Smith, 2007b), and Propose but Verify (Trueswell et al.,\n2013). These are described in detail in Appendix H.2, along\nwith a collection of representations and data structures for\nOntology Type\nOntology Subtypes\nthing\ninanimate-object, head, hand, person, animal,\nbody-part\nperson\nme, you, Mom, Dad, baby\nanimal\ndog, cat, bear, bird, chicken, cow\nrelation\nin-region, spatial-relation, partOf, size-relation,\naxis-relation, has\naction\nwalk, run, consume, put, push, shove, go, come, take,\ngrab, give, spin, sit, fall, throw, pass, move, jump,\nroll, ﬂy\nproperty\ncan-ﬁll-template-slot, substance, is-addressee,\nis-speaker, is-human, perceivable-property,\ncan-manipulate-objects, edible, rollable,\ncan-have-things-on-them, is-body-part,\nperson-can-have, transfer-of-possession,\ncan-jump, can-ﬂy, has-space-under,\ncan-be-sat-on, fast, slow, side, left, right,\nhard-force, soft-force, aboutSameSizeAsLearner\ncolor\nred, blue, green, black, white, light-brown,\ndark-brown, transparent\ninanimate-object\nsubstance, ground, food, table, bed, ball,\npaper, book, house, car, cup, box, chair,\ntruck, door, hat, (furniture) leg, chairback,\nchairseat, tabletop, mattress, headboard, wall,\nroof, tire, truckcab, trailer, ﬂatbed\nTable 1: The sample ontology included with ADAM. These ontology nodes describe scenes, which are used to generate\nplausible perception graphs with geons and other perceptual features, and then provided as input to the learner\nlearning that can help researchers implement their own algo-\nrithms more easily. While the default learning algorithms are\nrelatively simple and heuristic, arbitrary learning algorithms\n(e.g. deep neural networks, etc.) could be substituted.\n3.6\nEvaluation\nThe evaluation module allows the user to monitor learning\nprogress (during training) and the effectiveness of the learned\nrepresentations (at runtime). Details on this module are pro-\nvided in Appendix I.\n4\nEvaluation on a Sample Curriculum\nThe ADAM system includes a simple sample curriculum. Be-\nlow, we describe the content of this curriculum and provide\nexamples of what ADAM learns from it.\n4.1\nSample Curriculum Content\nThe ontology of the sample curriculum is shown in Table 1.\nIt consists of the following situations with:\n• single objects\n• single objects sitting on the ground\n• objects described by color\n• objects possessed by the speaker\n• objects possessed by a non-speaker\n• objects resting upon other objects\n• objects placed horizontally beside other objects\n• objects positioned vertically relative to other objects\n• objects inside other objects\n• objects positioned in front of or behind other objects\n• examples of all the verbs in the ontology\n• examples of objects deﬁned by their function (e.g. chair)\n• multiple instances of the same type of object (plurals)\n• objects with part-whole relationships\nSome samples of what ADAM learns from the curriculum\nare given in ﬁgures 4 through 10. The representation used in\nthese ﬁgures is described in Appendix G.1.\n5\nConclusions and Future Work\nIn this report, we presented ADAM, a software system based\nin Python that can be used to simulate language acquisition\nexperiments. Various system capabilities outlined in this re-\nport, including the modular abilities to craft learning curricula\nas desired (section 3.1) and to build different language learn-\ning algorithms (section 3.5) make ADAM suitable for system-\natic studies of language and how language acquisition models\nare impacted by the learning data, the perceptual complex-\nity of the learning environment, and the cognitive resources\navailable to the learner.\nThere are many future directions to improve the ADAM\nsystem. On the language processing side, the linguistic real-\nization learned for objects, properties, relations, and actions\ncould be enriched to have syntactic structure, rather than sur-\nface templates (Appendix G.3). Also, the concept to language\ngeneration can be improved to take a situation and generate\nmultiple potential language descriptions of the scene. Lan-\nguage processing can be done in real time using audio inputs\nfor generating Speech-To-Text sequences. Similarly, percep-\ntual processing can be further improved by processing real-\nworld inputs from 2D or 3D visuals. This would be an un-\ndertaking to replace the perception generation module (Ap-\npendix F) with a different processing system.\nExperiments with complex scenes is sometimes inconve-\nnient due to ADAM’s core graph matching algorithms being\nslow on some perception graphs. To scale up experimenta-\ntion, we need to shift the graph matching code from pure\nPython to a more efﬁcient language and to explore newer\ngraph matching algorithms than VF2.\n6\nAcknowledgements\nApproved for public release; distribution is unlimited. This\nmaterial is based upon work supported by the Defense Ad-\nvanced Research Projects Agency (DARPA) under Agree-\nment No. HR00111990060. The views and conclusions con-\ntained herein are those of the authors and should not be in-\nterpreted as necessarily representing the ofﬁcial policies or\nendorsements, either expressed or implied, of DARPA or the\nU.S. Government.\nReferences\nAravind, A., de Villiers, J., Pace, A., Valentine, H., Golinkoff,\nR., Hirsch-Pasek, K., ... Wilson, M. S. (2018). Fast map-\nping word meanings across trials: Young children forget all\nbut their ﬁrst guess. Cognition, 177, 177–188.\nBiederman, I. (1987). Recognition-by-components: a the-\nory of human image understanding. Psychological review,\n94(2), 115.\nBlythe, R. A., Smith, A. D., & Smith, K.\n(2016).\nWord\nlearning under inﬁnite uncertainty. Cognition, 151, 18–27.\nConboy, B. T., Rivera-Gaxiola, M., Silva-Pereyra, J., & Kuhl,\nP. K. (2008). Event-related potential studies of early lan-\nguage processing at the phoneme, word, and sentence lev-\nels.\nCordella, L. P., Foggia, P., Sansone, C., & Vento, M. (2001).\nAn improved algorithm for matching large graphs. In 3rd\niapr-tc15 workshop on graph-based representations in pat-\ntern recognition (pp. 149–159).\nCordella, L. P., Foggia, P., Sansone, C., & Vento, M. (2004).\nA (sub) graph isomorphism algorithm for matching large\ngraphs. IEEE transactions on pattern analysis and machine\nintelligence, 26(10), 1367–1372.\nDowty, D. (1991). Thematic proto-roles and argument selec-\ntion. language, 67(3), 547–619.\nFazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilis-\ntic computational model of cross-situational word learning.\nCognitive Science, 34(6), 1017–1063.\nGoodman, J. C., Dale, P. S., & Li, P. (2008). Does frequency\ncount? parental input and the acquisition of vocabulary.\nJournal of child language, 35(3), 515–531.\nHagberg, A. A., Schult, D. A., & Swart, P. J. (2008). Explor-\ning network structure, dynamics, and function using net-\nworkx. In G. Varoquaux, T. Vaught, & J. Millman (Eds.),\nProceedings of the 7th python in science conference (p. 11\n- 15). Pasadena, CA USA.\nLandau, B., & Jackendoff, R. (1993). \" what\" and\" where\"\nin spatial language and spatial cognition. Behavioral and\nbrain sciences, 16, 217–217.\nMarr, D. (1982). Vision: A computational investigation into\nthe human representation and processing of visual informa-\ntion.\nMedina, T. N., Snedeker, J., Trueswell, J. C., & Gleitman,\nL. R. (2011). How words can and cannot be learned by\nobservation. Proceedings of the National Academy of Sci-\nences, 108(22), 9014–9019.\nSiskind, J. M.\n(1996).\nA computational study of cross-\nsituational techniques for learning word-to-meaning map-\npings. Cognition, 61(1), 39–91.\nSmith, K., Smith, A. D., & Blythe, R. A. (2011). Cross-\nsituational learning:\nAn experimental study of word-\nlearning mechanisms. Cognitive Science, 35(3), 480–498.\nSpelke, E. S., & Kinzler, K. D. (2007). Core knowledge.\nDevelopmental science, 10(1), 89–96.\nSpencer, J. P., Perone, S., Smith, L. B., & Samuelson, L. K.\n(2011). Learning words in space and time: Probing the\nmechanisms behind the suspicious-coincidence effect. Psy-\nchological science, 22(8), 1049–1057.\nSpiegel, C., & Halberda, J. (2011). Rapid fast-mapping abil-\nities in 2-year-olds. Journal of experimental child psychol-\nogy, 109(1), 132–140.\nStevens, J. S., Gleitman, L. R., Trueswell, J. C., & Yang, C.\n(2017). The pursuit of word meanings. Cognitive science,\n41, 638–676.\nTrueswell, J. C., Medina, T. N., Hafri, A., & Gleitman, L. R.\n(2013).\nPropose but verify: Fast mapping meets cross-\nsituational word learning.\nCognitive psychology, 66(1),\n126–156.\nVogt, P. (2012). Exploring the robustness of cross-situational\nlearning under zipﬁan distributions.\nCognitive Science,\n36(4), 726–739.\nWebster, M., & Marcus, M. (1989). Automatic acquisition of\nthe lexical semantics of verbs from sentence frames ‘. In\n27th annual meeting of the association for computational\nlinguistics (pp. 177–184).\nXu, F., & Tenenbaum, J. B.\n(2007).\nWord learning as\nBayesian inference. Psychological Review, 114(2), 245.\nYu, C., & Smith, L. B. (2007a). Rapid word learning under\nuncertainty via cross-situational statistics. Psychological\nScience, 18(5), 414–420.\nYu, C., & Smith, L. B. (2007b). Rapid word learning under\nuncertainty via cross-situational statistics. Psychological\nscience, 18(5), 414–420.\nFigure 4: ADAM learns that my indicates possession by the speaker.\nFigure 5: ADAM learns that for one object to be on another, it needs to not only be above it, but also in contact with it.\nFigure 6: ADAM learns that you can only roll things with circular cross-sections (see the cross-section property on slot2)\nFigure 7: ADAM learns that goes in involves the movement of an object into the interior of a hollow object.\nFigure 8: ADAM learns that if someone is eating something, the eater is animate, while the eatee is inanimate and smaller than\nthe eater, and the eatee becomes interior to the eater\nFigure 9: ADAM learns that the thrower loses possession of the object, which traverses a path above the ground towards\nsomeone else. It does not learn that the catcher necessarily gains possession of the ball (e.g. The quarterback threw the ball to\nthe open man, but the receiver dropped it)\nFigure 10: ADAM learns you can only drink liquids.\nAppendices\nA\nOntologies\nAn ontology is a collection of object, attribute, relation, and\naction types used for two purposes:\n• to deﬁne the objects, attributes, relations, and actions in\na situation and to provide the necessary information for\ntransforming them into the perceptual representation\n• to deﬁne how variables in situation templates may be in-\nstantiated. For example, the arguments of a certain action\nin a template may be restricted to animate objects.\nAn ontology is structured as a collection of ontology nodes\nwith parent-child relationships. Every node may be associ-\nated with a set of ontology node properties which are inher-\nited by all its child nodes. There are six special nodes which\nmust be present in any ontology.\nTHING, PROPERTY, RE-\nLATION, and ACTION are self-explanatory. PERCEIVABLE is\na meta-property used to mark which other properties can be\nperceived by the learner.\nCAN_FILL_TEMPLATE_SLOT in-\ndicates those nodes which can potentially be arguments of\nrelations and actions.\nAn ontology node derived from OBJECT may be associated\nwith an object structural scheme which provides a hierar-\nchical representation of the internal structure of a type object\nand guides how that object is perceived as a collection of sub-\nobjects and geons. A structural schema represents the general\npattern of the structure of an object, rather than the structure\nof any particular object. For example a person’s body is made\nup of a head, torso, left arm, right arm, left leg, and right leg.\nThese sub-objects have various relations to one another (e.g.\nthe head is above and supported by the torso).\nAny ontology node derived from ACTION may be associ-\nated with an action description which guides how that action\nis perceived in terms of changes to objects and their relations.\nAn action description consists of:\n• A mapping of semantic roles to action description vari-\nables. All the relations and conditions speciﬁed below will\nbe in terms of these variables, which will be bound to con-\ncrete objects when an action is instantiated in a particular\nsituation.\n• a collection of pre-conditions, which are relations which\nhold only in the frame before the action happens\n• a collection of post-conditions, which are relations which\nhold only in the frame after the action happens\n• a collection of enduring conditions, which are relations\nwhich hold before and after the action happens.\n• a description of what happens during the action, consisting\nof\n– a mapping from action description variables to paths tra-\nversed by those objects during the action\n– a set of relations which hold continuously during the ac-\ntion\n– a set of relations which hold at some point, but not nec-\nessarily continuously, during the action.\n• properties required on objects involved in the action (e.g.\nthe AGENT must be ANIMATE\n• auxiliary variables, which do not occupy semantic roles\nbut are still referred to by conditions, paths, etc. An ex-\nample would be the container for the LIQUID which is an\nargument of a DRINK action.\nB\nSituation Representation\nA situation consists of\n• a set of objects, some subset of which may be marked as\nsalient. These are the ones which will be foregrounded in\nany linguistic descriptions of the situation. For example,\nthe ground is always present but rarely salient. Every ob-\nject in a situation possesses a type from the ontology and\nthree axes; these axes can be used to construct regions to\ndescribe the spatial relationships of objects to one another\nin an abstract way.\n• a set of relations between objects in the situation which\nhold before and/or after any actions occur. It is not neces-\nsary to state every relationship which holds in a situation.\nRather this should contain the salient relationships which\nshould be expressed in the linguistic description. It is also\nnot necessary to state relations which are implied by ac-\ntions occurring in the situation.\n• which objects in the situation are gazed at by the speaker.\n• a set of actions which occur in the situation. An action\nis an action description from the ontology together with a\nmapping from the semantic roles and auxiliary variables of\nthe action to the objects in the situation.\n• syntax hints, which can constrain how the language gener-\nator expresses the situation. A curriculum designer can, for\nexample, use these to force actions in a situations to always\nbe described in the passive voice, if possible.\nC\nRegions\nRegions (which we derive almost entirely from (Landau &\nJackendoff, 1993)’s account of the semantics of prepositions)\nare used to describe the locations of objects relative to one\nanother in an abstract way. They may be used in action de-\nscriptions, situations, perceptions, and situation templates.\nA region consists of:\n• a reference object\n• a\ndistance,\none\nof\nINTERIOR,\nEXTE-\nRIOR_BUT_IN_CONTACT, PROXIMAL, and DISTAL.\n• a direction, which is an object axis together with a positive\nor negative polarity.\nRegions can also be used to construct paths which describe\nhow objects move. A path (whose representation we again\nderived almost entirely from (Landau & Jackendoff, 1993)):\n• source and destination reference object or regions\n• an optional path operator, which must be one of VIA, TO,\nTOWARD, FROM, or AWAY_FROM\n• an optional reference axis\n• whether the orientation of an object changes as it moves\nalong the path\nD\nSituation Templates\nA situation template has the same structure as a template,\nexcept all objects, properties, relations, and actions may be\nreplaced by variables. The instantiations of these variable is\ncontrolled by constraints, the most common being restrictions\non which types from the ontology can be used.\nSituation templates may be instantiated into concrete situa-\ntions using either a sampling or exhaustive enumeration strat-\negy.\nE\nInitial Language Generator\nImplementations\nADAM by default provides language generators for English\nand Chinese. These implementations both work by translat-\ning the portions of situations marked as salient into depen-\ndency trees. The learner has no access to these dependency\ntrees but only the resulting token sequence.\nF\nDefault Perceptual Representation\nThe world is represented to the learner as either one or two\nperception frames (one for static situations; two for those\ncontaining actions, encoding the state of the world before and\nafter the action). A perception frame consists of a collection\nof perception objects which have properties and have rela-\ntions with one another. Perception objects\n• may have sub-objects with which they have a SUB-PART\nrelation.\n• have three axes, one of which is marked as PRIMARY (typ-\nically the longest). These axes may have relations between\neach other (for example, to indicate that an object is longer\nalong one axis than another). Each axis has boolean ﬂags\nto indicate whether it is\n– curved\n– directed\n– aligned with gravity\n• a geon (with a few exceptions, such as objects which ap-\npear two-dimensional). A geon has\n– three axes. Axes can have coarse-grained size relation-\nships with each other (e.g. for a pencil, its extent along\none axis is much greater than the other two, which are\nroughly the same).\n– a generating axis, which usually corresponds to its\nlongest axis.\n– a cross-section data structure which speciﬁes for the\ncross-section of the object along its primary axis\n* whether it is curved\n* whether it has reﬂective symmetry\n* whether it has rotational symmetry\n* its size, which is one of\n· CONSTANT if the cross-section is of roughly constant\nsize along the generating axis\n· SMALL-TO-LARGE if the cross-section gets bigger\nalong the positive direction of the generating axis\n· LARGE-TO-SMALL if the cross-section gets smaller\nalong the positive direction of the generating axis\n· SMALL-TO-LARGE-TO-SMALL if the cross section\nstarts small, gets large in the middle, and then shrinks\n(along the generating axis).\n• may have properties. The following properties can be per-\nceived:\n– SELF-MOVING\n– ANIMATE\n– INANIMATE\n– TWO-DIMENSIONAL\n– LIQUID\n– HOLLOW\n– RECOGNIZED-PARTICULAR\n(IS-DAD,\nIS-MOM,\nIS-\nBABY, IS-LEARNER, IS-GROUND)\n– GAZED-AT\n– Proto-roles (Dowty, 1991)\n* VOLITIONALLY-INVOLVED\n* SENTIENT-OR-PERCEIVES\n* CAUSES-CHANGE\n* MOVES\n* UNDERGOES-CHANGE\n* INCREMENTAL-THEME (roughly speaking, this is the\nobject involved in an action whose part-whole state cor-\nresponds to the progress of the action itself, e.g. sand-\nwich in eat a sandwich)\n* STATIONARY\n* CAUSALLY-AFFECTED\n– colors, expressed as an red-green-blue triples\n• may have a path (see A).\nThe following relations are perceivable\n• PART-OF\n• BIGGER-THAN, SMALLER-THAN, MUCH-BIGGER-THAN,\nMUCH-SMALLER-THAN\n• POSSESSION\n• IN-REGION (which takes a region as an argument)\nTwo-frame perceptions of actions can also contain descrip-\ntions of what happens during the action itself, consisting of:\n• a mapping from objects to paths (C) traversed by those ob-\njects during the action\n• a set of relations which hold continuously during the action\n• a set of relations which hold at some point, but not neces-\nsarily continuously, during the action.\nPerceptions are generated from situations by combining the\nsituation representations with information from the ontology\nsuch as ontology node properties, object structural schemata,\nand action descriptions.\nG\nLearning Representations\nADAM provides representations and algorithms used by all\nthe sample learning algorithms implemented by ADAM (Ap-\npendix H). Although user-implemented algorithms are not re-\nquired to use them, they do make the process of writing a\nlearning algorithm much simpler.\nG.1\nPerception Graphs\nFor our learning algorithms, it is convenient to convert our\nlearner’s perceptual representation into a directed acyclic\ngraph (a perception graph; Figure 11). A node in a per-\nception graph can represent:\n• an object perception (Appendix F)\n• geons (Appendix F)\n• an object property (Appendix F)\n• a spatial region (Appendix C)\n• an axis (Appendix C)\n• a path (Appendix C)\nNote that some nodes, such as object perceptions and axes,\nhave labels on them for the convenience of developers when\ndebugging (box_0, axis:side-to-side-0). These are not\naccessible to the learner.\nEdges between nodes represent relations from the inven-\ntory in Section 3.4 (e.g.\nPARTOF).\nGeons and paths are\nrepresented as multi-node sub-graphs, with each aspect of\ntheir structure described in Appendix F getting its own node,\njoined by special edge types. In dynamic situations, every\nedge in a perception graph is labelled with a set of temporal\nscopes (either BEFORE, AFTER, or both) indicating at which\nframe(s) during the action the relationship encoded by the\nedges holds true.\nPerception Graph Patterns\nADAM’s default learner rep-\nresents the “meaning” of an object, relation, or action by\nperception graph patterns which can be matched against\nthe perception graphs for new situations.\nLike perception\ngraphs, these are directed acyclic graphs, but their nodes and\nedge are boolean functions (predicates) which accept or re-\nject matches against nodes and edges of perception graphs,\nrespectively (Figure 12).\nThe predicates can be complex; for example, we can re-\nquire an edge have both a certain type and a certain temporal\nscope. Patterns can also contain special slot variable nodes;\nthese are ‘wildcards’ corresponding to the arguments of the\nproperty, relation, or action the pattern represents.\nG.2\nPerception Graph Algorithms\nPerception Graph Pattern Matching\nMatching a percep-\ntual graph pattern against a perception graph becomes an\ninstance of a restricted form of the sub-graph isomorphism\nproblem.\nAlthough this problem is NP-complete in gen-\neral, in practice it runs efﬁciently over the sort of graphs\nencountered in ADAM. ADAM uses the implementation of\nthe VF2 sub-graph isomorphism algorithm (Cordella, Fog-\ngia, Sansone, & Vento, 2001, 2004) from the networkx li-\nbrary (Hagberg, Schult, & Swart, 2008) with a few of custom\noptimizations.\nADAM also supports matching perception graphs patterns\nagainst one another (e.g. to determine if one pattern is a gen-\neralization rather than another pattern). This can also be ac-\ncomplished by a slight variation on the same algorithm by\nimposing a partial order on the node and edge predicates ac-\ncording to the set of nodes and edges they can possibly match.\nPerception Graph Pattern Generalization\nADAM learn-\ning algorithms encounter the need to generalize patterns in\none of two ways:\n• Given a pattern and perception graph the pattern fails to\nmatch, determine if there is some generalization of the pat-\ntern which would successfully match the graph.\n• Given two patterns, compute their intersection (the largest\nsub-pattern which is implied by both input patterns).\nBoth of these can be approximated using this greedy match\ngeneralization algorithm:\n• Attempt a regular match. If successful, stop.\n• If the match failed, use information gathered during the\nmatch process to determine which pattern node blocked the\nexpansion of the largest alignment found between pattern\nnodes and graph nodes. ‘Relax’ this node either by deleting\nit or (in a few cases) performing a node-speciﬁc relaxation\n(e.g. if the node was a color predicate, we might expand\nthe range of colors it is willing to match).\n• When certain nodes are deleted, the deletion is propagated\nto other graph nodes:\nFigure 11: A simple perception graph for the learner’s perception of a box sitting on the ground.\nFigure 12: An example of a perception graph pattern which would match any perception graph that has an object perception\nnode with properties HOLLOW and INANIMATE, such as that of a box or a cup.\n– if an object is deleted, so are its sub-objects\n– if during relaxation a portion of the graph becomes\n(weakly-)disconnected and contains no slot variables,\nremove it.\nG.3\nSurface Templates and Perception Graph\nTemplates\nA surface template is a sequence of tokens or argument slots,\nsuch as ARG1 gives ARG2 to ARG3 or green ARG1. The ar-\ngument slots are referred to as surface template variables.\nA surface template can be instantiated to produce a token\nsequence by providing a binding of surface template vari-\nables to tokens. Alternate representations with richer syn-\ntactic structure can easily be introduced.\nA perception graph template is a perception graph pattern\ntogether with an alignment of its slot variable nodes to sur-\nface template variables. the linguistic realization of learned\nobjects, properties, relations, and actions (including objects,\nwhich are degenerate argumentless templates).\nConcepts\nADAM’s default implementation uses an ex-\ntremely simpliﬁed semantic representation consisting of a set\nof concepts, which are of four types:\n• object concepts, which have no argument slots\n• attribute concepts, which take one other concept as an ar-\ngument\n• relation concepts, which take two other concepts as (num-\nbered) arguments\n• action concepts, which take a variable number of other con-\ncepts as (numbered) arguments\nAlignment Data Structures\nA language-concept align-\nment (LCA) is a mapping between concepts and their corre-\nsponding token sequences. A perception-concept alignment\n(PCA) is a mapping between concepts and sub-graphs in a\nperception graph.\nA language-perception-concept align-\nment (LPCA) is pair of an LCA and a PCA over the same\nset of concepts.\nDuring training, the sample integrated learner implementa-\ntion builds up an LPCA. At inference time, it builds up a PCA\nand then translates it into language.\nH\nSample Learners\nH.1\nTop-level Integrated learner\nIn the sample implementation, learning is coordinated at\nthe top-level by an integrated learner which contains sub-\nlearners for objects, attributes, relations, and actions, each of\nwhich can be customized to use different learning algorithms.\nThe integrated learner ﬁrst initializes an empty LPCA (G.3)\nto represent its understanding of the relationship between the\nlinguistic utterance and the perception graph. It then applies\neach sub-learner in the order objects, then attributes, then re-\nlations, then actions. Each sub-learner observes the linguistic\nutterances, the perception graph, and the LPCA. After updat-\ning its internal state, each learner then has the opportunity to\nupdate the LPCA before it is passed down the pipeline. For\nexample, if the object learner is conﬁdent it has learned the\nconcept DOG and it recognizes one in the perception graph,\nit can align DOG to the token dog in the utterance and to the\ncorresponding portion of the perception graph.\nAt inference time, the same process is applied, except that\na PCA is used instead of an LPCA. After all sub-learners\nhave ﬁnished, the PCA is discarded and the set of concepts is\ntranslated to language using the algorithm below, which will\nbe illustrated using the following concepts: { DAD, BALL,\nTHROWS(DAD, BALL), RED(BALL), SHINY (BALL ) }\n1. each object concept is translated to a set of token sequences\nas follows:\n• ﬁrst, get the (argument-less) token sequence learned for\nthe object concept by the object sub-learner (e.g.BALL\nto ball). If there is none, the object concept is not trans-\nlated. In the initial implementation, ADAM does not try\nto learn English determiners and they are added by spe-\ncial logic.\n• second, get the set of surface templates associated with\nthe attributes3 of this object by the attribute sub-learner\n(if any). For example, {red X, shiny X}.\n• return a set consisting of the result of recursively instan-\ntiating all subsets (of size <= 3) of the relation surface\ntemplates (using the object surface template to bind the\nargument slot of the ﬁrst attribute surface template). For\nexample, {ball, shiny ball, red ball, shiny red ball}\n2. each relation concept mapped to a surface template by the\nrelation sub-learner is translated to a set of token sequences\nby instantiating the surface template with its two arguments\nbound to each pair from the cross-product of all instantia-\ntions of the argument object concepts (from step 1).\n3. each action concept mapped to a surface template by the\naction sub-learner is translated to a set of token sequences\nby instantiating its the surface template with its arguments\nbound to each tuple from the cross-product of all instanti-\nations of the argument object concepts (from step 1). For\nexample, Dad throws the ball, Dad throws the red ball,\nDad throws the shiny ball, Dad throws the shiny red ball.\nThe scope of language covered by this process is limited\nand enhancing this is a direction for future work.\nH.2\nSub-Learners\nAll of our sample sub-learners share a common structure.\nFirst, all learners except the object learner preprocess the per-\nception graph to replace all sub-graphs aligned to object con-\ncepts with single nodes annotated with those concepts. Next,\ncandidate surface templates are generated from the language-\nconcept alignment:\n3This should also be done for relations, but this has not yet been\nimplemented.\n• for objects, any unaligned token.\n• for attributes, any unaligned token before or after a token\naligned to an object concept forms a candidate surface tem-\nplate (e.g. if red ball on the table is the utterance and ball\nis the only aligned token, then red ARG1 and X on would\nbe candidates).\n• for relations, any span of unaligned tokens for length at\nmost 2 between two aligned tokens aligned to object con-\ncepts is a candidate.\n• for verbs, ADAM uses a complex set of patterns to gen-\nerate surface templates based which account for different\npossible word order patterns in a language.\nFinally, for each candidate surface template one or more per-\nception hypotheses is created for each candidate surface tem-\nplate:\n• for objects, any object perception subgraph not already\naligned is a candidate.\n• for attributes, only single property perception nodes at-\ntached to the arguments are candidates.\n• for relations, the single initial hypothesis is the sub-graph\nconsisting of all shortest paths between the arguments to-\ngether with any nodes one hop off this path\n• for actions, the initial hypothesis is the entire graph.\nIf there was not an existing hypothesis for this surface tem-\nplate from previously observed situations, these new hypothe-\nses are stored for it. Otherwise, this new hypothesis set is used\nto update the existing hypothesis set in a way which depends\non the particular learning algorithm; this can cause the hy-\npothesis set to become empty, in which case the learner con-\ncludes the corresponding surface pattern candidate should be\ndiscarded (e.g. the learner may eventually conclude that red\nis not an action).\nThe following sample learning algorithms are provided in\nADAM:\n• Subset. This learns the ‘meaning’ of a word to be those as-\npects of the perception graph which are present every time\nit is used (inspired by (Webster & Marcus, 1989)). It is\nprovided as a simple baseline, but it will fail in the pres-\nence of noise. ADAM’s sample implementation uses it for\nlearning actions.\n• Pursuit, which implements the word learning model of\n(Stevens et al., 2017) into a variant we call PURSUIT-NA.\n(see appendix H.3) It learns well under noisy conditions,\nbut struggles with complex hypothesis spaces. ADAM’s\nsample implementation uses it for object, attribute, and re-\nlation learning.\n• ADAM includes a less-tested implementation of Cross-\nSituational Learning (Yu & Smith, 2007b), a global learn-\ning model that uses distributional statistics across learning\ninstances to make hypotheses for word meanings.\n• Propose but Verify (Trueswell et al., 2013), a local learning\nmodel that proposes and retains a word meaning hypothe-\nsis after a single word-learning instance, and abandons the\nhypothesis if the subsequent learning instances do not con-\nﬁrm it is also provided. Similarly to the Cross-Situation\nLearning model, this is also less tested.\nH.3\nPursuit Non-Atomic\nThe Pursuit algorithm was designed to learn an alignment\nbetween words and atomic meanings, as is typically the case\nin word learning experiments. However, the grounded world\nas perceived by the learner in ADAM is not made up of sim-\nple atomic objects. Rather objects form sub-graphs of a rich,\ncomplex perceptual structure. This requires that the meaning\nrepresentations assigned to words must have a similarly rich\nand complex structure. We use ADAM perception patterns\n(appendix G.1) for this representation.\nAdapting Pursuit to use perception patterns as meaning\nrepresentations (resulting in a variant we call PURSUIT-NA\nfor ‘non-atomic’) requires several changes to the base algo-\nrithm. First, PURSUIT-NA needs to select a new hypothe-\nsis from the intractably large space of possible perception\nsub-graphs both in (1) when a new word is encountered (to\nchoose the initial hypothesis) and in (3) when rewarding a\nnew random hypothesis because the leading hypothesis is dis-\nconﬁrmed. Fortunately, ADAM’s perceptual representation\nassumes the boundaries between objects (but not their iden-\ntities) are known at a pre-linguistic stage (Spelke & Kinzler,\n2007). Our implementation of PURSUIT-NA therefore uses\nthese object boundaries to deﬁne the hypothesis space when-\never it needs to make random choices.\nSecond, the perceptual representation of an object will con-\ntain both elements essential to the meaning of the correspond-\ning word and elements which are accidental. For example,\nthe shape of a ball is crucial to the meaning of the word ball,\nbut its color is not; however, from the perspective of a child,\nthe color of juice may be essential to how they use the word.\nWithout a way to generalize our hypotheses and distinguish\nthe essential from the accidental, PURSUIT-NA will be unable\nto learn because its leading hypotheses will consistently fail\nto match in situations outside those where they were initially\nperceived. To address this, PURSUIT-NA modiﬁes PURSUIT\nin two ways:\n• when attempting to conﬁrm whether or not the cur-\nrent leading hypothesis matches a new situation in (2),\nPURSUIT-NA exploits ADAM’s ability to ﬁnd partial\nmatches of patterns.\nIf a perfect match is not found,\nPURSUIT-NA still disconﬁrms and penalizes the leading hy-\npotheses. However, instead of rewarding a random hypoth-\nesis in (3), the sub-graph of the leading hypothesis which\ndid successfully match the scene’s perception graph is re-\nwarded, so long as the number of nodes in the matched sub-\npattern is of sufﬁcient size relative to the original pattern\n(this size ratio is the partial match parameter). For exam-\nple, if the learner believes being red is part of the meaning\nof ball and then observes the word ball being used in a sit-\nuation with a green ball, it will reward the (possibly new)\nhypothesis which contains the shape information but omits\nthe color information.\n• Whenever any hypothesis is rewarded (in (2) or (3)),\nPURSUIT-NA also rewards any hypotheses which are gen-\neralizations of that hypothesis, where one pattern is a gen-\neralization of another if its perception pattern graph repre-\nsentation is a sub-graph of the other’s. This helps prevent\nthe learner from learning very slowly by continually jump-\ning between overly speciﬁc hypotheses, with more general\nhypotheses taking a very long time to become the leading\nhypothesis, which is necessary for their scores to increase.\nH.4\nSpecialized Learners\nADAM provides two specialized learners which are modules\nthat augment the normal sub-learners to provide additional\nlearning capability. These specialized learner’s operate on\nADAM’s perceptual representation (see Appendix F) either\nbefore or after the other sub-learners discussed in Appendix\nH.2. These learners target linguistic structures which depend\non actions, relations, attributes, or objects to construct a rep-\nresentation of.\n• Plural Learner that learns templates for plurals (e.g many\ncats, two balls). Although the plural learner module learns\nlinguistic templates very similarly to the attribute learner\nmodule that learn attributes such as colors, it runs only\nwhen the perceived scene contains multiple objects that\nmatch the same pattern.\nThe module preprocesses the\nscene to mark these objects perceptually with nodes indi-\ncating counts to be learned as count attributes.\n• Functional Object Learner that learns meanings of ob-\njects based on their function (e.g.\na person sits on a\n_chair. An independent model. unlike the standard object\nlearner, this module tracks the interactions between a spe-\nciﬁc known object concept and how the object is used in an\naction concept. Allowing the learner to make an assump-\ntion to what an unknown object functioning in the same roll\nof an action might be. For example, if a chair in the action\n“A man sits on a chair” is replaced by a chair with only\nthree legs, or a stool which has no back the learner could\nstill describe the action as “A man sits on a chair” even\nthough the speciﬁc object is not immediately recognized\ncorrectly.\nI\nExperimentation and Evaluation\nFramework\nADAM provides a convenient framework for setting up and\nexecuting experiments. The user speciﬁes:\n• the language learner to use. This may optionally be ini-\ntialized from a saved learner state (e.g. to run experiments\nwith actions with a learner which has already learned the\nobjects appearing in the actions).\n• one\nor\nmore\ntraining\nstages.\nA\ntrain-\ning\n(or\ntest)\nstage\nis\na\nsequence\nof\n(situation,linguisticutterances, perceptualrepresentation)\ntuples. Typically these are not provided directly; instead\nthe user provides a collection of situation templates, a\ntemplate instantiation strategy (e.g. random sampling), a\nlanguage generator, and a perception generator.\n• zero or more test warm-up stages. These are observed by\nthe learner without evaluation before evaluating on the test\nset.\n• one or more test stages.\n• a collection of experiment observers. These observers\ncan hook into different points in the experiment process\n(before and after the observation of each training example,\nafter training is complete, after each test instance).\nExperiment observers are provided to:\n• track how often the top-scoring linguistic description pro-\nduce by the learner on an instance matches the ‘gold’ ut-\nterance associated with the instance.\n• track how often the ‘gold’ utterance associated with an in-\nstance appears anywhere in the learner’s candidate descrip-\ntion list. This is typically more useful to track since there\nare numerous valid and reasonable linguistic utterances in\nany given situation.\n• write a detailed report of the learning process to a human-\nreadable HTML ﬁle. This report contains the situation,\nlinguistic utterance, and perceptual representation of ev-\nery training and test instance, together with the linguistic\nutterances produced by the learner.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-05-05",
  "updated": "2021-05-05"
}