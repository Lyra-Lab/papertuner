{
  "id": "http://arxiv.org/abs/2105.05222v2",
  "title": "Including Signed Languages in Natural Language Processing",
  "authors": [
    "Kayo Yin",
    "Amit Moryossef",
    "Julie Hochgesang",
    "Yoav Goldberg",
    "Malihe Alikhani"
  ],
  "abstract": "Signed languages are the primary means of communication for many deaf and\nhard of hearing individuals. Since signed languages exhibit all the fundamental\nlinguistic properties of natural language, we believe that tools and theories\nof Natural Language Processing (NLP) are crucial towards its modeling. However,\nexisting research in Sign Language Processing (SLP) seldom attempt to explore\nand leverage the linguistic organization of signed languages. This position\npaper calls on the NLP community to include signed languages as a research area\nwith high social and scientific impact. We first discuss the linguistic\nproperties of signed languages to consider during their modeling. Then, we\nreview the limitations of current SLP models and identify the open challenges\nto extend NLP to signed languages. Finally, we urge (1) the adoption of an\nefficient tokenization method; (2) the development of linguistically-informed\nmodels; (3) the collection of real-world signed language data; (4) the\ninclusion of local signed language communities as an active and leading voice\nin the direction of research.",
  "text": "Including Signed Languages in Natural Language Processing\nKayo Yin1, Amit Moryossef2, Julie Hochgesang3, Yoav Goldberg2,4, Malihe Alikhani5\n1Language Technologies Institute, Carnegie Mellon University\n2Bar-Ilan University\n3Department of Linguistics, Gallaudet University\n4Allen Institute for AI\n5School of Computing and Information, University of Pittsburgh\nkayoy@cs.cmu.edu, amitmoryossef@gmail.com\njulie.hochgesang@gallaudet.edu, yogo@cs.biu.ac.il, malihe@pitt.edu\nAbstract\nSigned languages are the primary means of\ncommunication for many deaf and hard of\nhearing individuals. Since signed languages\nexhibit all the fundamental linguistic proper-\nties of natural language, we believe that tools\nand theories of Natural Language Processing\n(NLP) are crucial towards its modeling. How-\never, existing research in Sign Language Pro-\ncessing (SLP) seldom attempt to explore and\nleverage the linguistic organization of signed\nlanguages.\nThis position paper calls on the\nNLP community to include signed languages\nas a research area with high social and sci-\nentiﬁc impact. We ﬁrst discuss the linguistic\nproperties of signed languages to consider dur-\ning their modeling. Then, we review the limi-\ntations of current SLP models and identify the\nopen challenges to extend NLP to signed lan-\nguages. Finally, we urge (1) the adoption of\nan efﬁcient tokenization method; (2) the devel-\nopment of linguistically-informed models; (3)\nthe collection of real-world signed language\ndata; (4) the inclusion of local signed language\ncommunities as an active and leading voice in\nthe direction of research.\n1\nIntroduction\nNatural Language Processing (NLP) has revolu-\ntionized the way people interact with technology\nthrough the rise of personal assistants and machine\ntranslation systems, to name a few. However, the\nvast majority of NLP models require a spoken lan-\nguage input (speech or text), thereby excluding\naround 200 different signed languages and up to 70\nmillion deaf people1 from modern language tech-\nnologies.\n1According to World Federation of the Deaf\nhttps://wfdeaf.org/our-work/\n2000\n2002\n2004\n2006\n2008\n2010\n2012\n2014\n2016\n2018\n2020\n0\n100\n200\n300\n400\n# of publications\nAll Computer Science\nACL Anthology\nFigure 1: Evolution of the number of publications refer-\nring to sign language in its title from computer science\nvenues and in the ACL anthology. Publications in com-\nputer science are extracted from the Semantic Scholar\narchive (Ammar et al., 2018).\nThroughout history, Deaf communities fought\nfor the right to learn and use signed languages, as\nwell as for the recognition of signed languages as le-\ngitimate languages (§2). Indeed, signed languages\nare sophisticated communication modalities that\nare at least as capable as spoken languages in all\nmanners, linguistic and social. However, in a pre-\ndominantly oral society, deaf people are constantly\nencouraged to use spoken languages through lip-\nreading or text-based communication. The exclu-\nsion of signed languages from modern language\ntechnologies further suppresses signing in favor of\nspoken languages. This disregards the preferences\nof the Deaf communities who strongly prefer to\ncommunicate in signed languages both online and\nfor in-person day-to-day interactions, among them-\nselves and when interacting with spoken language\ncommunities (Padden and Humphries, 1988; Glick-\nman and Hall, 2018). Thus, it is essential to make\narXiv:2105.05222v2  [cs.CL]  22 Jul 2021\nsigned languages accessible.\nTo date, a large amount of research on Sign Lan-\nguage Processing (SLP) has been focused on the\nvisual aspect of signed languages, led by the Com-\nputer Vision (CV) community, with little NLP in-\nvolvement (Figure 1). This is not unreasonable,\ngiven that a decade ago, we lacked the adequate\nCV tools to process videos for further linguistic\nanalyses. However, like spoken languages, signed\nlanguages are fully-ﬂedged systems that exhibit\nall the fundamental characteristics of natural lan-\nguages (§3), and current SLP techniques fail to ad-\ndress or leverage the linguistic structure of signed\nlanguages (§4). This leads us to believe that NLP\ntools and theories are crucial to process signed\nlanguages. Given the recent advances in CV, this\nposition paper argues that now is the time to in-\ncorporate linguistic insight into signed language\nmodeling.\nSigned languages introduce novel challenges for\nNLP due to their visual-gestural modality, simul-\ntaneity, spatial coherence, and lack of written form.\nBy working on signed languages, the community\nwill gain a more holistic perspective on natural\nlanguages through a better understanding of how\nmeaning is conveyed by the visual modality and\nhow language is grounded in visuospatial concepts.\nMoreover, SLP is not only an intellectually ap-\npealing area but also an important research area\nwith a strong potential to beneﬁt signing communi-\nties. Examples of beneﬁcial applications enabled\nby signed language technologies include better doc-\numentation of endangered sign languages; educa-\ntional tools for sign language learners; tools for\nquery and retrieval of information from signed\nlanguage videos; personal assistants that react to\nsigned languages; real-time automatic sign lan-\nguage interpretations, and more. Needless to say,\nin addressing this research area, researchers should\nwork alongside and under the direction of deaf\ncommunities, and to the beneﬁt of the signing com-\nmunities’ interest above all (Harris et al., 2009).\nAfter identifying the challenges and open prob-\nlems to successfully include signed languages in\nNLP (§5), we emphasize the need to: (1) develop\na standardized tokenization method of signed lan-\nguages with minimal information loss for its mod-\neling; (2) extend core NLP technologies to signed\nlanguages to create linguistically-informed mod-\nels; (3) collect signed language data of sufﬁcient\nsize that accurately represents the real world; (4)\ninvolve and collaborate with the Deaf communities\nat every step of research.\n2\nBackground and Related Work\n2.1\nHistory of Signed Languages and Deaf\nCulture\nOver the course of modern history, spoken lan-\nguages were dominant so much so that signed lan-\nguages struggled to be recognized as languages\nin their own right and educators developed mis-\nconceptions that signed language acquisition may\nhinder the development of speech skills. For ex-\nample, in 1880, a large international conference\nof deaf educators called the Second International\nCongress on Education of the Deaf banned teach-\ning signed languages, favoring speech therapy in-\nstead. It was not until the seminal work on Amer-\nican Sign Language (ASL) by Stokoe (1960) that\nsigned languages started gaining recognition as\nnatural, independent, and well-deﬁned languages,\nwhich then inspired other researchers to further\nexplore signed languages as a research area. Never-\ntheless, antiquated notions that deprioritized signed\nlanguages continue to do harm and subjects many\nto linguistic neglect (Humphries et al., 2016). Sev-\neral studies have shown that deaf children raised\nsolely with spoken languages do not gain enough\naccess to a ﬁrst language during their critical pe-\nriod of language acquisition (Murray et al., 2020).\nThis language deprivation can lead to life-long con-\nsequences on the cognitive, linguistic, socioemo-\ntional, and academic development of the deaf (Hall\net al., 2017).\nSigned languages are the primary languages of\ncommunication for the Deaf2 and are at the heart of\nDeaf communities. Failing to recognize signed lan-\nguages as fully-ﬂedged natural language systems\nin their own right has had harmful effects in the\npast, and in an increasingly digitized world, the\nNLP community has an important responsibility\nto include signed languages in its research. NLP\nresearch should strive to enable a world in which\nall people, including the Deaf, have access to lan-\nguages that ﬁt their lived experience.\n2.2\nSign Language Processing in the\nLiterature\nJaffe (1994); Ong and Ranganath (2005); Parton\n2When capitalized, “Deaf” refers to a community of deaf\npeople who share a language and a culture, whereas the lower-\ncase “deaf” refers to the audiological condition of not hearing.\n(2006) survey early works in SLP that were mostly\nlimited to using sensors to capture ﬁngerspelling\nand isolated signs, or use rules to synthesize signs\nfrom spoken language text, due to the lack of ade-\nquate CV technology at the time to process videos.\nThis paper will instead focus on more recent vision-\nbased and data-driven approaches that are non-\nintrusive and more powerful. The introduction of\na continuous signed language benchmark dataset\n(Forster et al., 2014; Cihan Camg¨oz et al., 2018),\ncoupled with the advent of deep learning for visual\nprocessing, lead to increased efforts to recognize\nsigned expressions from videos. Recent surveys\non SLP mostly review these different approaches\nfor sign language recognition developed by the CV\ncommunity (Koller, 2020; Rastgoo et al., 2020;\nAdaloglou et al., 2020).\nMeanwhile, signed languages have remained rel-\natively overlooked in NLP literature (Figure 1).\nBragg et al. (2019) argue the importance of an\ninterdisciplinary approach to SLP, raising the im-\nportance of NLP involvement among other disci-\nplines. We take this argument further by diving into\nthe linguistic modeling challenges for signed lan-\nguages and providing a roadmap of open questions\nto be addressed by the NLP community, in hopes\nof stimulating efforts from an NLP perspective to-\nwards research on signed languages. Conversely,\nMoryossef and Goldberg (2021) organize the sign\nlanguage processing literature, datasets, and tasks;\nhowever, they do not suggest and actionable re-\nsearch directions.\n3\nSign Language Lingusitics\nSigned languages consist of phonological, morpho-\nlogical, syntactic, and semantic levels of structure\nthat fulﬁll the same social, cognitive, and com-\nmunicative purposes as other natural languages.\nWhile spoken languages primarily channel the oral-\nauditory modality, signed languages use the visual-\ngestural modality, relying on the face, hands, body\nof the signer, and the space around them to create\ndistinctions in meaning. We present the linguistic\nfeatures of signed languages3 that must be taken\ninto account during their modeling.\nPhonology\nSigns are composed of minimal units\nthat combine manual features such as hand conﬁg-\nuration, palm orientation, placement, contact, path\nmovement, local movement, as well as non-manual\n3We mainly refer to ASL, where most sign language re-\nsearch has been conducted, but not exclusively.\nfeatures including eye aperture, head movement,\nand torso positioning4 (Liddell and Johnson, 1989;\nJohnson and Liddell, 2011; Brentari, 2011; Sandler,\n2012). In both signed and spoken languages, not all\npossible phonemes are realized, and inventories of\ntwo languages’ phonemes/features may not overlap\ncompletely. Different languages are also subject to\nrules for the allowed combinations of features.\nSimultaneity\nThough an ASL sign takes about\ntwice as long to produce than an English word,\nthe rates of transmission of information between\nthe two languages are similar (Bellugi and Fischer,\n1972). One way signed languages compensate for\nthe slower production rate of signs is through si-\nmultaneity: signed languages make use of multiple\nvisual cues to convey different information simul-\ntaneously(Sandler, 2012). For example, the signer\nmay produce the sign for ’cup’ on one hand while\nsimultaneously pointing to the actual cup with the\nother to express “that cup”. Similarly to tone in\nspoken languages, the face and torso can convey ad-\nditional affective information (Liddell et al., 2003;\nJohnston and Schembri, 2007). Facial expressions\ncan modify adjectives, adverbs, and verbs; a head\nshake can negate a phrase or sentence; eye direction\ncan help indicate referents.\nReferencing\nThe signer can introduce referents\nin discourse either by pointing to their actual loca-\ntions in space, or by assigning a region in the sign-\ning space to a non-present referent and by pointing\nto this region to refer to it (Rathmann and Mathur,\n2011; Schembri et al., 2018). Signers can also\nestablish relations between referents grounded in\nsigning space by using directional signs or em-\nbodying the referents using body shift or eye gaze\n(Dudis, 2004; Liddell and Metzger, 1998). Spatial\nreferencing also impacts morphology when the di-\nrectionality of a verb depends on the location of the\nreference to its subject and/or object (de Beuzeville,\n2008; Fenlon et al., 2018): for example, a direc-\ntional verb can move from the location of its sub-\nject and ending at the location of its object. While\nthe relation between referents and verbs in spo-\nken language is more arbitrary, referent relations\nare usually grounded in signed languages. The vi-\nsual space is heavily exploited to make referencing\nclear.\n4In this work, we focus on visual signed languages rather\nthan tactile systems such as Pro-Tactile ASL which DeafBlind\nAmericans sometimes prefer.\nAnother way anaphoric entities are referenced\nin sign language is by using classiﬁers or depicting\nsigns (Supalla, 1986; Wilcox and Hafer, 2004; Roy,\n2011) that help describe the characteristics of the\nreferent. Classiﬁers are typically one-handed signs\nthat do not have a particular location or movement\nassigned to them, or derive features from mean-\ningful discourse (Liddell et al., 2003), so they can\nbe used to convey how the referent relates to other\nentities, describe its movement, and give more de-\ntails. For example, to tell about a car swerving and\ncrashing, one might use the hand classiﬁer for a\nvehicle, move it to indicate swerving, and crash it\nwith another entity in space.\nTo quote someone other than oneself, signers per-\nform role shift (Cormier et al., 2015), where they\nmay physically shift in space to mark the distinc-\ntion, and take on some characteristics of the people\nthey are representing. For example, to recount a\ndialogue between a taller and a shorter person, the\nsigner may shift to one side and look up when tak-\ning the shorter person’s role, shift to the other side\nand look down when taking the taller person’s role.\nFingerspelling\nFingerspelling is a result of lan-\nguage contact between a signed language and a\nsurrounding spoken language written form (Bat-\ntison, 1978; Wilcox, 1992; Brentari and Padden,\n2001; Patrie and Johnson, 2011). A set of manual\ngestures correspond with a written orthography or\nphonetic system. Fingerspelling is often used to\nindicate names or places or new concepts from the\nspoken language but often have become integrated\ninto the signed languages themselves as another\nlinguistic strategy (Padden, 1998; Montemurro and\nBrentari, 2018).\n4\nCurrent State of SLP\nIn this section, we present the existing methods,\nresources, and tasks in SLP, and discuss their limi-\ntations to lay the ground for future research.\n4.1\nRepresentations of Signed Languages\nRepresentation is a signiﬁcant challenge for SLP,\nas unlike spoken languages, signed languages have\nno widely adopted written form. Figure 2 illus-\ntrates each signed language representation we will\ndescribe below.\nVideos\nare the most straightforward representa-\ntion of a signed language and can amply incorpo-\nrate the information conveyed through sign. One\nmajor drawback of using videos is their high dimen-\nsionality: they usually include more information\nthan needed for modeling, and are expensive to\nstore, transmit, and encode. As facial features are\nessential in sign, anonymizing raw videos also re-\nmains an open problem, limiting the possibility\nof making these videos publicly available (Isard,\n2020).\nPoses\nreduce the visual cues from videos to\nskeleton-like wireframe or mesh representing the\nlocation of joints. While motion capture equip-\nment can often provide better quality pose estima-\ntion, it is expensive and intrusive, and estimating\npose from videos is the preferred method currently\n(Pishchulin et al., 2012; Chen et al., 2017; Cao\net al., 2019; G¨uler et al., 2018). Compared to video\nrepresentations, accurate poses are lower in com-\nplexity and anonymized, while observing relatively\nlow information loss. However, they remain a con-\ntinuous, multidimensional representation that is not\nadapted to most NLP models.\nWritten notation systems\nrepresent signs as dis-\ncrete visual features. Some systems are written\nlinearly and others use graphemes in two dimen-\nsions. While various universal (Sutton, 1990; Prill-\nwitz and Zienert, 1990) and language-speciﬁc no-\ntation systems (Stokoe Jr, 2005; Kakumasu, 1968;\nBergman, 1979) have been proposed, no writing\nsystem has been adopted widely by any sign lan-\nguage community, and the lack of standard hinders\nthe exchange and uniﬁcation of resources and ap-\nplications between projects. Figure 2 depicts two\nuniversal notation systems: SignWriting (Sutton,\n1990), a two-dimensional pictographic system, and\nHamNoSys (Prillwitz and Zienert, 1990), a linear\nstream of graphemes that was designed to be read-\nable by machines.\nGlossing\nis the transcription of signed languages\nsign-by-sign, where every sign has a unique identi-\nﬁer. While various sign language corpus projects\nhave provided gloss annotation guidelines (Mesch\nand Wallin, 2015; Johnston and De Beuzeville,\n2016; Konrad et al., 2018), again, there is no sin-\ngle agreed-upon standard. Linear gloss annotations\nare also an imprecise representation of signed lan-\nguage: they do not adequately capture all infor-\nmation expressed simultaneously through different\ncues (i.e. body posture, eye gaze) or spatial re-\nlations, which leads to an inevitable information\nloss up to a semantic level that affects downstream\nYOUR\n\nHamNoSys\numbrella\nSignWriting\nASL Gloss\nNAME\n\nWHAT\n\nPose Stream\nVideo Stream\n107 FRAMES\nFigure 2: Representations of an American Sign Language phrase with video frames, pose estimations, SignWriting,\nHamNoSys and glosses. English translation: “What is your name?”\nperformance on SLP tasks (Yin and Read, 2020b).\n4.2\nExisting Sign Language Resources\nNow, we introduce the different formats of re-\nsources and discuss how they can be used for\nsigned language modeling.\nBilingual\ndictionaries\nfor\nsigned\nlanguage\n(Mesch and Wallin, 2012; Fenlon et al., 2015;\nCrasborn et al., 2016; Gutierrez-Sigut et al., 2016)\nmap a spoken language word or short phrase to\na signed language video. One notable dictionary\nis, SpreadTheSign5 is a parallel dictionary con-\ntaining around 23,000 words with up to 41 differ-\nent spoken-signed language pairs and more than\n500,000 videos in total. While dictionaries may\nhelp create lexical rules between languages, they\ndo not demonstrate the grammar or the usage of\nsigns in context.\nFingerspelling\ncorpora\nusually\nconsist\nof\nvideos of words borrowed from spoken languages\nthat are signed letter-by-letter.\nThey can be\nsynthetically created (Dreuw et al., 2006) or mined\nfrom online resources (Shi et al., 2018, 2019).\nHowever, they only capture one aspect of signed\nlanguages.\nIsolated sign corpora\nare collections of anno-\ntated single signs. They are synthesized (Ebling\net al., 2018; Huang et al., 2018; Sincan and Keles,\n2020; Hassan et al., 2020) or mined from online\nresources (Vaezi Joze and Koller, 2019; Li et al.,\n2020), and can be used for isolated sign language\nrecognition or for contrastive analysis of minimal\nsigning pairs (Imashev et al., 2020). However, like\n5https://www.spreadthesign.com/\ndictionaries, they do not describe relations between\nsigns nor do they capture coarticulation during sign-\ning, and are often limited in vocabulary size (20-\n1000 signs)\nContinuous sign corpora\ncontain parallel se-\nquences of signs and spoken language. Available\ncontinuous sign corpora are extremely limited, con-\ntaining 4-6 orders of magnitude fewer sentence\npairs than similar corpora for spoken language ma-\nchine translation (Arivazhagan et al., 2019). More-\nover, while automatic speech recognition (ASR)\ndatasets contain up to 50,000 hours of recordings\n(Pratap et al., 2020), the largest continuous sign lan-\nguage corpus contain only 1,150 hours, and only 50\nof them are publicly available (Hanke et al., 2020).\nThese datasets are usually synthesized (Databases,\n2007; Crasborn and Zwitserlood, 2008; Ko et al.,\n2019; Hanke et al., 2020) or recorded in studio con-\nditions (Forster et al., 2014; Cihan Camg¨oz et al.,\n2018), which does not account for noise in real-life\nconditions. Moreover, some contain signed inter-\npretations of spoken language rather than naturally-\nproduced signs, which may not accurately repre-\nsent native signing since translation is now a part\nof the discourse event.\nAvailability\nUnlike the vast amount and diversity\nof available spoken language resources that allow\nvarious applications, signed language resources are\nscarce and currently only support translation and\nproduction. Unfortunately, most of the signed lan-\nguage corpora discussed in the literature are either\nnot available for use or available under heavy re-\nstrictions and licensing terms. Signed language\ndata is especially challenging to anonymize due to\nthe importance of facial and other physical features\nin signing videos, limiting its open distribution, and\ndeveloping anonymization with minimal informa-\ntion loss, or accurate anonymous representations is\na promising research problem.\n4.3\nSign Language Processing Tasks\nThe CV community has mainly led the research\non SLP so far to focus on processing the visual\nfeatures in signed language videos. As a result,\ncurrent SLP methods do not fully address the lin-\nguistic complexity of signed languages. We survey\ncommon SLP tasks and limitations of current meth-\nods by drawing on linguistic theories of signed\nlanguages.\nDetection\nSign language detection is the binary\nclassiﬁcation task to determine whether a signed\nlanguage is being used or not in a given video frame.\nWhile recent detection models (Borg and Camilleri,\n2019; Moryossef et al., 2020) achieve high perfor-\nmance, we lack well-annotated data that include\ninterference and distractions with non-signing in-\nstances for proper evaluation. A similar task in\nspoken languages is voice activity detection (VAD)\n(Sohn et al., 1999; Ramırez et al., 2004), the de-\ntection of when a human voice is used in an au-\ndio signal. However, as VAD methods often rely\non speech-speciﬁc representations such as spectro-\ngrams, they are not always applicable to videos.\nIdentiﬁcation\nSign language identiﬁcation clas-\nsiﬁes which signed language is being used in a\ngiven video automatically. Existing works utilize\nthe distribution of phonemes (Gebre et al., 2013)\nor activity maps in signing space (Monteiro et al.,\n2016) to identify the signed language in videos.\nHowever, these methods only rely on low-level\nvisual features, while signed languages have sev-\neral distinctive features on a linguistic level, such\nas lexical or structural differences (McKee and\nKennedy, 2000; Kimmelman, 2014; Ferreira-Brito,\n1984; Shroyer and Shroyer, 1984) which have not\nbeen explored for this task.\nSegmentation\nSegmentation consists of detect-\ning the frame boundaries for signs or phrases in\nvideos to divide them into meaningful units. Cur-\nrent methods resort to segmenting units loosely\nmapped to signed language units (Santemiz et al.,\n2009; Farag and Brock, 2019; Bull et al., 2020),\nand does not leverage reliable linguistic predictors\nof sentence boundaries such as prosody in signed\nlanguages (i.e. pauses, sign duration, facial expres-\nsions, eye apertures) (Sandler, 2010; Ormel and\nCrasborn, 2012).\nRecognition\nSign language recognition (SLR)\ndetects and label signs from a video, either on\nisolated (Imashev et al., 2020; Sincan and Keles,\n2020) or continuous (Cui et al., 2017; Camg¨oz\net al., 2018, 2020b) signs. Though some previous\nworks have referred to this as “sign language trans-\nlation”, recognition merely determines the associ-\nated label of each sign, without handling the syntax\nand morphology of the signed language (Padden,\n1988) to create a spoken language output. Instead,\nSLR has often been used as an intermediate step\nduring translation to produce glosses from signed\nlanguage videos.\nTranslation\nSign language translation (SLT)\ncommonly refers to the translation of signed lan-\nguage to spoken language. Current methods either\nperform translation with glosses (Camg¨oz et al.,\n2018, 2020b; Yin and Read, 2020a,b; Moryossef\net al., 2021) or on pose estimations and sign ar-\nticulators from videos (Ko et al., 2019; Camg¨oz\net al., 2020a), but do not, for instance, handle spa-\ntial relations and grounding in discourse to resolve\nambiguous referents.\nProduction\nSign language production consists\nof producing signed language from spoken lan-\nguage and often use poses as an intermediate rep-\nresentation to overcome challenges in animation.\nTo overcome the challenges in generating videos\ndirectly, most efforts use poses as an intermedi-\nate representation, with the goal of either using\ncomputer animation or pose-to-video models to\nperform video production. Earlier methods gen-\nerate and concatenate isolated signs (Stoll et al.,\n2018, 2020), while more recent methods (Saun-\nders et al., 2020b,a; Zelinka and Kanis, 2020; Xiao\net al., 2020) autoregressively decode a sequence of\nposes from an input text. Due to the lack of suitable\nautomatic evaluation methods of generated signs,\nexisting works resort to measuring back-translation\nquality, which cannot accurately capture the quality\nof the produced signs nor its usability in real-world\nsettings. A better understanding of how distinc-\ntions in meaning are created in signed language\nmay help develop a better evaluation method.\n5\nTowards Including Signed Languages\nin Natural Language Processing\nThe limitations in the design of current SLP models\noften stem from the lack of exploring the linguistic\npossibilities of signed languages. We therefore in-\nvite the NLP community to collaborate with the CV\ncommunity, for their expertise in visual processing,\nand signing communities and sign linguists, for\ntheir expertise in signed languages and the lived\nexperiences of signers, in researching SLP. We be-\nlieve that ﬁrst, the development of known tasks\nin the standard NLP pipeline to signed languages\nwill help us better understand how to model them,\nas well as provide valuable tools for higher-level\napplications. Although these tasks have been thor-\noughly researched for spoken languages, they pose\ninteresting new challenges in a different modality.\nWe also emphasize the need for real-world data to\ndevelop such methods, and a close collaboration\nwith signing communities to have an accurate un-\nderstanding of how signed language technologies\ncan beneﬁt signers, all the while respecting the\nDeaf community’s ownership of signed languages.\n5.1\nBuilding NLP Pipelines\nAlthough signed and spoken languages differ in\nmodality, we argue that as both express the syntax,\nsemantics, and pragmatics of natural languages,\nfundamental theories of NLP can and should be\nextended to signed languages. NLP applications\noften rely on low-level tools such as tokenizers\nand parsers, so we invite more research efforts on\nthese core NLP tasks that often lay the foundation\nof other applications. We also discuss what con-\nsiderations should be taken into account for their\ndevelopment to signed languages and raise open\nquestions that should be addressed.\nTokenization\nThe vast majority of NLP meth-\nods require a discrete input. To extend NLP tech-\nnologies to signed languages, we must ﬁrst and\nforemost be able to develop adequate tokenization\ntools that maps continuous signed language videos\nto a discrete, accurate representation with mini-\nmal information loss. While existing SLP systems\nand datasets often use glosses as discrete lexical\nunits of signed phrases, this poses three signiﬁcant\nproblems: (1) linear, single-dimensional glosses\ncannot fully capture the spatial constructions of\nsigned languages, which downgrades downstream\nperformance (Yin and Read, 2020b); (2) glosses\nare language-speciﬁc and requiring new glossing\nmodels for each language is impractical given the\nscarcity of resources; (3) glosses lack standard\nacross corpora which limits data sharing and adds\nsigniﬁcant overhead in modeling.\nWe thus urge the adoption of an efﬁcient, univer-\nsal, and standardized method for tokenization of\nsigned languages, all the while considering: how do\nwe deﬁne lexical units in signed languages? (John-\nston and Schembri, 1999; Johnston, 2010) To what\ndegree can phonological units of signed languages\nbe mapped to lexical units? Should we model the\narticulators of signs separately or together? What\nare the cross-linguistic phonological differences\nto consider? To what extent can ideas used in au-\ntomatic speech recognition be applied to signed\nlanguages?\nSyntactic Analysis\nPart-of-speech (POS) tag-\nging and syntactic parsing are fundamental to un-\nderstand the meaning of words in context. Yet, no\nsuch linguistic tools for automatic syntactic anal-\nyses exist. To develop such tools, we must ﬁrst\ndeﬁne to what extent POS tagging and syntactic\nparsing for spoken languages also generalize to\nsigned languages - do we need a new set of POS\nand dependency tags for signed languages? How\nare morphological features expressed? What are\nthe annotation guidelines to create datasets on syn-\ntax? Can we draw on linguistic theories to design\nfeatures and rules that perform these tasks? Are\nthere typologically similar spoken languages for\nsome signed languages we can perform transfer\nlearning with?\nNamed Entity Recognition (NER)\nRecogniz-\ning named entities and ﬁnding relationships be-\ntween them are highly important components in\ninformation retrieval and classiﬁcation. Named\nentities in signed languages can be produced by\na ﬁngerspelled sequence, a sign, or even through\nmouthing of the name while the referent is intro-\nduced through pointing. Bleicken et al. (2016)\nattempt NER in German Sign Language (DGS) to\nperform anonymization, but only do so indirectly,\nby either performing NER on the gold DGS gloss\nannotations and German translations or manually\non the videos. We instead propose NER in a fully\nautomated fashion while considering, what are the\nvisual markers of named entities? How are they\nintroduced and referenced? How are relationships\nbetween them established?\nCoreference Resolution\nResolving coreference\nis crucial for language understanding. In signed lan-\nguages, present referents, where the signer explic-\nitly points to the entity in question, are relatively\nunambiguous. In contrast, non-present referents\nand classiﬁers are heavily grounded in the signing\nspace, so good modeling of the spatial coherence in\nsign language is required. Evidence suggests that\nclassic theoretical frameworks, such as discourse\nrepresentation theory, may extend to signed lan-\nguages (Steinbach and Onea, 2016). We pose the\nfollowing questions: to what extent can automatic\ncoreference resolution of spoken languages be ap-\nplied to signed languages? How do we keep track\nof referents in space? How can we leverage spatial\nrelations to resolve ambiguity?\nTowards Linguistically Informed and Multi-\nmodal SLP\nWe highly encourage the collabora-\ntion of multimodal and SLP research communities\nto develop powerful SLP models informed by core\nNLP tools such as the ones discussed, all the while\nprocessing and relating information from both lin-\nguistic and visual modalities. On the one hand, the-\nories and methods to reason multimodal messages\ncan enhance the joint modeling of vision and lan-\nguage in signed languages. SLP is especially sub-\nject to three of the core technical challenges in mul-\ntimodal machine learning (Baltruˇsaitis et al., 2018):\ntranslation - how do we map visual-gestural in-\nformation to/from audio-oral and textual informa-\ntion? alignment - how do we relate signed lan-\nguage units to spoken language units? co-learning\n- can we transfer high-resource spoken language\nknowledge to signed language? On the other hand,\nmeaning in spoken languages is not only conveyed\nthrough speech or text but also through the visual\nmodality. Studying signed languages can give a\nbetter understanding of how to model co-speech\ngestures, spatial discourse relations, and conceptual\ngrounding of language through vision.\n5.2\nCollect Real-World Data\nData is essential to develop any of the core NLP\ntools previously described, and current efforts in\nSLP are often limited by the lack of adequate data.\nWe discuss the considerations to keep in mind when\nbuilding datasets, challenges of collecting such\ndata, and directions to facilitate data collection.\nWhat is Good Signed Language Data?\nFor\nSLP models to be deployable, they must be de-\nveloped using data that represents the real world ac-\ncurately. What constitutes an ideal signed language\ndataset is an open question, we suggest including\nthe following requirements: (1) a broad domain; (2)\nsufﬁcient data and vocabulary size; (3) real-world\nconditions; (4) naturally produced signs; (5) a di-\nverse signer demographic; (6) native signers; and\nwhen applicable, (7) dense annotations.\nTo illustrate the importance of data quality dur-\ning modeling, we ﬁrst take as an example a cur-\nrent benchmark for SLP, the RWTH-PHOENIX-\nWeather 2014T dataset (Cihan Camg¨oz et al., 2018)\nof German Sign Language, that does not meet most\nof the above criteria: it is restricted to the weather\ndomain (1); contains only around 8K segments\nwith 1K unique signs (2); ﬁlmed in studio condi-\ntions (3); interpreted from German utterances (4);\nand signed by nine Caucasian interpreters (5,6).\nAlthough this dataset successfully addressed data\nscarcity issues at the time and successfully rendered\nresults comparable and fueled competitive research,\nit does not accurately represent signed languages in\nthe real world. On the other hand, the Public DGS\nCorpus (Hanke et al., 2020) is an open-domain (1)\ndataset consisting of 50 hours of natural signing\n(4) by 330 native signers from various regions in\nGermany (5,6), annotated with glosses, HamNoSys\nand German translations (7), meeting all but two\nrequirements we suggest.\nWe train a gloss-to-text sign language translation\ntransformer (Yin and Read, 2020b) on both datasets.\nOn RWTH-PHOENIX-Weather 2014T, we obtain\n22.17 BLEU on testing; on Public DGS Corpus, we\nobtain a mere 3.2 BLEU. Although Transformers\nachieve encouraging results on RWTH-PHOENIX-\nWeather 2014T (Saunders et al., 2020b; Camg¨oz\net al., 2020a), they fail on more realistic, open-\ndomain data. These results reveal that ﬁrstly, for\nreal-world applications, we need more data to train\nsuch types of models, and secondly, while available\ndata is severely limited in size, less data-hungry\nand more linguistically-informed approaches may\nbe more suitable. This experiment reveals how it\nis crucial to use data that accurately represent the\ncomplexity and diversity of signed languages to\nprecisely assess what types of methods are suitable,\nand how well our models would deploy to the real\nworld.\nChallenges of Data Collection\nCollecting and\nannotating signed data inline with the ideal requires\nmore resources than speech or text data, taking up\nto 600 minutes per minute of an annotated signed\nlanguage video (Hanke et al., 2020). Moreover,\nannotation usually requires a speciﬁc set of knowl-\nedge and skills, which makes recruiting or train-\ning qualiﬁed annotators challenging. Additionally,\nthere is little existing signed language data in the\nwild that are open to use, especially from native\nsigners that are not interpretations of speech. There-\nfore, data collection often requires signiﬁcant ef-\nforts and costs of on-site recording as well.\nAutomating Annotation\nTo collect more data\nthat enables the development of deployable SLP\nmodels, one useful research direction is creating\ntools that can simplify or automate parts of the col-\nlection and annotation process. One of the largest\nbottleneck in obtaining more adequate signed lan-\nguage data is the amount of time and scarcity of\nexperts required to perform annotation. Therefore,\ntools that perform automatic parsing, detection of\nframe boundaries, extraction of articulatory fea-\ntures, suggestions for lexical annotations, and allow\nparts of the annotation process to be crowdsourced\nto non-experts, to name a few, have a high potential\nto facilitate and accelerate the availability of good\ndata.\n5.3\nPractice Deaf Collaboration\nFinally, when working with signed languages, it is\nvital to keep in mind who this technology should\nbeneﬁt, and what they need. Researchers in SLP\nmust honor that signed languages belong to the\nDeaf community and avoid exploiting their lan-\nguage as a commodity (Bird, 2020).\nSolving Real Needs\nMany efforts in SLP have\ndeveloped intrusive methods (e.g. requiring signers\nto wear special gloves), which are often rejected\nby signing communities and therefore have limited\nreal-world value. Such efforts are often marketed\nto perform “sign language translation” when they,\nin fact, only identify ﬁngerspelling or recognize a\nvery limited set of isolated signs at best. These ap-\nproaches oversimplify the rich grammar of signed\nlanguages, promote the misconception that signs\nare solely expressed through the hands, and are\nconsidered by the Deaf community as a manifes-\ntation of audism, where it is the signers who must\nmake the extra effort to wear additional sensors\nto be understood by non-signers (Erard, 2017). In\norder to avoid such mistakes, we encourage close\nDeaf involvement throughout the research process\nto ensure that we direct our efforts towards appli-\ncations that will be adopted by signers, and do not\nmake false assumptions about signed languages or\nthe needs of signing communities.\nBuilding\nCollaboration\nDeaf\ncollaborations\nand leadership are essential for developing signed\nlanguage technologies to ensure they address the\ncommunity’s needs and will be adopted, and that\nthey do not rely on misconceptions or inaccuracies\nabout signed language (Harris et al., 2009; Kusters\net al., 2017). Hearing researchers cannot relate to\nthe deaf experience or fully understand the con-\ntext in which the tools being developed would be\nused, nor can they speak for the deaf. Therefore,\nwe encourage the creation of a long-term collab-\norative environment between signed language re-\nsearchers and users, so that deaf users can identify\nmeaningful challenges, and provide insights on the\nconsiderations to take, while researchers cater to\nthe signers’ needs as the ﬁeld evolves. We also\nrecommend reaching out to signing communities\nfor reviewing papers on signed languages, to en-\nsure an adequate evaluation of this type of research\nresults published at ACL venues. There are sev-\neral ways to connect with Deaf communities for\ncollaboration: one can seek deaf students in their\nlocal community, reach out to schools for the deaf,\ncontact deaf linguists, join a network of researchers\nof sign-related technologies6, and/or participate in\ndeaf-led projects.\n6\nConclusions\nWe urge the inclusion of signed languages in\nNLP. We believe that the NLP community is well-\npositioned, especially with the plethora of success-\nful spoken language processing methods coupled\nwith the recent advent of computer vision tools\nfor videos, to bring the linguistic insight needed\nfor better signed language models. We hope to\nsee an increase in both the interests and efforts in\ncollecting signed language resources and develop-\ning signed language tools while building a strong\ncollaboration with signing communities.\nAcknowledgements\nWe would like to thank Marc Schulder, Claude\nMauk, David Mortensen, Chaitanya Ahuja, Sid-\ndharth Dalmia, Shruti Palaskar, Graham Neubig,\nand Becky Norton as well as the anonymous re-\nviewers for their helpful feedback and insightful\ndiscussions.\n6https://www.crest-network.com/\nReferences\nNikolas Adaloglou, Theocharis Chatzis, Ilias Papas-\ntratis, Andreas Stergioulas, Georgios Th Papadopou-\nlos, Vassia Zacharopoulou, George J Xydopou-\nlos, Klimnis Atzakas, Dimitris Papazachariou, and\nPetros Daras. 2020.\nA comprehensive study on\nsign language recognition methods. arXiv preprint\narXiv:2007.12530.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\nters, Joanna Power, Sam Skjonsberg, Lucy Wang,\nChris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\nand Oren Etzioni. 2018. Construction of the litera-\nture graph in semantic scholar. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 84–91, New Orleans - Louisiana. As-\nsociation for Computational Linguistics.\nNaveen Arivazhagan,\nAnkur Bapna,\nOrhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. arXiv preprint arXiv:1907.05019.\nTadas Baltruˇsaitis,\nChaitanya Ahuja,\nand Louis-\nPhilippe Morency. 2018. Multimodal machine learn-\ning:\nA survey and taxonomy.\nIEEE transac-\ntions on pattern analysis and machine intelligence,\n41(2):423–443.\nRobbin Battison. 1978. Lexical borrowing in American\nsign language. ERIC.\nUrsula Bellugi and Susan Fischer. 1972. A comparison\nof sign language and spoken language. Cognition,\n1(2-3):173–200.\nBrita Bergman. 1979.\nSigned Swedish.\nNational\nSwedish Board of Education [Skol¨overstyr.]:.\nLouise de Beuzeville. 2008. Pointing and verb modiﬁ-\ncation: the expression of semantic roles in the auslan\ncorpus. In Workshop Programme, page 13. Citeseer.\nSteven Bird. 2020.\nDecolonising speech and lan-\nguage technology. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 3504–3519, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nJulian Bleicken, Thomas Hanke, Uta Salden, and Sven\nWagner. 2016.\nUsing a language technology in-\nfrastructure for German in order to anonymize Ger-\nman Sign Language corpus data.\nIn Proceedings\nof the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 3303–\n3306, Portoroˇz, Slovenia. European Language Re-\nsources Association (ELRA).\nMark Borg and Kenneth P Camilleri. 2019. Sign lan-\nguage detection ”in the wild” with recurrent neu-\nral networks. In ICASSP 2019-2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 1637–1641. IEEE.\nDanielle Bragg, Oscar Koller, Mary Bellard, Lar-\nwan Berke, Patrick Boudreault, Annelies Braffort,\nNaomi Caselli, Matt Huenerfauth, Hernisa Ka-\ncorri, Tessa Verhoef, Christian Vogler, and Mered-\nith Ringel Morris. 2019. Sign language recognition,\ngeneration, and translation: An interdisciplinary per-\nspective.\nIn The 21st International ACM SIGAC-\nCESS Conference on Computers and Accessibility,\nASSETS ’19, page 16–31, New York, NY, USA. As-\nsociation for Computing Machinery.\nDiane Brentari. 2011. Sign language phonology. The\nhandbook of phonological theory, pages 691–721.\nDiane Brentari and Carol Padden. 2001. A language\nwith multiple origins: Native and foreign vocabu-\nlary in american sign language. Foreign vocabulary\nin sign language: A cross-linguistic investigation of\nword formation, pages 87–119.\nHannah Bull, Mich`ele Gouiff`es, and Annelies Braffort.\n2020.\nAutomatic segmentation of sign language\ninto subtitle-units. In European Conference on Com-\nputer Vision, pages 186–198. Springer.\nNecati Cihan Camg¨oz, Simon Hadﬁeld, Oscar Koller,\nHermann Ney, and Richard Bowden. 2018.\nNeu-\nral sign language translation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 7784–7793.\nNecati Cihan Camg¨oz, Oscar Koller, Simon Hadﬁeld,\nand Richard Bowden. 2020a. Multi-channel trans-\nformers for multi-articulatory sign language transla-\ntion. In European Conference on Computer Vision,\npages 301–319.\nNecati Cihan Camg¨oz, Oscar Koller, Simon Hadﬁeld,\nand Richard Bowden. 2020b. Sign language trans-\nformers: Joint end-to-end sign language recognition\nand translation.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10023–10033.\nZ. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and\nY. A. Sheikh. 2019.\nOpenpose: Realtime multi-\nperson 2d pose estimation using part afﬁnity ﬁelds.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence.\nYu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao\nLiu, and Jian Yang. 2017.\nAdversarial posenet:\nA structure-aware convolutional network for human\npose estimation.\nIn Proceedings of the IEEE In-\nternational Conference on Computer Vision, pages\n1212–1221.\nNecati Cihan Camg¨oz, Simon Hadﬁeld, Oscar Koller,\nHermann Ney, and Richard Bowden. 2018.\nNeu-\nral sign language translation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 7784–7793.\nKearsy Cormier, Sandra Smith, and Zed Sevcikova-\nSehyr. 2015. Rethinking constructed action. Sign\nLanguage & Linguistics, 18(2):167–204.\nOnno Crasborn, R Bank, I Zwitserlood, E Van der\nKooij, E Ormel, J Ros, A Sch¨uller, A de Meijer,\nM van Zuilen, YE Nauta, et al. 2016.\nNgt sign-\nbank.\nNijmegen: Radboud University, Centre for\nLanguage Studies.\nOnno A Crasborn and IEP Zwitserlood. 2008. The cor-\npus ngt: an online corpus for professionals and lay-\nmen.\nRunpeng Cui, Hu Liu, and Changshui Zhang. 2017.\nRecurrent convolutional neural networks for con-\ntinuous sign language recognition by staged opti-\nmization. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n7361–7369.\nNCSLGR Databases. 2007. Volumes 2–7.\nPhilippe Dreuw, Thomas Deselaers, Daniel Keysers,\nand Hermann Ney. 2006.\nModeling image vari-\nability in appearance-based gesture recognition. In\nECCV workshop on statistical methods in multi-\nimage and video processing, pages 7–18.\nPaul G Dudis. 2004. Body partitioning and real-space\nblends. Cognitive linguistics, 15(2):223–238.\nSarah Ebling,\nNecati Cihan Camg\n¨o z,\nPenny\nBoyes Braem, Katja Tissi, Sandra Sidler-Miserez,\nStephanie Stoll, Simon Hadﬁeld, Tobias Haug,\nRichard Bowden, Sandrine Tornay, Marzieh Razavi,\nand Mathew Magimai-Doss. 2018. SMILE Swiss\nGerman sign language dataset. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nMichael Erard. 2017. Why sign-language gloves don’t\nhelp deaf people. The Atlantic, 9.\nIva Farag and Heike Brock. 2019.\nLearning motion\ndisﬂuencies for automatic sign language segmenta-\ntion. In ICASSP 2019-2019 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7360–7364. IEEE.\nJordan Fenlon, Kearsy Cormier, and Adam Schembri.\n2015. Building bsl signbank: The lemma dilemma\nrevisited.\nInternational Journal of Lexicography,\n28(2):169–206.\nJordan Fenlon, Adam Schembri, and Kearsy Cormier.\n2018.\nModiﬁcation of indicating verbs in british\nsign language: A corpus-based study.\nLanguage,\n94(1):84–118.\nLucinda Ferreira-Brito. 1984.\nSimilarities & differ-\nences in two brazilian sign languages.\nSign lan-\nguage studies, 42:45–56.\nJens Forster, Christoph Schmidt, Oscar Koller, Martin\nBellgardt, and Hermann Ney. 2014. Extensions of\nthe sign language recognition and translation corpus\nrwth-phoenix-weather. In LREC, pages 1911–1916.\nBinyam Gebrekidan Gebre, Peter Wittenburg, and Tom\nHeskes. 2013. Automatic sign language identiﬁca-\ntion. In 2013 IEEE International Conference on Im-\nage Processing, pages 2626–2630. IEEE.\nNeil S Glickman and Wyatte C Hall. 2018. Language\ndeprivation and deaf mental health. Routledge.\nRıza Alp G¨uler, Natalia Neverova, and Iasonas Kokki-\nnos. 2018. Densepose: Dense human pose estima-\ntion in the wild. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition,\npages 7297–7306.\nEva Gutierrez-Sigut, Brendan Costello, Cristina Baus,\nand Manuel Carreiras. 2016.\nLse-sign: A lexical\ndatabase for spanish sign language.\nBehavior Re-\nsearch Methods, 48(1):123–137.\nWyatte C Hall, Leonard L Levin, and Melissa L An-\nderson. 2017. Language deprivation syndrome: A\npossible neurodevelopmental disorder with sociocul-\ntural origins. Social psychiatry and psychiatric epi-\ndemiology, 52(6):761–776.\nThomas Hanke, Marc Schulder, Reiner Konrad, and\nElena Jahn. 2020. Extending the Public DGS Cor-\npus in size and depth.\nIn Proceedings of the\nLREC2020 9th Workshop on the Representation and\nProcessing of Sign Languages: Sign Language Re-\nsources in the Service of the Language Commu-\nnity, Technological Challenges and Application Per-\nspectives, pages 75–82, Marseille, France. European\nLanguage Resources Association (ELRA).\nRaychelle Harris, Heidi M Holmes, and Donna M\nMertens. 2009.\nResearch ethics in sign language\ncommunities.\nSign Language Studies, 9(2):104–\n131.\nSaad Hassan, Larwan Berke, Elahe Vahdani, Long-\nlong Jing, Yingli Tian, and Matt Huenerfauth. 2020.\nAn isolated-signing RGBD dataset of 100 American\nSign Language signs produced by ﬂuent ASL sign-\ners. In Proceedings of the LREC2020 9th Workshop\non the Representation and Processing of Sign Lan-\nguages: Sign Language Resources in the Service\nof the Language Community, Technological Chal-\nlenges and Application Perspectives, pages 89–94,\nMarseille, France. European Language Resources\nAssociation (ELRA).\nJie Huang, Wengang Zhou, Qilin Zhang, Houqiang Li,\nand Weiping Li. 2018. Video-based sign language\nrecognition without temporal segmentation. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 32.\nTom Humphries, Poorna Kushalnagar, Gaurav Mathur,\nDonna Jo Napoli, Carol Padden, Christian Rath-\nmann, and Scott Smith. 2016.\nAvoiding linguis-\ntic neglect of deaf children. Social Service Review,\n90(4):589–619.\nAlfarabi Imashev, Medet Mukushev, Vadim Kimmel-\nman, and Anara Sandygulova. 2020.\nA dataset\nfor linguistic understanding, visual evaluation, and\nrecognition of sign languages: The k-rsl.\nIn Pro-\nceedings of the 24th Conference on Computational\nNatural Language Learning, pages 631–640.\nAmy Isard. 2020.\nApproaches to the anonymisation\nof sign language corpora.\nIn Proceedings of the\nLREC2020 9th Workshop on the Representation and\nProcessing of Sign Languages: Sign Language Re-\nsources in the Service of the Language Community,\nTechnological Challenges and Application Perspec-\ntives, pages 95–100.\nDavid L Jaffe. 1994.\nEvolution of mechanical ﬁn-\ngerspelling hands for people who are deaf-blind.\nJournal of rehabilitation research and development,\n31(3):236–244.\nRobert E Johnson and Scott K Liddell. 2011. Toward a\nphonetic representation of signs: Sequentiality and\ncontrast. Sign Language Studies, 11(2):241–274.\nTrevor Johnston. 2010. From archive to corpus: Tran-\nscription and annotation in the creation of signed lan-\nguage corpora. International journal of corpus lin-\nguistics, 15(1):106–131.\nTrevor Johnston and Louise De Beuzeville. 2016. Aus-\nlan corpus annotation guidelines. Auslan Corpus.\nTrevor Johnston and Adam Schembri. 2007.\nAus-\ntralian Sign Language (Auslan): An introduction\nto sign language linguistics. Cambridge University\nPress.\nTrevor Johnston and Adam C Schembri. 1999.\nOn\ndeﬁning lexeme in a signed language.\nSign lan-\nguage & linguistics, 2(2):115–185.\nJim Kakumasu. 1968. Urubu sign language. Interna-\ntional journal of American linguistics, 34(4):275–\n281.\nVadim Kimmelman. 2014. Information structure in rus-\nsian sign language and sign language of the nether-\nlands.\nSign Language & Linguistics, 18(1):142–\n150.\nSang-Ki Ko, Chang Jo Kim, Hyedong Jung, and\nChoongsang Cho. 2019. Neural sign language trans-\nlation based on human keypoint estimation. Applied\nSciences, 9(13):2683.\nOscar Koller. 2020. Quantitative survey of the state of\nthe art in sign language recognition. arXiv preprint\narXiv:2008.09918.\nReiner Konrad, Thomas Hanke, Gabriele Langer, Su-\nsanne K¨onig, Lutz K¨onig, Rie Nishio, and Anja Re-\ngen. 2018. Public dgs corpus: Annotation conven-\ntions. Technical report, Project Note AP03-2018-01,\nDGS-Korpus project, IDGS, Hamburg University.\nAnnelies Kusters, Maartje De Meulder, and Dai\nO’Brien. 2017. Innovations in deaf studies: The role\nof deaf scholars. Oxford University Press.\nDongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong\nLi. 2020. Word-level deep sign language recogni-\ntion from video: A new large-scale dataset and meth-\nods comparison. In The IEEE Winter Conference on\nApplications of Computer Vision, pages 1459–1469.\nScott K Liddell and Robert E Johnson. 1989. Amer-\nican sign language: The phonological base.\nSign\nlanguage studies, 64(1):195–277.\nScott K Liddell and Melanie Metzger. 1998. Gesture\nin sign language discourse. Journal of pragmatics,\n30(6):657–697.\nScott K Liddell et al. 2003.\nGrammar, gesture, and\nmeaning in American Sign Language. Cambridge\nUniversity Press.\nDavid McKee and Graeme Kennedy. 2000.\nLexi-\ncal comparison of signs from american, australian,\nbritish and new zealand sign languages. The signs\nof language revisited: An anthology to honor Ursula\nBellugi and Edward Klima, pages 49–76.\nJohanna Mesch and Lars Wallin. 2012. From meaning\nto signs and back: Lexicography and the swedish\nsign language corpus.\nIn Proceedings of the 5th\nWorkshop on the Representation and Processing of\nSign Languages: Interactions between Corpus and\nLexicon [Language Resources and Evaluation Con-\nference (LREC)], pages 123–126.\nJohanna Mesch and Lars Wallin. 2015.\nGloss anno-\ntations in the swedish sign language corpus. Inter-\nnational Journal of Corpus Linguistics, 20(1):102–\n120.\nCaio DD Monteiro, Christy Maria Mathew, Ricardo\nGutierrez-Osuna, and Frank Shipman. 2016. Detect-\ning and identifying sign languages through visual\nfeatures. In 2016 IEEE International Symposium on\nMultimedia (ISM), pages 287–290. IEEE.\nKathryn Montemurro and Diane Brentari. 2018. Em-\nphatic ﬁngerspelling as code-mixing in american\nsign language. Proceedings of the Linguistic Soci-\nety of America, 3(1):61–1.\nAmit\nMoryossef\nand\nYoav\nGoldberg.\n2021.\nSign\nlanguage\nprocessing.\nhttps://\nsign-language-processing.github.io/.\nAmit Moryossef, Ioannis Tsochantaridis, Roee Aha-\nroni, Sarah Ebling, and Srini Narayanan. 2020. Real-\ntime sign language detection using human pose esti-\nmation. In European Conference on Computer Vi-\nsion, pages 237–248. Springer.\nAmit Moryossef, Kayo Yin, Graham Neubig, and\nYoav Goldberg. 2021.\nData augmentation for\nsign language gloss translation.\narXiv preprint\narXiv:2105.07476.\nJoseph J Murray, Wyatte C Hall, and Kristin Snoddon.\n2020. The importance of signed languages for deaf\nchildren and their families.\nThe Hearing Journal,\n73(3):30–32.\nSylvie CW Ong and Surendra Ranganath. 2005. Auto-\nmatic sign language analysis: A survey and the fu-\nture beyond lexical meaning. IEEE Computer Archi-\ntecture Letters, 27(06):873–891.\nEllen Ormel and Onno Crasborn. 2012. Prosodic cor-\nrelates of sentences in signed languages: A litera-\nture review and suggestions for new types of studies.\nSign Language Studies, 12(2):279–315.\nC. Padden. 1988. Interaction of Morphology and Syn-\ntax in American Sign Language. Outstanding Disc\nLinguistics Series. Garland.\nCarol A Padden. 1998. The asl lexicon. Sign language\n& linguistics, 1(1):39–60.\nCarol A Padden and Tom Humphries. 1988. Deaf in\nAmerica. Harvard University Press.\nBecky Sue Parton. 2006.\nSign language recognition\nand translation: A multidisciplined approach from\nthe ﬁeld of artiﬁcial intelligence.\nJournal of deaf\nstudies and deaf education, 11(1):94–101.\nCarol J Patrie and Robert E Johnson. 2011.\nFinger-\nspelled word recognition through rapid serial visual\npresentation: RSVP. DawnSignPress.\nLeonid Pishchulin, Arjun Jain, Mykhaylo Andriluka,\nThorsten Thorm ¨a hlen, and Bernt Schiele. 2012.\nArticulated people detection and pose estimation:\nReshaping the future.\nIn 2012 IEEE Conference\non Computer Vision and Pattern Recognition, pages\n3178–3185. IEEE.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel\nSynnaeve, and Ronan Collobert. 2020.\nMLS: A\nLarge-Scale Multilingual Dataset for Speech Re-\nsearch.\nIn Proc. Interspeech 2020, pages 2757–\n2761.\nSiegmund Prillwitz and Heiko Zienert. 1990. Hamburg\nnotation system for sign language: Development of\na sign writing with computer application. In Current\ntrends in European Sign Language Research. Pro-\nceedings of the 3rd European Congress on Sign Lan-\nguage Research, pages 355–379.\nJavier Ramırez, Jos´e C Segura, Carmen Benıtez, Angel\nDe La Torre, and Antonio Rubio. 2004. Efﬁcient\nvoice activity detection algorithms using long-term\nspeech information. Speech communication, 42(3-\n4):271–287.\nRazieh Rastgoo, Kourosh Kiani, and Sergio Escalera.\n2020.\nSign language recognition: A deep survey.\nExpert Systems with Applications, page 113794.\nChristian Rathmann and Gaurav Mathur. 2011. A feat-\nural approach to verb agreement in signed languages.\nTheoretical Linguistics, 37(3-4):197–208.\nCynthia B Roy. 2011. Discourse in signed languages.\nGallaudet University Press.\nWendy Sandler. 2010.\nProsody and syntax in sign\nlanguages. Transactions of the philological society,\n108(3):298–328.\nWendy Sandler. 2012. The phonological organization\nof sign languages. Language and linguistics com-\npass, 6(3):162–182.\nPinar Santemiz, Oya Aran, Murat Saraclar, and Lale\nAkarun. 2009. Automatic sign segmentation from\ncontinuous signing via multiple sequence align-\nment. In 2009 IEEE 12th International Conference\non Computer Vision Workshops, ICCV Workshops,\npages 2001–2008. IEEE.\nBen Saunders, Necati Cihan Camg¨oz, and Richard\nBowden. 2020a.\nEverybody sign now: Translat-\ning spoken language to photo realistic sign language\nvideo. arXiv preprint arXiv:2011.09846.\nBen Saunders, Necati Cihan Camg¨oz, and Richard\nBowden. 2020b. Progressive transformers for end-\nto-end sign language production. In European Con-\nference on Computer Vision, pages 687–705.\nAdam Schembri, Kearsy Cormier, and Jordan Fenlon.\n2018. Indicating verbs as typologically unique con-\nstructions: Reconsidering verb ‘agreement’in sign\nlanguages. Glossa: a journal of general linguistics,\n3(1).\nB. Shi, A. Martinez Del Rio, J. Keane, D. Brentari,\nG. Shakhnarovich, and K. Livescu. 2019.\nFinger-\nspelling recognition in the wild with iterative visual\nattention. ICCV.\nB. Shi, A. Martinez Del Rio, J. Keane, J. Michaux,\nG. Shakhnarovich D. Brentari, and K. Livescu. 2018.\nAmerican sign language ﬁngerspelling recognition\nin the wild. SLT.\nEdgar H Shroyer and Susan P Shroyer. 1984. Signs\nacross America: A look at regional differences in\nAmerican Sign Language.\nGallaudet University\nPress.\nOzge Mercanoglu Sincan and Hacer Yalim Keles. 2020.\nAutsl: A large scale multi-modal turkish sign lan-\nguage dataset and baseline methods. IEEE Access,\n8:181340–181355.\nJongseo Sohn, Nam Soo Kim, and Wonyong Sung.\n1999.\nA statistical model-based voice activity de-\ntection. IEEE signal processing letters, 6(1):1–3.\nMarkus Steinbach and Edgar Onea. 2016. A DRT anal-\nysis of discourse referents and anaphora resolution\nin sign language. Journal of Semantics, 33(3):409–\n448.\nJr. Stokoe, William C. 1960. Sign Language Structure:\nAn Outline of the Visual Communication Systems\nof the American Deaf. The Journal of Deaf Studies\nand Deaf Education, 10(1):3–37.\nWilliam C Stokoe Jr. 2005. Sign language structure:\nAn outline of the visual communication systems of\nthe american deaf. Journal of deaf studies and deaf\neducation, 10(1):3–37.\nStephanie Stoll, Necati Cihan Camg¨oz, Simon Had-\nﬁeld, and Richard Bowden. 2018.\nSign language\nproduction using neural machine translation and gen-\nerative adversarial networks.\nIn Proceedings of\nthe 29th British Machine Vision Conference (BMVC\n2018). British Machine Vision Association.\nStephanie Stoll, Necati Cihan Camg¨oz, Simon Had-\nﬁeld, and Richard Bowden. 2020.\nText2sign: to-\nwards sign language production using neural ma-\nchine translation and generative adversarial net-\nworks.\nInternational Journal of Computer Vision,\npages 1–18.\nTed Supalla. 1986.\nThe classiﬁer system in ameri-\ncan sign language. Noun classes and categorization,\n7:181–214.\nValerie Sutton. 1990. Lessons in sign writing. Sign-\nWriting.\nHamid Vaezi Joze and Oscar Koller. 2019. Ms-asl: A\nlarge-scale data set and benchmark for understand-\ning american sign language. In The British Machine\nVision Conference (BMVC).\nSherman Wilcox. 1992.\nThe phonetics of ﬁnger-\nspelling, volume 4. John Benjamins Publishing.\nSherman Wilcox and Sarah Hafer. 2004. Rethinking\nclassiﬁers. emmorey, k.(ed.).(2003). perspectives on\nclassiﬁer constructions in sign languages. mahwah,\nnj: Lawrence erlbaum associates. 332 pages. hard-\ncover.\nQinkun Xiao, Minying Qin, and Yuting Yin. 2020.\nSkeleton-based chinese sign language recognition\nand generation for bidirectional communication be-\ntween deaf and hearing people. Neural Networks,\n125:41–55.\nKayo Yin and Jesse Read. 2020a. Attention is all you\nsign: sign language translation with transformers.\nIn Sign Language Recognition, Translation and Pro-\nduction (SLRTP) Workshop-Extended Abstracts, vol-\nume 4.\nKayo Yin and Jesse Read. 2020b. Better sign language\ntranslation with STMC-transformer.\nIn Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5975–5989, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nJan Zelinka and Jakub Kanis. 2020. Neural sign lan-\nguage synthesis: Words are our glosses.\nIn The\nIEEE Winter Conference on Applications of Com-\nputer Vision, pages 3395–3403.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-05-11",
  "updated": "2021-07-22"
}