{
  "id": "http://arxiv.org/abs/2105.06868v3",
  "title": "Priors in Bayesian Deep Learning: A Review",
  "authors": [
    "Vincent Fortuin"
  ],
  "abstract": "While the choice of prior is one of the most critical parts of the Bayesian\ninference workflow, recent Bayesian deep learning models have often fallen back\non vague priors, such as standard Gaussians. In this review, we highlight the\nimportance of prior choices for Bayesian deep learning and present an overview\nof different priors that have been proposed for (deep) Gaussian processes,\nvariational autoencoders, and Bayesian neural networks. We also outline\ndifferent methods of learning priors for these models from data. We hope to\nmotivate practitioners in Bayesian deep learning to think more carefully about\nthe prior specification for their models and to provide them with some\ninspiration in this regard.",
  "text": "arXiv:2105.06868v3  [stat.ML]  18 Mar 2022\nPRIORS IN BAYESIAN DEEP LEARNING: A REVIEW\nVincent Fortuin\nDepartment of Computer Science\nETH Zürich\nZürich, Switzerland\nfortuin@inf.ethz.ch\nABSTRACT\nWhile the choice of prior is one of the most critical parts of the Bayesian infer-\nence workﬂow, recent Bayesian deep learning models have often fallen back on\nvague priors, such as standard Gaussians. In this review, we highlight the im-\nportance of prior choices for Bayesian deep learning and present an overview of\ndifferent priors that have been proposed for (deep) Gaussian processes, variational\nautoencoders, and Bayesian neural networks. We also outline different methods\nof learning priors for these models from data. We hope to motivate practitioners\nin Bayesian deep learning to think more carefully about the prior speciﬁcation for\ntheir models and to provide them with some inspiration in this regard.\n1\nIntroduction\nBayesian models have gained a stable popularity in data analysis [1] and machine learning [2]. Es-\npecially in recent years, the interest in combining these models with deep learning has surged1. The\nmain idea of Bayesian modeling is to infer a posterior distribution over the parameters θ of the\nmodel given some observed data D using Bayes’ theorem [3, 4] as\np(θ | D) = p(D | θ) p(θ)\np(D)\n=\np(D | θ) p(θ)\nR\np(D | θ) p(θ) dθ\n(1)\nwhere p(D|θ) is the likelihood, p(D) is the marginal likelihood (or evidence), and p(θ) is the prior.\nThe prior can often be parameterized by hyperparametersψ, in which case we will write it as p(θ; ψ)\nif we want to highlight this dependence. This posterior can then be used to model new unseen data\nD∗using the posterior predictive\np(D∗| D) =\nZ\np(D∗| θ) p(θ | D) dθ\n(2)\nThe integral in Eq. (2) is also called the Bayesian model average, because it averages the predic-\ntions of all plausible models weighted by their posterior probability. This is in contrast to standard\nmaximum-likelihood learning, where only one parameter θ∗is used for the predictions as\np(D∗| D) ≈p(D∗| θ∗)\nwith\nθ∗= arg max\nθ\np(D | θ)\n(3)\nWhile much previous work has focused on the properties of the posterior predictive [5, 6], the\napproximation of the integrals in Eq. (1) and Eq. (2) [7–9], or the use of the marginal likelihood for\nBayesian model selection [10, 11], in this thesis we want to shed some light on the often-neglected\nterm in Eq. (1): the prior p(θ).\nIn orthodox Bayesianism, the prior should be chosen in a way such that it accurately reﬂects our\nbeliefs about the parameters θ before seeing any data [12]. This has been described as being the most\n1As attested, for instance, by the growing interest in the Bayesian Deep Learning workshop at NeurIPS.\ncrucial part of Bayesian model building, but also the hardest one, since it is often not trivial to map\nthe subjective beliefs of the practitioner unambiguously onto tractable probability distributions [13].\nHowever, in practice, choosing the prior is often rather seen as a nuisance, and there have been many\nattempts to try to avoid having to choose a meaningful prior, for instance, through objective priors\n[14, 15], reference priors [16], empirical Bayes [17], or combinations of these [18]. One problem\nwith these methods is that in Bayesian deep learning, they are often not tractable due to the high\ndimensionality of the inference problem, since they either require computing the Fisher information\nmatrix, solving a series of increasingly high-dimensional integrals, or splitting the model parameters\ninto “parameters of interest” and “nuisance parameters” [16]. Especially in Bayesian deep learning,\nit is therefore common practice to choose a (seemingly) “uninformative” prior, such as a standard\nGaussian [c.f., 19].\nThis trend is troubling, because choosing a bad prior can have detrimental consequences for the\nwhole inference endeavor. While the choice of uninformative (or weakly informative) priors is often\nbeing motivated by invocation of the asymptotic consistency guarantees of the Bernstein-von-Mises\ntheorem [20], this theorem does not in fact hold in many applications, since its regularity conditions\nare not satisﬁed [21]. Moreover, in the non-asymptotic regime of our practical inferences, especially\nin high-dimensional settings, the prior can have a strong inﬂuence on the posterior, often forcing\nthe probability mass onto arbitrary subspaces of the parameter space [22]. This means, for instance,\nthat the seemingly innocuous standard Gaussian prior is not uninformative at all [23], but forces the\nposterior mass onto a thin spherical subspace, which in most cases does not reﬂect any useful prior\nknowledge and can severely bias the inference [24, 25].\nWorse yet, prior misspeciﬁcation can undermine the very properties that compel us to use Bayesian\ninference in the ﬁrst place. For instance, marginal likelihoods can become meaningless under prior\nmisspeciﬁcation, leading us to choose suboptimal models when using Bayesian model selection\n[26]. Moreover, de Finetti’s famous Dutch book argument [27] can be extended to cases where we\ncan be convinced to take wagers that lose money in expectation when using bad priors, which even\nholds for the aforementioned objective (Jeffreys) priors [28]. In a similar vein, Savage’s theorem\n[29], which promises us optimal decisions under Bayesian decision theory, breaks down under prior\nmisspeciﬁcation [30]. Finally, it can even be shown that PAC-Bayesian inference can exceed the\nBayesian one in terms of generalization performance when the prior is misspeciﬁed [31, 32].\nOn a more optimistic note, the no-free-lunch theorem [33] states that no learning algorithm is univer-\nsally superior, or in other words, that different learning algorithms outperform each other on different\ndatasets. Applied to Bayesian learning, this means that there is also no universally preferred prior,\nbut that each task is potentially endowed with its own optimal prior. Finding (or at least approxi-\nmating) this optimal prior then offers the potential for signiﬁcantly improving the performance of\nthe inference or even enabling successful inference in cases where it otherwise would not have been\npossible.\nAll these observations should at least motivate us to think a bit more carefully about our priors\nthan is often done in practice. But do we really have reason to believe that the commonly used\npriors in Bayesian deep learning are misspeciﬁed? One recent piece of evidence is the fact that in\nBayesian linear models, it can be shown that prior misspeciﬁcation leads to the necessity to temper\nthe posterior for optimal performance (i.e., use a posterior pT (θ | D) ∝p(θ | D)1/T for some T < 1)\n[34]. And indeed, this need for posterior tempering has also been observed empirically in modern\nBayesian deep learning models [e.g., 35–37, 19].\nBased on all these insights, it is thus high time that we critically reﬂect upon our choices of priors\nin Bayesian deep learning models. Luckily for us, there are many alternative priors that we could\nchoose over the standard uninformative ones. This survey shall attempt to provide an overview\nof them. We will review existing prior designs for (deep) Gaussian processes in Section 2, for\nvariational autoencoders in Section 3, and for Bayesian neural networks in Section 4. We will then\nﬁnish by giving some brief outline of methods for learning priors from data in Section 5.\n2\nPriors in (Deep) Gaussian Processes\nGaussian processes (GPs) have a long history in Bayesian machine learning and enjoy many useful\nproperties [38, 39]. They are nonparametric models, which means that we are not actually specifying\na prior over parameters p(θ), but instead a prior over functions p(f). This prior can also have\n2\nhyperparameters ψ, which parameterize a mean function mψ and a kernel function kψ as\np(f; ψ) = GP (mψ(·), kψ(·, ·))\n(4)\nThis prior is called a Gaussian process because it has the property that when evaluating the function\nat any ﬁnite set of points x, the function values f := f(x) are distributed as p(f) = N(mx, Kxx),\nwhere mx = mψ(x) is the vector of mean function outputs, the (i, j)’th element of the kernel\nmatrix Kxx is given by kψ(xi, xj), and the d-dimensional multivariate Gaussian N(f; µ, Σ) is\np(f) = N(µ, Σ) :=\n1\np\n(2π)d det Σ\nexp\n\u0012\n−1\n2(f −µ)⊤Σ−1(f −µ)\n\u0013\n(5)\nThe Gaussian process can also be seen as an inﬁnite-dimensional version of this multivariate Gaus-\nsian distribution, following the Kolmogorov extension theorem [40].\nThis model is often combined with a Gaussian observation likelihood p(y | f) = N(f, σ2I), since\nit then allows for a closed-form posterior inference [39] on unseen data points (x∗, y∗) as\np(y∗| x∗, x, y) = N(m∗, K∗)\nwith\n(6)\nm∗= mx∗+ Kx∗x\n\u0000Kxx + σ2I\n\u0001−1 (y −mx)\nK∗= Kx∗x∗−Kx∗x\n\u0000Kxx + σ2I\n\u0001−1 Kxx∗+ σ2I\nWhile these models are not deep per se, there are many ways in which they connect to Bayesian\ndeep learning, which merits their appearance in this thesis. In the following, we are going to present\nhow GP priors can be parameterized by deep neural networks (Section 2.1), how GPs can be stacked\nto build deeper models (Section 2.2), and how deep neural networks can themselves turn into GPs\nor be approximated by GPs (Section 2.3).\n2.1\nGaussian processes parameterized by deep neural networks\nAs mentioned above, the GP prior is determined by the parameterized functions mψ and kψ. It will\ncome as no surprise that these functions can be chosen to be deep neural networks (DNNs). In the\ncase of deep kernels [41–43], however, one has to proceed with care, since most neural network\nfunctions will not actually yield proper kernels. One option to get a kernel out of a neural network\nis to use the last layer of the network as a feature space and deﬁne the kernel to be the inner product\nin this space, that is, kψ(x, x′) = ⟨φ(x; ψ), φ(x′; ψ)⟩, where φ(· ; ψ) is the neural network with\nparameters ψ and ⟨·, ·⟩is the inner product. This actually leads to a Bayesian linear regression in\nthe feature space of the network and is also sometimes called a Bayesian last layer (BLL) model\n[44–49].\nAnother option to develop deep kernels is to start with a base kernel kbase(·, ·), for instance a radial\nbasis function (RBF) kernel kRBF(x, x′) = exp(−λ(x −x′)2) with lengthscale λ. This kernel can\nthen be applied in the DNN feature space, yielding the kernel\nkψ(x, x′) = kbase(φ(x; ψ), φ(x′; ψ))\n(7)\nIf one chooses the linear kernel klin(x, x′) = ⟨x, x′⟩as the base kernel, this reduces to the BLL\nmodel above. However, when choosing a kernel like the RBF, this model still yields an inﬁnite-\ndimensional reproducing kernel Hilbert space (RKHS) and thus offers a full GP that does not reduce\nto a ﬁnite Bayesian linear regression. These approaches can not only lead to very expressive models,\nbut have also been shown to improve properties such as adversarial robustness [50].\nWhen using deep mean functions [51, 52] instead of (or in combination with) deep kernels, less pre-\ncautions have to be taken, since virtually any function is a valid GP mean function. Thus, the neural\nnetwork can simply be used as the mean function itself, as mψ(x) = φ(x; ψ). Moreover, deep mean\nfunctions in GPs have been related to other popular learning paradigms, such as functional PCA [52].\nHowever, the main problem with these, as with the deep kernels above, is the question how to choose\nthem. Since DNNs are notoriously hard to interpret, choosing their parameters truly a priori, that\nis, before seeing any data, seems like an impossible task. These approaches are thus usually used\nin combination with some additional learning algorithms, which set their parameters based on some\nobjective function. We will provide further details on these algorithms in Section 5. As an additional\nsidenote, we would also like to mention a speciﬁc type of GP kernel, namely the convolutional ker-\nnel [53], which is not itself parameterized by a neural networks, but inspired by convolutional neural\nnetworks (CNNs) in its construction, leading to improved performance on images.\n3\n2.2\nDeep Gaussian processes\nWhile GPs can be combined with deep neural networks, as we saw in the previous section, they can\nalso be used to construct deep models in their own right. This is done by adding k additional la-\ntent functions {f1, . . . , fk} with function outputs {f1, . . . , fk} and latent variables {z1, . . . , zk−1},\nwhere each function uses the previous latent variable as its inputs, that is, fi+1 = fi+1(zi) and\nf1 = f1(x).\nIn the simplest case, all these latent GPs still have Gaussian latent likelihoods\np(zi | fi) = N(fi, σ2\ni I) and a Gaussian output likelihood p(y | fk) = N(fk, σ2\nkI). If each of\nthese functions is endowed with a GP prior p(fi) = GP(mψi(·), kψi(·, ·)), this model is called a\ndeep Gaussian process [54]. Similarly to deep neural networks, these models can represent increas-\ningly complex distributions with increasing depth, but unlike neural networks, they still offer a fully\nBayesian treatment. Crucially, in contrast to standard GPs, deep GPs can model a larger class of out-\nput distributions [55], which includes distributions with non-Gaussian marginals [56]. For increased\nﬂexibility, these models can also be coupled with warping functions between the GP layers [57].\nWhile these models seem to be strictly superior and preferable to standard GPs, their additional\nﬂexibility comes at a price: the posterior inference is not tractable in closed form anymore. This\nmeans that the posterior has to be estimated using approximate inference techniques, such as varia-\ntional inference [54, 58], expectation propagation [59], or amortized inference [60]. A very popular\napproximate inference technique for GPs is based on so-called inducing points, which are chosen\nto be a subset of the training points or generally of the training domain [61–65]. This technique\ncan also be extended to inference in deep GPs [54, 66] or replaced by variational random features\n[67]. Moreover, it has recently been shown that for certain choices of kernels, neural networks can\nbe trained as point estimates for deep GP posteriors [68].\nIn contrast to the inference techniques, the choice of priors for deep GPs has generally been under-\nstudied. While a deep GP as a whole can model a rather complex prior over functions, the priors\nfor the single layers in terms of mψi and kψi are often chosen to be quite simple, for instance, RBF\nkernels with different lengthscales [54]. An exception to this are combinations of deep GPs with\nthe convolutional GP kernels mentioned above, which yield models that are similar in spirit to deep\nCNNs [69–71]. Moreover, recent software packages for deep GP inference have made it easier to\nexperiment with different priors [72, 73]. One can thus be carefully optimistic that research into\nbetter deep GP priors will blossom in the years to come.\nAnother option to build models with more expressive kernels is to actually parameterize the ker-\nnel of a GP with another GP [74, 75].\nParticularly, the (hierarchical) prior is then p(f) =\nGP(mψ(·), ˆk(·, ·)) with ˆk(x, x′) = FT−1(exp s(x −x′)) and p(s) = GP(0, kψ(·, ·)), where FT−1\nis the inverse Fourier transform. This can also be seen as a deep GP with one hidden layer, and it\nalso does not allow closed-form inference, but relies on approximate inference, for instance, using\nelliptical slice sampling [75]. Finally, one can also achieve a similarly expressive model, at lower\ncomputational cost, by transforming the GP using a normalizing ﬂow [76], which generalizes the\nidea of a copula process [77].\n2.3\nGaussian process limits of neural networks\nAnother way to connect GPs to DNNs is via neural network limits. It has been known for some\ntime now that the function-space prior p(f) induced by a Bayesian neural network (BNN) with a\nsingle hidden layer and any independent ﬁnite-variance parameter prior p(θ) converges in the limit\nof inﬁnite width to a GP, due to the central limit theorem [78, 79]. The limiting GP prior is given by\np(f) = GP(0, kNN(·, ·))\nwith\nkNN(x, x′) = σ2\nw2Ew,b\n\u0002\nϕ(w⊤x + b) ϕ(w⊤x′ + b)\n\u0003\n+ σ2\nb2\nwhere\n(8)\nw ∼N(0, σ2\nw1I)\nand\nb ∼N(0, σ2\nb1I)\nwith the prior weight and bias variances σ2\nw1, σ2\nb1 in the ﬁrst layer, σ2\nw2, σ2\nb2 in the second layer, and\nnonlinear activation function ϕ(·). Note that here it is usually assumed that the weight variances are\nset as σ2\nwi ∝1/ni, where ni is the number of units in the i’th layer. The kernel kNN(·, ·) is then\ncalled the neural network GP (NNGP) kernel. This result has recently been extended to BNNs with\nReLU activations [80] and deep BNNs [81–83], where the lower layer GP kernel takes the same\n4\nform as above and the kernel for the higher layers assumes the recursive form\nkℓ\nNN(x, x′) = σ2\nwℓE(z1,z2)∼N(0,Kℓ−1\nxx′ ) [ϕ(z1) ϕ(z2)] + σ2\nbℓ\n(9)\nwith Kℓ−1\nxx′ being the 2 × 2 kernel matrix of the (ℓ−1)’th layer kernel evaluated at x and x′.\nMoreover, these convergence results can also be shown for convolutional BNNs [84, 85], even with\nweight correlations [86], and for attention neural networks [87].\nWhile these results only hold for independent ﬁnite-variance priors, they can be extended to de-\npendent priors, where they yield GPs that are marginalized over a hyperprior [88], and to inﬁnite-\nvariance priors, where they lead to α-stable processes [89]. Excitingly, it has been shown that this\nconvergence of the BNN prior to a stochastic process also implies the convergence of the posterior\nunder mild regularity assumptions [90]. While these results have typically been derived manually,\nthe recent theoretical framework of tensor programs allows to rederive them in a uniﬁed way, in-\ncluding for recurrent architectures and batch-normalization [91–93]. Moreover, it allows to derive\nlimits for networks where only a subset of the layers converge to inﬁnite width, which recovers the\nmodels’ ability to learn latent features [94].\nNot only inﬁnitely wide BNNs can lead to GP limits, but this is also true for inﬁnitely wide standard\nDNNs. Crucially however, in this case, the GP arises not as a function-space prior at initialization,\nbut as a model of training under gradient descent [95, 96]. Speciﬁcally, neural networks under\ngradient descent training can be shown to follow the kernel gradient of their functional loss with\nrespect to the so-called neural tangent kernel (NTK) which is\nkNTK(x, x′) = Jθ(x)Jθ(x′)⊤\n(10)\nwhere Jθ(x) is the Jacobian of the neural network with respect to the parameters θ evaluated at\ninput x. In the limit of inﬁnite width, this kernel becomes stable over training and can be recursively\ncomputed as\nkℓ\nNTK(x, x′) = kℓ−1\nNTK(x, x′) ˙Σℓ(x, x′) + Σℓ(x, x′)\nwith\n(11)\nk1\nNTK(x, x′) = Σ1(x, x′) = 1\nn0\nx⊤x′ + 1\nΣℓ(x, x′) = E(z1,z2)∼N(0,Kℓ−1\nxx′ ) [ϕ(z1) ϕ(z2)]\n˙Σℓ(x, x′) = E(z1,z2)∼N(0,Kℓ−1\nxx′ ) [ ˙ϕ(z1) ˙ϕ(z2)]\nwhere n0 is the number of inputs, Kℓ−1\nxx′ is again the kernel matrix of the NTK at the previous layer,\n˙ϕ(·) is the derivative of the activation function, and the Σℓare so-called activation kernels.\nIn the case of ﬁnite width, this kernel will not model the training behavior exactly, but there exist\napproximate corrections [97]. Interestingly, this same kernel can also be derived from approximate\ninference in the neural network, leading to an implicit linearization [98]. This linearization can\nalso be made explicit and can then be used to improve the performance of BNN predictives [99]\nand for fast domain adaptation in multi-task learning [100]. Moreover, when using the NTK in a\nkernel machine, such as a support vector machine, it can outperform the original neural network it\nwas derived from, at least in the small-data regime [101]. Similarly to the aforementioned NNGP\nkernels, the NTKs for different architectures can also be rederived using the framework of tensor\nprograms [91, 102] and there exist practical Python packages for the efﬁcient computation of NNGP\nkernels and NTKs [103]. Finally, it should be noted that this linearization of neural networks has\nalso been linked to the scaling of the parameters and described as lazy training, which has been\nargued to be inferior to standard neural network training [104].\n3\nPriors in Variational Autoencoders\nMoving on from GPs, another popular class of Bayesian deep learning model is the variational\nautoencoder (VAE) [105, 106]. VAEs are Bayesian latent variable models which assume a generative\nprocess in which the observations x are generated from unobserved latent variables z through a\nlikelihood p(x | z). In the case of VAEs, this likelihood is parameterized by a neural network which\nis trained on the observed data. Since the nonlinearity of this neural network renders exact inference\non the posterior p(z | x) intractable, it is approximated with a variational approximation q(z | x),\n5\nwhich is typically also parameterized by a neural network. The whole model is then trained by\noptimizing the evidence lower bound (ELBO)\nL(x, ϑ) = Ez∼qϑ(z | x) [log pϑ(x | z)] −DKL(qϑ(z | x) ∥p(z)) ≤p(x)\n(12)\nwhere ϑ are the parameters of the likelihood and inference network and DKL(q ∥p)\n=\nEq [log q −log p] is the Kullback-Leibler divergence. In practice, evaluating this term requires tak-\ning training examples x, then computing qϑ(z | x) and sampling a z, and then computing pϑ(x | z)\nand sampling an x again. This is reminiscent of an autoencoder, where qϑ is the encoder and pϑ is\nthe decoder, hence the name of the model.\nThe likelihood and approximate posterior are usually chosen to be Gaussian. While the prior is\ntypically also chosen to be standard Gaussian, that is p(z) = N(0, I), there are many other possible\nchoices, which we will explore in the following. Particularly, we will look at some proper probability\ndistributions that can directly replace the standard Gaussian (Section 3.1), at some structural priors\nthat also require changes to the architecture (Section 3.2), and ﬁnally at a particularly interesting\nVAE model with idiosyncratic architecture and prior, namely the neural process (Section 3.3).\n3.1\nDistributional VAE priors\nWe will use the term distributional priors to refer to distributions p(z) that can be plugged into the\nstandard VAE architecture described above without changing the rest of the model. However, note\nthat often it can be beneﬁcial to also change the functional form of the variational posterior q(z | x)\nto ﬁt the prior better. The ﬁrst type of prior that has been shown to yield some beneﬁts compared to\nthe standard Gaussian one is a spherical prior, namely a von-Mises-Fisher (vMF) prior [107], that is\np(z) =\nκd/2−1\n(2π)d/2 Id/2−1(κ) exp(κµ⊤z)\n(13)\nfor a d-dimensional latent space, where µ is the mean, κ is a concentration parameter, and Ik is\nthe modiﬁed Bessel function of the ﬁrst kind of order k. This distribution can be seen as a version\nof the multivariate Gaussian distribution that is supported on the hypersphere. However, its main\ndisadvantage is that the modiﬁed Bessel function cannot generally be computed in closed form and\nthus has to be approximated numerically.\nThese hyperspherical priors have been shown to improve VAE performance on benchmark data over\nstandard Gaussian ones, however mostly in low-dimensional latent spaces (up to d ≈20) [107].\nThis could be due to the Gaussian annulus theorem [108, Thm. 2.9], which states that the measure\nof a multivariate Gaussian in high dimensions concentrates on a hypersphere anyway. For higher-\ndimensional latent spaces, it has thus been proposed to replace the vMF prior with a product of\nlower-dimensional vMF distributions [109].\nTo overcome the numerical issues of the modiﬁed Bessel functions, the power-spherical distribution\nhas been suggested as a replacement for the vMF [110]. Its d-dimensional density is given by\np(z) =\n \n2κ+d−1π(d−1)/2 Γ(κ + d−1\n2 )\nΓ(κ + d −1)\n!−1 \u00001 + µ⊤z\n\u0001κ\n(14)\nwhere µ is again the mean, κ the concentration parameter, and Γ(·) is the Gamma function. Since\nthe Gamma function is easier to evaluate than the modiﬁed Bessel function, this density allows for\nclosed-form evaluation and reparameterizable sampling. Empirically, it yields the same performance\nin VAEs as the vMF prior, while being numerically more stable [110].\nAnother type of priors are mixture priors [111–113], typically mixtures of Gaussian of the form\np(z) =\nK\nX\ni=1\nπi N(µi, σ2\ni I)\nwith\nK\nX\ni=1\nπi = 1\n(15)\nwith K mixture components where πi are the mixture weights that are often set to πi = 1/K\nin the prior. These priors have been motivated by the idea that the data might consist of clusters,\nwhich should also be disjoint in the latent space [111], and they have been shown to outperform\nmany other clustering methods on challenging datasets [113]. However, similarly to many other\n6\nclustering methods, one challenge is to choose the number of clusters K a priori. This can also be\noptimized automatically, for instance by specifying a stick-breaking or Dirichlet process hyperprior\n[114], albeit at the cost of more involved inference.\nFinally, most of these priors assume independence between data points. If we have prior knowledge\nabout potential similarity between data points and we can encode it into a kernel function, a Gaussian\nprocess can be a powerful prior for a VAE [115–117]. The prior is usually deﬁned as\np(Z) = N(0, Kzz)\n(16)\nwhere Z = (z1, . . . , zn) is the matrix of latent variables and Kzz is again the kernel matrix with\n(i, j)’th element k(zi, zj) for some suitable kernel function k(·, ·). These models have been shown\nto excel at conditional generation [115], time series modeling [117], missing data imputation [116],\nand disentanglement [118, 119]. It should be noted that this comes at additional computational cost\ncompared to standard VAEs, since it requires the O(n3) inversion of the kernel matrix (see Eq. (6)).\nHowever, this operation can be made more scalable, either through the use of inducing point methods\n[120, 121] (c.f., Section 2.2) or through factorized kernels [122]. Moreover, depending on the prior\nknowledge of the generative process, these models can also be extended to use additive GP priors\n[123] or tensor-valued ones [124].\n3.2\nStructural VAE priors\nIn contrast to the distributional priors discussed above, we will use the term structural priors to\nrefer to priors that do not only change the actual prior distribution p(z) in the VAE model, but also\nthe model architecture itself. Some of these structural priors are extensions of the distributional\npriors mentioned above. For instance, the aforementioned Gaussian mixture priors can be extended\nwith a mixture-of-experts decoder, that is, a factorized generative likelihood, where each factor only\ndepends on one of the latent mixture components [113]. Another example are the Gaussian process\npriors, which are deﬁned over the whole latent dataset Z and thus beneﬁt from a modiﬁed encoder\n(i.e., inference network), which encodes the complete dataset X jointly [116].\nIn addition to these distributional priors with modiﬁed architectures, there are also structural priors\nwhich could not be realized with the standard VAE architecture. One example are hierarchical priors\n[125–127], such as\np(z1, . . . , zK) = p(z1)\nK\nY\ni=2\np(zi | zi−1)\n(17)\nor\np(z1, . . . , zK) = p(z1)\nK\nY\ni=2\np(zi | z1, . . . , zi−1)\n(18)\nWe see here that instead of having a single latent variable z, these models feature K different latent\nvariables {zi, . . . , zK}, which depend on each other hierarchically. These models require additional\ngenerative networks to parameterize the conditional probabilities in Eq. (17) or Eq. (18), which then\nenable them to better model data with intrinsically hierarchical features [125, 126] and to reach\nstate-of-the-art performance in image generation with VAEs [127].\nAnother type of structural priors are discrete latent priors, such as the VQ-VAE prior [128]\np(zq) =\n1\n|E|\nwith\nzq = arg min\ne∈E\n∥ze −e∥2\n2\n(19)\nwhere E is a ﬁnite dictionary of prototypes and ze is a continuous latent variable that is then dis-\ncretized to zq. Crucially, the prior is not placed over the continuous ze, but over the discrete\nzq, namely as a uniform prior over the dictionary E. These discrete latent variables can then be\nsaved very cheaply and thus lead to much stronger compression than standard VAEs [128]. When\ncombining these models with the hierarchical latent variables described above, they can also reach\ncompetitive image generation performance [129]. Moreover, these discrete latent variables can be\nextended to include neighborhood structures such as self-organizing maps [130], leading to more\ninterpretable latent representations that can also be used for clustering [131–133]. Finally, similar\ntopological priors can also be induced on continuous latent variables using ideas from persistent\nhomology [134, 135].\n7\n3.3\nNeural processes\nTo conclude this section, we will look at a structural VAE prior that has spawned a lot of inter-\nest in recent years and thus deserves its own subsection: the neural process (NP). This model has\nbeen independently proposed under the names of partialVAE [136] and (conditional) neural process\n[137, 138], but the latter nomenclature has caught on in the literature. The main novelty of this VAE\narchitecture is that it not only models the distribution of one type of observed variable x, but of\ntwo variables (x, y), which can be split into a context and target set (x, y) = (xc, yc) ∩(xt, yt).\nThese sets are conditionally independent given z, that is, p(x, y | z) = p(xc, yc | z) p(xt, yt | z).\nThis then allows to infer an unobserved yt based on the other variables using a variational approx-\nimation q(z | xc, yc) and the conditional likelihood p(yt | z, xt). Thus, the model can be used for\nmissing data imputation [136] and regression [138] tasks. Note that, since the likelihood is typically\nconditioned on xt instead of just on z, this model can be framed as a conditional VAE [139].\nOne remarkable feature of this model is the used prior, which is namely\np(z) = p(z | xc, yc) ≈q(z | xc, yc)\n(20)\nThis means that instead of using an unconditional prior p(z) for the full posterior p(z | x, y), a\npart of the data (the context set) is used to condition the prior, which is in turn approximated by\nthe variational posterior with reduced conditioning set. While this is atypical for classical Bayesian\ninference and generally frowned upon by orthodox Bayesians, it bears resemblence to the data-\ndependent oracle priors that can be used in PAC-Bayesian bounds and have been shown to make\nthose bounds tighter [140, 141].\nThe NP model has been heavily inspired by stochastic processes (hence the name) and has been\nshown to constitute a stochastic process itself under some assumptions [138]. Moreover, when the\nconditional likelihood p(yt | z, xt) is chosen to be an afﬁne transformation, the model is actually\nequivalent to a Gaussian process with neural network kernel [142].\nSince their inception, NP models have been extended in expressivity in different ways, both in\nterms of their inference and their generative model. On the inference side, there are attentive NPs\n[143], which endow the encoder with self-attention (and thus make it Turing complete [144]), and\nconvolutional (conditional) NPs [145, 146], which add translation equivariance to the model. On the\ngenerative side, there are functional NPs [147], which introduce dependence between the predictions\nby learning a relational graph structure over the latents z, and Gaussian NPs [148], which achieve a\nsimilar property by replacing the generative likelihood with a Gaussian process, the mean and kernel\nof which are inferred based on the latents.\n4\nPriors in Bayesian Neural Networks\nBayesian neural networks [149, 78] are neural network models in which the parameters are de-\ntermined through Bayesian inference (see Eq. (1)) and predictions are made using the posterior\npredictive (see Eq. (2)). These models have gained increasing popularity in recent years [150],\nmostly due to their uncertainty calibration properties [6]. While many different priors have been\nproposed for these models [e.g., 151, and references therein], it has often been argued that standard\nGaussian priors over the parameters are sufﬁcient and that the modeler’s inductive biases should\nbe represented through the choice of architecture instead [152]. This view had been supported by\npreliminary studies on small networks and simple problems that did not ﬁnd conclusive evidence for\nthe misspeciﬁcation of Gaussian priors [153].\nHowever, in recent work, the adequacy of Gaussian priors has been put into question, particularly by\nthe discovery that Gaussian priors can cause a cold posterior effect [37] which is not caused by some\nother priors [19]. Following the general considerations regarding prior misspeciﬁcation (see above),\nit thus seems advisable to also consider alternative priors for BNNs. In the following, we will review\npriors deﬁned in the weight space (Section 4.1) and in the function-space (Section 4.2) and will also\nshow how to extend these ideas to (Bayesian) ensembles of neural networks (Section 4.3).\n4.1\nWeight-space priors\nAs mentioned before, the most widely used priors for BNNs are isotropic Gaussian (see Eq. (5))\npriors [e.g., 78, 154–156, 99, 157]. When these priors are used in combination with ReLU non-\n8\nlinearities, it has been shown that the distributions of activations within the network grow more\nheavy-tailed with increasing depth [158]. However, it has also been shown that these networks con-\nverge to GPs in the inﬁnite limit (see Section 2.3), which has famously led Dave MacKay to inquire\nwhether we have “thrown the baby out with the bath water” [159], since we usually choose BNN\nmodels for their increased expressivity over GPs (where we mean with expressivity the ability to\napproximate different distributions over function space in their respective predictives). Moreover,\nGaussian priors have recently been shown to cause a cold posterior effect in BNNs. That is, the\ntempered posterior pT (θ | D) ∝p(θ | D)1/T with T ≪1 performs better than the true Bayesian\nposterior, suggesting prior misspeciﬁcation [37].\nA simple extension of standard Gaussian priors are matrix-valued Gaussians, which allow for addi-\ntional correlations between weights [160]. Their density is given by\np(θ) = MN(M, U, V ) = exp\n\u0000−1\n2 tr\n\u0002\nV −1(θ −M)⊤U−1(θ −M)\n\u0003\u0001\n[(2π)p det U det V ]\nn\n2\n(21)\nwhere M is the mean matrix, U and V are the row and column covariances, and tr[·] is the trace\noperator. These matrix-valued Gaussians can then also be used as variational distributions, leading\nto increased performance compared to isotropic Gaussians on many tasks [160].\nAnother way to improve the expressiveness of Gaussian priors is to combine them with hierarchical\nhyperpriors [161, 162], which has already been proposed in early work on BNNs [149] as\np(θ) =\nZ\nN(µ, Σ) p(Σ) dΣ\n(22)\nwhere p(Σ) is a hyperprior over the covariance. An example of such a hyperprior is the inverse\nWishart distribution [e.g., 47], which is in d dimensions given by\np(Σ) = IWd(ν, K) = (det K)\nν+d−1\n2\n(det Σ)−ν+2d\n2\nexp\n\u0000−1\n2tr\n\u0002\nKΣ−1\u0003\u0001\n2\n(ν+d−1)d\n2\nΓd( ν+d−1\n2\n)\n(23)\nwhere ν are the degrees of freedom and K is the mean of p(Σ). When marginalizing the prior\nin Eq. (22) over the hyperprior in Eq. (23), it turns out that one gets a d-dimensional multivariate\nStudent-t distribution with ν degrees of freedom [163], namely\np(θ) = Γ( ν+d\n2 ) (det K)−1\n2\n((ν −2)π)\nn\n2 Γ( ν\n2)\n\u0012\n1 + (θ −µ)⊤K−1(θ −µ)\nν −2\n\u0013−ν+d\n2\n(24)\nSuch distributions have been shown to model the predictive variance more ﬂexibly in stochastic pro-\ncesses [163] and BNNs [47]. Moreover, in BNNs, it has been shown that priors like these, which\nare heavy-tailed (also including Laplace priors [164]) and allow for weight correlations, can reduce\nthe cold posterior effect [19], suggesting that they are less misspeciﬁed than isotropic Gaussians. Fi-\nnally, when using Student-t priors, it has been shown that one can obtain expressive BNN posteriors\neven when forcing the posterior mean of the weights to be zero [165], which highlights the ﬂexibility\nof these distributions.\nAnother Gaussian scale mixture prior is the horseshoe prior [166], which is\np(θi) = N(0, τ 2σ2\ni )\nwith\np(τ) = C+(0, b0)\nand\np(σi) = C+(0, b1)\n(25)\nwhere b0 and b1 are scale parameters and C+ is the half-Cauchy distribution\np(σ) = C+(µ, b) =\n(\n2\nπ b\n\u0010\n1 + (σ−µ)2\nb2\n\u0011−1\nif σ ≥µ\n0\notherwise\n(26)\nIn BNNs, the horseshoe prior can encourage sparsity [167] and enable interpretable feature selection\n[168]. It can also be used to aid compression of the neural network weights [169]. Moreover, in ap-\nplication areas such as genomics, where prior knowledge about the signal-to-noise ratio is available,\nthis knowledge can be encoded in such sparsity-inducing hierarchical priors [170]. Interestingly,\nthe popular neural network regularization technique dropout [171] can also be understood as an ap-\nproximation to these types of priors [172] and they can also be used to explicitly model uncertainty\n9\nover the network architecture, using doubly stochastic inference techniques [173]. Finally, Indian\nbuffet process priors can also be used to similarly encourage sparsity and select smaller numbers of\nweights [174].\nAnother interesting prior is the radial-directional prior, which disentangles the direction of the\nweight vector from its length [175]. It is given by\nθ = θr θd\nwith\nθr ∼prad(θr)\nand\nθd ∼pdir(θd)\n(27)\nwhere pdir is a distribution over the d-dimensional unit sphere and prad is a distribution over R. It\nhas been proposed by Oh et al. [175] to use the von-Mises-Fisher distribution (see Eq. (13)) for\npdir and the half-Cauchy (see Eq. (26)) for prad. Conversely, Farquhar et al. [176] suggest to use a\nGaussian for prad and a uniform distribution over the unit sphere for pdir, which they reparameterize\nby sampling from a standard Gaussian and normalizing the sampled vectors to unit length. It should\nbe noted that the idea of the radial-directional prior is related to the Goldilocks zone hypothesis,\nwhich says that there exists an annulus at a certain distance from the origin which has a particularly\nhigh density of high-performing weight vectors [177].\nIn the speciﬁc case of convolutional neural networks for vision tasks, early research has already\nnoted that the weight distributions of the convolutional ﬁlters follow the statistics of natural images\n[178, 179]. Based on this insight, weight priors have been suggested for Bayesian CNNs that either\nuse correlated Gaussians to encourage weights that are similar for neighboring pixels [19] or Gabor\nfunction priors for the whole ﬁlters to encourage, for instance, edge detection [180].\nIn terms of even more expressive priors, it has been proposed to model the parameters in terms of\nthe units of the neural network instead of the weights themselves [181]. The weight θij between\nunits i and j would then have the prior\np(θij) = g(zi, zj, ǫ)\nwith\np(z) = p(ǫ) = N(0, I)\n(28)\nwhere the function g can be either parameterized by a neural network [181] or by a Gaussian process\n[182]. A similarly implicit model, with even more ﬂexibility, has been proposed by Atanov et al.\n[183] and is simply given by\np(θ) = g(z, ǫ)\nwith\np(z) = p(ǫ) = N(0, I)\n(29)\nIn both of these priors, the main challenge is to choose the function g. Since this is hard to do\nmanually, the function is usually (meta-)learned (see Section 5.3). Finally, recent work on software\npackages for BNN inference (e.g., using gradient-guided MCMC inference [184]) has made it easier\nto try different weight-space priors, thus fostering research to discover better prior distributions\n[185].\n4.2\nFunction-space priors\nAs we saw, there are many different weight-space priors that one can choose for Bayesian neural\nnetworks. When using certain non-standard architectures, such as radial basis function networks\n[186], desired functional properties (e.g., lengthscale or amplitude) can be directly encoded into\nthose priors [187]. However, when using standard BNNs, choosing the right parameter prior can be\nchallenging, since we often have better intuitions about the functions we would expect rather than\nthe parameters themselves. The trouble is then that the mapping from parameters to functions in\nneural networks is highly non-trivial due to their many weight-space symmetries [188] and complex\nfunction-space geometries [189]. This has led to an alternative approach to prior speciﬁcation in\nBNNs, namely to specify the priors directly in function space, such that\nZ\nδ(φ(· ; θ)) p(θ | D) dθ ≈p(f | D) ∝p(D | f) p(f)\n(30)\nwhere p(f) is the function-space prior, φ(· ; θ) is the function implemented by a neural network with\nparameters θ and δ(·) is the Dirac delta measure (in function space).\nAs we have seen before (c.f., Section 2), Gaussian processes offer an excellent model class to\nencode functional prior knowledge through the choice of kernel and mean functions, that is,\np(f) = GP(m(·), k(·, ·)). It is thus a natural idea to use GP priors as function-space priors for\nBNNs. If one applies this idea in the most straightforward way, one can just optimize a posterior\n10\nthat now depends on the KL divergence between the BNN posterior and the GP prior. However,\nsince this KL is deﬁned in an inﬁnite-dimensional space, it requires approximations, such as Stein\nkernel gradient estimators [190]. Alternatively, one can ﬁrst optimize a weight-space distribution on\na BNN to minimize the KL divergence with the desired GP prior (e.g., using Monte Carlo estimates)\nand then use this optimized weight prior as the BNN prior during inference [191].\nWhile both of these approaches seem reasonable at ﬁrst sight, it has been discovered that GP and\nBNN function-space distributions do not actually have the same support and that the true KL diver-\ngence is thus inﬁnite (or undeﬁned) [192]. It has therefore recently been proposed to use the Wasser-\nstein distance instead, although this also requires approximations [193]. If one wants to forego the\nneed for a well-deﬁned divergence, one can also use a hypernetwork [194, 195] as an implicit dis-\ntribution of BNN weights and then train the network to match the GP samples on a certain set of\nfunction outputs [196]. Finally, it has recently been discovered that the ridgelet transform [197] can\nbe used to approximate GP function-space distributions with BNN weight-space distributions [198].\nAs a sidenote, it should be noted that the reverse can actually be achieved more easily, namely ﬁtting\na GP to the outputs of a BNN [199], which can also be of interest in certain applications.\nIf one does not want to use a GP prior in function space, one can still encode useful functional prior\nknowledge into BNN priors. For instance, through the study of the inﬁnite-width limits of BNNs\n(see Section 2.3), one ﬁnds that the activation function of the network has a strong inﬂuence on the\nfunctions being implemented and one can, for instance, modulate the smoothness or periodicity of\nthe BNN output by choosing different activation functions [200]. Moreover, one can directly deﬁne\npriors over the BNN outputs, which can encode strong prior assumptions about the values that the\nfunctions are allowed to take in certain parts of the input space [201], that is,\np(θ) = pbase(θ) D(φ(Cx; θ), Cy) =⇒p(θ | D) ∝p(D | θ) D(φ(Cx; θ), Cy) pbase(θ)\n(31)\nwhere pbase(θ) is some base prior in weight space, (Cx, Cy) are the inputs and outputs in terms of\nwhich the functional constraint is deﬁned and D(·, ·) is a discrepancy function. We see that these\npriors on output constraints end up looking like additional likelihood terms in the posterior and can\nthus help to encourage speciﬁc features of the output function, for instance, to ensure safety features\nin critical applications. A similar idea are noise-contrastive priors, which are also speciﬁed in func-\ntion space directly through a prior over unseen data p( ˜D) [202], which yields the prior predictive\np(D∗) =\nZZ\np(D∗| θ) p(θ | ˜D) p( ˜D) dθ d ˜D\n(32)\nThis prior can encode the belief that the epistemic uncertainty should grow away from the in-\ndistribution data and can thus also lead to more GP-like behavior in BNN posteriors. Finally, if\nwe have the prior belief that the BNN functions should not be much more complex than the ones\nof a different function class (e.g., shallower or even linear models), we can use this other class as a\nfunctional reference prior and thus regularize the predictive complexity of the model [203].\n4.3\nBayesian neural network ensembles\nDeep neural network ensembles, or deep ensembles, are a frequentist method similar to the bootstrap\n[204] that has been used to gain uncertainty estimates in neural networks [205]. However, it has been\nrecently argued that these ensembles actually approximate the BNN posterior predictive [152], that\nis\np(D∗| D) =\nZ\np(D∗| θ) p(θ | D) dθ ≈1\nK\nK\nX\ni=1\np(D∗| θi)\n(33)\nwhere θi are the weights of K independently trained ensemble members of the same architecture.\nFor linear models, ensembles can actually be made to sample exactly from the posterior [206], while\nin deeper models they can at least provide lower bounds on the marginal likelihood of the true pos-\nterior [207]. These models can also be extended to ensembles with different hyperparameters [208],\nthus also approximating a hierarchical hyperposterior. Moreover, they can be made more parameter-\nefﬁcient by sharing certain parameters between ensemble members [209], which can then also be\nused for approximate BNN inference [157]. While these models have performed well in many\npractical tasks [6], they can still severely overﬁt in some scenarios [210], leading to ill-calibrated\nuncertainties [211]. However, it has been shown recently that each ensemble member can be com-\nbined with a random function that is sampled from a function-space prior [212, 213], and that this\n11\ncan indeed yield uncertainties that are conservative with respect to the Bayesian ones [214]. More\nspeciﬁcally, the uncertainties of such ensembles are with high probability at least as large as the ones\nfrom a Gaussian process with the corresponding NNGP kernel (see Section 2.3). These results can\nalso be extended to the NTK [215].\nAnother way of making these deep ensembles more Bayesian and incorporating priors are particle-\nbased approximate inference methods, such as Stein variational gradient descent (SVGD) [216]. In\nSVGD, the ensemble members (or particles) are updated according to\nθi ←θi + η φ(θi)\nwith\nφ(θi) =\nK\nX\nj=1\nk(θi, θj) ∇θj log p(θj | D) −∇θik(θi, θj)\n(34)\nwhere η is a step-size and k(·, ·) is a kernel function in weight space. With the right step-size\nschedule, this update rule converges asymptotically to the true posterior [217] and even enjoys some\nnon-asymptotic guarantees [218]. Moreover, note that it only requires sample-based access to the\ngradient of the log posterior (and thus also the log prior), which allows it to be used with different\nweight-space priors [219, 220] and even function-space priors, such as GPs [221]. Finally, standard\ndeep ensembles can also be directly extended with a kernelized repulsive force, similar to the one in\nSVGD, which then also leads to asymptotic convergence to the true Bayesian posterior [222].\n5\n(Meta-)Learning Priors\nSo far, we have explored different types of distributions and methods to encode our prior knowledge\ninto Bayesian deep learning models. But what if we do not have any useful prior knowledge to\nencode? While orthodox Bayesianism would prescribe an uninformative prior in such a case [15, 1],\nthere are alternative ways to elicit priors, namely by learning them from data. If we go the traditional\nroute of Bayesian model selection using the marginal likelihood (the term p(D) in Eq. (1)), we can\nchoose a functional form p(θ; ψ) for the prior and optimize its hyperparameters ψ with respect to\nthis quantity. This is called empirical Bayes [17] or type-II maximum likelihood (ML-II) estima-\ntion [39]. While there are reasons to be worried about overﬁtting in such a setting, there are also\narguments that the marginal likelihood automatically trades off the goodness of ﬁt with the model\ncomplexity and thus leads to model parsimony in the spirit of Occam’s razor principle [223].\nIn the case where we have previously solved tasks that are related to the task at hand (so-called\nmeta-tasks), we can alternatively also rely on the framework of learning to learn [224, 225] or\nmeta-learning [226]. If we apply this idea to learning priors for Bayesian models in a hierarchical\nBayesian way, we arrive at Bayesian meta-learning [227–230]. This can then also be extended to\nmodern gradient-based methods [231–233].\nWhile these ML-II optimization and Bayesian meta-learning ideas can in principle be used to learn\nhyperparameters for most of the priors discussed above, we will brieﬂy review some successful\nexamples of their application below. Following the general structure from above, we will explore\nlearning priors for Gaussian processes (Section 5.1), variational autoencoders (Section 5.2), and\nBayesian neural networks (Section 5.3).\n5.1\nLearning GP priors\nFollowing the idea of ML-II optimization, we can use the marginal likelihood to select hyperparam-\neters for the mean and kernel functions of GPs. Conveniently, the marginal likelihood for GPs (with\nGaussian observation likelihood) is available in closed form as\npψ(y | x) =\nZ\np(y | f, x) GP(mψ(·), kψ(·, ·)) df\n(35)\n= −1\n2\n\u0002\n(y −m(x)⊤(Kxx + σ2I)−1(y −m(x)) + log det(Kxx + σ2I) + N log 2π\n\u0003\nwith N being the number of data points, Kxx the kernel matrix on the data points, and σ2 the noise\nof the observation likelihood. We can see that the ﬁrst term measures the goodness of ﬁt, while the\nsecond term (the log determinant of the kernel matrix) measures the complexity of the model and\nthus incorporates the Occam’s razor principle [39].\n12\nWhile this quantity can be optimized to select the hyperparameters of simple kernels, such as the\nlengthscale of an RBF kernel, it can also be used for more expressive ones. For instance, one can\ndeﬁne a spectral mixture kernel in the Fourier domain and then optimize the basis functions’ coefﬁ-\ncients using the marginal likelihood, which can recover a range of different kernel functions [234].\nTo make the kernels even more expressive, we can also allow for addition and multiplication of dif-\nferent kernels [235], which can ultimately lead to an automatic statistician [236], that is, a model\nthat can choose its own problem-dependent kernel combination based on the data and some kernel\ngrammar. While this model naïvely scales rather unfavorably due to the size of the combinatorial\nsearch space, it can be made more scalable through cheaper approximations [237] or by making the\nkernel grammar differentiable [238].\nAnother avenue, which was already alluded to above (see Section 2.1), is to use a neural network\nto parameterize the kernel. The ﬁrst attempt at this trained a deep belief network on the data and\nthen used it as the kernel function [44], but later approaches optimized the neural network kernel\ndirectly using the marginal likelihood [41], often in combination with sparse approximations [42]\nor stochastic variational inference [43] for scalability (see Eq. (7)). In this vein, it has recently been\nproposed to regularize the Lipschitzness of the used neural network, in order for the learned kernel\nto preserve distances between data points and thus improve its out-of-distribution uncertainties [239,\n240]. While all these approaches still rely on the log determinant term in Eq. (35) to protect them\nfrom overﬁtting, it has been shown that this is unfortunately not effective enough when the employed\nneural networks are overparameterized [241]. However, this can be remedied by adding a prior over\nthe neural network parameters, thus effectively turning them into BNNs and the whole model into a\nproper hierarchical Bayesian model. It should be noted that these techniques cannot only be used to\nlearn GP priors that work well for a particular task, but also to learn certain invariances from data\n[242] or to ﬁt GP priors to other (implicit) function-space distributions [199] (c.f., Section 4.2).\nAs mentioned above, if we have related tasks available, we can use them to meta-learn the GP prior.\nThis can be applied to the kernel [52, 243] as well as the mean function [52], by optimizing the\nmarginal likelihood on these meta-tasks as\nψ∗= arg max\nψ\nK\nX\ni=1\nlog pψ(yi | xi)\nwith\nDM = {(xi, yi)}K\ni=1\n(36)\nwhere DM is the set of meta-tasks. Note that the mean function can only safely be optimized in this\nmeta-learning setting, but not in the ML-II setting, since Eq. (35) does not provide any complexity\npenalty on the mean function and it would thus severely overﬁt. While meta-learning does not\nrisk overﬁtting on the actual training data (since it is not used), it might overﬁt on the meta-tasks,\nif there are too few of them, or if they are too similar to each other [244, 245]. In the Bayesian\nmeta-learning setting, this can be overcome by specifying a hierarchical hyperprior, which turns out\nto be equivalent to optimizing a PAC-Bayesian bound [246]. This has been shown to successfully\nmeta-learn GP priors from as few as ﬁve meta-tasks.\n5.2\nLearning VAE priors\nVariational autoencoders are already trained using the ELBO (see Eq. (12)), which is a lower bound\non the marginal likelihood. Moreover, their likelihood p(x | z) is trained on this objective, as op-\nposed to being ﬁxed a priori as in most other Bayesian models. One could thus expect that VAEs\nwould be well suited to also learn their prior using their ELBO. Indeed, the ELBO can be further\ndecomposed as\nL(x, ϑ) = Ez∼qϑ(z | x) [log pϑ(x | z)] −Iqϑ(z,x)(z, x) −DKL(¯qϑ(z) ∥p(z))\n(37)\nwhere Iqϑ(z,x)(z, x) is the mutual information between z and x under the joint distribu-\ntion qϑ(z, x)\n=\nqϑ(z | x) p(x) and ¯qϑ is the aggregated approximate posterior ¯qϑ(z)\n=\n1\nK\nPK\ni=1 qϑ(z | xi). Since the KL term in this objective is the only term that depends on the prior\nand the complexity of qϑ(z | x) is already penalized by the mutual information term, it has been\nargued that optimizing the prior p(z) with respect to the ELBO could be beneﬁcial [247]. One can\nthen show that the optimal prior under this objective is the aggregated posterior Ex∼p(x) [p(z | x)],\nwhere p(x) is the data distribution [248].\nAs mentioned above, a more expressive family of prior distributions than the common standard\nGaussian priors are Gaussian mixture priors [111] (see Section 3.1). In particular, with an increasing\n13\nnumber of components, these mixtures can approximate any smooth distribution arbitrarily closely\n[249]. These VAE priors can be optimized using the ELBO [250], however it has been found that this\ncan severely overﬁt [248], highlighting again that the marginal likelihood (or its lower bound) cannot\nalways protect against overﬁtting (see Section 5.1). Instead, it has been proposed to parameterize\nthe mixture components as variational posteriors on certain inducing points, that is\np(z) = 1\nK\nK\nX\ni=1\nq(z | xi)\n(38)\nwhere the xi’s are learned [248]. This can indeed improve the VAE performance without overﬁtting,\nand since the prior is deﬁned in terms of inducing points in data space, it can also straightforwardly\nbe used with hierarchical VAEs [251].\nSince mixture models can exacerbate the computation of the KL divergence and require the difﬁcult\nchoice of a number of components K, an alternative are implicit priors which are parameterized by\nlearnable functions. One speciﬁc example for image data has been proposed for VAEs in which the\nlatent space preserves the shape of the data, that is, the z’s are not just vectors, but 2D or 3D tensors.\nIn such models, one can deﬁne a hierarchical prior over z, which is parameterized by learnable\nconvolutions over the latent dimensions [252]. Another way of specifying a learnable hierarchical\nprior is to use memory modules, where the prior is then dependent on the stored memories and the\nmemory is learned together with the rest of the model [253]. More generally, one can deﬁne implicit\nprior distributions in VAEs as\nz = g(ξ; ψ)\nwith\np(ξ) = N(0, I)\n(39)\nwhere g(· ; ψ) is a learnable diffeomorphism, such as a normalizing ﬂow [254]. This has been\nsuccessfully demonstrated with RealNVP ﬂows [255], where it has been shown that the VAE can\nlearn very expressive latent representations even with a single latent dimension [256]. Moreover,\nit has been shown that using an autoregressive ﬂow [257] in this way for the prior is equivalent to\nusing an inverse autoregressive ﬂow as part of the decoder [258].\nFinally, one can also reshape some base prior by a multiplicative term, that is\np(z) ∝pbase(z) α(z; ψ)\nwith\npbase(z) = N(0, I)\n(40)\nwhere α(z; ψ) is some learnable acceptance function [259]. Depending on the form of the α-\nfunction, the normalization constant of this prior might be intractable, thus requiring approxima-\ntions such as accept/reject sampling [259]. Interestingly, when deﬁning an energy E(z; ψ) =\n−log α(z; ψ), the model above can be seen as a latent energy-based model [260, 261]. More-\nover, when deﬁning this function in terms of a discriminator d(·) in the data space, that is,\nα(z; ψ) = Ex∼p(x | z) [d(x; ψ)], this yields a so-called pull-back prior [262], which is related to\ngenerative adversarial networks [263].\n5.3\nLearning BNN priors\nFinally, we will consider learning priors for Bayesian neural networks. Due to the large dimen-\nsionality of BNN weight spaces and the complex mapping between weights and functions (see\nSection 4.1), learning BNN priors has not been attempted very often in the literature. A manual\nprior speciﬁcation procedure that may be loosely called “learning” is the procedure in Fortuin et al.\n[19], where the authors train standard neural networks using gradient descent and use their empiri-\ncal weight distributions to inform their prior choices. When it comes to proper ML-II optimization,\nBNNs pose an additional challenge, because their marginal likelihoods are typically intractable and\neven lower bounds are hard to compute. Learning BNN priors using ML-II has therefore so far\nonly focused on learning the parameters of Gaussian priors in BNNs with Gaussian approximate\nposteriors, where the posteriors were computed either using moment-matching [162] or using the\nLaplace-Generalized-Gauss-Newton method [264], that is\nlog p(D)\nLap\n≈log q(D)\nGGN\n≈log p(D | θ∗) −1\n2 log det\n\u0012 1\n2π\nˆ\nHθ∗\n\u0013\n(41)\nwith\nˆ\nHθ∗= J⊤\nθ∗HL\nθ∗Jθ∗+ HP\nθ∗\nwhere q(D) is the marginal likelihood of a Laplace approximation, θ∗= arg maxθ p(θ | D) is the\nmaximum a posteriori (MAP) estimate of the parameters, ˆ\nHθ∗is an approximate Hessian around\n14\nθ∗, Jθ∗is the Jacobian of the BNN outputs with respect to the parameters, HL\nθ∗is the Hessian of\nthe log likelihood, and HP\nθ∗is the Hessian of the log prior. Using this approximation, the marginal\nlikelihood is actually differentiable with respect to the prior hyperparameters ψ, such that they can\nbe trained together with the BNN posterior [264].\nAgain, if meta-tasks are available, one can try to meta-learn the BNN prior. For CNNs, one can for\ninstance train standard neural networks on the meta-tasks and then learn a generative model (e.g., a\nVAE) for the ﬁlter weights. This generative model can then be used as a BNN prior for convolutional\nﬁlters [183]. In the case of only few meta-tasks, one can also again use PAC-Bayesian bounds to\navoid meta-overﬁtting, at least when meta-learning Gaussian BNN priors [246]. Finally, if we do\nnot have access to actual meta-tasks, but we are aware of invariances in our data, we can construct\nmeta-tasks using data augmentation and use them to learn a prior that is (approximately) invariant\nto these augmentations [265], that is\nψ∗= arg min\nψ\nEθ∼p(θ;ψ)\n\u0002\nE˜x∼q(˜x | x) [DKL (p(y | x, θ) ∥p(y | ˜x, θ))]\n\u0003\n(42)\nwhere q(˜x | x) is the data augmentation distribution.\n6\nConclusion\nWe have argued that choosing good priors in Bayesian models is crucial to actually achieve the\ntheoretical and empirical properties that they are commonly celebrated for, including uncertainty\nestimation, model selection, and optimal decision support. While practitioners in Bayesian deep\nlearning currently often resort to the option of isotropic Gaussian (or similarly uninformative) pri-\nors, we have also highlighted that these priors are usually misspeciﬁed and can lead to several unin-\ntended negative consequences during inference. On the other hand, well chosen priors can improve\nperformance and even enable novel applications. Luckily, a plethora of alternative prior choices is\navailable for popular Bayesian deep learning models, such as (deep) Gaussian processes, variational\nautoencoders, and Bayesian neural networks. Moreover, in certain cases, useful priors for these\nmodels can even be learned from data alone.\nWe hope that this survey—while necessarily being incomplete in certain ways—has provided the\ninterested reader with a ﬁrst overview of the existing literature on priors for Bayesian deep learning\nand with some guidance on how to choose them. We also hope to encourage practitioners in this\nﬁeld to consider their prior choices a bit more carefully, and to potentially choose one of the priors\npresented here instead of the standard Gaussian ones, or better yet, to use inspiration from these\npriors and come up with even better suited ones for their own models. If only a small fraction of the\ntime usually spent thinking about increasingly elaborate inference techniques will be instead spent\non thinking about the priors used, this effort will have been worthwhile.\nAcknowledgments\nWe acknowledge funding from the Swiss Data Science Center through a PhD fellowship. We thank\nAlex Immer, Adrià Garriga-Alonso, and Claire Vernade for helpful feedback on the draft and Arnold\nWeber for constant inspiration.\nReferences\n[1] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B\nRubin. Bayesian data analysis. CRC press, 2013.\n[2] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT Press, 2012.\n[3] Thomas Bayes. An essay towards solving a problem in the doctrine of chances. Philosophical\ntransactions of the Royal Society of London, 53:370–418, 1763. By the late Rev. Mr. Bayes,\nFRS communicated by Mr. Price, in a letter to John Canton, AMFRS.\n[4] Pierre Simon Laplace. Mémoire sur la probabilité de causes par les évenements. Memoire de\nl’Academie Royale des Sciences, 1774.\n[5] Andrew Gelman, Xiao-Li Meng, and Hal Stern. Posterior predictive assessment of model\nﬁtness via realized discrepancies. Statistica sinica, pages 733–760, 1996.\n15\n[6] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin,\nJoshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s\nuncertainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural\nInformation Processing Systems, pages 13991–14002, 2019.\n[7] Robert E Kass, Bradley P Carlin, Andrew Gelman, and Radford M Neal. Markov chain\nMonte Carlo in practice: a roundtable discussion. The American Statistician, 52(2):93–100,\n1998.\n[8] Martin J Wainwright and Michael Jordan. Graphical models, exponential families, and varia-\ntional inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.\n[9] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for\nstatisticians. Journal of the American Statistical Association, 112(518):859–877, 2017.\n[10] Fernando Llorente, Luca Martino, David Delgado, and Javier Lopez-Santiago. Marginal\nlikelihood computation for model selection and hypothesis testing: an extensive review. arXiv\npreprint arXiv:2005.08334, 2020.\n[11] Edwin Fong and CC Holmes. On the marginal likelihood and cross-validation. Biometrika,\n107(2):489–496, 2020.\n[12] Andrew Gelman. Bayesian model-building by pure thought: some principles and examples.\nStatistica Sinica, pages 215–232, 1996.\n[13] Christian Robert. The Bayesian choice: from decision-theoretic foundations to computational\nimplementation. Springer Science & Business Media, 2007.\n[14] Harold Jeffreys. An invariant form for the prior probability in estimation problems. Pro-\nceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186\n(1007):453–461, 1946.\n[15] Edwin T Jaynes. Prior probabilities. IEEE Transactions on systems science and cybernetics,\n4(3):227–241, 1968.\n[16] James O Berger and José M Bernardo. On the development of the reference prior method.\nBayesian statistics, 4(4):35–60, 1992.\n[17] Herbert Robbins. An Empirical Bayes Approach to Statistics. Ofﬁce of Scientiﬁc Research,\nUS Air Force, 1955.\n[18] Ilja Klebanov, Alexander Sikorski, Christof Schütte, and Susanna Röblitz. Objective priors\nin the empirical Bayes framework. Scandinavian Journal of Statistics, 2020.\n[19] Vincent Fortuin, Adrià Garriga-Alonso, Florian Wenzel, Gunnar Rätsch, Richard Turner,\nMark van der Wilk, and Laurence Aitchison. Bayesian Neural Network Priors Revisited.\narXiv preprint arXiv:2102.06571, 2021.\n[20] Joseph L Doob. Application of the theory of martingales. Le calcul des probabilites et ses\napplications, pages 23–27, 1949.\n[21] Bas JK Kleijn, Aad W van der Vaart, et al. The Bernstein-von-Mises theorem under misspec-\niﬁcation. Electronic Journal of Statistics, 6:354–381, 2012.\n[22] Andrew Gelman, Daniel Simpson, and Michael Betancourt. The prior can often only be\nunderstood in the context of the likelihood. Entropy, 19(10):555, 2017.\n[23] A Philip Dawid, Mervyn Stone, and James V Zidek. Marginalization paradoxes in Bayesian\nand structural inference. Journal of the Royal Statistical Society: Series B (Methodological),\n35(2):189–213, 1973.\n[24] Andrew Gelman. Prior distributions for variance parameters in hierarchical models (comment\non article by Browne and Draper). Bayesian analysis, 1(3):515–534, 2006.\n[25] Anindya Bhadra, Jyotishka Datta, Nicholas G Polson, and Brandon Willard. Default Bayesian\nanalysis with global-local shrinkage priors. Biometrika, 103(4):955–969, 2016.\n[26] Andrew Gelman and Yuling Yao. Holes in Bayesian statistics. Journal of Physics G: Nuclear\nand Particle Physics, 48(1):014002, 2020.\n[27] Bruno De Finetti. Sul signiﬁcato soggettivo della probabilita. Fundamenta mathematicae, 17\n(1):298–329, 1931.\n16\n[28] Morris L Eaton and David A Freedman.\nDutch book against some ’objective’ priors.\nBernoulli, 10(5):861–872, 2004.\n[29] Leonard J Savage. The foundations of statistics. Courier Corporation, 1972.\n[30] Simone Cerreia-Vioglio, Lars Peter Hansen, Fabio Maccheroni, and Massimo Marinacci.\nMaking Decisions under Model Misspeciﬁcation. arXiv preprint arXiv:2008.01071, 2020.\n[31] Andrés R Masegosa. Learning under model misspeciﬁcation: Applications to variational and\nensemble methods. arXiv preprint arXiv:1912.08335, 2019.\n[32] Warren R Morningstar, Alexander A Alemi, and Joshua V Dillon.\nPACm-Bayes: Nar-\nrowing the Empirical Risk Gap in the Misspeciﬁed Bayesian Regime.\narXiv preprint\narXiv:2010.09629, 2020.\n[33] David H Wolpert. The lack of a priori distinctions between learning algorithms. Neural\ncomputation, 8(7):1341–1390, 1996.\n[34] Peter Grünwald and Thijs Van Ommen. Inconsistency of Bayesian inference for misspeciﬁed\nlinear models, and a proposal for repairing it. Bayesian Analysis, 12(4):1069–1103, 2017.\n[35] Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient\nas variational inference. In International Conference on Machine Learning, pages 5852–5861.\nPMLR, 2018.\n[36] Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh Jain, Runa Es-\nchenhagen, Richard E Turner, and Rio Yokota. Practical deep learning with Bayesian princi-\nples. In Advances in neural information processing systems, pages 4287–4299, 2019.\n[37] Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub ´Swi atkowski, Linh Tran, Stephan\nMandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good\nis the Bayes posterior in deep neural networks really? In International Conference on Ma-\nchine Learning, 2020.\n[38] Christopher KI Williams and Carl E Rasmussen. Gaussian Processes for Regression. In Ninth\nAnnual Conference on Neural Information Processing Systems (NIPS 1995), pages 514–520.\nMIT Press, 1996.\n[39] Carl Edward Rasmussen and Christopher KI Williams.\nGaussian Processes for Machine\nLearning. MIT Press, 2006.\n[40] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer\nScience & Business Media, 2013.\n[41] Roberto Calandra, Jan Peters, Carl Edward Rasmussen, and Marc Peter Deisenroth. Mani-\nfold Gaussian processes for regression. In 2016 International Joint Conference on Neural\nNetworks (IJCNN), pages 3338–3345. IEEE, 2016.\n[42] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel\nlearning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016.\n[43] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Stochastic\nvariational deep kernel learning. arXiv preprint arXiv:1611.00336, 2016.\n[44] Ruslan Salakhutdinov and Geoffrey Hinton. Using deep belief nets to learn covariance ker-\nnels for Gaussian processes. In Proceedings of the 20th International Conference on Neural\nInformation Processing Systems, pages 1249–1256, 2007.\n[45] Miguel Lázaro-Gredilla and Aníbal R Figueiras-Vidal. Marginalized neural network mixtures\nfor large-scale regression. IEEE transactions on neural networks, 21(8):1345–1351, 2010.\n[46] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sun-\ndaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using\ndeep neural networks. In International conference on machine learning, pages 2171–2180.\nPMLR, 2015.\n[47] Sebastian W Ober and Laurence Aitchison. Global inducing point variational posteriors for\nBayesian neural networks and deep Gaussian processes. arXiv preprint arXiv:2005.08140,\n2020.\n[48] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, ﬁxes\noverconﬁdence in relu networks. In International Conference on Machine Learning, pages\n5436–5446. PMLR, 2020.\n17\n[49] Joe Watson, Jihao Andreas Lin, Pascal Klink, Joni Pajarinen, and Jan Peters. Latent Deriva-\ntive Bayesian Last Layer Networks. In International Conference on Artiﬁcial Intelligence\nand Statistics, pages 1198–1206. PMLR, 2021.\n[50] John Bradshaw, Alexander G de G Matthews, and Zoubin Ghahramani. Adversarial examples,\nuncertainty, and transfer testing robustness in Gaussian process hybrid deep networks. arXiv\npreprint arXiv:1707.02476, 2017.\n[51] Tomoharu Iwata and Zoubin Ghahramani.\nImproving output uncertainty estimation and\ngeneralization in deep learning via neural network Gaussian processes.\narXiv preprint\narXiv:1707.05922, 2017.\n[52] Vincent Fortuin, Heiko Strathmann, and Gunnar Rätsch. Meta-Learning Mean Functions for\nGaussian Processes. arXiv preprint arXiv: 1901.08098, 2019.\n[53] Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional Gaussian\nprocesses. In Proceedings of the 31st International Conference on Neural Information Pro-\ncessing Systems, pages 2845–2854, 2017.\n[54] Andreas Damianou and Neil D Lawrence. Deep gaussian processes. In Artiﬁcial intelligence\nand statistics, pages 207–215. PMLR, 2013.\n[55] David Duvenaud, Oren Rippel, Ryan Adams, and Zoubin Ghahramani. Avoiding pathologies\nin very deep networks. In Artiﬁcial Intelligence and Statistics, pages 202–210. PMLR, 2014.\n[56] Tim GJ Rudner, Dino Sejdinovic, and Yarin Gal. Inter-domain deep Gaussian processes. In\nInternational Conference on Machine Learning, pages 8286–8294. PMLR, 2020.\n[57] Matthew M Dunlop, Mark A Girolami, Andrew M Stuart, and Aretha L Teckentrup. How\ndeep are deep Gaussian processes?\nJournal of Machine Learning Research, 19(54):1–46,\n2018.\n[58] Hugh Salimbeni, Vincent Dutordoir, James Hensman, and Marc Deisenroth. Deep Gaussian\nprocesses with importance-weighted variational inference. In International Conference on\nMachine Learning, pages 5589–5598. PMLR, 2019.\n[59] Thang Bui, Daniel Hernández-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard\nTurner. Deep Gaussian processes for regression using approximate expectation propagation.\nIn International conference on machine learning, pages 1472–1481. PMLR, 2016.\n[60] Zhenwen Dai, Andreas C Damianou, Javier González, and Neil D Lawrence. Variational\nAuto-encoded Deep Gaussian Processes. In ICLR, 2016.\n[61] Joaquin Quiñonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approx-\nimate Gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939–\n1959, 2005.\n[62] Edward Snelson and Zoubin Ghahramani. Local and global sparse Gaussian process approx-\nimations. In Artiﬁcial Intelligence and Statistics, pages 524–531. PMLR, 2007.\n[63] Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In\nArtiﬁcial intelligence and statistics, pages 567–574. PMLR, 2009.\n[64] James Hensman, Nicolò Fusi, and Neil D Lawrence. Gaussian processes for Big data. In\nProceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, pages\n282–290, 2013.\n[65] Vincent Fortuin, Gideon Dresdner, Heiko Strathmann, and Gunnar Rätsch. Sparse Gaussian\nProcesses on Discrete Domains. IEEE Access, 9, 2021.\n[66] Hugh Salimbeni and Marc Peter Deisenroth. Doubly stochastic variational inference for deep\nGaussian processes. In Proceedings of the 31st International Conference on Neural Informa-\ntion Processing Systems, pages 4591–4602, 2017.\n[67] Kurt Cutajar, Edwin V Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature\nexpansions for deep Gaussian processes. In International Conference on Machine Learning,\npages 884–893. PMLR, 2017.\n[68] Vincent Dutordoir, James Hensman, Mark van der Wilk, Carl Henrik Ek, Zoubin Ghahra-\nmani, and Nicolas Durrande. Deep Neural Networks as Point Estimates for Deep Gaussian\nProcesses. arXiv preprint arXiv:2105.04504, 2021.\n18\n[69] Vinayak Kumar, Vaibhav Singh, PK Srijith, and Andreas Damianou. Deep Gaussian pro-\ncesses with convolutional kernels. arXiv preprint arXiv:1806.01655, 2018.\n[70] Kenneth Blomqvist, Samuel Kaski, and Markus Heinonen. Deep convolutional Gaussian\nprocesses. In Joint European Conference on Machine Learning and Knowledge Discovery in\nDatabases, pages 582–597. Springer, 2019.\n[71] Vincent Dutordoir, Mark Wilk, Artem Artemev, and James Hensman. Bayesian image classiﬁ-\ncation with deep convolutional Gaussian processes. In International Conference on Artiﬁcial\nIntelligence and Statistics, pages 1529–1539. PMLR, 2020.\n[72] Dustin Tran, Michael W Dusenberry, Mark van der Wilk, and Danijar Hafner. Bayesian\nlayers: A module for neural network uncertainty. arXiv preprint arXiv:1812.03973, 2018.\n[73] Vincent Dutordoir, Hugh Salimbeni, Eric Hambro, John McLeod, Felix Leibfried, Artem\nArtemev, Mark van der Wilk, James Hensman, Marc P Deisenroth, and ST John. GPﬂux: A\nLibrary for Deep Gaussian Processes. arXiv preprint arXiv:2104.05674, 2021.\n[74] Felipe Tobar, Thang D Bui, and Richard E Turner. Learning stationary time series using\nGaussian processes with nonparametric kernels. In Proceedings of the 28th International\nConference on Neural Information Processing Systems-Volume 2, pages 3501–3509, 2015.\n[75] Gregory W Benton, Wesley J Maddox, Jayson P Salkey, Júlio Albinati, and Andrew Gordon\nWilson. Function-space distributions over kernels. Advances in Neural Information Process-\ning Systems, 32, 2019.\n[76] Juan Maroñas, Oliver Hamelijnck, Jeremias Knoblauch, and Theodoros Damoulas. Trans-\nforming Gaussian processes with normalizing ﬂows. In International Conference on Artiﬁcial\nIntelligence and Statistics, pages 1081–1089. PMLR, 2021.\n[77] Andrew Gordon Wilson and Zoubin Ghahramani. Copula processes. In Proceedings of the\n23rd International Conference on Neural Information Processing Systems-Volume 2, pages\n2460–2468, 2010.\n[78] Radford M Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto,\n1995.\n[79] Christopher KI Williams. Computing with inﬁnite networks. In Proceedings of the 9th Inter-\nnational Conference on Neural Information Processing Systems, pages 295–301, 1996.\n[80] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Proceedings\nof the 22nd International Conference on Neural Information Processing Systems, pages 342–\n350, 2009.\n[81] Tamir Hazan and Tommi Jaakkola. Steps toward deep kernel methods from inﬁnite neural\nnetworks. arXiv preprint arXiv:1508.05133, 2015.\n[82] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and\nJascha Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. In International Con-\nference on Learning Representations, 2018.\n[83] Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin\nGhahramani. Gaussian Process Behaviour in Wide Deep Neural Networks. In International\nConference on Learning Representations, 2018.\n[84] Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep Convolu-\ntional Networks as shallow Gaussian Processes. In 7th International Conference on Learning\nRepresentations, 2019.\n[85] Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolaﬁa,\nJeffrey Pennington, and Jascha Sohl-dickstein. Bayesian Deep Convolutional Networks with\nMany Channels are Gaussian Processes. In International Conference on Learning Represen-\ntations, 2019.\n[86] Adrià Garriga-Alonso and Mark van der Wilk. Correlated Weights in Inﬁnite Limits of Deep\nConvolutional Neural Networks. arXiv preprint arXiv:2101.04097, 2021.\n[87] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak.\nInﬁnite attention:\nNNGP and NTK for deep attention networks. In International Conference on Machine Learn-\ning, pages 4376–4386. PMLR, 2020.\n19\n[88] Russell Tsuchida, Fred Roosta, and Marcus Gallagher. Richer priors for inﬁnitely wide multi-\nlayer perceptrons. arXiv preprint arXiv:1911.12927, 2019.\n[89] Stefano Peluchetti, Stefano Favaro, and Sandra Fortini. Stable behaviour of inﬁnitely wide\ndeep neural networks. In International Conference on Artiﬁcial Intelligence and Statistics,\npages 1137–1146. PMLR, 2020.\n[90] Jiri Hron, Yasaman Bahri, Roman Novak, Jeffrey Pennington, and Jascha Sohl-Dickstein.\nExact posterior distributions of wide Bayesian neural networks.\narXiv preprint\narXiv:2006.10541, 2020.\n[91] Greg Yang.\nScaling limits of wide neural networks with weight sharing: Gaussian pro-\ncess behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint\narXiv:1902.04760, 2019.\n[92] Greg Yang. Tensor programs i: Wide feedforward or recurrent neural networks of any archi-\ntecture are gaussian processes. arXiv preprint arXiv:1910.12478, 2019.\n[93] Greg Yang. Tensor programs iii: Neural matrix laws. arXiv preprint arXiv:2009.10685, 2020.\n[94] Greg Yang and Edward J Hu. Feature Learning in Inﬁnite-Width Neural Networks. arXiv\npreprint arXiv:2011.14522, 2020.\n[95] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: convergence and\ngeneralization in neural networks. In Proceedings of the 32nd International Conference on\nNeural Information Processing Systems, pages 8580–8589, 2018.\n[96] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha\nSohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear\nmodels under gradient descent. Journal of Statistical Mechanics: Theory and Experiment,\n2020(12):124002, 2020.\n[97] Boris Hanin and Mihai Nica. Finite Depth and Width Corrections to the Neural Tangent\nKernel. In International Conference on Learning Representations, 2019.\n[98] Mohammad Emtiyaz Khan, Alexander Immer, Ehsan Abedi, and Maciej Jan Korzepa. Ap-\nproximate Inference Turns Deep Networks into Gaussian Processes. In 33rd Conference on\nNeural Information Processing Systems, page 1751. Neural Information Processing Systems\nFoundation, 2019.\n[99] Alexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of Bayesian\nneural nets via local linearization. In International Conference on Artiﬁcial Intelligence and\nStatistics, pages 703–711. PMLR, 2021.\n[100] Wesley Maddox, Shuai Tang, Pablo Moreno, Andrew Gordon Wilson, and Andreas Dami-\nanou. Fast Adaptation with Linearized Neural Networks. In International Conference on\nArtiﬁcial Intelligence and Statistics, pages 2737–2745. PMLR, 2021.\n[101] Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli\nYu. Harnessing the Power of Inﬁnitely Wide Deep Nets on Small-data Tasks. In International\nConference on Learning Representations, 2019.\n[102] Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint\narXiv:2006.14548, 2020.\n[103] Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-\nDickstein, and Samuel S Schoenholz. Neural Tangents: Fast and Easy Inﬁnite Neural Net-\nworks in Python. In International Conference on Learning Representations, 2019.\n[104] Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Pro-\ngramming. Advances in Neural Information Processing Systems, 32:2937–2947, 2019.\n[105] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Con-\nference on Learning Representations, 2014.\n[106] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation\nand approximate inference in deep generative models. In International conference on machine\nlearning, pages 1278–1286. PMLR, 2014.\n[107] Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyper-\nspherical variational auto-encoders. In 34th Conference on Uncertainty in Artiﬁcial Intelli-\ngence 2018, UAI 2018, pages 856–865. Association For Uncertainty in Artiﬁcial Intelligence\n(AUAI), 2018.\n20\n[108] Avrim Blum, John Hopcroft, and Ravindran Kannan. Foundations of data science. Cambridge\nUniversity Press, 2020.\n[109] Tim R Davidson, Jakub M Tomczak, and Efstratios Gavves. Increasing Expressivity of a\nHyperspherical VAE. arXiv preprint arXiv:1910.02912, 2019.\n[110] Nicola De Cao and Wilker Aziz.\nThe power spherical distribution.\narXiv preprint\narXiv:2006.04437, 2020.\n[111] Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni,\nKai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mix-\nture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.\n[112] Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational\ndeep embedding: an unsupervised and generative approach to clustering. In Proceedings of\nthe 26th International Joint Conference on Artiﬁcial Intelligence, pages 1965–1972, 2017.\n[113] Andreas Kopf, Vincent Fortuin, Vignesh Ram Somnath, and Manfred Claassen. Mixture-of-\nExperts Variational Autoencoder for clustering and generating from similarity-based repre-\nsentations. PLoS Computational Biology, 2021.\n[114] Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. arXiv preprint\narXiv:1605.06197, 2016.\n[115] Francesco Paolo Casale, Adrian Dalca, Luca Saglietti, Jennifer Listgarten, and Nicolo Fusi.\nGaussian Process Prior Variational Autoencoders. In Advances in Neural Information Pro-\ncessing Systems, pages 10369–10380, 2018.\n[116] Vincent Fortuin, Dmitry Baranchuk, Gunnar Rätsch, and Stephan Mandt. GP-VAE: Deep\nProbabilistic Time Series Imputation. In International Conference on Artiﬁcial Intelligence\nand Statistics, pages 1651–1661. PMLR, 2020.\n[117] Michael Pearce. The gaussian process prior vae for interpretable latent dynamics from pixels.\nIn Symposium on Advances in Approximate Bayesian Inference, pages 1–12. PMLR, 2020.\n[118] Sarthak Bhagat, Shagun Uppal, Zhuyun Yin, and Nengli Lim. Disentangling Multiple Fea-\ntures in Video Sequences using Gaussian Processes in Variational Autoencoders. In European\nConference on Computer Vision, pages 102–117. Springer, 2020.\n[119] Simon Bing, Vincent Fortuin, and Gunnar Rätsch. On Disentanglement in Gaussian Process\nVariational Autoencoders. arXiv preprint arXiv:2102.05507, 2021.\n[120] Metod Jazbec, Matthew Ashman, Vincent Fortuin, Michael Pearce, Stephan Mandt, and Gun-\nnar Rätsch. Scalable Gaussian Process Variational Autoencoders. In International Conference\non Artiﬁcial Intelligence and Statistics, 2021.\n[121] Matthew Ashman, Jonathan So, William Tebbutt, Vincent Fortuin, Michael Pearce, and\nRichard E Turner.\nSparse Gaussian Process Variational Autoencoders.\narXiv preprint\narXiv:2010.10177, 2020.\n[122] Metod Jazbec, Michael Pearce, and Vincent Fortuin. Factorized Gaussian Process Variational\nAutoencoders. arXiv preprint arXiv:2011.07255, 2020.\n[123] Siddharth Ramchandran, Gleb Tikhonov, Miika Koskinen, and Harri Lähdesmäki. Longitudi-\nnal Variational Autoencoder. arXiv preprint arXiv:2006.09763, 2020.\n[124] Alex Campbell and Pietro Liò. tvGP-VAE: Tensor-variate Gaussian process prior variational\nautoencoder. arXiv preprint arXiv:2006.04788, 2020.\n[125] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther.\nLadder variational autoencoders. In Proceedings of the 30th International Conference on\nNeural Information Processing Systems, pages 3745–3753, 2016.\n[126] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep\ngenerative models. In International Conference on Machine Learning, pages 4091–4099.\nPMLR, 2017.\n[127] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. arXiv\npreprint arXiv:2007.03898, 2020.\n[128] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representa-\ntion learning. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems, pages 6309–6318, 2017.\n21\n[129] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images\nwith VQ-VAE-2. arXiv preprint arXiv:1906.00446, 2019.\n[130] Teuvo Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464–1480, 1990.\n[131] Vincent Fortuin, Matthias Hüser, Francesco Locatello, Heiko Strathmann, and Gunnar Rätsch.\nSOM-VAE: Interpretable Discrete Representation Learning on Time Series. In International\nConference on Learning Representations, 2019.\n[132] Florent Forest, Mustapha Lebbah, Hanane Azzag, and Jérôme Lacaille. Deep architectures\nfor joint clustering and visualization with self-organizing maps. In Paciﬁc-Asia Conference\non Knowledge Discovery and Data Mining, pages 105–116. Springer, 2019.\n[133] Laura Manduchi, Matthias Hüser, Julia Vogt, Gunnar Rätsch, and Vincent Fortuin. DPSOM:\nDeep probabilistic clustering with self-organizing maps. arXiv preprint arXiv:1910.01590,\n2019.\n[134] Michael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders.\nIn International conference on machine learning, pages 7045–7054. PMLR, 2020.\n[135] Rickard Brüel Gabrielsson, Bradley J Nelson, Anjan Dwaraknath, and Primoz Skraba. A\ntopology layer for machine learning. In International Conference on Artiﬁcial Intelligence\nand Statistics, pages 1553–1563. PMLR, 2020.\n[136] Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Se-\nbastian Nowozin, and Cheng Zhang. EDDI: Efﬁcient Dynamic Discovery of High-Value\nInformation with Partial VAE.\nIn International Conference on Machine Learning, pages\n4234–4243. PMLR, 2019.\n[137] Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Mur-\nray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional Neural\nProcesses. In International Conference on Machine Learning, pages 1690–1699, 2018.\n[138] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Es-\nlami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.\n[139] Kihyuk Sohn, Xinchen Yan, and Honglak Lee. Learning structured output representation us-\ning deep conditional generative models. In Proceedings of the 28th International Conference\non Neural Information Processing Systems-Volume 2, pages 3483–3491, 2015.\n[140] Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvári, and John Shawe-Taylor. PAC-Bayes\nanalysis beyond the usual bounds. arXiv preprint arXiv:2006.13057, 2020.\n[141] Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy.\nOn the role of data in PAC-Bayes. In International Conference on Artiﬁcial Intelligence and\nStatistics, pages 604–612. PMLR, 2021.\n[142] Tim GJ Rudner, Vincent Fortuin, Yee Whye Teh, and Yarin Gal. On the connection between\nneural processes and Gaussian processes with deep kernels. In Workshop on Bayesian Deep\nLearning, NeurIPS, 2018.\n[143] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum,\nOriol Vinyals, and Yee Whye Teh. Attentive Neural Processes. In International Conference\non Learning Representations, 2018.\n[144] Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is Turing-Complete. Journal\nof Machine Learning Research, 22(75):1–35, 2021.\n[145] Jonathan Gordon, Wessel P Bruinsma, Andrew YK Foong, James Requeima, Yann Dubois,\nand Richard E Turner. Convolutional Conditional Neural Processes. In International Confer-\nence on Learning Representations, 2019.\n[146] Andrew Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and\nRichard Turner. Meta-Learning Stationary Stochastic Process Prediction with Convolutional\nNeural Processes. Advances in Neural Information Processing Systems, 33, 2020.\n[147] Christos Louizos, Xiahan Shi, Klamer Schutte, and Max Welling. The functional neural\nprocess. arXiv preprint arXiv:1906.08324, 2019.\n[148] Wessel P Bruinsma, James Requeima, Andrew YK Foong, Jonathan Gordon, and Richard E\nTurner. The Gaussian Neural Process. arXiv preprint arXiv:2101.03606, 2021.\n22\n[149] David J.C. MacKay. A practical Bayesian framework for backpropagation networks. Neural\ncomputation, 4(3):448–472, 1992.\n[150] Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Ben-\nnamoun. Hands-on Bayesian Neural Networks–a Tutorial for Deep Learning Users. arXiv\npreprint arXiv:2007.06823, 2020.\n[151] Eric T Nalisnick. On priors for Bayesian neural networks. PhD thesis, UC Irvine, 2018.\n[152] Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic per-\nspective of generalization. arXiv preprint arXiv:2002.08791, 2020.\n[153] Daniele Silvestro and Tobias Andermann. Prior choice affects ability of Bayesian neural\nnetworks to identify unknowns. arXiv preprint arXiv:2005.04987, 2020.\n[154] José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable\nlearning of bayesian neural networks. In International Conference on Machine Learning,\npages 1861–1869. PMLR, 2015.\n[155] Christos Louizos and Max Welling. Multiplicative normalizing ﬂows for variational bayesian\nneural networks.\nIn International Conference on Machine Learning, pages 2218–2227.\nPMLR, 2017.\n[156] Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson.\nCyclical Stochastic Gradient MCMC for Bayesian Deep Learning. In International Con-\nference on Learning Representations, 2019.\n[157] Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller,\nBalaji Lakshminarayanan, and Dustin Tran. Efﬁcient and scalable bayesian neural nets with\nrank-1 factors. In International conference on machine learning, pages 2782–2792. PMLR,\n2020.\n[158] Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel. Understanding priors in\nBayesian neural networks at the unit level. In International Conference on Machine Learning,\npages 6458–6467. PMLR, 2019.\n[159] David JC MacKay. Introduction to Gaussian processes. NATO ASI series F computer and\nsystems sciences, 168:133–166, 1998.\n[160] Christos Louizos and Max Welling. Structured and efﬁcient variational deep learning with\nmatrix gaussian posteriors. In International Conference on Machine Learning, pages 1708–\n1716. PMLR, 2016.\n[161] Alex Graves. Practical variational inference for neural networks. In Advances in neural\ninformation processing systems, pages 2348–2356. Citeseer, 2011.\n[162] Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard E Turner, José Miguel Hernández-\nLobato, and Alexander L Gaunt. Deterministic Variational Inference for Robust Bayesian\nNeural Networks. In International Conference on Learning Representations, 2018.\n[163] Amar Shah, Andrew Wilson, and Zoubin Ghahramani. Student-t processes as alternatives to\nGaussian processes. In Artiﬁcial intelligence and statistics, pages 877–885. PMLR, 2014.\n[164] Peter M Williams. Bayesian regularization and pruning using a Laplace prior. Neural com-\nputation, 7(1):117–143, 1995.\n[165] Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variance Net-\nworks: When Expectation Does Not Meet Your Expectations. In International Conference\non Learning Representations, 2018.\n[166] Carlos M Carvalho, Nicholas G Polson, and James G Scott. Handling sparsity via the horse-\nshoe. In Artiﬁcial Intelligence and Statistics, pages 73–80. PMLR, 2009.\n[167] Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez.\nStructured variational learning of\nBayesian neural networks with horseshoe priors. In International Conference on Machine\nLearning, pages 1744–1753. PMLR, 2018.\n[168] Hiske Overweg, Anna-Lena Popkes, Ari Ercole, Yingzhen Li, José Miguel Hernández-\nLobato, Yordan Zaykov, and Cheng Zhang. Interpretable Outcome Prediction with Sparse\nBayesian Neural Networks in Intensive Care. arXiv preprint arXiv:1905.02599, 2019.\n23\n[169] Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learn-\ning. In Proceedings of the 31st International Conference on Neural Information Processing\nSystems, pages 3290–3300, 2017.\n[170] Tianyu Cui, A. Havulinna, P. Marttinen, and S. Kaski. Informative Gaussian Scale Mixture\nPriors for Bayesian Neural Networks. arXiv preprint arXiv:2002.10243, 2020.\n[171] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural networks from overﬁtting. The journal of\nmachine learning research, 15(1):1929–1958, 2014.\n[172] Eric Nalisnick, José Miguel Hernández-Lobato, and Padhraic Smyth. Dropout as a structured\nshrinkage prior. In International Conference on Machine Learning, pages 4712–4722.PMLR,\n2019.\n[173] Aliaksandr Hubin and Geir Storvik. Combining model and parameter uncertainty in Bayesian\nneural networks. arXiv preprint arXiv:1903.07594, 2019.\n[174] Samuel Kessler, Vu Nguyen, Stefan Zohren, and Stephen Roberts. Hierarchical Indian Buffet\nNeural Networks for Bayesian Continual Learning. arXiv preprint arXiv:1912.02290, 2019.\n[175] Changyong Oh, Kamil Adamczewski, and Mijung Park. Radial and directional posteriors for\nbayesian neural networks. arXiv preprint arXiv:1902.02603, 2019.\n[176] Sebastian Farquhar, Michael A Osborne, and Yarin Gal. Radial bayesian neural networks:\nBeyond discrete support in large-scale bayesian deep learning. In International Conference\non Artiﬁcial Intelligence and Statistics, pages 1352–1362. PMLR, 2020.\n[177] Stanislav Fort and Adam Scherlis. The Goldilocks Zone: Towards Better Understanding\nof Neural Network Loss Landscapes. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 3574–3581, 2019.\n[178] Anuj Srivastava, Ann B Lee, Eero P Simoncelli, and S-C Zhu. On advances in statistical\nmodeling of natural images. Journal of mathematical imaging and vision, 18(1):17–33, 2003.\n[179] Eero P Simoncelli. Capturing visual image properties with probabilistic models. In The\nEssential Guide to Image Processing, pages 205–223. Elsevier, 2009.\n[180] Tim Pearce, Andrew YK Foong, and Alexandra Brintrup. Structured Weight Priors for Con-\nvolutional Neural Networks. arXiv preprint arXiv:2007.14235, 2020.\n[181] Theofanis Karaletsos,\nPeter Dayan,\nand Zoubin Ghahramani.\nProbabilistic meta-\nrepresentations of neural networks. arXiv preprint arXiv:1810.00555, 2018.\n[182] Theofanis Karaletsos and Thang D Bui. Hierarchical Gaussian Process Priors for Bayesian\nNeural Network Weights. Advances in Neural Information Processing Systems, 33, 2020.\n[183] Andrei Atanov, Arsenii Ashukha, Kirill Struminsky, Dmitriy Vetrov, and Max Welling. The\nDeep Weight Prior. In International Conference on Learning Representations, 2018.\n[184] Adrià Garriga-Alonso and Vincent Fortuin. Exact Langevin dynamics with stochastic gradi-\nents. arXiv preprint arXiv:2102.01691, 2021.\n[185] Vincent Fortuin, Adrià Garriga-Alonso, Mark van der Wilk, and Laurence Aitchison. BN-\nNpriors: A library for Bayesian neural network inference with different prior distributions.\nSoftware Impacts, page 100079, 2021.\n[186] Richard P Lippmann. Pattern classiﬁcation using neural networks. IEEE communications\nmagazine, 27(11):47–50, 1989.\n[187] Beau Coker, Melanie F Pradier, and Finale Doshi-Velez.\nTowards Expressive Priors for\nBayesian Neural Networks: Poisson Process Radial Basis Function Networks. arXiv preprint\narXiv:1912.05779, 2019.\n[188] Johanni Brea, Berﬁn Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry\nin deep networks gives rise to permutation saddles, connected by equal-loss valleys across the\nloss landscape. arXiv preprint arXiv:1907.02911, 2019.\n[189] Stanislav Fort and Stanislaw Jastrzebski. Large scale structure of neural network loss land-\nscapes. arXiv preprint arXiv:1906.04724, 2019.\n[190] Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse.\nFunctional Variational\nBayesian Neural Networks. In International Conference on Learning Representations, 2018.\n24\n[191] Daniel Flam-Shepherd, James Requeima, and David Duvenaud. Mapping Gaussian process\npriors to Bayesian neural networks. In NeurIPS Bayesian deep learning workshop, 2017.\n[192] David R Burt, Sebastian W Ober, Adrià Garriga-Alonso, and Mark van der Wilk. Understand-\ning Variational Inference in Function-Space. arXiv preprint arXiv:2011.09421, 2020.\n[193] Ba-Hien Tran, Simone Rossi, Dimitrios Milios, and Maurizio Filippone. All You Need is a\nGood Functional Prior for Bayesian Deep Learning. arXiv preprint arXiv:2011.12829, 2020.\n[194] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106,\n2016.\n[195] David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron\nCourville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017.\n[196] Daniel Flam-Shepherd, James Requeima, and David Duvenaud. Characterizing and Warping\nthe Function Space of Bayesian Neural Networks. In NeurIPS Workshop on Bayesian Deep\nLearning, 2018.\n[197] Emmanuel Jean Candes. Ridgelets: Theory and application. Ph. D. dissertation, Dept. of\nStatistics, Stanford Univ., 1998.\n[198] Takuo Matsubara, Chris J Oates, and François-Xavier Briol. The Ridgelet Prior: A Covari-\nance Function Approach to Prior Speciﬁcation for Bayesian Neural Networks. arXiv preprint\narXiv:2010.08488, 2020.\n[199] Chao Ma, Yingzhen Li, and José Miguel Hernández-Lobato. Variational implicit processes.\nIn International Conference on Machine Learning, pages 4222–4233. PMLR, 2019.\n[200] Tim Pearce, Russell Tsuchida, Mohamed Zaki, Alexandra Brintrup, and Andy Neely. Ex-\npressive priors in Bayesian neural networks: Kernel combinations and periodic functions. In\nUncertainty in Artiﬁcial Intelligence, pages 134–144. PMLR, 2020.\n[201] Wanqian Yang, Lars Lorch, Moritz A Graule, Srivatsan Srinivasan, Anirudh Suresh, Jiayu\nYao, Melanie F Pradier, and Finale Doshi-Velez. Output-constrained Bayesian neural net-\nworks. arXiv preprint arXiv:1905.06287, 2019.\n[202] Danijar Hafner, Dustin Tran, Timothy Lillicrap, Alex Irpan, and James Davidson. Noise\ncontrastive priors for functional uncertainty. In Uncertainty in Artiﬁcial Intelligence, pages\n905–914. PMLR, 2020.\n[203] Eric Nalisnick, Jonathan Gordon, and José Miguel Hernández-Lobato. Predictive Complexity\nPriors. In International Conference on Artiﬁcial Intelligence and Statistics, pages 694–702.\nPMLR, 2021.\n[204] Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994.\n[205] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles. In Proceedings of the 31st International\nConference on Neural Information Processing Systems, pages 6405–6416, 2017.\n[206] Alexander G de G Matthews, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Sample-\nthen-optimize posterior sampling for bayesian linear models. In NeurIPS Workshop on Ad-\nvances in Approximate Bayesian Inference, 2017.\n[207] Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A Bayesian Perspective\non Training Speed and Model Selection. Advances in Neural Information Processing Systems,\n33, 2020.\n[208] Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter En-\nsembles for Robustness and Uncertainty Quantiﬁcation. In Advances in Neural Information\nProcessing Systems, 2020.\n[209] Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: an Alternative Approach to\nEfﬁcient Ensemble and Lifelong Learning. In International Conference on Learning Repre-\nsentations, 2019.\n[210] Rahul Rahaman and Alexandre H Thiery. Uncertainty quantiﬁcation and deep ensembles.\narXiv preprint arXiv:2007.08792, 2020.\n[211] Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of uncertainty quan-\ntiﬁcation for Bayesian neural network inference. arXiv preprint arXiv:1906.09686, 2019.\n25\n[212] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep rein-\nforcement learning. In Proceedings of the 32nd International Conference on Neural Informa-\ntion Processing Systems, pages 8626–8638, 2018.\n[213] Ian Osband, Benjamin Van Roy, Daniel J Russo, and Zheng Wen. Deep Exploration via\nRandomized Value Functions. Journal of Machine Learning Research, 20(124):1–62, 2019.\n[214] Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, and Richard Turner. Con-\nservative uncertainty estimation by ﬁtting prior networks. In International Conference on\nLearning Representations, 2020.\n[215] Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the\nneural tangent kernel. arXiv preprint arXiv:2007.05864, 2020.\n[216] Qiang Liu and Dilin Wang. Stein variational Gradient descent: a general purpose Bayesian\ninference algorithm. In Proceedings of the 30th International Conference on Neural Informa-\ntion Processing Systems, pages 2378–2386, 2016.\n[217] Qiang Liu. Stein variational gradient descent as gradient ﬂow. In Proceedings of the 31st In-\nternational Conference on Neural Information Processing Systems, pages 3118–3126, 2017.\n[218] Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymptotic\nanalysis for Stein variational gradient descent. arXiv preprint arXiv:2006.09797, 2020.\n[219] Xinyu Hu, Paul Szerlip, Theofanis Karaletsos, and Rohit Singh.\nApplying SVGD to\nBayesian Neural Networks for Cyclical Time-Series Prediction and Inference. arXiv preprint\narXiv:1901.05906, 2019.\n[220] Francesco D’Angelo, Vincent Fortuin, and Florian Wenzel. On Stein Variational Neural Net-\nwork Ensembles. arXiv preprint arXiv:2106.10760, 2021.\n[221] Ziyu Wang, Tongzheng Ren, Jun Zhu, and Bo Zhang. Function Space Particle Optimization\nfor Bayesian Neural Networks. In International Conference on Learning Representations,\n2018.\n[222] Francesco D’Angelo and Vincent Fortuin. Repulsive Deep Ensembles are Bayesian. In Ad-\nvances in Neural Information Processing Systems, 2021.\n[223] Carl E Rasmussen and Zoubin Ghahramani. Occam’s razor. Advances in Neural Information\nProcessing Systems, pages 294–300, 2001.\n[224] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how\nto learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[225] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning\nto learn, pages 3–17. Springer, 1998.\n[226] Jonathan Baxter. A model of inductive bias learning. Journal of artiﬁcial intelligence re-\nsearch, 12:149–198, 2000.\n[227] Tom Heskes. Solving a Huge Number of Similar Tasks: A Combination of Multi-Task Learn-\ning and a Hierarchical Bayesian Approach. In Proceedings of the Fifteenth International\nConference on Machine Learning, pages 233–241, 1998.\n[228] Joshua B Tenenbaum. A Bayesian Framework for Concept Learning. PhD thesis, Citeseer,\n1999.\n[229] Li Fei-Fei, Rob Fergus, and Pietro Perona. A Bayesian approach to unsupervised one-shot\nlearning of object categories. In Proceedings Ninth IEEE International Conference on Com-\nputer Vision, pages 1134–1141. IEEE, 2003.\n[230] Neil D Lawrence and John C Platt. Learning to learn with the informative vector machine. In\nProceedings of the twenty-ﬁrst international conference on Machine learning, page 65, 2004.\n[231] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting\nGradient-Based Meta-Learning as Hierarchical Bayes. In International Conference on Learn-\ning Representations, 2018.\n[232] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.\nBayesian model-agnostic meta-learning. In Proceedings of the 32nd International Conference\non Neural Information Processing Systems, pages 7343–7353, 2018.\n26\n[233] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In\nProceedings of the 32nd International Conference on Neural Information Processing Systems,\npages 9537–9548, 2018.\n[234] Andrew Wilson and Ryan Adams. Gaussian process kernels for pattern discovery and extrap-\nolation. In International conference on machine learning, pages 1067–1075. PMLR, 2013.\n[235] David Duvenaud, James Lloyd, Roger Grosse, Joshua Tenenbaum, and Ghahramani Zoubin.\nStructure discovery in nonparametric regression through compositional kernel search.\nIn\nInternational Conference on Machine Learning, pages 1166–1174. PMLR, 2013.\n[236] James Lloyd, David Duvenaud, Roger Grosse, Joshua Tenenbaum, and Zoubin Ghahramani.\nAutomatic construction and natural-language description of nonparametric regression models.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 28, 2014.\n[237] Hyunjik Kim and Yee Whye Teh. Scaling up the Automatic Statistician: Scalable structure\ndiscovery using Gaussian processes. In International Conference on Artiﬁcial Intelligence\nand Statistics, pages 575–584. PMLR, 2018.\n[238] Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, and Roger\nGrosse. Differentiable compositional kernel learning for Gaussian processes. In International\nConference on Machine Learning, pages 4828–4837. PMLR, 2018.\n[239] Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Laksh-\nminarayanan. Simple and principled uncertainty estimation with deterministic deep learning\nvia distance awareness. arXiv preprint arXiv:2006.10108, 2020.\n[240] Vincent Fortuin, Mark Collier, Florian Wenzel, James Allingham, Jeremiah Liu, Dustin\nTran, Balaji Lakshminarayanan, Jesse Berent, Rodolphe Jenatton, and Effrosyni Kokiopoulou.\nDeep Classiﬁers with Label Noise Modeling and Distance Awareness.\narXiv preprint\narXiv:2110.02609, 2021.\n[241] Sebastian W Ober, Carl E Rasmussen, and Mark van der Wilk. The Promises and Pitfalls of\nDeep Kernel Learning. arXiv preprint arXiv:2102.12108, 2021.\n[242] Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning Invariances\nusing the Marginal Likelihood. In Advances in Neural Information Processing Systems, vol-\nume 31, pages 9938–9948, 2018.\n[243] Massimiliano Patacchiola, Jack Turner, Elliot J Crowley, Michael O’Boyle, and Amos J\nStorkey. Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels. Advances\nin Neural Information Processing Systems, 33, 2020.\n[244] Yunxiao Qin, Weiguo Zhang, Chenxu Zhao, Zezheng Wang, Hailin Shi, Guojun Qi, Jingping\nShi, and Zhen Lei. Rethink and redesign meta learning. arXiv preprint arXiv:1812.04955,\n2018.\n[245] Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-\nLearning without Memorization. In International Conference on Learning Representations,\n2019.\n[246] Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause. PACOH: Bayes-\nOptimal Meta-Learning with PAC-Guarantees.\nIn International Conference on Machine\nLearning, 2021.\n[247] Matthew D Hoffman and Matthew J Johnson. ELBO Surgery: Yet another way to carve up\nthe evidence lower bound. In Advances in Approximate Bayesian Inference, 2016.\n[248] Jakub Tomczak and Max Welling. VAE with a VampPrior. In International Conference on\nArtiﬁcial Intelligence and Statistics, pages 1214–1223. PMLR, 2018.\n[249] SR Dalal and WJ Hall. Approximating priors by mixtures of natural conjugate priors. Journal\nof the Royal Statistical Society: Series B (Methodological), 45(2):278–286, 1983.\n[250] Chunsheng Guo, Jialuo Zhou, Huahua Chen, Na Ying, Jianwu Zhang, and Di Zhou. Varia-\ntional autoencoder with optimizing Gaussian mixture model priors. IEEE Access, 8:43992–\n44005, 2020.\n[251] Philip Botros and Jakub M Tomczak. Hierarchical VampPrior variational fair auto-encoder.\narXiv preprint arXiv:1806.09918, 2018.\n27\n[252] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David\nVazquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. Interna-\ntional Conference on Learning Representations, 2017.\n[253] Jörg Bornschein, Andriy Mnih, Daniel Zoran, and Danilo J Rezende. Variational memory\naddressing in generative models. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems, pages 3923–3932, 2017.\n[254] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In\nInternational Conference on Machine Learning, pages 1530–1538. PMLR, 2015.\n[255] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp.\narXiv preprint arXiv:1605.08803, 2016.\n[256] Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad Havaei, Lau-\nrent Charlin, and Aaron Courville. Learnable explicit density for continuous latent space and\nvariational inference. arXiv preprint arXiv:1710.02248, 2017.\n[257] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max\nWelling. Improved variational inference with inverse autoregressive ﬂow. In Proceedings of\nthe 30th International Conference on Neural Information Processing Systems, pages 4743–\n4751, 2016.\n[258] Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schul-\nman, Ilya Sutskever, and Pieter Abbeel.\nVariational lossy autoencoder.\narXiv preprint\narXiv:1611.02731, 2016.\n[259] Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. In The\n22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 66–75. PMLR,\n2019.\n[260] Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning Latent\nSpace Energy-Based Prior Model. Advances in Neural Information Processing Systems, 33,\n2020.\n[261] Jyoti Aneja, Alexander Schwing, Jan Kautz, and Arash Vahdat. NCP-VAE: Variational Au-\ntoencoders with Noise Contrastive Priors. arXiv preprint arXiv:2010.02917, 2020.\n[262] Wenxiao Chen, Wenda Liu, Zhenting Cai, Haowen Xu, and Dan Pei. VAEPP: Variational\nAutoencoder with a Pull-Back Prior. In International Conference on Neural Information\nProcessing, pages 366–379. Springer, 2020.\n[263] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proceedings\nof the 27th International Conference on Neural Information Processing Systems-Volume 2,\npages 2672–2680, 2014.\n[264] Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and Mohammad Emtiyaz\nKhan. Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. In\nInternational Conference on Machine Learning, 2021.\n[265] Eric Nalisnick and Padhraic Smyth. Learning priors for invariance. In International Confer-\nence on Artiﬁcial Intelligence and Statistics, pages 366–375. PMLR, 2018.\n28\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2021-05-14",
  "updated": "2022-03-18"
}