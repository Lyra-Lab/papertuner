{
  "id": "http://arxiv.org/abs/2105.11115v3",
  "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages",
  "authors": [
    "Shunyu Yao",
    "Binghui Peng",
    "Christos Papadimitriou",
    "Karthik Narasimhan"
  ],
  "abstract": "Despite their impressive performance in NLP, self-attention networks were\nrecently proved to be limited for processing formal languages with hierarchical\nstructure, such as $\\mathsf{Dyck}_k$, the language consisting of well-nested\nparentheses of $k$ types. This suggested that natural language can be\napproximated well with models that are too weak for formal languages, or that\nthe role of hierarchy and recursion in natural language might be limited. We\nqualify this implication by proving that self-attention networks can process\n$\\mathsf{Dyck}_{k, D}$, the subset of $\\mathsf{Dyck}_{k}$ with depth bounded by\n$D$, which arguably better captures the bounded hierarchical structure of\nnatural language. Specifically, we construct a hard-attention network with\n$D+1$ layers and $O(\\log k)$ memory size (per token per layer) that recognizes\n$\\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and\n$O(\\log k)$ memory size that generates $\\mathsf{Dyck}_{k, D}$. Experiments show\nthat self-attention networks trained on $\\mathsf{Dyck}_{k, D}$ generalize to\nlonger inputs with near-perfect accuracy, and also verify the theoretical\nmemory advantage of self-attention networks over recurrent networks.",
  "text": "Self-Attention Networks Can Process Bounded Hierarchical Languages\nShunyu Yao†\nBinghui Peng‡\nChristos Papadimitriou‡\nKarthik Narasimhan†\n†Princeton University\n‡Columbia University\n{shunyuy, karthikn}@princeton.edu\n{bp2601, christos}@columbia.edu\nAbstract\nDespite their impressive performance in NLP,\nself-attention networks were recently proved\nto be limited for processing formal languages\nwith hierarchical structure, such as Dyckk, the\nlanguage consisting of well-nested parenthe-\nses of k types.\nThis suggested that natural\nlanguage can be approximated well with mod-\nels that are too weak for formal languages, or\nthat the role of hierarchy and recursion in nat-\nural language might be limited. We qualify\nthis implication by proving that self-attention\nnetworks can process Dyckk,D, the subset of\nDyckk with depth bounded by D, which ar-\nguably better captures the bounded hierarchi-\ncal structure of natural language. Speciﬁcally,\nwe construct a hard-attention network with\nD + 1 layers and O(log k) memory size (per\ntoken per layer) that recognizes Dyckk,D, and\na soft-attention network with two layers and\nO(log k) memory size that generates Dyckk,D.\nExperiments show that self-attention networks\ntrained on Dyckk,D generalize to longer inputs\nwith near-perfect accuracy, and also verify the\ntheoretical memory advantage of self-attention\nnetworks over recurrent networks.1\n1\nIntroduction\nTransformers (Vaswani et al., 2017) are now the\nundisputed champions across several benchmark\nleaderboards in NLP. The major innovation of this\narchitecture, self-attention, processes input tokens\nin a distributed way, enabling efﬁcient parallel com-\nputation as well as long-range dependency mod-\nelling. The empirical success of self-attention in\nNLP has led to a growing interest in studying its\nproperties, with an eye towards a better understand-\ning of the nature and characteristics of natural lan-\nguage (Tran et al., 2018; Papadimitriou and Juraf-\nsky, 2020).\n1Code\nis\navailable\nat\nhttps://github.com/\nprinceton-nlp/dyck-transformer.\nIn particular, it was recently shown that self-\nattention networks cannot process various kinds of\nformal languages (Hahn, 2020; Bhattamishra et al.,\n2020a), among which particularly notable is Dyckk,\nthe language of well-balanced brackets of k types.\nBy the Chomsky-Schützenberger Theorem (Chom-\nsky and Schützenberger, 1959), any context-free\nlanguage can be obtained from a Dyckk language\nthrough intersections with regular languages and\nhomomorphisms. In other words, this simple lan-\nguage contains the essence of all context-free lan-\nguages, i.e. hierarchical structure, center embed-\nding, and recursion – features which have been long\nclaimed to be at the foundation of human language\nsyntax (Chomsky, 1956).\nConsider for example the long-range and nested\ndependencies in English subject-verb agreement:\n(Laws (the lawmaker) [writes] [and revises]) [pass].\n.\n.\n.\nThe sentence structure is captured by Dyck2 string\n(()[][])[]. Given the state-of-the-art performance of\nTransformers in parsing natural language (Zhang\net al., 2020; He and Choi, 2019), the Dyckk blind\nspot seems very suggestive. If the world’s best\nNLP models cannot deal with this simple language\n— generated by a grammar with k + 2 rules and\nrecognized by a single-state pushdown automaton\n— does this not mean that the role of hierarchy and\nrecursion in natural language must be limited? This\nquestion has of course, been extensively debated\nby linguists on the basis of both theoretical and psy-\ncholinguistic evidence (Hauser et al., 2002; Frank\net al., 2012; Nelson et al., 2017; Brennan and Hale,\n2019; Frank and Christiansen, 2018).\nSo, what can self-attention networks tell us about\nnatural language and recursion?\nHere we pro-\nvide a new twist to this question by considering\nDyckk,D, the subset of Dyckk with nesting depth\nat most D, and show that Transformers can process\narXiv:2105.11115v3  [cs.CL]  13 Mar 2023\nInput \n(\n[\n]\n{\n[\n]\n(\n)\n}\n)\nLayer 1 (\n[\n]\n{\n[\n]\n(\n)\n}\n)\nLayer 2 (\n[\n]\n{\n[\n]\n(\n)\n}\n)\nLayer 3 (\n[\n]\n{\n[\n]\n(\n)\n}\n)\n햣헒햼헄3,3\nInput\n[\n]\n[\n(\n(\n[\n]\n)\n)\nLayer 1 1 0 1 2 3 4 3 2 1\nLayer 2 1 0 1 2 3 4 3 2 1\n햣헒햼헄2,4\n(a)\n(b)\nnext token prediction: ( [ ]\nFigure 1: Illustrations of our self-attention network constructions to recognize and generate Dyckk,D. In construc-\ntion (a), at each layer, the innermost brackets attend to their matching brackets and “cancel” each other, yielding\n“shallower” spans for successive layers to process. In construction (b), the ﬁrst layer computes the depth of each\ntoken by attending to all previous tokens, while the second layer uses depth information to ﬁnd the most recent\nunclosed open bractket in the history.\nit. Dyckk,D models bounded (or ﬁnite) recursion,\nthus captures the hierarchical structure of human\nlanguage much more realistically. For example,\ncenter-embedding depth of natural language sen-\ntences is known to rarely exceed three (Karlsson,\n2007; Jin et al., 2018), and while pragmatics, dis-\ncourse, and narrative can result in deeper recursion\nin language (Levinson, 2014), there is arguably a\nrelatively small limit to the depth as well.\nIn particular, we prove that self-attention net-\nworks can both recognize and generate Dyckk,D,\nwith two conceptually simple yet different construc-\ntions (Figure 1). The ﬁrst network requires D + 1\nlayers and a memory size of O(log k) (per layer per\ntoken) to recognize Dyckk,D, using a distributed\nmechanism of parenthesis matching. The second\nnetwork has two layers and memory size O(log k).\nIt works by attending to all previous tokens to count\nthe depth for each token in the ﬁrst layer, and then\nuses this depth information to attend to the most\nrecent unclosed open bracket in the second layer.\nOur constructions help reconcile the result in Hahn\n(2020) with the success of Transformers in han-\ndling natural languages.\nOur proof requires certain assumptions about the\npositional encodings, an issue that is often consid-\nered in empirical papers (Ke et al., 2021; Shaw\net al., 2018; Wang et al., 2020; Shiv and Quirk,\n2019) but not in the more theoretical literature.\nFirst, positional encodings must have log n bits\nwhen the input length is n, as otherwise differ-\nent positions would share the same representation.\nMore importantly, positional encodings should sup-\nport easy position comparisons, since token order\nis vital in formal language processing. Our exper-\niments show that two standard practices, namely\nlearnable or ﬁxed sine/cosine positional encodings,\ncannot generalize well on Dyckk,D beyond the\ntraining input lengths. In contrast, using a single\nﬁxed scalar monotonic positional encoding such\nas pos/n achieves near-perfect accuracy even on\ninputs signiﬁcantly longer than the training ones.\nOur ﬁndings provide a novel perspective on the\nfunction of positional encodings, and implies that\ndifferent applications of self-attention networks (in\nthis case, natural vs. formal language) may require\ndifferent model choices.\nOur theoretical results also bring about interest-\ning comparisons to recurrent networks (e.g. RNNs,\nLSTMs) in terms of the resource need to process\nhierarchical structure. While recurrent networks\nwith ﬁnite precision need at least Ω(D log k) mem-\nory to process Dyckk,D (Hewitt et al., 2020), our\nsecond construction requires only O(log k) mem-\nory but a O(log n) precision. In experiments where\nprecision is not an issue for practical input lengths\n(< 104), we conﬁrm that a Transformer requires\nless memory than a LSTM to reach high test accu-\nracies. This may help explain why Transformers\noutperform RNNs/LSTMs in syntactical tasks in\nNLP, and shed light into fundamental differences\nbetween recurrent and non-recurrent sequence pro-\ncessing.\n2\nRelated work\nOur work primarily relates to the ongoing effort\nof characterizing theoretical abilities (Pérez et al.,\n2019; Bhattamishra et al., 2020b; Yun et al., 2020)\nand limitations of self-attention networks, partic-\nularly through formal hierarchical structures like\nDyckk. Hahn (2020) proves that (even with posi-\ntional encodings) hard-attention Transformers can-\nnot model Dyckk, and soft-attention Transformers\nwith bounded Lipschitz continuity cannot model\nDyckk with perfect cross entropy. Bhattamishra\net al. (2020a) prove a soft-attention network with\npositional masking (but no positional encodings)\ncan solve Dyck1 but not Dyck2. Despite the expres-\nsivity issues theoretically posed by the above work,\nempirical ﬁndings have shown Transformers can\nlearn Dyckk from ﬁnite samples and outperform\nLSTM (Ebrahimi et al., 2020). Our work addresses\nthe theory-practice discrepancy by using positional\nencodings and modeling Dyckk,D.\nA parallel line of work with much lengthier tra-\ndition (Elman, 1990; Das et al., 1992; Steijvers and\nGrünwald, 1996) investigates the abilities and limi-\ntations of recurrent networks to process hierarchi-\ncal structures. In particular, RNNs or LSTMs are\nproved capable of solving context-free languages\nlike Dyckk given inﬁnite precision (Korsky and\nBerwick, 2019) or external memory (Suzgun et al.,\n2019; Merrill et al., 2020). However, Merrill et al.\n(2020) also prove RNNs/LSTMs cannot process\nDyckk without such assumptions, which aligns\nwith experimental ﬁndings that recurrent networks\nperform or generalize poorly on Dyckk (Bernardy,\n2018; Sennhauser and Berwick, 2018; Yu et al.,\n2019). Hewitt et al. (2020) propose to consider\nDyckk,D as it better captures natural language, and\nshow ﬁnite-precision RNNs can solve Dyckk,D\nwith Θ(D log k) memory.\nFor the broader NLP community, our results\nalso contribute to settling whether self-attention\nnetworks are restricted to model hierarchical struc-\ntures due to non-recurrence, a concern (Tran et al.,\n2018) often turned into proposals to equip Trans-\nformers with recurrence (Dehghani et al., 2019;\nShen et al., 2018; Chen et al., 2018; Hao et al.,\n2019). On one hand, Transformers are shown to en-\ncode syntactic (Lin et al., 2019; Tenney et al., 2019;\nManning et al., 2020) and word order (Yang et al.,\n2019) information, and dominate syntactical tasks\nin NLP such as constituency (Zhang et al., 2020)\nand dependency (He and Choi, 2019) parsing. On\nthe other hand, on several linguistically-motivated\ntasks like English subject-verb agreement (Tran\net al., 2018), recurrent models are reported to out-\nperform Transformers. Our results help address\nthe issue by conﬁrming that distributed and recur-\nrent sequence processing can both model hierarchi-\ncal structure, albeit with different mechanisms and\ntradeoffs.\n3\nPreliminaries\n3.1\nDyck Languages\nConsider the vocabulary of k types of open and\nclose brackets Σ = ∪i∈[k]{⟨i, ⟩i}, and deﬁne\nDyckk ⊂γΣ∗ω (γ, ω being special start and end\ntokens) to be the formal language of well-nested\nbrackets of k types. It is generated starting from\nγXω through the following context-free grammar:\nX →ϵ | ⟨i X ⟩i X\n(i ∈[k])\n(1)\nwhere ϵ denotes the empty string.\nIntuitively, Dyckk can be recognized by sequen-\ntial scanning with a stack (i.e., a pushdown au-\ntomaton). Open brackets are pushed into the stack,\nwhile a close bracket causes the stack to pop, and\nthe popped open bracket is compared with the cur-\nrent close bracket (they should be of the same type).\nThe depth of a string w1:n at position i is the stack\nsize after scanning w1:i, that is, the number of open\nbrackets left in the stack:\nd(w1:i) = count(w1:i, ⟨) −count(w1:i, ⟩)\n(2)\nFinally, we deﬁne Dyckk,D to be the subset of\nDyckk strings with depth bounded by D:\nDyckk,D =\n\u001a\nw1:n ∈Dyckk\n\f\f\f\f max\ni∈[n] d(w1:i) ≤D\n\u001b\nThat is, a string in Dyckk,D only requires a stack\nwith bounded size D to process.\n3.2\nSelf-attention Networks\nWe consider the encoder part of the original Trans-\nformer (Vaswani et al., 2017), which has multiple\nlayers of two blocks each: (i) a self-attention block\nand (ii) a feed-forward network (FFN). For an input\nstring w1:n ∈Σ∗, each input token wi is converted\ninto a token embedding via fe : Σ →Rdmodel, then\nadded with a position encoding pi ∈Rdmodel. Let\nxi,ℓ∈Rdmodel be the i-th representation of the ℓ-th\nlayer (i ∈[n], ℓ∈[L]). Then\nxi,0 = fe(wi) + pi\n(3)\nai,ℓ= Attℓ(Qℓ(xi), Kℓ(x), Vℓ(x))\n(4)\nxi,ℓ+1 = Fℓ(ai,ℓ)\n(5)\nAttention\nIn each head of a self-attention block,\nthe input vectors x1:n undergo linear transforms\nQ, K, V yielding query, key, and value vectors.\nThey are taken as input to a self-attention mod-\nule, whose t-th output, Att(Qxi, Kx, V x), is\na vector ai = P\nj∈[T] αjV xj, where α1:n =\nsoftmax(⟨Qxi, Kx1⟩, · · · , ⟨Qxi, Kxn⟩). The ﬁ-\nnal attention output is the concatenation of multi-\nhead attention outputs. We also consider variants\nof the basic model along these directions:\n(i) Hard attention, as opposed to soft attention\ndescribed above, where hardmax is used in place\nfor softmax (i.e.\nAtt(Qxi, Kx, V x) = V xj′\nwhere j′ = arg maxj⟨Qxi, Kxj⟩). Though im-\npractical for NLP, it has been used to model formal\nlanguages (Hahn, 2020).\n(ii) Positional masking, where α1:i (past) or αi:n\n(future) is masked for position i. Future-positional\nmasking is usually used to train auto-regressive\nmodels like GPT-2 (Radford et al., 2019).\nFeed-forward network\nA feed-forward network\nF transforms each self-attention output vector\nai →F(ai) individually. It is usually implemented\nas a multi-layer perceptron (MLP) with ReLU ac-\ntivations. Residual connections (He et al., 2016)\nand layer normalization (Ba et al., 2016) are two\noptional components to aid learning.\nPositional encodings\nVaswani et al. (2017) pro-\nposes two kinds of positional encoding: (i) Fourier\nfeatures (Rahimi and Recht, 2007), i.e. sine/cosine\nvalues of different frequencies; (ii) learnable fea-\ntures for each position. In this work we propose to\nuse a single scalar i/n to encode position i ∈[n],\nand show that it helps process formal languages\nlike Dyckk,D, both theoretically and empirically.\nPrecision and memory size\nWe deﬁne precision\nto be the number of binary bits used to represent\neach scalar, and memory size per layer (dmodel) to\nbe the number of scalars used to represent each\ntoken at each layer. The memory size (L · dmodel)\nis the total memory used for each token.\n3.3\nLanguage Generation and Recognition\nFor a Transformer with L layers and input w1:i, we\ncan use a decoder (MLP + softmax) on the ﬁnal\ntoken output xi,L to predict wi+1. This deﬁnes\na language model fθ(wi+1|wi) where θ denotes\nTransformer and decoder parameters. We follow\nprevious work (Hewitt et al., 2020) to deﬁne how a\nlanguage model can generate a formal language:\nDeﬁnition 3.1 (Language generation). Language\nmodel fθ over Σ⋆generates a language L ⊆Σ⋆if\nthere exists ϵ > 0 such that L = {w1:n ∈Σ⋆| ∀i ∈\n[n], fθ(wi|w1:i−1) ≥ϵ}.\nWe also consider language recognition by a lan-\nguage classiﬁer gθ(w1:i), where a decoder on xi,L\ninstead predicts a binary label.\nDeﬁnition 3.2 (Language recognition). Language\nclassiﬁer gθ over Σ⋆recognizes a language L ⊆\nΣ⋆if L = {w1:n ∈Σ⋆|gθ(w1:n) = 1}.\n4\nTheoretical Results\nIn this section we state our theoretical results along\nwith some remarks. Proof sketches are provided in\nthe next section, and details in Appendix A,B,C.\nTheorem 4.1 (Hard-attention, Dyckk,D recogni-\ntion). For all k, D ∈N+, there exists a (D + 1)-\nlayer hard-attention network that can recognize\nDyckk,D. It uses both future and past positional\nmasking heads, positional encoding of the form i/n\nfor position i, O(log k) memory size per layer, and\nO(log n) precision, where n is the input length.\nTheorem 4.2 (Soft-attention, Dyckk,D generation).\nFor all k, D ∈N+, there exists a 2-layer soft-\nattention network that can generate Dyckk,D. It\nuses future positional masking, positional encod-\ning of form i/n for position i, O(log k) memory\nsize per layer, and O(log n) precision, where n is\nthe input length. The feed-forward networks use\nresidual connection and layer normalization.\nTheorem 4.3 (Precision lower bound). For all\nk ∈N+, no hard-attention network with o(log n)\nprecision can recognize Dyckk,2 where n is the\ninput length.\nRequired precision\nBoth constructions require\na precision increasing with input length, as indi-\ncated by Theorem 4.3. The proof of the lower\nbound is inspired by the proof in Hahn (2020),\nbut several technical improvements are necessary;\nsee Appendix C. Intuitively, a vector with a ﬁxed\ndimension and o(log n) precision cannot even rep-\nresent n positions uniquely. The required precision\nis not unreasonable, since log n is a small overhead\nto the n tokens the system has to store.\nComparison to recurrent processing\nHewitt\net al. (2020) constructs a 1-layer RNN to gener-\nate Dyckk,D with Θ(D log k) memory, and proves\nit is optimal for any recurrent network. Thus The-\norem 4.2 establishes a memory advantage of self-\nattention networks over recurrent ones. However,\nthis is based on two tradeoffs: (i) Precision. Hewitt\net al. (2020) assumes O(1) precision while we re-\nquire O(log n). (ii) Runtime. Runtime of recurrent\nand self-attention networks usually scale linearly\nand quadratically in n, respectively.\nComparison between two constructions\nTheo-\nrem 4.2 requires fewer layers (2 vs. D) and memory\nsize (O(log k) vs. O(D log k)) than Theorem 4.1,\nthanks to the use of soft-attention, residual con-\nnection and layer normalization. Though the two\nconstructions are more suited to the tasks of recog-\nnition and generation respectively (Section 5), each\nof them can also be modiﬁed for the other task.\nConnection to Dyckk\nIn Hahn (2020) it is shown\nthat no hard-attention network can recognize\nDyckk even for k = 1. Theorem 4.1 establishes\nthat this impossibility can be circumvented by\nbounding the depth of the Dyck language. Hahn\n(2020) also points out soft-attention networks can\nbe limited due to bounded Lipschitz continuity.\nIn fact, our Theorem 4.2 construction can also\nwork on Dyckk with some additional assumptions\n(e.g. feed n also in input embeddings), and we cir-\ncumvent the impossibility by using laying normal-\nization, which may have an O(n) Lipschitz con-\nstant. More details are in Appendix B.4.\n5\nConstructions\n5.1\n(D + 1)-layer Hard-Attention Network\nOur insight underlying the construction in Theo-\nrem 4.1 is that, by recursively removing matched\nbrackets from innermost positions to outside, each\ntoken only needs to attend to nearest unmatched\nbrackets to ﬁnd its matching bracket or detect er-\nror within D layers. Speciﬁcally, at each layer\nℓ≤D, each token will be in one of three states\n(Figure 2 (c)): (i) Matched, (ii) Error, (iii) Un-\nmatched, and we leverage hard-attention to imple-\nment a dynamic state updating process to recognize\nDyckk,D.\nRepresentation\nFor an input w1:n ∈γΣ∗ω, the\nrepresentation at position i of layer ℓhas ﬁve parts\nxi,ℓ= [ti, oi, pi, mi,ℓ, ei,ℓ]: (i) a bracket type em-\nbedding ti ∈R⌈log k⌉that denotes which bracket\ntype (1 · · · k) the token is (or if the token is start/end\ntoken); (ii) a bracket openness bit oi ∈{0, 1},\nwhere 1 denotes open brackets (or start token) and\n0 denotes close one (or end token); (iii) a posi-\ntional encoding scalar pi = i/n; (iv) a match bit\nmi,ℓ∈{0, 1}, where 1 denotes matched and 0 un-\nmatched; (v) an error bit ei,ℓ∈{0, 1}, where 1\ndenotes error and 0 no error. Token identity parts\nti, oi, pi are maintained unchanged throughout lay-\ners. The match and error bits are initialized as\nei,0 = mi,0 = 0.\nThe ﬁrst D layers have identical self-attention\nblocks and feed-forward networks, detailed below.\nAttention\nConsider the ℓ-th self-attention layer\n(ℓ∈[D]), and denote xi = xi,ℓ−1, mi = mi,ℓ−1,\nai = ai,ℓ, yi = xi,ℓfor short. We have 3 atten-\ntion heads: (i) an identity head Attid, where each\ntoken only attends to itself with attention output\naid\ni\n= xi; (ii) a left head Attleft with future po-\nsitional masking; (iii) a right head Attright with\npast positional masking. The query, key, and value\nvectors for Attleft are deﬁned as Qxi = 1 ∈R,\nKxi = pi −mi ∈R, V xi = xi ∈Rdmodel, so that\naleft\ni\n= xj1,\nj1 = arg max\nj<i (j/n −mj)\nis the representation of the nearest unmatched token\nto i on its left side. Similarly\naright\ni\n= xj2,\nj2 = arg max\nj>i (1 −j/n −mj)\nis the representation of the nearest unmatched to-\nken to i on its right side. The attention output for\nposition i is the concatenation of these three out-\nputs: ai = [aid\ni , aleft\ni\n, aright\ni\n] = [xi, xj1, xj2].\nFeed-forward network (FFN)\nFollowing the\nnotation above, the feed-forward network F : ai →\nyi serves to update each position’s state using in-\nformation from xj1, xj2. The high level logic (Fig-\nure 2 (c)) is that, if wi is an open bracket, its po-\ntential matching half should be wj = wj2 (j2 > i),\notherwise it should be wj = wj1 (j1 < i). If wi and\nwj are one open and one close, they either match\n(same type) or cause error (different types). If wi\nand wj are both open or both close, no state update\nis done for position i. Besides, token identity parts\nti, oi, pi are copied from aid\ni to pass on. The idea\ncan be translated into a language of logical opera-\ntions (∧, ∨, ¬) plus a SAME(t, t′) operation, which\nreturns 1 if vectors t = t′ and 0 otherwise:\nyi = [ti, oi, pi, m′\ni, e′\ni]\nm′\ni = mi ∨(oi ∧¬oj2 ∧s2) ∨(¬oi ∧oj1 ∧s1)\ne′\ni = ei ∨(oi ∧¬oj2 ∧¬s2) ∨(¬oi ∧oj1 ∧¬s1)\ns1 = SAME(ti, tj1)\ns2 = SAME(ti, tj2)\n(\n[\n]\n(\n]\n)\nLayer 1\n(\n[\n]\n(\n]\n)\nLayer 2\n(\n[\n]\n(\n]\n)\nLayer 3\noutput: 0/1\n(\n[\n]\n(\n]\n)\nFFN\n(\n[\n]\n(\n]\n)\n(\n[\n]\n(\n]\n)\n(\n[\n]\n(\n]\n)\n(\n[\n]\n(\n]\n)\n(\n[\n]\n(\n]\n)\n(\n[\n]\n(\n]\n)\n햠헍헍헅햾햿헍\n햠헍헍헋헂헀헁헍\n햠헍헍헂햽\n(\n[\n]\n(\n]\n)\nmatched\nerror\nunmatched\nunmatched\nw1:n\nx1:n\n(\n[\n]\n(\n]\n)\nγ\nγ\nγ\nω\nω\nω\nω\nγ\nγ\nω\nγ\nω\nω\nω\nγ\nγ\nγ\nγ\nγ\nγ\nω\nω\nω\nω\n(a)\n(b)\n(c)\ny1:n\nFigure 2: Our construction for Theorem 4.1. (a) The network has multiple identical layers to match brackets and\ndetect errors. (b) Each layer consists of three hard-attention heads so that a token attends to itself and the nearest\nunmatched tokens on both sides, and uses representations from these positions to update its state. (c) Each position\ncan be in three states: matched, error, or unmatched.\nAs we show in Appendix A, a multi-layer percep-\ntion with ReLU activations can simulate all oper-\nations (∧, ∨, ¬, SAME), thus the existence of our\ndesired FFN.\nFinal layer\nAt the (D + 1)-th layer, the self at-\ntention is designed as Qxi = 1 ∈R, Kxi =\nei+1−mi ∈R, V xi = (ei, mi) ∈R2. If all brack-\nets are matched without error ((ei, mi) = (0, 1)),\nall keys would be 0, and the attention output of the\nlast token an would be (0, 1). If any bracket ﬁnds\nerror (ei = 1) or is not matched (mi = 0), the key\nwould be at least 1 and an would not be (0, 1). An\nFNN that emulates (a, b) 7→¬a ∧b will deliver yn\nas the recognition answer.\n5.2\nTwo-layer Soft-Attention Network\nOur Theorem 4.2 construction takes advantage of\nsoft attention, residual connection, and layer nor-\nmalization to calculate each token depth and trans-\nlate it into a vector form at the ﬁrst layer. Using the\ndepth information, at the second layer each wi can\nattend to the stack-top open bracket at the position,\nin order to decide if open brackets or which type of\nclose brackets can be generated as the next token\n(Figure 3).\nRepresentation\nThe representation at position i,\nlayer ℓhas four parts xi,ℓ= [ti, oi, pi, di,ℓ], with\nbracket type embedding ti, bracket openness bit\noi, position encoding pi already speciﬁed in Sec-\ntion 5.1. The last part di,ℓ∈R2 is used to store\ndepth information for position i, and initialized as\ndi,0 = (0, 0).\nFirst Layer – Depth Counting\nThe ﬁrst self-\nattention layer has two heads, where an Attid head\nis still used to inherit ti, oi, pi, and a future po-\nsitional masking head2 Attd aims to count depth\nwith Qxi = Kxi = 1 and V xi = 2oi −1, result-\ning in uniform attention scores and attention output\nad\ni = P\nj≤i\n1\ni · (2oj −1) = d(w1:i)/i.\nHowever, our goal is to enable matching based\non depth di = d(w1:i), and the attention output\ndi/i isn’t readily usable for such a purpose: the\ndenominator i is undesirable, and even a scalar di\ncannot easily attend to the same value using dot-\nproduct attention. Thus in the ﬁrst feed-forward\nnetwork, we leverage residual connection and layer\nnormalization to transform\ndi/i 7→di = (cos(θ(di)), sin(θ(di)))\n(6)\nwhere θ(d) = arctan\n\u0010\nd\nD+2−d\n\u0011\nhas an unique\n2Here we assume wi+1:n is masked for position i, just for\nconvenience of description.\nҁ\n[\n]\n(\n)\n(\n[\n]\n(\n)\n(\n[\n]\n(\n)\nMLP\n0\n1\n(\n2\n[\n1\n]\n2\n(\n1\n)\n(\n[\n]\n(\n)\n0\n1\n2\n1\n2\n1\nMLP\nAttention layer 1\nMLP layer 1\nAttention layer 2\nMLP layer 2\ndepth\nw1:i\n0 #\n1 (\n2 [\n1 ]\n2 (\n1 )\nγ\nγ\nγ\nγ\nγ\n prediction: ( [ )\nwi+1\nx1:i,0\nFigure 3: Our construction for Theorem 4.2. The ﬁrst self-attention layer calculates token depths, while the second\nlayer uses them so that each token attends to the closest unmatched open bracket ign the history, which is useful\nfor next token prediction.\nvalue for every d ∈{0, · · · , D + 1}, so that\ndi · dj\n(\n= 1\ndi = dj\n< 1 −\n1\n10D2\ndi ̸= dj\n(7)\nThe representation by the end of ﬁrst layer is xi,1 =\n[ti, oi, pi, di]. The full detail for the ﬁrst FFN is in\nAppendix B.1.\nSecond layer – Depth Matching\nThe second\nself-attention layer has a depth matching hard-\nattention head Attmatch, with query, key, value\nvectors as Qxi = [20D2 · di, 1, 2] ∈R4, Kxi =\n[di, pi, oi] ∈R4, V xi = xi, so that attention score\n⟨Qxi, Kxj⟩= 20D2di · dj + j/n + 2oj\n(\n= 20D2 + 2 + j/n\ndi = dj, oj = 1\n≤20D2 + 1\notherwise\nwould achieve its maximum when wj (j ≤i) is the\nopen bracket (or start token) closest to wi with dj =\ndi. The attention output is ai = [aid\ni , amatch\ni\n] =\n[xi, xj] where j = max{j ≤i|di = dj ∧oj = 1}.\nWith such a [xi, xj], the second-layer FFN can\nreadily predict what wi+1 could be. It could be\nany open bracket when di < D (i.e. cos(θ(di)) >\ncos(θ(D))), and it could be a close bracket with\ntype as tj (or end token if wj is start token). The\ndetailed construction for such a FFN is in Ap-\npendix B.2.\nOn Dyckk Generation\nIn fact, this theoretical\nconstruction can also generate Dyckk, as intuitively\nthe O(log n) precision assumption allows counting\ndepth up to O(n). But it involves extra conditions\nlike feeding n into network input, and may not\nbe effectively learned in practice. Please refer to\ndetails in Appendix B.4.\nConnection to Empirical Findings\nOur theo-\nretical construction explains the observation in\nEbrahimi et al. (2020): the second layer of a two-\nlayer Transformer trained on Dyckk often produces\nvirtually hard attention, where tokens attend to the\nstack-top open bracket (or start token). It also ex-\nplains why such a pattern is found less systemati-\ncally as input depth increases, as (6) is hard to learn\nand generalize to unbounded depth in practice.\n6\nExperiments\nOur constructions show the existence of self-\nattention networks that are capable of recognizing\nand generating Dyckk,D. Now we bridge theoret-\nical insights into experiments, and study whether\nsuch networks can be learned from ﬁnite samples\nand generalize to longer input. The answer is af-\nﬁrmative when the right positional encodings and\nmemory size are chosen according to our theory.\nWe ﬁrst present results on Dyck8,10 (Sec-\ntion 6.1) as an example Dyckk,D language to in-\nvestigate the effect of different positional encod-\ning schemes, number of layers, and hidden size\non the Transformer performance, and to compare\nwith the LSTM performance.\nWe then extend\nthe Transformer vs. LSTM comparison on more\nDyckk,D languages (k ∈{2, 8, 32, 128}, D ∈\n{3, 5, 10, 15}) in Section 6.2. Finally, we apply\n1\n2\n3\n4\n5\n10\n# Layers\n0.6\n0.7\n0.8\n0.9\n1.0\nClose Accuracy\n(a) Transformers \n(Dyck-(8, 10) Test)\nPositional Encoding\ncos\nlearn\npos/N\n20\n40\n60\n80\n100\nMemory Dim.\n0.8\n0.9\n1.0\nClose Accuracy\n(b) Transformer v. LSTM \n (Dyck-(8, 10) Validation)\nModel\nTransformer (pos/N)\nLSTM\n20\n40\n60\n80\n100\nMemory Dim.\n0.8\n0.9\n1.0\nClose Accuracy\n(c) Transformer v. LSTM \n (Dyck-(8, 10) Test)\nModel\nTransformer (pos/N)\nLSTM\nFigure 4: Results on Dyck8,10 validation set (same input lengths as training) and test set (longer inputs). (a)\ncompares Transformers of different layers (L ∈{1, 2, 3, 4, 5, 10}) and with different positional encodings (COS,\nLEARN,POS/N) on the test set. (b) and (c) compare a 2-layer Transformer (POS/N) with a 1-layer LSTM over\nvarying memory sizes on the validation and test sets respectively.\nthe novel scalar positional encoding to natural lan-\nguage modeling with some preliminary ﬁndings\n(Section 6.3).\n6.1\nEvaluation on Dyck8,10\nSetup\nFor Dyck8,10, we generate training and val-\nidation sets with input length n ≤700, and test set\nwith length 700 < n ≤1400. We train randomly\ninitialized Transformers using the Huggingface li-\nbrary (Wolf et al., 2019), with one future positional\nmasking head, L ∈{1, 2, 3, 4, 5, 10} layers, and\na default memory size dmodel = 30. We search\nfor learning rates in {0.01, 0.001}, run each model\nwith 3 trials, and report the average accuracy of\ngenerating close brackets, the major challenge of\nDyckk,D. More setup details are in Appendix D.1.\nPositional Encodings\nWe compare 3 types of po-\nsitional encodings: (i) Fourier features (COS); (ii)\nlearnable features (LEARN); (iii) a scalar i/6000\nfor position i (POS/N). Note that (i, ii) are original\nproposals in Vaswani et al. (2017), where positional\nencoding vectors are added to the token embed-\ndings, while our proposal (iii) encodes the position\nas a ﬁxed scalar separated from token embeddings.\nOn the validation set of Dyck8,10 (see Ap-\npendix D.2), all three models achieve near-perfect\naccuracy with L ≥2 layers. On the test set (Fig-\nure 4(a)) however, only POS/N maintains near-\nperfect accuracy, even with L = 10 layers. Mean-\nwhile, LEARN and COS fail to generalize, because\nencodings for position 700 < i ≤1400 are not\nlearned (for LEARN) or experienced (for COS) dur-\ning training. The result validates our theoretical\nconstruction, and points to the need for separate\nand systemic positional encodings for processing\nlong and order-sensitive sequences like Dyckk,D.\nMemory Size and Comparison with LSTM\nWe compare a two-layer Transformer (POS/N) with\na one-layer LSTM3 (Hochreiter and Schmidhu-\nber, 1997) using varying per-layer memory sizes\ndmodel ∈{10, 20, · · · , 100}.\nAs Figure 4 (b)\nshows, the Transformer consistently outperforms\nthe LSTM on the validation set. On the test set\n(Figure 4 (c)), the Transformer and the LSTM ﬁrst\nachieve a > 90% accuracy using dmodel = 20 and\n40 respectively, and an accuracy of > 95% with\ndmodel = 30 and 50, respectively. These ﬁndings\nagree with our theoretical characterization that self-\nattention networks have a memory advantage over\nrecurrent ones.\n6.2\nEvaluation on More Dyckk,D Languages\nSetup\nIn order to generalize some of the above\nresults, we generate a wide range of Dyckk,D\nlanguages with different vocabulary sizes (k ∈\n{2, 8, 32, 128}) and recursion bounds (D\n∈\n{3, 5, 10, 15}). We continue to compare the one-\nlayer LSTM versus the two-layer Transformer\n(POS/N). For each model on each language, we\nperform a hyperparameter search for learning rate\nin {0.01, 0.001} and memory size dmodel\n∈\n{10, 30, 50}, and report results from the best set-\nting based on two trials for each setting.\n3LSTMs only need one layer to process Dyckk,D (Hewitt\net al., 2020), while Transformers at least need two in our\nconstructions. We also experimented with two-layer LSTMs\nbut did not ﬁnd improved performance.\n3\n5\n10\n15\nD\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nClose Accuracy\n(a) Dyck-(k, D) Validation\nModel\nTransformer\nLSTM\nk\n2\n8\n32\n128\n3\n5\n10\n15\nD\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nClose Accuracy\n(b) Dyck-(k, D) Test\nFigure 5: Results on more Dyckk,D languages.\n0\n50\n100\n150\nEpoch\n2\n4\n6\n8\n10\nLoss\nRoBERTa (WikiText-103)\nPositional Encoding\nlearn\npos/N\nSplit\nTrain\nValidation\nFigure 6: Results on WikiText-103.\nResults\nThe validation and test accuracy of the\nmodels are reported in Figure 5, and more ﬁne-\ngrained results for each dmodel ∈{10, 30, 50}\nare in Appendix D.2. The Transformer attains a\n> 99.9% validation accuracy and a > 94% test\naccuracy across all languages, strengthening the\nmain claim that self-attention networks can learn\nDyckk,D languages and generalize to longer input.\nOn the other hand, the validation and test accu-\nracy of the LSTM model are less than 80% when\nthe vocabulary size and recursion depth are large,\ni.e. (k, D)\n∈\n{(32, 15), (128, 10), (128, 15)}4,\nwhich reconﬁrms Transformers’ memory advan-\ntage under limited memory (dmodel ≤50).\n6.3\nEvaluation on WikiText-103\nIn Section 6.1, we show a Transformer with the\nscalar positional encoding scheme (POS/N) can\nlearn Dyckk,D and generalize to longer input, while\ntraditional positional encoding schemes ((COS),\n(LEARN)) lead to degraded test performance. To\ninvestigate whether such a novel scheme is also use-\nful in NLP tasks, we train two RoBERTa5 models\n(POS/N, LEARN) from scratch on the WikiText-\n103 dataset (Merity et al., 2017) for 150 epochs.\nFigure 6 shows the masked language modeling\nloss on both training and validation sets. By the end\nof the training, POS/N has a slightly larger valida-\ntion loss (1.55) than LEARN (1.31). But throughout\nthe optimization, POS/N shows a gradual decrease\nof loss while LEARN has a sudden drop of loss\naround 20-30 epochs. We believe it will be interest-\n4Note that Hewitt et al. (2020) only reports D ∈{3, 5}.\n5We also tried language modeling with GPT-2 models, and\nPOS/N has slightly larger train/validation losses than LEARN\nthroughout the training. Interestingly, using no positional en-\ncoding leads to the same loss curves as LEARN, as positional\nmasking leaks positional information.\ning for future work to explore how POS/N performs\non different downstream tasks, and why POS/N\nseems slightly worse than LEARN (at least on this\nMLM task), though theoretically it provides the\ncomplete positional information for Transformers.\nThese topics will contribute to a deeper understand-\ning of positional encodings and how Transformers\nleverage positional information to succeed on dif-\nferent tasks.\n7\nDiscussion\nIn this paper, we theoretically and experimen-\ntally demonstrate that self-attention networks can\nprocess bounded hierarchical languages Dyckk,D,\neven with a memory advantage over recurrent net-\nworks, despite performing distributed processing\nof sequences without explicit recursive elements.\nOur results may explain their widespread success at\nmodeling long pieces of text with hierarchical struc-\ntures and long-range, nested dependencies, includ-\ning coreference, discourse and narratives. We hope\nthese insights can enhance knowledge about the\nnature of recurrence and parallelism in sequence\nprocessing, and lead to better NLP models.\nAcknowledgement\nWe thank Xi Chen, members of the Princeton NLP\nGroup, and anonymous reviewers for suggestions\nand comments.\nEthical Consideration\nOur work is mainly theoretical with no foreseeable\nethical issues.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016.\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nJean-Phillipe Bernardy. 2018. Can recurrent neural net-\nworks learn nested recursion? In Linguistic Issues in\nLanguage Technology, Volume 16, 2018. CSLI Pub-\nlications.\nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal.\n2020a. On the ability of self-attention networks to\nrecognize counter languages. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7096–7116.\nSatwik Bhattamishra, Arkil Patel, and Navin Goyal.\n2020b. On the computational power of transformers\nand its implications in sequence modeling. In Pro-\nceedings of the 24th Conference on Computational\nNatural Language Learning, pages 455–475, Online.\nAssociation for Computational Linguistics.\nJonathan R Brennan and John T Hale. 2019. Hierarchi-\ncal structure guides rapid linguistic predictions dur-\ning naturalistic listening. PloS one, 14(1):e0207741.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 76–86, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nNoam Chomsky. 1956. Three models for the descrip-\ntion of language. IRE Transactions on information\ntheory, 2(3):113–124.\nNoam Chomsky and Marcel P Schützenberger. 1959.\nThe algebraic theory of context-free languages. In\nStudies in Logic and the Foundations of Mathemat-\nics, volume 26, pages 118–161. Elsevier.\nSreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992.\nLearning context-free grammars: Capabilities and\nlimitations of a recurrent neural network with an ex-\nternal stack memory. In Proceedings of The Four-\nteenth Annual Conference of Cognitive Science So-\nciety. Indiana University, page 14. Citeseer.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJavid Ebrahimi, Dhruv Gelda, and Wei Zhang. 2020.\nHow can self-attention networks recognize Dyck-n\nlanguages? In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 4301–\n4306, Online. Association for Computational Lin-\nguistics.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nStefan L Frank, Rens Bod, and Morten H Christiansen.\n2012.\nHow hierarchical is language use?\nPro-\nceedings of the Royal Society B: Biological Sciences,\n279(1747):4522–4531.\nStefan L Frank and Morten H Christiansen. 2018. Hi-\nerarchical and sequential processing of language: A\nresponse to: Ding, melloni, tian, and poeppel (2017).\nrule-based and word-level statistics-based process-\ning of language: insights from neuroscience. lan-\nguage, cognition and neuroscience. Language, Cog-\nnition and Neuroscience, 33(9):1213–1218.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics,\n8:156–171.\nJie Hao, Xing Wang, Baosong Yang, Longyue Wang,\nJinfeng Zhang, and Zhaopeng Tu. 2019. Modeling\nrecurrence for transformer. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1198–1207, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMarc D Hauser, Noam Chomsky, and W Tecumseh\nFitch. 2002.\nThe faculty of language:\nwhat is\nit, who has it, and how did it evolve?\nscience,\n298(5598):1569–1579.\nHan He and Jinho D Choi. 2019. Establishing strong\nbaselines for the new decade: Sequence tagging,\nsyntactic and semantic parsing with bert.\narXiv\npreprint arXiv:1908.04943.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV, USA, June 27-30, 2016, pages 770–778.\nIEEE Computer Society.\nJohn Hewitt, Michael Hahn, Surya Ganguli, Percy\nLiang, and Christopher D. Manning. 2020. RNNs\ncan generate bounded hierarchical languages with\noptimal memory. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1978–2010, Online. As-\nsociation for Computational Linguistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nLifeng Jin,\nFinale Doshi-Velez,\nTimothy Miller,\nWilliam Schuler, and Lane Schwartz. 2018.\nUn-\nsupervised grammar induction with depth-bounded\nPCFG. Transactions of the Association for Compu-\ntational Linguistics, 6:211–224.\nFred Karlsson. 2007. Constraints on multiple center-\nembedding of clauses. Journal of Linguistics, pages\n365–392.\nGuolin Ke, Di He, and Tie-Yan Liu. 2021. Rethink-\ning the positional encoding in language pre-training.\nIn International Conference on Learning Represen-\ntations, (ICLR 2021).\nSamuel A Korsky and Robert C Berwick. 2019. On\nthe computational power of rnns.\narXiv preprint\narXiv:1906.06349.\nStephen C Levinson. 2014. Pragmatics as the origin of\nrecursion. In Language and recursion, pages 3–13.\nSpringer.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame:\nGetting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nChristopher D Manning, Kevin Clark, John Hewitt, Ur-\nvashi Khandelwal, and Omer Levy. 2020.\nEmer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences, 117(48):30046–30054.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels.\nIn 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nWilliam Merrill, Gail Weiss, Yoav Goldberg, Roy\nSchwartz, Noah A. Smith, and Eran Yahav. 2020. A\nformal hierarchy of RNN architectures. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 443–459, On-\nline. Association for Computational Linguistics.\nMatthew J Nelson, Imen El Karoui, Kristof Giber,\nXiaofang Yang, Laurent Cohen, Hilda Koopman,\nSydney S Cash, Lionel Naccache, John T Hale,\nChristophe Pallier, et al. 2017.\nNeurophysiolog-\nical dynamics of phrase-structure building during\nsentence processing.\nProceedings of the National\nAcademy of Sciences, 114(18):E3669–E3678.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learn-\ning Music Helps You Read: Using transfer to study\nlinguistic structure in language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6829–6839, Online. Association for Computational\nLinguistics.\nJorge Pérez, Javier Marinkovic, and Pablo Barceló.\n2019. On the turing completeness of modern neural\nnetwork architectures. In 7th International Confer-\nence on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAli Rahimi and Benjamin Recht. 2007. Random fea-\ntures for large-scale kernel machines. In Advances\nin Neural Information Processing Systems 20, Pro-\nceedings of the Twenty-First Annual Conference on\nNeural Information Processing Systems, Vancouver,\nBritish Columbia, Canada, December 3-6, 2007,\npages 1177–1184. Curran Associates, Inc.\nLuzi Sennhauser and Robert Berwick. 2018. Evaluat-\ning the ability of LSTMs to learn context-free gram-\nmars.\nIn Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 115–124, Brussels, Bel-\ngium. Association for Computational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2018. Disan: Di-\nrectional self-attention network for rnn/cnn-free lan-\nguage understanding. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5446–5455. AAAI Press.\nVighnesh Leonardo Shiv and Chris Quirk. 2019. Novel\npositional encodings to enable tree-based transform-\ners. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n12058–12068.\nMark Steijvers and Peter Grünwald. 1996. A recurrent\nnetwork that performs a context-sensitive prediction\ntask. In Proceedings of the 18th annual conference\nof the cognitive science society, pages 335–339.\nMirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov,\nand Stuart M Shieber. 2019. Memory-augmented re-\ncurrent neural networks can learn generalized dyck\nlanguages. arXiv preprint arXiv:1911.03329.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline.\nIn\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hi-\nerarchical structure.\nIn Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4731–4736, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nBenyou Wang, Donghao Zhao, Christina Lioma, Qi-\nuchi Li, Peng Zhang, and Jakob Grue Simonsen.\n2020. Encoding word order in complex embeddings.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019.\nHuggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019. Assessing\nthe ability of self-attention networks to learn word\norder.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3635–3644, Florence, Italy. Association\nfor Computational Linguistics.\nXiang Yu, Ngoc Thang Vu, and Jonas Kuhn. 2019.\nLearning the Dyck language with attention-based\nSeq2Seq models. In Proceedings of the 2019 ACL\nWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 138–146, Florence,\nItaly. Association for Computational Linguistics.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat,\nSashank J. Reddi,\nand Sanjiv Kumar.\n2020.\nAre transformers universal approximators\nof sequence-to-sequence functions?\nIn 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nYu Zhang, Houquan Zhou, and Zhenghua Li. 2020.\nFast and accurate neural CRF constituency parsing.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI\n2020, pages 4046–4053. ijcai.org.\nA\nConstruction Details of Section 5.1\nWe provide missing details on the construction of\n(D + 1)-layer Transformer with hard attention. In\nparticular, we prove that neural networks are capa-\nble of simulating logic gates: AND, OR, NOT, SAME\nand arithmic gates: GREATERTHAN and EQUAL\ngate. For input x, y ∈R, the GREATERTHAN sat-\nisﬁes that GREATERTHAN(x, y) = 1 if x ≥y + c\nand GREATERTHAN(x, y) = 0 when x < y; the\nEQUAL gate satisﬁes EQUAL(x, y) = 1 if x = y\nand EQUAL(x, y) = 0 when x < y−c or x > y+c.\nHere c is a constant independent of x, y.\nLemma A.1. A constant layer neural network can\nsimulate logic gates: AND, OR, NOT, SAME and\narithmic gates: GREATERTHAN, EQUAL.\nProof. Our construction is as follows.\n(1) AND gate. Given input x1, . . . , xm ∈{0, 1},\nwe compute z = max{x1 +· · ·+xm −m+1, 0}.\nWe conclude that z = 1 iff x1 = · · · = xm = 1\nand z = 0 otherwise.\n(2) NOT gate. Given input x ∈{0, 1}, it sufﬁces\nto compute z = max{1 −x, 0}.\n(3) OR gate. Given input x1, . . . , xm ∈{0, 1},\nwe compute z = max{1 −max{1 −x1 −· · · −\nxm, 0}, 0}. It is easy to see that z = 1 iff one of\nxi = 1 (i ∈[m]) and z = 0 otherwise.\n(3) SAME gate. Given input x1, . . . , xm ∈{0, 1}\nand y1, . . . , ym ∈{0, 1}.\nThe SAME gate is\nequivalent to z = ((x1 ∨y1) ∧(x1 ∨y1)) ∨· · · ∨\n((xm ∨ym) ∧(xm ∨ym)). We can construct it us-\ning logic gates: AND, OR, NOT .\n(4) GREATERTHAN gate. Given x, y ∈R, com-\npute z1 = 1\nc max{c −max{x −y, 0}, 0}, we have\nthat z1 = 0 when x > y + c and z = 1 when\nx ≤y. Taking z = max{1 −z1, 0} completes the\nconstruction.\n(5) EQUAL gate.\nGiven x, y\n∈\nR.\nLet\nz1\n=\nGREATEREQUAL(x, y)\nand\nz2\n=\nGREATEREQUAL(y, x). It sufﬁces to take z =\n¬z1 ∧¬z2.\nWe have proved a (D + 1)-layer Transformer\nwith hard attention can recognize Dyckk,D. We\nnext extend the construction for recognition task\nto generation task and prove that a D layer Trans-\nformer is capable of generating Dyckk,D.\nCorollary A.2. ∀k, D ∈N+, there exists a D-\nlayer hard-attention network that can generate\nDyckk,D. It uses both a future-position masking\nhead and a past-position masking head, a O(log k)\nmemory size, and O(log n) precision for process-\ning input length up to n.\nProof. In the generation task, we are given an input\nsegment w = γw1w2 . . . wT (γ is the start token)\nand we want to generate the next symbol wT+1.\nWe augment the input with a generation token ν\nat the end and feed input γw1w2 . . . wT ν to the\nTransformer, The output probability distribution is\ncontained in yT+2,ℓ, i.e., the internal representation\nof the generation token ν at the last layer.\nIn the generation task, we add two extra parts\nfor the representation at position i of layer ℓ\nand augment the position encoding:\nxi,ℓ\n=\n[ti, oi, pi, mi,ℓ, ei,ℓ, emi,ℓ, ci,ℓ].\nIn particular, we\nadd emi,ℓand ci,ℓto record the matched open\nbracket and the depth of generation token sepa-\nrately. The position encoding consists of three\nparts: pi1 =\ni\nn, pi2 = i and pi3 = n, where n\ndenotes an upper bound on the length of sequence\nwe aim to generate.\nThe ﬁrst D layers are the same as the recogni-\ntion task, except that we also record the matched\nopen bracket of the generation token and count its\ndepth. For ease of presentation, we assume there\nis a special bit gi contained in ti that indicates the\ngeneration token. We add the following operations\nto each FNN layer,\nci,ℓ= ci,ℓ−1 + m′\ni\nemi,ℓ= emi,ℓ−1 ∧em′\ni\nwhere m′\ni ∈{0, 1}, em′\ni ∈{0, 1}⌈log k⌉obeys\nm′\ni = gi ∧((oi ∧¬oj2 ∧s2) ∨(¬oi ∧oj1 ∧s1))\nem′\ni = gi ∧¬mi ∧oj1 ∧tj1\ns1 = SAME(ti, tj1)\ns2 = SAME(ti, tj2).\nHere m′\ni indicates whether there is a matched\nbracket in the ℓ-th layer,\nem′\ni equals the ﬁrst\nmatched open bracket for generation token, and\nit is 0 otherwise. We also slightly abuse of no-\ntation in the above equations and when we write\nc = a∧b ∈{0, 1}q for a boolean value a ∈{0, 1}\nand a boolean-valued vector b ∈{0, 1}q, we means\nci = a ∧bi (i ∈[q]), i.e. we perform coordinate-\nwise logic operations.\nWe also need to make some modiﬁcations to the\nlast FNN-layer. Ideally, we can choose between\nk open brackets and the matched close bracket,\nbut we also need to consider some boundary case,\nincluding (1) the depth of the generation token\nreaches the maximum, i.e. ci,L = D, (2) the length\nof the sequence is about to reach the maximum,\ni.e. i + ci,L = n. We implement the last layer as\nfollow.\nyi = [eoi, zi, zi]\neoi = ¬(ci,L = D) ∨¬(pi2 = ci,L + pi3)\nzi = ¬(ci,L = 0) ∧emi,D+1\nzi = = 1 −zi.\nThe ﬁnal output is determined by on V yT+2,\nwhere V ∈R2k×2⌈log k⌉+1 satisﬁes Vi,1 = 0 and\nVi,1: is the binary encoding of the i-th close bracket\nand its complement when i ∈{1, · · · , k}; Vi,1 =\n⌈log k⌉and Vi,j = 0 when i ∈{k + 1, · · · , 2k}\nand j > 1. Let S ⊆[2k] denote the index of valid\noutput, we conclude that (V yT+2)i = ⌈log k⌉for\ni ∈S and (V yT+2)i ≤⌈log k⌉−1 for i /∈S.\nSoft attention\nBoth Theorem 4.1 and Corol-\nlary A.2 can be adapted to soft attention, by setting\nthe temperature parameter η in softmax operator\nto be sufﬁcient large, say η = Ω(n log nD). Then\none can use soft attention to simulate hard attention.\nIn order to ﬁt the precision, for the soft attention\ndistribution p = [p1, · · · , pm], we round pi to the\nclosest multiple of\n1\nCn, where C is a large constant.\nB\nConstruction details of Section 5.2\nWe provide missing details of the construction in\nSection 5.2.\nB.1\nFirst Layer FFN\nRecall the output of the ﬁrst attention layer is\nai,1 = [ti, oi, pi, di,1], where ti, oi, pi are the\nbracket type embedding, the bracket openness bit\nand the position encoding. di,1 ∈R2 contains the\ninformation di/i, where di = d(w1:i) equals the\ndepth at position i. For ease of presentation, we\nassume it also contains an entry with 1/i, this can\nbe derived with an extra attention head in the ﬁrst\nlayer or be inherited from an extra position encod-\ning. Deﬁne θ(d) = arctan\n\u0010\nd\nD+2−d\n\u0011\n. We prove\nLemma B.1. With residual connection and layer\nnormalization, a two-layer MLP can perform the\nfollowing transformation\n(di/i, 1/i) 7→di = (cos(θ(di)), sin(θ(di)))\nwhile keeping ti, oi, pi unchanged.\nProof. Consider the following series of operations.\n\u0012\nti, oi, pi, di\ni , 1\ni , 0, 0\n\u0013\n7→\n\u0012\n0, 0, 0, −di\ni , di −D −2\ni\n, di\ni , D + 2 −di\ni\n\u0013\n7→\n\u0012\n0, 0, 0, −1\n2 sin(θ(di)), −1\n2 cos(θ(di)),\n1\n2 sin(θ(di)), 1\n2 cos(θ(di))\n\u0013\n7→\n\u0012\n0, 0, 0, 0, 0, 1\n2 sin(θ(di)), 1\n2 cos(θ(di))\n\u0013\n7→\n\u0012\nti, oi, pi, di\ni , 1\ni , 1\n2 sin(θ(di)), 1\n2 cos(θ(di))\n\u0013\n7→(ti, oi, pi, cos(θ(di)), sin(θ(di)), 0, 0))\nThe ﬁrst steps can be achieved with a linear trans-\nformation, the second step can be achieved by layer\nnormalization and the third step follows from the\nReLU activation gate, the fourth step comes from\nthe residual connection and the last step can be ob-\ntained with an extra layer of MLP. We conclude the\nproof here.\nB.2\nSecond Layer FFN\nWe can choose between k open brackets and the\nmatched close bracket, with the exception on a\nfew boundary cases: (1) The depth of the current\nbracket reaches the maximum; (2) The length of\nthe sequence is about to reach the maximum. Let\nemi be the bracket type of the matched bracket at\nposition i, we implement the last layer as follow.\nyi = [oi, zi, zi]\noi = ¬(di1 = sin(θ(D))) ∧¬(di1 = sin(θ( eD)))\neD = min{n −i, D + 1}\nzi = ¬(di1 = 0) ∧emi\nzi = 1 −zi.\nWe elaborate on a few details here. (1) We can\nderive the term sin(θ( eD)) via the similar method\nin Lemma B.1. (2) Since | sin(θ(i))−sin(θ(j))| =\nΩ\n\u0000 1\nD2\n\u0001\nholds for any i ̸= j ∈{0, 1, · · · , D + 1},\nwe know that the input gap (i.e. the constant c\nin Lemma A.1) for of all three EQUAL gates is\nat least Ω\n\u0000 1\nd2\n\u0001\n. Thus we can apply Lemma A.1.\n(3) We can obtain n −i by either augmenting the\nposition encoding with n and i, or normalizing\n(i/n, 1 −i/n) (see Lemma B.1).\nOutput mechanism\nThe ﬁnal output is deter-\nmined by on V yT+2, where V ∈R2k×2⌈log k⌉+1\nsatisﬁes Vi,1 = 0 and Vi,1: is the binary encod-\ning of the i-th close bracket and its complement\nwhen i ∈{1, · · · , k}; Vi,1 = ⌈log k⌉and Vi,j = 0\nwhen i ≤{k + 1, · · · , 2k} and j > 1.\nLet\nS ⊆[2k] denote the index of valid output, we\nconclude that (V yT+2)i = ⌈log k⌉for i ∈S and\n(V yT+2)i ≤⌈log k⌉−1 for i /∈S.\nB.3\nExtension to Recognition task\nOur construction can be adapted to recognition task\nwith some extra efforts. For recognition task, we\nconstruct a three-layer Transformer.\nCorollary B.2. For all k, D ∈N+, there exists\na 3-layer soft-attention network that can generate\nDyckk,D. It uses future positional masking, posi-\ntional encoding of form i/n for position i, O(log k)\nmemory size per layer, and O(log n) precision\nwhere n is the input length. The feed-forward\nnetworks use residual connection and layer nor-\nmalization.\nProof. For any input w1:n ∈γσ⋆ω, the repre-\nsentation at position i of layer ℓhas six parts\nxi,ℓ= [ti, oi, pi, mi,ℓ, ei,ℓ, di,ℓ] which are bracket\ntype embedding ti, bracket openness bit oi, posi-\ntion embedding pi, the matching bit mi,ℓ, the error\nbit ei,ℓ, the depth information di.\nThe ﬁrst attention layer is identical to the recog-\nnition task and we obtain di/i. There are two\ndifference in the upcoming FNN layer for the\nrecognition task.\nFirst, we need to make sure\ndi ∈{0, 1, · · · , D}. Hence, our ﬁrst step is to\nclip this value. Taking\nd(1)\ni\n= max\n\u001adi\ni + 1\ni , 0\n\u001b\nd(2)\ni\n= max\n\u001aD + 2\ni\n−d(1)\ni , 0\n\u001b\nd(3)\ni\n= max\n\u001aD + 2\ni\n−d(2)\ni , 0\n\u001b\n:=\nedi\ni ,\nwe have that\nedi =\n\n\n\n0\ndi < 0\ndi + 1\ndi ∈{0, 1, · · · , D}\nD + 2\ndi ≥D + 1.\nUsing the transformation in Lemma B.1, we turn\nthis into (di1, di2) = (sin(θ(edi)), cos(θ(edi)). We\nmake use of the EQUAL gate and set\nei = (di1 = 1) ∨(di1 = 0).\nSecond, for recognition task, we match a close\nbracket at position i to the closest position j that\nhas an open bracket and satisﬁes di−1 = dj. To\nget representation of (sin(θ(di−1)), cos(θ(di−1))),\nwe ﬁrst obtain di3 · · · di6\n=\n(sin(θ(di −\n1)), cos(θ(di −1)), sin(θ(di + 1)), cos(θ(di + 1))\nand set\ndi7 = max{di3 −(1 −oi), 0} + {di5 −oi, 0}\ndi8 = max{di4 −(1 −oi), 0} + {di5 −oi, 0}\nThe second self-attention layer has a depth\nmatching attention head Attmatch, with query, key,\nvalue vector as Qxi = [20D2 · di−1, 1, 1] ∈R4,\nKxi = [di, pi, oi] ∈R4 and V xi = xi so that the\nattention score\n⟨Qxi, Kxj⟩= 20D2di−1 · dj + j/n + 2oj\n(\n= 20D2 + 2 + j/n\ndi−1 = dj, oj = 1\n≤20D2 + 1\notherwise\nIt achieves its maximum when wj (j < i) is\nthe open bracket closest to wi with depth dj =\ndi−1. The attention output is ai = [aid\ni , amatch\ni\n] =\n[xi, xj] where j = max{j < i|di−1 = dj ∨oj =\n1}. With this attention output, we can easily check\nwhether two brackets are matched, and we use the\nﬁnal layer to segregate the error bits and matching\nbits. This part is identical to the construction in\nTheorem 4.1.\nB.4\nExtension to Dyckk\nWe extend the above construction to recognize lan-\nguage Dyckk. Our construction bypasses the lower\nbound in Hahn (2020) since the layer normalization\noperation is not constant Lipschitz (it can be O(n)\nin the proof). Our result is formally stated below,\nwe present the detailed construction for complete-\nness.\nTheorem B.3 (Soft-attention, Dyckk generation).\nFor all k ∈N+, there exists a 2-layer soft-attention\nnetwork that can generate Dyckk. It uses future po-\nsitional masking, O(log k) memory size per layer,\nand O(log n) precision where n is the input length.\nThe feed-forward networks use residual connection\nand layer normalization.\nRepresentation\nThe representation at position i,\nlayer ℓhas four parts xi,ℓ= [ti, oi, pi, di,ℓ]. with\nbracket type embedding ti, bracket openness bit\noi, position encoding pi and depth information\ndi,ℓ∈R3. The position encoding contains three\nparts: pi1 = i/n3, pi2 = i/n and pi3 = n. For\nconvenience of stating this construction, we assume\nifuture position masking heads tokens can also at-\ntend to itself, i.e. j > i is masked for i.\nFirst Layer – Depth Counting\nThe ﬁrst self-\nattention layer has four heads, where an Attid\nhead is used to inherit ti, oi, pi, and a future posi-\ntion masking head Attd aims to count depth with\nQxi = Kxi = 1 and V xi = 2oi −1, resulting\nin uniform attention scores and attention output\nad\ni = P\nj≤i\n1\ni · (2oj −1) = d(w1:i)/i. The third\nand fourth attention heads aim to reveals position\ninformation and they generate n/i and 1/i sepa-\nrately. We set Qxi = Kxi = 1 and V xi = n (resp.\nV xi = 1), resulting in uniform attention scores\nand attention output n/i (resp. 1/i).\nOur next step is to transform\ndi/i 7→di = (cos(θ(di)), sin(θ(di)))\nwhere θ(d) = arctan\n\u0010\nd\nn+2−d\n\u0011\nhas an unique\nvalue for every d ∈{0, · · · , n + 1}, so that\ndi · dj\n(\n= 1\ndi = dj\n< 1 −\n1\n10n2\ndi ̸= dj\nThis step can be done similarly as Lemma B.1\nby replacing D with n, this can be done because\nwe have n/i after the ﬁrst attention layer. The\nrepresentation by the end of ﬁrst layer is xi,1 =\n[ti, oi, pi, di].\nSecond layer – Depth Matching\nThe second\nself-attention layer has a depth matching hard-\nattention head Attmatch, with query, key, value\nvectors as Qxi = [20di, 1, 2] ∈R4, Kxi =\n[di, pi1, oi] ∈R4, V xi = xi, so that attention\nscore\n⟨Qxi, Kxj⟩= di · dj + j/n3 + 2oj\n(\n= 20 + j/n3 + 2\ndi = dj, oj = 1\n≤20 −1/n2 + 2\notherwise\nwould achieve its maximum when wj (j ≤i) is the\nopen bracket (or start token) closest to wi with dj =\ndi. So the attention output is ai = [aid\ni , amatch\ni\n] =\n[xi, xj] where j = max{j ≤i|di = dj ∧oj = 1}.\nIn the second FNN, we choose to generate\namong k open brackets and a matched close\nbracket, with the exceptions on some boundary\ncases. Let emi be the bracket type of the matched\nbracket at position i, we implement the last layer\nas follow.\nyi = [oi, zi, zi]\noi = ¬(di1 = sin(θ(n −i)))\nzi = ¬(di1 = 0) ∧emi\nzi = 1 −zi\nWe elaborate on some details here. First, we can\nobtain (sin(θ(n −i)), cos(θ(n −i))) by apply-\ning the normalization trick in Lemma B.1 on in-\nput (1 −pi2, pi2) = (1 −i\nn, i\nn). Second, there\nare two EQUAL gates in the above construction.\nThe input gap for these gates, however, can be\nas small as O(1/n2). Hence we can not directly\napply Lemma A.1.\nFortunately, we also know\nthat the input gap is at least Ω\n\u0000 1\nn2\n\u0001\n.\nDenote\n(a −b)+ = max{a −b, 0} and take x1 = (a −\nb)+, x2 = ( 1\nn2 −(a−b)+)+, x3 = (b−a)+, x4 =\n( 1\nn2 −(b −a)+)+. Let (ex1, ex2) be the the normal-\nized version of (x1, x2) and (ex3, ex4) be the nor-\nmalized version of (x3, x4), this can realized simi-\nlarly as Lemma B.1. Then we know that if a = b,\nthen ex1 = ex3 = 0. On the contrary, if |a −b| ≥\nΩ(1/n2), max{ex1, ex3} ≥Ω(1). Hence we can\nset EQUAL(a, b) = EQUAL(ex1, 0)∧EQUAL(ex3, 0).\nThis concludes the construction.\nThe ﬁnal output is determined by on V yT+2,\nwhere V ∈R2k×2⌈log k⌉+1 satisﬁes Vi,1 = 0 and\nVi,1: is the binary encoding of the i-th close bracket\nand its complement when i ∈{1, · · · , k}; Vi,1 =\n⌈log k⌉and Vi,j = 0 when i ≤{k + 1, · · · , 2k}\nand j > 1. Let S ⊆[2k] denote the index of valid\noutput, we conclude that (V yT+2)i = ⌈log k⌉for\ni ∈S and (V yT+2)i ≤⌈log k⌉−1 for i /∈S. We\nconclude the construction here.\nC\nTheoretical limits for ﬁnite position\nencoding\nWe prove that a Transformer with ﬁnite preci-\nsion can not recognize Dyckk,D language.\nIn\nfact, we show a stronger result: no transformer\nwith o(log n) precision can recognize Dyckk,D lan-\nguage of length more than n.\nTheorem C.1 (Formal statement of Theorem 4.3).\nFor any k ∈N, using hard attention, no trans-\nformer with o(log n) encoding precision can rec-\nognize Dyckk,2 language with input length n.\nOur proof is inspired by Hahn (2020) but with\nseveral different technique ingredient: (1) we allow\narbitrary attention masking (both future and past\nposition masking); (2) we allow arbitrary position\nencoding (3) our lower bounds holds for bounded\ndepth language Dyckk,D; (4) we provide an quanti-\ntative bound for precision in terms of input length\nn. In general, our lower bound is incomparable\nwith Hahn (2020), we prove a ﬁne grained bound\non the precision requirement for bounded depth\nlanguage Dyckk,D, while the proof in Hahn (2020)\napplies only for language with Depth Ω(n) but al-\nlows arbitrary precision on position encoding.\nThe high level intuition behind our proof is that\nthe attention head can only catch o(n) input posi-\ntions when we properly ﬁx a small number of sym-\nbol in the input sequence. This limits the capability\nof a Transformer and makes it fail to recognize\nDyckk,D language.\nWe consider a L-layer transformer and assume\n3H attention heads in total: H normal attention\nheads, H attention heads with future position mask-\ning, H attention heads with past position mask-\ning. To make our hardness result general, we allow\nresidual connection for the attention layer, and we\nassume the FNN can be arbitrary function deﬁning\non the attention outcome. In the proof, we would\ngradually ﬁx o(n) positions of the input sequence.\nWe only perform the follow two kinds of assign-\nment (1) we assign matching brackets to position\ni, i + 1 where i is odd; (2) we assign matching\nbrackets (e.g., we assign ‘[’, ‘(’, ‘)’, ‘]’) to position\ni, i + 1, i + 2, i + 3 for odd i. A partial assignment\nto the input sequence is said to be well-aligned if it\nfollows these two rules. Throughout the proof, we\nguarantee that for any i ∈[n], ℓ∈[L], the output\nof the ℓ-th layer xi,ℓdepends only the input symbol\nat position i. This is clearly satisﬁed for ℓ= 0,\ngiven the it is composed by position embedding\nand word embedding only. We gradually ﬁx the\ninput and conduction induction on ℓ. We use cℓto\ndenote the number of positions we ﬁxed before the\nℓ-th layer, and we use sℓto denote the number of\nconsecutive assigned blocks of the input sequence.\nIt is clear that sℓ≤2cℓ. The following Lemma is\nkey to our analysis.\nLemma C.2. For any ℓ∈{1, · · · , L}, given a\nwell-aligned partially assigned input sequence,\nsuppose the input of ℓ-th layer xi,ℓ−1 depends\non the symbol at position i only. Then by ﬁxing\ncℓH2(k + 1)O(ℓH)2O(ℓHp) additional positions of\nthe input sequence, we guarantee that the output of\nℓ-th layer xi,ℓalso depends solely on the symbol at\nposition i.\nProof. We ﬁrst perform some calculations on the\ntotal number of possible value for input xi,ℓ−1 and\nwe denote this number as Nℓ. For any m ∈[ℓ], we\nhave the following recursion rule. The total number\nof possible input value for m-th layer is Nm, and\ntherefore, each head in the attention layer can take\nvalue in at most Nm numbers. Given the output of\nattention layer depends only on the input xi,m and\nthe outcome of 3H attention head, the total number\nof possible output is at most N3H+1\nm\n, i.e. Nm+1 ≤\nN3H+1\nm\n. Moreover, when ℓ= 1, the number of\nposition encoding is 2p and the number of possible\nsymbol is 2k + 2. Therefore, one has N1 = (2k +\n2)2p, and thus, Nℓ≤(k + 1)O(ℓH)2O(ℓHp)\nGiven a well-aligned partially assigned input\nsequence, such that the input of ℓ-th layer xi,ℓ−1\ndepends on the symbol at position i only. We are\ngoing to restrict some additional positions in the\ninput sequence, such that the output of ℓ-th layer\nxi,ℓdepends solely on the symbol at position i. It\nsufﬁces to make the outcome of each attention head\nto depends solely on the symbol.\nWe focus on the backward head and other type\nof attention head can be treated similarly. Recall\nthe total number of possible input of ℓ-th layer\nis at most Nℓ, for h ∈[H] and w ∈[Nℓ], we\nuse Lℓ,h,w to denote the priority list of head h on\ninput w, i.e. Lh,w sorts element in [Nm] by its\nattention score with w in descending order (we\nbreak ties arbitrarily). Below, we show how to ﬁx\nO(·) positions of the input sequence such that the\nattention head h only attends to ﬁxed positions and\nits outcome sole depends on w.\nConsider the following assignment procedure.\nInitially, we start from the highest priority element\nin Lh,w and start from the beginning of the se-\nquence (i.e. position 0). We gradually clear the list\nand move towards. Let wτ be of the top priority\nelement in the current list Lh,w and let iτ be the\ncurrent position (τ ∈[Nm], i ∈[n]).\nConsider all positions j later than i (i.e. j ∈\n[n], i ≤j) whose input symbol have not been as-\nsigned and there exists an assignment such that\nxj,ℓ−1 = wτ. For simplicity, let us assume we can\nrealize xj,ℓ−1 = wτ by assigning an open bracket,\nthis is WLOG. We scan through all these indices j,\nfrom the last to the ﬁrst, and do\n1. If j is in an odd position, then we realize\nxj,ℓ−1 = wτ by assigning the open bracket,\nthen assign the same type of close bracket at\nposition j + 1. Stop and pop uτ from Lh,w.\n2. If j is in an even position, and at the same\ntime, position j + 1, j + 2 has no assignment.\nThen we assign two pairs of matched brackets\nto position j −1, j, j + 1, j + 2, fulﬁlling the\nrequirement of xj,ℓ−1 = wτ. Stop and pop uτ\nfrom Lh,w\n3. If j is in an even position, and at the same\ntime, the position j −2 has no assignment\nand can be assigned to satisfy xj−2,ℓ−1 = wτ.\nThen we assign two pairs of matched brackets\nto position j −3, j −2, j −1, j, fulﬁlling the\nrequirement of xj−2,ℓ−1 = wτ. Stop and pop\nuτ from Lh,w.\n4. Otherwise, we just assign a random pair of\nmatched bracket to position j −1 and j.\nWe set iτ+1 to be the place where we stop, and if\nthere is no such place, we just set iτ+1 = iτ. Let’s\nclarify what we achieve after this step. After this\nstep, the attention outcome for head h and input\nw is ﬁxed before position iτ+1 has all been ﬁxed.\nWhile after position iτ+1, the attention head h for\ninput w either attend to position that has already\nbeen ﬁxed, or attends to a position with priority\nless that uτ.\nWe next count the number of positions we as-\nsigned. Let sℓ,h,w,τ (τ ∈[Nℓ], h ∈[H], τ ∈[Nℓ])\nbe the number of consecutive assigned slots. We\nknow that initially sℓ≤cℓ. Our assignment pro-\ncedure guarantees that the number of slots grows\nat most one at a time, and therefore, sℓ,H,Nℓ,Nℓ≤\ncℓ+ HN2\nℓ. Furthermore, we make assignment to\nat most 2sℓ,h,w,τ + 2 positions each time. Thus the\ntotal number of position we assigned is bounded as\nX\nh,w,τ\nsℓ,h,w,τ ≲cℓ· O(HN2\nℓ) + O(H2N4\nℓ)\n≲cℓH2(k + 1)O(ℓH)2O(ℓHp).\nWe plug in Nℓ≤(k + 1)O(ℓH)2O(ℓHp) in the last\nstep.\nFinally, we remark that our partial assignment is\nstill well-aligned and after these additional assign-\nments, the output of ℓ-th layer xℓ,i depends solely\non the symbol at position i.\nProof of Theorem C.1. We apply Lemma C.2 and\ncompute the number of positions cL+1 we need to\nrestrict, in order to guarantee that the output of L-th\nlayer xi,L+1 depends only on the input at position\n(i ∈[n]). Since cℓ+1 ≤cℓH2(k + 1)O(ℓH)2O(ℓHp)\nand c1 = O(1), we have\ncL+1 ≲HO(L)(k + 1)O(L2H)2O(L2Hp).\nBy taking\nHO(L)(k + 1)O(L2H)2O(L2Hp) ≤0.01n.\nWe know the partial assigned sequence is well-\naligned, has depth at most two, and the number of\nassignment is only 0.01. Thus, we assert that that\nwhen p = o(log n), the output of Transformer is\ncompletely determined by the partial assignment\nand it do not detect whether there exists error in the\nunassigned positions and thus can not recognize\nDyckk,2 language. We conclude the proof here.\nD\nExperiment Details\nD.1\nSetup\nData\nWe follow Hewitt et al. (2020) to gener-\nate Dyckk,D by randomly sampling stack decisions\n(push, pop, or end) and maintaining length condi-\ntions (Table 1) for a O(D2) hitting time of differ-\nent DFA states. The number of tokens for train,\nvalidation, and test set is 2 × 106, 2 × 105, 106\nrespectively.\nD\n3\n5\n10\n15\nTrain/val lengths\n1:84\n1:180\n1:700\n1:1620\nTest lengths\n85:168\n181:360\n701:1400\n1621:3240\nTable 1: Input lengths for Dyckk,D with different D.\nModels\nWe use the LSTM model implemented\nin Hewitt et al. (2020). For Transformer models,\nwe turn off all drop outs as we ﬁnd them to hurt\nperformance greatly. We also use only 1 head as\nwe ﬁnd more heads to hurt performance. We use\nAdam optimizer with initial learning rate being\n0.01 or 0.001, and choose the better learning rate in\nterms of validation accuracy for each experiment.\nWe train for at most 100 epochs but allow early\nstopping if the validation loss converges.\nMetric\nWe follow Hewitt et al. (2020) and use\nthe accuracy of correct close bracket predictions:\np(⟩j|⟩) =\np(⟩j)\nP\ni p(⟩i)\nLet pl be the empirical probability that the model\nconﬁdently predicts a close bracket (deﬁned as\np(⟩j|⟩) > .8), conditioned on it being separated\nfrom its open bracket by l tokens. Unlike Hewitt\net al. (2020) where meanlpl is reported, we report\nElpl for two reasons: (i) when l is large pl might\nbe only deﬁned by one trail, thus meanlpl ampliﬁes\nthe randomness; (ii) the ﬁndings remain similar\nwith either metrics.\nD.2\nMore Results\nIn Figure 7, we show the validation performance\nfor Transformers of different positional encoding\nschemes.\nThey all reach near-perfect accuracy\nwhen having at least 2 layers.\nIn Figure 8, we break down the results in Sec-\ntion 6.2 when dmodel ∈{10, 30, 50}. We also\nadd results for a ﬁve-layer Transformer, which per-\nforms similarly as the two-layer Transformer. This\nshows (i) a two-layer Transformer, as suggested\nby our theory, is enough to process Dyckk,D, and\n(ii) Transformers with more layers can also learn\nto process Dyckk,D without overﬁtting or degraded\nperformance.\n1\n2\n3\n4\n5\n10\n# Layers\n0.6\n0.7\n0.8\n0.9\n1.0\nClose Accuracy\nTransformers \n(Dyck-(8, 10) Validation)\nPositional Encoding\ncos\nlearn\npos/N\nFigure 7: Validation results on Dyck8,10.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndev_close_acc\nk = 2 | D = 3\nModel\nLSTM (1 layer)\nTransformer (2 layers)\nTransformers (5 layers)\nk = 2 | D = 5\nk = 2 | D = 10\nk = 2 | D = 15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndev_close_acc\nk = 8 | D = 3\nk = 8 | D = 5\nk = 8 | D = 10\nk = 8 | D = 15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndev_close_acc\nk = 32 | D = 3\nk = 32 | D = 5\nk = 32 | D = 10\nk = 32 | D = 15\n10\n30\n50\nhidden_dim\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndev_close_acc\nk = 128 | D = 3\n10\n30\n50\nhidden_dim\nk = 128 | D = 5\n10\n30\n50\nhidden_dim\nk = 128 | D = 10\n10\n30\n50\nhidden_dim\nk = 128 | D = 15\nTransformers v. LSTM (Dyck-(k, D) Validation)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntest_close_acc\nk = 2 | D = 3\nk = 2 | D = 5\nk = 2 | D = 10\nk = 2 | D = 15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntest_close_acc\nk = 8 | D = 3\nk = 8 | D = 5\nk = 8 | D = 10\nk = 8 | D = 15\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntest_close_acc\nk = 32 | D = 3\nk = 32 | D = 5\nk = 32 | D = 10\nk = 32 | D = 15\n10\n30\n50\nhidden_dim\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntest_close_acc\nk = 128 | D = 3\n10\n30\n50\nhidden_dim\nk = 128 | D = 5\n10\n30\n50\nhidden_dim\nk = 128 | D = 10\n10\n30\n50\nhidden_dim\nk = 128 | D = 15\nTransformers v. LSTM (Dyck-(k, D) Test)\nFigure 8: Validation and test results on Dyckk,D (k ∈\n{2, 8, 32, 128} and D ∈{3, 5, 10, 15}). Enlarge for\ndetails.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.FL"
  ],
  "published": "2021-05-24",
  "updated": "2023-03-13"
}