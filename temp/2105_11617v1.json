{
  "id": "http://arxiv.org/abs/2105.11617v1",
  "title": "A Comparison of Reward Functions in Q-Learning Applied to a Cart Position Problem",
  "authors": [
    "Amartya Mukherjee"
  ],
  "abstract": "Growing advancements in reinforcement learning has led to advancements in\ncontrol theory. Reinforcement learning has effectively solved the inverted\npendulum problem and more recently the double inverted pendulum problem. In\nreinforcement learning, our agents learn by interacting with the control system\nwith the goal of maximizing rewards. In this paper, we explore three such\nreward functions in the cart position problem. This paper concludes that a\ndiscontinuous reward function that gives non-zero rewards to agents only if\nthey are within a given distance from the desired position gives the best\nresults.",
  "text": "A Comparison of Reward Functions in Q-Learning Applied to\na Cart Position Problem\nAmartya Mukherjee1\n1Department of Applied Mathematics, University of Waterloo\nMay 2021\nAbstract\nGrowing advancements in reinforcement learning has led to advancements in control the-\nory. Reinforcement learning has eﬀectively solved the inverted pendulum problem [1] and more\nrecently the double inverted pendulum problem [2]. In reinforcement learning, our agents learn\nby interacting with the control system with the goal of maximizing rewards. In this paper, we\nexplore three such reward functions in the cart position problem. This paper concludes that a\ndiscontinuous reward function that gives non-zero rewards to agents only if they are within a\ngiven distance from the desired position gives the best results.\nKeywords: cart position problem, control theory, reinforcement learning, reward function\n1\nIntroduction\nReinforcement Learning (RL) is a branch of Machine Learning that deals with agents that learn\nfrom interacting with an environment. This is inspired by the trial-and-error method of learning\ndealt with in psychology [1]. The goal of any RL problem is to maximize the total reward, and the\nreward is calculated using a reward function. At any point, the goal of an RL agent is to choose\nan action that maximizes not only its immediate reward, but also the total reward it expects to\nget if it follows a certain sequence of actions. The RL algorithm used in this paper is Q-Learning.\nIt is a basic algorithm used in RL, and the math used in it is easy to understand for a reader that\ndoes not have much machine learning background. It also comes with the merit that training a\nQ-Learning algorithm is signiﬁcantly faster compared to other commonly used RL algorithms.\nThe cart position problem is a toy problem that will be explored in this paper. The goal is to\nmove a cart from a position x = 0 to a position x = r. This will be done by adjusting the voltage\nof the cart at every time step. Currently there exists theoretical input functions that solve the cart\nposition problem. It will be interesting to see how RL compares with these functions.\nThe objective of this paper is to compare three diﬀerent reward functions based on the per-\nformance of the RL agent. The comparison is done in three ways. First, we see which RL agent\nreaches steady-state motion the earliest. Second, we see which RL agent has the lowest variability\nin its steady-state motion. Third, we see which RL agent is the closest to x = r in its steady-state\nmotion.\nThis paper is organized as follows. In section 2, we describe the cart position problem. In\nsection 3, we present the RL algorithm and the reward functions we intend to use. In section 4, the\n1\narXiv:2105.11617v1  [cs.LG]  25 May 2021\ntheoretical solution to the cart position problem is explained. In section 5, we present the results of\nthe RL algorithm and compare them. In section 6, we discuss the applicability of the RL algorithm\nin real life situations.\n2\nCart Position Problem\nThis paper concerns the control of the position of a cart by a rotary motor. Let x(t) be the position\nof a cart at time t and V (t) the motor voltage at time t. ηg is the gearbox eﬃciency, Kg the gearbox\ngear ratio, ηm the motor eﬃciency, Kt the motor torque constant, rmp the motor pinion radius, Km\nthe back-EMF constant, Rm the motor armature resistance, ν the coeﬃcient of viscous friction,\nand M the mass of the cart. These are all constants. The equation for this control system is a\nsecond-order diﬀerential equation shown in equation 1 below:\nM ¨x(t) = ηgKgηmKt(rmpV (t) −KgKm ˙x(t))\nRmr2mp\n−ν ˙x(t)\n(1)\nThroughout the course of this paper, we can deﬁne constants α and β as:\nα = −ηgK2\ngηmKtKm\nMRmr2mp\n−ν\nM\nβ = ηgKgηmKtrmp\nMRmr2mp\nThis simpliﬁes our governing equation to equation 2 below:\n¨x(t) = α ˙x(t) + βV (t)\n(2)\nWhere α < 0 and β > 0. In the absence of any control input V (t), the solution to the diﬀerential\nequation is:\nx(t) = C1eαt + C2\nIn this model, α < 0, so limt→∞x(t) = C2. The steady-state position is C2. The goal of the\ninput function V (t) is to change the steady-state position to a position of our choice.\nWe can express our governing equation as a system of ﬁrst order diﬀerential equations. Let\ns(t) = ˙x(t) represent the velocity of the cart. The system of ﬁrst order equations is expressed\nbelow:\n˙x(t) = s(t)\n˙s(t) = αx(t) + βV (t)\nLet X(t) =\n\u0014x(t)\ns(t)\n\u0015\n. This system of ﬁrst order equations can be expressed in matrix form, as\nshown below:\n˙X(t) =\n\u00140\n1\n0\nα\n\u0015\nX(t) +\n\u00140\nβ\n\u0015\nV (t)\n2\nWe also know that the objective of our control problem is to drive the cart to a particular\nposition. For this reason, we want our output y(t) to be the position x(t). To express this in a\nmatrix form involving X(t), this can be written as:\ny(t) =\n\u00021\n0\u0003\nX(t)\nThe state-space form of our equation is:\n˙X(t) =\n\u0014\n0\n1\n0\nα\n\u0015\nX(t) +\n\u0014\n0\nβ\n\u0015\nV (t)\n(3a)\ny(t) =\n\u00021\n0\u0003\nX(t) + [0]V (t)\n(3b)\nWhere X(t) is the state, V (t) is the input, y(t) is the output. The objective of the cart position\nproblem is to ﬁnd an input function V (t) so that the steady-state value of x is r. And this will be\ndone using RL.\n3\nUse of RL in the Cart Position Problem\nThe initial conditions used in the control system are: x(0) = ˙x(0) = 0. In this paper, we will let\nα = −1, β = 10, r = 10. The solution to Equation 3 is computed numerically using Euler’s method\nwith a step-size of δt = 0.2s.\nAt each time step, the RL agent takes the output of the control system, which is the position\nx(t) of the cart, and returns the voltage V (t) that should be used as the input to the control system\nat that time step. The voltage V (t) is an integer between −5 and 5. In order to ensure that the\ncart does not go too far from x = r, the bounds of x has been set to [−10, 20].\nThe training process involves running 100 samples. Each sample has 30 rounds. When a round\nstarts, the cart is at position x = 0. The round ends if the cart goes out of bounds (i.e. x ̸∈[−10, 20])\nor 50 time steps have passed since the start of the round. This means the training process of the\nRL agent involves observing at most 150000 time steps.\nThe objective of the RL agent is to ﬁnd a sequence of voltage inputs such that the total reward\nis maximized.\n3.1\nQ-Learning\nThe RL algorithm used in this paper is Q-Learning. Q-Learning was ﬁrst coined by Watkins (1989)\nin his PhD thesis [3]. It ﬁnds an optimal policy Q(s, a) where s is the state and a is the action.\nThis optimal policy maximizes the total reward in the system. Q(s, a) can be thought of a table of\nvalues for all states and actions.\nAt ﬁrst, Q(s, a) is initialized to 0 for all s, a. At every time step t, the action at is determined\nfrom the state st through an ϵ-greedy process. In order to ﬁnd the best action at from Q(s, a), we\nmust ﬁrst use a random process to ﬁgure out the result of the action. When the training of the\nagent starts, we let ϵ = 1. At every time step, we take a random sample from the Uniform(0, 1)\ndistribution. If the random sample is greater than ϵ, then at = arg maxa(Q(st, a)). Otherwise, at\nis a randomly selected action.\nLet Q[n](s, a) be the policy after n updates. When st+1 is calculated using st and at, then\nQ[n](s, t) is updated using the following equation:\n3\nQ[n+1](st, at) = Q[n](st, at) + ζ(rt + γ max\na\nQ(st+1, a) −Q(st, at))\n(4)\nWhere rt is the reward, ζ is the learning rate. maxa Q(st+1, a) is a measure of the guess of\nthe future reward, and γ is a measure of the weight we give to future rewards.\nζ and γ are\nhyperparameters to this agent. The hyperparameters used in this paper are: ζ = 0.05, γ = 0.9\nIn Q(s, a), the state s refers to the position x of the cart, and the action a refers to the voltage\nV of the cart.\nThe Q-Learning code used in this paper is taken from Lin’s (2018) GitHub repository. This code\nwas originally used to solve the Blackjack problem using Q-Learning. The equation for updating ϵ\nafter every time step t according to Lin’s code is given by the equation below [4]:\nϵt+1 =\n\n\n\n\n\nϵt −ϵ0/(3ntrain)\nntrain −t > 0.7ntrain or 0.3ntrain > ntrain −t > 0\nϵt −2ϵ0/ntrain\n0.7ntrain > ntrain −t > 0.3ntrain\n0\nOtherwise\n(5)\nWhere ntrain = 30000 is the number of time steps used to train the agent.\n3.2\nReward Functions\nAs shown previously, reward functions are used to update the policy function at each time step with\nthe intention that the RL agent maximizes the total reward. This paper will explore three reward\nfunctions. The ﬁrst reward function takes the negative of the square of the distance between x and\nr with the intention that the RL agent will be rewarded better if it is closer to r.\nR(x) = −(x −r)2\n(6)\nThe second reward function is piece-wise and is always positive. This function is linear instead\nof quadratic.\nR(x) =\n(\nr\n2 −|x −r|\nr\n2 < x < 3r\n2\n0\nOtherwise\n(7)\nThe third reward function is discontinuous. It only rewards a 1 if the cart is a distance of less\nthan 1 unit away from r and it rewards a 5 if the cart is a distance of less than 0.1 units away from\nr.\nR(x) =\n\n\n\n\n\n1\nx ∈[r −1, r −0.1] ∪[r + 0.1, r + 1]\n5\nx ∈(r −0.1, r + 0.1)\n0\nOtherwise\n(8)\nTo reproduce our results, we provided the code used for this problem [7].\n4\nTheoretical solution\nConsider the following voltage function:\nV (t) = Kp(r −x(t))\n(9)\n4\nWhere Kp > 0. Substituting the voltage function into equation 2 gives us the following:\n¨x(t) = α ˙x(t) + βKp(r −x(t))\n(10)\nThis equation gives us a solution x(t) whose steady-state position is x = r [6]. The proof is\ngiven in Appendix A. Numerical simulations of Equation 10 have been done using Euler’s method\nwith Kp = 0.2 and Kp = 0.1. The plot of the two trajectories are given below:\nFigure 1: Trajectory of the cart using the theoretical solution with Kp = 0.2\nFigure 2: Trajectory of the cart using the theoretical solution with Kp = 0.1\n5\nWith Kp = 0.2, the position of the cart is within [r −1, r + 1] after 35 time steps (or 7 seconds).\nWith Kp = 0.1, the position of the cart is within [r −1, r + 1] after 23 time steps (or 4.6 seconds).\nThis theoretical solution will be compared to the solution derived from Q-Learning.\n5\nResults of Q-Learning\nIn this section, we train three RL agents using each of the reward functions provided in section 3.2.\nWe then compare the results of each of our RL agents.\n5.1\nReward Function 1\nThe ﬁrst reward function takes the negative of the square of the distance between x and r with\nthe intention that the RL agent will be rewarded better if it is closer to r, as shown in Equation\n6. Figure 3 below shows how the average reward varies with each sample. The average reward is\ncalculated by taking the mean of the total reward of each of the trajectories in the sample.\nFigure 3: Average reward plot for reward function 1\nIt is clear that, after training for 70 samples, the training is complete and the average reward is\napproximately −468. Figure 4 shows the trajectory of each cart in sample 1. These carts follow a\nrandom motion. All trajectories of carts in later samples will be compared to this plot.\n6\nFigure 4: Trajectory of each cart at sample 1 for reward function 1\nIn this graph, most of the carts go out of bounds within the ﬁrst 10 time steps. During the ﬁrst\nsample, the value of ϵ used in the ϵ-greedy algorithm is almost 1, which is why the voltage applied\nat each time step is picked randomly. Thus, the trajectory that each cart takes is random.\nAt samples 60 to 70, the average reward plot in Figure 3 shows an increasing trend. Figure 5\nshows the trajectory of each cart at sample 65.\nFigure 5: Trajectory of each cart at sample 65 for reward function 1\nIn this plot, clearly the RL agent has learned that the cart needs to move towards x = r during\nthe start of the round. The trajectories are less likely to move out of bounds. Several trajectories\nappear to move back and forth x = r. It shows a signiﬁcant improvement compared to Figure 4.\n7\nFigure 6 shows the trajectory of each cart in sample 100.\nFigure 6: Trajectory of each cart at sample 100 for reward function 1\nThis graph shows that the cart reaches a steady-state motion after 5 time steps (or 1.0 seconds).\nThe position alters between x = 10.0 and x = 8.0 and the voltage alters between V = −1 and V = 1.\nThe total reward in this trajectory is −468, which is signiﬁcantly high compared to the average\nrewards in the ﬁrst few samples shown in Figure 3.\nThe steady-state position is within ±2 of r. If V = 0 at x = 10, then the cart is expected to\ndrift past x = r, which is why V = −1 is the best action to take here.\n5.2\nReward Function 2\nThe second function gives a positive reward only if the position of the cart is in the range [ r\n2, 3r\n2 ] =\n[5, 15], as shown in Equation 7. Contrary to reward function 1 and 3, the step-size used here is\n0.1s because it leads to better results, and the number of time steps in a round is still 50. Figure 7\nbelow shows how the average reward varies with each sample.\n8\nFigure 7: Average reward plot for reward function 2\nIt is clear that, after training for 40 samples, each trajectory of the cart has a total reward of\n86. Figure 8 shows the trajectory of each cart in sample 1. These carts follow a random motion.\nAll trajectories of carts in later samples will be compared to this plot. The distance from origin is\nthe value x(t) at time t and the time step is t/δt.\nFigure 8: Trajectory of each cart at sample 1 for reward function 2\nIn this graph, most of the carts go out of bounds within the ﬁrst 40 time steps. This makes\nsense since the time step size used here is half the step size used for the ﬁrst reward function. For\nthis reason, the trajectories shown in this graph are diﬀerent from the trajectories shown in Figure\n9\n4. The trajectory that each cart takes is random because the voltage applied at each time step is\npicked randomly.\nAt samples 20 to 30, the average reward in Figure 7 shows an increasing trend. Figure 9 shows\nthe trajectory of each cart in sample 25.\nFigure 9: Trajectory of each cart at sample 25 for reward function 2\nIn this plot, it is clear the cart has learned to move towards x = r during the start of the round.\nSeveral trajectories appear to stay near x = r and the cart is less likely to go out of bounds. It\nshows a signiﬁcant improvement compared to Figure 8.\nFigure 10 shows the trajectory of each cart in sample 100.\n10\nFigure 10: Trajectory of each cart at sample 100 for reward function 2\nThis graph shows that the cart reaches a steady-state motion after 10 time steps (or 1.0 seconds).\nThe validation plot is the same as this plot. The position alters between x = 12.0, x = 13.0 and\nx = 17.0 and the voltage alters between V = 1, V = 4 and V = −5. The total reward in this\ntrajectory is 86.0, which is signiﬁcantly high compared to the average rewards in the ﬁrst few\nsamples shown in Figure 7.\nHowever, the variation in the position during the steady-state motion is too big. The steady-\nstate position is within ±7 of r. And r is not in the range of the positions in the steady-state\nmotion. This is because the agent has learned that a positive reward comes if the cart is in the\nrange [5, 15]. Thus, there is less of an incentive to stay closer to x = r since the current trajectory\nalready signiﬁcantly maximizes the total reward. This shows that the second reward function is\nnot as reliable as the ﬁrst reward function.\n5.3\nReward Function 3\nAs a measure of ensuring that the variability of the steady-state position is minimized, the third\nreward function gives a positive reward only if the position of the cart is in the range [r −1, r +1] =\n[9, 11], as shown in Equation 8. Figure 11 shows how the average reward varies with each sample.\n11\nFigure 11: Average reward plot for reward function 3\nIt is clear that, after training for 85 samples, each trajectory of the cart has a total reward of\n230. Figure 12 shows the trajectory of each cart in sample 1. These carts follow a random motion.\nAll trajectories of carts in later samples will be compared to this plot.\nFigure 12: Trajectory of each cart at sample 1 for reward function 3\nIn this graph, most of the carts go out of bounds within the ﬁrst 10 time steps. This graph\nshows similar trends to Figure 4. The direction each cart goes at the start of the round is random\nas the voltage applied at each time step is picked randomly.\nAt samples 80 to 85, the average reward in Figure 11 shows an increasing trend. Figure 13\n12\nshows the trajectory of each cart in sample 84.\nFigure 13: Trajectory of each cart at sample 84 for reward function 3\nIn this plot, it is clear that the agent has learned to move the cart towards x = r during the\nstart of the round. Trajectories are less likely to go out of bounds. In some trajectories, the cart\nmoves back and forth the interval [r −1, r + 1] multiple times in order to increase its total reward.\nFigure 14 shows the trajectory of each cart in sample 100.\nFigure 14: Trajectory of each cart at sample 100 for reward function 3\nThis graph shows that the cart reaches exactly x = r after 4 time steps (0.8 seconds) and stays\nthere. The validation plot is the same as this plot. The steady-state motion has no variability\n13\ncompared to the ﬁrst and second reward function. On the other hand, a drawback of using this\nreward function is that training this agent takes more samples compared to training RL agents that\nuse the ﬁrst or second reward function, as shown in Figure 11.\nWe ﬁrst compare which RL agent reaches steady-state motion the earliest. For the ﬁrst and\nsecond reward function, the Q-Learning agents take equally as long to reach steady-state motion\n(1.0s).\nThe Q-Learning agent using the third reward function reaches steady-state motion the\nquickest (0.8s).\nWe then compare which RL agent has the lowest variability in its steady-state motion. In the\nﬁrst reward function, the steady-state motion alternates between positions 8.0 and 10.0, thus having\na width of 2.0. In the second reward function, the steady-state motion alternates between positions\n12.0, 13.0 and 17.0, thus having a width of 5.0. In the third reward function, the steady-state motion\njust takes values of 10.0, thus having a width of 0.0. This shows that, while the ﬁrst reward function\nleads to a smaller width compared to the second reward function, the third reward function has the\nsmallest width.\nLastly, we compare which RL agent is closest to x = r in its steady-state motion. Clearly, the\nthird reward function performs the best since it’s steady state position is exactly r. And the second\nreward function performs the worst since r is not in its set of steady-state positions.\nThus, the third reward function gives the best results.\n6\nDiscussion\nWhile the previous section discussed the steady-state behaviour of the Q-Learning agent for each\nof the reward functions, this section will discuss the practicality of Q-Learning agents on real carts\nbased on the results of the third reward function.\nWe ﬁrst compare the trajectory of the Q-Learning agent shown in Figure 14 with the trajectories\nof the theoretical solutions shown in Figure 1 and 2. It is clear that the Q-Learning agent shows\nbetter performance compared to the theoretical solutions as the Q-Learning agent reached steady-\nstate motion within 0.8 seconds and the theoretical solutions reached a position within ±1 of x = r\nwithin 7 seconds and 4.6 seconds.\n6.1\nApplicability of Q-Learning on real carts\nIn this paper, the cart position problem is thought of as a toy problem where Q-Learning may be\nuseful. Suppose a real cart follows the model shown in Equation 1 and its goal is to get from a\nposition x = 0 to x = r. This means the position is updated continuously as opposed to time steps\nshown in Euler’s method. In Q-Learning, the policy function Q(s, a) picks actions based on trial-\nand-error. The set of states s in Q(s, a) are ﬁnite and discrete. Thus, Q-Learning is impractical on\na real cart where the position can be any real number. In this situation, the theoretical solution is\nmore eﬀective as it adjusts the voltage based on a continuous set of positions.\nHaving a discrete set of actions can also be undesirable in this problem. In the situation with\nreward function 1 (Figure 6), it is clear that V has to alter between −1 and 1 in order to maintain\na steady-state motion. If V = 0 at x = 10, then the cart will drift away from x = 10 since its\nvelocity is non-zero. Thus, further areas of exploration involve adding values of V with a smaller\nmagnitude (i.e. 0.1, 0.2) into the range of actions the Q-learning agent could take.\nFurther areas of exploration also involve using Deep Q learning.\nArtiﬁcial neural networks\n(ANNs) are trained by modifying weights rather than entries in a Q(s, a) table, thus, ANNs may\n14\nbe more reliable in problems involving continuous states or continuous actions.\nConsider the trajectory of the Q-Learning agent shown in Figure 14. Between time t = 0 and\nt = δt = 0.2, the velocity of the cart increases from ˙x = 0ms−1 to ˙x = 40ms−1. This means the\nacceleration is approximately ¨x = 200ms−2. In a real cart, this could be dangerously high, given\nthat cars that have an acceleration of approximately 10ms−2 to 20ms−2 are considered one of the\nfastest accelerating cars [5]. This shows that the Q-Learning agent is impractical on real carts.\nLastly, in real world carts, there is always an error associated with measuring the mass of the\ncart, the coeﬃcient of friction, or any of the constants described in Equation 1. This means there\ncan be an error associated with measuring α or β. Suppose we trained a Q-Learning agent with\nα = −1 and we intend to test it on a cart with α = −1.01. Figure 15 shows what the trajectory\nwill look like.\nFigure 15: Trajectory of a cart with α = −1.01\nIf we use a cart with α = −1, we get the trajectory shown in Figure 14. If we use a cart\nwith α = −1.01, we get the trajectory shown in Figure 15. This shows that a small error in the\nmeasurement of α can lead to a signiﬁcant change in the trajectory of the cart. This occurs because\nthe set of states s in Q(s, a) are ﬁnite and discrete. This is why a small change in α means that\nthe cart will reach positions that are either not in the set of states in Q(s, a), or have a diﬀerent\naction associated with it. This again shows that Q-Learning is not reliable for real carts.\n7\nConclusion\nThis paper compared three diﬀerent reward functions based on the performance of the RL agent\nif we intend to use Q-learning to solve the cart position problem. In conclusion, a discontinuous\nreward function that rewards the RL agent only if the position of the cart lies in [r −1, r + 1] gives\nthe best results. Through this analysis, we also created a RL agent that outperforms theoretical\nsolutions.\n15\nAcknowledgements\nThe author gratefully acknowledges Jun Liu and Milad Farsi for their continued support, feedback\nand suggestions on this paper.\n16\nAppendix A: Proof of the theoretical solution\nConsider the following input function:\nV (t) = Kp(r −x(t))\nSubstituting the voltage function into equation 2, gives us the following:\n¨x(t) = α ˙x(t) + βKp(r −x(t))\nRe-arranging this gives us a non-homogenous second order diﬀerential equation:\n¨x(t) −α ˙x(t) + βKpx(t) = βKpr\n(11)\nThe goal of this subsection is to ﬁnd the steady-state position limt→∞x(t). This involves ﬁnding\na general solution to equation 11. This will be done by ﬁnding the general solution homogenous\nversion of the equation, xh(t) and by ﬁnding a particular solution to the non-homogenous version\nof the equation, xp(t).\nA.1: Solve the homogenous equation\n¨xh(t) −α ˙xh(t) + βKpxh(t) = 0\nWriting this in matrix form with Xh(t) =\n\u0014\nxh(t)\n˙xh(t)\n\u0015\ngives the following equation:\n˙\nXh(t) =\n\u0014\n0\n1\n−βKp\nα\n\u0015\nXh(t) =: AhXh(t)\nThe solution to this equation is:\nXh(t) = exp(Aht)Xh(0)\nThe eigenvalues of Ah are:\nα\n2 ±\nr\nα2\n4 −βKp\nSince β > 0, this means:\nRe(\nr\nα2\n4 −βKp) < |α\n2 |\nAnd since α < 0, this means:\nRe(\nr\nα2\n4 −βKp) < −α\n2\nRe(±\nr\nα2\n4 −βKp) < −α\n2\n17\nRe(α\n2 ±\nr\nα2\n4 −βKp) < 0\nThis shows that Ah is Hurwitz, and by extension:\nlim\nt→∞Xh(t) =\n\u00140\n0\n\u0015\nThus, the steady-state value of the homogenous equation is: xh = 0.\nA.2: Find a particular solution to the non-homogenous equation\nConsider:\nxp(t) = r\nSubstituting it into equation 2 gives:\n¨x(t) −α ˙x(t) + βKpx(t) = 0 + 0 + βKpr = βKpr\nThus, xp(t) = r is a particular solution to the non-homogenous equation. The steady-state value\nof xp(t) is r.\nA.3: Steady-state value of equation 11\nAll steady-state values of the homogenous equation are 0 and the steady-state solution to the\nparticular solution of the non-homogenous equation is r.\nThis means the steady-state value of\nequation 11 is r.\nlim\nt→∞x(t) = r\nThis shows that using the input function V (t) = Kp(r −x(t)) is eﬀective in moving the cart\nfrom position x = 0 to position x = r.\n18\nReferences\n[1] Sutton RS, Barto AG. (1998) Reinforcement Learning: an Introduction . MIT Press.\n[2] Zheng Y, Luo S,Lv Z. (2006). Control Double Inverted Pendulum by Reinforcement Learning\nwith Double CMAC Network. Proceedings - International Conference on Pattern Recognition.\n4. 639 - 642. 10.1109/ICPR.2006.416.\n[3] Watkins, C.J.C.H. (1989), Learning from Delayed Rewards (Ph.D. thesis), Cambridge Univer-\nsity\n[4] Lin\nC\n(2018),\nBlackjack–Reinforcement-Learning,\nGitHub\nRepository\nhttps://github.com/ml874/Blackjack–Reinforcement-Learning\n[5] Autocar\n(2020),\nThe\ntop\nfastest-accelerating\ncars\nin\nthe\nworld\n2021,\nhttps://www.autocar.co.uk/car-news/best-cars/top-fastest-accelerating-cars-world\n[6] Farsi, M (2021), Lab 1: Cart Position, AMATH 455/655, University of Waterloo\n[7] Mukherjee,\nA\n(2021),\nQ\nLearning\nCart\nProblem,\nGitHub\nRepository\nhttps://github.com/amartyamukherjee/ReinforcementLearningCartPosition\n19\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "math.OC"
  ],
  "published": "2021-05-25",
  "updated": "2021-05-25"
}