{
  "id": "http://arxiv.org/abs/2106.01516v1",
  "title": "Hyperbolically-Discounted Reinforcement Learning on Reward-Punishment Framework",
  "authors": [
    "Taisuke Kobayashi"
  ],
  "abstract": "This paper proposes a new reinforcement learning with hyperbolic discounting.\nCombining a new temporal difference error with the hyperbolic discounting in\nrecursive manner and reward-punishment framework, a new scheme to learn the\noptimal policy is derived. In simulations, it is found that the proposal\noutperforms the standard reinforcement learning, although the performance\ndepends on the design of reward and punishment. In addition, the averages of\ndiscount factors w.r.t. reward and punishment are different from each other,\nlike a sign effect in animal behaviors.",
  "text": "Hyperbolically-Discounted Reinforcement Learning on\nReward-Punishment Framework\nTaisuke Kobayashi1\nAbstract— This paper proposes a new reinforcement learning\nwith hyperbolic discounting. Combining a new temporal differ-\nence error with the hyperbolic discounting in recursive manner\nand reward-punishment framework, a new scheme to learn\nthe optimal policy is derived. In simulations, it is found that\nthe proposal outperforms the standard reinforcement learning,\nalthough the performance depends on the design of reward and\npunishment. In addition, the averages of discount factors w.r.t.\nreward and punishment are different from each other, like a\nsign effect in animal behaviors.\nI. INTRODUCTION\nReinforcement learning (RL) basically acquires the opti-\nmal policy so as to maximize a return, which accumulates\nfuture rewards with exponential discounting [1]. Recent work\nhas shown that RL is an excellent approach to achieve\ncomplicated tasks by robots [2], [3]. The reason why the\nexponential discounting is used is that it is mathematically\neasy to handle with a recursive manner.\nHowever, animals show behaviors, which cannot be ex-\nplained when using the exponential discounting [4]. Specif-\nically, immediate and small reward is preferred to future\nand large reward, but such a decision is reversed when a\nmoderate offset (delay) is added to the time. To explain\nsuch behaviors, a hyperbolic discounting is proposed in the\ncontext of behavioral economics.\nIt is natural to judge that the hyperbolic discounting has\nsome advantages (e.g., the above decision making, long-\ntailed discounting of the future reward, etc.) if animals\ncertainly use it. Following this intuition, this paper aims\nto switch RL from with the exponential discounting to\nwith the hyperbolic discounting. To this end, a temporal\ndifference (TD) error for learning is redeﬁned with the\nhyperbolic discounting from the literature [5]. Its original\ndeﬁnition, however, assumes that the value function (i.e.,\nthe expectatoin of the return) an reward are positive. To\nhandle real number of reward for generality, a reward-\npunishment framework in RL [6], [7], which divides real\nreward into positive one (called reward) and negative one\n(called punishment), is employed. Note that this framework\nis also proposed from biological features.\nThe proposed method is simply investigated in numerical\nsimulations. The results imply that it can outperform the\nconventional RL depending on the design of reward and\npunishment. In addition, it is found that the average discount\nfactors for reward and punishment are in asymmetry, which\nis a similar feature to animals, called a sign effect [8].\n1T. Kobayashi is with the Division of Information Science, Nara Institute\nof Science and Technology, 8916-5 Takayama-cho, Ikoma, Nara 630-0192,\nJapan kobayashi@is.naist.jp\nII. PROPOSAL\nA. Hyperbolically-discounted temporal difference\nLet’s deﬁne the value function (the return), V , with the\nhyperbolic discounting as follows:\nVt = E\n\" ∞\nX\nk=0\nrt+k\n1 + κk | st\n#\n(1)\nwhere r and s denote reward and state, respectively, and κ is\nthe hyperparameter related to the discount factor γ (γ = 1−κ\nwhen the exponential discounting).\nActually, this equation is difﬁcult to solve with the re-\ncursive manner. The literature [5], however, has derived a\nhyperbolically-discounted TD error, δ, as follows:\nδt = rt +\n\u0012\n1 −\nκVt\n(µr + b)p\n\u0013\nVt+1 −Vt\n(2)\nwhere the bias b is given below in this paper.\nb = βσr\n(3)\nµr and σr mean the statistics of reward (mean and standard\ndeviation, respectively). β and p are the hyperparameter.\nHere, γ is deﬁned as 1 −\nκVt\n(µr+βσr)p .\nHere, two problems in this deﬁnition are raised. One is the\ndifference of the scale between r (and µr) and V . From the\nsum of geometric progression, V would be 1/(1 −γ) times\nlarger than r if γ is constant. To compensate this scale gap,\nreward to calculate TD error is multiplied with 1 −¯γ where\n¯γ the average discount factor.\nB. Reward-punishment framework\nAnother problem is signs of numerator and denominator.\nReward in RL is deﬁned as real number, so there is the\npossibility to get γ > 1 when V and µr have the different\nsigns. To avoid this without loss of generality, the reward-\npunishment framework is employed. Speciﬁcally, real reward\nis divided into positive one r, called reward, and negative one\nwith inverted sign p, called punishment, in environment [6]\nside or agent side [7]. In that case, both reward and punish-\nment are deﬁned to be positive real numbers, and therefore,\nγ > 1 would never be caused.\nHowever, the value functions for reward and punishment,\nVr and Vp, are actually approximated using some function\napproximators (e.g., deep neural networks). The approxi-\nmated values should have correct domain, i.e., the positive\nreal numbers, as follows:\nVr,p = max(0, yr,p)\n(4)\nwhere yr,p are the outputs of the function approximators.\narXiv:2106.01516v1  [cs.LG]  3 Jun 2021\n100\n200\n300\n400\n500\nEpisode\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\n0.20\nReturn\nexponential\nhyperbolic\nexponential2\n(a) Learning curves of Acrobot\n100\n200\n300\n400\n500\nEpisode\n0.990\n0.991\n0.992\n0.993\n0.994\n0.995\n0.996\n0.997\n0.998\nDiscount factor\nreward\npunishment\n(b) Discount factors of Acrobot\n100\n200\n300\n400\n500\nEpisode\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReturn\nexponential\nhyperbolic\nexponential2\n(c) Learning curves of CartPole\n100\n200\n300\n400\n500\nEpisode\n0.990\n0.991\n0.992\n0.993\n0.994\n0.995\n0.996\n0.997\n0.998\nDiscount factor\nreward\npunishment\n(d) Discount factors of CartPole\nFig. 1.\nSimulation results\nIII. SIMULATIONS\nA. Conditions\nIn this paper, the performance of RL with the hyperbolic\ndiscounting is investigated. To do so, two environments for\nnumerical simulations are prepared: Acrobot and CartPole\n(see https://github.com/kbys-t/gym_rp). In Ac-\nrobot, reward is given only when the target motion is\nachieved while punishment is continuously given. On the\nother hand, CartPole has continuous reward and event-based\npunishment. This setting is because, as shown in eq. (2), TD\nerror depends on the statics of reward and punishment, which\nare affected by how to be given.\nThe hyperparameters κ, p, and β are empirically set as\n0.01, 1, and 0.1, respectively. The other parameters for RL\nis decided to succeeded in learning with the conventional\nRL. For comparison, two cases, where the conventional RL\nhas γr,p = 0.99 or γr,p the averages of the case with the\nhyperbolic discounting, are ocnducted.\nB. Results\nFig. 1 summarized the simulation results of 50 trials per\neach case. The case of exonential2 (with the averages of the\ndiscount factors in the hyperbolic discounting) had the worst\nperformance in both environments. Namely, it is expected\nthat the discount factors depending on the value function or\nstate contribute to the learning performance.\nThe proposal in CartPole outperformed the other cases,\nalthough it in Acrobot was inferior to the result of expo-\nnential1 (with γ = 0.99). Focusing on the average discount\nfactors during each episode (see (b) and (d) in Fig. 1), one for\nreward was likely to be greater than one for punishment in\nAcrobot; in contrast, their relation was reversed in CartPole.\nThis difference may be given from the design of reward\nand punishment: i.e., the continuous design would make the\ndiscount factor small; and the event-based design would be\na vice versa. This thought comes from the bias deﬁned in\neq. (3) as the variance of reward (or punishment), namely,\nthe event-based or sparse design would cause large variance,\nthereby increasing the discount factor, as expected in eq. (2).\nIndeed, when learning progressed to some extent, reward was\nstable gained, so its variance became small, which made its\ndiscount factor small accordingly.\nIf the continuous reward and the event-based punish-\nment are generally better for learning like these results, the\nasymmetry of the discount factors between them would be\nexpected in this scheme. That is, if so, the hyperbolically-\ndiscounted reinforcement learning on the reward-punishment\nframework would be related to a sign effect in animals [8],\nand would contribute to analyze such animals behaviors\nmathematically.\nIV. CONCLUSION\nThis\npaper\nproposed\nhyperbolically-discounted\nRL\non\nthe\nreward-punishment\nframework.\nCombining\nthe\nhyperbolically-discounted\nTD\nerror\nand\nthe\nreward-\npunishment framework, optimization by RL was enabled.\nIn simulations, the proposal outperformed the conventional\nRL, although the performance depends on the design of\nreward and punishment. In addition, the averages of discount\nfactors for reward and punishment were in asymmetry, like\nthe sign effect in animal behaviors.\nFuture work is further analyses of the proposed scheme\nand to establish the way to design reward and punishment.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press Cambridge, 1998.\n[2] J. Luo, R. Edmunds, F. Rice, and A. M. Agogino, “Tensegrity robot\nlocomotion under limited sensory inputs via deep reinforcement learn-\ning,” in IEEE International Conference on Robotics and Automation.\nIEEE, 2018, pp. 6260–6267.\n[3] Y. Tsurumine, Y. Cui, E. Uchibe, and T. Matsubara, “Deep reinforce-\nment learning with smooth policy update: Application to robotic cloth\nmanipulation,” Robotics and Autonomous Systems, vol. 112, pp. 72–83,\n2019.\n[4] S. Kobayashi and W. Schultz, “Inﬂuence of reward delays on responses\nof dopamine neurons,” Journal of neuroscience, vol. 28, no. 31, pp.\n7837–7846, 2008.\n[5] W. H. Alexander and J. W. Brown, “Hyperbolically discounted temporal\ndifference learning,” Neural computation, vol. 22, no. 6, pp. 1511–1527,\n2010.\n[6] H. Okada, H. Yamakawa, and T. Omori, “Two dimensional evaluation\nreinforcement learning,” in International Work-Conference on Artiﬁcial\nNeural Networks.\nSpringer, 2001, pp. 370–377.\n[7] S. Elfwing and B. Seymour, “Parallel reward and punishment control\nin humans and robots: Safe reinforcement learning using the maxpain\nalgorithm,” in Joint IEEE International Conference on Development\nand Learning and Epigenetic Robotics.\nIEEE, 2017, pp. 140–147.\n[8] S. C. Tanaka, K. Yamada, H. Yoneda, and F. Ohtake, “Neural mech-\nanisms of gain–loss asymmetry in temporal discounting,” Journal of\nNeuroscience, vol. 34, no. 16, pp. 5595–5602, 2014.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-06-03",
  "updated": "2021-06-03"
}