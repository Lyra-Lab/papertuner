{
  "id": "http://arxiv.org/abs/2106.03688v1",
  "title": "A Computational Model of Representation Learning in the Brain Cortex, Integrating Unsupervised and Reinforcement Learning",
  "authors": [
    "Giovanni Granato",
    "Emilio Cartoni",
    "Federico Da Rold",
    "Andrea Mattera",
    "Gianluca Baldassarre"
  ],
  "abstract": "A common view on the brain learning processes proposes that the three classic\nlearning paradigms -- unsupervised, reinforcement, and supervised -- take place\nin respectively the cortex, the basal-ganglia, and the cerebellum. However,\ndopamine outbursts, usually assumed to encode reward, are not limited to the\nbasal ganglia but also reach prefrontal, motor, and higher sensory cortices. We\npropose that in the cortex the same reward-based trial-and-error processes\nmight support not only the acquisition of motor representations but also of\nsensory representations. In particular, reward signals might guide\ntrial-and-error processes that mix with associative learning processes to\nsupport the acquisition of representations better serving downstream action\nselection. We tested the soundness of this hypothesis with a computational\nmodel that integrates unsupervised learning (Contrastive Divergence) and\nreinforcement learning (REINFORCE). The model was tested with a task requiring\ndifferent responses to different visual images grouped in categories involving\neither colour, shape, or size. Results show that a balanced mix of unsupervised\nand reinforcement learning processes leads to the best performance. Indeed,\nexcessive unsupervised learning tends to under-represent task-relevant features\nwhile excessive reinforcement learning tends to initially learn slowly and then\nto incur in local minima. These results stimulate future empirical studies on\ncategory learning directed to investigate similar effects in the extrastriate\nvisual cortices. Moreover, they prompt further computational investigations\ndirected to study the possible advantages of integrating unsupervised and\nreinforcement learning processes.",
  "text": "A COMPUTATIONAL MODEL OF REPRESENTATION\nLEARNING IN THE BRAIN CORTEX, INTEGRATING\nUNSUPERVISED AND REINFORCEMENT LEARNING\nGiovanni Granato*\nLaboratory of Computational Embodied Neuroscience\nInstitute of Cognitive Sciences and Technologies\nNational Research Council of Italy,\nRome, Italy\nSchool of Computing, Electronics and Mathematics\nUniversity of Plymouth\nPlymouth, U.K.\ngiovanni.granato@istc.cnr.it\nEmilio Cartoni\nLaboratory of Computational Embodied Neuroscience\nInstitute of Cognitive Sciences and Technologies\nNational Research Council of Italy\nRome, Italy\nemilio.cartoni@istc.cnr.it\nFederico Da Rold\nBody Action Language Lab\nInstitute of Cognitive Sciences and Technologies\nNational Research Council of Italy\nRome, Italy\nfederico.darold@istc.cnr.it\nAndrea Mattera\nLaboratory of Computational Embodied Neuroscience\nInstitute of Cognitive Sciences and Technologies\nNational Research Council of Italy\nRome, Italy\nandrea.mattera@istc.cnr.it\nGianluca Baldassarre\nLaboratory of Computational Embodied Neuroscience\nInstitute of Cognitive Sciences and Technologies\nNational Research Council of Italy\nRome, Italy\ngianluca.baldassarre@istc.cnr.it\nJune 8, 2021\nABSTRACT\nA common view on the brain learning processes proposes that the three classic learning paradigms—\nunsupervised, reinforcement, and supervised—take place in respectively the cortex, the basal-\nganglia, and the cerebellum. However, dopamine outbursts, usually assumed to encode reward,\nare not limited to the basal ganglia but also reach prefrontal, motor, and higher sensory cortices. We\npropose that in the cortex the same reward-based trial-and-error processes might support not only the\nacquisition of motor representations but also of sensory representations. In particular, reward sig-\nnals might guide trial-and-error processes that mix with associative learning processes to support the\nacquisition of representations better serving downstream action selection. We tested the soundness\nof this hypothesis with a computational model that integrates unsupervised learning (Contrastive\nDivergence) and reinforcement learning (REINFORCE). The model was tested with a task requiring\ndifferent responses to different visual images grouped in categories involving either colour, shape,\nor size. Results show that a balanced mix of unsupervised and reinforcement learning processes\nleads to the best performance. Indeed, excessive unsupervised learning tends to under-represent\ntask-relevant features while excessive reinforcement learning tends to initially learn slowly and then\nto incur in local minima. These results stimulate future empirical studies on category learning di-\nrected to investigate similar effects in the extrastriate visual cortices. Moreover, they prompt further\narXiv:2106.03688v1  [q-bio.NC]  7 Jun 2021\nA PREPRINT - JUNE 8, 2021\ncomputational investigations directed to study the possible advantages of integrating unsupervised\nand reinforcement learning processes.\n2\nA PREPRINT - JUNE 8, 2021\n1\nIntroduction\nA classic computational view differentiates the brain\nlearning processes between unsupervised learning (UL)\ntaking place in the cortex, reinforcement learning (RL)\nbased on dopamine dynamics within the basal ganglia,\nand supervised learning in the cerebellum (Doya, 1999,\n2000). The biological literature (Houk et al., 1995; Red-\ngrave and Gurney, 2006), supported by reinforcement-\nlearning computational models (Sutton and Barto, 2018;\nFiore et al., 2014), has shown how basal-ganglia learn-\ning is strongly driven by reward signals encoded by\nthe dopamine released by the neuromodulator mesolim-\nbic system. A similarly extensive literature shows how\ncortical learning is largely based on associative learn-\ning (Markram et al., 2011; Caporale and Dan, 2008),\nas also operationalised with computational models (Hop-\nﬁeld, 1982; Gerstner and Kistler, 2002; Zappacosta et al.,\n2018).\nHowever, empirical evidence shows that dopamine also\ndirectly innervates the cortex through the neuromodulator\nmesocortical system having decreasing mediodorsal and\nanterior-posterior projection gradients spanning far be-\nyond motor cortices (Williams and Goldman-Rakic, 1993;\nJacob and Nienborg, 2018; Niu et al., 2020; Froudist-\nWalsh et al., 2020). Dopamine-based reward signals thus\nplay an important role not only for basal-ganglia but also\nfor the cortex (Wise, 2004). For example, different pre-\nfrontal and motor cortices encode different aspects re-\nlated to reward, such as rewarded outcomes, stimuli as-\nsociated with these outcomes, actions leading to them,\nand working memory upload-download (O’Reilly, 2006;\nRushworth et al., 2011; Mannella and Baldassarre, 2015;\nZeithamova et al., 2019).\nRecently we have extended (Caligiore et al., 2019) the\nprevious view on the brain learning processes (Doya,\n1999, 2000) by proposing that each of the different macro-\nsystems of the brain—the basal ganglia, the cortex, and\nthe cerebellum—involve multiple learning mechanisms.\nIn this view, cortical plasticity involves both associa-\ntive (unsupervised) learning processes and trial-and-error\n(reinforcement) learning processes based on dopamine.\nHowever, to our knowledge how these two learning pro-\ncesses might integrate and affect the learned internal rep-\nresentations of stimuli has not been investigated with\ncomputational models.\nThe goal of this work is to propose a hypothesis stating\nthat reward-based trial-and-error processes might lead not\nonly to the acquisition of motor representations, as it is\nusually assumed, but also of sensory representations. In\nparticular, the hypothesis we propose is that: (a) within\ncortices, reward-based trial-and-error learning processes\nand associative learning processes are mixed; (b) the trial-\nand-error mechanisms learning non-motor representations\nare analogous to those learning motor representations,\nconsisting of exploratory noise and the reward-based ﬁx-\nation of the found effective solutions; (c) the joint effect\nof these associative and trial-and-error learning processes\nleads to the acquisition of action-oriented representations\nbetter serving downstream action selection processes.\nTo operationalise the hypothesis, we propose a compu-\ntational model having an overall actor-critic architecture\n(Sutton and Barto, 2018) and based on a generative model\n(Goodfellow et al., 2017). The generative model is a Deep\nBelief Network based on two stacked Restricted Boltz-\nmann Machines, (Hinton, 2002; Hinton et al., 2006) in-\ntegrating the UL Contrastive Divergence (Hinton, 2002)\nand RL, using the REINFORCE algorithm (Williams,\n1992) .\nThe model has an architecture that is different from those\ncommonly used in reinforcement-learning neural network\nmodels (Mnih et al., 2015; Arulkumaran et al., 2017;\nNguyen et al., 2020; Shao et al., 2019). In these systems,\n(a) the RL algorithms and reward signals are used to train\nonly the output ‘motor’ layers and (b) the effects of re-\nward on inner layers are caused by error gradients that\nback-propagate from output layers. On the contrary, we\npropose here that in the cortex the reward signals directly\naffect the representations formed within the inner layers.\nThe model is tested with sorting tasks requiring the exe-\ncution of consistent actions in response to different prop-\nerties (colour, shape, or size) of simple geometric images.\nThis task is inspired to sorting tasks used in the research\non category learning (Hanania and Smith, 2010) where\nthe participants have to sort cards with simple shapes by\nputting them onto other cards showing similar attributes\n(e.g., red) according to a certain category (e.g., colour).\nThe results show that a balanced mix of UL and RL pro-\ncesses leads to higher performance. Moreover, the learned\nrepresentations exhibit a category-based action-oriented\ndisentanglement effect for which the encoding encom-\npasses both intrinsic statistical regularities and action-\nrelevant visual features of images. These results corrobo-\nrate the hypothesis for which reward-based trial-and-error\nprocesses can directly affect sensory representations in the\ncortex thus tuning them towards action.\n2\nMethods\n2.1\nTask and experimental conditions\nThe task we used to test the model is based on category\nlearning tasks requiring the production of a response on\nthe basis of speciﬁc visual features of stimuli such as\ncolour, shape, and size (see Ashby and Maddox, 2005,\n2011a for an extended analysis of these tasks).\nIn particular, we focused on a sub-class of these tasks in\nwhich a classiﬁcation rule is ﬁxed and the participant has\nto execute a motor action on the basis of the features of a\ncard (Hanania and Smith, 2010). The task uses a series of\n2D input images of geometrical shapes varying in colour,\nshape, and size, for example as those shown in Figure 1A.\nFor example in the case of a colour classiﬁcation rule, the\n3\nA PREPRINT - JUNE 8, 2021\nFigure 1: (A) Examples of the 64 geometrical shapes (circles, squares, parallelepipeds, triangles) used to produce the\nimages. Each image encompasses a different attribute out of the four attributes of each of the three categories colour,\nshape, and size. (B) A schema of the main model processes involved in its interaction with the environment.\nagent should learn to respond with a different output to the\ndifferent colours (red, green, blue, yellow), hence ignor-\ning the shape and the size. Figure 1B summarises the main\nprocesses performed by the system during the task perfor-\nmance: perception of the input, behavioural response, per-\nformance monitoring, and processing of the reward. The\ntask was repeated for all the three classiﬁcation rules in-\nvolving colour, shape, and size.\n2.2\nThe architecture of the model and its biological\nunderpinning\nFigure 2 summarises at a high level the elements of the\nhypothesis proposed here that are captured by the compu-\ntational model. This in particular encompasses intermedi-\nate layers corresponding to extra-striate cortices that host\nmixed UL and RL processes. Figure 3 shows the archi-\ntecture of the model. We now illustrate the components of\nthe architecture and their biological underpinning.\nPerceptual component\nThis component is based on a\nneural network that processes visual inputs by performing\ninformation abstraction and mimics the brain visual corti-\ncal system. In particular, the component executes a hier-\narchical information processing (Felleman and Van Essen,\n1991; Baldassarre et al., 2013) from the low-level retino-\ntopic features in the striate cortex (V1), to the extraction\nof high-level image features (colour, shape, size) in the\nextrastriate cortices (DeYoe et al., 1996; Konen and Kast-\nner, 2008).\nDifferently from the biologically implausible gradient-\ndescent methods, the network learns through a bio-\nplausible mechanism (Illing et al., 2019). In particular,\nFigure 2: Scheme of learning processes and targeted brain\nareas that are addressed by the hypothesis and computa-\ntional model presented here. The non-motor cortex un-\ndergoes both associative learning (UL) and trial-and-error\nlearning (RL). The latter presents a gradient having a de-\ncreasing strength moving from the motor cortex towards\nthe striate cortex.\nThe model studies the effects that\nthe mix of unsupervised and reinforcement-learning pro-\ncesses have in extrastriate cortices.\nthe learning processes used in the model update each con-\nnection weight (synapse) on the basis of locally available\ninformation related to the pre-synaptic and post-synaptic\nunits. Another biologically plausible feature of the model,\nat the core of the novelty of the hypothesis presented here,\nis that the top layer of this component is trained during the\n4\nA PREPRINT - JUNE 8, 2021\ntask through a mechanism that integrates associative and\nreward-based RL (Figure 2).\nThe bottom layer of the component, which mimics early\nvisual cortices, is instead trained before the task execution\nto reﬂect the learning of these areas during early develop-\nment (Siu and Murphy, 2018). Critical for our hypoth-\nesis, this architecture captures the essence of the effects\nof dopamine reward signals onto extra-striate cortices and\nthe lack of it in striate cortices (Williams and Goldman-\nRakic, 1993; Jacob and Nienborg, 2018; Impieri et al.,\n2019; Niu et al., 2020; Froudist-Walsh et al., 2020). Fi-\nnally, the model relies on distributed representations, for\nwhich information on each content (e.g., a percept) is en-\ncoded by many units of the layer, and each unit takes part\nin the representations of different contents. This encod-\ning is more bio-plausible than localistic representations\n(‘grandmother-cells’; McClelland et al., 1986; Quiroga\net al., 2008).\nMotor component\nThis component is supported by a\nneural network that, on the basis of the perceptual compo-\nnent activation, produces an ‘action’ affecting the world.\nThe network is trained through a trial-and-error learning\nalgorithm using a reward signal, mimicking the interac-\ntions of basal ganglia with motor cortices during the learn-\ning of actions (Kim et al., 2017; Seger, 2008).\nMotivational component\nThis component is formed by\nthree sub-modules that emulate the motivational functions\nsupported by different brain sub-systems.\nFirst, a motivator sub-module produces a reward signal\non the basis of the perceived outcome following action\nperformance. Here the outcome is received from the en-\nvironment and informs the system on the ‘correctness’ of\nthe performed action (see below). This action-outcome\nmight correspond to an ‘extrinsic reward’, for example to\nthe receipt of food or other rewarding resources; this is\nsuitably processed by the system sensors and motivator\ncomponent to produce a reward signal guiding the system\nlearning processes. In other conditions (Baldassarre and\nMirolli, 2013; Baldassarre, 2011) the reward signal might\nbe produced by intrinsic motivation processes, for exam-\nple related to the novelty or surprise of the experienced\nstimuli (Barto et al., 2013) or to the acquisition of compe-\ntence during the accomplishment of a desired goal (White,\n1959; Santucci et al., 2016). In the brain, structures such\nas the hypothalamus and the pedunculopontine nucleus,\nand the ventromedial, orbital, and anterior-cingulate cor-\ntices, support extrinsic rewards (Panksepp, 1998; Mirolli\net al., 2010), while other structures, such as the superior\ncolliculus, hippocampus, and the dorsolateral prefrontal\ncortex, support the computation of intrinsic reward signals\n(Lisman and Grace, 2005; Ribas-Fernandes et al., 2011;\nBaldassarre, 2011).\nSecond, a predictor sub-module, based on a multi-layer\nneural network, uses the high-level perceptual representa-\ntions encoding the current perceived state, received from\nthe top layer of the perceptual component, to predict the\nrewards that can be attained from it. This module func-\ntionally mimics the brain basal-ganglia striosomes (Houk\net al., 1995).\nLast, a prediction error sub-module integrates the ob-\ntained and predicted rewards and produces a learning sig-\nnal (‘surprise’). This signal inﬂuences the learning of the\npredictor, of the motor component and, most importantly,\nof the perceptual component. In the brain, this signal is\nrepresented by the phasic dopamine bursts reaching vari-\nous target areas (Schultz, 2002), as also modelled by the\nactor-critic RL architecture (Barto, 1995).\n2.3\nComputational implementation and learning\nalgorithms of the model\nThe system proposed here (Figure 4) is formed by a gen-\nerative model integrated into an actor-critic architecture\n(Sutton et al., 1998), both modiﬁed to study the role of re-\nward in perceptual representation learning. Further details\nregarding the system parameters (e.g., the number of units\nof each layer, the learning rates, the training epochs, etc.)\nare reported in Table S1 of the Supplementary Materials.\nThe code of the system will be made publicly available\nonline in GitHub in the case of publication.\nPerceptual component\nThis component is a generative\nDeep Belief Network (DBN; Hinton et al., 2006; Le Roux\nand Bengio, 2008) composed of two stacked Restricted\nBoltzmann Machines (RBM; Hinton, 2012). Each RBM\nis composed of an input layer (‘visible layer’) and a sec-\nond layer (‘hidden layer’) formed by Bernoulli-logistic\nstochastic units where each unit j has an activation hj ∈\n{0, 1}:\nhj =\n\u001a1 if ν ≥σ(pj)\n0 if ν < σ(pj)\n(1)\nσ(pj) =\n1\n1 + e−pj\npj =\nX\ni\n(wji · vi)\nwhere σ(x) is the sigmoid function, pj is the activation\npotential of the unit hj, ν is a random number uniformly\ndrawn from (0, 1) for each unit, and wji is the connection\nweight between the visible unit vi and hj. The RBM is ca-\npable of reconstructing the input by following an inverse\nactivation from the hidden layer to the input layer.\nThe DBN consists of a stack of RBMs—two in the\nmodel—where each RBM receives as input the activa-\ntion of the hidden latent layer of the previous RBM. The\nmodel is trained layer-wise, starting from the RBM which\nreceives inputs from the environment and towards the in-\nner layers. On this basis, the DBN executes an incremen-\ntal dimensionality reduction of the input, as higher lay-\ners further compress the representations received from the\nlower/previous RBM (Hinton and Salakhutdinov, 2006).\n5\nA PREPRINT - JUNE 8, 2021\nFigure 3: Schema of the model components and functions, the ﬂows of information between the components, and the\nlearning signals.\nIn the model, the ﬁrst RBM directly receives the input im-\nages and it is trained to encode them ‘ofﬂine’ before the\ntask. This training uses the Contrastive Divergence, an\nunsupervised-learning algorithm that computes each con-\nnection weight update ∆wij as follows:\n∆wij = ϵ(⟨vi · hj⟩data −⟨vi · hj⟩model)\n(2)\nwhere ϵ is the learning rate, ⟨vi · hj⟩data is the product\nbetween the initial input (initial visible activation) and\nthe consequent hidden activation averaged over all data\npoints, ⟨vi · hj⟩model is the product between the recon-\nstructed visible activation and a second activation of the\nhidden layer following it averaged over all data points.\nThe third and fourth activations are denoted with ‘model’\nas they tend to more closely reﬂect the spontaneous input-\nindependent activations of the RBM.\nThe second RBM of the model is trained ‘online’ during\nthe task performance based on the novel algorithm pro-\nposed here. The algorithm integrates Contrastive Diver-\ngence (Eq. 2) with the REINFORCE algorithm described\nin the next session (Eq. 4) as follows:\n∆wij = λ (ϵ (⟨vi · hj⟩data −⟨vi · hj⟩model)) +\n(1 −λ) (α (r −¯r)(yj −pj)xi)\n(3)\nwhere λ is the contribution of Contrastive Divergence to\nthe update of weights, and (1 −λ) the contribution of\nREINFORCE. Crucial for this work, λ mixes the contri-\nbution of UL and RL processes to the weight update, in\nparticular a high value of it implies a dominance of UL\nwhereas a low value of it implies a dominance of RL. In\nthe simulations, we tested ﬁve values of the parameter:\nλ ∈{1, 0.1, 0.01, 0.001, 0}.\nMotor component\nThis component is a single-layer\nneural network trained with the RL algorithm REIN-\nFORCE (Williams, 1992). The input of the network is\nthe activation of the last layer of the perceptual compo-\nnent. The network output layer is composed of Bernoulli-\nlogistic units as for the perceptual component. The al-\ngorithm computes the update ∆wji of each connection\nweight linking the input unit i and the output unit j of\nthe component as follows:\n∆wji = α(r −¯r)(yj −σ(pj))xi\n(4)\nwhere α is the learning rate, r is the reward signal received\nfrom the motivator, ¯r is the reward signal expected by the\npredictor, xi is the input of the network (from the outer\nsecond hidden layer of the DBN), σ(pj) is the sigmoidal\nactivation potential of the unit encoding its probability of\nﬁring, and yj is the unit binary activation.\nMotivational component\nThis component implements\nthe functions of the critic component of an actor-critic\narchitecture (Sutton et al., 1998) based on three sub-\nmodules introduced in the previous section.\nThe motivator module computes the reward signal by\nscaling the reward perceived from the external environ-\nment into a standard value, the reward signal r ∈(0, 1):\nr = f(Reward)\n(5)\nwhere Reward is the reward perceived from the environ-\nment and f(.) is a linear scaling function ensuring that\nthe reward signal ranges between 0, corresponding to a\nwrong action, to 1, corresponding to an optimal action.\nThis reward signal represents the pivotal guidance of the\nRL processes driving the acquisition of not only the ac-\ntions but also the DBN second hidden layer. As discussed\nabove, in other cases the motivator might compute more\ncomplex reward signals based on more sophisticated types\nof extrinsic and/or intrinsic motivation mechanisms.\n6\nA PREPRINT - JUNE 8, 2021\nFigure 4: A computational schema of the model components and their training algorithms, the ﬂows of information\nbetween the components, and the learning signals.\nThe predictor module is a multi-layer perceptron com-\nposed of an input layer (DBN second hidden layer), a\nhidden layer, and an output linear unit predicting the ex-\npected reward signal ¯r. The perceptron is trained with a\nstandard gradient descendent method (McClelland et al.,\n1986; Amari, 1993) using a learning rate α and the error\ne computed by the prediction-error component.\nThe prediction error module is a function that computes\nthe reward prediction error (surprise) e as follows:\ne = r −¯r\n(6)\nwhere r is the reward signal from the motivator, and ¯r\nis the expected reward signal produced by the evaluator.\nThis error is used to train the predictor itself, the motor\ncomponent, and the perceptual component.\nAuxiliary elements\nThe input dataset is formed by\nRGB images with a black background and a polygon at\nthe centre (Figure 1). The polygon is characterised by\na unique combination of speciﬁc attributes chosen from\nthree visual categories: colour, form and size. There are\nfour attributes for each category: red, green, blue, yel-\nlow (colour); square, circle, triangle, bar (form); large,\nmedium-large, medium-small, small (size).\nThese at-\ntributes generate 43 = 64 combinations forming the im-\nages used in the test.\nThe retina component is implemented as a 28 × 28 × 3\nmatrix containing the RGB visual input. The matrix is\nunrolled into a vector of 2, 352 elements that represents\nthe input of the perceptual component.\nThe environment is implemented as a function that pro-\nvides an image to the model at each trial. In one trial the\nmodel perceives and processes one input image and un-\ndergoes a cycle of the aforementioned learning processes\nbased on the reward received from the environment after\nthe action performance. Here the environment computes\nthe reward r′ simply on the basis of the Euclidean distance\nbetween the model action and an ‘optimal action’:\nReward = ∥y∗−y∥1\n(7)\nwhere y∗is the optimal action binary vector that the\nmodel should produce for the current input, y is the model\nbinary action, and ∥.∥1 is the L1 norm of the vectors dif-\nference. The optimal actions are four binary random vec-\ntors that the model should produce in correspondence to\nthe items of the four input categories of the given task.\n3\nResults\nWe tested the model with different tasks each involving\none out of three sorting rules based on the three categories,\nfor example, a task required sorting the cards by colour\nand another one by shape. The model was tested with\nﬁve different levels of UL/RL contribution (λ parameter,\nsee Section 2.3) and two levels of internal resources, in\nparticular respectively 10 and 50 units in the second DBN\nhidden layer. Note that 50 units were sufﬁcient to allow\nthe system to fully encode the image features, as shown by\na preliminary test indicating a close-to-null reconstruction\nerror.\nWe varied the parameters of these environmental and\nmodel conditions with a random grid search based on\n1016 simulations. The simulations were run in the Neu-\nroscience Gateway platform (NSG, Sivagnanam et al.,\n2013). The different values of the critical parameter λ\ngave rise to ﬁve conditions labelled as follows: Level 0\n(L0): no RL (i.e., only UL); Level 1 (L1): low RL; Level\n2 (L2): moderate RL; Level 3 (L3): high RL; Level 4\n7\nA PREPRINT - JUNE 8, 2021\n(L4): extreme RL (no UL). The simulations using differ-\nent amounts of internal resources allowed us to investi-\ngate how the available computational resources affect the\nresults related to the UL/RL mix.\nThe presentation of results is organised in three parts. The\nﬁrst part investigates the effects on the performance of the\ndifferent contributions of UL/RL. The second part investi-\ngates the nature of the perceptual representations acquired\nthrough a different UL/RL learning mix. Finally, the third\npart presents a graphical reconstruction of the original in-\nput patterns produced by the generative perceptual com-\nponent of the model and the related reconstruction errors.\nPerformances analysis\nFigure 5 shows the training\ncurves of the models trained with different RL contribu-\ntions in 15,000 epochs. The L0 models, using only UL,\nlearn faster during the ﬁrst 1,000 epochs but exhibits the\nworst ﬁnal performance. Figures S1, S2 and S3 in Sup-\nplementary Materials show that this effect is also present\nin subsets of all simulations. Instead, the highest ﬁnal per-\nformance is achieved by the L3 models where UL and RL\nare better balanced.\nFigure 6 shows the ﬁnal performance of the models,\nnamely the maximum reward they achieved. A correla-\ntion analysis shows the presence of a linear relation be-\ntween such performance and the level of the RL, but this is\nnot very high thus indicating the relevance of the inverted\nU shape of the curve visible from the ﬁgure (r = 0.5,\np < 0.001).\nA one-way ANOVA conﬁrms the pres-\nence of a statistical difference between the ﬁnal perfor-\nmance of the ﬁve groups (F\n= 47.51, p < 0.001).\nPost hoc tests (Table 1) conﬁrm that the performances of\nmodels with an absent RL contribution (L0) are statisti-\ncally different with respect to each of the other models\n(0.81 ± 0.08, p < 0.001). The L3 models show a higher\nperformance compared to the L0 models (0.92 ± 0.06 vs.\n0.81 ± 0.08, p < 0.001), the L1 models (0.92 ± 0.06 vs.\n0.89 ± 0.04, p < 0.001), and the L4 models (0.92 ± 0.06\nvs. 0.90 ± 0.07, p < 0.05). The L2 and L3 models do not\nshow a signiﬁcant difference (0.92±0.06 vs. 0.91±0.05).\nTo further investigate the relationship between the perfor-\nmance of the models and the different levels of RL con-\ntribution, we grouped the results of the simulations on the\nbasis of the computational resources or the sorting rule.\nHere we present a summary of the results while Section\nS2.1 in the Supplemental Materials reports the posthoc\ntests.\nTable 2 shows that the increase of computational re-\nsources available for the representations tends to lower the\namount of RL contribution leading to the highest perfor-\nmance. Indeed, a one-way ANOVA shows a statistical\ndifference between the models (F = 3.85, p < 0.001)\nand the post-hoc tests show that the L2 model leads to the\nbest result (0.95 ± 0.05).\nThe table also highlights differences between the simu-\nlations using different sorting rules (colour, shape, size).\nThe simulations with the colour sorting rule show ﬂat-\ntened reward values with respect to the different RL con-\ntribution. In the case of low computational resources the\nmodel does not show statistically signiﬁcant differences\n(F = 0.88, p > 0.05). A difference emerges in the case\nof high computational resources (F = 19.8, p < 0.001)\nwhere the L2 models, having a balanced UL/RL mix,\nshow the best ﬁnal performance (0.98 ± 0.02).\nThe simulations with the shape sorting rule show statis-\ntical differences with both low computational resources\n(F = 120.9, p < 0.001) and high computational re-\nsources (F = 20.4, p < 0.001). In both cases, the models\nusing a mixed level of UL and RL prevail: the extreme\ncases of the L0 models (only UL), and L4 models (only\nRL) have lower performances with respect to the L1, L2\nand L3 models having a more balanced UL/RL mix.\nFinally, the simulations with the size sorting rule show\nstatistical differences with low computational resources\n(F = 43.4, p < 0.001) but not with ‘high computational\nresources’ (F = 1.12, p > 0.05). In the ﬁrst case, the L0\nmodels have the lowest performance.\nAnalysis of internal representations\nTo investigate the\nnature of the perceptual representations acquired by the\nmodels, we show the results of some example simulations\nin the cases of different sorting rules and different levels\nof the RL. Other simulations lead to qualitatively similar\nresults.\nTo visualise the representations we used a Principal Com-\nponent Analysis (PCA), allowing a dimensionality reduc-\ntion, and a K-means algorithm, supporting clustering. In\nparticular, we extracted the ﬁrst two principal compo-\nnents of the input patterns reconstructed by the model into\nthe visible layer in correspondence to the original 64 in-\nput patterns. The reconstructed images were obtained by\nspreading the activity from the visible layer of the DBN,\nactivated with an image, to its ﬁrst and second hidden\nlayer, and then back towards the visible layer. We anal-\nysed the reconstructed visual representations, rather than\nthe hidden representations, to assess which features of the\noriginal visual images are retained by the internal repre-\nsentations. The results of the PCA extraction of the two\ndimensions of the representations can be plotted in a 2D\nscatter plot to visualise the results of the following K-\nmeans algorithm. The K-means algorithm was applied\nto PCA 2D codes of the internal representations. We set\nK = 4, so the algorithm grouped the representations into\nfour classes, as the number of the actions. This made it\npossible to analyse how the model internally represents\nthe different input images.\nFurther details and results\nregarding these methods, as the cumulative variance ex-\nplained by the PCA components and the silhouette scores\nof the K-mean algorithm, are reported in Section S2.2 of\nSupplemental Materials.\nThe results of the analyses (Figures 7-9) highlight that the\nreward-based RL contribution strongly affects the internal\nrepresentations as revealed by the reconstructed inputs.\n8\nA PREPRINT - JUNE 8, 2021\nLearning curves of models\nFigure 5: Reward per epoch of the ﬁve models involving different UL/RL levels, averaged over the models using a\ngiven level. Shaded areas represent the curves standard deviations.\nPerformance of models\nFigure 6: Performances (maximum reward obtained at the end of training) of models featuring different levels of RL\ncontribution.\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nAbsent (L0)\n//\n//\n//\n//\n//\nLow (L1)\np < 0.001\n//\n//\n//\n//\nModerate (L2)\np < 0.001\np > 0.05 (NS)\n//\n//\n//\nHigh (L3)\np < 0.001\np < 0.001\np > 0.05 (NS)\n//\n//\nExtreme (L4)\np < 0.001\np > 0.05 (NS)\np > 0.05 (NS)\np < 0.05\n//\nTable 1: Post-hoc comparisons (t-test with Bonferroni correction) between the performance of models with different\nlevels of RL contribution. ‘NS’ indicates ‘non statistically signiﬁcant’.\nFor each sorting rule considered, models with a medium\n(L2) and high (L3) level of RL show the emergence of\ncategory-based clusters, with their radius progressively\ndiminishing with an increasing weight of the RL. Con-\nversely, the L0 and L1 models do not show this effect in\nany task condition. The only exceptions to this are the\nmodels with an absent or low RL (L0 and L1) showing a\nclustering effect that does not depend on the task but only\non the colour of the shapes. This is due to the high distinc-\n9\nA PREPRINT - JUNE 8, 2021\nAbsent\nLow\nModerate\nHigh\nExtreme\nLow Resources\n0.81 ± 0.08\n0.89 ± 0.04\n0.91 ± 0.05\n0.92 ± 0.06\n0.90 ± 0.07\nColour\n0.92 ± 0.02\n0.92 ± 0.02\n0.91 ± 0.04\n0.91 ± 0.07\n0.90 ± 0.08\nShape\n0.75 ± 0.02\n0.89 ± 0.04\n0.94 ± 0.04\n0.95 ± 0.04\n0.93 ± 0.06\nSize\n0.76 ± 0.02\n0.88 ± 0.05\n0.89 ± 0.06\n0.90 ± 0.06\n0.86 ± 0.07\nHigh Resources\n0.92 ± 0.03\n0.93 ± 0.04\n0.95 ± 0.05\n0.93 ± 0.06\n0.93 ± 0.05\nColour\n0.94 ± 0.01\n0.94 ± 0.01\n0.98 ± 0.02\n0.95 ± 0.03\n0.96 ± 0.02\nShape\n0.93 ± 0.02\n0.97 ± 0.02\n0.97 ± 0.02\n0.96 ± 0.02\n0.94 ± 0.02\nSize\n0.88 ± 0.02\n0.88 ± 0.03\n0.90 ± 0.05\n0.88 ± 0.07\n0.88 ± 0.07\nTable 2: Performance of models with different RL contributions in correspondence to two different amounts of compu-\ntational resources (number of neurons in the second hidden layer of the DBN) and three different sorting rules (colour,\nshape, size). Figures in bold highlight the highest value per each condition (along the raws).\nColour sorting category: reconstructed input\nFigure 7: Principal components of the reconstructed image representations in the case of the colour sorting rule and in\ncorrespondence to different levels of RL (shown in different graphs). The dimensionality of the reconstructed image\nwas reduced to two through a PCA (x-axis: ﬁrst component; y-axis: second component). Within each graph, each\nreconstructed image is represented by a point marked by an icon that summarises the colour, shape, and size of the\nshape in the image (some icons are not visible as they overlap). The centroids of the four clusters found by the K-\nmeans algorithm are marked with a black dot, while the maximum distance of the points of the cluster from its centroid\nis shown by a grey circle. A: Level 0 (L0), absent RL (only UL); B: Level 1 (L1), low RL; C: Level 2 (L2), moderate\nRL; D: Level 3 (L3), high RL; E: Level 4 (L4), extreme RL (no UL).\ntiveness of colours, largely activating different portions of\nthe input units with respect to the other image features.\nFigure 9-E shows that the model with an extreme RL in-\ncurred a clustering error. In particular, in this condition\nthe model should group the images into four clusters (as\nin the conditions of Figure 9-C,D) whereas it tends to use\nonly three clusters.\nInformation stored by the model\nTo further investi-\ngate what type of information is stored by the model,\nwe show the results of two additional analyses. The ﬁrst\nanalysis examined the DBN reconstruction error (see Sec-\ntion S2.3 in Supplementary Materials for further details),\nwhile the second analysis qualitatively inspected the re-\nconstructions of the input images.\n10\nA PREPRINT - JUNE 8, 2021\nShape sorting category: reconstructed input\nFigure 8: Principal components of the reconstructed image representations in the case of the shape sorting rule and in\ncorrespondence to different levels of RL. The plots are drawn as in Figure 7.\nFigure 10 shows the results of the ﬁrst analysis and high-\nlights the presence of a strong positive linear relation-\nship between the level of RL and the reconstruction error\n(r = 0.68, p < 0.001). A one-way ANOVA conﬁrmed\nthe presence of a statistical difference between the ﬁve\ngroups (F > 100.0, p < 0.001). These results indicate\nthat an increasing RL contribution causes a progressive\nloss of information on the input images.\nThe results of the second analysis show the kind of in-\nformation that the internal representations tend to retain,\nin particular if the system tends to store task-independent\nand/or task-related features.\nIn this respect, Figure 11\nhighlights the emergence of shapeless coloured blobs in\nthe case of the colour sorting rule, the emergence of\ncolourless and sizeless prototypical shapes in case of\nshape sorting rule, and the emergence of colourless blobs\nwith different sizes in the case of the size sorting rule.\n4\nDiscussion\nIn this work, we propose a novel hypothesis on the broad\nnature of learning in the brain cortex. A previous seminal\nview proposed that the cortex, basal ganglia, and cerebel-\nlum use the different learning mechanisms studied in ma-\nchine learning, respectively unsupervised, reinforcement,\nand supervised learning (Doya, 1999, 2000). Recently,\nwe have proposed a theory (Caligiore et al., 2019) that ex-\npands such view by proposing that, although those learn-\ning processes might be predominant in the three macro\nbrain systems, plasticity within them mixes the three\nlearning mechanisms. The hypothesis proposed here fo-\ncuses on the cortex and on the empirical evidence show-\ning that dopamine reaches cortical targets (Williams and\nGoldman-Rakic, 1993; Niu et al., 2020). On this basis, the\nhypothesis proposes that dopamine directly conveys in-\nformation on the reward to the target cortices. Therefore,\nlearning processes happening within them integrate asso-\nciative (UL) and reward-based trial-and-error mechanisms\n(RL). Here we also present a computational model that ac-\ntually mixes the two learning processes. The key idea at\nthe core of the model is to exploit the stochastic nature\nof the units of Boltzmann neural networks to implement\nthe RL key noise-based search process through the REIN-\nFORCE algorithm (Williams, 1992). The author of this\nalgorithm envisaged the link between the stochastic units\nused in Boltzmann neural networks and those used in RE-\nINFORCE (Williams, 1992). However, the original work\nproposed the algorithm only to support the acquisition of\nactions as usually done in RL (Sutton et al., 2000, 1998).\nInstead, the model proposed here uses REINFORCE to\nlearn inner representations in the neural network. This\ndiffers from other neural network models where reward\nonly informs the training at the output layer of the network\nwhereas the deeper representations are updated based on\nbiologically non-plausible error back-propagation mecha-\nnisms (Arulkumaran et al., 2017; Mnih et al., 2015). To\nour knowledge, the proposed model is the ﬁrst to allow\n11\nA PREPRINT - JUNE 8, 2021\nSize sorting category: reconstructed input\nFigure 9: Principal components of the reconstructed image representations in the case of the size sorting rule and in\ncorrespondence to different levels of RL. The graphs are drawn as in Figure 7. The red arrow in graph E indicates the\ncentroid of a cluster that contains only the small bars but not the other small shapes.\nInformation loss for different levels of RL\nFigure 10: Information loss (reconstruction error at the end of the training) of models with different levels of RL.\nreward signals to directly bias the acquisition of the rep-\nresentations encoded in the inner neural layers of the net-\nwork in a bio-plausible manner. The model thus repre-\nsents a new tool to investigate the potential utility of this\nfor computational purposes and also to study the possi-\nble effects of the mixed UL/RL processes within the brain\ncortex.\nThe model was tested with a task having the features\nof experiments used in the ﬁeld of category learning, in\nparticular requiring sorting images based on different at-\ntributes of a given category (Hanania and Smith, 2010).\nThis was done to start to consider how the model could\nbe used to study speciﬁc phenomena in category learning\n(Zeithamova et al., 2019), as further discussed below.\n12\nA PREPRINT - JUNE 8, 2021\nInput reconstructions (sorting category: colour)\nInput reconstructions (sorting category: shape)\nInput reconstructions (sorting category: size)\nFigure 11: Image reconstructions with different sorting\nrules and different levels of RL. A: Original inputs; B:\nLevel 0 (L0) - absent RL (only UL); C: Level 1 (L1) - low\nRL; D: Level 2 (L2) - moderate RL; E: Level 3 (L3) - high\nRL; F: Level 4 (L4) - extreme RL (only RL).\nThe main result of the tests of the model is that a suitably\nbalanced mix of UL and RL leads the model to achieve the\nbest performance. This result holds for all tested condi-\ntions, as shown in Figure 6 and Table 1. Further analyses\nexplained the possible causes of this.\nThe case of absent or low RL has some initial advantages\nat the beginning of training. This can be seen from Fig-\nure 5 where the pure UL case exhibits the learning curve\nwith the sharpest initial increase of performance. Figures\nS1, S2 and S3 in Supplementary Materials show that this\neffect is also robustly present in sub-groups of simula-\ntions. The reason for this is that initially the models with\nhigh RL produce a highly variable exploratory behaviour,\nand thus the resulting reward signals that guide the learn-\ning process involving the deeper layers of the network are\nrare and unreliable. Instead, since the initial phases of\ntraining the UL process can proceed independently of the\nsuccess of behaviour, and so it can build representations\nneeded to support the learning of behaviour itself. How-\never, with the advancement of training the conditions with\nabsent/low levels of RL achieve a lower performance than\nthe more balanced conditions, as shown in Figure 6 and\nTable 1. Figures S1, S2 and S3 in Supplementary Materi-\nals conﬁrm the generality of this result. The reason is that\nUL tends to encode all features of the images, and so the\ntask-relevant features compete with them and remain with\ninsufﬁcient computational resources.\nThis interpretation is corroborated by the tests where we\nmanipulated the computational resources that were avail-\nable to the system, and speciﬁcally the number of the\nreward-biased units at the second layer of the DBN (Ta-\nble 2). These tests show that with a higher amount of\ncomputational resources the best performance is achieved\nby the models having a better balance of UL and RL, in\nparticular a higher level of UL. This is because the en-\ncoding of task-irrelevant features is less impairing for the\nencoding of task-relevant features. To appreciate the rel-\nevance of this result, it should be considered that in eco-\nlogical conditions the information received from sensors\n(e.g., the information from the retina) always overwhelms\nthe available computational resources, and so some task-\nbased feature selection is always advantageous.\nAt the opposite side of the spectrum, also models where\nRL is predominant or exclusive have computational lim-\nitations, as shown in Figure 6 and Table 1. The acquisi-\ntion of internal representations is slow in models solely\nbased on RL plasticity as learning is guided only by re-\nward. Hence, the reconﬁguration of the synaptic strengths\nare initially attained in an erratic fashion, as also discussed\nabove (Figure 5). Importantly, the system tends to incur in\nlocal minima with the progression of learning, as shown\nin Figure 9E.\nThese results suggest the general possibility that UL and\nRL might express advantages at different stages of learn-\ning, in particular, UL might be more useful at the begin-\nning of learning while RL at later stages. Future work\nmight thus aim to study how to dynamically regulate the\nUL/RL balance during learning.\nThe generative nature of the perceptual component of the\nmodel allowed the investigation of how the internal rep-\nresentations tend to cluster the images depending on the\ncategorisation rule of the task and the UL/RL mix bal-\nance. Graphs ‘A-B’ of Figures 7, 8, and 9 show that when\nRL is absent or low, UL tends to lead to the acquisition of\nall features of the image independently of their relevance\nfor the task, as also shown by the low reconstruction er-\nror obtained in this cases (Figure 10). Instead, graphs ‘C,\n13\nA PREPRINT - JUNE 8, 2021\nD’ of the same ﬁgures show how a mix of the two learn-\ning processes leads to the encoding of different attributes\nof the visual shapes. The images are grouped in relation\nto the responses to be associated to them, thus facilitat-\ning downstream action selection. Finally, when learning\nis only driven by reward, as shown by the graphs ‘E’ of\nthe ﬁgures, the internal representations collapse to only\none representation per group of images requiring a given\naction. This interpretation is also supported by the higher\nlevels of the reconstruction error obtained in these cases\n(Figure 10).\nFigure 11 exploits the generativity of the model to high-\nlight the features of the images that RL tends to isolate\nwhen it is increasingly strong. As the ﬁgure shows, these\nare few features that strongly differentiate the shapes into\nthe desired categories, for example features related to spe-\nciﬁc colours, or speciﬁc elements of the shapes or size.\nThese representations favour learning of the downstream\nactions but imply less robustness and make the system\nvulnerable to local minima, as we have seen above. More-\nover, this is expected to create representations that gen-\neralise poorly to new tasks, an important issue that de-\nserves further investigation as it might justify why the\nbrain seems to rely less on reward in early visual process-\ning stages.\nThese results show how the presented model has the po-\ntential to be used to interpret the empirical experiments\ninvestigating the well-known phenomenon for which the\ntasks accomplished tend to modulate the acquired percep-\ntual representations. This phenomenon has been shown\nin mice (Poort et al., 2015), primates (Sigala and Logo-\nthetis, 2002; De Baene et al., 2008; Emadi and Esteky,\n2014), and also humans (de Beeck et al., 2006; Astaﬁev\net al., 2004). In particular, the studies on humans involve\nthe wide research ﬁeld of ‘category learning’ (for reviews,\nsee Ashby and Maddox 2011b; Zeithamova et al. 2019).\nThe bulk of the research in this ﬁeld has traditionally fo-\ncused on the contrast between explicit versus procedural\nmechanisms for category learning (Maddox and Ashby,\n2004; Seger and Miller, 2010) and more recently on the\nprototype versus example-based nature of the acquired\nsensory representations (Mack et al., 2013; Bowman and\nZeithamova, 2018; Zeithamova et al., 2019). Although re-\nward and RL are considered important for category learn-\ning (Seger and Peterson, 2013; Chelazzi et al., 2013), only\nrecently few works have started to investigate how reward\naffects the acquired representations, the issue relevant for\nthe hypothesis and model proposed here. For example, it\nhas been shown that category learning in the ventrome-\ndial prefrontal cortex, inferior parietal cortex, and intra-\nparietal sulcus are affected by the reward (Braunlich and\nSeger, 2016; Zeithamova et al., 2019). To our knowledge,\nhowever, we still do not know how reward might inﬂu-\nence the formation of low-level representations of cate-\ngory features, for example within the extrastriate cortex\nof the brain parietal areas.\nA ﬁnal remark is that the proposed model is coherent\nwith the theoretical framework of embodied perception\n(DiFerdinando and Parisi, 2004; Vernon, 2008; Foglia and\nWilson, 2013) proposing that the brain constructs internal\nrepresentations of the world “for being ready to act”. In\nthis respect, the model speciﬁes a possible ways in which\nthe brain, based on the reward signals directly reaching its\ninner areas, might ‘warp’ representations in favour of the\npursued tasks.\n5\nConclusions\nWe have proposed the hypothesis that also non-motor cor-\ntices, in particular the extra-striate cortices, learn through\nboth associative mechanisms (unsupervised learning)\nand reward-based mechanisms (reinforcement learning).\nMoreover, we have proposed a bio-plausible computa-\ntional model facing a category-based sorting task to start\nto study how these mixed learning processes might affect\nthe acquired representations of stimuli.\nThe results obtained with the tests of the model show that\na suitably balanced mix of unsupervised and reinforce-\nment learning processes leads to the highest performance.\nOn one hand, excessive unsupervised learning tends to\nuse computational resources to represent all input features\nand thus to leave scarce resources for the representation of\ntask-relevant features. On the other hand, an excessive RL\ntends to lead to initial slow learning and to incur in local\nminima. Moreover, the results show how reward might\nlead to the acquisition of action-oriented representations\non the basis of bio-plausible mechanisms, and this favours\nthe selection of downstream actions.\nIn future work, the model could be used to address speciﬁc\ndata on the reward-based modulation of category learning\nin the brain cortex. Moreover, the model prompts further\ncomputational studies directed to investigate the possible\nadvantages of reward signals directly reaching the deep\nlayers of artiﬁcial neural networks.\n6\nAcknowledgements\nWe thank the Neuroscience Gateway (Sivagnanam et al.,\n2013) used to run most of the simulations. This work\nhas received funding from the European Union’s Hori-\nzon 2020 Research and Innovation Program, under Grant\nAgreement No 713010 of the project ‘GOAL-Robots –\nGoal-based Open-ended Autonomous Learning Robots’,\nand under Grant No 796135 of the H2020-MSCA-IF-\n2017 project ‘INTENSS’.\nReferences\nAmari, S.I., 1993. Backpropagation and stochastic gradi-\nent descent method. Neurocomputing 5, 185–196.\nArulkumaran, K., Deisenroth, M.P., Brundage, M.,\nBharath, A.A., 2017. Deep reinforcement learning: A\n14\nA PREPRINT - JUNE 8, 2021\nbrief survey. IEEE Signal Processing Magazine 34, 26–\n38. doi:10.1109/MSP.2017.2743240.\nAshby, F.G., Maddox, W.T., 2005. Human category learn-\ning. Annul Review of Psychology 56, 149–178.\nAshby, F.G., Maddox, W.T., 2011a.\nHuman category\nlearning 2.0. Annals of the New York Academy of Sci-\nences 1224, 147.\nAshby, F.G., Maddox, W.T., 2011b.\nHuman category\nlearning 2.0. Annals of the New York Academy of Sci-\nence 1224, 147–161.\nAstaﬁev, S.V., Stanley, C.M., Shulman, G.L., Corbetta,\nM., 2004. Extrastriate body area in human occipital\ncortex responds to the performance of motor actions.\nNature neuroscience 7, 542–548.\nBaldassarre, G., 2011.\nWhat are intrinsic motivations?\na biological perspective, in: Cangelosi, A., Triesch,\nJ., Fasel, I., Rohlﬁng, K., Nori, F., Oudeyer, P.Y.,\nSchlesinger, M., Nagai, Y. (Eds.), Proceedings of the\nInternational Conference on Development and Learn-\ning and Epigenetic Robotics (ICDL-EpiRob-2011).\nIEEE, New York, NY, pp. E1–8.\ndoi:10.1109/\nDEVLRN.2011.6037367.\nBaldassarre, G., Caligiore, D., Mannella, F., 2013. The\nhierarchical organisation of cortical and basal-ganglia\nsystems: a computationally-informed review and in-\ntegrated hypothesis, in: Baldassarre, G., Mirolli, M.\n(Eds.), Computational and Robotic Models of the Hi-\nerarchical Organisation of Behaviour. Springer-Verlag,\nBerlin, pp. 237–270.\nBaldassarre, G., Mirolli, M. (Eds.), 2013.\nIntrinsically\nmotivated learning in natural and artiﬁcial systems.\nSpringer, Berlin. Cost 91.62 euros, pp. 458, 82 illus-\ntrations, 55 illustrations in color.\nBarto, A., Mirolli, M., Baldassarre, G., 2013. Novelty or\nsurprise? Frontiers in Psychology – Cognitive Science\n4, E1–15. doi:10.3389/fpsyg.2013.00907.\nBarto, A.G., 1995. Adaptive critics and the basal ganglia,\nin: Houk, J.C., Davids, J.L., Beiser, D.G. (Eds.), Mod-\nels of Information Processing in the Basal Ganglia. The\nMIT Press, Cambridge, MA, pp. 215–232.\nde Beeck, H.P.O., Baker, C.I., DiCarlo, J.J., Kanwisher,\nN.G., 2006. Discrimination training alters object rep-\nresentations in human extrastriate cortex.\nJournal of\nNeuroscience 26, 13025–13036.\nBowman, C.R., Zeithamova, D., 2018.\nAbstract mem-\nory representations in the ventromedial prefrontal cor-\ntex and hippocampus support concept generalization.\nJournal of Neuroscience 38, 2605–2614.\nBraunlich, K., Seger, C.A., 2016. Categorical evidence,\nconﬁdence, and urgency during probabilistic catego-\nrization. Neuroimage 125, 941–952.\nCaligiore, D., Arbib, M.A., Miall, R.C., Baldassarre, G.,\n2019. The super-learning hypothesis: Integrating learn-\ning processes across cortex, cerebellum and basal gan-\nglia. Neuroscience & Biobehavioral Reviews 100, 19–\n34.\nCaporale, N., Dan, Y., 2008.\nSpike timing-dependent\nplasticity: a hebbian learning rule.\nAnnual Review\nof Neuroscience 31, 25–46. doi:10.1146/annurev.\nneuro.31.060407.125639.\nChelazzi, L., Perlato, A., Santandrea, E., Della Libera, C.,\n2013. Rewards teach visual selective attention. Vision\nresearch 85, 58–72.\nDe Baene, W., Ons, B., Wagemans, J., Vogels, R., 2008.\nEffects of category learning on the stimulus selectiv-\nity of macaque inferior temporal neurons. Learning &\nMemory 15, 717–727.\nDeYoe, E.A., Carman, G.J., Bandettini, P., Glickman, S.,\nWieser, J., Cox, R., Miller, D., Neitz, J., 1996. Map-\nping striate and extrastriate visual areas in human cere-\nbral cortex. Proceedings of the National Academy of\nSciences 93, 2382–2386.\nDiFerdinando, A., Parisi, D., 2004.\nInternal represen-\ntations of sensory input reﬂect the motor output with\nwhich organisms respond to the input, in: Carsetti, E.\n(Ed.), Seeing, Thinking and Knowing Theory and De-\ncision Library. volume 38, pp. 115–141.\nDoya, K., 1999. What are the computations of the cerebel-\nlum, the basal ganglia and the cerebral cortex? Neural\nNetworks 12, 961–974.\nDoya, K., 2000. Complementary roles of basal ganglia\nand cerebellum in learning and motor control. Current\nOpinion in Neurobiology 10, 732–739.\nEmadi, N., Esteky, H., 2014. Behavioral demand modu-\nlates object category representation in the inferior tem-\nporal cortex. Journal of neurophysiology 112, 2628–\n2637.\nFelleman, D.J., Van Essen, D.C., 1991. Distributed hierar-\nchical processing in the primate cerebral cortex. Cereb\nCortex 1, 1–47.\nFiore, V.G., Sperati, V., Mannella, F., Mirolli, M., Gur-\nney, K., Firston, K., Dolan, R.J., Baldassarre, G., 2014.\nKeep focussing: striatal dopamine multiple functions\nresolved in a single mechanism tested in a simulated\nhumanoid robot. Frontiers in Psychology – Cognitive\nScience 5, e1–17. doi:10.3389/fpsyg.2014.00124.\nFoglia, L., Wilson, R.A., 2013.\nEmbodied cognition.\nWiley Interdisciplinary Reviews: Cognitive Science 4,\n319–325.\nFroudist-Walsh, S., Bliss, D.P., Ding, X., Jankovic-Rapan,\nL., Niu, M., Knoblauch, K., Zilles, K., Kennedy,\nH., Palomero-Gallagher, N., Wang, X.J., 2020.\nA\ndopamine gradient controls access to distributed work-\ning memory in monkey cortex. bioRxiv .\nGerstner, W., Kistler, W.M., 2002. Spiking neuron mod-\nels: single neurons, populations, plasticity. Cambridge\nUniversity Press, Cambridge.\nGoodfellow, I., Bengio, Y., Courville, A., 2017.\nDeep\nLearning. The MIT Press, Boston, MA.\n15\nA PREPRINT - JUNE 8, 2021\nHanania, R., Smith, L.B., 2010. Selective attention and\nattention switching: Towards a uniﬁed developmental\napproach. Developmental Science 13, 622–635.\nHinton, G.E., 2002. Training products of experts by mini-\nmizing contrastive divergence. Neural computation 14,\n1771–1800.\nHinton, G.E., 2012. A practical guide to training restricted\nboltzmann machines, in: Neural networks: Tricks of\nthe trade. Springer, pp. 599–619.\nHinton, G.E., Osindero, S., Teh, Y.W., 2006. A fast learn-\ning algorithm for deep belief nets. Neural computation\n18, 1527–1554.\nHinton, G.E., Salakhutdinov, R.R., 2006. Reducing the\ndimensionality of data with neural networks. science\n313, 504–507.\nHopﬁeld, J.J., 1982. Neural networks and physical sys-\ntems with emergent collective computational abilities.\nProceedings of the national academy of sciences 79,\n2554–2558.\nHouk, J.C., Davids, J.L., Beiser, D.G. (Eds.), 1995. Mod-\nels of Information Processing in the Basal Ganglia. The\nMIT Press, Cambridge, MA.\nIlling, B., Gerstner, W., Brea, J., 2019. Biologically plau-\nsible deep learning—but how far can we go with shal-\nlow networks? Neural Networks 118, 90–101.\nImpieri, D., Zilles, K., Niu, M., Rapan, L., Schubert, N.,\nGalletti, C., Palomero-Gallagher, N., 2019. Receptor\ndensity pattern conﬁrms and enhances the anatomic-\nfunctional features of the macaque superior parietal\nlobule areas. Brain Structure and Function 224, 2733–\n2756.\nJacob, S.N., Nienborg, H., 2018. Monoaminergic neuro-\nmodulation of sensory processing. Frontiers in neural\ncircuits 12, 51.\nKim, T., Hamade, K.C., Todorov, D., Barnett, W.H.,\nCapps, R.A., Latash, E.M., Markin, S.N., Rybak, I.A.,\nMolkov, Y.I., 2017.\nReward based motor adaptation\nmediated by basal ganglia. Frontiers in computational\nneuroscience 11, 19.\nKonen, C.S., Kastner, S., 2008. Two hierarchically or-\nganized neural systems for object information in hu-\nman visual cortex. Nat Neurosci 11, 224–231. doi:10.\n1038/nn2036.\nLe Roux, N., Bengio, Y., 2008. Representational power\nof restricted boltzmann machines and deep belief net-\nworks. Neural computation 20, 1631–1649.\nLisman, J.E., Grace, A.A., 2005. The hippocampal-vta\nloop: controlling the entry of information into long-\nterm memory. Neuron 46, 703–713. doi:10.1016/j.\nneuron.2005.05.002.\nMack, M.L., Preston, A.R., Love, B.C., 2013. Decoding\nthe brain’s algorithm for categorization from its neural\nimplementation. Current Biology 23, 2023–2027.\nMaddox, W.T., Ashby, F.G., 2004. Dissociating explicit\nand procedural-learning based systems of perceptual\ncategory learning. Behavioural processes 66, 309–332.\nMannella, F., Baldassarre, G., 2015. Selection of corti-\ncal dynamics for motor behaviour by the basal ganglia.\nBiological Cybernetics 109, 575–595. doi:10.1007/\ns00422-015-0662-6.\nMarkram, H., Gerstner, W., Sjostrom, P.J., 2011.\nA\nhistory of spike-timing-dependent plasticity. Frontiers\nin Synaptic Neuroscience 3, 4. doi:10.3389/fnsyn.\n2011.00004.\nMcClelland,\nJ.L.,\nRumelhart,\nD.E.,\nthe PDP Re-\nsearch Group, 1986.\nParallel distributed processing:\nexplorations in the microstructure of cognition. volume\n1-2. The MIT Press, Cambridge,MA.\nMirolli, M., Mannella, F., Baldassarre, G., 2010.\nThe\nroles of the amygdala in the affective regulation of\nbody, brain and behaviour.\nConnection Science 22,\n215–245. doi:10.1080/09540091003682553.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Ve-\nness, J., Bellemare, M.G., Graves, A., Riedmiller, M.,\nFidjeland, A.K., Ostrovski, G., Petersen, S., Beattie,\nC., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,\nWierstra, D., Legg, S., Hassabis, D., 2015. Human-\nlevel control through deep reinforcement learning. Na-\nture 518, 529–533. doi:10.1038/nature14236.\nNguyen, T.T., Nguyen, N.D., Nahavandi, S., 2020. Deep\nreinforcement learning for multiagent systems: A re-\nview of challenges, solutions, and applications. IEEE\nTransactions on Cybernetics 50, 3826–3839. doi:10.\n1109/TCYB.2020.2977374.\nNiu, M., Impieri, D., Rapan, L., Funck, T., Palomero-\nGallagher, N., Zilles, K., 2020. Receptor-driven, mul-\ntimodal mapping of cortical areas in the macaque mon-\nkey intraparietal sulcus. Elife 9, e55979.\nO’Reilly, R.C., 2006. Biologically based computational\nmodels of high-level cognition. Science 314, 91–94.\ndoi:10.1126/science.1127242.\nPanksepp, J., 1998. Affective neuroscience: the founda-\ntions of human and animal emotions. Oxford Unversity\nPress, Oxford.\nPoort, J., Khan, A.G., Pachitariu, M., Nemri, A., Or-\nsolic, I., Krupic, J., Bauza, M., Sahani, M., Keller,\nG.B., Mrsic-Flogel, T.D., et al., 2015.\nLearning en-\nhances sensory and multiple non-sensory representa-\ntions in primary visual cortex. Neuron 86, 1478–1490.\nQuiroga, R.Q., Kreiman, G., Koch, C., Fried, I., 2008.\nSparse but not ‘grandmother-cell’coding in the medial\ntemporal lobe. Trends in cognitive sciences 12, 87–91.\nRedgrave, P., Gurney, K., 2006.\nThe short-latency\ndopamine signal: a role in discovering novel actions?\nNature Reviews Neuroscience 7, 967–975.\ndoi:10.\n1038/nrn2022.\nRibas-Fernandes, J.J.F., Solway, A., Diuk, C., McGuire,\nJ.T., Barto, A.G., Niv, Y., Botvinick, M.M., 2011. A\n16\nA PREPRINT - JUNE 8, 2021\nneural signature of hierarchical reinforcement learning.\nNeuron 71, 370–379. doi:10.1016/j.neuron.2011.\n05.042.\nRushworth, M.F., Noonan, M.P., Boorman, E.D., Wal-\nton, M.E., Behrens, T.E., 2011.\nFrontal cortex and\nreward-guided learning and decision-making. Neuron\n70, 1054–1069.\nSantucci, V.G., Baldassarre, G., Mirolli, M., 2016.\nGrail:\nA goal-discovering robotic architecture for\nintrinsically-motivated learning.\nIEEE Transactions\non Cognitive and Developmental Systems 8, 214–231.\ndoi:10.1109/TCDS.2016.2538961.\nSchultz, W., 2002.\nGetting formal with dopamine and\nreward. Neuron 36, 241–263.\nSeger, C.A., 2008. How do the basal ganglia contribute to\ncategorization? their roles in generalization, response\nselection, and learning via feedback. Neuroscience &\nBiobehavioral Reviews 32, 265–278.\nSeger, C.A., Miller, E.K., 2010. Category learning in the\nbrain. Annual Review of Neuroscience 33, 203–219.\nSeger, C.A., Peterson, E.J., 2013. Categorization= deci-\nsion making+ generalization. Neuroscience & Biobe-\nhavioral Reviews 37, 1187–1200.\nShao, K., Tang, Z., Zhu, Y., Li, N., Zhao, D., 2019. A\nsurvey of deep reinforcement learning in video games.\narXiv preprint .\nSigala, N., Logothetis, N.K., 2002. Visual categorization\nshapes feature selectivity in the primate temporal cor-\ntex. Nature 415, 318–320.\nSiu, C.R., Murphy, K.M., 2018. The development of hu-\nman visual cortex and clinical implications. Eye and\nbrain 10, 25.\nSivagnanam, S., Majumdar, A., Yoshimoto, K., Astakhov,\nV., Bandrowski, A.E., Martone, M.E., Carnevale, N.T.,\n2013. Introducing the neuroscience gateway. IWSG\n993.\nSutton, R., McAllester, D., Singh, S., Mansour, Y., 2000.\nPolicy gradient methods for reinforcement learning\nwith function approximation, in: Advances in neural\ninformation processing systems. The MIT Press, Cam-\nbridge, MA. 12, pp. 1057–1063.\nSutton, R.S., Barto, A.G., 2018. Reinforcement Learning:\nAn Introduction. Second edition, in progress ed., The\nMIT Press, Cambridge, Massachusetts.\nSutton, R.S., Barto, A.G., et al., 1998.\nReinforcement\nlearning: An introduction. MIT press.\nVernon, D., 2008. Cognitive vision: The case for em-\nbodied perception. Image and Vision Computing 26,\n127–140.\nWhite, R.W., 1959. Motivation reconsidered: the concept\nof competence. Psychol Rev 66, 297–333.\nWilliams,\nR.J.,\n1992.\nSimple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine learning 8, 229–256.\nWilliams, S.M., Goldman-Rakic, P.S., 1993.\nCharac-\nterization of the dopaminergic innervation of the pri-\nmate frontal cortex using a dopamine-speciﬁc antibody.\nCerebral Cortex 3, 199–222.\nWise, R.A., 2004. Dopamine, learning and motivation.\nNature Reviews Neuroscience 5, 483–494.\ndoi:10.\n1038/nrn1406.\nZappacosta, S., Mannella, F., Mirolli, M., Baldassarre,\nG., 2018. General differential hebbian learning: Cap-\nturing temporal relations between events in neural net-\nworks and the brain. Plos Computational Biology 14,\ne1006227. doi:10.1371/journal.pcbi.1006227.\nZeithamova, D., Mack, M.L., Braunlich, K., Davis, T.,\nSeger, C.A., van Kesteren, M.T.R., Wutz, A., 2019.\nBrain mechanisms of concept learning.\nThe Jour-\nnal of neuroscience : the ofﬁcial journal of the Soci-\nety for Neuroscience 39, 8259–8266.\ndoi:10.1523/\nJNEUROSCI.1166-19.2019.\n17\nA PREPRINT - JUNE 8, 2021\nSupplementary Material\nMethods: further details on the model simulations\nWe tested the model solving the sorting task with different task conditions (sorting rule, i.e. colour, shape or size)\nand perceptual component conﬁgurations (the number of neurons of top hidden layer and the reward contribution into\nthe learning process). We randomly changed these parameters, keeping the others ﬁxed. Table S1 shows the key\nparameters of simulations.\nSimulations parameters\nLabel\nValue/Range\nDescription\nSorting rule\n{colour, shape, size}\nVariable.\nlatent rule to solve the sorting task\nTraining epochs\n15 ∗103\nFixed.\nTraining epochs of sorting task.\nSingle-layer perceptron output units\n10\nFixed.\nOutput neurons of motor component.\nSingle-layer perceptron learning rate (REINFORCE)\n1 ∗10-2\nFixed.\nTraining learning rate of motor component.\nMulti-layers perceptron hidden units\n50\nFixed.\nHidden neurons of predictor component.\nMulti-layers perceptron learning rate (Backpropagation)\n1 ∗10-3\nFixed.\nTraining learning rate of predictor component.\nSingle-layer perceptron learning rate (REINFORCE)\n1 ∗10-2\nFixed.\nTraining learning rate of motor component.\nDBN units (visible layer)\n2352\nFixed.\nNeurons of visible layer.\nDBN units (ﬁrst hidden layer)\n200\nFixed.\nNeurons of ﬁrst hidden layer.\nDBN units (second hidden layer)\n{10, 50}\nVariable.\nNeurons of second hidden layer.\nFirst RBM (off-line) training epochs\n1 ∗103\nFixed.\nTraining epochs necessary to achieve a\ndataset reconstruction error of 0.001.\nFirst RBM learning rate (Constrastive Divergence)\n1 ∗10-2\nFixed.\nTraining (ofﬂine) learning rate.\nFirst RBM momentum (Constrastive Divergence)\n0.9\nFixed.\nTraining (ofﬂine) momentum.\nSecond RBM learning rate (Constrastive Divergence)\n1 ∗10-3\nFixed.\nTraining learning rate.\nSecond RBM momentum (Constrastive Divergence)\n0.9\nFixed.\nTraining momentum.\nSecond RBM learning rate (REINFORCE)\n1 ∗10-2\nFixed.\nTraining learning rate.\nλ\n{1, 0.1, 0.01, 0.001, 0}\nVariable.\nContribution of the Contrastive Divergence to the weights update\nSecond RBM reward contribution\n(1 −λ),\nwith λ ∈{1, 0.1, 0.01, 0.001, 0}\nVariable.\nContribution of the REINFORCE to the weights update\nTable S1: The table shows the simulations parameters.\nResults: further statistical analysis\nPerformances analysis\nFigures S1, S2, and S3 show the training curves of the models trained with different RL contributions in 15,000 epochs,\nfor the three sorting categories and for the condition with 10 units of the DBN second hidden layer. Figures S4, S5, and\nS6 show the analogous curves for the condition involving 50 units of the DBN second hidden layer. In particular, these\nare the models of the condition with a high-level of computational resources, namely 50 units at the level of the second\nhidden layer of the DBN. In all conditions, the L0 models (with no reinforcement learning - RL, i.e. relying only on\nunsupervised learning - UL) show an initial highest performance with respect to the other models L1, L2, L3, and L4.\nThis conﬁrms that in L0 models representation learning is initially facilitated with respect to models with a higher RL\n18\nA PREPRINT - JUNE 8, 2021\nLearning curves of models: colour category, low computational resources\nFigure S1: Reward per epoch in the task task involving the colour category and low computational resources, of the\nﬁve models involving different UL/RL levels, averaged over the models using a given level. Shaded areas represent\nthe curves standard deviations.\nLearning curves of models: shape category, low computational resources\nFigure S2: Reward per epoch in the task task involving the shape category and low computational resources, of the\nﬁve models involving different UL/RL levels, averaged over the models using a given level. Shaded areas represent\nthe curves standard deviations.\ncontribution as the reward is initially erratic. Moreover, for all three category tasks the reward achieves a maximum\nﬁnal performance for the L2 models having a balanced level of UL and RL. Indeed, these models outperform the\nmodels with absent or very low RL (L0 and L1) because these employ a lot of computational resources for non-task\nspeciﬁc features; moreover they outperform the models with very high or extreme RL (L3 and L4) because these tend\nto incur in local minima.\nTable S2 shows the post-hoc tests with the Bonferroni correction. The tests are grouped for each speciﬁc combination\nof the three main conditions, that is, computational resources (2 conditions), sorting rule used in the task (3 conditions),\nand reward contribution (5 conditions).\n19\nA PREPRINT - JUNE 8, 2021\nLearning curves of models: size category, low computational resources\nFigure S3: Reward per epoch in the task task involving the size category and low computational resources, of the ﬁve\nmodels involving different UL/RL levels, averaged over the models using a given level. Shaded areas represent the\ncurves standard deviations.\nLearning curves of models: colour category, high computational resources\nFigure S4: Reward per epoch in the task task involving the colour category and high computational resources, of the\nﬁve models involving different UL/RL levels, averaged over the models using a given level. Shaded areas represent\nthe curves standard deviations.\n20\nA PREPRINT - JUNE 8, 2021\nLearning curves of models: shape category, high computational resources\nFigure S5: Reward per epoch in the task task involving the shape category and high computational resources, of the\nﬁve models involving different UL/RL levels, averaged over the models using a given level. Shaded areas represent\nthe curves standard deviations.\nLearning curves of models: size category, high computational resources\nFigure S6: Reward per epoch in the task task involving the size category and high computational resources, of the ﬁve\nmodels involving different UL/RL levels, averaged over the models using a given level. Shaded areas represent the\ncurves standard deviations.\n21\nA PREPRINT - JUNE 8, 2021\nSorting rule: Colour, Computational Resources: Low\nAbsent (L0, N = 40)\nLow (L1, N = 34)\nModerate (L2, N = 21)\nHigh (L3, N = 38)\nExtreme (L4, N = 32)\nAbsent (L0)\n//\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\nLow (L1)\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\nModerate (L2)\n//\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\nHigh (L3)\n//\n//\n//\n//\np > 0.05 (NS)\nExtreme (L4)\n//\n//\n//\n//\n//\nTable S2: The table shows the post hoc multiple comparisons (t-test with Bonferroni correction) of models in case of\nthe colour sorting rule and low computational resources. NS = not signiﬁcant.\nSorting rule: Shape, Computational Resources: Low\nAbsent (L0, N = 32)\nLow (L1, N = 44)\nModerate (L2, N = 29)\nHigh (L3, N = 42)\nExtreme (L4, N = 39)\nAbsent (L0)\n//\np < 0.001\np < 0.001\np < 0.001\np < 0.001\nLow (L1)\n//\n//\np < 0.001\np < 0.001\np < 0.01\nModerate (L2)\n//\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\nHigh (L3)\n//\n//\n//\n//\np > 0.05 (NS)\nExtreme (L4)\n//\n//\n//\n//\n//\nTable S3: The table shows the post hoc multiple comparisons (t-test with Bonferroni correction) of models in case of\nthe shape sorting rule and low computational resources. NS = not signiﬁcant.\nSorting rule: Size, Computational Resources: Low\nAbsent (L0, N = 38)\nLow (L1, N = 35)\nModerate (L2, N = 39)\nHigh (L3, N = 28)\nExtreme (L4, N = 41)\nAbsent (L0)\n//\np < 0.001\np < 0.001\np < 0.001\np < 0.001\nLow (L1)\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\nModerate (L2)\n//\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\nHigh (L3)\n//\n//\n//\n//\np > 0.05 (NS)\nExtreme (L4)\n//\n//\n//\n//\n//\nTable S4: The table shows the post hoc multiple comparisons (t-test with Bonferroni correction) of models in case of\nthe size sorting rule and low computational resources. NS = not signiﬁcant.\nSorting rule: Colour, Computational Resources: High\nAbsent (L0, N = 39)\nLow (L1, N = 33)\nModerate (L2, N = 31)\nHigh (L3, N = 20)\nExtreme (L4, N = 43)\nAbsent (L0)\n//\np > 0.05 (NS)\np < 0.001\np > 0.05 (NS)\np < 0.01\nLow (L1)\n//\n//\np < 0.001\np > 0.05 (NS)\np < 0.05\nModerate (L2)\n//\n//\n//\np < 0.001\np < 0.01\nHigh (L3)\n//\n//\n//\n//\np > 0.05 (NS)\nExtreme (L4)\n//\n//\n//\n//\n//\nTable S5: The table shows the post hoc multiple comparisons (t-test with Bonferroni correction) of models in case of\nthe colour sorting rule and high computational resources. NS = not signiﬁcant.\nSorting rule: Shape, Computational Resources: High\nAbsent (L0, N = 41)\nLow (L1, N = 35)\nModerate (L2, N = 33)\nHigh (L3, N = 29)\nExtreme (L4, N = 33)\nAbsent (L0)\n//\np < 0.001\np < 0.001\np < 0.001\np > 0.05 (NS)\nLow (L1)\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\np < 0.001\nModerate (L2)\n//\n//\n//\np > 0.05 (NS)\np < 0.001\nHigh (L3)\n//\n//\n//\n//\np < 0.05\nExtreme (L4)\n//\n//\n//\n//\n//\nTable S6: The table shows the post hoc multiple comparisons (t-test with Bonferroni correction) of models in case of\nthe shape sorting rule and high computational resources. NS = not signiﬁcant.\n22\nA PREPRINT - JUNE 8, 2021\nSorting rule: Size, Computational Resources: High\nAbsent (L0, N = 24)\nLow (L1, N = 30)\nModerate (L2, N = 35)\nHigh (L3, N = 29)\nExtreme (L4, N = 29)\nAbsent (L0)\n//\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\nLow (L1)\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\np > 0.05 (NS)\nModerate (L2)\n//\n//\n//\np > 0.05 (NS)\np > 0.05 (NS)\nHigh (L3)\n//\n//\n//\n//\np > 0.05 (NS)\nExtreme (L4)\n//\n//\n//\n//\n//\nTable S7: The table shows the post hoc multiple comparisons (t-test with Bonferroni correction) of models in case of\nthe size sorting rule and high computational resources. NS = not signiﬁcant.\nReconstruction error and information stored\nIn this section we explain why the reconstruction errors of the DBN reported in the main text can be considered a\nmeasure of the information on the input patterns retained by this component of the models. Restricted Boltzmann\nMachines and Deep Belief Networks are generative models able to store the joint probability between an input and\nthe consequent hidden layer activation (Hinton et al., 2006; Hinton, 2012). This property makes these models able\nto execute a dimensional reduction of input patterns (Hinton and Salakhutdinov, 2006) and to ‘generate’ such input\npatters based on an inverse spread of activation spread from a hidden layer towards the visible layer. Due to the\ndifﬁculty of meaningfully activating the distributed representations within the hidden layers in a direct way, a typical\nway to exploit this generativity property also followed here is to precede the hidden-visible activation spreading by\na standard visible-hidden activation. This allows the computation of the reconstruction error, corresponding to the\ndifference between an input pattern and the corresponding reconstruction. This error is relevant as it represents a\nmeasure of the information that the system has retained on the input pattern.\nInternal representations analysis: PCA and K-means details\nIn the main test we illustrated the results obtained on average over whole classes of simulations. Here we show the\noutcome of the PCA (principal component analysis) and K-means analyses exemplifying the results within each class.\nIn particular, we considered examples that were more aligned with the average scores of the classes as they should be\nmore representative of the classes themselves.\nTables S8, S9, and S10 show the cumulative explained variance of the the PCA in correspondence to a growing number\nof principal components. The plots presented in the main text had an n = 2 corresponding to the ﬁrst two principal\ncomponents. This value is acceptable because it is almost always higher than the median cumulative explained variance\nand at the same time allowed us to plot the components of the reconstructed images. An interesting feature that emerges\nfrom the values is that with a higher value of RL the ‘elbow’ of the curves represented by the numbers reported in the\ntables become sharper. This is in line with the fact that with a higher RL contribution the images tend to be increasingly\nclustered into groups corresponding to the actions to be returned while the task-irrelevant features are discarded, thus\nneeding less components to be represented.\nTables S11, S12, S13 show the silhouette values of the k-means algorithm corresponding to different K values estab-\nlishing the number of the searched classes. The tables show that the the highest silhouette values tend to correspond to\nK = 4, the value used in the analyses reported in the main text. This value is relevant as it corresponds to the number\nof attributes in each category and to which the model has to assign a different action (colour: red, green, blue, yellow;\nform: square, circle, triangle, bar; size: large, medium-large, medium-small, small). It is also interesting to observe\nthat the best silhouette value is more highly differentiated from other values in correspondence to higher levels of RL\ncontribution: this agrees with the fact that in these conditions the model tends to encode features that are more closely\nrelated to the actions.\n23\nA PREPRINT - JUNE 8, 2021\nPCA cumulative variance explained\n(Sorting rule: Colour)\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nN = 1\n0.39\n0.48\n0.67\n0.63\n0.64\nN = 2\n0.62\n0.76\n0.99\n0.99\n0.99\nN = 3\n0.74\n0.86\n1\n1\n1\nN = 4\n0.81\n0.91\n1\n1\n1\nN = 5\n0.85\n0.94\n1\n1\n1\nN = 6\n0.89\n0.96\n1\n1\n1\nN = 7\n0.91\n0.97\n1\n1\n1\nMedian\n0.81\n0.85\n1\n1\n1\nTable S8: Cumulative explained variance (CEV) of the PCA run over the reconstructed images of the models, in the\ncase of the colour sorting rule and low computational resources. The n = 2 CEV values are highlighted in bold.\nPCA cumulative variance explained\n(Sorting rule: Shape)\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nN = 1\n0.39\n0.48\n0.53\n0.73\n0.67\nN = 2\n0.64\n0.74\n0.87\n0.94\n0.98\nN = 3\n0.76\n0.83\n0.99\n0.99\n1\nN = 4\n0.82\n0.89\n0.99\n1\n1\nN = 5\n0.85\n0.93\n1\n1\n1\nN = 6\n0.88\n0.94\n1\n1\n1\nN = 7\n0.91\n0.96\n1\n1\n1\nMedian\n0.82\n0.89\n1\n1\n1\nTable S9: Cumulative explained variance (CEV) of the PCA run over the reconstructed images of the models, in the\ncase of the shape sorting rule and low computational resources. The n = 2 CEV values are highlighted in bold.\nPCA cumulative variance explained\n(Sorting rule: Size)\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nN = 1\n0.39\n0.47\n0.61\n0.70\n0.60\nN = 2\n0.63\n0.76\n0.91\n0.90\n1\nN = 3\n0.75\n0.86\n0.99\n0.99\n1\nN = 4\n0.81\n0.92\n1\n1\n1\nN = 5\n0.85\n0.95\n1\n1\n1\nN = 6\n0.88\n0.96\n1\n1\n1\nN = 7\n0.90\n0.97\n1\n1\n1\nMedian\n0.81\n0.92\n1\n1\n1\nTable S10: Cumulative explained variance (CEV) of the PCA run over the reconstructed images of the models, in the\ncase of the size sorting rule and low computational resources. The n = 2 CEV values are highlighted in bold.\n24\nA PREPRINT - JUNE 8, 2021\nK-means Silhouette values\n(Sorting rule: Colour)\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nK = 2\n0.47\n0.50\n0.71\n0.55\n0.63\nK = 3\n0.56\n0.61\n0.91\n0.80\n0.80\nK = 4\n0.64\n0.65\n1\n1\n1\nK = 5\n0.66\n0.63\n0.86\n0.69\n0.91\nK = 6\n0.69\n0.64\n0.53\n0.53\n0.78\nK = 7\n0.73\n0.67\n0.52\n0.27\n0.66\nK = 8\n0.72\n0.66\n0.44\n0.42\n0.60\nMean\n0.64\n0.62\n0.71\n0.61\n0.77\nTable S11: The table shows the K-means silhouette values of models in case of colour sorting rule and low computa-\ntional resources. The K = 4 silhouette values are highlighted in bold\nK-means Silhouette values\n(Sorting rule: Shape)\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nK = 2\n0.44\n0.51\n0.55\n0.69\n0.63\nK = 3\n0.53\n0.61\n0.74\n0.84\n0.82\nK = 4\n0.63\n0.68\n0.94\n0.99\n0.92\nK = 5\n0.65\n0.66\n0.99\n0.98\n0.98\nK = 6\n0.67\n0.66\n0.98\n0.96\n0.98\nK = 7\n0.70\n0.66\n0.80\n0.93\n0.71\nK = 8\n0.66\n0.63\n0.80\n0.94\n0.65\nMean\n0.61\n0.63\n0.83\n0.90\n0.81\nTable S12: The table shows the K-means silhouette values of models in case of shape sorting rule and low computa-\ntional resources. The K = 4 silhouette values are highlighted in bold\nK-means Silhouette values\n(Sorting rule: Size)\nAbsent (L0)\nLow (L1)\nModerate (L2)\nHigh (L3)\nExtreme (L4)\nK = 2\n0.47\n0.53\n0.65\n0.70\n0.75\nK = 3\n0.55\n0.63\n0.87\n0.83\n0.86\nK = 4\n0.64\n0.72\n0.95\n1.0\n0.99\nK = 5\n0.64\n0.71\n0.97\n0.87\n0.98\nK = 6\n0.66\n0.71\n0.94\n0.71\n0.69\nK = 7\n0.68\n0.72\n0.97\n0.48\n0.66\nK = 8\n0.68\n0.72\n0.95\n0.48\n0.49\nMean\n0.62\n0.68\n0.90\n0.73\n0.78\nTable S13: The table shows the K-means silhouette values of models in case of size sorting rule and low computational\nresources. The K = 4 silhouette values are highlighted in bold\n25\n",
  "categories": [
    "q-bio.NC",
    "cs.LG"
  ],
  "published": "2021-06-07",
  "updated": "2021-06-07"
}