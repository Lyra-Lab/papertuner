{
  "id": "http://arxiv.org/abs/2106.03738v1",
  "title": "Unsupervised Action Segmentation for Instructional Videos",
  "authors": [
    "AJ Piergiovanni",
    "Anelia Angelova",
    "Michael S. Ryoo",
    "Irfan Essa"
  ],
  "abstract": "In this paper we address the problem of automatically discovering atomic\nactions in unsupervised manner from instructional videos, which are rarely\nannotated with atomic actions. We present an unsupervised approach to learn\natomic actions of structured human tasks from a variety of instructional videos\nbased on a sequential stochastic autoregressive model for temporal segmentation\nof videos. This learns to represent and discover the sequential relationship\nbetween different atomic actions of the task, and which provides automatic and\nunsupervised self-labeling.",
  "text": "Unsupervised Action Segmentation for Instructional Videos\nAJ Piergiovanni\nRobotics at Google\nAnelia Angelova\nRobotics at Google\nMichael Ryoo\nRobotics at Google\nIrfan Essa\nGoogle Research\nAbstract\nIn this paper we address the problem of automatically\ndiscovering atomic actions in unsupervised manner from in-\nstructional videos, which are rarely annotated with atomic\nactions.\nWe present an unsupervised approach to learn\natomic actions of structured human tasks from a variety of\ninstructional videos based on a sequential stochastic au-\ntoregressive model for temporal segmentation of videos.\nThis learns to represent and discover the sequential rela-\ntionship between different atomic actions of the task, and\nwhich provides automatic and unsupervised self-labeling.\n1. Introduction\nInstructional videos cover a wide range of tasks: cook-\ning, furniture assembly, repairs, etc. The availability of on-\nline instructional videos for almost any task provides a valu-\nable resource for learning, especially in the case of learning\nrobotic tasks. So far, the primary focus of activity recog-\nnition has been on supervised classiﬁcation or detection of\ndiscrete actions in videos. However, instructional videos\nare rarely annotated with atomic action-level instructions.\nIn this work, we propose a method to learn to segment in-\nstructional videos in atomic actions in an unsupervised way,\ni.e., without any annotations. To do this, we take advan-\ntage of the structure in instructional videos: they comprise\ncomplex actions which inherently consist of smaller atomic\nactions with predictable order. While the temporal struc-\nture of activities in instructional videos is strong, there is\nhigh variability of the visual appearance of actions, which\nmakes the task, especially in its unsupervised setting, very\nchallenging. For example, videos of preparing a salad can\nbe taken in very different environments, using kitchenware\nand ingredients of varying appearance.\nThe central idea is to learn a stochastic model that gen-\nerates multiple, different candidate sequences, which can\nbe ranked based on instructional video constraints. The top\nranked sequence is used as self-labels to train the action seg-\nmentation model. By iterating this process in an EM-like\nprocedure, the model converges to a good segmentation of\nInput\nVideo\nN \nGenerated \nSequences\nTop ranked self-label sequence used for learning\nModel\nFigure 1. Overview: Our model generates multiple sequences for\neach video which are ranked based on several constraints (colors\nrepresent different actions). The top ranked sequence is used as\nself-labels to train the action segmentation model. This processes\nis repeated until convergence. No annotations are used.\nInitial State\nN1\nFC-layer\nRule logits\nR-dim\nGumbel-\nSoftmax\nSelected Rule\nr\nFC-layers\nState\nN2\nAction\na1\nFC-layer\nRule logits\nR-dim\nGumbel-\nSoftmax\nSelected Rule\nr\nFC-layers\nState\nN3\nAction\na2\nFigure 2. Overview of the stochastic recurrent model which gen-\nerates an output action per step and a latent state (which will in\nturn generate next actions). Each time the model is run, a different\nrule is selected, thanks to the Gumbel-Softmax trick, leading to a\ndifferent action and state. This results in multiple sequences.\nactions (Figure 1). In contrast to previous weakly [12, 4]\nand unsupervised [1, 8] action learning works, our method\nonly requires input videos, no further annotations are used.\nWe evaluate the approach on multiple datasets and com-\npare to previous methods on unsupervised action segmenta-\ntion. We also compare to weakly-supervised and supervised\nbaselines. Our unsupervised method outperforms all state-\nof-the-art models, in some cases considerably, with perfor-\nmance at times outperforming weakly-supervised methods.\nOur contributions are (1) a stochastic model capable of\ncapturing multiple possible sequences, (2) a set of con-\nstraints and training method that is able to learn to segment\nactions without any labeled data.\nRelated Work Studying instructional videos has gained\na lot of interest recently [1, 11], largely fueled by ad-\nvancements in feature learning and activity recognition for\nvideos. However, most work on activity segmentation has\nfocused on the fully-supervised case, which requires per-\n1\narXiv:2106.03738v1  [cs.CV]  7 Jun 2021\nCrack egg\nFry Egg\nServe Egg\nFlip Egg\nCrack egg\nFry Egg\nServe \nEgg\nFlip Egg\nCrack egg\nServe \nEgg\nFry Egg\nFlip Egg\nCandidate Sequences\nFry Egg\nFigure 3. Multiple candidate sequences are generated and ranked.\nThe best sequence according to the ranking function is chosen as\nthe labels for the iteration.\nframe labels of the occurring activities. Since it is expen-\nsive to fully annotate videos, weakly-supervised activity\nsegmentation has been proposed. Initial works use movie\nscripts to obtain weak estimates of actions [9] or localize\nactions based on related web images [3]. Several unsuper-\nvised methods have also been proposed [1, 8, 15].\n2. Method\nOur goal is to discover atomic actions from a set of\ninstructional videos, while capturing and modeling their\ntemporal structure. Formally, given a set of videos V =\n{V 1, V 2, ...} of a task or set of tasks, the objective is to\nlearn a model that maps a sequence of frames V i = [It]T\nt=1\nfrom any video to a sequence of atomic action symbols\n[at ∈O]T\nt=1 where O is a set of possible action symbols.\nIn the unsupervised case, similar to previous works [1,\n8], we assume no action labels or boundaries are given. Our\nmodel, however, works with a ﬁxed k-the number of actions\nper task (analogous to setting k in k-means clustering). This\nis not a very strict assumption as the number of expected\natomic actions per instruction is roughly known.\nSequential Stochastic Autoregressive Model.\nThe\nmodel consists of three components: (H, O, R) where H\nis a ﬁnite set of states, O is a ﬁnite set of output sym-\nbols, and R is a ﬁnite set of transition rules mapping from\na state to an output symbol and next state. Importantly, this\nmodel is stochastic, i.e., each rule is additionally associated\nwith a probability of being selected. To implement this, we\nuse fully-connected layers and the Gumbel-Softmax trick\n[5]. The model is applied autoregressively to generate a se-\nquence (Figure 2).\nFor a video as input, we process each RGB frame by a\nCNN, resulting in a sequence of feature vectors. The model\ntakes each feature as input and concatenates it with the state\nwhich is used as input to produce the output action. Once\napplied to every frame, this results in a sequence of actions.\nLearning by Self-Labeling of Videos. In order to train\nthe model without ground truth action sequences, we intro-\nduce an approach of learning by ‘self-labeling’ videos. The\nidea is to optimize the model by generating self-supervisory\nlabels that best satisﬁes the constraints required for atomic\nactions.\nWe ﬁrst generate multiple candidate sequences,\nthen rank them based on the instructional video constraints,\nwhich importantly require no labeled data.\nSince the\nGumbel-Softmax adds randomness to the model, the out-\nput can be different each time G is run with the same in-\nput, which is key to the approach. The ranking function we\npropose to capture the structure of instructional videos has\nmultiple components: (1) Every atomic action must occur\nonce in the task. (2) Every atomic action should have simi-\nlar lengths across videos of the same task. (3) Each symbol\nshould reasonably match the provided visual feature.\nThe best sequence according to the ranking is selected as\nthe action labels for the iteration (Fig. 3), and the network\nis trained using a standard cross-entropy loss. We note that\ndepending on the structure of the dataset, these constraints\nmay be adjusted, or others more suitable ones can be de-\nsigned. In Fig. 4, we show the top 5 candidate sequences\nand show how they improve over the learning process.\nAction Occurrence: Given a sequence S of output ac-\ntions, the ﬁrst constraint ensures that every action appears\nonce.\nFormally, it is implemented as C1(S) = |O| −\nP\na∈O App(a), where App is 1 if a is in S otherwise 0.\nModeling Action Length: This constraint ensures each\natomic action has a similar duration across different videos.\nThe simplest approach is to compute the difference in length\ncompared to the average action length in the video. We also\ncompare to sampling the length from a distribution (e.g.,\nPoisson or Gaussian). C2(S) = P\na∈O(1 −p(L(a, S)))\nwhere L(a, S) computes the length of action a in sequence\nS and is Poisson or Gaussian. The Poisson and Gaussian\ndistributions have parameters which control the expected\nlength of the actions in videos. The parameters can be set\nstatically or learned for each action.\nModeling Action Probability: The third constraint is\nimplemented using the separate classiﬁcation layer of the\nnetwork p(a|f), which gives the probability of the frame\nbeing classiﬁed as action a. Formally, C3(S) = PT\nt=1(1 −\np(at|ft)), which is the probability that the given frame be-\nlongs to the selected action. This constraint is separate from\nthe sequential model and captures independent appearance\nbased probabilities.auto-regressive model. We ﬁnd that us-\ning both allows for the creation of the action probability\nterm, which is useful empirically.\nWe can then compute the rank of any sequence as\nC(S) = γ1C1(S)+γ2C2(S)+γ3C3(S), where γi weights\nthe impact of each term. In practice setting γ2 and γ3 to\n1\n|S|\nand γ1 =\n1\n|O| works well.\nLearning Actions: To choose the self-labeling, we sam-\nple K sequences, compute the cost and select the one that\nminimizes the above cost function. This gives the best seg-\nmentation of actions (at this iteration of labeling) based on\nthe deﬁned constraints.\nEpoch 0\nEpoch 50\nEpoch 100\nEpoch 400\nEpoch 300\nEpoch 200\nFigure 4. Candidate sequences at different stages of training. The\nsequences shown are the top 5 ranked sequences (rows) at the\ngiven epoch. The top one is selected as supervision for the given\nstep. The colors represent the discovered action (with no labels).\nMethod\nNIV (F1)\n50Sal (Acc)\nBR (MoF)\nBR (Jac)\nSupervised Baselines\nVGG from [1]\n0.376\n60.8\n62.8\n75.4\nI3D\n0.472\n72.8\n67.8\n79.4\nAssembleNet [14]\n0.558\n77.6\n72.5\n82.1\nWeakly-supervised\nCTC [4] + [14]\n0.312\n11.9\n72.5\n82.1\nHTK [7]\n-\n24.7\n-\n-\nHMM + RNN [12]\n-\n45.5\n33.3\n47.3\nNN-Viterbi [13]\n-\n49.4\n-\n-\nECTC [4] + [14]\n0.334\n-\n27.7\n-\nUnsupervised\nUniform Sampling\n0.187\n-\n-\n-\nAlayrac et al. [1]\n0.238\n-\n-\n-\nKukleva et al [8]\n0.283\n30.2\n41.8\n-\nJointSeqFL, [2]\n0.373\n-\n-\n-\nSCV [10]\n-\n-\n30.2\n-\nSener et al. [15]\n-\n-\n34.6\n47.1\nOurs\n0.457\n39.7\n43.5\n54.4\nTable 1. Results on the NIV (left column), 50-salads (50Sal) (mid-\ndle) and Breakfast(BR) (right) datasets. We report metrics adopted\nfrom prior work per each dataset, where available.\nCross-Video Matching\nThe above constraints work well\nfor a single video, however when we have multiple videos\nwith the same actions, we can further improve the ranking\nfunction by adding a cross-video matching constraint. The\nmotivation for this is that while breaking an egg can be vi-\nsually different between two videos, the action is the same.\nGiven a video segment the model labeled as an action\nfa from one video, a segment ˆfa the model labeled as the\nsame action from a second video, and a segment fb the mod-\neled labeled as a different action from any video, the cross-\nvideo similarity is computed using a triplet loss or a con-\ntrastive loss. As these functions are differentiable, they can\nbe added to the loss or cost function or both.\nSelf-labeling Training Method. We now describe the\nfull training method, which follows an EM-like procedure.\nIn the ﬁrst step, we ﬁnd the optimal set of action self-labels\ngiven the current model parameters and the ranking func-\ntion. In the second step, we optimize the model parame-\nters (and optionally some ranking function parameters) for\nthe selected self-labeling. After taking both steps, we have\ncompleted one iteration.\nSegmenting a video at inference: CNN features are\ncomputed for each frame and the learned model is applied\non those features. During rule selection, we greedily se-\nlect the most probable rule. Future work can improve this\nby considering multiple possible sequences (e.g., following\nthe Viterbi algorithm).\n3. Experiments\nWe evaluate our unsupervised atomic action discovery\napproach on multiple video segmentation datasets: (1) 50-\nsalads dataset [16], which contains 50 videos of people\nmaking salads (i.e., a single task). The videos contain the\nsame set of actions (e.g., cut lettuce, cut tomato, etc.), but\nthe ordering of actions is different in each video, (2) Nar-\nrated Instructional Videos (NIV) dataset [1], which con-\ntains 5 different tasks (CPR, changing a tire, making coffee,\njumping a car, re-potting a plant), (3) Breakfast [6] which\ncontains videos of people making breakfast dishes from var-\nious camera angles and environments.\nEvaluation Metrics: We follow all previously established\nprotocols for evaluation in each dataset. We ﬁrst use the\nHungarian algorithm to map the predicted action symbols\nto action classes in the ground truth. Since different met-\nrics are used for different datasets we report the previously\nadopted metrics per dataset.\n3.1. Comparison to the state-of-the-art\nWe compare to previous state-of-the-art methods on the\nthree datasets (Table 1). Our approach provides better seg-\nmentation results than previous unsupervised approaches\nand even for some weakly-supervised methods.\nQualitative Analysis Fig. 4 shows the generated candidate\nsequences at different stages of learning. It can be seen that\ninitially the generated sequences are entirely random and\nover-segmented. As training progresses, the generated se-\nquences start to match the constraints. After 400 epochs,\nthe generated sequences show similar order and length con-\nstraints, and better match the ground truth (as shown in the\nevaluation). Fig. 6 shows example results of our method.\n3.2. Ablation experiments\nEffects of the cost function constraints. To determine how\neach cost function impacts the resulting performance, we\ncompare various combinations of the terms. The results are\nshown in Table 2. We ﬁnd that each term is important to the\nself-labeling of the videos1. Generating better self-labels\nimproves model performance, and each component is bene-\nﬁcial to the selection process. Intuitively, this makes sense,\n0For the weakly-supervised setting, we use activity order as supervi-\nsion, equivalent to previous works.\n1These ablation methods do not use our full cross-video matching or\naction duration learning, thus the performances are slightly lower than the\nour best results.\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nChange Tire (11)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nCPR (7)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nRepot Plant (7)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nMake Coffee (10)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nOurs\nAlayrac et al.\nUniform\nSener et al.\nJump Car (12)\nFigure 5. F1 value for varying the number of actions used in the model, compared to prior work. The number in parenthesis indicates the\nground-truth number of actions for each activity. Full results are in the sup. materials.\nCost\n50-Salads\nBrkfst\nRandomly pick candidate\n12.5\n10.8\nNo Gumbel-Softmax\n10.5\n9.7\nOccurrence (C1)\n22.4\n19.8\nLength (C2)\n19.6\n17.8\np(a|f) (C3)\n21.5\n18.8\nC1 + C2\n27.5\n25.4\nC1 + C3\n30.3\n28.4\nC2 + C3\n29.7\n27.8\nC1 + C2 + C3\n33.4\n29.8\nTable 2. Ablation with cost function terms2\nMethod\nchng\nCPR\nrepot\nmake\njump\nAvg.\ntire\nplant\ncoffee\ncar\nAlaryac et al. [1]\n0.41\n0.32\n0.18\n0.20\n0.08\n0.238\nKukleva et al. [8]\n-\n-\n-\n-\n-\n0.283\nOurs VGG\n0.53\n0.46\n0.29\n0.35\n0.25\n0.376\nOurs AssembleNet\n0.63\n0.54\n0.381\n0.42\n0.315\n0.457\nTable 3. Comparison on the NIV dataset of the proposed approach\non VGG and AssembleNet features.\nBreak On\nGet things out\nStart Loose \nJack Up\nUnscrew\nPut on Wheel\nTighten Wheel\nPut Away\n1\n2\n3\n4\n5\n6\n7\n8\nGet things \nout\nStart Loose \nJack Up\nUnscrew\nPut On\nTighten Wheel\nJack \nDown\nScrew Wheel\n1\n2\n3\n4\n5\n8\n6\n7\nFigure 6. Two example videos from the ‘change tire’ activity. The\nground truth is shown in grey, the model’s top rank segmentation\nis shown in colors. NIV dataset.\nas the terms were picked based on prior knowledge about\ninstructional videos. We also compare to random selection\nof the candidate labeling and a version without using the\nGumbel-Softmax. Both alternatives perform poorly, con-\nﬁrming the beneﬁt of the proposed approach.\nVarying the number of actions. As O is a hyper-parameter\ncontrolling the number of actions to segment the video into,\nwe conduct experiments on NIV varying the number of ac-\ntions/size of O to evaluate the effect this hyper-parameter\nhas. The results are shown in Figure 5. Overall, we ﬁnd that\nthe model is not overly-sensitive to this hyper-parameter,\nbut it does have some impact on the performance due to the\nfact that each action must appear at least once in the video.\nFeatures. As our work uses AssembleNet [14] features,\nin Table 3 we compare the proposed approach to previous\nones using both features. As shown, even when using VGG\nfeatures, our approach outperforms previous methods.\nReferences\n[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,\nJosef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu-\npervised learning from narrated instruction videos. In CVPR,\n2016. 1, 2, 3, 4\n[2] Ehsan Elhamifar and Zwe Naing. Unsupervised procedure\nlearning via joint dynamic summarization. 2019. 3\n[3] Chuang Gan, Chen Sun, Lixin Duan, and Boqing Gong.\nWebly-supervised video recognition by mutually voting for\nrelevant web images and web video frames. In ECCV. 2\n[4] De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Con-\nnectionist temporal modeling for weakly supervised action\nlabeling. In ECCV, 2016. 1, 3\n[5] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-\nrameterization with gumbel-softmax. In ICLR, 2017. 2\n[6] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language\nof actions: Recovering the syntax and semantics of goal-\ndirected human activities. In CVPR, 2014. 3\n[7] Hilde Kuehne, Alexander Richard, and Juergen Gall. Weakly\nsupervised learning of actions from transcripts. CVIU, 2017.\n3\n[8] Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen\nGall. Unsupervised learning of action classes with contin-\nuous temporal embedding. In CVPR, 2019. 1, 2, 3, 4\n[9] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Ben-\njamin Rozenfeld.\nLearning realistic human actions from\nmovies. In CVPR. 2\n[10] Jun Li and Sinisa Todorovic. Set-constrained viterbi for set-\nsupervised action segmentation. In CVPR, 2020. 3\n[11] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowto100m: Learning a text-video embedding by watching\nhundred million narrated video clips. In ICCV. 1\n[12] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly\nsupervised action learning with rnn based ﬁne-to-coarse\nmodeling. In CVPR, 2017. 1, 3\n[13] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juer-\ngen Gall. NeuralNetwork-Viterbi: A framework for weakly\nsupervised video learning. In CVPR. 3\n[14] Michael Ryoo, AJ Piergiovanni, Mingxing Tan, and Anelia\nAngelova. Assemblenet: Searching for multi-stream neural\nconnectivity in video architectures. In ICLR, 2020. 3, 4\n[15] Fadime Sener and Angela Yao. Unsupervised learning and\nsegmentation of complex activities from video. In CVPR,\npages 8368–8376, 2018. 2, 3\n[16] Sebastian Stein and Stephen J McKenna. Combining em-\nbedded accelerometers with computer vision for recognizing\nfood preparation activities. In ACM Pervasive and ubiquitous\ncomputing, 2013. 3\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-06-07",
  "updated": "2021-06-07"
}