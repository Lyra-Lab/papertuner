{
  "id": "http://arxiv.org/abs/2106.05426v4",
  "title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses",
  "authors": [
    "Richard Antonello",
    "Javier Turek",
    "Vy Vo",
    "Alexander Huth"
  ],
  "abstract": "How related are the representations learned by neural language models,\ntranslation models, and language tagging tasks? We answer this question by\nadapting an encoder-decoder transfer learning method from computer vision to\ninvestigate the structure among 100 different feature spaces extracted from\nhidden representations of various networks trained on language tasks. This\nmethod reveals a low-dimensional structure where language models and\ntranslation models smoothly interpolate between word embeddings, syntactic and\nsemantic tasks, and future word embeddings. We call this low-dimensional\nstructure a language representation embedding because it encodes the\nrelationships between representations needed to process language for a variety\nof NLP tasks. We find that this representation embedding can predict how well\neach individual feature space maps to human brain responses to natural language\nstimuli recorded using fMRI. Additionally, we find that the principal dimension\nof this structure can be used to create a metric which highlights the brain's\nnatural language processing hierarchy. This suggests that the embedding\ncaptures some part of the brain's natural language representation structure.",
  "text": "Low-Dimensional Structure in the Space of Language\nRepresentations is Reﬂected in Brain Responses\nRichard Antonello\nUT Austin\nrjantonello@utexas.edu\nJavier Turek\nIntel Labs\njavier.turek@intel.com\nVy Vo\nIntel Labs\nvy.vo@intel.com\nAlexander Huth\nUT Austin\nhuth@cs.utexas.edu\nAbstract\nHow related are the representations learned by neural language models, translation\nmodels, and language tagging tasks? We answer this question by adapting an\nencoder-decoder transfer learning method from computer vision to investigate\nthe structure among 100 different feature spaces extracted from hidden represen-\ntations of various networks trained on language tasks. This method reveals a\nlow-dimensional structure where language models and translation models smoothly\ninterpolate between word embeddings, syntactic and semantic tasks, and future\nword embeddings. We call this low-dimensional structure a language represen-\ntation embedding because it encodes the relationships between representations\nneeded to process language for a variety of NLP (natural language processing) tasks.\nWe ﬁnd that this representation embedding can predict how well each individual\nfeature space maps to human brain responses to natural language stimuli recorded\nusing fMRI. Additionally, we ﬁnd that the principal dimension of this structure can\nbe used to create a metric which highlights the brain’s natural language processing\nhierarchy. This suggests that the embedding captures some part of the brain’s\nnatural language representation structure.\n1\nIntroduction\nThere are a multitude of common techniques for analytically representing the information contained in\nnatural language. At the word level, language is often represented by word embeddings, which capture\nsome aspects of word meaning using word co-occurrence statistics [8, 25]. Language representations\nthat highlight speciﬁc linguistic properties, such as parts-of-speech [29] or sentence chunks [1], or\nthat utilize well-known NLP models such as the intermediate layers of pretrained language models\n[6, 10, 21, 27], are also frequently studied [12, 31]. In ﬁelds such as linguistics, natural language\nprocessing, and cognitive neuroscience, qualitative adjectives are often used to describe these language\nrepresentations – e.g. “low-level” or “high-level” and “syntactic” or “semantic”. The use of these\nwords belies an unstated hypothesis about the nature of the space of language representations –\nnamely that this space is fundamentally low-dimensional, and therefore that the information from\nthe representations in this space can be efﬁciently described using a few categorical descriptors. In\nthis work, we attempt to directly map the low-dimensional space of language representations by\ngenerating “representation embeddings” using a method inspired by the work of Zamir et al. [44].\nThis method uses the transfer properties between representations to map their relationships.\nThe work described here has two main contributions. First, we used the representation embeddings to\ndemonstrate the existence of low-dimensional structure within the space of language representations.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.05426v4  [cs.CL]  12 Jan 2022\nWe then used this structure to explore the relationships between – and gain deeper insight about –\nfrequently used language representations. How do the intermediate layers of prominent language\nmodels relate to one another? How do the abstractions used by these layers evolve from low-level\nword embeddings of a context to a representation of the predicted next word for that context? Do\ndifferent language models follow similar representation patterns? What are the differences between\nhow unidirectional and bidirectional language models represent information? These are examples\nof the types of questions we can explore utilizing our representation embedding space. Second,\nwe showed that this low-dimensional structure is reﬂected in brain responses predicted by these\nrepresentation embeddings. In particular, we show that mapping the principal dimension of the\nrepresentation embeddings onto the brain recovers, broadly, known language processing hierarchies.\nWe also show that the representation embeddings can be used to predict which representations map\nwell to each area in the brain.\n2\nRelated Work\nOur work closely follows the methods developed by Zamir et al. [44]. Those authors generated a\ntaxonomy of vision tasks and analyzed their relationships in that space. This was achieved by applying\ntransfer learning across vision tasks, and showed that this can reduce data labeling requirements.\nWe used some of their methods, such as transfer modeling and normalization using an analytic\nhierarchy process (AHP) [28], for our analysis on language tasks. Following the same lines but for\nNLP, the work from Vu et al. [37] explores transferability across NLP tasks. Those authors focus on\nthe application of transfer learning to show that the amount of labeled data, source and target tasks,\nand domain similarity are all relevant factors for the quality of transferability. Kornblith et al. [20]\ninvestigated and compared techniques for measuring the similarity of neural network representations,\nsuch as canonical correlation analysis (CCA) and centered kernel alignment (CKA).\nOur analysis of the relationship of between the language representation space and the brain leans\nheavily on the concept of “encoding models”. Recently, encoding models have been widely applied\nas predictive models of brain responses to natural language [17, 40]. Jain and Huth [18] applied\nencoding models to representations of an LSTM language model that incorporates context into neural\nrepresentations. These representations are explored for different timescales of language processing\nin Jain et al. [19]. Toneva and Wehbe [33] explored the encoding model performance of different\nlanguage model layers to improve neural network performance. Caucheteux and King [5] observed\ndifferences in the convergence of hidden state representations from ANNs trained on various visual\nand language processing tasks to brain-like representations. Schrimpf et al. [30] systematically\nexamined the performance of encoding models across a variety of representations and neural response\ndatasets and observed consistent high performance of Transformer-based model hidden states at\npredicting neural responses to language. Wang et al. [39] constructed encoding models on a set\nof 21 vision tasks from Zamir et al. [44] with the objective of localizing tasks to speciﬁc brain\nregions. Those authors built a task graph of brain regions showing similar representation for highly\ntransferable tasks.\n3\nMethods Overview\nOur method aims to use the transfer properties between different language representations to generate\nquantitative descriptions of their relationships. These descriptions can then be used as an embedding\nspace without labelling any individual representation with predeﬁned notions of what that repre-\nsentation is or how it relates to other representations. This is analogous to how word embeddings\nuse co-occurrence statistics to generate quantitative descriptions of each word without labelling any\nindividual word with our predeﬁned notions of what that word means. By using transfer properties to\ngenerate the embedding for each representation, we implicitly assume that language representations\nwith similar transfer properties are themselves similar.\nWe deﬁne a representation t as a function over a language input that extracts a vector of meaningful\nfeatures for each word. Some representations can be computed independently for each word, such as\nword embeddings [1, 10, 25], while others depend on both a word and its context, such as part of\nspeech labels [1], or the hidden state in some intermediate layer of a language model [5, 6, 27, 30].\nWe deﬁne a stimulus s as a string of words drawn from some natural language source, and S the set\nof all stimuli from that source. To generate a representation embedding over a set of representations\nT that are derivable from the same stimuli S we will construct time-dependent mappings between\n2\nrepresentations. For a representation t ∈T , and for all times j in our stimulus s ∈S, the vector t(sj)\nis deﬁned. Next, for each pair of representations t1, t2 ∈T we will attempt to map t1(sj) to t2(sj).\nThis provides a transfer mapping ft1→t2(·) that can be learned for each pair of representations,\nft1→t2(t1(sj)) ≈t2(sj)\n∀j.\nHowever, learning these transfer mappings directly would make comparison across representations\ndifﬁcult, as many of the representations have different dimensionalities. Instead, we adapted the\nencoder-decoder framework proposed in Zamir et al. [44], which ensures that every transfer mapping\nhas the same input dimensionality. This method proceeds in three parts (Figure 1):\nFirst, we train a linear encoder for each representation that compresses the information in the input\ndown into a latent space. The latent space should only contain information necessary to predict the\ngiven representation’s features from our input. This is accomplished by training an encoder-decoder\nfrom the input to the given representation via a latent space, then discarding the decoder.\nSecond, we train a new decoder for each pair of representations that uses the previously generated\nlatent space for one representation to predict the other representation.\nThird, we evaluate the performance of each decoder targeting a given representation relative to every\nother decoder targeting that same representation. This provides a measure of how much of the\ninformation in one representation is present in every other representation. Just as words that have\nsimilar co-occurrence statistics—when measured against a large enough group of other words—are\nthought to have similar meanings, we argue that pairs of representations that have similar transfer\nproperties—when measured against a large enough group of other representations—have similar\ninformation content.\ne.g. GPT-2 S\nlayer 7\ne.g. PoS\nGloVe\nS\nU(S)\nt1(S)\nt2(S)\nDt1!t2\nDt1!t1\nDt2!t1\nDt2!t2\nEt1\nEt2\nLt2\nLt1\nFigure 1: The encoder-decoder strategy used in our method, adapted from Zamir et al. [44]. S is the\nnatural language stimuli. We chose to represent stimuli in the universal input feature space U(S) as\nGloVe word embeddings. Encoders Eti were trained using a bottlenecked linear encoder-decoder\nnetwork, which outputs to ti(S) (blue arrows). The decoding half of this network was then discarded,\nand the encoding half used to generate a latent space Lti for each representation ti. Then, a decoder\nDti→tj is trained from each latent space i to each representation j (orange arrows). The performance\nof decoders that map to the same ﬁnal representation are then compared to one another.\n3.1\nGenerating Representation Encoders\nWe deﬁne1 U(S) as the universal input feature space for our stimuli S. In practice, this space should\nrepresent the input with high ﬁdelity, so could be selected as a one-hot encoding of each input token,\nor word embeddings for each input token. We use GloVe word embeddings for U(S). Now let T be\na set of representations over S. For each representation t ∈T , we generate an encoder Et(·) such\nthat the encoder extracts only information in U(S) that is needed to predict t(S). We do this by using\na bottlenecked linear neural network that maps every u ∈U(S) to an intermediate low-dimensional\nlatent space Lt = Et(U(S)) and then maps it to the given representation space,\nt(s) ≈f(Et(u))\n∀s ∈S ∧u = U(s),\nwhere f(·) is mapping from Lt to t(S). We used a small latent space of 20 dimensions to encourage\nthe encoder to extract only the information in U(S) that is relevant to compute t(S). Experimentation\nshowed that variations in the latent space size do not meaningfully change encoder performance.\n1We assume that elements j in a string sj are properly treated, such that we can exclude the positional indexes\nfor all s. Thus, we deﬁne U(S) = {U(s)|s ∈S} as the result of applying a function U(·) to all elements s ∈S.\n3\nOnce we have learned these mappings, we assign Et to be our representation encoder for each\nrepresentation t ∈T . Regardless of the dimensionality of the representation, each latent space has a\nﬁxed dimensionality, which enables a fair comparison between representations.\n3.2\nGenerating Representation Decoders\nThe encoders for each representation generate a latent space Lt that extracts the information in\nU(S) relevant to computing t(S), while compressing away irrelevant information. For every pair\nof representations (t1, t2) ∈T , we next generate a decoder Dt1→t2 such that Dt1→t2(Lt1) =\nDt1→t2(Et1(U(S))) approximates t2(S). This yields a total of n2 decoders, where n = |T | is the\ntotal number of representations. All networks were trained with batches of size 1024 and standard\nstochastic gradient descent with a learning rate of 10−4 for the initial encoders and 2 × 10−5 for the\ndecoders. Hyperparameters were chosen via coordinate descent.\n3.3\nGenerating the Representation Embedding Matrix\nFinally, we compare the performance of the n decoders for each representation in order to generate\nthe representation embedding space. To ensure that comparisons are made on equal footing, we\nonly compare decoders with the same output representation (e.g. Dt1→t2 is compared with Dt3→t2,\nDt4→t2, etc.). This is critical, because evaluating representations with varying dimensionalities would\nunfairly bias the results. This and related challenges were explored in detail in Zamir et al. [44].\nWe used the decoders to generate a pairwise tournament matrix Wt for each representation t by\n“ﬁghting” all pairs of decoders that output to representation t using a held-out test set Stest of\nsentences. Element (i, j) in Wt contains the ratio of samples in the test set for which Dti→t has\nlower MSE than Dtj→t, i.e., Wt(i,j) =\nEs∈Stest[Dti→t(s)<Dtj →t(s)]\nEs∈Stest[Dti→t(s)>Dtj →t(s)]. For example, if the decoder\nDA→C has lower mean squared error than decoder DB→C for 75% of the data in Stest, we assign\nthe ratio of 0.75/0.25 = 3 to entry (A, B) in the tournament matrix WC for representation C.\nWe then use this set of pairwise comparisons to approximate a total order over the quality of all\nencoders for each decoded representation. This was done using the Analytic Hierarchy Process\n(AHP) [28], a technique commonly used in operations research to convert many pairwise comparisons\ninto an estimated total order. AHP establishes that the elements of the principal eigenvector of Wt\nconstitutes a good candidate for a total order over the encoders for t. This eigenvector is proportional\nto the time that an inﬁnite length random walk on the weighted bidirected graph induced by Wt\nwill spend at any given representation. Thus, if the encoder for one representation is better than the\nothers, the weight on that representation will be higher as compared to the weights of other encoders.\nThis eigenvector is then normalized to sum to 1. This procedure yields a length n vector for each\nof the n representations. We set a value of 0.1 into each vector at the position corresponding to the\ntarget representation, and then stack the resulting length n vectors together into the representation\nembedding matrix, R.\n4\nResults\n4.1\nLanguage Representation Embeddings\nWe applied our method to a set of 100 language representations taken from precomputed word\nembeddings or pretrained neural networks for NLP tasks operating on English language inputs.2\nWe extracted representations from numerous different tasks, which included three word embedding\nspaces (GloVe, BERT-E, FLAIR) [25, 10, 1], three unidirectional language models (GPT-2 Small,\nGPT-2 Medium, Transformer-XL) [27, 6, 41], two masked bidirectional language models (BERT,\nALBERT) [10, 21], four common interpretable language tagging tasks (named entity recognition, part-\nof-speech identiﬁcation, sentence chunking, frame semantic parsing) [1], and two machine translation\nmodels (English →Mandarin, English →German) [32]. We also included the GloVe embedding for\nthe next word in the sequence, which constitutes a low-dimensional representation of the ideal output\nof a language model. For multilayered networks (language and translation models), we extracted all\nintermediate layers as separate representations. Full descriptions of all representations, as well as\ndetails on how each feature space is extracted, can be found in Appendix A.\n2Data and code for this paper are available at https://github.com/HuthLab/rep_structure.\n4\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M\nTrans-XL\nBERT\nALBERT\nEng\nZh\nEng\nDe\nEng\nDe\nEng\nZh\nALBERT\nBERT\nTrans-XL\nGPT-2 M\nGPT-2 S\nNWE\nFRAME\nNER\nCHUNK\nPOS\nFLAIR\nBERT-E\nGloVe\nEncoded Feature Space\nDecoded Feature Space\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nGloVe\nBERT-E\nFLAIR\nTransformer-XL\nALBERT\nBERT\nGPT-2 Small\nGPT-2 Medium\nPOS\nCHUNK\nNER\nFRAME\nGloVe Next Word\nEnglish\nGerman\nEnglish\nMandarin\n0\n4\n8\n12\n16\nNumber of Factors\n10\n20\n% Var. Explained\nFigure 2: Language Representation Embeddings with Low Dimensionality: (Left): The representation\nembedding matrix R shows how well a given linguistic feature space (encoder, columns) transfers\nto another feature space (decoder, rows). For better visualization, rows and columns corresponding\nto different layers from the same network have been scaled down in this plot. A full-scale matrix is\nin the supplementary material. (Right): Applying multi-dimensional scaling to the representation\nembedding matrix reveals low-dimensional structure in the linguistic feature spaces. It is dominated\nby a left-to-right progression from the input word embedding, to syntactic and semantic tagging tasks\nnear the middle layers of language models, to the next word embedding. Multidimensional scaling\nwas weighted such that each full model had equal weight, ensuring that language models were not\nmore inﬂuential on account of having more layers. The dominant main diagonal was set to 0.1 to\npreserve the effects of off-diagonal values. The scree plot in the lower left shows that these ﬁrst two\ndimensions explain substantially more variance (22% and 10%) than other dimensions, demonstrating\nthat the structure in this space is low-dimensional.\nApplying our method to these representations, we generated a representation embedding space\n(Figure 2). Generating this space took roughly one week of compute time on a cluster of 53 GPUs (a\ncombination of Titan X, Titan V, and Quadro RTXs) and 64 CPU servers of different characteristics\n(Intel3 Broadwell, Skylake, and Cascade Lake). The representations were trained on a text corpus of\nstories from The Moth Radio Hour [36] which totalled approximately 54,000 words. The held-out\ntest set for the tournament matrix included a story of 1,839 words. Each row shows the relative\nperformance of each of the 99 other representations in decoding a given representation. Each column\nshows how well a given encoded representation can decode to the other representations. Note that\nthere is a distinct asymmetry: if a given representation (e.g. part-of-speech tagging) is never the\nbest encoder for any other representation, then its column will contain very low values. However,\nother representations may decode to this one quite well, so the corresponding part-of-speech row can\ncontain higher values.\n4.1.1\nMultidimensional Scaling Analysis\nTo better visualize the relationships between the representations, we performed multidimensional\nscaling (MDS) [34] on the rows of the representation embedding matrix. We then plotted the\nrepresentations according to their locations on the ﬁrst two MDS dimensions (Figure 2, right). This\nshowed that the layers of each of the unidirectional language models form similar trajectories through\nthe space, beginning near the word embeddings (green) and ending relatively close to the next-word\nembedding (black). This trajectory is monotonic on the ﬁrst MDS dimension, but values on the\nsecond MDS dimension rise and then fall, with the highest values assigned to the middle layers\n(e.g. layer 5 of 12 in GPT-2 small). Language tagging tasks that require semantic information,\n3Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other\nnames and brands may be claimed as the property of others.\n5\nsuch as framing and named entity recognition (blue), are close in space to the middle layers of the\nunidirectional language models. Language tagging tasks that required syntactic information, such as\npart-of-speech identiﬁcation and sentence chunking (red), are similarly clustered.\nWe also observe several other interesting relationships. Like language models, the machine translation\nrepresentations also begin near the word embeddings. However, translation to Mandarin (Eng→Zh)\nappears to align more closely to the unidirectional language models than translation from English\nto German. This may be because English and Mandarin are less closely related to one another, and\ncould thus require deeper syntactic and semantic processing for successful translation.\nWe also observe that the bidirectional language model representations begin farther away from the\ninput word embedding, closer to the middle of the unidirectional language models. However, they\nalso end up at a similar point, closer to the next-word embedding. This may be a result of the\nprocedure to extract a feature space from the intermediate layers of the bidirectional language models,\nwhich differed meaningfully from extracting the intermediate layers of the unidirectional language\nmodels. Since these networks are trained on a masked language modeling task, we extracted the\nhidden representation for the token immediately preceding the mask token.\nTo study the dimensionality of the space of language representations, we examined the variance\nexplained by low-dimensional approximations of the embedding matrix. The scree plot in Figure 2\nshows the result of applying exploratory factor analysis (EFA) [43] to the representation embedding\nmatrix. We see that the ﬁrst few dimensions explain much more variance in the matrix than the other\ndimensions. The ﬁrst dimension alone constitutes 22% of the total variance. This demonstrates that\nthe representation embedding matrix—and thus, we infer, the space of language representations—has\nlow-dimensional structure.\n4.2\nRepresentation Embeddings Predict fMRI Encoding Model Performance\nNext, we hypothesized that these language representation embeddings are a useful representation\nof how natural language is processed in the human brain. Since it is the only system truly capable\nof producing and understanding complex language, the human brain is an excellent litmus test to\ndetermine whether this low-dimensional embedding captures the underlying structure of linguistic\ninformation. To test this hypothesis, we used the representation embeddings generated above to\npredict how well each representation is able to predict fMRI data collected from the human brain as\nthe subject is listening to natural language stories.\n4.2.1\nfMRI Data\nWe used functional magnetic resonance imaging (fMRI) data collected from 5 human subjects as\nthey listened to English language podcast stories over Sensimetrics S14 headphones. Subjects were\nnot asked to make any responses, but simply to listen attentively to the stories. For encoding model\ntraining, each subject listened to at approximately 5 hours of unique stories across 5 scanning sessions,\nyielding a total of 9,189 datapoints for each voxel across the whole brain. For model testing, the\nsubjects listened to the same test story once in each session (i.e. 5 times). These responses were\nthen averaged across repetitions. Training and test stimuli are listed in Appendix B.1. Functional\nsignal-to-noise ratios in each voxel were computed using the mean-explainable variance method from\nNishimoto et al. [24] on the repeated test data. Only voxels within 8 mm of the mid-cortical surface\nwere analyzed, yielding roughly 90,000 voxels per subject.\nMRI data were collected on a 3T Siemens Skyra scanner at the University of Texas at Austin\nBiomedical Imaging Center using a 64-channel Siemens volume coil. Functional scans were collected\nusing a gradient echo EPI sequence with repetition time (TR) = 2.00 s, echo time (TE) = 30.8 ms,\nﬂip angle = 71°, multi-band factor (simultaneous multi-slice) = 2, voxel size = 2.6mm x 2.6mm x\n2.6mm (slice thickness = 2.6mm), matrix size = 84x84, and ﬁeld of view = 220 mm. Anatomical data\nwere collected using a T1-weighted multi-echo MP-RAGE sequence with voxel size = 1mm x 1mm x\n1mm following the Freesurfer [15] morphometry protocol.\nExperiments were approved by the University of Texas at Austin IRB. All subjects gave written\ninformed consent. Subjects were compensated for their time at a rate of $25 per hour, or $262 for the\nentire experiment. Compensation for the 5 subjects totaled $1260.\n6\n1st MDS Dimension\nLeft Medial View\nRight Medial View\nLeft Lateral View\nRight Lateral View\nGroup Average per Anatomical ROI \nSingle Subject \n_ \n+\nAC\nAC\nBA\nBA\nsPMv\nsPMv\nAG\nAG\nPrCu\nPrCu\nFigure 3: Embedding Brain Voxels in the ﬁrst MDS dimension: Projection of the encoding perfor-\nmance vectors for each voxel in one subject (lower center ﬂatmap and all 3D views) and averaged\nover all subjects within anatomical regions (upper center ﬂatmap) over the 100 representations\nonto the ﬁrst MDS dimension of the representation embeddings, which explains about 20% of the\nvariance in the representation embeddings. Voxels with high values in this embedding (red) are better\nexplained by representations that are more positive on the main MDS dimension (e.g. later language\nmodel layers), and voxels with low values (blue) are better explained by representations that are more\nnegative (e.g. word embeddings). This dimension is notable as it is the main dimension along which\nlanguage representations evolve from “earlier” representations such as word embeddings, to “later”\nrepresentations such as intermediate layers in deep language models. Anatomical ROIs were deﬁned\nautomatically in each subject using Freesurfer with the Destrieux 2009 atlas [9]. Similar maps for the\nother subjects and a plot showing the numerical projection of regions in the MDS space are shown in\nAppendix D.1.\n4.2.2\nfMRI Encoding Models\nEncoding models predict a measured brain response B, e.g. blood-oxygen-level dependent (BOLD)\nresponses recorded with fMRI, based on the stimulus observed by the subject. Due to limitations on\ndataset size, encoding models are typically structured as linearized regression, where each regression\npredictor is some feature of the stimulus. Encoding models can provide insight into where and how\ninformation is represented in the brain. For instance, an encoding model that has a feature for whether\na given auditory stimulus is a person’s name can be used to determine which regions of the brain are\nactivated by hearing names.\nHere we constructed voxelwise encoding models using ridge regression for each of the 100 language\nrepresentations t analyzed above. Let g(ti) indicate a linearized ridge regression model that uses a\ntemporally transformed version of the representation ti as predictors. The temporal transformation\naccounts for the lag in the hemodynamic response function [23, 17]. We use time delays of 2, 4, 6,\nand 8 seconds of the representation to generate this temporal transformation. For each subject x,\nvoxel v, and representation ti, we ﬁt a separate encoding model to predict the BOLD response ˆB,\ni.e. ˆB(x,v,ti) = g(x,v,ti)(ti). An optimal ridge parameter α was estimated for each g(x,v,ti) by using\n50-fold Monte Carlo cross-validation, with a held-out validation set of 20% of the datapoints from\nthe training dataset. We then measured encoding model performance ρ by computing the correlation\nbetween the true and predicted BOLD responses for a separate test dataset consisting of one story.\nThat is, ρ(x,v,ti) measures the capacity of representation ti to explain the responses of voxel v in\nsubject x.\n7\n4.2.3\nMapping the First Dimension of the Representation Embedding Space to the Brain\nIf a brain area was specialized for processing a speciﬁc type of information, then we would expect\nrepresentations that capture that information to be good predictors of that brain area. Each voxel\nin the brain can thus be thought of as another language representation, and we can infer where that\nrepresentation would lie in the MDS space by projecting it onto those dimensions.\nThe main MDS dimension (left to right in Figure 2) is especially interesting as it seems to capture\nan intuitive notion of a language representation hierarchy. Representations with low values along\nthe main dimension include word embeddings as well as the earliest layers of most of the language\nmodels and machine translation models. Representations with high values along the main dimension\ninclude the deeper layers of these models, as well as the majority of the interpretable syntactic\nand semantic representations. We tested whether this MDS dimension could capture patterns of\nhierarchical processing observed in cortex. First we deﬁned p(x,v) = zscore([ρ(x,v,t1) . . . ρ(x,v,tn)])\nas the 100-dimensional vector that denotes the encoding model performances for each subject x\nand voxel v, z-scored over all representations T . We then projected each p(x,v) onto the ﬁrst MDS\ndimension by taking their dot product. This gave us a value that quantiﬁes which part of the ﬁrst\nMDS dimension best predicts the activity of a given voxel.\nFigure 3 shows shows the projection of each voxel for one subject (lower center) and averaged across\nsubjects in each anatomical ROI (upper center) onto the ﬁrst MDS dimension. Blue voxels and\nregions are better predicted by representations that are low on the ﬁrst MDS dimension, whereas red\nvoxels and regions are better predicted by representations that are high on the ﬁrst MDS dimension.\nWhile the literature has not settled on a single hierarchical view of language processing in human\ncortex, one point of general agreement among existing theories is that there is a set of “lower”\nlanguage areas, including Wernicke’s area/auditory cortex (AC), Broca’s area (BA), and the premotor\nspeech area sPMv [13, 16]. Across our ﬁve subjects, we see negative projections on the ﬁrst PC\n(corresponding to earlier LM layers) in AC, but both positive and negative projections in BA and\nsPMv. This matches other results using narrative stories and encoding models [7] where it was found\nthat, of these three core language areas, AC is the best explained by lower-level features (phonemes\nand sound spectrum).\nOutside of the core language areas, the literature is more divided on representational hierarchy. It is\nbroadly agreed upon that these other areas, including much of the temporal, parietal, and prefrontal\ncortex, constitute the “semantic system” [4] in which language meaning is derived and represented.\nOur data show that most of these regions have positive projections on the ﬁrst PC, corresponding\nto later LM layers. More ﬁne-grained analyses have suggested that some areas, such as the angular\ngyrus (AG) and precuneus (PrCu), contain the highest-level representations [19, 22]. This matches\nour group-level results, which show that the AG and PrCu have the most positive projections on the\nﬁrst PC of any brain areas.\n4.2.4\nPredicting Encoding Model Performance with Representation Embeddings\nIf the representation embeddings reﬂect how the brain represents linguistic information, then it should\nbe possible to match each representation embedding rk to its corresponding performance vector\nρ(x,•,tk), which describes how well that representation can predict responses in each cortical voxel.\nWe test this via a leave-two-out experiment, where we train a learner using 98 (rk, ρ(x,•,tk)) pairs\nand then use this learner to predict which of the remaining two representation embeddings match\nthe remaining two performance vectors. If the correlation between the predicted performance vector\nfor representation A matches the ground truth performance vector for representation A better than\nthe ground truth performance vector for representation B (and similarly B matches B better than\nit matches A), then we have succeeded in correctly discriminating a performance vector from its\ncorresponding representation embedding.\nFor each subject x and pair of language representations (ti, tj), we trained a linear regression ˆh(x,i,j)\nto map a representation embedding rk to the corresponding encoding model performance built from\ntk over all voxels.\nˆh(x,ti,tj)(rk) ≈corr(g(x,•,k)(tk), B(x,•)) = ρ(x,•,tk),\nrk is deﬁned as the 196-element vector that concatenates the row and column that correspond to the\nrepresentation ti, leaving out the four elements corresponding to the held out pair: Rii, Rij, Rji, and\n8\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M\nTrans-XL\nBERT\nALBERT\nEng\nZh\nEng\nDe\nEng\nDe\nEng\nZh\nALBERT\nBERT\nTrans-XL\nGPT-2 M\nGPT-2 S\nNWE\nFRAME\nNER\nCHUNK\nPOS\nFLAIR\nBERT-E\nGloVe\nDiscriminability Score\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n90\n92\n94\n96\n98\n100\nPercent Correct Pairwise Matches\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M\nTrans-XL\nBERT\nALBERT Eng\nZhEng\nDe\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nMean Voxelwise Correlation\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M\nTrans-XL\nBERT\nALBERT Eng\nZhEng\nDe\nRepresentation\nFigure 4: Representation Embeddings Reﬂect Brain Responses: (Left): Discriminability score matrix\nM, the average across each subject matrix Mx, which is computed as described in Section 4.2.4.\nFor each representation, we ﬁt and tested encoding models that predict the fMRI response in each\ncortical voxel, yielding a pattern of prediction performance across the brain. For each pair, we then\ntested whether the brain patterns could be correctly matched to the representations on the basis\nof the representation embeddings shown in Figure 2. Highly discriminable pairs appear red, non-\ndiscriminable pairs white, and pairs that are less discriminable than expected by chance appear blue.\nMost pairs of representations yield brain patterns that are easy to distinguish using the embeddings,\nsuggesting that these embeddings reﬂect the structure of representations in the human brain. However,\nsome pairs are similar enough in both embedding and brain that discrimination between them falls\nto chance level, such as the word embeddings (green labels) and nearby layers of language models.\n(Upper Right): The percentage of pairwise matches for each representation where the match is\ncorrect more often than not (on 3 or more subjects). Almost all representations can be correctly\nmatched with their corresponding performance vector p, with the interpretable representations being\nthe most difﬁcult to distinguish. (Lower Right): Mean voxelwise correlation for encoding models\nbuilt using each representation of the natural language story stimuli. As seen in other literature\n[30], the intermediate layers of Transformer-based language models work best as encoding model\nrepresentations.\nRjj. The model ˆh(x,ti,tj) was then used to predict the held out encoding model performance across\nall voxels, ρ(x, •, ti) and ρ(x, •, tj). For each subject, we then computed discriminability scores for\nall pairs of representations to generate a matrix Mx. This quantiﬁes how much better the predicted\nencoding model performance for ti matched with the true encoding model performance for ti than\nthe true encoding model performance for tj (and vice versa):\nMx(i,j) =(corr(ˆh(x,ti,tj)(ri), ρ(x,•,ti)) + (corr(ˆh(x,ti,tj)(rj), ρ(x,•,tj))))\n−(corr(ˆh(x,ti,tj)(ri), ρ(x,•,tj)) + (corr(ˆh(x,ti,tj)(rj), ρ(x,•,ti))))\nFigure 4 shows the discriminability scores for all representation pairs averaged across subjects, as\nwell as the average encoding model performance for each individual representation. The vast majority\nof pairs have high discriminability scores (red), indicating that the representation embeddings broadly\ncapture the structure of the brain’s language representations.\nHowever, a few pairs have negative scores (blue), indicating below-chance discrimination perfor-\nmance. This could happen by chance, or it could happen if the task afﬁnities are very different from\nthe brain representations. We believe the negative scores for GloVe NWE are caused by the latter.\nThis representation is highly dissimilar from the others, hence its far-removed location in the MDS\nspace. However, the temporal imprecision of the BOLD responses recorded by fMRI (on the order of\n1 second, or 2-5 words) renders the GloVe NWE and normal GloVe embeddings nearly identical for\npredicting brain data. This disconnect is likely the cause of the negative scores.\n9\nWe also computed how often the discriminability score was higher than 0 in the majority of subjects\n(at least 3 out of 5). We see (Figure 4, upper right) that encoding model performances are well\npredicted by the representation embeddings, with representation embeddings getting matched to their\ncorresponding encoding model performances over 90% of the time for all representations, and 100%\nof the time for the majority of representations.\n5\nFuture Work, Limitations, and Conclusions\nBy measuring transferability between different language representations, we were able to visualize\nthe relationships between 100 representations extracted from common NLP tasks. This uncovered\na low-dimensional structure that illustrates how different tasks are related to one another, and how\nrepresentations evolve over depth in different language models. Finally, we show evidence that these\nrepresentation embeddings capture the structure of language representations in the human brain.\nThere are further possible iterations of this method that may shed additional light on the structure\nof language representations in the brain. Since this method relies on using the transfer properties\nbetween representations to generate embeddings, it is necessary that the chosen set of representations\nspans the rich space of language representations. If some part of the space of language representations\nhas not been sampled, then the representation embeddings may fail to capture some aspects of\nthe structure of that space. We tried to ameliorate this concern by sampling a large number of\nrepresentations (100). However, it is likely that these chosen representations still do not capture the\nfull diversity of the space. In future work, an even larger number of representations could be used to\nbetter map this space.\nIn a similar vein, we selected GloVe embeddings as the “universal” input representation for our\nencoder-decoder setup. GloVe embeddings are likely not “universal” in the same way that a one-hot\nencoding of the story might be, but practical concerns made this approximation necessary. Use of\nother universal spaces could also be enlightening, such as directly using fMRI data to map to a latent\nrepresentation space—i.e. replace U(S) in Figure 1 with fMRI BOLD responses B recorded from\nthe same natural language experiment. Of course, the possibilities are not restricted to language\nrepresentations, as previous work had focused on vision instead [39, 44].\nWe also used linear as opposed to nonlinear networks for our encoder-decoders. This limits the\nexpressiveness of the transfer networks, and thus may under-estimate the relatedness between tasks.\nHowever, using only linear networks also increases interpretability by ensuring that only simply-\nrelated representations can transfer to one another, as also noted by others [2, 26]. Future work may\nexplore the use of regularized nonlinear models for the encoder-decoder networks.\nThis method does require extracting representations from a neural network trained to accomplish\nsome task. For many tasks used in linguistics, neuroscience, and cognitive psychology, there is\nno existing pre-trained network. However, training artiﬁcial neural networks on a task that is used\nto measure cognition in humans and animals is an emerging method that can shed light on how\nthese tasks are accomplished [11, 35, 42], and provide standardized benchmarks for comparing\nbetween different computational cognitive models. Furthermore, we can use our proposed method\nto understand what representations are needed to accomplish different tasks, and to visualize the\nrelationships between those representations. For example, the debate on whether lexico-semantic\nand syntactic representations are actually separable [14] centers on evidence from different tasks\nthat claim to measure syntactic representations versus semantic ones. The claims about the tasks\nthemselves are difﬁcult to test, but methods like those presented in this work can directly quantify\nhow much information is shared between representations needed to accomplish those tasks, either\nin artiﬁcial neural networks or in neuroimaging data. Our proposed method may even provide new\nevidence in other domains where overlap has been observed between neural representations elicited\nby ostensibly different cognitive tasks [3, 38].\nIn sum, we believe that our work can provide a template for investigating relationships between\nlinguistic and cognitive representations in many different domains.\nAcknowledgements\nWe would like to thank Amanda LeBel for collecting the fMRI data and reconstructing cortical\nsurfaces, Lauren Wagner for annotating the experimental stimuli, and the anonymous reviewers for\ntheir insights and suggestions. Funding for this work was provided by the Burroughs-Wellcome Fund\n10\nCareer Award at the Scientiﬁc Interface (CASI), Intel Corporation, the Whitehall Foundation, and the\nAlfred P. Sloan Foundation. The authors declare no conﬂicts of interest.\nReferences\n[1] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland\nVollgraf. Flair: An easy-to-use framework for state-of-the-art nlp. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics\n(Demonstrations), pages 54–59, 2019.\n[2] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer\nprobes. arXiv preprint arXiv:1610.01644, 2016.\n[3] Edward Awh and John Jonides. Overlapping mechanisms of attention and spatial working\nmemory. Trends in Cognitive Sciences, 5(3):119–126, Mar 2001. ISSN 1364-6613. doi:\n10.1016/S1364-6613(00)01593-X.\n[4] Jeffrey R Binder, Rutvik H Desai, William W Graves, and Lisa L Conant. Where is the semantic\nsystem? a critical review and meta-analysis of 120 functional neuroimaging studies. Cerebral\ncortex, 19(12):2767–2796, 2009.\n[5] Charlotte Caucheteux and Jean-Rémi King. Language processing in brains and deep neural\nnetworks: computational convergence and its limits. bioRxiv, 2020. doi: 10.1101/2020.07.03.\n186288. URL https://www.biorxiv.org/content/early/2020/07/04/2020.07.03.\n186288.\n[6] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,\nand Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext. arXiv preprint arXiv:1901.02860, 2019.\n[7] Wendy A de Heer, Alexander G Huth, Thomas L Grifﬁths, Jack L Gallant, and Frédéric E\nTheunissen. The hierarchical cortical organization of human speech processing. Journal of\nNeuroscience, 37(27):6539–6557, 2017.\n[8] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard\nHarshman. Indexing by latent semantic analysis. Journal of the American society for information\nscience, 41(6):391–407, 1990.\n[9] Christophe Destrieux, Bruce Fischl, Anders Dale, and Eric Halgren. Automatic parcellation\nof human cortical gyri and sulci using standard anatomical nomenclature. Neuroimage, 53(1):\n1–15, 2010.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.\n[11] Daniel B. Ehrlich, Jasmine T. Stone, David Brandfonbrener, Alexander Atanasov, and John D.\nMurray. Psychrnn: An accessible and ﬂexible python package for training recurrent neu-\nral network models on cognitive tasks. eNeuro, 8(1), Jan 2021. ISSN 2373-2822. doi:\n10.1523/ENEURO.0427-20.2020. URL https://www.eneuro.org/content/8/1/ENEURO.\n0427-20.2020.\n[12] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the\ngeometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019.\n[13] Evelina Fedorenko, John Duncan, and Nancy Kanwisher. Language-selective and domain-\ngeneral regions lie side by side within broca’s area. Current Biology, 22(21):2059–2062,\n2012.\n[14] Evelina Fedorenko, Idan Asher Blank, Matthew Siegelman, and Zachary Mineroff. Lack of\nselectivity for syntax relative to word meanings throughout the language network. Cognition,\n203, Oct 2020. ISSN 0010-0277. doi: 10.1016/j.cognition.2020.104348.\n11\n[15] Bruce Fischl. Freesurfer. Neuroimage, 62(2):774–781, 2012.\n[16] Gregory Hickok and David Poeppel. The cortical organization of speech processing. Nature\nreviews neuroscience, 8(5):393–402, 2007.\n[17] Alexander G Huth, Wendy A De Heer, Thomas L Grifﬁths, Frédéric E Theunissen, and Jack L\nGallant. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532\n(7600):453–458, 2016.\n[18] Shailee Jain and Alexander Huth.\nIncorporating context into language encoding models\nfor fmri.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-\nran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/\nf471223d1a1614b58a7dc45c9d01df19-Paper.pdf.\n[19] Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S Turek, and Alexander Huth.\nInterpretable multi-timescale models for predicting fmri responses to continuous natural\nspeech. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Ad-\nvances in Neural Information Processing Systems, volume 33, pages 13738–13749. Cur-\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n9e9a30b74c49d07d8150c8c83b1ccf07-Paper.pdf.\n[20] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural\nnetwork representations revisited. In International Conference on Machine Learning, pages\n3519–3529. PMLR, 2019.\n[21] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv\npreprint arXiv:1909.11942, 2019.\n[22] Yulia Lerner, Christopher J Honey, Lauren J Silbert, and Uri Hasson. Topographic mapping of\na hierarchy of temporal receptive windows using a narrated story. Journal of Neuroscience, 31\n(8):2906–2915, 2011.\n[23] Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant.\nReconstructing visual experiences from brain activity evoked by natural movies. Current\nbiology, 21(19):1641–1646, 2011.\n[24] Shinji Nishimoto, Alexander G Huth, Natalia Y Bilenko, and Jack L Gallant. Eye movement-\ninvariant representations in the human visual system. Journal of vision, 17(1):11–11, 2017.\n[25] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532–1543, 2014.\n[26] Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan\nCotterell. Information-theoretic probing for linguistic structure. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages 4609–4622, 2020.\n[27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\n[28] R.W. Saaty.\nThe analytic hierarchy process—what it is and how it is used.\nMathe-\nmatical Modelling, 9(3):161–176, 1987.\nISSN 0270-0255.\ndoi: https://doi.org/10.1016/\n0270-0255(87)90473-8.\nURL https://www.sciencedirect.com/science/article/\npii/0270025587904738.\n[29] Helmut Schmid. Part-of-speech tagging with neural networks. arXiv preprint cmp-lg/9410018,\n1994.\n[30] Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy Kan-\nwisher, Joshua Tenenbaum, and Evelina Fedorenko. The neural architecture of language: Inte-\ngrative reverse-engineering converges on a model for predictive processing. bioRxiv, 2020. doi:\n10.1101/2020.06.26.174482.\nURL https://www.biorxiv.org/content/early/2020/\n10/09/2020.06.26.174482.\n12\n[31] Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, and Tao Jiang. Utilizing bert intermediate\nlayers for aspect based sentiment analysis and natural language inference. arXiv preprint\narXiv:2002.04815, 2020.\n[32] Jörg Tiedemann and Santhosh Thottingal. OPUS-MT — Building open translation services for\nthe World. In Proceedings of the 22nd Annual Conferenec of the European Association for\nMachine Translation (EAMT), Lisbon, Portugal, 2020.\n[33] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing\n(in machines) with natural language-processing (in the brain). In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.\nneurips.cc/paper/2019/file/749a8e6c231831ef7756db230b4359c8-Paper.pdf.\n[34] Warren S Torgerson. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):\n401–419, 1952.\n[35] Ben Tsuda, Kay M. Tye, Hava T. Siegelmann, and Terrence J. Sejnowski. A modeling framework\nfor adaptive lifelong learning with transfer and savings through gating in the prefrontal cortex.\nProceedings of the National Academy of Sciences, 117(47):29872–29882, Nov 2020. ISSN\n0027-8424, 1091-6490. doi: 10.1073/pnas.2009591117.\n[36] https://themoth.org. The moth radio hour, 2020.\n[37] Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew\nMattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability\nacross NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7882–7926, Online, November 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.emnlp-main.635. URL https://www.\naclweb.org/anthology/2020.emnlp-main.635.\n[38] Vincent Walsh. A theory of magnitude: common cortical metrics of time, space and quantity.\nTrends in Cognitive Sciences, 7(11):483–488, Nov 2003. ISSN 1364-6613. doi: 10.1016/j.tics.\n2003.09.002.\n[39] Aria Wang, Michael Tarr, and Leila Wehbe. Neural taskonomy: Inferring the similarity of\ntask-derived representations from brain activity. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.\ncc/paper/2019/file/f490c742cd8318b8ee6dca10af2a163f-Paper.pdf.\n[40] Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell.\nSimultaneously uncovering the patterns of brain regions involved in different story reading\nsubprocesses. PloS one, 9(11):e112575, 2014.\n[41] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s\ntransformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\n[42] Guangyu Robert Yang, Madhura R. Joglekar, H. Francis Song, William T. Newsome, and Xiao-\nJing Wang. Task representations in neural networks trained to perform many cognitive tasks.\nNature Neuroscience, 22(2), Feb 2019. ISSN 1546-1726. doi: 10.1038/s41593-018-0310-2.\n[43] An Gie Yong, Sean Pearce, et al. A beginner’s guide to factor analysis: Focusing on exploratory\nfactor analysis. Tutorials in quantitative methods for psychology, 9(2):79–94, 2013.\n[44] Amir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n13\nA\nRepresentation Descriptions\nGloVE [25] is a 300-dimensional word embedding space. It is an dimensionality-representation\nrepresentation of word-word co-occurrence statistics.\nGloVe embeddings were sourced from\nhttp://nlp.stanford.edu/data/glove.6B.zip\nBERT-E [10] is a 3072-dimensional contextualized word embedding space extracted from BERT.\nWe used the Flair NLP [1] implementation of BERT embeddings.\nFLAIR [1] is a 4096-dimensional contextualized character level word embedding space. We used\nthe Flair NLP implementation of this word embedding space.\nPOS is the 53-dimensional representation stored at the pre-softmaxed output logit layer of the\npre-trained FLAIR [1] part-of-speech LSTM-based sentence tagger.\nCHUNK is the 45-dimensional representation stored at the pre-softmaxed output logit layer of the\npre-trained FLAIR [1] LSTM-based sentence chunker.\nNER is the 76-dimensional representation stored at the pre-softmaxed output logit layer of the\npre-trained FLAIR [1] LSTM-based named entity recognition sentence tagger.\nFRAME is the 5196-dimensional representation stored at the pre-softmaxed output logit layer of the\npre-trained FLAIR [1] LSTM-based semantic framing (verb disambiguation) tagger.\nNWE is the GloVe representation, offset by one word in the future.\nThe FlairNLP repository which we used for the preceding represenations can be found here:\nhttps://github.com/flairNLP/\nGPT-2 Small is a set of 12 representations, each 768-dimensional, built from each hidden state\noutput from the 12 transformers that compose the GPT-2 Small unidirectional language model [27].\nRepresentations were built using a sliding window of 64 words as a context. We used the HuggingFace\n[41] implementation of this network to extract feature for these representations.\nGPT-2 Medium is a set of 24 representations, each 1024-dimensional, built from each hidden state\noutput from the 24 transformers that compose the GPT-2 Medium unidirectional language model\n[27]. Representations were built using a sliding window of 64 words as a context. We used the\nHuggingFace [41] implementation of this network to extract feature for these representations.\nTransformer-XL is a set of 18 representations, each 1024-dimensional, built from each hidden state\noutput from the 18 transformers that compose the Transformer-XL unidirectional language model\n[6]. Representations were built using a sliding window of 64 words as a context. We used the\nHuggingFace [41] implementation of this network to extract feature for these representations.\nBERT is a set of 13 representations, each 768-dimensional, built from the encoded input and each\nhidden state output from the 12 transformers that compose the BERT bidirectional masked language\nmodel [10]. Representations were built using a sliding window of 64 words as a context, with the\nlast token being the designated mask token. We used the HuggingFace [41] implementation of this\nnetwork to extract feature for these representations.\nALBERT is a set of 13 representations, each 768-dimensional, built from the encoded input and\neach hidden state output from the 12 transformers that compose the ALBERT bidirectional masked\nlanguage model [21]. Representations were built using a sliding window of 64 words as a context,\nwith the last token being the designated mask token. We used the HuggingFace [41] implementation\nof this network to extract feature for these representations.\nEng ⇒Zh is the set of 6 encoder hidden state representations, each 512-dimensional, of\nthe HuggingFace/Helsinki-NLP [41, 32] implementation of a Transformer-based machine trans-\nlation model from English to Mandarin Chinese.\nRepresentations were built using a slid-\ning window of 64 words as a context.\nThe pretrained network we used can be found here:\nhttps://huggingface.co/Helsinki-NLP/opus-mt-en-zh.\nEng ⇒De is the set of 6 encoder hidden state representations, each 512-dimensional, of\nthe HuggingFace/Helsinki-NLP [41, 32] implementation of a Transformer-based machine trans-\nlation model from English to German.\nRepresentations were built using a sliding win-\ndow of 64 words as a context.\nThe pretrained network we used can be found here:\nhttps://huggingface.co/Helsinki-NLP/opus-mt-en-de.\n14\nTable 1: Stories used for stimuli for fMRI scanning\nStory\nUse\nWords\nTRs\nAlternate Ithaca Tom\nTrain\n2174\n343\nSaving Souls\nTrain\n1868\n355\nAvatar\nTrain\n1469\n367\nLegacy\nTrain\n1893\n400\nOde to Stepfather\nTrain\n2675\n404\nUnder the Inﬂuence\nTrain\n1641\n304\nHow to Draw a Naked Man\nTrain\n1964\n354\nNaked\nTrain\n3218\n422\nLife Flight\nTrain\n2209\n430\nStage Fright\nTrain\n2067\n293\nTill Death or Homosexuality Do Us Part\nTrain\n2297\n323\nFrom Boyhood to Fatherhood\nTrain\n2755\n348\nSloth\nTrain\n2403\n437\nExorcism\nTrain\n2949\n467\nHave You Met Him Yet\nTrain\n2985\n496\nA Doll’s House\nTrain\n1656\n241\nThe Closet that Ate Everything\nTrain\n1928\n314\nAdventures in Saying Yes\nTrain\n2309\n391\nBuck\nTrain\n1677\n332\nSwimming With Astronauts\nTrain\n2127\n385\nThat Thing on My Arm\nTrain\n2336\n434\nEye Spy\nTrain\n2127\n379\nIt’s a Box\nTrain\n2336\n355\nHangtime\nTrain\n1708\n324\nWhere There’s Smoke\nTest\n1839\n291\nB\nfMRI Data Details\nB.1\nStimulus details.\nTraining stimuli for encoding models consisted of stories from The Moth Radio Hour: “Alternate\nIthaca Tom”, “Saving Souls”, “Avatar”, “Legacy”, “Ode to Stepfather”, “Under the Inﬂuence”, “How\nto Draw a Naked Man”, “Naked”, “Life Flight”, “Stage Fright”, “Till Death or Homosexuality Do Us\nPart”, “From Boyhood to Fatherhood”, “Sloth”, “Exorcism”, “Have You Met Him Yet”, “A Doll’s\nHouse”, “The Closet that Ate Everything”, “Adventures in Saying Yes”, “Buck”, “Swimming With\nAstronauts”, “That Thing on My Arm”, “Eye Spy”, “It’s a Box”, and “Hangtime”.\nThe test stimulus was a single story, also taken from The Moth Radio Hour: “Where There’s Smoke”.\nThe amount of data in terms of words and TRs from each story is given in Table 1.\nB.2\nROI labels\nRegions of interest (ROIs) were labelled in each subject using separate localizer data, as follows:\nThe auditory cortex (AC) was selected as a region with highly repeatably responses to ten presentations\nof a one-minute stimulus consisting of music, speech, and natural environmental sounds.\nVisual category-selective areas were deﬁned using a category localizer where subjects looked at a\nseries of images while ﬁxating on the center of the screen. Place-selective visual areas, including\nthe retrosplenial complex (RSC), occipital place area (OPA, called TOS for trans-occipital sulcus\nin some subjects), and parahippocampal place area (PPA), were deﬁned by contrasting responses to\nplace images (e.g. buildings) with responses to object images (e.g. teapots). Face-selective visual\nareas, including the fusiform face area (FFA), occipital face area (OFA), and inferior frontal sulcus\nface patch (IFSFP) were deﬁned by contrasting responses to face images with responses to object\nimages. Body-selective visual areas, including the extrastriate body area (EBA) and fusiform body\narea (FBA) were deﬁned by contrasting responses to body images with responses to object images.\n15\nMotor-related areas were deﬁned using a ten-minute motor localizer in which subjects were asked\nto wiggle their ﬁngers, wiggle their toes, mouth nonsense syllables (e.g. balabalabala), make\nsaccades, or silently generate speech. Eye movement areas, including the frontal eye ﬁelds (FEF),\nintraparietal sulcus (IPS), fronto-opercular eye movement area (FO), and supplementary eye ﬁelds\n(SEF) were deﬁned by contrasting the eye movement condition with rest. Hand areas, including\nprimary somatomotor hand cortex (M1H, S1H), ventral premotor hand area (PMvh), secondary\nsomatosensory hand area (S2H), and supplementary motor hand area (SMHA) were deﬁned by\ncontrasting the hand movement condition with rest. Foot areas, including primary somatomotor foot\ncortex (M1F, S1F), supplementary motor foot area (SMFA), and secondary somatosensory foot area\n(S2F) were deﬁned by contrasting the foot movement condition with rest. Mouth movement areas,\nincluding primary somatomotor mouth cortex (M1M, S1M) and secondary somatosensory mouth\ncortex (S2M) were deﬁned by contrasting the mouth movement condition with rest. Speech areas,\nincluding Broca’s area (Broca) and the superior ventral premotor speech area (sPMv) were deﬁned\nby contrasting the silent language generation condition with rest.\nIn some subjects, retinotopic areas (V1-V4, LO, V7, V3A, V3B) were deﬁned using standard\nretinotopic mapping protocols.\nIn some subjects, the human analogue of the middle temporal visual motion area (hMT or MT) was\ndeﬁned using a contrast of coherently moving visual dots versus randomly ﬂickering visual dots.\nC\nfMRI Encoding Performance Similarity Matrix\nA similarity matrix showing the correlations between the performance vectors for each pair of\nrepresentations is given below. This can be compared to the dicriminablity matrix in Figure 4 as\na ground truth metric of the the difﬁculty of comparing two encoding models if the representation\nembeddings perfectly captured all of the differences across representations. This suggests that the\nrepresentation embeddings are better at capturing certain kinds of differences between representations,\nsuch as the differences across layers of the same network.\n16\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M Trans-XL\nBERT\nALBERT Eng\nZhEng\nDe\nEng\nDe Eng\nZh\nALBERT\nBERT\nTrans-XL GPT-2 M\nGPT-2 S\nNWE\nFRAME\nNER\nCHUNK\nPOS\nFLAIR\nBERT-E\nGloVe\n0.0\n0.2\n0.4\n0.6\n0.8\n17\nD\nEmbedding Brain Voxels in the MDS Space\nD.1\nOther Subjects\nA 3D visualization of a single subject’s brain voxels embedded into the ﬁrst main MDS dimension\nwas presented in Figure 3. Flatmaps showing the same metric the other four subjects are shown below.\nScales have been adjusted on a per-subject basis to maximize visual contrast.\n18\n19\nD.2\nAnatomical ROI Projection in the MDS Space\nThe numerical projections of the anatomical ROIs from Figure 3 are shown below. ROIs are labeled\nas in the Destrieux 2009 atlas [9].\nE\nExperimental Setup Details\nThe encoder-decoder setup (before replacing decoders) was initially trained using batches of size 1024\nusing stochastic gradient descent using a nonstandard negative correlation coefﬁcient loss function.\nWe found empirically that this setup provided faster convergence and good compression into the\nlatent spaces as it was scale-invariant to the different dimensions in the various representations.\nThe decoders were then trained on a standard mean squared error loss after they were replaced\nand retrained. When decoders were retrained, all decoders were retrained, including decoders to\nthe same task (e.g. Dt1→t1). This was to ensure that pairwise comparisons across decoders was\nperformed with a consistent methodology between all pairs. Hyperparameter values were chosen\nvia coordinate descent over a small set of values: (10, 20, 50) for the latent state dimensionality,\n(10−6, 2 × 10−5, 10−4, 2 × 10−4, 10−4) for the learning rates, for each of the encoder and decoder\nhalves of the training, and (256, 512, 1024) for batch size. Overall results did not vary signiﬁcantly\nwith latent space dimensionality. Training was done using an early stopping criterion of 1000 batches\nor the ﬁrst batch where loss failed to improve, whichever came ﬁrst. Scripts for the experimental\nsetup have been included in the supplementary material.\n20\nF\nDetails on the Generation of the Pairwise Tournament Matrix W and\nRepresentation Embedding Matrix R\n0\n1/3\n0\n0\n0\n3\nvs.\n75%\nbetter\n25%\nbetter\n) Wt2,(t4!t2,t5!t2) = 3\nDt1!t2\nDt3!t2\nDt4!t2\n4=\"tMq13CUgl6C5+r4YautjeQBVdJM=\">ACA\nHicbVBNS8NAEN3Ur1q/oh48eFksgqeSlBY9FvT\ngsYL9gDaEzXbTLt1kw+5EKaEX/4oXD4p49Wd48\n9+4bXPQ1gcDj/dmJkXJIJrcJxvq7C2vrG5Vdw\nu7ezu7R/Yh0dtLVNFWYtKIVU3IJoJHrMWcBCs\nmyhGokCwTjC+nvmdB6Y0l/E9TBLmRWQY85BTAk\nby7ZMbPwO/jvuKD0dAlJKPGPzq1LfLTsWZA68S\nNydlKPp21/9gaRpxGKgmjdc50EvIwo4FSwa\nmfapYQOiZD1jM0JhHTXjZ/YIrPjTLAoVSmYsBz\n9fdERiKtJ1FgOiMCI73szcT/vF4K4ZWX8ThJgc\nV0sShMBQaJZ2ngAVeMgpgYQqji5lZMR0QRCia\nzkgnBX5lbSrFbdece5q5UYtj6OITtEZukAu\nkQNdIuaqIUomqJn9IrerCfrxXq3PhatBSufOUZ\n/YH3+ACbilhQ=</latexit>Dt5!t2\nT\neswe30hoLc0o=\">ACAHicbVBNS8NAE\nN34WetX1IMHL4tF8FSUtFjQ8eK9gPaEPYbDf\nt0k02\n7E6UEnLxr3jxoIhXf4Y3/43bNgdtfTDweG+GmXlBIrgGx\n/m2VlbX1jc2S1vl7Z3dvX374LCtZao\noa1EpOoGRDP\nBY9YCDoJ1E8VIFAjWCcbXU7/zwJTmMr6HScK8iAxjHnJK\nwEi+fXzjZ+C7uK\n/4cAREKfmIwa/lvl1xqs4MeJm4Ba\nmgAk3f/uoPJE0jFgMVROue6yTgZUQBp4Ll5X6qWULomAx\nZz9CYRE\nx72eyBHJ8ZYBDqUzFgGfq74mMRFpPosB0Rg\nRGetGbiv95vRTCKy/jcZICi+l8UZgKDBJP0\n8ADrhgFMTG\nEUMXNrZiOiCIUTGZlE4K7+PIyadeq7kXVuatXGvUij\nhI6QafoHLnoEjXQLWqiFqIoR8/oFb\n1ZT9aL9W59zFtXrG\nLmCP2B9fkDI6WEA=</latexit>Dt1!t2\nDt3!t2\n<latexit sha1_base64=\"SWN\nkx85u9RNuE\nwAbDWdLDOceUE=\">ACAHicbVBNS8NA\nEN34WetX1IMHL4tF8FSUtFjQ8eK9gPaEPYbDf\nt0s0m7\nE6UEnLxr3jxoIhXf4Y3/43bNgdtfTDweG+GmXlBIrg\nGx/m2VlbX1jc2S1vl7Z3dvX374LCt41R\nR1qKxiFU3IJoJ\nLlkLOAjWTRQjUSBYJxhfT/3OA1Oax/IeJgnzIjKUPOS\nUgJF8+/jGz8Cv47\n7iwxEQpeJHDH4t9+2KU3VmwMvELUgF\nFWj69ld/ENM0YhKoIFr3XCcBLyMKOBUsL/dTzRJCx2\nTIeoZKEj\nHtZbMHcnxmlAEOY2VKAp6pvycyEmk9iQLTGRE\nY6UVvKv7n9VIr7yMyQFJul8UZgKDGep\noEHXDEKY\nmIoYqbWzEdEUomMzKJgR38eVl0q5V3Yuqc1evNOpFHC\nV0gk7ROXLRJWqgW9RELURjp7RK3\nqznqwX6936mLeuW\nMXMEfoD6/MHJU2WEw=</latexit>Dt4!t2\nDt5!t2\nDt4!t2\nDt5!t2\nFigure 5: Generation and Use of the Pairwise Tournament Matrix W: The decoders from a given\nlatent space are compared in the generation of the pairwise tournament matrix for task t2 Wt2. The\nproportion of the data for which each decoder outperforms the other is measured and compared.\nDiagonals of this matrix are set to 0. The eigenvector of this matrix with the highest eigenvalue is\nthen assigned to be the row of our representation embedding matrix R corresponding to that matrix’s\nencoded task. Eigenvectors and eigenvalues are computed using the differential quotient-difference\nalgorithm. Complex components of the eigenvalue are ignored.\n21\nG\nVisual Aid Reproductions\nThe matrices from Figure 2 and Figure 4 are very dense. As a visual aid, these matrices have been\nreproduced here at a larger scale.\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M\nTrans-XL\nBERT\nALBERT\nEng\nZh Eng\nDe\nEng\nDe\nEng\nZh\nALBERT\nBERT\nTrans-XL\nGPT-2 M\nGPT-2 S\nNWE\nFRAME\nNER\nCHUNK\nPOS\nFLAIR\nBERT-E\nGloVe\nEncoded Feature Space\nDecoded Feature Space\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n22\nGloVe\nBERT-E\nFLAIR\nPOS\nCHUNK\nNER\nFRAME\nNWE\nGPT-2 S\nGPT-2 M\nTrans-XL\nBERT\nALBERT\nEng\nZh\nEng\nDe\nEng\nDe\nEng\nZh\nALBERT\nBERT\nTrans-XL\nGPT-2 M\nGPT-2 S\nNWE\nFRAME\nNER\nCHUNK\nPOS\nFLAIR\nBERT-E\nGloVe\nDiscriminability Score\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n23\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-06-09",
  "updated": "2022-01-12"
}