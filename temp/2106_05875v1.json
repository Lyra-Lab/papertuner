{
  "id": "http://arxiv.org/abs/2106.05875v1",
  "title": "Interferometric Graph Transform for Community Labeling",
  "authors": [
    "Nathan Grinsztajn",
    "Louis Leconte",
    "Philippe Preux",
    "Edouard Oyallon"
  ],
  "abstract": "We present a new approach for learning unsupervised node representations in\ncommunity graphs. We significantly extend the Interferometric Graph Transform\n(IGT) to community labeling: this non-linear operator iteratively extracts\nfeatures that take advantage of the graph topology through demodulation\noperations. An unsupervised feature extraction step cascades modulus\nnon-linearity with linear operators that aim at building relevant invariants\nfor community labeling. Via a simplified model, we show that the IGT\nconcentrates around the E-IGT: those two representations are related through\nsome ergodicity properties. Experiments on community labeling tasks show that\nthis unsupervised representation achieves performances at the level of the\nstate of the art on the standard and challenging datasets Cora, Citeseer,\nPubmed and WikiCS.",
  "text": "Interferometric Graph Transform for Community\nLabeling\nNathan Grinsztajn∗\nInria, Univ. Lille, CNRS\nLille, France\nnathan.grinsztajn@inria.fr\nLouis Leconte∗\nLIP6, Sorbonne University\nCMAP, Ecole Polytechnique, France\nlouis.leconte@ens-paris-saclay.fr\nPhilippe Preux\nInria, Univ. Lille, CNRS\nLille, France\nEdouard Oyallon\nCNRS, LIP6, Sorbonne University\nParis, France\nAbstract\nWe present a new approach for learning unsupervised node representations in\ncommunity graphs. We signiﬁcantly extend the Interferometric Graph Transform\n(IGT) to community labeling: this non-linear operator iteratively extracts features\nthat take advantage of the graph topology through demodulation operations. An\nunsupervised feature extraction step cascades modulus non-linearity with linear\noperators that aim at building relevant invariants for community labeling. Via a\nsimpliﬁed model, we show that the IGT concentrates around the E-IGT: those\ntwo representations are related through some ergodicity properties. Experiments\non community labeling tasks show that this unsupervised representation achieves\nperformances at the level of the state of the art on the standard and challenging\ndatasets Cora, Citeseer, Pubmed and WikiCS.\n1\nIntroduction\nGraph Convolutional Networks (GCNs) [25] are now the state of the art for solving many supervised\n(using labeled nodes) and semi-supervised (using unlabeled nodes during training) graph tasks, such\nas nodes or community labeling. They consist in a cascade of layers that progressively average node\nrepresentations, while maintaining discriminative properties through supervision. In this work, we are\nmainly interested in the principles that allow such models to outperform other baselines: we propose\na speciﬁc class of GCNs, which is unsupervised, interpretable, with several theoretical guarantees\nwhile obtaining good accuracies on standard datasets.\nOne of the reasons why GCNs lack interpretability is because no training objective is assigned to a\nspeciﬁc layer except the ﬁnal one: end-to-end training makes their analysis difﬁcult [34]. They also\ntend to oversmooth graph representations [47], because applying successively an averaging operator\nleads to smoother representations. Also, the reason of their success is in general unclear [27]. In\nthis work, we propose to introduce a novel architecture which, by design, will address those issues.\nOur model can be interpreted through the lens of Stochastic Block Models (SBMs) [19] which are\nstandard, yet are not originally designed to analyze graph attributes through representation learning.\nFor example, several works [23, 1] prove that a Laplacian matrix concentrates around a low-rank\nexpected Laplacian matrix, via simpliﬁed models like a SBM [10]. In the context of community\ndetection, it is natural to assume that the intra-class, inter-class connectivity and feature distributions\n∗Equal contributions.\nPreprint. Under review.\narXiv:2106.05875v1  [cs.LG]  4 Jun 2021\nof a random graph are ruled by an SBM. To our knowledge, this work is the ﬁrst to make a clear\nconnection with those unsupervised models and the self-supervised deep GCNs which solve datasets\nlike Cora, Citeseer, Pubmed, or WikiCS.\nOur model is driven by ideas from the Graph Signal Processing [18] community and based on the\nInterferometric Graph Transform [35], a class of models mainly inspired by the (Euclidean) Scattering\nTransform [30]. The IGT aims at learning unsupervised (not using node labels at the representation\nlearning stage), self-supervised representations that correspond to a cascade of isometric layer and\nmodulus non-linearity, whose goal is to obtain a form of demodulation [36] that will lead to smoother\nbut discriminative representation, in the particular case of community labeling. Smooth means\nhere, by analogy with Signal Processing [29], that the signal is in the low-frequency domain, which\ncorresponds to a quite lower dimensional space if the spectral decay is fast enough: this is for instance\nthe case with a standard Laplacian [16] or a low-rank SBM adjacency matrix [28]. Here, the degree\nof invariance of a given representation is thus characterized by the smoothness of the signal.\nOur main contribution is to introduce a simpliﬁed framework that allows to analyze node labeling tasks\nbased on a non-linear model, via concentration bounds and which is numerically validated. Our other\ncontributions are as follows. First, we introduce a novel graph representation for community labeling,\nwhich doesn’t involve community labels. It consists in a cascade of linear isometry, band-pass\nﬁltering, pointwise absolute value non-linearity. We refer to it as an Interferometric Graph Transform\n(IGT) (for community labeling), and we show that under standard assumptions on the graph of our\ninterest, a single realization of our representation concentrates around the Expected Interferometric\nGraph Transform (E-IGT), which can be deﬁned at the node level without incorporating any graph\nknowledge. We also introduce a novel notion of localized low-pass ﬁlter, whose invariance can be\nadjusted to a speciﬁc task. Second, we study the behavior of this representation under an SBM model:\nwith our model and thanks to the structure of the IGT, we are able to demonstrate theoretically that\nIGT features accumulate around the corresponding E-IGT. We further show that the architecture\ndesign of IGTs allows to outperform GCNs in a synthetic setting, which is consistent with our\ntheoretical ﬁndings. Finally, we show that this semi-supervised and unsupervised representation is\nnumerically competitive with supervised representations on standard community labeling datasets\nlike Cora, Citeseer, Pubmed and WikiCS.\nOur paper is organized as follows. First, we deﬁne the IGT in Sec. 3.1 and study its basic properties.\nSec. 3.2 deﬁnes the E-IGT and bounds its distance from the IGT. Then, we discuss our model in the\ncontext of a SBM in Sec. 3.3 and we explain our optimization procedure in Sec. 3.4. Finally, Sec. 4 cor-\nresponds to our numerical results. Our source can be found at https://github.com/nathangrinsztajn/igt-\ncommunity-detection and all proofs of our results can be found in the Appendix.\n2\nRelated Work\nWe now discuss a very related line of work, namely the IGT [35], which takes source in several\nconceptual ideas from the Scattering Transform [30]. Both consist in a cascade of unitary transform,\nabsolute value non-linearity and linear averaging, except that the Euclidean structure is neatly\nexploited via Wavelets Transforms for complex classiﬁcation tasks in the case of the standard\nScattering Transform [5, 37, 2, 36], whereas this structure is implicitly used in the case of IGT. In\nparticular, similarly to a Scattering Transform, an IGT aims at projecting the feature representation in\na lower dimensional space (low-frequency space) while being discriminative: the main principle is\nto employ linear operators, which combined with a modulus non-linearity, leads to a demodulation\neffect. In our case however, this linear operator is learned. The IGT for community labeling is rather\ndifferent from standard IGT: ﬁrst, [35] is not amenable to node labeling because it doesn’t preserve\nnode localization, contrary to ours. Second, we do not rely on the Laplacian spectrum explicitely\ncontrary to [12, 35]. Third, the community experiments of [12, 35] are rather the classiﬁcation\nof a diffusion process than a node labeling task. This is also similar to the Expected Scattering\nTransform [31], yet it is applied in a rather different context for reducing data variance, in order\nto shed lights on standard Deep Neural Networks. Our E-IGT and the Expected-Scattering have a\nvery close architecture, however the linear operators are obtained with rather different criteria (e.g.,\nours are obtained from a concave procedure rather than convex) and goals (e.g., preserving energy,\nwhereas we try to reduce it). Note however there is no equivalent of the E-IGT for other context that\ncommunity detection or labeling, which is another major difference with [35]. In addition, our Prop.\n3 is new compared to similar results of [31]. Thus while having similar architectures, those works\nhave quite different outcomes and objectives.\n2\nAnother line of works corresponds to the Graph Scattering Transform [12, 13, 21], which proposes to\nemploy a cascade of Wavelet Transforms that respects the graph structure [18]. Yet, the principles that\nallow good generalization of those representations are unclear and they have only been tested until\nnow on small datasets. Furthermore, this paper extends all those works by proposing an architecture\nand theoretical principles which are speciﬁc to the task of community labeling. A last related line\nof work corresponds to the hybrid Scattering-GCNs [33], which combines a GCN with the inner\nrepresentation of a Scattering Transform on Graphs, yet they employ massive supervision to reﬁne\nthe weights of their architecture, which we do not do.\nThe architecture of an IGT model for community labeling takes also inspiration from Graph Convolu-\ntional Networks (GCNs) [25, 4]. They are a cascade of linear operators and ReLU non-linearities\nwhose each layer is locally averaged along local nodes. Due to this averaging, GCNs exhibit two\nundesirable properties: ﬁrst, the oversmoothing phenomenon [27], which makes learning of high-\nfrequencies features difﬁcult; second, the training of deeper GCNs is harder [20] because much\ninformation has been discarded by those averaging steps. Other types of Graph Neural Networks\nsucceeded in approximating message-passing methods [9], or have worked on the spatial domain\nsuch as Spectral GCNs [6], and Chebynet [11]. In our work, we solely use a well chosen averaging\nfor separating high-frequencies and low-frequencies without using any other extra-structure, which\nmakes our method more generic than those approaches, without using supervision at all.\nWe further note that theoretical works often address the problem of estimating the expected Laplacian\nunder SBM assumptions [23, 1, 26]. However up to our knowledge, none of those works is applied in a\nsemi-supervised context and they aim at discovering communities rather than estimating communities\nfrom a small subset of labels. Moreover, the model remains mostly linear (e.g. based on the\nspectrum of the adjacency matrix). Here, our representation is non-linear and amenable for learning\nwith a supervised classiﬁer. We also note that several theoretical results have allowed to obtain\napproximation or stability guarantees for GCNs [41, 5, 22]: our work follows those lines and analyzes\na speciﬁc type of GCN through the lens of Graph Signal Processing theory [18].\n3\nFramework\nNotations.\nFor a matrix X, we write ∥X∥2 = Tr(XT X) = P\ni,j X2\ni,j its Frobenius-norm and for\nan operator L (acting on X), we might consider the related operator norm ∥L∥≜sup∥X∥≤1 ∥LX∥.\nThe norm of the concatenation {B, C} of two operators B, C is ∥{B, C}∥2 = ∥B∥2 + ∥C∥2 and\nthis deﬁnition can be extended naturally to more than two operators. Note also that we use a different\ncalligraphy between quantities related to the graph (e.g., adjacency matrix A) and operators (e.g.,\naveraging matrix A). We write A ≼B if B −A is a symmetric positive matrix. Here, an ∼bn\nmeans that ∃α > 0, β > 0 : α|an| ≤|bn| ≤β|bn| and an = O(bn) means ∃α > 0 : |an| ≤α|bn|.\n3.1\nDeﬁnition of IGT\nOur initial graph data are node features X ∈Rn×P obtained from a graph with n nodes and\nunormalized adjacency matrix A. We then write Anorm the normalized adjacency matrix with self-\nconnexion, as introduced by [25]. We note that Anorm satisﬁes 0 ≼Anorm ≼I and has positive entries.\nIn Graph Signal Processing [18], those properties allow to interpret Anorm as an averaging operator.\nIt means that applying Anorm to X leads to a linear representation AnormX which is smoother than\nX because Anorm projects the data in a subspace ruled by the topology (or connectivity) of a given\ncommunity [12]. The degree of smoothness can be adjusted to a given task simply by considering:\nAJ ≜AJ\nnorm .\n(1)\nThis step is analogeous to the rescaling of a low-pass ﬁlter in Signal Processing [29], and AJ satisﬁes:\nLemma 1. If 0 ≼Anorm ≼I and Anorm has positive entries, then for any J ∈N, AJ has positive\nentry and satisﬁes also 0 ≼AJ ≼I.\nApplying solely AJ leads to a loss of information that we propose to recover via I −AJ. This allows\nto separate low and high-frequencies of the graph in two channels, as expressed by the next lemma:\nLemma 2. If 0 ≼A ≼I, then ∥AX∥2 + ∥(I −A)X∥2 ≤∥X∥2 with equality iff A2 = A.\nYet, contrary to AJX, (I −AJ)X is not smooth and thus, it might not be amenable for learning\nbecause community structures might not be preserved. Furthermore, a linear classiﬁer will not be\n3\nsensitive to the linear representation {AJX, (I −AJ)X}. Similarly to [35], we propose to apply\nan absolute value |.| point-wise non-linearity to our representations. Section 3.4 will explain how\nto estimate isometries {Wn}, which combined with a modulus, will smooth the signal envelope\nwhile preserving signal energy. We now formally describe our architecture and we consider {Wn} a\ncollection of isometries, that we progressively apply to an input signal representation U0 ≜X via:\nUn+1 ≜|(I −AJ)UnWn| ,\n(2)\nand we introduce the IGT representation of order N ∈N with averaging scale J ∈N deﬁned by:\nSN\nJ X ≜{AJU0, ..., AJUN} .\n(3)\nFig. 1 depicts our architecture. The following explains that SN\nJ is non-expansive, thus stable to noise:\nProposition 1. For N ∈N, SN\nJ X is 1-Lipschitz leading to:\n∥SN\nJ X −SN\nJ Y ∥≤∥X −Y ∥and, ∥SN\nJ X∥≤∥X∥.\n(4)\nU1\norder 0\norder 1\norder 2\nAJ\nI −AJ\n|W0|\nAJ\nI −AJ\n|W1|\nAJ\nU0 , X\nuwqS2OyKblxWNG2hDWUynbRDJ5MwMxFqySe41Q9wJ279GNf+iJO2ghU9cOFwzr3ce0+QcKY0Qh/Wyura+sZmYau4vbO7t186OGypOJWEeiTmsewEWFHOBPU05x2EklxFHDaDsZXud+p1KxWNzpSUL9CA8FCxnB2ki3X\nr/SL5WR7SLHdc8hshFya/VqTmrVXHGMkqMFmj2S5+9QUzSiApNOFaq6BE+1MsNSOcZsVeqmiCyRgPadQgSOq/Ons1AyeGmUAw1iaEhrO1J8TUxwpNYkC0xlhPVK/vVz8y+umOqz7UyaSVFNB5ovClEMdw/xvOGCSEs0\nnhmAimbkVkhGWmGiTztIWzcYPmUnl+3X4P2lVbKdmo5tquXG5yKcAjsEJOAMOuANcA2awAMEDMEjeALPVma9WK/W27x1xVrMHIElWO9fiwOVbg=</latexit>U2\nFigure 1: We illustrate our model\nfor N = 2.\nLow and high fre-\nquencies are separated (blue) and\nthen the high frequencies are de-\nmodulated (red) via an isometry\nand a non-linear point wise abso-\nlute value, and then propagated to\nthe next layer.\nThe next section will describe the E-IGT, which was introduced\nas the Expected Scattering [31], but in a rather different context:\nwe will show under simplifying assumptions that an IGT for\ncommunity labeling concentrates around the E-IGT.\n3.2\nDeﬁnition of the Expected-IGT (E-IGT)\nSimilarly to the previous section, for an input signal ¯U0 ≜X,\nwe consider the following recursion, introduced in [31]:\n¯Un+1 ≜|( ¯Un −E ¯Un)Wn| ,\n(5)\nwhich leads to the E-IGT 2 of order N deﬁned by:\n¯SN ≜{E ¯U0, ..., E ¯UN} .\n(6)\nSimilarly to Prop. 7, we prove the following stability result:\nProposition 2. For N ∈N, ¯SNX is 1-Lipschitz, meaning that:\n∥¯SNX −¯SNY ∥2 ≤E[∥X −Y ∥2] ,\n(7)\nand furthermore:\n∥¯SNX∥2 ≤E[∥X∥2] .\n(8)\nProof. Indeed, [31] have proven this for the columns of X.\nNote that this represention is also more amenable to standard supervised classiﬁers such as SVMs\nbecause no operation mixing nodes is involved. Prop. 2 highlights the fact that the E-IGT is non-\nexpansive, and [46] shows that this allows to discriminate the attributes of the distribution of X.\nHowever, it is difﬁcult in general to estimate the E-IGT because one does not know the distribution\nof a given node and it is difﬁcult to estimate it from a single realization as there is a clear curse\nof dimensionality. However, we will show that SN\nJ will be very similar to ¯SN under standard\nassumptions on communities. We now state the following proposition, which allows to quantify the\ndistance between an IGT and its E-IGT:\nProposition 3. For any X, N, J, we get:\n∥SN\nJ X −¯SNX∥≤\n√\n2\nN\nX\nm=0\n∥(AJ −E) ¯Um∥.\n(9)\nThe proof of this proposition can be found in the Appendix: it fully uses the tree structure of Fig. 1,\nin order to obtain tighter bounds than [31], as it allows N to be of arbitrary size without diverging.\nWe now bound the distance between the IGT and the E-IGT:\n2We rename it here because we use rather different principles to obtain the {W0..., WN−1} compared to the\noriginal Scattering.\n4\nCorollary 1. For N ∈N, we have:\nsup\nE∥X∥≤1\nE[∥SN\nJ X −¯SNX∥] ≤2N+2\nsup\nE∥X∥≤1\nE[∥AJX −EX∥] .\n(10)\nProof. The next Lemma combined with the norm homogeneity allows to conclude with Prop. 3.\nLemma 3. If ∥X∥≤1, then ∥¯Un∥≤2n, with ¯U0 = X. Also, if E[∥X∥] ≤1, then E[∥¯Un∥] ≤2n\nProof. This is true for n = 0, and then by induction, since isometry preserves the ℓ2-norm: ∥¯Un+1∥≤\n∥¯Un∥+ ∥E ¯Un∥≤∥¯Un∥+ E∥¯Un∥≤2n+1. The proof is similar for the second part.\nThe right term of Eq. 10 measures the ergodicity properties of a given AJ. For instance, in the\ncase of images, a stationary assumption on X implies that AJf(X) ≈Ef(X) for all measurable f,\nwhich is the case for instance for textures [29]. The following proposition shows that in case of exact\nergodicity, the two representations have bounded moments of order 2:\nProposition 4. If E[AJX] = EX, and if X has variance σ2 = E∥X∥2 −∥EX∥2, then:\nE[∥SN\nJ X −¯SNX∥2] ≤2σ2 .\n(11)\n3.3\nGraph model and concentration bounds\nIn this subsection, we propose to demonstrate novel bounds which improve the upper bound obtained\nat Prop. 1 by introducing a Stochastic Block Model [19]. We will show that the IGT features of a\ngiven community concentrates around the E-IGT feature of this community: IGT features are thus\nmore amenable to be linearly separable. Recall from Sec. 3.1 that A1 = Anorm, thus we note that for\nsome m > 0, via the triangular inequality we get:\n∥A1 ¯Um −E ¯Um∥= ∥Anorm ¯Um −E ¯Um∥≤∥(Anorm −E[A]norm) ¯Um∥+ ∥E[A]norm ¯Um −E[ ¯Um]∥.\nNow, the left term can be upper bounded as:\n∥(Anorm −E[Anorm]) ¯Um∥≤∥Anorm −E[Anorm]∥∥¯Um∥.\n(12)\nFor the sake of simplicity, we will consider a model with two communities, yet the extension to more\ncommunities is straightforward and would simply involve a linear term in the number of communities.\nWe now describe our model. Once the n nodes have been split in two groups of size n ∼n1, n ∼n2,\nwe assume that each edge between two different nodes is sampled independently with probability pn\n(or simply p if not ambiguous) if they belong to the same community and q otherwise. We assume\nthat q = τp for some constant τ ∼\n1\n√n ≪1 and the features belonging to the same community\nare i.i.d. and σ-sub-Gaussian, and ∥X∥≤1. Those assumptions are not restrictive as they hold in\nmany practical applications (and the second, always holds up to a constant). For a given community\ni ∈{1, 2}, we write (µi\nm)m≤N its E-IGT. We impose that pn ∼log(n)\nn\nin this particular Bernoulli\nmodel. Sparse random graphs do not generally concentrate. Yet, according to [23], in the relatively\nsparse case where pn ∼log n\nn , we get the following spectral concentration bound of the normalized\nadjacency matrix:\nLemma 4. Let A be a symmetric matrix with independent entries Aij obtained as above. If\nn1 ∼n, n2 ∼n, and p is relatively sparse as above, then for all ν > 0, there is a constant Cν such\nthat, with high probability ≥1 −n−ν:\n∥Anorm −E[A]norm∥≤\nCν\n√log n .\n(13)\nProof. Can be found in [23].\nNote that in general, E[A]norm ̸= E[Anorm] and here, because of our model:\nE[A]norm =\n\u0014\np\nn1p+n2q1n1×n1\nq\nn1p+n2q1n1×n2\nq\nn1q+n2p1n2×n1\np\nn1q+n2p1n2×n2\n\u0015\n,\n(14)\n5\nwhere 1m×n is a matrix of ones of size m × n. Now, note also that:\nE[ ¯Um] = [µ1\nm1T\nn1, µ2\nm1T\nn2] ,\n(15)\nNow, we prove that the IGT will concentrate around the E-IGT, under a Stochastic Block Model\nand sub-Gaussianity assumptions. We note that a bias term of the order of √nτ is present, which\nis consistent with our model assumptions. Note it is also possible to leverage the boundedness\nassumption yet it will lead to an additional constant term.\nProposition 5. Under the assumptions above, there exists C > 1 s.t. for all N > 0, δ > 0, we have\nwith high probability, larger than 1 −O(Nδ + n−ν):\n∥SN\n1 X −¯SNX∥= O(σ 1 + CN\n1 −C (\nr\nln 1\nδ +\n1\n√log n)) + O(τ√n\nX\nm≤N\n∥µ2\nm −µ1\nm∥) .\n(16)\nThe following proposition allows to estimate the concentration of each IGT order:\nProposition 6. Assume that each line of X ∈Rn×P is σ-sub-Gaussian. There exists C > 1, K >\n0, C′ > 1 such that ∀m, δ > 0 with probability 1 −8Pδ, we have:\n∥E[A]norm ¯Um −E[ ¯Um]∥≤KσCm\nr\nln 1\nδ + C′√nτ∥µ2\nm −µ1\nm∥.\n(17)\nThis Lemma shows that a cascade of IGT linear isometries preserves sub-Gaussianity:\nLemma 5. If each line of X is σ-sub-Gaussian, then each (independent) line of ¯Um is Cmσ-sub-\nGaussian for some universal constant C.\nIn order to show the previous Lemma, we need to demonstrate that the modulus of a sub-Gaussian\nvariable is itself sub-Gaussian, which is shown below:\nLemma 6. There is C > 0, s.t. X ∈RP is σ-sub-Gaussian, then |(X−EX)W| is Cσ-sub-Gaussian.\n3.4\nOptimization procedure\nWe now describe the optimization procedure of each of our operators {Wn}, that consists in a greedy\nlayer-wise procedure [3]. Our goal is to specify |Wn| such that it leads to a demodulation effect, as\nwell as to have a fast energy decay. Demodulation means that the envelope of a signal should be\nsmoother, whereas fast decay will allow the use of shallower networks. In practice, it means that at\ndepth n, the energy along the direction of averaging should be maximized, which leads to consider:\nmax\nW T W =I ∥AJ|(I −AJ)UnW|∥.\n(18)\nAs observed in [35], because the extremal points of the ℓ2 ball are the norm preserving matrix, this\noptimization problem is equivalent to:\nmax\n∥W ∥2≤1 ∥AJ|(I −AJ)UnW|∥.\n(19)\nNote that this can be approximatively solved via a projected gradient procedure which projects the\noperator W on the unit ball for the ℓ2-norm at each iteration. Furthermore, contrary to [35], we might\nconstrain W to have a rank lower than the ambient space, that we denote by k: increasing k as well\nas the order N allows to potentially increase the capacity of our model, yet we as discussed in the\nnext section, this wasn’t necessary to obtain accuracies at the level of the state of the art.\n4\nNumerical Experiments\nWe test our unsupervised IGT features on a synthetic example, and on challenging semi-supervised\ntasks, in various settings that appeared in the graph community labeling litterature: the full [40],\npredeﬁned [25] and random splits [25] of Cora, Citeseer, Pubmed, as well as the WikiCS dataset.\n6\n4.1\nSynthetic example\nAs GCNs progressivly apply a smoothing operator on subsequent layers, deeper features are less\nsensitive to intra-community variability. This progressive projection can have a big impact on\ndatasets where discriminative features are close in average, yet have very different distributions over\nseveral communities. In order to underline this phenomenon, we propose to study the following\nsynthetic example: following the model and notations of Sec. 3.3, we consider two communi-\nties, with an equal number of samples in each and we assume that P = 1, J = 1, p = 0.001\nand q = 0 and n = 10000. Here, we assume the features are centered Gaussians with variance\nσ1 = 1 for the ﬁrst community and σ2 = σ1 + ∆σ for the second. In other words, ∆σ con-\ntrols the relative spread of the community features. Our goal is to show numerically that an IGT\nrepresentation is, by construction, more amenable to distinguish the two communities than a GCN.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n50\n60\n70\n80\n90\nAccuracy (%)\nMethod\nGCN\nS0\nS1\nS2\nSubset\nTrain\nTest\nFigure 2: Accuracies of GCN against our method\non a synthetic example, for several values of ∆σ.\nAs a training set, we randomly sample 20 nodes\nand use the remaining ones as a validation and\ntest set. For IGT parameters, we pick J = 2,\nk = 1 and N ∈{0, 1, 2}.\nOn top of our\nstandardized IGT features, our classiﬁcation\nlayer consists in a 1-hidden layer MLP of width\n128. We train the IGT operators for 50 epochs.\nWe compare our representation with a standard\nGCN [25] that has two hidden layers and a hid-\nden dimension of 128 for the sake of comparison.\nBoth supervised architectures are trained during\n200 epochs using Adam [24] with a learning rate\nof 0.01 (ϵ = 10−8, β1 = 0.9, β2 = 0.999). We\ndiscuss next the accuracies averaged over 5 dif-\nferent seeds on Fig. 2 for various representations\nand values of ∆σ.\nWe observe that an order 0 IGT performs poorly\nfor any values of ∆σ, which is consistent because the linear smoothing will dilute important informa-\ntions for node classiﬁcation. However, non-linear model like IGT (of order larger than 0) or GCN\noutperforms this linear representation. The IGT outperforms the GCN for all values of ∆σ because\nas Sec. 3.1 shows, by construction, this representation extracts explicitly the high frequency of the\ngraph, whereas a GCN can only smooth its features and thus will tend to lose in discriminability\ndespite supervision. We note that orders 1 and 2 perform similarly, which is not surprising given the\nsimplistic assumption of this model: all the informative variability is contained in the order 1 IGT\nand the order 2 is likely to only bring noisy features in this speciﬁc case.\n4.2\nSupervised community detection\nFirst, we describe our experimental protocol on the datasets Cora, CiteSeer and PubMed. Each dataset\nconsists in a set of bag-of-words vectors with citation links between documents. They are made of\nrespectively 5.4k, 4.7k, 44k and 216k edges with features of size respectively 1.4k, 3.7k, 0.5k and\n0.3k. For the three ﬁrst datasets, we test our method in three semi-supervised settings, which consist\nin three different approaches to split the dataset into train, validation and test sets: at this stage, we\nwould like to highlight we are one of the few methods to try its architecture on those three splits\n(which we discuss for clarity), which allows to estimate the robustness to various sample complexity.\nEach accuracy is averaged over 5 runs and we report the standard-deviation in the Appendix. The\nmost standard split is the predeﬁned split setting: each training set is provided by [25] and consist\nin 20 training nodes per class, which represent a fraction 0.052, 0.036, and 0.003 of the data for Cora,\nCiteSeer and PubMed respectively. 500 and 1000 nodes are respectively used as a validation and\ntest set. Then, we consider the random split setting introduced in [25], which is exactly as above\nexcept that we randomly extract 5 splits of the data, and we average the accuracies among those splits.\nFinally, we consider the full split setting which was used in [40] and employs 5 random splits of a\nlarger training set: a fraction 0.45, 0.54 and 0.92 of the whole labeled datasets respectively. Note\nthat each of those tasks is transductive yet our method would require minimal adaptation to ﬁt an\ninductive pipeline. For WikiCS, we followed the only guideline of [32].\n7\nOur architectures are designed as follow: an IGT representation only requires 4 hyper-parameters: an\nadjacency matrix A, an output-size k for each linear isometry, a smoothness parameter J and an IGT\norder N. Given that the graphs are undirected, A satisﬁes the assumption described in Sec. 3.1, yet\nit would be possible to symmetrize the adjacency matrix of a directed graph. This corresponds to\nour unsupervised graph representation that will be then fed to a supervised classiﬁer. Sec. 3.3 shows\nthat our IGT representation should concentrate around the E-IGT of their respective community,\nwhich means that they should be well separated by a Linear classiﬁer. However, there might be\nmore intra-class variability than the one studied from the lens of our model, thus we decided to use\npotentially deeper models, e.g., Multi Layer Perceptrons (MLPs) as well as Linear classiﬁers. We\nuse the same ﬁxed MLP architecture for every dataset: a single hidden layer with 128 features. Our\nlinear model is simply a fully connected layer, and each model is fed to a cross-entropy loss. We\nnote that our MLP is shallow, with few units, and does not involve the graph structure by contrast to\nsemi-supervised GCNs: we thus refer to the combination of IGT and a MLP or a Linear layer as an\nunsupervised graph representation for node labeling. Note also that a MLP is a scalable classiﬁer in\nthe context of graphs: once the IGT representation is estimated, one can learn the weight of the MLP\nby splitting the training set in batches, contrary to standard GCNs.\nWe now describe our training procedure as well as the regularizaton that we incorporated: it was\nidentical for any splits of the data. We optimized our pipeline on Cora and applied it on Citeseer,\nPubmed and WikiCS, unless otherwise stated. Each parameter was cross-validated on a validation\nset, and we report the test accuracy on a test set that was not used until the end. First, we learn\neach {Wm}m≤N via Adam for 50 epochs and a learning rate of 0.01. Once computed, the IGT\nfeatures are normalized and are fed to our supervised classiﬁer, that we train again using Adam and a\nlearning rate of 0.01 for at most 200 epochs, with a early stopping procedure and a patience of 30. A\ndropout ratio which belongs to {0, 0.2, 0.4, 0.5, 0.6, 0.8} is potentially incorporated to the one hidden\nlayer of the MLP. On CiteSeer and PubMed, our procedure selected 0.2, on WikiCS 0.8, whereas\nno-dropout was added on Cora. Furthermore, we incorporated an ℓ2-regularization with our linear\nlayer which we tried amongst {0, 0.001, 0.005, 0.01}: we picked 0.005 via cross-validation. We\ndiscuss here WikiCS: by cross-validation, we used J = 1, N = 1, k = 150 for the linear experiment\nand J = 2, N = 1, k = 35 for the MLP experiment. For the other datasets and every splits, we\nused N = 2 and k = 10: we note that less capacity is needed compared to WikiCS, because those\ndatasets are simpler. For the three other datasets, for both the predeﬁned and random splits, we ﬁx\nJ = 4. For the full split, we used J = 1 for each dataset: we noticed that increasing J degrades the\nperformance, likely because less invariance is required and can be learned from the data, because\nmore samples are available. This makes sense, as the amount of smoothing depends on the variability\nexhibited by the data. Thanks to the amount of available data, the supervised classiﬁer can estimate\nthe degree of invariance needed for the classiﬁcation task, which was not possible if using only 20\nsamples per community.\nTab. 4 reports the semi-supervised accuracy for each dataset, in various settings, and compares\nstandard supervised [14, 25, 40, 8, 7, 42] and unsupervised [38, 43, 15, 39, 17] architectures. Note\nthat each supervised model is trained in an end-to-end manner. The unsupervised models are built\ndifferently and we discuss them now brieﬂy: for instance, EP [15], uses a node embedding with\na rather different architecture from GCNs. Also, DeepWalk [38] is analogous to a random walk,\nGraphSage [17] learns an embedding with a local criterion, DGI [43] relies on a mutual information\ncriterion and ﬁnally [39] relies on a random ﬁeld model. Note that each of those models are\nsigniﬁcantly different from ours and they do not have the same theoretical foundations and properties\nas ours. As expected, accuracy in the full setting is higher than the others. We observe that in general,\nsupervised models outperforms unsupervised models by a large margin except on WikiCS and\nCiteseer for the random and predeﬁned splits, for which an IGT obtains better accuracy: it indicates\nthat it has a better inductive bias for this dataset. Note that an IGT obtains competitive accuracies\namongst unsupervised representations and this is consistent with the fact that those datasets, discussed\nabove, are likely to satisfy the hypothesis described in Sec. 3.3. In general, a MLP outperforms a\nlinear layer (because it has better approximation properties), except on Citeseer for which the accuracy\nis similar, which seems to validate that the data of Citeseer follow the model that we introduced in\n3.3 on Citeseer, that leads to linear separability.\n4.3\nAblation experiments\n8\nTable 1: Classiﬁcation accuracies (in %) for each splits of Cora, Citeseer, Pubmed as well as WikiCS.\nMethod/Dataset\nCora\nCiteseer\nPubmed\nWikiCS\nFull\nRand\nPred\nFull\nRand\nPred\nFull\nRand\nPred\nSupervised\nGAT [42]\n83.0\n72.5\n79.0\n77.2\nGCN [25]\n80.1\n81.5\n67.9\n70.3\n78.9\n79.0\n77.7\nGraph U-Net [14]\n84.4\n73.2\n79.6\nDropEdge [40]\n88.2\n80.5\n91.7\nFastGCN [8]\n85.0\n77.6\n88.0\nOS [7]\n82.3\n69.7\n77.4\nUnsupervised\nRaw [43, 32]\n47.9\n49.3\n69.1\n72.0\nDeepWalk [38]\n67.2\n43.2\n65.3\n74.4\nIGT + MLP (ours)\n87.7\n78.3\n80.3\n78.4\n67.6\n73.1\n88.2\n76.2\n76.4\n77.2\nIGT + Lin. (ours)\n83.3\n77.6\n77.4\n78.4\n73.0\n73.1\n88.1\n74.5\n73.9\n76.7\nEP [15]\n78.1\n71.0\n79.6\nGraphSage [17]\n82.2\n71.4\n87.1\nDGI [43]\n82.3\n71.8\n76.8\n75.4\nGMNN [39]\n82.8\n71.5\n81.6\nTable 2: Linear classiﬁcation accuracies (in %) for\nthe predeﬁned split on Cora’s validation set, for\nvarious values of N, J.\nJ\nN\n0\n1\n2\n3\n1\n62.4\n60.8\n62.8\n61.4\n2\n68.6\n70.6\n72.2\n68.6\n3\n71.4\n72.2\n74.6\n72.6\n4\n72.4\n73.2\n74.6\n73.0\nIn order to understand better the IGT represen-\ntation, we propose to study the accuracy of an\nIGT representation on Cora’s validation set, as\na function of the scale J and the IGT order N.\nFor the sake of simplicity, we consider a linear\nclassiﬁer. Each linear operator is learned with\n40 epochs. We picked k = 10 and train our\nbasic model for 200 epochs with SGD, the val-\nidation accuracies are reported in Tab. 2. As\nN, J increase, we feed the features to a linear\nclassiﬁer: in general, for 0 ≤N ≤2, as N grows the accuracy improves. However, the order 3\nIGT decreases the accuracy: this is consistent because it conveys a noise which is ampliﬁed by the\nstandardization. As J increases, we smooth our IGT features on more neighbor nodes, which results\nin better performances for a ﬁxed order N, and is also consistent with the ﬁnding of Sec. 3.1.\nWe performed a second ablation experiment in order to test the inductive bias of our architecture: we\nconsidered random {Wn} at evaluation time and we obtained respectively on the full split of Cora,\nCiteseer and Pubmed some accuracy drops of respectively 6.3%, 5.2% and 5.6%. This is relatively\nsmaller drops than DGI [43] which reports for instance some drops of about 10%: our architecture is\nlikely to have a better inductive bias for this task.\n5\nConclusion\nIn this work, we introduced the IGT which is an unsupervised and semi-supervised representation for\ncommunity labeling. It consists in a cascade of linear isometries and point-wise absolute values. This\nrepresentation is similar to a semi-supervised GCN, yet it is trained layer-wise, without using labels,\nand has strong theoretical fundations. Indeed, under a SBM assumption and a large graph hypothesis,\nwe show that an IGT representation can discriminate communities of a graph from a single realization\nof this graph. It is numerically supported by a synthetic example based on Gaussian features, which\nshows that an IGT can estimate the community of a given node better than a GCN because it tends to\nalleviate the over-smoothing phenomenon. This is further supported by our numerical experiments\non the standard, challenging datasets Cora, CiteSeer, PubMed and WikiCS: with shallow supervised\nclassiﬁers, we obtain numerical accuracy which is competitive with semi-supervised approaches.\nFuture directions could be to either reﬁne our theoretical analysis by weakening our assumptions,\nor to test our method on inductive tasks. Furthermore, following [35], one can also wonder if this\ntype of approach could be extended to more complex data, in order to obtain stronger theoretical\nguarantees (e.g., manifold). Finally, future works could also be dedicated to scale our algorithms to\nvery large graphs: this is a challenging task both in terms of memory and computations.\n9\nBroader impact.\nGraph Neural Networks can be used in many domains, like protein prediction, or\nnetwork analysis to cite only a few, and could become even more prevalent tomorrow. Our work is\nthus included in a large literature whose societal impact and ethical considerations are to become\nmore and more important. We provide here a new model aiming at learning unsupervised node\nrepresentation in community graphs graphs. While its most natural application lies in community\ndetection in social science, we hope that the provided theoretical guarantees could be used in the\nfuture to provide safer and more readable models toward more various directions.\nAcknowledgements\nEO, LL, NG would like to acknowledge the CIRM for its one week hospitality which was helpful to\nthis project. EO acknowledges NVIDIA for its GPU donation and this work was granted access to\nthe HPC resources of IDRIS under the allocation 2020-[AD011011216R1] made by GENCI. EO,\nLL were partly supported by ANR-19-CHIA \"SCAI\" and ANR-20-CHIA-0022-01 \"VISA-DEEP\".\nNG and PP would like to acknowledge the support of the French Ministry of Higher Education and\nResearch, Inria, and the Hauts-de-France region; they also want to thank the Scool research group for\nproviding a great research environment. The authors would like to thank Mathieu Andreux, Alberto\nBietti, Edouard Leurent, Nicolas Keriven, Ahmed Mazari, Aladin Virmaux for helpful comments and\nsuggestions.\nReferences\n[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The\nJournal of Machine Learning Research, 18(1):6446–6531, 2017.\n[2] J. Andén, V. Lostanlen, and S. Mallat. Joint time-frequency scattering for audio classiﬁcation.\nIn 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),\npages 1–6, 2015.\n[3] Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can\nscale to imagenet. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of\nthe 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages\n583–593. PMLR, 2019.\n[4] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.\nGeometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,\n34(4):18–42, 2017.\n[5] Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions\non pattern analysis and machine intelligence, 35(8):1872–1886, 2013.\n[6] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\nconnected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.\n[7] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the\nover-smoothing problem for graph neural networks from the topological view. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 3438–3445, 2020.\n[8] Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks\nvia importance sampling. In International Conference on Learning Representations, 2018.\n[9] Zhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph\nneural networks. In International Conference on Learning Representations, 2018.\n[10] Vincent Cohen-Addad, Adrian Kosowski, Frederik Mallmann-Trenn, and David Saulpic. On\nthe power of louvain in the stochastic block model. Advances in Neural Information Processing\nSystems, 33, 2020.\n[11] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks\non graphs with fast localized spectral ﬁltering. arXiv preprint arXiv:1606.09375, 2016.\n10\n[12] Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural\nnetworks. IEEE Transactions on Signal Processing, 68:5680–5695, 2020.\n[13] Feng Gao, Guy Wolf, and Matthew Hirn. Geometric scattering for graph data analysis. In\nInternational Conference on Machine Learning, pages 2122–2131. PMLR, 2019.\n[14] Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning,\npages 2083–2092. PMLR, 2019.\n[15] Alberto García-Durán and Mathias Niepert. Learning graph representations with embedding\npropagation. arXiv preprint arXiv:1710.03059, 2017.\n[16] Nathan Grinsztajn, Philippe Preux, and Edouard Oyallon. Low-rank projections of GCNs\nlaplacian. In ICLR 2021 Workshop on Geometrical and Topological Representation Learning,\n2021.\n[17] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large\ngraphs. In Proceedings of the 31st International Conference on Neural Information Processing\nSystems, pages 1025–1035, 2017.\n[18] David K Hammond, Pierre Vandergheynst, and Rémi Gribonval. Wavelets on graphs via spectral\ngraph theory. Applied and Computational Harmonic Analysis, 30(2):129–150, 2011.\n[19] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels:\nFirst steps. Social networks, 5(2):109–137, 1983.\n[20] Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-\nsmoothing for general graph convolutional networks. arXiv e-prints, pages arXiv–2008, 2020.\n[21] Vassilis N Ioannidis, Siheng Chen, and Georgios B Giannakis. Efﬁcient and stable graph scat-\ntering transforms via pruning. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2020.\n[22] Nicolas Keriven and Gabriel Peyré. Universal invariant and equivariant graph neural networks.\narXiv preprint arXiv:1905.04943, 2019.\n[23] Nicolas Keriven and Samuel Vaiter. Sparse and smooth: improved guarantees for spectral\nclustering in the dynamic stochastic block model. arXiv preprint arXiv:2002.02892, 2020.\n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[25] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional\nnetworks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\n[26] Can M Le, Elizaveta Levina, and Roman Vershynin. Concentration of random graphs and\napplication to community detection. arXiv preprint arXiv:1801.08724, 2018.\n[27] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks\nfor semi-supervised learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 32, 2018.\n[28] Andreas Loukas and Pierre Vandergheynst. Spectrally approximating large graphs with smaller\ngraphs. In International Conference on Machine Learning, pages 3237–3246. PMLR, 2018.\n[29] Stéphane Mallat. A wavelet tour of signal processing. Elsevier, 1999.\n[30] Stéphane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics,\n65(10):1331–1398, 2012.\n[31] Stéphane Mallat and Irene Waldspurger.\nDeep learning by scattering.\narXiv preprint\narXiv:1306.5532, 2013.\n11\n[32] Péter Mernyei and C˘at˘alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural\nnetworks. arXiv preprint arXiv:2007.02901, 2020.\n[33] Yimeng Min, Frederik Wenkel, and Guy Wolf. Scattering gcn: Overcoming oversmoothness in\ngraph convolutional networks. arXiv preprint arXiv:2003.08414, 2020.\n[34] Edouard Oyallon. Building a regular decision boundary with deep networks. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 5106–5114, 2017.\n[35] Edouard Oyallon. Interferometric graph transform: a deep unsupervised graph representation.\nIn International Conference on Machine Learning, pages 7434–7444. PMLR, 2020.\n[36] Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko, and Michal Valko. Compressing\nthe input for cnns with the ﬁrst-order scattering transform. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 301–316, 2018.\n[37] Edouard Oyallon and Stéphane Mallat. Deep roto-translation scattering for object classiﬁcation.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n2865–2873, 2015.\n[38] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pages 701–710, 2014.\n[39] Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In Interna-\ntional conference on machine learning, pages 5241–5250. PMLR, 2019.\n[40] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph\nconvolutional networks on node classiﬁcation. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[41] Luana Ruiz, Zhiyang Wang, and Alejandro Ribeiro. Graph and graphon neural network stability.\narXiv preprint arXiv:2010.12529, 2020.\n[42] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n[43] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon\nHjelm. Deep graph infomax. In ICLR (Poster), 2019.\n[44] Roman Vershynin. Concentration of Sums of Independent Random Variables, page 11–37.\nCambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,\n2018.\n[45] Martin J. Wainwright. Basic tail and concentration bounds, page 21–57. Cambridge Series in\nStatistical and Probabilistic Mathematics. Cambridge University Press, 2019.\n[46] Irène Waldspurger. Wavelet transform modulus: phase retrieval and scattering. Journées\néquations aux dérivées partielles, pages 1–10, 2017.\n[47] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning\nwith graph embeddings. In International conference on machine learning, pages 40–48. PMLR,\n2016.\n12\n6\nAppendix\n6.1\nProofs\nLemma 2. If 0 ≼A ≼I, then ∥AX∥2 + ∥(I −A)X∥2 ≤∥X∥2 with equality iff A2 = A.\nProof. We note that for any x, we get:\n∥Ax∥2 + ∥(I −A)x∥2 = ∥Ax∥2 + ∥x∥2 + ∥Ax∥2 −2⟨x, Ax⟩\n(20)\nYet, ∥Ax∥2 = ⟨x, AT Ax⟩≤⟨x, Ax⟩because Sp(A) ⊂[0, 1]. Thus,\n2(∥Ax∥2 −⟨x, Ax⟩) + ∥x∥2 ≤∥x∥2 ,\n(21)\nwith equality ∀x iff A = A2. It is now enough to observe that {A, I −A} inherits from those\nproperties.\nThe following proposition explains that our representation is non-expansive, and thus stable to noise:\nProposition 7. For N ∈N, SN\nJ X is 1-Lipschitz leading to:\n∥SN\nJ X −SN\nJ Y ∥≤∥X −Y ∥.\n(22)\nand furthermore:\n∥SN\nJ X∥≤∥X∥.\n(23)\nProof. For two feature matrices X, Y , let us consider Ui and ˜Ui deﬁned from Equation (2), with\nU0 = X and ˜U0 = Y . Because |Wi| is a contractive and from Lemma 2,\n∥Ui+1 −˜Ui+1∥2 ≤∥Ui −˜Ui −AJ(Ui −˜Ui)∥2\n(24)\n≤∥Ui −˜Ui∥2 −∥AJ(Ui −˜Ui)∥2\n(25)\nHence,\nN\nX\ni\n∥AJ(Ui −˜Ui)∥2 ≤∥X −Y ∥2 −∥Un −˜Un∥2\n(26)\n≤∥X −Y ∥2\n(27)\nTaking X = 0 leads to the second part as then SX = 0.\nThis Lemma shows that a cascade of IGT linear isometries preserve sub-Gaussianity:\nLemma 5. If each line of X is σ-sub-Gaussian, then each (independent) line of ¯Um is Cmσ-sub-\nGaussian for some universal constant C.\nProof. Apply the Lemma 6 with W = Wn for n ≤m leads to the result.\nIn order to show the previous Lemma, we need to demonstrate that the modulus of a sub-Gaussian\nvariable is itself sub-Gaussian, which is shown below:\nLemma 6. If X ∈RP is σ-sub-Gaussian, then |(X−EX)W| is Cσ-sub-Gaussian for some absolute\nvalue C.\nProof. If X is σ-sub-Gaussian, then X −EX is C′σ-subGaussian by recentering [44]. We note that\nas W is unitary, thus (X −EX)W is also C′σ-subgaussian. Then, let u ∈Rp an unit vector. We\nnote that:\nP(\np\nX\ni=1\nui|Xi| ≥t)\n(28)\n≤\nX\nϵi∈{−1,1}\nP({ϵiXi ≥0} ∩{\nX\ni\nui|Xi| ≥t})\n(29)\n=\nX\nϵi∈{−1,1}\nP({ϵiXi ≥0} ∩{\nX\ni\nϵiuiXi ≥t})\n(30)\n≤2pe−\nt2\n2C′2σ2 = ep ln 2−\nt2\n2C′2σ2 .\n(31)\n13\nThis leads to the conclusion by sub-Gaussian characterization.\nProposition 3. For any X, N, J, we get:\n∥SN\nJ X −¯SNX∥≤\n√\n2\nN\nX\nm=0\n∥(AJ −E) ¯Um∥.\n(32)\nProof. Here, write V m\nJ\n= |(X −AJX)Wm|, V 0\nJ X = X, and deﬁne:\nY n,m\nJ\nX ={AJV n\nJ ...V n−m+1\nJ\nX, AJV n−1\nJ\n...V n−m+1\nJ\nX\n(33)\n, ..., AJV n−m+1\nJ\nX, AJX},\n(34)\nLemma.\nIf AJ is a unitary projector and each Wi is unitary, then Y m\nJ X is 1-Lipschitz w.r.t. X.\nProof. We can apply the proposition 2 with the operators {Wn−m+1, ..., Wn}, as this can be inter-\nprated as an IGT with different unitary operators.\nHere the idea is to take advantage of the tree structure of the IGT features. Thus when Y n,m\nJ\nis\ncomputing Sn\nJ to orders limited in [n, n −m + 1], we chain the features with the order n −m to\nrecover Y n,m−1\nJ\n. To do so, we introduce for m ≥1 :\n∆n,m\nJ\nX = {Y m\nJ V n−m\nJ\nX −Y m\nJ ¯V n−mX, AJX −EX}\n(35)\n= {−Y m\nJ ¯V n−mX, −EX} + Y m−1\nJ\nX ,\n(36)\nwhere ¯V nX = |(X −EX)Wn|, ¯V 0X = X and {x, y} stands for a concatenation. This implies that\n∆n,m\nJ\nX is a (m + 1)-uplet (and the symbol + in (36) is thus a couple addition and the convention is\nthat left corresponds to highest order of the couple), and ∆0,0\nJ X = AJX −EX = −EX + S0\nJX.\nThe sum over m-uplet with different size is done such that the left elements are summed ﬁrst. We\nthen notice that:\nN\nX\nm=0\n∆N,m\nJ\n¯V N−m−1... ¯V1X = SN\nJ X −¯SNX\n(37)\nbecause each term of the couple is a telescopic sum (again here, we chain the features with orders in\n[n −m −1, 1] to obtain the telescopy).\nAs Y n,m\nJ\nis 1-Lipschitz w.r.t. X and since a modulus is non expansive, ∥|(X −AJX)Wn| −|(X −\nEX)Wn|∥≤∥EX −AJX∥, combining those ingredients we get:\n∥∆n,m\nJ\nX∥2 =∥AJX −EX∥2+\n(38)\n∥Y m−1\nJ\n|(X −AJX)Wn| −Y m−1\nJ\n|(X −EX)Wn∥2\n(39)\n≤2∥AJX −EX∥2 .\n(40)\nThen, we further apply the triangular inequality to get the desired result.\nThe following proposition shows that in case of exact ergodicity, the IGT and Expected-IGT repre-\nsentations have bounded moments of order 2:\nProposition 4. Assume that E[AJX] = EX, and that X has variance σ2 = E∥X∥2 −∥EX∥2,\nthen:\nE[∥SN\nJ X −¯SNX∥2] ≤2σ2 .\n(41)\nProof.\nE[∥SN\nJ X −¯SNX∥2] = E[∥SN\nJ X∥2] + E[∥¯SNX∥2]\n(42)\n−2\nN\nX\nm=0\nE[Tr((AJUm)T E[ ¯Um])]\n(43)\n≤2(E∥X∥2 −\nN\nX\nm=0\nE[Tr((AJUm)T E[ ¯Um])])\n(44)\n14\nThe inequality follows from Prop. 2 and Prop. 7. Now, from Lemma 1, AJ, Um, ¯Um have positive\ncoefﬁcients, thus we get: 2 PN\nm=1 E[Tr((AJUm)T E[ ¯Um])] ≥0. The ﬁrst term allows to conclude as\n¯U0 = U0 = X.\nProposition 6. Assume that each line of X is σ-sub-Gaussian. There exists C > 1, K > 0, C′ > 0\nsuch that ∀m, δ > 0 with probability 1 −8Pδ, we have:\n∥E[A]norm ¯Um −E[ ¯Um]∥\n(45)\n≤KσCm\nr\nln 1\nδ + τ√nC′∥µ2\nm −µ1\nm∥.\n(46)\nProof. Here, for the sake of simplicity, Xp corresponds to the p-th row of X. We write µj\nm the\nexpected-IGT of the node distribution of community j. Here, we have for t ≤n1 (note that the right\ndoes not depend on t):\n[E[A]norm ¯Um]t −E[ ¯Um]t\n(47)\n=\n1\nn1p + n2q\n\u0000p\nn1\nX\ni=1\n( ¯U i\nm −µ1\nm) + q\nn1+n2\nX\ni=n1+1\n( ¯U i\nm −µ2\nm)\n\u0001\n(48)\n+\nn2q\nn1p + n2q (µ2\nm −µ1\nm) .\n(49)\nNow, we note that from Lemma 5, { ¯U i\nm}i≤n is a family of σCm-sub-Gaussian independant r.v. From\nHoeffding lemma [45], we obtain that for any δ, we have with probability 1 −4Pδ:\n∥\nn1\nX\ni=1\n( ¯U i\nm −µ1\nm)∥≤√n1\n√\n2σCm\nr\nln 1\nδ\nand\n∥\nn1+n2\nX\ni=n1+1\n( ¯U i\nm −µ2\nm)∥≤√n2\n√\n2σCm\nr\nln 1\nδ .\nAs if n is large, by hypothesis ( p√2n1+q√2n2\nn1p+n2q\n)√n = O(1). We perform the same for n1 < t ≤\nn1 + n2 We then sum along n and use that\nn1\nn2+τn1 +\nn2\nn1+τn2 = O(1) and\n√\na + b ≤√a +\n√\nb.\n6.2\nDataset statistics\nTable 3: Dataset Statistics\nDatasets\nNodes\nEdges\nClasses\nFeatures\nfull Train/Val/Test\nsemi Train/Val/Test\nCora\n2,708\n5,429\n7\n1,433\n1,208/500/1,000\n140/500/1,000\nCiteseer\n3,327\n4,732\n6\n3,703\n1,812/500/1,000\n120/500/1,000\nPubmed\n19,717\n44,338\n3\n500\n18,217/500/1,000\n60/500/1,000\nWikiCS\n11,701\n216,123\n10\n300\n20 canonical train/valid/test splits\n15\nTable 4: Standard deviations of classiﬁcation accuracies for each splits of Cora, Citeseer, Pubmed as\nwell as WikiCS.\nMethod/Dataset\nCora\nCiteseer\nPubmed\nWikiCS\nFull\nRand\nPred\nFull\nRand\nPred\nFull\nRand\nPred\nUnsupervised\nIGT + MLP (ours)\n0.5\n0.8\n0.9\n0.4\n0.8\n0.7\n0.6\n0.5\n0.3\n0.5\nIGT + Lin. (ours)\n0.1\n0.8\n0.2\n0.3\n0.7\n0.5\n0.1\n0.2\n0.1\n0.5\n6.3\nCode and Data availability\nAll the code is accessible in the folder given in the supplementary materials.\n6.4\nTraining time\nWe informally noticed that the training of our isometry layers converges quickly. During the\nsupervised training, no multiplication with the adjacency matrix is involved, which can speed up the\ntraining compared to GCNs. We further report wall-clock training time in seconds until convergence\nfor our method and for GCNs. For the latter, we used an implementation provided by the authors\nand trained on the same hardware (with GPU) as our IGT model. For Cora, Citeseer and PubMed\nrespectively, the training time of our IGT layers was 0.45s, 0.57s and 4.88s, whereas the training time\nof the classiﬁcation head was 0.25s, 0.24s and 0.94s. By way of comparison, GCN training time was\n0.86s, 1.82s, and 1.12s. We would like to highlight that our code works on limited resources and we\nused a total of 10 GPU hours for developing and benchmarking this project.\n16\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-06-04",
  "updated": "2021-06-04"
}