{
  "id": "http://arxiv.org/abs/2106.06097v4",
  "title": "Neural Optimization Kernel: Towards Robust Deep Learning",
  "authors": [
    "Yueming Lyu",
    "Ivor Tsang"
  ],
  "abstract": "Deep neural networks (NN) have achieved great success in many applications.\nHowever, why do deep neural networks obtain good generalization at an\nover-parameterization regime is still unclear. To better understand deep NN, we\nestablish the connection between deep NN and a novel kernel family, i.e.,\nNeural Optimization Kernel (NOK). The architecture of structured approximation\nof NOK performs monotonic descent updates of implicit regularization problems.\nWe can implicitly choose the regularization problems by employing different\nactivation functions, e.g., ReLU, max pooling, and soft-thresholding. We\nfurther establish a new generalization bound of our deep structured\napproximated NOK architecture. Our unsupervised structured approximated NOK\nblock can serve as a simple plug-in of popular backbones for a good\ngeneralization against input noise.",
  "text": "Neural Optimization Kernel: Towards Robust Deep Learning\nYueming Lyu\nIvor Tsang\nAustralian Artiﬁcial Intelligence Institute\nUniversity of Technology Sydney\nAustralian Artiﬁcial Intelligence Institute\nUniversity of Technology Sydney\nAbstract\nDeep neural networks (NN) have achieved\ngreat success in many applications. However,\nwhy do deep neural networks obtain good\ngeneralization at an over-parameterization\nregime is still unclear. To better understand\ndeep NN, we establish the connection be-\ntween deep NN and a novel kernel family,\ni.e., Neural Optimization Kernel (NOK). The\narchitecture of structured approximation of\nNOK performs monotonic descent updates of\nimplicit regularization problems. We can im-\nplicitly choose the regularization problems by\nemploying diﬀerent activation functions, e.g.,\nReLU, max pooling, and soft-thresholding.\nWe further establish a new generalization\nbound of our deep structured approximated\nNOK architecture. Our unsupervised struc-\ntured approximated NOK block can serve as\na simple plug-in of popular backbones for a\ngood generalization against input noise.\n1\nIntroduction\nDeep neural networks (DNNs) have obtained great\nsuccess in many applications, including computer vi-\nsion [19], reinforcement learning [31] and natural lan-\nguage processing [37], etc. However, the theory of deep\nlearning is much less explored compared with its great\nempirical success.\nA key challenge of deep learning\ntheory is that deep neural networks are heavily over-\nparameterized. Namely, the number of parameters is\nmuch larger than training samples. In practice, as the\ndepth and width increasing, the performance of deep\nNN also becomes better [36, 41], which is far beyond\nthe traditional learning theory regime.\nPreliminary work.\nCorrespondence to:\nYueming Lyu\n(yueminglyu@gmail.com)\nIn the traditional neural networks and kernel meth-\nods literature, it is well known the connection between\nthe inﬁnite width neural networks and Gaussian pro-\ncess [21], and the universal approximation power of\nNN [27]. However, these theories cannot explain why\nthe success of deep neural networks. A recent work,\nNeural Tangent Kernel [23] (NTK), shows the connec-\ntion between training an inﬁnite-width NN and per-\nforming functional gradient descent in a Reproduc-\ning Kernel Hilbert Space(RKHS) associated with the\nNTK. Because of the convexity of the functional op-\ntimization problem, Jacot et al. show the global con-\nvergence for inﬁnite-width NN under the NTK regime.\nAlong this direction, Hanin et al. [17] analyze the NTK\nwith ﬁnite width and depth. Shankar et al. [35] empir-\nically investigate the performance of some simple com-\npositional kernels, NTKs, and deep neural networks.\nNitanda et al. [32] further show the minimax optimal\nconvergence rate of average stochastic gradient descent\nin a two-layer NTK regime.\nDespite the success of NTK [23] on showing the global\nconvergence of NN, its expressive power is limited. Zhu\net al. [1] provide an example that shallow kernel meth-\nods (including NTK) need a much larger number of\ntraining samples to achieve the same small population\nrisk compared with a three-layer ResNet. They fur-\nther point out the importance of hierarchical learn-\ning in deep neural networks [2].\nIn [2], they give\nthe theoretical analysis of learning a target network\nfamily with square activation function under deep NN\nregime. Besides, there are quite a few works focus on\nthe analysis of two-layer networks [4, 6, 24, 28, 11, 39]\nand shallow kernel methods without hierarchical learn-\ning [3, 14, 44, 8, 26].\nAlthough some particular examples show deep mod-\nels have more powerful expressive power than shallow\nones [1, 12, 2], how and why deep neural networks ben-\neﬁt from the depth remain unclear. Zhu et al. [2] high-\nlight the importance of a backward feature correction.\nTo better under deep neural networks, we investigate\nthe deep NN from a diﬀerent kernel method perspec-\ntive.\narXiv:2106.06097v4  [stat.ML]  1 Dec 2021\nNeural Optimization Kernel: Towards Robust Deep Learning\nOur contributions are summarized as follows:\n• We propose a novel Neural Optimization Kernel\n(NOK) family that broadens the connection be-\ntween kernel methods and deep neural networks.\n• Theoretically, we show that the architecture of\nNOK performs optimization of implicit regular-\nization problems.\nWe prove the monotonic de-\nscent property for a wide range of both convex\nand non-convex regularized problems. Moreover,\nwe prove a O(1/T) convergence rate for convex\nregularized problems. Namely, our NOK family\nperforms an optimization through model architec-\nture. A T-layer model performs T-step monotonic\ndescent updates.\n• We propose a novel data-dependent structured\napproximation method,\nwhich establishes the\nconnection between training deep neural net-\nworks and kernel methods associated with NOKs.\nThe resultant computation graph is a ResNet-\ntype ﬁnite width NN. The activation function of\nNN speciﬁes the regularization problem explic-\nitly or implicitly. Our structured approximation\npreserved the monotonic descent property and\nO(1/T) convergence rate. Furthermore, we pro-\npose both supervised and unsupervised learning\nschemes. Moreover, we prove a new Rademacher\ncomplexity bound and generalization bound of\nour structured approximated NOK architecture.\n• Empirically, we show that our unsupervised data-\ndependent structured approximation block can\nserve as a simple plug-in of popular backbones\nfor a good generalization against input noise. Ex-\ntensive experiments on CIFAR10 and CIFAR100\nwith ResNet and DenseNet backbones show the\ngood generalization of our structured approxi-\nmated NOK against the Gaussian noise, Laplace\nnoise, and FGSM adversarial attack [16].\n2\nNeural Optimization Kernel\nDenote L2 as the Gaussian square-integrable function\nspace, i.e., L2 := {f\n\f\fEw∼N(0,Id)[f(w)2] < ∞}, and\ndenote L2 as the spherically square-integrable function\nspace, i.e., L2 := {f\n\f\fEw∼Uni[\n√\ndSd−1][f(w)2] < ∞}.\nDenote F = L2 or F = L2, f(·, x) ∈F is a func-\ntion indexed by x. We simplify the notation f(w, x)\nas f(w) when the dependence of x is clear from the\ncontext.\nFor ∀f(·, x), f(·, y) ∈F, where F = L2 or F = L2,\ndeﬁne function k(·, ·) : X × X →R as\nk(x, y) = Ew[f(w, x)f(w, y)].\n(1)\nThen, we know k(·, ·) is a bounded kernel, which is\nshown in Proposition 1. All detailed proofs are given\nin Appendix.\nProposition 1. For ∀f(·, x), f(·, y) ∈F ( F\n=\nL2\nor\nF\n=\nL2),\ndeﬁne\nfunction\nk(x, y)\n=\nEw[f(w, x)f(w, y)] : X × X →R, then k(x, y) is\na bounded kernel, i.e., k(x, y) = k(y, x) < ∞and\nk(x, y) is positive deﬁnite.\nFor ∀f ∈F, where F = L2 or F = L2, deﬁne\noperator A(·) : F →Rd as A(f) := Ew[wf(w)].\nDeﬁne operator A∗: Rd →F as A∗(x) = w⊤x,\nw ∼N(0, Id) or w ∼Uni[\n√\ndSd−1].\nWe know\nA ◦A∗(·) = Ew[ww⊤] = Id : Rd →Rd. Details are\nprovided in Appendix. Deﬁne operator Φλ(·) : F →R\nas Φλ(f) := Ew[φλ(f(w))], where φλ(·) is a func-\ntion with parameter λ and bounded from below, and\nEw[φλ(f(w))] exists for some f ∈F. Several exam-\nples of φλ and the corresponding proximal operators\nare shown in Table 1. It is worth noting that φλ(·) can\nbe either convex or non-convex.\nOur Neural Optimization Kernel (NOK) is deﬁned\nupon the solution of optimization problems.\nBefore\ngiving our Neural Optimization Kernel (NOK) deﬁ-\nnition, we ﬁrst introduce a family of functional opti-\nmization problems. The Φλ-regularized optimization\nproblem is deﬁned as\nmin\nf∈F\n1\n2∥x −A(f)∥2\n2 + Φλ(f)\n= 1\n2∥x −Ew[wf(w)]∥2\n2 + Ew[φλ(f(w))],\n(2)\nwhere F = L2 or F = L2. f(·) := f(·, x) is a function\nindexed by x.\nWe simplify the notation f(w, x) as\nf(w) as the dependence of x is clear from the context.\nIntuition: The reconstruction problem in Eq.(2) can\nbe viewed as an autoencoder. We ﬁnd a function em-\nbedding f(·, x) to represent the input data x. In con-\ntrast, the standard autoencoders usually extract ﬁnite-\ndimensional vector features to represent the input data\nfor downstream tasks [15, 18]. Function representation\nmay encode richer information than ﬁnite-dimensional\nvector features.\nFor φλ(·) with eﬃcient proximal operators h(·) deﬁned\nas h(z) = arg minx\n1\n2(x −z)2 + φλ(x), we can optimize\nthe problem (2) by iterative updating with Eq.(3):\nft+1(·) = h\n\u0000A∗(x) + ft(·) −A∗◦A(ft(·))\n\u0001\n.\n(3)\nThe initialization is f0(·) = 0.\nRemark: In the update rule (3), the term −A∗◦\nA(ft(·)) can be viewed as a two-layer transformed\nresidual modular of ft(·).\nThen adding a skip con-\nnection ft(·) and a biased term A∗(x).\nAs shown\nYueming Lyu, Ivor Tsang\nTable 1: Regularizers and Proximal Operators\nl0-norm [10]\nl1-norm [9]\nMCP [42]\nφλ(z)\nλ∥z∥0\nλ∥z∥1\nλ\nR |z|\n0\nmax(0, 1 −x/(γλ)) dx\nh(z)\nh(z)=\n\u001a z,\n|z| ≥\n√\n2λ\n0,\n|z| <\n√\n2λ .\nh(z)=sign(z) max(0, |z|−λ)\nh(z)=\n\n\n\nz,\n|z| > γλ\nsign(z)(|z|−λ)\n1−1/γ\n, λ < |z| ≤γλ\n0,\n|z| ≤λ\n.\nCapped l1-norm [43]\nSCAD [13]\nMCP0 [33]\nφλ(z)\nλ min(|z|, γ)\nλ\nR |z|\n0\nmin(1, max(0,γλ−z)\n(γ−1)λ\n) dx , (γ > 2)\nφλ(z)= 1\n2(λ−max(\n√\nλ−|z|, 0)2)\nh(z)\nh(z)=\n\u001a x1, q(x1) ≤q(x2)\nx2, q(x1) > q(x2) , where\nx1 = sign(z) max(|z|, γ)\nx2 = sign(z) min(γ, max(0, |z| −λ))\nq(x) = 0.5(x −z)2 + λ min(|x|, γ)\nh(z)=\n\n\n\nz,\n|z| > γλ\n(γ−1)z−sign(z)γλ\nγ−2\n, 2λ < |z| ≤γλ\nsign(z) max(|z| −λ, 0),\n|z| ≤2λ\nh(z)=\n\n\n\nz,\n|z| >\n√\nλ\nβ\n√\nλ\n(|β| ≤1), |z| =\n√\nλ\n0,\n|z| <\n√\nλ\n.\nin [1, 2], a ResNet-type architecture (residual mod-\nular with skip connections) is crucial for obtaining a\nsmall error with sample and time eﬃciency.\nFor both convex and non-convex function φλ, our up-\ndate rule in Eq.(3) leads to a monotonic descent.\nTheorem 1. (Monotonic Descent) For a function\nφλ(·), denote h(·) as the proximal operator of φλ(·).\nSuppose |h(x)| ≤c|x| (or |h(x)| ≤c), 0 < c < ∞(e.g.,\nhard thresholding function). Given a bouned x ∈Rd,\nset function ft+1(·) = h\n\u0000A∗(x) + ft(·) −A∗◦A(ft(·))\n\u0001\nand f0 ∈F (e.g., f0 = 0). Denote Q(f) =\n1\n2∥x −\nA(f)∥2\n2 + Φλ(f). For ∀t ≥0, we have\nQ(ft+1)≤Q(ft)\n−1\n2Ew[(ft+1(w)−ft(w)−w⊤Ew[w(ft+1(w)−ft(w))])2]\n≤Q(ft)\n(4)\nRemark: Assumption|h(x)| ≤c|x| (or |h(x)| ≤c)\nis used to ensure that each ft ∈F. Neural networks\nwith a activation function h(·), e.g., sigmoid, tanh, and\nReLU, as long as h(·) satisﬁes the above assumption,\nit corresponds to a (implicit) φ(·)-regularized prob-\nlem.\nTheorem 1 shows that a T-layer network per-\nforms T-steps monotonic descent updates of the φ(·)-\nregularized objective Q(·).\nFor a convex φλ, we can achieve a O( 1\nT ) convergence\nrate, which is formally shown in Theorem 2.\nTheorem 2. For a convex function φλ(·), denote h(·)\nas the proximal operator of φλ(·). Suppose |h(x)| ≤\nc|x| (or |h(x)| ≤c), 0 < c < ∞.\nGiven a bouned\nx ∈Rd, set function ft+1(·) = h\n\u0000A∗(x) + ft(·) −A∗◦\nA(ft(·))\n\u0001\nand f0 ∈F (e.g., f0 = 0). Denote Q(f) =\n1\n2∥x −A(f)∥2\n2 + Φλ(f) and f∗∈F as an optimal of\nQ(·). For ∀T ≥1, we have\nT\n\u0000Q(fT ) −Q(f∗)\n\u0001\n≤1\n2Ew[(f0(w)−f∗(w))2]−1\n2Ew[(fT (w)−f∗(w))2]\n−1\n2\nT −1\nX\nt=0\n∥Ew[w\n\u0000ft(w)−f∗(w)\n\u0001\n]∥2\n2\n−1\n2\nT −1\nX\nt=0\n(t + 1)Ew[(ft+1(w)−ft(w))2].\n(5)\nRemark: A ReLU h(z) = max(z, 0) corresponds a\nφλ(z) ==\n\u001a 0,\nz ≥0\n+∞, z < 0\n(a lower semi-continuous\nconvex function) , which results in a convex regular-\nization problem. A T-layer NN obtains O(1/T) con-\nvergence rate , which is faster than non-convex cases.\nThis explains the success of ReLU on training deep\nNN from a NN architecture optimization perspective.\nWhen ReLU and f0 = 0 is used, the resultant ﬁrst-\nlayer kernel is the arc-cosine kernel in [7]. More in-\nterestingly, when ReLU is used (related to indica-\ntor function φλ(·)), the learned function representa-\ntion p(w)ft(w) is an unnormalized non-negative mea-\nsure, where p(w) denotes the density of Gaussian or\nUniform sphere surface distribution. We can achieve\na probability measure representation by normalizing\np(w)ft(w) with Z =\nR\np(w)ft(w) dw.\nOur Neural Optimization Kernel (NOK) is de-\nﬁned upon the optimized function fT (T-layer) as\nkT,∞(x, y) := Ew[fT (w, x)fT (w, y)].\n(6)\nWith\n∀f0(·, x), f0(·, y)\n∈\nF,\nwe\nknow\nfT (·, x), fT (·, y)\n∈\nF.\nFrom the Proposition 1,\nwe know kT,∞(x, y) is a bounded kernel.\n3\nStructured Approximation\nThe orthogonal sampling [40] and spherically struc-\ntured sampling [29, 30] have been successfully used\nfor Gaussian and spherical integral approximation. In\nthe QMC area, randomization of structured points set\nis standard and widely used to achieve an unbiased\nestimator (the same marginal distribution p(w)). In\nthe hypercube domain [0, 1]d, a uniformly distributed\nvector shift is employed. In the hypersphere domain\nSd−1, a uniformly random rotation is used. For the\npurpose of acceleration, [29] employs a diagonal ran-\ndom rotation matrix to approximate the full matrix\nrotation, which results in a O(d) rotation time com-\nplexity instead of O(d3) complexity in computing SVD\nof random Gaussian matrix (full rotation). When the\ngoal is to reduce approximation error, we can use the\nstandard full matrix random orthogonal rotation of\nthe structured points [29] as an unbiased estimator of\nNeural Optimization Kernel: Towards Robust Deep Learning\nintegral on Uni[Sd−1]. Moreover, we propose a new di-\nagonal rotation method that maintains the O(nlogn)\ntime complexity and O(d) space complexity by FFT\nas [29], which may of independent interest for integral\napproximation.\nFor all-layer trainable networks, we propose a data-\ndependent structured approximation as\nW =\n√\ndR⊤B ∈Rd×N,\n(7)\nwhere R⊤R = RR⊤= Id is a trainable orthogonal\nmatrix parameter, N denotes the number of samples,\nand W W ⊤/N = Id. The structured matrix B can\neither be a concatenate of random orthogonal matri-\nces [40], or be the structured matrix in [29, 30] to sat-\nisfy W W ⊤/N = Id.\nDeﬁne operator b\nA :=\n1\nN W\n: RN →Rd and b\nA∗:=\nW ⊤: Rd →RN. Operator b\nA is an approximation of\nA by taking expectation over ﬁnite samples. Remark-\nably, by using our structured approximation, we have\nb\nA ◦b\nA∗= 1\nN W W ⊤= Id.\nRemark: The orthogonal property of the operator\nA ◦A∗= Id is vitally important to achieve O( 1\nT )\nconvergence rate with our update rule.\nIt leads to\na ResNet-type network architecture, which enables\na stable gradient ﬂow for training.\nWhen approx-\nimation with ﬁnite samples, standard Monte Carlo\nsampling does not maintain the orthogonal property,\nwhich degenerates the convergence. In contrast, our\nstructured approximation preserves the second order\nmoment E[ww⊤] = Id. Namely, our approximation\nmaintains the orthogonal property, i.e., b\nA ◦b\nA∗= Id .\nWith the orthogonal property, we can obtain the same\nconvergence rate (w.r.t. the approximation objective)\nwith our update rule. Moreover, for a k-sparse con-\nstrained problem, we prove the strictly monotonic de-\nscent property of our structured approximation when\nusing B in [29, 30].\n3.1\nConvergence Rate for Finite Dimensional\nApproximation Problem\nThe ﬁnite approximation of problem (2) is given as\nbQ(y):= 1\n2∥x −b\nA(y)∥2\n2+ 1\nN φλ(y) = 1\n2∥x −1\nN W y∥2\n2+ 1\nNφλ(y),\n(8)\nwhere y ∈RN and φλ(y) := PN\ni=1 φλ(yi).\nThe ﬁnite dimension update rule is given as :\nyt+1 = h\n\u0000W ⊤x + (I −1\nN W ⊤W )yt\n\u0001\n.\n(9)\nThanks to the structured W =\n√\ndR⊤B, we show\nthe monotonic descent property, convergence rate for\nconvex φλ, and a strictly monotonic descent for a k-\nsparse constrained problem.\nFor both convex and non-convex φλ, our update rule\nin Eq.(9) leads to a monotonic descent.\nTheorem 3. (Monotonic Descent) For a function\nφλ(·), denote h(·) as the proximal operator of φλ(·).\nGiven a bouned x ∈Rd, set yt+1 = h\n\u0000W ⊤x + (I −\n1\nN W ⊤W )yt\n\u0001\nwith\n1\nN W W ⊤= Id. Denote bQ(y) :=\n1\n2∥x −b\nA(y)∥2\n2 + 1\nN φλ(y). For t ≥0, we have\nbQ(yt+1)≤bQ(yt)−1\n2N ∥yt+1−yt∥2\n2+ 1\n2∥1\nN W (yt+1−yt)∥2\n2\n= bQ(yt) −1\n2N ∥(Id −1\nN W ⊤W )(yt+1 −yt)∥2\n2\n≤bQ(yt).\n(10)\nRemark: For the ﬁnite dimensional case, the mono-\ntonic descent property is preserved. For popular ac-\ntivation function, e.g., sigmoid, tanh and ReLU, it\ncorresponds to a ﬁnite dimensional (implicit) φ(·)-\nregularized problem. A T-layer NN performs T-steps\nmonotonic descent of the φ(·)-regularized problem bQ(·)\ndesipte of the non-convexity of the activation function\nh(·).\nInterestingly, by choosing diﬀerent activation\nfunctions, we implicitly choose the regularizations of\nthe optimization problem. Our network structure can\nperform monotonic descent for a wide range of implicit\noptimization problems.\nFor convex φλ, we can achieve a O( 1\nT ) convergence\nrate, which is formally shown in Theorem 4.\nTheorem 4. For a convex function φλ(·), denote h(·)\nas the proximal operator of φλ(·). Given a bouned x ∈\nRd, set yt+1 = h\n\u0000W ⊤x + (I −\n1\nN W ⊤W )yt\n\u0001\nwith\n1\nN W W ⊤= Id.\nDenote bQ(y) :=\n1\n2∥x −b\nA(y)∥2\n2 +\n1\nN φλ(y) and y∗as an optimal of bQ(·). For T ≥1, we\nhave\nT\n\u0000 bQ(yT ) −bQ(y∗)\n\u0001\n≤1\n2N∥y0−y∗∥2\n2−1\n2N∥yT −y∗∥2\n2−1\n2\nT −1\nX\nt=0\n∥1\nN W (yt−y∗)∥2\n2\n−1\n2\nT −1\nX\nt=0\nt + 1\nN\n∥yt+1 −yt∥2\n2.\n(11)\nRemark:\nTerm\n−1\n2\nPT −1\nt=0\nt+1\nN ∥yt+1 −yt∥2\n2,\n−1\n2\nPT −1\nt=0 ∥1\nN W (yt−y∗)∥2\n2, and term −1\n2N∥yT −y∗∥2\n2,\nare\nalways\nnon-positive.\nThus,\nwe\nknow\nbQ(yT ) −bQ(y∗) ≤O( 1\nT ).\nTheorem 5. (Strictly Monotonic Descent of k-sparse\nproblem) Let L(y) = 1\n2∥x −Dy∥2\n2, s.t. ∥y∥0 ≤k with\nD =\n√\nd\n√\nN R⊤B, where B is constructed as in [30] with\nYueming Lyu, Ivor Tsang\nN = 2n, d = 2m. Set yt+1 = h(at+1) with sparity k\nand at+1 = D⊤x + (I −D⊤D)yt. For ∀t ≥1, we\nhave\nL(yt+1) ≤L(yt) + 1\n2∥yt+1 −at+1∥2\n2 −1\n2∥yt −at+1∥2\n2\n−n −(2k −1)√n −m\n2n\n∥yt+1 −yt∥2\n2 ≤L(yt),\n(12)\nwhere h(·) is deﬁned as\nh(zj) =\n\u001azj\nif\n|zj| is one of the k-highest values of |z|\n0\notherwise\n(13)\nRemark:\nWhen sparsity k <\nn−m+√n\n2√n\n, we have\nL(yt+1) < L(yt) unless yt+1 = yt. Our update with\nthe structured D makes a strictly monotonic descent\nprogress each step.\n3.2\nLearning Parameter R\nSupervised Learning: For the each tth layer, we\ncan maintain an orthogonal matrix Rt. The orthog-\nonal matrix Rt can be parameterized by exponential\nmapping or Cayley mapping [20] of a skew-symmetric\nmatrix. We can employ the Cayley mapping to en-\nable gradient update w.r.t a loss function ℓ(·) in an\nend-to-end training. Speciﬁcally, the orthogonal ma-\ntrix Rt can be obtained by the Cayley mapping of a\nskew-symmetric matrix as\nRt = (I + M t)(I −M t)−1,\n(14)\nwhere M t is a skew-symmetric matrix, i.e., M t =\n−M ⊤\nt\n∈Rd×d.\nFor a skew-symmetric matrix M t,\nonly the upper triangular matrix (without main diag-\nonal) are free parameters. Thus, total the number of\nfree parameters of T-Layer is Td(d−1)/2. Particularly,\nwhen sharing the orthogonal matrix parameter,\ni.e., R1 = · · · = RT = R, the monotonic descent prop-\nerty and the convergence rate of the regularized opti-\nmization problems are well maintained. In this case,\nit is a recurrent neural network architecture.\nUnsupervised Learning: The parameter R can also\nbe learned in an unsupervised manner. Speciﬁcally, for\na ﬁnite dataset X, the ﬁnite dimensional approxima-\ntion problem with the structured W =\n√\ndR⊤B is\ngiven as\nmin\nY ,R\n1\n2∥X −\n√\nd\nN R⊤BY ∥2\nF + 1\nN φλ(Y )\n(15)\nsubject to R⊤R = RR⊤= Id,\nwhere φλ(·) is a separable non-convex or convex reg-\nularization function with parameter λ, i.e., φλ(Y ) =\nP\ni φλ(y(i)).\nThe problem (15) can be solved by the alternative de-\nscent method. For a ﬁxed R, we perform a iterative\nupdate of Y a few steps to decrease the objective. For\nthe ﬁxed Y , parameter R has a closed-form solution.\nFix R, Optimize Y : The problem (15) can be rewrit-\nten as :\n1\n2∥X −\n√\nd\nN R⊤BY ∥2\nF + 1\nN φλ(Y ) =\nX\ni\nbQ(y(i)).\n(16)\nThus, with ﬁxed R, we can update each y(i) by Eq.(9)\nin parallel. We can perform T1 steps update with ini-\ntialization as the output of previous alternative phase,\ni.e., Y j\n0 = Y j−1\nT1\n(and initialization Y 0\n0 = 0 and\nR0 = Id).\nFix Y , Optimize R: This is the nearest orthogonal\nmatrix problem, which has a closed-form solution as\nshown in [34]. Let\n√\nd\nN BY X⊤= UΓV ⊤obtained by\nsingular value decomposition (SVD), where U, V are\northgonal matrix. Then, Eq.(15) is minimized by R =\nUV ⊤.\nRemark: A T2-step alternative descent computation\ngraph of R and Y can be viewed as a T1T2-layer NN\nblock, which can be used as a plug-in of popular back-\nbones for a good generalization against input noise.\n3.3\nKernel Approximation\nDeﬁne kT,N(x, x′) =\n1\nN < yT (W , x), yT (W , x′) >,\nwhere yT (W , x) : Rd →RN is a ﬁnite approximation\nof fT (·, x) ∈HkT . We know kT,N(x, x′) is bounded\nkernel, and it is an approximation of kernel kT,∞=\nEw[fT (w, x)fT (w, x′)].\nRemark: Let B be a points set that marginally uni-\nformly distributed on the surface of sphere Sd−1 (e.g,\nblock-wise random orthogonal rotation of structured\nsamples [29]). Employing our structured approxima-\ntion W = R⊤B, we know ∀R ∈SO(d) and ∀f ∈L2,\nlimN→∞\nPN\ni=1 f(wi)\nN\n= Ew∼Uni[Sd−1][f(w)].\nIt means\nthat although the orthogonal rotation parameter R\nis learned, we still maintain an unbiased estimator of\nEw[f(w)].\nFirst-Layer Kernel: Set y = 0 and f0 = 0, we know\nyi(x) = h(w⊤\ni x) and f1(w, x) = h(w⊤x). Suppose\n|h(x)| ≤c|x| (or |h(x)| ≤c), 0 < c < ∞, it follows\nthat\nlim\nN→∞k1,N(x, x′) = lim\nN→∞\n1\nN\nN\nX\ni=1\nh(w⊤\ni x)h(w⊤\ni x′)\n= Ew[h(w⊤Rx)h(w⊤Rx′)]\n= Ew[h(w⊤x)h(w⊤x′)] = k1,∞(x, x′)\n(17)\nNeural Optimization Kernel: Towards Robust Deep Learning\nIn Eq.(17), we use the fact that a rotation does not\nchange the uniform surface measure on Sd−1. The ﬁrst\nlayer kernel k1,N uniformly converge to k1,∞over a\nbounded domain X × X.\nHigher-Layer Kernel: For both the shared R case\nand the unsupervised updating R case, the mono-\ntonic descent property and convergence rate is well\npreserved for any bounded x ∈X.\nWith the same\nassumption of h(·) and y = 0, as N →∞, we know\nyt →bft ∈L2 , where bft is a countable-inﬁnite di-\nmensional function. And inequality (10) and inequal-\nity (11) uniformly converges to inequality (18) and in-\nequality (19) over a bounded domain X, respectively.\nQ( bft+1)≤Q( bft)−1\n2Ew[\n\u0000 bft+1(w)−bft(w)\n\u00012]+ 1\n2∥Ew[w\n\u0000 bft+1(w)−bft(w)\n\u0001\n]∥2\n2\n= Q( bft) −1\n2Ew[( bft+1(w)−bft(w) −w⊤Ew[w( bft+1(w)−bft(w))])2]\n≤Q( bft).\n(18)\nT\n\u0000Q( bfT)−Q(f∗)\n\u0001\n≤1\n2Ew[(f0(w)−f∗(w))2]−1\n2Ew[( bfT (w)−f∗(w))2]\n−1\n2\nPT −1\nt=0 ∥Ew[w\n\u0000 bft(w)−f∗(w)\n\u0001\n]∥2\n2−1\n2\nPT −1\nt=0 (t+1)Ew[( bft+1(w)−bft(w))2]\n(19)\nIt is worth noting that limN→∞kT,N converge to a\nbkT,∞that is determined by the initialization R0 and\ndataset X.\nSpeciﬁcally, for both the unsupervised\nlearning case and the shared parameter case, the ap-\nproximated kernel converge to a ﬁxed kernel as the\nwidth tends to inﬁnity. As N →∞, training a ﬁnite\nstructured NN with GD tends to perform a functional\ngradient descent with a ﬁxed kernel. For a strongly\nconvex regularized regression problem, functional gra-\ndient descent leads to global convergence.\nFor the case of updating T-layer parameter Rt, t ∈\n{1, · · · , T} in a supervised manner, the sequence {Rt}\ndetermines the kernel. When the data distribution is\nisotropic, e.g., Uni[Sd−1], the monotonic descent prop-\nerty is preserved for the expectation EX[Q( bft, X)] (at\nleast one step descent).\nActually, when parameters\nof each layer are learned in a supervised manner, the\nmodel is adjusted to ﬁt the supervised signal. When\nthe prior regularization Ew[φλ( bf(w, X))] is consistent\nwith learning the supervised signal, the monotonic de-\nscent property is well preserved. When the prior regu-\nlarization contradicts the supervised signal, the mono-\ntonic descent property for prior is weakened.\n4\nFunctional Optimization\nWe can minimize a regularized expected risk given as\nJ(f) := EX,Y [ℓ(g(X)), Y ]+ λ\n2 ∥g∥2\nHk\n|\n{z\n}\nJ1\n+ β EX\n\u00021\n2∥X−Ew[wf(w, X)]∥2\n2 + Ew[φλ(f(w, X))]\n\u0003\n|\n{z\n}\nJ2\n,\n(20)\nwhere the function space Hk ∋g is is determined\nby the kernel k(x, y) = Ew[f(w, x)f(w, y)]. J2 can\nbe viewed as an implicit regularization to determine\nthe candidate function family for supervised learning.\nOur NOK enables us to implicitly optimize the objec-\ntive J2 through neural network architecture. Namely,\nthe function space HkT ∋g is determined by the ker-\nnel associated with the T-step update fT . With our\nNOK, J2 with convex regularization φλ(·) can be op-\ntimized with a convergence rate O( 1\nT ) by the T-layer\nnetwork architecture. When φλ(·) is an indicator func-\ntion, the optimal J2 actually is the l2-norm optimal\ntransport between p(X) and a probability measure in-\nduced by the transform of random variable X (i.e.,\nEw[wf ∗\nX(w, X)]).\nBy employing diﬀerent activation\nfunction h(·), we implicitly choose the regularization\nterm J2 to be optimized.\nFor a convex function ℓ(·), J1(g) is strongly convex\nw.r.t the function g ∈Hk.\nFunctional gradient de-\nscent can converge to a minimizer of J1. For regression\nproblems, ℓ(z, y) = 1\n2(z −y)2, the functional gradient\nof J1 is\n∂J1(g) = EX,Y [∂z=g(X)ℓ(g(X), Y )k(·, X)] + λg\n= (Σ + λI)g −EX,Y [Y k(·, X)],\n(21)\nwhere Σ := EX∼pX[k(·, X) ⊗H k(X, ·)] denotes the co-\nvariance operator.\nWe can perform the average stochastic gradient de-\nscent using a stochastic unbiased estimator of Eq.(21).\nSince it is a strongly convex problem, we can achieve\nO( 1\nT ) convergence rate (Theorem A in [32]). It means\nthat training deep NN (with our structured approxi-\nmated NOK architecture) at the inﬁnity width regime\nconverges to a global minimum.\n5\nRademacher Complexity and\nGeneralization Bound\nWe show the Rademacher complexity bound and the\ngeneralization bound of our structured approximated\nNOK (SNOK).\nNeural Network Structure: For structured approx-\nimated NOK networks (SNOK), the 1-T layers are\ngiven as\nyt+1 = h(D⊤Rtx + (I −D⊤D)yt),\n(22)\nwhere Rt are free parameters such that R⊤\nt Rt =\nR⊤\nt Rt = Id.\nAnd D is a scaled structured spheri-\ncal samples such that DD⊤= Id [29], and y0 = 0.\nThe last layer ( (T +1)th layer) is given by z =\nw⊤yT+1. Consider a L-Lipschitz continuous loss func-\nYueming Lyu, Ivor Tsang\ntion ℓ(z, y) : Z × Y →[0, 1] with Lipschitz constant L\nw.r.t the input z.\nRademacher Complexity [5]: Rademacher com-\nplexity of a function class G is deﬁned as\nRN(G) := 1\nN E\n\"\nsup\ng∈G\nN\nX\ni=1\nϵig(xi)\n#\n,\n(23)\nwhere ϵi, i ∈{1, · · · , N} are i.i.d. samples drawn uni-\nformly from {+1, −1} with probality P[ϵi = +1] =\nP[ϵi = −1] = 1/2. And xi, i ∈{1, · · · , N} are i.i.d.\nsamples from X.\nTheorem 6. (Rademacher Complexity Bound) Con-\nsider a Lipschitz continuous loss function ℓ(z, y) :\nZ×Y →[0, 1] with Lipschitz constant L w.r.t the input\nz. Let eℓ(z, y) := ℓ(z, y)−ℓ(0, y). Let bG be the function\nclass of our (T+1)-layer SNOK mapping from X to Z.\nSuppose the activation function |h(y)| ≤|y| (element-\nwise), and the l2-norm of last layer weight is bounded,\ni.e., ∥w∥2 ≤Bw.\nLet (xi, yi)N\ni=1 be i.i.d.\nsamples\ndrawn from X × Y. Let Y T+1 = [y(1)\nT+1, · · · , y(N)\nT+1] be\nthe T th layer output with input X. Denote the mu-\ntual coherence of Y T+1 as µ∗, i.e., µ∗= µ(Y T+1) ≤1.\nThen, we have\nRN(eℓ◦bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵieℓ(g(xi), yi)\n#\n≤\nLBw\nq\u0000(N −1)µ∗+ 1\n\u0001\nT\nN\n∥X∥F ,\n(24)\nwhere X = [x1,· · ·, xN], and ∥· ∥F and ∥· ∥2 de-\nnote the matrix Frobenius norm and matrix spectral\nnorm,respectively.\nRemark: A small mutual coherence µ(Y T+1) leads\nto a small Rademacher complexity bound. Moreover,\nthe Rademacher complexity bound has a complexity\nO(\n√\nT) w.r.t. the depth of NN (SNOK).\nTheorem 7. (Generalization Bound) Consider a Lip-\nschitz continuous loss function ℓ(z, y) : Z × Y →\n[0, 1] with Lipschitz constant L w.r.t the input z. Let\neℓ(z, y) := ℓ(z, y)−ℓ(0, y). Let bG be the function class of\nour (T+1)-layer SNOK mapping from X to Z. Suppose\nthe activation function |h(y)| ≤|y| (element-wise),\nand the l2-norm of last layer weight is bounded, i.e.,\n∥w∥2 ≤Bw. Let (xi, yi)N\ni=1 be i.i.d. samples drawn\nfrom X × Y. Let Y T+1 = [y(1)\nT+1, · · · , y(N)\nT+1] be the T th\nlayer output with input X. Denote the mutual coher-\nence of Y T+1 as µ∗, i.e., µ∗= µ(Y T+1) ≤1. Then,\nfor ∀N and ∀δ, 0 < δ < 1, with a probability at least\n1 −δ, ∀g ∈bG, we have\nE\n\u0002\nℓ(g(X), Y )\n\u0003\n≤1\nN\nN\nX\ni=1\nℓ(g(xi), yi)\n+\nLBw\nq\u0000(N −1)µ∗+1\n\u0001\nT\nN\n∥X∥F +\nr\n8 ln(2/δ)\nN\n(25)\nwhere X = [x1,· · ·, xN], and ∥· ∥F and ∥· ∥2 de-\nnote the matrix Frobenius norm and matrix spectral\nnorm,respectively.\nRemark:\nThe\nmutual\ncoherence\nµ(Y T+1)\n(or\n∥Y ⊤\nT+1Y T+1−I∥2\nF , etc.)\nof the last layer represen-\ntation can serve as a good regularization to reduce\nRademacher complexity and generalization bound. A\nsmall mutual coherence means that the direction fea-\nture vectors (\nyi\nT +1\n∥yi\nT +1∥) are well spaced on the hyper-\nsphere.\nNamely, encouraging the last-layer embed-\nding direction feature vectors (\nyi\nT +1\n∥yi\nT +1∥) well spaced on\nthe hypersphere leads to small Rademacher complexity\nand generalization bounds. When the width of SNOK\n(ND) is large enough, speciﬁcally, when ND > N, it is\npossible to obtain µ(Y T+1) = 0 (orthogonal represen-\ntation), which signiﬁcantly reduces the generalization\nbound. Namely, overparameterized deep NNs can in-\ncrease the expressive power to reduce empirical risk\n[1, 12, 2] and reduce the generalization bound at the\nsame time.\n6\nExperiments\nWe evaluate the performance of our unsupervised\nSNOK blocks on classiﬁcation tasks with input noise\n(Gaussian noise or Laplace noise), and under FGSM\nadversarial attack [16]. In all the experiments, the in-\nput noise is added after input normalization. The stan-\ndard deviation of input noise is set to {0, 0.1, 0.2, 0.3},\nrespectively. We employ both DenseNet-100 [22] and\nResNet-34 [19] as backbone. We test the performance\nof four methods in comparison: (1) Vanilla Backbone,\n(2) Backbone + Mean Filter, (3) Backbone + Median\nFilter, (4) Backbone + SNOK. For both Mean Filter\nand Median Filter cases, we set the ﬁlter neighbor-\nhood size as 3 × 3 same as in [38].\nFor our SNOK\ncase, we plug two SNOK blocks before and after the\nﬁrst learnable Conv2D layer. In all the experiments,\nCIFAR10 and CIFAR100 datasets [25] are employed\nfor evaluation.\nAll the methods are evaluated over\nﬁve independent runs with seeds {1, 2, 3, 4, 5}. During\ntraining, we stored the model every ﬁve epochs, and\nreported all evaluation results over all the stored mod-\nNeural Optimization Kernel: Towards Robust Deep Learning\n(a) DenseNet-CIFAR10\n(b) DenseNet-CIFAR100\n(c) ResNet34-CIFAR10\n(d) ResNet34-CIFAR100\nFigure 1:\nMean test accuracy ± std over 5 independent runs on CIFAR10/CIFAR100 dataset under FGSM\nadversarial attack for DenseNet and ResNet backbone.\n(a) CIFAR10-Clean\n(b) CIFAR10-Gaussian-0.1\n(c) CIFAR10-Gaussian-0.2\n(d) CIFAR10-Gaussian-0.3\n(e) CIFAR100-Clean\n(f) CIFAR100-Gaussian-0.1\n(g) CIFAR100-Gaussian-0.2\n(h) CIFAR100-Gaussian-0.3\nFigure 2:\nMean test accuracy ± std over 5 independent runs on DenseNet with Gaussian noise.\nels. It covers the whole training trajectory, which is\nmore informative.\nThe experimental results of diﬀerent models under the\nFGSM attack are shown in Fig. 1. Our SNOK plug-\nin achieves a signiﬁcantly higher test accuracy than\nbaselines. The results of classiﬁcation with Gaussian\ninput noise on DenseNet backbone are shown in Fig. 2.\nOur SNOK obtains competitive performance on the\nclean case and increasingly better performance as the\nstd increases. More detailed experimental results are\npresented in Appendix K.\n7\nConclusion and Future Work\nWe proposed a novel kernel family NOK that broad-\nens the connection between deep neural networks and\nkernel methods. The architecture of our structured ap-\nproximated NOK performs monotonic descent updates\nof implicit regularization problems.\nWe can implic-\nitly choose the regularization problems by employing\ndiﬀerent activation functions, e.g., ReLU, max pool-\ning, and soft-thresholding.\nMoreover, by taking ad-\nvantage of the connection to kernel methods, we show\nthat training regularized NOK at an inﬁnite width\nregime with functional gradient descent converges to\na global minimizer. Furthermore, we establish gener-\nalization bounds of our SNOK. We show that increas-\ning the width of SNOK can increase the expressive\npower to reduce the empirical risk and potentially re-\nduce the generalization bound simultaneously through\nlast-layer feature mutual coherence regularization (i.e.,\nµ(Y T +1)).\nIn particular, when the width of SNOK\nis larger than the number of training data, last-layer\northogonal representation can signiﬁcantly reduce the\ngeneralization bound.\nOur unsupervised structured\napproximated NOK block can serve as a simple plug-in\nof popular backbones for a good generalization against\ninput noise. Extensive experiments on CIFAR10 and\nCIFAR100 with ResNet and DenseNet backbones show\nthe good generalization of our structured approxi-\nmated NOK against the Gaussian noise, Laplace noise,\nand FGSM adversarial attack.\nIn the future, we will investigate the convergence be-\nhavior of training the supervised SNOK with SGD at a\nﬁnite width regime. More interestingly, we will investi-\ngate our SNOK with shared parameter R as recurrent\nneural network architectures.\nYueming Lyu, Ivor Tsang\nReferences\n[1] Zeyuan Allen-Zhu and Yuanzhi Li.\nWhat can\nresnet learn eﬃciently, going beyond kernels?\narXiv preprint arXiv:1905.10337, 2019.\n[2] Zeyuan Allen-Zhu and Yuanzhi Li. Backward fea-\nture correction: How deep learning performs deep\nlearning. arXiv preprint arXiv:2001.04413, 2020.\n[3] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li,\nRuslan Salakhutdinov, and Ruosong Wang. On\nexact computation with an inﬁnitely wide neural\nnet. arXiv preprint arXiv:1904.11955, 2019.\n[4] Ainesh Bakshi, Rajesh Jayaram, and David P\nWoodruﬀ.\nLearning two layer rectiﬁed neural\nnetworks in polynomial time. In Conference on\nLearning Theory, pages 195–268. PMLR, 2019.\n[5] Peter\nL\nBartlett\nand\nShahar\nMendelson.\nRademacher and gaussian complexities:\nRisk\nbounds\nand\nstructural\nresults.\nJournal\nof\nMachine\nLearning\nResearch,\n3(Nov):463–482,\n2002.\n[6] Digvijay Boob and Guanghui Lan.\nTheoretical\nproperties of the global optimizer of two layer\nneural network. arXiv preprint arXiv:1710.11241,\n2017.\n[7] Youngmin Cho and Lawrence K. Saul.\nKernel\nmethods for deep learning. In NIPS, 2009.\n[8] Amit Daniely, Roy Frostig, and Yoram Singer.\nToward deeper understanding of neural networks:\nThe power of initialization and a dual view on\nexpressivity.\narXiv preprint arXiv:1602.05897,\n2016.\n[9] David L Donoho, Iain M Johnstone, Jeﬀrey C\nHoch, and Alan S Stern.\nMaximum entropy\nand the nearly black object.\nJournal of the\nRoyal Statistical Society: Series B (Methodolog-\nical), 54(1):41–67, 1992.\n[10] David L Donoho and Jain M Johnstone. Ideal spa-\ntial adaptation by wavelet shrinkage. biometrika,\n81(3):425–455, 1994.\n[11] Simon S Du, Xiyu Zhai, Barnabas Poczos, and\nAarti Singh.\nGradient descent provably opti-\nmizes over-parameterized neural networks. arXiv\npreprint arXiv:1810.02054, 2018.\n[12] Ronen Eldan and Ohad Shamir.\nThe power of\ndepth for feedforward neural networks. In Con-\nference on learning theory, pages 907–940. PMLR,\n2016.\n[13] Jianqing Fan and Runze Li.\nVariable selection\nvia nonconcave penalized likelihood and its ora-\ncle properties. Journal of the American statistical\nAssociation, 96(456):1348–1360, 2001.\n[14] Behrooz Ghorbani, Song Mei, Theodor Misi-\nakiewicz, and Andrea Montanari. Linearized two-\nlayers neural networks in high dimension.\nThe\nAnnals of Statistics, 49(2):1029–1054, 2021.\n[15] Ian Goodfellow,\nYoshua Bengio,\nand Aaron\nCourville. Deep learning. MIT press, 2016.\n[16] Ian J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. Explaining and harnessing adversarial\nexamples. ICLR, 2015.\n[17] Boris Hanin and Mihai Nica. Finite depth and\nwidth corrections to the neural tangent kernel.\narXiv preprint arXiv:1909.05989, 2019.\n[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao\nLi, Piotr Doll´ar, and Ross Girshick. Masked au-\ntoencoders are scalable vision learners.\narXiv\npreprint arXiv:2111.06377, 2021.\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image recog-\nnition.\nIn Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n770–778, 2016.\n[20] Kyle Helfrich, Devin Willmott, and Qiang Ye. Or-\nthogonal recurrent neural networks with scaled\ncayley transform.\nIn International Conference\non Machine Learning, pages 1969–1978. PMLR,\n2018.\n[21] Kurt Hornik, Maxwell Stinchcombe, and Halbert\nWhite. Multilayer feedforward networks are uni-\nversal approximators. Neural networks, 2(5):359–\n366, 1989.\n[22] Gao\nHuang,\nZhuang\nLiu,\nLaurens\nVan\nDer Maaten, and Kilian Q Weinberger. Densely\nconnected convolutional networks. In Proceedings\nof the IEEE conference on computer vision and\npattern recognition, pages 4700–4708, 2017.\n[23] Arthur Jacot, Franck Gabriel, and Cl´ement Hon-\ngler.\nNeural tangent kernel:\nConvergence and\ngeneralization in neural networks. arXiv preprint\narXiv:1806.07572, 2018.\n[24] Kenji Kawaguchi.\nDeep learning without poor\nlocal minima.\nAdvances in Neural Information\nProcessing Systems, 2016.\n[25] Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning\nmultiple layers of features from tiny images. 2009.\nNeural Optimization Kernel: Towards Robust Deep Learning\n[26] Jaehoon Lee, Lechao Xiao, Samuel S Schoen-\nholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeﬀrey Pennington. Wide neural\nnetworks of any depth evolve as linear models un-\nder gradient descent. Neural Information Process-\ning Systems, 2019.\n[27] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus,\nand Shimon Schocken.\nMultilayer feedforward\nnetworks with a nonpolynomial activation func-\ntion can approximate any function. Neural net-\nworks, 6(6):861–867, 1993.\n[28] Yuanzhi Li and Yingyu Liang. Learning overpa-\nrameterized neural networks via stochastic gradi-\nent descent on structured data.\narXiv preprint\narXiv:1808.01204, 2018.\n[29] Yueming Lyu. Spherical structured feature maps\nfor kernel approximation. In International Con-\nference on Machine Learning, pages 2256–2264,\n2017.\n[30] Yueming Lyu, Yuan Yuan, and Ivor W Tsang.\nSubgroup-based rank-1 lattice quasi-monte carlo.\nIn NeurIPS, 2020.\n[31] Volodymyr Mnih, Koray Kavukcuoglu, David\nSilver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari\nwith deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[32] Atsushi Nitanda and Taiji Suzuki. Optimal rates\nfor averaged stochastic gradient descent under\nneural tangent kernel regime. ICLR, 2021.\n[33] Carl Olsson, Marcus Carlsson, Fredrik Andersson,\nand Viktor Larsson.\nNon-convex rank/sparsity\nregularization and local minima. In Proceedings of\nthe IEEE International Conference on Computer\nVision, pages 332–340, 2017.\n[34] Peter H Sch¨onemann. A generalized solution of\nthe orthogonal procrustes problem.\nPsychome-\ntrika, 31(1):1–10, 1966.\n[35] Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara\nFridovich-Keil, Jonathan Ragan-Kelley, Ludwig\nSchmidt, and Benjamin Recht.\nNeural kernels\nwithout tangents.\nIn International Conference\non Machine Learning, pages 8614–8623. PMLR,\n2020.\n[36] Rupesh Kumar Srivastava,\nKlaus Greﬀ,\nand\nJ¨urgen Schmidhuber.\nTraining very deep net-\nworks. NeurIPS, 2015.\n[37] Thomas Wolf, Lysandre Debut, Victor Sanh,\nJulien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R´emi Louf, Mor-\ngan Funtowicz, et al.\nHuggingface’s transform-\ners: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771, 2019.\n[38] Cihang Xie, Yuxin Wu, Laurens van der Maaten,\nAlan L Yuille, and Kaiming He. Feature denoising\nfor improving adversarial robustness. In Proceed-\nings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 501–509,\n2019.\n[39] Gilad Yehudai and Ohad Shamir.\nOn the\npower and limitations of random features for\nunderstanding neural networks.\narXiv preprint\narXiv:1904.00687, 2019.\n[40] Felix X Yu, Ananda Theertha Suresh, Krzysztof\nChoromanski, Daniel Holtmann-Rice, and San-\njiv Kumar. Orthogonal random features. arXiv\npreprint arXiv:1610.09072, 2016.\n[41] Sergey\nZagoruyko\nand\nNikos\nKomodakis.\nWide\nresidual\nnetworks.\narXiv\npreprint\narXiv:1605.07146, 2016.\n[42] Cun-Hui Zhang et al. Nearly unbiased variable\nselection under minimax concave penalty.\nThe\nAnnals of statistics, 38(2):894–942, 2010.\n[43] Tong Zhang. Analysis of multi-stage convex re-\nlaxation for sparse regularization. Journal of Ma-\nchine Learning Research, 11(3), 2010.\n[44] Difan Zou and Quanquan Gu. An improved anal-\nysis of training over-parameterized deep neural\nnetworks. Advances in Neural Information Pro-\ncessing Systems, 2019.\nYueming Lyu, Ivor Tsang\nA\nProof of Proposition 1\nKernel Property:\nProposition. For ∀f(·, x), f(·, y) ∈F ( F = L2 or F = L2), deﬁne function k(x, y) = Ew[f(w, x)f(w, y)] :\nX × X →R, then k(x, y) is a bounded kernel, i.e., k(x, y) = k(y, x) < ∞and k(x, y) is positive deﬁnite.\nProof. (i) Symmetric property is straightforward by deﬁnition.\n(ii) From Cauchy–Schwarz inequality,\nk(x, y) = Ew[f(w, x)f(w, y)] ≤\np\nEw[f(w, x)2]Ew[f(w, y)2] < ∞\n(26)\n(iii) Positive deﬁnite property. For ∀n ∈N, ∀α1 · · · , αn ∈R and ∀x1, · · · , xn ∈X, we have\nX\ni\nX\nj\nαiαjk(xi, xj) = Ew\n\u0002\u0000 X\ni\nαif(w, xi)\n\u00012\u0003\n≥0\nB\nProof of Theorem 2\nConvex φ-regularization:\nmin\nf∈F\n1\n2∥x −Ew[wf(w)]∥2\n2 + Ew[φλ(f(w))]\n(27)\nwhere F\n= L2 or F\n= L2.\nAnd L2 denotes the Gaussian square integrable functional space, i.e.,\nL2 := {f\n\f\fEw∼N(0,Id)[f(w)2] < ∞}, L2 denotes the sphere square integrable functional space, i.e., L2 :=\n{f\n\f\fEw∼Uni[\n√\ndSd−1][f(w)2] < ∞} and φλ(·) denotes a convex function bounded from below.\nLemma 1. Ew∼Uni[\n√\ndSd−1][ww⊤] = Id.\nProof.\nId = Ex∼N(0,Id)[xx⊤]\n=\nZ\n1\n(2π)\nd\n2 e−\n∥x∥2\n2\n2 xx⊤dx\n=\nZ ∞\n0\nZ\nSd−1\n2π\nd\n2\nΓ( d\n2)rd−1 · e−r2\n2 r2 ·\n1\n(2π)\nd\n2 vv⊤dσ(v) dr\n(28)\n=\nZ ∞\n0\n2π\nd\n2\nΓ( d\n2)rd−1e−r2\n2 r2 ·\n1\n(2π)\nd\n2 dr\nZ\nSd−1 vv⊤dσ(v)\n(29)\n=\nZ ∞\n0\nrd−1e−r2\n2\n2\nd\n2 −1Γ( d\n2)\nr2 dr\nZ\nSd−1 vv⊤dσ(v)\n(30)\n= Er∼χ(d)[r2]\nZ\nSd−1 vv⊤dσ(v)\n(31)\n= d\nZ\nSd−1 vv⊤dσ(v)\n(32)\n= Ew∼Uni[\n√\ndSd−1][ww⊤]\n(33)\nwhere σ(·) denotes the normalized surface measure, χ(d) denotes the Chi distribution with degree d, Γ(·) denotes\nthe gamma function.\nLemma 2. Let f ∈F with F = L2 or F = L2, then we have\nEw[f(w)2] −∥Ew[wf(w)]∥2\n2 = Ew[\n\u0000f(w) −w⊤Ew[wf(w)]\n\u00012] ≥0\n(34)\nNeural Optimization Kernel: Towards Robust Deep Learning\nProof. Let wi denote the ith component of w, from Cauchy–Schwarz inequality, we know that\n(Ew[wif(w)])2 ≤Ew[w2\ni ]Ew[f(w)2] = Ew[f(w)2] < ∞\n(35)\nThus the expectation Ew[wf(w)] exits.\nSince Ew[ww⊤] = Id, we have\n∥Ew[wf(w)]∥2\n2 = (Ew[wf(w)])⊤(Ew[wf(w)])\n(36)\n= (Ew[wf(w)])⊤Ew[ww⊤](Ew[wf(w)])\n(37)\n= Ew[(w⊤Ew[wf(w)])2]\n(38)\nIt follows that\nEw[f(w)2] −∥Ew[wf(w)]∥2\n2\n= Ew[f(w)2] −2∥Ew[wf(w)]∥2\n2 + ∥Ew[wf(w)]∥2\n2\n(39)\n= Ew[f(w)2] −2(Ew[wf(w)])⊤(Ew[wf(w)]) + Ew[(w⊤Ew[wf(w)])2]\n(40)\n= Ew[f(w)2] −2Ew[f(w)w⊤Ew[wf(w)]] + Ew[(w⊤Ew[wf(w)])2]\n(41)\n= Ew[\n\u0000f(w) −w⊤Ew[wf(w)]\n\u00012] ≥0\n(42)\nLemma 3. Let f ∈F with F = L2 or F = L2, we have\n∥x −Ew[wf(w)]∥2\n2 = Ew[(w⊤(x −Ew[wf(w)]))2]\n(43)\nProof. Since Ew[ww⊤] = Id, we have\n∥x −Ew[wf(w)]∥2\n2 = (x −Ew[wf(w)])⊤(x −Ew[wf(w)])\n(44)\n= (x −Ew[wf(w)])⊤Ew[ww⊤](x −Ew[wf(w)])\n(45)\n= Ew[(w⊤(x −Ew[wf(w)]))2]\n(46)\nLemma 4. Denote L(f) := 1\n2∥x −Ew[wf(w)]∥2\n2. For ∀f, g ∈F with F = L2 or F = L2, we have L(f) =\nL(g) + Ew[w⊤(Ew[wg(w)] −x)\n\u0000f(w) −g(w)\n\u0001\n] + 1\n2∥Ew[w\n\u0000f(w) −g(w)\n\u0001\n]∥2\n2.\nProof.\n1\n2∥x −Ew[wf(w)]∥2\n2 = 1\n2∥x −Ew[wg(w)] + Ew[wg(w)] −Ew[wf(w)]∥2\n2\n(47)\n= 1\n2∥x −Ew[wg(w)]∥2\n2 + 1\n2\n\r\rEw[wg(w)] −Ew[wf(w)]\n\r\r2\n2\n+ ⟨x −Ew[wg(w)], Ew[wg(w)] −Ew[wf(w)]⟩\n(48)\nThe inner product term can be rewritten as\n⟨x −Ew[wg(w)], Ew[wg(w)] −Ew[wf(w)]⟩= (x −Ew[wg(w)])⊤Ew[w\n\u0000g(w) −f(w)\n\u0001\n]\n(49)\n= Ew[(x −Ew[wg(w)])⊤w\n\u0000g(w) −f(w)\n\u0001\n]\n(50)\nIt follows that\n1\n2∥x −Ew[wf(w)]∥2\n2 = 1\n2∥x −Ew[wg(w)]∥2\n2 + 1\n2\n\r\rEw[w\n\u0000g(w) −f(w)\n\u0001\n]\n\r\r2\n2\n+ Ew[w⊤(Ew[wg(w)] −x)\n\u0000f(w) −g(w)\n\u0001\n]\n(51)\nYueming Lyu, Ivor Tsang\nLemma 5. For ∀ft, ft+1, f ∗∈F with F = L2 or F = L2, we have\nL(ft+1)=L(f ∗)+Ew[w⊤(Ew[wft(w)]−x)\n\u0000ft+1(w)−f ∗(w)\n\u0001\n]+1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n−1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(52)\nProof. From Lemma 4, we know that\nL(ft+1) = L(ft) + Ew[w⊤(Ew[wft(w)] −x)\n\u0000ft+1(w) −ft(w)\n\u0001\n] + 1\n2∥Ew[w\n\u0000ft+1(w) −ft(w)\n\u0001\n]∥2\n2\n(53)\nL(f ∗) = L(ft) + Ew[w⊤(Ew[wft(w)] −x)\n\u0000f ∗(w) −ft(w)\n\u0001\n] + 1\n2∥Ew[w\n\u0000f ∗(w) −ft(w)\n\u0001\n]∥2\n2\n(54)\nPlug L(ft) into Eq.(53), we can obtain that\nL(ft+1)=L(f ∗)−Ew[w⊤(Ew[wft(w)]−x)\n\u0000f ∗(w)−ft(w)\n\u0001\n] −1\n2∥Ew[w\n\u0000f ∗(w)−ft(w)\n\u0001\n]∥2\n2\n+Ew[w⊤(Ew[wft(w)]−x)\n\u0000ft+1(w)−ft(w)\n\u0001\n]+ 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n(55)\n=L(f ∗)+Ew[w⊤(Ew[wft(w)]−x)\n\u0000ft+1(w)−f ∗(w)\n\u0001\n]+1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n−1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(56)\nLemma 6. For a convex function φλ(·), denote h(·) as the proximal operator of φλ(·), i.e., h(z)\n=\narg minx\n1\n2(x −z)2 + φλ(x), let ft+1 = h ◦gt+1 ∈F with F = L2 or F = L2, then for ∀f ∗∈F, we have\nEw[φλ(ft+1(w))] ≤Ew[φλ(f ∗(w))] −Ew[\n\u0000gt+1(w) −ft+1(w)\n\u0001\u0000f ∗(w) −ft+1(w)\n\u0001\n]\n(57)\nProof. Since φλ(·) is convex function and ft+1(w) = arg minx φλ(x) + 1\n2∥x −gt+1(w)∥2\n2, we have\n0 ∈∂φλ(ft+1(w)) + (ft+1(w) −gt+1(w)) =⇒(gt+1(w) −ft+1(w)) ∈∂φλ(ft+1(w))\n(58)\nFrom the deﬁnition of subgradient and convex function φλ(·), we have\nφλ(ft+1(w)) ≤φλ(f ∗(w)) −(gt+1(w) −ft+1(w))(f ∗(w) −ft+1(w))\n(59)\nIt follows that\nEw[φλ(ft+1(w))] ≤Ew[φλ(f ∗(w))] −Ew[\n\u0000gt+1(w) −ft+1(w)\n\u0001\u0000f ∗(w) −ft+1(w)\n\u0001\n]\n(60)\nLemma 7. Denote h(·) as the proximal operator of φλ(·). Suppose |h(x)| ≤c|x| (or |h(x)| ≤c) , 0 < c < ∞\n. Given a bouned x ∈Rd, set function gt+1(w) = w⊤x + ft(w) −w⊤Ew[wft(w)] with ft ∈L2 and w ∼\nN(0, Id) (or ft ∈L2 and w ∼Uni[\n√\ndSd−1]). Set ft+1 = h ◦gt+1, then, we know ft+1 ∈F with F = L2 or\nF = L2,respectively.\nProof. Case |h(x)| ≤c, 0 < c < ∞: It is straightforward to know Ew[h(gt+1(w))2] ≤c2 < ∞, thus ft+1 ∈F.\nCase |h(x)| ≤c|x|, 0 < c < ∞: Since |h(x)| ≤c|x|, we know that\nh(gt+1(w))2 ≤c2gt+1(w)2 = c2\u0000w⊤x + ft(w) −w⊤Ew[wft(w)]\n\u00012\n(61)\n≤2c2(w⊤(x −Ew[wft(w)]))2 + 2c2ft(w)2\n(62)\nNeural Optimization Kernel: Towards Robust Deep Learning\nIt follows that\nEw[h(gt+1(w))2] ≤c2Ew[\n\u0000w⊤x + ft(w) −w⊤Ew[wft(w)]\n\u00012]\n(63)\n≤2c2Ew[(w⊤(x −Ew[wft(w)]))2] + 2c2Ew[ft(w)2]\n(64)\n= 2c2∥x −Ew[wft(w)]∥2\n2 + 2c2Ew[ft(w)2]\n(65)\n≤4c2∥x∥2\n2 + 4c2∥Ew[wft(w)]∥2\n2 + 2c2Ew[ft(w)2]\n(66)\nFrom Lemma 2, we know ∥Ew[wft(w)]∥2\n2 ≤Ew[ft(w)2] is bounded, together with ∥x∥2 < ∞,it follows that\nEw[ft+1(w))2] = Ew[h(gt+1(w))2] < ∞. Thus, ft+1 ∈F.\nLemma 8. For a convex function φλ(·), denote h(·) as the proximal operator of φλ(·), i.e., h(z)\n=\narg minx\n1\n2(x −z)2 + φλ(x). Suppose |h(x)| ≤c|x| (or |h(x)| ≤c), 0 < c < ∞(e.g., soft thresholding func-\ntion).\nGiven a bouned x ∈Rd, set function gt+1(w) = w⊤x + ft(w) −w⊤Ew[wft(w)] with ft ∈L2 and\nw ∼N(0, Id) (or ft ∈L2 and w ∼Uni[\n√\ndSd−1]). Set ft+1 = h ◦gt+1. Denote Q(f) = L(f) + Ew[φλ(f(w))]\nwith L(f) := 1\n2∥x −Ew[wf(w)]∥2\n2, for ∀f ∗∈F with F = L2 or F = L2, we have\nQ(ft+1) ≤Q(f ∗)+ 1\n2Ew[(ft(w)−f ∗(w))2]−1\n2Ew[(ft+1(w)−f ∗(w))2]\n−1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(67)\nProof. From Lemma 5, we know that\nL(ft+1)=L(f ∗)+Ew[w⊤(Ew[wft(w)]−x)\n\u0000ft+1(w)−f ∗(w)\n\u0001\n]+1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n−1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(68)\nTogether with Lemma 6, it follows that\nQ(ft+1)\n(69)\n≤Q(f ∗)−Ew[\n\u0000gt+1(w)−ft+1(w)\n\u0001\u0000f ∗(w)−ft+1(w)\n\u0001\n]+Ew[w⊤(Ew[wft(w)]−x)\n\u0000ft+1(w)−f ∗(w)\n\u0001\n]\n+ 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2 −1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(70)\n=Q(f ∗)+Ew[\n\u0000gt+1(w)−ft+1(w) + w⊤(Ew[wft(w)]−x)\n\u0001\u0000ft+1(w)−f ∗(w)\n\u0001\n]\n+ 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2 −1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(71)\n=Q(f ∗)+Ew[\n\u0000ft(w)−ft+1(w)\n\u0001\u0000ft+1(w)−f ∗(w)\n\u0001\n]\n+ 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2 −1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(72)\nNote that\nEw[\n\u0000ft(w)−ft+1(w)\n\u0001\u0000ft+1(w)−f ∗(w)\n\u0001\n]\n= Ew[\n\u0000ft(w)−ft+1(w)\n\u0001\u0000ft+1(w)−ft(w)+ft(w)−f ∗(w)\n\u0001\n]\n(73)\n= Ew[\n\u0000ft(w)−ft+1(w)\n\u0001\u0000ft(w)−f ∗(w)\n\u0001\n] −Ew[\n\u0000ft(w)−ft+1(w)\n\u00012]\n(74)\nAlso note that ab = a2+b2−(a−b)2\n2\n, it follows that\n\u0000ft(w)−ft+1(w)\n\u0001\u0000ft(w)−f ∗(w)\n\u0001\n= (ft(w)−ft+1(w))2 + (ft(w)−f ∗(w))2−(ft+1(w)−f ∗(w))2\n2\n(75)\nIt follows that\nEw[\n\u0000ft(w)−ft+1(w)\n\u0001\u0000ft+1(w)−f ∗(w)\n\u0001\n]\n= 1\n2Ew[(ft(w)−f ∗(w))2] −1\n2Ew[(ft+1(w)−f ∗(w))2] −1\n2Ew[(ft+1(w)−ft(w))2]\n(76)\nYueming Lyu, Ivor Tsang\nPlug Eq.(76) into Eq.(72), we can obtain that\nQ(ft+1) ≤Q(f ∗) + 1\n2Ew[(ft(w)−f ∗(w))2] −1\n2Ew[(ft+1(w)−f ∗(w))2] −1\n2Ew[(ft+1(w)−ft(w))2]\n+ 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2 −1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\n(77)\nFrom Lemma 2, we know ∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2 ≤Ew[(ft+1(w)−ft(w))2]. It follows that\nQ(ft+1) ≤Q(f ∗)+ 1\n2Ew[(ft(w)−f ∗(w))2]−1\n2Ew[(ft+1(w)−f ∗(w))2]\n(78)\n−1\n2∥Ew[w\n\u0000ft(w)−f ∗(w)\n\u0001\n]∥2\n2\nLemma 9. (Strictly Monotonic Descent (a.s.)) Following the same condition of Lemma 8, we have\nQ(ft+1) ≤Q(ft)−1\n2Ew[(ft+1(w)−ft(w))2]\n(79)\nProof. It follows directly from Lemma 8 by setting f ∗= ft.\nTheorem. For a convex function φλ(·), denote h(·) as the proximal operator of φλ(·), i.e., h(z)\n=\narg minx\n1\n2(x −z)2 + φλ(x).\nSuppose |h(x)| ≤c|x| (or |h(x)| ≤c), 0 < c < ∞.\nGiven a bouned x ∈Rd,\nset function gt+1(w) = w⊤x + ft(w) −w⊤Ew[wft(w)] with w ∼N(0, Id) (or w ∼Uni[\n√\ndSd−1]).\nSet\nft+1 = h ◦gt+1 and f0 ∈F with F = L2 or F = L2 (e.g., f0 = 0). Denote Q(f) = L(f) + Ew[φλ(f(w))]\nwith L(f) := 1\n2∥x −Ew[wf(w)]∥2\n2. Denote f∗∈F as an optimal of Q(·), we have\nT\n\u0000Q(fT ) −Q(f∗)\n\u0001\n≤1\n2Ew[(f0(w)−f∗(w))2]−1\n2Ew[(fT (w)−f∗(w))2]\n−1\n2\nT −1\nX\nt=0\n∥Ew[w\n\u0000ft(w)−f∗(w)\n\u0001\n]∥2\n2 −1\n2\nT −1\nX\nt=0\n(t + 1)Ew[(ft+1(w)−ft(w))2]\n(80)\nProof. From Lemma 8, by setting f ∗= f∗, we can obtain that\nQ(ft+1) ≤Q(f∗)+ 1\n2Ew[(ft(w)−f∗(w))2]−1\n2Ew[(ft+1(w)−f∗(w))2]\n−1\n2∥Ew[w\n\u0000ft(w)−f∗(w)\n\u0001\n]∥2\n2\n(81)\nTelescope the inequality (81) from t = 0 to t = T −1, we can obtain that\nT −1\nX\nt=0\nQ(ft+1) −TQ(f∗) ≤1\n2Ew[(f0(w)−f∗(w))2]−1\n2Ew[(fT (w)−f∗(w))2]\n−1\n2\nT −1\nX\nt=0\n∥Ew[w\n\u0000ft(w)−f∗(w)\n\u0001\n]∥2\n2\n(82)\nIn addition, from Lemma 9, we can obtain that\nQ(fT ) ≤Q(ft) −1\n2\nT −1\nX\ni=t\nEw[(fi+1(w)−fi(w))2]\n(83)\nIt follows that\nTQ(fT ) −TQ(f∗) ≤\nT −1\nX\nt=0\nQ(ft+1) −TQ(f∗) −1\n2\nT −1\nX\nt=0\nT −1\nX\ni=t\nEw[(fi+1(w)−fi(w))2]\n(84)\n=\nT −1\nX\nt=0\nQ(ft+1) −TQ(f∗) −1\n2\nT −1\nX\nt=0\n(t + 1)Ew[(ft+1(w)−ft(w))2]\n(85)\nNeural Optimization Kernel: Towards Robust Deep Learning\nPlug inequality (82) into inequality (85), we can obtain that\nTQ(fT ) −TQ(f∗) ≤1\n2Ew[(f0(w)−f∗(w))2]−1\n2Ew[(fT (w)−f∗(w))2]\n−1\n2\nT −1\nX\nt=0\n∥Ew[w\n\u0000ft(w)−f∗(w)\n\u0001\n]∥2\n2 −1\n2\nT −1\nX\nt=0\n(t + 1)Ew[(ft+1(w)−ft(w))2]\n(86)\nC\nProof of Theorem 1\nNon-convex φ-regularization:\nTheorem. For a (non-convex) regularization function φλ(·), denote h(·) as the proximal operator of φλ(·), i.e.,\nh(z) = arg minx\n1\n2(x −z)2 + φλ(x). Suppose |h(x)| ≤c|x| (or |h(x)| ≤c), 0 < c < ∞(e.g., hard thresholding\nfunction). Given a bouned x ∈Rd, set function gt+1(w) = w⊤x + ft(w) −w⊤Ew[wft(w)] with ft ∈L2 and\nw ∼N(0, Id) (or ft ∈L2, w ∼Uni[\n√\ndSd−1]). Set ft+1 = h ◦gt+1. Denote Q(f) = L(f) + Ew[φλ(f(w))] with\nL(f) := 1\n2∥x −Ew[wf(w)]∥2\n2, we have\nQ(ft+1)≤Q(ft) −1\n2Ew[(ft+1(w)−ft(w) −w⊤Ew[w(ft+1(w)−ft(w))])2] ≤Q(ft)\n(87)\nProof. From Lemma 4, we know that\nL(ft+1)=L(ft) + Ew[w⊤(Ew[wft(w)]−x)\n\u0000ft+1(w)−ft(w)\n\u0001\n] + 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n(88)\nLet gt+1(w) = w⊤x + ft(w) −w⊤Ew[wft(w)], together with Eq.(88), we can obtain that\nL(ft+1)=L(ft) + Ew[\n\u0000ft(w) −gt+1(w)\n\u0001\u0000ft+1(w)−ft(w)\n\u0001\n] + 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n(89)\nNote that ab = (a+b)2−a2−b2\n2\n, it follows that\n\u0000ft(w)−gt+1(w)\n\u0001\u0000ft+1(w)−ft(w)\n\u0001\n=\n\u0000ft+1(w)−gt+1(w)\n\u00012−\n\u0000ft(w)−gt+1(w)\n\u00012−\n\u0000ft+1(w)−ft(w)\n\u00012\n2\n(90)\nSince ft+1 = h ◦gt+1 is the solution of the proximal problem,\ni.e., ft+1(w) = arg minx\n(x−gt+1(w))2\n2\n+ φλ(x), we know that\n\u0000ft+1(w)−gt+1(w)\n\u00012\n2\n−\n\u0000ft(w)−gt+1(w)\n\u00012\n2\n≤φλ(ft(w)) −φλ(ft+1(w))\n(91)\nIt follows that\nEw[\n\u0000ft(w) −gt+1(w)\n\u0001\u0000ft+1(w)−ft(w)\n\u0001\n]\n= Ew [\n\u0000ft+1(w)−gt+1(w)\n\u00012−\n\u0000ft(w)−gt+1(w)\n\u00012−\n\u0000ft+1(w)−ft(w)\n\u00012\n2\n]\n(92)\n≤Ew[φλ(ft(w))] −Ew[φλ(ft+1(w))] −1\n2Ew[\n\u0000ft+1(w)−ft(w)\n\u00012]\n(93)\nPlug inequality (93) into Eq.(89), we can achieve that\nL(ft+1) + Ew[φλ(ft+1(w))]≤L(ft) + Ew[φλ(ft(w))] −1\n2Ew[\n\u0000ft+1(w)−ft(w)\n\u00012]\n+ 1\n2∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n(94)\nYueming Lyu, Ivor Tsang\nFrom Lemma 2, we know that\nEw[\n\u0000ft+1(w)−ft(w)\n\u00012] −∥Ew[w\n\u0000ft+1(w)−ft(w)\n\u0001\n]∥2\n2\n= Ew[\n\u0000ft+1(w)−ft(w) −w⊤Ew[w(ft+1(w)−ft(w))]\n\u00012]\n(95)\nIt follows that\nQ(ft+1)≤Q(ft) −1\n2Ew[(ft+1(w)−ft(w) −w⊤Ew[w(ft+1(w)−ft(w))])2] ≤Q(ft)\n(96)\nD\nProof of Theorem 3\nTo prove the Theorem 3, we ﬁrst show some useful Lemmas.\nLemma 10. Suppose\n1\nN W W ⊤= Id, for any bounded y ∈RN, we have\n1\nN ∥y∥2\n2 −∥1\nN W y∥2\n2 =\n1\nN ∥y −\n1\nN W ⊤W y∥2\n2 ≥0.\nProof.\n1\nN ∥y∥2\n2 −∥1\nN W y∥2\n2 = ∥y∥2\n2 −2∥1\nN W y∥2\n2 + ∥1\nN W y∥2\n2\n(97)\n= 1\nN ∥y∥2\n2 −2\nN 2 y⊤W ⊤W y + 1\nN 2 y⊤W ⊤W y\n(98)\n= 1\nN ∥y∥2\n2 −2\nN 2 y⊤W ⊤W y + 1\nN 2 y⊤W ⊤1\nN W W ⊤W y\n(99)\n= 1\nN ∥y −1\nN W ⊤W y∥2\n2 ≥0\n(100)\nLemma\n11. Denote L(y)\n:=\n1\n2∥x −\n1\nN W y∥2\n2.\nFor ∀y, z\n∈\nRN,\nwe have L(z)\n=\nL(y) +\n<\n1\nN2 W ⊤W y −1\nN W ⊤x, z −y > + 1\n2∥1\nN W (z −y)∥2\n2\nProof.\n1\n2∥x −1\nN W z∥2\n2 = 1\n2∥x −1\nN W y + 1\nN W y −1\nN W z∥2\n2\n(101)\n= L(y) + < 1\nN W y −x, 1\nN W (z −y) > +1\n2∥1\nN W (z −y)∥2\n2\n(102)\n= L(y) + <\n1\nN 2 W ⊤W y −1\nN W ⊤x, z −y > +1\n2∥1\nN W (z −y)∥2\n2\n(103)\nTheorem. (Monotonic Descent) For a function φλ(·), denote h(·) as the proximal operator of φλ(·). Given a\nbouned x ∈Rd, set yt+1 = h\n\u0000W ⊤x+(I −1\nN W ⊤W )yt\n\u0001\nwith\n1\nN W W ⊤= Id. Denote bQ(y) := 1\n2∥x−1\nN W y∥2\n2 +\n1\nN φλ(y). For t ≥0, we have\nbQ(yt+1)≤bQ(yt) −1\n2N ∥(Id −1\nN W ⊤W )(yt+1 −yt)∥2\n2 ≤bQ(yt)\n(104)\nProof. Denote L(y) := 1\n2∥x −1\nN W y∥2\n2, from Lemma 11, we know that\nL(yt+1) = L(yt) + < 1\nN 2 W ⊤W yt −1\nN W ⊤x, yt+1 −yt > +1\n2∥1\nN W (yt+1 −yt)∥2\n2\n(105)\nNeural Optimization Kernel: Towards Robust Deep Learning\nLet at+1 = W ⊤x + (I −1\nN W ⊤W )yt. Together with Eq.(105), we can obtain that\nL(yt+1) = L(yt) + < 1\nN 2 W ⊤W yt −1\nN W ⊤x, yt+1 −yt > +1\n2∥1\nN W (yt+1 −yt)∥2\n2\n(106)\n= L(yt) + 1\nN <yt −at+1, yt+1 −yt > +1\n2∥1\nN W (yt+1 −yt)∥2\n2\n(107)\nNote that a⊤b = ∥a+b∥2\n2−∥a∥2\n2−∥b∥2\n2\n2\n, it follows that\n<yt −at+1, yt+1 −yt > = ∥yt+1 −at+1∥2\n2 −∥yt −at+1∥2\n2 −∥yt+1 −yt∥2\n2\n2\n(108)\nSince yt+1 = h(at+1) is the solution of the proximal problem,\ni.e., yt+1 = arg miny\n1\n2∥y −at+1∥2\n2 + φλ(y), we can achieve that\n1\n2∥yt+1 −at+1∥2\n2 + φλ(yt+1) ≤1\n2∥yt −at+1∥2\n2 + φλ(yt)\n(109)\nIt can be rewritten as\n1\n2∥yt+1 −at+1∥2\n2 −1\n2∥yt −at+1∥2\n2 ≤φλ(yt) −φλ(yt+1)\n(110)\nTogether with Eq.(107), Eq.(108) and inequality (110), it follows that\nL(yt+1) + 1\nN φλ(yt+1) ≤L(yt) + 1\nN φλ(yt) −1\n2N ∥yt+1 −yt∥2\n2 + 1\n2∥1\nN W (yt+1 −yt)∥2\n2\n(111)\nTogether with Lemma 10, we can achieve that\nbQ(yt+1)≤bQ(yt) −1\n2N ∥(Id −1\nN W ⊤W )(yt+1 −yt)∥2\n2 ≤bQ(yt)\n(112)\nE\nProof of Theorem 4\nBefore proving Theorem 4, we ﬁrst show some useful Lemmas.\nLemma 12. Denote L(y) := 1\n2∥x −1\nN W y∥2\n2. For any bounded yt, yt+1, z ∈RN, we have\nL(yt+1) = L(z) +\n\u001c 1\nN 2 W ⊤W yt−1\nN W ⊤x, yt+1 −z\n\u001d\n+ 1\n2∥1\nN W (yt+1 −yt)∥2\n2\n−1\n2∥1\nN W (z −yt)∥2\n2\n(113)\nProof. Denote L(y) := 1\n2∥x −1\nN W y∥2\n2. From Lemma 11, we can achieve that\nL(z) = L(yt) + < 1\nN 2 W ⊤W yt −1\nN W ⊤x, z −yt > +1\n2∥1\nN W (z −yt)∥2\n2\n(114)\nL(yt+1) = L(yt) + < 1\nN 2 W ⊤W yt −1\nN W ⊤x, yt+1 −yt > +1\n2∥1\nN W (yt+1 −yt)∥2\n2\n(115)\nIt follows that\nL(yt+1)=L(z)−\n\u001c 1\nN 2 W ⊤W yt−1\nN W ⊤x, z−yt\n\u001d\n+\n\u001c 1\nN 2 W ⊤W yt−1\nN W ⊤x, yt+1−yt\n\u001d\n+ 1\n2∥1\nN W (yt+1 −yt)∥2\n2 −1\n2∥1\nN W (z −yt)∥2\n2\n(116)\n= L(z) +\n\u001c 1\nN 2 W ⊤W yt−1\nN W ⊤x, yt+1 −z\n\u001d\n+ 1\n2∥1\nN W (yt+1 −yt)∥2\n2\n−1\n2∥1\nN W (z −yt)∥2\n2\n(117)\nYueming Lyu, Ivor Tsang\nLemma 13. For a convex function φλ(·), let h(·) be the proximal operator w.r.t φλ(·). Denote bQ(y) := 1\n2∥x −\n1\nN W y∥2\n2 + 1\nN φλ(y), for any bounded yt, z ∈RN, set at+1 = W ⊤x + (I −1\nN W ⊤W )yt and yt+1 = h(at+1),\nthen we have\nbQ(yt+1) ≤bQ(z) + 1\n2N\n\u0000∥yt −z∥2\n2 −∥yt+1 −z∥2\n2\n\u0001\n−1\n2∥1\nN W (z −yt)∥2\n2\n(118)\nProof. Since yt+1 = arg miny φλ(y) + 1\n2∥y −at+1∥2\n2, we have\n0 ∈∂φ(yt+1) + (yt+1 −at+1) =⇒(at+1 −yt+1) ∈∂φ(yt+1)\n(119)\nFor a convex function φλ(y) and subgradient g ∈∂φλ(y), we know φλ(z) ≥φλ(y) + ⟨g, z −y⟩, it follows that\nφλ(z) ≥φλ(yt+1) +\n\nat+1 −yt+1, z −yt+1\n\u000b\n(120)\nTogether with Lemma 12, we can obtain that\nL(yt+1) + 1\nN φλ(yt+1) ≤L(z) + 1\nN φλ(z) −1\nN\n\nat+1 −yt+1, z −yt+1\n\u000b\n+\n\u001c 1\nN 2 W ⊤W yt−1\nN W ⊤x, yt+1 −z\n\u001d\n+ 1\n2∥1\nN W (yt+1 −yt)∥2\n2\n−1\n2∥1\nN W (z −yt)∥2\n2\n(121)\nIt follows that\nbQ(yt+1) ≤bQ(z) + 1\nN\n\nyt −yt+1, yt+1 −z\n\u000b\n+ 1\n2∥1\nN W (yt+1 −yt)∥2\n2 −1\n2∥1\nN W (z −yt)∥2\n2\n(122)\nNote that a⊤b = ∥a+b∥2\n2−∥a∥2\n2−∥b∥2\n2\n2\n, it follows that\n\nyt −yt+1, yt+1 −z\n\u000b\n= 1\n2∥yt −z∥2\n2 −1\n2∥yt+1 −z∥2\n2 −1\n2∥yt −yt+1∥2\n2\n(123)\nTogether with inequality (122), we can achieve that\nbQ(yt+1) ≤bQ(z) + 1\n2N\n\u0000∥yt −z∥2\n2 −∥yt+1 −z∥2\n2 −∥yt −yt+1∥2\n2\n\u0001\n+ 1\n2∥1\nN W (yt+1 −yt)∥2\n2\n−1\n2∥1\nN W (z −yt)∥2\n2\n(124)\nFrom Lemma 10, we know\n1\nN ∥yt −yt+1∥2\n2 ≥∥1\nN W (yt+1 −yt)∥2\n2, it follows that\nbQ(yt+1) ≤bQ(z) + 1\n2N\n\u0000∥yt −z∥2\n2 −∥yt+1 −z∥2\n2\n\u0001\n−1\n2∥1\nN W (z −yt)∥2\n2\n(125)\nLemma 14. (Strictly Monotonic Descent) For a convex function φλ(·), let h(·) be the proximal operator w.r.t\nφλ(·). Denote bQ(y) := 1\n2∥x−1\nN W y∥2\n2+ 1\nN φλ(y), for any bounded yt ∈RN, set at+1 = W ⊤x+(I−1\nN W ⊤W )yt\nand yt+1 = h(at+1), then we have\nbQ(yt+1) ≤bQ(yt) −1\n2N ∥yt+1 −yt∥2\n2\n(126)\nProof. From Lemma 13, setting z = yt, we can directly get the result.\nTheorem. For a convex function φλ(·), denote h(·) as the proximal operator of φλ(·). Given a bounded x ∈Rd,\nset yt+1 = h\n\u0000W ⊤x + (I −1\nN W ⊤W )yt\n\u0001\nwith\n1\nN W W ⊤= Id. Denote bQ(y) := 1\n2∥x −b\nA(y)∥2\n2 + 1\nN φλ(y) and y∗\nas an optimal of bQ(·), for T ≥1, we have\nT\n\u0000 bQ(yT ) −bQ(y∗)\n\u0001\n≤\n1\n2N ∥y0 −y∗∥2\n2 −1\n2N ∥yT −y∗∥2\n2 −1\n2\nT −1\nX\nt=0\n∥1\nN W (yt −y∗)∥2\n2\n−1\n2\nT −1\nX\nt=0\nt + 1\nN\n∥yt+1 −yt∥2\n2\n(127)\nNeural Optimization Kernel: Towards Robust Deep Learning\nProof. From Lemma 13, setting z = y∗, we can achieve that\nbQ(yt+1) ≤bQ(y∗) + 1\n2N\n\u0000∥yt −y∗∥2\n2 −∥yt+1 −y∗∥2\n2\n\u0001\n−1\n2∥1\nN W (y∗−yt)∥2\n2\n(128)\nTelescope the inequality (128) from t = 0 to t = T −1, we can obtain that\nT −1\nX\nt=0\nbQ(yt+1) −T bQ(y∗) ≤\n1\n2N ∥y0 −y∗∥2\n2 −1\n2N ∥yT −y∗∥2\n2 −1\n2\nT −1\nX\nt=0\n∥1\nN W (yt −y∗)∥2\n2\n(129)\nFrom Lemma 14, we know that\nbQ(yt+1) ≤bQ(yt) −1\n2N ∥yt+1 −yt∥2\n2\n(130)\nIt follows that\nbQ(yT ) ≤bQ(yt) −1\n2N\nT −1\nX\ni=t\n∥yi+1 −yi∥2\n2\n(131)\nThen, we can achieve that\nT bQ(yT ) −T bQ(y∗) ≤\nT −1\nX\nt=0\nbQ(yt+1) −T bQ(y∗) −1\n2N\nT −1\nX\nt=0\nT −1\nX\ni=t\n∥yi+1 −yi∥2\n2\n(132)\n=\nT −1\nX\nt=0\nbQ(yt+1) −T bQ(y∗) −1\n2N\nT −1\nX\nt=0\n(t + 1)∥yt+1 −yt∥2\n2\n(133)\nPlug inequality (129) into inequality (133), we obtain that\nT\n\u0000 bQ(yT ) −bQ(y∗)\n\u0001\n≤\n1\n2N ∥y0 −y∗∥2\n2 −1\n2N ∥yT −y∗∥2\n2 −1\n2\nT −1\nX\nt=0\n∥1\nN W (yt −y∗)∥2\n2\n−1\n2\nT −1\nX\nt=0\nt + 1\nN\n∥yt+1 −yt∥2\n2\n(134)\nF\nProof of Theorem 5\nWe ﬁrst show the structured samples B constructed in [29, 30].\nWithout loss of generality, we assume that d = 2m, N = 2n. Let F ∈Cn×n be an n × n discrete Fourier matrix.\nF k,j = e\n2πikj\nn\nis the (k, j)thentry of F , where i = √−1. Let Λ = {k1, k2, ..., km} ⊂{1, ..., n −1} be a subset of\nindexes.\nThe structured matrix B can be constructed as Eq.(135).\nB =\n√n\n√m\n\u0014 ReF Λ\n−ImF Λ\nImF Λ\nReF Λ\n\u0015\n∈Rd×N\n(135)\nwhere Re and Im denote the real and imaginary parts of a complex number, and F Λ in Eq. (136) is the matrix\nconstructed by m rows of F\nF Λ= 1\n√n\n\n\ne\n2πik11\nn\n· · ·\ne\n2πik1n\nn\n...\n...\n...\ne\n2πikm1\nn\n· · ·\ne\n2πikmn\nn\n\n∈Cm×n.\n(136)\nYueming Lyu, Ivor Tsang\nThe index set can be constructed by a closed-form solution [30] or by a coordinate descent method [29].\nSpeciﬁcally, for a prime number n such that m divides n−1, i.e., m|(n −1), we can employ a closed-form\nconstruction as in [30]. Let g denote a primitive root modulo n. We can construct the index Λ = {k1, k2, ..., km}\nas\nΛ = {g0, g\nn−1\nm , g\n2(n−1)\nm\n, · · · , g\n(m−1)(n−1)\nm\n} mod n.\n(137)\nThe resulted structured matrix B has a bounded mutual coherence, which is shown in Theorem 8.\nTheorem 8. [30] Suppose d = 2m, N = 2n, and n is a prime such that m|(n −1). Construct matrix B as in\nEq.(135) with index set Λ as Eq.(137). Let mutual coherence µ(B) := maxi̸=j\n|b⊤\ni bj|\n∥bi∥2∥bj∥2 . Then µ(B) ≤\n√n\nm .\nRemark: The bound of mutual coherence in Theorem 8 is non-trivial when n < m2. For the case n ≥m2, we\ncan use the coordinate descent method in [29] to minimize the mutual coherence.\nWe now show the orthogonal property of our data-dependent structured samples D =\n√\nd\n√\nN R⊤B\nProposition 2. Suppose d = 2m, N = 2n.\nLet D =\n√\nd\n√\nN R⊤B with B constructed as in Eq.(135).\nThen\nDD⊤= Id and column vector has constant norm, i.e., ∥dj∥2 = p m\nn , ∀j ∈{1, · · · , N}.\nProof. Since DD⊤= d\nN BB⊤= m\nn BB⊤= eB eB\n⊤, where eB =\n√m\n√n B. It follows that\neB =\n\u0014 ReF Λ\n−ImF Λ\nImF Λ\nReF Λ\n\u0015\n∈Rd×N\n(138)\nLet ci ∈C1×n be the ith row of matrix F Λ ∈Cm×n in Eq.(136). Let vi ∈R1×2n be the ith row of matrix\neB ∈R2m×2n in Eq.(138). For 1 ≤i, j ≤m, i ̸= j, we know that\nviv⊤\ni+m = 0,\n(139)\nvi+mv⊤\nj+m = viv⊤\nj = Re(cic∗\nj),\n(140)\nvi+mv⊤\nj = −viv⊤\nj+m = Im(cic∗\nj),\n(141)\nwhere ∗denotes the complex conjugate, Re(·) and Im(·) denote the real and imaginary parts of the input complex\nnumber.\nFor a discrete Fourier matrix F , we know that\ncic∗\nj = 1\nn\nn−1\nX\nk=0\ne\n2π(i−j)ki\nn\n=\n(\n1,\nif i = j\n0,\notherwise\n(142)\nWhen i ̸= j, from Eq.(142), we know cic∗\nj = 0. Thus, we have\nvi+mv⊤\nj+m = viv⊤\nj = Re(cic∗\nj) = 0,\n(143)\nvi+mv⊤\nj = −viv⊤\nj+m = Im(cic∗\nj) = 0,\n(144)\nWhen i = j, we know that vi+mv⊤\ni+m = viv⊤\ni = cic∗\ni = 1.\nPut two cases together, also note that d = 2m, we have DD⊤= eB eB\n⊤= Id.\nThe l2-norm of the column vector of eB is given as\n∥ebj∥2\n2 = 1\nn\nm\nX\ni=1\n\u0000sin2 2πkij\nn\n+ cos2 2πkij\nn\n\u0001\n= m\nn\n(145)\nThus, we have ∥dj∥2 = ∥ebj∥2\n2 = p m\nn for j ∈{1, · · · , M}\nNeural Optimization Kernel: Towards Robust Deep Learning\nLemma 15. Let D =\n√\nd\n√\nN R⊤B, where B is constructed as as in Eq.(135) with index set Λ as Eq.(137) [30]\nwith N = 2n, d = 2m. ∀y ∈RN, ∥y∥0 ≤2k, we have ∥Dy∥2\n2 −∥y∥2\n2 ≤−n−(2k−1)√n−m\nn\n∥y∥2\n2\nProof. Denote M = D⊤D. Since the column vector of D has constant norm, i.e., ∥dj∥2\n2 = m\nn , it follows that\n∥Dy∥2\n2 = y⊤My = ∥dj∥2\n2\n\u0000 N\nX\ni=1\ny2\ni +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nyiyjMij\n\u0001\n(146)\n= m\nn ∥y∥2\n2 + m\nn\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nyiyjMij\n(147)\n≤m\nn ∥y∥2\n2 + m\nn µ(D)\n\u0000 N\nX\ni=1\nN\nX\nj=1,j̸=i\n|yi||yj|\n\u0001\n(148)\n= m\nn ∥y∥2\n2 + m\nn µ(D)\n \n\u0000 N\nX\ni=1\n|yi|\n\u00012 −\nN\nX\ni=1\ny2\ni\n!\n(149)\nSince ∥y∥0 ≤2k, we know there is at most 2k non-zero elements among y. Thus, we know that\n∥Dy∥2\n2 ≤m\nn ∥y∥2\n2 + m\nn µ(D)\n \n\u0000 N\nX\ni=1\n|yi|\n\u00012 −\nN\nX\ni=1\ny2\ni\n!\n(150)\n≤m\nn ∥y∥2\n2 + m\nn µ(D)\n\u00002k\nN\nX\ni=1\ny2\ni −\nN\nX\ni=1\ny2\ni\n\u0001\n(151)\n= m\nn ∥y∥2\n2 + m\nn µ(D)(2k −1)∥y∥2\n2\n(152)\nSince µ(D) = B, from Theorem 8, we know µ(D) ≤\n√n\nm . It follows that\n∥Dy∥2\n2 ≤m\nn ∥y∥2\n2 + m\nn µ(D)(2k −1)∥y∥2\n2\n(153)\n≤m\nn ∥y∥2\n2 + m\nn\n(2k −1)√n\nm\n∥y∥2\n2\n(154)\n= (2k −1)√n + m\nn\n∥y∥2\n2\n(155)\nIt follows that ∥Dy∥2\n2 −∥y∥2\n2 ≤(2k−1)√n+m−n\nn\n∥y∥2\n2.\nTheorem. (Strictly Monotonic Descent of k-sparse problem ) Let L(y) = 1\n2∥x −Dy∥2\n2, s.t. ∥y∥0 ≤k with\nD =\n√\nd\n√\nN R⊤B, where B is constructed as as in Eq.(135) with index set Λ as Eq.(137) [30] with N = 2n, d = 2m.\nSet yt+1 = h(at+1) with sparity k and at+1 = D⊤x + (I −D⊤D)yt, we have\nL(yt+1) ≤L(yt) + 1\n2∥yt+1 −at+1∥2\n2 −1\n2∥yt −at+1∥2\n2 −n −(2k −1)√n −m\n2n\n∥yt+1 −yt∥2\n2 ≤L(yt)\n(156)\nwhere h(·) is deﬁned as\nh(zj) =\n\u001a\nzj\nif\n|zj| is one of the k-highest values of |z| ∈RN\n0\notherwise\n.\n(157)\nProof. Denote L(y) := 1\n2∥x −Dy∥2\n2. It follows that\nL(yt+1) = 1\n2∥x −Dyt+1∥2\n2 = 1\n2∥x −Dyt + Dyt −Dyt+1∥2\n2\n(158)\n= L(yt) +\n\nx −Dyt, D(yt −yt+1)\n\u000b\n+ ∥D(yt −yt+1)∥2\n2\n(159)\n= L(yt) +\nD\nD⊤x −D⊤Dyt, yt −yt+1\nE\n+ ∥D(yt −yt+1)∥2\n2\n(160)\n= L(yt) +\nD\nD⊤Dyt −D⊤x, yt+1 −yt\nE\n+ ∥D(yt −yt+1)∥2\n2\n(161)\nYueming Lyu, Ivor Tsang\nLet at+1 = D⊤x + (I −D⊤D)yt, together with Eq.(161), we can obtain that\nL(yt+1) = L(yt) +\n\nyt −at+1, yt+1 −yt\n\u000b\n+ 1\n2∥D(yt+1 −yt)∥2\n2\n(162)\n= L(yt) + ∥yt+1 −at+1∥2\n2 −∥yt −at+1∥2\n2 −∥yt+1 −yt∥2\n2\n2\n+ 1\n2∥D(yt+1 −yt)∥2\n2\n(163)\nFrom Lemma 15, we know that\n1\n2∥D(yt+1 −yt)∥2\n2 −1\n2∥yt+1 −yt∥2\n2 ≤−n −(2k −1)√n −m\n2n\n∥yt+1 −yt∥2\n2\n(164)\nIt follows that\nL(yt+1) ≤L(yt) + 1\n2∥yt+1 −at+1∥2\n2 −1\n2∥yt −at+1∥2\n2 −n −(2k −1)√n −m\n2n\n∥yt+1 −yt∥2\n2\n(165)\nNote that yt+1 := arg miny,∥y∥0≤k ∥y −at+1∥2\n2, we know ∥yt+1 −at+1∥2\n2 ≤∥yt −at+1∥2\n2.\nIt follows that\nL(yt+1) ≤L(yt), in which the equality holds true when ∥yt+1 −at+1∥2\n2 = ∥yt −at+1∥2\n2 and ∥yt+1 −yt∥2\n2 = 0\nG\nA Better Diagonal Random Rotation for SSF [29]\nIn [29], a diagonal rotation matrix D is constructed by sampling its diagonal elements uniformly from {−1, +1}.\nIn this section, we propose a better diagonal random rotation.\nWithout loss of generality, we assume that\nd = 2m, N = 2n.\nWe ﬁrst generate a diagonal complex matrix D ∈Cm×m, in which the diagonal elements are constructed as\nDjj = cosθj + i sinθj , ∀j ∈{1, · · · , m}\n(166)\nwhere θj, ∀j ∈{1, · · · , m} are i.i.d. samples from the uniform distribution Uni[0, 2π), and i = √−1.\nWe then generate a uniformly random permutation Π : {1, · · · , d} →{1, · · · , d}.\nThe SSF samples can be\nconstructed as H = Π ◦eB with eB:\neB =\n√n\n√m\n\"\nReeF Λ\n−ImeF Λ\nImeF Λ\nReeF Λ\n#\n∈Rd×N\n(167)\nwhere eF Λ = DF Λ.\nIt is worth noting that H⊤H = B⊤B, which means that the proposed the diagonal rotation scheme preserved\nthe pairwise inner product of SSF [29]. Moreover, the SSF with the proposed random rotation maintains O(d)\nspace complexity and O(n log n) (matrix-vector product) time complexity by FFT.\nH\nRademacher Complexity\nNeural Network Structure: For structured approximated NOK networks (SNOK), the 1-T layers are given\nas\nyt+1 = h(D⊤Rtx + (I −D⊤D)yt)\n(168)\nwhere Rt are free parameters such that R⊤\nt Rt = R⊤\nt Rt = Id. And D is the scaled structured spherical samples\nsuch that DD⊤= Id, and y0 = 0.\nThe last layer ( (T+1)th layer) is given by z = w⊤yT+1.\nConsider a L-Lipschitz continuous loss function\nℓ(z, y) : Z × Y →[0, 1] with Lipschitz constant L w.r.t the input z.\nNeural Optimization Kernel: Towards Robust Deep Learning\nRademacher Complexity: Rademacher complexity of a function class G is deﬁned as\nRN(G) := 1\nN E\n\"\nsup\ng∈G\nN\nX\ni=1\nϵig(xi)\n#\n(169)\nwhere ϵi, i ∈{1, · · · , N} are i.i.d. samples drawn uniformly from {+1, −1} with probality P[ϵi = +1] = P[ϵi =\n−1] = 1/2. And xi, i ∈{1, · · · , N} are i.i.d. samples from X.\nTheorem. (Rademacher Complexity Bound) Consider a Lipschitz continuous loss function ℓ(z, y) : Z × Y →\n[0, 1] with Lipschitz constant L w.r.t the input z. Let eℓ(z, y) := ℓ(z, y) −ℓ(0, y). Let bG be the function class of\nour (T+1)-layer SNOK mapping from X to Z. Suppose the activation function |h(y)| ≤|y| (element-wise), and\nthe l2-norm of last layer weight is bounded, i.e., ∥w∥2 ≤Bw. Let (xi, yi)N\ni=1 be i.i.d. samples drawn from X × Y.\nLet Y T+1 = [y(1)\nT+1, · · · , y(N)\nT+1] be the T th layer output with input X. Denote the mutual coherence of Y T+1 as µ∗,\ni.e., µ∗= µ(Y T+1) = maxi̸=j\ny(i)⊤\nT+1 y(j)\nT+1\n∥y(i)\nT+1∥2∥y(j)\nT+1∥2 ≤1. Then, we have\nRN(eℓ◦bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵieℓ(g(xi), yi)\n#\n≤\nLBw\nq\u0000(N −1)µ∗+ 1\n\u0001\nT\nN\n∥X∥F\n(170)\nwhere X = [x1, · · · , xN]. ∥· ∥2 and ∥· ∥F denote the spectral norm and the Frobenius norm of input matrix,\nrespectively.\nRemark: The Rademacher complexity bound has a complexity O(\n√\nT) w.r.t. the depth of NN (SNOK).\nProof. Since eℓis L-Lipschitz continuous function, from the composition rule of Rademacher complexity, we know\nthat\nRN(eℓ◦bG) ≤L RN( bG)\n(171)\nYueming Lyu, Ivor Tsang\nIt follows that\nRN( bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵif(xi)\n#\n(172)\n= 1\nN E\n\"\nsup\nw,{Rt∈SO(d)}T\nt=1\nN\nX\ni=1\nϵi\n\nw, y(i)\nT+1\n\u000b\n#\n(173)\n= 1\nN E\n\"\nsup\nw,{Rt∈SO(d)}T\nt=1\n\nw,\nN\nX\ni=1\nϵiy(i)\nT+1\n\u000b\n#\n(174)\n≤1\nN E\n\"\nsup\nw,{Rt∈SO(d)}T\nt=1\n∥w∥2\n\r\r\nN\nX\ni=1\nϵiy(i)\nT+1\n\r\r\n2\n#\n(Cauchy-Schwarz inequality)\n(175)\n≤Bw\nN E\n\"\nsup\n{Rt∈SO(d)}T\nt=1\n\r\r\nN\nX\ni=1\nϵiy(i)\nT+1\n\r\r\n2\n#\n(176)\n= Bw\nN E\n\n\nsup\n{Rt∈SO(d)}T\nt=1\nv\nu\nu\nt\nN\nX\ni=1\n\r\rϵiy(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(177)\n= Bw\nN E\n\n\nsup\n{Rt∈SO(d)}T\nt=1\nv\nu\nu\nt\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(178)\n= Bw\nN E\n\n\nv\nu\nu\nt\nsup\n{Rt∈SO(d)}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(179)\n≤Bw\nN\nv\nu\nu\nu\ntE\n\n\nsup\n{Rt∈SO(d)}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(180)\nInequality (180) is because of the Jensen inequality and concavity of the square root function.\nNote that |ϵi| = 1, ∀i ∈{1, · · · , N}, and the mutual coherence of Y T+1 is µ∗, i.e., µ∗= µ(Y T+1) ≤1, it follows\nthat\nRN( bF) ≤Bw\nN\nv\nu\nu\nu\ntE\n\n\nsup\n{Rt∈SO(d)}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(181)\n≤Bw\nN\nv\nu\nu\nt\nsup\n{Rt∈SO(d)}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\n\r\ry(i)\nT+1\n\r\r\n2\n\r\ry(j)\nT+1\n\r\r\n2µ∗\n(182)\n= Bw\nN\nv\nu\nu\nt\nsup\n{Rt∈SO(d)}T\nt=1\n(1 −µ∗)\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 + µ∗\u0000 N\nX\ni=1\n\r\ry(i)\nT+1\n\r\r\n2\n\u00012\n(183)\n≤Bw\nN\nr\nsup\n{Rt∈SO(d)}T\nt=1\n(1 −µ∗)∥Y T+1∥2\nF + Nµ∗∥Y T+1∥2\nF Cauchy-Schwarz\n(184)\n≤Bw\nN\nr\nsup\n{Rt∈SO(d)}T\nt=1\n\u0000(N −1)µ∗+ 1\n\u0001\n∥Y T+1∥2\nF\n(185)\nwhere Y T+1 = [y(1)\nT+1, · · · , y(N)\nT+1] and ∥· ∥F denotes the Frobenius norm.\nSince |h(Y )| ≤|Y | (element-wise), (e.g., ReLU, max-pooling, soft-thresholding), it follows that\n∥Y T+1∥2\nF = ∥h\n\u0000D⊤RT X + (I −D⊤D)Y T\n\u0001\n∥2\nF\n(186)\n≤∥D⊤RT X + (I −D⊤D)Y T ∥2\nF\n(187)\nNeural Optimization Kernel: Towards Robust Deep Learning\nIn addition, we have\n∥D⊤RT X + (I −D⊤D)Y T ∥2\nF = ∥D⊤RT X∥2\nF + ∥(I −D⊤D)Y T ∥2\nF\n+ 2\n\nD⊤RT X, (I−D⊤D)Y T\n\u000b\n(188)\nNote that DD⊤= Id and R⊤\nT RT = RT R⊤\nT = Id, we have\n∥D⊤RT X∥2\nF = ∥X∥2\nF\n(189)\n\nD⊤RT X, (I−D⊤D)Y T\n\u000b\n= tr\n\u0010\nX⊤R⊤\nT D(I−D⊤D)Y T\n\u0011\n= 0\n(190)\nDenote β = ∥I −D⊤D∥2\n2, it follows that\n∥D⊤RT X + (I −D⊤D)Y T ∥2\nF = ∥D⊤RT X∥2\nF + ∥(I −D⊤D)Y T ∥2\nF\n+ 2\n\nD⊤RT X, (I−D⊤D)Y T\n\u000b\n(191)\n= ∥X∥2\nF + ∥(I −D⊤D)Y T ∥2\nF\n(192)\n≤∥X∥2\nF + ∥I −D⊤D∥2\n2∥Y T ∥2\nF = ∥X∥2\nF + β∥yT ∥2\nF\n(193)\nRecursively apply the above procedure from t = T to t = 1, together with Y 0 = 0, we can achieve that\n∥Y T+1∥2\nF ≤∥X∥2\nF\n\u0000 T −1\nX\ni=0\nβi\u0001\n(194)\nTogether with inequality (185), it follows that\nRN( bG) ≤Bw\nN\nr\nsup\n{Rt∈SO(d)}T\nt=1\n\u0000(N −1)µ∗+ 1\n\u0001\n∥Y T+1∥2\nF\n(195)\n≤\nBw\nq\u0000(N −1)µ∗+ 1\n\u0001\nN\nv\nu\nu\nt\nT −1\nX\ni=0\nβi∥X∥F\n(196)\nFinally, we obtain that\nRN(eℓ◦bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵieℓ(g(xi), yi)\n#\n≤\nLBw\nq\u0000(N −1)µ∗+ 1\n\u0001\nN\nv\nu\nu\nt\nT −1\nX\ni=0\nβi∥X∥F\n(197)\nNow, we show that β = ∥I −D⊤D∥2\n2 ≤1. From the deﬁnition of spectral norm, we have that\nβ = ∥I −D⊤D∥2\n2 =\nsup\n∥y∥2=1\n∥(I −D⊤D)y∥2\n2\n(198)\n=\nsup\n∥y∥2=1\ny⊤(I −D⊤D)⊤(I −D⊤D)y\n(199)\n=\nsup\n∥y∥2=1\ny⊤\u0000I −2D⊤D + D⊤DD⊤D\n\u0001\ny\n(200)\n=\nsup\n∥y∥2=1\ny⊤(I −D⊤D)y\n(201)\n= 1 −min\n∥y∥2=1 ∥Dy∥2\n2 ≤1\n(202)\nSince matrix D is not full rank, we know β = 1.\nYueming Lyu, Ivor Tsang\nI\nGeneralization Bound\nTheorem. Consider a Lipschitz continuous loss function ℓ(z, y) : Z × Y →[0, 1] with Lipschitz constant L w.r.t\nthe input z. Let eℓ(z, y) := ℓ(z, y) −ℓ(0, y). Let bG be the function class of our (T+1)-layer SNOK mapping\nfrom X to Z. Suppose the activation function |h(y)| ≤|y| (element-wise), and the l2-norm of last layer weight\nis bounded, i.e., ∥w∥2 ≤Bw. Let (xi, yi)N\ni=1 be i.i.d. samples drawn from X × Y. Let Y T+1 be the T th layer\noutput with input X. Denote the mutual coherence of Y T+1 as µ∗, i.e., µ∗= µ(Y T+1) ≤1. Then, for ∀N and\n∀δ, 0 < δ < 1, with a probability at least 1 −δ, ∀g ∈bG, we have\nE\n\u0002\nℓ(g(X), Y )\n\u0003\n≤1\nN\nN\nX\ni=1\nℓ(g(xi), yi) +\nLBw\nq\u0000(N −1)µ∗+ 1\n\u0001\nT\nN\n∥X∥F +\nr\n8 ln(2/δ)\nN\n(203)\nwhere X = [x1, · · · , xN], and ∥· ∥F denotes the Frobenius norm.\nProof. Plug the Rademacher complexity bound of SNOK (our Theorem 6) into the Theorem 8 in [5], we can\nobtain the bound.\nJ\nRademacher Complexity and Generalization Bound for A More General\nStructured Neural Network Family\nNeural Network Structure: For a more general structured neural network family that includes SNOK, the\n1-T layers are given as\nyt+1 = h(D⊤\nt x + (I −D⊤\nt Dt)yt)\n(204)\nwhere Dt ∈RdD×d are free parameters such that DtD⊤\nt = Id and dD > d, and y0 = 0.\nThe last layer ( (T+1)th layer) is given by z = w⊤yT+1.\nConsider a L-Lipschitz continuous loss function\nℓ(z, y) : Z × Y →[0, 1] with Lipschitz constant L w.r.t the input z.\nTheorem 9. (Rademacher Complexity Bound) Consider a Lipschitz continuous loss function ℓ(z, y) : Z × Y →\n[0, 1] with Lipschitz constant L w.r.t the input z. Let eℓ(z, y) := ℓ(z, y)−ℓ(0, y). Let bG be the function class of the\nabove (T+1)-layer structured NN mapping from X to Z. Suppose the activation function |h(y)| ≤|y| (element-\nwise), and the l2-norm of last layer weight is bounded, i.e., ∥w∥2 ≤Bw. Let (xi, yi)N\ni=1 be i.i.d. samples drawn\nfrom X × Y. Let Y T+1 = [y(1)\nT+1, · · · , y(N)\nT+1] be the T th layer output with input X. Denote the mutual coherence\nof Y T+1 as µ∗, i.e., µ∗= µ(Y T+1) = maxi̸=j\ny(i)⊤\nT+1 y(j)\nT+1\n∥y(i)\nT+1∥2∥y(j)\nT+1∥2 ≤1. Then, we have\nRN(eℓ◦bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵieℓ(g(xi), yi)\n#\n≤\nLBw\nq\nT\n\u0000(N −1)µ∗+ 1\n\u0001\nN\n∥X∥F\n(205)\nwhere X = [x1, · · · , xN]. ∥· ∥2 and ∥· ∥F denote the spectral norm and the Frobenius norm of input matrix,\nrespectively.\nRemark: The Rademacher complexity bound has a complexity O(\n√\nT) w.r.t. the depth of NN.\nProof. Since eℓis L-Lipschitz continuous function, from the composition rule of Rademacher complexity, we know\nthat\nRN(eℓ◦bG) ≤L RN( bG)\n(206)\nNeural Optimization Kernel: Towards Robust Deep Learning\nIt follows that\nRN( bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵig(xi)\n#\n(207)\n= 1\nN E\n\"\nsup\nw,{Dt∈M}T\nt=1\nN\nX\ni=1\nϵi\n\nw, y(i)\nT+1\n\u000b\n#\n(208)\n= 1\nN E\n\"\nsup\nw,{Dt∈M}T\nt=1\n\nw,\nN\nX\ni=1\nϵiy(i)\nT+1\n\u000b\n#\n(209)\n≤1\nN E\n\"\nsup\nw,{Dt∈M}T\nt=1\n∥w∥2\n\r\r\nN\nX\ni=1\nϵiy(i)\nT+1\n\r\r\n2\n#\n(Cauchy-Schwarz inequality)\n(210)\n≤Bw\nN E\n\"\nsup\n{Dt∈M}T\nt=1\n\r\r\nN\nX\ni=1\nϵiy(i)\nT+1\n\r\r\n2\n#\n(211)\n= Bw\nN E\n\n\nsup\n{Dt∈M}T\nt=1\nv\nu\nu\nt\nN\nX\ni=1\n\r\rϵiy(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(212)\n= Bw\nN E\n\n\nsup\n{Dt∈M}T\nt=1\nv\nu\nu\nt\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(213)\n= Bw\nN E\n\n\nv\nu\nu\nt\nsup\n{Dt∈M}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(214)\n≤Bw\nN\nv\nu\nu\nu\ntE\n\n\nsup\n{Dt∈M}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(215)\nInequality (215) is because of the Jensen inequality and concavity of the square root function.\nNote that |ϵi| = 1, ∀i ∈{1, · · · , N}, and the mutual coherence of Y T+1 is µ∗, i.e., µ∗= µ(Y T+1) ≤1, it follows\nthat\nRN( bG) ≤Bw\nN\nv\nu\nu\nu\ntE\n\n\nsup\n{Dt∈M}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\nϵiϵjy(i)⊤\nT+1 y(j)\nT+1\n\n\n(216)\n≤Bw\nN\nv\nu\nu\nt\nsup\n{Dt∈M}T\nt=1\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 +\nN\nX\ni=1\nN\nX\nj=1,j̸=i\n\r\ry(i)\nT+1\n\r\r\n2\n\r\ry(j)\nT+1\n\r\r\n2µ∗\n(217)\n= Bw\nN\nv\nu\nu\nt\nsup\n{Dt∈M}T\nt=1\n(1 −µ∗)\nN\nX\ni=1\n\r\ry(i)\nT+1\n\r\r2\n2 + µ∗\u0000 N\nX\ni=1\n\r\ry(i)\nT+1\n\r\r\n2\n\u00012\n(218)\n≤Bw\nN\nr\nsup\n{Dt∈M}T\nt=1\n(1 −µ∗)∥Y T+1∥2\nF + Nµ∗∥Y T+1∥2\nF Cauchy-Schwarz\n(219)\n≤Bw\nN\nr\nsup\n{Dt∈M}T\nt=1\n\u0000(N −1)µ∗+ 1\n\u0001\n∥Y T+1∥2\nF\n(220)\nwhere Y T+1 = [y(1)\nT+1, · · · , y(N)\nT+1] and ∥· ∥F denotes the Frobenius norm.\nSince |h(Y )| ≤|Y | (element-wise), (e.g., ReLU, max-pooling, soft-thresholding), it follows that\n∥Y T+1∥2\nF = ∥h\n\u0000D⊤\nT X + (I −D⊤\nT DT )Y T\n\u0001\n∥2\nF\n(221)\n≤∥D⊤\nT X + (I −D⊤\nT DT )Y T ∥2\nF\n(222)\nYueming Lyu, Ivor Tsang\nIn addition, we have\n∥D⊤\nT X + (I −D⊤\nT DT )Y T ∥2\nF = ∥D⊤\nT X∥2\nF + ∥(I −D⊤\nT DT )Y T ∥2\nF + 2\n\nD⊤\nT X, (I−D⊤\nT DT )Y T\n\u000b\n(223)\nNote that DT D⊤\nT = Id, we have\n∥D⊤\nT X∥2\nF = ∥X∥2\nF\n(224)\n\nD⊤\nT X, (I−D⊤\nT DT )Y T\n\u000b\n= tr\n\u0010\nX⊤\nT DT (I−D⊤\nT DT )Y T\n\u0011\n= 0\n(225)\nIt follows that\n∥D⊤\nT X + (I −D⊤\nT DT )Y T ∥2\nF = ∥D⊤\nT X∥2\nF + ∥(I −D⊤\nT DT )Y T ∥2\nF + 2\n\nD⊤\nT X, (I−D⊤\nT DT )Y T\n\u000b\n(226)\n= ∥X∥2\nF + ∥(I −D⊤\nT DT )Y T ∥2\nF\n(227)\n≤∥X∥2\nF + ∥I −D⊤\nT DT ∥2\n2∥Y T ∥2\nF = ∥X∥2\nF + ∥yT ∥2\nF\n(228)\nRecursively apply the above procedure from t = T to t = 1, together with Y 0 = 0, we can achieve that\n∥Y T+1∥2\nF ≤T∥X∥2\nF\n(229)\nTogether with inequality (220), it follows that\nRN( bG) ≤Bw\nN\nr\nsup\n{Dt∈M}T\nt=1\n\u0000(N −1)µ∗+ 1\n\u0001\n∥Y T+1∥2\nF\n(230)\n≤\nBw\nq\nT\n\u0000(N −1)µ∗+ 1\n\u0001\nN\n∥X∥F\n(231)\nFinally, we obtain that\nRN(eℓ◦bG) = 1\nN E\n\"\nsup\ng∈b\nG\nN\nX\ni=1\nϵieℓ(g(xi), yi)\n#\n≤\nLBw\nq\nT\n\u0000(N −1)µ∗+ 1\n\u0001\nN\n∥X∥F\n(232)\nTheorem 10. Consider a Lipschitz continuous loss function ℓ(z, y) : Z × Y →[0, 1] with Lipschitz constant L\nw.r.t the input z. Let eℓ(z, y) := ℓ(z, y) −ℓ(0, y). Let bG be the function class of our general (T+1)-layer structured\nNN mapping from X to Z. Suppose the activation function |h(y)| ≤|y| (element-wise), and the l2-norm of last\nlayer weight is bounded, i.e., ∥w∥2 ≤Bw. Let (xi, yi)N\ni=1 be i.i.d. samples drawn from X × Y. Let Y T+1 be the\nT th layer output with input X. Denote the mutual coherence of Y T+1 as µ∗, i.e., µ∗= µ(Y T+1) ≤1. Then, for\n∀N and ∀δ, 0 < δ < 1, with a probability at least 1 −δ, ∀g ∈bG, we have\nE\n\u0002\nℓ(g(X), Y )\n\u0003\n≤1\nN\nN\nX\ni=1\nℓ(g(xi), yi) +\nLBw\nq\nT\n\u0000(N −1)µ∗+ 1\n\u0001\nN\n∥X∥F +\nr\n8 ln(2/δ)\nN\n(233)\nwhere X = [x1, · · · , xN], and ∥· ∥F denotes the Frobenius norm.\nProof. Plug the Rademacher complexity bound of general structured NN (our Theorem 9) into the Theorem 8\nin [5], we can obtain the bound.\nK\nExperimental Results on Classiﬁcation with Gaussian Input Noise and\nLaplace Input Noise\nNeural Optimization Kernel: Towards Robust Deep Learning\n(a) DenseNet-Clean\n(b) DenseNet-Gaussian-0.1\n(c) DenseNet-Gaussian-0.2\n(d) DenseNet-Gaussian-0.3\n(e) ResNet-Clean\n(f) ResNet-Gaussian-0.1\n(g) ResNet-Gaussian-0.2\n(h) ResNet-Gaussian-0.3\nFigure 3:\nMean test accuracy ± std over 5 independent runs on CIFAR10 dataset with Gaussian noise for\nDenseNet and ResNet backbone\n(a) DenseNet-Clean\n(b) DenseNet-Gaussian-0.1\n(c) DenseNet-Gaussian-0.2\n(d) DenseNet-Gaussian-0.3\n(e) ResNet-Clean\n(f) ResNet-Gaussian-0.1\n(g) ResNet-Gaussian-0.2\n(h) ResNet-Gaussian-0.3\nFigure 4:\nMean test accuracy ± std over 5 independent runs on CIFAR100 dataset with Gaussian noise for\nDenseNet and ResNet backbone\nYueming Lyu, Ivor Tsang\n(a) DenseNet-Clean\n(b) DenseNet-Laplace-0.1\n(c) DenseNet-Laplace-0.2\n(d) DenseNet-Laplace-0.3\n(e) ResNet-Clean\n(f) ResNet-Laplace-0.1\n(g) ResNet-Laplace-0.2\n(h) ResNet-Laplace-0.3\nFigure 5:\nMean test accuracy ± std over 5 independent runs on CIFAR10 dataset with Laplace noise for\nDenseNet and ResNet backbone\n(a) DenseNet-Clean\n(b) DenseNet-Laplace-0.1\n(c) DenseNet-Laplace-0.2\n(d) DenseNet-Laplace-0.3\n(e) ResNet-Clean\n(f) ResNet-Laplace-0.1\n(g) ResNet-Laplace-0.2\n(h) ResNet-Laplace-0.3\nFigure 6:\nMean test accuracy ± std over 5 independent runs on CIFAR100 dataset with Laplace noise for\nDenseNet and ResNet backbone\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2021-06-11",
  "updated": "2021-12-01"
}