{
  "id": "http://arxiv.org/abs/2106.06536v1",
  "title": "Unsupervised Neural Hidden Markov Models with a Continuous latent state space",
  "authors": [
    "Firas Jarboui",
    "Vianney Perchet"
  ],
  "abstract": "We introduce a new procedure to neuralize unsupervised Hidden Markov Models\nin the continuous case. This provides higher flexibility to solve problems with\nunderlying latent variables. This approach is evaluated on both synthetic and\nreal data. On top of generating likely model parameters with comparable\nperformances to off-the-shelf neural architecture (LSTMs, GRUs,..), the\nobtained results are easily interpretable.",
  "text": "Unsupervised Neural Hidden Markov Models\nwith a Continuous latent state space\nFiras JARBOUI\nANEO\nCentre Borelli - ENS Paris-saclay\nfirasjarboui@gmail.com\nVianey PERCHET\nCriteo AI Lab\nCrest, ENSAE\nvianney.perchet@normalesup.org\nAbstract\nWe introduce a new procedure to neuralize unsupervised Hidden Markov Models\nin the continuous case. This provides higher ﬂexibility to solve problems with\nunderlying latent variables. This approach is evaluated on both synthetic and real\ndata. On top of generating likely model parameters with comparable performances\nto off-the-shelf neural architecture (LSTMs, GRUs,..), the obtained results are\neasily interpretable.\n1\nIntroduction\nProbabilistic Graphical Models (PGM) are standard techniques to discover highly interpretable hidden\nstructure in data [15, 5]. Even with sequential data, off-the-shelf PGMs such as Hidden Markov\nModels (HMMs) are able to uncover latent patterns, as assessed by recent breakthroughs in Natural\nLanguage Processing [14, 20], Speech Recognition [21, 24] and Reinforcement Learning [3, 10, 7].\nUsing ﬂexible parametric probability distributions to model the dynamics of PGMS yields better and\nmore accurate results [12]. Given that deep neural networks are universal approximators that can\neventually learn highly expressive non-convex functions [6], PGMs were recently and successfully\ninterfaced with Artiﬁcial Neural Networks (ANNs) [8, 11]. This motivates to further bridge neural\nnetworks with HMMs, as already attempted back in the 90s [1]. Indeed, hybrid HMM/ANN models\nwith better accuracy have been created by extracting features using neural networks : typical examples\ninclude word embeddings for unsupervised word alignment [22], convolutional neural networks for\nspeech recognition [17], etc.\nA generic approach [23] to parameter estimation in ﬁnite latent space neural HMMs, suggests to\nmodel the parameters with neural networks. In that case, gradients are back-propagated with respect\nto the neural network parameters during the maximisation step (M) of the Baum-Welch algorithm\n(the Viterbi algorithm is used in the E-step). Our main objective is to introduce a new procedure for\nthe continuous latent state space case with a similar ﬂavor.\nMore precisely, we will consider HMMs with continuous latent spaces as they encompass a very\nlarge class of non-parametric models well adapted to sequential processes. However, they were\nrecently outperformed by recurrent neural networks (RNNs), which led to some decreasing interest\n[18]. On the other hand, they are highly interpretable, a key property not necessarily shared with\nRNN. Our main contribution is to advocate that using neural networks to estimate the parameters of\nthe transition and emission probabilities of an HMM improves its performance up to be comparable\nto RNNs, while retaining the interpretability of the latent variables, therefore reaching the best of\nboth worlds: performances and interpretability.\nIn details, we will investigate the theoretical framework of SMC methods to construct an optimisation\nalgorithm that maximises the log-likelihood of the observations with respect to the model neural\nPreprint. Under review.\narXiv:2106.06536v1  [cs.LG]  10 Jun 2021\nnetwork parameters. Driven by the successes of stochastic gradient descent, we also provide a\nstochastic and more efﬁcient variant that preserves its experimental effectiveness.\nOur methods are evaluated both on simulated and real data. The toy example is the simulation of\nan HMM and, without exploiting prior knowledge of the environment, our algorithm solves the\nparameters estimation problem. The two real-life applications use data from tourism and music.\nTheir analysis showcase the simplicity of the implementation of our methods and, more importantly,\nstrongly highlight the interpretability of the obtained results.\n2\nNeural Hidden Markov Model\nIn order to deﬁne ANN-based HMM, we consider two neural networks fθ and gθ. To alleviate\nnotations, we use θ to denote the parameters of both ANN, even though the two networks do not\nnecessarily share parameters. Without loss of generality, we consider a latent space X = Rdh and\nan observation space Y = Rdo. Otherwise, an additional embedding layer would do the trick. The\nneural-equivalent version of HMM models (c.f. appendix A for a review of HMM models) writes as:\nx0 ∼µ(x);\nyt ∼pgθ(xt)(y);\nxt+1 ∼qfθ(xt)(x)\n(1)\nIn practical applications, the objectives are to optimise the model parameters for multiple independent\ntrajectories (yj\n0:T )K\nj=0. Thus, the log-likelihood of the observations under a given θ can be written as:\nL(θ, (yj\n0:T )K\nj=0) = P\nj Lj(θ, yj\n0:T );\nLj(θ, yj\n0:T ) = log P(yj\n0:T |θ)\n(2)\nEquation (1) introduces non-linearity in the system, hence SMC approaches are well suited to\noptimise L. Inspired by EM algorithms (see Appendix A.2), the particle ﬁlter from Algorithm 1 can\nbe used to construct an asymptotically unbiased estimator of the Q function.\nUnfortunately, the maximiser of the following function ˆQ, that approximates Q, does not have an\nexplicit form in the case of the neural HMM (in Equation (1)), because the emission and transition\nprobabilities are no longer within the exponential family. This function ˆQ is deﬁned by\nˆQ(θ, θk) = P\ni,j,t W i,j\nT\n\u0002\nlog\n\u0000pgθ( ˆ\nXi,j\nt\n)(yj\nt )\n\u0001\n+ log\n\u0000qfθ( ˆ\nXi,j\nt−1)( ˆXi,j\nt )\n\u0001\u0003\n,\n(3)\nwhere ˆXi,j\nt\nis the ith particle generated at time t for the jth trajectory using an SMC method and yj\nt is\nthe associated observation. However, it is possible, using Equation (12), to prove that a candidate\nparameter θk+1 must satisfy:\nˆQ(θk+1, θk) ≥ˆQ(θk, θk)\n(4)\nAs long as the probability distribution functions associated with the emission and transition probabil-\nities are differentiable, the gradient of ˆQ is well deﬁned. Moreover, functions of the form of Equation\n(3) are well suited for stochastic gradient-based (SGD) optimisation. In fact, the loss function l\nintroduced in Equation (5) for a given sample s = (yj\nt , ˆXi,j\nt , ˆXi,j\nt−1) can be used to construct a new\nparameter θk+1, improving the likelihood:\nlθk(s)(θ) = W i,j\nT\n\u0002\nlog\n\u0000pgθ( ˆ\nXi,j\nt\n)(yj\nt )\n\u0001\n+ log\n\u0000qfθ( ˆ\nXi,j\nt−1)( ˆXi,j\nt )\n\u0001\u0003\n.\n(5)\nOptimising the log-likelihood with EM requires the evaluation of the posterior distribution approxi-\nmation through the estimation of\n\u0002\n(Xi,j\n0:T ), (W i,j\nT )\n\u0003N,K\ni,j=0 in the E-step, and maximising the ˆQ function\nin the M-step. Equation (6) provides an iterative algorithm that solves neural HMMs:\n\u001a\n( ˆXi,j\nt )i,j,t, (W i,j\nT )i,j = SMC(θk, (Y j\nt )j,t)\nθk+1 = SGD(l, θk, ( ˆXi,j\nt )i,j,t, (W i,j\nT )i,j, (Y j\nt )j,t)\n(6)\n2.1\nReducing computational complexity\nA drawback of the EM algorithm is that the (E) step is computationally expensive. The main reason\nis that Equation (6) requires to run the particle ﬁlter over the full data set. When the available\ntrajectories share a lot of similarities, we suggest that this is not necessary. An alternative is to\nrandomly sample a batch of trajectories on which we run the optimisation. This obviously leads to\nfaster convergence. The downside is that the same performance might not be reached; however, this\nis highly mitigated with a few steps at the end of training where we use the full data set in order to\nﬁne tune the parameters. We will discuss this further in the following section.\n2\n3\nExperiments\nWe now provide experimental results to assess the relevancy and efﬁciency of our approach. In the\nsynthetic example (where data are generated by some HMM), the environment is a set of target\npositions in a d-dimensional space, and the observations are the trajectories of a particle bouncing\naround them (a description of the data generating process is provided in Appendix C.1). We compare\nthe performance of our parameter inference scheme with classical approaches to train HMMs as well\nas off the shelf neural network architectures (simple RNNs, LSTMs, GRUs,..). This establishes the\nability of neural HMMs to provide comparable results to RNNs. In the real life examples, we use\ntourists GPS tracks in an open air museum, and guitar chords recordings. Experimental results of\nthose experiments are provided in the Appendix. Based on those empirical results, we can establish\nthat neural networks does not affect the ability of HMMs to infer interpretable latent states !\nDepth of the Neural Network\nWe compare the performance of different HMM models on the task\nof predicting the next observation. In the continuous HMM, the probability distributions used for\nthe emission and transition model are Gaussian. The functions fθ and gθ are n-layer deep neural\nnetwork used to estimate the mean and variance of the distribution, with n designating the depth of\nthe neural network. The case where n = 0 is associated to vanilla HMMs where the parameters are\nlearned using Equation (16). For the other depth values, we use Equation (6). In discrete HMMs, the\ntransition probability distribution is a multinomial over {1, 2, 3, 4, 5}. fθ is an n-layer neural network\npredicting the multinomial probabilities. In the case where n = 0, we use Baum-Welch to learn the\nparameters, otherwise we use the neural version provided in [23].\nIn Figure 1a, we observe that using ANN to learn the HMM parameters yields an improved perfor-\nmance. The obtained results from the continuous HMM case are comparable to the ones obtained\nfrom the discrete case. Deeper neural networks provided higher log-likelihoods and perform better\non the one step prediction task.\nLatent space dimension:\nLet us now compare neural HMM performance (where the parameters\nof the emission and transition probabilities are predicted with a 3-layer ANNs) to off-the-shelf neural\nnetwork architecture used for sequence prediction (where a RNN’s output is fed to a 3-layer deep\nANN). We compare performances for different latent space dimensions. In the HMM case, this\ncorresponds to dh, and in the other cases, it corresponds to the output dimension of the recurrent\nlayer. In Figure 1b, we observe that at higher latent space dimension, all the models have comparable\nperformances. However, at lower dimension, the neural HMM outperforms by a wide margin all the\nother off-the-shelf methods. This experiment is re-conducted with higher feature dimensions d. The\nobtained results, presented in Table 1, conﬁrm this statement. These results indicate that there is a\ntrade-off between the interpretability of the latent variables and the accuracy of the prediction to be\ntaken into account when choosing an appropriate model. Even though recurrent neural architecture\nhave wildly replaced HMMs in the recent years, we believe neural HMM should mitigate this trend.\nTable 1: Prediction error for various feature and latent space dimensions.\nFEATURE DIM (d)\nMODELS\ndh = 2\ndh = 3\ndh = 4\ndh = 5\ndh = 10\ndh = 20\nNEURAL HMM\n.65±.17\n.64±.17\n.66±.16\n.63±.16\n.65±.19\n.66±.18\nHMM\n.92±.20\n.82±.16\n.82±.18\n.76±.13\n.71±.15\n.71±.13\n3\nRNN\n.86±.06\n.83±.06\n.82±.07\n.73±.10\n.64±.09\n.56±.10\nLSTM\n.71±.10\n.71±.08\n.75±.09\n.60±.09\n.56±.10\n.54±.10\nGRU\n.82±.06\n.82±.08\n.73±.08\n.67±.08\n.56±.08\n.54±.09\nNEURAL HMM\n.76±.13\n.77±.12\n.80±.13\n.75±.14\n.78±.13\n.78±.13\nHMM\n.98±.13\n.93±.11\n.87±.11\n.87±.10\n.81±.12\n.81±.10\n5\nRNN\n.98±.01\n.94±.02\n.86±.06\n.83±.07\n.71±.07\n.65±.07\nLSTM\n.84±.07\n.79±.07\n.76±.08\n.76±.07\n.61±.07\n.61±.07\nGRU\n.87±.04\n.86±.04\n.80±.05\n.75±.05\n.66±.06\n.59±.07\nNumber of particles:\nThe performance of the algorithm provided in Equation (6) depends on the\naccuracy of the Q function approximation. The performance of the used particle ﬁlter increases as the\nnumber of particles increases as stated in Equation (14). A better approximation allows Equation (6)\n3\n(a) Impact of the depth\n(b) Impact of the latent space dimension\n(c) Impact of the number of particles\n(d) Impact of the % of the data set used\nFigure 1: Performance analysis of neural HMM as a function of different parameters\nto converge to better parameters at the cost of a longer training time. This improvement is however\nbounded. Increasing the number of particles increases linearly the computational load whereas it\nimproves logarithmically the performance. In Figure 1c, we observe how increasing the number\nof particles from 8 to 128 improves the performances signiﬁcantly, whereas increasing the number\nfrom 128 to 512 have a marginal impact on the prediction error. The same happens to the average\nlog-likelihood. These experiments were conducted with a neural HMM with 3-layers ANN parameter\nestimators and with a 5-dimension latent space.\nReducing computational complexity:\nIn order to evaluate the impact of applying iteratively\nEquation (6) on randomly sampled subsets instead of the full data set, we use the same setting with\nhigher feature dimension (d = 3). we compare the prediction error reached after 100 iteration for\ndifferent proportions of the data set sampled randomly in the E step. We also evaluate the same error\nafter ﬁne tuning the parameter with 5 iterations using the whole data set. The conclusion we draw\nfrom Figure 1d is that randomly sampling trajectories before applying Equation (6) does not affect a\nlot the average performance of the obtained parameters. However, the variance of the error is affected.\nThe ﬁne tuning steps can mitigate this issue and result in an improved performance. This procedure is\nespecially useful in settings where an extremely large amount of data is available, such as the guitar\nchords example that we consider in the last section.\n4\nConclusion\nNeural Hidden Markov Models improve the performance of classical HMMs up to standard RNNs’\nwhile providing the same level of latent variable interpretability; thus achieving the best of both\nworlds. Our main contributions consist in generalising the classical approaches of HMMs training\nto the neural case and in providing alternatives when the data set is too large. On top of this,\nimplementing neural HMM is a relatively easy task given the available deep learning frameworks.\nThe same approach presented in this paper can be adapted to more complex probabilistic graphical\nmodels. This is a big step toward reducing gaps between PGMs and RNNs, as it renders HMMs an\noff-the-shelf tool for practitioner and mitigates the tendency to overlook PGMs in real life applications\nwhere explainability is key.\n4\nReferences\n[1] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. Global optimization of a neural network-\nhidden markov model hybrid. In IJCNN-91-Seattle International Joint Conference on Neural\nNetworks, volume 2, pages 789–794. IEEE, 1991.\n[2] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data\nvia the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological),\n39(1):1–22, 1977.\n[3] W. Ding, S. Li, H. Qian, and Y. Chen. Hierarchical reinforcement learning framework towards\nmulti-agent navigation. In 2018 IEEE International Conference on Robotics and Biomimetics\n(ROBIO), pages 237–242. IEEE, 2018.\n[4] R. Egger and C. Maurer. Iscontour 2017: Tourism Research Perspectives. BoD–Books on\nDemand, 2017.\n[5] D. Gunning. Explainable artiﬁcial intelligence (xai). Defense Advanced Research Projects\nAgency (DARPA), nd Web, 2, 2017.\n[6] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal\napproximators. Neural networks, 2(5):359–366, 1989.\n[7] F. Jarboui, C. Gruson-Daniel, A. Durmus, V. Rocchisani, S.-H. G. Ebongue, A. Depoux,\nW. Kirschenmann, and V. Perchet. Markov decision process for mooc users behavioral inference.\nIn European MOOCs Stakeholders Summit, pages 70–80. Springer, 2019.\n[8] M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R. Datta. Composing\ngraphical models with neural networks for structured representations and fast inference. In\nAdvances in neural information processing systems, pages 2946–2954, 2016.\n[9] N. Kantas, A. Doucet, S. S. Singh, J. Maciejowski, and N. Chopin. On particle methods for\nparameter estimation in state-space models. Statistical science, 30(3):328–351, 2015.\n[10] S. Katt, F. A. Oliehoek, and C. Amato. Bayesian reinforcement learning in factored pomdps.\nIn Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent\nSystems, pages 7–15. International Foundation for Autonomous Agents and Multiagent Systems,\n2019.\n[11] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[12] P. Larrañaga, H. Karshenas, C. Bielza, and R. Santana. A review on probabilistic graphical\nmodels in evolutionary computation. Journal of Heuristics, 18(5):795–819, 2012.\n[13] R. Laubenfels. Feynman–kac formulae: Genealogical and interacting particle systems with\napplications, 2005.\n[14] H. Murveit and R. Moore. Integrating natural language constraints into hmm-based speech\nrecognition. In International Conference on Acoustics, Speech, and Signal Processing, pages\n573–576. IEEE, 1990.\n[15] A. T. Nguyen, A. Kharosekar, M. Lease, and B. Wallace. An interpretable joint graphical model\nfor fact-checking from crowds. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,\n2018.\n[16] J. Osmalsky, J.-J. Embrechts, M. Van Droogenbroeck, and S. Pierard. Neural networks for\nmusical chords recognition. In Journees d’informatique Musicale, pages 39–46, 2012.\n[17] D. Palaz, M. Magimai-Doss, and R. Collobert. End-to-end acoustic modeling using convolu-\ntional neural networks for hmm-based automatic speech recognition. Speech Communication,\n108:15–32, 2019.\n[18] M. Panzner and P. Cimiano. Comparing hidden markov models and long short term memory\nneural networks for learning action representations. In International Workshop on Machine\nLearning, Optimization, and Big Data, pages 94–105. Springer, 2016.\n[19] M. K. Pitt and N. Shephard. Filtering via simulation: Auxiliary particle ﬁlters. Journal of the\nAmerican statistical association, 94(446):590–599, 1999.\n[20] N. Ranjan, K. Mundada, K. Phaltane, and S. Ahmad. A survey on techniques in nlp. Interna-\ntional Journal of Computer Applications, 134(8):6–9, 2016.\n5\n[21] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco. Connectionist probability\nestimators in hmm speech recognition. -, 1994.\n[22] T. Songyot and D. Chiang. Improving word alignment using word similarity. In Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n1840–1845, 2014.\n[23] K. Tran, Y. Bisk, A. Vaswani, D. Marcu, and K. Knight. Unsupervised neural hidden markov\nmodels. arXiv preprint arXiv:1609.09007, 2016.\n[24] S. Xue, H. Jiang, L. Dai, and Q. Liu. Speaker adaptation of hybrid nn/hmm model for speech\nrecognition based on singular value decomposition. Journal of Signal Processing Systems,\n82(2):175–185, 2016.\nA\nMathematical Background\nA.1\nHidden Markov Models with Continuous latent state space\nLet us describe formally HMMs. They are deﬁned by some latent state space X and observation space\nY. Two sequences of observations (yt)T\nt=0 ∈Y and latent states (xt)T\nt=0 ∈X, where T ∈N, are\ngenerated as follows. The ﬁrst state x0 is chosen at random according to some underlying distribution\nµ. Then the ﬁrst observation y0 is drawn accordingly to pθ(·|x0), a probability distribution over Y.\nThis distribution is called the emission probability distribution. The dynamics of xt is then controlled\nby the transition probability distribution qθ(·|xt−1) as in a standard Markov chain. Formally, variables\nsatisfy the following equation:\nx0 ∼µ(x);\nyt ∼pθ(y|xt);\nxt+1 ∼qθ(x|xt)\n(7)\nThe estimation of the model hidden parameters is usually done by the optimisation of the log-\nlikelihood of the observations with respect to the model parameters, i.e.:\nθ∗= argmaxθ L(θ, (yt)T\nt=0) = argmaxθ log\n\u0000P(y0:T |θ)\n\u0001\n.\n(8)\nAs usual, the ﬁrst step to compute L is to marginalise this quantity with respect to the latent variables:\nP(y0:T |θ) =\nZ\nP(y0:T , x0:T |θ) dx0:T ,\n(9)\nwhere the joint distribution can be rewritten into:\nP(y0:T , x0:T |θ) = µ(x0) ×\nT\nY\nt=0\n\u0002\npθ(yt|xt) × qθ(xt+1|xt)\n\u0003\n.\n(10)\nA popular choice to maximise the log likelihood L is the class of EM algorithms. Given the\ninitialisation θ0, a sequence of estimated parameters is constructed as follows:\nθk+1 = argmaxθ Q(θ, θk);\nQ(θ, θk) =\nR\nP(x:T |y:T , θk) log P(y:T , x:T |θ) dx:T\n(11)\nThe key property of EM techniques is that the sequence (L(θk, (yt)T\nt=0))k is non decreasing. In fact\na stronger result [2] even states that:\n(L(θ,(yt)T\nt=0))k ≥(L(θk, (yt)T\nt=0))k ⇐⇒Q(θ, θk) ≥Q(θk, θk)\n(12)\nMoreover, the EM algorithm, is favored to gradient based methods for its numerical stability [2].\nA.2\nParticle Filters for HMMs\nIn order to optimise the log-likelihood with respect to θ, see Equation (11), the posterior distribution\nof the latent states P(x0:T |y0:T , θ) must be evaluated. We propose to do this with particle ﬁlters,\na class of SMC methods. The principle is to generate N latent state candidates, re-sample them\naccording to their likelihood in the model, and use them to generate N candidate latent states for the\nnext time step. By sampling likely trajectories, and weighing them according to their corresponding\nlikelihood, a good approximation of the posterior is constructed. Various implementation of the\nparticle ﬁlters can be considered [19].\n6\nAlgorithm 1 The bootstrap ﬁlter\n1: Procedure: SMC(θ, (Yt)T\nt=0)\n2: Xi\n0 ∼µθ ∀i ∈[0, N]\n3: W i\n0 ∝P(Y0|Xi\n0, θ) × µθ(Xi\n0) s.t. P\ni W i\n0 = 1\n4: ˆXi\n0 ∼P\ni W i\n0δXi\n0\n5: for t ∈[1, T] do\n6:\nXi\nt ∼P(x| ˆXi\nt−1, θ) ∀i ∈[0, N]\n7:\nW i\nt ∝P(Yt|Xi\nt, θ) × W i\nt−1 s.t. P\ni W i\nt = 1\n8:\nˆXi\n0:t ∼P\ni W i\nt δXi\n0:t\n9: end for\n10: Return: ( ˆXi\n0:T )N\ni=0, (W i\nT )N\ni=0\nThe pseudo-code of the bootstrap variation of these ﬁlters is presented in Algorithm 1. We have\ndenoted by (Xi\n0:T )N\ni=0 the latent trajectories created by the particle ﬁlters, and by (W i\nT )N\ni=0, their\nweights; those two quantities are the keystones of the posterior distribution. Indeed, an asymptotically\nunbiased estimator is deﬁned by [13, 9]:\nˆP(x0:T |y0:T , θ) =\nX\ni\nW i\nT δ ˆ\nXi\n0:T (dx0:T ).\n(13)\nA key property of the particles based estimation of the posterior function is that it can also be used to\nconstruct asymptotically unbiased estimates of smooth additive functionals of the following form\n[13]:\nSθ\nT =\nZ\nlog\n\u0002 T\nY\nt=0\nst(xt−1:t)\n\u0003\nP(x0:T |y0:T , θ) dx0:T\nSubstituting the posterior distribution with its estimate gives an approximation ˆSθ\nT satisfying [13]:\n\f\fE\n\u0002 ˆSθ\nT\n\u0003\n−Sθ\nT\n\f\f ≤Fθ\nT\nN , Fθ ∈R∗,\n(14)\nwhere the expectation is with respect to the particle ﬁlter. Equation (10) implies that the Q function\ndeﬁned in Equation (11) has the same form as Sθ\nT . Notice that this corresponds to the special case\nwhere s0 = µ(x0) and st+1(xt:t+1) = pθ(yt|xt)×qθ(xt+1|xt). As a consequence, an asymptotically\nunbiased estimator of the Q function is:\nˆQ(θ, θk) =\nX\ni\nW i\nT log P(y0:T , ˆXi\n0:T |θ).\n(15)\nThis optimises the log-likelihood L using an approximation of the EM algorithm presented in\nEquation (11) by computing at the kth iteration argmaxθ ˆQ(θ, θk).\nA.3\nImplementation\nIn practice, when pθ(y|xt) and qθ(x|xt) are chosen from the exponential family, solving an HMM\nwith the EM algorithm boils down to computing a summary statistic of the Q(θ, θk) function\nusing a particle ﬁlter. The maximising argument of ˆQ(θ, θk) can be explicitly computed through a\nsuitable function Λ((Yt)T\nt=0, ( ˆXi\n0:T )N\ni=0, (W i\nT )N\ni=0). The standard open source HMM solvers (such\nas pomegranate and HMMlearn) propose the following algorithm:\n\u001a\n( ˆXi\n0:T )N\ni=0, (W i\nT )N\ni=0 = SMC(θk, (Yt)T\nt=0)\nθk+1 = Λ((Yt)T\nt=0, ( ˆXi\n0:T )N\ni=0, (W i\nT )N\ni=0)\n(16)\nB\nHigher order Neural HMMs\nHMMS can also easily incorporate bounded memory processes. Indeed, let τe, τt ≥0 be two memory\nwindow sizes, then Equation (7) can be transformed into:\nx0 ∼µ(x);\nyt ∼pgθ(xt,yt−τe:t−1)(y);\nxt+1 ∼qfθ(xt,yt−τt:t)(x)\n(17)\n7\nThis model can be seen as a neural HMM of order (τe, τt). All the previous theoretical results hold\ntrue, in particular Equation (6) can be used to optimise the parameters of this model by rewriting the\nloss function from Equation (5) as:\nlθk(s)(θ) = W i,j\nT\n\u0002\nlog\n\u0000pgθ( ˆ\nXi,j\nt\n,yj\nt−τe:t−1)(yj\nt )\n\u0001\n+ log\n\u0000qfθ( ˆ\nXi,j\nt−1,yj\nt−τt:t)( ˆXi,j\nt )\n\u0001\u0003\n(18)\nThis enables HMMs to piggy-ride on all the progress that Deep Learning is currently undergoing.\nC\nComplementary experimental results\nC.1\nSynthetic setting: data generating process\nWe consider a set of targets (Xi)N\n1 . Particles move toward their current targets Xit: the direction of\nthe movement is sampled according to a Gaussian N(Xit, σ). Once a particle is ϵ close to Xit, its\nnew target is sampled according to a uniform distribution, as modelled by Equation (19).\n\n\n\n\n\n\n\nit\n=\n\u001a\n∼U[1,2,..,N]\nif ||yt −Xit−1|| ≤ϵ\nit−1\nif ||yt −Xit−1|| > ϵ\nxt\n∼\nN(Xit, σ)\nyt+1\n=\nyt +\nxt−yt\n||xt−yt||\n(19)\nFigure 2 illustrates an example of the generated trajectories (yt) and latent states (xt), where the\nnumber of components is N = 5 and the feature dimension is d = 2.\nFigure 2: Synthetic data set\nC.2\nReal-life data sets\nThe real strength of using HMMs is the interpretability of the latent states. We explore two real-life\ndata sets in order to provide qualitative proof that this aspect is not lost when using neural HMMs.\nC.2.1\nOpen air museum\nWe consider the data set gathered during a study conducted in an open air museum in Austria [4].\n200 visitors of the Freilicht museum were equipped with GPS-Trackers and their behaviours were\nqualitatively analysed [4]. Given that a visitor’s intention and preferences can explain his path when\nvisiting the museum, the HMM model lend itself to explain the observed trajectories. We use the\ndata from the same study and model the trajectories using Equation (1) where yt is the position of the\ntourist at time t. We use Gaussian distribution for the emission and transition, where the parameters\nare estimated using 3-layer ANNs. The latent variables are encoded in 2 dimensions. Maximising\n8\nthe Log-likelihood of the model using Equation (6) converges in a few iterations. The parameters\nare then used to compute the latent states of a given trajectory. Clustering these latent states using\nK-mean, reveals two visiting behaviours of the museum as illustrated in Figure 3. When reviewing\nthe website of the museum, we discover that the visitors have the choice between discovering the\nattractions by walking or using a railway ride. The train trajectory can be matched to the blue data\npoint and the walking itinerary to the orange ones. Extracting these patterns in an unsupervised way\nusing only the position of the visitors and despite the fact that the walking and the railway paths\nintersect, is a good example of how highly interpretable neural HMMs can be.\nFigure 3: GPS track clusters\nC.2.2\nGuitar chords\nOur objective is to provide a prototype music recognition system that relies on neural HMMs, in an\nunsupervised manner. Let us describe more precisely this example.\nSetting: We limit the analysis to 10 basic guitar chords (A, Am, Bm, C, D, Dm, E, Em, F, G) for\ncomplexity reason, as standard music related research [16]. The objective is to train a classiﬁer that\ndecomposes a melody into a sequence of these chords.\nSuccessful attempts are based on the so-called Pitch Class Proﬁle (PCP) [16]. Roughly speaking, the\nPCP is a set of features obtained through a spectral analysis, followed by a Fourier transform, of the\nsignal, i.e., the melody. Mathematically, it is a mapping from [0, T] to R12. Very roughly speaking, it\nassociates to each instant t ∈[0, T] of the melody different energy levels of the 12 keys of a piano\noctave.\nDataset : A dataset of guitar chords recordings (200 per chord) in various settings (an anechoic\nchamber / a noisy environment), using four different guitars, and with different playing techniques [16]\nis used. These sequences can be seen as random pieces of music.\nIn theory, each chord’s ideal PCP would always activate the same different subsets of harmonics;\nHowever, the surrounding noise, the type of guitar, etc. are many different reasons that explain\nactual variations in PCP. In practice, a chord might not be recoverable directly from a single PCP\nobservation: the associated PCP diagram, may vary depending on the player technique, the order in\nwhich the different notes of a chord are heard.\nneuralHMM: The chord successions can be well modeled by higher order neural HMMs, introduced\nin Equation (17). The observations yt are the PCP while the latent variables are the chords played.\nWe recall that a higher order neural HMM is deﬁned by two hyper-parameters (the window memory)\nand two neural networks. The experimental results corresponds to\n– Hyper-parameters: τe = 1 and τt = 15.\n– Emission network gθ: a two layer neural network (width of 32 neurons)\n– Transition network fθ: an LSTM constructs an embedding of the previous observations yt−τt−1:t−1;\nthey are concatenated with the previous representations and fed to a two layer neural network (same\nwidth of 32 neurons).\nThe optimization of the different neural nets (see Equation (6)) require samples of trajectories; for\nillustration purpose, we ﬁx their lengths to 5 guitar chords (see such a PCP illustrated in Figure 4\n9\ncorresponding to the sequence (’C’, ’E’, ’D’, ’E’, ’F’).) and we choose 5 chord recordings at random\nin the chord data-set. The total number of possible ’chord trajectories’ is prohibitively large, up to\n3.1016, so we either limit its size (for Equation (6)) or we simply use the stochastic variant of the\nalgorithm.\nFigure 4: PCP of a randomly sampled guitar chords\nResults: Performance wise, we observe in Table 2 that neural HMMs out-performs vanilla recurrent\nneural networks when using low latent space dimension. In the following, we will use dh = 5.\nAfter the training, we extract the latent states using Algorithm 1. Going back to the example of the\ntrajectory of Figure 4, we further cluster the associated representations into four classes (number of\nunique chords used in this sample) using K-means. This is illustrated in Figure 5a after a PCA of the\nlatent states coordinate.\n(a) Latent state embedding clustered using K-mean\n(b) Class evaluation over time\nFigure 5: Performance analysis of the regretted detection delay as a function of the cutting threshold\nTable 2: Prediction error for various latent space dimensions.\nMODELS\ndh = 2\ndh = 5\ndh = 10\ndh = 20\nN-HMM\n.30 ± .02\n.28 ± .03\n.31 ± .03\n.33 ± .04\nHMM\n1.04 ± .05\n.65 ± .03\n.46 ± .03\n.56 ± .04\nRNN\n.64 ± .07\n.41 ± .06\n.31 ± .07\n.29 ± .05\nLSTM\n.62 ± .06\n.42 ± .07\n.30 ± .07\n.29 ± .07\nGRU\n.60 ± .05\n.40 ± .07\n.31 ± .08\n.30 ± .08\nTo highlight even more the powerful results of our method, we ﬁlter the audio ﬁle according to the\nidentiﬁed clusters. Figure 5b reveals that each one of them can be associated with a particular chord.\nIndeed, the dashed red lines are associated with the time step where a new guitar chord is being\nplayed, the black line evaluates the cluster label of the audio ﬁle over time according to the learned\nlatent states. Those clusters almost coincide.\n10\nFurthermore, we can even reconstruct the typical PCP of a particular chord. We represent them\nin Figure 6, where the PCP of the latent spaces in the same speciﬁc cluster are concatenated for\nillustration purposes.\nFigure 6: PCP associated to the latent states classes\n11\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-06-10",
  "updated": "2021-06-10"
}