{
  "id": "http://arxiv.org/abs/2106.07035v1",
  "title": "Deep Bayesian Unsupervised Lifelong Learning",
  "authors": [
    "Tingting Zhao",
    "Zifeng Wang",
    "Aria Masoomi",
    "Jennifer Dy"
  ],
  "abstract": "Lifelong Learning (LL) refers to the ability to continually learn and solve\nnew problems with incremental available information over time while retaining\nprevious knowledge. Much attention has been given lately to Supervised Lifelong\nLearning (SLL) with a stream of labelled data. In contrast, we focus on\nresolving challenges in Unsupervised Lifelong Learning (ULL) with streaming\nunlabelled data when the data distribution and the unknown class labels evolve\nover time. Bayesian framework is natural to incorporate past knowledge and\nsequentially update the belief with new data. We develop a fully Bayesian\ninference framework for ULL with a novel end-to-end Deep Bayesian Unsupervised\nLifelong Learning (DBULL) algorithm, which can progressively discover new\nclusters without forgetting the past with unlabelled data while learning latent\nrepresentations. To efficiently maintain past knowledge, we develop a novel\nknowledge preservation mechanism via sufficient statistics of the latent\nrepresentation for raw data. To detect the potential new clusters on the fly,\nwe develop an automatic cluster discovery and redundancy removal strategy in\nour inference inspired by Nonparametric Bayesian statistics techniques. We\ndemonstrate the effectiveness of our approach using image and text corpora\nbenchmark datasets in both LL and batch settings.",
  "text": "Deep Bayesian Unsupervised Lifelong Learning\nTingting Zhao*a, Zifeng Wang*a, Aria Masoomia, Jennifer Dya\naDepartment of Electrical and Computer Engineering, Northeastern University\nAbstract\nLifelong Learning (LL) refers to the ability to continually learn and solve new\nproblems with incremental available information over time while retaining pre-\nvious knowledge. Much attention has been given lately to Supervised Lifelong\nLearning (SLL) with a stream of labelled data. In contrast, we focus on resolving\nchallenges in Unsupervised Lifelong Learning (ULL) with streaming unlabelled\ndata when the data distribution and the unknown class labels evolve over time.\nBayesian framework is natural to incorporate past knowledge and sequentially\nupdate the belief with new data. We develop a fully Bayesian inference frame-\nwork for ULL with a novel end-to-end Deep Bayesian Unsupervised Lifelong\nLearning (DBULL) algorithm, which can progressively discover new clusters\nwithout forgetting the past with unlabelled data while learning latent represen-\ntations. To eﬃciently maintain past knowledge, we develop a novel knowledge\npreservation mechanism via suﬃcient statistics of the latent representation for\nraw data. To detect the potential new clusters on the ﬂy, we develop an au-\ntomatic cluster discovery and redundancy removal strategy in our inference\ninspired by Nonparametric Bayesian statistics techniques. We demonstrate the\neﬀectiveness of our approach using image and text corpora benchmark datasets\nin both LL and batch settings.\nKeywords:\nUnsupervised Lifelong Learning, Bayesian Learning, Deep\nGenerative Models, Deep Neural Networks, Suﬃcient Statistics\n1* signiﬁes equal contribution.\nPreprint submitted to Neural Networks\nJune 17, 2021\narXiv:2106.07035v1  [cs.LG]  13 Jun 2021\n1. Introduction\nWith exposure to a continuous stream of information, human beings are able\nto learn and discover novel clusters continually by incorporating past knowl-\nedge; however, traditional machine learning algorithms mainly focus on static\ndata distributions. Training a model with new information often interferes with\npreviously learned knowledge, which typically compromises performance on pre-\nvious datasets [1]. In order to empower algorithms with the ability of adapting\nto emerging data while preserving the performance on seen data, a new ma-\nchine learning paradigm called Lifelong Learning (LL) has recently gained some\nattention.\nLL, also known as continual learning, was ﬁrst proposed in [2]. It provides a\nparadigm to exploit past knowledge and learn continually by transferring previ-\nously learned knowledge to solve similar but new problems in a dynamic envi-\nronment, without performance degradation on old tasks. LL is still an emerging\nﬁeld and most existing research work [3, 4, 5] has focused on Supervised Life-\nlong Learning (SLL), where the boundaries between diﬀerent tasks are known\nand each task refers to a supervised learning problem with output labels pro-\nvided. The term task can have diﬀerent meanings under various contexts. For\nexample, diﬀerent tasks can represent diﬀerent subsets of classes or labels in the\nsame supervised learning problem [6] and can also represent supervised learn-\ning problems in diﬀerent ﬁelds, where researchers target to perform continual\nlearning across diﬀerent domains [7] or lifelong transfer learning [8, 9].\nWhile most research on LL has focused on resolving challenges in SLL\nproblems with class labels provided, we instead consider Unsupervised Life-\nlong Learning (ULL) problems, where the learning system is interacting with a\nnon-stationary stream of unlabelled data and the cluster labels are unknown.\nOne objective of ULL is to discover new clusters by interacting with the environ-\nment dynamically and adapting to the changes in the unlabeled data without\nexternal supervision or knowledge. To avoid confusion, it is worth pointing out\nthat our work assumes that the non-stationary streaming unlabelled data come\n2\nfrom a single domain; and we target to develop a single dynamic model that can\nperform well on all sequential data at the end of each training stage without\nforgetting previous knowledge. This setting can serve as a reasonable starting\npoint for ULL. We leave the challenges in ULL across diﬀerent problem domains\nfor future work.\nTo retain knowledge from past data and learn new clusters from new data\ncontinually, good representations of the raw data make it easier to extract in-\nformation and, in turn, support eﬀective learning. Thus, it is more computa-\ntionally appealing if we can discover new clusters in a low-dimensional latent\nspace instead of the complex original data space while performing representa-\ntion learning. To achieve this, we propose Deep Bayesian Unsupervised Lifelong\nLearning (DBULL), which is a ﬂexible probabilistic generative model that can\nadapt to new data and expand with new clusters while seamlessly learning deep\nrepresentations in a Bayesian framework.\nA critical objective of LL is to achieve consistently good performance in-\ncrementally as new data arrive in a streaming fashion, without performance\ndecrease on previous data, even if the data may have been completely overwrit-\nten. Compared with a traditional batch learning setting, there are additional\nchallenges to resolve in a LL setting. One important question is how to design a\nknowledge preservation scheme to eﬃciently maintain previously learned infor-\nmation. To discover the new clusters automatically with streaming, unlabelled\ndata, another challenge is\nhow to design a dynamic model that can expand\nwith incoming data to perform unsupervised learning.\nThe last challenge is\nhow to design an end-to-end inference algorithm to obtain good performance in\nan incremental learning way. To answer these questions, we make the following\ncontributions:\n• To solve the challenges in ULL, we provide a fully Bayesian formulation\nthat performs representation learning, clustering and automatic new clus-\nter discovery simultaneously via our end-to-end novel variational inference\nstrategy DBULL.\n3\n• To eﬃciently extract and maintain knowledge seen in earlier data, we\nprovide innovation in our incremental inference strategy by ﬁrst using\nsuﬃcient statistics in the latent space in an LL context.\n• To discover new clusters in the emerging data, we choose a nonparametric\nBayesian prior to allow the model to grow dynamically. We develop a\nsequential Bayesian inference strategy to perform representation learning\nsimultaneously with our proposed Cluster Expansion and Redundancy Re-\nmoval (CERR) trick to discover new clusters on the ﬂy without imposing\nbounds on the number of clusters, unlike most existing algorithms, using\na truncated Dirichlet Process (DP) [10].\n• To show the eﬀectiveness of DBULL, we conduct experiments on image\nand text benchmarks.\nDBULL can achieve superior performance com-\npared with state-of-the-art methods in both ULL and classical batch set-\ntings.\n2. Related Work\n2.1. Alleviating Catastrophic Forgetting in Lifelong Learning\nResearch on LL aims to learn knowledge in a continual fashion without per-\nformance degradation on previous tasks when trained for new data. Reference\n[11] have provided a comprehensive review on LL with neural networks. The\nmain challenge of LL using Deep Neural Networks (DNNs) is that they often\nsuﬀer from a phenomenon called catastrophic forgetting or catastrophic inter-\nference, where a model’s performance on previous tasks may decrease abruptly\ndue to the interference of training with new information [1, 12]. Recent work\naims at adapting a learned model to new information while ensuring the per-\nformance on previous data does not decrease. Currently, there are no universal\npast knowledge preservation schemes for diﬀerent algorithms and settings [13]\nin LL. Regularization methods target to reduce interference of new learning by\nminimizing changes to certain parameters that are important to previous learn-\ning tasks [14, 15]. Alternative approaches based on rehearsal have also been\n4\nproposed to alleviate catastrophic forgetting while training DNNs sequentially.\nRehearsal methods use either past data [16, 17], coreset data summarization\n[18] or a generative model [19] to capture the data distribution of previously\nseen data.\nHowever, most of the existing LL methods focus on supervised learning tasks\n[14, 18, 7, 19], where each learning task performs supervised learning with out-\nput labels provided. In comparison, we propose a novel alternative knowledge\npreservation scheme in an unsupervised learning context via suﬃcient statistics.\nThis is in contrast to existing work which uses previous model parameters [5, 20],\nrepresentative items exacted from previous models [21] or past raw data, or core-\nset data summarization [16, 17, 18] as previous knowledge for new tasks. Our\nproposal to use suﬃcient statistics is novel and has the advantage of preserving\npast knowledge without the need of storing previous data while allowing incre-\nmental updates as new data arrive by taking advantage of the additive property\nof suﬃcient statistics.\n2.2. Comparable Methods in Unsupervised Lifelong Learning\nRecently, [22] proposed Continual Unsupervised Representation Learning\n(CURL) to deal with a fully ULL setting with unknown cluster labels.\nWe\nhave developed our idea independently in parallel with CURL but in a fully\nBayesian framework. CURL is the most related and comparable method to ours\nin the literature. CURL focuses on learning representations and discovering new\nclusters using a threshold method. One major drawback of CURL is that it has\nover-clustering issues as shown in their real data experiment. We also show this\nempirically and demonstrate the improvement of our method over CURL in\nour experiment section. In contrast to CURL, we provide a novel probabilistic\nframework with a nonparametric Bayesian prior to allow the model to expand\nwithout bound automatically instead of using an ad hoc threshold method as in\nCURL. We develop a novel end-to-end variational inference strategy for learning\ndeep representations and detecting novel clusters in ULL simultaneously.\n5\n2.3. Bayesian Lifelong Learning\nA Bayesian formulation is a natural choice for LL since it provides a system-\natic way to incorporate previously learnt information in the prior distribution\nand obtain a posterior distribution that combines both prior belief and new\ninformation.\nThe sequential nature of Bayes theorem also paves the way to\nrecursively update an approximation of the posterior distribution and then use\nit as a new prior to guide the learning for new data in LL.\nIn [18], the authors propose a Bayesian formulation in LL. Although both\nunder a Bayesian framework, our work is diﬀerent from [18] due to diﬀerent\nobjectives, inference strategies and knowledge preservation techniques. In [18],\nthe authors provide a variational online inference framework for deep discrimi-\nnative models and deep generative models, where they studied the approximate\nposterior distribution of the parameters in DNNs in a continual fashion. How-\never, their method does not have the capacity to ﬁnd out the latent clustering\nstructure of the data or detect new clusters for emerging data. In contrast, we\ndevelop a novel Bayesian framework for representation learning and discovering\nlatent clustering structure and new clusters on the ﬂy together with a novel\nend-to-end variational inference strategy in an ULL context.\n2.4. Deep Generative Unsupervised Learning Methods in a Batch Setting\nRecent research has focused on combining deep generative models to learn\ngood representations of the original data and conduct clustering analysis in an\nunsupervised learning context [23, 24, 25, 26, 27]. However, the latest exist-\ning methods are designed for an independent and identically distributed (i.i.d.)\nbatch training mode instead of a LL context. The majority of these methods\nare in a static unsupervised learning setting, where the number of clusters are\nﬁxed in advance.\nThus, these methods cannot detect potential new clusters\nwhen new data arrive or the data distribution changes. These methods cannot\nadapt to a LL setting.\nTo summarize, our work ﬁlls the gap by providing a fully Bayesian framework\nfor ULL, which has the unique capacity to use a deep generative model for\n6\nrepresentation learning while performing new cluster discovery on the ﬂy with a\nnonparametric Bayesian prior and our proposed CERR technique. To alleviate\ncatastrophic forgetting challenge in LL, we propose to use suﬃcient statistics\nto maintain knowledge as a novel alternative to existing methods. We further\ndevelop an end-to-end Bayesian inference strategy DBULL to achieve our goal.\n3. Model\n3.1. Problem Formulation\nIn our ULL setting, a sequence of datasets D1, D2, . . . , DN arrive in a stream-\ning order. When a new dataset DN arrives in memory, the previous dataset\nDN−1 is no longer available. Our goal is to automatically learn the clusters\n(unlabeled classes) in each dataset.\nLet x ∈X represent the unlabeled observation of the current dataset in\nmemory, where X can be a high-dimensional data space. We assume that a\nlow-dimensional latent representation z can be learned from x and in turn can\nbe used to reconstruct x. We assume that the variation among observations x\ncan be captured by its latent representation z. Thus, we let y represent the\nunknown cluster membership of z for observation x.\nWe target to ﬁnd: (1) a good low-dimensional latent representation z from\nx to eﬃciently extract knowledge from the original data; (2) the clustering\nstructure within the new dataset with the capacity to discover potentially novel\nclusters without forgetting the previously learned clusters of the seen datasets;\nand, (3) an incremental learning strategy to optimize the cluster learning per-\nformance for a new dataset without dramatically degrading the clustering per-\nformance in seen datasets.\nWe summarize our work in a ﬂow chart in Fig. 1, provide its graphical model\nrepresentation in Fig 2, and describe the generative process of our graphical\nmodel for DBULL in the next section.\n3.2. Generative Process of DBULL\nThe generative process for DBULL is as follows.\n7\nLatent representation\nClustering via \nDirichlet Process \nGaussian Mixture\nClustered digits because of \nclustering performed in z.\nLifelong Learning in \nan incremental fashion\nNovel Sufficient Statistics \nfor Knowledge Preservation\nNovel Cluster Expansion \nand Redundancy Removal\nEncoder\nDecoder\nNew Data\nFigure 1: Flow chart of DBULL, best viewed in color. Given observations x, DBULL learns\nits latent representation z via an encoder z = fψ(x) and performs clustering under a Dirichlet\nProcess Gaussian mixture model and reconstruct the original observation via a decoder ˆx =\ngθ(z), where ψ and θ denote the parameters in the encoder and decoder respectively.\nTo\nperform Lifelong Learning in an incremental fashion when dealing with streaming data, we\nintroduce two novel components: Suﬃcient Statistics for knowledge preservation and Cluster\nExpansion and Redundancy Removal to create and merge clusters.\n  \nFigure 2: Graphical model representation of DBULL. Nodes denote random variables, edges\ndenote possible dependence, and plates denote replication. Solid lines denote the generative\nmodel; Dashed lines denote the variational approximation.\n8\n(a) Draw a latent cluster membership y ∼Cat(π(v)), where the vector v\ncomes from the stick-breaking construction of a Dirichlet Process (DP).\n(b) Draw a latent representation vector z|y = k ∼N\n\u0000µ∗\nk, σ∗2\nk I\n\u0001\n, where k is\nthe cluster membership sampled from (a).\n(c) Generate data x from x|z = z ∼N\n\u0000µ(z; θ), diag(σ2(z; θ))\n\u0001\nin the origi-\nnal data space.\nIn (a), Cat(π(v)) is the categorical distribution parameterized by π(v),\nwhere we denote the kth element of π(v) as πk(v), which is the probability\nfor cluster k. The value of π(v) depends on a vector of scalars v coming from\nthe stick-breaking construction of a Dirichlet Process (DP) [28] and we describe\nan iterative process to draw πk(v) in Section 4.2. CURL uses a latent mixture\nof Gaussian components to capture the clustering structure in an unsupervised\nlearning context. In comparison, we adopt the DP mixture model in the la-\ntent space with the advantages that the number of mixture components can\nbe random and grow without bound as new data arrive, which is an appealing\nproperty desired by LL. We further explain in Section 4.2 why DP is an appro-\npriate prior for our problem in details. In (b), z is considered a low-dimensional\nlatent representation of the original data x. We describe in Section 4.2 that a\nDP Gaussian mixture is used for modelling z since it is often assumed that the\nvariation in z is able to reﬂect the variation within x. The current represen-\ntation in (b) is for easy understanding. In (c), we assume that the generative\nmodel pθ(x|z) is parameterized by gθ : Z →X and gθ(z) =\n\u0000µ(z; θ), σ2(z; θ)\n\u0001\n,\nwhere gθ is chosen as DNNs due to its powerful function approximation and\ngood feature learning capabilities [29, 30, 31].\nUnder this generative process, the joint probability density function can be\nfactorized as\np(x, y, z, φ, v) = pθ(x|z)p(z|y)p(y|π(v))p(v)p(φ),\n(1)\nwhere φ = (φ1, φ2, . . . , φk) and φk = (µ∗\nk, σ∗2\nk ) represents the parameters of\nthe kth mixture component (or cluster), p(v) and p(φ) represent the prior dis-\ntribution for v and φ.\n9\nNext, we discuss how to choose appropriate p(v) and p(φ) to endow our\nmodel with the ﬂexibility to grow the number of mixture components without\nbound with new data in a LL setting.\n4. Why Bayesian for DBULL\nIn this section, we illustrate why Bayesian framework is a natural choice for\nour ULL setting. Recall that we have a sequence of datasets D1, D2, . . . , DN\nfrom a single domain arriving in a streaming order. To mimic a LL setting, we\nassume each time only one dataset can ﬁt in memory. One key question in LL\nis how to eﬃciently maintain past knowledge to guide future learning.\n4.1. Bayesian Reasoning for Lifelong Learning\nBayesian framework is a suitable solution to this type of learning since it\nlearns a posterior distribution, or an approximation of a posterior distribution,\nthat takes advantage of both the prior belief and the additional information\nin the new dataset. The sequential nature of Bayes theorem ensures valid re-\ncursive updates on an approximation of the posterior distribution given the\nobservations. Later, the approximation of the posterior distribution serves as\nthe new prior to guide future learning for new data in LL. Before describing our\ninference strategy, we ﬁrst explain why utilizing the Bayesian updating rule is\nvalid for our problem.\nGiven (N −1) datasets Di, where i = 1, 2, . . . , (N −1), the posterior after\nconsidering the Nth dataset is\nP(y, z, φ, v|D1, D2, . . . , DN)\n∝P(DN|y, z, φ, v)P(y, z, φ, v|D1, . . . , DN−1),\n(2)\nwhich reﬂects that the posterior of (N −1) tasks and datasets can be considered\nas the prior for the next task and dataset. If we know exactly the normalizing\nconstant for P(y, z, φ, v|D1, D2, . . . , DN) and P(y, z, φ, v|D1, D2, . . . , DN−1),\nrepeatedly updating (2) is streaming without the need of reusing past data.\n10\nHowever, it is often intractable to compute the normalizing constant exactly.\nThus, an approximation of the posterior distribution is necessary to update (2)\nsince the exact posterior is infeasible to obtain.\n4.2. Dirichlet Process Prior\nThe DP is often used as a nonparametric prior for partitioning exchangeable\nobservations into discrete clusters. The DP mixture is a ﬂexible mixture model\nwhere the number of mixture components can be random and grow without\nbound as more data arrive. These properties make it a natural choice for our\nLL setting. In practice, we show in our inference how we expand and merge\nthe number of mixture components as new data arrive by starting from only\none cluster in Section 5.6. Next, we brieﬂy review DP and introduce our DP\nGaussian mixture model to derive the joint probability density p(x, y, z, φ, v)\ndeﬁned in (1).\nA DP is characterized by a base distribution G0 and a parameter α denoted\nas DP(G0, α). A constructive deﬁnition of DP via a stick-breaking process is of\nthe form G(·) = P∞\nk=1 πkδφk, where δφk is a discrete measure concentrated at\nφk ∼G0, which is a random sample from the base distribution G0 with mixing\nproportion πk [32]. In DP, the πks are random weights independent of G0 but\nsatisfy 0 ⩽πk ⩽1 and P∞\nk=1 πk = 1. The weights πk can be drawn through an\niterative process:\nπk =\n\n\n\n\n\nv1,\nif\nk = 1,\nvk\nQk−1\nj=1(1 −vj),\nfor\nk > 1,\nwhere vk ∼Beta(1, α).\nUnder the generative process of DBULL in Section 3.2, these πks represent\nthe probabilities for each cluster (mixture component) used in step (a) and φk\ncan be seen as the parameters of the Gaussian mixture for z in step (b). Thus,\ngiven our generative process, the corresponding joint probability density for our\n11\nmodel is\np(x, y, z, φ, v) = p(x|z; θ)p(z|y)p(y|v)p(v)G0(φ|λ0)\n=\nN\nY\nn=1\nN(xn|µ(zn; θ), diag(σ2(zn, θ)))\n∞\nY\nk=1\nN(zn|µ∗\nk, σ∗2\nk I)P(yn = k|π(v))\nBeta(vk|1, α0)G0(φk|λ0).\n(3)\nFor a Gaussian mixture model, the base distribution is often chosen as the\nNormal-Wishart (NW) denoted as G0 = NW(λ0) to generate the mixture pa-\nrameters φk = (µ∗\nk, σ∗\nk) ∼NW(λ0), where λ0 = (m0, β0, ν0, W0) = (0, 0.2, D +\n2, ID×D), and D is the dimension of the latent vector z. The values of the\nhyper-parameter λ0 are conventional choices in the Bayesian nonparameteric\nliterature for Gaussian mixture. Moreover, the performance of our method is\nrobust to the hyper-parameter values.\n5. Inference for DBULL\nThere are several new challenges to develop an end-to-end inference algo-\nrithm for our problem under the ULL setting compared with the batch setting:\none has to deal with catastrophic forgetting, mechanisms for past knowledge\npreservation, and dynamic model expansion capacity for novel cluster discov-\nery. For pedagogical reasons, we ﬁrst describe our general parameter learning\nstrategy via variational inference for DBULL in a standard batch setting. We\nthen describe how we resolve the additional challenges in the lifelong (streaming)\nlearning setting. We describe our novel components in the inference algorithm\nin terms of a new knowledge preservation scheme via suﬃcient statistics in Sec-\ntions 5.4 and an automatic CERR strategy in Section 5.6. A summary of our\nalgorithm in the LL setting is provided in Algorithm 1. Our implementation\nis available at https://github.com/KingSpencer/DBULL. The implementation\ndetails are provided in Appendix C. We explain the contribution of the suﬃcient\n12\nstatistics to the probabilistic density function of our problem and knowledge\npreservation in Section 5.5.\n5.1. Variational Inference and ELBO Derivation\nIn practice, it is often infeasible to obtain the exact posterior distribu-\ntion since the normalizing constant in the posterior distribution is intractable.\nMarkov Chains Monte Carlo (MCMC) methods are a family of algorithms that\nprovide a systematic way to sample from the posterior distribution but is often\nslow in a high-dimensional parameter space. Thus, eﬀective alternative methods\nare needed. Variational inference is a promising alternative, which approximates\nthe posterior distribution by casting inference as an optimization problem. It\naims to ﬁnd a surrogate distribution that is the most similar to the distribution\nof interest over a class of tractable distributions that can minimize the Kullback-\nLeibler (KL) divergence to the exact posterior distribution. Minimizing the KL\ndivergence between q(y, z, φ, v|x) and p(y, z, φ, v|x) in our setting is equiva-\nlent to maximizing the Evidence Lower Bound (ELBO), where q(y, z, φ, v|x)\nis the variational posterior distribution used to approximate the true posterior\ndistribution.\nTo make it easier for the readers to understand the core idea,\nwe provide a high-level explanation of variational inference and mathematical\ndetails can be found in Appendix A.\nGiven the generative process in Section 3.2 and using Jensen’s inequality,\nlog p(x) ⩾Eq(y,z,φ,v|x)\n\u001a\nlog p(x, y, z, φ, v)\nq(y, z, φ, v|x)\n\u001b\n(4)\n= LELBO(x),\nFor simplicity, we assume that q(y, z, φ, v|x) = qψ(z|x)q(y)q(v)q(φ). Thus,\n13\nthe ELBO is\nEq(y,z,φ,v|x)\n\u0014\nlog pθ(x|z)p(z|y, φ)p(y|v)p(v)p(φ)\nqψ(z|x)q(y)q(v)q(φ))\n\u0015\n= Eq(y,z,φ,v|x) [log pθ(x|z)] + Eq(y,z,φ,v|x) [log p(z|y, φ)]\n−Eq(y,z,φ,v|x) [log qψ(z|x)]\n+ Eq(y,z,φ,v|x) [log p(y|v)] + Eq(y,z,φ,v|x) [log p(v)]\n−Eq(y,z,φ,v|x) [log q(y)] −Eq(y,z,φ,v|x) [log q(v)]\n−Eq(y,z,φ,v|x) [log q(φ)] + Eq(y,z,φ,v|x) [log p(φ)]\n(5)\nWe assume our variational distribution takes the form of\nq(y, z, φ, v|x) = qψ(z|x)q(φ)q(v)q(y)\n= N(µ(x; ψ), diag(σ2(x; ψ)))\nT −1\nY\nt=1\nqηt(vt)\nT\nY\nt=1\nqζt(φt)\nN\nY\nn=1\nqρn(yn)\n= N(µ(x; ψ), diag(σ2(x; ψ)))\nT −1\nY\nt=1\nBeta(ηt1, ηt2)\nT\nY\nt=1\nN(µt|mt, (βtΛt)−1)W(Λt|Wt, νt)\nN\nY\nn=1\nMult(T, ρn),\n(6)\nwhere we denote fψ(x) = (µ(x; ψ), σ2(x; ψ)), which is a neural network pa-\nrameterized by ψ, T is the number of mixture components in the DP of the\nvariational distribution, zn ∼N(zn|µt, Λ−1\nt ), φt = (µt, Λt), and Mult(T, ρn)\nis a Multinomial distribution. The notation deﬁnitions in equation (5), (6) and\n(7) are provided in Table 1. Our inference strategy starts with only one mixture\ncomponent and uses CERR technique described in Section 5.6 to either increase\nor merge the number of clusters.\n5.2. General Parameter Learning Strategy\nIn equation (5), there are mainly two types of parameters which we need to\noptimize. The ﬁrst type includes parameters θ and ψ in the neural network.\nThe other type involves the latent cluster membership y and the parameters for\nthe DP Gaussian mixture model.\n14\nIn order to perform joint inference for both types of parameters, we adopt\nthe alternating optimization strategy. First we update the neural network pa-\nrameters (θ and ψ) to learn the latent representation z given the DP Gaussian\nmixture parameters. This is achieved by optimizing LELBO-VAE, which only in-\nvolves the ﬁrst three terms of equation (5) that make a contribution to optimize\nθ, ψ and z. Under our variational distribution assumptions in (6), by taking\nadvantage of the reparameterization trick [23] and the Monte Carlo estimate of\nexpectations, we obtain\nLELBO−VAE(x)\n= −1\n2\nT\nX\nk=1\nNkνk {Trace(UkWk)} −1\n2\nT\nX\nk=1\nNkνk\n\b\n(¯zk −mk)T Wk(¯zk −mk)\n\t\n−1\n2\n1\nL\nL\nX\nl=1\nN\nX\ni=1\nD\nX\nj=1\n\u0012\nlog(σ(z, θ)2)(l)\nj\n+\n\u0010\nxij −µ(z; θ)(l)\nj\n\u00112\n(σ(z; θ)2)(l)\nj\n\u0013\n+ 1\n2 log(Det(2πeΣ)).\n(7)\nWe provide the notations in Table 1. The derivation details are provided in\nAppendix A. Then, we update the DP Gaussian mixture parameters and the\ncluster membership given the current neural network parameters θ, ψ and the\nlatent representation z. This allows us to use improved latent representation\nto infer latent cluster memberships and the updated clustering will in turn\nfacilitate learning latent knowledge representation. The update equations for\nDP mixture model parameters can be found in [10]. We describe the core idea\nof automatic CERR in our inference in Section 5.6 to explain how we start with\nonly one cluster and achieve dynamic model expansion by creating new mixture\ncomponents (clusters) given new data in LL.\nOur general parameter learning strategy via variational inference may seem\nstraightforward for a batch setting at ﬁrst glance. However, both the derivation\nand the implementation is nontrivial especially when incorporating our new\ncomponents in the end-to-end inference procedure to address the additional\nchallenges in a LL setting. For illustration purposes, we choose to describe the\nhigh level core idea of our inference procedure. The main diﬃculty lies in how\n15\nTable 1: Notations in LELBO−VAE(x).\nNotations in the ELBO\nθ: parameters in the decoder.\nψ: parameters in the encoder.\nN: the total number of observations.\nT: the total number of clusters.\nD: the dimension of the latent representation z.\nΣ : diag(σ2(x; ψ)).\nxij: the jth dimension of the nth observation.\nyn: cluster membership for the nth observation.\np(yn = k) = γik, Nk = PN\nn=1 γnk.\nL: the number of Monte Carlo samples in Stochastic\nGradient Variational Bayes (SGVB).\nˆzn = 1\nL\nPL\nl=1 z(l)\nn . ¯zk =\n1\nNk\nPN\nn=1 γnk ˆzn.\nUk =\n1\nNk\nPN\nn=1 γnk(ˆzn −¯zk)(ˆzn −¯zk)T .\nβk = β0 + Nk: the posterior scalar precision in NW distribution.\nmk =\n1\nβk (β0m0 + Nk¯zk): the posterior mean of cluster k.\nW −1\nk\n= W −1\n0\n+ NkSk +\nβ0Nk\nβ0+Nk (¯zk −m0)(¯zk −m0)T .\nνk = ν0 + Nk: the kth posterior degrees of freedom of NW.\nto adapt our inference algorithm from a batch setting to a LL setting, which\nrequires us to overcome catastrophic forgetting, maintain past knowledge and\ndevelop a dynamic model that can expand with automatic cluster discovery and\nredundancy removal capacity. Next, we describe our novel solutions.\n5.3. Our Ingredients for Alleviating Catastrophic Forgetting\nCatastrophic forgetting or catastrophic interference is a dramatic issue for\nDNNs as witnessed in SLL [14, 19]. In our ULL setting, the issue is even more\nchallenging since we have more sources than SLL that may lead to abrupt model\nperformance decrease due to the interference of training with new data. The\nﬁrst source is the same as in SLL when the DNNs forget previously learned\ninformation upon learning new information. Additionally, in an unsupervised\n16\nsetting, the model is not able to recover the learned cluster membership and\nclustering related parameters in the DP mixture model when the previous data is\nno longer available, or when the previous learned information of DNNs has been\nwiped out upon learning new information, since the clustering structure learned\ndepends on the latent representation of the raw data, which is determined by\nthe DNNs’ parameters and the data distributions.\nTo resolve these issues, we develop our own novel solution via a combination\nof two ingredients: (1) generating and replaying a ﬁxed small number of sam-\nples based on our generative process in Section 3.2 given the current DNNs and\nDP Gaussian mixture parameter estimates, which is a computationally eﬀective\nbyproduct of our algorithm; and, (2) developing a novel hierarchical suﬃcient\nstatistics knowledge preservation strategy to remember the clustering informa-\ntion in an unsupervised setting.\nWe choose to replay a number of generative samples to preserve the previous\ndata distribution instead of using a subset of past real data, since storing past\ndata may require large memory and such data storage and replay may not be\nfeasible in real big data applications. More details of replaying deep generative\nsamples over real data in LL have been discussed in [19]. Moreover, our proposal\nto use suﬃcient statistics is novel and has the advantage of allowing incremental\nupdates of the clustering information as new data arrive without the need of\naccess to previous data because of the additive property of suﬃcient statistics.\nWe introduce this novel strategy in the next section.\n5.4. Suﬃcient Statistics for Knowledge Preservation\nAs LL is an emerging ﬁeld, and there is no well-accepted knowledge def-\ninition or an appropriate representation scheme to eﬃciently maintain past\nknowledge from seen data. Researchers have adopted prior distributions [18]\nor model parameters [33] to represent past knowledge in most SLL problems,\nwhere achieving high prediction accuracy incrementally is the main objective.\nHowever, there is no guidance on preserving past knowledge in an unsupervised\nlearning setup.\n17\nWe propose a novel knowledge preservation strategy in DBULL. In our prob-\nlem, there are two types of knowledge to maintain. The ﬁrst contains previously\nlearned DNNs’ parameters needed to encode the latent knowledge representa-\ntion z of the raw data and the reconstruction of the real data from z. The\nother involves the DP Gaussian mixture parameters to represent diﬀerent cluster\ncharacteristics and diﬀerent cluster mixing proportions. Our novel knowledge\nrepresentation scheme uses hierarchical suﬃcient statistics to preserve the infor-\nmation related to the DP Gaussian mixture. We develop a sequential updating\nrule to update our knowledge.\nAssume that we have encountered N datasets {Dj}N\nj=1 and each time only\none dataset Dj can be in memory. While in memory, each dataset can be di-\nvided into M mini-batches {Bi}M\ni=1. To deﬁne the suﬃcient statistics, we ﬁrst\ndeﬁne the global parameters of the DP Gaussian mixture as probabilities of each\nmixture component (cluster) πks and the mixture parameters (µ∗\nk, σ∗\nk) for each\ncluster k. We deﬁne the local parameters as the cluster membership for each\nobservation in memory. To remember the characteristics of all encountered data\nand the local information of the current dataset, we memorize three levels of suf-\nﬁcient statistics. The ith mini-batch suﬃcient statistics Sj,i\nk\n= (Nk(Bi), sk(Bi))\nof the current dataset Dj, where sk(Bi) = P\nn∈Bi ˆγnkt(zn) and t(zn) is the\nsuﬃcient statistics to represent a distribution within the exponential family\n(Gaussian distribution is within the exponential family and t(zn) = (zn, zT\nn zn)\nin our case) and ˆγnk represents the estimated probability of the nth observa-\ntions in mini-batch Bi belonging to cluster k. We also deﬁne the stream suﬃ-\ncient statistics Sj\nk = PM\ni=1 Sj,i\nk\nof dataset Dj and the overall suﬃcient statistics\nS0\nk = (Nk, sk(z)) of all encountered datasets {Dj}N\nj=1.\nTo eﬃciently maintain and update our knowledge, we develop our updating\nalgorithm as: (1) substract the old summary of each mini-batch and update\nthe local parameters; (2) compute a new summary for each mini-batch; and,\n(3) update the stream suﬃcient statistics for each cluster learned in the current\n18\ndataset.\nSj\nk ←Sj\nk −Sj,i\nk ,\n(8)\nSj,i\nk\n←\n X\nn∈Bi\nˆγnk,\nX\nn∈Bi\nˆγnkt(zn)\n!\n,\n(9)\nSj\nk ←Sj\nk + Sj,i\nk .\n(10)\nFor the dataset Dj in the learning phase, we repeat the updating process\nmultiple iterations to reﬁne our training while learning the DP Gaussian mixture\nparameters and the cluster membership. Finally, we update the overall suﬃcient\nstatistics by S0\nk ←S0\nk + Sj\nk. The correctness of the algorithm is guaranteed by\nthe additive property of the suﬃcient statistics.\n5.5. Contribution of Suﬃcient Statistics to Alleviate Forgetting\nThe suﬃcient statistics alleviate forgetting by preserving data characteristics\nand allow sequential updates in LL without the need of saving real data. To\nbe precise, the suﬃcient statistics allow us to update the log-likelihood and the\nELBO sequentially since both terms are linear functions of the expectation of\nthe suﬃcient statistics. Given the expected suﬃcient statistics, we are able to\nevaluate the ﬁrst two terms of LELBO−VAE(x) in equation (7) and p(z|y)p(y|v)\nin the joint probability density function of our model in equation (3). Next, we\nprovide mathematical derivations to illustrate this.\nDeﬁne suﬃcient statistics of all data S0 = (S0\n1, S0\n2, . . . ., S0\nk), where k is the\nnumber of clusters. Deﬁne S0\nk = (Nk, PN\nn=1 rnkt(zn)), where t(zn) = (zn, zT\nn zn)\nand rnk denotes the probability of the nth observation belonging to cluster k,\nNk = PN\nn=1 rnk, and N is the total number of observations. Given the suﬃcient\nstatistics and current mixture parameters (µ∗\nk, σ∗\nk) and π(v), we can evaluate\nP∞\nk=1 N(zn|µ∗\nk, σ∗2\nk I)P(yn = k|π(v)) in the joint probability density function in\nequation (3) without storing each latent representation zn for all data. Similarly,\nwe can also evaluate the ﬁrst two terms of LELBO−VAE(x) in equation (7) with\nnotations deﬁned in Table 1.\n19\n5.6. Cluster Expansion and Redundancy Removal Strategy\nOur model starts with one cluster and, as we have new data with diﬀerent\ncharacteristics, we expect the model to either dynamically grow the number of\nclusters or merge clusters if they have similar characteristics. To achieve this,\nwe perform birth and merge moves in a similar fashion as the Nonparametric\nBayesian literature [34] to allow automatic CERR. However, we would like to\nemphasize that our work is diﬀerent from [34] since our merge moves have\nextra constraints. To avoid losing information about clusters learned earlier,\nwe only allow merge moves between two novel clusters from the birth move or\none existing cluster with a newborn cluster. Two previously existing clusters\ncannot be merged. Reference [34] is designed for a batch learning, thus, it does\nnot require this constraint (but in LL this constraint is important for avoiding\ninformation loss).\nIt is challenging to give birth to new clusters with streaming data since\nthe number of observations may not be suﬃcient to inform good proposals.\nTo resolve this issue, we follow [34] by collecting a subsample of data for each\nlearned cluster k. Then, we cache the samples in the subsample if the probability\nof the nth observation to be assigned to cluster k is bigger than a threshold of\nvalue 0.1. This value has been suggested by [34]. In this paper, we try to choose\ncommonly used parameters in the literature and avoid dataset speciﬁc tuning\nas much as possible. We ﬁt the DP Gaussian mixture to the cached samples\nwith one cluster and expand the model with 10 novel clusters. However, only\nadopting the birth moves may overcluster the observations into diﬀerent clusters.\nAfter the birth move, we merge the clusters by (1) selecting candidate clusters\nto merge and by (2) merging two selected clusters if ELBO improves.\nThe\ncandidate clusters are selected if the marginal likelihood of two merged clusters\nis bigger than the marginal likelihood when keeping the two clusters separate.\n20\nAlgorithm 1 Variational Inference for DBULL\n1: Initialization:\nInitialize the DNNs, variational distributions and the hyperparameters for Dirich-\nlet Process Mixture Model (DPMM).\n2: for j = 1, 2, . . . , N datasets in memory do\n3:\nfor epoch = 1, 2, . . . do\n4:\nfor h = 1, 2, . . . , H iterations do\n5:\nUpdate the weights ω = {ψ, θ} of the encoder and decoder via ω(h+1) =\nω(h)+η ∂LELBO-VAE(x)\n∂ω(h)\nto maximize LELBO−VAE(x) in equation 7 given cur-\nrent DPMM parameters, where η is the learning rate when using stochastic\ngradient descent.\n6:\nend for\n7:\nCompute the deep representation z of observations x using the encoder z =\nfψ(x) = (µ(x; ψ), σ2(x; ψ)).\n8:\nfor i = 1, 2, . . . , Mj mini-batch of dataset j do\n9:\nwhile The ELBO of the DPMM has not converged do\n10:\nVisit the ith mini-batch of z in a full pass in the jth dataset.\n11:\nUpdate the mini-batch suﬃcient statistics and stream suﬃcient statistics\nin Section 5.4 via equation 8 and 9.\n12:\nUpdate the local and global parameters of DPMM using mini-batch i of\ndataset j via standard variational inference for DPMM [10] provided in\nAppendix A.\n13:\nPerform cluster expansion described in Section 5.6 to propose new clus-\nters.\n14:\nPerform redundancy removal described in Section 5.6 to merge clusters\nif the ELBO improves.\n15:\nend while\n16:\nend for\n17:\nUpdate the overall suﬃcient statistics deﬁned in Section 5.4 via equation 10.\n18:\nend for\n19: end for\n20: Output: Learned DNNs, variational approximation to the posterior, deep\nlatent representation, cluster assignment for each observation, and cluster\nrepresentative in the latent space.\n21\nTable 2: Summary statistics for benchmark datasets.\nDataset\n# Samples\nDimension\n# Classes\nMNIST\n70000\n784\n10\nReuters10k\n10000\n2000\n4\nSTL-10\n13000\n2048\n10\n6. Experiments\nDatasets. We adopt the most common text and image benchmark datasets\nin LL to evaluate the performance of our method. The MNIST database of\n70,000 handwritten digit images [35] is widely used to evaluate deep generative\nmodels [23, 25, 24, 27, 26] for representation learning and LL models in both\nsupervised [14, 18, 19] and unsupervised learning contexts [22]. To provide a fair\ncomparison with state-of-the-art competing methods and easy interpretation, we\nmainly use MNIST to evaluate the performance of our method and interpret\nour results with intuitive visualization patterns. To examine our method on\nmore complex datasets, we use text Reuters10k [36] and image STL-10 [37]\ndatabases. STL-10 is at least as hard as a well-known image database CIFAR-\n10 [38] since STL-10 has fewer labeled training examples within each class. The\nsummary statistics for the datasets are provided in Table 2.\nWe adopt the same neural network architecture as in [26]. All values of the\ntuning parameters and the implementation details in the DNNs are provided\nin Appendix C. Our implementation is publicly available at https://github.\ncom/KingSpencer/DBULL.\nCompeting Methods in Unsupervised Lifelong Learning. CURL is\nthe only lifelong unsupervised learning method currently with both represen-\ntation learning and new cluster discovery capacity, which makes CURL the\nlatest, most related, and comparable method to ours. We use CURL-D to\nrepresent CURL without the true number of clusters provided but to detect it\nDynamically given unlabelled streaming data.\nCompeting Methods in a Classic Batch Setting. Although designed\nfor LL, to show the generality of DBULL in a batch training mode, we compare\n22\nit against recent deep (generative) methods with representation learning and\nclustering capacity designed for batch settings, including DEC [25], VaDE [26],\nCURL-F [22] and VAE+DP. CURL-F represents CURL with the true number\nof clusters provided as a Fixed value. VAE+DP ﬁts a Variational Autoencoders\n(VAE) [23] to learn latent representations ﬁrst and then uses a DP to learn\nclustering in two separate steps. We list the capabilities of diﬀerent methods in\nTable 3.\nTable 3: DBULL and competing methods capacity comparison.\nLifelong Learning\nBatch Setting\nDBULL\nCURL-D\nDEC VaDE VAE+DP CURL-F\nRepresentation Learning\nyes\nyes\nyes\nyes\nyes\nyes\nLearns # of Clusters\nyes\nyes\nno\nno\nyes\nno\nDynamic Expansion\nyes\nyes\nno\nno\nyes\nno\nOvercome Forgetting\nyes\nyes\nno\nno\nno\nyes\nEvaluation Metrics. One of the main objectives of our method is to per-\nform new cluster discovery with streaming non-stationary data.\nThus, it is\ndesired if our method can achieve superior clustering quality in both ULL and\nbatch settings. We adopt the clustering quality metrics including Normalized\nMutual Information (NMI), Adjusted Rand Index (ARI), Homogeneity Score\n(HS), Completeness Score (CS) and V-measure Score (VM). These are all nor-\nmalized metrics ranging from zero to one, and larger values indicate better\nclustering quality. NMI, ARI and VM of value one represent perfect clustering\nas the ground truth. CS is a symmetrical metric to HS. Detailed deﬁnitions for\nthese metrics can be found in [39].\n6.1. Lifelong Learning Performance Comparison\nExperiment Objective. It is desired if LL methods can adapt a learned\nmodel to new data while retaining the information learned earlier. The objective\nof this experiment is to demonstrate DBULL has such desired capacity and\neﬀectiveness compared with state-of-the-art LL methods such that there is no\n23\nARS\nCS\nTask1\nTask2\nTask3\nTask4\nTask5\n                 TASK1\n                 TASK2\n                 TASK3\n                 TASK4\n                 TASK5                  TASK1\n                 TASK2\n                 TASK3\n                 TASK4\n                 TASK5\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nMethod\nCURL\nDBULL\nFigure 3:\nClustering quality performances in terms of ARS and CS measured on each task\nafter sequential training from TASK1 to TASK5 across every 500 iterations for each task.\ndramatic performance decrease on past data even if the model has been updated\nwith new information.\nExperiment Setup. To evaluate the performance of DBULL, we adopt the\nmost common experiment setup called Split MNIST in LL, which used images\nfrom MNIST [15, 18]. We divide MNIST into 5 disjoint subsets with each subset\ncontaining 10,000 random samples of two digit classes in the order of digits 0-1,\n2-3, 4-5, 6-7 and 8-9, denoted as DS1, DS2, DS3, DS4, and DS5. Each dataset is\ndivided into 20 subsets that arrive in a sequential order to mimic a LL setting.\nWe denote DSi:j as all data from DSi to DSj, where i, j = 1, 2, . . . , 5.\nDiscussion on Performance. To check if our method has dramatic per-\nformance loss due to catastrophic forgetting, we sequentially train our method\nDBULL and its LL competitor CURL-D on DS1, DS2, . . . , DS5.\nWe deﬁne\nTASKi as training on DSi, where i = 1, 2, . . . , 5. We measure the performance of\nTASK1 after training TASK1, TASK2, TASK3, TASK4, TASK5 with datasets\nDS1, DS1:2, DS1:3, DS1:4, and DS1:5, the performance of TASK2 after training\nTASK2, TASK3, TASK4, TASK5 with datasets DS2, DS2:3, DS2:4, DS2:5, etc. We\nreport the LL clustering quality performance for each task after sequential train-\ning ﬁve tasks in Fig. 3 and Fig. 4.\n24\nHS\nNMI\nVM\nTask1\nTask2\nTask3\nTask4\nTask5\n                 TASK1\n                 TASK2\n                 TASK3\n                 TASK4\n                 TASK5                 TASK1\n                 TASK2\n                 TASK3\n                 TASK4\n                 TASK5                 TASK1\n                 TASK2\n                 TASK3\n                 TASK4\n                 TASK5\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nMethod\nCURL\nDBULL\nFigure 4: Clustering quality performances in terms of HS, NMI and VM measured on each\ntask after sequential training from TASK1 to TASK5 across every 500 iterations for each task.\nFig. 3 and Fig. 4 reﬂect that DBULL has better performance in handling\ncatastrophic forgetting than CURL-D since DBULL has slightly less perfor-\nmance drop than CURL-D for previous tasks in almost all scenarios in terms of\nnearly all clustering metrics.\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n                TASK1\n                 TASK2\n                 TASK3\n                 TASK4\n                 TASK5\nNumber of Clusters\nMethod\nCURL\nDBULL\nTruth\nFigure 5: Number of clusters detected by CURL-D, DBULL after sequentially training from\nTASK1 to TASK5 across every 500 iterations for each task compared with the ground truth.\nFig. 5 reﬂects that DBULL has advantages over CURL-D in handling over-\nclustering issues. Since each task has two digits, the true number of clusters\nseen after training each task sequentially is 2, 4, 6, 8, 10. The number of clus-\n25\nters automatically detected by DBULL after training TASK1, . . ., TASK5 is 4,\n6, 8, 10, 12. DBULL clusters digit 1 into three clusters of diﬀerent handwritten\npatterns in TASK1. For other digits, DBULL discovers each new digit into one\nexact cluster as the ground truth. In contrast, CURL-D clustered digits 0-1 into\n14-16 clusters of TASK1 and obtained 23-25 clusters for 10 digits after training\nﬁve tasks sequentially. We provide visualization of the reconstructed cluster\nmean from the DP mixture model via our trained decoder of DBULL in Fig. 6.\nDBULL sequentially updated  order\n Task 1\n Task 2\n Task 3\n Task 4\n Task 5\nFigure 6: Decoded images using the DP Gaussian mixture posterior mean after sequentially\ntrained from TASK1 to TASK5 using DBULL.\nBesides the overall clustering quality reported, we also provide the precision\nand recall of DBULL to view the performance for each digit after sequentially\ntraining all tasks. CURL-D overclusters the 10 digits into 25 clusters, mak-\ning it hard to report the precision and recall of each digit. To visualize the\nresults, the three sub-clusters of digit one by DBULL have been merged into\none cluster. Overall, there is no signiﬁcant performance loss of previous tasks\nafter sequentially training multiple tasks for digits 0, 1, 3, 4, 6, 8, 9. Digit 2\nhas experienced precision decrease after training TASK4 of digits 6 and 7 since\nDBULL has trouble in diﬀerentiating some samples from digits 2 and 7.\nPrecision\nRecall\n               TASK 1\n               TASK 2\n               TASK 3\n               TASK 4\n               TASK 5\n               TASK 1\n               TASK 2\n               TASK 3\n               TASK 4\n               TASK 5\n0.00\n0.25\n0.50\n0.75\n1.00\nDigit\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 7: Precision and recall for each digit of DBULL evaluated after sequentially training\nfrom TASK1 to TASK5 across every 500 iterations for each task.\n26\n6.2. Batch Setting Clustering Performance Comparison\nExperiment Objective. The goal of this experiment is to demonstrate the\ngenerality our LL method DBULL, which can achieve comparable performance\nas competing methods in an unsupervised batch setting.\nExperiment Setup. To examine our method performance in a batch set-\nting, we test it on more complex datasets including Reuters10k obtained from\nthe original Reuters [36] and image STL-10 [37]. We use the same Reuters10k\nand STL-10 dataset from [25, 26]. The details of Reuters10k and STL-10 are\nprovided in Appendix B. For all datasets, we randomly select 80% of the sam-\nples as training and evaluate the performance on the rest 20% of the samples\nfor all methods.\nDiscussion on Performance. The true number of clusters is provided to\ncompeting methods: DEC, VaDE and CURL-F in advance since the total num-\nber of clusters is required. DBULL, CURL-D, VAE+DP have less information\nthan DEC, VaDE and CURL-F since they have no knowledge about the true\nnumber of clusters. DBULL, CURL-D and VAE+DP all start with one cluster\nand detect the number of clusters on the ﬂy. Thus, if DBULL can achieve similar\nperformance to DEC, VaDE and CURL-F and outperforms its LL counterpart\nCURL-D, it demonstrates DBULL’s eﬀectiveness. Table 4 shows DBULL per-\nforms the best in NMI, VM for MNIST and NMI, ARI and VM for STL10 and\noutperforms CURL-D in MNIST and STL10. Moreover, DBULL and DEC are\nmore stable in terms of all evaluation metrics because of smaller standard error\nthan other methods. We also report the number of clusters found by DBULL,\nCURL-D for MNIST, Reuters10k and STL-10 in Table 5 out of ﬁve replications.\nTable 5 shows that DBULL handles overclustering issues better in comparison\nwith CURL-D. In summary, Table 4 and 5 demonstrate DBULL’s eﬀectiveness\nin a batch setting. If we ﬁx the number of clusters as the true one for meth-\nods VaDE, DEC and CURL-F while training, the clustering accuracy can be\nconsidered as the classiﬁcation accuracy. However, we are not able to set the\nnumber of clusters as the true one in DBULL. As we have seen in Table 5, out\nof ﬁve replications, the number of clusters found by DBULL is from 11 to 15.\n27\nTo compute a clustering accuracy for DBULL, taking MNIST as an example,\nwe count the number of correctly clustered samples from the biggest 10 clusters\nand divide it by the total number of samples in the testing phase. We report\nthe accuracy comparison in Table 6.\n7. Conclusion\nIn this work, we introduce our approach DBULL for unsupervised LL prob-\nlems. DBULL is a novel end-to-end approximate Bayesian inference algorithm,\nwhich is able to perform automatic new task discovery via our proposed dynamic\nmodel expansion strategy, adapt to changes in the evolving data distributions,\nand overcome forgetting using our proposed information extraction mechanism\nvia summary suﬃcient statistics while learning the underlying representation\nsimultaneously. Experiments on MNIST, Reuters10k and STL-10 demonstrate\nthat DBULL has competitive performance compared with state-of-the-art meth-\nods in both a batch setting and an unsupervised LL setting.\nAcknowledgment\nThe work described was supported in part by Award Numbers U01 HL089856\nfrom the National Heart, Lung, and Blood Institute and NIH/NCI R01 CA199673.\n28\nTable 4: Clustering quality (%) comparison averaged over ﬁve replications with both the\naverage value and the standard error (in the parenthesis) provided.\nDataset\nMethod\nNMI\nARI\nMNIST\nDEC\n84.67 (2.25)\n83.67 (4.53)\nVaDE\n80.35 (4.68)\n74.06 (9.11)\nVAE+DP\n81.70 (0.825)\n70.49 (1.654)\nCURL-F\n69.76 (2.51)\n56.47 (4.11)\nCURL-D\n63.51 (1.32)\n36.84 (1.98)\nDBULL\n85.72 (1.02)\n83.53 (2.35)\nReuters10k\nDEC\n46.56 (5.36)\n46.86 (7.98)\nVaDE\n41.64 (4.73)\n38.49 (5.44)\nVAE + DP\n41.62 (2.99)\n37.93 (4.57)\nCURL-F\n51.92 (3.22)\n47.72 (4.00)\nCURL-D\n46.31 (1.83)\n22.00 (3.60)\nDBULL\n45.32 (1.79)\n42.66 (5.73)\nSTL10\nDEC\n71.92 (2.66)\n58.73 (5.09)\nVaDE\n68.35 (3.85)\n59.42 (6.84)\nVAE+DP\n43.18 (1.41)\n26.58 (1.32)\nCURL-F\n66.98 (3.38)\n51.24 (4.06)\nCURL-D\n65.71 (1.33)\n37.96 (4.69)\nDBULL\n75.26 (0.53)\n70.72 (0.81)\nDataset\nMethod\nHS\nVM\nMNIST\nDEC\n84.67 (2.25)\n84.67 (2.25)\nVaDE\n79.86 (4.93)\n80.36 (4.69)\nVAE+DP\n91.27 (0.215)\n81.19 (0.904)\nCURL-F\n68.60 (2.56)\n69.75 (2.51)\nCURL-D\n76.35 (1.53)\n62.45 (1.32)\nDBULL\n89.34 (0.25)\n85.65 (0.51)\nReuters10k\nDEC\n48.44 (5.44)\n46.52(5.36)\nVaDE\n43.64 (4.88)\n41.60 (4.73)\nVAE + DP\n46.64 (3.85)\n41.34 (2.94)\nCURL-D\n66.90 (2.09)\n43.34 (2.00)\nCURL-F\n54.38 (3.49)\n51.86 (3.21)\nDBULL\n48.88 (1.86)\n45.40 (2.04)\nSTL10\nDEC\n68.47 (3.48)\n71.83 (2.72)\nVaDE\n67.24 (4.23)\n68.37 (3.92)\nVAE+DP\n42.28 (1.03)\n43.16 (1.39)\nCURL-F\n65.46 (3.27)\n66.96 (3.37)\nCURL-D\n80.86 (2.94)\n64.31 (1.24)\nDBULL\n77.61 (1.29)\n75.22 (0.52)\n29\nTable 5: Number of clusters found by DBULL and CURL-D out of ﬁve replications, where\nthe upperbounds for the number of clusters for Reuters10k and STL-10 are set at 40 and 50.\nDatasets\nTrue # of Clusters\nDBULL\nCURL-D\nMNIST\n10\n11-15\n34\nReuters10k\n4\n5-10\n40\nSTL-10\n10\n12-15\n50\nTable 6: Clustering accuracy for VaDE, DEC, CURL-F and DBULL. We report the best\naccuracy for DBULL since DEC [25] and VaDE [26] only report their best accuracy. CURL-F\n[22] reports both the best accuracy and the average accuracy with the standard error.\nDatasets\nDEC\nVaDE\nCURL-F (best)\nCURL-F (average)\nDBULL\nMNIST\n84.30%\n94.46%\n84%\n79.38% (4.26%)\n92.27%\nReferences\n[1] M. McCloskey, N. J. Cohen, Catastrophic interference in connectionist net-\nworks: The sequential learning problem, in: Psychology of learning and\nmotivation, Vol. 24, Elsevier, 1989, pp. 109–165.\n[2] S. Thrun, T. M. Mitchell, Lifelong robot learning, Robotics and au-\ntonomous systems 15 (1-2) (1995) 25–46.\n[3] S. Thrun, Is learning the n-th thing any easier than learning the ﬁrst?, in:\nAdvances in neural information processing systems, 1996, pp. 640–646.\n[4] P. Ruvolo, E. Eaton, Ella: An eﬃcient lifelong learning algorithm, in: In-\nternational Conference on Machine Learning, 2013, pp. 507–515.\n[5] Z. Chen, N. Ma, B. Liu, Lifelong learning for sentiment classiﬁcation, in:\nProceedings of the 53rd Annual Meeting of the Association for Computa-\ntional Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 2: Short Papers), 2015, pp. 750–756.\n[6] S. S. Sarwar, A. Ankit, K. Roy, Incremental learning in deep convolutional\nneural networks using partial network sharing, IEEE Access.\n30\n[7] S. Hou, X. Pan, C. Change Loy, Z. Wang, D. Lin, Lifelong learning via\nprogressive distillation and retrospection, in: Proceedings of the European\nConference on Computer Vision (ECCV), 2018, pp. 437–452.\n[8] P. Ruvolo, E. Eaton, Active task selection for lifelong machine learning, in:\nTwenty-seventh AAAI conference on artiﬁcial intelligence, 2013.\n[9] D. Isele, M. Rostami, E. Eaton, Using task features for zero-shot knowledge\ntransfer in lifelong learning., in: IJCAI, 2016, pp. 1620–1626.\n[10] D. M. Blei, M. I. Jordan, et al., Variational inference for dirichlet process\nmixtures, Bayesian analysis 1 (1) (2006) 121–143.\n[11] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, S. Wermter, Continual\nlifelong learning with neural networks: A review, Neural Networks.\n[12] J. L. McClelland, B. L. McNaughton, R. C. O’Reilly, Why there are com-\nplementary learning systems in the hippocampus and neocortex: insights\nfrom the successes and failures of connectionist models of learning and\nmemory., Psychological review 102 (3) (1995) 419.\n[13] Z. Chen, B. Liu, Lifelong Machine Learning, Morgan & Claypool Publish-\ners, 2018.\n[14] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A.\nRusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al., Over-\ncoming catastrophic forgetting in neural networks, Proceedings of the na-\ntional academy of sciences 114 (13) (2017) 3521–3526.\n[15] F. Zenke, B. Poole, S. Ganguli, Continual learning through synaptic intel-\nligence, in: Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, JMLR. org, 2017, pp. 3987–3995.\n[16] A. Robins, Catastrophic forgetting, rehearsal and pseudorehearsal, Con-\nnection Science 7 (2) (1995) 123–146.\n31\n[17] H. Xu, B. Liu, L. Shu, P. S. Yu, Lifelong domain word embedding via\nmeta-learning, in: Proceedings of the 27th International Joint Conference\non Artiﬁcial Intelligence, 2018, pp. 4510–4516.\n[18] C. V. Nguyen, Y. Li, T. D. Bui, R. E. Turner, Variational continual learn-\ning, in: International Conference on Learning Representations (ICLR),\n2018.\n[19] H. Shin, J. K. Lee, J. Kim, J. Kim, Continual learning with deep generative\nreplay, in: Advances in Neural Information Processing Systems, 2017, pp.\n2990–2999.\n[20] L. Shu, B. Liu, H. Xu, A. Kim, Lifelong-rl: Lifelong relaxation labeling\nfor separating entities and aspects in opinion targets, in: Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing.\nConference on Empirical Methods in Natural Language Processing, Vol.\n2016, NIH Public Access, 2016, p. 225.\n[21] L. Shu, H. Xu, B. Liu, Lifelong learning crf for supervised aspect extrac-\ntion, in: Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), 2017.\n[22] D. Rao, F. Visin, A. Rusu, R. Pascanu, Y. W. Teh, R. Hadsell, Continual\nunsupervised representation learning, in: Advances in Neural Information\nProcessing Systems, 2019, pp. 7645–7655.\n[23] D. P. Kingma, M. Welling, Auto-encoding variational bayes, in: Interna-\ntional Conference on Learning Representations (ICLR), 2014.\n[24] M. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, S. R. Datta,\nComposing graphical models with neural networks for structured represen-\ntations and fast inference, in: Advances in neural information processing\nsystems, 2016, pp. 2946–2954.\n32\n[25] J. Xie, R. Girshick, A. Farhadi, Unsupervised deep embedding for clustering\nanalysis, in: International conference on machine learning, 2016, pp. 478–\n487.\n[26] Z. Jiang, Y. Zheng, H. Tan, B. Tang, H. Zhou, Variational deep embedding:\nan unsupervised and generative approach to clustering, in: Proceedings of\nthe 26th International Joint Conference on Artiﬁcial Intelligence, AAAI\nPress, 2017, pp. 1965–1972.\n[27] P. Goyal, Z. Hu, X. Liang, C. Wang, E. P. Xing, Nonparametric variational\nauto-encoders for hierarchical representation learning, in: Proceedings of\nthe IEEE International Conference on Computer Vision, 2017, pp. 5094–\n5102.\n[28] J. Sethuraman, R. C. Tiwari, Convergence of dirichlet measures and the\ninterpretation of their parameter, in: Statistical decision theory and related\ntopics III, Elsevier, 1982, pp. 305–315.\n[29] K. Hornik, Approximation capabilities of multilayer feedforward networks,\nNeural networks 4 (2) (1991) 251–257.\n[30] D. P. Kingma, S. Mohamed, D. J. Rezende, M. Welling, Semi-supervised\nlearning with deep generative models, in: Advances in neural information\nprocessing systems, 2014, pp. 3581–3589.\n[31] E. Nalisnick, L. Hertel, P. Smyth, Approximate inference for deep latent\ngaussian mixtures, in: NIPS Workshop on Bayesian Deep Learning, Vol. 2,\n2016.\n[32] H. Ishwaran, L. F. James, Gibbs sampling methods for stick-breaking pri-\nors, Journal of the American Statistical Association 96 (453) (2001) 161–\n173.\n[33] S. Lee, J. Stokes, E. Eatonr, Learning shared knowledge for deep life-\nlong learning using deconvolutional networks, in: 6th Proceedings of the\n33\nTwenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJ-\nCAI), 2019, pp. 2837–2844.\n[34] M. C. Hughes, E. Sudderth, Memoized online variational inference for\ndirichlet process mixture models, in: Advances in Neural Information Pro-\ncessing Systems, 2013, pp. 1133–1141.\n[35] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, et al., Gradient-based learning\napplied to document recognition, Proceedings of the IEEE 86 (11) (1998)\n2278–2324.\n[36] D. D. Lewis, Y. Yang, T. G. Rose, F. Li, Rcv1: A new benchmark collec-\ntion for text categorization research, Journal of machine learning research\n5 (Apr) (2004) 361–397.\n[37] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsu-\npervised feature learning, in: Proceedings of the fourteenth international\nconference on artiﬁcial intelligence and statistics, 2011, pp. 215–223.\n[38] A. Krizhevsky, G. Hinton, et al., Learning multiple layers of features from\ntiny images.\n[39] A. Rosenberg, J. Hirschberg, V-measure: A conditional entropy-based ex-\nternal cluster evaluation measure, in: Proceedings of the 2007 joint con-\nference on empirical methods in natural language processing and compu-\ntational natural language learning (EMNLP-CoNLL), 2007.\n[40] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-\nnition, in: Proceedings of the IEEE conference on computer vision and\npattern recognition, 2016, pp. 770–778.\n[41] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv\npreprint arXiv:1412.6980.\n34\nAppendix A. Derivation of LELBO-VAE\nHere we provide the derivation of LELBO-VAE described in Section 5.1. The\nnotations are deﬁned in Table A.7.\nRecall that in Section 5.1, the ELBO is\nEq(y,z,φ,v|x)\n\u0014\nlog pθ(x|z)p(z|y, φ)p(y|v)p(v)p(φ)\nqψ(z|x)q(y)q(v)q(φ))\n\u0015\n= Eq(y,z,φ,v|x) [log pθ(x|z)] + Eq(y,z,φ,v|x) [log p(z|y, φ)]\n−Eq(y,z,φ,v|x) [log qψ(z|x)]\n+ Eq(y,z,φ,v|x) [log p(y|v)] + Eq(y,z,φ,v|x) [log p(v)]\n−Eq(y,z,φ,v|x) [log q(y)] −Eq(y,z,φ,v|x) [log q(v)]\n−Eq(y,z,φ,v|x) [log q(φ)] + Eq(y,z,φ,v|x) [log p(φ)] .\n(A.1)\nLELBO-VAE only involves the ﬁrst three terms of equation (A.1) that make a\ncontribution to optimize θ, ψ and z since we adopt the alternating optimization\ntechnique.\nWe assume our variational distribution takes the form of\nq(y, z, φ, v|x) = qψ(z|x)q(φ)q(v)q(y)\n= N(µ(x; ψ), diag(σ2(x; ψ)))\nT −1\nY\nt=1\nqηt(vt)\nT\nY\nt=1\nqζt(φt)\nN\nY\nn=1\nqρn(yn)\n= N(µ(x; ψ), diag(σ2(x; ψ)))\nT −1\nY\nt=1\nBeta(ηt1, ηt2)\nT\nY\nt=1\nN(µt|mt, (βtΛt)−1)W(Λt|Wt, νt)\nN\nY\nn=1\nMult(T, ρn),\n(A.2)\nwhere we denote fψ(x) = (µ(x; ψ), σ2(x; ψ)), which is a neural network, T is the\nnumber of mixture components in the DP of the variational distribution, zn ∼\nN(zn|µt, Λ−1\nt ), φt = (µt, Λt), and Mult(T, ρn) is a Multinomial distribution.\nUnder the assumptions in (A.2), we derive each of the ﬁrst three terms in\nEquation A.1 to obtain LELBO-VAE.\n(1) Eqψ(z|x)q(y)q(v)q(φ) [log Pθ(x|z)]:\nWe assume in the generative model that x|z = z ∼N\n\u0000µ(z; θ), diag(σ2(z; θ))\n\u0001\n35\nTable A.7: Notations in the ELBO.\nNotations in the ELBO\nN: the total number of observations.\nL: the number of Monte Carlo samples in Stochastic\nGradient Variational Bayes (SGVB).\nΣ : diag(σ2(x; ψ)).\nxn: the nth observation.\nyn: cluster membership for the nth observation.\np(yn = k) = γik, Nk = PN\nn=1 γnk.\nˆzn = 1\nL\nPL\nl=1 z(l)\nn .\n¯zk =\n1\nNk\nPN\nn=1 γnk ˆzn.\nSk =\n1\nNk\nPN\nn=1 γnk(ˆzn −¯zk)(ˆzn −¯zk)T .\nβk = β0 + Nk: the scalar precision in NW distribution.\nmk =\n1\nβk (β0m0 + Nk¯zk): the posterior mean of cluster k.\nW −1\nk\n= W −1\n0\n+ NkSk +\nβ0Nk\nβ0+Nk (¯zk −m0)(¯zk −m0)T .\nνk = ν0 + Nk: the kth posterior degrees of freedom of NW.\nφ: variational parameters of the kth NW components.\nηt: variational parameters of a Beta distribution for the\ntth component in Equation A.2.\nζt: variational parameters of the NW distribution for φt.\nρn: the variational parameters of a categorical distribution\nfor the cluster membership for each observation.\nand pθ(x|z) is parameterized by a neural network gθ : Z →X and gθ(z) =\n\u0000µ(z; θ), σ2(z; θ)\n\u0001\n. Using the reparameterization trick [23] and the Monte Carlo\nestimate of expectations, we have\nEqψ(z|x)q(y)q(v)q(φ)[log Pθ(x|z)]\n= Eqψ(z|x)q(y)q(v)q(φ)\n\u0012\n−1\n2 log(σ2(z; θ)j) + (xj −µ(z; θ)j)2\nσ2(z; θ)j\n\u0013\n= −1\n2\n1\nL\nL\nX\nl=1\nN\nX\ni=1\nD\nX\nj=1\n\u0012\nlog(σ2(z; θ)(l)\nj\n+\n\u0010\nxij −µ(z; θ)(l)\nj\n\u00112\nσ2(z; θ)(l)\nj\n\u0013\n.\n(A.3)\n36\n(2) Eqψ(z|x)q(y)q(v)q(φ)[log p(z|y, φ)] :\nRecall that ψ, where qψ(z|x) = N(µ(x; ψ), σ2(x; ψ)) and (µ(x; ψ), σ2(x; ψ)) =\nf(x; ψ), where f is a neural network. Following [23], we use the reparameter-\nization and sampling trick to allow backpropagation, for l = 1, 2, . . . , L, where\nL is the number of Monte Carlo samples, we have\nϵ(l) ∼N(0, I) and z(l) = µ(x; ψ) + ϵ(l)σ(x; ψ).\nDeﬁne ˆzn = 1\nL\nPL\nl=1 z(l)\nn . We have\nEqψ(z|x)q(y)q(v)q(φ)[log p(z|y, φ)]\n= 1\n2\nT\nX\nk=1\nNk\nn\nlog ˜Λk −Dβ−1\nk\n−νkTr(SkWk)\no\n−1\n2\nT\nX\nk=1\nNk\n\b\nνk(¯zk −mk)T Wk(¯zk −mk) −D log(2π)\n\t\n,\nwhere\nlog ˜Λk = E[log Λk] =\nD\nX\nj=1\nψ\n\u0012νk + 1 −i\n2\n\u0013\n+ D log 2 + log |Wk|.\n(A.4)\n(3) Eqψ(z|x)q(y)q(v)q(φ)(log qψ(z|x)):\nSince qψ(z|x) = N(µ(x; ψ), diag(σ2(x; ψ))), Eqψ(z|x)q(y)q(v)q(φ)(log qψ(z|x))\nis equal to the negative entropy of a multivariate Gaussian distribution:\nEqψ(z|x)q(y)q(v)q(φ)(log qψ(z|x)) = 1\n2 log(Det(2πeΣ)),\n(A.5)\nwhere Σ = diag(σ2(x; ψ)).\nWhen we update the neural network parameters θ and ψ and the latent\nrepresentation z, the DPMM parameters will be ﬁxed. Thus, the terms that do\n37\nnot involve z, θ, ψ will not contribute to the LELBO−VAE. Hence,\nLELBO−VAE(x) = −1\n2\nT\nX\nk=1\nNkνk {Tr(SkWk)}\n−1\n2\nT\nX\nk=1\nNkνk\n\b\n(¯zk −mk)T Wk(¯zk −mk)\n\t\n−1\n2\n1\nL\nL\nX\nl=1\nN\nX\ni=1\nD\nX\nj=1\n\u0012\nlog(σ(z, θ)2)(l)\nj\n+\n\u0010\nxij −µ(z; θ)(l)\nj\n\u00112\n(σ(z; θ)2)(l)\nj\n\u0013\n+ 1\n2 log(Det(2πeΣ)).\n(A.6)\nDetails of the standard variational inference for parameters in the DP Gaus-\nsian mixture can be found in [10]. We only list the updating equations for the\nvariational parameters below.\n• q(yn = i) = γn,i.\n• q(yn > i) = PT\nj=i+1 γn,j.\n• Eq[log Vi] = Ψ(γi,1) −Ψ(γi,1 + γi,2).\n• Eq[log(1 −Vi)] = Ψ(γi,2) −Ψ(γi,1 + γi,2).\n• St = E[log Vi]+Pt−1\ni=1 Eq[log(1−Vi)]+ 1\n2 log ˜Λk−D\n2βk −νk\n2 (ˆzn−mk)T Wk(ˆzn−\nmk).\n• γn,t ∝exp(St), γn,t =\nexp(St)\nPT\nt=1 exp(St).\n• Under the Normal-Wishart variational distribution assumption,\nEqψ(z|x)q(y)q(v)q(φ)(log p(φ))\n= 1\n2\nT\nX\nk=1\nn\nD log(β0/2π) + log ˜Λk\no\n−1\n2\nT\nX\nk=1\n\u001aDβ0\nβk\n+ β0νk(mk −m0)T Wk(mk −m0)\n\u001b\n+ T log B(W0, ν0)ν0 −D −1\n2\nT\nX\nk=1\nlog ˜Λk −1\n2\nT\nX\ni=1\nνkTr(W −1\n0\nWk),\n38\nwhere\nB(W, ν) = |W|−ν/2\n \n2νD/2πD(D−1)/4\nD\nY\ni=1\nΓ\n\u0012ν + 1 −i\n2\n\u0013!−1\n.\n• Similarly, we have\nEqψ(z|x)q(y)q(v)q(φ)[log q(φ)]\n=\nT\nX\nk=1\n\u00121\n2 log ˜Λk + D\n2 log\n\u0012βk\n2π\n\u0013\n−D\n2 −H[q(Λk)]\n\u0013\n,\nH[Λ] = −log B(W, ν) −ν −D −1\n2\nE[log |Λ|] + νD\n2 .\n39\nAppendix B. Dataset Description\nReuters10k. We are using the same Reuters10k dataset from [25, 26]. The\noriginal Reuters dataset has 810000 English news stories labeled with a category\ntree. Following DEC and VaDE, we use 4 root categories: corporate/industrial,\ngovernment/social, markets, and economics as labels and discard all documents\nwith multiple labels and obtain 685071 articles. We computed tf-idf features\non the top 2000 most frequently occurring words to represent all articles. Since\nsome algorithms do not scale to the full Reuters dataset, a randomly 10000\nexamples are sampled and is referred to as Reuters10 in [25].\nSTL-10. The original STL-10 dataset consists of color images of dimension\n96-by-96. There are 10 classes with a total of 1300 samples. Following [26], we\nextract features of STL-10 images using ResNet-50 [40] to train and test the\nperformance of DBULL and all other competing methods. To be speciﬁc, we\napplied a 3 × 3 average polling over the last feature map of ResNet-50.\n40\nAppendix C. Tuning Parameters and Implementation Details\nAppendix C.1. Hyperparameters in the DPMMs\nIn this paper, we choose the values of the hyperparameters in the DP fol-\nlowing the common choices in the nonparametric Bayesian literature.\nAs we have discussed in Section 4.2, in a DP, the mixture proportions are\ndrawn from a stick-breaking process as\nπk =\n\n\n\n\n\nv1,\nif\nk = 1,\nvk\nQk−1\nj=1(1 −vj),\nfor\nk > 1,\nwhere vk ∼Beta(1, α).\nHere, α is a hyperparameter and we set α = 1.0.\nUnder our Dirichlet Process Gaussian mixture model assumption, we choose\nthe base distribution G0 to be a Normal-Wishart distribution denoted as G0 =\nNW(λ0) to generate the parameters for each Gaussian mixture, where λ0 =\n(m0, β0, ν0, W0) = (0, 0.2, D + 2, ID×D), and D is the dimension of the latent\nvector z. The values of the hyper-parameter λ0 are conventional choices in the\nliterature for Gaussian mixture.\nIn Section 5.6, while performing the cluster expansion and redundancy re-\nmoval strategy, we cache the samples in the subsample if the probability of the\nnth observation to be assigned to cluster k is bigger than a threshold of value\n0.1. We choose the value of 0.1 since [34] have suggested this value while per-\nforming the birth and merge moves for clustering in the nonparametric Bayesian\ncontext.\nAppendix C.2. Tuning Parameters in DNNs and Implementation Details\nIn order to compare fairly with state-of-the-art methods such as VaDE and\nDEC, we adopted the same neural network architecture as DEC and VaDE. The\npipeline is d−500−500−2000−l and l−2000−500−500−d for the encoder and\ndecoder, respectively, where d and l denote the dimensionality of the input and\nlatent features. All layers are fully connected and a sampling layer bridges the\nencoder and decoder. We adopt the same pre-trained Stacked Autoencoder in\n41\nVaDE as the initialization for the neural network. Adam optimizer [41] is used\nas the optimization engine to update the neural network. The batch size is set\nto 1500 for a batch setting. The size of each data stream is set to 1000 and we\ndivide it into two mini-batches. The learning rate for Reuters-10K and STL-10\nis set as 0.002 and 0.0002 for MNIST with a common decay rate of 0.9 for every\nepoch in a batch setting. For the clustering initialization, DEC and VaDE start\nwith K-means or GMM and ﬁx the number of clusters as the ground truth.\nDBULL starts with one cluster and grows or merges clusters using the cluster\nexpansion and redundancy removal technique. In a lifelong learning setting, the\nﬁxed number of samples for replay is 100.\n42\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2021-06-13",
  "updated": "2021-06-13"
}