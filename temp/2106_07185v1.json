{
  "id": "http://arxiv.org/abs/2106.07185v1",
  "title": "Modeling Object Recognition in Newborn Chicks using Deep Neural Networks",
  "authors": [
    "Donsuk Lee",
    "Denizhan Pak",
    "Justin N. Wood"
  ],
  "abstract": "In recent years, the brain and cognitive sciences have made great strides\ndeveloping a mechanistic understanding of object recognition in mature brains.\nDespite this progress, fundamental questions remain about the origins and\ncomputational foundations of object recognition. What learning algorithms\nunderlie object recognition in newborn brains? Since newborn animals learn\nlargely through unsupervised learning, we explored whether unsupervised\nlearning algorithms can be used to predict the view-invariant object\nrecognition behavior of newborn chicks. Specifically, we used feature\nrepresentations derived from unsupervised deep neural networks (DNNs) as inputs\nto cognitive models of categorization. We show that features derived from\nunsupervised DNNs make competitive predictions about chick behavior compared to\nsupervised features. More generally, we argue that linking controlled-rearing\nstudies to image-computable DNN models opens new experimental avenues for\nstudying the origins and computational basis of object recognition in newborn\nanimals.",
  "text": "Modeling Object Recognition in Newborn Chicks using Deep Neural Networks \nDonsuk Lee (donslee@iu.edu)1,2, Denizhan Pak (denpak@iu.edu)1, Justin N. Wood (woodjn@iu.edu)1-4 \n1Department of Informatics, \n2Cognitive Science Program, \n3Center for the Integrated Study of Animal Behavior, \n4Department of Neuroscience, \nIndiana University, Bloomington, IN 47408 USA \nAbstract \nIn recent years, the brain and cognitive sciences have made \ngreat strides developing a mechanistic understanding of object \nrecognition in mature brains. Despite this progress, \nfundamental questions remain about the origins and \ncomputational foundations of object recognition. What \nlearning algorithms underlie object recognition in newborn \nbrains? Since newborn animals learn largely through \nunsupervised learning, we explored whether unsupervised \nlearning algorithms can be used to predict the view-invariant \nobject recognition behavior of newborn chicks. Specifically, \nwe used feature representations derived from unsupervised \ndeep neural networks (DNNs) as inputs to cognitive models of \ncategorization. We show that features derived from \nunsupervised DNNs make competitive predictions about chick \nbehavior compared to supervised features. More generally, we \nargue that linking controlled-rearing studies to image-\ncomputable DNN models opens new experimental avenues for \nstudying the origins and computational basis of object \nrecognition in newborn animals. \nKeywords: deep neural networks; development; controlled \nrearing; categorization; object recognition; unsupervised \nlearning \nIntroduction \nOne of the great unsolved mysteries in cognitive science \nconcerns the origins and computational basis of object \nrecognition. What mechanisms underlie object recognition in \nnewborn brains, and how are those mechanisms shaped by \nexperience? To date, the vast majority of studies have used \nrepresentations learned by deep neural network (DNN) \nmodels to predict behavioral and neural responses in mature \nvisual systems (Bashivan, Kar, & DiCarlo, 2019; Battleday, \nPeterson, & Griffiths, 2020; Cadieu et al., 2014; Kriegeskorte, \n2015; Schrimpf et al., 2020). Such DNN models are typically \ntrained with supervised learning, in which the model learns \nto categorize objects from thousands to millions of labeled \ntraining images. The feature representations of DNN models \ntrained with supervised learning can accurately predict \nbehavioral and neural responses to novel images of objects in \nhumans and nonhuman primates (Battleday et al., 2020; \nYamins & DiCarlo, 2016). Furthermore, DNN models can be \nused to control the activity state of entire populations of \nneural sites (Bashivan et al., 2019). DNN models thus \nprovide a mechanistic understanding of object recognition in \nmature visual systems. \nHowever, supervised DNN models do not accurately \ndescribe how object recognition emerges and develops in \nbiological brains. Unlike supervised DNN models, both \nmature animals and newborn animals learn largely through \nunsupervised learning. Human infants start receiving labeled \nobject input only when they begin understanding language, \nand nonhuman animals receive little (if any) labeled training \ninput during development. Understanding the origins of \nobject recognition therefore requires building computational \nmodels that learn like newborn brains, using unsupervised \nlearning algorithms. \nRecently, researchers have made significant progress \nbuilding unsupervised learning systems that can perform well \non challenging object recognition tasks (e.g. Image Net; \nDeng et al., 2009). Moreover, one particular class of \nunsupervised DNN models—deep unsupervised contrastive \nembedding \nmethods—can \nachieve \nneural \nprediction \naccuracy in multiple areas of the ventral visual system that \nequals or exceeds supervised methods (Zhuang et al., 2021). \nThese methods produce brain-like representations even when \ntrained solely on head-mounted camera data collected from \nyoung children, suggesting that these unsupervised DNNs \ncan serve as biologically plausible hypotheses of learning in \ndeveloping brains. To date, however, it has not been possible \nto apply computational modeling approaches to understand \nhow newborn brains learn about the world.  \nThere are two major barriers in building computational \nmodels of object recognition in newborn brains. First, \nnewborn subjects are hard to study. Most methods for \nstudying development in newborn subjects are low powered \nFigure 1: Illustration of the controlled-rearing chambers. \nThe chambers contained no objects other than the virtual \nobjects projected on the display walls. During the input \nphase, the chicks were exposed to a single virtual object \n(imprinted object). During the test phase, the imprinted \nobject was projected on one display wall and an unfamiliar \nobject was projected on the opposite display wall, in a two-\nalternative forced choice test.  \nand produce noisy measurements of behavior (Wood & \nWood, 2019). This makes it challenging to obtain the precise \nbenchmarks needed to build accurate computational models. \nSecond, it is not possible to control the environment in which \nmost newborn animals are raised. Thus, for most animals, we \ndo not know which visual experiences were used to ‘train’ \ntheir visual system during development. Since the outputs of \nDNN models change radically as a function of the training \ndata, an accurate comparison of DNN models and newborn \nanimals requires training the models and animals with the \nsame set of visual experiences.  \nTo overcome these barriers, Wood (2013) developed an \nautomated controlled-rearing method (described in the next \nsection). This method allows researchers to strictly control \nthe visual experiences of newborn animals (newly hatched \nchicks) and obtain precise behavioral benchmarks of each \nchick’s object recognition performance (Figure 1). Since the \nchicks are reared in controlled environments, we can simulate \n(reproduce) all of the visual training data available to the \nchicks, and then give that same training data to computational \nmodels. By training animals and models on the same set of \ntraining data, we can directly compare their learning abilities.    \nHere, we investigated whether feature representations \nderived from unsupervised DNNs can be used to model \nobject recognition (categorization) behavior of newborn \nchicks in one of these automated controlled-rearing studies \n(Wood, 2013). If the learning mechanisms in DNN models \nresemble those used by newborn brains, then it should be \npossible to use the features learned by DNN models to predict \nchicks’ recognition behavior. Importantly, the DNN models \nwere trained using the same visual experiences presented to \nthe newborn chicks. \nSpecifically, we trained DNNs with unsupervised objective \nfunctions and used the images of the objects that the newborn \nchicks received during the controlled-rearing experiment. \nThen, we used object representations extracted from the \nnetworks as inputs for cognitive models of categorization \n(modified versions of prototype and exemplar models) in \norder to predict the behavior of the chicks. We show that the \ncategorization models operating over unsupervised DNN \nfeature representations \nmake competitive predictions \ncompared to the features derived from supervised DNNs. \nThese results suggest that combining controlled-rearing \nexperiments, DNNs, and cognitive models can be a \npromising research direction for studying the origins and \ncomputational basis of object recognition in newborn brains. \nAutomated Controlled-Rearing Studies of \nNewborn Chicks \nReverse engineering the origins of object recognition \nrequires precise benchmarks showing how specific visual \ninputs shape the development of object recognition. To obtain \nthese benchmarks, we use newborn chicks as a model system. \nUnlike most animals, chicks are uniquely suited for studying \nthe earliest stages of visual development. Chicks are mobile \non the first day of life, require no parental care, and can be \nraised in strictly controlled environments immediately after \nhatching. With chicks, it is therefore possible to study how \nexperience shapes the earliest stages of postnatal visual \ndevelopment. \nResults from previous studies suggest that many object \nperception abilities emerge rapidly during development; for \ninstance, newborn chicks are capable of visual parsing (Wood \n& Wood, 2021), visual binding (Wood, 2014), view-invariant \nobject recognition (Wood, 2013, 2015; Wood & Wood, 2020; \nWood & Wood, 2015), rapid object recognition (Wood & \nWood, 2017), and object permanence (Prasad, Wood, & \nWood, 2019). Remarkably, all of these abilities emerge when \nchicks are reared with a single object, indicating that newborn \nbrains are capable of “one-shot” visual learning. From an \nartificial intelligence perspective, this is an impressive feat. \nMachine learning systems typically require thousands to \nmillions of labeled training images to develop object \nrecognition, whereas chicks develop object recognition from \ninput of a single object seen from a limited range of views.  \nModeling Object Recognition Behavior of \nNewborn Chicks \nIn this work, we focus on modeling the behavioral results \nfrom Wood (2013). In the study, newborn chicks were raised \nfor 2 weeks in controlled-rearing chambers. In the ﬁrst week, \nthe chicks’ visual experience was limited to a single virtual \nobject rotating through a 60° viewpoint range. In the second \nweek, the chicks were tested on their ability to recognize that \nobject from novel viewpoints, using a two-alternative forced \nchoice test. The chicks succeeded on the task, generating \nview-invariant representations that generalized across large, \nnovel, and complex changes in the object’s appearance. \nTask Definition \nWe first provide a formal definition of the two-alternative \nforced choice task that was used to measure view-invariant \nobject recognition in newborn chicks (Wood, 2013). In the \ntask, a visually-naive subject receives a set of k examples X =\n{x1, … , x𝑘} from a single object category during the Input \n(Imprinting) phase. Specifically, X consists of images of the \nimprinted object rotating through a 60° viewpoint range. \nNote that, unlike in traditional categorization tasks, the \nexamples of the second category are not provided during the \nInput phase. During each test trial, the chick is then shown a \npair of stimuli 𝐲= (y+, y−), where y+ and 𝑦− are images of \nobjects from the familiar (positive) and novel (negative) \ncategories, respectively. The set of test stimuli consisted of \nfamiliar and novel images presented across a variety of \nviewpoint ranges. The task is to decide which of the two test \nstimuli belongs to the same category as the learned examples \nin X. Task performance was evaluated based on the \npercentage of time the chicks spent with the imprinted object \nversus the unfamiliar object. If the chicks developed view-\ninvariant object recognition, then they should have spent \nmore time with the imprinted object than the unfamiliar \nobject.  \nCategorization Models for Two-Alternative Forced \nChoice Task \nOur categorization models for the two-alternative forced \nchoice task are derived from the prototype (Rosch, 1973) and \nexemplar (Nosofsky, 1986) theories of categorization. Both \ntheories have been formalized as quantitative models and \nwidely used to study human (McKinley & Nosofsky, 1995) \nand animal (Smith, Zakrzewski, Johnson, Valleau, & Church, \n2016) categorization behaviors. While there is a long-\nstanding debate about whether categories are represented by \nexemplars or by prototypes, it is not our primary goal to \nprovide evidence for one or the other in this study. Rather, \nour goal was to explore whether either model could predict \nperformance in this task when combined with features \nderived from unsupervised DNNs. \nIn the case of typical 2-way categorization tasks, the \ncategorization models express the probability that an input \nstimulus is classified into Category A as follows: \np(Choose A | y) =\nSim(y, cA)γ \nSim(y, cA)γ + Sim(y, cB)γ , \nwhere Sim(⋅) is some function that measures the similarity of  \nan input stimulus y to the category representation cA or cB of \neither category A or B. γ  is a free parameter which \ndetermines the steepness of the decision boundary. Prototype \nand exemplar models differ in the types of similarity \nfunctions they use. In prototype models, category \nabstractions (usually the average of the members in each \ncategory) are used for comparison, whereas in exemplar \nmodels, all of the learned examples are stored and used for \ncomparison.  \nIn our two-alternative forced choice variant of prototype \nand exemplar models, the probability of choosing the positive \nobject y+  (familiar category) given the learned single-\ncategory examples X and a pair of test stimuli 𝐲= (y+, y−) \nis expressed as: \np(Choose y+|y+, y−)  =\n Sim(y+, cX)γ\nSim(y+ , cX)γ + Sim(y−, cX)γ . \nIn the above expression, cX  is the representation of the \nfamiliar category constructed by examples in X. The choice \nof similarity functions for prototype and exemplar models are \ndescribed below. \nIn the prototype model, the familiar (imprinted) category is \nrepresented by the average of members in X: \ncX = 1\n|X| ∑\n𝑥.\n𝑥∈𝑋\n \nThen, the similarity function is an exponential function of the \nnegative distance D between a stimulus y and the category \nrepresentation 𝑐𝑋: \nSim(y, cX) = e−D(y,cX) , \nIn the exemplar model, the familiar category is represented \nby all of the members in X, i.e. cX = X. Then, the similarity \nfunction for the exemplar model is the summation of the \ndistances between a stimulus y and each exemplar x ∈cX: \nSim(y, cX) = ∑e−βD(y,x)\nx∈cX\n, \nwhere β is a free model parameter that determines the rate at \nwhich similarity declines with distance. \nIn both exemplar and prototype models, we used squared \nMahalanobis distance with diagonal covariance matrix 𝐈σ to \nexpress their similarity functions: \nD(y, x) = ∑σ𝑖|x𝑖−y𝑖|2\n𝑑\n𝑖=1\n. \nHere, x, y and σ are vectors in d-dimensional space. In \nexemplar models, σi is often referred to as attention weights \n(McKinley & Nosofsky, 1995). When σ𝑖= 1, the set of \npoints equidistant from a given location is a sphere. With σ𝑖 \nas free parameters, the models have flexibility to stretch this \nsphere to correct the respective scales of feature dimensions. \nLearning Feature Representations Using \nUnsupervised Deep Neural Networks \nTypically, psychological models of categorization use hand-\ncoded descriptions or features derived from similarity \njudgements to represent stimuli. In the real world, however, \nbrains must convert high-dimensional sensory inputs (106 \noptic nerve fibers) into object-centric representations that can \nguide adaptive behavior. Given that early visual experience \ncan heavily shape object recognition (e.g., Wood & Wood, \n2016; 2018), we argue that the features that serve as inputs to \nthese cognitive models should be learned from visual \nexperience. We take a first step towards this goal by using \nfeature representations learned by DNNs.  \nWe used a similar approach as Battleday, Peterson, & \nGriffiths (2020), extracting features from DNNs to serve as \ninputs to the categorization models. We then compared the \nfeatures extracted from supervised and unsupervised DNNs \nin terms of their ability to predict the chicks’ performance. \nCrucially, we trained our DNN feature extractors using \nimages sampled from virtual controlled-rearing chambers \nthat were designed to mimic the controlled-rearing chambers \nin which we collected the data from the newborn chicks. \nThe key principle of unsupervised DNN models is that self-\nsupervised “proxy” tasks (e.g. reconstructing the input image) \ncan produce representations that are useful for downstream \ntasks (e.g. object recognition). We used two classes of \nunsupervised learning algorithms to train our DNN feature \nextractors. \nVariational autoencoder. The variational autoencoder \n(Kingma & Welling, 2014) is a class of unsupervised \ngenerative models that approximates the true distribution of \nthe observed data using variational objective functions. It has \nbeen proposed that a modification (β-VAE; Higgins et al., \n2017) to the original objective function of VAEs can result in \n“disentangled” visual representations, which is critical for \nbiological visual systems to solve invariant object \nrecognition tasks (DiCarlo & Cox, 2007). 𝜷-VAE models are \ntrained to maximize the following objective: \n𝔼𝑝(𝐱) [𝔼𝑞𝜙 (𝐳|𝐱)[log 𝑝𝜃(𝐱|𝐳)] −𝛽KL (𝑞𝜙(𝐳|𝐱) ∥𝑝(𝐳))], \nwhere 𝐳 is the latent factor generating data 𝐱, and  𝑝𝜃(𝐱|𝐳) \nand 𝑞𝜙(𝐳|𝐱) are learned by the encoder and decoder neural \nnetworks. This objective with 𝛽= 1  corresponds to the \noriginal formulation by Kingma & Welling (2014). The \nhyperparameter 𝛽 is set to a value greater than 1 in order to \nlearn disentangled representations 𝐳. \nContrastive learning. In addition, we used a contrastive \nlearning method entitled SimCLR (Chen, Kornblith, Norouzi, \n& Hinton, 2020). Contrastive algorithms rely on self-\nsupervised prediction tasks for learning discriminative \nrepresentations. These methods learn an embedding space by \nmaximizing the distance between the “dissimilar” (negative) \nsamples while minimizing the distance between “similar” \n(positive) samples. Unlike supervised methods, contrastive \nlearning methods require an unsupervised approach to \ncharacterize positive and negative examples. The SimCLR \n(Chen et al., 2020) creates a positive pair by applying random \nimage transformations such as color distortion, rotation, or \nGaussian blur to each image 𝒙. A DNN encoder is then used \nto produce representations for the transformed image pairs. \nThe contrastive loss is calculated to minimize the distance \nbetween the representations of positive pairs, while \nmaximizing their distance to other samples. \nMethod \nStimuli. The stimulus set from Wood (2013) consisted of 24 \nanimations of two 3D objects (12 animations for each object). \nEach animation displayed an object rotating through a 60° \nviewpoint range about an axis passing through its centroid, \ncompleting the full back and forth rotation every 6s. For each \nobject animation, we sampled 26 frames spanning the 60° \nrotation (2.3° rotation in 0.12s between two adjacent frames), \nso the final stimulus set consisted of 624 images. Each of the \nsampled images was treated as a single input stimulus for the \ncategorization models. \nChick behavioral data. We used the behavioral data \ncollected by Wood (2013), which included 35 newborn \nchicks. After hatching, each chick was moved from the \nincubator to a controlled-rearing chamber in darkness. The \ncontrolled-rearing chamber was equipped with two monitors \nthat were used to display the object animations. The chicks’ \nentire visual object experience was limited to the virtual \nobjects projected on the monitors. In the Input phase, the \nimprinted object was displayed from a single 60° viewpoint \nrange. Half of the chicks were imprinted to object A, and half \nof the chicks were imprinted to object B. In the Test phase, \nthe chicks were tested with 12 viewpoint ranges of familiar \nand unfamiliar objects. Test trials were scored as “correct” \nwhen the chicks spent a greater proportion of time with their \nimprinted object and “incorrect” when the chicks spent a \ngreater proportion of time with the unfamiliar object. \nDNN architectures and training. In order to mimic the \nvisual experiences of the chicks raised in the controlled-\nrearing chambers, we constructed an image dataset by \nsampling visual observations of an agent that moved \nrandomly within a virtual controlled-rearing chamber (Figure \n2). The virtual chamber and agent were created with the Unity \n3D game engine and the ML-Agents Toolkit (Juliani et al., \n2020). We programmatically removed images that did not \ncontain an object. The resulting dataset contained 240,000 \nimages (10k images for each of the 24 object animations). All \nDNN feature extractors were trained on this synthetic dataset. \nWe used a standard ResNet architecture (He, Zhang, Ren, \n& Sun, 2016) with 18 layers as the base encoder for all of our \nDNN feature extractors. For 𝛽-VAE, the encoder was \nfollowed by two fully-connected heads with 256 units, which \ninferred Gaussian mean and variance of the posterior latent \ndistribution. The decoder architecture was in the reverse \norder of the encoder architecture. We trained three 𝛽-VAEs \nwith 𝛽= 0, 1, and 4. For the SimCLR model, the encoder \nwas followed by a 2-layer MLP projection head. All DNNs \nwere trained and optimized using the Adam optimization \nalgorithm (Kingma & Ba, 2017) for 100 epochs. \nFigure 2: Sample images from the synthetic dataset used to train the DNN feature extractors. The dataset was constructed by \nrecording the visual observations of an agent moving within a virtual controlled-rearing chamber.  \nFeature representations. We extracted features for each \nstimulus using DNNs trained with the unsupervised \nobjectives described in the previous section. For both \nSimCLR and 𝜷-VAE, the features were taken from the last \nlayer of the ResNet base encoder. The feature dimensions \nwere 512. \nCategorization model fitting. We fitted the categorization \nmodels by maximizing the log-likelihood of the observed \nbehavioral data. We used the Adam algorithm (Kingma & Ba, \n2017) with a learning rate of 0.003 and a batch size of 256 to \nfind the parameters of the categorization models. We \nperformed a 6-fold cross validation on the chick behavioral \ndata by randomly splitting the data by test conditions. Model \nparameters were chosen based on the average log-likelihood \non held-out data across the 6 validation folds. We used a \nPyTorch \npackage \nto \nimplement \nand \noptimize \nall \ncategorization models. \nModel comparison. We used the cross-validated negative \nlog-likelihood (NLL) averaged across folds to evaluate \ngoodness of fit (lower NLL indicates better fit). In addition, \nwe calculated the Pearson correlation between the model \npredictions and the task accuracies of the chicks across the \ntest conditions to measure how much behavioral variance is \ncaptured by each model. \nWe estimated the noise ceiling by computing the split-half \ncorrelation of the behavioral data. We randomly split the \nsubjects’ responses for each pair of test stimuli into two half-\nsets, computed the correlation between the two sets, and \napplied a Spearman-Brown correction. We repeated these \nsteps 100 times and averaged the corrected correlations to \nobtain the final estimate. \nBaselines. We had two baseline features: untrained and \nsupervised. The untrained features were extracted from a \nrandomly initialized ResNet-18 encoder. For the supervised \nfeatures, we trained a ResNet-18 encoder followed by a fully-\nconnected classification layer. The supervised ResNet was \ntrained with the same image dataset as the unsupervised \nfeature extractors, but critically, the model was optimized for \nbinary cross-entropy loss using ground truth category labels. \nWe expected that supervised features would make accurate \npredictions about chicks’ categorization behavior because \nthey compactly encode the category structure of the stimuli. \nResults \nTable 1 shows the NLL and correlation scores for 12 different \ncombinations of DNN features and categorization models. \nFirst, the results show that prototype and exemplar models \nproduce comparable results for all feature representations. \nWhile exemplar models outperformed prototype models with \nmost of the feature representations, the  differences were not \nlarge. As discussed in Battleday et al. (2020), this pattern \nsuggests that the structure of the categories might be \nrelatively simple in relation to the dimensionality of features \nwe used in our experiments. \nNext, we compared the best model scores across the feature \nrepresentations (bold numbers in Table 1). As expected, \nuntrained features resulted in poor fits and supervised \nfeatures produced the best fit out of all feature representations. \nAlso, all unsupervised features outperformed untrained \nfeatures. Notably, the contrastive features (SimCLR) \nperformed similar to the supervised features, with only 0.003 \ndifference in NLL compared to supervised features. Contrary \nto our expectation, β-VAE features performed the best with \nthe lowest disentanglement parameter (β = 0), in which case \nthe β-VAE reduces to a basic convolutional autoencoder. \nWhile all unsupervised features obtained slightly lower \nNLL scores compared to the supervised features, SimCLR \nFigure 3: Actual task performance of the chicks (dotted lines) and predicted task performance of the models (blue lines). \nEach axis represents the average task accuracy across each of the six experimental conditions (Exp. 1A/B, 2A/B, & 3A/B, \nwhere A and B denote each of the two imprinted objects). The graphs show the predictions for each of the four feature \nrepresentations (supervised, SimCLR, VAE, & untrained) in the prototype (proto) and exemplar (exem) categorization \nmodels. \nTable 1: Model results. Lower NLL scores, and higher \ncorrelation scores, indicate better fit to the chick data. \n \nand VAE (β =0) features achieved higher correlation scores \nthan the supervised features. This implies that these \nunsupervised features can outperform the supervised features \nin accounting for variance in newborn chicks’ categorization \nbehavior. In Figure 3, we display the model predictions and \nthe chicks’ categorization performance across the six \nexperimental conditions from Wood (2013). The supervised \nfeatures often overestimated the chicks’ performance (in \nExp1B, 3A, 3B) because the supervised encoder learned \nhighly discriminative features. Conversely, unsupervised \nfeatures mostly underestimated the chicks’ performance but \ncould better capture their confusion patterns. \nIn general, these results indicate that unsupervised methods \ncan build representational spaces that predict chicks’ object \nrecognition performance, without the need for the \nbiologically unrealistic labelling of training data required for \nsupervised methods. \nDiscussion \nPsychological models of categorization have produced \nformal and predictive models of categorization behavior in \nhuman adults. Here we build on this rich research tradition by \nlinking cognitive models of object categorization to \ncontrolled-rearing studies of newborn animals. Specifically, \nwe explored whether cognitive models can accurately predict \nthe emerging object recognition behavior of newborn chicks, \nwithin natural settings that involve complex, high-\ndimensional stimuli. We found that when cognitive models \nare provided with input features derived from supervised and \nunsupervised DNNs, the cognitive models could predict \nchicks’ behavioral performance. \nOne benefit of using DNNs is that they are image \ncomputable, meaning that they can generate responses for \narbitrary input images. By using DNNs to learn high-\ndimensional feature spaces, we can make comparisons across \nanimals and models without making prior assumptions about \nthe specific features used to build object representations in \nnewborn brains. Additionally, the fact that unsupervised \nfeatures perform comparably to supervised features indicates \nthat we can use self-supervised learning algorithms to \ndevelop computational theories of visual learning, using \nbehavioral data from newborn animals as benchmarks.   \nThere are, of course, many more unsupervised learning \nmodels that could be explored within this framework. There \nis also much more work that needs to be done to build a better \nunderstanding of the feature spaces produced by DNN \nmodels. The approach described here—which involves \nlinking controlled-rearing studies of newborn animals to \ncomputational models in artificial intelligence—may prove \nhelpful in this regard. By training newborn animals and \nDNNs with the same set of high-dimensional sensory data, \nwe can control a potentially large source of variation in the \nlearning of visual features. Ultimately, we argue that this \napproach provides an experimental avenue for reverse \nengineering the learning mechanisms in newborn brains and \ndiscovering the algorithmic principles that drive intelligent \nbehavior. \nLimitations and Future Work \nThere are at least two limitations to the present work. First, \nalthough controlled-rearing studies allow us to impose strong \nconstraints on the training data provided to DNNs, there are \nstill some mismatches in the training data presented to the \nchicks and the DNNs that prohibit direct comparison between \nnewborn animals and computational models. Besides some \nobvious mismatches like input resolution and field of view, \nour current approach ignores the temporal order of the visual \ninput. Moreover, to make this modeling approach more \ntractable, we assumed that the visual inputs from all of the \ntest stimuli were available to the chicks during learning. In \nthe experiment, however, the chicks were only presented with \na single viewpoint range during the input phase. In future \nwork, we will reduce the amount of training data available to \nthe DNNs to more precisely match the training data provided \nto the newborn chicks during the imprinting period.   \nSecond, the objective functions of contrastive learning \nalgorithms may not be (entirely) biologically plausible. As \nfar as we know, there is no biological evidence that the visual \nsystem employs stochastic transformations of sensory inputs \n(e.g. \nrandom \ncolor \ndistortion) \nfor \nlearning \nvisual \nrepresentations. A promising future direction is to use more \nbiologically plausible unsupervised learning algorithms that \nexploit spatiotemporal information. For instance, recent \nstudies suggest that deep spatiotemporal contrastive learning \nmethods can achieve primate-level representation learning \n(Zhuang et al., 2021). In future work, we will use a similar \napproach to explore whether contrastive learning methods \ncan build the same kinds of object representations as newborn \nchicks when provided with similar streams of high-\ndimensional sensory data. \nAcknowledgements \nFunded by NSF CAREER Grant BCS-1351892 and a James \nS. McDonnell Foundation Understanding Human Cognition \nScholar Award. \nFeatures \nModel \nNLL \nCorr. \nUntrained \nPrototype \n0.640 \n0.407 \n \nExemplar \n0.641 \n0.442 \nSupervised \nPrototype \n0.608 \n0.464 \n \nExemplar \n0.606 \n0.543 \nVAE (β =0)\nPrototype \n0.622 \n0.398 \n \nExemplar \n0.616 \n0.561 \nVAE (β =1) \nPrototype \n0.675 \n-0.190 \n \nExemplar \n0.673 \n-0.152 \nVAE (β =4) \nPrototype \n0.693 \n0.0 \n \nExemplar \n0.693 \n0.0 \nSimCLR \nPrototype \n0.625 \n0.449 \n \nExemplar \n0.609 \n0.649 \n* Correlation noise ceiling \n- \n0.753 \nReferences  \nBashivan, P., Kar, K., & DiCarlo, J. J. (2019). Neural \npopulation control via deep image synthesis. Science, 364, \neaav9436. \nBattleday, R. M., Peterson, J. C., & Griffiths, T. L. (2020). \nCapturing human categorization of natural images by \ncombining deep networks and cognitive models. Nature \nCommunications, 11, 5418. \nCadieu, C. F., Hong, H., Yamins, D. L. K., Pinto, N., Ardila, \nD., Solomon, E. A., … DiCarlo, J. J. (2014). Deep Neural \nNetworks Rival the Representation of Primate IT Cortex \nfor Core Visual Object Recognition. PLOS Computational \nBiology, 10, e1003963. \nChen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A \nSimple Framework for Contrastive Learning of Visual \nRepresentations. ArXiv:2002.05709 [Cs, Stat]. Retrieved \nfrom http://arxiv.org/abs/2002.05709 \nDiCarlo, J. J., & Cox, D. D. (2007). Untangling invariant \nobject recognition. Trends in Cognitive Sciences, 11, 333–\n341. \nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., \nBotvinick, M., … Lerchner, A. (2017). β-VAE: Learning \nBasic Visual Concepts with a Constrained Variational \nFramework. 22. \nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, & Li Fei-Fei. \n(2009). ImageNet: A large-scale hierarchical image \ndatabase. 2009 IEEE Conference on Computer Vision and \nPattern Recognition, 248–255. \nJuliani, A., Berges, V.-P., Teng, E., Cohen, A., Harper, J., \nElion, C., … Lange, D. (2020). Unity: A General Platform \nfor Intelligent Agents. ArXiv:1809.02627 [Cs, Stat]. \nRetrieved from http://arxiv.org/abs/1809.02627 \nK. He, X. Zhang, S. Ren, & J. Sun. (2016). Deep Residual \nLearning for Image Recognition. 2016 IEEE Conference \non Computer Vision and Pattern Recognition (CVPR), \n770–778. \nKingma, D. P., & Ba, J. (2017). Adam: A Method for \nStochastic Optimization. ArXiv:1412.6980 [Cs]. Retrieved \nfrom http://arxiv.org/abs/1412.6980 \nKingma, D. P., & Welling, M. (2014). Auto-Encoding \nVariational Bayes. ArXiv:1312.6114 [Cs, Stat]. Retrieved \nfrom http://arxiv.org/abs/1312.6114 \nKriegeskorte, N. (2015). Deep Neural Networks: A New \nFramework for Modeling Biological Vision and Brain \nInformation Processing. Annual Review of Vision Science, \n1, 417–446. \nMcKinley, S. C., & Nosofsky, R. M. (1995). Investigations \nof exemplar and decision bound models in large, ill-\ndefined category structures. Journal of Experimental \nPsychology: Human Perception and Performance, 21, \n128–148. \nNosofsky, R. M. (1986). Attention, similarity, and the \nidentification–categorization relationship. Journal of \nExperimental Psychology: General, 115, 39–57. \nPrasad, A., Wood, S. M. W., & Wood, J. N. (2019). Using \nautomated controlled rearing to explore the origins of \nobject \npermanence. \nDevelopmental \nScience, \n22. \nhttps://doi.org/10.1111/desc.12796 \nRosch, E. H. (1973). Natural categories. Cognitive \nPsychology, 4, 328–350. \nSchrimpf, M., Kubilius, J., Lee, M. J., Ratan Murty, N. A., \nAjemian, R., & DiCarlo, J. J. (2020). Integrative \nBenchmarking to Advance Neurally Mechanistic Models \nof Human Intelligence. Neuron, S089662732030605X. \nSmith, J., Zakrzewski, A., Johnson, J., Valleau, J., & Church, \nB. (2016). Categorization: The View from Animal \nCognition. Behavioral Sciences, 6, 12. \nWood, J. N. (2013). Newborn chickens generate invariant \nobject representations at the onset of visual object \nexperience. Proceedings of the National Academy of \nSciences, 110, 14000–14005. \nWood, J. N. (2014). Newly Hatched Chicks Solve the Visual \nBinding Problem. Psychological Science, 25, 1475–1481. \nWood, J. N. (2015). Characterizing the information content \nof a newly hatched chick’s first visual object representation. \nDevelopmental Science, 18, 194–205. \nWood, J. N., & Wood, S. M. W. (2016). The development of \nnewborn object recognition in fast and slow visual worlds. \nProceedings of the Royal Society B: Biological Sciences, \n283, 20160166. \nWood, J. N., & Wood, S. M. W. (2017). Measuring the speed \nof newborn object recognition in controlled visual worlds. \nDevelopmental Science, 20, e12470. \nWood, J. N., & Wood, S. M. W. (2018). The Development of \nInvariant Object Recognition Requires Visual Experience \nWith Temporally Smooth Objects. Cognitive Science, 42, \n1391–1406. \nWood, J. N., & Wood, S. M. W. (2020). One-shot learning of \nview-invariant object representations in newborn chicks. \nCognition, 199, 104192. \nWood, S. M. W., & Wood, J. N. (2015). A chicken model for \nstudying the emergence of invariant object recognition. \nFrontiers \nin \nNeural \nCircuits, \n9. \nhttps://doi.org/10.3389/fncir.2015.00007 \nWood, S. M. W., & Wood, J. N. (2019). Using automation to \ncombat the replication crisis: A case study from controlled-\nrearing studies of newborn chicks. Infant Behavior and \nDevelopment, 57, 101329. \nWood, S. M. W., & Wood, J. N. (2021). One-shot object \nparsing in newborn chicks. Journal of Experimental \nPsychology: General. \nYamins, D. L. K., & DiCarlo, J. J. (2016). Using goal-driven \ndeep learning models to understand sensory cortex. Nature \nNeuroscience, 19, 356–365. \nZhuang, C., Yan, S., Nayebi, A., Schrimpf, M., Frank, M. C., \nDiCarlo, J. J., & Yamins, D. L. K. (2021). Unsupervised \nneural network models of the ventral visual stream. \nProceedings of the National Academy of Sciences, 118, \ne2014196118. \n \n",
  "categories": [
    "cs.AI",
    "q-bio.NC"
  ],
  "published": "2021-06-14",
  "updated": "2021-06-14"
}