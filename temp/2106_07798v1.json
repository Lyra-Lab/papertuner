{
  "id": "http://arxiv.org/abs/2106.07798v1",
  "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers",
  "authors": [
    "Chace Ashcraft",
    "Kiran Karra"
  ],
  "abstract": "In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.",
  "text": "Published at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\nPOISONING\nDEEP\nREINFORCEMENT\nLEARNING\nAGENTS WITH IN-DISTRIBUTION TRIGGERS\nChace Ashcraft, Kiran Karra\nJohns Hopkins University\nApplied Physics Lab\nLaurel, MD 20723, USA\n{chace.ashcraft,kiran.karra}@jhuapl.edu\nABSTRACT\nIn this paper, we propose a new data poisoning attack and apply it to deep rein-\nforcement learning agents. Our attack centers on what we call in-distribution trig-\ngers, which are triggers native to the data distributions the model will be trained\non and deployed in. We outline a simple procedure for embedding these, and\nother, triggers in deep reinforcement learning agents following a multi-task learn-\ning paradigm, and demonstrate in three common reinforcement learning environ-\nments. We believe that this work has important implications for the security of\ndeep learning models.\n1\nINTRODUCTION\nAlthough deep learning has shown very impressive performance on a plethora of tasks, because of\nthe high cost of data collection, maintenance, and model training, much of the deep learning pipeline\nis, or could be, outsourced to third parties. This leaves model consumers vulnerable to supply chain\nvulnerabilities, including backdoor attacks. As part of this work we propose a new attack variant\nwhich we believe can be just as detrimental as other attacks while being more difﬁcult to detect or\nmitigate, even through human inspection.\nBackdoor attacks, also known as trojan attacks, against deep neural networks for classiﬁcation have\nbeen well studied (Gu et al., 2017), and many variants of backdoor attacks have been presented\n(Shafahi et al., 2018; Bagdasaryan & Shmatikov, 2020; Chen et al., 2017; Kurita et al., 2020; Li\net al., 2019; Liu et al., 2020; Tran et al., 2018; Turner et al., 2018; Yao et al., 2019). Most of the\ncurrent research on attacks, detection, and mitigation is focused on classiﬁcation networks for vision\napplications. Some preliminary research into backdoor attacks for text classiﬁcation networks also\nexist (Dai et al., 2019; Sun, 2020).\nKiourti et al. (2020) were among the ﬁrst to show that DRL agents are also susceptible to poi-\nsoning attacks, and outline an algorithm to poison DRL agents under a man-in-the-middle attack\nmodel assumption. In this paper, we expand upon Kiourti et al. (2020) in the following ways: 1)\nWe demonstrate how to poison deep reinforcement learning (DRL) agents under an alternate attack\nmodel, which motivates an important distinction between simple and in-distribution triggers that im-\npacts vulnerability and defense research. 2) We outline and demonstrate a procedure for embedding\nbackdoors into RL agents through the lens of multitask learning, which is interesting due to its sim-\nplicity and its connection to another research area in DRL; and provide concrete implementations in\npopular DRL environments 1.\n2\nTROJANS IN REINFORCEMENT LEARNING AGENTS\nAll methods of embedding backdoors into neural networks involve some manner of data manipula-\ntion. Unlike classiﬁcation, in DRL, data is generated by an agent interacting with the environment.\nTrojDRL (Kiourti et al., 2020) proposes data poisoning attacks on DRL agents by altering the ob-\nservations, after they have been generated by the environment, under a man-in-the-middle (MITM)\n1Code will be released at https://github.com/trojai\n1\narXiv:2106.07798v1  [cs.LG]  14 Jun 2021\nPublished at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\nattack model. More precisely, the observations are altered by a third party, before arriving at the\nagent to be processed. The altered observations serve as triggers to the agent to behave with an\nalternate goal.\nIn this work, we consider an attack model where the DRL environments generate poisoned obser-\nvations. Our contributions are two-fold: 1) Our proposed method of training agents with triggered\nbehavior is simpler than the algorithms outlined by Kiourti et al. (2020), cast as a multitask learning\nproblem, which is an approach that, to the best of our knowledge, has not been explored by other\npublished methods for this purpose, and 2) It enables further research into triggers which may not\nbe as easily supported by the MITM attack model, such as triggers that may emerge from multiple\nagents interacting in the environment.\n2.1\nFORMALISM\nThe goal of a DRL agent is maximize the total reward received by interacting with its environment,\nwhich is typically described as a Markov Decision Process (MDP) (S, A, P, R, γ), where S is\nthe set of states in the environment, A is the set of actions (or the action space) available to the\nagent, P : S × A × S −→[0, 1] is the transition probability of going from state s to state s′ when\ntaking action a, R : S × A −→R is the reward for taking action a from state s, and γ ∈[0, 1]\nis the discount factor. More accurately, many DRL environments are better represented as Partially\nObservable Markov Decision Process (POMDP), (S, A, P, R, Ω, O, γ), where Ωis a set of possible\nobservations that depend on the state of the environment, and O : O × A × S −→[0, 1] is the\nprobability of receiving observation o given that the agent is in state s′ after taking action a.\n2.2\nTRIGGERS\nBehavior change in poisoned RL agents is enabled by a trigger in the agent’s observation. A trigger\nis deﬁned as an attribute of the agent’s observation that causes the agent to behave with an alternative\ngoal. Triggers can occur in whatever representation the RL agent uses to accomplish its goal; the two\nrepresentations that we consider in this paper are state-space and visual representations. State-space\nrepresentations are vectors which contain meaningful information about the state of the environment\nand are usually expertly deﬁned, whereas visual representations are images of the agent’s ﬁeld of\nview.\nFor these two representations, we differentiate between simple triggers and in-distribution triggers.\nA simple trigger refers to any change to an observation which can cause the agent to behave differ-\nently. Examples of simple triggers in state-space may include adding or multiplying a constant to\nthe state vector, and applying the modulo operator to be within the agent’s input space. In the vision\nspace, simple triggers may include changes to inconspicuous pixels, such as artiﬁcially altering the\ncolor of the top right pixel in the environment. While these triggers highlight the vulnerability of\nDRL agents to small perturbations in their observations, they require unnatural modiﬁcations to the\nRL environment.\nIn contrast, we deﬁne an in-distribution trigger as changes to an observation that are not anomalous\nto the environment. More formally, borrowing from POMDP terminology, let Ωto be the set of pos-\nsible observations that the environment can produce. We deﬁne in-distribution triggers to be patterns\nin observations contained in Ω, and simple triggers to be patterns in observations not contained in\nΩ.\nExamples of in-distribution triggers in the state-space are speciﬁc patterns of input which can oc-\ncur as a result of agent interaction with the environment, and which may have a speciﬁc semantic\nmeaning. For example, suppose the state-space representation for an agent playing Atari Breakout\nis a vector where the ﬁrst element represents the ball speed, the second element represents the ball\ndirection, the third represents the ball location, and the fourth represents the agent location. An\nin-distribution trigger, here, could be if the ball location, speed, and direction are equal to a speciﬁc,\npreprogrammed value. Similarly, an example of the in-distribution trigger in the image-space repre-\nsentation could be a certain conﬁguration of the color of game objects, if that information is in the\nagent’s ﬁeld of view. Note that in-distribution triggers do not require modiﬁcations to the RL envi-\nronment observations, although RL environments could be designed to contain usable in-distribution\ntriggers. We hypothesize that this makes in-distributions more difﬁcult to detect, because trigger de-\n2\nPublished at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\ntection methods typically rely on some manner of anomaly detection (Xu et al., 2019; Chen et al.,\n2018; 2019; Huang et al., 2019; Liu et al., 2019), and consequently more dangerous.\n2.3\nTRAINING METHODOLOGY\nAs a proof of concept, we embedded both simple and in-distribution triggers in DRL agents using\na training strategy akin to multitask learning (Mnih et al., 2015) with Proximal Policy Optimization\n(PPO) (Schulman et al., 2017) 2. If we consider the clean behavior as one task, and the trojan, or\npoisoned, behavior as another, then the problem of embedding the trigger reduces to a multitask\nlearning problem, which we solve by training on both triggered environments and clean environ-\nments in parallel. We found this strategy to work very reasonably, when the number of triggered\nand clean environments was tuned to ensure that both tasks were learned. We refer to this as data\nbalancing.\nInspired by previous work with embedding trojans in classiﬁcation models (Karra et al., 2020),\nwe attempt to balance the amount of clean and triggered data that the RL agent receives during\ntraining. We found that having an appropriate balance was often critical for efﬁcient learning. For\nexample, 10%-20% triggered data often lead to successful training, which implemented by training\non 2 triggered environments and 8 clean environments in parallel.\n2.4\nEXAMPLES\n2.4.1\nSIMPLE TRIGGER: ATARI BOXING\nWe implemented two simple triggers via a modiﬁed version of OpenAI Gym’s (Brockman et al.,\n2016) version of Atari Boxing. The ﬁrst uses the game’s RAM observations (i.e. “Boxing-ram-v0”),\nand the trigger is to set the observation to 10× the original observation modulo 255. The second\ntrigger uses RGB images (i.e. “Boxing-v0”), with the trigger being the top left corner greyed out,\nsimilar to Kiourti et al. (2020). We produce trojaned behavior by negating the reward when the\ntrigger is active (i.e. the environment is triggered or poisoned).\nWe use a fully-connected network for the RAM observations, and a CNN for the RGB observations.\nThe fully connected network consists of a shared embedding that is two hidden layers of 512 nodes\nwith ReLU activations, and an actor and critic each consisting of one hidden layer of 512 nodes and\nReLU activations. The CNN is similar, but has three convolution layers with ReLU activations as\nthe shared embedding. Observations are resized to 48x48 gray-scale images before being passed\ninto the network.\nTraining required 21 million frames of data for the RAM model and 25 million frames for the\nimage model. We used a split of 8 clean, 2 triggered environments for each observation type. The\nresulting performances where scores of 52 average reward for clean environments for both types\nof observations, and -66 and -60 average reward for triggered environments for RAM and image\nobservation types, respectively.\n2.4.2\nIN-DISTRIBUTION TRIGGER: PARAMETERIZED LAVAWORLD\nIn\norder\nto\ndemonstrate\nan\nexample\nof\nthe\nin-distribution\ntrigger,\nwe\nintroduce\nthe\nParameterized\nLavaWorld\nenvironment,\nwhich\nis\na\nmodiﬁed\nversion\nof\nthe\nMiniGrid-LavaCrossingS9N1-v0\nenvironment\nfrom\ngym-minigrid\n(Chevalier-\nBoisvert et al., 2018) Python module. We modify the original environment to include an extra line\nof 3 lava squares, and parameterize the placement of the main “river” of lava, the location of the\ngap in the river, and the location of the extra lava squares, via arguments to the reset method.\nThe trigger occurs when the extra line of lava squares forms a cross or “T” with the main river;\nFigure 1. If the agent sees this cross in one of its observations, the reward function returns a positive\nreward only if the agent enters one of the lava squares in the trigger. Otherwise, the reward function\nremains the same. By deﬁnition, this trigger is in-distribution because it is a valid observation of\n2We used an implementation of PPO from the torch ac module publicly available (https://github.\ncom/lcswillems/torch-ac) for all of our examples.\n3\nPublished at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\nFigure 1: The goal of Parameterized Lavaworld is to get to the green goal square without stepping\nin an orange lava square. The trigger is a cross or “T” pattern (left) of lava squares, such that when\nthe agent sees the pattern, it enters one of the pattern’s squares instead of going to the goal.\nthe normal environment. Indeed, the same environment can be run without the reward modiﬁcation,\nand would be similar to the original gym-minigrid LavaCrossing tasks.\nThis environment may output a simpliﬁed, but image like, (7, 7, 3) state observation, or an RGB\nimage observation. We were able to embed a trigger for both observation types, but focus here\non simpliﬁed observation case. We trained a standard actor-critic model with a CNN based image\nembedding but omitting the recurrent network 3, using 10 environments in parallel where the trigger\nwas active in each, and trained for 40 million frames. The resulting agent successfully navigated to\nthe goal in 97 of the 100 test episodes when the trigger was not present, and navigated to the trigger\npattern in 94 of the 100 test episodes when the trigger was observed. Further, we believe we could\nincrease both clean and triggered performance via basic hyperparameter tuning.\n2.5\nIN-DISTRIBUTION TRIGGER: PURSUIT\nFor a deep multi-agent reinforcement learning (DMARL) example, we consider the Pursuit environ-\nment from the PettingZoo library (Terry et al., 2020a), which consists of various “pursuer” agents\nthat must work together in a 2D space in order to surround other “evader” agents. The pursuers are\ncontrolled by DMARL agents, and the evaders move randomly.\nTo insert a trigger we ﬁrst follow the approach in Terry et al. (2020b) to train clean agents using the\ndeep reinforcement learning package RLlib (Liang et al., 2018). For simplicity, we conﬁgure the\nenvironment to have 4 agents. During training, we only populate the environment with 1 evader and\nwe set all “touch” rewards, which are given when a pursuer is next to an evader, to 0. We found this\nhelped force the agents to learn to seek out evaders and surround them.\nWe then insert a trigger by ﬁne-tuning a trained, clean model for a single pursuer using an altered\nreward function. The triggered reward structure provides positive reward (the same as the surround\nreward) when the agent sees itself one step away from surrounding the evader, but then receives\na negative reward for the actual surround, and is 0 otherwise. We train the triggered agent for\napproximately 1.2 million frames, after which we observe mean triggered reward converge to around\n250, meaning that the triggered agent is frequently entering the “almost surround” state without\nmaking the surrounds.\n3\nCONCLUSION\nIn this paper, we have introduced an alternate backdoor attack using in-distribution triggers and a\nnew, but simple, training method for embedding backdoors into RL agents. We hypothesize that\nin-distribution triggers, due to their nature, are more difﬁcult to detect and present a new problem\nfor the security community to address. The problem seems particularly signiﬁcant given the ease\nwith which these triggers may be embedded.\nACKNOWLEDGMENTS\nThis research is based upon work supported in part by the Ofﬁce of the Director of National Intelli-\ngence (ODNI), Intelligence Advanced Research Projects Activity (IARPA).\n3https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n4\nPublished at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\nREFERENCES\nEugene Bagdasaryan and Vitaly Shmatikov.\nBlind backdoors in deep learning models.\narXiv\npreprint arXiv:2005.03823, 2020.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym, 2016.\nBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung\nLee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by\nactivation clustering. arXiv preprint arXiv:1811.03728, 2018.\nHuili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan\ndetection and mitigation framework for deep neural networks. In IJCAI, pp. 4658–4664, 2019.\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep\nlearning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\nMaxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment\nfor openai gym. https://github.com/maximecb/gym-minigrid, 2018.\nJiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classiﬁca-\ntion systems. IEEE Access, 7:138872–138878, 2019.\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the\nmachine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\nXijie Huang, Moustafa Alzantot, and Mani Srivastava.\nNeuroninspect: Detecting backdoors in\nneural networks via output explanations. arXiv preprint arXiv:1911.07399, 2019.\nKiran Karra, Chace Ashcraft, and Neil Fendley. The trojai software framework: An opensource tool\nfor embedding trojans into deep learning models. arXiv preprint arXiv:2003.07233, 2020.\nPanagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: evaluation of backdoor\nattacks on deep reinforcement learning. In 2020 57th ACM/IEEE Design Automation Conference\n(DAC), pp. 1–6. IEEE, 2020.\nKeita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models.\narXiv preprint arXiv:2004.06660, 2020.\nShaofeng Li, Benjamin Zi Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, and Haojin Zhu. Invisible\nbackdoor attacks against deep neural networks. arXiv preprint arXiv:1909.02742, 2019.\nEric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E.\nGonzalez, Michael I. Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement\nlearning. 2018.\nYingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs:\nScanning neural networks for back-doors by artiﬁcial brain stimulation. In Proceedings of the\n2019 ACM SIGSAC Conference on Computer and Communications Security, pp. 1265–1282,\n2019.\nYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reﬂection backdoor: A natural backdoor\nattack on deep neural networks. In European Conference on Computer Vision, pp. 182–199.\nSpringer, 2020.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level\ncontrol through deep reinforcement learning. nature, 518(7540):529–533, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms, 2017.\n5\nPublished at ICLR 2021 Workshop on Security and Safety in Machine Learning Systems\nAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,\nand Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks.\narXiv preprint arXiv:1804.00792, 2018.\nLichao Sun. Natural backdoor attack on text data. arXiv preprint arXiv:2006.16176, 2020.\nJustin K Terry, Benjamin Black, Mario Jayakumar, Ananth Hari, Luis Santos, Clemens Dieffendahl,\nNiall L Williams, Yashas Lokesh, Ryan Sullivan, Caroline Horsch, and Praveen Ravi. Pettingzoo:\nGym for multi-agent reinforcement learning. arXiv preprint arXiv:2009.14471, 2020a.\nJustin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, Benjamin Black, and Dinesh\nManocha. Parameter sharing is surprisingly useful for multi-agent deep reinforcement learning.\narXiv preprint arXiv:2005.13625, 2020b.\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. arXiv\npreprint arXiv:1811.00636, 2018.\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.\nXiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans\nusing meta neural analysis. arXiv preprint arXiv:1910.03137, 2019.\nYuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural\nnetworks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communica-\ntions Security, pp. 2041–2055, 2019.\n6\n",
  "categories": [
    "cs.LG",
    "cs.CR"
  ],
  "published": "2021-06-14",
  "updated": "2021-06-14"
}