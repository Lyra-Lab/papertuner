{
  "id": "http://arxiv.org/abs/2106.07846v2",
  "title": "Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification",
  "authors": [
    "Mingkun Li",
    "Chun-Guang Li",
    "Jun Guo"
  ],
  "abstract": "Unsupervised person re-identification (Re-ID) aims to match pedestrian images\nfrom different camera views in unsupervised setting. Existing methods for\nunsupervised person Re-ID are usually built upon the pseudo labels from\nclustering. However, the quality of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated by the colors in\nimages especially in the unsupervised setting. In this paper, we propose a\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\nperson Re-ID, in which cluster structure is leveraged to guide the feature\nlearning in a properly designed asymmetric contrastive learning framework. To\nbe specific, we propose a novel cluster-level contrastive loss to help the\nsiamese network effectively mine the invariance in feature learning with\nrespect to the cluster structure within and between different data augmentation\nviews, respectively. Extensive experiments conducted on three benchmark\ndatasets demonstrate superior performance of our proposal.",
  "text": "IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n1\nCluster-guided Asymmetric Contrastive Learning\nfor Unsupervised Person Re-Identiï¬cation\nMingkun Li, Chun-Guang Li, Senior Member, IEEE, and Jun Guo\nAbstractâ€”Unsupervised person re-identiï¬cation (Re-ID) aims\nto match pedestrian images from different camera views in an\nunsupervised setting. Existing methods for unsupervised person\nRe-ID are usually built upon the pseudo labels from clustering.\nHowever, the result of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated\nby colors in images. In this paper, we attempt to suppress the\nnegative dominating inï¬‚uence of colors to learn more effective\nfeatures for unsupervised person Re-ID. Speciï¬cally, we propose\na Cluster-guided Asymmetric Contrastive Learning (CACL) ap-\nproach for unsupervised person Re-ID, in which clustering result\nis leveraged to guide the feature learning in a properly designed\nasymmetric contrastive learning framework. In CACL, both\ninstance-level and cluster-level contrastive learning are employed\nto help the siamese network learn discriminant features with\nrespect to the clustering result within and between different\ndata augmentation views, respectively. In addition, we also\npresent a cluster reï¬nement method, and validate that the cluster\nreï¬nement step helps CACL signiï¬cantly. Extensive experiments\nconducted on three benchmark datasets demonstrate the superior\nperformance of our proposal.\nIndex Termsâ€”Unsupervised Person Re-Identiï¬cation, Asym-\nmetric Contrastive Learning, Cluster Reï¬nement.\nI. INTRODUCTION\nU\nNSUPERVISED person Re-identiï¬cation (Re-ID) aims\nto match pedestrian images from different camera views\nin unsupervised setting without demanding massive labelled\ndata, and has attracted increasing attention in computer vision\nand pattern recognition community in recent years [1]. The\ngreat challenge we face in unsupervised person Re-ID is to\ntackle heavy variations from different viewpoints, varying illu-\nminations, changing weather conditions, cluttered background\nand etc., without supervision labels.\nRecently, existing methods for unsupervised person Re-ID\nare usually built on exploiting weak supervision information\n(e.g., pseudo labels) from clustering. For example, MMT [2]\nuses DBSCAN [3] algorithm to generate pseudo labels and\nexploit the pseudo labels to train two networks. HCT [4]\nuses a hierarchical clustering algorithm to gradually assign\npseudo labels to the training samples during the training stage.\nSSG [5] uses k-means on training samples with multi-views.\nHowever, the performance of these methods heavily relies on\nthe quality of the pseudo labels, which directly depends on\nthe feature representation of the input images.\nM. Li, C.-G. Li and J. Guo are with the School of Artiï¬cial Intelligence,\nBeijing University of Posts and Telecommunications, Beijing, 100876 P.R.\nChina e-mail: {mingkun.li, lichunguang, guojun}@bupt.edu.cn.\nChun-Guang Li is the corresponding author.\nManuscript received xx, 2021; revised xx, xxxx.\nGray-Scale Images\nColor Images\nColor Images\nCluster\nContrastive\nLearning\nFig. 1. Illustration for basic idea of our proposal. We attempt to leverage the\nclustering information into contrastive learning to ï¬nd more effective features\nby exploring the invariance between color images and gray-scale images.\nMore recently, contrastive learning is applied to perform\nfeature learning in unsupervised setting, e.g., [6], [7], [8],\n[9], [10]. The primary idea in these methods is to learn\nsome invariance in feature representation with self-supervised\nmechanism based on data augmentation. In SimCLR [8], each\nsample and its multiple augmentations are treated as positive\npairs, and the rest of the samples in the same batch are treated\nas negative pairs and, a contrastive loss is used to distinguish\nthe positive and negative samples to prevent the model from\nfalling into a trivial solution. We note that SimCLR requires\nto use a large batch size, e.g., 256 âˆ¼4096, to contain enough\nnegative samples for effectively training the networks. In\nBYOL [10] and SimSiam [9], a predictor layer is used to\nprevent feature collapse without using negative samples. In\nInterCLR [6] and SwAV [7], clustering is used to prevent\nthe feature collapse. In particular, in SwAV [7], a scalable\nonline clustering loss is proposed to train the siamese network\nwith multi-crop data augmentation; whereas in InterCLR [6],\na MarginNCE loss is proposed to enhance the discriminant\npower. While promising performance has been reported on\nImageNet [11], however, these contrastive learning methods\nare not suitable for unsupervised person Re-ID due to serious\nfeature collapse.\nIn this paper, we attempt to leverage cluster information\ninto contrastive learning to develop an effective framework for\nunsupervised person Re-ID. We notice that the performance\nof person Re-ID depends heavily on the effectiveness of the\nlearned features. However, the learned features are overwhelm-\narXiv:2106.07846v2  [cs.CV]  9 May 2022\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n2\ningly dominated by the colors in pedestrian images (such as\nthe clothing color and background color), especially in the\nunsupervised setting. For example, the pedestrian images with\nsimilar color clothes often have smaller distances in feature\nspace, which may result in mistakes in clustering, and the\nmistakes in clustering may further bring wrong guidance to\nthe pseudo labels for training the network. Although colors\nare important feature to match pedestrian images for person\nRe-ID, it may also become an obstacle to learn more subtle\nand effective texture features that are important ï¬ne-level cues\nfor person Re-ID. Thus it is desirable to learn more robust\nand discriminating features that can resist dominant colors for\nperson Re-ID task.\nUnfortunately, it is quite challenging to properly suppress\nthe negative impact of colors for learning more effective ï¬ne-\ngrain level features without loss of discriminant information.\nFor example, directly using random color changing (i.e., color-\njitter [12]) for data augmentation in contrastive training may\ndamage the consistency in color distribution, not that helpful to\ngain generalization ability on unseen samples. To this end, in\nthis paper, we propose a novel and effective framework for un-\nsupervised person Re-ID, termed Cluster-guided Asymmetric\nContrastive Learning (CACL), in which clustering information\nis properly incorporated into contrastive learning to learn\nrobust and discriminant features while suppressing dominant\ncolors, as illustrated in Fig. 1. To be speciï¬c, we explore\nsupervision information from the perspective of suppressing\ncolors in the framework of cluster-guided contrastive learning,\nin which the samples in asymmetric views of speciï¬cally\ndesigned data augmentations (e.g., color images vs. gray-\nscale images) as shown in Fig. 2â€”are exploited to provide\nstrong supervision to impose invariance in feature learning.\nBy integrating the clustering results into contrastive learning,\nthe proposed framework is able to avoid feature collapse.\nBy suppressing dominant colors, the proposed framework is\nable to effectively learn robust and discriminating features\nother than colors. In addition, we also present a simple but\neffective cluster reï¬nement method to improve the clustering\nresult and thus further enhancing the contrastive learning. We\nconduct extensive experiments on three benchmark datasets,\nand experimental results validate the effectiveness of our\nproposal.\nPaper Contributions. The contributions of the paper are\nhighlighted as follows.\n1) We propose an effective unsupervised framework that\nleverages clustering information into contrastive learning\nwhile suppressing the dominant colors in images to learn\nï¬ne-grained features.\n2) We propose a novel cluster-level loss function to perform\ninter-views and intra-view contrastive learning that can\neffectively exploit the cluster-level hidden information\nfrom different data augmentation views.\n3) We also present a cluster reï¬nement method and verify\nthat the reï¬ned clustering information helps the con-\ntrastive learning framework signiï¬cantly.\nThe remainder of this paper is organized as follows. Sec-\ntion II describes the relevant work. Section III presents our\n(a) Raw\n(b) T\n(c) T â€²\n(d)G â—¦T â€²\nFig. 2.\nIllustration for the raw images and the augmented images. The\nï¬rst column shows the raw images. The middle two columns show the\nimages generated with transforms T (Â·) and T â€²(Â·). The last column shows\nthe corresponding gray-scale images which are generated with both transform\nT â€²(Â·) and color-to-grayscale transform G(Â·), i.e., G â—¦T â€²(Â·).\nproposal. Section IV shows experiments and Section V gives\nthe conclusions.\nII. RELATED WORK\nA. Unsupervised Person Re-identiï¬cation\nPerson Re-ID aims to ï¬nd speciï¬c pedestrians from videos\nor images according to targets. For the increasing demand\nin real life and avoiding the high consumption of labeling\ndatasets, unsupervised person Re-ID has become popular in\nrecent years [1]. The existing unsupervised person Re-ID\nmethods can be divided into two categories: a) unsupervised\ndomain adaptation methods, that need labeled source dataset\nand unlabeled target dataset [13], [14], [15], [16]; and b)\npure unsupervised methods, that need with only unlabeled\ndataset [17], [18], [19], [20].\nThe unsupervised domain adaptation methods train the\nnetwork with the help of labeled datasets, and transfer the\nnetwork to unlabeled datasets by reducing the gap between two\ndatasets. For example, [21] proposed to align the second-order\nstatistics of the distributions in the two domains through linear\ntransformations to reduce the domain shift; [17] proposed a\ncombined loss function to co-train with samples from the\nsource and target domains and the merging memory bank; [22]\nproposed to maximize the inter-domain classiï¬cation loss and\nminimize the intra-domain classiï¬cation loss to learn domain\nrobust features. However, unsupervised domain adaptation\nmethods are limited by the requirement of the target dataset\nhaving close distribution to the source dataset.\nMost purely unsupervised person Re-ID methods rely on the\npseudo labels to train the network. For example, HCT [4] uses\nhierarchical clustering to generate pseudo labels and train the\nconvolution neural network for feature learning; [23] assigns\nmultiple labels to samples and proposes a new loss function\nfor multi-label training. Note that the quality of the pseudo\nlabels relies on the feature representation of the input images.\nHowever, in the early stage, the feature representation is not\ngood enough to generate high-quality pseudo labels, and thus\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n3\nRaw\nImage\nCluster-level\nContrastive\nLoss â„’!\nInstanceâˆ’level \nContrastive \nLoss â„’\"\nFlow\nLoss\nBack Propagate\nPredictor G( â‹…|Î¨)\nResNet\nğ¹(â‹…|Î˜)\nFeature embedding\nUpdate\nImage\nğ¼'%\nImage\nğ¼(%\nğ’¯(â‹…)\nClustering & Cluster Refinement\nCluster-level\nContrastive\nLoss â„’!\nğ“%\nğ¼%\nğ‘§%\nğ“-%\nğ“%\nğ“-%\nPseudo Labels\nğ’¢âˆ˜ğ’¯&(â‹…)\nResNet\nğ¹&(â‹…|Î˜â€²)\nInstance-Level Contrastive Learning\nCluster-Level Contrastive Learning\nInstance Memory Bank\nâ„³&\nInstance Memory Bank\nâ„³\nUpdate\nFig. 3. Illustration for our proposed Cluster-guided Asymmetric Contrastive Learning (CACL) framework. After training, we keep only the ResNet F(Â·|Î˜)\nin the ï¬rst branch for inference and use the feature xi for testing.\nthe low-quality pseudo labels will contaminate the network\ntraining. Therefore, it is needed to design a cluster reï¬nement\nmethod to improve the clustering quality before feeding the\npseudo labels to train the network.\nB. Contrastive Learning\nIn recent years, with the development and application of\nthe siamese network, contrastive learning began to emerge in\nthe ï¬eld of unsupervised learning. Contrastive learning aims\nat learning good image representation. It learns invariance in\nfeatures by manipulating a set of positive samples and negative\nsamples with data augmentation.\nThe existing methods for contrastive learning can be further\ncategorized to: a) instance-level methods [8], [9], [24], [25],\n[10] and b) cluster-level methods [7], [6], [26]. Instance-level\nmethods regard each image as an individual class and consider\ntwo augmented views of the same image as positive pairs and\ntreat others in the same batch (or memory bank) as negative\npairs. For example, SimCLR [8] regards samples in the current\nbatch as the negative samples; MoCo [27] uses a dictio-\nnary to implement contrastive learning, which converts one\nbranch of the contrastive learning into a momentum encoder;\nSimSiam [9] proposed a stop-gradient method that can train\nthe siamese network without negative samples. Cluster-level\nmethods regard the samples in the same clusters as positive\nsamples and other samples as negative samples. For example,\nin [6] InfoNCE loss is combined with MarginNCE loss to\nattract positive samples and repelled negative samples; in [7]\nmulti-crop data augmentation is used to enhance the robustness\nof the network and a scalable online clustering method is\nproposed to explore the inter-invariance of clusters; in [26]\nweights-sharing deep neural networks are used to extract\nfeatures from sample pairs with different data augmentations,\nand contrastive clustering is performed with respect to both\nthe features in the row and column spaces.\nHowever, in the unsupervised setting, the instance-level\ncontrastive learning methods simply make each sample in-\ndependently repel each other, which will undoubtedly ignore\nthe cluster information. In contrast, cluster-level contrastive\nlearning can effectively mine cluster information, but it relies\nheavily on the clustering result. Unfortunately, in the early\ntraining stage, the features are not good enough to yield good\nclustering result. Thus, an effective way to train the network by\ncombining both the two lines of contrastive learning methods\nis needed.\nIn this paper, we attempt to bridge the two lines of con-\ntrastive learning methods into a uniï¬ed framework to form\neffective mutual learning and joint training: a) the instance-\nlevel contrastive learning helps training the network to perform\nfeature learningâ€”especially in the early training stage; mean-\nwhile b) the cluster-level contrastive learning helps training\nthe networkâ€”especially when the quality of the clustering has\nbeen improved. In this way, the self-supervision information\nimposed by data augmentation and the weak supervision\ninformation obtained from clustering can be fully exploited\nwithout the need to use negative samples pairs.\nIII. OUR PROPOSAL: CLUSTER-GUIDED ASYMMETRIC\nCONTRASTIVE LEARNING (CACL)\nThis section presents our proposalâ€”Cluster-guided Asym-\nmetric Contrastive Learning (CACL) approach for unsuper-\nvised person Re-ID.\nFor clarity, we show the architecture of our proposed CACL\nin Fig. 3. Overall, our CACL is a siamese network, which\nconsists of two branches of backbone networks F(Â·|Î˜) and\nF â€²(Â·|Î˜â€²) without sharing parameters, where Î˜ and Î˜â€² are the\nparameters in the two networks, respectively, and a predictor\nlayer G(Â·|Î¨) is added after the ï¬rst branch, where Î¨ denotes\nthe parameters in the predictor layer. The backbone networks\nF(Â·|Î˜) and F â€²(Â·|Î˜â€²) are implemented1 via ResNet-50 [28] for\nfeature learning.\n1It also works if the backbone networks other than ResNet-50 are used.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n4\nGiven an unlabeled image dataset I = {Ii}N\ni=1 consisting\nof N samples. For an input image Ii âˆˆI, we generate two\nsamples Ë†Ii and ËœIi via different data augmentation strategies\nas the inputs of the two branches, respectively, in which\nË†Ii = T (Ii) and ËœIi = G(T â€²(Ii)), where T (Â·) and T â€²(Â·) denote\ntwo different transforms and G(Â·) denotes the operation to\ntransform color image into gray-scale image. For simplicity,\nwe denote the output features of the ï¬rst network branch and\nthe second network branch as xi and Ëœxi, and denote the output\nof the predictor layer in the ï¬rst branch as zi, respectively,\nwhere xi, Ëœxi, zi âˆˆRD.\nThe\nclustering\nresult\nof\nthe\noutput\nfeatures\nX\n:=\n{x1, Â· Â· Â· , xN} from the ï¬rst network branch is used to gener-\nate the pseudo labels Y := {y1, Â· Â· Â· , yN}. We exploit the\npseudo labels to leverage the cluster information into the\ncontrastive learning. Speciï¬cally, in the training stage, the\ntwo network branches F(Â·|Î˜) and F â€²(Â·|Î˜â€²) are trained with\nthe augmented samples without sharing parameters, and the\npseudo labels Y are used to guide the training of both network\nbranches.\nIn CACL, we use instance memory banks M = {vi}N\ni=1\nand\nËœ\nM = {Ëœvi}N\ni=1 where vi, Ëœvi âˆˆRD to store the outputs of\ntwo branches, respectively. Both instance memory banks M\nand\nËœ\nM are initialized with X := {x1, Â· Â· Â· , xN} and Ëœ\nX :=\n{Ëœx1, Â· Â· Â· , ËœxN}, which are the outputs of the network branches\nF(Â·|Î˜) and F â€²(Â·|Î˜â€²) pre-trained on ImageNet, respectively.\nA. Cluster-guided Contrastive Learning\nAt beginning, we pre-train the two network branches F(Â·|Î˜)\nand F â€²(Â·|Î˜â€²) on ImageNet [11], and use the features from\nthe ï¬rst network branch F(Â·|Î˜) to yield m clusters, which\nare denoted as C := {C(1), C(2), Â· Â· Â· , C(m)}. The clustering\nresult is used to form pseudo labels to train the cluster-guided\ncontrastive learning module.\nTo exploit the label invariance between the two augmented\nviews and leverage the cluster structure, we employ two types\nof contrastive losses: a) instance-level contrastive loss, denoted\nas LI, and b) cluster-level contrastive loss, denoted as LC.\nInstance-Level Contrastive Loss. To match the feature out-\nputs zi and Ëœxi of the two network branches at instance-level,\nsimilar to [8], [10], we introduce the negative cosine similarity\nof the prediction outputs zi in the ï¬rst branch and the feature\noutput of the second branch Ëœxi to deï¬ne an instance-level\ncontrastive loss LI as follows:\nLI := âˆ’zâŠ¤\ni\nâˆ¥ziâˆ¥2\nËœxi\nâˆ¥Ëœxiâˆ¥2\n,\n(1)\nwhere âˆ¥Â· âˆ¥2 is the â„“2-norm.\nCluster-Level Contrastive Loss. To leverage the cluster struc-\nture to further explore the hidden information from different\nviews, we propose a cluster-level contrastive loss LC, which\nis further divided into inter-views cluster-level contrastive loss\nand intra-views cluster-level contrastive loss.\nâ€¢ Inter-views Cluster-level contrastive loss, denoted as\nL(inter)\nC\n, which is deï¬ned as:\nL(inter)\nC\n:= âˆ’zâŠ¤\ni\nâˆ¥ziâˆ¥2\nËœuÏ‰(Ii)\nâˆ¥ËœuÏ‰(Ii)âˆ¥2\n,\n(2)\nwhere Ï‰(Ii) is to ï¬nd the cluster index â„“for zi, and Ëœuâ„“\nis the center vector of the â„“-th cluster in which ËœU :=\n{Ëœu1, Â· Â· Â· , Ëœumâ€²} and the cluster center Ëœuâ„“is deï¬ned as\nËœuâ„“=\n1\n|C(â„“)|\nX\nIiâˆˆC(â„“)\nËœvi,\n(3)\nwhere Ëœvi is the instance feature of image ËœIi in the instance\nmemory bank Ëœ\nM, C(â„“) is the â„“-th cluster. The inter-views\ncluster-level contrastive loss L(inter)\nC\ndeï¬ned in Eq. (2)\nis used to reduce the discrepancy between the projection\noutput zi of the ï¬rst network branch and the cluster center\nËœuâ„“of the feature output of the second branch with the\ngray-scale view.\nâ€¢ Intra-views Cluster-level contrastive loss, denoted as\nL(intra)\nC\n, which is deï¬ned as:\nL(intra)\nC\n= âˆ’(1 âˆ’qi)2 ln(qi)\nâˆ’(1 âˆ’Ëœqi)2 ln(Ëœqi),\n(4)\nwhere qi and Ëœqi are the softmax of the inner product of the\nnetwork outputs and the corresponding instance memory\nbank, which are deï¬ned as\nqi =\nexp(uâŠ¤\nÏ‰(Ii)xi/Ï„)\nPmâ€²\nâ„“=1 exp(uâŠ¤\nâ„“xi/Ï„)\n,\n(5)\nËœqi =\nexp(ËœuâŠ¤\nÏ‰(Ii)Ëœxi/Ï„)\nPmâ€²\nâ„“=1 exp(ËœuâŠ¤\nâ„“Ëœxi/Ï„)\n,\n(6)\nwhere uâ„“and Ëœuâ„“are the center vectors of the â„“-th cluster\nfor the ï¬rst branch and the second branch, respectively,\nin which Ëœuâ„“is deï¬ned in Eq. (3) and uâ„“is deï¬ned as\nuâ„“=\n1\n|C(â„“)|\nX\nIiâˆˆC(â„“)\nvi,\n(7)\nwhere vi is the instance feature of image Ë†Ii in the instance\nmemory bank M. Note that both xi and Ëœxi share the\nsame pseudo labels Ï‰(Ii) from clustering. The intra-\nviews cluster-level contrastive loss L(intra)\nC\nin Eq. (4) is\nused to encourage the siamese network to learn features\nwith respect to the corresponding cluster center for the\ntwo branches, respectively.\nPutting the loss functions in Eqs. (2) and (4) together, we\nhave the cluster-level contrastive loss LC as follows:\nLC := L(inter)\nC\n+ L(intra)\nC\n.\n(8)\nRemark 1. The cluster-level contrastive loss LC in Eq. (8)\naims to leverage the clustering information to minimize the\ndifference between the samples of the same cluster from\ndifferent augmentation views via L(inter)\nC\n, and within the same\naugmentation view via L(intra)\nC\n. This will help the siamese\nnetwork to mine the hidden information brought by the basic\naugmented view in the ï¬rst branch and the gray-scale aug-\nmented view in the second branch to prevent feature collapse\nto a trivial solution and impose the supervision information to\nlearn features other than colors.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n5\nB. Clustering and Cluster Reï¬nement\nNote that the cluster-level contrast loss is greatly affected by\nthe quality of the clustering result. When the clusters are noisy,\nit will cause negative effects on the training. To improve the\nquality of the clustering result, we propose a cluster reï¬nement\nmethod which removes a proportion of noisy samples in larger\nclusters, helping the model to better learn the information at\nthe cluster level.\nFor a cluster, we want to keep the samples with higher sim-\nilarity and remove the samples with lower similarity. Given a\nset of raw clusters, denoted as {C(1), C(2), Â· Â· Â· , C(m)}, without\nloss of generality, we pick C(i) to perform cluster reï¬nement.\nAt ï¬rst, we obtain an over-segmentation of C(i), i.e., C(i) is\nfurther divided into {C(i)\n1 , C(i)\n2 , Â· Â· Â· , C(i)\nni }. Then we perform\ncluster reï¬nement according to the following criterion:\nif D(C(i)\nj |C(i)) < D(C(i)), then C(i)\nj is kept;\n(9)\notherwise C(i)\nj\nis removed, where D(C(i)\nj |C(i)) is the average\ninter-distance from all samples in the sub-cluster C(i)\nj\nto other\nsamples in cluster C(i), and D(C(i)) is the average intra-\ndistance among samples in cluster C(i).\nAfter such a post-processing step, the clusters of larger\nsize are improved and at meantime, more singletons or tiny\nclusters are also produced. We denote the reï¬ned clusters\nas Câ€² = {C(1), C(2), Â· Â· Â· , C(mâ€²)}, where mâ€² â‰¥m. Compared\nto tiny clusters and singletons, the larger clusters are more\ninformative to provide pseudo supervision information to\nguide the contrastive learning.\nRemark 2. In implementation, we use DBSCAN algorithm [3]\nto generate the raw clusters and to generate the over-\nsegmentation of the clusters. DBSCAN [3] is a density-based\nclustering algorithm. It regards a data point as density reach-\nable if the data point lies within a small distance threshold d to\nother samples, where the parameter d is the distance threshold\nto ï¬nd neighboring point. Speciï¬cally, to generate the raw\nclusters, we employ DBSCAN with a slightly larger distance\nthreshold parameter d (e.g., d = 0.6); whereas to generate the\nover-segmentation, we use a slightly smaller distance threshold\nparameter dâ€², where dâ€² := dâˆ’Î´ (e.g., Î´ = 0.02). We will show\nthe inï¬‚uence of the parameters Î´ and d in experiments.\nC. Training Procedure for Our CACL Approach\nIn CACL, the two branches in the siamese network are\nimplemented with ResNet-50 [28] and they are not sharing\nparameters. We pre-train the two network branches on Ima-\ngeNet at ï¬rst and use the learned features to initialize the two\nmemory banks M and\nËœ\nM, respectively.\nIn training stage, we train both network branches at the\nsame time with the total loss:\nL := LI + LC.\n(10)\nWe update the two instance memory banks M and\nËœ\nM,\nrespectively, as follows:\nv(t)\ni\nâ†Î±v(tâˆ’1)\ni\n+ (1 âˆ’Î±)xi,\n(11)\nËœv(t)\ni\nâ†Î±Ëœv(tâˆ’1)\ni\n+ (1 âˆ’Î±)Ëœxi,\n(12)\nwhere Î± is set as 0.2 by default (and we will discuss the\ninï¬‚uence of Î± in experiments).\nIn order to save the computation cost2, we also use a stop-\ngradient operation as mentioned in SimSiam [9]. Note that we\nadopt the stop-gradient operation [9] to the second network\nbranch F â€²(Â·|Î˜â€²) when using the instance level loss LI in\nEq. (1) to perform back propagation. Thus, the parameters\nÎ˜â€² in the second network branch are updated only with the\nintra-views cluster-level contrastive loss L(intra)\nC\nin Eq. (4).\nRemark 3. For clarity, we summarize the details of the train-\ning procedure in Algorithm 1. We note that the â€œasymmetryâ€ in\nthe proposed framework for cluster-guided contrastive learning\nlies in following three aspects: a) asymmetry in network\nstructure, i.e., a predictor layer is only added after the ï¬rst\nbranch3; and b) asymmetry in data augmentation, i.e., the\naugmented samples provided to the second branch are further\ntransformed into gray-scale; c) asymmetry in pseudo labels\ngeneration, i.e., the output features of the ï¬rst branch are\nused to generate pseudo labels which are shared with the\nsecond branch. Because of the asymmetry in the three aspects\nmentioned above, we term the proposed framework as Cluster-\nguided Asymmetric Contrastive Learning (CACL).\nRemark 4. There have been many unsupervised Re-ID meth-\nods [17], [13], [29], [12] used the contrastive learning to learn\ndiscriminant features. Most of them [13], [29], [12] are Gener-\native Adversarial Networks (GANs)-based methods and need\nadditional supervised information to assist the training. For\nexample, ATNet [13] trains multiple GANs through utilizing\nillumination and camera information, GCL [12] introduces\nthe pose information in training, and AD-cluster [29] uses\ngenerating cross-camera samples to assist the training. Un-\nlike these methods, our proposed CACL uses an asymmetric\nSiamese network to effectively learn ï¬ne-grained features by\nsuppressing color with simple data augmentation operations\nduring the training, rather than using an expensive sample\ngeneration via GANs. Compared to GANs based methods, our\nCACL is simple, efï¬cient and effective.\nD. Inference Procedure for CACL\nAfter training, we keep only the ResNet F(Â·|Î˜) in the ï¬rst\nbranch for inference in testing.\nTo be speciï¬c, in the inference procedure, we use the output\nfeatures X of the ï¬rst branch F(Â·|Î˜) to calculate the similarity\nbetween images. Given the query image dataset Ig = {Ig\ni }N g\ni=1\nand the query image dataset Iq = {Iq\ni }Nq\ni=1, where N g and\nN q are the sizes of the two datasets, respectively. For each\nimage Ig\ni in the query, we compute the distances between the\nquery image and the images in the gallery Iq via the feature\n2Note that it is not necessary to use the stop-gradient operation in our\nCACL because the clustering result provides enough guide information under\nthe asymmetric structure to prevent collapse. Although this is similar to the\nmethod in SimSiam [9], the purpose is different and it is not necessary to use\nin our proposal.\n3It is also feasible to add another predictor layer after the second branch\nto have a symmetric network structure. Nevertheless, our experimental results\nshow that merely marginal performance improvement can be yielded after\nadding an extra predictor layer. Thus, we prefer to use the asymmetric network\narchitecture for the contrastive learning framework.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n6\nAlgorithm 1 Training Procedure for CACL\nInput: Given a dataset I = {Ii}N\ni=1.\nOutput:\n1: Pre-train the two network branches on ImageNet.\n2: Initialize the two instance memory banks M and Ëœ\nM and\nset P = Pbest = 0.\n3: while epoch â‰¤total epoch do\n4:\nGenerate Ë†Ii and ËœIi via data augmentation T (Â·) and\nG(T â€²(Â·));\n5:\nPerform feature extraction to get xi and Ëœxi;\n6:\nPerform clustering and clustering reï¬nement via Eq. (9)\nto yield pseudo label Y = {y1, Â· Â· Â· , yN};\n7:\nUpdate the two cluster centers U and ËœU via Eq. (7);\n8:\nTrain siamese network, i.e., updatomg Î˜, Î¨ and Î˜â€² via\nthe total loss in Eq. (10);\n9:\nUpdate instance memory bank M and\nËœ\nM via Eq. (11)\nand Eq. (12);\n10:\nEvaluate the model performance P with F(Â·|Î˜);\n11:\nif P > Pbest then\n12:\nOutput the best model F(Â·|Î˜) and set Pbest â†P;\n13:\nend if\n14: end while\nobtained from the output of the ï¬rst branch. And then, we sort\nthe distance in ascending order to ï¬nd the matched images.\nIV. EXPERIMENTS\nIn this section, we describe the used benchmark datasets\nand the detailed parameter settings in experiments at ï¬rst,\nand then provide extensive experiments on these datasets,\nincluding a set of detailed ablation study and a set of evaluation\nexperiments to show the effect of each component. Finally, we\ngive a set of data visualization experiments. 4\nA. Dataset Description\nTo evaluate the effectiveness of our proposal, we use\nthe following three benchmark datasets: Market-1501 [41],\nDukeMTMC-ReID [45] and MSMT17 [46].\nMarket-1501 has 32,668 photos of 1501 people from six\ndifferent camera views. The training set contains 12,936 of\n751 identities. The testing set contains 19,732 images of 750\nidentities.\nDukeMTMC-ReID consists of images sampling from\nDukeMTMC-ReID video dataset, 120 frames per video, with\na total of 36,411 images of people of 1404 identities. The\ntraining set contains 16,522 images of 702 identities and the\ntesting set contains 2228 query images of 702 identities and\n17,661 gallery images. These images are taken from eight\ncameras.\nMSMT17 has a total of 126,441 images under 15 camera\nviews. The training set contains 32,621 images of 1041 identi-\nties. The testing set contains 93,820 images of 3060 identities\nare used for testing. MSMT17 is larger than Market-1501 and\nDukeMTMC-ReID.\n4The code can be downloaded from https://github.com/MingkunLishigure/\nCACL.\nB. Implementation Details\nSettings for Training. In our CACL approach, we use\nResNet-50 [28] pre-trained on ImageNet [11] for both network\nbranches.5 The feature outputs xi âˆˆRD and Ëœxi âˆˆRD of the\ntwo networks F(Â·|Î˜) and F(Â·|Î˜â€²) are D-dimensional vectors\nwhere D = 2048. We use the features output xi of the ï¬rst\nbranch F(Â·|Î˜) to perform clustering, where xi = F(Ë†Ii|Î˜) âˆˆ\nRD.\nThe prediction layer G(Â·) is a D Ã— D full connection layer.\nWe initialize the two memory banks with the outputs of the\nfeature from the corresponding network branches F(Â·|Î˜) and\nF â€²(Â·|Î˜â€²), respectively. We optimize the network through Adam\noptimizer [47] with a weight decay of 0.0005 and train the\nnetwork with 80 epochs in total. The learning rate is initially\nset as 0.00035 and decreased to one-tenth per 20 epochs. The\nbatch size is set to 64. The temperature coefï¬cient Ï„ in Eq. (6)\nis set to 0.05 and the update factor Î± in Eqs. (11) and (12) is\nset to 0.2.\nSettings for Data Augmentation. In our experiments, we use\nthe same data augmentation operations as other methods [17],\n[2], including random horizontal ï¬‚ip, random erasing and\nrandom crop, to deï¬ne data augmentation T (Â·) and T â€²(Â·).\nBesides, we add a gray-scale transform to the input of the\nsecond branch.\nMetrics for Performance Evaluation. In evaluation, we use\nthe mean average precision (mAP) and cumulative matching\ncharacteristic (CMC) at Rank-1, 5, 10 to evaluate the perfor-\nmance.\nC. Comparison to the State-of-the-art Methods\nWe compare our proposed CACL to the state-of-the-art\nunsupervised domain adaptation methods and purely unsu-\npervised methods for person Re-ID. The purely unsuper-\nvised methods for person Re-ID include: CAMEL [40],\nPUL [19], SSL [20], LOMO [42], BOW [41], BUC [18],\nHCT [4], SpCL [17], and CAP [43]. The unsupervised\ndomain adaptation methods for person Re-ID include: PT-\nGAN [30], ADTC [36], HHL [35], SSG [5], MMCL [23],\nAD-Cluster [29], MEB [38], NRMT [39], SPGAN [32], TJ-\nAIDL [16], JVTC [37], PGPPM [34], and MMT [2].\nThe comparison results of the state-of-the-art unsupervised\ndomain adaptation methods and purely unsupervised methods\nare shown in Table I. We can ï¬nd that our proposed CACL\nachieves 80.9/92.7% at mAP/Rank-1 on Market-1501 and\n69.6/82.6% at mAP/Rank-1 on DukeMTMC-ReID, respec-\ntively. It can be found that CACL not only performs better\nthan all pure unsupervised methods but also achieves the best\nperformance than unsupervised domain adaptation methods.\nMoreover, we also conduct experiments on a much larger\ndataset MSMT17 and report the experimental results in Table\nII. Again, we can observe that our proposed CACL achieves\na leading performance, i.e., 23.0/48.4% at mAP/Rank-1. It is\nworth to note that our CACL yields superior performance than\nsome UDA methods on this challenging dataset. These results\nconï¬rm the effectiveness of our proposal.\n5In Section IV-C, we also provide the performance evaluation with other\nbackbone networks for the two branches.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n7\nTABLE I\nCOMPARISON TO OTHER STATE-OF-THE-ART METHODS. â€™UDAâ€™ IS TO REFER THE UNSUPERVISED DOMAIN ADAPTATION METHODS AND â€™USâ€™ IS TO\nREFER THE PURELY UNSUPERVISED LEARNING METHODS. â€™*â€™ MEANS THAT THE USED BACKBONE IS PRE-TRAINED ON IMAGENET.\nMethod\nType\nReference\nBakcbone\nMarket-1501\nDukeMTMC-ReID\nmAP\nRank-1\nRank-5\nRank-10\nmAP\nRank-1\nRank-5\nRank-10\nPTGAN [30]\nUDA\nCVPRâ€™18\nGoogleNet [31]\n15.7\n38.6\n57.3\n-\n13.5\n27.4\n43.6\n-\nSPGAN [32]\nUDA\nCVPRâ€™18\nResNet50* [28]\n26.7\n58.1\n76.0\n82.7\n26.4\n46.9\n62.6\n68.5\nTJ-AIDL [16]\nUDA\nCVPRâ€™18\nMobileNet* [33]\n26.5\n58.2\n74.8\n-\n23.0\n44.3\n59.6\n-\nPGPPM [34]\nUDA\nCVPRâ€™18\nResNet50* [28]\n33.9\n63.9\n81.1\n86.4\n17.9\n36.3\n54.0\n61.6\nHHL [35]\nUDA\nECCVâ€™18\nResNet50* [28]\n31.4\n62.2\n78.0\n84.0\n27.2\n46.9\n61.0\n66.7\nSSG [5]\nUDA\nECCVâ€™19\nResNet50* [28]\n58.3\n80.0\n90.0\n92.4\n53.4\n73.0\n80.6\n83.2\nAD-cluster [29]\nUDA\nCVPRâ€™20\nResNet50* [28]\n68.3\n86.7\n94.4\n96.5\n54.1\n72.6\n82.5\n85.5\nADTC [36]\nUDA\nECCVâ€™20\nResNet50* [28]\n59.7\n79.3\n90.8\n94.1\n52.5\n71.9\n84.1\n87.5\nMMCL [23]\nUDA\nCVPRâ€™20\nResNet50* [28]\n60.4\n84.4\n92.8\n95.0\n51.4\n72.4\n82.9\n85.0\nMMT [2]\nUDA\nICLRâ€™20\nResNet50* [28]\n73.8\n89.5\n96.0\n97.6\n62.3\n76.3\n87.7\n91.2\nJVTC [37]\nUDA\nECCVâ€™20\nResNet50* [28]\n67.2\n86.8\n95.2\n97.1\n66.5\n80.4\n89.9\n93.7\nMEB [38]\nUDA\nECCVâ€™20\nResNet50* [28]\n76.0\n89.9\n95.2\n96.9\n65.3\n81.2\n90.9\n92.2\nNRMT [39]\nUDA\nECCVâ€™20\nResNet50* [28]\n71.7\n87.8\n94.6\n96.5\n62.2\n77.8\n86.9\n89.5\nSpCL [17]\nUDA\nNIPSâ€™20\nResNet50* [28]\n76.7\n90.3\n96.2\n97.7\n68.8\n82.9\n90.1\n92.5\nCAMEL [40]\nUS\nICCVâ€™17\nResNet50* [28]\n26.3\n54.4\n73.1\n79.6\n19.8\n40.2\n57.5\n64.9\nBow [41]\nUS\nICCVâ€™15\n-\n14.8\n35.8\n52.4\n60.3\n8.5\n17.1\n28.8\n34.9\nPUL [19]\nUS\nTOMMâ€™18\nResNet50* [28]\n22.8\n51.5\n70.1\n76.8\n22.3\n41.1\n46.6\n63.0\nLOMO [42]\nUS\nCVPRâ€™15\n-\n8.0\n27.2\n41.6\n49.1\n4.8\n12.3\n21.3\n26.6\nBUC [18]\nUS\nAAAIâ€™19\nResNet50* [28]\n30.6\n61.0\n71.6\n76.4\n21.9\n40.2\n52.7\n57.4\nHCT [4]\nUS\nCVPRâ€™20\nResNet50* [28]\n56.4\n80.0\n91.6\n95.2\n50.1\n69.6\n83.4\n87.4\nSSL [20]\nUS\nCVPRâ€™20\nResNet50* [28]\n37.8\n71.7\n83.8\n87.4\n28.6\n52.5\n63.5\n68.9\nSpCL [17]\nUS\nNIPSâ€™20\nResNet50* [28]\n73.1\n88.1\n96.3\n97.7\n65.3\n81.2\n90.3\n92.2\nCAP [43]\nUS\nAAAIâ€™20\nResNet50* [28]\n79.2\n91.4\n96.3\n97.7\n67.3\n81.1\n89.3\n91.8\nCACL\nUS\nThis paper\nResNet50* [28]\n80.9\n92.7\n97.4\n98.5\n69.6\n82.6\n91.2\n93.8\nCACL\nUS\nThis paper\nIBN-ResNet* [44]\n83.6\n93.3\n97.7\n98.3\n72.5\n85.5\n92.9\n94.9\nTABLE II\nEXPERIMENTAL RESULTS ON MSMT17.\nMethod\nType\nReference\nMSMT17\nmAP Rank-1 Rank-5 Rank-10\nPTGAN [30]\nUDA\nCVPRâ€™18\n3.3\n11.8\n-\n27.4\nECN [48]\nUDA\nCVPRâ€™19\n10.2\n30.2\n41.5\n46.8\nSSG [5]\nUDA\nICCVâ€™19\n13.3\n32.2\n-\n51.2\nMMCL [23]\nUDA\nCVPRâ€™20\n16.2\n43.6\n54.3\n58.9\nJVTC+ [37]\nUS\nECCVâ€™20\n17.3\n43.1\n53.8\n59.4\nSpCL [17]\nUS\nNIPSâ€™20\n19.1\n42.3\n55.6\n61.2\nMMT [2]\nUDA\nICLRâ€™20\n24.0\n50.1\n63.5\n69.3\nSpCL [17]\nUDA\nNIPSâ€™20\n26.8\n53.7\n79.3\n83.1\nCACL\nUS\nThis paper 23.0\n48.9\n61.2\n66.4\nCACL w/ IBN-ResNet\nUS\nThis paper 29.9\n57.1\n68.4\n73.1\nNote that Instance-Batch Normalization (IBN) [44] has been\nused in object recognition and has been proved very effective.\nHere, we evaluate our CACL, in which the backbone is im-\nplemented with Instance-Batch Normalization ResNet (IBN-\nResNet). Similar to CACL with ResNet [28], we introduce\nan Instance-Batch Normalization (IBN) layer to replace the\nBN layer and call it an IBN-ResNet. As shown in Table I,\nthe performance of our CACL can be further improved when\ncombining with IBN-ResNet.\nD. Ablation Study\nTo evaluate the effectiveness of each component: LI,\nL(inter)\nC\n, L(intra)\nC\nand clustering with reï¬nement in our CACL\napproach, we conduct a set of ablation experiments on Market-\n1501 and DukeMTMC-ReID.\nIn the baseline method, we train both branches with data\naugmentation T â€²(Â·) and T â€²(Â·) by using the Non-Parametric\nSoftmax loss [49], which is deï¬ned as\nL(xi) = âˆ’ln(\nexp(uâŠ¤\nÏ‰(Ii)xi/Ï„)\nPmâ€²\nâ„“=1 exp(uâŠ¤\nâ„“xi/Ï„)\n),\n(13)\nand both the training process and the memory updating strat-\negy in the baseline method are kept the same as our CACL\nmethod.\nTo comprehensively evaluate the contribution of each com-\nponent, we conduct a set of ablation experiments by test-\ning each component in our CACL framework individually,\ni.e., cluster reï¬nement, instance-level contrastive loss LI and\ncluster-level contrastive loss LC. To further evaluate the sub-\npart of the cluster-level contrastive loss, we also conduct\nexperiments to evaluate the inï¬‚uence of using L(inter)\nC\nor\nL(intra)\nC\n, separately.\nIn the ablation experiments, to test the model with con-\ntrastive loss LC or LI, we train both branches with data aug-\nmentation T â€²(Â·) and G(T â€²(Â·)), respectively. To test the model\nperformance with the cluster-level contrastive loss LC and the\nsub-part of L(intra)\nC\n, compared to the baseline method, we\nneed to replace the Non-Parametric Softmax loss in Eq. (13)\nby the loss in Eq. (4) for both branches. The results of the\nablation study are reported in Table III.\nAs can be read in Table III, the performance improves when\neach component is used individually. This validates that each\ncomponent contributes to the performance improvements. For\nthe experiments of using both LC and LI, it does not signif-\nicantly better than just using LI, and in the experiments of\nusing LC we observe a slight improvement than the baseline.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n8\nTABLE III\nABLATION STUDY ON MARKET-1501 AND DUKEMTMC-REID.\nComponents\nCluster Reï¬ne\nLI\nLintra\nC\nLinter\nC\nMarket-1501\nDukeMTMC-ReID\nmAP Rank-1 Rank-5 Rank-10 mAP Rank-1 Rank-5 Rank-10\nBaseline\n68.1\n85.2\n94.0\n96.0\n62.5\n78.5\n88.5\n90.3\n+ LC\nâœ“\nâœ“\n70.8\n87.5\n94.4\n96\n62.5\n79.5\n88.4\n90.8\n+ LI\nâœ“\n74.7\n88.7\n95\n96.6\n64.2\n80.7\n89\n91.6\n+ LI + LC\nâœ“\nâœ“\nâœ“\n74.4\n89.3\n95.9\n96.7\n63.8\n79.2\n89.2\n91.7\n+ Cluster Reï¬ne\nâœ“\n73\n87.8\n95.7\n97.2\n65.7\n81.1\n90.6\n93.2\n+ Cluster Reï¬ne + LI\nâœ“\nâœ“\n78.2\n91.2\n97\n98.1\n67.6\n81.8\n90.2\n93\n+ Cluster Reï¬ne + LI +Linter\nC\nâœ“\nâœ“\nâœ“\n78.7\n91.2\n97\n97.9\n68.5\n81.9\n91.2\n93.8\n+ Cluster Reï¬ne + LI +Lintra\nC\nâœ“\nâœ“\nâœ“\n79.2\n91.9\n96.7\n98\n68.3\n82.1\n90.3\n93.2\n+ Cluster Reï¬ne + LC\nâœ“\nâœ“\nâœ“\n80.4\n92.2\n97.1\n98.2\n68.8\n82.2\n91.3\n93.8\nOur CACL\nâœ“\nâœ“\nâœ“\nâœ“\n80.9\n92.7\n97.4\n98.5\n69.6\n82.6\n91.2\n93.8\nThis is because the clustering result is not high quality and\nusing LC will make the training pay more attention to the\nnoisy cluster information. Therefore, it might bring misleading\ninformation to the network training. In the experiments of\nusing both LC and cluster reï¬nement, we observe signiï¬cant\nperformance improvement than using the cluster reï¬nement\nalone. This also validates that the cluster reï¬nement improves\nthe clustering result and the reï¬ned clustering information can\nfurther enhance the effectiveness of using LC to train the\nnetwork.\nE. More Evaluation and Analysis\nEvaluation on Importance of Cluster-Guided. We use an\ninstance-level contrastive loss in our method to mine the invari-\nance between different augment views based on SimSiam [9].\nTo verify whether the clustering guidance is vital in the\ncontrast learning framework, we train our CACL framework\nbut just using the instance-level contrastive loss in Eq. (1)\nwithout the clustering guidance. The experimental results are\nshown in Table IV. As can be read from Table IV, surprisingly,\nthe contrastive learning framework without clustering guidance\ndid not work at all.\nTABLE IV\nABLATION STUDY ON MARKET-1501.\nComponents\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\nCACL w/o clustering\n0.3\n0.5\n1.2\n2.3\nCACL w/o stopGrad\n80.2\n92.0\n97.0\n97.6\nCACL\n80.9\n92.7\n97.4\n98.5\nImprovements Brought by Suppressing Colors. To suppress\ncolors inï¬‚uence, CACL uses a gray-scale process G(Â·) over\nthe data augmentation T â€²(Â·) for the second network branch.\nTo validate the effectiveness of suppressing colors, we conduct\na set of experiments under different settings: a) simply using\ndata augmentation T â€²(Â·) with raw color; b) using another data\naugmentation approach, named â€œcolor-jitterâ€, which denoted\nas J (Â·) to replace G(Â·), which output is still a color image; c)\nwith gray-scale transform G(Â·) after T â€²(Â·). It should be empha-\nsized that in the implementation, the â€œcolor-jitterâ€ operation\nwill give random amplitude values to the image changing.\nWe display the image samples processed with different data\nFig. 4.\nIllustration for the raw images and the augmented images. The 1st\nrow: â€œraw imagesâ€. The 2nd row: â€œcolor-jitterâ€. Bottom row: â€œgray-scaleâ€.\nTABLE V\nPERFORMANCE COMPARISON ON USING COLOR DATA AUGMENTATIONS\nAND GRAY-SCALE TRANSFORM TO THE SECOND NETWORK BRANCH.\nComponents\nCluster Reï¬ne\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\nT â€²(Â·)\n70.3\n87.4\n94.6\n96.5\nJ (T â€²(Â·))\n72.5\n87.8\n95.3\n96.9\nG(T â€²(Â·))\n74.4\n89.3\n95.9\n96.7\nT â€²(Â·)\nâœ“\n79.0\n90.6\n96.3\n97.1\nJ (T â€²(Â·))\nâœ“\n79.1\n90.8\n96.7\n97.8\nG(T â€²(Â·))\nâœ“\n80.9\n92.7\n97.4\n98.5\naugmentation methods in Fig. 4. As can be observed, â€œcolor-\njitterâ€ did change the image, but the color information still\ndominates.\nExperimental results are provided in Table V. We can\nread that using â€œcolor-jitterâ€ J (Â·) yields some performance\nimprovement, but using â€œgray-scaleâ€ G(Â·) yield the best\nperformance improvement. When combined with the cluster\nreï¬nement step, we can observe the similar result that: using\nâ€œgray-scaleâ€ G(Â·) yields better performance improvement than\nusing â€œcolor-jitterâ€ J (Â·). These results validate that suppress-\ning colors is effective to gain performance improvement.\nCompared to using â€œgray-scaleâ€, using â€œcolor-jitterâ€ does not\ntruly eliminate the inï¬‚uence brought by colors, that is to say,\nafter using color-jitter, the color information still dominates.\nTo further reveal the mechanisms why using â€œgray-scaleâ€\nworks better than using â€œcolor-jitterâ€ in the proposed frame-\nwork, we show the statistic histograms of color distributions\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n9\n(a) Raw Images\n(b) Color-Jitter\n(c) Gray-Scale\nFig. 5.\nComparison on distributions in histogram of intensity in RGB channels under different data augmentation operations.\nof using raw image, color-jitter, and gray-scale, respectively.\nSpeciï¬cally, we compute the statistical histograms of the\nintensity values in the RGB channels of the raw color images\nand the images after using â€œcolor-jitterâ€ and â€œgray-scaleâ€ with\n500 images sampled at random in the training data from\nMarket-1501. The statistical results are shown in Fig. 5.\nWe can observe that: using â€œgray-scaleâ€ yields roughly\nconsistent distribution in the histogram compared to the raw\nimages; whereas using the distribution in the histogram of the\nimages after using â€œcolor-jitterâ€ has some notable deviations\nfrom that of the raw images. In the histogram of using\nâ€œgray-scaleâ€, the proportion of the pixels at the two extreme\nvalues (i.e., 0 and 255) are signiï¬cantly reduced; whereas\nin the histogram of using â€œcolor-jitterâ€, the proportion of\nthe pixels at the two extreme values, especially at 0, are\nsigniï¬cantly magniï¬edâ€”this phenomenon might damage the\ncontent consistency with the raw image. The difference in the\nconsistency of the histogram reveals the essential advantage of\nusing â€œgray-scaleâ€ to suppress the inï¬‚uence of colors, rather\nthan using â€œcolor-jitterâ€.\nEvaluation on Parameters in DBSCAN. We conduct ex-\nperiments evaluate the parameter d to ï¬nd the neighbors. In\ncluster reï¬nement, we use DBSCAN with a smaller parameter\ndâ€², where dâ€² := d âˆ’Î´ to ï¬nd the over-segmentation. We\nconduct experiments on Market-1501 to evaluate the effects\nof changing the two parameters. Experiments are recorded\nin Table VI. we can ï¬nd that while the change of d will\naffect the baseline performance, our CACL still improves the\nmodel performance signiï¬cantly. Note that even though the\nbaseline performance will sharply drop when using d = 0.7,\nour method can also achieve a good performance which is also\nhigher than other unsupervised methods in Table I.\nThe cluster reï¬nement is an important component in our\nproposed CACL, and Î´ is an important parameter to ï¬nd the\nover-segmentation of the raw clusters. Thus, we further con-\nduct experiments to evaluate the performance of using different\nvalues of Î´. Experimental results are shown in Table VII.\nWe can ï¬nd that the performance is not too sensitive to Î´.\nWhen using Î´ = 0.02, the performance achieves the best, i.e.,\n80.9/92.7% at mAP/Rank-1 on Market-1501 and 69.6/82.6%\nat mAP/Rank-1 on DukeMTMC-ReID.\nMoreover, we also test the stop-gradient operations under\nTABLE VI\nPERFORMANCE COMPARISON OF DIFFERENT CLUSTER PARAMETER d\n(THE MAXIMUM DISTANCE BETWEEN NEIGHBOR POINTS) ON CACL AND\nBASELINE METHOD.\nd\nMarket-1501\nDukeMTMC-ReID\nBaseline\nCACL\nBaseline\nCACL\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\n0.4\n68.6\n85.9\n75.2\n91.4\n60.1\n77.5\n62.0\n77.7\n0.5\n71.2\n86.5\n81.6\n93.0\n63.4\n80.3\n67.5\n81.8\n0.6\n68.1\n85.2\n80.9\n92.7\n62.5\n78.5\n69.6\n82.6\n0.7\n43.8\n71.5\n75.8\n90.1\n4.1\n10.3\n66.7\n80.6\nTABLE VII\nILLUSTRATION FOR THE MODEL PERFORMANCE WITH DIFFERENT Î´ ON\nMARKET-1501.\nÎ´\nMarket-1501\nd = 0.4\nd = 0.5\nd = 0.6\nd = 0.7\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\n0.02\n75.2\n91.4\n81.6\n93.0\n80.9\n92.7\n75.8\n90.1\n0.04\n70.8\n89.5\n80.4\n92.6\n80.3\n92.3\n68.7\n86.2\n0.06\n65.8\n87.2\n77.7\n91.7\n79.0\n91.4\n8.20\n20.3\n0.08\n64.3\n86.2\n76.6\n91.2\n78.5\n91.3\n6.10\n15.6\ndifferent structures. In Table IV, as can be read that, the\nperformance of the framework with asymmetric structure\ndrops slightly (i.e., only 0.7% lower than that of using the\nstop-gradient operation) when the stop-gradient operation is\nnot used. This hints that the framework with asymmetric\nstructure in CACL does not highly depend on the stop-gradient\noperation.\nEvaluation Performance of Two Branches. To further re-\nveal the performance of the trained networks, we record the\nperformance of using the output features of each branch of\ntwo networks F(Â·|Î˜) and F â€²(Â·|Î˜â€²), separately, for person Re-\nID in Table VIII. We can read that using the output features\nof the second branch F â€²(Â·|Î˜â€²) did yield signiï¬cantly lower\nperformance than that of using the output feature of the ï¬rst\nbranch F(Â·|Î˜), and the result of using F â€²(Â·|Î˜â€²) is similar to\nthe result of the experiments without using L(intra)\nC\n. This is\nbecause the second network branch pays attention to learning\nfeatures from gray-scale images, lacking of the ability to\ncapture richer information from color images.\nEvaluation on Memory Update Parameter Î±. We conduct\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n10\nTABLE VIII\nPERFORMANCE COMPARISON ON F(Â·|Î˜) AND F â€²(Â·|Î˜â€²).\nBranch\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\nF(Â·|Î˜) (Color)\n80.9\n92.7\n97.4\n98.5\nF â€²(Â·|Î˜â€²) (Gray-Scale)\n43.8\n71.5\n83.9\n87.1\nTABLE IX\nPERFORMANCE COMPARISON ON DIFFERENT Î±.\nBranch\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\n0.0\n75.1\n89.8\n96.3\n97.3\n0.2\n80.9\n92.7\n97.4\n98.5\n0.4\n80.8\n92.5\n97.1\n98.2\n0.6\n80.2\n92.4\n97.2\n98.3\n0.8\n77.3\n90.9\n96.6\n98.0\n1.0\n4.3\n10.9\n19.9\n24.9\nexperiments to evaluate the effects of the memory update\nparameter Î± and show the results in Table IX. We can ï¬nd\nthat our CACL is not sensitive to the changing of memory\nupdate parameter Î±, except for Î± = 1. When using Î± = 1, the\nmodel performance signiï¬cantly drops because the memory\nbank has not been updated at this time. When using Î± = 0.2\nthe model achieves the best performance on Market-1501, i.e.,\n80.9/92.7% at mAP/Rank-1.\nEvaluation on Performance with Ground-truth Labels.\nWe compare our CACL to the baseline method with the\nground-truth labels (i.e., in supervised setting). The results\nare shown in Table X. We can ï¬nd that CACL could achieve\ngood performance under unsupervised setting, which is merely\nlower 3/1.1% at mAP/Rank-1 than the baseline method, which\nis trained with the ground-truth labels on Market-1501. More-\nover, if we provide ground-truth labels to train our CACL (i.e.,\nCACL+labels), notable improvements in performance than the\nsupervised baseline method can be observed.\nF. Data Visualization\nTo gain some intuitive understanding of the performance of\nour proposed CACL, we conduct a set of data visualization\nexperiments on Market-1501 to visualize the clustering results\nof the learned features when different training strategies are\nused: a) without using the contrastive loss LC + LI; and b)\nusing the contrastive losses LC + LI.\nTABLE X\nPERFORMANCE COMPARISON TO BASELINE METHOD IN SUPERVISED\nSETTING. â€œBASELINE + LABELSâ€ MEANS THAT WE USE THE\nGROUND-TRUTH LABELS TO TRAIN THE BASELINE METHOD; WHEREAS\nâ€œCACL + LABELSâ€ MEANS THAT WE USE THE GROUND-TRUTH LABELS\nTO TRAIN OUR CACL.\nMethod\nMarket-1501\nDukeMTMC-ReID\nmAP\nRank-1\nmAP\nRank-1\nCACL\n80.9\n92.7\n69.6\n82.6\nBaseline + labels\n83.9\n93.6\n73.3\n86.6\nCACL + labels\n85.7\n94.2\n74.9\n87.2\nExperimental results are shown in Fig. 6. We can observe\nthat the contrastive loss LC + LI did help the model dis-\ntinguish those similar images while maintaining the cluster\ncompactness, and also separate the overlapping individual\nsamples from each other. This conï¬rms the effectiveness of\nour proposed approach, and it also shows that our approach\ncan attenuate the inï¬‚uence of clothing color.\nAt the same time, we also selected some query samples\nwith the top-10 best matching images in the gallery set and\nshow them in Fig. 7. Compared to the baseline model, our\napproach returns more accurate results. We can ï¬nd that\nmost of the wrong samples matched by the baseline model\nare dressed in the same color with the query sample. These\nresults suggest that our approach can effectively ignore the\ninterference caused by samples with similar colors and thus\nï¬nd more accurate matches.\nV. CONCLUSION\nWe have proposed a Cluster-guided Asymmetric Contrastive\nLearning (CACL) approach for unsupervised person Re-ID,\nin which cluster information is leveraged to guide the feature\nlearning in a properly designed contrastive learning frame-\nwork. Speciï¬cally, in our proposed CACL, instance-level con-\ntrastive learning is conducted with respect to the asymmetric\ndata augmentation and cluster-level contrastive learning is\nconducted with respect to the reï¬ned clustering result. By\nleveraging the reï¬ned cluster result into contrastive learning,\nCACL is able to effectively exploit the invariance within and\nbetween different data augmentation views for learning more\neffective features beyond the dominating colors. In addition,\nwe conï¬rmed that reï¬ned clustering result could help our\nCACL approach mine invariant information more effectively\nat the cluster level. We have conducted extensive experiments\non three benchmark datasets and demonstrated the superior\nperformance of our proposal.\nAs the future work, it is interesting and promising to\nincorporate attention mechanism (e.g., [50], [51]), clustering\nensemble and hybrid contrastive learning strategy (e.g., [52])\nor side information in dataset (e.g., [12]) to further enrich\nthe representation capacity, improve the stability and enhance\nthe overall performance of the proposed framework. Whatâ€™s\nmore, in other related ï¬elds, such as face recognition or\nvehicle re identiï¬cation (e.g., [53], [54]), whether suppresses\nthe dominating color can also bring positive inï¬‚uence is a very\ninteresting and worth exploring direction.\nREFERENCES\n[1] L. Zheng, Y. Yang, and A. G. Hauptmann, â€œPerson re-identiï¬cation:\nPast, present and future,â€ arXiv preprint arXiv:1610.02984, 2016. 1, 2\n[2] Y. Ge, D. Chen, and H. Li, â€œMutual mean-teaching: Pseudo label\nreï¬nery for unsupervised domain adaptation on person re-identiï¬cation,â€\nin International Conference on Learning Representations, 2020. 1, 6, 7\n[3] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, â€œA density-based algorithm\nfor discovering clusters in large spatial databases with noise,â€ in Second\nInternational Conference on Knowledge Discovery and Data Mining,\n1996, p. 226â€“231. 1, 5\n[4] K. Zeng, M. Ning, Y. Wang, and Y. Guo, â€œHierarchical clustering with\nhard-batch triplet loss for person re-identiï¬cation,â€ in IEEE Conference\non Computer Vision and Pattern Recognition, 2020, pp. 13 657â€“13 665.\n1, 2, 6, 7\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n11\nCACL (Without â„’! and â„’\")\nCACL (Full)\nFig. 6. Data Visualization via t-SNE of the learned feature and clusters under two different training strategies: Training without LC and LI (left) as mentioned\nin Table III and our CACL (right). The data points come from the Market-1501 training set (1,000 images of 60 identities). The points with the same color\nmean the image of the same identity. To demonstrate the difference between the two distributions in detail, we further zoom in on the circled clusters and\nshow the corresponding images. The images in the boxes are similar to each other and the corresponding data points are very close to each other or even\noverlapping in the feature space if the model is trained without using LC and LI, as shown in the left box; whereas using the contrastive losses LC and LI\nwill effectively distinguish these data points and maintain the cluster compactness as shown in the right box.\nBaseline\nCACL\nQuery\n1st\n10th\n1st\n10th\nFig. 7. Visualization of the top-10 best matched images. We show the top-10 best matching samples in the gallery set for the query sample with the baseline\nmethod and our proposed CACL. The images with frames in green and in red are the correctly matched images and mismatched images, respectively.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n12\n[5] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, and T. S. Huang, â€œSelf-\nsimilarity grouping: A simple unsupervised cross domain adaptation\napproach for person re-identiï¬cation,â€ in The IEEE International Con-\nference on Computer Vision, October 2019, pp. 6112â€“6121. 1, 6, 7\n[6] J. Xie, X. Zhan, Z. Liu, Y. S. Ong, and C. C. Loy, â€œDelving into inter-\nimage invariance for unsupervised visual representations,â€ in Conference\nand Workshop on Neural Information Processing Systems, 2020. 1, 3\n[7] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,\nâ€œUnsupervised learning of visual features by contrasting cluster assign-\nments,â€ Advances in Neural Information Processing Systems, pp. 9912â€“\n9924, 2020. 1, 3\n[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, â€œA simple framework\nfor contrastive learning of visual representations,â€ in International\nConference on Machine Learning, 2020, pp. 1597â€“1607. 1, 3, 4\n[9] X. Chen and K. He, â€œExploring simple siamese representation learning,â€\nin IEEE Conference on Computer Vision and Pattern Recognition, 2021,\npp. 15 750â€“15 758. 1, 3, 5, 8\n[10] J.-B. Grill, F. Strub, F. AltchÂ´e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, B. Piot,\nk. kavukcuoglu, R. Munos, and M. Valko, â€œBootstrap your own latent\n- a new approach to self-supervised learning,â€ in Advances in Neural\nInformation Processing Systems, 2020, pp. 21 271â€“21 284. 1, 3, 4\n[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classiï¬cation\nwith deep convolutional neural networks,â€ in Conference and Workshop\non Neural Information Processing Systems, 2012, pp. 1097â€“1105. 1, 4,\n6\n[12] H. Chen, Y. Wang, B. Lagadec, A. Dantcheva, and F. Bremond,\nâ€œJoint generative and contrastive learning for unsupervised person re-\nidentiï¬cation,â€ in IEEE Conference on Computer Vision and Pattern\nRecognition, June 2021, pp. 2004â€“2013. 2, 5, 10\n[13] J. Liu, Z.-J. Zha, D. Chen, R. Hong, and M. Wang, â€œAdaptive transfer\nnetwork for cross-domain person re-identiï¬cation,â€ in IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 7202â€“7211. 2,\n5\n[14] S. Bak, P. Carr, and J.-F. Lalonde, â€œDomain adaptation through synthesis\nfor unsupervised person re-identiï¬cation,â€ in European Conference on\nComputer Vision, 2018, pp. 189â€“205. 2\n[15] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and\nY. Tian, â€œUnsupervised cross-dataset transfer learning for person re-\nidentiï¬cation,â€ in IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 1306â€“1315. 2\n[16] J. Wang, X. Zhu, S. Gong, and W. Li, â€œTransferable joint attribute-\nidentity deep learning for unsupervised person re-identiï¬cation,â€ in IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n2275â€“2284. 2, 6, 7\n[17] Y. Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, â€œSelf-paced contrastive\nlearning with hybrid memory for domain adaptive object re-id,â€ in\nAdvances in Neural Information Processing Systems, 2020, pp. 11 309â€“\n11 321. 2, 5, 6, 7\n[18] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, â€œA bottom-up clustering\napproach to unsupervised person re-identiï¬cation,â€ in The Association\nfor the Advancement of Artiï¬cial Intelligence, vol. 33, 2019, pp. 8738â€“\n8745. 2, 6, 7\n[19] H. Fan, L. Zheng, C. Yan, and Y. Yang, â€œUnsupervised person re-\nidentiï¬cation: Clustering and ï¬ne-tuning,â€ ACM Transactions on Mul-\ntimedia Computing, Communications, and Applications, vol. 14, no. 4,\np. 83, 2018. 2, 6, 7\n[20] Y. Lin, L. Xie, Y. Wu, C. Yan, and Q. Tian, â€œUnsupervised person re-\nidentiï¬cation via softened similarity learning,â€ in IEEE Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 3390â€“3399. 2, 6,\n7\n[21] B. Sun, J. Feng, and K. Saenko, â€œReturn of frustratingly easy domain\nadaptation,â€ in Association for the Advancement of Artiï¬cial Intelligence,\nvol. 30, 2016. 2\n[22] Y. Ganin and V. Lempitsky, â€œUnsupervised domain adaptation by\nbackpropagation,â€ in International Conference on Machine Learning,\n2015, pp. 1180â€“1189. 2\n[23] D. Wang and S. Zhang, â€œUnsupervised person re-identiï¬cation via\nmulti-label classiï¬cation,â€ in IEEE Conference on Computer Vision and\nPattern Recognition, 2020, pp. 10 981â€“10 990. 2, 6, 7\n[24] P. Bojanowski and A. Joulin, â€œUnsupervised learning by predicting\nnoise,â€ in International Conference on Machine Learning.\nPMLR,\n2017, pp. 517â€“526. 3\n[25] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, and\nT. Brox, â€œDiscriminative unsupervised feature learning with exemplar\nconvolutional neural networks,â€ IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 38, no. 9, pp. 1734â€“1747, 2015. 3\n[26] Y. Li, P. Hu, Z. Liu, D. Peng, J. T. Zhou, and X. Peng, â€œContrastive\nclustering,â€ in AAAI Conference on Artiï¬cial Intelligence, 2021. 3\n[27] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, â€œMomentum contrast\nfor unsupervised visual representation learning,â€ in IEEE Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 9729â€“9738. 3\n[28] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for\nimage recognition,â€ in IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 770â€“778. 3, 5, 6, 7\n[29] Y. Zhai, S. Lu, Q. Ye, X. Shan, J. Chen, R. Ji, and Y. Tian, â€œAd-\ncluster: Augmented discriminative clustering for domain adaptive person\nre-identiï¬cation,â€ in IEEE Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 9021â€“9030. 5, 6, 7\n[30] L. Wei, S. Zhang, W. Gao, and Q. Tian, â€œPerson transfer gan to\nbridge domain gap for person re-identiï¬cation,â€ in IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 79â€“88. 6, 7\n[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, â€œGoing deeper with convolutions,â€ in\nIEEE Conference on Computer Vision and Pattern Recognition, 2015,\npp. 1â€“9. 7\n[32] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, â€œImage-\nimage domain adaptation with preserved self-similarity and domain-\ndissimilarity for person re-identiï¬cation,â€ in IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2018, pp. 994â€“1003. 6, 7\n[33] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, â€œMobilenets: Efï¬cient convo-\nlutional neural networks for mobile vision applications,â€ arXiv preprint\narXiv:1704.04861, 2017. 7\n[34] F. Yang, Z. Zhong, Z. Luo, S. Lian, and S. Li, â€œLeveraging virtual and\nreal person for unsupervised person re-identiï¬cation,â€ IEEE Transac-\ntions on Multimedia, vol. 22, no. 9, pp. 2444â€“2453, 2019. 6, 7\n[35] Z. Zhong, L. Zheng, S. Li, and Y. Yang, â€œGeneralizing a person\nretrieval model hetero-and homogeneously,â€ in European Conference on\nComputer Vision, 2018, pp. 172â€“188. 6, 7\n[36] Z. Ji, X. Zou, X. Lin, X. Liu, T. Huang, and S. Wu, â€œAn attention-driven\ntwo-stage clustering method for unsupervised person re-identiï¬cation,â€\nin European Conference on Computer Vision, 2020, pp. 20â€“36. 6, 7\n[37] J. Li and S. Zhang, â€œJoint visual and temporal consistency for unsuper-\nvised domain adaptive person re-identiï¬cation,â€ in European Conference\non Computer Vision, 2020. 6, 7\n[38] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, and Y. Tian, â€œMultiple expert\nbrainstorming for domain adaptive person re-identiï¬cation,â€ in European\nConference on Computer Vision, 2020, pp. 594â€“611. 6, 7\n[39] F. Zhao, S. Liao, G.-S. Xie, J. Zhao, K. Zhang, and L. Shao, â€œUn-\nsupervised domain adaptation with noise resistible mutual-training for\nperson re-identiï¬cation,â€ in European Conference on Computer Vision.\nSpringer, 2020, pp. 526â€“544. 6, 7\n[40] H.-X. Yu, A. Wu, and W.-S. Zheng, â€œCross-view asymmetric metric\nlearning for unsupervised person re-identiï¬cation,â€ in IEEE Interna-\ntional Conference on Computer Vision, 2017, pp. 994â€“1002. 6, 7\n[41] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, â€œScalable\nperson re-identiï¬cation: A benchmark,â€ in IEEE International Confer-\nence on Computer Vision, 2015, pp. 1116â€“1124. 6, 7\n[42] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, â€œPerson re-identiï¬cation by\nlocal maximal occurrence representation and metric learning,â€ in IEEE\nConference on Computer Vision and Pattern Recognition, 2015, pp.\n2197â€“2206. 6, 7\n[43] M. Wang, B. Lai, J. Huang, X. Gong, and X.-S. Hua, â€œCamera-aware\nproxies for unsupervised person re-identiï¬cation,â€ in AAAI Conference\non Artiï¬cial Intelligence, vol. 2, 2021, p. 4. 6, 7\n[44] X. Pan, P. Luo, J. Shi, and X. Tang, â€œTwo at once: Enhancing learning\nand generalization capacities via ibn-net,â€ in European Conference on\nComputer Vision (ECCV), 2018, pp. 464â€“479. 7\n[45] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, â€œPerformance\nmeasures and a data set for multi-target, multi-camera tracking,â€ in\nEuropean Conference on Computer Vision.\nSpringer, 2016, pp. 17â€“\n35. 6\n[46] L. Wei, S. Zhang, W. Gao, and Q. Tian, â€œPerson transfer gan to\nbridge domain gap for person re-identiï¬cation,â€ in IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 79â€“88. 6\n[47] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€\nin 3rd International Conference on Learning Representations, 2015. 6\n[48] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, â€œInvariance matters:\nExemplar memory for domain adaptive person re-identiï¬cation,â€ in IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp. 598â€“\n607. 7\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n13\n[49] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, â€œUnsupervised feature learning\nvia non-parametric instance discrimination,â€ in IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 3733â€“3742. 7\n[50] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, and G. Wang,\nâ€œDual attention matching network for context-aware feature sequence\nbased person re-identiï¬cation,â€ in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 5363â€“5372. 10\n[51] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, â€œAn image is worth 16x16 words:\nTransformers for image recognition at scale,â€ International Conference\non Learning Representations, 2021. 10\n[52] H. Sun, M. Li, and C.-G. Li, â€œHybrid contrastive learning with clus-\nter ensemble for unsupervised person re-identiï¬cation,â€ arXiv preprint\narXiv:2201.11995, 2022. 10\n[53] X. Liu, W. Liu, H. Ma, and H. Fu, â€œLarge-scale vehicle re-identiï¬cation\nin urban surveillance videos,â€ in 2016 IEEE international conference on\nmultimedia and expo (ICME).\nIEEE, 2016, pp. 1â€“6. 10\n[54] X. Liu, W. Liu, T. Mei, and H. Ma, â€œProvid: Progressive and multi-\nmodal vehicle reidentiï¬cation for large-scale urban surveillance,â€ IEEE\nTransactions on Multimedia, vol. 20, no. 3, pp. 645â€“658, 2017. 10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-06-15",
  "updated": "2022-05-09"
}