{
  "id": "http://arxiv.org/abs/2106.07846v2",
  "title": "Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification",
  "authors": [
    "Mingkun Li",
    "Chun-Guang Li",
    "Jun Guo"
  ],
  "abstract": "Unsupervised person re-identification (Re-ID) aims to match pedestrian images\nfrom different camera views in unsupervised setting. Existing methods for\nunsupervised person Re-ID are usually built upon the pseudo labels from\nclustering. However, the quality of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated by the colors in\nimages especially in the unsupervised setting. In this paper, we propose a\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\nperson Re-ID, in which cluster structure is leveraged to guide the feature\nlearning in a properly designed asymmetric contrastive learning framework. To\nbe specific, we propose a novel cluster-level contrastive loss to help the\nsiamese network effectively mine the invariance in feature learning with\nrespect to the cluster structure within and between different data augmentation\nviews, respectively. Extensive experiments conducted on three benchmark\ndatasets demonstrate superior performance of our proposal.",
  "text": "IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n1\nCluster-guided Asymmetric Contrastive Learning\nfor Unsupervised Person Re-Identiﬁcation\nMingkun Li, Chun-Guang Li, Senior Member, IEEE, and Jun Guo\nAbstract—Unsupervised person re-identiﬁcation (Re-ID) aims\nto match pedestrian images from different camera views in an\nunsupervised setting. Existing methods for unsupervised person\nRe-ID are usually built upon the pseudo labels from clustering.\nHowever, the result of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated\nby colors in images. In this paper, we attempt to suppress the\nnegative dominating inﬂuence of colors to learn more effective\nfeatures for unsupervised person Re-ID. Speciﬁcally, we propose\na Cluster-guided Asymmetric Contrastive Learning (CACL) ap-\nproach for unsupervised person Re-ID, in which clustering result\nis leveraged to guide the feature learning in a properly designed\nasymmetric contrastive learning framework. In CACL, both\ninstance-level and cluster-level contrastive learning are employed\nto help the siamese network learn discriminant features with\nrespect to the clustering result within and between different\ndata augmentation views, respectively. In addition, we also\npresent a cluster reﬁnement method, and validate that the cluster\nreﬁnement step helps CACL signiﬁcantly. Extensive experiments\nconducted on three benchmark datasets demonstrate the superior\nperformance of our proposal.\nIndex Terms—Unsupervised Person Re-Identiﬁcation, Asym-\nmetric Contrastive Learning, Cluster Reﬁnement.\nI. INTRODUCTION\nU\nNSUPERVISED person Re-identiﬁcation (Re-ID) aims\nto match pedestrian images from different camera views\nin unsupervised setting without demanding massive labelled\ndata, and has attracted increasing attention in computer vision\nand pattern recognition community in recent years [1]. The\ngreat challenge we face in unsupervised person Re-ID is to\ntackle heavy variations from different viewpoints, varying illu-\nminations, changing weather conditions, cluttered background\nand etc., without supervision labels.\nRecently, existing methods for unsupervised person Re-ID\nare usually built on exploiting weak supervision information\n(e.g., pseudo labels) from clustering. For example, MMT [2]\nuses DBSCAN [3] algorithm to generate pseudo labels and\nexploit the pseudo labels to train two networks. HCT [4]\nuses a hierarchical clustering algorithm to gradually assign\npseudo labels to the training samples during the training stage.\nSSG [5] uses k-means on training samples with multi-views.\nHowever, the performance of these methods heavily relies on\nthe quality of the pseudo labels, which directly depends on\nthe feature representation of the input images.\nM. Li, C.-G. Li and J. Guo are with the School of Artiﬁcial Intelligence,\nBeijing University of Posts and Telecommunications, Beijing, 100876 P.R.\nChina e-mail: {mingkun.li, lichunguang, guojun}@bupt.edu.cn.\nChun-Guang Li is the corresponding author.\nManuscript received xx, 2021; revised xx, xxxx.\nGray-Scale Images\nColor Images\nColor Images\nCluster\nContrastive\nLearning\nFig. 1. Illustration for basic idea of our proposal. We attempt to leverage the\nclustering information into contrastive learning to ﬁnd more effective features\nby exploring the invariance between color images and gray-scale images.\nMore recently, contrastive learning is applied to perform\nfeature learning in unsupervised setting, e.g., [6], [7], [8],\n[9], [10]. The primary idea in these methods is to learn\nsome invariance in feature representation with self-supervised\nmechanism based on data augmentation. In SimCLR [8], each\nsample and its multiple augmentations are treated as positive\npairs, and the rest of the samples in the same batch are treated\nas negative pairs and, a contrastive loss is used to distinguish\nthe positive and negative samples to prevent the model from\nfalling into a trivial solution. We note that SimCLR requires\nto use a large batch size, e.g., 256 ∼4096, to contain enough\nnegative samples for effectively training the networks. In\nBYOL [10] and SimSiam [9], a predictor layer is used to\nprevent feature collapse without using negative samples. In\nInterCLR [6] and SwAV [7], clustering is used to prevent\nthe feature collapse. In particular, in SwAV [7], a scalable\nonline clustering loss is proposed to train the siamese network\nwith multi-crop data augmentation; whereas in InterCLR [6],\na MarginNCE loss is proposed to enhance the discriminant\npower. While promising performance has been reported on\nImageNet [11], however, these contrastive learning methods\nare not suitable for unsupervised person Re-ID due to serious\nfeature collapse.\nIn this paper, we attempt to leverage cluster information\ninto contrastive learning to develop an effective framework for\nunsupervised person Re-ID. We notice that the performance\nof person Re-ID depends heavily on the effectiveness of the\nlearned features. However, the learned features are overwhelm-\narXiv:2106.07846v2  [cs.CV]  9 May 2022\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n2\ningly dominated by the colors in pedestrian images (such as\nthe clothing color and background color), especially in the\nunsupervised setting. For example, the pedestrian images with\nsimilar color clothes often have smaller distances in feature\nspace, which may result in mistakes in clustering, and the\nmistakes in clustering may further bring wrong guidance to\nthe pseudo labels for training the network. Although colors\nare important feature to match pedestrian images for person\nRe-ID, it may also become an obstacle to learn more subtle\nand effective texture features that are important ﬁne-level cues\nfor person Re-ID. Thus it is desirable to learn more robust\nand discriminating features that can resist dominant colors for\nperson Re-ID task.\nUnfortunately, it is quite challenging to properly suppress\nthe negative impact of colors for learning more effective ﬁne-\ngrain level features without loss of discriminant information.\nFor example, directly using random color changing (i.e., color-\njitter [12]) for data augmentation in contrastive training may\ndamage the consistency in color distribution, not that helpful to\ngain generalization ability on unseen samples. To this end, in\nthis paper, we propose a novel and effective framework for un-\nsupervised person Re-ID, termed Cluster-guided Asymmetric\nContrastive Learning (CACL), in which clustering information\nis properly incorporated into contrastive learning to learn\nrobust and discriminant features while suppressing dominant\ncolors, as illustrated in Fig. 1. To be speciﬁc, we explore\nsupervision information from the perspective of suppressing\ncolors in the framework of cluster-guided contrastive learning,\nin which the samples in asymmetric views of speciﬁcally\ndesigned data augmentations (e.g., color images vs. gray-\nscale images) as shown in Fig. 2—are exploited to provide\nstrong supervision to impose invariance in feature learning.\nBy integrating the clustering results into contrastive learning,\nthe proposed framework is able to avoid feature collapse.\nBy suppressing dominant colors, the proposed framework is\nable to effectively learn robust and discriminating features\nother than colors. In addition, we also present a simple but\neffective cluster reﬁnement method to improve the clustering\nresult and thus further enhancing the contrastive learning. We\nconduct extensive experiments on three benchmark datasets,\nand experimental results validate the effectiveness of our\nproposal.\nPaper Contributions. The contributions of the paper are\nhighlighted as follows.\n1) We propose an effective unsupervised framework that\nleverages clustering information into contrastive learning\nwhile suppressing the dominant colors in images to learn\nﬁne-grained features.\n2) We propose a novel cluster-level loss function to perform\ninter-views and intra-view contrastive learning that can\neffectively exploit the cluster-level hidden information\nfrom different data augmentation views.\n3) We also present a cluster reﬁnement method and verify\nthat the reﬁned clustering information helps the con-\ntrastive learning framework signiﬁcantly.\nThe remainder of this paper is organized as follows. Sec-\ntion II describes the relevant work. Section III presents our\n(a) Raw\n(b) T\n(c) T ′\n(d)G ◦T ′\nFig. 2.\nIllustration for the raw images and the augmented images. The\nﬁrst column shows the raw images. The middle two columns show the\nimages generated with transforms T (·) and T ′(·). The last column shows\nthe corresponding gray-scale images which are generated with both transform\nT ′(·) and color-to-grayscale transform G(·), i.e., G ◦T ′(·).\nproposal. Section IV shows experiments and Section V gives\nthe conclusions.\nII. RELATED WORK\nA. Unsupervised Person Re-identiﬁcation\nPerson Re-ID aims to ﬁnd speciﬁc pedestrians from videos\nor images according to targets. For the increasing demand\nin real life and avoiding the high consumption of labeling\ndatasets, unsupervised person Re-ID has become popular in\nrecent years [1]. The existing unsupervised person Re-ID\nmethods can be divided into two categories: a) unsupervised\ndomain adaptation methods, that need labeled source dataset\nand unlabeled target dataset [13], [14], [15], [16]; and b)\npure unsupervised methods, that need with only unlabeled\ndataset [17], [18], [19], [20].\nThe unsupervised domain adaptation methods train the\nnetwork with the help of labeled datasets, and transfer the\nnetwork to unlabeled datasets by reducing the gap between two\ndatasets. For example, [21] proposed to align the second-order\nstatistics of the distributions in the two domains through linear\ntransformations to reduce the domain shift; [17] proposed a\ncombined loss function to co-train with samples from the\nsource and target domains and the merging memory bank; [22]\nproposed to maximize the inter-domain classiﬁcation loss and\nminimize the intra-domain classiﬁcation loss to learn domain\nrobust features. However, unsupervised domain adaptation\nmethods are limited by the requirement of the target dataset\nhaving close distribution to the source dataset.\nMost purely unsupervised person Re-ID methods rely on the\npseudo labels to train the network. For example, HCT [4] uses\nhierarchical clustering to generate pseudo labels and train the\nconvolution neural network for feature learning; [23] assigns\nmultiple labels to samples and proposes a new loss function\nfor multi-label training. Note that the quality of the pseudo\nlabels relies on the feature representation of the input images.\nHowever, in the early stage, the feature representation is not\ngood enough to generate high-quality pseudo labels, and thus\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n3\nRaw\nImage\nCluster-level\nContrastive\nLoss ℒ!\nInstance−level \nContrastive \nLoss ℒ\"\nFlow\nLoss\nBack Propagate\nPredictor G( ⋅|Ψ)\nResNet\n𝐹(⋅|Θ)\nFeature embedding\nUpdate\nImage\n𝐼'%\nImage\n𝐼(%\n𝒯(⋅)\nClustering & Cluster Refinement\nCluster-level\nContrastive\nLoss ℒ!\n𝓍%\n𝐼%\n𝑧%\n𝓍-%\n𝓍%\n𝓍-%\nPseudo Labels\n𝒢∘𝒯&(⋅)\nResNet\n𝐹&(⋅|Θ′)\nInstance-Level Contrastive Learning\nCluster-Level Contrastive Learning\nInstance Memory Bank\nℳ&\nInstance Memory Bank\nℳ\nUpdate\nFig. 3. Illustration for our proposed Cluster-guided Asymmetric Contrastive Learning (CACL) framework. After training, we keep only the ResNet F(·|Θ)\nin the ﬁrst branch for inference and use the feature xi for testing.\nthe low-quality pseudo labels will contaminate the network\ntraining. Therefore, it is needed to design a cluster reﬁnement\nmethod to improve the clustering quality before feeding the\npseudo labels to train the network.\nB. Contrastive Learning\nIn recent years, with the development and application of\nthe siamese network, contrastive learning began to emerge in\nthe ﬁeld of unsupervised learning. Contrastive learning aims\nat learning good image representation. It learns invariance in\nfeatures by manipulating a set of positive samples and negative\nsamples with data augmentation.\nThe existing methods for contrastive learning can be further\ncategorized to: a) instance-level methods [8], [9], [24], [25],\n[10] and b) cluster-level methods [7], [6], [26]. Instance-level\nmethods regard each image as an individual class and consider\ntwo augmented views of the same image as positive pairs and\ntreat others in the same batch (or memory bank) as negative\npairs. For example, SimCLR [8] regards samples in the current\nbatch as the negative samples; MoCo [27] uses a dictio-\nnary to implement contrastive learning, which converts one\nbranch of the contrastive learning into a momentum encoder;\nSimSiam [9] proposed a stop-gradient method that can train\nthe siamese network without negative samples. Cluster-level\nmethods regard the samples in the same clusters as positive\nsamples and other samples as negative samples. For example,\nin [6] InfoNCE loss is combined with MarginNCE loss to\nattract positive samples and repelled negative samples; in [7]\nmulti-crop data augmentation is used to enhance the robustness\nof the network and a scalable online clustering method is\nproposed to explore the inter-invariance of clusters; in [26]\nweights-sharing deep neural networks are used to extract\nfeatures from sample pairs with different data augmentations,\nand contrastive clustering is performed with respect to both\nthe features in the row and column spaces.\nHowever, in the unsupervised setting, the instance-level\ncontrastive learning methods simply make each sample in-\ndependently repel each other, which will undoubtedly ignore\nthe cluster information. In contrast, cluster-level contrastive\nlearning can effectively mine cluster information, but it relies\nheavily on the clustering result. Unfortunately, in the early\ntraining stage, the features are not good enough to yield good\nclustering result. Thus, an effective way to train the network by\ncombining both the two lines of contrastive learning methods\nis needed.\nIn this paper, we attempt to bridge the two lines of con-\ntrastive learning methods into a uniﬁed framework to form\neffective mutual learning and joint training: a) the instance-\nlevel contrastive learning helps training the network to perform\nfeature learning—especially in the early training stage; mean-\nwhile b) the cluster-level contrastive learning helps training\nthe network—especially when the quality of the clustering has\nbeen improved. In this way, the self-supervision information\nimposed by data augmentation and the weak supervision\ninformation obtained from clustering can be fully exploited\nwithout the need to use negative samples pairs.\nIII. OUR PROPOSAL: CLUSTER-GUIDED ASYMMETRIC\nCONTRASTIVE LEARNING (CACL)\nThis section presents our proposal—Cluster-guided Asym-\nmetric Contrastive Learning (CACL) approach for unsuper-\nvised person Re-ID.\nFor clarity, we show the architecture of our proposed CACL\nin Fig. 3. Overall, our CACL is a siamese network, which\nconsists of two branches of backbone networks F(·|Θ) and\nF ′(·|Θ′) without sharing parameters, where Θ and Θ′ are the\nparameters in the two networks, respectively, and a predictor\nlayer G(·|Ψ) is added after the ﬁrst branch, where Ψ denotes\nthe parameters in the predictor layer. The backbone networks\nF(·|Θ) and F ′(·|Θ′) are implemented1 via ResNet-50 [28] for\nfeature learning.\n1It also works if the backbone networks other than ResNet-50 are used.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n4\nGiven an unlabeled image dataset I = {Ii}N\ni=1 consisting\nof N samples. For an input image Ii ∈I, we generate two\nsamples ˆIi and ˜Ii via different data augmentation strategies\nas the inputs of the two branches, respectively, in which\nˆIi = T (Ii) and ˜Ii = G(T ′(Ii)), where T (·) and T ′(·) denote\ntwo different transforms and G(·) denotes the operation to\ntransform color image into gray-scale image. For simplicity,\nwe denote the output features of the ﬁrst network branch and\nthe second network branch as xi and ˜xi, and denote the output\nof the predictor layer in the ﬁrst branch as zi, respectively,\nwhere xi, ˜xi, zi ∈RD.\nThe\nclustering\nresult\nof\nthe\noutput\nfeatures\nX\n:=\n{x1, · · · , xN} from the ﬁrst network branch is used to gener-\nate the pseudo labels Y := {y1, · · · , yN}. We exploit the\npseudo labels to leverage the cluster information into the\ncontrastive learning. Speciﬁcally, in the training stage, the\ntwo network branches F(·|Θ) and F ′(·|Θ′) are trained with\nthe augmented samples without sharing parameters, and the\npseudo labels Y are used to guide the training of both network\nbranches.\nIn CACL, we use instance memory banks M = {vi}N\ni=1\nand\n˜\nM = {˜vi}N\ni=1 where vi, ˜vi ∈RD to store the outputs of\ntwo branches, respectively. Both instance memory banks M\nand\n˜\nM are initialized with X := {x1, · · · , xN} and ˜\nX :=\n{˜x1, · · · , ˜xN}, which are the outputs of the network branches\nF(·|Θ) and F ′(·|Θ′) pre-trained on ImageNet, respectively.\nA. Cluster-guided Contrastive Learning\nAt beginning, we pre-train the two network branches F(·|Θ)\nand F ′(·|Θ′) on ImageNet [11], and use the features from\nthe ﬁrst network branch F(·|Θ) to yield m clusters, which\nare denoted as C := {C(1), C(2), · · · , C(m)}. The clustering\nresult is used to form pseudo labels to train the cluster-guided\ncontrastive learning module.\nTo exploit the label invariance between the two augmented\nviews and leverage the cluster structure, we employ two types\nof contrastive losses: a) instance-level contrastive loss, denoted\nas LI, and b) cluster-level contrastive loss, denoted as LC.\nInstance-Level Contrastive Loss. To match the feature out-\nputs zi and ˜xi of the two network branches at instance-level,\nsimilar to [8], [10], we introduce the negative cosine similarity\nof the prediction outputs zi in the ﬁrst branch and the feature\noutput of the second branch ˜xi to deﬁne an instance-level\ncontrastive loss LI as follows:\nLI := −z⊤\ni\n∥zi∥2\n˜xi\n∥˜xi∥2\n,\n(1)\nwhere ∥· ∥2 is the ℓ2-norm.\nCluster-Level Contrastive Loss. To leverage the cluster struc-\nture to further explore the hidden information from different\nviews, we propose a cluster-level contrastive loss LC, which\nis further divided into inter-views cluster-level contrastive loss\nand intra-views cluster-level contrastive loss.\n• Inter-views Cluster-level contrastive loss, denoted as\nL(inter)\nC\n, which is deﬁned as:\nL(inter)\nC\n:= −z⊤\ni\n∥zi∥2\n˜uω(Ii)\n∥˜uω(Ii)∥2\n,\n(2)\nwhere ω(Ii) is to ﬁnd the cluster index ℓfor zi, and ˜uℓ\nis the center vector of the ℓ-th cluster in which ˜U :=\n{˜u1, · · · , ˜um′} and the cluster center ˜uℓis deﬁned as\n˜uℓ=\n1\n|C(ℓ)|\nX\nIi∈C(ℓ)\n˜vi,\n(3)\nwhere ˜vi is the instance feature of image ˜Ii in the instance\nmemory bank ˜\nM, C(ℓ) is the ℓ-th cluster. The inter-views\ncluster-level contrastive loss L(inter)\nC\ndeﬁned in Eq. (2)\nis used to reduce the discrepancy between the projection\noutput zi of the ﬁrst network branch and the cluster center\n˜uℓof the feature output of the second branch with the\ngray-scale view.\n• Intra-views Cluster-level contrastive loss, denoted as\nL(intra)\nC\n, which is deﬁned as:\nL(intra)\nC\n= −(1 −qi)2 ln(qi)\n−(1 −˜qi)2 ln(˜qi),\n(4)\nwhere qi and ˜qi are the softmax of the inner product of the\nnetwork outputs and the corresponding instance memory\nbank, which are deﬁned as\nqi =\nexp(u⊤\nω(Ii)xi/τ)\nPm′\nℓ=1 exp(u⊤\nℓxi/τ)\n,\n(5)\n˜qi =\nexp(˜u⊤\nω(Ii)˜xi/τ)\nPm′\nℓ=1 exp(˜u⊤\nℓ˜xi/τ)\n,\n(6)\nwhere uℓand ˜uℓare the center vectors of the ℓ-th cluster\nfor the ﬁrst branch and the second branch, respectively,\nin which ˜uℓis deﬁned in Eq. (3) and uℓis deﬁned as\nuℓ=\n1\n|C(ℓ)|\nX\nIi∈C(ℓ)\nvi,\n(7)\nwhere vi is the instance feature of image ˆIi in the instance\nmemory bank M. Note that both xi and ˜xi share the\nsame pseudo labels ω(Ii) from clustering. The intra-\nviews cluster-level contrastive loss L(intra)\nC\nin Eq. (4) is\nused to encourage the siamese network to learn features\nwith respect to the corresponding cluster center for the\ntwo branches, respectively.\nPutting the loss functions in Eqs. (2) and (4) together, we\nhave the cluster-level contrastive loss LC as follows:\nLC := L(inter)\nC\n+ L(intra)\nC\n.\n(8)\nRemark 1. The cluster-level contrastive loss LC in Eq. (8)\naims to leverage the clustering information to minimize the\ndifference between the samples of the same cluster from\ndifferent augmentation views via L(inter)\nC\n, and within the same\naugmentation view via L(intra)\nC\n. This will help the siamese\nnetwork to mine the hidden information brought by the basic\naugmented view in the ﬁrst branch and the gray-scale aug-\nmented view in the second branch to prevent feature collapse\nto a trivial solution and impose the supervision information to\nlearn features other than colors.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n5\nB. Clustering and Cluster Reﬁnement\nNote that the cluster-level contrast loss is greatly affected by\nthe quality of the clustering result. When the clusters are noisy,\nit will cause negative effects on the training. To improve the\nquality of the clustering result, we propose a cluster reﬁnement\nmethod which removes a proportion of noisy samples in larger\nclusters, helping the model to better learn the information at\nthe cluster level.\nFor a cluster, we want to keep the samples with higher sim-\nilarity and remove the samples with lower similarity. Given a\nset of raw clusters, denoted as {C(1), C(2), · · · , C(m)}, without\nloss of generality, we pick C(i) to perform cluster reﬁnement.\nAt ﬁrst, we obtain an over-segmentation of C(i), i.e., C(i) is\nfurther divided into {C(i)\n1 , C(i)\n2 , · · · , C(i)\nni }. Then we perform\ncluster reﬁnement according to the following criterion:\nif D(C(i)\nj |C(i)) < D(C(i)), then C(i)\nj is kept;\n(9)\notherwise C(i)\nj\nis removed, where D(C(i)\nj |C(i)) is the average\ninter-distance from all samples in the sub-cluster C(i)\nj\nto other\nsamples in cluster C(i), and D(C(i)) is the average intra-\ndistance among samples in cluster C(i).\nAfter such a post-processing step, the clusters of larger\nsize are improved and at meantime, more singletons or tiny\nclusters are also produced. We denote the reﬁned clusters\nas C′ = {C(1), C(2), · · · , C(m′)}, where m′ ≥m. Compared\nto tiny clusters and singletons, the larger clusters are more\ninformative to provide pseudo supervision information to\nguide the contrastive learning.\nRemark 2. In implementation, we use DBSCAN algorithm [3]\nto generate the raw clusters and to generate the over-\nsegmentation of the clusters. DBSCAN [3] is a density-based\nclustering algorithm. It regards a data point as density reach-\nable if the data point lies within a small distance threshold d to\nother samples, where the parameter d is the distance threshold\nto ﬁnd neighboring point. Speciﬁcally, to generate the raw\nclusters, we employ DBSCAN with a slightly larger distance\nthreshold parameter d (e.g., d = 0.6); whereas to generate the\nover-segmentation, we use a slightly smaller distance threshold\nparameter d′, where d′ := d−δ (e.g., δ = 0.02). We will show\nthe inﬂuence of the parameters δ and d in experiments.\nC. Training Procedure for Our CACL Approach\nIn CACL, the two branches in the siamese network are\nimplemented with ResNet-50 [28] and they are not sharing\nparameters. We pre-train the two network branches on Ima-\ngeNet at ﬁrst and use the learned features to initialize the two\nmemory banks M and\n˜\nM, respectively.\nIn training stage, we train both network branches at the\nsame time with the total loss:\nL := LI + LC.\n(10)\nWe update the two instance memory banks M and\n˜\nM,\nrespectively, as follows:\nv(t)\ni\n←αv(t−1)\ni\n+ (1 −α)xi,\n(11)\n˜v(t)\ni\n←α˜v(t−1)\ni\n+ (1 −α)˜xi,\n(12)\nwhere α is set as 0.2 by default (and we will discuss the\ninﬂuence of α in experiments).\nIn order to save the computation cost2, we also use a stop-\ngradient operation as mentioned in SimSiam [9]. Note that we\nadopt the stop-gradient operation [9] to the second network\nbranch F ′(·|Θ′) when using the instance level loss LI in\nEq. (1) to perform back propagation. Thus, the parameters\nΘ′ in the second network branch are updated only with the\nintra-views cluster-level contrastive loss L(intra)\nC\nin Eq. (4).\nRemark 3. For clarity, we summarize the details of the train-\ning procedure in Algorithm 1. We note that the “asymmetry” in\nthe proposed framework for cluster-guided contrastive learning\nlies in following three aspects: a) asymmetry in network\nstructure, i.e., a predictor layer is only added after the ﬁrst\nbranch3; and b) asymmetry in data augmentation, i.e., the\naugmented samples provided to the second branch are further\ntransformed into gray-scale; c) asymmetry in pseudo labels\ngeneration, i.e., the output features of the ﬁrst branch are\nused to generate pseudo labels which are shared with the\nsecond branch. Because of the asymmetry in the three aspects\nmentioned above, we term the proposed framework as Cluster-\nguided Asymmetric Contrastive Learning (CACL).\nRemark 4. There have been many unsupervised Re-ID meth-\nods [17], [13], [29], [12] used the contrastive learning to learn\ndiscriminant features. Most of them [13], [29], [12] are Gener-\native Adversarial Networks (GANs)-based methods and need\nadditional supervised information to assist the training. For\nexample, ATNet [13] trains multiple GANs through utilizing\nillumination and camera information, GCL [12] introduces\nthe pose information in training, and AD-cluster [29] uses\ngenerating cross-camera samples to assist the training. Un-\nlike these methods, our proposed CACL uses an asymmetric\nSiamese network to effectively learn ﬁne-grained features by\nsuppressing color with simple data augmentation operations\nduring the training, rather than using an expensive sample\ngeneration via GANs. Compared to GANs based methods, our\nCACL is simple, efﬁcient and effective.\nD. Inference Procedure for CACL\nAfter training, we keep only the ResNet F(·|Θ) in the ﬁrst\nbranch for inference in testing.\nTo be speciﬁc, in the inference procedure, we use the output\nfeatures X of the ﬁrst branch F(·|Θ) to calculate the similarity\nbetween images. Given the query image dataset Ig = {Ig\ni }N g\ni=1\nand the query image dataset Iq = {Iq\ni }Nq\ni=1, where N g and\nN q are the sizes of the two datasets, respectively. For each\nimage Ig\ni in the query, we compute the distances between the\nquery image and the images in the gallery Iq via the feature\n2Note that it is not necessary to use the stop-gradient operation in our\nCACL because the clustering result provides enough guide information under\nthe asymmetric structure to prevent collapse. Although this is similar to the\nmethod in SimSiam [9], the purpose is different and it is not necessary to use\nin our proposal.\n3It is also feasible to add another predictor layer after the second branch\nto have a symmetric network structure. Nevertheless, our experimental results\nshow that merely marginal performance improvement can be yielded after\nadding an extra predictor layer. Thus, we prefer to use the asymmetric network\narchitecture for the contrastive learning framework.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n6\nAlgorithm 1 Training Procedure for CACL\nInput: Given a dataset I = {Ii}N\ni=1.\nOutput:\n1: Pre-train the two network branches on ImageNet.\n2: Initialize the two instance memory banks M and ˜\nM and\nset P = Pbest = 0.\n3: while epoch ≤total epoch do\n4:\nGenerate ˆIi and ˜Ii via data augmentation T (·) and\nG(T ′(·));\n5:\nPerform feature extraction to get xi and ˜xi;\n6:\nPerform clustering and clustering reﬁnement via Eq. (9)\nto yield pseudo label Y = {y1, · · · , yN};\n7:\nUpdate the two cluster centers U and ˜U via Eq. (7);\n8:\nTrain siamese network, i.e., updatomg Θ, Ψ and Θ′ via\nthe total loss in Eq. (10);\n9:\nUpdate instance memory bank M and\n˜\nM via Eq. (11)\nand Eq. (12);\n10:\nEvaluate the model performance P with F(·|Θ);\n11:\nif P > Pbest then\n12:\nOutput the best model F(·|Θ) and set Pbest ←P;\n13:\nend if\n14: end while\nobtained from the output of the ﬁrst branch. And then, we sort\nthe distance in ascending order to ﬁnd the matched images.\nIV. EXPERIMENTS\nIn this section, we describe the used benchmark datasets\nand the detailed parameter settings in experiments at ﬁrst,\nand then provide extensive experiments on these datasets,\nincluding a set of detailed ablation study and a set of evaluation\nexperiments to show the effect of each component. Finally, we\ngive a set of data visualization experiments. 4\nA. Dataset Description\nTo evaluate the effectiveness of our proposal, we use\nthe following three benchmark datasets: Market-1501 [41],\nDukeMTMC-ReID [45] and MSMT17 [46].\nMarket-1501 has 32,668 photos of 1501 people from six\ndifferent camera views. The training set contains 12,936 of\n751 identities. The testing set contains 19,732 images of 750\nidentities.\nDukeMTMC-ReID consists of images sampling from\nDukeMTMC-ReID video dataset, 120 frames per video, with\na total of 36,411 images of people of 1404 identities. The\ntraining set contains 16,522 images of 702 identities and the\ntesting set contains 2228 query images of 702 identities and\n17,661 gallery images. These images are taken from eight\ncameras.\nMSMT17 has a total of 126,441 images under 15 camera\nviews. The training set contains 32,621 images of 1041 identi-\nties. The testing set contains 93,820 images of 3060 identities\nare used for testing. MSMT17 is larger than Market-1501 and\nDukeMTMC-ReID.\n4The code can be downloaded from https://github.com/MingkunLishigure/\nCACL.\nB. Implementation Details\nSettings for Training. In our CACL approach, we use\nResNet-50 [28] pre-trained on ImageNet [11] for both network\nbranches.5 The feature outputs xi ∈RD and ˜xi ∈RD of the\ntwo networks F(·|Θ) and F(·|Θ′) are D-dimensional vectors\nwhere D = 2048. We use the features output xi of the ﬁrst\nbranch F(·|Θ) to perform clustering, where xi = F(ˆIi|Θ) ∈\nRD.\nThe prediction layer G(·) is a D × D full connection layer.\nWe initialize the two memory banks with the outputs of the\nfeature from the corresponding network branches F(·|Θ) and\nF ′(·|Θ′), respectively. We optimize the network through Adam\noptimizer [47] with a weight decay of 0.0005 and train the\nnetwork with 80 epochs in total. The learning rate is initially\nset as 0.00035 and decreased to one-tenth per 20 epochs. The\nbatch size is set to 64. The temperature coefﬁcient τ in Eq. (6)\nis set to 0.05 and the update factor α in Eqs. (11) and (12) is\nset to 0.2.\nSettings for Data Augmentation. In our experiments, we use\nthe same data augmentation operations as other methods [17],\n[2], including random horizontal ﬂip, random erasing and\nrandom crop, to deﬁne data augmentation T (·) and T ′(·).\nBesides, we add a gray-scale transform to the input of the\nsecond branch.\nMetrics for Performance Evaluation. In evaluation, we use\nthe mean average precision (mAP) and cumulative matching\ncharacteristic (CMC) at Rank-1, 5, 10 to evaluate the perfor-\nmance.\nC. Comparison to the State-of-the-art Methods\nWe compare our proposed CACL to the state-of-the-art\nunsupervised domain adaptation methods and purely unsu-\npervised methods for person Re-ID. The purely unsuper-\nvised methods for person Re-ID include: CAMEL [40],\nPUL [19], SSL [20], LOMO [42], BOW [41], BUC [18],\nHCT [4], SpCL [17], and CAP [43]. The unsupervised\ndomain adaptation methods for person Re-ID include: PT-\nGAN [30], ADTC [36], HHL [35], SSG [5], MMCL [23],\nAD-Cluster [29], MEB [38], NRMT [39], SPGAN [32], TJ-\nAIDL [16], JVTC [37], PGPPM [34], and MMT [2].\nThe comparison results of the state-of-the-art unsupervised\ndomain adaptation methods and purely unsupervised methods\nare shown in Table I. We can ﬁnd that our proposed CACL\nachieves 80.9/92.7% at mAP/Rank-1 on Market-1501 and\n69.6/82.6% at mAP/Rank-1 on DukeMTMC-ReID, respec-\ntively. It can be found that CACL not only performs better\nthan all pure unsupervised methods but also achieves the best\nperformance than unsupervised domain adaptation methods.\nMoreover, we also conduct experiments on a much larger\ndataset MSMT17 and report the experimental results in Table\nII. Again, we can observe that our proposed CACL achieves\na leading performance, i.e., 23.0/48.4% at mAP/Rank-1. It is\nworth to note that our CACL yields superior performance than\nsome UDA methods on this challenging dataset. These results\nconﬁrm the effectiveness of our proposal.\n5In Section IV-C, we also provide the performance evaluation with other\nbackbone networks for the two branches.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n7\nTABLE I\nCOMPARISON TO OTHER STATE-OF-THE-ART METHODS. ’UDA’ IS TO REFER THE UNSUPERVISED DOMAIN ADAPTATION METHODS AND ’US’ IS TO\nREFER THE PURELY UNSUPERVISED LEARNING METHODS. ’*’ MEANS THAT THE USED BACKBONE IS PRE-TRAINED ON IMAGENET.\nMethod\nType\nReference\nBakcbone\nMarket-1501\nDukeMTMC-ReID\nmAP\nRank-1\nRank-5\nRank-10\nmAP\nRank-1\nRank-5\nRank-10\nPTGAN [30]\nUDA\nCVPR’18\nGoogleNet [31]\n15.7\n38.6\n57.3\n-\n13.5\n27.4\n43.6\n-\nSPGAN [32]\nUDA\nCVPR’18\nResNet50* [28]\n26.7\n58.1\n76.0\n82.7\n26.4\n46.9\n62.6\n68.5\nTJ-AIDL [16]\nUDA\nCVPR’18\nMobileNet* [33]\n26.5\n58.2\n74.8\n-\n23.0\n44.3\n59.6\n-\nPGPPM [34]\nUDA\nCVPR’18\nResNet50* [28]\n33.9\n63.9\n81.1\n86.4\n17.9\n36.3\n54.0\n61.6\nHHL [35]\nUDA\nECCV’18\nResNet50* [28]\n31.4\n62.2\n78.0\n84.0\n27.2\n46.9\n61.0\n66.7\nSSG [5]\nUDA\nECCV’19\nResNet50* [28]\n58.3\n80.0\n90.0\n92.4\n53.4\n73.0\n80.6\n83.2\nAD-cluster [29]\nUDA\nCVPR’20\nResNet50* [28]\n68.3\n86.7\n94.4\n96.5\n54.1\n72.6\n82.5\n85.5\nADTC [36]\nUDA\nECCV’20\nResNet50* [28]\n59.7\n79.3\n90.8\n94.1\n52.5\n71.9\n84.1\n87.5\nMMCL [23]\nUDA\nCVPR’20\nResNet50* [28]\n60.4\n84.4\n92.8\n95.0\n51.4\n72.4\n82.9\n85.0\nMMT [2]\nUDA\nICLR’20\nResNet50* [28]\n73.8\n89.5\n96.0\n97.6\n62.3\n76.3\n87.7\n91.2\nJVTC [37]\nUDA\nECCV’20\nResNet50* [28]\n67.2\n86.8\n95.2\n97.1\n66.5\n80.4\n89.9\n93.7\nMEB [38]\nUDA\nECCV’20\nResNet50* [28]\n76.0\n89.9\n95.2\n96.9\n65.3\n81.2\n90.9\n92.2\nNRMT [39]\nUDA\nECCV’20\nResNet50* [28]\n71.7\n87.8\n94.6\n96.5\n62.2\n77.8\n86.9\n89.5\nSpCL [17]\nUDA\nNIPS’20\nResNet50* [28]\n76.7\n90.3\n96.2\n97.7\n68.8\n82.9\n90.1\n92.5\nCAMEL [40]\nUS\nICCV’17\nResNet50* [28]\n26.3\n54.4\n73.1\n79.6\n19.8\n40.2\n57.5\n64.9\nBow [41]\nUS\nICCV’15\n-\n14.8\n35.8\n52.4\n60.3\n8.5\n17.1\n28.8\n34.9\nPUL [19]\nUS\nTOMM’18\nResNet50* [28]\n22.8\n51.5\n70.1\n76.8\n22.3\n41.1\n46.6\n63.0\nLOMO [42]\nUS\nCVPR’15\n-\n8.0\n27.2\n41.6\n49.1\n4.8\n12.3\n21.3\n26.6\nBUC [18]\nUS\nAAAI’19\nResNet50* [28]\n30.6\n61.0\n71.6\n76.4\n21.9\n40.2\n52.7\n57.4\nHCT [4]\nUS\nCVPR’20\nResNet50* [28]\n56.4\n80.0\n91.6\n95.2\n50.1\n69.6\n83.4\n87.4\nSSL [20]\nUS\nCVPR’20\nResNet50* [28]\n37.8\n71.7\n83.8\n87.4\n28.6\n52.5\n63.5\n68.9\nSpCL [17]\nUS\nNIPS’20\nResNet50* [28]\n73.1\n88.1\n96.3\n97.7\n65.3\n81.2\n90.3\n92.2\nCAP [43]\nUS\nAAAI’20\nResNet50* [28]\n79.2\n91.4\n96.3\n97.7\n67.3\n81.1\n89.3\n91.8\nCACL\nUS\nThis paper\nResNet50* [28]\n80.9\n92.7\n97.4\n98.5\n69.6\n82.6\n91.2\n93.8\nCACL\nUS\nThis paper\nIBN-ResNet* [44]\n83.6\n93.3\n97.7\n98.3\n72.5\n85.5\n92.9\n94.9\nTABLE II\nEXPERIMENTAL RESULTS ON MSMT17.\nMethod\nType\nReference\nMSMT17\nmAP Rank-1 Rank-5 Rank-10\nPTGAN [30]\nUDA\nCVPR’18\n3.3\n11.8\n-\n27.4\nECN [48]\nUDA\nCVPR’19\n10.2\n30.2\n41.5\n46.8\nSSG [5]\nUDA\nICCV’19\n13.3\n32.2\n-\n51.2\nMMCL [23]\nUDA\nCVPR’20\n16.2\n43.6\n54.3\n58.9\nJVTC+ [37]\nUS\nECCV’20\n17.3\n43.1\n53.8\n59.4\nSpCL [17]\nUS\nNIPS’20\n19.1\n42.3\n55.6\n61.2\nMMT [2]\nUDA\nICLR’20\n24.0\n50.1\n63.5\n69.3\nSpCL [17]\nUDA\nNIPS’20\n26.8\n53.7\n79.3\n83.1\nCACL\nUS\nThis paper 23.0\n48.9\n61.2\n66.4\nCACL w/ IBN-ResNet\nUS\nThis paper 29.9\n57.1\n68.4\n73.1\nNote that Instance-Batch Normalization (IBN) [44] has been\nused in object recognition and has been proved very effective.\nHere, we evaluate our CACL, in which the backbone is im-\nplemented with Instance-Batch Normalization ResNet (IBN-\nResNet). Similar to CACL with ResNet [28], we introduce\nan Instance-Batch Normalization (IBN) layer to replace the\nBN layer and call it an IBN-ResNet. As shown in Table I,\nthe performance of our CACL can be further improved when\ncombining with IBN-ResNet.\nD. Ablation Study\nTo evaluate the effectiveness of each component: LI,\nL(inter)\nC\n, L(intra)\nC\nand clustering with reﬁnement in our CACL\napproach, we conduct a set of ablation experiments on Market-\n1501 and DukeMTMC-ReID.\nIn the baseline method, we train both branches with data\naugmentation T ′(·) and T ′(·) by using the Non-Parametric\nSoftmax loss [49], which is deﬁned as\nL(xi) = −ln(\nexp(u⊤\nω(Ii)xi/τ)\nPm′\nℓ=1 exp(u⊤\nℓxi/τ)\n),\n(13)\nand both the training process and the memory updating strat-\negy in the baseline method are kept the same as our CACL\nmethod.\nTo comprehensively evaluate the contribution of each com-\nponent, we conduct a set of ablation experiments by test-\ning each component in our CACL framework individually,\ni.e., cluster reﬁnement, instance-level contrastive loss LI and\ncluster-level contrastive loss LC. To further evaluate the sub-\npart of the cluster-level contrastive loss, we also conduct\nexperiments to evaluate the inﬂuence of using L(inter)\nC\nor\nL(intra)\nC\n, separately.\nIn the ablation experiments, to test the model with con-\ntrastive loss LC or LI, we train both branches with data aug-\nmentation T ′(·) and G(T ′(·)), respectively. To test the model\nperformance with the cluster-level contrastive loss LC and the\nsub-part of L(intra)\nC\n, compared to the baseline method, we\nneed to replace the Non-Parametric Softmax loss in Eq. (13)\nby the loss in Eq. (4) for both branches. The results of the\nablation study are reported in Table III.\nAs can be read in Table III, the performance improves when\neach component is used individually. This validates that each\ncomponent contributes to the performance improvements. For\nthe experiments of using both LC and LI, it does not signif-\nicantly better than just using LI, and in the experiments of\nusing LC we observe a slight improvement than the baseline.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n8\nTABLE III\nABLATION STUDY ON MARKET-1501 AND DUKEMTMC-REID.\nComponents\nCluster Reﬁne\nLI\nLintra\nC\nLinter\nC\nMarket-1501\nDukeMTMC-ReID\nmAP Rank-1 Rank-5 Rank-10 mAP Rank-1 Rank-5 Rank-10\nBaseline\n68.1\n85.2\n94.0\n96.0\n62.5\n78.5\n88.5\n90.3\n+ LC\n✓\n✓\n70.8\n87.5\n94.4\n96\n62.5\n79.5\n88.4\n90.8\n+ LI\n✓\n74.7\n88.7\n95\n96.6\n64.2\n80.7\n89\n91.6\n+ LI + LC\n✓\n✓\n✓\n74.4\n89.3\n95.9\n96.7\n63.8\n79.2\n89.2\n91.7\n+ Cluster Reﬁne\n✓\n73\n87.8\n95.7\n97.2\n65.7\n81.1\n90.6\n93.2\n+ Cluster Reﬁne + LI\n✓\n✓\n78.2\n91.2\n97\n98.1\n67.6\n81.8\n90.2\n93\n+ Cluster Reﬁne + LI +Linter\nC\n✓\n✓\n✓\n78.7\n91.2\n97\n97.9\n68.5\n81.9\n91.2\n93.8\n+ Cluster Reﬁne + LI +Lintra\nC\n✓\n✓\n✓\n79.2\n91.9\n96.7\n98\n68.3\n82.1\n90.3\n93.2\n+ Cluster Reﬁne + LC\n✓\n✓\n✓\n80.4\n92.2\n97.1\n98.2\n68.8\n82.2\n91.3\n93.8\nOur CACL\n✓\n✓\n✓\n✓\n80.9\n92.7\n97.4\n98.5\n69.6\n82.6\n91.2\n93.8\nThis is because the clustering result is not high quality and\nusing LC will make the training pay more attention to the\nnoisy cluster information. Therefore, it might bring misleading\ninformation to the network training. In the experiments of\nusing both LC and cluster reﬁnement, we observe signiﬁcant\nperformance improvement than using the cluster reﬁnement\nalone. This also validates that the cluster reﬁnement improves\nthe clustering result and the reﬁned clustering information can\nfurther enhance the effectiveness of using LC to train the\nnetwork.\nE. More Evaluation and Analysis\nEvaluation on Importance of Cluster-Guided. We use an\ninstance-level contrastive loss in our method to mine the invari-\nance between different augment views based on SimSiam [9].\nTo verify whether the clustering guidance is vital in the\ncontrast learning framework, we train our CACL framework\nbut just using the instance-level contrastive loss in Eq. (1)\nwithout the clustering guidance. The experimental results are\nshown in Table IV. As can be read from Table IV, surprisingly,\nthe contrastive learning framework without clustering guidance\ndid not work at all.\nTABLE IV\nABLATION STUDY ON MARKET-1501.\nComponents\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\nCACL w/o clustering\n0.3\n0.5\n1.2\n2.3\nCACL w/o stopGrad\n80.2\n92.0\n97.0\n97.6\nCACL\n80.9\n92.7\n97.4\n98.5\nImprovements Brought by Suppressing Colors. To suppress\ncolors inﬂuence, CACL uses a gray-scale process G(·) over\nthe data augmentation T ′(·) for the second network branch.\nTo validate the effectiveness of suppressing colors, we conduct\na set of experiments under different settings: a) simply using\ndata augmentation T ′(·) with raw color; b) using another data\naugmentation approach, named “color-jitter”, which denoted\nas J (·) to replace G(·), which output is still a color image; c)\nwith gray-scale transform G(·) after T ′(·). It should be empha-\nsized that in the implementation, the “color-jitter” operation\nwill give random amplitude values to the image changing.\nWe display the image samples processed with different data\nFig. 4.\nIllustration for the raw images and the augmented images. The 1st\nrow: “raw images”. The 2nd row: “color-jitter”. Bottom row: “gray-scale”.\nTABLE V\nPERFORMANCE COMPARISON ON USING COLOR DATA AUGMENTATIONS\nAND GRAY-SCALE TRANSFORM TO THE SECOND NETWORK BRANCH.\nComponents\nCluster Reﬁne\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\nT ′(·)\n70.3\n87.4\n94.6\n96.5\nJ (T ′(·))\n72.5\n87.8\n95.3\n96.9\nG(T ′(·))\n74.4\n89.3\n95.9\n96.7\nT ′(·)\n✓\n79.0\n90.6\n96.3\n97.1\nJ (T ′(·))\n✓\n79.1\n90.8\n96.7\n97.8\nG(T ′(·))\n✓\n80.9\n92.7\n97.4\n98.5\naugmentation methods in Fig. 4. As can be observed, “color-\njitter” did change the image, but the color information still\ndominates.\nExperimental results are provided in Table V. We can\nread that using “color-jitter” J (·) yields some performance\nimprovement, but using “gray-scale” G(·) yield the best\nperformance improvement. When combined with the cluster\nreﬁnement step, we can observe the similar result that: using\n“gray-scale” G(·) yields better performance improvement than\nusing “color-jitter” J (·). These results validate that suppress-\ning colors is effective to gain performance improvement.\nCompared to using “gray-scale”, using “color-jitter” does not\ntruly eliminate the inﬂuence brought by colors, that is to say,\nafter using color-jitter, the color information still dominates.\nTo further reveal the mechanisms why using “gray-scale”\nworks better than using “color-jitter” in the proposed frame-\nwork, we show the statistic histograms of color distributions\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n9\n(a) Raw Images\n(b) Color-Jitter\n(c) Gray-Scale\nFig. 5.\nComparison on distributions in histogram of intensity in RGB channels under different data augmentation operations.\nof using raw image, color-jitter, and gray-scale, respectively.\nSpeciﬁcally, we compute the statistical histograms of the\nintensity values in the RGB channels of the raw color images\nand the images after using “color-jitter” and “gray-scale” with\n500 images sampled at random in the training data from\nMarket-1501. The statistical results are shown in Fig. 5.\nWe can observe that: using “gray-scale” yields roughly\nconsistent distribution in the histogram compared to the raw\nimages; whereas using the distribution in the histogram of the\nimages after using “color-jitter” has some notable deviations\nfrom that of the raw images. In the histogram of using\n“gray-scale”, the proportion of the pixels at the two extreme\nvalues (i.e., 0 and 255) are signiﬁcantly reduced; whereas\nin the histogram of using “color-jitter”, the proportion of\nthe pixels at the two extreme values, especially at 0, are\nsigniﬁcantly magniﬁed—this phenomenon might damage the\ncontent consistency with the raw image. The difference in the\nconsistency of the histogram reveals the essential advantage of\nusing “gray-scale” to suppress the inﬂuence of colors, rather\nthan using “color-jitter”.\nEvaluation on Parameters in DBSCAN. We conduct ex-\nperiments evaluate the parameter d to ﬁnd the neighbors. In\ncluster reﬁnement, we use DBSCAN with a smaller parameter\nd′, where d′ := d −δ to ﬁnd the over-segmentation. We\nconduct experiments on Market-1501 to evaluate the effects\nof changing the two parameters. Experiments are recorded\nin Table VI. we can ﬁnd that while the change of d will\naffect the baseline performance, our CACL still improves the\nmodel performance signiﬁcantly. Note that even though the\nbaseline performance will sharply drop when using d = 0.7,\nour method can also achieve a good performance which is also\nhigher than other unsupervised methods in Table I.\nThe cluster reﬁnement is an important component in our\nproposed CACL, and δ is an important parameter to ﬁnd the\nover-segmentation of the raw clusters. Thus, we further con-\nduct experiments to evaluate the performance of using different\nvalues of δ. Experimental results are shown in Table VII.\nWe can ﬁnd that the performance is not too sensitive to δ.\nWhen using δ = 0.02, the performance achieves the best, i.e.,\n80.9/92.7% at mAP/Rank-1 on Market-1501 and 69.6/82.6%\nat mAP/Rank-1 on DukeMTMC-ReID.\nMoreover, we also test the stop-gradient operations under\nTABLE VI\nPERFORMANCE COMPARISON OF DIFFERENT CLUSTER PARAMETER d\n(THE MAXIMUM DISTANCE BETWEEN NEIGHBOR POINTS) ON CACL AND\nBASELINE METHOD.\nd\nMarket-1501\nDukeMTMC-ReID\nBaseline\nCACL\nBaseline\nCACL\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\n0.4\n68.6\n85.9\n75.2\n91.4\n60.1\n77.5\n62.0\n77.7\n0.5\n71.2\n86.5\n81.6\n93.0\n63.4\n80.3\n67.5\n81.8\n0.6\n68.1\n85.2\n80.9\n92.7\n62.5\n78.5\n69.6\n82.6\n0.7\n43.8\n71.5\n75.8\n90.1\n4.1\n10.3\n66.7\n80.6\nTABLE VII\nILLUSTRATION FOR THE MODEL PERFORMANCE WITH DIFFERENT δ ON\nMARKET-1501.\nδ\nMarket-1501\nd = 0.4\nd = 0.5\nd = 0.6\nd = 0.7\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nRank-1\n0.02\n75.2\n91.4\n81.6\n93.0\n80.9\n92.7\n75.8\n90.1\n0.04\n70.8\n89.5\n80.4\n92.6\n80.3\n92.3\n68.7\n86.2\n0.06\n65.8\n87.2\n77.7\n91.7\n79.0\n91.4\n8.20\n20.3\n0.08\n64.3\n86.2\n76.6\n91.2\n78.5\n91.3\n6.10\n15.6\ndifferent structures. In Table IV, as can be read that, the\nperformance of the framework with asymmetric structure\ndrops slightly (i.e., only 0.7% lower than that of using the\nstop-gradient operation) when the stop-gradient operation is\nnot used. This hints that the framework with asymmetric\nstructure in CACL does not highly depend on the stop-gradient\noperation.\nEvaluation Performance of Two Branches. To further re-\nveal the performance of the trained networks, we record the\nperformance of using the output features of each branch of\ntwo networks F(·|Θ) and F ′(·|Θ′), separately, for person Re-\nID in Table VIII. We can read that using the output features\nof the second branch F ′(·|Θ′) did yield signiﬁcantly lower\nperformance than that of using the output feature of the ﬁrst\nbranch F(·|Θ), and the result of using F ′(·|Θ′) is similar to\nthe result of the experiments without using L(intra)\nC\n. This is\nbecause the second network branch pays attention to learning\nfeatures from gray-scale images, lacking of the ability to\ncapture richer information from color images.\nEvaluation on Memory Update Parameter α. We conduct\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n10\nTABLE VIII\nPERFORMANCE COMPARISON ON F(·|Θ) AND F ′(·|Θ′).\nBranch\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\nF(·|Θ) (Color)\n80.9\n92.7\n97.4\n98.5\nF ′(·|Θ′) (Gray-Scale)\n43.8\n71.5\n83.9\n87.1\nTABLE IX\nPERFORMANCE COMPARISON ON DIFFERENT α.\nBranch\nMarket-1501\nmAP\nRank-1\nRank-5\nRank-10\n0.0\n75.1\n89.8\n96.3\n97.3\n0.2\n80.9\n92.7\n97.4\n98.5\n0.4\n80.8\n92.5\n97.1\n98.2\n0.6\n80.2\n92.4\n97.2\n98.3\n0.8\n77.3\n90.9\n96.6\n98.0\n1.0\n4.3\n10.9\n19.9\n24.9\nexperiments to evaluate the effects of the memory update\nparameter α and show the results in Table IX. We can ﬁnd\nthat our CACL is not sensitive to the changing of memory\nupdate parameter α, except for α = 1. When using α = 1, the\nmodel performance signiﬁcantly drops because the memory\nbank has not been updated at this time. When using α = 0.2\nthe model achieves the best performance on Market-1501, i.e.,\n80.9/92.7% at mAP/Rank-1.\nEvaluation on Performance with Ground-truth Labels.\nWe compare our CACL to the baseline method with the\nground-truth labels (i.e., in supervised setting). The results\nare shown in Table X. We can ﬁnd that CACL could achieve\ngood performance under unsupervised setting, which is merely\nlower 3/1.1% at mAP/Rank-1 than the baseline method, which\nis trained with the ground-truth labels on Market-1501. More-\nover, if we provide ground-truth labels to train our CACL (i.e.,\nCACL+labels), notable improvements in performance than the\nsupervised baseline method can be observed.\nF. Data Visualization\nTo gain some intuitive understanding of the performance of\nour proposed CACL, we conduct a set of data visualization\nexperiments on Market-1501 to visualize the clustering results\nof the learned features when different training strategies are\nused: a) without using the contrastive loss LC + LI; and b)\nusing the contrastive losses LC + LI.\nTABLE X\nPERFORMANCE COMPARISON TO BASELINE METHOD IN SUPERVISED\nSETTING. “BASELINE + LABELS” MEANS THAT WE USE THE\nGROUND-TRUTH LABELS TO TRAIN THE BASELINE METHOD; WHEREAS\n“CACL + LABELS” MEANS THAT WE USE THE GROUND-TRUTH LABELS\nTO TRAIN OUR CACL.\nMethod\nMarket-1501\nDukeMTMC-ReID\nmAP\nRank-1\nmAP\nRank-1\nCACL\n80.9\n92.7\n69.6\n82.6\nBaseline + labels\n83.9\n93.6\n73.3\n86.6\nCACL + labels\n85.7\n94.2\n74.9\n87.2\nExperimental results are shown in Fig. 6. We can observe\nthat the contrastive loss LC + LI did help the model dis-\ntinguish those similar images while maintaining the cluster\ncompactness, and also separate the overlapping individual\nsamples from each other. This conﬁrms the effectiveness of\nour proposed approach, and it also shows that our approach\ncan attenuate the inﬂuence of clothing color.\nAt the same time, we also selected some query samples\nwith the top-10 best matching images in the gallery set and\nshow them in Fig. 7. Compared to the baseline model, our\napproach returns more accurate results. We can ﬁnd that\nmost of the wrong samples matched by the baseline model\nare dressed in the same color with the query sample. These\nresults suggest that our approach can effectively ignore the\ninterference caused by samples with similar colors and thus\nﬁnd more accurate matches.\nV. CONCLUSION\nWe have proposed a Cluster-guided Asymmetric Contrastive\nLearning (CACL) approach for unsupervised person Re-ID,\nin which cluster information is leveraged to guide the feature\nlearning in a properly designed contrastive learning frame-\nwork. Speciﬁcally, in our proposed CACL, instance-level con-\ntrastive learning is conducted with respect to the asymmetric\ndata augmentation and cluster-level contrastive learning is\nconducted with respect to the reﬁned clustering result. By\nleveraging the reﬁned cluster result into contrastive learning,\nCACL is able to effectively exploit the invariance within and\nbetween different data augmentation views for learning more\neffective features beyond the dominating colors. In addition,\nwe conﬁrmed that reﬁned clustering result could help our\nCACL approach mine invariant information more effectively\nat the cluster level. We have conducted extensive experiments\non three benchmark datasets and demonstrated the superior\nperformance of our proposal.\nAs the future work, it is interesting and promising to\nincorporate attention mechanism (e.g., [50], [51]), clustering\nensemble and hybrid contrastive learning strategy (e.g., [52])\nor side information in dataset (e.g., [12]) to further enrich\nthe representation capacity, improve the stability and enhance\nthe overall performance of the proposed framework. What’s\nmore, in other related ﬁelds, such as face recognition or\nvehicle re identiﬁcation (e.g., [53], [54]), whether suppresses\nthe dominating color can also bring positive inﬂuence is a very\ninteresting and worth exploring direction.\nREFERENCES\n[1] L. Zheng, Y. Yang, and A. G. Hauptmann, “Person re-identiﬁcation:\nPast, present and future,” arXiv preprint arXiv:1610.02984, 2016. 1, 2\n[2] Y. Ge, D. Chen, and H. Li, “Mutual mean-teaching: Pseudo label\nreﬁnery for unsupervised domain adaptation on person re-identiﬁcation,”\nin International Conference on Learning Representations, 2020. 1, 6, 7\n[3] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, “A density-based algorithm\nfor discovering clusters in large spatial databases with noise,” in Second\nInternational Conference on Knowledge Discovery and Data Mining,\n1996, p. 226–231. 1, 5\n[4] K. Zeng, M. Ning, Y. Wang, and Y. Guo, “Hierarchical clustering with\nhard-batch triplet loss for person re-identiﬁcation,” in IEEE Conference\non Computer Vision and Pattern Recognition, 2020, pp. 13 657–13 665.\n1, 2, 6, 7\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n11\nCACL (Without ℒ! and ℒ\")\nCACL (Full)\nFig. 6. Data Visualization via t-SNE of the learned feature and clusters under two different training strategies: Training without LC and LI (left) as mentioned\nin Table III and our CACL (right). The data points come from the Market-1501 training set (1,000 images of 60 identities). The points with the same color\nmean the image of the same identity. To demonstrate the difference between the two distributions in detail, we further zoom in on the circled clusters and\nshow the corresponding images. The images in the boxes are similar to each other and the corresponding data points are very close to each other or even\noverlapping in the feature space if the model is trained without using LC and LI, as shown in the left box; whereas using the contrastive losses LC and LI\nwill effectively distinguish these data points and maintain the cluster compactness as shown in the right box.\nBaseline\nCACL\nQuery\n1st\n10th\n1st\n10th\nFig. 7. Visualization of the top-10 best matched images. We show the top-10 best matching samples in the gallery set for the query sample with the baseline\nmethod and our proposed CACL. The images with frames in green and in red are the correctly matched images and mismatched images, respectively.\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n12\n[5] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, and T. S. Huang, “Self-\nsimilarity grouping: A simple unsupervised cross domain adaptation\napproach for person re-identiﬁcation,” in The IEEE International Con-\nference on Computer Vision, October 2019, pp. 6112–6121. 1, 6, 7\n[6] J. Xie, X. Zhan, Z. Liu, Y. S. Ong, and C. C. Loy, “Delving into inter-\nimage invariance for unsupervised visual representations,” in Conference\nand Workshop on Neural Information Processing Systems, 2020. 1, 3\n[7] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,\n“Unsupervised learning of visual features by contrasting cluster assign-\nments,” Advances in Neural Information Processing Systems, pp. 9912–\n9924, 2020. 1, 3\n[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” in International\nConference on Machine Learning, 2020, pp. 1597–1607. 1, 3, 4\n[9] X. Chen and K. He, “Exploring simple siamese representation learning,”\nin IEEE Conference on Computer Vision and Pattern Recognition, 2021,\npp. 15 750–15 758. 1, 3, 5, 8\n[10] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, B. Piot,\nk. kavukcuoglu, R. Munos, and M. Valko, “Bootstrap your own latent\n- a new approach to self-supervised learning,” in Advances in Neural\nInformation Processing Systems, 2020, pp. 21 271–21 284. 1, 3, 4\n[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Conference and Workshop\non Neural Information Processing Systems, 2012, pp. 1097–1105. 1, 4,\n6\n[12] H. Chen, Y. Wang, B. Lagadec, A. Dantcheva, and F. Bremond,\n“Joint generative and contrastive learning for unsupervised person re-\nidentiﬁcation,” in IEEE Conference on Computer Vision and Pattern\nRecognition, June 2021, pp. 2004–2013. 2, 5, 10\n[13] J. Liu, Z.-J. Zha, D. Chen, R. Hong, and M. Wang, “Adaptive transfer\nnetwork for cross-domain person re-identiﬁcation,” in IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 7202–7211. 2,\n5\n[14] S. Bak, P. Carr, and J.-F. Lalonde, “Domain adaptation through synthesis\nfor unsupervised person re-identiﬁcation,” in European Conference on\nComputer Vision, 2018, pp. 189–205. 2\n[15] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang, and\nY. Tian, “Unsupervised cross-dataset transfer learning for person re-\nidentiﬁcation,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 1306–1315. 2\n[16] J. Wang, X. Zhu, S. Gong, and W. Li, “Transferable joint attribute-\nidentity deep learning for unsupervised person re-identiﬁcation,” in IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n2275–2284. 2, 6, 7\n[17] Y. Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, “Self-paced contrastive\nlearning with hybrid memory for domain adaptive object re-id,” in\nAdvances in Neural Information Processing Systems, 2020, pp. 11 309–\n11 321. 2, 5, 6, 7\n[18] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, “A bottom-up clustering\napproach to unsupervised person re-identiﬁcation,” in The Association\nfor the Advancement of Artiﬁcial Intelligence, vol. 33, 2019, pp. 8738–\n8745. 2, 6, 7\n[19] H. Fan, L. Zheng, C. Yan, and Y. Yang, “Unsupervised person re-\nidentiﬁcation: Clustering and ﬁne-tuning,” ACM Transactions on Mul-\ntimedia Computing, Communications, and Applications, vol. 14, no. 4,\np. 83, 2018. 2, 6, 7\n[20] Y. Lin, L. Xie, Y. Wu, C. Yan, and Q. Tian, “Unsupervised person re-\nidentiﬁcation via softened similarity learning,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 3390–3399. 2, 6,\n7\n[21] B. Sun, J. Feng, and K. Saenko, “Return of frustratingly easy domain\nadaptation,” in Association for the Advancement of Artiﬁcial Intelligence,\nvol. 30, 2016. 2\n[22] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by\nbackpropagation,” in International Conference on Machine Learning,\n2015, pp. 1180–1189. 2\n[23] D. Wang and S. Zhang, “Unsupervised person re-identiﬁcation via\nmulti-label classiﬁcation,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2020, pp. 10 981–10 990. 2, 6, 7\n[24] P. Bojanowski and A. Joulin, “Unsupervised learning by predicting\nnoise,” in International Conference on Machine Learning.\nPMLR,\n2017, pp. 517–526. 3\n[25] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, and\nT. Brox, “Discriminative unsupervised feature learning with exemplar\nconvolutional neural networks,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 38, no. 9, pp. 1734–1747, 2015. 3\n[26] Y. Li, P. Hu, Z. Liu, D. Peng, J. T. Zhou, and X. Peng, “Contrastive\nclustering,” in AAAI Conference on Artiﬁcial Intelligence, 2021. 3\n[27] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast\nfor unsupervised visual representation learning,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 9729–9738. 3\n[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 770–778. 3, 5, 6, 7\n[29] Y. Zhai, S. Lu, Q. Ye, X. Shan, J. Chen, R. Ji, and Y. Tian, “Ad-\ncluster: Augmented discriminative clustering for domain adaptive person\nre-identiﬁcation,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 9021–9030. 5, 6, 7\n[30] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to\nbridge domain gap for person re-identiﬁcation,” in IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 79–88. 6, 7\n[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in\nIEEE Conference on Computer Vision and Pattern Recognition, 2015,\npp. 1–9. 7\n[32] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, “Image-\nimage domain adaptation with preserved self-similarity and domain-\ndissimilarity for person re-identiﬁcation,” in IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2018, pp. 994–1003. 6, 7\n[33] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-\nlutional neural networks for mobile vision applications,” arXiv preprint\narXiv:1704.04861, 2017. 7\n[34] F. Yang, Z. Zhong, Z. Luo, S. Lian, and S. Li, “Leveraging virtual and\nreal person for unsupervised person re-identiﬁcation,” IEEE Transac-\ntions on Multimedia, vol. 22, no. 9, pp. 2444–2453, 2019. 6, 7\n[35] Z. Zhong, L. Zheng, S. Li, and Y. Yang, “Generalizing a person\nretrieval model hetero-and homogeneously,” in European Conference on\nComputer Vision, 2018, pp. 172–188. 6, 7\n[36] Z. Ji, X. Zou, X. Lin, X. Liu, T. Huang, and S. Wu, “An attention-driven\ntwo-stage clustering method for unsupervised person re-identiﬁcation,”\nin European Conference on Computer Vision, 2020, pp. 20–36. 6, 7\n[37] J. Li and S. Zhang, “Joint visual and temporal consistency for unsuper-\nvised domain adaptive person re-identiﬁcation,” in European Conference\non Computer Vision, 2020. 6, 7\n[38] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, and Y. Tian, “Multiple expert\nbrainstorming for domain adaptive person re-identiﬁcation,” in European\nConference on Computer Vision, 2020, pp. 594–611. 6, 7\n[39] F. Zhao, S. Liao, G.-S. Xie, J. Zhao, K. Zhang, and L. Shao, “Un-\nsupervised domain adaptation with noise resistible mutual-training for\nperson re-identiﬁcation,” in European Conference on Computer Vision.\nSpringer, 2020, pp. 526–544. 6, 7\n[40] H.-X. Yu, A. Wu, and W.-S. Zheng, “Cross-view asymmetric metric\nlearning for unsupervised person re-identiﬁcation,” in IEEE Interna-\ntional Conference on Computer Vision, 2017, pp. 994–1002. 6, 7\n[41] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable\nperson re-identiﬁcation: A benchmark,” in IEEE International Confer-\nence on Computer Vision, 2015, pp. 1116–1124. 6, 7\n[42] S. Liao, Y. Hu, X. Zhu, and S. Z. Li, “Person re-identiﬁcation by\nlocal maximal occurrence representation and metric learning,” in IEEE\nConference on Computer Vision and Pattern Recognition, 2015, pp.\n2197–2206. 6, 7\n[43] M. Wang, B. Lai, J. Huang, X. Gong, and X.-S. Hua, “Camera-aware\nproxies for unsupervised person re-identiﬁcation,” in AAAI Conference\non Artiﬁcial Intelligence, vol. 2, 2021, p. 4. 6, 7\n[44] X. Pan, P. Luo, J. Shi, and X. Tang, “Two at once: Enhancing learning\nand generalization capacities via ibn-net,” in European Conference on\nComputer Vision (ECCV), 2018, pp. 464–479. 7\n[45] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, “Performance\nmeasures and a data set for multi-target, multi-camera tracking,” in\nEuropean Conference on Computer Vision.\nSpringer, 2016, pp. 17–\n35. 6\n[46] L. Wei, S. Zhang, W. Gao, and Q. Tian, “Person transfer gan to\nbridge domain gap for person re-identiﬁcation,” in IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 79–88. 6\n[47] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin 3rd International Conference on Learning Representations, 2015. 6\n[48] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, “Invariance matters:\nExemplar memory for domain adaptive person re-identiﬁcation,” in IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp. 598–\n607. 7\nIEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 14, NO. 8, APRIL 2022\n13\n[49] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\nvia non-parametric instance discrimination,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 3733–3742. 7\n[50] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, and G. Wang,\n“Dual attention matching network for context-aware feature sequence\nbased person re-identiﬁcation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 5363–5372. 10\n[51] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” International Conference\non Learning Representations, 2021. 10\n[52] H. Sun, M. Li, and C.-G. Li, “Hybrid contrastive learning with clus-\nter ensemble for unsupervised person re-identiﬁcation,” arXiv preprint\narXiv:2201.11995, 2022. 10\n[53] X. Liu, W. Liu, H. Ma, and H. Fu, “Large-scale vehicle re-identiﬁcation\nin urban surveillance videos,” in 2016 IEEE international conference on\nmultimedia and expo (ICME).\nIEEE, 2016, pp. 1–6. 10\n[54] X. Liu, W. Liu, T. Mei, and H. Ma, “Provid: Progressive and multi-\nmodal vehicle reidentiﬁcation for large-scale urban surveillance,” IEEE\nTransactions on Multimedia, vol. 20, no. 3, pp. 645–658, 2017. 10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-06-15",
  "updated": "2022-05-09"
}