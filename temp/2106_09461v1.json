{
  "id": "http://arxiv.org/abs/2106.09461v1",
  "title": "Modelling resource allocation in uncertain system environment through deep reinforcement learning",
  "authors": [
    "Neel Gandhi",
    "Shakti Mishra"
  ],
  "abstract": "Reinforcement Learning has applications in field of mechatronics, robotics,\nand other resource-constrained control system. Problem of resource allocation\nis primarily solved using traditional predefined techniques and modern deep\nlearning methods. The drawback of predefined and most deep learning methods for\nresource allocation is failing to meet the requirements in cases of uncertain\nsystem environment. We can approach problem of resource allocation in uncertain\nsystem environment alongside following certain criteria using deep\nreinforcement learning. Also, reinforcement learning has ability for adapting\nto new uncertain environment for prolonged period of time. The paper provides a\ndetailed comparative analysis on various deep reinforcement learning methods by\napplying different components to modify architecture of reinforcement learning\nwith use of noisy layers, prioritized replay, bagging, duelling networks, and\nother related combination to obtain improvement in terms of performance and\nreduction of computational cost. The paper identifies problem of resource\nallocation in uncertain environment could be effectively solved using Noisy\nBagging duelling double deep Q network achieving efficiency of 97.7% by\nmaximizing reward with significant exploration in given simulated environment\nfor resource allocation.",
  "text": " \n \n \nMODELLING RESOURCE ALLOCATION IN UNCERTAIN SYSTEM \nENVIRONMENT THROUGH DEEP REINFORCEMENT LEARNING \n \n \n \nNeel Gandhi \nStudent,School of Technology \nPandit Deendayal Energy University \nGandhinagar,Gujarat \nneel.gict18@sot.pdpu.ac.in \nShakti Mishra \nAssociate Professor,School of Technology \nPandit Deendayal Energy University \nGandhinagar,Gujarat \nshakti.mishra@sot.pdpu.ac.in \n \n \nABSTRACT \nReinforcement Learning has applications in field of mechatronics, robotics, and other resource- \nconstrained control system. Problem of resource allocation is primarily solved using traditional \npredefined techniques and modern deep learning methods. The drawback of predefined and most \ndeep learning methods for resource allocation is failing to meet the requirements in cases of uncertain \nsystem environment. We can approach problem of resource allocation in uncertain system environ- \nment alongside following certain criteria using deep reinforcement learning. Also, reinforcement \nlearning has ability for adapting to new uncertain environment for prolonged period of time. The \npaper provides a detailed comparative analysis on various deep reinforcement learning methods \nby applying different components to modify architecture of reinforcement learning with use of \nnoisy layers, prioritized replay, bagging, duelling networks, and other related combination to obtain \nimprovement in terms of performance and reduction of computational cost. The paper identifies \nproblem of resource allocation in uncertain environment could be effectively solved using Noisy \nBagging duelling double deep Q network achieving efficiency of 97.7% by maximizing reward with \nsignificant exploration in given simulated environment for resource allocation. \nKeywords Reinforcement Learning · Resource Allocation · Noisy Bagging D3 Q Network · Control Robotic systems \n1 Introduction \nDeep reinforcement learning [1] has intensively been used to solve complex problems that are rather difficult to be \nsolved using conventional techniques. Reinforcement learning aims at achieving long term reward by the mechanism of \nagent interacting with the environment and receiving an appropriate reward through a mechanism of state transition \nand evaluative feedback. Deep reinforcement learning, in recent years, has found applications in the field of robotics \n[2],system control [3] and other resource allocation problems [4–6]. Deep reinforcement learning is generally used in \ncases of exploring an unknown environment where an agent learns by interacting with the environment and develops a \nsuitable policy aiming at maximizing long-term reward. Reinforcement learning maintains balance between exploration \nand exploitation to obtain optimal result [7].Also, new horizons of decision making have been created using the power \nof reinforcement learning that were earlier not possible using supervised machine learning methods. Reinforcement \nlearning is used in resource Management that was found helpful in cases of job scheduling in computer clusters [8],relay \nselection in internet telephonic [9],traffic congestion control [10],adaptation of bit rate in video streaming [11] and \nother variants of reinforcement learning methods have been adopted for resource allocation in field of games [5] and \nbusiness process management [6]. This paper uses different variants of deep reinforcement learning algorithms majorly \nbased on Q learning approaches for the purpose of resource allocation.The paper suggests tools and techniques for \nsolving the problem of resource allocation using reinforcement learning.The paper contributes towards development of \nreinforcement learning models for resource allocation in control systems and robotics. The paper is divided into various \nsection as follows:- \nSection 1 includes application of Deep Reinforcement Learning in resource allocation problems and their respective \napproaches in field of robotics, control system, mechatronics, and other related fields followed by Problem statement \nA PREPRINT \n2 \n \n \nformulation along with description of the proposed Noisy Bagging D3 (duelling double deep )Q learning method is \nprovided along with an algorithmic view in Section 2.Further, results are obtained from various RL algorithms and \ncomparison of RL algorithm is done against our Noisy Bagging D3 (duelling double deep )Q learning with respect to \nexploration versus reward criteria in Section 3. Also, resource utilization graphs are provided in order to understand \nefficiency of the system. Section 4 includes concluding remarks for our proposed Noisy Bagging D3 Q learning method \nin resource allocation problem in an uncertain environment along with future perspective are provided. \n \n2 Applications of Reinforcement learning model in resource allocation \nReinforcement learning has found applications in Static and dynamic type of resource allocation from past couple \nof year [4, 12].Researchers have used open loop and closed loop systems and check their performance using model \nbased policies and result show significant performance in terms of reinforcement learning algorithm. Vengerov \n[13] proposed reinforcement learning along with fuzzy rule based approach for the purpose of dynamic resource \nallocation.Resource allocation has found applications in robotics [14].Reinforcement learning could be applied in the \nform of of Mobile Edge computing [15–17] were major research is done by various scientist for resources allocation at \nnetwork edge.Also,research has been done in field of cloud and power management in robotics were agent has to identify \na control policy for an interaction with partially observable system at the same time providing management capabilities \nin Complex environment [14, 18].Many Framework have also been developed for cloud Resource Management,some of \nwhich include deep reinforcement learning algorithm in their internal configuration. [19] Resource Management [12] \nand business process management have also utilise deep reinforcement learning approaches.They have been utilised \nfor control of vehicles through process of control resource with aid of internet of things.Reinforcement learning has \nbeen used for applications of UAV and vehicle Resource Management along with the integration of internet of things in \nvarious applications [14, 20].Also,grid computing [21],task scheduling and other allocation task are being handled by \ndynamic resource allocation algorithms derived from reinforcement learning.QCF algorithm was developed by Lian \net al [22] that integrated Q learning and chain feedback was used for resource allocation having a significant high \nconvergence.In this paper,we have proposed algorithm of noisy Bagging D3 learning to suitable for given allocation \nproblem after experimenting and analysing various Deep Q learning approaches. \n \n3 Proposed Methodology \n \n \nFigure 1: Learning Mechanism in Reinforcement Learning \n \nReinforcement learning applied to resource allocation problems where the agent reacts with the environment and \nlearns by making and rectifying errors [23]. For example, in the case of resource allocation, if a particular item is \nallocated to a particular resource that is incompatible, then it will immediately ask for a change request. In this way, a \nreinforcement learning agent reacts with the environment, gets a particular reward, and stores observation for future time \nsteps to achieve maximum cumulative long-term reward. Reinforcement learning is helpful in solving our complicated \nand complex problem of resource allocation that is changing with passage of time and it proves to be superior in \nan uncertain environment rather than the unsupervised and supervised learning approaches. The proposed model in \nthe long-run performs similar to human-level intelligence. Also, errors are corrected by the model with the help of \nreinforcement learning algorithms. Reinforcement learning algorithms prove to be effective for the purpose of robotic \napplications such as learning how to walk, locate a particular object in an unknown environment or resource allocation. \nThe advantage of using reinforcement learning for resource allocation problems is the property of adaptability to any \nA PREPRINT \n3 \n \n \ntype of setting as it is bound to learn from its experience even in absence of training data set. The proposed noisy \nbagging D3 reinforcement learning methods prove to be superior compared to any other algorithms. Noisy layer helps \nus to solve the exploration and exploitation dilemma indeed to maximize the long-term reward. The mechanism of \nfunctioning of reinforcement learning algorithm is depicted by fig1. \n \n3.1 Problem Formulation \nA system has a well defined number of resources and variable number of items varying in accordance to time. The \ngoal of reinforcement learning agents is assignment of resources to items in optimal manner following policy selecting \nactions with greatest Q value. Assumption is made that each item would require one resource at a time and would be \nable to hold that for at least a few periods of time, this type of setting is essential in case of robotic systems where \na particular operation has to be performed using a specific resource. Task of reinforcement learning algorithm is to \nassign resources to items in optimal manner to minimize changes required frequently alongside avoid cases of unused \nresources and non-performing items in the system environment. An imaginary instance of a simulation environment has \nbeen depicted in fig.2 to get an illusion about the actual problem statement. Various reinforcement learning algorithms \nwere applied to given problems in order to maximize the long-term reward and try to minimize the loss in case of \nimproper resource allocation. Simulation environment can be depicted in fig. 2 \n \nFigure 2: Simulation Environment \n \n3.1.1 State space,Action space and Rewards \nHere, detailed description regarding state space , action space and reward function of our problem formulated is \ndescribed as follows: \nState space- It helps us to understand current allocation of resources and resources available for being allocated \nto different items. In our environment, we have taken a fixed state representation in order to input into our neural \nnetwork. Suppose, there are M resources each being allocated to a single item according to requirement. The task \nof reinforcement learning algorithms would be to change the state of the system by allocating resources. Items have \nthe advantage of being constrained from maximum space due to a fixed set of resources and items make the learning \nprocess more efficient. Also, we have allocated one resource per item and allocated some delay before allocating it to \nanother resource in case of a change request. The simulation also provided many items at a certain time stamp in order \nto inspect the performance of the algorithm. \nAction space- Considering M resources for given number of items, for task of resource allocation, Several ways are \npossible for task of resource allocation generating large action space of 2M , which would make the performance of \nreinforcement learning more challenging and time consuming. So, agents are allowed to take more than one action at \na time and can allocate resources to items. Also, in case of a change request by item, it requires some delay before \nallocation of another resource. Agent observes the state of the system and takes appropriate actions in order to reduce \nthe number of re-allocation of resources and improve efficiency of the system trying to reduce the number of changes \nrequired at the same time being efficient in case of utilization of resources. \nRewards- Using reinforcement learning algorithms, agents try to maximize cumulative long-term reward. In the given \nproblem statement, the maximum reward that could be achieved is zero as it is a continuous process without definite \nterminal state due to uncertain environment simulation settings. Reinforcement learning algorithms try to minimize loss \nA PREPRINT \n4 \n \n \nas long as possible to maximize reward in our resource allocation problem. The equation for reward could be illustrated \nas \nreward = -abs(unutilized resource - target unutilized resource) \n \n3.2 Proposed Reinforcement Learning Model \nDeep Q networks aim to increase the future rewards with the help of selecting the greatest Q among available sets of \nactions using a particular policy for the state transition. Q learning follows a Bellman equation. \nQ(s, a) = r + γ · max Q (s′, a′) \n(1) \nwhere Q(s,a) is a resultant of reward received in addition to discounted maximum Q value of the next action and gamma() \nrepresents discount rate. Double deep Q networks [24] is used for the purpose of improving network architecture where \ncurrent state and target Q are compared with input features using policy net. In addition to policy net,target net is \nintroduced in order to reduce the number of operations due to prediction of Q values. Policy network replicated itself \nacross a network every thousand steps for further improvement in performance. For improvement of policy net, Duelling \nDeep Q networks [25] were introduced where policy net was split into two components resulting into one component \nbeing the state value of the model and other being an advantage in difference of Q between different actions taken by \nlearning model for resource allocation.Duelling Double Deep Q networks were used for achieving optimal policy net \nalong with advantage of target network. Aggregation was used for producing actions for resource allocation. In order to \ndeal with exploration-exploitation dilemma and find a better alternative to traditional epsilon greedy exploration to \nimprove performance of the model in an uncertain environment, noisy layers [26] were introduced where factorized \nGaussian Noise was preferred over Independent Gaussian Noise. Factorized Gaussian Noise comprised of two vectors, \nfirst vector for length of input sequence and second vector for length of output sequence, further specify mathematical \nfunction is used for calculating matrix multiplication and predict outcome.It results in formation of randomized matrix \nthat can be then added to our model in case of uncertain environment. Noisy layers were found appropriate for our \nmodel to get rid of Epsilon greedy exploration and to effectively deal with the exploitation exploration dilemma in an \nuncertain environment. Prioritized replay [27] didn’t prove to be effective in an uncertain environment due to greater \nattention towards wrong predictions to minimize the loss but it didn’t prove to be effective due to uncertainty in the \nenvironment and other related factors. Lastly, bootstrap aggregation or bagging [28] was used to train our model where \ndifferent samples trained on the same memory having difference in starting weights as well as trained on different \nbootstrapped networks from that memory. Actions were taken in accordance with random choice or most commonly \nrecommended action by aggregator. Bagging was helpful in exploration due to suggestions of different suggested \nactions from different networks. \nThe combination of above methodologies were integrated and applied to our proposed noisy bootstrap aggregation \nduelling double deep Q network in uncertain environment were actions are taken in accordance to particular policy \ntrying to maximize reward in our model based environment with use of policy based methods.The model tried to \nmaximize the cumulative long term reward in case of of resource allocation.The steps followed by the model included:- \n1) Simulation environment was created,memory was set up,selection of policy net,target net and setting up noisy layer is \ndone in the initial step of our resource allocation simulation \n2) Bootstrap samples are derived from memory and trained to recommend most appropriate action. \n3) Inside different nets training was done to suggest next action in order to maximize reward report.Aggregator is used \nfor selection of best possible action.Further,storing of the state,next state,reward and terminal to the memory operation \nis performed after choosing best recommended action from among different samples \n4) Lastly,evaluation was done for performance of proposed Noisy bagged D3 reinforcement learning algorithm \nNoisy bootstrapped aggregation Bagging D3 Q learning approach proves to be superior compared to other variants \nof Q learning models for resource allocation problems. The noisy layers are effective in exploration of uncertain \nenvironments and act as an effective alternative to epsilon-greedy exploration. Duelling nature helps in getting the state \nvalue as well as difference in Q value between different actions, which is helpful in selection of appropriate action with \ngreatest Q value. Bootstrap Aggregation(Bagging) helps in measuring the uncertainty of action which is effective in \nselecting the best possible actions from different networks based on criteria of random selection or average selections \nor most recommended action. Bagging was also found to be helpful in aiding stages of exploration where networks \nprovide different suggested actions. Hence, Noisy bootstrapped aggregation (‘Bagging’) D3 Q learning is effective in \nterms of reward as well as efficiency for resource allocation problem in uncertain environment. \nAlgorithm implementation of Reinforcement Learning for dealing with problem of resource allocation is depicted by \nunderlying algorithm view \nA PREPRINT \n5 \n \n \n \n \nAlgorithm 1 An overview of steps involved in resource allocation using RL algorithm \nCreate simulation environment \nCreate memory \nCreate policy net \nCreate target net \nwhile Training episodes not complete do \nReset simulation \nwhile not in terminal state do \nRecommend action from policy net \nPass action to simulation \nIncrease time-step in simulation \nReceive (next state, reward, terminal,info) from simulation \nStore (state, next state, reward, terminal)to memory \nUpdate policy net \nend \nUpdate target net \nend \nEvaluation for performance of policy net \n \n4 Results \n \n(a) Double Deep Q Learning \n(b) D3 Q Learning \n(c) Noisy D3 Q Learning \n(d) Prioritised Replay DDQN \n \n \n \n \n(e) Prioritised Replay Noisy D3 \nQ Learning \n \n(f) Bootstrapped Aggregation \n(’Bagging’) D3 Q Learning \n(g) Noisy Bootstrapped Aggre- (h) Proposed Noisy Boot- \ngation (’Bagging’) D3Q Learn- strapped \nAggregation \ning with Prioritised Replay (’Bagging’) D3 Q Learning \nFigure 3: Exploration vs Reward \n \nThe result obtained after simulation choosing various types of reinforcement learning algorithms right from double Q \nnetwork to noisy bootstrap aggregation duelling double Q network with prioritized replay showed that noisy bootstrap \naggregation D3 Q learning approach was supposed to be the best algorithm in terms of efficiency for resource allocation. \nHence, effective for operation of resource allocation in robotic control systems. Few algorithms having prioritized \nreplay suffered from under capacity for prolonged amount of time with significantly less accuracy due to reason \nA PREPRINT \n6 \n \n \nuncertain/fuzzy environment of simulation. Also, noisy layers proved to effective in improving accuracy of the system. \nDuelling nature, in some cases, would increase or decrease efficiency of system in the environment due to splitting of \npolicy net. Lastly, we have identified that noisy bagging D3 Q network approach was found to be superior in terms \nof exploration versus reward illustrated by fig 3 and also utilization of resources illustrated by fig 4. Noisy bagging \nD3 Q network shows that it is the best suitable algorithm for resource allocation. Comparative analysis of various \nalgorithms on basis of reward and resource utilization by items or agent present in the system along with their respective \ncalculation for analyzing efficiency of different reinforcement learning algorithm is illustrated by table given below \n \n \n \n \n \n(a) Double Deep Q Learning \n(b) D3 Q Learning \n(c) Noisy D3 Q Learning \n(d) Prioritised Replay DDQN \n \n \n \n \n(e) Prioritised Replay Noisy D3 \nQ Learning \n \n(f) Bootstrapped Aggregation \n(’Bagging’) D3 Q Learning \n(g) Noisy Bootstrapped Aggre- (h) Proposed Noisy Boot- \ngation (’Bagging’) D3Q Learn- strapped \nAggregation \ning with Prioritised Replay (’Bagging’) D3 Q Learning \nFigure 4: Resource utilization graphs \n \n \n \n \n \n1 \nDouble Deep Q Learning \n2 \nD3 Q Learning \n3 \nNoisy D3 Q Learning \n4 \nPrioritised Replay DDQN \n5 \nPrioritised Replay Noisy D3 Q Learning \n6 \nBootstrapped Aggregation (’Bagging’) D3 Q Learning \n7 \nNoisy Bootstrapped Aggregation (’Bagging’) \n8 \nProposed Noisy Bootstrapped Aggregation (’Bagging’) D3 Q Learning \nA PREPRINT \n7 \n \n \n \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \ntime period under capacity \n28 \n22 \n23 \n0 \n13 \n38 \n27 \n38 \ntime period over capacity \n72 \n77 \n77 \n100 \n86 \n62 \n73 \n62 \naverage items performing \n550 \n543 \n560 \n545 \n548 \n570 \n546 \n557 \naverage resources utilized \n575 \n567 \n589 \n673 \n591 \n584 \n571 \n570 \n% efficiency \n95.6 \n95.8 \n95 \n81 \n92.7 \n97.5 \n95.7 \n97.7 \n \n5 Conclusion \nReinforcement learning models have been used for resource allocation in robotics system to take control decisions \nwith significant performance at runtime. We propose a reinforcement learning method of noisy bagging D3 Q network \nfor modelling resource allocation in a way to maximize long-term reward, balance exploration-exploitation dilemma \nand performing operations with significant efficiency in the long run. The proposed method could be applied to other \nresource allocation problems like task scheduling, job scheduling and similar complicated scheduling tasks in the field \nof robotics, mechatronics and other control systems. The proposed model proved to be feasible and efficient at the \nsame time being effective in taking control decisions for resource allocation in an uncertain environment. In order \nto obtain maximum performance and solve a complicated problem in control systems. We have also compared our \nproposed method with different variants of deep reinforcement learning models across various evaluation parameters. \nReinforcement learning methods applied for resource allocation tasks still face few major challenges like explainability, \nscalability, complexity and flexibility. For future works, we can develop a generic framework for different kinds \nof allocation problems in resource constrained environments. The paper contributes towards the field of artificial \nintelligence by virtue of introduction of automation in dynamic resource allocation for robotic control systems. \n \nReferences \n[1] Maxim Lapan. Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, \nvalue iteration, policy gradients, TRPO, AlphaGo Zero and more. Packt Publishing Ltd, 2018. \n[2] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International \nJournal of Robotics Research, 32(11):1238–1274, 2013. \n[3] Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for networked system \ncontrol. arXiv preprint arXiv:2004.01339, 2020. \n[4] Gerald Tesauro, Nicholas K Jong, Rajarshi Das, and Mohamed N Bennani. A hybrid reinforcement learning \napproach to autonomic resource allocation. In 2006 IEEE International Conference on Autonomic Computing, \npages 65–73. IEEE, 2006. \n[5] Si-Ping Zhang, Jia-Qi Dong, Li Liu, Zi-Gang Huang, Liang Huang, and Ying-Cheng Lai. Reinforcement learning \nmeets minority game: Toward optimal resource allocation. Physical Review E, 99(3):32302, 2019. \n[6] Zhengxing Huang, Wil M P van der Aalst, Xudong Lu, and Huilong Duan. Reinforcement learning based resource \nallocation in business process management. Data Knowledge Engineering, 70(1):127–145, 2011. \n[7] Ithan Moreira, Javier Rivas, Francisco Cruz, Richard Dazeley, Angel Ayala, and Bruno Fernandes. Deep Rein- \nforcement Learning with Interactive Feedback in a Human–Robot Environment. Applied Sciences, 10(16):5574, \n2020. \n[8] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and Aditya Akella. Multi-resource \npacking for cluster schedulers. ACM SIGCOMM Computer Communication Review, 44(4):455–466, 2014. \n[9] Junchen Jiang, Rajdeep Das, Ganesh Ananthanarayanan, Philip A Chou, Venkata Padmanabhan, Vyas Sekar, \nEsbjorn Dominique, Marcin Goliszewski, Dalibor Kukoleca, and Renat Vafin. Via: Improving internet telephony \ncall quality using predictive relay selection. In Proceedings of the 2016 ACM SIGCOMM Conference, pages \n286–299, 2016. \n[10] Keith Winstein and Hari Balakrishnan. Tcp ex machina: Computer-generated congestion control. ACM SIGCOMM \nComputer Communication Review, 43(4):123–134, 2013. \n[11] Yi Sun, Xiaoqi Yin, Junchen Jiang, Vyas Sekar, Fuyuan Lin, Nanshu Wang, Tao Liu, and Bruno Sinopoli. CS2P: \nImproving video bitrate selection and adaptation with data-driven throughput prediction. In Proceedings of the \n2016 ACM SIGCOMM Conference, pages 272–285, 2016. \nA PREPRINT \n8 \n \n \n[12] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management with deep \nreinforcement learning. In Proceedings of the 15th ACM workshop on hot topics in networks, pages 50–56, 2016. \n[13] David Vengerov. A reinforcement learning approach to dynamic resource allocation. Engineering Applications of \nArtificial Intelligence, 20(3):383–390, 2007. \n[14] Yi Liu, Huimin Yu, Shengli Xie, and Yan Zhang. Deep reinforcement learning for offloading and resource \nallocation in vehicle edge computing and networks. IEEE Transactions on Vehicular Technology, 68(11):11158– \n11168, 2019. \n[15] Ji Li, Hui Gao, Tiejun Lv, and Yueming Lu. Deep reinforcement learning based computation offloading and \nresource allocation for MEC. In 2018 IEEE Wireless Communications and Networking Conference (WCNC), \npages 1–6. IEEE, 2018. \n[16] Zehui Xiong, Yang Zhang, Dusit Niyato, Ruilong Deng, Ping Wang, and Li-Chun Wang. Deep reinforcement \nlearning for mobile 5G and beyond: Fundamentals, applications, and challenges. IEEE Vehicular Technology \nMagazine, 14(2):44–52, 2019. \n[17] Nguyen Cong Luong, Dinh Thai Hoang, Shimin Gong, Dusit Niyato, Ping Wang, Ying-Chang Liang, and \nDong In Kim. Applications of deep reinforcement learning in communications and networking: A survey. IEEE \nCommunications Surveys Tutorials, 21(4):3133–3174, 2019. \n[18] Yue Jin, Makram Bouzid, Dimitre Kostadinov, and Armen Aghasaryan. Model-free resource management of \ncloud-based applications using reinforcement learning. In 2018 21st Conference on Innovation in Clouds, Internet \nand Networks and Workshops (ICIN), pages 1–6. IEEE, 2018. \n[19] Shuiguang Deng, Zhengzhe Xiang, Peng Zhao, Javid Taheri, Honghao Gao, Jianwei Yin, and Albert Y Zomaya. \nDynamical resource allocation in edge for trustable Internet-of-Things systems: A reinforcement learning method. \nIEEE Transactions on Industrial Informatics, 16(9):6103–6113, 2020. \n[20] Jingzhi Hu, Hongliang Zhang, Lingyang Song, Zhu Han, and H Vincent Poor. Reinforcement learning for a cellular \ninternet of UAVs: Protocol design, trajectory control, and resource management. IEEE Wireless Communications, \n27(1):116–123, 2020. \n[21] Jun Wu, Xin Xu, Pengcheng Zhang, and Chunming Liu. A novel multi-agent reinforcement learning approach for \njob scheduling in grid computing. Future Generation Computer Systems, 27(5):430–439, 2011. \n[22] Chuanqiang LIAN, Xin XU, Jun WU, and Zhaobin LI. Q-CF multi-Agent reinforcement learning for resource \nallocation problems. CAAI Transactions on Intelligent Systems, 6(2):95–100, 2011. \n[23] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel \nDulac-Arnold, Ian Osband, and John Agapiou. Learning from demonstrations for real world reinforcement \nlearning. 2017. \n[24] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In \nProceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016. \n[25] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network \narchitectures for deep reinforcement learning. In International conference on machine learning, pages 1995–2003. \nPMLR, 2016. \n[26] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad \nMnih, Remi Munos, Demis Hassabis, and Olivier Pietquin. Noisy networks for exploration. arXiv preprint \narXiv:1706.10295, 2017. \n[27] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint \narXiv:1511.05952, 2015. \n[28] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped \nDQN. arXiv preprint arXiv:1602.04621, 2016. \n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "cs.RO",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2021-06-17",
  "updated": "2021-06-17"
}