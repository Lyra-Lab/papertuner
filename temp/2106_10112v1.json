{
  "id": "http://arxiv.org/abs/2106.10112v1",
  "title": "Deep Reinforcement Learning Models Predict Visual Responses in the Brain: A Preliminary Result",
  "authors": [
    "Maytus Piriyajitakonkij",
    "Sirawaj Itthipuripat",
    "Theerawit Wilaiprasitporn",
    "Nat Dilokthanakul"
  ],
  "abstract": "Supervised deep convolutional neural networks (DCNNs) are currently one of\nthe best computational models that can explain how the primate ventral visual\nstream solves object recognition. However, embodied cognition has not been\nconsidered in the existing visual processing models. From the ecological\nstandpoint, humans learn to recognize objects by interacting with them,\nallowing better classification, specialization, and generalization. Here, we\nask if computational models under the embodied learning framework can explain\nmechanisms underlying object recognition in the primate visual system better\nthan the existing supervised models? To address this question, we use\nreinforcement learning to train neural network models to play a 3D computer\ngame and we find that these reinforcement learning models achieve neural\nresponse prediction accuracy scores in the early visual areas (e.g., V1 and V2)\nin the levels that are comparable to those accomplished by the supervised\nneural network model. In contrast, the supervised neural network models yield\nbetter neural response predictions in the higher visual areas, compared to the\nreinforcement learning models. Our preliminary results suggest the future\ndirection of visual neuroscience in which deep reinforcement learning should be\nincluded to fill the missing embodiment concept.",
  "text": "Deep Reinforcement Learning Models Predict Visual\nResponses in the Brain: A Preliminary Result\nMaytus Piriyajitakonkij 1,3∗, Sirawaj Itthipuripat 2\nTheerawit Wilaiprasitporn 1, Nat Dilokthanakul 1\n1 Vidyasirimedhi Institute of Science and Technology (VISTEC), Thailand\n2 King Mongkut’s University of Technology Thonburi (KMUTT), Thailand\n3 Imperial College London, United Kingdom\nnatd_pro@vistec.ac.th\nAbstract\nSupervised deep convolutional neural networks (DCNNs) are currently one of the\nbest computational models that can explain how the primate ventral visual stream\nsolves object recognition. However, embodied cognition has not been considered\nin the existing visual processing models. From the ecological standpoint, humans\nlearn to recognize objects by interacting with them, allowing better classiﬁcation,\nspecialization, and generalization. Here, we ask if computational models under\nthe embodied learning framework can explain mechanisms underlying object\nrecognition in the primate visual system better than the existing supervised models?\nTo address this question, we use reinforcement learning to train neural network\nmodels to play a 3D computer game and we ﬁnd that these reinforcement learning\nmodels achieve neural response prediction accuracy scores in the early visual\nareas (e.g., V1 and V2) in the levels that are comparable to those accomplished by\nthe supervised neural network model. In contrast, the supervised neural network\nmodels yield better neural response predictions in the higher visual areas, compared\nto the reinforcement learning models. Our preliminary results suggest the future\ndirection of visual neuroscience in which deep reinforcement learning should be\nincluded to ﬁll the missing embodiment concept.\n1\nIntroduction\nThe primate visual system has a remarkable ability in recognizing multiple classes of visual objects\nand scenes. Object identiﬁcation and recognition are thought to be supported in part by the ventral\nvisual stream, which comprises multiple cortical areas resided in the occipital and the temporal lobes.\nThese visual areas are organized in a hierarchical fashion, where higher visual areas process more\ncomplex visual features. The primary visual cortex (V1) is thought to encode low-level features,\nsuch as stimulus contrasts, orientations, edges and spatial frequencies. The visual inputs from V1\nare then relayed through the extrastriate visual areas like V2 and V4, thought to process mid-level\nvisual features such as textures, colors, and motions [3,13,16], and ultimately projected onto the\ndownstream visual area known as the inferior temporal (IT) cortex, where neural representations of\ndifferent object categories can be untangled or linearly separated [2].\nOver decades, several computational models have been proposed to explain how the ventral visual\nstream solves object recognition. Among these models, deep convolutional neural network (DCNN)\nis currently one of the most promising models that can explain neural mechanisms underlying object-\nrelated information processing in the ventral visual stream. Speciﬁcally, when the supervised DCNN\n∗Work done during working at VISTEC.\nPreprint. Under review.\narXiv:2106.10112v1  [cs.LG]  18 Jun 2021\nTest Image\nAction-Value \nFunction \nImage Class\nDoom’s States\nDeep Q-Network\n(DQN)\nSupervised Neural Network\n(SNN)\nMIX20 Images\nTraining Phase\nEvaluation Phase\nDQN or SNN\nArtificial Neural Network\nFigure 1: Training Phase: DQN models are trained to accomplish tasks in Doom environment. SNN\nmodels are trained to minimize classiﬁcation errors. Evaluation Phase: The neural network model\nand the brain receive the same images (blue arrows). Each model-layer’s activation is mapped by\nlinear transformation to neural responses in each brain area (green arrows). Model activation is used\nto predict neural responses in the area to which its layer is mapped.\nis trained with natural images to minimize the classiﬁcation error, its activation can be used to predict\npatterns of behavioral and neural responses in the primate ventral visual stream that are related to\nparticular sets of objects better than other existing models [15, 25,26], e.g., V1-like Gabor-based\nmodel [14] and HMAX [20].\nHowever, embodied visual cognition has not been considered in the previous studies. The brain does\nnot only perceive and process information, it also interacts with the physical world and changes the\nstate of what it perceives [5]. Embodied cognition helps the brain make sense of the three-dimensional\nworld from two-dimensional sensory information. For example, it helps the brain infer the depth\nof an object [4,6,24]. We argue that computational models of a biological visual system must be\nembodied.\nReinforcement learning (RL) is one way to create an embodied agent by using reward to shape action\nselections. The agent learns to select sequences of actions that maximize the total reward [22]. Deep\nRL leverages a representational advantage of deep neural networks and signiﬁcantly improve RL\nability to solve much more complex problems such as playing Atari games with raw pixels, winning\nthe Go’s world champion and robotic control [10,12,21].\nIn this work, we use deep Q-learning–a deep RL algorithm–to train DCNN [12]. The neural network\nmodel is trained to play a 3D computer game. It receives only raw pixels to take action and accomplish\ntasks in the game. We hypothesize that visual features learned by DQN are similar to those learned by\nthe ventral visual stream. We explore this hypothesis in our experiments, and ﬁnd that DQN achieves\ncomparable neural prediction accuracy scores to supervised learning model in V1 and V2 using the\nsame neural network architecture.\n2\nMethod\nThe overviews of model training and evaluation are shown in Figure 1. At the training phase, artiﬁcial\nneural network (ANN) models are trained with supervised and reinforcement learning paradigms.\nDQN models are trained to play Doom–a 3D shooting game. Supervised Neural Network (SNN)\nmodels are trained to recognize images.\nAt the evaluation phase, these models are evaluated with neural data by their neural predictivity\nscores. Neural predictivity is used to measure how well activations in the neural network model\npredict neural responses of visual stimuli. The set of images and neural responses, used for evaluation,\nare explained in subsection 2.1. The detail of neural predictivity, DQN, RL environment/datasets and\nmodel conﬁguration are provided in subsection 2.2, subsection 2.3, subsection 2.4 and subsection 2.5\nrespectively.\n2\n2.1\nNeural Data\nThere are two evaluation datasets: V1/V2 dataset and V4/IT dataset. Neural responses of V1 and V2\nareas were collected by [3] from 13 macaques with implanted electrodes. Neural data were recorded\nfrom 102 and 103 neurons in V1 and V2 respectively. Stimuli were from 15 texture families. Neural\nresponses of V4 and IT areas were collected by [11] from two macaques with implanted electrodes,\nrecorded from 88 neurons in V4 and 168 neurons in IT. There were a total of 2,560 stimuli from\n64 rendered three-dimensional objects from 8 categories with the random pose, position, size, and\nnatural background.\n2.2\nNeural Predictivity\nAt the evaluation phase, the brain and the model receives input images from evaluation datasets. The\npairs of neural responses y and model activations x are attained corresponding to the same input\nimages. Neural predictivity indicates a prediction performance of a linear regression model, which is\ntrained to predict neural responses y from model activations x. In case of DCNN, activations of each\nlayer are used to predict each brain area neural responses as shown in Figure 1. Predicted response y′\nis derived from linear transformation y′ = wx, where w is a parameter vector of the linear model.\nThen, neural predictivity score is computed using Equation 1.\nr =\nPn\ni=1(yi −¯y)(y′\ni −¯y′)\npPn\ni=1(yi −¯y)2(y′\ni −¯y′)2\n(1)\nWhere ¯y and ¯y′ are averages of y and y′ across samples respectively. Here, we use Brain-Score\nlibrary [18,19] to implement this process.\n2.3\nDeep Q-Networks\nQ-learning is a learning method that learns the value of a state-action pair, i.e. how good is an action a\nif taken in a state s. Deep Q-Network (DQN) is an extension to the Q-learning algorithm. It leverages\nthe representational power of a convolutional neural network to represent the state-action value or\nQ-value. In other word, it uses a deep neural network θ as a function approximator Q(s, a; θ), which\nmaps a state-action pair into a scalar value. Therefore, the early layers in the network θ have to be\nable to extract useful features for estimating the Q-value. DQN can be trained with the following\nobjective,\nLi(θi) = Es,a,s′[(yi −Q(s, a; θi))2]\n(2)\nwhere the target yi = r + γmaxa′Q(s′, a′; θi−1), r is reward given by an environment, and s′ and a′\nare a state and an action in the next time-step observed during the interactions with the environment.\nIn the experiments, we follows the dueling architecture by Wang et al. [23]\n2.4\nEnvironment and Dataset\nWe have eight trained models. Six models are DQN models trained on different RL tasks. The other\ntwo models are supervised models (SNN). All of the models have the same architecture except for the\noutput size. One of the SNNs is trained with CIFAR10 [9], containing 10 classes of natural images.\nAnother is trained with our own MIX20 dataset, which has more image variations and more texture\ninformation than CIFAR10. The details of RL tasks and MIX20 are explained below.\nEnvironment: DQN is an embodied agent. It needs the environment to interact with during training.\nHowever, during predictivity evaluation, we can get the model activations by directly pass any stimuli\nimages unrelated to the environment through the model.\nWe use VizDoom platform [8] (https://github.com/mwydmuch/ViZDoom) for our training envi-\nronments. Six VizDoom tasks are chosen. Simpler Basic, Defend Center and Predict Position are\nmonster shooting tasks. Predict Position and Take Cover are missile motion learning tasks. My Way\nHome is a visual navigation task in labyrinth layout. Health Gathering is an object collection task.\nThe set of actions an agent can take are shoot, turn left, turn right, move left, move right, move\nforward. The agent is allowed to take some of these actions depending on the tasks.\n3\nV1\nV2\nV4\nIT\nUntrained\nSimpler Basic\nDefend Center\nPredict Position\nTake Cover\nMy Way Home\nHealth Gathering\nCIFAR10\nMIX20\nNeural Predictivity\n*\n*\nDQN\nSNN\nFigure 2: Neural Predictivity scores of the untrained model, DQN and SNN models in four brain\nareas: V1, V2, V4 and IT. The error bars are computed using standard error.\nMIX20 dataset: The visual stimuli in V1/V2 dataset are texture images, and in V4/IT dataset are\nnatural object images. To create a best possible model, we want similar image statistics in our dataset\nand in the evaluation datasets. Therefore, we construct 20-class image dataset (MIX20) from 10\nobjects in ImageNet dataset [17] and from 10 textures from DTD dataset [1]. The baseline trained\nwith this dataset serves as our upper-bound, a cheat model that has been trained on a relatively better\ndataset.\n2.5\nModel\nThe models’ inputs are 128×128-pixel color images. The models that are used for all learning\nparadigms have almost the same architecture. The only exception in the number of output nodes which\ndepends on an action space or image classes. The architecture has four consecutive convolutional\nlayers, one fully-connected, and a output layer. The number of features map are 16, 32, 64 and\n32 respectively. Kernel size is 3 for all layers. Stride lengths are 2, 2, 1 and 1 respectively. The\nfully-connected layer has 64 hidden units. ReLu activation function is applied in all layers, and\nBatchNorm2D [7] is applied to all convolutional layers. The total number of parameters is about 1.5\nmillion, depending on the number of output nodes.\n3\nExperiment\nWe compare neural predictivity scores among all models. An untrained model, which is randomly\ninitialized, is used as a baseline. The untrained model gives us a lower-bound of neural predictivity\nscore. To support our hypothesis, we expect the DQN models to have higher neural predictivity\nscores than the untrained model.\nWe also compare the RL-trained models with two supervised models. The CIFAR10 model represents\na supervised model, which is trained on a dataset with relatively similar complexity to the DOOM\nenvironments. The MIX20 model represents a supervised model, which is trained on a good dataset\nwith high texture variations and natural looking images. This model serves as our upper-bound to the\nneural predictivity score achievable by the small architecture used in this study.\n4\nResult and Discussion\nFigure 2 shows the average neural predictivity scores of the models in four brain areas. In summary,\ntwo DQN models achieve comparable neural predictivity scores to MIX20-trained SNN in V1 and\nV2 areas. MIX20-trained SNN is the best model for neural prediction in all areas. Moreover, all\nDQN models achieve higher V4 neural predictivity scores than the untrained model. While CIFAR10-\ntrained SNN is the second-best model for V4 and IT areas, its neural predictivity scores in V1 and\nV2 are lower than the untrained model.\nWe see that RL-trained models have high predictivity scores compares to the untrained models. This\nresult supports the hypothesis that reinforcement learning (RL) can shape visual representation in the\nbiological brain and it is especially good at learning low-level features as seen in high predictivity\nscores in V1 and V2.\nDQN models are only trained in non-photorealistic and simple RL environments. In contrast, SNN is\ntrained with MIX20 which has much richer and more natural visual information. Interestingly, DQN\n4\nmodels achieve comparable performance to MIX20-trained SNN in V1 and V2 neural predictions. It\nis possible that the learning dynamics of RL leads to a closer texture representation to the texture\nrepresentation in the brain. This needs further investigation and we plan to investigate an RL agent in\na more complex and photorealistic environments in the future work.\nThe DQN models are trained in one layout. Therefore, these models receive images that have similar\nstatistics. As a result, we expect that these models to also learn similar visual features. Nevertheless,\nthe experimental results show that the DQN models, trained from different tasks, have different neural\npredictivity scores. This implies that they learn different visual features. Defend-Center-trained DQN\nachieves much higher neural predictivity than Simpler-Basic-trained DQN neural predictivity in V1\nand IT areas. The main difference between Simpler Basic and Defend Center is that the latter has\nhigher task complexity. We think that the DQN trained to achieve a simple task learn poorer visual\nfeatures than the more complex tasks.\nCIFAR10-trained SNN does not improve neural prediction in V1 and V2 areas from the untrained\nmodel. CIFAR10 is a much lower quality dataset than MIX20 dataset. Moreover, our DCNN models\nhave a small number of parameters. Poor images in CIFAR10 dataset and the low model complexity\nmight cause it to learn poor low-level visual features, resulting in a lower neural prediction accuracy\nin V1 and V2. Interestingly, CIFAR10-trained model performs relatively well in predicting neural\nresponses in V4 and IT.\nAll SNN models perform better than all the DQN models in V4 and IT neural predictions. The\nobject identity representations are disentangled in the late SNN layer and in IT cortex. Since SNN\nmodels are trained speciﬁcally to disentangle object class, it is understandable that they could perform\nbetter in IT neural predictivity. DQN models are not directly trained to classify an object. They are\ntrained to achieve higher level goals. It is possible that to achieve the goals in these environments the\nnetworks do not require to object disentanglement. In real-world, however, the tasks are much more\ncomplex. We also plan to investigate further in this direction.\nIn summary, we found that RL-trained models has relatively good neural predictivity scores compare\nto the supervised models, especially in V1 and V2, even though the environments are not photo-\nrealistic. For future work, we plan to use more realistic environments and investigate various kinds of\nRL algorithms and tasks. Another interesting question is how ventral stream interacts with dorsal\nstream, which is thought to encode object spatial representations and guidance of actions. Since deep\nRL models, trained in complex environment, should be able to encode object identity, object spatial\nlocations, and guidance of actions, we think that the encoded information in deep RL models might\nhelp explain the interaction between the two streams.\nAcknowledgement\nThis work was supported by Thailand Science Research and Innovation (SRI62W1501). The\nauthors would like to thank Chaipat Chunharas and Supanida Piyayotai for their help in generating\nthe preliminary ideas which lead to this work. The authors also would like to thank Wanitchaya\nPoonpatanapricha for useful discussion on the research problems and ideas in this work.\nReferences\n[1] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3606–3613, 2014.\n[2] James J DiCarlo, Davide Zoccolan, and Nicole C Rust. How does the brain solve visual object\nrecognition? Neuron, 73(3):415–434, 2012.\n[3] Jeremy Freeman, Corey M Ziemba, David J Heeger, Eero P Simoncelli, and J Anthony Movshon.\nA functional and perceptual signature of the second visual area in primates. Nature neuroscience,\n16(7):974–981, 2013.\n[4] Eleanor J Gibson. Exploratory behavior in the development of perceiving, acting, and the\nacquiring of knowledge. Annual review of psychology, 39(1):1–42, 1988.\n[5] James J Gibson. The theory of affordances. Hilldale, USA, 1(2):67–82, 1977.\n5\n[6] James J Gibson. The ecological approach to visual perception: classic edition. Psychology\nPress, 1979.\n[7] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In International conference on machine learning, pages\n448–456. PMLR, 2015.\n[8] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski.\nVizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE\nConference on Computational Intelligence and Games (CIG), pages 1–8. IEEE, 2016.\n[9] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n[10] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n[11] Najib J Majaj, Ha Hong, Ethan A Solomon, and James J DiCarlo. Simple learned weighted\nsums of inferior temporal neuronal ﬁring rates accurately predict human core object recognition\nperformance. Journal of Neuroscience, 35(39):13402–13418, 2015.\n[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[13] J Anthony Movshon, Ian D Thompson, and David J Tolhurst. Spatial summation in the receptive\nﬁelds of simple cells in the cat’s striate cortex. The Journal of physiology, 283(1):53–77, 1978.\n[14] Nicolas Pinto, David D Cox, and James J DiCarlo. Why is real-world visual object recognition\nhard? PLoS Comput Biol, 4(1):e27, 2008.\n[15] Rishi Rajalingham, Elias B Issa, Pouya Bashivan, Kohitij Kar, Kailyn Schmidt, and James J Di-\nCarlo. Large-scale, high-resolution comparison of the core visual object recognition behavior of\nhumans, monkeys, and state-of-the-art deep artiﬁcial neural networks. Journal of Neuroscience,\n38(33):7255–7269, 2018.\n[16] Anna W Roe, Leonardo Chelazzi, Charles E Connor, Bevil R Conway, Ichiro Fujita, Jack L\nGallant, Haidong Lu, and Wim Vanduffel. Toward a uniﬁed theory of visual area v4. Neuron,\n74(1):12–29, 2012.\n[17] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n[18] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa,\nKohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt, et al. Brain-score: Which\nartiﬁcial neural network for object recognition is most brain-like? BioRxiv, page 407007, 2018.\n[19] Martin Schrimpf, Jonas Kubilius, Michael J Lee, N Apurva Ratan Murty, Robert Ajemian, and\nJames J DiCarlo. Integrative benchmarking to advance neurally mechanistic models of human\nintelligence. Neuron, 2020.\n[20] Thomas Serre, Aude Oliva, and Tomaso Poggio. A feedforward architecture accounts for rapid\ncategorization. Proceedings of the national academy of sciences, 104(15):6424–6429, 2007.\n[21] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,\n2016.\n[22] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[23] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.\nDueling network architectures for deep reinforcement learning. In International conference on\nmachine learning, pages 1995–2003. PMLR, 2016.\n[24] Margaret Wilson. Six views of embodied cognition. Psychonomic bulletin & review, 9(4):625–\n636, 2002.\n6\n[25] Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand\nsensory cortex. Nature neuroscience, 19(3):356–365, 2016.\n[26] Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J\nDiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual\ncortex. Proceedings of the national academy of sciences, 111(23):8619–8624, 2014.\n7\n",
  "categories": [
    "cs.LG",
    "q-bio.NC"
  ],
  "published": "2021-06-18",
  "updated": "2021-06-18"
}