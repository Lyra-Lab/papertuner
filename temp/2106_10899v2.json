{
  "id": "http://arxiv.org/abs/2106.10899v2",
  "title": "Ad Text Classification with Transformer-Based Natural Language Processing Methods",
  "authors": [
    "Umut Özdil",
    "Büşra Arslan",
    "D. Emre Taşar",
    "Gökçe Polat",
    "Şükrü Ozan"
  ],
  "abstract": "In this study, a natural language processing-based (NLP-based) method is\nproposed for the sector-wise automatic classification of ad texts created on\nonline advertising platforms. Our data set consists of approximately 21,000\nlabeled advertising texts from 12 different sectors. In the study, the\nBidirectional Encoder Representations from Transformers (BERT) model, which is\na transformer-based language model that is recently used in fields such as text\nclassification in the natural language processing literature, was used. The\nclassification efficiencies obtained using a pre-trained BERT model for the\nTurkish language are shown in detail.",
  "text": "D¨on¨us¸t¨ur¨uc¨u Tabanlı Do˘gal Dil ˙Is¸leme Y¨ontemleri\nile Reklam Metni Sınıﬂandırması\nAd Text Classiﬁcation with Transformer-Based Natural Language\nProcessing Methods\nUmut ¨Ozdil\nAr-Ge Departmanı\nAdresGezgini A.S¸.\n˙Izmir, T¨urkiye\numutozdil@adresgezgini.com\nB¨us¸ra Arslan\nAr-Ge Departmanı\nAdresGezgini A.S¸.\n˙Izmir, T¨urkiye\nbusraarslan@adresgezgini.com\nDavut Emre Tas¸ar\nB¨uy¨uk Veri ve ˙Ileri Analitik B¨ol¨um\nGaranti BBVA Teknoloji\n˙Istanbul, T¨urkiye\nemretasa@garantibbva.com.tr\nG¨okc¸e Polat\nAr-Ge Departmanı\nAdresGezgini A.S¸.\n˙Izmir, T¨urkiye\ngokcepolat@adresgezgini.com\nS¸ ¨ukr¨u Ozan\nAr-Ge Departmanı\nAdresGezgini A.S¸.\n˙Izmir, T¨urkiye\nsukruozan@adresgezgini.com\n¨Oz—Bu\nc¸alıs¸mada,\nc¸evrimic¸i\nreklam\nplatformlarında\nolus¸turulan reklam metinlerinin sekt¨ore g¨ore otomatik olarak\nsınıﬂandırılması ic¸in do˘gal dil is¸leme (NLP) tabanlı bir y¨ontem\n¨onerilmis¸tir. E˘gitim veri setimiz 12 farklı sekt¨ore ait yaklas¸ık\n21.000 etiketli reklam metninden olus¸maktadır. C¸ alıs¸mada son\nyıllarda do˘gal dil is¸leme literat¨ur¨unde metin sınıﬂandırma gibi\nalanlarda sıkc¸a kullanılan d¨on¨us¸t¨ur¨uc¨u (transformers) tabanlı\nbir dil modeli olan Transformat¨orlerden C¸ ift Y¨onl¨u Kodlayıcı\nG¨osterimleri (BERT) modeli kullanılmıs¸tır. T¨urkc¸e diline y¨onelik\nolarak ¨onceden e˘gitilmis¸ olarak sec¸ilmis¸ bir BERT modelini\nkullanarak elde edilen sınıﬂandırma bas¸arımları detaylı olarak\ng¨osterilmis¸tir.\nAnahtar S¨ozc¨ukler—Dijital Pazarlama, Reklam Metni, Do˘gal\nDil ˙Is¸leme, Metin Sınıﬂandırma, Transformat¨orlerden C¸ ift Y¨onl¨u\nKodlayıcı G¨osterimleri.\nAbstract—In this study, a natural language processing-based\n(NLP-based) method is proposed for the sector-wise automatic\nclassiﬁcation of ad texts created on online advertising platforms.\nOur data set consists of approximately 21,000 labeled advertising\ntexts from 12 different sectors. In the study, the Bidirectional En-\ncoder Representations from Transformers (BERT) model, which\nis a transformer-based language model that is recently used in\nﬁelds such as text classiﬁcation in the natural language processing\nliterature, was used. The classiﬁcation efﬁciencies obtained using\na pre-trained BERT model for the Turkish language are shown\nin detail.\nKeywords—Digital Marketing, Ad Text, Natural Language\nProcessing, Text Classiﬁcation, Bidirectional Encoder Represen-\ntations from Transformers.\nI. G˙IR˙IS¸\nYıllardır reklam sekt¨or¨un¨un ¨onde gelen medyaları olan\nTV ve gazete gibi geleneksel yayın organları g¨un¨um¨uzde\nartık etkisini kaybetmeye bas¸lamıs¸tır. eMarketer’ın verilerine\ng¨ore c¸evrimic¸i reklamcılı˘gının g¨un¨um¨uzde en b¨uy¨uk reklam\nalanı olması beklenmektedir [1]. ˙Internet ekonomisinin hızla\nb¨uy¨umesi, reklam sekt¨or¨un¨un d¨on¨us¸¨um¨unde oldukc¸a b¨uy¨uk bir\netkiye sahiptir.\nGelis¸en teknoloji ve internetin yaygın kullanımı diji-\ntal pazarlamayı,\n¨ur¨un ve hizmetlerini internet\n¨uzerinden\nt¨uketicilere sunmak isteyen is¸letmeler ic¸in ¨onemli bir arac¸\nhaline getirmekte, ilgilenmesi muhtemel potansiyel kis¸ilere\ng¨osterilmesinin yanı sıra reklam performansının takibine ve\nanalizine olanak sa˘glamaktadır [2]. T¨um bunlarla birlikte\nc¸evrimic¸i reklamcılıkta, yatırım ve d¨on¨us¸¨um ilis¸kisinin in-\ncelendi˘gi etkili reklam c¸alıs¸maları ¨uzerine birc¸ok aras¸tırma\nyapılmaya bas¸lanmıs¸tır.\nBu aras¸tırmalar kapsamında insanların ﬁkirlerini, tutum-\nlarını, duygularını, varlıklara ve bireylere y¨onelik dosyala-\nmalarını analiz etmek, web sitesi ic¸erikleri, reklam metinleri,\nkullanıcı hareketleri, cihaz bilgisi, konum bilgisi veya de-\nmograﬁk veriler gibi veriler\n¨uzerinden anlamlı sonuc¸lar\nc¸ıkarmak ic¸in Makine ¨O˘grenimi Algoritmaları ve Do˘gal Dil\n˙Is¸leme (NLP) teknikleri kullanılmaya bas¸lanmıs¸tır [3].\nKaliteli\nbir\nreklam\nc¸alıs¸masının\nen\n¨onemli\n¨ozelli˘gi\nt¨uketiciye\nsunulan\nic¸eri˘gin\nalakalı\nve\netkili\nolmasıdır.\nC¸ evrimic¸i reklamcılık platformlarında reklam kalitesi; reklam\nmetinlerinin aramalarla ne kadar alakalı oldu˘gu, t¨uketicilerin\nreklama tıklanma olasılı˘gı ve reklam tıklandıktan sonra\ng¨or¨unt¨ulenen sayfada (landing page) yas¸anan deneyim gibi\nbirc¸ok farklı fakt¨ore ba˘glıdır. Y¨uksek reklam kalitesi daha iyi\nreklam konumu ve daha d¨us¸¨uk maliyet ile sonuc¸lanmaktadır\n[4]. Bu noktada t¨uketici ihtiyac¸larına ve aramalarına ce-\nvap veren reklam metinlerinin hazırlanması, reklamverenin\nsundu˘gu ¨ur¨un¨un/hizmetin potansiyel m¨us¸terilere etkili bir\ns¸ekilde aktarılmasında ¨onemli bir rol oynamaktadır.\narXiv:2106.10899v2  [cs.CL]  23 Jun 2021\nC¸ alıs¸mamızda reklam kalitesini artırdı˘gı bilinen reklam\nmetinlerinin hangi sekt¨or ile alakalı oldu˘gunu tespit etmek\namacıyla bir sınıﬂandırma y¨ontemi\n¨onerilmis¸tir. C¸ alıs¸ma\ngerc¸ekles¸tirilirken,\nbenzer\nsınıﬂandırma\nproblemlerinde\nWord2vec,\nLSTM\ngibi\nmodellerinden\ndaha\niyi\nsonuc¸\nverdi˘gi [5] c¸alıs¸masında da irdelenmis¸ olan d¨on¨us¸t¨ur¨uc¨u\n(transformers) tabanlı bir dil modeli olan Transformat¨orlerden\nC¸ ift Y¨onl¨u Kodlayıcı G¨osterimleri (BERT) [6] kullanılmıs¸tır.\nII. L˙ITERAT ¨UR ANAL˙IZ˙I\nSon yıllarda bilgisayarların hesaplama kapasitlerinin ¨ustel\nbir s¸ekilde artması ve yapay zeka ile derin ¨o˘grenme alanlarında\nhızlı gelis¸melere paralel olarak, NLP alanında da birc¸ok\nyeni mimari yapı gelis¸tirilmis¸tir. Sinir a˘glarının, verilerden\n¨ozniteliklerini ¨o˘grenme konusunda y¨uksek kapasitelerinin ol-\nması sayesinde derin ¨o˘grenme, NLP alanının yanında birc¸ok\nalanda aras¸tırma konusu olmus¸tur. Ana sinir a˘gları, do˘gru\nba˘glamı bulmak ic¸in sıklıkla kullanılan Uzun Kısa S¨ureli\nBellek (LSTM) [7], dikkat mekanizması [8] ve bellek a˘glarıdır\n[9]. Yinelemeli sinir a˘gları [10], kapalı sinir a˘gları [11] ve\nevris¸imli sinir a˘gları [12] da NLP alanında kullanılan sinir\na˘glarındandır.\nNergiz vd., [13] c¸alıs¸masında, Fasttext, Word2vec ve\nDoc2vec modellerinin T¨urkc¸e haber sitelerinde yer alan farklı\nkategorilere ait haber metinleri ¨uzerindeki bas¸arı oranlarını\nLSTM sinir a˘gı ile test ederek kars¸ılas¸tırmıs¸ ve en bas¸arılı\nsonucu Fasttext modelinin sa˘gladı˘gını sunmus¸tur.\nDo˘gru vd. [14] c¸alıs¸masında, T¨urkc¸e ve ingilizce haber\nmetinleri ¨uzerinde Doc2vec kelime g¨omme y¨ontemi ile e˘gitim\nmodeli olus¸turulmus¸, derin ¨o˘grenme sınıﬂandırma y¨ontemi\nCNN ve makine ¨o˘grenmesi sınıﬂandırma y¨ontemleri Gauss\nNaive Bayes (GNB), Random Forest (RF), Naive Bayes (NB)\nve Support Vector Machine (SVM) ile modelin do˘gruluk\noranlarını kars¸ılas¸tırarak en y¨uksek sonucun CNN ile yapılan\nsınıﬂandırmada elde edildi˘gi g¨osterilmis¸tir.\nGonz´alez-Carvajal ve Garrido-Merch´an, [15] c¸alıs¸masında,\nsosyal medyada paylas¸ılan iletiler, ﬁlm-dizi eles¸tirileri, haber\nic¸erikleri gibi birc¸ok alan ¨uzerinde BERT modeli ve geleneksel\ndo˘gal dil is¸leme yaklas¸ımlarının kars¸ılas¸tırmalı analizlerini\ngerc¸ekles¸tirmis¸ ve sonuc¸larını sunmus¸tur. Sonuc¸lar BERT’in\ngeleneksel NLP yaklas¸ımlarından farklı alanlarda daha iyi\nsonuc¸lar verdi˘gi ampirik de˘gerler ile kanıtlanmıs¸tır.\nZhengjie vd. [16] c¸alıs¸masında duygu analizine ba˘glı\nsınıﬂandırma yapmak ic¸in BERT modelinin hedefe ba˘glı\n¨uc¸\nvaryasyonu\nkullanılmıs¸tır.\n¨Uc¸\nveri\nk¨umesi\n¨uzerinde\nyapılan deneyler sonucunda modelin c¨umle d¨uzeyinde du-\nyarlılık sınıﬂandırması dahil olmak ¨uzere birc¸ok NLP prob-\nleminde mevcut y¨ontemlere g¨ore daha iyi sonuc¸lar verdi˘gi\ng¨osterilmis¸tir.\nIII. TASARIM VE Y ¨ONTEM\nA. Veri Seti\nKullanılan veri seti, 12 farklı is¸ sekt¨or¨une ait yaklas¸ık\n21.000 reklam metninden olus¸maktadır. ¨On is¸leme as¸amasında\nS¸ekil. 1. Her bir kategoride bulunan reklam metin sayısı\nS¸ekil. 2. T¨um reklam metinlerindeki kelime sayısı da˘gılımı\nveri set ¨oncelikle tekrar eden verilerden arındırılmıs¸tır. Nok-\ntalama is¸aretleri ve semboller c¸ıkarılmıs¸tır. Veri setinde kul-\nlanılan 12 temel kategori ic¸in rastgele sec¸ilmis¸ ¨uc¸ reklam\nmetni Tablo I’de g¨or¨ulmektedir. Bunun yanında S¸ekil.1 ve\nS¸ekil. 2 den de anlas¸ılabilece˘gi gibi her bir kategorinin farklı\nsayıda reklam metnine sahip olması ve reklam metinlerinin\nfarklı sayıda kelimeye sahip olması kullanılan veri setindeki\ndengesizli˘gi g¨ostermektedir. Bu dengesiz veri seti ile e˘gitilmis¸\nBERT modelinden alınan kategorik olasılıksal sınıﬂandırma\ny¨onteminin c¸ıktıları analiz edilerek ¨ust¨un bir bas¸arı g¨osterdi˘gi\ng¨ozlemlenmis¸tir.\nB. Y¨ontem\n1) BERT:\nTransformat¨orlerden\nC¸ ift\nY¨onl¨u\nKodlayıcı\nG¨osterimleri (BERT), etiketlenmemis¸ metinden c¸ift y¨onl¨u\ng¨osterimleri\n¨onceden e˘gitmek ve sonrasında farklı NLP\ng¨orevleri ic¸in etiketli metin kullanılarak ince ayar yap-\nmak ic¸in tasarlanmıs¸ [6] ve Google tarafından 2018 yılında\nsunulmus¸ bir d¨on¨us¸t¨ur¨uc¨u modelidir. D¨on¨us¸t¨ur¨uc¨uler, kuyruk\nyapısıyla olus¸turulmus¸ ¨oz dikkat mekanizması ile c¸alıs¸an\nbir yapıya sahiptir S¸ekil. 3. BERT modeli, bir sorguyu\nve bir dizi anahtar-de˘ger c¸iftini bir c¸ıktıya es¸ler. Burada\nsorgu, anahtarlar, de˘gerler ve c¸ıktının kendi arasındaki ko-\nrelasyonu ifade edebilecek vekt¨orler olus¸maktadır. C¸ ıktı,\nde˘gerlerin a˘gırlıklı toplamı ve bir de˘gere atanan a˘gırlık,\nsorguya kars¸ılık gelen anahtarla uyumluluk oranı ile hesaplanır\n[17]. BERT modeli, bir metni hem sa˘gdan sola hem de\nsoldan sa˘ga is¸lemektedir, bu sayede metin ic¸erisindeki ¨o˘geler\narasındaki ilis¸kileri ¨o˘grenebilmektedir. E˘gitim as¸amasında,\nMLM (Masked Language Modeling) ve NSP (Next Sentence\nPrediction)\nteknikleri\nkullanılmaktadır.\nMLM\ntekni˘ginde,\nTABLO I\nRASTGELE SEC¸ ILMIS¸ VERI SETI ¨ORNE ˘GI\nID\nKategori\nReklam Metni\nC¸ ic¸ek ve C¸ ikolata Hediyeleri\n0\nC¸ ic¸ek Siparis¸i\n¨Ozel G¨unlerinize, ¨Ozel C¸ ic¸ekler\nEs¸siz C¸ ic¸ek Tasarımları\nHayvan Dostunuz ic¸in En ˙Iyisi\n1\nEvcil Hayvan ¨Ur¨unleri\nK¨opek Maması ve Vitaminleri\nT¨urkiye’nin Rakipsiz Petshop’u\n¨Ozel Tas¸lı Bileklikler\n2\nM¨ucevher, Takı & Aksesuar\nSize En Yakıs¸an Saat Burada!\nEn Ucuz & En Yeni Kolye Modelleri.\nNakliye ve Depolamada ¨Onc¨u Firma\n3\nNakliyat, Kargo\nG¨uvenilir ve Hızlı Kargo\nPaketleyerek Profesyonel Tas¸ıma.\nSon Teknoloji Hidrolik Ya˘glar\n4\nOto Aksesuar & Yedek Parc¸a\nArac¸ Koltuk Temizli˘gi\nAradı˘gınız T¨um Jant C¸ es¸itleri\nOyunun Heyecanına Katılın\n5\nOyunlar,\nEn iyi Korku Evi\nOyun Konsolları & Ekipmanları\nOyunlarda Uygun Fiyatlar!\n˙Ilac¸sız Terapi Y¨ontemi\n6\nPsikolojik Danıs¸manlık\n¨Ofke Kontrol¨u Y¨ontem & Tedavi\nKis¸iye ¨Ozel Klinik Terapi Y¨ontemleri\nProfesyonel Yıkama Hizmetleri\n7\nTemizlik & Halı Yıkama\nTemizlik Malzemeleri ve Aparatlar.\nUygun Fiyatlarla M¨ukemmel Temizlik!\nYurtic¸i & Yurtdıs¸ı Tur Fırsatı\n8\nTur Acenteleri\nAvantajlı Otel & Tatil Fırsatı\nAradı˘gınız Konaklama Fırsatları\nHızlı & Kolay Vize ˙Is¸lemleri\n9\nVize ˙Is¸lemleri\nEvraksız Rusya Vizesi\nVize ˙Is¸lemlerinin Do˘gru Adresi\nM¨ufredata Uygun E˘glenerek ¨O˘grenme\n10\nYabancı Dil E˘gitimi\nErken Yas¸ta ˙Ingilizce Dil E˘gitimi\nKonus¸ma Odaklı Drama ile ˙Ingilizce\n¨Ucretsiz wireless & C¸ alıs¸ma Odaları\n11\nYurtlar\nEvinizdeki Konfor ic¸in Bize Ulas¸ın!\n7/24 Sıcak Su & G¨uvenlik Hizmeti.\nmaskelenen kelimeler, ac¸ık (maskelenmeyen) kelimeler kul-\nlanılarak tahmin edilmeye c¸alıs¸ılmaktadır. Bu teknik ile\nc¨umle ic¸indeki kelimeler ¨uzerinden inceleme ve tahminleme\nyapılmaktadır. NSP tekni˘ginde, c¨umlelerin birbirleri ile ilis¸kisi\nincelenmektedir. Bir c¨umlenin kendisinden sonra gelen c¨umle\nile ilis¸kisi incelenmektedir. BERT modeli ile kurulan yapılarda\n¨onceden e˘gitilmis¸ bir model gereksinimi vardır. Bu sebe-\nple c¸alıs¸mamızda Loodos takımı tarafından T¨urkc¸e dili ic¸in\n¨onceden MLM tekni˘gi ile e˘gitilmis¸ bir BERT modeli olan\nBERT-BASE-TURKISH-UNCASED [18] modelini tercih et-\ntik. Kullanılan modelin ¨on e˘gitim parametreleri Tablo II’de\nverilmis¸tir. Bu model ¨uzerinde ince ayar is¸lemi yapılarak\nkullanıma hazır hale getirilmis¸tir.\n2) F1 Skoru ve Bas¸arım Kriterleri:\nE˘gitilen modelin\nbas¸arısı: do˘gruluk de˘geri, F1 skoru ve hata dizeyi (confusion\nmatrix) ile ¨olc¸¨ulm¨us¸t¨ur. Bas¸arı ¨olc¸¨umleri: True Positive (TP),\nFalse Positive (FP), True Negative (TN) ve False Negative\n(FN) de˘gerleri ile ¨olc¸¨ulmektedir. TP, modelin tahmini ve\ngerc¸ek de˘gerin her ikisininde olumlu sonuc¸ vermesi TN, mod-\nelin tahmini ve gerc¸ek de˘gerin her ikisininde olumsuz sonuc¸\nvermesi, FP model tahmini olumlu iken gerc¸ek de˘gerin olum-\nsuz sonuc¸ vermesi ve FN ise modelin tahmini olumsuz iken\ngerc¸ek de˘gerin olumlu sonuc¸ vermesi s¸eklinde ac¸ıklanabilir.\nS¸ekil. 3. D¨on¨us¸t¨ur¨uc¨u Model Mimarisi [17]\nTABLO II\nKULLANILAN ¨ON E ˘GITIMLI BERT MODEININ PARAMETRELERI\nModel\nGizli\nKatman\nBoyutu\nMaks.\nSekans\nUzunlu˘gu\nDikkat\nAna\nBas¸lı˘gı\nSayısı\nGizli\nKatman\nSayısı\nMimari\nS¨ozl¨uk\nBoyutu\n(Loodos\n2020)\n768\n512\n12\n12\nBert\nFor\nMasked\nLM\n32000\n*Gizli Katman Boyutu: Kodlayıcı katmanlarının ve havuzlama katmanlarının boyutları\nMaksimum Sekans Uzunlu˘gu: Modelin kullanabilece˘gi maksimum sekans uzunlu˘gu\nDikkat Ana Bas¸lı˘gı Sayısı: D¨on¨us¸t¨ur¨uc¨u kodlayıcısı ic¸erisindeki her dikkat katmanı ic¸in dikkat ana bas¸lı˘gı sayısı.\nGizli katman Sayısı: D¨on¨us¸t¨ur¨uc¨u kodlayıcısı ic¸erisindeki gizli katmanların sayısı.\nBu durumda TP ve TN do˘gru sonuc¸, FP ve FN ise yanlıs¸\nsonuc¸ kabul edilir.\nDo˘gruluk de˘geri hesaplanırken, modelin do˘gru tahmin etti˘gi\nTP ve TN de˘gerlerinin, tahmin edilen t¨um TP, TN, FP,\nFN de˘gerlerine oranı ile hesaplanmaktadır (1). Ancak c¸ok\nsınıﬂı problemlerde bu kriter ile sınıf bazlı do˘gruluk de˘geri\nalınamamaktadır. Bu sıkıntıyı ortadan kaldırmak amacıyla F1\nskoru adı verilen kesinlik-do˘gruluk de˘gerlerine dayanan bir\nbas¸arı kriteri kullanılmaktadır.\nDo˘gruluk =\nTP + TN\nTP + TN + FP + FN\n(1)\nKesinlik (precision) de˘geri, modelin tahmin etti˘gi TP\nde˘gerlerin sayısının, modelin ¨uretti˘gi t¨um olumlu sonuc¸lar\nolan TP ve FP de˘gerlerinin sayısına oranıdır (2). Duyarlılık\nde˘geri ise modelin tahmin etti˘gi TP de˘gerlerinin sayısının,\nmodelin ¨uretmesi gereken t¨um olumlu sonuc¸lar olan TP ve FN\nsayılarına oranı ile bulunabilir (3). F1 skoru ise kesinlik ve du-\nyarlılık de˘gerlerinin harmonik ortalaması olarak tanımlanabilir\n(4).\nKesinlik =\nTP\nTP + FP\n(2)\nDuyarlılık =\nTP\nTP + FN\n(3)\nF1Skoru = 2\n\u0012Kesinlik × Duyarlılık\nKesinlik + Duyarlılık\n\u0013\n(4)\nC¸ ok sınıﬂı verilerin dengesiz da˘gıldı˘gı c¸alıs¸malar, modelin\nbas¸arımını do˘gru bir s¸ekilde de˘gerlendirmek ic¸in bir sorun\ntes¸kil etmektedir. Bu durumu ortadan kaldırmak adına F1 sko-\nrunun sınıf bazlı veri da˘gılımına g¨ore a˘gırlıklı ortalamasının\nalınarak hesaplanabilir.\nIV. BULGULAR VE TARTIS¸ MA\nYapılan\nanalizler\nsonucunda,\nc¸alıs¸mada\nkullandı˘gımız\nT¨urkc¸e dili ic¸in ¨on e˘gitime tabi tutulmus¸ BERT modeli, veri\nsetimizle ince ayar is¸lemine tabi tutulmus¸tur. ˙Ince ayar is¸lemi\nBERT k¨ut¨uphanesindeki kategorik sınıﬂandırma y¨ontemi ile\ngerc¸ekles¸tirilmis¸tir. Bu y¨ontemde veri seti ¨onceden belir-\nlenen kategorilere ayrılarak, veri setinin %70’i e˘gitim veri\nseti,%30’u ise modelin test edilmesi ic¸in test e˘gitim seti\nolarak kullanılmıs¸tır. E˘gitim-test ayrıs¸tırması kategorik olarak\nyapıldı˘gı ic¸in her kategoriden es¸it oranda veri ile e˘gitim\nve test gerc¸ekles¸tirilmis¸tir. Daha sonra 14349 e˘gitim verisi\nve 3588 test verisi ile model 10 iterasyonda e˘gitildikten\nsonra, modelin 3. iterasyonundan sonra, do˘gruluk kesinlik,\nduyarlılık ve F-1 de˘gerlerinde bir artıs¸ olmamasından ¨ot¨ur¨u 3.\niterasyonun sonuc¸ları ¨uzerinden analizler gerc¸ekles¸tirilmis¸tir.\nBurada farklı e˘gitim - test seti b¨ol¨unme oranları kars¸ılas¸tırılmıs¸\nolsa da literat¨urde yapılmıs¸ benzer c¸alıs¸malarda kullanılan\n%70 - %30 oranı ile uyumlu kalınarak sonuc¸ların ¨uretilmesi\nhedeﬂenmis¸tir.\nTest\nveri\nsetindeki\nkategorilerin\nkesinlik\nde˘gerleri arasındaki en d¨us¸¨uk de˘ger 25 adet test verisi ic¸eren\n5 numaralı kategori ic¸in gerc¸ekles¸mis¸tir. Duyarlılık analizinde\nyine en d¨us¸¨uk de˘ger 1 numaralı kategori ic¸in 97 adet test\nverisi olan sınıfta bulunmus¸tur. F-1 skoru ic¸in ise yine 5\nnumaralı kategoride minimum de˘gerle kars¸ılas¸ılmıs¸tır. Mod-\nelin en d¨us¸¨uk bas¸arım g¨osteren kategorilerinin e˘gitim ve\ntest veri seti ic¸erisinde en d¨us¸¨uk orana sahip olan veril-\nerden olus¸tu˘gu g¨or¨ulmektedir. Buradan yola c¸ıkarak benzer\nc¸alıs¸malar ic¸in ideal olan e˘gitim ve test verisinin kategori\nbazında 1000’den ¨uzeri oldu˘gu durumlarda %90 ¨uzerinde\nbir do˘grulu˘ga ulas¸abilece˘gi g¨or¨ulmektedir. Tablo III’te 3.\niterasyona ait sınıﬂandırma raporu, S¸ekil 4’te ise ilk 3 it-\nerasyon ic¸in olan hata dizeyleri g¨or¨ulmektedir. Burada da\ng¨or¨uld¨u˘g¨u gibi model ilk iterasyondan itibaren y¨uksek bir\ne˘gitim do˘grulu˘guna ulas¸makta, ancak, 3. iterasyondan sonra\ny¨uksek bir test do˘grulu˘gu kazanacak d¨uzeye eris¸ebilmektedir.\nTABLO III\n3.˙ITERASYON TEST RAPORU\nID\nKesinlik\nDuyarlılık\nF1 Skoru\nReklam Metinleri\n0\n0.90\n0.95\n0.92\n353\n1\n0.99\n0.77\n0.87\n97\n2\n0.92\n0.92\n0.92\n414\n3\n0.93\n0.87\n0.90\n378\n4\n0.88\n0.87\n0.87\n187\n5\n0.78\n0.84\n0.81\n25\n6\n0.95\n0.94\n0.90\n474\n7\n0.87\n0.93\n0.91\n560\n8\n0.92\n0.90\n0.94\n444\n9\n0.91\n0.98\n0.94\n168\n10\n0.94\n0.93\n0.94\n363\n11\n0.87\n0.82\n0.84\n125\nDo˘gruluk\n0.91\n3588\nA˘g. Ort\n0.91\n0.91\n0.91\n3588\nV. SONUC¸ , ¨ONER˙ILER VE KISITLAR\nC¸ evrimic¸i\nreklam\nplatformlarında\nolus¸turulan\nreklam\nmetinlerinin sekt¨ore g¨ore otomatik olarak sınıﬂandırılması\nic¸in gerc¸ekles¸tirdi˘gimiz bu do˘gal dil is¸leme uygulaması, veri\nsetimizdeki 12 farklı sekt¨ore ait yaklas¸ık 21.000 etiketli reklam\nmetnini %90’ın ¨uzerinde bir do˘gruluk ile ait oldu˘gu kate-\ngoriye g¨ore sınıﬂandırabilmis¸tir. Bu y¨ontemi gerc¸ekles¸tirirken\ng¨un¨um¨uzde\ndo˘gal\ndil\nis¸leme\nalanında\nsıkc¸a\nkullanılan\nd¨on¨us¸t¨ur¨uc¨u (transformers) tabanlı bir dil modeli olan Trans-\nformat¨orlerden C¸ ift Y¨onl¨u Kodlayıcı G¨osterimleri (BERT)\nmodeli veri setimiz ile ince ayar yapılarak kullanılmıs¸tır.\nGerc¸ekles¸tirdi˘gimiz bu sınıﬂandırma kapalı bir alan ic¸erisinde\nve ¨onceden belirlenmis¸ kategoriler ic¸in gerc¸ekles¸tirilmis¸tir.\nY¨ontem, c¸evrimic¸i reklam a˘glarında kullanılacak olan metin\ntabanlı reklamların, reklamverenin sekt¨or¨u ile uyumlulu˘gunun\ntest edilebilmesi ic¸in kullanılabilecektir. Bu sayede c¸evrimic¸i\nreklam kalitesinin belirlenmesinde ¨onemli bir biles¸en olan\nreklam metninin sekt¨or ile uygunlu˘gu durumu otomatik bir\ns¸ekilde test edilebilecektir.\nKAYNAKC¸ A\n[1] Emarketer,, “Advertisers will spend nearly $600 billion worldwide in\n2015,” 2014.\n[2] “1.1 C¸ evrimic¸i reklamcılık ve Google Ads’in avantajları - Google Ads\nYardım,” https://support.google.com/google-ads/answer/6123875?hl=tr,\nMay 2021.\n[3] Z, A. and Adali, E., “Opinion mining and sentiment analysis for contex-\ntual online-advertisement,” in 2016 IEEE 10th International Conference\non Application of Information and Communication Technologies (AICT).\nIEEE, 2016, pp. 1–3.\n[4] “Reklam kalitesi hakkında - Google Ads Yardım,” https://support.google.\ncom/google-ads/answer/156066?hl=tr&amp;ref topic=10549746,\nMay\n2021.\n[5] S¸ ¨ukr¨u Ozan ve Emre Tas¸ar,,\n“Kısa konus¸ma c¨umlelerinin do˘gal dil\n˙Is¸leme y¨ontemlerini kullanarak otomatik etiketlenmesi,” 29. IEEE Sinyal\n˙Is¸leme ve ˙Iletis¸im Uygulamaları Kurultayı, SIU 2021, C¸ evrimic¸i, Basım\nas¸amasında bildiri., 2021.\n[6] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[7] Hochreiter, S. and Schmidhuber, J., “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[8] Chen, P., Sun, Z., Bing, L., and Yang, W., “Recurrent attention network\non memory for aspect sentiment analysis,” in Proceedings of the 2017\nconference on empirical methods in natural language processing, 2017,\npp. 452–461.\n[9] Rajpurkar, P., Jia, R., and Liang, P.,\n“Know what you don’t know:\nUnanswerable questions for squad,” arXiv preprint arXiv:1806.03822,\n2018.\n[10] Dong, L., Wei, F., Tan, C., Tang, D., Zhou, M., and Xu, K., “Adaptive\nrecursive neural network for target-dependent twitter sentiment classiﬁ-\ncation,” in Proceedings of the 52nd annual meeting of the association\nfor computational linguistics (volume 2: Short papers), 2014, pp. 49–54.\n[11] Xue, W. and Li, T.,\n“Aspect based sentiment analysis with gated\nconvolutional networks,” arXiv preprint arXiv:1805.07043, 2018.\n[12] Huang, B. and Carley, K. M.,\n“Parameterized convolutional neural\nnetworks for aspect level sentiment classiﬁcation,”\narXiv preprint\narXiv:1909.06276, 2019.\n[13] Nergız, G., Safali, Y., Avaro˘glu, E., and Erdo˘gan, S., “Classiﬁcation of\nturkish news content by deep learning based lstm using fasttext model,”\nin 2019 International Artiﬁcial Intelligence and Data Processing Sym-\nposium (IDAP). IEEE, 2019, pp. 1–6.\n[14] Dogru, H. B., Tilki, S., Jamil, A., and Hameed, A. A., “Deep learning-\nbased classiﬁcation of news texts using doc2vec model,” in 2021 1st\nInternational Conference on Artiﬁcial Intelligence and Data Analytics\n(CAIDA). IEEE, 2021, pp. 91–96.\n[15] Gonz´alez-Carvajal, S. and Garrido-Merch´an, E. C., “Comparing bert\nagainst traditional machine learning text classiﬁcation,” arXiv preprint\narXiv:2005.13012, 2020.\n[16] Gao, Z., Feng, A., Song, X., and Wu, X., “Target-dependent sentiment\nclassiﬁcation with bert,” IEEE Access, vol. 7, pp. 154290–154299, 2019.\n[17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, L., and Polosukhin, I., “Attention is all you need,” arXiv\npreprint arXiv:1706.03762, 2017.\n[18] Loodos,,\n“loodos/bert-base-turkish-uncased · hugging face,” https://\ngithub.com/Loodos/turkish-language-models, Aug 2020.\nS¸ekil. 4. E˘gitim ve Test Verilerine ait Hata Dizeyleri.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-06-21",
  "updated": "2021-06-23"
}