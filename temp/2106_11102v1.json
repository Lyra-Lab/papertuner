{
  "id": "http://arxiv.org/abs/2106.11102v1",
  "title": "Low-rank Dictionary Learning for Unsupervised Feature Selection",
  "authors": [
    "Mohsen Ghassemi Parsa",
    "Hadi Zare",
    "Mehdi Ghatee"
  ],
  "abstract": "There exist many high-dimensional data in real-world applications such as\nbiology, computer vision, and social networks. Feature selection approaches are\ndevised to confront with high-dimensional data challenges with the aim of\nefficient learning technologies as well as reduction of models complexity. Due\nto the hardship of labeling on these datasets, there are a variety of\napproaches on feature selection process in an unsupervised setting by\nconsidering some important characteristics of data. In this paper, we introduce\na novel unsupervised feature selection approach by applying dictionary learning\nideas in a low-rank representation. Dictionary learning in a low-rank\nrepresentation not only enables us to provide a new representation, but it also\nmaintains feature correlation. Then, spectral analysis is employed to preserve\nsample similarities. Finally, a unified objective function for unsupervised\nfeature selection is proposed in a sparse way by an $\\ell_{2,1}$-norm\nregularization. Furthermore, an efficient numerical algorithm is designed to\nsolve the corresponding optimization problem. We demonstrate the performance of\nthe proposed method based on a variety of standard datasets from different\napplied domains. Our experimental findings reveal that the proposed method\noutperforms the state-of-the-art algorithm.",
  "text": "Low-rank Dictionary Learning for Unsupervised Feature\nSelection\nMohsen Ghassemi Parsaa, Hadi Zarea,∗, Mehdi Ghateeb\naFaculty of New Sciences and Technologies, University of Tehran, Tehran, Iran\nbDepartment of Mathematics and Computer Science, Amirkabir University of\nTechnology, Tehran, Iran\nAbstract\nThere exist many high-dimensional data in real-world applications such as\nbiology, computer vision, and social networks. Feature selection approaches\nare devised to confront with high-dimensional data challenges with the aim\nof eﬃcient learning technologies as well as reduction of models complexity.\nDue to the hardship of labeling on these datasets, there are a variety of\napproaches on feature selection process in an unsupervised setting by consid-\nering some important characteristics of data. In this paper, we introduce a\nnovel unsupervised feature selection approach by applying dictionary learn-\ning ideas in a low-rank representation. Dictionary learning in a low-rank\nrepresentation not only enables us to provide a new representation, but it\nalso maintains feature correlation. Then, spectral analysis is employed to\npreserve sample similarities. Finally, a uniﬁed objective function for unsu-\npervised feature selection is proposed in a sparse way by an ℓ2,1-norm regu-\nlarization. Furthermore, an eﬃcient numerical algorithm is designed to solve\nthe corresponding optimization problem. We demonstrate the performance\nof the proposed method based on a variety of standard datasets from dif-\nferent applied domains. Our experimental ﬁndings reveal that the proposed\nmethod outperforms the state-of-the-art algorithm.\nKeywords:\nUnsupervised feature selection, Dictionary Learning, Sparse\nLearning, Spectral analysis, Low-rank representation\n∗Corresponding author\nEmail addresses: mgparsa@ut.ac.ir (Mohsen Ghassemi Parsa), h.zare@ut.ac.ir\n(Hadi Zare ), ghatee@aut.ac.ir (Mehdi Ghatee)\narXiv:2106.11102v1  [cs.LG]  21 Jun 2021\n1. Introduction\nTechnological advancement and popularity of social networks provide\nmany huge and high-dimensional data and information sources. High-dimensional\ndata are available in many applications, including machine vision (Shi et al.,\n2015), text mining (Rogati & Yang, 2002), and biology (Hoseini & Mansoori,\n2019). High-dimensionality not only increases the complexity of the train-\ning process and the learned model but also degrades performance, that is\ncalled as the curse of dimensionality (Murphy, 2012). To address the issue,\ndimensionality reduction can be considered in two main approaches, feature\nextraction (FE) and feature selection (FS) (Pandit et al., 2020). The new\nfeatures are constituted by a linear or non-linear transformation of the orig-\ninal features in FE approaches, while FS methods aim to select appropriate\nfeatures by considering some evaluation criteria. FS attains more attraction\nthan FE in some situations, speciﬁcally when the primary aim is to take ad-\nvantage of more interpretable and understandable features (Li et al., 2017a).\nFS methods can be classiﬁed to ﬁlter, wrapper, and embedded approaches\naccording to feature evaluation. Filters (Krzanowski, 1987; He et al., 2005;\nZare & Niazi, 2016) exploit data properties to ﬁnd out the importance of the\nfeatures, while wrappers (Dy & Brodley, 2004) evaluate the feature subsets\nby a learning algorithm. In embedded methods (Li et al., 2012), the feature\nselection process is embedded in a learning algorithm. Recently, unsupervised\nfeature selection (UFS) has attracted many eﬀorts among researchers due to\nthe unavailability of the right answers on practical domains and real-world\napplications (Parsa et al., 2020; Zare et al., 2020).\nUFS algorithms are mainly categorized into similarity preserving, sparse\nlearning, reconstruction, and dictionary learning methods. Similarity pre-\nserving methods (He et al., 2005; Zhao & Liu, 2007) are tried to maintain\nthe local geometric structures among the selected features. Sparse learning\nmethods (Parsa et al., 2020; Zare et al., 2020) considered selecting more rel-\nevant features in a regularized way. Data reconstruction methods (Masaeli\net al., 2010; Farahat et al., 2013) re-express the features to eliminate unin-\nformative ones. One of the most important approaches in UFS is based on\ndictionary learning (Zhu et al., 2016, 2017; Ding et al., 2020), in which a new\nsparse representation of the data matrix is obtained on a dictionary basis\nspace.\nMost of the earlier dictionary learning approaches proposed to build the\ndictionary matrix without any restriction on the rank of the basis matrix. A\n2\nmore natural assumption is to employ the low-rank representation on high-\ndimensional data to alleviate the noisy and redundant features (Chen &\nHuang, 2012). In addition, the basis matrix can be learned in a parsimonious\nway by imposing the rank constraint, which can be improved the learning\nprocess. In this paper, we propose a dictionary learning-based unsupervised\nfeature selection method, named DLUFS, to provide a sparse representation\nof the original data. Furthermore, we employ a low-rank constraint to elim-\ninate the noisy and redundant features. The local sample structure is also\nconsidered by exploiting a spectral analysis.\nWe summarize the main contributions of this paper as,\n• A dictionary learning method is proposed to select features to obtain\na sparse representation of data.\n• Low-rank constraint on the basis matrix is imposed to eliminate the\nnoisy and redundant features.\n• Spectral analysis is employed to preserve the local similarities among\nsamples.\nThis paper is organized as follows. The existing UFS methods are re-\nviewed in Section 2.\nThe proposed method, an illustrative example, and\nthe corresponding optimization algorithm are presented in Section 3. We\nanalyze the convergence behavior of the proposed algorithm in Section 4.\nSection 5 presents the experimental results on benchmark datasets based on\nstate-of-the-art methods. The conclusions are given in Section 6.\n2. Related Works\nIn this section, we review unsupervised feature selection methods in four\ncategories, similarity preserving, sparse machine learning, data reconstruc-\ntion, and dictionary learning methods.\nSimilarity preserving methods consider the sample structure in the se-\nlected features, such as Laplacian score, LS (He et al., 2005), spectral feature\nselection, SPEC (Zhao & Liu, 2007), and trace ratio criterion for feature\nselection, TrRatio (Nie et al., 2008). The learning models are not employed\nin this category of methods which results in the selection of less relevant\nfeatures.\nIn sparse machine learning methods, feature selection is performed based\non learning a regularized model, such as low-dimensional embedding and\n3\nsparse regression, JELSR (Hou et al., 2014), local discriminative sparse sub-\nspace learning, LDSSL (Shang et al., 2019), multi-cluster feature selection,\nMCFS (Cai et al., 2010), non-negative discriminative feature selection, NDFS\n(Li et al., 2012), similarity preserving feature selection, SPFS (Zhao et al.,\n2013), structure preservation robust spectral feature selection, SRFS (Zhu\net al., 2018), unsupervised discriminative feature selection, UDFS (Yang\net al., 2011). These methods commonly select features based on learning\na regularized regression matrix without involving data reconstruction.\nData reconstruction approaches were proposed to select features based on\ntheir explanation on linear and non-linear transformation of the data, includ-\ning sparse principal component analysis, CPFS (Masaeli et al., 2010), greedy\nunsupervised feature selection, GreedyFS (Farahat et al., 2013), graph reg-\nularized feature selection, GRFS (Zhao et al., 2016), embedded reconstruc-\ntion based unsupervised feature selection, REFS (Li et al., 2017b), structure\npreserving unsupervised feature selection, SPUFS (Lu et al., 2018), recon-\nstruction error minimization, REMFS (Yang et al., 2019).\nWhile dictionary learning and reconstruction based methods can be re-\ngarded as similar techniques to learn the basis matrix, the dictionary learn-\ning methods enable us to provide a new data representation along with the\nelimination of redundant features. Feature selection process in dictionary\nlearning methods is conducted in two main phases, learning the basis ma-\ntrix and sparse new data representation. Most of the dictionary learning\nmethods were proposed by considering a two-step procedure such as (Zheng\net al., 2011; Zhu et al., 2017). Graph sparse coding, GSC (Zheng et al., 2011)\nperformed dictionary learning and spectral analysis to yield a sparse repre-\nsentation by an ℓ1-norm regularization. Robust joint graph sparse coding,\nRJGSC (Zhu et al., 2017), extended GSC by using an ℓ2,1-norm regularizer.\nOn the other hand, DGL (Ding et al., 2020) jointly learns the basis and\nsparse data matrix in a uniﬁed framework. Earlier dictionary-based methods\nhave not constructed the basis matrix by considering the natural low-rank\nassumption of it, which is justiﬁed on many real-world high-dimensional data\n(Chen & Huang, 2012).\nTable 1 presents a summary of the related methods by considering im-\nportant characteristics in a UFS process. Sparse learning employs a regu-\nlarization approach to the learning model. Subspace learning indicates the\nlow-dimensional representation of the original data. In spectral analysis, the\nlocal structure of the samples is taken into account. Joint learning refers to a\nuniﬁed objective function. In data reconstruction, features are expressed by\n4\nTable 1: Summary of the state-of-the-art unsupervised feature selection methods.\nAlgorithm\nSparse\nlearning\nSubspace\nlearning\nSpectral\nanalysis\nJoint\nlearning\nData\nreconstruction\nDictionary\nlearning\nLow-rank\nrepresentation\nLS (He et al., 2005)\n×\n×\n✓\n✓\n×\n×\n×\nMCFS (Cai et al., 2010)\n✓\n✓\n✓\n×\n×\n×\n×\nUDFS (Yang et al., 2011)\n✓\n✓\n×\n✓\n×\n×\n×\nNDFS (Li et al., 2012)\n✓\n✓\n✓\n✓\n×\n×\n×\nSPFS (Zhao et al., 2013)\n✓\n✓\n×\n✓\n×\n×\n×\nJELSR (Hou et al., 2014)\n✓\n✓\n✓\n✓\n×\n×\n×\nLDSSL (Shang et al., 2019)\n✓\n✓\n✓\n✓\n✓\n×\n×\nSRFS (Zhu et al., 2018)\n✓\n✓\n✓\n✓\n✓\n×\n✓\nRJGSC (Zhu et al., 2017)\n✓\n×\n✓\n×\n✓\n✓\n×\nDGL (Ding et al., 2020)\n✓\n×\n✓\n✓\n✓\n✓\n×\nDLUFS\n✓\n✓\n✓\n✓\n✓\n✓\n✓\na linear or non-linear combination of all features to discard redundant ones.\nIn dictionary learning, a new representation of the original data is learned\nin a basis space. By low-rank representation, the reconstruction matrix is\ndecomposed to low-rank matrices to consider the correlation among features.\nIn this paper, we propose a uniﬁed UFS method based on all of these main\ncharacteristics to yield an eﬃcient and robust procedure.\n3. The Proposed Method\nIn this section, at ﬁrst notations are presented.\nThen, the proposed\nmethod and its algorithm details are introduced. Finally, the proposed algo-\nrithm is illustrated through an example.\n3.1. Notations\nIn this paper, the vectors and the matrices are denoted by bold lowercase\nand bold uppercase characters. For a given vector v, its ℓ2-norm is denoted by\n∥v∥2. Suppose M is an arbitrary matrix, Mij represents its (i, j)-th element,\nmi is the i-th row, mj is the j-th column, tr(M) is the trace, and M⊤is the\ntranspose of the matrix. The Frobenius norm is denoted by ∥M∥F, and the\nℓ2,1-norm is deﬁned as,\n∥M∥2,1 =\nX\ni\nsX\nj\nM 2\nij.\nLet X ∈Rp×n represents the data matrix, where p is the number of features\nand n is the number of samples.\n5\n3.2. The Proposed Method\nAt ﬁrst, a new data representation matrix can be learned based on dic-\ntionary learning approach as,\nmin\nQ,Z ∥X −QZ∥2\nF,\n(1)\nwhere Z ∈Rp×n is a representation of the data matrix X in the space of\nthe dictionary matrix Q ∈Rp×p. The rank of high-dimensional data can be\nincreased by the noisy and outlier features (Chen & Huang, 2012). Based on\nthis fact, the data can be represented in a low-rank space. In this regard, a\nlow-rank constraint on the basis matrix is imposed as,\nmin\nQ,Z\n∥X −QZ∥2\nF\ns.t.\nrank(Q) = r,\n(2)\nwhere r ≪{n, p} is the induced rank to Q. The low-rank constraint on Eq.\n(2) is equivalent to multiply two rank r matrices as,\nmin\nA,B,Z ∥X −ABZ∥2\nF,\n(3)\nwhere A ∈Rp×r, B ∈Rr×p.\nBy Q = AB decomposition, the feature\ncorrelation is considered in a low-rank space. The matrix Z is transformed to\na low-dimensional matrix BZ ∈Rr×n to perform subspace learning, while A\nre-transforms BZ to the original space. More speciﬁcally, as further expressed\nin Eq. (11), the mentioned subspace learning is calculated based on LDA\n(Fukunaga, 1990).\nThe global feature correlations are maintained by low-rank constraint.\nFurthermore, the spectral analysis is applied to take the local sample struc-\nture into account as,\nmin\nA,B,Z ∥X −ABZ∥2\nF + α tr(ZLZ⊤),\n(4)\nwhere α is a tuning parameter. The Laplacian matrix L is calculated as\nL = D −S, where the diagonal matrix D is deﬁned as Dii = P\nj Sij and the\nsimilarity matrix S is calculated as follows,\nSij =\n(\nexp\n\u0010\n−∥xi−xj∥2\n2\nσ2\n\u0011\n,\nif xi ∈Nk\n\u0000xj\u0001\nor xj ∈Nk\n\u0000xi\u0001\n0,\notherwise,\n(5)\n6\nwhere Nk\n\u0000xi\u0001\nrepresents the set of k-nearest neighbors of xi, and σ is the\nwidth parameter for the Gaussian kernel.\nA feature selection framework can be provided by inducing a sparse learn-\ning on the new representation matrix Z. Hence, the ﬁnal objective function\nis proposed as,\nmin\nA,B,Z ∥X −ABZ∥2\nF + α tr(ZLZ⊤) + λ ∥Z∥2,1,\n(6)\nwhere λ is the regularization parameter. The ℓ2,1-norm regularizer provides\nsparsity on the rows of Z, inspired by the group lasso penalty (Yuan & Lin,\n2006). The rows are closer to zero, then the corresponding features are more\nlikely regarded as uninformative features.\n3.3. Optimization\nWe consider the main objective function as the following optimization\nproblem,\nmin\nA,B,Z f(A, B, Z) = ∥X −ABZ∥2\nF + α tr(ZLZ⊤) + λ ∥Z∥2,1,\n(7)\nFirst, by ﬁxing Z in the main optimization problem in Eq. (7) to get,\nmin\nA,B f(A, B) = ∥X −ABZ∥2\nF.\n(8)\nBy setting the derivative of the Eq. (8) with respect to A to zero,\nA = XZ⊤B⊤(BSwB⊤)−1,\n(9)\nwhere Sw = ZZ⊤is the correlation among features in the new representation\nmatrix Z.\nWe rewrite Eq. (8) as,\nmin\nA,B\ntr(XX⊤) −2tr(ABZX⊤) + tr(ABZZ⊤B⊤A⊤).\n(10)\nUsing obtained A from Eq. (9) in Eq. (10) to derive the objective function\nof B as,\nmin\nB\n−tr(XZ⊤B⊤(BSwB⊤)−1BZX⊤),\n⇔max\nB\ntr((BSwB⊤)−1BSbB⊤),\n(11)\n7\nwhere Sb = ZX⊤XZ⊤. Similar to discriminant analysis (Fukunaga, 1990),\nSw and Sb can be interpreted as within-class and between-class scatter matri-\nces. Therefore, B⊤can be learned by r eigenvectors of Sw\n−1Sb corresponding\nto top r eigenvalues.\nBy rewriting the objective function in Eq. (6),\nf(Z) = ∥X −ABZ∥2\nF + α tr(ZLZ⊤) + λ ∥Z∥2,1.\n(12)\nLet A and B are ﬁxed in Eq. (12). Then, by setting the derivative of f(Z)\nto zero, the following Sylvester equation (Bartels & Stewart, 1972) can be\nobtained,\n\u0000(AB)⊤AB + λD\n\u0001\nZ + Z (αL) = (AB)⊤X,\n(13)\nwhere D is a diagonal matrix as,\nDii =\n1\n2∥zi∥2 + ϵ.\n(14)\nHere, Algorithm 1 summarizes the iterative procedure of obtaining the main\noptimization variables in Eq. (7). By descending order of ∥zi∥2’s, the impor-\ntance of the corresponding features are determined.\nAlgorithm 1 DLUFS algorithm.\nInput: Data matrix X ∈Rp×n and parameters α and λ.\n1: t = 0.\n2: Initialize Zt = X.\n3: repeat\n4:\nUpdate Bt+1 by solving Eq. (11).\n5:\nUpdate At+1 by Eq. (9).\n6:\nUpdate the diagonal matrix Dt+1 by Eq. (14).\n7:\nUpdate Zt+1 by solving the Sylvester equation in (13).\n8: until the convergence of the objective function in Eq. (7).\nOutput: Sorting features in descending order of ∥zi∥2’s.\n3.4. Computational Complexity\nThe computational complexity of Algorithm 1 consists of computing B,\nA and Z in each iteration. By considering (11), cost of updating B equals\n8\nFeatures\nS1\nS2\nS3\nS4\nS5\nS6\nF1\nF2\nF3\nF4\nF5\nF6\nF7\nF8\nF9\nSamples\nData matrix: X\nS1\nS2\nS3\nS4\nS5\nS6\nS1\nS2\nS3\nS4\nS5\nS6\nSimilarity matrix: S\nSpectral \nanalysis\nLocal similarity \ncomputing\nNew data \nrepresentation\nNew data \nrepresentation\nLow-rank \ndictionary learning\nFeature \nselection\nX in selected features: Xsel\nS1\nS2\nS3\nS4\nS5\nS6\nF4\nF5\nF8\nS1\nS2\nS3\nS4\nS5\nS6\nF1\nF2\nF3\nF4\nF5\nF6\nF7\nF8\nF9\nNew sparse representation matrix: Z\nSelected\nSelected\nSelected\nLow-rank \ndictionary learning\nF1\nF2\nF3\nF4\nF5\nF6\nF7\nF8\nF9\nF1\nF2\nF3\nF4\nF5\nF6\nF7\nF8\nF9\nB\nA\n.\nBasis matrix: AB\n=\nF1\nF2\nF3\nF4\nF5\nF6\nF7\nF8\nF9\nF1\nF2\nF3\nF4\nF5\nF6\nF7\nF8\nF9\nFigure 1: An illustrative example for describing the proposed method.\nto max{O(p3), O(p2n)}. Next by (9), the time complexity of computing A is\nmax{O(p2n), O(p2r), O(r3)}. Furthermore, the time complexity of updating\nZ contains two elements, computing the input of Sylvester equation in (13),\nand solving the Sylvester equation, which derives as max{O(p3), O(p2n), O(p2r)}.\nSince the assumption of r ≪{p, n} holds on high-dimensional settings, com-\nputational complexity of Algorithm 1 reduces to max{O(p3), O(p2n)}.\n3.5. An illustrative example\nFig. 1 describes the proposed algorithm through an illustrative example.\nAll matrices are assumed to be non-negative, where the brighter elements\nindicate more closeness to zero and the darker ones are far from zero. X is\n9\nan artiﬁcial data matrix with nine features and six samples. First, a local\nsimilarity matrix S is calculated based on the sample similarities in X. A\nlow-rank basis matrix AB is obtained by dictionary learning which can be\ninterpreted as the correlation among features based on the low-dimensional\nmatrices A and B. Then, a new data representation matrix Z is computed\nbased on spectral analysis in the basis space. The B, A, and Z are iteratively\nupdated until convergence. Features are ranked by calculating the ℓ2-norm\non their corresponding rows of Z as our new data matrix. Finally, selected\nfeatures Xsel are given in the output.\nIn this example, samples are classiﬁed to three sets as {S1, S5}, {S2, S6},\nand {S3, S4} in terms of similarities. In addition, there are three categories\nof features, {F1, F5, F7}, {F2, F3, F8}, and {F4, F6, F9} by their similarities.\nThe basis matrix AB is learned based on low-rank dictionary learning with\nrank = 3. Fig. 1 indicates that the top three remaining rows of Z are the\nF4, F5, and F8. Therefore, the output Xsel is formed by selected features.\n4. Convergence Analysis\nOur aim is to show the non-increasing behavior of Algorithm 1 based on\nthe objective function in Eq. (7). Initially, a lemma is given, then the main\ntheorem is presented.\nLemma 1. Let u and v are two non-zero vectors, then this inequality holds,\n∥u∥2 −∥u∥2\n2\n2∥v∥2\n≤∥v∥2 −∥v∥2\n2\n2∥v∥2\n.\n(15)\nThe proof of Lemma 1 derived in (Nie et al., 2010).\nTheorem 1. Algorithm 1 behaves non-increasingly in each update through\nthe primary objective function in (7).\nProof. In the following, the t-th iteration of a vector v and a matrix M is\ndenoted by vt and Mt.\nThe non-increasing behavior of the objective function of Z in (12) is\nderived by assuming At and Bt to be ﬁxed. Since the non-smooth ∥Z∥2,1 is\niteratively optimized by updating D and Z, the following inequality can be\n10\npresented,\n∥X −QtZt+1∥\n2\nF + α tr(Zt+1L(Zt+1)\n⊤) + λ\np\nX\ni=1\n∥zit+1∥2\n2\n2∥zit∥2\n≤∥X −QtZt∥\n2\nF + α tr(ZtL(Zt)\n⊤) + λ\np\nX\ni=1\n∥zit∥2\n2\n2∥zit∥2\n.\n(16)\nWhere Q = AB. Then, the inequality (16) can be rewritten as,\n∥X −QtZt+1∥\n2\nF + α tr(Zt+1L(Zt+1)\n⊤) + λ ∥Zt+1∥2,1 −λ\np\nX\ni=1\n(∥zi\nt+1∥2 −∥zit+1∥2\n2\n2∥zit∥2\n)\n≤∥X −QtZt∥\n2\nF + α tr(ZtL(Zt)\n⊤) + λ ∥Zt∥2,1 −λ\np\nX\ni=1\n(∥zi\nt∥2 −∥zit∥2\n2\n2∥zit∥2\n).\n(17)\nAccording to Lemma 1,\n∥zi\nt+1∥2 −∥zit+1∥2\n2\n2∥zit∥2\n≤∥zi\nt∥2 −∥zit∥2\n2\n2∥zit∥2\n,\n(18)\nwe obtain,\n∥X −QtZt+1∥\n2\nF + α tr(Zt+1L(Zt+1)\n⊤) + λ ∥Zt+1∥2,1\n≤∥X −QtZt∥\n2\nF + α tr(ZtL(Zt)\n⊤) + λ ∥Zt∥2,1.\n(19)\nTherefore,\nf(At, Bt, Zt+1) ≤f(At, Bt, Zt).\n(20)\nIn the same way, by ﬁxing Zt+1, it can be shown that,\nf(At+1, Bt+1, Zt+1) ≤f(At, Bt, Zt+1).\n(21)\nBy considering inequalities (20) and (21),\nf(At+1, Bt+1, Zt+1) ≤f(At+1, Bt+1, Zt) ≤f(At, Bt, Zt).\n(22)\nTherefore, the non-increasing behavior of Algorithm 1 based on the primary\nobjective function in Eq. (7) is given.\n11\nTable 2: The statistics of datasets.\nDataset\nsamples\nfeatures\nclasses\nType\nCategory\nBA\n1404\n320\n36\nBinary\nImage\nColon\n62\n2000\n2\nDiscrete\nBiology\nGLIOMA\n50\n4434\n4\nContinuous\nBiology\nMadelon\n2600\n500\n2\nContinuous\nArtiﬁcial\nORL\n400\n1024\n40\nDiscrete\nImage\nPCMAC\n1943\n3289\n2\nDiscrete\nText\nWarpAR10P\n130\n2400\n10\nDiscrete\nImage\nYale\n165\n1024\n15\nDiscrete\nImage\n5. Experiments\nThis section is divided into ﬁve subsections to describe our experimental\nsetup.\nA brief summary of the applied datasets in our study, evaluation\nmeasures, the details of parameters setting, the obtained results, and the\nsensitivity analysis are presented in the following.\n5.1. Datasets\nA variety of domains of applications are considered in employed datasets\nsuch as images of digits (BA (Belhumeur et al., 1997)), colon cancer (Colon\n(Alon et al., 1999)), malignant brain tumor (GLIOMA (et al., 2003)), non-\nsparse artiﬁcial dataset (Madelon (Li et al., 2017a)), image of faces (ORL,\nYale (Cai et al., 2006), and WarpAR10P (Li et al., 2017a)), and PC ver-\nsus MAC from 20-newsgroups dataset (PCMAC (Lang, 1995)). The BA is\navailable on https://cs.nyu.edu/~roweis/data.html, while all others are\naccessed from (Li et al., 2017a). Table 2 reports the main characteristics of\ndatasets.\n5.2. Evaluation measures\nThe clustering techniques are usually applied to evaluate the UFS meth-\nods (Li et al., 2017a). Based on the attained clustering results and the ground\ntruth information, two common evaluation measures are used frequently, Ac-\ncuracy and Normalized Mutual Information.\nLet the ground truth and the predicted one based on clustering approach\nare shown by y and ˆy.\nThe Accuracy (called as ACC) is deﬁned as,\nACC(y, ˆy) = 1\nn\nn\nX\ni=1\nδ(yi, map(zi)),\n12\nwhere the function δ(a, b) equals to 1, when a = b, and 0 elsewhere. For the\nmap(.) function, the Kuhn-Munkres approach (Lovasz, 1986) is employed to\nﬁnd the best permutation for matching the categories in vectors y and ˆy.\nBased on deﬁnitions of entropy measure H(.) and the mutual information of\ny and ˆy denoted by I(y, ˆy) (Bishop, 2006), Normalized Mutual Information\n(called as NMI) is given as,\nNMI(y, ˆy) =\nI(y, ˆy)\nmax(H(y), H(ˆy)),\n5.3. The experimental setting\nWe compare the proposed method, DLUFS, with the state-of-the-art un-\nsupervised feature selection algorithms, including JELSR (Hou et al., 2014),\nLDSSL (Shang et al., 2019), LS (He et al., 2005), MCFS (Cai et al., 2010),\nNDFS(Li et al., 2012), SPFS (Zhao et al., 2013), SRFS (Zhu et al., 2018),\nUDFS (Yang et al., 2011), and selecting all features namely Baseline.\nThe number of neighborhoods in k-nearest neighbor algorithm is set to\nk = 5.\nWe set σ = 1 in our method and others requiring a similarity\nmatrix based on a Gaussian kernel. For NDFS method, the default γ =\n108 is considered. Finding the suitable choices of the tuning parameters α\nand λ in our method is performed by a grid search approach from the set\nof {10−4, 10−2, 1, 102, 104} candidates, where a similar approach is used to\nset the tuning parameters in the other methods. The stopping convergence\ncondition for all of the iterative algorithms, including our method, is given\nas |f(t)−f(t−1)|\nf(t)\n< 10−3, where f(t) equals to the objective function in the t-th\niteration. The NMI and ACC measures are reported on 20 times repetitions.\nIn each repetition, the k-means algorithm ia applied by setting the numbers\nof features from {50, 100, 150, 200, 250, 300}. We report two main descriptive\nstatistical quantities of NMI and ACC in these repeated experiments, mean\nand standard deviation (STD).\n5.4. Experimental results\nWe demonstrate the performance of the proposed method, DLUFS, by\ncomparing to benchmark UFS methods deﬁned in the earlier subsection.\nTable 3 and 4 represent the obtained results based on ACC and NMI (mean\n± STD). The bold numbers represent the best attained results. We used\nthe underlined numbers to indicate the second-best results. We summarize\nthe main ﬁndings from the obtained results of Table 3 and Table 4 in the\nfollowing,\n13\nTable 3: The results of clustering (ACC% ± std) of UFS methods on standard datasets.\nThe best and the second-best are presented in bold and underlined numbers.\nDataset\nBA\nColon\nGLIOMA\nMadelon\nORL\nPCMAC\nWarpAR10P\nYale\nBaseline\n43.04 ± 1.18\n54.84 ± 0.00\n61.30 ± 4.11\n50.30 ± 0.07\n59.24 ± 2.17\n50.54 ± 0.04\n21.04 ± 2.93\n41.91 ± 2.36\nJELSR\n39.97 ± 2.14\n56.76 ± 1.34\n52.53 ± 1.00\n57.53 ± 1.21\n57.22 ± 3.15\n50.55 ± 0.03\n33.96 ± 1.50\n37.56 ± 1.05\nLDSSL\n40.73 ± 3.28\n58.01 ± 0.47\n56.98 ± 0.70\n52.23 ± 1.57\n43.09 ± 5.51\n50.69 ± 0.07\n33.60 ± 0.51\n40.23 ± 0.85\nLS\n41.59 ± 3.47\n57.80 ± 0.60\n55.75 ± 2.28\n50.35 ± 0.04\n48.13 ± 2.92\n50.45 ± 0.03\n32.60 ± 3.43\n44.07 ± 2.42\nMCFS\n41.75 ± 2.16\n54.66 ± 1.44\n60.55 ± 4.73\n52.30 ± 0.99\n57.72 ± 1.27\n50.46 ± 0.14\n23.38 ± 3.39\n39.76 ± 0.49\nNDFS\n39.42 ± 4.06\n57.39 ± 1.45\n57.37 ± 2.14\n57.06 ± 1.64\n53.43 ± 2.73\n50.59 ± 0.03\n31.38 ± 1.14\n37.15 ± 1.47\nSPFS\n41.39 ± 2.07\n58.33 ± 1.61\n55.68 ± 4.48\n51.58 ± 0.12\n58.08 ± 1.28\n50.52 ± 0.04\n30.96 ± 5.55\n41.33 ± 0.71\nSRFS\n37.78 ± 4.73\n60.32 ± 0.92\n61.20 ± 1.32\n52.00 ± 1.65\n58.17 ± 1.24\n50.54 ± 0.02\n31.19 ± 2.61\n42.06 ± 1.88\nUDFS\n40.69 ± 3.05\n55.67 ± 0.86\n54.17 ± 3.05\n57.47 ± 1.66\n54.74 ± 2.10\n50.57 ± 0.05\n28.60 ± 3.78\n33.53 ± 1.33\nDLUFS\n42.38 ± 1.91\n64.83 ± 7.83\n64.22 ± 2.18\n59.13 ± 0.43\n59.22 ± 1.25\n50.80 ± 0.12\n35.65 ± 2.23\n47.31 ± 0.98\nTable 4: The results of clustering (NMI% ± std) of UFS methods on standard datasets.\nThe best and the second-best are presented in bold and underlined numbers.\nDataset\nBA\nColon\nGLIOMA\nMadelon\nORL\nPCMAC\nWarpAR10P\nYale\nBaseline\n58.21 ± 0.69\n00.62 ± 0.00\n50.93 ± 2.47\n00.00 ± 0.00\n77.81 ± 0.83\n00.04 ± 0.04\n17.41 ± 3.60\n48.93 ± 1.85\nJELSR\n55.62 ± 1.60\n02.76 ± 0.86\n31.11 ± 2.10\n01.60 ± 0.43\n75.70 ± 1.94\n00.90 ± 0.05\n33.85 ± 2.17\n44.90 ± 1.13\nLDSSL\n56.04 ± 3.26\n01.52 ± 0.30\n49.74 ± 0.37\n00.22 ± 0.20\n66.24 ± 3.99\n01.08 ± 0.12\n35.00 ± 1.17\n47.08 ± 0.80\nLS\n57.14 ± 3.02\n01.80 ± 0.20\n49.77 ± 2.14\n00.00 ± 0.00\n71.32 ± 2.13\n01.23 ± 0.30\n33.10 ± 4.42\n49.83 ± 2.15\nMCFS\n56.64 ± 1.58\n00.23 ± 0.13\n37.64 ± 7.80\n00.18 ± 0.13\n76.78 ± 0.61\n00.89 ± 0.37\n20.30 ± 4.59\n47.10 ± 0.68\nNDFS\n54.65 ± 4.23\n01.71 ± 1.40\n49.87 ± 0.46\n01.51 ± 0.54\n73.43 ± 2.00\n00.67 ± 0.07\n29.73 ± 1.19\n44.38 ± 1.31\nSPFS\n56.91 ± 1.62\n01.21 ± 0.48\n35.41 ± 6.24\n00.07 ± 0.01\n77.22 ± 0.77\n00.90 ± 0.06\n27.09 ± 6.04\n48.24 ± 0.76\nSRFS\n53.44 ± 4.68\n02.96 ± 0.89\n49.62 ± 0.59\n00.21 ± 0.32\n77.27 ± 0.84\n00.09 ± 0.09\n29.16 ± 1.37\n49.60 ± 1.10\nUDFS\n56.25 ± 2.72\n01.13 ± 0.26\n29.13 ± 4.92\n02.13 ± 1.14\n74.87 ± 1.52\n00.83 ± 0.26\n25.43 ± 3.63\n42.00 ± 1.00\nDLUFS\n57.63 ± 1.84\n07.59 ± 9.66\n51.75 ± 0.34\n02.43 ± 0.23\n78.02 ± 0.71\n02.24 ± 0.48\n36.65 ± 1.07\n56.20 ± 0.62\n• In most cases, DLUFS outperforms the Baseline, which would lead\nto the superiority of selecting features over learning on all features.\nTherefore, feature selection process improves the eﬃciency (i.e. ease of\ncomputation) and performance(i.e. better results).\n• DLUFS obtain good results as the best or the second-best (after Base-\nline) aligned with the good performance of LDSSL, SRFS which could\nbe due to data reconstruction.\n• Our method achieves good performance in most datasets. It can be\nattributed to low-rank representation as SRFS.\nFurthermore, the performance of the proposed method is demonstrated by\nselecting various number of features from 50 to 300. Fig. 2 and Fig. 3 show\nthe results for diﬀerent numbers of selected features, ignoring LS, MCFS,\nSPFS, and UDFS due to weak results. Obviously, DLUFS achieves the best\nperformance compared to the competitors even when the number of selected\nfeatures is varied.\n14\n50\n100\n150\n200\n250\n300\n30\n35\n40\n45\nNumber of selected features\nACC(%)\n(a) BA\n50\n100\n150\n200\n250\n300\n60\n70\n80\nNumber of selected features\nACC(%)\n(b) Colon\n50\n100\n150\n200\n250\n300\n50\n55\n60\n65\n70\nNumber of selected features\nACC(%)\n(c) GLIOMA\n50\n100\n150\n200\n250\n300\n50\n55\n60\nNumber of selected features\nACC(%)\n(d) Madelon\n50\n100\n150\n200\n250\n300\n35\n40\n45\n50\n55\n60\nNumber of selected features\nACC(%)\n(e) ORL\n50\n100\n150\n200\n250\n300\n50.6\n50.7\n50.8\n50.9\n51.0\nNumber of selected features\nACC(%)\n(f) PCMAC\n50\n100\n150\n200\n250\n300\n25.0\n30.0\n35.0\n40.0\nNumber of selected features\nACC(%)\n(g) WarpAR10P\n50\n100\n150\n200\n250\n300\n35.0\n40.0\n45.0\n50.0\nNumber of selected features\nACC(%)\n(h) Yale\nBaseline\nJELSR\nLDSSL\nNDFS\nSRFS\nDLUFS\nFigure 2: The achieved results in ACC measure by selecting diﬀerent numbers of features.\n5.5. Parameter sensitivity and convergence study\nFirst, the sensitivity of the parameters is investigated in DLUFS. The\nmain parameters of the method are α and λ, by considering the objective\nfunction of DLUFS (Eq. (7)). We explore the set of candidate values for\ntuning parameters α and λ from {10−4, 10−2, 1, 102, 104}. The datasets of\nTable 2 are used to investigate the aﬀect of variations in the main tuning\nparameters. We report the obtained results of DLUFS by considering ACC\nand NMI measures. Fig. 4 shows ACC values based on diﬀerent settings\n15\n50\n100\n150\n200\n250\n300\n45\n50\n55\n60\nNumber of selected features\nNMI(%)\n(a) BA\n50\n100\n150\n200\n250\n300\n10\n20\n30\nNumber of selected features\nNMI(%)\n(b) Colon\n50\n100\n150\n200\n250\n300\n25\n35\n45\n55\nNumber of selected features\nNMI(%)\n(c) GLIOMA\n50\n100\n150\n200\n250\n300\n1\n2\n3\nNumber of selected features\nNMI(%)\n(d) Madelon\n50\n100\n150\n200\n250\n300\n60\n70\n80\nNumber of selected features\nNMI(%)\n(e) ORL\n50\n100\n150\n200\n250\n300\n1\n2\n3\nNumber of selected features\nNMI(%)\n(f) PCMAC\n50\n100\n150\n200\n250\n300\n20\n25\n30\n35\n40\nNumber of selected features\nNMI(%)\n(g) WarpAR10P\n50\n100\n150\n200\n250\n300\n40\n45\n50\n55\n60\nNumber of selected features\nNMI(%)\n(h) Yale\nBaseline\nJELSR\nLDSSL\nNDFS\nSRFS\nDLUFS\nFigure 3: The achieved results in NMI measure by selecting diﬀerent numbers of features.\nof α and λ parameters, and Fig.\n5 represents NMI values.\nThe results\nindicate that there exists a slight sensitivity to these main parameters in the\nperformance of DLUFS.\nNext, the convergence behavior of the DLUFS algorithm is experimentally\ndemonstrated on the datasets of Table 2.\nThe convergence speed of the\nobjective function is depicted versus the number of iterations in Fig. 6. These\nﬁndings from Fig. 6 reveal the eﬃciency of the proposed algorithm based on\nthe convergence speed.\n16\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n50\n30\n10\n20\n0\nACC (%)\n(a) BA\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nACC (%)\n(b) Colon\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n80\n60\n0\nACC (%)\n(c) GLIOMA\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nACC (%)\n(d) Madelon\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nACC (%)\n(e) ORL\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nACC (%)\n(f) PCMAC\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n20\n10\n40\n30\n0\nACC (%)\n(g) WarpAR10P\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n50\n30\n10\n20\n0\nACC (%)\n(h) Yale\nFigure 4: Performance of DLUFS in ACC measure using diﬀerent values of the tuning\nparameters α and β in log10 .\n6. Conclusion\nIn this work, we proposed a new unsupervised feature selection method\nbased on dictionary learning to learn a new data representation in a basis\nspace. We devise a low-rank constraint on the basis matrix to preserve the\nfeature correlation along with subspace learning. Spectral analysis was em-\nployed to consider the sample similarities in the learned data representation\nmatrix. Moreover, an ℓ2,1-norm regularization was applied in our primary\nobjective function to discard uninformative features. We presented a uni-\nﬁed framework based on the important characteristics to select features. An\n17\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nNMI (%)\n(a) BA\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n2\n2.5\n1.5\n0.5\n1\n0\nNMI (%)\n(b) Colon\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nNMI (%)\n(c) GLIOMA\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n2\n2.5\n1.5\n0.5\n1\n0\nNMI (%)\n(d) Madelon\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n80\n60\n0\nNMI (%)\n(e) ORL\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n1\n0.5\n2\n1.5\n0\nNMI (%)\n(f) PCMAC\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n20\n10\n40\n30\n0\nNMI (%)\n(g) WarpAR10P\n4 \n2 \n0 \nλ\n-2\n-4\n4 \n2 \nα\n0 \n-2\n-4\n40\n20\n0\n60\nNMI (%)\n(h) Yale\nFigure 5: Performance of DLUFS in NMI measure using diﬀerent values of the tuning\nparameters α and β in log10 .\neﬃcient numerical algorithm is introduced for the proposed method. The\nperformance of the proposed method was investigated through state-of-the-\nart methods by the aid of a variety of standard datasets. The attained results\nveriﬁed the strength of the proposed approach in terms of accuracy and speed\nof convergence.\n18\n0\n10\n20\n30\n40\n50\n9.5\n9.55\n9.6\n9.65\n·104\nNumber of iterations\nObjective function value\n(a) BA\n0\n10\n20\n30\n40\n50\n3,850\n3,900\n3,950\nNumber of iterations\nObjective function value\n(b) Colon\n0\n10\n20\n30\n40\n50\n300\n400\n500\nNumber of iterations\nObjective function value\n(c) GLIOMA\n0\n10\n20\n30\n40\n50\n1.04\n1.04\n1.04\n·106\nNumber of iterations\nObjective function value\n(d) Madelon\n0\n10\n20\n30\n40\n50\n5.2\n5.4\n5.6\n5.8\n·104\nNumber of iterations\nObjective function value\n(e) ORL\n0\n10\n20\n30\n40\n50\n4.6\n4.65\n4.7\n4.75\n·106\nNumber of iterations\nObjective function value\n(f) PCMAC\n0\n10\n20\n30\n40\n50\n1.42\n1.44\n1.46\n1.48\n·104\nNumber of iterations\nObjective function value\n(g) WarpAR10P\n0\n10\n20\n30\n40\n50\n1.55\n1.6\n1.65\n·104\nNumber of iterations\nObjective function value\n(h) Yale\nFigure 6: Convergence curve of DLUFS on diﬀerent datasets.\nReferences\net al., C. L. N. (2003). Gene Expression-based Classiﬁcation of Malignant\nGliomas Correlates Better with Survival than Histological Classiﬁcation.\nCancer Research, 63, 1602–1607.\nAlon, U., Barkai, N., Notterman, D. A., Gish, K., Ybarra, S., Mack, D., &\nLevine, A. J. (1999). Broad patterns of gene expression revealed by cluster-\ning analysis of tumor and normal colon tissues probed by oligonucleotide\narrays. Proceedings of the National Academy of Sciences, 96, 6745–6750.\n19\nBartels, R. H., & Stewart, G. W. (1972). Solution of the matrix equation\nAX + XB = C [F4]. Communications of the ACM , 15, 820–826.\nBelhumeur, P., Hespanha, J., & Kriegman, D. (1997). Eigenfaces vs. Fisher-\nfaces: recognition using class speciﬁc linear projection. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 19, 711–720.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning (Informa-\ntion Science and Statistics). Berlin, Heidelberg: Springer-Verlag.\nCai, D., He, X., Han, J., & Zhang, H.-J. (2006). Orthogonal Laplacianfaces\nfor Face Recognition. IEEE Transactions on Image Processing, 15, 3608–\n3614.\nCai, D., Zhang, C., & He, X. (2010). Unsupervised Feature Selection for\nMulti-cluster Data. In Proceedings of the 16th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data Mining KDD ’10\n(pp. 333–342). New York, NY, USA: ACM.\nChen, L., & Huang, J. Z. (2012). Sparse Reduced-Rank Regression for Si-\nmultaneous Dimension Reduction and Variable Selection. Journal of the\nAmerican Statistical Association, 107, 1533–1545.\nDing, D., Xia, F., Yang, X., & Tang, C. (2020). Joint dictionary and graph\nlearning for unsupervised feature selection. Applied Intelligence, 50, 1379–\n1397.\nDy, J. G., & Brodley, C. E. (2004).\nFeature Selection for Unsupervised\nLearning. Journal of Machine Learning Research, 5, 845–889.\nFarahat, A. K., Ghodsi, A., & Kamel, M. S. (2013). Eﬃcient greedy feature\nselection for unsupervised learning. Knowledge and Information Systems,\n35, 285–310.\nFukunaga, K. (1990). Introduction to statistical pattern recognition. Com-\nputer science and scientiﬁc computing (2nd ed.). Boston: Academic Press.\nHe, X., Cai, D., & Niyogi, P. (2005). Laplacian Score for Feature Selection.\nIn Proceedings of the 18th International Conference on Neural Information\nProcessing Systems NIPS’05 (pp. 507–514). Cambridge, MA, USA: MIT\nPress.\n20\nHoseini, E., & Mansoori, E. G. (2019). Unsupervised feature selection in\nlinked biological data. Pattern Analysis & Applications, 22, 999–1013.\nHou, C., Nie, F., Li, X., Yi, D., & Wu, Y. (2014). Joint Embedding Learning\nand Sparse Regression: A Framework for Unsupervised Feature Selection.\nIEEE Transactions on Cybernetics, 44, 793–804.\nKrzanowski, W. J. (1987). Selection of Variables to Preserve Multivariate\nData Structure, Using Principal Components. Journal of the Royal Sta-\ntistical Society: Series C (Applied Statistics), 36, 22–33.\nLang, K. (1995). Newsweeder: Learning to ﬁlter netnews. In Proceedings of\nthe Twelfth International Conference on Machine Learning (pp. 331–339).\nLi, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R. P., Tang,\nJ.,\n& Liu,\nH. (2017a).\nFeature Selection:\nA Data Perspective,\nhttp://featureselection.asu.edu/.\nACM Computing Surveys, 50, 94:1–\n94:45.\nLi, J., Tang, J., & Liu, H. (2017b).\nReconstruction-based Unsupervised\nFeature Selection: An Embedded Approach. In Proceedings of the 26th In-\nternational Joint Conference on Artiﬁcial Intelligence IJCAI’17 (pp. 2159–\n2165). AAAI Press.\nLi, Z., Yang, Y., Liu, J., Zhou, X., & Lu, H. (2012). Unsupervised Fea-\nture Selection Using Nonnegative Spectral Analysis. In Proceedings of the\nTwenty-Sixth AAAI Conference on Artiﬁcial Intelligence AAAI’12 (pp.\n1026–1032). AAAI Press.\nLovasz, L. (1986). Matching Theory (North-Holland Mathematics Studies).\nOxford, UK, UK: Elsevier Science Ltd.\nLu, Q., Li, X., & Dong, Y. (2018). Structure Preserving Unsupervised Fea-\nture Selection. Neurocomputing, 301, 36–45.\nMasaeli, M., Yan, Y., Cui, Y., Fung, G., & Dy, J. (2010). Convex Principal\nFeature Selection. In Proceedings of the 2010 SIAM International Con-\nference on Data Mining Proceedings (pp. 619–628). Society for Industrial\nand Applied Mathematics.\n21\nMurphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT\npress.\nNie, F., Huang, H., Cai, X., & Ding, C. (2010). Eﬃcient and Robust Fea-\nture Selection via Joint L2,1-norms Minimization. In Proceedings of the\n23rd International Conference on Neural Information Processing Systems\n- Volume 2 NIPS’10 (pp. 1813–1821). USA: Curran Associates Inc.\nNie, F., Xiang, S., Jia, Y., Zhang, C., & Yan, S. (2008). Trace ratio criterion\nfor feature selection. In Proceedings of the 23rd national conference on\nArtiﬁcial intelligence - Volume 2 AAAI’08 (pp. 671–676). Chicago, Illinois:\nAAAI Press.\nPandit, A. A., Pimpale, B., & Dubey, S. (2020). A Comprehensive Review\non Unsupervised Feature Selection Algorithms. In G. Singh Tomar, N. S.\nChaudhari, J. L. V. Barbosa, & M. K. Aghwariya (Eds.), International\nConference on Intelligent Computing and Smart Communication 2019 Al-\ngorithms for Intelligent Systems (pp. 255–266). Singapore: Springer.\nParsa, M. G., Zare, H., & Ghatee, M. (2020). Unsupervised feature selection\nbased on adaptive similarity learning and subspace clustering. Engineering\nApplications of Artiﬁcial Intelligence, 95, 103855.\nRogati, M., & Yang, Y. (2002). High-performing feature selection for text\nclassiﬁcation. In Proceedings of the eleventh international conference on\nInformation and knowledge management CIKM ’02 (pp. 659–661). New\nYork, NY, USA: Association for Computing Machinery.\nShang, R., Meng, Y., Wang, W., Shang, F., & Jiao, L. (2019). Local dis-\ncriminative based sparse subspace learning for feature selection. Pattern\nRecognition, 92, 219–230.\nShi, C., Ruan, Q., Guo, S., & Tian, Y. (2015). Sparse feature selection based\non L2,1/2-matrix norm for web image annotation. Neurocomputing, 151,\n424–433.\nYang, S., Zhang, R., Nie, F., & Li, X. (2019). Unsupervised Feature Selection\nBased on Reconstruction Error Minimization. In ICASSP 2019 - 2019\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP) (pp. 2107–2111).\n22\nYang, Y., Shen, H. T., Ma, Z., Huang, Z., & Zhou, X. (2011). L2,1-norm\nRegularized Discriminative Feature Selection for Unsupervised Learning.\nIn Proceedings of the Twenty-Second International Joint Conference on\nArtiﬁcial Intelligence - Volume Volume Two IJCAI’11 (pp. 1589–1594).\nAAAI Press.\nYuan, M., & Lin, Y. (2006). Model selection and estimation in regression\nwith grouped variables. Journal of the Royal Statistical Society: Series B\n(Statistical Methodology), 68, 49–67.\nZare, H., & Niazi, M. (2016). Relevant based structure learning for feature\nselection. Engineering Applications of Artiﬁcial Intelligence, 55, 93–102.\nZare, H., Parsa, M. G., Ghatee, M., & Alizadeh, S. H. (2020). Similarity\nPreserving Unsupervised Feature Selection based on Sparse Learning. In\n2020 10th International Symposium on Telecommunications (IST) (pp.\n50–55).\nZhao, Z., He, X., Cai, D., Zhang, L., Ng, W., & Zhuang, Y. (2016). Graph\nRegularized Feature Selection with Data Reconstruction. IEEE Transac-\ntions on Knowledge and Data Engineering, 28, 689–700.\nZhao, Z., & Liu, H. (2007). Spectral Feature Selection for Supervised and\nUnsupervised Learning. In Proceedings of the 24th International Confer-\nence on Machine Learning ICML ’07 (pp. 1151–1157). New York, NY,\nUSA: ACM.\nZhao, Z., Wang, L., Liu, H., & Ye, J. (2013). On Similarity Preserving Fea-\nture Selection. IEEE Transactions on Knowledge and Data Engineering,\n25, 619–632.\nZheng, M., Bu, J., Chen, C., Wang, C., Zhang, L., Qiu, G., & Cai, D.\n(2011). Graph Regularized Sparse Coding for Image Representation. IEEE\nTransactions on Image Processing, 20, 1327–1336.\nZhu, P., Hu, Q., Zhang, C., & Zuo, W. (2016). Coupled dictionary learning\nfor unsupervised feature selection. In Proceedings of the Thirtieth AAAI\nConference on Artiﬁcial Intelligence AAAI’16 (pp. 2422–2428). Phoenix,\nArizona: AAAI Press.\n23\nZhu, X., Li, X., Zhang, S., Ju, C., & Wu, X. (2017). Robust Joint Graph\nSparse Coding for Unsupervised Spectral Feature Selection. IEEE Trans-\nactions on Neural Networks and Learning Systems, 28, 1263–1275.\nZhu, X., Zhang, S., Hu, R., Zhu, Y., & song, j. (2018). Local and Global\nStructure Preservation for Robust Unsupervised Spectral Feature Selec-\ntion. IEEE Transactions on Knowledge and Data Engineering, 30, 517–\n529.\n24\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-06-21",
  "updated": "2021-06-21"
}