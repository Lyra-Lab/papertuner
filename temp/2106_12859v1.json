{
  "id": "http://arxiv.org/abs/2106.12859v1",
  "title": "Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images",
  "authors": [
    "Lang Nie",
    "Chunyu Lin",
    "Kang Liao",
    "Shuaicheng Liu",
    "Yao Zhao"
  ],
  "abstract": "Traditional feature-based image stitching technologies rely heavily on\nfeature detection quality, often failing to stitch images with few features or\nlow resolution. The learning-based image stitching solutions are rarely studied\ndue to the lack of labeled data, making the supervised methods unreliable. To\naddress the above limitations, we propose an unsupervised deep image stitching\nframework consisting of two stages: unsupervised coarse image alignment and\nunsupervised image reconstruction. In the first stage, we design an\nablation-based loss to constrain an unsupervised homography network, which is\nmore suitable for large-baseline scenes. Moreover, a transformer layer is\nintroduced to warp the input images in the stitching-domain space. In the\nsecond stage, motivated by the insight that the misalignments in pixel-level\ncan be eliminated to a certain extent in feature-level, we design an\nunsupervised image reconstruction network to eliminate the artifacts from\nfeatures to pixels. Specifically, the reconstruction network can be implemented\nby a low-resolution deformation branch and a high-resolution refined branch,\nlearning the deformation rules of image stitching and enhancing the resolution\nsimultaneously. To establish an evaluation benchmark and train the learning\nframework, a comprehensive real-world image dataset for unsupervised deep image\nstitching is presented and released. Extensive experiments well demonstrate the\nsuperiority of our method over other state-of-the-art solutions. Even compared\nwith the supervised solutions, our image stitching quality is still preferred\nby users.",
  "text": "1\nUnsupervised Deep Image Stitching: Reconstructing\nStitched Features to Images\nLang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Member, IEEE, Yao Zhao, Senior Member, IEEE\nAbstract—Traditional feature-based image stitching technolo-\ngies rely heavily on feature detection quality, often failing to\nstitch images with few features or low resolution. The learning-\nbased image stitching solutions are rarely studied due to the lack\nof labeled data, making the supervised methods unreliable. To\naddress the above limitations, we propose an unsupervised deep\nimage stitching framework consisting of two stages: unsupervised\ncoarse image alignment and unsupervised image reconstruction.\nIn the ﬁrst stage, we design an ablation-based loss to constrain an\nunsupervised homography network, which is more suitable for\nlarge-baseline scenes. Moreover, a transformer layer is introduced\nto warp the input images in the stitching-domain space. In the\nsecond stage, motivated by the insight that the misalignments\nin pixel-level can be eliminated to a certain extent in feature-\nlevel, we design an unsupervised image reconstruction network\nto eliminate the artifacts from features to pixels. Speciﬁcally, the\nreconstruction network can be implemented by a low-resolution\ndeformation branch and a high-resolution reﬁned branch, learn-\ning the deformation rules of image stitching and enhancing the\nresolution simultaneously. To establish an evaluation benchmark\nand train the learning framework, a comprehensive real-world\nimage dataset for unsupervised deep image stitching is presented\nand released\n1. Extensive experiments well demonstrate the\nsuperiority of our method over other state-of-the-art solutions.\nEven compared with the supervised solutions, our image stitching\nquality is still preferred by users.\nIndex Terms—Computer vision, deep image stitching, deep\nhomogrpahy estimation\nI. INTRODUCTION\nRemember that all models are wrong; the practical question\nis how wrong do they have to be to not be useful.\nGeorge E. P. Box\nI\nMAGE stitching is a crucial and challenging computer\nvision task that has been well-studied in the past decades,\nwith the purpose to construct a panorama with a wider ﬁeld-\nof-view (FOV) from different images captured from different\nviewing positions. This technology can be of great use in vary-\ning ﬁelds such as biology [1], [2], medical [3], surveillance\nvideos [4], [5], autonomous driving [6], [7], virtual reality\n(VR) [8], [9].\nThis work was supported by the National Natural Science Foundation of\nChina (No.61772066, No.61972028). (Corresponding author: Chunyu Lin)\nLang Nie, Chunyu Lin, Kang Liao, Yao Zhao are with the Institute of\nInformation Science, Beijing Jiaotong University, Beijing 100044, China, and\nalso with the Beijing Key Laboratory of Advanced Information Science and\nNetwork Technology, Beijing 100044, China (email: nielang@bjtu.edu.cn,\ncylin@bjtu.edu.cn, kang liao@bjtu.edu.cn, yzhao@bjtu.edu.cn).\nShuaicheng Liu is with School of Information and Communication Engi-\nneering, University of Electronic Science and Technology of China, Chengdu,\n611731, China (liushuaicheng@uestc.edu.cn).\n1www.github.com/nie-lang/UnsupervisedDeepImageStitching\nUnsupervised Image \nReconstruction\nReferencce \nImage\nTarget \nImage\nReconstructed Feature Map\nReconstructed Pixel\nMisalignments in Pixel-level\nFig. 1. The pipeline of proposed unsupervised deep image stitching. In the\ncoarse alignment stage, the inputs are warped using a single homography. In\nthe reconstruction stage, the warped images are used for reconstructing the\nstitched image from feature to pixel.\nConventional image stitching solutions are feature-based\nmethods, where feature detection is the ﬁrst step that can\nprofoundly affect stitching performance. Then a parametric\nimage alignment model can be established using the matched\nfeatures, by which we can warp the target image to align\nwith the reference image. Finally, the stitched image can be\nobtained by assigning pixel values to each pixel in overlapping\nareas between the warped images.\nAmong these steps, establishing a parametric image align-\nment model is crucial in the feature-based methods. In fact, the\nhomography transformation is the most used image alignment\nmodel, which contains translation, rotation, scaling, and van-\nishing point transformation, accounting for the transformation\nfrom one 2D plane to another [10] correctly. However, each\nimage domain may contain multiple different depth levels in\nactual scenes, which contradicts the planar scene assumption\nof the homography. There are often ghosting effects in the\nstitched results since a single homography cannot account for\nall the alignments at different depth levels.\nConventional feature-based solutions alleviate the artifacts\nin two mainstream ways. The ﬁrst way is to eliminate the\nartifacts by aligning the target image with the reference image\nas much as possible [11]–[20]. These methods partition an\nimage into different areas and compute the homography matrix\nfor each diverse area. By exerting spatially-varying warpings\non these areas, the overlapping areas are well aligned, and the\nartifacts are signiﬁcantly reduced. The second way is to hide\nthe artifacts by researching for an optimal seam to stitch the\narXiv:2106.12859v1  [cs.CV]  24 Jun 2021\n2\nFig. 2. Motivation: the misalignments in pixel-level can be visually weakened\nin feature-level. Col 1: the results of stitching the warped images from\nunsupervised coarse alignment stage. Col 2: the results of stitching the warped\nfeatures extracted by the ‘conv1 2’ in VGG19 [27]. Col 3-4: reconstructing\nfrom feature to pixel by unsupervised reconstruction network.\nwarped images [21]–[26]. Through optimizing a seam-related\ncost, the overlapping can be divided into two complementary\nregions along the seam. Then, a stitched image is formed\naccording to two regions. The feature-based solutions can\nsigniﬁcantly reduce the artifacts in most scenes. Still, they rely\nheavily on feature detection so that the stitching performance\ncan drop sharply or even fail in scenes with few features or\nat low resolution.\nDue to the incredible feature extraction capability of Convo-\nlutional Neural Networks (CNNs), recently learning-based ap-\nproaches have achieved state-of-the-art performance in various\nﬁelds such as depth estimation [28], optical ﬂow estimation\n[29], [30], distortion rectiﬁcation [31]. Increasing researchers\ntry to apply CNNs to image stitching. In [32], [33], the CNNs\nare only used to extract feature points, while in [4], [7], [34],\nthe CNNs are proposed to stitch images with ﬁxed viewing\npositions. Regrettably, these methods are either not a complete\nlearning-based framework [32], [33], or can only be used to\nstitch images with ﬁxed views instead of arbitrary views [4],\n[7], [34]. Then, view-free deep image stitching methods [35],\n[36] are proposed to overcome the two problems simultane-\nously. In these view-free solutions, deep image stitching can\nbe completed by a deep homography module, a spatial trans-\nformer module, and a deep image reﬁned module. However,\nall the solutions are supervised methods, and there is no real\ndataset for deep image stitching because of the unavailability\nof stitched labels in actual scenes until now. Therefore, these\nnetworks can only be trained on a ‘no-parallax’ synthetic\ndataset, resulting in unsatisfying applications in real scenes.\nTo overcome the limitations of feature-based solutions and\nsupervised deep solutions, we propose an unsupervised deep\nimage stitching framework that comprises an unsupervised\ncoarse image alignment stage and an unsupervised image\nreconstruction stage. The pipeline is shown in Fig. 1. In\nthe ﬁrst stage, we coarsely align the input images using a\nsingle homography. Different from the existing unsupervised\ndeep homography solutions [37], [38] that require extra image\ncontents around the input images as supervision, we design\nan ablation-based loss to optimize our unsupervised deep\nhomography network that is more suitable for the large-\nbaseline scenes, where large-baseline is a relative concept to\nsmall-baseline in [38]. Besides, a stitching-domain transformer\nlayer is proposed to warp the input images in the stitching-\ndomain with less occupied space than the existing deep\nstitching works [35], [36]. In the second stage, we present\nan ingenious strategy to reconstruct the stitched images from\nfeature to pixel, eliminating the artifacts by unsupervised\nimage reconstruction. In particular, we design a low-resolution\ndeformation branch and a high-resolution reﬁned branch in the\nreconstruction network to learn the deformation rules of image\nstitching and enhances the resolution, respectively.\nThis reconstruction strategy is motivated by an observation:\nmisalignments in feature-level are more unnoticeable than in\npixel-level (Fig. 2 left). Compared with pixels, feature maps\nare more blurred, which indicates the misalignments in pixel-\nlevel can be eliminated to a certain extent in feature-level.\nTherefore, we believe it is easier to eliminate artifacts in\nfeature-level than in pixel-level. To implement this, we ﬁrst\nreconstruct the features of the stitched image that are as close\nto the two warped images as possible (Col 3 in Fig. 2). Then\nthe stitched image can then be reconstructed at pixel-level (Col\n4 in Fig. 2) based on the reconstructed features.\nThe existing dataset in learning-based solutions [35], [36]\nis a ‘no-parallax’ synthetic dataset that cannot represent the\npractical application scene. And the datasets in feature-based\nsolutions are too few to support deep learning training. To\nenable our framework the generalization ability in real scenar-\nios, we also propose a large real-world image stitching dataset\ncontaining varying overlap rates, varying degrees of parallax,\nand variable scenes such as indoor, outdoor, night, dark, snow,\nand zooming. Here, we deﬁne overlap rate as the percentage\nof the overlapping area in the total area of the image.\nIn experiments, we evaluate our performance in homog-\nraphy estimation and image stitching. Experimental results\ndemonstrate the superiority of our method over other state-\nof-the-art solutions in real scenes. The contributions of this\npaper are summarized as follows:\n• We present an unsupervised deep image stitching frame-\nwork consisting of an unsupervised coarse image align-\nment stage and an unsupervised image reconstruction\nstage.\n• We propose the ﬁrst large real dataset for unsupervised\ndeep image stitching (to the best of our knowledge),\nwhich we hope can work as a benchmark dataset and\npromote other related research work.\n• Our algorithm outperforms the state-of-the-art, including\nhomography estimation solutions and image stitching so-\nlutions in real scenes. Even compared with the supervised\nsolutions, our image stitching quality is still preferred by\nusers.\nII. RELATED WORK\nIn this section, we subsequently review the existing works\nin image stitching and deep homography estimation.\nA. Feature-Based Image Stitching\nAccording to different strategies to eliminate artifacts, the\nfeature-based image stitching algorithms can be divided into\nthe following two categories:\nAdaptive Warping Methods. Considering that a single trans-\nformation model is not enough to accurately align images with\n3\nparallax, the idea of combining multiple parametric alignment\nmodels to align the images as much as possible is introduced.\nIn [11], the dual-homography warping (DHW) is presented to\nalign the foreground and the background, respectively. This\nmethod works well in the scene composed of two predomi-\nnating planes but shows poor performance in more complex\nscenes. Lin et al. [12] apply multiple smoothly varying\nafﬁne (SVA) transformations in different regions, enhancing\nlocal deformation and alignment performance. Zaragoza et al.\n[13] propose the as-projective-as-possible (APAP) approach,\nwhere an image can be partitioned into dense grids, and\neach grid would be allocated a corresponding homography\nby weighting the features. In fact, APAP would still exhibit\nparallax artifacts in the vicinity of the object boundaries, for\ndramatic depth changes might occur in these areas. To get rid\nof this problem, the warping residual vectors are proposed to\ndistinguish matching features from different depth planes in\n[19], contributing to more naturally stitched images.\nSeam-Driven Methods Seam-driven image stitching methods\nare also inﬂuential, acquiring natural stitched images by hiding\nthe artifacts. Inspired by the idea of interactive digital pho-\ntomontage [39], Gao et al. [24] propose to choose the best\nhomography with the lowest seam-related cost from candidate\nhomography matrices. Then the artifacts are hidden through\nseam cutting. Referring to the optimization strategy of content-\npreserving warps (CPW) [40], Zhang and Liu [22] propose a\nseam-based local alignment approach while maintaining the\nglobal image structure using an optimal homography. This\nwork was also extended to stereoscopic image stitching [41].\nUsing the iterative warp and seam estimation, Lin et al. [23]\nﬁnd the optimal local area to stitch images, which can protect\nthe curve and line structure during image stitching.\nThese feature-based algorithms contribute to perceptually\nnature stitched results. However, they rely heavily on the\nquality of feature detection, often failing in scenes with few\nfeatures or at low resolution.\nB. Learning-Based Image Stitching\nGetting a real dataset for stitching is difﬁcult. In addition,\ndeep stitching is quite challenging for the scenes with low\noverlap rate and large parallax. Subjected to these two prob-\nlems, learning-based image stitching is still in development.\nView-Fixed Methods. View-ﬁxed image stitching methods are\ntask-driven, which are designed for the speciﬁc application\nscenes such as autonomous driving [6], [7], surveillance videos\n[4]. In these works, the end-to-end networks are proposed to\nstitch images from ﬁxed views while they cannot be extended\nto stitch images from arbitrary views.\nView-Free Methods. To stitch images from arbitrary views\nusing CNNs, some researchers propose to adopt CNNs in the\nstage of feature detection [32], [33]. However, these methods\ncan not be regarded as a complete learning-based framework\nstrictly. The ﬁrst complete learning-based framework to stitch\nimages from arbitrary views was proposed in [35]. The images\ncan be stitched through three stages: homography estimation,\nspatial transformation, and content reﬁnement. Nevertheless,\nthis work cannot handle input images with arbitrary resolutions\ndue to the fully connected layers in the network, and the\nstitching quality in real applications is unsatisfying. Following\nthis deep stitching pipeline, an edge-preserved deep image\nstitching solution was proposed in [36], freeing the limitation\nof input resolution and signiﬁcantly improving the stitching\nperformance in real scenes.\nC. Deep Homography Schemes\nThe ﬁrst deep homography method was put forward in [42],\nwhere a VGG-style [27] network was used to predict the eight\noffsets of four vertices of an image, thus uniquely determine\na corresponding homography. Nguyen et al. [37] proposed\nthe ﬁrst unsupervised deep homography approach with the\nsame architecture as [42] with an effective unsupervised loss.\nIntroducing spatial attention to deep homography network,\nZhang et al. [38] proposes a content-aware unsupervised\nnetwork, contributing to SOTA performance in small-baseline\ndeep homography. In [43], multi-scale features are extracted\nto predict the homography from coarse to ﬁne using image\npyramids.\nBesides that, the deep homography network is usually\nadopted as a part of the view-free image stitching frameworks\n[35], [36]. Different from [37], [38], [42], [43], the deep\nhomography in image stitching is more challenging, for the\nbaseline between input images is usually 2X∼3X larger.\nIII. UNSUPERVISED COARSE IMAGE ALIGNMENT\nGiven two high-resolution input images, we ﬁrst estimate\nthe homography using a deep homography network in an\nunsupervised manner. Then the input images can be warped\nto align each other coarsely in the proposed stitching-domain\ntransformer layer.\nA. Unsupervised Homography\nThe existing unsupervised deep homography methods [37],\n[38] take the image patches as the input, which is shown in\nthe white squares in Fig. 3 (a). The objective function of these\nmethods can be expressed as Eq. (1):\nLP W =\n\r\rP(IA) −P(H(IB))\n\r\r\n1 ,\n(1)\nwhere IA, IB represent the full images of the reference image\nand the target image, respectively. P(·) is the operation of\nextracting an image patch from a full image, and H(·) warps\none image to align with the other using estimated homography.\nFrom Eq. (1), we can see that to make the warped target\npatch close to the reference patch, the extra contents around\nthe target patch are utilized to pad the invalid pixels in the\nwarped target patch. We call it a padding-based constraint\nstrategy. This strategy works well in small-baseline [38], or\nmiddle-baseline [37] homography estimations while it fails\nin the large-baseline case. In particular, when the baseline\nis too large (as illustrated in Fig. 3 (a)), there might be no\noverlapping area between the input patches, which leads to the\nmeaningless estimation of homography from these patches.\n4\n(a) A failure case of padding-based\nstrategy.\n(b)\nThe\nproposed\nablation-based\nstrategy.\nFig. 3. An instance to show that the proposed ablation-based strategy is more\nsuitable for large-baseline unsupervised homography estimation.\n(a)\n(b)\nFig. 4.\nThe comparison between the spatial transformer layer in existing\ndeep image stitching and our stitching-domain transformer layer. (a): Warping\nby spatial transformer layer in existing deep image stitching [35], [36]. (b):\nWarping by our stitching-domain transformer layer.\nTo solve this problem, we design an ablation-based strategy\nto constrain large-baseline unsupervised homography estima-\ntion. Speciﬁcally, we take the full images as the input, ensuring\nthat all overlapping areas are included in our inputs. When\nwe enforce the warped target image close to the reference\nimage, we no longer pad the invalid pixels in the warped\nimage. Instead, we ablate the contents in the reference image\nwhere the invalid pixels in the warped target image locate, as\nshown in Fig. 3 (b). Our objective function for unsupervised\nhomography is formulated as Eq. (2):\nL\n′\nP W =\n\r\rH(E) ⊙IA −H(IB)\n\r\r\n1 ,\n(2)\nwhere ⊙is the pixel-wise multiplication and E is an all-one\nmatrix with identical size with IA.\nAs for the architecture of our unsupervised homography\nnetwork, we adopt a multi-scale deep model proposed in [36],\nwhich connects feature pyramid and feature correlation in a\nuniﬁed framework so that it can predict the honography from\ncoarse to ﬁne and handle relative large-baseline scenes.\nB. Stitching-Domain Transformer Layer\nThe spatial transformer layer was ﬁrst proposed in [44],\nwhere images can be spatially transformed with gradient\nbackpropagation guaranteed using the homography model. In\nimage stitching, input images of the same resolution can\noutput stitched images of different resolution according to\nthe varying overlapping rates, which brings a considerable\nchallenge to deep image stitching. The existing deep image\nstitching methods solve this problem by extending the spatial\ntransformer layer [35], [36]. Speciﬁcally, these solutions deﬁne\na maximum resolution for the stitched image so that all the\ninput contents can be included in the output. In addition, the\nnetwork will output images with the same resolution every\ntime. However, most of the space occupied by black pixels\noutside the white box in Fig. 4 (a) are wasted. To deal\nwith spatial waste, we propose a stitching-domain transformer\nlayer. We deﬁne the stitching-domain as the smallest bounding\nrectangle of the stitched image, which saves the most space\nwhile ensuring the integrity of the image contents. The warped\nresults of ours are illustrated in Fig. 4 (b), and our stitching-\ndomain transformer layer can be implemented as follows.\nFirst, we calculate the coordinates of the 4 vertices in the\nwarped target image by Eq. (3):\n(xW\nk , yW\nk ) = (xB\nk , yB\nk ) + (∆xk, ∆yk), k ∈{1, 2, 3, 4},\n(3)\nwhere (xW\nk , yW\nk ), (xB\nk , yB\nk ) are the k-th vertex coordinates of\nthe warped target image and the target image, respectively.\n(∆xk, ∆yk) donate the offsets of the k-th vertex that are es-\ntimated form the aforementioned homogrpahy network. Then,\nthe size of the warped image (H∗× W ∗) can be obtained by\nEq. (4):\nW ∗=\nmax\nk∈{1,2,3,4}{xW\nk , xA\nk } −\nmin\nk∈{1,2,3,4}{xW\nk , xA\nk },\nH∗=\nmax\nk∈{1,2,3,4}{yW\nk , yA\nk } −\nmin\nk∈{1,2,3,4}{yW\nk , yA\nk },\n(4)\nwhere (xA\nk , yA\nk ) are the vertex coordinates of the reference\nimage that have the same values as (xB\nk , yB\nk ). Finally, we\nassign the speciﬁc values to the pixels of the warped images\n(IAW , IBW ) from the input images (IA, IB), which can be\nrepresented as Eq. (5):\nIAW = W(IA, I),\nIBW = W(IB, H),\n(5)\nwhere I and H are the identity matrix and the estimated\nhomography matrix, respectively. And W(·) donates the oper-\nation of warping an image using a 3×3 transformation matrix\nwith the stitching-domain set to H∗× W ∗.\nIn this way, we transform the input images in the stitching-\ndomain space, effectively reducing the space occupied by fea-\nture maps in the subsequent reconstruction network. Compared\nwith the transformer layer used in [35], [36], the proposed\nlayer can help to stitch larger resolution images when the GPU\nmemory is limited.\nIV. UNSUPERVISED IMAGE RECONSTRUCTION\nConsidering the limitation that a single homography can\nonly represent the spatial transformation in the same depth\n[10], the input images cannot be completely aligned in the\nreal-world dataset in the ﬁrst stage. To break the bottleneck\nof single homography, we propose to reconstruct the stitched\nimage from feature to pixel. The overview of the proposed\nunsupervised deep image stitching framework is illustrated in\nFig. 5. The reconstruction network can be implemented by\ntwo branches: low-resolution deformation branch (Fig. 5 top)\nand high-resolution reﬁned branch (Fig. 5 bottom), learning\nthe deformation rules of image stitching and enhancing the\nresolution, respectively.\n5\nLarge-Baseline \nDeep Homography\nStitching-Domain\nTransformer\nTensor \nDLT\nC\nO\nN\nV\nR\nE\nL\nU\nC\nO\nN\nV\nS\nU\nM\nR\nE\nL\nU\nC\nO\nN\nV\nR\nE\nL\nU\nC\nO\nN\nV\nS\nU\nM\nR\nE\nL\nU\n……\nLR-HR Content \nConsistency Loss\n×\nLR Deformation \nLoss\n×\nHR Deformation \nLoss\nInput images\nOutput\nSkip Connection\nLoss Connection\nMultiplication\nLR/HR Content Mask\nLR/HR Seam Mask\n·\nConcatenation\nLegend\nH*×W*\n256×256\n256×256\n(H*×W*)\nDown-\nsampling\nUp-sampling\nUnsupervised Coarse Image Alignment\nUnsupervised Image Reconstruction\nH×W\n·\n·\n·\nFig. 5.\nAn overview of our unsupervised deep image stitching. Left: the unsupervised coarse image alignment stage. Right: the unsupervised image\nreconstruction stage.\nFig. 6.\nLearning deformation rules with masks in low-resolution. From\nleft to right, each column represents input images (IA, IB), low-resolution\nwarped images (IAW , IBW ), content masks (MAC, MBC), and seam\nmasks (MAS, MBS).\nA. Low-Resolution Deformation Branch\nReconstructing the images only in the high-resolution\nbranch is not appropriate because the receptive ﬁeld decreases\nrelatively as the resolution increases. To ensure that the recep-\ntive ﬁeld of the network can completely perceive misaligned\nregions (especially in the case of high resolution and large\nparallax), we designed a low-resolution branch to learn the\ndeformation rules of image stitching ﬁrst. As shown in Fig.\n5(top), the warped images are ﬁrst down-sampled to a low-\nresolution, deﬁned as 256×256, in our implementation. Then\nan encoder-decoder network consisting of 3 pooling layers and\n3 deconvolutional layers is used to reconstruct the stitched\nimage. The ﬁlter numbers of the convolutional layers are set\nto 64, 64, 128, 128, 256, 256, 512, 512, 256, 256, 128, 128,\n64, 64, and 3, respectively. Furthermore, skip connections are\nadopted to connect the low-level and high-level features with\nthe same resolution [45].\nIn this process, the deformation rules of image stitching\nare learned with content masks and seam masks (Fig. 6).\nThe content masks are adopted to constrain the features of\nthe reconstructed image close to the warped images, while\nthe seam masks are designed to constrain the edges of the\noverlapping areas to be natural and continuous. In particular,\nwe obtain the content masks (M AC, M BC) using Eq. (5) by\nreplacing the IA, IB with an all-one matrix EH×W , and the\nFig. 7. The outputs of the low-resolution branch and high-resolution branch.\nThe high-resolution branch is designed to enhance the resolution and reﬁne\nthe stitched image.\nseam masks can be calculated by Eq. (6) and Eq. (7):\n∇M AC = |M AC\ni,j −M AC\ni−1,j| + |M AC\ni,j −M AC\ni,j−1|,\n∇M BC = |M BC\ni,j −M BC\ni−1,j| + |M BC\ni,j −M BC\ni,j−1|,\n(6)\nM AS = C(∇M BC ∗E3×3 ∗E3×3 ∗E3×3) ⊙M AC,\nM BS = C(∇M AC ∗E3×3 ∗E3×3 ∗E3×3) ⊙M BC, (7)\nwhere (i, j) donates the coordinate location, ∗represents the\noperation of convolution, and C clips all the elements to\nbetween 0 and 1. Then we design the content loss and seam\nloss in low-resolution as Eq. (8) and Eq. (9):\nLl\nContent =LP (SLR ⊙M AC, IAW )\n+LP (SLR ⊙M BC, IBW ),\n(8)\nLl\nSeam =L1(SLR ⊙M AS, IAW ⊙M AS)\n+L1(SLR ⊙M BS, IBW ⊙M BS)\n(9)\nwhere SLR is the low-resolution stitched image. L1 and LP\ndonate the L1 loss and the perceptual loss [46], respectively.\nTo make the feature of the reconstructed image as close to that\nof the warped images as possible, we calculate the perceptual\nloss on layer ‘conv5 3’ of VGG-19 [27] which is deep enough\nto shrink the feature difference between the warped images.\nNext, the total loss function of low-resolution unsupervised\ndeformation can be formulated as Eq. (10):\n6\nConv1_2\nConv2_2\nConv3_2\nConv4_2\nConv5_2\nConv6_2\nConv7_2\nOutput\nEncoder\nDecoder\nOverlapping region\nFig. 8. Visualization of the learning process of the low-resolution deformation branch. The stitched images are reconstructed from overlapping regions to\nnon-overlapping regions.\nLLR = λcLl\nContent + λsLl\nSeam\n(10)\nwhere λs and λc weight the contribution of the content\nconstraint and seam constraint.\nB. High-Resolution Reﬁned Branch\nAfter the initialized deformation in the low-resolution\nbranch, we develop a high-resolution reﬁned branch to en-\nhance the resolution and reﬁne the stitched image. The high-\nresolution refers to the resolution of the output of the ﬁrst\nstage. Actually, in our dataset, the resolution is bigger than\n512×512. To illustrate the effect of high-resolution branch,\nwe exhibit the outputs of two branches in Fig. 7. This branch\nis composed of convolutional layers entirely, as shown in\nFig. 5 (bottom), which means it can deal with pictures of\narbitrary resolution. To be speciﬁc, it consists of three separate\nconvolutional layers and eight resblocks [47], of which the\nﬁlter number of each layer is set to 64 except that of the last\nlayer is set to 3. To prevent low-level information from being\ngradually forgotten as the convolutional network gets deep, the\nfeature of the ﬁrst layer is added with that of the penultimate\nlayer. Moreover, each resblock is composed of convolution,\nrelu, convolution, sum, and relu.\nWe up-sample SLR to the resolution of the warped images\nand concatenate them together as the input of this branch.\nThe output is the high-resolution stitched image SHR. And\nwe conclude the loss function of the high-resolution reﬁned\nbranch LHR imitating Eq. (10) as Eq. (11):\nLHR = λcLh\nContent + λsLh\nSeam\n(11)\nwhere Lh\nContent and Lh\nSeam are the content loss and seam\nloss in high-resolution which can be calculated using Eq. (8),\n(9) by replacing the SLR and low-resolution masks with the\nSHR and the high-resolution masks. When calculating the LP\nin high resolution, we adopt the layer ‘conv3 3’ of VGG-19,\nsince this layer is shallower than the layer ‘conv5 3’ (used in\nLP of low resolution) and the output using this layer is more\nclear.\nC. Objective Function\nThe high-resolution branch is designed to reﬁne the stitched\nimage, but it tends to cause artifacts in the stitched image,\nsince the increase in resolution can relatively reduce the\nreceptive ﬁeld of the network (more details can be found in\nSection V-D). To enable our network the abilities to enhance\nresolution and to eliminate parallax artifacts simultaneously, a\ncontent consistency loss is proposed as Eq. (12):\nLCS =\n\r\rS256×256\nHR\n−SLR\n\r\r\n1 ,\n(12)\nwhere S256×256\nHR\nis obtained by resizing SHR to 256×256 that\nis the resolution of the output in low-resolution branch.\nTaking all the constraints into consideration, we conclude\nour objective function of the image reconstruction stage as Eq.\n(13):\nLR = ωLRLLR + ωHRLHR + ωCSLCS,\n(13)\nwhere the ωLR, ωHR and ωCS represent weights of each part.\nD. Reconstruction from Feature to Pixel\nTo exhibit the learning process from feature to pixel, we\nvisualized the feature maps of the low-resolution deformation\nbranch in Fig. 8. At the very beginning of the encoder stage,\nthe network only focuses on the overlapping areas, and the\nfeatures of non-overlapping areas are all suppressed. Next, as\nthe resolution decreases, deeper semantic features are extracted\nand reconstructed. In the decoder stage, the network begins\nto pay attention to non-overlapping areas besides overlapping\nareas. As the resolution is restored, clearer feature maps are\nreconstructed. Finally, the stitched image is reconstructed at\nthe pixel level.\nV. EXPERIMENTS\nIn this section, extensive experiments are conducted to\nvalidate effectiveness of the proposed method.\nA. Dataset and Implement Details\nDataset. To train our network, we also propose an unsu-\npervised deep image stitching dataset that is obtained from\nvariable moving videos. Of these videos, some are from [38]\nand the others are captured by ourselves. By extracting the\nframes from these videos with different interval time, we\nget the samples with different overlap rates (Fig. 9 (b)).\nMoreover, these videos are not captured by the camera rotating\naround the optical center, and the shot scenes are far from a\n7\n(a) Varying scenes in our dataset.\n(b) Varying overlap rates in our dataset.\n(c) Varying degrees of parallax in our dataset.\nFig. 9.\nIllustrations of our proposed unsupervised deep image stitching\ndataset.\nplanar structure, which means this dataset contains different\ndegrees of parallax (Fig. 9 (c)). Besides, this real-world dataset\nincludes variable scenes such as indoor, outdoor, night, dark,\nsnow, and zooming (Fig. 9 (a)).\nTo quantitatively describe the distribution of different over-\nlap rates and varying degrees of parallax in our dataset. We\ndivide the overlap rates into 3 levels and deﬁne a high overlap\nrate greater than 90%, a middle overlap rate ranging from\n60%-90%, and a low overlap rate lower than 60%. This\nclassiﬁcation criterion is formulated according to [37], [38],\n[42], where [38] is the represnetative work in high overlap rate.\nThe average overlap rate of the proposed dataset is greater than\n90%. And [37], [42] are the representative works in middle\noverlap rate for the average overlap rate of Warped COCO\n(disturbance < 32) dataset [42] is about 75%. Besides, to\ndescribe parallax accurately, we align the target image with the\nreference image using a global homography and then calculate\nthe maximum misalignment error of corresponding feature\npoints in the coarse aligned images to show the magnitude of\nparallax. In this way, we divide the parallax into 2 levels: small\nparallax with error smaller than 30 pixels and large parallax\nwith error greater than 30 pixels. Fig. 9 (c) demonstrates the\ndifference of different parallax intuitively.\nIn particular, we get 10,440 cases for training and 1,106 for\ntesting. Among our dataset, the ratios of overlap rates from\nhigh to low are about 16%, 66%, and 18%, while the ratios of\nparallax from small to large are about 91% and 9%. Although\nour dataset contains no ground-truth, we include our testing\nresults in this dataset, which we hope can work as a benchmark\ndataset for other researchers to follow and compare.\nDetails. We train our unsupervised image stitching framework\nin three steps. First, we train our deep homography network\non the synthetic dataset (Stitched MS-COCO [35]) for 150\nepochs. Second, we ﬁnetune the homography network on the\nproposed real dataset for 50 epochs. Third, we train the deep\nimage reconstruction network on the proposed real dataset for\n20 epochs. All the training process is unsupervised, which\nmeans our framework only takes the reference/target image\nas input and requires no label. The optimizer is Adam [48]\nwith an exponentially decaying learning rate with an initial\nvalue of 10−4. We set λs and λc to 2 and 10−6. And\nωLR, ωHR and ωCS are set to 100, 1 and 1, respectively.\nIn testing, it takes about 0.4s to stitch 2 input images with\nresolution of 512×512. All the components of this framework\nare implemented on TensorFlow. Both the training and testing\nare conducted on a single GPU with NVIDIA RTX 2080 Ti.\nB. Comparison of Homography Estimation\nTo evaluate the performance of the proposed ablation-based\nunsupervised deep homography objectively, we compare our\nsolution with I3×3, SIFT [49]+RANSAC [50], DHN [42],\nUDHN [37], CA-UDHN [38], and LB-DHN [36] on the\nsynthetic dataset and real dataset respectively. The I3×3 refers\nto a 3 × 3 identity matrix as a ‘no-warping’ homography for\nreference, and SIFT+RANSAC is chosen as the representative\nof traditional homography solutions because it outperforms\nmost traditional solutions as shown in [37], [38]. The DHN,\nUDHN, CA-UDHN, and LB-DHN are the deep learning solu-\ntions, of which UDHN and CA-UDHN are the unsupervised\nsolutions that both adopt the padding-based strategy to train\ntheir networks.\nSynthetic dataset. The ﬁrst comparative experiment is con-\nducted on Warped MS-COCO that is the most known synthetic\ndataset for deep homography estimation. All the learning\nmethods are trained on Warped MS-COCO. The results are\nillustrated in Table I(a), where ‘Ours v1’ is our model trained\nwith this dataset in an unsupervised manner. From Table I(a),\nwe can observe:\n(1) Ours v1 outperforms the existing unsupervised deep\nhomography methods (UDHN, CA-UDHN), of which CA-\nUDHN is the SOTA solution in small-baseline deep homogra-\nphy. However, the performance of CA-UDHN in this dataset\ndegenerates to be close to that of I3×3 due to its limited\nreceptive ﬁeld.\n(2) After adopting our ablation-based unsupervised loss to\nLB-DHN, 4pt-Homography RMSE increases, which means\nthis loss is not suitable for this ‘no-parallax’ synthetic dataset.\nReal Dataset. Then, we carry on a comparison on the\nproposed real dataset, which consists of varying degrees of\nparallax. Since this dataset lacks ground truth, we adopt the\n8\nTABLE I\nCOMPARISON EXPERIMENT ON HOMOGRAPHY ESTIMATION. THE 1ST AND 2ND BEST SOLUTIONS ARE MARKED IN RED AND BLUE, RESPECTIVELY.\n(a) 4pt-Homography RMSE (↓) on Warped MS-COCO (synthetic)\nMethod\nTraditional homography\nDeep homography (supervised)\nDeep homography (unsupervised)\nI3×3\nSIFT [49]+RANSAC [50]\nDHN [42]\nLB-DHN [36]\nUDHN [37]\nCA-UDHN [38]\nOurs v1 (synthetic)\nTop 0∼30%\n15.0154\n0.6743\n3.2998\n0.2719\n2.1894\n15.0082\n1.1773\n30∼60%\n18.2515\n1.0964\n4.8839\n0.4140\n3.5272\n18.2498\n1.4544\n60∼100%\n21.3517\n19.0286\n7.6944\n0.9632\n6.4984\n21.3618\n3.0702\nAverage\n18.5220\n9.4782\n5.5358\n0.5962\n4.3179\n18.5234\n2.0239\n(b) PSNR (↑) of the overlapping regions on the proposed dataset (real)\nMethod\nTraditional homography\nDeep homography (supervised)\nDeep homography (unsupervised)\nI3×3\nSIFT [49]+RANSAC [50]\nDHN [42]\nLB-DHN [36]\nUDHN [37]\nOurs v1 (synthetic)\nOurs v2 (real)\nTop 0∼30%\n16.1923\n25.2300\n16.3957\n24.7515\n19.3851\n26.1958\n27.8386\n30∼60%\n13.0546\n22.2308\n13.3648\n21.1436\n15.9251\n22.6115\n23.9451\n60∼100%\n10.8747\n17.5791\n11.5001\n18.4594\n13.1016\n19.5277\n20.7013\nAverage\n13.1151\n21.2541\n13.5191\n21.1418\n15.8252\n22.4421\n23.8045\n(c) SSIM (↑) of the overlapping regions on the proposed dataset (real)\nMethod\nTraditional homography\nDeep homography (supervised)\nDeep homography (unsupervised)\nI3×3\nSIFT [49]+RANSAC [50]\nDHN [42]\nLB-DHN [36]\nUDHN [37]\nOurs v1 (synthetic)\nOurs v2 (real)\nTop 0∼30%\n0.3869\n0.8598\n0.4088\n0.8249\n0.5732\n0.8671\n0.9023\n30∼60%\n0.1730\n0.7662\n0.1699\n0.7124\n0.3344\n0.7844\n0.8298\n60∼100%\n0.0732\n0.5583\n0.0772\n0.5497\n0.1651\n0.6270\n0.6846\nAverage\n0.1969\n0.7105\n0.2042\n0.6805\n0.3379\n0.7456\n0.7929\nPSNR and SSIM of the overlapping regions to evaluate the\nperformance, which can be calculated as Eq. (14):\nPSNRoverlap = PSNR(H(E) ⊙IA, H(IB)),\nSSIMoverlap = SSIM(H(E) ⊙IA, H(IB)),\n(14)\nwhere PSNR(·) and SSIM(·) donates the operations of\ncomputing PSNR and SSIM between two images, respectively.\nWe test DHN and UDHN using the public pretrained models.\nLB-DHN and Ours v1 are trained on Stitched MS-COCO [35]\nwhich is similar to Warped MS-COCO with lower overlap rate.\nOurs v2 is the model of ﬁnetuning Ours v1 about 50 epochs\non the proposed real dataset. By analyzing the results shown\nin Table I(b) I(c), we can conclude:\n(1) The proposed unsupervised solution (Ours v2) outper-\nforms all the methods, including the supervised ones in the\nreal dataset.\n(2) Although Ours v1 and LB-DHN are both trained on the\nsynthetic dataset, Ours v1 achieves better performance under\nthe real dataset, which indicates the proposed unsupervised\nloss can equip the network with better generalization ability.\nC. Comparison of Image Stitching\nTo verify our method’s superiority in image stitching, we\ncompare our method with feature-based solutions and compare\nwith recent learning-based solutions (even if it is not fair\nto compare our unsupervised algorithms with the supervised\nones).\n1) Compared with Feature-Based Solutions\nIn this section, we choose global Homography [10], APAP\n[13], robust ELA [18] as the representatives of feature-based\nsolutions to compare with our algorithms. Of these methods,\nFig. 10.\nDemonstration of ‘failure’. Top: signiﬁcant distortion. Bottom:\nintolerable artifacts.\nwe implement Homography with global projective transfor-\nmation, and we get the stitched results of APAP and robust\nELA (adaptive warping methods) by running their open-source\ncodes with our testing instances. After alignment, image\nfusion is adopted to produce the stitched image and reduce\nartifacts. Speciﬁcally, we fuse the warped images with the\npixel-weighted principle, assigning a relatively large weight\nto the pixel with a high intensity value.\nStudy on Robustness. The performance of feature-based\nsolutions is easily affected by the quantity and distribution\nof the feature points, resulting in weak robustness in varying\nscenes. By contrast, the proposed method overcomes this prob-\nlem. To validate this view, we test the feature-based methods\nand ours on our test set (1,106 samples). To simulation the\nchange of feature quantity, we resize the test set to different\nresolutions, e.g., 512 × 512, 256 × 256, and 128 × 128. As\nthe resolution decreases, the number of features decreases\nexponentially. The results are shown in Table II, where ‘error’\nindicates the number of program crashes and ‘failure’ refers to\nthe number of stitching unsuccessfully. Speciﬁcally, we deﬁne\nsigniﬁcant distortion (Fig. 10 top) and intolerable artifacts\n(Fig. 10 bottom) as ‘failure’. All the stitched results of these\n9\nTABLE II\nCOMPARISON OF ROBUSTNESS FOR IMAGE STITCHING. THE NUMBER OF TESTING CASES IS 1,106.\nInput resolution\nMetrics\nFeature-based\nLearning-based (supervised)\nLearning-based (unsupervised)\nHomography [10]\nAPAP [13]\nrobust ELA [18]\nVFISNet [35]\nEPISNet [36]\nOurs\n512×512\nError\n0\n3\n0\n-\n0\n0\nFailure\n86\n31\n111\n-\n22\n15\nTotal\n86\n34\n111\n-\n22\n15\nSuccess rate\n92.22%\n96.93%\n89.96%\n-\n98.01%\n98.64%\n256×256\nError\n0\n10\n0\n-\n0\n0\nFailure\n88\n40\n124\n-\n22\n15\nTotal\n88\n50\n124\n-\n22\n15\nSuccess rate\n92.04%\n95.48%\n88.79%\n-\n98.01%\n98.64%\n128×128\nError\n1\n158\n9\n0\n0\n0\nFailure\n206\n66\n214\n131\n32\n15\nTotal\n207\n224\n223\n131\n32\n15\nSuccess rate\n81.28%\n79.75%\n79.84%\n88.16%\n97.11%\n98.64%\nFig. 11. Challenging samples to compare the robustness more intuitively in\nthe scenes of indoors and dark. Row 1: indoors. Row 2: dark. Row 3: image\naugmentation to the dark scene. The resolution of the inputs is 512 × 512.\nmethods will be public with our dataset. Comparing the\nsuccess rates in Table II, we can observe:\n(1) Ours is more robust than the feature-based methods.\nIn fact, the ‘error’ and ‘failure’ cases of the feature-based\nsolutions are mainly distributed in low-light and indoor scenes,\nwhile ours performed well in these challenging scenes.\n(2) As the resolution decreases, the success rates of\nlearning-based methods decrease while ours remains robust.\nBesides, to perceive the robustness more intuitively, Fig.\n11 demonstrates two challenging examples in the scenes of\nindoors and dark. Since the sample in dark is too dark to see\nclearly, we impose image augmentation to better exhibit these\nresults (Row 3 in Fig. 11). These examples are challenging\nfor the feature-based solutions because the features in these\nscenes are hard to detect. In contrast, our solution stitches them\nsuccessfully due to the fantastic feature extraction capabilities\nof CNNs.\nStudy on Visual Quality. The proposed deep image stitch-\ning framework should be regarded as a whole which takes two\nimages from arbitrary views as inputs and outputs the stitched\nresult. Therefore, the traditional indicator that calculates the\nsimilarity of the overlapping regions is not suitable for our\nmethod. To compare with other methods quantitatively, we\ndesign user studies on visual quality. Speciﬁcally, we compare\nour method with Homography, APAP, and robust ELA one\nby one. At each time, four images are shown on one screen:\nFig. 12. User study on visual quality: compared with feature-based methods.\nThe numbers are shown in percentage and averaged on 20 participants.\nthe inputs, our stitched result, and the result from Homog-\nraphy/APAP/robust ELA. The results of ours and the other\nmethod are illustrated in random order each time. The user\nmay zoom-in on the images and is required to answer which\nresult is preferred. In the case of “no preference,” the user\nneeds to answer whether the two results are “both good” or\n“both bad”. The studies are carried out in our testing set, which\nmeans every user has to compare each method with ours in\n1,106 images. In this study, we invite 20 participants, including\n10 researchers/students with computer vision backgrounds and\n10 volunteers outside this community.\nThe results are shown in Fig. 12. Neglecting the ratio of both\ngood and both bad, we ﬁnd that preferring ours is signiﬁcantly\nmore than preferring other methods, which means our results\nhave higher visual quality in users’ evaluation.\nTo further demonstrate our performance, we also display\nthe stitched results on the proposed real dataset (row 1-8 in\nFig. 13) and on the classic image stitching instances outside\nof our dataset (row 9-10 in Fig. 13). All the cases are with\nvarying degrees of parallax. Besides promising visual quality,\nit veriﬁes the generalization ability of our model.\n2) Compared with Learning-Based Solutions\nThe existing learning-based image stitching methods (VFIS-\nNet [35] and EPISNet [36]) are supervised learning methods,\nwhich require extra labels to train the network. In the case\nthat it is unfair to compare our unsupervised solution with\nthe supervised ones, our method still exhibits a superiority\nover them on robustness, continuity, illumination, and visual\nquality.\n10\nFig. 13. Visual comparison of the image stitching quality. Row 1-8: instances with varying degrees of parallax from the proposed dataset. Row 9-10: “yard”\n[24] and “temple” [11] (classic image stitching instances outside of our dataset).\nStudy on Robustness. VFISNet is the ﬁrst deep image\nstitching work that can stitch images from arbitrary views\nin a complete deep learning framework. However, it has\na nonnegligible shortcoming: it can only stitch images of\n128 × 128. Therefore, only the result under the resolution\nof 128 × 128 is given when measuring its robustness. The\ndetailed results in Table II shows that the robustness of ours\nis better than other supervised ones. This can be accounted for\nby the following two reasons: (1) Our unsupervised deep ho-\nmography model outperforms the other methods on robustness,\n11\n(a) Comparison of edge continuity. Left: EPISNet [36]. Right: ours.\n(b) Comparison of illumination difference. Left: EPISNet [36]. Right:\nours.\nFig. 14. Study on continuity and illumination.\nwhich signiﬁcantly reduces failure cases caused by inaccurate\nhomography estimation.\n(2) Our unsupervised deep image reconstruction model can\neffectively reduce artifacts by reconstructing the stitched image\nfrom feature to pixel, which reduces failure cases caused by\nintolerant artifacts.\nStudy on Continuity. The supervised deep image stitching\nmethods [35], [36] sacriﬁce the continuity of the edges (the\nedges between the reference image and the non-overlapping\nareas of the target image) to minimize artifacts. Although an\nedge-preserved network is proposed in EPISNet to weaken\nthis problem, this problem still exists in a few testing cases.\nThe discontinuity is demonstrated in the left picture of Fig. 14\n(a), where discontinuous areas are framed and enlarged. This\nproblem is settled perfectly in our unsupervised approach, as\nshown in the right picture of Fig. 14 (a). It gives credit to our\nconstraint on seam masks, which enforces the edges of the\noverlapping areas close to one of the warped images.\nStudy on Illumination. Another advantage of our method\nis that ours can smooth the illumination difference between the\ntwo images. The comparison with EPISNet are illustrated in\n14 (b). The supervised methods fail to smooth the illumination\ndifference because they are trained in a synthetic dataset with\nno illumination difference in the input images (the supervised\nmethods cannot be trained in a real dataset due to the lack\nof stitched labels). On the contrary, our method is trained in\nreal scenes, which can effectively learn how to smooth the\nillumination difference caused by different shooting positions.\nStudy on Visual Quality. Similar to the user study with\nfeature-based methods, we adopt the same strategy to in-\nFig. 15. User study on visual quality: compared with learning-based methods.\nThe numbers are shown in percentage and averaged on 20 participants.\nTABLE III\nFRAMEWORKS FOR ABLATION STUDIES.\nArchitecture\nLoss\nLR branch\nHR branch\nContent loss\nSeam loss\nCS loss\nv1\n✓\n✓\nv2\n✓\n✓\n✓\nv3\n✓\n✓\n✓\n✓\nOurs\n✓\n✓\n✓\n✓\n✓\nvestigate every participant to compare our method with the\nexisting learning-based ones. Considering VFISNet can only\nwork on the resolution of 128 × 128, we use Bicubic interpo-\nlation to resize the stitched images. The results are shown in\nFig. 15. Since Bicubic interpolation inevitably brings blurs\nwhen zooming in on images, the probability of preferring\nour method is further greater than that of preferring VFIS-\nNet+Bicubic. Even compared with EPISNet, our method is\nstill preferred on the visual quality of the stitched images.\nBesides that, Fig. 13 exhibits the visual comparative results\nwith these supervised methods, where the green rectangles\nindicate the severely blurred regions and the red rectangles\npoint to discontinuous edges.\nTo perceive our visual quality more intuitively, more results\nare illustrated in Fig. 16, where the inputs and the outputs are\ndemonstrated together.\nD. Ablation Studies\nIn this section, ablation studies are performed on both\nnetwork architectures and loss functions. In the architecture,\nwe validate the effectiveness of the low-resolution branch (LR\nbranch) and high-resolution branch (HR branch); in the loss,\nwe test the function of the content loss, seam loss, and content\nconsistency loss (CS loss). The properties of all the studied\nframeworks are shown in Table III.\nFrom the results which are illustrated in Fig. 17, we can\nobserve:\n(1) The most straightforward combination of LR branch\nand content loss can realize image stitching. However, there\nare still two issues unresolved: seam distortions (row 1, col 4\nin Fig. 17) and limited resolution. In our analysis, the seam\ndistortion is the side effect of the proposed content loss.\n(2) Compared v2 with v1, the HR branch can effectively\nenhance the resolution of the stitched image. As the cost, a\nfew artifacts (row 2, col 2 in Fig. 17) are introduced since the\nreceptive ﬁeld of HR branch convolution kernels is too small\nfor higher resolution images.\n12\n(a) Results on classic instances outside of our dataset. From left to right: “roof” [51], “theater” [20], “street” [52], “roadside” [14], and “ofﬁcedesk” [20].\n(b) Results on our proposed dataset. From left to right: “stairs”, “snow”, “grass”, “lake”, and “campus”.\nFig. 16. More results of ours.\nv1\nv2\nv3\nOurs\nFig. 17.\nAblation studies on our framework. Col 1: outputs of different\nframeworks. Col 2-4: enlarged image patches to show the differences on\nartifacts, deﬁnition, and seam distortions, respectively.\n(3) Compared with v2, v3 removes the seam distortions\n(row 3, col 4 in Fig. 17) using the proposed seam loss.\nBy imposing a pixel-level similarity constraint on the edge\nof the overlapping area, the seam distortions are suppressed\nsuccessfully. However, there are still artifacts (row 3, col 2 in\nFig. 17) in the stitched image.\n(4) Compared with v3, ours removes the artifacts (row 4,\ncol 2 in Fig. 17) using the proposed CS loss. The CS loss\nserves as an enhancer of the receptive ﬁeld, which promotes\nthe receptive ﬁeld of the HR branch from that of the LR\nbranch.\nFig. 18. A failure example. The red circle indicates the unsatisfying stitched\nareas.\nVI. LIMITATION AND FUTURE WORK\nThe proposed solution eliminates parallax artifacts through\nreconstructing the stitched images from feature to pixel. It is\nstill essentially a stitching method based on a single homog-\nraphy. As the parallax increases, the alignment performance\nof the ﬁrst stage will decrease, while the burden of the\nreconstruction network will also become heavier. When the\nparallax is too large, the reconstruction network may treat the\nmisalignments as new objects to reconstruct. An example is\nshown in Fig. 18. In the future, we hope to solve this problem\nin two directions: 1) Improve the alignment performance of the\nalignment network to decrease the burden of the reconstruction\nnetwork. 2) Increase the receptive ﬁeld of the reconstruction\nnetwork to deal with remained large misalignments.\nVII. CONCLUSION\nThis paper proposes an unsupervised deep image stitching\nframework, comprising unsupervised coarse image alignment\nand unsupervised image reconstruction. In the alignment stage,\nan ablation-based loss function is proposed to constrain the\nunsupervised deep homography estimation in large-baseline\nscenes, and a stitching-domain transformer layer is designed to\n13\nwarp the input images in the stitching-domain space. In the re-\nconstruction stage, an unsupervised deep image reconstruction\nnetwork is proposed to reconstruct the stitched images from\nfeature to pixel, eliminating the artifacts in an unsupervised\nreconstruction manner. Besides, a real dataset for unsupervised\ndeep image stitching is presented, which we hope can work as\na benchmark dataset for other methods. Experimental results\ndemonstrate the superiority of our method over other state-\nof-the-art solutions. Even if compared with the supervised\ndeep image stitching solutions, the results of our unsupervised\napproach are still preferred by users in terms of visual quality.\nHowever, the reconstruction ability is not unlimited, which\nindicates our solution may fail in the scenes with extremely\nlarge parallax. Considering our ﬁrst stage is essentially an\nalignment model based on a single homography, the ability to\nhandle large parallax can be improved by extending the linear\ndeep homography network to a non-linear homography model.\nMoreover, the reconstruction performance can be further in-\ncreased by increasing the receptive ﬁeld of the reconstruction\nnetwork, which is also an exploring direction of the future\nwork.\nREFERENCES\n[1] J. Chalfoun, M. Majurski, T. Blattner, K. Bhadriraju, W. Keyrouz,\nP. Bajcsy, and M. Brady, “Mist: accurate and scalable microscopy image\nstitching tool with stage modeling and error minimization,” Scientiﬁc\nreports, vol. 7, no. 1, pp. 1–10, 2017.\n[2] E. Semenishchev, V. Voronin, V. Marchuk, and I. Tolstova, “Method for\nstitching microbial images using a neural network,” in Mobile Multi-\nmedia/Image Processing, Security, and Applications 2017, vol. 10221,\np. 102210O, International Society for Optics and Photonics, 2017.\n[3] D. Li, Q. He, C. Liu, and H. Yu, “Medical image stitching using parallel\nsift detection and transformation ﬁtting by particle swarm optimization,”\nJournal of Medical Imaging and Health Informatics, vol. 7, no. 6,\npp. 1139–1148, 2017.\n[4] J. Li, Y. Zhao, W. Ye, K. Yu, and S. Ge, “Attentive deep stitching and\nquality assessment for 360 omnidirectional images,” IEEE Journal of\nSelected Topics in Signal Processing, vol. 14, no. 1, pp. 209–221, 2019.\n[5] V. R. Gaddam, M. Riegler, R. Eg, C. Griwodz, and P. Halvorsen,\n“Tiling in interactive panoramic video: Approaches and evaluation,”\nIEEE Transactions on Multimedia, vol. 18, no. 9, pp. 1819–1831, 2016.\n[6] L. Wang, W. Yu, and B. Li, “Multi-scenes image stitching based\non autonomous driving,” in 2020 IEEE 4th Information Technology,\nNetworking, Electronic and Automation Control Conference (ITNEC),\nvol. 1, pp. 694–698, IEEE, 2020.\n[7] W.-S. Lai, O. Gallo, J. Gu, D. Sun, M.-H. Yang, and J. Kautz, “Video\nstitching for linear camera arrays,” arXiv preprint arXiv:1907.13622,\n2019.\n[8] R. Anderson, D. Gallup, J. T. Barron, J. Kontkanen, N. Snavely,\nC. Hern´andez, S. Agarwal, and S. M. Seitz, “Jump: virtual reality video,”\nACM Transactions on Graphics (TOG), vol. 35, no. 6, pp. 1–13, 2016.\n[9] H. G. Kim, H.-T. Lim, and Y. M. Ro, “Deep virtual reality image quality\nassessment with human perception guider for omnidirectional image,”\nIEEE Transactions on Circuits and Systems for Video Technology,\nvol. 30, no. 4, pp. 917–928, 2019.\n[10] R. Hartley and A. Zisserman, Multiple view geometry in computer vision.\nCambridge university press, 2003.\n[11] J. Gao, S. J. Kim, and M. S. Brown, “Constructing image panoramas\nusing dual-homography warping,” in CVPR 2011, pp. 49–56, IEEE,\n2011.\n[12] W.-Y. Lin, S. Liu, Y. Matsushita, T.-T. Ng, and L.-F. Cheong, “Smoothly\nvarying afﬁne stitching,” in CVPR 2011, pp. 345–352, IEEE, 2011.\n[13] J. Zaragoza, T.-J. Chin, M. S. Brown, and D. Suter, “As-projective-as-\npossible image stitching with moving dlt,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 2339–2346,\n2013.\n[14] C.-H. Chang, Y. Sato, and Y.-Y. Chuang, “Shape-preserving half-\nprojective warps for image stitching,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 3254–\n3261, 2014.\n[15] C.-H. Chang and Y.-Y. Chuang, “A line-structure-preserving approach\nto image resizing,” in 2012 IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1075–1082, IEEE, 2012.\n[16] Y.-S. Chen and Y.-Y. Chuang, “Natural image stitching with the global\nsimilarity prior,” in European conference on computer vision, pp. 186–\n201, Springer, 2016.\n[17] C.-C. Lin, S. U. Pankanti, K. Natesan Ramamurthy, and A. Y. Aravkin,\n“Adaptive as-natural-as-possible image stitching,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\npp. 1155–1163, 2015.\n[18] J. Li, Z. Wang, S. Lai, Y. Zhai, and M. Zhang, “Parallax-tolerant\nimage stitching based on robust elastic warping,” IEEE Transactions\non Multimedia, vol. 20, no. 7, pp. 1672–1687, 2017.\n[19] K.-Y. Lee and J.-Y. Sim, “Warping residual based image stitching\nfor large parallax,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8198–8206, 2020.\n[20] J. Li, B. Deng, R. Tang, Z. Wang, and Y. Yan, “Local-adaptive image\nalignment based on triangular facet approximation,” IEEE Transactions\non Image Processing, vol. 29, pp. 2356–2369, 2019.\n[21] A. Eden, M. Uyttendaele, and R. Szeliski, “Seamless image stitching\nof scenes with large motions and exposure differences,” in 2006 IEEE\nComputer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR’06), vol. 2, pp. 2498–2505, IEEE, 2006.\n[22] F. Zhang and F. Liu, “Parallax-tolerant image stitching,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 3262–3269, 2014.\n[23] K. Lin, N. Jiang, L.-F. Cheong, M. Do, and J. Lu, “Seagull: Seam-guided\nlocal alignment for parallax-tolerant image stitching,” in European\nconference on computer vision, pp. 370–385, Springer, 2016.\n[24] J. Gao, Y. Li, T.-J. Chin, and M. S. Brown, “Seam-driven image\nstitching.,” in Eurographics (Short Papers), pp. 45–48, 2013.\n[25] H. Hejazifar and H. Khotanlou, “Fast and robust seam estimation to\nseamless image stitching,” Signal, Image and Video Processing, vol. 12,\nno. 5, pp. 885–893, 2018.\n[26] A. Zomet, A. Levin, S. Peleg, and Y. Weiss, “Seamless image stitching\nby minimizing false edges,” IEEE Transactions on Image Processing,\nvol. 15, no. 4, pp. 969–977, 2006.\n[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[28] Z. Zhang, C. Xu, J. Yang, J. Gao, and Z. Cui, “Progressive hard-mining\nnetwork for monocular depth estimation,” IEEE Transactions on Image\nProcessing, vol. 27, no. 8, pp. 3691–3702, 2018.\n[29] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, “Pwc-net: Cnns for optical\nﬂow using pyramid, warping, and cost volume,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\npp. 8934–8943, 2018.\n[30] L. Tian, Z. Tu, D. Zhang, J. Liu, B. Li, and J. Yuan, “Unsupervised\nlearning of optical ﬂow with cnn-based non-local ﬁltering,” IEEE\nTransactions on Image Processing, vol. 29, pp. 8429–8442, 2020.\n[31] K. Liao, C. Lin, Y. Zhao, and M. Xu, “Model-free distortion rectiﬁcation\nframework bridged by distortion distribution map,” IEEE Transactions\non Image Processing, vol. 29, pp. 3707–3718, 2020.\n[32] V.-D. Hoang, D.-P. Tran, N. G. Nhu, V.-H. Pham, et al., “Deep feature\nextraction for panoramic image stitching,” in Asian Conference on\nIntelligent Information and Database Systems, pp. 141–151, Springer,\n2020.\n[33] Z. Shi, H. Li, Q. Cao, H. Ren, and B. Fan, “An image mosaic method\nbased on convolutional neural network semantic features extraction,”\nJournal of Signal Processing Systems, vol. 92, no. 4, pp. 435–444, 2020.\n[34] C. Shen, X. Ji, and C. Miao, “Real-time image stitching with convo-\nlutional neural networks,” in 2019 IEEE International Conference on\nReal-time Computing and Robotics (RCAR), pp. 192–197, IEEE, 2019.\n[35] L. Nie, C. Lin, K. Liao, M. Liu, and Y. Zhao, “A view-free image\nstitching network based on global homography,” Journal of Visual\nCommunication and Image Representation, p. 102950, 2020.\n[36] L. Nie, C. Lin, K. Liao, and Y. Zhao, “Learning edge-preserved\nimage stitching from large-baseline deep homography,” arXiv preprint\narXiv:2012.06194, 2020.\n[37] T. Nguyen, S. W. Chen, S. S. Shivakumar, C. J. Taylor, and V. Kumar,\n“Unsupervised deep homography: A fast and robust homography esti-\nmation model,” IEEE Robotics and Automation Letters, vol. 3, no. 3,\npp. 2346–2353, 2018.\n14\n[38] J. Zhang, C. Wang, S. Liu, L. Jia, N. Ye, J. Wang, J. Zhou, and\nJ. Sun, “Content-aware unsupervised deep homography estimation,” in\nEuropean Conference on Computer Vision, pp. 653–669, Springer, 2020.\n[39] A. Agarwala, M. Dontcheva, M. Agrawala, S. Drucker, A. Colburn,\nB. Curless, D. Salesin, and M. Cohen, “Interactive digital photomon-\ntage,” in ACM SIGGRAPH 2004 Papers, pp. 294–302, 2004.\n[40] F. Liu, M. Gleicher, H. Jin, and A. Agarwala, “Content-preserving\nwarps for 3d video stabilization,” ACM Transactions on Graphics (TOG),\nvol. 28, no. 3, pp. 1–9, 2009.\n[41] F. Zhang and F. Liu, “Casual stereoscopic panorama stitching,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 2002–2010, 2015.\n[42] D. DeTone, T. Malisiewicz, and A. Rabinovich, “Deep image homogra-\nphy estimation,” arXiv preprint arXiv:1606.03798, 2016.\n[43] H. Le, F. Liu, S. Zhang, and A. Agarwala, “Deep homography estimation\nfor dynamic scenes,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 7652–7661, 2020.\n[44] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer\nnetworks,” in Advances in neural information processing systems,\npp. 2017–2025, 2015.\n[45] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on\nMedical image computing and computer-assisted intervention, pp. 234–\n241, Springer, 2015.\n[46] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time\nstyle transfer and super-resolution,” in European conference on computer\nvision, pp. 694–711, Springer, 2016.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 770–778, 2016.\n[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[49] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”\nInternational journal of computer vision, vol. 60, no. 2, pp. 91–110,\n2004.\n[50] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm\nfor model ﬁtting with applications to image analysis and automated\ncartography,” Communications of the ACM, vol. 24, no. 6, pp. 381–395,\n1981.\n[51] Y. Zhang, Y.-K. Lai, and F.-L. Zhang, “Content-preserving image stitch-\ning with piecewise rectangular boundary constraints,” IEEE Transactions\non Visualization and Computer Graphics, 2020.\n[52] B. He and S. Yu, “Parallax-robust surveillance video stitching,” Sensors,\nvol. 16, no. 1, p. 7, 2016.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-06-24",
  "updated": "2021-06-24"
}