{
  "id": "http://arxiv.org/abs/2106.12895v1",
  "title": "rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer",
  "authors": [
    "Felipe B. Martins",
    "Mateus G. Machado",
    "Hansenclever F. Bassani",
    "Pedro H. M. Braga",
    "Edna S. Barros"
  ],
  "abstract": "Reinforcement learning is an active research area with a vast number of\napplications in robotics, and the RoboCup competition is an interesting\nenvironment for studying and evaluating reinforcement learning methods. A known\ndifficulty in applying reinforcement learning to robotics is the high number of\nexperience samples required, being the use of simulated environments for\ntraining the agents followed by transfer learning to real-world (sim-to-real) a\nviable path. This article introduces an open-source simulator for the IEEE Very\nSmall Size Soccer and the Small Size League optimized for reinforcement\nlearning experiments. We also propose a framework for creating OpenAI Gym\nenvironments with a set of benchmarks tasks for evaluating single-agent and\nmulti-agent robot soccer skills. We then demonstrate the learning capabilities\nof two state-of-the-art reinforcement learning methods as well as their\nlimitations in certain scenarios introduced in this framework. We believe this\nwill make it easier for more teams to compete in these categories using\nend-to-end reinforcement learning approaches and further develop this research\narea.",
  "text": "rSoccer: A Framework for Studying\nReinforcement Learning in Small and Very\nSmall Size Robot Soccer\nFelipe B. Martins, Mateus G. Machado, Hansenclever F. Bassani, Pedro H. M.\nBraga, and Edna S. Barros\nCentro de Inform´atica - Universidade Federal de Pernambuco, Av. Jornalista Anibal\nFernandes, s/n - CDU 50.740-560, Recife, PE, Brazil.\n{fbm2, mgm4, hfb, phmb4, ensb}@cin.ufpe.br\nAbstract. Reinforcement learning is an active research area with a vast\nnumber of applications in robotics, and the RoboCup competition is\nan interesting environment for studying and evaluating reinforcement\nlearning methods. A known diﬃculty in applying reinforcement learning\nto robotics is the high number of experience samples required, being the\nuse of simulated environments for training the agents followed by transfer\nlearning to real-world (sim-to-real) a viable path. This article introduces\nan open-source simulator for the IEEE Very Small Size Soccer and the\nSmall Size League optimized for reinforcement learning experiments. We\nalso propose a framework for creating OpenAI Gym environments with a\nset of benchmarks tasks for evaluating single-agent and multi-agent robot\nsoccer skills. We then demonstrate the learning capabilities of two state-\nof-the-art reinforcement learning methods as well as their limitations in\ncertain scenarios introduced in this framework. We believe this will make\nit easier for more teams to compete in these categories using end-to-end\nreinforcement learning approaches and further develop this research area.\nKeywords: Reinforcement Learning · OpenAI Gym · Continuous Control ·\nRobot Soccer · Simulation\n1\nIntroduction\nReinforcement Learning (RL) [22], in conjunction with the machine learning\nﬁeld, has obtained interesting results in progressively more complex decision-\nmaking competitive scenarios, such as learning how to play Atari games [25],\nGo [21], and Starcraft 2 [24], achieving or even surpassing human-level perfor-\nmance. In robotics, RL showed promising results in simulated and real-world\nenvironments, including approaches for motion planning, optimization, grasp-\ning, manipulation, and control [1,5,11].\nRobot soccer competitions are an exciting ﬁeld for researching and validating\nRL usage, as it involves robotic systems capable of tackling challenging sequen-\ntial decision-making problems in a cooperative and competitive scenario [8].\narXiv:2106.12895v1  [cs.LG]  15 Jun 2021\n2\nF. Martins et al.\nDeveloping those systems can be very hard using traditional methods in which\nhard-coded behaviors need to foresee a multitude of possibilities in an unpre-\ndictable game such as soccer.\nIn the RoboCup Small Size League (SSL) competition, Fig. 1(a), teams with\nup to eleven omnidirectional mobile robots compete against each other to score\ngoals within a set of complex rules, such as limiting how far a robot can move\nwith the ball, therefore requiring explicit cooperation. Previous works in this\nsetting have successfully learned speciﬁc skills, such as moving to the ball, kick-\ning, and defending penalties by using RL control approaches [26–28]. However,\nthose works did not make the learning environment available, which can hinder\nreproducibility.\nOn the other hand, achieving an end-to-end control policy capable of coop-\nerating on a robot soccer match is still an open problem, requiring even further\nresearch and development as the league evolves. Similarly, training a single pol-\nicy capable of controlling a complete SSL team is also challenging, as the total\nnumber of control actions increases with the number of robots.\nThe IEEE Very Small Size Soccer (VSSS) competition, Fig. 1(b), compared\nto the SSL, establishes teams with three robots each, with a smaller ﬁeld and\nrobot sizes. The robot hardware does not have ball dribbling and kicking ca-\npabilities, and a match does not require explicit cooperation, by the rules. Al-\nthough the diﬀerential drive robots used pose a more challenging path planning,\nthe league can be seen as a simpliﬁed version of the SSL. In this domain, ear-\nlier work applied RL for learning speciﬁc skills [6]. And more recently, Bassani\net al. [3] achieved 4th place in an international VSSS competition, using an\nend-to-end learned control without explicit cooperation and made the learning\nenvironments publicly available. Still, they do not support the SSL setting and\nare not easily adaptable for diﬀerent scenarios.\n(a) SSL\n(b) IEEE VSSS\nFig. 1. Competition environments of SSL and IEEE VSS.\nWe consider that by supplying a simpler path towards creating and using RL\nSSL environments, the results achieved by Bassani et al. [3] can be replicated on\nthe SSL competition and be further used to encourage RL approaches in the SSL\ncontext. In summary, the contributions of the present work are the following:\nrSoccer Framework\n3\n1. An open-source framework following the OpenAI Gym [4] standards for de-\nveloping robot soccer RL environments, modeling multi-agent tasks in com-\npetitive and cooperative scenarios;\n2. An open-source SSL and VSSS robot soccer simulator, adapted from the\ngrSim Simulator [16], focused on RL use;\n3. A set of eight benchmark learning environments with a focus on reproducibil-\nity, for evaluating RL algorithms in robot soccer tasks, including four tasks\nbased on the RoboCup SSL 2021 hardware challenges.\nThe rest of this article is organized as follows: Section 2 presents related work\non robot soccer simulators and environments. Section 3 describes the proposed\nframework. Section 4 introduces a set of benchmark environments created using\nthe proposed framework. Section 5 presents the results, and ﬁnally, Section 6\ndraws the conclusions and suggests future work.\n2\nRelated Works\nThere is a large variety of RL environments and frameworks on the literature\nwhich aim at allowing the easy reproduction of state-of-the-art RL algorithms\nresults, such as the OpenAI Gym [4]. However, the existing robot soccer environ-\nments lack the needed characteristics, such as extensibility to diﬀerent scenarios,\nproper real-world robot simulation, and hard to reproduce results. Therefore,\nthey do not apply to the RoboCup categories. These issues are discussed as\nfollows.\nSuitable frameworks. There are frameworks for simulating soccer matches\nsuch as the RoboCup’s Soccer 2D [9] and The Google Research Football Envi-\nronment [10]. However, the actions deﬁned are too high level. The DeepMind\nMuJoCo Multi-Agent Soccer Environment [13] deﬁne low-level actions such as\naccelerating and rotating the body, but it is not related to a real robot soccer\nleague. Bassani, et. al [3] proposed an framework for the VSSS setting, but it\ndoes not enable the creation of new scenarios. Although Robocup’s Soccer 3D\nprovides a low-level action and believable environment, there is no framework\nthat enables the creation of scenarios.\nSimulator’s purpose. There are well known simulators for robot soccer\ncompetitions such as SSL [16] and VSSS [15,19]. They provide a real-time sim-\nulated environment with a rich graphical interface for developing robot soccer al-\ngorithms. However, the preferences for RL are simulation speed and synchronous\ncommunication.\nReproducibility issues. Previous work achieved interesting results using\nRL in robot soccer competition settings [6,20,26]. But they do not describe the\nenvironments and simulators used nor made them openly available, coupled with\na lack of clearly deﬁned tasks and availability of stable baseline implementations\nof robot soccer agents, poses several issues to the advancement of research in\nthis ﬁeld.\n4\nF. Martins et al.\n3\nrSoccer Gym Framework\nThe proposed framework1 is a tool for creating robot soccer environments rang-\ning from simple single-agent tasks to complex multi-robot competitive coopera-\ntive scenarios.\nIt is deﬁned by three modules: simulator, environment, and render. The\nsimulator module describes the physics simulation. The environment module is\ndesigned to receive the agent action, communicate with the other modules, and\nreturn the new observations and rewards. The render module does the environ-\nment visualization. Fig. 2 illustrates the modules architecture. A set of data\nstructures labeled entities are deﬁned to enable a common communication be-\ntween modules for every environment. The following subsections describe these\nmodules and entities.\nFig. 2. Framework modules architecture\n3.1\nEntities\nThe entities structures are standardized for consistency by deﬁning positional\nvalues using the ﬁeld center as a reference point. The units conform to the\nInternational System of Units (SI), except for robot angular position and speed\nvalues which are in degrees. The following entities are deﬁned:\n– Ball: Contains the ball position and velocity values and is used both to read\nthe current state or to set the initial position;\n– Robot: Contains a robot identiﬁcation, ﬂags, position, velocity, and wheel\ndesired speed values. Used to read the current state, to set the initial position,\nor to send control commands;\n– Frame: Contains a Ball entity and Robot entities for each robot in the en-\nvironment, structured in a way that each robot is easily indexable by team\ncolor and id. Used to store the complete state of the simulation;\n– Field: Contains speciﬁcations of the simulation values, such as the ﬁeld and\nrobot geometry and parameters.\n1 Code available at https://github.com/robocin/rSoccer\nrSoccer Framework\n5\n3.2\nSimulator Module\nThe simulator module carries out the environment physics calculations. It com-\nmunicates directly with the environment module, receiving actions and returning\nthe simulation state.\nFor physics calculations, we developed the rSim2 simulator specially for RL.\nIt was based on the grSim simulator [16], due to its reliable physical simulation,\nwith the following modiﬁcations:\n– Removal of graphical interfaces to increase performance, reduce memory\nusage, and ease server deployment on headless servers;\n– Synchronous operation for more consistent training results as in an asyn-\nchronous setting the synchronization between agents and simulator may de-\npend on hardware performance;\n– Support for a diﬀerent number of robots in each team, to enable more envi-\nronment possibilities;\n– Split simulated objects collision spaces to create separate collision groups;\n– Added motor speed constraints matching real-world observations;\n– Enable cylinder collision, removing the dummy collision object to reduce the\ntotal number of simulated bodies;\n– Deﬁned direct simulator calls in Python for fast communication. Enabling\nthe instantiation of multiple simulators without the need to manage network\ncommunication ports.\nAlthough the simulator is external to the framework, the simulator mod-\nule abstracts its interface. Table 1 presents the rSim simulator performance in\ncomparison with the grSim simulator in headless mode for a diﬀerent number\nof robots on ﬁeld. The grSim used in the comparison had slight modiﬁcations\nfor removing frequency limits, and it also includes a modiﬁed version with syn-\nchronous operations for comparison.\nTable 1. Simulation performance in average and standart deviation of steps per second,\nfor 1, 6 and 11 SSL robots in each team.\nSimulator\n1 vs 1\n6 vs 6\n11 vs 11\ngrSim (asynchronous)\n2167.9 (8.4)\n408.7 (0.3)\n228.3 (0.1)\ngrSim (synchronous)\n1894.0 (8.4)\n390.0 (0.5)\n219.0 (0.7)\nrSim (proposed, synchronous)\n2408.8 (9.3)\n510.8 (1.8)\n288.0 (0.4)\n3.3\nEnvironment Module\nThe environment module is where the environment task itself is deﬁned. It imple-\nments the interface with the agent and communication with the other framework\n2 Code available at https://github.com/robocin/rSim\n6\nF. Martins et al.\nmodules. The interface with the agent complies with the OpenAI Gym [4] frame-\nwork, and it communicates with the other modules using the entities structures.\nThe use of common interfaces enables the deﬁnition of base environments,\nwhich handle the communications with the other modules and the compliance\nwith Gym. The framework provides benchmark environments of important tasks\nrelated to the RoboCup challenges [9], serving as examples and making it easier\nfor other researchers to develop and evaluate new RL methods in these bench-\nmark scenarios. The work needed for deﬁning a new environment consists of the\nimplementation of only four methods:\n– get commands: Returns a list of Robot entities containing the commands\nwhich are sent to the simulator;\n– frame to observations: Returns an observation array which will be for-\nwarded to the agent as deﬁned by the environment;\n– calculate reward and done: Returns both the calculated step reward and\na boolean value indicating if the current state is terminal;\n– get initial positions frame: Returns a Frame entity used to deﬁne the\ninitial positions of the ball and robots.\n3.4\nRender Module\nAlthough we explicitly removed the graphical interface from the simulator for\nperformance, the render module enables visualization without previous draw-\nbacks. It renders on-demand a 2D image of the ﬁeld and has no performance\nreduction when not in use. Its implementation is independent of the simulator\nand enables it to be used at training time for monitoring purposes since it is\nbased on the Gym base environment solution.\n4\nProposed Robot Soccer Environments\nDue to the diﬀerences of the leagues mentioned in Section 1, we propose a com-\nplete soccer game environment based on Latin American Robotics Competition\ncompetition for the VSSS and simple skills learning environments for the SSL.\nA state is deﬁned as the complete set of data returned by the simulator after a\nperformed action and an observation as a subset or transformation of this state.\nOn the following proposed environments, we described the state by positions\n(x, y), angles (θ), and velocity (vx, vy, vθ) of each object (ball, teammate, and\nopponent) in reference to the ﬁeld center. On the SSL environments there is an\nadditional Infrared sensor (IR) signal of each robot, indicating if the ball is in\ncontact with the kicking device.\n4.1\nIEEE Very Small Size League Environments\nBased on Bassani et al. [3], we developed a single and a multi-agent benchmark\nfor the VSSS league. The observation is the complete state deﬁned above. We\nrSoccer Framework\n7\ndescribe the actions of each robot as the power percentage for each wheel that\nthe robot will apply in the next step. For the non-controlled agents, we use a\nrandom policy based on Ornstein-Uhlenbeck process (OU) [2]. The OU process\ncreates a more continuous motion trend for a few steps, which allows the agents\nto follow a more structured random trajectory instead of just oscillating around\nthe initial point. An episode ﬁnishes if the agent received/scored a goal or if\nthe timer reaches 30 seconds of simulation. In the IEEE VSSS Single-Agent\nenvironment, only one robot learns a policy, and the other ﬁve (two teammates\nand three opponents) follow a random policy that consists of executing actions\nsampled according to the OU process. In the IEEE VSSS Multi-Agent en-\nvironment, the controlled robots share the learning policy. See on Fig. 3(a) the\nrendered Frame entity of the IEEE VSSS environments.\n4.2\nSmall Size League Environments\nThe ﬁrst environment developed is the basic GoToBall. The other environments\nwere based on RoboCup’s 2021 hardware challenge [17].\nThe actions of the SSL environments are the global frame velocities on each\naxis, kick power, and dribbler state (on/oﬀ). For all environments, we deﬁned\nrewards based on energy spent by the robot, its distance to the ball, and for\nreaching the objective.\nThe GoToBall environment is the most straightforward skill to be learned.\nIn this environment, the controlled agent must reach the ball and position its IR\nsensor on it, i.e., arriving at the ball at a certain angle. The episode ends when\nthe robot completes the objective, if the agent exits the ﬁeld limits, or if the\nsimulation timer reaches 30 seconds. See on Fig. 3(b) an example of rendered\nFrame of the environment.\nThe Hardware Challenges environments consist of four environments based\non RoboCup’s 2021 hardware challenges. We made certain simpliﬁcations to the\noriginal environments to make them learnable by the currently available methods\nin a reasonable amount of time [17]. They are:\n1. Static Defenders: the episode begins with the controlled agent in the ﬁeld\ncenter and 6 opponents and the ball randomly positioned in opponent’s ﬁeld.\nThe episode ends if the agent scores a goal, the ball or the agent exits the\nopponent’s ﬁeld, the agent collides with an opponent, or the timer reaches\n30 simulated seconds. See on Fig. 3(c) an example of initial Frame.\n2. Contested Possession: the episode begins with the controlled agent in\nthe ﬁeld center and an opponent is randomly positioned in the opponent’s\nﬁeld, with the ball on its dribbler. The objective of this challenge is to sneak\nthe ball from the opponent and score a goal. The episode ends with the\nsame conditions of the Static Denfenders environment. See on Fig. 3(d) an\nexample of initial Frame.\n3. Dribbling: the episode begins with the controlled agent in the ﬁeld center\nwith the ball on its dribbler and four opponent robots positioned in a sparse\nrow, leaving ”gates” between each of them. The objective of this challenge is\n8\nF. Martins et al.\nto dribble the ball while the agent moves through these gates. The episode\nends if the agent collides with any robot or exits the ﬁeld. See on Fig. 3(e)\nan example of the initial Frame.\n4. Pass Endurance (single and multi-agent): the episode begins with the\ntwo robots at random positions, with the ball on the dribbler of one of them.\nThere are no opponents in this environment. In the single-agent environment,\nthe objective is to perform a pass in three seconds. For the multi-agent, they\nhave to perform as many passes as possible in 30 seconds. The episode ends\nif a pass does not reach the teammate or if the time is out. See on Fig. 3(f)\nan example of initial Frame.\n(a) IEEE VSSS\n(b) GoToBall\n(c) Static Defenders\n(d) Contested Possession\n(e) Dribbling\n(f) Pass Endurance\nFig. 3. Initial states of the proposed benchmark environments.\n5\nExperimental Results\nThis section presents and discusses the results obtained on our framework with\ntwo state-of-the-art deep reinforcement learning methods for continuous control.\nWe chose Deep Deterministic Policy Gradient (DDPG) [12] and Soft Ac-\ntor Critic (SAC) [7] because both are known for presenting great performance\nin robot control environments such as Deepmind Control Suite [23]. We have\nalso tested Proximal Policy Approximation (PPO) [18], however, despite all our\nrSoccer Framework\n9\n(a) IEEE VSSS Single-Agent\n(b) IEEE VSSS Multi-Agent\n(c) GoToBall\n(d) Dribbling\n(e) Contested Possession\n(f) Static Defenders\n(g) Pass Endurance\n(h) Pass Endurance MA\nFig. 4. Mean (lines) and standard deviation (shades) of the results obtained for each\nenvironment (DDPG in blue and SAC in orange). The Y axis represents: Goal Score for\na, b, e, and f; Ball Reached for c; Number of gates transversed for d; Inverse Distance\nto Receiver for g; and Pass Score for h.\neﬀorts in parameter tuning, it was not able to learn even in the easiest envi-\nronments. Therefore, we concentrated our eﬀorts on DDPG and SAC. On the\nmulti-agent environments, we used a shared policy to control all agents. For each\nenvironment, we executed ﬁve runs of each method. We ran 10 million steps for\n10\nF. Martins et al.\neach experiment, except for the Dribbling and Static Defenders environments,\nin which we ran 20 million steps. For the IEEE VSSS environments, Contested\nPossession and Static Defenders we use the goal score to evaluate the agents.\nIn the GoToBall, and Dribbling we evaluate if the agents complete or not the\nrespective objective. In the Pass Endurance Single-Agent we used 1/d to eval-\nuate the agent, where d is the distance of the ball to the receiver. In the Pass\nEndurance Multi-Agent we used the pass score to evaluate the agent. In Fig. 4\nwe present the average and standard deviation of the learning curves obtained\nwith each method in each environment.\nWe note that both algorithms presented a high standard deviation in all en-\nvironments, except Pass Endurance. We also point out that DDPG was more\nsample eﬃcient in most tasks (Figures 4(a) to 4(e)), an interesting result consid-\nering SAC usually performs better than DDPG in continuous control environ-\nments [7]. This performance may be explained by the fact that DDPG employs\nthe OU process for exploration, which seems to suite better for the environments\nconsidered here. As SAC uses an entropy-based exploration, it takes more sam-\nples for it to reach the performance of DDPG, although it surpassed DDPG by\na small margin at the end, in certain environments (Figures 4(c) and 4(f)).\nIn the multi-agent environments (IEEE VSSS and Pass Endurance), we high-\nlight that the results were worse than the respective single-agent ones. This\nindicates that the agents did not learn to collaborate, since more agents were\nexpected to perform better than a single one. For instance, in the VSSS, a visual\ninspection revels that, instead of collaborating, the agents block each other, as\ncan be observed in the frame sequences available in our repository3.\n6\nConclusions and Future Work\nThis article presented an open-source framework for developing robot soccer RL\nenvironments for the VSSS and SSL competitions. The framework includes a\nsimulator optimized for RL experiments and an API for deﬁning new environ-\nments compatible with the OpenAI Gym standards. It also provides eight bench-\nmark environments that can evaluate RL methods regarding diﬀerent types of\nrobot soccer challenges. The API is easily extensible for other types of environ-\nments and tasks. The simulator can be replaced by an interface with real robots\nfor evaluating Sim-to-Real as in [3].\nWith this, we aim to put forward research and application RL methods for\nrobot soccer by making it easier for other researchers to evaluate their strategies\nand compare the results in standardized scenarios, therefore improving repro-\nducibility.\nAlthough our results are promising in certain tasks, achieving better results\nthan we would be able to achieve with traditional handcrafted methods, it also\nmakes it clear that much research is needed to achieve an eﬀective robot soccer\nteam trained end-to-end by reinforcement learning. Studying why PPO per-\nformed so poorly is essential for our future works, once it achieved interesting\n3 https://github.com/robocin/rSoccer\nrSoccer Framework\n11\nresults on other studies. The multi-agent (Figures 4(b), 4(f)) and the Static De-\nfenders Fig. 4(h) environments show that certain benchmarks are too diﬃcult\nfor the currently available methods, indicating an open area of research. In the\nStatic Defenders environment, the best reward function we developed seems in-\nadequate when the dimensionality of the observations increases, hence the poor\nresults. In multi-agent environments, the methods could not learn to cooperate\nusing a shared policy. However, we believe that multi-agent speciﬁc algorithms\nfocusing on collaboration such as MADDPG [14] would improve the results.\nACKNOWLEDGMENTS\nThe authors would like to thank RoboCIn - UFPE Team and Mila - Quebec Arti-\nﬁcial Intelligence Institute for the collaboration and resources provided; Conselho\nNacional de Desenvolvimento Cientiﬁco e Tecnol´ogico (CNPq), and Coordena¸c˜ao\nde Aperfei¸coamento de Pessoal de N´ıvel Superior (CAPES) for ﬁnancial support.\nMoreover, the authors also gratefully acknowledge the support of NVIDIA Cor-\nporation with the donation of the RTX 2080 Ti GPU used for this research.\nReferences\n1. Andrychowicz, M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B., Pachocki,\nJ., Petron, A., Plappert, M., Powell, G., Ray, A., et al.: Learning dexterous in-hand\nmanipulation. arXiv preprint arXiv:1808.00177 (2018)\n2. Arnold, L.: Stochastic diﬀerential equations. New York (1974)\n3. Bassani, H.F., Delgado, R.A., de O. Lima Junior, J.N., Medeiros, H.R., Braga,\nP.H.M., Machado, M.G., Santos, L.H.C., Tapp, A.: A framework for studying re-\ninforcement learning and sim-to-real in robot soccer (2020)\n4. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,\nZaremba, W.: Openai gym (2016)\n5. Christiano, P., Shah, Z., Mordatch, I., Schneider, J., Blackwell, T., Tobin, J.,\nAbbeel, P., Zaremba, W.: Transfer from simulation to real world through learning\ndeep inverse dynamics model. arXiv preprint arXiv:1610.03518 (2016)\n6. Duan, Y., Liu, Q., Xu, X.: Application of reinforcement learning in robot soccer.\nEngineering Applications of Artiﬁcial Intelligence 20(7), 936–950 (2007)\n7. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V.,\nZhu, H., Gupta, A., Abbeel, P.: Soft actor-critic algorithms and applications. arXiv\npreprint:1812.05905 (2018)\n8. Kim, J.H., Kim, D.H., Kim, Y.J., Seow, K.T.: Soccer robotics, vol. 11. Springer\nScience & Business Media (2004)\n9. Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., Osawa, E.: Robocup: The robot\nworld cup initiative. In: Proceedings of the First International Conference on Au-\ntonomous Agents. p. 340–347. AGENTS ’97, Association for Computing Machin-\nery, New York, NY, USA (1997). https://doi.org/10.1145/267658.267738\n10. Kurach, K., Raichuk, A., Sta´nczyk, P., Zajac, M., Bachem, O., Espeholt, L.,\nRiquelme, C., Vincent, D., Michalski, M., Bousquet, O., et al.: Google research foot-\nball: A novel reinforcement learning environment. arXiv preprint arXiv:1907.11180\n(2019)\n12\nF. Martins et al.\n11. Levine, S., Finn, C., Darrell, T., Abbeel, P.: End-to-end training of deep visuomotor\npolicies. The Journal of Machine Learning Research 17(1), 1334–1373 (2016)\n12. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D.,\nWierstra, D.: Continuous control with deep reinforcement learning. arXiv preprint\narXiv:1509.02971 (2015)\n13. Liu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., Graepel, T.: Emergent\ncoordination through competition (2019)\n14. Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O.P., Mordatch, I.: Multi-agent\nactor-critic for mixed cooperative-competitive environments. In: Advances in Neu-\nral Information Processing Systems. pp. 6379–6390 (2017)\n15. Monajjemi,\nV.,\nKoochakzadeh,\nA.:\nFIRASim.\nhttps://github.com/\nfira-simurosot/FIRASim (2020), [Online; accessed 28-April-2021]\n16. Monajjemi, V., Koochakzadeh, A., Ghidary, S.S.: grsim – robocup small size robot\nsoccer simulator. In: R¨ofer, T., Mayer, N.M., Savage, J., Saranlı, U. (eds.) RoboCup\n2011: Robot Soccer World Cup XV. pp. 450–460. Springer Berlin Heidelberg,\nBerlin, Heidelberg (2012)\n17. RoboCup: Robocup small size league (ssl) hardware challenges 2021. https:\n//robocup-ssl.github.io/ssl-hardware-challenge-rules/rules.html,\nac-\ncessed: 2021-04-08\n18. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 (2017)\n19. SDK, V.: VSS SDK. https://vss-sdk.github.io/book/general.html (2019),\n[Online; accessed 5-April-2021]\n20. Shi, H., Lin, Z., Hwang, K.S., Yang, S., Chen, J.: An adaptive strategy selection\nmethod with reinforcement learning for robotic soccer games. IEEE Access 6,\n8376–8386 (2018)\n21. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,\nHubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of go without\nhuman knowledge. nature 550(7676), 354–359 (2017)\n22. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press\n(2018)\n23. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D.,\nAbdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., Riedmiller, M.: Deepmind\ncontrol suite (2018)\n24. Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czarnecki,\nW.M., Dudzik, A., Huang, A., Georgiev, P., Powell, R., et al.: Alphastar: Mastering\nthe real-time strategy game starcraft ii. DeepMind Blog (2019)\n25. Volodymyr, M., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.: Playing\natari with deep reinforcement learning. In: NIPS Deep Learning Workshop (2013)\n26. Yoon, M.: Developing basic soccer skills using reinforcement learning for the\nRoboCup Small Size League. Ph.D. thesis, Stellenbosch: Stellenbosch University\n(2015)\n27. Zhu, Y., Schwab, D., Veloso, M.: Learning primitive skills for mobile robots. In:\n2019 International Conference on Robotics and Automation (ICRA). pp. 7597–\n7603 (2019)\n28. Zolanvari, A., Shirazi, M., Menhaj, M.: A q-learning approach for controlling a\nrobotic goalkeeper during penalty procedure. In: II International Congress on Sci-\nence and Engineering. 2019. Hamburg-Germany. pp. 1–12 (2019)\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2021-06-15",
  "updated": "2021-06-15"
}