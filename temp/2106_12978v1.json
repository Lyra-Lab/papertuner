{
  "id": "http://arxiv.org/abs/2106.12978v1",
  "title": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings",
  "authors": [
    "Alessandro Solbiati",
    "Kevin Heffernan",
    "Georgios Damaskinos",
    "Shivani Poddar",
    "Shubham Modi",
    "Jacques Cali"
  ],
  "abstract": "Topic segmentation of meetings is the task of dividing multi-person meeting\ntranscripts into topic blocks. Supervised approaches to the problem have proven\nintractable due to the difficulties in collecting and accurately annotating\nlarge datasets. In this paper we show how previous unsupervised topic\nsegmentation methods can be improved using pre-trained neural architectures. We\nintroduce an unsupervised approach based on BERT embeddings that achieves a\n15.5% reduction in error rate over existing unsupervised approaches applied to\ntwo popular datasets for meeting transcripts.",
  "text": "Unsupervised Topic Segmentation of Meetings with BERT Embeddings\nAlessandro Solbiati\nFacebook, Inc.\nlessandro@fb.com\nShivani Poddar\nFacebook, Inc.\nshivanipi@fb.com\nKevin Heffernan\nFacebook, Inc.\nkheffernan@fb.com\nShubham Modi\nFacebook, Inc.\nshubhammodi@fb.com\nGeorgios Damaskinos\nFacebook, Inc.\ndamaskinos@fb.com\nJacques Cali\nFacebook, Inc.\njcali@fb.com\nAbstract\nTopic segmentation of meetings is the task of\ndividing multi-person meeting transcripts into\ntopic blocks.\nSupervised approaches to the\nproblem have proven intractable due to the\ndifﬁculties in collecting and accurately anno-\ntating large datasets. In this paper we show\nhow previous unsupervised topic segmentation\nmethods can be improved using pre-trained\nneural architectures. We introduce an unsuper-\nvised approach based on BERT embeddings\nthat achieves a 15.5% reduction in error rate\nover existing unsupervised approaches applied\nto two popular datasets for meeting transcripts.\n1\nIntroduction\nWith remote work being the norm, the average em-\nployee attends 62 company meetings every month\n(Atlassian, 2021). The majority of existing video\ncall tools used for professional meetings, enable\na recording functionality that is either rarely used\nor its output is stored and never used again after\nthe meeting. Nevertheless, these meeting record-\nings create a wonderful opportunity for increased\nproductivity and transparency.\nTopic segmentation is the task of dividing text\ninto a linear sequence of topically-coherent seg-\nments. In the context of meeting recordings and\ntheir transcripts, topic segmentation can quickly\nprovide users with a valuable high level under-\nstanding of past meetings. For example, upper\nmanagement can quickly locate critical decisions\ntaken during a product meeting among engineers.\nTopic segmentation can also signiﬁcantly boost\nsearch indexing and downstream retrieval (Hearst\nand Plaunt, 2002), where keyword search is not\neffective given the usually high transcription er-\nror rate of Automatic Speech Recognition (ASR)\nsystems. Finally topic segmentation can be use-\nful for other text analysis tasks such as passage\nretrieval (Salton et al., 1996), document summa-\nrization and discourse analysis (Galley et al., 2003).\nHowever, topic segmentation of meetings is a\nvery challenging task due to (a) the noisy nature of\nmeeting transcripts and (b) the lack of ground truth\ndata. First, these meetings involve multiple partici-\npants with each having their own personalised use\nof the language thus inevitably leading to transcript\nerrors. Second, topic segmentation can be a hard\ntask even for human annotators (Gruenstein et al.,\n2008a), and hence collecting labeled data for seg-\nmented meetings becomes complex and expensive.\nIn addition, organisations express strong sensitivity\ntowards their private meeting data, making the task\nof collecting large training datasets even harder.\nIn this paper we focus on unsupervised topic seg-\nmentation of meetings. The lack of ground truth\ndata impedes any beneﬁts from the latest advances\nin neural networks (Barrow et al., 2020), as op-\nposed to segmentation in other domains such as\nwritten text where a large amount of labeled data\nrecently brought signiﬁcant advancements through\nsupervised neural models (Badjatiya et al., 2018).\nWe remove this impediment by proposing a\nmechanism based on pre-trained transformer mod-\nels (Vaswani et al., 2017) applied to the task of\ntopic segmentation of meetings. Our method is\ncompletely unsupervised, i.e., does not require any\ntraining data. Our model utilizes a new similarity\nscore based on BERT embeddings (Devlin et al.,\n2018), that enables a 15.5% reduction in error rate\ncompared to existing unsupervised methods that\nuse similarity score heuristics not based on neu-\nral models. We also show a 26.6% reduction in\nerror rate compared to the current state of the art\nsupervised topic segmentation models (Badjatiya\net al., 2018) trained on text datasets. These models\nperform poorly due to the distinctive differences\nbetween written text datasets such as Wikipedia,\nand the standard meeting transcripts datasets ICSI\narXiv:2106.12978v1  [cs.LG]  24 Jun 2021\nMeeting Corpus (Janin et al., 2003) and AMI Meet-\ning Corpus (Kraaij et al., 2005).\n2\nRelated Work\nWritten text.\nThere are many recent advance-\nments on topic segmentation of written text, with\nmost of them being based on bidirectional-LSTM\nembeddings.\n(Li et al., 2018) combine a BiL-\nSTM with a pointer network, (Badjatiya et al.,\n2018) propose stacked BiLSTMs with attention\nfor topic segmentation of an 85 ﬁction books\ndataset (Kazantseva and Szpakowicz, 2011), and\n(Barrow et al., 2020) propose a custom LSTM ar-\nchitecture for topic segmentation on the WikiSec-\ntion dataset (Arnold et al., 2019) that consists of\n242k labeled segments from Wikipedia articles.\nMonologue transcripts.\nTopic segmentation of\nspoken language is signiﬁcantly more challenging\nthan written text due to the added complexity that\nthe underlying ASR system introduces. This task\nfocuses either on monologue data or on dialogue\ndata (Purver, 2011). Monologue data also wit-\nnessed recent advancements with neural-based ar-\nchitectures such as TCNs (Zhang and Zhou, 2019)\nand Bi-LSTMs (Sehikh et al., 2017). Monologue\ndatasets also feature an abundance of labeled train-\ning data, mainly comprised of large broadcast news\ntranscripts such as the Euronews Dataset with 24k\nlabeled segments (Sheikh et al., 2016).\nMulti-party dialogue transcripts.\nMulti-party\ndialogue speech data, mainly comprised of meeting\ntranscripts, has not yet beneﬁted from the advance-\nments that other sub-domains have seen with neural\nnetworks. Most of the existing methods in this do-\nmain are based on measuring similarity/coherence\nbetween sentences to detect topic changes. One of\nthe ﬁrst successful approaches of this sort has been\nTextTiling (Hearst, 1997), that uses word frequency\nvectors as a similarity metric. Despite TextTiling\nbeing originally designed for text documents, it has\nbeen successfully applied to meeting transcripts\nsegmentation (Georgescul et al., 2006).\nSentence embeddings.\nThese have been used\nto extract semantic similarity, a task formalised\nin (Cer et al., 2017) as the degree to which two\nsentences are semantically equivalent. BERT (De-\nvlin et al., 2018) is a pre-trained transformer net-\nwork (Vaswani et al., 2017) which reaches state-\nof-the-art-results for many NLP tasks. No inde-\npendent sentence embeddings are directly com-\nputed in the original model, hence a common\npractice is to derive a ﬁxed vector by either av-\neraging the outputs or by using the outputs spe-\ncial CLS toke (Zhang et al., 2019).\nSentence-\nBERT (Reimers and Gurevych, 2019) is a mod-\niﬁcation of BERT designed to derive semanti-\ncally meaningful sentence embeddings. Sentence-\nBERT employs siamese and triplet network struc-\ntures (Schroff et al., 2015) to derive embeddings\nthat can be compared with cosine similarity.\n3\nMethod\nIn this section we provide a formal presentation\nof the topic segmentation task, and a detailed\noverview of our model.\nInput: a meeting transcript produced by an\nASR system consists of a list of M utterances\nS = {S1, · · · , SM} with an underlying topic struc-\nture represented by a reference topic segmentation\nT = {T1, · · · , TN}, with each topic having a start\nand an end utterance Ti ∈[Sj, Sk].\nOutput: a label sequence Y = {y1, · · · , yM}\nwhere yi is a binary value that indicates whether\nthe utterance Si is the start of a new topic segment.\nOur topic segmentation model consists of (i) a\nsentence representation model to extract semantic\nsimilarity between sentences and (ii) a segmenta-\ntion scheme that employs semantic similarity vari-\nations over time to detect topic changes.\n3.1\nSentence Representation\nTo extract semantic similarity we experiment with\ntwo different sentence representation approaches.\nBERT.\nThe ﬁrst pre-trained model we use is\nRoBERTa (Liu et al., 2019), a pre-training con-\nﬁguration of BERT trained with the Masked Lan-\nguage Modelling objective on a ﬁve large English-\nlanguage corpora totaling over 160GB of uncom-\npressed text (Zhu et al., 2015). It is possible to\nextract ﬁxed features from the pre-trained model\nwithout additional ﬁne-tuning (Devlin et al., 2018):\nwe extract a ﬁxed sized vector via max pooling of\nthe second to last layer. RoBERTa is trained with\nhyperparameters L = 12 and H = 768, where L\nis the number of layers and H is the size of the\nhidden layer. A sentence of N words will hence\nresult in an N ∗H embedding vector. The closer\nto the last layer, the more the semantic information\ncarried by the weights (Zeiler et al., 2011); hence\nour choice of the second to last layer.\nTopic Label\nTopic Change\nCaption\nSpeaker\nWhat to do for next meeting\n0\nYeah, since they’re not at the meeting I think it’s in\n[disﬂuency] out of courtesy we should ﬁrst ask them.\nC\n.\n0\nYes.\nD\n.\n0\nAnd I’ll try to [disﬂuency]\nA\n.\n0\nYes.\nD\nCoffee Availability\n1\nBu I ju just before ﬁnishing uh, I mean, we have a cafe-\nteria or we don’t eat at all?\nB\n.\n0\nFine.\nA\n.\n0\nWe don’t have a cafeteria.\nD\n.\n0\nWhat do you mean by cafeteria?\nA\nTable 1: Example of meeting transcript from AMI Meeting Corpus (ID IB4003) as a sequence labelling problem.\nSentence-BERT.\nThe second pre-trained model\nwe experiment with, is the current state-of-the-\nart in sentence representation, namely Sentence-\nBERT (Reimers and Gurevych, 2019), pre-trained\non the SNLI dataset (Bowman et al., 2015). We ex-\ntract ﬁxed size sentence embeddings using a mean\nover all the output vectors, similar to the method\nwe used for BERT.\nMax pooling.\nOur extraction architecture is par-\nticularly robust to noisy speech data (Shriberg,\n2005), including ASR miss-transcriptions, disﬂu-\nencies of speakers or turn-taking. A sample of this\nnoisy characteristics can be seen in Table 1 where\nwe report the transcript of 8 example utterances.\nTo ﬁlter out words that hold limited semantic value\n(e.g., “uh, I mean.”), we apply repeatedly a max\npooling operation to extract words with high se-\nmantic value from a given utterance.\n3.2\nSegmentation Scheme\nGiven a valid sentence embedding, a common prac-\ntice is to train a supervised classiﬁer to perform se-\nquence labelling, for example using TCNs (Zhang\nand Zhou, 2019). On the contrary, we choose a\ncompletely unsupervised approach that does not\nrequire any labeled training data.\nOur approach is a modiﬁed version of the origi-\nnal TextTiling (Hearst, 1997) “gold-standard unsu-\npervised method for topic segmentation” (Purver,\n2011). TextTiling detects topic changes with a sim-\nilarity score based on word frequencies, whereas\nwe detect topic changes based on a new similarity\nscore using BERT embeddings as follows.\n1.\nCompute BERT embeddings for every utter-\nance Si of the meeting transcript.\n2.\nDivide the meeting corpus into blocks of utter-\nances {Si, · · · , Sk}, and perform a block-wise\nmax pooling operation to extract the embed-\nding Ri for each block.\n3.\nCompute cosine similarity simi between ad-\njacent blocks Ri and Ri+1, where simi rep-\nresents the semantic similarity between two\nblocks separated at utterance Si.\n4.\nDerive the topic boundaries as pairs of blocks\nRi and Ri+1 with semantic similarity simi\nlower than a certain threshold. In particu-\nlar, we obtain a sequence of topic changes\nT = {i ∈[0, M]|simi < µs −σs} where µs\nand σs are the mean and variance of the se-\nquence of block similarities simi.\n4\nEvaluation\nDatasets.\nTo demonstrate the effectiveness of our\nmodel we evaluate it on the two major collections\nof meeting data produced in recent years. The\nICSI Meeting Corpus (Janin et al., 2003) includes\n75 recorded and transcribed meetings with topic\nsegmentation annotations (Gruenstein et al., 2008b)\nand the AMI Meeting Corpus (Kraaij et al., 2005)\nincludes 100 hours of recorded and transcribed\nmeetings also with topic segmentation annotation.\nBoth datasets include a hierarchical structure of the\ntopic annotation. For the purpose of this paper we\nconsider only the top-level meeting changes, i.e.,\nwe perform linear topic segmentation.\nDespite the fact that AMI and ICSI annotations\ncould be used to train a small supervised segmenter\nmodel, in a practical application of meeting seg-\nmentation there will be often no labeled data avail-\nable given the complexity of the annotation task.\nHence our unsupervised evaluation methodology\nis representative of real world practical meeting\nsegmentation scenarios.\nMetrics.\nTo evaluate the performance of our\nmodel we use two standard evaluation metrics,\nnamely Pk (Beeferman et al., 1999) and WinDiff\n(Pevzner and Hearst, 2002). Both metrics use a\nﬁxed sliding window over the document, and com-\nFigure 1: Comparison of predicted segmentation by our\nBERT model (in blue) and reference segmentation by\nhuman annotators (in dotted brown) on two meetings\nof the AMI dataset. Meeting EN2002d is an example\nof a low error rate segmentation (Pk/WinDiff) as topic\nchanges 1⃝and 2⃝are true positives. Meeting IB4002\nis an example of high error rate segmentation as 3⃝is a\nfalse positive topic change, 5⃝is a false negative topic\nchange and only 4⃝is a true positive topic change.\npare the predicted segmentation with the reference\nsegmentation from the annotations to express a\nprobability of segmentation error. Figure 1 depicts\ntwo predicted segmentations by our model; one\nwith a high and one with a low Pk/WinDiff score.\nBaselines.\nFirst, we compare our method against\ntwo commonly referenced naive baselines as in\n(Beeferman et al., 1999). The Random method\nplaces topic boundaries uniformly at random and\nthe Even method places boundaries every n-th ut-\nterance. Second, we compare our method against\nstate-of-the-art supervised learning models for\ntopic segmentation (Badjatiya et al., 2018). These\nmodels are based on BiLSTM-CNN architectures\n(Hochreiter and Schmidhuber, 1997) that require\nlarge datasets to converge. Hence the only viable\napproach is to train them on written text segmenta-\ntion datasets (Kazantseva and Szpakowicz, 2011)\nsuch as ﬁction books or Wikipedia articles. Un-\nfortunately, meeting transcripts are fundamentally\ndifferent from written text, with most utterances\nconveying minimal semantic signiﬁcance (Table 1).\nFinally, we compare against the standard unsuper-\nvised topic segmentation baseline, namely TextTil-\ning (Hearst, 1997), that uses word frequencies to\nmeasure the similarity among the utterances.\nResults.\nAs shown in Table 2, the standard Text-\nTiling method obtains the lowest error rate (PK =\n0.382) on ICSI dataset, in accordance to existing\nreports (Georgescul et al., 2006). Our methods\nMethod\nAMI Pk\nAMI Wd\nICSI Pk\nICSI Wd\nRandom\n0.604\n0.751\n0.632\n0.837\nEven\n0.513\n0.543\n0.601\n0.660\nTextTiling\n0.391\n0.410\n0.382\n0.408\nBiLSTM\n0.447\n0.473\n0.410\n0.430\nBERT (our)\n0.331\n0.333\n0.337\n0.345\nS-BERT (our)\n0.339\n0.334\n0.336\n0.349\nTable 2: Performance of different topic segmentation\nmethods using Pk and Wd (WinDiff) on AMI and ICSI\ndatasets.\nRandom and Even are the naive baselines;\nTextTiling is the unsupervised segmentation baseline;\nBiLSTM is the CNN-based supervised segmentation\nbaseline trained on Wikipedia data; embeddings based\non BERT and Sentence-BERT are our proposal.\nbased on BERT and Sentence-BERT embeddings\nobtain a 0.337 score on the same dataset, perform-\ning 15.5% and 11.8% better than TextTiling on\nthe WinDiff and Pk metrics respectively for the\nICSI Dataset. The performance difference is justi-\nﬁed as the BERT embeddings carry more semantic\nmeaning compared to the word frequency scores of\nTextTiling. This richer semantic meaning allows\nfor better detection of more nuanced topic changes.\nOur BERT and Sentence-BERT embeddings have\ncomparable performance on the tested datasets.\nThe supervised BiLSTM model shows poor per-\nformance with a lowest error rate of PK = 0.410\non ICSI dataset, compared to our best performance\nof PK = 0.337. The supervised model is trained to\nsegment fully formed sentences that carry substan-\ntially higher semantic signal compared to meeting\ntranscript utterances. The performance difference\nis accentuated by the max pooling operation that\nmakes our unsupervised method particularly robust\nto the noisy meeting transcripts.\n5\nConclusion\nWe presented an unsupervised model based on\nBERT embeddings for segmenting meeting tran-\nscripts. Our new model leverages the strong se-\nmantic representation power of BERT alongside a\nnew semantic similarity scoring technique, to en-\nable unsupervised topic segmentation for meeting\ntranscripts. Our model shows improved segmenta-\ntion performance compared to the non neural-based\napproach, namely TextTiling. As part of our fu-\nture work, we would like to incorporate additional\nsignal to the BERT embeddings, such as speaker\ninformation, meeting agendas and cross-meeting\nfeatures, in order to boost similar tasks such as\nmeeting summarization (Zhu et al., 2020).\nReferences\nSebastian Arnold, Rudolf Schneider, Philippe Cudr´e-\nMauroux, Felix A. Gers, and Alexander L¨oser.\n2019.\nSECTOR: A neural model for coher-\nent topic segmentation and classiﬁcation.\nCoRR,\nabs/1902.04793.\nAtlassian. 2021.\nYou waste a lot of time at\nwork.\nhttps://www.atlassian.com/\ntime-wasting-at-work-infographic.\nAc-\ncessed: 2021-01-24.\nPinkesh Badjatiya, Litton J Kurisinkel, Manish Gupta,\nand Vasudeva Varma. 2018. Attention-based neural\ntext segmentation. In European Conference on In-\nformation Retrieval, pages 180–193. Springer.\nJoe Barrow, Rajiv Jain, Vlad Morariu, Varun Manju-\nnatha, Douglas Oard, and Philip Resnik. 2020. A\njoint model for document segmentation and segment\nlabeling. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 313–322, Online. Association for Com-\nputational Linguistics.\nDoug Beeferman, Adam Berger, and John Lafferty.\n1999. Statistical models for text segmentation. Ma-\nchine learning, 34(1-3):177–210.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation.\narXiv preprint\narXiv:1708.00055.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMichel Galley,\nKathleen McKeown,\nEric Fosler-\nLussier, and Hongyan Jing. 2003.\nDiscourse seg-\nmentation of multi-party conversation. In Proceed-\nings of the 41st Annual Meeting of the Association\nfor Computational Linguistics, pages 562–569.\nMaria Georgescul, Alexander Clark, and Susan Arm-\nstrong. 2006. An analysis of quantitative aspects in\nthe evaluation of thematic segmentation algorithms.\nIn Proceedings of the 7th SIGdial Workshop on Dis-\ncourse and Dialogue, pages 144–151.\nAlexander Gruenstein, John Niekrasz, and Matthew\nPurver. 2008a. Meeting Structure Annotation, pages\n247–274. Springer Netherlands, Dordrecht.\nAlexander Gruenstein, John Niekrasz, and Matthew\nPurver. 2008b. Meeting structure annotation. In Re-\ncent Trends in Discourse and Dialogue, pages 247–\n274. Springer.\nMarti Hearst and Christian Plaunt. 2002.\nSubtopic\nstructuring for full-length document access.\nMarti A Hearst. 1997. Text tiling: Segmenting text into\nmulti-paragraph subtopic passages. Computational\nlinguistics, 23(1):33–64.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nAdam Janin, Don Baron, Jane Edwards, Dan Ellis,\nDavid Gelbart, Nelson Morgan, Barbara Peskin,\nThilo Pfau, Elizabeth Shriberg, Andreas Stolcke,\net al. 2003. The icsi meeting corpus. In 2003 IEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing, 2003. Proceedings.(ICASSP’03).,\nvolume 1, pages I–I. IEEE.\nAnna Kazantseva and Stan Szpakowicz. 2011. Linear\ntext segmentation using afﬁnity propagation. In Pro-\nceedings of the 2011 Conference on Empirical Meth-\nods in Natural Language Processing, pages 284–\n293.\nWessel Kraaij, Thomas Hain, Mike Lincoln, and Wil-\nfried Post. 2005. The ami meeting corpus.\nJing Li, Aixin Sun, and Shaﬁq R Joty. 2018. Segbot: A\ngeneric neural text segmentation model with pointer\nnetwork. In IJCAI, pages 4166–4172.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLev Pevzner and Marti A Hearst. 2002.\nA critique\nand improvement of an evaluation metric for text\nsegmentation. Computational Linguistics, 28(1):19–\n36.\nMatthew Purver. 2011. Topic segmentation. Spoken\nlanguage understanding: systems for extracting se-\nmantic information from speech, pages 291–317.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nbert:\nSentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3973–3983.\nGerard Salton, Amit Singhal, Chris Buckley, and Man-\ndar Mitra. 1996. Automatic text decomposition us-\ning text segments and text themes. pages 53–65.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015.\nFacenet: A uniﬁed embedding for\nface recognition and clustering. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 815–823.\nImran Sehikh, Dominique Fohr, and Irina Illina. 2017.\nTopic segmentation in asr transcripts using bidirec-\ntional rnns for change detection. In 2017 IEEE Auto-\nmatic Speech Recognition and Understanding Work-\nshop (ASRU), pages 512–518. IEEE.\nImran Sheikh, Irina Illina, and Dominique Fohr. 2016.\nHow diachronic text corpora affect context based\nretrieval of oov proper names for audio news.\nIn\nLREC 2016.\nElizabeth Shriberg. 2005. Spontaneous speech: How\npeople really talk and why engineers should care. In\nNinth European Conference on Speech Communica-\ntion and Technology.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nMatthew D Zeiler, Graham W Taylor, and Rob Fergus.\n2011. Adaptive deconvolutional networks for mid\nand high level feature learning.\nIn 2011 Interna-\ntional Conference on Computer Vision, pages 2018–\n2025. IEEE.\nLeilan Zhang and Qiang Zhou. 2019. Topic segmenta-\ntion for dialogue stream. In 2019 Asia-Paciﬁc Sig-\nnal and Information Processing Association Annual\nSummit and Conference (APSIPA ASC), pages 1036–\n1043. IEEE.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nChenguang Zhu, Ruochen Xu, Michael Zeng, and Xue-\ndong Huang. 2020. A hierarchical network for ab-\nstractive meeting summarization with cross-domain\npretraining. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning: Findings, pages 194–203.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\nA\nImplementation Details\nFor a clear description of our proposed model\nand algorithm we refer to Section 3 of our\nmain paper.\nThe experimental code used\nto implement our method and baselines can\nbe found at https://github.com/gdamaskinos/\nunsupervised_topic_segmentation.\nPretrained models.\nThe publicly available pre-\ntrained models used in our experiments are avail-\nable for download by using the following links:\n1. RoBERTA: roberta-base from the hugging-\nface transformers Python library at https:\n//huggingface.co/roberta-base\n2. Sentence-BERT:\nstsb-roberta-base\nfrom\nthe\nSentence-BERT\nPython\nli-\nbrary\nat\nhttps://www.sbert.net/docs/\npretrained_models.html\nEvaluation runtime.\nOur approach is unsuper-\nvised and there are no particular computational re-\nquirements. We report here the prediction runtime\non the entire ICSI and AMI dataset:\n1. BERT embeddings method: max run time\n14 minutes, 33 seconds, CPU time 1 hour, 9\nminutes, 57 seconds, AWS equivalent $0.29\n2. Naive and TextTiling baselines: min run\ntime 2 minutes, 26 seconds, max run time\n4 minutes, 52 seconds\nEvaluation metrics.\nWe use the standard evalu-\nation metrics Pk and WinDiff, that are commonly\nused to represent error rates of segmenter systems.\nWe refer to Section 4 for more details. We are us-\ning in our methods the Natural Langauge Toolkit\nLibrary (https://www.nltk.org/) reference im-\nplementation of the evaluation metrics:\n1. Pk\nis\ncomputed\nusing\nnltk.metrics.segmentation.pk\n2. WinDiff\nis\ncomputed\nusing\nnltk.metrics.segmentation.windowdiff\nB\nEvaluation Data\nWe evaluate our methods and baselines on the two\nstandard meeting corpora AMI and ICSI. We refer\nto Section 4 for more deatils.\nDataset description.\nThe datasets are publicly\navailable and need to be consumed through the\nNTX meeting visualisation tool (http://groups.\ninf.ed.ac.uk/nxt/index.shtml). The datasets\nare used entirely as a test dataset, without a valida-\ntion dataset given that our unsupervised approach\nrelies purely on embeddings from pre-trained mod-\nels. In the attached code we did not include the\ndatasets since they are publicly available by using\nthe links below.\n1. AMI Corpus consists of 100 hours of\nrecorded meetings (http://groups.inf.ed.\nac.uk/ami/download/) .\nData are an-\nnotated by human annotators using the\nNXT tool following this annotation guide-\nlines (http://groups.inf.ed.ac.uk/ami/\ncorpus/Guidelines)\n2. ICSI Corpus consists of 70 hours of\nrecorded meetings (http://groups.inf.ed.\nac.uk/ami/icsi/download) and follows the\nsame human annotation process as AMI.\nDataset preprocessing.\nIn our code we apply\nsome lightweight text preprocessing that can be\nfound in dataset.preprocessing. The preprocessing\nincludes:\n1. removing ﬁller words and lower casing\n2. ﬁltering captions shorter than 20 characters\nsince they would not result in good quality\nembeddings\n",
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "published": "2021-06-24",
  "updated": "2021-06-24"
}