{
  "id": "http://arxiv.org/abs/2106.13045v2",
  "title": "Where are we in semantic concept extraction for Spoken Language Understanding?",
  "authors": [
    "Sahar Ghannay",
    "Antoine Caubrière",
    "Salima Mdhaffar",
    "Gaëlle Laperrière",
    "Bassam Jabaian",
    "Yannick Estève"
  ],
  "abstract": "Spoken language understanding (SLU) topic has seen a lot of progress these\nlast three years, with the emergence of end-to-end neural approaches. Spoken\nlanguage understanding refers to natural language processing tasks related to\nsemantic extraction from speech signal, like named entity recognition from\nspeech or slot filling task in a context of human-machine dialogue.\nClassically, SLU tasks were processed through a cascade approach that consists\nin applying, firstly, an automatic speech recognition process, followed by a\nnatural language processing module applied to the automatic transcriptions.\nThese three last years, end-to-end neural approaches, based on deep neural\nnetworks, have been proposed in order to directly extract the semantics from\nspeech signal, by using a single neural model. More recent works on\nself-supervised training with unlabeled data open new perspectives in term of\nperformance for automatic speech recognition and natural language processing.\nIn this paper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU, with or without the use of additional data. We\nalso present our last results that significantly outperform the current\nstate-of-the-art with a Concept Error Rate (CER) of 11.2%, instead of 13.6% for\nthe last state-of-the-art system presented this year.",
  "text": "arXiv:2106.13045v2  [cs.CL]  11 Oct 2022\nWhere are we in semantic concept extraction for\nSpoken Language Understanding? ⋆\nSahar Ghannay1, Antoine Caubri`ere2, Salima Mdhaﬀar2, Ga¨elle Laperri`ere2,\nBassam Jabaian2, Yannick Est`eve2\n1 Universit´e Paris-Saclay, CNRS, LISN, 91400, Orsay, France\nfirstname.lastname@limsi.fr\n2 LIA - Avignon Universit´e, France\nfirstname.lastname@univ-avignon.fr\nAbstract. Spoken language understanding (SLU) topic has seen a lot of\nprogress these last three years, with the emergence of end-to-end neural\napproaches. Spoken language understanding refers to natural language\nprocessing tasks related to semantic extraction from speech signal, like\nnamed entity recognition from speech or slot ﬁlling task in a context of\nhuman-machine dialogue. Classically, SLU tasks were processed through\na cascade approach that consists in applying, ﬁrstly, an automatic speech\nrecognition process, followed by a natural language processing module\napplied to the automatic transcriptions. These three last years, end-to-\nend neural approaches, based on deep neural networks, have been pro-\nposed in order to directly extract the semantics from speech signal, by\nusing a single neural model. More recent works on self-supervised train-\ning with unlabeled data open new perspectives in term of performance\nfor automatic speech recognition and natural language processing. In this\npaper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU, with or without the use of addi-\ntional data. We also present our last results that signiﬁcantly outperform\nthe current state-of-the-art with a Concept Error Rate (CER) of 11.2%,\ninstead of 13.6% for the last state-of-the-art system presented this year.\nKeywords: Spoken language understanding · End-to-end approach ·\nCascade approach · Self supervised training\n1\nIntroduction\nSpoken language understanding (SLU) refers to natural language processing\ntasks related to semantic extraction from the speech signal [34], like named en-\ntity recognition from speech, call routing, slot ﬁlling task in a context of human-\nmachine dialogue. . .\nUsually, SLU tasks were processed through a cascade approach that consists\nin applying ﬁrst an automatic speech recognition (ASR) process, followed by a\n⋆This work was granted access to the HPC resources of IDRIS under the alloca-\ntion 2020-AD011011838 made by GENCI and partially funded through the H2020\nSELMA project (grant No 957017).\n2\nS. Ghannay et al.\nnatural language processing module applied to the automatic transcription [14].\nFor both automatic speech recognition and natural language processing, deep\nneural networks (DNN) have made great advances possible, leading to impressive\nimprovements of qualitative performance for ﬁnal SLU tasks [1,11,35].\nThese three last years, end-to-end neural approaches, based on deep neural\nnetworks, have been proposed in order to directly extract the semantics from\nspeech signal, by using a single neural model [29,18]. A ﬁrst advantage of such\napproaches consists in a joint optimization of the ASR and NLP part, since the\nunique neural model is optimized only for the ﬁnal SLU task. Another advantage\nis the limitation of the error propagation: when using a cascade approach, an\nerror in the ﬁrst treatment implies errors in the following ones. In a neural\nend-to-end approach, the model decision is delayed to the output layer: all the\ninformation uncertainty is handled until the ﬁnal decision.\nVery recently, works on self-supervised training with unlabeled data open\nnew perspectives in term of performance for automatic speech recognition and\nnatural language processing [2,16]. They can be applied to SLU task.\nThis study presents experimental results on the French MEDIA benchmark\ndataset. This benchmark dataset is one of the most challenging benchmarks\nfor SLU task. In this paper, we present a brief overview of the performance\nevolution of state-of-the-art systems on this benchmark dataset. We also present\nan approach that takes beneﬁt from acoustic-based and linguistic-based models\npre-trained on unlabelled data: this approach represents the next milestone to\nbe surpassed.\n2\nMEDIA dataset\nThe French MEDIA corpus [5], is dedicated to semantic extraction from speech\nin a context of human-machine dialogues for a hotel booking task. This dataset\nwas created as a part of the Technolangue project of the French government in\n2002. Its main objective is to set up an infrastructure for the production and\ndissemination of language resources, the evaluation of written and oral language\ntechnologies, the participation in national and international standardisation bod-\nies and an information monitoring in the ﬁeld.\nThe MEDIA dataset is made of telephone dialogue recordings with their man-\nual transcriptions and semantic annotations. It is composed of 1257 dialogues\nfrom 250 diﬀerent speakers, collected with a Wizard-of-Oz setting between two\nhumans: one plays a computer, the other plays the user. The dataset is split into\nthree parts (train, dev, test) as described in table 1. In this work, we used the\nuser part of MEDIA, since it has both speech and semantic annotations.\nThe semantic domain of this corpus is represented by 76 semantic concept\ntags such as room number, hotel name, location, etc. Some more complex lin-\nguistic tags, like co-references, are also used in this corpus.\nThe following sentence (translated from French) is an example of the MEDIA\ncontent: ”I would like to book one double room in Paris up to one hundred and\nthirty euros”. It will be annotated as (I would like to book, reservation), (one,\nWhere are we in semantic concept extraction for SLU?\n3\nTable 1. The oﬃcial MEDIA dataset distribution\nData\nNb\nNb\nNb\nNb\nWords Utterances Concepts Hours\ntrain\n94.2k\n13.7k\n31.7k\n10h46m\ndev\n10.7k\n1.3k\n3.3k\n01h13m\ntest\n26.6k\n3.7k\n8.8 k\n02h59m\nnumber-room), (double room, room-type), (up to, comparative-payment), (one\nhundred and thirty, amount-payment), (euros, currency-payment).\nIn [4], B´echet and Raymond showed why the MEDIA task can be considered\nas the most challenging SLU benchmark available, in comparison to other well-\nknown benchmarks such as ATIS [13], SNIPS [12], and M2M [31].\n3\nOverview of approaches proposed for the MEDIA\nbenchmark\n3.1\nCascade approach\nConventional SLU systems are designed as a cascade of components. Each of\nthem solves separately a speciﬁc problem. First, an ASR module, trained on a\nlarge amount of data, maps speech signals to automatic transcriptions. This is\nthen passed on to a natural language understanding (NLU) module that predicts\nsemantic information from the automatic transcriptions. In this approach, error\npropagation is unavoidable, despite the performance of current ASR and NLU\nsystems. In addition, those modules are optimized separately under diﬀerent\ncriteria. The ASR system is trained to minimize the word error rate (WER),\nwhile the NLU module is trained to minimize the concept error rate (CER) in\ncase of slot ﬁlling task. This separate optimization suggests that a cascade SLU\nsystem is suboptimal.\nWorking on automatic transcriptions, for an SLU task on MEDIA corpus,\nis highly challenging. Many approaches have been proposed. Early NLU ap-\nproaches were based on generative models such as Stochastic ﬁnite state trans-\nducers (FST), on discriminative or conditional models such as conditional ran-\ndom ﬁelds (CRFs) and support vector machines(SVMs)[21]. In the light of the\nsuccess of neural approaches in diﬀerent ﬁelds, some studies developed neural\narchitectures for SLU task. In [27], the author presents the ﬁrst recurrent neu-\nral architecture dedicated to SLU for the ATIS benchmark corpus. This neural\nmodel was applied to transcriptions despite speech signals directly. In [33,32], for\nthe ﬁrst time, an encoder-decoder neural network structure with attention mech-\nanism [3] was proposed for this task. This time, it was on manual and automatic\ntranscriptions from the MEDIA corpus. In order to reduce the unavoidable SLU\nperformance decline due to ASR errors, the authors in [33] have proposed ASR\nconﬁdence measures to localize ASR errors. These conﬁdence measures have been\nused as additional SLU features to be combined with lexical and syntactic fea-\ntures, useful for characterizing concept mentions. In [32], the authors proposed\n4\nS. Ghannay et al.\nan approach to simulate ASR errors from manual transcriptions, to improve the\nperformance of SLU systems. The use of the resulting corpus prepares the SLU\nsystem to ASR errors during their training and makes it more robust to ASR\nerrors.\n3.2\nEnd-to-end approach\nAs seen in the previous section, one problem with cascaded approaches is the\npropagation of errors through the components. The intermediate transcription\nis noisy due to speech recognition errors, and the NLU component has to deal\nwith these errors. The other problem comes from the separate optimizations of\nthe diﬀerent modules.\nTo tackle these issues, end-to-end approaches were proposed in order not to\nuse an intermediate speech transcriptions. This kind of approach aims to develop\na single system directly optimized to extract semantic concepts from speech.\nSLU end-to-end systems are usually trained to generate both recognized\nwords and semantic tags [18,15].\nUntil now, mainly two kinds of neural architectures have been proposed on\nthe MEDIA benchmark. The ﬁrst one is based on the use of the Connectionist\nTemporal Classiﬁcation (CTC) loss function [20], while the other one is based\non the use of an encoder-decoder architecture with attention mechanism [3].\n3.2.1\nCTC approach\nIn this work, we call CTC approach the neural architecture trained by using the\nCTC loss function. This loss function allows the system to learn an alignment\nbetween the input speech and the word and concept sequences to produce.\nTo our knowledge, the best-published results with a CTC approach on ME-\nDIA were obtained by [6]. In this study, the authors proposed a neural architec-\nture largely inspired by the DeepSpeech 2 speech recognition system. The neural\narchitecture is a stack of two 2D-invariant convolutional layers (CNN), followed\nby ﬁve bidirectional long short term memory (bLSTM) layers with sequence-wise\nbatch normalization, a classical fully connected layer, and the softmax output\nlayer. As input features, we used spectrograms of power to normalize audio clips,\ncalculated on 20ms windows. This system was trained following the curriculum-\nbased transfer learning approach, which consists in training the same model\nthrough successive stages, with diﬀerent tasks ranked from the most generic one\nto the most speciﬁc one. The authors used speech recognition tasks, then named\nentity extraction and ﬁnally semantic concept extraction tasks.\n3.2.2\nEncoder-decoder approach with attention mechanism\nThe encoder-decoder architecture was initially implemented in the machine\ntranslation context. This approach quickly showed its beneﬁts for the speech\nrecognition task [9,7,8], and more recently for SLU tasks [29,28].\nWhere are we in semantic concept extraction for SLU?\n5\nThe encoder-decoder architecture is divided into two main parts. First, an\nencoder receives the speech features as input, and provides its hidden states to\nbuild a high-level representation of the features. This high-level representation\nis then passed on to an attention module. It identiﬁes the parts of these rep-\nresentations that are relevant for each step of the decoding process. Next, the\nattention module computes a context vector from these representations to feed\nthe decoder. Finally, the decoder processes the input context vectors to predict\nthe transcription of speech, enriched with semantic concepts. At each decoding\ntime step, a new context vector is computed from the encoded speech represen-\ntations. Unlike CTC approaches, the output sequence size of an encoder-decoder\napproach does not depend on the input sequence size.\nA recent study [28] used a similar architecture and obtained the state-of-\nthe-art performance for the MEDIA task. The encoder part is composed of four\n2-dimensional convolution layers followed by four bLSTM layers. Each convolu-\ntion layer is followed by a batch normalization. The decoder part is a stack of\nfour bLSTM layers, two fully connected layers, and a softmax layer. The input\nfeatures of the network are 40-dimensional MelFBanks with a Hamming window\nof 25ms and 10ms strides.\nThis encoder-decoder system is trained following the curriculum-based trans-\nfer learning, with the same data used for the CTC approach presented in sec-\ntion 3.2.1, except for the named entity extraction task which was not used.\n3.3\nSystem performance\nSLU systems can be evaluated with diﬀerent metrics. Historically, on the MEDIA\ncorpus, two metrics are jointly used: the Concept Error Rate (CER) and the\nConcept/Value Error Rate (CVER). The CER is computed similarly to the Word\nError Rate, by only taking into account the concepts occurrences in both the\nreference and the hypothesis ﬁles. The CVER metrics is an extension of the CER.\nIt considers the correctness of the complete concept/value pair. In the example in\nsection 2, both ”one hundred and thirty” and amount-payment have to be correct\nto consider the concept/value pair (one hundred and thirty,amount-payment)\nas correct. Errors on the value component can come from a bad segmentation\n(missing or additional words in the value) or from ASR errors.\nTable 2 presents the best results obtained on the oﬃcial MEDIA bench-\nmark dataset, by the main families of approaches presented in the two previous\nsections. By computing the 95% conﬁdence interval, we observe a 0.7 conﬁ-\ndence margin for CER and 0.8 for CVER, when the CER is 13.6% and the\nCVER 18.5%. Until now, the best result was reached by an end-to-end encoder-\ndecoder architecture with attention mechanism, trained by following a curricu-\nlum transfer-learning approach [28].\n4\nImproving the state of the art\nIn the previous section we present state-of-the-art performances. Recently, un-\nsupervised learning on huge amount of data have been successfully proposed\n6\nS. Ghannay et al.\nArchitecture\nModel\nCER CVER\nCascade (2018)\nHMM/DNN ASR + neural NLU [32]\n20.2\n26.0\nCascade (2018)\nHMM/DNN ASR + CRF [32]\n20.2\n25.3\nCascade (2019)\nHMM/TDNN ASR + CRF [6]\n16.1\n20.4\nEnd-to-end (2019) E2E CTC [6]\n16.4\n20.9\nEnd-to-end (2021) E2E encoder-decoder with attention [28] 13.6\n18.5\nTable 2. Best results obtained on the oﬃcial MEDIA benchmark dataset, by the main\nfamilies of approaches presented in this paper. Results are given in both Concept Error\nRate and Concept/Value Error Rate\nto pre-train Transformers-based models [16,2]. Thanks to these models, ASR\nstate-of-the-art performance [2] and NLP state-of-the-art performance [16] have\nbeen outperformed, with respectively wav2vec and BERT models. In this sec-\ntion, we present a cascade system using both BERT and wav2vec optimized on\nthe MEDIA task.\n4.1\nBERT and CamemBERT models\nFor the NLU module, we propose to use the one that achieved the state-of-\nthe-art result on manual transcriptions of MEDIA corpus [19]. This system is\nbased on a ﬁne-tuning of BERT [16] on MEDIA SLU task using the French\nCamemBERT [26] model.\nBERT [16] is a deeply bidirectional, unsupervised language representation\nmodel, which stands for Bidirectional Encoder Representations from Transform-\ners. It is designed to pre-train deep bidirectional representations from unlabeled\ntext, taking into account both left and right context in all layers. The resulting\npre-trained BERT model can be ﬁne-tuned with just one additional output layer,\nto create state-of-the-art models for a wide range of NLP tasks. BERT is pre-\ntrained using a combination of masked language modeling objective and next\nsentence prediction on a large corpus which include the Toronto Book Corpus\nand Wikipedia.\nThe French CamemBERT model is based on RoBERTa (Robustly Optimized\nBERT Pre-training Approach) [25] which is based on BERT. CamemBERT is\nsimilar to RoBERTa, which dynamically change the masking pattern applied to\nthe training data, and remove the next sentence prediction task. In addition,\nit uses the whole word masking and the SentencePiece tokenization [24]. The\nCamemBERT model is trained on the French CCNet corpus composed of 135GB\nof raw text.\n4.2\nWav2vec models\nWav2vec 2.0 [2] is a model pre-trained through self-supervision. It takes raw\naudio as input and computes contextual representations that can be used as\ninput for speech recognition systems. It contains three main components: a con-\nvolutional feature encoder, a context network and a quantization block. The\nWhere are we in semantic concept extraction for SLU?\n7\nconvolutional feature encoder converts the audio signal into a latent represen-\ntation. This representation is given to the context network which takes care of\nthe context. The context network architecture consists of a succession of several\ntransformer encoder blocks. The quantization network is used to map the latent\nrepresentation to quantized representation.\nIn [17], the authors released French pre-trained wav2vec 2.0 models. Two\nmodels have been released for public use3, a large one and a base one. In this\nstudy, we use the large conﬁguration which encodes raw audio into frames of\n1024-dimensional vectors. The models are pre-trained in a unsupervised way\nwith 3K hours of unlabeled speech. Details about data used to train the wav2vec\nmodels can be found in [17]. The trained model is composed of about 300M\nparameters.\nTo get better ASR results than the ones we could reach by ﬁne-tuning the\nFrench wav2vec 2.0 model, on the MEDIA training data only, we suggest to\n,ﬁrst, ﬁne-tune on external audio data, as proposed in [6] or [28]. To make the\nexperiments reproducible, instead of using the Broadcast News data used in\nthese works, we used the CommonVoice French dataset4 (version 6.1), collected\nby the Mozilla Foundation, and much easily accessible. The train set consists of\n425.5 hours of speech, while the validation and test sets contain around 24 hours\nof speech.\n4.3\nCascade approach with pre-trained models\nAs written before, we propose in this work to use a cascade approach, with pre-\ntrained models for each component. The ASR system is composed of the large\npre-trained French wav2vec model, a linear layer of 1024 units, and the softmax\noutput layer. First, we optimize the ASR system on the French CommonVoice\ndataset. Then, we ﬁne-tune it for speech recognition on the French MEDIA\ncorpus, the wav2vec weights being updated at each training stage. The loss\nfunction used at each ﬁne-tuning step is the CTC loss function. We call the ﬁnal\nASR model W2V • Common Voice • MASR.\nThe NLU system is applied on the automatic transcriptions provided from\nthe ASR system, to obtain semantic annotations. This system is based on the\nﬁne-tuning of the French CamemBERT [26] model, on the manual transcriptions\nof MEDIA corpus. It achieved state-of-the-art result on manual transcriptions\nof MEDIA corpus [19], yielding to 7.56 of CER when there is no error in the\ntranscription.\n4.4\nResults and discussion\nThe experimental result obtained with the proposed cascade approach is pre-\nsented in table 3. We compare the performance of this cascade system, named\nW2V • Common Voice • MASR + CamemBERT, to the E2E encoder-decoder\n3 https://huggingface.co/LeBenchmark\n4 https://commonvoice.mozilla.org/fr/datasets\n8\nS. Ghannay et al.\nmodel proposed in [28], that reached the best result on this task until now, and\nother wav2vec-based models. All the wav2vec-based models presented in table 3\nwere implemented thanks to the SpeechBrain toolkit5, including the ﬁne-tuning\nof the wav2vec models.\nLike in section 3.3, the results are evaluated in terms of CER and CVER.\nOur new system yields to 17.64% of relative CER improvement and 7.02% of\nrelative CVER improvement, by reaching respectively 11.2% of CER and 17.2%\nof WER. The result shows the eﬀectiveness of unsupervised pre-trained models\nlike wav2vec and BERT in such a scenario. Notice that the W2V • Common\nVoice • MASR model allows us to have an eﬀective ASR system that achieved\n8.5% of WER.\nIn system (1), the wav2vec model is ﬁne-tuned directly on MEDIA SLU\n(MSLU) task. In system (2), the wav2vec model is ﬁrst ﬁne-tuned on the Com-\nmon Voice data then on MSLU task, and a beam search decoding is applied. In\nsystems (3) and (4) the wav2vec model is ﬁrst ﬁne-tuned on the Common Voice\ndata, then on MEDIA ASR (MASR), and last on MSLU task, using the greedy\nor the beam search decoding using a 5-gram language model to rescore. This\nlanguage model is trained on the manual transcriptions of MSLU training data\nonly.\nIt is worth to mention that even before the generalisation of the use of neural\nnetworks for sequential tagging tasks, such as the slot ﬁlling task investigated\nin this paper, several eﬀorts have been made to better take into account the the\nASR system errors during the semantic labeling. Many approaches have been\nproposed for a joint decoding between speech recognition and understanding,\nconsidering the n-best recognition hypotheses during the semantic annotation\n[22,30,23]. When neural networks have become state-of-the-art systems for SLU,\nend-to-end approaches have gradually replaced cascade approaches and have\nshown very good performance, allowing the semantic labeling of a speech signal\nand minimising the impact of transcription errors on the SLU performance. How-\never, these architectures need a large amount of data and often use pre-trained\nexternal module that have been trained separately in out-of-context data. The\nresults presented in table 3 show that if such pre-trained models are used in a\ncascade architecture, the resulting system reaches or even exceeds the perfor-\nmance of the end-to-end based one. In addition, the result of the cascade system\nreinforces the idea of the use of pre-trained models at the encoder (wav2vec) and\ndecoder (BERT) levels within end-to-end architecture, as proposed in [10]. This\nleads us to conclude that the two architectures remain valid and competitive\nand that the choice should be made according to the availability of additional\ndata and the pre-training models.\n5\nConclusions\nIn this paper, we present a brief overview of the recent advances on the French\nMEDIA benchmark dataset for SLU. We propose a system based on a cascade\n5 https://speechbrain.github.io\nWhere are we in semantic concept extraction for SLU?\n9\nArchitecture Model\nCER CVER\nEnd-to-end\nencoder-decoder [28]\n13.6\n18.5\n(1) W2V • MSLU (Beam 5g)\n18.8\n23.6\n(2) W2V • Common Voice • MSLU (Beam 5g)\n15.8\n20.4\n(3) W2V • Common Voice • MASR • MSLU (greedy)\n15.4\n20.5\n(4) W2V • Common Voice • MASR • MSLU (Beam 5g) 14.5\n18.8\nCascade\nW2V • Common Voice • MASR + CamemBERT\n11.2\n17.2\nTable 3. Performance on Test MEDIA in terms of CER and CVER scores of the\nproposed cascade and end-to-end systems using pre-trained models. ”•” formalizes a\ntransfer learning step during the training of the E2E system.\napproach, that takes beneﬁt from acoustic-based and linguistic-based models pre-\ntrained on unlabelled data : wav2vec models for the ASR system, and BERT-\nlike model for the NLU system. Experimental results show that our system\noutperforms signiﬁcantly the current state of the art with a Concept Error Rate\n(CER) of 11.2% instead of 13.6% for the last state-of-the-art system presented\nthis year.\nThis new advance reinforces the idea of the use of pre-trained models at the\nencoder (wav2vec) and decoder (BERT) levels within an end-to-end architecture.\nThis will be explored in our future work.\nThis study leads us to conclude that the two architectures (cascade vs. end-to-\nend) remain valid and competitive and that the choice should be made according\nto the availability of additional data and relevant pre-trained models.\nReferences\n1. Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case,\nC., Casper, J., Catanzaro, B., Cheng, Q., Chen, G., et al.: Deep speech 2: End-\nto-end speech recognition in english and mandarin. In: International conference on\nmachine learning. pp. 173–182. PMLR (2016)\n2. Baevski, A., Zhou, H., Mohamed, A., Auli, M.: wav2vec 2.0: A framework for\nself-supervised learning of speech representations. arXiv preprint arXiv:2006.11477\n(2020)\n3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n4. B´echet, F., Raymond, C.: Benchmarking benchmarks: introducing new automatic\nindicators for benchmarking spoken language understanding corpora. In: Inter-\nspeech. Graz, Austria (2019)\n5. Bonneau-Maynard, H., Rosset, S., Ayache, C., Kuhn, A., Mostefa, D.: Semantic\nannotation of the french media dialog corpus. In: INTERSPEECH (2005)\n6. Caubri`ere, A., Tomashenko, N., Laurent, A., Morin, E., Camelin, N., Est`eve,\nY.: Curriculum-Based Transfer Learning for an Eﬀective End-to-End Spo-\nken Language Understanding and Domain Portability. In: Proc. Interspeech\n2019.\npp.\n1198–1202\n(2019).\nhttps://doi.org/10.21437/Interspeech.2019-1832,\nhttp://dx.doi.org/10.21437/Interspeech.2019-1832\n10\nS. Ghannay et al.\n7. Chan, W., Jaitly, N., Le, Q., Vinyals, O.: Listen, attend and spell: A neural network\nfor large vocabulary conversational speech recognition. In: 2016 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP). pp. 4960–4964.\nIEEE (2016)\n8. Chiu, C.C., Sainath, T.N., Wu, Y., Prabhavalkar, R., Nguyen, P., Chen, Z., Kan-\nnan, A., Weiss, R.J., Rao, K., Gonina, E., et al.: State-of-the-art speech recogni-\ntion with sequence-to-sequence models. In: 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). pp. 4774–4778. IEEE (2018)\n9. Chorowski, J., Bahdanau, D., Cho, K., Bengio, Y.: End-to-end continuous speech\nrecognition using attention-based recurrent nn: First results. arXiv preprint\narXiv:1412.1602 (2014)\n10. Chung, Y.A., Zhu, C., Zeng, M.: Splat: Speech-language joint pre-training for spo-\nken language understanding. In: Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies. pp. 1897–1907 (2021)\n11. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P.:\nNatural language processing (almost) from scratch. Journal of machine learning\nresearch 12(ARTICLE), 2493–2537 (2011)\n12. Coucke, A., Saade, A., Ball, A., Bluche, T., Caulier, A., Leroy, D., Doumouro, C.,\nGisselbrecht, T., Caltagirone, F., Lavril, T., et al.: Snips voice platform: an embed-\nded spoken language understanding system for private-by-design voice interfaces.\narXiv preprint arXiv:1805.10190 (2018)\n13. Dahl, D.A., Bates, M., Brown, M.K., Fisher, W.M., Hunicke-Smith, K., Pallett,\nD.S., Pao, C., Rudnicky, A., Shriberg, E.: Expanding the scope of the atis task:\nThe atis-3 corpus. In: HUMAN LANGUAGE TECHNOLOGY: Proceedings of a\nWorkshop held at Plainsboro, New Jersey, March 8-11, 1994 (1994)\n14. De Mori, R.: Spoken language understanding: a survey. In: 2007 IEEE Workshop\non Automatic Speech Recognition & Understanding (ASRU). pp. 365–376. IEEE\n(2007)\n15. Desot, T., Portet, F., Vacher, M.: Towards end-to-end spoken intent recognition in\nsmart home. In: 2019 International Conference on Speech Technology and Human-\nComputer Dialogue (SpeD). pp. 1–8. IEEE (2019)\n16. Devlin,\nJ.,\nChang,\nM.W.,\nLee,\nK.,\nToutanova,\nK.:\nBERT:\nPre-\ntraining\nof\ndeep\nbidirectional\ntransformers\nfor\nlanguage\nunderstand-\ning.\nIn:\nNAACL-HLT.\nAssociation\nfor\nComputational\nLinguistics,\nMin-\nneapolis,\nMinnesota\n(Jun\n2019).\nhttps://doi.org/10.18653/v1/N19-1423,\nhttps://www.aclweb.org/anthology/N19-1423\n17. Evain, S., Nguyen, H., Le, H., Zanon Boito, M., Mdhaﬀar, S., Alisamir, S., Tong, Z.,\nTomashenko, N., Dinarelli, M., Parcollet, T., Allauzen, A., Est`eve, Y., Lecouteux,\nB., Portet, F., Rossato, S., Ringeval, F., Schwab, D., Besacier, L.: Lebenchmark: A\nreproducible framework for assessing self-supervised representation learning from\nspeech. In: Interspeech. Brno, Czechia (2021)\n18. Ghannay, S., Caubri`ere, A., Est`eve, Y., Camelin, N., Simonnet, E., Laurent, A.,\nMorin, E.: End-to-end named entity and semantic concept extraction from speech.\nIn: 2018 IEEE Spoken Language Technology Workshop (SLT). pp. 692–699. IEEE\n(2018)\n19. Ghannay,\nS.,\nServan,\nC.,\nRosset,\nS.:\nNeural\nnetworks\napproaches\nfo-\ncused\non\nFrench\nspoken\nlanguage\nunderstanding:\napplication\nto\nthe\nMEDIA\nevaluation\ntask.\nIn:\nProceedings\nof\nthe\n28th\nInternational\nWhere are we in semantic concept extraction for SLU?\n11\nConference\non\nComputational\nLinguistics.\npp.\n2722–2727.\nInterna-\ntional\nCommittee\non\nComputational\nLinguistics,\nBarcelona,\nSpain\n(On-\nline)\n(december\n2020).\nhttps://doi.org/10.18653/v1/2020.coling-main.245,\nhttps://www.aclweb.org/anthology/2020.coling-main.245\n20. Graves, A., Fern´andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal\nclassiﬁcation: labelling unsegmented sequence data with recurrent neural networks.\nIn: Proceedings of the 23rd international conference on Machine learning. pp. 369–\n376 (2006)\n21. Hahn, S., Dinarelli, M., Raymond, C., Lefevre, F., Lehnen, P., De Mori, R., Mos-\nchitti, A., Ney, H., Riccardi, G.: Comparing stochastic approaches to spoken lan-\nguage understanding in multiple languages. IEEE Transactions on Audio, Speech,\nand Language Processing 19(6), 1569–1583 (2010)\n22. Hakkani-T¨u, D., B´echet, F., Riccardi, G., Tur, G.: Beyond ASR 1-best: Us-\ning word confusion networks in spoken language understanding. Computer\nSpeech\nand\nLanguage\n(Oct\n2005).\nhttps://doi.org/10.1016/j.csl.2005.07.005,\nhttps://hal.archives-ouvertes.fr/hal-01314993\n23. Jabaian, B., Lef`evre, F.: Error-corrective discriminative joint decoding of auto-\nmatic spoken language transcription and understanding. In: Bimbot, F., Cerisara,\nC., Fougeron, C., Gravier, G., Lamel, L., Pellegrino, F., Perrier, P. (eds.) INTER-\nSPEECH 2013, 14th Annual Conference of the International Speech Communica-\ntion Association, Lyon, France, August 25-29, 2013. pp. 2718–2722. ISCA (2013),\nhttp://www.isca-speech.org/archive/interspeech_2013/i13_2718.html\n24. Kudo, T., Richardson, J.: SentencePiece: A simple and language independent\nsubword tokenizer and detokenizer for neural text processing. In: Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Process-\ning: System Demonstrations. pp. 66–71. Association for Computational Lin-\nguistics, Brussels, Belgium (Nov 2018). https://doi.org/10.18653/v1/D18-2012,\nhttps://www.aclweb.org/anthology/D18-2012\n25. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 (2019)\n26. Martin, L., Muller, B., Ortiz Su´arez, P.J., Dupont, Y., Romary, L., de la Clerg-\nerie, ´E.V., Seddah, D., Sagot, B.: Camembert: a tasty french language model. In:\nProceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics (2020)\n27. Mesnil, G., He, X., Deng, L., Bengio, Y.: Investigation of recurrent-neural-network\narchitectures and learning methods for spoken language understanding. In: Inter-\nspeech. pp. 3771–3775 (2013)\n28. Pelloin,\nV.,\nCamelin,\nN.,\nLaurent,\nA.,\nDe\nMori,\nR.,\nCaubri`ere,\nA.,\nEst`eve,\nY.,\nMeignier,\nS.:\nEnd2end\nacoustic\nto\nsemantic\ntransduction.\nIn:\nICASSP\n2021\n-\n2021\nIEEE\nInternational\nConference\non\nAcous-\ntics,\nSpeech\nand\nSignal\nProcessing\n(ICASSP).\npp.\n7448–7452\n(2021).\nhttps://doi.org/10.1109/ICASSP39728.2021.9413581\n29. Serdyuk, D., Wang, Y., Fuegen, C., Kumar, A., Liu, B., Bengio, Y.: Towards end-\nto-end spoken language understanding. In: 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). pp. 5754–5758. IEEE (2018)\n30. Servan,\nC.,\nRaymond,\nC.,\nB´echet,\nF.,\nNocera,\nP.:\nConceptual\ndecod-\ning\nfrom\nword\nlattices:\napplication\nto\nthe\nspoken\ndialogue\ncorpus\nME-\nDIA.\nIn:\nThe\nNinth\nInternational\nConference\non\nSpoken\nLanguage\nPro-\ncessing (Interspeech 2006 - ICSLP). Pittsburgh, United States (Sep 2006),\nhttps://hal.archives-ouvertes.fr/hal-01160181\n12\nS. Ghannay et al.\n31. Shah, P., Hakkani-T¨ur, D., T¨ur, G., Rastogi, A., Bapna, A., Nayak, N., Heck, L.:\nBuilding a conversational agent overnight with dialogue self-play. arXiv preprint\narXiv:1801.04871 (2018)\n32. Simonnet, E., Ghannay, S., Camelin, N., Est`eve, Y.: Simulating ASR er-\nrors\nfor\ntraining\nSLU\nsystems.\nIn:\nProceedings\nof\nthe\nEleventh\nInterna-\ntional Conference on Language Resources and Evaluation (LREC 2018). Euro-\npean Language Resources Association (ELRA), Miyazaki, Japan (May 2018),\nhttps://www.aclweb.org/anthology/L18-1499\n33. Simonnet, E., Ghannay, S., Camelin, N., Est`eve, Y., De Mori, R.: ASR error\nmanagement for improving spoken language understanding. In: Interspeech 2017.\nStockholm, Sweden (Aug 2017)\n34. Tur, G., De Mori, R.: Spoken language understanding: Systems for extracting\nsemantic information from speech. John Wiley & Sons (2011)\n35. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. arXiv preprint arXiv:1706.03762 (2017)\n",
  "categories": [
    "cs.CL",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2021-06-24",
  "updated": "2022-10-11"
}