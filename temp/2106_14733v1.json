{
  "id": "http://arxiv.org/abs/2106.14733v1",
  "title": "Unsupervised Discovery of Actions in Instructional Videos",
  "authors": [
    "AJ Piergiovanni",
    "Anelia Angelova",
    "Michael S. Ryoo",
    "Irfan Essa"
  ],
  "abstract": "In this paper we address the problem of automatically discovering atomic\nactions in unsupervised manner from instructional videos. Instructional videos\ncontain complex activities and are a rich source of information for intelligent\nagents, such as, autonomous robots or virtual assistants, which can, for\nexample, automatically `read' the steps from an instructional video and execute\nthem. However, videos are rarely annotated with atomic activities, their\nboundaries or duration. We present an unsupervised approach to learn atomic\nactions of structured human tasks from a variety of instructional videos. We\npropose a sequential stochastic autoregressive model for temporal segmentation\nof videos, which learns to represent and discover the sequential relationship\nbetween different atomic actions of the task, and which provides automatic and\nunsupervised self-labeling for videos. Our approach outperforms the\nstate-of-the-art unsupervised methods with large margins. We will open source\nthe code.",
  "text": "Unsupervised Discovery of Actions in Instructional Videos\nAJ Piergiovanni\nRobotics at Google\nAnelia Angelova\nRobotics at Google\nMichael Ryoo\nRobotics at Google\nIrfan Essa\nGoogle Research\nAbstract\nIn this paper we address the problem of automatically\ndiscovering atomic actions in unsupervised manner from in-\nstructional videos. Instructional videos contain complex ac-\ntivities and are a rich source of information for intelligent\nagents, such as, autonomous robots or virtual assistants,\nwhich can, for example, automatically ‘read’ the steps from\nan instructional video and execute them. However, videos\nare rarely annotated with atomic activities, their bound-\naries or duration. We present an unsupervised approach\nto learn atomic actions of structured human tasks from a\nvariety of instructional videos. We propose a sequential\nstochastic autoregressive model for temporal segmentation\nof videos, which learns to represent and discover the se-\nquential relationship between different atomic actions of the\ntask, and which provides automatic and unsupervised self-\nlabeling for videos. Our approach outperforms the state-of-\nthe-art unsupervised methods with large margins. We will\nopen source the code.\n1. Introduction\nInstructional videos cover a wide range of tasks: cook-\ning, furniture assembly, repairs, etc. The availability of on-\nline instructional videos for almost any task provides a valu-\nable resource for learning, especially in the case of learning\nrobotic tasks. So far, the primary focus of activity recog-\nnition has been on supervised classiﬁcation or detection of\ndiscrete actions in videos, such as sports actions [15, 41, 34]\nor in-home activities, e.g. [32, 6] using fully annotated\nvideos. However, instructional videos are rarely annotated\nwith atomic action-level instructions. Several works have\nstudied weakly-supervised settings where the order or pres-\nence of actions per-video is given, but not their duration\n[25, 13]. In this work, we propose a method to learn to seg-\nment instructional videos in atomic actions in an unsuper-\nvised way, i.e., without any annotations. To do this, we take\nadvantage of the structure in instructional videos: they com-\nprise complex actions which inherently consist of smaller\natomic actions with predictable order. While the temporal\nInput\nVideo\nN \nGenerated \nSequences\nTop ranked self-label sequence used for learning\nModel\nFigure 1. Overview: Our model generates multiple sequences for\neach video which are ranked based on several constraints (colors\nrepresent different actions). The top ranked sequence is used as\nself-labels to train the action segmentation model. This processes\nis repeated until convergence. No annotations are used.\nstructure of activities in instructional videos is strong, there\nis high variability of the visual appearance of actions, which\nmakes the task, especially in its unsupervised setting, very\nchallenging. For example, videos of preparing a salad can\nbe taken in very different environments, using kitchenware\nand ingredients of varying appearance.\nThe central idea is to learn a stochastic model that gen-\nerates multiple, different candidate sequences, which can\nbe ranked based on instructional video constraints. The top\nranked sequence is used as self-labels to train the action seg-\nmentation model. By iterating this process in an EM-like\nprocedure, the model converges to a good segmentation of\nactions (Figure 1). In contrast to previous weakly [25, 13]\nand unsupervised [1, 18] action learning works, our method\nonly requires input videos, no further text, actions, or other\nannotations are used.\nWe evaluate the approach on multiple datasets and com-\npare to previous methods on unsupervised action segmenta-\ntion. We also compare to weakly-supervised and supervised\nbaselines. Our unsupervised method outperforms all state-\nof-the-art models, in some cases considerably, with perfor-\nmance at times outperforming weakly-supervised methods.\nOur contributions are (1) a stochastic model capable of\ncapturing multiple possible sequences, (2) a set of con-\nstraints and training method that is able to learn to segment\nactions without any labeled data.\n1\narXiv:2106.14733v1  [cs.CV]  28 Jun 2021\nInitial State\nN1\nFC-layer\nRule logits\nR-dim\nGumbel-\nSoftmax\nSelected Rule\nr\nFC-layers\nState\nN2\nAction\na1\nFC-layer\nRule logits\nR-dim\nGumbel-\nSoftmax\nSelected Rule\nr\nFC-layers\nState\nN3\nAction\na2\nFigure 2.\nOverview of the stochastic recurrent model which generates an output action per step and a latent state (which will in turn\ngenerate next actions). Each time the model is run, a different rule is selected, thanks to the Gumbel-Softmax trick, leading to a different\naction and state. This results in multiple sequences (see text for more details).\n2. Related Work\nStudying instructional videos has gained a lot of interest\nrecently [1, 23, 38, 7], largely fueled by advancements in\nfeature learning and activity recognition for videos [5, 40,\n39, 9, 28]. However, most work on activity segmentation\nhas focused on the fully-supervised case [31, 43], which\nrequires per-frame labels of the occurring activities.\nSince it is expensive to fully annotate videos, weakly-\nsupervised activity segmentation has been proposed. Initial\nworks use movie scripts to obtain weak estimates of actions\n[19, 22] or localize actions based on related web images [10,\n11, 37]. [3] perform weakly-supervised segmentation when\nassuming the ordering was given, both during training and\ntest time. Temporal ordering constraints [13] or language\n[2, 42, 30] have also been applied to learn segmentation.\nRelated ‘set-supervised’ learning [26, 20, 8] only assumes\nthe actions in the video are known, but not the ordering.\nSeveral unsupervised methods have also been pro-\nposed [1, 18, 35, 29]. Alayrac et al. [1] learn action seg-\nmentation without segmentation supervision, using text in\naddition to video data. [18] uses k-means clustering to do\na time-based clustering of features and the Viterbi algo-\nrithm segment the videos based on the clusters. [29] uses\na GMM to learn a transition model between actions. We\npropose a fully differentiable unsupervised action segmen-\ntation, which works from RGB inputs only.\nSeveral datasets for learning from instructional videos\nhave been introduced recently: Breakfast [16], 50-salads\n[36], the Narrated Instructional Videos (NIV) [1], COIN\n[38], HowTo100m [23], CrossTask [44] and PROCEL [7].\n3. Method\nOur goal is to discover atomic actions from a set of\ninstructional videos, while capturing and modeling their\ntemporal structure. Formally, given a set of videos V =\n{V 1, V 2, ...} of a task or set of tasks, the objective is to\nTime\nInput \nVideo\nCNN\nFeature\nStates\nSequential\nModel\nActions\nFigure 3. We use a CNN to process each frame, and concatenate\nthose features with the state. Our sequential stochastic model pro-\ncesses each frame, generating a sequence of actions.\nlearn a model that maps a sequence of frames V i = [It]T\nt=1\nfrom any video to a sequence of atomic action symbols\n[at ∈O]T\nt=1 where O is a set of possible action symbols\n(we drop the index i for simplicity).\nSupervised approaches assume that each frame is labeled\nwith an action, and most weakly supervised approaches as-\nsume the actions per video are given in their correct order,\nbut without start and end times. In the unsupervised case,\nsimilar to previous works [1, 18], we assume no action la-\nbels or boundaries are given. To evaluate the approach, we\nfollow the previous setting using the Hungarian algorithm to\nmatch predicted actions to ground truth labels. While previ-\nous methods used additional data such a subtitles or text [1],\nthe proposed approach does not use such information. Our\nmodel, however, works with a ﬁxed k-the number of actions\nper task (analogous to setting k in k-means clustering), and\nwe run it with a range of values for k. This is not a very\nstrict assumption as the number of expected atomic actions\nper instruction is roughly known, e.g., about 10 actions for\ndoing CPR, or 40 actions when making a salad. For ex-\nample, a video of making a fried egg will contain the same\natomic actions: e.g., cracking the egg, heating a pan, frying\nthe egg, and serving. However, the temporal order, duration\nand appearance of the actions will vary across videos.\n3.1. Sequential Stochastic Autoregressive Model\nOur method is based on a sequential stochastic autore-\ngressive model (e.g., [24, 4]). The model consists of three\ncomponents: (H, O, R) where H is a ﬁnite set of states,\nO is a ﬁnite set of output symbols, and R is a ﬁnite set\nof transition rules mapping from a state to an output sym-\nbol and next state. Importantly, this model is stochastic,\ni.e., each rule is additionally associated with a probability\nof being selected, and thus the sum of the rule probabilities\nfor a given state is 1. Note that during training, O is just\na set of symbols with no semantic meaning or connection\nto the ground truth labels. For evaluation, following previ-\nous works ([18]), we use the Hungarian algorithm to match\nthese to ground truth symbols.\nTo implement this method in a differentiable way, we\nuse fully-connected layers and the Gumbel-Softmax trick\n[14, 21]. Speciﬁcally, we use several FC layers taking the\ncurrent state as input and outputting a vector of logits, rep-\nresenting probabilities of each rule being selected. Next,\nusing the Gumbel-Softmax trick, the model differentiably\nsamples one of the rules. Each time this function is run, a\ndifferent rule can be selected, learning to generate different\nsequences. This property is important for the learning of\ndependencies in sequences.\nLet G(Ni) be this function which maps from the state\nNi ∈H to an output symbol (i.e., action) a ∈O and the\nnext state Ni+1: (a, Ni+1) = G(Ni) and G ∈R. Ni is a la-\ntent vector representation, to be learned through backprop-\nagation. The function G is applied autoregressively to gen-\nerate a sequence (Figure 2). Our approach could be viewed\nas a state model version of [24], capturing the stochastic\nsequential structure of the tasks.\nFor a full video V = [I1, I2, I3, . . . , IT ] as input, where\neach It is an RGB image frame from the video, we process\nthe frames by some CNN (e.g., ResNet, I3D [5], Assem-\nbleNet [28], we use the latter), resulting in a sequence of\nfeature vectors, [f1, f2, . . . , fT ]. These features are used as\ninput to the model, which will generate a sequence of output\nsymbols S = [a1, a2, . . . , aT ] as follows:\na1, N1 = G(N0, f1),\na2, N2 = G(N1, f2),\naT , NT = G(NT −1, fT )\n(1)\nThe model takes each feature as input and concatenates\nit with the state which is used as input to G to produce the\noutput. Once applied to every frame, this results in a se-\nquence of actions (Figure 3). We note that the size of O,\nk, is a hyper-parameter and controls the number of atomic\nactions expected in the videos. We include experiments on\nthe effect of varying the size of O.\nCrack egg\nFry Egg\nServe Egg\nFlip Egg\nCrack egg\nFry Egg\nServe \nEgg\nFlip Egg\nCrack egg\nServe \nEgg\nFry Egg\nFlip Egg\nCandidate Sequences\nFry Egg\nFigure 4. Multiple candidate sequences are generated and ranked.\nThe best sequence according to the ranking function is chosen as\nthe labels for the iteration.\n3.2. Learning by Self-Labeling of Videos\nIn order to train the model without ground truth action\nsequences, we introduce an approach of learning by ‘self-\nlabeling’ videos. The idea is to optimize the model by gen-\nerating self-supervisory labels that best satisﬁes the con-\nstraints required for atomic actions. Notably, the stochas-\ntic ability to generate multiple sequences is key to this ap-\nproach. As a result of the learning, a sequence with better\nconstraint score will become more likely to be generated\nthan the sequences with worse scores.\nWe ﬁrst generate multiple candidate sequences, then\nrank them based on the instructional video constraints,\nwhich importantly require no labeled data.\nSince the\nGumbel-Softmax adds randomness to the model, the out-\nput can be different each time G is run with the same input,\nwhich is key to the approach. Speciﬁcally, the model is run\nM times, giving M potentially different sequences of ac-\ntions. We then deﬁne a cost function to rank each of the\nM sequences. The top ranked sequence is selected as the\nlabels which are used for learning. This ranking function\nconstrains the possible generated sequences. The ranking\nfunction we propose to capture the structure of instructional\nvideos has multiple components:\n• Every atomic action must occur once in the task.\n• Every atomic action should have similar lengths across\nvideos of the same task.\n• Each symbol should reasonably match the provided vi-\nsual feature.\nThe best sequence according to the ranking is selected as\nthe action labels for the iteration (Fig. 4), and the network\nis trained using a standard cross-entropy loss. We note that\ndepending on the structure of the dataset, these constraints\nmay be adjusted, or others more suitable ones can be de-\nsigned. In Fig. 6, we show the top 5 candidate sequences\nand show how they improve over the learning process.\nAction Occurrence:\nGiven a sequence S of output\nsymbols (i.e., actions), the ﬁrst constraint ensures that ev-\nery action appears once. Formally, it is implemented as\nInput \nVideo\nCNN\nFeature\nStates\nSequential \nModel\nActions\nCrack egg\nFry Egg\nServe Egg\nFlip Egg\nBest Candidate Sequence:\nFigure 5. Once the best candidate sequence is selected, it is used\nto train the model using standard backpropagation. Both the state\nsequence model as well as the FC-layers generating frame predic-\ntions (for C3) are trained.\nC1(S) = |O| −P\na∈O App(a), where App is 1 if a ap-\npears in S otherwise it is 0.\nThis computes the number of actions that are not pre-\ndicted as part of the video and is minimized when all actions\noccur. Similarly, we also penalize sequences that produce\nthe same action multiple disconnected times, as we assume\nthat each video has actions that only appear once (i.e., only\nbreak eggs once when frying an egg). We penalize multi-\nple actions by subtracting the number of disconnected times\neach action appears. This constraint is optional, but we in-\nclude it as it is a property of instructional videos that can be\nleveraged.\nModeling Action Length: The constraint ensuring each\natomic action has a similar duration across different videos\ncan be implemented in several different ways. The simplest\napproach is to compute the difference in length compared\nto the average action length in the video (exact eq. in ap-\npendix).\nAnother way to model length is by considering the du-\nration of an action to be drawn from a distribution (e.g.,\nPoisson or Gaussian).\nC2(S) =\nX\na∈O\n(1 −p(L(a, S))),\n(2)\nwhere L(a, S) computes the length of action a in se-\nquence S and p(x) =\nλx exp (−λ)\nx!\nif Poisson or p(k) =\n1\nσ\n√\n2πe−(x−µ)2/ 2σ2 if Gaussian. Since we are minimizing\nthe overall cost function, we use 1 −p(x) so that it is mini-\nmized when the probability is maximal.\nThe Poisson and Gaussian distributions have parameters:\nλ or µ, σ. These parameters control the expected length of\nthe actions in videos. The parameters can be set statically\nor learned for each action. In the simplest case, we set the\nparameters to be\nT\n|O|, i.e., the length of each action is the\ndetermined by splitting the video equally into actions and\nσ = 1. In Section 3.4, we detail a method to learn the\naction length.\nModeling Action Probability: The third constraint is\nimplemented using the separate classiﬁcation layer of the\nnetwork p(a|f), which gives the probability of the frame\nbeing classiﬁed as action a. Formally, C3(S) = PT\nt=1(1 −\np(at|ft)), which is the probability that the given frame be-\nlongs to the selected action. This constraint is separate from\nthe sequential model and captures independent appearance\nbased probabilities. We note that at and pt are very similar,\nyet capture different aspects. pt is generated by a FC-layer\napplied independently to each frame, while at is generated\nby the auto-regressive model. We ﬁnd that using both al-\nlows for the creation of the action probability term, which\nis useful empirically.\nWe can then compute the rank of any sequence as\nC(S) = γ1C1(S)+γ2C2(S)+γ3C3(S), where γi weights\nthe impact of each term. In practice setting γ2 and γ3 to\n1\n|S|\nand γ1 =\n1\n|O| works well.\nLearning Actions: To choose the self-labeling, we sam-\nple K sequences, compute each cost and select the sequence\nthat minimizes the above cost function. This gives the best\nsegmentation of actions (at this iteration of labeling) based\non the deﬁned constraints.\nˆS = argminSC(S).\n(3)\nWe note that this cost function does not need to be\ndifferentiable.\nThe cost function is only used to choose\nthe self-labels. Once the labels are selected, the standard\ncross-entropy loss function with backpropagation is used to\ntrain the model. The cost function gives a strong prior for\nhow to choose the labels without any annotations, and al-\nlows unsupervised learning. Formally, given the selected\nlabels ˆS = [ˆa1, ˆa2, . . . ˆaT ] i.e., they can now serve as a\nweak ground truth at this iteration, the output of the model\nA = [a1, a2, . . . aT ], and the outputs of the classiﬁcation\nlayer P = [p1, p2, . . . pT ], where at and pt are probability\nvectors for each action, we deﬁne the loss as:\nL( ˆS, A, P) = −\nX\ni∈O\nT\nX\nt=1\nˆat,i log(at,i) + ˆat,i log(pt,i). (4)\nThis loss trains both the classiﬁcation layer as well as the\nmodel.\nWe also allow a null class to indicate that no actions are\noccurring in the given frames. This class is not used in any\nof the above constraints, i.e., it can occur wherever it wants,\nfor as long as needed and as many times as needed. We\nomit frames labeled with the null class when calculating the\ncost function and ﬁnd that the constraint encouraging each\naction to occur once eliminates the solution where only the\nnull class is chosen.\nEpoch 0\nEpoch 50\nEpoch 100\nEpoch 400\nEpoch 300\nEpoch 200\nFigure 6. Candidate sequences at different stages of training. The\nsequences shown are the top 5 ranked sequences (rows) at the\ngiven epoch. The top one is selected as supervision for the given\nstep. The colors represent the discovered action (with no labels).\n3.3. Cross-Video Matching\nThe above constraints work reasonably well for a sin-\ngle video, however when we have multiple videos with the\nsame actions, we can further improve the ranking function\nby adding a cross-video matching constraint. The motiva-\ntion for this is that while breaking an egg can be visually\ndifferent between two videos (different bowls, camera an-\ngles and background), the action, e.g., overall motion and\nobject, are the same.\nTo encourage the model to learn and use this, especially\nin videos having partially ordered sequences (e.g., break\negg, heat pan vs. heat pan, break egg), we add a cross-video\nmatching term. Given a video segment the model labeled as\nan action fa from one video, a segment ˆfa the model labeled\nas the same action from a second video, and a segment fb\nthe modeled labeled as a different action from any video,\nwe can measure the cross-video similarity using standard\nmethods, such as a triplet loss\nLT (fa, ˆfa, fb) = ||fa −ˆfa||2 −||fa −fb||2 + α,\n(5)\nor a contrastive loss\nLC(fa, ˆfa, fb) = 1\n2||fa−ˆfa||2+1\n2 max (0, α −||fa −fb||2).\n(6)\nThese two functions capture similar properties but in\nslightly different ways. The triplet loss maximizes the dif-\nference between anchor (e.g., fa) and positive ( ˆfa)/negative\n(fb) distance. While the contrastive loss maximizes the dis-\ntance between fa and fb separately from minimizing the\ndistance between fa and ˆfa. This results in slightly differ-\nent cross-video matching metrics.\nAs these functions are differentiable, we can directly add\nthis to the loss function (Eq. 4) or to the cost function (Eq.\n2) or both. By adding this to the cost function, we are ensur-\ning that the chosen labeling of the videos is most consistent\nfor feature representations. By adding it to the loss func-\ntion, we are encouraging the learned representations to be\nsimilar for the actions with the same selected labels and dif-\nferent for other actions. We analyze the effect of these in\nTable 6.\n3.4. Self-labeling Training Method\nUsing the previous components, we now describe the full\ntraining method, which follows an EM-like procedure. In\nMethod\nF1 score\nSupervised Baselines\nVGG [33], from Alayrac et al. [1]\n0.376\nI3D, Carreira et al. [5]\n0.472\nAssembleNet, Ryoo et al. [28]\n0.558\nWeakly-supervised\nCTC, Huang et al. [13] +AssembleNet [28]\n0.312\nECTC, Huang et al. [13] +AssembleNet [28]\n0.334\nUnsupervised\nUniform Sampling\n0.187\nAlayrac et al. [1]\n0.238\nKukleva et al [18]\n0.283\nJointSeqFL, Elhamifar et al. [7]\n0.373\nOurs\n0.457\nTable 1. Results on the NIV dataset\nthe ﬁrst step, we ﬁnd the optimal set of action self-labels\ngiven the current model parameters and the ranking func-\ntion. In the second step, we optimize the model parameters\n(and optionally some ranking function parameters) for the\nselected self-labeling (Figure 5). After taking both steps,\nwe have completed one iteration. Following standard neu-\nral network training, we do each step for a mini-batch of 32\nsamples. The model is trained for 500 epochs. Due to the\niterative update of the labels at each step, we observe that\nthis method requires more epochs than supervised learning.\nWe use gradient descent with momentum to optimize the\nnetwork parameters with a learning rate set to 0.1 following\na cosine decay schedule.\nLearning action length: As an optional training phase,\nwe update some parameters of the ranking function. The\nparticular parameters to learn are those determining the\nlength of each action, since some atomic actions will be\nlonger than others and we often do not know the actual\nlength of the action. To do this, we modify the length model\nso that it has a λa or µa, σa to represent the length of each\naction a. To estimate these values, after the backpropaga-\ntion of the gradients, we run the model in inference mode\nto obtain a segmentation of the video. For each action, we\nthen compute its average length (and optionally variance)\nwhich we can use to update λa or µa, σa.\nSegmenting a video at inference: CNN features are\ncomputed for each frame and the learned model is applied\non those features. During rule selection, we greedily se-\nlect the most probable rule. Future work can improve this\nby considering multiple possible sequences (e.g., following\nthe Viterbi algorithm).\nMethod\nMoF\nSupervised Baselines\nVGG [33], from Alayrac et al. [1]\n60.8\nI3D, Carreira et al. [5]\n72.8\nAssembleNet, Ryoo et al. [28]\n77.6\nWeakly-supervised\nCTC, Huang et al. [13]\n11.9\nHTK, Kuehne et al. [17]\n24.7\nHMM + RNN, Richard et al. [25]\n45.5\nNN-Viterbi, Richard et al. [27]\n49.4\nOurs, weakly supervised 1\n53.7\nUnsupervised\nKukleva et al [18]\n30.2\nOurs\n39.7\nTable 2. Results on the 50-salads dataset.\n4. Experiments\nWe evaluate our unsupervised atomic action discovery\napproach on multiple video segmentation datasets, conﬁrm-\ning that our self-generated action annotations form mean-\ningful action segments. We note that there is only a hand-\nful of methods that have attempted unsupervised activity\nsegmentation.\nThus, we also compare to several fully-\nsupervised methods and to weakly-supervised ones.\nDatasets: We compare results on the 50-salads dataset\n[36], which contains 50 videos of people making salads\n(i.e., a single task). The videos contain the same set of\nactions (e.g., cut lettuce, cut tomato, etc), but the order-\ning of actions is different in each video. We compare on\nNarrated Instructional Videos (NIV) dataset [1], which\ncontains 5 different tasks (CPR, changing a tire, making\ncoffee, jumping a car, re-potting a plant).\nThese videos\nhave a more structured order. Finally, we use the Break-\nfast dataset [16] which contains videos of people making\nbreakfast dishes from various camera angles and environ-\nments. We chose these datasets as they cover a wide variety\nof approaches focused on unsupervised, weakly-supervised,\nand fully-supervised action segmentation, allowing a com-\nparison to them. Furthermore, these datasets, unlike other\nrelated ones, provide appropriate annotations for the evalu-\nation of atomic action learning.\nEvaluation Metrics: We follow all previously established\nprotocols for evaluation in each dataset. We ﬁrst use the\nHungarian algorithm to map the predicted action symbols\nto action classes in the ground truth. Since different met-\nrics are used for different datasets we report the previously\nadopted metrics per dataset. Speciﬁcally, for NIV, we pre-\ndict a temporal interval for each action, then compute the\n1For the weakly-supervised setting, we use activity order as supervi-\nsion, equivalent to previous works.\nMethod\nMoF\nJaccard\nSupervised Baselines\nVGG [33], from Alayrac et al. [1]\n62.8\n75.4\nI3D, Carreira et al.[5]\n67.8\n79.4\nAssembleNet, Ryoo et al. [28]\n72.5\n82.1\nWeakly-supervised\nOCDC, Huang et al. [13]\n8.9\n23.4\nECTC, Huang et al.[13]\n27.7\n-\nHMM + RNN, Richard et al. [25]\n33.3\n47.3\nUnsupervised\nSCV, Li and Todorovic [20]\n30.2\n-\nSCT, Fayyaz and Gall [8]\n30.4\n-\nSener et al [29]\n34.6\n47.1\nKukleva et al [18]\n41.8\n-\nOurs\n43.5\n54.4\nTable 3. Results on the Breakfast dataset.\nF1 score if the interval falls within a ground truth inter-\nval (following [1]). For 50-salads, we compute the mean-\nover-frames (MoF) which is the per-frame accuracy for each\nframe. For Breakfast, we report both the MoF and Jaccard\nmeasure, following previous works [13, 25, 18].\n4.1. Comparison to the state-of-the-art\nIn Tables 1, 2, 3, we compare our approach to previ-\nous state-of-the-art methods. While there are few works on\nthe fully unsupervised case, we note that our approach, to-\ngether with strong video features, provides better segmen-\ntation results than previous unsupervised and even weakly-\nsupervised methods (JointSeqFL [7] uses optical ﬂow and\ndoes not provide results on 50-salads or Breakfast).\nFor full comparison we include strong supervised base-\nlines, e.g., I3D [5], and AssembleNet [28]. We also use\nimplementations of the CTC [12] and ECTC [13] meth-\nods using the AssembleNet backbone [28]. Our unsuper-\nvised approach outperforms many weakly-supervised ones\ntoo (Tables 1, 3).\nQualitative Analysis In Fig. 6, we show the generated can-\ndidate sequences at different stages of learning. It can be\nseen that initially the generated sequences are entirely ran-\ndom and over-segmented. As training progresses, the gen-\nerated sequences start to match the constraints. After 400\nepochs, the generated sequences show similar order and\nlength constraints, and better match the ground truth (as\nshown in the evaluation). Figures 8, 9 show example results\nof our method.\n4.2. Ablation experiments\nEffect of sequential models for weak-supervision. We\nconduct a set of experiments to determine the effect of\nlearning temporal information in different ways. The CTC\nloss, which bases the loss on the probability of the sequence\nMethod\nNIV\n50-Salads\nBreakfast\nSupervised\n0.558\n77.6\n72.5\nCTC\n0.312\n42.8\n38.7\nRNN + CTC\n0.388\n47.9\n42.4\nOurs + CTC\n0.480\n52.8\n45.3\nTable 4. Comparing different weakly-supervised models. All us-\ning AssembleNet features. The supervised counterpart at the top.\nCost\n50-Salads\nBrkfst\nRandomly pick candidate\n12.5\n10.8\nNo Gumbel-Softmax\n10.5\n9.7\nOccurrence (C1)\n22.4\n19.8\nLength (C2)\n19.6\n17.8\np(a|f) (C3)\n21.5\n18.8\nC1 + C2\n27.5\n25.4\nC1 + C3\n30.3\n28.4\nC2 + C3\n29.7\n27.8\nC1 + C2 + C3\n33.4\n29.8\nTable 5. Ablation with cost function terms2\nFunction\nUse\nNIV\n50-Salads\nBrkfst\nNone\nN/A\n0.420\n33.4\n31.7\ntriplet\ncost\n0.485\n37.8\n37.8\ncontr.\ncost\n0.478\n37.9\n36.4\ntriplet\nloss\n0.492\n38.4\n37.5\ncontr.\nloss\n0.478\n39.2\n38.4\ntriplet\nboth\n0.442\n35.7\n36.9\ncontr.\nboth\n0.448\n36.2\n35.2\nTable 6. Cross video matching3\nMethod\nLearned\nNIV\n50-Salads\nBrkfst\nAvg.\nno\n0.420\n33.4\n31.7\nGaussian\nno\n0.418\n32.8\n34.5\nPoisson\nno\n0.435\n33.6\n32.8\nGaussian\nyes\n0.432\n35.7\n36.5\nPoisson\nyes\n0.447\n37.9\n37.9\nTable 7. Different length models, including learned and ﬁxed\nlengths per action.3\noccurring [12], can be applied directly on per-frame fea-\ntures, without any underlying RNN or temporal model. In\nTable 4, we compare the effect of using the CTC loss with\nper-frame CNN features, an RNN, and our model. We note\nthat our model is an RNN with a restricted, discrete set of\nstates, but is able to stochastically select states. We ﬁnd that\nadding temporal modeling to the CNN features is beneﬁcial,\nand for these datasets, our model further improves perfor-\nmance. These experiments all use order-of-activity labels,\nand are weakly-supervised. They also all use AssembleNet.\nEffects of the cost function constraints. To determine how\neach cost function impacts the resulting performance, we\ncompare various combinations of the terms. The results are\nMethod\nchng\nCPR\nrepot\nmake\njump\nAvg.\ntire\nplant\ncoffee\ncar\nAlaryac et al. [1]\n0.41\n0.32\n0.18\n0.20\n0.08\n0.238\nKukleva et al. [18]\n-\n-\n-\n-\n-\n0.283\nOurs VGG\n0.53\n0.46\n0.29\n0.35\n0.25\n0.376\nOurs AssembleNet\n0.63\n0.54\n0.381\n0.42\n0.315\n0.457\nTable 8. Comparison on the NIV dataset of the proposed approach\non VGG and AssembleNet features.\nshown in Table 5. We ﬁnd that each term is important to the\nself-labeling of the videos2. Generating better self-labels\nimproves model performance, and each component is bene-\nﬁcial to the selection process. Intuitively, this makes sense,\nas the terms were picked based on prior knowledge about\ninstructional videos. We also compare to random selection\nof the candidate labeling and a version without using the\nGumbel-Softmax. Both alternatives perform poorly, con-\nﬁrming the beneﬁt of the proposed approach.\nMethods for cross-video matching. In Table 6, we com-\npare the results for the different methods of cross-video\nmatching on the 50-salads dataset. We compare both the\ntriplet loss (Eq. 5) and the constrastive loss (Eq. 6) using\nthem as part of the cost function, training loss function or\nboth. We ﬁnd that using the contrastive as part of the train-\ning loss performs the best, as this further encourages the\nlearned representation to match the chosen labels.\nMethods for length modeling. In Table 7, we compare the\ndifferent methods to model the length of each action. We\nﬁnd that learning the length of each action (Section 3.4) is\nmost beneﬁcial.\nVarying the number of actions. As O is a hyper-parameter\ncontrolling the number of actions to segment the video into,\nwe conduct experiments on NIV varying the number of ac-\ntions/size of O to evaluate the effect this hyper-parameter\nhas. The results are shown in Figure 7. Overall, we ﬁnd that\nthe model is not overly-sensitive to this hyper-parameter,\nbut it does have some impact on the performance due to the\nfact that each action must appear at least once in the video.\nFeature comparisons. As our work uses AssembleNet [28]\nfeatures, in Table 8 we compare the proposed approach to\nprevious ones using both VGG and AssembleNet features.\nAs shown, even using VGG features, our approach outper-\nforms previous methods.\n4.3. Using oracles\nTo better understand where the model succeeds and fails,\nwe compare effects of adding different oracle information.\nWe compare our model using 5 different oracles. (1) Ob-\nject oracle that tells us the object of interest for each frame.\nFor example, if cutting cucumber is the action, cucumber is\n2These ablation methods do not use our full cross-video matching or\naction duration learning, thus the performances are slightly lower than the\nour best results.\n3To isolate the effect, Table 6 uses the length model without learning,\nand Table 7 uses no cross video matching.\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nChange Tire (11)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nCPR (7)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nRepot Plant (7)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nMake Coffee (10)\n0.0\n0.2\n0.4\n0.6\n7\n9\n11\n13\nOurs\nAlayrac et al.\nUniform\nSener et al.\nJump Car (12)\nFigure 7. F1 value for varying the number of actions used in the model, compared to prior work. The number in parenthesis indicates the\nground-truth number of actions for each activity. Full results are in the sup. materials.\nBreak On\nGet things out\nStart Loose \nJack Up\nUnscrew\nPut on Wheel\nTighten Wheel\nPut Away\n1\n2\n3\n4\n5\n6\n7\n8\nGet things \nout\nStart Loose \nJack Up\nUnscrew\nPut On\nTighten Wheel\nJack \nDown\nScrew Wheel\n1\n2\n3\n4\n5\n8\n6\n7\nFigure 8. Two example videos from the ‘change tire’ activity. The\nground truth is shown in grey, the model’s top rank segmentation\nis shown in colors. NIV dataset.\nGet things \nout\nStart Loose \nJack Up\nUnscrew\nPut On\nTighten Wheel\nJack \nDown\nScrew Wheel\n1\n2\n3\n4\n5\n8\n6\n7\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\n4\n5\n8\n6\n7\n9\n1\n2\n3\n4\n5\n8\n6\n7\n9\n10\n11\nFigure 9. Example segmentation of the ‘change tire’ activity vary-\ning the number of actions from 5 to 11. The segmentations gener-\nally match, even when the number of actions does not match the\nground truth number.\npeel cucumber\ncut cucumber\ncucumber to bowl\ncut tomato\ntomato to bowl\ncut cheese\ncheese to bowl\ncut lettuce\nlettuce to bowl\nmix\nadd oil\nadd vinegar\nadd salt\nadd pepper\nmix dressing\nput on plate\nadd dressing\nPredicted label\npeel cucumber\ncut cucumber\ncucumber to bowl\ncut tomato\ntomato to bowl\ncut cheese\ncheese to bowl\ncut lettuce\nlettuce to bowl\nmix\nadd oil\nadd vinegar\nadd salt\nadd pepper\nmix dressing\nput on plate\nadd dressing\nTrue label\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 10. Confusion matrix for 50 salads dataset. Most of the\nconfusion is around the objects, e.g., ‘cut lettuce’ vs. ‘cut tomato.’\nthe given object; for frying an egg, egg is given. (2) Ac-\ntion oracle (e.g., cut, peel, mix, etc.). (3) High-level action\noracle, i.e., a grouping of several relation actions. For ex-\nample: ‘prepare salad’ which contains cut, peel, add to bowl\nactions. (4) Time oracle which gives the start and end times\nof each action, but not the action itself. (5) Order oracle:\ngives the previous and next action, but not the current ac-\ntion (only usable for classifying the current frame).\nThe results are shown in Table 9 for the 50-salads\ndataset. We ﬁnd that the model performs quite well in learn-\ning the temporal ordering and structure of the data, as the\nperformance only slightly increases when including addi-\ntional temporal information. Adding perfect object infor-\nOracle\nAccuracy %\nNone\n33.4\nAction\n39.8\nHigh-level action\n34.8\nTemporal\n36.8\nOrdering\n39.4\nObject\n48.5\nTable 9. Comparison of different oracles on 50-salads\n# labeled\n0\n1\n2\n3\n4\n5\n50\nAcc. %\n33.4\n42.8\n44.3\n45.2\n46.6\n47.1\n77.6\nTable 10. Classiﬁcation accuracy for different number of labeled\nexamples (50 labels means all examples are labeled). 50-salads.\nmation greatly boosts performance, suggesting that the cur-\nrent model struggles to learn objects.\nFigure 10 shows the confusion matrix for the 50 Salads\ndataset. As seen, actions are well separated from one an-\nother. There is confusion among objects (top left portion),\ne.g., ‘cut cucumber’, ‘cut tomato’ and ‘cut lettuce’ are con-\nfused, but actions, e.g., ‘cut’ and ‘peel’ are well separated.\nThis conﬁrms actions are well understood by the model.\nWeak Labeling Oracle. Here we have an oracle that gives\nN true examples and the model ‘mines’ the action from\nthe other videos. This allows further analysis of the im-\npact of unsupervised learning.\nWe conduct a set of ex-\nperiments comparing the unsupervised approach against N\nfully-labeled videos given. N videos are selected at random\nfor supervised learning. The we perform the iterative, un-\nsupervised training method for the remaining videos. The\nresults are averaged over 10 different runs, each with a dif-\nferent set of labeled videos. Table 10 shows the results. We\nﬁnd that adding one true video greatly boosts performance\n(+9%), and each additional video adds only about 1% to\nfully supervised performance, showing the strong beneﬁt of\nthe self-labeling approach.\n5. Conclusions and future work\nWe present a novel approach for unsupervised action\nsegmentation for instructional videos. Based on a stochastic\nautoregressive model and ranking function, the algorithm\nis able to learn to self-label and segment actions without\nsupervision. Our approach outperforms the unsupervised\nmethods, in some cases weakly supervised too.\nReferences\n[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,\nJosef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu-\npervised learning from narrated instruction videos. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 4575–4583, 2016. 1, 2,\n5, 6, 7\n[2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing mo-\nments in video with natural language. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV),\npages 5803–5812, 2017. 2\n[3] Piotr Bojanowski, R´emi Lajugie, Francis Bach, Ivan Laptev,\nJean Ponce, Cordelia Schmid, and Josef Sivic. Weakly su-\npervised action labeling in videos under ordering constraints.\nIn Proceedings of European Conference on Computer Vision\n(ECCV), pages 628–643. Springer, 2014. 2\n[4] Andrew D Brown and Geoffrey E Hinton. Products of hidden\nmarkov models. In AISTATS. Citeseer, 2001. 3\n[5] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 2, 3, 5, 6, 11\n[6] Srijan Das,\nRui Dai,\nMichal Koperski,\nLuca Minci-\nullo, Lorenzo Garattoni, Francois Bremond, and Gianpiero\nFrancesca. Toyota smarthome: Real-world activities of daily\nliving. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), 2019. 1\n[7] Ehsan Elhamifar and Zwe Naing. Unsupervised procedure\nlearning via joint dynamic summarization. 2019. 2, 5, 6\n[8] Mohsen Fayyaz and Jurgen Gall. Sct: Set constrained tem-\nporal transformer for set supervised action segmentation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020. 2, 6\n[9] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. arXiv\npreprint arXiv:1812.03982, 2018. 2\n[10] Chuang Gan, Chen Sun, Lixin Duan, and Boqing Gong.\nWebly-supervised video recognition by mutually voting for\nrelevant web images and web video frames. In Proceedings\nof European Conference on Computer Vision (ECCV), pages\n849–866. Springer, 2016. 2\n[11] Chuang Gan, Ting Yao, Kuiyuan Yang, Yi Yang, and Tao\nMei. You lead, we exceed: Labor-free video concept learn-\ning by jointly exploiting web videos and images. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 923–932, 2016. 2\n[12] Alex Graves, Santiago Fern´andez, Faustino Gomez, and\nJ¨urgen Schmidhuber. Connectionist temporal classiﬁcation:\nlabelling unsegmented sequence data with recurrent neural\nnetworks. In International Conference on Machine Learn-\ning (ICML), pages 369–376, 2006. 6, 7\n[13] De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Con-\nnectionist temporal modeling for weakly supervised action\nlabeling. In Proceedings of European Conference on Com-\nputer Vision (ECCV), 2016. 1, 2, 5, 6\n[14] Eric Jang, Shixiang Gu, and Ben Poole. Categorical repa-\nrameterization with gumbel-softmax. In International Con-\nference on Learning Representations, 2017. 3\n[15] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,\nM. Shah, and R. Sukthankar. THUMOS challenge: Action\nrecognition with a large number of classes, 2014. 1\n[16] Hilde Kuehne, Ali Arslan, and Thomas Serre.\nThe lan-\nguage of actions: Recovering the syntax and semantics of\ngoal-directed human activities. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 780–787, 2014. 2, 6\n[17] Hilde Kuehne, Alexander Richard, and Juergen Gall. Weakly\nsupervised learning of actions from transcripts. Computer\nVision and Image Understanding (CVIU), 2017. 6\n[18] Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen\nGall.\nUnsupervised learning of action classes with con-\ntinuous temporal embedding. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 12066–12074, 2019. 1, 2, 3, 5, 6, 7\n[19] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Ben-\njamin Rozenfeld.\nLearning realistic human actions from\nmovies. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 1–8.\nIEEE, 2008. 2\n[20] Jun Li and Sinisa Todorovic.\nSet-constrained viterbi for\nset-supervised action segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2020. 2, 6\n[21] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The\nconcrete distribution: A continuous relaxation of discrete\nrandom variables. In International Conference on Learning\nRepresentations, 2017. 3\n[22] Marcin Marszalek, Ivan Laptev, and Cordelia Schmid. Ac-\ntions in context.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2929–2936. IEEE, 2009. 2\n[23] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowto100m: Learning a text-video embedding by watching\nhundred million narrated video clips. In Proceedings of the\nIEEE International Conference on Computer Vision, pages\n2630–2640, 2019. 2\n[24] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo. Dif-\nferentiable grammars for videos. In Proceedings of AAAI\nConference on Artiﬁcial Intelligence (AAAI), 2020. 3\n[25] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly\nsupervised action learning with rnn based ﬁne-to-coarse\nmodeling. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2017. 1, 6\n[26] Alexander Richard, Hilde Kuehne, and Juergen Gall. Ac-\ntion sets: Weakly supervised action segmentation without or-\ndering constraints. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5987–5996, 2018. 2\n[27] Alexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juergen\nGall. NeuralNetwork-Viterbi: A framework for weakly su-\npervised video learning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 7386–7395, 2018. 6\n[28] Michael S. Ryoo, AJ Piergiovanni, Mingxing Tan, and\nAnelia Angelova. Assemblenet: Searching for multi-stream\nneural connectivity in video architectures. In International\nConference on Learning Representations, 2020. 2, 3, 5, 6, 7,\n11\n[29] Fadime Sener and Angela Yao. Unsupervised learning and\nsegmentation of complex activities from video. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8368–8376, 2018. 2, 6\n[30] Ozan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh\nSaxena.\nUnsupervised semantic parsing of video collec-\ntions. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 4480–4488, 2015. 2\n[31] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki\nMiyazawa, and Shih-Fu Chang.\nCdc: Convolutional-de-\nconvolutional networks for precise temporal action localiza-\ntion in untrimmed videos. arXiv preprint arXiv:1703.01515,\n2017. 2\n[32] Gunnar A. Sigurdsson, G¨ul Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta.\nHollywood in\nhomes: Crowdsourcing data collection for activity under-\nstanding. In Proceedings of European Conference on Com-\nputer Vision (ECCV), 2016. 1\n[33] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 5, 6, 11\n[34] K. Soomro, A. Roshan Zamir, and M. Shah. UCF101: A\ndataset of 101 human actions classes from videos in the wild.\nIn CRCV-TR-12-01, 2012. 1\n[35] Khurram Soomro and Mubarak Shah. Unsupervised action\ndiscovery and localization in videos. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV),\n2017. 2\n[36] Sebastian Stein and Stephen J McKenna. Combining em-\nbedded accelerometers with computer vision for recognizing\nfood preparation activities. In Proceedings of the 2013 ACM\ninternational joint conference on Pervasive and ubiquitous\ncomputing, pages 729–738, 2013. 2, 6\n[37] Chen Sun, Sanketh Shetty, Rahul Sukthankar, and Ram\nNevatia.\nTemporal localization of ﬁne-grained actions in\nvideos by domain transfer from web images. In Proceedings\nof the 23rd ACM international conference on Multimedia,\npages 371–380, 2015. 2\n[38] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nDanyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin:\nA large-scale dataset for comprehensive instructional video\nanalysis. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 1207–\n1216, 2019. 2\n[39] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 6450–6459, 2018. 2\n[40] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy.\nRethinking spatiotemporal feature learn-\ning: Speed-accuracy trade-offs in video classiﬁcation.\nIn\nProceedings of European Conference on Computer Vision\n(ECCV), pages 305–321, 2018. 2\n[41] Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo An-\ndriluka, Greg Mori, and Li Fei-Fei. Every moment counts:\nDense detailed labeling of actions in complex videos. In-\nternational Journal of Computer Vision (IJCV), pages 1–15,\n2015. 1\n[42] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In Proceedings of AAAI Conference on Artiﬁcial In-\ntelligence (AAAI), 2018. 2\n[43] Hongyuan Zhu, Romain Vial, and Shijian Lu. Tornado: A\nspatio-temporal convolutional regression network for video\naction proposal.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5813–5821, 2017. 2\n[44] D. Zhukov, J.-B. Alayrac, R. G. Cinbis, D. Fouhey, and.\nLaptev, and andJ. Sivic. Cross-task weakly supervised learn-\ning from instructional videos. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 2\nA. Implementation Details\nThe model is implemented in PyTorch. The pretrained\nmodels are on Kinetics-600 with overlapping classes re-\nmoved, as is standard practice for unsupervised approaches\n(see Section 3 below for all removed classes). As base net-\nworks, which are needed to obtain initial features from the\nvideos, we use and compare VGG [33], I3D [5] and As-\nsembleNet [28]. These cover a wide range of networks pre-\nviously used for video understanding (e.g. I3D), for un-\nsupervised video segmentation, where VGG is often used,\nand current state-of-the-art models (AssembleNet).\nOur\nmain model uses the AssembleNet backbone, which con-\ntains ResNet blocks of interleaved spatial and 1d temporal\nconvolutions. It is equivalent in the number of parameters\nto a ResNet-50 (2+1)D network.\nWe used all three constraints (C1, C2, C3) with the\nweights set as described in Section 3.2 of the main paper.\nWe used cross-video matching in the loss function with the\ntriplet loss formulation. We used the learned Poisson ver-\nsion of length modeling. These corresponded to the best\nvalues in each of Tables 5-7 of the paper.\nDuring evaluation, we use a greedy rule selection method\nto pick the rule at each time step, so only one sequence is\ngenerate for each sample. We note that other methods are\npossible, such as generating multiple sequences and picking\nthe best one. Using the greedy method, it is possible that it\ngenerates missing or repeated actions. However, since the\ncost function is not used during evaluation, we observe that\nthis has minimal impact on the model.\nInput Features.\nIn the experiments, as mentioned, we\nuse VGG, I3D and AssembleNet as initial features. VGG\nand I3D use RGB inputs, while AssembleNet (by network\ndesign) uses RGB and optical ﬂow as input. The optical\nﬂow is computed over RGB inputs on the ﬂy. The CTC\nand ECTC methods, which are also comprated in the paper,\nuse IDT features [?] features on the 50-salads and Break-\nfast datasets and AssembleNet on the NIV dataset, unless\notherwise noted.\nSpeciﬁc Model Details We provide speciﬁc details about\nthe model size. For the various experiments, |H|, the size\nof the set of states, was set to 50 for all experiments and\ndatasets. Changing this value did not signiﬁcantly impact\nperformance as long as it was greater than the expected\nnumber of outputs |O|. R, the set of transition rules, was\nset to 3 per-state, a total of 150, which is again ﬁxed for all\nexperiments. We use this strategy to be consistent across\nexperiments; this can be further tuned for speciﬁc dataset\nto improve performance. We set M = 32, we note that we\nfound the model was not sensitive to this setting, provided\nit was larger than 8.\nB. Action length equation\nComputing the average action length cost function can\nbe done as:\nC2(S) =\nv\nu\nu\nt 1\n|O|\nX\na∈O\n\u0012\nL(a, S) −1\n|O|\n\u0000P\ni∈O L(i, S)\n\u0001\u00132\n,\n(7)\nwhere L(a, S) computes the length (i.e., number of frames)\nlabeled as action a in sequence S.\nThis function will\nbe minimized when all actions occur for equal number of\nframes.\nC. Excluded Kinetics classes\nWe removed some classes from the Kinetics dataset,\nused to pretrain the models to obtain the initial features, in\norder to avoid overlap with the actions we are trying to dis-\ncover. We also provide a list of some similar classes we left\nin Kinetics.\n1. cooking egg\n2. scrambling eggs\n3. preparing salad\n4. making a sandwich\nSimilar actions left in:\n1. peeling apples/potatoes (similar to 50-salads peeling\ncucumber)\n2. cutting apple/orange/watermelon/pineapple (similar to\n50-salads cutting cucumber/tomaoto/cheese)\n3. changing wheel (similar to NIV changing a tire)\n4. planting trees (similar to NIV repotting plant)\n5. frying vegetables (similar to Breakfast frying an egg)\nD. Supplemental Results\nIn Table D, we report the quantitative results correspond-\ning to Figure 7 in the main paper, for future reference.\n# Steps\nChange Tire (11)\nCPR (7)\nRepot Plant (7)\nMake Coffee (10)\nJump Car (12)\nGT Steps\n0.60\n0.52\n0.32\n0.37\n0.29\n5\n0.45\n0.48\n0.25\n0.25\n0.12\n7\n0.48\n0.52\n0.32\n0.30\n0.18\n9\n0.54\n0.52\n0.32\n0.35\n0.22\n11\n0.60\n0.50\n0.30\n0.34\n0.27\n13\n0.58\n0.48\n0.28\n0.35\n0.26\nTable 11. Varying the number of steps used in the model. The\nnumber in parenthesis indicates the ground-truth number of steps\nfor each activity. NIV Dataset.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-06-28",
  "updated": "2021-06-28"
}