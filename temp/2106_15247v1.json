{
  "id": "http://arxiv.org/abs/2106.15247v1",
  "title": "Unsupervised Technique To Conversational Machine Reading",
  "authors": [
    "Peter Ochieng",
    "Dennis Mugambi"
  ],
  "abstract": "Conversational machine reading (CMR) tools have seen a rapid progress in the\nrecent past. The current existing tools rely on the supervised learning\ntechnique which require labeled dataset for their training. The supervised\ntechnique necessitates that for every new rule text, a manually labeled dataset\nmust be created. This is tedious and error prone. This paper introduces and\ndemonstrates how unsupervised learning technique can be applied in the\ndevelopment of CMR. Specifically, we demonstrate how unsupervised learning can\nbe used in rule extraction and entailment modules of CMR. Compared to the\ncurrent best CMR tool, our developed framework reports 3.3% improvement in\nmicro averaged accuracy and 1.4 % improvement in macro averaged accuracy.",
  "text": "UNSUPERVISED TECHNIQUE TO CONVERSATIONAL MACHINE\nREADING\nA PREPRINT\nPeter Ochieng∗\nDepartment of Computing and Informatics\nTaita Taveta University\nP.O. Box 635 – 80300 Voi,Kenya.\npeter.ochieng@ttu.ac.ke\nDennis Kaburu\nDepartment of Computing\nJomo Kenyatta University of Agriculture and Technology\nP.O. Box 62000 CITY SQUARE,Nairobi ,Kenya.\ndennis.kaburu@jkuat.ac.ke\nJune 30, 2021\nABSTRACT\nConversational machine reading (CMR) tools have seen a rapid progress in the recent past. The\ncurrent existing tools rely on the supervised learning technique which require labeled dataset for\ntheir training. The supervised technique necessitates that for every new rule text, a manually labeled\ndataset must be created. This is tedious and error prone. This paper introduces and demonstrates\nhow unsupervised learning technique can be applied in the development of CMR. Speciﬁcally, we\ndemonstrate how unsupervised learning can be used in rule extraction and entailment modules of\nCMR. Compared to the current best CMR tool, our developed framework reports 3.3% improvement\nin micro averaged accuracy and 1.4 % improvement in macro averaged accuracy.\nKeywords Chatbot · Conversational · NLP · Natural Language Processing · Dialog · Unsupervised Learning\n1\nIntroduction\nConversational machine reading (CMR) tools allow users to give a description of their scenario and pose a question to\nthem [1] [2]. The CMR tool then processes the rule text in relation to the user scenario and question and either picks an\nappropriate answer from the set of possible answers A = {Yes, No, Irrelevant} or chooses to seek futher clariﬁcation\nbefore giving an answer from the set A [3]. A number of systems [2] [3] [4] [5] have been developed with a goal\nto improve the precision of the answers given to the user. However, all the existing tools apply supervised learning\ntechnique which require manually labeled dataset. For every new rule text, the supervised techniques will require that a\nlabeled dataset be created. The task of manually labeling dataset is tedious and error prone [6]. Moreover, it may not\ngenerate enough dataset for proper training of the developed model. Due to limited training dataset, the model runs the\nrisk of performing over ﬁtting. Unsupervised learning has shown remarkable success in other ﬁelds such as machine\ntranslation [7]. They have attained this success using no labeled training data. Motivated by this, we also introduce\nunsupervised learning technique as part of the CMR tool.\nFurther, the existing systems take the view that they are interacting with a knowledgeable user who has some basic\nidea of the subject he or she wants to inquire about. Take an example of a farmer keeping chicken who has observed\na number of symptoms in his or her chicken. He or she will describe a scenario as shown on the left side of Fig 2.\nIt takes a user who has some prior knowledge about chicken diseases to pose question 2 (Q2) to the CMR system\nwhich processes the rule text of chicken diseases ( a snippet is shown in ﬁgure 1). However, for a novice in chicken\nfarming question 1 (Q1) is the most likely. Therefore, there is a need to develop a CMR system that is able to handle\nboth speciﬁc questions such as Q2 and general question such Q1. This work proposes a new CMR system that makes\nthe following key contribution:\n∗\narXiv:2106.15247v1  [cs.CL]  29 Jun 2021\nA PREPRINT\nFigure 1: Sample rule text\nFigure 2: User Scenario and possible questions\n1. We demonstrate how unsupervised learning technique can be exploited for both rule extraction and entailment\nmodules in the CMR system.\n2. We develop A CMR tool that can handle both speciﬁc and general questions\nExperiments conducted on the ShARC dataset [8] demonstrate that the proposed approach provides a state of the art\nresults.\n2\nRelated Work\nCMR systems are always built as an aggregation of different modules with each module exploiting a given technology\n[1] [2]. In this section we focus the discussion on the techniques the existing CMR tools exploit to implement the two\nkey modules of CMR i.e the rule extraction and rule entailment modules. Rule extraction module implements techniques\nto extract rules {r1, r2, · · · , rn} of a given subject from the rule text. Given the rule set {r1, r2, · · · rn} extracted from\na rule text, the rule entailment module seeks to check whether a given rule ri is entailed in the conversation history.\nTo extract rules from the rule text, work in [4] ﬁrst extracts elementary discourse units (EDUs). The extracted EDUs are\nthen exploited to construct an explicit discourse graph. To establish a link between the rule text and user scenario, a\nuser scenario representation is fed into the explicit graph as global vertex. Further, a second implicit discourse graph is\ndesigned for extracting the latent salient interactions among rule texts. The two graphs are then exploited for making\ndecisions. In order to process the rule text and to extract rules from it, [3] proposes to segment the rule text into\nelementary discourse units (EDUs) using a pretrained discourse segmentation model proposed by [5]. Each EDU is\nthen treated as a condition of the rule text. Similar to [3], the work in [9] also uses discourse segmentation to extract\nrules in the rule text. Work in [2] ﬁrst uses Bidirectional Encoder Representations from Transformers (BERT) [10] to\nencode the text in rule text, user scenario, user inquiry and system inquiry. It then uses attention based heuristics to\nextract rules that exist in the rule text.\nGiven a set of rules and a sequence of user-provided information, [3] utilizes the transformer encoder [11] to predict the\nentailment states for all the rules. The transformer outputs whether a rule is an entailment, a contradicion or a neutral.\nIn [4], once all the EDUs have been extracted, they train a model via a cross entropy loss to perform a multi-class\nclassiﬁcation with regards to entailment of a given EDU. To check for entailment, [9] processed the ShARC dataset to\n2\nA PREPRINT\ngenerate a training dataset where each EDU is linked with its most similar dialog history. They then manually label the\nlinked pair using the tags \"Entailment\" if the answer for the mentioned follow-up question is a Yes , Contradiction\" if\nthe answer is a No or \"Neural\" if the EDUs is not matched to any follow-up question. A model is the trained using a\nsupervised training technique to recognise the three classes. The work in [2] establishes rule entailment by computing a\nsimilarity score that exploits the number of shared tokens between the dialog history and the extracted rule.\n3\nUnsupervised based Conversational Machine Reading Tool\n3.1\nRule Extraction\nFigure 3: The overall structure of the proposed Unsupervised based Conversational Machine Reading Tool (UCMRT)\nIn a rule text, a given set of span of sentences contain a set of latent rules linked to a given topic or subject. This module\nﬁrst extracts a span of sentences in the rule text that address a given topic or subject. Then from this set of sentences\nthe module extracts the latent rules that are related to the topic (subject). The module outputs a number of rule sets\nwhere each set contains rules that relate to a given subject or topic. As an initial step, the module encodes sentences in a\ncontext aware manner. To do this, we exploit BERT which encodes the sentences in the rule text (RT), user scenario\n(US), user question (UQ), for each ith turn in the conversation, the system’s inquiry is concatenated with the user\nresponse to form a single inquiry response(IR) sentence. The sentences are then structured to construct BERT input\nsequence as [ [CLS] Sentence1 [SEP] Sentence2 ]. The input is then fed into BERT which ﬁrst tokenizes the words in a\nsentence using WordPiece tokenizer. The tokenized words are then embedded with both their positional embeddings\nand segmentation embeddings. These embeddings are subsequently encoded via a transformer network. The output of\nBERT encoder RH is a vector representing an encoding of a sentence Si.\nFor each sentence Si, we seek to establish a set of related sentences Srelativesi such that ∀Sj ∈Srelativesi, |j −i| ≥1\nwhere i and j are position of a sentence in the rule text. To create the set Srelativesi of Si, we use dissimilarity score\nDisScore(Si, Si+1) = −sim(Si, Si+1)\n(1)\n3\nA PREPRINT\nwhere i = 1, 2, · · · , L−1 where L is the number of sentences in the rule set. Intuitively, DisScore(Si, Si+1) evaluates\nthe conﬁdence that the next subsequent sentence Si+1 addresses a different topic (subject) from the current sentence\nSi. Thus sentences positions with high dissimilarity values is a signal of subject change within the rule text, and are\nconsidered as candidates for subject change boundary. To extract the set of all Srelativesi within a rule text, we apply a\npeak detection algorithm over the dissimilarity values, DisScore(Si, Si+1). The DisScore(Si, Si+1) for which the\nscore exceeds a peak set threshold ϑ are predicted as boundaries.\n3.1.1\nExtracting latent rules within a subject.\nFor a given set Srelativesi that contains sentences addressing a given subject, the latent rules r1, r2, · · · , rt are\nestablished. To do this, an undirected graph G is created with vertex and edge set V(G) and E(G) respectively.\nEach Sj ∈Srelativesi is a vertex vj ∈V (G). Each edge ei ∈E(G) is weighted using a weighting function w\nw : V (G) × V (G) 7→R+ using the similarity score\nSimScore(Si, Sj) = sim(Si, Sj)\n(2)\nFrom G, we extract a weighted adjacent matrix W = wij where i, j = 1, 2, · · · , |Srelativesi|. The degree di of a vertex\nvi ∈V (G) is deﬁned as:\ndi =\nn\nX\nn=1\nwij\n(3)\nWe then deﬁne the degree matrix D as a diagonal matrix with degrees di, · · · , dn on the diagonal. Using D, we then\nconstruct a Laplacian matrix L.\nL = D −W\n(4)\nL has a number of properties [12] [13]. The properties of interest to this work is that the unnormalized graph Laplacian\nmatrix L, its eigenvalues and eigenvectors can be used for spectral clustering of the sentences within Srelativesi. We\nhypothesize that the different clusters of Srelativesi represent different set of rules contained within a subject. We\ntherefore use the spectral clustering shown in Algorithm 1 to extract distinct rules contained in Srelativesi.\nAlgorithm 1 Rule extraction based on spectral clustering algorithm.\n1: Input: k number of clusters to be constructed and Srelativesi\n2: Construct similarity matrix graph G as described in section 3.1.\n3: Construct the weighted adjacent matrix w for the graph G.\n4: Construct the unnormalized Laplacian matrix L as shown in equation 3.\n5: Compute the ﬁrst k eigenvalues and the corresponding eigenvectors u1, u2, · · · un.\n6: let U ∈RH×k be a matrix constituting the vectors u1, u2, · · · , uk as columns.\n7: for i = 1, · · · , n let yi ∈Rk be a vector corresponding to the ith row of U.\n8: Cluster the points yi i = 1, · · · , n in Rk with the k-means algorithm into clusters c1, · · · , ck.\n9: Clusters K1, K2 · · · Kk with K1 = {rj|yj ∈Ci}.\n10: Merge the clusters K1, K2 · · · Kk into a single set Ri containing all distinct rules ri from the clusters K1, K2 · · · Kk\nOutput: The set Ri containing all rules in Srelativesi\nThe module ﬁnally creates a universal set U = {r1, r2, · · · , rn} containing all the rules extracted in the rule text.\nFurther, a set Q = {R1, R2, · · · , Rn} is created such that Q contains all rule sets contained in the rule text extracted by\nAlgorithm 1.\n4\nRule Entailment\nThe entailment module seeks to establish whether the conversational history fully covers all the rules in set Ri ∈Q or\nsome rules are still left out. This helps the system to make a decision on whether to seek further clariﬁcation based on\nthe uncovered rules or give a deﬁnitive answer to the user. Here, a Generative Adversarial Network (GAN) model [14]\nis set up which is able to output a set of rules P given a certain span of sentences. The GAN is an unsupervised model\nthat constitutes two key parts i.e the generator G and the discriminator D, where G generates samples which are then\njudged by the D. The discriminator D is trained to classify whether samples are from a real data distribution or not. The\nobjective of the generator is to produce samples that can appear to the discriminator as data from real data distribution.\nIn this work, G takes as its input a span of L sentences representation i.e S = {S1, S2, · · · , SL}. It then maps the span\nof sentences representation to a sequence of K rules {r1, r2, · · · , rK}. The generator essentially predicts the probability\n4\nA PREPRINT\ndistribution over the universal set U for each span of sentences L and outputs a set of rules P ⊆U with the highest\nprobability. The generator’s output has a dimension of |U| in form of one hot encoding with 1 indicating that rule ri is\ncontained in the span of L sentences while 0 indicates that rule ri is not contained in the L input sentences.\nThe discriminator D on the other hand takes as an input |U| dimension one hot encoding of a set of rules Px representing\na set either from P i.e from the generator or Ri ∈Q i.e from real set of rules. The discriminator D outputs a probability\nindicating the likelihood that the sample from the real rule set. This work uses the objective as original as proposed by\n[14] [15]\nmin\nG max\nD\n= EPx∼Px[log D(Px)] −ES∼S[log(1 −D(G(S))) −βGp + θSp\n(5)\nHere, Px represents a rule set from the set Q while G(S) is the generated set of rules P produced by the generator\ngiven a a span of sentence representation S. The ﬁrst term of the objective trains the discriminator to assign high\nprobability to a set Ri ∈Q i.e a set rules that were generated by Algorithm 1. The second term is trained such that\nthe discriminator assigns low probability to a set P from the generator. The smoothness Sp is added to encourage\nthe generator to produce similar rules for adjacent sentences. The gradient penalty Gp achieves stabilization of the\ndiscriminator by penalizing the gradient norm of the discriminator with regards to the input [16].\n4.1\nDecision Module.\nThis module uses the trained GAN model to generate the most likely rule set P when a BERT representation of\nsentences in the conversation history is fed into the GAN model. The goal of this module is to establish the set Ri ∈Q\nwhere the rules P generated by GAN are mapped to as shown in equation 6.\nSim(Ri, P) = |Ri ∩P|\n(6)\nwhere i = 1, 2 · · · , size(Q).\nSim(Ri, P) basically compares the one hot encodings of P and Ri both having dimension |U|. The set Ri ∈Q where\n|Ri ∩P| that has the highest overlap between Ri and P i.e max(Sim(Ri, P)) is picked as the set where the GAN\ngenerated set P belongs to. To establish which rules have not been entailed by the conversation history, we perform a\nset difference\nsetDiff(Ri, P) = Ri \\ P\n(7)\nThere are three key options that the module can adopt based on the Sim(Ri, P). If ∀Ri ∈Q, |Sim(Ri, P)| = 0, it\nmeans that the module has judged that the conversation history does not match any rule set Ri ∈Q and it should\ngenerate \"irrelevant\" as the answer. If the max(Sim(Ri, P)) > 0 and |setDiff(Ri, P)| = 0, it means all rules of a\nset Ri ∈Q of a given subject are entailed by the conversation history and a deﬁnitive answer to the user inquiry should\nbe generated by the system. Finally, if max(Sim(Ri, P)) > 0 and |setDiff(Ri, P)| ≥1, it means some rules of a\ngiven subject are not entailed by the conversation history hence further inquiry by the system is necessary.\n5\nAnswer Generation Module\nHere we ﬁne tune BERT to identify whether two sentences are a negation of each other. Therefore given two sentences\nBERT outputs 0 representing not a negation or 1 signaling that sentence A is a negation of B. If rule set Ri is deemed\nto match P as described in section 4.1, we use the ﬁne tuned BERT to detect if negation exists between sentences that\ngenerated the paired rules. If negation exists in any pair of rules the system generates \"No\" as an answer to the user.\nIf no negation is detected in all paired rules, and |setDiff(Ri, P)| ≥1 the system invokes the question generation\nmodule to seek further clariﬁcation on the rules that that are not entailed after which the answer generation module is\ninvoked again. If no negation is detected between paired rules and |setDiff(Ri, P)| = 0 a \"Yes\" answer is generated.\n6\nQuestion Generation Module\nGiven a rule ri, this module seeks to create a natural question that seeks to clarify information related to the rule ri.\nif |Sim(Ri, P)| > 0 and |setDiff(Ri, P)| ≥1 the system needs to clarify some or all the rules in setDiff(Ri, P).\nThe number of rules in in the set |setDiff(Ri, P)| is the potential number of follow up question that the system will\ngenerate. Once a system has asked a question relating to a rule ri ∈|setDiff(Ri, P)| the rule ri is removed from\n|setDiff(Ri, P)| and the answer generation module is invoked taking into account the user’s response to the inquiry.\n6.1\nRule encoding\nFor given rule ri that the system needs to perform an inquiry on, this module utilizes the span of sentences Si that\ngenerated the rule ri according to Algorithm 1. These sentences, are then encoded by BERT as described section 1.\n5\nA PREPRINT\nHowever, BERT is now conﬁgured to return word embeddings as opposed to an embedding for the whole sentence.\nAfter BERT encoding, sentences Si that created the rule ri are now represented as tokens x = {x1, x2, · · · , xn}. The\ngoal is to to generate a question y = {y1, y2 · · · , yk} given the tokens x = {x1, x2, · · · , xn}. The task of this module\ncan be framed as ﬁnding the most likely question ¯y such that:\n¯y = argmin\ny\nP(y|x)\n(8)\nHere, P(y|x) is the conditional log-likelihood of the predicted question sequence y, given the input x. To generate the\nquestion using word level embeddings, we employ the technique proposed in [17] [18] where the next word of the\nquestion is predicted based on the input sentence and the current predicted word of the question as shown in equation 9.\nP(y|x) =\nn\nY\ni=1\nP(yt|x, yi)\n(9)\nwhere i < t\nConcretely, we utilize the Long Short-Term Memory (LSTM) network [19] to generate the question. The hidden state\nof the recurrent network at time t is computed based on the representation of previous predicted word and previous\nhidden state ht−1 as shown in equation 10. The initial hidden state h0 is initiated as the representation of the sentence\nSi generated by the BERT encoder.\nht = LSTM(yt−1, ht−1)\n(10)\nThe prediction of a word yi belonging to the question is generated based on equation 11.\nP(yt|x, y < t) = softmax(tanh(Wstanh(Wt[ht; ct]))\n(11)\nwhere Ws and Wt are parameters to be learned during training while ct is the attention encoding of the input x at time t.\n7\nExperimental Setup\n7.1\nDataset\nTo evaluate our model we use ShARC dataset [8]. We ﬁrst construct an extensive rule text where we visit every unique\nURL contained in the ShARC dataset extract all the relevant text contained in that web page then merge the text in the\ndifferent web pages into a single continuous document. Text extracted from the different websites are placed directly\nnext to each. We discard most headings contained in the web pages. Bullet points in the web page are reconstructed\ninto sentences. User scenario, user question and a concatenation of system generated inquiry( follow up questions) and\nthe related user answer are placed directly after the relevant text as sentences. To extract sentences from the rule text we\nuse spaCy2.\n7.2\nSentence Encoding and Rule Extraction\nDuring the ﬁne tuning of BERT to encode the sentences, we use the following hyper-parameters: a batch size of 32, a\nlearning rate of 5e −5. The BERT is ﬁne-tuned with 100,000 steps and a warm-up of 10,000 steps\n7.3\nGraph Construction And Partitioning\nTo construct a graph G ,we use the Gaussian similarity function in equation 12 to compute the similarity between\nsentences Si and Sj representing the edges vi and vj respectively of the graph G.\nSim(Si, Si+1) = exp(−||Si −Sj||2)/(2σ2)\n(12)\nFor graph partitioning in Algorithm 1, we varied the value of k based on the number of vertices n on the graph G.\nWe set k = log(n). We found this as optimum value that provided a compromise between recall and precision when\nextracting rules in the rule text.\n2https://spacy.io/\n6\nA PREPRINT\n7.4\nRule entailment setup\nFor the GAN model, Both the Generator and Discriminator were trained using Adam[] optimizer with β1 = 0.5 and\nβ2 = 0.98. The weight decay for the discriminator was set to be 1e −4. The generator and the discriminator were\ntrained with a learning rate of 1e −4 and 1e −5 respectively. The GAN model was trained for a total of 100,000 steps.\nThe optimizing during training is alternated between discriminator and the generator hence both the discriminator and\ngenerator is updated 50,000 times.\nFor the generator, we set up a convolutional neural network (CNN) model proposed in [20] such that the input of the\nconvolutional layer is a BERT encoder word level representation of a sentence S i.e for the words {w1, w2, · · · , wn} ∈\nS, BERT encoder generates a matrix M ∈Rn×d where d is the dimension of each word wi generated by BERT. The\nwords {w1, w2, · · · , wn} are fed into the convolutional layer. A k sized sliding window is then passed over the words.\nFor every ui = {wi, · · · , wi+k+1} ∈Rd×k where 0 ≤i ≤n −k each ui is processed by a ﬁlter of similar dimension\ni.e fj ∈Rd×k we use m ﬁlters in the convolutional layer. The output C ∈Rn×M of the convolutional layer is a matrix\nof size n × m. We then apply max-pooling across the word dimension to generate a vector P ∈Rm which is is then\nfed into ReLU non-linearity. Finally, a linear fully connected layer F ∈RU×m generates the probability distribution\nover the rules in the set U associated with the sentence S, During implementation, we experimented with different ﬁlter\nsizes and noted that k = 3 produced the best results. We used 30 ﬁlters.\nFor the discriminator, we used two convolutional layers followed by a single max pooling layer followed by another\ntwo convolutional layers followed by a max pooling layer. In all the convolutional layers we used 30 convolutional\nﬁlters with a ﬁlter size of 5. A dropout of 0.1 is used after the second max pooling function. The discriminator takes as\nits input a vector of |U| dimension which represents probability distribution over all the rules in the universal set |U| .\nThe output is a single logit value which is an indication whether the sample is from the set Q or not.\n8\nEvaluation\nTo evaluate the competitiveness of the developed Unsupervised based Conversational Machine Reading Tool (UCMRT)\n, we perform a direct comparison to several state of the art tools.\n8.1\nBaseline\nDISCERN [3] splits the rule text into elemetary discourse units (EDU) using a pre-trained discourse segmentation\nmodel, it then trains a supervised model to predict whether each EDU is entailed by the user feedback in a conversation.\nUsing the trained model the system gives a feedback to the user’s question.\nE3 [2] proposes a number of threshold based heuristics that extracts rules from the rule text, checks for entailment\nof the extracted rules and uses LSTM based model to generate follow up questions to clarify rules that have not been\nentailed.\nEMT [21] ﬁrst encodes the conversational history using BERT, it then uses explicit memory tracking that relies on\nrecurrent network to update the entailment state of a rule sentence. For decision making, it exploits entailment oriented\nreasoning based on the current states of rule sentences.\nWe also compare the results of the our developed CMR tool to Seq2Seq and Pipeline whose results are reported in [8]\n8.2\nResults\nThe evaluation of the developed system is shown in table 1. The results of UCMRT compared to other state of the\nTable 1: Performance Comparison of UCMRT on blind held-out test of ShARC end to end task\nModel\nBLEU1\nBLEU 4\nMicro Accuracy\nMacro Accuracy\nE3\n54.1\n38.7\n73.3\n67.6\nPipeline\n54.4\n34.4\n61.9\n68.9\nSeq2Seq\n34.0\n7.8\n44.8\n42.8\nDISCERN\n64.0\n49.1\n73.2\n78.3\nEMT\n60.9\n46.0\n69.4\n74.8\nUCMRT\n66.7\n50.2\n76.5\n79.7\nart tools on held out test of ShARC is shown in table 1. UCMRT which is majorly based on unsupervised learning\n7\nA PREPRINT\nreports 3.3% improvement on micro averaged accuracy as compared to DISCERN which currently reports the highest\nvalue of 73.2%. Similarly, for macro averaged accuracy, UCMRT reports a 1.4% improvements as compared to\nDISCERN which currently reports the best macro-accuracy of 78.3%. Further, we investigate the performance of\nUCMRT on each distinct class of answers given to the user. The results are shown in table 2 The results in table 1\nTable 2: Prediction accuracy of UCMRT on answer generation per class(Yes, No, Inquire and Irrelevant) on ShARC\ndataset\nModel\nYes\nNo\nInquiry\nIrrelevant\nE3\n65.9\n70.6\n60.5\n96.4\nDISCERN\n71.9\n75.8\n73.3\n99.3\nEMT\n70.5\n73.2\n70.8\n98.6\nUCMRT\n74.1\n77.2\n76.5\n98.9\nand the signiﬁcance improvement of the prediction accuracy of UCMRT on the three classes i.e Yes,No and Inquiry\ndemonstrates that UCMRT generator of the GAN is able to extract majority of the rules in the rule text and the model\nhas better understanding of rule entailment and is able capture negation that exist between conversational history and\nrule text.\n9\nAblation Study\nMotivated by the evaluation done in [3], where they compared the results when RoBERTa encoder is replaced with\nBERT encoder while the entire system remains the same, we also set up another version UCMRT(RoBERTa) where\nBERT is replaced with RoBERTa in ﬁgure 3. The results is reported in table 3. Based on the results presented in table\nTable 3: RoBERTa vs BERT\nModel\nMicro Acc\nMacro Acc\nUCMRT(BERT)\n76.5\n79.7\nUCMRT(RoBERTa)\n77.6\n79.2\n3, RoBERTa reports an improvement of 0.9% on micro-accuracy and 0.5% degradation on the macro-accuracy. This\nshows that on the overall, replacing BERT encoder with RoBERTa in the UCMRT tool has no signiﬁcant impact on the\nperformance of the tool.\nUCMRT extracts a span of sentences that addresses a given subject(topic), then uses spectral partitioning technique to\nextract rules within a given subject. We investigated if this technique presented a performance advantage as compared\nto simple sentence splitting i.e that a a rule is composed within a given sentence From the results in table 4, performing\nTable 4: Spectral rule extraction vs Sentence splitting\nModel\nMicro Acc\nMacro Acc\nUCMRT\n76.5\n79.7\nUCMRT(Sentence Splitting)\n73.6\n74.2\na trivial sentence splitting signiﬁcantly degrades the performance of the tool. From our analysis we observed that when\nmultiple rules are contained within a sentence, most rules are ignored and treated as a single rule hence when it comes\nto the entailment module described in section 4 the generator of GAN fails to generate most of the rules which degrades\nthe performance downstream.\n10\nConclusion\nThis paper presents an unsupervised based CMR tool. The paper also looks into how CMR tool can be conﬁgured to\nanswer more general questions from users. We speciﬁcally exploit spectral clustering algorithm to extract rules from\nthe rule text and then we use the GAN unsupervised model to learn how the rules are extracted from rule text. The\ntrained GAN model is able to generate rule(s) given a sentence from the rule text. We then apply set theory to check\nfor rule entailment. For question generation, we apply a simple LSTM model to generate a question to the user. The\nexperiments based on the developed tool achieves state of the art results\n8\nA PREPRINT\nReferences\n[1] Somil Gupta, Bhanu Pratap Singh Rawat, and Hong Yu. Conversational Machine Comprehension: a Literature\nReview. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2739–2753,\n2021. doi:10.18653/v1/2020.coling-main.247.\n[2] Victor Zhong and Luke Zettlemoyer. E3: Entailment-driven extracting and editing for conversational machine\nreading. In ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the\nConference, pages 2310–2320, 2020. ISBN 9781950737482. doi:10.18653/v1/p19-1223.\n[3] Yifan Gao, Chien-sheng Wu, Jingjing Li, Shaﬁq Joty, Steven C.H. Hoi, Caiming Xiong, Irwin King, and Michael\nLyu. Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2439–2449,\n2020. doi:10.18653/v1/2020.emnlp-main.191.\n[4] Siru Ouyang, Zhuosheng Zhang, and Hai Zhao. Dialogue Graph Modeling for Conversational Machine Reading.\n2020. URL http://arxiv.org/abs/2012.14827.\n[5] Jing Li, Aixin Sun, and Shaﬁq Joty. S EG B OT : A Generic Neural Text Segmentation Model with Pointer\nNetwork. In In Proceedings of the Twenty- Seventh International Joint Conference on Artiﬁcial Intelligence, pages\n4166–4172, jul 2017.\n[6] Omar Alonso. Challenges with label quality for supervised learning. Journal of Data and Information Quality, 6\n(1):3–5, 2015. ISSN 19361963. doi:10.1145/2724721.\n[7] Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Word translation\nwithout parallel data. In 6th International Conference on Learning Representations, ICLR 2018 - Conference\nTrack Proceedings, pages 1–14, 2018.\n[8] Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rocktäschel, Mike Sheldon, Guillaume Bouchard,\nand Sebastian Riedel. Interpretation of natural language rules in conversational machine reading. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018, volume 1, pages\n2087–2097, 2020. ISBN 9781948087841. doi:10.18653/v1/d18-1233.\n[9] Yifan Gao, Jingjing Li, Michael R Lyu, and Irwin King. Open-Retrieval Conversational Machine Reading. In in\nProceedings of the 58th Annual Meeting of the Association for Computational Lin- guistic, pages 935–945, 2021.\nURL http://arxiv.org/abs/2102.08633.\n[10] Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. NAACL HLT 2019 - 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, 1\n(Mlm):4171–4186, 2019.\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017-Decem\n(Nips):5999–6009, 2017. ISSN 10495258.\n[12] Bojan Mohar. THE LAPLACIAN SPECTRUM OF GRAPHS. In Proc. 6th Quadrennial Intl. Conf. on Theory\nand Applications of Graphs, volume 2, pages 871–898, 1988.\n[13] B Mohar. Some applications of Laplace eigenvalues of graphs. Graph Symmetry: Algebraic Methods and\nApplications, 497:225–275, 1991. ISSN 0258-2023. doi:10.1.1.17.467. URL http://citeseerx.ist.psu.\nedu/viewdoc/summary?doi=10.1.1.17.467.\n[14] Ian Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. 2016. URL http://arxiv.org/abs/\n1701.00160.\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020. ISSN\n15577317. doi:10.1145/3422622.\n[16] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training\nof wasserstein GANs. In Advances in Neural Information Processing Systems, volume 2017-Decem, pages\n5768–5778, 2017.\n[17] Michael Auli and Alexander M Rush. Abstractive Sentence Summarization with Attentive Recurrent Neural\nNetworks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics:Human Language Technologies. Association for Computational Linguistics, pages\n93–98, San Diego, California, 2016.\n9\nA PREPRINT\n[18] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension.\nIn ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the\nConference (Long Papers), volume 1, pages 1342–1352, 2017. ISBN 9781945626753. doi:10.18653/v1/P17-1123.\n[19] J¨urgen Hochreiter, Sepp and Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1–32, 1997.\n[20] Alon Jacovi, Oren Sar Shalom, and Yoav Goldberg. Understanding Convolutional Neural Networks for Text\nClassiﬁcation. pages 56–65, 2019. doi:10.18653/v1/w18-5408.\n[21] Yifan Gao, Chien-Sheng Wu, Shaﬁq Joty, Caiming Xiong, Richard Socher, Irwin King, Michael Lyu, and\nSteven C.H. Hoi. Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading.\npages 935–945, 2020. doi:10.18653/v1/2020.acl-main.88.\n10\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-06-29",
  "updated": "2021-06-29"
}