{
  "id": "http://arxiv.org/abs/2106.15691v2",
  "title": "Deep Multiagent Reinforcement Learning: Challenges and Directions",
  "authors": [
    "Annie Wong",
    "Thomas Bäck",
    "Anna V. Kononova",
    "Aske Plaat"
  ],
  "abstract": "This paper surveys the field of deep multiagent reinforcement learning. The\ncombination of deep neural networks with reinforcement learning has gained\nincreased traction in recent years and is slowly shifting the focus from\nsingle-agent to multiagent environments. Dealing with multiple agents is\ninherently more complex as (a) the future rewards depend on multiple players'\njoint actions and (b) the computational complexity increases. We present the\nmost common multiagent problem representations and their main challenges, and\nidentify five research areas that address one or more of these challenges:\ncentralised training and decentralised execution, opponent modelling,\ncommunication, efficient coordination, and reward shaping. We find that many\ncomputational studies rely on unrealistic assumptions or are not generalisable\nto other settings; they struggle to overcome the curse of dimensionality or\nnonstationarity. Approaches from psychology and sociology capture promising\nrelevant behaviours, such as communication and coordination, to help agents\nachieve better performance in multiagent settings. We suggest that, for\nmultiagent reinforcement learning to be successful, future research should\naddress these challenges with an interdisciplinary approach to open up new\npossibilities in multiagent reinforcement learning.",
  "text": "Deep Multiagent Reinforcement Learning: Challenges\nand Directions\nAnnie Wong · Thomas Bäck · Anna V.\nKononova · Aske Plaat\nOctober 14, 2022\nAbstract This paper surveys the ﬁeld of deep multiagent reinforcement learning.\nThe combination of deep neural networks with reinforcement learning has gained\nincreased traction in recent years and is slowly shifting the focus from single-agent\nto multiagent environments. Dealing with multiple agents is inherently more com-\nplex as (a) the future rewards depend on multiple players’ joint actions and (b)\nthe computational complexity increases. We present the most common multiagent\nproblem representations and their main challenges, and identify ﬁve research areas\nthat address one or more of these challenges: centralised training and decentralised\nexecution, opponent modelling, communication, eﬃcient coordination, and reward\nshaping. We ﬁnd that many computational studies rely on unrealistic assumptions\nor are not generalisable to other settings; they struggle to overcome the curse of\ndimensionality or nonstationarity. Approaches from psychology and sociology cap-\nture promising relevant behaviours, such as communication and coordination, to\nhelp agents achieve better performance in multiagent settings. We suggest that, for\nmultiagent reinforcement learning to be successful, future research should address\nthese challenges with an interdisciplinary approach to open up new possibilities in\nmultiagent reinforcement learning.\nKeywords Reinforcement learning · Deep learning · Multiagent systems ·\nEvolutionary algorithms · Psychology · Survey\n1 Introduction\nReinforcement learning (RL) is a machine-learning method in which one agent\nor a group of agents maximises its long-term return through repeated interaction\nwith its environment. Agents are not told what actions to take and must learn\nits optimal behaviour via trial-and-error. Since rewards may be delayed, an agent\nLeiden Institute of Advanced Computer Science\nLeiden University, Leiden\nThe Netherlands\nE-mail: a.s.w.wong@liacs.leidenuniv.nl\narXiv:2106.15691v2  [cs.LG]  12 Oct 2022\n2\nWong et al.\nhas to make a trade-oﬀbetween exploiting states with the current highest reward\nand exploring states that may potentially yield higher rewards (Bellman, 1957).\nAs agents learn by receiving rewards for desirable actions and penalties (negative\nrewards) for undesired actions, RL can automate learning and decision-making\nwithout supervision or having complete models of the environment. However, one\ndrawback of RL methods is that they suﬀer from the curse of dimensionality (Bell-\nman, 1957): algorithms become less eﬃcient as the dimensions of the state-action\nspace increase (Sutton et al., 1998). In recent years the rise of deep reinforcement\nlearning (DRL), a combination of RL and deep learning, has enabled artiﬁcial\nagents to surpass human-level performance in a wide range of complex decision-\nmaking tasks, such as in the board game Go (Silver et al., 2016) and the card game\nPoker (Brown and Sandholm, 2018, 2019; Bowling et al., 2015). While prior RL\napplications required carefully handcrafted features based on human knowledge\nand experience (Sutton et al., 1998), deep neural networks can automatically ﬁnd\nlow-dimensional representations (features) of high-dimensional data (LeCun et al.,\n2015). This development has led to enormous growth in applying RL to more\ncomplicated problems. First in single-agent settings such as playing Atari (Mnih\net al., 2015), resource management (Wen et al., 2015; Mao et al., 2016), indoor\nrobot navigation (Zhu et al., 2017), cyber security (Huang et al., 2022), and trade\nexecution (Nevmyvaka et al., 2006), and more recently in multiagent settings such\nas bidding optimization (Jin et al., 2018), traﬃc-light control (Chu et al., 2020),\nautonomous driving (Sallab et al., 2017), ﬁnancial market trading (Bao and Liu,\n2019), energy usage (Prasad and Dusparic, 2019), ﬂeet optimization (Lin et al.,\n2018) and strategy games like Dota 2 (Berner et al., 2019) and Starcraft (Vinyals\net al., 2019).\nIt is challenging to translate the successes of DRL in single-agent settings to a\nmultiagent setting. Multiagent reinforcement learning (MARL) diﬀers from single-\nagent systems foremost in that the environment’s dynamics are determined by the\njoint actions of all agents in the environment, in addition to the uncertainty already\ninherent in the environment. As the environment becomes nonstationary, each\nagent faces the moving-target problem: the best policy changes as the other agents’\npolicies change (Busoniu et al., 2008; Papoudakis et al., 2019). The violation of\nthe stationarity assumption required in most single-agent RL algorithms poses a\nchallenge in solving multiagent learning problems. The curse of dimensionality is\nalso worse in a multiagent setting as every additional agent increases the state-\naction space. At the same time, MARL introduces a new set of opportunities as\nagents may share knowledge and imitate or directly learn from other learning\nagents (Da Silva and Costa, 2019; Ilhan et al., 2019), which may accelerate the\nlearning process and subsequently result in more eﬃcient ways of arriving at a\ngoal.\nDeep multiagent reinforcement learning (DMARL) constitutes a young ﬁeld\nthat is rapidly expanding. Many real-world problems can be modelled as a MARL\nproblem, and the emergence of DRL has enabled researchers to move from simple\nrepresentations to more realistic and complex environments. This survey examines\ncurrent research areas within DMARL, addresses critical challenges, and presents\nfuture research directions. Earlier surveys were driven by the theoretical diﬃculties\nin multiagent systems, including nonstationarity (Hernandez-Leal et al., 2019a; Pa-\npoudakis et al., 2019), partial observability, and continuous state and action spaces\n(Nguyen et al., 2020). Others focus on how agents learn, such as transfer learning\nDeep multiagent Reinforcement Learning\n3\n(Da Silva and Costa, 2019), modelling other agents (Albrecht and Stone, 2018), or\na theoretical domain such as game theory (Yang and Wang, 2021) and evolution-\nary algorithms (Bloembergen et al., 2015). A number of studies have looked into\nthe applications of MARL (Canese et al., 2021; Feriani and Hossain, 2021; Du and\nDing, 2021). This paper complements a group of surveys that provides a general\nframework to classify the deep learning algorithms used in recent DMARL studies\n(Hernandez-Leal et al., 2019; Gronauer and Diepold, 2021).\nWhen working on this survey, Google Scholar was the leading search engine\nfor ﬁnding relevant papers containing keywords such as \"multiagent\" or \"multia-\ngent\", \"reinforcement learning\", and \"deep learning\". We cover works from leading\njournals, conference proceedings, relevant arXiv papers, book chapters, and PhD\ntheses. We carefully evaluated the studies that came to our attention and devel-\noped a taxonomy based on the prominent research directions in the ﬁeld.\nIn contrast to prior surveys, we propose a taxonomy based on the challenges\ninherent in multiagent problem formalisations and their solutions. Modelling a\nmultiagent problem diﬀers from the single-agent setting due to the violation of\nthe stationarity assumption and the diﬀerence in learning objectives. Hence, al-\nternative problem formalisations and solutions have been introduced. While other\ntaxonomies also start from multiagent problem representations (Yang and Wang,\n2021; Zhang et al., 2021), these studies only focus on Markov and extensive-form\ngames. Recent MARL research has used additional representations to model mul-\ntiagent problems, such as the decentralised partially observable Markov game and\nthe partially observable Markov game, which we will also cover in this survey.\nThe remainder of this paper is organised as follows. In section 2 the prelimi-\nnaries of single-agent RL are discussed. In section 3 we present the most common\nDMARL problem frameworks. The taxonomy is introduced in section 4. The dis-\ncussion and recommendations for future research are given in section 5. We end\nwith the conclusion in section 6.\n2 Single-agent Reinforcement Learning\n2.1 Markov Decision Process\nMost RL problems can be framed as a Markov decision process (MDP) (Bellman,\n1957): a model for sequential decision making under uncertainty that deﬁnes the\ninteraction between a learning agent and its environment. Formally, it can be\ndeﬁned as a tuple ⟨S, A, P, R, γ⟩where S is the set of states, A is the set of\nactions, P is the transition probability function, R is the reward function and γ ∈\n[0, 1] is the discount factor for future rewards. The learning agent interacts with\nthe environment in discrete time steps. At each time step t, the agent is in some\nstate st ∈S and selects an action at ∈A. At time step tt+1 the agent receives a\nreward rt+1 ∈R and moves into a new state st+1. Speciﬁcally, the state transition\nfunction is deﬁned as P(s′, r|s, a) = Pr{St = s′, Rt = r|St−1 = s, At−1 = a} and\ndescribes the model dynamics. Each state in an MDP has the Markov property,\nwhich means that the future only depends on the current state and not on the\nhistory of earlier states and actions. MDPs further assume that the agent has full\nobservability of the states and that the environment is stationary: the transition\nprobabilities and rewards remain constant over time. A setting where the agent\n4\nWong et al.\ndoes not have full observability of the state is called a partially observable Markov\ndecision process (POMDP) (Åström, 1965).\nA policy π is a mapping from states to probabilities of selecting each ac-\ntion and can be deterministic or stochastic. The goal of the agent is to learn a\npolicy that maximises its performance and is typically deﬁned as the expected\nreturn, computed as the expected discounted sum of rewards, in a trajectory\nτ = (s0, a0, s1, a1, ...), a sequence of states and actions in the environment:\nEτ\nh\nT\nX\nt=0\nγtrt\ni\n.\n(1)\nThe discount factor γ ∈[0, 1] describes how rewards are valued. A γ closer to\n0 means that the agent places more value on immediate rewards, while a γ closer\nto 1 indicates that the agent favours future rewards. A policy that maximises the\nfunction above is optimal and is denoted as π∗.\nMost MDP solving algorithms can be divided into one of three groups: value-\nbased, policy-based, and model-based methods. This distinction is based on the\nthree primary functions to learn in RL (Graesser and Keng, 2019). Hybrid forms\nof the three primary functions also exist. We present a brief overview of each of\nthe three classes.\n2.2 Value-based Methods\nValue-based methods learn the value function and derive the optimal policy from\nthe optimal value function. There are two kinds of value functions. The state-value\nfunction describes how good it is to be in a state, and it is the expected return\nfrom being in state s and then following policy π and is denoted as:\nvπ(s) = Es0=s,τ∼π\nh\nT\nX\nt=0\nγtrt\ni\n.\n(2)\nThe action-value function or sometimes called the Q-function describes how\ngood it is to perform action a in state s and is denoted as:\nqπ(s, a) = Es0=s,a0=a,τ∼π\nh\nT\nX\nt=0\nγtrt\ni\n.\n(3)\nThe optimal policy π∗maximizes the state-value function such that vπ∗(s) >\nvπ(s) for all s ∈S and all policies π. If we have the optimal state-value func-\ntion, the optimal policy can be extracted by choosing the action that gives the\nmaximum action-value for state s. This relationship is given by π∗= max\nπ\nvπ(s) =\nmax\nπ\nqπ(s, a).\nDeep Q-networks (DQN) (Mnih et al., 2015) belong to the value-based methods\nthat have become increasingly popular as studies achieved remarkable results in\nmore complicated environments such as Atari games. Recent developments in RL\nresearch show a preference for policy-based strategies, even though value-based\nmethods can capture the underlying structure of the environment (Arulkumaran\net al., 2017).\nDeep multiagent Reinforcement Learning\n5\n2.3 Policy-based and Combined Methods\nIn contrast to value-based methods, policy-based methods search directly for the\noptimal policy and the output is represented as a probability distribution over\nactions. The optimal policy is found by optimising a θ-parameterized policy with\nrespect to the objective via gradient ascent. The policy network weights are up-\ndated iteratively so that state-action pairs that result in higher returns are more\nlikely to be selected. The objective is the expected return over all completed tra-\njectories and is deﬁned as follows:\nJ(θ) = Eτ∼πθ\nh\nT\nX\nt=0\nγtrt\ni\n.\n(4)\nMany policy gradients methods build upon REINFORCE (Williams, 1992),\none of the ﬁrst policy gradient implementations which used Monte Carlo sampling\nto estimate the policy gradient.\nPolicy gradient methods perform better in continuous and stochastic environ-\nments, learn speciﬁc probabilities for each action, and learn the appropriate level\nof exploration (Sutton and Barto, 2018). The main limitation of policy gradient\nmethods is the large variance in the gradient estimators (Greensmith et al., 2004)\ndue to sparse rewards and the fact that only a ﬁnite set of states and actions are\ntried. Policy gradient methods are not very sample-eﬃcient since new estimates of\nthe gradients are learned independently from past estimates (Konda and Tsitsiklis,\n2003; Peters and Schaal, 2008).\nActor-critic methods (Konda and Tsitsiklis, 2003; Grondman et al., 2012; Bah-\ndanau et al., 2017) combine policy-based and value-based methods to address\nthese limitations: Actor-critic methods preserve the desirable convergence prop-\nerties while maintaining stability during learning. Actor-critic methods consist of\nan actor that learns a policy and a critic that learns a value function to evaluate\nthe state-action pair. The critic approximates and updates the value function pa-\nrameters w for either the state-value v(s; w) or the action-value q(a|s; w), and the\nactor updates the policy parameters θ for πθ(a|s) in the direction suggested by\nthe critic.\nPopular actor-critic methods include Advantage Actor-Critic (A2C) (Wu et al.,\n2017a), Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016), Proxi-\nmal Policy Optimization (PPO) (Schulman et al., 2017), Soft Actor-Critic (SAC)\n(Haarnoja et al., 2018) and Twin-Delayed Deep Deterministic Policy Gradient\n(TD3) (Dankwa and Zheng, 2019). In A3C, multiple agents interact with a copy\nof the environment in parallel and update the global network parameters asyn-\nchronously (Mnih et al., 2016). In contrast, A2C performs the global network\nupdates synchronously and is found to be more eﬃcient on a GPU machine or\nwhen larger policies are trained (Wu et al., 2017b). PPO builds upon Trust Re-\ngion Policy Optimization (TRPO) (Schulman et al., 2015), a method in which\nthe gradient steps are constrained to prevent destructive policy updates. PPO\nuses ﬁrst-order optimisation to compute the updates, simplifying the algorithm’s\ntuning and implementation. In contrast to previous methods, SAC and TD3 are\noﬀ-policy methods that eﬃciently reuse past experiences. SAC uses entropy max-\nimization to encourage exploration, while TD3 is a combination of continuous\n6\nWong et al.\nDouble Deep Q-Learning (Van Hasselt et al., 2016), Policy Gradient (Silver et al.,\n2014) and Actor-Critic (Sutton et al., 1999).\n2.4 Model-based Methods\nModel-based approaches learn a model of the environment that captures the tran-\nsition and reward function. The agent can then use planning, the construction of\ntrajectories or experiences using the model (Hamrick et al., 2021) to ﬁnd the opti-\nmal policy. While model-free methods focus on learning, where the agent improves\na policy or value function from direct experiences generated by the environment,\nmodel-based methods focus on planning (Sutton and Barto, 2018).\nThe environment model can either be given or learned. Games such as chess and\nGo belong to the ﬁrst category. When there is no given model, the agent must learn\nit through repeated interaction with the environment using a base policy π0(at|st).\nThe experiences are stored in historical data D = (si\nt, ai\nt,si\nt+1), which is then used\nto learn the dynamics model P(s, a) by minimizing P\ni ||P(si\nt, ai\nt) −si\nt+1||2. Given\nthe current state s and action a, the next state st+1 is then given by st+1 =\nP(st, at). Planning is then performed through P(s, a) (Levine, 2017; Chua et al.,\n2018). Planning methods generally compute value functions via updates or backup\noperations to simulated experiences to ﬁnd the optimal policy (Sutton and Barto,\n2018).\nExamples of model-based algorithms include AlphaZero (Silver et al., 2017) and\nMuZero (Schrittwieser et al., 2020) that achieved state-of-the-art performance in\nAtari, Go, chess and Shogi. For a recent overview of deep reinforcement learning\nin model-based games, see (Plaat, 2020).\nThe main advantage of model-based approaches is better sample eﬃciency.\nAgents may use the model to simulate experiences to have fewer interactions with\nthe environment, resulting in faster convergence. However, it is diﬃcult to accu-\nrately represent the model, especially in real-world scenarios where the transition\ndynamics are unavailable. In addition, when bias and inaccuracies are present in\nthe model, errors may accumulate for each step (Graesser and Keng, 2019).\n3 Multiagent Problem Representations\nIn MARL, a set of autonomous agents interact within the environment to learn\nhow to achieve their objectives. While MDPs have proven helpful in modelling\noptimal decision-making in single-agent stochastic environments, multiagent en-\nvironments require a diﬀerent representation. The state dynamics and expected\nrewards change upon all agents’ joint action, violating the core stationarity as-\nsumption of an MDP.\nMDPs can be fully or partly visible to the agent. In a multiagent setting, the\nproblem representation is also dependent on the nature of the interaction between\nagents, which can be cooperative, competitive or mixed, and whether agents take\nactions sequentially or simultaneously. Figure 1 shows an overview of the most\ncommon theoretical frameworks used in the DMARL literature. When agents have\n1 Illustrations are created with BioRender.com\nDeep multiagent Reinforcement Learning\n7\nFig. 1 Diagram of problem representations and their main challenges\nmultiagent\nproblem representations can be categorised along a number of axes. First, whether the en-\nvironment is fully or partially observable. Second, whether the nature of the interaction is\ncollaborative, mixed or competitive. Third, whether turns are taken sequentially or simul-\ntaneously. Diﬀerent problem representations come with diﬀerent challenges. The four main\nchallenges include computational complexity, nonstationarity, partial observability and credit\nassignment. Computational complexity and nonstationarity are challenges found in all problem\nrepresentations, while partial observability and credit assignment are speciﬁc to some.1\nfull observability of the state, the problem is usually represented by a Markov\ngame. A particular type is the team Markov game, where agents collaborate to\nmaximise a common reward. If agents collaborate but execute actions decentrally,\nit is represented by a decentralised partially observable Markov decision process.\nThe partially observable variant for the mixed and competitive setting is known as\nthe partially observable Markov game. The extensive-form game representation is\nused when agents take turns sequentially instead of simultaneously. The following\nsections outline the theoretical frameworks pertinent to the DMARL literature,\nwhich are visually depicted in Figure 2.\n3.1 Markov Games\nMarkov games (e.g. Littman, 1994), or Stochastic games (Shapley, 1953)2, provide\na theoretical framework to study multiple interacting agents in a fully observable\nenvironment and can be applied to cooperative, collaborative and mixed settings.\nA Markov game is a collection of normal-form games (or matrix games) that\nthe agents play repeatedly. Each state of the game can be viewed as a matrix\nrepresentation with the payoﬀs for each joint action determined by the matrices.\n2 The terms Markov game and stochastic game are used interchangeably in the literature.\nFor consistency, we will continue using the term Markov game throughout the paper.\n8\nWong et al.\nFig. 2\nVisual depiction of the main problem representations in multiagent rein-\nforcement learning The MDP is the primary framework used in the single-agent setting. An\nagent is in some state S, performs action A, and receives a reward R from the environment.\nIn partially observable environments, the agent cannot view the true state S and receives an\nobservation O instead. For simplicity, all ﬁgures display the interaction between two agents\ni = 1, 2 but can be extended to more agents.\nIn its general form, a Markov game is a tuple ⟨I, S, A, R, T⟩where I is the\nset of N agents, S is a ﬁnite state space, A = A1 × A2 × ... × AN is the joint\naction space of N agents, R = (r1, r2, ..., rN) where ri : S × A →R is each\nagent’s reward function and T : S × A × S →[0, 1] is the transition function. In a\nteam Markov game, agents work together to achieve a goal and share the rewards\nfunction r1 = r2 = . . . = rN. A competitive Markov game is represented by a\nzero-sum game: the gains for one party automatically result in equal losses for\nthe other. A Markov game is a normal form game, which means that the game is\nrepresented in a tabular form, and all agents take their actions simultaneously.\nOne way to solve Markov games is to learn equilibria by optimising over\nan agent’s reward function and ignoring others in the environment (Tan, 1993;\nLittman, 1994). Another approach involves best response learners. Agents opti-\nmise their reward function while accounting for other agents’ changing policies. If\nthese algorithms converge during the play, then it must be an equilibrium (Bowl-\ning and Veloso, 2001, 2002). However, equilibrium concepts either assume inﬁnite\ncomputational resources or have been applied to smaller grid-word environments,\nas they do not scale well with the number of agents.\nThe majority of studies in DMARL focus on Markov games, such as Pong (Di-\nallo et al., 2017), predator games (Zheng et al., 2018a) and the iterated prisoner’s\ndilemma (Foerster et al., 2018a).\nDeep multiagent Reinforcement Learning\n9\n3.2 Extensive-Form Games\nWhen agents take turns sequentially, this is modelled as an extensive-form game\n(Kuhn and Tucker, 1953). An extensive-form game speciﬁes the sequential inte-\nraction between agents in the form of a game tree. The game tree shows the order\nof the agents’ moves and the possible actions at each point in time. Formally,\nan extensive-form game with ﬁnite and perfect information is given by the tuple\n⟨P, A, H, Z, χ, ρ, σ, u⟩where P is a set of players or agents, A is a single set of ac-\ntions, H is a set of non-terminal choice nodes, Z is a set of terminal outcome nodes,\nχ : H →2A is an action function, representing the set of possible actions at each\nnode, ρ : H →P is the player function, which assigns at each choice node a player\ni ∈P who is to take action at a given non-terminal node, σ : H ×A →H ∪Z is the\nsuccessor function, that maps a choice node and an action to a new choice node\nor terminal node and u is a set of utility functions (Shoham and Leyton-Brown,\n2008).\nWhen agents have incomplete information or a partial view of the global state,\nthis can be formalised as an imperfect information extensive-form game in which\ndecision nodes are portioned into information sets. When the game reaches the\ninformation set, the agent whose turn it is cannot distinguish between nodes within\nthe information set nor tell which node in the tree has been reached. Formally,\nan imperfect information extensive-form game is a tuple ⟨P, A, H, Z, χ, ρ, σ, u, I⟩\nwhere ⟨P, A, H, Z, χ, ρ, σ, u⟩is a perfect information extensive-form game and I =\nI1, ..., IN is the set of information partitions of all players.\nA strategy maps each agent’s information sets to a probability distribution over\npossible actions. The exploitability is a mean score over all the positions against a\nworst-case adversary who uses at each turn a best-response. In a Nash equilibrium,\nthe exploitability is equal to 0, and no agents have an incentive to change their\nstrategies (Johanson et al., 2013). Studies try to solve extensive-form games by\napproximating a Nash equilibrium, predominantly in the poker domain (Bowling\net al., 2015; Heinrich et al., 2015; Moravčík et al., 2017; Heinrich and Silver, 2016;\nBrown and Sandholm, 2018, 2019) and board games such as Go (Silver et al., 2016,\n2017) and Othello (Van Der Ree and Wiering, 2013).\n3.3 Decentralized Partially Observable Markov Decision Process\nIn a decentralised partially observable Markov decision process (Dec-POMDP),\nall agents attempt to maximise the joint reward function while having diﬀerent\nindividual objectives (Bernstein et al., 2002).\nA Dec-POMDP is deﬁned by the tuple ⟨I, S, A, Ω, O, T, R⟩, where I is the set\nof N agents, S is the ﬁnite state space, A is the joint action set, Ωis the joint\nobservations set, O is the observation probability function: O : Ω× A × S →[0, 1]\nand O(o1, ..., oN|a1, ..., aN, s′) are observed by agents 1, ..., N, respectively, given\nthat each action tuple ⟨a1, ..., aN⟩was taken and led to state s′. Each agent i has\na set of actions Ai ∈A for each observation Ωi ∈Ω. T is the state transition\nprobability function T : S ×A×S →[0, 1] that specify the transition probabilities\nP(s′|s, a1, ..., aN). Finally, R is the reward function R(s, a1, ..., aN).\nAt every time step, each agent takes an action and receives a local observation\nthat is correlated with the state and an immediate joint reward. A local policy\n10\nWong et al.\nmaps local histories of observations to actions, and a joint policy is a tuple of local\npolicies.\nThe computational complexity of Dec-POMDPs presents a big challenge for\nresearchers. These problems are not solvable with polynomial-time algorithms, and\nsearching directly for an optimal solution in the policy space is intractable (Bern-\nstein et al., 2002). One approach is to transform the Dec-POMDP into a simpler\nmodel and solve it with planning algorithms (Amato and Oliehoek, 2015; Ye et al.,\n2017). For instance, using a centralised controller that receives all agents’ private\ninformation converts the model into a POMDP, and allowing communication that\nis free of costs and noise reduces it to a multiagent POMDP (MPOMDP) (Am-\nato and Oliehoek, 2015; Gupta et al., 2017). Recent solutions also take advantage\nof the key assumption that planning can be centralised as long as execution is\ndecentralised.\nThe Dec-POMDP has been used to represent riddles (Foerster et al., 2016),\ncoordination of bipedal walkers (Gupta et al., 2017) and real-time strategy games\nsuch as Starcraft (Vinyals et al., 2019; Schroeder de Witt et al., 2019; Du et al.,\n2019), Dota 2 (Berner et al., 2019), and Capture the Flag (Jaderberg et al., 2019).\n3.4 Partially Observable Markov Game\nThe partially observable Markov game (POMG) (Hansen et al., 2004), also known\nas the partially observable stochastic game (POSG), is the counterpart of the\nDec-POMDP. Instead of a joint reward function, agents optimise their individual\nreward functions in a partially observable environment. The POMG implicitly\nmodels a distribution over other agents’ belief states. Formally, a POMG is a\ntuple ⟨I, S, A, O, b0, P, R⟩where I is the set of N agents, S is the set of states, Ai\nis the action set of agent i and A = A1 × A2 × ... × AN is the joint action set,\nOi is a set of observations for agent i and O = O1 × O2 × ... × ON is the joint\nobservation set. The game’s initial state, also called the initial belief, is drawn\nfrom a probability distribution b0 over the states. P is a set of state transitions\nand observation probabilities, where P(s′, o|s, a) is the probability of moving into\nstate s′ and joint observation o when taking joint action a in state s. Ri : S×A →R\nis the reward function for agent i where S refers to the joint state (s1, ..., sN) and\nA refers to the joint actions (a1, ..., aN). The model can be reduced to a POMDP\nwhen |I| = 1.\nDynamic programming algorithms have been developed for POMG (Hansen\net al., 2004; Kumar and Zilberstein, 2009), in which agents maintain a belief over\nthe actual state of the environment and other agents’ policies. However, applying\nit to high-dimensional problems becomes intractable, and assumptions are often\nrelaxed or applied to simpler problems. Complexities such as competing goals,\nnonstationarity and incomplete information make the problem even harder. Ex-\namples of POMG include autonomous driving (Palanisamy, 2020) and partially\nobservable grid world games (Moreno et al., 2021).\nDeep multiagent Reinforcement Learning\n11\nFig. 3\nOverview of taxonomy This ﬁgure shows how the paper is organised. We start\nby discussing the main training scheme in DMARL: centralised training and decentralised\nexecution. We then move to how agents learn through opponent modelling and interact with\nother agents via communication and coordination. Finally, we discuss how diﬀerent reward\nshaping methods act as a feedback mechanism.\n4 Taxonomy of Deep multiagent Reinforcement Learning Algorithms\nWe will now introduce the taxonomy of this paper. We ﬁrst discuss the four main\nchallenges inherent in multiagent settings: (1) computational complexity, (2) non-\nstationarity, (3) partial observability and (4) credit assignment. We then provide\nan overview of current deep learning approaches and discuss how these algorithms\naddress these challenges. The surveyed studies cover the whole learning process of\nan agent: starting from the training scheme, how it learns and interacts with the\nenvironment, to how an agent incorporates feedback, as shown in Figure 3. The\nreviewed algorithms have been categorised into one of the following groups: (1)\ncentralised training and decentralised execution, (2) opponent modelling, (3) com-\nmunication, (4) eﬃcient coordination and (5) reward shaping. Figure 4 shows the\nrelationship between the reviewed studies and the challenges that they address.\nFinally, Table 1 presents examples of some of the major studies along with their\nmain challenges and solutions.\n4.1 Challenges\nReinforcement learning in a multiagent environment comes with numerous chal-\nlenges. Addressing these challenges is a prerequisite for the development of eﬀective\nlearning approaches. Despite promising results in the literature, computational\ncomplexity, nonstationarity, partial observability, and credit assignment remain\nlargely unsolved.\nThe four challenges do not occur in isolation. In contrast, a multiagent problem\nusually deals with one or more challenges simultaneously. All multiagent problems\ndeal with high computational demands, and the higher the number of agents,\nthe more demanding it is on computing power. The problem of nonstationarity\n12\nWong et al.\nFig. 4\nVenn diagram of challenges and solutions The taxonomy of DMARL algo-\nrithms comprises ﬁve groups: centralised training and decentralised execution, opponent mo-\ndelling, communication, eﬃcient coordination and reward shaping. Approaches may tackle\none or more challenges: nonstationarity, partial observability, credit assignment and computa-\ntional complexity. Computational complexity is a universal challenge for all approaches. This\nVenn diagram shows the relations between the surveyed groups of studies and the addressed\nchallenges.\nDeep multiagent Reinforcement Learning\n13\nTable 1 Overview of studies along with the problem representation, main challenges, evaluation domains and solutions\nStudy\nEvaluation\ndomain\nProblem\nrepresentation\nMain challenge(s)\nApproach\nMethod\nMoreno et al. 2021\nRunning with\nScissors\nPOMG\nPartial observability\nOpponent\nmodelling\nLearning recursive belief\nmodels\nSukhbaatar, 2016\nTraﬃc\nJunc-\ntion\nDec-POMDP\nPartial\nobservability,\nnon-stationarity\nCommuni-\ncation\nCommunication\nusing\nbackpropagation\nSunehag et al., 2017\nSwitch\nDec-POMDP\nPartial\nobservability,\nnon-stationarity\nCentralized\ntraining\nand\ndecentralized\nexecution\nValue-Decomposition\nNetworks\nHeinrich and Silver, 2016\nLeduc Poker\nIncomplete\ninfor-\nmation\nextensive-\nform game\nPartial\nobservabi-\nlity,\ncomputational\ncomplexity\nOpponent\nmodelling\nNeural Fictious Self-Play\nBowling et al., 2015\nHeads-up\nlimit hold’em\nPoker\nIncomplete\ninfor-\nmation\nextensive-\nform game\nPartial\nobservabi-\nlity,\ncomputational\ncomplexity\nOpponent\nmodelling\nSelf-play based on coun-\nterfactual\nregret\nmini-\nmization\nSilver et al., 2018\nGo\nComplete informa-\ntion extensive-form\ngame\nComputational\ncom-\nplexity\nOpponent\nmodelling\nSelf-play\nand\nMonte\nCarlo Tree Search\nNguyen et al., 2018\nMatching\ntaxi\nsupply\nand demand\nDec-POMDP\nCredit\nassignment,\npartial\nobservability,\nnon-stationarity\nReward\nsha-\nping\nDiﬀerence rewards: won-\nderful life utility and aris-\ntocratic utility\nLeibo et al., 2017\nSequential\nSocial\nDilemma\nMarkov game\nCredit assignment\nEﬃcient coor-\ndination\nLearn policy dynamics of\nDQN agents by altering\nvariables\nVinyals et al., 2019\nStarCraft\nDec-POMDP\nCredit\nassignment,\npartial observability,\nOpponent\nmodelling\nPopulation-based self-play\nJaderberg et al., 2019\nCapture\nthe\nﬂag\nnonstationarity,\ncom-\nputational complexity\nBerner et al., 2019\nDota 2\nDec-POMDP\nCredit\nassignment,\npartial\nobservability,\nnonstationarity,\ncom-\nputational complexity\nOpponent\nmodelling\nOpenAI\nFive:\nself-play\nagainst\nitself\nand\npast\nselves\n14\nWong et al.\ncan lead to an inﬁnite loop of agents adapting to other agents (Papoudakis et al.,\n2019), and this problem is exacerbated when agents have only a partial view of the\nstate, which means they have less information, and it is even harder to distinguish\nthe eﬀects of their actions from that of other agents. Consequently, agents cannot\ndistil the individual contribution to the team reward, also known as the credit\nassignment problem. We turn to each of these aspects next.\n4.1.1 Computational Complexity\nA current limitation of RL algorithms is the low sample eﬃciency, which requires\nan agent to interact a vast amount of times with the environment to learn a useful\npolicy (Yu, 2018). For example, to teach an agent to play the game of Pong, at least\nten thousand samples are needed, while humans, on average, can master the game\nin dozens of trials (Ding and Dong, 2020). The sample complexity of reinforce-\nment, or the amount of data an agent needs to collect to learn a successful policy\n(Kakade, 2003), worsens when multiple interacting agents are learning simulta-\nneously. Computational complexity in reinforcement learning is then how much\ncomputation, in terms of time and memory requirements, is required to collect\nsuﬃcient data samples to output an approximation to the target (Kakade, 2003).\nA challenge of MARL research is to develop algorithms that can handle this high\nlevel of computational complexity. In particular, on complex or continuous-space\nproblems, we face slow learning of new tasks and, in the worst-case, tasks even\nbecome infeasible to master. Hence, many studies focus on designing better sample\neﬃciency and scalability of algorithms to deal with the computational complexity\nin reinforcement learning.\n4.1.2 Nonstationarity\nIn a multiagent environment, all agents learn and interact with the environment\nconcurrently. The state transitions and rewards are no longer stationary for an\nagent since the new state of the environment is dependent on the joint action of\nall agents instead of the agent’s own behavior. Consequently, agents need to keep\nadapting to other agents’ changing policies. The Markov assumption is violated\nas the state of the environment no longer gives suﬃcient information for optimal\ndecision-making (Van Otterlo and Wiering, 2012), which is problematic since most\nRL algorithms assume a stationary environment to guarantee convergence.\nRecent works have addressed nonstationarity diﬀerently, focusing on various\nvariables: such as the setting, which can be cooperative (Son et al., 2019), com-\npetitive (Berner et al., 2019) or mixed (Leibo et al., 2017), whether and how\nopponents are modelled (Brown, 1951; Bowling et al., 2015), the availability of\nopponent information (Foerster et al., 2018b; He et al., 2016), and whether the\nexecution of actions is centralised (Foerster et al., 2018b; Lowe et al., 2019) or\ndecentralised (Tan, 1993). There is also a wide range of sophistication across algo-\nrithms: some algorithms ignore that the environment is nonstationary, assuming\nthat other agents are part of the environment, while more complex methods in-\nvolve opponent modelling with recursive reasoning (Hernandez-Leal et al., 2019).\nOne way to address nonstationarity is to learn as much as possible about the\nenvironment, for example, using centralised training with decentralised execution\nDeep multiagent Reinforcement Learning\n15\n(section 4.2), through opponent modelling (section 4.3), and exchanging infor-\nmation between agents (section 4.4). For a thorough overview of how algorithms\nmodel and cope with nonstationarity, we refer to recent surveys on nonstationarity\n(Papoudakis et al., 2019; Hernandez-Leal et al., 2019).\n4.1.3 Partial Observabilty\nIn a partially observable environment, agents cannot access the global state and\nmust make decisions based on local observations. This results in incomplete and\nasymmetric information across agents, which makes training diﬃcult. Other agents’\nrewards and actions are not always visible, making it diﬃcult to attribute a change\nin the environment to an agent’s own action. Partial observability has been mainly\nstudied in the setting where a group of agents maximises a team reward via a joint\npolicy (e.g. in the Dec-POMDP setting). The two main approaches for dealing\nwith partial observability are the centralised training and decentralised execu-\ntion paradigm (Kraemer and Banerjee, 2016; Mahajan et al., 2019; Foerster et al.,\n2018b; Lowe et al., 2017) and using communication to exchange information about\nthe environment (Foerster et al., 2016; Mao et al., 2017; Peng et al., 2017).\n4.1.4 Credit Assignment\nTwo credit assignment problems are inherent in multiagent settings. The ﬁrst prob-\nlem is that an agent cannot always determine its individual contribution to the\njoint reward signal due to other concurrently acting agents in the same environ-\nment (Minsky, 1961). This makes learning a good policy more diﬃcult as the agent\ncannot tell whether changes in the global reward were due to its own actions or\nothers in the environment. An alternative to the global reward structure is to let\nagents learn based on a local reward: a reward based on the part of the environ-\nment that an agent can directly observe. However, while an agent may increase\nits local reward more quickly, this approach encourages selﬁsh behaviour that may\nlower overall group performance. Hence, reward shaping methods, the practice of\nsupplying the agent with additional rewards beyond those given by the underlying\nenvironment to improve learning, have been introduced to deal with the credit\nassignment problem (Ng et al., 1999).\nThe second problem involves constructing a reward function to promote eﬀec-\ntive collaborative behaviour. This is especially diﬃcult when mixed incentives exist\nin an environment, such as social dilemmas. The lazy agent problem is also unde-\nsirable (Sunehag et al., 2018): when multiple agents interact simultaneously, and\none agent learns a good policy, the second agent can hold back to avoid aﬀecting\nthe performance of the ﬁrst agent.\n4.2 Centralised Training and Decentralised Execution\nWe will now turn to the approaches developed to address these challenges.\nThe main challenge in DMARL is to design a multiagent training scheme that\nis eﬃcient, and that can deal with the nonstationarity and partial observability\nproblem. Figure 5 shows the three most common training schemes. One of the most\nsimple multiagent training schemes is to train multiple collaborating agents with\n16\nWong et al.\nFig. 5\nOverview of training schemes The three main training schemes in multiagent\nsettings are centralised training and decentralised execution, using a centralised controller and\nindependent learning. The most popular approach is centralised training with decentralised\nexecution, where agents can share information during training, but actions are executed de-\ncentrally based on local observations.\nUsing a centralised controller reduces the problem to a single-agent problem but is\ncomputationally infeasible. Finally, independent learners consider other agents part of the\nenvironment but ignore the nonstationarity problem.\na centralised controller and to reduce it to a single-agent problem. All agents send\ntheir observations and policies to a central controller, and the central controller\ndecides which action to take for each agent. This method mitigates the problem\nof partial observability when agents have incomplete information about the envi-\nronment. However, using a centralised controller is computationally expensive in\nlarge environments and risky as it is a single point of failure. Conversely, all agents\ncan learn an individual action-value function and view other agents as part of the\nenvironment (Tan, 1993). This method does not allow agents to coordinate with\neach other and ignores the nonstationarity problem.\nAn approach combining centralised and decentralised processing is centralised\ntraining and decentralised execution (Kraemer and Banerjee, 2016). The main idea\nis that agents can access extra information during training, such as other agents’\nobservations, rewards, gradients and parameters. Agents then execute their pol-\nicy based on local observations. Centralised training and decentralised execution\nmitigate nonstationarity and partial observability, as access to additional informa-\ntion during training stabilises agents’ learning, even when other agents’ policies are\nchanging. Centralised training and decentralised execution methods can be divided\ninto value-based and policy-based methods. Single-agent value-based methods fo-\ncus on learning and derive the optimal policy via the learned value function. In\nMARL, cooperating agents have to optimise a team value function, and studies\ninvestigate the best way to decompose and optimise this value function. On the\nother hand, traditional policy-based methods search directly for the optimal pol-\nicy. In a multiagent setting, nonstationarity makes learning more challenging as\nall agents update their policies simultaneously. Hence, most policy-based methods\nuse the actor-critic architecture, in which a centralised critic is used to exchange\nextra information during training.\nValue-based methods focus on how to decouple centrally learned value func-\ntions and use them for decentralised execution. Value-function factorisation is one\nDeep multiagent Reinforcement Learning\n17\nof the most popular methods in this category (Sunehag et al., 2018; Rashid et al.,\n2020b; Son et al., 2019; Mahajan et al., 2019; Rashid et al., 2020a; Yang et al.,\n2020a). Value Decomposition Networks (VDN) (Sunehag et al., 2018) decompose\nthe team value function into a sum of linear, individual value functions. The op-\ntimal policy arises by acting greedily with respect to the Q-value, an estimate of\nhow good it is to take an action in a particular state during execution. QMIX\n(Rashid et al., 2020b) improves VDN’s performance by treating the joint value\nfunction as a nonlinear combination of individual value functions and a monotonic\nconstraint. However, this constraint limits the performance of collaborating agents\nthat require signiﬁcant coordination (Rashid et al., 2020a). QTRAN (Son et al.,\n2019) employs a diﬀerent factorisation method that can escape the monotonic-\nity and additivity constraints. However, it relies on regularisations to maintain\ntractable computations, which may impede performance on complex multiagent\nsettings (Mahajan et al., 2019). Numerous algorithms build further upon QMIX.\nFor instance, Weighted QMIX extends QMIX to nonmonotonic environments by\nplacing more weights on joint actions with higher rewards (Rashid et al., 2020a).\nMultiagent Variational Exploration (MAVEN) (Mahajan et al., 2019) addresses\nthe ineﬃcient exploration problem in QMIX via committed exploration: coordi-\nnated exploratory actions over extended time steps in dealing with environments\nthat require long-term coordination. MAVEN uses a hybrid value and policy-based\nmethod approach by conditioning value-based agents on the shared latent variable\ncontrolled by a hierarchical policy. Value-Decomposition Actor-Critic (VDAC) en-\nforces the same monotonic relationship between the global state-value and the\nlocal state-values as QMIX. However, unlike QMIX, VDAC is compatible with\nA2C, which makes sampling more eﬃcient. In addition, the study demonstrates\nthat following a simple gradient calculated from a temporal-diﬀerence advantage,\nthe policy can converge to a local optimal (Su et al., 2021). Q-DPP (Yang et al.,\n2020b) do not rely on constraints to decompose the global value function. Instead,\nit builds upon determinantal point processes: probabilistic models that capture\nboth quality and diversity when a subset is sampled from a ground set, allowing\nfor a natural factorisation of the global value function.\nPolicy-based methods mainly focus on the actor-critic architecture (see sub-\nsection 2.3). These studies use a centralised critic to train decentralised actors.\nCounterfactual multiagent (COMA) (Foerster et al., 2018b) uses a centralised\ncritic to approximate the Q-function and decentralised actors to optimise policies.\nThe centralised critic has access to the joint action and all available state infor-\nmation, while each agent’s policy only depends on its historical action-observation\nsequence. Along the same line, multiagent Deep Deterministic Policy Gradient\n(MADDPG) extends the Actor-Critic algorithm so that the critic has access to\nextra information during training and the actor only has access to local informa-\ntion (Lowe et al., 2017). As opposed to COMA, which uses one centralised critic\nfor all agents, MADDPG has a centralised critic for each agent to have diﬀerent\nreward functions in competitive environments. MADDPG can learn continuous\npolicies, whereas COMA focuses on discrete policies. Several studies build upon\nMADDPG. For instance, R-MADDPG (Wang et al., 2019) extends the MADDPG\nalgorithm to the partially observable environment by having both a recurrent actor\nand critic that keep a history of previous observations, and M3DDPG (Li et al.,\n2019) incorporates minimax optimisation to learn robust policies against agents\nwith changing strategies. Since these methods concatenate all the observations in\n18\nWong et al.\nthe critic, the input dimension increases exponentially with each agent. Hence,\nseveral studies have devised more eﬃcient methods to deal with this problem. For\ninstance, Mean-Field Actor-Critic (Yang et al., 2018) factorises the Q-function us-\ning only the interaction with the neighbouring agents based on mean-ﬁeld theory\n(Stanley, 1971), and the idea of dropout3 can be extended to MADDPG to handle\nthe large input space (Kim et al., 2019).\nCentralised training and decentralised execution have been applied to solve\ncomplex strategy games such as StarCraft Micromanagement (Foerster et al.,\n2018b) and hide-and-seek (Baker et al., 2019).\n4.3 Opponent Modelling\nOpponent modelling belongs to the class of model-based methods (Markovitch and\nReger, 2005) and refers to the construction of models of the beliefs, behaviours,\nand goals of other agents in the environment (Albrecht and Stone, 2018). An agent\ncan use these opponent models to guide decision-making. Opponent modelling al-\ngorithms generally take a sequence of interactions with the modelled opponent as\ninput and predict action probabilities as output. After generating the opponent’s\nmodel, an agent can derive its policy based on that model. This method helps an\nagent discover the competitor’s intentions and weaknesses. Learning the model is\ngenerally considered more data-eﬃcient than model-free approaches in which the\npolicy is updated from direct observations (Markovitch and Reger, 2005). Oppo-\nnent modelling mitigates the nonstationarity and partially observability problem\nas agents collect historical observations to learn about the environment (i.e. op-\nponents), allowing agents to track and switch between policies. This method is\nespecially beneﬁcial in the adversarial setting when the opponent has opposing in-\nterests, and other approaches such as communication and centralised training that\nrequire the opponents’ information are unlikely. For a comprehensive overview of\nopponent modelling, we refer to other work (Albrecht and Stone, 2018).\nEarly opponent modelling methods assumed ﬁxed play of opponents. Neural\nFictitious Self-Play (NFSP) extends the idea of ﬁctitious play (Brown, 1951) with\nneural networks to approach a Nash equilibrium in imperfect information games\nsuch as Poker (Heinrich and Silver, 2016). The main idea is to keep track of the\nopponents’ historical behaviours and to choose a best response to the opponents’\naverage strategies.\nWhile NFSP requires actual interaction with the opponent, other methods do\nnot. For instance, counterfactual regret minimisation has achieved success in poker\n(Bowling et al., 2015). AlphaZero achieved remarkable results in Go, chess, and\nShogi, using a neural network with self-play and Monte Carlo Tree Search (Silver\net al., 2017). MuZero was able to achieve this without a given model. Instead\nof modelling the entire environment, it focused on the three core elements most\nrelevant for planning: the value, policy and reward (Schrittwieser et al., 2020).\nStill, these studies assume that the opponent follows a stationary strategy.\nLater approaches look at nonstationary environments in which an agent has to\ntrack, switch, and possibly predict behaviour. Several studies achieved superhuman\n3 Randomly dropping units in the neural network to avoid overﬁtting (Srivastava et al.,\n2014).\nDeep multiagent Reinforcement Learning\n19\nperformance using self-play in real-time strategy games characterised by long time\nhorizons, nonstationary environments, partially-observed states, and high dimen-\nsional state and action spaces. OpenAI Five employs a similar method to ﬁctitious\nplay in playing Dota 2, a video game in which two teams compete to conquer\neach other’s base, but the algorithm learns a distribution over opponents and uses\nthe latest policy instead of the average policy (Berner et al., 2019). This infras-\ntructure has also been used to solve hide-and-seek, but hide-and-seek agents can\nact independently as the training scheme is centralised training and decentralised\nexecution (Baker et al., 2019). In Capture-the-Flag and StarCraft II, a population\nof agents is trained to introduce variation. Policies are made more robust by let-\nting agents play with sampled opponents and teammates from this population in\na league (Jaderberg et al., 2019; Vinyals et al., 2019).\nSome studies assume that the opponent switches between a set of station-\nary policies over time (He et al., 2016; Everett and Roberts, 2018; Zheng et al.,\n2018b). These algorithms derive the optimal policy based on the learned oppo-\nnent’s model and identify when the opponent changes the behaviour, and the\nagent has to relearn a new policy. Over time, the agent has a library of inferred\nopponent strategies and associated best response policies. The two main chal-\nlenges are designing a policy detection mechanism and learning a best-response\npolicy. Some studies use a variant of Bayes’ rule to learn opponent models and\nassign probabilities to the opponent’s available actions. An agent starts with a\nprior belief that is continually updated during interaction to make it more accu-\nrate. Switching Agent Model (SAM) learns opponent models from observed state-\naction trajectories in combination with a Bayesian neural network (Everett and\nRoberts, 2018). A Deep Deterministic Policy Gradient algorithm (Lillicrap et al.,\n2016) is used to learn the best response. Distilled Policy Network-Bayesian Policy\nReuse+ (DPN-BPR+) (Zheng et al., 2018b) extends the Bayesian Policy Reuse+\nalgorithm (BPR+) (Hernandez-Leal et al., 2016) with a neural network to detect\nthe opponent’s policy via both its behaviour and the reward signal. The latter\nuses policy distillation (Rusu et al., 2016) to learn and reuse policies eﬃciently.\nOthers use a form of deep Q-learning (Mnih et al., 2013). Deep Reinforcement\nOpponent Network (DRON) (He et al., 2016) uses one network to learn the Q-\nvalues to derive an optimal policy and a second network to learn the opponent\npolicy representation, in addition to expert networks that capture diﬀerent types\nof opponent strategies. A drawback of DRON is that it relies on handcrafted op-\nponent features. Previous methods assume that the opponent remains stationary\nwithin an episode. Deep Policy Inference Q-Network (DPIQN) and Deep Recur-\nrent Policy Inference Q-Network (DRPIQN) (Hong et al., 2018) incorporate policy\nfeatures as a hidden vector into the deep Q-network to adapt itself to unfamiliar\nopponents. DRPIQN uses a Long Short Term Memory (LSTM) layer so agents can\nlearn in partially observable environments. This LSTM layer utilises a recurrent\nneural network architecture that can take observations as input and allow agents\nto model time dependencies and capture the underlying state (Hausknecht and\nStone, 2015).\nPrevious approaches do not consider an intellectual and reasoning opponent.\nAccording to the theory of mind, people attribute mental states to others, such as\nbeliefs, intents and emotions (Premack and Woodruﬀ, 1978). These models help to\nanalyse and infer others’ behaviours and are essential in social interaction (Frith\nand Frith, 2005). Learning with Opponent-Learning Awareness (LOLA) (Foerster\n20\nWong et al.\nFig. 6\nBasic referential game In this basic referential game example, two agents have to\ndevelop a communication protocol so that the speaker can translate the target into a message\nand the listener can understand which one is the target. The game works as follows. The\nspeaker receives as input three images. One is the target, and the other two are distractions.\nThe speaker has to use the symbols in the vocabulary, which consists of the symbols \"B\", \"C\",\n\"X\" and \"Z\", to send a message to the listener. The listener sees the messages and has to\nguess the target message. If the target is correct, both agents receive a reward.\net al., 2018a) anticipates and shapes opponents’ behaviour. Speciﬁcally, it includes\na term that considers the impact of an agent’s policy on the learning behaviour\nof opponents. One drawback is that LOLA assumes access to the opponent’s pa-\nrameters, which is unlikely in an adversarial setting. Others focus on recursive\nreasoning by learning models over the belief states of other players, a nesting of\nbeliefs that can be represented in the form: \"I believe that you believe that I be-\nlieve\" (Wen et al., 2019; Tian et al., 2021). The Probabilistic Recursive Reasoning\n(PR2) framework (Wen et al., 2019) ﬁrst reﬂects on the opponent’s perspective:\nwhat the opponents would do given that the opponents know the agent’s current\nstate and action. Given the potential actions of the opponent, the agent selects\na best response. The recursive reasoning process can be viewed as a hierarchical\nprocess with k-levels of reasoning. At level k = 0, agents take random actions\n(Dai et al., 2020) or act based on historical interactions, the main assumption in\ntraditional opponent modelling methods (Wen et al., 2019). At k = 1, an agent\nselects its best response to the agents acting at lower levels. Studies show that it\npays oﬀto reason about an opponent’s intelligence levels (Tian et al., 2021) and\nthat reasoning at a higher level is beneﬁcial as it leads to faster convergence (Dai\net al., 2020) and better performance (Moreno et al., 2021).\n4.4 Communication\nThrough communication, agents can pass information to reduce the complexity\nof ﬁnding good policies. For instance, agents exploring diﬀerent parts of the envi-\nronment can share observations to mitigate partial observability and share their\nintents to anticipate each others’ actions to deal with nonstationarity. Communi-\ncation can also be used for transfer learning so that more experienced agents can\nshare their knowledge to accelerate the learning of inexperienced agents (Taylor\nand Stone, 2009). One of the fundamental questions in communication is how lan-\nguage emerges between agents with no predeﬁned communication protocol (Lazari-\ndou et al., 2017), and, subsequently, how meaning and syntax evolve through inte-\nraction (Jaques et al., 2019). Learning this process will help researchers better\nDeep multiagent Reinforcement Learning\n21\nunderstand human language evolution and contribute to more eﬃcient problem-\nsolving in a team of interacting agents (Lazaridou and Baroni, 2020).\nSeveral studies investigate how agents learn a successful communication pro-\ntocol. A communication protocol should inform agents which concepts to commu-\nnicate and how to translate these concepts into messages (Hausknecht and Stone,\n2016). Many studies approach this problem as a referential game (Lazaridou et al.,\n2017; Havrylov and Titov, 2017). A referential game involves two or more agents\nin which speakers and listeners must develop a communication protocol to refer to\nan object (Figure 6). In the basic version of this game with two agents, the speaker\nsends two images and a message from a ﬁxed vocabulary to the receiver. One of\nthe images is the target, which the listener has to identify based on the message.\nBoth agents receive a reward when the classiﬁcation is correct (Lazaridou et al.,\n2017). To succeed in this game, agents must understand the image content and\nexpress the content through a common language. The language can be discrete,\nwhere messages are a single symbol (Lazaridou et al., 2017) or a sequence of sym-\nbols (Havrylov and Titov, 2017), or continuous, where messages are continuous\nvectors (Sukhbaatar et al., 2016).\nUsing DRL, end-to-end policies can be learned in which agents receive image\npixels as input and a corresponding message as output. For example, two agents\nrepresented as simple feed-forward networks can learn a communication protocol\nto solve the basic referential game (Lazaridou et al., 2017). Language also emerges\nin more complicated versions of the game that require dialogue (Jorge et al., 2017;\nDas et al., 2017; Kottur et al., 2017) or negotiation (Cao et al., 2018) between\nagents. Agents trained with deep recurrent Q-networks (DRQN) (Jorge et al.,\n2017) and REINFORCE (Das et al., 2017; Kottur et al., 2017) are able to learn a\ncommunication protocol from scratch. Since communication is not always mean-\ningful, it is important to develop metrics for emergent communication. An example\nis when an agent sends a message that has no actual impact on the environment.\nAgents with the capacity to communicate should exhibit positive signalling and\npositive listening (Lowe et al., 2019). Positive signalling means that messages cor-\nrelate with observations or actions, and positive listening refers to updating beliefs\nor behaviour after receiving a message. Most studies focus solely on positive sig-\nnalling metrics. However, positive signalling may occur without positive listening\n(Lowe et al., 2019), which indicates that there was no actual communication.\nIn contrast to earlier works that consider communication as the primary learn-\ning goal, other works consider communication an instrument to learn a speciﬁc\ntask. Most of these studies focus on coordination in collaborative environments\nand show that communication improves overall performance. Diﬀerentiable Inter-\nagent Learning (DIAL) (Foerster et al., 2016) uses centralised training and de-\ncentralised execution. Communication is continuous during training and discrete\nduring the execution of the task. Continuous communication during training is par-\nticularly eﬀective as it enables the exchange of gradients between agents, which\nimproves performance. CommNet shows that the exchange of discrete symbols is\nless eﬃcient than continuous communication, as the latter enables the use of back-\npropagation to train agents eﬃciently (Sukhbaatar et al., 2016). While DIAL and\nCommNet base their approach on DQRN, later studies propose the Actor-Critic\narchitecture, including Actor-Coordinator-Critic Net (ACCNet) (Mao et al., 2017),\nBidirectionally Coordinated Network (BiCNet) (Peng et al., 2017) and MADDPG\n(Lowe et al., 2017). This architecture can solve more complex problems than pre-\n22\nWong et al.\nvious approaches and works for continuous actions. In addition, when critics are\nindividually learned (Jiang and Lu, 2018) instead of centrally computed (Iqbal\nand Sha, 2019), agents have diﬀerent reward functions, which is suitable for com-\npetitive settings.\nCommunication also allows peer-to-peer teaching. More experienced agents\ncommunicate their knowledge to learning agents, accelerating the learning of a new\ntask (Da Silva et al., 2017; Omidshaﬁei et al., 2019; Ilhan et al., 2019; Amir et al.,\n2016). However, having agents send messages to all agents is costly and ineﬃcient.\nThus, an important question is how to ﬁlter the most important messages and to\nwhom to send them. One approach is to limit communication bandwidth (Foerster\net al., 2016; Kim et al., 2020) or use a communication budget (Ilhan et al., 2019;\nOmidshaﬁei et al., 2019). Others use metrics to identify relevant messages, such as\nattention mechanisms. In its simplest form, this is a vector of importance weights\n(Peng et al., 2018; Gu et al., 2021; Mao et al., 2020). An alternative is to keep\nconﬁdence scores about states (Da Silva et al., 2017). However, communication\ncomes at a cost and increased complexity. Negative transfer can also happen,\nfor example, when the message contains inaccurate or noisy information so that\nperformance may become worse (Taylor and Stone, 2009). Therefore, it is essential\nto trade oﬀbeneﬁts and costs or ﬁnd a better way to ﬁlter valuable information.\n4.5 Eﬃcient Coordination\nAnother group of studies investigate agents’ emergent behaviours and look at how\ncooperating agents can coordinate actions most eﬃciently. These studies are con-\nducted in mixed environments with elements of both cooperation and competition.\nA key question is how to design reward functions so that agents adapt to\neach others’ actions, avoid conﬂicting behaviour and achieve eﬃcient coordina-\ntion. By engineering the reward function, competitive or cooperative behaviours\ncan be stimulated (Tampuu et al., 2017). While early studies look at how agents\ncan maximise external rewards, recent works assume that agents are intrinsically\nmotivated.\nMost studies look at multiagent behaviour in social dilemmas (Eccles et al.,\n2019; Lerer and Peysakhovich, 2018; Leibo et al., 2017; McKee et al., 2020; Jaques\net al., 2019; Peysakhovich and Lerer, 2018). Earlier studies, mainly inﬂuenced by\ngame theory, have looked at social dilemmas as a matrix game in which agents\nchoose pure cooperation or pure defect. Recent studies generalise these social\ndilemmas to temporally and spatially extended Markov games, also known as a se-\nquential social dilemma (Leibo et al., 2017). This setting is more realistic as people\ncan adapt and change their strategies. One notable example is the repeated pris-\noner’s dilemma. In each turn, each agent decides whether to cooperate or defect.\nWhen both agents cooperate, both agents get good rewards. Contrary, defection\nimproves one agent’s reward at the expense of the other agent. Thus, an agent can\ndecide to retaliate or trust the opponent, dependent on the actions in the previous\nround.\nOne of the ﬁrst sequential social dilemma studies examined how policies change\ndue to environmental factors or agent properties (Leibo et al., 2017). They found\nthat agents learn more aggressive policies when resources are limited. In addition,\nDeep multiagent Reinforcement Learning\n23\nmanipulating the discount rate over the rewards, batch size, and the number of hid-\nden units in the network aﬀected emerging social behaviour. While this study took\na descriptive approach to understand how behaviours change to diﬀerent rules and\nconditions, others took a prescriptive approach in which agents learn to cooperate\nwithout being exploited (Lerer and Peysakhovich, 2018; Wang et al., 2018). The\ngeneral approach comprises two steps: ﬁrst, detect the level of cooperation of the\nopponent, and then mimic or reciprocate with a slightly higher-level cooperation\npolicy to induce cooperation without getting exploited. This approach is based\non the Tit-for-Tat principle (Axelrod and Hamilton, 1981): the strategy suggests\ncooperation in the ﬁrst round and copies the opponent’s behaviour afterwards.\nPrevious approaches assume that the only incentive for cooperation is the\nexternal reward. However, there is a rapidly growing literature where cooperation\noccurs from social behaviour and intrinsic motivation (McKee et al., 2020; Jaques\net al., 2019; Peysakhovich and Lerer, 2018; Hughes et al., 2018).\nPsychology research has shown that people do not always seek to maximise\nutility (Dovidio, 1984). In addition, an intrinsic reward may be a good alternative\nin sparse environments. Several attempts have been made to design these internal\nrewards. For instance, inequity aversion, which refers to the preference for fair-\nness and resistance against inequitable outcomes (Fehr and Schmidt, 1999), has\nimproved coordination in social dilemmas (Hughes et al., 2018). The main idea is\nto punish agents that deviate too much from the average behaviour. Underper-\nforming and overperforming agents are both undesirable, as the ﬁrst may exhibit\nfree-riding behaviour while the latter may be operating a defective policy. Another\napproach is to make agents care about the rewards of teammates (Peysakhovich\nand Lerer, 2018; Jaques et al., 2019).\nPro-social behaviour improves the convergence probabilities of policy gradient-\nbased agents, even if only one of the two players displays social behaviour\n(Peysakhovich and Lerer, 2018). In addition, rewarding actions that lead to a\nrelatively more signiﬁcant change in the other agent’s behaviour may lead to in-\ncreased cooperation (Jaques et al., 2019). Another study introduces heterogeneity\nin intrinsic motivation (McKee et al., 2020). Speciﬁcally, the study compares a\nteam of homogeneous agents, who share the same degree of social value orien-\ntation, to a heterogeneous group of agents with diﬀerent degrees of social value\norientation. The results show that homogeneous altruistic agents earn relatively\nhigh rewards, yet it appears that they adopt a lazy agent approach and produce\nhighly specialised agents. This problem is not evident in heterogeneous groups.\nHence, it shows that the widely adopted joint return approach may be undesirable\nas it masks high levels of inequality amongst agents.\nWhile studies show that shaping reward functions can lead to better coordina-\ntion (Devlin et al., 2011; Holmesparker et al., 2016; Peysakhovich and Lerer, 2018;\nTampuu et al., 2017; Jaques et al., 2019; Liu et al., 2019), it is very challenging to\ntune the trade-oﬀbetween the intrinsic and external reward, and whether it gives\nrise to cooperative behaviour may depend on the actual task and environment.\n4.6 Reward Shaping\nThe credit assignment problem refers to the situation when individual agents can-\nnot view their contribution to the joint team reward due to a partially observable\n24\nWong et al.\nTable 2 Overview of solutions to the credit assignment problem\nStudy\nImplicit/\nApproach\nAlgorithm\nexplicit\nFoerster et al. 2018\nExplicit\nDiﬀerence\nrewards\nCOMA: uses a counterfactual base-\nline to marginalise out the action of\nan agent.\nYu et al., 2019\nExplicit\nPotential-\nbased rewards\nMA-AIRL: extends maximum en-\ntropy inverse reinforcement learning\nto Markov games. A potential-based\nfunction is used to deal with reward\nshaping ambiguity.\nDevlin et al., 2014\nExplicit\nDiﬀerence\nrewards\nand\npotential-\nbased rewards\nDriP:\nuses\npotential\nbased\nre-\nward shaping to improve diﬀerence\nrewards.\nSunehag et al., 2017\nImplicit\nValue-based:\ndeep\nQ-\nlearning\nVDN: decomposes the team value\nfunction into a sum of linear, indivi-\ndual value functions.\nZhou et al., 2020\nImplicit\nPolicy-based:\nactor-critic\nLICA: a centralized critic maps cur-\nrent state information into a set of\nweights, and in turn, mixes indivi-\ndual action vectors into the joint ac-\ntion value estimate.\nenvironment. Researchers have introduced implicit and explicit reward shaping\nmethods to deal with this problem. Table 2 gives an overview of the reviewed\nreward shaping methods.\nThe general solution to this problem is reward shaping, with diﬀerence rewards\nand potential-based reward shaping as the two main classes. Diﬀerence rewards\nconsider both the individual and the global reward (Foerster et al., 2018b; Proper\nand Tumer, 2012; Nguyen et al., 2018; Castellini et al., 2021) and help an agent\nunderstand its impact on the environment by removing the noise created by other\nacting agents. Speciﬁcally, it is deﬁned as Di(z) = G(z) −G(z −zi) where Di is\nthe diﬀerence reward of agent i, G(z) is the global reward considering the joint\nstate-action z, and G(z −zi) is a modiﬁed version of the state-action vector z\nin which agent i takes a default action, or more intuitively, the global reward\nwithout the contribution of agent i (Yliniemi and Tumer, 2014). COMA (Foerster\net al., 2018b) takes inspiration from diﬀerence rewards. The centralised critic uses\na counterfactual baseline to reason about counterfactuals or alternatives to the\nstate when only that agent’s actions change. To marginalise out the action of\nan agent, an expected value is calculated over all the actions of an agent while\nkeeping other agents’ actions constant. Potential-based reward shaping has also\nreceived attention lately (Suay et al., 2016; Devlin et al., 2014). Formally, it is\ndeﬁned as F(s, s′) = γΦ(s′) −Φ(s) (Ng et al., 1999) where Φ(s) is a potential\nfunction which returns the potential for state s and γ is the discount factor. It\nis a method to incorporate additional information into the reward function to\naccelerate learning. This approach has been proven not to alter the set of Nash\nequilibria in a Markov game (Devlin and Kudenko, 2011), even when the potential\nfunction changes dynamically during learning (Devlin and Kudenko, 2012), and\ncombining the two approaches allows agents to converge signiﬁcantly faster than\nusing diﬀerence rewards alone (Devlin et al., 2014). However, these reward shaping\nmethods require manual tuning for each environment, which is ineﬃcient. Some\nDeep multiagent Reinforcement Learning\n25\nstudies have therefore started looking into the automatic generation of reward\nshaping, for example, through abstractions derived from an agent’s experience\n(Burden, 2020) or via meta-learning on a distribution of tasks (Zou et al., 2021).\nPrevious approaches evaluate an agent’s action against a baseline to extract its\nindividual eﬀect and belong to the class of explicit credit assignment. In contrast,\nimplicit methods do not work with baselines. Value-based methods decomposes the\nglobal value function into individual state-action values, also known as value mix-\ning methods, such as VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2020b) and\nQTRAN (Son et al., 2019) to ﬁlter out agent’s individual contribution. However,\nthese methods may not handle continuous action spaces eﬀectively. Policy-based\nalgorithms include Learning Implicit Credit Assignment (LICA) (Zhou et al., 2020)\nand Decomposed multiagent Deep Deterministic Policy Gradient (DE-MADDPG)\n(Sheikh and Bölöni, 2020). LICA extends the idea of value mixing to policy-based\nmethods. Under the centralised training and decentralised execution framework,\na centralised critic is represented by a hypernetwork that maps state informa-\ntion into a set of weights that mixes individual action values into the joint action\nvalue. DE-MADDPG extends previous deterministic policy gradient methods us-\ning a dual-critic framework. The global critic takes as input all agents’ observations\nand actions and estimates the global reward. The local critic receives as input only\nthe local observation and action of an agent and estimates the local reward. This\nframework achieves better and more stable performance than earlier deterministic\npolicy gradient methods.\n5 Discussion\nWe have surveyed a range of studies in DMARL. While integrating deep neural\nnetworks in reinforcement learning has dramatically improved agents’ learning in\nmore complex and larger environments, we wish to highlight current limitations\nand open challenges in the ﬁeld.\n– In the development from single-agent reinforcement learning to multiagent re-\ninforcement learning, most earlier studies used a game-theoretic lens to study\ninteractive decision-making, assuming perfectly rational agents who maximise\ntheir behaviour through a deliberate optimisation process. However, while\ngame theory’s strength lies in its generalizability and mathematical precision,\nexperiments have shown that it is often a poor representation of actual human\nbehaviour (Colman, 2003). Researchers must consider irrational and altruistic\ndecision-making, especially if we wish to extend artiﬁcial intelligence (AI) to\nmore realistic environments or design applications for human-AI interaction in\nlarger and more complex problems. We have seen that pro-social agents can\nachieve better group outcomes (Peysakhovich and Lerer, 2018; Hughes et al.,\n2018). However, studies are still limited, and we encourage fellow researchers\nto deepen our understanding in this ﬁeld.\n– We also want to bring attention to the design and assumptions in current re-\nsearch. Many studies assume homogeneous agents; from a practical viewpoint,\nthis may accelerate learning since agents can share policies and parameters.\nAgents thus only need to learn one policy and may better anticipate the be-\nhaviour of other agents. However, whether this also leads to better performance\n26\nWong et al.\nin the ﬁnal task is an open question. For instance, a soccer team usually con-\nsists of forwards, midﬁelders, defenders and a goalkeeper. The team’s success\nis partly determined by how well each fulﬁls these diﬀerent roles. Thus, an in-\nteresting question is whether letting each agent learn its own policy and have\nheterogeneous teams pays oﬀ. While homogeneous agents can still act diﬀer-\nently due to diﬀerent observations input, the observation space must be the\nsame size. This assumption does not always hold. For instance, agents have\ndiﬀerent observation spaces in soccer as individuals occupy diﬀerent positions\non the ﬁeld. Preliminary results show that despite making the learning slower\nat the beginning, heterogeneous teams perform better at the ﬁnal task (Kurek\nand Jaśkowski, 2016). Another study provides formal proof for parameter shar-\ning between heterogeneous agents (Terry et al., 2021), which may mitigate the\nslow start problem.\n– Studies may also rely on unrealistic assumptions. For instance, multiple studies\nrequire access to opponents’ information, such as trajectories or parameters,\nwhile their problem domain actually gives an incentive to hide information.\nOthers assume ﬁxed behaviours of agents or that agents can view the global\nstate.\n– Another issue is the generalizability of studies. For example, many studies re-\nquire handcrafted features or rewards speciﬁc to the environment. In addition,\na majority of the studies are evaluated in two-player games. As a result, a dan-\nger exists that the agent’s policy overﬁts to the behaviour of the second agent\n(i.e. the lazy agent problem) and does not generalise to other settings. Future\nresearch should integrate more realistic assumptions and work on the general-\nizability of studies to settings with more players or diﬀerent environments.\nWhile DMARL has seen a signiﬁcant improvement in the types and complex-\nities of challenges it can address, several hurdles remain. For example, problems\nassociated with large search spaces, partially observable environments, nonstation-\narity, sparse rewards and the exploration-exploitation trade-oﬀremain challenging.\nThese issues are partly due to computational constraints, such that assumptions\nare often relaxed. We want to point out two other research areas, namely evolu-\ntionary algorithms and psychology, that may help researchers address some of the\nopen questions.\n5.1 Evolutionary Algorithms\nEvolutionary algorithms (EAs) are inspired by nature’s creativity and simulate\nthe process of organic evolution to solve optimisation problems. In simple terms,\na randomly initialised population of individual solutions evolves toward better re-\ngions of the search space via selection, mutation and recombination operators. A\nﬁtness function evaluates the quality of the individuals and favours the reproduc-\ntion of those with a higher ﬁtness score, while mutation maintains diversity in the\npopulation (Bäck and Schwefel, 1993). An early study sheds light on how EAs deal\nwith RL problems (Moriarty et al., 1999) and has been conﬁrmed by recent studies\n(Bloembergen et al., 2015; Drugan, 2019; Arulkumaran et al., 2019; Lehman et al.,\n2018a,b,c; Such et al., 2018; Zhang et al., 2017). EAs oﬀer a novel perspective to\nscaling RL multiagent systems as it is highly parallelisable, and there is no need\nfor backpropagation (Such et al., 2018; Majumdar et al., 2020).\nDeep multiagent Reinforcement Learning\n27\nEAs have been compared with popular value-based and policy-gradient algo-\nrithms such as DQN and A3C (Such et al., 2018). Novelty search (Such et al., 2018;\nLehman et al., 2018c) is a promising area (Lehman and Stanley, 2008) since it\nencourages exploration on tasks with sparse rewards and deceptive local optima—\nproblems that remain an issue with conventional reward-maximising methods. EAs\nhave been shown to work well with nonstationarity and partial observability, as it\ncontinually uses and evolves a population of agents instead of a single agent (Mo-\nriarty et al., 1999; Liu et al., 2020). EAs can evolve agents with diﬀerent policies\n(Gomes et al., 2017, 2014; Nitschke et al., 2012), such that heterogeneity can be\nintroduced in team-based learning. Population-based training has proven powerful\nin achieving superhuman behaviour in Capture the Flag (Jaderberg et al., 2019),\nand StarCraft (Vinyals et al., 2019).\n5.2 Psychology\nMany key ideas in reinforcement learning, such as operant conditioning and trial-\nand-error, originated in psychology and cognitive science research (Sutton et al.,\n1998). Interestingly, recent DMARL studies started moving towards more human-\nlike agents, showing that characteristics like reciprocity and intrinsic motivation\npay oﬀ.\nWe believe psychology may provide more valuable insights into current prob-\nlems in DMARL. For instance, bounded rationality models (Simon, 1990, 1957)\ndescribe how individuals make decisions under a ﬁnite amount of knowledge, time\nand attention. To deal with bounded rationality, people use heuristics, or men-\ntal shortcuts, to solve problems quickly and eﬃciently (Gigerenzer and Goldstein,\n1996). While RL research already uses heuristics to deal with large and complex\nproblems (Cheng et al., 2021; Ma et al., 2021), selecting suitable heuristics is still\ninsuﬃciently explored. Psychology has a long tradition of investigating heuristics\nand may oﬀer new perspectives. In addition, heuristics aid in ﬁltering relevant\ninformation in a complex world, which may beneﬁt agents in partially observable\nenvironments or counter negative knowledge transfer (Marewski et al., 2010). How-\never, intuitive judgement can also lead to biases and suboptimal decision-making\n(Gilovich et al., 2002).\nHumans are also capable of creative problem-solving, a prerequisite for inno-\nvation. Likewise, agents need to explore the environment to ﬁnd more optimal\nsolutions. A ﬁrst approach of combining creativity with reinforcement learning\nshows that creativity oﬀers the potential to explore promising solution spaces,\nwhereas traditional methods fail (Colin et al., 2016).\nLastly, psychology can play an essential role in helping researchers understand\nhow agents make decisions and tackle the black-box problem of deep neural net-\nworks. Cognitive psychologists have developed robust models of human behaviour,\nsuch as decision making, attention and language, without observing these pro-\ncesses directly but through controlled behavioural experiments in which cognitive\nfunctions can be isolated (Taylor and Taylor, 2021). Open-source platforms are\nnow also available (Leibo et al., 2018) that allow researchers to use methods from\ncognitive psychology to study the behaviours of artiﬁcial agents in a controlled\nenvironment. We encourage researchers to draw from psychology research and its\n28\nWong et al.\nmethodologies to analyse agents’ complex interactions and better understand and\nimprove their decision-making.\n6 Conclusion\nThe current survey has presented an overview of the challenges inherent in multi-\nagent representations. We have identiﬁed ﬁve diﬀerent research areas in DMARL\nthat aim to mitigate one or multiple of these challenges: (1) centralised training\nand decentralised execution, (2) opponent modelling, (3) communication, (4) eﬃ-\ncient coordination, and (5) reward shaping. While early studies drew inspiration\nfrom game theory and were evaluated on grid-based games, the ﬁeld is moving to-\nwards more sophisticated and realistic representations. Nevertheless, dealing with\nlarge problem spaces and sparse rewards in nonstationary and partially observable\nsettings remains an open issue.\nExisting research has approached this problem mainly from traditional, com-\nputational, RL perspectives. While combining deep learning with value-based and\npolicy-based methods has been shown to mitigate the problem, they seem to be\nonly part of the answer. We encourage researchers to take an interdisciplinary\nperspective on developing new solutions and beneﬁt from the knowledge of other\nresearch domains. Speciﬁcally, evolutionary algorithms oﬀer insights into dealing\nwith larger, nonstationary and partially observable environments. At the same\ntime, sociology and psychology increase our understanding of agents’ reasoning\npatterns and oﬀer us alternatives in dealing with sparse rewards, such as intrin-\nsic motivation. Finally, we believe that integrating multiple research disciplines\nleads to more realistic scenarios humans encounter in practice, so the ﬁndings\nmay eventually be fruitful in real-world applications.\n7 Declarations\n7.1 Funding\nNo funding was received to assist with the preparation of this manuscript.\n7.2 Conﬂicts of interests\nThe authors have no conﬂicts of interest to declare that are relevant to the content\nof this article.\n7.3 Availability of data and material\nNot applicable\n7.4 Code availability\nNot applicable\nDeep multiagent Reinforcement Learning\n29\n7.5 Data availability\nData sharing not applicable to this article as no datasets were generated or anal-\nysed during the current study.\nReferences\nAlbrecht SV, Stone P (2018) Autonomous agents modelling other agents: A com-\nprehensive survey and open problems. Artiﬁcial Intelligence 258:66–95\nAmato C, Oliehoek F (2015) Scalable planning and learning for multiagent\npomdps. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol 29, pp 1995–2002\nAmir\nO,\nKamar\nE,\nKolobov\nA,\nGrosz\nB\n(2016)\nInteractive\nteach-\ning\nstrategies\nfor\nagent\ntraining.\nIn:\nProceedings\nof\nthe\nTwenty-\nFifth\nInternational\nJoint\nConference\non\nArtiﬁcial\nIntelligence\n2016,\nURL\nhttps://www.microsoft.com/en-us/research/publication/\ninteractive-teaching-strategies-agent-training/\nArulkumaran K, Deisenroth MP, Brundage M, Bharath AA (2017) Deep reinforce-\nment learning: A brief survey. IEEE Signal Processing Magazine 34(6):26–38\nArulkumaran K, Cully A, Togelius J (2019) Alphastar: An evolutionary computa-\ntion perspective. In: Proceedings of the Genetic and Evolutionary Computation\nConference Companion, pp 314–315\nÅström KJ (1965) Optimal control of markov decision processes with incomplete\nstate estimation. Journal of Mathematical Analysis and Applications 10:174–205\nAxelrod\nR,\nHamilton\nWD\n(1981)\nThe\nevolution\nof\ncooperation.\nScience\n211(4489):1390–1396\nBäck T, Schwefel HP (1993) An overview of evolutionary algorithms for parameter\noptimization. Evolutionary Computation 1(1):1–23\nBahdanau D, Brakel P, Xu K, Goyal A, Lowe R, Pineau J, Courville A, Bengio Y\n(2017) An actor-critic algorithm for sequence prediction. In: International Con-\nference on Learning Representations, URL https://openreview.net/forum?\nid=SJDaqqveg\nBaker B, Kanitscheider I, Markov T, Wu Y, Powell G, McGrew B, Mordatch I\n(2019) Emergent tool use from multi-agent autocurricula. In: Eigth International\nConference on Learning Representations (ICLR), ICLR\nBao W, Liu Xy (2019) Multi-agent deep reinforcement learning for liquidation\nstrategy analysis. arXiv preprint arXiv:190611046\nBellman R (1957) A markovian decision process. Journal of Mathematics and\nMechanics pp 679–684\nBerner C, Brockman G, Chan B, Cheung V, Debiak P, Dennison C, Farhi D,\nFischer Q, Hashme S, Hesse C, Józefowicz R, Gray S, Olsson C, Pachocki JW,\nPetrov M, de Oliveira Pinto HP, Raiman J, Salimans T, Schlatter J, Schneider\nJ, Sidor S, Sutskever I, Tang J, Wolski F, Zhang S (2019) Dota 2 with large\nscale deep reinforcement learning. arXiv preprint arXiv:191206680\nBernstein DS, Givan R, Immerman N, Zilberstein S (2002) The complexity of\ndecentralized control of markov decision processes. Mathematics of Operations\nResearch 27(4):819–840\n30\nWong et al.\nBloembergen D, Tuyls K, Hennes D, Kaisers M (2015) Evolutionary dynamics\nof multi-agent learning: A survey. Journal of Artiﬁcial Intelligence Research\n53:659–697\nBowling M, Veloso M (2001) Rational and convergent learning in stochastic games.\nIn: International Joint Conference on Artiﬁcial Intelligence, Citeseer, vol 17, pp\n1021–1026\nBowling M, Veloso M (2002) Multiagent learning using a variable learning rate.\nArtiﬁcial Intelligence 136(2):215–250\nBowling M, Burch N, Johanson M, Tammelin O (2015) Heads-up limit hold’em\npoker is solved. Science 347(6218):145–149\nBrown GW (1951) Iterative solution of games by ﬁctitious play. Activity Analysis\nof Production and Allocation 13(1):374–376\nBrown N, Sandholm T (2018) Superhuman ai for heads-up no-limit poker: Libratus\nbeats top professionals. Science 359(6374):418–424\nBrown N, Sandholm T (2019) Superhuman ai for multiplayer poker. Science\n365(6456):885–890\nBurden J (2020) Automating abstraction for potential-based reward shaping. PhD\nthesis, University of York\nBusoniu L, Babuska R, De Schutter B (2008) A comprehensive survey of multia-\ngent reinforcement learning. IEEE Transactions on Systems, Man, and Cyber-\nnetics, Part C (Applications and Reviews) 38(2):156–172\nCanese L, Cardarilli GC, Di Nunzio L, Fazzolari R, Giardino D, Re M, Spanò S\n(2021) Multi-agent reinforcement learning: A review of challenges and applica-\ntions. Applied Sciences 11(11):4948\nCao K, Lazaridou A, Lanctot M, Leibo JZ, Tuyls K, Clark S (2018) Emergent\ncommunication through negotiation. In: International Conference on Learn-\ning Representations (ICLR) (Poster), URL https://openreview.net/forum?\nid=Hk6WhagRW\nCastellini J, Devlin S, Oliehoek FA, Savani R (2021) Diﬀerence rewards policy\ngradients. In: Proceedings of the 20th International Conference on Autonomous\nAgents and MultiAgent Systems, International Foundation for Autonomous\nAgents and Multi Agent Systems, Richland, SC, AAMAS ’21, p 1475–1477\nCheng CA, Kolobov A, Swaminathan A (2021) Heuristic-guided reinforcement\nlearning. Advances in Neural Information Processing Systems 34\nChu T, Wang J, Codecà L, Li Z (2020) Multi-agent deep reinforcement learning for\nlarge-scale traﬃc signal control. IEEE Transactions on Intelligent Transporta-\ntion Systems 21(3):1086–1095\nChua K, Calandra R, McAllister R, Levine S (2018) Deep reinforcement learning\nin a handful of trials using probabilistic dynamics models. Advances in Neural\nInformation Processing Systems 31\nColin TR, Belpaeme T, Cangelosi A, Hemion N (2016) Hierarchical reinforcement\nlearning as creative problem solving. Robotics and Autonomous Systems 86:196–\n206\nColman AM (2003) Cooperation, psychological game theory, and limitations of\nrationality in social interaction. Behavioral and Brain Sciences 26:139–198\nDa Silva FL, Costa AHR (2019) A survey on transfer learning for multiagent rein-\nforcement learning systems. Journal of Artiﬁcial Intelligence Research 64:645–\n703\nDeep multiagent Reinforcement Learning\n31\nDa Silva FL, Glatt R, Costa AHR (2017) Simultaneously learning and advising\nin multiagent reinforcement learning. In: Proceedings of the 16th International\nConference on Autonomous Agents and Multiagent Systems (AAMAS 2017),\npp 1100–1108\nDai Z, Chen Y, Low BKH, Jaillet P, Ho TH (2020) R2-b2: Recursive reasoning-\nbased bayesian optimization for no-regret learning in games. In: Proceedings of\nthe 37th International Conference on Machine Learning, PMLR, pp 2291–2301\nDankwa S, Zheng W (2019) Twin-delayed ddpg: A deep reinforcement learning\ntechnique to model a continuous movement of an intelligent robot agent. In:\nProceedings of the 3rd International Conference on Vision, Image and Signal\nProcessing, pp 1–5\nDas A, Kottur S, Moura JM, Lee S, Batra D (2017) Learning cooperative visual\ndialog agents with deep reinforcement learning. In: Proceedings of the IEEE\ninternational Conference on Computer Vision, pp 2951–2960\nDevlin S, Kudenko D (2011) Theoretical considerations of potential-based reward\nshaping for multi-agent systems. In: The 10th International Conference on Au-\ntonomous Agents and Multiagent Systems, ACM, pp 225–232\nDevlin S, Kudenko D, Grześ M (2011) An empirical study of potential-based re-\nward shaping and advice in complex, multi-agent systems. Advances in Complex\nSystems 14(02):251–278\nDevlin S, Yliniemi L, Kudenko D, Tumer K (2014) Potential-based diﬀerence\nrewards for multiagent reinforcement learning. In: Proceedings of the 2014 In-\nternational Conference on Autonomous Agents and Multi-agent Systems, pp\n165–172\nDevlin SM, Kudenko D (2012) Dynamic potential-based reward shaping. In: Pro-\nceedings of the 11th International Conference on Autonomous Agents and Mul-\ntiagent Systems, IFAAMAS, pp 433–440\nDiallo EAO, Sugiyama A, Sugawara T (2017) Learning to coordinate with deep\nreinforcement learning in doubles pong game. In: 2017 16th IEEE International\nConference on Machine Learning and Applications (ICMLA), IEEE, pp 14–19\nDing Z, Dong H (2020) Challenges of reinforcement learning. Springer\nDovidio JF (1984) Helping behavior and altruism: An empirical and conceptual\noverview. Advances in Experimental Social Psychology 17:361–427\nDrugan MM (2019) Reinforcement learning versus evolutionary computation: A\nsurvey on hybrid algorithms. Swarm and Evolutionary Computation 44:228–246\nDu W, Ding S (2021) A survey on multi-agent deep reinforcement learning: from\nthe perspective of challenges and applications. Artiﬁcial Intelligence Review\n54(5):3215–3238\nDu Y, Han L, Fang M, Liu J, Dai T, Tao D (2019) Liir: Learning individual intrinsic\nreward in multi-agent reinforcement learning. Advances in Neural Information\nProcessing Systems 32:4403–4414\nEccles T, Hughes E, Kramár J, Wheelwright S, Leibo JZ (2019) Learning reci-\nprocity in complex sequential social dilemmas. arXiv preprint arXiv:190308082\nEverett R, Roberts S (2018) Learning against non-stationary agents with oppo-\nnent modelling and deep reinforcement learning. In: 2018 Association for the\nAdvancement of Artiﬁcial Intelligence spring symposium series\nFehr E, Schmidt KM (1999) A theory of fairness, competition, and cooperation.\nThe Quarterly Journal of Economics 114(3):817–868\n32\nWong et al.\nFeriani A, Hossain E (2021) Single and multi-agent deep reinforcement learning\nfor ai-enabled wireless networks: A tutorial. IEEE Communications Surveys &\nTutorials 23(2):1226–1252\nFoerster J, Assael IA, De Freitas N, Whiteson S (2016) Learning to communicate\nwith deep multi-agent reinforcement learning. Advances in Neural Information\nProcessing Systems 29:2137–2145\nFoerster J, Chen RY, Al-Shedivat M, Whiteson S, Abbeel P, Mordatch I (2018a)\nLearning with opponent-learning awareness. In: Proceedings of the 17th Inter-\nnational Conference on Autonomous Agents and MultiAgent Systems, AAMAS\n’18, p 122–130\nFoerster J, Farquhar G, Afouras T, Nardelli N, Whiteson S (2018b) Counterfac-\ntual multi-agent policy gradients. In: Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol 32\nFrith C, Frith U (2005) Theory of mind. Current Biology 15(17):644–645\nGigerenzer G, Goldstein DG (1996) Reasoning the fast and frugal way: models of\nbounded rationality. Psychological Review 103(4):650\nGilovich T, Griﬃn D, Kahneman D (2002) Heuristics and biases: The psychology\nof intuitive judgment. Cambridge University Press\nGomes J, Mariano P, Christensen AL (2014) Avoiding convergence in coopera-\ntive coevolution with novelty search. In: Proceedings of the 2014 International\nConference on Autonomous Agents and Multi-agent Systems, pp 1149–1156\nGomes J, Mariano P, Christensen AL (2017) Dynamic team heterogeneity in co-\noperative coevolutionary algorithms. IEEE Transactions on Evolutionary Com-\nputation 22(6):934–948\nGraesser L, Keng WL (2019) Foundations of deep reinforcement learning: theory\nand practice in Python. Addison-Wesley Professional\nGreensmith E, Bartlett PL, Baxter J (2004) Variance reduction techniques for gra-\ndient estimates in reinforcement learning. Journal of Machine Learning Research\n5(9)\nGronauer S, Diepold K (2021) Multi-agent deep reinforcement learning: a survey.\nArtiﬁcial Intelligence Review pp 1–49\nGrondman I, Busoniu L, Lopes GA, Babuska R (2012) A survey of actor-critic\nreinforcement learning: Standard and natural policy gradients. IEEE Transac-\ntions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)\n42(6):1291–1307\nGu S, Geng M, Lan L (2021) Attention-based fault-tolerant approach for multi-\nagent reinforcement learning systems. Entropy 23(9):1133\nGupta JK, Egorov M, Kochenderfer M (2017) Cooperative multi-agent control\nusing deep reinforcement learning. In: International Conference on Autonomous\nAgents and Multiagent Systems, Springer, pp 66–83\nHaarnoja T, Zhou A, Hartikainen K, Tucker G, Ha S, Tan J, Kumar V, Zhu H,\nGupta A, Abbeel P, et al. (2018) Soft actor-critic algorithms and applications.\narXiv preprint arXiv:181205905\nHamrick JB, Friesen AL, Behbahani F, Guez A, Viola F, Witherspoon S, Anthony\nT, Buesing LH, Veličković P, Weber T (2021) On the role of planning in model-\nbased deep reinforcement learning. In: International Conference on Learning\nRepresentations, URL https://openreview.net/forum?id=IrM64DGB21\nHansen EA, Bernstein DS, Zilberstein S (2004) Dynamic programming for partially\nobservable stochastic games. In: American Association for Artiﬁcial Intelligence,\nDeep multiagent Reinforcement Learning\n33\nvol 4, pp 709–715\nHausknecht M, Stone P (2015) Deep recurrent q-learning for partially observable\nmdps. In: 2015 aaai fall symposium series\nHausknecht M, Stone P (2016) Grounded semantic networks for learning shared\ncommunication protocols. In: International Conference on Machine Learning\n(Workshop)\nHavrylov S, Titov I (2017) Emergence of language with multi-agent games: Learn-\ning to communicate with sequences of symbols. Advances in neural information\nprocessing systems 30\nHe H, Boyd-Graber J, Kwok K, Daumé III H (2016) Opponent modeling in deep\nreinforcement learning. In: International Conference on Machine Learning, Pro-\nceedings of Machine Learning Research, pp 1804–1813\nHeinrich J, Silver D (2016) Deep reinforcement learning from self-play in imperfect-\ninformation games. arXiv preprint arXiv:160301121\nHeinrich J, Lanctot M, Silver D (2015) Fictitious self-play in extensive-form games.\nIn: International Conference on Machine Learning, PMLR, pp 805–813\nHernandez-Leal P, Rosman B, Taylor ME, Sucar LE, Munoz de Cote E (2016)\nA bayesian approach for learning and tracking switching, non-stationary op-\nponents. In: Proceedings of the 2016 International Conference on Autonomous\nAgents & Multiagent Systems, pp 1315–1316\nHernandez-Leal P, Kartal B, Taylor ME (2019) A survey and critique of multia-\ngent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems\n33(6):750–797\nHolmesparker C, Agogino AK, Tumer K (2016) Combining reward shaping and\nhierarchies for scaling to large multiagent systems. The Knowledge Engineering\nReview 31(1):3–18\nHong ZW, Su SY, Shann TY, Chang YH, Lee CY (2018) A deep policy infer-\nence q-network for multi-agent systems. In: Proceedings of the 17th Interna-\ntional Conference on Autonomous Agents and MultiAgent Systems, Interna-\ntional Foundation for Autonomous Agents and Multi Agent Systems, AAMAS\n’18, p 1388–1396\nHuang Y, Huang L, Zhu Q (2022) Reinforcement learning for feedback-enabled\ncyber resilience. Annual Reviews in Control\nHughes E, Leibo JZ, Phillips M, Tuyls K, Dueñez-Guzman E, García Castañeda A,\nDunning I, Zhu T, McKee K, Koster R, et al. (2018) Inequity aversion improves\ncooperation in intertemporal social dilemmas. Advances in Neural Information\nProcessing Systems 31\nIlhan E, Gow J, Perez-Liebana D (2019) Teaching on a budget in multi-agent deep\nreinforcement learning. In: 2019 IEEE Conference on Games (CoG), IEEE, pp\n1–8\nIqbal S, Sha F (2019) Actor-attention-critic for multi-agent reinforcement learning.\nIn: International Conference on Machine Learning, PMLR, pp 2961–2970\nJaderberg M, Czarnecki WM, Dunning I, Marris L, Lever G, Castaneda AG, Beat-\ntie C, Rabinowitz NC, Morcos AS, Ruderman A, et al. (2019) Human-level per-\nformance in 3d multiplayer games with population-based reinforcement learning.\nScience 364(6443):859–865\nJaques N, Lazaridou A, Hughes E, Gulcehre C, Ortega P, Strouse D, Leibo JZ,\nDe Freitas N (2019) Social inﬂuence as intrinsic motivation for multi-agent\ndeep reinforcement learning. In: International Conference on Machine Learn-\n34\nWong et al.\ning, PMLR, pp 3040–3049\nJiang J, Lu Z (2018) Learning attentional communication for multi-agent cooper-\nation. Advances in neural information processing systems 31\nJin J, Song C, Li H, Gai K, Wang J, Zhang W (2018) Real-time bidding with\nmulti-agent reinforcement learning in display advertising. In: Cuzzocrea A, Al-\nlan J, Paton NW, Srivastava D, Agrawal R, Broder AZ, Zaki MJ, Candan KS,\nLabrinidis A, Schuster A, Wang H (eds) Proceedings of the 27th ACM Interna-\ntional Conference on Information and Knowledge Management, pp 2193–2201\nJohanson M, Burch N, Valenzano R, Bowling M (2013) Evaluating state-space\nabstractions in extensive-form games. In: Proceedings of the 2013 International\nConference on Autonomous Agents and Multi-agent Systems, pp 271–278\nJorge E, Kågebäck M, Johansson FD, Gustavsson E (2017) Learning to play\nguess who? and inventing a grounded language as a consequence. arXiv preprint\narXiv:161103218\nKakade SM (2003) On the sample complexity of reinforcement learning. University\nof London, University College London (United Kingdom)\nKim DK, Liu M, Omidshaﬁei S, Lopez-Cot S, Riemer M, Habibi G, Tesauro G,\nMourad S, Campbell M, How JP (2020) Learning hierarchical teaching policies\nfor cooperative agents. In: Proceedings of the 19th International Conference\non Autonomous Agents and MultiAgent Systems, International Foundation for\nAutonomous Agents and Multi Agent Systems, Richland, SC, AAMAS ’20, p\n620–628\nKim W, Cho M, Sung Y (2019) Message-dropout: An eﬃcient training method\nfor multi-agent deep reinforcement learning. In: Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol 33, pp 6079–6086, DOI https://doi.org/10.\n1609/aaai.v33i01.33016079\nKonda VR, Tsitsiklis JN (2003) Actor-critic algorithms. Journal on Control and\nOptimization 42(4):1143–1166\nKottur S, Moura JMF, Lee S, Batra D (2017) Natural language does not\nemerge ’naturally’ in multi-agent dialog. In: Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), pp 2962–2967, URL https:\n//aclanthology.info/papers/D17-1321/d17-1321\nKraemer L, Banerjee B (2016) Multi-agent reinforcement learning as a rehearsal\nfor decentralized planning. Neurocomputing 190:82–94\nKuhn HW, Tucker AW (1953) Contributions to the theory of games, vol 2. Prince-\nton University Press\nKumar A, Zilberstein S (2009) Dynamic programming approximations for partially\nobservable stochastic games. In: Proceedings of the Twenty-Second International\nFLAIRS Conference, pp 547–552\nKurek M, Jaśkowski W (2016) Heterogeneous team deep q-learning in low-\ndimensional multi-agent environments. In: 2016 IEEE Conference on Computa-\ntional Intelligence and Games (CIG), IEEE, pp 1–8\nLazaridou A, Baroni M (2020) Emergent multi-agent communication in the deep\nlearning era. arXiv preprint arXiv:200602419\nLazaridou A, Peysakhovich A, Baroni M (2017) Multi-agent cooperation and the\nemergence of (natural) language. In: International Conference on Learning Rep-\nresentations, URL https://openreview.net/forum?id=Hk8N3Sclg\nLeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444\nDeep multiagent Reinforcement Learning\n35\nLehman J, Stanley KO (2008) Exploiting open-endedness to solve problems\nthrough the search for novelty. In: Artiﬁcial Life XI, Citeseer, pp 329–336\nLehman J, Chen J, Clune J, Stanley KO (2018a) Es is more than just a traditional\nﬁnite-diﬀerence approximator. In: Proceedings of the Genetic and Evolutionary\nComputation Conference, pp 450–457, DOI https://doi.org/10.1145/3205455.\n3205474\nLehman J, Chen J, Clune J, Stanley KO (2018b) Safe mutations for deep and recur-\nrent neural networks through output gradients. arXiv preprint arXiv:171206563\nLehman J, Chen J, Clune J, Stanley KO (2018c) Safe mutations for deep and\nrecurrent neural networks through output gradients. In: Proceedings of the Ge-\nnetic and Evolutionary Computation Conference, Association for Computing\nMachinery, New York, NY, USA, GECCO ’18, p 117–124, DOI 10.1145/3205455.\n3205473, URL https://doi.org/10.1145/3205455.3205473\nLeibo JZ, Zambaldi V, Lanctot M, Marecki J, Graepel T (2017) Multi-agent\nreinforcement learning in sequential social dilemmas. In: Proceedings of the\n16th Conference on Autonomous Agents and Multiagent Systems, International\nFoundation for Autonomous Agents and Multi Agent Systems, Richland, SC,\nAAMAS ’17, p 464–473\nLeibo JZ, d’Autume CdM, Zoran D, Amos D, Beattie C, Anderson K, Castañeda\nAG, Sanchez M, Green S, Gruslys A, et al. (2018) Psychlab: a psychology labo-\nratory for deep reinforcement learning agents. arXiv preprint arXiv:180108116\nLerer A, Peysakhovich A (2018) Maintaining cooperation in complex social dilem-\nmas using deep reinforcement learning. arXiv preprint arXiv:170701068\nLevine S (2017) Berkeley CS 294-112, Lecture Notes: Model-Based Reinforce-\nment Learning. URL: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/\nf17docs/lecture_9_model_based_rl.pdf. Last visited on 2021/05/12\nLi S, Wu Y, Cui X, Dong H, Fang F, Russell S (2019) Robust multi-agent reinforce-\nment learning via minimax deep deterministic policy gradient. In: Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, vol 33, pp 4213–4220\nLillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, Silver D, Wierstra D\n(2016) Continuous control with deep reinforcement learning. In: The Interna-\ntional Conference on Learning Representations, URL http://arxiv.org/abs/\n1509.02971\nLin K, Zhao R, Xu Z, Zhou J (2018) Eﬃcient large-scale ﬂeet management via\nmulti-agent deep reinforcement learning. In: Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining,\npp 1774–1783\nLittman ML (1994) Markov games as a framework for multi-agent reinforcement\nlearning. In: 11th International Conference on Machine Learning, Elsevier, pp\n157–163\nLiu S, Lever G, Merel J, Tunyasuvunakool S, Heess N, Graepel T (2019) Emergent\ncoordination through competition. arXiv preprint arXiv:190207151\nLiu Z, Chen B, Zhou H, Koushik G, Hebert M, Zhao D (2020) Mapper: Multi-agent\npath planning with evolutionary reinforcement learning in mixed dynamic en-\nvironments. In: 2020 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), IEEE, pp 11748–11754\nLowe R, Wu YI, Tamar A, Harb J, Pieter Abbeel O, Mordatch I (2017) Multi-\nagent actor-critic for mixed cooperative-competitive environments. Advances in\nneural information processing systems 30\n36\nWong et al.\nLowe R, Foerster J, Boureau YL, Pineau J, Dauphin Y (2019) On the pitfalls\nof measuring emergent communication. In: Proceedings of the 18th Interna-\ntional Conference on Autonomous Agents and Multiagent Systems, Interna-\ntional Foundation for Autonomous Agents and Multi Agent Systems, Richland,\nSC, AAMAS ’19, p 693–701\nMa Z, Luo Y, Ma H (2021) Distributed heuristic multi-agent path ﬁnding with\ncommunication. In: 2021 IEEE International Conference on Robotics and Au-\ntomation (ICRA), IEEE, pp 8699–8705\nMahajan A, Rashid T, Samvelyan M, Whiteson S (2019) Maven: Multi-agent vari-\national exploration. Advances in Neural Information Processing Systems 32\nMajumdar S, Khadka S, Miret S, Mcaleer S, Tumer K (2020) Evolutionary rein-\nforcement learning for sample-eﬃcient multiagent coordination. In: International\nConference on Machine Learning, PMLR, pp 6651–6660\nMao H, Alizadeh M, Menache I, Kandula S (2016) Resource management with deep\nreinforcement learning. In: Ford B, Snoeren AC, Zegura EW (eds) Proceedings\nof the 15th ACM Workshop on Hot Topics in Networks, ACM Press, pp 50–56,\nDOI https://doi.org/10.1145/3005745.3005750\nMao H, Gong Z, Ni Y, Xiao Z (2017) Accnet: Actor-coordinator-critic net for\n\"learning-to-communicate\" with deep multi-agent reinforcement learning. arXiv\npreprint arXiv:170603235\nMao H, Zhang Z, Xiao Z, Gong Z, Ni Y (2020) Learning multi-agent communica-\ntion with double attentional deep reinforcement learning. Autonomous Agents\nand Multi-Agent Systems 34(1):1–34\nMarewski JN, Gaissmaier W, Gigerenzer G (2010) Good judgments do not require\ncomplex cognition. Cognitive Processing 11(2):103–121\nMarkovitch S, Reger R (2005) Learning and exploiting relative weaknesses of op-\nponent agents. Autonomous Agents and Multi-Agent Systems 10(2):103–130\nMcKee KR, Gemp I, McWilliams B, Duèñez Guzmán EA, Hughes E, Leibo JZ\n(2020) Social diversity and social preferences in mixed-motive reinforcement\nlearning. In: Proceedings of the 19th International Conference on Autonomous\nAgents and Multiagent Systems, International Foundation for Autonomous\nAgents and Multi Agent Systems, Richland, SC, AAMAS ’20, p 869–877\nMinsky M (1961) Steps toward artiﬁcial intelligence. Proceedings of the IRE\n49(1):8–30, DOI 10.1109/JRPROC.1961.287775\nMnih V, Kavukcuoglu K, Silver D, Graves A, Antonoglou I, Wierstra D, Ried-\nmiller M (2013) Playing atari with deep reinforcement learning. arXiv preprint\narXiv:13125602\nMnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves\nA, Riedmiller M, Fidjeland AK, Ostrovski G, et al. (2015) Human-level control\nthrough deep reinforcement learning. Nature 518(7540):529–533\nMnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver D,\nKavukcuoglu K (2016) Asynchronous methods for deep reinforcement learning.\nIn: Balcan MF, Weinberger KQ (eds) Proceedings of The 33rd International\nConference on Machine Learning, PMLR, pp 1928–1937\nMoravčík M, Schmid M, Burch N, Lis`y V, Morrill D, Bard N, Davis T, Waugh K,\nJohanson M, Bowling M (2017) Deepstack: Expert-level artiﬁcial intelligence in\nheads-up no-limit poker. Science 356(6337):508–513\nMoreno P, Hughes E, McKee KR, Pires BA, Weber T (2021) Neural recursive belief\nstates in multi-agent reinforcement learning. arXiv preprint arXiv:210202274\nDeep multiagent Reinforcement Learning\n37\nMoriarty DE, Schultz AC, Grefenstette JJ (1999) Evolutionary algorithms for\nreinforcement learning. Journal of Artiﬁcial Intelligence Research 11:241–276\nNevmyvaka Y, Feng Y, Kearns M (2006) Reinforcement learning for optimized\ntrade execution. In: Proceedings of the 23rd international conference on Machine\nlearning, pp 673–680\nNg AY, Harada D, Russell S (1999) Policy invariance under reward transforma-\ntions: Theory and application to reward shaping. In: ICML, vol 99, pp 278–287\nNguyen DT, Kumar A, Lau HC (2018) Credit assignment for collective multia-\ngent rl with global rewards. In: Proceedings of the 31th Advances in Neural\nInformation Processing Systems, MIT Press\nNitschke GS, Eiben A, Schut MC (2012) Evolving team behaviors with specializa-\ntion. Genetic Programming and Evolvable Machines 13(4):493–536\nOmidshaﬁei S, Kim DK, Liu M, Tesauro G, Riemer M, Amato C, Campbell M,\nHow JP (2019) Learning to teach in cooperative multiagent reinforcement learn-\ning. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 33,\npp 6128–6136\nPalanisamy P (2020) Multi-agent connected autonomous driving using deep re-\ninforcement learning. In: International Joint Conference on Neural Networks,\nIEEE, pp 1–7\nPapoudakis G, Christianos F, Rahman A, Albrecht SV (2019) Dealing with\nnon-stationarity in multi-agent deep reinforcement learning. arXiv preprint\narXiv:190604737\nPeng P, Wen Y, Yang Y, Yuan Q, Tang Z, Long H, Wang J (2017) Multia-\ngent bidirectionally-coordinated nets: Emergence of human-level coordination\nin learning to play starcraft combat games. arXiv preprint arXiv:170310069\nPeng Z, Zhang L, Luo T (2018) Learning to communicate via supervised atten-\ntional message processing. In: Proceedings of the 31st International Conference\non Computer Animation and Social Agents, pp 11–16\nPeters J, Schaal S (2008) Natural actor-critic. Neurocomputing 71(7-9):1180–1190\nPeysakhovich A, Lerer A (2018) Prosocial learning agents solve generalized stag\nhunts better than selﬁsh ones. In: International Foundation for Autonomous\nAgents and Multi Agent Systems, Richland, SC, AAMAS ’18, p 2043–2044\nPlaat A (2020) Learning to Play: Reinforcement Learning and Games. Springer\nNature\nPrasad A, Dusparic I (2019) Multi-agent deep reinforcement learning for zero\nenergy communities. In: 2019 IEEE PES innovative smart grid technologies\nEurope (ISGT-Europe), IEEE, pp 1–5\nPremack D, WoodruﬀG (1978) Does the chimpanzee have a theory of mind?\nBehavioral and Brain Sciences 1(4):515–526\nProper S, Tumer K (2012) Modeling diﬀerence rewards for multiagent learning. In:\nProceedings of the 11th International Conference on Autonomous Agents and\nMultiagent Systems), Conitzer, Winikoﬀ, Padgham, pp 1397–1398\nRashid T, Farquhar G, Peng B, Whiteson S (2020a) Weighted qmix: Expand-\ning monotonic value function factorisation for deep multi-agent reinforcement\nlearning. Advances in Neural Information Processing Systems 33:10199–10210\nRashid T, Samvelyan M, Schroeder de Witt C, Farquhar G, Foerster JN, Whiteson\nS (2020b) Monotonic value function factorisation for deep multi-agent reinforce-\nment learning. Journal of Machine Learning Research 21\n38\nWong et al.\nRusu AA, Colmenarejo SG, Gulcehre C, Desjardins G, Kirkpatrick J, Pascanu R,\nMnih V, Kavukcuoglu K, Hadsell R (2016) Policy distillation. arXiv preprint\narXiv:151106295\nSallab AE, Abdou M, Perot E, Yogamani S (2017) Deep reinforcement learning\nframework for autonomous driving. Electronic Imaging 2017(19):70–76\nSchrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L, Schmitt S, Guez\nA, Lockhart E, Hassabis D, Graepel T, et al. (2020) Mastering atari, go, chess\nand shogi by planning with a learned model. Nature 588(7839):604–609\nSchulman J, Levine S, Abbeel P, Jordan M, Moritz P (2015) Trust region policy\noptimization. In: International Conference on Machine Learning, PMLR, pp\n1889–1897\nSchulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy\noptimization algorithms. arXiv preprint arXiv:170706347\nShapley LS (1953) Stochastic games. Proceedings of the National Academy of\nSciences 39(10):1095–1100\nSheikh HU, Bölöni L (2020) Multi-agent reinforcement learning for problems with\ncombined individual and team reward. In: 2020 International Joint Conference\non Neural Networks (IJCNN), IEEE, pp 1–8\nShoham Y, Leyton-Brown K (2008) Multiagent systems: Algorithmic, game-\ntheoretic, and logical foundations. Cambridge University Press\nSilver D, Lever G, Heess N, Degris T, Wierstra D, Riedmiller M (2014) Determinis-\ntic policy gradient algorithms. In: International conference on machine learning,\nPMLR, pp 387–395\nSilver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche G, Schrit-\ntwieser J, Antonoglou I, Panneershelvam V, Lanctot M, et al. (2016) Mastering\nthe game of go with deep neural networks and tree search. Nature 529(7587):484–\n489\nSilver D, Schrittwieser J, Simonyan K, Antonoglou I, Huang A, Guez A, Hubert\nT, Baker L, Lai M, Bolton A, et al. (2017) Mastering the game of go without\nhuman knowledge. Nature 550(7676):354–359\nSimon HA (1957) Models of man, social and rational: Mathematical essays on\nrational human behavior in a social setting. Wiley and Sons\nSimon HA (1990) Bounded rationality. Springer\nSon K, Kim D, Kang WJ, Hostallero DE, Yi Y (2019) Qtran: Learning to factor-\nize with transformation for cooperative multi-agent reinforcement learning. In:\nInternational Conference on Machine Learning, PMLR, pp 5887–5896\nSrivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R (2014)\nDropout: a simple way to prevent neural networks from overﬁtting. The Journal\nof Machine Learning Research 15(1):1929–1958\nStanley HE (1971) Phase transitions and critical phenomena. Clarendon, Oxford\nSu J, Adams S, Beling P (2021) Value-decomposition multi-agent actor-critics.\nIn: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 35, pp\n11352–11360\nSuay HB, Brys T, Taylor ME, Chernova S (2016) Learning from demonstration\nfor shaping through inverse reinforcement learning. In: Proceedings of the 2016\nInternational Conference on Autonomous Agents & Multiagent Systems, pp\n429–437\nSuch FP, Madhavan V, Conti E, Lehman J, Stanley KO, Clune J (2018) Deep\nneuroevolution: Genetic algorithms are a competitive alternative for training\nDeep multiagent Reinforcement Learning\n39\ndeep neural networks for reinforcement learning. arXiv preprint arXiv:171206567\nSukhbaatar S, Fergus R, et al. (2016) Learning multiagent communication with\nbackpropagation. Advances in neural information processing systems 29\nSunehag P, Lever G, Gruslys A, Czarnecki WM, Zambaldi V, Jaderberg M, Lanc-\ntot M, Sonnerat N, Leibo JZ, Tuyls K, Graepel T (2018) Value-decomposition\nnetworks for cooperative multi-agent learning based on team reward. In: Pro-\nceedings of the 17th International Conference on Autonomous Agents and Mul-\ntiAgent Systems, International Foundation for Autonomous Agents and Multi\nAgent Systems, Richland, SC, AAMAS ’18, p 2085–2087\nSutton RS, Barto AG (2018) Reinforcement learning: An introduction. MIT press\nSutton RS, Barto AG, et al. (1998) Introduction to reinforcement learning, vol\n135. MIT press Cambridge\nSutton RS, McAllester D, Singh S, Mansour Y (1999) Policy gradient methods for\nreinforcement learning with function approximation. Advances in neural infor-\nmation processing systems 12\nTampuu A, Matiisen T, Kodelja D, Kuzovkin I, Korjus K, Aru J, Aru J, Vicente R\n(2017) Multiagent cooperation and competition with deep reinforcement learn-\ning. PloS one 12(4):1–15, DOI https://doi.org/10.1371/journal.pone.0172395\nTan M (1993) Multi-agent reinforcement learning: Independent vs. cooperative\nagents. In: Proceedings of the tenth International Conference on Machine Learn-\ning, pp 330–337\nTaylor JET, Taylor GW (2021) Artiﬁcial cognition: How experimental psychology\ncan help generate explainable artiﬁcial intelligence. Psychonomic Bulletin &\nReview 28(2):454–475\nTaylor ME, Stone P (2009) Transfer learning for reinforcement learning domains:\nA survey. Journal of Machine Learning Research 10(7)\nTerry JK, Grammel N, Hari A, Santos L, Black B (2021) Revisiting pa-\nrameter sharing in multi-agent deep reinforcement learning. arXiv preprint\narXiv:200513625\nTian R, Tomizuka M, Sun L (2021) Learning human rewards by inferring their\nlatent intelligence levels in multi-agent games: A theory-of-mind approach with\napplication to driving data. In: 2021 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), IEEE, pp 4560–4567\nVan Der Ree M, Wiering M (2013) Reinforcement learning in the game of othello:\nLearning against a ﬁxed opponent and learning from self-play. In: 2013 IEEE\nSymposium on Adaptive Dynamic Programming And Reinforcement Learning\n(ADPRL), IEEE, pp 108–115\nVan Hasselt H, Guez A, Silver D (2016) Deep reinforcement learning with double\nq-learning. In: Proceedings of the AAAI conference on artiﬁcial intelligence,\nvol 30\nVan Otterlo M, Wiering M (2012) Reinforcement learning and markov decision\nprocesses. In: Reinforcement learning, Springer, pp 3–42\nVinyals O, Babuschkin I, Czarnecki WM, Mathieu M, Dudzik A, Chung J, Choi\nDH, Powell R, Ewalds T, Georgiev P, et al. (2019) Grandmaster level in starcraft\nii using multi-agent reinforcement learning. Nature 575(7782):350–354\nWang RE, Everett M, How JP (2019) R-maddpg for partially observable envi-\nronments and limited communication. In: International Conference on Machine\nLearning 2019 Workshop RL4RealLife\n40\nWong et al.\nWang W, Hao J, Wang Y, Taylor M (2018) Towards cooperation in sequential\nprisoner’s dilemmas: a deep multiagent reinforcement learning approach. arXiv\npreprint arXiv:180300162\nWen Y, Yang Y, Luo R, Wang J, Pan W (2019) Probabilistic recursive reasoning for\nmulti-agent reinforcement learning. In: 7th International Conference on Learning\nRepresentations, ICLR 2019\nWen Z, O’Neill D, Maei H (2015) Optimal demand response using device-based\nreinforcement learning. IEEE Transactions on Smart Grid 6(5):2312–2324\nWilliams RJ (1992) Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine Learning 8(3-4):229–256\nSchroeder de Witt C, Foerster J, Farquhar G, Torr P, Boehmer W, Whiteson\nS (2019) Multi-agent common knowledge reinforcement learning. Advances in\nNeural Information Processing Systems 32:9927–9939\nWu Y, Mansimov E, Grosse RB, Liao S, Ba J (2017a) Scalable trust-region method\nfor deep reinforcement learning using kronecker-factored approximation. Ad-\nvances in neural information processing systems 30:5279–5288\nWu Y, Mansimov E, Liao S, Radford A, Schulman J (2017b) openai baselines,\nacktr & a2c. http://https://openai.com/blog/baselines-acktr-a2c//, [On-\nline; accessed 16-December-2021]\nYang Y, Wang J (2021) An overview of multi-agent reinforcement learning from\ngame theoretical perspective. arXiv preprint arXiv:201100583\nYang Y, Luo R, Li M, Zhou M, Zhang W, Wang J (2018) Mean ﬁeld multi-\nagent reinforcement learning. In: International Conference on Machine Learning,\nPMLR, pp 5571–5580\nYang Y, Hao J, Chen G, Tang H, Chen Y, Hu Y, Fan C, Wei Z (2020a) Q-value\npath decomposition for deep multiagent reinforcement learning. In: International\nConference on Machine Learning, PMLR, pp 10706–10715\nYang Y, Wen Y, Wang J, Chen L, Shao K, Mguni D, Zhang W (2020b) Multi-agent\ndeterminantal q-learning. In: International Conference on Machine Learning,\nPMLR, pp 10757–10766\nYe N, Somani A, Hsu D, Lee WS (2017) Despot: Online pomdp planning with\nregularization. Journal of Artiﬁcial Intelligence Research 58:231–266\nYliniemi L, Tumer K (2014) Multi-objective multiagent credit assignment through\ndiﬀerence rewards in reinforcement learning. In: Asia-Paciﬁc Conference on Sim-\nulated Evolution and Learning, Springer, pp 407–418\nYu Y (2018) Towards sample eﬃcient reinforcement learning. In: International\nJoint Conference on Artiﬁcial Intelligence, pp 5739–5743\nZhang K, Yang Z, Başar T (2021) Multi-Agent Reinforcement Learning: A Selec-\ntive Overview of Theories and Algorithms, Springer International Publishing,\npp 321–384. DOI 10.1007/978-3-030-60990-0_12, URL https://doi.org/10.\n1007/978-3-030-60990-0_12\nZhang X, Clune J, Stanley KO (2017) On the relationship between the openai evo-\nlution strategy and stochastic gradient descent. arXiv preprint arXiv:171206564\nZheng Y, Meng Z, Hao J, Zhang Z (2018a) Weighted double deep multiagent\nreinforcement learning in stochastic cooperative environments. In: Paciﬁc Rim\nInternational Conference on Artiﬁcial Intelligence, Springer, pp 421–429\nZheng Y, Meng Z, Hao J, Zhang Z, Yang T, Fan C (2018b) A deep bayesian\npolicy reuse approach against non-stationary agents. In: Proceedings of the 32nd\nInternational Conference on Neural Information Processing Systems, pp 962–972\nDeep multiagent Reinforcement Learning\n41\nZhou M, Liu Z, Sui P, Li Y, Chung YY (2020) Learning implicit credit assign-\nment for cooperative multi-agent reinforcement learning. Advances in Neural\nInformation Processing Systems 33:11853–11864\nZhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, Fei-Fei L, Farhadi A (2017) Target-\ndriven visual navigation in indoor scenes using deep reinforcement learning.\nIn: 2017 IEEE international Conference on Robotics and Automation (ICRA),\nIEEE, pp 3357–3364\nZou H, Ren T, Yan D, Su H, Zhu J (2021) Learning task-distribution reward sha-\nping with meta-learning. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, Vancouver, BC, Canada, pp 2–9\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "cs.NE",
    "A.1; I.2.6; I.2.8; J.4"
  ],
  "published": "2021-06-29",
  "updated": "2022-10-12"
}