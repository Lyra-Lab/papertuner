{
  "id": "http://arxiv.org/abs/2106.16088v1",
  "title": "Application of deep reinforcement learning for Indian stock trading automation",
  "authors": [
    "Supriya Bajpai"
  ],
  "abstract": "In stock trading, feature extraction and trading strategy design are the two\nimportant tasks to achieve long-term benefits using machine learning\ntechniques. Several methods have been proposed to design trading strategy by\nacquiring trading signals to maximize the rewards. In the present paper the\ntheory of deep reinforcement learning is applied for stock trading strategy and\ninvestment decisions to Indian markets. The experiments are performed\nsystematically with three classical Deep Reinforcement Learning models Deep\nQ-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten\nIndian stock datasets. The performance of the models are evaluated and\ncomparison is made.",
  "text": "APPLICATION OF DEEP REINFORCEMENT LEARNING FOR\nINDIAN STOCK TRADING AUTOMATION\nA PREPRINT\nSupriya Bajpai\nIITB-Monash Research Academy, IIT Bombay, India\nMonash University, Australia\nsupriya.bajpai@monash.edu\nJuly 1, 2021\nABSTRACT\nIn stock trading, feature extraction and trading strategy design are the two important tasks to achieve\nlong-term beneﬁts using machine learning techniques. Several methods have been proposed to design\ntrading strategy by acquiring trading signals to maximize the rewards. In the present paper the theory\nof deep reinforcement learning is applied for stock trading strategy and investment decisions to Indian\nmarkets. The experiments are performed systematically with three classical Deep Reinforcement\nLearning models Deep Q-Network, Double Deep Q-Network and Dueling Double Deep Q-Netwonrk\non ten Indian stock datasets. The performance of the models are evaluated and comparison is made.\nKeywords Deep reinforcement learning, Stock trading automation, Deep Q-learning, Double DQN,\nDueling Double DQN\n1\nIntroduction\nA lot of work has been done to propose methods and algorithms to predict stock prices and optimal decision making in\ntrading. A large number of indicators, machine learning and deep learning techniques [1] such as Moving averages [2, 3],\nlinear regression [4, 5, 6], neural networks [7, 8], Recurrent neural network [9, 10, 11, 12] and Reinforcement learning\n(RL) have been developed to predict the stock and ﬁnancial price and strategies [13, 14]. The advanced techniques of\nartiﬁcial neural network have shown better performance as compared to the traditional indicators and methods [15, 16].\nThe stock price prediction is a very challenging task as the stock market changes rapidly and data availability is also\nincomplete and not sufﬁcient. Reinforcement learning is one of the methods to solve such complex decision problems.\nReinforcement learning can prove to be a better altemative approach for stock price prediction [17] and maximizing\nexpected return. Deep Learning methods have the ability to extract features from high dimentional data. However, it\nlacks the decision-making capabilities. Deep Reinforcement Learning (DRL) combines the Deep Learning approach\nwith the decision making ability of Reinforcement Learning. Researchers have investigated RL techniques to solve\nthe algorithmic trading problem. Recurrent Reinforcement Learning (RRL) algorithm have been used for discovering\nnew investment policies without the need to build forecasting models [18]. Adaptive Reinforcement Learning (ARL)\nhave been used to trade in foreign exchange markets [19]. Recently, people investigated DRL method to solve the\nalgorithmic trading problem [20, 21, 22, 23, 24, 20, 25].\nIn the present paper Deep Reinforcement Learning is applied to Indian stock market on ten randomly selected datsets to\nautomate the stock trading and to maximize the proﬁt. Model is trained with historical stock data to predict the stock\ntrading strategy by using Deep Q-Network (DQN), Double Deep Q-Network (DDQNn) and Dueling Double Deep\nQ-Network (Dueling DDQN) for holding, buying and selling the stocks. The model is validated on unseen data from\nthe later period and performance is evaluated and compared.\narXiv:2106.16088v1  [q-fin.TR]  18 May 2021\nA PREPRINT - JULY 1, 2021\n2\nMethods\nDeep Q-Network, Double Deep Q-Network and Dueling Double Deep Q-Network [26] are discussed in the following\nsections.\n2.1\nDeep Q-Network\nDeep Q-Network is a classical and outstanding algorithm of Deep Reinforcement Learning and it’s model architecture\nis shown in Figure 1. It is a model-free reinforcement learning that can deal with sequential decision tasks. The goal of\nthe learning is to learn an optimal policy π⋆that maximizes the long term reward or proﬁt. The agent takes action at\ndepending on the current state st of the environment and receives reward rt from the environment. The experience\nreplay is used to learn from the previous experiences and is used to store the previous states, actions, rewards, and\nnext states. The data from the replay memory is sampled randomly and fed to the train network in small batch sizes to\navoid overﬁtting. In deep Q-learning the Convolutional Neural Network (known as Q-Network) is used to learn the\nexpected future reward Q-value function (Q(st, at)). One major difference between the Deep Q-Network and the basic\nQ-learning algorithm is a new Target-Q-Network, which is given by:\nQtarget = rt+1 + γmaxa′[Q(s′\nt, a′\nt; θ)]\n(1)\nwhere, Qtarget is the target Q value obtained using the Bellman Equation and θ denotes the parameters of the Q-Network.\nIn DQN there are two Q-Networks: main Q-Network and target Q-Network. The target Q-Network is different from the\nmain Q-Network which is being updated at every step. The network values of the target Q-Network are the updated\nperiodically and are the copy of the main network’s values. Use of only one Q-Network in the model leads to delayed or\nsub-optimal convergence when the data incoming frequency is very high and the training data is highly correlated and it\nmay also lead to unstable target function. The use of two different Q-Networks increases the stability of the Q-Network.\nOptimal Q-value or the action-value pair is computed to select and measure the actions. DQN takes the max of all the\nactions that leads to overestimation of the Q-value, as with the number of iterations the errors keeps on accumulating [27].\nThis problem of overestimation of Q-value is solved by using Double DQN, as it uses another neural network that\noptimizes the inﬂuence of error.\n2.2\nDouble Deep Q-Network\nThe above problem of overestimation becomes more serious if the actions are taken on the basis of a Target Q-Network\nas the values of the Target Q-Network are not frequently updated. Double DQN uses two neural networks with same\nstructure as in DQN, the main network and the target network as it provides more stability to the target values for update.\nIn Double DQN the action is selected on the basis of the main Q-Network but uses the target state-action value that\ncorresponds to that particular state-action from the Target Q-Network. Thus, at each step all the action-value pairs for\nall possible actions in the present state is taken from the main Q-Network which is updated at each time step. Then an\nargmax is taken over all the state-action values of such possible actions (Equation 2), and the state-action value which\nmaximizes the value, that speciﬁc action is selected.\nQtarget = rt+1 + γQ(st, argmaxa′Q(s\n′\nt, a\n′\nt; θ); θ\n′)\n(2)\nBut to update the main Q-Network the value that corresponds to the selected state-action pair is taken from the target\nQ-Network. As such we can overcome both the problems of overestimation and instability in Q-values.\n2.3\nDueling Double Deep Q-Network\nThere are two Q-Networks in both DQN as well as in Double DQN, one is the main network and the other is the target\nnetwork where the network values are the periodic copy of the main network’s values. The Dueling Double DQN\nhas non-sequential network architecture where, the convolutional layers get separated into two streams and both the\nsub-networks have fully connected layer and output layers. The ﬁrst sub-network corresponds to the value function to\nestimate the value of the given state and the second sub-network estimates the advantage value of taking a particular\naction over the base value of being in the current state.\nQ(st, at; θ, α, β) = V (st; θ, β) + (A(st, at; θ, α)−max\na′∈|A| A(st, a′\nt; θ, α))\n(3)\nhere, A is the advantage value. We can get the Q-values or the action-value by combining the output of the ﬁrst\nsub-network, that is the base value of state with the advantage values of the actions of the second sub-network. θ is\ncommon parameter vector both the sub-networks. α and β are the parameter vectors of the “Advantage” sub-network\n2\nA PREPRINT - JULY 1, 2021\nand State-Value function respectively. The Q value for a given state-action pair is equal to the value of that state which\nis estimated from the state-value (V ) plus the advantage of taking that action in that state. We can write the above\nEquation 3 as follows.\nQ(st, at; θ, α, β) = V (st; θ, β) + (A(st, at; θ, α))\n(4)\nFrom the above equation we can get the Q-value if we know the value of S and A, but we cannot get the values of S and\nA if Q-value is known. The last part of the Equation 3 is slightly modiﬁed as follows, which also increases the stability\nof the algorithm.\nQ(st, at; θ, α, β) = V (s; θ, β) + (A(st, at; θ, α) −1\n|A|\nX\na′\nA(st, a′\nt; θ, α))\n(5)\nFigure 1: Deep Q-Network model architecture.\n3\nExperiments\nIn the present study we evaluate the performance of the deep reinforcement learning algorithms for stock market\ninvestment decisions on 10 Indian stock dataset. The dataset is obtained from National Stock Exchange (NSE) India,\nthat consists of the price history and trading volumes of stocks in the index NIFTY 50. We used Deep Q-Network\n(DQN), Double Deep Q-Network (DDQN), and Dueling Double Deep Q-Network (Dueling DDQN) to automate the\nstock trading and to maximize the proﬁt. We split the dataset for training and testing purpose in equal proportions. The\ntraining and testing dataset is fed to the models and the train and test rewards and proﬁt are estimated and compared.\n3.1\nAgent Training\nThe Q-Network has input, hidden and output layers and the hyperparameters are tuned 1 to obtain the optimal weights.\nTuning the hyperparameters of the model in time-series problems is very crucial for the long-term reward. The\nQ-Network is trained by minimizing the loss function as follows:\nL(θ) = E[(Qtarget −Q(st, at; θ))2]\n(6)\nThe learning rate is 0.00025 and the optimizer is Adam optimizer. The training is done for 50 episodes with batch size\nof 64 and the agent performs three actions: hold, buy and sell.\n3.2\nAgent Testing\nThe testing of the agent is done on the unseen test dataset of later periods of the same time series as the train dataset.\nThe performance of the agent is measured in terms of total proﬁt. The proﬁt is calculated by sale price - purchase price.\n3\nA PREPRINT - JULY 1, 2021\nFigure 2: Plot showing train and test dataset of ULTRACEMCO stock price.\nTable 1: Model hyperparameters\nHyperparameters\nValues\nWindow size\n90\nBatch size\n64\nEpisodes\n50\nGamma\n0.95\nEpsilon\n1\nLearning rate\n0.00025\nEpsilon minimum\n0.1\nEpsilon decay\n0.995\nOptimizer\nAdam\nLoss function\nMean square error\n4\nResults\nTen Indian stock datasets and three deep Q-networds are used to perform the experiments. Each dataset is trained\non train data and tested on the unseen test data. Total rewards and proﬁt of training data and test data is calculated\nfor ten Indian stocks using three deep reinforcement learning models (DQN, Double DQN and Dueling DDQN) are\nshown in Table 2,3,4 respectively. Figure 2 shows the train and test data used for each dataset. We randomly choose\none stock dataset (ULTRACEMCO dataset) and plot the train and test data and also the training loss and training\nrewards with respect to number of epochs for DQN (Figure 3a,b). Mean square error is used to calculate the loss that\nestimates the difference between the actual and predicted values. Figure 3c shows the time-market value of the DQN\nmodel corresponding to the ULTRACEMCO dataset. Red, green and blue points corresponds to hold, buy and sell\nthe stock respectively. Similarly, Figure 4a,b,c shows the training loss, training rewards and time-market value for the\n4\nA PREPRINT - JULY 1, 2021\n(a)\n(b)\n(c)\nFigure 3: Plots showing (a) train loss (b) train rewards (c) time-market proﬁle of ULTRACEMCO stock using DQN\nTable 2: Rewards and proﬁt obtained during training and testing of the Indian stock datasets using DQN.\nDQN\nDataset\nTrain Rewards\nTrain Proﬁt\nTest Rewards\nTest Proﬁt\nTCS\n246\n12382\n22\n4770\nRELIANCE\n117\n17103\n-77\n1246\nZEEL\n295\n6639\n124\n2923\nTATAMOTORS\n210\n10506\n-1\n1670\nTECHM\n-426\n66\n-409\n-678\nUPL\n179\n3671\n82\n4828\nULTRACEMCO\n199\n8818\n16\n25188\nTATASTEEL\n225\n3481\n36\n48\nNESTLEIND\n-120\n11774\n-180\n16389\nPOWERGRID\n199\n1145\n51\n807\nULTRACEMCO dataset using Double DQN. Figure 5a,b,c shows the training loss, training rewards and time-market\nvalue for the ULTRACEMCO dataset using Dueling Double DQN. From Table 2,3,4 we observe that on an average the\nDueling DDQN performs better than rest two models and the performance of DDQN is better than DQN.\n5\nA PREPRINT - JULY 1, 2021\n(a)\n(b)\n(c) NESTLEIND train and test data\nFigure 4: Plots showing (a) train loss (b) train rewards (c) time-market proﬁle of ULTRACEMCO stock using Double\nDQN\nTable 3: Rewards and proﬁt obtained during training and testing of the Indian stock datasets using Double DQN.\nDouble DQN\nDataset\nTrain Rewards\nTrain Proﬁt\nTest Rewards\nTest Proﬁt\nTCS\n225\n14946\n276\n38095\nRELIANCE\n-175\n0\n-211\n48\nZEEL\n-1\n17\n3\n12\nTATAMOTORS\n52\n718\n85\n1067\nTECHM\n-15\n52\n3\n117\nUPL\n6\n409\n6\n658\nULTRACEMCO\n23\n655\n319\n57626\nTATASTEEL\n36\n1158\n-8\n8\nNESTLEIND\n7\n8589\n8\n22016\nPOWERGRID\n169\n-174\n167\n814\n5\nConclusion\nWe implemented deep reinforcement learning to automate trade execution and generate proﬁt. We also showed how\nwell DRL performs in solving stock market strategy problems and compared three DRL networks: DQN, DDQL and\nDueling DDQN for 10 Indian sock datasets. The experiments showed that all these three deep learning algorithms\nperform well in solving the decision-making problems of stock market strategies. Since, the stock markets are highly\nstochastic and changes very fast, these algorithms respond to these changes quickly and perform better than traditional\n6\nA PREPRINT - JULY 1, 2021\n(a)\n(b)\n(c)\nFigure 5: Plots showing (a) train loss (b) train rewards (c) time-market proﬁle of ULTRACEMCO stock using Dueling\nDDQN\nTable 4: Rewards and proﬁt obtained during training and testing of the Indian stock datasets using Dueling DDQN.\nDueling DDQN\nDataset\nTrain Rewards\nTrain Proﬁt\nTest Rewards\nTest Proﬁt\nTCS\n47\n3497\n114\n17278\nRELIANCE\n361\n29392\n347\n29769\nZEEL\n28\n1701\n151\n2836\nTATAMOTORS\n250\n16592\n188\n8312\nTECHM\n64\n26024\n86\n14831\nUPL\n104\n7972\n176\n10284\nULTRACEMCO\n123\n7113\n35\n6257\nTATASTEEL\n1\n17\n3\n57\nNESTLEIND\n139\n43900\n79\n101731\nPOWERGRID\n59\n560\n102\n1252\nmethods. We observe that on an average the Dueling DDQN network performed better than DDQN and DQN and\nDouble DQN performed better than DQN.\n7\nA PREPRINT - JULY 1, 2021\nReferences\n[1] M Hiransha, E Ab Gopalakrishnan, Vijay Krishna Menon, and KP Soman. Nse stock market prediction using\ndeep-learning models. Procedia computer science, 132:1351–1362, 2018.\n[2] SGM Fiﬁeld, DM Power, and DGS Knipe. The performance of moving average rules in emerging stock markets.\nApplied Financial Economics, 18(19):1515–1532, 2008.\n[3] Adebiyi A Ariyo, Adewumi O Adewumi, and Charles K Ayo. Stock price prediction using the arima model. In\n2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation, pages 106–112. IEEE,\n2014.\n[4] Dinesh Bhuriya, Girish Kaushal, Ashish Sharma, and Upendra Singh. Stock market predication using a linear\nregression. In 2017 International conference of Electronics, Communication and Aerospace Technology (ICECA),\nvolume 2, pages 510–513. IEEE, 2017.\n[5] Yahya Eru Cakra and Bayu Distiawan Trisedya. Stock price prediction using linear regression based on sentiment\nanalysis. In 2015 international conference on advanced computer science and information systems (ICACSIS),\npages 147–154. IEEE, 2015.\n[6] J Gregory Trafton, Erik M Altmann, and Raj M Ratwani. A memory for goals model of sequence errors. Cognitive\nSystems Research, 12(2):134–143, 2011.\n[7] Goutam Dutta, Pankaj Jha, Arnab Kumar Laha, and Neeraj Mohan. Artiﬁcial neural network models for forecasting\nstock price index in the bombay stock exchange. Journal of Emerging Market Finance, 5(3):283–295, 2006.\n[8] Reza Gharoie Ahangar, Mahmood Yahyazadehfar, and Hassan Pournaghshband. The comparison of methods\nartiﬁcial neural network with linear regression using speciﬁc variables for prediction stock price in tehran stock\nexchange. arXiv preprint arXiv:1003.1457, 2010.\n[9] Zahra Berradi and Mohamed Lazaar. Integration of principal component analysis and recurrent neural network to\nforecast the stock price of casablanca stock exchange. Procedia computer science, 148:55–61, 2019.\n[10] Taewook Kim and Ha Young Kim. Forecasting stock prices with a feature fusion lstm-cnn model using different\nrepresentations of the same data. PloS one, 14(2):e0212320, 2019.\n[11] David MQ Nelson, Adriano CM Pereira, and Renato A de Oliveira. Stock market’s price movement prediction\nwith lstm neural networks. In 2017 International joint conference on neural networks (IJCNN), pages 1419–1426.\nIEEE, 2017.\n[12] Adil Moghar and Mhamed Hamiche. Stock market prediction using lstm recurrent neural network. Procedia\nComputer Science, 170:1168–1173, 2020.\n[13] Parag C Pendharkar and Patrick Cusatis. Trading ﬁnancial indices with reinforcement learning agents. Expert\nSystems with Applications, 103:1–13, 2018.\n[14] Terry Lingze Meng and Matloob Khushi. Reinforcement learning in ﬁnancial markets. Data, 4(3):110, 2019.\n[15] Ren Jie Kuo. A decision support system for the stock market through integration of fuzzy neural networks and\nfuzzy delphi. Applied Artiﬁcial Intelligence, 12(6):501–520, 1998.\n[16] Norio Baba and Motokazu Kozaki. An intelligent forecasting system of stock price using neural networks. In\n[Proceedings 1992] IJCNN International Joint Conference on Neural Networks, volume 1, pages 371–377. IEEE,\n1992.\n[17] Jae Won Lee. Stock price prediction using reinforcement learning. In ISIE 2001. 2001 IEEE International\nSymposium on Industrial Electronics Proceedings (Cat. No. 01TH8570), volume 1, pages 690–695. IEEE, 2001.\n[18] John Moody and Matthew Saffell. Learning to trade via direct reinforcement. IEEE transactions on neural\nNetworks, 12(4):875–889, 2001.\n[19] Michael AH Dempster and Vasco Leemans. An automated fx trading system using adaptive reinforcement learning.\nExpert Systems with Applications, 30(3):543–552, 2006.\n[20] Yuming Li, Pin Ni, and Victor Chang. Application of deep reinforcement learning in stock trading strategies and\nstock forecasting. Computing, pages 1–18, 2019.\n[21] Zhuoran Xiong, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid. Practical deep reinforcement\nlearning approach for stock trading. arXiv preprint arXiv:1811.07522, 2018.\n[22] Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, and Qionghai Dai. Deep direct reinforcement learning\nfor ﬁnancial signal representation and trading. IEEE transactions on neural networks and learning systems,\n28(3):653–664, 2016.\n8\nA PREPRINT - JULY 1, 2021\n[23] João Carapuço, Rui Neves, and Nuno Horta. Reinforcement learning applied to forex trading. Applied Soft\nComputing, 73:783–794, 2018.\n[24] Ioannis Boukas, Damien Ernst, Thibaut Théate, Adrien Bolland, Alexandre Huynen, Martin Buchwald, Christelle\nWynants, and Bertrand Cornélusse. A deep reinforcement learning framework for continuous intraday market\nbidding. arXiv preprint arXiv:2004.05940, 2020.\n[25] Jinho Lee, Raehyun Kim, Yookyung Koh, and Jaewoo Kang. Global stock market prediction based on stock chart\nimages using deep q-network. IEEE Access, 7:167260–167277, 2019.\n[26] Mohit Sewak. Deep q network (dqn), double dqn, and dueling dqn. In Deep Reinforcement Learning, pages\n95–108. Springer, 2019.\n[27] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016.\n9\n",
  "categories": [
    "q-fin.TR",
    "cs.LG"
  ],
  "published": "2021-05-18",
  "updated": "2021-05-18"
}