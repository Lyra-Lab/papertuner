{
  "id": "http://arxiv.org/abs/2107.03340v3",
  "title": "Pseudo-Model-Free Hedging for Variable Annuities via Deep Reinforcement Learning",
  "authors": [
    "Wing Fung Chong",
    "Haoen Cui",
    "Yuxuan Li"
  ],
  "abstract": "This paper proposes a two-phase deep reinforcement learning approach, for\nhedging variable annuity contracts with both GMMB and GMDB riders, which can\naddress model miscalibration in Black-Scholes financial and constant force of\nmortality actuarial market environments. In the training phase, an infant\nreinforcement learning agent interacts with a pre-designed training\nenvironment, collects sequential anchor-hedging reward signals, and gradually\nlearns how to hedge the contracts. As expected, after a sufficient number of\ntraining steps, the trained reinforcement learning agent hedges, in the\ntraining environment, equally well as the correct Delta while outperforms\nmisspecified Deltas. In the online learning phase, the trained reinforcement\nlearning agent interacts with the market environment in real time, collects\nsingle terminal reward signals, and self-revises its hedging strategy. The\nhedging performance of the further trained reinforcement learning agent is\ndemonstrated via an illustrative example on a rolling basis to reveal the\nself-revision capability on the hedging strategy by online learning.",
  "text": "Pseudo-Model-Free Hedging for Variable Annuities via\nDeep Reinforcement Learning∗\nWing Fung Chong⋆, Haoen Cui♯, and Yuxuan Li†‡\n⋆Maxwell Institute for Mathematical Sciences and Department of Actuarial Mathematics and Statistics, Heriot-Watt\nUniversity, Edinburgh, United Kingdom. alfred.chong@hw.ac.uk.\n♯School of Computer Science, Georgia Institute of Technology, Atlanta, United States. haoen.cui@gatech.edu.\n‡Department of Mathematics, University of Illinois at Urbana-Champaign, Urbana, United States. yuxuanl9@illinois.edu.\nOctober 4, 2022\nAbstract\nThis paper proposes a two-phase deep reinforcement learning approach, for hedging variable annuity contracts\nwith both GMMB and GMDB riders, which can address model miscalibration in Black-Scholes ﬁnancial and\nconstant force of mortality actuarial market environments. In the training phase, an infant reinforcement learning\nagent interacts with a pre-designed training environment, collects sequential anchor-hedging reward signals, and\ngradually learns how to hedge the contracts.\nAs expected, after a suﬃcient number of training steps, the\ntrained reinforcement learning agent hedges, in the training environment, equally well as the correct Delta while\noutperforms misspeciﬁed Deltas. In the online learning phase, the trained reinforcement learning agent interacts\nwith the market environment in real time, collects single terminal reward signals, and self-revises its hedging\nstrategy. The hedging performance of the further trained reinforcement learning agent is demonstrated via an\nillustrative example on a rolling basis to reveal the self-revision capability on the hedging strategy by online\nlearning.\nKeywords: Two-phase deep reinforcement learning; Variable annuities hedging; Training phase; Sequential\nanchor-hedging reward signals; Online learning phase; Single terminal reward signals; Hedging strategy self-\nrevision.\n1\nIntroduction\nVariable annuities are long-term life products, in which policyholders participate in ﬁnancial investments for proﬁt\nsharing with insurers. Various guarantees are embedded in these contracts, such as guaranteed minimum matu-\nrity beneﬁt (GMMB), guaranteed minimum death beneﬁt (GMDB), guaranteed minimum accumulation beneﬁt\n(GMAB), guaranteed minimum income beneﬁt (GMIB), and guaranteed minimum withdrawal beneﬁt (GMWB).\nAccording to the Insurance Information Institute in 2020, the sales of variable annuity contracts in the United\nStates have amounted to, on average, 100.7 billion annually, from 2016 to 2020.\nDue to their popularity in the market and their dual-risk bearing nature, valuation and risk management of\nvariable annuities have been substantially studied in the literature. By the risk-neutral option pricing approach, to\nname a few, Milevsky and Posner (2001) studied the valuation of the GMDB rider; valuation and hedging of the\nGMMB rider under the Black-Scholes (BS) ﬁnancial market model were covered in Hardy (2003); the GMWB rider\nwas extensively investigated by Milevsky and Salisbury (2006), Dai et al. (2008), and Chen et al. (2008); valuation\nand hedging of the GMMB rider were studied in Cui et al. (2017) under the Heston ﬁnancial market model; valuation\nof the GMMB rider, together with the feature that a contract can be surrendered before its maturity, was examined\nby Jeon and Kwak (2018), in which optimal surrender strategies were also provided. For a comprehensive review\nof this approach, see Feng (2018).\nValuation and risk management of variable annuities have recently been advanced via various approaches as\nwell.\nTrottier et al. (2018) studied the hedging of variable annuities in the presence of basis risk based on a\n∗This work was ﬁrst initiated by the authors at the Illinois Risk Lab in January 2020. This work was presented at the 2020 Actuarial\nResearch Conference in August 2020, the United As One: 24th International Congress on Insurance: Mathematics and Economics in\nJuly 2021, the 2021 Actuarial Research Conference in August 2021, Heriot-Watt University in November 2021, University of Amsterdam\nin June 2022, and the 2022 Insurance Data Science Conference in June 2022. The authors thank the participants for fruitful comments.\nThis work utilizes resources supported by the National Science Foundation’s Major Research Instrumentation program, grant #1725729,\nas well as the University of Illinois at Urbana-Champaign. The authors are grateful to anonymous reviewers for their careful reading\nand insightful comments.\n†Corresponding author.\n1\narXiv:2107.03340v3  [q-fin.RM]  1 Oct 2022\nlocal optimization method. Chong (2019) revisited the pricing and hedging problem of equity-linked life insurance\ncontracts utilizing the so-called principle of equivalent forward preferences.\nFeng and Yi (2019) compared the\ndynamic hedging approach to the stochastic reserving approach for the risk management of variable annuities.\nMoenig (2021a) investigated the valuation and hedging problem of a portfolio of variable annuities via a dynamic\nprogramming method. Moenig (2021b) explored the impact of market incompleteness on the policyholder’s behavior.\nWang and Zou (2021) solved the optimal fee structure for the GMDB and GMMB riders. Dang et al. (2020) and\nDang et al. (2022) proposed and analyzed eﬃcient simulation methods for measuring the risk of variable annuities.\nRecently, state-of-the-art machine learning methods have been deployed to revisit the valuation and hedging\nproblems of variable annuities at a portfolio level. Gan (2013) proposed a three-step technique, by (i) selecting\nrepresentative contracts with clustering method, (ii) pricing these contracts with Monte Carlo (MC) simulation,\nand (iii) predicting the value of the whole portfolio based on the values of representative contracts with kriging\nmethod. To further boost the eﬃciency and the eﬀectiveness of selecting and pricing the representative contracts,\nas well as valuating the whole portfolio, various methods at each of these three steps have been proposed. For\ninstance, Gan and Lin (2015) extended the ordinary kriging method to the universal kriging method; Hejazi and\nJackson (2016) used a neural network as the predictive model to valuate the whole portfolio; Gan and Valdez\n(2018) implemented the generalized beta of the second kind method instead of the kriging method to capture the\nnon-Gaussian behavior of the market price of variable annuities. See also, Gan (2018), Gan and Valdez (2020),\nGweon et al. (2020), Liu and Tan (2020), Lin and Yang (2020), Feng et al. (2020), and Quan et al. (2021) for recent\ndevelopments in this three-step technique. Similar idea has also been applied to the calculation of Greeks and risk\nmeasures of a portfolio of variable annuities; see Gan and Lin (2017), Gan and Valdez (2017), and Xu et al. (2018).\nAll of the above literature applying the machine learning methods involve the supervised learning, which requires\na pre-labelled dataset (in this case, it is the set of fair prices of the representative contracts) to train a predictive\nmodel.\nOther than valuating and hedging variable annuities, supervised learning methods have also been applied to\ndiﬀerent actuarial contexts. W¨uthrich (2018) used a neural network for the chain-ladder factors in the chain-ladder\nclaim reserving model to include heterogeneous individual claim features. Gao and W¨uthrich (2019) applied a\nconvolutional neural network to classify drivers using their telematics data. Cheridito et al. (2020) estimated the\nrisk measures of a portfolio of assets and liabilities with a feedforward neural network. Richman and W¨uthrich\n(2021) and Perla et al. (2021) studied the mortality rate forecasting problem, where Richman and W¨uthrich (2021)\nextended the traditional Lee-Carter model to multiple populations using a neural network, while Perla et al. (2021)\napplied deep learning techniques directly on a time-series data of mortality rate. Hu et al. (2022) modiﬁed the loss\nfunction in tree-based models to improve the predictive performance when applying to imbalanced datasets which\nare common in the insurance practice.\nMeanwhile, a ﬂourishing sub-ﬁeld in machine learning, called the reinforcement learning (RL), has been skyrock-\neting and has proved its powerfulness in various tasks; see Silver et al. (2017), and the references therein. Contrary\nto the supervised learning, the RL does not require a pre-labelled dataset for training. Instead, in the RL, an agent\ninteracts with an environment, by sequentially observing states, taking, as well as revising, actions, and collecting\nrewards. Without possessing any prior knowledge of the environment, the agent needs to, explore the environment\nwhile exploit the collected reward signals, for learning. For a representative monograph of RL, see Sutton and Barto\n(2018); for its broad applications in economics, game theory, operations research, and ﬁnance, see the recent survey\npaper by Charpentier et al. (2021).\nThe mechanism of RL resembles how a hedging agent hedges any contingent claim dynamically. Indeed, the\nhedging agent could not know any speciﬁcs of the market environment, but could only observe states from the\nenvironment, take a hedging strategy, and learn from reward signals to progressively improve the hedging strategy.\nHowever, in the context of hedging, if an insurer builds a hedging agent based on a certain RL method, called RL\nagent hereafter, and allows this infant RL agent to interact and learn from the market environment right away, the\ninsurer could bear enormous ﬁnancial loss while the infant RL agent is still exploring the environment before it\ncould eﬀectively exploit the reward signals. Moreover, provided that the insurer could not know any speciﬁcs of the\nmarket environment as well, she could not supply any information derived from theoretical models to the infant RL\nagent, and thus the agent could only obtain the reward signals via the realized terminal proﬁt and loss, based on\nthe realized net liability and hedging portfolio value; these signals should not be eﬀective for an infant RL agent to\nlearn from the market environment.\nTo resolve these two issues above, we propose a two-phase (deep) RL approach, which is composed of a training\nphase and an online learning phase. In the training phase, based on her best knowledge of the market, the insurer\nconstructs a training environment. An infant RL agent is then designated to interact and learn from this training\nenvironment for a period of time. Comparing to putting the infant RL agent in the market environment right away,\nthe infant RL agent could be supplied by more information derived from the constructed training environment, such\nas the net liabilities before any terminal times. In this paper, we propose that the RL agent collects anchor-hedging\nreward signals during the training phase. After the RL agent is experienced with the training environment, in the\nonline learning phase, the insurer ﬁnally designates the trained RL agent in the market environment. Again, since\nno theoretical model for the market environment is available to the insurer, the trained RL agent could only collect\n2\nsingle terminal reward signals in this phase. In this paper, an illustrative example is provided to demonstrate the\nhedging performance using this approach.\nAll RL methods can be classiﬁed into either MC or temporal-diﬀerence (TD) learning. As a TD method shall\nbe employed in this paper, in both the training and online learning phases, the following RL literature review\nfocuses on the latter method. Sutton (1984) and Sutton (1988) ﬁrst introduced the TD method for prediction of\nvalue function. Based upon their works, Watkins (1989) and Watkins and Dayan (1992) proposed the well-known\nQ-learning for ﬁnite state and action spaces. Since then, the Q-learning has been improved substantially, in Hasselt\n(2010) for the Double Q-learning, and in Mnih et al. (2013), as well as Mnih et al. (2015), for the deep Q-learning\nwhich allows inﬁnite state space. Any Q-learning approaches, or in general tabular solution methods and value\nfunction approximation methods, are only applicable to ﬁnite action space. However, in the context of hedging, the\naction space is inﬁnite. Instead of discretizing the action space, proximal policy optimization (PPO) by Schulman\net al. (2017), which is a policy gradient method, shall be applied in this paper; our Section 3.4 shall provide its\nself-contained review.\nTo the best of our knowledge, this paper is the ﬁrst work to implement the RL algorithms with online learning to\nhedge contingent claims, particularly variable annuities. Contrary to Xu (2020) and Carbonneau (2021), in which\nboth adapted the state-of-the-art DH approach in B¨uhler et al. (2019), this paper is in line with the recent works\nby Kolm and Ritter (2019) and Cao et al. (2021), while extends with actuarial components. We shall outline the\ndiﬀerences between the RL and DH approaches throughout Sections 3 and 4, as well as Appendices A and B. Kolm\nand Ritter (2019) discretized the action space and implemented RL algorithms for ﬁnitely many possible actions;\nhowever, as mentioned above, this paper does not discretize the action space but adapts the recently advanced\npolicy gradient method, namely, the PPO. Comparing with Cao et al. (2021), in addition to the actuarial elements,\nthis paper puts forward online learning to self-revise the hedging strategy.\nIn the illustrative example, we assume that the market environment is the BS ﬁnancial and constant force of\nmortality (CFM) actuarial markets, and the focus is on contracts with both GMMB and GMDB riders. Furthermore,\nwe assume that the model of the market environment being presumed by the insurer, which shall be supplied as\nthe training environment, is also the BS and the CFM, but with a diﬀerent set of parameters. That is, while the\ninsurer constructs correct dynamic models of the market environment for the training environment, the parameters\nin the model of the market environment are not the same as those in the market environment. Section 2.4 shall\nset the stage of this illustrative example, and shall show that, if the insurer forwardly implements, in the market\nenvironment, the incorrect Delta hedging strategy based on her presumed model of the market environment, then\nits hedging performance for the variable annuities is worse than that by the correct Delta hedging strategy based\non the market environment. In Sections 4 and 6, this illustrative example shall be revisited using the two-phase\nRL approach. As we shall see in Section 6, the hedging performance of the RL agent is even worse than that of\nthe incorrect Delta, at the very beginning of hedging in real time. However, delicate analysis shows that, with a\nfair amount of future trajectories (which are diﬀerent from simulated scenarios, with more details in Section 6),\nthe hedging performance of the RL agent becomes comparable with that of the correct Delta within a reasonable\namount of time. Therefore, the illustrative example addresses model miscalibration issue in hedging variable annuity\ncontracts with GMMB and GMDB riders in BS ﬁnancial and CFM actuarial market environments, which is common\nin practice.\nThis paper is organized as follows. Section 2 formulates the continuous hedging problem for variable annuities,\nreformulates it to the discrete and Markov setting, and motivates as well as outlines the two-phase RL approach.\nSection 3 discusses the RL approach in hedging variable annuities and provides a self-contained review of RL,\nparticularly the PPO, which is a TD policy gradient method, while Section 5 presents the implementation details\nof the online learning phase. Sections 4 and 6 revisit the illustrative example in the training and online learning\nphases respectively. Section 7 collates the assumptions of utilizing the two-phase RL approach for hedging contingent\nclaims, as well as their implications in practice. This paper ﬁnally concludes and comments on future directions in\nSection 8.\n2\nProblem Formulation and Motivation\n2.1\nClassical Hedging Problem and Model-Based Approach\nWe ﬁrst review the classical hedging problem for variable annuities and its model-based solution to introduce some\nnotations and to motivate the RL approach.\n2.1.1\nActuarial and Financial Market Models\nLet (Ω, F, P) be a rich enough complete probability space. Consider the current time t = 0 and ﬁx T > 0 as a\ndeterministic time in the future. Throughout this paper, all time units are in year.\nThere are one risk-free asset and one risky asset in the ﬁnancial market. Let Bt and St, for t ∈[0, T], be the\n3\ntime-t values of the risk-free asset and the risky asset respectively. Let G(1) =\nn\nG(1)\nt\no\nt∈[0,T ] be the ﬁltration which\ncontains all ﬁnancial market information; in particular, both processes B = {Bt}t∈[0,T ] and S = {St}t∈[0,T ] are\nG(1)-adapted.\nThere are N policyholders in the actuarial market. For each policyholder i = 1, 2, . . . , N, denote T (i)\nxi as her\nrandom future lifetime, who is of age xi at the current time 0. Deﬁne, for each i = 1, 2, . . . , N, and for any t ≥0,\nJ(i)\nt\n= 1n\nT (i)\nxi >t\no, be the corresponding time-t jump value generated by the random future lifetime of the i-th\npolicyholder; that is, if the i-th policyholder survives at some time t ∈[0, T], J(i)\nt\n= 1; otherwise, J(i)\nt\n= 0. Let\nG(2) =\nn\nG(2)\nt\no\nt∈[0,T ] be the ﬁltration which contains all actuarial market information; in particular, all single-jump\nprocesses J(i) =\nn\nJ(i)\nt\no\nt∈[0,T ], for i = 1, 2, . . . , N, are G(2)-adapted.\nLet F = {Ft}t∈[0,T ] be the ﬁltration which contains all actuarial and ﬁnancial market information; that is,\nF = G(1) ∨G(2). Therefore, the ﬁltered probability space is given by (Ω, F, F, P).\n2.1.2\nVariable Annuities with Guaranteed Minimum Maturity Beneﬁt and Guaranteed Minimum\nDeath Beneﬁt Riders\nAt the current time 0, an insurer writes a variable annuity contract to each of these N policyholders. Each contract\nis embedded with both GMMB and GMDB riders. Assume that all these N contracts expire at the same ﬁxed time\nT. In the following, ﬁx a generic policyholder i = 1, 2, . . . , N.\nAt the current time 0, the policyholder deposits F (i)\n0\ninto her segregated account to purchase ρ(i) > 0 shares\nof the risky asset; that is, F (i)\n0\n= ρ(i)S0. Assume that the policyholder does not revise the number of shares ρ(i)\nthroughout the eﬀective time of the contract.\nFor any t ∈\nh\n0, T (i)\nxi ∧T\ni\n, the time-t segregated account value of the policyholder is given by F (i)\nt\n= ρ(i)Ste−m(i)t,\nwhere m(i) ∈(0, 1) is the continuously compounded annualized rate at which the asset-value-based fees are deducted\nfrom the segregated account by the insurer. For any t ∈\n\u0010\nT (i)\nxi ∧T, T\ni\n, the time-t segregated account value F (i)\nt\nmust be 0; indeed, if the policyholder dies before the maturity, i.e.\nT (i)\nxi\n< T, then, due to the GMDB rider\nof a minimum guarantee G(i)\nD\n> 0, the beneﬁciary inherits max\n\u001a\nF (i)\nT (i)\nxi\n, G(i)\nD\n\u001b\n, which can be decomposed into\nF (i)\nT (i)\nxi\n+\n\u0012\nG(i)\nD −F (i)\nT (i)\nxi\n\u0013\n+\n, at the policyholder’s death time T (i)\nxi right away. Due to the GMMB rider of a minimum\nguarantee G(i)\nM > 0, if the policyholder survives beyond the maturity, i.e. T (i)\nxi\n> T, the policyholder acquires\nmax\nn\nF (i)\nT , G(i)\nM\no\nat the maturity, which can be decomposed into F (i)\nT\n+\n\u0010\nG(i)\nM −F (i)\nT\n\u0011\n+.\n2.1.3\nNet Liability of Insurer\nThe liability of the insurer thus has two parts. The liability from the GMMB rider at the maturity for the i-th\npolicyholder, where i = 1, 2, . . . , N, is given by\n\u0010\nG(i)\nM −F (i)\nT\n\u0011\n+ if the i-th policyholder survives beyond the maturity,\nand is 0 otherwise. The liability from the GMDB rider at the death time T (i)\nxi\nfor the i-th policyholder, where\ni = 1, 2, . . . , N, is given by\n\u0012\nG(i)\nD −F (i)\nT (i)\nxi\n\u0013\n+\nif the i-th policyholder dies before the maturity, and is 0 otherwise.\nTherefore, at any time t ∈[0, T], the future gross liability of the insurer accumulated to the maturity for these N\ncontracts is given by\nN\nX\ni=1\n \u0010\nG(i)\nM −F (i)\nT\n\u0011\n+ J(i)\nT\n+ BT\nBT (i)\nxi\n\u0012\nG(i)\nD −F (i)\nT (i)\nxi\n\u0013\n+\n1{T (i)\nxi <T }J(i)\nt\n!\n.\nDenote V GL\nt\n, for t ∈[0, T], as the time-t value of the discounted (via the risk-free asset B) future gross liability of\nthe insurer; if the liability is 0, the value will be 0.\nFrom the asset-value-based fees collected by the insurer, a portion, known as the rider charge, is used to fund\nthe liability due to the GMMB and GMDB riders; the remaining portion is used to cover overhead, commissions,\nand any other expenses. From the i-th policyholder, where i = 1, 2, . . . , N, the insurer collects m(i)\ne F (i)\nt J(i)\nt\nas the\nrider charge at any time t ∈[0, T], where m(i)\ne\n∈\n\u00000, m(i)\u0003\n. Therefore, the cumulative future rider charge to be\ncollected, from any time t ∈[0, T] onward, till the maturity, by the insurer from these N policyholders, is given by\nPN\ni=1\nR T\nt m(i)\ne F (i)\ns J(i)\ns\n(BT /Bs) ds. Denote V RC\nt\n, for t ∈[0, T], as its time-t discounted (via the risk-free asset B)\nvalue; if the cumulative rider charge is 0, the value will be 0.\n4\nHence, due to these N variable annuity contracts with both GMMB and GMDB riders, for any t ∈[0, T], the\ntime-t net liability of the insurer for these N contracts is given by Lt = V GL\nt\n−V RC\nt\n, which is Ft-measurable.\nOne of the many ways to set the rate m(i) ∈(0, 1) for the asset-value-based fees, and the rate m(i)\ne\n∈\n\u00000, m(i)\u0003\nfor\nthe rider charge, for i = 1, 2, . . . , N, is based on the time-0 net liability of the insurer for the i-th policyholder. More\nprecisely, m(i) and m(i)\ne\nare determined via L(i)\n0\n= V GL,(i)\n0\n−V RC,(i)\n0\n= 0, where V GL,(i)\n0\nand V RC,(i)\n0\nare the time-0\nvalues of, respectively, the discounted future gross liability and the discounted cumulative future rider charge, of\nthe insurer for the i-th policyholder.\n2.1.4\nContinuous Hedging and Hedging Objective\nThe insurer aims to hedge this dual-risk bearing net liability via investing in the ﬁnancial market. To this end, let\n˜T be the death time of the last policyholder; that is, ˜T = maxi=1,2,...,N T (i)\nxi , which is random.\nWhile the net liability Lt is deﬁned for any time t ∈[0, T], as the diﬀerence between the values of discounted\nfuture gross liability and discounted cumulative future rider charge, Lt = 0 for any t ∈\n\u0010\n˜T ∧T, T\ni\n. Indeed, if\n˜T < T, then, for any t ∈\n\u0010\n˜T ∧T, T\ni\n, one has T (i)\nxi < t ≤T for all i = 1, 2, . . . , N, and hence, the future gross\nliability accumulated to the maturity, and the cumulative rider charge from time ˜T onward, are both 0, so are their\nvalues. Therefore, the insurer only hedges the net liability Lt, for any t ∈\nh\n0, ˜T ∧T\ni\n.\nLet Ht be the hedging strategy, i.e. the number of shares of the risky asset being held by the insurer, at time\nt ∈[0, T). Hence, Ht = 0, for any t ∈\nh\n˜T ∧T, T\n\u0011\n. Let H be the admissible set of hedging strategies, which is\ndeﬁned by\nH =\nn\nH = {Ht}t∈[0,T ) : (i) H is F-adapted, (ii) H ∈R, P × L-a.s., and (iii) for any t ∈\nh\n˜T ∧T, T\n\u0011\n, Ht = 0\no\n,\nwhere L is the Lebesgue measure on R.\nLet Pt be the time-t value, for t ∈[0, T], of the insurer’s hedging portfolio. Then P0 = 0, and together with\nthe rider charges collected from the N policyholders, as well as the withdrawal for paying the liabilities due to the\nbeneﬁciaries’ inheritance from those policyholders who have already been dead, for any t ∈(0, T],\nPt =\nZ t\n0\n(Ps −HsSs) dBs\nBs\n+\nZ t\n0\nHsdSs +\nN\nX\ni=1\nZ t\n0\nm(i)\ne F (i)\ns J(i)\ns ds −\nN\nX\ni=1\n\u0012\nG(i)\nD −F (i)\nT (i)\nxi\n\u0013\n+\n1{T (i)\nxi ≤t<T },\nwhich obviously depends on {Hs}s∈[0,t).\nAs in Bertsimas et al. (2000), the insurer’s hedging objective function at the current time 0 should be given by\nthe root-mean-square error (RMSE) of the terminal proﬁt and loss (P&L), which is, for any H ∈H,\nr\nEP\nh\n(P ˜T ∧T −L ˜T ∧T )2i\n.\nIf the insurer has full knowledge of the objective probability measure P, and hence the correct dynamics of the risk-\nfree asset and the risky asset in the ﬁnancial market, as well as the correct mortality model in the actuarial market,\nthe optimal hedging strategy, being implemented forwardly, is given by minimizing the RMSE of the terminal P&L:\nH∗= arg min\nH∈H\nr\nEP\nh\n(P ˜T ∧T −L ˜T ∧T )2i\n.\n2.2\nPitfall of Model-Based Approach\nHowever, having correct model is usually not the case in practice. Indeed, the insurer, who is the hedging agent\nabove, usually has little information regarding the objective probability measure P, and hence easily misspeciﬁes\nthe ﬁnancial market dynamics and the mortality model, which will in turn yield a poor performance from the\nsupposedly optimal hedging strategy when it is implemented forwardly in the future. Section 2.4 outlines such an\nillustrative example which shall be discussed throughout the remaining of this paper.\nTo rectify this, we propose a two-phase (deep) RL approach to solve an optimal hedging strategy.\nIn this\napproach, an RL agent, which is not the insurer herself but is built by the insurer to hedge on her behalf, does\nnot have any knowledge of the objective probability measure P, the ﬁnancial market dynamics, and the mortality\nmodel; Section 2.5 shall explain this approach in details. Before that, in the following Section 2.3, the classical\nhedging problem shall ﬁrst be reformulated with a Markov decision process (MDP) in a discrete-time setting so\nthat RL methods can be implemented. The illustrative example outlined in Section 2.4 shall be revisited using the\nproposed two-phase RL approach in Sections 4 and 6.\n5\nIn the remaining of this paper, unless otherwise speciﬁed, all expectation operators shall be taken with respect\nto the objective probability measure P, and denoted simply as E [·].\n2.3\nDiscrete and Markov Hedging\n2.3.1\nDiscrete Hedging and Hedging Objective\nLet t0, t1, . . . , tn−1 ∈[0, T), for some n ∈N, be the time when the hedging agent decides the hedging strategy, such\nthat 0 = t0 < t1 < · · · < tn−1 < T. Denote also tn = T.\nLet t˜n be the ﬁrst time (right) after the last policyholder dies or all contracts expire, for some ˜n = 1, 2, . . . , n,\nwhich is random; that is, t˜n = min\nn\ntk, k = 1, 2, . . . , n : tk ≥˜T\no\n, and when ˜T > T, by convention, min ∅= tn.\nTherefore, Ht = 0, for any t = t˜n, t˜n+1, . . . , tn−1. With a slight abuse of notation, the admissible set of hedging\nstrategies in discrete time is\nH =\nn\nH = {Ht}t=t0,t1,...,tn−1 : (i) for any t = t0, t1, . . . , tn−1, Ht is Ft-measurable,\n(ii) for any t = t0, t1, . . . , tn−1, Ht ∈R, P-a.s., and\n(iii) for any t = t˜n, t˜n+1, . . . , tn−1, Ht = 0} .\nWhile the hedging agent decides the hedging strategy at the discrete time points, the actuarial and ﬁnancial\nmarket models are continuous. Hence, the net liability Lt = V GL\nt\n−V RC\nt\nis still deﬁned for any time t ∈[0, T] as\nbefore. Moreover, if t ∈[tk, tk+1), for some k = 0, 1, . . . , n −1, Ht = Htk; thus, P0 = 0, and, if t ∈(tk, tk+1], for\nsome k = 0, 1, . . . , n −1,\nPt = (Ptk −HtkStk) Bt\nBtk\n+ HtkSt +\nN\nX\ni=1\nZ t\ntk\nm(i)\ne F (i)\ns J(i)\ns\nBt\nBs\nds −\nN\nX\ni=1\nBt\nBT (i)\nxi\n\u0012\nG(i)\nD −F (i)\nT (i)\nxi\n\u0013\n+\n1{tk<T (i)\nxi ≤t<T }.\n(1)\nFor any H ∈H, the hedging objective of the insurer at the current time 0 is\nr\nE\nh\n(Pt˜n −Lt˜n)2i\n. Hence, the optimal\ndiscrete hedging strategy, being implemented forwardly, is given by\nH∗= arg min\nH∈H\nr\nE\nh\n(Pt˜n −Lt˜n)2i\n= arg min\nH∈H\nE\nh\n(Pt˜n −Lt˜n)2i\n.\n(2)\n2.3.2\nMarkov Decision Process\nAn MDP can be characterized by its state space, action space, Markov transition probability, and reward signal. In\nturn, these derive the value function and the optimal value function, which are equivalently known as, respectively,\nthe objective function and the value function, in optimization as in the previous sections. In the remaining of this\npaper, we shall adapt the MDP language.\n• (State) Let X be the state space in Rp, where p ∈N. Each state in the state space represents a possible\nobservation with p features in the actuarial and ﬁnancial markets. Denote Xtk ∈X as the observed state at\nany time tk, where k = 0, 1, . . . , n; the state should minimally include an information related to the number\nof surviving policyholders PN\ni=1 J(i)\ntk , and the term to maturity T −tk, in order to terminate the hedging at\ntime t˜n, which is the ﬁrst time when PN\ni=1 J(i)\nt˜n = 0, or which is when T −t˜n = 0. The states (space) shall be\nspeciﬁed in Sections 4 and 5.\n• (Action) Let A be the action space in R. Each action in the action space is a possible hedging strategy. Denote\nHtk (Xtk) ∈A as the action at any time tk, where k = 0, 1, . . . , n −1, which is assumed to be Markovian\nwith respect to the observed state Xtk; that is, given the current state Xtk, the current action Htk (Xtk) is\nindependent of the past states Xt0, Xt1, . . . , Xtk−1. In the sequel, for notational simplicity, we simply write\nHtk to represent Htk (Xtk), for k = 0, 1, . . . , n −1. If the feature of the number of surviving policyholders\nPN\ni=1 J(i)\ntk\n= 0, for k = 0, 1, . . . , n −1, in the state Xtk, then Htk = 0; in particular, for any tk, where\nk = ˜n, ˜n + 1, . . . , n −1, the hedging strategy Htk = 0.\n• (Markov property) At any time tk, where k = 0, 1, . . . , n −1, given the current state Xtk and the current\nhedging strategy Htk, the transition probability distribution of the next state Xtk+1 in the market is inde-\npendent of the past states Xt0, Xt1, . . . , Xtk−1 and the past hedging strategies Ht0, Ht1, . . . , Htk−1; that is, for\nany Borel set B ∈B (X),\nP\n\u0000Xtk+1 ∈B|Htk, Xtk, Htk−1, Xtk−1, . . . , Ht1, Xt1, Ht0, Xt0\n\u0001\n= P\n\u0000Xtk+1 ∈B|Htk, Xtk\n\u0001\n.\n(3)\n6\n• (Reward) At any time tk, where k = 0, 1, . . . , n −1, given the current state Xtk in the market and the current\nhedging strategy Htk, a reward signal Rtk+1\n\u0000Xtk, Htk, Xtk+1\n\u0001\nis received, by the hedging agent, as a result\nof transition to the next state Xtk+1. The reward signal shall be speciﬁed after introducing the (optimal)\nvalue function below. In the sequel, occasionally, for notational simplicity, we simply write Rtk+1 to represent\nRtk+1\n\u0000Xtk, Htk, Xtk+1\n\u0001\n, for k = 0, 1, . . . , n −1.\n• (State, action, and reward sequence) The states, actions, and reward signals form an episode, which is sequen-\ntially given by:\n\b\nXt0, Ht0, Xt1, Rt1, Ht1, Xt2, Rt2, Ht2, . . . , Xt˜n−1, Rt˜n−1, Ht˜n−1, Xt˜n, Rt˜n\n\t\n.\n• (Optimal value function) Based on the reward signals, the value function, at any time tk, where k = 0, 1, . . . , n−\n1, with the state x ∈X, is deﬁned by, for any hedging strategies Htk, Htk+1, . . . , Htn−1,\nV\n\u0000tk, x; Htk, Htk+1, . . . , Htn−1\n\u0001\n= E\n\"n−1\nX\nl=k\nγtl+1−tkRtl+1\n\f\f\fXtk = x\n#\n,\n(4)\nwhere γ ∈[0, 1] is the discount rate; the value function, at the time tn = T with the state x ∈X, is deﬁned\nby V (tn, x) = 0. Hence, the optimal discrete hedging strategy, being implemented forwardly, is given by\nH∗= arg max\nH∈H\nE\n\"n−1\nX\nk=0\nγtk+1Rtk+1\n\f\f\fX0 = x\n#\n.\n(5)\nIn turn, the optimal value function, at any time tk, where k = 0, 1, . . . , n −1, with the state x ∈X, is\nV ∗(tk, x) = V\n\u0010\ntk, x; H∗\ntk, H∗\ntk+1, . . . , H∗\ntn−1\n\u0011\n, and V ∗(tn, x) = 0.\n(6)\n• (Reward engineering) To ensure the hedging problem being reformulated with the MDP, the value functions,\ngiven by that in (5), and the negative of that in (2), should coincide; that is,\nE\n\"n−1\nX\nk=0\nγtk+1Rtk+1\n\f\f\fX0 = x\n#\n= −E\nh\n(Pt˜n −Lt˜n)2i\n.\n(7)\nHence, two possible constructions for the reward signals are proposed as follows; each choice of the reward\nsignals shall be utilized in one of the two phases in the proposed RL approach.\n– (Single terminal reward) An obvious choice is to only have a reward signal from the negative squared\nterminal P&L; that is, for any time tk,\nRtk+1 =\n(\n−(Pt˜n −Lt˜n)2\nif k = ˜n −1,\n0\notherwise.\n(8)\nNecessarily, the discount rate is given as γ = 1.\n– (Sequential anchor-hedging reward) A less obvious choice is via telescoping the RHS of Equation (7),\nthat\n−E\nh\n(Pt˜n −Lt˜n)2i\n= −E\n\"˜n−1\nX\nk=0\n\u0010\u0000Ptk+1 −Ltk+1\n\u00012 −(Ptk −Ltk)2\u0011\n+ (P0 −L0)2\n#\n.\nTherefore, when L0 = P0, another possible construction for the reward signal is, for any time tk,\nRtk+1 =\n(\n(Ptk −Ltk)2 −\n\u0000Ptk+1 −Ltk+1\n\u00012\nif k = 0, 1, . . . , ˜n −1,\n0\notherwise.\n(9)\nAgain, the discount rate is necessarily given as γ = 1. The constructed reward in (9) outlines an anchor-\nhedging scheme. First, note that, at the current time 0, when L0 = P0, there is no local hedging error.\nThen, at each future hedging time before the last policyholder dies and before the maturity, the hedging\nperformance is measured by the local squared P&L, i.e. (Ptk −Ltk)2, which serves as an anchor. At\nthe next hedging time, if the local squared P&L is smaller than the anchor, it will be rewarded, i.e.\nRtk+1 > 0; however, if the local squared P&L becomes larger, it will be penalized, i.e. Rtk+1 < 0.\n7\n2.4\nIllustrative Example\nThe illustrative example below demonstrates the poor hedging performance by the Delta hedging strategy when\nthe insurer miscalibrates the parameters in the market environment. We consider that the insurer hedges a variable\nannuity contract, with both GMMB and GMDB riders, of a single policyholder, i.e. N = 1, with the contract\ncharacteristics given in Table 1.\nParameter\nValue\nExpiration date T\n1\nMinimum guarantee at maturity GM\n100\nMinimum guarantee at death GD\n100\nTable 1: Contract Characteristics\nThe market environment follows the Black-Scholes (BS) in the ﬁnancial part and the constant force of mortality\n(CFM) in the actuarial front. The risk-free asset earns a constant risk-free interest rate r > 0 that, for any t ∈[0, T],\ndBt = rBtdt, while the value of the risky asset evolves as a geometric Brownian motion that, for any t ∈[0, T],\ndSt = µStdt + σStdWt, where µ is a constant drift, σ > 0 is a constant volatility, and W = {Wt}t∈[0,T ] is the\nstandard Brownian motion. The random future lifetime of the policyholder Tx has a CFM ν > 0; that is, for any\n0 ≤t ≤s ≤T, the conditional survival probability P (Tx > s|Tx > t) = e−ν(s−t). Moreover, the Brownian motion\nW in the ﬁnancial market and the future lifetime Tx in the actuarial market are independent. Table 2 summarizes\nthe parameters in the market environment. Note that the risk-free interest rate, the risky asset initial price, the\ninitial age of the policyholder, and the investment strategy of the policyholder, are observable by the insurer.\n(a) Black-Scholes Financial Market\nParameter\nValue\nRisk-free interest rate r\n0.02\nRisky asset initial price S0\n100\nRisky asset drift µ\n−0.2\nRisky asset volatility σ\n0.4\n(b) Constant Force of Mortality Actuarial Market\nParameter\nValue\nInitial number of policyholders N\n1\nInitial age of policyholders x\n20\nConstant force of mortality ν\n0.03\nInvestment strategy of policyholders ρ\n1.19\nTable 2: Parameters setting of market environment\nBased on her best knowledge of the market, the insurer builds a model of the market environment. Suppose\nthat the model happens to be the BS and the CFM as the market environment, but the insurer miscalibrates the\nparameters. Table 3 lists these parameters in the model of the market environment. In particular, the risky asset\ndrift and volatility, as well as the force of mortality constant are diﬀerent from those in the market environment.\nFor the observable parameters, they are the same as those in the market environment.\n(a) Black-Scholes Financial Market\nParameter\nValue\nRisk-free interest rate r\n0.02\nRisky asset initial price S0\n100\nRisky asset drift µ\n0.08\nRisky asset volatility σ\n0.2\n(b) Constant Force of Mortality Actuarial Market\nParameter\nValue\nInitial number of policyholders N\n1\nInitial age of policyholders x\n20\nConstant force of mortality ν\n0.02\nInvestment strategy of policyholders ρ\n1.19\nTable 3: Parameters setting of model of market environment, with bolded parameters being diﬀerent from those in\nmarket environment\nAt any time t ∈[0, T], the value of the hedging portfolio of the insurer is given by (17), with N = 1, in which\nthe values of the risky asset and the single-jump process follow the market environment with the parameters in\nTable 2. At any time t ∈[0, T], the value of the net liability of the insurer is given by (16), with N = 1, in\nboth the market environment and its model; for its detailed derivations, we defer it to Section 4.1, as the model\nof the market environment, with multiple homogeneous policyholders for eﬀective training, shall be supplied as the\ntraining environment. Since the parameters in the model of the market environment (see Table 3) are diﬀerent from\nthose in the market environment (see Table 2), the net liability evaluated by the insurer using the model is diﬀerent\nfrom that of the market environment. There are two implications. Firstly, the Delta hedging strategy of the insurer\nusing the parameters in Table 3 is incorrect, while the correct Delta hedging strategy should use the parameters in\n8\nTable 2. Secondly, the asset-value-based fee m and the rider charge me given in Table 4, which are determined by\nthe insurer based on the time-0 value of her net liability by Table 3 via the method in Section 2.1.3, are mispriced.\nThey would not lead to zero time-0 value of her net liability in the market environment which is based on Table 2.\nParameter\nValue\nRate for asset-value-based fee m\n0.02\nRate for rider charge me\n0.019\nTable 4: Fee structures derived from model of market environment\nTo evaluate the hedging performance of the incorrect Delta strategy by the insurer in the market environment\nfor the variable annuity of contract characteristics in Table 1, 5000 market scenarios using the parameters in Table\n2 are simulated to realize terminal P&Ls. For comparison, the terminal P&Ls by the correct Delta hedging strategy\nare also obtained. Figure 1 shows the empirical density and cumulative distribution functions of the 5000 realized\nterminal P&Ls by each Delta hedging strategy, while Table 5 outlines the summary statistics of the empirical\ndistributions, in which \\\nRMSE is the estimated RMSE of the terminal P&L similar to (2).\nIn Figure 1a, the empirical density function of realized terminal P&Ls by the incorrect Delta hedging strategy\nis depicted to be more heavy-tailed on the left than that by the correct Delta strategy. In fact, the terminal P&L\nby the incorrect Delta hedging strategy is stochastically dominated by that by the correct Delta strategy in the\nﬁrst-order; see Figure 1b. Table 5 shows that the terminal P&L by the incorrect Delta hedging strategy has a mean\nand a median farther from zero, a higher standard deviation, larger left-tail risks in terms of Value-at-Risk and Tail\nValue-at-Risk, and a larger RMSE than that by the correct Delta strategy.\nThese observations conclude that, even in a market environment as simple as the BS and the CFM, the incorrect\nDelta hedging strategy based on the miscalibrated parameters by the insurer does not perform well when it is being\nimplemented forwardly. In general, the hedging performance of model-based approaches depends crucially on the\ncalibration of parameters for the model of the market environment.\n(a) Empirical density\n(b) Empirical cumulative distribution\nFigure 1: Empirical density and cumulative distribution functions of realized terminal P&Ls by diﬀerent Delta\nstrategies\nTerminal P&L of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\n\\\nRMSE\nHedging Strategy\nCorrect Delta\n−0.24\n−0.14\n2.96\n−4.00\n−5.59\n−5.99\n−7.22\n2.97\nIncorrect Delta\n−1.25\n−0.22\n3.41\n−6.27\n−8.80\n−9.24\n−11.05\n3.63\nTable 5: Summary statistics of empirical distributions of realized terminal P&Ls by diﬀerent Delta strategies\n2.5\nTwo-Phase Reinforcement Learning Approach\nIn an RL approach, at the current time 0, the insurer builds an RL agent to hedge on her behalf in the future.\nThe agent interacts with a market environment, by sequentially observing states, taking, as well as revising, actions,\nwhich are the hedging strategies, and collecting rewards. Without possessing any prior knowledge of the market\n9\nenvironment, the agent needs to, explore the environment while exploit the collected reward signals, for eﬀective\nlearning.\nAn intuitive proposition would be allowing an infant RL agent to learn directly from such market environment,\nlike the one in Section 2.4, moving forward. However, recall that the insurer actually does not know any exact\nmarket dynamics in the environment and thus is not able to provide any theoretical model for the net liability to\nthe RL agent. In turn, the RL agent could not receive any sequential anchor-hedging reward signal in (9) from the\nenvironment, but instead receives the single terminal reward signal in (8). Since the rewards, except the terminal\none, are all zero, the infant RL agent would learn ineﬀectively from such sparse rewards, i.e. the RL agent shall\ntake a tremendous amount of time to ﬁnally learn a nearly optimal hedging strategy in the environment. Most\nimportantly, while the RL agent is exploring and learning from the environment, which is not a simulated one, the\ninsurer could suﬀer from huge ﬁnancial burden due to any sub-optimal hedging performances.\nIn view of this, we propose that the insurer should ﬁrst designate the infant RL agent to interact and learn from\na training environment, which is constructed by the insurer based on her best knowledge of the market, for example,\nthe model of the market environment in Section 2.4. Since the training environment is known to the insurer (but\nis unknown to the RL agent), the RL agent can be supplied by a net liability theoretical model, and consequently\nlearn from the sequential anchor-hedging reward signal in (9) of the training environment. Therefore, the infant RL\nagent would be guided by the net liability to learn eﬀectively from the local hedging errors. After interacting and\nlearning from the training environment for a period of time, in order to gauge the eﬀectiveness, the RL agent shall\nbe tested for its hedging performance in simulated scenarios from the same training environment. This ﬁrst phase\nis called the training phase.\nTraining Phase:\n(i) The insurer constructs the MDP training environment.\n(ii) The insurer builds the infant RL agent which uses the PPO algorithm.\n(iii) The insurer assigns the RL agent in the MDP training environment to interact and learn for a period of time,\nduring which the RL agent collects the anchor-hedging reward signal in (9).\n(iv) The insurer deploys the trained RL agent to hedge in simulated scenarios from the same training environment\nand documents the baseline hedging performance.\nIf the hedging performance of the trained RL agent in the training environment is satisfactory, the insurer\nshould then proceed to assign it to interact and learn from the market environment.\nSince the training and\nmarket environments are usually diﬀerent, such as having diﬀerent parameters as in Section 2.4, the initial hedging\nperformance of the trained RL agent in the market environment is expected to diverge from the ﬁne baseline\nhedging performance in the training environment. However, diﬀerent from an infant RL agent, the trained RL\nagent is experienced so that the sparse reward signal in (8) should be suﬃcient for the agent to revise the hedging\nstrategy, from the nearly optimal one in the training environment to that in the market environment, within a\nreasonable amount of time. This second phase is called the online learning phase.\nOnline Learning Phase:\n(v) The insurer assigns the RL agent in the market environment to interact and learn in real time, during which\nthe RL agent collects the single terminal reward signal in (8).\nThese summarize the proposed two-phase RL approach. Figure 2 depicts the above sequence clearly. There are\nseveral assumptions underneath this two-phase RL approach in order to apply it eﬀectively to a hedging problem\nof a contingent claim; as they involve speciﬁcs in later sections, we collate their discussions and elaborate their\nimplications in practice in Section 7. In the following section, we shall brieﬂy review the training essentials of RL\nin order to introduce the PPO algorithm. For the details of online learning phase, we defer them until Section 5.\n3\nReview of Reinforcement Learning\n3.1\nStochastic Action for Exploration\nOne of the fundamental ideas in RL is that, at any time tk, where k = 0, 1, . . . , n −1, given the current state Xtk,\nthe RL agent does not take a deterministic action Htk but extends it to a stochastic action, in order to explore the\nMDP environment and in turn learn from the reward signals. The stochastic action is sampled through a so-called\npolicy, which is deﬁned below.\nLet P (A) be a set of probability measures over the action space A; each probability measure µ (·) ∈P (A)\nmaps a Borel set A ∈B (A) to µ\n\u0000A\n\u0001\n∈[0, 1]. The policy π (·) is a mapping from the state space X to the set of\nprobability measures P (A); that is, for any state x ∈X, π (x) = µ (·) ∈P (A). The value function and the optimal\n10\nInsurer\nRL Agent\nMDP Training Environment\n(i) construct\n(ii) build\n(iii) interact and\nlearn\nRL Agent\nMDP Training Environment\n(iv) hedge and\nrealize performance\nTraining Phase\n(a) Training phase\nRL Agent\nMarket Environment\n(v) interact and\nlearn in real time\n(v) interact and\nlearn in real time\nOnline Learning Phase\n(b) Online learning phase\nFigure 2: The relationship among insurer, RL agent, MDP training environment, and market environment of the\ntwo-phase RL approach\nvalue function, at any time tk, where k = 0, 1, . . . , ˜n −1, with the state x ∈X, are then generalized as, for any\npolicy π (·),\nV (tk, x; π (·)) = E\n\"˜n−1\nX\nl=k\nRtl+1\n\f\f\fXtk = x\n#\n,\nV ∗(tk, x) = sup\nπ(·)\nV (tk, x; π (·)) ;\n(10)\nat any time tk, where k = ˜n, ˜n+1, . . . , n−1, with the state x ∈X, for any policy π (·), V (tk, x; π (·)) = V ∗(tk, x) = 0.\nIn particular, if P (A) contains only all Dirac measures over the action space A, which is the case in the DH approach\nof B¨uhler et al. (2019) (see Appendix A for more details), the value function and the optimal value function reduce\nto (4) and (6). With this relaxed setting, solving the optimal hedging strategy H∗boils down to ﬁnding the optimal\npolicy π∗(·).\n3.2\nPolicy Approximation and Parameterization\nAs the hedging problem has the inﬁnite action space A, tabular solution methods for problems of ﬁnite state space\nand ﬁnite action space (such as Q-learning), or value function approximation methods for problems of inﬁnite\nstate space and ﬁnite action space (such as deep Q-learning) are not suitable. Instead, a policy gradient method is\nemployed.\nTo this end, the policy π (·) is approximated and parametrized by the weights θp in an artiﬁcial neural network\n(ANN); in turn, denote the policy by π (·; θp). The ANN Np (·; θp) (to be deﬁned in (11) below) takes a state x ∈X\nas the input vector, and outputs parameters of a probability measure in P (A). In the sequel, the set P (A) contains\nall Gaussian measures (see, for example, Wang et al. (2020) and Wang and Zhou (2020)), in which each has a mean\nc and a variance d2, which depend on the state input x ∈X and the ANN weights θp. Therefore, for any state\nx ∈X,\nπ (x; θp) = µ (·; θp) ∼Gaussian\n\u0000c (x; θp) , d2 (x; θp)\n\u0001\n,\nwhere\n\u0000c (x; θp) , d2 (x; θp)\n\u0001\n= Np (x; θp).\nWith such approximation and parameterization, solving the optimal policy π∗further boils down to ﬁnding the\n11\noptimal ANN weights θ∗\np. Hence, denote the value function and the optimal value function in (10) by V (tk, x; θp)\nand V\n\u0000tk, x; θ∗\np\n\u0001\n, for any tk, where k = 0, 1, . . . , ˜n −1, with x ∈X. However, the (optimal) value function still\ndepends on the objective probability measure P, the ﬁnancial market dynamics, and the mortality model, which are\nunknown to the RL agent. Before formally introducing the policy gradient methods to tackle this issue, we shall\nﬁrst explicitly construct the ANNs for the approximated policy, as well as for an estimate of the value function (to\nprepare the algorithm of policy gradient method to be reviewed below).\n3.3\nNetwork Architecture\nAs alluded above, in this paper, the ANN involves two parts, which are the policy network and the value function\nnetwork.\n3.3.1\nPolicy Network\nLet Np be the number of layers for the policy network. For l = 0, 1, . . . , Np, let d(l)\np\nbe the dimension of the l-th\nlayer, where the 0-th layer is the input layer; the 1, 2, . . . , (Np −1)-th layers are hidden layers; the Np-th layer\nis the output layer. In particular, d(0)\np\n= p, which is the number of features in the actuarial and ﬁnancial parts,\nand d(Np)\np\n= 2, which outputs the mean c and the variance d2 of the Gaussian measure.\nThe policy network\nNp : Rp →R2 is deﬁned as, for any x ∈Rp,\nNp (x) =\n\u0010\nW (Np)\np\n◦ψ ◦W (Np−1)\np\n◦ψ ◦W (Np−2)\np\n◦. . . ◦ψ ◦W (1)\np\n\u0011\n(x) ,\n(11)\nwhere, for l = 1, 2, . . . , Np, the mapping W (l)\np\n: Rd(l−1)\np\n→Rd(l)\np\nis aﬃne, and the mapping ψ : Rd(l)\np\n→Rd(l)\np\nis a\ncomponentwise activation function. Let θp be the parameter vector of the policy network; in turn, denote the policy\nnetwork in (11) by Np (x; θp), for any x ∈Rp.\n3.3.2\nValue Function Network\nThe value function network is constructed similarly as in the policy network, except that all subscripts p (policy)\nare replaced by v (value). In particular, the value function network Nv : Rp →R is deﬁned as, for any x ∈Rp,\nNv (x) =\n\u0010\nW (Nv)\nv\n◦ψ ◦W (Nv−1)\nv\n◦ψ ◦W (Nv−2)\nv\n◦. . . ◦ψ ◦W (1)\nv\n\u0011\n(x) ,\n(12)\nwhich models an approximated value function ˆV (see Section 3.4 below). Let θv be the parameter vector of the\nvalue function network; in turn, denote the value function network in (12) by Nv (x; θv), for any x ∈Rp.\n3.3.3\nShared Layers Structure\nSince the policy and value function networks should extract features from the input state vector in a similar manner,\nthey are assumed to share the ﬁrst few layers. More speciﬁcally, let Ns (< min {Np, Nv}) be the number of shared\nlayers for the policy and value function networks; for l = 1, 2, . . . , Ns, W (l)\np\n= W (l)\nv\n= W (l)\ns , and hence, for any\nx ∈Rp,\nNp (x; θp) =\n\u0010\nW (Np)\np\n◦ψ ◦W (Np−1)\np\n◦. . . ◦ψ ◦W (Ns+1)\np\n◦ψ ◦W (Ns)\ns\n◦. . . ◦ψ ◦W (1)\ns\n\u0011\n(x) ,\nNv (x; θv) =\n\u0010\nW (Nv)\nv\n◦ψ ◦W (Nv−1)\nv\n◦. . . ◦ψ ◦W (Ns+1)\nv\n◦ψ ◦W (Ns)\ns\n◦. . . ◦ψ ◦W (1)\ns\n\u0011\n(x) .\nLet θ be the parameter vector of the policy and value function networks. Figure 3 depicts such a shared layers\nstructure.\n3.4\nProximal Policy Optimization: A Temporal-Diﬀerence Policy Gradient Method\nA policy gradient method entails that, starting from initial ANN weights θ(0), and via interacting with the MDP\nenvironment to observe the states and collect the reward signals, the RL agent gradually updates the ANN weights,\nby the (stochastic) gradient ascent on a certain surrogate performance measure deﬁned for the ANN weights. That\nis, at each update step u = 1, 2, . . . ,\nθ(u) = θ(u−1) + α\n\\\n∇θJ (u−1) \u0000θ(u−1)\u0001\n,\n(13)\nwhere the hyperparameter α ∈[0, 1] is the learning rate of the RL agent, and, based on the experienced episode(s),\n\\\n∇θJ (u−1) \u0000θ(u−1)\u0001\nis the estimated gradient of the surrogate performance measure J (u−1) (·) evaluating at θ =\n12\n...\n...\n...\n...\nx1\nx2\nx4\nˆV (x)\nc (x)\nd2 (x)\nInput\nLayer\nShared\nLayer\nNon-Shared\nLayer\nOuput\nLayer\nFigure 3: An example of policy and value function artiﬁcial neural networks with a shared hidden layer and a\nnon-shared hidden layer\nθ(u−1).\nREINFORCE, which is pioneered by Williams (1992), is a Monte Carlo policy gradient method, which updates\nthe ANN weights by each episode. As this paper applies a temporal-diﬀerence (TD) policy gradient method, we\nrelegate the review of REINFORCE to Appendix B, where the Policy Gradient Theorem, the foundation of any\npolicy gradient methods, is presented.\nPPO, which is pioneered by Schulman et al. (2017), is a TD policy gradient method, which updates the ANN\nweights by a batch of K ∈N realizations. At each update step u = 1, 2, . . . , based on the ANN weights θ(u−1), and\nthus the policy π\n\u0010\n·; θ(u−1)\np\n\u0011\n, the RL agent experiences E(u) ∈N realized episodes for the K realizations.\n• If E(u) = 1, the episode is given by\n\u001a\n. . . , x(u−1)\nt\nK(u)\ns\n, h(u−1)\nt\nK(u)\ns\n, x(u−1)\nt\nK(u)\ns\n+1, r(u−1)\nt\nK(u)\ns\n+1, h(u−1)\nt\nK(u)\ns\n+1,\n. . . , x(u−1)\nt\nK(u)\ns\n+K−1, r(u−1)\nt\nK(u)\ns\n+K−1, h(u−1)\nt\nK(u)\ns\n+K−1, x(u−1)\nt\nK(u)\ns\n+K, r(u−1)\nt\nK(u)\ns\n+K, . . .\n\u001b\n,\nwhere K(u)\ns\n= 0, 1, . . . , ˜n −1, such that the time tK(u)\ns\nis when the episode is initiated in this update, and\nh(u−1)\ntk\n, for k = 0, 1, . . . , ˜n −1, is the time-tk realized hedging strategy being sampled from the Gaussian\ndistribution with the mean c\n\u0010\nx(u−1)\ntk\n; θ(u−1)\np\n\u0011\nand the variance d2 \u0010\nx(u−1)\ntk\n; θ(u−1)\np\n\u0011\n; necessarily, ˜n−K(u)\ns\n≥K.\n• If E(u) = 2, 3, . . . , the episodes are given by\n\u001a\n. . . , x(u−1,1)\nt\nK(u)\ns\n, h(u−1,1)\nt\nK(u)\ns\n, x(u−1,1)\nt\nK(u)\ns\n+1, r(u−1,1)\nt\nK(u)\ns\n+1, h(u−1,1)\nt\nK(u)\ns\n+1,\n. . . , x(u−1,1)\nt˜n(1)−1 , r(u−1,1)\nt˜n(1)−1 , h(u−1,1)\nt˜n(1)−1 , x(u−1,1)\nt˜n(1)\n, r(u−1,1)\nt˜n(1)\no\n,\nn\nx(u−1,2)\nt0\n, h(u−1,2)\nt0\n, x(u−1,2)\nt1\n, r(u−1,2)\nt1\n, h(u−1,2)\nt1\n,\n. . . , x(u−1,2)\nt˜n(2)−1 , r(u−1,2)\nt˜n(2)−1 , h(u−1,2)\nt˜n(2)−1 , x(u−1,2)\nt˜n(2)\n, r(u−1,2)\nt˜n(2)\no\n,\n. . . ,\n13\n\u001a\nx(u−1,E(u)−1)\nt0\n, h(u−1,E(u)−1)\nt0\n, x(u−1,E(u)−1)\nt1\n, r(u−1,E(u)−1)\nt1\n, h(u−1,E(u)−1)\nt1\n,\n. . . , x(u−1,E(u)−1)\nt\n˜n(E(u)−1)−1\n, r(u−1,E(u)−1)\nt\n˜n(E(u)−1)−1\n, h(u−1,E(u)−1)\nt\n˜n(E(u)−1)−1\n, x(u−1,E(u)−1)\nt\n˜n(E(u)−1)\n, r(u−1,E(u)−1)\nt\n˜n(E(u)−1)\n)\n,\n\u001a\nx(u−1,E(u))\nt0\n, h(u−1,E(u))\nt0\n, x(u−1,E(u))\nt1\n, r(u−1,E(u))\nt1\n, h(u−1,E(u))\nt1\n,\n. . . , x(u−1,E(u))\nt\nK(u)\nf\n−1\n, r(u−1,E(u))\nt\nK(u)\nf\n−1\n, h(u−1,E(u))\nt\nK(u)\nf\n−1\n, x(u−1,E(u))\nt\nK(u)\nf\n, r(u−1,E(u))\nt\nK(u)\nf\n, . . .\n\u001b\n,\nwhere K(u)\nf\n= 1, 2, . . . , ˜n(E(u)), such that the time tK(u)\nf\nis when the last episode is ﬁnished (but not necessarily\nterminated) in this update; necessarily, ˜n(1) −K(u)\ns\n+ PE(u)−1\ne=2\n˜n(e) + K(u)\nf\n= K.\nThe surrogate performance measure of PPO consists of three components. In the following, ﬁx an update step\nu = 1, 2, . . . .\nInspired by Schulman et al. (2015), in which the time-0 value function diﬀerence between two policies is shown to\nbe equal to the expected advantage, together with importance sampling and KL divergence constraint reformulation,\nthe ﬁrst component in the surrogate performance measure of PPO is given by:\n• if E(u) = 1,\nL(u−1)\nCLIP (θp) = E\n\n\nK(u)\ns\n+K−1\nX\nk=K(u)\ns\nmin\n\u001a\nq(u−1)\ntk\nˆA(u−1)\nθ(u−1)\np\n,tk, clip\n\u0010\nq(u−1)\ntk\n, 1 −ϵ, 1 + ϵ\n\u0011\nˆA(u−1)\nθ(u−1)\np\n,tk\n\u001b\n,\nwhere the importance sampling ratio q(u−1)\ntk\n=\nφ\n\u0010\nH(u−1)\ntk\n;X(u−1)\ntk\n,θp\n\u0011\nφ\n\u0010\nH(u−1)\ntk\n;X(u−1)\ntk\n,θ(u−1)\np\n\u0011, in which φ\n\u0010\n·; X(u−1)\ntk\n, θp\n\u0011\nis the Gaus-\nsian density function with mean c\n\u0010\nX(u−1)\ntk\n; θp\n\u0011\nand variance d2 \u0010\nX(u−1)\ntk\n; θp\n\u0011\n, the estimated advantage is\nevaluated at θp = θ(u−1)\np\nand bootstrapped through the approximated value function that\nˆA(u−1)\nθ(u−1)\np\n,tk =\n\n\n\n\n\n\n\n\n\n\n\nPK(u)\ns\n+K−1\nl=k\nR(u−1)\ntl+1\n+ ˆV\n\u0012\ntK(u)\ns\n+K, X(u−1)\nt\nK(u)\ns\n+K; θ(u−1)\nv\n\u0013\n−ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\nif K(u)\ns\n+ K < ˜n,\nP˜n−1\nl=k R(u−1)\ntl+1\n−ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\nif K(u)\ns\n+ K = ˜n,\nand the function clip\n\u0010\nq(u−1)\ntk\n, 1 −ϵ, 1 + ϵ\n\u0011\n= min\nn\nmax\nn\nq(u−1)\ntk\n, 1 −ϵ\no\n, 1 + ϵ\no\n. The approximated value func-\ntion ˆV is given by the output of the value network, i.e. ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n= Nv\n\u0010\nX(u−1)\ntk\n; θ(u−1)\nv\n\u0011\nas deﬁned\nin (12) for k = 0, 1, . . . , ˜n −1.\n• if E(u) = 2, 3, . . . ,\nL(u−1)\nCLIP (θp) = E\n\n\n˜n(1)−1\nX\nk=K(u)\ns\nmin\n\u001a\nq(u−1,1)\ntk\nˆA(u−1,1)\nθ(u−1)\np\n,tk, clip\n\u0010\nq(u−1,1)\ntk\n, 1 −ϵ, 1 + ϵ\n\u0011\nˆA(u−1,1)\nθ(u−1)\np\n,tk\n\u001b\n+\nE(u)−1\nX\ne=2\n˜n(e)−1\nX\nk=0\nmin\n\u001a\nq(u−1,e)\ntk\nˆA(u−1,e)\nθ(u−1)\np\n,tk, clip\n\u0010\nq(u−1,e)\ntk\n, 1 −ϵ, 1 + ϵ\n\u0011\nˆA(u−1,e)\nθ(u−1)\np\n,tk\n\u001b\n+\nK(u)\nf\n−1\nX\nk=0\nmin\n\u001a\nq(u−1,E(u))\ntk\nˆA(u−1,E(u))\nθ(u−1)\np\n,tk\n, clip\n\u0012\nq(u−1,E(u))\ntk\n, 1 −ϵ, 1 + ϵ\n\u0013\nˆA(u−1,E(u))\nθ(u−1)\np\n,tk\n\u001b\n\n.\nSimilar to REINFORCE in Appendix B, the second component in the surrogate performance measure of PPO\nminimizes the loss between the bootstrapped sum of reward signals and the approximated value function. To this\nend, deﬁne:\n14\n• if E(u) = 1,\nL(u−1)\nVF\n(θv) = E\n\n\nK(u)\ns\n+K−1\nX\nk=K(u)\ns\n\u0012\nˆA(u−1)\nθ(u−1)\np\n,tk + ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n−ˆV\n\u0010\ntk, X(u−1)\ntk\n; θv\n\u0011\u00132\n\n;\n• if E(u) = 2, 3, . . . ,\nL(u−1)\nVF\n(θv) = E\n\n\n˜n(1)−1\nX\nk=K(u)\ns\n\u0012\nˆA(u−1,1)\nθ(u−1)\np\n,tk + ˆV\n\u0010\ntk, X(u−1,1)\ntk\n; θ(u−1)\nv\n\u0011\n−ˆV\n\u0010\ntk, X(u−1,1)\ntk\n; θv\n\u0011\u00132\n+\nE(u)−1\nX\ne=2\n˜n(e)−1\nX\nk=0\n\u0012\nˆA(u−1,e)\nθ(u−1)\np\n,tk + ˆV\n\u0010\ntk, X(u−1,e)\ntk\n; θ(u−1)\nv\n\u0011\n−ˆV\n\u0010\ntk, X(u−1,e)\ntk\n; θv\n\u0011\u00132\n+\nK(u)\nf\n−1\nX\nk=0\n\u0012\nˆA(u−1,E(u))\nθ(u−1)\np\n,tk\n+ ˆV\n\u0012\ntk, X(u−1,E(u))\ntk\n; θ(u−1)\nv\n\u0013\n−ˆV\n\u0012\ntk, X(u−1,E(u))\ntk\n; θv\n\u0013\u00132\n\n.\nFinally, to encourage the RL agent exploring the MDP environment, the third component in the surrogate\nperformance measure of PPO is the entropy bonus. Based on the Gaussian density function, deﬁne\n• if E(u) = 1,\nL(u−1)\nEN\n(θp) = E\n\n\nK(u)\ns\n+K−1\nX\nk=K(u)\ns\nln d\n\u0010\nX(u−1)\ntk\n; θp\n\u0011\n\n;\n• if E(u) = 2, 3, . . . ,\nL(u−1)\nEN\n(θp) = E\n\n\n˜n(1)−1\nX\nk=K(u)\ns\nln d\n\u0010\nX(u−1,1)\ntk\n; θp\n\u0011\n+\nE(u)−1\nX\ne=2\n˜n(e)−1\nX\nk=0\nln d\n\u0010\nX(u−1,e)\ntk\n; θp\n\u0011\n+\nK(u)\nf\n−1\nX\nk=0\nln d\n\u0012\nX(u−1,E(u))\ntk\n; θp\n\u0013\n\n.\nTherefore, the surrogate performance measure of PPO is given by:\nJ (u−1) (θ) = L(u−1)\nCLIP (θp) −c1L(u−1)\nVF\n(θv) + c2L(u−1)\nEN\n(θp) ,\n(14)\nwhere the hyperparameters c1, c2 ∈[0, 1] are the loss coeﬃcients of the RL agent. Its estimated gradient, based on\nthe K realizations, is then computed via automatic diﬀerentiation; see, for example, Baydin et al. (2018).\n4\nIllustrative Example Revisited: Training Phase\nRecall that, in the training phase, the insurer constructs a model of the market environment for an MDP training\nenvironment, while the RL agent, which does not know any speciﬁcs of this MDP environment, observes states and\nreceives the anchor-hedging reward signals in (9) from it, and hence gradually learns the hedging strategy by the\nPPO algorithm reviewed in the last section. This section revisits the illustrative example in Section 2.4 via the\ntwo-phase RL approach in the training phase.\n4.1\nMarkov Decision Process Training Environment\nThe model of the market environment is the BS and the CFM in the ﬁnancial and the actuarial parts. However,\nunlike the model following the market environment to write a single contract to a single policyholder, for eﬀective\ntraining, the insurer writes identical contracts to N homogeneous policyholders in the training environment. Because\nof the homogeneity of the contracts and the policyholders, for all i = 1, 2, . . . , N, xi = x, ρ(i) = ρ, m(i) = m,\nG(i)\nM = GM, G(i)\nD = GD, m(i)\ne\n= me, and F (i)\nt\n= Ft = ρSte−mt, for t ∈[0, T].\n15\nAt any time t ∈[0, T], the future gross liability of the insurer accumulated to the maturity is thus\n(GM −FT )+\nPN\ni=1 J(i)\nT\n+ PN\ni=1 er(T −T (i)\nx ) \u0010\nGD −FT (i)\nx\n\u0011\n+ 1{T (i)\nx\n<T }J(i)\nt , and its time-t discounted value is\nV GL\nt\n= e−r(T −t)EQ\n\"\n(GM −FT )+\nN\nX\ni=1\nJ(i)\nT\n\f\f\fFt\n#\n+ EQ\n\" N\nX\ni=1\ne−r(T (i)\nx\n−t) \u0010\nGD −FT (i)\nx\n\u0011\n+ 1{T (i)\nx\n<T }J(i)\nt\n\f\f\fFt\n#\n= e−r(T −t)EQ \u0002\n(GM −FT )+ |Ft\n\u0003 N\nX\ni=1\nEQ h\nJ(i)\nT |Ft\ni\n+\nN\nX\ni=1\nJ(i)\nt EQ\n\u0014\ne−r(T (i)\nx\n−t) \u0010\nGD −FT (i)\nx\n\u0011\n+ 1{T (i)\nx\n<T }\n\f\f\fFt\n\u0015\n,\nwhere the probability measure Q deﬁned on (Ω, F) is an equivalent martingale measure with respect to P. Herein,\nthe probability measure Q is chosen to be the product measure of each individual equivalent martingale measure\nin the actuarial or ﬁnancial part, which implies the independence among the Brownian motion W and the future\nlifetime T (1)\nx , T (2)\nx , . . . , T (N)\nx\n, clarifying the ﬁrst term in the second equality above. The second term in that equality\nis due to the fact that, for i = 1, 2, . . . , N, the single-jump process J(i) is F-adapted. Under the probability measure\nQ, all future lifetime are identically distributed and have a CFM ν > 0, which are the same as those under the\nprobability measure P in Section 2.4. Therefore, for any i = 1, 2, . . . , N, and for any 0 ≤t ≤s ≤T, the conditional\nsurvival probability Q\n\u0010\nT (i)\nx\n> s|T (i)\nx\n> t\n\u0011\n= e−ν(s−t) . For each policyholder i = 1, 2, . . . , N, by the independence\nand the Markov property, for any 0 ≤t ≤s ≤T,\nEQ h\nJ(i)\ns |Ft\ni\n= EQ h\nJ(i)\ns |J(i)\nt\ni\n=\n\n\n\nQ\n\u0010\nT (i)\nx\n> s|T (i)\nx\n≤t\n\u0011\n= 0\nif\nT (i)\nx\n(ω) ≤t\nQ\n\u0010\nT (i)\nx\n> s|T (i)\nx\n> t\n\u0011\n= e−ν(s−t)\nif\nT (i)\nx\n(ω) > t\n.\n(15)\nMoreover, under the probability measure Q, for any t ∈[0, T], dFt = (r −m) Ftdt + σFtdW Q\nt , where W Q =\nn\nW Q\nt\no\nt∈[0,T ] is the standard Brownian motion under the probability measure Q. Hence, the time-t value of the\ndiscounted future gross liability, for t ∈[0, T], is given by\nV GL\nt\n= e−ν(T −t) \u0010\nGMe−r(T −t)Φ (−d2 (t, GM)) −Fte−m(T −t)Φ (−d1 (t, GM))\n\u0011 N\nX\ni=1\nJ(i)\nt\n+\nZ T\nt\n\u0010\nGDe−r(T −s)Φ (−d2 (s, GD)) −Fte−m(T −s)Φ (−d1 (s, GD))\n\u0011\nνe−ν(s−t)ds\nN\nX\ni=1\nJ(i)\nt ,\nwhere, for s ∈[0, T) and G > 0, d1 (s, G) =\nln(\nFs\nG )+\n\u0010\nr−m+ σ2\n2 (T −s)\n\u0011\nσ√T −s\n, d2 (s, G) = d1 (s, G) −σ\n√\nT −s, d1 (T, G) =\nlims→T −d1 (s, G), d2 (T, G) = d1 (T, G), and Φ (·) is the standard Gaussian distribution function.\nNote that\nPN\ni=1 J(i)\nt\nrepresents the number of surviving policyholders at time t ∈[0, T].\nAs for the cumulative future rider charge to be collected by the insurer from any time t ∈[0, T] onward, it is\ngiven by PN\ni=1\nR T\nt meFsJ(i)\ns er(T −s)ds, and its time-t discounted value is\nV RC\nt\n= e−r(T −t)EQ\n\" N\nX\ni=1\nZ T\nt\nmeFsJ(i)\ns er(T −s)ds\n\f\f\fFt\n#\n=\nN\nX\ni=1\nZ T\nt\nmee−r(s−t)EQ [Fs|Ft] EQ h\nJ(i)\ns |J(i)\nt\ni\nds,\nwhere the second equality is again due to the independence and the Markov property. Under the probability measure\nQ, EQ [Fs|Ft] = e(r−m)(s−t)Ft. Together with (15),\nV RC\nt\n= 1 −e−(m+ν)(T −t)\nm + ν\nmeFt\nN\nX\ni=1\nJ(i)\nt .\n16\nTherefore, the time-t net liability of the insurer, for t ∈[0, T], is given by\nLt = V GL\nt\n−V RC\nt\n=\n \ne−ν(T −t) \u0010\nGMe−r(T −t)Φ (−d2 (t, GM)) −Fte−m(T −t)Φ (−d1 (t, GM))\n\u0011\n+\nZ T\nt\n\u0010\nGDe−r(T −s)Φ (−d2 (s, GD)) −Fte−m(T −s)Φ (−d1 (s, GD))\n\u0011\nνe−ν(s−t)ds\n−1 −e−(m+ν)(T −t)\nm + ν\nmeFt\n! N\nX\ni=1\nJ(i)\nt ,\n(16)\nwhich contributes parts of the reward signals in (9). The time-t value of the insurer’s hedging portfolio, for t ∈[0, T],\nas in (1), is given by: P0 = 0, and if t ∈(tk, tk+1], for some k = 0, 1, . . . , n −1,\nPt = (Ptk −HtkStk) er(t−tk) + HtkSt + me\nZ t\ntk\nFser(t−s)\nN\nX\ni=1\nJ(i)\ns ds −\nN\nX\ni=1\ner(t−T (i)\nx ) \u0010\nGD −FT (i)\nx\n\u0011\n+ 1{tk<T (i)\nx\n≤t<T },\n(17)\nwhich is also supplied to the reward signals in (9).\nAt each time tk, where k = 0, 1, . . . , n, the RL agent is given to observe four features from this MDP environment;\nthese four features are summarized in the state vector\nXtk =\n \nln Ftk, Ptk\nN ,\nPN\ni=1 J(i)\ntk\nN\n, T −tk\n!\n.\n(18)\nThe ﬁrst feature is the natural logarithm of the segregated account value of the policyholder. The second feature\nis the hedging portfolio value of the insurer, being normalized by the initial number of policyholders. The third\nfeature is the ratio of the number of surviving policyholders with respect to the initial number of policyholders.\nThese features are either log-transformed or normalized to prevent the RL agent from exploring and learning from\nfeatures with high variability. The last feature is the term to maturity. In particular, when either the third or the last\nfeature ﬁrst hits zero, i.e. at time t˜n, an episode is terminated. The state space X = R × R × [0, 1/N, 2/N, . . . , 1] ×\n{0, t1, t2, . . . , T}.\nRecall that, at each time tk, where k = 0, 1, . . . , ˜n −1, with the state vector (18) being the input, the output of\nthe policy network in (11) is the mean c (Xtk; θp) and the variance d2 (Xtk; θp) of a Gaussian measure; herein, the\nGaussian measure represents the distribution of the average number of shares of the risky asset being held by the\ninsurer at the time tk for each surviving policyholder. Hence, for k = 0, 1, . . . , ˜n −1, the hedging strategy Htk in\n(17) is given by Htk = Htk\nPN\ni=1 J(i)\ntk , where Htk is sampled from the Gaussian measure. Since the hedging strategy\nis assumed to be Markovian with respect to the state vector, it can be shown, albeit tedious, that the state vector,\nin (18), and the hedging strategy together, satisfy the Markov property in (3).\nAlso recall that the infant RL agent is trained in the MDP environment with multiple homogeneous policyholders.\nThe RL agent should then eﬀectively update the ANN weights θ, and learn the hedging strategies, via a more direct\ninference on the force of mortality from the third feature in the state vector. The RL agent hedges daily, so that\nthe diﬀerence between the consecutive discrete hedging time is δtk = tk+1 −tk =\n1\n252, for k = 0, 1, . . . , n −1. In this\nMDP training environment, the parameters of the model are given in Table 3, but with N = 500.\n4.2\nBuilding Reinforcement Learning Agent\nAfter constructing this MDP training environment, the insurer builds the RL agent which implements the PPO,\nwhich was reviewed in Section 3.4. Table 6a summarizes all hyperparameters of the implemented PPO, in which\nthree of them are determined via grid search1, while the remaining two are ﬁxed a priori since they alter the surrogate\nperformance measure itself, and thus should not be based on grid search. Table 6b outlines the hyperparameters of\nthe ANN architecture in Section 3.3, which are all pre-speciﬁed, in which ReLU stands for Rectiﬁed Linear Unit;\nthat is, the componentwise activation function is given by, for any z ∈R, ψ (z) = max {z, 0}.\n4.3\nTraining of Reinforcement Learning Agent\nWith all these being set up, the insurer assigns the RL agent experiencing this MDP training environment, in order\nto observe the state, decide, as well as revise, the hedging strategy, and collect the anchor-hedging reward signal\n1The grid search was performed using the Hardware-Accelerated Learning cluster in the National Center for Supercomputing Appli-\ncations; see Kindratenko et al. (2020).\n17\n(a) Hyperparameters for Proximal Policy Optimization\nGrid-Searched\nPre-Speciﬁed\nHyperparameter\nValue\nHyperparameter\nValue\nLearning rate α\n0.07\nCoeﬃcient of value function\n0.25\nBatch size K\n2048\napproximation loss c1\nClip factor ϵ\n0.18\nCoeﬃcient of entropy bonus c2\n0.01\n(b) Hyperparameters for Neural Network\nHyperparameter\nValue(s)\nNumber of layers in policy network Np\n6\nNumber of layers in value function network Nv\n6\nNumber of shared layers Ns\n3\nDimension of hidden layers in policy network d(l)\np\n[32, 64, 128, 64, 32]\nDimension of hidden layers in value function network d(l)\nv\n[32, 64, 128, 64, 32]\nActivation function ψ (·)\nReLU\nTable 6: Hyperparameters setting of Proximal Policy Optimization and neural network\nbased on (9), as much as possible. Let U ∈N be the number of update steps in the training environment on the\nANN weights. Hence, the policy of the experienced RL agent is given by π\n\u0000·; θ(U)\u0001\n= π\n\u0010\n·; θ(U)\np\n\u0011\n.\nFigure 4 depicts the training log of the RL agent in terms of bootstrapped sum of rewards and batch entropy.\nIn particular, Figure 4a shows that the value function in (2) reduces to almost zero after around 108 training\ntimesteps, which is equivalent to around 48828 update steps for the ANN weights; within the same number of\ntraining timesteps, Figure 4b illustrates a gradual depletion on the batch entropy, and hence the Gaussian measure\ngently becomes more concentrating around its mean, which implies that the RL agent progressively diminishes the\ndegree of exploration on the MDP training environment, while increases the degree of exploitation on the learned\nANN weights.\n(a) Bootstrapped sum of rewards\n(b) Batch entropy\nFigure 4: Training log in terms of bootstrapped sum of rewards and batch entropy\n4.4\nBaseline Hedging Performance\nIn the ﬁnal step of the training phase, the trained RL agent is assigned to hedge in simulated scenarios from the\nsame MDP training environment, except that N = 1 which is in line with hedging in the market environment. The\ntrained RL agent takes the deterministic action c\n\u0010\n·; θ(U)\np\n\u0011\nwhich is the mean of the Gaussian measure.\nThe number of simulated scenarios is 5000. For each scenario, the insurer documents the realized terminal P&L,\ni.e. Pt˜n −Lt˜n. After all scenarios are experienced by the trained RL agent, the insurer examines the baseline hedging\nperformance via the empirical distribution and the summary statistics of the realized terminal P&Ls. The baseline\nhedging performance of the RL agent is also benchmarked with those by other methods, namely, the classical Deltas\nand the DH; see Appendix C for the implemented hyperparameters of the DH training. The following four classical\n18\nDeltas are implemented in the simulated scenarios from the training environment, in which the (in)correctness of\nthe Deltas are with respect to the training environment:\n• (correct) Delta of the CFM actuarial and BS ﬁnancial models with the model parameters as in Table 3;\n• (incorrect) Delta of the increasing force of mortality (IFM) actuarial and BS ﬁnancial models, where, for\nany i = 1, 2, . . . , N, if T < b, the conditional survival probability Q\n\u0010\nT (i)\nx\n> s|T (i)\nx\n> t\n\u0011\n=\nb−s\nb−t , for any\n0 ≤t ≤s ≤T < b, while if b ≤T, the conditional survival probability Q\n\u0010\nT (i)\nx\n> s|T (i)\nx\n> t\n\u0011\n= b−s\nb−t , for any\n0 ≤t ≤s < b ≤T, and Q\n\u0010\nT (i)\nx\n> s|T (i)\nx\n> t\n\u0011\n= 0, for any 0 ≤t ≤b ≤s ≤T or 0 ≤b ≤t ≤s ≤T, with the\nmodel parameters as in Tables 3a and 7;\n• (incorrect) Delta in the CFM actuarial and Heston ﬁnancial models, where, for any t ∈[0, T], dSt = µStdt +\n√ΣtStdW (1)\nt\n, dΣt = κ\n\u0000Σ −Σt\n\u0001\ndt + η√ΣtdW (2)\nt\n, and\n\nW (1), W (2)\u000b\nt = φt, with the model parameters as in\nTables 3b and 8;\n• (incorrect) Delta in the IFM actuarial and Heston ﬁnancial models with the model parameters as in Tables 7\nand 8.\nParameter\nValue\nInitial number of policyholder N\n1\nInitial age of policyholder x\n20\nLower bound of uniformly distributed lifetime b\n0\nUpper bound of uniformly distributed lifetime b\n50\nInvestment strategy of policyholders ρ\n1.19\nTable 7: Parameters setting of increasing force of mortality actuarial model for Delta\nParameter\nValue\nRisk-free interest rate r\n0.02\nRisky asset initial price S0\n100\nRisky asset drift µ\n0.08\nVariance initial value Σ0\n0.04\nVariance mean reversion rate κ\n0.2\nVariance long-run average Σ\n0.04\nVariance volatility η\n0.1\nBrownian motions correlation φ\n−0.5\nTable 8: Parameters setting of Heston ﬁnancial model for Delta\nFigure 5 shows the empirical density and cumulative distribution functions via the 5000 realized terminal P&Ls\nby each hedging approach, while Table 9 outlines the summary statistics of these empirical distributions.\nTo\nclearly illustrate the comparisons, Figure 6 depicts the empirical density functions via the 5000 pathwise diﬀerences\nof the realized terminal P&Ls between the RL agent and each of the other approaches, while Table 10 lists the\nsummary statistics of the empirical distributions; for example, comparing with the DH approach, the pathwise\ndiﬀerence of the realized terminal P&Ls for the e-th simulated scenario, for e = 1, 2, . . . , 5000, is calculated by\n\u0000P RL\nt˜n (ωe) −LRL\nt˜n (ωe)\n\u0001\n−\n\u0000P DH\nt˜n\n(ωe) −LDH\nt˜n (ωe)\n\u0001\n.\n19\n(a) Empirical density\n(b) Empirical cumulative distribution\nFigure 5: Empirical density and cumulative distribution functions of realized terminal P&Ls by the approaches of\nreinforcement learning, classical Deltas, and deep hedging\nTerminal P&L of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\n\\\nRMSE\nHedging Approach\nReinforcement Learning\n0.02\n−0.01\n0.58\n−0.54\n−0.87\n−1.05\n−1.43\n0.58\nCFM & BS Delta\n−0.01\n0.00\n0.38\n−0.44\n−0.63\n−0.70\n−0.89\n0.38\nIFM & BS Delta\n−0.06\n−0.06\n0.45\n−0.60\n−0.77\n−0.85\n−1.02\n0.45\nCFM & Heston Delta\n−0.32\n−0.33\n0.87\n−1.41\n−1.73\n−1.85\n−2.17\n0.93\nIFM & Heston Delta\n−0.53\n−0.53\n1.20\n−1.94\n−2.48\n−2.70\n−3.23\n1.31\nDeep Hedging\n−0.01\n−0.02\n0.60\n−0.52\n−0.71\n−1.04\n−1.49\n0.60\nTable 9: Summary statistics of empirical distributions of realized terminal P&Ls by the approaches of reinforcement\nlearning, classical Deltas, and deep hedging\n20\n(a) Reinforcement learning versus Delta in constant force\nof mortality and Black-Scholes models\n(b) Reinforcement learning versus Delta in increasing\nforce of mortality and Black-Scholes models\n(c) Reinforcement learning versus Delta in constant force\nof mortality and Heston models\n(d) Reinforcement learning versus Delta in increasing\nforce of mortality and Heston models\n(e) Reinforcement learning versus deep hedging\nFigure 6: Empirical density functions of realized pathwise diﬀerences of terminal P&Ls comparing with the ap-\nproaches of classical Deltas and deep hedging\nPathwise Diﬀerence of\nMean\nMedian\nStd. Dev.\nProbability of\nTerminal P&Ls Comparing With\nNon-Negativity\nCFM & BS Delta\n0.02\n0.01\n0.62\n50.6%\nIFM & BS Delta\n0.08\n0.07\n0.66\n54.7%\nCFM & Heston Delta\n0.34\n0.34\n1.01\n64.3%\nIFM & Heston Delta\n0.54\n0.58\n1.29\n70.0%\nDeep Hedging\n0.02\n0.01\n0.75\n51.3%\nTable 10: Summary statistics of empirical distributions of realized pathwise diﬀerences of terminal P&Ls comparing\nwith the approaches of classical Deltas and deep hedging\nAs expected, the baseline hedging performance of the trained RL agent in this training environment is comparable\nwith those by, the correct CFM and BS Delta, as well as the DH approach. Moreover, the RL agent outperforms\nall the other three incorrect Deltas, which are based on either incorrect IFM actuarial or Heston ﬁnancial model,\nor both.\n21\n5\nOnline Learning Phase\nGiven the satisfactory baseline hedging performance of the experienced RL agent in the MDP training environment,\nthe insurer ﬁnally assigns the agent to interact and learn from the market environment.\nTo distinguish them from the simulated time in the training environment, let ˜tk, for k = 0, 1, 2, . . . , be the real\ntime when the RL agent decides the hedging strategy in the market environment, such that 0 = ˜t0 < ˜t1 < ˜t2 < · · ·,\nand δ˜tk = ˜tk+1−˜tk =\n1\n252. Note that the current time t = ˜t0 = 0 and the RL agent shall hedge daily on behalf of the\ninsurer. At the current time 0, the insurer writes a variable annuity contract with the GMMB and GMDB riders to\nthe ﬁrst policyholder. When this ﬁrst contract terminates, due to either the death of the ﬁrst policyholder or the\nexpiration of the contract, the insurer shall write an identical contract, i.e. contract with the same characteristics,\nto the second policyholder. And so on. These contract re-establishments ensure that the insurer shall hold only\none written variable annuity contract with the GMMB and GMDB riders at a time, and the RL agent shall solely\nhedge the contract being eﬀective at that moment.\nTo this end, iteratively, for the ι-th policyholder, where ι ∈N, let ˜t˜n(ι) be the ﬁrst time (right) af-\nter the ι-th policyholder dies or the contract expires, for some ˜n(ι) = ˜n(ι−1) + 1, ˜n(ι−1) + 2, . . . , ˜n(ι−1) + n;\nthat is ˜t˜n(ι) = min\nn\n˜tk, k = ˜n(ι−1) + 1, ˜n(ι−1) + 2, . . . , ˜n(ι−1) + n : ˜tk −˜t˜n(ι−1) ≥T (ι)\nxι ∧T\no\n, where, by convention,\n˜n(0) = 0.\nTherefore, the contract eﬀective time for the ι-th policyholder τ (ι)\nk\n= ˜t˜n(ι−1)+k, where ι ∈N and\nk = 0, 1, . . . , ˜n(ι) −˜n(ι−1); in particular, τ (ι)\n0\n= ˜t˜n(ι−1) is the contract inception time for the ι-th policyholder. Figure\n7 depicts one of the possible realizations for clearly illustrating the real time and the contract eﬀective time.\nt\nReal time\nContract\neﬀective time\n˜t0 = 0\n=\nτ (1)\n0\n˜t1\n=\nτ (1)\n1\n˜t2\n=\nτ (1)\n2\n· · ·\n· · ·\n˜t˜n(1)−1\n=\nτ (1)\n˜n(1)−1\n˜t˜n(1)\n=\nτ (2)\n0\n=\nτ (1)\n˜n(1)\n˜t˜n(1)+1\n=\nτ (2)\n1\n· · ·\n· · ·\n˜t˜n(2)−1\n=\nτ (2)\n˜n(2)−˜n(1)−1\n˜t˜n(2)\n=\nτ (3)\n0\n=\nτ (2)\n˜n(2)−˜n(1)\n˜t˜n(2)+1\n=\nτ (3)\n1\n· · ·\n· · ·\n· · ·\nFirst policyholder\nSecond policyholder\n· · ·\nFigure 7: An illustrative timeline with the real time and the contract eﬀective time in the online learning phase\nIn the online learning phase, the trained RL agent carries on with the PPO of policy gradient methods in the\nmarket environment. That is, as in Section 3.4, starting from the ANN weights θ(U) at the current time 0, and via\ninteracting with the market environment to observe the states and collect the reward signals, the RL agent further\nupdates the ANN weights by a batch of ˜K ∈N realizations and the (stochastic) gradient ascent in (13) with the\nsurrogate performance measure in (14), at each update step.\nHowever, there are subtle diﬀerences of applying the PPO in the market environment from that in the training\nenvironment. At each further update step v = 1, 2, . . . , based on the ANN weights θ(U+v−1), and thus the policy\nπ\n\u0010\n·; θ(U+v−1)\np\n\u0011\n, the RL agent hedges each eﬀective contract of ˜E(v) ∈N realized policyholders for the ˜K ∈N\nrealizations.\nIndeed, the concept of episodes in the training environment, by the state re-initiation when one\nepisode ends, should be replaced by sequential policyholders in the real-time market environment, via the contract\nre-establishment when one policyholder dies or contract expires.\n• If ˜E(v) = 1, which is when (v −1) ˜K, v ˜K ∈\n\u0002\n˜n(ι−1), ˜n(ι)\u0003\n, for some ι ∈N, the batch of ˜K realizations is\ncollected solely from the ι-th policyholder. The realizations are given by\n(\n. . . , x(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n, h(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n, x(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+1\n, r(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+1\n, h(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+1\n,\n. . . , x(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+ ˜\nK−1\n, r(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+ ˜\nK−1\n, h(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+ ˜\nK−1\n, x(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+ ˜\nK\n, r(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+ ˜\nK\n, . . .\n)\n,\nwhere ˜K(v)\ns\n= 0, 1, . . . , ˜n(ι) −˜n(ι−1) −1, such that the time τ (ι)\n˜\nK(v)\ns\nis when the ﬁrst state is observed for the ι-th\npolicyholder in this update; necessarily, ˜n(ι) −˜n(ι−1) −˜K(v)\ns\n≥˜K.\n• If ˜E(v) = 2, 3, . . . , which is when (v −1) ˜K ∈\n\u0002\n˜n(ι−1), ˜n(ι)\u0003\nand v ˜K ∈\n\u0002\n˜n(j−1), ˜n(j)\u0003\n, for some ι, j ∈N such\nthat ι < j, the batch of ˜K realizations is collected from the ι-th, (ι + 1)-th, . . . , and j-th policyholders; that\n22\nis, ˜E(v) = j −ι + 1. The realizations are given by\n(\n. . . , x(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n, h(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n, x(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+1\n, r(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+1\n, h(v−1,ι)\nτ (ι)\n˜\nK(v)\ns\n+1\n,\n. . . , x(v−1,ι)\nτ (ι)\n˜n(ι)−˜n(i−1)−1\n, r(v−1,ι)\nτ (ι)\n˜n(ι)−˜n(i−1)−1\n, h(v−1,ι)\nτ (ι)\n˜n(ι)−˜n(i−1)−1\n, x(v−1,ι)\nτ (ι)\n˜n(ι)−˜n(i−1)\n, r(v−1,ι)\nτ (ι)\n˜n(ι)−˜n(i−1)\n)\n,\n\u001a\nx(v−1,ι+1)\nτ (ι+1)\n0\n, h(v−1,ι+1)\nτ (ι+1)\n0\n, x(v−1,ι+1)\nτ (ι+1)\n1\n, r(v−1,ι+1)\nτ (ι+1)\n1\n, h(v−1,ι+1)\nτ (ι+1)\n1\n,\n. . . , x(v−1,ι+1)\nτ (ι+1)\n˜n(ι+1)−˜n(ι)−1\n, r(v−1,ι+1)\nτ (ι+1)\n˜n(ι+1)−˜n(ι)−1\n, h(v−1,ι+1)\nτ (ι+1)\n˜n(ι+1)−˜n(ι)−1\n, x(v−1,ι+1)\nτ (ι+1)\n˜n(ι+1)−˜n(ι)\n, r(v−1,ι+1)\nτ (ι+1)\n˜n(ι+1)−˜n(ι)\n)\n,\n. . . ,\n\u001a\nx(v−1,j−1)\nτ (j−1)\n0\n, h(v−1,j−1)\nτ (j−1)\n0\n, x(v−1,j−1)\nτ (j−1)\n1\n, r(v−1,j−1)\nτ (j−1)\n1\n, h(v−1,j−1)\nτ (j−1)\n1\n,\n. . . , x(v−1,j−1)\nτ (j−1)\n˜n(j−1)−˜n(j−2)−1\n, r(v−1,j−1)\nτ (j−1)\n˜n(j−1)−˜n(j−2)−1\n, h(v−1,j−1)\nτ (j−1)\n˜n(j−1)−˜n(j−2)−1\n, x(v−1,j−1)\nτ (j−1)\n˜n(j−1)−˜n(j−2)\n, r(v−1,j−1)\nτ (j−1)\n˜n(j−1)−˜n(j−2)\n)\n,\n\u001a\nx(v−1,j)\nτ (j)\n0\n, h(v−1,j)\nτ (j)\n0\n, x(v−1,j)\nτ (j)\n1\n, r(v−1,j)\nτ (j)\n1\n, h(v−1,j)\nτ (j)\n1\n,\n. . . , x(v−1,j)\nτ (j)\n˜\nK(v)\nf\n−1\n, r(v−1,j)\nτ (j)\n˜\nK(v)\nf\n−1\n, h(v−1,j)\nτ (j)\n˜\nK(v)\nf\n−1\n, x(v−1,j)\nτ (j)\n˜\nK(v)\nf\n, r(v−1,j)\nτ (j)\n˜\nK(v)\nf\n, . . .\n\n\n,\nwhere ˜K(v)\nf\n= 1, 2, . . . , ˜n(j) −˜n(j−1), such that the time τ (j)\n˜\nK(v)\nf\nis when the last state is observed for the j-th\npolicyholder in this update; necessarily, ˜n(j−1) −˜n(i−1) + ˜K(v)\nf\n−˜K(v)\ns\n= ˜K.\nMoreover, the ﬁrst two features in the state vector (18) are based on the real-time risky asset price realization from\nthe market, while all features depend on a particular eﬀective policyholder. For ι ∈N and k = 0, 1, . . . , ˜n(ι) −˜n(ι−1),\nX(v−1,ι)\nτ (ι)\nk\n=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0012\nln F (ι)\nτ (ι)\nk , P (ι)\nτ (ι)\nk , 1, T −\n\u0010\nτ (ι)\nk\n−τ (ι)\n0\n\u0011\u0013\nif k = 0, 1, . . . , ˜n(ι) −˜n(ι−1) −1\n\u0012\nln F (ι)\nτ (ι)\nk , P (ι)\nτ (ι)\nk , 0, T −\n\u0010\nτ (ι)\nk\n−τ (ι)\n0\n\u0011\u0013\nif k = ˜n(ι) −˜n(ι−1) and T (ι)\nxι ≤T\n\u0012\nln F (ι)\nτ (ι)\nk , P (ι)\nτ (ι)\nk , 1, 0\n\u0013\nif k = ˜n(ι) −˜n(ι−1) and T (ι)\nxι > T\n,\n(19)\nwhere F (ι)\nt\n= ρ(ι)Ste\n−m(ι)\u0010\nt−τ (ι)\n0\n\u0011\n, if t ∈\nh\nτ (ι)\n0 , ˜t˜n(ι)\ni\n, P (ι)\nτ (ι)\n0\n= 0, and\nP (ι)\nτ (ι)\nk\n=\n\u0012\nP (ι)\nτ (ι)\nk−1 −H(ι)\nτ (ι)\nk−1Sτ (ι)\nk−1\n\u0013\ne\nr\n\u0010\nτ (ι)\nk\n−τ (ι)\nk−1\n\u0011\n+ H(ι)\nτ (ι)\nk−1Sτ (ι)\nk\n+ m(ι)\ne\nZ τ (ι)\nk\nτ (ι)\nk−1\nF (ι)\ns e\nr\n\u0010\nτ (ι)\nk\n−s\n\u0011\nJ(ι)\ns ds\n−\n\u0012\nGD −F (ι)\nT (ι)\nxι\n\u0013\n+\n1{τ (ι)\nk−1<T (ι)\nxι ≤τ (ι)\nk\n}e\nr\n\u0010\nτ (ι)\nk\n−T (ι)\nxι\n\u0011\n,\nfor k = 1, 2, . . . , ˜n(ι) −˜n(ι−1). Recall also that the reward signals collecting from the market environment should be\nbased on that in (8); that is, for ι ∈N and k = 0, 1, . . . , ˜n(ι) −˜n(ι−1),\nR(v−1,ι)\nτ (ι)\nk\n=\n\n\n\n0\nif\nk = 0, 1, . . . , ˜n(ι) −˜n(ι−1) −1\n−\n\u0010\nP (ι)\n˜t˜n(ι) −L(ι)\n˜t˜n(ι)\n\u00112\nif\nk = ˜n(ι) −˜n(ι−1)\n,\nin which L(ι)\n˜t˜n(ι) = 0 if T (ι)\nxι ≤T, and L(ι)\n˜t˜n(ι) =\n\u0012\nGM −F (ι)\nτ (ι)\n0\n+T\n\u0013\n+\nif T (ι)\nxι > T.\nTable 11 summarizes all hyperparameters of the implemented PPO in the market environment, while the hy-\nperparameters of the ANN architecture are still given in Table 6b. In the online learning phase, the insurer should\nchoose a smaller batch size ˜K comparing to that in the training phase; this yields a higher updating frequency by\nthe PPO to ensure that the experienced RL agent could revise the hedging strategy within a reasonable amount of\ntime. However, fewer realizations in the batch cause less credible updates; hence, the insurer should also tune down\n23\nthe learning rate ˜α, from that in the training phase, to reduce the reliance on each further update step.\nHyperparameter\nValue\nHyperparameter\nValue\nLearning rate ˜α\n0.001\nCoeﬃcient of value function\n0.25\nBatch size ˜K\n30\napproximation loss c1\nClip factor ϵ\n0.18\nCoeﬃcient of entropy bonus c2\n0.01\nTable 11: Hyperparameters setting of Proximal Policy Optimization for online learning with bolded hyperparameters\nbeing diﬀerent from those for training\n6\nIllustrative Example Revisited: Online Learning Phase\nThis section revisits the illustrative example in Section 2.4 via the two-phase RL approach in the online learning\nphase. In the market environment, the policyholders being sequentially written of the contracts with both GMMB\nand GMDB riders are homogeneous. Due to contract re-establishments to these sequential homogeneous policy-\nholders, the number and age of policyholders shall be reset to the values as in Table 3b at each contract inception\ntime. Furthermore, via the approach discussed in Section 2.1.3, to determine the fee structures of each contract at\nits inception time, the insurer relies on the parameters of the model of the market environment in Table 3, except\nthat now the risky asset initial price therein is replaced by the risky asset price observed at the contract inception\ntime. Note that the fee structures of the ﬁrst contract are still given as in Table 4, since the risky asset price\nobserved at t = 0 is exactly the risky asset initial price.\nLet V ∈N be the number of further update steps in the market environment on the ANN weights. In order to\nshowcase the result that, (RLw/OL) the further trained RL agent with the online learning phase, could gradually\nrevise the hedging strategy, from the nearly optimal one in the training environment, to the one in the market\nenvironment, we evaluate the hedging performance of RLw/OL on a rolling-basis. That is, right after each further\nupdate step v = 1, 2, . . . , V, we ﬁrst simulate ˜\nM = 500 market scenarios stemming from the real-time realized state\nvector x(v−1,j)\nτ (j)\n˜\nK(v)\nf\nand by implementing the hedging strategy from the updated policy π\n\u0010\n·; θ(U+v)\np\n\u0011\n, i.e. the further\ntrained RL agent takes the deterministic action c\n\u0010\n·; θ(U+v)\np\n\u0011\nwhich is the mean of the Gaussian measure; we then\ndocument the realized terminal P&L, for each of the 500 simulated scenarios, i.e. P RLw/OL\nt\n(ωe) −Lt (ωe), for\ne = 1, 2, . . . , 500, where t = ˜t˜n(j) (ωe) if τ (j)\n˜\nK(v)\nf\n< ˜t˜n(j), and t = ˜t˜n(j+1) (ωe) if τ (j)\n˜\nK(v)\nf\n= ˜t˜n(j).\nSince the state vector x(v−1,j)\nτ (j)\n˜\nK(v)\nf\nis realized in real time, the realized terminal P&L in fact depends on, not only the\nsimulated scenarios after each update, but also the actual realization in the market environment. To this end, from\nthe current time 0, we simulate M = 1000 future trajectories in the market environment; for each future trajectory\nf = 1, 2, . . . , 1000, the aforementioned realized terminal P&Ls are obtained as P RLw/OL\nt\n(ωf, ωe) −Lt (ωf, ωe), for\ne = 1, 2, . . . , 500, where t = ˜t˜n(j) (ωf, ωe) if τ (j)\n˜\nK(v)\nf\n(ωf) < ˜t˜n(j) (ωf), and t = ˜t˜n(j+1) (ωf, ωe) if τ (j)\n˜\nK(v)\nf\n(ωf) = ˜t˜n(j) (ωf).\nThe rolling-basis hedging performance of RLw/OL is benchmarked with those by, (RLw/oOL) the trained\nRL agent without the online learning phase, (CD) the correct Delta based on the market environment, and\n(ID) the incorrect Delta based on the training environment.\nFor the same set of future trajectories ωf, for\nf = 1, 2, . . . , 1000, and the same sets of simulated scenarios ωe, for e = 1, 2, . . . , 500, the realized terminal P&Ls\nare also obtained, by implementing each of these benchmark strategies starting from the current time 0, which\ndoes not need to be updated throughout; denote the realized terminal P&L as P S\nt (ωf, ωe) −Lt (ωf, ωe), where\nS = RLw/OL, RLw/oOL, CD, or ID.\nThis example considers V = 25 further update steps of RLw/OL, for each future trajectory ωf, where f =\n1, 2, . . . , 1000; as the batch size in the online learning phase ˜K = 30, this is equivalent to 750 trading days,\nwhich is just less than 3 years (assuming that non-trading days are uniformly spread across a year). For each\nf = 1, 2, . . . , 1000, and v = 1, 2, . . . , 25, let µ(v,j)\nS\n(ωf) be the expected terminal P&L, right after the v-th further\nupdate step implementing the hedging strategy S for the future trajectory ωf:\nµ(v,j)\nS\n(ωf) = E\n\nP S\nt (ωf, ·) −Lt (ωf, ·)\n\f\f\fX(v−1,j)\nτ (j)\n˜\nK(v)\nf\n= X(v−1,j)\nτ (j)\n˜\nK(v)\nf\n(ωf)\n\n,\nwhich is a conditional expectation taking with respect to the scenarios from the time τ (j)\n˜\nK(v)\nf\nforward; let ˆµ(v,j)\nS\n(ωf)\n24\nbe the sample mean of the terminal P&L based on the simulated scenarios:\nˆµ(v,j)\nS\n(ωf) =\n1\n500\n500\nX\ne=1\n\u0000P S\nt (ωf, ωe) −Lt (ωf, ωe)\n\u0001\n.\n(20)\nFigure 8 plots the sample means of the terminal P&L in (20), right after each further update step and implement-\ning each hedging strategy, in two future trajectories. Firstly, notice that, in both future trajectories, the average\nhedging performance of RLw/oOL is even worse than that of ID. Secondly, the average hedging performances of\nRLw/OL between the two future trajectories are substantially diﬀerent. In the best-case future trajectory, the\nRLw/OL is able to swiftly self-revise the hedging strategy, and hence quickly catch up the average hedging per-\nformance of ID by simply twelve further updates on the ANN weights, as well as that of CD in around two years;\nhowever, in the worst-case future trajectory, within 3 years, the RLw/OL is not able to improve the average hedging\nperformance to even the level of ID, let alone to that of CD.\nFigure 8: Best-case and worst-case samples of future trajectories for rolling-basis evaluation of reinforcement learning\nagent with online learning phase, and comparisons with classical Deltas and reinforcement learning agent without\nonline learning phase\nIn view of the second observation above, the hedging performance of RLw/OL should not be concluded for\neach future trajectory alone; instead, it should be studied among the future trajectories. To this end, for each\nf = 1, 2, . . . , 1000, deﬁne\nvCD (ωf) = min\nn\nv = 1, 2, . . . , 25 : ˆµ(v,j)\nRLw/OL (ωf) > ˆµ(v,j)\nCD (ωf)\no\nas the ﬁrst further update step such that the sample mean of the terminal P&L by RLw/OL is strictly greater than\nthat by CD, for the future trajectory ωf; herein, let min ∅= 26, and also deﬁne tCD (ωf) = vCD (ωf) ×\n˜\nK\n252 as the\n25\ncorresponding number of years. Therefore, the estimated proportion of the future trajectories, where RLw/OL is\nable to exceed the average hedging performance of CD within 3 years, is given by\n1\n1000\n1000\nX\nf=1\n1{tCD(ωf )≤3} = 95.4%.\nFor each f = 1, 2, . . . , 1000, deﬁne vID (ωf) and tID (ωf) similarly for comparing RLw/OL with ID. Figure 9 shows\nthe empirical conditional density functions of tCD and tID, both subject to that RLw/OL exceeds the average\nhedging performance of CD within 3 years.\nTable 12 lists the summary statistics of the empirical conditional\ndistributions.\nFigure 9: Empirical conditional density functions of ﬁrst surpassing times conditioning on reinforcement learning\nagent with online learning phase exceeding correct Delta in terms of sample means of terminal P&L within 3 years\nReinforcement Learning Agent\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nwith Online Learning Phase\nFirst Surpassing Time to\nCorrect Delta\n1.84\n1.79\n0.32\n2.38\n2.50\n2.66\n2.73\nIncorrect Delta\n1.41\n1.43\n0.22\n1.67\n1.67\n2.05\n2.05\nTable 12: Summary statistics of empirical conditional distributions of ﬁrst surpassing times conditioning on rein-\nforcement learning agent with online learning phase exceeding correct Delta in terms of sample means of terminal\nP&L within 3 years\nThe above analysis obviously neglected the variance, due to the simulated scenarios, of hedging performance by\neach hedging strategy. In the following, for each future trajectory, we deﬁne a reﬁned ﬁrst further update step such\nthat the expected terminal P&L by RLw/OL is statistically signiﬁcant to be strictly greater than that by CD. To\nthis end, for each f = 1, 2, . . . , 1000, and v = 1, 2, . . . , 25, consider the following null and alternative hypotheses:\nH(v,j)\n0,S\n(ωf) : µ(v,j)\nRLw/OL (ωf) ≤µ(v,j)\nS\n(ωf)\nversus\nH(v,j)\n1,S\n(ωf) : µ(v,j)\nRLw/OL (ωf) > µ(v,j)\nS\n(ωf) ,\nwhere S = CD or ID; the analysis before supports this choice of the alternative hypothesis. Deﬁne respectively the\ntest statistics and the p-value by\nT (v,j)\nS\n(ωf) =\nˆµ(v,j)\nRLw/OL (ωf) −ˆµ(v,j)\nS\n(ωf)\nr\nˆσ(v,j)\nRLw/OL(ωf )2\n500\n+ ˆσ(v,j)\nS\n(ωf )2\n500\nand\np(v,j)\nS\n(ωf) = P\n\u0010\nTS (ωf) > T (v,j)\nS\n(ωf)\n\u0011\n,\n26\nwhere the random variable TS (ωf) follows a Student’s t-distribution with the degree of freedom\ndf(v,j)\nS\n(ωf) =\n \nˆσ(v,j)\nRLw/OL(ωf )2\n500\n+ ˆσ(v,j)\nS\n(ωf )2\n500\n!2\n\u0012\nˆσ(v,j)\nRLw/OL(ωf )2/500\n\u00132\n500−1\n+\n\u0010\nˆσ(v,j)\nS\n(ωf )2/500\n\u00112\n500−1\n,\nand the sample variance ˆσ(v,j)\nS\n(ωf)2 of the terminal P&L based on the simulated scenarios is given by\nˆσ(v,j)\nS\n(ωf)2 =\n1\n499\n500\nX\ne=1\n\u0010\u0000P S\nt (ωf, ωe) −Lt (ωf, ωe)\n\u0001\n−ˆµ(v,j)\nS\n(ωf)\n\u00112\n.\nFor a ﬁxed level of signiﬁcance α∗∈(0, 1), if p(v,j)\nS\n(ωf) < α∗, then the expected terminal P&L by RLw/OL is\nstatistically signiﬁcant to be strictly greater than that by S = CD or ID.\nIn turn, for each f = 1, 2, . . . , 1000, and for any α∗∈(0, 1), deﬁne\nvp\nS (ωf; α∗) = min\nn\nv = 1, 2, . . . , 25 : p(v,j)\nS\n(ωf) < α∗o\nas the ﬁrst further update step such that the expected terminal P&L by RLw/OL is statistically signiﬁcant to be\nstrictly greater than that by S = CD or ID, for the future trajectory ωf at the level of signiﬁcance α∗; again, herein,\nlet min ∅= 26, and deﬁne tp\nS (ωf; α∗) = vp\nS (ωf; α∗) ×\n˜\nK\n252 as the corresponding number of years. Table 13 lists the\nestimated proportion of the future trajectories, where RLw/OL is statistically signiﬁcant to be able to exceed the\nexpected terminal P&L of S within 3 years, which is given by P1000\nf=1 1{tp\nS(ωf ;α∗)≤3}/1000, with various levels of\nsigniﬁcance.\nEstimated Proportion\nα∗= 0.20\nα∗= 0.15\nα∗= 0.10\nα∗= 0.05\nα∗= 0.01\nof Exceeding\nCorrect Delta\n55.7%\n52.1%\n47.6%\n35.9%\n21.8%\nIncorrect Delta\n96.9%\n95.1%\n85.0%\n70.6%\n64.6%\nTable 13: Estimated proportions of future trajectories where reinforcement learning agent with online learning\nphase is statistically signiﬁcant to be exceeding correct Delta and incorrect Delta within 3 years with various levels\nof signiﬁcance\nWhen the level of signiﬁcance α∗gradually decreases from 0.20 to 0.01, both estimated proportions, of\nthe future trajectories for RLw/OL being statistically signiﬁcant to be exceeding CD or ID within 3 years,\ndecline.\nThis is because, for any α∗\n1, α∗\n2 ∈(0, 1) with α∗\n1 ≤α∗\n2, and for any ωf, for f = 1, 2, . . . , 1000,\ntp\nS (ωf; α∗\n1) ≤3 implies that tp\nS (ωf; α∗\n2) ≤3, and thus 1{tp\nS(ωf ;α∗\n1)≤3}\n≤1{tp\nS(ωf ;α∗\n2)≤3}, which leads to\nthat P1000\nf=1 1{tp\nS(ωf ;α∗\n1)≤3}/1000 ≤P1000\nf=1 1{tp\nS(ωf ;α∗\n2)≤3}/1000; indeed, since tp\nS (ωf; α∗\n1) ≤3, or equivalently\nvp\nS (ωf; α∗\n1) ≤25, we have p(vp\nS(ωf ;α∗\n1),j)\nS\n(ωf) < α∗\n1 ≤α∗\n2, and thus\nvp\nS (ωf; α∗\n2) = min\nn\nv = 1, 2, . . . , 25 : p(v,j)\nS\n(ωf) < α∗\n2\no\n≤vp\nS (ωf; α∗\n1) ≤25,\nor equivalently tp\nS (ωf; α∗\n2) ≤tp\nS (ωf; α∗\n1) ≤3. However, notably, the declining rate of the estimated proportion for\nexceeding CD is greater than that for exceeding ID.\nSimilar to Figure 9 and Table 12, one can depict the empirical conditional density functions and list the summary\nstatistics of tp\nCD (·; α∗) and tp\nID (·; α∗), for each level of signiﬁcance α∗, subject to that RLw/OL is statistically\nsigniﬁcant to be exceeding CD within 3 years. For example, with α∗= 0.1, Figure 10 and Table 14 illustrate that,\ncomparing with Figure 9 and Table 12, the distributions are right-shifted as well as more spread, and the summary\nstatistics are all increased.\n27\nFigure 10: Empirical conditional density functions of ﬁrst statistically signiﬁcant surpassing times conditioning on\nreinforcement learning agent with online learning phase being statistically signiﬁcant to be exceeding correct Delta\nwithin 3 years for 0.1 level of signiﬁcance\nReinforcement Learning Agent\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nwith Online Learning Phase\nFirst Surpassing Time to\nCorrect Delta\n1.92\n1.90\n0.34\n2.50\n2.62\n2.61\n2.70\nIncorrect Delta\n1.70\n1.55\n0.24\n2.02\n2.14\n2.07\n2.20\nTable 14: Summary statistics of empirical conditional distributions of ﬁrst statistically signiﬁcant surpassing times\nconditioning on reinforcement learning agent with online learning phase being statistically signiﬁcant to be exceeding\ncorrect Delta within 3 years for 0.1 level of signiﬁcance\nFinally, to further examine the hedging performance of RLw/OL in terms of the sample mean of the terminal\nP&L in (20), as well as take the random future trajectories into account, Figure 11 shows the snapshots of the\nempirical density functions, among the future trajectories, of the sample mean by each hedging strategy over time\nat t = 0, 0.6, 1.2, 1.8, 2.4 and 3; Table 15 outlines their summary statistics. Note that, at the current time t = 0,\nsince none of the future trajectories has been realized yet, the empirical density functions are given by Dirac delta\nat the corresponding sample mean by each hedging strategy, which only depends on the simulated scenarios. As\nthe time progresses, one can observe that the empirical density function by RLw/OL is gradually shifting to the\nright, substantially passing the one by ID and almost catching up the one by CD at t = 1.8. This sheds light on the\nhigh probability that RLw/OL is able to self-revise the hedging strategy from a very sub-optimal one to a nearly\noptimal one close to the CD.\n28\n(a) t = 0\n(b) t = 0.6\n(c) t = 1.2\n(d) t = 1.8\n(e) t = 2.4\n(f) t = 3\nFigure 11: Snapshots of empirical density functions of sample mean of terminal P&L by reinforcement learning\nagent with online learning phase, reinforcement learning agent without online learning phase, correct Delta, and\nincorrect Delta at diﬀerent time points\n29\nSample Mean of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nTerminal P&L by\nRL with OL\n−2.66\n−2.66\n0\n−2.66\n−2.66\n−2.66\n−2.66\nRL without OL\n−2.66\n−2.66\n0\n−2.66\n−2.66\n−2.66\n−2.66\nCorrect Delta\n−0.26\n−0.26\n0\n−0.26\n−0.26\n−0.26\n−0.26\nIncorrect Delta\n−1.01\n−1.01\n0\n−1.01\n−1.01\n−1.01\n−1.01\n(a) t = 0\nSample Mean of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nTerminal P&L by\nRL with OL\n−2.27\n−2.19\n0.45\n−2.86\n−3.11\n−3.17\n−3.38\nRL without OL\n−2.71\n−2.69\n0.42\n−3.20\n−3.39\n−3.52\n−3.76\nCorrect Delta\n−0.24\n−0.26\n0.15\n−0.40\n−0.45\n−0.46\n−0.49\nIncorrect Delta\n−0.99\n−0.99\n0.16\n−1.20\n−1.26\n−1.27\n−1.31\n(b) t = 0.6\nSample Mean of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nTerminal P&L by\nRL with OL\n−1.29\n−1.14\n0.55\n−1.83\n−2.75\n−2.80\n−3.10\nRL without OL\n−2.71\n−2.68\n0.42\n−3.22\n−3.45\n−3.54\n−3.78\nCorrect Delta\n−0.24\n−0.26\n0.14\n−0.41\n−0.44\n−0.45\n−0.48\nIncorrect Delta\n−0.99\n−0.99\n0.16\n−1.19\n−1.25\n−1.27\n−1.33\n(c) t = 1.2\nSample Mean of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nTerminal P&L by\nRL with OL\n−0.63\n−0.43\n0.69\n−1.14\n−2.50\n−2.52\n−2.94\nRL without OL\n−2.70\n−2.67\n0.43\n−3.22\n−3.42\n−3.58\n−3.86\nCorrect Delta\n−0.25\n−0.27\n0.15\n−0.41\n−0.45\n−0.47\n−0.50\nIncorrect Delta\n−0.99\n−0.99\n0.15\n−1.20\n−1.25\n−1.27\n−1.31\n(d) t = 1.8\nSample Mean of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nTerminal P&L by\nRL with OL\n−0.46\n−0.33\n0.71\n−0.72\n−2.24\n−2.13\n−3.24\nRL without OL\n−2.69\n−2.66\n0.41\n−3.18\n−3.40\n−3.48\n−3.69\nCorrect Delta\n−0.24\n−0.26\n0.15\n−0.40\n−0.45\n−0.46\n−0.50\nIncorrect Delta\n−0.98\n−0.98\n0.15\n−1.18\n−1.24\n−1.26\n−1.30\n(e) t = 2.4\nSample Mean of\nMean\nMedian\nStd. Dev.\nVaR90\nVaR95\nTVaR90\nTVaR95\nTerminal P&L by\nRL with OL\n−0.45\n−0.33\n0.56\n−0.66\n−1.66\n−1.75\n−2.59\nRL without OL\n−2.71\n−2.68\n0.41\n−3.24\n−3.38\n−3.49\n−3.68\nCorrect Delta\n−0.24\n−0.26\n0.15\n−0.40\n−0.44\n−0.46\n−0.49\nIncorrect Delta\n−0.99\n−0.99\n0.15\n−1.19\n−1.24\n−1.26\n−1.31\n(f) t = 3\nTable 15: Summary statistics of empirical distributions of sample mean of terminal P&L by reinforcement learning\nagent with online learning phase, reinforcement learning agent without online learning phase, correct Delta, and\nincorrect Delta at diﬀerent time points\n30\n7\nMethodological Assumptions and Implications in Practice\nTo apply the proposed two-phase RL approach to a hedging problem of contingent claims, there are at least four\nassumptions to be satisﬁed. This section discusses these assumptions and elaborates their implications in practice.\n7.1\nObservable, Suﬃcient, Relevant, and Transformed Features in State\nOne of the crucial components in an MDP environment of the training phase or the online learning phase is the\nstate, in which the features provide information from the environment to the RL agent. First, the features must\nbe observable by the RL agent for learning. For instance, in our proposed state vectors (18) and (19), all the four\nfeatures, namely, the segregated account value, the hedging portfolio value, the number of surviving policyholders,\nand the term to maturity, are observable. Any unobservable, albeit desirable, features cannot be included in the\nstate, such as insider information which could provide a better inference on the future value of a risky asset, or exact\nhealth condition of a policyholder. Second, the observable features in the state should be suﬃcient for the RL agent\nto learn. For example, due to the dual-risk bearing nature of the contract in this paper, the proposed state vectors\n(18) and (19) incorporate both ﬁnancial and actuarial features; also, the third and the fourth features in the state\nvectors (18) and (19) would inform the RL agent to halt its hedging at the terminal time. However, incorporating\nsuﬃcient observable features in the state does not imply that every observable feature in the environment should\nbe included; the observable features in the state need to be relevant for learning eﬃciently. Since the segregated\naccount value and the term to maturity have already been included in the state vectors (18) and (19) as features,\nthe risky asset value and the hedging time are respective similar information from the environment, and thus are\nredundant features to be contained in the state. Finally, the features in the state which have high variance might\nbe appropriately transformed for reducing the volatility due to exploration. For instance, the segregated account\nvalue in the state vectors (18) and (19) is log-transformed in both phases.\n7.2\nReward Engineering\nAnother crucial component in an MDP environment is the reward, which supplies signals to the RL agent to evaluate\nits actions, i.e. the hedging strategy, for learning. First, the reward signals, if available, should suggest the local\nhedging performance. For example, in this paper, the RL agent is provided by the sequential anchor-hedging reward,\ngiven in (9), in the training phase; through the net liability value in the MDP training environment, the RL agent\noften receives a positive (resp. negative) signal for encouragement (resp. punishment), which is more informative\nthan collecting the zero reward. However, any informative reward signals need to be computable from an MDP\nenvironment. In this paper, since the insurer does not know the MDP market environment, the RL agent could\nnot be supplied the sequential anchor-hedging reward signals, which consist of the net liability values, in the online\nlearning phase, even though they are more informative; instead, the RL agent is given the less informative single\nterminal reward, given in (8), in the online learning phase which can be computed from the market environment.\n7.3\nMarkov Property in State and Action\nIn an MDP environment of the training phase or the online learning phase, the state and action pair needs to\nsatisfy the Markov property as in (3). In the training phase, since the MDP training environment is constructed,\nthe Markov property can be veriﬁed theoretically for the state, with the included features in line with Section 7.1,\nand the action, which is the hedging strategy. For example, in this paper, with the model of the market environment\nbeing the BS and the CFM, the state vector in (18) and the Markovian hedging strategy satisfy the Markov property\nin the training phase. Since the illustrative example in this paper assumes that the market environment also follows\nthe BS and the CFM, the state vector in (19) and the Markovian hedging strategy satisfy the Markov property in\nthe online learning phase as well. However, in general, as the market environment is unknown, the Markov property\nfor the state and action pair would need to be checked statistically in the online phase as follows.\nAfter the training phase and before an RL agent proceeding to the online learning phase, historical state and\naction sequences in a time frame are derived by hypothetically writing identical contingent claims and using the\nhistorical realizations from the market environment.\nFor instance, historical values of risky assets are publicly\navailable, or an insurer retrieves historical survival status of its policyholders with similar demographic information\nand medical history as the policyholder being actually written. These historical samples of the state and action pair\nare then used to conduct hypothesis testing on whether the Markov property in (3) holds for the pair in the market\nenvironment, by, for example, the test statistics proposed in Chen and Hong (2012). If the Markov property holds\nstatistically, the RL agent could begin the online learning phase. Yet, if the property does not hold statistically,\nthe state and action pair should be revised and then the training phase should be revisited; since the hedging\nstrategy is the action in a hedging problem, only the state could be amended by including more features from\nthe environment. Moreover, during the online learning phase, right after each further update step, new historical\nstate and action sequences in a shifted time frame of the same duration are obtained together with the most recent\n31\nhistorical realizations from the market environment and using the action samples being drawn from the updated\npolicy. These regularly new samples should be applied to statistically verify the Markov property on a rolling basis.\nIf the property fails to hold at any time, the state needs to be revised and the RL agent must be re-trained before\nresuming the online learning.\n7.4\nRe-Establishment of Contingent Claims in Online Learning Phase\nAny contingent claims must have a ﬁnite terminal time realization.\nOn one hand, in the training phase, that\nwould be the time when an episode ends and the state is re-initialized so that the RL agent can be trained in the\ntraining environment as long as possible. On the other hand, in the online learning phase, the market environment,\nand hence the state, could not be re-initialized; instead, at each terminal time realization, the seller re-establishes\nidentical contingent claims of the same contract characteristics and writing on (more or less) the same assets so\nthat the RL agent can be trained in the market environment successively. In this paper, the terms to maturity\nand the minimum guarantees of all variable annuity contracts in the online learning phase are the same. Moreover,\nall re-established contracts therein write on the same ﬁnancial risky asset, though the initial values of the asset\nare given by the real-time realizations in the market environment. Finally, while a new policyholder is written at\neach contract inception time, these policyholders have similar, if not identical, distributions of their random future\nlifetimes via examining their demographic information and medical history.\n8\nConcluding Remarks and Future Directions\nThis paper proposed the two-phase deep RL approach which can tackle practically common model miscalibration\nin hedging variable annuity contracts with both GMMB and GMDB riders in the BS ﬁnancial and CFM actuarial\nmarket environments. The approach is composed of the training phase and the online learning phase. While the sat-\nisfactory hedging performance of the trained RL agent in the training environment was anticipated, the performance\nby the further trained RL agent in the market environment via the illustrative example should be highlighted. First,\nby comparing their sample means of terminal P&L from simulated scenarios, in most future trajectories, within a\nreasonable amount of time, the further trained RL agent was able to exceed the hedging performance by the correct\nDelta from the market environment and the incorrect Delta from the training environment. Second, through a\nmore delicate hypothesis testing analysis, similar conclusions can be drawn in a fair amount of future trajectories.\nFinally, snapshots of empirical density functions, among the future trajectories, of the sample means of terminal\nP&L from simulated scenarios by each hedging strategy, shed light on the high probability that, the further trained\nRL agent is indeed able to self-revise the hedging strategy.\nThere should be at least two future directions derived from this paper. (I) The market environment in the\nillustrative example of this paper was assumed to be the BS ﬁnancial and CFM actuarial models, which turned out\nto be the same as designed by the insurer for the training environment, with diﬀerent parameters though. Moreover,\nthe policyholders were assumed to be homogeneous that their survival probabilities and investment behaviors are all\nthe same, with even identical contracts of the same minimum guarantee and maturity. In the market environment,\nthe agent only had to hedge one contract at a time, instead of a portfolio of contracts.\nObviously, if any of\nthese is to be relaxed, the trained RL agent from the current training environment should not be able to produce\nsatisfactory hedging performance in a market environment. Therefore, the training environment will certainly need\nto be substantially extended in terms of its sophistication, in order for the trained RL agent to be able to further\nlearn and hedge well in any realistic market environments. (II) Beyond this, an even more ambitious question needs\nto be addressed is that how much similar do the training and market environments have to be, such that the online\nlearning for self-revision on hedging strategy is possible, if not eﬃcient. This second future direction is related to\nthe transfer learning being adapted to the variable annuities hedging problem, and shall be investigated carefully\nin the future.\nReferences\nBaydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2018). Automatic diﬀerentiation in machine\nlearning: A survey. Journal of Machine Learning Research, 18(2):1–43.\nBertsimas, D., Kogan, L., and Lo, A. W. (2000). When is time continuous?\nJournal of Financial Economics,\n55(2):173–204.\nB¨uhler, H., Gonon, L., Teichmann, J., and Wood, B. (2019). Deep hedging. Quantitative Finance, 19(8):1271–1291.\nCao, J., Chen, J., Hull, J., and Poulos, Z. (2021). Deep hedging of derivatives using reinforcement learning. Journal\nof Financial Data Science, 3(1):10–27.\n32\nCarbonneau, A. (2021). Deep hedging of long-term ﬁnancial derivatives. Insurance: Mathematics and Economics,\n99:327–340.\nCharpentier, A., ´Elie, R., and Remlinger, C. (2021). Reinforcement learning in economics and ﬁnance. Computational\nEconomics (2021).\nChen, B., and Hong, Y. (2012). Testing for the Markov property in time series. Econometric Theory, 28:130–178.\nChen, Z., Vetzal, K., and Forsyth, P. (2008). The eﬀect of modelling parameters on the value of GMWB guarantees.\nInsurance: Mathematics and Economics, 43(1):165–173.\nCheridito, P., Ery, J., and W¨uthrich, M. V. (2020). Assessing asset-liability risk with neural networks. Risks, 8(1)\narticle 16.\nChong, W. F. (2019). Pricing and hedging equity-linked life insurance contracts beyond the classical paradigm: The\nprinciple of equivalent forward preferences. Insurance: Mathematics and Economics, 88:93–107.\nCui, Z., Feng, R., and MacKay, A. (2017). Variable annuities with VIX-linked fee structure under a Heston-type\nstochastic volatility model. North American Actuarial Journal, 21(3):458–483.\nDai, M., Kwok, Y. K., and Zong, J. (2008). Guaranteed minimum withdrawal beneﬁt in variable annuities. Mathe-\nmatical Finance, 18(4):595–611.\nDang, O., Feng, M., and Hardy, M. R. (2020). Eﬃcient nested simulation for conditional tail expectation of variable\nannuities. North American Actuarial Journal, 24(2):187–210.\nDang, O., Feng, M., and Hardy, M. R. (2022). Dynamic importance allocated nested simulation for variable annuity\nrisk measurement. Annals of Actuarial Science, 16(2):319–348.\nFeng, B. M., Tan, Z., and Zheng, J. (2020). Eﬃcient simulation designs for valuation of large variable annuity\nportfolios. North American Actuarial Journal, 24(2):275–289.\nFeng, R. (2018). An Introduction to Computational Risk Management of Equity-Linked Insurance. CRC Press.\nFeng, R. and Yi, B. (2019). Quantitative modeling of risk management strategies: Stochastic reserving and hedging\nof variable annuity guaranteed beneﬁts. Insurance: Mathematics and Economics, 85:60–73.\nGan, G. (2013). Application of data clustering and machine learning in variable annuity valuation. Insurance:\nMathematics and Economics, 53(3):795–801.\nGan, G. (2018). Valuation of large variable annuity portfolios using linear models with interactions. Risks, 6(3):1–19.\nGan, G. and Lin, X. S. (2015). Valuation of large variable annuity portfolios under nested simulation: A functional\ndata approach. Insurance: Mathematics and Economics, 62:138–150.\nGan, G. and Lin, X. S. (2017). Eﬃcient Greek calculation of variable annuity portfolios for dynamic hedging: A\ntwo-level metamodeling approach. North American Actuarial Journal, 21(2):161–177.\nGan, G. and Valdez, E. A. (2017). Modeling partial Greeks of variable annuities with dependence. Insurance:\nMathematics and Economics, 76:118–134.\nGan, G. and Valdez, E. A. (2018). Regression modeling for the valuation of large variable annuity portfolios. North\nAmerican Actuarial Journal, 22(1):40–54.\nGan, G. and Valdez, E. A. (2020). Valuation of large variable annuity portfolios with rank order kriging. North\nAmerican Actuarial Journal, 24(1):100–117.\nGao, G. and W¨uthrich, M. V. (2019). Convolutional neural network classiﬁcation of telematics car driving data.\nRisks, 7(1) article 6.\nGweon, H., Li, S., and Mamon, R. (2020). An eﬀective bias-corrected bagging method for the valuation of large vari-\nable annuity portfolios. ASTIN Bulletin: The Journal of the International Actuarial Association, 50(3):853–871.\nHardy, M. (2003). Investment Guarantees: Modeling and Risk Management for Equity-Linked Life Insurance. John\nWiley & Sons, Inc.\nHasselt, H. (2010). Double Q-learning. In Advances in Neural Information Processing Systems, volume 23.\nHejazi, S. A. and Jackson, K. R. (2016). A neural network approach to eﬃcient valuation of large portfolios of\nvariable annuities. Insurance: Mathematics and Economics, 70:169–181.\n33\nHu, C., Quan, Z., and Chong, W. F. (2022). Imbalanced learning for insurance using modiﬁed loss functions in\ntree-based models. Insurance: Mathematics and Economics, 106:13–32.\nJeon, J. and Kwak, M. (2018). Optimal surrender strategies and valuations of path-dependent guarantees in variable\nannuities. Insurance: Mathematics and Economics, 83:93–109.\nKindratenko, V., Mu, D., Zhan, Y., Maloney, J., Hashemi, S. H., Rabe, B., Xu, K., Campbell, R., Peng, J., and\nGropp, W. (2020). HAL: Computer system for scalable deep learning. pages 41–48. In Practice and Experience\nin Advanced Research Computing (PEARC ’20).\nKolm, P. N. and Ritter, G. (2019). Dynamic replication and hedging: A reinforcement learning approach. Journal\nof Financial Data Science, 1(1):159–171.\nLin, X. S. and Yang, S. (2020). Fast and eﬃcient nested simulation for large variable annuity portfolios: A surrogate\nmodeling approach. Insurance: Mathematics and Economics, 91:85–103.\nLiu, K. and Tan, K. S. (2020). Real-time valuation of large variable annuity portfolios: A green mesh approach.\nNorth American Actuarial Journal, 25(3):313-333.\nMilevsky, M. A. and Posner, S. E. (2001). The Titanic option: Valuation of the guaranteed minimum death beneﬁt\nin variable annuities and mutual funds. The Journal of Risk and Insurance, 68(1):93–128.\nMilevsky, M. A. and Salisbury, T. S. (2006). Financial valuation of guaranteed minimum withdrawal beneﬁts.\nInsurance: Mathematics and Economics, 38(1):21–38.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing\nAtari with deep reinforcement learning. arXiv: 1312.5602.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,\nWierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature,\n518:529–533.\nMoenig, T. (2021a). Eﬃcient valuation of variable annuity portfolios with dynamic programming. Journal of Risk\nand Insurance, 88(4):1023–1055.\nMoenig, T. (2021b). Variable annuities: Market incompleteness and policyholder behavior. Insurance: Mathematics\nand Economics, 99:63–78.\nPerla, F., Richman, R., Scognamiglio, S. and W¨uthrich, M. V. (2021). Time-series forecasting of mortality rates\nusing deep learning. Scandinavian Actuarial Journal, 7:572–598.\nQuan, Z., Gan, G., and Valdez, E. (2021). Tree-based models for variable annuity valuation: Parameter tuning and\nempirical analysis. Annals of Actuarial Science, 16(1):95-118.\nRichman, R. and W¨uthrich, M. V. (2021). A neural network extension of the Lee-Carter model to multiple popu-\nlations. Annals of Actuarial Science, 15(2):346–366.\nSchulman, J., Levine, S., Moritz, P., Jordan, M., and Abbeel, P. (2015). Trust region policy optimization. arXiv:\n1502.05477.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algo-\nrithms. arXiv: 1707.06347.\nTrottier, D. A., Godin, F., and Hamel, E. (2018). Local hedging of variable annuities in the presence of basis risk.\nASTIN Bulletin: The Journal of the International Actuarial Association, 48(2):611–646.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., baker, L., Lai, M.,\nbolton, A., chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., and Hassabis, D. (2017).\nMastering the game of Go without human knowledge. Nature, 550:354–359.\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Mas-\nsachusetts.\nSutton, R. S. (1988). Learning to predict by the methods of temporal diﬀerences. Machine Learning, 3:9–44.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press.\n34\nWang, G. and Zou, B. (2021). Optimal fee structure of variable annuities. Insurance: Mathematics and Economics,\n101:587–601.\nWang, H., Zariphopoulou, T., and Zhou, X. (2020). Reinforcement learning in continuous time and space: A\nstochastic control approach. Journal of Machine Learning Research, 21:1–34.\nWang, H. and Zhou, X. (2020). Continuous-time mean-variance portfolio selection: A reinforcement learning frame-\nwork. Mathematical Finance, 30(4):1273–1308.\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, University of Cambridge.\nWatkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8:297–292.\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine Learning, 8:229–256.\nW¨uthrich, M. V. (2018). Neural networks applied to chain-ladder reserving. European Actuarial Journal, 8:407–436.\nXu, W., Chen, Y., Coleman, C., and Coleman, T. F. (2018). Moment matching machine learning methods for risk\nmanagement of large variable annuity portfolios. Journal of Economic Dynamics and Control, 87:1–20.\nXu, X. (2020). Variable Annuity Guaranteed Beneﬁts: An Integrated Study of Financial Modelling, Actuarial Val-\nuation and Deep Learning. PhD thesis, UNSW Business School.\n35\nAppendix A\nDeep Hedging Approach\nIn this section, we provide a brief review of the DH approach adapted from B¨uhler et al. (2019). In particular, the\nhedging objective of the insurer is still given as\nr\nE\nh\n(Pt˜n −Lt˜n)2i\n, with Equation (2) being the optimal (discrete)\nhedging strategy. The hedging agent built by the insurer using the DH algorithm shall be called the DH agent\nhereafter.\nA.1\nDeterministic Action\nDiﬀerent from Section 3.1, in which the RL agent takes a stochastic action which is sampled from the policy for the\nexploration in the MDP environment, the DH agent only deploys a deterministic action HDH : X →A, which is\na direct mapping from the state space to the action space. Speciﬁcally, at each time tk, where k = 0, 1, . . . , n −1,\ngiven the current state Xtk ∈X, the DH agent takes an action HDH (Xtk) ∈A. In this case, the objective of the\nDH agent is to solve for the optimal hedging strategy HDH,∗(·) that minimizes\nr\nE\nh\n(Pt˜n −Lt˜n)2i\n, or equivalently\nminimizes E\nh\n(Pt˜n −Lt˜n)2i\n.\nA.2\nAction Approximation and Parameterization\nThe deterministic action mapping HDH : X →A is then approximated and parameterized by an ANN with weights\nυa. The construction of such ANN Na (·; υa) is similar to that in Section 3.3.1, except that Na (x; υa) ∈R for any\nx ∈Rp; that is, Na (·; υa) takes a state vector x ∈Rp as the input, and directly outputs a deterministic action\na (x; υa) ∈R, instead of the Gaussian mean-variance tuple\n\u0000c (x; υa) , d2 (x; υa)\n\u0001\n∈R × R+ in the RL approach,\nwhich then samples an action from the Gaussian measure. Hence, in the DH approach, solving the optimal hedging\nstrategy HDH,∗(·) boils down to ﬁnding the optimal weights υ∗\na.\nA.3\nDeep Hedging Method\nThe DH agent starts from initial ANN weights υ(0)\na , deploys the hedging strategy to collect terminal P&Ls, and\ngradually updates the ANN weights by stochastic gradient ascent as shown in Equation (13), with θ replaced by υ.\nFor the DH agent, at each update step u = 1, 2, . . . , the surrogate performance measure is given as\nJ (u−1) \u0010\nυ(u−1)\na\n\u0011\n= −E\n\u0014\u0010\nP (u−1)\nt˜n\n−L(u−1)\nt˜n\n\u00112\u0015\n.\nCorrespondingly, the gradient of the surrogate performance measure with respect to the ANN weights υa is\n∇υaJ (u−1) \u0010\nυ(u−1)\na\n\u0011\n= −2E\nh\u0010\nP (u−1)\nt˜n\n−L(u−1)\nt˜n\n\u0011\n∇υaP (u−1)\nt˜n\ni\n.\nTherefore, based on the realized terminal P&L p(u−1)\nt˜n\nand l(u−1)\nt˜n\n, the estimated gradient is given as\n\\\n∇υaJ (u−1)\n\u0010\nυ(u−1)\na\n\u0011\n= −2\n\u0010\np(u−1)\nt˜n\n−l(u−1)\nt˜n\n\u0011\n∇υap(u−1)\nt˜n\n.\nAlgorithm 1 summarizes the DH method above.\nAlgorithm 1 Pseudo-code for deep hedging method\nInput initial ANN model Na\n\u0010\n·; υ(0)\na\n\u0011\n, total number of updates ˆ\nM ∈N, learning rate ˆα ∈[0, 1].\nfor u = 1, 2, · · · , ˆ\nM do\n· Initialize the MDP training environment and observe the initial state vector x(u−1)\nt0\n.\n· Follow the hedging strategy Na\n\u0010\n·; υ(u−1)\na\n\u0011\nto realize an episode and evaluate the terminal P&L p(u−1)\nt˜n\nand\nl(u−1)\nt˜n\n.\n· Update υ(u−1)\na\nas\nυ(u)\na\n= υ(u−1)\na\n−2ˆα\n\u0010\np(u−1)\nt˜n\n−l(u−1)\nt˜n\n\u0011\n∇υap(u−1)\nt˜n\n.\nend\nReturn the trained ANN model Na\n\u0012\n·; υ( ˆ\nM)\na\n\u0013\n.\n36\nCompared with policy gradient methods introduced in Section 3.4, the DH method shows two key diﬀerences.\nFirst, it assumes that the hedging portfolio value P (u−1)\nt˜n\nis diﬀerentiable with respect to υa at each update u =\n1, 2, . . . . Second, the update of ANN weights does not depend on intermediate rewards collected during an episode;\nthat is, to update the weights, the DH agent has to experience a complete episode to realize the terminal P&L.\nTherefore, the update frequency of the DH method is lower than that of the RL method with TD feature.\nAppendix B\nREINFORCE: A Monte Carlo Policy Gradient Method\nAt each update step u = 1, 2, . . . , based on the ANN weights θ(u−1), and thus the policy π\n\u0010\n·; θ(u−1)\np\n\u0011\n, the RL agent\nexperiences the realized episode:\nn\nx(u−1)\nt0\n, h(u−1)\nt0\n, x(u−1)\nt1\n, r(u−1)\nt1\n, h(u−1)\nt1\n, . . . , x(u−1)\nt˜n−1 , r(u−1)\nt˜n−1 , h(u−1)\nt˜n−1 , x(u−1)\nt˜n\n, r(u−1)\nt˜n\no\n,\nwhere h(u−1)\ntk\n, for k = 0, 1, . . . , ˜n −1, is the time-tk realized hedging strategy being sampled from the Gaussian\ndistribution with the mean c\n\u0010\nx(u−1)\ntk\n; θ(u−1)\np\n\u0011\nand the variance d2 \u0010\nx(u−1)\ntk\n; θ(u−1)\np\n\u0011\n. In the following, ﬁx an update\nstep u = 1, 2, . . . .\nREINFORCE takes directly the time-0 value function V (u−1) (0, x; θp), for any x ∈X, as a part of the surrogate\nperformance measure:\nV (u−1) (0, x; θp) = E\n\"˜n−1\nX\nk=0\nR(u−1)\ntk+1\n\f\f\fX(u−1)\n0\n= x\n#\n.\nIn Williams (1992), the Policy Gradient Theorem was proved, which states that\n∇θpV (u−1) (0, x; θp) = E\n\"˜n−1\nX\nk=0\n ˜n−1\nX\nl=k\nR(u−1)\ntl+1\n!\n∇θp ln φ\n\u0010\nH(u−1)\ntk\n; X(u−1)\ntk\n, θp\n\u0011 \f\f\fX(u−1)\n0\n= x\n#\n,\nwhere φ\n\u0010\n·; X(u−1)\ntk\n, θp\n\u0011\nis the Gaussian density function with mean c\n\u0010\nX(u−1)\ntk\n; θp\n\u0011\nand variance d2 \u0010\nX(u−1)\ntk\n; θp\n\u0011\n.\nTherefore, based on the realized episode, the estimated gradient of the time-0 value function is given by\n\\\n∇θpV (u−1)\n\u0010\n0, x; θ(u−1)\np\n\u0011\n=\n˜n−1\nX\nk=0\n ˜n−1\nX\nl=k\nr(u−1)\ntl+1\n!\n∇θp ln φ\n\u0010\nh(u−1)\ntk\n; x(u−1)\ntk\n, θ(u−1)\np\n\u0011\n.\nNotice that, thanks to the Policy Gradient Theorem, the gradient of the surrogate performance measure does not\ndepend on the gradient of the reward function, and hence the reward function could be discrete or non-diﬀerentiable\nwhile the estimated gradient of the surrogate performance measure only needs the numerical reward values. However,\nin the DH approach of B¨uhler et al. (2019), the gradient of the surrogate performance measure therein does depend\non the gradient of the terminal loss function, and thus that approach implicitly requires the diﬀerentiability of the\nhedging portfolio value while the estimated gradient of the surrogate performance requires its numerical gradient\nvalues. See Appendix A for more details.\nTo reduce the variance of estimated gradient above, Williams (1992) suggested to introduce an unbiased baseline\nin this gradient, where a natural choice is the value function:\n∇θpV (u−1) (0, x; θp) = E\n\"˜n−1\nX\nk=0\n ˜n−1\nX\nl=k\nR(u−1)\ntl+1\n−V\n\u0010\ntk, X(u−1)\ntk\n; θp\n\u0011!\n∇θp ln φ\n\u0010\nH(u−1)\ntk\n; X(u−1)\ntk\n, θp\n\u0011 \f\f\fX(u−1)\n0\n= x\n#\n;\nsee also Weaver and Tao (2001).\nHerein, at any time tk, for k = 0, 1, . . . , ˜n −1, A(u−1)\ntk\n= P˜n−1\nl=k R(u−1)\ntl+1\n−\nV\n\u0010\ntk, X(u−1)\ntk\n; θp\n\u0011\nis called an advantage. Since the true value function is unknown to the RL agent, it is ap-\nproximated by ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n= Nv\n\u0010\nX(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n, deﬁned in (12), and in which the ANN weights are\nevaluated at θv = θ(u−1)\nv\nas the gradient of the time-0 value function is independent of the ANN weights θv; hence,\nthe estimated advantage is given by ˆA(u−1)\ntk\n= P˜n−1\nl=k R(u−1)\ntl+1\n−ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n.\nDue to the value function approximation in the baseline, REINFORCE includes a second component in the\nsurrogate performance measure, which aims to minimize the loss between the sum of reward signals and the ap-\n37\nproximated value function by the ANN. Therefore, the surrogate performance measure is given by:\nJ (u−1) (θ) = V (u−1) (0, x; θp) −E\n\"˜n−1\nX\nk=0\n\u0012\nˆA(u−1)\nθ(u−1)\np\n,tk + ˆV\n\u0010\ntk, X(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n−ˆV\n\u0010\ntk, X(u−1)\ntk\n; θv\n\u0011\u00132 \f\f\fX(u−1)\n0\n= x\n#\n,\nwhere the estimated advantaged ˆA(u−1)\nθ(u−1)\np\n,tk is evaluated at θp = θ(u−1)\np\n.\nHence, at each update step u = 1, 2, . . . , based on the ANN weights θ(u−1), and thus the policy π\n\u0010\n·; θ(u−1)\np\n\u0011\n,\nthe estimated gradient of the surrogate performance measure is given by\n\\\n∇θJ (u−1) \u0000θ(u−1)\u0001\n=\n˜n−1\nX\nk=0\n ˜n−1\nX\nl=k\nr(u−1)\ntl+1\n−ˆV\n\u0010\ntk, x(u−1)\ntk\n; θ(u−1)\nv\n\u0011!\n∇θp ln φ\n\u0010\nh(u−1)\ntk\n; x(u−1)\ntk\n, θ(u−1)\np\n\u0011\n+\n˜n−1\nX\nk=0\n ˜n−1\nX\nl=k\nr(u−1)\ntl+1\n−ˆV\n\u0010\ntk, x(u−1)\ntk\n; θ(u−1)\nv\n\u0011!\n∇θv ˆV\n\u0010\ntk, x(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n=\n˜n−1\nX\nk=0\nˆa(u−1)\ntk\n\u0010\n∇θp ln φ\n\u0010\nh(u−1)\ntk\n; x(u−1)\ntk\n, θ(u−1)\np\n\u0011\n+ ∇θv ˆV\n\u0010\ntk, x(u−1)\ntk\n; θ(u−1)\nv\n\u0011\u0011\n,\nwhere ˆa(u−1)\ntk\n= P˜n−1\nl=k r(u−1)\ntl+1\n−ˆV\n\u0010\ntk, x(u−1)\ntk\n; θ(u−1)\nv\n\u0011\n, for k = 0, 1, . . . , ˜n −1, is the realized estimated advantage.\nAppendix C\nDeep Hedging Training\nThe state vector observed by the DH agent is the same as that by the RL agent in Equation (18). Table 16a\nsummarizes the hyperparameters of DH agent training, while Table 16b outlines the hyperparameters of the ANN\narchitecture of DH agent; see Appendix A.\n(a) Hyperparameters of Deep Hedging Training\nParameter\nValue\nNumber of updates ˆ\nM\n108\nLearning rate ˆα\n0.0001\nOptimizer\nAdam\n(b) Hyperparameters for Neural Network\nParameter\nValue(s)\nNumber of layers\n6\nDimension of hidden layers\n[32, 64, 128, 64, 32]\nActivation function\nReLU\nTable 16: The hyperparameters of deep hedging training and the neural network\n38\n",
  "categories": [
    "q-fin.RM",
    "q-fin.PM"
  ],
  "published": "2021-07-07",
  "updated": "2022-10-01"
}