{
  "id": "http://arxiv.org/abs/2107.03380v3",
  "title": "RRL: Resnet as representation for Reinforcement Learning",
  "authors": [
    "Rutav Shah",
    "Vikash Kumar"
  ],
  "abstract": "The ability to autonomously learn behaviors via direct interactions in\nuninstrumented environments can lead to generalist robots capable of enhancing\nproductivity or providing care in unstructured settings like homes. Such\nuninstrumented settings warrant operations only using the robot's\nproprioceptive sensor such as onboard cameras, joint encoders, etc which can be\nchallenging for policy learning owing to the high dimensionality and partial\nobservability issues. We propose RRL: Resnet as representation for\nReinforcement Learning -- a straightforward yet effective approach that can\nlearn complex behaviors directly from proprioceptive inputs. RRL fuses features\nextracted from pre-trained Resnet into the standard reinforcement learning\npipeline and delivers results comparable to learning directly from the state.\nIn a simulated dexterous manipulation benchmark, where the state of the art\nmethods fail to make significant progress, RRL delivers contact rich behaviors.\nThe appeal of RRL lies in its simplicity in bringing together progress from the\nfields of Representation Learning, Imitation Learning, and Reinforcement\nLearning. Its effectiveness in learning behaviors directly from visual inputs\nwith performance and sample efficiency matching learning directly from the\nstate, even in complex high dimensional domains, is far from obvious.",
  "text": "RRL: Resnet as representation for Reinforcement Learning\nRutav Shah * 1 Vikash Kumar * 2 3\nAbstract\nThe ability to autonomously learn behaviors via\ndirect interactions in uninstrumented environ-\nments can lead to generalist robots capable of en-\nhancing productivity or providing care in unstruc-\ntured settings like homes. Such uninstrumented\nsettings warrant operations only using the robot’s\nproprioceptive sensor such as onboard cameras,\njoint encoders, etc which can be challenging for\npolicy learning owing to the high dimensionality\nand partial observability issues. We propose RRL:\nResnet as representation for Reinforcement Learn-\ning – a straightforward yet effective approach that\ncan learn complex behaviors directly from pro-\nprioceptive inputs. RRL fuses features extracted\nfrom pre-trained Resnet into the standard rein-\nforcement learning pipeline and delivers results\ncomparable to learning directly from the state. In\na simulated dexterous manipulation benchmark,\nwhere the state of the art methods fails to make\nsigniﬁcant progress, RRL delivers contact rich\nbehaviors. The appeal of RRL lies in its simplic-\nity in bringing together progress from the ﬁelds\nof Representation Learning, Imitation Learning,\nand Reinforcement Learning. Its effectiveness\nin learning behaviors directly from visual inputs\nwith performance and sample efﬁciency matching\nlearning directly from the state, even in complex\nhigh dimensional domains, is far from obvious.\n1. Introduction\nRecently, Reinforcement learning (RL) has seen tremen-\ndous momentum and progress (Mnih et al., 2015; Silver\net al., 2017; Jaderberg et al., 2019; Espeholt et al., 2018)\nin learning complex behaviors from states (Schulman et al.,\n*Equal contribution\n1Department of Computer Science and\nEngineering, Indian Institute of Technology, Kharagpur, In-\ndia 2Department of Computer Science, University of Washing-\nton, Seattle, USA 3Facebook AI Research, USA. Correspon-\ndence to: Rutav Shah <rutavms@gmail.com>, Vikash Kumar\n<vikash@cs.washington.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nSupervised  Learning\nReinforcement\nLearning\nFigure 1. RRL Resnet as representation for Reinforcement Learn-\ning takes a small step in bridging the gap between Representation\nlearning and Reinforcement learning. RRL pre-trains an encoder\non a wide variety of real world classes like ImageNet dataset using\na simple supervised classiﬁcation objective. Since the encoder is\nexposed to a much wider distribution of images while pretrain-\ning, it remains effective whatever distribution the policy might\ninduce during the training of the agent. This allows us to freeze\nthe encoder after pretraining without any additional efforts.\n2017; Haarnoja et al., 2018; Rajeswaran et al., 2017). Most\nsuccess stories, however, are limited to simulations or instru-\nmented laboratory conditions as real world doesn’t provide\ndirect access to its internal state. Not only learning with\nstate-space, but visual observation spaces have also found\nreasonable success (Kalashnikov et al., 2018; OpenAI et al.,\n2019). However the sample complexity for methods that can\nlearn directly with visual inputs is far too extreme for direct\nreal-world training (Duan et al., 2016; Kaiser et al., 2020).\nIn order to deliver the promise presented by data-driven\ntechniques, we need efﬁcient techniques that can learn unob-\ntrusively without the need of environment instrumentation.\nLearning without environment instrumentation, especially\nin unstructured settings like homes, can be quite challeng-\ning (Zhu et al., 2020; Dulac-Arnold et al., 2019; Ahn et al.,\n2020). Challenges include – (a) Decision making with in-\ncomplete information owing to partial observability as the\nagents must rely only on proprioceptive on-board sensors\n(vision, touch, joint position encoders, etc) to perceive and\nact. (b) The inﬂux of sensory information makes the input\nspace quite high dimensional. (c) Information contamina-\ntion due to sensory noise and task-irrelevant conditions like\nlightning, shadows, etc. (d) Most importantly, the scene\nbeing ﬂushed with information irrelevant to the task (back-\nground, clutter, etc). Agents learning under these constraints\nis forced to take a large number of samples simply to untan-\narXiv:2107.03380v3  [cs.RO]  11 Nov 2021\nRepresentations for Reinforcement Learning\ngle these task-irrelevant details before it makes any progress\non the true task objective. A common approach to han-\ndle these high dimensionality and multi-modality issues\nis to learn representations that distil information into low\ndimensional features and use them as inputs to the policy.\nWhile such ideas have found reasonable success (Qin et al.,\n2019; Manuelli et al., 2019a), designing such representa-\ntions in a supervised manner requires a deep understanding\nof the problem and domain expertise. An alternative ap-\nproach is to leverage unsupervised representation learning\nto autonomously acquire representations based on either\nreconstruction (Higgins et al., 2016; Zhu et al., 2020; Yarats\net al., 2020) or contrastive (Srinivas et al., 2020; Stooke\net al., 2020) objective. These methods are quite brittle as the\nrepresentations are acquired from narrow task-speciﬁc dis-\ntributions (Stone et al., 2021), and hence, do not generalize\nwell across different tasks Table 1. Additionally, they ac-\nquire task-speciﬁc representations, often needing additional\nsamples from the environment leading to poor sample efﬁ-\nciency or domains speciﬁc data-augmentations for training\nrepresentations.\nThe key idea behind our method stems from an intuitive ob-\nservation over the desiderata of a good representation i.e. (a)\nit should be low dimensional for a compact representation.\n(b) it should be able to capture silent features encapsulating\nthe diversity and the variability present in a real-world task\nfor better generalization performance. (c) it should be robust\nto irrelevant information like noise, lighting, viewpoints, etc\nso that it is resilient to the changes in surroundings. (d) it\nshould provide effective representation in the entire distri-\nbution that a policy can induce for effective learning. These\nrequirements are quite harsh needing extreme domain ex-\npertise to manually design and an abundance of samples to\nautomatically acquire. Can we acquire this representation\nwithout any additional effort? Our work takes a very small\nstep in this direction.\nThe key insight behind our method (Figure 1) is embarrass-\ningly simple – representations do not necessarily have to\nbe trained on the exact task distribution; a representation\ntrained on a sufﬁciently wide distribution of real-world sce-\nnarios, will remain effective on any distribution a policy\noptimizing a task in the real world might induce. While\ntraining over such wide distribution is demanding, this is\nprecisely what the success of large image classiﬁcation mod-\nels (He et al., 2015; Simonyan & Zisserman, 2015; Tan &\nLe, 2020; Szegedy et al., 2015) in Computer Vision delivers\n– representations learned over a large family of real-world\nscenarios.\nOur Contributions: We list the major contributions\n1. We present a surprisingly simple method (RRL) at\nthe intersection of representation learning, imitation\nlearning (IL) and reinforcement learning (RL) that uses\nFeatures for ImageNet categories\nFeatures for our tasks\nFigure 2. Visualization of Layer 4 of Resnet model of the top 1\nclass using Grad-CAM (Selvaraju et al., 2019)[Top] and Guided\nBackpropogation (Springenberg et al., 2015)[Bottom]. This indi-\ncates that Resnet is indeed looking for the right features in our task\nimages (right) in spite of such high distributional shift.\nfeatures from pre-trained image classiﬁcation models\n(Resnet34) as representations in standard RL pipeline.\nOur method is quite general and can be incorporated\nwith minimal changes to most state based RL/IL algo-\nrithms.\n2. Task-speciﬁc representations learned by supervised as\nwell as unsupervised methods are usually brittle and\nsuffer from distribution mismatch. We demonstrate\nthat features learned by image classiﬁcation models\nare general towards different task (Figure 2), robust to\nvisual distractors, and when used in conjunction with\nstandard IL and RL pipelines can efﬁciently acquire\npolicies directly from proprioceptive inputs.\n3. While competing methods have restricted results pri-\nmarily to planar tasks devoid of depth perspectives, on\na rich collection of simulated high dimensional dexter-\nous manipulation tasks, where state-of-the-art methods\nstruggle, we demonstrate that RRL can learn rich be-\nhaviors directly from visual inputs with performance\n& sample efﬁciency approaching state-based methods.\n4. Additionally, we underline the performance gap be-\ntween the SOTA approaches and RRL on simple low\ndimensional tasks as well as high dimensional more\nrealistic tasks. Furthermore, we experimentally estab-\nlish that the commonly used environments for studying\nimage based continuous control methods are not a true\nrepresentative of real-world scenario.\n2. Related Work\nRRL rests on recent developments from the ﬁelds of Repre-\nsentation Learning, Imitation Learning and Reinforcement\nLearning. In this section, we outline related works lever-\naging representation learning for visual reinforcement and\nimitation learning.\nRepresentations for Reinforcement Learning\n2.1. Learning without explicit representation\nA common approach is to learn behaviors in an end to end\nfashion – from pixels to actions – without explicit distinction\nbetween feature representation and policy representations.\nSuccess stories in this categories range from seminal work\n(Mnih et al., 2013) mastering Atari 2600 computer games\nusing only raw pixels as input, to (Levine et al., 2016) which\nlearns trajectory-centric local policies using Guided Policy\nSearch (Levine & Koltun, 2013) for diverse continuous con-\ntrol manipulation tasks in the real world learned directly\nfrom camera inputs. More recently, Haarnoja et al. has\ndemonstrated success in acquiring multi-ﬁnger dexterous\nmanipulation (Zhu et al., 2018) and agile locomotion behav-\niors using off-policy action critic methods (Haarnoja et al.,\n2018). While learning directly from pixels has found reason-\nable success, it requires training large networks with high\ninput dimensionality. Agents require a prohibitively large\nnumber of samples to untangle task-relevant information\nin order to acquire behaviors, limiting their application to\nsimulations or constrained lab settings. RRL maintains an\nexplicit representation network to extract low dimensional\nfeatures. Decoupling representation learning from policy\nlearning delivers results with large gains in efﬁciency. Next,\nwe outline related works that use explicit representations.\n2.2. Learning with supervised representations\nAnother approach is to ﬁrst acquire representations using\nexpert supervision, and use features extracted from repre-\nsentation as inputs in standard policy learning pipelines. A\npredominant idea is to learn representative keypoints encap-\nsulating task details from the input images and using the\nextracted keypoints as a replacement of the state information\n(Kulkarni et al., 2019). Using these techniques, (Qin et al.,\n2019; Manuelli et al., 2019b) demonstrated tool manipu-\nlation behaviors in rich scenes ﬂushed with task-irrelevant\ndetails. (Nagabandi et al., 2019) demonstrated simultaneous\nmanipulation of multiple objects in the task of Baoding ball\ntasks on a high dimensional dexterous manipulation hand.\nAlong with the inbuilt proprioceptive sensing at each joint,\nthey use an RGB stereo image pair that is fed into a separate\npre-trained tracker to produce 3D position estimates (You\net al., 2020) for the two Baoding balls. These methods,\nwhile powerful, learn task-speciﬁc features and requires ex-\npert supervision, making it harder to (a) translate to variation\nin tasks/environments, and (b) scale with increasing task\ndiversity. RRL, on the other hand, uses single task-agnostic\nrepresentations with better generalization capability making\nit easy to scale.\n2.3. Learning with unsupervised representations\nWith the ambition of being scalable, this group of meth-\nods intends to acquire representation via unsupervised tech-\nniques. (Sermanet et al., 2018) uses contrastive learning to\ntime-align visual features across different embodiment to\ndemonstrate behavior transfer from human to a Fetch robot.\n(Burgess et al., 2018),\n(Finn et al.; Zhu et al., 2020) use\nvariational inference (Kingma & Welling, 2014; Burgess\net al., 2018) to learn compressed latent representations and\nuse it as input to standard RL pipeline to demonstrate rich\nmanipulation behaviors. (Hafner et al., 2020) additionally\nlearns dynamics models directly in the latent space and use\nmodel-based RL to acquire behaviors on simulated tasks.\nOn similar tasks, (Hafner et al., 2019) uses multi-step varia-\ntional inference to learn world dynamic as well as rewards\nmodels for off-policy RL. (Srinivas et al., 2020) use image\naugmentation with variational inference to construct features\nto be used in standard RL pipeline and demonstrate perfor-\nmance at par with learning directly from the state. (Laskin\net al., 2020; Kostrikov et al., 2020) demonstrate compara-\nble results by assimilating updates over features acquired\nonly via image augmentation. Similar to supervised meth-\nods, unsupervised methods often learns task-speciﬁc brittle\nrepresentations as they break when subjected to small varia-\ntions in the surroundings and often suffers challenges from\nnon-stationarity arising from the mismatch between the dis-\ntribution representations are learned on and the distribution\npolicy induces. To induce stability, RRL uses pre-trained\nstationary representations trained on distribution with wider\nsupport than what policy can induce. Additionally, repre-\nsentations learned over a wide distribution of real-world\nsamples are robust to noise and irrelevant information like\nlighting, illumination, etc.\n2.4. Learning with representations and demonstrations\nLearning from demonstrations has a rich history. We fo-\ncus our discussion on DAPG (Rajeswaran et al., 2017), a\nstate-based method which optimizes for the natural gradient\n(Kakade, 2001a) of a joint loss with imitation as well as\nreinforcement objective. DAPG has been demonstrated to\noutperform competing methods (Gupta et al., 2017; Hester\net al., 2017) on the high dimensional ADROIT dexterous ma-\nnipulation task suite we test on. RRL extends DAPG to solve\nthe task suite directly from proprioceptive signals with per-\nformance and sample efﬁciency comparable to state-DAPG.\nUnlike DAPG which is on-policy, FERM (Zhan et al., 2020)\nis a closely related off-policy actor-critic methods combin-\ning learning from demonstrations with RL. FERM builds\non RAD (Laskin et al., 2020) and inherits its challenges\nlike learning task-speciﬁc representations. We demonstrate\nvia experiments that RRL is more stable, more robust to\nvarious distractors, and convincingly outperforms FERM\nsince RRL uses a ﬁxed feature extractor pre-trained over\nwide variety of real world images and avoids learning task\nspeciﬁc representations.\nRepresentations for Reinforcement Learning\n3. Background\nRRL solves a standard Markov decision process (Section\n3.1) by combining three fundamental building blocks - (a)\nPolicy gradient algorithm (Section 3.2), (b) Demonstration\nbootstrapping (Section 3.3), and (c) Representation learning\n(Section 3.4). We brieﬂy outline these fundamentals before\ndetailing our method in Section 4.\n3.1. Preliminaries: MDP\nWe model the control problem as a Markov decision pro-\ncess (MDP), which is deﬁned using the tuple: M =\n(S, A, R, T , ρ0, γ). S ∈Rn and A ∈Rm represent the\nstate and actions. R : S × A →R is the reward function.\nIn the ideal case, this function is simply an indicator for\ntask completion (sparse reward setting). T : S × A →S\nis the transition dynamics, which can be stochastic. In\nmodel-free RL, we do not assume any knowledge about\nthe transition function and require only sampling access to\nthis function. ρ0 is the probability distribution over initial\nstates and γ ∈[0, 1) is the discount factor. We wish to solve\nfor a stochastic policy of the form π : S × A →R which\noptimizes the expected sum of rewards:\nη(π) = Eπ,M\nh ∞\nX\nt=0\nγtrt\ni\n(1)\n3.2. Policy Gradient\nThe goal of the RL agent is to maximise the expected\ndiscounted return η(π) (Equation 1) under the distribution\ninduced by the current policy π. Policy Gradient algorithms\noptimize the policy πθ(a | s) directly, where θ is the function\nparameter by estimating ∇η(π). First we introduce a few\nstandard notations, Value function : V π(s), Q function\n: Qπ(s, a) and the advantage function : Aπ(s, a). The\nadvantage function can be considered as another version of\nQ-value with lower variance by taking the state-value off as\nthe baseline.\nV π(s) = EπM\nh ∞\nX\nt=0\nγtrt | s0 = s\ni\nQπ(s, a) = EM\nh\nR(s, a)\ni\n+ Es′∼T (∫,⊣)\nh\nV π(s′)\ni\nAπ(s, a) = Qπ(s, a) −V π(s)\n(2)\nThe gradient can be estimated using the Likelihood ratio\napproach and Markov property of the problem (Williams,\n1992) and using a sampling based strategy,\n∇η(π) = g =\n1\nNT\nN\nX\ni=0\nT\nX\nt=0\n∇θ log πθ(ai\nt|si\nt) ˆAπ(si\nt, ai\nt, t)\n(3)\nAmongst the wide collection of policy gradient algorithms,\nwe build upon Natural Policy Gradient (NPG) (Kakade,\n2001a) to solve our MDP formulation owing to its stability\nand effectiveness in solving complex problems. We refer to\n(Weng, 2018) for a detailed background on different policy\ngradient approaches. In the next section, we describe how\nhuman demonstrations can be effectively used along with\nNPG to aid policy optimization.\n3.3. Demo Augmented Policy Gradient\nPolicy Gradients with appropriately shaped rewards can\nsolve arbitrarily complex tasks. However, real-world en-\nvironments seldom provide shaped rewards, and it must\nbe manually speciﬁed by domain experts. Learning with\nsparse signals, such as task completion indicator func-\ntions, can relax domain expertise in reward shaping but\nit results in extremely high sample complexity due to ex-\nploration challenges.\nDAPG (Rajeswaran et al.)\ncom-\nbines policy gradients with few demonstrations in two\nways to mitigate this issue and effectively learn from\nthem. We represent the demonstration dataset using ρD =\nn\u0010\ns(i)\nt , a(i)\nt , s(i)\nt+1, r(i)\nt\n\u0011o\nwhere t indexes time and i in-\ndexes different trajectories.\n(1) Warm up the policy using few demonstrations (25 in\nour setting) using a simple Mean Squared Error(MSE) loss,\ni.e, initialize the policy using behavior cloning [Eq 4]. This\nprovides an informed policy initialization that helps in re-\nsolving the early exploration issue as it now pays attention\nto task relevant state-action pairs and thereby, reduces the\nsample complexity.\nLBC(θ) = 1\n2\nX\ni,t∈minibatch\n\u0010\nπθ(s(i)\nt ) −a(i)H\nt\n\u00112\n(4)\nwhere, θ are the agent parameters and a(i)H\nt\nrepresents the\naction taken by the human/expert.\n(2) DAPG builds upon on-policy NPG algorithm (Kakade,\n2001a) which uses a normalized gradient ascent procedure\nwhere the normalization is under the Fischer metric.\nθk+1 = θk +\ns\nδ\ngT ˆF −1\nθk g\nˆF −1\nθk g\n(5)\nwhere ˆFθk is the Fischer Information Metric at the current\niterate θk,\nˆFθk = 1\nT\nT\nX\nt=0\n∇θlog πθ(at|st)∇θlog πθ(at|st)T\n(6)\nand g is the sample based estimate of the policy gradient\n[Eq 3]. To make the best use of available demonstrations,\nRepresentations for Reinforcement Learning\nDAPG proposes a joint loss gaug combining task as well\nas imitation objective. The imitation objective asymptoti-\ncally decays over time allowing the agent to learn behaviors\nsurpassing the expert.\ngaug =\nX\n(s,a)∈ρπ\n∇θ ln πθ(a|s)Aπ(s, a)\n+\nX\n(s,a)∈ρD\n∇θ ln πθ(a|s)w(s, a)\n(7)\nwhere, ρπ is the dataset obtained by executing the current\npolicy, ρD is the demonstration data and w(s, a) is the\nheuristic weighting function deﬁned as :\nw(s, a) = λ0λk\n1\nmax\n(s′,a′)∈ρπ Aπ(s′, a′)\n∀(s, a) ∈ρD (8)\nDAPG has proven to be successful in learning policy for\nthe dexterous manipulation tasks with reasonable sample\ncomplexity.\n3.4. Representation Learning\nDAPG has thus far only been demonstrated to be effec-\ntive with access to low-level state information which is not\nreadily available in real-world. DAPG is based on NPG\nwhich works well but faces issues with input dimensionality\nand hence, cannot be directly used with the input images\nacquired from onboard cameras. Representation learning\n(Bengio et al., 2014) is learning representations of input\ndata typically by transforming it or extracting features from\nit, which makes it easier to perform the task (in our case it\ncan be used in place of the exact state of the environment).\nLet I ∈Rn represents the high dimensional input image,\nthen\nh = fρ(I)\n(9)\nwhere f represents the feature extractor, ρ is the distribution\nover which f is valid and h ∈Rd with d << n is the\ncompact, low dimensional representation of I. In the next\nsection, we outline our method that scales DAPG to solve\ndirectly from visual information.\n4. RRL: Resnet as Representation for RL\nIn an ideal RL setting, the agent interacts with the envi-\nronment based on the current state, and in return, the en-\nvironment outputs the next state and the reward obtained.\nThis works well in a simulated environment but in a real-\nworld scenario, we do not have access to this low-level\nstate information. Instead we get the information from cam-\neras (It) and other onboard sensors like joint encoders (δt).\nTo overcome the challenges associated with learning from\nhigh dimensional inputs, we use representations that project\ninformation into a lower-dimensional manifolds. These\nAlgorithm 1 RRL\n1: Input: 25 Human Demonstrations ρD\n2: Initialize using Behavior Cloning [Eq.4].\n3: repeat\n4:\nfor i = 1 to n do\n5:\nfor t = 1 to horizon do\n6:\nTake action\nat = πθ([Encoder(It), δ t])\nand receive It+1, δ t+1, rt+1\nfrom the environment.\n7:\nend for\n8:\nend for\n9:\nCompute ∇θ log πθ(at|st) for each (s, a) ∈ρπ, ρD\n10:\nCompute Aπ(s, a) for each (s, a) ∈ρπ and w(s, a)\nfor each (s, a) ∈ρD according to Equations 2, 8\n11:\nCalculate policy gradient according to 7\n12:\nCompute Fisher matrix 6\n13:\nTake the gradient ascent step according to 5.\n14:\nUpdate the parameters of the value function in order\nto approximate(2) : V π\nk (s(n)\nt\n) ≈PT\nt′=t γt′−tr(n)\nt\n15: until Satisfactory performance\nrepresentations can be (a) learned in tandem with the RL ob-\njective. However, this leads to non-stationarity issue where\nthe distribution induced by the current policy πi may lie\noutside the expressive power of f, πi ̸⊂ρi at any step i\nduring training. (b) decoupled from RL by pre-training\nf. For this to work effectively, the feature extractor must\nbe trained on a sufﬁciently wide distribution such that it\ncovers any distribution that the policy might induce during\ntraining, πi ⊂ρ ∀i. Getting hold of such task speciﬁc\ntraining data beforehand becomes increasingly difﬁcult as\nthe complexity and diversity of the task increases. To this\nend, we propose to use a ﬁxed feature extractor (Section 5.2)\nthat is pretrained on a wide variety of real world scenarios\nlike ImageNet dataset [Highlighted in purple in Figure 1].\nWe experimentally demonstrate that the diversity (Section\n5.3) of the such feature extractor allows us to use it across\nall tasks we considered. The use of pre-trained representa-\ntions induces stability to RRL as our representations are\nfrozen and do-not face the non-stationarity issues encoun-\ntered while learning policy and representation in tandem.\nThe features (ht) obtained from the above feature extractor\nare appended with the information obtained from the inter-\nnal joint encoders of the Adroit Hand (δ t). As a substitute\nof the exact state (st), we empirically show that [ht, δ t]\ncan be used as an input to the policy. In principle any RL\nalgorithm can be deployed to learn the policy, in RRL we\nbuild upon Natural Policy Gradients (Kakade, 2001b) owing\nto effectiveness in solving complex high dimensional tasks\n(Rajeswaran et al., 2017). We present our full algorithm in\nAlgorithm-1.\nRepresentations for Reinforcement Learning\n5. Experimental Evaluations\nOur experimental evaluations aims to address the following\nquestions: (1) Does pre-tained representations acquired via\nlarge real world image dataset allow RRL to learn complex\ntasks directly from proprioceptive signals (camera inputs\nand joint encoders)? (2) How does RRL’s performance and\nefﬁciency compare against other state-of-the-art methods?\n(3) How various representational choices inﬂuence the gen-\nerality and versatility of the resulting behaviors? (5) What\nare the effects of various design decisions on RRL? (6)\nAre commonly used benchmarks for studying image based\ncontinuous control methods effective?\n5.1. Tasks\nApplicability of prior proprioception based RL methods\n(Laskin et al., 2020; Kostrikov et al., 2020; Hafner et al.,\n2020) have been limited to simple low dimensional tasks\nlike Cartpole, Cheetah, Reacher, Finger spin, Walker, Ball\nin cup, etc. Moving beyond these simple domains, we in-\nvestigate RRL on Adroit manipulation suite (Rajeswaran\net al., 2017) which consists of contact-rich high-dimensional\ndexterous manipulation tasks (Figure 3) that have found to\nbe challenging ever for state (st) based methods. Further-\nmore, unlike prior task sets, which are fundamentally planar\nand devoid of depth perspective, the Adroit manipulation\nsuite consists of visually-rich physically-realistic tasks that\ndemand representations untangling complex depth informa-\ntion.\n5.2. Implementation Details\nWe use standard Resnet-34 model as RRL’s feature extrac-\ntor. The model is pre-trained on the ImageNet dataset which\nconsists of 1000 classes. It is trained on 1.28 million images\non the classiﬁcation task of ImageNet. The last layer of\nthe model is removed to recover a 512 dimensional feature\nspace and all the parameters are frozen throughout the train-\ning of the RL agent. During inference, the observations\nobtained from the environment are of size 256 × 256, a\ncenter crop of size 224 × 224 is fed into the model. We also\nevaluate our model using different Resnet sizes (Figure 7).\nAll the hyperparameters used for training are summarized\nin Appendix(Table 2). We report an average performance\nover three random seeds for all the experiments.\n5.3. Results\nIn Figure 4, we contrast the performance of RRL against\nthe state of the art baselines. We begin by observing that\nNPG (Kakade, 2001b) struggles to solve the suite even with\nfull state information, which establishes the difﬁculty of\nour task suite. DAPG(State) (Rajeswaran et al., 2017) uses\nprivileged state information and a few demonstrations from\nFigure 3. ADROIT manipulation suite consisting of complex dex-\nterous manipulation tasks involving object relocation, in hand\nmanipulation (pen repositioning), tool use (hammering a nail), and\ninteracting with human centric environments (opening a door).\nthe environment to solve the tasks and pose as the best case\noracle. RRL demonstrates good performance on all the\ntasks, relocate being the hardest, and often approaches per-\nformance comparable to our strongest oracle-DAPG(State).\nA competing baseline FERM 1 (Zhan et al., 2020) is quite\nunstable in these tasks. It starts strong for hammer and door\ntasks but saturates in performance. It makes slow progress\nin pen, and completely fails for relocate. In Figure 5 [Left]\nwe compare the computational footprint of FERM (along\nwith other methods, discussed in later sections) with RRL.\nWe note that our method not only outperforms FERM but\nalso is approximately ﬁve times more compute-efﬁcient.\n5.4. Effects of Visual Distractors\nIn Figure 5 [Center, Right] we probe the robustness of the ﬁ-\nnal policies by injecting visual distractors in the environment\nduring inference. We note that the resilience of the resnet\nfeatures induces robustness to RRL’s policies. On the other\nhand, task-speciﬁc features learned by FERM are brittle\nleading to larger degradation in performance. In addition\nto improved sample and time complexity resulting from the\nuse of pre-trained features, the resilience, robustness, and\nversatility of Resnet features lead to policies that are also\nrobust to visual distractors, clutter in the scene. More details\nabout the experiment setting is provided in Section 7.8 in\nAppendix.\n1Reporting best performance amongst over 30 conﬁgurations\nper task we tried in consultation with the FERM authors.\nRepresentations for Reinforcement Learning\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\nDoor Opening\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\n0\n20\n40\n60\n80\n100\nSuccess Rate\nTool Use (Hammer)\n0\n10\n20\n30\nRobot Hours\n0\n10\n20\n30\nRobot Hours\n0\n10\n20\n30\nRobot Hours\n0\n10\n20\n30\nRobot Hours\n0\n2\n4\n6\n8\n10\n12\nsamples(M)\nIn-hand Manipulation (Pen)\n0\n5\n10\n15\n20\nRobot Hours\n0\n5\n10\n15\n20\nRobot Hours\n0\n5\n10\n15\n20\nRobot Hours\n0\n5\n10\n15\n20\nRobot Hours\n0\n2\n4\n6\n8\nsamples(M)\nObject Relocation\nRRL(Ours)\nFERM\nDAPG(State)\nNPG(State)\nFigure 4. Performance on ADROIT dexterous manipulation suite (Rajeswaran et al., 2017): State of the art policy gradient method\nNPG(State) (Rajeswaran et al., 2018) struggles to solve the suite even with privileged low level state information, establishing the difﬁculty\nof the suite. Amongst demonstration accelerated methods, RRL(Ours) demonstrates stable performance and approaches performance\nof DAPG(State) (Rajeswaran et al., 2017) (upper bound), a demonstration accelerated method using privileged state information. A\ncompeting baseline FERM (Zhan et al., 2020) makes good initial, but unstable, progress in a few tasks and often saturates in performance\nbefore exhausting our computational budget (40 hours/ task/ seed).\nTime(Hours)\n0.0\n10.0\n20.0\n30.0\n40.0\nRRL\n(Ours)\nFERM\nRRL\n(Resnet 18)\nRRL\n(Resnet 50)\nRRL\n(VAE)\nRRL\n(ShuffleNet)\nRRL\n(MobileNet)\nRRL\n(vdvae)\nCompute Cost\nSuccess Rate\n0\n25\n50\n75\n100\nDefault\nLight Position Light Direction Object Color Random Object\nRRL(Ours)\nFERM\nHammer-v0\nSuccess Rate\n0\n25\n50\n75\n100\nDefault\nLight Position Light Direction Object Color Random Object\nRRL(Ours)\nFERM\nDoor-v0\nFigure 5. LEFT: Comparison of the computational cost of RRL with Resnet34 i.e RRL(Ours), FERM - Strongest baseline, RRL with\nResnet 18, RRL with Resnet 50, RRL(VAE), RRL with ShufﬂeNet, RRL with MobileNet and RRL with Very Deep VAE baseline.\nCENTER,RIGHT: Inﬂuence of various environment distractions (lightning condition, object color) on RRL(Ours), and FERM. RRL(Ours)\nconsistently performs better than FERM in all the variations we considered.\n5.5. Effect of Representation\nIs Resnet lucky? To investigate if architectural choice of\nResnet is lucky, in Figure 6 we test different models pre-\ntrained on ImageNet dataset as RRL’s feature extractors –\nMobileNetV2 (Sandler et al., 2019), ShufﬂeNet (Ma et al.,\n2018) and state of the art hierarchical VAE (Child, 2021)\n[Refer Section 7.5 in Appendix for more details]. Not much\ndegradation in performance is observed with respect to the\nResnet model. This highlights that it is not the architecture\nchoices in particular, rather the dataset on which models are\nbeing pre-trained, that delivers generic features effective for\nthe RL agents.\nTask-speciﬁc vs Task-agnostic representation:\nIn Fig-\nure 7, we compare the performance between (a) learning\ntask speciﬁc representations (VAE) (b) generic representa-\ntion trained on a very wide distribution (Resnet). We note\nthat RRL using Resnet34 signiﬁcantly outperforms a vari-\nant RRL(VAE) (see appendix for details Section 7.7) that\nlearns features via commonly used variational inference\ntechniques on a task speciﬁc dataset (Ha & Schmidhuber,\n2018a;b; Higgins et al., 2018; Nair et al., 2018). This in-\ndicates that pre-trained Resnet provides task agnostic and\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\nDoor Opening\nRRL(Ours)\nRRL(ShuffleNet)\nRRL(vdvae)\nRRL(MobileNet)\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\n0\n20\n40\n60\n80\n100\nSuccess Rate\nTool Use (Hammer)\nFigure 6. Effect of different types of Feature extractor pretrained\non ImageNet dataset, highlighting that not just Resnet but any\nfeature extractor pretrained on a sufﬁciently wide distribution of\ndata remains effective.\nsuperior features compared to methods that explicitly learn\nbrittle (Section-5.8) and task-speciﬁc features using addi-\ntional samples from the environment. It is important to\nnote that the latent dimension of the Resnet34 and VAE\nare kept same (512) for a fair comparison, however, the\nmodel sizes are different as one operates on a very wide\nRepresentations for Reinforcement Learning\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\nDoor Opening\nRRL(Ours)\nRRL(Resnet18)\nRRL(Resnet50)\nRRL(VAE)\nNPG(Resnet34)\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\n0\n20\n40\n60\n80\n100\nSuccess Rate\nTool Use (Hammer)\nFigure 7. Inﬂuence of representation: RRL(Ours), using resnet34\nfeatures, outperforms commonly used representation\n\u0000RRL(VAE)\n\u0001\nlearning method VAE. Amongst different Resnet variations,\nResnet34 strikes the balance between representation capacity and\ncomputational overhead. NPG(Resnet34) showcases the perfor-\nmance with Resnet34 features but without demonstration boot-\nstrapping, indicating that only representational choices are not\nenough to solve the task suite.\ndistribution while the other on a much narrower task speciﬁc\ndataset. Additionally, we summarize the compute cost of\nboth the methods RRL(Ours) and RRL(VAE) in Figrue 5\n[Left]. We notice that even though RRL(VAE) is the cheap-\nest, its performance is quite low (Figure 7). RRL(Ours)\nstrikes a balance between compute and efﬁciency.\n5.6. Effects of proprioception choices and sensor noise\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\nDoor Opening\nRRL(Vision+Sensors)\nRRL(Noise)\nRRL(Vision)\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\n0\n20\n40\n60\n80\n100\nSuccess Rate\nTool Use (Hammer)\nFigure 8. Inﬂuence\nof\nproprioceptive\nsignals\non\nRRL(Vision+sensors-Ours):\nRRL(Noise)\ndemonstrates\nthat RRL remains effectiveness in presence of noisy (2%)\nproprioception. RRL(Vision) demonstrates that RRL remains\nperformant with (only) visual inputs as well.\nWhile it’s hard to envision a robot without proprioceptive\njoint sensing, harsh conditions of the real-world can lead\nto noisy sensing, even sensor failures. In Figure 8, we sub-\njected RRL to (a) signals with 2% noise in the information\nreceived from the joint encoders RRL(Noise), and (b) only\nvisual inputs are used as proprioceptive signals RRL(Vision).\nIn both these cases, our methods remained performant with\nslight to no degradation in performance.\n5.7. Ablations and Analysis of Design Decisions\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\n0\n20\n40\n60\n80\n100\nSuccess Rate\nTool Use (Hammer)\nRRL(Sparse)\nRRL(Dense)\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n2\n4\n6\n8\n10\nRobot Hours\n0\n1\n2\n3\n4\nsamples(M)\n0\n20\n40\n60\n80\n100\nTool Use (Hammer)\nRRL(64,64)\nRRL(128,128)\nRRL(256,256)\nRRL(512,512)\nFigure 9. LEFT: Inﬂuence of rewards signals: RRL(Ours), using\nsparse rewards, remains performant with a variation RRLdense\nusing well-shaped dense rewards. RIGHT: Effect of policy size on\nthe performance of RRL . We observe that it is quite stable with\nrespect to a wide range of policy sizes.\nIn our next set of experiments, we evaluate the effect of\nvarious design decisions on our method. In Figure 7, we\nstudy the effect of different Resnet features as our represen-\ntation. Resnet34, though computationally more demanding\n(Figure 5) than Resnet18, delivers better performance owing\nto its improved representational capacity and feature ex-\npressivity. A further boost in capacity (Resnet50) degrades\nperformance, likely due to the incorporation of less useful\nfeatures and an increase in samples required to train the\nresulting larger policy network.\nReward design, especially for complex high dimensional\ntasks, requires domain expertise. RRL replaces the needs of\nwell-shaped rewards by using a few demonstrations (to curb\nthe exploration challenges in high dimensional space) and\nsparse rewards (indicating task completion). This signiﬁ-\ncantly lowers the domain expertise required for our methods.\nIn Figure 9-LEFT, we observe that RRL (using sparse re-\nwards) delivers competitive performance to a variant of our\nmethods that uses well-shaped dense rewards while being\nresilient to variation in policy network capacity (Figure 9-\nRIGHT).\n5.8. Rethinking benchmarking for visual RL\nDMControl (Tassa et al., 2018) is a widely used benchmark\nfor proprioception based RL methods – RAD (Laskin et al.,\n2020), SAC+AE (Yarats et al., 2020), CURL (Srinivas et al.,\n2020), DrQ (Kostrikov et al., 2020). While these methods\nperform well (Table 1) on such simple DMControl tasks,\ntheir progress struggles to scale when met with task repre-\nsentative of real world complexities such as realistic Adroit\nManipulation benchmarks (Figure 4).\nFor example we demonstrate in Figure 4 that a representa-\ntive SOTA methods FERM (uses expert demos along with\nRepresentations for Reinforcement Learning\n500K Step Scores\nRRL+SAC\nRAD\nFixed RAD Encoder\nCURL\nSAC+AE\nState SAC\nFinger, Spin\n422 ± 102\n947 ± 101\n789 ± 190\n926 ± 45\n884 ± 128\n923 ± 211\nCartpole, Swing\n357 ± 85\n863 ± 9\n875 ± 01\n845 ± 45\n735 ± 63\n848 ± 15\nReacher, Easy\n382 ± 299\n955 ± 71\n53 ± 44\n929 ± 44\n627 ± 58\n923 ± 24\nCheetah, Run\n154 ± 23\n728 ± 71\n203 ± 31\n518 ± 28\n550 ± 34\n795 ± 30\nWalker, Walk\n148 ± 12\n918 ± 16\n182 ± 40\n902 ± 43\n847 ± 48\n948 ± 54\nCup, Catch\n447 ± 132\n974 ± 12\n719 ± 70\n959 ± 27\n794 ± 58\n974 ± 33\n100K Step Scores\nFinger, Spin\n135 ± 67\n856 ± 73\n655 ± 104\n767 ± 56\n740 ± 64\n811 ± 46\nCartpole, Swing\n192 ± 19\n828 ± 27\n840 ± 34\n582 ± 146\n311 ± 11\n835 ± 22\nReacher, Easy\n322 ± 285\n826 ± 219\n162 ± 40\n538 ± 233\n274 ± 14\n746 ± 25\nCheetah, Run\n72 ± 63\n447 ± 88\n188 ± 20\n299 ± 48\n267 ± 24\n616 ± 18\nWalker, Walk\n63 ± 07\n504 ± 191\n106 ± 11\n403 ± 24\n394 ± 22\n891 ± 82\nCup, Catch\n261 ± 57\n840 ± 179\n533 ± 148\n769 ± 43\n391 ± 82\n746 ± 91\nTable 1. Results on DMControl Benchmark. RAD outperforms all the baselines whereas RRL performs worse in the 100K and 500k\nEnvironmental step benchmark suggesting that it is quicker to learn task speciﬁc representation in simple tasks whereas Fixed Rad\nEncoder highlights that the representations learned by RAD are narrow and task speciﬁc.\nRAD) struggles to perform well on Adroit Manipulation\nbenchmark. On the contrary, RRL using Resnet features\npretrained on real world image dataset, delivers state com-\nparable results on Adroit Manipulation benchmark while\nstruggles on the DMControl (RRL+SAC: RRL using SAC\nand Resnet34 features 1). This highlights large domain gap\nbetween the DMControl suite and the real-world.\nWe further note that the pretrained features learned by SOTA\nmethods aren’t as widely applicable. We use a pre-trained\nRAD encoder (pretrained on Cartpole) as ﬁxed feature ex-\ntractor (Fixed RAD encoder in Table 1) and retrain the\npolicy using these features for all environments. The per-\nformance degrades for all the tasks except Cartpole. This\nhighlights that the representation learned by RAD (even\nwith various image augmentations) are task speciﬁc and fail\nto generalize to other tasks set with similar visuals. Further-\nmore, learning such task speciﬁc representations are easier\non simpler scenes but their complexity grows drastically\nas the complexity of tasks and scenes increases. To ensure\nthat important problems aren’t overlooked, we emphasise\nthe need for the community to move towards benchmarks\nrepresentative of realistic real world tasks.\n6. Strengths, Limitations & Opportunities\nThis paper presents an intuitive idea bringing together ad-\nvancements from the ﬁelds of representation learning, imita-\ntion learning, and reinforcement learning. We present a very\nsimple method named RRL that leverages Resnet features as\nrepresentation to learn complex behaviors directly from pro-\nprioceptive signals. The resulting algorithm approaches the\nperformance of state-based methods in complex ADROIT\ndexterous manipulation suite.\nStrengths: The strength of our insight lies in its simplicity,\nand applicability to almost any reinforcement or imitation\nlearning algorithm that intends to learn directly from high\ndimensional proprioceptive signals. We present RRL , an\ninstantiation of this insight on top of imitation + (on-policy)\nreinforcement learning methods called DAPG, to showcase\nits strength. It presents yet another demonstration that fea-\ntures learned by Resnet are quite general and are broadly\napplicable. Resnet features trained over 1000s of real-world\nimages are more robust and resilient in comparison to the\nfeatures learned by methods that learn representation and\npolicies in tandem using only samples from the task distri-\nbution. The use of such general but frozen representations\nin conjunction with RL pipelines additionally avoids the\nnon-stationary issues faced by competing methods that si-\nmultaneously optimizes reinforcement and representation\nobjectives, leading to more stable algorithms. Additionally,\nnot having to train your own features extractors results in a\nsigniﬁcant sample and compute gains, Refer to Figure 5.\nLimitations: While this work demonstrates promises of\nusing pre-trained features, it doesn’t investigate the data\nmismatch problem that might exist. Real-world datasets\nused to train resnet features are from human-centric environ-\nments. While we desire robots to operate in similar settings,\nthere are still differences in their morphology and mode\nof operations. Additionally, resent (and similar models)\nacquire features from data primarily comprised of static\nscenes. In contrast, embodied agents desire rich features of\ndynamic and interactive movements.\nOpportunities: RRL uses a single pre-trained representa-\ntion for solving all the complex and very different tasks.\nUnlike the domains of vision and language, there is a non-\ntrivial cost associated with data in robotics. The possibility\nof having a standard shared representational space opens up\navenues for leveraging data from various sources, building\nhardware-accelerated devices using feature compression,\nlow latency and low bandwidth information transmission.\nRepresentations for Reinforcement Learning\nReferences\nAhn, M., Zhu, H., Hartikainen, K., Ponte, H., Gupta, A.,\nLevine, S., and Kumar, V. Robel: Robotics benchmarks\nfor learning with low-cost robots. In Conference on Robot\nLearning, pp. 1300–1313. PMLR, 2020.\nBengio, Y., Courville, A., and Vincent, P. Representation\nlearning: A review and new perspectives, 2014.\nBurgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N.,\nDesjardins, G., and Lerchner, A. Understanding disentan-\ngling in β-vae, 2018.\nChild, R. Very deep vaes generalize autoregressive models\nand can outperform them on images, 2021.\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever,\nI., and Abbeel, P. Rl2: Fast reinforcement learning via\nslow reinforcement learning, 2016.\nDulac-Arnold, G., Mankowitz, D., and Hester, T. Chal-\nlenges of real-world reinforcement learning.\narXiv\npreprint arXiv:1904.12901, 2019.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,\nV., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,\nI., Legg, S., and Kavukcuoglu, K. Impala: Scalable dis-\ntributed deep-rl with importance weighted actor-learner\narchitectures, 2018.\nFinn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and\nAbbeel, P. Learning visual feature spaces for robotic\nmanipulation with deep spatial autoencoders.\nGupta, A., Eppner, C., Levine, S., and Abbeel, P. Learn-\ning dexterous manipulation for a soft robotic hand from\nhuman demonstration, 2017.\nHa, D. and Schmidhuber, J. Recurrent world models facili-\ntate policy evolution, 2018a.\nHa, D. and Schmidhuber, J. World models. arXiv preprint\narXiv:1803.10122, 2018b.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-\ncritic: Off-policy maximum entropy deep reinforcement\nlearning with a stochastic actor, 2018.\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S.,\nTan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., and\nLevine, S. Soft actor-critic algorithms and applications,\n2019.\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D.,\nLee, H., and Davidson, J. Learning latent dynamics for\nplanning from pixels, 2019.\nHafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to\ncontrol: Learning behaviors by latent imagination, 2020.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition, 2015.\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul,\nT., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Dulac-\nArnold, G., Osband, I., Agapiou, J., Leibo, J. Z., and\nGruslys, A. Deep q-learning from demonstrations, 2017.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nvae: Learning basic visual concepts with a constrained\nvariational framework. 2016.\nHiggins, I., Pal, A., Rusu, A. A., Matthey, L., Burgess, C. P.,\nPritzel, A., Botvinick, M., Blundell, C., and Lerchner,\nA. Darla: Improving zero-shot transfer in reinforcement\nlearning, 2018.\nJaderberg, M., Czarnecki, W. M., Dunning, I., Marris,\nL., Lever, G., Casta˜neda, A. G., Beattie, C., Rabi-\nnowitz, N. C., Morcos, A. S., Ruderman, A., and et al.\nHuman-level performance in 3d multiplayer games with\npopulation-based reinforcement learning. Science, 364\n(6443):859–865, May 2019. ISSN 1095-9203. doi: 10.\n1126/science.aau6249. URL http://dx.doi.org/\n10.1126/science.aau6249.\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Camp-\nbell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza-\nkowski, P., Levine, S., Mohiuddin, A., Sepassi, R.,\nTucker, G., and Michalewski, H. Model-based reinforce-\nment learning for atari, 2020.\nKakade, S. A natural policy gradient. In NIPS, 2001a.\nKakade, S. M. A natural policy gradient. Advances in neural\ninformation processing systems, 14, 2001b.\nKalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog,\nA., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M.,\nVanhoucke, V., and Levine, S. Qt-opt: Scalable deep rein-\nforcement learning for vision-based robotic manipulation,\n2018.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes, 2014.\nKostrikov, I., Yarats, D., and Fergus, R. Image augmentation\nis all you need: Regularizing deep reinforcement learning\nfrom pixels, 2020.\nKulkarni, T., Gupta, A., Ionescu, C., Borgeaud, S.,\nReynolds, M., Zisserman, A., and Mnih, V. Unsupervised\nlearning of object keypoints for perception and control,\n2019.\nLaskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and\nSrinivas, A. Reinforcement learning with augmented data,\n2020.\nRepresentations for Reinforcement Learning\nLevine, S. and Koltun, V.\nGuided policy search.\nIn\nDasgupta, S. and McAllester, D. (eds.), Proceedings of\nthe 30th International Conference on Machine Learn-\ning, volume 28 of Proceedings of Machine Learning\nResearch, pp. 1–9, Atlanta, Georgia, USA, 17–19 Jun\n2013. PMLR.\nURL http://proceedings.mlr.\npress/v28/levine13.html.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end\ntraining of deep visuomotor policies, 2016.\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufﬂenet v2:\nPractical guidelines for efﬁcient cnn architecture design,\n2018.\nManuelli, L., Gao, W., Florence, P., and Tedrake, R. kpam:\nKeypoint affordances for category-level robotic manipu-\nlation. arXiv preprint arXiv:1903.06684, 2019a.\nManuelli, L., Gao, W., Florence, P., and Tedrake, R. kpam:\nKeypoint affordances for category-level robotic manipu-\nlation, 2019b.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-\nness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., Ostrovski, G., Petersen, S., Beattie,\nC., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,\nWierstra, D., Legg, S., and Hassabis, D. Human-level\ncontrol through deep reinforcement learning. Nature, 518\n(7540):529–533, February 2015. ISSN 00280836. URL\nhttp://dx.doi.org/10.1038/nature14236.\nNagabandi, A., Konoglie, K., Levine, S., and Kumar, V.\nDeep dynamics models for learning dexterous manipula-\ntion, 2019.\nNair, A., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine,\nS. Visual reinforcement learning with imagined goals,\n2018.\nOpenAI, Akkaya, I., Andrychowicz, M., Chociej, M.,\nLitwin, M., McGrew, B., Petron, A., Paino, A., Plap-\npert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N.,\nTworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba,\nW., and Zhang, L. Solving rubik’s cube with a robot hand,\n2019.\nQin, Z., Fang, K., Zhu, Y., Fei-Fei, L., and Savarese, S. Keto:\nLearning keypoint representations for tool manipulation,\n2019.\nRajeswaran, A., Kumar, V., Gupta, A., Schulman, J.,\nTodorov, E., and Levine, S. Learning complex dexter-\nous manipulation with deep reinforcement learning and\ndemonstrations. CoRR, abs/1709.10087, 2017. URL\nhttp://arxiv.org/abs/1709.10087.\nRajeswaran, A., Lowrey, K., Todorov, E., and Kakade, S. To-\nwards generalization and simplicity in continuous control,\n2018.\nRajeswaran, A., Mordatch, I., and Kumar, V. A game theo-\nretic framework for model based reinforcement learning,\n2020.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\nChen, L.-C. Mobilenetv2: Inverted residuals and linear\nbottlenecks, 2019.\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and\nAbbeel, P. Trust region policy optimization, 2017.\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R.,\nParikh, D., and Batra, D. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localiza-\ntion. International Journal of Computer Vision, 128(2):\n336–359, Oct 2019. ISSN 1573-1405. doi: 10.1007/\ns11263-019-01228-7. URL http://dx.doi.org/\n10.1007/s11263-019-01228-7.\nSermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E.,\nSchaal, S., and Levine, S. Time-contrastive networks:\nSelf-supervised learning from video, 2018.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L.,\nvan den Driessche, G., Graepel, T., and Hassabis, D.\nMastering the game of go without human knowledge.\nNature, 550:354–, October 2017. URL http://dx.\ndoi.org/10.1038/nature24270.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition, 2015.\nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-\nmiller, M. Striving for simplicity: The all convolutional\nnet, 2015.\nSrinivas, A., Laskin, M., and Abbeel, P. Curl: Contrastive\nunsupervised representations for reinforcement learning,\n2020.\nStone, A., Ramirez, O., Konolige, K., and Jonschkowski, R.\nThe distracting control suite – a challenging benchmark\nfor reinforcement learning from pixels, 2021.\nStooke, A., Lee, K., Abbeel, P., and Laskin, M. Decou-\npling representation learning from reinforcement learning,\n2020.\nSubramanian, A. Pytorch-vae. https://github.com/\nAntixK/PyTorch-VAE, 2020.\nRepresentations for Reinforcement Learning\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision, 2015.\nTan, M. and Le, Q. V. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks, 2020.\nTassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y.,\nde Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J.,\nLefrancq, A., Lillicrap, T., and Riedmiller, M. Deepmind\ncontrol suite, 2018.\nWeng,\nL.\nPolicy\ngradient\nalgorithms.\nlilianweng.github.io/lil-log, 2018.\nURL https://\nlilianweng.github.io/lil-log/2018/04/\n08/policy-gradient-algorithms.html.\nWilliams, R. J. Simple statistical gradient-following al-\ngorithms for connectionist reinforcement learning. In\nMachine Learning, pp. 229–256, 1992.\nYarats, D. and Kostrikov, I.\nSoft actor-critic (sac) im-\nplementation in pytorch.\nhttps://github.com/\ndenisyarats/pytorch_sac, 2020.\nYarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J.,\nand Fergus, R. Improving sample efﬁciency in model-free\nreinforcement learning from images, 2020.\nYou, Y., Lou, Y., Li, C., Cheng, Z., Li, L., Ma, L., Wang,\nW., and Lu, C. Keypointnet: A large-scale 3d keypoint\ndataset aggregated from numerous human annotations,\n2020.\nZhan, A., Zhao, P., Pinto, L., Abbeel, P., and Laskin, M. A\nframework for efﬁcient robotic manipulation, 2020.\nZhu, H., Gupta, A., Rajeswaran, A., Levine, S., and Ku-\nmar, V. Dexterous manipulation with deep reinforcement\nlearning: Efﬁcient, general, and low-cost, 2018.\nZhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh,\nA., Kumar, V., and Levine, S. The ingredients of real-\nworld robotic reinforcement learning, 2020.\nRepresentations for Reinforcement Learning\n7. Appendix\n7.1. Project’s webpage\nFull\ndetails\nof\nthe\nproject\n(including\nvideo\nresults,\ncodebase,\netc)\nare\navailable\nat\nhttps://sites.google.com/view/abstractions4rl.\n7.2. Overview of all methods used in baselines and ablations\nThe environmental setting and the feature extractor used in all the variations and different methods considered is summarized\nin Table 7.2\nObservation\nLatent\nFeatures\nDemos\nRewards\nVision (RGB)\nJoint \nEncoders\nEnvironment \nState\nRRL(Ours)\n✓\n✓\nResnet34\n✓\nSparse\nRRL(Resnet18)\n✓\n✓\nResnet18\n✓\nSparse\nRRL(Resnet50)\n✓\n✓\nResnet50\n✓\nSparse\nRRL (VAE)\n✓\n✓\nVAE\n✓\nSparse\nRRL(Vision)\n✓\nResnet34\n✓\nSparse\nFERM\n✓\n✓\n✓\nSparse\nNPG(State)\n✓\n✓\nSparse\nNPG(Vision)\n✓\nResnet34\nSparse\nDAPG(State)\n✓\n✓\n✓\nSparse\nRRL(Sparse)\n✓\n✓\nResnet34\n✓\nSparse\nRRL(Dense)\n✓\n✓\nResnet34\n✓\nDense\nRRL(Noise)\n✓\n✓\nResnet34\n✓\nSparse\nRRL(Vision +    \n         Sensors)\n✓\n✓\nResnet34\n✓\nSparse\nRRL(ShuffleNet)\n✓\n✓\nShuffleNet-v2\n✓\nSparse\nRRL(MobileNet)\n✓\n✓\nMobileNet-v2\n✓\nSparse\nRRL(vdvae)\n✓\n✓\nVery Deep \nVAE\n✓\nSparse\n7.3. RRL(Ours)\nParameters\nSetting\nBC batch size\n32\nBC epochs\n5\nBC learning rate\n0.001\nPolicy Size\n(256, 256)\nvf batch size\n64\nvf epochs\n2\nrl step size\n0.05\nrl gamma\n0.995\nrl gae\n0.97\nlam 0\n0.01\nlam 1\n0.95\nTable 2. Hyperparameter details for all the RRL variations.\nSame parameters are used across all the tasks (Pen, Door, Hammer, Relocate, PegInsertion, Reacher) unless explicitly\nmentioned. Sparse reward setting is used in all the hand manipulation environments as proposed by Rajeswaran et al. along\nwith 25 expert demonstrations. We have directly used the parameters (summarize in Table 2) provided by DAPG without\nany additional hyperparameter tuning except for the policy size (used same across all tasks). On the Adroit Manipulation\nRepresentations for Reinforcement Learning\ntask, 200 trajectories for Hammer-v0, Door-v0, Relocate-v0 whereas 400 trajectories for Pen-v0 per iteration are collected\nin each iteration.\n7.4. Results on MJRL Environment\n0\n1\n2\n3\n4\nRobot Hours\n0\n1\n2\n3\n4\nRobot Hours\n0\n1\n2\n3\n4\nRobot Hours\n0\n1\n2\n3\n4\nRobot Hours\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nsamples(M)\nPegInsertion\nRRL(Ours)\nFERM\nDAPG(State)\nNPG(State)\n0\n1\n2\n3\n4\n5\nRobot Hours\n0\n1\n2\n3\n4\n5\nRobot Hours\n0\n1\n2\n3\n4\n5\nRobot Hours\n0\n1\n2\n3\n4\n5\nRobot Hours\n0.0\n0.5\n1.0\n1.5\n2.0\nsamples(M)\n0\n20\n40\n60\n80\n100\nSuccess Rate\nReacher\nFigure 10. Results on MJRL Environment. RRL outperforms\nFERM and delivers results on par with DAPG(State) in the PegIn-\nsertion task. In Reacher, FERM outperforms RRL following that\nlearning task speciﬁc representations is easier in simple tasks.\nWe benchmark the performance of RRL on two of the\nMJRL environments (Rajeswaran et al., 2020), Reacher\nand Peg Insertion in Figure 10. These environments are\nquite low dimensional (7DoF Robotic arm) compared to the\nAdroit hand (24 DoF) but still require rich understanding\nof the task.\nIn the peg insertion task, RRL delivers\nstate comparable (DAPG(State)) results and signiﬁcantly\noutperforms FERM. However, in the Reacher task, we\nnotice that DAPG(State) and FERM perform surprisingly\nwell whereas RRL struggles to perform initially.\nThis\nhighlights that using task speciﬁc representations in simple,\nlow dimensional environments might be beneﬁcial as it\nis easy to overﬁt the feature encoder for the task in hand\nwhile the Resnet features are quite generic. For the MJRL\nenvironment, shaped reward setting is used as provided in\nthe repository 2 along with 200 expert demonstrations. For\nthe Peg Insertion task 200 trajectories and for Reacher task\n400 trajectories are collected per iteration.\n7.5. Other variations of RRL\na) RRL(MobileNet), RRL(ShufﬂeNet) : The encoders (ShufﬂeNet (Ma et al., 2018) and MobileNet (Sandler et al., 2019))\nare pretrained on ImageNet Dataset using a classiﬁcation objective. We pick the pretrained models from torchvision directly\nand freeze the parameters during the entire training of the RL agent. Similar to RRL(Ours), the last layer of the model is\nremoved and a latent feature of dimension 1024 and 1280 in case of ShufﬂeNet and MobileNet respectively is used.\nb) RRL(vdvae) : We use a very recent state of the art hierarchical VAE (Child, 2021) that is trained on ImageNet dataset.\nThe code along with the pretrained weights are publically available 3 by the author. We use the intermediate features of the\nencoder of dimension 512. All the parameters are frozen similar to RRL(Ours).\n7.6. DMControl Experiment Details\nFor the RAD (Laskin et al., 2020), CURL (Srinivas et al., 2020), SAC+AE (Yarats et al., 2020) and State SAC (Haarnoja\net al., 2019), we report the numbers directly provided by Laskin et al.. For SAC+RRL, Resnet34 is used as a ﬁxed feature\nextractor and the past three output features (frame stack= 3) are used as a representative of state information in SAC\nalgorithm. For the ﬁxed RAD encoder, we train the RL agent along with RAD encoder using the default hyperparameters\nprovided by the authors for Cartpole environment. We used the trained encoder as a ﬁxed feature extractor and retrain the\npolicies for all the tasks. The frame skip values are task speciﬁc as mentioned in (Yarats et al., 2020) also outlined in Table 4.\nThe hyperparameters used are summarized in the Table 3 where a grid search is made on actor lr = {1e−3, 1e−4}, critic lr\n= {1e −3, 1e −4}, critic update freq = {1, 2}, critic tau = {0.01, 0.05, 0.1} and an average over 3 seeds is reported. SAC\nimplementation in PyTorch courtesy (Yarats & Kostrikov, 2020).\n7.7. RRL(VAE)\nFor training, we collected a dataset of 1 million images of size 64 x 64. Out of the 1 million images collected, 25% of the\nimages are collected using an optimal course of actions (expert policy), 25% with a little noise (expert policy + small noise),\n25% with even higher level of noise (expert policy + large noise) and remaining portion by randomly sampling actions\n(random actions). This is to ensure that the images collected sufﬁciently represents the distribution faced by policy during\n2https://github.com/aravindr93/mjrl\n3https://github.com/openai/vdvae\nRepresentations for Reinforcement Learning\nParameter\nSetting\nframe stack\n3\nreplay buffer capacity\n100000\ninit steps\n1000\nbatch size\n128\nhidden dim\n1024\ncritic lr\n1e-3\ncritic beta\n0.9\ncritic tau\n0.01\ncritic target update freq\n2\nactor lr\n1e-3\nactor beta\n0.9\nactor log std min\n-10\nactor log std max\n2\nactor update freq\n2\ndiscount\n0.99\ninit temperature\n0.1\nalpha lr\n1e-4\nalpha beta\n0.5\nTable 3. SAC hyperparameters.\nEnvironment\naction repeat\nCartpole, Swing\n8\nReacher, Easy\n4\nCheetah, Run\n4\nCup, Catch\n4\nWalker, Walk\n2\nFinger, Spin\n2\nTable 4. Action Repeat Values for DMControl Suite\nthe training of the agent. We observed that this signiﬁcantly helps compared to collecting data only from the expert policy.\nThe variational auto-encoder(VAE) is trained using a reconstruction objective (Kingma & Welling, 2014) for 10epochs.\nFigure 11 showcases the reconstructed images. We used a latent size of 512 for a fair comparison with Resnet. The weights\nof the encoder are freezed and used as feature extractors in place of Resnet in RRL. RRL(VAE) also uses the inputs from the\npro-prioceptive sensors along with the encoded features. VAE implementation courtesy (Subramanian, 2020).\n7.8. Visual Distractor Evaluation details\nIn order to test the generalisation performance of RRL and FERM (Zhan et al., 2020), we subject the environment to various\nkinds of visual distractions during inference (Figure 12). Note all parameters are freezed during this evaluation, an average\nperformance over 75 rollouts is reported. Following distractors were used during inference to test robustness of the ﬁnal\npolicy -\n• Random change in light position.\n• Random change in light direction.\n• Random object color. (Handle, door color for Door-v0; Different hammer parts and nail for Hammer-v0)\n• Introducing a new object in scene - random color, position, size and geometry (Sphere, Capsule, Ellipsoid, Cylinder,\nBox).\nRepresentations for Reinforcement Learning\nFigure 11. ROW1: Original input images of the Hammer task; ROW2: Corresponding Reconstructed images; ROW3: Original input\nimages of the Door task; ROW4: Corresponding Reconstructed images. These images depict that the latent features sufﬁciently encodes\nfeatures required to reconstruct the images.\nFigure 12. COL1: Original images; COL2: Change in light position; COL3: Change in light direction; COL4: Randomizing object colors;\nCOL5: Introducing a random object in the scene. All the parameters are randomly sampled every time in an episode.\n7.9. Compute Cost calculation\nWe calculate the actual compute cost involved for all the methods (RRL(Ours), FERM, RRL(Resnet-50), RRL(Resnet-18))\nthat we have considered. Since in a real-world scenario there is no simulation of the environment we do not include the\ncost of simulation into the calculation. For fair comparison we show the compute cost with same sample complexity (4\nmillion steps) for all the methods. FERM is quite compute intensive (almost 5x RRL(Ours)) because (a) Data augmentation\nis applied at every step (b) The parameters of Actor and Critic are updated once/twice at every step (Compute results shown\nare with one update per step) whereas most of the computation of RRL goes in the encoding of features using Resnet. The\ncost of VAE pretraining in included in the over all cost. RRL(Ours) that uses Resnet-34 strikes a balance between the\ncomputational cost and performance. Note: No parallel processing is used while calculating the cost.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2021-07-07",
  "updated": "2021-11-11"
}