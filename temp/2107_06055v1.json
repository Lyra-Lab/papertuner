{
  "id": "http://arxiv.org/abs/2107.06055v1",
  "title": "On the Difficulty of Translating Free-Order Case-Marking Languages",
  "authors": [
    "Arianna Bisazza",
    "Ahmet Üstün",
    "Stephan Sportel"
  ],
  "abstract": "Identifying factors that make certain languages harder to model than others\nis essential to reach language equality in future Natural Language Processing\ntechnologies. Free-order case-marking languages, such as Russian, Latin or\nTamil, have proved more challenging than fixed-order languages for the tasks of\nsyntactic parsing and subject-verb agreement prediction. In this work, we\ninvestigate whether this class of languages is also more difficult to translate\nby state-of-the-art Neural Machine Translation models (NMT). Using a variety of\nsynthetic languages and a newly introduced translation challenge set, we find\nthat word order flexibility in the source language only leads to a very small\nloss of NMT quality, even though the core verb arguments become impossible to\ndisambiguate in sentences without semantic cues. The latter issue is indeed\nsolved by the addition of case marking. However, in medium- and low-resource\nsettings, the overall NMT quality of fixed-order languages remains unmatched.",
  "text": "On the Difﬁculty of Translating Free-Order Case-Marking Languages\nArianna Bisazza\nAhmet Üstün\nStephan Sportel\nCenter for Language and Cognition\nUniversity of Groningen\n{a.bisazza, a.ustun}@rug.nl, research@spor.tel\nAbstract\nIdentifying factors that make certain lan-\nguages harder to model than others is es-\nsential to reach language equality in future\nNatural Language Processing technologies.\nFree-order case-marking languages, such as\nRussian, Latin or Tamil, have proved more\nchallenging than ﬁxed-order languages for\nthe tasks of syntactic parsing and subject-\nverb agreement prediction. In this work, we\ninvestigate whether this class of languages\nis also more difﬁcult to translate by state-\nof-the-art Neural Machine Translation mod-\nels (NMT). Using a variety of synthetic lan-\nguages and a newly introduced translation\nchallenge set, we ﬁnd that word order ﬂex-\nibility in the source language only leads\nto a very small loss of NMT quality, even\nthough the core verb arguments become im-\npossible to disambiguate in sentences with-\nout semantic cues. The latter issue is in-\ndeed solved by the addition of case marking.\nHowever, in medium- and low-resource set-\ntings, the overall NMT quality of ﬁxed-order\nlanguages remains unmatched.\n1\nIntroduction\nDespite the tremendous advances achieved in less\nthan a decade, Natural Language Processing re-\nmains a ﬁeld where language equality is far from\nbeing reached (Joshi et al., 2020).\nIn the ﬁeld\nof Machine Translation, modern neural models\nhave attained remarkable quality for high-resource\nlanguage pairs like German-English, Chinese-\nEnglish or English-Czech, with a number of stud-\nies claiming even human parity (Hassan et al.,\n2018; Bojar et al., 2018; Barrault et al., 2019;\nPopel et al., 2020). These results may lead to the\nunfounded belief that NMT methods will perform\nequally well in any language pair, provided similar\namounts of training data. In fact, several studies\nsuggest the opposite (Platanios et al., 2018; Ata-\nman and Federico, 2018; Bugliarello et al., 2020).\nWhy, then, do some language pairs have lower\ntranslation accuracy?\nAnd, more speciﬁcally:\nAre certain typological proﬁles more challenging\nfor current state-of-the-art NMT models? Every\nlanguage has its own combination of typological\nproperties, including word order, morphosyntactic\nfeatures and more (Dryer and Haspelmath, 2013).\nIdentifying language properties (or combinations\nthereof) that pose major problems to the current\nmodeling paradigms is essential to reach language\nequality in future MT (and other NLP) technolo-\ngies (Joshi et al., 2020), in a way that is orthogonal\nto data collection efforts. Among others, natural\nlanguages adopt different mechanisms to disam-\nbiguate the role of their constituents: Flexible or-\nder typically correlates with the presence of case\nmarking and, vice versa, ﬁxed order is observed\nin languages with little or no case marking (Com-\nrie, 1981; Sinnemäki, 2008; Futrell et al., 2015b).\nMorphologically rich languages in general are\nknown to be challenging for MT at least since the\ntimes of phrase-based statistical MT (Birch et al.,\n2008) due to their larger and sparser vocabularies,\nand remain challenging even for modern neural ar-\nchitectures (Ataman and Federico, 2018; Belinkov\net al., 2017). By contrast, the relation between\nword order ﬂexibility and MT quality has not been\ndirectly studied to our knowledge.\nIn this paper, we study this relationship using\nstrictly controlled experimental setups.\nSpeciﬁ-\ncally, we ask:\n• Are current state-of-the-art NMT systems bi-\nased towards ﬁxed-order languages?\n• To what extent does case marking compen-\nsate for the lack of a ﬁxed order in the source\nlanguage?\nUnfortunately parallel data is scarce in most of\nthe world languages (Guzmán et al., 2019), and\ncorpora in different languages are drawn from dif-\nferent domains. Exceptions exist, like the widely\narXiv:2107.06055v1  [cs.CL]  13 Jul 2021\nused Europarl (Koehn, 2005), but represent a small\nfraction of the large variety of typological feature\ncombinations attested in the world. This makes\nit very difﬁcult to run a large-scale comparative\nstudy and isolate the factors of interest from, e.g.,\ndomain mismatch effects. As a solution, we pro-\npose to evaluate NMT on synthetic languages (Gu-\nlordava and Merlo, 2016; Wang and Eisner, 2016;\nRavfogel et al., 2019) that differ from each other\nonly by speciﬁc properties, namely: the order of\nmain constituents, or the presence and nature of\ncase markers (see example in Table 1).\nWe use this approach to isolate the impact of\nvarious source-language typological features on\nMT quality and to remove the typical confounders\nof corpus size and domain. Using a variety of syn-\nthetic languages and a newly introduced challenge\nset, we ﬁnd that state-of-the-art NMT has little to\nno bias towards ﬁxed-order languages, but only\nwhen a sizeable training set is available.\n2\nFree-order Case-marking Languages\nThe word order proﬁle of a language is usually\nrepresented by the canonical order of its main\nconstituents, (S)ubject, (O)bject, (V)erb. For in-\nstance, English and French are SVO languages,\nwhile Turkish and Hindi are SOV. Other, less com-\nmonly attested, word orders are VSO and VOS,\nwhile OSV and OVS are extremely rare (Dryer,\n2013). While many other word order features ex-\nist (e.g., noun/adjective), they often correlate with\nthe order of main constituents (Greenberg, 1963).\nA different, but likewise important dimension is\nthat of word order freedom (or ﬂexibility). Lan-\nguages that primarily rely on the position of a\nword to encode grammatical roles typically dis-\nplay rigid orders (like English or Mandarin Chi-\nnese), while languages that rely on case marking\ncan be more ﬂexible allowing word order to ex-\npress discourse-related factors like topicalization.\nExamples of highly ﬂexible-order languages in-\nclude languages as diverse as Russian, Hungarian,\nLatin, Tamil and Turkish.1\nIn the ﬁeld of psycholinguistics, due to the his-\ntorical inﬂuence of English-centered studies, word\norder has long been considered the primary and\nmost natural device through which children learn\n1See Futrell et al. (2015b) for detailed ﬁgures of word\norder freedom (measured by the entropy of subject and ob-\nject dependency relation order) in a diverse sample of 34 lan-\nguages.\nFixed VSO follows the little cat the friendly dog\nVOS follows the friendly dog the little cat\nfollows the little cat#S the friendly dog#O\nFree+Case\nOR\nfollows the friendly dog#O the little cat#S\nTranslation de kleine kat volgt de vriendelijke hond\nTable 1: Example sentence in different ﬁxed/ﬂexible-\norder English-based synthetic languages and their SVO\nDutch translation.\nThe subject in each sentence is\nunderlined. Artiﬁcial case markers start with #.\nto infer syntactic relationships in their language\n(Slobin, 1966). However, cross-linguistic studies\nhave later revealed that children are equally pre-\npared to acquire both ﬁxed-order and inﬂectional\nlanguages (Slobin and Bever, 1982).\nComing to computational linguistics,\ndata-\ndriven MT and other NLP approaches were also\nhistorically developed around languages with re-\nmarkably ﬁxed order and very simple to moder-\nately simple morphological systems, like English\nor French. Luckily, our community has been giv-\ning increasing attention to more and more lan-\nguages with diverse typologies, especially in the\nlast decade.\nSo far, previous work has found\nthat free-order languages are more challenging\nfor parsing (Gulordava and Merlo, 2015, 2016)\nand subject-verb agreement prediction (Ravfogel\net al., 2019) than their ﬁxed-order counterparts.\nThis raises the question of whether word order\nﬂexibility also negatively affects MT quality.\nBefore the advent of modern NMT, Birch et al.\n(2008) used the Europarl corpus to study how var-\nious language properties affected the quality of\nphrase-based Statistical MT. Amount of reorder-\ning, target morphological complexity, and histor-\nical relatedness of source and target languages\nwere identiﬁed as strong predictors of MT qual-\nity.\nRecent work by Bugliarello et al. (2020),\nhowever, has failed to show a correlation between\nNMT difﬁculty (measured by a novel information-\ntheoretic metric) and several linguistic properties\nof source and target language, including Mor-\nphological Counting Complexity (Sagot, 2013)\nand Average Dependency Length (Futrell et al.,\n2015a). While that work speciﬁcally aimed at en-\nsuring cross-linguistic comparability, the sample\non which the linguistic properties could be com-\nputed (Europarl) was rather small and not very ty-\npologically diverse, leaving our research questions\nopen to further investigation.\nIn this paper, we\ntherefore opt for a different methodology: namely,\nsynthetic languages.\n3\nMethodology\nSynthetic languages\nThis paper presents two\nsets of experiments: In the ﬁrst (§4), we create par-\nallel corpora using very simple and predictable ar-\ntiﬁcial grammars and small vocabularies (Lupyan\nand Christiansen, 2002). See example in Table 1.\nBy varying the position of subject/verb/object and\nintroducing case markers to the source language,\nwe study the biases of two NMT architectures in\noptimal training data conditions and a fully con-\ntrolled setup, i.e. without any other linguistic cues\nthat may disambiguate constituent roles. In the\nsecond set of experiments (§5), we move to a more\nrealistic setup using synthetic versions of the En-\nglish language that differ from it in only one or\nfew selected typological features (Ravfogel et al.,\n2019). For instance, the original sentence’s order\n(SVO) is transformed to different orders, like SOV\nor VSO, based on its syntactic parse tree.\nIn both cases, typological variations are intro-\nduced in the source side of the parallel corpora,\nwhile the target language remains ﬁxed. In this\nway, we avoid the issue of non-comparable BLEU\nscores across different target languages. Lastly,\nwe make the simplifying assumption that, when\nverb-argument order varies from the canonical or-\nder in a ﬂexible-order language, it does so in a\ntotally arbitrary way. While this is rarely true in\npractice, as word order may be predictable given\npragmatics or other factors, we focus here on “the\nextent to which word order is conditioned on the\nsyntactic and compositional semantic properties\nof an utterance” (Futrell et al., 2015b).\nTranslation models\nWe consider two widely\nused NMT architectures that crucially differ in\ntheir encoding of positional information: (i) Re-\ncurrent sequence-to-sequence BiLSTM with at-\ntention (Bahdanau et al., 2015; Luong et al.,\n2015) processes the input symbols sequentially\nand has each hidden state directly conditioned on\nthat of the previous (or following, for the back-\nward LSTM) timestep (Elman, 1990; Hochreiter\nand Schmidhuber, 1997). (ii) The non-recurrent,\nfully attention-based Transformer (Vaswani et al.,\n2017) processes all input symbols in parallel re-\nlying on dedicated embeddings to encode each\ninput’s position.2\nTransformer has nowadays\nsurpassed recurrent encoder-decoder models in\nterms of generic MT quality. Moreover, Choshen\nand Abend (2019) have recently shown that\nTransformer-based NMT models are indifferent to\nthe absolute order of source words, at least when\nequipped with learned positional embeddings. On\nthe other hand, the lack of recurrence in Trans-\nformers has been linked to a limited ability to cap-\nture hierarchical structure (Tran et al., 2018; Hahn,\n2020). To our knowledge, no previous work has\nstudied the biases of either architectures towards\nﬁxed-order languages in a systematic manner.\n4\nToy Parallel Grammar\nWe start by evaluating our models on a pair of toy\nlanguages inspired by the English-Dutch pair and\ncreated using a Synchronous Context-Free Gram-\nmar (Chiang and Knight, 2006). Each sentence\nconsists of a simple clause with a transitive verb,\nsubject and object.\nBoth arguments are singu-\nlar and optionally modiﬁed by an adjective. The\nsource vocabulary contains 6 nouns, 6 verbs, 6\nadjectives, and the complete corpus contains 10k\ngenerated sentence pairs.\nWorking with such a\nsmall, ﬁnite grammar allows us to simulate an oth-\nerwise impossible situation where the NMT model\ncan be trained on (almost) the totality of a lan-\nguage’s utterances, canceling out data sparsity ef-\nfects.3\nSource Language Variants\nWe consider three\nsource language variants, illustrated in Table 1:\n• ﬁxed-order VSO;\n• ﬁxed-order VOS;\n• mixed-order (randomly chosen between VSO\nor VOS) with nominal case marking.\nWe choose these word orders so that, in the\nﬂexible-order corpus, the only way to disam-\nbiguate argument roles is case marking, realized\nby simple unambiguous sufﬁxes (#S and #O). The\ntarget language is always ﬁxed SVO. The same\nrandom split (80/10/10% training/validation/test)\nis applied to the three corpora.\n2We use sinusoidal embeddings (Vaswani et al., 2017).\nAll our models are built using OpenNMT: https://\ngithub.com/OpenNMT/OpenNMT-py\n3Data and code to replicate the toy grammar experiments\nin this section are available at https://github.com/\n573phn/cm-vs-wo\n200\n400\n600\n800\n1000\n0\n25\n50\n75\n100\nvos\nvso\nmix\n(a) BiLSTM with attention\n200\n400\n600\n800\n1000\n0\n25\n50\n75\n100\nvos\nvso\nmix\n(b) Large Transformer\n200\n400\n600\n800\n1000\n20\n40\n60\n80\n100\nvos\nvso\nmix\n(c) Small Transformer\nFigure 1: Toy language NMT sentence-level accuracy on validation set by number of training epochs. Source\nlanguages: ﬁxed-order VSO, ﬁxed-order VOS, and mixed-order (VSO/VOS) with case marking. Target language:\nalways ﬁxed SVO. Each experiment is repeated ﬁve times, and averaged results are shown.\nNMT Setup\nAs recurrent model, we trained a 2-\nlayer BiLSTM with attention (Luong et al., 2015)\nwith 500 hidden layer size. As Transformer mod-\nels, we trained one using the standard 6-layer con-\nﬁguration (Vaswani et al., 2017) and a smaller\none with only 2 layers given the simplicity of\nthe languages.\nAll models are trained at the\nword level using the complete vocabulary. More\nhyper-parameters are provided in Appendix A.1.\nNote that our goal is not to compare LSTM and\nTransformer accuracy to each other, but rather\nto observe the different trends across ﬁxed- and\nﬂexible-order language variants. Given the small\nvocabulary, we use sentence-level accuracy in-\nstead of BLEU for evaluation.\nResults\nAs shown in Figure 1, all models\nachieve perfect accuracy on all language pairs af-\nter 1000 training steps, except for the Large Trans-\nformer on the free-order language, likely due to\noverparametrization (Sankararaman et al., 2020).\nThese results demonstrate that our NMT architec-\ntures are equally capable of modeling translation\nof both types of language, when all other factors\nof variation are controlled for. Nonetheless, a pat-\ntern emerges when looking at the learning curves\nwithin each plot: While the two ﬁxed-order lan-\nguages have very similar learning curves, the free-\norder language with case markers always requires\nslightly more training steps to converge. This is\nalso the case, albeit to a lesser extent, when the\nmixed-order corpus is pre-processed by splitting\nall case sufﬁxes from the nouns (extra experiment\nnot shown in the plot).\nThis trend is notewor-\nthy, given the simplicity of our grammars and the\ntransparency of the case system. As our training\nsets cover a large majority of the languages, this\nresult might suggest that free-order natural lan-\nguages need larger training datasets to reach a sim-\nilar translation quality than their ﬁxed-order coun-\nterparts. In §5 we validate this hypothesis on more\nnaturalistic language data.\n5\nSynthetic English Variants\nExperimenting with toy languages has its short-\ncomings, like the small vocabulary size and non-\nrealistic distribution of words and structures. In\nthis section, we follow the approach of Ravfogel\net al. (2019) to validate our ﬁndings in a less con-\ntrolled but more realistic setup. Speciﬁcally, we\ncreate several variants of the Europarl English-\nFrench parallel corpus where the source sentences\nare modiﬁed by changing word order and adding\nartiﬁcial case markers. We choose French as target\nlanguage because of its ﬁxed order, SVO, and its\nrelatively simple morphology.4 As Indo-European\nlanguages, English and French are moderately re-\nlated in terms of syntax and vocabulary while be-\ning sufﬁciently distant to avoid a word-by-word\ntranslation strategy in many cases.\nSource language variants are obtained by trans-\nforming the syntactic tree of the original sen-\ntences. While Ravfogel et al. (2019) could rely\non the Penn Treebank (Marcus et al., 1993) for\ntheir monolingual task of agreement prediction,\nwe instead need parallel data. For this reason, we\nparse the English side of the Europarl v.7 corpus\n(Koehn, 2005) using the Stanza dependency parser\n(Qi et al., 2020; Manning et al., 2014). After pars-\ning, we adopt a modiﬁed version of the synthetic\nlanguage generator by Ravfogel et al. (2019) to\ncreate the following English variants:5\n4According to the Morphological Counting Complexity\n(Sagot, 2013) values reported by Cotterell et al. (2018), En-\nglish scores 6 (least complex), Dutch 26, French 30, Spanish\n71, Czech 195, and Finnish 198 (most complex).\n5Our revised language generator is available at https:\n• ﬁxed-order:\neither SVO, SOV, VSO or\nVOS;6\n• free-order: for each sentence in the corpus,\none of the six possible orders of (Subject, Ob-\nject, Verb) is chosen randomly;\n• shufﬂed words: all source words are shufﬂed\nregardless of their syntactic role. This is our\nlower bound, measuring the reordering abil-\nity of a model in the total absence of source-\nside order cues (akin to bag-of-words input).\nTo allow for a fair comparison with the artiﬁ-\ncial case-marking languages, we remove number\nagreement features from verbs in all the above\nvariants (cf. says →say in Table 2).\nTo answer our second research question, we ex-\nperiment with two artiﬁcial case systems proposed\nby Ravfogel et al. (2019) and illustrated in Table 2\n(overt sufﬁxes):\n• unambiguous case system: sufﬁxes indicat-\ning argument role (subject/object/indirect ob-\nject) and number (singular/plural) are added\nto the heads of noun and verb phrases;\n• syncretic case system: sufﬁxes indicating\nnumber but not grammatical function are\nadded to the heads of main arguments, pro-\nviding only partial disambiguation of argu-\nment roles. This system is inspired from sub-\nject/object syncretism in Russian.\nSyncretic case systems were found to be roughly\nas common as non-syncretic ones in a large sam-\nple of almost 200 world languages (Baerman and\nBrown, 2013). Case marking is always combined\nwith the fully ﬂexible order of main constituents.\nAs in (Ravfogel et al., 2019), English number\nmarking is removed from verbs and their argu-\nments before adding the artiﬁcial sufﬁxes.\n5.1\nNMT Setup\nModels\nAs recurrent model, we used a 3-layer\nBiLSTM with hidden size of 512 and MLP at-\ntention (Bahdanau et al., 2015). The Transformer\nmodel has the standard 6-layer conﬁguration with\nhidden size of 512, 8 attention heads, and sinu-\nsoidal positional encoding (Vaswani et al., 2017).\n//github.com/573phn/rnn_typology\n6To keep the number of experiments manageable, we omit\nobject-initial languages which are signiﬁcantly less attested\namong world languages (Dryer, 2013).\nOriginal (no case):\nThe woman says her sisters often invited her for dinner.\nSOV (no case):\nThe woman her sisters her often invited for dinner say.\nSOV, syncretic case marking (overt):\nThe woman.arg.sg her sisters.arg.pl she.arg.sg often in-\nvited.arg.pl for dinner say.arg.sg.\nSOV, unambiguous case marking (overt):\nThe woman.nsubj.sg her sisters.nsubj.pl she.dobj.sg of-\nten invited.dobj.sg.nsubj.pl for dinner say.nsubj.sg.\nSOV, unambiguous case (implicit):\nThe\nwomankar\nher\nsisterskon\nshekin\noften\nin-\nvitedkinkon for dinner saykar.\nSOV, unambiguous case (implicit with declensions):\nThe womankar her sisterspon shekit often invitedkitpon\nfor dinner saykar.\nFrench translation:\nLa femme dit que ses soeurs l’invitaient souvent à dîner.\nTable 2: Examples of synthetic English variants and\ntheir (common) French translation. The full list of suf-\nﬁxes is provided in Appendix A.3.\nAll models use subword representation based on\n32k BPE merge operations (Sennrich et al., 2016),\nexcept in the low-resource setup where this is re-\nduced to 10k operations. More hyper-parameters\nare provided in Appendix A.1.\nData and Evaluation\nWe train our models on\nvarious subsets of the English-French Europarl\ncorpus:\n1,9M sentence pairs (high-resource),\n100K (medium-resource), 10K (low-resource).\nFor evaluation, we use 5K sentences randomly\nheld-out from the same corpus. Given the impor-\ntance of word order to assess the correct transla-\ntion of verb arguments into French, we compute\nthe reordering-focused RIBES7 metric (Isozaki\net al., 2010) in addition to the more commonly\nused BLEU (Papineni et al., 2002). In each exper-\niment, the source side of training and test data is\ntransformed using the same procedure whereas the\ntarget side remains unchanged. We repeat each ex-\nperiment 3 times (or 4 for languages with random\norder choice) and report the averaged results.\n7BLEU captures local word-order errors only indirectly\n(lower precision of higher-order n-grams) and does not cap-\nture long-range word-order errors at all. By contrast, RIBES\ndirectly measures correlation between the word ranks in the\nreference and those in the MT output.\n5.2\nChallenge Set\nBesides syntactic structure, natural language of-\nten contains semantic and collocational cues that\nhelp disambiguate the role of an argument. Small\nBLEU/RIBES differences between our language\nvariants may indicate actual robustness of a model\nto word order ﬂexibility, but may also indicate that\na model relies on those cues rather than on syntac-\ntic structure (Gulordava et al., 2018). To discern\nthese two hypotheses, we create a challenge set\nof 7,200 simple afﬁrmative and negative sentences\nwhere swapping subject and object leads to an-\nother plausible sentence.8 Each English sentence\nand its reverse are included in the test set together\nwith the respective translations, as for example:\n(1) a. The president thanks the minister. /\nLe président remercie le ministre.\nb. The minister thanks the president. /\nLe ministre remercie le président.\nThe source side is then processed as explained in\n§5 and translated by the NMT model trained on\nthe corresponding language variant. Thus, trans-\nlation quality on this set reﬂects the extent to\nwhich NMT models have robustly learnt to detect\nverb arguments and their roles independently from\nother cues, which we consider an important sign\nof linguistic generalization ability. For space con-\nstraints we only present RIBES scores on the chal-\nlenge set.9\n5.3\nHigh-Resource Results\nTable 3 reports the high-resource setting results.\nThe ﬁrst row (original English to French) is given\nonly for reference and shows the overall highest\nresults. The BLEU drop observed when moving\nto any of the ﬁxed-order variants (including SVO)\nis likely due to parsing ﬂaws resulting in awkward\nreorderings. As this issue affects all our synthetic\nvariants, it does not undermine the validity of our\nﬁndings. For clarity, we center our main discus-\nsion on the Transformer results and comment on\nthe BiLSTM results at the end of this section.\n8More details can be found in Appendix A.2.\nWe\nrelease the challenge set at https://github.com/\narianna-bis/freeorder-mt\n9We also computed BLEU scores: they strongly correlate\nwith RIBES but ﬂuctuate more due to the larger effect of lex-\nical choice.\nFixed-Order Variants\nAll four tested ﬁxed-\norder variants obtain very similar BLEU/RIBES\nscores on the Europarl-test. This is in line with\nprevious work in NMT showing that linguistically\nmotivated pre-ordering leads to small gains (Zhao\net al., 2018) or none at all (Du and Way, 2017),\nand that Transformer-based models are not bi-\nased towards monotonic translation (Choshen and\nAbend, 2019). On the challenge set, scores are\nslightly more variable but a manual inspection re-\nveals that this is due to different lexical choices,\nwhile word order is always correct for this group\nof languages.\nTo sum up, in the high-resource\nsetup, our Transformer models are perfectly able\nto disambiguate the core argument roles when\nthese are consistently encoded by word order.\nFixed-Order\nvs\nRandom-Order\nSomewhat\nsurprisingly, the Transformer results are only\nmarginally affected by the random ordering of\nverb and core arguments.\nRecall that in the\n‘Random’ language all six possible permutations\nof (S,V,O) are equally likely. Thus, Transformer\nshows an excellent ability to reconstruct the cor-\nrect constituent order in the general-purpose test\nset. The picture is very different on the challenge\nset, where RIBES drops severely from 97.6 to\n74.1. These low results were to be expected given\nthe challenge set design (it is impossible even for\na human to recognize subject from object in the\n‘Random, no case’ challenge set). Nonetheless,\nthey demonstrate that the general-purpose set\ncannot tell us whether an NMT model has learnt\nto reliably exploit syntactic structure of the source\nlanguage, because of the abundant non-syntactic\ncues.\nIn fact, even when all source words are\nshufﬂed, Transformer still achieves a respectable\n25.8/71.2 BLEU/RIBES on the Europarl-test.\nCase Marking\nThe key comparison in our study\nlies between ﬁxed-order and free-order case-\nmarking languages. Here, we ﬁnd that case mark-\ning can indeed restore near-perfect accuracy on the\nchallenge set (98.1 RIBES). However, this only\nhappens when the marking system is completely\nunambiguous, which, as already mentioned, is true\nfor only about a half of the real case-marking lan-\nguages (Baerman and Brown, 2013). Indeed the\nsyncretic system visibly improves quality on the\nchallenge set (74.1 to 84.4 RIBES) but remains far\nbehind the ﬁxed-order score (97.6). In terms of\noverall NMT quality (Europarl-test), ﬁxed-order\nEnglish*→French\nBI-LSTM\nTRANSFORMER\nLarge Training (1.9M)\nEuroparl-Test\nChallenge\nEuroparl-Test\nChallenge\nBLEU\nRIBES\nRIBES\nBLEU\nRIBES\nRIBES\nOriginal English\n39.4\n85.0\n98.0\n38.3\n84.9\n97.7\nFixed Order:\nS-V-O\n38.3\n84.5\n98.1\n37.7\n84.6\n98.0\nS-O-V\n37.6\n84.2\n97.7\n37.9\n84.5\n97.2\nV-S-O\n38.0\n84.2\n97.8\n37.8\n84.6\n98.0\nV-O-S\n37.8\n84.0\n98.0\n37.6\n84.3\n97.2\nAverage (ﬁxed orders)\n37.9±0.4\n84.2±0.3\n97.9±0.2\n37.8±0.1\n84.5±0.1\n97.6±0.4\nFlexible Order:\nRandom, no case\n37.1\n83.7\n75.1\n37.5\n84.2\n74.1\nRandom + syncretic case\n36.9\n83.6\n75.4\n37.3\n84.2\n84.4\nRandom + unambig. case\n37.3\n83.9\n97.7\n37.3\n84.4\n98.1\nShufﬂe all words\n18.5\n65.2\n79.4\n25.8\n71.2\n83.2\nTable 3: Translation quality from various English-based synthetic languages into standard French, using the largest\ntraining data (1.9M sentences). NMT architectures: 3-layer BiLSTM seq-to-seq with attention; 6-layer Trans-\nformer. Europarl-Test: 5K held-out Europarl sentences; Challenge set: see §5.2. All scores are averaged over three\ntraining runs.\nlanguages score only marginally higher than the\nfree-order case-marking ones, regardless of the\nunambiguous/syncretic distinction. Thus our ﬁnd-\ning that Transformer NMT systems are equally ca-\npable of modeling the two types of languages (§4)\nis also conﬁrmed with more naturalistic language\ndata. That said, we will show in Sect. 5.4 that this\npositive ﬁnding is conditional on the availability\nof large amounts of training samples.\nBiLSTM vs Transformer\nThe LSTM-based re-\nsults generally correlate with the Transformer re-\nsults discussed above, however our recurrent mod-\nels appear to be slightly more sensitive to changes\nin the source-side order, in line with previous\nﬁndings (Choshen and Abend, 2019).\nSpeciﬁ-\ncally, translation quality on Europarl-test ﬂuctu-\nates slightly more than Transformer among dif-\nferent ﬁxed orders, with the most monotonic or-\nder (SVO) leading to the best results.\nWhen\nall words are randomly shufﬂed, BiLSTM scores\ndrop much more than Transformer.\nHowever,\nwhen comparing the ﬁxed-order variants to the\nones with free order of main constituents, BiL-\nSTM shows only a slightly stronger preference for\nﬁxed-order, compared to Transformer. This sug-\ngests that, by experimenting with arbitrary permu-\ntations, Choshen and Abend (2019) might have\noverestimated the bias of recurrent NMT towards\nmore monotonic translation, whereas the more re-\nalistic combination of constituent-level reordering\nwith case marking used in our study is not so prob-\nlematic for this type of model.\nInterestingly, on the challenge set, BiLSTM and\nTransformer perform on par, with the notable ex-\nception that syncretic case is much more difﬁcult\nfor the BiLSTM model. Our results agree with the\nlarge drop of subject-verb agreement prediction\naccuracy observed by Ravfogel et al. (2019) when\nexperimenting with the random order of main con-\nstituents.\nHowever, their scores were also low\nfor SOV and VOS, which is not the case in our\nNMT experiments. Besides the fact that our chal-\nlenge set only contains short sentences (hence no\nlong dependencies and few agreement attractors),\nour task is considerably different in that agreement\nonly needs to be predicted in the target language,\nwhich is ﬁxed-order SVO.\nSummary\nOur results so far suggest that state-\nof-the-art NMT models, especially if Transformer-\nbased, have little or no bias towards ﬁxed-order\nlanguages. In what follows, we study whether this\nﬁnding is robust to differences in data size, type of\nmorphology, and target language.\n5.4\nEffect of Data Size and Morphological\nFeatures\nData Size\nThe results shown in Table 3 rep-\nresent a high-resource setting (almost 2M train-\ning sentences). While recent successes in cross-\nlingual transfer learning alleviate the need for la-\nbeled data (Liu et al., 2020), their success still de-\npends on the availability of large unlabeled data\nas well as other, yet to be explained, language\nproperties (Joshi et al., 2020). We then ask: Do\nfree-order case-marking languages need more data\nthan ﬁxed-order non-case-marking ones to reach\nsimilar NMT quality?\nWe simulate a medium-\nand low-resource scenario by sampling 100K and\n10K training sentences, respectively, from the full\nEuroparl data. To reduce the number of exper-\niments, we only consider Transformer with one\nﬁxed-order language variant (SOV)10 and exclude\nsyncretic case marking.\nTo disentagle the ef-\nfect of word order from that of case marking on\nlow-resource translation quality, we also exper-\niment with a language variant combining ﬁxed-\norder (SOV) and case marking. Results are shown\nin Figure 2 and discussed below.\nMorphological Features\nThe artiﬁcial case sys-\ntems used so far included easily separable sufﬁxes\nwith a 1:1 mapping between grammatical cate-\ngories and morphemes (e.g. .nsubj.sg, .dobj.pl)\nreminiscent of agglutinative morphologies. Many\nworld languages, however, do not comply to this\n1:1 mapping principle but display ﬂexivity (multi-\nple categories conveyed by one morpheme) and/or\nexponence (the same category expressed by var-\nious, lexically determined, morphemes).\nWell-\nstudied examples of languages with case+number\nexponence include Russian and Finnish, while\nﬂexive languages include, again, Russian and\nLatin.\nMotivated by previous ﬁndings on the\nimpact of ﬁne-grained morphological features on\nlanguage modeling difﬁculty (Gerz et al., 2018),\nwe experiment with three types of sufﬁxes (see ex-\namples in Table 2):\n• overt: number and case are denoted by eas-\nily separable sufﬁxes (e.g. .nsubj.sg, .dobj.pl)\nsimilar to agglutinative languages (1:1);\n10We choose SOV because it is a commonly attested word\norder and is different from that of the target language, thereby\nrequiring some non-trivial reorderings during translation.\n• implicit:\nthe combination of number and\ncase is expressed by unique sufﬁxes without\ninternal structure (e.g. kar for .nsubj.sg, ker\nfor .dobj.pl) similar to fusional languages.\nThis system displays exponence (many:1);\n• implicit with declensions: like the previous,\nbut with three different paradigms each ar-\nbitrarily assigned to a different subset of the\nlexicon. This system displays exponence and\nﬂexivity (many:many).\nA complete overview of our morphological\nparadigms is provided in Appendix A.3 All our\nlanguages have moderate inﬂectional synthesis\nand, in terms of fusion, are exclusively concate-\nnative. Despite this, the effect on vocabulary size\nis substantial: 180% increase by overt and implicit\ncase marking, 250% by implicit marking with de-\nclensions (in the full data setting).\nResults\nResults are shown in the plots of Fig-\nure 2 (detailed numerical scores are given in Ap-\npendix A.4). We ﬁnd that reducing training size\nhas, not surprisingly, a major effect on transla-\ntion quality.\nAmong source language variants,\nﬁxed-order obtains the highest quality across all\nsetups.\nIn terms of BLEU (2(a)), the spread\namong variants increases somewhat with less data\nhowever differences are small. A clearer picture\nemerges from RIBES (2(b)), whereby less data\nclearly leads to more disparity.\nThis is already\nvisible in the 100k setup, with the ﬁxed SOV lan-\nguage dominating the others. Case marking, de-\nspite being necessary to disambiguate argument\nroles in the absence of semantic cues, does not im-\nprove translation quality and even degrades it in\nthe low-resource setup. Looking at the challenge\nset results (2(c)) we see that the free-order case-\nmarking languages are clearly disadvantaged: In\nthe mid-resource setup, case marking improves\nsubstantially over the underspeciﬁed random,no-\ncase language but remains far behind ﬁxed-order.\nIn low-resource, case marking notably hurts qual-\nity even in comparison with the underspeciﬁed\nlanguage.\nThese results thus demonstrate that\nfree-order case-marking languages require more\ndata than their ﬁxed-order counterparts to be ac-\ncurately translated by state-of-the-art NMT.11 Our\nexperiments also show that this greater learning\n11In the light of this ﬁnding, it would be interesting to re-\nvisit the evaluation of Bugliarello et al. (2020) in relation to\nvarying data sizes.\n1.9M\n100K\n10K\n10\n15\n20\n25\n30\n35\nsov\nsov+overt\nrandom\nr+overt\nr+implicit\nr+declens\n(a) Europarl-Test BLEU\n1.9M\n100K\n10K\n60\n65\n70\n75\n80\n85\nsov\nsov+overt\nrandom\nr+overt\nr+implicit\nr+declens\n(b) Europarl-Test RIBES\n1.9M\n100K\n10K\n60\n70\n80\n90\n100\nsov\nsov+overt\nrandom\nr+overt\nr+implicit\nr+declens\n(c) Challenge RIBES\nFigure 2: EN*-FR Transformer NMT quality versus training data size (x-axis). Source language variants: Fixed-\norder (SOV) and free-order (random) with different case systems (r+overt/implicit/declens). Scores averaged over\nthree training runs. Detailed numerical results are provided in Appendix A.4.\ndifﬁculty is not only due to case marking (and sub-\nsequent data sparsity), but also to word order ﬂexi-\nbility (compare sov+overt to r+overt in Figure 2).\nRegarding different morphology types, we do\nnot observe a consistent trend in terms of overall\ntranslation quality (Europarl-test): in some cases,\nthe richest morphology (with declensions) slightly\noutperforms the one without declensions —a re-\nsult that would deserve further exploration. On\nthe other hand, results on the challenge set, where\nmost words are case-marked, show that morpho-\nlogical richness inversely correlates with transla-\ntion quality when data is scarce. We postulate that\nour artiﬁcial morphologies may be too limited in\nscope (only 3-way case and number marking) to\nimpact overall translation quality and leave the in-\nvestigation of richer inﬂectional synthesis to future\nwork.\n5.5\nEffect of Target Language\nAll results so far involved translation into a ﬁxed-\norder (SVO) language without case marking. To\nverify the generality of our ﬁndings, we repeat a\nsubset of experiments with the same synthetic En-\nglish variants, but using Czech or Dutch as target\nlanguages. Czech has rich fusional morphology\nincluding case marking, and very ﬂexible order.\nDutch has simple morphology (no case marking)\nand moderately ﬂexible, syntactically determined\norder.12\nFigure 3 shows the results with 100k training\nsentences.\nIn terms of BLEU, differences are\neven smaller than in English-French. In terms of\nRIBES, trends are similar across target languages,\nwith the ﬁxed SOV source language obtaining best\nresults and the case-marked source language ob-\ntaining worst results. This suggests that the major\nﬁndings of our study are not due to the speciﬁc\nchoice of French as the target language.\nen*-FR\nen*-CS\nen*-NL\n12\n14\n16\n18\n20\n22\n24\n26\nsov\nrandom\nr+overt\n(a) Europarl BLEU\nen*-FR\nen*-CS\nen*-NL\n66\n68\n70\n72\n74\n76\n78\n80\n(b) Europarl RIBES\nFigure 3: Transformer results for more target languages\n(100k training size). Scores averaged over 2 runs.\n12Dutch word order is very similar to German, with the\nposition of S, V, and O depending on the type of clause.\n6\nRelated Work\nThe effect of word order ﬂexibility on NLP model\nperformance has been mostly studied in the ﬁeld\nof syntactic parsing, for instance using Aver-\nage Dependency Length (Gildea and Temperley,\n2010; Futrell et al., 2015a) or head-dependent or-\nder entropy (Futrell et al., 2015b; Gulordava and\nMerlo, 2016) as syntactic correlates of word or-\nder freedom. Related work in language model-\ning has shown that certain languages are intrinsi-\ncally more difﬁcult to model than others (Cotterell\net al., 2018; Mielke et al., 2019) and has further-\nmore studied the impact of ﬁne-grained morphol-\nogy features (Gerz et al., 2018) on LM perplexity.\nRegarding the word order biases of seq-to-seq\nmodels, Chaabouni et al. (2019) use miniature lan-\nguages similar to those of Sect. 4 to study the\nevolution of LSTM-based agents in a simulated\niterated learning setup.\nTheir results in a stan-\ndard “individual learning” setup show, like ours,\nthat a free-order case-marking toy language can be\nlearned just as well as a ﬁxed-order one, conﬁrm-\ning earlier results obtained by simple Elman net-\nworks trained for grammatical role classiﬁcation\n(Lupyan and Christiansen, 2002).\nTransformer\nwas not included in these studies. Choshen and\nAbend (2019) measure the ability of LSTM- and\nTransformer-based NMT to model a language pair\nwhere the same arbitrary (non syntactically mo-\ntivated) permutation is applied to all source sen-\ntences. They ﬁnd that Transformer is largely indif-\nferent to the order of source words (provided this\nis ﬁxed and consistent across training and test set)\nbut nonetheless struggles to translate long depen-\ndencies actually occurring in natural data. They\ndo not directly study the effect of order ﬂexibility.\nThe idea of permuting dependency trees to gen-\nerate synthetic languages was introduced indepen-\ndently by Gulordava and Merlo (2016) (discussed\nabove) and by Wang and Eisner (2016), the latter\nwith the aim of diversifying the set of treebanks\ncurrently available for language adaptation.\n7\nConclusions\nWe have presented an in-depth analysis of how\nNeural Machine Translation difﬁculty is affected\nby word order ﬂexibility and case marking in the\nsource language.\nAlthough these common lan-\nguage properties were previously shown to nega-\ntively affect parsing and agreement prediction ac-\ncuracy, our main results show that state-of-the-\nart NMT models, especially Transformer-based\nones, have little or no bias towards ﬁxed-order lan-\nguages. Our simulated low-resource experiments,\nhowever, reveal a different picture, that is: free-\norder case-marking languages require more data\nto be translated as accurately as their ﬁxed-order\ncounterparts. Since parallel data (like labeled data\nin general) is scarce for most of the world lan-\nguages (Guzmán et al., 2019; Joshi et al., 2020),\nwe believe this should be considered as a further\nobstacle to language equality in future NLP tech-\nnologies.\nIn future work, our analysis should be extended\nto target language variants using principled alter-\nnatives to BLEU (Bugliarello et al., 2020), and\nto other typological features that are likely to af-\nfect MT performance, such as inﬂectional synthe-\nsis and degree of fusion (Gerz et al., 2018). Fi-\nnally, the synthetic languages and challenge set\nproposed in this paper could be used to evaluate\nsyntax-aware NMT models (Eriguchi et al., 2016;\nBisk and Tran, 2018; Currey and Heaﬁeld, 2019),\nwhich promise to better capture linguistic struc-\nture, especially in low-resource scenarios.\nAcknowledgements\nArianna Bisazza was partly funded by the Nether-\nlands Organization for Scientiﬁc Research (NWO)\nunder project number 639.021.646. We would like\nto thank the Center for Information Technology of\nthe University of Groningen for providing access\nto the Peregrine HPC cluster, and the anonymous\nreviewers for their helpful comments.\nReferences\nDuygu Ataman and Marcello Federico. 2018. An\nevaluation of two vocabulary reduction methods\nfor neural machine translation. In Proceedings\nof the 13th Conference of the Association for\nMachine Translation in the Americas (Volume\n1: Research Papers), pages 97–110.\nMatthew Baerman and Dunstan Brown. 2013.\nCase syncretism.\nIn Matthew S. Dryer and\nMartin Haspelmath, editors, The World Atlas of\nLanguage Structures Online. Max Planck Insti-\ntute for Evolutionary Anthropology, Leipzig.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by\njointly learning to align and translate. In 3rd In-\nternational Conference on Learning Represen-\ntations, ICLR 2015.\nLoïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp\nKoehn,\nShervin Malmasi,\nChristof Monz,\nMathias Müller, Santanu Pal, Matt Post, and\nMarcos Zampieri. 2019. Findings of the 2019\nconference on machine translation (WMT19).\nIn Proceedings of the Fourth Conference on\nMachine Translation (Volume 2: Shared Task\nPapers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi,\nHassan Sajjad, and James Glass. 2017. What do\nneural machine translation models learn about\nmorphology?\nIn Proceedings of the 55th An-\nnual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers),\npages 861–872, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nAlexandra Birch, Miles Osborne, and Philipp\nKoehn. 2008.\nPredicting success in machine\ntranslation.\nIn Proceedings of the 2008 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing, pages 745–754, Honolulu,\nHawaii. Association for Computational Lin-\nguistics.\nYonatan Bisk and Ke Tran. 2018. Inducing gram-\nmars with and for neural machine translation.\nIn Proceedings of the 2nd Workshop on Neu-\nral Machine Translation and Generation, pages\n25–35, Melbourne, Australia. Association for\nComputational Linguistics.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn,\nand Christof Monz. 2018. Findings of the 2018\nconference on machine translation (WMT18).\nIn Proceedings of the Third Conference on Ma-\nchine Translation: Shared Task Papers, pages\n272–303, Belgium, Brussels. Association for\nComputational Linguistics.\nEmanuele Bugliarello, Sabrina J. Mielke, An-\ntonios Anastasopoulos, Ryan Cotterell, and\nNaoaki Okazaki. 2020. It’s easier to translate\nout of English than into it: Measuring neural\ntranslation difﬁculty by cross-mutual informa-\ntion. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Lin-\nguistics, pages 1640–1649, Online. Association\nfor Computational Linguistics.\nRahma Chaabouni, Eugene Kharitonov, Alessan-\ndro Lazaric, Emmanuel Dupoux, and Marco\nBaroni. 2019. Word-order biases in deep-agent\nemergent communication.\nIn Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics, pages 5166–5175,\nFlorence, Italy. Association for Computational\nLinguistics.\nDavid Chiang and Kevin Knight. 2006.\nAn in-\ntroduction to synchronous grammars.\nTuto-\nrial available at http://www.isi.edu/\n~chiang/papers/synchtut.pdf.\nLeshem Choshen and Omri Abend. 2019. Auto-\nmatically extracting challenge sets for non-local\nphenomena in neural machine translation.\nIn\nProceedings of the 23rd Conference on Compu-\ntational Natural Language Learning (CoNLL),\npages 291–303, Hong Kong, China. Associa-\ntion for Computational Linguistics.\nBenrard Comrie. 1981. Language Universals and\nLinguistic Typology. Blackwell.\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner,\nand Brian Roark. 2018.\nAre all languages\nequally hard to language-model?\nIn Proceed-\nings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 536–541,\nNew Orleans, Louisiana. Association for Com-\nputational Linguistics.\nAnna Currey and Kenneth Heaﬁeld. 2019. Incor-\nporating source syntax into transformer-based\nneural machine translation. In Proceedings of\nthe Fourth Conference on Machine Translation\n(Volume 1:\nResearch Papers), pages 24–33,\nFlorence, Italy. Association for Computational\nLinguistics.\nMatthew S. Dryer. 2013.\nOrder of subject, ob-\nject and verb. In Matthew S. Dryer and Martin\nHaspelmath, editors, The World Atlas of Lan-\nguage Structures Online. Max Planck Institute\nfor Evolutionary Anthropology, Leipzig.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for\nEvolutionary Anthropology, Leipzig.\nJinhua Du and Andy Way. 2017. Pre-reordering\nfor neural machine translation:\nHelpful or\nharmful?\nThe Prague Bulletin of Mathemati-\ncal Linguistics, 108(1):171–182.\nJeffrey L. Elman. 1990. Finding structure in time.\nCognitive Science, 14(2):179–211.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshi-\nmasa Tsuruoka. 2016. Tree-to-sequence atten-\ntional neural machine translation. In Proceed-\nings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume\n1: Long Papers), pages 823–833, Berlin, Ger-\nmany. Association for Computational Linguis-\ntics.\nRichard Futrell, Kyle Mahowald, and Edward\nGibson. 2015a.\nLarge-scale evidence of de-\npendency length minimization in 37 languages.\nProceedings of the National Academy of Sci-\nences, 112(33):10336–10341.\nRichard Futrell, Kyle Mahowald, and Edward\nGibson. 2015b. Quantifying word order free-\ndom in dependency corpora. In Proceedings of\nthe Third International Conference on Depen-\ndency Linguistics (Depling 2015), pages 91–\n100, Uppsala, Sweden. Uppsala University, Up-\npsala, Sweden.\nDaniela Gerz, Ivan Vuli´c, Edoardo Maria Ponti,\nRoi Reichart, and Anna Korhonen. 2018. On\nthe relation between linguistic typology and\n(limitations of) multilingual language model-\ning. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Pro-\ncessing, pages 316–327, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nDaniel Gildea and David Temperley. 2010.\nDo\ngrammars minimize dependency length? Cog-\nnitive Science, 34(2):286–310.\nJoseph H. Greenberg. 1963. Some universals of\ngrammar with particular reference to the order\nof meaningful elements. In Joseph H. Green-\nberg, editor, Universals of Human Language,\npages 73–113. MIT Press, Cambridge, Mass.\nKristina Gulordava, Piotr Bojanowski, Edouard\nGrave, Tal Linzen, and Marco Baroni. 2018.\nColorless green recurrent networks dream hi-\nerarchically.\nIn Proceedings of the 2018\nConference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1\n(Long Papers), pages 1195–1205, New Orleans,\nLouisiana. Association for Computational Lin-\nguistics.\nKristina Gulordava and Paola Merlo. 2015. Di-\nachronic trends in word order freedom and de-\npendency length in dependency-annotated cor-\npora of Latin and ancient Greek. In Proceed-\nings of the Third International Conference on\nDependency Linguistics (Depling 2015), pages\n121–130, Uppsala, Sweden. Uppsala Univer-\nsity, Uppsala, Sweden.\nKristina Gulordava and Paola Merlo. 2016. Multi-\nlingual dependency parsing evaluation: a large-\nscale analysis of word order properties using ar-\ntiﬁcial data. Transactions of the Association for\nComputational Linguistics, 4:343–356.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott,\nJuan Pino, Guillaume Lample, Philipp Koehn,\nVishrav Chaudhary, and Marc’Aurelio Ranzato.\n2019.\nThe FLORES Evaluation Datasets for\nLow-Resource Machine Translation: Nepali–\nEnglish and Sinhala–English. In Proceedings\nof the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the\n9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP),\npages 6100–6113.\nMichael Hahn. 2020.\nTheoretical limitations\nof self-attention in neural sequence models.\nTransactions of the Association for Computa-\ntional Linguistics, 8:156–171.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Fed-\nermann, Xuedong Huang, Marcin Junczys-\nDowmunt, William Lewis, Mu Li, et al. 2018.\nAchieving human parity on automatic Chinese\nto English news translation.\narXiv preprint\narXiv:1803.05567.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nHideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-\nsuhito Sudoh, and Hajime Tsukada. 2010. Au-\ntomatic evaluation of translation quality for dis-\ntant language pairs. In Proceedings of the 2010\nConference on Empirical Methods in Natural\nLanguage Processing, pages 944–952, Cam-\nbridge, MA. Association for Computational\nLinguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Ka-\nlika Bali, and Monojit Choudhury. 2020. The\nstate and fate of linguistic diversity and inclu-\nsion in the NLP world.\nIn Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6282–6293,\nOnline. Association for Computational Linguis-\ntics.\nPhilipp Koehn. 2005. Europarl: A parallel cor-\npus for statistical machine translation. In The\nTenth Machine Translation Summit Proceedings\nof Conference, pages 79–86. International As-\nsociation for Machine Translation.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li,\nSergey Edunov, Marjan Ghazvininejad, Mike\nLewis, and Luke Zettlemoyer. 2020. Multilin-\ngual denoising pre-training for neural machine\ntranslation. Transactions of the Association for\nComputational Linguistics, 8:726–742.\nMinh-Thang Luong, Hieu Pham, and Christo-\npher D. Manning. 2015. Effective approaches\nto attention-based neural machine translation.\nIn Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1412–1421, Lis-\nbon, Portugal. Association for Computational\nLinguistics.\nGary Lupyan and Morten H. Christiansen. 2002.\nCase, word order, and language learnability: In-\nsights from connectionist modeling.\nIn Pro-\nceedings of the Twenty-Fourth Annual Confer-\nence of the Cognitive Science Society.\nChristopher D. Manning, Mihai Surdeanu, John\nBauer, Jenny Finkel, Steven J. Bethard, and\nDavid McClosky. 2014. The Stanford CoreNLP\nnatural language processing toolkit. In Associ-\nation for Computational Linguistics (ACL) Sys-\ntem Demonstrations, pages 55–60.\nMitchell\nMarcus,\nBeatrice\nSantorini,\nand\nMary Ann Marcinkiewicz. 1993.\nBuilding\na large annotated corpus of English: The Penn\nTreebank.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman,\nBrian Roark, and Jason Eisner. 2019.\nWhat\nkind of language is hard to language-model?\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 4975–4989, Florence, Italy. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002. BLEU: A Method for Au-\ntomatic Evaluation of Machine Translation. In\nProceedings of the 40th Annual Meeting on As-\nsociation for Computational Linguistics, ACL\n’02, pages 311–318, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nEmmanouil\nAntonios\nPlatanios,\nMrinmaya\nSachan, Graham Neubig, and Tom Mitchell.\n2018.\nContextual parameter generation for\nuniversal neural machine translation.\nIn Pro-\nceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing,\npages 425–435.\nMartin Popel, Marketa Tomkova, Jakub Tomek,\nŁukasz Kaiser, Jakob Uszkoreit, Ondˇrej Bo-\njar, and Zdenˇek Žabokrtský. 2020. Transform-\ning machine translation: a deep learning system\nreaches news translation quality comparable to\nhuman professionals. Nature Communications,\n11(1):4381.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason\nBolton, and Christopher D. Manning. 2020.\nStanza: A Python natural language processing\ntoolkit for many human languages. In Proceed-\nings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics: System\nDemonstrations.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen.\n2019. Studying the inductive biases of RNNs\nwith synthetic variations of natural languages.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Pa-\npers), pages 3532–3542, Minneapolis, Min-\nnesota. Association for Computational Linguis-\ntics.\nBenoît Sagot. 2013. Comparing Complexity Mea-\nsures.\nIn Computational approaches to mor-\nphological complexity, Paris, France. Surrey\nMorphology Group.\nKarthik Abinav Sankararaman, Soham De, Zheng\nXu, W. Ronny Huang, and Tom Goldstein.\n2020.\nAnalyzing the effect of neural net-\nwork architecture on training performance. In\nProceedings of Machine Learning and Systems\n2020, pages 9834–9845.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural machine translation of rare\nwords with subword units. In Proceedings of\nthe 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Pa-\npers), pages 1715–1725, Berlin, Germany. As-\nsociation for Computational Linguistics.\nKaius Sinnemäki. 2008. Complexity trade-offs in\ncore argument marking. In Language Complex-\nity, pages 67–88. John Benjamins.\nDan I. Slobin. 1966. The acquisition of Russian as\na native language. The genesis of language: A\npsycholinguistic approach, pages 129–148.\nDan I. Slobin and Thomas G. Bever. 1982.\nChildren use canonical sentence schemas: A\ncrosslinguistic study of word order and inﬂec-\ntions. Cognition, 12(3):229–265.\nKe Tran, Arianna Bisazza, and Christof Monz.\n2018.\nThe importance of being recurrent for\nmodeling hierarchical structure. In Proceedings\nof the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4731–\n4736, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need.\nIn Advances in Neu-\nral Information Processing Systems 30: An-\nnual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017,\nLong Beach, CA, USA, pages 5998–6008.\nDingquan Wang and Jason Eisner. 2016.\nThe\ngalactic dependencies treebanks: Getting more\ndata by synthesizing new languages. Transac-\ntions of the Association for Computational Lin-\nguistics, 4:491–505.\nAdina Williams, Tiago Pimentel, Hagen Blix,\nArya D. McCarthy, Eleanor Chodroff, and Ryan\nCotterell. 2020.\nPredicting declension class\nfrom form and meaning.\nIn Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6682–6695,\nOnline. Association for Computational Linguis-\ntics.\nYang Zhao, Jiajun Zhang, and Chengqing Zong.\n2018. Exploiting pre-ordering for neural ma-\nchine translation.\nIn Proceedings of the\nEleventh International Conference on Lan-\nguage Resources and Evaluation (LREC-2018).\nA\nAppendices\nA.1\nNMT Hyperparameters\nIn the toy parallel grammar experiments (§4),\nbatch size of 64 (sentences) and 1K max update\nsteps are used for all models. We train BiLSTM\nwith learning rate 1, and Transformer with learn-\ning rate of 2 together with 40 warm-up steps by\nusing noam learning rate decay. Dropout ratio of\n0.3 and 0.1 are used in BiLSTM and Transformer\nmodels respectively. In the synthetic English vari-\nants experiments (§5), we set a constant learning\nrate of 0.001 for BiLSTM. We also increased batch\nsize to 128, number of warm-up steps to 80K and\nupdate steps to 2M for all models. Finally, for\n100k and 10k datasize experiments, we decreased\nthe warm-up steps to 4K. During evaluation we\nchose the best performing model on validation set.\nA.2\nChallenge Set\nThe English-French challenge set used in this pa-\nper, and available at https://github.com/\narianna-bis/freeorder-mt, is generated\nby a small synchronous context-free grammar and\ncontains 7,200 simple sentences consisting of a\nsubject, a transitive verb, and an object (see Ta-\nble 4). All sentences are in the present tense; half\nare afﬁrmative, and half negative. All nouns in\nthe grammar can plausibly act as both subject and\nobject of the verbs, so that an MT system must\nrely on sentence structure to get perfect translation\naccuracy. The sentences are from a general do-\nmain, but we speciﬁcally choose nouns and verbs\nwith little translation ambiguity that are well rep-\nresented in the Europarl corpus: most have thou-\nsands of occurrences, while the rarest word has\nabout 80. Sentence example (English side): ‘The\nteacher does not respect the student.’ and its re-\nverse: ‘The student does not respect the teacher.’\nNouns\nVerbs\npresident / président\nthank / remercier\nman / homme\nsupport / soutenir\nwoman / femme\nrepresent / représenter\nminister / ministre\ndefend / défendre\ncandidate / candidat\nwelcome / saluer\nsecretary / secrétaire\ninvite / inviter\ncommissioner / commissaire\nattack / attaquer\nchild / enfant\nrespect / respecter\nteacher / enseignant\nreplace / remplacer\nstudent / étudiant\nexploit / exploiter\nTable 4: The English/French vocabulary used to gen-\nerate the challenge set. Both singular and plural forms\nare used for each noun.\nA.3\nMorphological Paradigms\nThe complete list of morphological paradigms\nused in this work is shown in Table 5.\nThe\nimplicit language with exponence (many:1) uses\nonly the sufﬁxes of the 1st (default) declension.\nThe implicit language with exponence and ﬂexiv-\nity (many:many) uses three declensions, assigned\nas follows: First, the list of lemmas extracted\nfrom the training set is randomly split into three\nclasses,13 with distribution 1st:60%, 2nd:30%,\n3rd:10%. Then, each core verb argument occur-\nring in the corpus is marked with the sufﬁx corre-\nsponding to its lemma’s declension.\nOvert\nImplicit\n1st(default) 2nd\n3rd\nUnambiguous\n.nsubj.sg\nkar\npar\npa\n.nsubj.pl\nkon\npon\npo\n.dobj.sg\nkin\nit\nkit\n.dobj.pl\nker\net\nket\n.iobj.sg\nken\nkez\nke\n.iobj.pl\nkre\nkr\nre\nSyncretic\n.arg.sg\n–\n–\n–\n.arg.pl\n–\n–\n–\nTable 5: The artiﬁcial morphological paradigms used in\nthis work, extended from (Ravfogel et al., 2019). 1st,\n2nd and 3rd are the declensions in the ﬂexive language.\n13See (Williams et al., 2020) for an interesting account of\nhow declension classes are actually partly predictable from\nform and meaning.\nA.4\nEffect of Data Size and Morphological\nFeatures: Detailed Results\nTable 6 shows the detailed numerical results corre-\nsponding to the plots of Figure 2 in the main text.\nEparl-BLEU\n1.9M\n100k\n10k\noriginal\n38.3\n26.9\n11.0\nSOV\n37.9\n25.3\n8.8\nSOV+overt\n37.4\n24.6\n8.4\nrandom\n37.5\n24.6\n8.5\nrandom+overt\n37.3\n24.1\n7.8\nrandom+implicit\n37.3\n24.3\n7.1\nrandom+declens\n37.4\n23.1\n7.7\nEparl-RIBES\n1.9M\n100k\n10k\noriginal\n84.9\n80.1\n67.5\nSOV\n84.5\n78.7\n64.1\nSOV+overt\n84.5\n78.4\n63.1\nrandom\n84.2\n77.7\n61.7\nrandom+overt\n84.4\n77.6\n61.6\nrandom+implicit\n84.3\n77.4\n59.8\nrandom+declens\n84.3\n77.1\n61.3\nChallenge-RIBES\n1.9M\n100k\n10k\noriginal\n97.7\n92.2\n74.2\nSOV\n97.2\n89.8\n69.5\nSOV+overt\n97.7\n86.5\n64.9\nrandom\n74.1\n72.9\n63.1\nrandom+overt\n98.1\n84.5\n57.4\nrandom+implicit\n97.5\n85.4\n54.8\nrandom+declens\n97.6\n84.4\n53.1\nTable 6: Detailed results corresponding to the plots\nof Figure 2: EN*-FR Transformer NMT quality ver-\nsus training data size (1.9M, 100K, or 10K sentence\npairs). Source language variants: Fixed-order (SOV)\nand free-order (random) with different case systems\n(+overt/implicit/declens). Scores averaged over three\ntraining runs.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-07-13",
  "updated": "2021-07-13"
}