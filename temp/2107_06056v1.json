{
  "id": "http://arxiv.org/abs/2107.06056v1",
  "title": "Indian Legal NLP Benchmarks : A Survey",
  "authors": [
    "Prathamesh Kalamkar",
    "Janani Venugopalan Ph. D.",
    "Vivek Raghavan Ph. D"
  ],
  "abstract": "Availability of challenging benchmarks is the key to advancement of AI in a\nspecific field.Since Legal Text is significantly different than normal English\ntext, there is a need to create separate Natural Language Processing benchmarks\nfor Indian Legal Text which are challenging and focus on tasks specific to\nLegal Systems. This will spur innovation in applications of Natural language\nProcessing for Indian Legal Text and will benefit AI community and Legal\nfraternity. We review the existing work in this area and propose ideas to\ncreate new benchmarks for Indian Legal Natural Language Processing.",
  "text": "Benchmarks for Indian Legal NLP: A Survey\nPrathamesh Kalamkar\nThoughtWorks\nprathamk@thoughtworks.com\nJoint ﬁrst author\nJanani Venugopalan\nThoughtWorks\njanani.venugopalan@thoughtworks.com\nJoint ﬁrst author\nVivek Raghavan\nvivek@ekstep.org\nThis work is funded by EkStep\nAbstract\nAvailability of challenging benchmarks is the\nkey to advancement of AI in a speciﬁc ﬁeld.\nSince Legal Text is signiﬁcantly different than\nnormal English text, there is a need to create\nseparate Natural Language Processing bench-\nmarks for Indian Legal Text which are chal-\nlenging and focus on tasks speciﬁc to Legal\nSystems. This will spur innovation in appli-\ncations of Natural language Processing for In-\ndian Legal Text and will beneﬁt AI community\nand Legal fraternity. We review the existing\nwork in this area and propose ideas to create\nnew benchmarks for Indian Legal Natural Lan-\nguage Processing.\n1\nWhat is an NLP Benchmark\nA machine learning (ML) or artiﬁcial intelligence\n(AI) pipeline typically consists of the following\nsteps: data collection, evaluation and testing, and\ndevelopment of the AI/ ML models. A good so-\nlution typically involves the development of high\nperforming models which are evaluated against a\npre-speciﬁed evaluation criteria. In the past, it has\nbeen observed that having speciﬁc and challeng-\ning unsolved tasks with clearly speciﬁed evaluation\ncriteria, has triggered a tremendous spurt of inno-\nvation. The exponential growth of community con-\ntributed solutions and innovation has been seen to\nadvance the ﬁelds of ML such as image processing,\nnatural language processing (NLP), and language\nmodels. A benchmark in these ﬁelds consists of\nspeciﬁc tasks, pre-speciﬁed evaluation criteria, and\na high-quality data set. In NLP, a benchmark is for\nevaluating the NLP models for a speciﬁc task or set\nof tasks. Some of the famous NLP benchmarks are\nStanford Question & Answer Dataset SQuad2.0\nby (Rajpurkar et al., 2018) , General Language\nUnderstanding GLUE by (Wang et al., 2019) and\nsuper GLUE by (Wang et al., 2020), NIST Open\nMT . An NLP benchmark would typically have\nthree main components; training data , testing data\nand Automatic Model Evaluation system. Training\ndata is used for training the NLP models and made\navailable to all participants.\nAutomatic Model Evaluation system is for scor-\ning a given NLP model using testing data which\nis typically kept hidden. Based on the scores of\nthe NLP models submitted by the ML community,\nleaderboards are typically created. Machine learn-\ning community competes on such benchmarks and\nthis creates a lot of innovative solutions for the\nproblem at hand. Creation of such benchmarks has\nproven to be the foundation of progress of NLP\nin an area. Machine Learning Researchers also\nwrite research papers describing their solution and\nmany a times open source the code (pap) for the\ndeveloped solution. Hence creating a challenging\nbenchmark with the right dataset can spur innova-\ntion in desired ﬁelds.\n2\nNeed For Indian Legal NLP\nBenchmarks\nWhile there are some limited ofﬁcial applications\nof AI and ML techniques in the Indian legal sys-\ntem (Off), there exists several avenues and sub-\nproblems where these techniques can be leveraged.\nSince the legal system generates huge amounts of\ntextual data, NLP techniques can be leveraged to\nimprove the efﬁciency of repetitive tasks. To spur\narXiv:2107.06056v1  [cs.CL]  13 Jul 2021\ninnovation in processing of the legal text corpora,\nwe identify some NLP benchmarks which focus on\nIndian Law systems. They are needed because of\nfollowing reasons\n1. Indian Legal Language is different from gen-\neral purpose English. Indian Law uses its\nown language which is esoteric and Latin Lan-\nguage based. Many of the terms used are pe-\nculiar and not used in general purpose English.\nHence Models pre-trained on general purpose\nEnglish (like Wikipedia) produce degraded\nperformance on Indian Legal tasks. Hence In-\ndian Legal Benchmark datasets would enable\nNLP models to learn the legal language.\n2. Translation of Relevant Legal tasks to NLP\ntasks. Application of NLP techniques to solve\nLegal tasks needs understanding of the NLP\nworld and Legal world. A team of Legal and\nNLP experts need to brainstorm and come\nup with relevant legal problems which can be\nsolved with NLP. A good benchmark would\nbe valuable for the legal community and at\nthe same time challenging for the Machine\nLearning Community.\n3\nHow can NLP Benchmarks Help\nInnovation in Indian Legal System?\nThere are currently no NLP benchmarks specif-\nically for Indian Legal system. There are legal\nNLP benchmarks for other countries (Zhong et al.,\n2020a) like China, US and other EU countries\n(Leg). Since India follows Common Law, many\nof these datasets from countries which follow civil\nlaw are not much useful in Indian Context. Also\nthe problems faced by Indian Judiciary are differ-\nent which are not captured by current legal NLP\ndatasets and problems. Hence deﬁning Indian Le-\ngal speciﬁc NLP benchmarks along with relevant\ndata will attract the ML community to solve such\nunique problems. NLP benchmark will also act as\nan open evaluation platform for commercial legal\nNLP solutions providers. Using this platform they\ncan prove the effectiveness of the products and so-\nlutions created by them. ML community, startups\nand researchers will beneﬁt from the knowledge\nsharing through research papers and open source\ncode thereby helping to solve complex problems in\nIndian Legal NLP space.\n4\nPublic Datasets in Indian Law\nTo be able to create Indian Legal NLP benchmarks,\na lot Indian law speciﬁc text data is needed. Using\nthis data , a lot of task speciﬁc datasets could be cre-\nated using human annotations. E.g. For generating\nFactoid Question and Answer benchmark dataset,\nhumans would annotate the questions based on the\ncourt judgement text. Using Indian law text en-\nsures that the NLP models learn the language used\nin Indian Law.\n4.1\nData Availability\nThanks to many open data initiatives like National\nJudicial Data Grid (NJG) and Crime and Criminal\nTracking Network and System (CCT), a lot of data\nrelated to law and crime is publicly available. Table\n1 shows the public data sources which can be used\nfor multiple benchmarks.\n4.2\nChallenges\nFollowing are some of the challenges in dealing\nwith the legal documents\n4.2.1\nList of judgements not available for all\nthe courts\nThe list of published judgements along with a link\nto the judgement is needed to be able to download\nthe judgement documents. This list is available\nonly for a few high courts. Many high courts give\nsuch a list for short periods of time like the past 1\nmonth. Not having such a list limits the amount of\ninformation available.\n4.2.2\nInconsistent document formats\nThe format of the judgements change with high\ncourts. Some judgments are scanned documents\nwhile some are pdfs with text. Scanned documents\nrequire additional processing to convert them into\nmachine readable formats.\n4.2.3\nChargesheet which contain the bulk of\nthe case information are not made\npublic\nAlthough the summary of the case is written in\nthe court judgement, most of the relevant informa-\ntion for the case is present in chargesheets. This\nlimits benchmarks that can be created with such\nlimited information. E.g. Timeline Summary cre-\nation needs events to be described by time in the\ntext. Such information is not present in typical\njudgement.\nName\nDescription\nCourt Judgements\nDistricts, High & Supreme Courts judgements are publicly available on their\nwebsites. These judgements are in English and contain rich text data for Indian Law.\nFIR by Police\nPolice Departments of many states provide FIRs on their websites.\nSome of them are in local Indian Languages.\nIndia Code\nDigital Repository of All central and state acts\nTable 1: Publicly Available Sources of Data. Released through Government of India platforms.\n4.2.4\nFIRs are not always updated and often\ncontain a mixture of languages.\nFIRs published by police provide rich but limited\ninformation about the case. Publicly available FIRs\ndo not have updated information after further inves-\ntigation are done.\n5\nIndian Legal NLP Benchmark Areas\nMind map Figure 1 shows the NLP areas and NLP\nbenchmarks in each of the areas. 4 main NLP areas\nhave been identiﬁed and are marked in green color\nin the mind map below.\n5.1\nInformation Extraction\nInformation Extraction in the area of Natural Lan-\nguage Processing deals with extracting the informa-\ntion of interest from text. Following benchmarks\nidentify speciﬁc tasks within Information Retrieval\nwhich are useful for Indian Legal tasks.\n5.1.1\nFactoid Question & Answers\n• Description of Task\nJudges and lawyers are looking for certain in-\nformation based on the type of case. This can be\nframed as some template questions that need to\nbe answered from the case text. Along with the\ntemplate questions, there would be some context\nspeciﬁc questions.. E.g. For a criminal case, tem-\nplate questions could be “What was the place of\noccurrence?” , “What was the time of occurrence?”\n, “Is there any eye-witness?” , “What are Num-\nber of Heads and Sections under which charges\nwere framed?” etc. If the case is murder case then\nalong with the template questions the context spe-\nciﬁc questions could be “What was the murder\nweapon?”, “What is Nature of injuries mentioned\nin the Post-mortem Report?” etc. So this can be\ntreated as factoid questions i.e. you are looking\nfor answers which are written explicitly in the text\nas a span of few contiguous words. Factoid Q/A\nsystem will ﬁnd such answers written in text and\nhighlight the answers in the text. E.g. Question of\ninterest is “What is the place of occurence?” The\nQ/A system searches all the documents and ﬁnds\nrelevant passages that have answers to this question\nand highlight it like in the text below. “... e of\nprosecution alleged incident took place at the tea\nstall situated near Bombay Central Bus Stand at\naround 7.30 a.m. and as such there is every... ....”\n• Value Proposition\nThis automatic extraction of answers to such\nquestions would signiﬁcantly save the human read-\ning time. Having a challenging benchmark for\nFactoid Q/A would attract ML practitioners to fo-\ncus on this problem. This will create a foundation\nfor innovative approaches and open source models\nfocussing speciﬁcally on Indian Legal factoid Q/A.\n• Mapping to standard NLP Tasks\nThis benchmark maps to a NLP task called fac-\ntoid Q/A. This is a widely studied task in NLP and\nmany pretrained NLP models are available which\nfocus on general purpose English language. Given\navailability of data and many published approaches,\nthis NLP task falls into the easy category.\n• Similar NLP benchmarks\nSimilar benchmarks exist for open domain tasks\nlike general wikipedia factoid Question & answers\n(Squad2.0 (Rajpurkar et al., 2016), (Rajpurkar\net al., 2018), ComQA (Abujabal et al., 2019),\nGoogle Natural Questions (Kwiatkowski et al.,\n2019) etc.) But these datasets lack the domain\nspeciﬁc legal language which limits use of such\nbenchmarks. There is also a reasoning based legal\ndataset (JEC-QA (Zhong et al., 2020b)) which in-\nvolves complex questions based on the questions\nfrom the National Judicial Examination of China.\nThese questions need prior knowledge of law and\nreasoning. Hence such datasets from civil law coun-\ntries and using general English text are not relevant\nin the context of Indian Legal domain. Hence a\nFigure 1: Indian Legal Bench Marks: An Overview\nfactoid question and answer dataset using Indian\nLegal text would ensure that the solutions devel-\noped work well with Indian Legal Language.\n• Dataset to be collected\nData needed for this benchmark would be tem-\nplate questions about a given legal text and humans\nwould mark the answers in that text. The person\nwill also create the context speciﬁc questions which\ncould be generated by him/her or taken from the au-\ntomatically generated context speciﬁc questions (as\nmentioned in the next question generation bench-\nmark). For creation of such human annotated data ,\nexisting tools like haystack (hay) or cdQA (CDQ)\ncould be used with some changes needed. The hu-\nman annotations could be done by a literate person\nwith basic reading comprehension skills and some\ntraining of the legal language.\n• Evaluation Metrics\nThe evaluation of this benchmark would be done\nby matching the answers generated by NLP models\nwith human extracted answers. Evaluation Metric\ncould be F1 score using exact match.\n5.1.2\nQuestion Generation\n• Description of Task\nIn order to get meaningful insights from answers\nof questions, it is important to ask the right ques-\ntions which are context speciﬁc. These questions\ncan be of two types: factoid and non-factoid or\nreasoning based questions. Factoid questions are\nthe questions for which the answers are explicitly\nstated in text as facts of contiguous text. E.g. Con-\ntext: \"... companion were thieves and therefore\ntook away driving licence of Mohammad Nisar\nKhan along with some visiting cards and coins.\nAt the same time took ...\" From this context the\ngenerated factoid Question would be “What is the\nname of the victim?” Reasoning based questions\nare those for which the answers have to be created\nusing information written in text along with using\nsome logic. E.g. Context: \"... companion were\nthieves and therefore took away the driving licence\nof Mohammad Nisar Khan along with some vis-\niting cards and coins. At the same time took ...\".\nBased on this context, a reasoning based question\nwould be , “Did Mohammad Nissar Khan contra-\ndict himself when he gave evidence between the\ndistrict and the high court?” Generation of reason-\ning based questions and answering is more difﬁ-\ncult than factoid based questions. Hence it would\nbe useful to start with factoid based questions and\nthen move to reasoning based questions. Recent ad-\nvancements in Neural question generation allow us\nto create answer-aware questions i.e. given a con-\ntext and an answer , the question can be generated.\nSo one can create a lot of questions from a context\nusing different answers. Hence it is important to\nidentify which information in the context is impor-\ntant and generate correct questions which would\nlead to those answers. Hence the candidate model\nshould submit the question answer pairs for a given\njudgement. These QA pairs would be compared\nwith the Human generated QA pairs to evaluate\nhow good the question generation is.\n• Value Proposition\nFor each case it is very imperative that the right\nquestions are asked. Sometimes, even the outcome\nof the case is dependent on the type of questions\nasked. This question generation is a very time-\nconsuming process and often needs the documents\nto be perused several times. In this task, we propose\nthe use of NLP to automate the bulk of question\ngeneration, allowing lawyers and others in the legal\nsystem to focus on the game-changing questions.\nIn addition, context speciﬁc question generation\nalso helps in collecting human annotated data for\nthe creation of the NLP benchmarks which can be\nby the technology community In many benchmark\ndatasets like Squad (Rajpurkar et al., 2016), (Ra-\njpurkar et al., 2018), the questions are generated\nby humans by looking at the text and answers are\nmarked in that text. A lot of time would be saved if\nthe questions are also generated automatically and\nhumans just have to mark the answers in the text.\n• Mapping to standard NLP Tasks\nThis benchmark maps to a NLP task called ques-\ntion generation and language generation.With re-\ncent advancements in question answering, question\ngeneration, there are several studies which focus\non both factoid and reasoning based question gen-\neration. The NLP models for this are also focused\non general purpose English language. Given avail-\nability of data and some published approaches, this\nNLP task falls into the moderate category. The\ndifferent approaches can be summarized in Figure\n2\nFigure 2: Question generation using state of the art techniques.\n• Similar NLP benchmarks\nSimilar benchmarks exist for selection speciﬁc\nquestion generation benchmarks such as Sel-QA\n(Jurczyk et al., 2016), Wiki-QA (Yang et al., 2015),\nand GlGE (Liu et al., 2020).\nThey are taken\nfrom a curated list of topics from corpora such as\nwikipedia. They are essentially question-answering\nbenchmarks where the questions are generated\nbased on a selected text, and cover various ver-\nsions of the questions through human annotations.\nThe questions generated are essentially evaluated\nfor coverage of the text and linguistic correctness.\nBut these datasets lack the domain speciﬁc legal\nlanguage which limits use of such benchmarks.\nThere is also a reasoning based legal dataset (JEC-\nQA (Zhong et al., 2020b)) which involves complex\nquestions based on the questions from the National\nJudicial Examination of China. However, this is a\nquestion answering benchmark and not a question\ngeneration one. Hence a generation dataset using\nIndian Legal text would ensure that the solutions\ndeveloped work well with Indian Legal Language\nand are speciﬁc to this task.\n• Dataset to be collected\nThe data for this task could be collected from\njudgments, FIRs, chargesheets. Of these sources,\nthe judgments from district-courts, high-courts, and\nthe supreme-courts of India are available for use\nin such tasks. The humans will mark the QA pairs\ngenerated by a baseline model as valid, and also\ngenerate additional QA pairs for the same selected\npiece of text to get as many variations as possi-\nble. This is similar to methods employed in other\nquestion generation benchmarks. We need only\npeople who are proﬁcient in the language, not nec-\nessarily legal experts, for creating the data in this\nbenchmark. We can leverage existing tools such as\nProphetNET (Qi et al., 2021), T5 (Kumar, 2021)\nand Squash (Krishna and Iyyer, 2019) ﬁne-tune\nthem if needed for Legal question generation.\n• Evaluation Metrics\nThere are two main aspects of the evaluation of\nthis benchmark. First one is whether the QG pro-\ncess has identiﬁed the right answers from context\nto create questions (called as coverage). The sec-\nond is to check the quality of question text. For\ncoverage of the answers, the generated answers\nwould be compared with Human answers to cal-\nculate the overlap. The Quality of the questions\ncould be checked by checking the grammar of the\nquestions, passing the question through the stan-\ndard question answering model to check whether\nthe answer matches with the given answer.\n5.2\nSummary Generation\n5.2.1\nJudgement Summarization\n• Description of Task\nThe court judgements, especially from high\ncourts and supreme court, tend to be very long.\nFinding the right context from such long judge-\nment is time consuming and error prone. Supreme\nCourt judgements from 1950 to 1994 used to have\nsummaries created by lawyers while publishing.\nBut after that the summaries are not present in the\nsupreme court judgement texts. This benchmark\nwould focus on evaluating various aspects of the\nsummaries created for a given judgement.\n• Value Proposition\nThe summary of high courts and supreme court\nis often used in establishing precedent. Searching\nin summaries rather than entire text would help\nlawyers to establish better arguments for prece-\ndence. Summary of judgements from lower courts\nalso help in reducing processing time when case\nmoves to higher court.\n• Mapping to standard NLP Tasks\nSummarization of texts is a standard NLP task.\nThere are two types of summaries; extractive and\nabstractive. Extractive summaries focus on extract-\ning important complete sentences from text to form\nthe summary. Abstractive summaries on the other\nhand create summaries by constructing new sen-\ntences which effectively summarizes the text.\n• Similar NLP benchmarks\nWhile there are no summarization benchmarks\nthat focus on the legal documents, there are many\nbenchmarks that focus on summarizing general\nEnglish text like Wikipedia articles, news articles\n(Narayan et al., 2018), CNN/dailynews (Nallapati\net al., 2016), research(ArXiv) articles (Leskovec\net al., 2005), (Gehrke et al., 2003), pubmed articles\n(Sen et al., 2008). Similarly, the current state of\nthe art research focuses on comparing the perfor-\nmance of various algorithms on Indian Judgements\n(Bhattacharya et al., 2019b).\n• Dataset to be collected\nSince the summaries created by lawyers of the\nsupreme court for judgments from 1950 to 1994\nare available in the form of headnotes, this data\ncould be utilized to create the benchmark.\n• Evaluation Metrics\nThe automatic evaluation of summaries created\nis a complex topic. Recent research (Fabbri et al.,\n2021) proposes that there are 4 main aspects of\nsummary evaluation: Coherence, Consistency , Flu-\nency & Relevance. Many of the existing evaluation\nmetrics like ROUGE, METEOR etc. depend on\navailability of the human generated summary to\ncompare against. But these metrics do not cap-\nture the Faithfulness and factuality (Maynez et al.,\n2020) of the generated summaries. The proposed\nways to capture Hallucinations are computationally\nexpensive. Hence there is a need to formulate better\nevaluation metrics which can measure faithfulness\nand factuality efﬁciently.\n5.2.2\nTimeline Extraction\n• Description of Task\nIn this use-case scenario we propose the use of\nNLP to extract timelines from these case docu-\nments (Figure 3). In this white paper, we propose a\ntimeline extraction benchmark to be of use to both\nthe legal informatics community and the machine\nlearning (ML) community.\n• Value Proposition\nTypically a judgment in the Supreme Court of\nIndia is several pages long and has information per-\ntaining to different aspects of the case including\nprevious judgements, witness accounts, references,\narguments and counterarguments. In addition to\nthese, the cases may have one or more of the fol-\nlowing; chargesheet, FIRs, lower court judgements\nand orders, district, high court and appellate court\njudgements and orders. As a result, depending on\nthe type of case, each case may have hundreds or\neven thousands of pages worth documentation. The\nlegal representation and decision making process\nfor each court case is also handled by multiple peo-\nple. The fact that the cases are often handled by\nmultiple lawyers and judges, extracting the facts\nof the case accordings to events which occurred in\ntime can get very challenging. With witness state-\nments, and arguments being recorded several times,\nthis can get even more challenging.\n• Mapping to standard NLP Tasks\nThis benchmark maps to a NLP task called tem-\nporal event extraction. There is a very nascent\nand growing ﬁeld in NLP with a few seminal stud-\nies. The NLP models for this are also focused on\ngeneral purpose English language. The different\napproaches can be summarized in Figure 4. To cre-\nate targeted models which go on the leaderboard\nFigure 3: Context timeline extractor system which takes the case text (e.g. judgement, FIRs, chargesheet etc.) to\nget a timeline of events.\nFigure 4: Timeline generation using state of the art techniques.\nfor this benchmark, one can leverage existing time-\nline extraction techniques including event extrac-\ntion and temporal sequence techniques (Piskorski\net al., 2020), (Ning et al., 2018), (Chieu and Lee,\n2004), (Finlaysona et al.), (Yu et al., 2020) Figure\n4. Given availability of data and some published\napproaches, this NLP task falls into the moderate-\ndifﬁcult category.\n• Similar NLP benchmarks\nDespite the existence of a few companies (Eve),\n(Pri), there exist very few open event, timeline ex-\ntraction datasets and benchmarks in the ML com-\nmunity e.g.[CoNLL2012 (Pradhan et al., 2012), Se-\nmEval2015 (Minard et al., 2015)]. A lot of works\nrely on text such as WikiPedia and news articles.\nA major challenge with this is that there exist very\nfew temporal relationships in these datasets or the\nstudies only extract dated events. In this whitepa-\nper, we propose a timeline extraction benchmark\nto ﬁll these gaps and also to provide a brand new\nlegal timeline extraction benchmark.\n• Dataset to be collected\nThe data for this task could be collected from\njudgements, FIRs, chargesheets. Of these sources,\nthe judgements from district-courts, high-courts,\nand the supreme-courts of India are available for\nuse in such tasks. The training/ test data we propose\nare human extracted events ordered in time. We\nneed only people who are proﬁcient in the language,\nnot necessarily legal experts, for creating the data\nin this benchmark. To assist in the creation of\nthe training and test data, we will create an initial\nbaseline model for extracting timelines and events\nfrom a selected text. The annotator has to add\nmissing information and reorder timelines where\nnecessary.\n• Evaluation Metrics\nThe automatically generated benchmarks are\nevaluated against the expert created ones using ac-\ncuracy and degree of overlap metrics.\n5.2.3\nContext speciﬁc Summary\n• Description of Task\nContext speciﬁc summary systems take the case\ntext (e.g. judgement, FIRs, chargesheet etc.) and\nthe focus area (e.g. court judgements) to get a sum-\nmary speciﬁc to the given focus area. An example\nof a context speciﬁc summary is shown in ﬁgure 5.\n• Value Proposition\nAs mentioned above, the case documents can run\nto a few hundred or even thousands of pages, with\nmultiple legal representation at each stage. It will\nbe very useful to the legal community, if they had\naccess to context speciﬁc summaries e.g. summary\nof the statements given by primary witnesses across\nthe different courts. It will be a very time-saving\ncomponent for the legal system.\n• Mapping to standard NLP Tasks\nThis benchmark maps to summarization and\nas an extension, entity speciﬁc summarization in\nNLP. Currently, the state of the art ML summa-\nrization focuses on single (Haque et al., 2013) and\nmulti-document summaries (Batra et al., 2020) for\nthe entire text. The different types of solutions\ninclude, concept map based benchmarks (Falke\nand Gurevych, 2017), machine-generated and hu-\nman generated text based solutions.\nSimilarly,\nthe state of the art legal benchmarks contain sum-\nmary generation tools trained on legal documents\n(Farzindar and Lapalme, 2004a), (Kanapala et al.,\n2019), (Farzindar and Lapalme, 2004b) including\nthe Indian Legal Documents (Merchant and Pande,\n2018), (Bhattacharya et al., 2019b), which they\ntested using human generated summaries (Wes).\nIn addition to these there are text summarization\ntracks in NLP conferences such as Text REtrieval\nConference (Tex), Text Analysis Conference (Tex),\nand Forum for Information Retrieval Evaluation\n(FIR). To create targeted models which go on the\nleaderboard for this benchmark, one can leverage\nexisting summary generation models such as con-\ntext speciﬁc sentence extraction and text categoriza-\ntion and subsequent summary generation methods.\nSome examples of the said methods are shown in\nﬁgure 6.\n• Similar NLP benchmarks\nIn this white paper, we propose a context spe-\nciﬁc summarization benchmark to be of use to both\nthe legal informatics community and the machine\nlearning (ML) community. However, to the best\nof our knowledge, there exists no ML and legal\nbenchmarks on context/ actor speciﬁc summary\ngeneration.\n• Dataset to be collected\nIn our benchmark, we propose the use of open\ndocuments from the Indian Legal System (e.g. case\nFigure 5: Context speciﬁc summary system takes the case text (e.g. judgement, FIRs, chargesheet etc.) and the\nfocus area (e.g. court judgements) to get a summary speciﬁc to the focus area\nFigure 6: Context speciﬁc summary generation using current summarization techniques\nFigure 7: Context speciﬁc summary evaluation: Metrics can be applied either on the input documents or expert\ngenerated summaries. Figure shows the evaluation metrics which can be applied to either of the two scenarios\njudgements) as the dataset. The training/ test data\nhere can be either the full-text and the human la-\nbelled summaries. The focus areas for this bench-\nmark will be limited to the actors identiﬁed in\neach case using named entity recognition (relation-\nship extraction) and a few expert curated focus\nareas such as judgments, citations, primary witness\nstatements, introduction, context, conclusion, ar-\nguments, counter-arguments etc. If we use expert\ncreated summaries , then we need legal experts to\ncreate the summaries. However, it is perfectly valid\nin this benchmark to not use expert summaries but\nvalidate against input data.\n• Evaluation Metrics\nThe evaluation metrics (Figure below) can be\ndone with respect to the input documents or with\nexpert generated benchmarks. When we compare\nwith the input documents, we can rely on evaluation\nmetrics which can get the similarity between se-\nquences of different lengths (e.g. Kullback Liebler\ndivergence, Jensen Shannon divergence, cosine\nsimilarity). When we use expert generated sum-\nmaries for the evaluation, we can rely on the met-\nrics which also account for the summary lengths in\naddition to similarity (e.g. precision, ROUGE-N,\nRecall, F measure, embedding based metrics).\n5.2.4\nRelationship Extraction\n• Description of Task\nThe legal text describes multiple relations be-\ntween the entities involved. These entities could\nbe person , place , acts , court , judges etc. The\nextracted textual relation can be categorized into\npredeﬁned relation categories. This allows us to ﬁl-\nter for interesting relations. The extracted relations\ncan be visualized in the form of a graph. Indi-\nrect relations can be inferred using this graph. E.g.\nCase Judgement Text says ,“Complainer: Ashok\nSethi ... Accused: Pranav Jog ... ....Compainer\n& accused live in the same society...Therefore, ac-\ncused assaulted complainer with rod....”. Following\nrelations could be found out from this text. Rela-\ntionship extraction can be treated as an unsuper-\nvised task where the entities , relations are text.\nBut categorization of extracted relationship text\ninto meaningful categories needs machine learning\nmodel training. E.g. mapping of “live in same\nsociety” to “is neighbor”.\n• Value Proposition\nThis task would provide a quick Summary of\nrelation between entities in text. Indirect relations\ncan easily be inferred using graphs. The relations\nwould be ones that are important from a legal per-\nspective. Having a challenging benchmark for In-\ndian Legal Relation Extraction would attract ML\npractitioners to focus on this problem. This will\ncreate a foundation for innovative approaches and\nopen source models focussing speciﬁcally on In-\ndian Legal Relation Extraction.\n• Mapping to standard NLP Tasks\nThis benchmark maps to Knowledge Base Popu-\nlation (KBP) , Open Information Extraction(Ope).\nBoth of the NLP tasks are harder to evaluate and\nhence tricky to improve on. Most of the research fo-\ncuses on open domain text and using ontologies to\nget domain speciﬁc relations. Many of the existing\nNLP components like Legal Named Entity Rela-\ntion Extractor (Bla) , open information extraction\n(Ope) can be leveraged for this.\n• Similar NLP benchmarks\nThere are many existing benchmarks which deal\nwith open domain relation extraction from text doc-\numents like wikipedia or news articles ((Yao et al.,\n2019), (Zhang et al., 2017), (Riedel et al., 2010)\netc.). But none of them focus on relations that\nare interesting in legal documents like a person is\na complainer or accused, a person is a judge etc.\nCreating Indian legal domain speciﬁc relations and\nbenchmarks would trigger innovation in solutions\nthat perform better on relations in Indian Legal Text\n.\n• Dataset to be collected\nThe data needed for this benchmark is Human\nextracted relations in given text. Humans would\nbe presented with text and categorized relations.\nHumans would then extract the relations between\nthe entities as written in text and map them to the\ncategorized relations. Skill Sets needed for human\nannotations would be understanding the basics of\nLegal language and relations. These could be eas-\nily picked by a layman literate person with basic\ntraining. A custom annotator tool needs to be built\nto allow such human annotations.\n• Evaluation Metrics\nThe evaluation of this benchmark could be done\nbased on matching the relations extracted by candi-\ndate NLP model with the human extracted relations.\nEntity1\nRelation\nEntity2\nAshok Sethi\nis\nComplainer\nPranav Jog\nis\nAccussed\nComplainer\nis neighbor\nAccussed\nComplainer\nassaulted\nAccussed\nTable 2: Example of Extracted Relations\nNLP Models should be able to correctly identify\nmost of the relations and correctly categorize those.\n5.3\nCase Similarity\nIndian Law is common law. This means that prece-\ndent set by previous courts to interpret laws must\nbe respected. Hence ﬁnding out cases which are\nsimilar to a given case becomes important for le-\ngal research. This activity is manual and a lot of\ncommercial tools use keyword based or act based\nsimilarities. But to be able to search the similar\njudgements based on longer text like case facts,\nFIR etc. would make the search more relevant and\nautomated. Following benchmarks focus on these\ntasks.\n5.3.1\nFind Similar Cases\n• Description of Task\nThis benchmark deals with ﬁnding most rele-\nvant cases for given text descriptions about a case.\nMany existing search engines of Indian Legal doc-\numents (India Kanoon , Legitquest, aironline etc.)\nfocus on searches based on keywords. Searching\nwith text descriptions allows you to look for more\ndetailed information rather than keywords based\nsearch. Hence one can input the FIR, chargesheets\n, case description and search for the judgements\nwhich are similar. E.g. Case description says ,”Re-\npublic Editor Arnab Goswami was arrested in crim-\ninal case of suicide. His claims that he was targeted\nby the state govt and his personal liberty was vi-\nolated”. Then similar cases might be judgements\nthat interpret personal liberty like Maneka Gandhi\nv. Union of India and Another (1978) , Kharak\nSingh v. State of U.P. and Others etc. The sim-\nilar cases could come from other high courts or\nsupreme courts.\n• Value Proposition\nReduce time needed for manual search of similar\ncases and provide more meaningful results. Pro-\nvision of search by larger text instead of just key-\nwords would be very helpful. Many startups are\ndeveloping such search engines as well. But there\nis no benchmark about how good the search en-\ngines are. Hence this benchmark would provide an\nobjective way of evaluating such products\n• Mapping to standard NLP Tasks\nThis maps to case2vec where the cases are\nmapped to vectors in latent space. This mapping\nof cases to latent vectors is learnt while training.\nThese types of tasks are harder to evaluate and\nhence hard to improve on.\n• Similar NLP benchmarks\nIn the Forum of Information Retrieval Evalua-\ntion 2019 (Bhattacharya et al., 2019a), there is a\ndataset created which identiﬁes the most similar\ncases for a given text description of a case from a\nset of 2914 supreme court judgements. This data is\navailable for 50 text description queries. Chinese\nAI Law competition (Xiao et al., 2019) has created\na similar case matching competition (CAIL 2019 -\nSCM). This competition focuses on ﬁnding which\n2 supreme court cases are similar in a triplet of\ncases. But the data is in Chinese Language. Other\nExisting benchmarks focus on ﬁnding similarity\nbetween sentences (Semantic Textual Similarity\nBenchmark and Microsoft Research Paraphrase\nCorpus). But these focus on open domain sen-\ntences similarity and not on similarity of the entire\ndocument. Competition on Legal Information Ex-\ntraction/Entailment (COL) focuses on extracting\nsupporting cases for a given new case using Canada\ncases. Hence there is a need for benchmark of In-\ndian Legal cases similarity.\n• Dataset to be collected\nThe data needed for this benchmark is past judge-\nments and metadata about the case like acts etc.\nUsing this data, triplets of cases (A,B,C) would be\nmade. Legal Experts would tag if case A is more\nsimilar to B or C. Such triplets need to be created\ncarefully so that cases in a triplet share some com-\nmon factors and are not random.This is because it is\nmuch easier to distinguish between two completely\ndifferent cases like land case vs. murder case than\nto ﬁnd similarity of one murder case to other 2\nmurder cases. A custom annotator tool needs to\nbe built to allow such human annotations. Legal\nexperts are needed for the manual tagging of such\ntriplets and multiple such opinions would be taken\nfor a record. Consensus of experts would be used\nas ﬁnal data.\n• Evaluation Metrics\nThis task can be evaluated by comparing model\npredictions with the consensus of the legal experts.\nThe score could be the accuracy of the predictions.\n5.3.2\nFind supporting paragraph for new\njudgement from an existing Relevant\nJudgement\n• Description of Task\nMany times while writing judgements , a judge\nwould take reference of an old judgement which\nhas interpreted law in detail. This is a crucial step\nto make sure case precedent is followed. A Judge\nmay already know such existing relevant judge-\nments or use the search system explained in the\nprevious benchmark. Since the judgment is similar,\nthere could be multiple paragraphs that support the\nnew decision. So ﬁnding the exact paragraph which\nsupports the new judgement can be time consum-\ning. E.g. A review case of Visa rejection comes\nto a judge where the appellant says that the visa\nwas rejected without an interview by the Visa ofﬁ-\ncer based on information collected by other people.\nJudge is writing a judgement where he wants to\nwrite “in matters of administrative decisions, the\nrule of “he who hears must decide” does not apply”\n. Judge has found an existing relevant judgement\nbased on similarity search. Now the judge wants\nthe exact paragraph from the existing judgement\nwhich interprets this. So he searches using query\nas “in matters of administrative decisions, the rule\nof “he who hears must decide” does not apply” and\ngives an existing document. The system returns\nthe paragraph from existing judgement which inter-\npretes this law in detail. “The decision is essentially\nan administrative one, made in the exercise of dis-\ncretion by the visa ofﬁcer. There is no requirement\nin the circumstances of this or any other case that\nhe personally interview a visa applicant. There may\nbe circumstances where failure to do so could con-\nstitute unfairness, but I am not persuaded that is the\ncase here. Here the IPO did interview the applicant\nand reported on the results of that interview. That\nreport was considered by the visa ofﬁcer who made\nthe decision. Staff processing and reporting on ap-\nplications is a normal part of many administrative\nprocesses and it is not surprising it was here that\nfollowed. This is not a circumstance of a judicial\nor quasi judicial decision by the visa ofﬁcer which\nwould attract the principle that he who hears must\ndecide, or the reverse that he who decides must\nhear the applicant.”\n• Value Proposition\nTime saved to ﬁnd supporting exact text from\nexisting judgement would enable legal stakeholders\nto process the decisions faster.\n• Mapping to standard NLP Tasks\nThis maps to paragraph2vec where similarity\nbetween new judgment text and paragraphs from a\ngiven judgement are found out. These type of tasks\nare hard to evaluate and hence hard to improve on.\n• Similar NLP benchmarks\nThere\nis\nan\nexisting\nsimilar\nbenchmark\nby Competition on Legal Information Extrac-\ntion/Entailment ((COL)) which focuses on extract-\ning an entailing paragraph from a relevant case for\na new judgement using Canada cases. Creating a\nsimilar benchmark for Indian Law would be more\nuseful.\n• Dataset to be collected\nData needed for this benchmark would be cre-\nated using Indian Courts judgements. Legal ex-\nperts would create a triplet (relevant case text , new\njudgement, paragraph id supporting new judge-\nment). Evaluation of this benchmark would be\ndone by matching the paragraph extracted by hu-\nmans to paragraphs extracted by model. A custom\nannotator tool needs to be built to allow such hu-\nman annotations. The annotator would show the\nnew judgement line and suggest paragraphs based\non NLP model output. The answer suggested can\nbe accepted by the expert or he can change it to an\nappropriate one. This suggestion would reduce the\nhuman processing time signiﬁcantly.\n• Evaluation Metrics\nThe evaluation of this benchmark can be done\nby comparing if the paragraph id predicted by the\nmodel matches the one provided by the expert. F1\nscore of the match can be used to rank the submis-\nsions.\n5.3.3\nInﬂuential Case Law Extraction\n• Description of Task\nSome of the judgements have far reaching impli-\ncations and are commonly cited in multiple judge-\nments. The idea of this benchmark is to objectively\nidentify such inﬂuential judgements for each of the\nLegal Topics. Some of such cases are mentioned in\nthe table 3. Many supreme court judgements cite\nother judgements for interpretation of laws. Such\ncitations can be extracted automatically with NLP\ntechniques and a network of such citations can be\ncreated. Using such a network, one can ﬁnd in-\nﬂuential judgements overall and for speciﬁc legal\ntopics.\n• Value Proposition\nThe benchmark would provide objective evalua-\ntion of inﬂuence of a judgement. This would also\nmean time saving for legal research.\n• Mapping to standard NLP Tasks\nThis maps to citations extraction and algorithms\nlike pagerank to decide inﬂuence once the network\nof citations is created. These NLP tasks are con-\nsidered easy because of availability of pretrained\nmodels and libraries.\n• Similar NLP benchmarks\nThere is an existing benchmark that focuses on\nprediction of case importance for European Human\nRights Cases (Chalkidis et al., 2019). To the best of\nour knowledge, there are no benchmarks that focus\non establishing inﬂuential judgments in speciﬁc\nareas of Indian Law.\n• Dataset to be collected\nData needed for such benchmark is supreme and\nhigh court judgements with citations. Legal experts\nwould be needed to create such lists. Opinions of\nmultiple legal experts need to be combined to cre-\nate the curated list in speciﬁc areas. Since data to\nbe collected involves ranked lists in a given area,\ncomplex tools may not be needed for data collec-\ntion. Experts can use simple tools like MS Excel to\ncreate such lists. Many of the existing NLP compo-\nnents like Legal Named Entity Relation Extractor\ncan be leveraged for creating structured data for\nhuman annotations.\n• Evaluation Metrics\nEvaluation of this benchmark could be done in a\nsimilar way like case similarity benchmark above\nby matching the model created ranked list of expert\ncreated list\n5.4\nReasoning Based NLP\n5.4.1\nPredict Relevant Acts based on Case\nFacts\n• Description of Task\nPredicting the relevant act of the law based on\nthe text description of the fact is an important legal\nresearch task. This is done typically by lawyers\nand police while making the chargesheet. Automat-\ning this process can help layman people who don’t\nunderstand law. This would help people to collect\nthe right information about the case in a timely\nmanner before they interact with lawyers or police.\nThis will also increase familiarity of law in com-\nmon people. E.g. A citizen enters text “Thieves\ntook away my Rs. 10000 and Mobile last night....”\nthe NLP system would return “Section 378 under\nIndian Penal Code”. It can also return what are\nthe keywords in the input text description that trig-\ngered this prediction. In this case keywords could\nbe “took away”.\n• Value Proposition\nInformed with the right section of the law, citi-\nzens can make better decisions about documents to\nbe collected, lawyers to contact etc. This will also\nIncrease familiarity with law among citizens\n• Mapping to standard NLP Tasks\nThis maps to standard tasks of text classiﬁca-\ntion.The tasks of text classiﬁcation are well studied\nand hence this falls into the easy category.\n• Similar NLP benchmarks\nThere is a similar existing benchmark about pre-\ndicting which speciﬁc human rights articles and/or\nprotocols have been violated on European Human\nRights Cases (Chalkidis et al., 2019). Another simi-\nlar benchmark in Chinese language is Chinese crim-\ninal judgment prediction dataset, C-LJP (Xiao et al.,\nLegal Topic\nInﬂuential Judgement\nRight to personal liberty\nManeka Gandhi Vs. Union of India , 1978 , ...\nSupreme Court’s authority over the\nConstitution\nKesavananda Bharati vs state of Kerala , 1973,.....\nUniform Civil Code\nMohammed Ahmed Khan v. Shah Bano Begum , 1985,...\nTable 3: Examples of Inﬂuential Judgements\n2018) which is a part of Chinese AI Law Challenge.\nIn the Forum of Information Retrieval Evaluation\n2019 (Bhattacharya et al., 2019a), there is a dataset\ncreated which ﬁnds most relevant statutes for 50\ntext descriptions from 197 Sections of Acts. The\ndescription of these 197 sections is also provided.\n• Dataset to be collected\nData needed for this benchmark are FIRs,\nchargesheets and judgements which describe the\nstatements describing the incidence and applicable\nlaw sections. The labelled data could be created\nusing an unsupervised approach like using pattern\nmatching to extract acts and sections from text or\nusing pretrained models for such extraction. This\ndata can be used to train the NLP model and eval-\nuation. There is no human annotation needed for\nthis benchmark.\n• Evaluation Metrics\nEvaluation of this benchmark could be done by\nmeasuring the accuracy of predicted acts and sec-\ntions by comparing them with actual acts and sec-\ntion\n5.4.2\nStatement Contradiction Discovery\n• Description of Task\nIdentiﬁcation of contradictions in witnesses , ac-\ncused and victims statements has a lot of impact\non the verdict. These contradictions are found out\nby lawyers , judges and legal research teams. Au-\ntomatic identiﬁcation of such contradictions can\nsigniﬁcantly reduce the processing time of a case\nfor both lawyers and judges. To achieve this, the\nﬁrst step is to identify the statements by multiple\npeople about the same topic. The topic could be\nspeciﬁc to cases like arrival time of police, obser-\nvations about incidence etc. Then these statements\ncan be compared with each other to ﬁnd out poten-\ntial contradictions. Example of this is as shown in\ntable 4 These contradictions by the NLP model can\nbe validated by humans to accept or reject them.\nThis feedback about acceptance or rejection can be\nused to improve the model.\n• Value Proposition\nThis task will greatly reduce the time needed for\nidentiﬁcation of contradictions in case documents\nwhich is an important part of legal research.\n• Mapping to standard NLP Tasks\nThis maps to an NLP task called Textual En-\ntailment also called Natural Language Inference.\nDepending on the dataset, these tasks can be of\nmedium to hard complexity.\n• Similar NLP benchmarks\nSimilar benchmarks focus on ﬁnding Textual\nEntailment in general English text (The Stanford\nNatural Language Inference Corpus (NLI) , Recog-\nnizing Textual Entailment as part of super (Wang\net al., 2020). Another similar benchmark (COL) is\nabout ﬁnding a speciﬁc paragraph from case R that\nis relevant to a new case such that the paragraph\nentails decision Q of a new case.\n• Dataset to be collected\nData needed for this benchmark would be state-\nments recorded in FIRs and chargesheets. These\nthen would be classiﬁed into various topics and\nthen humans would annotate if these are contradic-\ntions or not. This data would be used to evaluate\nthe NLP models based on accuracy of the predic-\ntion. Humans with experience in legal research\nwould be needed to create such data. These could\nbe lawyers or assistants who perform legal research.\nA custom annotator tool needs to be built to allow\nsuch human annotations. This tool would show the\nsuggested contradictions .The answer suggested\ncan be accepted by the expert or he can change it\nto an appropriate one. Experts will also add the\ncontradictions that are not present in suggestions.\nThis suggestion would reduce the human process-\ning time signiﬁcantly.\nStatement 1\nStatement 2\nPrediction\nPolice reached\nPolice reached the\nContradiction\nthe site at 5pm\nsite at 6 pm\nI saw the dead body\nDead body had a lot\nEntailment\nwith blood clots\nof blood marks\nI saw the accused\nWhen I reached site I saw the dead\nNeutral\ncommitting murder\nbody with no one around\nTable 4: Examples of Contradiction Discovery\n• Evaluation Metrics\nThis benchmark can be evaluated by comparing\nthe human labeled triplets with model predictions.\nAccuracy of such predictions can be used as a met-\nric.\n5.4.3\nSentence Rhetorical Roles Prediction\n• Description of Task\nAlthough the style of writing a judgement varies\nby the Judge, most of the judgements have an in-\nherent structure. Giving structure to the judgment\ntext is important for many Information retrieval and\nother downstream tasks. Sentence Rhetorical Roles\nprediction means identifying what role a sentence\nis playing in the judgement. Recent research has\ncreated a small dataset and built an ML model to\npredict the rhetorical role of each sentence.\n• Value Proposition\nThe identiﬁcation of right section of the judge-\nment narrows the text to focus for a given task. E.g.\nIf a person wants to know the ﬁnal decision then it\ncould be found in the section marked as “Current\nCourt Decision”. If Someone wants to know the\ndescription of the case then it could be found in\n“Facts” section. The rhetorical roles identiﬁcation\nwould also help signiﬁcantly in creating summary\nof the judgements and semantic search.\n• Mapping to standard NLP Tasks\nThis would ﬁt into the task of multi-class text\nclassiﬁcation where each sentence is assigned with\na rhetorical role. The rhetorical role of a sentence is\nalso dependent on the previous and next sentences.\n• Similar NLP benchmarks\nThere are some datasets released about this task\nbut there is no benchmark. The dataset published\nby (Bhattacharya et al., 2019c) is very small and\nnoisy in nature.\n• Dataset to be collected\nThe manual annotations at the sentence level\nabout which sentence belongs to what rhetorical\nrole need to be collected. The size of the dataset\nshould not be as small as the one mentioned in the\noriginal paper by (Bhattacharya et al., 2019c).\n• Evaluation Metrics\nThe accuracy of the prediction could be used as\nevaluation metric\n6\nUseful NLP components across\nBenchmarks\nThis section talks about some building blocks\nwhich are useful across multiple benchmarks.\n6.1\nIndian Legal BERT\nBERT is a pre-trained language model that helps\nto use intelligence from vast amounts of unlabelled\ntext and use it for doing speciﬁc tasks where less\ndata is available. Many of the current BERT models\nare trained on General English Text like Wikipedia.\nCreating BERT speciﬁcally in the context of Indian\nLaw will help many of the Indian Legal Tasks. This\nmodel will learn the vocabulary of the Indian Legal\nsystem and semantics of it. Creating such domain\nspeciﬁc BERT models has shown promising results\nin literature. This BERT model will help across the\nmultiple Indian Legal NLP benchmarks.\n6.2\nLegal Entities Extractor\nExtraction of legal entities like court name, judge\nname, parties involved, acts and sections etc. are\nuseful. These would act as inputs to multiple bench-\nmarks. Having a component which understands the\nlanguage used by Indian Law would make it per-\nform better than other out of the box entity recog-\nnition components.\n6.3\nCleaned Textual Repository\nTo overcome the challenges about public datasets\nmentioned in the previous section, it is important\nto create an open textual repository of indian legal\ntext. Having such a repository would save a lot of\ntime in scraping the legal websites and bringing\nthem to uniﬁed format. This will also provide data\nfor human annotations for multiple benchmarks.\n7\nOperationalizing Indian NLP\nBenchmarks\nThe timeline chart below shows how the NLP\nbenchmark will start showing beneﬁts for the Legal\nsystem with time. Starting point is identiﬁcation\nof NLP benchmarks and doing efforts and impacts\nstudy of these NLP benchmarks. This white pa-\nper is part of this step. In the next step, relevant\ndatasets would be collected (either using humans\nor automatically from the text based on the bench-\nmark). Baseline NLP models would be created\nwhich indicate bare minimum evaluation metric\nvalues. The datasets would be split into two sets:\ntraining data and testing data. Training data would\nbe provided to the participants of the competition.\nTesting data would be kept hidden and would be\nused for evaluation purposes.\nThe competitions would be launched with these\ndatasets along with prizes for winners. Many times\nthe research value of such solutions is more im-\nportant than the prize money. So it is important\nto publish the dataset in reputed journals which\nattract the ML community. For legal communities\nlike startups & legal product development compa-\nnies, the benchmark would present opportunities to\nobjectively show how good their product is. Once\nthe Indian Legal NLP benchmarks become widely\nrecognized in the ML community and legal com-\nmunity then the beneﬁts of such benchmarks start\nto show up. Many of the researchers open source\nthe code they developed for research for the bench-\nmarks. This helps the overall community and re-\nsearchers can spend their time in solving more com-\nplex problems. Since these benchmarks present\nchallenging tasks they will open up more possibili-\nties and applications. All these things would help\nIndian Legal systems to be more efﬁcient, open and\ncitizen centric.\nAcknowledgements\nThis paper is funded by EkStep Foundation\nReferences\nBlackstone spacy model.\ncdqa: A libraries for transformers.\nColiee2020.\nCriminal1 tracking network and system publicly avail-\nable ﬁr data.\nEventregistry: Timeline extraction company.\nFire: Forum for information retrieval evaluation.\nHaystack: A natural language interface for data.\nKbp: Knowlege base population.\nLegal nlp datasets a curation of data for legal nlp.\nNational judicial grid publicly available judgement\ndata.\nNli.\nOfﬁcial ai govt of india ai initiatives for legal.\nOpenie:open information extraction.\npaperswithcode natural language processing.\nPrimerai: Timeline extraction company.\nTac: Text analysis conference.\nWestlaw: Case summaries for cases.\nAbdalghani Abujabal, Rishiraj Saha Roy, Mohamed\nYahya, and Gerhard Weikum. 2019.\nComQA:\nA community-sourced dataset for complex factoid\nquestion answering with paraphrase clusters.\nIn\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 307–317,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nPooja Batra, Sarika Chaudhary, Kavya Bhatt, Sa-\nloni Varshney, and Srashti Verma. 2020.\nA re-\nview: Abstractive text summarization techniques us-\ning nlp. In 2020 International Conference on Ad-\nvances in Computing, Communication & Materials\n(ICACCM), pages 23–28. IEEE.\nPaheli Bhattacharya, Kripabandhu Ghosh, Saptarshi\nGhosh, Arindam Pal, Parth Mehta, Arnab Bhat-\ntacharya, and Prasenjit Majumder. 2019a. Fire 2019\naila track: Artiﬁcial intelligence for legal assistance.\npages 4–6.\nPaheli Bhattacharya, Kaustubh Hiware, Subham Raj-\ngaria, Nilay Pochhi, Kripabandhu Ghosh, and Sap-\ntarshi Ghosh. 2019b. A comparative study of sum-\nmarization algorithms applied to legal case judg-\nments. In European Conference on Information Re-\ntrieval, pages 413–428. Springer.\nFigure 8: Operationalizing NLP Benchmarks\nPaheli Bhattacharya,\nShounak Paul,\nKripabandhu\nGhosh, Saptarshi Ghosh, and Adam Wyner. 2019c.\nIdentiﬁcation of rhetorical roles of sentences in in-\ndian legal judgments.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos\nAletras. 2019. Neural legal judgment prediction in\nenglish.\nHai Leong Chieu and Yoong Keok Lee. 2004. Query\nbased event extraction along a timeline. In Proceed-\nings of the 27th annual international ACM SIGIR\nconference on Research and development in infor-\nmation retrieval, pages 425–432.\nAlexander R Fabbri, Wojciech Kry´sci´nski, Bryan\nMcCann, Caiming Xiong, Richard Socher, and\nDragomir Radev. 2021. Summeval: Re-evaluating\nsummarization evaluation. Transactions of the Asso-\nciation for Computational Linguistics, 9:391–409.\nTobias Falke and Iryna Gurevych. 2017.\nBringing\nstructure into summaries: Crowdsourcing a bench-\nmark corpus of concept maps.\narXiv preprint\narXiv:1704.04452.\nAtefeh Farzindar and Guy Lapalme. 2004a. Legal text\nsummarization by exploration of the thematic struc-\nture and argumentative roles. In Text Summarization\nBranches Out, pages 27–34.\nAtefeh Farzindar and Guy Lapalme. 2004b. Letsum,\nan automatic text summarization system in law ﬁeld.\nLegal Knowledge and Information Systems 17 th\nAnnual Conference, Jurix.\nMark A Finlaysona, Andres Cremisini, and Mustafa\nOcal. Extracting and aligning timelines.\nJohannes Gehrke, Paul Ginsparg, and Jon Kleinberg.\n2003. Overview of the 2003 kdd cup. Acm SIGKDD\nExplorations Newsletter, 5(2):149–151.\nMd Majharul Haque, Suraiya Pervin, and Zerina Be-\ngum. 2013.\nLiterature review of automatic single\ndocument text summarization using nlp.\nInterna-\ntional Journal of Innovation and Applied Studies,\n3(3):857–865.\nTomasz Jurczyk, Michael Zhai, and Jinho D. Choi.\n2016. Selqa: A new benchmark for selection-based\nquestion answering.\nIn 2016 IEEE 28th Interna-\ntional Conference on Tools with Artiﬁcial Intelli-\ngence (ICTAI), pages 820–827.\nAmbedkar Kanapala, Sukomal Pal, and Rajendra Pa-\nmula. 2019.\nText summarization from legal doc-\numents: a survey.\nArtiﬁcial Intelligence Review,\n51(3):371–402.\nKalpesh Krishna and Mohit Iyyer. 2019.\nGenerat-\ning question-answer hierarchies.\narXiv preprint\narXiv:1906.02622.\nParteek Kumar. 2021. Deep learning based question\ngeneration using t5 transformer. In Advanced Com-\nputing: 10th International Conference, IACC 2020,\nPanaji, Goa, India, December 5-6, 2020, Revised\nSelected Papers, Part I, volume 1367, page 243.\nSpringer Nature.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics,\n7:453–466.\nJure Leskovec, Jon Kleinberg, and Christos Faloutsos.\n2005. Graphs over time: densiﬁcation laws, shrink-\ning diameters and possible explanations. In Proceed-\nings of the eleventh ACM SIGKDD international\nconference on Knowledge discovery in data mining,\npages 177–187.\nDayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi,\nHang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Lin-\njun Shou, Ming Gong, et al. 2020. Glge: A new\ngeneral language generation evaluation benchmark.\narXiv preprint arXiv:2011.11928.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. arXiv preprint\narXiv:2005.00661.\nKaiz Merchant and Yash Pande. 2018. Nlp based latent\nsemantic analysis for legal text summarization. In\n2018 International Conference on Advances in Com-\nputing, Communications and Informatics (ICACCI),\npages 1803–1807.\nAnne-Lyse Myriam Minard, Manuela Speranza, Eneko\nAgirre, Itziar Aldabe, Marieke van Erp, Bernardo\nMagnini, German Rigau, and Ruben Urizar. 2015.\nSemeval-2015 task 4: Timeline: Cross-document\nevent ordering. In 9th international workshop on se-\nmantic evaluation (SemEval 2015), pages 778–786.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summariza-\ntion using sequence-to-sequence rnns and beyond.\narXiv preprint arXiv:1602.06023.\nShashi Narayan, Shay B Cohen, and Mirella Lap-\nata. 2018.\nDon’t give me the details, just the\nsummary!\ntopic-aware convolutional neural net-\nworks for extreme summarization.\narXiv preprint\narXiv:1808.08745.\nQiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, and\nDan Roth. 2018. Cogcomptime: A tool for under-\nstanding time in natural language. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 72–77.\nJakub Piskorski, Vanni Zavarella, Martin Atkinson, and\nMarco Verile. 2020. Timelines: Entity-centric event\nextraction from online news. In Text2Story@ ECIR,\npages 105–114.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012.\nConll-\n2012 shared task:\nModeling multilingual unre-\nstricted coreference in ontonotes. In Joint Confer-\nence on EMNLP and CoNLL-Shared Task, pages 1–\n40.\nWeizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao,\nBartuer Zhou, Biao Cheng, Daxin Jiang, Jiusheng\nChen, Ruofei Zhang, et al. 2021.\nProphetnet-x:\nLarge-scale pre-training models for english, chinese,\nmulti-lingual, dialog, and code generation.\narXiv\npreprint arXiv:2104.08006.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016.\nSquad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSebastian Riedel, Limin Yao, and Andrew McCal-\nlum. 2010. Modeling relations and their mentions\nwithout labeled text.\nIn Machine Learning and\nKnowledge Discovery in Databases, pages 148–163,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise\nGetoor, Brian Galligher, and Tina Eliassi-Rad. 2008.\nCollective classiﬁcation in network data. AI Maga-\nzine, 29(3):93.\nAlex Wang,\nYada Pruksachatkun,\nNikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2020. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Yansong Feng, Xi-\nanpei Han, Zhen Hu, Heng Wang, and Jianfeng Xu.\n2018. Cail2018: A large-scale legal dataset for judg-\nment prediction.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang,\nXianpei Han, Zhen Hu, Heng Wang, and Jianfeng\nXu. 2019. Cail2019-scm: A dataset of similar case\nmatching in legal domain.\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiQA: A challenge dataset for open-domain ques-\ntion answering.\nIn Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2013–2018, Lisbon, Portugal. As-\nsociation for Computational Linguistics.\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\nand Maosong Sun. 2019.\nDocred: A large-scale\ndocument-level relation extraction dataset.\nManzhu Yu, Myra Bambacus, Guido Cervone, Keith\nClarke, Daniel Duffy, Qunying Huang, Jing Li, Wen-\nwen Li, Zhenlong Li, Qian Liu, et al. 2020. Spa-\ntiotemporal event detection: a review. International\nJournal of Digital Earth, 13(12):1339–1365.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-\ngeli, and Christopher D. Manning. 2017. Position-\naware attention and supervised data improve slot\nﬁlling. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2017), pages 35–45.\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020a.\nHow does nlp beneﬁt legal system: A summary\nof legal artiﬁcial intelligence.\narXiv preprint\narXiv:2004.12158.\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang\nZhang, Zhiyuan Liu, and Maosong Sun. 2020b. Jec-\nqa: A legal-domain question answering dataset. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 34, pages 9701–9708.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-07-13",
  "updated": "2021-07-13"
}