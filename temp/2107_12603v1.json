{
  "id": "http://arxiv.org/abs/2107.12603v1",
  "title": "Federated Learning Meets Natural Language Processing: A Survey",
  "authors": [
    "Ming Liu",
    "Stella Ho",
    "Mengqi Wang",
    "Longxiang Gao",
    "Yuan Jin",
    "He Zhang"
  ],
  "abstract": "Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.",
  "text": "arXiv:2107.12603v1  [cs.CL]  27 Jul 2021\nFederated Learning Meets Natural Language\nProcessing: A Survey\nMing Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang\nDeakin University\nAbstract. Federated Learning aims to learn machine learning models\nfrom multiple decentralized edge devices (e.g. mobiles) or servers with-\nout sacriﬁcing local data privacy. Recent Natural Language Processing\ntechniques rely on deep learning and large pre-trained language models.\nHowever, both big deep neural and language models are trained with\nhuge amounts of data which often lies on the server side. Since text\ndata is widely originated from end users, in this work, we look into re-\ncent NLP models and techniques which use federated learning as the\nlearning framework. Our survey discusses major challenges in federated\nnatural language processing, including the algorithm challenges, system\nchallenges as well as the privacy issues. We also provide a critical review\nof the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.\nKeywords: Federated Learning · Natural Language Processing · Lan-\nguage Modelling · Privacy.\n1\nIntroduction\nModern machine learning algorithms rely on big amounts of data, especially\nwhen training deep neural models from high dimensional data such as text and\nimage. Most data naturally come from end users, which are distributed and\nseparated by diﬀerent end devices. It is necessary to learn well performed ma-\nchine learning models while preserving users’ privacy. Federated learning (FL)\nhas become a new machine learning paradigm to train a model across multiple\ndecentralized edge devices or servers holding local data samples without ex-\nchanging them. The term federated learning was ﬁrst proposed in 2016 by [40]:\n”We term our approach Federated Learning, since the learning task is solved by\na loose federation of participating devices (which we refer to as clients) which\nare coordinated by a central server.” In the real world scenario, organizations\nsuch as diﬀerent hospitals hold conﬁdential data, while these hospitals would like\nto train a disease classiﬁcation model for common use, it is hard to ask them\nto upload their own data to the cloud. Even within the same hospital, diﬀerent\ndepartments often save patients’ information locally. Another example is human\nbeings create lots of text data by their smartphones, these data are building\nblocks for now-days big language models. However, it is shown that most lan-\nguage models suﬀer from ethic problems, since they may leak users’ personal\ninformation in an unexpected way.\n2\nL. Ming et al.\nRecent eﬀorts in federated learning have been devoted to interdisciplinary ar-\neas: not only machine learning is required, but also techniques from distributed\noptimization, statistics, cybersecurity, communication, systems, cryptography\nand many more. Meanwhile, the data ranges from structured to unstructured\nformat, which is not limited to tabulars, time series and images. Among most\nfederated learning studies, Google has led the use of federated learning in Nat-\nural Language Processing through Gboard mobile keyboard, Pixel phones and\nAndroid Messages. While Google has launched several applications on langauge\nmodeling tasks, Apple is using FL for wake-up word detection in Siri, doc.ai\nis developing cross-device FL solutions for biomedical research, and Snips has\nintroduced cross-device FL for hotword detection.\nIn this paper, we take a survey on the existing FL algorithms for Natural\nLanguage Processing (NLP). Starting from language modeling, we will review\ncurrent federated learning algorithms on various NLP tasks: classiﬁcation, rec-\nommendation, speech recognition and health text mining and others . We or-\nganize the survey as follows: in Section 2, basic federated learning concepts,\nframeworks, optimization toward non-IID data, privacy are discussed. Section\n3 reviews federated learning in NLP. Section 4 discusses the common evalua-\ntion aspects and tools. Section 5 highlights current research challenges and some\nfuture directions. Section 6 gives the conclusion.\n2\nFederated learning\nIn this section, we ﬁrst review basics of federated learning, including the prob-\nlem setup, non-iid data distribution, frameworks, optimization algorithms and\nprivacy preservation. Then, we extend federated learning to other distributed\nmachine learning paradigms and discuss their diﬀerence.\nProblem formulation In this wrok, we consider the following distributed opti-\nmization process:\nminw{L(w) = PN\nk=1 pkLk(w)},\nwhere N is the total number of user devices, pk is the weight of the k-th device\nsuch that pk ≥0 and PN\nk=1 pk = 1. Suppose the k-th device has the amount of\nnk training data: xk = (xk,1, xk,2, ..., xk,nk). The local training objective Lk(.)is\ndeﬁned by:\nLk(w) =\n1\nnk\nPnk\nj=1 l(w; xk,j),\nwhere l(.; .) is a user-speciﬁed loss function.\nNon-IID data and Learning Strategies Typical centralized supervised learning\nalgorithms have the IID assumption, i.e., the training and test data is indepen-\ndently identically distributed. In decentralized settings like federated learning,\nnon-IID poses a challenge because the diﬀerent data distribution result in signiﬁ-\ncant skewness across devices or locations. Non-IID data among devices/locations\nFederated Learning Meets Natural Language Processing: A Survey\n3\nencompass many diﬀerent forms. There can be skewed distribution of features\n(probability P(x)), labels (probability P(y)), or the relationship between features\nand labels (e.g., varying P(y|x) or P(x|y)) among devices/locations. Previous\nreviews categorized this this as horizontal, vertical and hybrid data partitions\nin Federated Learning. In this review, we focus on skewed distribution of labels,\ni.e., PPi(y) ̸= PPj(y) for diﬀerent data partitions Pi and Pj.\nPrevious study has shown DNN models with batch normalization suﬀer from\nNon-IID data [25], the accuracy of FL reduces signiﬁcantly by up to 55% for\nneural networks trained for highly skewed non-IID data [69], where each clinet\ndevice trains only on a single class of data.\nCommon techiniques to deal with non-IID include:\n– data augmentation: create a common dataset which can be shared globally,\nthe dataset can come from a publicly available proxy data source [69], or\nperhaps a distillation of the raw data following [61].\n– schedule client participation during training : FedFMC, FedCD, cluster sim-\nilar devices / multi-center/ hirarchical clustering of local updates, FedPD,\nadapts the communication frequency of decentralized learning algorithms to\nthe (skew-induced) accuracy loss between data partitions [25]\n– greater number of models, but more communication cost:\n– ensemble: similar to scheduling\n– regularization on the server, e.g. FedAwS, server imposes a geo- metric reg-\nularizer after each round to encourage classes to be spreadout in the embed-\nding space\n– personalized FL/ continual local training, based on MAML\nOptimization While a variety of studies have made assumptions for the per-\nclient optimization functions in the IID setting, we review basic convergence\nresults for H-smooth convex functions under the assumption that the variance\nof the stochastic gradients is bounded by σ2. Given the following notations in\na standard FL setting: N is the total number of clients, M is the number of\nparticipated clients per round, T is the total number of communication rounds,\nK is the local SGD steps per round. Federated averaging can conducted in either\nof the following two settings: one is to keep x ﬁxed in local updates during each\nround and compute a total of KM gradients at the current x, in order to run\naccelerated minibatch SGD, the convergence rate is then upper bounded by\nO( H\nT 2 +\nµ\n√\nT KM ). The other is to ignore all but 1 of the M active clients, which\nallows sequential SGD to run for KT steps, this approach has an upper bound of\nO(\nH\n(T K)2 +\nµ\n√\nT K ). As in the non-IID settings, key assumptions are given for inter-\nclient gradient, local functions on each client and other participation constraints.\nA detailed discussion of diﬀerent convergence rates for non-IID setting can be\nfound in [30].\nFrameworks There are three basic frameworks for FL: centralized, decentral-\nized and heterogeneous. In the centralized federated learning setting, a central\nserver is used to orchestrate the diﬀerent steps of the algorithms and coordinate\n4\nL. Ming et al.\nall the participating nodes during the learning process. The server is responsi-\nble for the nodes selection at the beginning of the training process and for the\naggregation of the received model updates. Since all the selected nodes have\nto send updates to a single entity, the server may become a bottleneck of the\nsystem. Most NLP applications like keyboard word prediction is using the ce-\ntralized setting. In the decentralized federated learning setting, the nodes are\nable to coordinate themselves to obtain the global model. This setup prevents\nsingle point failures as the model updates are exchanged only between intercon-\nnected nodes without the orchestration of the central server. Nevertheless, the\nspeciﬁc network topology may aﬀect the performances of the learning process.\nMost blockchain-based federated learning falls into the decentralized setting. An\nincreasing number of application domains involve a large set of heterogeneous\nclients, e.g., mobile phones and IoT devices. Most of the existing Federated learn-\ning strategies assume that local models share the same global model architecture.\nRecently, a new federated learning framework named HeteroFL was developed\nto address heterogeneous clients equipped with very diﬀerent computation and\ncommunication capabilities. The HeteroFL technique can enable the training of\nheterogeneous local models with dynamically-varying computation complexities\nwhile still producing a single global inference model.\nPrivacy In most FL settings, privacy preservation is conducted to make sure\nusers’ information is not leaked during the learning process. Physically, local\ndata is not allowed to leave end users’ devises. However, it is still possible to re-\nconstruct the original data by taking the model weights or gradients. Therefore,\nwe consider privacy preservstion on three aspects in FL: users’ personal informa-\ntion, local data and machine learning models. Take the smart phone keyboard\nnext word prediction as an example, users personal information refers to facts\nabout their location, name, sex as well as hidden information like keyboard typ-\ning pattern. Local data then concludes the messages, photos and videos in their\nphones, while a machine learning model could be the a language model which\npredicts the next word given some preceding words. Users’ personal information\nis often correlated with the local data, e.g., a person’s age can be inferred with\nhis/her chat messages.\nCommon techniques for privacy preservation of user data includes: diﬀerential\nprivacy [10], secure Multi-Party Computation [15], homomorphic encryption [43]\nand trusted execution environments [11]. Veriﬁability enables parties to prove\nthat they have executed their parts of a computation faithfully. Techniques for\nveriﬁability include both zero knowledge proofs (ZKPs) [13] and trusted ex-\necution environments (TEEs) [11]. As for model attack, adversarial learning\ntechniques can be leveraged in FL setting,\nFederated Learning Meets Natural Language Processing: A Survey\n5\n3\nFederated learning in NLP\n3.1\nLanguage modeling\nA language model (LM) refers to a model that provides probabilities of word\nsequences through an unsupervised distribution estimation. As an essential com-\nponent for NLP systems, LM is utilised in a variety of NLP tasks, i.e., machine\ntranslation, text classiﬁcation, relation extraction, question-answering, etc. In\nFL, most LMs are deployed on a virtual mobile keyboard, i.e., the Google Key-\nboard (Gboard). Thereby, recent literature are mostly produced by authors from\nGoogle, LLC. Recent works on language modelling in Federated NLP mainly\ntarget on solving a word-level LM problem in mobile industry. That is mobile\nkeyboard suggestion, which is a well representative of federated NLP applica-\ntions. To improve mobile keyboard suggestions, federated NLP models aim to be\nmore reliable and resilient. Existing models oﬀer quality improvements in typing\nor even expression (e.g., emoji) domains, such as next-world predictions, emoji\npredictions, query suggestions, etc.\nConsidering the characteristics of mobile devices, a decentralized computa-\ntion approach is constrained by computation resource and low-latency require-\nment. A mobile device has limited RAM and CPU budgets, while we expect\nkeyboards to provide a quick and visible response of an input event within 20\nmilliseconds. Thereby, the model deployed in client sides should perform fast\ninference.\nMost works [67,20,47,5,29,52,58] consider variants of LSTMs [24] as the client\nmodel. Given the limited computation budget on each device, we expect the pa-\nrameter space of a neural language model to be as small as possible without\ndegrading model performance. CIFG [17] is a promising candidate to diminish\nLM’s complexity and inference-time latency. It employs a single gate to harness\nboth the input and recurrent cell self-connections. In such a way, the amount of\nparameters is downsized by 25 % [17]. [20] leverages CIFG for next word pre-\ndictions, and simpliﬁes the CIFG model by removing peephole connections. To\nfurther optimise the model in size and training time, they ties input embedding\nand CIFG output projection matrices. [47] applies a pretrained CIFG network\nas an emoji prediction model. In particular, the pretraining process involves all\nlayers, excluding the output projection layer, using a language learning task. To\nenhance performance, the authors enable embedding sharing between inputs and\noutputs. The pretraining of LM exhibits fast convergence for the emoji model.\n[5] employs a character-level RNN [63], targeting on out-of-vocabulary(OOV)\nlearning tasks, under FL settings. Speciﬁcally, they use CIFG with peephole con-\nnections and a projection layer. The projection layer diminishes the dimension\nof output and accelerates the training. They use multi-layer LSTMs to enhance\nthe representation power of the model, which learns the probability of word oc-\ncurrence. GRU [8] is another simpler variant of the LSTM. [29] leverage GRU as\nthe neural language model for mobile keyboard next-word predictions. Similar\nto CIFG, it reduces the model complexity on parameter spaces without hurting\nthe model performance. To downsize the amount of trainable parameters, they\n6\nL. Ming et al.\nalso apply tied embedding in the embedding layer and output layer by share of\nthe weights and biases.\n[67] proposes another LM for keyboard query suggestions to reduce the bur-\nden of training LSTMs. Speciﬁcally, they train a LSTM model on the server for\ngenerating suggestion candidates, while merely federated training a triggering\nmodel, that decides the occurrence of the candidates. The triggering model uses\nlogistic regression to infer the probability of a user click, signiﬁcantly lessening\nthe computation budgets in comparison of RNN models. [6] also states the di-\nrect use of RNN is not the proper means to decode due to its large parameters\nsize, which further causes slow inference. Hereby, they propose to leverage a n-\ngram LM that derived from a federated RNN for decoding. In particular, they\novercome large memory footprints problem and enhance model performance by\nintroducing an approximation algorithm based on SampleApprox [57]. It approx-\nimates RNN models into n-gram models. Still, they use CIFG and group-LSTM\n(GLSTM) [32] for approximation. While, GPT2 [46] is one of the state-of-the-art\ntransformer-based LMs with 1.5 billion parameters. Considering its performance\non centralized language modeling tasks, [52] uses GPT2 as LM. They propose\na dimensionality reduction algorithm to downsize the dimension of GPT2 word\nembedding to desired values (100 and 300).\nFor federated optimization, existing federated optimization algorithms diﬀer\nin client model aggregation on the server-side. In federated language modeling,\nmost existing works [67,20,6,47,5,52,58] use FedAvg as the federated optimiza-\ntion algorithm. Another optimization strategy, called FedAtt, has also shown its\nfeasibility and validity in language models [29].\nIn FedAvg, gradients, that computed locally over a large population of clients,\nare aggregated by the server to build a novel global model. Every client is trained\nby locally stored data and computes the average gradient with the current global\nmodel via one or more steps of SGD. Then, it communicates model updates with\nthe server. The server performs the weighted aggregation of the client updates\nto build a novel global model. Client updates are immediately abandoned on\nthe sever once the accumulation is completed. [20] trains the global model from\nscratch in the server, using FedAvg. Speciﬁcally, the initial global model has\neither been randomly initialized or pretrained on proxy data. However, it in-\ncreases the federated training rounds on clients. Thereby, it leads to a high\ncommunication and computation costs in FL. They also use SGD as the server-\nsided optimizer for training. They found Adam and AdaGrad provide no ben-\neﬁcial improvement on convergence. [52] introduces a novel federated training\napproach, called central pre-training with federated ﬁne-tuning. To address the\ndrawback in [20], the server pretrains a model with centralized and public data\nas the global model at the initial time. Each clients then obtains the pretrained\nweights as the initial weights, and later trained on local data in a federated\nfashion. But the improvement is limited to large network, i.e., GPT2. They also\npropose a pretrained word embedding layer for federated training, which only\nenhance accuracy for the large word embedding network (i.e., GPT2). Whereas,\nwith the combination of pretraining models, it harms the performance. They\nFederated Learning Meets Natural Language Processing: A Survey\n7\nleverage Adam as the optimizer for training. [5] uses momentum and adaptive\nL2-norm clipping on each client’s gradient in FedAvg, leading to a faster con-\nvergence. The authors argue momentum and adaptive clipping performed on\ngradients improves the robustness of model convergence and performance. [58]\nalso uses clipping for regularization in FedAvg by setting the upper bound of\nuser updating to constrain each client contribution (i.e., clipping). In addition,\n[47] founds using momentum with Nesterov accelerated gradients signiﬁcantly\noutperforms using SGD as server optimizer, in terms of convergence rate and\nmodel performance. [6] applies Nesterov momentum as both the local and the\nserver optimizer.\n[29] ﬁrst introduces the attention mechanism into federated aggregation of\nclient models. This optimization algorithm is referred as Attentive Federated\nAggregation (FedAtt). It is a layer-wise soft attention mechanism applied on\nthe trained parameters of the NN model. Intuitively, the federated optimization\nalgorithm learns to optimize the global model by providing a good generalization\non each client model for a quick local adaptation. Hereby, it reduces local training\nrounds and saves the computation budgets, further accelerating the learning\nprocess. The generalization in FedAtt is decided by the similarity between each\nclient and the server, and the relative importance of each client. For a good\ngeneralization, they minimise the weighted summed distance of each client model\nand the global model on parameters spaces. They introduce attentive weights\nas the weights of the client models. Particularly, the attentive weight of each\nclient model is a non-parametric attention score derived from each layer of NN.\nDiﬀer from pre-trained FedAvg, FedAtt ﬁnds a well-generalized global model on\neach federated training round by iteratively updating parameters. Consequently,\nit further lessens the federated communication budgets. For local training, the\nclient-sided optimizer is momentum. While, for global parameters updates, they\nuses SGD.\nThe existing works on federated language modeling mainly contribute on\noptimizing model aggregation process, but not focusing on privacy preserving\napproach. Adding privacy preserving techniques into federated optimization pro-\ncess is seen as a bonus, rather than an essential means of privacy guarantees. In\nFederated LMs, the commonly used privacy preserving technique is diﬀerential\nprivacy (DP) [9]. A DP algorithm is expected to characterize the underlying\nprobability distribution without compromising personally identiﬁable data. In\ngeneral, it injects calibrated noise into the aggregated data while not aﬀecting\nthe outcomes. Most DP approaches are used for user-level privacy guarantees.\nIn FL, we deﬁne user-level DP as a privacy guarantees, to preserve the trained\nmodels with or without the presence of any one client’s data. DP usually serves\non the client sides before model aggregation [62]. [29] integrates a randomized\nmechanism in FedAtt optimization by introducing a white noise with the mean\nof 0 and the standard deviation σ. They also introduce a magnitude coeﬃcient\nβ ∈(0, 1] to govern the eﬀect of the randomization in FL. However, the level of\nits DP guarantees is unrevealed. Hereby, it fails to show the trade-oﬀbetween\ndata utility and privacy protection for its privacy-preserving countermeasure im-\n8\nL. Ming et al.\nplementation. [58] incorporates the Gaussian mechanism in FedAvg to cope with\nthe user-based heterogeneity of data in language models. In particular, it per-\nform DP guarantees by adding Gaussian noise with a noise multiplier of 1, after\nclipping. They argue a high level of DP guarantees exhibits a notable reduction\nin unintended memorization, caused by heterogeneity of training data.\n3.2\nClassiﬁcation\nText Classiﬁcation is procedure of identifying the pre-deﬁned Text Classiﬁca-\ntion is the procedure of identifying the pre-deﬁned category for varied-length\nof text [1]. It can be extended to many NLP applications including sentiment\nanalysis, question answering and topic labeling . Traditional text classiﬁcation\ntasks can be deconstructed into four steps: text preprocessing, dimension re-\nduction, classiﬁcation and evaluation. Though the deep learning models have\nachieved state-of-the-art results in text classiﬁcation [41], uploading or sharing\ntext data to improve model performance is not always feasible due to diﬀer-\nent privacy requirements of clients. For example, ﬁnancial institutions that wish\nto train a chatbot for their clients cannot be allowed to upload all text data\nfrom the client-side to their central server due to strict privacy protection state-\nments. Then applying the federated learning paradigm is an approach to solve\nthe dilemma due to its advances in privacy preservation and collaborative train-\ning. In which, the central server can train a powerful model collaboratively with\ndiﬀerent local labeled data at client devices without uploading the raw data\nconsidering increasing privacy concerns in public.\nHowever, there are several challenges for applying federated learning to text\nclassiﬁcation tasks in NLP. One is to design proper aggregating algorithms to\nhandle the gradients or weights uploaded by diﬀerent client models. Traditional\nfederated learning can be considered as a special paradigm of distributed learn-\ning, thus aggregating algorithms, such as FedAvg [40], FedAtt [29] has been\nproposed to generalize the model on the central server. Considering the un-\nevenly distributed data at diﬀerent client devices and diﬀerent amounts of data\nat the diﬀerent local datasets. [71] has attempted the text classiﬁcation using the\nstandard FedAvg algorithm to update the model parameter with local trained\nmodels. It uses diﬀerent local datasets to pre-train the word embeddings, and\nthen concatenate all word embeddings. After ﬁltering the widths and feature\nmaps from the concatenated word embeddings, the max-over-time pooling was\nused to aggregate the features, thus getting vectors with the same length. Fi-\nnally, they use softmax activation on the fully connected layer, it will translate\nthe vectors to the ﬁnal sentence classiﬁcation results (categories). Later, scien-\ntists from the Machine learning area brought in new approaches of uploading\nand aggregating, for example, using Knowledge distillation [23]. [22] however\nuse ﬁne-tuning instead of FedAvg to update parameters. [36] average the logits\noutputs from the last layer of the model instead of directly take the average of\nmodel parameters. It then uses knowledge distillation to learn the knowledge\nfrom the client devices instead of traditional.\nFederated Learning Meets Natural Language Processing: A Survey\n9\nIn addition, model compression has been introduced to federated text clas-\nsiﬁcation tasks due to the dilemma of computation restraints on the client-side.\nThey attempted to reduce the model size on the client-side to enable the real ap-\nplication of federated learning. The computation restriction on the client devices\nlimits the application of traditional FL. For example, 4-layer BERT or 6-layer\nBERT is still too large for mobile devices such as smartphones. The scholars\nthen focus to perform the model compression while still following the federated\nlearning paradigm. The knowledge distillation then has been applied to transfer\nlocal model information while keeping the model size small at the local devices\nin [48]. It utilises knowledge distillation instead of model parameter fusion to up-\ndate parameters. The soft-label predictions on a public distillation dataset are\nsent to the central model to be distilled. Thus, the central model can learn the\nlocal knowledge on client devices through distilling the logits of diﬀerent client\nmodels without sharing or uploading the local model parameters and gradients.\nTo ensure the privacy preservation of FL while keeping the communication,\nthe encryption of data is one of the top priority considerations in applying fed-\nerated learning in NLP. Encryption on communication between edge-device and\ncentral server is a standard approach in federated learning to preserve privacy\nfor end-users on edge devices. [71] adds encryption on client-central server com-\nmunication using diﬀerential privacy. It used the approach [60] proposed the\nattack-adaptive aggregation which prevent the attack at the central server ag-\ngregation module.\nTo overcome the communication dilemma of FL, one-shot or few-shot feder-\nated learning was proposed to allow the central server can successfully train the\ncentral model with only one or a few rounds of communication under poor com-\nmunication scenarios. However, the shared data restriction of federated learning\nis still left to be solved. Considering the trend of higher restriction of data shar-\ning and uploading, it will be harder to get a suﬃcient size of data shared to\nboth central servers and client servers. In this way, the knowledge distillation\ncannot be used to solve the model compression problem in federated learning.\n[70] reduced the communication of previous federated learning by utilising the\nsoft labels dataset distillation mentioned in [18] and [54]. It thus successfully\nextend the soft-labeling methods to two new techniques: soft-reset and random\nmasking, and then successfully using the dataset distillation [61] to realise the\none-round communication federated learning for text classiﬁcation tasks. Each\nclient in [70] distils their local dataset to a much smaller synthetic one, and then\nonly uploads the small-sized synthetic dataset to the server. Thus, no gradients\nor weights is transmitting from the client model to the central server model. The\ndistilled dataset can be as small as one data sample per category, in this way\nthe communication in federated learning can be reduced to as low as one round.\n3.3\nSpeech Recognition\nSpeech recognition is the task of recognising speech within audio and converting\nit into text. Voice assistants such as Amazon Alexa or Apple Siri use on-device\nprocessing to detect wake-up words (e.g. ”Hey Siri”), which is a typical usage\n10\nL. Ming et al.\nfor speech recognition on smartphones. Only when the wake-up words are de-\ntected, further processing like information retrieval or question answering will\nbe running on the cloud. Methods for speech recognition include dynamic time\nwraping [42], Hidden Markov Models [45] and modern end-to-end deep neural\nmodels [16]. More recently, wav2vec [3] masks the speech input in the latent\nspace and solves a contrastive task deﬁned over a quantization of the latent rep-\nresentations which are jointly learned, this method demonstrates the feasibility\nof speech recognition with limited amounts of labeled data.\nOn device wake-up word detectors face two main challenges: First, it should\nrun with minimal memory footprint and computational cost. Second, the wake\nword detector should behave consistently in any usage setting, and show robust-\nness to background noise. [68] performed neural network architecture evaluation\nand exploration for running keyword spotting on resource-constrained microcon-\ntrollers, they showed that it is possible to optimize these neural network archi-\ntectures to ﬁt within the memory and compute constraints of microcontrollers\nwithout sacriﬁcing accuracy. [34] investigated the use of federated learning on\ncrowdsourced speech data to learn a resource-constrained wake word detector.\nThey showed that a revisited Federated Averaging algorithm with per-coordinate\naveraging based on Adam in place of standard global averaging allows the train-\ning to reach a target stopping criterion of 95% recall per 5 FAH within 100 com-\nmunication rounds on their crowdsourced dataset for an associated upstream\ncommunication costs per client of 8MB. They also open sourced the Hey Snips\nwake word dataset 1. [65] proposed a decentralized feature extraction approach\nin federated learning to address privacy-preservation issues for speech recog-\nnition, which is built upon a quantum convolutional neural network (QCNN)\ncomposed of a quantum circuit encoder for feature extraction, and a recurrent\nneural network (RNN) based end-to-end acoustic model (AM). The proposed\ndecentralized framework takes advantage of the quantum learning progress to\nsecure models and to avoid privacy leakage attacks. [19] introduced a framework\nfor speech recognition by which the degree of non-IID-ness can be varied, con-\nsequently illustrating a trade-oﬀbetween model quality and the computational\ncost of federated training. They also showed that hyperparameter optimization\nand appropriate use of variational noise are suﬃcient to compensate for the\nquality impact of non-IID distributions, while decreasing the cost.\n3.4\nSequence Tagging\nSequence tagging, e.g. POS tagging, Named Entity Recognition, plays an impor-\ntant role in both natural language understanding and information extraction.\nStatistical models like Hidden Markov Model and Conditional Random Fields\nwere heavily used, modern approaches rely on deep representations from Recur-\nrent Neural Net, Convolution Neural Net or transformer like architectures. A few\nrecent works focus on biomedical Named Entity Recognition in the federated set-\nting. [39] pretrained and ﬁne tuned BERT models for NER tasks in a federated\n1 http:// research.snips.ai/datasets/keyword-spotting\nFederated Learning Meets Natural Language Processing: A Survey\n11\nmanner using clinical texts, their results suggested that conducting pretraining\nand ﬁne tuning in a federated manner using data from diﬀerent silos resulted\nin reduced performance compared with training on centralized data. This loss\nof performance is mainly due to separation of data as ”federated communica-\ntion loss” . Given the limit of data access, the experiments were conducted with\nclinical notes from a single healthcare system to simulate diﬀerent silos. [14]\nproposed a FedNER method for medical NER, they decomposed the medical\nNER model in each platform into a shared module and a private module. The\nprivate module was updated in each platform using the local data to model the\nplatform-speciﬁc characteristics. The shared module was used to capture the\nshareable knowledge among diﬀerent platforms, and was updated in a server\nbased on the aggregated gradients from multiple platforms. The private module\nconsists of two top layers in our medical NER model, i.e, Bi-LSTM and CRF,\nwhich aim to learn platform-speciﬁc context representations and label decoding\nstrategies. The private module was only trained with local data and exchange\nneither its parameters nor gradients. The shared module consisted of the other\nbottom layers in our NER model, such as the word-level CNN and all types of\nembedding. Diﬀerent from the private module, the shared one mainly aims to\ncapture the semantic information in texts. [56] introduced a privacy preserving\nmedical relation extraction model based on federated learning, they leveraged\na strategy based on knowledge distillation. Such a strategy uses the uploaded\npredictions of ensemble local models to train the central model without requiring\nuploading local parameters.\n3.5\nRecommendation\nRecommendation systems are heavily data-driven. Typical recommendation mod-\nels use collaborative ﬁltering methods [55], in which past user item interactions\nare suﬃcient to detect similar users and/or similar items and make predictions\nbased on these estimated proximities. Collaborative ﬁltering algorithms can be\nfurther divided into two sub-categories that are generally called memory based\nand model based approaches. Memory based approaches directly works with\nvalues of recorded interactions, assuming no model, and are essentially based\non nearest neighbours search (for example, ﬁnd the closest users from a user of\ninterest and suggest the most popular items among these neighbours). Model\nbased approaches assume an underlying “generative” model that explains the\nuser-item interactions and try to discover it in order to make new predictions.\nUnlike collaborative methods that only rely on the user-item interactions, con-\ntent based approaches [44] use additional information about users and/or items.\nIf we consider the example of a movies recommendation system, this additional\ninformation can be, for example, the age, the sex, the job or any other personal\ninformation for users as well as the category, the main actors, the duration or\nother characteristics for the movies (items).\nGiven diﬀerent partitions of users and items, federated recommendation mod-\nels can be horizontal, vertical or transfered. In horizontal federated recommen-\ndation systems, items are shared but users belong to diﬀerent parties. A typical\n12\nL. Ming et al.\nwork is Federated Collaborative Filter (FCF) [2] proposed to use a central server\nto keep the item latent factor matrix, while the user latent factors are stored\nlocally on each device. In the training time, the server distributes the item la-\ntent factor to each party, the participants update their user latent factor by\nlocal rating matrix data and send the item latent factor updates back to the\nserver for aggregation. To avoid the inter trust problem, [21] introduced a fully\ndecentralized setting where participants have full access to the item latent factor\nand communicate with each other to update the model. Moreover, meta learn-\ning has been used for personalized federated recommendation. [12] designed a\nmeta learner to learn generalized model parameters for each participant, then\neach participant’s recommendation is regarded as a personalized task where a\nsupport set is used to generate the recommendation model and the gradient is\ncomputed on a query set. [28] introduced another fedrated meta learning algo-\nrithm for recommendation, in which separate support and query sets are not\nnecessary. Their approach performs relatively well within less amount of train-\ning episodes. Besides, [35] proposed DiFacto, which is a distributed factorization\nmethod and addressed the eﬃciency problem when it comes to large scale user\nitem matrices. In comparison, vertical federated systems have been designed for\nfeature distributed learning problem where participants hold diﬀerent feature\nsets. [26] proposed an asynchronous stochastic gradient descent algorithm. Each\nparty could use an arbitrary model to map its local features to a local prediction.\nThen local predictions from diﬀerent parties are aggregated into a ﬁnal output\nusing linear and nonlinear transformations. The training procedure of each party\nis allowed to be at various iterations up to a bounded delay. This approach does\nnot share any raw data and local models. Therefore, it has fewer privacy risks.\nBesides, for a higher level of privacy, it can easily incorporate the DP technique.\nSimilar to horizontal FedRec, there are also works that further utilize cryptog-\nraphy techniques. [7] presented a secure gradient-tree boosting algorithm. This\nalgorithm adopts HE methods to provide lossless performance as well as pre-\nserving privacy. And [51] proposed a secure linear regression algorithm. MPC\nprotocols are designed using garbled circuits to obtain a highly scalable solu-\ntion. Parties of vertical FedRec could also be two recommenders with diﬀerent\nitem sets. For instance, a movie RecSys and a book RecSys have a large user\noverlapping but diﬀerent items to recommend. It is assumed that users share a\nsimilar taste in movies with books. With FedRec, the two parties want to train\nbetter recommendation algorithms together in a secure and privacy-preserving\nway. [50] proposed a secure, distributed item-based CF method. It jointly im-\nproves the eﬀect of several RecSys, which oﬀer diﬀerent subsets of items to the\nsame underlying population of users. Both the predicted ratings of items and\ntheir predicted rankings could be computed without compromising privacy nor\npredictions’ accuracy. We refer readers to [66] for more detailed discussion for\nfederated recommendation systems.\nFederated Learning Meets Natural Language Processing: A Survey\n13\n3.6\nHealth Text Mining\nFederated learning has emerged as an important framework for health text min-\ning, due to the privacy concern among diﬀerent hospitals and medical organi-\nzations. Besides, most health data exhibits systemic bias towards some speciﬁc\ngroups or patterns, e.g. hospitals, diseases and communities. Again, this non-IID\nissue raises big challenges when applying federated learning into heath text min-\ning tasks. There have been some tasks that were studied in federated learning\nsetting in healthcare, including patient similarity learning [33], patient repre-\nsentation learning and phenotyping [31,38], predictive or classiﬁcation modeling\n[4,27,49], biomedical named entity recognition.\nSpeciﬁcally, [38] designed a two-stage federated approach for medical record\nclassiﬁcation. In the ﬁrst stage, they pre-trained a patient representation model\nby training an neural network to predict ICD and CPT codes from the text of the\nnotes. In the second stage, a phenotyping machine learning model was trained in\na federated manner using clinical notes that are distributed across multiple sites\nfor the target phenotype. In this stage, the notes mapped to ﬁxed-length rep-\nresentations from stage one are used as input features and whether the patient\nhas a certain disease is used as a label with one of the three classes: presence,\nabsence or questionable. [37] proposed a simple federated architecture for early\ndetection of Type-2 diabetes. After comparing the proposed federated learning\nmodel against the centralised approach, they showed that the federated learning\nmodel ensures signiﬁcant privacy over centralised learning model whereas com-\npromising accuracy for a subtle extend. To cope with the imbalanced and non-IID\ndistribution inherent in user’s monitoring data, [64] designed a generative convo-\nlutional autoencoder (GCAE), which aims to achieve accurate and personalized\nhealth monitoring by reﬁning the model with a generated class-balanced dataset\nfrom user’s personal data. It is noticed that GCAE is lightweight to transfer\nbetween the cloud and edges, which is useful to reduce the communication cost\nof federated learning in FedHome. [53] described a federated approach on a brain\nage prediction model on structural MRI scans distributed across multiple sites\nwith diverse amounts of data and subject (age) distributions. In these heteroge-\nneous environments, a Semi-Synchronous protocol provides faster convergence.\n3.7\nOther NLP Tasks\nMore recently, FedNLP provided a research-oriented benchmarking framework\nfor advancing federated learning (FL) in natural language processing (NLP). It\nuses FedML repository as the git submodule. In other words, FedNLP only fo-\ncuses on adavanced models and dataset, while FedML supports various federated\noptimizers (e.g., FedAvg) and platforms (Distributed Computing, IoT/Mobile,\nStandalone). A text generation example can also be found in TensorFlow Tuto-\nrial 2. So far, we have not found any work on other generation works on Machine\nTranslation and Summatization.\n2 https://colab.research.google.com/github/tensorﬂow/federated/blob/master/docs/tutorials\n/federated learning for text generation.ipynb#scrollTo=iPFgLeZIsZ3Q\n14\nL. Ming et al.\n4\nBenchmarks\n4.1\nEvaluation Aspects\nModel Evaluation In principle, FL based NLP models should not only be evalu-\nated against traditional performance metrics (such as model accuracy), but also\nthe change of model performance with diﬀerent system and data settings. Vari-\nous systems settings consider the number of nodes, the weight of the nodes, the\nquality of the nodes. While the data setting focus on diﬀerent data distribution\ncaused by either label bias or feature bias.\nCommunication Evaluation There is no doubt that the communication rounds of\nnodes play an important role in the performance of the model. Due to the uncer-\ntainty of the federated network, communication is huge resource consumption.\nThere is always a natural trade oﬀbetween computation and communication\namong the nodes and server.\nPrivacy Evaluation The goal of privacy metrics is to measure the degree of\nprivacy enjoyed by users in a system and the amount of protection oﬀered by\nprivacy-enhancing technologies. [59] discussed a selection of over eighty privacy\nmetrics and introduce categorizations based on the aspect of privacy they mea-\nsure, their required inputs, and the type of data that needs protection. In general,\nprivacy metrics can be classiﬁed with four common characteristics: adversary\nmodels, data sources, inputs and output meatures.\nUser Response Evaluation Apart from the above automatic evaluation meth-\nods, on-line FL-based NLP models also consider the response from users, e.g.\nFL language models take next word click rate as an important metric, FL rec-\nommendation systems would not only like to keep old customers but also attract\nnew customers.\n4.2\nTools\nThere are a few tools for common federated learning, including PySyft 3, TFF\n4, FATE 5, Tensor/IO 6, FedML 7 and FedNLP 8. PySyft decouples private\ndata from model training using federated learning, DP and MPC within Py-\nTorch. With TFF, TensorFlow provides users with a ﬂexible and open framework\nthrough which they can simulate distributed computing locally. FATE support\nthe Federated AI ecosystem, where a secure computing protocol is implemented\nbased on homomorphic encryption and MPC. Tensor/IO is a lightweight cross-\nplatform library for on-device machine learning, bringing the power of Tensor-\nFlow and TensorFlow Lite to iOS, Android, and React native applications.\n3 https://github.com/OpenMined/PySyft\n4 https://www.tensorﬂow.org/federated\n5 https://github.com/FederatedAI/FATE\n6 https://doc-ai.github.io/tensorio/\n7 https://github.com/FedML-AI/FedML\n8 https://github.com/FedML-AI/FedNLP\nFederated Learning Meets Natural Language Processing: A Survey\n15\n5\nChallenges and Future Directions\n5.1\nAlgorithm-Level\nBig language models Since the paradigm pre-training + ﬁne tuning has domi-\nnated most NLP tasks, pre-trained language models such as BERT and GPT are\nuseful and transferable to develop downstream tasks. Often times the larger the\npre-trained language model is, the more likely it will be for downstream model\nperformance. However, in the FL setting, it is not possible to allocate large size\nlanguage models like GPT-3 on the participants. Technique like knowledge dis-\ntillation could be useful, it remains unknown whether downsized language can\nmaintain the performance.\nNon-iid Data distributions Real world data from diﬀerent participants is always\nnon-iid, the challenge is how to learn from high quality data and labels. Given\na ﬁxed annotation budget, active learning may be leveraged to not only select\nsigniﬁcant data points, but also actively choose worthwhile participants. Fur-\nthermore, weakly supervised learning and meta learning algorithms may also be\nused to use more unlabeled data from diﬀerent participants.\nPersonalization Personalized FL can be viewed as an intermediate paradigm be-\ntween the server-based FL paradigm that produces a global model and the local\nmodel training paradigm. The challenge is to strike a careful balance between\nlocal task-speciﬁc knowledge and shared knowledge useful for the generaliza-\ntion properties of FL models. For most deep NLP models, techniques like early\nshaping, sample weighing and transfer learning can be explored.\n5.2\nSystem-Level\nSpatial Adaptability This refers to the ability of the FL system to handle varia-\ntions across client data sets as a result of (i) the addition of new clients, and/or\n(ii) dropouts and stragglers. These are practical issues prevalent in complex\nedge computing environments, where there is signiﬁcant variability in hardware\ncapabilities in terms of computation, memory, power and network connectivity\nComputation Communication Trade-oﬀFrequent and large scale deployment\nof updates, monitoring, and debugging for FL NLP models is challenging. The\ntrade-oﬀbetween local and global update frequency, as well as the communica-\ntion frequency can be explored.\nPrivacy concern Even though FL assumes the data never leave the device, it\nis still possible to reconstruct the original data by taking the model weights or\ngradients. Privacy preservation on three aspects in FL can be explored: users’\npersonal information, local data and machine learning models.\n16\nL. Ming et al.\n6\nConclusion\nIn this paper, we review common NLP tasks in the FL setting, including language\nmodeling, text classiﬁcation, speech recognition, sequence tagging, recommenda-\ntion, health text mining and other tasks. In general, the performance federated\nNLP models still lie behind that of centralized ones. Also, large scale pre-trained\nlanguage models and advanced privacy preservation techniques have not widely\nbeen used in the FL based NLP, which could be the potentials for future re-\nsearch. We point out both algorithm and system level challenges for FL based\nNLP models. In the future, we will further evaluate representative NLP models\n(e.g. transformers) in the FL environment and give more comparable insights on\nreal world applications.\nReferences\n1. Aggarwal, C.C., Zhai, C.: A survey of text classiﬁcation algorithms. In: Mining\ntext data, pp. 163–222. Springer (2012)\n2. Ammad-Ud-Din, M., Ivannikova, E., Khan, S.A., Oyomno, W., Fu, Q., Tan, K.E.,\nFlanagan, A.: Federated collaborative ﬁltering for privacy-preserving personalized\nrecommendation system. arXiv preprint arXiv:1901.09888 (2019)\n3. Baevski, A., Zhou, H., Mohamed, A., Auli, M.: wav2vec 2.0: A framework for\nself-supervised learning of speech representations. arXiv preprint arXiv:2006.11477\n(2020)\n4. Brisimi, T.S., Chen, R., Mela, T., Olshevsky, A., Paschalidis, I.C., Shi, W.: Feder-\nated learning of predictive models from federated electronic health records. Inter-\nnational journal of medical informatics 112, 59–67 (2018)\n5. Chen, M., Mathews, R., Ouyang, T., Beaufays, F.: Federated learning of out-of-\nvocabulary words. ArXiv abs/1903.10635 (2019)\n6. Chen,\nM.,\nSuresh,\nA.T.,\nMathews,\nR.,\nWong,\nA.,\nAllauzen,\nC.,\nBeau-\nfays,\nF.,\nRiley,\nM.:\nFederated\nlearning\nof\nn-gram\nlanguage\nmodels.\nIn:\nProceedings\nof\nthe\n23rd\nConference\non\nComputational\nNatural\nLanguage\nLearning\n(CoNLL).\npp.\n121–130.\nAssociation\nfor\nComputational\nLinguis-\ntics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/K19-1012,\nhttps://www.aclweb.org/anthology/K19-1012\n7. Cheng, K., Fan, T., Jin, Y., Liu, Y., Chen, T., Yang, Q.: Secureboost: A lossless\nfederated learning framework. arXiv preprint arXiv:1901.08755 (2019)\n8. Cho, K., Merrienboer, B.V., Gulcehre, C., Ba Hdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning phrase representations using rnn encoder-decoder for\nstatistical machine translation. Computer Science (2014)\n9. Dwork, C., Mcsherry, F., Nissim, K., Smith, A.: Calibrating noise to sensitivity\nin private data analysis. In: Proceedings of the Third conference on Theory of\nCryptography (2006)\n10. Dwork, C.: Diﬀerential privacy: A survey of results. In: International conference\non theory and applications of models of computation. pp. 1–19. Springer (2008)\n11. Ekberg, J.E., Kostiainen, K., Asokan, N.: Trusted execution environments on mo-\nbile devices. In: Proceedings of the 2013 ACM SIGSAC conference on Computer\n& communications security. pp. 1497–1498 (2013)\n12. Fallah, A., Mokhtari, A., Ozdaglar, A.: Personalized federated learning: A meta-\nlearning approach. arXiv preprint arXiv:2002.07948 (2020)\nFederated Learning Meets Natural Language Processing: A Survey\n17\n13. Feige, U., Fiat, A., Shamir, A.: Zero-knowledge proofs of identity. Journal of cryp-\ntology 1(2), 77–94 (1988)\n14. Ge, S., Wu, F., Wu, C., Qi, T., Huang, Y., Xie, X.: Fedner: Privacy-preserving\nmedical named entity recognition with federated learning. arXiv e-prints pp. arXiv–\n2003 (2020)\n15. Goldreich, O.: Secure multi-party computation. Manuscript. Preliminary version\n78 (1998)\n16. Graves, A., Jaitly, N.: Towards end-to-end speech recognition with recurrent neural\nnetworks. In: International conference on machine learning. pp. 1764–1772. PMLR\n(2014)\n17. Greﬀ, K., Srivastava, R., Koutn´ık, J., Steunebrink, B., Schmidhuber, J.: Lstm:\nA search space odyssey. IEEE Transactions on Neural Networks and Learning\nSystems 28, 2222–2232 (2017)\n18. Guha, N., Talwalkar, A., Smith, V.: One-shot federated learning. arXiv preprint\narXiv:1902.11175 (2019)\n19. Guliani, D., Beaufays, F., Motta, G.: Training speech recognition models with fed-\nerated learning: A quality/cost framework. arXiv preprint arXiv:2010.15965 (2020)\n20. Hard, A., Rao, K., Mathews, R., Beaufays, F., Augenstein, S., Eichner, H., Kid-\ndon, C., Ramage, D.: Federated learning for mobile keyboard prediction. ArXiv\nabs/1811.03604 (2018)\n21. Heged˝us, I., Danner, G., Jelasity, M.: Decentralized recommendation based on\nmatrix factorization: a comparison of gossip and federated learning. In: Joint Eu-\nropean Conference on Machine Learning and Knowledge Discovery in Databases.\npp. 317–332. Springer (2019)\n22. Hilmkil, A., Callh, S., Barbieri, M., S¨utfeld, L.R., Zec, E.L., Mogren, O.: Scal-\ning federated learning for ﬁne-tuning of large language models. arXiv preprint\narXiv:2102.00875 (2021)\n23. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 (2015)\n24. Hochreiter, Sepp, Schmidhuber, Jurgen: Long short-term memory. Neural Compu-\ntation (1997)\n25. Hsieh, K., Phanishayee, A., Mutlu, O., Gibbons, P.: The non-iid data quagmire of\ndecentralized machine learning. In: International Conference on Machine Learning.\npp. 4387–4398. PMLR (2020)\n26. Hu, Y., Niu, D., Yang, J., Zhou, S.: Fdml: A collaborative machine learning frame-\nwork for distributed features. In: Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining. pp. 2232–2240 (2019)\n27. Huang, L., Shea, A.L., Qian, H., Masurkar, A., Deng, H., Liu, D.: Patient clustering\nimproves eﬃciency of federated machine learning to predict mortality and hospi-\ntal stay time using distributed electronic medical records. Journal of biomedical\ninformatics 99, 103291 (2019)\n28. Jalalirad, A., Scavuzzo, M., Capota, C., Sprague, M.: A simple and eﬃcient fed-\nerated recommender system. In: Proceedings of the 6th IEEE/ACM International\nConference on Big Data Computing, Applications and Technologies. pp. 53–58\n(2019)\n29. Ji, S., Pan, S., Long, G., Li, X., Jiang, J., Huang, Z.: Learning private neural\nlanguage modeling with attentive aggregation. 2019 International Joint Conference\non Neural Networks (IJCNN) pp. 1–8 (2019)\n30. Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,\nBonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open\nproblems in federated learning. arXiv preprint arXiv:1912.04977 (2019)\n18\nL. Ming et al.\n31. Kim, Y., Sun, J., Yu, H., Jiang, X.: Federated tensor factorization for compu-\ntational phenotyping. In: Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining. pp. 887–895 (2017)\n32. Kuchaiev, O., Ginsburg, B.: Factorization tricks for lstm networks (2017)\n33. Lee, J., Sun, J., Wang, F., Wang, S., Jun, C.H., Jiang, X.: Privacy-preserving\npatient similarity learning in a federated environment: development and analysis.\nJMIR medical informatics 6(2), e20 (2018)\n34. Leroy, D., Coucke, A., Lavril, T., Gisselbrecht, T., Dureau, J.: Federated learning\nfor keyword spotting. In: ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). pp. 6341–6345. IEEE (2019)\n35. Li, M., Liu, Z., Smola, A.J., Wang, Y.X.: Difacto: Distributed factorization ma-\nchines. In: Proceedings of the Ninth ACM International Conference on Web Search\nand Data Mining. pp. 377–386 (2016)\n36. Lin, T., Kong, L., Stich, S.U., Jaggi, M.: Ensemble distillation for robust model\nfusion in federated learning (2021)\n37. Lincy, M., Kowshalya, A.M.: Early detection of type-2 diabetes using federated\nlearning (2020)\n38. Liu, D., Dligach, D., Miller, T.: Two-stage federated phenotyping and patient rep-\nresentation learning. arXiv preprint arXiv:1908.05596 (2019)\n39. Liu, D., Miller, T.: Federated pretraining and ﬁne tuning of bert using clinical\nnotes from multiple silos. arXiv preprint arXiv:2002.08562 (2020)\n40. McMahan, H.B., Moore, E., Ramage, D., y Arcas, B.A.: Federated learn-\ning of deep networks using model averaging. CoRR abs/1602.05629 (2016),\nhttp://arxiv.org/abs/1602.05629\n41. Minaee, S., Kalchbrenner, N., Cambria, E., Nikzad, N., Chenaghlu, M., Gao, J.:\nDeep learning–based text classiﬁcation: A comprehensive review. ACM Computing\nSurveys (CSUR) 54(3), 1–40 (2021)\n42. M¨uller, M.: Dynamic time warping. Information retrieval for music and motion pp.\n69–84 (2007)\n43. Naehrig, M., Lauter, K., Vaikuntanathan, V.: Can homomorphic encryption be\npractical? In: Proceedings of the 3rd ACM workshop on Cloud computing security\nworkshop. pp. 113–124 (2011)\n44. Pazzani, M.J., Billsus, D.: Content-based recommendation systems. In: The adap-\ntive web, pp. 325–341. Springer (2007)\n45. Rabiner, L., Juang, B.: An introduction to hidden markov models. ieee assp mag-\nazine 3(1), 4–16 (1986)\n46. Radford, A.: Language models are unsupervised multitask learners\n47. Ramaswamy, S.I., Mathews, R., Rao, K., Beaufays, F.: Federated learning for emoji\nprediction in a mobile keyboard. ArXiv abs/1906.04329 (2019)\n48. Sattler, F., Marban, A., Rischke, R., Samek, W.: Communication-eﬃcient federated\ndistillation. arXiv preprint arXiv:2012.00632 (2020)\n49. Sharma, P., Shamout, F.E., Clifton, D.A.: Preserving patient privacy while training\na predictive model of in-hospital mortality. arXiv preprint arXiv:1912.00354 (2019)\n50. Shmueli, E., Tassa, T.: Secure multi-party protocols for item-based collaborative\nﬁltering. In: Proceedings of the eleventh ACM conference on recommender systems.\npp. 89–97 (2017)\n51. Slavkovic, A.B., Nardi, Y., Tibbits, M.M.: ” secure” logistic regression of horizon-\ntally and vertically partitioned distributed databases. In: Seventh IEEE Interna-\ntional Conference on Data Mining Workshops (ICDMW 2007). pp. 723–728. IEEE\n(2007)\nFederated Learning Meets Natural Language Processing: A Survey\n19\n52. Stremmel, J., Singh, A.: Pretraining federated text models for next word prediction.\nArXiv abs/2005.04828 (2020)\n53. Stripelis, D., Ambite, J.L., Lam, P., Thompson, P.: Scaling neuroscience research\nusing federated learning. arXiv preprint arXiv:2102.08440 (2021)\n54. Sucholutsky, I., Schonlau, M.: Soft-label dataset distillation and text dataset dis-\ntillation. arXiv preprint arXiv:1910.02551 (2019)\n55. Suganeshwari, G., Ibrahim, S.S.: A survey on collaborative ﬁltering based rec-\nommendation system. In: Proceedings of the 3rd International Symposium on Big\nData and Cloud Computing Challenges (ISBCC–16’). pp. 503–518. Springer (2016)\n56. Sui, D., Chen, Y., Zhao, J., Jia, Y., Xie, Y., Sun, W.: Feded: Federated learning via\nensemble distillation for medical relation extraction. In: Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP). pp.\n2118–2128 (2020)\n57. Suresh, A.T., Roark, B., Riley, M., Schogol, V.: Approximating probabilistic mod-\nels as weighted ﬁnite automata. ArXiv abs/1905.08701 (2019)\n58. Thakkar, O., Ramaswamy, S.I., Mathews, R., Beaufays, F.: Understanding unin-\ntended memorization in federated learning. ArXiv abs/2006.07490 (2020)\n59. Wagner, I., Eckhoﬀ, D.: Technical privacy metrics: a systematic survey. ACM Com-\nputing Surveys (CSUR) 51(3), 1–38 (2018)\n60. Wan, C.P., Chen, Q.: Robust federated learning with attack-adaptive aggregation.\narXiv preprint arXiv:2102.05257 (2021)\n61. Wang, T., Zhu, J.Y., Torralba, A., Efros, A.A.: Dataset distillation. arXiv preprint\narXiv:1811.10959 (2018)\n62. Wei, K., Li, J., Ding, M., Ma, C., Yang, H.H., Farhad, F., Jin, S., Quek, T.,\nPoor, H.: Federated learning with diﬀerential privacy: Algorithms and performance\nanalysis. IEEE Transactions on Information Forensics and Security 15, 3454–3469\n(2020)\n63. Williams, R.J., Zipser, D.: A learning algorithm for continually running fully re-\ncurrent neural networks. Neural Computation 1(2) (1998)\n64. Wu, Q., Chen, X., Zhou, Z., Zhang, J.: Fedhome: Cloud-edge based personalized\nfederated learning for in-home health monitoring. IEEE Transactions on Mobile\nComputing (2020)\n65. Yang, C.H.H., Qi, J., Chen, S.Y.C., Chen, P.Y., Siniscalchi, S.M., Ma, X., Lee,\nC.H.: Decentralizing feature extraction with quantum convolutional neural network\nfor automatic speech recognition. arXiv preprint arXiv:2010.13309 (2020)\n66. Yang, L., Tan, B., Zheng, V.W., Chen, K., Yang, Q.: Federated recommendation\nsystems. In: Federated Learning, pp. 225–239. Springer (2020)\n67. Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., Beau-\nfays, F.: Applied federated learning: Improving google keyboard query suggestions.\nArXiv abs/1812.02903 (2018)\n68. Zhang, Y., Suda, N., Lai, L., Chandra, V.: Hello edge: Keyword spotting on mi-\ncrocontrollers. arXiv preprint arXiv:1711.07128 (2017)\n69. Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning\nwith non-iid data. arXiv preprint arXiv:1806.00582 (2018)\n70. Zhou, Y., Pu, G., Ma, X., Li, X., Wu, D.: Distilled one-shot federated learning\n(2020)\n71. Zhu, X., Wang, J., Hong, Z., Xiao, J.: Empirical studies of institutional federated\nlearning for natural language processing. In: Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: Findings. pp. 625–634 (2020)\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.DC"
  ],
  "published": "2021-07-27",
  "updated": "2021-07-27"
}