{
  "id": "http://arxiv.org/abs/2107.12732v1",
  "title": "Towards Black-box Attacks on Deep Learning Apps",
  "authors": [
    "Hongchen Cao",
    "Shuai Li",
    "Yuming Zhou",
    "Ming Fan",
    "Xuejiao Zhao",
    "Yutian Tang"
  ],
  "abstract": "Deep learning is a powerful weapon to boost application performance in many\nfields, including face recognition, object detection, image classification,\nnatural language understanding, and recommendation system. With the rapid\nincrease in the computing power of mobile devices, developers can embed deep\nlearning models into their apps for building more competitive products with\nmore accurate and faster responses. Although there are several works about\nadversarial attacks against deep learning models in mobile apps, they all need\ninformation about the models' internals (i.e., structures, weights) or need to\nmodify the models. In this paper, we propose an effective black-box approach by\ntraining a substitute model to spoof the deep learning system inside the apps.\nTo evaluate our approach, we select 10 real-world deep-learning apps with high\npopularity from Google Play to perform black-box adversarial attacks. Through\nthe study, we find three factors that can influence the performance of attacks.\nOur approach can reach a relatively high attack success rate of 66.60% on\naverage. Compared with other adversarial attacks on mobile deep learning\nmodels, in terms of the average attack success rates, our approach outperforms\ncounterparts by 27.63%.",
  "text": "Towards Black-box Attacks on Deep Learning Apps\nHongchen Cao\nShanghaiTech University\ncaohch1@shanghaitech.edu.cn\nShuai Li\nThe Hong Kong Polytechnic University\ncsshuaili@comp.polyu.edu.hk\nYuming Zhou\nNanjing University\nzhouyuming@nju.edu.cn\nMing Fan\nXi’an Jiaotong University\nmingfan@xjtu.edu.cn\nXuejiao Zhao\nNanyang Technological University\nxjzhao@ntu.edu.sg\nYutian Tang*\nShanghaiTech University\ntangyt1@shanghaitech.edu.cn\nAbstract—Deep learning is a powerful weapon to boost ap-\nplication performance in various ﬁelds, including face recog-\nnition, object detection, image classiﬁcation, natural language\nunderstanding, and recommendation system. With the rapid\nincrease in the computing power of mobile devices, developers\ncan embed deep learning models into their apps for building more\ncompetitive products with more accurate and faster responses.\nAlthough there are several works of adversarial attacks against\ndeep learning models in apps, they all need information about\nthe models’ internals (i.e., structures, weights) or need to modify\nthe models. In this paper, we propose an effective black-box\napproach by training substitute models to spoof the deep learning\nsystems inside the apps. We evaluate our approach on 10 real-\nworld deep-learning apps from Google Play to perform black-\nbox adversarial attacks. Through the study, we ﬁnd three factors\nthat can affect the performance of attacks. Our approach can\nreach a relatively high attack success rate of 66.60% on average.\nCompared with other adversarial attacks on mobile deep learning\nmodels, in terms of the average attack success rates, our approach\noutperforms counterparts by 27.63%.\nIndex Terms—Black-box attacks, Deep Learning apps, Android\nI. INTRODUCTION\nThe explosive progress of deep learning techniques makes\nan increasing number of app developers pay attention to\nthis technology. Deep learning apps reach a certain scale\nin the market and keep growing rapidly [1]. To lower the\nthreshold for developers to utilize and deploy deep learning\nmodels on their apps, companies (i.e., Google) develop deep\nlearning frameworks, including Tensorﬂow Lite [2], Pytorch\nMobile [3], Caffe2 Mobile [4], MindSpore Lite [5], Core\nML [6] and so on.\nTo build a deep learning app, mainstream platforms offer\ntwo common deploying strategies, on-device deployment and\non-cloud deployment [7]. On-cloud deployment allows de-\nvelopers to deploy their models on the remote server, and\ncollect the runtime inputs from app users. Then, the inputs\nare processed by the remote server. Finally, the results are\nreturned to app users. However, the efﬁciency of this method\nis often limited by the network quality and power usage [8].\nEven worse, on-cloud deployment has potential risks of user\nprivacy leakage [9]. Therefore, lots of developers turn to the\non-device deployment strategy.\nMotivation. The aforementioned mainstream mobile deep\nlearning frameworks and deployment platforms rarely provide\ndevelopers with solutions or APIs to protect their deep learning\nmodels. Unprotected deep learning models are widely used\nin apps in various ﬁelds such as ﬁnance, health, and self-\ndriving [10]. This may lead to potential user data leakage,\ncommercial information theft, and the malfunction of im-\nportant functional modules of the app (e.g. a polluted face\nrecoginition model fails to recoginize an authorized user).\nState-of-art. Although some studies [11] focus on attacks\non mobile deep learning models, their approaches are often\nwhite-box, which means that the attacker can know or even\nmodify the structure (the number, type, and arrangement of\nlayers) and weights (parameters of each layer) of the model.\nThese white-box attacks rely on the source of victim models,\nwhich can be extracted by decompressing the victim app.\nHowever, a recent study shows that developers are more likely\nto use encryption or hash code to protect their models [10].\nIt means that the knowledge of a model (i.e., structure and\nweights) inside the app is always unknown, which nulliﬁes\nthese white-box attacks. Work [12] claims to be a black-box\nattack method towards deep learning models. However, they\nstill require knowledge of the structure or weight of the model.\nSuch information is commonly unknown in the real world,\nespecially for commercial apps.\nOur solution. In this paper, we developed a practical pipeline\nthat covers data preparation, student model (i.e., a substitute\nmodel to mimic the behavior of the victim model) learning,\nand adversarial examples generation. Our black-box attack ap-\nproach can bypass many existing model protection measures.\nFirst, we instrument a DummyActivity to the victim app to\ninvoke the deep learning model (a.k.a teacher model) adopted\nby the victim app. The raw data fetched from the Internet\nor public dataset are fed into the teacher model through the\nDummyActivity to generate the corresponding labels (See Sec.\nIII-A). Next, a substitute model (a.k.a. student model), which\nis selected to mimic the behavior of the teacher model, is then\ntrained with the labeled data (See Sec. III-B). Last, adversarial\nattack algorithms are applied to the student model, and the\ngenerated adversarial examples are leveraged to attack the\nteacher model in the victim app (See Sec. III-C).\nContributions. In summary, our main contributions are de-\narXiv:2107.12732v1  [cs.SE]  27 Jul 2021\nscribed as follows:\n• We propose a black-box attack pipeline, from the model\ncollection to adversarial example generation, to attack deep\nlearning apps.\n• We conduct a series of experiments to evaluate how different\nfactors affect the attack success rate, including the structure\nof the student models, the scale of the teaching dataset, and\nthe adversarial attack algorithms; and\n• We propose a series of strategies to defend against the\nproposed attacks.\nWe release the source code and data on the online arte-\nfact [13].\nII. BACKGROUND\nA. On-device Deep Learning Model\nDevelopers are increasingly using deep learning models to\nimplement face recognition [14], image classiﬁcation [15],\nobject detection [15], natural language understanding [16],\nspeech recognition [17], and recommendation systems [18] in\ntheir apps.\nTo meet the needs of app developers, vendors build deep\nlearning frameworks for mobile platforms, such as TensorFlow\nLite [2] and PyTorch Mobile [3]. They also provide platforms\n(e.g. TensorFlow Hub [19], MindSpore Model [20]) to share\npre-trained deep learning models for mobile apps.\nThere are two common deploying strategies, on-device de-\nployment and on-cloud deployment. Developers nowadays are\ninclined to use the on-device method to deploy deep learning\nmodels. On the one hand, the hardware of modern mobile\ndevices is qualiﬁed to complete computationally intensive deep\nlearning tasks. As the CPU and GPU of modern mobile devices\nhave stronger computing power, on-device inference can be\ncompleted promptly. The neural processing units (NPU) in\nmobile devices further expand the feasibility of on-device\ndeployment of deep learning models [21].\nOn the other hand, deep learning frameworks provide simple\nand easy-to-use APIs to invoke the model for inference tasks.\nFor example, as shown in List. 1, Interpreter deﬁned in the\nTensorFlow Lite is used to load the model and run inference.\nﬁle of a tﬂite model represents the ﬁle path of the deep\nlearning model. input presents the user’s input and output\nrepresents the result of the inference which is a vector. Based\non the output, the app can give a response to the user. For\nexample, the user feeds an image of a rose as input into the\nInterpreter. After inference, the output is assigned to a one-\nhot vector like [0, 0, 1, 0, ..., 0]. According to pre-determined\nrules from developers, the vector is then interpreted into a\nhuman-readable format such as a string ”rose”.\nListing 1: TensorFlow Lite API\n1\n# TensorFlow Lite\n2\nInterpreter interpreter = new\nInterpreter(file_of_a_tflite_model);\n3\ninterpreter.run(input, output);\nIn the apps, developers can deﬁne the locations of their deep\nlearning models. A common practice is to store the models\nin the assets folder (/assets/) of the apk (Android Package,\nwhich is the binary format of an app). Data including user\ninput, output vector, and training dataset are invisible to users.\nAlthough models in some deep learning apps can be directly\nextracted by decompiling the apk, lots of them still cannot\nbe fetched even with the state-of-art tools [10]. Therefore, the\ninformation about the structures and weights of these deep\nlearning models is unknown to attackers.\nB. Black-box Attack\nSince the information about the victim is always unknown,\nattacks must be performed in a black-box manner. A black-\nbox attack is not a speciﬁc attack technique but describes\nan attack scenario. In this scenario, the attackers have no\ninternal knowledge of the victim, and cannot modify the\ninternal structure and other attributes of the victim. In our\ncontext, the attackers have no internal knowledge of the deep\nlearning model in the victim app and cannot modify the\ninternal attributes of the model. The attackers can only query\nthe model and get its prediction.\nC. Adversarial Attack\nDeep learning models are vulnerable to adversarial attacks.\nThe adversarial attack utilizes adversarial examples to spoof\nthe deep learning models. An adversarial example is generated\nby adding tiny perturbations to the input data. It can be\nrepresented as xadv = x + ϵ ∗p, where xadv is the adversarial\nexample, x is the input data, p is the perturbations and ϵ is\nthe multiplier to control the degree of the perturbations [22],\n[12]. An adversarial attack can be brieﬂy summarized in the\nfollowing example. With a deep learning model for image\nclassiﬁcation M(x) and an input image x, ytrue is the label\nof x (ytrue = M(x)). Attackers can generate an adversarial\nexample x′ by adding inﬁnitesimal perturbations that a human\nbeing cannot distinguish from the original image x. But the\nadversarial example x′ can spoof the victim deep learning\nmodel to make an incorrect prediction (M(x′) ̸= ytrue).\n(a) orignal input\n(b) adversarial example\nFig. 1: Comparsion for orignal input and adversarial example\nFor example, Fig. 1(a) is the original input image, which is\nlabeled as ’rose’ by a deep learning model M. After applying\nadversarial attacks to it, an adversarial example is generated\nas shown in Fig. 1(b), which is labeled as ”tulips” by M.\n2\nVictim \nAPP\nUnlabeled \nData\nLabeled \nData\nUntrained \nStudent Model\nTrained \nStudent Model\nAttacker’s \nData\nAdversarial \nExample\nTeaching Dataset \nPreparation (Sec.Ⅲ-A)\nStudent Model \nLearning (Sec.Ⅲ-B)\nAdversarial Examples \nGeneration (Sec.Ⅲ-C)\nInstrument\nDummyActivity\nInstrumented\nAPP\nData\nInference\nModel\nTrainer\nAdversarial\nGenerator\nAdversarial\nAttack\nInput\nOutput\nTeacher Model\nFig. 2: Overview of adversarial attack pipeline\nIII. METHODOLOGY\nBefore introducing the details of our approach, we ﬁrst\ndeﬁne two key concepts used in our work: teacher model and\nstudent model. A teacher model is the deep learning model that\nis used in the victim app. A student model can be considered\nas a substitute or imitator of the teacher model. A qualiﬁed\nstudent model can imitate the teacher’s behaviors well. With\nthe same input, the student and the teacher model are supposed\nto output the same prediction. A well-behaved student model\ncan guarantee that the adversarial examples generated can be\ndirectly transferred to the teacher model.\nOur black-box attack pipeline, as illustrated in Fig. 2,\nconsists of three procedures: teaching dataset preparation,\nstudent model learning, and adversarial examples generation.\n• Input: The input of our pipeline is a deep learning app\nand an unlabeled dataset. The dataset is either crawled from\nthe web or a public dataset that is related to the app’s task.\nFor example, the dataset for an app whose task is ﬂower\nclassiﬁcation can be images of different kinds of ﬂowers.\n• Teaching dataset preparation: Given a deep learning app,\nwe ﬁrst locate and learn how the deep learning model is\ninvoked by tracking the ofﬁcial APIs from deep learning\nframeworks in the app. Then, we instrument a DummyActivity,\nin which the teacher model is invoked, to the victim app with\nSoot [23], [24]. Next, we feed the unlabeled dataset into the\nDummyActivity to generate the corresponding labels (see Sec.\nIII-A).\n• Student model learning: To imitate the teacher model, we\nselect mainstream deep learning models with the same task\nas the student model candidates. With the labeled teaching\ndataset generated, a student model can be trained to imitate\nthe teacher model (see Sec. III-B).\n• Adversarial examples generation: With the trained student\nmodel, adversarial examples can be generated with adversarial\nattack algorithms (see Sec. III-C).\n• Output: The output of the whole pipeline is a set of\nadversarial examples, which are used to attack the victim app.\nA. Teaching Dataset Preparation\nFor a deep learning app, we identify some basic attributes of\nthe teacher model in the app. These attributes mainly consist\nof the type of its task, the name of classes it predicts, and the\nformatting requirement of the input.\n•Type of task and Name of classes : Determining the task\nof the deep learning model is a prerequisite for collecting a\nsuitable dataset. The name of the output (i.e., classes) of the\ndeep learning model can further help narrow the selection\nrange of the dataset. This information can be obtained by\ntrying the app.\n•Formatting requirement of the input : The input format of\nthe deep learning model is determined by the model developer.\nThese formatting requirements include size (e.g., 128*128),\ndata type (e.g., int8, ﬂoat32), and so forth. By reverse engi-\nneering the app, these requirements can be obtained manually.\nFor a plant identiﬁer app, its task is image classiﬁcation.\nIts names of classes can be the names of plants (e.g, lily,\nplane tree). Its formatting requirement of the input can be a\n255×255×3 RGB image.\nBased on this information, we search a public dataset D\nfrom the web. However, it is common that no public related\ndataset is available for real-world apps. There are two options\nto collect the dataset for such apps. The ﬁrst is to use a\npre-trained student model without ﬁne-tuning by the teaching\ndataset, which is simple and fast. The second is to crawl the\nrelated data from the Internet. We compare these two options\nwith experiments in Sec. IV-C.\nSince the input format of the deep learning model is\ndetermined by the model developer, the collected dataset needs\nto be preprocessed to make it meet the speciﬁc formatting\nrequirements. Based on the previously obtained information\nabout the input formatting requirements, the input data can\nbe manipulated (e.g., zoom in size, update ﬁgure accuracy) to\nmeet the input formatting requirements.\nThe dataset we crawled is unlabeled. However, to better\nimitate the teacher model, we need to train the student model\n3\nListing 2: API pattern of invoking the deep learning model\n1\npublic class Xception {\n2\npublic Xception (...,\nﬁnal\nString str ,\n...)\n{\n3\n...\n4\nﬁnal\nInterpreter .Options options = new\nInterpreter .Options() ;\n5\nthis . tﬂite\n= new Interpreter (map, options);}\n6\n}\n7\npublic abstract\nclass AIActivity extends CameraActivity {\n8\nprotected void onCreate (...)\n{\n9\nthis . xception = new Xception (...,\n”xception103. tﬂite ”,\n...) ;}\n10\n}\nwith a labeled dataset. Therefore, we need to extract the labels\nof the dataset with the teacher model. For a black-box attack,\nit can be unrealistic to directly retrieve the teacher model from\nthe app. Therefore, we perform the following steps to transfer\nan unlabeled dataset D into a labeled one D′:\n• STEP 1: We collect the API patterns of mainstream mobile\ndeep learning model deployment frameworks. The correspond-\ning APIs can be found in the ofﬁcial docs (i.e., Tensorﬂow Lite\ninference [25], Pytorch Mobile inference [26]).\n• STEP 2: We use Soot to locate the signatures of these APIs\nin the app.\n• STEP 3: We instrument DummyActivity into the app with\nSoot and load the deep learning model inside the onCreate()\nmethod of DummyActivity with the same API found in STEP\n2.\n• STEP 4: We pass the dataset D to the inference API and\noutput the corresponding labels with logs.\n• STEP 5: We leverage the Android Debug Bridge (ADB) to\nlaunch our DummyActivity (i.e., am start -n DummyActivity)\nto trigger the teacher model. As a result, we can obtain the\ncorresponding labels for the inputs.\nExample.\nThe\nMushroom\nIdentiﬁer\napp\n(com.gabotechindustries.mushroomIdentifier)\ndetects\nand identiﬁes mushrooms in the picture and predicts their\ncategories. This app has a class called ”Xception”. List. 2\nsummarizes how this app invokes the deep learning model in\n”Xception” with the APIs provided by TensorFlow Lite.\nAs illustrated in List. 3, we embed the extracted pattern\n(in List. 2) into DummyActivity. Then, we instrument the\nDummayActivity to the app.\nAfter instrumenting the DummyActivity to the app, we\nlaunch the DummyActivity with an ADB command (i.e., am\nstart -n DummyActivity). As a result, the corresponding la-\nbels of D can be obtained. The labeled dataset D′ is then\nleveraged to train the student model.\nB. Student Model Learning\nSelection criteria for student model. The adversarial exam-\nples generated based on the student model are used to attack\nthe teacher model. The selection criteria of the student model\nis that it should be able to complete the same task as the\nListing 3: Instrumented DummyActivity\n1\nclass DummyActivity extends AppCompatActivity {\n2\npublic void onCreate(Bundle savedInstanceState ) {\n3\n...\n4\n//\nInit and run the\nInterpreter\n5\nInterpreter\ninterpreter\n= new\nInterpreter (”xception103. tﬂite ”);\n6\ninterpreter .run( input , output);\n7\nint maxIdx = output.indexOf(output .max()); // Get label\n8\nLog.i(”DummyActivity”,maxIdx.toString()); // Output label\n9\n}\n10\n...\n11\n}\nteacher model. Different student models can result in different\nattack success rates. We discuss this correlation in Sec. IV-B.\nStudent model learning. In our approach, the student model\ncan be trained in a supervised way with the teaching dataset\ngenerated in the previous step. In supervised learning, the\nmodel is trained on a labeled dataset, where the labels are\nused to evaluate the model’s prediction accuracy. To improve\nthe prediction accuracy, the model needs to adjust the weights\nin the process of training. The adjusting strategy relies on\nthe error between the model’s predictions and the ground\ntruths(i.e., labels). The loss function is used to calculate this\nerror. For example, the CrossEntropyLoss is widely used in\nimage classiﬁcation [27].\nExample. Recall the app in Sec. III-A, its task is to identify\nmushrooms in an input image and predict their categories.\nThus, the selected student model must be a model related\nto image classiﬁcation. The image classiﬁcation model VG-\nGll [28] can be used as the student model to be trained with\nthe labeled teaching dataset generated in Sec. III-A. Note that\na general image classiﬁcation model can be trained to identify\nmushrooms with a suitable dataset.\nC. Adversarial Examples Generation\nWith the trained student model in the previous step, the\nnext step is to use the student model to generate adversarial\nexamples that can spoof the teacher model. The adversarial\nexamples are proved to have the property of transferability [29]\nwhich means that the adversarial examples generated for one\nmodel through an attack algorithm can also attack another\nmodel. Motivated by this, adversarial attack algorithms are\nutilized to generate adversarial examples based on the stu-\ndent model, and these generated adversarial examples can\nbe directly applied to attack the teacher model in the victim\napp. Note that different domains (e.g., NLP, computer vision)\nrequire different adversarial attack algorithms. For example, in\ncomputer vision, the following algorithms can be representa-\ntive: FGSM [22], RFGSM [30], FFGSM [31], MIFGSM [32],\nPGD [33], TPGD [34], and BIM [35]. Different attack algo-\nrithms can result in different attack performances. We discuss\nthis correlation in Sec. IV-D.\n4\n25\n30\n35\n40\n45\n50\n55\nAttack Success Rates(%)\nVGGs\nResnets\nDensenets\nSmall nets\nStudent Networks\n51.44\n35.38\n37.82\n29.4\nFig. 3: Distributions of attack success rate for differnet models\nD. Robustness Evaluation\nGiven a teacher model, the teaching dataset is divided into\ntraining and testing sets. The training set is used to train the\nstudent model and the testing set is used to generate adversarial\nexamples. Compared with current methods [12] that use a very\nlimited number of attack examples, our evaluation set is much\nlarger and thus our results are more convincing.\nSuppose there are total M inputs in the testing set for\na deep learning app. T (out of M) inputs can be correctly\nclassiﬁed by the teacher model. These T inputs are used\nto generate adversarial examples since it is inappropriate to\ngenerate an adversarial example when the input cannot be\ncorrectly recognized by the teacher model. Then we generate\nT adversarial examples with the student model through an\nexisting attack algorithm (e.g., FGSM [22]). These adversarial\nexamples are then fed to the teacher model to obtain the\nprediction results. An attack is considered to be successful if\nthe adversarial example is misclassiﬁed by the teacher model.\nIf Q out of T adversarial examples are misclassiﬁed by the\nteacher model, then the attack success rate is:\np = Q\nT\n(1)\nThe quality of the student model is key to reach a high\nattack success rate. A qualiﬁed student model can imitate the\nbehavior of the teacher model well and generate adversarial\nexamples with higher transferability. These adversarial exam-\nples are more likely to spoof the teacher model [29]. Therefore,\nwe conduct experiments to investigate the inﬂuence factors on\nthe attack success rate of the student model, including the\nscale of the teaching dataset and the model structure of the\nstudent model. To further evaluate the effectiveness of our\nmethod, we conduct one control experiment that uses a random\npre-trained model to generate adversarial examples termed as\n‘blind attack’ (see Sec. IV-E).\nIV. EVALUATION\nA. Experiment Settings\nIn the following experiments, We reuse 10 deep learning\napps in the work of Huang et al. [12] so that we can compare\nour approach with theirs. Considering the workload of training\nstudent models and applying different attack methods to gen-\nerate adversarial examples, it is not trivial to experiment on\n10 apps. For example, one teaching dataset is ImageNet Large\nScale Visual Recognition Challenge 2012 (ILSVRC2012) [36]\nwhose size is above 138GB and the number of images is about\n14 million. This takes more than a week for a computer with\n8 NVIDIA Tesla K40s.\nTABLE I: 10 selected apps\nID\nApp Name\nFunctionalities\n1\nFresh Fruits\nIdentify if the fruit is rotten\n2\nImage Checker\nImage classiﬁer based on ImageNet\n3\nTencent Map\nIdentify road condition\n4\nBaidu Image\nIdentify ﬂowers\n5\nMy Pokemon\nIdentify pokemon\n6\nPalm Doctor\nIdentify skin cancer\n7\nQQ browser\nIdentify plants\n8\nTaobao\nIdentify products\n9\niQIYI\nIdentify actors\n10\nBei Ke\nIdentify scenes\nThe selected apps are listed in Table.I. For convenience, in\nthe following experiments, every app is represented by its ID\ninstead of name. Deep learning functionalities of each app are\nalso listed, which are useful for ﬁnding suitable dataset.\nResearch questions (RQ) 1 to 3 discuss the relationship\nbetween attack performance and student structure (RQ1),\nteaching dataset size (RQ2), and hyper-parameter of attack\nalgorithm (RQ3). We compare the attack success rates between\nour approach with\n[12] in RQ4. All these experiments1 are\nconducted in a Linux box with Nvidia(R) GeForce RTX(TM)\n3080 GPU and 128G RAM.\nB. RQ1: How the structure of the student model inﬂuences the\nattack rate.\nMotivation. The stronger the student model’s ability to imitate\nthe teacher model is, the higher the ﬁnal attack success rate\nis. Since the internal information of the teacher model is un-\nknown, choosing the most similar student model by comparing\nweights and structure can be impossible. Therefore, in this RQ,\nwe intend to explore how the structure of the student model\ninﬂuences the attack rate and analyze the underlying reasons.\nThe attack rate is deﬁned in Sec. III-D.\n1In the process of training student models, we set optimizer to SGD [37],\nloss function to CrossEntropyLoss [38], learning rate to 0.001, and epoch to\n30.\n5\n0\n20\n40\n60\n80\n100\nTeaching Dataset Size(%)\n10\n15\n20\n25\n30\nAttack Success Rate(%)\nBlind attack (0, 7.0)\n(a) Relationship of attack success rate and teaching dataset size\n0\n20\n40\n60\n80\n100\nTeaching Dataset Size(%)\n20\n30\n40\n50\n60\n70\n80\n90\nStudent Model Accuracy(%)\nBlind attack (0, 19.59)\nKey point (20, 86.02)\n(b) Relationship of student model accuracy and teaching dataset size\nFig. 4: Performance of attack and student model when varying teaching dataset size\nApproach. This RQ explores how the structure of the student\nmodel inﬂuences the attack rate, so only one teacher model\nand only one attack algorithm are needed. In this RQ, we\nrandomly select one app (i.e., No. 1). The deep learning model\ninside the app is considered as the teacher model, MIFGSM\nas the attack algorithm. The teaching dataset is a dataset\nfrom Kaggle [39] with 13.6K images. The teaching dataset\nis shufﬂed and then divided into training and testing sets at\n4:1. We carefully select 16 student models that are qualiﬁed\nfor this image classiﬁcation task. These student models are\ngrouped into four categories with the decrease of their model\nsizes and complexity:\n• VGGs [28]: VGG11, VGG13, VGG16, VGG19;\n• Resnets [40]: Resnet18, Resnet34, Resnet50, Resnet101,\nResnet152;\n• Densenets\n[41]:\nDensenet161,\nDensenet169,\nDensenet201; and\n• Small\nnets:\nMobilenetV2\n[42],\nShufﬂenetV2\n[43],\nSqueezenet [44], Googlenet [45].\nAmong the above four categories, VGGs have the highest\ncomplexity in terms of the number of weights, which is 3-\n4 times that of Resnets and Densenets, and 20-30 times that\nof Small nets. Resnets and Densenets have similar sizes and\ncomplexity.\nResult. Fig. 3 shows the comparison of attack performance\namong four student model categories. The abscissa represents\nattack success rates in percentage, and the ordinate gives four\ncategories. In every single box, the triangle represents the\nmean of attack success rate and the line represents the median.\nThe leftmost and rightmost edges represent minimum and\nmaximum attack success rates in the corresponding categories.\nThe attack success rates are divided into three segments. The\nﬁrst watershed is between Small nets and Resnets, the average\nattack success rate of Small nets is 29.4%, the minimum is\n23.97% (Q : T = 637 : 2657, where Q and T are deﬁned in\nSec. III-D), and the maximum is 32.86% (873 : 2657). The\nsecond watershed is between Densenets and VGGS. Densenets\nand Resnets who have similar model complexities also have\nsimilar attack success rates. Densenets’ average attack success\nrate is 37.82%, the minimum is 36.88% (980 : 2657), and the\nmaximum is 38.92% (1034 : 2657). Resnets’ average attack\nsuccess rate is 35.28%, the minimum is 33.68% (895 : 2657),\nand the maximum is 37.79% (1004\n:\n2657). The most\nsuccessful student models are VGGs. Their average attack\nsuccess rate is 51.44%, the minimum is 47.42% (1260 : 2657),\nand the maximum is 56.57% (1503 : 2657).\nThroughout the experiment, we found that a more sophisti-\ncated student model has a better opportunity and ﬂexibility to\nadjust its weights to better imitate the teacher model. The bet-\nter the student model imitates, the stronger the transferability\nof the adversarial examples generated by the student model is.\nNum. of apps. Due to page restriction, we only present the\nresult of one app. We also perform the same experiments on\nother apps and observe the same trend on them.\nAnswer to RQ1\nTo better imitate the teacher model and reach a higher attack\nsuccess rate, a student model with a more complex structure\nis a better choice. Among 16 models in 4 categories, VGGs\nhave a relatively high average attack success rate compared\nwith other student model candidates.\nC. RQ2: How the scale of the teaching dataset can inﬂuence\nthe attack rate.\nMotivation.\nTraining a student model on a large teaching\ndataset consumes lots of time and computing resources. The\nsize of the teaching dataset can inﬂuence the prediction\naccuracy of the student model. The prediction accuracy reﬂects\nhow well the student model imitates the teacher model. Thus,\nthis RQ explores how the scale of the teaching dataset can\ninﬂuence the attack rate.\n6\n4/255\n8/255\n12/255\n16/255\n20/255\neps\n0\n10\n20\n30\n40\n50\n60\n70\nAttack S ccess Rate(%)\nFGSM\nRFGSM\nFFGSM\nTPGD\nMIFGSM\nBIM\nPGD\nFig. 5: Relationship between attack success rate and paramater eps\nApproach. As this RQ explores how the scale of the teaching\ndataset inﬂuences the attack success rate, so only one teacher\nmodel, one attack algorithm, and one structure of the student\nmodel are needed. In this RQ, we randomly select one app\n(i.e., No.4) as the teacher model, MIFGSM as the attack\nalgorithm, and VGG11 trained on a teaching dataset from\nTensorFlow [46] with 3.6K images as the student model.\nThe teaching dataset is shufﬂed and then divided into\ntraining and testing sets at 4:1. The attack success rate and\nstudent model accuracy of each student model is evaluated on\nthe same testing set. training sets are obtained from the left\npart of the teaching dataset. Note that without training, the\nmethod degenerated to ‘blind attack’.\nResult. Fig. 4(a) shows the relationship between the size of\nthe teaching dataset (i.e., abscissa in Fig. 4(a)) and attack\nsuccess rate (i.e., ordinate in Fig. 4(a)). For example, point\n(10, 28.28) represents that the attack success rate of the student\nmodel trained on the teaching dataset with 367 images (10%\nof original size) is 28.28%.\nCompared with the models trained on our teaching dataset\n(black points), the performance of blind attack (blue points) is\n2-4 times worse. It proves that a teaching dataset generated by\nour pipeline is indispensable. To better reﬂect the relationship,\ntrend line is based on a logarithmic function y = a · ln(b +\nx) + c, where a, b and c control how well the trend line\nﬁts the scattered points. The trend line shows that the attack\nsuccess rate increases rapidly and gradually stabilizes with the\nteaching dataset increases. With the increase of the scale of\nthe teaching dataset, the student model can further adjust its\nweights, resulting in better imitation of the teacher model and\nhigher transferability of generated adversarial examples.\nThe reason for the gradual stabilization of the trend line\nneeds to be explained with the help of Fig. 4(b). The abscissa\nin Fig. 4(b) represents the teaching dataset size, which is the\npercentage of the original dataset, and the ordinate represents\nthe accuracy of the student model in percentage. For example,\npoint (20, 86.02) represents that the prediction accuracy of the\nstudent model trained on the teaching dataset with 687 images\n(20% of original size) is 86.02%. Compared with other cases,\na model without training (i.e., (0, 19.59) in Fig. 4(b)) has a\npoor performance. (20, 86.02) is a key point, after which the\naccuracy of the student model becomes stable.\nHigher accuracy means a higher model similarity between\nthe student model and the teacher model. The key point (20,\n86.02) in Fig. 4(b) represents that the imitation ability of the\nstudent model is close to its upper bound. Therefore, further\nincreasing the teaching dataset brings negligible improvement\nin terms of the attack success rate. This is why the growth of\nattack success rate in Fig. 4(a) becomes stable after this key\npoint.\nNum. of apps. Same as RQ1, we perform the same exper-\niments on other apps and observe the same trend on other\napps.\nAnswer to RQ2\nA teaching dataset generated by our pipeline is necessary for\na high attack success rate. The accuracy of the student model\nincreases as the size of the teaching dataset grows. The\naccuracy of the student model reﬂects how well it imitates\nthe teacher model. When the accuracy of the student model\nreaches around 85%-90%, its imitation ability is close to\nthe upper bound. Therefore, blindly increasing the teaching\ndataset contribute less to the attack performance.\nD. RQ3: How the hyper-parameter of attack algorithms inﬂu-\nences the attack performance.\nMotivation. The attack performance is evaluated on both the\nattack success rate and the degree of the added perturbation to\nthe original input. A key property of the adversarial example is\nthat it is generated by adding tiny perturbations to the original\ninput. If perturbations are too large, they can be noticed by\nhuman beings. If the perturbations are too small, the attack\nsuccess rate reduces. The following experiment explores how\n7\n(a) MIFGSM with eps =\n4/255\n(b) MIFGSM with eps =\n8/255\n(c) MIFGSM with eps =\n12/255\n(d) MIFGSM with eps =\n16/255\n(e) MIFGSM with eps =\n20/255\n(f) BIM\nwith\neps\n=\n4/255\n(g) BIM\nwith\neps\n=\n8/255\n(h) BIM\nwith\neps\n=\n12/255\n(i) BIM\nwith\neps\n=\n16/255\n(j) BIM\nwith\neps\n=\n20/255\n(k) PGD with eps\n=\n4/255\n(l) PGD\nwith\neps\n=\n8/255\n(m) PGD with eps\n=\n12/255\n(n) PGD with eps\n=\n16/255\n(o) PGD with eps\n=\n20/255\nFig. 6: Comparison between adversarial example generated with different eps\nthe hyper-parameters of attack algorithms can inﬂuence attack\nperformance.\nApproach. This RQ explores how the hyper-parameters of\nattack algorithms inﬂuences the attack performance, so only\none teacher model and one student model are needed. In this\nRQ, we randomly select one app (i.e., No.1) as the teacher\nmodel, VGG11 trained on a teaching dataset from Kaggle [39]\nwith 13.6K images as the student model.\nThe important hyper-parameter eps controls the degree of\nimage perturbation in all seven attack algorithms (FGSM,\nBIM, RFGSM, PGD, FFGSM, TPGD, MIFGSM). eps varies\nfrom 4/255 to 20/255 with step 4/255. The initial value\neps = 4/255 is a default value used by the author of these\n7 attack algorithms [32], [22], [30], [31], [33], [34], [35]. We\nset the end value to eps = 20/255. Although larger eps can\nbring a higher attack success rate, the perturbations of the\nimage also become larger. A higher degree of perturbations\nbrings a more signiﬁcant difference between the original input\nimage and the adversarial example. As a result, people can\ndistinguish the difference between the original input and the\nadversarial example. To ensure the degree of the perturbations\nis minuscule, the maximum of eps is set to 20/255. This\nreduces the upper bound of the success rate but can ensure\nperturbations to be unperceivable.\nResult. As shown in Fig. 5, the attack success rate of FGSM\nranges from 23.22% (617 : 2657) to 48.74% (1295 : 2657),\nRFGSM ranges from 1.96% (52 : 2657) to 48.85% (1298 :\n2657), FFGSM ranges from 22.28% (592 : 2657) to 51.79%\n(1376 : 2657), TPGD ranges from 21.11% (561 : 2657) to\n40.72% (1082 : 2657), MIFGSM ranges from 34.25% (910 :\n2657) to 71.47% (1899 : 2657), BIM ranges from 34.36%\n(913 : 2657) to 62.93% (1672 : 2657), and PGD (914 : 2657)\nranges from 34.40% (1668 : 2657) to 62.97%. All algorithms\nreach its highest attack succes rate with eps = 20/255.\nFig. 5 also indicates that different attack algorithms have\ndifferent sensitivity to the adjustment of eps. However, eps =\n8/255 is an important watershed for all seven attack algo-\nrithms. When eps ≤8/255, the attack success rate decrease\nrapidly. This is because that the perturbations are too minus-\ncule to spoof the teacher model.\nAnother important watershed is eps = 16/255. As shown in\nFig. 6, when eps ≥16/255 (4th and 5th colomun in Fig. 6)\nthe adversarial examples have relatively large perturbations.\nSuch perturbation can be detected by human beings, which\ncauses the adversarial example unqualiﬁed.\nSince attack performance is determined by the success rate\nand the degree of perturbation, we evaluate the performance\non both sides.\n• Attack success rate. MIFGSM shows the highest attack\nsuccess rate through all 7 attack algorithms, reaching 71.47%\n8\nwhen eps = 20/255. Also, it can maintain a relatively rapid\ngrowth rate after the ﬁrst watershed eps = 8/255. The reason\nfor its strong attack capability is that the adversarial examples\ngenerated by it are more transferable. Compared with other\nFGSM based attack algorithms, the highest attack success rates\nof FFGSM, RFGSM, and FGSM are 51.79%, 48.85%, and\n48.74%, respectively. The reason why MIFGSM outperforms\nother algorithms is that it invites the momentum term into the\niterative process [32].\n• Degree of perturbation. Although the attack success rate of\nMIFGSM keeps increasing quickly after both watersheds, the\nperturbations in adversarial examples become detectable by\nthe human visual system when eps ≥16/255. This behavior\nis common among all 4 FGSM based attack algorithms, so\nadjusting eps to 16/255 or higher is not recommended when\nusing this type of algorithm. For BIM and PGD, the highest\nattack success rates of them are the same (62.93%). Although\nthe attack success rate is less than MIFGSM’s, the degree of\nperturbation in their generated adversarial example is much\nsmaller than MIFGSM. Therefore BIM and PGD with eps ≥\n16/255 are recommended.\nNum. of apps. Same as RQ1, we perform the same exper-\niments on other apps and observe the same trend on other\napps.\nAnswer to RQ3\nAn ideal eps interval is [8/255, 16/255]. Keeping eps in\nthis interval can guarantee a higher attack success rate with\nlimited added perturbation.\nE. RQ4: Comparison among our approach, ModelAttacker\nand blind attack\nMotivation. We compare the performance among our ap-\nproach, ModelAttacker proposed by Huang et al. [12] and\nblind attack on 10 apps in Table. I. The experiment settings of\nModelAttacker and blind attack are as same as our approach.\nApproach. The attack success rates of all apps in Table. I are\ntested with the best student model structure, the most cost-\neffective teaching dataset size, and attack algorithms with a\nsuitable value of eps (deﬁned in Sec. IV-D).\nTeaching dataset. The teaching datasets for most apps are\nfetched from public datasets, including Kaggle-Fruit [39] (No.\n1), ILSVRC2012 [36] (No. 2, 8, 10), Kaggle-Road [47] (No.\n3), Tendorﬂow-Flowers [46] (No. 4), Kaggle-Pokemon [48]\n(No. 5), and HAM10000 [49] (No. 6). For the other 2 apps\n(No. 7, 9), the teaching datasets are crawled from Google\nImages [50]. These datasets are available in our online arte-\nfact [13]. Each teaching dataset is shufﬂed and then divided\ninto training and testing sets at 4:1.\nStudent model. Based on the result of RQ1, we select VGG11\nas the student model for all 10 apps.\nAttack algorithm. Based on the result of RQ3, we select\nMIFGSM as the attack algorithm and set eps to 12/255.\nResult. As shown in Fig. 7, the height of the ﬁrst bar for\neach app represents our attack success rate, the second is for\nModelAttacker, and the third is for the blind attack. Apps with\nNo.4,5,6,7,10 can be attacked by the blind attack so the other\n5 apps do not have the bars to show the results of blind attack.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nApp ID\n0\n20\n40\n60\n80\nAttack Success Rates(%)\nOur Approach\nModelAttacker\nBlind Attack\nFig. 7: Comparison among our approach, ModelAttacker and\nblind attack\nThe range of our attack success rate is from 34.98% to\n91.10% and on average is 66.60%. Our approach does not\nneed the weights and structures of deep learning models inside\nthe victim apps. Although with these constraints, our approach\nstill gets a higher attack success rate for 8 of 10 apps compared\nwith ModelAttacker. Speciﬁcally, the attack success rate of our\napproach is 8.27% to 50.03% higher than ModelAttacker, and\non average is 36.79% higher than ModelAttacker.\nFor the left 2 apps, ModelAttacker can reach a higher\nattack success rate. This is because that they can compare the\nsimilarity between the victim model (i.e., teacher model) and\ntheir substitute model (i.e., student model) with the knowledge\nof weights and structure of the teacher model [12]. However,\nour approach is a black box attack so the weights and structure\nof the teacher model are unknown.\nCompared with blind attack which is the common basic\nblack-box attack method, our approach can attack all 10 apps\nsuccessfully while the blind attack can only attack 5 of them.\nMeanwhile, the attack success rate of our approach is at least\n2.6 times higher than the blind attack.\nAnswer to RQ4\nOur approach can indeed effectively attack 10 selected deep\nlearning apps, thus providing a new perspective and method\nfor evaluating app reliability. Compared with existing meth-\nods, we can reach a relatively high attack success rate of\n66.60% on average, outperforming others by 27.63%.\nV. DISCUSSION\nA. Threats to Validity\nTo minimize the bias of the results, all three experiments in\nthis paper vary a single variable while ﬁxing other variables to\nevaluate its impact on attack performance. Different from other\nstudies on deep learning apps [11], [12], our experiments are\nbased on a larger dataset and generate a considerable number\nof adversarial examples for the victim app in each experiment.\nConsidering the workload of training, we select 10 apps that\nare also used by [12] to evaluate our approaches. Since the\n9\nsuccess rates of attacks are based on these apps, we do not\nattempt to generalize our results to all deep learning apps.\nB. Limitations\nOur study has two limitations. The main limitation is that\nour approach only suitable for non-obfuscated apps. To get\nthe teaching dataset from a teacher model inside the app, we\nhave to locate and learn how the model is loaded and used\nin the app. However, if protection techniques, such as code\nobfuscation, are applied to the app, it can be hard to ﬁnd the\nAPI patterns. Another limitation is that the pipeline developed\nin this paper focuses on computer vision apps. On the one\nhand, the existing studies [12], [11] on adversarial attacks are\nmainly in this ﬁeld. It can be easy to compare our work with\nother approaches. On the other hand, adversarial attacks in\nother deep learning ﬁelds lack a widely accepted evaluation\nstandard. Without widely adopted and convincing criteria, it\ncan be unfair to compare the results of our approach with\nothers [51], [52]. Thus, we only consider computer vision apps\nin our experiments.\nC. Consequences of Proposed Attacks\nFrom the perspective of a malicious attacker, the adversarial\nexamples can threaten users’ property, privacy, and even safety.\nDong et al. [53] applied adversarial examples to a real-world\nface recognition system successfully. As a popular way to\nunlock devices, the breach of the face recognition system\nmeans users’ privacy is at risk of leakage. Although it is not\neasy to use adversarial examples in the real world, relevant\nresearch has made progress [54], [55], [56]. The threat of\nadversarial examples is worthy of attention.\nD. Countermeasure\nAccording to the study by Sun et al. [10], only 59% of deep\nlearning apps have protection methods for their models. To ﬁll\nthis gap, we propose several protection methods for both deep\nlearning models and deep learning apps.\nDeep learning model protection scheme. To protect the deep\nlearning model inside an app from black-box attacks, there are\ntwo practical solutions:\n• Using self-developed deep learning models instead of open-\nsource models can reduce the probability of being attacked.\nFor a self-developed deep learning model, it can be hard to\nﬁnd or train a qualiﬁed student model to imitate the teacher\nmodel. Without a qualiﬁed student model, it can be hard to\nreach a high attack success rate; and\n• Training the deep learning model on a private dataset instead\nof a public dataset can reduce the success rate of being\nattacked. The size of the teaching dataset is critical for training\na qualiﬁed student model (See Sec. IV-C). If the deep learning\nmodel inside the app is trained on a public dataset, the attacker\ncan ﬁnd the same dataset with ease. A student model can gain a\npowerful imitating ability by being trained on the same dataset\nas the teacher model. As a result, the adversarial examples\ngenerated on the student model can spoof the teacher model\nwith a high success rate.\nDeep\nlearning\napp\nprotection\nscheme. Using a self-\ndeveloped deep learning model and collecting a private dataset\nis time-consuming and costly. There are also some com-\nmon techniques to protect deep learning apps from being\nattacked [10], [11], [12]:\n• Code obfuscation can prevent attackers from ﬁnding out how\nthe apps invoke the deep learning models. As a result, attackers\ncannot generate a labeled teaching dataset by instrumenting a\nDummyActivity. Thus, the attack method degenerates to the\nblind attack with a low attack success rate;\n• Hash code can prevent attackers from modifying the model\ninside the victim app. Li et al. [11] use images with special\npatterns to fool the models by adding speciﬁc layers into deep\nlearning models inside apps. With hash code, such backdoor\nattacks can be detected with ease; and\n• Encryption can prevent attackers from knowing the structure\nand weights of the model. It is a fundamental method to protect\nthe deep learning model inside the app. Without the knowledge\nof the victim model’s structure and weights, attackers can\nonly rely on black-box attack methods, which can be time-\nconsuming (i.e., ﬁnding dataset, training the student model).\nVI. RELATED WORK\nA. Substitute-based adversarial attacks\nTo overcome black-box attacks’ unreachability to the inter-\nnals of victim deep learning models, attackers can train a local\nmodel to mimic the victim model, which is called substitute\ntraining. With the in-depth study of black-box attacks on deep\nlearning models, substitute-based adversarial attacks have re-\nceived a lot of attention. Cui et al. [57] proposed an algorithm\nto generate the substitute model of CNN models by using\nknowledge distillation and boost the attacking success rate by\n20%. Gao et al. [58] integrated linear augmentation into substi-\ntute training and achieved success rates of 97.7% and 92.8% in\nMNIST and GTSRB classiﬁers. In addition, some researchers\nstudied substitute attacks from a data perspective. Wang et\nal. [59] proposed a substitute training approach that designs\nthe distribution of data used in the knowledge stealing process.\nZhou et al. [60] proposed a data-free substitute training method\n(DaST) to obtain substitute models by utilizing generative\nadversarial networks (GANs). However, the research on how\nto apply substitute-based adversarial attacks to the apps and\nthe analysis of their performance is lacking. This paper ﬁlls\nthe blank in this direction.\nB. Deep Learning Model Security of Apps\nPrevious works on the security of deep learning models\nin mobile apps mainly focus on how to obtain information\nabout the structure and weights of the model. Sun et al. [10]\ndeveloped a pipeline that can analyze the model structure and\nweights and revealed that many deep learning models can not\nbe extracted directly from mobile apps by decompiling the\napks. Huang et al. [12] developed a pipeline to ﬁnd the most\nsimilar model on TensorFlow Hub and then use it to generate\nadversarial examples. However, they still need to know the\ninformation of layers in the model to calculate similarity,\n10\nwhich is used to ﬁnd the most similar model on TensorFlow\nHub for launching attacks. Different from Huang et al.’s\nwork [12], we do not require the knowledge of the structure\nand weights of the model. Li et al. [11] proved the possibility\nto perform backdoor attacks on deep learning models and\nsucceeded to use images with unique patterns to fool the\nmodels. To perform such attacks, Li et al. [11] modiﬁed the\ninternal structure of the model inside the app. Whereas, as a\nblack-box approach, we do not need to alter anything inside\nthe deep learning model. Different from existing works, our\nwork focuses on investigating the security of the deep learning\nmodels in mobile apps in a complete black-box manner. Our\nwork offers a new perspective and shed the light on security\nresearch in deep learning models for mobile.\nC. Adversarial Attacks and Defenses to Deep Learning Model\nAdversarial attacks show their power in spooﬁng deep\nlearning models related to computer vision. Researches on\nimage classiﬁcation attack methods account for a large part.\nThe most popular technique is adversarial image perturbations\n(AIP) [61]. Dai et al. [62] developed an attack method based\non genetic algorithms and gradient descent, which demon-\nstrates that the Graph Neural Network models are vulnerable\nto these attacks. Stepan Komkov and Aleksandr Petiushko [63]\nproposed a reproducible technique to attack a real-world Face\nID system. Baluja et al. [64] developed an Adversarial Trans-\nformation Network (ATN) to generate adversarial examples.\nIt reaches a high success rate on MNIST-digit classiﬁers and\nGoogle ImageNet classiﬁers.\nAs adversarial attacks pose a huge threat to deep learning\nmodels, researches on how to protect models from such attacks\nhave also drawn wide attention. Li et al. [65] presented\nthe gradient leaking hypothesis to motivate effective defense\nstrategies. Liao et al. [66] proposed a high-level representation\nguided denoiser (HGD) as a defense for deep learning models\nrelated to image classiﬁcation. Zhou et al. [67] proposed a de-\nfense method to improve the robustness of DNN-based image\nranking systems. Ciss´e et al [68] proposed Parseval networks\nto improve robustness to adversarial examples. Alexandre et\nal. [69] used a Randomized Adversarial Training method to\nimprove the robustness of deep learning neural networks.\nHowever, even though there are a lot of researches on\nthe adversarial attack and defense methods of deep learning\nmodels, research on this topic about mobile apps is scant. Our\nwork proves that a black-box attack on the deep learning model\ninside the apps is feasible and provides a new perspective to\nevaluate the robustness of deep learning apps.\nVII. CONCLUSION\nIn this paper, we propose a practical black-box attack\napproach on deep learning apps and develop a corresponding\npipeline. The experiment on 10 apps shows that the average\nattack success rate reaches 66.60%. Compared with existing\nadversarial attacks on deep learning apps, our approach outper-\nforms counterparts by 27.63%. We also discuss how student\nmodel structure, teaching dataset size, and hyper-parameters\nof attack algorithms can affect attack performance.\nREFERENCES\n[1] M. Xu, J. Liu, Y. Liu, F. X. Lin, Y. Liu, and X. Liu, “A ﬁrst look at\ndeep learning apps on smartphones,” in Proceedings of WWW, 2019, pp.\n2125–2136.\n[2] “Tensorﬂow lite,” https://tensorﬂow.google.cn/lite/.\n[3] “Pytorch mobile,” https://pytorch.org/mobile/home/.\n[4] “Caffe2 mobile,” https://caffe2.ai/docs/mobile-integration.html.\n[5] “Mindspore lite,” https://www.mindspore.cn/lite/en.\n[6] “Coreml,” https://developer.apple.com/machine-learning/core-ml/.\n[7] “Firebase-cloud\nvs\non-device,”\nhttps://ﬁrebase.google.com/docs/ml#\ncloud vs on-device.\n[8] A. McIntosh, S. Hassan, and A. Hindle, “What can android mobile\napp developers do about the energy consumption of machine learning?”\nEmpirical Software Engineering, vol. 24, no. 2, pp. 562–601, 2019.\n[9] C. Kumar, R. Ryan, and M. Shao, “Adversary for social good: Protecting\nfamilial privacy through joint adversarial attacks,” in Proceedings of\nAAAI, 2020, pp. 11 304–11 311.\n[10] Z. Sun, R. Sun, L. Lu, and A. Mislove, “Mind your weight(s): A large-\nscale study on insufﬁcient machine learning model protection in mobile\napps,” in Proceedings of the 30th USENIX Security Symposium, ser.\nProcessings of USENIX Security, August 2021, pp. 1–17.\n[11] Y. Li, J. Hua, H. Wang, C. Chen, and Y. Liu, “Deeppayload: Black-\nbox backdoor attack on deep learning models through neural payload\ninjection,” in Proceedings of ICSE-SEIP, 2021, pp. 1–12.\n[12] Y. Huang, H. Hu, and C. Chen, “Robustness of on-device models: Adver-\nsarial attack to deep learning models on android apps,” in Proceedings\nof ICSE-SEIP, 2021, pp. 1–12.\n[13] “Online\nartefact,”\nhttps://sites.google.com/view/\nblackbox-attack-on-dl-apps/home.\n[14] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker, “Feature transfer\nlearning for face recognition with under-represented data,” in Proceed-\nings of CVPR, 2019, pp. 5704–5713.\n[15] D. Zoran, M. Chrzanowski, P. Huang, S. Gowal, A. Mott, and P. Kohli,\n“Towards robust image classiﬁcation using sequential attention models,”\nin Proceedings of CVPR, 2020, pp. 9480–9489.\n[16] J. Chen, C. Chen, Z. Xing, X. Xu, L. Zhut, G. Li, and J. Wang, “Unblind\nyour apps: Predicting natural-language labels for mobile gui components\nby deep learning,” in Proceedings of ICSE, 2020, pp. 322–334.\n[17] P. Dong, S. Wang, W. Niu, C. Zhang, S. Lin, Z. Li, Y. Gong, B. Ren,\nX. Lin, and D. Tao, “Rtmobile: Beyond real-time mobile acceleration\nof rnns for speech recognition,” in Proceedings of DAC, 2020, pp. 1–6.\n[18] Q. Wang, H. Yin, T. Chen, Z. Huang, H. Wang, Y. Zhao, and N. Q. V.\nHung, “Next point-of-interest recommendation on resource-constrained\nmobile devices,” in Proceedings of WWW, 2020, pp. 906–916.\n[19] “Tensorﬂow hub,” https://www.tensorﬂow.org/hub.\n[20] “Mindspore,” https://www.mindspore.cn/lite/models/en.\n[21] T. Tan and G. Cao, “Fastva: Deep learning video analytics through edge\nprocessing and npu in mobile,” in Proceedings of INFOCOM, 2020, pp.\n1947–1956.\n[22] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” in Proceedings of ICLR, 2015, pp. 1–11.\n[23] “Soot:a java optimization framework,” https://github.com/soot-oss/soot.\n[24] “Flowdroid,”\nhttps://github.com/secure-software-engineering/\nFlowDroid.\n[25] “Tensorﬂow\nlite\ninference,”\nhttps://tensorﬂow.google.cn/lite/guide/\ninference#load and run a model in java.\n[26] “Pytorch\nmobile\ninference,”\nhttps://pytorch.org/mobile/android/\n#api-docs.\n[27] H. Takeda, S. Yoshida, and M. Muneyasu, “Learning from noisy labeled\ndata using symmetric cross-entropy loss for image classiﬁcation,” in\nProceedings on GCCE, 2020, pp. 709–711.\n[28] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in Proceedings of ICLR, 2015, pp. 1–14.\n[29] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable\nadversarial examples and black-box attacks,” in Proceedings of ICLR,\n2017, pp. 1–14.\n[30] F. Tram`er, A. Kurakin, N. Papernot, I. J. Goodfellow, D. Boneh, and\nP. D. McDaniel, “Ensemble adversarial training: Attacks and defenses,”\nin Proceedings of ICLR, 2018, pp. 1–22.\n11\n[31] H. Kim, “Torchattacks: A pytorch repository for adversarial attacks,”\narXiv preprint arXiv:2010.01950, pp. 1–6, 2020.\n[32] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting\nadversarial attacks with momentum,” in Proceedings of CVPR, 2018,\npp. 9185–9193.\n[33] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards\ndeep learning models resistant to adversarial attacks,” in Proceedings of\nICLR, 2018, pp. 1–28.\n[34] H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan,\n“Theoretically principled trade-off between robustness and accuracy,” in\nProceedings of ICML, 2019, pp. 7472–7482.\n[35] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in\nthe physical world,” in Proceedings of ICLR, 2017, pp. 1–15.\n[36] “Imagenet,” http://www.image-net.org/challenges/LSVRC/2012/index.\n[37] “Pytorch\nsgd,”\nhttps://pytorch.org/docs/master/generated/torch.optim.\nSGD.html.\n[38] “Pytorch\ncrossentropyloss,”\nhttps://pytorch.org/docs/stable/generated/\ntorch.nn.CrossEntropyLoss.html.\n[39] “Kaggle\ndataset-fruit,”\nhttps://www.kaggle.com/sriramr/\nfruits-fresh-and-rotten-for-classiﬁcation.\n[40] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of CVPR, 2016, pp. 770–778.\n[41] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of CVPR, 2017, pp.\n4700–4708.\n[42] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L. Chen,\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings\nof CVPR, 2018, pp. 4510–4520.\n[43] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufﬂenet v2: Practical\nguidelines for efﬁcient cnn architecture design,” in Proceedings of\nECCV, 2018, pp. 116–131.\n[44] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,\nand K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer\nparameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,\npp. 1–13, 2016.\n[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”\nin Proceedings of CVPR, 2015, pp. 1–9.\n[46] “Tensorﬂow\ndataset,”\nhttps://www.tensorﬂow.org/datasets/catalog/tf\nﬂowers.\n[47] “Kaggle\ndataset-road,”\nhttps://www.kaggle.com/virenbr11/\npothole-and-plain-rode-images.\n[48] “Kaggle\ndataset-pokemon,”\nhttps://www.kaggle.com/lantian773030/\npokemonclassiﬁcation.\n[49] “Ham10000,”\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=\ndoi:10.7910/DVN/DBW86T.\n[50] “Google images,” https://images.google.com/.\n[51] J. Dong, Z. Guan, L. Wu, X. Du, and M. Guizani, “A sentence-level text\nadversarial attack algorithm against iiot based smart grid,” Computer\nNetworks, pp. 1–11, 2021.\n[52] X. Wang, H. Jin, and K. He, “Natural language adversarial attacks and\ndefenses in word level,” arXiv preprint arXiv:1909.06723, pp. 1–16,\n2019.\n[53] Y. Dong, H. Su, B. Wu, Z. Li, W. Liu, T. Zhang, and J. Zhu, “Efﬁcient\ndecision-based black-box adversarial attacks on face recognition,” in\nProceedings of CVPR, 2019, pp. 7714–7722.\n[54] L. Sun, M. Tan, and Z. Zhou, “A survey of practical adversarial example\nattacks,” Cybersecur., pp. 1–9, 2018.\n[55] A. Boloor, X. He, C. D. Gill, Y. Vorobeychik, and X. Zhang, “Simple\nphysical adversarial examples against end-to-end autonomous driving\nmodels,” in Proceedings of ICESS, 2019, pp. 1–7.\n[56] X. Xu, J. Chen, J. Xiao, L. Gao, F. Shen, and H. T. Shen, “What\nmachines see is not what they get: Fooling scene text recognition\nmodels with adversarial text images,” in Proceedings of CVPR, 2020,\npp. 12 304–12 314.\n[57] W. Cui, X. Li, J. Huang, W. Wang, S. Wang, and J. Chen, “Substitute\nmodel generation for black-box adversarial attack based on knowledge\ndistillation,” in Proceedings of ICIP, 2020, pp. 648–652.\n[58] X. Gao, Y.-a. Tan, H. Jiang, Q. Zhang, and X. Kuang, “Boosting\ntargeted black-box attacks via ensemble substitute training and linear\naugmentation,” Applied Sciences, pp. 1–14, 2019.\n[59] W. Wang, B. Yin, T. Yao, L. Zhang, Y. Fu, S. Ding, J. Li, F. Huang,\nand X. Xue, “Delving into data: Effectively substitute training for black-\nbox attack,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2021, pp. 4761–4770.\n[60] M. Zhou, J. Wu, Y. Liu, S. Liu, and C. Zhu, “Dast: Data-free substitute\ntraining for adversarial attacks,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2020,\npp. 234–243.\n[61] S. J. Oh, M. Fritz, and B. Schiele, “Adversarial image perturbation for\nprivacy protection a game theory perspective,” in Proceedings of ICCV,\n2017, pp. 1491–1500.\n[62] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song,\n“Adversarial attack on graph structured data,” in Proceedings of ICML,\n2018, pp. 1115–1124.\n[63] S. Komkov and A. Petiushko, “Advhat: Real-world adversarial attack on\narcface face ID system,” CoRR, pp. 1–9, 2019.\n[64] S. Baluja and I. Fischer, “Learning to attack: Adversarial transformation\nnetworks,” in Proceedings of AAAI, 2018, pp. 1–13.\n[65] Y. Li, S. Cheng, H. Su, and J. Zhu, “Defense against adversarial attacks\nvia controlling gradient leaking on embedded manifolds,” in Proceedings\nof ECCV, 2020, pp. 753–769.\n[66] F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu, “Defense against\nadversarial attacks using high-level representation guided denoiser,” in\nProceedings of CVPR, 2018, pp. 1778–1787.\n[67] M. Zhou, Z. Niu, L. Wang, Q. Zhang, and G. Hua, “Adversarial ranking\nattack and defense,” in Proceedings of ECCV, 2020, pp. 781–799.\n[68] M. Ciss´e, P. Bojanowski, E. Grave, Y. N. Dauphin, and N. Usunier,\n“Parseval networks: Improving robustness to adversarial examples,” in\nProceedings of ICML, 2017, pp. 854–863.\n[69] A. Araujo, R. Pinot, B. N´egrevergne, L. Meunier, Y. Chevaleyre, F. Yger,\nand J. Atif, “Robust neural networks using randomized adversarial\ntraining,” CoRR, pp. 1–9, 2019.\n12\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2021-07-27",
  "updated": "2021-07-27"
}