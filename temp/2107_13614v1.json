{
  "id": "http://arxiv.org/abs/2107.13614v1",
  "title": "Clones in Deep Learning Code: What, Where, and Why?",
  "authors": [
    "Hadhemi Jebnoun",
    "Md Saidur Rahman",
    "Foutse Khomh",
    "Biruk Asmare Muse"
  ],
  "abstract": "Deep Learning applications are becoming increasingly popular. Developers of\ndeep learning systems strive to write more efficient code. Deep learning\nsystems are constantly evolving, imposing tighter development timelines and\nincreasing complexity, which may lead to bad design decisions. A copy-paste\napproach is widely used among deep learning developers because they rely on\ncommon frameworks and duplicate similar tasks. Developers often fail to\nproperly propagate changes to all clones fragments during a maintenance\nactivity. To our knowledge, no study has examined code cloning practices in\ndeep learning development. Given the negative impacts of clones on software\nquality reported in the studies on traditional systems, it is very important to\nunderstand the characteristics and potential impacts of code clones on deep\nlearning systems. To this end, we use the NiCad tool to detect clones from 59\nPython, 14 C# and 6 Java-based deep learning systems and an equal number of\ntraditional software systems. We then analyze the frequency and distribution of\ncode clones in deep learning and traditional systems. We do further analysis of\nthe distribution of code clones using location-based taxonomy. We also study\nthe correlation between bugs and code clones to assess the impacts of clones on\nthe quality of the studied systems. Finally, we introduce a code clone taxonomy\nrelated to deep learning programs and identify the deep learning system\ndevelopment phases in which cloning has the highest risk of faults. Our results\nshow that code cloning is a frequent practice in deep learning systems and that\ndeep learning developers often clone code from files in distant repositories in\nthe system. In addition, we found that code cloning occurs more frequently\nduring DL model construction. And that hyperparameters setting is the phase\nduring which cloning is the riskiest, since it often leads to faults.",
  "text": "Empirical Software Engineering manuscript No.\n(will be inserted by the editor)\nClones in Deep Learning Code: What, Where, and Why?\nHadhemi Jebnoun · Md Saidur Rahman ·\nFoutse Khomh · Biruk Asmare Muse\nReceived: date / Accepted: date\nAbstract Deep Learning applications are becoming increasingly popular world-\nwide. Developers of deep learning systems like in every other context of software\ndevelopment strive to write more eﬃcient code in terms of performance, complex-\nity, and maintenance. The continuous evolution of deep learning systems imposing\ntighter development timelines and their increasing complexity may result in bad\ndesign decisions by the developers. Besides, due to the use of common frameworks\nand repetitive implementation of similar tasks, deep learning developers are likely\nto use the copy-paste practice leading to clones in deep learning code. Code clone\nis considered to be a bad software development practice since developers can in-\nadvertently fail to properly propagate changes to all clones fragments during a\nmaintenance activity. However, to the best of our knowledge, no study has in-\nvestigated code cloning practices in deep learning development. The majority of\nresearch on deep learning systems mostly focusing on improving the dependability\nof the models. Given the negative impacts of clones on software quality reported\nin the studies on traditional systems and the inherent complexity of maintain-\ning deep learning systems (e.g., bug ﬁxing), it is very important to understand\nthe characteristics and potential impacts of code clones on deep learning systems.\nThis paper examines the frequency, distribution, and impacts of code clones and\nthe code cloning practices in deep learning systems. To accomplish this, we use the\nNiCad clone detection tool to detect clones from 59 Python, 14 C#, and 6 Java\nbased deep learning systems and an equal number of traditional software systems.\nWe then analyze the comparative frequency and distribution of code clones in deep\nlearning systems and the traditional ones. Further, we study the distribution of\nthe detected code clones by applying a location based taxonomy. In addition, we\nstudy the correlation between bugs and code clones to assess the impacts of clones\non the quality of the studied systems. Finally, we introduce a code clone taxon-\nomy related to deep learning programs based on 6 DL software systems (from 59\nDL systems) and identify the deep learning system development phases in which\ncloning has the highest risk of faults. Our results show that code cloning is a fre-\nHadhemi Jebnoun · Md Saidur Rahman · Foutse Khomh · Biruk Asmare Muse\nE-mail: {hadhemi.jebnoun,saidur.rahman,foutse.khomh, biruk-asmare.muse}@polymtl.ca\nDGIGL, Polytechnique Montreal, QC, Canada,\n2\nJebnoun et al.\nquent practice in deep learning systems and that deep learning developers often\nclone code from ﬁles contain in distant repositories in the system. In addition, we\nfound that code cloning occurs more frequently during DL model construction,\nmodel training, and data pre-processing. And that hyperparameters setting is the\nphase of deep learning model construction during which cloning is the riskiest,\nsince it often leads to faults.\nKeywords Code Clones · Deep Learning · Clone Taxonomy\n1 Introduction\nDeep learning (DL) is a subset of machine learning (ML), which in turn is a\nsubset of artiﬁcial intelligence (AI), the science of mimicking human capabilities by\nmachines. DL is part of a broader family of ML methods inspired by the mechanism\nand structure of the human brain, a network of billions of neurons. This structure\nis simulated by networks of artiﬁcial neurons known as artiﬁcial neural networks\n(ANN). DL networks are artiﬁcial neural networks with more than three layers.\nDL enables computers to build concepts from data based on simpler concepts [26].\nThe ﬁeld of DL is currently revolutionizing almost every industry in countless\nways and is having a gigantic impact on domains like healthcare, communication,\ntransport, ﬁnance, etc. DL models have high learning capacity that allows them\nto capture increasingly complex patterns directly from data without the need of\nhandcrafted feature engineering [26].\nTraditionally, software systems are constructed deductively by writing down\nthe rules that govern the behavior of the system as program code. However, with\nDL, these rules are inferred from training data and they are generated inductively.\nThis consequently reduces the considerable manual work necessary to handcrafting\nfeatures as required for classical machine learning approaches. DL models also have\nmore sophisticated architecture than traditional ML models, capturing long-range\ndependencies and modeling them with data.\nThe main purpose of deep learning is to construct models with high perfor-\nmance that is able to learn patterns from the input data in order to make predic-\ntions for new data. To ﬁnd the optimal model, deep learning practitioners used\nto implement quickly prototypes by experimenting with diﬀerent conﬁgurations.\nThey then compare the performance of the diﬀerent models to identify the best\nconﬁguration leading to the most eﬃcient model. And as DL developers may have\nto follow the same or similar steps to build models with or without some mod-\niﬁcations, they often may end up writing poor code and duplicated functions or\nblocks known as clones. Also, code clones are often created through code reuse.\nIndeed, reusing code with or without modiﬁcation by copying and pasting frag-\nments from one location to another is a common practice during software de-\nvelopment and maintenance activities, including deep learning development [57].\nFor example, deep learning developers can clone models’ architectures and model\n(hyper)parameters settings or initialization for similar model implementations.\nCode clones are a kind of code smell and code smells are violations of some\nfundamental design principles. The existence of code smells [23] may aﬀect the\nmaintenance and the evolution of the software systems. In traditional systems,\nthey negatively impact software quality [93] and tend to increase technical debts,\nClones in Deep Learning Code: What, Where, and Why?\n3\nand consequently incur additional development and maintenance costs. Some types\nof code smells may also increase the consumption of certain resources (processor,\nmemory, etc.) [27,97] and, consequently, hinder the deployment of eﬃcient and\nsustainable solutions. Anawer et. al. [3] reported that duplicate code may be related\nto increased energy consumption by the software applications. For deep learning\nsystems, concerns are growing regarding the high energy consumption by deep\nlearning systems.Given the known negative impacts of code clones on traditional\nsoftware systems and the complexity of deep learning systems, it is reasonable\nto assume that code duplication is likely to pose challenges to the maintenance\nof deep learning projects. In fact, ML-based systems are expected to have the\nmaintenance concerns of traditional systems as well as those speciﬁc to ML.\nEarlier studies on traditional software systems show that about 7%-23% of code\nin the software repositories are cloned code [83]. The intuitive beneﬁt of clones\nis a short term productivity gain by code reuse. Some earlier studies reported\npositive impacts of clones [44] and showed that clones are not necessarily harmful\n[34,50]. However, there are substantial empirical evidences showing that clones\ncan negatively impact software quality by aﬀecting the stability of the code [58,\n64,74] and making it bug-prone [6,7,42,56,54,75,98], and consequently adding\ncomplexities and costs to software maintenance. To leverage the beneﬁts of code\nreuse while consciously avoiding possible issues, developers should be aware of\nclones and manage clones properly to ensure their consistent evolution [84].\nAlthough code cloning practices, as well as their impacts, have been widely\ninvestigated for traditional software systems, we know little about the coding\npractices in ML-based systems despite the recent upsurge in the development of\nmachine learning; in particular DL-based systems. No study to date has examined\nthe code quality of DL software systems by studying the distribution and impacts\nof code clones and by investigating the risks associated with their existence in the\nDL-based systems. We aim to ﬁll in this gap by empirically investigating code\ncloning practices in DL systems. Speciﬁcally, we aim to understand the extent to\nwhich DL developers duplicate code, as well as the impact of code clones on the\nmaintenance of DL systems. To the best of our knowledge, this paper presents the\nﬁrst empirical study on DL code clones, where one performs a comparative study\nof the distribution and bug-proneness of clones in deep learning and traditional\nsoftware systems. In this paper, we analyze clones in 59 Python, 14 C#, and 6\nJava based deep learning systems and an equal number of traditional software sys-\ntems. We compare the distribution of clones from the perspectives of clone types\nand their locations. To gain further insights into the reasons behind developers’\ncode cloning practices in the studied deep learning systems, we randomly select\nsix deep learning projects to perform a manual analysis of their code clones. We\nbuild a taxonomy of DL code clones in which we assign each detected code clones\nto the corresponding deep learning phase. We further study the relation between\nbug-proneness and code cloning in the context of deep learning systems. Finally,\nwe identify the phases of the development process of deep learning systems in\nwhich code cloning has the highest risk of bugs.\nOur empirical study resulted in the following key ﬁndings:\n– Cloning is frequent in deep learning code. We found that code clones occur-\nrences in deep learning code are higher than in traditional code. All three clone\n4\nJebnoun et al.\ntypes (Type 1, Type 2, and Type 3) are more prevalent in deep learning code\nthan in traditional systems.\n– Fragments of code clones in deep learning systems are dispersed. The majority\nof code clones in deep learning code are located in diﬀerent ﬁles i.e, in the ﬁles\nin same or in diﬀerent directories.\n– Code clones in deep learning code are likely to be more defect-prone compared\nto non-cloned code. Type 3 clones (i.e., clones with diﬀerences because of added,\ndeleted, or modiﬁed lines) are at higher risk of bugs compared to other clone\ntypes.\n– We classify code clones by deep learning phases and found that three main DL\nphases are more prone to code cloning: model construction (36.08%), model\ntraining (18.56%), and data preprocessing (18.56%).\n– We found clones from the following phases to have the highest risk of bugs:\nmodel construction (50%), model training (20%), data collection (13.3%), data\npreprocessing (10%), data post-processing (3.3%), and hyperparameter tuning\n(3.3%). In brackets we provided the percentage of clones that experienced a\nbug ﬁxing change.\nThe rest of the paper is organized as follows. In Section 2, we describe basic\nconcepts of code clones, deep learning, and the bug-proneness of code clones. In\nSection 3, we present our study design by introducing the research questions and\nmethodology. Our ﬁndings and discussions are presented in Section 4. Section 6\ndiscusses the threats to validity. Section 7 presents the related work and Section\n8 concludes the paper and introduces some future work.\n2 Background\nIn this section, we brieﬂy discuss the concepts and terminology related to our\nempirical study. We give an overview of the characteristics of the deep learning\nsystems and trends in the deep learning application development. We also provide\nbrief descriptions of the phases of deep learning application development. In addi-\ntion, we give an overview of the taxonomy of code clones, common causes behind\ncode cloning, and the impacts of clones on software systems based on existing\nliterature.\n2.1 Deep Learning\nIn this section, we review the main concepts of DL and discuss the diﬀerent phases\nof the life cycle of DL system development.\nDeep Learning (DL) is a sub-domain of Machine Learning (ML) that involves\nstacking of multiple layers of neural networks to provide a powerful model with the\nability to learn from data. Machine learning techniques other than deep learning\nrely heavily on feature engineering. Whereas deep learning is capable to learn rep-\nresentation from raw data which makes deep learning a powerful ML technique.\nThanks to the increase in the amount of data available and the advancement in\ncomputer infrastructure both hardware and software, deep learning has earned\ngrowing popularity recently and has dealt with complex applications with increas-\ning accuracy over time [26]. Deep learning has been applied in various ﬁelds and\nClones in Deep Learning Code: What, Where, and Why?\n5\nin several activities and services in daily life including transportation, health, and\nﬁnance [32,61,69]. Deep learning has contributed to several research ﬁelds such\nas computer vision [22,96], speech recognition [33,85], machine translation [9,18],\nsoftware engineering [28,55,103], etc.\nThere is an abundant literature on the software quality assurance of traditional\nsoftware systems [1,14,30,51,78]. However, very few studies have investigated qual-\nity issues in deep learning software systems. Deep learning software development\nmay face all the development and maintenance challenges of a traditional software\nsystem, in addition to DL/ML speciﬁc challenges related to their dependence to\ndata, inductive nature, and the complexity to understand their behavior [88]. To\nhelp developers create reliable deep learning systems eﬃciently, it is important\nto understand their design and implementation practices and how these practices\nimpact the quality of DL software systems. In the following, we describe the steps\nfollowed to construct a deep learning model. We adopted this development phases\nfrom the workﬂow described by Han et al. [31]. This workﬂow was inferred from an-\nalyzing various deep learning frameworks (e.g, Tensorﬂow, Pytorch, and Theano)\nFig. 1 shows the steps of this workﬂow that we detail in the following. We present\n9 steps represented in a linear diagram, however, deep learning workﬂows are non-\nlinear and may contain several feedback loops [2]. We add the data post-processing\nand the data collection phases to this workﬂow to get a more accurate classiﬁca-\ntion of code clones regarding development activities. We provide our conjectures\nof why deep learning practitioners may duplicate code to perform each step of this\nworkﬂow. Regarding other code smells, we assume that they may exist in every\nstep of the deep learning workﬂow, since they correspond to poor coding practices\non lines and functions or classes, regarding their complexity and length.\nFig. 1 Deep Learning Workﬂow\nPreliminary preparation: It is a coding-based phase in which developers pre-\npare the environment, resolve installation issues, frameworks/libraries versions,\nconﬁguring hardware requirements (CPU, GPU management), etc. This initial\nstep may be prone to code clones since if deep learning developers use diﬀerent\nmodels, each model requires a speciﬁc conﬁguration. These conﬁgurations are likely\nto be same or similar for closely similar models. Consequently, we may ﬁnd exact\nduplication and near-miss clones in the code used to perform this task. In addition,\nto parallelize data preparation, setting the number of cores, and the number of\nthreads per core, developers may duplicate code with diﬀerent values. Hence, the\nspread of code duplication to perform this DL phase.\nData collection: Deep learning practitioners start by gathering data required\nto model the target business problem. The collection could be from available open\nsource or internal dataset. This step could be done by reading ﬁle(s) from the disk,\n6\nJebnoun et al.\nor by calling a REST or Web service API, or by using data collector functionalities\nprovided by the deep learning frameworks. Whenever deep learning practitioners\nneed to collect data, they will use the same or similar call or logic possibly by\nmodifying only the source paths. Thus, they may end up duplicating code for data\nacquisition or data streaming.\nData preprocessing: Once the dataset is selected and collected, data should\nbe prepared as input to the chosen architecture of the models. Each model requires\ninput data of speciﬁc characteristics regarding size, shape, format, and data type.\nThis phase is done before model training and it depends on the required input\nfor each model. It is common for each model and the developers may opt for code\nreuse by copy-paste to implement common functions leading to duplicate code.\nEven if those are not exact clones, they will be near-miss (closely similar) clones\nas they have the same or similar algorithmic logic. After preprocessing the data\nto be suitable to model learning, the dataset is split into three diﬀerent subsets\nas follows: training data, validation data, and test data. As for splitting data, it\nis a common practice used by DL practitioners. The majority of deep learning\nframeworks provide ready to use functions to perform those pre-processing data\ntasks. Calling functions to collect data with or without modiﬁcations for each\nspeciﬁc need is likely to introduce clones in deep learning code.\nModel implementation: In this step, deep learning developers construct and\nconﬁgure the DL model with the chosen architecture. Then comes the hyperparam-\neters set up and the selection of activation functions, loss functions, and model\noptimizers. Another option is to use pretrained models that are available from\nonline sources or load them from disk storage. This practice is used to speed up\nmodel construction and training steps. This phase is considered as the most crucial\nphase in deep learning development since it is dedicated to the issues related to the\nmodel itself: the choice of the model [31]. The setup of deep learning models has\ncommon steps. These steps are blocks of code that are common between models,\nand each performs a sequence of calls to DL routines. Due to the use of the same\nframeworks and libraries, the DL code can have duplicated blocks of code between\nfunctions or cloned functions.\nModel training: Once the model implementation and data preparation are\ncomplete, the model is ready to be trained. The training process updates the\nparameters iteratively to minimize the loss, i.e., the prediction error. At the end\nof the training, the model is generated with better accuracy and performance\n[31]. Since many DL models may share the same algorithmic logic and share some\ncomputational functions (e.g., loss function, activation function), the deep learning\ncode may have duplicated code fragments that are either exact or near-miss clones.\nModel evaluation: At this level, the trained model is ready to be evaluated\non its performance. Thus, deep learning practitioners need the validation dataset,\nthat was hidden from the model during training, for evaluation. The evaluation\nof the model is frequently done by visualizing the performance metrics of the\ntrained model; assessing the changes to the loss function, model accuracy, etc\n[31]. Koenzen et al. [48] have shown that code snippets related to visualization\ntend to have a duplication rate of up to 21% in Jupyter notebook. Evaluating\ndeep learning models is an integral part of the DL development process. It allows\ndevelopers to ﬁnd the best model’s conﬁguration. This step gives a better idea of\nhow well the model may perform on unseen data. This is an essential phase in the\nmodel construction process, hence one can ﬁnd the code for model evaluation in\nClones in Deep Learning Code: What, Where, and Why?\n7\neach deep learning project. Their logic is similar if not exact. Hence, deep learning\ncode may contain duplicated functions or blocks of code used to perform model\nevaluations.\nModel tuning: To optimize the performance of the model, hyper/parameters\nare tuned. Using an unsuitable loss function, incorrectly initializing the weights, or\nchoosing an inappropriate learning rate will negatively aﬀect the model’s perfor-\nmance. This step is empirical, it is usually done through a trial and error process\nthat aims to compute the optimum values of models hyper-parameters (e.g, grid\nsearch technique [8]). Hyper-parameter tuning is an essential phase to improve the\nmodel’s performance. The implementation of this technique is either done by deep\nlearning practitioners or by invoking ready to use optimization functions from a\nframework. Because these functions have the same logic, calling the same or sim-\nilar set of framework routines to perform this task will likely result in duplicated\ncode blocks or functions in the deep leaning code.\nData postprocessing: After an inductive process of learning from raw data,\nthe output of the model may not be well-suited to represent the prediction results\nin an application speciﬁc and user-interpretable from. Hence, prediction results\nshould be post-processed to be more meaningful and informative to end-users.\nThis phase is frequently used in object detection, where the code interprets output\nby assigning the class with a higher probability to each object or by drawing the\nresulted bounding boxes on an image. If models have a common objective, such as\ndetecting objects, they may induce code duplication, during data post-processing.\nModel prediction: After training the model or using a pretrained model, the\nmodel is ready to make a prediction for new given data. The model prediction is\nimplemented frequently as a function named predict and a call to this function.\nWhen using the same logic, steps of implementation, and renaming strategy, the\ndeep learning model prediction code may have duplicated code blocks or functions.\n2.2 Code clones\nCode clones are exact or similar copies of code fragments usually created by copy-\ning and pasting code fragments for code reuse. It can be similar code fragments,\nwith renamed or added lines. The code fragments are usually identiﬁed by their\nﬁle names, start line number, and end line number. Code clones could be detected\nby pair or by class.\nClone pair: Clone detection result can be represented by pairs of fragments.\nTwo fragments that are clones to each other form a clone pair.\nClone class: Code clones can also be presented as clone classes. Each clone\nclass contains a set of fragments that are clones to each other.\n2.2.1 Clone Taxonomies\nIn our study, we are interested in exploring two kinds of clone taxonomies, similarity-\nbased and location-based clone taxonomy. We will explain both of them in the next\ntwo subsections.\n8\nJebnoun et al.\nSimilarity-based Clone Taxonomy Basically, there are two kinds of similarities\nbetween the two code fragments: functional (semantic) and textual (syntactic).\nTextual similarity is when a copied fragment is used with or without minor\nmodiﬁcation. There are three types of syntactically similar clones:\n– Type 1: Identical code clones except for diﬀerences in white-spaces, layouts\nand comments. It is known as exact clones. Table 1 presents an example of two\nfragments of code clones where the diﬀerence between them is the comment\nhighlighted in grey. The pair of code fragments are exact copies of each other.\nHence, they are clones of Type 1.\n– Type 2: Syntactically identical code clones except for diﬀerences in identiﬁers\nname, data types, whitespace, layouts, and comments are Type 2 clones. As\nshown in Table 2, the two fragments will be exact when we ignore the naming\ndiﬀerences (function name, name of input variables). These two code fragments\nare Type 2 clones of each other.\n– Type 3: Code clones with some modiﬁcation, addition or deletion of lines in\naddition to a diﬀerence in identiﬁers, data types, whitespaces, and comments.\nExamples of two Type 3 code fragments are showed in Table 3. These two\ncode fragments are diﬀerent in the function name and the addition of 2 lines\nfor another condition in the second code fragment.\nTable 1 Type 1 Clones\ndef forward_activation(self, X):\n#compute post activation value of X\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-X))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(X)\ndef forward_activation(self, X):\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-X))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(X)\nTable 2 Type 2 Clones\ndef forward_activation_fct(self, X):\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-X))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(X)\ndef forward_activation(self, input):\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-input))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(input)\nFunctional similarity is when two pieces of code are similar in functionality\nwithout being written in a textually identical/similar way. This kind of similarity\nis called semantic clones and referred to as Type 4. Table 4 shows an example of\nType 4 clone. The two functions diﬀer syntactically, but they achieve the same\nresult, which is to compute the post-activation of X with respect to the activation\nfunction.\nIn our study, we are interested in detecting both exact (Type 1) and near-miss\nclones (Type 2 and Type 3). Thus, we use the most recent version of Nicad (NiCad-\n5.2) [20], at the date of launching clone detection on our subject systems. We use\nClones in Deep Learning Code: What, Where, and Why?\n9\nTable 3 Type 3 Clones\ndef forward_activation_fct(self, X):\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-X))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(X)\ndef forward_activation(self, x):\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-x))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(x)\nelif self.activation_fct == \"relu\":\nreturn np.maximum(0,x)\nTable 4 Type 4 Clones\ndef forward_activation(self, X):\nif self.activation_fct == \"sigmoid\":\nreturn 1.0/(1.0 + np.exp(-X))\nelif self.activation_fct == \"tanh\":\nreturn np.tanh(X)\ndef forward_activation(self, x):\nvals = { \"sigmoid\" : 1.0/(1.0+np.exp(-x)),\n\"tanh\" : np.tanh(x) }\nreturn vals[self.activation_fct]\nNiCad because it was found to achieve higher precision and recall in near-miss\nclone detection [94].\nLocation-based Clone Taxonomy Clone taxonomies are categorized based on\nthree attributes: similarities, location, and refactoring opportunities as introduced\nin the survey by Roy and Cordy [80]. In this paper, we follow the location-based\ntaxonomy proposed by Kapser and Godfrey [43]. They introduced a categorization\nscheme for code clones, and applied their taxonomy in a case study performed on\nthe ﬁle system of the Linux operating system. They provide a hierarchical classiﬁ-\ncation of clones using attributes such as locations and functionality. Their taxon-\nomy mainly consists of three partitions of the physical locations of clones in the\nsource code as follows:\n– Same ﬁle when clones reside in diﬀerent locations of the same ﬁle.\n– Same directory when clones belong to diﬀerent ﬁles but within the same\ndirectory.\n– Diﬀerent directories when clones are detected in diﬀerent ﬁles and diﬀerent\ndirectories.\nThey further sub-classiﬁed the clones by the type of the region in which they\nare located (i.e., function, loop, function ending, etc). In our study, we apply the\nsame location-based classiﬁcation and propose a sub-classiﬁcation of clones based\non the functionalities related to deep learning. We perform this classiﬁcation by\nmanual analysis of the clones and using the location-based taxonomy. We use the\ndiﬀerent steps of the deep learning workﬂow presented above (Figure 1) to label\nthe detected clones.\n2.2.2 Bug-proneness of Code Clones\nCode cloning facilitates code reuse and thus intuitively increases productivity.\nHowever, this productivity gain may be outweighed by the negative impacts of\nclones on software maintenance as suggested by empirical evidences from diﬀer-\nent studies [58,64,74]. For example, code-duplication increases both size (code\n10\nJebnoun et al.\nbloating) and complexity of the software system. Because of these confounding\nfactors, software maintainability may become increasingly complicated. One of\nthe key challenges posed by code clones is ensuring the consistent evolution of\nclones during software maintenance; meaning that all cloned copies should be up-\ndated with necessary changes. This is because inconsistent changes to clones or\nmissing change propagation are likely to introduce bugs [7,5,42,25]. As consistent\nchanges to clones is important, missing changes, once identiﬁed, should also be\npropagated (late propagation) accordingly. However, late propagation of changes\nto clones have also been found to be prone to bugs introduction [4,6,7].\nAgain, as the cloned copies of code fragments are expected to evolve consis-\ntently, cloned code are likely to experience frequent changes, and thus negatively\naﬀect the stability of the software systems [58,59,64,74,25]. The instability of\nclones, in turn, has also been found to be related to bugs [75]. The bug-proneness\nof clones may also vary based on the types of clones [65]. Several other prior re-\nsearch works have also investigated the bug-proneness of code clones [63,56,54].\nThese multiple studies on the impacts of clones, from diﬀerent perspectives, show\nthat the bug-proneness of clones is an important concern. Given the complexity\nand lack of explainability or the ‘black-box’ nature of the deep learning models,\ntesting deep learning based systems and thus ﬁxing bugs are quite challenging\n[10,11]. Thus, it will be of interest to study the relationship between software\nbug-proneness and code clones in the deep learning applications context. Since,\nduplicating code is a common practice in the deep learning development process,\nas reported by deep learning practitioners in a recent survey [57]. In this paper,\nwe aim to empirically analyze deep learning systems to understand the extent and\nimpacts of clones. We aim to raise the awareness of deep learning developers on\nthe impact of code cloning, since it is likely to add more complexity and cost to\nthe development and maintenance of deep learning systems.\n3 Study Design\nIn this section, we ﬁrst present our research questions by highlighting the motiva-\ntion and research objectives. Then we describe our research methodologies.\n3.1 Study Objectives\nIn our empirical study, we ﬁrst examine the distribution of code clones in deep\nlearning code in terms of clone type and clone location. Then, we compare them\nto the distribution of code clones in traditional code. Second, we investigate the\nrelationship between code clones and bug-proneness in deep learning code. Third,\nwe examine the reasons behind code duplication in deep learning code and build a\ntaxonomy of code clones occurring in diﬀerent phases of the development process\nof deep learning systems. Finally, we determine the riskiest phases or activities of\ndeep learning application development by analyzing the bug-proneness of clones in\neach phase. To achieve these above-mentioned research objectives, we empirically\ninvestigate the following ﬁve research questions:\nRQ1: Are code clones more prevalent in deep learning code than tradi-\ntional source code?\nCode reuse by code cloning is a common practice in software development.\nDespite the intuitive productivity gain that one can expect from reusing code\nClones in Deep Learning Code: What, Where, and Why?\n11\nthrough code cloning, there are evidences showing that clones can negatively im-\npact software quality; increasing complexity and maintenance costs. [42] Although\ncode clones have been widely investigated for traditional software systems [84], the\nimpact of code cloning in deep learning systems is still unknown. The widespread\nuse of common open-source libraries and frameworks and the use of code exam-\nples from crowd-source question-answering web sites (e.g., Stack Overﬂow) may\nintroduce duplicate code in DL systems. Moreover, given the code reuse from\nopen-source repositories and the repetitive use of similar development phases or\ntasks (e.g., data preprocessing, model training) during deep learning system de-\nvelopment, it is reasonable to expect that code clones would exist in deep learning\nsystems. Since we know from the studies on traditional software systems, that\nthe impacts of clones vary based on the types and frequency of clone occurrences,\nit is therefore important to examine the prevalence and distribution of clones in\ndeep learning systems. A comparative analysis of code cloning in traditional and\ndeep learning based systems is important to understand if deep learning code is\nmore prone to clones than traditional programs, and therefore deserve a special\nattention, from the research community and tools builders to help practitioners\nmanage clones in deep learning systems eﬃciently.\nRQ2: How are code clones distributed in deep learning code in compar-\nison to traditional source code?\nCode clones location impacts the refactoring cost. Navigating into distant du-\nplicated code fragments adds comprehension overhead. Respectively, dispersed\ncode clones can be hard to manage and may incur an increased cost of mainte-\nnance. To understand where deep learning practitioners duplicate code, we study\nthe distribution of code clones in deep learning and traditional codes. We use the\ntaxonomy proposed by Kapser and Godfrey [43] to categorize the detected code\nclones by their locations (i.e, same ﬁle, same directory, and diﬀerent directories).\nRQ3: Do cloned and non-cloned code suﬀer similarly from bug-proneness\nin deep learning projects?\nStudies of code clones in traditional software systems suggest that clones can\nhave an adverse impact on the maintainability of the system; increasing the risk\nof fault [6,7]. However, it is important but yet to know the impacts of cloning\nin deep learning systems especially on the bug-proneness. This research question\ninvestigates the relationship between bug-proneness and code clones occurrences\nin deep learning code by performing Mann Whitney statistical test and analyzing\nthe eﬀect size. We examine the impacts of diﬀerent types of clones on the bug-\nproneness of deep learning code and assess the eﬀort required to ﬁx bugs in cloned\nand non-cloned code (by computing the time to ﬁx of each bug).\nRQ4: Why do deep learning developers clone code?\nGiven the complexity of deep learning code, constructing an eﬃcient DL model\ncan be a tedious job. DL developers should be experienced in the problem domains\nand should also have a suﬃcient understanding of deep learning techniques. They\nalso need to have coding skills with deep learning frameworks as well as the ability\nto manage the computing resources. When faced with a new task, to mitigate the\nrisk of writing erroneous code, DL developers may often duplicate the code of an\nexisting tested model with the same or similar logic with or without modiﬁcations,\ndepending on the requirements of their task. To understand activities in the de-\nvelopment of deep learning applications that are more prone to code duplication,\nwe conduct a manual analysis of code clone classes. We categorize clones based on\n12\nJebnoun et al.\nthe development phases in which they occurred. This analysis allows us to iden-\ntify activities (i.e., phases) in the deep learning development process where code\ncloning occurs frequently.\nRQ5: In which phases of deep learning development code cloning is more\nprone to faults?\nSince previous works on traditional systems [5] have shown that the risk of faults in\ncloned code vary depending on the types of code that is cloned, we are interested in\ninvestigating whether clones occurring at certain stages of the development process\nof deep learning systems or in certain functions of the deep learning code are more\nbug-prone than others. Therefore, for this research question, we compare the risk\nof faults of clones found in the diﬀerent functions of the deep learning code.\n3.2 Study Overview\nIn this section, we present our study design as shown in Figure 2. We divide our\nmethodology into ﬁve main steps.\nFig 2-A We ﬁrst clone 79 DL and 79 traditional repositories from GitHub, the\ndetailed methodology is described in the Subsection 3.2.1. We then detect code\nclones for both DL and traditional open-source projects using the NiCad clone\ndetector (details are presented in section 3.2.3). We ﬁnally compare the clone\ndistribution between both type of systems (i.e., DL and traditional systems)\nin terms of lines of code and clone types.\nFig 2-B We analyze the distribution of code clone by applying the location tax-\nonomy. As shown in Fig 2-B, we have three locations where there might be\ncode clones classes: same ﬁle, same directory, and diﬀerent directories (details\nare presented in Section 3.2.4). An arrow towards a balance as shown in the\nﬁgure 2-B represents the comparative analysis of the distribution of code clones\nbetween deep learning and traditional systems in terms of localization.\nFig 2-C The third part involves studying the relationship between code clones oc-\ncurrences and bug-proneness. We detect code clones for each commit of a set\nof six deep learning repositories (discussed in Section 3.2.2). Then, we iden-\ntify bug-ﬁxing commits relying on the commit history. Next, we extract bug-\ninducing commits by applying the SZZ algorithm. Once we have bug-inducing\ncommits with their corresponding changed lines, we match these buggy lines\nwith lines that are cloned to ﬁnd the relationship between bug and clones\n(details are shown in Section 3.2.6).\nFig 2-D We manually classify code clones that are DL-related to construct a tax-\nonomy of code clones in DL code (see Section 3.2.5).\nFig 2-E Finally, we examine the risk of bugs of code clones occurring in speciﬁc\nphases of the DL development.\n3.2.1 Subject Systems\nWe mined 138 open-source projects (59 Python, 6 Java and 14 C# deep learning\nprojects with equal number of traditional projects in each programming language)\nfrom GitHub. We used the same Python projects investigated in Jebnoun et al.\n[40] study on detecting code smells in DL code. Our empirical study is primarily\nClones in Deep Learning Code: What, Where, and Why?\n13\nFig. 2 Study Overview A- Detecting Code Clones in Deep Learning and Traditional Repos-\nitories, B- Applying Code Clone Location Taxonomy, C- Studying the Relationship between\nBug-proneness and Code Clones, D-Classifying Code Clones Manually Based on DL-related\nFunctionalities, E-Exploring riskier DL-related Code Clones\nfocused on the set of projects [29] in Python, as it is the most widely used language\nin the machine learning ﬁeld [12]. However, we also analyzed Java and C# projects\nfor the generalizability of the ﬁndings. Deep learning projects were selected by ﬁrst\nsearching repositories using DL-related keywords (e.g., deep learning, deep neural\nnetwork, convolutional neural network) [40] and manually ﬁltering out tutorials\nand some projects with a low number of releases (to obtain our 59 Python, 6 Java\nand 14 C# DL repositories). To obtain our dataset of traditional systems (i.e.,\nnon deep-learning code), we used a benchmark from an existing study by Chen\net al. [17] (similarly to Jebnoun et al. [40] study). This benchmark consists of 106\nGitHub repositories with at least 1k stars each from which we randomly selected\na subset of 59 traditional Python projects. We use the same set of 59 traditional\nPython projects studied by Jebnoun et al. [40], for the comparative analysis in\nthis study. We apply the same selection criteria for selecting 6 Java and 14 C#\ntraditional projects.\n3.2.2 Preprocessing of source code repositories\nSelection of a Subset of Subject Systems: Our research questions (RQ1, RQ2)\nare based on the analysis of 59 Python, 6 Java, and 14 C# deep learning systems\nand an equal number of traditional systems. For RQ3, we analyzed 59 Python, 2\nJava and 2 C# systems for each of the DL and traditional categories. However,\nwe randomly select 6 Python deep learning projects out of the 59 projects to\nanalyze the distribution and impacts of clones associated with diﬀerent phases\nor activities in deep learning system development by manual investigation (RQ4\nand RQ5). We select a subset of systems to keep the cost and eﬀort of manual\nanalysis in a feasible range. We provide more details about the 6 DL repositories\n14\nJebnoun et al.\n: name of the repository, number of commits, total lines of code, url and size in\nappendix A. We use this subset of systems to study the relationship between bug-\nproneness and code clones in deep learning code because detection of code clones\nby NiCad can be very time-consuming, especially for the detection of clones at\nevery commit as we did for RQ4 and RQ5. So, we use the six selected systems\nto perform manual analysis for extracting the taxonomy of code clones in deep\nlearning code regarding development steps. Our selected systems are from diverse\napplication domains with varying (small to medium) size (SLOC) and lengths (in\nnumber of commits) of evolution history.\nComputing Source Code Lines: We use the SLOCCount [102] tool to compute\nthe total source lines of code (SLOC) of Python, Java and C# code in each project\nfrom corresponding languages. SLOCCount is a software metrics open-source tool\ndeveloped by David A. Wheeler supporting several programming languages. It is\nrobust to handle variations in many languages such as the use of string constants\nas comments in Python code. Since one project could contain diﬀerent program-\nming languages, SLOCCount results list the available languages in the system. We\nconsider only the size of the Python, Java and C00# code based on the type of\nthe systems. We execute the SLOCCount tool for both DL and traditional repos-\nitories. We also measure the SLOC for each commit of each DL repository from\nthe selected set of six deep learning repositories for further analysis. We normal-\nize the detected lines of code clones by the size of the project (SLOC) to have a\nmore accurate comparison of deep learning and traditional systems regarding the\nfrequency and distribution of clones. We divide the total lines of cloned code for\neach project by the total lines of code of the corresponding project. This gives a\nnormalized (per LOC) representation of the frequency of clones for comparison.\n3.2.3 Code Clone Detection\nFor detecting code clones, we use the NiCad tool [20]. NiCad [19] can detect both\nexact and near-miss clones with high precision and recall [82] with respect to blocks\nand functions granularity.\nNiCad Settings: Table 5 shows the setup for detecting the three types of clones.\nWe detect code clones with a minimum size of ﬁve lines of code. We detect both\nexact clones (Type 1) and near-miss (Type 2 and Type 3) clones. We use the blind\nrenaming option of NiCad for the detection of Type 2 and Type 3 clones. Type 3\nclones are detected with a dissimilarity threshold of 30%.\nTable 5 NiCad Settings\nClone Type\nIdentiﬁer Renaming\nDissimilarity Threshold\nSize (LOC)\nType 1\nnone\n0%\n[5-2500]\nType 2\nblind\n0%\n[5-2500]\nType 3\nblind\n30%\n[5-2500]\nClones in Deep Learning Code: What, Where, and Why?\n15\nClone Detection:\nWe detect code clones for a particular snapshot and for each\ncommit of each repository. To perform a comparative analysis of the frequency\nand distribution of clones in deep learning and traditional code, we detect three\ntypes of code clones (Type 1, Type 2, and Type 3) in both deep learning and\ntraditional code using the NiCad with settings detailed in Table 5. We detect\nclones on a particular snapshot, which in our case is the last available version of\nthe project on GitHub at the time of cloning the repository [38]. For RQ1, we use\nboth granularities (function and block) to detect code clones. For the rest of the\nresearch questions (RQ2-RQ5), we analyze code clones at function granularity. To\nstudy the relation between the bug-proneness and code clones, we use the commit\nhistory to extract commits information of each repository from the set of six deep\nlearning repositories and detect code clones for every commit of the repositories.\nResults Cleaning:\nTo perform clone-type based investigation, we need to separate\neach type of clones. This is because by deﬁnition (as in Section 2.2.1), Type 2 clones\ninclude Type 1 clones and Type 3 clones include both Type 1 and Type 2 clones.\nTo separate Type 2 clones, we exclude matching clone classes between Type 1\nand Type 2 from the outputs of Type 2 clone detection results from NiCad. As\nNiCad results for Type 2 contain clone classes from Type 1 since the set of Type\n1 clones (exact copies) is a subset of the set of Type 2 clones (e.g. with diﬀerences\nin identiﬁer names). Hence, we remove matching clone classes based on the clone\nfragment speciﬁcations (ﬁle path, start and end line number, etc). Thus, if the\nsame clone class (i.e., containing the same set of clone fragments speciﬁcations)\nexists in both Type 1 and Type 2 clone detection results, we remove such clone\nclasses from Type 2 results as they are Type 1 clones. So, the ﬁltered Type 2\nresults contain exclusively Type 2 clones without any Type 1 clone fragment in it.\nSimilarly, we exclude the matching Type 1 and Type 2 classes from Type 3 clone\ndetection results to have Type 3 clones exclusively. This is because Type 3 clone\nresults by deﬁnition contain Type 1 and Type 2 clone fragments which need to be\nremoved to perform the clone-type centric analysis correctly.\n3.2.4 Location taxonomy based labeling of clones\nTo answer RQ2 that investigates the distribution of code clones in deep learning\nand traditional codes in terms of location, we apply the taxonomy used by Kapser\nand Godfrey [43]. It categorizes the detected clones based on their relative locations\nin the ﬁle system structure for both types of projects. We label a clone class by\n‘Same ﬁle’ when all the fragments in the detected clone class are from the same\nﬁle. We label a clone class by ‘Same directory’ when all the ﬁles associated with\nthe detected clone class belong to the same directory. And ﬁnally, we assign a clone\nclass to the ‘Diﬀerent directories’ label, when clone fragments are from diﬀerent\nﬁles located in diﬀerent directories.\nWe then calculate the proportion (in percentage) of both clones fragments and\ncloned LOCs distributed over the location types deﬁned by the taxonomy. We\nperform Mann-Whitney statistical test to compare whether the distribution of\ncode clones in DL and traditional systems are signiﬁcantly diﬀerent. We further\nextend our comparative analysis by considering individual clone types.\n16\nJebnoun et al.\n3.2.5 Labeling clones based on the taxonomy of deep learning tasks\nSince common functional steps are followed to build deep learning models and\nthe same deep learning libraries are used, deep learning practitioners may often\ncopy-paste ready to use functionalities with or without modiﬁcation. This aims\nto gain productivity and to reduce the risk of writing erroneous new code when\ntested implementations are available. We expect code duplication not only in all\ndeep learning phases (i.e., building model and training) but also in model testing.\nTherefore, we categorize diﬀerent types of cloning practices by DL phases and\nlabel them via a manual inspection of the code clones found in the selected six DL\nrepositories. We found 595 clone fragments in these six deep learning repositories.\nWe manually analyzed each of these clone fragments. We used a bottom-up ap-\nproach, where we ﬁrst assigned each clone fragment with a label corresponding to\nthe DL task or functionality. We further grouped the clone fragments in each sub-\ncategory and mapped them to the diﬀerent phases of the development workﬂow of\ndeep learning models, as discussed in Section 2.1. We ensure that the relation be-\ntween a subcategory and a category is: “to perform”. For example, initialize weights\nto perform a model construction. Here, the sub-category is ‘initializing weights’ and\ncategory is ‘model construction’. We also categorize and label functions that are\nnot related to deep learning such as logging, test, etc, as ‘others’. The manual\nclassiﬁcation for generating this DL taxonomy was done by at least two authors,\nall with academic and industry backgrounds. The resulting taxonomy was then\ncross-validated, and disagreements were resolved by group discussion.\nTables 6, 7, and 8 present some real-world examples from the detected clones.\nIn case of exact (Type 1) clones, we show only one code fragment from the clone\nclass since all the fragments are identical except for formatting diﬀerences. For\nnear-miss clones, we show both fragments in a clone pair. In Tables 6, 7, and 8,\nwe present the clone types of the example fragments, and the sub-category and\ncategory of DL-related phases to which the clone class or fragments belong to. Our\nobjective is to ﬁrst assign a sub-category (DL sub-task) for each clone class and\nthen group them into top-level categories (DL phases).\nThe ﬁrst example in Table 6 represents a Type 1 clone fragment. Here, the\nfunction iou(box1, box2) computes Intersection Over Union (IOU) between the\npredicted box and any ground truth box. It is a metric that computes the accuracy\nof YOLO (You Only Look Once) [77], a real-time object detection system based\non Convolutional Neural Network (CNN). Therefore, we designate this clone class\nto the sub-category ‘Measure model accuracy’. As this computation function is\nperformed to train the model, we label it as the ‘Model training’ category.\nThe second example contains two fragments of Type 3 clones as shown in Table\n7. The purpose of the ﬁrst function (read images from file) is to read multiple\nimages from the given ﬁle. Whereas the second function (read images from url)\nreads input images from a given URL. Thus, the function of this clone class is to\nload data. Since load data is performed to collect data, we assign this clones class\nto the ‘Data collection’ phase of deep learning.\nThe last example in Table 8 is a Type 1 clone fragment. This code fragment\n(process inceptionv3 input) prepares a given image for the model input require-\nments. In this case, the model is Inception v3 [95], which is a Deep Convolutional\nNeural Network with 48 layers. We assign this clone class to the ‘Resize image’\nsub-category and we label it as belonging to the ‘Data preprocessing’ category.\nClones in Deep Learning Code: What, Where, and Why?\n17\nTable 6 Example of Clone Fragment related to ’Model Training’\nCode Fragment\nClone Type\nSub-category\nCategory\ndef iou(box1, box2):\ntb = (min(box1[0] + 0.5 * box1[2],\nbox2[0] + 0.5 * box2[2]) -\nmax(box1[0] - 0.5 * box1[2],\nbox2[0] - 0.5 * box2[2]))\nlr = (min(box1[1] + 0.5 * box1[3],\nbox2[1] + 0.5 * box2[3]) -\nmax(box1[1] - 0.5 * box1[3],\nbox2[1] - 0.5 * box2[3]))\nif tb < 0 or lr < 0:\nintersection = 0\nelse:\nintersection =\ntb*lr\nreturn intersection / (box1[2]\n* box1[3] + box2[2] * box2[3]\n- intersection)\nType 1\nMeasure\nmodel\naccuracy\nModel training\nTable 7 Example of Clone Fragment related to ’Data Collection’\nCode Fragment\nClone Type\nSub-category\nCategory\ndef read_images_from_file(filename,\nrows, cols, num_images, depth=1):\n\"\"\"Reads multiple images\nfrom a single file.\"\"\"\n...\n_check_describes_image_geometry\n(rows, cols, depth)\nwith open(filename, 'rb')\nas bytestream:\nreturn images_from_bytestream\n(bytestream, rows, cols,\nnum_images, depth)\nType 3\nLoad data\nData collection\ndef read_images_from_url(url,\nrows, cols, num_images, depth=1):\n\"\"\"Reads multiple images\nfrom a single URL.\"\"\"\n...\n_check_describes_image_geometry\n(rows,cols, depth)\nwith urllib.request.urlopen(url)\nas bytestream:\nreturn images_from_bytestream\n(bytestream, rows, cols,\nnum_images, depth)\n3.2.6 Code Clone and Bugs\nSeveral studies have been focused on investigating the relationship between clones\nand bug-proneness. Some studies based on traditional software systems have shown\nthat code clones may introduce bugs and negatively impact software maintainabil-\nity [6,7,42,56,54,58,64,74,75,98]. Therefore, it is important to study whether and\nto what extent this relationship holds in the context of deep learning systems.\nDetecting bug ﬁxing commits: We extract all the commits information from each\nof the six selected repositories. We leverage a keyword-based approach to classify\n18\nJebnoun et al.\nTable 8 Example of Clone Fragment related to ’Data pre-processing’\nCode Fragment\nClone Type\nSub-category\nCategory\ndef process_inceptionv3_input(img):\nimage_size = 299\nmean = 128\nstd = 1.0/128\ndx, dy, dz = img.shape\ndelta = float(abs(dy - dx))\nif dx > dy:\n#crop the x dimension\nimg = img[int(0.5*delta):dx\n-int(0.5*delta), 0:dy]\nelse:\nimg = img[0:dx,\nint(0.5*delta):dy\n-int(0.5*delta)]\nimg = cv2.resize(img,\n(image_size, image_size))\nimg = cv2.cvtColor(img,\ncv2.COLOR_BGR2RGB)\nfor i in range(3):\nimg[:, :, i] = (img[:, :, i]\n- mean) * std\nreturn img\nType 1\nResize image\nData\npreprocessing\ncommits relying on keywords occurrence such as ‘bug’, ‘ﬁx’, ‘solve’, ‘problem’ in the\ncommit messages. We use the set of keywords used by Rosen et al. [79]. At least\none of the keywords from this set should be in the commit message to consider it\nas a bug-ﬁxing commit.\nBug Inducing commits: We use PyDriller [92] to extract bug-inducing commits\nfor each bug-ﬁx commit. PyDriller is a python-based framework that supports\nmining information from Git repositories. We used PyDriller to extract information\nsuch as commits and diﬀs from each of the selected repositories. We mainly use\nthis framework to get bug-inducing commits given a bug-ﬁx commit. It returns the\nset of commits that last modiﬁed the lines that are changed in the ﬁles modiﬁed\nby the bug-ﬁx commit by applying the SZZ algorithm.\nBug Proneness of code clone: We deﬁne a bug to be related to code clone if the\nlines changed in bug-ﬁx commits are in between the start line and end line of the\ndetected code clones. We further analyze to identify riskier DL-related functional-\nities that are likely to introduce bugs. We similarly match lines changed in bug-ﬁx\ncommits with the corresponding lines of the cloned DL-related functions. We then\nextract the most frequently occurring cloned DL-related functions related to bugs\nin DL projects. Therefore, we obtain which tasks of DL code are more likely to\nintroduce bugs than others when cloned. We perform MWW tests for the distri-\nbutions of code clones and non-clone code in the DL bug-ﬁx commits along with\neﬀect size analyses. To determine if there is a statistically signiﬁcant diﬀerence\nbetween the number of commits that ﬁx bugs on cloned lines and the number of\ncommits that ﬁx bugs on non-cloned lines.\nTime to ﬁx bugs when it is related to code clones: We investigate whether\nor not clones have impacts on the time required to ﬁx a bug. The objective is\nClones in Deep Learning Code: What, Where, and Why?\n19\nto know whether clones hinder bug ﬁxing; making bugs long-lived in the deep\nlearning systems. We thus study the comparative time it takes to ﬁx bugs when it\nis related and not related to clones respectively. We compute the diﬀerence in time\nbetween bug ﬁxing commit and their related bug inducing commit as introduced\nby Kim and Whitehead [47]. To do so, we extract from the commit history the\ntime of each bug-introducing commit as well as the time of their corresponding\nbug-ﬁx commit. We calculate the bug-ﬁx time by taking the diﬀerence between\nbug-ﬁx commit and bug inducing commit. Once we have the time diﬀerence (in\nseconds) we carry out a non-parametric Man-Whitney test to ﬁnd if there is any\nsigniﬁcant diﬀerence in the time required to ﬁx bugs related and not related to\nclones.\nTesting statistical signiﬁcance and the eﬀect size: To answer the research ques-\ntions, we computed diﬀerent metrics based on the quantitative analysis of clones\nin deep learning and traditional systems, respectively. To verify whether the dif-\nferences between the corresponding metrics for DL and traditional code are statis-\ntically signiﬁcant, we performed Mann-Whiteny-Wilcoxon (MWW) [68] test (two-\ntailed, p-value signiﬁcant at <0.5). We apply MWW test because this is a non-\nparametric test i.e., it does not require data to be normally distributed. Moreover,\nthis test can be applied on small data set. However, MWW test only conﬁrms\nwhether the observed diﬀerences between two sets of values are by chance. So,\nthe observed diﬀerences to be statistically signiﬁcant, we also need to measure\nthe eﬀect size. We apply Cliﬀ’s delta [60] eﬀect size along with the MWW test.\nThis also a non-parametric measure of eﬀect size. The range of the values for cliﬀs\ndelta eﬀect size is [-1, +1]. The value of eﬀect size is generally interpreated as small\n(∼0.20), medium (∼0.5), and large (∼0.8). Larger eﬀect size refers to stronger\nrelationships between the set of observations tested. Here, +1 and -1 values for the\neﬀect size indicate the absence of overlap while value 0 indicates a lot of overlap\nbetween two set of samples. We used mannwhitneyu() functions for the Python\npackage scipy.stats for MWW test and calculated eﬀect size by cliffsDelat()\nfunction as implemented in [21].\n4 Study Findings and Discussions\nIn this section, we present the results of our study in details, and answer ﬁve\nresearch questions as follows.\n4.1 RQ1 : Are code clones more prevalent in deep learning code than\ntraditional source code?\nDue to the complexity, the lack of explainability of deep learning code, and the\nexcessive use of ready to use routines from popular deep learning frameworks\nand libraries, deep learning code is likely to have duplicated code fragments i.e.,\ncode clones. Although many studies investigated the distribution, evolution, and\nimpacts of clones regarding traditional software, none of the existing studies, has\ninvestigated the code cloning practices in deep learning code. Thus, in this research\nquestion, we study the distribution of diﬀerent types of code clones in both deep\n20\nJebnoun et al.\nlearning and traditional systems, to understand and compare the prevalence of\nclones in these two types of software systems.\nTo answer this research question, we detect code clones in 59 deep learning\nsystems and 59 traditional software systems developed in Python. We calculate\nthe number of occurrences of code clones across diﬀerent dimensions (e.g., project\ntype, clone type, clone granularity). Then we compare the density of clones in DL-\nbased systems with that of the traditional systems. To compare we need normalized\nvalues of the clone densities in diﬀerent systems as the systems varies in code size.\nFor normalization, ﬁrst we calculate the total number of lines of code clones in\neach project and then divide that by the total number of source code lines (i.e.,\nSLOC) for normalized representation of the clone density. We count SLOC using\nthe tool SLOCCount as discussed in section 3.2.2. We then perform the Mann-\nWhitney Wilcoxon (MWW) test [68] to compare the distribution of clones in\ndeep learning and traditional systems by testing if there exists any statistically\nsigniﬁcant diﬀerences in clone densities in these two types of systems. To have\ndeeper insights, we also compare the clone densities with respect to the three\nclone types (Type 1, Type 2, and Type 3). However, MWW test only veriﬁes\nwhether the diﬀerence between two set of observations is by chance and does not\nexpress the eﬀect or magnitude of the diﬀerences. Thus, we calculate the eﬀect size\nto determine the magnitude of the diﬀerences between each two distributions. We\nmeasure Cliﬀ’s Delta [60], which is a non-parametric estimate of eﬀect size and\ndoes not require data to be normally distributed. When Cliﬀ’s Delta is beyond of\n0.2 and below 0.5, then the eﬀect size is low. When it is beyond 0.5 and below\n0.8, the eﬀect size is medium. Beyond 0.8, the eﬀect size is large. In this research\nquestion, we present the ﬁndings for both clone granularities: function and block.\nClone Occurrences by Project Type: Fig. 3 shows the comparison of clone occur-\nrences between DL-based and traditional systems considering all clone types for\nboth function and block granularity. The box plots represent clone density deﬁned\nby the normalized lines of code clones per lines of source code for both deep learn-\ning and traditional software systems. This metric computes the density of clones\nas a ratio of the total lines of cloned code and the total lines of source code in the\ncorresponding systems. In Fig. 3, we observe that the median of the clone density\nfor deep learning systems is comparatively higher than that of traditional systems.\nWe observe the similar diﬀerences for both function and block granularity. This\nsuggests that deep learning systems tend to have higher proportions of cloned code\ncompared to traditional systems.\nTo investigate whether the observed diﬀerences that DL systems having higher\ndensity of clones compared to traditional systems are statistically signiﬁcant i.e,\nthe diﬀerence is not by chance, we perform Mann-Whitney Wilcoxon (MWW)\ntests [68] (two-tailed, signiﬁcance at < 0.05). We chose the MWW test because it\nis a non-parametric test and thus does not assume data to be normally distributed.\nAlso, it can be applied on small sample sizes. We present our statistical test results\nin Table 9. The column ‘All’ in Table 9 shows the p-values for MWW test for all\nclone types. The p-values for function and block granularity are 1.78e-07 and 3.01e-\n10 respectively which are < 0.05 indicating that our observation is statistically\nsigniﬁcant. Thus, DL systems have higher density of clones compared to traditional\nsystems.\nClones in Deep Learning Code: What, Where, and Why?\n21\nNow to observe the magnitude of the diﬀerences in clone density of DL and\ntraditional systems, we analyze Cliﬀ’s delta eﬀect sizes. As shown in Table 9, the\neﬀect size in column ‘Total’ (which includes all clone types) for function granularity\nbelongs to the large category as it is equal to 0.8. Thus, we have an 80% chance that\ndeep learning code will have higher density of function clones than traditional code.\nWhereas for block granularity, we have an equal likelihood (0.53 ∼0.5) of having\nhigher density of block clones in deep learning code in comparison to traditional\ncode. When we consider all clone types, the observed higher clone density in deep\nlearning systems in comparison to traditional systems is statistically signiﬁcant\nwith medium to large eﬀect size.\nDeep Learning\nTraditional\nproject_type\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nLOCC / total SLOC\n(a) Function Granularity\nDeep Learning\nTraditional\nproject_type\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLOCC / total SLOC\n(b) Block Granularity\nFig. 3 Code Clones Occurrences in DL and Traditional Python Projects for Both Code Clones\nGranularities: (a) Function, (b) Block. LOCC: Lines Of Code Clones, SLOC: Source Lines\nOf Code.\nWe also studied 6 Java and 14 C# systems to investigate whether the clone\ndistributions observed in Python-based systems generalize to other programming\nlanguages. Fig. 4 and Fig. 5 show the comparison of clone densities between DL-\nbased and traditional Java and C# systems respectively, considering all clone types\nfor both function and block granularity. In both Fig. 4 and Fig. 5, we observe that\nthe median of the clone density for deep learning systems is comparatively higher\nthan that of traditional systems. We observe similar diﬀerences for both function\nand block granularity. These observations for Java and C# systems are similar\nto the observation from Python based systems. This suggests that deep learning\nsystems tend to have higher proportions of cloned code compared to traditional\nsystems. Table 10 and Table 11 show the results for statistical signiﬁcance test\n(two-tailed MWW test, signiﬁcant at <0.05) results. We do not observe statistically\nsigniﬁcant diﬀerences between the density of cloned code in Deep Learning and\nTraditional Java and C# systems costrasting our results based on Python based\nsystems. These diﬀerences between clone densities in Python system from that of\nJava and C# systems could be explained by the diﬀerences in syntactic structures,\ncompactness of code, availability and usage patterns of libraries, diﬀerences in\nobject-oriented design, etc.\n22\nJebnoun et al.\nDeep Learning\nTraditional\nProject Type\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nlog(LOCC / total SLOC)\n(a) Function Granularity\nDeep Learning\nTraditional\nproject_type\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nLOCC / total SLOC\n(b) Block Granularity\nFig. 4 Code Clones Occurrences in DL and Traditional Java Projects for Both Code Clones\nGranularities: (a) Function, (b) Block. LOCC: Lines Of Code Clones, SLOC: Source Lines\nOf Code.\nDeep Learning\nTraditional\nproject_type\n0.0\n0.1\n0.2\n0.3\n0.4\nLOCC / total SLOC\n(a) Function Granularity\nDeep Learning\nTraditional\nproject_type\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nLOCC / total SLOC\n(b) Block Granularity\nFig. 5 Code Clones Occurrences in DL and Traditional C# Projects for Both Code Clones\nGranularities: (a) Function, (b) Block. LOCC: Lines Of Code Clones, SLOC: Source Lines\nOf Code.\nWe therefore conclude that deep learning systems tend to have higher propor-\ntions of cloned code compared to traditional systems. However, this may depend on\nconfounding factors, i.e, programming languages, using the same libraries/frameworks,\nhaving the same decision logic across deep learning models construction, as ex-\nplained by the diﬀerence in eﬀect size for function (high) and block (medium)\ngranularity, despite the diﬀerences in clone density being statistically signiﬁcant\nfor both granularities.\nCode Clone occurrences by Clone Type: In Fig. 6, we compare clone densities\nfor deep learning and traditional systems regarding individual clone types (Type\n1, Type 2, and Type 3) for both function and block granularity. To calculate clone\nClones in Deep Learning Code: What, Where, and Why?\n23\nDeep Learning\nTraditional\nProject Type\n10\n3\n10\n2\n10\n1\n10\n0\nlog(LOCC / total SLOC)\n(a) Function Granularity\nType 1\nType 2\nType 3\nDeep Learning\nTraditional\nProject Type\n10\n3\n10\n2\n10\n1\n10\n0\nlog(LOCC / total SLOC)\n(b) Block Granularity\nType 1\nType 2\nType 3\nFig. 6 Clone Density in DL and Traditional Python Projects for Clone Types and Granularity\nLOCC: Lines Of Code Clones\nDeep Learning\nTraditional\nProject Type\n10\n1\nlog(LOCC / total SLOC)\n(a) Function Granularity\nType 1\nType 2\nType 3\nDeep Learning\nTraditional\nProject Type\n10\n1\nlog(LOCC / total SLOC)\n(b) Block Granularity\nType 1\nType 2\nType 3\nFig. 7 Clone Density in DL and Traditional Java Projects for Clone Types and Granularity\nLOCC: Lines Of Code Clones\nDeep Learning\nTraditional\nProject Type\n10\n3\n10\n2\n10\n1\nlog(LOCC / total SLOC)\n(a) Function Granularity\nType 1\nType 2\nType 3\nDeep Learning\nTraditional\nProject Type\n10\n2\n10\n1\nlog(LOCC / total SLOC)\n(b) Block Granularity\nType 1\nType 2\nType 3\nFig. 8 Clone Density in DL and Traditional C# Projects for Clone Types and Granularity\nLOCC: Lines Of Code Clones\n24\nJebnoun et al.\nTable 9 Mann-Whitney Test and Cliﬀ’s Delta Results between DL and Traditional Python\nProjects\nClone Types\nType 1\nType 2\nType 3\nAll\nGranularity\nFunction\nBlock\nFunction\nBlock\nFunction\nBlock\nFunction\nBlock\np-value\n3.13e-06\n7.61e-08\n3.38e-05\n4.17e-06\n1.77e-03\n2.87e-04\n1.78e-07\n3.01e-10\nCliﬀ’s Delta\n0.58\n0.62\n0.46\n0.60\n0.32\n0.37\n0.8\n0.53\nTable 10 Mann-Whitney Test and Cliﬀ’s Delta Results between DL and Traditional Java\nProjects\nClone Types\nType 1\nType 2\nType 3\nAll\nGranularity\nFunction\nBlock\nFunction\nBlock\nFunction\nBlock\nFunction\nBlock\np-value\n0.93\n0.92\n0.57\n0.64\n0.81\n0.64\n0.73\n0.60\nCliﬀ’s Delta\n0.055\n0.066\n0.222\n0.2\n0.111\n0.2\n0.018\n0.111\nTable 11 Mann-Whitney Test and Cliﬀ’s Delta Results between DL and Traditional C#\nProjects\nClone Types\nType 1\nType 2\nType 3\nAll\nGranularity\nFunction\nBlock\nFunction\nBlock\nFunction\nBlock\nFunction\nBlock\np-value\n0.64\n0.39\n0.62\n0.42\n0.05\n0.22\n0.22\n0.31\nCliﬀ’s Delta\n0.115\n0.197\n0.119\n0.183\n0.438\n0.510\n0.159\n0.127\ndensity for individual clone types, we divide the total lines of cloned code for a\nparticular clone type by the total number of source code lines in a given system. We\ncalculate clone density for both function and block granularity. We use a log scale\nfor the y-axis to make the results more clear. As shown in Fig. 6, for deep learning\nsystems, the density of Type 3 clones is the highest followed by Type 2 and Type\n1 clones respectively. We observe the same trends in comparative densities for all\ntypes of clones in traditional systems, and for both function and block granularity\nin both types of systems. Now, when we compare the clone densities in deep\nlearning and traditional systems based on the box-plots in Fig. 6, we observe that\nfor all types of clones, deep learning systems have higher median for clone densities\ncompared to that of traditional systems. The same overall trend holds for both\nfunction and block granularity. This suggests that deep learning systems tend to\nhave higher density of cloned code compared to traditional systems regarding all\nthree clone types.\nTo verify whether these diﬀerences of deep learning systems having higher\ndensities of clones compared to traditional systems are statistically signiﬁcant, we\nperform MWW tests (two-tailed, signiﬁcance at 0.05) on the corresponding clone\ndensities for all individual clone types, for both function and block granularity.\nWe also compute the Cliﬀ’s delta eﬀect size to determine the magnitude of the\ndiﬀerences in clone densities for DL and traditional systems. Table 9 shows the p-\nvalues of the MWW tests along with the values for the corresponding eﬀect size. We\nobserve that there is a statistically signiﬁcant diﬀerence between clone densities in\ndeep learning code and traditional code with respect to Type 1 clones with p-values\nof 3.13e-06 and 7.61e-08, which are < 0.05 for both function and block granularities,\nrespectively. The values of Cliﬀ’s Delta are 0.58 and 0.62 which belong to the\n‘medium’ category. We also found a statistically signiﬁcant diﬀerence in clone\ndensities for Type 2 clones (p-values of 3.38e-05, 4.17e-06). The values of eﬀect\nsize for Type 2 for both function and block granularities are medium with values\nof 0.46 and 0.60. Similarly, we also obtained a statistically signiﬁcant diﬀerence in\nClones in Deep Learning Code: What, Where, and Why?\n25\nclone densities between deep learning code and traditional code regarding Type 3\nclones. The eﬀect size for Type 3 clones is small with values of 0.32 for function\ngranularity and 0.37 for the block granularity. Overall, the results of our clone-type\nbased analysis show that Type 3 clones comprise the highest proportion of clones\nin deep learning code. Moreover, for all clone types, deep learning code has higher\ndensity of clones than traditional systems, and these diﬀerences are statistically\nsigniﬁcant. This indicates an overall trend of deep learning systems having more\nclones than traditional systems, although the magnitude of the diﬀerences may\nnot always be ‘large’.\nFig. 7 and Fig. 8 show the distributions of clone densities in DL and traditional\nJava and C# systems respectively. We observe that the median density of clones\nin DL systems tend to be higher compared to traditional systems for clone types\nin Java and C# systems except for Type 3 clones in Java systems (Fig. 7). Our\nclone-type centric analysis show that densities of cloned code tend to be higher in\nDL systems compared to traditional systems. However, we do not observe these\ndiﬀerences to be statistically signiﬁcant.\nAs the dissimilarity threshold for the clone detection tool is known to be im-\nportant confounding factor for the empirical analysis of clones, we also perform\nthe analysis with 20% dissimilarity threshold for NiCad. We presented our results\nfor 20% threshold in Appendix B (Fig. 29, Fig. 31, Fig. 30, and Fig. 32). Our\nresults show that the overall trends in the prevalence of clones in deep learning\nand traditional systems for 20% threshold do not change from what we observed\nfor 30% threshold.\nThese results support our hypothesis about the existence of a higher proportion\nof code clones in deep learning code compared to traditional systems. The observed\nprevalence of code clones in deep learning systems underscores the importance of\ninvestigating the reasons for such cloning practices and their impacts on the quality\nof deep learning systems, which we address in the remaining research questions.\nHowever, further investigation is necessary to generalize the ﬁndings and to identify\nthe factors that inﬂuence the prevalence of clones in DL and traditional software\nsystems.\nSummary of ﬁndings (RQ1): As shown by our experimental results,\nthe density of code clones in DL-based systems tend to be higher\nthan that of traditional systems, and the diﬀerence is statistically\nsigniﬁcant for Python systems. Regarding clone types, all three clone\ntypes (Type 1, Type 2, and Type 3) are more prevalent in DL-based\nsystems than in traditional software systems. The comparative higher\nprevalence of clones in DL systems compared to traditional systems\nare likely to be programming language dependent.\n4.2 RQ2: How are code clones distributed in deep learning code in\ncomparison to traditional source code?\nAs we observed in RQ1, the density of code clones in deep learning code tends to be\nhigher compared to traditional systems. We aim to further explore the distribution\nof code clones in terms of their locations. Since, distant code clones may hinder\nthe maintenance process by adding navigation and code comprehension overhead,\n26\nJebnoun et al.\nit is of interest to study how the code clones are distributed in the deep learning\nsystem in terms of their locations.\nWe categorize the detected code clones classes by their location based on the\ntaxonomy proposed by Kapser and Godfrey [43]. If a clone class contains code\nfragments that are all from the same ﬁle, we label them as belonging to the ‘Same\nﬁle’ category. When the clone fragments are from the same directory but from\ndiﬀerent ﬁles, we assign them to the ‘Same directory’ category. Otherwise, if the\nclone fragments of a clone class are from ﬁles from diﬀerent directories, we catego-\nrize them as belonging to the ‘Diﬀerent directories’ category (further details about\nthis classiﬁcation are provided in section 3.2.4). We then count the proportion of\nlines of code clones of diﬀerent location categories for deep learning and tradi-\ntional systems. We also perform the same analysis for individual types of clones\n(Type 1, Type 2, and Type 3) to have insights on their comparative location-based\ndistributions in DL and traditional systems.The analysis based on the proportion\n(in percentage) of the lines of code clones, shows the distribution of clones from\na relative code size point of view. A few larger clone fragments in closer prox-\nimity are likely to have less navigation overhead than a higher number of small\nclone fragments scattered in distant location, even if they have equal total LOC\nof the larger clone fragments. So, fragment-based analysis of the distribution of\nclones is also important to have insights on the potential impacts of clones on the\nmaintenance of the systems. So, we also analyze the location-based distribution\nof the percentages of clone fragments for diﬀerent types of clones. In addition, we\nmanually investigate samples of cloned fragments from each location category to\ngain insights about the characteristics of the clones and to better understand the\nintents and potential impacts of the proximity of clones (in the code base) and\ntheir relative distribution, and impacts on software quality.\nOverall location-based distribution of clones: Fig. 9 shows the distribution of\ncode clones (for the function granularity) in DL and traditional Python source\ncode, regarding the locations of the clones. Based on the median values of the\npercentages of cloned lines of code for each location type in deep learning systems,\nwe observe the following location-based distribution of clones: the percentages of\ncloned lines of code in ‘Same ﬁle’ are higher than that in ‘same directory’ which in\nturn are higher than the percentages of cloned lines of code in ‘diﬀerent directories’.\nIn traditional systems, we observe a location-based distribution similar to that of\nDL systems with the highest percentage of cloned lines being in the ‘same ﬁle’\nfollowed by the ‘same directory’ category and ﬁnally the ‘diﬀerent directories’\ncategory. The distributions of ‘same directory’ and ‘diﬀerent directories’ categories\noverlap signiﬁcantly for clones in traditional code.\nTable 12 shows the p-values from the Mann-Whitney test and Cliﬀ’s delta val-\nues for diﬀerent code clones location between the same type of systems (DL and\ntraditional code) with respect to the relation between code clones locations and\nclone types. ‘ALL’ in the Table 12 designates the unﬁltered Type 3 (i.e, include\nall fragments from Type 1, Type 2, and Type 3). We found statistically signiﬁcant\ndiﬀerences between the ‘same ﬁle’ category and both the ‘same directory’ and the\n‘diﬀerent directories’ categories in the DL code with p-values equal to 2.91e-04\nand 2.9e-10 (which are < 0.05) respectively and with medium eﬀect size of 0.4\nand 0.7, respectively. Thus, in DL systems, a signiﬁcantly higher percentages of\nthe cloned code reside in ‘same ﬁle’ compared to the percentages of cloned code\nClones in Deep Learning Code: What, Where, and Why?\n27\nDeep Learning\nTraditional\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\nSame file\nSame directory\nDifferent directories\nFig. 9 Code Clones Distribution by Location in DL and Traditional Python Systems Regard-\ning Percentage of Lines of Code Clones (LOCC). i.e, (LOCC/total LOCC)x 100\nTable 12 Mann-Whitney Test and Cliﬀ’s Delta Results Regarding the Distributions of Clones\nin DL and Traditional (Trad) Python Projects.\nLocation\nSF-SD\nSF-DD\nSD-DD\nClone Type\nProj Type\np-value\nCD\np-value\nCD\np-value\nCD\nALL\nDL\n2.911e-04\n0.4\n2.9e-10\n0.7\n4.79e-05\n0.46\nTrad\n8.34e-13\n0.83\n2.05e-11\n0.81\n0.15\n0.13\nType 1\nDL\n1.94e-3\n0.41\n0.018\n0.32\n0.03\n0.28\nTrad\n0.16\n0.15\n0.01\n0.42\n0.02\n0.35\nType 2\nDL\n7.64e-09\n0.76\n5.98e-08\n0.77\n0.09\n0.21\nTrad\n3.4e-10\n0.86\n2.82e-10\n0.91\n6.36e-03\n0.41\nType 3\nDL\n6.98e-4\n0.36\n3.66e-10\n0.7\n2.57e-05\n0.48\nTrad\n1.42e-12\n0.83\n1.59e-10\n0.77\n0.29\n0.07\nSF: Same File, SD: Same Directory, DD: Diﬀerent Directories, CD: Cliﬀ’s Delta\nthat reside in the ‘same directory’ and in ‘diﬀerent directories’. Similarly, the per-\ncentage of cloned lines in the ‘same directory’ is signiﬁcantly higher compared to\nthe percentage of clones contained in ‘diﬀerent directories’ with p-value equals to\n4.79e-05 (< 0.05) and with a medium eﬀect size. For traditional code, we observe\nthat a higher percentage of cloned lines of code is located in the ‘same ﬁle’ com-\npared to the percentage of clones that are located in the ‘same directory’ and in\n‘diﬀerent directories’, with p-values < 0.05 (8.34e-13 and 2.05e-11 respectively)\n28\nJebnoun et al.\nDeep Learning\nTraditional\nProject Type (Java)\n10\n20\n30\n40\n50\n60\nProportion of Cloned LOC (%)\nSame file\nSame directory\nDifferent directories\nFig. 10 Code Clones Distribution by Location in DL and Traditional Java Systems Regarding\nPercentage of Lines of Code Clones (LOCC). i.e, (LOCC/total LOCC)x 100\nTable 13 Mann-Whitney Test and Cliﬀ’s Delta Results Regarding the Distributions of Clones\nin DL and Traditional (Trad) Java Projects.\nLocation\nSF-SD\nSF-DD\nSD-DD\nClone Type\nProj Type\np-value\nCD\np-value\nCD\np-value\nCD\nALL\nDL\n0.936\n0.05\n0.378\n0.33\n0.229\n0.44\nTrad\n0.092\n0.61\n0.810\n0.11\n0.297\n0.38\nType 1\nDL\n0.155\n0.66\n0.155\n0.66\n0.810\n0.11\nTrad\n0.022\n0.86\n0.012\n1.0\n0.082\n0.66\nType 2\nDL\n0.229\n0.44\n0.378\n0.33\n0.936\n0.05\nTrad\n0.065\n0.66\n0.784\n0.13\n0.315\n0.4\nType 3\nDL\n0.810\n0.11\n0.297\n0.38\n0.297\n0.38\nTrad\n0.170\n0.53\n0.936\n0.05\n0.522\n0.26\nSF: Same File, SD: Same Directory, DD: Diﬀerent Directories, CD: Cliﬀ’s Delta\nClones in Deep Learning Code: What, Where, and Why?\n29\nTraditional\nDeep Learning\nProject Type (C#)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nProportion of Cloned LOC (%)\nSame file\nSame directory\nDifferent directories\nFig. 11 Code Clones Distribution by Location in DL and Traditional C# Systems Regarding\nPercentage of Lines of Code Clones (LOCC). i.e, (LOCC/total LOCC)x 100\nTable 14 Mann-Whitney Test and Cliﬀ’s Delta Results Regarding the Distributions of Clones\nin DL and Traditional (Trad) C# Projects.\nLocation\nSF-SD\nSF-DD\nSD-DD\nClone Type\nProj Type\np-value\nCD\np-value\nCD\np-value\nCD\nALL\nDL\n0.368\n0.21\n0.752\n0.07\n0.56\n0.14\nTrad\n0.001\n0.71\n0.044\n0.46\n0.544\n0.14\nType 1\nDL\n0.039\n1.0\n0.124\n0.77\n0.901\n0.04\nTrad\n0.087\n0.51\n0.0169\n0.72\n0.221\n0.29\nType 2\nDL\n0.488\n0.18\n0.113\n0.43\n0.230\n0.35\nTrad\n0.0001\n0.86\n0.004\n0.66\n0.85\n0.04\nType 3\nDL\n0.341\n0.22\n0.644\n0.11\n0.849\n0.05\nTrad\n0.0003\n0.80\n0.009\n0.59\n0.576\n0.13\nSF: Same File, SD: Same Directory, DD: Diﬀerent Directories, CD: Cliﬀ’s Delta\nand with large eﬀect sizes (0.83 and 0.81 respectively). We found no statistically\nsigniﬁcant diﬀerence between the percentage of clones contained in the ‘same di-\nrectory’ category and the ’diﬀerent directories’ category, for traditional code.\nFig. 10 shows the proportion of cloned code (LOC %) in diﬀerent location\ncategory for Java systems. Based on the median values of the proportion of cloned\ncode, we observe that the highest proportions of cloned code for Java DL systems\nreside in ‘diﬀerent directory’ followed by the proportion of cloned code in ’same\n30\nJebnoun et al.\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nProportion of Cloned LOC (%)\n45.8%\n33.0%\n21.2%\nDeep Learning\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\n50\nProportion of Cloned LOC (%)\n55.22%\n23.54%\n21.24%\nTraditional\nFig. 12 Percentages of Lines of Code Clones by Location of Clones in both Deep Learning\nand Traditional Systems (Python)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nProportion of Cloned LOC (%)\n31.5%\n27.16%\n41.32%\nDeep Learning (java)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\n50\nProportion of Cloned LOC (%)\n36.05%\n14.49%\n49.45%\nTraditional (java)\nFig. 13 Percentages of Lines of Code Clones by Location of Clones in both Deep Learning\nand Traditional Systems (Java)\ndirectory’ and in ‘same ﬁle’ categories. Similar trend in the distribution of cloned\ncode is observed in Java traditional systems (except for ‘same directory’ category)\nwith ‘diﬀerent directory’ category containing the highest median percentage of\ncloned LOC. This is an indication that clones in Java DL and traditional systems\ntend to be dispersed. However, from the results of the MWW test and Cliﬀ’s delta\neﬀect size for ‘All’ types as shown in Table 13, we do not observe the diﬀerences\nto be statistically signiﬁcant.\nFor C# systems as in Fig. 11, the median value for the distribution of clones\nin DL systems is the lowest for ‘same ﬁle’ location category while it is the opposite\nfor traditional systems. This shows that clones in C# DL systems are likely to\nbe dispersed compared to traditional C# systems. However, from our statistical\ntest results for C# systems in Table 14, we do not observe statistically signiﬁ-\ncant diﬀerences in distribution of cloned code in diﬀerent location categories. For\ntraditional C# systems, the diﬀerences in proportion of clones in ‘same ﬁle’ are\nsigniﬁcantly high (p-values < 0.05 with medium to large eﬀect size) compared to\n‘same directory’ and ‘diﬀerent directory’ categories.\nClones in Deep Learning Code: What, Where, and Why?\n31\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\n50\nProportion of Cloned LOC (%)\n22.8%\n47.9%\n29.3%\nDeep Learning (c#)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nProportion of Cloned LOC (%)\n46.4%\n29.5%\n24.1%\nTraditional (c#)\nFig. 14 Percentages of Lines of Code Clones by Location of Clones in both Deep Learning\nand Traditional Systems (C#)\nFor further insights, we analyzed the average percentages of lines of cloned\ncode by their locations in deep learning and traditional code. As shown in Fig.\n12, we identify that 45.8% of the DL-related clones are in the ‘same ﬁle’, 33%\nare in the ‘same directory’ and 21.2% are in ‘diﬀerent directories’. Hence, DL\nclones are more dispersed, having fewer percentages of clones in the same ﬁle and\nmore than 54% (33% + 21.2%) in diﬀerent ﬁles and directories. Code clones in\ntraditional code, on the other hand, are more localized. More than the half of\nthe code clones in non-DL code (55.22%) are in the same ﬁle, 23.54% are in the\nsame directory and 21.24% are in diﬀerent directories. Therefore, according to our\nresults, code clones in Python deep learning code are more dispersed than code\nclones in non-DL systems. For Java as in Fig. 13, the highest proportion of clones\nfor both DL (41.32%) and traditional (49.45%) systems are in ‘diﬀerent directories’\ncontrasting to Python systems (Fig. 12). Although, traditional Java systems have\nhigher proportion of clones in diﬀerent directories compared to Java DL systems,\nDL systems have 68.48% (41.32 + 27.16) of clones in ‘same directory’ and ‘diﬀerent\ndirectories’ combined, compared to that of traditional Java code (49.45+14.49=\n63.94%). Thus, clones in Java DL systems are relatively more dispersed compared\nto clones in traditional systems. For C# systems as in Fig. 14, 77.2% (47.9 + 29.3)\nof cloned code is in ‘same directory’ and ‘diﬀerent directories’ compared to 53.6%\n(29.5 + 24.1) for traditional. This shows that clones in C# DL systems are more\ndispersed compared to traditional cloned code.\nFor our location-based analysis of the distribution of clones, we observe that\nclones in DL systems are more dispersed compared to traditional clones. We also\nobserve that clones in Java and C# DL systems are more dispersed compared\nto Python DL systems. This dispersion of cloned code in the deep learning sys-\ntems may harm the maintenance of duplicated code due to potential navigation\nand comprehension overhead. The degree of dispersion of clones may also vary\ndepending on programming languages.\n32\nJebnoun et al.\nLocation-based distribution of diﬀerent types of clones: We analyze the location-\nbased distribution of diﬀerent types of clones as follows:\nType 1: As shown in Fig. 15-A, the median of the distribution of the percentage\nof Type 1 cloned lines in ‘same ﬁle’ in Python DL code is the lowest compared to\nthe percentages of cloned lines in ‘same directory’ and in ‘diﬀerent directory’. This\nshows that Type 1 clones in DL code are dispersed in diﬀerent ﬁles and directories.\nHowever, for traditional systems, we observe that majority of the Type 1 clones are\nin ‘same ﬁle’ compared to ‘same directory’ and ‘diﬀerent directories’. This suggests\nthat Type 1 clones in traditional systems reside in closer proximity, unlike the Type\n1 clones in Python DL systems.\nTo investigate whether the observed diﬀerences are statistically signiﬁcant, we\nperform MWW tests (two-tailed, signiﬁcance at 0.05) and measure Cliﬀ’s delta\neﬀect size. In Table 12, we highlight in bold the statistically signiﬁcant diﬀerences\nfor the distributions of cloned lines in diﬀerent clone locations for both DL and\ntraditional Python code with respect to clone types where p-values are < 0.05.\nFig. 15 represents these diﬀerences by showing the distribution of percentages of\nlines of code clones for each clone type and for both types of systems. Type 1\nclones in deep learning Python code is less localized with a statistically signiﬁcant\ndiﬀerence between the same ﬁle location category and the others categories and\nwith a small eﬀect size.\nWhereas proportion of Type 1 clones in non-DL code shows a statistically\nsigniﬁcant diﬀerence only between the ‘diﬀerent directories’ location and the others\nlocations. The distribution of lines of code clones located in ‘diﬀerent directories’\nis lower in comparison to the distribution of line of code clones in other locations\n(Same ﬁle (SF), and Same Directory (SD)).\nThe existence of exact clones (Type 1) in the same directory could shed lights\non some implementation practices of deep learning developers in Python. For ex-\nample, the occurrence of exact functions in the same directory may suggest that\nwhen DL developers have a working code that builds a model properly, they are\ninclined to copy-paste this same code in another ﬁle in the same directory to con-\nstruct a similar model to try another conﬁguration. One example is shown in Table\n15, where we found in the same directory ’inference’, two models Movidius and\nYolo that contains the exact function to calculate the accuracy of the model ’iou’\nwhich stands for Intersection Over Union for object detection. Building models\nmay have the same common functions like computing accuracy or implementing\nthe activation function. These functions could be exact for each model, which may\nexplain the high occurrence of Type 1 clones in the same directory in deep learning\nprojects, compared to traditional projects.\nType 2: As shown in Fig. 15-B, the distribution of the percentage of the lines\nof Type 2 cloned code in ‘same ﬁle’ in Python DL code is higher compared to\nthe distributions of Type 2 clones in ‘same directory’ and ‘diﬀerent directories’\ncategories. This implies that Type 2 clones in Python DL code reside in closer\nproximity. For traditional systems, we see a similar distribution of the percentage\nof Type 2 cloned lines in diﬀerent location categories. However, the percentages\nof Type 2 clones in ‘diﬀerent directories’ in Python DL code tend to be slightly\nhigher compared to that in traditional code.\nClones in Deep Learning Code: What, Where, and Why?\n33\nWe found these diﬀerences between the percentages of cloned lines in ‘same ﬁle’\nand in both ‘same directory’ and ‘diﬀerent directories’ categories for Type 2 clones\nin Python DL code statistically signiﬁcant with p-values equal to 7.64e-09 and\n5.98e-08, respectively (Table 12). The values of eﬀect size of these diﬀerences are\nlarge between ‘same ﬁle’ and ‘same directory’ (0.76) and ‘same ﬁle’ and ‘diﬀerent\ndirectories’ (0.77).\nDeep Learning\nTraditional\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (A) Type 1 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (B) Type 2 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (C) Type 3 \nSame file\nSame directory\nDifferent directories\nFig. 15 Distribution of Diﬀerent Types of Clones by Clone Location in DL and Traditional\nCode (Python)\nHence, Type 2 clones are less dispersed in Python deep learning code than\nother types of clones. We notice a similar level of dispersion for Type 2 clones in\nnon-DL Python code (as shown in Fig. 15-B).\nType 3: As shown in Fig. 15-C, Type 3 clones are less dispersed compared to Type\n1 clones but comparatively more dispersed than Type 2 clones in Python deep\nlearning code. We present the results of the MWW tests in Table 12 where the p-\nvalues are < 0.05 for deep learning code. The percentage of Type 3 clones located\nin the same ﬁle is the highest, in comparison to the percentages of Type 3 clones\n34\nJebnoun et al.\nTable 15 Clone Codes Example where the Location is in the Same Directory and Type 1\nClone\npath: BerryNet/inference/movidius.py\ndef iou(box1, box2):\ntb = (min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) -\nmax(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2]))\nlr = (min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) -\nmax(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3]))\nif tb < 0 or lr < 0:\nintersection = 0\nelse:\nintersection =\ntb*lr\nreturn intersection / (box1[2] * box1[3] + box2[2] * box2[3] - intersection)\npath: BerryNet/inference/yoloutils.py\ndef iou(box1, box2):\ntb = (min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) -\nmax(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2]))\nlr = (min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) -\nmax(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3]))\nif tb < 0 or lr < 0:\nintersection = 0\nelse:\nintersection =\ntb*lr\nreturn intersection / (box1[2] * box1[3] + box2[2] * box2[3] - intersection)\nlocated in the ‘same directory’ and ‘diﬀerent directories’. In non-DL code, Type 3\nclones are also located in the ‘same ﬁle’ in high numbers. For traditional code, we\nfound a higher percentage of lines of code clones located in the ‘same ﬁle’ com-\npared to the ‘same directory’ and ‘diﬀerent directories’ categories. However, the\ndiﬀerence is not statistically signiﬁcant between the ‘same directory’ and ‘diﬀerent\ndirectories’ categories, in traditional code as shown in Table 12.\nFrom our clone-type centric analysis of the distribution of cloned code in Java\n(Fig. 33) and C# (Fig. 34) systems in the Appendix C, Type 1 clones in DL\nsystems are dispersed with major proportion of cloned code in ’same directory’\nand ‘diﬀerent directories’ location categories than the proportion of cloned code\nin ‘same ﬁle’. Type 2 clones on the other hand, tend to reside in ‘same ﬁle’ having\nhigher median value of the percentages of cloned code in both DL and traditional\nsystems. The median values for the percentage of Type 2 cloned code is slightly\nhigher in DL systems compared to the traditional code. For Type 3 clones in\nTraditional Java and C# systems, the percentages of cloned code in ‘same ﬁle’\ncategory is higher compared to the percentages of clones in ‘same directory’ and\n‘diﬀerent directories’. Java and C# Traditional systems also have higher percent-\nage of Type 3 clones in ’same ﬁle’ compared to DL systems. This indicates that\nType 3 clones in Java and C# DL systems are relatively more dispersed compared\nto traditional systems. Although we observe clones tend to be relatively dispersed\nin DL systems compared to traditional code, we do not observe statistically signif-\nicant diﬀerences in the location based distribution of clones in most cases for Java\nsystems. For traditional C# systems, we observe the diﬀerences between propor-\ntion of clones in ’same ﬁle’ and in ’same directory’ or ’diﬀerent directories’ to be\nstatistically signiﬁcant (except for Type 1).\nFrom this analysis of the distribution of clones across the diﬀerent ﬁles and\ndirectories of the studied projects, we can conclude that clones in deep learning\ncode is more dispersed compared to clones in traditional code, although the dif-\nClones in Deep Learning Code: What, Where, and Why?\n35\nferences in distribution may vary. Type 1 and Type 3 clones have relatively higher\ntrends of being dispersed while Type 2 clones tend to be more localized (in the\nsame ﬁle). However, the percentages of lines of cloned code may not always fully\nreﬂect their relative impacts on the systems. For example, a higher number of\nsmall sized cloned fragments scattered in distant locations may pose higher chal-\nlenges in change propagation than a few larger cloned fragments located not too\nfar apart. Therefore, we further analyze the distribution of the number of cloned\nfragments in diﬀerent location categories.\nDeep Learning\nTraditional\nLocation Category\n0\n20\n40\n60\n80\n100\nPercentage (%) of Cloned Fragments\nSame file\nSame directory\nDifferent directories\nFig. 16 Distribution of Percentage of Number of Fragments of Code Clones Classes per Clone\nLocation in Python Systems.\nLocation-based distribution of clone fragments: Figure 16 shows the percent-\nages of number of fragments ((number of clone fragments in a location cate-\ngory/total number of clone fragments)x100) for each location category in Python\nsystems. We observe that the proportion of clone fragments tend to be higher in\nthe ‘same ﬁle’ location category. We found a statistically signiﬁcant diﬀerence be-\ntween the distribution of the proportion of clone fragments located in the ’same ﬁle’\nand in both ‘same directory’ and ‘diﬀerent directories’. Their p-values are equal\nto 4.46e-05 and 1.94e-07 respectively with values of eﬀect size of 0.44 (small) and\n0.57 (medium), respectively in deep learning code.\nThe mean value of the percentages of code fragments that belong to the same\ndirectory category in Python deep learning code is 32.04% with a standard de-\nviation (STD) of 18.48. The number of fragments inﬂuences the degree to which\nthe identiﬁed code clones from the same directory are diﬃcult to maintain: the\nhigher the number of fragments, the more troublesome their maintenance is likely\n36\nJebnoun et al.\nto be. Hence, DL code may become more problematic with the spread of many\nduplicated code fragments that tend to be exact (Fig. 20-A) and in diﬀerent ﬁles,\nbut in the same directory. We observe that the distributions of the percentages of\nclone fragments of diﬀerent clone types in diﬀerent locations (Fig. 20) is similar\nto the distributions of the percentages of lines of cloned code (Fig. 15).\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n46.7%\n33.1%\n20.2%\nDeep Learning (Python)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\n50\nPercentage (%) of Cloned Fragements\n56.1%\n20.6%\n23.3%\nTraditional (Python)\nFig. 17 Percentages of Average Number of Fragments of Code Clones by Location of Clones\nin both Deep Learning and Traditional Python Systems\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n27.6%\n30.66%\n41.72%\nDeep Learning (Java)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\n50\nPercentage (%) of Cloned Fragements\n33.33%\n18.97%\n47.7%\nTraditional (Java)\nFig. 18 Percentages of Average Number of Fragments of Code Clones by Location of Clones\nin both Deep Learning and Traditional Java Systems\nClones in Deep Learning Code: What, Where, and Why?\n37\nSame file\nSame directory\nDifferent directories\n0\n5\n10\n15\n20\n25\n30\n35\nPercentage (%) of Cloned Fragements\n35.5%\n37.35%\n34.96%\nDeep Learning (c#)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n45.99%\n21.02%\n35.52%\nTraditional (c#)\nFig. 19 Percentages of Average Number of Fragments of Code Clones by Location of Clones\nin both Deep Learning and Traditional C# Systems\nFor further insights, we analyze the proportion of the number of clones frag-\nments in each clone class by their locations in deep learning and traditional code.\nAs shown in Fig. 17 for Python systems, we identify that the distribution of the\nnumber of fragments in each clone class is of 46.7% when clones are DL-related\nclones and are in the ‘same ﬁle’, 33.1% are in the ‘same directory’ and 20.2% are\nin ‘diﬀerent directories’. Hence, based on the distribution of clone fragments, DL\nclones are found to be located in the same ﬁle with a percentage equals to 46.7%,\nwhich is nearly the same proportion as it is located in diﬀerent ﬁles (i.e, in the\nsame directory or in diﬀerent directories (33.1%+20.2%)). However, fragments of\nclones in Python deep learning code are relatively more dispersed compared to\nfragments of clones in traditional system as traditional systems have higher pro-\nportion of clones fragments in ‘same ﬁle’ compared to deep learning code as shown\nin Fig. 17. For Java systems (Fig. 18), the major proportion of clone fragments\nin both DL and traditional code are in ’same directory’ and ’diﬀerent directories’\ncontaining 72.38% (30.66 + 41.72) and 66.67% (18.97 + 47.70) of cloned fragments\nrespectively. So, clone fragments in Java DL systems are more dispersed compared\nto the clones in traditional Java systems. In Fig. 18 showing the distribution of\ncloned fragments in C# systems, the propertion of clones in DL systems and tradi-\ntional systems are 72.31% (37.35+34.96) and 56.54% (21.02 + 35.52) respectively.\nThis also indicates that for C#, clone fragments in DL code is more dispersed\ncompared to traditional systems similar to Python and Java systems. This higher\ndispersion clone fragments in the deep learning code likely to have negative conse-\nquences on maintenance due navigation and comprehension overhead for distant\ncode.\nWe also repeated the analysis for RQ2 with 20% dissimilarity threshold for\nNiCad clone detector. However, the overall trends in the distribution of clones in\ndiﬀerent location categories remain the same as shown by our results as shown in\nAppendix C.\n38\nJebnoun et al.\nDeep Learning\nTraditional\nLocation Category\n0\n20\n40\n60\n80\n100\nPercentage (%) of Cloned Fragments\n (A) Type 1 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\nLocation Category\n0\n20\n40\n60\n80\n100\nPercentage (%) of Cloned Fragments\n (B) Type 2 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\nLocation Category\n0\n20\n40\n60\n80\n100\nPercentage (%) of Cloned Fragments\n (C) Type 3 \nSame file\nSame directory\nDifferent directories\nFig. 20 Distribution of Percentage of Number of Fragments of Code Clones Classes per Clone\nLocation.\nQualitative analysis of the clones in diﬀerent locations category:\nWe manually\nexamined the clones contained in the diﬀerent locations categories and observed\nthat:\nSame ﬁle: DL practitioners often duplicate functions in the same ﬁle when con-\nﬁguring diﬀerent models. We found cloned functions in the same ﬁle with names\nrepresenting the name of the model with slight modiﬁcation in initializing (hy-\nper)parameters of each model as shown in Table 16. This type of code clones\nlocated in close proximity may be relatively less problematic. This is mainly due\nto the ease of navigation between code clones during maintenance as their loca-\ntions are not too distant from each other. Consequently, they may be less prone\nto inconsistent updates, which is a key reason behind the introduction of faults\nin cloned code. Duplicating code in close proximity is widely used to simplify the\nconception of the system [43] by renaming functions to facilitate code reuse and\nto make the cloned functions’ name more related to its purpose, which improves\nprogram comprehension. These type of cloned functions with structural similarity\nClones in Deep Learning Code: What, Where, and Why?\n39\nbut with identiﬁer naming and data type diﬀerences are Type 2 clones. The trends\nof such clones to be in closer proximity is also reﬂected in our results from Fig.\n15-B.\nTable 16 Clone Codes Example where the Location is in the Same File (the Diﬀerences are\nHighlighted in Gray)\ndef mobile_imagenet_config():\nreturn tf.contrib.training.HParams(\nstem_multiplier=1.0,\ndense_dropout_keep_prob=0.5,\nnum_cells=12,\nfilter_scaling_rate=2.0,\ndrop_path_keep_prob=1.0,\nnum_conv_filters=44,\nuse_aux_head=1,\nnum_reduction_layers=2,\ndata_format='NHWC',\nskip_reduction_layer_input=0,\ntotal_training_steps=250000,\nuse_bounded_activation=False,\n)\ndef large_imagenet_config():\nreturn tf.contrib.training.HParams(\nstem_multiplier=3.0,\ndense_dropout_keep_prob=0.5,\nnum_cells=18,\nfilter_scaling_rate=2.0,\nnum_conv_filters=168,\ndrop_path_keep_prob=0.7,\nuse_aux_head=1,\nnum_reduction_layers=2,\ndata_format='NHWC',\nskip_reduction_layer_input=1,\ntotal_training_steps=250000,\nuse_bounded_activation=False,\n)\nTable 16 represents examples of cloned code where the location is in the same\nﬁle. Through a manual analysis, we tried to understand where DL developers du-\nplicate code regarding DL phases perspectives. It was found that 40 code clones\nclasses were in the same ﬁle and were corresponding to the DL phase. We found\n27 of them to be similar functions with a common purpose. These functions were\ncloned to perform the same or closely similar tasks (to build the model with some\nmodiﬁcations). The modiﬁcations were achieved by renaming functions (giving\nthem names that are more meaningful and relevant to the task context) and pa-\nrameterizing the code with diﬀerent values that are speciﬁc to each model. As\nshown in Table 16, we have two functions that parameterize two diﬀerent models.\nThis is performed by calling a library routine capable of setting the hyperparam-\neters of the model. It is conﬁgured as key-value pairs to build the model. Each\nfunction is renamed to be relevant to the model and we see slight diﬀerences be-\ntween the values of each hyperparameter. An important number of this type of\nduplication is cloning similar functions with diﬀerent names and parameter types,\nleading to Type 2 clones. This type of code duplication may explain the high per-\ncentage of Type 2 clones that are found to reside in the same ﬁle (Fig. 15-B). Our\nﬁndings also conﬁrm the results of previous work [43].\nSame directory: Regarding the second category where clones exist in diﬀerent\nﬁles but in the same directory, it is common to ﬁnd duplicated functions without\nor with minor changes [43]. In our case and as shown in Figure 15-A, for deep\nlearning code, Type 1 clones are the type of clones that are frequently located in\nthe same directory but in diﬀerent ﬁles. From a manual examination of all of the\ncode clones (85) that exist in the same directory but in diﬀerent ﬁles, we found\nin 49 code clones classes (57%) a ﬁle named utility containing useful functions\nneeded to perform the construction of deep learning models. This suggests that\nDL developers either refactor their code without deleting the old functions, or\ndiﬀerent developers working on the same project are not conscious about the\nexistence of such ﬁles.\n40\nJebnoun et al.\nDiﬀerent directories: Regarding clones located in diﬀerent directories, we\nmanually analyzed all the code clones fragments (102) that were located in diﬀerent\ndirectories that 63% of them are not related to deep learning code. We often detect\nthis type of duplication when it comes to verifying libraries’ versions to choose the\nright routine call, deallocate memory, or getting model metadata (logging).\nSummary of ﬁndings (RQ2): According to our results, code clones in deep\nlearning code are more dispersed than in traditional code in terms of the distri-\nbution of code clones. While for clone types, Type 1 clones are more dispersed\nin DL systems while Type 2 clones tend to be localized in the same ﬁle. Type\n3 clones are spread in diﬀerent locations but with a high percentage of lines of\ncode clones residing in the same ﬁle. Regarding the distribution of the number of\nclone fragments, clones in deep learning code tend be more dispersed compared\nto clones in traditional software systems.\n4.3 RQ3: Do cloned and non-cloned code suﬀer similarly from\nbug-proneness in deep learning projects?\nSince deep learning systems are relatively fast to develop and deploy, code quality\nis often overlooked and it is frequently the case that the code is re-used and rarely\nrefactored [99]. Several previous studies in traditional code highlighted the negative\nimpacts of code clones on the maintenance and comprehension of code. Barbour\net al. [5] found clones to be related to a high risk of bugs. Given the complexity\nof deep learning systems, it is likely that clones can have a similar averse eﬀect\non maintenance and bug-proneness. Hence, in this work, we analyze the bug-\nproneness of clones in deep learning code from two perspectives: (1) we examine\ncorrelations between the co-occurrences of clones and bugs in deep learning code,\nand (2) examine whether clones aﬀect the time required to ﬁx bugs in deep learning\nsystems. We perform these investigations ﬁrst on all clones and then for individual\nclone types (Type 1, Type 2, Type 3). We analyze all the commit history to identify\nall buggy commits (details are presented in Section 3.2.6). In order to determine\nthe co-existence between bugs and code clones in deep learning code, we match\ncode changes in bug-ﬁxing commits with code clones by ﬁnding the intersection\nbetween the lines changed to ﬁx bugs and the cloned lines of code.\nWe consider that a bug ﬁx commit is related to code clones when the buggy lines\nbelong to any duplicated code, otherwise it is considered as related to non-cloned\ncode. Then, we calculate the percentage of bug-ﬁx commits related to cloned and\nnon-cloned code for each project. Finally, we compute the average percentage of\nbug-ﬁx commits related to cloned and non-cloned code, to comparatively evaluate\ntheir bug-proneness in the context of deep learning code.\nOur results show that 75.85% of bug-ﬁx commits in Python DL systems are\nrelated to clones, i.e., in other words, more than three-quarters of the bugs in\ndeep learning code are related to clones. When we do similar analysis on bug-ﬁx\ncommits in C# and Java DL systems, we found that 27.54% are related to clones\nin C# DL systems and 45.31% are related to clones in Java DL systems. We\nperform MWW tests for the distributions of code clones and non-clone code in the\nPython DL bug-ﬁx commits. We found a statistically signiﬁcant diﬀerence between\nthe distribution of the number of commits that ﬁx bugs on cloned lines and the\nClones in Deep Learning Code: What, Where, and Why?\n41\ndistribution of bug-ﬁx commits on non-cloned code, with a p-value equals to 0.026\nand an eﬀect size of 0.55 (medium). Thus, the bug-proneness of cloned code in deep\nlearning code is higher compared to that of non-clone code. However, the degree\nof bug-proneness may vary with systems of diﬀerent programming languages, as\nwe observe in our results.\nNow, we further investigate the bug-proneness of diﬀerent types of clones in\nDL code to gain deeper insights on the types of clones that are likely to be more\nrisky (in terms of bug occurrence). This information would help deep learning\ndevelopers to carefully prioritize clones for refactoring and tracking. Figure 21, 22\nand 23 show the percentages of clones from diﬀerent types (Type 1, Type 2, and\nType 3) that are related to bugs for Python, C# and Java DL systems respectively.\nType 1\nType 2\nType 3\n0\n10\n20\n30\n40\n50\n60\n70\npercentage of buggy clone\n5.75%\n19.48%\n74.77%\nFig. 21 Buggy Code Clones Occurrences by Clone Type for Python DL systems\nType 1\nType 2\nType 3\n0\n10\n20\n30\n40\n50\n60\n70\npercentage of buggy clone\n12.22%\n13.1%\n74.68%\nFig. 22 Buggy Code Clones Occurrences by Clone Type for C# DL systems\nWe ﬁnd that Type 3 clones are the most likely to be buggy in all types of\nDL systems as 74.77%, 74.68% and 61.67% of clone related bugs are Type 3\nclones for Python, C# and Java DL projects. Then, we have Type 2 clones with a\npercentage of 19.48%, 13.1% and 29.04% for Python, C# and Java respectively,\n42\nJebnoun et al.\nType 1\nType 2\nType 3\n0\n10\n20\n30\n40\n50\n60\npercentage of buggy clone\n9.28%\n29.04%\n61.67%\nFig. 23 Buggy Code Clones Occurrences by Clone Type for Java DL systems\nand ﬁnally Type 1 clones with a percentage of 5.75%, 12.22% and 9.28% clones\nrelated to bugs. These results obtained from deep learning code are similar to the\nﬁndings of previous works comparing the bug-proneness of Type 1, Type 2, and\nType 3 clones in traditional software systems [65].\nSince Type 3 clones are higher in density (Fig. 6) and prevalent according to\nthe distribution of the percentages of lines of code in diﬀerent types of clones,\nthey are possibly being associated with higher percentages of bugs too. Thus,\nwe investigate further their bug-proneness by studying the percentage of clone\nfragments in each clone types (Type 1, Type 2, Type 3) that are related to bugs.\nAs shown in Figure 24, we ﬁnd that 1.71% of clone fragments are buggy in Type\n1 clones, 2.26% of clone fragments are buggy in Type 2 clones and 2.11% of clone\nfragments are buggy in Type 3 clones. This shows that a higher percentage of\nType 2 clone fragments are related to bugs, followed by Type 3 clones and then\nType 1 clones. However, there are more Type 3 clones in the deep learning code\n(Fig. 6) compared to Type 1 and Type 2 clones. This explains the observation\nthat Type 3 clones contains the highest fractions of clone related bugs (Fig. 21)\ndespite the percentages of buggy clone fragments in Type 3 not being higher than\nthat of Type 2 clones.\nType 1\nType 2\nType 3\n0.0\n0.5\n1.0\n1.5\n2.0\npercentage of buggy fragments\n1.71%\n2.26%\n2.11%\nFig. 24 Percentages of Buggy Code Fragments by Clone Type for python DL systems\nClones in Deep Learning Code: What, Where, and Why?\n43\nType 1\nType 2\nType 3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\npercentage of buggy fragments\n0.73%\n0.63%\n0.72%\nFig. 25 Percentages of Buggy Code Fragments by Clone Type for C# DL systems\nType 1\nType 2\nType 3\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\npercentage of buggy fragments\n0.16%\n0.27%\n0.28%\nFig. 26 Percentages of Buggy Code Fragments by Clone Type for Java DL systems\nFigure 25 and 26 shows the trends for C# and Java DL systems. We ﬁnd that\n0.73% and 0.16% clone fragments are buggy in type 1 clones, 0.63% and 0.27%\nare buggy in Type 2 clones and 0.72% and 0.28% are buggy in Type 3 clones for\nC# and Java DL systems respectively. The highest percentage is from Type 3 for\nJava and Type 1 for C#.\nOur results show that clones in deep learning systems are more related to bugs\ncompared to non-cloned code. This observed bug-proneness of clones in DL code\nis likely to be related to diﬀerent confounding factors such as relative size and\ndistribution of cloned and non-cloned code, clone types, programming language,\nclone detection tool and diﬀerent conﬁguration settings, etc. We carefully consid-\nered these confounding factors to avoid potential threats. As size of the clones and\nnon-cloned is known to be an important confounding factor for empirical analysis\nof clones [86], we investigated the relative size of the cloned and non-cloned code.\nWe observed that on an average 27.45% of code in the DL systems are cloned code.\nWe also compared the relative size of the cloned and non-cloned functions. NiCad\nextracts all functions in the system as candidate clone list for clone detection. We\nused those extracted functions to separate cloned and non-cloned functions for\n44\nJebnoun et al.\nour analysis by comparing to the clone detection results. We then analyzed the\ncomparative size distribution of cloned and non-cloned functions. As shown in the\nAppendix D (Fig. 41, Fig. 42, Fig. 43), we do not observe any statistically signiﬁ-\ncant diﬀerence between the size of the cloned and non-cloned functions. However,\nnon-cloned functions tend to be larger in size compared to cloned functions as\ncan be seen on the median values of their distributions. The diﬀerences between\nthe sizes of cloned and non-cloned functions are not statistically signiﬁcant in DL\nand traditional code, and we observe a similar trend for programming language\nvariations (Fig. 43).\nNow, we study how clones in deep learning code aﬀect the time to ﬁx bugs\nin deep learning code. We investigate whether the time required to ﬁx bugs in\ncode clones is more than when bugs are not in code clones. Thus, we calculated\nthe time between bug-ﬁxing commit and the corresponding bug-inducing commit.\nThen, we compare the average time spent to ﬁx bugs when bugs are related to\nclones (bug-ﬁx lines intersect with code clones) and when the bug is not clone\nrelated.\nCloned Code\nNon-Cloned Code\n \n 0:0:0:0\n 23:3:33:20\n 46:7:6:40\n 69:10:40:0\n 92:14:13:20\n 115:17:46:40\n 138:21:20:0\n 162:0:53:20\n 185:4:26:40\nAvg Bug Fix Time (day:hour:minutes:seconds)\nFig. 27 Comparative Bug-Fix Times for Cloned and non-Cloned Code in python DL Systems.\nFigure 27 shows the distribution of the average times to ﬁx bugs for python\nDL systems when the bug is located in a clone and when it is not. Comparing\nthe median between the two distributions of time, we can see on Figure 27 that\nthe time required to ﬁx bugs when there is cloned code is slightly higher than\nthe time required to ﬁx a bug when the bug is not related to a code clone. The\nmean value of the average time to ﬁx bugs in clones in DL code is 134.0 days,\n6.0 hours, 13.0 minutes and 20.0 seconds with a STD of 95.0 days, 22.0 hours,\n46.0 minutes and 40.0 seconds, and when it comes to non cloned code, the mean\ntime to ﬁx a bug is 122.0 days, 16.0 hours, 26.0 minutes and 40.0 seconds with a\nSTD of 96.0 days 15.0 hours 26.0 minutes and 40.0 seconds. Buggy cloned code\nClones in Deep Learning Code: What, Where, and Why?\n45\nseems to be taking comparatively more time to get ﬁxed in deep learning code.\nWe perform a Man-Whitney test comparing the distribution of times. However, we\nfound no statistically signiﬁcant diﬀerence between the time to ﬁx bugs in cloned\nand non-cloned code (p-value =0.34, eﬀect size 0.2).\nSimilarly, the average bug ﬁx time (mean=77 days) for cloned-codes in C#\nDL systems is higher than non-cloned code (mean=51 days). ConvNetSharp DL\nsystem has average bug ﬁx time of 132 days for cloned code and 89 days for non\ncloned code while NeuralNetwork.NET DL system has average bug ﬁx time of\n22 days for cloned code and 13 days for non cloned code. On the other hand, the\naverage bug ﬁx time of cloned-codes in Java DL systems (mean=63 days) is slightly\nlower than that of non-cloned codes (mean=64 days). The knime-deeplearning Java\nDL system has average bug ﬁx time of 105 days for clone-code and 102 days for\nnon-clone code while Neuralnetworks DL system has average bug-ﬁx time of 21\nfor cloned code and 26 for non-cloned code.\nWe further check what percentages of DL systems have higher bug-ﬁx time for\nclones and what percentages of systems have the opposite. Among 10 DL subject\nsystems, we observe in seven DL systems that the bug-ﬁx time for clones is higher\nand for the rest (i.e., three systems), the bug-ﬁx time for clones is lower. Overall,\nwe can conclude that in a majority of cases (70%), bugs related to clones take a\nlonger time to get ﬁxed, suggesting that bugs occurring in cloned code may be\nmore challenging to ﬁx.\nThe observation that bugs in cloned code take comparatively longer time to\nﬁx means that the existence of code clones in deep learning code may hinder the\nmaintenance of this type of system.\nSummary of ﬁndings (RQ3): According to our results, we ﬁnd that cloned\ncode is likely to be more bug-prone than non-cloned code in deep learning sys-\ntems. In addition, Type 3 clones have a relatively higher odd to be involved in\nbugs in the deep learning code, followed by Type 2 and Type 1 clones respec-\ntively. Also, bugs related to clones in DL code tend to take more time to get ﬁxed\ncompared to other bugs.\n4.4 RQ4: Why do deep learning developers clone code?\nIn this research question, we examine the reasons behind the practice of code\ncloning in deep learning systems. We manually analyzed the detected code clones\nfor a selected subset of six deep learning projects. We labeled each detected code\nclone class by the functionality it serves (task). Then, we assign each labeled clone\nclass to its corresponding DL phase making sure that the relation between the\nlabeled task and the DL phase is ‘to perform’ (more details in section 3.2.5).\nTable 17 shows the taxonomy of code clones that resulted from our manual\nanalysis of the selected six DL projects. We show only the related DL phases\nthat co-occur with the detected code clones. Therefore, we have neither all the\nDL phases nor all of its subcategories presented in Table 17. Also, the DL phase\nsubcategories are not exclusive, since functions may be used in tasks related to\ndiﬀerent phases.\nIn this research question, we study clones at the granularity of the function.\nA function can implement one or more tasks that are involved in a DL phase.\n46\nJebnoun et al.\nIn the following, we discuss the characteristics of clones in deep learning systems\nthat are associated with diﬀerent functionalities and development phases of deep\nlearning systems. The following phases are listed based on the clones occurrences\nratio from highest to lowest.\nTable 17 Percentages of Occurrence of Code Clones in DL Phases\ndl phase\ncategory\ndl phase\nsubcategory\nType 1\n%occs\nType 2\n% occs\nType 3\n% occs\n% occs\nin subcat\n% occs\nin total\nPreliminary\npreparation\nhardware requirements\n100\n0\n0\n100.0\n1.03\nData\ncollection\nload data\n20\n20\n40\n80.0\n5.15\nload label\n20\n0\n0\n20.0\nData\npostprocessing\ncompute output shape\n0\n0\n12.5\n12.5\n8.25\nobject localization\n25\n0\n12.5\n37.5\nprocess output\n25\n0\n12.5\n37.5\nset shape of output data\n12.5\n0\n0\n12.5\nData\npreprocessing\napply data augmentation\n5.55\n0\n0\n5.55\n18.56\ndata normalization\n0\n0\n11.11\n11.11\nget batches of data\n0\n5.55\n0\n5.55\nget numerical feature columns\n5.55\n0\n5.55\n11.11\nparse arguments\n0\n0\n5.55\n5.55\nprepare tensor\n11.11\n0\n0\n11.11\nprocess input\n0\n0\n16.66\n16.66\nresize image\n5.55\n0\n0\n5.55\nset shape of input data\n0\n0\n11.11\n11.11\nset type of input data\n0\n0\n5.55\n5.55\nsetting format input data\n0\n0\n5.55\n5.55\nsplit data\n0\n0\n5.55\n5.55\nModel\nprediction\ninference\n100\n0\n0\n100.0\n2.06\nModel\nconstruction\nmodel component format verif.\n2.86\n0\n0\n2.86\n36.08\nactivation function call\n0\n0\n2.86\n2.86\nbuild model\n2.86\n0\n0\n2.86\nbuild one subnetwork\n0\n0\n2.86\n2.86\ncompute model outputs\n0\n0\n2.86\n2.86\ninit evaluation metrics\n0\n0\n2.86\n2.86\ninitialize model graph\n0\n0\n2.86\n2.86\ninitialize model output\n2.86\n0\n0\n2.86\nlayer construction\n0\n2.86\n2.86\n5.71\nmodel architecture instantiation\n0\n0\n5.71\n5.71\nmodel (hyper)parameters init\n14.29\n20\n28.75\n62.86\nModel\nevaluation\nperformance metric computation\n0\n22.22\n66.66\n88.89\n9.28\ntest data prediction\n0\n0\n11.11\n11.11\nModel\ntraining\ncompute loss\n27.77\n0\n0\n27.78\n18.56\nget pooling info\n0\n0\n5.56\n5.56\nmeasure model accuracy\n5.56\n0\n0\n5.56\nmodel training\n5.56\n0\n11.11\n16.67\none model step training\n11.11\n5.56\n11.11\n27.78\ntraining procedure\n0\n0\n5.56\n5.56\nweight normalization\n0\n0\n11.11\n11.11\nModel\ntuning\nMinibatch size\n0\n0\n100\n100\n1.03\nModel construction: Our manual analysis shows that the most frequent DL-\nphase category that co-exists with code clones is the model construction phase with\n36.08% of DL-related code clones. This is an indication that DL practitioners du-\nplicate code frequently when building the model and speciﬁcally when initializing\nClones in Deep Learning Code: What, Where, and Why?\n47\nhyperparameters/parameters with 62.86% of code clones classes being related to\nthe DL-phase subcategory of the model construction. Table 17 shows that the ma-\njority of clones created by developers when initializing (hyper)parameters during\nthe construction of the model are Type 3 clones (they represent 28.75% overall).\nThe data preprocessing and model training phases contained 18.56% of all the clones\nthat we manually analyzed.\nModel training: Computing loss and training in each step of the model are the\nmost frequent activities performed during model training. These activities are as-\nsociated with 27.78% of clones from the model training subcategory. According to\nour manual analysis, computing loss functions are often copied/pasted from other\nfunctions located in the same location as the model implementation, or written\nfrom scratch. i.e., by calling DL libraries routines to perform loss computation.\nSome developers may also reuse the corresponding code from the online sources.\nThe duplication of code for loss function is illustrated in the example in the table\n18. The ﬁrst line in the table corresponds to calculating the loss of RankingLoss.\nThe second line corresponds to computing the loss of Softmax. The two functions\nare Type 3 clones to each other. Ranking and Softmax are two types of loss func-\ntions in deep learning. In fact, the loss computation is often common between\ndeep learning models. Even when they are diﬀerent, some of them have similar\nimplementation logic. Hence, the prevalence of duplicated code that computes loss\nfunctions.\nTable 18 Example of Model Training (Compute Loss) Type 3 Clone\ndef compute(self, labels, logits, weights, reduction):\n\"\"\"Computes the reduced loss for tf.estimator (not tf.keras).\nNote that this function is not compatible with keras.\nArgs:\nlabels: A `Tensor` of the same shape as `logits` representing graded\nrelevance.\nlogits: A `Tensor` with shape [batch_size, list_size]. Each value is the\nranking score of the corresponding item.\nweights: A scalar, a `Tensor` with shape [batch_size, 1] for list-wise\nweights, or a `Tensor` with shape [batch_size, list_size] for item-wise\nweights.\nreduction: One of `tf.losses.Reduction` except `NONE`. Describes how to\nreduce training loss over batch.\nReturns:\nReduced loss for training and eval.\n\"\"\"\nlosses, loss_weights = self.compute_unreduced_loss(labels, logits)\nweights = tf.multiply(self.normalize_weights(labels, weights), loss_weights)\nreturn tf.compat.v1.losses.compute_weighted_loss(\nlosses, weights, reduction=reduction)\ndef compute(self, labels, logits, weights, reduction):\n\"\"\"See `_RankingLoss`.\"\"\"\nlabels, logits = self.precompute(labels, logits, weights)\nlosses, weights = self.compute_unreduced_loss(labels, logits)\nreturn tf.compat.v1.losses.compute_weighted_loss(\nlosses, weights, reduction=reduction)\n48\nJebnoun et al.\nData preprocessing Processing input is related to 16.66% of clones associ-\nated with the ‘data preprocessing’ phase of the DL development workﬂow. Input\nprocessing includes all the transformation needed to apply on the input data to\nprepare data for model training (e.g., processing input of model inception v3 by\nnormalizing each pixel of input image).\nModel evaluation: Each DL model is evaluated to improve its performance.\nOverall, 9.28% of the DL-related cloned code corresponds to model evaluation of\nwhich 89% of clones are related to performance metric computation and 11.11%\nto test data prediction. Measurement metrics used to evaluate the models tend\nto be duplicated frequently for each model and for each metric. One example\nof cloning metric computation code can be seen in Table 19 1, where we have\nthe implementation of two measures: Mean Reciprocal Rank and Mean Average\nPrecision. The two functions are clones of each other. The clone is of Type 3. The\ndiﬀerences are in the renaming and function calls that corresponds to each metric\ncomputation.\nData post-processing: Data is often post-processed after an inductive process\nand converted into a format recommended by the stakeholders of the model and to\nsatisfy the requirement of the application using the model. Our ﬁndings show that\n8.25% of cloned code are related to DL functions used in the data post-processing\nphase. There are various techniques to perform this phase. We found that object\nlocalization functions are duplicated functions with 37.5% from the total cloned\nfunctions to perform data post-processing phase. For example, in object detection\nthe object localization is used to interpret the output by assigning each object to\na class with a higher probability or by drawing bounding boxes on an image from\ninference results. 37.5% of cloned functions are classiﬁed as processing output and\nthe rest are found duplicated for computing shape of output data.\nData collection: Data collection operations are frequently cloned. 5.15% of\nour manually analyzed clones were related to data collection. Among them, we\nfound 80% of clones to be related to loading data either from ﬁles or from an URL\nor using a library to get data. The rest are derived from load class labels data\n(20%).\nModel prediction: 2.06% of our manual analysis code clones are related to\ninference. All of them are Type 1 clones. Subsequently, according to our manual\nanalysis, we can say that DL developers often duplicate the same code to create\nan inference process from a trained model.\nModel tuning: We found clones in the code used to tune diﬀerent model\nconﬁgurations (e.g., best batch size to train the model) or hyperparameters. All\nthe clones found to be related to this category were Type 3 clones. Meaning that\nDL developers often duplicate the hyperparameter tuning code of other models\nand apply some modiﬁcations to it (adding few extra lines), for example to adjust\nthe batch size. Clones in hyperparameter tuning code represents 1.03% of the\nanalyzed clones.\nPreliminary preparation: The code used to prepare the environment for\nmodel training appears to also contain clones. To optimise the model training\ntime, developers write code to manage the hardware, e.g., CPU and GPU man-\nagement. Among the manually analyzed clones, 1.03% of them belong to the en-\n1 https://github.com/tensorflow/ranking\nClones in Deep Learning Code: What, Where, and Why?\n49\nTable 19 Example of Model Evaluation (Compute Metrics) Type 3 Clone\ndef mean_reciprocal_rank(labels,\npredictions,\nweights=None,\ntopn=None,\nname=None):\n\"\"\"Computes mean reciprocal rank (MRR).\nArgs:\nlabels: A `Tensor` of the same shape as `predictions`. A value >= 1 means a\nrelevant example.\npredictions: A `Tensor` with shape [batch_size, list_size]. Each value is\nthe ranking score of the corresponding example.\nweights: A `Tensor` of the same shape of predictions or [batch_size, 1]. The\nformer case is per-example and the latter case is per-list.\ntopn: An integer cutoff specifying how many examples to consider for this\nmetric. If None, the whole list is considered.\nname: A string used as the name for this metric.\nReturns:\nA metric for the weighted mean reciprocal rank of the batch.\n\"\"\"\nmetric = metrics_impl.MRRMetric(name, topn)\nwith tf.compat.v1.name_scope(metric.name, 'mean_reciprocal_rank',\n(labels, predictions, weights)):\nmrr, per_list_weights = metric.compute(labels, predictions, weights)\nreturn tf.compat.v1.metrics.mean(mrr, per_list_weights)\ndef mean_average_precision(labels,\npredictions,\nweights=None,\ntopn=None,\nname=None):\n\"\"\"Computes mean average precision (MAP).\nArgs:\nlabels: A `Tensor` of the same shape as `predictions`. A value >= 1 means a\nrelevant example.\npredictions: A `Tensor` with shape [batch_size, list_size]. Each value is\nthe ranking score of the corresponding example.\nweights: A `Tensor` of the same shape of predictions or [batch_size, 1]. The\nformer case is per-example and the latter case is per-list.\ntopn: A cutoff for how many examples to consider for this metric.\nname: A string used as the name for this metric.\nReturns:\nA metric for the mean average precision.\n\"\"\"\nmetric = metrics_impl.MeanAveragePrecisionMetric(name, topn)\nwith tf.compat.v1.name_scope(metric.name, 'mean_average_precision',\n(labels, predictions, weights)):\nper_list_map, per_list_weights = metric.compute(labels, predictions,\nweights)\nreturn tf.compat.v1.metrics.mean(per_list_map, per_list_weights)\nvironment conﬁguration category. All these clones were Type 1 clones, suggesting\nthat developers often duplicate these conﬁguration codes without modiﬁcations.\nOur analysis show that code duplication is a common practice among DL de-\nvelopers. They duplicate code during almost all the phases of the development\nprocess of deep learning models, in addition to duplicating traditional methods\nlike test and logging. Since duplicating code may lead to bug propagation and\n50\nJebnoun et al.\ninconsistency in the program, we recommend that DL developers pay a close at-\ntention to these clones during the maintenance and evolution of their systems.\nSummary of ﬁndings (RQ4): According to our ﬁndings, code duplication\nis more prevalent during the model construction phase of deep learning devel-\nopment. Code related to the initialization of model hyperparameters are cloned\nmost frequently, followed by code related to model training and data preprocess-\ning.\n4.5 RQ5: In which phases of deep learning development code cloning is\nmore prone to faults?\nAfter applying our taxonomy to the selected code clones from the analyzed subset\nof six systems, it is of interest to identify deep learning activities during which\ncloning has the highest risk of bugs. Our results of RQ3 show that code cloning\ncan lead to bugs. A better understanding of the circumstances in which bugs\nfrequently occur on cloned code will help raise the awareness of the developers to\nunderstand and mitigate the risks associated with their code cloning activities.\nTo carry out this investigation, we consider the relation between bugs and code\nclones and determine which part of the DL code is more prone to bugs when it is\nduplicated. We consider a clone fragment as ’buggy’ when cloned lines intersect\nwith lines modiﬁed by bug-ﬁxing commit. We use the labelling of clones as in RQ4\nto group the clones regarding the DL phases the clones are related. We computed\nthe percentages of bugs related cloned functions for each DL phase. Our result\nshows that code cloned during the model construction phase are related to bugs\nin higher numbers; 50% of them are buggy as shown in Figure 28.\nTable 21 shows which DL-related cloned functions (tasks) are the most in-\nvolved with bugs. The corresponding DL phases are also provided in the Table.\nWe display only DL-related tasks and phases where clones are involved in bugs\n(i.e., other phases of the DL workﬂow were the phenomenon is not observed are\nomitted). As mentioned earlier, the model construction phase contains the highest\nproportion of buggy clones (i.e., 50% of all buggy clones in our manually analyzed\nclone data). The majority of them are related to model (hyper)parameters ini-\ntialization (46.66%). Table 20 shows an example of bug ﬁx in a buggy clone. The\npresented cloned code fragments are from a bug-ﬁx commit with the message ‘Mi-\nnor optimizer consistency ﬁxes’2. The optimizers in deep learning are capable of\nreducing the losses by changing the attributes of the neural network (i.e., learning\nrate). Those optimizers have a common implementation of the hyperparameter\ninitialization function. In the example of commit bug-ﬁx from Table 20, to ﬁx the\ninstantiation of the number of iteration by adding a data type, the deep learning\ndeveloper had to propagate the same change to several optimizer initializations. In\nthis example, we have seven optimizers, i.e, SGD, RMSProp, Adagrad, Adadelta,\nAdam, Adamax, and Nadam that share the same initialization implementation\nand the developer needed to propagate the ﬁxing change seven times.\nThe DL phase with the second highest proportion of buggy clones is ‘Model\ntraining’ with a proportion of 20%. The phase with the third highest proportion\n2 https://github.com/keras-team/keras/commit/2d8739d\nClones in Deep Learning Code: What, Where, and Why?\n51\nModel construction\n50 %\nModel training\n 20 %\nData collection\n 13.3 %\nData preprocessing\n10 %\nHyperparameter tuning\n 3.3 %\nData postprocessing\n3.3 %\nFig. 28 Percentage of Bug-Fix Occurred with Cloned Functions with respect to Deep Learning\nPhases\nTable 20 Example of Bug Fix Commit Code Change\ndef __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\nsuper(Adagrad, self).__init__(**kwargs)\nwith K.name_scope(self.__class__.__name__):\nself.lr = K.variable(lr, name='lr')\nself.decay = K.variable(decay, name='decay')\nself.iterations = K.variable(0, name='iterations')\nif epsilon is None:\nepsilon = K.epsilon()\nself.epsilon = epsilon\nself.initial_decay = decay\ndef __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\nsuper(Adagrad, self).__init__(**kwargs)\nwith K.name_scope(self.__class__.__name__):\nself.lr = K.variable(lr, name='lr')\nself.decay = K.variable(decay, name='decay')\nself.iterations = K.variable(0, dtype='int64', name='iterations')\nif epsilon is None:\nepsilon = K.epsilon()\nself.epsilon = epsilon\nself.initial_decay = decay\nof buggy clones is the data collection phase with 13.3%. 10% of buggy clones\nare related to data pre-processing; the majority of them are in code related to\ntensor operations (66.66%) and code for setting the shape of input data. Only a\nsmall amount of the analyzed buggy clones were found to be related to data post-\nprocessing (3.3%) and tuning of the hyperparameters of the model (3.3%). In light\n52\nJebnoun et al.\nof these results, we recommend that DL developers pay particular attention when\nduplicating code during the model construction phase. Although it may seems\nlike a good idea to copy the code of an existing model, to speed up the model\nconstruction phase, there are perils to this practice.\nTable 21 Percentage of DL-related Cloned Functions with Bugs\nTaxonomy\nTask\n% occs in DL step\n% occs from total\nData collection\nLoad data\n100\n13.3\nData post-processing\nSet shape of output data\n100\n3.3\nData Pre-processing\nPrepare tensor\n66.66\n10\nSet shape of input data\n33.33\nHyperparameter Tuning\nHyperparameter tuning\n100\n3.3\nModel Construction\nModel component format veriﬁcation\n6.66\n50\nInitialize model graph\n6.66\nInitialize model output\n6.66\nLayer construction\n13.33\nModel architecture instantiation\n20\nModel (hyper)parameters initialization\n46.66\nModel training\nModel training\n33.33\n20\nOne model step training\n66.66\nSummary of ﬁndings (RQ5): Code clones that are related to model con-\nstruction are the most bug-prone among deep learning clones followed by the\nclones related to model training and data collection.\n5 Research Implications\nIn this section, we discuss the implications of our ﬁndings with regard to cloning\nactivity in the DL code.\nCode clones are prevalent in deep learning code: In light of the higher density of\ncode-clones identiﬁed in deep learning code, it appears that DL developers prefer\nto reuse existing solutions instead of creating new ones from scratch. As they need\nto experiment with diﬀerent conﬁgurations to ﬁnd the best DL model, duplicating\na code that works often seem a good idea to save time and eﬀort. Our results show\nthat developers often copy-paste exact code (frequently for loss computation) in\nthe same location as the calling statement. We assume that this proximity aims to\nease the maintenance of the resulting code. However, maintaining multiple clone\ncopies always increase the risk of failing to propagate changes consistently; leading\nto bugs. Our results show that clones are more prevalent in deep learning code in\ncomparison to traditional code. We attribute this phenomenon to the fact that a\nsame decision logic can be used several times in a deep learning model and also\nacross models. For example, when creating a convolutional neural network [52,53]\nmodel that consists of a set of layers. Each layer is initialized according to its type\nand its parameter values needed, and blocks of codes are stacked to create the\narchitecture. These blocks are exact or similar copies of each other. Hence, the\nprevalence of code clones in the code of this model.\nClones in Deep Learning Code: What, Where, and Why?\n53\nDeep learning code clones are dispersed: Considering the code clones being dis-\ntant as found in deep learning code, such dispersion of code clones is problematic\n[49] from a maintenance point of view. When changing code, it is most likely easier\nto change the code in the same ﬁle than in diﬀerent ﬁles or folders. Fragments of\nrelated code in distant locations may add navigation and comprehension overhead\nduring code change. Thus, the maintenance will be diﬃcult to handle. Due to the\nhigh percentages of distant code clones (same directory and diﬀerent directories),\ndeep learning practitioners should be aware of the potential negative impacts of\nsuch cloning practices. In addition, our analysis of the percentages of clone frag-\nments in diﬀerent location categories show that clones in deep learning code are\nmore dispersed in the code, which is also problematic. Because of the negative im-\npact of code clones on maintenance, developers should consider refactoring them.\nWe noticed some signs of refactoring in some of the studied deep learning systems.\nSpeciﬁcally, we observed the use of ﬁles with a name ending with ‘ utility’ that\ncontains all the useful functions and functions likely to be used in diﬀerent parts\nof the system [15,70,101].\nCode clones in DL code are related to bugs: Our results show that cloned code\nmay be more bug-prone than non-cloned code in deep learning systems. In addi-\ntion, Type 3 clones have a relatively higher odd to be involved in bugs in the deep\nlearning code than Type 2 and Type1 clones. Also, code clones that are related\nto model construction phase are the most bug-prone. In particular those related\nto model (hyper)parameters initialization. Since the main challenge of DL devel-\nopers is to provide a model with high accuracy, setting model (hyper)parameters\nis an important step to implement an eﬃcient model. Therefore bugs occurring in\nthe code responsible for this critical task is likely to have a severe impact on the\nquality of the deep learning system.\nDue to the data-driven nature of deep learning, data collection is a crucial task\n[67] and any bug occurring in the code responsible for this phase is also likely to\nsigniﬁcantly impact the quality of the deep learning system.\nOur ﬁndings regarding the prevalence and distribution of clones in deep learn-\ning code, their bug-proneness and insights on the characteristics and impacts of\nthe clones related to diﬀerent DL phases are thus important for deep learning prac-\ntitioners. These ﬁndings can help researchers to further investigate the character-\nistics and evolution of clones in deep learning code and also guide practitioners to\nadapt the best software development practices to the maintenance and evolution\nof the deep learning systems.\n6 Threats to Validity\nIn this section, we discuss the potential threats to the validity of our research\nmethodology and ﬁndings.\nIn terms of Internal validity, we manually labeled each detected code clones\nclass to its corresponding DL phase. Then, we also manually assigned them to\none of the steps of the DL code process. However, this relies on the subjective\njudgment of the persons who performed the manual classiﬁcation. This is a threat\nto the internal validity of our experiment. To mitigate this threat, the manual\nclassiﬁcation for creating the taxonomy was done by two authors having academic\n54\nJebnoun et al.\nand industry background. The results were then cross-validated, and disagreements\nwere resolved by group discussion. We believe this process decreased the chances\nof incorrect tagging. However, future research may further improve our approach\nand provide additional perspectives about our results by surveying deep learning\npractitioners.\nIn terms of construct validity threats, which concern the relationship between\ntheory and observation. We followed the approach proposed by Rosen et al. [79]\nto detect bug-ﬁx commits by employing a set of keywords that are bug ﬁxing\nrelated. If the commit message contains one of the keywords, it will be labeled\nas a bug-ﬁx commit. To reduce the imprecision in the bug-ﬁx commit detection\nprocess, we reviewed a sample of the labeled commits (102) and conﬁrmed that\nthey corresponded to bug-ﬁxes with high accuracy (100%).\nFor detecting clones, we used the NiCad clone detector [19]. Since diﬀerent\nsettings can have diﬀerent eﬀects, that we call a confounding conﬁguration problem\n[100], we have carefully set the parameters of NiCad by employing a standard\nconﬁguration [81] and with these settings, NiCad is reported to be very accurate\nin clone detection [81,82]. Thus, we believe that our ﬁndings on code clones in\ndeep learning code have relevance signiﬁcance. We also repeated our analysis for\nvarying dissimilarity thresholds (20% and 30%) for clone detection.\nWith regard to external validity, our analysis is primarily focused on deep\nlearning repositories that are written in Python with small number of Java and\nC# systems. As Python is the most popular programming language in the machine\nlearning ﬁeld, we can assume that the small data used brings a lot of knowledge.\nIn addition, we selected only six DL repositories to be manually analyzed for\nthe creation of the taxonomy. Therefore, this may threaten the generalizability of\nour results. We believe that our small scale but detail analysis of deep learning\nrepositories will provide a comprehensive overview of how and why deep learn-\ning practitioners had to duplicate code. We assume that, for each deep learning\nproject, we can have diﬀerent percentages of code clones occurrences alternating\nbetween the diﬀerent phases of deep learning workﬂow. The fact that they exist\nmay challenge the development of this type of system. Future studies should val-\nidate the generalizability of our ﬁndings with other DL systems that are written\nin other programming languages.\nIn terms of threats to reliability, we investigate in our study open-source deep\nlearning and traditional projects that are available on GitHub. And we provide a\nreplication package that contains needed data and scripts to replicate our study\n[39].\nAnd with respect to threats to conclusion validity, we use non-parametric\nstatistical tests to analyze the diﬀerence between distributions. Non-parametric\ntests are adequate because they make no assumption on the nature of the data\ndistribution.\n7 Related Work\nIn this section, we discuss relevant studies from the literature that tackle the\nchallenges faced by developers when building AI/ML/DL systems. We also review\nprevious studies that examine the quality assurance of deep learning systems. In\naddition, we examine the impact of code clones on traditional systems quality.\nClones in Deep Learning Code: What, Where, and Why?\n55\n7.1 Software Engineering for AI-based system\nThanks to the democratization of powerful open-source AI/ML/DL libraries/\nframeworks, complex prediction systems are built quickly. Due to this rapid release\nof this type of system, the software quality is often sacriﬁced. Thus, it becomes\nchallenging and expensive to maintain them over time because of technical debt.\nSculley et al. [89] discuss the challenges in designing ML systems and explain how\npoor engineering choices can be very expensive. The challenges discussed include:\nhidden feedback loops, data dependencies, conﬁguration debt, common ML code\nsmells, etc. Amershi et al [2] reported the best practices used by Microsoft soft-\nware engineers while developing projects that are related to Artiﬁcial Intelligence\nand Machine learning. They mainly focused on the diﬀerences between ML-based\nsoftware projects and traditional projects, and the challenges of adapting Agile\nprinciples to ML-based systems. Their study was conducted via interviews with\nselected Microsoft developers and a large-scale survey within the company. They\nreport that maintaining and versioning data is a crucial task for ML-based sys-\ntems. They also remark that data is harder to version than code. And that in\naddition to being a software engineer, ML skills are needed to build ML-based\nsystems. Furthermore, it is more challenging to handle distant modules in ML-\nbased systems.\nTesting and Monitoring: One of the important strategies to reduce technical\ndebt and lower long-term maintenance costs is testing and monitoring. ML-based\nsystems are more diﬃcult to test than traditional software systems [11]. This is a\nconsequence of the heavy dependence of ML on data and models. Breck et al. [13]\nhave outlined speciﬁc testing and monitoring needs based on practical experience\nat Google. They provide 28 actionable tests that can be used to measure the pro-\nduction readiness of a ML-based system and reduce technical debt. Breck et al.’s\nstudy focuses more on model quality rather than the infrastructure quality of ma-\nchine learning systems. Zhang et al. [104] provide a comprehensive survey of ML\ntesting covering 138 papers. The study of Zhang et al. [104] presents deﬁnitions\nand research status of many testing properties such as correctness, robustness,\nand fairness. In addition, they discuss the need to test the diﬀerent components\ninvolved in the ML model building (data, learning program, and framework). Since\nML testing remains at an early stage in its development, they present many chal-\nlenges. Among these challenges, they found challenges in test input generation,\ntest assessment criteria, the oracle problem, and testing cost reduction. Further-\nmore, Zhang et al. [104] analyze some research directions to beneﬁt ML developers\nand the research community. They suggest testing more application scenarios since\nmost of previous studies focus on image classiﬁcation. It will be worth investigat-\ning many other areas such as speech recognition or natural language processing.\nThey also mentioned uncovered testing opportunities like testing unsupervised and\nreinforcement learning systems.\nSoftware Engineering Practices and Challenges: Amershi et al. [2] per-\nformed a survey of Software Engineering practices for ML-based systems at Mi-\ncrosoft. They interviewed developers from Microsoft to understand their devel-\nopment practices and the beneﬁts of these practices. Another study related to\nsoftware engineering practices for DL applications was conduced by Zhang et al.\n[105]. Zhang et al. also surveyed DL practitioners about their software engineering\npractices. They proposed recommendations to improve the development process of\n56\nJebnoun et al.\nDL applications. Wan et al.[99] studied the features and impacts of machine learn-\ning on software development. They compare various aspects of software engineering\nand work characteristics in both the machine learning systems and non-machine\nlearning software systems. A recent study by Chen et al. [16] examined challenges\nin deploying DL software by analyzing Stack Overﬂow posts and posts from other\npopular Q&A website for developers. They proposed a taxonomy of the challenges\nfaced by developers when deploying DL software.\nBugs in Deep Learning Code: Islam et al. [37] analyzed stack overﬂow posts\nand bug-ﬁx commits from popular deep learning frameworks to understand the\ncharacteristics of DL systems’ bugs (their types, root causes, and eﬀects). Zhang\net al. [106] studied deep learning applications built on top of TensorFlow [76] by\ncollecting their program bugs from GitHub and Stack Overﬂow. They identiﬁed the\nroot causes and symptoms of the collected bugs. They also studied the detection\nand localization challenges of these bugs.\nCode Smells in Deep Learning Applications: Jebnoun et al. [40] studied the\nprevalence, evolution, and bug proneness of code smells in deep learning applica-\ntions. They identify three types of smells that occur more frequently: long lambda\nfunctions, long ternary conditional expression, and complex container comprehen-\nsion. They report that the number of code smells increases across releases. They\nfocused on the relationship between code smells and bugs reported the overlap of\nnearly 63% of the changed ﬁle with ﬁles that contain code smells, and that frequent\nsmells co-exist with software bugs more frequently than the others. Jiakun Liu et\nal [57] investigated technical debt in deep learning frameworks by mining the self-\nadmitted technical debt comments provided by developers. Among the types of\ndesign debt, Jiakun Liu et al [57] report that DL developers consider code du-\nplication to be a contributing factor to technical debt and increased maintenance\ncosts.\nComputational Notebooks: Nowadays, we are witnessing a proliferation of\ncomputational notebooks in data science studies, thanks to their strengths in pre-\nsenting data stories and their ﬂexibility. However, using these notebooks in a real\nproject may induce technical debt, because of their lack of abstraction, modular-\nisation, and automated tests. Recently, a fair amount of research works has been\nconducted on computational notebooks, mostly focusing on the challenges that\nthey pose to data scientists and the poor software engineering practices observed\nin these notebooks.\nOne common bad practice that is frequently observed in computational note-\nbooks is the copying and pasting of code, by data scientists in order to save time\nand eﬀort. Kery et al. [45] conducted two case studies where they interviewed 21\ndata scientists and surveyed 45 data scientists to understand the use of literate pro-\ngramming tools. They studied Jupyter Notebook as it is the most popular literate\ntool [91]. They [45] identiﬁed the good and bad practices employed by data sci-\nentists. One practical limit of Jupyter Notebook is its size and performance. The\nlimited size often leads data scientists to copy-paste code into a new Notebook\nwhen the maximum size is reached. In addition, they copy-paste code to ensure\nthat the code dependencies are properly located next to the new code, instead of\nextracting the code to a function. Pimentel et al. [71] conducted an empirical study\nof 1.4 million notebooks from GitHub; examining reproducibility issues, and chal-\nlenges related to the implementation of projects within Jupyter notebook. They\nalso provide a set of best practices to improve reproducibility. Additionally, they\nClones in Deep Learning Code: What, Where, and Why?\n57\nidentiﬁed and reported good and bad practices followed by developers of compu-\ntational notebooks. One best practice that is reported is the use of markdown\nand visualization, which are two key features of literate notebooks. The use of\na convenient and comprehensive ﬁlename is also reported to be a frequent good\npractice in computational notebooks. The bad practices identiﬁed include the lack\nof testing code, as well as poor programming practices that make reasoning and\nreproducing results more diﬃcult, such as non-executed code cells and hidden\nstates. Psallidas et al. [72] also examined the quality of notebooks (through an\nanalysis of 6 million python notebooks from GitHub and 2 million enterprise DS\npipelines developed within COMPANYX). They also performed an analysis of 12\npopular deep learning libraries over 900 releases. They report that the majority of\nnotebooks use only a few libraries and that commonly used tools are mature and\npopular. Koenzen et al. [48] examined how code is cloned in Jupyter notebooks\nand found that 7.6% of code clones are self-duplication. They also performed an\nobservational lab study and found that frequently reuse code is copied from web\ntutorial sites, API documentation, and Stack Overﬂow.\n7.2 Impacts of Code Clones\nSince we investigate code clones in the deep learning code, we are interested in\nreviewing the existing literature on the impact of code clones in traditional software\nsystems.\nRoy and Cordy [83] report that code clones represents between 7%-23% of\nthe code of traditional software systems. Multiple studies from the literature have\nexamined the impacts of clones on traditional software systems from diﬀerent\nsoftware quality perspectives, e.g., change-proneness, bug-proneness, challenges in\nconsistent update, and overall maintenance eﬀorts and costs.\nSajnani et al. [87] showed that, contrary to intuition, the cloned code contains\nless problematic patterns than non-cloned code. Along the same line, Rahman et al.\n[73] reports no correlation between bug-proneness and code clones. However, these\nconclusions about the lack of harmfulness of clones are contradicted by Islam et al.\n[35] who found that code cloning activities contribute to replicating bugs. Islam\net al. suggest to prioritize refactoring and tracking for clone fragments containing\nmethod calls and/or if-conditions, to prevent bugs being replicated. Another study\nby Islam et al. [36] examined bugs that were reported during the evolution of a\nsoftware system for two diﬀerent programming languages (Java and C) and found\nthat clone code tends to be more bug-prone than non-clone code.\nAversano et al. [4] investigated how clones are maintained considering the\ninconsistency that may induce code clones when ﬁxing a bug in just one fragment\nor when evolving code fragments. They found that the majority of clone classes\nare always maintained consistently. Similar work was also performed by G¨ode et\nal. [24], conﬁrming the ﬁndings of Aversano et al. G¨ode et al. also found cloned\ncode to be even more stable than non cloned code. They also report that near-\nmiss clones (Type 2 and Type 3) are more stable than exact clones (Type 1).\nKrinke [50] conducted a comparative study (in terms of the average age) between\ncloned code and non-cloned code. They observed that cloned code is usually older\nthan non-cloned code and that cloned code in a ﬁle is usually older than the non-\ncloned code in the same ﬁle. This conﬁrms previous observations that code clones\n58\nJebnoun et al.\nare more stable than non-cloned code. Therefore, maintaining code clones is not\nnecessarily more expensive than maintaining a non-cloned code.\nJiang et al. [41] examined the bug-proneness of cloned code and conﬁrmed that\ncode cloning can be error-prone and directly related to inconsistencies in the code.\nThey proposed an algorithm able to locate clone related bugs by detecting such\ninconsistencies. When a code fragment contains bugs and is reused by duplicating\nit with some adjustments w.r.t to the need, it may increase the spread of bugs\nin the system. Several other previous studies from the literature [6,7,42,56,54,75,\n98] report a similar conclusion about code clones, i.e., that they make the code\nbug-prone and increase maintenance costs. Juergens et al. [42] examined the root\ncause of faults in cloned code and report that one of the major sources of faults is\ninconsistent code clones. They provided an open-source algorithm for the detection\nof inconsistent clones. G¨ode and Koschke. [25] provide empirical evidences showing\nthat unintentional inconsistencies of code clones leads to faults.\nBarbour et al. [6] examined late propagation evolutionary patterns of clones\nand identiﬁed 8 types of late propagation. They further examined the risk of faults\nin these evolutionary patterns and found that late propagation in which a clone\nis modiﬁed and then re-synchronized without any modiﬁcation to the other clone\nin the clone pair is the riskiest pattern. Researchers have also examined the main-\ntenance eﬀorts that result from duplicating code. Hotta et al. [34] conducted an\nempirical study on 15 open-source systems, comparing the modiﬁcation frequency\nof code clones and non-clones code. They concluded that the existence of clones\ndoes not impact software evolution negatively. Kapser and Godfrey [44] performed\nan empirical study of code cloning patterns, reporting the reasons behind the dif-\nferent patterns. They also report that the majority of clones have a positive impact\non software maintainability. However, their claim is contradicted by Kim et al. [46]\nwho suggest that refactoring techniques cannot tackle consistently changing code\nclones. Li et al [56] advise that maintaining duplicate code would be very beneﬁcial\nfor developers, as this would avoid introducing hard to detect bugs. Lozano and\nWermelinger [59] have also shown the negative impacts of code clones in terms of\nmaintenance cost and system stability. They found that code clones have a higher\ndensity of changes than non-cloned code. Lozano and Wermelinger [58] also show\nthat the existence of code clones may increase the change eﬀort.\nA recent study by Mondal et al. [62] shows that cloned code are more unstable\nthan non-cloned code in general. However according to Selim et al. [90], the bug-\nproneness of code clones might be system dependent. Mondal et al. empirical study\n[66] shows that cloned code tends to require more eﬀort in maintenance than non-\ncloned code and that Type 2 and Type 3 clones often need a special attention\nwhen making management decisions since they require more eﬀort.\nWhile previous works examined the prevalence and impacts of code clones\nin traditional software systems, we investigate the distribution and impacts (bug-\nproneness) of code clones in the deep learning code. We manually investigate clones\nin deep learning systems with the aim to derive insights on ’what’ functions deep\nlearning practitioners clone and ’why’ they clone. Previous studies on duplicated\ncodes in data scientists’ projects have almost exclusively focused on analyzing\ncomputational notebooks. A number of studies’ results have shown that copying\nand pasting of cells within the same notebook is a widespread practice. There\nare some works [45,48] discussing the common code duplication practices of deep\nlearning practitioners. However, these works were limited to interviews with prac-\nClones in Deep Learning Code: What, Where, and Why?\n59\ntitioners and focuses only on computational notebooks. In this paper, we examine\nthe distribution of code clones deep learning systems, in terms of occurrences and\nlocation, and propose a taxonomy of code clones in deep learning systems. We also\nstudy the relationship between code clones and bug ﬁxes, and examine the model\nconstruction phases in which cloning has the highest risk of bugs.\n8 Conclusion\nThis paper presents an empirical study of code clones in deep learning systems.\nThrough quantitative and qualitative analyses, we have examined the characteris-\ntics, distribution, and impacts of clones in deep learning code. We have shown that\ncode clones are prevalent and dispersed in deep learning systems (which may add\nnavigation and comprehension overhead). In addition, our results show a higher\nassociation between code clones and bug occurrences. Furthermore, cloning code\nresponsible for model (hyper)parameters initialisation appeared to be a very risky\nactivity, since a large proportion of clones in this part of the code were found to\nbe buggy. Although duplicating code may lead to short term productivity gains,\ndeep learning practitioners should be aware of the perils of such practice.\nAs future work, we need further studies on the evolution patterns and the\nimpacts of clones on diﬀerent aspects of the quality of deep learning code, to\nguide the practitioners to better manage clones in deep learning systems. Thus,\nwe plan our future research towards the investigation of the clone genealogy in deep\nlearning code, to have deeper insights into how clones in deep learning code evolve,\nwhich in turn can help practitioners adopt safer code reuse practices, leverage\nexisting libraries and open-source resources in the rapidly growing domain of deep\nlearning and other machine learning based system development.\nAcknowledgements This work is supported by Fonds de Recherche du Quebec (FRQ) and\nthe Natural Sciences and Engineering Research Council of Canada (NSERC). We would like\nto thank Dr. Amin Nikanjam for his valuable comments on the manuscript.\nReferences\n1. Al Dallal, J., Abdin, A.: Empirical evaluation of the impact of object-oriented code\nrefactoring on quality attributes: A systematic literature review. IEEE Transactions on\nSoftware Engineering 44(1), 44–69 (2017)\n2. Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan, N., Nushi,\nB., Zimmermann, T.: Software engineering for machine learning: A case study. In: 2019\nIEEE/ACM 41st International Conference on Software Engineering: Software Engineer-\ning in Practice (ICSE-SEIP), pp. 291–300. IEEE (2019)\n3. Anwar, H., Pfahl, D., Srirama, S.N.: Evaluating the impact of code smell refactoring on\nthe energy consumption of android applications. In: 2019 45th Euromicro Conference\non Software Engineering and Advanced Applications (SEAA), pp. 82–86 (2019). DOI\n10.1109/SEAA.2019.00021\n4. Aversano, L., Cerulo, L., Di Penta, M.: How clones are maintained: An empirical study.\nIn: 11th European Conference on Software Maintenance and Reengineering (CSMR’07),\npp. 81–90. IEEE (2007)\n5. Barbour, L., An, L., Khomh, F., Zou, Y., Wang, S.: An investigation of the fault-proneness\nof clone evolutionary patterns. Software Quality Journal 26(4), 1187–1222 (2018)\n60\nJebnoun et al.\n6. Barbour, L., Khomh, F., Zou, Y.: Late propagation in software clones. In: 2011 27th\nIEEE International Conference on Software Maintenance (ICSM), pp. 273–282. IEEE\n(2011)\n7. Barbour, L., Khomh, F., Zou, Y.: An empirical study of faults in late propagation clone\ngenealogies. Journal of Soft. Evol. and Proc 25, 1139–1165 (2013)\n8. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. The Journal\nof Machine Learning Research 13(1), 281–305 (2012)\n9. Bordes, A., Chopra, S., Weston, J.: Question answering with subgraph embeddings. arXiv\npreprint arXiv:1406.3676 (2014)\n10. Braiek, H.B., Khomh, F.: Deepevolution: A search-based testing approach for deep neural\nnetworks. 2019 IEEE International Conference on Software Maintenance and Evolution\n(ICSME) pp. 454–458 (2019)\n11. Braiek, H.B., Khomh, F.: On testing machine learning programs. Journal of Systems and\nSoftware 164, 110542 (2020)\n12. Braiek, H.B., Khomh, F., Adams, B.: The open-closed principle of modern machine learn-\ning frameworks. In: 2018 IEEE/ACM 15th International Conference on Mining Software\nRepositories (MSR), pp. 353–363. IEEE (2018)\n13. Breck, E., Cai, S., Nielsen, E., Salib, M., Sculley, D.: The ml test score: A rubric for ml\nproduction readiness and technical debt reduction. In: 2017 IEEE International Confer-\nence on Big Data (Big Data), pp. 1123–1132. IEEE (2017)\n14. Buckley, F.J., Poston, R.: Software quality assurance. IEEE Transactions on Software\nEngineering 10(1), 36–41 (1984)\n15. Chen, B.: Berrynet deep learning gateway on raspberry pi and other edge devices. https:\n//github.com/DT42/BerryNet (2019)\n16. Chen, Z., Cao, Y., Liu, Y., Wang, H., Xie, T., Liu, X.: Understanding challenges in deploy-\ning deep learning based software: An empirical study. arXiv preprint arXiv:2005.00760\n(2020)\n17. Chen, Z., Chen, L., Ma, W., Zhou, X., Zhou, Y., Xu, B.: Understanding metric-based\ndetectable smells in python software: A comparative study. Information and Software\nTechnology 94, 14–29 (2018)\n18. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P.: Nat-\nural language processing (almost) from scratch. Journal of machine learning research\n12(ARTICLE), 2493–2537 (2011)\n19. Cordy, J.R., Roy, C.K.: The nicad clone detector.\nIn: 2011 IEEE 19th International\nConference on Program Comprehension, pp. 219–220. IEEE (2011)\n20. Cordy, J.R., Roy, C.K.: NiCad clone detector. https://www.txl.ca/txl-nicaddownload.\nhtml (2019). [Online; accessed 20-February-2020]\n21. Ernst, N.: Cliﬀ’s delta implementation.\nhttps://github.com/neilernst/cliffsDelta\n(2019)\n22. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features for scene\nlabeling. IEEE transactions on pattern analysis and machine intelligence 35(8), 1915–\n1929 (2012)\n23. Fowler, M., Beck, K., Brant, J., Opdyke, W., Roberts, D.: Refactoring: Improving the\ndesign of existing code addison-wesley professional. Berkeley, CA, USA (1999)\n24. Gode, N., Harder, J.: Clone stability. In: 2011 15th European Conference on Software\nMaintenance and Reengineering, pp. 65–74. IEEE (2011)\n25. G¨ode, N., Koschke, R.: Frequency and risks of changes to clones. In: Proceedings of the\n33rd International Conference on Software Engineering, pp. 311–320 (2011)\n26. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning.\nMIT Press (2016).\nhttp:\n//www.deeplearningbook.org\n27. Gottschalk, M., Joseﬁok, M., Jelschen, J., Winter, A.: Removing energy code smells with\nreengineering services. INFORMATIK 2012 (2012)\n28. Gupta, R., Pal, S., Kanade, A., Shevade, S.: Deepﬁx: Fixing common c language errors\nby deep learning.\nIn: Proceedings of the Thirty-First AAAI Conference on Artiﬁcial\nIntelligence, pp. 1345–1351 (2017)\n29. Hadhemii: DLCodeSmells. https://github.com/Hadhemii/DLCodeSmells/blob/master/\ndata/dlRepos.csv (2019)\n30. Hamdan, S., Alramouni, S.: A quality framework for software continuous integration.\nProcedia Manufacturing 3, 2019–2025 (2015)\n31. Han, J., Shihab, E., Wan, Z., Deng, S., Xia, X.: What do programmers discuss about\ndeep learning frameworks. EMPIRICAL SOFTWARE ENGINEERING (2020)\nClones in Deep Learning Code: What, Where, and Why?\n61\n32. Heaton, J.B., Polson, N.G., Witte, J.H.: Deep learning for ﬁnance: deep portfolios. Ap-\nplied Stochastic Models in Business and Industry 33(1), 3–12 (2017)\n33. Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.r., Jaitly, N., Senior, A., Van-\nhoucke, V., Nguyen, P., Sainath, T.N., et al.: Deep neural networks for acoustic modeling\nin speech recognition: The shared views of four research groups. IEEE Signal processing\nmagazine 29(6), 82–97 (2012)\n34. Hotta, K., Sano, Y., Higo, Y., Kusumoto, S.: Is duplicate code more frequently modiﬁed\nthan non-duplicate code in software evolution? an empirical study on open source soft-\nware. In: Proceedings of the Joint ERCIM Workshop on Software Evolution (EVOL) and\nInternational Workshop on Principles of Software Evolution (IWPSE), pp. 73–82 (2010)\n35. Islam, J.F., Mondal, M., Roy, C.K.: Bug replication in code clones: An empirical study. In:\n2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengi-\nneering (SANER), vol. 1, pp. 68–78. IEEE (2016)\n36. Islam, J.F., Mondal, M., Roy, C.K., Schneider, K.A.: A comparative study of software\nbugs in clone and non-clone code. In: SEKE, pp. 436–443 (2017)\n37. Islam, M.J., Nguyen, G., Pan, R., Rajan, H.: A comprehensive study on deep learning\nbug characteristics. arXiv preprint arXiv:1906.01388 (2019)\n38. JEBNOUN, H.: 6dlreposdata.\nhttps://github.com/Hadhemii/ClonesInDLCode/blob/\nmaster/data/6DLReposData.csv (2020)\n39. Jebnoun, H.: Clones in deep learning code (2020). URL https://github.com/Hadhemii/\nClonesInDLCode\n40. Jebnoun, H., Ben Braiek, H., Rahman, M.M., Khomh, F.: The scent of deep learning\ncode: An empirical study. In: Proceedings of the 17th International Conference on Mining\nSoftware Repositories, pp. 1—-11 (2020)\n41. Jiang, L., Su, Z., Chiu, E.: Context-based detection of clone-related bugs. In: Proceedings\nof the the 6th joint meeting of the European software engineering conference and the ACM\nSIGSOFT symposium on The foundations of software engineering, pp. 55–64 (2007)\n42. Juergens, E., Deissenboeck, F., Hummel, B., Wagner, S.: Do code clones matter? In: 2009\nIEEE 31st International Conference on Software Engineering, pp. 485–495. IEEE (2009)\n43. Kapser, C., Godfrey, M.W.: Toward a taxonomy of clones in source code: A case study.\nEvolution of large scale industrial software architectures 16, 107–113 (2003)\n44. Kapser, C.J., Godfrey, M.W.: “cloning considered harmful” considered harmful: patterns\nof cloning in software. Empirical Software Engineering 13(6), 645 (2008)\n45. Kery, M.B., Radensky, M., Arya, M., John, B.E., Myers, B.A.: The story in the notebook:\nExploratory data science using a literate programming tool. In: Proceedings of the 2018\nCHI Conference on Human Factors in Computing Systems, pp. 1–11 (2018)\n46. Kim, M., Sazawal, V., Notkin, D., Murphy, G.: An empirical study of code clone ge-\nnealogies. In: Proceedings of the 10th European software engineering conference held\njointly with 13th ACM SIGSOFT international symposium on Foundations of software\nengineering, pp. 187–196 (2005)\n47. Kim, S., Whitehead Jr, E.J.: How long did it take to ﬁx bugs? In: Proceedings of the\n2006 international workshop on Mining software repositories, pp. 173–174 (2006)\n48. Koenzen, A., Ernst, N., Storey, M.A.: Code duplication and reuse in jupyter notebooks.\narXiv preprint arXiv:2005.13709 (2020)\n49. Koschke, R.: Survey of research on software clones. In: Dagstuhl Seminar Proceedings.\nSchloss Dagstuhl-Leibniz-Zentrum f¨ur Informatik (2007)\n50. Krinke, J.: Is cloned code older than non-cloned code? In: Proceedings of the 5th Inter-\nnational Workshop on Software Clones, pp. 28–33 (2011)\n51. Kumlander, D.: Towards a new paradigm of software development: an ambassador driven\nprocess in distributed software companies. In: Advanced Techniques in Computing Sci-\nences and Software Engineering, pp. 487–490. Springer (2010)\n52. LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E.,\nJackel, L.D.: Handwritten digit recognition with a back-propagation network. In: Ad-\nvances in neural information processing systems, pp. 396–404 (1990)\n53. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n54. Li, J., Ernst, M.D.: CBCD: Cloned buggy code detector. In: Proc. ICSE, pp. 310–320\n(2012)\n55. Li, X., Jiang, H., Ren, Z., Li, G., Zhang, J.: Deep learning in software engineering. arXiv\npreprint arXiv:1805.04825 (2018)\n62\nJebnoun et al.\n56. Li, Z., Lu, S., Myagmar, S., Zhou, Y.: CP-Miner: Finding copy-paste and related bugs in\nlarge-scale software code. IEEE TSE 32, 176–192 (2006)\n57. Liu, J., Huang, Q., Xia, X., Shihab, E., Lo, D., Li, S.: Is using deep learning frameworks\nfree? characterizing technical debt in deep learning frameworks. In: Proceedings of the\nACM/IEEE 42nd International Conference on Software Engineering: Software Engineer-\ning in Society, pp. 1–10 (2020)\n58. Lozano, A., Wermelinger, M.: Assessing the eﬀect of clones on changeability. In: Proc.\nICSM, pp. 227–236 (2008)\n59. Lozano, A., Wermelinger, M.: Tracking clones’ imprint. In: Proceedings of the 4th Inter-\nnational Workshop on Software Clones, pp. 65–72 (2010)\n60. Macbeth, G., Razumiejczyk, E., Ledesma, R.D.: Cliﬀ’s delta calculator: A non-parametric\neﬀect size program for two groups of observations. Universitas Psychologica 10(2), 545–\n555 (2011)\n61. Miotto, R., Wang, F., Wang, S., Jiang, X., Dudley, J.T.: Deep learning for healthcare:\nreview, opportunities and challenges. Brieﬁngs in bioinformatics 19(6), 1236–1246 (2018)\n62. Mondal, M., Rahman, M.S., Roy, C.K., Schneider, K.A.: Is cloned code really stable?\nEmpirical Softw. Engg. 23(2), 693–770 (2018)\n63. Mondal, M., Roy, B., Roy, C.K., Schneider, K.A.: Investigating context adaptation bugs\nin code clones. In: 2019 IEEE International Conference on Software Maintenance and\nEvolution (ICSME), pp. 157–168 (2019)\n64. Mondal, M., Roy, C.K., Rahman, M.S., Saha, R.K., Krinke, J., Schneider, K.A.: Com-\nparative stability of cloned and non-cloned code: An empirical study. In: Proc. ACM\nSAC, pp. 1227–1234 (2012)\n65. Mondal, M., Roy, C.K., Schneider, K.A.: A comparative study on the bug-proneness\nof diﬀerent types of code clones. In: 2015 IEEE International Conference on Software\nMaintenance and Evolution (ICSME), pp. 91–100 (2015)\n66. Mondal, M., Roy, C.K., Schneider, K.A.: Does cloned code increase maintenance eﬀort?\nIn: 2017 IEEE 11th International Workshop on Software Clones (IWSC), pp. 1–7. IEEE\n(2017)\n67. Munappy, A., Bosch, J., Olsson, H.H., Arpteg, A., Brinne, B.: Data management chal-\nlenges for deep learning. In: 2019 45th Euromicro Conference on Software Engineering\nand Advanced Applications (SEAA), pp. 140–147. IEEE (2019)\n68. Neuh¨auser, M.: Wilcoxon–Mann–Whitney Test, pp. 1656–1658.\nSpringer Berlin Hei-\ndelberg, Berlin, Heidelberg (2011). DOI 10.1007/978-3-642-04898-2 615. URL https:\n//doi.org/10.1007/978-3-642-04898-2_615\n69. Nguyen, H., Kieu, L.M., Wen, T., Cai, C.: Deep learning methods in transportation\ndomain: a review. IET Intelligent Transport Systems 12(9), 998–1004 (2018)\n70. Pasumarthi, R.K., Bruch, S., Wang, X., Li, C., Bendersky, M., Najork, M., Pfeifer, J.,\nGolbandi, N., Anil, R., Wolf, S.: Tensorﬂow ranking. https://github.com/tensorflow/\nranking (2019)\n71. Pimentel, J.F., Murta, L., Braganholo, V., Freire, J.: A large-scale study about qual-\nity and reproducibility of jupyter notebooks. In: 2019 IEEE/ACM 16th International\nConference on Mining Software Repositories (MSR), pp. 507–517. IEEE (2019)\n72. Psallidas, F., Zhu, Y., Karlas, B., Interlandi, M., Floratou, A., Karanasos, K., Wu, W.,\nZhang, C., Krishnan, S., Curino, C., et al.: Data science through the looking glass and\nwhat we found there. arXiv preprint arXiv:1912.09536 (2019)\n73. Rahman, F., Bird, C., Devanbu, P.: Clones: What is that smell?\nEmpirical Software\nEngineering 17(4-5), 503–530 (2012)\n74. Rahman, M.S., Roy, C.K.: A change-type based empirical study on the stability of cloned\ncode. In: Proc. SCAM, pp. 31–40 (2014)\n75. Rahman, M.S., Roy, C.K.: On the relationships between stability and bug-proneness of\ncode clones: An empirical study. In: Proc. SCAM, pp. 131–140 (2017)\n76. Rampasek, L., Goldenberg, A.: Tensorﬂow: Biology’s gateway to deep learning?\nCell\nsystems 2(1), 12–14 (2016)\n77. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed, real-time\nobject detection. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 779–788 (2016)\n78. Rochimah, S., Ariﬁani, S., Insanittaqwa, V.F.: Non-source code refactoring: a systematic\nliterature review.\nInternational Journal of Software Engineering and Its Applications\n9(6), 197–214 (2015)\nClones in Deep Learning Code: What, Where, and Why?\n63\n79. Rosen, C., Grawi, B., Shihab, E.: Commit guru: analytics and risk prediction of software\ncommits. In: Proceedings of the 2015 10th Joint Meeting on Foundations of Software\nEngineering, pp. 966–969. ACM (2015)\n80. Roy, C.K., Cordy, J.R.: A survey on software clone detection research. Queen’s School\nof Computing TR 541(115), 64–68 (2007)\n81. Roy, C.K., Cordy, J.R.: Nicad: Accurate detection of near-miss intentional clones us-\ning ﬂexible pretty-printing and code normalization. In: 2008 16th iEEE international\nconference on program comprehension, pp. 172–181. IEEE (2008)\n82. Roy, C.K., Cordy, J.R.: A mutation/injection-based automatic framework for evaluat-\ning code clone detection tools. In: 2009 International Conference on Software Testing,\nVeriﬁcation, and Validation Workshops, pp. 157–166. IEEE (2009)\n83. Roy, C.K., Cordy, J.R.: Near-miss function clones in open source software: an empirical\nstudy. Journal of Software Maintenance and Evolution: Research and Practice 22(3),\n165–189 (2010)\n84. Roy, C.K., Zibran, M.F., Koschke, R.: The vision of software clone management: Past,\npresent, and future (keynote paper). In: proc. CSMR-WCRE, pp. 18–33 (2014)\n85. Sainath, T.N., Mohamed, A.r., Kingsbury, B., Ramabhadran, B.: Deep convolutional\nneural networks for lvcsr. In: 2013 IEEE international conference on acoustics, speech\nand signal processing, pp. 8614–8618. IEEE (2013)\n86. Saini, V., Sajnani, H., Lopes, C.: Cloned and non-cloned java methods: A comparative\nstudy. Empirical Softw. Engg. 23(4), 2232–2278 (2018). DOI 10.1007/s10664-017-9572-7\n87. Sajnani, H., Saini, V., Lopes, C.V.: A comparative study of bug patterns in java cloned\nand non-cloned code. In: 2014 IEEE 14th International Working Conference on Source\nCode Analysis and Manipulation, pp. 21–30. IEEE (2014)\n88. Samek, W., Wiegand, T., M¨uller, K.R.: Explainable artiﬁcial intelligence: Understanding,\nvisualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296 (2017)\n89. Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D., Chaud-\nhary, V., Young, M., Crespo, J.F., Dennison, D.: Hidden technical debt in ma-\nchine learning systems.\nIn: C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama,\nR.\nGarnett\n(eds.)\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n28,\npp.\n2503–2511. Curran Associates, Inc. (2015).\nURL http://papers.nips.cc/paper/\n5656-hidden-technical-debt-in-machine-learning-systems.pdf\n90. Selim, G.M., Barbour, L., Shang, W., Adams, B., Hassan, A.E., Zou, Y.: Studying the\nimpact of clones on software defects.\nIn: 2010 17th Working Conference on Reverse\nEngineering, pp. 13–21. IEEE (2010)\n91. Shen, H.: Interactive notebooks: Sharing the code. Nature 515(7525), 151–152 (2014)\n92. Spadini, D., Aniche, M., Bacchelli, A.: Pydriller: Python framework for mining software\nrepositories. In: Proceedings of the 2018 26th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering, p.\n908–911. Association for Computing Machinery (2018)\n93. Suryanarayana, G., Samarthyam, G., Sharma, T.: Refactoring for software design smells:\nmanaging technical debt. Morgan Kaufmann (2014)\n94. Svajlenko, J., Roy, C.K.: Evaluating modern clone detection tools. In: 2014 IEEE Inter-\nnational Conference on Software Maintenance and Evolution, pp. 321–330. IEEE (2014)\n95. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the inception\narchitecture for computer vision. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2818–2826 (2016)\n96. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional network\nand a graphical model for human pose estimation. In: Advances in neural information\nprocessing systems, pp. 1799–1807 (2014)\n97. Vetro, A., Ardito, L., Morisio, M.: Deﬁnition, implementation and validation of energy\ncode smells: an exploratory study on an embedded system. Energy (2013)\n98. Wagner, S., Abdulkhaleq, A., Kaya, K., Paar, A.: On the relationship of inconsistent\nsoftware clones and faults: An empirical study. In: Proc. SANER, pp. 79–89 (2016)\n99. Wan, Z., Xia, X., Lo, D., Murphy, G.C.: How does machine learning change software\ndevelopment practices? IEEE Transactions on Software Engineering (2019)\n100. Wang, T., Harman, M., Jia, Y., Krinke, J.: Searching for better conﬁgurations: a rig-\norous approach to clone evaluation. In: Proceedings of the 2013 9th Joint Meeting on\nFoundations of Software Engineering, pp. 455–465 (2013)\n101. Weill, C., Gonzalvo, J., Kuznetsov, V., Yang, S., Yak, S., Mazzawi, H., Hotaj, E., Jer-\nfel, G., Macko, V., Adlam, B., Mohri, M., Cortes, C.: Adanet.\nhttps://github.com/\ntensorflow/adanet (2019)\n64\nJebnoun et al.\n102. Wheeler, D.A.: SLOCCount.\nhttps://dwheeler.com/sloccount/ (2004).\n[Online; ac-\ncessed 19-May-2020]\n103. White, M., Tufano, M., Vendome, C., Poshyvanyk, D.: Deep learning code fragments for\ncode clone detection. In: 2016 31st IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE), pp. 87–98. IEEE (2016)\n104. Zhang, J.M., Harman, M., Ma, L., Liu, Y.: Machine learning testing: Survey, landscapes\nand horizons. IEEE Transactions on Software Engineering (2020)\n105. Zhang, X., Yang, Y., Feng, Y., Chen, Z.: Software engineering practice in the development\nof deep learning applications. arXiv preprint arXiv:1910.03156 (2019)\n106. Zhang, Y., Chen, Y., Cheung, S.C., Xiong, Y., Zhang, L.: An empirical study on tensor-\nﬂow program bugs. In: Proceedings of the 27th ACM SIGSOFT International Symposium\non Software Testing and Analysis, pp. 129–140. ACM (2018)\nAppendix A\nStudy Design\nTable 22 shows the name, url, number of lines of code (SLOC), number of commits\nand the size of each selected 6 DL repository.\nTable 22 The 6 analyzed DL repositories details\nRepository\nURL\nSLOC\n#commits\nSize\nkeras-applications\nhttps://github.com/keras-team/keras-applications.git\n4263\n89\nsmall\nDeepCTR\nhttps://github.com/shenweichen/DeepCTR.git\n4886\n91\nsmall\nnn-wtf\nhttps://github.com/lene/nn-wtf.git\n2688\n119\nsmall\nranking\nhttps://github.com/tensorﬂow/ranking.git\n10778\n145\nmedium\nBerryNet\nhttps://github.com/DT42/BerryNet.git\n12963\n396\nmedium\nadanet\nhttps://github.com/tensorﬂow/adanet.git\n25165\n432\nlarge\nAppendix B\nRQ1 Additional Results\nB.1\nResults of Clone Detection Using Threshold of 20%\nIn this section, we provide additional results for both programming languages\n(Java and C#) when using a dissimilarity threshold 20% in order to explore the\nimpact of threshold on clone detection as we use in our analysis 30% as threshold.\nFigure 29 shows the code clones occurrences in DL and Traditional Java projects\nfor both code clones granularities. Figure 30 shows the same analysis but for C#\nprojects.\nClones in Deep Learning Code: What, Where, and Why?\n65\nDeep Learning\nTraditional\nProject Type (Java Th20%)\n0.05\n0.10\n0.15\n0.20\nlog(LOCC / total SLOC)\n(a) Function Granularity\nDeep Learning\nTraditional\nProject Type (java Th20)\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nlog(LOCC / total SLOC)\n(b) Block Granularity\nFig. 29 Code Clones Occurrences in DL and Traditional Java Projects Using threshold as\n20% for Both Code Clones Granularities: (a) Function, (b) Block. LOCC: Lines Of Code\nClones, SLOC: Source Lines Of Code.\nDeep Learning\nTraditional\nProject Type (c# Th20)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nlog(LOCC / total SLOC)\n(a) Function Granularity\nDeep Learning\nTraditional\nProject Type (c# Th20)\n0.0\n0.1\n0.2\n0.3\n0.4\nlog(LOCC / total SLOC)\n(b) Block Granularity\nFig. 30 Code Clones Occurrences in DL and Traditional C# Projects for Both Code Clones\nGranularities: (a) Function, (b) Block and using 20% as threshold. LOCC: Lines Of Code\nClones, SLOC: Source Lines Of Code.\nWe further extend our analysis by comparing clones types. Figure 31 and Figure\n32 illustrate the clone density in DL and traditional projects for clone types and\ngranularity for the two programming languages (Java and C# respectively).\nAppendix C\nRQ2 Additional Results\nC.1\nOther Programming Languages Analysis Results\nWe study the distribution of diﬀerent clones types by clone location in DL and\ntraditional code in java projects (Figure 33) and in C# projects (Figure 34)\n66\nJebnoun et al.\nDeep Learning\nTraditional\nProject Type (java Th20)\n10\n1\nlog(LOCC / total SLOC)\n(a) Function Granularity\nType 1\nType 2\nType 3\nDeep Learning\nTraditional\nProject Type (java Th20)\n10\n1\nlog(LOCC / total SLOC)\n(b) Block Granularity\nType 1\nType 2\nType 3\nFig. 31 Clone Density in DL and Traditional Java Projects for Clone Types and Granularity\nUsing threshold as 20% LOCC: Lines Of Code Clones\nDeep Learning\nTraditional\nProject Type (c# Th20)\n10\n3\n10\n2\n10\n1\nlog(LOCC / total SLOC)\n(a) Function Granularity\nType 1\nType 2\nType 3\nDeep Learning\nTraditional\nProject Type (c# Th20)\n10\n2\n10\n1\nlog(LOCC / total SLOC)\n(b) Block Granularity\nType 1\nType 2\nType 3\nFig. 32 Clone Density in DL and Traditional C# Projects for Clone Types and Granularity\nand using 20% as threshold. LOCC: Lines Of Code Clones\nC.2\nResults of Clone Detection Using Threshold of 20%\nIn this section, we present the additional analysis we performed to address RQ2.\nWe examine the code clone distribution by location in DL and traditional java\n(Figure 35) and C# (Figure 36) systems using 20% as dissimilarity threshold.\nWe further study the percentages of average number of fragments of code clones\nby location of clones in both deep learning and traditional using 20% dissimilarity\nthreshold for the two programming language Java (Figure 37) and C# (Figure\n38).\nWe then study the distribution of diﬀerent types of clones in the diﬀerent\nclones location (Same ﬁle, Same directory, and diﬀerent directories) using 20%\ndissimilarity threshold for the two programming languages Java (Figure 39) and\nC# (Figure 40).\nClones in Deep Learning Code: What, Where, and Why?\n67\nDeep Learning\nTraditional\nProject Type (Java)\n0\n20\n40\n60\n80\nProportion of Cloned LOC (%)\n (A) Type 1 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\nProject Type (Java)\n0\n20\n40\n60\n80\nProportion of Cloned LOC (%)\n (B) Type 2 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\nProject Type (Java)\n10\n20\n30\n40\n50\n60\n70\n80\nProportion of Cloned LOC (%)\n (C) Type 3 \nSame file\nSame directory\nDifferent directories\nFig. 33 Distribution of Diﬀerent Types of Clones by Clone Location in DL and Traditional\nCode (Java)\nAppendix D\nRQ3 Additional Results\nIn this section, we provide additional analysis on the distribution of the size of\ncloned and non-cloned functions in DL and traditional systems (Figure 41). This is\ndone to understand if size is playing an important confounding role in identifying\nbug ﬁxing commits that are related to clones. We study the distribution of the\nmean size of cloned and non-cloned functions per systems in DL and traditional\nsystems in Python projects (Figure 42) and for Java and C# (Figure 43).\nAppendix E\nRQ4 Additional Results\nAs explained in RQ4 but in percentages, Table 23 shows the total number of code\nclones attributed to the DL phases. The total number of code clones manually\nanalyzed is 595.\n68\nJebnoun et al.\nTraditional\nDeep Learning\nProject Type C#\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (A) Type 1 \nSame file\nSame directory\nDifferent directories\nTraditional\nDeep Learning\nProject Type C#\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (B) Type 2 \nSame file\nSame directory\nDifferent directories\nTraditional\nDeep Learning\nProject Type C#\n0\n10\n20\n30\n40\n50\n60\n70\nProportion of Cloned LOC (%)\n (C) Type 3 \nSame file\nSame directory\nDifferent directories\nFig. 34 Distribution of Diﬀerent Types of Clones by Clone Location in DL and Traditional\nCode (C#)\nClones in Deep Learning Code: What, Where, and Why?\n69\nDeep Learning\nTraditional\nProject Type (java Th20)\n0\n10\n20\n30\n40\n50\n60\n70\nProportion of Cloned LOC (%)\nSame file\nSame directory\nDifferent directories\nFig. 35 Code Clones Distribution by Location in DL and Traditional java Systems using\n20% as Threshold Regarding Percentage of Lines of Code Clones (LOCC). i.e, (LOCC/total\nLOCC)x 100\nTraditional\nDeep Learning\nProject Type (C# Th20)\n0\n20\n40\n60\n80\nProportion of Cloned LOC (%)\nSame file\nSame directory\nDifferent directories\nFig. 36 Code Clones Distribution by Location in DL and Traditional C# Systems using\n20% as Threshold Regarding Percentage of Lines of Code Clones (LOCC). i.e, (LOCC/total\nLOCC)x 100\n70\nJebnoun et al.\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n28.02%\n32.7%\n39.28%\nDeep Learning (Java Th20)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n34.2%\n21.46%\n44.34%\nTraditional (java Th20)\nFig. 37\nPercentages of Average Number of Fragments of Code Clones by Location of Clones\nin both Deep Learning and Traditional Java Systems using 20% as threshold\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n36.24%\n39.54%\n32.17%\nDeep Learning (C# Th20)\nSame file\nSame directory\nDifferent directories\n0\n10\n20\n30\n40\nPercentage (%) of Cloned Fragements\n41.79%\n22.92%\n35.29%\nTraditional (java Th20)\nFig. 38\nPercentages of Average Number of Fragments of Code Clones by Location of Clones\nin both Deep Learning and Traditional C# Systems using 20% as threshold\nClones in Deep Learning Code: What, Where, and Why?\n71\nDeep Learning\nTraditional\nProject Type (java Th20)\n0\n20\n40\n60\n80\nProportion of Cloned LOC (%)\n (A) Type 1 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\nProject Type (java Th20)\n0\n20\n40\n60\n80\nProportion of Cloned LOC (%)\n (B) Type 2 \nSame file\nSame directory\nDifferent directories\nDeep Learning\nTraditional\nProject Type (java Th20)\n10\n20\n30\n40\n50\n60\n70\n80\nProportion of Cloned LOC (%)\n (C) Type 3 \nSame file\nSame directory\nDifferent directories\nFig. 39 Distribution of Diﬀerent Types of Clones by Clone Location in DL and Traditional\nCode (Java) with 20% as threshold\n72\nJebnoun et al.\nTraditional\nDeep Learning\nProject Type (C# Th20)\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (A) Type 1 \nSame file\nSame directory\nDifferent directories\nTraditional\nDeep Learning\nProject Type (C# Th20)\n0\n20\n40\n60\n80\n100\nProportion of Cloned LOC (%)\n (B) Type 2 \nSame file\nSame directory\nDifferent directories\nTraditional\nDeep Learning\nProject Type (C# Th20)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nProportion of Cloned LOC (%)\n (C) Type 3 \nSame file\nSame directory\nDifferent directories\nFig. 40 Distribution of Diﬀerent Types of Clones by Clone Location in DL and Traditional\nCode (C#) with 20% as threshold\nCloned\nNon-cloned\nClone Status\n10\n20\n30\n40\n50\nFunction Size (LOC)\n(a) Comparative Size of Functions (DL)\nCloned\nNon-cloned\nClone Status\n5\n10\n15\n20\n25\n30\n35\n40\nFunction Size (LOC)\n(b) Comparative Size of Functions (Traditional)\nFig. 41 Distribution of the Size of Cloned and Non-cloned Functions in DL and Traditional\nSystems\nClones in Deep Learning Code: What, Where, and Why?\n73\nCloned\nNon-cloned\nClone Status\n10\n15\n20\n25\n30\nFunction Size (LOC)\n(a) Comparative Average Size of Functions (DL)\nCloned\nNon-cloned\nClone Status\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nFunction Size (LOC)\n(a) Comparative Average Size of Functions (Traditional)\nFig. 42 Distribution of Mean Size of Cloned and Non-cloned Functions Per Systems in DL\nand Traditional Systems\nCloned\nNon-cloned\nClone Status\n10\n12\n14\n16\n18\n20\n22\nFunction Size (LOC)\n(a) Average Size of Java Functions (DL)\nCloned\nNon-cloned\nClone Status\n10\n15\n20\n25\n30\nFunction Size (LOC)\n(a) Average Size of C# Functions (DL)\nFig. 43 Distribution of Mean Size of Cloned and Non-cloned Functions Per Systems in Java\nand C# DL Systems\n74\nJebnoun et al.\nTable 23 Total number of Occurrence of Code Clones in DL Phases\ndl phase\ncategory\ndl phase\nsubcategory\nType 1\noccs\nType 2\noccs\nType 3\noccs\noccs\nin subcat\noccs\nin total\nPreliminary\npreparation\nhardware requirements\n6\n0\n0\n6\n6\nData\ncollection\nload data\n6\n6\n12\n24\n30\nload label\n6\n0\n0\n6\nData\npostprocessing\ncompute output shape\n0\n0\n6\n6\n50\nobject localization\n12\n0\n7\n19\nprocess output\n12\n0\n7\n19\nset shape of output data\n6\n0\n0\n6\nData\npreprocessing\napply data augmentation\n6\n0\n0\n6\n110\ndata normalization\n0\n0\n12\n12\nget batches of data\n0\n6\n0\n6\nget numerical feature columns\n6\n0\n6\n12\nparse arguments\n0\n0\n6\n6\nprepare tensor\n12\n0\n0\n12\nprocess input\n0\n0\n20\n20\nresize image\n6\n0\n0\n6\nset shape of input data\n0\n0\n12\n12\nset type of input data\n0\n0\n6\n6\nsetting format input data\n0\n0\n6\n6\nsplit data\n0\n0\n6\n6\nModel\nprediction\ninference\n12\n0\n0\n12\n12\nModel\nconstruction\nmodel component format verif.\n6\n0\n0\n6\n216\nactivation function call\n0\n0\n6\n6\nbuild model\n6\n0\n0\n6\nbuild one subnetwork\n0\n0\n6\n6\ncompute model outputs\n0\n0\n6\n6\ninit evaluation metrics\n0\n0\n6\n6\ninitialize model graph\n0\n0\n6\n6\ninitialize model output\n6\n0\n0\n6\nlayer construction\n0\n6\n6\n12\nmodel architecture instantiation\n0\n0\n12\n12\nmodel (hyper)parameters init\n33\n46\n65\n144\nModel\nevaluation\nperformance metric computation\n0\n12\n36\n48\n55\ntest data prediction\n0\n0\n7\n7\nModel\ntraining\ncompute loss\n30\n0\n0\n30\n110\nget pooling info\n0\n0\n6\n6\nmeasure model accuracy\n7\n0\n0\n7\nmodel training\n6\n0\n12\n18\none model step training\n12\n6\n12\n30\ntraining procedure\n0\n0\n7\n7\nweight normalization\n0\n0\n12\n12\nModel\ntuning\nMinibatch size\n0\n0\n6\n6\n6\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2021-07-28",
  "updated": "2021-07-28"
}