{
  "id": "http://arxiv.org/abs/2108.02002v1",
  "title": "Online unsupervised Learning for domain shift in COVID-19 CT scan datasets",
  "authors": [
    "Nicolas Ewen",
    "Naimul Khan"
  ],
  "abstract": "Neural networks often require large amounts of expert annotated data to\ntrain. When changes are made in the process of medical imaging, trained\nnetworks may not perform as well, and obtaining large amounts of expert\nannotations for each change in the imaging process can be time consuming and\nexpensive. Online unsupervised learning is a method that has been proposed to\ndeal with situations where there is a domain shift in incoming data, and a lack\nof annotations. The aim of this study is to see whether online unsupervised\nlearning can help COVID-19 CT scan classification models adjust to slight\ndomain shifts, when there are no annotations available for the new data. A\ntotal of six experiments are performed using three test datasets with differing\namounts of domain shift. These experiments compare the performance of the\nonline unsupervised learning strategy to a baseline, as well as comparing how\nthe strategy performs on different domain shifts. Code for online unsupervised\nlearning can be found at this link:\nhttps://github.com/Mewtwo/online-unsupervised-learning",
  "text": " \nONLINE UNSUPERVISED LEARNING FOR DOMAIN SHIFT IN COVID-19 CT SCAN \nDATASETS \n \nNicolas Ewen*, Naimul Khan* \n \n* Electrical, Computer, and Biomedical Engineering, Ryerson University, Toronto, ON \n \n \nABSTRACT \nNeural networks often require large amounts of expert anno- \ntated data to train. When changes are made in the process of \nmedical imaging, trained networks may not perform as well, \nand obtaining large amounts of expert annotations for each \nchange in the imaging process can be time consuming and \nexpensive. Online unsupervised learning is a method that has \nbeen proposed to deal with situations where there is a domain \nshift in incoming data, and a lack of annotations. The aim \nof this study is to see whether online unsupervised learning \ncan help COVID-19 CT scan classification models adjust to \nslight domain shifts, when there are no annotations available \nfor the new data. A total of six experiments are performed \nusing three test datasets with differing amounts of domain \nshift. These experiments compare the performance of the \nonline unsupervised learning strategy to a baseline, as well \nas comparing how the strategy performs on different domain \nshifts. Code for online unsupervised learning can be found \nat this link: https://github.com/Mewtwo/online-unsupervised- \nlearning \nIndex Terms— online unsupervised learning, self super- \nvision, CNN, transfer learning, neural network, medical im- \nage, COVID-19, CT scan \n \n1. INTRODUCTION \n \nIn this study we aim to determine whether unsupervised on- \nline learning can increase classification performance of con- \nvolutional neural networks (CNNs) on COVID-19 CT scan \ndatasets [1]. We will explore the scenario where the target \ndatasets have no annotations, and have slight domain shifts \nfrom the available training data. A strategy for unsupervised \nonline learning for COVID-19 CT scans is proposed, and its \nperformance is evaluated on three different test sets. \nCNN models have been effective for classification on \nmedical imaging datasets [2][3][4][5][6]. CNN models can \nbe hard to train on medical imaging datasets because they \nrequire large amounts of annotated data. Large amounts of \nannotated data may not be available for a variety of reasons, \nincluding cost and availability of expert annotators [5][6]. \nData augmentation, transfer learning, and self supervision are \nmethods that can be used to help increase CNN performance \n \n \n \n \nFig. 1. Example of a CT scan with Covid-19 (left), and with \nCAP (right). \n \n \nwhen there is a small amount of annotated data [5][6][7], \nhowever, unsupervised learning methods are needed when \nthere are no annotations [8]. \nPerforming transfer learning from a CNN pre-trained on \nImageNet has improved model performance for COVID-19 \nCT scan datasets over training from scratch in a number of \npapers [9][4]. Some recent works further improved classifi- \ncation performance by using self supervision before transfer \nlearning [8][10][11]. \nOne method that is used to deal with few available anno- \ntations for training is transfer learning.  Transfer learning is \nperformed by taking a pre-trained network and then fine tun- \ning it using the target dataset onto the target task [4].  This \nmethod allows simple and reusable features in the early lay- \ners of the network to benefit from training on an unrelated \ndataset.  Since early layers are generally more reusable, the \nnetwork will not have to train them as much, thus reducing the \namount of annotated data needed to train the network [4][12]. \nAnother method of dealing with a limited amount of avail- \nable annotations for training is self supervised learning, which \nis a form of unsupervised learning. Self supervised learning \nuses unlabelled or automatically labelled data to pre-train the \nnetwork to learn useful feature semantics in the target im- \nages [8].  Once the network has been pre-trained, it is then \nfine tuned onto the target dataset using transfer learning. This \nmethod allows the network to learn on limited labelled train- \ning data since less data will be needed after the network has \nalready learned some useful feature semantics [8]. \nOnline unsupervised learning is a combination of online \nmachine learning and unsupervised learning. Online machine \nlearning is a type of machine learning where the model con- \ntinuously updates itself with new data as the new data arrives. \nUnsupervised learning allows models to learn from data with- \nout any expert annotations [8]. Online unsupervised learning \nallows the model to continuously improve as new data with- \nout annotations comes in [13]. \nOnline unsupervised learning is a field of machine learn- \ning that can help predictive models adapt to new situations. \nNew illnesses and screening methods, combined with a lack \nof expert annotators may cause domain shifts in incoming \ndata, with no labels. Online unsupervised learning can help \ntrain models under such circumstances, where other tech- \nniques may have to wait for more data or annotations [13]. \nOur main contribution in this work is to highlight and \ndemonstrate an online unsupervised learning strategy. While \nthe idea of using unsupervised online learning to increase \nclassification performance is not new [14], to the best of our \nknowledge it has not been done for medical imaging on a \nCOVID-19 CT scan dataset. \nWe felt online learning was a good approach for COVID- \n19 CT scans because as more data is collected, the models can \nbe updated. We decided to model what this might look like in \npractise by dividing each dataset into quarters and performing \nthe online updates after each quarter. This would allow real- \ntime results, while also continuously increasing classification \nperformance. \nThe three test datasets are good to use for this experiment. \nThe first test dataset comes from the same settings as the train- \ning and validation set. This set should be a benchmark for \nhow much the model improves only due to extra training and \ndata, since there is no domain shift. This means that with \nthe first test set, we are performing semi-supervised learning, \nas opposed to unsupervised learning. The second test dataset \nis only COVID-19, and healthy patients, but with low dosage. \nThis should be a slight domain shift. The third test dataset has \nCOVID-19, CAP, and healthy patients. The patients also have \na heart condition, and the dosage and slice thickness vary. \nThis test set has the largest domain shift from the training and \nvalidation sets. Combined, the results from these three test \ndatasets should show how well our proposed strategy of on- \nline unsupervised learning adapts to slight domain shifts. \nThis work is very important because if a method of unsu- \npervised online learning can be used to increase classification \nperformance under domain shift, then new models will not \nhave to be trained from scratch each time the domain shifts \nslightly. It also means that a model can be updated as new \ndata becomes available, without having to wait for an expert \nto annotate the new data. \nWe believe that our proposed strategy can be used in prac- \ntise in the real world. Hospitals performing CT scans for \nCOVID-19 can use their models to produce real time predic- \ntions, and the models update themselves as more data comes \nin. Since this strategy does not use annotations on the new \ndata coming in, the model can be updated at a rate depen- \ndant on the rate of CT scans. As a demonstration of how this \nstrategy can be used, we divide our three test datasets into \nfour quarters. Each quarter will be treated as patients coming \nin sequential order. This means that the first quarter will be \nevaluated with our base models. The data from the first quar- \nter will then be used to update the models. Then the second \nquarter will be evaluated using the updated models, and so \non. By dividing our three test datasets into quarters, we are \nable to run six experiments to test how well this method of \nonline unsupervised learning adapts to slight domain shifts in \nCOVID-19 CT scan datasets. \n \n2. METHODOLOGY \n \n2.1. Dataset \nThe dataset we used for this paper was the dataset used in the \nSPGC COVID-19 competition [1]. This dataset is a dataset of \nchest CT scan images organized by patient. The patients can \nbe in one of three classes: Healthy, COVID-19, or CAP. There \nare a total of 307 patients in the training and validation sets. \nThere are 76 healthy patients, 171 COVID-19 patients, and 60 \nCAP patients. Patient-level labels are provided by three radi- \nologists, who have greater than 90 percent agreement. Images \nwere taken under different circumstances, including different \nmedical centres, scanners, using different slice thicknesses, \neffective mA, and exposure time. 55 of the COVID-19 pa- \ntients, and 25 of the CAP patients have slice-level labels pro- \nvided by a single radiologist. There are about 5000 slices \nlabelled positive for infection, and about 18,500 labelled neg- \native for infection. \nIn addition to the training and validation sets provided, \nthere are also three test sets. The first test set is from patients \nclassified as healthy, COVID-19 positive, and CAP positive, \nand comes from the same distribution as the training and val- \nidation sets. The second test set from patients classified as \nhealthy and COVID-19 positive only, and a lower dosage was \nused for the scans. The third test set comes from patients clas- \nsified as healthy, COVID-19 positive, and CAP positive. The \nscans were administered under various settings. The healthy \npatients in this test set also had an unrelated disease. \n \n2.2. Pre-processing \nThe SPGC COVID-19 dataset has pre-set training, validation, \nand testing sets. We used these sets and did not change them. \nWe extracted the slice-level labels and images where slice- \nlevel annotations were provided. We kept the images and la- \nbels that were positive for COVID-19 and CAP, but we did \nnot keep the negative ones. Instead we extracted slices from \nhealthy patients with large lung area, in a similar manner to \nRahimzadeh et al. [15]. We made an image selection algo- \nrithm to filter out images without lungs, or with small sections \nof lung, as well as images with lungs where much of the lung \nis not visible. This was done by setting an inner area of the \nimage, and counting darker pixels in said area. A threshold \nwas calculated for each patient on the fly using the average \nnumber of dark pixels, and images with less dark pixels in the \narea than the threshold were removed. These were used for \nnormal slices. Images were resized to 224 x 224, and rescaled \nto between 0 and 1. The validation set was used to tune hy- \nperparameters. For each patient, we also saved an array of the \nimages with large lung area. \n \n2.3. Slice-level models \nWe trained two different slice-level models. The first model \nwas trained to classify slices as healthy or not healthy. This \nmodel was trained with the healthy slices we extracted from \nhealthy patients, and the labelled slices provided for COVID- \n19 and CAP patients. The second model was trained to clas- \nsify unhealthy slices as either COVID-19 or CAP. This model \nwas trained with the labelled slices provided for COVID-19 \nand CAP patients. \nFor both models, we used the same network architecture \nand training strategy. The only difference was the data used. \nThe network uses a DenseNet169 base, with a dense layer \nwith 8 nodes followed by a softmax output. Batch normal- \nization, regularization, and dropout were used as well. Our \ntraining strategy was a two-step process. For each model, we \nfirst performed targeted self supervision in a similar manner \nto Ewen and Khan [16]. We made horizontally flipped copies \nof our training images, and trained the network to determine \nwhether an image was flipped or not. The second step in train- \ning was to then transfer onto our target dataset. \n \n2.4. Patient-level models \nAt the patient level, we used the slices for each patient that \nwe had previously extracted with larger lung area. The cho- \nsen slices were first sent through our slice-level model that \nclassifies the slices as healthy or unhealthy. We took the av- \nerage score of the patient’s softmax scores to get two aver- \nage scores: a healthy score, and an unhealthy score. If the \nhealthy score was greater than five times the unhealthy score, \nthe patient was classified as healthy.  Otherwise, the patient \n \n \n \n \n \nFig. 2. Example of a healthy patient’s slices with large lung \narea (left) and without (right). \nwas classified as unhealthy. This threshold of five times was \nchosen after testing on the validation set. \nIf the patient was classified as unhealthy, then the pa- \ntients’ slices were sent to the next slice-level model. The cho- \nsen slices were then classified as either COVID-19 or CAP. \nWe again took the average softmax scores to get two average \nscores: a COVID-19 score, and a CAP score. If the COVID- \n19 score was greater than the CAP score, the patient was clas- \nsified as having COVID-19. Otherwise, the patient was clas- \nsified as having CAP. \n \n2.5. Adjusting patient-level thresholds \nOn a number of runs, the slice level models produced a large \ndifference in the recall of the classes. For example, due to \nmany more images with COVID-19 than CAP, the slice level \nmodel could have a recall of about 0.99 for COVID-19 im- \nages, but only 0.72 for CAP images on the validation set. This \nsuggests that the slice level model is more likely to classify \nCAP incorrectly than COVID-19. This could cause incorrect \nclassification in borderline cases. \nFor example, if the slice level model classified 31 slices as \nCOVID-19, and 30 as CAP, then the patient level model will \nmore likely classify the patient as having COVID-19. How- \never, since the patient only has one of either CAP or COVID- \n19, about 30 slices have been misclassified. Given the differ- \nence in the recalls of the slice level model, it is more likely \nthat CAP images were classified incorrectly, and that it would \nbe better to classify this patient as having CAP. \nTo deal with this problem, we came up with a method of \nadjusting the set threshold levels in the patient level models \non the fly with a multiplier. To calculate these multipliers, we \ntook the ratio of the two recalls. In our example this is CAP \nrecall divided by COVID-19 recall.  With a CAP recall of \n0.72 and a COVID-19 recall of 0.99, this would be 0.72/0.99 \n= 0.725. We then multiply the number of COVID-19 slices by \nthe multiplier. This would give us 31*0.725 = 22.5 effective \nslices. Since this number is lower than the 30 CAP slices, \nthe patient is then more likely to be classified as having CAP. \nSimilarly, a second threshold multiplier was also calculated \nfor the healthy threshold. \nIf performances of the respective recalls are reversed, the \nthreshold multiplier will be greater than 1. This means that \nthe threshold adjustment will always be made in favour of \nthe class with lower recall, and that as the difference in recall \ngrows, the threshold multiplier, and therefore adjustment, gets \nlarger. \n \n2.6. Online Unsupervised learning \nWe tested this method using  three  different  COVID-19 \nCT scan test sets. Our “baseline” contains two networks \ntrained on the training and validation sets, first one providing \nhealthy/unhealthy binary slice level classification, second one \n \nTable 1. Experiments \nProposed Experiments \nTest Set \nModel \nExp. 1 \nTest Set 1 \nBaseline \nExp. 2 \nTest Set 2 \nBaseline \nExp. 3 \nTest Set 3 \nBaseline \nExp. 4 \nTest Set 1 \nOnline Unsupervised \nExp. 5 \nTest Set 2 \nOnline Unsupervised \nExp. 6 \nTest Set 3 \nOnline Unsupervised \n \n \n \n \nproviding COVID/CAP slice classification on the unhealthy \nslices from the first model. For each test dataset, we retrained \nthe slice level models using our online unsupervised learning \nscheme. This gave us 4 total models, the baseline model, and \na model specifically fine tuned to its respective test dataset. \nTo perform the online unsupervised updates to the mod- \nels, we first ran the next quarter of test data through our mod- \nels to obtain predictions. From these predictions we took the \nconfident slices. Confident slices were those with a softmax \nscore of at least 0.9 in agreement with the patient’s classifi- \ncation.  We then used these slices, along with our label that \nwe assigned to it during predictions, and our original training \ndata, to retrain the model. The aim was for this to allow the \nmodel to adjust to slight domain shifts, such as lower dosage. \nWe used a strategy that is adjusted from what was pro- \nposed by Cao and He [14].  After predictions were gener- \nated for a batch, a new slice-level model was trained for both \nhealthy vs unhealthy classification as well as COVID-19 vs \nCAP classification.   These two new models were initiated \nfrom the point that the self supervision step had finished for \nthe base models.  The confident images from the test batch, \nalong with their predictions, were added to the original train- \ning and validation sets, and then trained in the same way as \nthe base slice-level models.  This means that after receiving \nevery test batch, two new slice-level models were trained. \n \n2.7. Experiments \nWe ran six experiments in total. For each test set, we ran an \nexperiment using the baseline method to predict the class of \nthe patients, and another experiment using the online unsu- \npervised method to predict the patients’ class. There are three \ntest sets, so this resulted in a total of six experiments. \n \n3. RESULTS \n \nThe results of the experiments can be seen in Table 2. On \nthe first test set, the baseline method got 90 percent accuracy, \nwhile the online unsupervised method got 86.7 percent accu- \nracy.This result was unexpected, since test set 1 comes from \nthe same distribution as the training and validation set, and \nthe baseline performed well. A possible explanation for this \nresult is that the test set was too small. 30 patients may not \nbe sufficient to demonstrate the proposed method. The ini- \ntial guesses from the online method are the same as for the \nbaseline, since the models have not yet updated. This means \nthat model performance decreased even though most patients \nwere correctly classified initially. \nOn the second test set, the baseline method got 66.7 per- \ncent accuracy, and the online unsupervised method got 76.7 \npercent accuracy. This dataset had a small domain shift from \nthe training and validation sets, and seemed ideal for our \nmethod. The increase in performance is promising. \nThe third test set had the larger domain shift from the \ntraining and validation sets. The baseline method got 63.3 \npercent accuracy, while the online unsupervised method got \n53.3 percent accuracy. This result is not entirely unexpected, \nas this test set had the largest domain shift. Another possi- \nble explanation for the poor performance on this test set is \nthat the image selection algorithm may not be well suited to \nimages from patients with heart conditions, as it may throw \nout useful images. A number of patients in this test set were \nleft with significantly fewer images after the selection process \ncompared to patients from the other test sets. \n \n4. CONCLUSIONS \n \nIn this paper we demonstrated an online unsupervised learn- \ning method to boost performance of a COVID-19 classifica- \ntion model when tested with data with a domain shift from \nthe training and validation sets. The aim of this method is to \nallow a model to adapt to a small domain shift in the data, \nwithout the need for expert labels. \nGiven the results of the experiments, we conclude that an \nonline unsupervised learning method may be able to boost \nclassification performance of COVID-19 diagnosis models \nunder slight domain shift. However, further fine tuning is \nneeded to see how much it can boost performance. Further \nexplorations using different image selection algorithms may \nhelp boost performance on test set three. Testing on larger \ndatasets may help clarify some of the current issues. \n \n \nTable 2. Results of Experiments \n \n \n \n \n \n \n \n \naAccuracy with a 95 percent confidence interval. \nTest set \nModel \nAccuracya \nExp.1 \nSet 1 \nBaseline \n0.9 +- 0.107 \nExp.2 \nSet 2 \nBaseline \n0.667 +- 0.169 \nExp.3 \nSet 3 \nBaseline \n0.633 +- 0.172 \nExp.4 \nSet 1 \nOnline Unsupervised \n0.867 +- 0.122 \nExp.5 \nSet 2 \nOnline Unsupervised \n0.767 +- 0.151 \nExp.6 \nSet 3 \nOnline Unsupervised \n0.533 +- 0.179 \n5. REFERENCES \n \n[1] Parnian Afshar, Shahin Heidarian, Nastaran Enshaei, \nFarnoosh Naderkhani, Moezedin Javad Rafiee, Anasta- \nsia Oikonomou, Faranak Babaki Fard, Kaveh Samimi, \nKonstantinos N. Plataniotis, and Arash Mohammadi, \n“Covid-ct-md: Covid-19 computed tomography (ct) \nscan dataset applicable in machine learning and deep \nlearning,” 2020. \n[2] \nParnian \nAfshar, \nShahin \nHeidarian, \nFarnoosh \nNaderkhani, Anastasia Oikonomou, Konstantinos N. \nPlataniotis, and Arash Mohammadi, “Covid-caps: A \ncapsule network-based framework for identification of \ncovid-19 cases from x-ray images,” 2020. \n[3] Shahin Heidarian, Parnian Afshar, Nastaran Enshaei, \nFarnoosh Naderkhani, Anastasia Oikonomou, S. Farokh \nAtashzar, Faranak Babaki Fard, Kaveh Samimi, Kon- \nstantinos N. Plataniotis, Arash Mohammadi, and \nMoezedin Javad Rafiee, “Covid-fact: A fully-automated \ncapsule network-based framework for identification of \ncovid-19 cases from chest ct scans,” 2020. \n[4] Xuehai He, Xingyi Yang, Shanghang Zhang, Jinyu \nZhao, Yichen Zhang, Eric Xing, and Pengtao Xie, \n“Sample-efficient deep learning for covid-19 diagnosis \nbased on ct scans,” medRxiv, 2020. \n[5] Richa Agarwal, Oliver Diaz, Xavier Llado´, Moi Hoon \nYap,  and Robert Mart´ı,  “Automatic mass detection \nin mammograms using deep convolutional neural net- \nworks,” Journal of Medical Imaging, vol. 6, no. 3, pp. 1 \n– 9, 2019. \n[6] Hiba Chougrad, Hamid Zouaki, and Omar Alheyane, \n“Convolutional neural networks for breast cancer \nscreening: Transfer learning with exponential decay,” \nCoRR, vol. abs/1711.10752, 2017. \n[7] N. Khalifa M. Loey, G. Manogaran, “A deep trans- \nfer learning model with classical data augmentation and \ncgan to detect covid-19 from chest ct radiography digital \nimages,” 2020. \n[8] Longlong Jing and Yingli Tian, “Self-supervised visual \nfeature learning with deep neural networks: A survey,” \nCoRR, vol. abs/1902.06162, 2019. \n[9] Dina Ragab, Maha Sharkas, Stephen Marshall, and Jin- \nchang Ren, “Breast cancer detection using deep convo- \nlutional neural networks and support vector machines,” \nPeerJ, vol. 7, pp. e6201, 01 2019. \n[10] L Chen, P Bentley, K Mori, K Misawa, M Fujiwara, and \nD Rueckert,  “Self-supervised learning for medical im- \nage analysis using image context restoration,” Medical \nImage Analysis, vol. 58, pp. 1–12, 2019. \n[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and \nRoss Girshick, “Momentum contrast for unsupervised \nvisual representation learning,” 2020. \n[12] Francois Chollet, Deep Learning with Python, Manning \nPublications Co., USA, 1st edition, 2017. \n[13] J. H. Moon, Debasmit Das, and C.S. George Lee, \n“Multi-step online unsupervised domain adaptation,” \nICASSP 2020 - 2020 IEEE International Conference \non Acoustics, Speech and Signal Processing (ICASSP), \nMay 2020. \n[14] Yuan Cao and Haibo He, “Learning from testing data: \nA new view of incremental semi-supervised learning,” \nin 2008 IEEE International Joint Conference on Neu- \nral Networks (IEEE World Congress on Computational \nIntelligence), 2008, pp. 2872–2878. \n[15] Mohammad  Rahimzadeh,  Abolfazl   Attar,   and \nSeyed Mohammad Sakhaei, “A  fully  automated \ndeep learning-based network for detecting covid-19 \nfrom a new and large lung ct scan dataset,” medRxiv, \n2020. \n[16] Nicolas Ewen and Naimul Khan, “Targeted self su- \npervision for classification on a small covid-19 ct scan \ndataset,” 2020. \n \n6. ACKNOWLEDGEMENTS \n \nWe acknowledge NSERC’s funding through an Alliance grant \nto conduct this study. \n",
  "categories": [
    "eess.IV",
    "cs.CV"
  ],
  "published": "2021-07-31",
  "updated": "2021-07-31"
}