{
  "id": "http://arxiv.org/abs/2108.02170v1",
  "title": "Curriculum learning for language modeling",
  "authors": [
    "Daniel Campos"
  ],
  "abstract": "Language Models like ELMo and BERT have provided robust representations of\nnatural language, which serve as the language understanding component for a\ndiverse range of downstream tasks.Curriculum learning is a method that employs\na structured training regime instead, which has been leveraged in computer\nvision and machine translation to improve model training speed and model\nperformance. While language models have proven transformational for the natural\nlanguage processing community, these models have proven expensive,\nenergy-intensive, and challenging to train. In this work, we explore the effect\nof curriculum learning on language model pretraining using various\nlinguistically motivated curricula and evaluate transfer performance on the\nGLUE Benchmark. Despite a broad variety of training methodologies and\nexperiments we do not find compelling evidence that curriculum learning methods\nimprove language model training.",
  "text": "CURRICULUM LEARNING FOR LANGUAGE MODELING\nDaniel Campos ∗\nUniversity of Illinois Urbana-Champaign\ndcampos3@illinois.edu\nABSTRACT\nLanguage Models like ELMo and BERT have provided robust representations of natural lan-\nguage, which serve as the language understanding component for a diverse range of downstream\ntasks.Curriculum learning is a method that employs a structured training regime instead, which has\nbeen leveraged in computer vision and machine translation to improve model training speed and\nmodel performance. While language models have proven transformational for the natural language\nprocessing community, these models have proven expensive, energy-intensive, and challenging to\ntrain. In this work, we explore the effect of curriculum learning on language model pretraining\nusing various linguistically motivated curricula and evaluate transfer performance on the GLUE\nBenchmark. Despite a broad variety of training methodologies and experiments we do not ﬁnd\ncompelling evidence that curriculum learning methods improve language model training.\nKeywords Curriculum Learning · Language Modeling\n1\nIntroduction\nSeeking to represent natural language, researchers have found language models (LM) with Sesame Street-inspired\nnames [1] [2] [3] to be incredibly effective methods of producing language representations (LR). These LM’s have\nleverage transfer learning by training on a large text corpus to learn a good representation of language which can then be\nused in a down steam task like Question Answering or Entity Resolution. While these LMs have shown to be excellent\nmethods to enable language understanding, the ability to train these models is becoming increasingly computationally\nexpensive [4]. Since model performance is closely tied to the size of training data, model size, and compute used to\ntrain [5] the bulk of existing research has focused on scaling these aspects without much focus on increasing efﬁciency\nof training. Seeking to explore what methods could be used to make LM training more efﬁcient we study the effect of\ncurriculum learning by training ELMo with a wide variety of curricula.\nCurriculum learning (CL) is a training methodology which applies structure to a models training data. CL has been\nstudied broadly in natural language processing and has been very successful in domains like Neural Machine Translation\n(NMT) where CL based models are able to train faster and produce better results [6] [7] [8] than unstructured, stochastic\nsampling. Focusing on LMs, Xu et al. [9] showed that CL can be used in LM ﬁnetuning as a way to improve task\nperformance. Despite an abundance of work exploring CL and LMs to the best of our knowledge we are the ﬁrst to\nexamine the effect of curriculum learning in LM pre-training and transfer performance.\nTo evaluate the effect of CL on LMs we train ELMo with a variety of curricula on the wikitext-2 and wikitext-103 [10]\nwithout modiﬁcation of training time or model hyperparameters. We evaluate model performance on the pre-training\ntask and on the GLUE Benchmark [11] building on the work of Competence Based Curriculum Learning [12] by\nmodifying training sampler within the LM to produce a dataset with gradually increasing difﬁculty 2. The contributions\nof our work are:\n• Exploration of the effects of curriculum learning for language modeling ﬁnding no clear improvement to\nmodels that use curriculum methods for training.\n• Experiments suggesting random curriculum in which the structure of the training regime is random can be just\nas effective as linguistically motivated methods.\n∗Work done pursing Masters Degree at University of Washington\n2Code and results available at https://github.com/spacemanidol/CurriculumLearningForLanguageModels\narXiv:2108.02170v1  [cs.CL]  4 Aug 2021\nCurriculum Learning for Language Modeling\n2\nRelated Work\n2.1\nCurriculum Learning\nCL subset of training regimes which introduce structure to improve training efﬁciency, model performance, or model\nmodel robustness by optimizing what kind of information a model has access at each training step. Experiments with\nRNNs [13] suggested that learning of complex grammatical structure improves when the initial examples the models\nlearn with are more easier. Experiments in modifying language modeling training data ﬁnd a lower loss can be achieved\nby training on incrementally more difﬁcult data [14]. Recently, competence based curriculum [6] has been used to\nimprove machine translation progressively modifying the training corpus until it matches the original distribution. It\nhas been used to reduce training time by up to 70% and improve BLEU performance by 2.2 points on the WMT dataset.\nFor further readings about curriculum learning, applications and current bottlenecks we recommend Soviany et al.’s\nsurvey [15]\n2.2\nLanguage Modeling\nLanguage modeling is a way to assign a probability distribution over some textual representation. In other words, if the\ntask is to model n-grams, the probability of a current input is the probability of a token wi given the previous i tokens.\nLanguage Models like ELMo [1] and BERT [2] leverage large text corpora to learn language representations that can\nbe used for downstream tasks like text classiﬁcation or question answering. While LMs lead to large improvement in\nperformance for downstream tasks they are both expensive and complex to train. A single training run of a model like\nGPT-2 can cost upward of $40,000, the architecture search and hyperparameter tuning can be upwards of $3,000,000,\nand the C02 released by training one of these models can be similar to the C02 released in the entire life-cycle of a car\n[16].\n3\nMethod\nLanguage modeling is a way to assign a probability distribution over some textual representation. This probability\ndistribution is commonly modeled as the probability of a current token wi given the previous i tokens as formally\nrepresented in equation 2. Using language modeling as a pre-training method, LMs learn representations which can be\nused in downstream tasks. Since language has structure, we believe that structuring the pre-training methodology can\nlead to improved model performance. To introduce structure into LM training we leverage Platanios et al.’s competence\nbased curriculum (CBC)[6] as shown in Algorithm 1. CBC uses a notion of model competence and sample difﬁculty\nto control what a model learns. First, the corpus, X, a collection of samples S, where each sample si is a sequence\nof words si = wi\no, wi\n1, ..., wi\nn is sorted by difﬁculty using a which Using a heuristic like sentence length or unigram\nrarity is assigned a difﬁculty ϵsi = [0, 1]. Given a processed corpus, a model is assigned a initial competence λ0 and a\ncompetence increment lambdaincrement. A model’s competence score is a representation of how far along in a training\nregime the model is. At each training step, a model samples from data that is lower than its current competence, updates\nits weights, and increases its competence. The model is only able to train on samples that have a difﬁculty where\nϵsi <= λt.\nUsing CBC we explore 8 proxies for sample difﬁculty: no curriculum, random, sample length, unigram sample proba-\nAlgorithm 1: CBC Training Regime\nResult: Model Trained with Competence Based Curriculum\nInput: X, λ0, λincrement ;\nCompute difﬁculty, ϵsi for si ∈X;\nCompute Cumulative density of ϵsi;\nλt = λ0;\nfor training step t = 1,...,n do\nSample batch b from X such that ϵsi < λt;\nTrain on batch b;\nλt+1 = λt + λincrement;\nend\nbility, bigram sample probability, trigram sample probability, part of speech diversity (POS), and sample dependency\nparse complexity (DEP). For each methodology, for each si in X, we compute a difﬁculty value for each sample ϵsi\nand then sort the dataset by this difﬁculty score. Using the sorted dataset we compute the cumulative density function\n(CDF) giving each sample of the difﬁculty score ϵsi ∈[0, 1]. No curriculum sets λ0 = 1 which means training samples\n2\nCurriculum Learning for Language Modeling\nstochastically and serves as a baseline. A random curriculum is generated by assigning values for ϵsi at random and\nestablishes the effect of any arbitrary structure. The remaining six heuristics are based on common NLP difﬁculty\nmetrics and linguistically motivated heuristics.\n3.0.1\nSample Length\nSample Length builds on the idea that is a lot harder to model longer sentences, as longer sentences require better\ntracking of dependencies. It is calculated by creating a CDF on sentence-length-ϵsi = length(si).\n3.0.2\nSentence Entropy: N-gram difﬁculty\nSentence Entropy builds uses the notion that can be difﬁcult to model is words with a variety of frequency in the\ncorpora. Models, if assumed to behave like humans, would ﬁnd it difﬁcult to understand the meaning of a word if\nthey do not see it in a corpus nor have a diversity of usages to infer meaning. Since the statistical strength of training\nsamples with rare words is low and the early model learned word embeddings are likely to have high variance it is likely\nthat exposing a model early to rare words can result in badly estimated representations. To quantify this difﬁculty we\npropose producing a sentence entropy for each sentence with respect to its unigram, bigram, and trigram probabilities.\nThese are calculated using standard sample entropy calculations as shown below Sample entropy for each N-gram can\nbe thought of the probability of the sample occurring given an approximate naive language modeling assuming words\nare sampled independently. Samples are scored by calculating the product of n-gram log likelihood given the sample.\nNote, we are not calculating the conditional probability of each word given the preceding N words but the probability\nof the N-gram given the text corpus. Calculation of ϵsi is shown in equation 1 where uc, bc, and tc are the counts of\nunique unigrams, bigrams, and trigrams in the corpus, C is the corpus, c(y) is the count of y in a sample, x ∈C is a\nsample in the corpus and wi ∈x is a word in a line, and l(x) is the length of x in n-grams.\np(wn) =\nP\nx∈C c(wn)\nus\np(wn, wm) =\nP\nx∈C c(wn, wm)\nbs\np(wn, wm, wj) =\nP\nx∈C c(wn, wm, wj)\nts\nunigram-ϵ(si) =\nl(si)\nY\nn=0\nlog(p(wn))\nbigram-ϵ(si) =\nl(si)−1\nY\nn=0\nlog(p(wn−1, wn))\ntrigram-ϵsi =\nl(si)−2\nY\nn=0\nlog(p(wn, wn+1, wn+2))\n(1)\n3.0.3\nDependency Tree\nSentences are often modeled as dependency trees to model the interaction between words and groups of words in a text\nsample. While not infallible, sentences that have a deeper tree usually more complex and as a result more difﬁcult. We\nleverage the language processing framework SPACY.IO’s to generate parse trees for each sample and measure the depth\nof each tree. This information is then used to calculate difﬁcult as dep-ϵsi = depth(si). Since there are fewer unique\nvalues for tree depth this method can be inferred to have a high commonality with random difﬁculty.\n3.1\nPart of Speech Diversity\nAnother core part of language complexity can be derived by the diversity of parts-of-speech in a sentence. We believe\nthat more difﬁcult sentences feature a higher diversity of parts-of-speech (POS) and use SPACY.IO’s part of speech\n3\nCurriculum Learning for Language Modeling\ntagger to produce a set for in each sample and calculate difﬁculty with pos-ϵsi = len(set(pos(si)))\nP(w1, . . . , wm) =\nm\nY\ni=1\nP(wi | w1, . . . , wi−1)\n≈\nm\nY\ni=1\nP(wi | wi−(n−1), . . . , wi−1)\n(2)\n4\nExperiments\nTo evaluate the effect of curriculum learning on language modeling we train ELMo models varying the training\ncorpus and using our aforementioned difﬁculty proxies to generate various language models. Training leverages\nwell-established language modeling benchmarks of wikitext-2, and wikitext-103 [10] with details can be found in table\n1 . These datasets collect veriﬁed good and featured articles from English Wikipedia and feature 2 million and 103\nmillion tokens and were selected for the variations in size and speed of training. After training, each language model\nperformance is then evaluated based on performance on the training corpus (measured in perplexity) and transfer ability\non The General Language Understanding Evaluation Benchmark (GLUE) [11]. GLUE is a set of resources focused\non the evaluation of natural language understanding systems. This benchmark pools eleven sentence-level language\nunderstanding tasks tasks.\nCorpus Name\nvocabulary Size\nTokens\nlines\nsentences\nwikitext=2\n33278\n2507007\n44836\n131262\nwikitext-103\n267735\n103690236\n1809468\n5343947\n1B Word Benchmark\n793471\n829250940\nN/A\nN/A\nTable 1: Training Corpus details\n4.1\nPre-Training Details\nUsing the 16 curricula (8 for each corpus) we train an ELMo model using the original code 3 with a modiﬁed batch\nsampler created for competence based sampling. For baselines, we train Elmo models without our modiﬁed CBC\nsampling using wikitext-2, wikitext-103. Following the original work, we train each curriculum-model variant for 10\nepochs on the pre-training corpus, use 2 stacked 4096 dimensional BiLSTMs, use dropout of 0.1, batch size of 128,\nand a context window of 20 tokens. Training was performed using 3 Nvidia 2080 TI GPUs requiring about 30 hours\nfor the wikitext-103 and about an hour for wikitext-2. For CBC training hyperparameters, we performed a grid search\non λincrement and λ0 values ﬁnding the lowest training perplexity at λ0 = 1e −1 λ0 = 1e −1 for wikitext-2 and\nλ0 = 1e −3 λincrement = 1e −5 for wikitext-103.\nIn the original implementation, the training loader loads a ﬁle, shufﬂe all the lines, and samples batches by iterating\nthrough the shufﬂed corpus. Our method load the full corpus, then select a batch at random from the examples that meet\nour model’s current competence. This changes data sampling to unconstrained random sampling without replacement to\nsampling with replacement. Since our competence based sampling has a variety of sample lengths we use the padding\ntoken < PAD > as is common in NMT. All samples are padded to length 20 and the loss on these padding tokens\nis set to zero. Since padding tokens for wikitext-103 we introduce 12,204,311 tokens equating to approximately 12\npercent more FLOPs.\n4.2\nTransfer Learning\nAfter models have pretrained we evaluate on GLUE performance using the JIANT toolkit [17]. JIANT is an open-source\ntool for conducting multi-task and transfer learning experiments in English to implement the GLUE benchmark. JIANT\nbuilds on the notion of a conﬁguration which provides all settings needed to run and reproduce an experiment in a\nsimple text ﬁle. JIANT provides consistent data processing, classiﬁer implementation, and evaluation to ensure that\nusers of the framework can focus on the outputs and not worry about implementing benchmarking tasks like GLUE.\nJIANT uses the pretrained model weight along with a multi-layer perceptron with 512 hidden dimensions to train\non each GLUE tasks. Each JIANT experiment ﬁxes training identically across tasks and inputs using a batch size of\n3https://github.com/allenai/bilm-tf\n4\nCurriculum Learning for Language Modeling\n8, random seed of 42 initial learning rate of 1e-1, dropout of 0.2. Training of each model continues until the model\nlearning rate dips below 1e-6 or the model performance has not improved in 10 epochs. Unless another metric is\nexplicitly mentioned, the GLUE sub-task metric is accuracy.\n4.3\nExperimental Results\nFocusing on model pretraining performance, despite attempts in variation of λ0 and λincrement, no implementation of\nCBC is able to approximate the baselines in term of perplexity on the held out portion of the wikitext-* dataset. Complete\ngraphs can be found in the appendix but all curricula perplexities including the baseline are order of magnitudes higher\nthan stochastic sampling. On wikitext-2, the best performance is achieved by the curricula baseline (λ0 = 1) with a\nperplexity of 770, followed by random with a perplexity of 2105 well above the baseline of 151. We believe this is\ncaused by the change in dataset distribution caused by our curriculum learning implementation. Similar effects are seen\non wikitext-103 where unlike stochastic sampling, which achieve a perplexity of 36, curriculum methods are unable to\nachieve a perplexity under one thousand. Surprisingly, as data size scales we see a larger volatility in perplexity during\ntraining with respect to validation perplexity scores which we attribute to constantly shifting sample distribution caused\nby curriculum methods.\nAs we move our focus to GLUE results on wikitext-2 based models in table 2, we ﬁnd that curriculum methods generally\noutperform stochastic sampling by 10%. We do not ﬁnd strong evidence that the structure of the curriculum matters\nas non curriculum (λ0 = 1) performs better than 4 other curricula and the baseline. Perhaps most surprising, random\noutperforms the baseline when measure by overall glue score despite their being no formal structure in the training\nregime. Observing variability at an individual task level we ﬁnd only CoLA, STS-B and SST have broad variability in\nperformance. We believe this is because these tasks are smaller and more linguistically challenging.\nFocusing on results on larger corpus in table 3 we ﬁnd trends found in wikitext-2 no longer hold as top performance is\nachieved by stochastic unmodiﬁed baseline. We also note that the orderign of system performance does not hold across\ndatasets and as the pretraining dataset grows the variability between models decreases. Similar to the smaller corpus,\nwe ﬁnd the highest sensitivity in ColA and ﬁnd variability in SST and STS-B has become more muted. Surprisingly,\ngiven their had the worse performance in pretraining perplexity, the trigram curricula generate the best transfer task.\nOverall, we ﬁnd that CBC training provided worse performance on validation perplexity portion and improved\nperformance on transfer tasks when he pretraining corpus is small. We believe this reinforces the importance of the size\nof the pretraining corpus since a large corpus allows the model to learn better language representations without any\nstructured training. We also ﬁnd a large disconnect in model pretraining perplexity and transfer task performance as\nperformance on one is not predicative of the other.\nMethod\nOverall\nCola\nSST\nMRPC\nSTS-B\nQQP\nMNLI\nQNLI\nRTE\nWNLI\nDX\ndep\n0.63\n0.19\n0.73\n0.85/0.78\n0.71/0.71\n0.74/0.78\n0.60\n0.75\n0.58\n0.56\n0.11\nunigram\n0.63\n0.18\n0.77\n0.86/0.78\n0.68/0.67\n0.74/0.79\n0.60\n0.75\n0.56\n0.54\n0.13\ntrigram\n0.63\n0.15\n0.76\n0.84/0.76\n0.70/0.69\n0.73/0.78\n0.62\n0.76\n0.54\n0.56\n0.14\nlength\n0.63\n0.19\n0.75\n0.84/0.77\n0.66/0.65\n0.73/0.78\n0.60\n0.75\n0.57\n0.56\n0.13\nno curricula\n0.62\n0.15\n0.75\n0.84/0.77\n0.71/0.71\n0.73/0.78\n0.61\n0.72\n0.54\n0.56\n0.12\nbigram\n0.62\n0.18\n0.77\n0.86/0.78\n0.68/0.67\n0.74/0.79\n0.60\n0.75\n0.56\n0.44\n0.13\nrandom\n0.61\n0.00\n0.76\n0.85/0.78\n0.70/0.70\n0.72/0.78\n0.61\n0.75\n0.58\n0.56\n0.14\npos\n0.61\n0.00\n0.74\n0.84/0.77\n0.66/0.66\n0.71/0.77\n0.61\n0.75\n0.59\n0.56\n0.16\nbaseline\n0.59\n0.00\n0.70\n0.85/0.78\n0.66/0.66\n0.70/0.75\n0.59\n0.72\n0.54\n0.56\n0.13\nlength\n0.53\n0.01\n0.75\n0.81/0.67\n0.71/0.71\n0.54/0.68\n0.33\n0.51\n0.59\n0.52\n0.01\nTable 2: GLUE results for CBC models trained on wikitext-2.\n4.4\nFailure of Competence Based Curriculum\nIn our experiments we were quite surprised at the failure to learn the training data found in our implementation of\ncompetence based curriculum as shown by the high perplexity on the wikitext-* datasets. Based on the changes in\nvalidation perplexity we believe the model is over-ﬁtting on the altered training data. We believe the cause of this is\nour hyperparameter selection for λ0 and λincrement. We realize that since each method is effusively sampling from a\ndifferent training distribution comparison of training perplexities are not comparable. Additionally, if we look at the\ndifference in validation perplexity curves of various methods it is apparent that they are not learning at the same rate.\nSome methods like DEP, and POS do not see major ﬂuctuations indicating the chosen curriculum parameters work well\nwhile many of the n-gram methods consistently ﬂuctuate in a similar fashion indicating the chosen hyperparameters\nare sub optimal for them. Given the non trivial computational cost to explore λ0 and λincrement for each method\n5\nCurriculum Learning for Language Modeling\nMethod\nOverall\nCola\nSST\nMRPC\nSTS-B\nQQP\nMNLI\nQNLI\nRTE\nWNLI\nDX\nbaseline\n0.67\n0.28\n0.86\n0.87/0.80\n0.77/0.77\n0.72/0.76\n0.64\n0.76\n0.61\n0.54\n0.14\ntrigram\n0.66\n0.21\n0.85\n0.87/0.80\n0.78/0.78\n0.75/0.80\n0.66\n0.77\n0.56\n0.55\n0.14\nno curriculum\n0.66\n0.21\n0.83\n0.87/0.8\n0.77/0.77\n0.75/0.79\n0.64\n0.77\n0.58\n0.56\n0.15\nbigram\n0.66\n0.18\n0.83\n0.85/0.79\n0.77/0.77\n0.75/0.79\n0.65\n0.77\n0.56\n0.56\n0.14\nlength\n0.66\n0.21\n0.82\n0.85/0.72\n0.77/0.77\n0.73/0.79\n0.63\n0.75\n0.58\n0.56\n0.14\nunigram\n0.65\n0.19\n0.82\n0.86/0.79\n0.76/0.75\n0.75/0.79\n0.63\n0.75\n0.57\n0.56\n0.13\nrandom\n0.65\n0.18\n0.84\n0.86/0.79\n0.77/0.77\n0.75/0.80\n0.64\n0.77\n0.58\n0.49\n0.14\npos\n0.65\n0.16\n0.83\n0.86/0.79\n0.76/0.76\n0.75/0.79\n0.63\n0.73\n0.57\n0.56\n0.14\ndep\n0.64\n0.23\n0.85\n0.86/0.78\n0.78/0.78\n0.75/0.79\n0.64\n0.76\n0.54\n0.42\n0.14\nTable 3: GLUE results for CBC methods trained on wikitext-103.\nand the disconnect seen between pre-training perplexity and performance on GLUE we did not perform additional\nhyperparameter optimization.\n5\nConclusion and Future Work\nIn our work we do not ﬁnd strong evidence that the use of curriculum learning is able to improve language model\npretraining. Our CBC based training regimes are unable to learn a good representation of the training corpus but their\nrepresentations transfer well to downstream NLP tasks. We ﬁnd that with a small pretraining corpus, CBC methods can\noutperform stochastic sampling but as corpus size scales the beneﬁt is lost. Moreover we do not ﬁnd any evidence that\nany type of heuristic for difﬁculty to be more apt for CBC.\nLeveraging recent work on training BERT in an academic setting [18] we will explore how our top performing methods\nlike trigram and random perform when their code becomes public. Building on promising results in model compression,\n[19] [20] we would like to explore how transfer learning effects compression. Inspired by the structure transformer-\nbased models, we would explore jointly how progressively scaling the number of transformer encoders while increasing\ndata difﬁculty, or context windows as this has proven useful for GANs [21].\nReferences\n[1] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. ArXiv, abs/1802.05365, 2018.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT, 2019.\n[3] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,\nand Hua Wu. Ernie: Enhanced representation through knowledge integration. ArXiv, abs/1904.09223, 2019.\n[4] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in\nnlp. In ACL, 2019.\n[5] Jean Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.\n[6] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, and Tom Michael Mitchell.\nCompetence-based curriculum learning for neural machine translation. ArXiv, abs/1903.09848, 2019.\n[7] Xuan Zhang, Pamela Shapiro, Manish Kumar, P. McNamee, Marine Carpuat, and Kevin Duh. Curriculum learning\nfor domain adaptation in neural machine translation. ArXiv, abs/1905.05816, 2019.\n[8] Gustavo Penha and C. Hauff. Curriculum learning strategies for ir. Advances in Information Retrieval, 12035:699\n– 713, 2020.\n[9] Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Curriculum\nlearning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6095–6104, Online, July 2020. Association for Computational Linguistics.\n[10] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. ArXiv,\nabs/1609.07843, 2016.\n[11] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018.\n6\nCurriculum Learning for Language Modeling\n[12] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, and Tom Mitchell. Competence-\nbased curriculum learning for neural machine translation. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\n[13] J. Elman. Learning and development in neural networks: the importance of starting small. Cognition, 48:71–99,\n1993.\n[14] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In ICML ’09, 2009.\n[15] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and N. Sebe.\nCurriculum learning: A survey.\nArXiv,\nabs/2101.10382, 2021.\n[16] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n3645–3650, Florence, Italy, July 2019. Association for Computational Linguistics.\n[17] Yada Pruksachatkun, Phil Yeres, Haokun Liu, Jason Phang, Phu Mon Htut, Alex Wang, Ian Tenney, and\nSamuel R. Bowman. jiant: A software toolkit for research on general-purpose text understanding models. ArXiv,\nabs/2003.02249, 2020.\n[18] Peter Izsak, Moshe Berchansky, and Omer Levy.\nHow to train bert with an academic budget.\nArXiv,\nabs/2104.07705, 2021.\n[19] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.\narXiv: Learning, 2019.\n[20] Adrian de Wynter and D. Perry. Optimal subarchitecture extraction for bert. ArXiv, abs/2010.10499, 2020.\n[21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,\nstability, and variation. ArXiv, abs/1710.10196, 2017.\n6\nAppendix\n6.1\nCompetence Based Curricula Perplexity Results\nFigure 1: Validation perplexity of each curriculum trained on based wikitext-2 measured every 100 batches.\n7\nCurriculum Learning for Language Modeling\nFigure 2: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches.\nFigure 3: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Unigram,\nbigram, and baseline model performance removed improved interpretation\nFigure 4: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Bigram\nperformance is removed for ease of interpretation.\n8\nCurriculum Learning for Language Modeling\nFigure 5: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Bigram and\nBaseline performance is removed for ease of interpretation.\nFigure 6: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Bigram,\ntrigram, and Baseline performance is removed for ease of interpretation.\nFigure 7: Validation perplexity of each curriculum trained on wikitext-103 measured every 100 batches. Unigram,\nBigram, Trigram and Baseline performance is removed for ease of interpretation.\n9\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-08-04",
  "updated": "2021-08-04"
}