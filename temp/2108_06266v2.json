{
  "id": "http://arxiv.org/abs/2108.06266v2",
  "title": "Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning",
  "authors": [
    "Lukas Brunke",
    "Melissa Greeff",
    "Adam W. Hall",
    "Zhaocong Yuan",
    "Siqi Zhou",
    "Jacopo Panerati",
    "Angela P. Schoellig"
  ],
  "abstract": "The last half-decade has seen a steep rise in the number of contributions on\nsafe learning methods for real-world robotic deployments from both the control\nand reinforcement learning communities. This article provides a concise but\nholistic review of the recent advances made in using machine learning to\nachieve safe decision making under uncertainties, with a focus on unifying the\nlanguage and frameworks used in control theory and reinforcement learning\nresearch. Our review includes: learning-based control approaches that safely\nimprove performance by learning the uncertain dynamics, reinforcement learning\napproaches that encourage safety or robustness, and methods that can formally\ncertify the safety of a learned control policy. As data- and learning-based\nrobot control methods continue to gain traction, researchers must understand\nwhen and how to best leverage them in real-world scenarios where safety is\nimperative, such as when operating in close proximity to humans. We highlight\nsome of the open challenges that will drive the field of robot learning in the\ncoming years, and emphasize the need for realistic physics-based benchmarks to\nfacilitate fair comparisons between control and reinforcement learning\napproaches.",
  "text": "Safe Learning in Robotics:\nFrom Learning-Based\nControl to Safe\nReinforcement Learning\nLukas Brunke∗, Melissa Greeﬀ∗, Adam W.\nHall∗, Zhaocong Yuan∗, Siqi Zhou∗, Jacopo\nPanerati, and Angela P. Schoellig\n∗Equal contribution\nInstitute for Aerospace Studies, University of Toronto, Toronto, Ontario,\nCanada, M3H 5T6; University of Toronto Robotics Institute, Toronto, Ontario,\nCanada, M5S 1A4; Vector Institute for Artiﬁcial Intelligence, Toronto, Ontario,\nCanada, M5G 1M1; emails: {firstname.lastname}@robotics.utias.utoronto.ca\nPublished in the Annual Review of\nControl, Robotics, and Autonomous\nSystems.\nThis unoﬃcial version compiled on:\nDecember 8, 2021.\nKeywords\nsafe learning, robotics, robot learning, learning-based control, safe\nreinforcement learning, adaptive control, robust control, model\npredictive control, machine learning, benchmarks\nAbstract\nThe last half-decade has seen a steep rise in the number of contri-\nbutions on safe learning methods for real-world robotic deployments\nfrom both the control and reinforcement learning communities. This\narticle provides a concise but holistic review of the recent advances\nmade in using machine learning to achieve safe decision making under\nuncertainties, with a focus on unifying the language and frameworks\nused in control theory and reinforcement learning research.\nOur re-\nview includes: learning-based control approaches that safely improve\nperformance by learning the uncertain dynamics, reinforcement learn-\ning approaches that encourage safety or robustness, and methods that\ncan formally certify the safety of a learned control policy.\nAs data-\nand learning-based robot control methods continue to gain traction,\nresearchers must understand when and how to best leverage them in\nreal-world scenarios where safety is imperative, such as when operating\nin close proximity to humans. We highlight some of the open challenges\nthat will drive the ﬁeld of robot learning in the coming years, and em-\nphasize the need for realistic physics-based benchmarks to facilitate fair\ncomparisons between control and reinforcement learning approaches.\n1\narXiv:2108.06266v2  [cs.RO]  6 Dec 2021\nContents\n1. INTRODUCTION ............................................................................................\n2\n2. PRELIMINARIES AND BACKGROUND OF SAFE LEARNING CONTROL...............................\n3\n2.1. Problem Statement .....................................................................................\n4\n2.2. A Control Theory Perspective ..........................................................................\n6\n2.3. A Reinforcement Learning Perspective .................................................................\n8\n2.4. Bridging Control Theory and RL for Safe Learning Control ...........................................\n9\n3. SAFE LEARNING CONTROL APPROACHES..............................................................\n10\n3.1. Learning Uncertain Dynamics to Safely Improve Performance .........................................\n11\n3.2. Encouraging Safety and Robustness in Reinforcement Learning .......................................\n14\n3.3. Certifying Learning-Based Control Under Dynamics Uncertainty ......................................\n18\n4. BENCHMARKS ..............................................................................................\n22\n5. DISCUSSION AND PERSPECTIVES FOR FUTURE DIRECTIONS .......................................\n25\nA. SUMMARY OF REVIEWED LITERATURE ................................................................\n34\nB. REVISION HISTORY ........................................................................................\n36\n1. INTRODUCTION\nRobotics researchers strive to design systems that can operate autonomously in increas-\ningly complex scenarios, often in close proximity to humans. Examples include self-driving\nvehicles (1), aerial delivery (2), and the use of mobile manipulators for service tasks (3).\nHowever, the dynamics of these complex applications are often uncertain or only partially\nknown—for example, the mass distribution of a carried payload might not be given a pri-\nori. Uncertainties arise from various sources. For example, the robot dynamics may not be\nperfectly modeled, sensor measurements may be noisy, and/or the operating environment\nmay not be well-characterized or may include other agents whose dynamics and plans are\nnot known.\nIn these real-world applications, robots must make decisions despite having only partial\nknowledge of the world. In recent years, the research community has multiplied its eﬀorts\nto leverage data-based approaches to address this problem. This was motivated in part by\nthe success of machine learning in other areas such as computer vision and natural language\nprocessing.\nA crucial, domain-speciﬁc challenge of learning for robot control is the need to implement\nand formally guarantee safety of the robot’s behavior, not only for the optimized policy (or\ncontroller, which is essential for the certiﬁcation of systems that interact with humans) but\nalso during learning, to avoid costly hardware failures and improve convergence. Ultimately,\nthese safety guarantees can only be derived from the assumptions and structure captured\nby the problem formalization.\nBoth control theory and machine learning—reinforcement learning (RL) in particular—\nhave proposed approaches to tackle this problem. Control theory has traditionally taken\na model-driven approach (see Figure 1): it leverages a given dynamics model and pro-\nvides guarantees with respect to known operating conditions. RL has traditionally taken\na data-driven approach, which makes it highly adaptable to new contexts at the expense\nof providing formal guarantees. Combining model-driven and data-driven approaches, and\nleveraging the advantages of each, is a promising direction for safe learning in robotics. The\nmethods we review encourage robustness (by accounting for the worst-case scenarios and\ntaking conservative actions), enable adaptation (by learning from online observations and\nadapting to unknown situations), and build and leverage prediction models (based on a\ncombination of domain knowledge, real-world data, and high-ﬁdelity simulators).\nWhile control is still the bedrock of all current robot applications, the body of safe RL\nwww.dynsyslab.org\n\u001f\u001e\u001d\u001c\u001b\n\u001a\u001e\u001b\u0019\u001c\u0018\u0017\u001d\u0016\u0015\u0019\u0014\u0013\u0012\u0011\u0011\u001d\u001e\u0010\u000f\u000e\u0019\r\n\u0017\u0010\f\u0010\u0018\u0017\u001d\u0016\u0015\u0019\u0014\u0013\u0012\u0011\u0011\u001d\u001e\u0010\u000f\u000e\u0019\r\n\u000b\u001e\n\t\u0016\u0014\u0019\u001b\u0013\u0012\u0011\u0011\u001d\u001e\u0010\u000f\u000e\u0019\r\n\u001f\u001e\u001d\u001e\u001c\u001b\u001a\u0019\n\u001a\u001e\u001b\u0019\u001c\n\u0018\u001d\u0017\u0016\u0015\u0014\u0015\u0019\u0013\u0014\u0017\u0017\u0015\u0012\u0014\u0011\u001a\u0015\u0010\u001c\u0015\u001a\u000f\u001e\u0015\u000e\u0010\u0011\u0017\r\u0015\f\u0014\u001d\u0015\u000b\u001e\u0015\u0014\f\f\n\u0011\u0014\u001a\u001e\u0017\u0016\u0015\n\u0013\u0010\r\u001e\u0017\u001e\r\t\u0015\b\u000f\u001e\u0011\u001e\u0015\u001b\u0019\u0015\u0014\u0015\f\u0017\u001e\u0014\u0011\u0015\u000b\u0010\n\u001d\r\u0014\u0011\u0016\u0015\u000b\u001e\u001a\u000e\u001e\u001e\u001d\u0015\u000e\u000f\u0014\u001a\u0015\n\f\u0014\u001d\u0015\u000b\u001e\u0015\u0014\f\f\n\u0011\u0014\u001a\u001e\u0017\u0016\u0015\u0013\u0010\r\u001e\u0017\u001e\r\u0015\u0007\u0014\u001d\r\u0015\u001b\u0019\u0015\u0019\u0014\u001c\u001e\u0006\u0015\u0014\u001d\r\u0015\u000e\u000f\u0014\u001a\u0015\n\f\u0014\u001d\u001d\u0010\u001a\u0015\u000b\u001e\u0015\u0014\f\f\n\u0011\u0014\u001a\u001e\u0017\u0016\u0015\u0013\u0010\r\u001e\u0017\u001e\r\u0015\u0007\u0014\u001d\r\u0015\u001b\u0019\u0015\n\u001d\u0019\u0014\u001c\u001e\u0006\t\n\u0005\u0014\u001c\u001e\n\u0004\u001d\u0019\u0014\u001c\u001e\n\u0005\u001a\u0011\u0010\u001d\u0003\u0015\u0003\n\u0014\u0011\u0014\u001d\u001a\u001e\u001e\u0019\u0015\u000e\u001b\u001a\u000f\u001b\u001d\u0015\u0019\u0012\u001e\f\u001b\u001c\u001b\f\u0015\f\u0010\u001d\u001a\u001e\u0002\u001a\u0019\n\u0001\u001e\u001d\u001e\u0011\u0014\u0017\u001b\u0014\u001a\u001b\u0010\u001d\u0015\u001a\u0010\u0015\u001d\u001e\u000e\u0015\f\u0010\u001d\u001a\u001e\u0002\u001a\u0019\n\u000f\u0014\u0017\u0017\u001e\u001d\u0003\u001e\u0019\n\u001b\u0003\u000f\u0017\u0016\u0015\u0003\u001e\u001d\u001e\u0011\u0014\u0017\u001b\u0014\u000b\u0017\u001e\u0015\u001a\u0010\u0015\u001d\u001e\u000e\u0015\f\u0010\u001d\u001a\u001e\u0002\u001a\u0019\n\u0011\u0010\u001b\r\u001b\u001d\u0003\u0015\u000f\u0014\u0011\r\u0015\u0003\n\u0014\u0011\u0014\u001d\u001a\u001e\u001e\u0019\n\u0001\u001e\u001d\u001e\u0011\u0014\u0017\u001b\u0014\u000b\u0017\u001e\u0015\u0014\u001d\r\u0015\u0019\u0014\u001c\u001e\u0015\u000e\u001b\u001a\u000f\u001b\u001d\u0015\r\u001e\u001c\u001b\u001d\u001e\r\u0015\u000b\u0010\n\u001d\r\u0014\u0011\u001b\u001e\u0019\n\u0005\u0014\u001c\u001e\u0017\u0016\u0015\u0014\u001d\r\u0015\u001e\u001b\f\u001b\u001e\u001d\u001a\u0017\u0016\u0015\u001e\u0002\u0012\u0017\u0010\u0011\u001b\u001d\u0003\u0015\u001a\u000f\u001e\u0015\n\u001d \u001d\u0010\u000e\u001d\u0019\n\b\u0019\u0010\u001d\u0014\u0016\u0014\u0007\n­\u0015\u0013\u0010\r\u001e\u0017\u0015\u0010\u001c\u0015\u001a\u000f\u001e\u0015\u000e\u0010\u0011\u0017\r\u0015\f\u0014\u001d\u0015\u000b\u001e\u0015\u0017\u001e\u0014\u0011\u001d\u001e\r\u0015\u000b\u0016\u0015\f\u0010\u0017\u0017\u001e\f\u001a\u001b\u001d\u0003\u0015\n\r\u0014\u001a\u0014\u0015\u0010\u001e\u0011\u0015\u001a\u001b\u0013\u001e\t\u0015\b\u000f\u001e\u0011\u001e\u0015\u001b\u0019\u0015\u0014\u001e\u0013\f\u0017\u001e\u0014\u0011\u0015\u000b\u0010\n\u001d\r\u0014\u0011\u0016\u0015\u000b\u001e\u001a\u000e\u001e\u001e\u001d\u0015\n\u000e\u000f\u0014\u001a\u0015\f\u0014\u001d\u0015\u000b\u001e\u0015\u0014\f\f\n\u0011\u0014\u001a\u001e\u0017\u0016\u0015\u0013\u0010\r\u001e\u0017\u001e\r\u0015\u0007\u0014\u001d\r\u0015\u001b\u0019\u0015\u0019\u0014\u001c\u001e\u0006\u0015\u0014\u001d\r\u0015\n\u000e\u000f\u0014\u001a\u0015\f\u0014\u001d\u001d\u0010\u001a\u0015\u000b\u001e\u0015\u0014\f\f\n\u0011\u0014\u001a\u001e\u0017\u0016\u0015\u0013\u0010\r\u001e\u0017\u001e\r\u0015\u0007\u0014\u001d\r\u0015\u001b\u0019\u0015\n\u001d\u0019\u0014\u001c\u001e\u0006\t\n\u001a\u001e\u001b\u0019\u001c\u0013\u0006\u0013\b\u0019\u0010\u001d\u0014\u0016\u0014\u0007\n­\u001d\u0015\u0014\f\f\n\u0011\u0014\u001a\u001e\u0015\u0013\u0010\r\u001e\u0017\u0015\u0010\u001c\u0015\u001a\u000f\u001e\u0015\u000e\u0010\u0011\u0017\r\u0015\f\u0014\u001d\u0015\u000b\u001e\u0015\u0017\u001e\u0014\u0011\u001d\u001e\r\u0015\n\u0010\u001e\u0011\u0015\u001a\u001b\u0013\u001e\u0015\u000b\u0016\u0015\f\u0010\u0017\u0017\u001e\f\u001a\u001b\u001d\u0003\u0015\r\u0014\u001a\u0014\t\u0015\b\u000f\u001e\u0015\u0019\u0014\u001c\u001e\u0015\u000b\u0010\n\u001d\r\u0014\u0011\u0016\u0015\n\u001b\u0019\u0015\u000e\u001e\u0017\u0017\r\u001e\u001c\u001b\u001d\u001e\r\u0015\u0014\u001d\r\u0015\u001e\u0002\u001a\u001e\u001d\u0019\u001b\u000b\u0017\u001e\t\n\u0005\u0014\u001c\u001e\u0015\n\u001e\u0003\u001b\u0010\u001d\n\u0014\u001d\u0003\u001e\u0015\u0010\u001c\u0015\u0014\f\f\u001e\u0012\u001a\u0014\u000b\u0017\u001e\u0015\u0011\u001b\u0019 \u0015\u0013\n\u0019\u001a\u0015\n\u000b\u001e\u0015\r\u001e\u001c\u001b\u001d\u001e\r\u0015\u001c\u0010\u0011\u0015\u001e\u0002\u0012\u0017\u0010\u0011\u0014\u001a\u001b\u0010\u001d\t\n\u0005\u0014\u001c\u001e\u0015\n\u001e\u0003\u001b\u0010\u001d\nFigure 1: A comparison of model-driven, data-driven, and combined approaches.\nliterature has ballooned from tens to over a thousand publications in just a few years since\nits most recent review (4). Physics-based simulation (5), which we leverage in our open-\nsource benchmark implementation1, has played an important role in the recent progress of\nRL, however, the transfer to real systems remains a research area in itself (6).\nPrevious review works focused on speciﬁc techniques—for example, learning-based\nmodel predictive control (MPC) (7), iterative learning control (ILC) (8, 9), model-based\nRL (10), data-eﬃcient policy search (11), imitation learning (12), or the use of RL in\nrobotics (13, 14) and in optimal control (15)—without emphasizing the safety aspect.\nRecent surveys on safe learning control focus on either control-theoretic (16) or RL ap-\nproaches (17), and do not provide a unifying perspective.\nIn this article, we provide a bird’s eye view of the most recent work in learning-based\ncontrol and reinforcement learning that implement safety and provide safety guarantees for\nrobot control. We focus on safe learning control approaches where the data generated by\nthe robot system is used to learn or modify the feedback controller (or policy). We hope\nto help shrink the gap between the control and RL communities by creating a common\nvocabulary and introducing benchmarks for algorithm evaluation that can be leveraged by\nboth (18, 19). Our target audience are researchers, with either a control or RL background,\nwho are interested in a concise but holistic perspective on the problem of safe learning\ncontrol. While we do not cover perception, estimation, planning, or multi-agent systems,\nwe do connect our discussion to these additional challenges and opportunities.\n2. PRELIMINARIES AND BACKGROUND OF SAFE LEARNING CONTROL\nIn this review, we are interested in the problem of safe decision making under uncertainties\nusing machine learning (i.e., safe learning control). Intuitively, in safe learning control, our\ngoal is to allow a robot to fulﬁl a task while respecting a set of safety constraints despite\nthe uncertainties present in the problem. In this section, we deﬁne the safe learning control\nproblem (Sec. 2.1) and provide an overview of how the problem of decision making under\nuncertainties has traditionally been tackled by the control (Sec. 2.2) and RL communities\n(Sec. 2.3). We highlight the main limitations of these approaches and articulate how novel\ndata-based, safety-focused methods can address these gaps (Sec. 2.4).\n1Safe control benchmark suite on GitHub: https://github.com/utiasDSL/safe-control-gym\nSafe Learning in Robotics\n3\n\u001f\u001e\u001d\u001e\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u001c\u0015\u0014\u0013\u001b\u0012\u0014\u0011\u0015\u0017\u001e\u0014\u0010\u0018\u0014\u001c\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u0017\u0016\u0015\u001d\n\u0014\u0016\u001d\u0018\u0013\u0012\u001c\u0011\u0019\u001b\u0010\n\u0014\u0016\u0015\u001d\u0018\u001b\u0019\u0017\u0015\u0018\u000f\u000e\u0019\r\f\u001c\u001d\n\u0014\u001c\u001b\u0018\u0017\u000b\u0017\u001c\u0010\u000f\n\u0015\t\f\u0018\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001b\u0017\u001a\u0018\u0016\u0015\u0014\u001b\u0013\u0015\u0012\u0018\u001e\u0015\u0012\u0011\u0016\u0010\u0018\u001c\u0014\u001d\u000f\u0015\u0013\u0018\u001b\u0017\u001a\u0018\u0019\u001c\u000e\u001a\u001b\u0010\u0018\u000e\u0011\u0012\u001b\u001a\u0013\u0018\r\u001c\u0019\u001a\u001d\u0018\u0015\u0014\u0018\u001d\u001c\u001b\u001c\u0018\f\u001a\u0014\u001a\u0013\u001c\u001b\u001a\u001d\u0018\r\u0010\u0018\u001b\u0017\u001a\u0018\u001a\u0014\u000b\u0011\u0013\u0015\u0014\n\u001a\u0014\u001b\n\t\u0015\n\u001e\b\u001b\u001a\u0019\u0018\u0011\u0014\u001e\b\u001b\u0019\u0018\u001a\u0014\u001c\r\u0012\u0011\u0014\f\u0018\n\u001b\u0017\u001a\u0018\u0013\u0015\r\u0015\u001b\u0018\u001b\u0015\u0018\u000e\b\u0012\u000e\u0011\u0012\u0018\u001b\u0017\u001a\u0018\u001b\u001c\u0019\u0007\u0018\n\u0006\u0017\u0011\u0012\u001a\u0018\u0013\u001a\u0019\u001e\u001a\u0016\u001b\u0011\u0014\f\u0018\u0019\u001c\u000e\u001a\u001b\u0010\u0018\n\u0016\u0015\u0014\u0019\u001b\u0013\u001c\u0011\u0014\u001b\u0019\n\u0005\u0015\u001d\u0011\u000e\u0011\u001a\u0019\u0018\u0011\u0014\u001e\b\u001b\u0019\u0018\u0011\u000e\u0018\n\u001e\u001a\u0013\u0016\u001a\u0011\u000b\u001a\u001d\u0018\u001b\u0015\u0018\r\u001a\u0018\b\u0014\u0019\u001c\u000e\u001a\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\n\u001d\u001d\u001d\u001d\u0018\u0017\u001e\u0016\u001e\u0015\u001d\u0014\u0013\u0012\u0012\u0013\u0011\u0010\u000f\u0016\u001d\u001b\u001d\u000e\u0017\u001a\u0010\r\u0017\u000e\u001d\f\u001b\u000b\n\t\n\b\u001e\u001d\u0007\u001b\u0014\u0017\u000b\u0006\u001d\u0005\u0013\u000f\u001a\u000b\r\u001b\u0010\u000f\u000b\u001a\u001d\n\u0004\u001e\u001d\u0007\u0006\u001a\u000b\u0017\u0003\u001d\u0002\u0013\u000e\u0017\u0012\n\u0001\u0006\u000f\u001b\u0003\u0010\u001a\n\u000f\u0017\r\u000b\u001b\u0010\u000f\u000b\u0006\n\u000f\u001e\u0014\u001c\u0017\u001e\u000e\u001b\r\u001e\u000e\u0015\f\u000b\n\n\t\u0018\f\b\u001b\u0007\b\u0006\u001b\u0016\u0014\u0005\u001b\t\u0018\f\b\u001b\u0007\b\u0004\u0003\n\t\u0016\u0002\u0018\u001c\u000b\u001b\u000f\u0018\u0017\u001c\u0015\u0002\u0015\f\u0016\u001c\u0018\u001b\u0016\u0014\u0005\u001b\u0001\u0015\u000e\u001c\u0018\u0017\n\n\t\u0018\f\b\u001b\u0007\b\u0007\u0003\n\n\u0015\t\f\u0018\n\u0016\u001c\u0016\u001b\u0018\u0017\u001b\u001b\u0018\u0016\u0017\u0014\u0015\u0014\u0013\u001b \u000e\u0013\u001e\u0017\u0015\u001c­\u0010\n\u001f\u001e\u001d\u001c\u001e\u001b\u001a\u0019\u0018\u0017\u0016\u0015\u001b\u0014\u001c\u0013\u0016\u0012\n\u001f\u001e\u001d\u001c\u001e\u001b\u0011\u001c\u0018\u0017\u001b\u0010\u000f\u000e\r\u0017\u001d\u001c\u000e\n\u001f\u001e\u001d\u001c\u001e\u001b\u001a\f\u000b\u0016\u0017\u0019\u001b\u0011\u001c\u000e\u0018\u0017\u001e\f\u001d\u000e\u0017\u0018\n\u001f\u001e\u001d\u001c\u001e\u001b\u001a\u0019\u0018\u0017\u0016\u0015\u001b\u0014\u001c\u0013\u0016\u0012\n\u001f\u001e\u0016\u0013\u0016\u000b\u001d\u000e\u0016\u0013\u001b\u001a\f\u000b\u0016\u0017\u0019\u001b\u0011\u001c\u000e\u0018\u0017\u001e\f\u001d\u000e\u0017\u0018\n\t\u0016\u0002\u0018\u001b\u0018\u0016\u0017\u0014\u0015\u0014\u0013\u001b\u000f\u001e\u0014\u001c\u0017\u001e\u000e\u000e\u0018\u0017\n\u0004\u001a\u0010\u0018\t\u0015\n\u001e\u0015\u0014\u001a\u0014\u001b\u0019\n\u0003\u001e\u001b\u0011\u0015\u0014\u001c\u0012\u0018\t\u0015\n\u001e\u0015\u0014\u001a\u0014\u001b\u0019\n\u0002\u0011\f\u0014\u001c\u0012\u0019\nFigure 2: The safe learning control problem is deﬁned by the cost function J, the system\nmodel f, and the constraints c, which may all be initially unknown. Data is used to update\nthe control policy (see Sec. 3.1 and Sec. 3.2) or the safety ﬁlter (see Sec. 3.3).\n2.1. Problem Statement\nWe formulate the safe learning control problem as an optimization problem to capture the\neﬀorts of both the control and RL communities. The optimization problem has three main\ncomponents (see Figure 2): (i) a system model that describes the dynamic behavior of the\nrobot, (ii) a cost function that deﬁnes the control objective or task goal, and (iii) a set of\nconstraints that specify the safety requirements. The goal is to ﬁnd a controller or policy\nthat computes commands (also called inputs) that enable the system to fulﬁl the task while\nrespecting given safety constraints. In general, any of the components could be initially\nunknown or only partially known. Below, we ﬁrst introduce each of the three components\nand conclude by stating the overall safe learning control problem.\nM1. Transition\nProbability Function:\nThe dynamics model\nin Equation 1 can be\nequivalently\nrepresented as a\nstate transition\nprobability func-\ntion Tk(xk+1 | xk, uk),\ncommonly seen in\nRL approaches (20).\n2.1.1. System Model. We consider a robot whose dynamics can be represented by the\nfollowing discrete-time model:\nxk+1 = fk(xk, uk, wk),\n1.\nwhere k ∈Z≥0 is the discrete-time index, xk ∈X is the state with X denoting the state\nspace, uk ∈U is the input with U denoting the input space, fk is the dynamics model of the\nrobot, wk ∈W is the process noise distributed according to a distribution W (see also M1).\nThroughout this review, we assume direct access to (possibly noisy) measurements of the\nrobot state xk and neglect the problem of state estimation. Equation 1 represents many\ncommon robot platforms (e.g., quadrotors, manipulators, and ground vehicles). More com-\nplex models (e.g., partial diﬀerential equations) may be necessary for other robot designs.\n2.1.2. Cost Function. The robot’s task is deﬁned by a cost function. We consider a ﬁnite-\nhorizon optimal control problem with time horizon N. Given an initial state x0, the cost\nis computed based on the sequence of states x0:N = {x0, x1, ..., xN} and the sequence of\ninputs u0:N−1 = {u0, u1, ..., uN−1}:\nJ(x0:N, u0:N−1) = lN(xN) +\nN−1\nX\nk=0\nlk(xk, uk),\n2.\nwww.dynsyslab.org\nwhere lk : X × U 7→R is the stage cost incurred at each time step k, and lN : X 7→R is\nthe terminal cost incurred at the end of the N-step horizon (see also M2). The stage and\nM2. Discounted\nRewards: The stage\ncost can be related\nto the discounted\nrewards in RL:\nlk = −γkrk(xk, uk),\nwhere\nrk : X × U 7→R is\nthe reward function,\nand γ ∈[0, 1] is the\ndiscount factor (20).\nWe formulate a cost\nminimization\nproblem for safe\nlearning control,\nwhile RL typically\nsolves a reward\nmaximization\nproblem.\nterminal cost functions map the state and input sequences, which may be random variables,\nto a real number, and may, for example, include the expected value or variance operators.\n2.1.3. Safety Constraints. Safety constraints ensure, or encourage, the safe operation of the\nrobot and include: (i) state constraints Xc ⊆X, which deﬁne the set of safe operating states\n(e.g., the lane in self-driving), (ii) input constraints Uc ⊆U (e.g., actuation limits), and\n(iii) stability guarantees (e.g., the robot’s motion converging to a desired path, see M3).\nM3. Stability: There\nare diﬀerent notions\nof stability in the\ncontrol\nliterature (21).\nStability generally\nimplies boundedness\nof the system state.\nOne common notion\nof stability is\nLyapunov stability,\nwhich requires the\nsystem to either stay\nclose to (stable) or\nconverge to\n(asymptotically\nstable) a desired\nstate.\nTo encode the safety constraints, we deﬁne nc constraint functions: ck(xk, uk, wk) ∈\nRnc with each constraint cj\nk being a real-valued, time-varying function.\nStarting with\nthe strongest guarantee, we introduce three levels of safety: hard, probabilistic, and soft\nconstraints (illustrated in Figure 3). In practice, safety levels are often mixed. For example,\ninput constraints are typically hard constraints but state constraints may be soft constraints.\nSafety Level III: Constraint Satisfaction Guaranteed. The system satisﬁes hard con-\nstraints:\ncj\nk(xk, uk, wk) ≤0 ,\n3.\nfor all times k ∈{0, ..., N} and constraint indexes j ∈{1, ..., nc}.\nSafety Level II: Constraint Satisfaction with Probability p. The system satisﬁes prob-\nabilistic constraints:\nPr\n\u0010\ncj\nk (xk, uk, wk) ≤0\n\u0011\n≥pj ,\n4.\nwhere Pr(·) denotes the probability and pj ∈(0, 1) deﬁnes the likelihood of the j-th con-\nstraint being satisﬁed, with j ∈{1, ..., nc} and for all times k ∈{0, ..., N}. The chance\nconstraint in Equation 4 is identical to the hard constraint in Equation 3 for pj = 1.\nSafety Level I: Constraint Satisfaction Encouraged. The system encourages con-\nstraint satisfaction. This can be achieved in diﬀerent ways. One way is to add a penalty\nterm to the objective function that discourages the violation of constraints with a high cost.\nA non-negative ϵj is added to the right-hand side of the inequality Equation 3, for all times\nk ∈{0, ..., N} and j ∈{1, ..., nc},\ncj\nk(xk, uk, wk) ≤ϵj ,\n5.\nand an appropriate penalty term lϵ(ϵ) ≥0 with lϵ(ϵ) = 0\n⇐⇒\nϵ = 0 is added to the\nobjective function. The vector ϵ includes all elements ϵj and is an additional variable of the\noptimization problem. Alternatively, although cj\nk(xk, uk, wk) is a step-wise quantity, some\napproaches only aim to provide guarantees on its expected value E[·] on a trajectory level:\nJcj = E\n\"N−1\nX\nk=0\ncj\nk (xk, uk, wk)\n#\n≤dj ,\n6.\nwhere Jcj represents the expected total constraint cost, and dj deﬁnes the correspond-\ning constraint threshold.\nThe constraint function can optionally be discounted as\nγkcj\nk (xk, uk, wk), similar to the stage cost (see M2).\n2.1.4. Formulation of the Safe Learning Control Problem. The functions introduced above,\ni.e., the system model f, the constraints c, and the cost function J, represent the true\nfunctions of the robot control problem. In practice, f, c, and J may be unknown or partially\nSafe Learning in Robotics\n5\n\u001f\u001e\u001d\u001c\u001b\u001e\u001a\u0019\u0018\u001b\u001e\u0017\u0016\u001d\u0015\u001b\u0018\u0014\n\u0013\u001b\u0012\u0014\u001d\u0011\u0010\u001e\u000f\u0019\u000f\u001b\u001a\u001b\u0018\u000e\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u001e\u0018\u0017\u0019\u0016\u0017\u0015\u001b\n\u0014\u0013\u001b\u001d\u001c\u0017\u001b\u0012\u0011\u0014\u0011\u001d\n\u0010\u000f\u0016\u001d\u0019\u000f\u0014\u000e\u001d\u000f\u0011\r\u001b\u0011\f\u001b\n\u001f\u0011\u0016\u0016\u000f\u0014\u000b\u0017\u001b\u001f\u001e\u001d\u001c\u0016\u001b\n\u001d\u001c\u0017\u001b\u0012\u0011\u0014\u0011\u001d\u001b\n\u0011\u000e\u000b\u0015\u001b\n\u001a\u0019\u001e\u0018\u0017\u0019\u0016\u0017\n\u001f\u001e\u001d\u001c\u001b\u001a\u001e\u0019\u0018\u001c\u0017\u0016\u0015\u0019\u001c\u0018\u001b\n\r\u0019\f\u000b\u0018\u000e\u001d\n\u000b\t\u000b\u001a\u001d\b \n\u0014\u0017\u001e\u0013\u0016\u0013\u0015\u0012\u0015\u0018\u001c\u0015\u0011\u001b\u001a\u001e\u0019\u0018\u001c\u0017\u0016\u0015\u0019\u001c\u0018\n\r\u0019\f\u000b\u0018\u000e\u001d\n\u000b\t\u000b\u001a\u001d\b\b \nGOAL\n\u0010\u0016\u0017\u000f\u001b\u001a\u001e\u0019\u0018\u001c\u0017\u0016\u0015\u0019\u001c\u0018\n\r\u0019\f\u000b\u0018\u000e\u001d\n\u000b\t\u000b\u001a\u001d\b\b\b\u001d \n\u001f\u001e\u001d\u001c\u001b\u001e\u001a\u0019\u0018\u001b\u001e\u0017\u0016\n\u0011\u001e\u0016\u0016\u001b\u000f\u001a\u000b\u001d\u0007\u001b\u0017\u001b\u0006\u0019\u001a\n\u001c\u001b\u001e\u001a\u0019\u0018\u001b\u001e\u0017\u0016\nGOAL\nGOAL\nFigure 3: Illustration of the diﬀerent safety levels.\nknown. Without loss of generality, we assume that each of the true functions f, c, and J\ncan be decomposed into a nominal component ¯(·), reﬂecting our prior knowledge, and an\nunknown component ˆ(·), to be learned from data. For instance, the dynamics model f can\nbe decomposed as\nfk(xk, uk, wk) = ¯fk(xk, uk) + ˆfk(xk, uk, wk),\n7.\nwhere ¯f is the prior dynamics model and ˆf are the uncertain dynamics.\nSafe learning control (SLC) leverages our prior knowledge P = {¯f, ¯c, ¯J} and the data\ncollected from the system D = {x(i), u(i), c(i), l(i)}i=D\ni=0 to ﬁnd a policy (or controller) πk(xk)\nthat achieves the given task while respecting all safety constraints,\nSLC : (P, D) 7→πk,\n8.\nwhere (·)(i) denotes a sample of a quantity (·)k and D is the data set size. More speciﬁcally,\nwe aim to ﬁnd a policy πk that best approximates the true optimal policy π⋆\nk, which is the\nsolution to the following optimization problem:\nJπ⋆(¯x0) =\nmin\nπ0:N−1,ϵ\nJ(x0:N, u0:N−1) + lϵ(ϵ)\n9a.\ns.t.\nxk+1 = fk(xk, uk, wk),\nwk ∼W,\n∀k ∈{0, ..., N −1},\n9b.\nuk = πk(xk),\n9c.\nx0 = ¯x0,\n9d.\nSafety constraints according to either\n9e.\nEquation 3, Equation 4, Equation 5 or Equation 6, and ϵ ≥0,\nwhere ¯x0 ∼X0 is the initial state with X0 being the initial state distribution, and ϵ and lϵ\nare introduced to account for the soft safety constraint case (Safety Level I, Equation 5) and\nare set to zero, for example, if only hard and probabilistic safety constraints are considered\n(Safety Levels II and III).\n2.2. A Control Theory Perspective\nSafe decision making under uncertainty has a long history in the ﬁeld of control theory.\nTypical assumptions are that a model of the system is available and it is either parameterized\nby an unknown parameter or it has bounded unknown dynamics and noise.\nWhile the\ncontrol approaches are commonly formulated using continuous-time dynamic models, they\nare usually implemented in discrete time with sampled inputs and measurements (see M4).\nM4. Continuous\nTime:\nMany\nclassical control\napproaches rely on a\ncontinuous-time\nformulation.\nConsider the\nuncertain\ntime-varying,\ncontinuous-time\nmodel\n˙x = ft(x, u, w),\nwhere x, u and w\nare functions of\ntime t. We can\nrecover our discrete\nstate through\nsampling\nxk = x(k∆t), where\n∆t is the sampling\ntime. Similarly, the\ndiscrete control\ninput is\nuk = u(k∆t) and\noften kept\nconstant (22) over\nthe time interval\n[k∆t, (k + 1)∆t).\nAdaptive control considers systems with parametric uncertainties (see M5) and adapts\nthe controller parameters online to optimize performance. Adaptive control requires knowl-\nedge of the parametric form of the uncertainty (23) and typically considers a dynamics\nwww.dynsyslab.org\nmodel that is aﬃne in u and the uncertain parameters θ ∈Θ:\nxk+1 = ¯fx(xk) + ¯fu(xk)uk + ¯fθ(xk)θ ,\n10.\nwhere ¯fx, ¯fu, and ¯fθ are known functions often derived from ﬁrst principles and Θ is a pos-\nsibly bounded parameter set. The control input is uk = π(xk, ˆθk), which is parameterized\nby ˆθk. The parameter ˆθk is adapted by using either a Lyapunov function to guarantee that\nthe closed-loop system is stable (see M6) or Model Reference Adaptive Control (MRAC)\nto make the system behave as a predeﬁned stable reference model (23). Adaptive control\nis typically limited to parametric uncertainties and relies on a speciﬁc model structure.\nMoreover, adaptive control approaches tend to “overﬁt” to the latest observations and con-\nvergence to the true parameters is generally not guaranteed (23, 24). These limitations\nmotivate the learning-based adaptive control approaches in Sec. 3.1.1.\nM5. Parametric\nUncertainty: A\nparametric model\ndepends on a ﬁnite\nnumber of\nparameters that may\nhave a physical\ninterpretation or\nreﬂect our prior\nknowledge about the\nsystem structure in\nother ways.\nParametric\nuncertainty is the\nuncertainty in the\nmodel parameters.\nRobust control is a control design technique that guarantees stability for pre-speciﬁed\nbounded disturbances, which can include unknown dynamics and noise.\nIn contrast to\nadaptive control, which adapts to the parameters currently present, robust control ﬁnds\na suitable controller for all possible disturbances and keeps the controller unchanged after\nthe initial design. Robust control is largely limited to linear, time-invariant (LTI) systems\nwith linear nominal model ¯f(xk, uk) = ¯Axk + ¯Buk and unknown dynamics ˆfk(xk, uk, wk) =\nˆAxk + ˆBuk + wk ∈D, with D being known and bounded, and ¯A, ¯B, ˆA, ˆB being static\nmatrices of appropriate size. That is,\nxk+1 = ( ¯A + ˆA)xk + ( ¯B + ˆB)uk + wk .\n11.\nRobust control design techniques, such as robust H∞- and H2-control design (25), yield\ncontrollers that are robustly stable for all ˆfk ∈D.\nRobust control can be extended to\nnonlinear systems whose dynamics can be decomposed into a linear nominal model ¯f and a\nnonlinear function ˆf with known bound ˆf ∈D (26).\nRobust Model Predictive Control (MPC) extends classical adaptive and robust\ncontrol by additionally guaranteeing state and input constraints, xk ∈Xc and uk ∈Uc, for\nall possible bounded disturbances ˆf ∈D. At every time step k, MPC solves a constrained\noptimization problem over a control input sequence uk:k+H−1 for a ﬁnite horizon H, and\napplies the ﬁrst optimal control input to the system and then re-solves the optimization\nproblem in the next time step based on the current state (27). A common approach in\nM6. Lyapunov\nFunction:\nLyapunov\nfunctions are used to\nanalyze the stability\nof a dynamical\nsystem (21).\nConsider a\nclosed-loop system\nunder some\npolicy π(x):\nxk+1 = fπ(xk) =\nf(xk, π(xk)) with\nfπ(xk) being\nLipschitz\ncontinuous. A\nLipschitz continuous\npositive deﬁnite\nfunction\nL : X →R≥0 with\nL(0) = 0 and\nL(x) > 0, ∀x ̸= 0, is\na Lyapunov\nfunction, if L maps\nstates under\nclosed-loop state\nfeedback to strictly\nsmaller values (i.e.,\n∆L(x) = L(fπ(x)) −\nL(x) < 0). This\nimplies that the\nstate converges to\nthe equilibrium at\nthe origin. There\nexists an analogous\nformulation for\ncontinuous-time\nsystems based on the\nderivatives of L (21).\nrobust MPC (RMPC) is tube-based MPC (28), which uses a nominal prediction model\n¯f(xk, uk) in the MPC optimization and tightens the constraints to account for unmodeled\ndynamics.\nA stabilizing controller keeps the true state inside a bounded set of states\naround the nominal state, called tube, for all possible disturbances.\nSince the nominal\nstates satisfy the tightened constraints and the true states stay inside the tube around the\nnominal states, constraint satisfaction for the true states is guaranteed. Tube-based MPC\ntypically considers a linear nominal model ¯f(¯xk, ¯uk) = ¯A¯xk + ¯B¯uk with nominal state ¯xk\nand input ¯uk. In its simplest implementation, prior knowledge of set D is combined with\na known stabilizing linear controller, uk,stab = K(xk −¯xk) with gain K, to determine the\nbounded tube Ωtube from the matrices ¯A, ¯B, K, and the set D. The stabilizing controller\nuk,stab keeps all potential errors within the tube, xk −¯xk ∈Ωtube, for all k.\nFor the\nnominal model, tube-based MPC solves the following constrained optimization problem at\nevery time step k to obtain the optimal sequence ¯u∗\n0:H−1:\nJ∗\nRMPC(¯xk) =\nmin\n¯u0:H−1\nlH(zH) +\nH−1\nX\ni=0\nli(zi, ¯ui)\n12a.\ns.t.\nzi+1 = ¯Azi + ¯B¯ui,\n∀i ∈{0, ..., H −1}\n12b.\nzi ∈Xc ⊖Ωtube,\n¯ui ∈Uc ⊖KΩtube,\n12c.\nz0 = ¯xk,\nzH ∈Xterm ,\n12d.\nSafe Learning in Robotics\n7\nwhere zi is the open-loop nominal state at time step k+i and the state and input constraints\nare tightened using the bounded tube Ωtube (see M7).Combined with the stabilizing control\ninput uk,stab, the control input uk = uk,stab + ¯u∗\n0 is applied to the system at every time\nstep. Stability and satisfaction of the tightened constraints is guaranteed by selecting the\nterminal cost lH in Equation 12a, such that after the prediction horizon H the nominal state\nzH is within a terminal constraint set Xterm (see Equation 12d) at which point a known\nlinear controller can be safely applied (27).\nM7. Constraint\nTightening:\nThe\nconstraint sets Xc\nand Uc are tightened\nby an error tube\nΩtube using the\nPontryagin\ndiﬀerence,\nXc ⊖Ωtube = {x ∈\nX : x + ω ∈Xc, ∀ω ∈\nΩtube} and\nUc ⊖KΩtube, where\nK maps all elements\nin Ωtube to the\ncontrol input\nspace U with\nKΩtube.\nBoth robust control and robust MPC are conservative, as they guarantee stability and—\nin the case of MPC—state and input constraints for the worst-case scenario. This usually\nyields poor performance, as described in (7). For example, a conservative uncertainty set\nD generates a large tube Ωtube resulting in tight hard constraints that are prioritized over\ncost optimization. Learning-based robust control and RMPC improve performance by using\ndata to (i) learn a less conservative state- and input-dependent uncertainty set D and/or\n(ii) learn the unknown dynamics ˆf and, as a result, reduce the remaining model uncertainty,\nsee Sec. 3.1.2 and Sec. 3.1.3 respectively.\n2.3. A Reinforcement Learning Perspective\nReinforcement learning (RL) is the standard machine learning framework to address the\nproblem of sequential decision making under uncertainty. Unlike traditional control, RL\ngenerally does not rely on an a priori dynamics model ¯f and can be directly applied to\nuncertain dynamics f. However, the lack of explicit assumptions and constraints in many\nof the works limit their applicability to safe control. RL algorithms attempt to ﬁnd π∗\nwhile gathering data and knowledge of f from interaction with the system—taking ran-\ndom actions, initially, and improving afterwards. A long-standing challenge of RL, which\nhampers safety during the learning stages, is the exploration-exploitation dilemma—that\nis, whether to (i) act greedily with the available data or to (ii) explore, which means taking\nsub-optimal—possibly unsafe—actions u to learn a more accurate ˆf.\nRL typically assumes that the underlying control problem is a Markov decision pro-\ncess (MDP). An MDP comprises a state space X, an input (action) space U, stochastic\ndynamics (also called transition model, see M1), and a per-step reward function (see M2).\nWhen all the components of an MDP are known (in particular, f and J in Sec. 2.1), then\nit solves the problem in Equations 9a, 9b, 9c without the constraints. Dynamic program-\nming (DP) algorithms such as value and policy iteration can be used to ﬁnd an optimal\npolicy π∗. Many RL approaches, however, make no assumptions on any part of f being\nknown a priori.\nM8. Value and\nAction-Value\nFunctions:\nThe\nvalue function of\nstate xk under\npolicy πk, V πk(xk),\nis the expected\nreturn J of applying\nπk from xk.\nSimilarly, the\naction-value function\nof action uk in state\nxk under πk,\nQπk(xk, uk), is the\nexpected return J\nwhen taking the\naction uk at xk, and\nthen following πk.\nWe can distinguish model-based RL approaches, which learn an explicit model ˆf of the\nsystem dynamics f and use it to optimize a policy, from model-free RL algorithms. The\nlatter algorithms (29) can be broadly categorized as: (i) value function-based methods,\nlearning a value function (see M8); (ii) policy-search and policy-gradient methods, directly\ntrying to ﬁnd an optimal policy π∗; and (iii) actor-critic methods, learning both a value\nfunction (critic) and the policy (actor). We also note that the convergence of these methods\nhas been shown for simple scenarios but is still a challenge for more complex scenarios or\nwhen function approximators are used (30).\nThere are multiple practical hurdles to the deployment of RL algorithms in real-world\nrobotics problems (6), for example, (i) the continuous, possibly high-dimensional X and U in\nrobotics (often assumed to be ﬁnite, discrete sets in RL), (ii) the stability and convergence\nof the learning algorithm (31) (necessary, albeit not suﬃcient, to produce a stable policy,\nsee M3), (iii) learning robust policies from limited samples, (iv) the interpretability of the\nlearned policy—especially in deep RL when leveraging neural networks (NNs) (see M9) for\nfunction approximation (29)—and, importantly, (v) providing provable safety guarantees.\nM9. Neural\nNetwork:\nA neural\nnetwork (NN) is a\ncomputational\nmodel with\ninterconnected layers\nof neurons\n(parameterized by\nweights) that can be\nused to approximate\nhighly nonlinear\nfunctions.\nThe exploration-exploitation dilemma can be mitigated using Bayesian inference. This\nwww.dynsyslab.org\nis achieved by computing posterior distributions over the states X, possible dynamics f, or\ntotal cost J from past observed data (32). These posteriors provide explicit knowledge of\nthe problem’s uncertainty and can be used to guide exploration. In practice, however, full\nposterior inference is almost always prohibitively expensive and concrete implementations\nmust rely on approximations.\nTo achieve constraint satisfaction (over states or sequences of states) and robustness to\ndiﬀerent, possibly noisy dynamics f, constrained MDPs and robust MDPs are extensions\nof traditional MDPs that more closely resemble the problem statement in Sec. 2.1.\nConstrained MDPs (CMDPs) (33) extend simple MDPs with constraints and opti-\nmize the problem in Equation 9 when Equation 9e takes the discounted form of Safety Level\nI ’s Equation 6. We refer to the discounted constraint cost Jcj under policy π as Jπ\ncj. Tradi-\ntional approaches to solve CMDPs, such as Linear Programming and Lagrangian methods,\noften assume discrete state-action spaces and cannot properly scale to complex tasks such\nas robot control. Deep RL promises to mitigate this, yet, applying it to the constrained\nproblem still suﬀers from the computational complexity of the oﬀ-policy evaluation (M10)\nof trajectory-level constraints Jcj (34). In Sec. 3.2.3, we present some recent advances in\nCMDP-based work that feature: (i) integration of deep learning techniques in CMDPs for\nmore complex control tasks; (ii) provable constraint satisfaction throughout the exploration\nor learning process; and (iii) constraint transformation for the eﬃcient evaluation of Jcj\nfrom data collected oﬀ-policy.\nM10. Oﬀ-policy,\nOn-policy, and\nOﬄine RL:\nOﬀ-policy RL\nimproves the value\nfunctions estimates\nwith data collected\noperating under\ndiﬀerent policies.\nOn-policy methods\nonly use data\ngenerated by the\ncurrent policy.\nOﬄine RL uses data\npreviously collected\nby an unknown\npolicy.\nRobust MDPs (35), inspired by robust control (see Sec. 2.2), extend MDPs such that\nthe dynamics can include parametric uncertainties or disturbances, and the cost of the\nworst-case scenario is optimized. This is captured by the min-max optimization problem:\nJπ(¯x0) =\nmin\nπ0:N−1 max\nˆf∈D\nJ(x0:N, u0:N−1)\n13a.\ns.t.\nEquation 9b, Equation 9c, Equation 9d,\n13b.\nwhere D is a given uncertainty set of ˆf. To keep solutions tractable, practical implemen-\ntations typically restrict D to certain classes of models. This can limit the applicability of\nrobust MDPs beyond toy problems. Recent work (36, 37, 38) applied deep RL to robust de-\ncision making, targeting key theoretical and practical hurdles such as (i) how to eﬀectively\nmodel uncertainty with deep neural networks, and (ii) how to eﬃciently solve the min-max\noptimization (e.g., via sampling or two-player, game-theoretic formulations). These ideas,\nincluding adversarial RL and domain randomization, are presented in Sec. 3.2.4.\n2.4. Bridging Control Theory and RL for Safe Learning Control\nWhen designing a learning-based controller, we typically have two sources of information:\n(i) our prior knowledge, and (ii) data generated by the robot system. Control approaches\nrely on prior knowledge and on assumptions such as parametric dynamics models to pro-\nvide safety guarantees. RL approaches typically make use of expressive learning models\nto extract patterns from data that facilitate learning complex tasks, but these can impede\nthe provision of any formal guarantees. In recent literature, we see an eﬀort from both the\ncontrol and RL communities to develop safe learning control algorithms, with the goal of\nsystematically leveraging expressive models for closed-loop control (see Figure 4). Ques-\ntions that arise from these eﬀorts include: how can control-theoretic tools be applied to\nexpressive machine learning models? Or, how can expressive models be incorporated into\ncontrol frameworks?\nM11. Gaussian\nProcess:\nA\nGaussian process\n(GP) is a set of\nrandom variables in\nwhich any ﬁnite\nnumber of the\nrandom variables\ncan be modeled as a\njoint Gaussian\ndistribution.\nPractically, a GP is\na probabilistic model\nspecifying a\ndistribution over\nfunctions.\nExpressive learning models can be categorized as deterministic (e.g., standard deep\nneural networks (DNNs)) or probabilistic (e.g., Gaussian processes (GPs), see M11, and\nBayesian linear regression (BLR)). Deep learning techniques such as feed-forward NNs,\nconvolutional NNs, and long short-term memory networks have the advantage of being able\nSafe Learning in Robotics\n9\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u001a\u0018\u0017\u0016\u0016\u0015\n\u0014\u001e\u0013\u0017\u001d\u0012\u001e\u0018\u0015\u0017\u0019\u001a\u0011\u0018\u0017\u001d\n\u0014\u001a\u0017\u0011\u0015\u0013\u001a\u0010\u0017\u0011\u001e\u0018\n\u000f\u0014\u001a\u0013\u0016\u0017\u000e\u001d\r\u0016\f\u0016\u000b\u001d\n\t\n\b\u0019\u001e\u0007\u001a\u0007\u0011\u000b\u0011\u0015\u0017\u0011\u0010\u001d\u0012\u001e\u0018\u0015\u0017\u0019\u001a\u0011\u0018\u0017\u001d\n\u0014\u001a\u0017\u0011\u0015\u0013\u001a\u0010\u0017\u0011\u001e\u0018\n\u000f\u0014\u001a\u0013\u0016\u0017\u000e\u001d\r\u0016\f\u0016\u000b\u001d\n\n\t\n\u0006\u001a\u0019\u0005\u001d\u0012\u001e\u0018\u0015\u0017\u0019\u001a\u0011\u0018\u0017\u001d\n\u0014\u001a\u0017\u0011\u0015\u0013\u001a\u0010\u0017\u0011\u001e\u0018\n\u000f\u0014\u001a\u0013\u0016\u0017\u000e\u001d\r\u0016\f\u0016\u000b\u001d\n\n\n\t\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\u0017\u0016\u0015\u001a\u0014\u001b\u0013\u0012\u0016\n\u0011\u0010\u001a\u001c\u001a\u001e\u0013\u001b\u001b\u0019\n\u001f\u001e\u001d\u001c\u001b\u001a\u0019\u0018\u001e\u0017\u0016\n\u000f\u001b\u000e\u0018\u001a\u001e\u001d\u001b\u0016\r\u001e\u0016\n\f\u001a\u0013\u001a\n\b\u0019\u0011\u001e\u0019\u001d\r\u0011\u0018\u0016\u001a\u0019\n\u0004\u000e\u0018\u001a\u0003\u0011\u0010\u0015\n\b\u0019\u0011\u001e\u0019\u001d\u0012\u001e\u0018\u0017\u0019\u001e\u000b\u0002\n\u0001\u0011\u0018\u0016\u001d\u0004\u000e\u0018\u001a\u0003\u0011\u0010\u0015\n\b\u0019\u0011\u001e\u0019\u001d\u0014\u0017\u0019\u001b\u0010\u0017\u001b\u0019\u0016\u0005\n\u001f\u001e\u0018\u000b\u0011\u0018\u0016\u001a\u0019\u001d\u0004\u000e\u0018\u001a\u0003\u0011\u0010\u0015\n\u0018\u0018\u001e\u0018\n\u0004\u000e\u0018\u001a\u0003\u0011\u0010\u0015\n\u0018\u001e\u0018\n\u0004\u000e\u0018\u001a\u0003\u0011\u0010\u0015\n\u001f\u001e\u001d\u001c\u001b\u001d\u001a\u001b\n\u0019\u0018\u001c\u001e\u001a\u0018\u0017\u0016\n\u0015\u0014\u0014\u001a\u0018\u001d\u0013\u0012\u0011\u0010\n\u001f\u001e\u001d\u001c\u001b\u001d\u001a\u001b\u0016\u000f\u0011\u000e\u001c\r\u0018\u001a\u0013\u0011\f\u0011\u001c\u001e\u0016\u000b\u0011\u001d\u001a\u001c\u000e\u001c\n\u000f\u0011\u000e\u001c\r\u0018\u001a\u0013\u0011\f\u0011\u001c\u001e\u0016\u000b\u0011\u001d\u001a\u001c\u000e\u001c\n\u0016\t\u001c\u0013\u0018\b\u001a\u001d\n\u000e\u001c\n\u0016\u001f\u001d\r\u0011\u001e\u0007\u0016\u001d\u001c\u001b\u0016\u000f\u0018\u0006\b\u0010\u001e\u001c\u0011\u0010\u0010\n\u0015\u001a\u0014\u001b\u0016\u000b\n\t\u000e\r\u001c\u001a\u0013\u0018\r\u001e\u0016\u001a\u001e\b\u0016\u0007\t\u0013\u0018\u0006\u0018\u0005\u001a\u0013\u0018\r\u001e\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0001\u0003\n\u000f\u0018\u0019\u001b\u001c\u0019\u001b\u0016\u001a\u001e\b\u0016 \u001e\u001d\u001b\u001c\u0013\u001a\u0018\u001e\u0013\u0012­\u001a\u001c\u001b\u0016\u000f\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0001\u0003\u0001\n\r\u001e\u0019\u0013\u001c\u001a\u0018\u001e\u001b\b\u0016\f\u0019\u0016\u001a\u001e\b\u0016\u000f\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0001\u0003\u0002\n\u000f\r\u0010\u0019\u0013\u0016\f\u0019\u0016\u001a\u001e\b\u0016\u000f\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0001\u0003\n\u001f\u001d\r\u0011\u001e\u0007\n\u0019\u0011\u001a\u001e\u000e\r\u000e\u0013\u001d\u001e\u000e\u0018\u001c\n\u0015\u0013\u001a\u0018\u000e\u0018\u0013\u0012\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0002\u0003\n\r\u001e\u0019\u0013\u001c\u001a\u0018\u001e\u0013\u0016\u0015\u001b\u0013\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0002\u0003\u0001\n\u001f\u001d\r\u0011\u0017\u0007\u0016\u000b\u0011\u001d\u001a\u001c\u000e\u001c\n\u0016\n\u0005\u001c\u0013\u0011\u001a\u001e\u001d\u000e\u001c\u0016\u0004\u0007\u001c\u001d\f\u000e\u0013\u0010\n\u001b\u001a\u001c\u001e\u0018\u001e\u0017\u0016\b\u001a\t\u0013\u0018\u001b\u0016\r\u001e\u0013\u001c\r\u000e\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0003\n\u001b\u001a\u001c\u001e\u0018\u001e\u0017\u0016\u000f\r\u0010\u0019\u0013\u0016\r\u001e\u0013\u001c\r\u000e\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0003\u0001\u0016\n\u001b\u001a\u001c\u001e\u0018\u001e\u0017\u0016\u000f\r\u0010\u0019\u0013\u0016\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0003\u0002\n\u0015\u001a\u0014\u001b\u0016\r\b\u001b\u000e\u001a\u0019\u001b\b\u0016\u000f\u0016\u0004\u0015\u001b\u001d\u0003\u0016\u0002\u0003\u0003\n\u001f\u001e\u001d\u001c\u001b\u001a\u001a\u0019\u0018\u001b\u0017\u0016\u0015\u0014\u001b\u0013\u0017\u0012\u0019\u0011\u0010\n\u000f\u0011\u001c\u0015\u000e\r\u0017\u000f\f\u000b\u001b\u0011\n\u0017\t\b\f\u001c\f\u000e\u0011\u001b\u001b\u001a\n\u0003\u0002\u0015\u000b\n\b\u0019\u0011\u001e\u0019\u001d\u001c\u0016\u0018\u0016\u0019\u0011\u0010\n\u001f\u001e\u0018\u000b\u0011\u0018\u0016\u001a\u0019\u001d\u0004\u000e\u0018\u001a\u0003\u0011\u0010\u0015\n\u0007\u0006\u001d\u001b\u001c\u000b\u001b\u0005\u0011\u0017\u0004\u001c\u0019\u0015\u001c\u0017\u0003\u000e\u0015\u0012\u0013\u001b\u0014\r\u001b\u0002\u0016\u0015\u0014\u001b\u0013\n\u0001\u0019\u001b\u0017\n\u000e\f\u0006\u0019\u0005\u001a\u0017\u000e\u0005\u001b\u001c\u0011\f\u0019\u000e\u0011\n\nFigure 4: A summary of the safe learning control approaches reviewed in Sec. 3.\nto abstract large volumes of data, enabling real-time execution in a control loop. On the\nother hand, their probabilistic counterparts, such as GPs and BLRs, provide model output\nuncertainty estimates that can be naturally blended into traditional adaptive and robust\ncontrol frameworks. We note that there are approaches aiming to combine the advantages of\nthe two types of learning (e.g., Bayesian NNs), and quantifying uncertainty in deep learning\nis still an active research direction.\nIn this review, we focus on approaches that address the problem of safe learning control\nat two stages: (i) online adaptation or learning, where online data is used to adjust the\nparameters of the controller, the robot dynamics model, the cost function, or the constraint\nfunctions during closed-loop operation; and (ii) oﬄine learning, where data collected from\neach trial is recorded and used to update a model in a batch manner in between trials\nof closed-loop operation.\nIn safe learning control, data is generally used to address the\nissue of uncertainties in the problem formulation and reduce the conservatism in the system\ndesign, while the safety aspect boils down to “knowing what is not known” and cautiously\naccounting for the incomplete knowledge via algorithm design.\n3. SAFE LEARNING CONTROL APPROACHES\nThe ability to guarantee safe robot control is inevitably dependent on the amount of prior\nknowledge available and the type of uncertainties present in the problem of interest. In this\nsection, we discuss approaches for safe learning control in robotics based on the following\nthree categories (see Figure 2 and Figure 4):\n• Learning uncertain dynamics to safely improve performance: The ﬁrst set\nof works we review relies on an a priori model of the robot dynamics. The robot’s\nperformance is improved by learning the uncertain dynamics from data. Safety is\ntypically guaranteed based on standard control-theoretic frameworks, achieving Safety\nwww.dynsyslab.org\nLevel II or III.\n• Encouraging safety and robustness in RL: The second set of works encompasses\napproaches that usually do not have knowledge of an a priori robot model or the\nsafety constraints. Rather than providing hard safety guarantees, these approaches\nencourage safe robot operation (Safety Level I), for example, by penalizing dangerous\nactions.\n• Certifying learning-based control under dynamics uncertainty: The last set\nof works aims to provide safety certiﬁcates for learning-based controllers that do\nnot inherently consider safety constraints.\nThese approaches modify the learning\ncontroller output by either constraining the control policy, leveraging a known safe\nbackup controller, or modifying the controller output directly to achieve stability\nand/or constraint satisfaction. They typically achieve Safety Level II or III.\nFigure 4 categorizes the approaches reviewed in this section based on their safety level and\nreliance on data. A more detailed summary of the approaches can be found in Table 1 of\nAppendix A.\n3.1. Learning Uncertain Dynamics to Safely Improve Performance\nIn this section, we consider approaches that improve the robot’s performance by learning\nthe uncertain dynamics and provide safety guarantees (Safety Level II or III) via control\nframeworks such as adaptive control, robust control, and robust MPC (outlined in Sec. 2.2).\nThese approaches make assumptions about the unknown parts of the problem (e.g., Lips-\nchitz continuity) and often rely on a known model structure (e.g., control-aﬃne or linear\nsystem with bounded uncertainty) to prove stability and/or constraint satisfaction (see\nFigure 4).\n3.1.1. Integrating Machine Learning and Adaptive Control. There are three main ideas to\nincorporating online machine learning into traditional adaptive control (see Sec. 2.2), each\nwith its own distinct beneﬁts. These include: (i) using black-box machine learning models\nto accommodate nonparametric unknown dynamics, (ii) using probabilistic learning and\nexplicitly accounting for the learned model uncertainties, in order to achieve cautious adap-\ntation, and (iii) augmenting adaptive control with deep learning approaches for experience\nmemorization in order to minimize the need for re-adaptation.\nLearning Nonparametric Unknown Dynamics with Machine Learning Models. One\ngoal of integrating adaptive control and machine learning is to improve the performance of\na robot subject to nonparametric dynamics uncertainties. As opposed to Equation 10, we\nconsider a system with nominal dynamics ¯f(xk, uk) = ¯Axk + ¯Buk and unknown dynam-\nics ˆf(xk, uk) = ¯B ψk(xk), where ψk(xk) is an unknown nonlinear function without an obvi-\nous parametric structure. Learning-based MRAC approaches (39, 40) make the uncertain\nsystem behave as the linear nominal model ¯f by using a combination of L1 adaptation (41)\nand online learning to approximate ψk(xk) by ˆψk(xk) = ˆψL1,k + ˆψlearn,k(xk), where ˆψL1,k is\nthe input computed by the L1 adaptive controller and ˆψlearn,k is the input computed by the\nlearning module, which can be an NN (39) or a GP (40). The estimated ˆψk(xk) is then used\nin the controller πk(xk) to account for the unknown nonlinear dynamics ψk(xk), improving\na linear nominal control policy designed based on ¯f. In these approaches, fast adaptation\nand stability guarantees are provided by the L1 adaptation framework (Safety Level III),\nwhile the learning module provides additional ﬂexibility to capture the unknown dynamics.\nAs shown in (39), the addition of learning improves the performance of the standard L1\nadaptive controller and allows fast adaptation to be achieved at a lower sampling rate.\nCautious Adaptation with Probabilistic Model Learning. Another stream of adap-\nSafe Learning in Robotics\n11\ntive control approaches leverage probabilistic models to achieve cautious adaptation by\nweighting the contribution of the learned model based on the model output uncertainty.\nWe consider a system with nominal dynamics ¯f(xk, uk) = ¯fx(xk) + ¯fu(xk)uk and unknown\ndynamics of the same form ˆf(xk, uk) = ˆfx(xk) +ˆfu(xk)uk, where ˆfx(xk) and ˆfu(xk) are un-\nknown nonparametric nonlinear functions. In a model inversion-based MRAC framework,\nan approximate feedback linearization is achieved via the nominal model to facilitate the\ndesign of MRAC, and a GP-based adaptation approach is used to compensate for feedback\nlinearization errors due to the unknown dynamics (42). To account for the uncertainty in\nthe GP model learning, the controller relies on the GP model only if the conﬁdence in the\nlatter is high: πk(xk) = πnom(xk) −γ(xk, uk) πlearn,k(xk), where πnom(xk) is the control\npolicy designed based the nominal model, πlearn,k(xk) is the adaptive component designed\nbased on the GP model, and γ(xk, uk) ∈[0, 1] is a scaling factor with 0 indicating low\nconﬁdence in the GP. The stability of the overall system (Safety Level III) is guaranteed\nvia a stochastic stability analysis, and the eﬃcacy of the approach has been demonstrated\nin quadrotor experiments (43, 42).\nMemorizing Experience with Deep Architectures. Apart from compensating for non-\nlinear and nonparametric dynamics uncertainties, deep learning approaches have also been\napplied to adaptive control for “memorizing” generalizable feature functions as the system\nadapts. In particular, an asynchronous DNN adaptation approach is proposed in (44, 45).\nSimilar to (39, 40), we consider a linear nominal model ¯f(xk, uk) = ¯Axk+ ¯Buk and unknown\nnonlinear dynamics ˆf(xk, uk) = ¯B ψk(xk) with ψk(xk) being an unknown nonlinear func-\ntion. In the proposed approach, the last layer of the DNN is updated at a higher frequency\nfor fast adaptation, while the inner layers are updated at a lower frequency to “memorize”\npertinent features for the particular operation regimes. To provide safety guarantees, the\nauthors derive an upper bound on the sampling complexity of the DNN to achieve a pre-\nscribed level of modelling error and leverage this result to show that Lyapunov stability of\nthe adapted system can be guaranteed (Safety Level III) by ensuring the modelling error\nof the DNN is lower than a given bound. In contrast to other MRAC approaches, which do\nnot usually retain a memory of the past experience, the inner layers of the asynchronous\nDNN in (44, 45) store relevant features that facilitate adaptation when similar scenarios\narise in the future.\n3.1.2. Learning-based Robust Control. Learning-based robust control improves the perfor-\nmance of classical robust control in Sec. 2.2 by using data to improve the linear dynamics\nmodel and reduce the uncertainty in Equation 11.\nUsing a Gaussian Process Dynamics Model for Linear Robust Control. The conser-\nvative performance of robust control in Sec. 2.2 is improved by updating the linear dynamics\nmodel and uncertainty in Equation 11 with a Gaussian Process (46) (GP). The unknown\nnonlinear dynamics ˆf(xk, uk) are learnt as a GP, which is then linearized about an operat-\ning point. By linearizing the GP (as opposed to directly ﬁtting a linear model) data close\nto the operating point are prioritized. The uncertain linear dynamics in Equation 11 are\nassumed to be modelled as ˆA = A0 + ˜A∆, ˆB = B0 + ˜B∆, where A0 and B0 are obtained\nfrom the linearized GP mean, ˜A and ˜B are obtained from the linearized GP variance (often\ntwo standard deviations), and ∆represents a matrix with elements taking any value in the\nrange of [−1, +1]. Further performance improvement is achieved by modelling ˜A and ˜B\nas state dependent (47). Additionally, (48) was able to achieve better performance than\n(46) by leveraging the GP’s distribution, while maintaining the same level of safety. The\nmain advantage of these approaches is that they can robustly guarantee Safety Level II\nstability while improving performance. This is achieved by shrinking the GP uncertainty as\nmore data is added, which improves the linear model A0 and B0 and reduces the uncertain\ncomponent ˜A and ˜B. This has been shown on a quadrotor (46). However, these approaches\nwww.dynsyslab.org\nare limited to stabilization tasks and do not account for state and input constraints.\nExploiting Feedback Linearization for Robust Learning-Based Tracking. Trajectory\ntracking convergence, as opposed to the simpler stabilization task performed in (46), is\nguaranteed by exploiting the special structure of exactly feedback linearizable systems (49).\nThis structure assumes that the nonlinear system dynamics in Equation 1 can be described\nby a linear nominal model, where ¯A and ¯B have an integrator chain structure. It also\nassumes that the unknown dynamics are ˆf(xk, uk) = ¯Bψ(xk, uk), where ψ(xk, uk) is an\nunknown invertible function.\nA probabilistic upper bound is obtained for ψ(xk, uk) by\nlearning this function as a GP. A robust linear controller is designed for the uncertain system\nbased on this learnt probabilistic bound. Further performance improvement is achieved by\nalso updating the feedback linearization (50) through improving the estimate of the inverse\nψ−1(·). These approaches have been applied to trajectory tracking, with Safety Level II,\non Lagrangian mobile manipulators in (49) and quadrotor models in (50). However, they\nhinge on this special structure and cannot account for state and input constraints.\n3.1.3. Reducing Conservatism in Robust MPC with Learning and Adaptation. The conser-\nvative nature of robust MPC (Sec. 2.2) is improved—while still satisfying input and state\nconstraints—through (i) robust adaptive MPC, which adapts to parametric uncertainty,\nand (ii) learning-based robust MPC, which learns the unknown dynamics ˆf or, in one case,\ncost ˆJ.\nRobust Adaptive MPC. Robust adaptive MPC assumes parametric uncertainties, and\n(i) uses data to reduce the set of possible parameters over time or (ii) uses an inner-loop\nadaptive controller and an outer-loop robust MPC. This leads to improved performance\ncompared to standard robust MPC (see Sec. 2.2), while satisfying hard constraints (Safety\nLevel III).\nThe ﬁrst set of approaches considers stabilization tasks, where the full system dynamics,\nEquation 10, are assumed to be linear and stable (51) or linear and unstable (52) with\nuncertain parameter θ ∈Θ0:\nxk+1 = ( ¯A + ˆA(θ))xk + ( ¯B + ˆB(θ))uk + wk .\n14.\nThe process noise and the parameters are assumed to be bounded by known sets W and\nthe initially conservative Θ0, respectively. Given W and Θ0, we can derive a conservative\nupper bound on the uncertain dynamics ˆf(xk, uk, wk, θ) = ˆA(θ)xk + ˆB(θ)uk + wk ∈D0,\nwhere D0 is a compact set determined from W and Θ0. To guarantee constraint satisfaction,\ntube-based MPC (see Sec. 2.2) is applied, where the initial tube Ωtube,0 is based on D0.\nTo reduce the conservatism of the approach, an adaptive control method is introduced to\nimprove the estimate of the parameter set Θk and reduce the size of the tube Ωtube,k at\neach time step k. This idea has been extended to stochastic process noise for probabilistic\nconstraint satisfaction (Safety Level II) (53), time-varying parameters (54), and linearly\nparameterized uncertain nonlinear systems (55, 56). Further performance improvement is\nachieved by combining robust adaptive MPC with iterative learning (57), which updates\nthe terminal constraint set Xterm in Equation 12d and the terminal cost lH in Equation 12a\nafter every full iteration using the closed-loop state trajectory and cost (58). In the second\napproach, an underlying MRAC (see Sec. 2.2) is used to make the closed-loop system\ndynamics resemble a linear reference model with bounded disturbance set D (59). This\nlinear model and its bounds are then used in an outer-loop robust MPC to achieve fast\nstabilization in the presence of model errors.\nLearning-Based Robust MPC. Learning-based robust MPC uses data to (i) improve the\nunknown dynamics estimate, (ii) reduce the uncertainty set, or (iii) update the cost to\navoid states with high uncertainty. Unlike robust adaptive control, learning-based robust\nMPC considers nonparameterized systems.\nSafe Learning in Robotics\n13\nUnder the assumption of a linear nominal model ¯f(x, u) = ¯Ax + ¯Bu and bounded\nunknown dynamics ˆf(xk, uk) ∈D, the unknown dynamics can be safely learned from data,\ne.g., using a NN (60). Robust constraint satisfaction, Equation 12c, is guaranteed by using\ntube-based MPC for the linear nominal model and performance improvement is achieved by\noptimizing over the control inputs for the combined nominal and learned dynamics. If the\nbounded unknown dynamics are assumed to be state-dependent ˆf(xk) ∈D(xk), instead of\nusing a constant tube Ωtube, the state constraints in Equation 12c can be tightened based\non the state-dependent uncertainty set D(x) (61). In a numerical stabilization task, a GP\nis used to model ˆf(xk) and its covariance determines D(x). This yields less conservative,\nprobabilistic state constraints (Safety Level II).\nLearning-based robust MPC can be extended to nonlinear nominal models. Typically, a\nGP is used to learn the unknown dynamics ˆf(x, u): the mean updates the dynamics model\nin Equation 12b and the state- and input-dependent uncertainty set D(x, u) is derived from\nthe GP’s covariance and contains the true uncertainty with high probability. Similarly, the\nstate- and input-dependent tube in Equation 12c is determined from the uncertainty set\nD(x, u). The main challenge, compared to using a linear nominal model, is the uncertainty\npropagation over the prediction horizon in the MPC because Gaussian uncertainty (obtained\nfrom the GP) is no longer Gaussian when propagated through the nominal nonlinear dy-\nnamics. Approximation schemes are required, such as using a sigma-point transform (62),\nlinearization (63), exact moment matching (64), or ellipsoidal uncertainty set propaga-\ntion (65). Additionally, further approximations (e.g., ﬁxing the GP’s covariances over the\nprediction horizon (63)) are usually required to achieve real-time implementation. These\napproximations can lead to violations of the probabilistic constraints (Safety Level II) in\nEquation 9e. An alternative approach to address the challenges of GPs uses a NN regres-\nsion model that predicts the quantile bounds of the tail of a state trajectory distribution\nfor a tube-based MPC (66). However, this approach is hindered by typically nonexhaustive\ntraining data sets, which can lead to the underestimation of the tube size.\nM12. Lipschitz\nContinuity:\nA\nfunction h : A 7→B\nis said to be\nLipschitz continuous\non A if (∃ρ > 0)\nsuch that for any\na1, a2 ∈A the\ncondition\n||h(a1) −h(a2)|| ≤\nρ||a1 −a2|| holds,\nwhere A and B are\nthe domain and\ncodomain of h, and\n|| · || denotes any\nlp-norm of a vector.\nThe constant ρ is\ncalled a Lipschitz\nconstant of h.\nFor repetitive tasks, another approach is to adjust the cost function based on data\ninstead of the system model.\nThe predicted cost error is learned from data using the\ndiﬀerence between the predicted cost at each time step and the actual closed-loop cost at\nexecution. By adding this additional term to the cost function, the MPC penalizes states\nthat had previously resulted in higher closed-loop cost than expected Equation 12a (67),\nresulting in reliable performance despite model errors.\nM13. Region of\nAttraction:\nLet x∗\nbe an equilibrium of\na closed-loop system\nunder some\npolicy π(x):\nxk+1 = fπ(xk) =\nf(xk, π(xk)) with\nfπ(xk) being\nLipschitz\ncontinuous. The\nregion of attraction\n(ROA) of x∗is the\nset of states from\nwhich the system\nwill converge to x∗.\n3.1.4. Safe Model-Based RL with A Priori Dynamics. Safe model-based RL augments\nmodel-based RL (see Sec. 2.3) with safety guarantees (see Figure 4). Stability can be\nprobabilistically guaranteed (Safety Level II) under the assumption that the known nomi-\nnal model ¯f(xk, uk) and the unknown part ˆf(xk, uk) are Lipschitz continuous with known\nLipschitz constants (see M12) (68). Given a Lyapunov function (see M6), an initial safe\npolicy π0, and a GP to learn the unknown dynamics ˆf, a control policy πk is chosen so\nthat it maximizes a conservative estimate of the region of attraction (ROA) (see M13). The\nmost uncertain states (based on the GP’s covariance) inside the ROA, are explored. This\nreduces the uncertainty over time and allows the ROA to be extended. The practical im-\nplementation resorts to discrete states for tractability and retains the stability guarantees\nwhile being sub-optimal in (exploration) performance.\n3.2. Encouraging Safety and Robustness in Reinforcement Learning\nThe approaches in this section are safety-augmented variations of the traditional MDP and\nRL frameworks. In general, these methods do not assume knowledge of an a priori nominal\nmodel ¯f and some also learn the reward or step cost l (69), or the safety constraints c (70).\nRather than providing strict safety guarantees, these approaches encourage constraint satis-\nwww.dynsyslab.org\nfaction during and after learning, or robustness of the learned control policy π to uncertain\ndynamics (Safety Level I, see Figure 4). In plain MDP formulations, (i) states and inputs\n(or actions) are assumed to have known, often discrete and ﬁnite, domains but they are not\nfurther constrained while searching for an optimal policy π∗and (ii) only loose assumptions\nare made on the dynamics f, for example, the system being Markovian (20) (see M14).\nM14. Markov\nProperty:\nThe\nprobability for a\nsystem to be in state\nxk at time k only\ndepends on the state\nat time k −1, xk−1\nand, in MDPs,\nuk−1.\nA previous taxonomy of safe RL (4)—covering research published up until 2015—\ndistinguished methods that either modify the exploration process to include external knowl-\nedge or modify the optimality criterion J with a safety factor. However, as pointed out\nin (71), the number and breadth of publications in RL, including safe RL, has since greatly\nincreased. Because the approaches recently proposed in this area are numerous and diverse,\nwe provide a high-level review of some of the most signiﬁcant trends with a focus on their\napplicability to robotics. These include: (i) the safe exploration of MDPs; (ii) risk-aware\nand cautious RL adaptation; (iii) RL based on constrained MDPs; and (iv) robust RL.\nM15. Ergodicity:\nAn MDP is ergodic\nif, by following a\ngiven policy, any\nstate is reachable\nfrom any other state.\nErgodic properties of\ndynamical systems\nare key to establish\nconditions for their\ncontrollability (72).\n3.2.1. Safe Exploration and Optimization. An RL algorithm’s need to explore poses a chal-\nlenge to its safety as it must select inputs with unpredictable consequences in order to learn\nabout them.\nSafe Exploration. The problem of safely exploring an MDP is tackled in (73) through\nthe notion of ergodicity (see M15). Policy updates that preserve the ergodicity of the MDP\nenable the system to return to an arbitrary state from any state.\nThus, the core idea\nis to restrict the space of eligible policies to those that make the MDP ergodic (with at\nleast a given probability). Exactly solving this problem, however, is NP-hard. A simpliﬁed\nproblem is solved in (73), using a heuristic exploration algorithm (74), which leads to sub-\noptimal but safe exploration that only considers a subset of the ergodic policies. In practice,\n(73) is demonstrated in two simulated scenarios with discrete, ﬁnite X and U.\nRather\nthan designing for a recoverable exploration through ergodicity, safe exploration strategies\nthat provide constraint satisfaction are developed in (75, 70). A safety layer is used to\nconvert an optimal but potentially unsafe action ulearn,k produced by an NN policy, to the\nclosest safe action usafe,k with respect to some safety state constraint. Both (75) and (70)\ninvolve solving a constrained least squares problem, usafe,k = arg minuk∥uk −ulearn,k∥2\n2,\nwhich is akin to the safety ﬁlter approaches further discussed in Sec. 3.3.2. Concretely,\n(75) assumes full knowledge of the (linear) constraint functions and solves usafe,k using a\ndiﬀerentiable Quadratic Programming (QP) solver. In contrast, (70) assumes constraints\nare unknown a priori but can be evaluated. Thus, the approach learns the parameters\nof linear approximations to these constraints and then uses them in the solver. Notably,\n(75, 70) consider single time-step state constraints.\nIn Sec. 3.2.3, we will also discuss\nmethods that deal with more general trajectory-level constraints (Equation 6).\nSafe Optimization. Several works address the problem of safely optimizing an unknown\nfunction (typically the cost function), often exploiting GP models (76). Safety refers to\nsampling inputs that do not violate a given safety threshold (Equation 3, Equation 4). These\napproaches fall under the category of Bayesian optimization (77) and include SafeOpt (78);\nSafeOpt-MC (79), an extension towards multiple constraints; StageOpt (80), a the more\neﬃcient two-stage implementation; and GoSafe (81), to explore beyond the initial safe\nregion. In particular, SafeOpt infers two subsets of the safe set from the GP model: (i) one\nwith candidate inputs to extend the safe set, and (ii) one with candidate input to optimize\nthe unknown function—from which it greedily picks the most uncertain. The ideas pioneered\nby SafeOpt were applied to MDPs in SafeMDP (69) resulting in safe exploration of MDPs\nwith an unknown cost function l(x, u), which (69) models as a GP. In SafeMDP, the single-\nstep reward represents the safety feature that should not violate a given threshold. Another\nextension of (78), SafeExpOpt-MDP (82), treats the safety feature cj and the MDP’s cost l as\ntwo separate, unknown functions, allowing for the constraint of the ﬁrst and the optimization\nSafe Learning in Robotics\n15\nof the latter. A recent survey of these techniques was provided in (76), highlighting the\ndistinction between safe learning in regression—i.e., minimizing the selection and evaluation\nof non-safe training inputs—and safe exploration in MDPs and stochastic dynamical systems\nlike Equation 9b—i.e., selecting action inputs that also preserve ergodicity).\nM16. Conservative\nQ-learning (CQL):\nA recent oﬄine RL\napproach (83) to\nmitigate the\noverestimation\nproblem in value\nfunctions.\nQ-learning is known\nto overestimate\naction-state values\nbecause of its\nmaximization\noperator, and oﬄine\nRL can lead to\noverestimated values\nof the actions that\nare more likely\nunder the policy\nused to collect the\ndata. CQL tackles\nthis by using a novel\nupdate rule with a\nsimple regularizer to\nlearn value lower\nbounds instead.\nM17. Model\nEnsembles:\nCollections of\nlearned models can\nbe used to mitigate\nnoise or better\ncapture the\nstochasticity of a\nsystem. Probabilistic\nEnsembles with\nTrajectory Sampling\n(PETS) (84) is a\nmodel-based RL\nmethod that uses\nmodel ensembles\nand probabilistic\nnetworks to capture\nuncertainties in the\nsystem dynamics.\nLearning a Safety Critic. A safety critic is a learnable action-value function Qπ\nsafe that can\nbe used to detect if a proposed action can lead to unsafe conditions. The works (85, 86, 87)\nmake use of such a critic and then resort to various fallback schemes to determine a safer\nalternative input. These works diﬀer from those in Sec. 3.3.2 in that the ﬁltering criterion\ndepends on a model-free, learned value function, which can only grant the satisfaction of\nSafety Level I.\nM18. Conditional\nValue at Risk\n(CVaR): CVaRα is\na risk measure that\nis equal to the\naverage of the worst\nα-percentile of the\ntotal cost J (88).\nIn (85), safety Q-functions for reinforcement learning (SQRL) are used to (i) learn a\nsafety critic from only abstract, sparse safety labels (e.g., a binary indicator), and (ii) trans-\nfer knowledge of safe action inputs to new but similar tasks. SQRL trains Qπ\nsafe to predict\nthe future probability of failure in a trajectory, and uses it to ﬁlter out unsafe actions from\nthe policy π. The knowledge transfer is achieved by pre-training Qπ\nsafe and π in simulations,\nand then ﬁne-tuning π on the new task (with similar dynamics f and safety constraints),\nstill in simulation, while reusing Qπ\nsafe to discriminate unsafe inputs. However, the success\nof the ﬁnal safe policy still depends on the task- and environment-speciﬁc hyperparame-\nters (85) (which must be found via parameter search, in simulation, prior to the actual\nexperiment). Building on (85), Recovery RL (86) additionally learns a recovery policy πrec\nto produce fallback actions for Qπ\nsafe as an alternative to ﬁltering out unsafe inputs and\nresorting to potentially sub-optimal ones in π. In (87), the authors extend Conservative Q-\nLearning (CQL, see M16) and propose Conservative Safety Critic (CSC). Similarly to (85),\nCSC assumes sparse safety labels and uses Qπ\nsafe for action ﬁltering, but it trains Qπ\nsafe to\nupper bound the probability of failure and ensures provably safe policy improvement at\neach iteration.\n3.2.2. Risk-Averse RL and Uncertainty-Aware RL. Safety in RL can also be encouraged\nby deriving and using risk (or uncertainty) estimates during learning. These estimates are\ntypically computed for the system dynamics or the overall cost function, and leveraged to\nproduce more conservative (and safer) policies.\nRisk can be deﬁned as the probability of collision for a robot performing a navigation\ntask (89, 90).\nA collision model, captured by a neural network ensemble trained with\nMonte Carlo dropout, predicts the probability distribution of a collision, given the current\nstate and a sequence of future actions. The collision-averse behavior is then achieved by\nincorporating the collision model in a MPC planner.\nIn (91), the authors build upon PETS (84) (see M17) to propose cautious adaptation for\nsafe RL (CARL). CARL is composed of two training steps: (i) a pre-training phase that is\nnot risk-aware, where a PETS agent is trained on multiple diﬀerent system dynamics, and\n(ii) an adaptation phase, where the agent is ﬁne-tuned on the target system by taking risk-\naverse actions. CARL also proposed two notions of risk, one to avoid low-reward trajectories\nand another to avoid catastrophes (e.g., irrecoverable states or constraint violations). In\nSAVED (92), a PETS (84) agent is used to predict the probability of a robot’s collision\nand to evaluate a chance constraint for safe exploration. Similarly to the methods in the\nlast paragraph of Sec. 3.2.1, SAVED also learns a value function from sparse costs, which\nit uses as a terminal cost estimate.\nTo learn risk-averse policies using only oﬄine data, the approach in (93) optimizes for\na risk measure of the cost such as Conditional Value-at-Risk (CVaR) (see M18). Instead of\nusing model ensembles as in (89, 90, 91), the work in (93) uses distributional RL (see M19)\nto explicitly model the distribution of the total cost of the task (control of a simulated 1D\ncar), and oﬄine learning to improve scalability.\nwww.dynsyslab.org\n3.2.3. Constrained MDPs and RL. The CMDP framework is frequently used in safe RL, as\nit introduces constraints that can be used to express arbitrary safety notions in the form of\nEquation 6. RL for CMDPs, however, faces two important challenges (see Sec. 2.3): (i) how\nto incorporate and enforce constraints in the RL algorithm; and (ii) how to eﬃciently solve\nthe constrained RL problem—especially when using deep learning models, which are the de\nfacto standard in non-safe RL. Works in this section that address these questions include\n(i) Lagrangian methods for RL, (ii) generalized Lyapunov functions for constraints, and\n(iii) Backward Value Functions. Nonetheless, much of the work in this section remains con-\nﬁned to rather naive simulated tasks, motivating further investigation into the applicability\nof constrained RL in real-world control.\nM19. Distributional\nRL: Distributional\nRL (94) focuses on\nthe modeling and\nlearning of the\ndistribution of the\ncost J, rather than\nits expected value.\nThe greater\ndescriptive power\ncan enable richer\npredictions and\nrisk-aware\nbehaviours.\nM20. Lagrangian\nMethods:\nTo solve\na constrained\noptimization\nproblem\nminx f(x) s.t. g(x) ≤\n0, Lagrangian\nmethods deﬁne a\nLagrangian function\nL(x, λ) =\nf(x) + λg(x) on\nprimal variable x,\ndual variable λ, and\nsolve the equivalent\nunconstrained\nproblem\nmaxλ≥0 minx L(x, λ).\nIn practice,\nnumerical\napproaches, such as\ngradient methods,\nare often used to\nperform the\nprimal-dual\nupdates (88).\nLagrangian Methods in RL Optimization. In (88, 95), the CMDP optimization problem\n(see Sec. 2.3) is ﬁrst transformed into an unconstrained optimization one (see M20) over\nthe primal variable π and dual variable λ using the Lagrangian L(π, λ), and RL is used as\na subroutine in the primal-dual updates for Equation 15b:\nL(π, λ) = Jπ +\nX\nj\nλj\n\u0010\nJπ\ncj −dj\u0011\n15a.\n(π∗, λ∗) = arg max\nλ≥0 min\nπ L(π, λ)\ns.t.\nEquation 9b, 9c, 9d .\n15b.\nIn particular, (88) deﬁnes a constraint on the CVaR of cost Jπ, and uses policy gradient\nor actor-critic methods to update the policy in Equation 15b. In subsequent work (95),\nthe authors improve upon (88) by incorporating oﬀ-policy updates of the dual variable\n(with the on-policy primal-dual updates).\nThis is empirically shown to achieve better\nsample eﬃciency and faster convergence. In (34), the authors extend a standard trust-region\n(see M21) RL algorithm (96) to CMDPs using a novel bound that relates the expected cost\nof two policies to their state-averaged divergence (see M21). The key idea is performing\nprimal-dual updates with surrogates/approximations of the cost Jπ and constraint cost Jπ\ncj\nderived from the bound. The beneﬁts are two-fold: (i) the surrogates can be estimated\nwith only state-action data, bypassing the challenge of trajectory evaluation from oﬀ-policy\ndata; and (ii) the updates guarantee monotonic policy improvement and near constraint\nsatisfaction at each iteration (Safety Level I). However, unlike (88, 95), each update involves\nsolving the dual variables from scratch, which can be computationally expensive.\nA Lyapunov Approach to Safe RL. Lyapunov functions (see M6) are used extensively in\ncontrol to analyze system stability and they are a powerful tool to translate a system’s global\nproperties into local conditions. In (97, 98), Lyapunov functions are used to transform the\ntrajectory-level constraints Jπ\ncj in Equation 6 into step-wise, state-based constraints. This\nallows a more eﬃcient computation of Jπ\ncj and mitigates the cost of oﬀ-policy evaluation.\nThese approaches, however, require the system to start from a baseline policy π0 that\nalready satisﬁes the constraints. In (97), four diﬀerent algorithms are proposed to solve\nCMDPs by combining traditional RL methods and the Lyapunov constraints, but they\nare only applicable to discrete input spaces (with continuous state spaces). In subsequent\nwork (98), the authors of (97) extend the approach to continuous input spaces and standard\npolicy gradient methods, addressing its computational tractability.\nLearning Backward Value Functions. The work in (99) proposes Backward Value Func-\ntions (BVF) as a way to overcome the excessive computational cost of the approaches in\nthe previous paragraphs (34, 98). Similarly to a (forward) value function V π that estimates\nthe total future cost from each state, a BVF V b,π estimates the accumulated cost so far (up\nto the current state). We can decompose a trajectory-level constraint at any time step k as\nsum of V π\ncj (xk) and V b,π\ncj (xk) for the constraint cost Jπ\ncj. This decomposition also alleviates\nthe problem of oﬀ-policy evaluation as these value functions can be learned concurrently\nand eﬃciently via Temporal Diﬀerence (TD) methods (see M22). In practice, V π\ncj , V b,π\ncj ,\nSafe Learning in Robotics\n17\nV π are jointly learned (99) and used for policy improvement at each time step, allowing to\nimplement Safety Level I. The approach is intended for discrete action spaces but can be\nadapted to continuous ones as in (70).\nM21. Divergence:\nDivergence\nquantiﬁes the\nsimilarity between\nprobability\ndistributions. It may\nnot satisfy the\nsymmetry and the\ntriangle inequality of\na distance metric. In\nRL, commonly used\nones include\nKullback-Leibler\ndivergence (KL),\ntotal variation\ndistance (TV), and\nthe Wasserstein\ndistance, which is\nalso a valid distance\nmetric. Divergence\nmeasures can be\nused to constrain\npolicy updates in\ntrust-region RL\nalgorithms (96).\n3.2.4. Robust MDPs and RL. Works in this section aim to implement robustness in RL,\nspeciﬁcally, learning policies that can operate under disturbances and generalize across sim-\nilar tasks or robotic systems. This is typically done by framing the learning problem as a\nrobust MDP (Equation 13). In (100), Robust RL (RRL) implements an Actor-Disturber-\nCritic architecture. The authors observes that the learned policy and value function coincide\nwith those derived analytically from H∞control theory for linear systems (see Sec. 2.2).\nHowever, more recent robust RL literature often abstains from assumptions on dynamics\nor disturbances, and applies model-free RL (101) to seek empirically robust performance at\nthe expense of theoretical guarantees. In the following paragraphs, we introduce two lines\nof work: (i) robust adversarial RL, which explicitly models the min-max problem in Equa-\ntion 13 in a game theoretic fashion; and (ii) domain randomization, which approximates\nthe same problem in Equation 13 by learning on a set of randomized perturbed dynamics.\nM22. Temporal\nDiﬀerence Learning:\nTemporal Diﬀerence\n(TD) learning (20)\nis an important class\nof model-free RL\nmethods that\nupdates the value\nfunction by\nbootstrapping from\nthe current estimate\nusing the Bellman\nequation to\nformulate\nsampling-based,\niterative updates.\nFor example, given a\ncontrol step data\ntuple (xk, lk, xk+1)\nwe can perform a\nlearning step as\nV π(xk) ←V π(xk)+\nα(lk + γV π(xk+1) −\nV π(xk)) where α, γ\nare the step size and\ndiscount factor.\nRobustness Through Adversarial Training.\nCombining RL with adversarial learn-\ning (102) results in robust adversarial RL (RARL) (36), where the robust optimization\nproblem (Equation 13) is set up as a two-player, discounted zero-sum Markov game in\nwhich an agent (protagonist) learns policy π to control the system and another agent (ad-\nversary) learns a separate policy to destabilize the system. The two agents learn in an\nalternated fashion (each is updated while ﬁxing the other), attempting to progressively\nimprove both the robustness of the protagonist’s policy and the strength of its adversary.\nThe work of (37) extends (36) with risk-aware agents (see Sec. 3.2.2), with the protagonist\nbeing risk-averse and the adversary being risk-seeking. This method learns an ensemble of\nDeep Q-Networks (DQN) (103) and deﬁnes the risk of an action based on the variance of its\nvalue predictions. In another extension of (36), a population of adversaries (rather than a\nsingle one) is trained (38), leading to the resulting protagonist being less exploitable by new\nadversaries. Finally, the work in (104) proposes certiﬁed lower bounds for the value predic-\ntions from a DQN (103), given bounded observation perturbations. The action selection is\nbased on these value lower bounds, assuming adversarial perturbation.\nRobustness Through Domain Randomization.\nDomain randomization methods at-\ntempt to learn policies that empirically generalize to a wider range of tasks or robot systems.\nInstead of considering worst-case disturbances or scenarios, learning happens on systems\nwith randomly perturbed parameters (e.g., inertial properties, friction coeﬃcients). These\nparameters often have pre-speciﬁed ranges, eﬀectively inducing a robust set that (i) ap-\nproximates the uncertainty set D in Equation 13, and (ii) allows the system to reuse any\nstandard RL algorithm. As an example, in (105), a quadrotor learns vision-based ﬂight\npurely in simulation from scenes with randomized properties. Using this model-free policy\nin the real world resulted in improved collision avoidance performance. Instead of learning\na policy directly, the work in (106) uses learned visual predictions with a MPC controller\nto enable eﬃcient and scalable real-world performance. Besides the uniform randomization\nin (105, 106), adaptive randomization strategies such as Bayesian search (107) are also a\npromising direction. In (108), a discriminator is adversarially trained and used to guide\nthe randomization process that generates systems that are less explored or exploited by the\ncurrent policy.\n3.3. Certifying Learning-Based Control Under Dynamics Uncertainty\nIn this section, we review methods providing certiﬁcation to learning-based control ap-\nproaches that do not inherently account for safety constraints, see Figure 2. We divide the\nwww.dynsyslab.org\ndiscussion into two parts: (i) stability certiﬁcation, and (ii) constraint set certiﬁcation. The\nworks in this section leverage an a priori dynamics model of the system and provide hard\nor probabilistic safety guarantees (Safety Level II or III) under dynamics uncertainties (see\nFigure 4).\n3.3.1. Stability Certiﬁcation. This section introduces certiﬁcation approaches that guaran-\ntee closed-loop stability under a learning-based control policy.\nLipschitz-Based Safety Certiﬁcation for DNN-Based Learning Controllers. These\napproaches exploit the expressive power of DNNs for policy parametrization and guaran-\ntee closed-loop stability through a Lipschitz constraint on the DNN. Let ρ be a Lipschitz\nconstant of a DNN policy (see M12), an upper bound on the Lipschitz constant ρ that\nguarantees closed-loop stability (Safety Level III) can be established by using a small-gain\nstability analysis (109), solving a semi-deﬁnite program (SDP) (110), or applying a sliding\nmode control framework (111). This bound on ρ can be used in either (i) a passive, iter-\native enforcement approach where the Lipschitz constant ρ is ﬁrst estimated (e.g., via an\nSDP-based estimation (112)) and then used to guide re-training until the Lipschitz con-\nstraint is satisﬁed, or (ii) an active enforcement approach where the Lipschitz constraint\nis directly enforced by the training algorithm of the DNN (e.g., via spectral normaliza-\ntion (111)). While guaranteeing stability, the Lipschitz-based certiﬁcation approaches often\nrely on the particular structure of the system dynamics (e.g., a control-aﬃne structure or\nlinear structure with additive nonlinear uncertainty) to ﬁnd the certifying Lipschitz con-\nstant.\nIt remains to be explored how this idea can be extended to more generic robot\nsystems.\nLearning Regions of Attraction for Safety Certiﬁcation. The region of attraction\n(ROA) of a closed-loop system (see M13) is used in the learning-based control literature as\na means to guarantee safety. For a nonlinear system with a given state-feedback controller,\nthe ROA is the set of states that is guaranteed to converge to the equilibrium, which is\ntreated as a safe state. This notion of safety provides a mean to certify a learning-based\ncontroller. It guarantees that there is a region in state space from which the controller can\ndrive the system back to the safe state (Safety Level III) (113). ROAs can, for example, be\nused to guide data acquisition for model or controller learning (68, 114).\nWe consider deterministic closed-loop systems xk+1 = fπ(xk) = f(xk, π(xk)) with\nfπ(xk) being Lipschitz continuous.\nA Lyapunov neural network (LNN) can be used to\niteratively learn the ROA of a controlled nonlinear system from the system’s input-output\ndata (113). As compared to the typical Lyapunov functions in control (e.g., quadratic Lya-\npunov functions), the proposed method uses the LNN as a more ﬂexible Lyapunov function\nrepresentation to provide a less conservative estimate of the system’s ROA. The necessary\nproperties of a Lyapunov function are preserved via the network’s architectural design.\nIn (114), an ROA estimation approach for high-dimensional systems is presented, combin-\ning a sum of squares (SOS) programming method for the ROA computation (115) and a\ndynamics model order reduction technique to curtail computational complexity (116).\n3.3.2. Constraint Set Certiﬁcation. This section summarizes approaches that provide con-\nstraint set certiﬁcation to a learning-based controller based on the notion of robust positive\ncontrol invariant (RPCI) safe sets Ωsafe ⊆Xc (see M23). Certiﬁed learning, which can be\nM23. Robust\nPositive Control\nInvariant Safe Set:\nA robust positive\ncontrol invariant\n(RPCI) safe\nset Ωsafe is a set\ncontained in Xc such\nthat, if we start\nwithin the safe set,\nthere exists a\nfeedback policy π(x)\nto keep the system\nin the safe set\ndespite all possible\nmodel errors and\nprocess noise,\ncaptured by D.\nachieved through a safety ﬁlter (7) or shielding (117), ﬁnds the minimal modiﬁcation of a\nlearning-based control input ulearn (see Figure 2) such that the system’s state stays inside\nSafe Learning in Robotics\n19\nthe set Ωsafe:\nusafe,k = arg min\nuk∈Uc\n∥uk −ulearn,k∥2\n2\n16a.\ns.t.\nxk+1 = ¯fk(xk, uk) + ˆfk(xk, uk, wk) ∈Ωsafe,\n16b.\n∀ˆfk(xk, uk, wk) ∈D(xk, uk) ,\nwhere the range of possible disturbances D(xk, uk) is given.\nSince the safety ﬁlter and\ncontroller are usually decoupled, suboptimal behavior can emerge as the learning-based\ncontroller may try to violate the constraints (65).\nControl Barrier Functions (CBFs). CBFs are used to deﬁne safe sets. More speciﬁcally,\nthe safe set Ωsafe is deﬁned as the superlevel set of a continuously diﬀerentiable CBF Bc,\nBc : Rnx →R, as Ωsafe = {x ∈Rnx : Bc(x) ≥0}. The function Bc is generally considered\nfor continuous-time, control-aﬃne systems (see also M4) of the form:\n˙x = fx(x) + fu(x)u ,\n17.\nwhere fx and fu are locally Lipschitz and x, u are functions of time (118). In the simplest\nform, the function Bc is a CBF if there exists a state-feedback control input u, such that\nthe time derivative ˙Bc(x) = ∂Bc\n∂x ˙x satisﬁes (118):\nsup\nu∈U\n∂Bc\n∂x (fx(x) + fu(x)u) ≥−Bc(x) .\n18.\nThe CBF condition in Equation 18 is a continuous-time version of the robust positive\ncontrol invariance constraint in Equation 16b. In addition to the RPCI constraint, a similar\nM24. Control\nLyapunov Functions:\nControl Lyapunov\nFunctions (CLFs)\nextend the notion of\nLyapunov stability\nto continuous-time,\ncontrol-aﬃne\nsystems\n(Equation 17). A\npositive deﬁnite\nfunction Lc is a\nCLF, if there exists\na state-feedback\ncontrol input u, such\nthat the time\nderivative ˙Lc\nsatisﬁes:\n˙Lc(x) ≤−Lc(x).\nThe existence of a\nCLF guarantees that\nthere also exists a\nstate-feedback\ncontroller π(x) that\nasymptotically\nstabilizes the\nsystem.\nconstraint for asymptotic stability can be added to Equation 16 in the form of a constraint on\nthe time derivative of a control Lyapunov function (CLF, see M24) Lc. However, uncertain\ndynamics also yield uncertain time derivatives of Bc and Lc.\nLearning-based approaches extend CBF and CLF analyses to control-aﬃne systems\nin Equation 17 with known nominal ¯fx, ¯fu and unknown ˆfx, ˆfu. The time derivative of\nthe unknown dynamics for CLFs and/or CBFs (e.g., ∂Bc\n∂x (ˆfx(x) + ˆfu(x)u)) can be learned\nfrom iterative trials (119, 120) or data collected by an RL agent (121, 122).\nGiven a\nCBF and/or CLF for the true dynamics f, improving the estimate of the CBF’s or CLF’s\ntime derivative for the unknown dynamics ˆf using data, collected either oﬄine or online,\nyields a more precise estimate of the constraint in Equation 18. However, any learning\nerror in the CBF’s or CLF’s time derivative of the unknown dynamics can still lead to\napplying falsely certiﬁed control inputs.\nTo this end, ideas from robust control can be\nused to guarantee set invariance (Safety Level III) during the learning process by mapping\na bounded uncertainty in the dynamics to a bounded uncertainty in the time derivatives\nof the CBF or CLF (123, 124), or by accounting for all model errors consistent with the\ncollected data (125).\nIn addition, adaptive control approaches have also been proposed\nto allow safe adaptation of parametric uncertainties in the time derivatives of the CBF\nor CLF (126, 127). Probabilistic learning techniques for CBFs and CLFs have been used\nto achieve set invariance probabilistically (Safety Level II) with varying assumptions on\nthe system dynamics: the function fu(x) is fully known (128, 129), a nominal model is\nknown (130), and no nominal model is available (131).\nHamilton-Jacobi Reachability Analysis. Another approach for state constraint set cer-\ntiﬁcation of a learning-based controller is via the Hamilton-Jacobi (HJ) reachability analysis.\nThe HJ reachability analysis provides a means to estimate a robust positive control invari-\nant safe set Ωsafe under dynamics uncertainties (see M23 and Equation 16). Consider a\nnonlinear system subject to unknown but bounded disturbances ˆf(x) ∈D(x), where D(x) is\nwww.dynsyslab.org\nassumed to be known but possibly conservative. To compute Ωsafe, a two-player, zero-sum\ndiﬀerential game is formulated:\nV (x) =\nmax\nusig∈Usig\nmin\nˆfsig∈Dsig\n\u0012\ninf\nk≥0 lc\n\u0010\nφ(x, k; usig, ˆfsig)\n\u0011\u0013\n,\n19.\nwhere V is the value function associated with a point x ∈X, lc : X 7→R is a cost function\nthat is non-negative for x ∈Xc and negative otherwise, φ(x, k; usig, ˆfsig) denotes the state at\nk along a trajectory initialized at x following input signal usig and disturbance signal ˆfsig,\nand Usig and Dsig are collections of input and disturbance signals such that each time\ninstance is in U and D, respectively. The value function V can be found as the unique\nviscosity solution of the Hamilton-Jacobi Isaacs (HJI) variational inequality (132).\nThe\nsafe set is then Ωsafe = {x ∈X | V (x) ≥0}.\nBased on this formulation, we can also\nobtain an optimally safe policy π∗\nsafe that maximally steers the system towards the safe\nset Ωsafe (i.e., in the greatest ascent direction of V ). The HJ reachability analysis allows\nus to deﬁne a safety ﬁlter for learning-based control approaches to guarantee constraint set\nsatisfaction (Safety Level II or III). In particular, given Ωsafe and π∗\nsafe, one can safely learn\nin the interior of Ωsafe and apply the optimally safe policy π∗\nsafe if the system reaches the\nboundary of Ωsafe. To reduce the conservativeness of the approach, a GP-based learning\nscheme is proposed in (133) to adapt (and shrink) the unknown dynamics set D(x) based\non observed data.\nThe general HJ reachability analysis framework (132) has also been combined with on-\nline dynamics model learning for a target tracking task (134), with online planning for safe\nexploration (135), and with temporal diﬀerence algorithms for safe RL (136). In\n(137),\nHJ reachability analysis and CBFs have been integrated to compute smoother control poli-\ncies while circumventing the need to hand-design appropriate CBFs. Another recent ex-\ntension (138) proposed modiﬁcations that improve the scalability of the HJ safety analy-\nsis approach for higher-dimensional systems and was demonstrated on a ten-dimensional\nquadrotor tracking problem.\nPredictive Safety Filters. Predictive safety ﬁlters can augment any learning-based con-\ntroller to enforce state constraints, x ∈Xc, and input constraints, u ∈Uc. They do this by\ndeﬁning the safe invariant set Ωsafe in Equation 16b as the set of states (at the next time\nstep) where a sequence of safe control inputs (e.g., from a backup controller) exists that\nallows the return to (i) a terminal safe set Xterm (see M25), or (ii) to previously visited safe\nstates.\nModel Predictive Safety Certiﬁcation (MPSC) uses the theory of robust MPC\nin Sec. 2.2 and learning-based robust MPC in Sec. 3.1.3 to ﬁlter the output of any learning-\nbased controller, such as of an RL method, to ensure robust constraint satisfaction. The\nsimplest implementation of MPSC (139) uses tube-based MPC and considers the constraints\nin Equations 12b–12d but replaces the cost in Equation 12a with the cost in Equation 16a\nto ﬁnd the closest input uk to the learned input ulearn,k at the current time step that\nguarantees that we will continue to satisfy state and input constraints in the future. The\nmain diﬀerence between MPSC and learning-based robust MPC described in Sec. 3.1.3 is\nthat the terminal safe set Xterm in Equation 12d is not coupled with the selection of the cost\nfunction in Equation 12a. Instead, the terminal safe set is conservatively initialized with\nXterm = Ωtube and can grow to include state trajectories from previous iterations. This\napproach has been extended to probabilistic constraints by considering a probabilistic tube\nΩtube in (140), and to nonlinear nominal models in (141) (Safety Level II).\nM25. Terminal Safe\nSet:\nA terminal\nsafe set, denoted\nby Xterm, is a subset\nof the safe state\nconstraint space\nwherein a known\nauxiliary controller\nis guaranteed to\npreserve the\nsystem’s state.\nEntering the\nterminal safe set at\nsome ﬁxed horizon\nH is enforced as a\nconstraint.\nBackup\ncontrol\nfor\nsafe\nexploration ensures hard state constraint satisfac-\ntion (Safety Level III) by ﬁnding a safe backup controller for any given RL policy π (142).\nUnder the assumptions of a known bound D(x, u) on the dynamics f and a distance measure\nto the state constraints Xc, the backup controller is used to obtain a future state in the\nSafe Learning in Robotics\n21\nNumerical Examples &\nGrid-worlds (47, 39, 52,\n53, 58, 54, 55, 56, 61, 60, 51,\n88, 78, 97, 73, 69, 82, 99, 90,\n70, 80, 86, 137, 110, 139)\nRobot Sim. & Physics-\nbased RL Environments\n(64, 65, 66, 40, 44, 43, 142,\n68, 34, 36, 38, 91, 85, 87, 93,\n37, 104, 107, 123, 125, 126,\n130, 131, 128, 122, 113, 136,\n143, 138, 119, 140, 141, 127)\nRobots (46, 49, 42, 45, 62,\n63, 98, 75, 92, 79, 89, 105,\n106, 108, 114, 111, 133, 134,\n129, 120, 124, 121, 135)\n25% 28% 32%\n29% 34%\n61%\n46% 38%\n7%\nOpen-source\nCertifying Learning-\nbased Ctrl., Sec. 3.3\nEncouraging Safety\nin RL, Sec. 3.2\nLearning Uncertain\nDynamics, Sec. 3.1\n−1\n−0.5\n0\n0.5\n1\n−1\n−0.5\n0\n0.5\n1\n푍퓁1\n푥1\n푥2\n퐱∗\nRMPC−퓁1(푥0)\n퐱RMPC−퓁1(푥0)\n−4\n−2\n0\n2\n4\n−2\n−1\n0\n1\n2\n핏\n̄핏퓁1\n핂퐻,퓁1\n̄핏푓,퓁1\n푍퓁1\n푥1\n푥2\n퐱∗\nRMPC−퓁1(푥0)\n퐱RMPC−퓁1(푥0)\ngym-minigrid\ngym-minigrid\ndm-control\ngym\ngym\npybullet\n(135)\n(75)\n(46)\n(62)\nFigure 5: Summary of the environments used for evaluation. With increasing complexity,\nthey can be classiﬁed as: abstract numerical examples, robot simulations, and real-world\nrobot experiments. The histograms show the prevalence of each in Sec. 3.1, Sec. 3.2, and\nSec. 3.3, as well as the fraction of those whose code is open-sourced.\nneighborhood of a previously visited safe state in some prediction horizon. Before a control\ninput uk from π is applied to the system, all possible predicted states xk+1 must satisfy\n(i) xk+1 ∈Xc and (ii) the existence of a safe backup action ucertiﬁed(xk+1). Otherwise, the\nprevious backup control input ucertiﬁed(xk) is applied. This procedure guarantees that the\nsystem state stays inside a robust positive control invariant set Ωsafe ⊆Xc.\n4. BENCHMARKS\nThe approaches presented in Sec. 3 have been evaluated in vastly diﬀerent ways, see Fig-\nure 5. The trends we observe are: The works learning uncertain dynamics (Sec. 3.1) include\na preponderance of abstract numerical examples; encouraging safety in RL (Sec. 3.2) mostly\nleverages simulations—often based on physics engines (144) like MuJoCo—but grid worlds\nare also common; and works certifying learning-based control (Sec. 3.3) are still mostly\nsimulated but also has the largest fraction of real-world experiments.\nWhile numerical\nexamples make it diﬃcult to gauge the practical applicability of a method, we also note\nthat even many RL environments, including physics-based ones, are not representative of\nexisting robotic platforms–for example, they often are deterministic and do not account for\nvariations in the environment.\nIn an ideal world, all research would be demonstrated in simulations that closely re-\nsemble the target system—and brought to (ideally diﬀerent) real robots, whenever possi-\nble. Furthermore, only a minority of the published research open-sourced their software\nimplementations.\nEven in RL, where sharing code is more common (see, e.g., red bars\nin Figure 5)—and standard environments and interfaces such as gym (145) have been\nproposed—the reproducibility of results (that often rely on careful hyper-parameter tun-\ning) remains challenging (71). With regard to safety, simple RL environments augmented\nwith constraint evaluation (18) and disturbances (5) have been proposed, but lack a uniﬁed\nsimulation interface for both safe RL and learning-based control approaches—notably, one\nthat also exposes the available a priori knowledge of a dynamical system.\nWe believe that a necessary stepping stone for the advancement of safe learning control\nis to create physics-based environments that are (i) simple enough to promote adoption,\n(ii) realistic enough to represent meaningful robotic platforms, (iii) equipped with intuitive\ninterfaces for both control and RL researchers, and (iv) provide useful comparison metrics\n(e.g., the amount of data required by diﬀerent approaches).\nwww.dynsyslab.org\nGoal\n01\n2\n3\n4\n5\n6\n7\n8\n9\n10\n01\n2\n3\n4\n5\n6\n7\n8\n9\n10\n−1.0\n−0.8\n−0.6\n−0.4\n−0.2\n0.0\n0.0\n0.5\n1.0\nx (m)\nz (m)\nLinear-MPC\nPrediction Horizon\nGP-MPC\nPrediction Horizon\nConstraint\n2σ of Prediction\nFigure 6: A position comparison of the 2D quadrotor stabilization using linear MPC (red)\nand GP-MPC (blue), along with the prediction horizons at the second time step, subject\nto a diagonal state constraint (gray) and input constraints.\n0\n200\n400\n600\n800\n1,000\n·103\n0\n50\n100\n150\n200\nTraining Step\nAverage Cost J\nPPO\nl0.15\nshaping\nl0.2\nshaping\nSafe0.15\nSafe0.2\n0\n200\n400\n600\n800\n1,000\n·103\n0\n100\n200\n300\nTraining Step\nAverage Constraint Violations\nFigure 7: Total cost and constraint violations during learning for PPO, PPO with cost\nshaping, and PPO with safety layer (safe exploration). Plotted are medians with upper and\nlower quantiles over 10 seeds. The parameters in superscript represent values of the slack\nvariable, which controls responsiveness to near constraint violations.\nCart-Pole and Quadrotor Benchmark Environments. For this reason, we have created\nan open-source benchmark suite2 simulating two popular platforms for the evaluation of\ncontrol and RL research: (i) the cart-pole (64, 104, 136) and (ii) a quadrotor (142, 66,\n44, 130, 146). Our simulation environments are based on the open-source Bullet physics\nengine (147) and we adopt OpenAI’s gym interface (API) (145) for seamless integration\nwith many of the current RL libraries. What sets apart our implementation from previous\nattempts to create safety-aware RL environments (18, 5) is the extension of the traditional\nAPI (145) with features to facilitate: (i) the evaluation of safe learning control approaches\n(such as the randomization of inertial properties and of initial positions and velocities,\nrandom and adversarial disturbances, and the speciﬁcation and evaluation of state and\ninput constraints) and (ii) the integration with approaches developed by the control theory\ncommunity that leverage the knowledge of nominal models—notably, we use the symbolic\nframework CasADi (148) to enrich our gym environments with symbolic a priori dynamics,\nconstraints, and cost functions.\nSafe Learning Control Results. We focus on a constrained stabilization task, in each of\nour environments (cartpole and quadrotor). The results we present here are not meant\nto establish the superiority of one approach over another. Instead, we show how the ap-\nproaches in (63, 70, 139), taken from each of Sec. 3.1 (learning uncertain dynamics), Sec. 3.2\n(encouraging safety in RL), and Sec. 3.3 (certifying learning-based control), can improve\n2Safe control benchmark suite on GitHub: https://github.com/utiasDSL/safe-control-gym\nSafe Learning in Robotics\n23\n0\n10\n20\n−10\n−5\n0\n5\nStep k\nInput uk\nMPSC+PPO\nPPO\nModiﬁed\nConstr.\n−0.2\n−0.1\n0.0\n0.1\n0.2\n−0.5\n0.0\n0.5\n1.0\nθ (rad)\n˙θ (rad/s)\nFigure 8: Left: The uncertiﬁed PPO input (red) is plotted against the certiﬁed MPSC+PPO\ninput (blue). Right: The cart-pole state diagram (θ and ˙θ) comparing the MPSC+PPO\ncertiﬁed trajectory (blue) and the uncertiﬁed PPO trajectory (red). The MPSC is most\nactive (green dots) when the system is about to leave the constraint boundary (gray) or the\nset of states from which the MPSC can correct the system.\ncontrol performance while pursuing constraint satisfaction. By doing so, we demonstrate\nthat our benchmark can be easily integrated with learnable policies developed by either the\ncontrol or RL research community. This also allows us to fairly compare the data hungriness\nof the diﬀerent safe learning control approaches.\nA learning-based robust MPC with a GP estimate of ˆf (GP-MPC), representative of\nSec. 3.1 (learning uncertain dynamics), was implemented to stabilize a quadrotor subject\nto a state and input constraints, following the approach in (63).\nFor this experiment,\na linearization about hover with a mass and moment of inertia 150% of the true values\nwas used as the prior model. Eight-hundred randomly selected state-action pairs (or 80\nseconds), sampled from a couple of minutes of training data, were used for hyper-parameter\noptimization, which was performed oﬄine. Figure 6 compares the performance of a linear\nMPC, using the inaccurate prior model, with the GP-MPC approach.\nWe see that the\nLinear MPC, using the heavier prior model, predicts the trajectory of the quadrotor will be\nrelatively shallow when maximum thrust is applied. This results in the quadrotor quickly\nviolating the position constraint.\nIn contrast, the GP-MPC is able to account for the\ninaccurate prior model and respects the constraint boundary by a margin proportional to\nthe 95% conﬁdence interval on its dynamics prediction, stabilizing the quadrotor.\nWe also applied the Safe Exploration approach (70) to the popular deep RL algorithm\nPPO (149), as a representative of Sec. 3.2 (encouraging safety in RL), and tested it on\nthe cart-pole stabilization task with constraints on the cart position. Notably, the task\nterminates upon any constraint violation.\nWe compared it against two baselines, stan-\ndard PPO and PPO with naive cost shaping (adding a penalty when close to constraint\nviolation).\nEach RL approach used over 9 hours of simulation time collecting data for\ntraining. Figure 7 shows that the two constraint-aware approaches (cost shaping and safe\nexploration (70)) achieve the same performance after learning, but with substantially lower\nconstraint violations than the standard PPO. The safe exploration approach outperforms\ncost shaping in terms of constraint satisfaction, while not compromising convergence speed\ntowards reaching the optimal cost. Safe Exploration, however, requires careful parameter\ntuning, especially on the slack variable that determines its responsiveness to near constraint\nviolation.\nFinally, a Model Predictive Safety Certiﬁcation (MPSC) algorithm, based on (139), was\nchosen as a representative of Sec. 3.3 (certifying learning-based control). This particular\nformulation uses an MPC framework to modify an unsafe learning controller’s actions. Here,\na sub-optimal PPO controller provides the uncertiﬁed inputs trying to stabilize the cart-\npole. The advantages of using MPSC are highlighted in Figure 8. In Figure 8 (right), the\ninputs are modiﬁed by the MPSC early in the stabilization to keep the cart-pole within the\nwww.dynsyslab.org\nconstraint boundaries. Figure 8 (left) shows that, without the MPSC, PPO violates the\nconstraints, but with MPSC, it manages to stay within the boundaries. The plot also shows\nthat MPSC is most active when the system is close to the constraint boundaries (the green\ndots show when MPSC modiﬁed the learning controller’s input). This provides a proof of\nconcept of how safety ﬁlters can be combined with RL control for safer performance.\nIn our future work, we intend to use our cartpole and quadrotor benchmark envi-\nronments to evaluate the robustness, performance, safety and data eﬃciency of diﬀerent\ncontrol and learning approaches.\n5. DISCUSSION AND PERSPECTIVES FOR FUTURE DIRECTIONS\nThe problem of safe learning control is emerging as a crucial topic for next-generation\nrobotics.\nIn this review, we summarized approaches from the control and the machine\nlearning communities that allow data to be safely used to improve the closed-loop perfor-\nmance of robot control systems. We show that machine learning techniques, particularly\nRL, can help generalization towards larger classes of systems (i.e., fewer prior model as-\nsumptions), while control theory provides the insights and frameworks necessary to provide\nconstraint satisfaction guarantees and closed-loop stability guarantees during the learning.\nDespite the many advances to date, there remain many opportunities for future research.\nOPEN CHALLENGES\n1. Capturing a Broader Class of Systems. Work to date has focused on nonlinear\nsystems in the form of Equation 1. While they can model many robotic platforms,\nrobots can also exhibit hybrid dynamics (e.g., legged robots or other contact dynam-\nics with the environment (150)), time-varying dynamics (e.g., operation in changing\nenvironments (151, 152)), time delays (e.g., in actuation, sensing, or observing the\nreward (6)), partial diﬀerential dependencies in the dynamics (e.g., in continuum\nrobotics (153)). Expanding safe learning control approaches to these scenarios is\nessential for their broader applicability in robotics (see Figure 4).\n2. Accounting for Imperfect State Measurements. The majority of safe learn-\ning control approaches assume direct access to (possibly noisy) state measurements\nand neglect the problem of state estimation. In practice, obtaining accurate state\ninformation is challenging due to sensors that do not provide state measurements\ndirectly (e.g., images as measurements), inaccurate process and observation models\nused for state estimation, and/or improper state feature representations. One open\nchallenge is to account for state estimation errors and learned process and observa-\ntion models in safe learning control (143). Expanding existing approaches to work\nwith (possibly high-dimensional) sensor data is essential for a broad applicability\nof these methods in robotics.\n3. Considering Scalability, and Sampling and Computational Eﬃciency.\nMany of the approaches presented here have only been demonstrated on small\ntoy problems and applying them to high-dimensional robotics problems is not triv-\nial. Moreover, in practice, we often face issues such as data sparsity, distribution\nshifts, and the optimality-complexity trade-oﬀfor real-time implementations. Ef-\nﬁcient robot learning relies on multiple factors including control architecture de-\nsign (154), systematic training data collection (155), and appropriate function class\nselection (156).\nWhile current approaches focus on providing theoretical safety\nguarantees, formal analysis of sampling complexity and computational complexity\nis indispensable to facilitate the implementation of safe learning control algorithms\nin real-world robot applications.\nSafe Learning in Robotics\n25\n4. Verifying System and Modeling Assumptions. The safety guarantees pro-\nvided often rely on a set of assumptions (e.g., Lipschitz continuous true dynamics\nwith a known Lipschitz constant or bounded disturbance sets). It is diﬃcult to\nverify these assumptions prior to a robot’s operation. To facilitate algorithm imple-\nmentation, we also see other approximations being made (e.g., linearization, data\nassumed to be independent and identically distributed (i.i.d.) Gaussian samples).\nSystematic approaches to verify or quantify the impact of the assumptions and the\napproximations with minimal (online) data are crucial to allow the safe learning\napproaches to be applied in real-world applications. This can also include investi-\ngations into the interpretability of trained models, especially black-box models such\nas DNNs, for safe closed-loop operation (6).\nThis list of challenges is by no means complete. Other important and open questions\npertinent to safe learning control are:\n• What are the appropriate benchmarks, evaluation metrics, and practices to\nprovide practical insights and perform fair comparisons of algorithms that rely on\ndiﬀerent assumptions?\n• What should be the role of simulation in the oﬄine design and evaluation phases?\nCan simulation be used to ﬁnd safe hyperparameters?\n• How do we deﬁne safety in human-robot interaction?\n• How can data be safely used in multi-agent learning settings?\nEﬀorts that combine control theory and machine learning for safe learning control have\nbeen shown to result in improved control performance and system safety. Recent successes\nand growing interest should motivate the further development of a systematic body of\ntheory, advanced methodologies, and computational methods for safe learning in robotics,\nbringing the large potential of learning into safety-critical control and robotics applications.\nThe open challenges and questions are intended to push us further, towards a future of real-\nworld safe autonomy where robots reliably and safely function in complex environments.\nACKNOWLEDGMENTS\nThe authors would like to acknowledge the early contributions to this work by Karime\nPereida and Sepehr Samavi, the invaluable suggestions and feedback by Hallie Siegel, as\nwell as the support from the Natural Sciences and Engineering Research Council of Canada\n(NSERC), the Canada Research Chairs Program, and the CIFAR AI Chair.\nLITERATURE CITED\n1. Burnett K, Qian J, Du X, Liu L, Yoon DJ, et al. 2021. Zeus: A system description of the two-\ntime winner of the collegiate SAE autodrive competition. Journal of Field Robotics 38:139–166\n2. Boutilier JJ, Brooks SC, Janmohamed A, Byers A, Buick JE, et al. 2017. Optimizing a drone\nnetwork to deliver automated external deﬁbrillators. Circulation 135:2454–2465\n3. Dong K, Pereida K, Shkurti F, Schoellig AP. 2020. Catch the ball:\nAccurate high-speed\nmotions for mobile manipulators via inverse dynamics learning. arXiv:2003.07489 [cs.RO]\n4. Garc´ıa J, Fern, o Fern´andez. 2015. A comprehensive survey on safe reinforcement learning.\nJournal of Machine Learning Research 16:1437–1480\n5. Dulac-Arnold G, Levine N, Mankowitz DJ, Li J, Paduraru C, et al. 2021. An empirical inves-\ntigation of the challenges of real-world reinforcement learning. arXiv:2003.11881 [cs.LG]\n6. Dulac-Arnold G, Mankowitz D, Hester T. 2019. Challenges of real-world reinforcement learn-\ning. arXiv:1904.12901 [cs.LG]\nwww.dynsyslab.org\n7. Hewing L, Wabersich KP, Menner M, Zeilinger MN. 2020. Learning-based model predictive\ncontrol: Toward safe learning in control. Annual Review of Control, Robotics, and Autonomous\nSystems 3:269–296\n8. Bristow D, Tharayil M, Alleyne A. 2006. A survey of iterative learning control. IEEE Control\nSystems Magazine 26:96–114\n9. Ahn HS, Chen Y, Moore KL. 2007. Iterative learning control: Brief survey and categorization.\nIEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)\n37:1099–1121\n10. Polydoros AS, Nalpantidis L. 2017. Survey of model-based reinforcement learning: Applica-\ntions on robotics. Journal of Intelligent & Robotic Systems 86:153–173\n11. Chatzilygeroudis K, Vassiliades V, Stulp F, Calinon S, Mouret JB. 2020. A survey on policy\nsearch algorithms for learning robot controllers in a handful of trials. IEEE Transactions on\nRobotics 36:328–347\n12. Ravichandar H, Polydoros AS, Chernova S, Billard A. 2020. Recent advances in robot learning\nfrom demonstration. Annual Review of Control, Robotics, and Autonomous Systems 3:297–\n330\n13. Kober J, Bagnell JA, Peters J. 2013. Reinforcement learning in robotics: A survey. The In-\nternational Journal of Robotics Research 32:1238–1274\n14. Recht B. 2019. A tour of reinforcement learning: The view from continuous control. Annual\nReview of Control, Robotics, and Autonomous Systems 2:253–279\n15. Kiumarsi B, Vamvoudakis KG, Modares H, Lewis FL. 2018. Optimal and autonomous control\nusing reinforcement learning: A survey. IEEE Transactions on Neural Networks and Learning\nSystems 29:2042–2062\n16. Osborne M, Shin HS, Tsourdos A. 2021. A Review of Safe Online Learning for Nonlinear\nControl Systems. In 2021 International Conference on Unmanned Aircraft Systems (ICUAS),\npp. 794–803. Piscataway, NJ: IEEE\n17. Tambon F, Laberge G, An L, Nikanjam A, Mindom PSN, et al. 2021. How to certify machine\nlearning based safety-critical systems? a systematic literature review. arXiv:2107.12045 [cs.LG]\n18. Ray A, Achiam J, Amodei D. 2019. Benchmarking Safe Exploration in Deep Reinforcement\nLearning. https://cdn.openai.com/safexp-short.pdf\n19. Leike J, Martic M, Krakovna V, Ortega PA, Everitt T, et al. 2017. AI safety gridworlds.\narXiv:1711.09883 [cs.LG]\n20. Sutton RS, Barto AG. 2018. Reinforcement Learning: An Introduction. Cambridge, MA: MIT\nPress, 2nd ed.\n21. Khalil H. 2002. Nonlinear Systems. Pearson Education. Prentice Hall\n22. ˚Astr¨om K, Wittenmark B. 2011. Computer-Controlled Systems: Theory and Design, Third\nEdition. Dover Books on Electrical Engineering. Dover Publications\n23. Sastry S, Bodson M. 2011. Adaptive Control: Stability, Convergence and Robustness. Dover\nBooks on Electrical Engineering Series. Dover Publications\n24. Nguyen-Tuong D, Peters J. 2011. Model learning for robot control: a survey. Cognitive pro-\ncessing 12:319–340\n25. Zhou K, Doyle J, Glover K. 1996. Robust and Optimal Control. Prentice Hall\n26. Dullerud G, Paganini F. 2005. A Course in Robust Control Theory: A Convex Approach.\nTexts in Applied Mathematics. Springer New York\n27. Rawlings J, Mayne D, Diehl M. 2017. Model Predictive Control: Theory, Computation, and\nDesign. Nob Hill Publishing\n28. Mayne D, Seron M, Rakovi´c S. 2005. Robust model predictive control of constrained linear\nsystems with bounded disturbances. Automatica 41:219–224\n29. Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. 2017. Deep reinforcement learn-\ning: A brief survey. IEEE Signal Processing Magazine 34:26–38\n30. Dai B, Shaw A, Li L, Xiao L, He N, et al. 2018. SBEED: Convergent Reinforcement Learning\nwith Nonlinear Function Approximation. In Proceedings of the 35th International Conference\non Machine Learning, ed. J Dy, A Krause, pp. 1125–1134, vol. 80 of Proceedings of Machine\nLearning Research, pp. 1125–1134. N.p.: PMLR\n31. Cheng R, Verma A, Orosz G, Chaudhuri S, Yue Y, Burdick J. 2019. Control regularization\nfor reduced variance reinforcement learning. In Proceedings of the 36th International Con-\nference on Machine Learning, ed. K Chaudhuri, R Salakhutdinov, pp. 1141–1150, vol. 97 of\nSafe Learning in Robotics\n27\nProceedings of Machine Learning Research. N.p.: PMLR\n32. Ghavamzadeh M, Mannor S, Pineau J, Tamar A. 2015. Bayesian reinforcement learning: A\nsurvey. Foundations and Trends® in Machine Learning 8:359–483\n33. Altman E. 1999. Constrained Markov decision processes, vol. 7. Boca Raton, FL: Chapman\n& Hall/CRC Press\n34. Achiam J, Held D, Tamar A, Abbeel P. 2017. Constrained policy optimization. In Proceedings\nof the 34th International Conference on Machine Learning, ed. D Precup, YW Teh, pp. 22–31,\nvol. 70 of Proceedings of Machine Learning Research. N.p.: PMLR\n35. Nilim A, El Ghaoui L. 2005. Robust control of Markov decision processes with uncertain\ntransition matrices. Operations Research 53:780–798\n36. Pinto L, Davidson J, Sukthankar R, Gupta A. 2017. Robust adversarial reinforcement learning.\nIn Proceedings of the 34th International Conference on Machine Learning, ed. D Precup,\nYW Teh, pp. 2817–2826, vol. 70 of Proceedings of Machine Learning Research. N.p.: PMLR\n37. Pan X, Seita D, Gao Y, Canny J. 2019. Risk Averse Robust Adversarial Reinforcement Learn-\ning. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8522–8528.\nPiscataway, NJ: IEEE\n38. Vinitsky E, Du Y, Parvate K, Jang K, Abbeel P, Bayen A. 2020. Robust reinforcement learning\nusing adversarial populations. arXiv:2008.01825 [cs.LG]\n39. Cooper J, Che J, Cao C. 2014. The use of learning in fast adaptation algorithms. International\nJournal of Adaptive Control and Signal Processing 28:325–340\n40. Gahlawat A, Zhao P, Patterson A, Hovakimyan N, Theodorou E. 2020. L1-GP: L1 adaptive\ncontrol with Bayesian learning. In Proceedings of the 2nd Conference on Learning for Dy-\nnamics and Control, ed. AM Bayen, A Jadbabaie, G Pappas, PA Parrilo, B Recht, C Tomlin,\nM Zeilinger, pp. 826–837, vol. 120 of Proceedings of Machine Learning Research. N.p.: PMLR\n41. Hovakimyan N, Cao C. 2010. L1 Adaptive Control Theory: Guaranteed Robustness with Fast\nAdaptation. USA: Society for Industrial and Applied Mathematics\n42. Grande RC, Chowdhary G, How JP. 2014. Experimental validation of Bayesian nonparametric\nadaptive control using Gaussian processes. Journal of Aerospace Information Systems 11:565–\n578\n43. Chowdhary G, Kingravi HA, How JP, Vela PA. 2015. Bayesian nonparametric adaptive con-\ntrol using Gaussian processes. IEEE Transactions on Neural Networks and Learning Systems\n26:537–550\n44. Joshi G, Chowdhary G. 2019. Deep model reference adaptive control. In 2019 IEEE 58th\nConference on Decision and Control (CDC), pp. 4601–4608. Piscataway, NJ: IEEE\n45. Joshi G, Virdi J, Chowdhary G. 2020. Asynchronous deep model reference adaptive control.\narXiv:2011.02920 [cs.RO]\n46. Berkenkamp F, Schoellig AP. 2015. Safe and robust learning control with Gaussian processes.\nIn 2015 European Control Conference (ECC), pp. 2496–2501. Piscataway, NJ: IEEE\n47. Holicki T, Scherer CW, Trimpe S. 2021. Controller design via experimental exploration with\nrobustness guarantees. IEEE Control Systems Letters 5:641–646\n48. von Rohr A, Neumann-Brosig M, Trimpe S. 2021. Probabilistic robust linear quadratic regu-\nlators with Gaussian processes. In Proceedings of the 3rd Conference on Learning for Dynam-\nics and Control, ed. A Jadbabaie, J Lygeros, GJ Pappas, PA Parrilo, B Recht, CJ Tomlin,\nMN Zeilinger, pp. 324–335, vol. 144 of Proceedings of Machine Learning Research. N.p.: PMLR\n49. Helwa MK, Heins A, Schoellig AP. 2019. Provably robust learning-based approach for high-\naccuracy tracking control of lagrangian systems. IEEE Robotics and Automation Letters\n4:1587–1594\n50. GreeﬀM, Schoellig AP. 2021. Exploiting diﬀerential ﬂatness for robust learning-based tracking\ncontrol using Gaussian processes. IEEE Control Systems Letters 5:1121–1126\n51. Tanaskovic M, Fagiano L, Smith R, Morari M. 2014. Adaptive receding horizon control for\nconstrained MIMO systems. Automatica 50:3019–3029\n52. Lorenzen M, Cannon M, Allg¨ower F. 2019. Robust MPC with recursive model update. Auto-\nmatica 103:461–471\n53. Bujarbaruah M, Zhang X, Borrelli F. 2018. Adaptive MPC with Chance Constraints for FIR\nSystems. In 2018 Annual American Control Conference (ACC), pp. 2312–2317. Piscataway,\nNJ: IEEE\n54. Bujarbaruah M, Zhang X, Tanaskovic M, Borrelli F. 2019. Adaptive MPC under Time Varying\nwww.dynsyslab.org\nUncertainty: Robust and Stochastic. arXiv:1909.13473 [eess.SY]\n55. Gon¸calves GA, Guay M. 2016. Robust discrete-time set-based adaptive predictive control for\nnonlinear systems. Journal of Process Control 39:111–122\n56. K¨ohler J, K¨otting P, Soloperto R, Allg¨ower F, M¨uller MA. 2020. A robust adaptive model\npredictive control framework for nonlinear uncertain systems. International Journal of Robust\nand Nonlinear Control https://doi.org/10.1002/rnc.5147\n57. Rosolia U, Borrelli F. 2018. Learning model predictive control for iterative tasks. a data-driven\ncontrol framework. IEEE Transactions on Automatic Control 63:1883–1896\n58. Bujarbaruah M, Zhang X, Rosolia U, Borrelli F. 2018. Adaptive MPC for Iterative Tasks.\nIn 2018 IEEE Conference on Decision and Control (CDC), pp. 6322–6327. Piscataway, NJ:\nIEEE\n59. Pereida K, Brunke L, Schoellig AP. 2021. Robust adaptive model predictive control for guar-\nanteed fast and accurate stabilization in the presence of model errors. International Journal\nof Robust and Nonlinear Control in-press\n60. Aswani A, Gonzalez H, Sastry SS, Tomlin C. 2013. Provably safe and robust learning-based\nmodel predictive control. Automatica 49:1216–1226\n61. Soloperto R, M¨uller MA, Trimpe S, Allg¨ower F. 2018. Learning-based robust model predictive\ncontrol with state-dependent uncertainty. IFAC-PapersOnLine 51(20):442–447\n62. Ostafew CJ, Schoellig AP, Barfoot TD. 2016. Robust constrained learning-based NMPC en-\nabling reliable mobile robot path tracking. The International Journal of Robotics Research\n35:1547–1563\n63. Hewing L, Kabzan J, Zeilinger MN. 2020. Cautious model predictive control using Gaussian\nprocess regression. IEEE Transactions on Control Systems Technology 28:2736–2743\n64. Kamthe S, Deisenroth M. 2018. Data-eﬃcient reinforcement learning with probabilistic model\npredictive control. In Proceedings of the Twenty-First International Conference on Artiﬁcial\nIntelligence and Statistics, ed. A Storkey, F Perez-Cruz, pp. 1701–1710, vol. 84 of Proceedings\nof Machine Learning Research. N.p.: PMLR\n65. Koller T, Berkenkamp F, Turchetta M, Boedecker J, Krause A. 2019. Learning-based model\npredictive control for safe exploration and reinforcement learning. arXiv:1906.12189 [eess.SY]\n66. Fan D, Agha A, Theodorou E. 2020. Deep Learning Tubes for Tube MPC. In Proceedings of\nRobotics: Science and Systems XVI, pp. pap. 87. N.p.: Robot. Sci. Syst. Found.\n67. McKinnon CD, Schoellig AP. 2020. Context-aware Cost Shaping to Reduce the Impact of Model\nError in Receding Horizon Control. In 2020 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 2386–2392. Piscataway, NJ: IEEE\n68. Berkenkamp F, Turchetta M, Schoellig A, Krause A. 2017. Safe model-based reinforcement\nlearning with stability guarantees. In Advances in Neural Information Processing Systems,\ned. I Guyon, UV Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, pp.\n908–919, vol. 30. Red Hook, NY, USA: Curran Associates, Inc.\n69. Turchetta M, Berkenkamp F, Krause A. 2016. Safe exploration in ﬁnite Markov decision pro-\ncesses with Gaussian processes. In Advances in Neural Information Processing Systems, ed.\nDD Lee, M Sugiyama, UV Luxburg, I Guyon, R Garnett, pp. 4312–4320, vol. 29. Red Hook,\nNY, USA: Curran Associates, Inc.\n70. Dalal G, Dvijotham K, Vecerik M, Hester T, Paduraru C, Tassa Y. 2018. Safe exploration in\ncontinuous action spaces. arXiv:1801.08757 [cs.AI]\n71. Henderson P, Islam R, Bachman P, Pineau J, Precup D, Meger D. 2018. Deep Reinforcement\nLearning That Matters. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol.\n32(1). Palo Alto, CA: AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/view/11694\n72. Mezi´c I. 2003. Controllability, integrability and ergodicity. In Multidisciplinary Research in\nControl: The Mohammed Dahleh Symposium 2002, ed. L Giarr´e, B Bamieh, pp. 213–229.\nBerlin, Heidelberg: Springer Berlin Heidelberg\n73. Moldovan TM, Abbeel P. 2012. Safe Exploration in Markov Decision Processes. In Proceedings\nof the 29th International Conference on Machine Learning (ICML), pp. 1451–1458. Madison,\nWI: Omnipress\n74. Brafman RI, Tennenholtz M. 2002. R-max-a general polynomial time algorithm for near-\noptimal reinforcement learning. Journal of Machine Learning Research 3:213–231\n75. Pham TH, De Magistris G, Tachibana R. 2018. OptLayer - Practical Constrained Optimization\nfor Deep Reinforcement Learning in the Real World. In 2018 IEEE International Conference\nSafe Learning in Robotics\n29\non Robotics and Automation (ICRA), pp. 6236–6243. Piscataway, NJ: IEEE\n76. Kim Y, Allmendinger R, L´opez-Ib´a˜nez M. 2021. Safe learning and optimization techniques:\nTowards a survey of the state of the art. arXiv:2101.09505 [cs.LG]\n77. Duivenvoorden RR, Berkenkamp F, Carion N, Krause A, Schoellig AP. 2017. Constrained\nBayesian optimization with particle swarms for safe adaptive controller tuning. IFAC-\nPapersOnLine 50(1):11800–11807\n78. Sui Y, Gotovos A, Burdick J, Krause A. 2015. Safe exploration for optimization with Gaussian\nprocesses. In Proceedings of the 32nd International Conference on Machine Learning, ed.\nF Bach, D Blei, pp. 997–1005, vol. 37 of Proceedings of Machine Learning Research. N.p.:\nPMLR\n79. Berkenkamp F, Krause A, Schoellig AP. 2020. Bayesian optimization with safety constraints:\nSafe and automatic parameter tuning in robotics. arXiv:1602.04450 [cs.RO]\n80. Sui Y, vincent Zhuang, Burdick J, Yue Y. 2018. Stagewise safe Bayesian optimization with\nGaussian processes. In Proceedings of the 35th International Conference on Machine Learning,\ned. J Dy, A Krause, pp. 4781–4789, vol. 80 of Proceedings of Machine Learning Research. N.p.:\nPMLR\n81. Baumann D, Marco A, Turchetta M, Trimpe S. 2021. GoSafe: Globally optimal safe robot\nlearning. arXiv:2105.13281 [cs.RO]\n82. Wachi\nA,\nSui\nY,\nYue\nY,\nOno\nM.\n2018.\nSafe\nExploration\nand\nOptimization\nof\nConstrained\nMDPs\nUsing\nGaussian\nProcesses.\nIn\nProceedings\nof\nthe\nAAAI\nConference\non\nArtiﬁcial\nIntelligence,\nvol.\n32(1).\nPalo\nAlto,\nCA:\nAAAI\nPress.\nhttps://ojs.aaai.org/index.php/AAAI/article/view/12103\n83. Kumar A, Zhou A, Tucker G, Levine S. 2020. Conservative Q-learning for oﬄine reinforcement\nlearning. arXiv:2006.04779 [cs.LG]\n84. Chua K, Calandra R, McAllister R, Levine S. 2018. Deep reinforcement learning in a handful\nof trials using probabilistic dynamics models. In Advances in Neural Information Processing\nSystems, ed. S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, pp.\n4759–4770, vol. 31. Red Hook, NY, USA: Curran Associates, Inc.\n85. Srinivasan K, Eysenbach B, Ha S, Tan J, Finn C. 2020. Learning to be safe: Deep RL with a\nsafety critic. arXiv:2010.14603 [cs.LG]\n86. Thananjeyan B, Balakrishna A, Nair S, Luo M, Srinivasan K, et al. 2021. Recovery RL: Safe\nreinforcement learning with learned recovery zones. IEEE Robotics and Automation Letters\n6:4915–4922\n87. Bharadhwaj H, Kumar A, Rhinehart N, Levine S, Shkurti F, Garg A. 2021. Conservative\nsafety critics for exploration. arXiv:2010.14497 [cs.LG]\n88. Chow Y, Ghavamzadeh M, Janson L, Pavone M. 2017. Risk-constrained reinforcement learning\nwith percentile risk criteria. Journal of Machine Learning Research 18:1–51\n89. Kahn G, Villaﬂor A, Pong V, Abbeel P, Levine S. 2017. Uncertainty-aware reinforcement\nlearning for collision avoidance. arXiv:1702.01182 [cs.LG]\n90. L¨utjens B, Everett M, How JP. 2019. Safe Reinforcement Learning With Model Uncertainty\nEstimates. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8662–\n8668. Piscataway, NJ: IEEE\n91. Zhang J, Cheung B, Finn C, Levine S, Jayaraman D. 2020. Cautious adaptation for reinforce-\nment learning in safety-critical settings. In Proceedings of the 37th International Conference on\nMachine Learning, ed. HD III, A Singh, pp. 11055–11065, vol. 119 of Proceedings of Machine\nLearning Research. N.p.: PMLR\n92. Thananjeyan B, Balakrishna A, Rosolia U, Li F, McAllister R, et al. 2020. Safety augmented\nvalue estimation from demonstrations (SAVED): Safe deep model-based rl for sparse cost\nrobotic tasks. IEEE Robotics and Automation Letters 5:3612–3619\n93. Urp´ı NA, Curi S, Krause A. 2021. Risk-averse oﬄine reinforcement learning. arXiv:2102.05371\n[cs.LG]\n94. Bellemare MG, Dabney W, Munos R. 2017. A Distributional Perspective on Reinforcement\nLearning. In Proceedings of the 34th International Conference on Machine Learning, ed.\nD Precup, YW Teh, pp. 449–458, vol. 70 of Proceedings of Machine Learning Research, pp.\n449–458. N.p.: PMLR\n95. Liang Q, Que F, Modiano E. 2018. Accelerated primal-dual policy optimization for safe rein-\nforcement learning. arXiv:1802.06480 [cs.AI]\nwww.dynsyslab.org\n96. Schulman J, Levine S, Abbeel P, Jordan M, Moritz P. 2015. Trust region policy optimization.\nIn Proceedings of the 32nd International Conference on Machine Learning, ed. F Bach, D Blei,\npp. 1889–1897, vol. 37 of Proceedings of Machine Learning Research. N.p.: PMLR\n97. Chow Y, Nachum O, Duenez-Guzman E, Ghavamzadeh M. 2018. A Lyapunov-based approach\nto safe reinforcement learning. In Advances in Neural Information Processing Systems, ed.\nS Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, pp. 8103–8112,\nvol. 31. Red Hook, NY, USA: Curran Associates, Inc.\n98. Chow Y, Nachum O, Faust A, Duenez-Guzman E, Ghavamzadeh M. 2019. Lyapunov-based\nsafe policy optimization for continuous control. arXiv:1901.10031 [cs.LG]\n99. Satija H, Amortila P, Pineau J. 2020. Constrained Markov decision processes via backward\nvalue functions. In Proceedings of the 37th International Conference on Machine Learning,\ned. HD III, A Singh, pp. 8502–8511, vol. 119 of Proceedings of Machine Learning Research.\nN.p.: PMLR\n100. Morimoto J, Doya K. 2005. Robust reinforcement learning. Neural Computation 17:335–359\n101. Turchetta M, Krause A, Trimpe S. 2020. Robust Model-free Reinforcement Learning with\nMulti-objective Bayesian Optimization. In 2020 IEEE International Conference on Robotics\nand Automation (ICRA), pp. 10702–10708. Piscataway, NJ: IEEE\n102. Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, et al. 2014. Generative\nadversarial networks. arXiv:1406.2661 [stat.ML]\n103. Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, et al. 2015. Human-level control\nthrough deep reinforcement learning. Nature 518:529–533\n104. L¨utjens B, Everett M, How JP. 2020. Certiﬁed adversarial robustness for deep reinforcement\nlearning. In Proceedings of the Conference on Robot Learning, ed. LP Kaelbling, D Kragic,\nK Sugiura, pp. 1328–1337, vol. 100 of Proceedings of Machine Learning Research. N.p.: PMLR\n105. Sadeghi F, Levine S. 2017. CAD2RL: Real Single-Image Flight Without a Single Real Image.\nIn Proceedings of Robotics: Science and Systems XIII, pp. pap. 34. N.p.: Robot. Sci. Syst.\nFound.\n106. Loquercio A, Kaufmann E, Ranftl R, Dosovitskiy A, Koltun V, Scaramuzza D. 2020. Deep\ndrone racing: From simulation to reality with domain randomization. IEEE Transactions on\nRobotics 36:1–14\n107. Rajeswaran A, Ghotra S, Ravindran B, Levine S. 2017. EPOpt: Learning robust neural net-\nwork policies using model ensembles. arXiv:1610.01283 [cs.LG]\n108. Mehta B, Diaz M, Golemo F, Pal CJ, Paull L. 2020. Active domain randomization. In Pro-\nceedings of the Conference on Robot Learning, ed. LP Kaelbling, D Kragic, K Sugiura, pp.\n1162–1176, vol. 100 of Proceedings of Machine Learning Research. N.p.: PMLR\n109. Zhou S, Helwa MK, Schoellig AP. 2020. Deep neural networks as add-on modules for enhancing\nrobot performance in impromptu trajectory tracking. The International Journal of Robotics\nResearch 39:1397–1418\n110. Jin M, Lavaei J. 2020. Stability-certiﬁed reinforcement learning: A control-theoretic perspec-\ntive. IEEE Access 8:229086–229100\n111. Shi G, Shi X, O’Connell M, Yu R, Azizzadenesheli K, et al. 2019. Neural Lander: Stable Drone\nLanding Control Using Learned Dynamics. In 2019 International Conference on Robotics and\nAutomation (ICRA), pp. 9784–9790. Piscataway, NJ: IEEE\n112. Fazlyab M, Robey A, Hassani H, Morari M, Pappas GJ. 2019. Eﬃcient and accurate estimation\nof Lipschitz constants for deep neural networks. arXiv:1906.04893 [cs.LG]\n113. Richards SM, Berkenkamp F, Krause A. 2018. The Lyapunov neural network: Adaptive stabil-\nity certiﬁcation for safe learning of dynamical systems. In Proceedings of The 2nd Conference\non Robot Learning, ed. A Billard, A Dragan, J Peters, J Morimoto, pp. 466–476, vol. 87 of\nProceedings of Machine Learning Research. N.p.: PMLR\n114. Zhou Z, Oguz OS, Leibold M, Buss M. 2020. A general framework to increase safety of learning\nalgorithms for dynamical systems based on region of attraction estimation. IEEE Transactions\non Robotics 36:1472–1490\n115. Jarvis-Wloszek Z, Feeley R, Tan W, Sun K, Packard A. 2003. Some controls applications\nof sum of squares programming. In 42nd IEEE International Conference on Decision and\nControl (CDC), vol. 5, pp. 4676–4681 Vol.5. Piscataway, NJ: IEEE\n116. Schilders WH, Van der Vorst HA, Rommes J. 2008. Model order reduction: theory, research\naspects and applications, vol. 13. Springer-Verlag Berlin Heidelberg\nSafe Learning in Robotics\n31\n117. Alshiekh\nM,\nBloem\nR,\nUdiger\nEhlers\nR,\nK¨onighofer\nB,\nNiekum\nS,\nTopcu\nU.\n2018.\nSafe\nReinforcement\nLearning\nvia\nShielding.\nIn\nProceedings\nof\nthe\nAAAI\nConference\non\nArtiﬁcial\nIntelligence,\nvol.\n32(1).\nPalo\nAlto,\nCA:\nAAAI\nPress.\nhttps://ojs.aaai.org/index.php/AAAI/article/view/11797\n118. Ames AD, Coogan S, Egerstedt M, Notomista G, Sreenath K, Tabuada P. 2019. Control\nBarrier Functions:\nTheory and Applications. In 2019 18th European Control Conference\n(ECC), pp. 3420–3431. Piscataway, NJ: IEEE\n119. Taylor AJ, Dorobantu VD, Le HM, Yue Y, Ames AD. 2019. Episodic Learning with Con-\ntrol Lyapunov Functions for Uncertain Robotic Systems*. In 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp. 6878–6884. Piscataway, NJ: IEEE\n120. Taylor A, Singletary A, Yue Y, Ames A. 2020. Learning for safety-critical control with control\nbarrier functions. In Proceedings of the 2nd Conference on Learning for Dynamics and Control,\ned. AM Bayen, A Jadbabaie, G Pappas, PA Parrilo, B Recht, C Tomlin, M Zeilinger, pp. 708–\n717, vol. 120 of Proceedings of Machine Learning Research. N.p.: PMLR\n121. Ohnishi M, Wang L, Notomista G, Egerstedt M. 2019. Barrier-certiﬁed adaptive reinforcement\nlearning with applications to brushbot navigation. IEEE Transactions on Robotics 35:1186–\n1205\n122. Choi J, Casta˜neda F, Tomlin C, Sreenath K. 2020. Reinforcement Learning for Safety-Critical\nControl under Model Uncertainty, using Control Lyapunov Functions and Control Barrier\nFunctions. In Proceedings of Robotics: Science and Systems XVI, pp. pap. 88. N.p.: Robot.\nSci. Syst. Found.\n123. Taylor AJ, Dorobantu VD, Krishnamoorthy M, Le HM, Yue Y, Ames AD. 2019. A Control\nLyapunov Perspective on Episodic Learning via Projection to State Stability. In 2019 IEEE\n58th Conference on Decision and Control (CDC), pp. 1448–1455. Piscataway, NJ: IEEE\n124. Taylor AJ, Singletary A, Yue Y, Ames AD. 2020. A control barrier perspective on episodic\nlearning via projection-to-state safety. arXiv:2003.08028 [eess.SY]\n125. Taylor AJ, Dorobantu VD, Dean S, Recht B, Yue Y, Ames AD. 2020. Towards robust data-\ndriven control synthesis for nonlinear systems with actuation uncertainty. arXiv:2011.10730\n[eess.SY]\n126. Taylor AJ, Ames AD. 2020. Adaptive Safety with Control Barrier Functions. In 2020 American\nControl Conference (ACC), pp. 1399–1405. Piscataway, NJ: IEEE\n127. Lopez BT, Slotine JJE, How JP. 2021. Robust adaptive control barrier functions: An adaptive\nand data-driven approach to safety. IEEE Control Systems Letters 5:1031–1036\n128. Cheng R, Orosz G, Murray RM, Burdick JW. 2019. End-to-end safe reinforcement learning\nthrough barrier functions for safety-critical continuous control tasks. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, vol. 33(1), pp. 3387–3395. Palo Alto, CA: AAAI\nPress\n129. Fan DD, Nguyen J, Thakker R, Alatur N, Agha-mohammadi Aa, Theodorou EA. 2019.\nBayesian learning-based adaptive control for safety critical systems. arXiv:1910.02325 [eess.SY]\n130. Wang L, Theodorou EA, Egerstedt M. 2018. Safe learning of quadrotor dynamics using barrier\ncertiﬁcates. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp.\n2460–2465. Piscataway, NJ: IEEE\n131. Khojasteh MJ, Dhiman V, Franceschetti M, Atanasov N. 2020. Probabilistic safety constraints\nfor learned high relative degree system dynamics. In Proceedings of the 2nd Conference on\nLearning for Dynamics and Control, ed. AM Bayen, A Jadbabaie, G Pappas, PA Parrilo,\nB Recht, C Tomlin, M Zeilinger, pp. 781–792, vol. 120 of Proceedings of Machine Learning\nResearch. N.p.: PMLR\n132. Mitchell I, Bayen A, Tomlin C. 2005. A time-dependent Hamilton-Jacobi formulation of reach-\nable sets for continuous dynamic games. IEEE Transactions on Automatic Control 50:947–957\n133. Fisac JF, Akametalu AK, Zeilinger MN, Kaynama S, Gillula J, Tomlin CJ. 2019. A general\nsafety framework for learning-based control in uncertain robotic systems. IEEE Transactions\non Automatic Control 64:2737–2752\n134. Gillula JH, Tomlin CJ. 2012. Guaranteed safe online learning via reachability: tracking a\nground target using a quadrotor. In 2012 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 2723–2730. Piscataway, NJ: IEEE\n135. Bajcsy A, Bansal S, Bronstein E, Tolani V, Tomlin CJ. 2019. An Eﬃcient Reachability-Based\nFramework for Provably Safe Autonomous Navigation in Unknown Environments. In 2019\nwww.dynsyslab.org\nIEEE 58th Conference on Decision and Control (CDC), pp. 1758–1765. Piscataway, NJ: IEEE\n136. Fisac JF, Lugovoy NF, Rubies-Royo V, Ghosh S, Tomlin CJ. 2019. Bridging Hamilton-Jacobi\nsafety analysis and reinforcement learning. In 2019 International Conference on Robotics and\nAutomation (ICRA), pp. 8550–8556. Piscataway, NJ: IEEE\n137. Choi JJ, Lee D, Sreenath K, Tomlin CJ, Herbert SL. 2021. Robust control barrier-value func-\ntions for safety-critical control. arXiv:2104.02808 [eess.SY]\n138. Herbert S, Choi JJ, Sanjeev S, Gibson M, Sreenath K, Tomlin CJ. 2021. Scalable\nlearning of safety guarantees for autonomous systems using Hamilton-Jacobi reachability.\narXiv:2101.05916 [cs.RO]\n139. Wabersich KP, Zeilinger MN. 2018. Linear Model Predictive Safety Certiﬁcation for Learning-\nBased Control. In 2018 IEEE Conference on Decision and Control (CDC), pp. 7130–7135.\nPiscataway, NJ: IEEE\n140. Wabersich KP, Hewing L, Carron A, Zeilinger MN. 2019. Probabilistic model predictive safety\ncertiﬁcation for learning-based control. arXiv:1906.10417 [eess.SY]\n141. Wabersich KP, Zeilinger MN. 2021. A predictive safety ﬁlter for learning-based control of\nconstrained nonlinear dynamical systems. Automatica 129:109597\n142. Mannucci T, van Kampen E, de Visser C, Chu Q. 2018. Safe exploration algorithms for rein-\nforcement learning controllers. IEEE Transactions on Neural Networks and Learning Systems\n29:1069–1081\n143. Dean S, Taylor AJ, Cosner RK, Recht B, Ames AD. 2020. Guaranteeing safety of learned per-\nception modules via measurement-robust control barrier functions. arXiv:2010.16001 [eess.SY]\n144. Liu CK, Negrut D. 2021. The role of physics-based simulators in robotics. Annual Review of\nControl, Robotics, and Autonomous Systems 4:35–58\n145. Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, et al. 2016. OpenAI Gym.\narXiv:1606.01540 [cs.LG]\n146. Panerati J, Zheng H, Zhou S, Xu J, Prorok A, Schoellig AP. 2021. Learning to Fly—a Gym En-\nvironment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Con-\ntrol. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).\nPiscataway, NJ: IEEE\n147. Coumans E, Bai Y. 2016–2021. PyBullet, a Python module for physics simulation for games,\nrobotics and machine learning. http://pybullet.org\n148. Andersson JAE, Gillis J, Horn G, Rawlings JB, Diehl M. 2019. CasADi – A software frame-\nwork for nonlinear optimization and optimal control. Mathematical Programming Computation\n11:1–36\n149. Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. 2017. Proximal policy optimization\nalgorithms. arXiv:1707.06347 [cs.LG]\n150. Wieber PB, Tedrake R, Kuindersma S. 2016. Modeling and control of legged robots. In\nSpringer Handbook of Robotics, ed. B Siciliano, O Khatib, pp. 1203–1234. Cham: Springer\nInternational Publishing\n151. McKinnon CD, Schoellig AP. 2018. Experience-Based Model Selection to Enable Long-Term,\nSafe Control for Repetitive Tasks Under Changing Conditions. In 2018 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems (IROS), pp. 2977–2984. Piscataway, NJ:\nIEEE\n152. Chandak Y, Jordan S, Theocharous G, White M, Thomas PS. 2020. Towards safe policy im-\nprovement for non-stationary MDPs. In Advances in Neural Information Processing Systems,\ned. H Larochelle, M Ranzato, R Hadsell, MF Balcan, H Lin, pp. 9156–9168, vol. 33. Red Hook,\nNY, USA: Curran Associates, Inc.\n153. Burgner-Kahrs J, Rucker DC, Choset H. 2015. Continuum robots for medical applications: A\nsurvey. IEEE Transactions on Robotics 31:1261–1280\n154. Mueller FL, Schoellig AP, D’Andrea R. 2012. Iterative learning of feed-forward corrections for\nhigh-performance tracking. In 2012 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pp. 3276–3281. Piscataway, NJ: IEEE\n155. Dean S, Tu S, Matni N, Recht B. 2018. Safely learning to control the constrained linear\nquadratic regulator. arXiv :5582–5588\n156. McKinnon CD, Schoellig AP. 2019. Learning Probabilistic Models for Safe Predictive Control\nin Unknown Environments. In 2019 18th European Control Conference (ECC), pp. 2472–2479.\nPiscataway, NJ: IEEE\nSafe Learning in Robotics\n33\nA. SUMMARY OF REVIEWED LITERATURE\nTable 1 summarizes the safe learning control approaches reviewed in this paper. Learning\napproaches are classiﬁed as Model-Based if a dynamics model is used to produce control\ninputs or Model-Free if there is a direct mapping from states or measurements to control\ninputs. For model-based approaches, we categorize the structure of the A Priori and the\nLearned component of the model as Gaussian process (GP), neural network (NN ), or Other\nfor non-standard methods (e.g., set computation from data).\nThe Safety Properties category highlights which algorithms can handle state and/or\ninput constraints (Constraint Satisfaction) and which ones provide stability guarantees\n(Stability). For both of these properties, we report whether they are enforced while the\npolicy is being trained (During Learning) or by the resultant policy (After Learning). The\nRobustness category indicates whether the ﬁnal policy is designed to be robust to Input\ndisturbances; model Parameter uncertainty, and/or other Dynamics uncertainties broader\nthan the previous two types of disturbances.\nFinally, the last category reports the Task(s) to which each method was applied: Sta-\nbilization for stabilizing an equilibrium of the system; Tracking for the tracking of a given\ntrajectory; Navigation for navigating or planning a sequence of actions to reach a given\ngoal; Locomotion for the movement of legged robots like humanoids or quadrupeds; and\nManipulation for pushing, reaching, or grasping operations using a robotic arm.\nwww.dynsyslab.org\nTable 1: Summary of the key properties of the works referenced in Sec. 3\nSection\nLearning\nSafety Properties\nTask\nModel-based\nModel-free\nDuring Learning\nAfter Learning\nA Priori\nLearned\nConstraint\nStability\nConstraint\nStability\nRobustness\nSatisfaction\nSatisfaction\nw.r.t.\nLearning\nUncertain\nDynamics\nSec. 3.1\nSec.\n3.1.1\nLinear (39, 40,\n44, 45), Nonlin-\near (42, 43)\nNN\n(39,\n44,\n45), GP (40,\n42, 43)\n(39,\n40,\n42, 43, 44,\n45)\n(39,\n40,\n42, 43, 44,\n45)\nInput (39, 40),\nDynamics (42,\n43, 44, 45)\nTracking (39, 40, 42, 43, 44,\n45)\nSec.\n3.1.2\nLinear (46, 47,\n48),\nNonlin-\near (49, 50)\nGP\n(46,\n48,\n49,\n50)\nOther (47)\n(46,\n48,\n49, 50)\n(46,\n47,\n48, 49, 50)\nDynamics (46,\n47, 48, 49, 50)\nStabilization (46, 47, 48),\nTracking (49, 50)\nSec.\n3.1.3\nLinear (51, 52,\n53, 54, 58, 59,\n60, 61), Nonlin-\near (55, 56, 62,\n63, 64, 65, 66,\n67)\nNN (60, 66),\nGP\n(61,\n62,\n63,\n64,\n65),\nOther (51, 52,\n53, 54, 55, 56,\n58, 59, 67)\nInput & state\n(51,\n52,\n53,\n54,\n55,\n56,\n58, 59, 60, 61,\n62, 63, 64, 65,\n66)\n(51,\n52,\n53, 54, 55,\n56, 58, 59,\n60, 61, 62,\n63, 64, 65,\n66)\nInput & state\n(51, 52, 53, 54,\n55, 56, 58, 59,\n60, 61, 62, 63,\n64, 65, 66, 67)\n(51,\n52,\n53, 54, 55,\n56, 58, 59,\n60, 61, 62,\n63, 64, 65,\n66, 67)\nInput\n(59),\nDynamics (60,\n61, 62, 63, 64,\n65, 66, 67), Pa-\nrameters\n(51,\n52, 53, 54, 55,\n56, 58, 59)\nStabilization (52, 53, 54, 55,\n56, 58, 59, 60, 61, 64, 65),\nTracking (51, 62, 63, 66, 67)\nSec.\n3.1.4\nNonlinear (68)\nGP (68)\n(68)\n(68)\nStabilization (68)\nEncouraging\nSafety\nin RL\nSec. 3.2\nSec.\n3.2.1\nNonlinear\n(69,\n79, 82),\nGP\n(78,\n79,\n80, 81)\nNN (70, 75,\n85, 86, 87),\nTabu-\nlar (73)\nInput & state\n(69,\n70,\n73,\n75, 78, 79, 80,\n81, 82) Tra-\njectory (87)\nInput & state\n(69,\n70,\n73,\n75, 78, 79, 80,\n81, 82) Trajec-\ntory\n(85,\n86,\n87)\nStabilization\n(81),\nTrack-\ning\n(79),\nNavigation\n(69,\n70, 73, 82, 85, 86), Loco-\nmotion (85, 87), Manipula-\ntion (75, 85, 86, 87)\nSec.\n3.2.2\nNonlinear\n(89,\n90)\nNN\n(89,\n90,\n91, 92)\nNN (93)\nInput & state\n(89, 90, 91, 92)\nDynamics (93)\nStabilization (91), Naviga-\ntion (89, 90, 91, 92), Loco-\nmotion (91, 93), Manipula-\ntion (92)\nSec.\n3.2.3\nTabular\n(97,\n99)\nNN (34, 95,\n97, 98, 99)\nTrajectory (34,\n97, 98, 99)\nTrajectory (34,\n95, 97, 98, 99)\nNavigation (34, 95, 97, 98,\n99), Locomotion (34, 98, 99)\nSec.\n3.2.4\nNN (106)\nNN\n(36,\n37, 38, 104,\n105,\n107,\n108)\nInput & state\n(106)\n(106)\nInput & state\n(106)\n(106)\nInput (37, 38),\nDynamics (36,\n104, 105, 106),\nParame-\nters (107, 108)\nStabilization (36, 104, 108),\nTracking\n(106),\nNaviga-\ntion\n(104,\n105),\nLocomo-\ntion\n(36,\n37,\n38,\n107),\nManipulation (108)\nCertifying\nLearning-\nbased\nControl\nSec. 3.3\nSec.\n3.3.1\nLinear\n(110),\nNonlinear (109,\n111, 113, 114)\nNN (109, 110,\n111,\n113),\nOther (114)\n(110)\n(109, 110,\n111,\n113,\n114)\nDynamics (109,\n110, 111, 113,\n114)\nStabilization\n(113,\n114),\nTracking\n(109,\n110,\n111,\n114), Locomotion (114)\nSec.\n3.3.2\nLinear\n(139,\n140),\nNonlin-\near (119, 120,\n121, 122, 123,\n124, 125, 126,\n127, 128, 129,\n130, 133, 134,\n135, 141, 142)\nGP\n(121,\n128, 130, 131,\n133,\n140),\nNN (119, 120,\n122, 123, 124,\n128, 129, 135)\nOther\n(125,\n126, 127, 134,\n139, 140, 141)\nNN\n(121,\n122, 136)\nInput\n&\nstate\n(123,\n124, 125, 126,\n127, 128, 129,\n130, 139, 140,\n141,\n142),\nState\n(133,\n134, 135)\n(123, 124,\n125,\n126,\n127,\n128,\n129,\n130,\n133,\n134,\n135,\n139,\n140, 141)\nInput & state\n(119,\n120,\n121, 122, 123,\n124, 125, 126,\n127, 128, 129,\n130, 131, 139,\n140, 141, 142),\nState\n(133,\n134, 135, 136)\n(119, 120,\n121,\n122,\n123,\n124,\n125,\n126,\n127,\n128,\n129,\n130,\n131,\n133,\n134,\n135,\n136,\n139,\n140, 141)\nDynamics (119,\n120, 121, 122,\n123, 124, 125,\n128, 129, 130,\n133, 134, 135,\n136, 139, 140,\n142), Parame-\nters (126, 127,\n141)\nStabilization (128, 136, 139,\n141, 142),\nTracking (119,\n120, 123, 124, 125, 126, 127,\n129, 130, 131, 133, 134, 140),\nNavigation (121, 135), Loco-\nmotion (122)\nSafe Learning in Robotics\n35\nB. REVISION HISTORY\nSend errata to angela.schoellig@robotics.utias.utoronto.ca.\nDate\nChange\nDecember 6, 2021\nFigure 6 update\nAugust 13, 2021\nInitial submission\nwww.dynsyslab.org\n",
  "categories": [
    "cs.RO",
    "cs.LG",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2021-08-13",
  "updated": "2021-12-06"
}