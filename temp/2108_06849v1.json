{
  "id": "http://arxiv.org/abs/2108.06849v1",
  "title": "Introduction to Quantum Reinforcement Learning: Theory and PennyLane-based Implementation",
  "authors": [
    "Yunseok Kwak",
    "Won Joon Yun",
    "Soyi Jung",
    "Jong-Kook Kim",
    "Joongheon Kim"
  ],
  "abstract": "The emergence of quantum computing enables for researchers to apply quantum\ncircuit on many existing studies. Utilizing quantum circuit and quantum\ndifferential programming, many research are conducted such as \\textit{Quantum\nMachine Learning} (QML). In particular, quantum reinforcement learning is a\ngood field to test the possibility of quantum machine learning, and a lot of\nresearch is being done. This work will introduce the concept of quantum\nreinforcement learning using a variational quantum circuit, and confirm its\npossibility through implementation and experimentation. We will first present\nthe background knowledge and working principle of quantum reinforcement\nlearning, and then guide the implementation method using the PennyLane library.\nWe will also discuss the power and possibility of quantum reinforcement\nlearning from the experimental results obtained through this work.",
  "text": "Introduction to Quantum Reinforcement Learning:\nTheory and PennyLane-based Implementation\n◦Yunseok Kwak, ◦Won Joon Yun, †Soyi Jung, ◦Jong-Kook Kim, and ◦Joongheon Kim\n◦School of Electrical Engineering, Korea University, Seoul, Republic of Korea\n†School of Software, Hallym University, Chungcheon, Republic of Korea\nE-mails: rhkrdbstjr0@korea.ac.kr, ywjoon95@korea.ac.kr, jungsoyi@korea.ac.kr,\njongkook@korea.ac.kr, joongheon@korea.ac.kr\nAbstract—The emergence of quantum computing enables for\nresearchers to apply quantum circuit on many existing studies.\nUtilizing quantum circuit and quantum differential program-\nming, many research are conducted such as Quantum Machine\nLearning (QML). In particular, quantum reinforcement learning\nis a good ﬁeld to test the possibility of quantum machine learning,\nand a lot of research is being done. This work will introduce the\nconcept of quantum reinforcement learning using a variational\nquantum circuit, and conﬁrm its possibility through implemen-\ntation and experimentation. We will ﬁrst present the background\nknowledge and working principle of quantum reinforcement\nlearning, and then guide the implementation method using the\nPennyLane library. We will also discuss the power and possibility\nof quantum reinforcement learning from the experimental results\nobtained through this work.\nI. INTRODUCTION\nSince deep reinforcement learning has opened a new chapter\nin reinforcement learning by leveraging the power of artiﬁ-\ncial neural networks, there have been achievements such as\nsurpassing human limits in complex games such as chess and\nGo, and it has already become an unavoidable ﬂow of artiﬁcial\nintelligence (AI) research.\nOn the other hand, advanced quantum computing tech-\nnology has already reached a level close to the imple-\nmentation of quantum computational gain predicted through\nmany algorithm studies [1]–[3]. In addition, the advent of\na variational quantum circuit (VQC) that mimics the prin-\nciples and functions of artiﬁcial neural networks has made\nit possible to apply these quantum calculations to existing\nmachine learning algorithms. This has established itself as a\nmajor trend in quantum machine learning research, and many\nstudies using it are being actively conducted. In this context,\nmany studies are conducted using VQC as a Quantum Neural\nNetwork (QNN) [4]–[7], including variational classiﬁer, image\npreprocessor, federated learning, reinforcement learning, etc.\nAmong them, in this paper, we introduce and discuss quantum\nreinforcement learning, a reinforcement learning model that\nreplaces the artiﬁcial neural network of a deep Q network\n(DQN) with a VQC.\nSince QPU has not been commercialized yet, there is\nnothing better in speed than the existing machine learn-\ning framework that utilizes NPU. However, as development\nlibraries such as Tensorﬂow-quantum [8], Qiskit [9], and\nPennyLane [10] for future quantum computing environments\nand quantum computing clouds such as IBMQ, IonQ, and\nAmazon Braket are provided to developers, various stud-\nies on QML are in progress. Particularly, PennyLane [10],\nis a suitable library for starting quantum machine learning\nresearch because it provides a simulator that allows users\nto easily implement quantum circuits by using a CPU to\nperform QPU operations. Therefore, we aim to increase access\nto quantum reinforcement learning and facilitate subsequent\nresearch by brieﬂy introducing the implementation process\nthrough PennyLane. In addition, we would like to discuss the\nimpacts and potentials of quantum computing in reinforcement\nlearning through experimentation and evaluation of quantum\nreinforcement learning models in the CartPole environment\nprovided by OpenAI.\nII. BACKGROUNDS\nA. Reinforcement Learning\nReinforcement learning is mathematically modeled with\nMarkov Decision Process (MDP) as a tuple (S, A, P, R, T),\nwhere S is a ﬁnite set of state information, and A is a ﬁnite\nset of action information. The function P : S × A →P(S)\nis a transition probability function, with P(s′ | s, a) being\nthe probability of transitioning into state s′ if an agent starts\nexecuting action a in state s. The function R : S ×A×S →R\ndenotes the reward function, with Rt = R(st, at, st+1). The\nMDP has a ﬁnite time horizon T, and solving an MDP means\nﬁnding a optimal policy π∗\nθ ∈Π : S ×A →[0, 1], where πθ is\nneural network-based policy with parameter θ; Observing s, πθ\ndetermines agent’s action a ∈A to maximize the cumulative\nrewards received during the ﬁnite time T.\nWhen the environment transitions and the policy are\nstochastic, the probability of a T-step trajectory is deﬁned as\nP(τ | πθ) = ρ(s0) QT −1\nt=0 P(st+1 | st, at)πθ(at | st) where ρ\nis the initial state distribution. Then, the expected return J (πθ)\nis deﬁned as J (πθ) =\nR\nτ P(τ | πθ)R(τ) = Eτ∼πθ [R(τ)]\nwhere the trajectory τ is a sequence of states and actions in\nthe environment. The objective of reinforcement learning is\nto learn a policy that maximizes the expected return J (πθ)\nwhen the agent acts according to the policy πθ. Therefore, the\noptimization objective is expressed by\nπ∗\nθ = arg max\nθ\nJ (πθ)\n(1)\nwith π∗\nθ being the optimal policy.\narXiv:2108.06849v1  [cs.LG]  16 Aug 2021\nDeep Q-Network (DQN) [11]. One of the conventional\nmethod for solving MDP is Q-Learning. Q-Learning utilizes\nQ-table to ﬁnd optimal policy. However, Q-Learning has limi-\ntation that it obtains optimal policy when the state dimension is\nsmall. Inspired to Q-Learning, deep Q-network (DQN) which\nis a model-free reinforcement learning, is proposed to learn the\noptimal policy with a high-dimensional state space. Experience\nreplay D and target network are two key features used for\ntraining deep neural network with stabilization. Experiences\net\n= (st, at, Rt+1, st+1) of the agent are stored in the\nexperience buffer D = (e1, e2, . . . , eT ), and are periodically\nresampled to train the Q-networks. Sampled experience is used\nto update the parameters θi of the policy with the loss function\nat the i-th training iteration where the loss function is deﬁned\nas\nL(θi) = E [(Rt+1+\nγ max\na′ Q(st+1, a′; θ−\ni ) −Q(st, at; θi))2i\n(2)\nwhere θ−\ni\nare the target network parameters. The target\nnetwork parameters θ−\ni\nare updated using the Q-network\nparameters θ in every predeﬁned step. The stochastic gradient\ndescent method is used to optimize the loss function.\nProximal Policy Optimization (PPO) [12]. PPO is one of the\nbreakthroughs of DRL algorithms for improving the training\nstability by ensuring that πθ updates at every iteration are small\nby clipping the probability ratio rπ(θ) = πθ(a | s)/πθold(a |\ns), where θold is that of previous updated parameters of policy.\n(Schulman et al., 2017) proposed a surrogate function that has\nobjective that prevents the new policy from straying away from\nthe old one is used to train the policy πθ. The clipped objective\nfunction is as follows:\nLCLIP\nt\n(θ) = min(rt(θ)At, clip(rt(θ), 1 −ϵ, 1 + ϵ)At),\n(3)\nwhere At is the estimated advantage function under hyperpa-\nrameter ϵ < 1, which means how far away the new policy is\nallowed to update from the old policy. PPO uses the stochastic\ngradient descent to maximize the objective (3).\nB. Quantum Computing\nQuantum computers use a qubit as the basic unit of compu-\ntation, which represent a quantum superposition state between\ntwo basis state |0⟩and |1⟩. It is controlled by unitary gates in\na quantum circuit to perform various quantum operations. It\ncan be represented as a normalized two-dimensional complex\nvector as:\n|ψ⟩= α|0⟩+ β|1⟩, where ∥α∥2\n2 + ∥β∥2\n2 = 1,\n(4)\nand there also is a geometrical representation of a qubit space,\nusing polar coordinates θ and φ:\n|ψ⟩= cos(θ/2)|0⟩+ eiφ sin(θ/2)|1⟩,\n(5)\nwhere 0 ≤θ ≤π and 0 ≤φ ≤π. Qubit state is mapped into\nthe surface of 3-dimensional unit sphere, which is called Bloch\nsphere. Quantum gate is a unitary operator transforming a\n𝑅y(s2)\n| ۧ\n0\n| ۧ\n0\n| ۧ\n0\n| ۧ\n0\n𝑅y(s1)\n𝑅y(s3)\n𝑅y(s4)\n𝑅x,𝑦,z(𝜃11 , 𝜃12, 𝜃13)\n𝑅x,𝑦,z(𝜃21 , 𝜃22, 𝜃23)\n𝑅x,𝑦,z(𝜃31 , 𝜃32, 𝜃33)\n𝑅x,𝑦,z(𝜃41 , 𝜃42, 𝜃43)\nState \nEncoding\nParameterized Rotation  & Entanglement Layers\nMeasure Decoding to \nAction Probability\nFig. 1. A policy-VQC for deep-Q learning with parameter θ.\nqubit state into another qubit state, which can be represented as\na 2×2 matrix with complex entries. There are some important\nquantum gates, Pauli-X, Pauli-Y , and Pauli-Z, rotating by π\naround their corresponding axes in Bloch sphere. The rotation\noperator gates Rx(θ), Ry(θ), and Rz(θ) rotate by θ instead\nof π in Pauli-X, Pauli-Y , and Pauli-Z gates, and it is known\nthat any single-qubit unitary gate in SU(2) can be written as\na product of three rotation operators of each axis. In addition,\nthere are quantum gates which operate on multiple qubits,\ncalled controlled rotation gates. They act on a qubit accord-\ning to the signal of several control qubits, which generates\nquantum entanglement between multiple qubits. Among them,\nControlled X(or CNOT) gate is one of the most used control\ngates, changing the sign of the second qubit if the ﬁrst qubit is\n|1⟩. These gates allow quantum algorithms to work using their\nfeatures on a quantum circuit that will be introduced later.\nC. Variational Quantum Circuit\nThe variational quantum circuit (or parameterized quantum\ncircuit) is a quantum circuit using learnable parameters to\nperform various numerical tasks, such as optimization, approx-\nimation, and classiﬁcation. Operation of general VQC model\ncan be divided into 4 steps. First one is state preperation\nstep, the input information is encoded into corresponding qubit\nstates, which can be treated in the quantum circuit. Next step is\nvariational step, entangling qubit states by controlled gates and\nrotating qubits by parameterized rotation gates. This process\ncan be repeated in a multi-layer manner with more parameters,\nwhich possibly enhance the performance of the circuit. In the\nthird step, processed qubit states are measured and decoded\nto the form of appropriate output information. Last step is\nconducted outside the circuit. The quantum circuit parameters\nare updated in the direction of optimizing the objective func-\ntion of the algorithm by a classical CPU algorithm, like Adam\noptimizer. Then the circuit updated with the new parameters\nperforms the calculation again from the beginning. This circuit\nis known to be able to approximate any continuous function\nlike classical neural network [13], so VQC is often called\nQuantum Neural Network (QNN) [14]. It has been widely\napplied in quantum machine learning researches.\nIII. QUANTUM REINFORCEMENT LEARNING\nA. Variational Quantum Policy Circuit\nIn recent studies of quantum reinforcement learning [15],\n[16], VQC substitutes the policy training DNN of existing\nDRL. At each episode, agent with given state information\nAlgorithm 1 Variational Quantum Deep Q Learning with PPO\nInitialize replay memory D to capacity N\nInitialize action-value function quantum circuit Q with random parameters θ\nInitialize state value function V (s; φ)\nfor episode = 1, 2, . . . , M do\nInitialise state s1 and encode into the quantum state\n# 1. Inference Process #\nfor t = 1, 2, . . . , T do\nWith probability ϵ select a random action at\notherwise select at = maxa Q∗(st, a; θ) from the output of the quantum circuit\nExecute action at in emulator and observe reward rt and next state st+1\nStore transition (st, at, Rt, st+1) in D\nend for\n# 2. Training Process #\nfor i = 1, ..., Kepoch do\nSample random mini-batch of transitions (sj, aj, Rj, sj+1) from D\nCalculate temporal difference target, yj =\n\u001a\nRj\nfor terminal sj+1\nRj + γ maxa′ Q(sj+1, a′; θ)\nfor non-terminal sj+1\nCalculate temporal difference, δj = yj −V (sj)\nCalculate estimated advantage function, ˆAj = δj + (γλ)δj+1 + ... + (γλ)J−j+1δJ−1\nCalculate ratio, rj =\nπθ(aj|sj)\nπθOLD (aj|sj)\nCalculate surrogate actor loss function using (3)\nCalculate critic loss function, |V (s) −yj|.\nCalculate gradient and update actor and critic parameters\nend for\nend for\nEnvironment\n(Cartpole)\nHybrid (Quantum+Classical) RL Agent\nClassical Optimizer\nQuantum Actor\nClassical Critic\nReplay Buffer\n𝜋𝜃(𝑎|𝑠)\nAction 𝑎\nTraining Process\n⟨𝑆, 𝐴, 𝑃, 𝑅, 𝑆′⟩\nState 𝑠\nReward 𝑠\nNext State 𝑠′\nEvaluate Quantum actor\nAdam Optimizer, PPO\n(1) Train Classical Actor Parameters\n(2) Train Classical Critic\nVariational Quantum Circuit \n(see. Fig. 1)\nFig. 2. Quantum Reinforcement Learning System\ndetermines its action from policy-VQC and parameters are\nupdated with a classical CPU algorithm like Adam Optimizer.\nThis paper approaches similarly, by using a VQC depicted in\nFig. 1.\nThe quantum circuit in Fig. 1 is a prototype of policy-\nVQC, which consists of the basic structure of VQC. State\nencoding part of the circuit includes Ry gates parameterized\nby normalized state input s, having their values between −π\nand π. Variational part in the center consists of entangling\nCX Gates and Rx, Ry, Rz gates parameterized with free\nparameter θ. This part is called a layer, and several can be\nrepeatedly stacked in a circuit. After that, measured output of\nthe circuit is decoded into the action space, yielding the action\nprobabilities. Then the obtained πθ is evaluated and updated\nin a classical computer.\nB. Quantum Reinforcement Learning Systems\nThe quantum reinforcement learning system in this paper\nis as described in Fig. 2. In the beginning of an episode,\nquantum-classical hybrid agent receives a state information\nfrom the environment, and determine its action by θπ made\nfrom the VQC. Then the policy of the agent is evaluated and\nupdated by PPO algorithm, which is introduced before and de-\nscribed in Algorithm 0. The PPO algorithm are effectively the\nsame as in the previous study [12]. The replay buffer functions\nin the same way as in traditional approaches, keeping track of\nthe ⟨s, a, R, s′⟩tuples. One does not have to fundamentally or\ndrastically change an algorithm in order to apply the power of\nVQCs to it. The algorithm presented in Algorithm 0.\nIV. THE IMPLEMENTATION OF QRL\nA. Implementation Guidelines\nAs quantum computing and quantum machine learning\nresearch is actively progressing, many development libraries\nfor researchers have emerged, such as TensorFlow-quantum,\nQiskit, and PennyLane. Among them, PennyLane was cre-\nated to support quantum machine learning research, allowing\nanyone to easily test the performance of quantum circuits\nthrough quantum simulators. The quantum simulator supported\nby PennyLane allows the CPU to imitate the operation of\nQPU, and especially supports the use of parameters in the\nform of PyTorch tensor and gradient operation [17]. Thanks\nto these features, PennyLane makes it easy for anyone who\n#Parameterized Rotation & Entanglement Layers\ndef layer(W):\nfor i in range(n_qubit):\nqml.RX(W[i,0], wires=i)\nqml.RY(W[i,1], wires=i)\nqml.RZ(W[i,2], wires=i)\n#Classical Critic\nclass V(nn.Module):\ndef __init__(self):\nsuper(V, self).__init__()\nself.fc1\n= nn.Linear(4,256)\nself.fc_v\n= nn.Linear(256,1)\ndef forward(self,x):\nx = F.relu(self.fc1(x))\nv = self.fc_v(x)\nreturn v\n#Variational Quantum Policy Circuit (Actor)\n@qml.qnode(dev, interface=’torch’)\ndef circuit(W,s):\n# W: Layer Variable Parameters, s: State Variable\n# Input Encoding\nfor i in range(n_qubit):\nqml.RY(np.pi*s[i],wires=i)\n#Variational Quantum Circuit\nlayer(W[0])\nfor i in range(n_qubit-1):\nqml.CNOT(wires=[i,i+1])\nlayer(W[1])\nfor i in range(n_qubit-1):\nqml.CNOT(wires=[i,i+1])\nlayer(W[2])\nfor i in range(n_qubit-1):\nqml.CNOT(wires=[i,i+1])\nlayer(W[3])\nqml.CNOT(wires=[0,2])\nqml.CNOT(wires=[1,3])\nreturn [qml.expval(qml.PauliY(ind)) for ind in range(2,4)]\n#Declare Quantum Circuit and Parameters\nW\n= Variable(torch.DoubleTensor(np.random.rand(4,4,3)),requires_grad=True)\nv = V()\ncircuit_pi = circuit\noptimizer1 = optim.Adam([W], lr=1e-3)\noptimizer2 = optim.Adam(v.parameters(), lr=1e-5)\nFig. 3. Variational Quantum Policy Circuit with PennyLane\nhas previously done machine learning research using PyTorch\nto start researching quantum machine learning. Based on this\nbackground, in this paper, a quantum reinforcement learning\nmodel was implemented using PennyLane and PyTorch as\nshown in Fig. 3.\nB. The CartPole Environment\nCartpole, the implementation environment in this paper, is\na test environment for reinforcement learning provided by\nOpenAI [18]. This is a game where the agent moves the cart\nback and forth to avoid dropping the stick on the cart and the\nlonger one holds the stick, the greater the reward. At every\nmoment, the player observes the cart’s position, velocity, and\nthe angle and angular velocity of the rod to determine which\ndirection to accelerate accordingly. The VQC in Fig. 1 using 4\nqubits is suitable for policy making in this environment. Each\nof the four pieces of information provided by the environment\nis normalized and fed into the circuit as values between −π\nand π, and the two measures are decoded into the probability\nvalues of taking two actions via the softmax function. This\nprocess continues until the agent can take the optimal action on\nthe given state information by optimizing the given parameters\nin the reinforcement learning algorithm. The experimental\nresult is showed later in this paper.\n0\n2000\n4000\n6000\nEpisode\n0\n20\n40\n60\n80\n100\n120\nTotal Reward\nFig. 4. Comparison of Total Reward on Average in Environment (CartPole-\nv0)\nC. Experimental Setup\nOur experiment is conducted with the software packages,\nPyTorch for speed and convenience of tensor operation and\nand PennyLane for quantum circuit simulation. The quantum\nsimulator provided by Pennylane is very convenient to use,\nbut it is difﬁcult to use many qubits because of its slow\ncomputational speed. Therefore, the CartPole environment was\nused as a simple environment that can be operated with a\ncircuit of small qubits. We used classical parameter optimizer\nas Adam optimizer with learning rate 0.001 for quantum policy\nand 0.00001 for classical critic. Other hyperparameter settings\nare γ = 0.98, λ = 0.95, ϵ = 0.01. The baseline model is a\nrandom version of this model, using random parameters in\nevery time step without optimization.\nD. Experimental Results\nFig. 4 shows the performance of the proposed quantum re-\ninforcement learning model. Comparing with random actions,\none can see that the model is learning to ﬁnd the optimal\naction. Also, it can be seen that the deviation of rewards\nduring the learning process is extremely high. This is due to\nuncertainty within quantum systems, although the impact of re-\ninforcement learning algorithms facilitating exploration cannot\nbe ignored either. This uncertainty simultaneously implies the\npossibilities and limitations of quantum reinforcement learn-\ning. This allows effective policy exploration with relatively\nfew tens of parameters, but makes it difﬁcult to maintain the\ngood results once reached. Leveraging these characteristics is\nan important challenge for quantum reinforcement learning.\nV. CONCLUSIONS AND FUTURE WORK\nThrough this work, we implemented and tested a quantum\nreinforcement learning model in the CartPole environment\nbased on PPO, one of the latest deep reinforcement learning\ntechniques, and demonstrated implementation guidelines in the\nPennyLane library. We discussed the principle and potential of\nthe reinforcement learning using a variational quantum circuit.\nFurthermore, this work aims to enable researchers who are\nnew to quantum reinforcement learning to start research with\ninterest. Although the performance of quantum reinforcement\nlearning cannot be said to be better than that of the existing\nmethod, it is expected that many follow-up studies will yield\nresults that exceed the limitations of existing reinforcement\nlearning.\nACKNOWLEDGMENT\nThis work was supported by the National Research Foun-\ndation of Korea (2019M3E4A1080391). Joongheon Kim is a\ncorresponding author of this paper.\nREFERENCES\n[1] J. Kim, Y. Kwak, S. Jung, and J.-H. Kim, “Quantum scheduling for\nmillimeter-wave observation satellite constellation,” in Proceedings of the\nIEEE VTS Asia Paciﬁc Wireless Communications Symposium (APWCS),\n2021, pp. 1–1.\n[2] J. Choi and J. Kim, “A tutorial on quantum approximate optimization\nalgorithm (QAOA): Fundamentals and applications,” in Proceedings of\nthe IEEE International Conference on Information and Communication\nTechnology Convergence (ICTC), 2019, pp. 138–142.\n[3] J. Choi, S. Oh, and J. Kim, “The useful quantum computing techniques for\nartiﬁcial intelligence engineers,” in Proceedings of the IEEE International\nConference on Information Networking (ICOIN), 2020, pp. 1–3.\n[4] Y. Kwak, W. J. Yun, S. Jung, and J. Kim, “Quantum neural networks:\nConcepts, applications, and challenges,” CoRR, vol. abs/2108.01468,\n2021.\n[5] S. Oh, J. Choi, and J. Kim, “A tutorial on quantum convolutional neural\nnetworks (QCNN),” in Proceedings of the IEEE International Conference\non Information and Communication Technology Convergence (ICTC),\n2020, pp. 236–239.\n[6] S. Oh, J. Choi, J.-K. Kim, and J. Kim, “Quantum convolutional neural\nnetwork for resource-efﬁcient image classiﬁcation: A quantum random\naccess memory (QRAM) approach,” in Proceedings of the IEEE Interna-\ntional Conference on Information Networking (ICOIN), 2021, pp. 50–52.\n[7] J. Choi, S. Oh, and J. Kim, “A tutorial on quantum graph recurrent neural\nnetwork (QGRNN),” in Proceedings of the IEEE International Conference\non Information Networking (ICOIN), 2021, pp. 46–49.\n[8] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V.\nIsakov, P. Massey, M. Y. Niu, R. Halavati, E. Peters, M. Leib, A. Skolik,\nM. Streif, D. V. Dollen, J. R. McClean, S. Boixo, D. Bacon, A. K. Ho,\nH. Neven, and M. Mohseni, “Tensorﬂow quantum: A software framework\nfor quantum machine learning,” arxiv, 2020.\n[9] M. S. A. et al., “Qiskit: An open-source framework for quantum com-\nputing,” arxiv, 2021.\n[10] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, M. S. Alam, S. Ahmed,\nJ. M. Arrazola, C. Blank, A. Delgado, S. Jahangiri, K. McKiernan,\nJ. J. Meyer, Z. Niu, A. Sz´ava, and N. Killoran, “Pennylane: Automatic\ndifferentiation of hybrid quantum-classical computations,” arxiv, 2020.\n[11] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing Atari with deep reinforcement learning,”\narXiv:1312.5602, 2013.\n[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.\n[13] J. Biamonte, “Universal variational quantum computation,” Physical\nReview A, vol. 103, no. 3, p. L030401, 2021.\n[14] N. Wiebe, A. Kapoor, and K. M. Svore, “Quantum deep learning,” arXiv\npreprint arXiv:1412.3489, 2014.\n[15] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S.\nGoan, “Variational quantum circuits for deep reinforcement learning,”\nIEEE Access, vol. 8, pp. 141 007–141 024, 2020.\n[16] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and V. Dunjko, “Vari-\national quantum policies for reinforcement learning,” arXiv preprint\narXiv:2103.05577, 2021.\n[17] A. e. a. Paszke, “Pytorch: An imperative style, high-performance deep\nlearning library,” in Advances in Neural Information Processing Systems\n(NIPS), 2019, pp. 8024–8035.\n[18] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-08-16",
  "updated": "2021-08-16"
}