{
  "id": "http://arxiv.org/abs/2108.09814v1",
  "title": "UzBERT: pretraining a BERT model for Uzbek",
  "authors": [
    "B. Mansurov",
    "A. Mansurov"
  ],
  "abstract": "Pretrained language models based on the Transformer architecture have\nachieved state-of-the-art results in various natural language processing tasks\nsuch as part-of-speech tagging, named entity recognition, and question\nanswering. However, no such monolingual model for the Uzbek language is\npublicly available. In this paper, we introduce UzBERT, a pretrained Uzbek\nlanguage model based on the BERT architecture. Our model greatly outperforms\nmultilingual BERT on masked language model accuracy. We make the model publicly\navailable under the MIT open-source license.",
  "text": "1 \n \n \n \n \n \nUzBERT: pretraining a BERT model for Uzbek \nB. Mansurov and A. Mansurov \nCopper City Labs \n{b,a}mansurov@coppercitylabs.com \nAugust 22, 2021 \nAbstract \nPretrained language models based on the Transformer architecture have \nachieved state-of-the-art results in various natural language processing tasks \nsuch as part-of-speech tagging, named entity recognition, and question \nanswering. However, no such monolingual model for the Uzbek language is \npublicly available. In this paper, we introduce UzBERT, a pretrained Uzbek \nlanguage model based on the BERT architecture. Our model greatly \noutperforms multilingual BERT on masked language model accuracy. We \nmake the model publicly available under the MIT open-source license. \nKeywords: Uzbek language, Cyrillic script, pretrained language model, BERT, \nnatural language processing \n \n \n1 Introduction \nIn natural language processing (NLP), Transformer-based [Vaswani et al. 2017] \npretrained language models have achieved state-of-the-art results on a wide range of \ntasks. Such models are publicly available for high-resource languages, e.g., BERT \n[Devlin et al. 2018] and RoBERTa [Liu et al. 2019] for English, and CamemBERT \n[Martin et al. 2019] for French. However, there is no such public monolingual \nmodel for the low-resource Uzbek language. \nTransformer-based multilingual models such as multilingual BERT [Devlin et al. \n2018], XLM [Lample and Conneau 2019], and XLM-R [Conneau et al. 2019] have \nbeen trained on multiple languages (including Uzbek) with the hope of transferring \nknowledge from resource-rich languages to low-resource languages. These \nmultilingual models demonstrate convincing results in zero-shot cross-lingual model \ntransfer [Pires et al. 2019], but they underperform their monolingual counterparts \n[Martin et al. 2019; Virtanen et al. 2019] on downstream tasks. Moreover, \nmultilingual models contain larger vocabularies and more parameters1 than \n \n1The vocabulary size of monolingual BERT is 30K, while it is 119K for multilingual BERT. Also \nmonolingual BERT has 110M parameters, and multilingual BERT has 168M parameters. \n2 \n \n \nmonolingual models, thus requiring high-memory GPUs in order to fine-tune them. \nAs a result, monolingual models in various languages [Virtanen et al. 2019; Lee \net al. 2020] have been pretrained and made publicly available. \nIn this paper, we introduce the first publicly available Uzbek model based on the \nBERT architecture. Uzbek is a low-resource language — it lacks publicly available \nlanguage models, labeled datasets, or even large amounts of raw text. In order to build \nthe model, we first develop a high-quality news corpus consisting of ~142M words. \nWe then pretrain a model, and call it UzBERT. We evaluate our model’s performance \nagainst multilingual BERT (mBERT) on masked language model accuracy.2 Our \ncomparisons show that UzBERT achieves far superior results than mBERT on this \nmetric. We make our model publicly available3 under the MIT open-source license \nto encourage research and application development for the Uzbek language. \n \n2 Background \nUzbek language \nUzbek is a morphologically rich, agglutinative language in the Turkic language family. \nIt is the national language of Uzbekistan and has two active writing systems — Cyrillic \nand Latin. According to Togʻayev et al. 1999, the Cyrillic script was introduced in \n1956, and the Latin script was (re-)introduced in 1995. \nAlthough the Explanatory Dictionary of the Uzbek Language [Mirzayev et al. 2006] \ncontains over 80K words and expressions, many more words can be formed by \naffixation. Even sentences in English can be expressed as single words in Uzbek. \nFor example, ”Are you one of those who won?” can be translated as \n”Ютганларданмисиз?”. Here is how the stem and suffixes of the Uzbek word \ncorrespond to the English words: ют (won) ган (who) лар (those) дан (one of) \nми (are) сиз (you). \nUzbek morphology is so rich that words with infinite number of letters can be \nformed. For example, the word уйдагилар means ”those, who/that are at home”, \nand уйдагилардигилар means ”those in the possession of those, who/that are \nat home”, while уйдагилардигилардагилар means ”those in the possession \nof those, who/that are in the possession of those, who/that are at home”. We can \nkeep adding the suffix дагилар to create longer and longer words. At some point, \nit becomes hard to understand the meaning of the word, but nevertheless, the Uzbek \nmorphology allows for such constructions. \nOn the other hand, Uzbek is a resource-poor language. Very few public datasets are \navailable for NLP research in the language. Two of the main resources are the \n \n2Because of the lack of public datasets for part-of-speech tagging, named entity recognition, and \nother NLP tasks, we haven’t yet been able to evaluate UzBERT on these tasks. \n3You may download UzBERT from https://huggingface.co/coppercitylabs/ \nuzbert-base-uncased. \n3 \n \n \nUzbek Wikipedia and the Common Crawl corpus. According to the Wikipedia \ndump of 2021-07-19, there are 12.2M words4 in the Uzbek Wikipedia. In addition \nto being small, the Uzbek Wikipedia suffers from low quality articles. Most articles \nwere bulk imported from an Uzbek encyclopedia [Aminov et al. 2006], where full \nwords are replaced by their abbreviations to save space in print. The filtered \nCommon Crawl corpus5 used by XLM-R [Conneau et al. 2019] is not big either — \nit contains ~91M words.6 This corpus contains not only article content but also \nsurrounding user-interface texts, making it noisy. \nEven when one gathers large amounts of raw text, it is more than likely some of \nthe data will be in Cyrillic, and the rest in Latin. In order to utilize all of the data, \nCyrillic text needs to be converted into Latin, or vice versa. Mansurov and Mansurov \n2021 describe an approach to transliterating between these two writing systems using \nmachine learning. To train UzBERT we collect and use only Cyrillic texts. \n \nTransformer-based pretrained language models \nIn NLP, traditionally, downstream tasks are solved by obtaining word embeddings \n[Mikolov et al. 2013; Peters et al. 2018] from raw text and feeding those \nembeddings into task-specific architectures. Recurrent Neural Networks (RNN) are \nmainly used in such architectures. A recent trend has been to pretrain a \nTransformer-based language model such as BERT and RoBERTa on large amounts \nof unannotated text, and fine-tune it for a downstream task with much less labeled \ndata. This approach outperforms previous attempts on multiple tasks such as \nlanguage understanding and question answering [Devlin et al. 2018]. \nThe Transformer utilizes the attention mechanism without recurrence and is \ncomposed of an encoder and decoder stacks. BERT consists of an encoder stack \nonly and is pretrained with a ”masked language model” (MLM) and a ”next sentence \nprediction” (NSP) tasks. In the MLM task, some input tokens are randomly masked, \nand the objective is to predict the original masked tokens. In the NSP task, given \ntwo sequences of text, the goal is to predict whether the second sequence follows the \nfirst in the original text. Once trained, the language model can be extended with an \nadditional layer, and fine-tuned to solve a variety of downstream tasks such as \npart-of-speech tagging. \n \n4An Uzbek Wikipedia dump is downloaded from https://dumps.wikimedia. \norg/other/cirrussearch/20210719/uzwiki-20210719-cirrussearch- \ncontent.json.gz and the ”text” property of each JSON line is extracted. The number of \nwords is counted with wc -w. \n5http://data.statmt.org/cc-100/uz.txt.xz \n6For comparison, BERT was trained on 3300M English words. \n4 \n \n \n3 Previous Work \nAttempts have been made to create word embeddings (but not Transformer-based \npretrained language models) for Uzbek. \nAl-Rfou et al. 2013 train embeddings for more than 100 languages (including \nUzbek) using Wikipedia in those languages. Their vocabulary contains the most \nfrequently occurring 100K words. Grave et al. 2018 create distributed word \nrepresentations for 157 languages using the Wikipedia and Common Crawl corpora. \nTheir Uzbek Wikipedia model7 contains ~110K words, while the Common Crawl \nmodel8 contains ~830K words. Kuriyozov et al. 2020 also develop word \nembeddings for Uzbek (among other Turkic languages) using fastText [Bojanowski \net al. 2017] (and align these embeddings with embeddings for other Turkic \nlanguages). Their Uzbek training data contains 24M words crawled from websites \n[Baisa et al. 2012] and their model contains ~200K words9. \nAll of the above-mentioned embeddings are trained for the Latin script of the Uzbek \nlanguage. Mansurov and Mansurov 2020 develop word embeddings for the Cyrillic \nscript using the word2vec [Mikolov et al. 2013], GloVe [Pennington et al. 2014], and \nfastText [Bojanowski et al. 2017] algorithms. The authors gather data by crawling \nwebsites in the ”uz” domain. Their training data include more than 79M words. \nThe main limitation of embeddings produced by the above-mentioned models is that \nthey are context independent — each word is assigned one vector regardless of the \nnumber of meanings it may have. Moreover, word2vec and GloVe are word-level \nmodels and cannot encode out-of-vocabulary words. \nAs far as we know, no Transformer-based monolingual Uzbek language model has \nbeen made publicly available. Therefore, two main contributions of this paper are \ngathering a high quality Cyrillic corpus and training a BERT-based model for Uzbek \nusing this corpus. \n \n4 UzBERT \nModel \nUzBERT has the same architecture (12 layers, 768 hidden dimensions, 12 attention \nheads, 110M parameters), training objectives (MLM and NSP), hyperparameters \nsuch as the dropout probability of 0.1 and a gelu activation, and the vocabulary \nsize of 30K tokens as the original BERT. \n \n7https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.uz. \nvec \n8https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.uz. \n300.vec.gz \n9https://zenodo.org/record/3666697/files/uz.sg.300.vec.tar.gz? \ndownload=1 \n5 \n \n \nData \nOur data consists of ~625K news articles that are obtained by crawling websites in \nthe Uzbek language. The articles span across many domains such as agriculture, \neconomics, history, literature, and law. However, almost all of them are written in \nthe same journalistic genre. The total number of words in those articles is ~142M, \nof which ~140M words are used for training, and the remaining for validation. All \nwords are down-cased and split into sub-word units using the WordPiece tokenizer \n[Wu et al. 2016], which we trained on the same corpus. \nFor evaluation, we gather a new dataset of news and encyclopedia articles (seen by \nneither UzBERT, nor mBERT) in both Cyrillic and Latin. Both the news and \nencyclopedia datasets consist of ~7K words each. We split the data into partly-\noverlapping sequences of 128 words and randomly mask one word in each \nsequence. In total, we have 872 such sequences. Similar to the Uzbek Wikipedia, \nthe encyclopedia evaluation dataset is written in a terse style with lots of \nabbreviations, such as ш. for шаҳар (city) and й. for йил (year). \n \nPretraining \nWe use the transformers library [Wolf et al. 2019] (version 4.8.2) and train a model on \nan NVIDIA RTX A6000 graphics card for about 2.5 days. To speed up the process, \nwe first pretrain the model with batch size of 300 and sequence length of 128 tokens \nfor 36 epochs. Then, to learn positional embeddings, we continue pretraining for 4 \nmore epochs, but increase the sequence length to 512 tokens and reduce the batch \nsize to 50. \n \nExperiments \nWe evaluate our model’s performance by comparing its MLM accuracy to that of \nmBERT10 using the evaluation dataset described in the previous section. We use the \nCyrillic version of the dataset to evaluate UzBERT, and the Latin version to evaluate \nmBERT (as these models were trained on Cyrillic and Latin texts, respectively). \nIdeally, we would have liked to evaluate UzBERT on a downstream task. \nUnfortunately, we could not find any public datasets in Uzbek suitable for such an \nevaluation. We hope to revisit this kind of extrinsic evaluation once we produce \nsuch a dataset. \n \n10The mBERT model we used is available at https://huggingface.co/bert-base- \nmultilingual-uncased and is trained on the top largest 102 language Wikipedias. \n6 \n \n \n5 Results \nThe MLM accuracy results are shown in Table 1. The table presents the mean \naccuracy and standard deviation (in parentheses) of five separate tests. ”Top 1 \nMatch” means the masked word was the top suggested word by the model. \nSimilarly, ”Top 3 Match” means the masked word was among the top 3 words \nsuggested by the model. \n \nModel \nEvaluation dataset \nTop 1 Match \nTop 3 Match \nTop 5 Match \nUzBERT \nmBERT \nNews \nNews \n64.06 (1.08%) \n14.52% (0.92%) \n81.40% (0.90%) \n18.81% (1.75%) \n85.32% (0.83%) \n20.32% (1.61%) \nUzBERT \nmBERT \nEncyclopedia \nEncyclopedia \n32.25% (0.84%) \n14.33% (1.36%) \n44.40% (1.04%) \n19.24% (1.87%) \n48.37% (0.84%) \n20.64% (2.05%) \nTable 1: The MLM accuracy of UzBERT vs. mBERT on the news and encyclopedia \nevaluation datasets. \n \n6 Discussion \nRecall that UzBERT was trained on news articles, while mBERT was trained on \nWikipedia. Consequently, UzBERT outperforms mBERT by a wide margin on the \nnews evaluation set. It also has a higher accuracy than mBERT on the encyclopedia \ndataset, albeit with a reduced margin. \nIn our opinion, mainly three things contribute to the superior performance of \nUzBERT: \n1. Uzbek language data used for training mBERT is about eleven times less than \nthe amount of data used for training UzBERT. \n2. UzBERT training data is of higher quality that that of mBERT. In order to \ncollect training data for UzBERT, we hand-pick our sources, and create site- \nspecific content extractors to extract only the article title an content. \n3. Transfer learning to Uzbek from other languages may not have been \nsuccessful for mBERT. We assume that the benefits of transfer learning will \nbecome apparent when mBERT is fine-tuned and evaluated on downstream \ntasks. \nWhen we compare UzBERT against itself on the two evaluation datasets, the lower \nscores on the encyclopedia set can be attributed to the terse writing style in the \nevaluation data, to which UzBERT didn’t have access during training. As mentioned \nabove, the articles contain many abbreviations. \nSomewhat surprisingly, the performance of mBERT is relatively the same regardless \nof the evaluation set. We expected it to perform better on the encyclopedia articles. \nIt may be an indication that not enough Uzbek text was used for pretraining mBERT. \n7 \n \n \n7 Conclusion \nThis research has focused on building a monolingual pretrained Uzbek language \nmodel based on the BERT architecture. The result is the first publicly available such \nmodel — UzBERT. Even though our model was trained on a small corpus (~140M \nwords), its accuracy on masked language model is far better than that of multilingual \nBERT. \nOne of the advantages of UzBERT over mBERT is that its vocabulary size is smaller \n(thus it requires less resources for fine-tuning) and theoretically better captures the \nintricacies of the language because it was trained only on Uzbek texts. However, \nmBERT is more desirable in situations where task-specific fine-tuning data is available \nin a non-Uzbek language. In such cases mBERT can be fine-tuned in a language other \nthan Uzbek, and tested on Uzbek texts. For that to happen, though, mBERT should be \ntrained on a much higher quality Uzbek text than the Uzbek Wikipedia as the model’s \nperformance on MLM accuracy lags far behind UzBERT. \nFuture research on UzBERT should consider training a model with additional texts \nwritten in different genres. Martin et al. 2019 show that a model trained on 4GB of \ntext is as good as a model trained on 130+GB of text. UzBERT is trained on 1.9GB \nof text. Doubling its size will hopefully result in a language model that is as good as \na model trained on tens of GB of text. \nBecause of the lack of public datasets for downstream tasks in Uzbek, we were unable \nto test its performance on such tasks. Another direction for future work is to produce \nsuch datasets and evaluate UzBERT on downstream tasks. \nFinally, it would be interesting to see the impact of the tokenizer on model’s \nperformance, similar to the work done in Lee et al. 2020 for Korean. Being a highly \ninflectional language, Uzbek may benefit from a tokenizer that is able to correctly \nsplit words into stems and suffixes. For example, the UzBERT tokenizer considers \nthe word менинг (my) to consist of one token. Morphologically speaking, it’s a \nword consisting of the stem мен (I) and the suffix нинг (’s). Such trivial cases can \nprobably be solved by an unsupervised tokenizer such as WordPiece, but to \ncorrectly tokenize texts such as Ютганларданмисиз? (Are you one of those \nwho won?), we will most certainly need a carefully crafted tokenizer based on Finite \nState Machines, for example. \nWe hope that UzBERT generates more interest from scholars to the Uzbek language \nand serves as a catalyst for developing new resources in building robust Uzbek \nlanguage models. \n \n8 Acknowledgments \nWe would like to thank N. Mansurov for manually gathering the evaluation datasets. \n8 \n \n \n9 References \nAl-Rfou, R., Perozzi, B., and Skiena, S. (2013). Polyglot: Distributed word \nrepresentations for multilingual nlp. arXiv preprint arXiv:1307.1662. \nAminov, M., Ahmedov, B., Boboev, H., Daminov, T., Dolimov, T., Jo‘raev, T., \nZiyo, A., Ibrohimov, N., Karimov, N., Karomatov, H., Komilov, N., Mansur, \nA., Musaev, J., Nabiev, E., Oripov, A., Risqiev, T., Tuxliev, N., Shorahmedov, \nD., Shog‘ulomov, R., Qo‘ziev, T., G‘ulomov, S., and Hojiev, A. (2000-2006). \nO’zbekiston milliy ensiklopediyasi. ”Ozbekiston milliy ensiklopediyasi” Davlat ilmiy \nnashryoti. \nBaisa, V., Suchomel, V., et al. (2012). Large corpora for Turkic languages and \nunsupervised morphological analysis. In Proceedings of the Eighth conference \non International Language Resources and Evaluation (LREC’12), Istanbul, Turkey. \nEuropean Language Resources Association (ELRA). \nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). Enriching word vectors \nwith subword information. Transactions of the Association for Computational \nLinguistics, 5:135–146. \nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., \nGrave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2019). Unsupervised cross- \nlingual representation learning at scale. arXiv preprint arXiv:1911.02116. \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training \nof deep bidirectional transformers for language understanding. arXiv preprint \narXiv:1810.04805. \nGrave, E., Bojanowski, P., Gupta, P., Joulin, A., and Mikolov, T. (2018). Learning \nword vectors for 157 languages. arXiv preprint arXiv:1802.06893. \nKuriyozov, E., Doval, Y., and Gómez-Rodríguez, C. (2020). Cross-lingual word \nembeddings for Turkic languages. arXiv preprint arXiv:2005.08340. \nLample, G. and Conneau, A. (2019). Cross-lingual language model pretraining. arXiv \npreprint arXiv:1901.07291. \nLee, S., Jang, H., Baik, Y., Park, S., and Shin, H. (2020). Kr-bert: A small-scale \nkorean-specific language model. arXiv preprint arXiv:2008.03979. \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., \nZettlemoyer, L., and Stoyanov, V. (2019). RoBERTa: A robustly optimized bert \npretraining approach. arXiv preprint arXiv:1907.11692. \nMansurov, B. and Mansurov, A. (2020). Development of word embeddings for Uzbek \nlanguage. arXiv preprint arXiv:2009.14384. \n9 \n \n \nMansurov, B. and Mansurov, A. (2021). Uzbek Cyrillic-Latin-Cyrillic Machine \nTransliteration. arXiv preprint arXiv:2101.05162. \nMartin, L., Muller, B., Suárez, P. J. O., Dupont, Y., Romary, L., de La Clergerie, \nÉ. V., Seddah, D., and Sagot, B. (2019). CamemBERT: a tasty french language \nmodel. arXiv preprint arXiv:1911.03894. \nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed \nrepresentations of words and phrases and their compositionality. In Advances in \nneural information processing systems, pages 3111–3119. \nMirzayev, T., Begmatov, E., Madvaliyev, A., Mahkamov, N., To’xliyev, N., Umarov, \nE. Khudoyberganova, D., and Hojiyev, A. (2006). O’zbek tilining izohli lug’ati. \n”Ozbekiston milliy ensiklopediyasi” Davlat ilmiy nashryoti. \nPennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for \nword representation. In Proceedings of the 2014 conference on empirical methods \nin natural language processing (EMNLP), pages 1532–1543. \nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and \nZettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint \narXiv:1802.05365. \nPires, T., Schlinger, E., and Garrette, D. (2019). How multilingual is multilingual \nBERT? arXiv preprint arXiv:1906.01502. \nTogʻayev, T., Tavaldiyeva, G., Akromova, M., and Nazarov, K. (1999). Oʻzbek tilining \nkirill va lotin alifbolaridagi imlo lugʻati: 14 mingdan ortiq soʻz. ”Sharq” nashriyot- \nmatbaa konserni bosh tahririyati. \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \nŁ., and Polosukhin, I. (2017). Attention is all you need. In Advances in neural \ninformation processing systems, pages 5998–6008. \nVirtanen, A., Kanerva, J., Ilo, R., Luoma, J., Luotolahti, J., Salakoski, T., Ginter, \nF., and Pyysalo, S. (2019). Multilingual is not enough: BERT for Finnish. arXiv \npreprint arXiv:1912.07076. \nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., \nRault, T., Louf, R., Funtowicz, M., et al. (2019). Huggingface’s transformers: \nState-of-the-art natural language processing. arXiv preprint arXiv:1910.03771. \nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., \nCao, Y., Gao, Q., Macherey, K., et al. (2016). Google’s neural machine translation \nsystem: Bridging the gap between human and machine translation. arXiv preprint \narXiv:1609.08144. \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-08-22",
  "updated": "2021-08-22"
}