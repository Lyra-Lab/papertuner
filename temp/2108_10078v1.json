{
  "id": "http://arxiv.org/abs/2108.10078v1",
  "title": "Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents",
  "authors": [
    "Ling Zhang",
    "Jian Cao",
    "Yuan Zhang",
    "Bohan Zhou",
    "Shuo Feng"
  ],
  "abstract": "Spiking neural network (SNN), compared with depth neural network (DNN), has\nfaster processing speed, lower energy consumption and more biological\ninterpretability, which is expected to approach Strong AI. Reinforcement\nlearning is similar to learning in biology. It is of great significance to\nstudy the combination of SNN and RL. We propose the reinforcement learning\nmethod of spike distillation network (SDN) with STBP. This method uses\ndistillation to effectively avoid the weakness of STBP, which can achieve SOTA\nperformance in classification, and can obtain a smaller, faster convergence and\nlower power consumption SNN reinforcement learning model. Experiments show that\nour method can converge faster than traditional SNN reinforcement learning and\nDNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN\n200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which\nproves that SDN has lower power consumption than DNN, and the power consumption\nof SDN is more than 600 times lower than DNN on large-scale devices. SDN\nprovides a new way of SNN reinforcement learning, and can achieve SOTA\nperformance, which proves the possibility of further development of SNN\nreinforcement learning.",
  "text": " \n \nDistilling Neuron Spike with High Temperature  \nin Reinforcement Learning Agents \nLing Zhang1, Jian Cao1*, Yuan Zhang1, Bohan Zhou1, Shuo Feng1 \n1 School of Software and Microelectronics, Peking University, Beijing, China. \n{zhangling@stu.pku.edu.cn, caojian@ss.pku.edu.cn, zhangyuan@stu.pku.edu.cn} \n \n \n \nAbstract \nSpiking neural network (SNN), compared with depth neural \nnetwork (DNN), has faster processing speed, lower energy \nconsumption and more biological interpretability, which is \nexpected to approach Strong AI. Reinforcement learning is \nsimilar to learning in biology. It is of great significance to \nstudy the combination of SNN and RL. We propose the rein-\nforcement learning method of spike distillation network \n(SDN) with STBP. This method uses distillation to effec-\ntively avoid the weakness of STBP, which can achieve SOTA \nperformance in classification, and can obtain a smaller, faster \nconvergence and lower power consumption SNN reinforce-\nment learning model. Experiments show that our method can \nconverge faster than traditional SNN reinforcement learning \nand DNN reinforcement learning methods, about 1000 \nepochs faster, and obtain SNN 200 times smaller than DNN. \nWe also deploy SDN to the PKU nc64c chip, which proves \nthat SDN has lower power consumption than DNN, and the \npower consumption of SDN is more than 600 times lower \nthan DNN on large-scale devices. SDN provides a new way \nof SNN reinforcement learning, and can achieve SOTA per-\nformance, which proves the possibility of further develop-\nment of SNN reinforcement learning. \n Introduction  \nBrain is the most important organ of animals. Since Cesar \nJulien Jean legallois defined the specific functions of brain \nregions [1], brain neurology has attracted many researchers, \nand many experiments to reproduce the brain system have \nbeen carried out. As early as 1958, Rosenblatt built a per-\nceptron model based on brain neurons [2], which opened the \nway for neural network (NN). Neuralink made an attempt to \nconvert the activity of brain neurons into commands to con-\ntrol external devices in monkeys [3]. The neural pattern \nrecognition machine architecture [4] built by Stephen \nGrossberg et al. also provided ideas for the later design of \nvarious neural morphology chips, including TrueNorth [5], \nLoihi [6] and Tianjic [7]. \n \n \nSince AlexNet [8], deep neural network (DNN) has \nshown superior performance in speech and image pro-\ncessing. However, the high accuracy and performance of \nDNN are based on very high power consumption. In addi-\ntion, the development of DNN has gradually deviated from \nits original intention of imitating human brain and creating \nintelligent life. Spiking neural network (SNN), as the third \ngeneration neural network, has lower power consumption \nand faster speed than DNN, and is also more biologically \ninterpretable [9]. It is expected to complete more complex \ncognitive problems in the form closest to the brain neuron \nparadigm and create the so-called \"electronic life\". On the \nother side, the implementation and application of SNN is a \nbold trial of brain system modeling, which may provide pre-\ncious and meaningful experience for neurology. \nSNN based applications are gradually improving: in \nterms of classification, Riccardo Massa et al. use DVS for \ngesture recognition [10]; In terms of object detection, Kim \nS et al. proposed spiking Yolo [11]. The above supervised \nlearning applications have achieved good results in accuracy \nby learning labels. However, in order to achieve strong AI, \nunsupervised learning and reinforcement learning are indis-\npensable. Reinforcement learning is a way for agents to con-\ntinuously learn through interaction with the environment \nand reward and punishment mechanism. This way is very \nsimilar to the growth mode of organisms and coincides with \nthe research concept of SNN. \nResearch on SNN reinforcement learning is also ongoing: \nNan Zheng et al. trained the hardware friendly actor critical \nnetwork by using spiking-timing depedent plasticity rule \n(STDP) [12]. Mengwen Yuan et al. trained the SDC network \nby combining the hedonistic synapse model and STDP [13]. \nVahid Azimirad et al. trained the TCT network by using \nSTDP [14]. Fatemeh Sharifizadeh et al. trained r-snn net-\nwork using r-stdp [15]. Devdhar Patel et al. trained DQN \nnetwork based on DNN, and then converted it to SNN [16]. \nMost studies tend to use STDP and its improved version to \ncomplete SNN reinforcement learning. At the same time, \nsome studies look for new solutions. \nIn 2018, Yujie Wu et al. proposed the spatio-temporal \nbackpropagation (STBP) algorithm for training high-perfor-\nmance SNN [17]. Experiments showed that the performance \nof SNN trained by STBP algorithm is much higher than that \nof STDP algorithm, which is a new way to train SNN. How-\never, STBP algorithm uses spike-rate coding [18], which \nwill extremely compress the reinforcement learning action \nvalue’s search space, resulting in the difficulty of using \nSTBP algorithm directly for training action value based \nSNN reinforcement learning. Guangzhi Tang et al. imple-\nmented the network based on action value in DDPG with \nDNN and the network based on deterministic strategy with \nSNN to avoid the operation of action value search by \nSNN[19]. This is one of the few studies research using \nSTBP algorithm to train SNN reinforcement learning model. \nThis paper proposes another method to realize the action \nvalue based SNN, and is expected to use this method to train \nsmaller SNN. \nIn this paper, we propose the SNN reinforcement learning \nmethod of Spiking Distillation Network(SDN)with STBP, \nwhich has achieved better results than the traditional SNN \nreinforcement learning and DNN reinforcement learning al-\ngorithms. We also give the proof that spike-rate coding in \nSTBP will extremely compress the reinforcement learning \naction value’s search space. We got inspiration from \nknowledge distillation [20] and transformed the original \nway of DNN teacher network training DNN student network \ninto DNN teacher network training SNN student network. \nIn this way, DNN will carry out reinforcement learning \ntraining and search action space. Then SNN will learn from \nDNN. This effectively avoids the work of SNN searching \naction space, and can further compress the capacity of SNN. \nIn the following, this paper first introduces the prior \nknowledge, then introduce our SDN method in detail, and \nproves the disadvantage of action value search for STBP \nthrough the formula. Then, we compare SDN with other \nSNN and DNN reinforcement learning methods to prove the \neffectiveness and efficiency of SDN, and prove the com-\npressibility of SDN by comparing the capacity of teacher \nnetwork and student network in SDN. In addition, we also \ndeploy the SDN model on PKU NC64C chip to prove the \nlow power consumption of SDN. \nMETHODS AND MATERIALS \nWe focus on how to effectively use STBP to train a more \nintelligent SNN reinforcement learning model, while avoid-\ning the problem of STBP compressing the action search \nspace. In this section, we introduce our method in detail. At \nthe same time, we prove that STBP will greatly compress \nthe reinforcement learning action search space to illustrate \nthe necessity of our method. \nTraining of Teacher Net \nThe training of Spiking Distillation Network (SDN) is \nbased on a well-trained DNN reinforcement learning \nteacher network. We use DQN [21] to train out DNN \nteacher network. \nWe consider a scene where an agent interacts with the \nenvironment. The environment provides the agent with T \nprevious states from the current moment，      ，and \nreward at each time. The state provided by the environment \nis the image, H and W represent the height and width of \nthe image, and the agent chooses the next action      or    \naccording to state. The agent inputs the obtained state into \nDNN. DNN predicts the Q value of       and      , which rep-\nresents the accumulation of reward that can be obtained \nfrom the current moment to the end of the game. The agent \nchooses the action with the highest Q value as its next in-\nstant action. \nFor DQN, the loss function is as follows: \n \n \n \nWhere       is the DNN of layer L,     is the suspect value \nof DNN prediction result,      and      are randomly taken \nfrom the experience pool, which stores the environment \nstate and reward within a period of time. \nIt should be noted that SDN is not limited to the training \nmethods of reinforcement learning DNN. We can use any \ndeep reinforcement learning methods to train our DNN \nteacher network, such as DDPG[22], TD3[23], PPO[24], \netc., and then use SDN to train SNN. \nArchitecture of Spiking Distillation Network \nSpiking Distillation Network (SDN) is a training frame-\nwork that uses DNN teachers to guide SNN students. Com-\npared with training of teacher network, the environment \nprovides different preprocessed states to DNN teacher net-\nwork and SNN student network respectively, and does not \nprovide reward. \nFor DNN teacher network, the environment provides it \nwith T previous states from the current moment,       . \nFor SNN student network, the environment does not pro-\nvide it with T previous states at the same time. Instead, \nthese states,                , are provided in chronological order, \nwhich will provide SNN with strong time dimension infor-\nmation.  \nWe use leaky integrate-and-fire (LIF) neurons to de-\nscribe our SNN model. The state of LIF neurons is deter-\nmined by spike input, membrane potential and threshold \npotential. The membrane potential accumulates with the \nspike input. When the membrane potential exceeds the \nthreshold potential, LIF neurons fire spikes. The membrane  \nT H W\nS\nR \n\n1a\n2a\n1a\n2a\nL\nQ\n\nt\nX\ntR\n1\n2\n1\n(\n(\n(\n))\n(\n(\n)))\n1\nn\nt\nt\ni\nL\nL\nt\nL\nR\nmax Q\nX\nma\nn\nx Q\nX\n\n\n\n\n\n\n\n(1)\nT H W\nS\nR \n\nH W\nts\nR\n\n\nEnvironment\nTeacher\nStudent\nStack on Channel \nStack on Timestep \nDNN\nSNN\nM1\nM2\nM3\nUniform\nC4\nPool\nHard Label\nDistilling\nSoft Label \nAccumulation Soft Loss\nLoss\nSoft Label \nHard Loss\nT4\n \nFig 1. Flow chart of SDN algorithm \n \npotential will leak and decrease over time. The differential \nexpression of LIF neurons is as follows: \n \n \n \nWhere         is the membrane potential of LIF neuron at \ntime t,     is leakage rate, and         is the product of input \nspike and weight.  \nIn order to use STBP, we need to divide the expression \nof LIF neuron into three parts: Calculation of         , accu-\nmulation of membrane potential         and firing spike, \nwhich is described in algorithm 1. Where    is the time,      \nis the number of SNN student network’s layers,        repre-\nsents the spike firing’s situation of SNN layer L at time t, \nand            is the threshold potential,      represents the \nnumber of output spikes, and     represents the output with \nspike-rate coding. \n \n    is an important parameter used by LIF neurons in \nSTBP to obtain time dimension information, which can \ndistinguish input in different time sequences. Assuming \nthat      has not reached the threshold potential from time 0 \nto time t, the membrane potential of LIF neurons at time t \nis as follows:  \n \n \nIt can be seen that     completes the distinction of input \nin different time sequences through its own cumulative \nmultiplication.  \nThe convolution of DNN is difficult to make a strong \ndistinction between the input in different time sequences \non different channels, because the pixels at the same posi-\ntion on all channels will be uniformly multiplied and added \nto obtain an output pixel, as shown in the following for-\nmula: \n \n \n \nWhere            represents the pixel at position           and \nchannel     in the first layer of DNN,           represents the \npixel at position          and channel    in the input, and            \nrepresents the weight in DNN. \nThis means that when the input with different time order \non the channel is sent into the first layer of DNN, each \nchannel output by the first layer will no longer have a \nstrong time order difference, although each pixel of the \noutput is fused with the information of different time di-\nmensions, which we called weak difference.  \nSDN will first accumulate experience pool, saving     \nand       given by the environment.     is sent to the SNN \nstudent network, obtains the output     , and we provide        \n  to the environment as the judgment basis for the next \naction selection. When a certain epoch is reached and the \nexperience pool is full, SDN will begin to use the content \nstored in the experience pool to train the SNN student net-\nwork. Among them, the role of experience pool is to dis-\nrupt the relevance of samples and improve the utilization \n( )\n( )\n( )\ndu t\nu t\ni t\ndt\n\n\n\n(2)\n( )\nu t\n\n( )\nI t\nt\n( )\nI t\n( )\nu t\n1l\n1l\nto\nthresh\nsp\nfr\n\ntu\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n2\n1\n...\nl\nl\nl\nl\nl\nT\nT\nT\nT\nl\nl\nT\nT\nT\nu\nu\nu\nu\nu\ni\nb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3)\n\n)\n,\n1\nb+\n,\n( ,\n,\no\no\no\no\ni\no\ni\no\ni\ni\nx\nt x\ny\ny\nk\nt\nx\ny\ny\nx\nk\nT\nw s t x\ny\nt\na\n\n\n\n\n\n\n\n\n,\n,\no\no\nt x\ny\na\n,\no\no\nx\ny\nt\n,\n,\ni\ni\nt x y\ns\n,\ni\ni\nx y\nt\nt\nw\nS\nts\nts\nfr\nfr\n(4)\nAlgorithm 1: Accumulation of experience pool for SDN \nInput: Environment state               and                   \nParameter: weights      and biases     of  SNN student \nnetwork \nOutput: firing rates       of output                 \n1: Environment provides state     and         \n2: store     and     to pool        \n3: while                  do \n4:  \ninput to SNN student network \n5:  \nwhile                   do \n6: \nupdate LIF neurons:                                                 \n7:  \n \n \n8:  \n \n9: \n \n10: \nend while \n11: \nAccumulating the output spikes of student SNN: \n12:  \n13: end while \n14: Compute firing rates of output: \n/\nfr\nsp T\n\n                     \n15: Provide the environment with  fr                          \nW\nT H W\nS\nR \n\n\nH W\nts\nR\n\n\nb\nfr\nS\nts\nS\nD\nts\n1,\n,\nt\nT\n\n\nts\n1\n1\n1,\n,\nl\nL\n\n\n1\n1\n1\n1\n1\nl\nt\nl\nl\nl\nti\nW o\nb\n\n\n\n1\n1\n1\n1\n1\n1\n1\n(1\n)\nl\nl\nl\nt\nl\nt\nt\nt\nl\ntu\no\nb\nu\ni\n\n\n\n\n\n\n\n\n1\n1\n(\n)\nl\nl\nt\nt\no\nthre\nu\nsh\n\n1\nt\nL\nsp\no\n\n \n \nFig 2. Exploration of action space in DNN reinforcement learning\n \nof data. The overall architecture of SDN is shown in the \nfigure below: \nTraining of SDN \nThe training process of SDN is shown in algorithm 2. At \nthe beginning, SDN accumulates experience pool. When \nepoch is greater than                       ,  the experience pool is \nfull, and we will take out the corresponding environment \nstate                  from experience pool D in batches, input \nthem into SNN student network and DNN teacher network \nrespectively  for forward propagation,  obtain the corre-\nsponding output,                , and calculate the loss of these \ntwo outputs. \nThe loss function we use is CrossEntropy with T: we \nwill divide the output by a temperature value to soften the \noutput, and then calculate the SoftLoss through Softmax \nand CrossEntropy. At the same time, we will make the out-\nput of the teacher network one hot as a hard label, and then \nuse this label to calculate the HardLoss through CrossEn-\ntropy with the output of the student network. Add Softloss \nand Hardloss in proportion to obtain the final loss function. \nThen, STBP back propagation is performed on the stu-\ndent SNN using the calculated loss, and the parameters of \nthe student SNN are updated from SD and TD. \nImpact of STBP spike-rate coding on action value \nexploration space \nWe have counted the change of the output and output \nprecision of DNN based on DQN and DDQN [25], as \nshown in the figure below. It can be seen that the action \nvalue of DNN output is trained from a small negative num-\nber to a large positive number. However, the output range \nof spike-rate coding is only between 0 and 1, which is  \ndifficult to express the action value with such a large  \n \n \nchange range, which makes it very difficult to directly train \nSNN in DQN and DDQN based on action value reinforce-\nment learning with STBP. \nand\nt\nb\nb\nS\ns\nand\nb\nb\nfr\no\nAlgorithm 2: Training of SDN \nInput:                  randomly sampled in batches from ex-\nperience pool        \nParameter: weights and biases of  SNN student network \nand DNN teacher network \nOutput: Total Loss              \n1: while epoch = 0, …, epoch_end  do \n2:   \nAccumulation of experience pool for SDN \n3:        if (epoch > thresh_epoch) then \n4:                    input to SNN Student network  \n5:  \n6: \ninput to DNN Teacher network \n7:  \n \n8: \nLIF Loss compute:                                               \n9:  \n \n10:  \n \n11: \n12: \n \n \n13: \n \n14:                                                                                      \n15:  \nSTBP backward with loss in Student SNN \n16:       end if \n17: end while  \n_\nthresh\nepoch\nand\nt\nb\nb\nS\ns\nD\nb\nts\n( )\nb\nSNN\nt\nforward\nf\ns\nr \nS\n( )\nb\nDNN\nforward\no\nS\n\n(\n(\n)\n(1\n)\n(1\n/\n))\nb\nb\nb\nb\nb\nSoftLoss\nz log q\nz\nlog\nq\nT\n\n\n\n\n\n(\n/\n) /\nexp(\n/\n)\nb\nb\nb\nj\nq\no\np\nT\nT\nex\no\n\n\n(\n/\n) /\nexp(\n/\n)\nb\nb\nb\nj\nz\nfr\nexp fr\nT\nT\n\n\n(\n(\n)\n(1\n)\n(1\n))\nb\nb\nb\nb\nb\nHardLoss\nl log fr\nl\nlog\nfr\n\n\n\n\n\n(\n)\nb\nb\nl\nonehot o\n\n(\n)\n1\nLoss\nSoftLoss\nHardLoss\n\n\n\n\n\n\n\nWe consider using affine transformation to map the out-\nput     of DNN training to the range of STBP spike-rate \ncoding        with accuracy          . \n \n \n \nWhere \n \n \n \n  In order that this affine transformation will not cause loss, \nwe need to specify at least:  \n \n \n \n \nCombined with the above data, the step required for \nspike-rate coding will reach the order of 1e5, which means \nthat it needs to consume a lot of energy and inference \nspeed, which brings a great burden to inference and train-\ning. SDN effectively avoids such search and completes the \nreinforcement learning training of SNN with few step. \nExperiments \nIn the first part, we tried different Loss Functions for \nSDN and got the best Loss Function: CrossEntropy with \nTemperature(T). Then, we compared our SDN with other \nSNN methods to confirm the effectiveness of SDN. In ad-\ndition, we compared DNN reinforcement learning methods \nwith SDN, and the result shows that SDN has a good learn-\ning ability. All experiments were performed in Pong game \nenvironment, as shown by Figure 3. \n \n \nFig 3. Pong Game \n \nComparison of different Loss treatments of SDN \nFor the Loss Function in SDN, we tested the mean \nsquare error function and the cross-entropy function, as \nshown in Figure 4. For mean square error, since the range \nof DNN’s output is not fit for the range of SNN output \nspike-rate coding, we tried to map the output of DNN into \na range we set, so that the Loss Function can be calculated \nby the DNN and SNN output. Based on the degree of soft-\nness of DNN output, we divided the mapping range into \nthree separate experiments: (1) Set the maximum value of \nDNN output to 1, and the other to 0. (2) Linear zoom the \nmaximum value in DNN output to a range of 0 to 1, and \nthe other to 0. (3) Linear scaling of the maximum value in \nDNN output (2) and adjust the remaining values in formula \nX. The adjustment in formula x is to widen the difference \nbetween the maximum and other values. \n \n \n \n \nWhere the scale is the scaling factor for linear scaling \nand D is used to widen the difference between the maxi-\nmum and other values. \nAs for CrossEntropy with T, we set                      . \nFrom the Fig x, reward in CrossEntropy with T training \nmethod is the most, so we chose CrossEntropy with T as \nthe loss function in SDN. \n \n \nFig 3. Comparison of different Loss treatments of SDN \n \nComparison of SDN and other SNN reinforcement \nlearning methods \nWe compared SDN with other SNN reinforcement \nlearning methods, as shown in Figure x. It can be found  \n \n \nFig 4. Comparison of SDN and other SNN reinforcement \nlearning methods \n \nthat the STBP combined with DNN can not achieve an \nideal learning effect, but the effect of SDN is much better  \nx\nfr\nx\nfr\nacc\nmax\nmin\n(\n) /\nfr\nfr\nfr\nx\nro\nx\nx\nund\nacc\nacc\nz\nx\n\n\n\n\n(\n) /\nmin\nfr\nfr\nx\nm n\nma\ni\nz\nround\nacc\nx\nx\nacc\nx\n\n\n\n/\nmax\nmax\no\no\nscale\n\n/\nother\nother\no\no\nscale\nd\n\n\nmax\nmin\n1\nx\nfr\nac\nx\nx\nacc\nc\n\n\n\n，\nmax\nmin\nx\nacc\nx\nste\nx\np \n\n(5)\n(6)\n(7)\n(8)\n(9)\n=10,\n=0.9\nT\n\nthan that of reinforcement learning with STBP, which is in \nline with the above explanation for impact of STBP spike-\nrate coding on action value exploration space. \nComparison of teacher network and student net-\nwork in SDN \nWe took DNN teacher network with larger structure to \ntrain smaller SNN student network. In pong game, we used \n10 layers of teacher network with a size of 892 KB, while \nthe trained SNN student network has only 3 layers and a \nsize of 5 KB, which is nearly 200 times compressed, which \nproved that SDN has the ability to train smaller SNNs.  \n \nTab1. Capacity of Teacher Network and Student Network \nModel\nLayer nums\nCapacity(KB)\nTeacher \nNetwork\n10 \n892 \nStudent \nNetwork\n3 \n50 \n \nComparison of SDN and DNN \nFurthermore, we tried to compare the DNN reinforce-\nment learning methods with SDN, as shown in Figure XX. \nDue to the combination of time dimension information, the \nlearning speed of SDN is faster than that of DNN and the \nreward obtained is more than that of DNN.  \n \n \nFig 5. Comparison of SDN and DNN \n \nDeployment \nIn order to prove the effect of SDN in low power con-\nsumption, we deployed it on PKU NC64C chip [26] and \ncompared it with various devices, as shown in tab.xx. It \ncan be seen that compared with large-scale devices, such as \nNVIDIA rtx2080, the power consumption of SDN is re-\nduced by more than 600 times; Compared with embedded \ndevices, such as NVIDIA Jetson NX, SDN also consumes \nmore than 10 times less power. \n \n \n \nTab2. Power performance of different devices \nModel\nDevice\nPower(W)\nDNN\nNvidia RTX2080\n64\nDNN\nNvidia RTX8000\n62\nDNN\nNvidia Jetson NX\n1.429\nSDN\nPKU NC64C\n0.103\n \nDiscussion and Conclusion \nCompared with DNN, SNN has lower power consump-\ntion and faster speed, and is closer to the form of brain \nneuron paradigm. Compared with other types of machine \nlearning, reinforcement learning is closer to the essence of \nbiological learning growth. Therefore, it is of great signifi-\ncance to explore the combination of the two. \nThis paper proposes a new SNN reinforcement learning \nalgorithm: Spiking Distillation Network (SDN) with \nSTBP. This method hands over the search of action value \nspace to DNN, and makes DNN teacher network train SNN \nstudent network through similar methods of knowledge \ndistillation. SDN can achieve better results than traditional \nSNN and DNN reinforcement learning algorithms. \nWe use the formula to prove that STBP is not conducive \nto the search of action value space, indicating the necessity \nof SDN. In addition, we proved through experiments that \nthe convergence speed of SDN is faster than the traditional \nSNN and DNN reinforcement learning algorithms, which \nshows high efficiency of SDN. At the same time, we also \nproved through experiments that the capacity of student net \nin SDN can be much smaller than that of teacher net, \nwhich shows compressibility of SDN. Finally, we deploy \nSDN on PKU NC64C chip and the results show that its \npower consumption is less than other devices, which shows \nlow power consumption of SDN. \nFor the future work, we can transplant more DNN rein-\nforcement learning algorithms into SDN, such as A3C, \nDDPG, etc., try to use other distillation methods to im-\nprove the accuracy of SNN in SDN, and use new methods \nto conduct more game training. devices, such as NVIDIA \nJetson NX, SDN also consumes less than 10 times power. \nReference \n[1] Finger S. Origins of neuroscience: a history of explorations \ninto brain function[M]. Oxford University Press, USA, 2001. \n[2] Rosenblatt F. The perceptron: a probabilistic model for in-\nformation storage and organization in the brain[J]. Psychological \nreview, 1958, 65(6): 386. \n[3] Pisarchik A N, Maksimenko V A, Hramov A E. From novel \ntechnology to novel applications: Comment on “An integrated \nbrain-machine interface platform with thousands of channels” by \nElon Musk and Neuralink[J]. Journal of medical Internet re-\nsearch, 2019, 21(10): e16356. \n[4] Carpenter G A, Grossberg S. A massively parallel architec-\nture for a self-organizing neural pattern recognition machine[J]. \nComputer vision, graphics, and image processing, 1987, 37(1): \n54-115. \n[5] Akopyan F, Sawada J, Cassidy A, et al. Truenorth: Design \nand tool flow of a 65 mw 1 million neuron programmable neu-\nrosynaptic chip[J]. IEEE transactions on computer-aided design \nof integrated circuits and systems, 2015, 34(10): 1537-1557. \n[6] Davies M, Srinivasa N, Lin T H, et al. Loihi: A neuromor-\nphic manycore processor with on-chip learning[J]. Ieee Micro, \n2018, 38(1): 82-99. \n[7] Pei J, Deng L, Song S, et al. Towards artificial general in-\ntelligence with hybrid Tianjic chip architecture[J]. Nature, 2019, \n572(7767): 106-111. \n[8] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classifi-\ncation with deep convolutional neural networks[J]. Advances in \nneural information processing systems, 2012, 25: 1097-1105. \n[9] Ghosh-Dastidar S, Adeli H. Spiking neural networks[J]. In-\nternational journal of neural systems, 2009, 19(04): 295-308. \n[10] Massa R, Marchisio A, Martina M, et al. An efficient spik-\ning neural network for recognizing gestures with a dvs camera on \nthe loihi neuromorphic processor[J]. arXiv preprint \narXiv:2006.09985, 2020. \n[11] Kim S, Park S, Na B, et al. Spiking-YOLO: Spiking neu-\nral network for energy-efficient object detection[C]//Proceedings \nof the AAAI Conference on Artificial Intelligence. 2020, 34(07): \n11270-11277. \n[12] Zheng N, Mazumder P. Hardware-Friendly Actor-Critic \nReinforcement Learning Through Modulation of Spiking-Timing \nDependent Plasticity[J]. IEEE Transactions on Computers, 2017, \n66(2). \n[13] Yuan M, Wu X, Yan R, et al. Reinforcement learning in \nspiking neural networks with stochastic and deterministic synap-\nses[J]. Neural computation, 2019, 31(12): 2368-2389. \n[14] Azimirad V, Sani M F. Experimental study of reinforce-\nment learning in mobile robots through spiking architecture of \nThalamo-Cortico-Thalamic circuitry of mammalian brain[J]. Ro-\nbotica, 2020, 38(9): 1558-1575. \n[15] Azimirad V, Sani M F. Experimental study of reinforce-\nment learning in mobile robots through spiking architecture of \nThalamo-Cortico-Thalamic circuitry of mammalian brain[J]. Ro-\nbotica, 2020, 38(9): 1558-1575. \n[16] Patel D, Hazan H, Saunders D J, et al. Improved robust-\nness of reinforcement learning policies upon conversion to spik-\ning neuronal network platforms applied to Atari Breakout \ngame[J]. Neural Networks, 2019, 120: 108-115. \n[17] Wu Y, Deng L, Li G, et al. Spatio-temporal backpropaga-\ntion fo r training high-performance spiking neural networks[J]. \nFrontiers in neuroscience, 2018, 12: 331. \n[18] Gerstner W, Kistler W M. Spiking neuron models: Single \nneurons, populations, plasticity[M]. Cambridge university press, \n2002. \n[19] Tang G, Kumar N, Michmizos K P. Reinforcement co-\nlearning of deep and spiking neural networks for energy-efficient \nmapless navigation with neuromorphic hardware[J]. arXiv pre-\nprint arXiv:2003.01157, 2020. \n[20] Hinton G, Vinyals O, Dean J. Distilling the knowledge in \na neural network[J]. arXiv preprint arXiv:1503.02531, 2015. \n[21] Mnih V, Kavukcuoglu K, Silver D, et al. Playing atari \nwith deep reinforcement learning[J]. arXiv preprint \narXiv:1312.5602, 2013. \n[22] Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control \nwith deep reinforcement learning[J]. arXiv preprint \narXiv:1509.02971, 2015. \n[23] Fujimoto S, Hoof H, Meger D. Addressing function ap-\nproximation error in actor-critic methods[C]//International Con-\nference on Machine Learning. PMLR, 2018: 1587-1596. \n[24] Schulman J, Wolski F, Dhariwal P, et al. Proximal policy \noptimization algorithms[J]. arXiv preprint arXiv:1707.06347, \n2017. \n[25] Van Hasselt H, Guez A, Silver D. Deep reinforcement \nlearning with double q-learning[C]//Proceedings of the AAAI \nconference on artificial intelligence. 2016, 30(1). \n[26] Kuang Y, Cui X, Zhong Y, et al. A 64K-neuron 64M-1b-\nsynapse 2.64 pJ/SOP Neuromorphic Chip with All Memory on \nChip for Spike-based Models in 65nm CMOS[J]. IEEE Transac-\ntions on Circuits and Systems II: Express Briefs, 2021. \n \n \n \n \n \n \n \n \n",
  "categories": [
    "cs.NE",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-08-05",
  "updated": "2021-08-05"
}