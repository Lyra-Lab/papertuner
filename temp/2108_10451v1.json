{
  "id": "http://arxiv.org/abs/2108.10451v1",
  "title": "Adversarial Robustness of Deep Learning: Theory, Algorithms, and Applications",
  "authors": [
    "Wenjie Ruan",
    "Xinping Yi",
    "Xiaowei Huang"
  ],
  "abstract": "This tutorial aims to introduce the fundamentals of adversarial robustness of\ndeep learning, presenting a well-structured review of up-to-date techniques to\nassess the vulnerability of various types of deep learning models to\nadversarial examples. This tutorial will particularly highlight\nstate-of-the-art techniques in adversarial attacks and robustness verification\nof deep neural networks (DNNs). We will also introduce some effective\ncountermeasures to improve the robustness of deep learning models, with a\nparticular focus on adversarial training. We aim to provide a comprehensive\noverall picture about this emerging direction and enable the community to be\naware of the urgency and importance of designing robust deep learning models in\nsafety-critical data analytical applications, ultimately enabling the end-users\nto trust deep learning classifiers. We will also summarize potential research\ndirections concerning the adversarial robustness of deep learning, and its\npotential benefits to enable accountable and trustworthy deep learning-based\ndata analytical systems and applications.",
  "text": "Adversarial Robustness of Deep Learning:\nTheory, Algorithms, and Applications\nWenjie Ruan\nUniversity of Exeter\nExeter, UK\nw.ruan@exeter.ac.uk\nXinping Yi\nUniversity of Liverpool\nLiverpool, UK\nxinping.yi@liverpool.ac.uk\nXiaowei Huang\nUniversity of Liverpool\nLiverpool, UK\nxiaowei.huang@liverpool.ac.uk\nABSTRACT\nThis tutorial aims to introduce the fundamentals of adversarial robust-\nness of deep learning, presenting a well-structured review of up-to-date\ntechniques to assess the vulnerability of various types of deep learning\nmodels to adversarial examples. This tutorial will particularly high-\nlight state-of-the-art techniques in adversarial attacks and robustness\nverification of deep neural networks (DNNs). We will also introduce\nsome effective countermeasures to improve robustness of deep learn-\ning models, with a particular focus on adversarial training. We aim\nto provide a comprehensive overall picture about this emerging di-\nrection and enable the community to be aware of the urgency and\nimportance of designing robust deep learning models in safety-critical\ndata analytical applications, ultimately enabling the end-users to trust\ndeep learning classifiers. We will also summarize potential research\ndirections concerning the adversarial robustness of deep learning,\nand its potential benefits to enable accountable and trustworthy deep\nlearning-based data analytical systems and applications.\nCCS CONCEPTS\n• Computing methodologies →Artificial intelligence.\nACM Reference Format:\nWenjie Ruan, Xinping Yi, and Xiaowei Huang. 2021. Adversarial Robustness\nof Deep Learning: Theory, Algorithms, and Applications. In Proceedings of the\n30th ACM International Conference on Information and Knowledge Management\n(CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York,\nNY, USA, 4 pages. https://doi.org/10.1145/3459637.3482029\n1\nRATIONALE\nIn recent years, we witness significant progress has been made in\ndeep learning, which can achieve human- or superhuman-level perfor-\nmance on various data analytical tasks, such as image data recognition\n[25], natural language processing [4], and medical data analysis [5].\nGiven the prospect of a broad deployment of DNNs in a wide range\nof applications, concerns regarding the safety and trustworthiness of\ndeep learning have been recently raised [7, 39]. There is significant\nresearch that aims to address these concerns, with many publications\nappearing since the year of 2014 [11]. As we seek to deploy deep learn-\ning systems not only on virtual domains, but also in real systems, it\nbecomes critical that a deep learning model can obtain satisfactory\nperformance, but which are truly robust and reliable. Although many\nnotions of robustness and reliability exist in different communities,\none particular topic in machine learning community that has attract\nenormous attention in recent years is the adversarial robustness of\ndeep learning: a deep learning model is fragile or extremely non-robust\nCIKM ’21, November 1–5, 2021, Virtual Event, QLD, Australia\n© 2021 Association for Computing Machinery.\nThis is the author’s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of the\n30th ACM International Conference on Information and Knowledge Management (CIKM ’21),\nNovember 1–5, 2021, Virtual Event, QLD, Australia, https://doi.org/10.1145/3459637.3482029.\nto an input that is adversarially perturbed, and such perturbations\nusually are invisible or insensible by humans [7]. This is although a\nvery specific notion of robustness in general, but one that concerns\nthe safety and trustworthiness of modern deep learning systems.\nThis tutorial seeks to provide a broad, hands-on introduction to the\ntopic concerning adversarial robustness: the widespread vulnerability\nof state-of-the-art deep learning models to adversarial misclassification\n(i.e., adversarial examples). The goal is to combine both formal mathe-\nmatical treatments and practical tools and applications that highlight\nsome of the key methods and challenges concerning the adversarial\nrobustness. This tutorial will specifically concentrate on three major\nresearch progress on this direction - adversarial attacks, defences and\nverification. As detailed in Section 5, some tutorials regarding this\nemerging direction already appeared in flagship conferences in ma-\nchine learning, AI and computer vision communities, including IJCAI\n20211, ECML/PKDD 20212, ICDM 20213, CVPR 2020, KDD 20194, etc.\nOur tutorial is fundamentally different to those existing ones, which\nis i) more comprehensive: we not only cover adversarial attacks but\nparticularly concentrate on verification-based approaches which is\nable to establish formal robustness guarantees; ii) more application-\noriented: in the second part of our tutorial, we emphasize one par-\nticular defence technique that can significantly improve robustness\nof DNNs - adversarial training, which will shed a light on the devel-\nopment of robust deep learning models for real-world data analytical\napplications. The specific differences to each current similar tutorials\nare detailed in Section 5.\nThis tutorial can alarm the community to be aware of the safety\nvulnerabilities of deep learning on real-world data analytical solutions\ndespite its appealing performance. We also envision, through this\ntutorial, AI and data mining researchers and engineers get a sense\non how to evaluate the robustness of deep learning models (e.g., via\nadversarial attacks/perturbations and verification-based approaches)\nand how to design/train robust deep learning models (e.g., via defence,\nespecially adversarial training).\n2\nCONTENT DETAILS\n• Introduction to adversarial robustness: this part will introduce\nthe concept of adversarial robustness by showing some examples\nfrom computer vision [45], natural language processing [13], medi-\ncal systems [35], and autonomous systems [34]. Specifically, we will\ndemonstrate the vulnerabilities of various types of deep learning\nmodels to different adversarial examples. We will also highlight the\ndissimilarities of research focuses on adversarial robustness from\ndifferent communities, i.e., attack, defense and verification.\n1Towards Robust Deep Learning Models: Verification, Falsification, and Rectification in\nIJCAI 2021 (https://tutorial-ijcai.trustai.uk/)\n2https://tutorial-ecml.trustai.uk/\n3https://tutorial.trustdeeplearning.com/\n4Recent Progress in Zeroth Order Optimization and Its Applications to Adversarial Ro-\nbustness in Data Mining and Machine Learning, in KDD 2019, CVPR 2020\narXiv:2108.10451v1  [cs.LG]  24 Aug 2021\nCIKM ’21, November 1–5, 2021, Virtual Event, QLD, Australia\nWenjie Ruan, Xinping Yi, and Xiaowei Huang\n• Adversarial attacks: this part will detail some famous adversar-\nial attack methods with an aim to provide some insights of why\nadversarial examples exit and how to generate adversarial per-\nturbation effectively and efficiently. Specifically, we will present\nsix typical adversarial attacks, including L-BFGS [30], FGSM [8],\nC&W [3], ZeroAttack [31], spatial-transformed attacks [38], univer-\nsal attacks [44]. In the end of this part, we will also briefly introduce\nsome adversarial attacks on other domains, including attacks on sen-\ntiment analysis systems [13], attacks on 3D point cloud models [9],\nattacks on audio recognition systems [1].\n• Adversarial defense: this part will present an overview of state-\nof-the-art robust optimisation techniques for adversarial training\n[18], with emphasis on distributional robustness and the interplay\nbetween robustness and generalisation. In particular, adversarial\ntraining with Fast Gradient Method (FGM) [8], Projected Gradient\nMethod (PGM) [16] will be introduced briefly, followed by the ad-\nvanced methods promoting distributional robustness [27] from the\nviewpoints of robustness versus accuracy [42], supervised versus\nsemi-supervised learning [20], and the exploitation of local and\nglobal data information [22, 41]. In addition, the interplay between\nrobustness and generalisation will be discussed with respect to\ngeneralisable robustness and robust generalisation. A variety of reg-\nularisation techniques such as spectral normalisation [19], Lipschitz\nregularisation [32], and weight correlation regularisation [14] to\npromote generalisable robustness will be discussed, together with\nsome recent advances to improve robust generalisation [26, 40, 43].\n• Verification and validation: this part will review the state-of-\nthe-art on the verification techniques for checking whether a deep\nlearning model is dependable. First, we will discuss verification tech-\nniques for checking whether a convolutional neural network is ro-\nbust against an input, including constraint solving based techniques\n(MILP, Reluplex [15]), approximation techniques (MaxSens [37],\nAI2 [6], DeepSymbol [17]), and global optimisation based techniques\n(DLV [12], DeepGO [23, 24], DeepGame [33, 36]). Second, we will\ndiscuss some software testing based methods which generate a large\nset of test cases according to coverage metrics, including e.g., Deep-\nXplore [21], and DeepConcolic [2, 28, 29], and their extension to\nrecurrent neural networks [10]. The dependability of a learning\nmodel can then be estimated through the test cases. Third, we will\ndiscuss how to extend these above techniques to work with a relia-\nbility notion which considers all possible inputs in an operational\nscenario [46]. This requires the consideration of robustness and gen-\neralisation in a holistic way [14, 47]. Finally, we will summarize and\noutlook current state of this research field and future perspectives.\n3\nTARGET AUDIENCE AND PREREQUISITES\nThis tutorial motivates and explains a topic of emerging importance\nfor AI, and it is particularly devoted to anyone who is concerning the\nsafety and robustness of deep learning models. The target audience\nwould be data mining and AI researchers and engineers who wish to\nlearn how the techniques of adversarial attacks and verification as\nwell as adversarial training can be effectively used in evaluating and\nimproving the robustness of deep learning models. No knowledge of\nthe tutorial topics is assumed. A basic knowledge of deep learning and\nstatistical pattern classification is requested.\n4\nBENEFITS\nDeep learning techniques now is not only pervasive in the community\nof computer vision and machine learning, but also widely applied\non data analytical systems. For researchers and industrial practition-\ners who are developing safety-critical systems such as health data\nanalytics, malware detection, and automatic disease diagnosis, the\nrobustness and reliability of deep learning models are profoundly im-\nportant. CIKM, as a flagship conference in data mining and knowledge\nmanagement, has attracted huge amount of data scientists and en-\ngineers and many of them are using deep learning techniques. We\nenvision, by presenting a tutorial concerning the robustness of deep\nlearning at CIKM’21, the community can i) be aware the vulnerability of\ndeep learning models, ii) understand why such vulnerability exits in deep\nlearning and how to evaluate its adversarial robustness, and iii) know\nhow to train a robust deep learning model. We believe, those mentioned\ngoals are appealing to the audience in CIKM’21. In the meantime, a\ntutorial5 conerning similar topic already appeared in SIGKDD’19 and\nICDM 2020, two flagship conferences in data mining. As such, we\nbelieve it is necessary and urgent to propose a more comprehensive\ntutorial concentrating this topic in CIKM’21 as well.\n5\nDIFFERENCE TO SIMILAR TUTORIALS\n• Rigorous Verification and Explanation of ML Models, in AAAI 2020.\nLink: https://alexeyignatiev.github.io/aaai20-tutorial/\n– The overlapping will be on the verification part, where the above\ntutorial only considers from the logic/binary perspective, while\nwe will cover comprehensively constraint-solving based methods,\napproximation methods, and global optimisation methods. The\nother two topics of this tutorial, i.e., adversarial attack (Part-I)\nand defense (Part-III), are not covered in the above tutorial.\n• Adversarial Machine Learning, in AAAI 2019, AAAI 2018. Link:\nhttps://aaai19adversarial.github.io/index.html#org\n– The above tutorial focuses on adversarial attacks of classifier\nevasion and poisoning as well as the corresponding defense tech-\nniques, while this tutorial places the emphasis on the fundamen-\ntals of adversarial examples (Part I) and generalisable robust opti-\nmisation techniques for defense (Part III), which are not covered\nby the above tutorial. In addition, the topic of verification of this\ntutorial (Part II) is not covered at all in the above one.\n• Adversarial Machine Learning, in IJCAI 2018, ECCV 2018, ICCV 2017.\nLink: https://www.pluribus-one.it/research/sec-ml/wild-patterns\n– The above tutorial concentrates on demonstrating vulnerability\nof various machine learning models and the design of learning-\nbased pattern classifiers in adversarial environments. Our tutorial,\nhowever, is primarily about adversarial robustness of deep neural\nnetworks, especially the safety verification (Part-II) and adver-\nsarial defense (Part-III) on DNNs are not covered by the above\ntutorial. The only overlapping will be the part of adversarial at-\ntacks, but ours is more comprehensive and deep learning-focused.\n• Adversarial Robustness: Theory and Practice, in NIPS 2018\nLink: https://adversarial-ml-tutorial.org/\n– The above tutorial was given two years ago, concentrating on the\nverification-based approaches to establishing formal guarantees\nfor adversarial robustness. Then it presents adversarial training\nand regularization-based methods for improving robustness. Our\ntutorial will be more up-to-date. The major differences are: 1)\nin adversarial attacks, we present more recent and advanced ad-\nversarial examples, such as universal and spatial-transformed\none; 2) in verification, we are more comprehensive, except for\n5Recent Progress in Zeroth Order Optimization and Its Applications to Adversarial Ro-\nbustness in Data Mining and Machine Learning, in CVPR2020, KDD 2019.\nAdversarial Robustness of Deep Learning:\nTheory, Algorithms, and Applications\nCIKM ’21, November 1–5, 2021, Virtual Event, QLD, Australia\nthe constraint-solving based methods, we also cover the approxi-\nmation and global optimisation methods; and (3) in adversarial\ndefense, our focus is on generalisable adversarial training via\nadvanced spectral regularisation.\n• Recent Progress in Zeroth Order Optimization and Its Applications\nto Adversarial Robustness in Data Mining and Machine Learning,\nin CVPR 2020, KDD 2019.\nLink: https://sites.google.com/view/adv-robustness-zoopt\n– The above tutorial concentrates on Zero-order optimization meth-\nods with a particular focus on black-box adversarial attacks to\nDNNs. Our tutorial is more comprehensive, which not only covers\na wider range of adversarial attacks, but also presents verification\napproaches that can establish formal guarantees on adversarial\nrobustness, as well as review state-of-the-art adversarial training\nmethods that can defence adversarial attacks and improve DNN’s\nrobustness.\n6\nRELEVANT EXPERIENCE ON THE TOPIC\n6.1\nRelevant Tutorial Experience\n• W. Ruan, X. Yi, X. Huang, Tutorial “Adversarial Robustness of\nDeep Learning Theory, Algorithms, and Applications”, The 20th\nIEEE International Conference on Data Mining (ICDM 2020),\n17-20 Nov 2020, Sorrento, Italy\nLink: https://tutorial.trustdeeplearning.com/\n• W. Ruan, E. Botoeva, X. Yi, X. Huang, Tutorial \"Towards Robust\nDeep Learning Models: Verification, Falsification, and Rectifi-\ncation\", The 30th International Joint Conference on Artificial\nIntelligence (IJCAI 2021), 21-26 Aug 2021, Canada\nLink: http://tutorial-ijcai.trustai.uk/\n• W. Ruan, X. Yi, X. Huang, Tutorial “Adversarial Robustness of\nDeep Learning Theory, Algorithms, and Applications”, The 2021\nEuropean Conference on Machine Learning and Principles and\nPractice of Knowledge Discovery in Databases (ECML/PKDD\n2021), 13-17 Sep 2021, Virtual\nLink: http://tutorial-ecml.trustai.uk/\n6.2\nCitations of Relevant Works\nThe following are the selected papers published by the presenters\nwhich are related to this tutorial6.\n(1) Safety verification of deep neural networks, CAV 2017, Citation\n= 623\n(2) Structural Test Coverage Criteria for Deep Neural Networks,\nin ACM Transactions on Embedded Computing Systems 2018,\nCitation = 196\n(3) Concolic Testing for Deep Neural Networks, in ASE 2018, Cita-\ntion = 183\n(4) Feature-guided black-box safety testing of deep neural net-\nworks, in TACAS 2018, Citation = 162\n(5) Reachability Analysis of Deep Neural Networks with Provable\nGuarantees, in IJCAI 2018, Citation = 163\n(6) A survey of safety and trustworthiness of deep neural networks:\nVerification, testing, adversarial attack and defence, and inter-\npretability, in Computer Science Review, 2020. Citation = 111\n(7) A game-based approximate verification of deep neural networks\nwith provable guarantees, in Theoretical Computer Science,\n2020. Citation = 61\n6The citation numbers are from Google Scholar on 22 August 2021\n(8) Global Robustness Evaluation of Deep Neural Networks with\nProvable Guarantees for the Hamming Distance, in IJCAI 2019,\nCitation = 53\nThe overall citations of the above papers that are closely related to\nthis tutorial are over 1,200 since 2017. The details are as below:\n• A seminal paper (1), on the safety verification of deep learning,\nhas attracted 600+ Google Scholar citations, which is one of\nthe first papers on the verification of deep learning.\n• A few papers including (5) (7) (8), on the verification of neu-\nral networks through global optimisation algorithms, have at-\ntracted 250+ citations.\n• A few papers including (2) (3) (4) (5), on the adversarial at-\ntacks of the robustness of neural networks, have attracted 500+\ncitations.\n• A recent survey paper (6), closely aligned with the topic of this\ntutorial, has attracted 100+ Google scholar citations since its\npublication in 2020.\n7\nBRIEF RESUMES OF PRESENTERS\n7.0.1\nDr Wenjie Ruan. Dr Wenjie Ruan is a Senior Lecturer of Data\nScience at University of Exeter, UK. Previously, he has worked at\nLancaster University as a lecturer, and University of Oxford as a post-\ndoctoral researcher. Dr Ruan got his PhD from University of Adelaide,\nAustralia. His series of research works on Device-free Human Localiza-\ntion and Activity Recognition for Supporting the Independent Living of\nthe Elderly have received Doctoral Thesis Excellence from The Univer-\nsity of Adelaide. He was also the recipient of the prestigious DECRA\nfellowship from ARC (Australian Research Council). Dr Ruan has pub-\nlished 30+ top-tier papers in top venues such as AAAI, IJCAI, ICDM,\nUbiComp, CIKM, ASE, etc. His recent work on reachability analysis\non deep learning is one of the most citable papers in IJCAI’18 (150+\ncitations since 2018), and his work on testing-based falsification on deep\nlearning is also one of the most citable papers in ASE’18 (150+ citations\nsince 2018). Dr. Ruan has served as Senior PC, or PC member for over\n10 conferences including IJCAI, AAAI, ICML, NeurlPS, CVPR, ICCV,\nAAMAS, ECML-PKDD, etc. His homepage is: http://wenjieruan.com/.\n7.0.2\nDr Xinping Yi. Dr Xinping Yi is a Lecturer (Assistant Professor)\nof Electrical Engineering at the University of Liverpool, UK. He re-\nceived his Ph.D. degree from Telecom ParisTech, Paris, France. Prior\nto Liverpool, he worked at Technische Universitat Berlin, Germany,\nEURECOM, France, UC Irvine, US, and Huawei Technologies, China.\nDr Yi’s recent research lies in deep learning theory with emphasis\non generalisation and adversarial robustness. Dr Yi has published\n40+ papers in IEEE transactions such as IEEE Transactions on In-\nformation Theory (TIT), and machine learning conferences such as\nICML, NeurIPS. He has served as programme committee members\nand reviewers at a number of international conferences and jour-\nnals, such as ICML, ICLR, IJCAI, CVPR, ICCV, ISIT, ICC, TIT, Pro-\nceedings of IEEE, JSAC, TWC, Machine Learning. His homepage is:\nhttps://sites.google.com/site/xinpingyi00/\n7.0.3\nDr Xiaowei Huang. Dr Xiaowei Huang is Reader of Computer\nScience at the University of Liverpool, UK. Prior to Liverpool, he\nworked at Oxford and UNSW Sydney. Dr Huang’s research concerns\nthe safety and trustworthiness of autonomous intelligent systems. He\nis now leading a research group working on the verification, validation,\nand interpretability of deep neural networks. Dr Huang is the prin-\nciple investigator of several Dstl projects concerning the safety and\nassurance of artificial intelligence, and the Liverpool lead on a H2020\nCIKM ’21, November 1–5, 2021, Virtual Event, QLD, Australia\nWenjie Ruan, Xinping Yi, and Xiaowei Huang\nproject on the foundation of trustworthy autonomy and an EPSRC\nproject on the security of neural symbolic architectures. He is actively\nengaged with both formal verification and artificial intelligence com-\nmunities and has given a number of invited talks on topics related\nto the safety of artificial intelligence. Dr Huang has published 50+\npapers in international conferences such as AAAI, IJCAI, CAV, TACAS,\nASE, etc., and has served in program committees of 20+ international\nconferences. His homepage is: https://cgi.csc.liv.ac.uk/~xiaowei/\nAcknowledgement. Wenjie Ruan is supported by Offshore Robotics\nfor Certification of Assets (ORCA) Partnership Resource Fund (PRF) on\nTowards the Accountable and Explainable Learning-enabled Autonomous\nRobotic Systems (AELARS) [EP/R026173/1].\nXH has received fund-\ning from the European Union’s Horizon 2020 research and innovation\nprogramme under grant agreement No 956123, and is also supported\nby the UK EPSRC (through the Offshore Robotics for Certification\nof Assets [EP/R026173/1] and End-to-End Conceptual Guarding of\nNeural Architectures [EP/T026995/1]).\nREFERENCES\n[1] Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin RB\nButler, and Joseph Wilson. 2019. Practical hidden voice attacks against speech and\nspeaker recognition systems. arXiv preprint arXiv:1904.05734 (2019).\n[2] Nicolas Berthier, Youcheng Sun, Wei Huang, Yanghao Zhang, Wenjie Ruan, and\nXiaowei Huang. 2021.\nTutorials on Testing Neural Networks.\narXiv preprint\narXiv:2108.01734 (2021).\n[3] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of\nneural networks. In 2017 ieee symposium on security and privacy (sp). IEEE, 39–57.\n[4] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu,\nand Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal\nof machine learning research 12, ARTICLE (2011), 2493–2537.\n[5] Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark\nDePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean.\n2019. A guide to deep learning in healthcare. Nature medicine 25, 1 (2019), 24–29.\n[6] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaud-\nhuri, and Martin Vechev. 2018. Ai2: Safety and robustness certification of neural\nnetworks with abstract interpretation. In S&P 2018. 3–18.\n[7] Ian Goodfellow, Patrick McDaniel, and Nicolas Papernot. 2018. Making machine\nlearning robust against adversarial inputs. Commun. ACM 61, 7 (2018), 56–66.\n[8] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. [n.d.]. Explaining and\nharnessing adversarial examples. ICLR 2015 ([n. d.]).\n[9] Abdullah Hamdi, Sara Rojas, Ali Thabet, and Bernard Ghanem. 2020. Advpc: Trans-\nferable adversarial perturbations on 3d point clouds. In European Conference on\nComputer Vision. Springer, 241–257.\n[10] Wei Huang, Youcheng Sun, Xingyu Zhao, James Sharp, Wenjie Ruan, Jie Meng, and\nXiaowei Huang. 2021. Coverage-Guided Testing for Recurrent Neural Networks. IEEE\nTransactions on Reliability (2021), 1–16. https://doi.org/10.1109/TR.2021.3080664\n[11] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese\nThamo, Min Wu, and Xinping Yi. 2020. A survey of safety and trustworthiness\nof deep neural networks: Verification, testing, adversarial attack and defence, and\ninterpretability. Computer Science Review 37 (2020), 100270.\n[12] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety ver-\nification of deep neural networks. In International Conference on Computer Aided\nVerification. Springer, 3–29.\n[13] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust?\na strong baseline for natural language attack on text classification and entailment. In\nProceedings of the AAAI conference on artificial intelligence, Vol. 34. 8018–8025.\n[14] Gaojie Jin, Xinping Yi, Liang Zhang, Lijun Zhang, Sven Schewe, and Xiaowei Huang.\n2020. How does Weight Correlation Affect the Generalisation Ability of Deep Neural\nNetworks. In NeurIPS.\n[15] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. 2017.\nReluplex: An efficient SMT solver for verifying deep neural networks. In International\nConference on Computer Aided Verification. Springer, 97–117.\n[16] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial machine\nlearning at scale. International Conference on Learning Representations (2017).\n[17] Jianlin Li, Jiangchao Liu, Pengfei Yang, Liqian Chen, Xiaowei Huang, and Lijun\nZhang. 2019. Analyzing Deep Neural Networks with Symbolic Propagation: Towards\nHigher Precision and Faster Verification. In Static Analysis, Bor-Yuh Evan Chang\n(Ed.). Springer International Publishing, Cham, 296–319.\n[18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks.\nIn International Conference on Learning Representations.\n[19] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018. Spec-\ntral Normalization for Generative Adversarial Networks. In International Conference\non Learning Representations. https://openreview.net/forum?id=B1QRgziT-\n[20] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2018. Virtual\nadversarial training: a regularization method for supervised and semi-supervised\nlearning. IEEE transactions on pattern analysis and machine intelligence 41, 8 (2018),\n1979–1993.\n[21] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Automated\nwhitebox testing of deep learning systems. In SOSP. 1–18.\n[22] Zhuang Qian, Shufei Zhang, Kaizhu Huang, Qiufeng Wang, Rui Zhang, and Xinping\nYi. 2021. Improving Model Robustness with Latent Distribution Locally and Globally.\narXiv preprint arXiv:2107.04401 (2021).\n[23] Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska. 2018. Reachability anal-\nysis of deep neural networks with provable guarantees. In Proceedings of the 27th\nInternational Joint Conference on Artificial Intelligence (IJCAI). 2651–2659.\n[24] Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening, and Marta\nKwiatkowska. 2019. Global Robustness Evaluation of Deep Neural Networks with\nProvable Guarantees for Hamming Distance. In IJCAI. 5944–5952.\n[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015.\nImagenet large scale visual recognition challenge. International journal of computer\nvision 115, 3 (2015), 211–252.\n[26] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander\nMadry. 2018. Adversarially robust generalization requires more data. In NeurIPS2018.\n5019–5031.\n[27] Aman Sinha, Hongseok Namkoong, and John Duchi. 2018. Certifying Some Distribu-\ntional Robustness with Principled Adversarial Training. In International Conference\non Learning Representations.\n[28] Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, and\nRob Ashmore. 2019. Structural Test Coverage Criteria for Deep Neural Networks.\nACM Trans. Embed. Comput. Syst. 18, 5s, Article 94 (Oct. 2019), 23 pages.\n[29] Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and\nDaniel Kroening. 2018. Concolic Testing for Deep Neural Networks. In ASE. 109–119.\n[30] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.\narXiv preprint arXiv:1312.6199 (2013).\n[31] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui\nHsieh, and Shin-Ming Cheng. 2019. Autozoom: Autoencoder-based zeroth order\noptimization method for attacking black-box neural networks. In Proceedings of the\nAAAI Conference on Artificial Intelligence, Vol. 33. 742–749.\n[32] Aladin Virmaux and Kevin Scaman. 2018. Lipschitz regularity of deep neural net-\nworks: analysis and efficient estimation. In Advances in Neural Information Processing\nSystems. 3835–3844.\n[33] Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. 2018. Feature-guided\nblack-box safety testing of deep neural networks. In TACAS. 408–426.\n[34] Han Wu and Wenjie Ruan. 2021. Adversarial Driving: Attacking End-to-End Au-\ntonomous Driving Systems. arXiv preprint arXiv:2103.09151 (2021).\n[35] Han Wu, Wenjie Ruan, Jiangtao Wang, Dingchang Zheng, Bei Liu, Yayuan Geng,\nXiangfei Chai, Jian Chen, Kunwei Li, Shaolin Li, et al. 2021. Interpretable machine\nlearning for covid-19: an empirical study on severity prediction task. IEEE Transac-\ntions on Artificial Intelligence (2021).\n[36] Min Wu, Matthew Wicker, Wenjie Ruan, Xiaowei Huang, and Marta Kwiatkowska.\n2020. A game-based approximate verification of deep neural networks with provable\nguarantees. Theoretical Computer Science 807 (2020), 298–329.\n[37] Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. 2018. Output reachable\nset estimation and verification for multilayer neural networks. IEEE transactions on\nneural networks and learning systems 29, 11 (2018), 5777–5783.\n[38] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. 2018.\nSpatially Transformed Adversarial Examples. In International Conference on Learning\nRepresentations.\n[39] Peipei Xu, Wenjie Ruan, and Xiaowei Huang. 2020. Towards the quantification of\nsafety risks in deep neural networks. arXiv preprint arXiv:2009.06114 (2020).\n[40] Dong Yin, Ramchandran Kannan, and Peter Bartlett. 2019. Rademacher complexity for\nadversarially robust generalization. In International conference on machine learning.\nPMLR, 7085–7094.\n[41] Haichao Zhang and Jianyu Wang. 2019. Defense against adversarial attacks using fea-\nture scattering-based adversarial training. Advances in Neural Information Processing\nSystems 32 (2019), 1831–1841.\n[42] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael\nJordan. 2019. Theoretically principled trade-off between robustness and accuracy. In\nInternational Conference on Machine Learning. PMLR, 7472–7482.\n[43] Shufei Zhang, Zhuang Qian, Kaizhu Huang, Qiufeng Wang, Rui Zhang, and Xinping\nYi. 2021. Towards Better Robust Generalization with Shift Consistency Regularization.\nIn International Conference on Machine Learning. PMLR, 12524–12534.\n[44] Yanghao Zhang, Wenjie Ruan, Fu Wang, and Xiaowei Huang. 2020. Generalizing Uni-\nversal Adversarial Attacks Beyond Additive Perturbations. In 2020 IEEE International\nConference on Data Mining (ICDM). IEEE, 1412–1417.\n[45] Yanghao Zhang, Fu Wang, and Wenjie Ruan. 2021. Fooling Object Detectors: Adver-\nsarial Attacks by Half-Neighbor Masks. arXiv preprint arXiv:2101.00989 (2021).\n[46] Xingyu Zhao, Alec Banks, James Sharp, Valentin Robu, David Flynn, Michael Fisher,\nand Xiaowei Huang. 2020. A Safety Framework for Critical Systems Utilising Deep\nNeural Networks. In SafeCOMP. 244–259.\n[47] Xingyu Zhao, Wei Huang, Alec Banks, Victoria Cox, David Flynn, Sven Schewe, and\nXiaowei Huang. 2021. Assessing the Reliability of Deep Learning Classifiers Through\nRobustness Evaluation and Operational Profiles. In AISafety.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-08-24",
  "updated": "2021-08-24"
}