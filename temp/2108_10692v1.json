{
  "id": "http://arxiv.org/abs/2108.10692v1",
  "title": "Generalized Optimal Linear Orders",
  "authors": [
    "Rishi Bommasani"
  ],
  "abstract": "The sequential structure of language, and the order of words in a sentence\nspecifically, plays a central role in human language processing. Consequently,\nin designing computational models of language, the de facto approach is to\npresent sentences to machines with the words ordered in the same order as in\nthe original human-authored sentence. The very essence of this work is to\nquestion the implicit assumption that this is desirable and inject theoretical\nsoundness into the consideration of word order in natural language processing.\nIn this thesis, we begin by uniting the disparate treatments of word order in\ncognitive science, psycholinguistics, computational linguistics, and natural\nlanguage processing under a flexible algorithmic framework. We proceed to use\nthis heterogeneous theoretical foundation as the basis for exploring new word\norders with an undercurrent of psycholinguistic optimality. In particular, we\nfocus on notions of dependency length minimization given the difficulties in\nhuman and computational language processing in handling long-distance\ndependencies. We then discuss algorithms for finding optimal word orders\nefficiently in spite of the combinatorial space of possibilities. We conclude\nby addressing the implications of these word orders on human language and their\ndownstream impacts when integrated in computational models.",
  "text": "arXiv:2108.10692v1  [cs.CL]  13 Aug 2021\nGENERALIZED OPTIMAL LINEAR ORDERS\nA Thesis\nPresented to the Faculty of the Graduate School\nof Cornell University\nin Partial Fulﬁllment of the Requirements for the Degree of\nMasters of Science\nby\nRishi Bommasani\nAugust 2020\n© 2020 Rishi Bommasani\nALL RIGHTS RESERVED\nABSTRACT\nThe sequential structure of language, and the order of words in a sentence specif-\nically, plays a central role in human language processing. Consequently, in de-\nsigning computational models of language, the de facto approach is to present sen-\ntences to machines with the words ordered in the same order as in the original\nhuman-authored sentence. The very essence of this work is to question the im-\nplicit assumption that this is desirable and inject theoretical soundness into the\nconsideration of word order in natural language processing. In this thesis, we be-\ngin by uniting the disparate treatments of word order in cognitive science, psy-\ncholinguistics, computational linguistics, and natural language processing under\na ﬂexible algorithmic framework. We proceed to use this heterogeneous theoreti-\ncal foundation as the basis for exploring new word orders with an undercurrent\nof psycholinguistic optimality. In particular, we focus on notions of dependency\nlength minimization given the diﬃculties in human and computational language\nprocessing in handling long-distance dependencies. We then discuss algorithms\nfor ﬁnding optimal word orders eﬃciently in spite of the combinatorial space of\npossibilities. We conclude by addressing the implications of these word orders on\nhuman language and their downstream impacts when integrated in computational\nmodels.\nBIOGRAPHICAL SKETCH\nRishi Bommasani was born in Red Bank, New Jersey and raised in Marlboro, New\nJersey. Rishi received his B.A. from Cornell University with degrees in Computer\nScience and in Mathematics. He graduated magna cum laude with distinction in all\nsubjects. He continued studying at Cornell University to pursue a M.S. in Com-\nputer Science and was advised by Professor Claire Cardie. During his time as an\nundergraduate and M.S. student, Rishi received several awards including the Com-\nputer Science Prize for Academic Excellence and Leadership and multiple Out-\nstanding Teaching Assistant Awards. He has been fortunate to have completed\ntwo internships at Mozilla in the Emerging Technologies team under the advise-\nment of Dr. Kelly Davis in the DeepSpeech group. In his ﬁrst summer at Mozilla,\nhis work considered genuinely abstractive summarization systems; in his second\nsummer, his research centered on interpreting pretrained contextualized represen-\ntations via reductions to static embeddings as well as social biases encoded within\nthese representations. He has been a strong advocate for advancing undergraduate\nresearch opportunities in computer science and was the primary organizer of nu-\nmerous undergraduate reading groups, the ﬁrst Cornell Days Event for Computer\nScience, and the inaugural Cornell Computer Science Research Night as well as its\nsubsequent iterations. Rishi has been graciously supported by a NAACL Student\nVolunteer Award, ACL Student Scholarship, Mozilla Travel Grant, and NeurIPS\nStudent Travel Grant. He will begin his PhD at Stanford University in the Com-\nputer Science Department and the Natural Language Processing group in Fall 2020.\nHis PhD studies will be funded by a NSF Graduate Research Fellowship.\niii\nTo my adviser, Claire, for your unrelenting support and unwavering conﬁdence.\nYou will forever be my inspiration as a researcher and computer scientist.\nIn loving memory of Marseille.\niv\nACKNOWLEDGEMENTS\nThere are many people to whom I am grateful and without whom the thesis would\nhave been almost impossible to write (much less ﬁnish):1\nMy adviser, Claire Cardie, has shaped who I am as a researcher and computer\nscientist with seemingly eﬀortless grace. There is truly no way for me to compress\nmy gratitude for her into a few words here. In part, I must thank her for putting\nup with my constant ﬂow of ideas and for having the patience to allow me to learn\nfrom my own errant ideas and mistakes. She has truly adapted herself to accom-\nmodate my research interests and it is exactly that freedom that permitted me to\ndevelop this very thesis. She occasionally (quite kindly) remarked that she “will\nbe lost” when I leave but it is really me who will be lost without her. She has set\nan unimaginably high standard for both my PhD adviser(s) and myself to match\nin the future.\nI am also thankful for Bobby Kleinberg for the many hats he has worn (one of\nwhich was as my minor adviser). While there are countless encounters and small\nnuances I have learned from him well beyond algorithms, I think I will always hope\nto match his relentless curiosity and desire to learn. And I would like to thank\nMarty van Schijndel as his arrival at Cornell NLP has drastically changed how I\nview language, psycholinguistics, computational linguistics, and NLP. There has\nyet to be a dull moment in any of our interactions.\nI am deeply fortunate to have learned from and been guided by three rising\nstars — Vlad Niculae, Arzoo Katiyar, and Xanda Schoﬁeld. As three remarkable\n1These words are also the ﬁrst words of Claire’s thesis.\nv\nyoung professors, I am quite lucky to have been one of their ﬁrst students. What\nthey might not know is that their theses were also quite inspiring in writing my\nown; thanks for that as well. In similar spirit, Kelly Davis has been a fabulous\nadviser during my two summers at Mozilla and I truly appreciate his willingness\nto let me explore and work on problems that I proposed. Thanks to Steven Wu for\nbeing a patient and insightful collaborator as well.\nCornell NLP has been my home for the past few years and I must thank Lil-\nlian Lee for the role she played many years prior to my arriving in helping build\nthis exceptional group with Claire. Many of her papers from the late 90’s and early\n2000’s are my exact inspiration for writing well-executed research papers; her char-\nacteristic and constant insightfulness is simply sublime. I must also especially note\nYoav Artzi, who as a researcher and a friend has deeply inspired my work and\nmy commitment to being disciplined and principled. Cristian Danescu-Niculescu-\nMizil, Sasha Rush, David Mimno, and Mats Rooth have been great members at the\nweekly NLP seminar and have further broadened the set of diverse perspectives\ntowards NLP that I was privy to, further enriching me as a young scholar. More\nrecently, the C.Psyd group has become an exciting community for me to prop-\nerly face the complexities of language and the intriguing perspectives aﬀorded by\npsycholinguistics.\nAt a broader scale, Cornell CS has been truly formative in how I view the world.\nI hope I will do Bob Constable proud in viewing the world computationally. I am\nvery grateful to Kavita Bala for her untiring eﬀorts to make the department a pos-\nitive community that supports learning. And I am thankful to Joe Halpern and\nvi\nEva Tardos for being excellent all-encompassing role models of what it means to\nbe a great computer scientist and great faculty member. Similarly, Anne Bracy and\nDaisy Fan have been especially superlative for me in exemplifying great teaching.\nAdrian Sampson, Lorenzo Alvisi, Eshan Chattopadhyay, and countless others have\nall shown me the warm and collegial spirit that radiates throughout our depart-\nment. I hope to carry this forward to the next places along my journey. Too often\nunderappreciated, Vanessa Maley, Becky Stewart, Nicole Roy, Ryan Marchenese,\nand Randy Hess were all great resources that made my journey as an undergrad\nand masters student that much easier.\nRavi Ramakrishna is the person who showed me how exciting research can\ntruly be and reignited my passion for mathematics. He might not know it, but\ncounterfactually without him and the environment he fostered in MATH 2230, it\nis hard to imagine me being where I am now.\nBut that is enough with acknowledging faculty and old folks. I have been very\nfortunate to have a great number of research friends in Cornell NLP and across\nCornell who have mentored me and been great to learn alongside:\nEsin Durmus, Ge Gao, Forrest Davis, Tianze Shi, Maria Antoniak, Jack Hessel,\nXilun Chen, Xinya Du, Kai Sun, Ryan Benmalek, Laure Thompson, Max Grusky,\nAlane Suhr, Ana Smith, Justine Zhang, Greg Yauney, Liye Fu, Jonathan Chang,\nNori Kojima, Andrea Hummel, Jacob Collard, Matt Milano, Malcolm Bare.\nFurther, I have had many wonderful friends who have encouraged me, espe-\ncially Linus Setiabrata, Janice Chan2, Avani Bhargava, Isay Katsman, Tianyi Zhang,\n2I am also grateful to Janice for proofreading parts of this thesis.\nvii\nCosmo Viola, and Will Gao. I have also cherished my time with:\nAndy Zhang, Eric Feng, Jill Wu, Jerry Qu, Haram Kim, Kevin Luo, Dan Glus,\nSam Ringel, Maria Sam, Zach Brody, Tjaden Hess, Horace He, Kabir Kapoor,\nYizhou Yu, Rachel Shim, Nancy Sun, Jacob Markin, Harry Goldstein, Chris Colen,\nAyush Mittal, Cynthia Rishi, Devin Lehmacher, Brett Clancy, Daniel Nosrati, Vic-\ntoria Schneller, Jimmy Briggs, Irene Yoon, Abrahm Magaña, Danny Qiu, Katie\nBorg, Katie Gioioso, Swathi Iyer, Florian Hartmann, Dave Connelly, Sasha Badov,\nSourabh Chakraborty, Daniel Galaragga, Qian Huang, Judy Huang, Keely Wan,\nAmrit Amar, Daniel Weber, Ji Hun Kim, Victor Butoi, Priya Srikumar, Caleb Koch,\nShantanu Gore, Grant Storey, Jialu Li, Frank Li, Seraphina Lee.\nThroughout my time at Cornell CS, two sources of persistent inspiration were\nRediet Abebe and Jehron Petty. It was truly remarkable to witness the change they\ndrove in the department, and beyond, while I was there.\nI am grateful to all of the students who I have TA-d for for helping me grow as\na teacher. Of special note are the students of CS 4740 in Fall 2019 when I co-taught\nthe course with Claire; I appreciated their patience in tolerating my ﬁrst attempts to\nprepare lectures for a course.3 Similarly, I have been extremely privileged to have\nworked with and advised a number of exceptional undergraduates and masters\nstudents. I hope that I have helped them grow as researchers and better appre-\nciate the exciting challenges involved in pursuing NLP/computational linguistics\nresearch:\nAga Koc, Albert Tsao, Anna Huang, Anusha Nambiar, Joseph Kihang’a, Julie Phan,\n3My intent is for this thesis to be understandable to any student who has completed CS 4740.\nviii\nQuintessa Qiao, Sabhya Chhabria, Wenyi Guo, Ye Jiang.\nAs I prepare for the next step in my academic journey as a PhD student in the\nStanford Computer Science Department and Stanford NLP group, I would like\nto thank a number of faculty, current (or recently graduated) PhD students, and\nmembers of my graduate cohort who helped me during the decision process:\nFaculty: Percy Liang4, Dan Klein, Tatsu Hashimoto, Dan Jurafsky, Chris Potts,\nChris Manning, John Duchi, Jacob Steinhardt, Noah Smith, Yejin Choi, Jason Eis-\nner, Ben van Durme, Tal Linzen, Graham Neubig, Emma Strubell, Zach Lipton,\nDanqi Chen5, Karthik Narasimhan.\nPhD students during the process: Nelson Liu, John Hewitt, Pang Wei Koh, Urvashi\nKhandelwal, Aditi Raghunathan, Robin Jia, Shiori Sagawa, Kawin Ethayarajh, Eva\nPortelance, Sidd Karamcheti, Nick Tomlin, Eric Wallace, Cathy Chen, Sabrina\nMielke, Adam Poliak, Tim Vieira, Ryan Cotterell, Oﬁr Press, Soﬁa Serrano, Vic-\ntor Zhong, Julian Michael, Divyansh Kaushik.\n2020 PhD admits: Alisa Liu, Han Guo, Suchin Gururangan, Katherine Lee, Lisa Li,\nXikun Zhang, Aditya Gupta, Victor Sanh, Mengzhou Xia, Megha Srivastava.\nA special thank you is also due to those who helped organize virtual visits in\nlight of the unique challenges posed by the COVID-19 pandemic that spring.\nConference travel to present my research was funded by a NAACL Student\nVolunteer Award, ACL Student Scholarship, Mozilla Travel Grant, and NeurIPS\nStudent Travel Grant in addition to funding from Cornell University and Claire.\n4Percy’s own masters thesis at MIT was quite inﬂuential in writing/formatting this thesis.\n5This section was inspired by Danqi’s own dissertation.\nix\nThe ﬁnal thank you must go to my parents, Ram and Saila Bommasani, for their\npatience to allow me to explore what made me happy and their enduring encour-\nagement in allowing me to forge my own path. Few parents understand these\nsubtleties of parenting better than you.\nx\nTABLE OF CONTENTS\nBiographical Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niii\nDedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niv\nAcknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nv\nTable of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxi\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxiii\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviii\n1\nIntroduction\n2\n1.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.3\nOrganizational Outline . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n1.4\nPrevious Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2\nBackground\n10\n2.1\nPrimitives\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2\nDependency Grammars\n. . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.2.1\nDependency Parsing . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nWord Order in Natural Language Processing . . . . . . . . . . . . .\n14\n2.3.1\nOrder-Agnostic Models\n. . . . . . . . . . . . . . . . . . . . .\n14\n2.3.2\nSequential Models\n. . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.3.3\nPosition-Aware Models\n. . . . . . . . . . . . . . . . . . . . .\n21\n2.3.4\nAlternative Word Orders\n. . . . . . . . . . . . . . . . . . . .\n23\n3\nWord Order in Human Language Processing\n30\n3.1\nOrdering Behaviors . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n3.2\nLanguage Universals . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3.3\nSequential and Incremental Processing . . . . . . . . . . . . . . . . .\n36\n3.3.1\nExpectation-based Theories . . . . . . . . . . . . . . . . . . .\n38\n3.3.2\nMemory-based Theories . . . . . . . . . . . . . . . . . . . . .\n40\n3.3.3\nJoint Theories . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n3.4\nDependency Length Minimization . . . . . . . . . . . . . . . . . . .\n45\n4\nAlgorithmic Framing\n48\n4.1\nNotation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n4.2\nObjectives\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.2.1\nBandwidth . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n4.2.2\nMinimum Linear Arrangement . . . . . . . . . . . . . . . . .\n53\nxi\n4.2.3\nCutwidth\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n4.3\nAlgorithms for Combinatorial Optimization . . . . . . . . . . . . . .\n59\n4.3.1\nProjectivity Constraints\n. . . . . . . . . . . . . . . . . . . . .\n63\n4.4\nHeuristics for Mixed-Objective Optimization\n. . . . . . . . . . . . .\n68\n4.4.1\nTransposition Monte Carlo\n. . . . . . . . . . . . . . . . . . .\n69\n5\nOptimal Linear Orders for Natural Language Processing\n72\n5.1\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n5.2\nMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n5.3\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n5.4\nExperimental Conditions . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n5.4.1\nHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n5.5\nResults and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n6\nConclusions\n94\n6.1\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n6.2\nOpen Problems\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n6.3\nFuture Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n6.4\nConsequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\n6.5\nLimitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\nA Reproducibility\n174\nA.1 Additional Experimental Details\n. . . . . . . . . . . . . . . . . . . .\n174\nA.2 Code Release . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n176\nA.3 Data Access\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n176\nA.4 Contact Information\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n177\nB\nAdditional Results\n178\nxii\nLIST OF TABLES\n3.1\nBasic word orders across the world’s languages. Statistics regard-\ning the fraction of the world’s languages that primarily use a certain\nordering come from Dryer (2013a). 1376 natural languages were\nthe total number of languages in considering these statistics. Refer-\nences refer to entire works dedicated to studying the corresponding\nlanguage which rigorously demonstrate the language’s dominant\nword order. The unexplained probability mass corresponds to lan-\nguages without a dominant word order (e.g. German) or with dis-\ncontiguous constituents (e.g Wampiri). . . . . . . . . . . . . . . . .\n32\n5.1\nSummary statistics for text classiﬁcation datasets. Train, valida-\ntion, and test refer to the number of examples in the correspond-\ning dataset partition. Words\nex.\nrefers to the average number of words\nin the input sentence for each example in the union of the train-\ning data and the validation data. Unique words is the number of\nunique words in the union of the training data and the validation\ndata. Classes is the size of the label space for the task. Fail % is\nthe percentage of examples in the union of the training and valida-\ntion set where the spaCy dependency parser emits an invalid parse\n(e.g multiple syntactic roots, not a connected graph).\n. . . . . . .\n78\n5.2\nBandwidth (B), cutwidth (C), and minimum linear arrangement\n(M) scores for every (dataset, ordering rule) pair considered. . . .\n86\n5.3\nDuplicated from Table 5.2 for convenience.\nBandwidth (B),\ncutwidth (C), and minimum linear arrangement (M) scores for ev-\nery (dataset, ordering rule) pair considered. . . . . . . . . . . . . .\n87\n5.4\nFull classiﬁcation results where the result reported is the max\nacross hyperparameter settings. Results use pretrain-permute-finetune\nframework with the order speciﬁed in each row. All other hyperpa-\nrameters are set as described previously. The top part of the table\nrefers to baselines. The middle part of the table refers to orders de-\nrived from pure optimization algorithms. The bottom part of the ta-\nble refers to orders derived from heuristic algorithms we introduce\nusing Transposition Monte Carlo. The best performing or-\ndering rule for a given dataset is indicated in bold. Any ordering\nrule (that is neither the best-performing order rule nor rI) that per-\nforms at least as well as rI for a given dataset is indicated in italicized\nmagenta. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\nxiii\n5.5\nDuplicated from Table 5.4 for convenience. Full classiﬁcation re-\nsults where the result reported is the max across hyperparameter\nsettings. Results use pretrain-permute-finetune framework\nwith the order speciﬁed in each row. All other hyperparameters\nare set as described previously. The top part of the table refers\nto baselines. The middle part of the table refers to orders derived\nfrom pure optimization algorithms. The bottom part of the table\nrefers to orders derived from heuristic algorithms we introduce us-\ning Transposition Monte Carlo. The best performing order-\ning rule for a given dataset is indicated in bold. Any ordering rule\n(that is neither the best-performing order rule nor rI) that performs\nat least as well as rI for a given dataset is indicated in italicized ma-\ngenta.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\nB.1\nFull classiﬁcation results for h\n=\n32, p\n=\n0.0.\nResults use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n179\nB.2\nFull classiﬁcation results for h\n=\n64, p =\n0.02.\nResults use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n180\nB.3\nFull classiﬁcation results for h\n=\n64, p\n=\n0.2.\nResults use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n181\nxiv\nB.4\nFull classiﬁcation results for h = 128, p = 0.02.\nResults use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n182\nB.5\nFull classiﬁcation results for h\n=\n128, p =\n0.2.\nResults use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n183\nB.6\nFull classiﬁcation results for h\n=\n256, p =\n0.2.\nResults use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n184\nB.7\nFull classiﬁcation results for h = 32, p = 0.0.\nResults are re-\nported for models after they were trained for 15 epochs. Results use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n185\nxv\nB.8\nFull classiﬁcation results for h = 64, p = 0.02.\nResults are re-\nported for models after they were trained for 15 epochs. Results use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n186\nB.9\nFull classiﬁcation results for h = 64, p = 0.2.\nResults are re-\nported for models after they were trained for 15 epochs. Results use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n187\nB.10 Full classiﬁcation results for h = 128, p = 0.02. Results are re-\nported for models after they were trained for 15 epochs. Results use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n188\nB.11 Full classiﬁcation results for h = 128, p = 0.2.\nResults are re-\nported for models after they were trained for 15 epochs. Results use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n189\nxvi\nB.12 Full classiﬁcation results for h = 256, p = 0.2.\nResults are re-\nported for models after they were trained for 15 epochs. Results use\npretrain-permute-finetune framework with the order spec-\niﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle\npart of the table refers to orders derived from pure optimization al-\ngorithms. The bottom part of the table refers to orders derived from\nheuristic algorithms we introduce using Transposition Monte\nCarlo.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n190\nxvii\nLIST OF FIGURES\n2.1\nDependency parse of the given sentence.\nDependency arcs are\ndrawn canonically (above the linear sequence of words) and the\nsequence has been lowercased and dependency parsed using the\nspaCy parser (Honnibal and Montani, 2017) for English.\n. . . . .\n12\n4.1\nA garden path construction with a long-distance dependency link-\ning horse and fell. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n4.2\nA graph G with a linear layout speciﬁed by the vertex labels in the\nﬁgure. Given this linear layout, the bandwidth is 12 (this is 13 −1),\nthe cutwidth is 12 (this is due to position 1), and the minimum lin-\near arrangement score is 80\n\u0010\nthis is ∑13\ni=2 (i −1) + (4 −3) + (5 −4)\n\u0011\n. 60\n4.3\nSolutions for optimizing each of the three objectives for the graph\ngiven in Figure 4.2.\nThe linear layout is conveyed via the lin-\near ordering and the numbers refer to the original vertices in the\ngraph (as shown in Figure 4.2). The top/green graph is bandwidth-\noptimal (bandwidth of 6), the middle/blue graph is minimum lin-\near arrangement-optimal (minimum linear arrangement score of\n44), the bottom/red graph cutwidth-optimal (cutwidth of 6). The\ncyan edges drawn below the linear sequence convey the diﬀerence\nin the optimal solutions. . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n4.4\nIllustration of the disjoint strategy. The root h is denoted in bold\nand it has 2k children denoted by c1, . . . , c2k. Its children and their\nsubtrees are organized on either side. The order within each child\nsubtree is speciﬁed by a linear layouts that has previously been com-\nputed in the dynamic program. The order of the children and their\nsubtrees alternates and moving from outside to inside based on\ntheir score according to some scoring function. Hence, the subtree\nrooted at child c1 receives the highest score and the subtree roots at\nchild c2k receives the lowest score. If the root h had 2k + 1 (an odd\nnumber) of children, the strategy is followed for the ﬁrst 2k and we\ndiscuss the placement of the last child subsequently.\n. . . . . . . .\n64\nxviii\n4.5\nLinear layouts exemplifying the diﬀerence between the solutions\nproduced by the Gildea and Temperley (2007) algorithm (top) and\nour algorithm (bottom). The root h is denoted in bold. In both algo-\nrithms, the linear layouts for the children with the largest subtrees\n— the blue subtree rooted at c and the brown subtree rooted at j\n— are placed on opposite sides. The diﬀerence is the placement of\nthe green subtree rooted at child f. The arcs whose edge lengths\nchange across the two layouts are those in cyan, notably (c, h), (f, h),\nand (j, h). However, the sum of the edge lengths for (c, h) and (j, h)\nis constant across the linear layouts. Hence, the diﬀerence in mini-\nmum linear arrangement scores between the linear layouts is solely\ndictated by the length of (f, h), which is shorter in our algorithm’s\nlayout (bottom layout).\n. . . . . . . . . . . . . . . . . . . . . . . . .\n66\nxix\nLIST OF ALGORITHMS\n1\nDisjoint Strategy\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n2\nTransposition Monte Carlo . . . . . . . . . . . . . . . . . . . .\n69\n1\nCHAPTER 1\nINTRODUCTION\nIn this chapter, we set forth the motivations and contributions of this work.\n1.1\nMotivation\nNatural language plays a critical role in the arsenal of mechanisms that humans\nuse to communicate. Inherently, natural language is a rich code with fascinating\nlinguistic structure that humans rely upon to transfer information and achieve\ncommunicative goals (Shannon, 1948; Miller, 1951; Chomsky, 1956, 1965; Hockett,\n1960; Greenberg, 1963; Chomsky, 1986; Pinker and Bloom, 1990; Hawkins et al.,\n1994; Pinker, 2003; Pinker and Jackendoﬀ, 2005; Pinker, 2007; Jaeger and Tily, 2011;\nChomsky, 2014a,b; Gibson et al., 2019).\nIn spite of the fact that natural lan-\nguage is fundamentally a mechanism for human-human discourse, in recent\nyears we have witnessed the emergence of potent computational models of nat-\nural language.\nIn particular, society as a whole has come to rely on a va-\nriety of language technologies.\nProminent examples include machine transla-\ntion (Weaver, 1949/55; Shannon and Weaver, 1963; Lopez, 2008; Koehn, 2010;\nWu et al., 2016), speech recognition and synthesis (Dudley, 1939; Dudley et al.,\n1939; Yu and Deng, 2014; Chiu et al., 2018; Wang et al., 2017), information re-\ntrieval and search (Luhn, 1957; Salton, 1968, 1971; Spärck Jones, 1972; Salton, 1975;\nSalton et al., 1975; Salton and McGill, 1986; Salton, 1991; Page et al., 1999; Singhal,\n2\n2005; Manning et al., 2008; Dean, 2009; Nayak, 2019), large-scale information ex-\ntraction (Andersen et al., 1992; Chinchor et al., 1993; Grishman and Sundheim,\n1996; Cardie, 1997; Caliﬀand Mooney, 1997; Wilks, 1997; Gaizauskas and Wilks,\n1998; Etzioni et al., 2004; Choi et al., 2005; Etzioni et al., 2008; Mintz et al., 2009;\nEtzioni et al., 2011; Navigli and Ponzetto, 2012; Piskorski and Yangarber, 2013),\nand sentiment analysis (Pang et al., 2002; Turney, 2002; Pang and Lee, 2004,\n2005; Godbole et al., 2007; Pang and Lee, 2008; Bautin et al., 2008; Ye et al.,\n2009; Asur and Huberman, 2010; Liu, 2012; Chau and Xu, 2012; Li et al., 2014;\nRavi and Ravi, 2015; Xing et al., 2017). And the scope for language technologies is\nonly projected to grow even larger in the coming years (Hirschberg and Manning,\n2015).\nIn designing computational models of language, a natural consideration is spec-\nifying the appropriate algorithmic primitives. Classical approaches to algorithm\ndesign have been considered but generally have struggled to model language faith-\nfully; the exacting nature of deterministic algorithms like quick-sort is ill-suited\nto the myriad ambiguities found within natural language. Based on empirical\nﬁndings, the ﬁeld of natural language processing (NLP) has drifted towards ma-\nchine learning and probabilistic methods (Charniak, 1994; Manning and Schütze,\n1999; Jurafsky and Martin, 2000; Steedman, 2008; Hirschberg and Manning, 2015;\nGoldberg and Hirst, 2017; Eisenstein, 2019; McClelland et al., 2019) despite the ini-\ntial dismissal of such statistical approaches by Chomsky (1956). However, this\ntransition alone does not reconcile that the mathematical primitives used in ma-\n3\nchine learning and deep learning (Mitchell, 1997; Bishop, 2006; Goodfellow et al.,\n2016), i.e. vectors, aﬃne transformations, and nonlinearities, are inconsistent with\nthose present in natural language, i.e. characters, words, sentences. One of the\ncharacteristic successes of NLP in the past decade has been the development of\nword embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al.,\n2011; Mikolov et al., 2013; Pennington et al., 2014; Faruqui, 2016): explicit meth-\nods for encoding words as vectors where the abstract semantic similarity between\nwords is codiﬁed as concrete geometric similarity between vectors. In general, a\nhallmark of modern NLP is the inherent tension between linguistic representations\nand computational representations as, simply put, words are not numbers.\nIn this thesis, we study computational representations of a fundamental aspect\nof language: word order. Within natural language, sentences are a standard unit\nof analysis1 and every sentence is itself a sequence of words. The central question\nthat we consider in this thesis is whether the order of words in a sentence, as-\ncribed by the human who produced it, is the appropriate order for computational\nmodels that attempt to comprehend the sentence (in order to perform some down-\nstream task). In order to make principled progress towards answering this ques-\ntion, we contextualize our work against the backdrop of considerations of word\norder/linear order in the literature bodies of psycholinguistics and algorithms.\nFrom a psycholinguistic standpoint, the word order already attested by natural\n1The annual CUNY conference, now in its 34th iteration, is entirely dedicated to the topic of\nsentence processing.\n4\nlanguage sentences can be argued to be indicative of an optimization to facilitate\nhuman processing. Simultaneously, from an algorithmic perspective, word orders\nobserved in natural language may be computationally suboptimal with respect to\ncertain combinatorial objectives, which naturally begs the question of how (com-\nputational) processing may change when presented with optimal word orders. In\nthis sense, the unifying approach we adopt in this thesis is to interlace motivat-\ning prior work from both psycholinguistics and algorithms to specify novel word\norders, which we then evaluate empirically for downstream NLP tasks.\n1.2\nContributions\nGeneralized Optimal Linear Orders. The central contribution of this work is\na framework for constructing novel word orders via an optimization procedure\nand, thereafter, studying the impacts of these orders on downstream NLP tasks.\nConsequently, we begin by extending and connecting previously disconnected\nliterature from the algorithms community with work that focuses on modelling\nword order in NLP. We also present three novel word orders generated via the\nTransposition Monte Carlo algorithm that we introduce. These orders rely\non a simple greedy heuristic that allows for (somewhat-transparent) balancing\nof the original sentence’s word order, therefore preserving information encoded\nin the original word order, and optimization against the objectives we introduce.\nWe demonstrate how to incorporate these novel word orders, which are optimal\n5\n(with respect to a combinatorial objective), with downstream NLP. In particular,\nwe propose the pretrain-permute-finetune framework, which seamlessly\nintegrates our novel orders with large-scale pretraining. We empirically evaluate\nthe beneﬁts of our method and show it can yield improvements for English lan-\nguage text classiﬁcation tasks.\nQuantiﬁed (sub)optimality of natural language. Due to the explicit computa-\ntional framework we develop, we can further quantify the extent to which various\nnatural languages are suboptimal with respect to objectives related to dependency\nlength minimization. As we discuss subsequently, there has been signiﬁcant work\nin the psycholinguistics community towards demonstrating that human languages\nare eﬀective at dependency minimizing (compared to random word orders) and\nour work helps provide the dual perspective by clarifying the extent to which they\nare suboptimal.\nSurvey of word order in language processing. Research in human language pro-\ncessing, and sentence processing speciﬁcally, has a rich history of studying the\ninﬂuence of word order on processing capabilities in humans. While the corre-\nsponding study in natural language processing has arguably lacked similar rigor,\nthis thesis puts forth a joint summary of how multiple communities have studied\nword order in language processing.\n6\n1.3\nOrganizational Outline\nThe remainder of this thesis is organized as follows.\nWe begin in Chapter 2 (§2) by introducing fundamental preliminaries. These\ninclude a self-contained introduction to dependency grammars as well as a discus-\nsion of the disparate treatments of word order within NLP. We then examine some\nof the literature on studying word order in human languages in Chapter 3 (§3),\nwith a speciﬁc focus on cognitive and psycholinguistic arguments centered on hu-\nman language processing. We pay special attention to the line of work focused on\ndependency length minimization and dependency locality eﬀects (§3.4).\nIn Chapter 4 (§4), we shift gears by providing a generalized framework for\nstudying notions of optimality with respect to dependency length and word or-\nder. We further provide several algorithms introduced in prior work that permit\ntractable (polynomial-time) optimization of various combinatorial objectives. We\naugment these with heuristic algorithms that allow for balance between retaining\nthe original sentence’s order and purely optimizing objectives related to depen-\ndency parses.\nIn Chapter 5 (§5), we consider how the novel word orders we have constructed\ninﬂuence dependency-related costs and downstream performance in NLP. We ﬁnd\n7\nthat English already substantially optimizes for the objectives we study compared\nto a random word order baseline. Further, we show that there is still a substantial\nmargin for further optimization over English and that the heuristic algorithms we\nintroduce perform slightly worse than algorithms that are established in the liter-\nature from an optimization perspective. Intriguingly, we ﬁnd that optimizing for\nsome objectives (most notably minimum linear arrangement) can yield to improve-\nments on other objectives but does not in all cases (especially for the bandwidth\nobjective). Given these observations, we then evaluate on downstream text classi-\nﬁcation tasks. We ﬁnd that the standard English order is a strong baseline but can\nbe improved over in four of the ﬁve datasets we study (by using a novel word or-\nder introduced in this work). In particular, we show that word orders generated by\nour heuristic approach often outperform those generated by standards algorithms,\nsuggesting that word order design that strictly optimizes combinatorial objectives\nis arguably naive and may not be suﬃcient/desirable for modelling natural lan-\nguage.\nWe conclude this thesis by providing a contextualized summary of the results\nin Chapter 6 (§6). We further provide a discussion of open problems, future direc-\ntions, and broader lessons. We complement this with a transparent reporting of\nthe inherent limitations of this work.\nIn Appendix A, we provide an exhaustive set of details to fully reproduce this\n8\nwork. This includes references to code we used to conduct all experiments and gen-\nerate all tables/ﬁgures used in this work. We further provide details for accessing\nall datasets used in the work. In Appendix B, we provide additional results that\nwe did not include in the main thesis. These results help clarify the performance\nof models for suboptimal hyperparameter settings (and, implicitly, the stability of\nthe results to various hyperparameter settings).\n1.4\nPrevious Works\nThe underlying foundation for this work was originally published in Bommasani\n(2019), which was presented at ACL 2019 during the main conference in the Stu-\ndent Research Workshop. It was further presented to a machine learning audience\nat NeurIPS 2019 in the Context and Compositionality in Biological and Artiﬁcial Neural\nSystems Workshop. In both past works, part of the content that appears in §4 and\n§5 was introduced. The remainder of the thesis was speciﬁcally created for the\npurpose of this thesis. The oﬃcial version of this thesis is published in the Cornell\nUniversity Library.2\n2See https://ecommons.cornell.edu/handle/1813/103195.\n9\nCHAPTER 2\nBACKGROUND\nIn this chapter we introduce preliminary machinery that we will use throughout\nthe thesis — the dependency parse — and the existing treatments of word order\nin NLP.\n2.1\nPrimitives\nIn this thesis, we will denote a sentence by ¯s which is alternatively denoted by a\nsequence of words ⟨w1 . . . wn⟩. For simplicity of prose, we will assume sentences\ncontain no duplicates though none of the algorithms or results we present make\nuse of this assumption. Given a sentence, the task of decomposing it into its cor-\nresponding sequence of words is known as tokenization.1 In practice, while tok-\nenization technically describes breaking “natural language text [...] into distinct\nmeaningful units (or tokens)” (Kaplan, 2005), it is often conﬂated with various\ntext/string normalization processes (e.g. lowercasing).\nIn ‘separating’ languages, such as English, the use of whitespace can be taken as\na reasonably proxy for token boundaries whereas in other languages, such as Man-\n1In this thesis, we will make no distinction between the terms word and token. Similarly, we will\nnot distinguish word types (lexical categories) from word tokens (individual occurrences of word\ntypes).\n10\ndarin Chinese, this is not feasible. In general, in this work we will assume access to\na tokenizer for the language being studied and will not reconsider any errors intro-\nduced during tokenization. In particular, while tokenization is not strictly solved\n(Dridan and Oepen, 2012), high-quality tokenizers exist for a variety of languages,\nincluding some low-resource languages, in standard packages such as Stanford\nCoreNLP (Manning et al., 2014) and Stanza (Qi et al., 2020).\n2.2\nDependency Grammars\nIn this work, we consider syntactic representations of language. Speciﬁcally, we\nfocus our eﬀorts on dependency grammars, which were ﬁrst formalized in the\nmodern sense by Lucien Tesnière (Tesnière, 1959).2 Under a dependency gram-\nmar, every sentence has a corresponding dependency parse which encodes binary\nrelations between words that mark syntactic dependencies. This approach for\nspecifying a sentence-level syntactic structure diﬀers greatly from the phrase-\nbased/constituency grammars championed by Leonard Bloomﬁeld and Noam\nChomsky (Bloomﬁeld, 1933; Chomsky, 1965).\nThe central diﬀerence rests on\nhow clauses are handled: phrase-structure grammars split clauses into subject\nnoun phrases and predicate verb phrases whereas dependency grammars are verb-\nfocused. Further, phrase-structure grammar may generate nodes that do not cor-\nrespond to any single word in the sentence.\n2Nivre (2005) provides a more comprehensive primer on dependency grammars and depen-\ndency parsing.\n11\nthe reject , unlike the highly celebrated actor , won\n.\n.\n.\n.\n. .\n.\n.\n.\nFigure 2.1: Dependency parse of the given sentence. Dependency arcs are drawn canonically\n(above the linear sequence of words) and the sequence has been lowercased and dependency\nparsed using the spaCy parser (Honnibal and Montani, 2017) for English.\nFormally, a dependency grammar attributes a dependency parse G¯s to ev-\nery sentence ¯s\n=\n⟨w1 . . . wn⟩where G¯s is a directed graph with vertex set\nV =\n\b\nwi | i ∈[n]\n\t\nand edge set Eℓgiven by the directed binary dependency re-\nlations. Each dependency relation is labelled (hence a dependency parse is an\nedge-labelled directed graph) and the speciﬁc labels are based on the speciﬁc de-\npendency formalism used, which we describe subsequently. The direction of the\nedges is from the syntactic head (the source of the edge) to the syntactic child\n(the target of the edge); the head is of greater syntactic prominence/salience than\nthe child. Dependency parses are constrained to be trees and, since the main verb\nplays a central role, are often conceived as rooted trees that are rooted at the main\nverb.\nIn Figure 2.1, we provide an example of a dependency parse for the given sen-\ntence.3 As is shown, we will drawn dependency parses in this canonicalized for-\nmat where all arcs are strictly above the linear sequence of words. If the depen-\n3We do not illustrate the direction or labels of any dependency relations. The reasons for doing\nso will be made clear in §4.\n12\ndency parse, when depicted this way, has no intersecting edges (i.e. the drawing\nis a constructive proof that the underlying graph is planar), we call the depen-\ndency parse projective (Hays, 1964). Under many theories for dependency gram-\nmars, most/all sentences in most/all languages are argued to satisfy projectiv-\nity constraints. In particular, violations of projectivity in English are very infre-\nquent (Gildea and Temperley, 2007) and McDonald et al. (2005a) estimates that\nin Czech, a language that is one of the most frequent to violate projectivity, non-\nprojective sentences constitute less than 2% of all sentences. We revisit the notion\nof projectivity, as it will prove to be useful for algorithms we subsequently study,\nin §4.3.1.\n2.2.1\nDependency Parsing\nIn this work, we consider sentences that are both annotated and not annotated\nwith a gold-standard dependency parse.\nWhen sentences are annotated, they\nare taken from the Universal Dependencies Treebank4 and were introduced by\nNivre et al. (2016) with annotations following the Universal Dependencies de-\npendency formalism. When sentences are not annotated, we parse them using\noﬀ-the-shelf pretrained parsed that we describe in later sections. In particular,\nwe strictly consider unannotated data for English.\nIn English, there exist sev-\neral high-quality pretrained parsers (Dozat and Manning, 2016; Dozat et al., 2017;\nHonnibal and Montani, 2017; Shi and Lee, 2018) and dependency parsing is rel-\n4 https://universaldependencies.org/\n13\natively mature. Comparatively, for other natural languages, and especially low-\nresource languages, oﬀ-the-shelf dependency parsing is less viable (Vania et al.,\n2019) and we revisit this in §6.5.\n2.3\nWord Order in Natural Language Processing\nIn order to understand how word order should be modelled computationally, we\nbegin by cataloguing the dominant approaches to word order in the NLP literature.\nWe revisit the most pertinent methods more extensively in §5.\n2.3.1\nOrder-Agnostic Models\nGiven the challenges of modelling word order faithfully, several approaches in\nNLP to word order have entirely sacriﬁced modelling order to prioritize other per-\ntinent phenomena. In some settings, where document-scale representations are\ndesired, it has been argued that the nuances of word order within sentences is\nfairly marginal. Two well-studied regimes are the design of topic models and word\nembeddings.\nTopic Models.\nTopic models are (probabilistic) generative models of text col-\nlections that posit that the texts are generated from a small set of latent topics.\n14\nThis tradition of proposing generative models of text originates in information\nretrieval (Salton and McGill, 1986) and has led to a series of works towards de-\nsigning topic models that yield topics that well-aligned with human notions of\ntopics. Almost all topic models represent documents by their bag-of-words repre-\nsentation, hence neglecting order. The most famous topic model is Latent Dirich-\nlet Allocation (LDA) (Blei et al., 2003), which proposes a hierarchical Bayesian\napproach towards generative modelling; model parameters can be eﬃciently in-\nferred via Markov Chain Monte Carlo (Griﬃths and Steyvers, 2004), variational\ninference (Blei et al., 2003; Hoﬀman et al., 2010), Bayesian Poisson factorization\n(Gopalan et al., 2015; Schein et al., 2015, 2019), and spectral methods using either\nmethod of moments (Anandkumar et al., 2012) or anchor words/separability as-\nsumptions (Arora et al., 2012, 2013; Lee et al., 2019). Order-agnostic topic mod-\nels have seen a wide array of applications in computational social science and the\ndigital humanities; contributions have been made via textual analysis to the disci-\nplines of political science (Gerrish and Blei, 2012), literature (Underwood, 2012),\nand history (Newman and Block, 2006) among several others. For a more exten-\nsive consideration of topic models, see Alghamdi and Alfalqi (2015); Jelodar et al.\n(2019); Schoﬁeld (2019).\nWord Embeddings. Word embeddings (Bengio et al., 2003; Collobert and Weston,\n2008; Collobert et al., 2011) are learned mappings from lexical items to vectors\nthat encode natural language semantics in vector spaces. Most word embeddings\nhinge on a particular interpretation of the distributional hypothesis (Harris, 1954;\n15\nFirth, 1957). Classical methods such as LSA (Deerwester et al., 1990) factorized\nco-occurrence statistics whereas more recent neural methods (Mikolov et al., 2013;\nPennington et al., 2014; Bojanowski et al., 2017) predict various co-occurrence\nstatistics (Baroni et al., 2014). In both cases, most approaches are largely order-\nagnostic (or use low order n-gram statistics) and subsequent work has shown\nthat many neural methods for word embedding can be re-interpreted as factor-\nizations (of the pointwise mutual information) as well (Levy and Goldberg, 2014;\nLevy et al., 2015; Ethayarajh et al., 2019). Similar to topic models, word embed-\ndings have seen a wide array of applications not just within NLP (as initial-\nizations using pretrained word representations) but also beyond NLP including\nto study diachronic societal biases (Garg et al., 2018) and cultural associations\n(Kozlowski et al., 2019). For a more extensive consideration of word embeddings,\nsee Wang et al. (2019c); Faruqui (2016).\nBag-of-Words Classiﬁers. While topic models and word embeddings learn rep-\nresentations from a collection of documents, bag-of-words and order-agnostic\ntechniques have also been considered in building representations of language\nwithin a document and even within a sentence.\nMany of these methods are\nclassical approaches to text classiﬁcation. Initial approaches adapted standard\nalgorithms from the machine learning community (Mitchell, 1997) for linear\nand log-linear classiﬁcation such as the Naive Bayes and maximum entropy al-\ngorithms (Lewis and Gale, 1994; Berger et al., 1996; Domingos and Pazzani, 1997;\nMcCallum and Nigam, 1998; Lewis, 1998; Pang et al., 2002), whereas later works\n16\nconsidered nonlinear classiﬁers such as SVMs (Joachims, 1998; Pang et al., 2002)\nand feed-forward neural networks (Collobert and Weston, 2008; Collobert et al.,\n2011). Simultaneously, many of these works such as those of Lewis (1998) and\nTurney (2002) had their origins in information retrieval and information theory. In\nthese works, it was standard to use order-agnostic term frequency (TF) (Luhn, 1957)\nand inverse document frequency (IDF) (Spärck Jones, 1972) features, commonly un-\nder the joint framing of the TF-IDF weighting schema (Salton, 1991) as in the Roc-\nchio classiﬁer (Rocchio, 1971; Joachims, 1997). Comprehensive surveys and anal-\nyses of models for text classiﬁcation are provided by Yang and Pedersen (1997);\nYang and Liu (1999); Yang (1999); Aggarwal and Zhai (2012).\nOrder-Agnostic Sentence Encoders.\nFollowing the introduction of Word2Vec\nand neural networks in NLP in the early 2010’s, the community gravitated to-\nwards deep learning approaches that no longer required explicitly feature engi-\nneering. Consequently, order-agnostic approaches within sentences became less\nfrequent. Nonetheless, order-agnostic representation learning over word repre-\nsentations for sentence encoding has proven to be eﬀective as a strong (and cheap)\nbaseline. Learning-based order-agnostic sentence encoding often uses variants of\ndeep averaging networks for text classiﬁcation tasks (Iyyer et al., 2015; Chen et al.,\n2018). However, subsequent work showed that the deep averaging was unnec-\nessary and that simple averaging was suﬃcient (Wieting et al., 2016). Addition-\nally, some works have viewed averaging methods theoretically (often as random\nwalks) (Arora et al., 2016; Ethayarajh, 2018) and diﬀerent weighting schema have\n17\nemerged to better encode the fact that word-level representations do not contribute\nuniformly towards the meaning of a sequence (Arora et al., 2017).\n2.3.2\nSequential Models\nGiven that natural language has an explicit sequential structure and this structure\nis informative (hence our interest in word order), a large family of approaches in\nNLP have attempted to model the sequential nature directly.\nMarkov Models. Markov models are a family of statistical models which make\nMarkovian assumptions — assumptions that strictly bound the length of depen-\ndencies that can be modelled. In particular, a Markov model of Markov order\nn cannot model a distance of length at least n + 1 directly. Nonetheless, a re-\ncent line of theoretical results suggest that there are workarounds for modelling\nlong-distance dependencies in such models (Sharan et al., 2017, 2018). Within\nNLP, hidden Markov models (HMMs) have been used for a variety of sequence-\ntagging applications including part-of-speech tagging, named entity recogni-\ntion, and information extraction (Jelinek, 1976; Freitag and McCallum, 1999, 2000;\nToutanova et al., 2002; Collins, 2002). In using HMMs in NLP, the causal factor-\nization of the desired probabilities is generally estimated using n-gram statistics.\nIn maximum entropy Markov models (MEMMs), a maximum entropy classiﬁer\nis introduced to add expressiveness and this has been shown to be more eﬀec-\n18\ntive in most settings (Lau et al., 1993; Ratnaparkhi et al., 1994; Ratnaparkhi, 1996;\nReynar and Ratnaparkhi, 1997; Toutanova and Manning, 2000; McCallum et al.,\n2000). Alternatively, conditional random ﬁelds (CRFs) proved to be eﬀective in\nweakening the strong independence assumptions that are built into HMMs and\nthe biases5 that are inherent to MEMMs (Laﬀerty et al., 2001; Sha and Pereira,\n2003; Pinto et al., 2003; Roark et al., 2004; Peng et al., 2004; Sutton et al., 2007;\nSutton and McCallum, 2012).\nParsing.\nSequence-tagging problems, which were extensively studied using\nMarkov models, are a special case of structured prediction problems that are\nprevalent in NLP. In the well-studied setting of parsing, whether it was syn-\ntactic constituency parsing, syntactic dependency parsing, or semantic pars-\ning, several approaches were taken to jointly model the structure of the parse\nand the sequential structure of language (Kay, 1967; Earley, 1970; Charniak,\n1983; Pereira and Warren, 1983; Kay, 1986, 1989; Eisner, 1996; Collins, 1996, 1997;\nCharniak et al., 1998; Gildea and Jurafsky, 2002; Collins, 2003; Klein and Manning,\n2003b,a; Taskar et al., 2004; McDonald et al., 2005b; McDonald and Pereira, 2006;\nChen and Manning, 2014; Dozat and Manning, 2016; Dozat et al., 2017; Shi et al.,\n2017a,b; Gómez-Rodríguez et al., 2018; Shi and Lee, 2018, 2020). When compared\nto other settings where sequential modelling is required in NLP, parsing often in-\nvokes highly-specialized routines that center on the unique and rich structure in-\nvolved.\n5Towards states that had few successors.\n19\nRecurrent Neural Networks. Given the cognitive motivations for modelling lan-\nguage sequentially in computational methods, Elman (1990) pioneered the use\nof recurrent neural networks (RNNs). While these networks have a connection-\nist interpretation (Rumelhart et al., 1986; Jordan, 1989), they ultimately proved\nto be ineﬀective due to technical challenges such as vanishing/exploding gradi-\nents in representing long-distance relationships.\nConsequently, later works in-\ntroduced gated networks such as the long short-term memory (LSTM) network\n(Hochreiter and Schmidhuber, 1997). Analogous to the dramatic performance im-\nprovements experienced due to word embeddings such as Word2Vec, the com-\nmunity observed similarly beneﬁts in the early to mid 2010’s due to LSTMs. This\nprompted further inquiry into a variety of RNN variants (e.g. Cho et al., 2014a;\nBradbury et al., 2017; Lei et al., 2018; Melis et al., 2020). More recently, a line of\ntheoretical works has worked towards classifying the theoretical diﬀerences be-\ntween these variants (Schwartz et al., 2018; Weiss et al., 2018; Peng et al., 2018;\nSuzgun et al., 2019b,a; Merrill, 2019; Lin et al., 2019). This has recently culminated\nin the work of Merrill et al. (2020) which establishes a formal taxonomy that re-\nsolves the relationship between various RNN varieties and other methods from\nclassical automata theory such as weighted ﬁnite state machines.\nAttention. The emergence of neural networks in NLP for sequence modelling nat-\nurally led to their adoption in natural language generation tasks such as machine\n20\ntranslation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al.,\n2014) and summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al.,\n2016). In these settings, attention came to be a prominent modelling innovation to\nhelp induce alignment between the source and target sequences (Bahdanau et al.,\n2015; Luong et al., 2015). Since then, attention has seen application in many other\nsettings that involve sequential modelling in NLP as it enables networks to model\nlong-distance dependencies that would be otherwise diﬃcult to model due to\nthe sequential/recurrent nature of the networks. Given attention’s widespread\nadoption, a line of work has been dedicated to adding sparsity and structure\nto attention (Martins and Astudillo, 2016; Kim et al., 2017; Niculae and Blondel,\n2017; Mensch and Blondel, 2018; Malaviya et al., 2018; Peters et al., 2018a; Niculae,\n2018; Correia et al., 2019; Peters et al., 2019a) whereas a separate line of work\nhas studied its potential utility as an interpretability tool for explaining model\nbehavior (Jain and Wallace, 2019; Serrano and Smith, 2019; Strout et al., 2019;\nWiegreﬀe and Pinter, 2019; Pruthi et al., 2020).\n2.3.3\nPosition-Aware Models\nSequential models directly model the sequential nature of language. In recent\nyears, there has been an emergence and considerable shift towards using position-\naware models/set encoders. In particular, these models implicitly choose to rep-\n21\nresent a sequence ⟨w1 . . . wn⟩as the set {(wi, i) | i ∈[n]}6 as was described in\nVinyals et al. (2016). In this sense, the encoder is aware of the position but does not\nexplicitly model order (e.g. there is no explicit notion of adjacency or contiguous\nspans in this encoding process). Early works in relation extraction also considered\nposition-aware representations (Zhang et al., 2017).7\nTransformers.\nVaswani et al. (2017) introduced the Transformer architecture,\nwhich has become the dominant position-aware architecture in modern NLP. In\nparticular, all sequences are split into 512 subword units and subwords are as-\nsigned lexical embeddings and position embeddings, which are then summed\nto yield non-contextual subword representations. These 512 subword vectors are\nthen iteratively passed through a series of Transformer layers, which decompose\ninto a self-attentive layer8 and a feed-forward layer. Since these operations are\nfully parallelizable, as they have no sequential dependence, large-scale training\nof Transformers on GPU/TPU computing resources has propelled performance\nforward on a number of tasks.\nSimilarly, since these models can compute on\nmore data per unit time than sequential models like LSTMS9, they have led to\na series of massive pretrained models that include: GPT (Radford et al., 2018),\nBERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019),\n6The correspondence between arbitrary sequences and sets of this structure is bijective\n7To the author’s knowledge, this observation and citing of Vinyals et al. (2016) and Zhang et al.\n(2017) has been entirely neglected in all past works in the NLP community (c.f. Vaswani et al., 2017;\nDai et al., 2019).\n8Self-attention is attention in the sense of Bahdanau et al. (2015) where the given sequence is\nused in both roles in the attention computation.\n9Given the constraints of current hardware.\n22\nRoBERTa (Liu et al., 2019). SpanBERT (Joshi et al., 2020a), ELECTRA (Clark et al.,\n2020), ALBERT (Lan et al., 2020) and T5 (Raﬀel et al., 2019).\nPosition Representations. Given that what diﬀerentiates position-aware models\nfrom order-agnostic models is their position representations, surprisingly little\nwork has considered these representations (Bommasani and Cardie, 2019). In the\noriginal Transformer paper, position embeddings were frozen using cosine waves\nto initialize them. Recent work has put forth alternative approaches for encoding\nposition (Almarwani et al., 2019). In particular, Wang et al. (2020) demonstrate\nthat using complex-valued vectors, where the amplitude corresponds to the lex-\nical identity and the periodicity corresponds to the variation in position, can be\na principled theoretical approach for better modelling word order in Transformer\nmodels. Separately, Shaw et al. (2018) and Dai et al. (2019) argue for encoding\nposition in a relative fashion to accommodate modelling longer sequences (as the\nstandard Transformer is constrained to 512 positions).\n2.3.4\nAlternative Word Orders\nGiven that natural language processing tasks often requiring understanding an\ninput text, it is unsurprising that most works which model the input in an order-\ndependent way (generally implicitly) choose to specify the word order to be the\nsame as the order already given in the input. A frequent exception is bidirectional\n23\nmodels, which have seen applications in a number of settings. Beyond this, other\napproaches have considered alternative word orders as a mechanism for studying\nalignment between diﬀerent sequences. Much of this literature has centered on\nmachine translation.\nBidirectional Models. One natural choice for an alternative order is to use the\nreverse of the order given. For a language such as English which is read from\nleft-to-right, this would mean the order given by reading the input sequence from\nright-to-left. While some works have compared between left-to-right models and\nright-to-left models (Sutskever et al., 2014), in most downstream settings, bidirec-\ntional models are preferred. A bidirectional model is simply one that integrates\nboth the left-to-right and right-to-left models; the bidirectional RNN is a classic\nmodel of this type (Schuster and Paliwal, 1997). Shallowly bidirectional models\ndo this by independently modelling the input from left-to-right and right-to-left\nand subsequently combining (generally by concatenation or vector addition) the\nresulting output representations. Such approaches have seen widespread appli-\ncation in NLP; the ELMo pretrained model is trained in a shallowly bidirectional\nfashion (Peters et al., 2018b). In comparison, with the emergence of Transformers,\nit is possible to process part of the input (e.g. a single token) while conditioning on\nthe entirety of the remainder of the input at once. Such models are often referred\nto as deeply bidirectional; BERT (Devlin et al., 2019) is a model pretrained in this\nway by making use of a denoising objective in masked language modelling10 as\n10Masked language modelling is a cloze task where the objective is to predict the masked word\n24\nopposed to the standard causal language modelling used in ELMo.\nPermutation Models.\nFrom a language modelling perspective, a unidirec-\ntional left-to-right (causal) language model factorizes the sequence probability\np(⟨w1 . . . wn⟩) as\np (⟨w1 . . . wn⟩) =\nn\n∏\ni=1\np (wi | ⟨w1 . . . wi−1⟩) .\n(2.1)\nIn comparison, a unidirectional right-to-left language model factorizes the se-\nquence probability as\np (⟨w1 . . . wn⟩) =\nn\n∏\ni=1\np (wi | ⟨wi+1 . . . wn⟩) .\n(2.2)\nIn the recent work of Yang et al. (2019), the authors introduce a strikingly new ap-\nproach which generalizes this perspective. In particular, any given ordering of the\nsequence ⟨w1 . . . wn⟩corresponds to a unique factorization of this sequence proba-\nbility. In their model, XLNet, the authors sample factorizations uniformly (hence\nconsidering the behavior in expectation across all n! possible factorizations) and,\nalongside other modelling innovations, demonstrate that this can be eﬀective in\nlanguage modelling. As we will demonstrate, our approach could be seen adopt-\ning the perspective of trying to identify a single optimal order than sampling from\nall possible orders with equal likelihood.\nOrder Alignment. For tasks that involve multiple sequences, order plays an ad-\nin the input sequence conditional on the remainder of the sequence, which is unmasked.\n25\nditional role of facilitating (or inhibiting) alignment between the diﬀerence se-\nquences. In machine translation, the notion of alignment between the source and\ntarget languages is particularly important.11 As a consequence, two sets of ap-\nproaches towards ensuring improved alignment (explicitly) are preorders (chang-\ning the order of the source language input to resemble the target language) and\npostorders (changing the order of a monotone output translation to resemble the\ntarget language).\n• Preorders — Preorders have been well-studied in several machine transla-\ntion settings.\nIn particular, preorders have been designed using hand-\ncrafted rules (Brown et al., 1992; Collins et al., 2005; Wang et al., 2007;\nXu et al., 2009; Chang et al., 2009), using learned reorderings/rewritings\nbased on syntactic patterns (Xia and McCord, 2004; Li et al., 2007; Genzel,\n2010; Dyer and Resnik, 2010; Katz-Brown et al., 2011; Lerner and Petrov,\n2013), or based on learning-based methods that induce hierarchical fea-\ntures instead of exploiting overt syntactic cues (Tromble and Eisner, 2009;\nDeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012).\nMuch of the early work in this setting worked towards integrating the up-\nand-coming12 (phrase-based) statistical machine translation with the long-\nstanding tradition of using syntax in machine translation. With the emer-\ngence of purely neural machine translation, recent work has studied how to\n11In fact, attention (Bahdanau et al., 2015; Luong et al., 2015) emerged in machine translation\nprecisely for the purpose of better aligning the ﬁxed input sequence and the autoregressively gen-\nerated output sequence.\n12At the time.\n26\nintegrate preorders in an end-to-end fashion using neural methods as well\n(Hoshino et al., 2014; de Gispert et al., 2015; Botha et al., 2017; Kawara et al.,\n2018). Especially relevant to the current thesis is the work of Daiber et al.\n(2016), which studies the relationship between preorders (and their eﬀec-\ntiveness) and the ﬂexibility in word orders in diﬀerent languages.\n• Postorders — Given that there are two sequences involved in machine trans-\nlation, it is natural to consider postorders as the complement to preorders.\nHowever, there is a fundamental asymmetry in that preorders involve chang-\ning the input (which can be arbitrarily interacted with) whereas postorders\ninvolve changing the output after it has been generated.\nTherefore pos-\ntorders require more complex inference procedures and (ideally) require\njoint training procedures. Given this, postorders have been comparatively\nunder-studied and little evidence has been provided to indicate that there\nare signiﬁcant advantages to compensate for these substantial complications\nwhen compared to preorders. The one caveat is when developing preorders\nwould be challenging. For example, while generate a preorder for English to\nJapanese may be viable, generating a preorder for Japanese to English is far\nmore complicated (due to the syntactic patterning of both languages). There-\nfore, one may use a preorder to improve English to Japanese translation but\nwould struggle to do the same for improving Japanese to English translation.\nGiven these diﬃculties, a postorder may be attractive in the Japanese to En-\nglish setting as it is reminiscent of the English to Japanese preorder (and can\nleverage insights/learned features/parameters from generating an English to\n27\nJapanese preorder). Sudoh et al. (2011) introduced postorders for precisely\nthis reason and Goto et al. (2012) extended their method with superior pos-\ntordering techniques. Further, Mehta et al. (2015) introduced an oracle algo-\nrithm for generating orders for ten Indian languages but their work received\nlittle traction thereafter due to empirical shortcomings.\nWhile reordering to induce alignment has received the most interest in the ma-\nchine translation, the phenomena is arguably more general. In the extreme, it may\nbe appropriate in every task where there are multiple sequences of any type. In\nparticular, Wang and Eisner (2018) propose the inspired approach of constructing\nsynthetic languages from high-resource languages (where parsing data is avail-\nable) whose word order mimics a low-resource language of interest (where pars-\ning data is unavailable/limited) to facilitate cross-lingual transfer in dependency\nparsing. Rasooli and Collins (2019) also consider a similarly reordering method\non the source side to improve cross-lingual transfer in dependency parsing. In par-\nticular, it is likely that a similar approach may be of more general value in design-\ning cross-lingual and multi-lingual methods, especially when in the low-resource\nregime for the language of interest. Very recently, Goyal and Durrett (2020) pro-\npose to adapt ideas from work on preorders in machine translation to generate\nparaphrase. In particular, they repurpose the notion of preorders to construct con-\ntrollable and ﬂexible preorders based on learned syntactic variations. While most\nother subareas of NLP have yet to consider word order in dynamic ways, the ﬁnd-\nings of Wang and Eisner (2016) may prove to be a valuable resource for such study.\nIn this work, the authors introduce the Galactic Treebank, which is a collection of\n28\nhundreds of synthetic languages that are constructed as hybrids or mutations of\nreal/attested human languages (by intertwining the word order/syntactic patterns\nof the real natural languages to produce mutants).\n29\nCHAPTER 3\nWORD ORDER IN HUMAN LANGUAGE PROCESSING\nIn this chapter, we examine how word order manifests across languages and within\ncertain contexts. We go on to discuss the relationship between word order and\nsequential processing, honing in on a memory-based theory known as dependency\nlength minimization.\n3.1\nOrdering Behaviors\nThe interpretation of word order is highly language-speciﬁc. In particular, the\npremise that ordering information is meaningful to begin with is itself language-\ndependent. Languages with ﬁxed or rigid word orders tend to reliably order con-\nstituents in a certain way to convey grammaticality. English is an example of such a\nlanguage. On the other hand, other languages, such as Russian and Nunggubuyu,\nmay have more ﬂexible word orders and are therefore said to have free or ﬂexible\nword orders. Within these languages, some, like Russian, may exhibit multiple\nword ordering structures but prefer one in most settings; this is known as the dom-\ninant word order. For other languages, there is no dominant word order, as is the\ncase for Nunggubuyu (Heath, 1984). In languages with ﬂexible word orders, mor-\nphological markings (such as inﬂection) are frequently employed to convey infor-\nmation to listeners/comprehenders. In particular, Comrie (1981) and Haspelmath\n(1999) have argued that it is precisely these morphological markings that allow\n30\nﬂexible word order languages to \"compensate\" for the information that is not en-\ncoder in word order.1 In discussing word order, it is natural to narrow the scope\nto certain aspects that are of linguistic interest.\nBasic Word Orders. The ordering of constituents is a standard method for cate-\ngorizing languages (Greenberg, 1963). At the coarsest granularity, languages can\nexhibit diﬀerent canonical orderings of the subject (S), main verb (V), and object (O)\nwithin sentences that feature all three.2 In fact, it is standard to refer to this as the\nlanguage’s basic word order. In Table 3.1, we depict languages that attest each of\nthe six possible arrangements of S, V, and O as well as typological statistics regard-\ning their relative frequencies. In general, we observe that subject-initial languages\nconstitute an overwhelming fraction of the world’s languages and that OSV is the\nminority ordering by a considerable margin. While such analyses are incomplete\n(Dryer, 2013a)3, they oﬀer an immediate illustration that word ordering properties\ncan be of interest typologically (Dryer, 1997, 2013b). Next, we consider whether\nthese order properties can be used to deduce underlying properties of language as\n1These claims have been disputed by Müller (2002), but the concern here is the causal relation-\nship between ﬂexible word order and morphological markers. In particular, prior works contest\nthat morphological case is prerequisite to free word order whereas (Müller, 2002) ﬁnds evidence\nto the contrary. We take no position on this and simply note that morphological markings and\nﬂexible word orders often co-occur.\n2From a linguistic perspective, the terms subject and object are ill-speciﬁed. In accordance with\nstandard practice, we will think of the subject as the noun or noun phrase that generally exhibits\nagent-like properties and the object as the noun or noun phrase that generally exhibits patient-like\nproperties.\n3As many languages exhibit diﬀerent basic word orders across sentences whereas others. In\nparticular, in a language like German, both SOV and SVO orderings are quite common across sen-\ntences. Alternatively, in languages such as Latin and Wampiri, constituents may not be contiguous\nspans, which may complicate the notion of ordering constituents.\n31\na whole and whether we can formulate theories to explain why these orders arise.\nOrdering\n% Languages\nExample Language\nReference\nSOV\n40.99\nJapanese\n(Kuno, 1973)\nSVO\n35.47\nMandarin\n(Li and Thompson, 1981)\nVSO\n6.90\nIrish\n(Dillon and Ó Cróinin, 1961)\nVOS\n1.82\nNias\n(Brown, 2001)\nOVS\n0.80\nHixkaryana\n(Derbyshire, 1979)\nOSV\n0.29\nNadëb\n(Weir, 1994)\nTable 3.1: Basic word orders across the world’s languages. Statistics regarding the fraction of the\nworld’s languages that primarily use a certain ordering come from Dryer (2013a). 1376 natural\nlanguages were the total number of languages in considering these statistics. References refer to\nentire works dedicated to studying the corresponding language which rigorously demonstrate the\nlanguage’s dominant word order. The unexplained probability mass corresponds to languages\nwithout a dominant word order (e.g. German) or with discontiguous constituents (e.g Wampiri).\n3.2\nLanguage Universals\nGiven the set of word ordering eﬀects we have seen so far, it is natural to ask\nwhether certain patterns emerge across language languages. More strongly, one\ncan question whether there are certain universal properties which exist (and such\nhypotheses can be readily tested with experimental and statistical techniques at\npresent). Greenberg (1963) initiated this study, with early work towards studying\nthe basic word orders we have seen previously. Greenberg argued that there are\nthree determining factors that speciﬁc a basic typology over languages:\n1. A language’s basic word order\n32\n2. The prevalence of prepositions or postpositions. In languages such as Turk-\nish, arguments of a constituent systematically appear before it. In particu-\nlar, adjectives precede nouns, objects precede words, adverbs precede adjec-\ntives, and so forth. For this reason, such a language is labelled prepositional.\nIn contrast, in languages such as Thai, the argument of a constituent sys-\ntematically appears after it.\nFor this reason, such a language is labelled\npostpositional. Since many languages, such as English display both prepo-\nsitional behavior (e.g. adjectives before nouns) and postpositional behavior\n(e.g. objects after verbs), Greenberg determined the more prevalent of the\ntwo to assign this binary feature to languages.4\n3. The relative position of adjectives with respect to the nouns they modify.\nAgain, in English, the adjective precedes the noun whereas in Thai, the ad-\njective follows the noun.\nGiven these features, there are 24 = 6 × 2 × 2 possible feature triples that a lan-\nguage could display. As the wording of items 2 and 3 suggests, these can be viewed\nas instances of a broader class of local ordering preferences, we return to this point\nlater in this section. Greenberg excluded all basic word orders that had objects pre-\nceding subjects since he argued that these were never dominant word orders in a\nlanguage.5 Greenberg then studied 30 natural languages and categorized them\n4Greenberg did not consider circumpositional languages, such as Pashto and Kurdish, where\naspects of the argument appear on either side of the constituent. Circumposition is generally ob-\nserved more frequently at the morphological rather than syntactic level.\n5While this claim is false in general, it can be argued to be true for the languages Greenberg\nstudied.\n33\ninto each of these 12 groups. While the statistical validity of his work has been\nquestioned (Dryer, 1988; Hawkins, 1990; Dryer, 1998), subsequent works (espe-\ncially in recent times when data is more readily accessible and large-scale corpus\nanalyses can be conducted computationally) have clariﬁed the validity of his the-\nories (e.g. Dryer, 1992, 2013a; Hahn et al., 2020). More generally, the enterprise\nGreenberg initiated of unearthing language universals based on consistent patterns\nacross a set of sampled languages has spawn important lines of work in cognitive\nscience and linguistics.\nHarmonic Word Orders. Of the language universals that Greenberg put forth,\nperhaps the most notable have been the harmonic word orders. The term harmonic\nrefers to the fact that in some languages, the modiﬁers of a certain syntactic class\n(e.g. nouns) consistently either precede or succeed the class. For example, many\nlanguages have both numerals and adjectives precede the noun or both succeed the\nnoun as compared to language where one precedes and the other follows; the latter\ncollection of languages are referred to as disharmonic. While there has been signiﬁ-\ncant inquiry towards enumerating languages and the types of (dis)harmonies ob-\nserved (see Hawkins, 1983), our interest in harmonic word orders is the cognitive\napproach towards understanding how they may inﬂuence learning. In this sense,\nharmonic word orders have emerged as a direct line of attack for cognitive inquiry\ntowards connecting word ordering eﬀects, language learning and acquisition, and\nbroader theories of human cognition and language processing.\n34\nIn general, consideration of word order harmonies can be attributed to the reli-\nable and overwhelming statistical evidence. Given this evidence, it is natural to fur-\nther consider whether a broader cognitive explanation that extends beyond linguis-\ntics may be the source for the observed phenomena. One especially relevant line of\nwork has argued that a bias towards word order harmonies can be indicative of gen-\neral cognitive and/or processing constraints for humans (Culbertson and Kirby,\n2016). In this sense, word order harmonies contribute to simpler grammars and\na proclivity for shorter dependencies that is seen across other domains for human\ncognition. Culbertson et al. (2012) strengthen this position by demonstrating that\nadult language learners learning artiﬁcial/synthetic languages demonstrate strong\ntendencies towards word order harmonies. Culbertson and Newport (2015) fur-\nther extend these results by showing similar behaviors for child language learners\nwhile clarifying the distinction with respect to adult language learners regarding\nthe strength and nature of the bias towards harmonic word orders. More recently,\nCulbertson and Newport (2017) provide fairly resolute conﬁrmation of this the-\nory and separation of adult and child language learning with regards to harmonic\nword orders. When both children and adults are tasks with learning languages\nthat are regularly disharmonic, children fail to learn the language correctly and\ninstead innovate/fabricate novel word orders which are harmonic (where the cor-\nrect harmonic is disharmonic). In contrast, adults are able to replicate the nonhar-\nmonic patterns correctly.\nIn our work, while we do not directly appeal to cognitive results for language\n35\nlearning (especially for children), we take this to be motivation that insightful\nchoice of word orders (perhaps in a way that aligns with a learner’s inductive bias)\ncan facilitate language acquisition. Conversely, suboptimal choices may compli-\ncate language learning substantially and can cause humans (and potentially ma-\nchines) to resort to alternative strategies that better reconcile the nature of the input\nwith the underlying latent biases.\n3.3\nSequential and Incremental Processing\nIn the previous section, we catalogued a series of word ordering eﬀects in natural\nlanguage. Subsequent work has tried to directly explain the word ordering eﬀects\nand the potential underlying language universals (e.g. Hawkins, 1988) In many\nof these cases, the corresponding works in linguistics, psycholinguistics, or cogni-\ntive science that studied these phenomena either oﬀered theoretical explanations\nor empirical evidence. However, a loftier goal for psycholinguistics in particular is\nto create a broader theory for sequential language processing. In particular, such\na theory might explain the word ordering behaviors we have described previously\nas special cases.\nLanguage is processed incrementally (Sag et al., 2003).\nConsequently, any\ntheory that explains general sequential language processing must grapple with\nthis property of how humans process language. In the study of incremental lan-\n36\nguage processing, the integration function is deﬁned to be the function describ-\ning the processing diﬃculty in processing a given word wi given the preced-\ning context ⟨w1 . . . wi−1⟩(Ford et al. (1982), c.f. Tanenhaus and Trueswell, 1995;\nGibson and Pearlmutter, 1998; Jurafsky, 2003).6 In both theoretical and empirical\ninquiry towards understand human incremental language processing, most works\nmake use of some mechanism that allows for controlled variation (e.g. minimal\npair constructions) in the input and analyze the incremental processing diﬃculty\nof a human(s) comprehending the input. In empirical work, this analysis is often\nexecuted by considering diﬀerential eﬀects using a measurement mechanism for\nhuman processing (e.g reading times, reading from the scalp, pupil dilation).\nThe consequence of this work is a canonicalized pair of theories: expectation-\nbased incremental language processing and memory-based incremental language\nprocessing. The central tenet of the former is that most processing is done pre-\nemptively, since many words can be predicted by their context7 and any further\ndiﬃculty can be attributed to how surprising wi is given the preceding context. In\ncontrast, the latter theory posits that the integration cost of the current word wi\nis proportional to the challenges of integrating it with units that must have been\nretained in memory. Given the longstanding tradition of studying incremental\n6In some works (e.g. Venhuizen et al., 2019), additional context beyond the preceding linguistic\ncontext, such as the social context or world knowledge, is further modelled. We deliberately neglect\nany description of such work in our review of past work as we restrict ourselves to language under-\nstanding and language modelling that is fully communicated via the preceding linguistic context\nthroughout this thesis.\n7It is this principle that motivates causal language modelling.\n37\nlanguage processing, joint theories that seek to reconcile the approaches have also\nbeen proposed. In particular, given there is strong evidence for both theories (and\nboth often have been showed to be reasonably complementary in their explanatory\npower), joint theories seek to merge the two, as there is undisputed proof of both\npredictive processing and memory-driven forgetting in human language process-\ning.\n3.3.1\nExpectation-based Theories\nIn positing a theory of incremental processing that hinges on prediction, it is nec-\nessary to operationalize what is predicted and how processing diﬃculty emerges\nfrom failures in prediction. For this reason, expectation-based theories have largely\ncome to be dominated by surprisal-based theories (Hale, 2001; Levy, 2008a), as\nthe predictions given rise to the processing diﬃculty inherently. In particular, sur-\nprisal is an information-theoretic measure that measures how surprised or unlikely\na word wi is given the preceding context ⟨w1 . . . wi−1⟩as\nsurp (wi | ⟨w1 . . . wi−1⟩) ≜−log (p (wi | ⟨w1 . . . wi−1⟩)) .\n(3.1)\nWe will use surpθ as notation to denote when the probability distribution p is es-\ntimated using a model parameterized by weights θ. From a modelling perspec-\ntive, many methods have been used to estimate surprisal.\nIn particular, prob-\nabilistic context-free grammars (Hale, 2001; Levy, 2008a), classical n-gram lan-\nguage models (Smith and Levy, 2013), recurrent neural network language mod-\n38\nels (van Schijndel and Linzen, 2018) and syntactically-enriched recurrent neural\nnetworks grammars (Dyer et al., 2016; Hale et al., 2018) have all been used as lan-\nguage models, i.e choices of θ, to estimate this probability. Crucially, surprisal has\nbeen shown to be a reliable predictor of human reading times (robust to six orders\nof magnitude) by Smith and Levy (2013).\nSurprisal has emerged to be a workhorse of several lines of psycholinguistic\ninquiry since it provides a natural and strong linking hypothesis between density\nestimation and human behavior as well as due to its information-theoretic interpre-\ntation. In particular, surprisal can be attributed as exactly specifying the change\nto a representation that is caused by the given word wi where the representation\nencodes ⟨w1 . . . wi−1⟩, i.e. the sequence seen so far. In this sense, surprisal codi-\nﬁes the optimal Bayesian behavior and has come to be part of a broader theory of\ncognition centered on prediction and predictive coding (Friston and Kiebel, 2009;\nClark, 2013). Further, since it speciﬁes the purely optimal behavior, surprisal re-\ntains both the advantages and disadvantages associated with being representation-\nagnostic. We will revisit these in considering motivations for joint theories.\nGiven these ﬁndings, among many others, surprisal theory has strong explana-\ntory power in describing human incremental language processing. As it pertains\nto this thesis, surprisal has also been recently8 considered for the purposes of ex-\n8Prior works (e.g Ferrer-i Cancho and Solé, 2003; Ferrer-i Cancho, 2006) also considered infor-\nmation theoretic approaches to language to explain word orders but were considerably less eﬀective\n39\nplaining word ordering behaviors. In particular, Hahn et al. (2018) demonstrate\nthat surprisal and other information theoretic measures, such as point-wise mu-\ntual information, can be used to explain adjective ordering preferences in the sense\nof Greenberg (Greenberg, 1963). In particular, they are able to predict adjective\norders reliably (96.2% accuracy) using their cognitive model that is grounded in\nmutual information and makes use of memory constraints. Futrell (2019) also pro-\nvides similar evidence for word ordering behaviors being explained eﬀectively via\nthe use of information theory. Very recently, Hahn et al. (2020) strengthened this\nposition by showcasing that surprisal-based methods can be used to demonstrate\nthat Greenberg’s language universals emerge out of eﬃcient optimization within\nlanguage to facilitate communication.\n3.3.2\nMemory-based Theories\nUnder memory-based theories of incremental processing, the processing diﬃculty\nof associated with the current word wi is proportional to the diﬃculty in/error as-\nsociated with retrieving units from the context ⟨w1 . . . wi−1⟩. In particular, consider\nthe following four examples (reproduced from Futrell et al., 2020):\n(1)\na.\nBob threw out the trash.\nb.\nBob threw the trash out.\nthan the recent line of work. Further, these works used information theoretical tools but did not\nnecessarily appeal to the expectation-based theories which we consider here.\n40\nc.\nBob threw out the old trash that had been sitting in the kitchen for\nseveral days.\nd.\nBob threw the old trash that had been sitting in the kitchen for several\ndays out.\nObserve that in the ﬁrst pair of sentences, the sentences are perfectly lexically-\nmatched and both convey identical semantic interpretations. For humans, these\nsentences have similar processing complexity. However, in the latter pair of sen-\ntences, while they are again perfectly lexically-matched and again convey identical\nsemantic interpretations, they have starkly diﬀerent processing complexities. Hu-\nmans systematically ﬁnd sentence (1d) to be more challenging to process than\nsentence (1c), as has been observed by Lohse et al. (2004). Under memory-based\ntheories, many of which stem from the dependency locality theory of Gibson (1998,\n2000), this diﬃculty arises due to the increased length of the dependency between\nthrew and out. In other words, in (1d), a human must retrieve the information\nregarding threw when processing out and the error in this retrieval or its general\ndiﬃculty increases as a function of the dependency’s length. In particular, to inter-\npret any of these four sentences, it is necessary to process the syntactic dependency\nlinking threw and out; it is insuﬃcient to only process only one lexical item or the\nother to obtain the correct semantic interpretation (Jackendoﬀ, 2002).\nSeveral hypotheses have been proposed to explain what underlying mecha-\nnisms explain the observed increase in dependency as a function of length. Some\n41\nposit that there is an inherent decay in the quality of the representation in mem-\nory over time (consistent with other types of memory representations through-\nout human cognition) whereas others argue that the degradation is tightly con-\nnected with the nature of the intervening material and how it interferes with ﬂaw-\nless retention of the context. Regardless, there are numerous eﬀects in linguistics\nwhere processing diﬃculty has been showed to increase with increasing depen-\ndency length (e.g. multiple center-embeddings, prepositional phrase attachment;\nc.f. Futrell et al., 2020). Akin to surprisal theories, there is also evidence that de-\npendency locality and memory-based theories are predictive of human behaviors\n(Grodner and Gibson, 2005; Bartek et al., 2011). However, some subsequent works\nhave questioned whether dependency locality eﬀects are strong predictors of hu-\nman behavior beyond the laboratory setting; Demberg and Keller (2008a) ﬁnd no\nsuch eﬀects when evaluating using naturalistic reading time data.\n3.3.3\nJoint Theories\nGiven the representation-agnostic nature of expectation-based and surprisal the-\nories of incremental processing and the representation-dependent nature of\nmemory-based theories, joint theories must commit to being either representation-\nagnostic or representation-dependent, thereby adopting one theory as a basis.\nThen, these approaches engineer mechanisms by which to integrate the other the-\nory. In general, the motivation for studies towards building joint theories is to\ncapitalize on the observation that expectation-based and memory-based theories\n42\nof incremental processing have been generally shown to explain complementary\nphenomena.\nThe Psycholinguistically-Motivated Lexicalized Tree Adjoining Grammar of\nDemberg and Keller (2008b), which was further extended in Demberg and Keller\n(2009), Demberg (2010), and Demberg et al. (2013), was one of the ﬁrst joint ap-\nproaches. In particular, a parser (under the tree adjoining grammar formalism)\nis equipped with predict and verify operations. The predict operation is akin to\nexpectation-based predictions of processing diﬃculty. Dually, the verify operation\nis is memory-driven as it requires validating that the previously predicted struc-\ntures are indeed correct (and the cost of this veriﬁcation scales in the length of the\ndependencies/varies inversely in the strength of the dependency locality eﬀects).\nThis approach more broadly adopts the perspective of endowing a representation-\ndependent framework (here speciﬁed using the tree adjoining grammar) with pre-\ndict operations and further constraints.\nConversely, Futrell et al. (2020) have recently introduced the lossy-context sur-\nprisal model which extends the author’s previous work on noisy-context surprisal\n(Levy, 2008b, 2011; Futrell and Levy, 2017). In this work, the authors adopt a\nrepresentation-agnostic perspective grounded in surprisal theory. Based on the\nobservation that pure surprisal theory, which uses information theoretic primi-\ntives, cannot account for forgetting eﬀects, the authors suggest making the repre-\n43\nsentation of the context lossy. What the authors is a more general concern with\ninformation theory, in that information theory in the style of Shannon (1948) does\nnot account for models of bounded or imperfect computation. Consequently, if\nany information can be recovered from the (possibly arbitrarily long) preceding\ncontext, information theory will account for this information. Recovering this in-\nformation without error is likely not viable for humans.9\nInformation Locality. Given the constraints of humans (as have been implicitly\nshown in the literature on dependency locality), Futrell et al. (2020) argue for a\ntheory of information locality, which was ﬁrst introduced by Futrell (2019). Un-\nder such a theory, a memory representation mt is build at every timestep t and this\nrepresentation likely imperfectly encodes ⟨w1 . . . wt⟩.10 Consequently, specifying\nthe memory representation (and its forgetting eﬀects) appropriately, via a noise\nmodel or other lossy information-encoding mechanism, provides the grounds for\naddressing the forgetting eﬀects that surprisal theory is ill-equipped to handle.\nIn particular, the authors suggest that operationalizing this by using RNNs with\nbounded context, as in Alemi et al. (2017); Hahn and Futrell (2019), may be an\neﬀective approach. We remark that a separate line of inquiry, that directly stud-\nies information theory under computational constraints, may be more elegant and\nsensible. In particular, the theory of V-information put forth by Xu et al. (2020)\nmay prove to be a strong formalism for encoding the information theoretic prim-\n9This fact also likely holds for machines and computational models, which have bounded mem-\nory and constrained reasoning capabilities.\n10If it perfectly encodes the context, pure surprisal theory is recovered.\n44\nitives that ground surprisal as well as the bounded computational resources that\ninduce memory/forgetting eﬀects.\n3.4\nDependency Length Minimization\nBoth expectation-based theories such as surprisal theory and memory-based the-\nories such as dependency locality theory have been reasonably eﬀective is ex-\nplaining human behavior in online language processing. Arguably, the evidence\nfor expectation-based theories is stronger and it is this that prompts the recent\ndevelopment of joint theories that are primarily based on predictive processing\n(Futrell et al., 2020). However, dependency locality theories also have a longstand-\ning tradition of enjoying explanatory power with respect to word ordering eﬀects.\nIn particular, dependency locality theory naturally predicts that humans will pro-\nduce sentences that employ word orders that minimize dependency length ceteris\nparibus. While similar statement can be made regarding expectation-based theo-\nries — humans use word orders that maximize the predictability of subsequent\nwords — there is comparatively less evidence.11\nA very early predecessor of dependency length minimization is attributed to\n11However, it should be noted that recent works such as Futrell (2019) and Futrell et al. (2020)\nargue for this in instantiating a theory of information locality. In particular, Futrell et al. (2020)\nargue that the word ordering eﬀects suggested by dependency length minimization are merely\nestimates or approximations of what is truly predicted under information locality by neglecting\neﬀects beyond those marked by syntactic dependencies.\n45\nBehaghel (1932), who stated that \"what belongs together mentally is placed close\ntogether\".12 Similarly, Greenberg (1963) also provided early accounts of depen-\ndency length minimization. More nuance and statistically valid evidence of de-\npendency length minimization has been discovered for many natural languages.\nYamashita and Chang (2001) demonstrated statistically meaningful eﬀects via cor-\npus analysis for Japanese. More recently, Futrell et al. (2015) extended these re-\nsults by showing strong dependency length minimization (well beyond what\nwould be predicted by random word orders), with p < 0.0001 for 35 of the 37\nlanguages considered and p < 0.01 for the other languages (Telugu and Latin),\nby making use of the Universal Dependencies Treebank (Nivre et al., 2016). Ad-\nditional evidence has been introduced which suggests that the grammars of natu-\nral languages are designed such that word orders which necessitate long-distance\ndependencies are dispreferred (Rijkhoﬀ, 1990; Hawkins, 1990). More broadly, de-\npendency length minimization, and therefore the word order preferences it pre-\ndicts, is a core aspect of a broader argument presented by Hawkins et al. (1994);\nJaeger and Tily (2011); Gibson et al. (2019) that natural language emerges as an\neﬃcient symbolic system for facilitating human communication from the perspec-\ntive of both a speaker (language production) and a listener (language comprehen-\nsion). An excellent multi-faceted survey of the literature on dependency length\nminimization is provided by Temperley and Gildea (2018).\nGiven the ubiquitous and diverse grounds for justifying dependency length\n12This sentence is translated from German, as reproduced by Temperley and Gildea (2018).\n46\nminimization, computational research has considered the question of devising\nprovably minimal artiﬁcial languages to push dependency length minimization\nto its extreme.\nWhile the associated optimization problem of minimizing the\ncumulative/average dependency length has previously been studied in the algo-\nrithms and theory computer science community, with suﬃciently general results\n(Goldberg and Klipker, 1976; Chung, 1984), Gildea and Temperley (2007) intro-\nduce an algorithm for ﬁnding the word order that provably minimizes dependency\nsubject to projectivity constraints. We discuss this algorithm in §4.3, ﬁnding that\nthe algorithm is marginally incorrect, and study its impacts in §5.5. Further, in the\nparsing community, biasing parsers to generate short dependencies has proven to\nbe a bona ﬁde heuristic (Collins, 2003; Klein and Manning, 2004; Eisner and Smith,\n2005). In Smith and Eisner (2006), the authors note that \"95% of dependency links\ncover ≤4 words in English, Bulgarian, and Portuguese; ≤5 words in German and\nTurkish; and ≤6 words in Mandarin\", which provides further evidence to the fact\nthat dependency lengths are minimized and hence are fairly local.\n47\nCHAPTER 4\nALGORITHMIC FRAMING\nIn this chapter, we introduce the algorithmic perspective that we use to formalize\nour approach towards studying linear order in natural language.\n4.1\nNotation\nGiven a sentence ¯s = ⟨w1, . . . , wn⟩and it dependency parse G¯s = (V, Eℓ), we will\ndeﬁne E as the unlabelled and undirected underlying edge set of Eℓ.\nDeﬁnition 4.1. Linear layout — A bijective mapping π : V →[n].\nTherefore, a linear layout speciﬁes an ordering on the vertices of G¯s or, equivalently,\na re-ordering (hence a permutation1) of the words in ¯s. Denote the space of linear\nlayouts on ¯s by Sn2. Since the linear order of a sentence innately speciﬁes a linear\nlayout, we deﬁne the identity linear layout.\nDeﬁnition 4.2. Identity linear layout — A linear layout πI : V →[n] speciﬁed by:\nπI(wi) = i\n(4.1)\nDeﬁnition 4.3. Edge distance/length — A mapping dπ : E →N speciﬁed by:\ndπ(wi, wj) = |π(wi) −π(wj)|\n(4.2)\n1This is why we denote linear layouts by π.\n2Formally, Sn denotes the symmetric group on n elements.\n48\nFor example, dπI(wi, wj) = |i −j|.\nWe further introduce the sets Li and Ri which are the set of vertices to the left\n(or at) position i or the right of position i:\nLπ(i) = {u ∈V : π(u) ≤i} and Rπ(i) = {v ∈V : π(v) > i}\n(4.3)\nDeﬁnition 4.4. Edge cut — A mapping θπ : [n] →N speciﬁed by:\nθπ(i) =\n\f\f{(u, v) ∈E|u ∈Lπ(i) ∧v ∈Rπ(i)}\n\f\f\n(4.4)\nFor a more complete introduction on linear layouts, see the survey of Díaz et al.\n(2002) which details both problems and algorithms involving linear layouts.\n4.2\nObjectives\nIn studying human language processing, we are inherently constrained to view\nword order as speciﬁed by humans/according to the linear layout πI. As we con-\nsider alternative orders, we begin by assuming we have a dataset D of N examples.\nFor every sentence ¯si = ⟨w1, . . . , wn⟩∈D3, there are many possible orderings.\nConsequently, we deﬁne an ordering rule r : D →Sn as a mapping which speciﬁes\na re-ordering for every sentence in D. Given that there are a superexponential num-\nber of ordering rules (n!N)4, it is intractable to explicitly consider every possible\n3For simplicity, we assume every sentence in the dataset is length n in this section.\n4Recall that we have assumed there are no duplicate words within any sentence.\n49\nthe horse raced past the barn fell\nFigure 4.1: A garden path construction with a long-distance dependency linking horse and fell.\nordering rule for such a combinatorially-sized set, even for a single task/dataset.\nGiven that exhaustively considering all orderings is infeasible, we instead cast the\nproblem of selecting a word order for a sentence as a combinatorial optimization\nproblem. Consequently, we deﬁne an ordering rule r f , parameterized by an objec-\ntive function f, as follows:\nr f (¯si) = arg min\nπ∈Sn\nf(π, ¯si)\n(4.5)\nfor a cost function f. In §6.3, we revisit how we may handle the case when such an\noptimization is ill-posed/there exist multiple solutions.\n4.2.1\nBandwidth\nIn the previous chapter, we presented several accounts that suggest that humans\nhave fundamental limitations on their abilities to process long-distance dependen-\ncies. In general, long-distance dependencies can be a substantial complication in\nmaintaining incremental parses of a sentence. As an illustration, consider the\nexample given in Figure 4.1 as a particularly pathological case. Here, the long-\ndistance dependency between horse and fell may contribute to the confusion in\nparsing this sentence for many readers on an initial pass.\n50\nIn computational models, we have also seen treatments that restrict the ability\nto model arbitrarily long dependencies. Most saliently, in models with Markovian\nassumptions, such as the HMMs described in §2.3.2, there is a fundamental con-\nstraint that prohibits modelling dependencies of length greater than the Markov\norder. Similarly, in Transformer models when the stride size is the context window\nlength, dependencies of length greater than the context window length can simply\nnot be modelled.\nGiven the diﬃculty of handling long-distance dependencies in both the hu-\nman language processing and computational language processing settings, we can\nconsider an ordering rule which ensures that the longest dependency in every re-\nordered sentence is as short as possible. As such, we deﬁne the bandwidth cost\nfunction as follows:\nbandwidth(π, ¯s) =\nmax\n(wi,wj)∈E dπ(wi, wj)\n(4.6)\nConsequently, this deﬁnes the ordering rule rb under the framework given in Equa-\ntion 4.5.\nrb(¯s) = arg min\nπ∈Sn\nbandwidth(π, ¯s)\n(4.7)\nThe term bandwidth refers to the fact that the optimization problem given in Equa-\ntion 4.7 is known in the combinatorial optimization literature as the bandwidth\nproblem.\nThe problem was introduced in 1967 by Harary (1967) for graphs,\nthough it has been posed previously for matrices in the mid 1950s. The matrix\nformulation is equivalent, as a graph can be viewed as its adjacency matrix and\n51\nthe bandwidth of a matrix is exactly the bandwidth of a graph as it measures the\nmaximal distance from the main diagonal of any non-zero elements.\nThe bandwidth problem for matrices has a rich history that has been especially\nprompted by its applications in numerical analysis. Speciﬁcally, numerical com-\nputations can be improved (generally by reduction of space costs and ) for matri-\nces with low bandwidth in several matrix factorization (e.g. Cholesky) and ma-\ntrix multiplication schemes. As a result, bandwidth reduction has been integrated\nto various numerical analysis software (Fellows and Langston, 1994) and some li-\nbraries include algorithms for matrices with small bandwidth (Saad, 2003). In\nother contexts, bandwidth reduction has also been combined with other methods\nfor various high-volume information retrieval problems (Botafogo, 1993).\nBeyond its applied value, the problem has been also the subject of deep theo-\nretical inquiry. Extensive surveys have been written on the problem (Chinn et al.,\n1982) as well as the corresponding complexity results (Garey et al., 1978). In par-\nticular, Papadimitriou (1976) demonstrated the problem was NP-Hard for general\ngraphs.\n52\n4.2.2\nMinimum Linear Arrangement\nIn general, using bandwidth as a cost function to induce an ordering rule implies\nthat there are many improvements that are (potentially) missed. For example, a lin-\near layout for a graph with two edges, where the edge lengths are 6 and 5 achieves\nequivalent bandwidth as the the linear layout where the edge lengths are 6 and 1.\nIn this sense, the bandwidth objective may be at odds with the realities of language\nprocessing as both humans and computers must model every dependency and not\njust the longest one.\nOnce again, we turn to the prior work on human language processing for in-\nspiration. In particular, we have seen in §3.4 that the literature on dependency\nlength minimization has suggested an alternative processing cost. In particular,\nthe works of Gibson (1998, 2000) describe a cost function as given in Equation 4.8.\nThis is the exact cost function used in work demonstrating large-scale evidence of\ndependency length minimization by Futrell et al. (2015).\nminLA(π, ¯s) =\n∑\n(wi,wj)∈E\ndπ(wi, wj)\n(4.8)\nAs we have seen before, this allows us to deﬁne the associated ordering rule rm\nunder the framework given in Equation 4.5.\nrm(¯s) = arg min\nπ∈Sn\nminLA(π, ¯s)\n(4.9)\nReminiscent of the bandwidth problem, we refer to this objective as the minLA ob-\njective as a shorthand that refers to the minimum linear arrangement problem in\n53\nthe algorithms literature.5 Introduced by Harper (1964), the problem has been re-\nferred by various names including optimal linear ordering or edge sum and is some-\ntimes conﬂated with its edge-weighted analogue. Harper considered the prob-\nlem originally in the context of generating eﬀective error-correcting codes (Harper,\n1964, 1966).\nThe problem has arisen in a number of diﬀerent applications. In particular, in\nwiring problems for circuit design (e.g. VLSI layout problems), reductions to the\nminimum linear arrangement problem are frequent (Adolphson and Hu, 1973).\nSimilarly, the problem has been often used for job scheduling (Correa and Schulz,\n2004; Ravi et al., 1991).\nThe problem shares important theoretical connections\nwith the crossing number, which emerges in aesthetic graph drawing problems\n(Pach et al., 1996).\nAs we have seen, the problem and objective are also stud-\nied in more abstract settings, removed from the pure combinatorial optimization\nparadigm, including in the dependency minimization literature and for rudimen-\ntary models of neural behavior (Mitchison and Durbin, 1986).\nSimilar to the bandwidth problem, the problem has seen a number of theoreti-\ncal techniques applied to it. This has led to a number of improvements in complex-\nity results for the problem6 for restricted graph families but the general problem\n5While the objective in Equation 4.8 has also been studied in the psycholinguistics literature\nunder the name of dependency length, we choose to use the more general algorithmic jargon. In\nparticular, this helps to disambiguate this objective from others we have seen (such as bandwidth).\n6We consider this in §4.3.\n54\nis NP-Hard (Garey et al., 1974). Petit and Salgado (1998) have benchmarked the\nproblem in several settings (along with providing approximation heuristics) and\nLiu and Vannelli (1995) have given general arguments for arriving at lower bounds\non the problems.\nRelating bandwidth and minLA. The bandwidth and minimum linear arrange-\nment cost functions (Equation 4.6 and Equation 4.8) are related in that both op-\nerate over edge lengths with one invoking a max where the other invokes a sum.\nIn this sense, this is highly reminiscent of the relationship shared by p norms for\np = 1 and p = ∞. More generally, we can deﬁne a family of ordering rules rp\nparameterized by input p ∈N ∪∞as follows:\nrp(π, ¯s) = arg min\nπ∈Sn\n\u0012\n∑\n(wi,wj)∈E\ndπ(wi, wj)p\n\u00131/p\n(4.10)\nIn particular, setting p = 1 recovers the ordering rule rm for minLA as in Equa-\ntion 4.9 and setting p = ∞recovers the ordering rule rb for bandwidth as in Equa-\ntion 4.7.\n4.2.3\nCutwidth\nIn introducing the bandwidth and minLA objectives, the motivation was that pro-\ncessing long-distance dependencies was challenging for both humans and ma-\nchines. With that in mind, the length of the dependencies are not the sole property\n55\nthat may correlate with the complexity of processing (and therefore motivate re-\nordering to facilitate processing). As a complement to the length of dependencies,\nhumans also have limits to their processing capabilities regarding memory capac-\nity. In this sense, having many dependencies simultaneously active/uncompleted\nmay also correlate with cognitive load. This phenomenon has been shown for hu-\nmans across a variety of fronts, perhaps most famously in the experiments of Miller\n(1956). Miller demonstrated that humans may have fundamental hard constraints\non the number of objects they can simultaneously track in their working short-term\nmemories.7 Given that similar challenges have been found in computational lan-\nguage processing, memory mechanisms and attentional architectures have been\nproposed to circumvent this issue. Rather than introducing computational expres-\nsiveness, we next consider how to devise orders that explicitly minimize quantities\nrelated with the number of active dependencies.\nIn order to the track the number of active dependencies, we introduced the\nnotion of the edge cut previously, which describes the number of dependencies\nthat begin at or before position i under π but have yet to be completed (the other\nvertex of the edge appears after position i under π). Consequently, we deﬁne the\ncutwidth cost:\ncutwidth(π, ¯s) = max\nwi∈V θπ(i)\n(4.11)\nAs we have seen before, this allows us to deﬁne the associated ordering rule rc\n7Canonically, Miller claimed this was 7 ± 2.\n56\nunder the framework given in Equation 4.5.\nrc(¯s) = arg min\nπ∈Sn\ncutwidth(π, ¯s)\n(4.12)\nAkin to the previous two settings, the cutwidth problem is also a problem in the\ncombinatorial optimization literature pertaining to linear layouts. The problem\nemerged in the 1970’s due to Adolphson and Hu (1973) and in the late 80’s from\nMakedon and Sudborough (1989) as a formalism for circuit layout problems. In\nparticular, the cutwidth of a graph scales in the area needed for representing linear\nVLSI circuit layouts.\nAs with the previous problems, the cutwidth problem has continued to arise\nin several other applications. Botafogo (1993) used the cutwidth problem along-\nside the bandwidth problem for information retrieval, Karger (2001) studied the\nproblem for designing a PTAS for network reliability problems, and Mutzel (1995)\nconsidered the problem in automated graph drawing.\nSomewhat unlike the previous two problems, the problem has seen less theo-\nretical interest despite its numerous applications. Nonetheless, the problem was\nshown to be NP-Hard by Gavril (2011), which continues the trend seen for the\nother combinatorial optimization problems we consider.\nLinking capacity and dependency-length. Previously, we introduced the minLA\n57\ncost function as arguably rectifying an issue with bandwidth cost function. In par-\nticular, the bandwidth cost function does not explicitly model the aggregate cost\nwhich is ultimately perceived in language processing. Both humans and machine\nmust model and process all dependencies in a sequence in order to fully under-\nstand the sentential meaning. A similar inadequacy could be posited regarding\nthe cutwidth optimization problem and objective. Consequently, we deﬁne sum-\ncutwidth as:\nsum-cutwidth(π, ¯s) = ∑\nwi∈V\nθπ(i)\n(4.13)\nAs we have seen before, this allows us to deﬁne the associated ordering rule rm′\nunder the framework given in Equation 4.5.\nrm′(¯s) = arg min\nπ∈Sn\nsum-cutwidth(π, ¯s)\n(4.14)\nAs the naming convention for rm′ suggests, we note that we have already encoun-\ntered rm′ and sum-cutwidth previously.\nTheorem 4.1 (Equivalence of average edge cut and average dependency length).\nsum-cutwidth = minLA\nProof. For every edge (wi, wj) ∈E, the edge contributes its length dπ(wi, wj) to\nthe minLA cost. On the other hand, edge (wi, wj) contributes 1 to the edge cut θk\nfor k ∈\n\u0002\nπ(wi), π(wj)\n\u0001\n.8 Therefore, in aggregate, edge (wi, wj) contributes exactly\n8WLOG assume that π(wi) < π(wj), the notation [a, b) indicates {a, a + 1, . . . , b −1} for integral\na, b.\n58\n\f\f\f\f\n\u0002\nπ(wi), π(wj)\n\u0001\f\f\f\f = dπ(wi, wj) to the cutwidth cost. As this holds for every edge,\nit follows that sum-cutwidth = minLA.\n□\nCorollary 4.1.1. rm = rm′ up to the uniqueness of the solution of the combinatorial\noptimization problem.\nTo the knowledge of the authors, the following argument has not been consid-\nered in the literature on modelling language processing with relation to costs per-\ntaining to dependency length/capacity. From this perspective, the result aﬀords\nan interesting reinterpretation that dependency length minimization is equivalent\nto minimizing the number of active dependencies. In this sense, it may suggest\nthat related ﬁndings (such as those of Miller (1956)) may be more pertinent and\nthat the relationship between dependency length and memory capacity may be\nmuch more direct than previously believed.\n4.3\nAlgorithms for Combinatorial Optimization\nIn the previous section (§4.2), we introduced objectives and corresponding order-\ning rules that are motivated by challenges in both computational and human lan-\nguage processing. As a recap of the section, we consider Figure 4.2, which depicts\na graph and the evaluation of the three cost functions on the graph (given the per-\nmutation depicted using vertex labels). Further, in Figure 4.3, we observe that so-\nlutions to the problem of ﬁnding the re-ordering that minimizes each of the three\n59\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nFigure 4.2: A graph G with a linear layout speciﬁed by the vertex labels in the ﬁgure. Given this\nlinear layout, the bandwidth is 12 (this is 13 −1), the cutwidth is 12 (this is due to position 1), and\nthe minimum linear arrangement score is 80\n\u0010\nthis is ∑13\ni=2 (i −1) + (4 −3) + (5 −4)\n\u0011\n.\nobjectives can be diﬀerent. Note that, in this speciﬁc case, the minimum linear\narrangement-optimal solution is optimal for the other objectives and the cutwidth\nsolution is optimal for the bandwidth objective but not for the minimum linear ar-\nrangement objective.\nIn order to make use of these ordering rules for natural language processing\napplications, it is necessary to tractable solve each of the associated optimization\nproblems. Recall that each of these problems is NP-Hard for general graphs. In\nspite of this roadblock, also recall that we are considering applying this optimiza-\ntion to sentences equipped with dependency parses, where dependency parses are\ngraphs that are guaranteed/constrained to be trees.\n60\n13\n12\n11\n10\n5\n8\n1\n7\n6\n4\n9\n3\n2\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n13\n12\n11\n10\n9\n8\n1\n7\n6\n5\n4\n3\n2\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n13\n12\n11\n10\n9\n8\n1\n7\n6\n4\n5\n2\n3\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nFigure 4.3: Solutions for optimizing each of the three objectives for the graph given in Figure 4.2.\nThe linear layout is conveyed via the linear ordering and the numbers refer to the original vertices\nin the graph (as shown in Figure 4.2). The top/green graph is bandwidth-optimal (bandwidth of\n6), the middle/blue graph is minimum linear arrangement-optimal (minimum linear arrangement\nscore of 44), the bottom/red graph cutwidth-optimal (cutwidth of 6). The cyan edges drawn below\nthe linear sequence convey the diﬀerence in the optimal solutions.\nBandwidth. Unfortunately, the bandwidth problem remains NP-Hard for trees\nas well (Garey et al., 1978). In fact, the problem is well-known for remaining NP-\nHard for a number of graph relaxations (Monien, 1986; Díaz et al., 1999) includ-\ning fairly simple graphs like caterpillar with hair-length at most 3 (Monien, 1986).\nGiven this, one natural approach to ﬁnd tractable algorithms is to consider prov-\nable approximations. However, approximations to a factor of 1.5 do not even exist\nfor both general graphs and trees (Blache et al., 1997).9 Regardless, approximation\nguarantees are generally unhelpful as we are considering sentences-scale graphs\nand therefore small graphs, where the approximation factor may be quite signiﬁ-\ncant. Instead, in this thesis, we consider heuristics for the bandwidth problem. In\nparticular, we make use of the frequently-employed heuristic of Cuthill and McKee\n(1969). We discuss this algorithm below and defer the implementation details to\n9This further implies that the bandwidth problem does not admit a PTAS.\n61\na subsequent chapter.\nThe Cuthill-McKee algorithm begins by starting at the vertex and conducting a\nbreadth-ﬁrst search (BFS) from that vertex. The key to the algorithm’s empirical ef-\nfectiveness is that vertices are visited based on their degree (hence the starting ver-\ntex is the vertex with lowest degree). Instead of using the standard algorithm, we\nuse the Reverse Cuthill-McKee algorithm (Chan and George, 1980), which merely\nexecutes the algorithm with reversed index numbers. In theory, this modiﬁcation\nhas no beneﬁts for general graphs but empirically, it seems this modiﬁcation turns\nout to be reliably beneﬁcial (Davis, 2006; Azad et al., 2017).\nMinimum Linear Arrangement. Unlike the bandwidth problem, the minLA prob-\nlem has poly-time solutions for trees. In particular, in a series of results, the run-\ntime for the tree setting was improved from O(n3) (Goldberg and Klipker, 1976) to\nO(n2.2) (Shiloach, 1979) to O(n1.58) (Chung, 1984). While there has been progress\non developing lower bounds (Liu and Vannelli, 1995), matching bounds have been\nyet to be achieved. In this work, we elect not to use the algorithm of Chung (1984)\nand instead introduce an additional constraint and a corresponding algorithm in\na subsequent section (§4.3.1).\nCutwidth. Similar to minLA, cutwidth also permits poly-time solutions for trees.\nWhile the problem remained open as to whether this was possible for a number\n62\nof years, Yannakakis (1985) gave a O(n log(n)) algorithm. Further akin to minLA,\nwe forego this general algorithm (for trees) for one that involves projectivity con-\nstraints (§4.3.1).\n4.3.1\nProjectivity Constraints\nRecall that poly-time algorithms exist for both the minLA and cutwidth problems.\nIn both cases, the works introducing the corresponding algorithms we discussed\npreviously develop signiﬁcant algorithmic machinery to arrive at the ﬁnal algo-\nrithms. We will next see that a linguistically-motivated constraint in projectivity\nyields faster10 and simpler algorithms. Recall that projectivity refers to the prop-\nerty of a dependency parse that when the nodes are ordered on a line and the edges\nare drawn above the line, the parse has no intersecting edges. If we constraint\nthe order π outputted by either rm or rc to be projective, linear time algorithms\nare known for minLA (Gildea and Temperley, 2007) and cutwidth (Yannakakis,\n1985).11\nWe next discuss the algorithms of Gildea and Temperley (2007) and\nYannakakis (1985), showing how they both can be generalized using the frame-\nwork of disjoint strategies (Yannakakis, 1985).\n10From a practical perspective, the algorithms given in §4.3 are likely suﬃciently fast for almost\nany language data. In particular, sentences are generally short from a complexity perspective and\nhence algorithms with asymptotic complexity of O(n1.58) and O(n log n) are unlikely to be of con-\ncerning cost, especially since the methods we study in §5 are one-time costs.\n11In some algorithmic contexts, the problem of returning a linear layout that is constrained to be\nprojective is known as the planar version of a problem, e.g. planar-cutwidth.\n63\n. . . c1 . . .\n|        {z        }\nc1’s subtree\n...\n. . . c2k−1 . . .\n|            {z            }\nc2k−1’s subtree\nh\n. . . c2k . . .\n|         {z         }\nc2k’s subtree\n...\n. . . c2 . . .\n|        {z        }\nc2’s subtree\n.\n.\n.\n.\nFigure 4.4: Illustration of the disjoint strategy. The root h is denoted in bold and it has 2k children\ndenoted by c1, . . . , c2k. Its children and their subtrees are organized on either side. The order within\neach child subtree is speciﬁed by a linear layouts that has previously been computed in the dynamic\nprogram. The order of the children and their subtrees alternates and moving from outside to inside\nbased on their score according to some scoring function. Hence, the subtree rooted at child c1\nreceives the highest score and the subtree roots at child c2k receives the lowest score. If the root h\nhad 2k + 1 (an odd number) of children, the strategy is followed for the ﬁrst 2k and we discuss the\nplacement of the last child subsequently.\nAlgorithm 1: Disjoint Strategy\n1 Input: A tree rooted at h with children c1, . . . , c2k.\n2 Input: Optimal linear layouts π1, . . . , π2k previously computed in the\ndynamic program. πi is the optimal linear layout for the tree rooted at ci.\n3 πh ←{h : 1}\n4 ranked-children ←sort\n\u0000 [1, 2, . . . , 2k] , λx.score (cx)\n\u0001\n5 π ←\n\u0010Lk\ni=1 πranked-children[2i−1]\n\u0011\n⊕πh ⊕\n\u0010Lk−1\ni=0 πranked-children[2(k−i)]\n\u0011\n6 return π\nDynamic Programming for tree-based optimization. Since we are considering\nboth the cutwidth and minLA problems in the setting of trees, dynamic program-\nming approaches are well-motivated. In particular, we will consider how optimal\nlinear layouts for each of the children at a given node can be integrated to yield\nthe optimal linear layout at the given node. As we will show, both algorithms we\nconsider can be thought of as speciﬁc instantiations of the abstract disjoint strategy\nintroduced by Yannakakis (1985).12 In Figure 4.4, we depict what the disjoint strat-\n12Remark: Gildea and Temperley (2007) develop the same general framework in their own work.\nSince both algorithms share a similar template, we prefer the standardized template to their ad hoc\ndescription.\n64\negy looks like and in Algorithm 1, we provide the template for the disjoint strategy\nalgorithm. We denote linear layouts programmatically as dictionaries, hence πh is\nthe function πh : {h} →{1} given by πh(h) = 1. We deﬁne the ⊕operator over\nlinear layouts below.\nDeﬁnition 4.5. ⊕— A binary operator for arbitrary parameters n, m of type ⊕:\nSn × Sm →Sn+m speciﬁed by:\n⊕(πx, πy)(wi) =\n\n\n\n\n\n\n\nπx(wi)\nwi ∈Dom(πx)\nπy(wi) + n\nwi ∈Dom(πy)\n(4.15)\nL is given by the repeated application of ⊕(analogous to the relationship between\n+ and ∑or ∪and S).\nMinimum Linear Arrangement. The function score in Algorithm 1 is deﬁned\nsuch that score(ci) is the size of the subtree rooted at ci.\nCutwidth. The function score in Algorithm 1 is deﬁned such that score(ci) is\nthe modiﬁed cutwidth of the subtree rooted at ci under πi. The modiﬁed cutwidth\nof a tree under π is the cutwidth of the tree under π plus a bit indicating whether\nthat are positions on either side of the root of the tree at which the cutwidth (the\nmaximum edge cut) is obtained. Hence, the modiﬁed cutwidth is the cutwidth\nif all positions at which the edge cut is maximized occur strictly on the left of the\nroot XOR if all positions at which the edge cut is maximized occur strictly on the\nright of the root. Otherwise, the modiﬁed cutwidth is the cutwidth plus one.\n65\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\na\nb\nc\nd\ne\nh\nf\ng\ni\nj\nk\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nFigure 4.5: Linear layouts exemplifying the diﬀerence between the solutions produced by the\nGildea and Temperley (2007) algorithm (top) and our algorithm (bottom). The root h is denoted\nin bold. In both algorithms, the linear layouts for the children with the largest subtrees — the blue\nsubtree rooted at c and the brown subtree rooted at j — are placed on opposite sides. The diﬀerence\nis the placement of the green subtree rooted at child f. The arcs whose edge lengths change across\nthe two layouts are those in cyan, notably (c, h), ( f, h), and (j, h). However, the sum of the edge\nlengths for (c, h) and (j, h) is constant across the linear layouts. Hence, the diﬀerence in minimum\nlinear arrangement scores between the linear layouts is solely dictated by the length of ( f, h), which\nis shorter in our algorithm’s layout (bottom layout).\nProofs of correctness. Intuitively, both proofs of correctness hinge on the fact that\nthe disjoint strategy involves placing ‘larger’ subtrees in an alternating outside-\nto-inside fashion around the current node/root. By selecting the correct measure\nof ‘large’, the ‘adverse’ eﬀects of the large subtrees aﬀect as few other subtrees as\npossible (since they are outside the link from these subtrees to the root, which\nis the only way in which the costs interact across subtrees/with the root). For\nreaders interested in the fully formal proof of correctness, we direct them to the\ncorresponding works: Yannakakis (1985) and Gildea and Temperley (2007).\nCorrecting the algorithm of Gildea and Temperley (2007). We note that one\nsmall correction is made in our algorithm that was not originally addressed by\nGildea and Temperley (2007). In particular, consider the case when the given node\nhas an odd number of children (i.e. 2k + 1 children for some non-negative inte-\n66\nger k). In this case, the 2k largest children and their associated subtrees are alter-\nnated as dictated by the disjoint strategy. Gildea and Temperley (2007) claim that\nthe placement of the ﬁnal child does not matter, however this is incorrect. We\nshow this in Figure 4.5. That said, it is fairly simple to correct this error. The ﬁnal\nchild’s subtree is left-leaning if more of the child’s subtree is oriented to its left than\nright (according to the linear layout already computed), right-leaning if more of the\nchild’s subtree oriented to its right than left, and centered otherwise. If the child’s\nsubtree is leaning in either direction, it should be placed on that side of the root\n(closest to the root relative to the root’s other children, i.e. in accordance with the\ndisjoint strategy). Otherwise, the side it is placed on does not matter.\nThe proof of correctness that this globally improves the outputted linear lay-\nout is fairly straightforward. In particular, since there are k children of the root on\neither side, the objective will be incremented by k ∗the size of the 2k + 1 child’s sub-\ntree in irrespective of this decision (where to place child 2k + 1 and its subtree). All\narcs above the root and within any of the root’s children’s subtrees will not change\nlength. Hence the only arc of interest is the one connecting child 2k + 1 and the\nroot. In this case, clearly placing the root on the side opposite of the way the child’s\nsubtree is leaning will yield a smaller such arc (as depicted in Figure 4.5).\nAs a concluding remark, we note that the error we presented with the algorithm\nof Gildea and Temperley (2007) is exceptionally pervasive. In particular, for every\n67\nchild with an odd number of children in the tree, the algorithm they present may\nhave contributed to a misplacement with probability 1\n2. When evaluated over data\nwe describe in §5.3, we ﬁnd over 95% of the sentences contain such an instance.\nAdditionally, the runtime analysis of Gildea and Temperley (2007) indicates that\ntheir algorithm is O(n). Given the sorting required, this is not naively true but\ncan be rectiﬁed using appropriate data structures and bucket sort as suggested by\nYannakakis (1985).\n4.4\nHeuristics for Mixed-Objective Optimization\nIn the previous section, we considered an algorithm for each of the bandwidth,\nminLA, and cutwidth problems. In all cases, the algorithm had the goal of a pro-\nducing that a linear layout that minimized the corresponding objective to the ex-\ntent possible.13 However, in our setting, we are considering re-ordering the words\nof a sentence for modelling sentences. From this perspective, there is a clear risk\nthat re-ordering the words to optimize some objective regarding dependencies may\nobscure other types of order-dependent information in the original sentence. We\nnext consider a template that speciﬁes a simple heuristic for each of these opti-\nmization problems. In particular, the heuristic involves a parameter T which bears\nsome relationship with the notion of trading oﬀthe extent to which the original\nsentence’s word order is distorted with the extent to which the combinatorial ob-\njective is minimized.\n13Perhaps with additional constraints such as projectivity.\n68\n4.4.1\nTransposition Monte Carlo\nAlgorithm 2: Transposition Monte Carlo\n1 Input: A sentence ¯s and its dependency parse G¯s = (V, E).\n2 Initialize π = πI\n3 Initialize c = OBJ(π, ¯s)\n4 for t ←1, . . . , T do\n5\nwi, wj ∼UV\n6\nπtemp ←π\n7\nπtemp(wi), πtemp(wj) ←πtemp(wj), πtemp(wi)\n8\nctemp ←OBJ(πtemp, ¯s)\n9\nif c > ctemp then\n10\nπ ←πtemp\n11\nc ←ctemp\n12 end\n13 return π\nIn Algorithm 2, we present the template we consider for our three heuristic al-\ngorithms. Intuitively, the algorithm is a Monte Carlo algorithm that at each time\nstep randomly samples a transposition14 and considers altering the current permu-\ntation π according to this transposition. For this reason, we call the algorithm the\nTransposition Monte Carlo algorithm.\nWe refer to this algorithm as a heuristic since, like any Monte Carlo algorithm,\nthere is no provable guarantee that the algorithm produces the optimal solution.\nIn particular, we note that the algorithm greedily decides whether to update in ac-\ncordance with a candidate transposition and hence makes locally optimal decisions.\n14The notation permits the wi = wj but there is no beneﬁt to this, so the notation should be taken\nas sampling wi and then sampling wj without replacement from the resulting distribution.\n69\nConsequently, there is no guarantee that this procedure will lead to the globally op-\ntimal linear layout. However, unlike the algorithms in the section on algorithms for\nproducing linear layouts under projectivity constraints (§4.3.1), the permutation\nproduced by this algorithm need not be projective. We will revisit this in empiri-\ncally considering the quality of the optimization on natural language data (§5.3).\nThe Transposition Monte Carlo algorithm is parameterized by two quan-\ntities: the objective/cost function being minimized OBJ and the stopping crite-\nrion/threshold T.\n1. OBJ — In this work, we specify cost functions OBJ in accordance with those\nwhich we have seen previously — the bandwidth cost in Equation 4.6, the\nminLA cost in Equation 4.8, and the cutwidth cost in Equation 4.11. We will\nrefer to the associated permutations as ˜πb, ˜πm, and ˜πc respectively and will\nsimilarly denote the induced ordering rules as ˜rb, ˜rm, and ˜rc.\n2. T — In this work, we ﬁx T = 1000 which approximately corresponds\nto Transposition Monte Carlo taking the same amount of wall-clock\ntime as it takes to run any of the algorithms in §4.3 on the same data. How-\never, varying T allows for the possibility of ﬂexibility controlling the extent\nto which the returned linear layout is distorted. For T = 0, we recover the\nordering rule rI which returns πI for a given input sentence. For T = ∞, we\nare not guaranteed to ﬁnd a global optima to the author’s knowledge (due to\nthe local greediness of the algorithm), but we are guaranteed to ﬁnd a local\n70\noptima (in the sense that no transposition from the solution yields a better so-\nlution). Treating T as a task-dependent hyperparameter in NLP applications\nand optimizing for it (perhaps on validation data) may encode the extent to\nwhich dependency locality is important for the given task.\n71\nCHAPTER 5\nOPTIMAL LINEAR ORDERS FOR NATURAL LANGUAGE PROCESSING\nIn this chapter, we describe how to integrate the novel word orders we have con-\nsidered with models in natural language processing. We speciﬁcally introduce\nthe pretrain-permute-finetune framework which fully operationalizes this.\nWe then conduct an empirical evaluation of our method using the word orders we\nhave introduced previously.\n5.1\nMotivation\nIn §2.3, we discuss several approaches to addressing order, and word order specif-\nically, in computational models of language.\nThe dominant recent paradigm\nhas been to introduce computational expressiveness (e.g. LSTMs in place of\nsimple RNNs, attention, syntax-augmented neural methods (Socher et al., 2013a;\nBowman et al., 2015; Dyer et al., 2016; Dyer, 2017; Kim et al., 2019)) as a mecha-\nnism for handling the diﬃculties of long-distance dependencies and word order\nmore generally. Alternatively, position-aware Transformers have been considered\nbut their widespread eﬀectiveness hinges on large amounts of data with suﬃcient\nhardware resources to exploit the increased ability for parallelism. These methods\nalso tend to be signiﬁcantly more challenge to optimize in practice (Liu et al., 2020).\nUltimately, it remains unclear whether it is necessary to rely on additional model\ncomplexity or whether restructuring the problem (and the input speciﬁcally) may\n72\nbe more appropriate. To this end, we consider the orders which we have intro-\nduced in §4 as a mechanism for codifying new views to the problem of sequential\nmodelling language. Next, we explore the potential newfound advantages (and\ncosts/limitations) of using these novel word orders in computational models of\nlanguage.\n5.2\nMethods\nWe began by considering training models where, for every sentence in the train-\ning data, we instead use its re-ordered analogue.\nIn initial experiments, we\nfound the results to be surprisingly promising.\nHowever, we found that this\nparadigm may not be particularly interesting given there are very few, if any, tasks\nin NLP that are currently conducted using representations built upon strictly task-\nspeciﬁc data. Instead, almost all NLP models leverage pretrained (trained on large\namounts of unlabelled data in a downstream task-agnostic fashion) representa-\ntions to serve as potent initializations (Ruder, 2019). The past few years have\nseen the introduction of pretrained contextualized representations, which are func-\ntions c : S∞\ni=1 Vi →S∞\ni=1\n\u0000Rd\u0001i, that map word sequences (e.g. sentences) to\nvector sequences of equal length. In particular, the resulting vectors encode the\ncontextual meaning of the input words. Initial pretrained contextualized repre-\nsentations include: CoVe (McCann et al., 2017), ELMo (Peters et al., 2018b), and\n73\nGPT (Radford et al., 2018); the current best1 pretrained contextualized represen-\ntations include: BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet\n(Yang et al., 2019), RoBERTa (Liu et al., 2019).\nSpanBERT (Joshi et al., 2020a),\nELECTRA (Clark et al., 2020), ALBERT (Lan et al., 2020) and T5 (Raﬀel et al.,\n2019).\nGiven the widespread use of transfer learning and pretraining methods in NLP\n(Ruder, 2019), we begin by describe the pretrain-and-finetune framework\nthat has featured prominently across NLP. In particular, the mapping from an in-\nput sentence ¯s to the predicted output ˆy is given by the following process:\n1. Tokenize the input sentence ¯s into words ⟨w1 . . . wn⟩.\n2. Embed the sentence ⟨w1 . . . wn⟩using the pretrained encoder/contextualized\nmodel c to yield vectors ⃗x1, . . . ,⃗xn where ∀i ∈[n] ,⃗xi ∈Rd.\n3. Pass ⃗x1, . . . ,⃗xn into a randomly initialized component F that is trained on the\ntask of interest that outputs the prediction ˆy.\nIn order to modify this process to integrate our novel word orders, one straight-\nforward approach would be simply feeding the permuted sequence of inputs into\nthe pretrained encoder. Perhaps unsurprisingly, we found this to perform quite\n1It is not suﬃciently speciﬁed to rank pretrained representations in a task-agnostic fashion\ngiven that they may have disparate and inconsistent performance across diﬀerent downstream\ntasks/datasets.\nIn labelling some of these representations as the ‘best’, we refer to the GLUE\n(Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) benchmarks for natural language un-\nderstanding and similar benchmarks for natural language generation (c.f Raﬀel et al., 2019).\n74\npoorly in initial experiments. After all, the pretrained encoder never observed\nsuch permuted constructions during training and these inputs are eﬀectively out-\nof-distribution for the model. Another natural approach that is more reasonable\nwould be to re-pretrain the encoder using the same training data but with all\nsentences permuted according to the order being studied. Unfortunately, this is\nwell beyond our computational constraints and likely is not practical for almost all\nresearch groups given the tremendous time, cost, and unique GPU and TPU re-\nsources required to pretrain modern models. Even for such entities who can bear\nthese expenses, this is unlikely to be feasible if multiple orders are to be exper-\nimented with and there are substantial ethical considerations given the sizeable\nenvironmental impact (Strubell et al., 2019).\nGiven these roadblocks, we propose permuting the vectors ⃗x1, . . . ,⃗xn in be-\ntween steps 2 and 3. In doing so, we seamlessly integrate our permuted orders\nin a model-agnostic and task-agnostic fashion while preserving the beneﬁts of pre-\ntraining. Further, since the permutation for a given example can be pre-computed\n(and is a one-time cost), the additional runtime cost of our method during both\ntraining and inference is simply the cost of computing the optimal orders over the\ndataset.2 We name our framework pretrain-permute-finetune and we ex-\n2As a reference, for the 5 datasets we study (and over 40000 examples), optimization across all\nsix orders takes less than 3 hours on a single CPU. Several aspects of this are embarrassingly par-\nallel and others can be more cleverly designed using vector/matrix operations in place of dynamic\nprogramming and for loops. This suggests that highly parallel GPU implementations can render\nthis runtime cost to be near-zero. Regardless, it is already dramatically dwarfed by the costs of\ntraining.\n75\nplicitly state the procedure below.\n1. Tokenize the input sentence ¯s into words ⟨w1 . . . wn⟩.\n2. Embed the sentence ⟨w1 . . . wn⟩using the pretrained encoder/contextualized\nmodel c to yield vectors ⃗x1, . . . ,⃗xn where ∀i ∈[n] ,⃗xi ∈Rd.\n3. Permute the vectors ⃗x1, . . . ,⃗xn according to linear layout π. In other words,\n∀i ∈[n] ,⃗zπ(wi) ≜⃗xi.\n4. Pass⃗z1, . . . ,⃗zn into a randomly initialized component F that is trained on the\ntask of interest that outputs the prediction ˆy.\nPretrained Contextualized Representations. In this thesis, we use a pretrained\nELMo (Peters et al., 2018b) encoder to specify c. ELMo is a pretrained shallowly-\nbidirectional language model that was pretrained on 30 million sentences, or\nroughly one billion words, using the 1B Word Benchmark (Chelba et al., 2013).\nThe input is ﬁrst tokenized and then each word is deconstructed into its corre-\nsponding character sequences. Each character is embedded independently and\nthen a convolutional neural network is used to encode the sequence and produce\nword representations. The resulting word representations are passed through a\ntwo-layer left-to-right LSTM and a two-layer right-to-left LSTM. Two represen-\ntations are produced for every word wi. The ﬁrst is ELMo1 (wi | ⟨w1 . . . wn⟩) ∈\nRd, which is the concatenated hidden states from the ﬁrst layer of both LSTMs.\nSimilarly, the second is ELMo2 (wi | ⟨w1 . . . wn⟩) ∈\nRd, which is the concate-\nnated hidden states from the second layer of both LSTMs. In our work, ⃗xi ≜\n76\n[ELMo1 (wi | ⟨w1 . . . wn⟩) ; ELMo2 (wi | ⟨w1 . . . wn⟩)] ∈R2d.3\nTask-speciﬁc Component. We decompose the task-speciﬁc model F into a bidirec-\ntional LSTM, a max pooling layer, and a linear classiﬁer:\n←−\nh1:n, −→\nh1:n ←BidiLSTM(⃗z1, . . . ,⃗zn)\n(5.1)\n⃗h ←\nh\nmax\n\u0010←−\nh1:n\n\u0011\n; max\n\u0010−→\nh1:n\n\u0011i\n(5.2)\nˆy ←Softmax\n\u0010\nW⃗h +⃗b\n\u0011\n(5.3)\nwhere BidiLSTM, W,⃗b are all learnable parameters and max(·) is the element-wise\nmax operation.\n5.3\nData\nIn this work, we evaluate our methods and baselines on ﬁve single-sentence text\nclassiﬁcation datasets. Summary statistics regarding the distributional properties\nof these datasets are reported in Table 5.1. We use oﬃcial dataset splits4 when\navailable and otherwise split the dataset randomly into 80% training data, 10% val-\nidation data, and 10% test data.5 We also report the fraction of examples that the\nspaCy parser generates an invalid parse. For examples with invalid parse, since\n3; denotes concatenation\n4For datasets where the standard split is only into two partitions, we further partition the train-\ning set into 8\n9 training data and 1\n9 validation data.\n5When we create data splits, we elect to ensure that the distribution over labels in each dataset\npartition is equal across partitions.\n77\nTrain\nValidation\nTest\nWords\nex.\nUnique Words\nClasses\nFail %\nCR\n3016\n377\n378\n20\n5098\n2\n19.2\nSUBJ\n8000\n1000\n1000\n25\n20088\n2\n13.6\nSST-2\n6151\n768\n1821\n19\n13959\n2\n6.7\nSST-5\n7594\n949\n2210\n19\n15476\n5\n6.8\nTREC\n4846\n605\n500\n10\n9342\n6\n1.0\nTable 5.1: Summary statistics for text classiﬁcation datasets. Train, validation, and test refer to the\nnumber of examples in the corresponding dataset partition. Words\nex.\nrefers to the average number of\nwords in the input sentence for each example in the union of the training data and the validation\ndata. Unique words is the number of unique words in the union of the training data and the valida-\ntion data. Classes is the size of the label space for the task. Fail % is the percentage of examples in\nthe union of the training and validation set where the spaCy dependency parser emits an invalid\nparse (e.g multiple syntactic roots, not a connected graph).\nwe cannot meaningfully compute optimal linear layouts, we back-oﬀto using the\nidentity linear layout πI. All data is publicly available and means for accessing the\ndata are described in §A.3.\nCustomer Reviews Sentiment Analysis.\nThis dataset was introduced by\nHu and Liu (2004) as a collection of web-scraped customer reviews of products.\nThe task is to predict whether a given review is positive or negative. This dataset\nwill be abbreviated CR hereafter.\nMovie Reviews Subjectivity Analysis This dataset was introduced by Pang and Lee\n(2004) as a collection of movie-related texts from rottentomatoes.com and\nimdb.com. The task is to predict whether a given sentence is subjective or ob-\n78\njective. Subjective examples were sentences extracted from movie reviews from\nrottentomatoes.com and objectives examples were sentences extracted from\nmovie plot summaries from imdb.com. This dataset will be abbreviated SUBJ\nhereafter.\nMovie Reviews Sentiment Analysis This dataset was introduced by Socher et al.\n(2013b) as the Stanford Sentiment Treebank.\nThe dataset extends the prior\ndataset of Pang and Lee (2005),\nwhich is a set of movie reviews from\nrottentomatoes.com, by re-annotating them using Amazon Mechanical Turk.\nIn the binary setting, the task is to predict whether the review is positive or nega-\ntive. In the ﬁne-grained or ﬁve-way setting, the task is to predict whether the re-\nview is negative, somewhat negative, neutral, somewhat positive, or positive. This\ndataset will be abbreviated as SST-2 to refer to the binary setting and as SST-2\nto refer to the ﬁve-way setting hereafter.\nQuestion Classiﬁcation This dataset was introduced by Li and Roth (2002) as\na collection of data for question classiﬁcation and was an aggregation of data\nfrom Hovy et al. (2001) and the TREC 8 (Voorhees and Harman, 2000a), 9\n(Voorhees and Harman, 2000b), and 10 (Voorhees and Harman, 2001) evalua-\ntions. The task is to predict the semantic class6 of a question from the following cat-\n6The notion of a semantic class for questions follows Singhal et al. (2000) and is distinct from the\nconceptual classes of Lehnert (1977a,b, 1986). The distinction primarily centers on the handling of\nfactual questions (c.f. Li and Roth, 2002).\n79\negories: abbreviation, entity, description, human, location, and numerical value.7\nThis dataset will be abbreviated TREC hereafter.\nWhy single-sentence text classiﬁcation? Text classiﬁcation is a standard and sim-\nple test-bed for validating that new methods in NLP have potential. It is also one of\nthe least computationally demanding settings, which allowed us to be more com-\nprehensive and rigorous in considering more datasets, ordering rules, and hyper-\nparameter settings. In this work, we choose to further restrict ourselves to single-\nsentence datasets as the optimization we do is ill-posed. Consequently, without\nfurther constraints/structure, the solutions our algorithms generate may lack ﬂu-\nency across sentence boundaries. In the single-sentence setting, models must learn\nto grapple with this lack of systematicity in modelling across examples but the task\nis signiﬁcantly simpler as there are no modelling challenges within a single exam-\nple due to this lack of systematicity. As we were mainly interested in what could\nbe done with relatively pure algorithmic optimization, we began with this simpler\nsetting with fewer confounds. We discuss this as both a limitation and opportunity\nfor future work in §6.\n7The data was annotated a course-grained granularity of six classes and a ﬁne-grained granu-\nlarity of 50 classes. In this work, we use the coarse-grained granularity.\n80\n5.4\nExperimental Conditions\nIn this work, we tokenize input sentences using spaCy (Honnibal and Montani,\n2017). We additionally dependency parse sentences using the spaCy dependency\nparser, which uses the CLEAR dependency formalism/annotation schema.8 We\nthen embed tokens using ELMo pretrained representations. We then freeze these\nrepresentations, permute them according to the order we are studying, and then\npass them through our bidirectional LSTM encoder to ﬁne-tune for the speciﬁc\ntask we are considering. We use a linear classiﬁer with a Softmax function on top\nof the concatenated max-pooled hidden states produced by the LSTM.\nWhy spaCy? Using spaCy for both dependency parsing and tokenizing ensured\nthat there were no complications due to misalignment between the dependency\nparser’s training data’s tokenization and our tokenization. Further, the spaCy de-\npendencer parser is reasonably high-quality for English and is easily used oﬀ-the-\nshelf.\nWhy ELMo? ELMo representations are high-quality pretrained representations.\nWhile superior pretrained representations exist (e.g. BERT), we chose to use ELMo\nas it is was compatible with the tokenization schema of spaCy and introduced\nno further complications due to subwords. While in other works of ours, we rig-\n8https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency\n81\norously mechanisms for converting from subword-level to word-level representa-\ntions for BERT and other transformers (Bommasani et al., 2020), we chose to avoid\nthis complication in this work as it was not of interest and was merely a confound.\nWhy frozen representations? Peters et al. (2019b) provides compelling and com-\nprehensive evidence the frozen representations perform better than ﬁne-tuned rep-\nresentations when using ELMo as the pretrained model. Further, in this work we\nare interested in integrating our novel word orders with pretrained representations\nand wanted to isolate this eﬀect from possible further confounds due to the nature\nof ﬁne-tuning with a diﬀerent word order from the pretraining word order.\nWhy bidirectional LSTM encoder?\nAs we discuss in §2.3.2, LSTMs have proven to be reliably and performant en-\ncoders across a variety of NLP tasks.\nFurther, bidirectional LSTMs have al-\nmost uniformly led to further improvemenets, as we discuss in §2.3.4. In our\npretrain-permute-finetune framework, it was necessary to use a task-\nspeciﬁc encoder that was not order-agnostic as otherwise the permutations would\nhave no eﬀect.\nWhy max-pooling? In initial experiments, we found that the pooling decision\nhad an unclear impact on results but that there was marginal evidence to sug-\ngest that max-pooling outperformed averaging (which is the only other pooling\nchoice we considered; we did not consider the concatenation of the two as in\n82\nHoward and Ruder (2018)). Further, recent work that speciﬁcally studies ELMo\nfor text classiﬁcation with extensive hyperparameter study also uses max-pooling\n(Peters et al., 2019b). Separately, Zhelezniak et al. (2019) demonstrate that max-\npooling may have both practical advantages and theoretical justiﬁcation.\n5.4.1\nHyperparameters\nWe use input representations from ELMo that are 2048 dimensional.\nWe use\na single layer bidirectional LSTM with output dimensionality h and dropout\n(Srivastava et al., 2014) is introduced to the classiﬁer input with dropout probabil-\nity p as form of regularization. The weights of the LSTM and classiﬁer are initial-\nized according to random samples from PyTorch default distribution.9 Optimiza-\ntion is done using the Adam optimizer (Kingma and Ba, 2015) and the standard\ncross-entropy loss, which has proven to be a robust pairing of (optimizer, objective)\nin numerous NLP applications. Optimizer parameters are set using PyTorch de-\nfaults.10 Examples are presented in minibatches of size 16. The stopping condition\nfor training is the model after training for 12 epochs. We found this threshold after\nlooking at performance across several diﬀerent epochs (those in {3, 6, 9, 12, 15}).\nWe found that standard early-stopping methods did not reliably lead to improve-\nments, perhaps due to the fact that models converge so quickly (hence early-\nstopping and a ﬁxed threshold are near-equal). All hyperparameters that were\n9https://pytorch.org/docs/stable/nn.init.html\n10https://pytorch.org/docs/stable/optim.html\n83\nspeciﬁed above were optimized on the SUBJ dataset using the identity word order\n(rI) to ensure this baseline was as well-optimized as possible. Parameter choices\nfor h and p were initially optimized over {16, 32, 64, 128, 256} × {0.0, 0.02, 0.2} for\nthe SUBJ (one of the easiest datasets) and SST-5 (one of the hardest datasets),\nagain using rI to ensure the optimization was done to favor the baseline. From\nthese 15 candidate hyperparameter settings, the six highest performing were cho-\nsen11 and all results are provided for these settings (for all word orders and all\ndatasets) in Appendix B. All results reported subsequently are for hyperparam-\neters individually optimized for the (word order, dataset) pair being considered.\nAll further experimental details are deferred to §A.1.\n5.5\nResults and Analysis\nOrders. We analyze the results for the following eight orders:\n1. rI — Each sentence is ordered as written.\n2. rr — Each sentence is ordered according to a linear layout sampled from the\nuniform distribution over Sn.\n3. rb — Each sentence is ordered according to the Cuthill-McKee heuristic to\nminimize bandwidth.\n11We did this initial reduction of the hyperparameter space due to computational constraints. In\ndoing so, we tried to ensure that the settings yielded the strongest possible baselines.\n84\n4. rc — Each sentence is ordered according to the algorithm of Yannakakis\n(1985) to minimize cutwidth. The linear layout is constrained to be optimal\namong all projective orderings.\n5. rm — Each sentence is ordered according to the algorithm of Gildea and Temperley\n(2007) to minimize the minimum linear arrangement objective. The linear\nlayout is constrained to be optimal among all projective orderings.\n6. r˜b — Each sentence is ordered according the Transposition Monte\nCarlo algorithm to minimize bandwidth.\n7. r˜c — Each sentence is ordered according the Transposition Monte\nCarlo algorithm to minimize cutwidth.\n8. r ˜m — Each sentence is ordered according the Transposition Monte\nCarlo algorithm to minimize the minimum linear arrangement objective.\nOptimization eﬀects. Beyond the standard distributional properties of interest\nin NLP for datasets (Table 5.1), it is particularly pertinent to consider the eﬀects\nof our optimization algorithms on the bandwidth, minimum linear arrangement\nscore, and cutwidth across these datasets. We report this in Table 5.2.\nWe begin by considering the relationship between the random word orders rr\nand the standard English word orders rI (top band of Table 5.2). In particular, we\nobserve that across all ﬁve datasets, standard English substantially optimizes these\nthree costs better than a randomly chosen ordering would. In the case of minimum\n85\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nB\nC\nM\nB\nC\nM\nB\nC\nM\nB\nC\nM\nB\nC\nM\nrr\n16.03\n10.07\n146.9\n20.42\n13.20\n221.9\n15.92\n10.92\n153.1\n15.84\n10.90\n151.6\n7.55\n6.05\n39.24\nrI\n11.52\n4.86\n49.87\n16.12\n5.49\n69.52\n13.16\n5.03\n54.29\n13.04\n5.02\n53.84\n6.87\n3.95\n21.99\nrb\n5.44\n5.44\n55.21\n6.37\n6.37\n78.94\n5.52\n5.52\n58.20\n5.50\n5.50\n57.68\n3.36\n3.36\n17.76\nrc\n6.58\n3.34\n34.68\n8.41\n3.62\n46.78\n6.54\n3.20\n35.69\n6.51\n3.20\n35.43\n3.57\n2.45\n14.76\nrm\n6.19\n3.35\n34.13\n7.69\n3.64\n45.70\n6.00\n3.21\n34.90\n5.98\n3.21\n34.65\n3.46\n2.45\n14.62\n˜rb\n6.64\n5.29\n55.84\n8.68\n6.40\n81.12\n7.11\n5.61\n61.74\n7.08\n5.57\n60.97\n3.84\n3.75\n21.88\n˜rc\n10.42\n4.02\n47.00\n14.60\n4.57\n66.03\n11.66\n4.08\n50.89\n6.96\n4.07\n50.44\n3.79\n3.00\n19.66\n˜rm\n6.85\n3.29\n35.68\n8.60\n3.66\n49.17\n7.00\n3.29\n37.64\n11.57\n3.29\n37.32\n5.77\n2.54\n15.40\nTable 5.2: Bandwidth (B), cutwidth (C), and minimum linear arrangement (M) scores for every\n(dataset, ordering rule) pair considered.\nlinear arrangement, this provides further evidence to corpus analyses conducted\nby Futrell et al. (2015). Similarly, for the bandwidth and cutwidth objectives, this\nsuggests that these objectives at least correlate with costs that humans may opti-\nmize for in sentence production and processing.\nWe then consider the optimization using existing algorithms in the literature\nas compared to standard English and random word orders (top and middle bands\nof Table 5.2). We observe that across all datasets, all optimized ordering rules per-\nform random word orders across all objectives. While it is unsurprising that the\noptimal order for a given objective outperforms the other orders and standard En-\nglish, we do note that the margins are quite substantial in comparing standard\nEnglish to each rule. That is to say, standard English can still be substantially fur-\nther optimized for any of the given orders. Additionally, we consider the scores\nassociated for an ordering rule that are not being optimized for. In particular, we\nsee that optimizing for either cutwidth or minimum linear arrangement yields\n86\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nB\nC\nM\nB\nC\nM\nB\nC\nM\nB\nC\nM\nB\nC\nM\nrr\n16.03\n10.07\n146.9\n20.42\n13.20\n221.9\n15.92\n10.92\n153.1\n15.84\n10.90\n151.6\n7.55\n6.05\n39.24\nrI\n11.52\n4.86\n49.87\n16.12\n5.49\n69.52\n13.16\n5.03\n54.29\n13.04\n5.02\n53.84\n6.87\n3.95\n21.99\nrb\n5.44\n5.44\n55.21\n6.37\n6.37\n78.94\n5.52\n5.52\n58.20\n5.50\n5.50\n57.68\n3.36\n3.36\n17.76\nrc\n6.58\n3.34\n34.68\n8.41\n3.62\n46.78\n6.54\n3.20\n35.69\n6.51\n3.20\n35.43\n3.57\n2.45\n14.76\nrm\n6.19\n3.35\n34.13\n7.69\n3.64\n45.70\n6.00\n3.21\n34.90\n5.98\n3.21\n34.65\n3.46\n2.45\n14.62\n˜rb\n6.64\n5.29\n55.84\n8.68\n6.40\n81.12\n7.11\n5.61\n61.74\n7.08\n5.57\n60.97\n3.84\n3.75\n21.88\n˜rc\n10.42\n4.02\n47.00\n14.60\n4.57\n66.03\n11.66\n4.08\n50.89\n6.96\n4.07\n50.44\n3.79\n3.00\n19.66\n˜rm\n6.85\n3.29\n35.68\n8.60\n3.66\n49.17\n7.00\n3.29\n37.64\n11.57\n3.29\n37.32\n5.77\n2.54\n15.40\nTable 5.3: Duplicated from Table 5.2 for convenience. Bandwidth (B), cutwidth (C), and minimum\nlinear arrangement (M) scores for every (dataset, ordering rule) pair considered.\nsimilar scores across all three objectives and all ﬁve datasets. We hypothesize\nthis is due to both algorithms have a shared algorithmic subroutine (the disjoint\nstrategy). Optimizing for either order yields substantial improvements over stan-\ndard English across all three objectives and only marginally underperforms the\nbandwidth-optimal order rb. On the other hand, we ﬁnd an interesting empiri-\ncal property that the cutwidth and bandwidth scores for rb are consistently equal.\nThis may suggest that this is a theoretical property of the algorithm that be formally\nproven. Further, rb generally (except for TREC) yields greater minimum linear ar-\nrangements compared to English.\nNext, we consider the optimization using the algorithms we introduce with\nthe Transposition Monte Carlo method as compared to English and ran-\ndom word orders (top and bottom bands of Table 5.2). We observe that the same\ncomparison between random word orders and the word orders we introduce to\noptimize objectives holds as in the case of the algorithms from the prior literature.\n87\nFurther, the relationship between standard English and these heuristic-based word\norders mimics the trends between standard English and the word orders derived\nfrom prior algorithms. However, we ﬁnd that the margins are substantially smaller.\nThis is not particularly surprising, as it suggests that our heuristics are less eﬀective\nat pure optimization than the more well-established algorithms in the literature.\nWhen we strictly consider the word orders generated by the heuristics we in-\ntroduce (bottom band of Table 5.2), we observe one striking ﬁnding immediately.\nIn particular, the cutwidth objective evaluated on the cutwidth-minimizing or-\nder ˜rc is often greater than the same objective evaluated on the minimum lin-\near arrangement-minimizing order ˜rm.\nA similar result holds between ˜rb and\n˜rm for the SUBJ and SST-2 datasets. What this implies is that the greediness\nand transposition-based nature of Transposition Monte Carlo may more di-\nrectly favor optimizing minimum linear arrangement (and that this coincides with\nminimizing the other objectives). Alternatively, this may suggest that a more prin-\ncipled and nuanced procedure is needed for the stopping parameter T in the algo-\nrithm.\nFinally, we discuss the relationship between the algorithms given in prior work\n(and the corresponding word orders) and the algorithms we present under the\nTransposition Monte Carlo framework (and the corresponding word or-\nders). Here, we compare the middle and bottom bands of Table 5.2. As observed\n88\npreviously, we see that for the score being optimized, the corresponding word or-\nder based on an algorithm from the literature outperforms the corresponding word\norder based on an algorithm we introduce. More broadly, we see the quality of the\noptimization across almost all objectives and datasets for all ordering rule pairs\nrk, ˜rk for k ∈{b, c, m} is better for rk than ˜rk. This is consistent with our intuition\npreviously — our heuristics sacriﬁce the extent to which they purely optimize the\nobjective being considered to retain aspects of the original linear layout πI. In\nthe analysis of downstream results, we will address whether this compromise pro-\nvides any beneﬁts in modelling natural language computationally.\nDownstream Performance. Given the framing and backdrop we have developed\nthus far, we next consider the downstream eﬀects of our novel word orders. In\nTable 5.4, we report these results as well as the results for the random word order\nbaseline rr and the standard English word order rI. In particular, recall that the\nrI performance is reﬂective of the performance of the state-of-the-art paradigm in\ngeneral: pretrain-and-finetune.\nEach entry reﬂects the optimal choice of hyperparameters (among those we\nconsidered) for the corresponding model on the corresponding dataset (based on\nvalidation set performance). In Appendix B, we provide additional results for all\nhyperparameter settings we studied12 as well as results for all hyperparameters\nwith the stopping condition of training for 15 epochs (we observed no sustained\n12These results appear in Tables B.1–B.6.\n89\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.852\n0.955\n0.896\n0.485\n0.962\nrr\n0.842\n0.95\n0.877\n0.476\n0.954\nrb\n0.854\n0.952\n0.873\n0.481\n0.966\nrc\n0.86\n0.953\n0.874\n0.481\n0.958\nrm\n0.841\n0.951\n0.874\n0.482\n0.962\n˜rb\n0.852\n0.949\n0.882\n0.478\n0.956\n˜rc\n0.849\n0.956\n0.875\n0.494\n0.968\n˜rm\n0.844\n0.958\n0.876\n0.476\n0.962\nTable 5.4: Full classiﬁcation results where the result reported is the max across hyperparameter set-\ntings. Results use pretrain-permute-finetune framework with the order speciﬁed in each\nrow. All other hyperparameters are set as described previously. The top part of the table refers to\nbaselines. The middle part of the table refers to orders derived from pure optimization algorithms.\nThe bottom part of the table refers to orders derived from heuristic algorithms we introduce us-\ning Transposition Monte Carlo. The best performing ordering rule for a given dataset is\nindicated in bold. Any ordering rule (that is neither the best-performing order rule nor rI) that\nperforms at least as well as rI for a given dataset is indicated in italicized magenta.\nimprovements for any model using any order on any dataset beyond this thresh-\nold).13\nWe begin by considering the comparison between the random word order rr\nand the standard English word order rI (top band of Table 5.4). Note that for rr,\nwe are using a bidirectional LSTM in the task-speciﬁc modelling as a type of set en-\ncoder. While previous works such as Bommasani et al. (2019) have also used RNN\n13These results appear in Tables B.7–B.12.\n90\nvariants in order-invariant settings, this is fairly nonstandard and requires that the\nmodel learns permutation equivariance given that the order bears no information.\nUnsurprisingly, we see that rI outperforms rr across all datasets. However, the mar-\ngin is fairly small for all ﬁve datasets. From this, we can begin by noting that ELMo\nalready is a powerful pretrained encoder and much of the task-speciﬁc modelling\ncould have just been achieved by a shallow classiﬁer on top of the ELMo represen-\ntations. Further, we note that this attunes us to a margin that is fairly signiﬁcant\n(the diﬀerence between whatever can be gained from the English word order and\nthe pretrained contextualized word representations versus what can be gleaned\nonly from the pretrained contextualized word representations).\nNext, we consider the comparison of the word orders derived via combinato-\nrial optimization algorithms in the literature and the baselines of standard English\nand the randomized word order (top and middle bands of Table 5.4). We begin by\nnoting that the optimized word orders do not always outperform rr. This is most\nsalient in looking at the results for SST-2. While the margin is exceedingly small\nwhen the optimized word orders underperform against rr, this suggests that the\nlack of systematicity or regularity in the input (perhaps due to the optimization be-\ning underdetermined) makes learning from the word order diﬃcult. While none\nof rb, rc, rm consistently perform as well or better than rI (rb performs slightly better\non two datasets, slightly worse on two datasets, and substantially worse on SST-2\nas, arguably, the best-performing of the three), there are multiple instances where,\nfor speciﬁc datasets (CR and TREC), they do outperform rI. This alone may sug-\n91\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.852\n0.955\n0.896\n0.485\n0.962\nrr\n0.842\n0.95\n0.877\n0.476\n0.954\nrb\n0.854\n0.952\n0.873\n0.481\n0.966\nrc\n0.86\n0.953\n0.874\n0.481\n0.958\nrm\n0.841\n0.951\n0.874\n0.482\n0.962\n˜rb\n0.852\n0.949\n0.882\n0.478\n0.956\n˜rc\n0.849\n0.956\n0.875\n0.494\n0.968\n˜rm\n0.844\n0.958\n0.876\n0.476\n0.962\nTable 5.5: Duplicated from Table 5.4 for convenience. Full classiﬁcation results where the result\nreported is the max across hyperparameter settings. Results use pretrain-permute-finetune\nframework with the order speciﬁed in each row. All other hyperparameters are set as described\npreviously. The top part of the table refers to baselines. The middle part of the table refers to\norders derived from pure optimization algorithms. The bottom part of the table refers to orders\nderived from heuristic algorithms we introduce using Transposition Monte Carlo. The best\nperforming ordering rule for a given dataset is indicated in bold. Any ordering rule (that is neither\nthe best-performing order rule nor rI) that performs at least as well as rI for a given dataset is\nindicated in italicized magenta.\ngest that a more careful design of ordering rules could yield improvements. Again,\nrecall this is a highly controlled comparison with something that is a reasonable\nstand-in for the state-of-the-art regime of pretrain-and-finetune.\nLooking at the comparison between the word orders derived via algorithms\nfrom the literature and their analogues derived from Transposition Monte\nCarlo variants (middle and bottom bands of Table 5.5), we observe that neither\n92\nset of orders strictly dominates the other. However, on four of the ﬁve datasets (all\nbut CR), the best-performing order is one produced by Transposition Monte\nCarlo. This provides evidence to the claim that the sole goal in constructing or-\nder rules for downstream NLP of pure combinatorial optimization is insuﬃcient\nand arguably naive. Instead, attempting a balance between retaining informa-\ntion encoded in the original linear layout πI while performing some reduction of\ndependency-related costs may be beneﬁcial. We revisit this in §6.3.\nFinally, we compare the word orders produced using our heuristic to the base-\nlines (top and bottom bands of Table 5.5). We begin by observing that in this case,\nalmost every ordering rule outperforms the random word order (the sole excep-\ntion being ˜rb for SUBJ by a margin of just 0.001). Further, while no single ordering\nrule reliably outperforms rI (˜rc signiﬁcantly outperforms rI on two datasets, is sig-\nniﬁcantly outperformed on one dataset, performs marginally better on one dataset,\nand performs marginally worse on dataset), we do ﬁnd a word order that outper-\nforms rI among ˜rb, ˜rc, ˜rm in four of the ﬁve datasets (the exception being SST-2). In\nfact, the majority (i.e. on three of the ﬁve datasets) of the best-performing ordering\nrules of all considered are word orders produced using Transposition Monte\nCarlo. This is additional evidence to suggest the merits of novel ordering rules\n(that are not standard English) for downstream NLP and that their design likely\nshould try to maintain some of the information-encoding properties of standard\nEnglish.\n93\nCHAPTER 6\nCONCLUSIONS\nTo conclude, in this chapter, we review the ﬁndings presented. We further pro-\nvide the open problems we ﬁnd most pressing, the future directions we ﬁnd most\ninteresting, and the inherent limitations we ﬁnd most important.\n6.1\nSummary\nIn this thesis, we have presented a discussion of the current understanding of word\norder as it pertains to human language. Dually, we provide a description of the set\nof approaches that have been taken towards modelling word order in computa-\ntional models of language.\nWe then focus on how psycholinguistic approaches to word order can be general-\nized and formalized under a rich algorithmic framework. In particular, we concen-\ntrate on word orders that can be understood in terms of linear layout optimization\nusing dependency parses as a scaﬀold that speciﬁes the underlying graph struc-\nture. We describe existing algorithms for the bandwidth, minLA, and cutwidth\nproblems with a special interest on algorithms when projectivity constraints are\nimposed. In doing so, we show that the algorithms of Gildea and Temperley (2007)\nand Yannakakis (1985) can be interpreted on a shared framework. Further, we cor-\n94\nrect some nuances in the algorithm and analysis of Gildea and Temperley (2007).\nAs a taste of how algorithmic interpretation could be used to reinterpret exist-\ning psycholinguistic approaches, we also prove Theorem 4.1. This result estab-\nlishes a connection between capacity and memory constraints that was previously\nunknown in the psycholinguistics literature (to the author’s knowledge). To ac-\ncompany these provable results, we also introduce simple heuristic algorithms\n(i.e. Transposition Monte Carlo algorithms for each of the three objectives\nwe considered). These algorithms provide a mechanism for balancing pure algo-\nrithmic optimization with the additional constraints that might be of interest in\nlanguage (e.g preserving other types of information encoded in word order).\nWith these disparate foundations established, the last portion of this thesis dis-\ncusses the relationship between novel/alternative word orders and computational\nmodels of language. We introduce the pretrain-permute-finetune frame-\nwork to seamlessly integrate our novel orders into the de facto pretrain-and-finetune\nparadigm.\nIn particular, our approach incurs a near-trivial one-time cost and\nmakes no further changes to the pretraining process, training and inference costs\n(both with respect to time and memory), or model architecture. We then exam-\nine the empirical merits of our method on several tasks (and across several hyper-\nparameter settings), showing that our heuristic algorithms may sometimes out-\nperform their provable brethren and that, more broadly, novel word orders can\noutperform using standard English. In doing so, we establish the grounds for con-\nsidering the utility of this approach for other languages, for other tasks, and for\n95\nother models.\n6.2\nOpen Problems\nWeighted optimization. In this work, we consider orders speciﬁed based on objec-\ntives evaluated over the dependency parse G = (V, E). In representing the parse in\nthis way, we ignore the labels and directionality of the edges.1 While using a bidi-\nrectional encoder in the ﬁnetuning step of the pretrain-permute-finetune\nframework may alleviate concerns with directionality, we are ultimately neglect-\ning information made available in the dependency parse. We take this step in our\napproach since there are known algorithms for the unweighted and undirected\nversion of the combinatorial optimization problems we study. Nonetheless, study-\ning provable and heuristic algorithms for the weighted version of these problems\nmay be of theoretical interest and of empirical value. In particular, this would al-\nlow one to specify a weighting on edges dependent on their dependency relation\ntypes, which may better represent how certain dependencies are more or less im-\nportant for downstream modelling.\nAlternative scaﬀolds. In this work, we use dependency parses as the syntactic for-\nmalism for guiding the optimization. While this choice was well-motivated given\nthe longstanding literature on dependency locality eﬀects and that the linear tree\n1Recall how we deﬁned E as the unlabelled and undirected version of the true edge set of the\ndependency parse, Eℓ.\n96\nstructure of dependency parses facilitates algorithmic framing in the language of\nlinear layouts, alternative syntactic structures such as constituency trees or seman-\ntic formalisms such as AMR semantic parses are also interesting to consider. This\nyields natural open questions of what appropriate scaﬀolds are, what resulting ob-\njectives and algorithmic approaches are appropriate, and how both the linguistic\nand algorithmic primitive interact with the downstream NLP task. Further inquiry\nmight also consider the merits of combining multiple approaches given the ben-\neﬁts of joint and multi-task learning and fundamental questions regarding what\none linguistic representation may provide that is unavailable from others.\nInformation in Order. A central scientiﬁc question in studying human language\nis understanding what information is encoded in word order and how this infor-\nmation is organized. In this work, given that we argue for the merits of repre-\nsenting sentences using two distinct orders (one during the embedding stage and\none during the ﬁne-tuning stage), this question is of added value. In particular, a\nformal theory of what information is available with one order and another (from\nan information theoretic perspective) might be of value. Arguable an even more\ninteresting question revolves around considering the ease of extracting the infor-\nmation given one ordering or another. Such a framing of orders facilitating the ease\nof extraction of information (from computational agents with bounded capacities,\nwhich is quite diﬀerent from standard information theory) might thereafter induce\na natural optimization-based approach to studying order — select the order that\nmaximizes the eases of extraction of certain information that is of interest.\n97\n6.3\nFuture Directions\nIn the previous section, we state open problems that are of interest and merit study.\nIn this section, we provide more concrete future directions and initial perspectives\non how to operationalize these directions.\nEnd-to-end permutation learning. In this work, we generate novel word orders\nthat are task-agnostic and that optimizes objectives that are not directly related\nwith downstream performance.\nWhile there are fundamental questions about\nthe relationships between discrete algorithmic ideas, classical combinatorial op-\ntimization, and modern deep learning, our approach runs stylistically counter to\nthe dominant paradigms of NLP at present. In particular, the notion of attention\nemerged in machine translation as a soft way of aligning source and target and\nhas proven to empirically more eﬀective than harder alignments. Dually, such\nend-to-end optimization of attention weights implies that attention can be opti-\nmized in a task-speciﬁc way, which likely implies better performance. For these\nreason, studying end-to-end methods for (diﬀerentiably) ﬁnding orders may be\nparticularly attractive as this would imply the permutation component could be\ndropped into existing end-to-end diﬀerentiable architecture. While reinforcement\nlearning approaches to permutation generation/re-ordering may be natural, we\nbelieve a promising future direction would be direct learning of permutations. In\nthis sense, a model would learn permutations of the input that correspond with\nimproved downstream performance.\n98\nAt ﬁrst glance, such optimization of permutations seems hard to imagine given\nthat permutations are sparse/discrete and therefore unnatural for diﬀerentiable\noptimization. A recent line of work in the computer vision and machine learning\ncommunities has proposed methods towards diﬀerentiable/gradient-based opti-\nmization (Santa Cruz et al., 2019; Mena et al., 2018). In particular, many of these\nmethods consider a generalization of the family of permutation matrices to the fam-\nily of double stochastic matrices (DSMs). Importantly, DSMs specify a polytope\n(the Birkhoﬀpolytope) which can be reached in a diﬀerentiable way via Sinkhorn\niterations (Sinkhorn, 1964; Sinkhorn and Knopp, 1967). Given these works, a nat-\nural question is whether these methods can be extended to sequential data (which\nare not of a ﬁxed length, unlike image data), perhaps with padding as is done in\nTransformers, and what that might suggest for downstream NLP performance.\nAdditional constraints to induce well-posed optimization.\nIn our approach,\nthe solution to the optimization is not necessarily unique. In particular, for all\nthree objectives, the reverse of an optimal sequence is also optimal.\nFurther,\nGildea and Temperley (2007) claim (without proof) that there can be 2\nn\n2 optimal\nsolutions for the optimization problem they study. Given that there may be many\noptima and potentially exponentially many, downstream models are faced with\nuncertainty about the structure of the input for any given input. In particular, the\nword orders we are considering may lack regularity and systematicity properties\nthat are found in natural languages, though the structure of the disjoint strategy\n99\nmay help alleviate this.2 Developing additional constraints to further regularize\nthe optima may beneﬁt modelling as diﬀerent examples will have more standard-\nized formats. Given the observations we discuss in §3.2 regarding harmonic word\norders improving language acquisition for child language learners (by appealing\nto their inductive biases), such as those of Culbertson and Newport (2017), enforc-\ning word order harmonies globally may be a natural means for increases regular-\nity and systematicity in the resultant word orders. Similarly, such approaches may\nprovide additional beneﬁts when extended to the (more general) setting where\ninputs (may) contain multiple sentences.\nInformation locality. In §3.3.3, we discuss the recent proposal for information lo-\ncality as a generalization of dependency locality. As syntactic dependencies are\nclearly only a subset of the linguistic information of interest for a downstream\nmodel, a richer theory of locality eﬀects and a broad classes of information may\nprove fruitful both in study human language processing and in motivating further\nnovel orders. More broadly, we speciﬁcally point to the work of (Xu et al., 2020)\nwhich introduces the V-information theory. In particular, their work builds the\nstandard preliminaries and primitives of Shannon’s information theory (Shannon,\n1948) with the notion of computational constraints. While they envision this in the\ncontext of motivating modern representation learning, we believe this may also be\nbetter theoretical machinery for information theoretic treatments of human lan-\n2They also may lack overt linear proxies for compositionality to the extent found in the corre-\nsponding natural language.\n100\nguage processing. After all, humans also have severely bounded processing capa-\nbilities.\nUnderstanding impact of novel orders. In this work, we put forth a rigorous and\nhighly controlled empirical evaluation of the word orders we introduce. However,\nour results and analysis do not explain why and how the novel word orders lead to\nimproved performance. In particular, a hypothesis that serves as an undercurrent\nfor this work is that the novel word orders we introduce simplify computational\nprocessing by reducing the need for modelling long-distance dependencies and/or\nmany dependencies simultaneously. Future work that executes a more thorough\nand nuanced analysis of model behavior and error patterns would help to validate\nthe extent to which this hypothesis is correct. Additionally, it would help to disam-\nbiguate what modelling improvements that the work yields coincide with other\nmodelling techniques and what improvements are entirely orthogonal to suggest\nhow this work can integrate with other aspects of the vast literature on computa-\ntional modelling in NLP.\nBroader NLP evaluation. Perhaps the most obvious future direction is to consider\na broader evaluation of our framework. In particular, evaluating across pretrain-\ning architectures, ﬁne-tuning architectures, datasets, tasks, and languages are all\nworthwhile for establishing the boundaries of this method’s viability and utility.\nAdditionally, such evaluations may help to further reﬁne the design of novel word\n101\norders and suggest more nuanced procedures for integrating them (especially in\nsettings where the dominant paradigm itself of pretrain-and-finetune is in-\nsuﬃcient). Additionally, we posit that there may be natural interplay with our\nwork and work on cross-lingual and multi-lingual methods that may especially\nmerit concentrated study. Concretely, one could imagine applying the same or-\ndering rule cross-linguistically and therefore normalizing some of the diﬀerences\nacross languages at the input level. Consequently, there may be potential for this\nto improve alignment (e.g. between word embeddings, between pretrained en-\ncoders) for diﬀerent language or better enable cross-lingual transfer/multi-lingual\nrepresentation learning.\n6.4\nConsequences\nPreviously, we have summarized our contributions (§6.1). In this section, we in-\nstead provide more abstract lessons or suggestive takeaways from this work.\nWord order in NLP. In this work, we clearly substantiate that there is tremendous\nscope for improving how word order is considered within NLP and explicitly illu-\nminate well-motivated directions for further empirical work. While our empirical\nresults legitimately and resolutely conﬁrm that theoretically-grounded algorith-\nmic optimization may coincide with empirical NLP improvements, it remains open\nto what extent this is pervasive across NLP tasks and domains.\n102\nGeneralizing measures of language (sub)optimality. In this work, we put forth\nalternative objectives beyond those generally considered in the dependency length\nminimization literature that may merit further psycholinguistic inquiry. In partic-\nular, by adopting the generality of an algorithmic framework, one can naturally\npose many objectives that intuitively may align with human optimization in lan-\nguage production and comprehension. Further, in this work we clarify the extent\nto which human language is optimal and we note that the suboptimality of human\nlanguage may be equally useful for understanding language’s properties as its op-\ntimality.\nInterlacing psycholinguistic or algorithmic methods with NLP. In this work,\nwe present work that is uniquely at the triple intersection of psycholinguis-\ntics, algorithms, and NLP. While such overtly interdisciplinary work is of in-\nterest, we also argue that there is great value in considering the interplay\nbetween psycholinguistics and NLP or between algorithms and NLP, if not\nall three simultaneously.\nThere is a long-standing tradition of connecting\nalgorithms/computational theory with computational linguistics and NLP in\nthe study of grammars and parsing (Chomsky, 1957, 1965; Kay, 1967; Earley,\n1970; Joshi et al., 1975; Charniak, 1983; Pereira and Warren, 1983; Kay, 1986;\nSteedman, 1987; Kay, 1989; Eisner, 1996; Collins, 1996, 1997; Charniak et al., 1998;\nGildea and Jurafsky, 2002; Collins, 2003; Klein and Manning, 2003b,a; Steedman,\n103\n2004; McDonald et al., 2005b; McDonald and Pereira, 2006; Mitkov and Joshi, 2012;\nChomsky, 2014b,a).\nTrailblazers of the ﬁeld, such as Noam Chomsky and\nthe recently-passed Arvind Joshi made great contributions in this line.\nSim-\nilarly, there are has been a recent uptick in the borrowing of methods from\npsycholinguistics to rigorously study human language processing3 to interpret\nand understand neural models of language (Linzen et al., 2016; Gulordava et al.,\n2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Marvin and Linzen, 2018;\nRavfogel et al., 2019; van Schijndel et al., 2019; McCoy et al., 2019; Futrell and Levy,\n2019; Wilcox et al., 2019; Futrell et al., 2019; van Schijndel and Linzen, 2019;\nPrasad et al., 2019; Ettinger, 2020; Davis and van Schijndel, 2020).4\nAnd once\nagain, another seminal mind of the ﬁeld, Ron Kaplan, spoke to precisely this point\nin his keynote talk \"Computational Psycholinguistics\" (Kaplan, 2020) at ACL 2019 as\nhe received the most recent ACL Lifetime Achievement Award. However, in the\npresent time, beyond parsing and interpretability research, we see less interplay\nbetween either algorithms or psycholinguistics and NLP. Perhaps this thesis may\nserve as an implicit hortative to reconsider this.\n6.5\nLimitations\nIn the spirit of diligent science, we enumerate the limitations we are aware of with\nthis work. We attempt to state these limitations fairly without trying to undercut\n3The ﬁrst blackbox language learner we have tried to understand.\n4The second blackbox language learner we have tried to understand.\n104\nor lessen their severity. We also note that we have considered the ethical ramiﬁca-\ntions of this work5 and remark that we did not come to ﬁnd any ethical concerns.6\nDependence on Dependencies. In this work, we generate novel word orders con-\ntingent on access to a dependency parse. Therefore, the generality of our method\nis dependent on access to such a parse. Further, since we exclusively consider En-\nglish dependency parsers, which are substantially higher quality than dependency\nparsers for most languages and especially low-resource languages, and datasets\nwhich are drawn from similar data distributions as the training data of the depen-\ndency parse, it is unclear how well are model with perform in setting where weaker\ndependency parsers are available or there are domain-adaptation concerns.\nRigid Optimization. By design, our approach is to purely optimize an objective\npertaining to dependency locality. While the heuristic algorithms we introduce un-\nder the Transposition Monte Carlo framework aﬀord some consideration\nof negotiating optimization quality with retention of the original sentence’s order\n(through the parameter T), our methods provide no natural recipe for integration\nof arbitrary metadata or auxiliary information/domain knowledge.\n5This was inspired by the NeurIPS 2020 authorship guidelines, which require authors to con-\nsider the social and ethical impacts of their work.\n6While not an ethical concern of the research, we do note that all ﬁgures and tables in this\nwork are designed to be fully colorblind-friendly. In particular, while many ﬁgures/tables use\ncolor, they can also be interpreted using non-color markers and these markers are referenced in the\ncorresponding caption.\n105\nEvaluation Settings. As we note in §5.4, we elect to evaluate on single-sentence\ntext classiﬁcation datasets. While we do provide valid and legitimate reasons for\nthis decision, it does imply that the eﬀectiveness of our methods for other tasks\nand task types (e.g. sequence labelling, natural language generation, question an-\nswering) is left unaddressed. Further, this means that the diﬃculties of intra-\nexample irregularity across sentences (due to optimization not being suﬃciently\ndetermined/constrained) are not considered. As a more general consideration,\nLinzen (2020) argues that the evaluation protocol used in this work may not be\nsuﬃcient for reaching the desired scientiﬁc conclusions regarding word order.\nAlternative Orders in Pretraining. Due to computational constraints, we only\nstudy introducing permutations/re-orderings we generate after embedding us-\ning a pretrained encoder. However, a more natural (and computationally inten-\nsive/environmentally detrimental (Strubell et al., 2019)) approach that may oﬀer\nadditional value is to use the orders during pretraining. As such, our work does\nnot make clear that pretrain-permute-finetune is the best way to introduce\nour permutations into the standard pretrain-and-finetune regime and fails\nto consider an \"obvious\" alternative.\nEnglish Language Processing. In studying the eﬀects of our orders empirically,\nwe only use English language data.7 Several works have shown that performance\n7In this work, we did not reconsider the source of the pretraining data for ELMo or the datasets\nwe used in evaluating our method. Therefore we are not certain, but it seems likely that data mainly\n106\non English data is neither inherently representative nor likely to be representative\nof performance in language processing across the many languages of the world\n(Bender, 2009, 2011, 2012; Joshi et al., 2020b). Further, recent works have shown\nthat English may actually be favorable for approaches that make use of LSTMs\nand RNNs (Dyer et al., 2019; Davis and van Schijndel, 2020), which are models we\nexplicitly use in this work. And ﬁnally, the entire premise of dependency locality\nheavily hinges on typological features of the language being studied.8 Therefore,\nthe results of this work are severely limited in terms of claims that can be made for\nnatural languages that are not English.\nrepresents Standard American English and not other variants of English such as African American\nVernacular English.\n8This can be inferred from our discussion of dependency locality and word ordering eﬀects in\nvarious natural languages in §3; a particularly salient example is that the notion of dependency lo-\ncality is quite diﬀerent in morphologically-rich languages as information is often conveyed through\nmorphological markers and intra-word units rather than word order.\n107\nBIBLIOGRAPHY\nDonald L. Adolphson and T. C. Hu. 1973. Optimal linear ordering. SIAM Journal\non Applied Mathematics, 25(3):403–423.\nCharu C. Aggarwal and ChengXiang Zhai. 2012. A survey of text classiﬁcation\nalgorithms. In Mining text data, pages 163–222. Springer.\nAlexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. 2017.\nDeep variational information bottleneck. In International Conference on Learning\nRepresentations.\nRubayyi Alghamdi and Khalid Alfalqi. 2015. A survey of topic modeling in text mining.\nInternational Journal of Advanced Computer Science and Applications, 6.\nNada\nAlmarwani,\nHanan\nAldarmaki,\nand\nMona\nDiab.\n2019.\nEﬃcient sentence embedding using discrete cosine transform.\nIn\nProceed-\nings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 3672–3678, Hong Kong, China. Association for Computational\nLinguistics.\nAnima Anandkumar, Dean P. Foster, Daniel J. Hsu, Sham M. Kakade, and Yi-Kai\nLiu. 2012. A spectral algorithm for latent dirichlet allocation. In Advances in\nNeural Information Processing Systems, pages 917–925.\nPeggy\nM.\nAndersen,\nPhilip\nJ.\nHayes,\nSteven\nP.\nWeinstein,\nAlison\nK.\nHuettner,\nLinda\nM.\nSchmandt,\nand\nIrene\nB.\nNirenburg.\n1992.\n108\nAutomatic extraction of facts from press releases to generate news stories.\nIn Third Conference on Applied Natural Language Processing, pages 170–177, Trento,\nItaly. Association for Computational Linguistics.\nSanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David\nSontag, Yichen Wu, and Michael Zhu. 2013. A practical algorithm for topic mod-\neling with provable guarantees. In International Conference on Machine Learning,\npages 280–288.\nSanjeev Arora, Rong Ge, and Ankur Moitra. 2012. Learning topic models–going\nbeyond svd. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer\nScience, pages 1–10. IEEE.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2016.\nA latent variable model approach to PMI-based word embeddings. Transactions\nof the Association for Computational Linguistics, 4:385–399.\nSanjeev\nArora,\nYingyu\nLiang,\nand\nTengyu\nMa.\n2017.\nA simple but tough-to-beat baseline for sentence embeddings.\nIn International\nConference on Learning Representations.\nSitaram Asur and Bernardo A. Huberman. 2010. Predicting the future with social\nmedia. In 2010 IEEE/WIC/ACM International Conference on Web Intelligence and\nIntelligent Agent Technology, volume 1, pages 492–499.\nAriful Azad, Mathias Jacquelin, Aydin Buluç, and Esmond G. Ng. 2017. The re-\n109\nverse cuthill-mckee algorithm in distributed-memory. In 2017 IEEE International\nParallel and Distributed Processing Symposium (IPDPS), pages 22–31.\nDzmitry\nBahdanau,\nKyunghyun\nCho,\nand\nYoshua\nBengio.\n2015.\nNeural machine translation by jointly learning to align and translate.\nIn In-\nternational Conference on Learning Representations.\nMarco\nBaroni,\nGeorgiana\nDinu,\nand\nGermán\nKruszewski.\n2014.\nDon’t count, predict! a systematic comparison of context-counting vs. context-predicting semanti\nIn Proceedings of the 52nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1:\nLong Papers), pages 238–247, Baltimore, Maryland.\nAssociation for Computational Linguistics.\nBrian Bartek, Richard L Lewis, Shravan Vasishth, and Mason R. Smith. 2011. In\nsearch of on-line locality eﬀects in sentence comprehension. Journal of experimen-\ntal psychology. Learning, memory, and cognition, 37 5:1178–98.\nMikhael Bautin, Lohit Vijayarenu, and Steven Skiena. 2008. International senti-\nment analysis for news and blogs. pages 19–26.\nOtto Behaghel. 1932. Deutsche Syntax: eine geschichtliche Darstellung : Wortstellung ; Periodenbau.\nv. 4.\nEmily M. Bender. 2009. Linguistically naïve != language independent: Why NLP needs linguistic typ\nIn Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and\nComputational Linguistics: Virtuous, Vicious or Vacuous?, pages 26–32, Athens,\nGreece. Association for Computational Linguistics.\n110\nEmily M. Bender. 2011. On achieving and evaluating language-independence in\nnlp. Linguistic Issues in Language Technology, 6.\nEmily M. Bender. 2012. 100 things you always wanted to know about linguistics but were afraid to a\nIn Tutorial Abstracts at the Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Montréal, Canada.\nAssociation for Computational Linguistics.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003.\nA neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155.\nAdam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maxi-\nmum entropy approach to natural language processing. Computational linguistics,\n22(1):39–71.\nChristopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information\nScience and Statistics). Springer-Verlag, Berlin, Heidelberg.\nGunter Blache, Marek Karpinski, and Jürgen Wirtgen. 1997. On approximation\nintractability of the bandwidth problem. Technical report.\nDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet alloca-\ntion. Journal of machine Learning research, 3(Jan):993–1022.\nLeonard Bloomﬁeld. 1933. Language. The University of Chicago Press.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.\nEnriching word vectors with subword information.\nTransactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\n111\nRishi Bommasani. 2019. Long-distance dependencies don’t have to be long: Simplifying through pro\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Lin-\nguistics: Student Research Workshop, pages 89–99, Florence, Italy. Association for\nComputational Linguistics.\nRishi Bommasani and Claire Cardie. 2019. Towards understanding position em-\nbeddings. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, Florence, Italy. Association for Computa-\ntional Linguistics.\nRishi Bommasani, Kelly Davis, and Claire Cardie. 2020. Interpreting pretrained\ncontextualized representations via reductions to static embeddings. In Proceed-\nings of the 58th Annual Meeting of the Association for Computational Linguistics, Seat-\ntle, Washington. Association for Computational Linguistics.\nRishi\nBommasani,\nArzoo\nKatiyar,\nand\nClaire\nCardie.\n2019.\nSPARSE: Structured prediction using argument-relative structured encoding.\nIn Proceedings of the Third Workshop on Structured Prediction for NLP, pages 13–17,\nMinneapolis, Minnesota. Association for Computational Linguistics.\nRodrigo A. Botafogo. 1993. Cluster analysis for hypertext systems. In Proceedings\nof the 16th annual international ACM SIGIR conference on Research and development\nin information retrieval, pages 116–125.\nJan\nA.\nBotha,\nEmily\nPitler,\nJi\nMa,\nAnton\nBakalov,\nAlex\nSal-\ncianu,\nDavid\nWeiss,\nRyan\nMcDonald,\nand\nSlav\nPetrov.\n2017.\n112\nNatural language processing with small feed-forward networks. In Proceedings\nof the 2017 Conference on Empirical Methods in Natural Language Processing, pages\n2879–2885, Copenhagen, Denmark. Association for Computational Linguistics.\nSamuel R. Bowman, Christopher Potts, and Christopher D. Manning. 2015.\nRecursive neural networks can learn logical semantics. In Proceedings of the 3rd\nWorkshop on Continuous Vector Space Models and their Compositionality, pages 12–\n21, Beijing, China. Association for Computational Linguistics.\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. 2017.\nQuasi-recurrent neural networks. In International Conference on Learning Repre-\nsentations.\nLea Brown. 2001. A grammar of nias selatan.\nPeter Brown, Stephen Della Pietra, Vincent Dellapietra, John Laﬀerty, and L. Mer-\ncer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In\nProc. International Conference on Theoretical Methodological Issues in Machine Trans-\nlation, 1992.\nMary\nElaine\nCaliﬀ\nand\nRaymond\nJ.\nMooney.\n1997.\nRelational learning of pattern-match rules for information extraction.\nIn\nCoNLL97: Computational Natural Language Learning.\nRamon Ferrer-i Cancho. 2006. Why do syntactic links not cross? Europhysics Let-\nters (EPL), 76(6):1228–1235.\n113\nRamon\nFerrer-i\nCancho\nand\nRicard\nV.\nSolé.\n2003.\nLeast eﬀort and the origins of scaling in human language.\nProceedings\nof\nthe National Academy of Sciences of the United States of America, 100(3):788–791.\nClaire Cardie. 1997. Empirical methods in information extraction. AI Magazine,\n18(4):65–80.\nW. M. Chan and Alan George. 1980. A linear time implementation of the reverse\ncuthill-mckee algorithm. BIT, 20:8–14.\nPi-Chuan\nChang,\nDaniel\nJurafsky,\nand\nChristopher\nD.\nManning.\n2009.\nDisambiguating “DE” for Chinese-English machine translation.\nIn Proceed-\nings of the Fourth Workshop on Statistical Machine Translation, pages 215–223,\nAthens, Greece. Association for Computational Linguistics.\nEugene Charniak. 1983. A parser with something for everyone. Parsing natural\nlanguage, pages 117–149.\nEugene Charniak. 1994. Statistical Language Learning. MIT Press, Cambridge, MA,\nUSA.\nEugene\nCharniak,\nSharon\nGoldwater,\nand\nMark\nJohnson.\n1998.\nEdge-based best-ﬁrst chart parsing. In Sixth Workshop on Very Large Corpora.\nMichael Chau and Jennifer Xu. 2012. Business intelligence in blogs: Understanding\nconsumer interactions and communities. MIS quarterly, pages 1189–1216.\nCiprian\nChelba,\nTomas\nMikolov,\nMike\nSchuster,\nQi\nGe,\nThorsten\nBrants,\nPhillipp\nKoehn,\nand\nTony\nRobinson.\n2013.\n114\nOne billion word benchmark for measuring progress in statistical language modeling.\nTechnical report, Google.\nDanqi\nChen\nand\nChristopher\nD.\nManning.\n2014.\nA fast and accurate dependency parser using neural networks. In Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages 740–750, Doha, Qatar. Association for Computational Linguistics.\nXilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. 2018.\nAdversarial deep averaging networks for cross-lingual sentiment classiﬁcation.\nTransactions of the Association for Computational Linguistics, 6:557–570.\nNancy\nChinchor,\nLynette\nHirschman,\nand\nDavid\nD.\nLewis.\n1993.\nEvaluating message understanding systems: An analysis of the third message understanding con\nComputational Linguistics, 19(3):409–450.\nPhyllis Z. Chinn, Jarmila Chvátalová, Alexander K. Dewdney, and Norman E.\nGibbs. 1982. The bandwidth problem for graphs and matrices — a survey. Jour-\nnal of Graph Theory, 6(3):223–254.\nChung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick\nNguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekate-\nrina Gonina, et al. 2018. State-of-the-art speech recognition with sequence-to-\nsequence models. In 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 4774–4778. IEEE.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.\n115\n2014a. On the properties of neural machine translation: Encoder–decoder approaches.\nIn Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Sta-\ntistical Translation, pages 103–111, Doha, Qatar. Association for Computational\nLinguistics.\nKyunghyun Cho,\nBart van Merriënboer,\nCaglar Gulcehre,\nDzmitry Bah-\ndanau,\nFethi\nBougares,\nHolger\nSchwenk,\nand\nYoshua\nBengio.\n2014b.\nLearning phrase representations using RNN encoder–decoder for statistical machine translation.\nIn Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1724–1734, Doha, Qatar. Association for Computational\nLinguistics.\nYejin Choi,\nClaire Cardie,\nEllen Riloﬀ,\nand Siddharth Patwardhan. 2005.\nIdentifying sources of opinions with conditional random ﬁelds and extraction patterns.\nIn Proceedings of Human Language Technology Conference and Conference on Empir-\nical Methods in Natural Language Processing, pages 355–362, Vancouver, British\nColumbia, Canada. Association for Computational Linguistics.\nNoam Chomsky. 1956. Three models for the description of language. IRE Transac-\ntions on Information Theory, 2(3):113–124.\nNoam Chomsky. 1957. Syntactic structures.\nNoam Chomsky. 1965. Aspects of the Theory of Syntax. The MIT Press, Cambridge.\nNoam Chomsky. 1986. Knowledge of language: Its nature, origin, and use. Greenwood\nPublishing Group.\n116\nNoam Chomsky. 2014a. Aspects of the Theory of Syntax, volume 11. MIT Press,\nCambridge, MA, USA.\nNoam Chomsky. 2014b. The minimalist program. MIT Press, Cambridge, MA, USA.\nSumit\nChopra,\nMichael\nAuli,\nand\nAlexander\nM.\nRush.\n2016.\nAbstractive sentence summarization with attentive recurrent neural networks.\nIn Proceedings of the 2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 93–98, San\nDiego, California. Association for Computational Linguistics.\nFan Chung. 1984. On optimal linear arrangements of trees. Computers & mathemat-\nics with applications, 10(1):43–60.\nAndy Clark. 2013. Whatever next? predictive brains, situated agents, and the future of cognitive scie\nBehavioral and Brain Sciences, 36(3):181–204.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.\nElectra: Pre-training text encoders as discriminators rather than generators. In\nInternational Conference on Learning Representations.\nMichael Collins. 1997. Three generative, lexicalised models for statistical parsing.\nIn 35th Annual Meeting of the Association for Computational Linguistics and 8th Con-\nference of the European Chapter of the Association for Computational Linguistics, pages\n16–23, Madrid, Spain. Association for Computational Linguistics.\nMichael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and exp\n117\nIn Proceedings of the 2002 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP 2002), pages 1–8. Association for Computational Linguistics.\nMichael Collins. 2003. Head-driven statistical models for natural language parsing.\nComputational Linguistics, 29(4):589–637.\nMichael\nCollins,\nPhilipp\nKoehn,\nand\nIvona\nKučerová.\n2005.\nClause restructuring for statistical machine translation. In Proceedings of the 43rd\nAnnual Meeting of the Association for Computational Linguistics (ACL’05), pages\n531–540, Ann Arbor, Michigan. Association for Computational Linguistics.\nMichael John Collins. 1996. A new statistical parser based on bigram lexical dependencies.\nIn 34th Annual Meeting of the Association for Computational Linguistics, pages 184–\n191, Santa Cruz, California, USA. Association for Computational Linguistics.\nRonan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: De\nIn Proceedings of the 25th International Conference on Machine Learning, ICML ’08,\npages 160–167, New York, NY, USA. ACM.\nRonan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu,\nand Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J.\nMach. Learn. Res., 12:2493–2537.\nBernard Comrie. 1981. Language universals and linguistic typology. Oxford.\nJosé R Correa and Andreas S Schulz. 2004. Single machine scheduling with prece-\ndence constraints. In International Conference on Integer Programming and Combi-\nnatorial Optimization, pages 283–297. Springer.\n118\nGonçalo\nM.\nCorreia,\nVlad\nNiculae,\nand\nAndré\nF.\nT.\nMartins.\n2019.\nAdaptively sparse transformers.\nIn Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184,\nHong Kong, China. Association for Computational Linguistics.\nJennifer Culbertson and Simon Kirby. 2016. Simplicity and speciﬁcity in language:\nDomain-general biases have domain-speciﬁc eﬀects. Frontiers in Psychology, 6.\nJennifer\nCulbertson\nand\nElissa\nL.\nNewport.\n2015.\nHarmonic biases in child learners: In support of language universals.\nCog-\nnition, 139:71 – 82.\nJennifer\nCulbertson\nand\nElissa\nL.\nNewport.\n2017.\nInnovation of word order harmony across development.\nOpen Mind, 1(2):91–\n100.\nJennifer\nCulbertson,\nPaul\nSmolensky,\nand\nGéraldine\nLegendre.\n2012.\nLearning biases predict a word order universal. Cognition, 122(3):306 – 329.\nElizabeth Cuthill and James McKee. 1969. Reducing the bandwidth of sparse sym-\nmetric matrices. In Proceedings of the 1969 24th national conference, pages 157–172.\nZihang\nDai,\nZhilin\nYang,\nYiming\nYang,\nJaime\nCar-\nbonell,\nQuoc\nLe,\nand\nRuslan\nSalakhutdinov.\n2019.\nTransformer-XL: Attentive language models beyond a ﬁxed-length context.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational\n119\nLinguistics, pages 2978–2988, Florence, Italy. Association for Computational\nLinguistics.\nJoachim Daiber, Miloš Stanojević, Wilker Aziz, and Khalil Sima’an. 2016.\nExamining the relationship between preordering and word order freedom in machine translation\nIn Proceedings of the First Conference on Machine Translation: Volume 1, Research\nPapers, pages 118–130, Berlin, Germany. Association for Computational Linguis-\ntics.\nForrest\nDavis\nand\nMarten\nvan\nSchijndel.\n2020.\nRecurrent neural network language models always learn english-like relative clause attachment.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Lin-\nguistics, Seattle, Washington. Association for Computational Linguistics.\nTimothy A. Davis. 2006. Direct Methods for Sparse Linear Systems.\nJeﬀrey Dean. 2009. Challenges in building large-scale information retrieval systems: invited talk.\nIn WSDM ’09: Proceedings of the Second ACM International Conference on Web\nSearch and Data Mining, pages 1–1, New York, NY, USA.\nScott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and\nRichard Harshman. 1990. Indexing by latent semantic analysis. Journal of the\nAmerican society for information science, 41(6):391–407.\nVera Demberg. 2010. Broad-coverage model of prediction in human sentence pro-\ncessing.\n120\nVera Demberg and Frank Keller. 2008a. Data from eye-tracking corpora as evidence for theories of sy\nCognition, 109(2):193 – 210.\nVera Demberg and Frank Keller. 2008b. A psycholinguistically motivated version of TAG.\nIn Proceedings of the Ninth International Workshop on Tree Adjoining Grammar and\nRelated Frameworks (TAG+9), pages 25–32, Tübingen, Germany. Association for\nComputational Linguistics.\nVera Demberg and Frank Keller. 2009. A computational model of prediction in hu-\nman parsing: Unifying locality and surprisal eﬀects. In CogSci 2009 Proceedings,\npages 1888–1893. Cognitive Science Society.\nVera\nDemberg,\nFrank\nKeller,\nand\nAlexander\nKoller.\n2013.\nIncremental, predictive parsing with psycholinguistically motivated tree-adjoining grammar.\nComputational Linguistics, 39(4):1025–1066.\nJohn DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reord\nIn Proceedings of the 2011 Conference on Empirical Methods in Natural Language\nProcessing, pages 193–203, Edinburgh, Scotland, UK. Association for Computa-\ntional Linguistics.\nDesmond C. Derbyshire. 1979. Hixkaryana, volume 1 of Lingua Descriptive Studies.\nNorth-Holland, Amsterdam.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\nBERT: Pre-training of deep bidirectional transformers for language understanding.\n121\nIn Proceedings of the 2019 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJosep\nDíaz,\nMathew\nPenrose,\nJordi\nPetit,\nand\nMaria\nSerna.\n1999.\nLayout problems on lattice graphs. volume 1627, pages 103–112.\nJosep Díaz, Jordi Petit, and Maria Serna. 2002. A survey of graph layout problems.\nACM Computing Surveys (CSUR), 34(3):313–356.\nMyles Dillon and Donncha Ó Cróinin. 1961. Teach Yourself Irish. The English Uni-\nversities Press Ltd., London.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith.\n2019. Show your work: Improved reporting of experimental results. In Proceed-\nings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 2185–2194, Hong Kong, China. Association for Computational\nLinguistics.\nPedro Domingos and Michael Pazzani. 1997.\nOn the optimality of the simple\nbayesian classiﬁer under zero-one loss. Machine learning, 29(2-3):103–130.\nTimothy Dozat and Christopher D. Manning. 2016. Deep biaﬃne attention for\nneural dependency parsing. ArXiv, abs/1611.01734.\n122\nTimothy\nDozat,\nPeng\nQi,\nand\nChristopher\nD.\nManning.\n2017.\nStanford’s graph-based neural dependency parser at the CoNLL 2017 shared task.\nIn Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text\nto Universal Dependencies, pages 20–30, Vancouver, Canada. Association for\nComputational Linguistics.\nRebecca Dridan and Stephan Oepen. 2012. Tokenization: Returning to a long solved problem — a su\nIn Proceedings of the 50th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 378–382, Jeju Island, Korea. Association\nfor Computational Linguistics.\nMatthew S. Dryer. 1988. Object-verb order and adjective-noun order: Dispelling a myth.\nLingua, 74(2):185 – 217. Papers in Universal Grammar: Generative and Typo-\nlogical Approaches.\nMatthew S. Dryer. 1992.\nThe greenbergian word order correlations.\nLanguage,\n68(1):81–138.\nMatthew S. Dryer. 1997. On the six-way word order typology. Studies in Language.\nInternational Journal sponsored by the Foundations of Language, 21(1):69–103.\nMatthew S. Dryer. 1998. Why statistical universals are better than absolute univer-\nsals. pages 1–23.\nMatthew S. Dryer. 2013a. Determining dominant word order. In Matthew S. Dryer\nand Martin Haspelmath, editors, The World Atlas of Language Structures Online.\nMax Planck Institute for Evolutionary Anthropology, Leipzig.\n123\nMatthew S. Dryer. 2013b. On the six-way word order typology, again. Studies in\nLanguage. International Journal sponsored by the Foundations of Language, 37(2):267–\n301.\nHomer Dudley. 1939. The vocoder. pages 122–126. Bell Labs.\nHomer Dudley, RR Riesz, and SSA Watkins. 1939. A synthetic speaker. Journal of\nthe Franklin Institute, 227(6):739–764.\nChris Dyer. 2017. Should neural network architecture reﬂect linguistic structure?\nIn Proceedings of the 21st Conference on Computational Natural Language Learning\n(CoNLL 2017), page 1, Vancouver, Canada. Association for Computational Lin-\nguistics.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016.\nRecurrent neural network grammars. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pages 199–209, San Diego, California. Association for Compu-\ntational Linguistics.\nChris\nDyer,\nGábor\nMelis,\nand\nPhil\nBlunsom.\n2019.\nA critical analysis of biased parsers in unsupervised parsing.\nChris Dyer and Philip Resnik. 2010. Context-free reordering, ﬁnite-state translation.\nIn Human Language Technologies: The 2010 Annual Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics, pages 858–866, Los\nAngeles, California. Association for Computational Linguistics.\n124\nJay Earley. 1970.\nAn eﬃcient context-free parsing algorithm.\nCommun. ACM,\n13(2):94–102.\nJacob Eisenstein. 2019. Introduction to Natural Language Processing. Adaptive Com-\nputation and Machine Learning series. MIT Press.\nJason Eisner and Noah A. Smith. 2005. Parsing with soft and hard constraints on dependency length\nIn Proceedings of the Ninth International Workshop on Parsing Technology, pages\n30–41, Vancouver, British Columbia. Association for Computational Linguistics.\nJason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration.\nIn COLING 1996 Volume 1: The 16th International Conference on Computational\nLinguistics.\nJeﬀrey L. Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.\nKawin Ethayarajh. 2018. Unsupervised random walk sentence embeddings: A strong but simple bas\nIn Proceedings of The Third Workshop on Representation Learning for NLP, pages\n91–100, Melbourne, Australia. Association for Computational Linguistics.\nKawin\nEthayarajh,\nDavid\nDuvenaud,\nand\nGraeme\nHirst.\n2019.\nTowards understanding linear word analogies.\nIn Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pages 3253–3262,\nFlorence, Italy. Association for Computational Linguistics.\nAllyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics fo\nTransactions of the Association for Computational Linguistics, 8:34–48.\n125\nOren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008.\nOpen information extraction from the web. Commun. ACM, 51(12):68–74.\nOren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu,\nTal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004.\nWeb-scale information extraction in knowitall: (preliminary results).\nIn Pro-\nceedings of the 13th International Conference on World Wide Web, WWW 2004, pages\n100–110, New York, NY, USA. Association for Computing Machinery.\nOren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and\nMausam Mausam. 2011. Open information extraction: The second generation.\nIn Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial Intel-\nligence - Volume Volume One, IJCAI 2011, pages 3–10. AAAI Press.\nManaal Faruqui. 2016. Diverse Context for Learning Word Representations. Ph.D. the-\nsis, Carnegie Mellon University.\nMichael R. Fellows and Michael A. Langston. 1994. On search, decision, and the\neﬃciency of polynomial-time algorithms. Journal of Computer and System Sciences,\n49(3):769–779.\nJohn Rupert Firth. 1957. Applications of general linguistics. Transactions of the\nPhilological Society, 56(1):1–14.\nMarilyn Ford, Joan Bresnan, and Ronald M. Kaplan. 1982. A competence-based\ntheory of syntactic closure. The Mental Representation of Grammatical Relations,\npages 727–796.\n126\nDayne Freitag and Andrew McCallum. 1999. Information extraction with hmms\nand shrinkage.\nDayne Freitag and Andrew McCallum. 2000. Information extraction with hmm\nstructures learned by stochastic optimization. In AAAI/IAAI.\nKarl Friston and Stefan Kiebel. 2009.\nPredictive coding under the free-energy\nprinciple.\nPhilosophical Transactions of the Royal Society B: Biological Sciences,\n364(1521):1211–1221.\nRichard Futrell. 2019. Information-theoretic locality properties of natural language.\nIn Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest\n2019), pages 2–15, Paris, France. Association for Computational Linguistics.\nRichard\nFutrell,\nEdward\nGibson,\nand\nRoger\nP.\nLevy.\n2020.\nLossy-context surprisal: An information-theoretic model of memory eﬀects in sentence processin\nCognitive Science, 44(3):e12814.\nRichard Futrell and Roger Levy. 2017. Noisy-context surprisal as a human sentence processing cost m\nIn Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 1, Long Papers, pages 688–698, Valencia, Spain.\nAssociation for Computational Linguistics.\nRichard Futrell and Roger P. Levy. 2019. Do RNNs learn human-like abstract word order preferences\nIn Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages\n50–59.\n127\nRichard\nFutrell,\nKyle\nMahowald,\nand\nEdward\nGibson.\n2015.\nLarge-scale evidence of dependency length minimization in 37 languages.\nProceedings of the National Academy of Sciences, 112(33):10336–10341.\nRichard\nFutrell,\nEthan\nWilcox,\nTakashi\nMorita,\nPeng\nQian,\nMiguel\nBallesteros,\nand\nRoger\nLevy.\n2019.\nNeural language models as psycholinguistic subjects: Representations of syntactic state.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRobert Gaizauskas and Yorick Wilks. 1998. Information extraction: beyond document retrieval.\nJournal of Documentation, 54(1):70–105.\nMichael R. Garey, Ronald L. Graham, David S. Johnson, and Donald E. Knuth.\n1978. Complexity results for bandwidth minimization. SIAM Journal on Applied\nMathematics, 34(3):477–495.\nMichael R. Garey, David S. Johnson, and Larry Stockmeyer. 1974. Some simpli-\nﬁed np-complete problems. In Proceedings of the sixth annual ACM symposium on\nTheory of computing, pages 47–63.\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Word em-\nbeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the\nNational Academy of Sciences, 115(16):E3635–E3644.\n128\nFanica Gavril. 2011. Some np-complete problems on graphs. In CISS 2011.\nDmitriy Genzel. 2010. Automatically learning source-side reordering rules for large scale machine tr\nIn Proceedings of the 23rd International Conference on Computational Linguistics\n(Coling 2010), pages 376–384, Beijing, China. Coling 2010 Organizing Commit-\ntee.\nSean Gerrish and David M. Blei. 2012. How they vote: Issue-adjusted models of\nlegislative behavior. In Advances in Neural Information Processing Systems, pages\n2753–2761.\nEdward Gibson. 1998.\nLinguistic complexity: locality of syntactic dependencies.\nCognition, 68(1):1 – 76.\nEdward Gibson. 2000. The dependency locality theory: A distance-based theory\nof linguistic complexity.\nEdward\nGibson,\nRichard\nFutrell,\nSteven\nP.\nPiantadosi,\nIsabelle\nDautriche,\nKyle\nMahowald,\nLeon\nBergen,\nand\nRoger\nLevy.\n2019.\nHow eﬃciency shapes human language. Trends in Cognitive Sciences, 23(5):389\n– 407.\nEdward\nGibson\nand\nNeal\nJ\nPearlmutter.\n1998.\nConstraints on sentence comprehension. Trends in Cognitive Sciences, 2(7):262 –\n268.\nDaniel Gildea and Daniel Jurafsky. 2002.\nAutomatic labeling of semantic roles.\nComputational Linguistics, 28(3):245–288.\n129\nDaniel Gildea and David Temperley. 2007. Optimizing grammars for minimum dependency length.\nIn Proceedings of the 45th Annual Meeting of the Association of Computational Lin-\nguistics, pages 184–191, Prague, Czech Republic. Association for Computational\nLinguistics.\nAdrià\nde\nGispert,\nGonzalo\nIglesias,\nand\nBill\nByrne.\n2015.\nFast and accurate preordering for SMT using neural networks.\nIn\nProceed-\nings of the 2015 Conference of the North American Chapter of the Association for\nComputational Linguistics:\nHuman Language Technologies, pages 1012–1017,\nDenver, Colorado. Association for Computational Linguistics.\nNamrata Godbole, Manjunath Srinivasaiah, and Steven Skiena. 2007. Large-scale\nsentiment analysis for news and blogs (system demonstration). In ICWSM.\nMark K. Goldberg and I. A. Klipker. 1976. Minimal placing pf trees on a line.\nYoav Goldberg and Graeme Hirst. 2017. Neural Network Methods in Natural Language\nProcessing. Morgan & Claypool Publishers.\nCarlos\nGómez-Rodríguez,\nTianze\nShi,\nand\nLillian\nLee.\n2018.\nGlobal transition-based non-projective dependency parsing.\nIn Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2664–2675, Melbourne, Australia. Association for\nComputational Linguistics.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The\nMIT Press.\n130\nPrem Gopalan, Jake M. Hofman, and David M. Blei. 2015. Scalable recommen-\ndation with hierarchical poisson factorization. In Proceedings of the Thirty-First\nConference on Uncertainty in Artiﬁcial Intelligence, pages 326–335.\nIsao\nGoto,\nMasao\nUtiyama,\nand\nEiichiro\nSumita.\n2012.\nPost-ordering by parsing for Japanese-English statistical machine translation.\nIn Proceedings of the 50th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 311–316, Jeju Island, Korea. Association\nfor Computational Linguistics.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic preordering for controlled\nparaphrase generation. In Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, Seattle, Washington. Association for Computa-\ntional Linguistics.\nJoseph Greenberg. 1963. Some universals of grammar with particular reference\nto the order of meaningful elements. In J. Greenberg, ed., Universals of Language.\n73-113. Cambridge, MA.\nThomas L. Griﬃths and Mark Steyvers. 2004. Finding scientiﬁc topics. Proceedings\nof the National academy of Sciences, 101(suppl 1):5228–5235.\nRalph Grishman and Beth Sundheim. 1996. Message understanding conference- 6: A brief history.\nIn COLING 1996 Volume 1: The 16th International Conference on Computational\nLinguistics.\n131\nDaniel Grodner and Edward Gibson. 2005. Consequences of the serial nature of\nlinguistic input for sentenial complexity. Cognitive science, 29 2:261–90.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Ba-\nroni. 2018. Colorless green recurrent networks dream hierarchically. In Proceed-\nings of the 2018 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages\n1195–1205, New Orleans, Louisiana. Association for Computational Linguistics.\nMichael Hahn, Judith Degen, Noah D. Goodman, Dan Jurafsky, and Richard\nFutrell. 2018. An information-theoretic explanation of adjective ordering pref-\nerences. Cognitive Science.\nMichael Hahn and Richard Futrell. 2019. Estimating predictive rate–distortion curves via neural vari\nEntropy, 21(7).\nMichael\nHahn,\nDan\nJurafsky,\nand\nRichard\nFutrell.\n2020.\nUniversals of word order reﬂect optimization of grammars for eﬃcient communication.\nProceedings of the National Academy of Sciences, 117(5):2347–2353.\nJohn Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Sec-\nond Meeting of the North American Chapter of the Association for Computational Lin-\nguistics.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro,\nand Jonathan Brennan. 2018.\nFinding syntax in human encephalography with beam search. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\n132\nLong Papers), pages 2727–2736, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nFrank Harary. 1967. Problem 16. Theory of Graphs and its Applications, page 161.\nLawrence H. Harper. 1964. Optimal assignments of numbers to vertices. Journal of\nthe Society for Industrial and Applied Mathematics, 12(1):131–135.\nLawrence H. Harper. 1966. Optimal numberings and isoperimetric problems on\ngraphs. Journal of Combinatorial Theory, 1(3):385–393.\nZellig S. Harris. 1954. Distributional structure. Word, 10(2-3):146–162.\nMartin Haspelmath. 1999. Optimality and diachronic adaptation. Zeitschrift für\nSprachwissenschaft, 18:180 – 205.\nJ.A. Hawkins. 1983. Word Order Universals. Quantitative analyses of linguistic struc-\nture. Academic Press.\nJ.A. Hawkins, S.R. Anderson, J. Bresnan, B. Comrie, W. Dressler, C.J. Ewen, and\nR. Huddleston. 1994. A Performance Theory of Order and Constituency. Cambridge\nStudies in Linguistics. Cambridge University Press.\nJohn A Hawkins. 1988. Explaining language universals. Blackwell.\nJohn A. Hawkins. 1990. A parsing theory of word order universals. Linguistic In-\nquiry, 21(2):223–261.\nDavid G. Hays. 1964. Dependency theory: A formalism and some observations.\nLanguage, 40(4):511–525.\n133\nJeﬀrey Heath. 1984. A Functional Grammar of Nunggubuyu. Humanities Press /\nAustralian Institute of Aboriginal Studies, Atlantic Highlands N. J. / Canberra.\nJulia\nHirschberg\nand\nChristopher\nD.\nManning.\n2015.\nAdvances in natural language processing. Science, 349(6245):261–266.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural\nComput., 9(8):1735–1780.\nCharles F. Hockett. 1960. The origin of speech. Scientiﬁc American, 203(3):88–97.\nMatthew Hoﬀman, Francis R. Bach, and David M. Blei. 2010. Online learning for la-\ntent dirichlet allocation. In advances in neural information processing systems, pages\n856–864.\nMatthew Honnibal and Ines Montani. 2017. spacy 2: Natural language understand-\ning with bloom embeddings, convolutional neural networks and incremental\nparsing. To appear.\nSho\nHoshino,\nHubert\nSoyer,\nYusuke\nMiyao,\nand\nAkiko\nAizawa.\n2014.\nJapanese to English machine translation using preordering and compositional distributed seman\nIn Proceedings of the 1st Workshop on Asian Translation (WAT2014), pages 55–63,\nTokyo, Japan. Workshop on Asian Translation.\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichan-\ndran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the\nFirst International Conference on Human Language Technology Research.\n134\nJeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcati\nIn Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nMinqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In\nProceedings of the tenth ACM SIGKDD international conference on Knowledge discov-\nery and data mining, pages 168–177.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. 2015.\nDeep unordered composition rivals syntactic methods for text classiﬁcation. In\nProceedings of the 53rd Annual Meeting of the Association for Computational Linguis-\ntics and the 7th International Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1681–1691, Beijing, China. Association for Computa-\ntional Linguistics.\nRay Jackendoﬀ. 2002. English particle constructions, the lexicon, and the autonomy of syntax,\npages 67 – 94. De Gruyter Mouton, Berlin, Boston.\nT. Florian Jaeger and Harry Tily. 2011. On language ‘utility’: processing complexity and communicat\nWIREs Cognitive Science, 2(3):323–335.\nSarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceed-\nings of the 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota. Association for Computational\nLinguistics.\n135\nFred Jelinek. 1976. Continuous speech recognition by statistical methods. Proceed-\nings of the IEEE, 64(4):532–556.\nHamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng, Xiahui Jiang, Yanchao Li, and\nLiang Zhao. 2019. Latent dirichlet allocation (lda) and topic modeling: models,\napplications, a survey. Multimedia Tools and Applications, 78(11):15169–15211.\nThorsten Joachims. 1997. A probabilistic analysis of the rocchio algorithm with\ntﬁdf for text categorization. In Proceedings of the 14th International Conference on\nMachine Learning.\nThorsten Joachims. 1998. Text categorization with support vector machines: Learn-\ning with many relevant features. In European conference on machine learning, pages\n137–142. Springer.\nMichael I. Jordan. 1989. Serial order: A parallel distributed processing approach.\nAdvances in Connectionist Theory.\nAravind\nK.\nJoshi,\nLeon\nS.\nLevy,\nand\nMasako\nTakahashi.\n1975.\nTree adjunct grammars.\nJournal of Computer and System Sciences, 10(1):136\n– 163.\nMandar\nJoshi,\nDanqi\nChen,\nYinhan\nLiu,\nDaniel\nS.\nWeld,\nLuke\nZettlemoyer,\nand\nOmer\nLevy.\n2020a.\nSpanbert: Improving pre-training by representing and predicting spans. Trans-\nactions of the Association for Computational Linguistics, 8:64–77.\n136\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury.\n2020b.\nThe state and fate of linguistic diversity and inclusion in the nlp world.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Lin-\nguistics, Seattle, Washington. Association for Computational Linguistics.\nDan Jurafsky. 2003. Probabilistic modeling in psycholinguistics: Linguistic compre-\nhension and production. In PROBABILISTIC LINGUISTICS, pages 39–96. MIT\nPress.\nDaniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An\nIntroduction to Natural Language Processing, Computational Linguistics, and Speech\nRecognition, 1st edition. Prentice Hall PTR, USA.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pages 1700–1709, Seattle, Washington, USA. Association for Compu-\ntational Linguistics.\nRonald M. Kaplan. 2005. A method for tokenizing text. Inquiries into words, con-\nstraints and contexts, 55.\nRonald M. Kaplan. 2020. Computational psycholinguistics. Computational Linguis-\ntics, 45(4):607–626.\nDavid R. Karger. 2001.\nA randomized fully polynomial time approximation\nscheme for the all-terminal network reliability problem. SIAM review, 43(3):499–\n522.\n137\nJason Katz-Brown,\nSlav Petrov,\nRyan McDonald,\nFranz Och,\nDavid Tal-\nbot,\nHiroshi\nIchikawa,\nMasakazu\nSeno,\nand\nHideto\nKazawa.\n2011.\nTraining a parser for machine translation reordering.\nIn Proceedings of the\n2011 Conference on Empirical Methods in Natural Language Processing, pages\n183–192, Edinburgh, Scotland, UK. Association for Computational Linguistics.\nYuki\nKawara,\nChenhui\nChu,\nand\nYuki\nArase.\n2018.\nRecursive neural network based preordering for English-to-Japanese machine translation.\nIn Proceedings of ACL 2018, Student Research Workshop, pages 21–27, Melbourne,\nAustralia. Association for Computational Linguistics.\nMartin Kay. 1967. Experiments with a powerful parser. In COLING 1967 Volume 1:\nConference Internationale Sur Le Traitement Automatique Des Langues.\nMartin Kay. 1986. Parsing in functional uniﬁcation grammar. In Readings in natural\nlanguage processing, pages 125–138.\nMartin Kay. 1989. Head-driven parsing. In Proceedings of the First International Work-\nshop on Parsing Technologies, pages 52–62, Pittsburgh, Pennsylvania, USA. Carn-\negy Mellon University.\nYoon Kim,\nCarl Denton,\nLuong Hoang,\nand Alexander M. Rush. 2017.\nStructured attention networks. In International Conference on Learning Represen-\ntations.\nYoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gábor\nMelis. 2019. Unsupervised recurrent neural network grammars. In Proceedings\n138\nof the 2019 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npages 1105–1117, Minneapolis, Minnesota. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization.\nIn International Conference on Learning Representations.\nDan Klein and Christopher Manning. 2004. Corpus-based induction of syntactic structure: Models o\nIn Proceedings of the 42nd Annual Meeting of the Association for Computational\nLinguistics (ACL-04), pages 478–485, Barcelona, Spain.\nDan\nKlein\nand\nChristopher\nD.\nManning.\n2003a.\nA* parsing: Fast exact Viterbi parse selection. In Proceedings of the 2003 Human\nLanguage Technology Conference of the North American Chapter of the Association for\nComputational Linguistics, pages 119–126.\nDan Klein and Christopher D. Manning. 2003b. Accurate unlexicalized parsing. In\nProceedings of the 41st Annual Meeting of the Association for Computational Linguis-\ntics, pages 423–430, Sapporo, Japan. Association for Computational Linguistics.\nPhilipp Koehn. 2010. Statistical Machine Translation, 1st edition. Cambridge Univer-\nsity Press, USA.\nAustin C. Kozlowski, Matt Taddy, and James A. Evans. 2019. The geometry of\nculture: Analyzing the meanings of class through word embeddings. American\nSociological Review, 84(5):905–949.\n139\nSusumo Kuno. 1973. The Structure of the Japanese Language. Massachusetts Institute\nof Technology Press, Cambridge, Massachusetts.\nJohn D. Laﬀerty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Condi-\ntional random ﬁelds: Probabilistic models for segmenting and labeling sequence\ndata. In Proceedings of the Eighteenth International Conference on Machine Learning,\npages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\nZhenzhong\nLan,\nMingda\nChen,\nSebastian\nGoodman,\nKevin\nGimpel,\nPiyush\nSharma,\nand\nRadu\nSoricut.\n2020.\nAlbert: A lite bert for self-supervised learning of language representations.\nIn International Conference on Learning Representations.\nRaymond\nLau,\nRonald\nRosenfeld,\nand\nSalim\nRoukos.\n1993.\nAdaptive language modeling using the maximum entropy principle. In Human\nLanguage Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey,\nMarch 21-24, 1993.\nMoontae\nLee,\nSungjun\nCho,\nDavid\nBindel,\nand\nDavid\nMimno.\n2019.\nPractical correlated topic modeling and analysis via the rectiﬁed anchor word algorithm.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 4991–5001, Hong Kong, China. Association for\nComputational Linguistics.\nW Lehnert. 1986. A Conceptual Theory of Question Answering, pages 651–657. Mor-\ngan Kaufmann Publishers Inc., San Francisco, CA, USA.\n140\nWendy G. Lehnert. 1977a. A conceptual theory of question answering. In Pro-\nceedings of the 5th International Joint Conference on Artiﬁcial Intelligence - Volume 1,\npages 158–164, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\nWendy Grace Lehnert. 1977b. The process of question answering.\nTao\nLei,\nYu\nZhang,\nSida\nI.\nWang,\nHui\nDai,\nand\nYoav\nArtzi.\n2018.\nSimple recurrent units for highly parallelizable recurrence.\nIn\nProceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing, pages\n4470–4481, Brussels, Belgium. Association for Computational Linguistics.\nUri Lerner and Slav Petrov. 2013. Source-side classiﬁer preordering for machine translation.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pages 513–523, Seattle, Washington, USA. Association for Computa-\ntional Linguistics.\nOmer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix\nfactorization. In Advances in neural information processing systems, pages 2177–\n2185.\nOmer\nLevy,\nYoav\nGoldberg,\nand\nIdo\nDagan.\n2015.\nImproving distributional similarity with lessons learned from word embeddings.\nTransactions of the Association for Computational Linguistics, 3:211–225.\nRoger Levy. 2008a.\nExpectation-based syntactic comprehension.\nCognition,\n106(3):1126 – 1177.\n141\nRoger Levy. 2008b. A noisy-channel model of human sentence comprehension under uncertain inpu\nIn Proceedings of the 2008 Conference on Empirical Methods in Natural Language\nProcessing, pages 234–243, Honolulu, Hawaii. Association for Computational\nLinguistics.\nRoger Levy. 2011. Integrating surprisal and uncertain-input models in online sentence comprehensi\nIn Proceedings of the 49th Annual Meeting of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 1055–1065, Portland, Oregon, USA.\nAssociation for Computational Linguistics.\nDavid D. Lewis. 1998. Naive (bayes) at forty: The independence assumption in\ninformation retrieval.\nIn European conference on machine learning, pages 4–15.\nSpringer.\nDavid D. Lewis and William A. Gale. 1994. A sequential algorithm for training text\nclassiﬁers. In SIGIR’94, pages 3–12. Springer.\nCharles N. Li and Sandra A. Thompson. 1981. Mandarin Chinese: a Functional Ref-\nerence Grammar. University of California Press, Berkeley.\nChi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007.\nA probabilistic approach to syntax-based reordering for statistical machine translation.\nIn Proceedings of the 45th Annual Meeting of the Association of Computational Lin-\nguistics, pages 720–727, Prague, Czech Republic. Association for Computational\nLinguistics.\nXiaodong Li, Haoran Xie, Li Chen, Jianping Wang, and Xiaotie Deng. 2014.\n142\nNews impact on stock price return via sentiment analysis. Knowledge-Based Sys-\ntems, 69:14 – 23.\nXin Li and Dan Roth. 2002. Learning question classiﬁers. In COLING 2002: The\n19th International Conference on Computational Linguistics.\nChu-Cheng Lin, Hao Zhu, Matthew R. Gormley, and Jason Eisner. 2019.\nNeural ﬁnite-state transducers: Beyond rational relations. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\n272–283, Minneapolis, Minnesota. Association for Computational Linguistics.\nTal Linzen. 2020. How can we accelerate progress towards human-like linguistic\ngeneralization?\nIn Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, Seattle, Washington. Association for Computational\nLinguistics.\nTal\nLinzen,\nEmmanuel\nDupoux,\nand\nYoav\nGoldberg.\n2016.\nAssessing the ability of LSTMs to learn syntax-sensitive dependencies. Transac-\ntions of the Association for Computational Linguistics, 4:521–535.\nBing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan & Claypool Pub-\nlishers.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020.\nUnderstanding the diﬃculty of training transformers.\n143\nWeiguo Liu and Anthony Vannelli. 1995. Generating lower bounds for the linear\narrangement problem. Discrete applied mathematics, 59(2):137–151.\nYinhan Liu,\nMyle Ott,\nNaman Goyal,\nJingfei Du,\nMandar Joshi,\nDanqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019.\nRoberta: A robustly optimized BERT pretraining approach.\nCoRR,\nabs/1907.11692.\nBarbara Lohse, Barbara A. Hawkins, and Barbara John A Thomas Wasow. 2004.\nDomain minimization in english verb-particle constructions. Language, 80:238 –\n261.\nAdam Lopez. 2008.\nStatistical machine translation.\nACM Computing Surveys\n(CSUR), 40(3):1–49.\nHans Peter Luhn. 1957. A statistical approach to mechanized encoding and search-\ning of literary information. IBM Journal of Research and Development, 1(4):309–\n317.\nThang\nLuong,\nHieu\nPham,\nand\nChristopher\nD.\nManning.\n2015.\nEﬀective approaches to attention-based neural machine translation. In Proceed-\nings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npages 1412–1421, Lisbon, Portugal. Association for Computational Linguistics.\nFillia Makedon and Ivan Hal Sudborough. 1989. On minimizing width in linear\nlayouts. Discrete Applied Mathematics, 23(3):243–265.\n144\nChaitanya\nMalaviya,\nPedro\nFerreira,\nand\nAndré\nF.\nT.\nMartins.\n2018.\nSparse and constrained attention for neural machine translation.\nIn\nPro-\nceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 370–376, Melbourne, Australia. Association for\nComputational Linguistics.\nChristopher\nManning,\nMihai\nSurdeanu,\nJohn\nBauer,\nJenny\nFinkel,\nSteven\nBethard,\nand\nDavid\nMcClosky.\n2014.\nThe Stanford CoreNLP natural language processing toolkit.\nIn Proceedings\nof 52nd Annual Meeting of the Association for Computational Linguistics: System\nDemonstrations, pages 55–60, Baltimore, Maryland. Association for Computa-\ntional Linguistics.\nChristopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Intro-\nduction to Information Retrieval. Cambridge University Press, USA.\nChristopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natu-\nral Language Processing. MIT Press, Cambridge, MA, USA.\nAndre Martins and Ramon Astudillo. 2016. From softmax to sparsemax: A sparse model of attention\nIn Proceedings of The 33rd International Conference on Machine Learning, volume 48\nof Proceedings of Machine Learning Research, pages 1614–1623, New York, New\nYork, USA. PMLR.\nRebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language\n145\nProcessing, pages 1192–1202, Brussels, Belgium. Association for Computational\nLinguistics.\nAndrew McCallum, Dayne Freitag, and Fernando C. N. Pereira. 2000. Maximum\nentropy markov models for information extraction and segmentation. In Proceed-\nings of the Seventeenth International Conference on Machine Learning, ICML 2000,\npages 591–598, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\nAndrew McCallum and Kamal Nigam. 1998. A comparison of event models for\nnaive bayes text classiﬁcation. In AAAI 1998.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.\nLearned in translation: Contextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294–6305.\nJames\nL.\nMcClelland,\nFelix\nHill,\nMaja\nRudolph,\nJason\nBaldridge,\nand\nHinrich\nSchütze.\n2019.\nExtending machine language models toward human-level language understanding.\nTom\nMcCoy,\nEllie\nPavlick,\nand\nTal\nLinzen.\n2019.\nRight for the wrong reasons: Diagnosing syntactic heuristics in natural language inference.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Lin-\nguistics, pages 3428–3448, Florence, Italy. Association for Computational\nLinguistics.\nRyan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing al\n146\nIn 11th Conference of the European Chapter of the Association for Computational Lin-\nguistics, Trento, Italy. Association for Computational Linguistics.\nRyan McDonald,\nFernando Pereira,\nKiril Ribarov,\nand Jan Hajič. 2005a.\nNon-projective dependency parsing using spanning tree algorithms. In Proceed-\nings of Human Language Technology Conference and Conference on Empirical Meth-\nods in Natural Language Processing, pages 523–530, Vancouver, British Columbia,\nCanada. Association for Computational Linguistics.\nRyan McDonald,\nFernando Pereira,\nKiril Ribarov,\nand Jan Hajič. 2005b.\nNon-projective dependency parsing using spanning tree algorithms. In Proceed-\nings of Human Language Technology Conference and Conference on Empirical Meth-\nods in Natural Language Processing, pages 523–530, Vancouver, British Columbia,\nCanada. Association for Computational Linguistics.\nPratik\nMehta,\nAnoop\nKunchukuttan,\nand\nPushpak\nBhattacharyya.\n2015.\nInvestigating the potential of post-ordering SMT output to improve translation quality.\nIn Proceedings of the 12th International Conference on Natural Language Processing,\npages 351–356, Trivandrum, India. NLP Association of India.\nGábor Melis, Tomáš Kočiský, and Phil Blunsom. 2020. Mogriﬁer lstm. In Interna-\ntional Conference on Learning Representations.\nGonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. 2018.\nLearning latent permutations with gumbel-sinkhorn networks. In International\nConference on Learning Representations.\n147\nArthur Mensch and Mathieu Blondel. 2018. Diﬀerentiable dynamic programming for structured pre\nIn Proceedings of the 35th International Conference on Machine Learning, volume 80\nof Proceedings of Machine Learning Research, pages 3462–3471, Stockholmsmässan,\nStockholm Sweden. PMLR.\nWilliam Merrill. 2019. Sequential neural networks as automata. In Proceedings of\nthe Workshop on Deep Learning and Formal Languages: Building Bridges, pages 1–13,\nFlorence. Association for Computational Linguistics.\nWilliam Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, and\nEran Yahav. 2020. A formal hierarchy of rnn architectures. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, Seattle, Wash-\nington. Association for Computational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and JeﬀDean. 2013.\nDistributed representations of words and phrases and their compositionality.\nIn Advances in neural information processing systems, pages 3111–3119.\nGeorge A Miller. 1956. The magical number seven, plus or minus two: Some limits\non our capacity for processing information. Psychological review, 63(2):81.\nGeorge Armitage Miller. 1951. Language and communication.\nMike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision\nfor relation extraction without labeled data. In Proceedings of the Joint Conference\nof the 47th Annual Meeting of the ACL and the 4th International Joint Conference on\n148\nNatural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011.\nAssociation for Computational Linguistics.\nThomas M. Mitchell. 1997. Machine Learning, 1 edition. McGraw-Hill, Inc., USA.\nGraeme Mitchison and Richard Durbin. 1986. Optimal numberings of an n*n array.\nSIAM Journal on Algebraic Discrete Methods, 7(4):571–582.\nRuslan Mitkov and Aravind K. Joshi. 2012. Tree-adjoining grammars.\nBurkhard Monien. 1986. The bandwidth minimization problem for caterpillars\nwith hair length 3 is np-complete. SIAM Journal on Algebraic Discrete Methods,\n7(4):505–512.\nGereon Müller. 2002. Free word order, morphological case, and sympathy the-\nory. Resolving Conﬂicts in Grammars: Optimality Theory in Syntax, Morphology,\nand Phonology.\nPetra Mutzel. 1995. A polyhedral approach to planar augmentation and related\nproblems. In European Symposium on Algorithms, pages 494–507. Springer.\nRamesh\nNallapati,\nBowen\nZhou,\nCicero\ndos\nSan-\ntos,\nÇağlar\nG˙ulçehre,\nand\nBing\nXiang.\n2016.\nAbstractive text summarization using sequence-to-sequence RNNs and beyond.\nIn Proceedings of The 20th SIGNLL Conference on Computational Natural Language\nLearning, pages 280–290, Berlin, Germany. Association for Computational\nLinguistics.\n149\nRoberto Navigli and Simone Paolo Ponzetto. 2012. Babelnet: The automatic con-\nstruction, evaluation and application of a wide-coverage multilingual semantic\nnetwork. Artiﬁcial Intelligence, 193:217–250.\nPandu Nayak. 2019. Understanding searches better than ever before. Technical re-\nport, Google.\nGraham\nNeubig,\nTaro\nWatanabe,\nand\nShinsuke\nMori.\n2012.\nInducing a discriminative parser to optimize machine translation reordering.\nIn Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Language Learning, pages 843–853,\nJeju Island, Korea. Association for Computational Linguistics.\nDavid J. Newman and Sharon Block. 2006. Probabilistic topic decomposition of\nan eighteenth-century american newspaper. Journal of the American Society for\nInformation Science and Technology, 57(6):753–767.\nVlad Niculae. 2018. Learning Deep Models with Linguistically-Inspired Structure. Ph.D.\nthesis, Cornell University.\nVlad Niculae and Mathieu Blondel. 2017. A regularized framework for sparse and structured neural\nIn I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, editors, Advances in Neural Information Processing Systems 30,\npages 3338–3348. Curran Associates, Inc.\nJoakim Nivre. 2005. Dependency grammar and dependency parsing.\n150\nJoakim\nNivre,\nMarie-Catherine\nde\nMarneﬀe,\nFilip\nGinter,\nYoav\nGold-\nberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, Slav Petrov,\nSampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016.\nUniversal dependencies v1: A multilingual treebank collection.\nIn Proceed-\nings of the Tenth International Conference on Language Resources and Evaluation\n(LREC’16), pages 1659–1666, Portorož, Slovenia. European Language Resources\nAssociation (ELRA).\nJános Pach, Farhad Shahrokhi, and Mario Szegedy. 1996. Applications of the cross-\ning number. Algorithmica, 16(1):111–117.\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999.\nThe\npagerank citation ranking: Bringing order to the web. Technical report, Stan-\nford InfoLab.\nBo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summ\nIn Proceedings of the 42nd Annual Meeting of the Association for Computational\nLinguistics (ACL-04), pages 271–278, Barcelona, Spain.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorizatio\nIn Proceedings of the 43rd Annual Meeting of the Association for Computational Lin-\nguistics (ACL’05), pages 115–124, Ann Arbor, Michigan. Association for\nComputational Linguistics.\nBo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found.\nTrends Inf. Retr., 2(1–2):1–135.\n151\nBo\nPang,\nLillian\nLee,\nand\nShivakumar\nVaithyanathan.\n2002.\nThumbs up? sentiment classiﬁcation using machine learning techniques.\nIn\nProceedings of the 2002 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP 2002), pages 79–86. Association for Computational Linguistics.\nChristos H Papadimitriou. 1976. The np-completeness of the bandwidth minimiza-\ntion problem. Computing, 16(3):263–270.\nFuchun\nPeng,\nFangfang\nFeng,\nand\nAndrew\nMcCallum.\n2004.\nChinese segmentation and new word detection using conditional random ﬁelds.\nIn COLING 2004: Proceedings of the 20th International Conference on Computational\nLinguistics, pages 562–568, Geneva, Switzerland. COLING.\nHao\nPeng,\nRoy\nSchwartz,\nSam\nThomson,\nand\nNoah\nA.\nSmith.\n2018.\nRational recurrences. In Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1203–1214, Brussels, Belgium. Association\nfor Computational Linguistics.\nJeﬀrey\nPennington,\nRichard\nSocher,\nand\nChristopher\nManning.\n2014.\nGlove: Global vectors for word representation.\nIn Proceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages\n1532–1543, Doha, Qatar. Association for Computational Linguistics.\nFernando C. N. Pereira and David H. D. Warren. 1983. Parsing as deduction. In\n21st Annual Meeting of the Association for Computational Linguistics, pages 137–144,\nCambridge, Massachusetts, USA. Association for Computational Linguistics.\n152\nBen\nPeters,\nVlad\nNiculae,\nand\nAndré\nF.\nT.\nMartins.\n2018a.\nInterpretable structure induction via sparse attention.\nIn Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, pages 365–367, Brussels, Belgium. Association for Computational\nLinguistics.\nBen\nPeters,\nVlad\nNiculae,\nand\nAndré\nF.\nT.\nMartins.\n2019a.\nSparse sequence-to-sequence models. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, pages 1504–1519, Florence, Italy.\nAssociation for Computational Linguistics.\nMatthew\nPeters,\nMark\nNeumann,\nMohit\nIyyer,\nMatt\nGardner,\nChristopher\nClark,\nKenton\nLee,\nand\nLuke\nZettlemoyer.\n2018b.\nDeep contextualized word representations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers), pages 2227–2237, New\nOrleans, Louisiana. Association for Computational Linguistics.\nMatthew\nE.\nPeters,\nSebastian\nRuder,\nand\nNoah\nA.\nSmith.\n2019b.\nTo tune or not to tune? adapting pretrained representations to diverse tasks.\nIn Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-\n2019), pages 7–14, Florence, Italy. Association for Computational Linguistics.\nJordi Petit and Jordi Girona Salgado. 1998. Approximation heuristics and benchmark-\nings for the MinLA problem.\n153\nSteven Pinker. 2003. The language instinct: How the mind creates language. Penguin\nUK.\nSteven Pinker. 2007. The stuﬀof thought: Language as a window into human nature.\nPenguin.\nSteven Pinker and Paul Bloom. 1990. Natural language and natural selection. Be-\nhavioral and Brain Sciences, 13(4):707–727.\nSteven Pinker and Ray Jackendoﬀ. 2005. The faculty of language: what’s special about it?\nCognition, 95(2):201 – 236.\nDavid Pinto,\nAndrew McCallum,\nXing Wei,\nand W. Bruce Croft. 2003.\nTable extraction using conditional random ﬁelds. In Proceedings of the 26th An-\nnual International ACM SIGIR Conference on Research and Developmentin Informaion\nRetrieval, SIGIR 2003, pages 235–242, New York, NY, USA. Association for Com-\nputing Machinery.\nJakub Piskorski and Roman Yangarber. 2013. Information Extraction: Past, Present and Future,\npages 23–49. Springer Berlin Heidelberg, Berlin, Heidelberg.\nGrusha\nPrasad,\nMarten\nvan\nSchijndel,\nand\nTal\nLinzen.\n2019.\nUsing priming to uncover the organization of syntactic representations in neural language model\nIn Proceedings of the 23rd Conference on Computational Natural Language Learning\n(CoNLL), pages 66–76, Hong Kong, China. Association for Computational\nLinguistics.\n154\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C.\nLipton. 2020. Learning to deceive with attention-based explanations. In Proceed-\nings of the 58th Annual Meeting of the Association for Computational Linguistics, Seat-\ntle, Washington. Association for Computational Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning.\n2020. Stanza: A python natural language processing toolkit for many human\nlanguages. ArXiv, abs/2003.07082.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by generative pre-training.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners.\nColin\nRaﬀel,\nNoam\nShazeer,\nAdam\nRoberts,\nKatherine\nLee,\nSharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nMohammad\nSadegh\nRasooli\nand\nMichael\nCollins.\n2019.\nLow-resource syntactic transfer with unsupervised source reordering.\nIn\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 3845–3856, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n155\nAdwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging.\nIn Conference on Empirical Methods in Natural Language Processing.\nAdwait\nRatnaparkhi,\nJeﬀ\nReynar,\nand\nSalim\nRoukos.\n1994.\nA maximum entropy model for prepositional phrase attachment.\nIn\nHu-\nman Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey,\nMarch 8-11, 1994.\nShauli\nRavfogel,\nYoav\nGoldberg,\nand\nTal\nLinzen.\n2019.\nStudying the inductive biases of RNNs with synthetic variations of natural languages.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 3532–3542, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKumar Ravi and Vadlamani Ravi. 2015. A survey on opinion mining and sentiment analysis: Tasks, a\nKnowledge-Based Systems, 89:14 – 46.\nRamamurthy Ravi, Ajit Agrawal, and Philip Klein. 1991. Ordering problems ap-\nproximated: single-processor scheduling and interval graph completion. In In-\nternational Colloquium on Automata, Languages, and Programming, pages 751–762.\nSpringer.\nJeﬀrey\nC.\nReynar\nand\nAdwait\nRatnaparkhi.\n1997.\nA maximum entropy approach to identifying sentence boundaries.\nIn Fifth\nConference on Applied Natural Language Processing, pages 16–19, Washington, DC,\nUSA. Association for Computational Linguistics.\n156\nJan Rijkhoﬀ. 1990. Explaining word order in the noun phrase. Linguistics, 28(1):5\n– 42.\nBrian Roark,\nMurat Saraclar,\nMichael Collins,\nand Mark Johnson. 2004.\nDiscriminative language modeling with conditional random ﬁelds and the perceptron algorithm\nIn Proceedings of the 42nd Annual Meeting of the Association for Computational\nLinguistics (ACL-04), pages 47–54, Barcelona, Spain.\nJoseph Rocchio. 1971. Relevance feedback in information retrieval. The Smart re-\ntrieval system-experiments in automatic document processing, pages 313–323.\nSebastian Ruder. 2019.\nNeural Transfer Learning for Natural Language Processing.\nPh.D. thesis, National University of Ireland, Galway.\nDavid E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. 1986. Learning\nrepresentations by back-propagating errors. nature, 323(6088):533–536.\nAlexander\nM.\nRush,\nSumit\nChopra,\nand\nJason\nWeston.\n2015.\nA neural attention model for abstractive sentence summarization.\nIn Proceed-\nings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npages 379–389, Lisbon, Portugal. Association for Computational Linguistics.\nYousef Saad. 2003. Iterative Methods for Sparse Linear Systems, second edition. Soci-\nety for Industrial and Applied Mathematics.\nI.A. Sag, T. Wasow, and E.M. Bender. 2003. Syntactic Theory: A Formal Introduction.\nCSLI lecture notes. Center for the Study of Language and Information.\n157\nGerald Salton. 1971. The SMART Retrieval System — Experiments in Automatic Doc-\nument Processing. Prentice-Hall, Inc., USA.\nGerald\nSalton,\nA.\nWong,\nand\nC.\nS.\nYang.\n1975.\nA vector space model for automatic indexing.\nCommun. ACM, 18(11):613–\n620.\nGerard Salton. 1968. Automatic Information Organization and Retrieval. McGraw Hill\nText.\nGerard Salton. 1975. Theory of Indexing. Society for Industrial and Applied Mathe-\nmatics, USA.\nGerard Salton. 1991.\nDevelopments in automatic text retrieval.\nscience,\n253(5023):974–980.\nGerard Salton and Michael J McGill. 1986. Introduction to modern information\nretrieval.\nRodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. 2019.\nVisual permutation learning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 41(12):3100–3114.\nAaron Schein, John Paisley, David M. Blei, and Hanna Wallach. 2015. Bayesian pois-\nson tensor factorization for inferring multilateral relations from sparse dyadic\nevent counts. In Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 1045–1054.\n158\nAaron Schein, Zhiwei Steven Wu, Alexandra Schoﬁeld, Mingyuan Zhou, and\nHanna Wallach. 2019. Locally private Bayesian inference for count models. In\nProceedings of the 36th International Conference on Machine Learning, volume 97 of\nProceedings of Machine Learning Research, pages 5638–5648, Long Beach, Califor-\nnia, USA. PMLR.\nMarten van Schijndel and Tal Linzen. 2018. A neural model of adaptation in reading.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pages 4704–4710, Brussels, Belgium. Association for Computational\nLinguistics.\nMarten van Schijndel and Tal Linzen. 2019. Can entropy explain successor surprisal eﬀects in readin\nIn Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 1–7.\nMarten\nvan\nSchijndel,\nAaron\nMueller,\nand\nTal\nLinzen.\n2019.\nQuantity doesn’t buy quality syntax with neural language models.\nIn Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 5831–5837, Hong Kong, China. Association for Computational\nLinguistics.\nAlexandra Schoﬁeld. 2019. Text Processing for the Eﬀective Application of Latent Dirich-\nlet Allocation. Ph.D. thesis, Cornell University.\nMike Schuster and Kuldip K. Paliwal. 1997. Bidirectional recurrent neural networks.\nTrans. Sig. Proc., 45(11):2673–2681.\n159\nRoy\nSchwartz,\nSam\nThomson,\nand\nNoah\nA.\nSmith.\n2018.\nBridging CNNs, RNNs, and weighted ﬁnite-state machines.\nIn Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Vol-\nume 1:\nLong Papers), pages 295–305, Melbourne, Australia. Association for\nComputational Linguistics.\nSoﬁa Serrano and Noah A. Smith. 2019. Is attention interpretable? In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages\n2931–2951, Florence, Italy. Association for Computational Linguistics.\nFei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random ﬁelds.\nIn Proceedings of the 2003 Human Language Technology Conference of the North\nAmerican Chapter of the Association for Computational Linguistics, pages 213–220.\nClaude E. Shannon. 1948. A mathematical theory of communication. Bell system\ntechnical journal, 27(3):379–423.\nClaude\nE.\nShannon\nand\nWarren\nWeaver.\n1963.\nThe Mathematical Theory of Communication. pt. 11. University of Illinois Press.\nVatsal Sharan, Sham M. Kakade, Percy S. Liang, and Gregory Valiant. 2017.\nLearning overcomplete hmms. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wal-\nlach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30, pages 940–949. Curran Associates, Inc.\nVatsal Sharan, Sham M. Kakade, Percy S. Liang, and Gregory Valiant. 2018.\nPrediction with a short memory. In Proceedings of the 50th Annual ACM SIGACT\n160\nSymposium on Theory of Computing, STOC 2018, pages 1074–1087, New York, NY,\nUSA. Association for Computing Machinery.\nPeter\nShaw,\nJakob\nUszkoreit,\nand\nAshish\nVaswani.\n2018.\nSelf-attention with relative position representations.\nIn Proceedings of the\n2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computational Linguistics.\nTianze\nShi,\nLiang\nHuang,\nand\nLillian\nLee.\n2017a.\nFast(er) exact decoding and global training for transition-based dependency parsing via a minim\nIn Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pages 12–23, Copenhagen, Denmark. Association for Computational\nLinguistics.\nTianze Shi and Lillian Lee. 2018. Valency-augmented dependency parsing. In Pro-\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 1277–1291, Brussels, Belgium. Association for Computational Linguistics.\nTianze Shi and Lillian Lee. 2020. Extracting headless mwes from dependency parse\ntrees: Parsing, tagging, and joint modeling approaches. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, Seattle, Washing-\nton. Association for Computational Linguistics.\nTianze\nShi,\nFelix\nG.\nWu,\nXilun\nChen,\nand\nYao\nCheng.\n2017b.\nCombining global models for parsing universal dependencies.\nIn\nProceed-\nings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal\n161\nDependencies, pages 31–39, Vancouver, Canada. Association for Computational\nLinguistics.\nYossi Shiloach. 1979. A minimum linear arrangement algorithm for undirected\ntrees. SIAM Journal on Computing, 8(1):15–32.\nAmit Singhal. 2005. Challenges in running a commercial search engine. In SIGIR,\npage 432.\nAmit Singhal, Steve Abney, Michiel Bacchiani, Michael Collins, Donald Hindle,\nand Fernando Pereira. 2000. At&t at trec-8.\nRichard Sinkhorn. 1964. A relationship between arbitrary positive matrices and\ndoubly stochastic matrices. The annals of mathematical statistics, 35(2):876–879.\nRichard Sinkhorn and Paul Knopp. 1967. Concerning nonnegative matrices and\ndoubly stochastic matrices. Paciﬁc Journal of Mathematics, 21(2):343–348.\nNathaniel J. Smith and Roger Levy. 2013. The eﬀect of word predictability on reading time is logarith\nCognition, 128(3):302 – 319.\nNoah A. Smith and Jason Eisner. 2006. Annealing structural bias in multilingual weighted grammar\nIn Proceedings of the 21st International Conference on Computational Linguistics\nand 44th Annual Meeting of the Association for Computational Linguistics, pages\n569–576, Sydney, Australia. Association for Computational Linguistics.\nRichard\nSocher,\nAlex\nPerelygin,\nJean\nWu,\nJason\nChuang,\nChristo-\npher\nD.\nManning,\nAndrew\nNg,\nand\nChristopher\nPotts.\n2013a.\n162\nRecursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pages 1631–1642, Seattle, Washington, USA. Association for Compu-\ntational Linguistics.\nRichard\nSocher,\nAlex\nPerelygin,\nJean\nWu,\nJason\nChuang,\nChristo-\npher\nD.\nManning,\nAndrew\nNg,\nand\nChristopher\nPotts.\n2013b.\nRecursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pages 1631–1642, Seattle, Washington, USA. Association for Compu-\ntational Linguistics.\nKaren Spärck Jones. 1972. A statistical interpretation of term speciﬁcity and its\napplication in retrieval. Journal of Documentation, 60(5):493–502.\nNitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: a simple way to prevent neural networks from\noverﬁtting. The journal of machine learning research, 15(1):1929–1958.\nMark Steedman. 1987. Combinatory grammars and parasitic gaps. Natural language\nand linguistic theory.\nMark Steedman. 2004. The syntactic process. In Language, speech, and communica-\ntion.\nMark Steedman. 2008.\nOn becoming a discipline.\nComputational Linguistics,\n34(1):137–144.\n163\nJulia\nStrout,\nYe\nZhang,\nand\nRaymond\nMooney.\n2019.\nDo human rationales improve machine explanations?\nIn Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, pages 56–62, Florence, Italy. Association for Computational Linguistics.\nEmma\nStrubell,\nAnanya\nGanesh,\nand\nAndrew\nMcCallum.\n2019.\nEnergy and policy considerations for deep learning in NLP.\nIn Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages\n3645–3650, Florence, Italy. Association for Computational Linguistics.\nKatsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Na-\ngata. 2011. Post-ordering in statistical machine translation. In Proceedings of the\nMachine Translation Summit.\nIlya\nSutskever,\nOriol\nVinyals,\nand\nQuoc\nV\nLe.\n2014.\nSequence to sequence learning with neural networks.\nIn Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances\nin Neural Information Processing Systems 27, pages 3104–3112. Curran Associates,\nInc.\nCharles Sutton and Andrew McCallum. 2012. An introduction to conditional random ﬁelds.\nFoundations and Trends in Machine Learning, 4(4):267–373.\nCharles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007.\nDy-\nnamic conditional random ﬁelds: Factorized probabilistic models for labeling\nand segmenting sequence data. The journal of machine learning research, 8:693–\n723.\n164\nMirac Suzgun, Yonatan Belinkov, Stuart Shieber, and Sebastian Gehrmann. 2019a.\nLSTM networks can perform dynamic counting. In Proceedings of the Workshop\non Deep Learning and Formal Languages: Building Bridges, pages 44–54, Florence.\nAssociation for Computational Linguistics.\nMirac\nSuzgun,\nYonatan\nBelinkov,\nand\nStuart\nM.\nShieber.\n2019b.\nOn evaluating the generalization of LSTM models in formal languages.\nIn\nProceedings of the Society for Computation in Linguistics (SCiL) 2019, pages\n277–286.\nMichael K Tanenhaus and John C Trueswell. 1995. Sentence comprehension.\nBen Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher D. Man-\nning. 2004. Max-margin parsing. In Proceedings of the 2004 Conference on Empirical\nMethods in Natural Language Processing, pages 1–8, Barcelona, Spain. Association\nfor Computational Linguistics.\nDavid Temperleyand Daniel Gildea. 2018. Minimizing syntactic dependency lengths: Typological/c\nAnnual Review of Linguistics, 4(1):67–80.\nLucien Tesnière. 1959. Les éléments de syntaxe structurale.\nKristina\nToutanova,\nH.\nTolga\nIlhan,\nand\nChristopher\nManning.\n2002.\nExtentions to HMM-based statistical word alignment models.\nIn\nProceed-\nings of the 2002 Conference on Empirical Methods in Natural Language Processing\n(EMNLP 2002), pages 87–94. Association for Computational Linguistics.\n165\nKristina\nToutanova\nand\nChristopher\nD.\nManning.\n2000.\nEnriching the knowledge sources used in a maximum entropy part-of-speech tagger.\nIn 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Pro-\ncessing and Very Large Corpora, pages 63–70, Hong Kong, China. Association for\nComputational Linguistics.\nRoy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation.\nIn Proceedings of the 2009 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 1007–1016, Singapore. Association for Computational Linguistics.\nPeter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised clas\nIn Proceedings of the 40th Annual Meeting of the Association for Computational Lin-\nguistics, pages 417–424, Philadelphia, Pennsylvania, USA. Association for\nComputational Linguistics.\nWilliam E. Underwood. 2012. What can topic models of pmla teach us about the\nhistory of literary scholarship? Journal of Digital Humanities, 2(1).\nClara Vania, Yova Kementchedjhieva, Anders Søgaard, and Adam Lopez. 2019.\nA systematic comparison of methods for low-resource dependency parsing on genuinely low-res\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 1105–1116, Hong Kong, China. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n166\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In Advances in neural information processing systems, pages 5998–6008.\nNoortje\nJ.\nVenhuizen,\nMatthew\nW.\nCrocker,\nand\nHarm\nBrouwer.\n2019.\nExpectation-based comprehension: Modeling the interaction of world knowledge and linguistic e\nDiscourse Processes, 56(3):229–255.\nOriol\nVinyals,\nSamy\nBengio,\nand\nManjunath\nKudlur.\n2016.\nOrder matters: Sequence to sequence for sets.\nIn International Conference on\nLearning Representations.\nKarthik\nVisweswariah,\nRajakrishnan\nRajkumar,\nAnkur\nGandhe,\nAnanthakrishnan\nRamanathan,\nand\nJiri\nNavratil.\n2011.\nA word reordering model for improved machine translation.\nIn Proceedings\nof the 2011 Conference on Empirical Methods in Natural Language Processing, pages\n486–496, Edinburgh, Scotland, UK. Association for Computational Linguistics.\nEllen M. Voorhees and Donna K. Harman. 2000a. The eighth text retrieval confer-\nence (trec-8). Technical report.\nEllen M. Voorhees and Donna K. Harman. 2000b. The ninth text retrieval confer-\nence (trec-9). Technical report.\nEllen M. Voorhees and Donna K. Harman. 2001. The tenth text retrieval conference\n(trec-10). Technical report.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. Superglue: A\n167\nstickier benchmark for general-purpose language understanding systems. In\nAdvances in Neural Information Processing Systems, pages 3261–3275.\nAlex\nWang,\nAmanpreet\nSingh,\nJulian\nMichael,\nFe-\nlix\nHill,\nOmer\nLevy,\nand\nSamuel\nR.\nBowman.\n2019b.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\nIn International Conference on Learning Representations.\nBenyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and\nJakob Grue Simonsen. 2020. Encoding word order in complex embeddings. In\nInternational Conference on Learning Representations.\nBin Wang, Angela Wang, Fenxiao Chen, Yuncheng Wang, and C.-C. Jay Kuo.\n2019c. Evaluating word embedding models: methods and experimental results.\nAPSIPA Transactions on Signal and Information Processing, 8:e19.\nChao\nWang,\nMichael\nCollins,\nand\nPhilipp\nKoehn.\n2007.\nChinese syntactic reordering for statistical machine translation.\nIn Proceed-\nings of the 2007 Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning (EMNLP-CoNLL), pages\n737–745, Prague, Czech Republic. Association for Computational Linguistics.\nDingquan Wang and Jason Eisner. 2016. The galactic dependencies treebanks: Getting more data by\nTransactions of the Association for Computational Linguistics, 4:491–505.\nDingquan Wang and Jason Eisner. 2018. Synthetic data made to order: The case of parsing.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language\n168\nProcessing, pages 1325–1337, Brussels, Belgium. Association for Computational\nLinguistics.\nYuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss,\nNavdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc V.\nLe, Yannis Agiomyrgiannakis, Rob Clark, and Rif A. Saurous. 2017. Tacotron: To-\nwards end-to-end speech synthesis. In INTERSPEECH.\nWarren Weaver. 1949/55. Translation. In William N. Locke and A. Donald Boothe,\neditors, Machine Translation of Languages, pages 15–23. MIT Press, Cambridge,\nMA. Reprinted from a memorandum written by Weaver in 1949.\nE. M. Helen Weir. 1994. Nadëb. In Peter Kahrel and René van den Berg, editors,\nTypological Studies in Negation, pages 291–323. John Benjamins, Amsterdam.\nGail\nWeiss,\nYoav\nGoldberg,\nand\nEran\nYahav.\n2018.\nOn the practical computational power of ﬁnite precision RNNs for language recognition.\nIn Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2:\nShort Papers), pages 740–745, Melbourne, Australia.\nAssociation for Computational Linguistics.\nSarah Wiegreﬀe and Yuval Pinter. 2019. Attention is not not explanation. In Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 11–20, Hong Kong, China. Association for Computational Lin-\nguistics.\n169\nJohn\nWieting,\nMohit\nBansal,\nKevin\nGimpel,\nand\nKaren\nLivescu.\n2016.\nTowards universal paraphrastic sentence embeddings.\nIn International Con-\nference on Learning Representations.\nEthan\nWilcox,\nRoger\nLevy,\nand\nRichard\nFutrell.\n2019.\nHierarchical representation in neural language models: Suppression and recovery of expectation\nIn Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pages 181–190, Florence, Italy. Association for\nComputational Linguistics.\nEthan\nWilcox,\nRoger\nLevy,\nTakashi\nMorita,\nand\nRichard\nFutrell.\n2018.\nWhat do RNN language models learn about ﬁller–gap dependencies?\nIn\nProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 211–221, Brussels, Belgium. Association for\nComputational Linguistics.\nYorick Wilks. 1997. Information extraction as a core language technology. In Infor-\nmation Extraction A Multidisciplinary Approach to an Emerging Information Technol-\nogy, pages 1–9, Berlin, Heidelberg. Springer Berlin Heidelberg.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeﬀ\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, CliﬀYoung, Jason Smith, Jason Riesa, Alex\nRudnick, Oriol Vinyals, Gregory S. Corrado, MacduﬀHughes, and Jeﬀrey Dean.\n170\n2016. Google’s neural machine translation system: Bridging the gap between\nhuman and machine translation. ArXiv, abs/1609.08144.\nFei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rew\nIn COLING 2004: Proceedings of the 20th International Conference on Computational\nLinguistics, pages 508–514, Geneva, Switzerland. COLING.\nFrank Z. Xing, Erik Cambria, and Roy E. Welsch. 2017. Natural language based\nﬁnancial forecasting: a survey. Artiﬁcial Intelligence Review, 50:49–73.\nPeng\nXu,\nJaeho\nKang,\nMichael\nRinggaard,\nand\nFranz\nOch.\n2009.\nUsing a dependency parser to improve SMT for subject-object-verb languages.\nIn Proceedings of Human Language Technologies: The 2009 Annual Conference of\nthe North American Chapter of the Association for Computational Linguistics, pages\n245–253, Boulder, Colorado. Association for Computational Linguistics.\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon.\n2020. A theory of usable information under computational constraints. In Inter-\nnational Conference on Learning Representations.\nHiroko Yamashita and Franklin Chang. 2001. “long before short” preference in the production of a h\nCognition, 81(2):B45 – B55.\nYiming Yang. 1999. An evaluation of statistical approaches to text categorization.\nInformation retrieval, 1(1-2):69–90.\nYiming Yang and Xin Liu. 1999. A re-examination of text categorization methods.\n171\nIn Proceedings of the 22nd annual international ACM SIGIR conference on Research\nand development in information retrieval, pages 42–49.\nYiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection\nin text categorization. In Proceedings of the Fourteenth International Conference on\nMachine Learning, pages 412–420.\nZhilin\nYang,\nZihang\nDai,\nYiming\nYang,\nJaime\nCar-\nbonell,\nRuslan\nSalakhutdinov,\nand\nQuoc\nV\nLe.\n2019.\nXlnet: Generalized autoregressive pretraining for language understanding.\nIn Advances in Neural Information Processing Systems 33. Curran Associates, Inc.\nMihalis Yannakakis. 1985. A polynomial algorithm for the min-cut linear arrangement of trees.\nJournal ACM, 32(4):950–988.\nQiang\nYe,\nZiqiong\nZhang,\nand\nRob\nLaw.\n2009.\nSentiment classiﬁcation of online reviews to travel destinations by supervised machine learning a\nExpert Systems with Applications, 36(3, Part 2):6527 – 6535.\nDong Yu and Li Deng. 2014. Automatic Speech Recognition: A Deep Learning Approach.\nSpringer Publishing Company, Incorporated.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Man-\nning. 2017.\nPosition-aware attention and supervised data improve slot ﬁlling.\nIn Proceedings of the 2017 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 35–45, Copenhagen, Denmark. Association for Computational Lin-\nguistics.\n172\nVitalii\nZhelezniak,\nAleksandar\nSavkov,\nApril\nShen,\nFrancesco\nMoramarco,\nJack\nFlann,\nand\nNils\nY.\nHammerla.\n2019.\nDon’t settle for average, go for the max: Fuzzy sets and max-pooled word vectors.\nIn International Conference on Learning Representations.\n173\nAPPENDIX A\nREPRODUCIBILITY\nIn this appendix, we provide further details required to exactly reproduce this\nwork. Particularly signiﬁcant is that we release the code (§A.2) for running all ex-\nperiments and generating all tables/ﬁgures/visualizations used in this work. We\nadhere to the guidelines presented in Dodge et al. (2019), which were further ex-\ntended in the EMNLP 2020 reproducibility guidelines1, to provide strong and rig-\norous guarantees on the reproducibility of our work.\nA.1\nAdditional Experimental Details\nWe use Python 3.6.9 throughout this work along with PyTorch 1.5.0.\nTokenization and Dependency Parsing. Tokenization is done using the English\nen_core_web_lg model released in spaCy version 2.2.4. Dependency parsing\nis done using the same English en_core_web_lg model released in spaCy ver-\nsion 2.2.4. The model is 789MB and is trained on OntoNotes 5 using a multi-task2\nconvolutional neural network-based model with pretrained word embeddings ini-\ntialized using GloVe.3\n1https://2020.emnlp.org/call-for-papers\n2The other tasks are part-of-speech tagging and named entity recognition.\n3The GloVe embeddings are trained on Common Crawl data.\n174\nData Preprocessing. Beyond tokenizing the data, we do no further pre-processing\nexcept removing ill-formed examples in any datasets (where there is an input and\nno label or vice versa). We ﬁnd that there are 4 such examples in the CR dataset\nand none in any of the other four datasets.\nPretrained Representations. We use pretrained ELMo representations that are are\nobtained by using data tokenized using spaCy as described previously. The exact\npretrained ELMo encoders are available here4 and concatenate the representations\nfrom each of the two layers (yielding 2048-dimensional vectors).\nRandomness. We ﬁx the Python and PyTorch random seeds to be random seed 0.\nReverse Cuthill-McKee. We use the implementation of the algorithm provided in\nSciPy 1.4.1.\n4https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway\nﬁle name describes model parameters under the AllenNLP naming conventions.\n175\nA.2\nCode Release\nAll code for this work is made publicly available.\nThe code is hosted at\nhttps://github.com/rishibommasani/MastersThesis. We note that we\nprovide documentation for most core functionality and clariﬁications can be pro-\nvided upon request.\nA.3\nData Access\nAll data used in this work is publicly available. The copies of the datasets we use\nare available at https://github.com/harvardnlp/sent-conv-torch/tree/master/data\nvia the Harvard NLP group. The data can also be accessed from the corresponding\nwebsites for each of the datasets:\n• CR — https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#dataset\nHosted by Bing Liu.\n• SUBJ — https://www.cs.cornell.edu/people/pabo/movie-review-data/\nHosted by Lillian Lee.\n• SST-2 — https://nlp.stanford.edu/sentiment/\nHosted by Stanford NLP group.\n• SST-5 — https://nlp.stanford.edu/sentiment/\nHosted by Stanford NLP group.\n176\n• TREC — https://cogcomp.seas.upenn.edu/Data/QA/QC/\nHosted by Dan Roth and UPenn CogComp group.\nA.4\nContact Information\nQuestions, concerns, and errata should be directed to the thesis author at any of:\n• nlprishi@stanford.edu\n• rb724@cornell.edu\n• rishibommasani@gmail.com\nAny and all remaining errors in this thesis are strictly due to the author.\n177\nAPPENDIX B\nADDITIONAL RESULTS\nIn this appendix, we provide further results that were not included in the main\nbody of the thesis. We ﬁrst provide the results for all hyperparameters with the\nstopping condition we used in the main body of thesis: stopping after the ﬁxed\nthreshold of 12 epochs. These results appear in Tables B.1–B.6. For further com-\npleteness, we provide the results for all hyperparameters with the stopping con-\ndition after which we never saw any improvements (for all models, datasets, and\norders): stopping after the ﬁxed threshold of 15 epochs. These results appear in\nTables B.7–B.12.\n178\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.833\n0.95\n0.87\n0.464\n0.95\nrr\n0.823\n0.947\n0.87\n0.442\n0.944\nrb\n0.839\n0.941\n0.867\n0.452\n0.962\nrc\n0.852\n0.95\n0.873\n0.453\n0.952\nrm\n0.836\n0.946\n0.862\n0.456\n0.948\n˜rb\n0.839\n0.945\n0.879\n0.478\n0.954\n˜rc\n0.833\n0.952\n0.875\n0.464\n0.952\n˜rm\n0.841\n0.958\n0.876\n0.448\n0.954\nTable\nB.1:\nFull\nclassiﬁcation\nresults\nfor\nh\n=\n32, p\n=\n0.0.\nResults\nuse\npretrain-permute-finetune framework with the order speciﬁed in each row.\nAll other\nhyperparameters are set as described previously. The top part of the table refers to baselines.\nThe middle part of the table refers to orders derived from pure optimization algorithms. The\nbottom part of the table refers to orders derived from heuristic algorithms we introduce using\nTransposition Monte Carlo.\n179\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.833\n0.955\n0.882\n0.481\n0.956\nrr\n0.840\n0.945\n0.864\n0.458\n0.954\nrb\n0.854\n0.948\n0.865\n0.481\n0.958\nrc\n0.831\n0.942\n0.871\n0.478\n0.95\nrm\n0.831\n0.95\n0.864\n0.464\n0.958\n˜rb\n0.839\n0.946\n0.866\n0.46\n0.952\n˜rc\n0.849\n0.956\n0.873\n0.46\n0.958\n˜rm\n0.831\n0.949\n0.868\n0.457\n0.962\nTable\nB.2:\nFull\nclassiﬁcation\nresults\nfor\nh\n=\n64, p\n=\n0.02.\nResults\nuse\npretrain-permute-finetune framework with the order speciﬁed in each row.\nAll other\nhyperparameters are set as described previously. The top part of the table refers to baselines.\nThe middle part of the table refers to orders derived from pure optimization algorithms. The\nbottom part of the table refers to orders derived from heuristic algorithms we introduce using\nTransposition Monte Carlo.\n180\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.783\n0.95\n0.868\n0.477\n0.956\nrr\n0.823\n0.94\n0.876\n0.446\n0.952\nrb\n0.786\n0.937\n0.869\n0.471\n0.962\nrc\n0.852\n0.946\n0.861\n0.474\n0.948\nrm\n0.836\n0.918\n0.868\n0.456\n0.95\n˜rb\n0.841\n0.947\n0.864\n0.478\n0.956\n˜rc\n0.844\n0.946\n0.87\n0.47\n0.952\n˜rm\n0.825\n0.948\n0.872\n0.461\n0.96\nTable\nB.3:\nFull\nclassiﬁcation\nresults\nfor\nh\n=\n64, p\n=\n0.2.\nResults\nuse\npretrain-permute-finetune framework with the order speciﬁed in each row.\nAll other\nhyperparameters are set as described previously. The top part of the table refers to baselines.\nThe middle part of the table refers to orders derived from pure optimization algorithms. The\nbottom part of the table refers to orders derived from heuristic algorithms we introduce using\nTransposition Monte Carlo.\n181\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.852\n0.951\n0.884\n0.485\n0.946\nrr\n0.840\n0.95\n0.877\n0.476\n0.952\nrb\n0.841\n0.95\n0.873\n0.481\n0.956\nrc\n0.86\n0.953\n0.874\n0.481\n0.954\nrm\n0.836\n0.951\n0.874\n0.482\n0.962\n˜rb\n0.847\n0.947\n0.882\n0.469\n0.95\n˜rc\n0.847\n0.953\n0.871\n0.494\n0.962\n˜rm\n0.828\n0.951\n0.876\n0.467\n0.962\nTable\nB.4:\nFull\nclassiﬁcation\nresults\nfor\nh\n=\n128, p\n=\n0.02.\nResults\nuse\npretrain-permute-finetune framework with the order speciﬁed in each row.\nAll other\nhyperparameters are set as described previously. The top part of the table refers to baselines.\nThe middle part of the table refers to orders derived from pure optimization algorithms. The\nbottom part of the table refers to orders derived from heuristic algorithms we introduce using\nTransposition Monte Carlo.\n182\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.847\n0.945\n0.874\n0.455\n0.952\nrr\n0.839\n0.942\n0.851\n0.419\n0.952\nrb\n0.825\n0.948\n0.865\n0.445\n0.95\nrc\n0.852\n0.944\n0.867\n0.469\n0.954\nrm\n0.817\n0.942\n0.87\n0.441\n0.954\n˜rb\n0.831\n0.943\n0.874\n0.468\n0.948\n˜rc\n0.849\n0.948\n0.873\n0.462\n0.952\n˜rm\n0.844\n0.933\n0.863\n0.473\n0.958\nTable\nB.5:\nFull\nclassiﬁcation\nresults\nfor\nh\n=\n128, p\n=\n0.2.\nResults\nuse\npretrain-permute-finetune framework with the order speciﬁed in each row.\nAll other\nhyperparameters are set as described previously. The top part of the table refers to baselines.\nThe middle part of the table refers to orders derived from pure optimization algorithms. The\nbottom part of the table refers to orders derived from heuristic algorithms we introduce using\nTransposition Monte Carlo.\n183\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.836\n0.951\n0.896\n0.476\n0.962\nrr\n0.842\n0.934\n0.864\n0.451\n0.952\nrb\n0.825\n0.952\n0.87\n0.462\n0.966\nrc\n0.836\n0.946\n0.864\n0.462\n0.958\nrm\n0.841\n0.951\n0.859\n0.461\n0.96\n˜rb\n0.852\n0.949\n0.874\n0.461\n0.956\n˜rc\n0.825\n0.932\n0.874\n0.467\n0.968\n˜rm\n0.841\n0.941\n0.86\n0.476\n0.958\nTable\nB.6:\nFull\nclassiﬁcation\nresults\nfor\nh\n=\n256, p\n=\n0.2.\nResults\nuse\npretrain-permute-finetune framework with the order speciﬁed in each row.\nAll other\nhyperparameters are set as described previously. The top part of the table refers to baselines.\nThe middle part of the table refers to orders derived from pure optimization algorithms. The\nbottom part of the table refers to orders derived from heuristic algorithms we introduce using\nTransposition Monte Carlo.\n184\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.841\n0.948\n0.867\n0.462\n0.944\nrr\n0.825\n0.945\n0.87\n0.443\n0.942\nrb\n0.841\n0.94\n0.864\n0.45\n0.932\nrc\n0.844\n0.948\n0.87\n0.462\n0.948\nrm\n0.836\n0.946\n0.858\n0.457\n0.954\n˜rb\n0.833\n0.941\n0.877\n0.469\n0.952\n˜rc\n0.828\n0.951\n0.873\n0.466\n0.95\n˜rm\n0.844\n0.953\n0.877\n0.456\n0.948\nTable B.7: Full classiﬁcation results for h = 32, p = 0.0. Results are reported for models after\nthey were trained for 15 epochs. Results use pretrain-permute-finetune framework with\nthe order speciﬁed in each row. All other hyperparameters are set as described previously. The\ntop part of the table refers to baselines. The middle part of the table refers to orders derived from\npure optimization algorithms. The bottom part of the table refers to orders derived from heuristic\nalgorithms we introduce using Transposition Monte Carlo.\n185\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.833\n0.954\n0.882\n0.43\n0.956\nrr\n0.841\n0.945\n0.863\n0.459\n0.95\nrb\n0.852\n0.95\n0.871\n0.439\n0.956\nrc\n0.836\n0.944\n0.871\n0.468\n0.932\nrm\n0.831\n0.95\n0.862\n0.463\n0.958\n˜rb\n0.847\n0.948\n0.864\n0.459\n0.95\n˜rc\n0.847\n0.955\n0.873\n0.462\n0.958\n˜rm\n0.833\n0.949\n0.865\n0.461\n0.96\nTable B.8: Full classiﬁcation results for h = 64, p = 0.02. Results are reported for models after\nthey were trained for 15 epochs. Results use pretrain-permute-finetune framework with\nthe order speciﬁed in each row. All other hyperparameters are set as described previously. The\ntop part of the table refers to baselines. The middle part of the table refers to orders derived from\npure optimization algorithms. The bottom part of the table refers to orders derived from heuristic\nalgorithms we introduce using Transposition Monte Carlo.\n186\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.823\n0.951\n0.874\n0.458\n0.956\nrr\n0.82\n0.945\n0.861\n0.455\n0.954\nrb\n0.844\n0.947\n0.87\n0.455\n0.948\nrc\n0.847\n0.947\n0.871\n0.465\n0.964\nrm\n0.839\n0.938\n0.867\n0.461\n0.96\n˜rb\n0.849\n0.946\n0.855\n0.47\n0.958\n˜rc\n0.836\n0.957\n0.855\n0.47\n0.948\n˜rm\n0.825\n0.952\n0.874\n0.465\n0.958\nTable B.9: Full classiﬁcation results for h = 64, p = 0.2. Results are reported for models after\nthey were trained for 15 epochs. Results use pretrain-permute-finetune framework with\nthe order speciﬁed in each row. All other hyperparameters are set as described previously. The\ntop part of the table refers to baselines. The middle part of the table refers to orders derived from\npure optimization algorithms. The bottom part of the table refers to orders derived from heuristic\nalgorithms we introduce using Transposition Monte Carlo.\n187\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.852\n0.949\n0.883\n0.49\n0.966\nrr\n0.833\n0.949\n0.876\n0.466\n0.946\nrb\n0.844\n0.948\n0.873\n0.477\n0.944\nrc\n0.854\n0.953\n0.874\n0.474\n0.958\nrm\n0.839\n0.95\n0.87\n0.452\n0.916\n˜rb\n0.847\n0.947\n0.878\n0.422\n0.95\n˜rc\n0.849\n0.951\n0.873\n0.471\n0.96\n˜rm\n0.831\n0.952\n0.875\n0.451\n0.958\nTable B.10: Full classiﬁcation results for h = 128, p = 0.02. Results are reported for models after\nthey were trained for 15 epochs. Results use pretrain-permute-finetune framework with\nthe order speciﬁed in each row. All other hyperparameters are set as described previously. The\ntop part of the table refers to baselines. The middle part of the table refers to orders derived from\npure optimization algorithms. The bottom part of the table refers to orders derived from heuristic\nalgorithms we introduce using Transposition Monte Carlo.\n188\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.844\n0.944\n0.868\n0.467\n0.96\nrr\n0.852\n0.941\n0.869\n0.435\n0.948\nrb\n0.828\n0.945\n0.864\n0.448\n0.954\nrc\n0.857\n0.917\n0.87\n0.462\n0.96\nrm\n0.817\n0.938\n0.865\n0.461\n0.954\n˜rb\n0.82\n0.942\n0.871\n0.466\n0.944\n˜rc\n0.849\n0.953\n0.845\n0.482\n0.956\n˜rm\n0.847\n0.951\n0.868\n0.45\n0.958\nTable B.11: Full classiﬁcation results for h = 128, p = 0.2. Results are reported for models after\nthey were trained for 15 epochs. Results use pretrain-permute-finetune framework with\nthe order speciﬁed in each row. All other hyperparameters are set as described previously. The\ntop part of the table refers to baselines. The middle part of the table refers to orders derived from\npure optimization algorithms. The bottom part of the table refers to orders derived from heuristic\nalgorithms we introduce using Transposition Monte Carlo.\n189\nCR\nSUBJ\nSST-2\nSST-5\nTREC\nrI\n0.841\n0.953\n0.891\n0.471\n0.968\nrr\n0.844\n0.945\n0.873\n0.462\n0.948\nrb\n0.825\n0.955\n0.841\n0.455\n0.96\nrc\n0.839\n0.947\n0.871\n0.475\n0.952\nrm\n0.839\n0.953\n0.868\n0.483\n0.956\n˜rb\n0.849\n0.95\n0.861\n0.466\n0.956\n˜rc\n0.823\n0.947\n0.87\n0.474\n0.968\n˜rm\n0.839\n0.947\n0.87\n0.471\n0.956\nTable B.12: Full classiﬁcation results for h = 256, p = 0.2. Results are reported for models after\nthey were trained for 15 epochs. Results use pretrain-permute-finetune framework with\nthe order speciﬁed in each row. All other hyperparameters are set as described previously. The\ntop part of the table refers to baselines. The middle part of the table refers to orders derived from\npure optimization algorithms. The bottom part of the table refers to orders derived from heuristic\nalgorithms we introduce using Transposition Monte Carlo.\n190\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-08-13",
  "updated": "2021-08-13"
}