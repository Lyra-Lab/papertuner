{
  "id": "http://arxiv.org/abs/2108.11510v1",
  "title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
  "authors": [
    "Ngan Le",
    "Vidhiwar Singh Rathour",
    "Kashu Yamazaki",
    "Khoa Luu",
    "Marios Savvides"
  ],
  "abstract": "Deep reinforcement learning augments the reinforcement learning framework and\nutilizes the powerful representation of deep neural networks. Recent works have\ndemonstrated the remarkable successes of deep reinforcement learning in various\ndomains including finance, medicine, healthcare, video games, robotics, and\ncomputer vision. In this work, we provide a detailed review of recent and\nstate-of-the-art research advances of deep reinforcement learning in computer\nvision. We start with comprehending the theories of deep learning,\nreinforcement learning, and deep reinforcement learning. We then propose a\ncategorization of deep reinforcement learning methodologies and discuss their\nadvantages and limitations. In particular, we divide deep reinforcement\nlearning into seven main categories according to their applications in computer\nvision, i.e. (i)landmark localization (ii) object detection; (iii) object\ntracking; (iv) registration on both 2D image and 3D image volumetric data (v)\nimage segmentation; (vi) videos analysis; and (vii) other applications. Each of\nthese categories is further analyzed with reinforcement learning techniques,\nnetwork design, and performance. Moreover, we provide a comprehensive analysis\nof the existing publicly available datasets and examine source code\navailability. Finally, we present some open issues and discuss future research\ndirections on deep reinforcement learning in computer vision",
  "text": "Deep Reinforcement Learning in Computer Vision:\nA Comprehensive Survey\nNgan Le∗∗\nVidhiwar Singh Rathour∗\nKashu Yamazaki∗\nKhoa Luu\nMarios Savvides\nAugust 27, 2021\nAbstract\nDeep reinforcement learning augments the reinforcement learning framework and\nutilizes the powerful representation of deep neural networks. Recent works have demon-\nstrated the remarkable successes of deep reinforcement learning in various domains in-\ncluding ﬁnance, medicine, healthcare, video games, robotics, and computer vision. In\nthis work, we provide a detailed review of recent and state-of-the-art research advances\nof deep reinforcement learning in computer vision. We start with comprehending the\ntheories of deep learning, reinforcement learning, and deep reinforcement learning. We\nthen propose a categorization of deep reinforcement learning methodologies and discuss\ntheir advantages and limitations. In particular, we divide deep reinforcement learning\ninto seven main categories according to their applications in computer vision, i.e. (i)\nlandmark localization (ii) object detection; (iii) object tracking; (iv) registration on\nboth 2D image and 3D image volumetric data (v) image segmentation; (vi) videos\nanalysis; and (vii) other applications. Each of these categories is further analyzed with\nreinforcement learning techniques, network design, and performance. Moreover, we\nprovide a comprehensive analysis of the existing publicly available datasets and exam-\nine source code availability. Finally, we present some open issues and discuss future\nresearch directions on deep reinforcement learning in computer vision.\n1\nIntroduction\nReinforcement learning (RL) is a machine learning technique for learning a sequence of ac-\ntions in an interactive environment by trial and error that maximizes the expected reward\n[351]. Deep Reinforcement Learning (DRL) is the combination of Reinforcement Learning\nand Deep Learning (DL) and it has become one of the most intriguing areas of artiﬁcial intel-\nligence today. DRL can solve a wide range of complex real-world decision-making problems\nwith human-like intelligence that were previously intractable. DRL was selected by [316],\n[106] as one of ten breakthrough techniques in 2013 and 2017, respectively.\nThe past years have witnessed the rapid development of DRL thanks to its amazing\nachievement in solving challenging decision-making problems in the real world. DRL has\n1\narXiv:2108.11510v1  [cs.CV]  25 Aug 2021\nbeen successfully applied into many domains including games, robotics, autonomous driving,\nhealthcare, natural language processing, and computer vision.\nIn contrast to supervised\nlearning which requires large labeled training data, DRL samples training data from an\nenvironment. This opens up many machine learning applications where big labeled training\ndata does not exist.\nFar from supervised learning, DRL-based approaches focus on solving sequential decision-\nmaking problems. They aim at deciding, based on a set of experiences collected by interacting\nwith the environment, the sequence of actions in an uncertain environment to achieve some\ntargets. Diﬀerent from supervised learning where the feedback is available after each system\naction, it is simply a scalar value that may be delayed in time in the DRL framework. For\nexample, the success or failure of the entire system is reﬂected after a sequence of actions.\nFurthermore, the supervised learning model is updated based on the loss/error of the output\nand there is no mechanism to get the correct value when it is wrong. This is addressed by\npolicy gradients in DRL by assigning gradients without a diﬀerentiable loss function. This\naims at teaching a model to try things out randomly and learn to do correct things more.\nMany survey papers in the ﬁeld of DRL including [13] [97] [414] have been introduced\nrecently.\nWhile [13] covers central algorithms in DRL, [97] provides an introduction to\nDRL models, algorithms, and techniques, where particular focus is the aspects related to\ngeneralization and how DRL can be used for practical applications. Recently, [414] introduces\na survey, which discusses the broad applications of RL techniques in healthcare domains\nranging from dynamic treatment regimes in chronic diseases and critical care, an automated\nmedical diagnosis from both unstructured and structured clinical data, to many other control\nor scheduling domains that have inﬁltrated many aspects of a healthcare system. Diﬀerent\nfrom the previous work, our survey focuses on how to implement DRL in various computer\nvision applications such as landmark detection, object detection, object tracking, image\nregistration, image segmentation, and video analysis.\nOur goal is to provide our readers good knowledge about the principle of RL/DRL and\nthorough coverage of the latest examples of how DRL is used for solving computer vision\ntasks. We structure the rest of the paper as follows: we ﬁrst introduce fundamentals of\nDeep Learning (DL) in section 2 including Multi-Layer Perceptron (MLP), Autoencoder,\nDeep Belief Network, Convolutional Neural Networks (CNNs), Recurrent Neural Networks\n(RNNs). Then, we present the theories of RL in section 3, which starts with the Markov\nDecision Process (MDP) and continues with value function and Q-function. In the end of\nsection 3, we introduce various techniques in RL under two categories of model-based and\nmodel-free RL. Next, we introduce DRL in section 4 with main techniques in both value-\nbased methods, policy gradient methods, and actor-critic methods under model-based and\nmodel-free categories. The application of DRL in computer vision will then be introduced\nin sections 5, 6, 7, 8, 9, 10, 11 corresponding respectively to DRL in landmark detection,\nDRL in object detection, DRL in object tracking, DRL in image registration, DRL in im-\nage segmentation, DRL in video analysis and other applications of DRL. Each application\ncategory ﬁrst starts with a problem introduction and then state-of-the-art approaches in the\nﬁeld are discussed and compared through a summary table. We are going to discuss some\n2\nfuture perspectives in section 12 including challenges of DRL in computer vision and the\nrecent advanced techniques.\n2\nIntroduction to Deep Learning\n2.1\nMulti-Layer Perceptron (MLP)\nDeep learning models, in simple words, are large and deep artiﬁcial neural networks. Let us\nconsider the simplest possible neural network which is called ”neuron” as illustrated in Fig.\n1. A computational model of a single neuron is called a perceptron which consists of one or\nmore inputs, a processor, and a single output.\no\nx0\nx1\nx2\n+1\nFigure 1: An example of one neuron which takes input x = [x1, x2, x3], the intercept term\n+1 as bias, and the output o.\no\nx2\nx1\n+1\nx0\n+1\nLayer l0\nLayer l1\nLayer l2\nFigure 2: An example of multi-layer perceptron network (MLP)\nIn this example, the neuron is a computational unit that takes x = [x0, x1, x2] as input,\nthe intercept term +1 as bias b, and the output o. The goal of this simple network is to\nlearn a function f : RN →RM where N is the number of dimensions for input x and M is\nthe number of dimensions for output which is computed as o = f(x, θ), where θ is a set of\n3\nLatent Space\nLatent Space\nLatent Distribution\nInput Layer\nHidden Layer\nHidden Layer\nOutput Layer\nInput Layer\nHidden Layer\nOutput Layer\n(a)\n(b)\n(c)\n(d)\nRBM 1\nRBM 2\nRBM 3\nOutput\nFigure 3: An illustration of various DL architectures. (a): Autoencoder (AE); (b): Deep\nBelief Network; (c): Convolutional Neural Network (CNN); (d): Recurrent Neural Network\n(RNN).\nweights and are known as weights θ = {wi}. Mathematically, the output o of a one neuron\nis deﬁned as:\no = f(x, θ) = σ\n N\nX\ni=1\nwixi + b\n!\n= σ(WTx + b)\n(1)\nIn this equation, σ is the point-wise non-linear activation function. The common non-\nlinear activation functions for hidden units are hyperbolic tangent (Tanh), sigmoid, softmax,\nReLU, and LeakyReLU. A typical multi-layer perception (MLP) neural network is composed\nof one input layer, one output layer, and many hidden layers. Each layer may contain many\nunits. In this network, x is the input layer, o is the output layer. The middle layer is called\nthe hidden layer. In Fig. 2(b), MLP contains 3 units of the input layer, 3 units of the hidden\nlayer, and 1 unit of the output layer.\nIn general, we consider a MLP neural network with L hidden layers of units, one layer\nof input units and one layer of output units. The number of input units is N, output units\nis M, and units in hidden layer lth is N l. The weight of the jth unit in layer lth and the ith\nunit in layer (l + 1)th is denoted by wl\nij. The activation of the ith unit in layer lth is hl\ni.\n2.2\nAutoencoder\nAutoencoder is an unsupervised algorithm used for representation learning, such as feature\nselection or dimension reduction. A gentle introduction to Variational Autoencoder (VAE)\nis given in [11] and VAE framework is illustrated in Fig.3(a). In general, VAE aims to learn\na parametric latent variable model by maximizing the marginal log-likelihood of the training\ndata.\n2.3\nDeep Belief Network\nDeep Belief Network (DBN) and Deep Autoencoder are two common unsupervised ap-\nproaches that have been used to initialize the network instead of random initialization.\n4\nFigure 4: Architecture of a typical convolutional network for image classiﬁcation containing\nthree basic layers: convolution layer, pooling layer and fully connected layer\nWhile Deep Autoencoder is based on Autoencoder, Deep Belief Networks is based on Re-\nstricted Boltzmann Machine (RBM), which contains a layer of input data and a layer of\nhidden units that learn to represent features that capture high-order correlations in the data\nas illustrated in Fig.3(b).\n2.4\nConvolutional Neural Networks (CNN)\nConvolutional Neural Network (CNN) [204] [203] is a special case of fully connected MLP that\nimplements weight sharing for processing data. CNN uses the spatial correlation of the signal\nto utilize the architecture in a more sensible way. Their architecture, somewhat inspired by\nthe biological visual system, possesses two key properties that make them extremely useful\nfor image applications: spatially shared weights and spatial pooling. These kinds of networks\nlearn features that are shift-invariant, i.e., ﬁlters that are useful across the entire image (due\nto the fact that image statistics are stationary).\nThe pooling layers are responsible for\nreducing the sensitivity of the output to slight input shifts and distortions, and increasing\nthe reception ﬁeld for next layers.\nSince 2012, one of the most notable results in Deep\nLearning is the use of CNN to obtain a remarkable improvement in object recognition in\nImageNet classiﬁcation challenge [72] [187].\nA typical CNN is composed of multiple stages, as shown in Fig. 3(c). The output of each\nstage is made of a set of 2D arrays called feature maps. Each feature map is the outcome of\none convolutional (and an optional pooling) ﬁlter applied over the full image. A point-wise\nnon-linear activation function is applied after each convolution. In its more general form, a\nCNN can be written as\nh0 =x\nhl =pooll(σl(wlhl−1 + bl)), ∀l ∈1, 2, ...L\no =hL\n(2)\n5\nwhere wl, bl are trainable parameters as in MLPs at layer lth. x ∈Rc×h×w is vectorized from\nan input image with c being the color channels, h the image height and w the image width.\no ∈Rn×h′×w′ is vectorized from an array of dimension h′ × w′ of output vector (of dimension\nn). pooll is a (optional) pooling function at layer lth.\nCompared to traditional machine learning methods, CNN has achieved state-of-the-\nart performance in many domains including image understanding, video analysis and au-\ndio/speech recognition. In image understanding [404], [426], CNN outperforms human ca-\npacities [39]. Video analysis [422], [217] is another application that turns the CNN model\nfrom a detector [374] into a tracker [94]. As a special case of image segmentation [194], [193],\nsaliency detection is another computer vision application that uses CNN [381], [213]. In addi-\ntion to the previous applications, pose estimation [290], [362] is another interesting research\nthat uses CNN to estimate human-body pose. Action recognition in both still images and\nvideos is a special case of recognition and is a challenging problem. [110] utilizes CNN-based\nrepresentation of contextual information in which the most representative secondary region\nwithin a large number of object proposal regions, together with the contextual features,\nis used to describe the primary region. CNN-based action recognition in video sequences\nis reviewed in [420]. Text detection and recognition using CNN is the next step of optical\ncharacter recognition (OCR) [406] and word spotting [160]. Not only in computer vision,\nCNN has been successfully applied into other domains such as speech recognition and speech\nsynthesis [274], [283], biometrics [242], [85], [281], [350],[304], [261], biomedical [191], [342],\n[192], [411].\n2.5\nRecurrent Neural Networks (RNN)\nRNN is an extremely powerful sequence model and was introduced in the early 1990s [172].\nA typical RNN contains three parts, namely, sequential input data (xt), hidden state (ht)\nand sequential output data (yt) as shown in Fig. 3(d).\nRNN makes use of sequential information and performs the same task for every element\nof a sequence where the output is dependent on the previous computations. The activation\nof the hidden states at time-step t is computed as a function f of the current input symbol\nxt and the previous hidden states ht−1. The output at time t is calculated as a function g\nof the current hidden state ht as follows\nht = f(Uxt + Wht−1)\nyt = g(Vht)\n(3)\nwhere U is the input-to-hidden weight matrix, W is the state-to-state recurrent weight\nmatrix, V is the hidden-to-output weight matrix. f is usually a logistic sigmoid function or\na hyperbolic tangent function and g is deﬁned as a softmax function.\nMost works on RNN have made use of the method of backpropagation through time\n(BPTT) [318] to train the parameter set (U, V, W) and propagate error backward through\ntime. In classic backpropagation, the error or loss function is deﬁned as\nE(y’, y) =\nX\nt\n||y’t −yt||2\n(4)\n6\nwhere yt is the prediction and y’t is the labeled groundtruth.\nFor a speciﬁc weight W, the update rule for gradient descent is deﬁned as Wnew =\nW −γ ∂E\n∂W, where γ is the learning rate. In RNN model, the gradients of the error with\nrespect to our parameters U, V and W are learned using Stochastic Gradient Descent\n(SGD) and chain rule of diﬀerentiation.\nThe diﬃculty of training RNN to capture long-term dependencies has been studied in\n[26]. To address the issue of learning long-term dependencies, Hochreiter and Schmidhuber\n[139] proposed Long Short-Term Memory (LSTM), which can maintain a separate memory\ncell inside it that updates and exposes its content only when deemed necessary. Recently, a\nGated Recurrent Unit (GRU) was proposed by [51] to make each recurrent unit adaptively\ncapture dependencies of diﬀerent time scales. Like the LSTM unit, the GRU has gating units\nthat modulate the ﬂow of information inside the unit but without having separate memory\ncells.\nSeveral variants of RNN have been later introduced and successfully applied to wide\nvariety of tasks, such as natural language processing [257], [214], speech recognition [115],\n[54], machine translation [175], [241], question answering [138], image captioning [247], [78],\nand many more.\n3\nBasics of Reinforcement Learning\nThis section serves as a brief introduction to the theoretical models and techniques in RL. In\norder to provide a quick overview of what constitutes the main components of RL methods,\nsome fundamental concepts and major theoretical problems are also clariﬁed. RL is a kind\nof machine learning method where agents learn the optimal policy by trial and error. Unlike\nsupervised learning, the feedback is available after each system action, it is simply a scalar\nvalue that may be delayed in time in RL framework, for example, the success or failure of the\nentire system is reﬂected after a sequence of actions. Furthermore, the supervised learning\nmodel is updated based on the loss/error of the output and there is no mechanism to get\nthe correct value when it is wrong. This is addressed by policy gradients in RL by assigning\ngradients without a diﬀerentiable loss function which aims at teaching a model to try things\nout randomly and learn to do correct things more.\nInspired by behavioral psychology, RL was proposed to address the sequential decision-\nmaking problems which exist in many applications such as games, robotics, healthcare, smart\ngrids, stock, autonomous driving, etc. Unlike supervised learning where the data is given,\nan artiﬁcial agent collects experiences (data) by interacting with its environment in RL\nframework. Such experience is then gathered to optimize the cumulative rewards/utilities.\nIn this section, we focus on how the RL problem can be formalized as an agent that\ncan make decisions in an environment to optimize some objectives presented under reward\nfunctions. Some key aspects of RL are: (i) Address the sequential decision making; (ii) There\nis no supervisor, only a reward presented as scalar number; and (iii) The feedback is highly\ndelayed. Markov Decision Process (MDP) is a framework that has commonly been used to\nsolve most RL problems with discrete actions, thus we will ﬁrst discuss MDP in this section.\n7\nWe then introduce value function and how to categorize RL into model-based or model-free\nmethods. At the end of this section, we discuss some challenges in RL.\nEnvironment\nAction\nReward\nObservations\nFigure 5: An illustration of agent-environment interaction in RL\n3.1\nMarkov Decision Process\nThe standard theory of RL is deﬁned by a Markov Decision Process (MDP), which is an\nextension of the Markov process (also known as the Markov chain). Mathematically, the\nMarkov process is a discrete-time stochastic process whose conditional probability distribu-\ntion of the future states only depends upon the present state and it provides a framework to\nmodel decision-making situations. An MDP is typically deﬁned by ﬁve elements as follows:\n• S: a set of state or observation space of an environment. s0 is starting state.\n• A: set of actions the agent can choose.\n• T: a transition probability function T(st+1|st, at), specifying the probability that the\nenvironment will transition to state st+1 ∈S if the agent takes action at ∈A in state\nst ∈S.\n• R: a reward function where rt+1 = R(st, st+1) is a reward received for taking action at\nat state st and transfer to the next state st+1.\n• γ: a discount factor.\nConsidering MDP(S, A, γ, T, R), the agent chooses an action at according to the pol-\nicy π(at|st) at state st.\nNotably, agent’s algorithm for choosing action a given current\nstate s, which in general can be viewed as distribution π(a|s), is called a policy (strat-\negy). The environment receives the action, produces a reward rt+1 and transfers to the next\nstate st+1 according to the transition probability T(st+1|st, at). The process continues until\nthe agent reaches a terminal state or a maximum time step. In RL framework, the tuple\n(st, at, rt+1, st+1) is called transition. Several sequential transitions are usually referred to as\n8\nroll-out. Full sequence (s0, a0, r1, s1, a1, r2, ...) is called a trajectory. Theoretically, trajectory\nis inﬁnitely long, but the episodic property holds in most practical cases. One trajectory of\nsome ﬁnite length τ is called an episode. For given MDP and policy π, the probability of\nobserving (s0, a0, r1, s1, a1, r2, ...) is called trajectory distribution and is denoted as:\nTπ =\nY\nt\nπ(at|st)T(st+1|st, at)\n(5)\nThe objective of RL is to ﬁnd the optimal policy π∗for the agent that maximizes the cumu-\nlative reward, which is called return. For every episode, the return is deﬁned as the weighted\nsum of immediate rewards:\nR =\nτ−1\nX\nt=0\nγtrt+1\n(6)\nBecause the policy induces a trajectory distribution, the expected reward maximization can\nbe written as:\nETπ\nτ−1\nX\nt=0\nrt+1 →max\nπ\n(7)\nThus, given MDP and policy π, the discounted expected reward is deﬁned:\nG(π) = ETπ\nτ−1\nX\nt=0\nγtrt+1\n(8)\nThe goal of RL is to ﬁnd an optimal policy π∗, which maximizes the discounted expected\nreward, i.e. G(π) →maxπ.\n3.2\nValue and Q- functions\nThe value function is applied to evaluate how good it is for an agent to utilize policy π\nto visit state s. The concept of ”good” is deﬁned in terms of expected return, i.e. future\nrewards that can be expected to receive in the future and it depends on what actions it will\ntake. Mathematically, the value is the expectation of return, and value approximation is\nobtained by Bellman expectation equation as follows:\nV π(st) = E[rt+1 + γV π(st+1)]\n(9)\nV π(st) is also known as state-value function, and the expectation term can be expanded as\na product of policy, transition probability, and return as follows:\nV π(st) =\nX\nat∈A\nπ(at|st)\nX\nst+1∈S\nT(st+1|st, at)[R(st, st+1) + γV π(st+1)]\n(10)\nThis equation is called the Bellman equation. When the agent always selects the action\naccording to the optimal policy π∗that maximizes the value, the Bellman equation can be\n9\nexpressed as follows:\nV ∗(st) = max\nat\nX\nst+1∈S\nT(st+1|st, at)[R(st, st+1) + γV ∗(st+1)]\n∆= max\nat Q∗(st, at)\n(11)\nHowever, obtaining optimal value function V ∗does not provide enough information to re-\nconstruct some optimal policy π∗because the real-world environment is complicated. Thus,\na quality function (Q-function) is also called the action-value function under policy π. The\nQ-function is used to estimate how good it is for an agent to perform a particular action (at)\nin a state (st) with a policy π and it is introduced as:\nQπ(st, at) =\nX\nst+1\nT(st+1|st, at)[R(st, st+1) + γV π(st+1)]\n(12)\nUnlike value function which speciﬁes the goodness of a state, a Q-function speciﬁes the\ngoodness of action in a state.\n3.3\nCategory\nIn general, RL can be divided into either model-free or model-based methods. Here, ”model”\nis deﬁned by the two quantity: transition probability function T(st+1|st, at) and the reward\nfunction R(st, st+1).\n3.3.1\nModel-based RL\nModel-based RL is an approach that uses a learnt model, i.e.\nT(st+1|st, at) and reward\nfunction R(st, st+1) to predict the future action. There are four main model-based techniques\nas follows:\n• Value Function: The objective of value function methods is to obtain the best policy\nby maximizing the value functions in each state. A value function of a RL problem\ncan be deﬁned as in Eq.10 and the optimal state-value function is given in Eq.11\nwhich are known as Bellman equations. Some common approaches in this group are\nDiﬀerential Dynamic Programming [208], [266], Temporal Diﬀerence Learning [249],\nPolicy Iteration [334] and Monte Carlo [137].\n• Transition Models: Transition models decide how to map from a state s, taking\naction a to the next state (s’) and it strongly aﬀects the performance of model-based\nRL algorithms. Based on whether predicting the future state s’ is based on the proba-\nbility distribution of a random variable or not, there are two main approaches in this\ngroup: stochastic and deterministic. Some common methods for deterministic models\nare decision trees [280] and linear regression [265]. Some common methods for stochas-\ntic models are Gaussian processes [71], [1], [12], Expectation-Maximization [59] and\ndynamic Bayesian networks [280].\n10\n• Policy Search: Policy search approach directly searches for the optimal policy by\nmodifying its parameters, whereas the value function methods indirectly ﬁnd the ac-\ntions that maximize the value function at each state. Some of the popular approaches\nin this group are: gradient-based [87], [267], information theory [1], [189] and sampling\nbased [21].\n• Return Functions: Return functions decide how to aggregate rewards or punishments\nover an episode. They aﬀect both the convergence and the feasibility of the model.\nThere are two main approaches in this group: discounted returns functions [21], [75],\n[393] and averaged returns functions [34], [3]. Between the two approaches, the former\nis the most popular which represents the uncertainty about future rewards. While\nsmall discount factors provide faster convergence, its solution may not be optimal.\nIn practice, transition and reward functions are rarely known and hard to model.\nThe\ncomparative performance among all model-based techniques is reported in [385] with over 18\nbenchmarking environments including noisy environments. The Fig.6 summarizes diﬀerent\nmodel-based RL approaches.\n3.3.2\nModel-free methods\nLearning through the experience gained from interactions with the environment, i.e. model-\nfree method tries to estimate the t. discrete problems transition probability function and the\nreward function from the experience to exploit them in acquisition of policy. Policy gradient\nand value-based algorithms are popularly used in model-free methods.\n• The policy gradient methods: In this approach, RL task is considered as optimiza-\ntion with stochastic ﬁrst-order optimization. Policy gradient methods directly optimize\nthe discounted expected reward, i.e. G(π) →maxπ to obtains the optimal policy π∗\nwithout any additional information about MDP. To do so, approximate estimations of\nthe gradient with respect to policy parameters are used. Take [392] as an example,\npolicy gradient parameterizes the policy and updates parameters θ,\nGθ(π) = ETφ\nX\nt=0\nlog(πθ(at|st))γtR\n(13)\nwhere R is the total accumulated return and deﬁned in Eq. 6. Common used policies\nare Gibbs policies [20], [352] and Gaussian policies [294]. Gibbs policies are used in\ndiscrete problems whereas Gaussian policies are used in continuous problems.\n• Value-based methods: In this approach, the optimal policy π∗is implicitly con-\nducted by gaining an approximation of optimal Q-function Q∗(s, a). In value-based\nmethods, agents update the value function to learn suitable policy while policy-based\nRL agents learn the policy directly. To do that, Q-learning is a typical value-based\nmethod. The update rule of Q-learning with learning rate λ is deﬁned as:\nQ(st, at) = Q(st, at) + λδt\n(14)\n11\nTable 1: Comparison between model-based RL and model-free RL\nFactors\nModel-based RL\nModel-free RL\nNumber of iterations between\nagent and environment\nSmall\nBig\nConvergence\nFast\nSlow\nPrior knowledge of transitions\nYes\nNo\nFlexibility\nStrongly depend on\na learnt model\nAdjust based\non trials and errors\nwhere δt = R(st, st+1)+γarg maxa Q(st+1, a) −Q(st, a) is the temporal diﬀerence (TD)\nerror.\nTarget at self-play Chess, [394] investigates inasmuch it is possible to leverage the\nqualitative feedback for learning an evaluation function for the game. [319] provides\nthe comparison of learning of linear evaluation functions between using preference\nlearning and using least-squares temporal diﬀerence learning, from samples of game\ntrajectories. The value-based methods depend on a speciﬁc, optimal policy, thus it is\nhard for transfer learning.\n• Actor-critic is an improvement of policy gradient with an value-based critic Γ, thus,\nEq.13 is rewritten as:\nGθ(π) = ETφ\nX\nt=0\nlog(πθ(at|st))γtΓt\n(15)\nThe critic function Γ can be deﬁned as Qπ(st, at) or Qπ(st, at) −V π\nt\nor R[st−1, st] +\nV π\nt+1 −V π\nt\nActor-critic methods are combinations of actor-only methods and critic-only methods. Thus,\nactor-critic methods have been commonly used RL. Depend on reward setting, there are two\ngroups of actor-critic methods, namely discounted return [282], [30] and average return [289],\n[31]. The comparison between model-based and model-free methods is given in Table 1.\n4\nIntroduction to Deep Reinforcement Learning\nDRL, which was proposed as a combination of RL and DL, has achieved rapid developments,\nthanks to the rich context representation of DL. Under DRL, the aforementioned value and\npolicy can be expressed by neural networks which allow dealing with a continuous state or\naction that was hard for a table representation. Similar to RL, DRL can be categorized into\nmodel-based algorithms and model-free algorithms which will be introduced in this section.\n4.1\nModel-Free Algorithms\nThere are two approaches, namely, Value-based DRL methods and Policy gradient DRL\nmethods to implement model-free algorithms.\n12\nModel-based RL\nValue Functions\nDiﬀerential Dynamic Programming [208], [266]\nTemporal Diﬀerence Learning [249]\nPolicy Iteration [334]\nMonte Carlo [137]\nTransition Models\nDeterministic models\nDecision trees [280]\nLinear regression [265]\nStochastic models\nGaussian processes [71], [1], [12]\nExpectation-Maximization [59]\nDynamic Bayesian networks [280]\nPolicy Search\nGradient-based [87], [267]\nInformation theory [1], [189]\nSampling based [21]\nReturn Functions\nDiscounted returns functions [21], [75], [393]\nAveraged returns functions [34], [3]\nFigure 6: Summarization of model-based RL approaches\n13\n4.1.1\nValue-based DRL methods\nDeep Q-Learning Network (DQN): Deep Q-learning [264] (DQN) is the most famous\nDRL model which learns policies directly from high-dimensional inputs by CNNs. In DQN,\ninput is raw pixels and output is a quality function to estimate future rewards as given in\nFig.7. Take regression problem as an instance. Let y denote the target of our regression\ntask, the regression with input (s, a), target y(s, a) and the MSE loss function is as:\nLDQN = L(y(st, at), Q∗(st, at, θt))\n= ||y(st, at) −Q∗(st, at, θt)||2\ny(st, at) = R(st, st+1) + γ max\nat+1 Q∗(st1, at+1, θt)\n(16)\nWhere θ is vector of parameters, θ ∈R|S||R| and st+1 is a sample from T(st+1|st, at) with\ninput of (st, at).\nMinimizing the loss function yields a gradient descent step formula to update θ as follows:\nθt+1 = θt −αt\n∂LDQN\n∂θ\n(17)\nFigure 7: Network structure of Deep Q-Network (DQN), where Q-values Q(s,a) are generated\nfor all actions for a given state.\nDouble DQN: In DQN, the values of Q∗in many domains were leading to overestimation\nbecause of max. In Eq.16, y(s, a) = R(s, s′) + γ maxa′ Q∗(s′, a′, θ) shifts Q-value estima-\ntion towards either to the actions with high reward or to the actions with overestimating\napproximation error. Double DQN [370] is an improvement of DQN that combines double\nQ-learning [130] with DQN and it aims at reducing observed overestimation with better\n14\nperformance. The idea of Double DQN is based on separating action selection and action\nevaluation using its own approximation of Q∗as follows:\nmax\nat+1 Q∗(st+1, at+1; θ) = Q∗(st+1, arg max\nat+1\nQ∗(st+1, at+1; θ1); θ2)\n(18)\nThus\ny = R(st, st+1) + γQ∗(st+1, arg max\nat+1\nQ∗(st+1, at+1; θ1); θ2)\n(19)\nThe easiest and most expensive implementation of double DQN is to run two independent\nDQNs as follows:\ny1 = R(st, st+1)+\nγQ∗\n1(st+1, arg max\nat+1\nQ∗\n2(st+1, at+1; θ2); θ1)\ny2 = R(st, st+1)+\nγQ∗\n2(st+1, arg max\nat+1\nQ∗\n1(st+1, at+1; θ1); θ2)\n(20)\nDueling DQN: In DQN, when the agent visits an unfavorable state, instead of lowering its\nvalue V ∗, it remembers only low pay-oﬀby updating Q∗. In order to address this limitation,\nDueling DQN [390] incorporates approximation of V ∗explicitly in a computational graph\nby introducing an advantage function as follows:\nAπ(st, at) = Qπ(st, at) −V π(st)\n(21)\nTherefore, we can reformulate Q-value: Q∗(s, a) = A∗(s, a) + V ∗(s). This implies that after\nDL the feature map is decomposed into two parts corresponding to V ∗(v) and A∗(s, a) as\nillustrated in Fig.8.\nThis can be implemented by splitting the fully connected layers in\nthe DQN architecture to compute the advantage and state value functions separately, then\ncombining them back into a single Q-function. An interesting result has shown that Dueling\nDQN obtains better performance if it is formulated as:\nQ∗(st, at) = V ∗(st) + A∗(st, at) −max\nat+1 A∗(st, at+1)\n(22)\nIn practical implementation, averaging instead of maximum is used, i.e.\nQ∗(st, at) = V ∗(st) + A∗(st, at) −meanat+1A∗(st, at+1)\nFurthermore, to address the limitation of memory and imperfect information at each decision\npoint, Deep Recurrent Q-Network (DRQN) [131] employed RNNs into DQN by replacing\nthe ﬁrst fully-connected layer with an RNN. Multi-step DQN [68] is one of the most popular\nimprovements of DQN by substituting one-step approximation with N-steps.\n15\nFigure 8: Network structure of Dueling DQN, where value function V (s) and advantage\nfunction A(s, a) are combined to predict Q-values Q(s, a) for all actions for a given state.\n4.1.2\nPolicy gradient DRL methods\nPolicy Gradient Theorem: Diﬀerent from value-based DRL methods, policy gradient\nDRL optimizes the policy directly by optimizing the following objective function which is\ndeﬁned as a function of θ.\nG(θ) = ET ∼πθ\nX\nt=1\nγt−1R(st−1, st) →max\nθ\n(23)\nFor any MDP and diﬀerentiable policy πθ, the gradient of objective Eq.23 is deﬁned by policy\ngradient theorem [353] as follows:\n▽θ G(θ) = ET ∼πθ\nX\nt=0\nγtQπ(st, at) ▽θ logπθ(at|st)\n(24)\nREINFORCE: REINFORCE was introduced by [392] to approximately calculate the gra-\ndient in Eq.24 by using Monte-Carlo estimation. In REINFORCE approximate estimator,\nEq.24 is reformulated as:\n▽θ G(θ) ≈\nN\nX\nT\nX\nt=0\nγt ▽θ logπθ(at|st)(\nX\nt′=t\nγt′−tR(st′, st′+1))\n(25)\nwhere T is trajectory distribution and deﬁned in Eq.5. Theoretically, REINFORCE can be\nstraightforwardly applied into any parametric πtheta(a|s). However, it is impractical to use\nbecause of its time-consuming nature for convergence and local optimums problem. Based\non the observation that the convergence rate of stochastic gradient descent directly depends\n16\non the variance of gradient estimation, the variance reduction technique was proposed to\naddress naive REINFORCE’s limitations by adding a term that reduces the variance without\naﬀecting the expectation.\n4.1.3\nActor-Critic DRL algorithm\nBoth value-based and policy gradient algorithms have their own pros and cons, i.e. policy\ngradient methods are better for continuous and stochastic environments, and have a faster\nconvergence whereas, value-based methods are more sample eﬃcient and steady. Lately,\nactor-critic [182] [262] was born to take advantage from both value-based and policy gradient\nwhile limiting their drawbacks. Actor-critic architecture computes the policy gradient using\na value-based critic function to estimate expected future reward. The principal idea of actor-\ncritic is to divide the model into two parts: (i) computing an action based on a state and (ii)\nproducing the Q values of the action. As given in Fig.9, the actor takes as input the state\nst and outputs the best action at. It essentially controls how the agent behaves by learning\nthe optimal policy (policy-based). The critic, on the other hand, evaluates the action by\ncomputing the value function (value-based). The most basic actor-critic method (beyond the\ntabular case) is naive policy gradients (REINFORCE). The relationship between actor-critic\nis similar to kid-mom. The kid (actor) explores the environment around him/her with new\nactions i.e. tough ﬁre, hit a wall, climb a tree, etc while the mom (critic) watches the kid\nand criticizes/compliments him/her. The kid then adjusts his/her behavior based on what\nhis/her mom told. When the kids get older, he/she can realize which action is bad/good.\nFigure 9: Flowchart showing the structure of actor critic algorithm.\nAdvantage Actor-Critic (A2C) Advantage Actor-Critic (A2C) [263] consist of two neural\nnetworks i.e.\nactor network πθ(at|st) representing for policy and critic network V π\nω with\nparameters ω approximately estimating actor’s performance.\nIn order to determine how\nmuch better, it is to take a speciﬁc action compared to the average, an advantage value is\n17\nFigure 10: An illustration of Actor-Critic algorithm in two cases: sharing parameters (a)\nand not sharing parameters (b).\ndeﬁned as:\nAπ(st, at) = Qπ(st, at) −V π(st)\n(26)\nInstead of constructing two neural networks for both the Q value and the V value, using the\nBellman optimization equation, we can rewrite the advantage function as:\nAπ(st, at) = R(st, st+1) + γV π\nω (st+1) −V π\nω (st)\n(27)\nFor given policy π, its value function can be obtained using point iteration for solving:\nV π(st) = Eat∼π(at|st)Est+1∼T(st+1|at,st)(R(st, st+1) + γV π(st+1))\n(28)\nSimilar to DQN, on each update a target is computed using current approximation:\ny = R(st, st+1) + γV π\nω (st+1)\n(29)\nAt time step t, the A2C algorithm can be implemented as following steps:\n• Step 1: Compute advantage function using Eq.27.\n• Step 2: Compute target using Eq.29.\n• Step 3: Compute critic loss with MSE loss: L =\n1\nB\nP\nT ||y −V π(st))||2, where B is\nbatch size and V π(st) is deﬁned in Eq.28.\n18\n• Step 4: Compute critic gradient: ▽critic = ∂L\n∂ω.\n• Step 5: Compute actor gradient: ▽actor = 1\nB\nP\nT ▽θlogπ(at|st)Aπ(st, at)\nAsynchronous Advantage Actor Critic (A3C) Besides A2C, there is another strategy\nto implement an Actor-Critic agent.\nAsynchronous Advantage Actor-Critic (A3C) [263]\napproach does not use experience replay because this requires a lot of memory. Instead, A3C\nasynchronously executes diﬀerent agents in parallel on multiple instances of the environment.\nEach worker (copy of the network) will update the global network asynchronously. Because\nof the asynchronous nature of A3C, some workers (copy of the agents) will work with older\nvalues of the parameters. Thus the aggregating update will not be optimal. On the other\nhand, A2C synchronously updates the global network. A2C waits until all workers ﬁnished\ntheir training and calculated their gradients to average them, to update the global network.\nIn order to update the entire network, A2C waits for each actor to ﬁnish their segment of\nexperience before updating the global parameters. As a consequence, the training will be\nmore cohesive and faster. Diﬀerent from A3C, each worker in A2C has the same set of\nweights since and A2C updates all their workers at the same time. In short, A2C is an\nalternative to the synchronous version of the A3C. In A2C, it waits for each actor to ﬁnish\nits segment of experience before updating, averaging over all of the actors. In a practical\nexperiment, this implementation is more eﬀectively uses GPUs due to larger batch sizes. The\nstructure of an actor-critic algorithm can be divided into two types depending on parameter\nsharing as illustrated in Fig.10.\nIn order to overcome the limitation of speed, GA3C [16] was proposed and it achieved\na signiﬁcant speedup compared to the original CPU implementation. To more eﬀectively\ntrain A3C, [141] proposed FFE which forces random exploration at the right time during a\ntraining episode, that can lead to improved training performance.\n4.2\nModel-Based Algorithms\nWe have discussed so far model-free methods including the value-based approach and pol-\nicy gradient approach. In this section, we focus on the model-based approach, that deals\nwith the dynamics of the environment by learning a transition model that allows for sim-\nulation of the environment without interacting with the environment directly. In contrast\nto model-free approaches, model-based approaches are learned from experience by a func-\ntion approximation. Theoretically, no speciﬁc prior knowledge is required in model-based\nRL/DRL but incorporating prior knowledge can help faster convergence and better-trained\nmodel, speed up training time as well as the number of training samples. While using raw\ndata with pixel, it is diﬃcult for model-based RL to work on high dimensional and dynamic\nenvironments. This is addressed in DRL by embedding the high-dimensional observations\ninto a lower-dimensional space using autoencoders [95]. Many DRL approaches have been\nbased on scaling up prior work in RL to high-dimensional problems. A good overview of\nmodel-based RL for high-dimensional problems can be found in [297] which partition model-\nbased DRL into three categories: explicit planning on given transitions, explicit planning on\n19\nlearned transitions, and end-to-end learning of both planning and transitions. In general,\nDRL targets training DNNs to approximate the optimal policy π∗together with optimal\nvalue functions V ∗and Q∗. In the following, we will cover the most common model-based\nDRL approaches including value function and policy search methods.\n4.2.1\nValue function\nWe start this category with DQN [264] which has been successfully applied to classic Atari\nand illustrated in Fig.7. DQN uses CNNs to deal with high dimensional state space like\npixels, to approximate the Q-value function.\nMonte Carlo tree search (MCTS) MCTS [62] is one of the most popular methods to\nlook-ahead search and it is combined with a DNN-based transition model to build a model-\nbased DRL in [9]. In this work, the learned transition model predicts the next frame and\nthe rewards one step ahead using the input of the last four frames of the agent’s ﬁrst-person-\nview image and the current action. This model is then used by the Monte Carlo tree search\nalgorithm to plan the best sequence of actions for the agent to perform.\nValue-Targeted Regression (UCRL-VTR) Alex, et al. proposed model-based DRL for\nregret minimization [167]. In their work, a set of models, that are ‘consistent’ with the data\ncollected, is constructed at each episode. The consistency is deﬁned as the total squared\nerror, whereas the value function is determined by solving the optimistic planning problem\nwith the constructed set of models\n4.2.2\nPolicy search\nPolicy search methods aim to directly ﬁnd policies by means of gradient-free or gradient-\nbased methods.\nModel-Ensemble Trust-Region Policy Optimization (ME-TRPO) ME-TRPO [190]\nis mainly based on Trust Region Policy Optimization (TRPO) [327] which imposes a trust\nregion constraint on the policy to further stabilize learning.\nModel-Based Meta-Policy-Optimization (MB-MPO) MB-MPO [58] addresses the\nperformance limitation of model-based DRL compared against model-free DRL when learn-\ning dynamics models. MB-MPO learns an ensemble of dynamics models, a policy that can\nquickly adapt to any model in the ensemble with one policy gradient step. As a result, the\nlearned policy exhibits less model bias without the need to behave conservatively.\nA summary of both model-based and model-free DRL algorithms is given in Table 2.\nIn this Table, we also categorized DRL techniques into either on-policy or oﬀ-policy. In\non-policy RL, it allows the use of older samples (collected using the older policies) in the\ncalculation. The policy πk is updated with data collected by πk itself. In oﬀ-policy RL, the\ndata is assumed to be composed of diﬀerent policies π0, π0, ..., πk. Each policy has its own\ndata collection, then the data collected from π0, π1, ..., πk is used to train πk+1.\n20\nTable 2: Summary of model-based and model-free DRL algorithms consisting of value-based\nand policy gradient methods.\nDRL Algorithms\nDescription\nCategory\nDQN [264]\nDeep Q Network\nValue-based\nOﬀ-policy\nDouble DQN [370]\nDouble Deep Q Network\nValue-based\nOﬀ-policy\nDueling DQN [390]\nDueling Deep Q Network\nValue-based\nOﬀ-policy\nMCTS [9]\nMonte Carlo tree search\nValue-based\nOn-policy\nUCRL-VTR[167]\noptimistic planning problem\nValue-based\nOﬀ-policy\nDDPG [223]\nDQN with Deterministic Policy Gradient\nPolicy gradient\nOﬀ-policy\nTRPO [327]\nTrust Region Policy Optimization\nPolicy gradient\nOn-policy\nPPO [328]\nProximal Policy Optimization\nPolicy gradient\nOn-policy\nME-TRPO [190]\nModel-Ensemble Trust-Region Policy Optimization\nPolicy gradient\nOn-policy\nMB-MPO [58]\nModel-Based Meta- Policy-Optimization\nPolicy gradient\nOn-policy\nA3C [263]\nAsynchronous Advantage Actor Critic\nActor Critic\nOn-Policy\nA2C [263]\nAdvantage Actor Critic\nActor Critic\nOn-Policy\n21\n4.3\nGood practices\nInspired by Deep Q-learning [264], we discuss some useful techniques that are used during\ntraining an agent in DRL framework in practices.\nExperience replay Experience replay [417] is a useful part of oﬀ-policy learning and is\noften used while training an agent in RL framework. By getting rid of as much information\nas possible from past experiences, it removes the correlations in training data and reduces\nthe oscillation of the learning procedure. As a result, it enables agents to remember and\nre-use past experiences sometimes in many weights updates which increases data eﬃciency.\nMinibatch learning Minibatch learning is a common technique that is used together with\nexperience replay. Minibatch allows learning more than one training sample at each step,\nthus, it makes the learning process robust to outliers and noise.\nTarget Q-network freezing As described in [264], two networks are used for the training\nprocess.\nIn target Q-network freezing: one network interacts with the environment and\nanother network plays the role of a target network. The ﬁrst network is used to generate\ntarget Q-values that are used to calculate losses. The weights of the second network i.e.\ntarget network are ﬁxed and slowly updated to the ﬁrst network [224].\nReward clipping A reward is the scalar number provided by the environment and it aims\nat optimizing the network. To keep the rewards in a reasonable scale and to ensure proper\nlearning, they are clipped to a speciﬁc range (-1 ,1). Here 1 refers to as positive reinforcement\nor reward and -1 is referred to as negative reinforcement or punishment.\nModel-based v.s.\nmodel-free approach Whether the model-free or model-based ap-\nproaches is chosen mainly depends on the model architecture i.e. policy and value function.\n5\nDRL in Landmark Detection\nAutonomous landmark detection has gained more and more attention in the past few years.\nOne of the main reasons for this increased inclination is the rise of automation for evaluating\ndata. The motivation behind using an algorithm for landmarking instead of a person is that\nmanual annotation is a time-consuming tedious task and is prone to errors. Many eﬀorts\nhave been made for the automation of this task. Most of the works that were published for\nthis task using a machine learning algorithm to solve the problem. [64] proposed a regression\nforest-based method for detecting landmark in a full-body CT scan. Although the method\nwas fast it was less accurate when dealing with large organs. [101] extended the work of [64]\nby adding statistical shape priors that were derived from segmentation masks with cascade\nregression.\nIn order to address the limitations of previous works on anatomy detection, [105] re-\nformulated the detection problem as a behavior learning task for an artiﬁcial agent using\nMDP. By using the capabilities of DRL and scale-space theory [226], the optimal search\nstrategies for ﬁnding anatomical structures are learned based on the image information at\nmultiple scales. In their approach, the search starts at the coarsest scale level for capturing\nglobal context and continues to ﬁner scales for capturing more local information. In their\n22\nRL conﬁguration, the state of the agent at time t, st = I(⃗pt) is deﬁned as an axis-aligned\nbox of image intensities extracted from the image I and centered at the voxel-position ⃗pt\nin image space. An action at allows the agent to move from any voxel position ⃗pt to an\nadjacent voxel position ⃗pt+1. The reward function represents distance-based feedback, which\nis positive if the agent gets closer to the target structure and negative otherwise. In this\nwork, a CNN is used to extract deep semantic features. The search starts with the coarsest\nscale level M −1, the algorithm tries to maximize the reward which is the change in distance\nbetween ground truth and predicted landmark location before and after the action of moving\nthe scale window across the image. Upon convergence, the scale level is changed to M −2\nand the search continued from the convergence point at scale level M −1. The process is\nrepeated on the following scales until convergence on the ﬁnest scale. The authors performed\nexperiments on 3D CT scans and obtained an average accuracy increase of 20-30% and lower\ndistance error than the other techniques such as SADNN [104] and 3D-DL [427]\nFocus on anatomical landmark localization in 3D fetal US images, [10] proposed and\ndemonstrated use cases of several diﬀerent Deep Q-Network RL models to train agents that\ncan precisely localize target landmarks in medical scans. In their work, they formulate the\nlandmark detection problem as an MDP of a goal-oriented agent, where an artiﬁcial agent\nis learned to make a sequence of decisions towards the target point of interest. At each time\nstep, the agent should decide which direction it has to move to ﬁnd the target landmark.\nThese sequential actions form a learned policy forming a path between the starting point\nand the target landmark. This sequential decision-making process is approximated under\nRL. In this RL conﬁguration, the environment is deﬁned as a 3D input image, action A is a\nset of six actions ax+, ax−, ay+, ay−, az+, az−corresponding to three directions, the state\ns is deﬁned as a 3D region of interest (ROI) centered around the target landmark and the\nreward is chosen as the diﬀerence between the two Euclidean distances: the previous step\nand current step. This reward signiﬁes whether the agent is moving closer to or further away\nfrom the desired target location. In this work, they also proposed a novel ﬁxed- and multi-\nscale optimal path search strategy with hierarchical action steps for agent-based landmark\nlocalization frameworks.\nWhereas pure policy or value-based methods have been widely used to solve RL-based\nlocalization problems, [7] adopts an actor-critic [262] based direct policy search method\nframed in a temporal diﬀerence learning approach. In their work, the state is deﬁned as\na function of the agent-position which allows the agent at any position to observe an m ×\nm × 3 block of surrounding voxels.\nSimilar to other previous work, the action space is\nax+, ax−, ay+, ay−, az+, az−.\nThe reward is chosen as a simple binary reward function,\nwhere a positive reward is given if an action leads the agent closer to the target landmark,\nand a negative reward is given otherwise. Far apart from the previous work, their approach\nproposes a non-linear policy function approximator represented by an MLP whereas the value\nfunction approximator is presented by another MLP stacked on top of the same CNN from\nthe policy net. Both policy (actor) and value (critic) networks are updated by actor-critic\nlearning. To improve the learning, they introduce a partial policy-based RL to enable solving\nthe large problem of localization by learning the optimal policy on smaller partial domains.\n23\nThe objective of the partial policy is to obtain multiple simple policies on the projections\nof the actual action space, where the projected policies can reconstruct the policy on the\noriginal action space.\nBased on the hypothesis that the position of all anatomical landmarks is interdependent\nand non-random within the human anatomy and this is necessary as the localization of dif-\nferent landmarks requires learning partly heterogeneous policies, [377] concluded that one\nlandmark can help to deduce the location of others. For collective gain, the agents share\ntheir accumulated knowledge during training. In their approach, the state is deﬁned as RoI\ncentered around the location of the agent. The reward function is deﬁned as the relative\nimprovement in Euclidean distance between their location at time t and the target land-\nmark location. Each agent is considered as Partially Observable Markov Decision Process\n(POMDP) [107] and calculates its individual reward as their policies are disjoint. In or-\nder to reduce the computational load in locating multiple landmarks and increase accuracy\nthrough anatomical interdependence, they propose a collaborative multi-agent landmark de-\ntection framework (Collab-DQN) where DQN is built upon a CNN. The backbone CNN is\nshared across all agents while the policy-making fully connected layers are separate for each\nagent.\nTable 3: Comparing various DRL-based landmark detection meth-\nods. The ﬁrst group on Single Landmark Detection (SLD) and the\nsecond group for Multiple Landmark Detection (MLD)\nApproaches Year\nTraining\nTech-\nnique\nActions\nRemarks\nPerformance\nDatasets\nand\nsource code\nSLD [105]\n2017\nDQN\n6 action:\n2 per axis\nState:\nan\naxis-\naligned\nbox\ncen-\ntered at the voxel-\nposition.\nAction:\nmove\nfrom\n⃗pt\nto\n⃗pt+1.\nReward:\ndistance-based\nfeedback\nAverage accuracy in-\ncrease 20-30%. Lower\ndistance\nerror\nthan\nother techniques such\nas SADNN [104] and\n3D-DL [427]\n3D CT Scan\nSLD [10]\n2019\nDQN,\nDDQN,\nDuel\nDQN\nand\nDuel\nDDQN\n6 action:\n2 per axis\nEnvironment:\n3D\ninput image. State:\n3D\nRoI\ncentered\naround\nthe\ntarget\nlandmark.\nRe-\nward:\nEuclidean\ndistance\nbetween\npredicted points and\ngroundtruth points.\nDuel DQN performs\nthe\nbest\non\nRight\nCerebellum\n(FS),\nLeft Cerebellum (FS,\nMS) Duel DDQN is\nthe\nbest\non\nRight\nCerebellum\n(MS)\nDQN\nperforms\nthe\nbest on Cavum Sep-\ntum\nPellucidum(FS,\nMS)\nFetal head, ul-\ntrasound\nscans\n[219].\nAvailable code\n24\nSLD [7]\n2019\nActor-\nCritic\n-based\nPartial\n-Policy\nRL\n6 action:\n2 per axis\nState: a function of\nthe\nagent-position.\nReward:\nbinary\nreward\nfunction.\npolicy\nfunction:\nMLP.\nvalue\nfunc-\ntion: MLP\nFaster\nand\nbetter\nconvergence,\nout-\nperforms than other\nconventional\nactor-\ncritic and Q-learning\nCT\nvolumes:\nAortic\nvalve.\nCT\nvolumes:\nLAA\nseed-\npoint.\nMR\nimages:\nVer-\ntebra\ncenters\n[42].\nMLD\n[377]\n2019\nCollab\nDQN\n6 action:\n2 per axis\nState:\nRoI centred\naround\nthe\nagent.\nReward:\nrelative\nimprovement\nin\nEuclidean distance.\nEach\nAgent\nis\na\nPOMDP\nhas\nits\nown reward. Collab-\nDQN:\nreduce\nthe\ncomputational load\nColab DQN got bet-\nter results than su-\npervised\nCNN\nand\nDQN\nBrain\nMRI\nlandmark [158],\nCardiac\nMRI\nlandmark\n[70],\nFetal\nbrain\nlandmark [10].\nAvailable code\nMLD\n[161]\n2020\nDQN\n6 action 2\nper axis\nState:\n3D\nimage\npatch. Reward: Eu-\nclidean distance and\n∈[−1, 1]. Backbone\nCNN is share among\nagents\nEach\nagent\nhas it own Fully con-\nnected layer\nDetection\nerror\nin-\ncreased as the degree\nof\nmissing\ninforma-\ntion increased Perfor-\nmance is aﬀected by\nthe choice of land-\nmarks\n3D\nHead\nMR\nimages\nDiﬀerent from the previous works on RL-based landmark detection, which detect a single\nlandmark,[161] proposed a multiple landmark detection approach to better time-eﬃcient and\nmore robust to missing data. In their approach, each landmark is guided by one agent. The\nMDP is models as follows: The state is deﬁned as a 3D image patch. The reward, clipped in\n[-1, +1], is deﬁned as the diﬀerence in the Euclidean distance between the landmark predicted\nin the previous time step and the target, and in the landmark predicted in the current time\nstep and the target. The action space is deﬁned as in other previous works i.e. there are 6\nactions ax+, ax−, ay+, ay−, az+, az−in the action space. To enable the agents to share the\ninformation learned by detecting one landmark for use in detecting other landmarks, hard\nparameter sharing from multi-task learning is used. In this work, the backbone network is\nshared among agents and each agent has its own fully connected layer.\nTable 3 summarizes and compares all approaches for DRL in landmark detection, and\na basic implementation of landmark detection using DRL has been shown in Fig. 11. The\nﬁgure illustrates a general implementation of landmark detection with the help of DRL, where\nthe state is the Region of interest (ROI) around the current landmark location cropped from\nthe image, The actions performed by the DRL agent are responsible for shifting the ROI\nacross the image forming a new state and the reward corresponds to the improvement in\n25\neuclidean distance between ground truth and predicted landmark location with iterations as\nused by [105],[7],[10],[377],[161].\nFigure 11: DRL implementation for landmark detection, The red point corresponds to the\ncurrent landmark location and Red box is the Region of Interest (ROI) centered around the\nlandmark, the actions of DRL agent shift the ROI across the image to maximize the re-\nward corresponding to the improvement in distance between the ground truth and predicted\nlandmark location.\n6\nDRL in Object Detection\nObject detection is a task that requires the algorithm to ﬁnd bounding boxes for all objects\nin a given image. Many attempts have been made towards object detection. A method for\nbounding box prediction for object detection was proposed by [109], in which the task was\nperformed by extracting region proposals from an image and then feeding each of them to\na CNN to classify each region. An improvement to this technique was proposed by [108],\nwhere they used the feature from the CNN to propose region proposals instead of the image\nitself, this resulted in fast detection. Further improvement was proposed by [309], where the\nauthors proposed using a region proposal network (RPN) to identify the region of interest,\nresulting in much faster detection. Other attempts including focal loss [225] and Fast YOLO\n[332] have been proposed to address the imbalanced data problem in object detection with\nfocal loss [225], and perform object detection in video on embedded devices in a real-time\nmanner [332].\nConsidering MDP as the framework for solving the problem, [43] used DRL for active\nobject localization. The authors considered 8 diﬀerent actions (up, down, left, right, bigger,\nsmaller, fatter, taller) to improve the ﬁt of the bounding box around the object and additional\naction to trigger the goal state. They used a tuple of feature vector and history of actions\nfor state and change in IOU across actions as a reward.\n26\nAn improvement to [43] was proposed by [25], where the authors used a hierarchical\napproach for object detection by treating the problem of object detection as an MDP. In\ntheir method, the agent was responsible to ﬁnd a region of interest in the image and then\nreducing the region of interest to ﬁnd smaller regions from the previously selected region and\nhence forming a hierarchy. For the reward function, they used the change in Intersection over\nunion (IOU) across the actions and used DQN as the agent. As described in their paper, two\nnetworks namely, Image-zooms and Pool45-crops with VGG-16 [340] backbone were used to\nextract the feature information that formed the state for DQN along with a memory vector\nof the last four actions.\nUsing a sequential search strategy, [251] proposed a method for object detection using\nDRL. The authors trained the model with a set of image regions where at each time step\nthe agent returned ﬁxate actions that speciﬁed a location in image for actor to explore next\nand the terminal state was speciﬁed by done action. The state consisted of a tuple three\nelements: the observed region history Ht, selected evidence region history Et and ﬁxate\nhistory Ft. The fixate action was also a tuple of three elements: fixate action, index of\nevidence region et and image coordinate of next ﬁxate zt. The done action consisted of: done\naction, index of region representing the detected output bt and the detection conﬁdence ct.\nThe authors deﬁned the reward function that was sensitive to the detection location, the\nconﬁdence at the ﬁnal state and incurs a penalty for each region evaluation.\nTo map the inter-dependencies among the diﬀerent objects, [170] proposed a tree-structured\nRL agent (Tree-RL) for object localization by considering the problem as an MDP. The au-\nthors in their implementation considered actions of two types: translation and scaling, where\nthe scaling consisted of ﬁve actions whereas translation consisted of eight actions. In the\nspeciﬁed work, the authors used the state as a concatenation of the feature vector of the\ncurrent window, feature vector of the whole image, and history of taken actions. The feature\nvector were extracted from an ImageNet [72] [320] trained VGG-16 [340] model and for re-\nward the change in IOU across an action was used. Tree-RL utilized a top-down tress search\nstarting from the whole image where each window recursively takes the best action from\neach action group which further gives two new windows. This process is repeated recursively\nto ﬁnd the object.\nThe task of breast lesion detection is a challenging yet very important task in the medical\nimaging ﬁeld. A DRL method for active lesion detection in the breast was proposed by [246],\nwhere the authors formulated the problem as an MDP. In their formulation, a total of nine\nactions consisting of 6 translation actions, 2 scaling actions, and 1 trigger action were used.\nIn the speciﬁed work, the change in dice coeﬃcient across an action was used as the reward for\nscaling and translation actions, and for trigger action, the reward was +η for dice coeﬃcient\ngreater than rw and −η otherwise, where η and rw were the hyperparameters chosen by the\nauthors. For network structure, ResNet [133] was used as the backbone and DQN as the\nagent.\nDiﬀerent from the previous methods, [386] proposed a method for multitask learning using\nDRL for object localization. The authors considered the problem as an MDP where the agent\nwas responsible to perform a series of transformations on the bounding box using a series\n27\nof actions. Utilizing an RL framework the states consisted of feature vector and historical\nactions concatenated together, and a total of 8 actions for Bounding box transformation\n(left, right, up, down, bigger, smaller, fatter, and taller) were used. For reward the authors\nused the change in IOU between actions, the reward being 0 for an increase in IOU and -1\notherwise. For terminal action, however, the reward was 8 for IOU greater than 0.5 and -8\notherwise. The authors in the paper used DQN with multitask learning for localization and\ndivided terminal action and 8 transformation actions into two networks and trained them\ntogether.\nAn improvement for the Region proposal networks that greedily select the ROIs was\nproposed by [295], where they used RL for the task.\nThe authors in this paper used a\ntwo-stage detector similar to Fast and Faster R-CNN But used RL for the decision-making\nProcess. For the reward, they used the normalized change in Intersection over Union (IOU).\nInstead of learning a policy from a large set of data, [15] proposed a method for bounding\nbox reﬁnement (BAR) using RL. In the paper, once the authors have an inaccurate bounding\nbox that is predicted by some algorithm they use the BAR algorithm to predict a series of\nactions for reﬁnement of a bounding box. They considered a total of 8 actions (up, down,\nleft, right, wider, taller, fatter, thinner) for bounding box transformation and considered\nthe problem as a sequential decision-making problem (SDMP). They proposed an oﬄine\nmethod called BAR-DRL and an online method called BAR-CB where training is done on\nevery image. In BAR-DRL the authors trained a DQN over the states which consisted of\nfeatures extracted from ResNet50 [133] [354] pretrained on ImageNet [72] [320] and a history\nvector of 10 actions. The Reward for BAR-DRL was 1 if the IOU increase after action and -3\notherwise. For BAR-CB they adapted the LinUCB [216] algorithm for an episodic scenario\nand considered The Histogram of Oriented Gradients (HOG) for the state to capture the\noutline and edges of the object of interest. The actions in the online method (BAR-CB) were\nthe same as the oﬄine method and the reward was 1 for increasing IOU and 0 otherwise.\nFor both the implementations, the authors considered β as terminal IOU.\nAn improvement to sequential search strategy by [251] was proposed by [367], where\nthey used a framework consisting of two modules, Coarse and ﬁne level search. According to\nthe authors, this method is eﬃcient for object detection in large images (dimensions larger\nthan 3000 pixels). The authors ﬁrst performed a course level search on a large image to\nﬁnd a set of patches that are used by ﬁne level search to ﬁnd sub-patches. Both ﬁne and\ncoarse levels were conducted using a two-step episodic MDP, where The policy network\nwas responsible for returning the probability distribution of all actions. In the paper, the\nauthors considered the actions to be the binary action array (0,1) where 1 means that the\nagent would consider acquiring sub-patches for that particular patch. The authors in their\nimplementation considered a number of patches and sub-patches as 16 and 4 respectively\nand used the linear combination of Racc (detection recall) and Rcost which combines image\nacquisition cost and run-time performance reward.\nTable 4: Comparing various DRL-based object detection methods\n28\nApproaches\nYear Training\nTech-\nnique\nActions\nRemarks\nBackbone Performance\nDatasets and\nsource code\nActive\nObject\nLocal-\nization\n[43]\n2015 DQN\n8\nac-\ntions: up,\ndown,\nleft,\nright,\nbigger,\nsmaller,\nfatter,\ntaller\nStates:\nfeature\nvector\nof\nob-\nserved\nregion\nand\naction\nhis-\ntory.\nReward:\nChange in IOU.\n5 layer\npre-\ntrained\nCNN\nHigher\nmAP\nas compared to\nmethods that did\nnot\nuse\nregion\nproposals\nlike\nMultiBox\n[89],\nRegionLets [433],\nDetNet\n[356],\nand second best\nmAP\nas\ncom-\npared to R-CNN\n[109]\nPascal VOC-\n2007\n[90],\n2012 [91] Im-\nage Dataset.\nHierarchical\nObject\nDetection\n[25]\n2016 DQN\n5 actions:\n1\naction\nper image\nquarter\nand\n1\nat\nthe\ncenter\nStates:\ncurrent\nregion and mem-\nory vector using\nImage-zooms\nand\nPool45-\ncrops.\nReward:\nchange in IOU.\nVGG-\n16\n[340]\nObjects detected\nwith\nvery\nfew\nregion proposals\nper image\nPascal VOC-\n2007\nImage\nDataset [90].\nAvailable Code\nVisual Ob-\nject Detec-\ntion [251]\n2016 Policy\nsam-\npling\nand\nstate\ntran-\nsition\nalgo-\nrithm\n2 actions:\nﬁxate\nand done,\nwhere\neach is a\ntuple\nof\nthree.\nStates: Observed\nregion\nhistory,\nevidence\nregion\nhistory\nand\nﬁxate\nhistory.\nReward:\nsensi-\ntive to detection\nlocation\nDeep\nNN\n[187]\nComparable\nmAP and lower\nrun time as com-\npared\nto\nother\nmethods\nsuch\nas to exhaustive\nsliding\nwindow\nsearch(SW),\nex-\nhaustive\nsearch\nover the CPMC\nand region pro-\nposal\nset(RP)\n[112] [366]\nPascal\nVOC\n2012\nObject\ndetection\nchallenge [91].\nTree-\nStructured\nSequential\nObject Lo-\ncalization\n(Tree-RL)\n[170]\n2016 DQN\n13\nac-\ntions:\n8\ntransla-\ntion,\n5\nscaling.\nStates:\nFeature\nvector\nof\ncur-\nrent region, and\nwhole\nimage.\nReward:\nchange\nin IOU.\nCNN\ntrained\non Im-\nageNet\n[72]\n[320]\nTree-RL\nwith\nfaster\nR-CNN\noutperformed\nRPN\nwith\nfast\nR-CNN\n[108]\nin terms of AP\nand\ncomparable\nresults to Faster\nR-CNN [309]\nPascal\nVOC\n2007 [90] and\n2012 [91].\n29\nActive\nBreast\nLesion\nDetection\n[246]\n2017 DQN\n9 actions:\n6\ntrans-\nlation,\n2\nscaling, 1\ntrigger\nStates:\nfeature\nvector of current\nregion,\nReward:\nimprovement\nin\nlocalization.\nResNet\n[133]\nComparable true\npositive\nand\nfalse\npositive\nproportions\nas\ncompared\nto\nSL\n[253]\nand\nMs-C [116], but\nwith lesser mean\ninference time.\nDCE-MRI\nand\nT1-\nweighted\nanatomical\ndataset [253]\nMultitask\nobject\nlo-\ncalization\n[386]\n2018 DQN\n8 actions:\nleft,\nright, up,\ndown,\nbigger,\nsmaller,\nfatter\nand taller\nStates:\nfeature\nvector, historical\nactions. Reward:\nchange in IOU.\ndiﬀerent network\nfor\ntransforma-\ntion actions and\nterminal actions.\nPretrained\nVGG-\n16\n[340]\nwith\nIma-\ngeNet\n[72]\n[320]\nBetter mAP as\ncompared\nto\nMultiBox\n[89],\nCaicedo\net\nal.\n[43] and second\nbest to R-CNN\n[109].\nPascal VOC-\n2007\nImage\nDataset [90].\nBounding-\nBox\nAu-\ntomated\nReﬁne-\nment\n[15]\n2020 DQN\n8\nac-\ntions: up,\ndown,\nleft,\nright,\nbigger,\nsmaller,\nfatter,\ntaller\nOﬄine\nand\nonline implemen-\ntation\nStates:\nfeature\nvector\nfor oﬄine (BAR-\nDRL), HOG for\nonline\n(BAR-\nCB).\nReward:\nchange in IOU\nResNet50\n[133]\nBetter ﬁnal IOU\nfor boxes gener-\nated\nby\nmeth-\nods such as Reti-\nnaNet [225].\nPascal VOC-\n2007\n[90],\n2012 [91] Im-\nage Dataset.\nEﬃcient\nObject\nDetection\nin\nLarge\nImages\n[367]\n2020 DQN\nbinary\naction\narray:\nwhere\n1\nmeans\nthat\nthe\nagent\nwould\nconsider\nacquiring\nsub-\npatches\nfor\nthat\npar-\nticular\npatch\nCourse\nCPNet\nand ﬁne FPNet\nlevel\nsearch.\nStates:\nselected\nregion.\nReward:\ndetection\nrecall\nimage\nacquisi-\ntion cost. Policy:\nREINFORCE\n[351]\nResNet32\n[133]\nfor\npolicy\nnet-\nwork.\nand\nYOLOv3\n[306]\nwith\nDarkNet-\n53\nfor\nObject\ndetec-\ntor\nHigher mAP and\nlower\nrun\ntime\nas compared to\nother\nmethods\nsuch as [99].\nCaltech\nPedestrian\ndataset\n(CPD) [77]\nAvailable Code\n30\nOrgan Lo-\ncalization\nin\nCT\n[275]\n2020 DQN\n11\nac-\ntions:\n6\ntransla-\ntion,\n2\nscaling, 3\ndeforma-\ntion\nStates: region in-\nside the Bound-\ning box. Reward:\nchange in IOU.\nArchitecture\nsimilar\nto [10]\nLower\ndistance\nerror\nfor\norgan\nlocalization\nand\nrun time as com-\npared\nto\nother\nmethods such as\n3D-RCNN\n[409]\nand CNNs [152]\nCT\nscans\nfrom\nthe\nVISCERAL\ndataset [171]\nMonocular\n3D Object\nDetection\n[231]\n2020 DQN\n[264]\n15\nac-\ntions,\neach\nmodiﬁes\nthe\n3D\nbounding\nbox in a\nspeciﬁc\nparame-\nter\nState:\n3D\nbounding\nbox\nparameters,\n2D\nimage\nof\nob-\nject\ncropped\nby\n2D\nits\nde-\ntected bounding\nbox.\nReward:\naccuracy\nim-\nprovement\nafter\napplying\nan\naction.\nResNet-\n101\n[133]\nHigher\naverage\nprecision\n(AP)\ncompared\nto\n[268], [302], [210]\nand [35]\nKITTI [102]\nLocalization of organs in CT scans is an important pre-processing requirement for taking\nthe images of an organ, planning radiotherapy, etc. A DRL method for organ localization was\nproposed by [275], where the problem was formulated as an MDP. In the implementation,\nthe agent was responsible for predicting a 3D bounding box around the organ. The authors\nused the last 4 states as input to the agent to stabilize the search and the action space\nconsists of Eleven actions, 6 for the position of the bounding box, 2 for zoom in and zoom\nout the action, and last 3 for height, width, and depth. For Reward, they used the change\nthe in Intersection over union (IOU) across an action.\nMonocular 3D object detection is a problem where 3D bounding boxes of objects are\nrequired to be detected from a single 2D image. Even the sampling-based method is the\nSOTA approach, it has a huge ﬂaw, in which most of the samples it generates do not\noverlap with the groundtruth. To leverage that method, [231] introduced Reinforced Axial\nReﬁnement Network (RARN) for monocular 3D object detection by utilizing an RL model to\niteratively reﬁning the sampled bounding box to be more overlapped with the groundtruth\nbounding box. Given a state having the coordinates of the 3D bounding box and image\npatch of the image, the model predicts an action out of a set of 15 actions to reﬁne one of\nthe bounding box coordinates in a direction at every timestep, the model is trained by DQN\nmethod with the immediate reward is the improvement in detection accuracy between every\npair of timesteps. The whole pipeline, namely RAR-Net, was evaluated on the real-world\nKITTI dataset [102] and achieved state-of-the-art performance.\nAll these methods have been summarised and compared in Table 4, and a basic imple-\nmentation of object detection using DRL has been shown in Fig. 12. The ﬁgure illustrates\na general implementation of object detection using DRL, where the state is an image seg-\nment cropped using a bounding box produced by some other algorithm or previous iteration\nof DRL, actions predicted by the DRL agent predict a series of bounding box transforma-\n31\ntion to ﬁt the object better, hence forming a new state and Reward is the improvement in\nIntersection over union (IOU) with iterations as used by [43],[25],[15],[386],[170],[275].\nFigure 12: DRL implementation for object detection.\nThe red box corresponds to the\ninitial bounding box which for t=0 is predicted by some other algorithm or the transformed\nbounding box by previous iterations of DRL using the actions to maximize the improvement\nin IOU.\n7\nDRL in Object Tracking\nReal-time object tracking has a large number of applications in the ﬁeld of autonomous\ndriving, robotics, security, and even in sports where the umpire needs accurate estimation of\nball movement to make decisions. Object tracking can be divided into two main categories:\nSingle object tracking (SOT) and Multiple object tracking (MOT).\nMany attempts have been made for both SOT and MOT. SOT can be divided into two\ntypes, active and passive. In passive tracking it is assumed that the object that is being\ntracked is always in the camera frame, hence camera movement is not required. In active\ntracking, however, the decision to move the camera frame is required so that the object is\nalways in the frame. Passive tracking has been performed by [397], [146], where [146] per-\nformed tracking for both single and multiple objects. The authors of these papers proposed\nvarious solutions to overcome common problems such as a change in lighting and occlusion.\nActive tracking is a little bit harder as compared to a passive one because additional deci-\nsions are required for camera movement. Some eﬀorts towards active tracking include [74]\n[270] [178]. These solutions treat object detection and object tracking as two separate tasks\nand tend to fail when there is background noise.\nAn end-to-end active object tracker using DRL was proposed by [240], where the authors\nused CNNs along with an LSTM [139] in their implementation. They used the actor-critic\nalgorithm [262] to calculate the probability distribution of diﬀerent actions and the value\n32\nof state and used the object orientation and distance from the camera to calculate rewards.\nFor experiments, the authors used VizDoom and Unreal Engine as the environment.\nAnother end-to-end method for SOT using sequential search strategy and DRL was\nproposed by [418].\nThe method included using an RNN along with REINFORCE [392]\nalgorithm to train the network. The authors used a function f(W0) that takes in St and\nframe as input, where St is the object location for the ﬁrst frame and is zero elsewhere. The\noutput is fed to an LSTM module [139] with past hidden state ht. The authors calculated the\nreward function by using insertion over union (IoU) and the diﬀerence between the average\nand max.\nA deformable face tracking method that could predict bounding box along with facial\nlandmarks in real-time was proposed by [118].\nThe dual-agent DRL method (DADRL)\nmentioned in the paper consisted of two agents: a tracking and an alignment agent. The\nproblem of object tracking was formulated as an MDP where state consisted of image regions\nextracted by the bounding box and a total of 8 actions (left, right, up, down, scale-up, scale\ndown, stop and continue) were used, where ﬁrst six consists of movement actions used by\ntracking agent and last two for alignment agent.\nThe tracking agent is responsible for\nchanging the current observable region and the alignment agent determines whether the\niteration should be terminated.\nFor the tracking agent, the reward corresponded to the\nmisalignment descent and for the alignment agent the reward was +η for misalignment\nless than the threshold and −η otherwise. The DADRL implementation also consisted of\ncommunicated message channels beside the tracking agent and the alignment agent. The\ntracking agent consisted of a VGG-M [340] backbone followed by a one-layer Q-Network\nand the alignment agent was designed as a combination of a stacked hourglass network\nwith a conﬁdence network. The two communicated message channels were encoded by a\ndeconvolution layer and an LSTM unit [139] respectively.\nVisual object tracking when dealing with deformations and abrupt changes can be a\nchallenging task. A DRL method for object tracking with iterative shift was proposed by\n[308]. The approach (DRL-IS) consisted of three networks: The actor network, the prediction\nnetwork, and the critic network, where all three networks shared the same CNN and a fully\nconnected layer. Given the initial frame and bounding box, the cropped frame is fed to the\nCNNs to extract the features to be used as a state by the networks. The actions included\ncontinue, stop and update, stop and ignore, and restart. For continue, the bounding boxes\nare adjusted according to the output of the prediction network, for stop and update the\niteration is stopped and the appearance feature of the target is updated according to the\nprediction network, for stop and ignore the updating of target appearance feature is ignored\nand restart means that the target is lost and the algorithm needs to start from the initial\nbounding box. The authors of the paper used reward as 1 for change in IoU greater than\nthe threshold, 0 for change in IOU between + and - threshold, and -1 otherwise.\nConsidering the performance of actor-critic framework for various applications, [45] pro-\nposed an actor-critic [262] framework for real-time object tracking. The authors of the paper\nused a pre-processing function to obtain an image patch using the bounding box that is fed\ninto the network to ﬁnd the bounding box location in subsequent frames. For actions the\n33\nauthors used △x for relative horizontal translation, △y for relative vertical translation, and\n△s for relative scale change, and for a reward they used 1 for IoU greater than a threshold\nand -1 otherwise. They proposed oﬄine training and online tracking, where for oﬄine train-\ning a pre-trained VGG-M [340] was used as a backbone, and the actor-critic network was\ntrained using the DDPG approach [224].\nAn improvement to [45] for SOT was proposed by [84], where a visual tracker was for-\nmulated using DRL and an expert demonstrator. The authors treated the problem as an\nMDP, where the state consists of two consecutive frames that have been cropped using the\nbounding box corresponding to the former frame and used a scaling factor to control the\noﬀset while cropping. The actions consisted of four elements: △x for relative horizontal\ntranslation, △y for relative vertical translation, △w for width scaling, and △h for height\nscaling, and the reward was calculated by considering whether the IoU is greater than a\nthreshold or not. For the agent architecture the authors used a ResNet-18 [133] as backbone\nfollowed by an LSTM unit [391][139] to encode past information, and performed training\nbased on the on-policy A3C framework [262].\nIn MOT the algorithm is responsible to track trajectories of multiple objects in the given\nvideo. Many attempts have been made with MOT including [53], [55] and [143]. However,\nMOT is a challenging task because of environmental constraints such as crowding or object\noverlapping. MOT can be divided into two main techniques: Oﬄine [53] and Online [55]\n[143]. In oﬄine batch, tracking is done using a small batch to obtain tracklets and later\nall these are connected to obtain a complete trajectory. The online method includes using\npresent and past frames to calculate the trajectory. Some common methods include Kalman\nﬁltering [177], Particle Filtering [284] or Markov decision [401]. These techniques however\nare prone to errors due to environmental constraints.\nTo overcome the constraints of MOT by previous methods, [401] proposed a method for\nMOT where the problem was approached as an MDP. The authors tracked each object in\nthe frame through the Markov decision process, where each object has four states consisting:\nActive, Tracked, Lost, and Inactive.\nObject detection is the active state and when the\nobject is in the lost state for a suﬃcient amount of time it is considered Inactive, which is\nthe terminal state. The reward function in the implementation was learned through data by\ninverse RL problem [279].\nPrevious approaches for MOT follow a tracking by detection technique that is prone\nto errors.\nAn improvement was proposed by [307], where detection and tracking of the\nobjects were carried out simultaneously. The authors used a collaborative Q-Network to\ntrack trajectories of multiple objects, given the initial position of an object the algorithm\ntracked the trajectory of that object in all subsequent frames. For actions the authors used\n△x for relative horizontal translation, △y for relative vertical translation, △w for width\nscaling, and △h for height scaling, and the reward consisted of values 1,0,-1 based on the\nIoU.\nAnother method for MOT was proposed by [168], where the authors used LSTM [139]\nand DRL to approach the problem of multi-object tracking. The method described in the\npaper used three basic components: a YOLO V2 [260] object detector, many single object\n34\ntrackers, and a data association module. Firstly the YOLO V2 object detector is used to\nﬁnd objects in a frame, then each detected object goes through the agent which consists of\nCNN followed by an LSTM to encode past information for the object. The state consisted\nof the image patch and history of past 10 actions, where six actions (right, left, up, down,\nscale-up, scale down) were used for bounding box movement across the frame with a stop\naction for the terminal state. To provide reinforcement to the agent the reward was 1 if the\nIOU is greater than a threshold and 0 otherwise. In their experiments, the authors used\nVGG-16 [340] for CNN backbone and performed experiments on MOT benchmark [201] for\npeople tracking.\nTable 5: Comparing various DRL-based object tracking methods.\nThe First group for Single object tracking (SOT) and the second\ngroup for multi-object tracking (MOT)\nApproaches\nYear\nTraining\nTech-\nnique\nActions\nRemarks\nBackbone\nPerformance\nDatasets and\nSource code\nEnd\nto\nend\nac-\ntive\nobject\ntracking\n[240]\n2017\nActor-\nCritic\n(a3c)\n[262]\n6 actions:\nturn\nleft, turn\nright,\nturn\nleft\nand move\nforward,\nturn right\nand move\nforward,\nmove\nforward,\nno-op\nEnvironment:\nvirtual environ-\nment.\nReward:\ncalculated\nusing\nobject\norientation\nand\nposition.\nTracking Using\nLSTM [139]\nConvNet-\nLSTM\nHigher\nac-\ncumulated\nreward\nand\nepisode\nlength\nas\ncompared\nto\nmethods\nlike\nMIL\n[17],\nMeanshift\n[60],\nKCF [134].\nViZDoom\n[176],\nUnreal\nEngine\nDRL for ob-\nject\ntrack-\ning [418]\n2017\nDRLT\nNone\nState:\nfeature\nvector, Reward:\nchange in IOU\nuse\nof\nLSTM\n[139] and REIN-\nFORCE [392]\nYOLO\nnetwork\n[305]\nHigher\narea\nunder\ncurve\n(success rate Vs\noverlap\nthresh-\nold),\nprecision\nand speed (fps)\nas compared to\nSTUCK\n[126]\nand DLT [384].\nObject track-\ning\nbench-\nmark [397].\nAvailable Code\n35\nDual-agent\ndeformable\nface tracker\n[118]\n2018\nDQN\n8 actions:\nleft,\nright, up,\ndown,\nscale\nup, scale\ndown,\nstop and\ncontinue.\nStates:\nimage\nregion\nusing\nBounding\nbox.\nReward:\ndis-\ntance\nerror.\nFacial landmark\ndetection\nand\ntracking\nusing\nLSTM [139]\nVGG-M\n[340]\nLower\nnormal-\nized\npoint\nto\npoint error for\nlandmarks\nand\nhigher\nsuccess\nrate\nfor\nfacial\ntracking\nas\ncompared\nto\nICCR\n[187],\nMDM\n[336],\nXiao et al [32],\netc.\nLarge-scale\nface\ntracking\ndataset,\nthe\n300-VW\ntest\nset [336]\nTracking\nwith\niter-\native\nshift\n[308]\n2018\nActor-\ncritic\n[262]\n4 actions:\ncontinue,\nstop and\nupdate,\nstop\nand\nig-\nnore and\nrestart\nStates:\nim-\nage\nregion\nusing bounding\nbox.\nReward:\nchange in IOU.\nThree networks:\nactor,\ncritic\nand\nprediction\nnetwork\n3\nLayer\nCNN and\nFC layer\nHigher\narea\nunder\ncurve\nfor success rate\nVs\noverlap\nthreshold\nand\nprecision\nVs\nlocation\nerror\nthreshold\nas\ncompared\nto\nCREST\n[345],\nADNet\n[416],\nMDNet\n[273],\nHCFT\n[243],\nSINT\n[358],\nDeepSRDCF\n[67], and HDT\n[301]\nOTB-2015\n[398], Temple-\nColor\n[220],\nand\nVOT-\n2016 Dataset\n[186]\nTracking\nwith actor-\ncritic [45]\n2018\nActor-\ncritic\n[262]\n3 actions:\n△x,\n△y\nand △s\nStates:\nimage\nregion\nusing\nbounding\nbox.\nReward:\nIOU\ngreater\nthen\nthreshold.\nOf-\nﬂine\ntraining,\nonline tracking\nVGG-M\n[340]\nHigher\naverage\nprecision\nscore\nthen PTAV [93],\nCFNet\n[368],\nACFN\n[52],\nSiameFC\n[29],\nECO-HC\n[67],\netc.\nOTB-2013\n[397],\nOTB-\n2015\n[398]\nand\nVOT-\n2016\ndataset\n[186]\nAvailable Code\n36\nVisual\ntracking\nand expert\ndemon-\nstrator\n[84]\n2019\nActor-\ncritic\n(a3c)\n[262]\n4 actions:\n△x,\n△y,△w\nand △h\nStates:\nimage\nregion\nusing\nbounding\nbox.\nReward: change\nin\nIOU.\nSOT\nusing\nLSTM\n[391][139]\nResNet-\n18 [133]\nComparable\nsuccess\nand\nprecision scores\nas compared to\nLADCF\n[408],\nSiamRPN [209]\nand ECO [66]\nGOT-10k\n[148], LaSOT\n[92], UAV123\n[269],\nOTB-\n100\n[397],\nVOT-2018\n[185]\nand\nVOT-2019.\nObject\ntracking\nby decision\nmaking\n[401]\n2015\nTLD\nTracker\n[174]\n7 actions:\ncorre-\nsponding\nto\nmov-\ning\nthe\nobject\nbetween\nstates\nsuch\nas\nActive,\ntracked,\nlost\nand\nInactive\nStates: 4 states:\nActive, tracked,\nlost\nand\nInac-\ntive.\nReward:\ninverse\nRL\nproblem [279]\nNone\nComparable\nmultiple object\ntracking\naccu-\nracy\n(MOTA)\nand\nmultiple\nobject\ntrack-\ning\nprecision\n(MOTP)\n[28]\nas compared to\nDPNMS\n[296],\nTCODAL\n[18],\nSegTrack [259],\nMotiCon\n[200],\netc\nM0T15\ndataset [201]\nAvailable Code\nCollaborative\nmulti\nob-\nject tracker\n[307]\n2018\nDQN\n4 actions:\n△x, △y,\n△w\nand\n△h\nStates:\nimage\nregion\nusing\nbounding\nbox.\nReward:\nIOU\ngreater\nthen\nthreshold.\n2\nnetworks:\npre-\ndiction\nand\ndecision\nnet-\nwork\n3\nLayer\nCNN and\nFC Layer\nComparable\nmultiple object\ntracking\naccu-\nracy\n(MOTA)\nand\nmultiple\nobject\ntrack-\ning\nprecision\n(MOTP)\n[28]\nas\ncompared\nto SCEA [143],\nMDP\n[401],\nCDADDALpb\n[19],\nAMIR15\n[321]\nMOT15 [201]\nand\nMOT16\n[258] datasets\n37\nMulti\nobject\ntracking in\nvideo [168]\n2018\nDQN\n6 actions:\nright,\nleft,\nup,\ndown,\nscale\nup, scale\ndown\nStates:\nimage\nregion\nusing\nbounding\nbox.\nReward:\nIOU\ngreater\nthen\nthreshold.\nDe-\ntection\nusing\nYOLO-V2 [260]\nfor detector and\nLSTM [139] .\nVGG-16\n[340]\nComparable\nif\nnot\nbetter\nmultiple object\ntracking\naccu-\nracy\n(MOTA)\nand\nmultiple\nobject\ntrack-\ning\nprecision\n(MOTP)\n[28]\nas\ncompared\nto RNN-LSTM\n[201], LP-SSVM\n[401], MDPSub-\nCNN [199], and\nSiameseCNN\n[123]\nMOT15\nDataset [201]\nMulti agent\nmulti\nob-\nject tracker\n[169]\n2019\nDQN\n9 actions:\nmove\nright,\nmove left,\nmove\nup, move\ndown,\nscale\nup, scale\ndown,\nfatter,\ntaller and\nstop\nStates:\nimage\nregion\nusing\nbounding\nbox.\nReward:\nIOU\ngreater\nthen\nthreshold.\nYOLO-V3 [306]\nfor\ndetection\nand\nLSTM\n[139].\nVGG-16\n[340]\nHigher\nrun-\nning time, and\ncomparable\nif\nnot\nbetter\nmultiple object\ntracking\naccu-\nracy\n(MOTA)\nand\nmultiple\nobject\ntrack-\ning\nprecision\n(MOTP)\n[28]\nas\ncompared\nto RNN-LSTM\n[201], LP-SSVM\n[401], MDPSub-\nCNN [199], and\nSiameseCNN\n[123]\nMOT15 chal-\nlenge\nbench-\nmark [201].\nTo address the problems in existing tracking methods such as varying numbers of tar-\ngets, non-real-time tracking, etc, [169] proposed a multi-object tracking algorithm based on\na multi-agent DRL tracker (MADRL). In their object tracking pipeline the authors used\nYOLO-V3 [306] as object detector, where multiple detections produced by YOLO-V3 were\nﬁltered using the IOU and the selected results were used as multiple agents in multiple agent\ndetector. The input agents were fed into a pre-trained VGG-16 [340] followed by an LSTM\nunit [139] that could share information across agents and return the actions encoded in a\n38\n9-dimensional vector( move right, move left, move up, move down, scale-up, scale down, as-\npect ratio change fatter, aspect ratio change taller and stop), also a reward function similar\nto [168] was used.\nVarious works in the ﬁeld of object tracking have been summarized in Table 5, and a\nbasic implementation of object tracking using DRL has been shown in Fig. 13. The ﬁgure\nillustrates a general implementation of object tracking in videos using DRL, where the state\nconsists of two consecutive frames (Ft, Ft+1) with a bounding box for the ﬁrst frame produced\nby another algorithm for the ﬁrst iteration or by the previous iterations of DRL agent. The\nactions corresponds to the moving the bounding on the image to ﬁt the object in frame Ft+1,\nhence forming a new state with frame Ft+1 and frame Ft+2 along with the bounding box\nfor frame Ft+1 predicted by previous iteration and reward corresponds to whether IOU is\ngreater then a given threshold as used by [118],[308],[45], [84],[307],[168],[169].\nFigure 13: DRL implementation for object tracking. Here the state consists of two consecu-\ntive frames with bounding box locations for the ﬁrst frame that is predicted by some object\ndetection algorithm or by the previous iteration of DRL, the actions move the bounding box\npresent in the ﬁrst frame to ﬁt the object in the second frame to maximize the reward which\nis the whether the IOU is greater than a given threshold or not.\n8\nDRL in Image Registration\nImage registration is a very useful step that is performed on 3D medical images for the\nalignment of two or more images. The goal of 3D medical image registration is to ﬁnd a\ncorrelation between two images from either diﬀerent patients or the same patients at diﬀerent\ntimes, where the images can be Computed Tomography (CT), Magnetic Resonance Imaging\n(MRI), or Positron Emission Tomography (PET). In the process, the images are brought to\nthe same coordinate system and aligned with each other. The reason for image registration\nbeing a challenging task is the fact that the two images used may have a diﬀerent coordinate\nsystem, scale, or resolution.\nMany attempts have been made toward automated image registration. A multi-resolution\nstrategy with local optimizers to perform 2D or 3D image registration was performed by\n39\n[359]. However, multi-resolution tends to fail with diﬀerent ﬁeld of views. Heuristic semi-\nglobal optimization schemes were proposed to solve this problem and used by [252] through\nsimulated annealing and through genetic algorithm [317], However, their cost of computation\nwas very high. A CNN-based approach to this problem was suggested by [256], and [79]\nproposed an optical ﬂow method between 2D RGB images. A descriptor learned through a\nCNN was proposed by [395], where the authors encoded the posture and identity of a 3D\nobject using the 2D image. Although all of these formulations produce satisfactory results\nyet, the methods could not be applied directly to 3D medical images.\nTo overcome the problems faced by previous methods, [238] proposed a method for im-\nproving probabilistic image registration via RL and uncertainty evaluation. The method in-\nvolved predicting a regression function that predicts registration error from a set of features\nby using regression random forests (RRF) [37] method for training. The authors performed\nexperiments on 3D MRI images and obtained an accuracy improvement of up to 25%.\nPrevious image registration methods are often customized to a speciﬁc problem and are\nsensitive to image quality and artifacts. To overcome these problems, [221] proposed a robust\nmethod using DRL. The authors considered the problem as an MDP where the goal is to ﬁnd\na set of transformations to be performed on the ﬂoating image to register it on the reference\nimage. They used the gamma value for future reward decay and used the change in L2\nNorm between the predicted transformation and ground truth transformation to calculate\nthe reward. The authors also used a hierarchical approach to solve the problem with varying\nFOVs and resolutions.\nTable 6: Comparing various DRL-based image registration meth-\nods.\nApproaches\nYear\nTraining\nTech-\nnique\nActions\nRemarks\nBackbone\nPerformance\nDatasets\nImage\nreg-\nistration\nusing\nun-\ncertainity\nevaluation\n[238]\n2013\nDQN\nNot spec-\niﬁed\nProbabilistic\nmodel\nusing\nregression\nran-\ndom\nforests\n(RRF) [37]\nNot spec-\niﬁed\nHigher\nﬁnal\nDice\nscore\n(DSC) as com-\npared to other\nmethods\nlike\nrandom\nseed\nselection\nand\ngrid-based seed\nselection\n3D\nMRI\nimages\nfrom\nLONI\nProb-\nabilistic\nBrain\nAtlas\n(LPBA40)\nDataset\n40\nRobust\nImage\nreg-\nistration\n[221]\n2017\nDQN\n12\nac-\ntions:\ncorre-\nsponding\nto\ndif-\nferent\ntransfor-\nmations\nStates:\ncurrent\ntransforma-\ntion.\nReward:\ndistance error.\n5\nConv3D\nlayers\nand 3 FC\nlayers\nBetter\nsuccess\nrate then ITK\n[153],\nQuasi-\nglobal\n[255]\nand\nSemantic\nregistration[277]\nAbdominal\nspine\nCT\nand\nCBCT\ndataset, Car-\ndiac CT and\nCBCT\nMultimodal\nimage\nreg-\nistration\n[244]\n2017\nDuel-\nDQN\nDouble-\nDQN\nActions\nupdate\nthe trans-\nforma-\ntions\non\nﬂoating\nimage\nStates: cropped\n3D\nimage.\nDuel-DQN\nfor\nvalue\nes-\ntimation\nand\nDouble\nDQN\nfor\nupdating\nweights.\nBatch\nnormal-\nization\nfollowed\nby\n5\nConv3D\nand\n3\nMaxpool\nlayers\nLower\nEu-\nclidean\ndis-\ntance\nerror\nas\ncompared\nto\nmethods\nlike\nHausdorﬀ, ICP,\nDQN\n[264],\nDueling\n[390],\netc.\nThorax\nand\nAbdomen\n(ABD)\ndataset\nRobust\nnon-rigid\nagent-based\nregistration\n[184]\n2017\nDQN\n2n\nac-\ntions\nfor\nn dimen-\nsional\nθ\nvector\nStates:\nﬁxed\nand\nmoving\nimage. Reward:\nchange in trans-\nformation error.\nWith\nStatisti-\ncal deformation\nmodel and fuzzy\naction control.\nMulti\nlayer\nCNN,\npooling\nand\nFC\nlayers.\nHigher\nMean\nDice score and\nlower Hausdorﬀ\ndistance\nas\ncompared\nto\nmethods\nlike\nLCC-Demons\n[237]\nand\nElastix [180].\nMICCAI\nchallenge\nPROMISE12\n[227]\nRobust\nMultimodal\nregistration\n[349]\n2018\nActor-\nCritic\n(a3c)\n[262]\n8 actions:\nfor\ndif-\nferent\ntransfor-\nmations\nStates:\nﬁxed\nand\nmoving\nimage. Reward:\nDistance\nerror.\nMonte-carlo\nmethod\nwith\nLSTM [139].\nMulti\nlayer\nCNN and\nFC layer\nComparable\nif\nnot\nlower\ntarget\nregistra-\ntion\nerror\n[96]\nas\ncompared\nto\nmethods\nlike\nSIFT\n[239],\nElastix\n[180],\nPure\nSL, RL-matrix,\nRL-LME, etc.\nCT and MR\nimages\nA multi-modal method for image registration was proposed by [244], where the authors\nused DRL for alignment of depth data with medical images. In the speciﬁed work Duel\nDQN was used as the agent for estimating the state value and the advantage function, and\nthe cropped 3D image tensor of both data modalities was considered as the state.\nThe\n41\nalgorithm’s goal was to estimate a transformation function that could align moving images\nto a ﬁxed image by maximizing a similarity function between the ﬁxed and moving image.\nA large number of convolution and pooling layer were used to extract high-level contextual\ninformation, batch normalization and concatenation of feature vector from last convolution\nlayer with action history vector was used to solve the problem of oscillation and closed loops,\nand Double DQN architecture for updating the network weights was used.\nPrevious methods for image registration fail to cope with large deformations and variabil-\nity in appearance. To overcome these issues [184] proposed a robust non-rigid agent-based\nmethod for image registration. The method involves ﬁnding a spatial transformation Tθ that\ncan map the ﬁxed image with the ﬂoating image using actions at each time step, that is\nresponsible for optimizing θ. If the θ is a d dimensional vector then there will be 2d possi-\nble actions. In this work, a DQN was used as an agent for value estimation, along with a\nreward that corresponded to the change in θ distance between ground truth and predicted\ntransformations across an action.\nAn improvement to the previous methods was proposed by [349], where the authors used\na recurrent network with RL to solve the problem. Similar to [221], they considered the\ntwo images as a reference/ﬁxed and ﬂoating/moving, and the algorithm was responsible\nfor predicting transformation on the moving image to register it on a ﬁxed image. In the\nspeciﬁed work an LSTM [139] was used to encode past hidden states, Actor-critic [262] for\npolicy estimation, and a reward function corresponding to distance between ground truth\nand transformed predicted landmarks were used.\nVarious methods in the ﬁeld of Image registration have been summarized and compared\nin Table 6, and a basic implementation of image registration using DRL has been shown in\nFig. 14. The ﬁgure illustrates a general implementation of image registration using DRL\nwhere the state consists of a ﬁxed and ﬂoating image. The DRL agent predicts actions in\nform of a set of transformations on a ﬂoating image to register it onto the ﬁxed image hence\nforming a new state and accepts reward in form of improvement in distance error between\nground truth and predicted transformations with iterations as described by [349],[184],[221].\n9\nDRL in Image Segmentation\nImage segmentation is one of the most extensively performed tasks in computer vision,\nwhere the algorithm is responsible for labeling each pixel position as foreground or back-\nground corresponding to the object being segmented in the image. Image segmentation has\na wide variety of applications in medical, robotics, weather, etc. One of the earlier attempts\nwith image segmentation includes [125]. With the improvement in detection techniques and\nintroduction of CNN, new methods are introduced every year for image segmentation. Mask\nR-CNN [132] extended the work by Faster R-CNN [309] by adding a segmentation layer\nafter the Bounding box has been predicted. Some earlier works include [109], [127], [128]\netc. Most of these works give promising results in image segmentation. However, due to\nthe supervised nature of CNN and R-CNN, these algorithms need a large amount of data.\nIn ﬁelds like medical, the data is sometimes not readily available hence we needed a way to\n42\nFigure 14: DRL implementation for image registration.\nThe state consists of ﬁxed and\nﬂoating image and the actions in form of transformations are performed on the ﬂoating\nimage so as to maximize reward by minimizing distance between the ground truth and\npredicted transformations.\ntrain algorithms to perform a given task when there are data constraints. Luckily RL tends\nto shine when the data is not available in a large quantity.\nOne of the ﬁrst methods for Image segmentation through RL was proposed by [324],\nwhere the authors proposed an RL framework for medical image segmentation. In their\nwork, they used a Q-Matrix, where the actions were responsible for adjusting the threshold\nvalues to predict the mask and the reward was the normalized change in quality measure\nbetween action steps. [325] also used a similar technique of Tabular method.\nTo overcome the constraints of the previous method for segmentation, [310] proposed a\nmethod for indoor semantic segmentation through RL. In their paper, the authors proposed\na sequential strategy using RL to combine binary object masks of diﬀerent objects into a\nsingle multi-object segmentation mask. They formulated the binary mask in a Conditional\nRandom Field Framework (CRF), and used a logistic regression version of AdaBoost [140] for\nclassiﬁcation. The authors considered the problem of adding multiple binary segmentation\ninto one as an MDP, where the state consisted of a list of probability distributions of diﬀer-\nent objects in an image, and the actions correspond to the selection of object/background\nsegmentation for a particular object in the sequential semantic segmentation. In the RL\nframework, the reward was considered in terms of pixel-wise frequency weighted Jaccard\nIndex computed over the set of actions taken at any stage of an episode.\nInteractive segmentation is the task of producing an interactive mask for objects in an\nimage. Most of the previous works in this ﬁeld greatly depend on the distribution of inputs\nwhich is user-dependent and hence produce inadequate results. An improvement was pro-\nposed by [343], where the authors proposed SeedNet, an automatic seed generation method\n43\nfor robust interactive segmentation through RL. With the image and initial seed points, the\nalgorithm is capable of generating additional seed points and image segmentation results.\nThe implementation included Random Walk (RW) [114] as the segmentation algorithm and\nDQN for value estimation by considering the problem as an MDP. They used the current\nbinary segmentation mask and image features as the state, the actions corresponded to se-\nlecting seed points in a sparse matrix of size 20×20(800 diﬀerent actions were possible), and\nthe reward consisted of the change in IOU across an action. In addition, the authors used\nan exponential IOU model to capture changes in IOU values more accurately.\nMost of the previous work for image segmentation fail to produce satisfactory results\nwhen it comes to 3D medical data. An attempt on 3D medical image segmentation was done\nby [222], where the authors proposed an iteratively-reﬁned interactive multi-agent method\nfor 3D medical image segmentation. They proposed a method to reﬁne an initial course\nsegmentation produced by some segmentation methods using RL, where the state consisted of\nthe image, previous segmentation probability, and user hint map. The actions corresponded\nto adjusting the segmentation probability for reﬁnement of segmentation, and a relative\ncross-entropy gain-based reward to update the model in a constrained direction was used.\nIn simple words, it is the relative improvement of previous segmentation to the current one.\nThe authors utilized an asynchronous advantage actor-critic algorithm for determining the\npolicy and value of the state.\nTable 7: Comparing various DRL-based image segmentation meth-\nods\nApproaches Year\nTraining\nTechnique\nActions\nRemarks\nBackbone Performance\nDatasets\nSemantic\nSegmen-\ntation for\nindoor\nscenes[310]\n2016\nDQN\n2\nac-\ntions per\nobject:\nobject,\nback-\nground\nStates:\ncurrent\nprobability\ndistribution.\nReward:\npixel-\nwise\nfrequency\nweighted\nJac-\ncard\nindex.\nConditional\nRandom\nField\nFramework\n(CRF) and lo-\ngistic regression\nversion of Ad-\naBoost [140] for\nclassiﬁcation.\nNot\nSpeci-\nﬁed\nPixel-wise\npercentage\njaccard\nindex\ncomparable\nto\nGupta-L\n[121]\nand\nGupta-P\n[120].\nNYUD\nV2\ndataset [338]\n44\nSeedNet\n[343]\n2018\nDQN,\nDouble-\nDQN,\nDuel-DQN\n800\nac-\ntions:\n2\nper pixel\nStates:\nimage\nfeatures\nand\nsegmentation\nmask.\nReward:\nchange in IOU.\nRandom\nWalk\n(RW) [114] for\nsegmentation\nalgorithm.\nMulti\nlayer\nCNN\nBetter\nIOU\nthen\nmethods\nlike FCN [236]\nand iFCN [407].\nMSRA10K\nsaliency\ndataset [49]\nIteratively\nreﬁned\nmulti\nagent\nsegmen-\ntation\n[222]\n2020\nActor-critic\n(a3c) [262]\n1\naction\nper voxel\nfor\nad-\njusting\nsegmen-\ntation\nprobabil-\nity\nStates:\n3D\nimage\nsegmen-\ntation\nproba-\nbility and hint\nmap.\nReward:\ncross\nentropy\ngain\nbased\nframework.\nR-net\n[378]\nBetter\nperfor-\nmance\nthen\nmethods\nlike\nMinCut\n[183],\nDeepIGeoS\n(R-Net)\n[378]\nand\nInterCNN\n[36].\nBraTS\n2015[254],\nMM-WHS\n[432]\nand\nNCI-ISBI\n2013\nChal-\nlenge [33]\nMulti-\nstep\nmedical\nimage\nsegmen-\ntation\n[360]\n2020\nActor-critic\n(a3c) [262]\nActions\ncontrol\nthe posi-\ntion\nand\nshape\nof\nbrush\nstroke to\nmodify\nsegmen-\ntation\nStates:\nimage,\nsegmentation\nmask and time\nstep.\nReward:\nchange\nin\ndis-\ntance\nerror.\nPolicy:\nDPG\n[339].\nResNet18\n[133]\nHigher\nMean\nDice score and\nlower Hausdorﬀ\ndistance\nthen\nmethods\nlike\nGrab-Cut [315],\nPSPNet\n[425],\nFCN\n[236],\nU-Net\n[313],\netc.\nProstate MR\nimage dataset\n(PROMISE12,\nISBI2013)\nand\nretinal\nfundus\nim-\nage\ndataset\n(REFUGE\nchallenge\ndataset [285])\nAnomaly\nDetection\nin Images\n[56]\n2020\nREINFORCE\n[392]\n9 actions,\n8 for di-\nrections\nto\nshift\ncenter\nof\nthe\nextracted\npatch to,\nthe\nlast\naction\nis\nto switch\nto a ran-\ndom new\nimage\nEnvironment:\ninput\nimage\nto\nthe\nmodel.\nState:\nob-\nserved\npatch\nfrom the image\ncentered\nby\npredicted center\nof interest.\nNone\nSuperior perfor-\nmance\nin\n[27]\nand\n[337]\non\nall metrics e.g.\nprecision, recall\nand\nF1\nwhen\ncompared\nwith\nU-Net\n[313]\nand\nbaseline\nunsupervised\nmethod in [27]\nbut\nonly\nwins\non recall in [44]\nMVTec\nAD\n[27],\nNan-\noTWICE\n[44],\nCrack-\nForest [337]\n45\nFurther improvement in the results of medical image segmentation was proposed by\n[360]. The authors proposed a method for multi-step medical image segmentation using RL,\nwhere they used a deep deterministic policy gradient method (DDPG) based on actor-critic\nframework [262] and similar to Deterministic policy gradient (DPG) [339]. The authors used\nResNet18 [133] as backbone for actor and critic network along with batch normalisation\n[157] and weight normalization with Translated ReLU [400]. In their MDP formulation,\nthe state consisted of the image along with the current segmentation mask and step-index,\nand the reward corresponded to the change in mean squared error between the predicted\nsegmentation and ground truth across an action. According to the paper the action was\ndeﬁned to control the position and shape of brush stroke used to modify the segmentation.\nAn example in image segmentation outside the medical ﬁeld is [56] proposing to tackle the\nproblem of anomalies detection and segmentation in images (i.e. damaged pins of an IC chip,\nsmall tears in woven fabric). [56] utilizes an additional module to attend only on a speciﬁc\npatch of the image centered by a predicted center instead of the whole image, this module\nhelps a lot in reducing the imbalance between normal regions and abnormal locations. Given\nan image, this module, namely Neural Batch Sampling (NBS), starts from a random initiated\ncenter and recurrently moves that center by eight directions to the abnormal location in the\nimage if it exists, and it has an additional action to stop moving the center when it has already\nconverged to the anomaly location or there is not any anomaly can be observed. The NBS\nmodule is trained by REINFORCE algorithm [392] and the whole model is evaluated on\nmultiple datasets e.g. MVTec AD [27], NanoTWICE [44], CrackForest [337].\nVarious works in the ﬁelds of Image segmentation have been summarised and compared\nin Table 7, and a basic implementation of image segmentation using DRL has been shown\nin Fig. 15. The ﬁgure shows a general implementation of image segmentation using DRL.\nThe states consist of the image along with user hint (landmarks or segmentation mask by\nthe other algorithm) for the ﬁrst iteration or segmentation mask by the previous iteration.\nThe actions are responsible for labeling each pixel as foreground and background and reward\ncorresponds to an improvement in IOU with iterations as used by [343],[222].\n10\nDRL in Video Analysis\nObject segmentation in videos is a very useful yet challenging task in computer vision ﬁeld.\nVideo object segmentation task focuses on labelling each pixel for each frame as foreground\nor background. Previous works in the ﬁeld of video object segmentation can be divided\ninto three main methods.\nunsupervised [288][402], weakly supervised [48][163] [419] and\nsemi-supervised [41] [164][292].\nA DRL-based framework for video object segmentation was proposed by [323], where\nthe authors divided the image into a group of sub-images and then used the algorithm on\neach of the sub-image. They proposed a group of actions that can perform to change the\nlocal values inside each sub-image and the agent received reward based on the change in the\nquality of segmented object inside each sub-image across an action. In the proposed method\ndeep belief network (DBN) [47] was used for approximating the Q-values.\n46\nFigure 15: DRL implementation for Image segmentation. The state consists of the image to\nbe segmented along with a user hint for t=0 or the segmentation mask by the previous itera-\ntions. The DRL agent performs actions by labeling each pixel as foreground and background\nto maximize the improvement in IOU over the iterations.\nSurgical gesture recognition is a very important yet challenging task in the computer\nvision ﬁeld. It is useful in assessing surgical skills and for eﬃcient training of surgeons. A\nDRL method for surgical gesture classiﬁcation and segmentation was proposed by [228]. The\nproposed method could work on features extracted by video frames or kinematic data frames\ncollected by some means along with the ground truth labels. The problem of classiﬁcation\nand segmentation was considered as an MDP, where the state was a concatenation of TCN\n[195][199] features of the current frame, 2 future frames a speciﬁed number of frames later,\ntransition probability of each gesture computed from a statistical language model [311] and\na one-hot encoded vector for gesture classes. The actions could be divided into two sub-\nactions, One to decide optimal step size and one for choosing gesture class, and the reward\nwas adopted in a way that encouraging the agent to adopt a larger step and also penalizes the\nagent for errors caused by the action. The authors used Trust Region Policy Optimization\n(TRPO) [326] for training the policy and a spacial CNN [196] to extract features.\nEarlier approaches for video object segmentation required a large number of actions to\ncomplete the task.\nAn Improvement was proposed by [124], where authors used an RL\nmethod for object segmentation in videos. They proposed a reinforcement cutting-agent\nlearning framework, where the cutting-agent consists of a cutting-policy network (CPN)\nand a cutting-execution network (CEN). The CPN learns to predict the object-context box\npair, while CEN learns to predict the mask based on the inferred object-context box pair.\nThe authors used MDP to solve the problem in a semi-supervised fashion. For the state of\nCPN the authors used the input frame information, the action history, and the segmentation\nmask provided in the ﬁrst frame. The output boxes by CPN were input for the CEN. The\nactions for CPN network included 4 translation actions (Up, Down, Left, Right), 4 scaling\n47\naction (Horizontal shrink, Vertical shrink, Horizontal zoom, Vertical zoom), and 1 terminal\naction (Stop), and the reward corresponded to the change in IOU across an action. For the\nnetwork architecture, a Fully-Convolutional DenseNet56 [166] was used as a backbone along\nwith DQN as the agent for CPN and down-sampling followed by up-sampling architecture\nfor CEN.\nUnsupervised video object segmentation is an intuitive task in the computer vision ﬁeld.\nA DRL method for this task was proposed by [111], where the authors proposed a motion-\noriented unsupervised method for image segmentation in videos (MOREL). They proposed\na two-step process to achieve the task in which ﬁrst a representation of input is learned to\nunderstand all moving objects through unsupervised video object segmentation, Then the\nweights are transferred to the RL framework to jointly train segmentation network along\nwith policy and value function. The ﬁrst part of the method takes two consecutive frames\nas input and predicts a number of segmentation masks, corresponding object translations,\nand camera translations. They used a modiﬁed version of actor-critic [262][329][371] for the\nnetwork of ﬁrst step. Following the unsupervised fashion, the authors used the approach\nsimilar to [375] and trained the network to interpolate between consecutive frames and used\nthe masks and translations to estimate the optical ﬂow using the method that was proposed\nin Spatial Transformer Networks [159]. They also used structural dissimilarity (DSSIM)\n[388] to calculate reconstruction loss and actor-critic [262] algorithm to learn policy in the\nsecond step.\nA DRL method for dynamic semantic face video segmentation was proposed by [387],\nwhere Deep Feature Flow [431] was utilized as the feature propagation framework and RL\nwas used for an eﬃcient and eﬀective scheduling policy.\nThe method involved dividing\nframes into key (Ik) and non-key (Ii), and using the last key frame features for performing\nsegmentation of non-key frame. The actions made by the policy network corresponded to\ncategorizing a frame as Ik or Ii and the state consisted of deviation information and expert\ninformation, where the deviation information described the diﬀerence between current Ii and\nlast Ik and expert information encapsulated the key decision history. The authors utilized\nFlowNet2-s model [156] as an optical ﬂow estimation function, and divided the network into\nfeature extraction module and task-speciﬁc module. After policy network which consisted\nof one convolution layer, 4 fully connected layers and 2 concatenated channels consisting of\nKAR (Key all ratio: Ratio between key frame and every other frame in decision history) and\nLKD (Last key distance: Distance between current and last key frame) predicted the action,\nIf the current frame is categorized as key frame the feature extraction module produced\nthe frame features and task-speciﬁc module predicted the segmentation, However if the\nframe is categorized as a non-key frame the features from the last key frame along with\nthe optical ﬂow was used by the task-speciﬁc module to predict the segmentation.\nThe\nauthors proposed two types of reward functions, The ﬁrst reward function was calculated by\nconsidering the diﬀerence between the IOU for key and non-key actions. The second reward\nfunction was proposed for a situation when ground truth was not available and was calculated\nby considering the accuracy score between segmentation for key and non-key actions.\n48\nTable 8: Comparing various methods associated with video. First\ngroup for video object segmentation, second group for action recog-\nnition and third group for video summarisation\nApproaches Year\nTraining\nTechnique\nActions\nRemarks\nBackbone\nPerformance\nDatasets and\nSource code\nObject\nsegmen-\ntation in\nvideos[323]\n2016\nDeep\nBelief\nNetwork\n[47]\nActions\nchanged\nlocal\nvalues\nin\nsub-\nimages\nStates:\nsub-\nimages.\nRe-\nward: quality of\nsegmentation.\nNot spec-\niﬁed\nNot speciﬁed\nNot speciﬁed\nSurgical\ngesture\nsegmen-\ntation\nand clas-\nsiﬁcation\n[228]\n2018\nTrust\nRegion\nPolicy Op-\ntimization\n(TRPO)\n[326]\n2\ntypes:\noptimal\nstep\nsize\nand\ngesture\nclass\nStates:\nTCN\n[[195], [199]] and\nfuture\nframes.\nReward:\nen-\ncourage\nlarger\nsteps and min-\nimize\naction\nerrors.\nStatis-\ntical\nlanguage\nmodel\n[311]\nfor\ngesture\nprobability.\nSpacial\nCNN\n[196]\nComparable\naccuracy,\nand\nhigher edit and\nF1\nscores\nas\ncompared\nto\nmethods\nlike\nSD-SDL\n[331],\nBidir\nLSTM\n[76],\nLC-SC-\nCRF\n[197],\nSeg-ST-CNN\n[196],\nTCN\n[198], etc\nJIGSAWS\n[[6],\n[100]]\nbenchmark\ndataset\nAvailable Code\n49\nCutting\nagent\nfor video\nobject\nsegmen-\ntation\n[124]\n2018\nDQN\n8 actions:\n4\ntrans-\nlation\nactions\n(Up,\nDown,\nLeft,\nRight),\n4 scaling\naction\n(Hori-\nzontal\nshrink,\nVertical\nshrink,\nHorizon-\ntal zoom,\nVertical\nzoom)\nand\n1\nterminal\naction\n(Stop)\nStates:\ninput\nframe,\naction\nhistory\nand\nsegmentation\nmask.\nReward:\nchange in IOU.\ncutting-policy\nnetwork for box-\ncontext\npair\nand\ncutting-\nexecution\nnet-\nwork for mask\ngeneration\nDenseNet\n[166]\nHigher\nmean\nregion\nsimi-\nlarity,\ncounter\naccuracy\nand\ntemporal\nsta-\nbility\n[293]\nas\ncompared\nto\nmethods\nlike\nMSK\n[292],\nARP\n[173],\nCTN\n[165],\nVPN [164], etc.\nDAVIS\ndataset\n[293] and the\nYouTube Ob-\njects\ndataset\n[162], [300]\nUnsupervised\nvideo ob-\nject\nsegmen-\ntation\n(MOREL)\n[111]\n2018\nActor-\ncritic\n(a2c) [262]\nNot spec-\niﬁed\nStates:\nconsec-\nutive\nframes.\nTwo\nstep\nprocess\nwith\noptical\nﬂow\nusing\nSpatial\nTransformer\nNetworks\n[159]\nand\nrecon-\nstruction\nloss\nusing structural\ndissimilarity\n[388].\nMulti-\nlayer\nCNN\nHigher\ntotal\nepisodic reward\nas compared to\nmethods\nthat\nused\nactor-\ncritic\nwithout\nMOREL\n59\nAtari\ngames.\nAvailable Code\n50\nFace\nvideo\nsegmen-\ntation\n[387]\n2020\nNot speci-\nﬁed\n2 actions:\ncategoris-\ning\na\nframe\nas\na key or\na non-key\nStates:\ndevia-\ntion information\nwhich described\nthe\ndiﬀerence\nbetween\ncur-\nrent\nnon-key\nand\nlast\nkey\ndecision,\nand\nexpert\ninfor-\nmation\nwhich\nencapsulated\nthe key decision\nhistory.\nRe-\nward:\nimprove-\nment\nin\nmean\nIOU/accuracy\nscore\nbetween\nsegmentation of\nkey and non-key\nframes\nMulti-\nlayer\nCNN\nHigher\nmean\nIOU then other\nmethods\nlike\nDVSNet\n[410],\nDFF [431].\n300VW\ndataset\n[336]\nand Cityscape\ndataset [61]\nMulti-\nagent\nVideo\nObject\nSegmen-\ntation\n[373]\n2020\nDQN\nActions\nof\n2\ntypes:\nmove-\nment\nactions\n(up,\ndown,\nleft\nand\nright)\nand\nset\naction\n(action\nto\nplace\nlocation\nprior\nat\na random\nlocation\non\nthe\npatch)\nStates:\ninput\nframe,\noptical\nﬂow [156] from\nprevious\nframe\nand action his-\ntory.\nReward:\nclicks generated\nby\ngamiﬁca-\ntion.\nDown-\nsampling\nand\nup-sampling\nsimilar\nto\nU-Net [313]\nDenseNet\n[147]\nHigher\nmean\nregion\nsimilar-\nity and contour\naccuracy\n[293]\nas compared to\nsemi-supervised\nmethods such as\nSeamSeg\n[14],\nBSVS\n[248],\nVSOF\n[363],\nOSVOS\n[41]\nand\nweakly-\nsupervised\nmethods\nsuch\nas GVOS [346],\nSpftn [419]\nDAVIS-17\ndataset [293]\n51\nSkeleton-\nbased\nAction\nRecog-\nnition\n[357]\n2018\nDQN\n3 actions:\nshifting\nto\nleft,\nstaying\nthe same\nand shift-\ning\nto\nright\nStates:\nGlobal\nvideo\ninfor-\nmation\nand\nselected frames.\nReward: change\nin\ncategorical\nprobability.\n2\nstep\nnetwork\n(FDNet)\nto\nﬁlter\nframes\nand GCNN for\naction labels\nMulti-\nlayer\nCNN\nHigher\ncross\nsubject\nand\ncross\nview\nmetrics\nfor\nNTU+RGBD\ndataset\n[333],\nand\nhigher\naccuracy\nfor\nSYSU-3D [145]\nand UT-Kinect\nDataset\n[399]\nwhen\ncom-\npared\nwith\nother\nmethods\nlike\nDynamic\nSkeletons [145],\nHBRNN-L [81],\nPart-aware\nLSTM\n[333],\nLieNet-3Blocks\n[151],\nTwo-\nStream\nCNN\n[211], etc.\nNTU+RGBD\n[333],\nSYSU-\n3D [145] and\nUT-Kinect\nDataset [399]\nVideo\nsummari-\nsation\n[429]\n2018\nDQN\n2 actions:\nselecting\nand\nre-\njecting\nthe frame\ntates:\nbidirec-\ntional\nLSTM\n[150]\nproduced\nstates by input\nframe\nfea-\ntures.\nReward:\nDiversity-\nRepresentativeness\nReward\nFunc-\ntion.\nGoogLeNet\n[355]\nHigher\nF-\nscore\n[421]\nas\ncompared\nto\nmethods\nlike\nUniform\nsampling,\nK-medoids,\nDictionary\nse-\nlection\n[88],\nVideo-MMR\n[218],\nVsumm\n[69], etc.\nTVSum [344]\nand\nSumMe\n[122].\nAvailable Code\n52\nVideo\nsumma-\nrization\n[430]\n2018\nDuel DQN\nDouble\nDQN\n2 actions:\nselecting\nand\nre-\njecting\nthe frame\nStates:\nse-\nquence\nof\nframes Reward:\nDiversity-\nRepresentativeness\nReward\nFunc-\ntion\n2\nstage\nimplementa-\ntion:\nclassi-\nﬁcation\nand\nsummarisation\nnetwork\nusing\nbidirectional\nGRU\nnetwork\nand LSTM [150]\nGoogLeNet\n[355]\nHigher\nF-\nscore\n[421]\nas\ncompared\nto\nmethods\nlike\nDictionary\nse-\nlection\n[88],\nGAN\n[245],\nDR-DSN\n[429],\nBackprop-Grad\n[287],\netc\nin\nmost cases.\nTVSum [344]\nand\nCoSum\n[57] datasets.\nAvailable Code\nVideo\nsumma-\nrization\nin\nUl-\ntrasound\n[233]\n2020\nNot speci-\nﬁed\n2 actions:\nselecting\nand\nre-\njecting\nthe frame\nStates:\nframe\nlatent\nscores\nReward:\nRdet,\nRrep\nand Rdiv\nbidirectional\nLSTM [150] and\nKernel temporal\nsegmentation\n[298]\nNot spec-\niﬁed\nHigher\nF1-\nscores\nin\nsu-\npervised\nand\nunsupervised\nfashion\nas\ncompared\nto\nmethods\nlike\nFCSN [312] and\nDR-DSN [429].\nFetal\nUl-\ntrasound\n[179]\nVideo object segmentation using human-provided location priors have been capable of\nproducing promising results. An RL method for this task was proposed by [373], in which\nthe authors proposed MASK-RL, a multiagent RL framework for object segmentation in\nvideos. They proposed a weakly supervised method where the location priors were provided\nby the user in form of clicks using gamiﬁcation (Web game to collect location priors by\ndiﬀerent users) to support the segmentation and used a Gaussian ﬁlter to emphasize the\nareas. The segmentation network is fed a 12 channel input tensor that contained a sequence\nof video frames and their corresponding location priors (3 × 3 color channels + three gray-\nscale images). The authors used a fully convoluted DenseNet [147] with down-sampling and\nup-sampling similar to U-Net [313] and an LSTM [139] for the segmentation network. For\nthe RL method, the actor takes a series of steps over a frame divided into a grid of equal\nsize patches and makes the decision whether there is an object in the patch or not. In their\nMDP formulation the states consisted of the input frame, optical ﬂow (computed by [156])\nfrom the previous frame, patch from the previous iteration, and the episode location history,\nthe actions consisted of movement actions (up, down, left and right) and set action (action\nto place location prior at a random location on the patch), and two types of rewards one for\nset actions and one for movement actions were used. The reward was calculated using the\n53\nclicks generated by the game player.\nAction recognition is an important task in the computer vision ﬁeld which focuses on\ncategorizing the action that is being performed in the video frame. To address the problem\na deep progressive RL (DPRL) method for action recognition in skeleton-based videos was\nproposed by [357]. The authors proposed a method that distills the most informative frames\nand discards ambiguous frames by considering the quality of the frame and the relationship\nof the frame with the complete video along with a graph-based structure to map the human\nbody in form of joints and vertices. DPRL was utilized to ﬁlter out informative frames in a\nvideo and graph-based CNNs were used to learn the spatial dependency between the joints.\nThe approach consisted of two sub-networks, a frame distillation network (FDNet) to ﬁlter a\nﬁxed number of frames from input sequence using DPRL and GCNN to recognize the action\nlabels using output in form of a graphical structure by the FDNet. The authors modeled the\nproblem as an MDP where the state consisted of the concatenation of two tensors F and M,\nwhere F consisted of global information about the video and M consisted of the frames that\nwere ﬁltered, The actions which correspond to the output of FDNet were divided into three\ntypes: shifting to left, staying the same and shifting to the right, and the reward function\ncorresponded to the change in probability of categorizing the video equal to the ground truth\nclipped it between [-1 and 1] and is provided by GCNN to FDNet.\nVideo summarization is a useful yet diﬃcult task in the computer vision ﬁeld that involves\npredicting the object or the task that is being performed in a video. A DRL method for\nunsupervised video summarisation was proposed by [429], in which the authors proposed a\nDiversity-Representativeness reward system and a deep summarisation network (DSN) which\nwas capable of predicting a probability for each video frame that speciﬁed the likeliness of\nselecting the frame and then take actions to form video summaries. They used an encode-\ndecoder framework for the DSN where GoogLeNet [355] pre-trained on ImageNet [320] [72]\nwas used as an encoder and a bidirectional RNNs (BiRNNs) topped with a fully connected\n(FC) layer was used as a decoder. The authors modeled the problem as an MDP where\nthe action corresponded to the task of selecting or rejecting a frame.\nThey proposed a\nnovel Diversity-Representativeness Reward Function in their implementation, where diversity\nreward corresponded to the degree of dissimilarity among the selected frames in feature space,\nand representativeness reward measured how well the generated summary can represent the\noriginal video.\nFor the RNN unit they used an LSTM [139] to capture long-term video\ndependencies and used REINFORCE algorithm for training the policy function.\nAn improvement to [429] was proposed by [430], where the summarisation network was\nimplemented using Deep Q-learning (DQSN), and a trained classiﬁcation network was used\nto provide a reward for training the DQSN. The approach included using (Bi-GRU) bidirec-\ntional recurrent networks with a gated recurrent unit (GRU) [50] for both classiﬁcation and\nsummarisation network. The authors ﬁrst trained the classiﬁcation network using a super-\nvised classiﬁcation loss and then used the classiﬁcation network with ﬁxed weights for the\nclassiﬁcation of summaries generated by the summarisation network. The summarisation\nnetwork included an MDP-based framework in which states consisted of a sequence of video\nframes and actions reﬂected the task of either keeping the frame or discarding it. They used\n54\nFigure 16: DRL implementation for video summarization. For state a sequence of consecutive\nframes are used and the DRL agent decided whether to include the frame in the summary\nset that is used to predict video summary.\na structure similar to Duel-DQN where value function and advantage function are trained\ntogether. In their implementation, the authors considered 3 diﬀerent rewards: Global Recog-\nnisability reward using the classiﬁcation network with +1 as reward and -5 as punishment,\nLocal Relative Importance Reward for rewarding the action of accepting or rejecting a frame\nby summarisation network, and an Unsupervised Reward that is computed globally using\nthe unsupervised diversity-representativeness (DR) reward proposed in [429]. The authors\ntrained both the networks using the features generated by GoogLeNet [355] pre-trained on\nImageNet [72].\nA method for video summarization in Ultrasound using DRL was proposed by [233], in\nwhich the authors proposed a deep summarisation network in an encoder-decoder fashion\nand used a bidirectional LSTM (Bi-LSTM) [150] for sequential modeling. In their implemen-\ntation, the encoder-decoder convolution network extracted features from video frames and\nfed them into the Bi-LSTM. The RL network accepted states in form of latent scores from\nBi-LSTM and produced actions, where the actions consist of the task of including or discard-\ning the video frame inside the summary set that is used to produce video summaries. The\nauthors used three diﬀerent rewards Rdet, Rrep and Rdiv where Rdet evaluated the likelihood\nof a frame being a standard diagnostic plane, Rrep deﬁned the representativeness reward and\nRdiv was the diversity reward that evaluated the quality of the selected summary. They used\nKernel temporal segmentation (KTS) [298] for video summary generalization.\nVarious works associated with video analysis have been summarised and compared in\nTable 8 and a basic implementation of video summarization using DRL has been shown in\nFig. 16, where the states consist of a sequence of video frames. The DRL agent performs\n55\nactions to include or discard a frame from the summary set that is later used by the summa-\nrization network to predict video summary. Each research paper propose their own reward\nfunction for this application, for example [429] and [430] used diversity representativeness\nreward function and [233] used a combination of various reward functions.\n11\nOthers Applications\nObject manipulation refers to the task of handling and manipulating an object using a robot.\nA method for deformable object manipulation using RL was proposed by [250], where the\nauthors used a modiﬁed version of Deep Deterministic Policy Gradients (DDPG) [224]. They\nused the simulator Pybullet [63] for the environment where the observation consisted of a\n84 × 84 × 3 image, the state consists of joint angles and gripper positions and action of four\ndimensions: ﬁrst three for velocity and lasts for gripper velocity was used. The authors used\nsparse reward for the task that returns the reward at the completion of the task. They used\nthe algorithm to perform tasks such as folding and hanging cloth and got a success rate of\nup to 90%.\nVisual perception-based control refers to the task of controlling robotic systems using a\nvisual input. A virtual to real method for control using semantic segmentation was proposed\nby [142], in which the authors combined various modules such as, Perception module, con-\ntrol policy module, and a visual guidance module to perform the task. For the perception\nmodule, the authors directly used models such as DeepLab [46] and ICNet [424], pre-trained\non ADE20K [428] and Cityscape [61], and used the output of these model as the state for the\ncontrol policy module. They implemented the control policy module using the actor-critic\n[262] framework, where the action consisted of forward, turn right, and turn left. In their im-\nplementation, a reward of 0.001 is given at each time step. They used the Unity3D engine for\nthe environment and got higher success and lower collision rate than other implementations\nsuch as ResNet-A3C and Depth-A3C.\nAutomatic tracing of structures such as axons and blood vessels is an important yet chal-\nlenging task in the ﬁeld of biomedical imaging. A DRL method for sub-pixel neural tracking\nwas proposed by [65], where the authors used 2D grey-scale images as the environment. They\nconsidered a full resolution 11px × 11px window and a 21px × 21px window down-scaled to\n11px × 11px as state and the actions were responsible for moving the position of agent in 2D\nspace using continuous control for sub-pixel tracking because axons can be smaller then a\npixel. The authors used a reward that was calculated using the average integral of intensity\nbetween the agent’s current and next location, and the agent was penalized if it does not\nmove or changes directions more than once. They used an Actor-critic [262] framework to\nestimate value and policy functions.\nAn RL method for automatic diagnosis of acute appendicitis in abdominal CT images\nwas proposed by [8], in which the authors used RL to ﬁnd the location of the appendix and\nthen used a CNN classiﬁer to ﬁnd the likelihood of Acute Appendicitis, ﬁnally they deﬁned\na region of low-entropy (RLE) using the spatial representation of output scores to obtain\noptimal diagnosis scores. The authors considered the problem of appendix localization as\n56\nan MDP, where the state consisted of a 50 × 50 × 50 volume around the predicted appendix\nlocation, 6 actions (2 per axis) were used and the reward consisted of the change in distance\nbetween the predicted appendix location and actual appendix location across an action.\nThey utilized an Actor-critic [262] framework to estimate policy and value functions.\nTable 9: Comparing various other methods besides landmark de-\ntection, object detection, object tracking, image registration, image\nsegmentation, video analysis, that is associated with DRL\nApproaches\nYear\nTraining\nTechnique\nActions\nRemarks\nBackbone\nPerformance\nDatasets\nSource code\nObject\nmanip-\nulation\n[250]\n2018\nRainbow\nDDPG\n4\nactions:\n3\nfor\nve-\nlocity 1 for\ngripper ve-\nlocity\nState: joint an-\ngle and gripper\nposition.\nRe-\nward:\nat\nthe\nend of task.\nMulti\nlayer CNN\nSuccess rate up\nto 90%\nPybullet\n[63].\nCode\nVisual\nbased\ncontrol\n[142]\n2018\nActor-\ncritic\n(a3c) [262]\n3\nactions:\nforward,\nturn right\nand\nturn\nleft\nState:\noutput\nby\nbackbones.\nReward:\n0.001\nat\neach\ntime-\nstep.\nDeepLab\n[46]\nand\nICNet\n[424]\nHigher\nsuccess\nand lower col-\nlision rate then\nResNet-A3C\nand Depth-A3C\nUnity3D\nengine\nAutomatic\ntracing\n[65]\n2019\nActor-\ncritic\n[262]\n4 actions\nState:\n11px ×\n11px\nwindow.\nReward:\nav-\nerage\nintegral\nof\nintensity\nbetween\nthe\nagent’s\ncur-\nrent\nand\nnext\nlocation.\nMulti\nlayer CNN\nComparable\nconvergence\n%\nand\naverage\nerror\nas\ncom-\npared to other\nmethods\nlike\nVaa3D software\n[291] and APP2\nneuron\ntracer\n[403]\nSynthetic\nand\nmi-\ncroscopy\ndataset\n[24]\nAutomatic\ndiagnosis\n(RLE) [8]\n2019\nActor-\ncritic\n[262]\n6\nactions:\n2 per axis\nState: 50 × 50 ×\n50 volume. Re-\nward: change in\ndistance error.\nFully con-\nnected\nCNN\nHigher\nsen-\nsitivity\nand\nspeciﬁcity\nas\ncompared\nto\nonly CNN clas-\nsiﬁer and CNN\nclassiﬁer\nwith\nRL\nwithout\nRLE.\nAbdominal\nCT Scans\n57\nLearning\nto\npaint\n[149]\n2019\nActor-\ncritic with\nDDPG\nActions\ncontrol the\nstoke\npa-\nrameter:\nlocation,\nshape,\ncolor\nand\ntrans-\nparency\nState:\nRefer-\nence\nimage,\nDrawing\ncan-\nvas\nand\ntime\nstep.\nReward:\nchange\nin\ndis-\ncriminator score\n(calculated\nby\nWGAN-GP\n[117] across an\naction.\nGANs\n[113] to improve\nimage quality\nResNet18\n[133]\nAble\nto\nrepli-\ncate the original\nimages\nto\na\nlarge\nextent,\nand\nbetter\nresemblance\nto\nthe\norigi-\nnal\nimage\nas\ncompared\nto\nSPIRAL\n[98]\nwith same num-\nber\nof\nbrush\nstrokes.\nMNIST\n[202],\nSVHN\n[276],\nCelebA\n[235]\nand\nImageNet\n[320].\nCode\nGuiding\nmedical\nrobots\n[129]\n2020\nDouble-\nDQN,\nDuel-DQN\n5\nactions:\nup, down,\nleft,\nright\nand stop\nState:\nprobe\nposition.\nRe-\nward:\nMove\ncloser:\n0.05,\nMove\naway:\n-0.1,\nCorrect\nstop:\n1.0,\nIn-\ncorrect\nstop:\n-0.25.\nResNet18\n[133]\nHigher % of pol-\nicy\ncorrectness\nand reachability\nas compared to\nCNN Classiﬁer,\nwhere MS-DQN\nshowed the best\nresults\nUltrasound\nImages\nDataset.\nCode\nCrowd\ncounting\n[230]\n2020\nDQN\n9\nactions:\n-10,\n-5,\n-2, -1, +1,\n+2,\n+5,\n+10\nand\nend\nState:\nweight\nvector Wt and\nimage\nfeature\nvector\nFVI.\nReward:\nInter-\nmediate reward\nand\nending\nreward\nVGG16\n[340]\nLower/comparable\nmean\nsquared\nerror\n(MSE)\nand\nmean\nab-\nsolute\nerror\n(MAE) as com-\npared to other\nmethods\nlike\nDRSAN\n[232],\nPGCNet\n[412],\nMBTTBF [341],\nS-DCNet [405],\nCAN [234], etc.\nThe\nShang-\nhaiTech\n(SHT)\nDataset\n[423], The\nUCFCC50\nDataset\n[154]\nand\nThe UCF-\nQNRF\nDataset\n[155].\nCode\n58\nAutomated\nExposure\nbracketing\n[389]\n2020\nNot Speci-\nﬁed\nselecting\noptimal\nbracket-\ning\nfrom\ncandidates\nState: quality of\ngenerated HDR\nimage. Reward:\nimprovement in\npeak signal to\nnoise ratio\nAlexNet\n[188]\nHigher\npeak\nsignal to noise\nratio\nas\ncom-\npared to other\nmethods\nlike\nBarakat\n[22],\nPourreza-\nShahri\n[299],\nBeek [369], etc.\nProposed\nbench-\nmark\ndataset.\nCode/data\nUrban Au-\ntonomous\ndriving\n[361]\n2020\nRainbow-\nIQN\n36 or 108\nactions:\n(9 × 4) or\n(27 × 4),\n9/27 steer-\ning and 4\nthrottle\nState:\nenviron-\nment\nvariables\nlike traﬃc light,\npedestrians,\nposition\nwith\nrespect\nto\ncenter\nlane.\nReward: gener-\nated by CARLA\nwaypoint API\nResnet18\n[133]\nWon\nthe\n2019\ncamera\nonly\nCARLA\nchal-\nlenge [314].\nCARLA\nurban\ndriving\nsimulator\n[314]\nCode\nMitigating\nbias\nin\nFacial\nRecog-\nnition\n[382]\n2020\nDQN\n3\nac-\ntions:(Margin\nadjust-\nment)\nstaying\nthe\nsame,\nshifting to\na\nlarger\nvalue\nand\nshifting to\na\nsmaller\nvalue\nState:\nthe race\ngroup,\ncurrent\nadaptive\nmar-\ngin\nand\nbias\nbetween\nthe\nrace group and\nCaucasians.\nReward: change\nin the sum of\ninter-class\nand\nintra-class bias\nMulti-\nlayer CNN\nProposed\nal-\ngorithm\nhad\nhigher\nveriﬁca-\ntion\naccuracy\nas compared to\nother\nmethods\nsuch\nas\nCos-\nFace [379] and\nArcFace [73].\nRFW\n[383]\nand\nproposed\nnovel\ndatasets:\nBUPT-\nGlobalface\nand\nBUPT-\nBalancedface\nData\nAttention\nmecha-\nnism\nto\nimprove\nCNN per-\nformance\n[212]\n2020\nDQN [264]\nActions\nare\nweights\nfor\nevery\nlocation or\nchannel in\nthe feature\nmap.\nState:\nFeature\nmap\nat\neach\nintermediate\nlayer of model.\nReward:\npre-\ndicted\nby\na\nLSTM model.\nResNet-\n101 [133]\nImproves\nthe\nperformances\nof\n[144],\n[205]\nand [396], which\nattend on fea-\nture\nchannel,\nspatial-channel\nand\nstyle,\nrespectively\nImageNet\n[72]\n59\nFigure 17: A general DRL implementation for agent movement with visual inputs. The state\nis provided by the environment based on which the agent performs movement actions to get\na new state and a reward from the environment.\nPainting using an algorithm is a fantastic yet challenging task in the computer vision\nﬁeld. An automated painting method was proposed by [149], where the authors introduced\na model-based DRL technique for this task. The speciﬁed work involved using a neural\nrenderer in DRL, where the agent was responsible for making a decision about the position\nand color of each stroke, and making long-term decisions to organize those strokes into a\nvisual masterpiece. In this work, GANs [113] were employed to improve image quality at\npixel-level and DDPG [224] was utilized for determining the policy. The authors formulated\nthe problem as an MDP, where the state consisted of three parts: the target image I,\nthe canvas on which actions (paint strokes) are performed Ct, and the time step.\nThe\nactions corresponding to a set of parameters that controlled the position, shape, color, and\ntransparency of strokes, and for reward the WGAN with gradient penalty (WGAN-GP) [117]\nwas used to calculate the discriminator score between the target image I and the canvas Ct,\nand the change in discriminator score across an action (time-step) was used as the reward.\nThe agent that predicted the stroke parameters was trained in actor-critic [262] fashion with\nbackbone similar to Resnet18 [133], and the stroke parameters by the actor were used by\nthe neural renderer network to predict paint strokes. The network structure of the neural\nrenderer and discriminator consisted of multiple convolutions and fully connected blocks.\nA method for guiding medical robots using Ultrasound images with the help of DRL\nwas proposed by [129]. The authors treated the problem as an MDP where the agent takes\nthe Ultrasound images as input and estimates the state hence the problem became Partially\nobservable MDP (POMDP). They used Double-DQN and Duel-DQN for estimating Q-Values\nand ResNet18 [133] backbone for extracting feature to be used by the algorithm along with\nPrioritized Replay Memory. In their implementation the action space consisted of 8 actions\n(up, down, left, right, and stop), probe position as compared to the sacrum was used as the\nstate and the reward was calculated by considering the agent position as compared to the\ntarget (Move closer: 0.05, Move away: -0.1, Correct stop: 1.0, Incorrect stop: -0.25). In\n60\ntheir implementation, the authors proposed various architectures such as V-DQN, M-DQN,\nand MS-DQN for the task and performed experimentation on Ultrasound images.\nCrowd counting is considered a tricky task in computer vision and is even trickier for\nhumans. A DRL method for crowd counting was proposed by [230], where the authors used\nsequential decision making to approach the task through RL. In the speciﬁed work, the\nauthors proposed a DQN agent (LibraNet) based on the motivation of a weighing scale. In\ntheir implementation crowd counting was modeled using a weighing scale where the agent was\nresponsible for adding weights on one side of the scale sequentially to balance the crowded\nimage on the other side. The problem of adding weights on one side of the pan for balancing\nwas formulated as an MDP, where state consisted weight vector Wt and image feature vector\nFVI, and the actions space was deﬁned similar to scale weighing and money system [372]\ncontaining values (−10, −5, −2, −1, +1, +2, +5, +10, end).\nFor reinforcing the agent two\ndiﬀerent rewards: ending reward and intermediate reward were utilized, where ending reward\n(following [43]) was calculated by comparing the absolute value error between the ground-\ntruth count and the accumulated value with the error tolerance, and three counting speciﬁc\nrewards: force ending reward, guiding reward and squeezing reward were calculated for the\nintermediate rewards.\nExposure bracketing is a method used in digital photography, where one scene is captured\nusing multiple exposures for getting a high dynamic range (HDR) image. An RL method\nfor automated bracketing selection was proposed by [389]. For ﬂexible automated bracketing\nselection, an exposure bracketing selection network (EBSNet) was proposed for selecting\noptimal exposure bracketing and a multi-exposure fusion network (MEFNet) for generating\nan HDR image from selected exposure bracketing which consisted of 3 images. Since there is\nno ground truth for the exposure bracketing selection procedure, an RL scheme was utilized\nto train the agent (EBSNet). The authors also introduced a novel dataset consisting of a\nsingle auto-exposure image that was used as input to the EBSNet, 10 images with varying\nexposures from which EBSNet generated probability distribution for 120 possible candidate\nexposure bracketing (C3\n10) and a reference HDR image. The reward for EBSNet was deﬁned\nas the diﬀerence between peak signal-to-noise ratio between generated and reference HDR\nfor the current and previous iteration, and the MEFNet was trained by minimizing the\nCharbonnier loss [23]. For performing the action of bracketing selection ESBNet consisted\nof a semantic branch using AlexNet [188] for feature extraction, an illumination branch\nto understand the global and local illuminations by calculating a histogram of input and\nfeeding it to CNN layers, and a policy module to generate a probability distribution for the\ncandidate exposure bracketing from semantic and illumination branches. The neural network\nfor MEFNet was derived from HDRNet [103].\nAutonomous driving in an urban environment is a challenging task, because of a large\nnumber of environmental variables and constraints. A DRL approach to this problem was\nproposed by [361]. In their implementation, the authors proposed an end-to-end model-free\nRL method, where they introduced a novel technique called Implicit Aﬀordances. For the\nenvironment, the CARLA Simulator [80] was utilized, which provided the observations and\nthe training reward was obtained by using the CARLA waypoint API. In the novel implicit\n61\naﬀordances technique the training was broken into two phases, The ﬁrst phase included\nusing a Resnet18 [133] encoder to predict the state of various environment variables such\nas traﬃc light, pedestrians, position with respect to the center lane, etc., and the output\nfeatures were used as a state for the RL agent, For which a modiﬁed version of Rainbow-IQN\nApe-X [136] was used. CARLA simulator accepts actions in form of continuous steering and\nthrottle values, so to make it work with Rainbow-IQN which supports discrete actions, the\nauthors sampled steering values into 9 or 27 discrete values and throttle into 4 discrete values\n(including braking), making a total of 36(9 × 4) or 108(27 × 4) actions.\nRacial discrimination has been one of the hottest topics of the 21st century. To mitigate\nracial discrimination in facial recognition, [382] proposed a facial recognition method using\nskewness-aware RL. According to the authors, the reason for racial bias in facial recognition\nalgorithms can be either due to the data or due to the algorithm, so the authors provided\ntwo ethnicity-aware datasets, BUPT-Globalface and BUPT-Balancedface along with an RL\nbased race balanced network (RL-RBN). In their implementation, the authors formulated\nan MDP for adaptive margin policy learning where the state consisted of three parts: the\nrace group (0: Indian, 1: Asian, 2: African), current adaptive margin, and bias or the\nskewness between the race group and Caucasians. A DQN was used as a policy network\nthat performed three actions (staying the same, shifting to a larger value, and shifting to a\nsmaller value) to change the adaptive margin, and accepted reward in form of change in the\nsum of inter-class and intra-class bias.\nAttention mechanisms are currently gaining popularity because of their powerful ability\nin eliminating uninformative parts of the input to leverage the other parts having a more\nuseful information. Recently, attention mechanism has been integrated into typical CNN\nmodels at every individual layer to strengthen the intermediate outputs of each layer, in\nturn improving the ﬁnal predictions for recognition in images. This model is usually trained\nwith a weakly supervised method, however, this optimization method may lead to sub-\noptimal weights in the attention module. Hence, [212] proposed to train attention module\nby deep Q-learning with an LSTM model is trained to predict the reward, the whole process\nis called Deep REinforced Attention Learning (DREAL).\nVarious works speciﬁed here have been summarised and compared in Table 9 and general\nimplementation of a DRL method to control an agents movement in an environment has\nbeen shown in ﬁg 17 where state consists of an image frame provided by the environment,\nthe DRL agent predicts actions to move the agent in the environment providing next state\nand the reward is provided by the environment, for example, [142].\n12\nFuture Perspectives\n12.1\nChallenge Discussion\nDRL is a powerful framework, which has been successfully applied to various computer\nvision applications including landmark detection, object detection, object tracking, image\nregistration, image segmentation, video analysis, and other computer vision applications.\n62\nDRL has also demonstrated to be an eﬀective alternative for solving diﬃcult optimization\nproblems, including tuning parameters, selecting augmentation strategies, and neural archi-\ntecture search (NAS). However, most approaches, that we have reviewed, assume a stationary\nenvironment, from which observations are made. Take landmark detection as an instance,\nthe environment takes into account the image itself, and each state is deﬁned as an image\npatch consisting of the landmark location. In such a case, the environment is known while\nthe RL/DRL framework naturally accommodates a dynamic environment, that is the en-\nvironment itself evolves with the state and action. Realizing the full potential of DRL for\ncomputer vision requires solving several challenges. In this section, we would like to discuss\nthe challenges of DRL in computer vision for real-world systems.\n• Reward function: In most real-world applications, it is hard to deﬁne a speciﬁed\nreward function because it requires the knowledge from diﬀerent domains that may\nnot always be available. Thus, the intermediate rewards at each time step are not\nalways easily computed. Furthermore, a reward function with too long delay will make\ntraining diﬃcult. In contrast, assigning a reward for each action requires careful and\nmanual human design.\n• Continuous state and action space: Training an RL system on a continuous state\nand action space is challenging because most RL algorithms, i.e. Q learning, can only\ndeal with discrete states and discrete action space. To address this limitation, most\nexisting works discretize the continuous state and action space.\n• High-dimensional state and action space: Training Q-function on a high-dimensional\naction space is challenging. For this reason, existing works use low-dimensional param-\neterization, whose dimensions are typically less than 10 with an exception [184] that\nuses 15-D and 25-D to model 2D and 3D registration, respectively.\n• Environment is complicated: Almost all real-world systems, where we would want\nto deploy DRL/RL, are partially observable and non-stationary. Currently, the ap-\nproaches we have reviewed assume a stationary environment, from which observations\nare made. However, the DRL/RL framework naturally accommodates dynamic envi-\nronment, that is the environment itself evolves with the state and action. Furthermore,\nthose systems are often stochastic and noisy (action delay, sensor and action noise) as\ncompared to most simulated environments.\n• Training data requirement: RL/DRL requires a large amount of training data or\nexpert demonstrations. Large-scale datasets with annotations are expensive and hard\nto come by.\nMore details of challenges that embody diﬃculties to deploy RL/DRL in the real world\nare discussed in [82]. In this work, they designed a set of experiments and analyzed their\neﬀects on common RL agents. Open-sourcing an environmental suite, realworldrl-suite [83]\nis provided in this work as well.\n63\n12.2\nDRL Recent Advances\nSome advanced DRL approaches such as Inverse DRL, Multi-agent DRL, Meta DRL, and\nimitation learning are worth the attention and may promote new insights for many machine\nlearning and computer vision tasks.\n• Inverse DRL: DRL has been successfully applied into domains where the reward\nfunction is clearly deﬁned. However, this is limited in real-world applications because\nit requires knowledge from diﬀerent domains that may not always be available. Inverse\nDRL is one of the special cases of imitation learning.\nAn example is autonomous\ndriving, the reward function should be based on all factors such as driver’s behavior,\ngas consumption, time, speed, safety, driving quality, etc. In real-world scenario, it is\nexhausting and hard to control all these factors. Diﬀerent from DRL, inverse DRL [278],\n[4], [413], [86] a speciﬁc form of imitation learning [286], infers the reward function of an\nagent, given its policy or observed behavior, thereby avoiding a manual speciﬁcation of\nits reward function. In the same problem of autonomous driving, inverse RL ﬁrst uses a\ndataset collected from the human-generated driving and then approximates the reward\nfunction. Inverse RL has been successfully applied to many domains [4]. Recently, to\nanalyze complex human movement and control high-dimensional robot systems, [215]\nproposed an online inverse RL algorithm. [2] combined both RL and Inverse RL to\naddress planning problems in autonomous driving.\n• Multi-Agent DRL: Most of the successful DRL applications such as game[38], [376],\nrobotics[181], and autonomous driving [335], stock trading [206], social science [207],\netc., involve multiple players that requires a model with multi-agent. Take autonomous\ndriving as an instance, multi-agent DRL addresses the sequential decision-making prob-\nlem which involves many autonomous agents, each of which aims to optimize its own\nutility return by interacting with the environment and other agents [40]. Learning\nin a multi-agent scenario is more diﬃcult than a single-agent scenario because non-\nstationarity [135], multi-dimensionality [40], credit assignment [5], etc., depend on the\nmulti-agent DRL approach of either fully cooperative or fully competitive. The agents\ncan either collaborate to optimize a long-term utility or compete so that the utility\nis summed to zero. Recent work on Multi-Agent RL pays attention to learning new\ncriteria or new setup [348].\n• Meta DRL: As aforementioned, DRL algorithms consume large amounts of experience\nin order to learn an individual task and are unable to generalize the learned policy to\nnewer problems.\nTo alleviate the data challenge, Meta-RL algorithms [330], [380]\nare studied to enable agents to learn new skills from small amounts of experience.\nRecently, there is a research interest in meta RL [271], [119], [322], [303], [229], each\nusing a diﬀerent approach. For benchmarking and evaluation of meta RL algorithms,\n[415] presented Meta-world, which is an open-source simulator consisting of 50 distinct\nrobotic manipulation tasks.\n64\n• Imitation Learning: Imitation learning is close to learning from demonstrations\nwhich aims at training a policy to mimic an expert’s behavior given the samples col-\nlected from that expert.\nImitation learning is also considered as an alternative to\nRL/DRL to solve sequential decision-making problems. Besides inverse DRL, an im-\nitation learning approach as aforementioned, behavior cloning is another imitation\nlearning approach to train policy under supervised learning manner.\nBradly et al.\n[347] presented a method for unsupervised third-person imitation learning to observe\nhow other humans perform and infer the task. Building on top of Deep Deterministic\nPolicy Gradients and Hindsight Experience Replay, Nair et al. [272] proposed behavior\ncloning Loss to increase imitating the demonstrations. Besides Q-learning, Generative\nAdversarial Imitation Learning [364] proposes P-GAIL that integrates imitation learn-\ning into the policy gradient framework. P-GAIL considers both smoothness and causal\nentropy in policy update by utilizing Deep P-Network [365].\nConclusion\nDeep Reinforcement Learning (DRL) is nowadays the most popular technique for an artiﬁ-\ncial agent to learn closely optimal strategies by experiences. This paper aims to provide a\nstate-of-the-art comprehensive survey of DRL applications to a variety of decision-making\nproblems in the area of computer vision.\nIn this work, we ﬁrstly provided a structured\nsummarization of the theoretical foundations in Deep Learning (DL) including AutoEncoder\n(AE), Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recur-\nrent Neural Network (RNN). We then continued to introduce key techniques in RL research\nincluding model-based methods (value functions, transaction models, policy search, return\nfunctions) and model-free methods (value-based, policy-based, and actor-critic). Main tech-\nniques in DRL were thirdly presented under two categories of model-based and model-free\napproaches. We fourthly surveyed the broad-ranging applications of DRL methods in solv-\ning problems aﬀecting areas of computer vision, from landmark detection, object detection,\nobject tracking, image registration, image segmentation, video analysis, and many other ap-\nplications in the computer vision area. We ﬁnally discussed several challenges ahead of us in\norder to realize the full potential of DRL for computer vision. Some latest advanced DRL\ntechniques were included in the last discussion.\n65\nReferences\n[1] Model-based contextual policy search for data-eﬃcient generalization of robot skills.\nArtiﬁcial Intelligence, 247:415 – 439, 2017.\n[2] Advanced planning for autonomous vehicles using reinforcement learning and deep\ninverse reinforcement learning. Robotics and Autonomous Systems, 114:1 – 18, 2019.\n[3] Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobat-\nics through apprenticeship learning. The International Journal of Robotics Research,\n29(13):1608–1639, 2010.\n[4] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement\nlearning.\nIn Proceedings of the Twenty-First International Conference on Machine\nLearning, pages 1–8. Association for Computing Machinery, 2004.\n[5] Adrian K. Agogino and Kagan Tumer. Unifying temporal and structural credit as-\nsignment problems. In Proceedings of the Third International Joint Conference on\nAutonomous Agents and Multiagent Systems - Volume 2, page 980–987. IEEE Com-\nputer Society, 2004.\n[6] Narges Ahmidi, Lingling Tao, Shahin Sefati, Yixin Gao, Colin Lea, Benjamin Be-\njar Haro, Luca Zappella, Sanjeev Khudanpur, Ren´e Vidal, and Gregory D Hager.\nA dataset and benchmarks for segmentation and recognition of gestures in robotic\nsurgery. IEEE Transactions on Biomedical Engineering, 64(9):2025–2041, 2017.\n[7] Walid Abdullah Al and Il Dong Yun. Partial policy-based reinforcement learning for\nanatomical landmark localization in 3d medical images. IEEE transactions on medical\nimaging, 2019.\n[8] Walid Abdullah Al, Il Dong Yun, and Kyong Joon Lee.\nReinforcement learning-\nbased automatic diagnosis of acute appendicitis in abdominal ct.\narXiv preprint\narXiv:1909.00617, 2019.\n[9] Stephan Alaniz. Deep reinforcement learning with model learning and monte carlo tree\nsearch in minecraft. In Conference on Reinforcement Learning and Decision Making,\n2018.\n[10] Amir Alansary, Ozan Oktay, Yuanwei Li, Loic Le Folgoc, Benjamin Hou, Ghislain\nVaillant, Konstantinos Kamnitsas, Athanasios Vlontzos, Ben Glocker, Bernhard Kainz,\net al. Evaluating reinforcement learning agents for anatomical landmark detection.\nMedical image analysis, 53:156–164, 2019.\n[11] Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using\nreconstruction probability. Special Lecture on IE, 2(1):1–18, 2015.\n66\n[12] O. Andersson, F. Heintz, and P. Doherty.\nModel-based reinforcement learning in\ncontinuous environments using real-time constrained optimization. In AAAI, 2015.\n[13] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony\nBharath.\nA\nbrief\nsurvey\nof\ndeep\nreinforcement\nlearning.\narXiv preprint\narXiv:1708.05866, 2017.\n[14] S Avinash Ramakanth and R Venkatesh Babu. Seamseg: Video object segmentation\nusing patch seams. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 376–383, 2014.\n[15] Morgane Ayle, Jimmy Tekli, Julia El-Zini, Boulos El-Asmar, and Mariette Awad.\nBar-a reinforcement learning agent for bounding-box automated reﬁnement.\n[16] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz.\nGA3C: gpu-based A3C for deep reinforcement learning. CoRR, abs/1611.06256, 2016.\n[17] Boris Babenko, Ming-Hsuan Yang, and Serge Belongie. Visual tracking with online\nmultiple instance learning. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 983–990. IEEE, 2009.\n[18] Seung-Hwan Bae and Kuk-Jin Yoon. Robust online multi-object tracking based on\ntracklet conﬁdence and online discriminative appearance learning. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1218–1225,\n2014.\n[19] Seung-Hwan Bae and Kuk-Jin Yoon. Conﬁdence-based data association and discrimi-\nnative deep appearance learning for robust online multi-object tracking. IEEE trans-\nactions on pattern analysis and machine intelligence, 40(3):595–610, 2017.\n[20] J. Bagnell. Learning decision: Robustness, uncertainty, and approximation. 04 2012.\n[21] J. A. Bagnell and J. G. Schneider. Autonomous helicopter control using reinforce-\nment learning policy search methods. In Proceedings 2001 ICRA. IEEE International\nConference on Robotics and Automation (Cat. No.01CH37164), volume 2, pages 1615–\n1620, 2001.\n[22] Neil Barakat, A Nicholas Hone, and Thomas E Darcie.\nMinimal-bracketing sets\nfor high-dynamic-range image capture.\nIEEE Transactions on Image Processing,\n17(10):1864–1875, 2008.\n[23] Jonathan T Barron. A general and adaptive robust loss function. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 4331–4339,\n2019.\n67\n[24] Cher Bass, Pyry Helkkula, Vincenzo De Paola, Claudia Clopath, and Anil An-\nthony Bharath. Detection of axonal synapses in 3d two-photon images. PloS one,\n12(9):e0183309, 2017.\n[25] Miriam Bellver, Xavier Gir´o-i Nieto, Ferran Marqu´es, and Jordi Torres. Hierarchical\nobject detection with deep reinforcement learning. arXiv preprint arXiv:1611.03718,\n2016.\n[26] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies\nwith gradient descent is diﬃcult. IEEE Trans. Neural Networks, 5(2):157–166, 1994.\n[27] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad — a comprehensive\nreal-world dataset for unsupervised anomaly detection. In 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 9584–9592, 2019.\n[28] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking perfor-\nmance: the clear mot metrics. EURASIP Journal on Image and Video Processing,\n2008:1–10, 2008.\n[29] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS\nTorr. Fully-convolutional siamese networks for object tracking. In European conference\non computer vision, pages 850–865. Springer, 2016.\n[30] Shalabh Bhatnagar. An actor–critic algorithm with function approximation for dis-\ncounted cost constrained markov decision processes.\nSystems & Control Letters,\n59(12):760–766, 2010.\n[31] Shalabh Bhatnagar, Richard S. Sutton, Mohammad Ghavamzadeh, and Mark Lee.\nNatural actorˆa-critic algorithms. Automatica, 45(11):2471 – 2482, 2009.\n[32] Michael J Black and Yaser Yacoob.\nTracking and recognizing rigid and non-rigid\nfacial motions using local parametric models of image motion. In Proceedings of IEEE\ninternational conference on computer vision, pages 374–381. IEEE, 1995.\n[33] N Bloch, A Madabhushi, H Huisman, J Freymann, J Kirby, M Grauer, A Enquobahrie,\nC Jaﬀe, L Clarke, and K Farahani. Nci-isbi 2013 challenge: automated segmentation\nof prostate structures. The Cancer Imaging Archive, 370, 2015.\n[34] J. Boedecker, J. T. Springenberg, J. W¨ulﬁng, and M. Riedmiller. Approximate real-\ntime optimal control based on sparse gaussian process models. In 2014 IEEE Sym-\nposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),\npages 1–8, 2014.\n[35] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal network for\nobject detection. In Proceedings of the IEEE International Conference on Computer\nVision, Seoul, South Korea, 2019.\n68\n[36] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative interaction training\nfor segmentation editing networks. In International Workshop on Machine Learning\nin Medical Imaging, pages 363–370. Springer, 2018.\n[37] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n[38] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science,\n365(6456):885–890, 2019.\n[39] Antoine Buetti-Dinh, Vanni Galli, S˜A¶ren Bellenberg, Olga Ilie, Malte Herold, Stephan\nChristel, Mariia Boretska, Igor V. Pivkin, Paul Wilmes, Wolfgang Sand, Mario Vera,\nand Mark Dopson. Deep neural networks outperform human expert’s capacity in char-\nacterizing bioleaching bacterial bioﬁlm composition. Biotechnology Reports, 22:e00321,\n2019.\n[40] L. Busoniu, R. Babuska, and B. De Schutter. A comprehensive survey of multiagent\nreinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part\nC (Applications and Reviews), 38(2):156–172, 2008.\n[41] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix´e, Daniel Cre-\nmers, and Luc Van Gool. One-shot video object segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 221–230, 2017.\n[42] Yunliang Cai, Said Osman, Manas Sharma, Mark Landis, and Shuo Li. Multi-modality\nvertebra recognition in arbitrary views using 3d deformable hierarchical model. IEEE\ntransactions on medical imaging, 34(8):1676–1693, 2015.\n[43] Juan C Caicedo and Svetlana Lazebnik. Active object localization with deep rein-\nforcement learning. In Proceedings of the IEEE international conference on computer\nvision, pages 2488–2496, 2015.\n[44] D. Carrera, F. Manganini, G. Boracchi, and E. Lanzarone. Defect detection in sem im-\nages of nanoﬁbrous materials. IEEE Transactions on Industrial Informatics, 13(2):551–\n561, 2017.\n[45] Boyu Chen, Dong Wang, Peixia Li, Shuang Wang, and Huchuan Lu. Real-time’actor-\ncritic’tracking.\nIn Proceedings of the European Conference on Computer Vision\n(ECCV), pages 318–334, 2018.\n[46] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L\nYuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous\nconvolution, and fully connected crfs. IEEE transactions on pattern analysis and ma-\nchine intelligence, 40(4):834–848, 2017.\n[47] Yushi Chen, Xing Zhao, and Xiuping Jia. Spectral–spatial classiﬁcation of hyperspec-\ntral data based on deep belief network. IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing, 8(6):2381–2392, 2015.\n69\n[48] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-Hsuan Yang.\nSegﬂow:\nJoint learning for video object segmentation and optical ﬂow. In Proceedings of the\nIEEE international conference on computer vision, pages 686–695, 2017.\n[49] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu.\nGlobal contrast based salient region detection. IEEE transactions on pattern analysis\nand machine intelligence, 37(3):569–582, 2014.\n[50] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representa-\ntions using rnn encoder-decoder for statistical machine translation.\narXiv preprint\narXiv:1406.1078, 2014.\n[51] Kyunghyun Cho, Bart van Merrienboer, C¸aglar G¨ul¸cehre, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-\ndecoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n[52] Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris, and\nJin Young Choi. Attentional correlation ﬁlter network for adaptive visual tracking. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\n4807–4816, 2017.\n[53] Wongun Choi. Near-online multi-target tracking with aggregated local ﬂow descriptor.\nIn Proceedings of the IEEE international conference on computer vision, pages 3029–\n3037, 2015.\n[54] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, KyungHyun Cho, and Yoshua\nBengio. Attention-based models for speech recognition. CoRR, abs/1506.07503, 2015.\n[55] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai Yu. On-\nline multi-object tracking using cnn-based single object tracker with spatial-temporal\nattention mechanism. In Proceedings of the IEEE International Conference on Com-\nputer Vision, pages 4836–4845, 2017.\n[56] Wen-Hsuan Chu and Kris M. Kitani. Neural batch sampling with reinforcement learn-\ning for semi-supervised anomaly detection.\nIn European Conference on Computer\nVision, pages 751–766, 2020.\n[57] Wen-Sheng Chu, Yale Song, and Alejandro Jaimes. Video co-summarization: Video\nsummarization by visual co-occurrence. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 3584–3592, 2015.\n[58] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour,\nand Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization.\nCoRR, abs/1809.05214, 2018.\n70\n[59] Adam Coates, Pieter Abbeel, and Andrew Y. Ng. Apprenticeship learning for heli-\ncopter control. Commun. ACM, 52(7):97–105, July 2009.\n[60] Dorin Comaniciu, Visvanathan Ramesh, and Peter Meer. Real-time tracking of non-\nrigid objects using mean shift. In Proceedings IEEE Conference on Computer Vision\nand Pattern Recognition. CVPR 2000 (Cat. No. PR00662), volume 2, pages 142–149.\nIEEE, 2000.\n[61] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,\nRodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset\nfor semantic urban scene understanding. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3213–3223, 2016.\n[62] R´emi Coulom. Eﬃcient selectivity and backup operators in monte-carlo tree search.\nIn Proceedings of the 5th International Conference on Computers and Games, page\n72–83, 2006.\n[63] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for\ngames, robotics and machine learning. 2016.\n[64] Antonio Criminisi, Jamie Shotton, Duncan Robertson, and Ender Konukoglu. Regres-\nsion forests for eﬃcient anatomy detection and localization in ct studies. In Inter-\nnational MICCAI Workshop on Medical Computer Vision, pages 106–117. Springer,\n2010.\n[65] Tianhong Dai, Magda Dubois, Kai Arulkumaran, Jonathan Campbell, Cher Bass, Ben-\njamin Billot, Fatmatulzehra Uslu, Vincenzo De Paola, Claudia Clopath, and Anil An-\nthony Bharath. Deep reinforcement learning for subpixel neural tracking. In Interna-\ntional Conference on Medical Imaging with Deep Learning, pages 130–150, 2019.\n[66] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Eco:\nEﬃcient convolution operators for tracking. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 6638–6646, 2017.\n[67] Martin Danelljan, Gustav Hager, Fahad Shahbaz Khan, and Michael Felsberg. Learn-\ning spatially regularized correlation ﬁlters for visual tracking. In Proceedings of the\nIEEE international conference on computer vision, pages 4310–4318, 2015.\n[68] Kristopher De Asis, J Fernando Hernandez-Garcia, G Zacharias Holland, and\nRichard S Sutton. Multi-step reinforcement learning: A unifying algorithm. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, 2018.\n[69] Sandra Eliza Fontes De Avila, Ana Paula Brand˜ao Lopes, Antonio da Luz Jr, and\nArnaldo de Albuquerque Ara´ujo. Vsumm: A mechanism designed to produce static\nvideo summaries and a novel evaluation method. Pattern Recognition Letters, 32(1):56–\n68, 2011.\n71\n[70] Antonio de Marvao, Timothy JW Dawes, Wenzhe Shi, Christopher Minas, Niall G\nKeenan, Tamara Diamond, Giuliana Durighel, Giovanni Montana, Daniel Rueckert,\nStuart A Cook, et al. Population-based studies of myocardial hypertrophy: high reso-\nlution cardiovascular magnetic resonance atlases improve statistical power. Journal of\ncardiovascular magnetic resonance, 16(1):16, 2014.\n[71] M. P. Deisenroth, P. Englert, J. Peters, and D. Fox.\nMulti-task policy search for\nrobotics. In 2014 IEEE International Conference on Robotics and Automation (ICRA),\npages 3876–3881, 2014.\n[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248–255. Ieee, 2009.\n[73] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive\nangular margin loss for deep face recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 4690–4699, 2019.\n[74] Joachim Denzler and Dietrich WR Paulus. Active motion detection and object track-\ning. In Proceedings of 1st International Conference on Image Processing, volume 3,\npages 635–639. IEEE, 1994.\n[75] B. Depraetere, M. Liu, G. Pinte, I. Grondman, and R. Babu˚A¡ka. Comparison of\nmodel-free and model-based methods for time optimal hit control of a badminton\nrobot. Mechatronics, 24(8):1021 – 1030, 2014.\n[76] Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S Swaroop Vedula,\nGyusung I Lee, Mija R Lee, and Gregory D Hager. Recognizing surgical activities\nwith recurrent neural networks. In International conference on medical image comput-\ning and computer-assisted intervention, pages 551–558. Springer, 2016.\n[77] Piotr Doll´ar, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian detection:\nA benchmark. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npages 304–311. IEEE, 2009.\n[78] JeﬀDonahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini\nVenugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional\nnetworks for visual recognition and description. CoRR, abs/1411.4389, 2014.\n[79] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,\nVladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical ﬂow with convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 2758–2766, 2015.\n[80] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen\nKoltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938,\n2017.\n72\n[81] Yong Du, Wei Wang, and Liang Wang.\nHierarchical recurrent neural network for\nskeleton based action recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1110–1118, 2015.\n[82] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru,\nSven Gowal, and Todd Hester. An empirical investigation of the challenges of real-\nworld reinforcement learning, 2020.\n[83] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru,\nSven Gowal, and Todd Hester. An empirical investigation of the challenges of real-\nworld reinforcement learning. 2020.\n[84] Matteo Dunnhofer, Niki Martinel, Gian Luca Foresti, and Christian Micheloni. Visual\ntracking by means of deep reinforcement learning and an expert demonstrator. In\nProceedings of the IEEE International Conference on Computer Vision Workshops,\npages 0–0, 2019.\n[85] Chi Nhan Duong, Kha Gia Quach, Ibsa Jalata, Ngan Le, and Khoa Luu. Mobiface:\nA lightweight deep learning face recognition on mobile devices. In 2019 IEEE 10th\nInternational Conference on Biometrics Theory, Applications and Systems (BTAS),\npages 1–6. IEEE, 2019.\n[86] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Le, Marios Savvides, and\nTien D. Bui.\nLearning from longitudinal face demonstration–where tractable deep\nmodeling meets inverse reinforcement learning. 127(6–7), 2019.\n[87] A. El-Fakdi and M. Carreras. Policy gradient based reinforcement learning for real\nautonomous underwater cable tracking. In 2008 IEEE/RSJ International Conference\non Intelligent Robots and Systems, pages 3635–3640, 2008.\n[88] Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking at a few: Sparse\nmodeling for ﬁnding representative objects. In 2012 IEEE conference on computer\nvision and pattern recognition, pages 1600–1607. IEEE, 2012.\n[89] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scal-\nable object detection using deep neural networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2147–2154, 2014.\n[90] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew\nZisserman. The pascal visual object classes challenge 2007 (voc2007) results. 2007.\n[91] Mark Everingham and John Winn. The pascal visual object classes challenge 2012\n(voc2012) development kit. Pattern Analysis, Statistical Modelling and Computational\nLearning, Tech. Rep, 8, 2011.\n73\n[92] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu,\nChunyuan Liao, and Haibin Ling. Lasot: A high-quality benchmark for large-scale\nsingle object tracking. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5374–5383, 2019.\n[93] Heng Fan and Haibin Ling. Parallel tracking and verifying: A framework for real-time\nand high accuracy visual tracking. In Proceedings of the IEEE International Conference\non Computer Vision, pages 5486–5494, 2017.\n[94] Jialue Fan, Wei Xu, Ying Wu, and Yihong Gong. Human tracking using convolutional\nneural networks. IEEE Transactions on Neural Networks, 21(10):1610–1623, 2010.\n[95] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter\nAbbeel. Deep spatial autoencoders for visuomotor learning. In Danica Kragic, Anto-\nnio Bicchi, and Alessandro De Luca, editors, 2016 IEEE International Conference on\nRobotics and Automation, ICRA 2016, Stockholm, Sweden, May 16-21, 2016, pages\n512–519.\n[96] J Michael Fitzpatrick and Jay B West. The distribution of target registration error in\nrigid-body point-based registration. IEEE transactions on medical imaging, 20(9):917–\n927, 2001.\n[97] Vincent Fran¸cois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and\nJoelle Pineau.\nAn introduction to deep reinforcement learning.\narXiv preprint\narXiv:1811.12560, 2018.\n[98] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami, and Oriol Vinyals. Syn-\nthesizing programs for images using reinforced adversarial learning. arXiv preprint\narXiv:1804.01118, 2018.\n[99] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and Larry S Davis. Dynamic zoom-in\nnetwork for fast object detection in large images. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, pages 6926–6935, 2018.\n[100] Yixin Gao, S Swaroop Vedula, Carol E Reiley, Narges Ahmidi, Balakrishnan Varadara-\njan, Henry C Lin, Lingling Tao, Luca Zappella, Benjamın B´ejar, David D Yuh, et al.\nJhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset\nfor human motion modeling. In Miccai workshop: M2cai, volume 3, page 3, 2014.\n[101] Romane Gauriau, R´emi Cuingnet, David Lesage, and Isabelle Bloch.\nMulti-organ\nlocalization combining global-to-local regression and conﬁdence maps. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages\n337–344. Springer, 2014.\n[102] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti\nvision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3354–3361, 2012.\n74\n[103] Micha¨el Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoﬀ, and Fr´edo\nDurand. Deep bilateral learning for real-time image enhancement. ACM Transactions\non Graphics (TOG), 36(4):1–12, 2017.\n[104] Florin C Ghesu, Edward Krubasik, Bogdan Georgescu, Vivek Singh, Yefeng Zheng,\nJoachim Hornegger, and Dorin Comaniciu. Marginal space deep learning: eﬃcient\narchitecture for volumetric image parsing.\nIEEE transactions on medical imaging,\n35(5):1217–1228, 2016.\n[105] Florin-Cristian Ghesu, Bogdan Georgescu, Yefeng Zheng, Sasa Grbic, Andreas Maier,\nJoachim Hornegger, and Dorin Comaniciu. Multi-scale deep reinforcement learning for\nreal-time 3d-landmark detection in ct scans. IEEE transactions on pattern analysis\nand machine intelligence, 41(1):176–189, 2017.\n[106] M Giles. Mit technology review. Google researchers have reportedly achieved” quantum\nsupremacy” URL: https:/www.technologyreview. com/f, 614416, 2017.\n[107] Justin Girard and M Reza Emami. Concurrent markov decision processes for robot\nteam learning. Engineering applications of artiﬁcial intelligence, 39:223–234, 2015.\n[108] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on\ncomputer vision, pages 1440–1448, 2015.\n[109] Ross Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik. Rich feature hierar-\nchies for accurate object detection and semantic segmentation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 580–587, 2014.\n[110] Georgia Gkioxari, Ross Girshick, and Jitendra Malik. Contextual action recognition\nwith r* cnn. In Proceedings of the IEEE international conference on computer vision,\npages 1080–1088, 2015.\n[111] Vikash Goel, Jameson Weng, and Pascal Poupart. Unsupervised video object segmen-\ntation for deep reinforcement learning. In Advances in Neural Information Processing\nSystems, pages 5683–5694, 2018.\n[112] Abel Gonzalez-Garcia, Alexander Vezhnevets, and Vittorio Ferrari. An active search\nstrategy for eﬃcient object class detection. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 3022–3031, 2015.\n[113] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In\nAdvances in neural information processing systems, pages 2672–2680, 2014.\n[114] Leo Grady. Random walks for image segmentation. IEEE transactions on pattern\nanalysis and machine intelligence, 28(11):1768–1783, 2006.\n75\n[115] Alex Graves, Abdel-rahman Mohamed, and Geoﬀrey E. Hinton. Speech recognition\nwith deep recurrent neural networks. CoRR, abs/1303.5778, 2013.\n[116] Albert Gubern-M´erida, Robert Mart´ı, Jaime Melendez, Jakob L Hauth, Ritse M Mann,\nNico Karssemeijer, and Bram Platel. Automated localization of breast cancer in dce-\nmri. Medical image analysis, 20(1):265–274, 2015.\n[117] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. Improved training of wasserstein gans. In Advances in neural information\nprocessing systems, pages 5767–5777, 2017.\n[118] Minghao Guo, Jiwen Lu, and Jie Zhou. Dual-agent deep reinforcement learning for\ndeformable face tracking. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 768–783, 2018.\n[119] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.\nMeta-reinforcement learning of structured exploration strategies. In Advances in Neu-\nral Information Processing Systems, pages 5302–5311, 2018.\n[120] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization and\nrecognition of indoor scenes from rgb-d images. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 564–571, 2013.\n[121] Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, and Jitendra Malik. Learning rich fea-\ntures from rgb-d images for object detection and segmentation. In European conference\non computer vision, pages 345–360. Springer, 2014.\n[122] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating\nsummaries from user videos. In European conference on computer vision, pages 505–\n520. Springer, 2014.\n[123] Seyed Hamid Rezatoﬁghi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony Dick, and\nIan Reid. Joint probabilistic data association revisited. In Proceedings of the IEEE\ninternational conference on computer vision, pages 3047–3055, 2015.\n[124] Junwei Han, Le Yang, Dingwen Zhang, Xiaojun Chang, and Xiaodan Liang. Reinforce-\nment cutting-agent learning for video object segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 9080–9089, 2018.\n[125] Robert M Haralick and Linda G Shapiro. Image segmentation techniques. Computer\nvision, graphics, and image processing, 29(1):100–132, 1985.\n[126] Sam Hare, Stuart Golodetz, Amir Saﬀari, Vibhav Vineet, Ming-Ming Cheng, Stephen L\nHicks, and Philip HS Torr. Struck: Structured output tracking with kernels. IEEE\ntransactions on pattern analysis and machine intelligence, 38(10):2096–2109, 2015.\n76\n[127] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Jitendra Malik. Simultaneous\ndetection and segmentation. In European Conference on Computer Vision, pages 297–\n312. Springer, 2014.\n[128] Bharath Hariharan, Pablo Arbel´aez, Ross Girshick, and Jitendra Malik. Hypercolumns\nfor object segmentation and ﬁne-grained localization.\nIn Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 447–456, 2015.\n[129] Hannes Hase, Mohammad Farid Azampour, Maria Tirindelli, Magdalini Paschali, Wal-\nter Simson, Emad Fatemizadeh, and Nassir Navab. Ultrasound-guided robotic naviga-\ntion with deep reinforcement learning. arXiv preprint arXiv:2003.13321, 2020.\n[130] Hado V Hasselt. Double q-learning. In Advances in neural information processing\nsystems, pages 2613–2621, 2010.\n[131] Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for partially ob-\nservable mdps. CoRR, abs/1507.06527, 2015.\n[132] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick.\nMask r-cnn.\nIn\nProceedings of the IEEE international conference on computer vision, pages 2961–2969,\n2017.\n[133] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n[134] Jo˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking\nwith kernelized correlation ﬁlters. IEEE transactions on pattern analysis and machine\nintelligence, 37(3):583–596, 2014.\n[135] Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A\nsurvey of learning in multiagent environments: Dealing with non-stationarity. CoRR,\nabs/1707.09183, 2017.\n[136] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will\nDabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Com-\nbining improvements in deep reinforcement learning. arXiv preprint arXiv:1710.02298,\n2017.\n[137] Todd Hester, Michael Quinlan, and Peter Stone. A real-time model-based reinforce-\nment learning architecture for robot control. CoRR, abs/1105.1749, 2011.\n[138] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston.\nThe goldilocks\nprinciple: Reading children’s books with explicit memory representations.\nCoRR,\nabs/1511.02301, 2015.\n77\n[139] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural compu-\ntation, 9(8):1735–1780, 1997.\n[140] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recovering surface layout from an\nimage. International Journal of Computer Vision, 75(1):151–172, 2007.\n[141] James B. Holliday and Ngan T.H. Le.\nFollow then forage exploration: Improving\nasynchronous advantage actor critic.\nInternational Conference on Soft Computing,\nArtiﬁcial Intelligence and Applications (SAI 2020), pages 107–118, 2020.\n[142] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang,\nHsuan-Kung Yang, Brian Hsi-Lin Ho, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching\nHsiao, et al.\nVirtual-to-real: Learning to control in visual semantic segmentation.\narXiv preprint arXiv:1802.00285, 2018.\n[143] Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online multi-\nobject tracking via structural constraint event aggregation. In Proceedings of the IEEE\nConference on computer vision and pattern recognition, pages 1392–1400, 2016.\n[144] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 7132–7141, 2018.\n[145] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly learning\nheterogeneous features for rgb-d activity recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages 5344–5352, 2015.\n[146] Weiming Hu, Xi Li, Wenhan Luo, Xiaoqin Zhang, Stephen Maybank, and Zhongfei\nZhang. Single and multiple object tracking using log-euclidean riemannian subspace\nand block-division appearance model. IEEE transactions on pattern analysis and ma-\nchine intelligence, 34(12):2420–2440, 2012.\n[147] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely\nconnected convolutional networks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700–4708, 2017.\n[148] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity bench-\nmark for generic object tracking in the wild. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2019.\n[149] Zhewei Huang, Wen Heng, and Shuchang Zhou. Learning to paint with model-based\ndeep reinforcement learning. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 8709–8718, 2019.\n[150] Zhiheng Huang, Wei Xu, and Kai Yu.\nBidirectional lstm-crf models for sequence\ntagging. arXiv preprint arXiv:1508.01991, 2015.\n78\n[151] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning on\nlie groups for skeleton-based action recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 6099–6108, 2017.\n[152] Gabriel Efrain Humpire-Mamani, Arnaud Arindra Adiyoso Setio, Bram van Ginneken,\nand Colin Jacobs. Eﬃcient organ localization using multi-label convolutional neural\nnetworks in thorax-abdomen ct scans. Physics in Medicine & Biology, 63(8):085003,\n2018.\n[153] Luis Ibanez, Will Schroeder, Lydia Ng, and Josh Cates. The itk software guide: up-\ndated for itk version 2.4, 2005.\n[154] Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source multi-\nscale counting in extremely dense crowd images. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2547–2554, 2013.\n[155] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed,\nNasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map esti-\nmation and localization in dense crowds. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 532–546, 2018.\n[156] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and\nThomas Brox. Flownet 2.0: Evolution of optical ﬂow estimation with deep networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 2462–2470, 2017.\n[157] Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[158] Cliﬀord R Jack Jr, Matt A Bernstein, Nick C Fox, Paul Thompson, Gene Alexander,\nDanielle Harvey, Bret Borowski, Paula J Britson, Jennifer L. Whitwell, Chadwick\nWard, et al. The alzheimer’s disease neuroimaging initiative (adni): Mri methods.\nJournal of Magnetic Resonance Imaging: An Oﬃcial Journal of the International\nSociety for Magnetic Resonance in Medicine, 27(4):685–691, 2008.\n[159] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer net-\nworks. In Advances in neural information processing systems, pages 2017–2025, 2015.\n[160] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for text spot-\nting. In European conference on computer vision, pages 512–528. Springer, 2014.\n[161] Arjit Jain, Alexander Powers, and Hans J Johnson. Robust automatic multiple land-\nmark detection. In 2020 IEEE 17th International Symposium on Biomedical Imaging\n(ISBI), pages 1178–1182. IEEE, 2020.\n[162] Suyog Dutt Jain and Kristen Grauman. Supervoxel-consistent foreground propagation\nin video. In European conference on computer vision, pages 656–671. Springer, 2014.\n79\n[163] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to combine\nmotion and appearance for fully automatic segmentation of generic objects in videos.\nIn 2017 IEEE conference on computer vision and pattern recognition (CVPR), pages\n2117–2126. IEEE, 2017.\n[164] Varun Jampani, Raghudeep Gadde, and Peter V Gehler. Video propagation networks.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 451–461, 2017.\n[165] Won-Dong Jang and Chang-Su Kim. Online video object segmentation via convolu-\ntional trident network. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5849–5858, 2017.\n[166] Simon J´egou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua Bengio.\nThe one hundred layers tiramisu: Fully convolutional densenets for semantic segmenta-\ntion. In Proceedings of the IEEE conference on computer vision and pattern recognition\nworkshops, pages 11–19, 2017.\n[167] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang.\nModel-based reinforce-\nment learning with value-targeted regression. In Proceedings of the 2nd Conference on\nLearning for Dynamics and Control, volume 120 of Proceedings of Machine Learning\nResearch, pages 666–686, The Cloud, 10–11 Jun 2020.\n[168] Ming-xin Jiang, Chao Deng, Zhi-geng Pan, Lan-fang Wang, and Xing Sun. Multiobject\ntracking in videos based on lstm and deep reinforcement learning. Complexity, 2018,\n2018.\n[169] Mingxin Jiang, Tao Hai, Zhigeng Pan, Haiyan Wang, Yinjie Jia, and Chao Deng. Multi-\nagent deep reinforcement learning for multi-object tracker. IEEE Access, 7:32400–\n32407, 2019.\n[170] Zequn Jie, Xiaodan Liang, Jiashi Feng, Xiaojie Jin, Wen Lu, and Shuicheng Yan.\nTree-structured reinforcement learning for sequential object localization. In Advances\nin Neural Information Processing Systems, pages 127–135, 2016.\n[171] Oscar Jimenez-del Toro, Henning M¨uller, Markus Krenn, Katharina Gruenberg, Ab-\ndel Aziz Taha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-Rodr´ıguez, Or-\ncun Goksel, Andr´as Jakab, et al. Cloud-based evaluation of anatomical structure seg-\nmentation and landmark detection algorithms: Visceral anatomy benchmarks. IEEE\ntransactions on medical imaging, 35(11):2459–2475, 2016.\n[172] V Craig Jordan.\nLong-term adjuvant tamoxifen therapy for breast cancer. Breast\ncancer research and treatment, 15(3):125–136, 1990.\n[173] Yeong Jun Koh and Chang-Su Kim. Primary object segmentation in videos based\non region augmentation and reduction.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3442–3450, 2017.\n80\n[174] Zdenek Kalal, Krystian Mikolajczyk, and Jiri Matas.\nTracking-learning-detection.\nIEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422,\n2011.\n[175] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. Asso-\nciation for Computational Linguistics, October 2013.\n[176] Micha l Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech\nJa´skowski.\nVizdoom: A doom-based ai research platform for visual reinforcement\nlearning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG),\npages 1–8. IEEE, 2016.\n[177] Du Yong Kim and Moongu Jeon. Data fusion of radar and image measurements for\nmulti-object tracking via kalman ﬁltering. Information Sciences, 278:641–652, 2014.\n[178] Kye Kyung Kim, Soo Hyun Cho, Hae Jin Kim, and Jae Yeon Lee. Detecting and\ntracking moving object using an active camera. In The 7th International Conference\non Advanced Communication Technology, 2005, ICACT 2005., volume 2, pages 817–\n820. IEEE, 2005.\n[179] Donna Kirwan.\nNhs fetal anomaly screening programme.\nNational Standards and\nGuidance for England, 18(0), 2010.\n[180] Stefan Klein, Marius Staring, Keelin Murphy, Max A Viergever, and Josien PW Pluim.\nElastix: a toolbox for intensity-based medical image registration. IEEE transactions\non medical imaging, 29(1):196–205, 2009.\n[181] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics:\nA survey. The International Journal of Robotics Research, 32(11):1238–1274, 2013.\n[182] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural\ninformation processing systems, pages 1008–1014, 2000.\n[183] Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Eﬃcient inference in fully connected crfs\nwith gaussian edge potentials. In Advances in neural information processing systems,\npages 109–117, 2011.\n[184] Julian Krebs, Tommaso Mansi, Herv´e Delingette, Li Zhang, Florin C Ghesu, Shun\nMiao, Andreas K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-\nrigid registration through agent-based action learning. In International Conference\non Medical Image Computing and Computer-Assisted Intervention, pages 344–352.\nSpringer, 2017.\n[185] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pﬂugfelder, Luka\nCehovin Zajc, Tomas Vojir, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,\net al. The sixth visual object tracking vot2018 challenge results. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages 0–0, 2018.\n81\n[186] Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin, Gustavo\nFernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, and Roman Pﬂugfelder. The\nvisual object tracking vot2015 challenge results. In Proceedings of the IEEE interna-\ntional conference on computer vision workshops, pages 1–23, 2015.\n[187] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks.\nIn Advances in neural information processing\nsystems, pages 1097–1105, 2012.\n[188] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\n[189] A. Kupcsik, M. Deisenroth, Jan Peters, and G. Neumann. Data-eﬃcient generalization\nof robot skills with contextual policy search. In AAAI, 2013.\n[190] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-\nensemble trust-region policy optimization. 02 2018.\n[191] Ngan Le, Trung Le, Kashu Yamazaki, Toan Duc Bui, Khoa Luu, and Marios Savides.\nOﬀset curves loss for imbalanced problem in medical segmentation. arXiv preprint\narXiv:2012.02463, 2020.\n[192] Ngan Le, Kashu Yamazaki, Dat Truong, Kha Gia Quach, and Marios Savvides. A\nmulti-task contextual atrous residual network for brain tumor detection & segmenta-\ntion. arXiv preprint arXiv:2012.02073, 2020.\n[193] T Hoang Ngan Le, Chi Nhan Duong, Ligong Han, Khoa Luu, Kha Gia Quach, and\nMarios Savvides. Deep contextual recurrent residual networks for scene labeling. Pat-\ntern Recognition, 80:32–41, 2018.\n[194] T Hoang Ngan Le, Kha Gia Quach, Khoa Luu, Chi Nhan Duong, and Marios Sav-\nvides. Reformulating level sets as deep recurrent neural network approach to semantic\nsegmentation. IEEE Transactions on Image Processing, 27(5):2393–2407, 2018.\n[195] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Tem-\nporal convolutional networks for action segmentation and detection. In proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 156–165,\n2017.\n[196] Colin Lea, Austin Reiter, Ren´e Vidal, and Gregory D Hager. Segmental spatiotempo-\nral cnns for ﬁne-grained action segmentation. In European Conference on Computer\nVision, pages 36–52. Springer, 2016.\n[197] Colin Lea, Ren´e Vidal, and Gregory D Hager. Learning convolutional action primitives\nfor ﬁne-grained action recognition. In 2016 IEEE international conference on robotics\nand automation (ICRA), pages 1642–1649. IEEE, 2016.\n82\n[198] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional\nnetworks: A uniﬁed approach to action segmentation. In European Conference on\nComputer Vision, pages 47–54. Springer, 2016.\n[199] Laura Leal-Taix´e, Cristian Canton-Ferrer, and Konrad Schindler. Learning by tracking:\nSiamese cnn for robust target association. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition Workshops, pages 33–40, 2016.\n[200] Laura Leal-Taix´e, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, and Silvio\nSavarese. Learning an image-based motion context for multiple people tracking. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 3542–3549, 2014.\n[201] Laura Leal-Taix´e, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler.\nMotchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint\narXiv:1504.01942, 2015.\n[202] Yann LeCun.\nThe mnist database of handwritten digits.\nhttp://yann. lecun.\ncom/exdb/mnist/, 1998.\n[203] Yann LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller.\nEﬃcient\nbackprop. In Neural networks: Tricks of the trade, pages 9–50. Springer, 1998.\n[204] Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for\nback-propagation.\nIn Proceedings of the 1988 connectionist models summer school,\npages 21–28. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.\n[205] Hyunjae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration\nmodule for convolutional neural networks. pages 1854–1862, 10 2019.\n[206] Jae Won Lee, Jonghun Park, Jangmin O, Jongwoo Lee, and Euyseok Hong. A multi-\nagent approach to q-learning for daily stock trading. Trans. Sys. Man Cyber. Part A,\n37(6):864–877, November 2007.\n[207] Joel Z. Leibo, Vin´ıcius Flores Zambaldi, Marc Lanctot, Janusz Marecki, and Thore\nGraepel. Multi-agent reinforcement learning in sequential social dilemmas. CoRR,\nabs/1702.03037, 2017.\n[208] Sergey Levine and Vladlen Koltun. Learning complex neural network policies with tra-\njectory optimization. In Proceedings of the 31st International Conference on Machine\nLearning, pages 829–837, 2014.\n[209] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual\ntracking with siamese region proposal network. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 8971–8980, 2018.\n83\n[210] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang. GS3D: an\neﬃcient 3d object detection framework for autonomous driving. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,\nJune 16-20, 2019, pages 1019–1028. Computer Vision Foundation / IEEE, 2019.\n[211] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu. Co-occurrence feature learning\nfrom skeleton data for action recognition and detection with hierarchical aggregation.\narXiv preprint arXiv:1804.06055, 2018.\n[212] Duo Li and Qifeng Chen. Deep reinforced attention learning for quality-aware visual\nrecognition. In European Conference on Computer Vision, pages 493–509, 2020.\n[213] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. arXiv\npreprint arXiv:1503.08663, 2015.\n[214] Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder\nfor paragraphs and documents. CoRR, abs/1506.01057, 2015.\n[215] K. Li, M. Rath, and J. W. Burdick. Inverse reinforcement learning via function ap-\nproximation for clinical motion analysis. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pages 610–617, 2018.\n[216] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit\napproach to personalized news article recommendation. In Proceedings of the 19th\ninternational conference on World wide web, pages 661–670, 2010.\n[217] Peixia Li, Dong Wang, Lijun Wang, and Huchuan Lu. Deep visual tracking: Review\nand experimental comparison. Pattern Recognition, 76:323–338, 2018.\n[218] Yingbo Li and Bernard Merialdo. Multi-video summarization based on video-mmr. In\n11th International Workshop on Image Analysis for Multimedia Interactive Services\nWIAMIS 10, pages 1–4. IEEE, 2010.\n[219] Yuanwei Li, Amir Alansary, Juan J Cerrolaza, Bishesh Khanal, Matthew Sinclair,\nJacqueline Matthew, Chandni Gupta, Caroline Knight, Bernhard Kainz, and Daniel\nRueckert. Fast multiple landmark localisation using a patch-based iterative network.\nIn International Conference on Medical Image Computing and Computer-Assisted In-\ntervention, pages 563–571. Springer, 2018.\n[220] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information for vi-\nsual tracking: Algorithms and benchmark. IEEE Transactions on Image Processing,\n24(12):5630–5644, 2015.\n[221] Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen, Tommaso Mansi,\nand Dorin Comaniciu. An artiﬁcial agent for robust image registration. In Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, 2017.\n84\n[222] Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng\nWang, and Ya Zhang. Iteratively-reﬁned interactive 3d medical image segmentation\nwith multi-agent reinforcement learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9394–9402, 2020.\n[223] Timothy P. Lillicrap, Jonathan J. Hunt, Alexand er Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep rein-\nforcement learning. arXiv e-prints, page arXiv:1509.02971, September 2015.\n[224] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep rein-\nforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n[225] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss\nfor dense object detection. In Proceedings of the IEEE international conference on\ncomputer vision, pages 2980–2988, 2017.\n[226] Tony Lindeberg. Scale-space theory in computer vision, volume 256. Springer Science\n& Business Media, 2013.\n[227] Geert Litjens, Robert Toth, Wendy van de Ven, Caroline Hoeks, Sjoerd Kerkstra,\nBram van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang,\net al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge.\nMedical image analysis, 18(2):359–373, 2014.\n[228] Daochang Liu and Tingting Jiang. Deep reinforcement learning for surgical gesture\nsegmentation and classiﬁcation. In International conference on medical image comput-\ning and computer-assisted intervention, pages 247–255. Springer, 2018.\n[229] Hao Liu, Richard Socher, and Caiming Xiong.\nTaming maml: Eﬃcient unbiased\nmeta-reinforcement learning. In International Conference on Machine Learning, pages\n4061–4071, 2019.\n[230] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua Shen.\nWeighing counts: Sequential crowd counting by reinforcement learning. 2020.\n[231] Lijie Liu, Chufan Wu, Jiwen Lu, Lingxi Xie, Jie Zhou, and Qi Tian. Reinforced axial\nreﬁnement network for monocular 3d object detection. In European Conference on\nComputer Vision ECCV, pages 540–556, 2020.\n[232] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang, and Liang Lin. Crowd count-\ning using deep recurrent spatial-aware network. arXiv preprint arXiv:1807.00601, 2018.\n[233] Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel Rueckert, and\nBernhard Kainz. Ultrasound video summarization using deep reinforcement learning.\narXiv preprint arXiv:2005.09531, 2020.\n85\n[234] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 5099–5108, 2019.\n[235] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes\nin the wild. In Proceedings of the IEEE international conference on computer vision,\npages 3730–3738, 2015.\n[236] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for\nsemantic segmentation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3431–3440, 2015.\n[237] Marco Lorenzi, Nicholas Ayache, Giovanni B Frisoni, Xavier Pennec, Alzheimer’s Dis-\nease Neuroimaging Initiative (ADNI, et al. Lcc-demons: a robust and accurate sym-\nmetric diﬀeomorphic registration algorithm. NeuroImage, 81:470–483, 2013.\n[238] Tayebeh Lotﬁ, Lisa Tang, Shawn Andrews, and Ghassan Hamarneh. Improving prob-\nabilistic image registration via reinforcement learning and uncertainty evaluation. In\nInternational Workshop on Machine Learning in Medical Imaging, pages 187–194.\nSpringer, 2013.\n[239] David G Lowe. Distinctive image features from scale-invariant keypoints. International\njournal of computer vision, 60(2):91–110, 2004.\n[240] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, and Yizhou\nWang. End-to-end active object tracking via reinforcement learning. arXiv preprint\narXiv:1705.10561, 2017.\n[241] Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Ad-\ndressing the rare word problem in neural machine translation. CoRR, abs/1410.8206,\n2014.\n[242] Khoa Luu, Chenchen Zhu, Chandrasekhar Bhagavatula, T Hoang Ngan Le, and Marios\nSavvides.\nA deep learning approach to joint face detection and segmentation.\nIn\nAdvances in Face Detection and Facial Image Analysis, pages 1–12. Springer, 2016.\n[243] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical convo-\nlutional features for visual tracking. In Proceedings of the IEEE international confer-\nence on computer vision, pages 3074–3082, 2015.\n[244] Kai Ma, Jiangping Wang, Vivek Singh, Birgi Tamersoy, Yao-Jen Chang, Andreas\nWimmer, and Terrence Chen. Multimodal image registration with deep context re-\ninforcement learning. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 240–248. Springer, 2017.\n86\n[245] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video sum-\nmarization with adversarial lstm networks. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition, pages 202–211, 2017.\n[246] Gabriel Maicas, Gustavo Carneiro, Andrew P Bradley, Jacinto C Nascimento, and\nIan Reid. Deep reinforcement learning for active breast lesion detection from dce-\nmri. In International conference on medical image computing and computer-assisted\nintervention, pages 665–673. Springer, 2017.\n[247] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. Deep captioning with\nmultimodal recurrent neural networks (m-rnn). CoRR, abs/1412.6632, 2014.\n[248] Nicolas M¨arki, Federico Perazzi, Oliver Wang, and Alexander Sorkine-Hornung. Bilat-\neral space video segmentation. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 743–751, 2016.\n[249] T. Martinez-Marin and T. Duckett.\nFast reinforcement learning for vision-guided\nmobile robots. In Proceedings of the 2005 IEEE International Conference on Robotics\nand Automation, pages 4170–4175, 2005.\n[250] Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement learning\nfor deformable object manipulation. arXiv preprint arXiv:1806.07851, 2018.\n[251] Stefan Mathe, Aleksis Pirinen, and Cristian Sminchisescu. Reinforcement learning for\nvisual object detection. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2894–2902, 2016.\n[252] George K Matsopoulos, Nicolaos A Mouravliansky, Konstantinos K Delibasis, and\nKonstantina S Nikita. Automatic retinal image registration scheme using global opti-\nmization techniques. IEEE Transactions on Information Technology in Biomedicine,\n3(1):47–60, 1999.\n[253] Darryl McClymont, Andrew Mehnert, Adnan Trakic, Dominic Kennedy, and Stuart\nCrozier. Fully automatic lesion segmentation in breast mri using mean-shift and graph-\ncuts on a region adjacency graph. Journal of Magnetic Resonance Imaging, 39(4):795–\n804, 2014.\n[254] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan\nFarahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest,\net al. The multimodal brain tumor image segmentation benchmark (brats). IEEE\ntransactions on medical imaging, 34(10):1993–2024, 2014.\n[255] Shun Miao, Rui Liao, Marcus Pﬁster, Li Zhang, and Vincent Ordy. System and method\nfor 3-d/3-d registration between non-contrast-enhanced cbct and contrast-enhanced ct\nfor abdominal aortic aneurysm stenting. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pages 380–387. Springer, 2013.\n87\n[256] Shun Miao, Z Jane Wang, and Rui Liao. A cnn regression approach for real-time 2d/3d\nregistration. IEEE transactions on medical imaging, 35(5):1352–1363, 2016.\n[257] Tomas Mikolov, Stefan Kombrink, Luk´as Burget, Jan Cernock´y, and Sanjeev Khu-\ndanpur. Extensions of recurrent neural network language model. In ICASSP, pages\n5528–5531, 2011.\n[258] Anton Milan, Laura Leal-Taix´e, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16:\nA benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831, 2016.\n[259] Anton Milan, Laura Leal-Taix´e, Konrad Schindler, and Ian Reid. Joint tracking and\nsegmentation of multiple targets. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 5397–5406, 2015.\n[260] Anton Milan, S Hamid Rezatoﬁghi, Anthony Dick, Ian Reid, and Konrad Schindler.\nOnline multi-target tracking using recurrent neural networks. In Thirty-First AAAI\nConference on Artiﬁcial Intelligence, 2017.\n[261] Shervin Minaee, AmirAli Abdolrashidi, Hang Su, Mohammed Bennamoun, and David\nZhang. Biometric recognition using deep learning: A survey. CoRR, abs/1912.00271,\n2019.\n[262] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods\nfor deep reinforcement learning. In International conference on machine learning, pages\n1928–1937, 2016.\n[263] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods\nfor deep reinforcement learning. In Proceedings of The 33rd International Conference\non Machine Learning, pages 1928–1937, 20–22 Jun 2016.\n[264] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Os-\ntrovski, et al.\nHuman-level control through deep reinforcement learning.\nNature,\n518(7540):529–533, 2015.\n[265] I. Mordatch, N. Mishra, C. Eppner, and P. Abbeel. Combining model-based policy\nsearch with online model learning for control of physical humanoids. In 2016 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 242–248, 2016.\n[266] J. Morimoto, G. Zeglin, and C. G. Atkeson.\nMinimax diﬀerential dynamic pro-\ngramming: application to a biped walking robot.\nIn Proceedings 2003 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS 2003) (Cat.\nNo.03CH37453), volume 2, pages 1927–1932, 2003.\n88\n[267] Jun Morimoto and Christopher G. Atkeson. Nonparametric representation of an ap-\nproximated poincar´e map for learning biped locomotion. In Autonomous Robots, page\n131–144, 2009.\n[268] A. Mousavian, D. Anguelov, J. Flynn, and J. Koˇseck´a. 3d bounding box estimation\nusing deep learning and geometry. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 5632–5640, 2017.\n[269] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator for\nuav tracking. In European conference on computer vision, pages 445–461. Springer,\n2016.\n[270] Don Murray and Anup Basu. Motion tracking with an active camera. IEEE transac-\ntions on pattern analysis and machine intelligence, 16(5):449–459, 1994.\n[271] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey\nLevine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments\nthrough meta-reinforcement learning. arXiv preprint arXiv:1803.11347, 2018.\n[272] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming\nexploration in reinforcement learning with demonstrations. In 2018 IEEE International\nConference on Robotics and Automation (ICRA), pages 6292–6299, 2018.\n[273] Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural net-\nworks for visual tracking. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4293–4302, 2016.\n[274] Ali Bou Nassif, Ismail Shahin, Imtinan Attili, Mohammad Azzeh, and Khaled Shaalan.\nSpeech recognition using deep neural networks: A systematic review. IEEE Access,\n7:19143–19165, 2019.\n[275] Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C Peeken,\nStephanie E Combs, and Bjoern H Menze.\nDeep reinforcement learning for organ\nlocalization in ct. arXiv preprint arXiv:2005.04974, 2020.\n[276] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y\nNg. Reading digits in natural images with unsupervised feature learning. 2011.\n[277] Dominik Neumann, Saˇsa Grbi´c, Matthias John, Nassir Navab, Joachim Hornegger, and\nRazvan Ionasec. Probabilistic sparse matching for robust 3d/3d fusion in minimally\ninvasive surgery. IEEE transactions on medical imaging, 34(1):49–60, 2014.\n[278] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In\nProceedings of the Seventeenth International Conference on Machine Learning, ICML\n’00, page 663–670, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.\n89\n[279] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning.\nIn Icml, volume 1, page 2, 2000.\n[280] Trung Thanh Nguyen, Zhuoru Li, Tomi Silander, and Tze-Yun Leong. Online feature\nselection for model-based reinforcement learning. In Proceedings of the 30th Inter-\nnational Conference on International Conference on Machine Learning - Volume 28,\npage I–498–I–506, 2013.\n[281] Chi Nhan Duong, Kha Gia Quach, Khoa Luu, Ngan Le, and Marios Savvides. Tem-\nporal non-volume preserving approach to facial age-progression and age-invariant face\nrecognition. In Proceedings of the IEEE International Conference on Computer Vision,\npages 3735–3743, 2017.\n[282] C. Niedzwiedz, I. Elhanany, Zhenzhen Liu, and S. Livingston. A consolidated actor-\ncritic model with function approximation for high-dimensional pomdps.\nIn AAAI\n2008Workshop for Advancement in POMDP Solvers, 2008.\n[283] Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang.\nA\nreview of deep learning based speech synthesis. Applied Sciences, 9(19), 2019.\n[284] Kenji Okuma, Ali Taleghani, Nando De Freitas, James J Little, and David G Lowe. A\nboosted particle ﬁlter: Multitarget detection and tracking. In European conference on\ncomputer vision, pages 28–39. Springer, 2004.\n[285] Jos´e Ignacio Orlando, Huazhu Fu, Jo˜ao Barbosa Breda, Karel van Keer, Deepti R\nBathula, Andr´es Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo\nLee, et al. Refuge challenge: A uniﬁed framework for evaluating automated methods\nfor glaucoma assessment from fundus photographs. Medical image analysis, 59:101570,\n2020.\n[286] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters. 2018.\n[287] Rameswar Panda, Abir Das, Ziyan Wu, Jan Ernst, and Amit K Roy-Chowdhury.\nWeakly supervised summarization of web videos. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pages 3657–3666, 2017.\n[288] Anestis Papazoglou and Vittorio Ferrari. Fast object segmentation in unconstrained\nvideo. In Proceedings of the IEEE international conference on computer vision, pages\n1777–1784, 2013.\n[289] I. C. Paschalidis, K. Li, and R. Moazzez Estanjini. An actor-critic method using least\nsquares temporal diﬀerence learning. In Proceedings of the 48h IEEE Conference on\nDecision and Control (CDC) held jointly with 2009 28th Chinese Control Conference,\npages 2564–2569, 2009.\n90\n[290] Massimiliano Patacchiola and Angelo Cangelosi. Head pose estimation in the wild using\nconvolutional neural networks and adaptive gradient methods. Pattern Recognition,\n71:132–143, 2017.\n[291] Hanchuan Peng, Zongcai Ruan, Fuhui Long, Julie H Simpson, and Eugene W Myers.\nV3d enables real-time 3d visualization and quantitative analysis of large-scale biological\nimage data sets. Nature biotechnology, 28(4):348–353, 2010.\n[292] Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, and Alexander\nSorkine-Hornung. Learning video object segmentation from static images. In Pro-\nceedings of the IEEE conference on computer vision and pattern recognition, pages\n2663–2672, 2017.\n[293] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross,\nand Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology\nfor video object segmentation. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 724–732, 2016.\n[294] Jan Peters and Stefan Schaal.\nReinforcement learning of motor skills with policy\ngradients. Neural Networks, 21(4):682 – 697, 2008.\n[295] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforcement learning of region pro-\nposal networks for object detection. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 6945–6954, 2018.\n[296] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal greedy\nalgorithms for tracking a variable number of objects. In CVPR 2011, pages 1201–1208.\nIEEE, 2011.\n[297] Aske Plaat, Walter Kosters, and Mike Preuss. Deep model-based reinforcement learn-\ning for high-dimensional problems, a survey, 2020.\n[298] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid. Category-\nspeciﬁc video summarization. In European conference on computer vision, pages 540–\n555. Springer, 2014.\n[299] Reza Pourreza-Shahri and Nasser Kehtarnavaz. Exposure bracketing via automatic ex-\nposure selection. In 2015 IEEE International Conference on Image Processing (ICIP),\npages 320–323. IEEE, 2015.\n[300] Alessandro Prest, Christian Leistner, Javier Civera, Cordelia Schmid, and Vittorio\nFerrari. Learning object class detectors from weakly annotated video. In 2012 IEEE\nConference on Computer Vision and Pattern Recognition, pages 3282–3289. IEEE,\n2012.\n91\n[301] Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo Lim,\nand Ming-Hsuan Yang. Hedged deep tracking. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 4303–4311, 2016.\n[302] Zengyi Qin, Jinglu Wang, and Yan Lu. Monogrnet: A geometric reasoning network for\nmonocular 3d object localization. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 33(01):8851–8858, Jul. 2019.\n[303] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Ef-\nﬁcient oﬀ-policy meta-reinforcement learning via probabilistic context variables. In\nInternational conference on machine learning, pages 5331–5340, 2019.\n[304] Vidhiwar Singh Rathour, Kashu Yamakazi, and T Le. Roughness index and roughness\ndistance for benchmarking medical segmentation. arXiv preprint arXiv:2103.12350,\n2021.\n[305] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once:\nUniﬁed, real-time object detection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 779–788, 2016.\n[306] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint\narXiv:1804.02767, 2018.\n[307] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and Jie Zhou. Collaborative deep\nreinforcement learning for multi-object tracking. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 586–602, 2018.\n[308] Liangliang Ren, Xin Yuan, Jiwen Lu, Ming Yang, and Jie Zhou. Deep reinforcement\nlearning with iterative shift for visual tracking. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 684–700, 2018.\n[309] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-\ntime object detection with region proposal networks. In Advances in neural information\nprocessing systems, pages 91–99, 2015.\n[310] Md Reza, Jana Kosecka, et al. Reinforcement learning for semantic segmentation in\nindoor scenes. arXiv preprint arXiv:1606.01178, 2016.\n[311] Alexander Richard and Juergen Gall. Temporal action detection using a statistical\nlanguage model.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3131–3140, 2016.\n[312] Mrigank Rochan, Linwei Ye, and Yang Wang. Video summarization using fully convo-\nlutional sequence networks. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 347–363, 2018.\n92\n[313] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation.\nIn International Conference on Medical image\ncomputing and computer-assisted intervention, pages 234–241. Springer, 2015.\n[314] German Ros, Vladfen Koltun, Felipe Codevilla, and Antonio Lopez. The carla au-\ntonomous driving challenge, 2019.\n[315] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.\n” grabcut” interactive\nforeground extraction using iterated graph cuts. ACM transactions on graphics (TOG),\n23(3):309–314, 2004.\n[316] David Rotman. Mit technology review. Retrieved from Meet the Man with a Cheap and\nEasy Plan to Stop Global Warming: http://www. technologyreview. com/featuredstor\ny/511016/a-cheap-and-easy-plan-to-stop-globalwarming, 2013.\n[317] J-M Rouet, J-J Jacq, and Christian Roux. Genetic algorithms for a robust 3-d mr-ct\nregistration. IEEE transactions on information technology in biomedicine, 4(2):126–\n136, 2000.\n[318] David E Rumelhart.\nThe architecture of mind: A connectionist approach.\nMind\nreadings, pages 207–238, 1998.\n[319] T. P. Runarsson and S. M. Lucas. Imitating play from game trajectories: Temporal\ndiﬀerence learning versus preference learning. In 2012 IEEE Conference on Computa-\ntional Intelligence and Games (CIG), pages 79–82, 2012.\n[320] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International Journal of Computer Vision,\n115(3):211–252, 2015.\n[321] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese.\nTracking the untrackable:\nLearning to track multiple cues with long-term dependencies. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 300–311, 2017.\n[322] Steind´or Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth.\nMeta re-\ninforcement learning with latent variable gaussian processes.\narXiv preprint\narXiv:1803.07551, 2018.\n[323] Farhang Sahba.\nDeep reinforcement learning for object segmentation in video se-\nquences. In 2016 International Conference on Computational Science and Computa-\ntional Intelligence (CSCI), pages 857–860. IEEE, 2016.\n[324] Farhang Sahba, Hamid R Tizhoosh, and Magdy MA Salama. A reinforcement learning\nframework for medical image segmentation. In The 2006 IEEE International Joint\nConference on Neural Network Proceedings, pages 511–517. IEEE, 2006.\n93\n[325] Farhang Sahba, Hamid R Tizhoosh, and Magdy MMA Salama.\nApplication of\nopposition-based reinforcement learning in image segmentation. In 2007 IEEE Sym-\nposium on Computational Intelligence in Image and Signal Processing, pages 246–251.\nIEEE, 2007.\n[326] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.\nTrust region policy optimization. In International conference on machine learning,\npages 1889–1897, 2015.\n[327] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel.\nTrust Region Policy Optimization. arXiv e-prints, February 2015.\n[328] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal Policy Optimization Algorithms. arXiv e-prints, July 2017.\n[329] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[330] Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural\nNetworks, 16(1):5–9, 2003.\n[331] Shahin Sefati, Noah J Cowan, and Ren´e Vidal. Learning shared, discriminative dic-\ntionaries for surgical gesture segmentation and classiﬁcation. In MICCAI Workshop:\nM2CAI, volume 4, 2015.\n[332] Mohammad Javad Shaﬁee, Brendan Chywl, Francis Li, and Alexander Wong. Fast\nyolo: A fast you only look once system for real-time embedded object detection in\nvideo. arXiv preprint arXiv:1709.05943, 2017.\n[333] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A large\nscale dataset for 3d human activity analysis. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1010–1019, 2016.\n[334] M. R. Shaker, Shigang Yue, and T. Duckett. Vision-based reinforcement learning using\napproximate policy iteration. In 2009 International Conference on Advanced Robotics,\npages 1–6, 2009.\n[335] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, re-\ninforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016.\n[336] Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kossaiﬁ, Georgios Tzimiropou-\nlos, and Maja Pantic. The ﬁrst facial landmark tracking in-the-wild challenge: Bench-\nmark and results. In Proceedings of the IEEE international conference on computer\nvision workshops, pages 50–58, 2015.\n94\n[337] Y. Shi, L. Cui, Z. Qi, F. Meng, and Z. Chen. Automatic road crack detection using\nrandom structured forests. IEEE Transactions on Intelligent Transportation Systems,\n17(12):3434–3445, 2016.\n[338] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmen-\ntation and support inference from rgbd images. In European conference on computer\nvision, pages 746–760. Springer, 2012.\n[339] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin\nRiedmiller. Deterministic policy gradient algorithms. 2014.\n[340] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[341] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom\nfeature fusion for crowd counting. In Proceedings of the IEEE International Conference\non Computer Vision, pages 1002–1012, 2019.\n[342] Satya P. Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan,\nand Bal´azs Guly´as. 3d deep learning on medical images: A review, 2020.\n[343] Gwangmo Song, Heesoo Myeong, and Kyoung Mu Lee.\nSeednet: Automatic seed\ngeneration with deep reinforcement learning for robust interactive segmentation. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\n1760–1768, 2018.\n[344] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Sum-\nmarizing web videos using titles. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 5179–5187, 2015.\n[345] Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson WH Lau, and Ming-Hsuan\nYang. Crest: Convolutional residual learning for visual tracking. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 2555–2564, 2017.\n[346] Concetto Spampinato, Simone Palazzo, and Daniela Giordano. Gamifying video ob-\nject segmentation. IEEE transactions on pattern analysis and machine intelligence,\n39(10):1942–1958, 2016.\n[347] Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning.\nCoRR, abs/1703.01703, 2017.\n[348] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary\nmean-ﬁeld games. page 251–259. International Foundation for Autonomous Agents\nand Multiagent Systems, 2019.\n95\n[349] Shanhui Sun, Jing Hu, Mingqing Yao, Jinrong Hu, Xiaodong Yang, Qi Song, and\nXi Wu.\nRobust multimodal image registration using deep recurrent reinforcement\nlearning. In Asian Conference on Computer Vision, pages 511–526. Springer, 2018.\n[350] Kalaivani Sundararajan and Damon L. Woodard. Deep learning for biometrics: A\nsurvey. 51(3), 2018.\n[351] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.\nMIT press, 2018.\n[352] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy\ngradient methods for reinforcement learning with function approximation. In Proceed-\nings of the 12th International Conference on Neural Information Processing Systems,\nNIPS’99, page 1057–1063, 1999.\n[353] Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy\ngradient methods for reinforcement learning with function approximation. In Advances\nin Neural Information Processing Systems 12, pages 1057–1063. 2000.\n[354] Christian Szegedy, Sergey Ioﬀe, Vincent Vanhoucke, and Alexander A Alemi.\nInception-v4, inception-resnet and the impact of residual connections on learning. In\nThirty-ﬁrst AAAI conference on artiﬁcial intelligence, 2017.\n[355] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper\nwith convolutions.\nIn Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 1–9, 2015.\n[356] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for\nobject detection. In Advances in neural information processing systems, pages 2553–\n2561, 2013.\n[357] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive rein-\nforcement learning for skeleton-based action recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 5323–5332, 2018.\n[358] Ran Tao, Efstratios Gavves, and Arnold WM Smeulders.\nSiamese instance search\nfor tracking. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1420–1429, 2016.\n[359] Philippe Th´evenaz and Michael Unser. Optimization of mutual information for mul-\ntiresolution image registration. IEEE transactions on image processing, 9(12):2083–\n2099, 2000.\n[360] Zhiqiang Tian, Xiangyu Si, Yaoyue Zheng, Zhang Chen, and Xiaojian Li.\nMulti-\nstep medical image segmentation based on reinforcement learning. JOURNAL OF\nAMBIENT INTELLIGENCE AND HUMANIZED COMPUTING, 2020.\n96\n[361] Marin Toromanoﬀ, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free re-\ninforcement learning for urban driving using implicit aﬀordances. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7153–\n7162, 2020.\n[362] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1653–1660, 2014.\n[363] Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation via object\nﬂow. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 3899–3908, 2016.\n[364] Y. Tsurumine, Y. Cui, K. Yamazaki, and T. Matsubara. Generative adversarial im-\nitation learning with deep p-network for robotic cloth manipulation. In 2019 IEEE-\nRAS 19th International Conference on Humanoid Robots (Humanoids), pages 274–280,\n2019.\n[365] Yoshihisa Tsurumine, Yunduan Cui, Eiji Uchibe, and Takamitsu Matsubara. Deep\nreinforcement learning with smooth policy update: Application to robotic cloth ma-\nnipulation. Robotics and Autonomous Systems, 112:72 – 83, 2019.\n[366] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeul-\nders. Selective search for object recognition. International journal of computer vision,\n104(2):154–171, 2013.\n[367] Burak Uzkent, Christopher Yeh, and Stefano Ermon.\nEﬃcient object detection in\nlarge images using deep reinforcement learning. In The IEEE Winter Conference on\nApplications of Computer Vision, pages 1824–1833, 2020.\n[368] Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, and Philip HS Torr.\nEnd-to-end representation learning for correlation ﬁlter based tracking. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pages 2805–\n2813, 2017.\n[369] Peter van Beek. Improved image selection for stack-based hdr imaging. arXiv preprint\narXiv:1806.07420, 2018.\n[370] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with\nDouble Q-learning. arXiv e-prints, page arXiv:1509.06461, September 2015.\n[371] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with\ndouble q-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016.\n[372] Leo Van Hove. Optimal denominations for coins and bank notes: in defense of the\nprinciple of least eﬀort. Journal of Money, Credit and Banking, pages 1015–1021, 2001.\n97\n[373] Giuseppe Vecchio, Simone Palazzo, Daniela Giordano, Francesco Rundo, and Concetto\nSpampinato. Mask-rl: Multiagent video object segmentation framework through re-\ninforcement learning. IEEE Transactions on Neural Networks and Learning Systems,\n2020.\n[374] Kashu Yamakazi Akihiro Sugimoto Viet-Khoa Vo-Ho, Ngan T.H. Le and Triet Tran.\nAgent-environment network for temporal action proposal generation. In International\nConference on Acoustics, Speech and Signal Processing. 2021.\n[375] Sudheendra Vijayanarasimhan, Susanna Ricco, Cordelia Schmid, Rahul Sukthankar,\nand Katerina Fragkiadaki. Sfm-net: Learning of structure and motion from video.\narXiv preprint arXiv:1704.07804, 2017.\n[376] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jader-\nberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard\nPowell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Aga-\npiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky,\nSasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar\nGulcehre, Ziyu Wang, Tobias Pfaﬀ, Toby Pohlen, Dani Yogatama, Julia Cohen,\nKatrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps,\nKoray Kavukcuoglu, Demis Hassabis, and David Silver.\nAlphaStar:\nMaster-\ning the Real-Time Strategy Game StarCraft II.\nhttps://deepmind.com/blog/\nalphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.\n[377] Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel Rueckert, and\nBernhard Kainz. Multiple landmark detection using multi-agent reinforcement learn-\ning. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pages 262–270. Springer, 2019.\n[378] Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt, Premal A Patel, Michael\nAertsen, Tom Doel, Anna L David, Jan Deprest, S´ebastien Ourselin, et al. Deepi-\ngeos: a deep interactive geodesic framework for medical image segmentation. IEEE\ntransactions on pattern analysis and machine intelligence, 41(7):1559–1572, 2018.\n[379] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng\nLi, and Wei Liu.\nCosface: Large margin cosine loss for deep face recognition.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 5265–5274, 2018.\n[380] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, R´emi\nMunos, Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to\nreinforcement learn. CoRR, abs/1611.05763, 2016.\n[381] Lijun Wang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Deep networks for\nsaliency detection via local estimation and global search. In Computer Vision and Pat-\ntern Recognition (CVPR), 2015 IEEE Conference on, pages 3183–3192. IEEE, 2015.\n98\n[382] Mei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware\nreinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9322–9331, 2020.\n[383] Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces\nin the wild: Reducing racial bias by information maximization adaptation network. In\nProceedings of the IEEE International Conference on Computer Vision, pages 692–702,\n2019.\n[384] Naiyan Wang and Dit-Yan Yeung. Learning a deep compact image representation for\nvisual tracking. In Advances in neural information processing systems, pages 809–817,\n2013.\n[385] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois,\nShunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-\nbased reinforcement learning. CoRR, abs/1907.02057, 2019.\n[386] Yan Wang, Lei Zhang, Lituan Wang, and Zizhou Wang. Multitask learning for object\nlocalization with deep reinforcement learning. IEEE Transactions on Cognitive and\nDevelopmental Systems, 11(4):573–580, 2018.\n[387] Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, and Maja Pantic.\nDynamic face video segmentation via reinforcement learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6959–\n6969, 2020.\n[388] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality\nassessment: from error visibility to structural similarity. IEEE transactions on image\nprocessing, 13(4):600–612, 2004.\n[389] Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, and Jimmy Ren.\nLearning a reinforced agent for ﬂexible exposure bracketing selection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n1820–1828, 2020.\n[390] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando\nDe Freitas.\nDueling network architectures for deep reinforcement learning.\narXiv\npreprint arXiv:1511.06581, 2015.\n[391] Wayne A Wickelgren.\nThe long and the short of memory.\nPsychological Bulletin,\n80(6):425, 1973.\n[392] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist\nreinforcement learning. Machine learning, 8(3-4):229–256, 1992.\n99\n[393] Aaron Wilson, Alan Fern, and Prasad Tadepalli. Using trajectory data to improve\nbayesian optimization for reinforcement learning. Journal of Machine Learning Re-\nsearch, 15(8):253–282, 2014.\n[394] C. Wirth and J. F¨urnkranz. On learning from game annotations. IEEE Transactions\non Computational Intelligence and AI in Games, 7(3):304–316, 2015.\n[395] Paul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition and\n3d pose estimation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3109–3118, 2015.\n[396] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convo-\nlutional block attention module. In European Conference on Computer Vision, pages\n3–19, 2018.\n[397] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 2411–2418, 2013.\n[398] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 37(9):1834–1848, 2015.\n[399] Lu Xia, Chia-Chih Chen, and Jake K Aggarwal. View invariant human action recog-\nnition using histograms of 3d joints. In 2012 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition Workshops, pages 20–27. IEEE, 2012.\n[400] Sitao Xiang and Hao Li. On the eﬀects of batch and weight normalization in generative\nadversarial networks. arXiv preprint arXiv:1704.03971, 2017.\n[401] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online multi-object\ntracking by decision making. In Proceedings of the IEEE international conference on\ncomputer vision, pages 4705–4713, 2015.\n[402] Fanyi Xiao and Yong Jae Lee. Track and segment: An iterative unsupervised approach\nfor video object proposals. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 933–942, 2016.\n[403] Hang Xiao and Hanchuan Peng. App2: automatic tracing of 3d neuron morphology\nbased on hierarchical pruning of a gray-weighted image distance-tree. Bioinformatics,\n29(11):1448–1454, 2013.\n[404] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy\nstudent improves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10687–10698, 2020.\n100\n[405] Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, and Chunhua Shen.\nFrom open set to closed set: Counting objects by spatial divide-and-conquer.\nIn\nProceedings of the IEEE International Conference on Computer Vision, pages 8362–\n8371, 2019.\n[406] Hailiang Xu and Feng Su. Robust seed localization and growing with deep convolu-\ntional features for scene text detection. In Proceedings of the 5th ACM on International\nConference on Multimedia Retrieval, pages 387–394. ACM, 2015.\n[407] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep interactive\nobject selection.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 373–381, 2016.\n[408] Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, and Josef Kittler. Learning adaptive\ndiscriminative correlation ﬁlters via temporal consistency preserving spatial feature\nselection for robust visual object tracking. IEEE Transactions on Image Processing,\n28(11):5596–5609, 2019.\n[409] Xuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Eﬃcient multiple\norgan localization in ct image using 3d region proposal network. IEEE transactions\non medical imaging, 38(8):1885–1898, 2019.\n[410] Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, and Chun-Yi Lee.\nDynamic video\nsegmentation network. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6556–6565, 2018.\n[411] Kashu Yamazaki, Vidhiwar Singh Rathour, and T Le.\nInvertible residual net-\nwork with regularization for eﬀective medical image segmentation.\narXiv preprint\narXiv:2103.09042, 2021.\n[412] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen,\nand Errui Ding.\nPerspective-guided convolution networks for crowd counting.\nIn\nProceedings of the IEEE International Conference on Computer Vision, pages 952–\n961, 2019.\n[413] Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory Zelinsky,\nDimitris Samaras, and Minh Hoai. Predicting goal-directed human attention using\ninverse reinforcement learning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2020.\n[414] Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: a\nsurvey. arXiv preprint arXiv:1908.08796, 2019.\n[415] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn,\nand Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta\nreinforcement learning. In Conference on Robot Learning, pages 1094–1100, 2020.\n101\n[416] Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, and Jin Young Choi.\nAction-decision networks for visual tracking with deep reinforcement learning. In Pro-\nceedings of the IEEE conference on computer vision and pattern recognition, pages\n2711–2720, 2017.\n[417] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, and Xia Hu. Experience replay opti-\nmization. arXiv preprint arXiv:1906.08387, 2019.\n[418] Da Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang. Deep reinforcement learning\nfor visual object tracking in videos. arXiv preprint arXiv:1701.08936, 2017.\n[419] Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, and Junwei Han. Spftn: A self-paced\nﬁne-tuning network for segmenting objects in weakly labelled videos. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 4429–4437,\n2017.\n[420] Jing Zhang, Wanqing Li, Philip O Ogunbona, Pichao Wang, and Chang Tang. Rgb-\nd-based action recognition datasets: A survey. Pattern Recognition, 60:86–105, 2016.\n[421] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization with\nlong short-term memory. In European conference on computer vision, pages 766–782.\nSpringer, 2016.\n[422] Pengyu Zhang, Dong Wang, and Huchuan Lu. Multi-modal visual tracking: Review\nand experimental comparison, 2020.\n[423] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image\ncrowd counting via multi-column convolutional neural network. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 589–597, 2016.\n[424] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet\nfor real-time semantic segmentation on high-resolution images. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages 405–420, 2018.\n[425] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid\nscene parsing network. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2881–2890, 2017.\n[426] Zhong-Qiu Zhao, Shou-Tao Xu, Dian Liu, Wei-Dong Tian, and Zhi-Da Jiang. A review\nof image set classiﬁcation. Neurocomputing, 335:251–260, 2019.\n[427] Yefeng Zheng, David Liu, Bogdan Georgescu, Hien Nguyen, and Dorin Comaniciu.\n3d deep learning for eﬃcient and robust landmark detection in volumetric data. In\nInternational Conference on Medical Image Computing and Computer-Assisted Inter-\nvention, pages 565–572. Springer, 2015.\n102\n[428] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Tor-\nralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 633–641, 2017.\n[429] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for unsupervised\nvideo summarization with diversity-representativeness reward. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[430] Kaiyang Zhou, Tao Xiang, and Andrea Cavallaro. Video summarisation by classiﬁca-\ntion with deep reinforcement learning. arXiv preprint arXiv:1807.03089, 2018.\n[431] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature ﬂow\nfor video recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2349–2358, 2017.\n[432] Xiahai Zhuang and Juan Shen. Multi-scale patch and multi-modality atlases for whole\nheart segmentation of mri. Medical image analysis, 31:77–87, 2016.\n[433] Will Y Zou, Xiaoyu Wang, Miao Sun, and Yuanqing Lin. Generic object detection\nwith dense neural patterns and regionlets. arXiv preprint arXiv:1404.4316, 2014.\n103\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2021-08-25",
  "updated": "2021-08-25"
}