{
  "id": "http://arxiv.org/abs/2108.11887v2",
  "title": "Federated Reinforcement Learning: Techniques, Applications, and Open Challenges",
  "authors": [
    "Jiaju Qi",
    "Qihao Zhou",
    "Lei Lei",
    "Kan Zheng"
  ],
  "abstract": "This paper presents a comprehensive survey of Federated Reinforcement\nLearning (FRL), an emerging and promising field in Reinforcement Learning (RL).\nStarting with a tutorial of Federated Learning (FL) and RL, we then focus on\nthe introduction of FRL as a new method with great potential by leveraging the\nbasic idea of FL to improve the performance of RL while preserving\ndata-privacy. According to the distribution characteristics of the agents in\nthe framework, FRL algorithms can be divided into two categories, i.e.\nHorizontal Federated Reinforcement Learning (HFRL) and Vertical Federated\nReinforcement Learning (VFRL). We provide the detailed definitions of each\ncategory by formulas, investigate the evolution of FRL from a technical\nperspective, and highlight its advantages over previous RL algorithms. In\naddition, the existing works on FRL are summarized by application fields,\nincluding edge computing, communication, control optimization, and attack\ndetection. Finally, we describe and discuss several key research directions\nthat are crucial to solving the open problems within FRL.",
  "text": "1\nFederated Reinforcement Learning: Techniques,\nApplications, and Open Challenges\nJiaju Qi, Qihao Zhou, Lei Lei, Kan Zheng\nAbstract\nThis paper presents a comprehensive survey of Federated Reinforcement Learning (FRL), an emerging and\npromising ﬁeld in Reinforcement Learning (RL). Starting with a tutorial of Federated Learning (FL) and RL,\nwe then focus on the introduction of FRL as a new method with great potential by leveraging the basic idea of\nFL to improve the performance of RL while preserving data-privacy. According to the distribution characteristics\nof the agents in the framework, FRL algorithms can be divided into two categories, i.e., Horizontal Federated\nReinforcement Learning (HFRL) and Vertical Federated Reinforcement Learning (VFRL). We provide the detailed\ndeﬁnitions of each category by formulas, investigate the evolution of FRL from a technical perspective, and highlight\nits advantages over previous RL algorithms. In addition, the existing works on FRL are summarized by application\nﬁelds, including edge computing, communication, control optimization, and attack detection. Finally, we describe\nand discuss several key research directions that are crucial to solving the open problems within FRL.\nIndex Terms\nFederated Learning, Reinforcement Learning, Federated Reinforcement Learning\nI. INTRODUCTION\nAs Machine Learning (ML) develops, it becomes capable of solving increasingly complex problems,\nsuch as image recognition, speech recognition, and semantic understanding. Despite the effectiveness of\ntraditional machine learning algorithms in several areas, the researchers found that scenes involving many\nparties are still difﬁcult to resolve, especially when privacy is concerned. Federated Learning (FL), in\nthese cases, has attracted increasing interest among ML researchers. Technically, the FL is a decentralized\ncollaborative approach that allows multiple partners to train data respectively and build a shared model\nwhile maintaining privacy. With its innovative learning architecture and concepts, FL provides safer\nexperience exchange services and enhances capabilities of ML in distributed scenarios.\nIn ML, Reinforcement Learning (RL) is one of the branches that focuses on how individuals, i.e.,\nagents, interact with their environment and maximize some portion of the cumulative reward. The process\nallows agents to learn to improve their behavior in a trial and error manner. Through a set of policies, they\ntake actions to explore the environment and expect to be rewarded. Research on RL has been hot in recent\nyears, and it has shown great potential in various applications, including games, robotics, communication,\nand so on.\nHowever, there are still many problems in the implementation of RL in practical scenarios. For example,\nconsidering that in the case of large action space and state space, the performance of agents is vulnerable\nto collected samples since it is nearly impossible to explore all sampling spaces. In addition, many RL\nalgorithms have the problem of learning efﬁciency caused by low sample efﬁciency. Therefore, through\ninformation exchange between agents, learning speed can be greatly accelerated. Although distributed RL\nand parallel RL algorithms [1], [2], [3] can be used to address the above problems, they usually need\nto collect all the data, parameters, or gradients from each agent in a central server for model training.\nHowever, one of the important issues is that some tasks need to prevent agent information leakage and\nprotect agent privacy during the application of RL. Agents’ distrust of the central server and the risk of\neavesdropping on the transmission of raw data has become a major bottleneck for such RL applications.\nFL can not only complete information exchange while avoiding privacy disclosure, but also adapt various\nagents to their different environments. Another problem of RL is how to bridge the simulation-reality\ngap. Many RL algorithms require pre-training in simulated environments as a prerequisite for application\narXiv:2108.11887v2  [cs.LG]  24 Oct 2021\n2\ndeployment, but one problem is that the simulated environments cannot accurately reﬂect the environments\nof the real world. FL can aggregate information from both environments and thus bridge the gap between\nthem. Finally, in some cases, only partial features can be observed by each agent in RL. However, these\nfeatures, no matter observations or rewards, are not enough to obtain sufﬁcient information required to\nmake decisions. At this time, FL makes it possible to integrate this information through aggregation.\nThus, the above challenges give rise to the idea of federated reinforcement learning (FRL). As FRL\ncan be considered as an integration of FL and RL under privacy protection, several elements of RL can\nbe presented in FL frameworks to deals with sequential decision-making tasks. For example, these three\ndimensions of sample, feature and label in FL can be replaced by environment, state and action respectively\nin FRL. Since FL can be divided into several categories according to the distribution characteristics\nof data, including Horizontal Federated Learning (HFL) and Vertical Federated Learning (VFL), we\ncan similarly categorize FRL algorithms into Horizontal Federated Reinforcement Learning (HFRL) and\nVertical Federated Reinforcement Learning (VFRL).\nThough a few survey papers on FL [4], [5], [6] have been published, to the best of our knowledge,\nthere are currently no relevant survey papers focused on FRL. Due to the fact that FRL is a relatively new\ntechnique, most researchers may be unfamiliar with it to some extent. We hope to identify achievements\nfrom current studies and serve as a stepping stone to further research. In summary, this paper sheds light\non the following aspects.\n1) Systematic tutorial on FRL methodology. As a review focusing on FRL, this paper tries to explain\nthe knowledge about FRL to researchers systematically and in detail. The deﬁnition and categories\nof FRL are introduced ﬁrstly, including system model, algorithm process, etc. In order to explain\nthe framework of HFRL and VFRL and the difference between them clearly, two speciﬁc cases are\nintroduced, i.e., autonomous driving and smart grid. Moreover, we comprehensively introduce the\nexisting research on FRL’s algorithm design.\n2) Comprehensive summary for FRL applications. This paper collects a large number of references in\nthe ﬁeld of FRL, and provides a comprehensive and detailed investigation of the FRL applications\nin various areas, including edge computing, communications, control optimization, attack detection,\nand some other applications. For each reference, we discuss the authors’ research ideas and methods,\nand summarize how the researchers combine the FRL algorithm with the speciﬁc practical problems.\n3) Open issues for future research. This paper identiﬁes several open issues for FRL as a guide for\nfurther research. The scope covers communication, privacy and security, join and exit mechanisms\ndesign, learning convergence and some other issues. We hope that they can broaden the thinking of\ninterested researchers and provide help for further research on FRL.\nThe organization of this paper is as follows. To quickly gain a comprehensive understanding of FRL, the\npaper starts with FL and RL in Section 2 and Section 3, respectively, and extends the discussion further\nto FRL in Section 4. The existing applications of FRL are summarized in Section 5. In addition, a few\nopen issues and future research directions for FRL are highlighted in Section 6. Finally, the conclusion\nis given in Section 7.\nII. FEDERATED LEARNING\nA. Federated learning deﬁnition and basics\nIn general, FL is a ML algorithmic framework that allows multiple parties to perform ML under\nthe requirements of privacy protection, data security, and regulations [7]. In FL architecture, model\nconstruction includes two processes: model training and model inference. It is possible to exchange\ninformation about the model between parties during training, but not the data itself, so that data privacy\nwill not be compromised in any way. An individual party or multiple parties can possess and maintain the\ntrained model. In the process of model aggregation, more data instances collected from various parties\ncontribute to updating the model. As the last step, a fair value-distribution mechanism should be used\nto share the proﬁts obtained by the collaborative model [8]. The well-designed mechanism enables the\n3\nfederation sustainability. Aiming to build a joint ML model without sharing local data, FL involves\ntechnologies from different research ﬁelds such as distributed systems, information communication, ML\nand cryptography [9]. FL has the following characteristics as a result of these techniques, i.e.,\n• Distribution. There are two or more parties that hope to jointly build a model to tackle similar tasks.\nEach party holds independent data and would like to use it for model training.\n• Data protection. The data held by each party does not need to be sent to the other during the training\nof the model. The learned proﬁts or experiences are conveyed through model parameters that do not\ninvolve privacy.\n• Secure communication. The model is able to be transmitted between parties with the support of an\nencryption scheme. The original data cannot be inferred even if it is eavesdropped during transmission.\n• Generality. It is possible to apply FL to different data structures and institutions without regard to\ndomains or algorithms.\n• Guaranteed performance. The performance of the resulting model is very close to that of the ideal\nmodel established with all data transferred to one centralized party.\n• Status equality. To ensure the fairness of cooperation, all participating parties are on an equal footing.\nThe shared model can be used by each party to improve its local models when needed.\nA formal deﬁnition of FL is presented as follows. Consider that there are N parties {Fi}N\ni=1 interested in\nestablishing and training a cooperative ML model. Each party has their respective datasets Di. Traditional\nML approaches consist of collecting all data {Di}N\ni=1 together to form a centralized dataset R at one\ndata server. The expected model MSUM is trained by using the dataset R. On the other hand, FL is\na reform of ML process in which the participants Fi with data Di jointly train a target model MFED\nwithout aggregating their data. Respective data Di is stored on the owner Fi and not exposed to others. In\naddition, the performance measure of the federated model MFED is denoted as VFED, including accuracy,\nrecall, and F1-score, etc, which should be a good approximation of the performance of the expected model\nMSUM, i.e., VSUM. In order to quantify differences in performance, let δ be a non-negative real number\nand deﬁne the federated learning model MFED has δ performance loss if\n|VSUM −VFED| < δ.\nSpeciﬁcally, the FL model hold by each party is basically the same as the ML model, and it also\nincludes a set of parameters wi which is learned based on the respective training dataset Di [10]. A\ntraining sample j typically contains both the input of FL model and the expected output. For example, in\nthe case of image recognition, the input is the pixel of the image, and the expected output is the correct\nlabel. The learning process is facilitated by deﬁning a loss function on parameter vector w for every data\nsample j. The loss function represents the error of the model in relation to the training data. For each\ndataset Di at party Fi, the loss function on the collection of training samples can be deﬁned as follow\n[11],\nFi (w) =\n1\n|Di|\nX\nj∈Di\nfj (w),\nwhere fj (w) denotes the loss function of the sample j with the given model parameter vector w and\n| · | represents the size of the set. In FL, it is important to deﬁne the global loss function since multiple\nparties are jointly training a global statistical model without sharing a dataset. The common global loss\nfunction on all the distributed datasets is given by,\nFg (w) =\nN\nX\ni=1\nηiFi (w),\nwhere ηi indicates the relative impact of each party on the global model. In addition, ηi > 0 and\nPN\ni=1 ηi = 1. This term η can be ﬂexibly deﬁned to improve training efﬁciency. The natural setting is\n4\naveraging between parties, i.e., η = 1/N. The goal of the learning problem is to ﬁnd the optimal parameter\nthat minimizes the global loss function Fg (w). It is presented in formula form,\nw∗= arg min\nw\nFg (w) .\nConsidering that FL is designed to adapt to various scenarios, the objective function may be appropriate\ndepending on the application. However, a closed-form solution is almost impossible to ﬁnd with most FL\nmodels due to their inherent complexity. A canonical federated averaging algorithm (FedAvg) based on\ngradient-descent techniques is presented in the study from McMahan et al. [12], which is widely used\nin FL systems. In general, the coordinator has the initial FL model and is responsible for aggregation.\nDistributed participants know the optimizer settings and can upload information that does not affect\nprivacy. The speciﬁc architecture of FL will be discussed in the next subsection. Each participant uses\ntheir local data to perform one step (or multiple steps) of gradient descent on the current model parameter\n¯w (t) according to the following formula,\n∀i, wi (t + 1) = ¯w (t) −γ∇Fi ( ¯wi (t)) ,\nwhere γ denotes a ﬁxed learning rate of each gradient descent. After receiving the local parameters\nfrom participants, the central coordinator updates the global model using a weighted average, i.e.,\n¯wg (t + 1) =\nN\nX\ni=1\nni\nn wi (t + 1),\nwhere ni indicates the number of training data samples of the i-th participant has and n denotes the\ntotal number of samples contained in all the datasets. Finally, the coordinator sends the aggregated model\nweights ¯wg (t + 1) back to the participants. The aggregation process is performed at a predetermined\ninterval or iteration round. Additionally, FL leverages privacy-preserving techniques to prevent the leakage\nof gradients or model weights. For example, the existing encryption algorithms are added on top of the\noriginal FedAvg to provide secure FL [13], [14].\nB. Architecture of federated learning\nAccording to the application characteristics, the architecture of FL can be divided into two types [7],\ni.e., client-server model and peer-to-peer model.\nAs shown in Figure 1, there are two major components in the client-server model, i.e., participants\nand coordinators. The participants are the data owners and can perform local model training and updates.\nIn different scenarios, the participants are made up of different devices, the vehicles in the Internet\nof Vehicles (IoV), or the smart devices in the IoT. In addition, participants usually possess at least two\ncharacteristics. Firstly, each participant has a certain level of hardware performance, including computation\npower, communication and storage. The hardware capabilities ensure that the FL algorithm operates\nnormally. Secondly, participants are independent of one another and located in a wide geographic area. In\nthe client-server model, coordinator can be considered as a central aggregation server, which can initialize\na model and aggregate model updates from participants [12]. As participants train both based on local\ndata sets concurrently and share their experience through the coordinator with the model aggregation\nmechanism, it will greatly enhance the efﬁciency of the training and enhance the performance of the\nmodel. However, since participants won’t be able to communicate directly, the coordinator must perform\nwell to train the global model and maintain communication with all participants. Therefore, the model\nhas security challenges such as a single point of failure. If the coordinator fails to complete the model\naggregation task, the local model of participant has difﬁculty meeting target performance. The basic\nworkﬂow of the client-server model can be summarized in the following ﬁve steps. The process continues\nto repeat the steps from 2 to 5 until the model converges, or until the maximum number of iterations is\nreached.\n5\n• Step 1: In the process of setting up a client-server-based learning system, the coordinator creates an\ninitial model and sends it to each participant. Those participants who join later can access the latest\nglobal model.\n• Step 2: Each participant trains a local model based on their respective dataset.\n• Step 3: Updates of model parameters are sent to the central coordinator.\n• Step 4: The coordinator combines the model updates using speciﬁc aggregation algorithms.\n• Step 5: The combined model is sent back to the corresponding participant.\nParticipant\nParticipant\nParticipant\nLocal Model A\nLocal Model B\nDataset A\nLocal Model C\nDataset B\nDataset C\nGlobal Model\nTrain\nTrain\nTrain\nCoordinator\nAggregation\nSubmit\nUpdated Model\nUpdated Model\nInitialization\nUpdate\nUpdate\nSubmit\n1\n2\n3\n4\n5\n3\nFig. 1. An Example of Federated Learning Architecture: Client-Server Model.\nThe peer-to-peer based FL architecture does not require a coordinator as illustrated in Figure 2.\nParticipants can directly communicate with each other without relying on a third party. Therefore, each\nparticipant in the architecture is equal and can initiate a model exchange request with anyone else. As\nthere is no central server, participants must agree in advance on the order in which model should be sent\nand received. Common transfer modes are cyclic transfer and random transfer. The peer-to-peer model is\nsuitable and important for speciﬁc scenarios. For example, multiple banks jointly develop an ML-based\nattack detection model. With FL, there is no need to establish a central authority between banks to manage\nand store all attack patterns. The attack record is only held at the server of the attacked bank, but the\ndetection experience can be shared with other participants through model parameters. The FL procedure\nof the peer-to-peer model is simpler than that of the client-server model.\n• Step 1: Each participant initializes their local model depending on its needs.\n• Step 2: Train the local model based on the respective dataset.\n• Step 3: Create a model exchange request to other participants and send local model parameters.\n• Step 4: Aggregate the model received from other participants into the local model.\nThe termination conditions of the process can be designed by participants according to their needs. This\narchitecture further guarantees security since there is no centralized server. However, it requires more\ncommunication resources and potentially increased computation for more messages.\n6\nParticipant B\nLocal Model B\nAggregation\nUpdated Model A\nTrained Model B\nTrain\nDataset B\nInitialization\nParticipant A\nLocal Model A\nAggregation\nUpdated Model B\nTrained Model A\nTrain\nDataset A\nInitialization\nParticipant C\nLocal Model C\nAggregation\nUpdated Model A\nTrained Model C\nTrain\nDataset C\nInitialization\nTrained Model A \nUpdated Model C\n1\n2\n3\nExchange\n4\nFig. 2. An Example of Federated Learning Architecture: Peer-to-Peer Model.\nC. Categories of federated learning\nBased on the way data is partitioned within a feature and sample space, FL may be classiﬁed as HFL,\nVFL, or federated transfer learning (FTL)[8]. In Figure 3, Figure 4, and Figure 5, these three federated\nlearning categories for a two-party scenario are illustrated. In order to deﬁne each category more clearly,\nsome parameters are formalized. We suppose that the i-th participant has its own dataset Di. The dataset\nincludes three types of data, i.e., the feature space Xi, the label space Yi and the sample ID space Ii.\nIn particular, the feature space Xi is a high-dimensional abstraction of the variables within each pattern\nsample. Various features are used to characterize data held by the participant. All categories of association\nbetween input and task target are collected in the label space Yi. The sample ID space Ii is added in\nconsideration of actual application requirements. The identiﬁcation can facilitate the discovery of possible\nconnections among different features of the same individual.\nHFL indicates the case in which participants have their dataset with a small sample overlap, while most\nof the data features are aligned. The word ”horizontal” is derived from the term ”horizontal partition”.\nThis is similar to the situation where data is horizontally partitioned inside the traditional tabular view\nof a database. As shown in Figure. 3, the training data of two participants with the aligned features is\nhorizontally partitioned for HFL. A cuboid with a red border represents the training data required in\nlearning. Especially, a row corresponds to complete data features collected from a sampling ID. Columns\ncorrespond to different sampling IDs. The overlapping part means there can be more than one participant\nsampling the same ID. In addition, HFL is also known as feature-aligned FL, sample-partitioned FL, or\nexample-partitioned FL. Formally, the conditions for HFL can be summarized as\nXi = Xj, Yi = Yj, Ii ̸= Ij, ∀Di, Dj, i ̸= j,\nwhere Di and Dj denote the datasets of participant i and participant j respectively. In both datasets,\nthe feature space X and label space Y are assumed to be the same, but the sampling ID space I is\nassumed to be different. The objective of HFL is to increase the amount of data with similar features,\nwhile keeping the original data from being transmitted, thus improving the performance of the training\nmodel. Participants can still perform feature extraction and classiﬁcation if new samples appear. HFL\n7\ncan be applied in various ﬁelds because it beneﬁts from privacy protection and experience sharing [15].\nFor example, regional hospitals may receive different patients, and the clinical manifestations of patients\nwith the same disease are similar. It is imperative to protect the patient’s privacy, so data about patients\ncannot be shared. HFL provides a good way to jointly build a ML model for identifying diseases between\nhospitals.\nDataset A\nDataset B\nFeatures\nFeatures\nHorizontal Federated Learning\nOverlapping \nSample IDs\nAligned Features\nTraining Data\nLabels\nLabels\nFig. 3. Illustration of horizontal federated learning (HFL).\nVFL refers to the case where different participants with various targets usually have datasets that\nhave different feature spaces, but those participants may serve a large number of common users. The\nheterogeneous feature spaces of distributed datasets can be used to build more general and accurate\nmodels without releasing the private data. The word “vertical” derives from the term “vertical partition”,\nwhich is also widely used in reference to the traditional tabular view. Different from HFL, the training data\nof each participant are divided vertically. Figure 4 shows an example of VFL in a two-party scenario. The\nimportant step in VFL is to align samples, i.e., determine which samples are common to the participants.\nAlthough the features of the data are different, the sampled identity can be veriﬁed with the same ID.\nTherefore, VFL is also called sample-aligned FL or feature-partitioned FL. Multiple features are vertically\ndivided into one or more columns. The common samples exposed to different participants can be marked\nby different labels. The formal deﬁnition of VFL’s applicable scenario is given.\nXi ̸= Xj, Yi ̸= Yj, Ii = Ij, ∀Di, Dj, i ̸= j,\nwhere Di and Di represent the dataset held by different participants, and the data feature space pair\n(Xi, Xj) and label space pair (Yi, Yj) are assumed to be different. The sample ID space Ii and Ij are\nassumed to be the same. It is the objective of VFL to collaborate in building a shared ML model by\nexploiting all features collected by each participant. The fusion and analysis of existing features can even\ninfer new features. An example of the application of VFL is the evaluation of trust. Banks and e-commerce\ncompanies can create a ML model for trust evaluation for users. The credit card record held at the bank\nand the purchasing history held at the e-commerce company for the set of same users can be used as\ntraining data to improve the evaluation model.\nFTL applies to a more general case where the datasets of participants are not aligned with each other\nin terms of samples or features. FTL involves ﬁnding the invariant between a resource-rich source domain\nand a resource-scarce target domain, and exploiting that invariant to transfer knowledge. In comparison\nwith traditional transfer learning [16], FTL focuses on privacy-preserving issues and addresses distributed\nchallenges. An example of FTL is shown in Figure 5. The training data required by FTL may include\nall data owned by multiply parties for comprehensive information extraction. In order to predict labels\n8\nLabels\nLabels\nFeatures\nFeatures\nDataset B\nDataset A\nVertical Federated Learning\nDifferent Features\nDifferent Features\nTraining Data\nOverlapping \nSample IDs\nFig. 4. Illustration of vertical federated learning (VFL).\nfor unlabeled new samples, a prediction model is built using additional feature representations for mixed\nsamples from participants A and B. More formally, FTL is applicable for the following scenarios:\nXi ̸= Xj, Yi ̸= Yj, Ii ̸= Ij, ∀Di, Dj, i ̸= j,\nIn datasets Di and Dj , there is no duplication or similarity in terms of features, labels and samples. The\nobjective of FTL is to generate as accurate a label prediction as possible for newly incoming samples or\nunlabeled samples already present. Another beneﬁt of FTL is that it is capable of overcoming the absence\nof data or labels. For example, a bank and an e-commerce company in two different countries want to\nbuild a shared ML model for user risk assessment. In light of geographical restrictions, the user groups\nof these two organizations have limited overlap. Due to the fact that businesses are different, only a small\nnumber of data features are the same. It is important in this case to introduce FTL to solve the problem\nof small unilateral data and fewer sample labels, and improve the model performance.\nFederated Transfer Learning\nLabels\nLabels\nFeatures\nFeatures\nTraining Data\nNew Features\nLabels\nFig. 5. Illustration of federated transfer learning (FTL).\n9\nIII. REINFORCEMENT LEARNING\nA. Reinforcement learning deﬁnition and basics\nGenerally, the ﬁeld of ML includes supervised learning, unsupervised learning, RL, etc[17]. While\nsupervised and unsupervised learning attempt to make the agent copy the data set, i.e., learning from the\npre-provided samples, RL is to make the agent gradually stronger in the interaction with the environment,\ni.e., generating samples to learn by itself [18]. RL is a very hot research direction in the ﬁeld of ML\nin recent years, which has made great progress in many applications, such as IoT [19], [20], [21], [22],\nautonomous driving [23], [24], and game design [25]. For example, the AlphaGo program developed\nby DeepMind is a good example to reﬂect the thinking of RL [26]. The agent gradually accumulates\nthe intelligent judgment on the sub-environment of each move by playing game by game with different\nopponents, so as to continuously improve its level.\nThe RL problem can be deﬁned as a model of the agent-environment interaction, which is represented\nin Figure 6. The basic model of RL contains several important concepts, i.e.,\n• Environment and agent: Agents are a part of a RL model that exists in an external environment,\nsuch as the player in the environment of chess. Agents can improve their behavior by interacting\nwith the environment. Speciﬁcally, they take a series of actions to the environment through a set of\npolicies and expect to get a high payoff or achieve a certain goal.\n• Time step: The whole process of RL can be discretized into different time steps. At every time step,\nthe environment and the agent interact accordingly.\n• State: The state reﬂects agents’ observations of the environment. When agents take action, the state\nwill also change. In other words, the environment will move to the next state.\n• Actions: Agents can assess the environment, make decisions and ﬁnally take certain actions. These\nactions are imposed on the environment.\n• Reward: After receiving the action of the agent, the environment will give the agent the state of the\ncurrent environment and the reward due to the previous action. Reward represents an assessment of\nthe action taken by agents.\nFig. 6. The agent-environment interaction of the basic reinforcement learning model.\nMore formally, we assume that there are a series of time steps t = 0,1,2,... in a basic RL model. At\na certain time step t, the agent will receive a state signal St of the environment. In each step, the agent\nwill select one of the actions allowed by the state to take an action At. After the environment receives\nthe action signal At, the environment will feed back to the agent the corresponding status signal St+1 at\nthe next step t + 1 and the immediate reward Rt+1. The set of all possible states, i.e., the state space, is\n10\ndenoted as S. Similarly, the action space is denoted as A. Since our goal is to maximize the total reward,\nwe can quantify this total reward, usually referred to as return with\nGt = Rt+1 + Rt+2 + ... + RT,\nwhere T is the last step, i.e., ST as the termination state. An episode is completed when the agent\ncompletes the termination action.\nIn addition to this type of episodic task, there is another type of task that does not have a termination\nstate, in other words, it can in principle run forever. This type of task is called a continuing task. For\ncontinuous tasks, since there is no termination state, the above deﬁnition of return may be divergent. Thus,\nanother way to calculate return is introduced, which is called discounted return, i.e.,\nGt = Rt+1 + γRt+2 + γ2Rt+3 + ... =\n∞\nX\nk=0\nγkRt+k+1,\nwhere the discount factor γ satisﬁes 0 ⩽γ ⩽1. When γ = 1, the agent can obtain the full value of all\nfuture steps, while when γ = 0, the agent can only see the current reward. As γ changes from 0 to 1, the\nagent will gradually become forward-looking, looking not only at current interests, but also for its own\nfuture.\nThe value function is the agent’s prediction of future rewards, which is used to evaluate the quality of\nthe state and select actions. The difference between the value function and rewards is that the latter is\ndeﬁned as evaluating an immediate sense for interaction while the former is deﬁned as the average return\nof actions over a long period of time. In other words, the value function of the current state St = s is its\nlong-term expected return. There are two signiﬁcant value functions in the ﬁeld of RL, i.e., state value\nfunction Vπ (s) and action value function Qπ (s, a). The function Vπ (s) represents the expected return\nobtained if the agent continues to follow strategy π all the time after reaching a certain state St, while\nthe function Qπ (s, a) represents the expected return obtained if action At = a is taken after reaching the\ncurrent state St = s and the following actions are taken according to the strategy π. The two functions\nare speciﬁcally deﬁned as follows, i.e.,\nVπ (s) = Eπ [Gt|St = s] , ∀s ∈S\nQπ (s, a) = Eπ [Gt|St = s, At = a] , ∀s ∈S, a ∈A.\nThe results of RL are action decisions, called as the policy. The policy gives agents the action a that\nshould be taken for each state s. It is noted as =π (At = a|St = s), which represents the probability of\ntaking action At = a in state St = s. The goal of RL is to learn the optimal policy that can maximize\nthe value function by interacting with the environment. Our purpose is not to get the maximum reward\nafter a single action in the short term, but to get more reward in the long term. Therefore, the policy can\nbe ﬁgured out as,\nπ∗= argmax\nπ Vπ (s) , ∀s ∈S.\nB. Categories of reinforcement learning\nIn RL, there are several categories of algorithms. One is value-based and the other is policy-based. In\naddition, there is also an actor-critic algorithm that can be obtained by combining the two, as shown in\nFigure 7.\n11\nFig. 7. The categories and representative algorithms of reinforcement learning.\n1) Value-based methods: Recursively expand the formulas of the action value function, the correspond-\ning Bellman equation is obtained, which describes the recursive relationship between the value function\nof the current state and subsequent state. The recursive expansion formula of the action value function\nQπ (s, a) is\nQπ (s, a) =\nX\ns′,r\np\n\u0010\ns\n′, r|s, a\n\u0011 \"\nr + γ\nX\na′\nπ\n\u0010\na\n′|s\n′\u0011\nQπ\n\u0010\ns\n′, a\n′\u0011#\n,\nwhere the function p\n\u0000s\n′, r|s, a\n\u0001\n= Pr {St = s′, Rt = r|St−1 = s, At−1 = a} deﬁnes the trajectory prob-\nability to characterize the environment’s dynamics. Rt = r indicates the reward obtained by the agent\ntaking action At−1 = a in state St−1 = s. Besides, St = s′ and At = a′ respectively represent the state\nand the action taken by the agent at the next moment t.\nIn the value-based algorithms, the above value function Qπ (s, a) is calculated iteratively, and the strategy\nis then improved based on this value function. If the value of every action in a given state is known, the\nagent can select an action to perform. In this way, if the optimal Qπ (s, a = a∗) can be ﬁgured out, the\nbest action a∗will be found. There are many classical value-based algorithms, including Q-learning [27],\nState–Action–Reward–State–Action (SARSA) [28], etc.\nQ-learning is a typical widely-used value-based RL algorithm. It is also a model-free algorithm, which\nmeans that it does not need to know the model of the environment but directly estimates the Q value of\neach executed action in each encountered state through interacting with the environment [27]. Then, the\noptimal strategy is formulated by selecting the action with the highest Q value in each state. This strategy\nmaximizes the expected return for all subsequent actions from the current state. The most important part\nof Q-learning is the update of Q value. It uses a table, i.e., Q-table, to store all Q value functions. Q-table\nuses state as row and action as column. Each (s, a) pair corresponds to a Q value, i.e. Q(s, a), in the\nQ-table, which is updated as follows,\nQ (s, a) ←Q (s, a) + α\nh\nr + γmax\na′ Q\n\u0010\ns\n′, a\n′\u0011\n−Q (s, a)\ni\nwhere r is the reward given by taking action a under state s at the current time step. s′ and a′ indicate\nthe state and the action taken by the agent at the next time step respectively. α is the learning rate to\ndetermine how much error needs to be learned, and γ is the attenuation of future reward. If the agent\ncontinuously accesses all state-action pairs, the Q-learning algorithm will converge to the optimal Q\nfunction. Q-learning is suitable for simple problems, i.e., small state space, or a small number of actions.\nIt has high data utilization and stable convergence.\n2) Policy-based methods: The above value-based method is an indirect approach to policy selection,\nand has trouble handling an inﬁnite number of actions. Therefore, we want to be able to model the policy\ndirectly. Different from the value-based method, the policy-based algorithm does not need to estimate the\nvalue function, but directly ﬁts the policy function, updates the policy parameters through training, and\ndirectly generates the best policy. In policy-based methods, we input a state and output the corresponding\n12\naction directly, rather than the value V (s) or Q value Q (s, a) of the state. One of the most representative\nalgorithms is strategy gradient, which is also the most basic policy-based algorithm.\nPolicy gradient chooses to optimize the policy directly and update the parameters of the policy network\nby calculating the gradient of expected reward [29]. Therefore, its objective function J (θ) is directly\ndesigned as expected cumulative rewards, i.e.,\nJ (θ) = Eτ\nθ(τ) [r (τ)] =\nZ\nτ π(τ)\nr (τ) πθ (τ) dτ .\nBy taking the derivative of J (θ), we get\n∇θJ (θ) = Eτ πθ(τ)\n\" T\nX\nt=1\n∇θ log πθ (At|St)\nT\nX\nt=1\nr (St, At)\n#\n.\nThe above formula consists of two parts. One is PT\nt=1 ∇θ log πθ (At|St) which denotes the probability\nof the gradient in the current trace. The other is PT\nt=1 r (St, At) which represents the return of the current\ntrace. Since the return is total rewards and can only be obtained after one episode, the policy gradient\nalgorithm can only be updated for each episode, not for each time step.\nThe expected value can be expressed in a variety of ways, corresponding to different ways of calculating\nthe loss function. The advantage of the strategy gradient algorithm is that it can be applied in the continuous\naction space. In addition, the change of the action probability is smoother, and the convergence is better\nguaranteed.\nREINFORCE algorithm is a classic policy gradient algorithm [30]. Since the expected value of the\ncumulative reward cannot be calculated directly, the Monte Carlo method is applied to approximate the\naverage value of multiple samples. REINFORCE updates the unbiased estimate of the gradient by using\nMonte Carlo sampling. Each sampling generates a trajectory, which runs iteratively. After obtaining a\nlarge number of trajectories, the cumulative reward can be calculated by using certain transformations and\napproximations as the loss function for gradient update. However, the variance of this algorithm is large\nsince it needs to interact with the environment until the terminate state. The reward for each interaction\nis a random variable, so each variance will add up when the variance is calculated. In particular, the\nREINFORCE algorithm has three steps:\n• Step 1: sample τi from πθ (At|St)\n• Step 2: ∇θJ (θ) ≈P\ni\nhPT\nt=1 ∇θ log πθ (Ai\nt|Si\nt) PT\nt=1 r (Si\nt, Ai\nt)\ni\n• Step 3: theta ←θ + α∇θJ (θ)\nThe two algorithms, value-based and policy-based methods, both have their own characteristics and\ndisadvantages. Firstly, the disadvantages of the value-based methods are that the output of the action\ncannot be obtained directly, and it is difﬁcult to extend to the continuous action space. The value-based\nmethods can also lead to the problem of high bias, i.e., it is difﬁcult to eliminate the error between the\nestimated value function and the actual value function. For the policy-based methods, a large number of\ntrajectories must be sampled, and the difference between each trajectory may be huge. As a result, high\nvariance and large gradient noise are introduced. It leads to the instability of training and the difﬁculty\nof policy convergence.\n3) Actor-critic methods: The actor-critic architecture combines the characteristics of the value-based\nand policy-based algorithms, and to a certain extent solves their respective weaknesses, as well as the\ncontradictions between high variance and high bias. The constructed agent can not only directly output\npolicies, but also evaluate the performance of the current policies through the value function. Speciﬁcally,\nthe actor-critic architecture consists of an actor which is responsible for generating the policy and a critic\nto evaluate this policy. When the actor is performing, the critic should evaluate its performance, both of\nwhich are constantly being updated [31]. This complementary training is generally more effective than a\npolicy-based method or value-based method.\n13\nIn speciﬁc, the input of actor is state St, and the output is action At. The role of actor is to approximate\nthe policy model πθ (At|St). Critic uses the value function Q as the output to evaluate the value of the\npolicy, and this Q value Q (St, At) can be directly applied to calculate the loss function of actor. The\ngradient of the expected revenue function J (θ) in the action-critic framework is developed from the\nbasic policy gradient algorithm. The policy gradient algorithm can only implement the update of each\nepisode, and it is difﬁcult to accurately feedback the reward. Therefore, it has poor training efﬁciency.\nInstead, the actor-critic algorithm replaces PT\nt=1 r (Si\nt, Ai\nt) with Q (St, At) to evaluate the expected returns\nof state-action tuple {St, At} in the current time step t. The gradient of J (θ) can be expressed as\n∇θJ (θ) = Eτ πθ(τ)\n\" T\nX\nt=1\n∇θ log πθ (At|St) Q (St, At)\n#\n.\nC. Deep reinforcement learning\nWith the continuous expansion of the application of deep learning, its wave also swept into the RL\nﬁeld, resulting in Deep Reinforcement Learning (DRL), i.e., using a multi-layer deep neural network\nto approximate value function or policy function in the RL algorithm [32], [33]. DRL mainly solves\nthe curse-of-dimensionality problem in real-world RL applications with large or continuous state and/or\naction space, where the traditional tabular RL algorithms cannot store and extract a large amount of feature\ninformation [17], [34].\nQ-learning, as a very classical algorithm in RL, is a good example to understand the purpose of DRL.\nThe big issue with Q-learning falls into the tabular method, which means that when state and action spaces\nare very large, it cannot build a very large Q table to store a large number of Q values [35]. Besides, it\ncounts and iterates Q values based on past states. Therefore, on the one hand, the applicable state and\naction space of Q-learning is very small. On the other hand, if a state never appears, Q-learning cannot\ndeal with it [36]. In other words, Q-learning has no prediction ability and generalization ability at this\npoint.\nIn order to make Q-learning with prediction ability, considering that neural network can extract feature\ninformation well, Deep Q Network (DQN) is proposed by applying deep neural network to simulate Q\nvalue function. In speciﬁc, DQN is the continuation of Q-learning algorithm in continuous or large state\nspace to approximate Q value function by replacing Q table with neural networks [37].\nIn addition to the value-based DRL algorithm such as DQN, we summarize a variety of classical\nDRL algorithms according to algorithm types by referring to some DRL related surveys [38] in Table I,\nincluding not only the policy-based and actor-critic DRL algorithms, but also the advanced DRL algorithms\nof Partially Observable Markov Decision Process (POMDP) and multi-agents.\nIV. FEDERATED REINFORCEMENT LEARNING\nIn this section, the detailed background and categories of FRL will be discussed.\nA. Federated reinforcement learning background\nDespite the excellent performance that RL and DRL have achieved in many areas, they still face\nseveral important technical and non-technical challenges in solving real-world problems. The successful\napplication of FL in supervised learning tasks arouses interest in exploiting similar ideas in RL, i.e.,\nFRL. On the other hand, although FL is useful in some speciﬁc situations, it fails to deal with cooperative\ncontrol and optimal decision-making in dynamic environments [10]. FRL not only provides the experience\nfor agents to learn to make good decisions in an unknown environment, but also ensures that the privately\ncollected data during the agent’s exploration does not have to be shared with others. A forward-looking\nand interesting research direction is how to conduct RL under the premise of protecting privacy. Therefore,\n14\nTABLE I\nTAXONOMY OF REPRESENTATIVE ALGORITHMS FOR DRL.\nTypes\nRepresentative algorithms\nValue-based\nDeep Q-Network (DQN) [37], Double Deep Q-Network (DDQN) [39],\nDDQN with proportional prioritization [40]\nPolicy-based\nREINFORCE [30], Q-prop [41]\nActor-critic\nSoft Actor-Critic (SAC) [42], Asynchronous Advantage Actor Critic (A3C) [43],\nDeep Deterministic Policy Gradient (DDPG) [44],\nDistributed Distributional Deep Deterministic Policy Radients (D4PG) [45],\nTwin Delayed Deep Deterministic (TD3) [46],\nTrust Region Policy Optimization (TRPO) [47],\nProximal Policy Optimization (PPO) [48]\nAdvanced\nPOMDP\nDeep Belief Q-Network (DBQN) [49],\nDeep Recurrent Q-Network (DRQN) [50],\nRecurrent Deterministic Policy Gradients (RDPG) [51]\nMulti-agents\nMulti-Agent Importance Sampling (MAIS) [52],\nCoordinated Multi-agent DQN [53],\nMulti-agent Fingerprints (MAF) [52],\nCounterfactual Multiagent Policy Gradient (COMAPG) [54],\nMulti-Agent DDPG (MADDPG) [55]\nit is proposed to use FL framework to enhance the security of RL and deﬁne FRL as a security-\nenhanced distributed RL framework to accelerate the learning process, protect agent privacy and handle\nNot Independent and Identically Distributed (Non-IID) data [8]. Apart from improving the security and\nprivacy of RL, we believe that FRL has a wider and larger potential in helping RL to achieve better\nperformance in various aspects, which will be elaborated in the following subsections.\nIn order to facilitate understanding and maintain consistency with FL, FRL is divided into two categories\ndepending on environment partition [7], i.e., HFRL and VFRL. Figure 8 gives the comparison between\nHFRL and VFRL. In HFRL, the environment that each agent interacts with is independent of the others,\nwhile the state space and action space of different agents are aligned to solve similar problems. The\naction of each agent only affects its own environment and results in corresponding rewards. As an agent\ncan hardly explore all states of its environment, multiple agents interacting with their own copy of the\nenvironment can accelerate training and improve model performance by sharing experience. Therefore,\nhorizontal agents use server-client model or peer-to-peer model to transmit and exchange the gradients\nor parameters of their policy models (actors) and/or value function models (critics). In VFRL, multiple\nagents interact with the same global environment, but each can only observe limited state information in\nthe scope of its view. Agents can perform different actions depending on the observed environment and\nreceive local reward or even no reward. Based on the actual scenario, there may be some observation\noverlap between agents. In addition, all agents’ actions affect the global environment dynamics and total\nrewards. As opposed to the horizontal arrangement of independent environments in HFRL, the vertical\narrangement of observations in VFRL poses a more complex problem and is less studied in the existing\nliterature.\nB. Horizontal federated reinforcement learning\nHFRL can be applied in scenarios in which the agents may be distributed geographically, but they face\nsimilar decision-making tasks and have very little interaction with each other in the observed environments.\nEach participating agent independently executes decision-making actions based on the current state of\nenvironment and obtains positive or negative rewards for evaluation. Since the environment explored by\none agent is limited and each agent is unwilling to share the collected data, multiple agents try to train\nthe policy and/or value model together to improve model performance and increase learning efﬁciency.\nThe purpose of HFRL is to alleviate the sample-efﬁciency problem in RL, and help each agent quickly\n15\nHorizontal Federated Reinforcement Learning\nVertical Federated Reinforcement Learning\nGlobal Environment\nVertical\nEnvironment A\nEnvironment B\nEnvironment C\nEnvironment N\nLocal Model C\nLocal Model B\nLocal Model A\nLocal Model X\nAgent C\nAgent N\nAction\nState\nReward\nst+1\nrt+1\nReward\nState\nAction\nat\nrt\nst\nTrain\nAgent A\nAgent B\nObserved\nEnvironment A\nAction\nObserved \nEnvironment B\nObserved Environment N\nAgent A\nAgent B\nAgent N\nReward\nState\nFig. 8. Comparison of horizontal federated reinforcement learning (HFRL) and vertical federated reinforcement learning (VFRL).\nobtain the optimal policy which can maximize the expected cumulative reward for speciﬁc tasks, while\nconsidering privacy protection.\nIn the HFRL problem, the environment, state space, and action space can replace the data set, feature\nspace, and label space of basic FL. More formally, we assume that N agents {Fi}N\ni=1 can observe\nthe environment {Ei}N\ni=1 within their ﬁeld of vision. G denotes the collection of all environments. The\nenvironment Ei where the i-th agent is located has a similar model, i.e., state transition probability and\nreward function compared to other environments. Note that the environment Ei is independent of the other\nenvironments, in that the state transition and reward model of Ei do not depend on the states and actions\nof the other environments. Each agent Fi interacts with its own environment Ei to learn an optimal policy.\nTherefore, the conditions for HFRL are presented as follows, i.e.,\nSi = Sj, Ai = Aj, Ei ̸= Ej, ∀i, j ∈{1,2,...,N} , Ei, Ej ∈G, i ̸= j,\nwhere Si and Sj denote the similar state space encountered by the i-th agent and j-th agent, respectively.\nAi and Aj denote the similar action space of the i-th agent and j-th agent, respectively The observed\nenvironment Ei and Ei are two different environments that are assumed to be independent and ideally\nidentically distributed.\nFigure 9 shows the HFRL in graphic form. Each agent is represented by a cuboid. The axes of the\ncuboid denote three dimensions of information, i.e., the environment, state space, and action space. We\ncan intuitively see that all environments are arranged horizontally, and multiple agents have aligned state\nand action spaces. In other words, each agent explores independently in its respective environment, and\nneeds to obtain optimal strategies for similar tasks. In HFRL, agents share their experiences by exchanging\nmasked models to enhance sample efﬁciency and accelerate the learning process.\nA typical example of HFRL is the autonomous driving system in IoV. As vehicles drive on roads through-\nout the city and country, they can collect various environmental information and train the autonomous\ndriving models locally. Due to driving regulations, weather conditions, driving routes, and other factors,\none vehicle cannot be exposed to every possible situation in the environment. Moreover, the vehicles have\nbasically the same operations, including braking, acceleration, steering, etc. Therefore, vehicles driving\non different roads, different cities, or even different countries could share their learned experience with\neach other by FRL without revealing their driving data according to the premise of privacy protection. In\n16\nStates\nEnvironment A\nActions\nEnvironment B\nEnvironment N\nAgent A\nAgent B\nAgent N\nHorizontal Federated \nReinforcement Learning\nStates\nActions\nEnvironment\nFig. 9. Illustration of horizontal federated reinforcement learning (HFRL).\nthis case, even if other vehicles have never encountered a situation, they can still perform the best action\nby using the shared model. The exploration of multiple vehicles together also creates an increased chance\nof learning rare cases to ensure the reliability of the model.\nFor a better understanding of HFRL, Figure 10 shows an example of HFRL architecture using the\nserver-client model. The coordinator is responsible for establishing encrypted communication with agents\nand implementing aggregation of shared models. The multiple parallel agents may be composed of het-\nerogeneous equipment (e.g., IoT devices, smart phone and computers, etc.) and distributed geographically.\nIt is worth noting that there is no speciﬁc requirement for the number of agents, and agents are free to\nchoose to join or leave. The basic procedure for conducting HFRL can be summarized as follows.\n• Step 1: The initialization/join process can be divided into two cases, one is when the agent has no\nmodel locally, and the other is when the agent has a model locally. For the ﬁrst case, the agent can\ndirectly download the shared global model from a coordinator. For the second case, the agent needs\nto conﬁrm the model type and parameters with the central coordinator.\n• Step 2: Each agent independently observes the state of the environment and determines the private\nstrategy based on the local model. The selected action is evaluated by the next state and received\nreward. All agents train respective models in state-action-reward-state (SARS) cycles.\n• Step 3: Local model parameters are encrypted and transmitted to the coordinator. Agents may submit\nlocal models at any time as long as the trigger conditions are met.\n• Step 4: The coordinator conducts the speciﬁc aggregation algorithm to evolve the global federated\nmodel. Actually, there is no need to wait for submissions from all agents, and appropriate aggregation\nconditions can be formulated depending on communication resources.\n• Step 5: The coordinator sends back the aggregated model to the agents.\n• Step 6: The agents improve their respective models by fusing the federated model.\nFollowing the above architecture and process, applications suitable for HFRL should meet the following\ncharacteristics. First, agents have similar tasks to make decisions under dynamic environments. Different\nfrom the FL setting, the goal of the HFRL-based application is to ﬁnd the optimal strategy to maximize\nreward in the future. For the agent to accomplish the task requirements, the optimal strategy directs them\n17\nEnvironment B\nEnvironment A\nLocal Model A\nLocal Model B\nAgent A\nGlobal Model\nTrain\nCoordinator\nAggregation\nSubmit\nUpdated Model\nInitialization\n4\nAction\nState S(t+1)\nState S(t)\nObservation\nExploration / \nExploitation\nReward\nSend\nAgent B\nSubmit\n1\n2\n3\n5\nUpdate\n6\nFig. 10. An example of horizontal federated reinforcement learning (HFRL) architecture.\nto perform certain actions, such as control, scheduling, navigation, etc. Second, distributed agents maintain\nindependent observations. Each agent can only observe the environment within its ﬁeld of view, but does\nnot ensure that the collected data follows the same distribution. Third, it is important to protect the data\nthat each agent collects and explores. Agents are presumed to be honest but curious, i.e., they honestly\nfollow the learning mechanism but are curious about private information held by other agents. Due to\nthis, the data used for training is only stored at the owner and is not transferred to the coordinator. HFRL\nprovides an implementation method for sharing experiences under the constraints of privacy protection.\nAdditionally, various reasons limit the agent’s ability to explore the environment in a balanced manner.\nParticipating agents may include heterogeneous devices. The amount of data collected by each agent is\nunbalanced due to mobility, observation, energy and other factors. However, all participants have sufﬁcient\ncomputing, storage, and communication capabilities. These capabilities assist the agent in completing\nmodel training, merging, and other basic processes. Finally, the environment observed by a agent may\nchange dynamically, causing differences in data distribution. The participating agents need to update the\nmodel in time to quickly adapt to environmental changes and construct a personalized local model.\nIn existing RL studies, some applications that meet the above characteristics can be classiﬁed as\nHFRL. Nadiger et al. [56] presents a typical HFRL architecture, which includes the grouping policy,\nthe learning policy, and the federation policy. In this work, RL is used to show the applicability of\ngranular personalization and FL is used to reduce training time. To demonstrate the effectiveness of the\nproposed architecture, a non-player character in the Atari game Pong is implemented and evaluated. In the\nstudy from Liu et al. [57], the authors propose the Lifelong Federated Reinforcement Learning (LFRL)\nfor navigation in cloud robotic systems. It enables the robot to learn efﬁciently in a new environment and\nuse prior knowledge to quickly adapt to the changes in the environment. Each robot trains a local model\naccording to its own navigation task, and the centralized cloud server implements a knowledge fusion\nalgorithm for upgrading a shared model. In considering that the local model and the shared model might\nhave different network structures, this paper proposes to apply transfer learning to improve the performance\nand efﬁciency of the shared model. Further, researchers also focus on HFRL-based applications in the\nIoT due to the high demand for privacy protection. Ren et al. [58] suggest deploying the FL architecture\n18\nbetween edge nodes and IoT devices for computation ofﬂoading tasks. IoT devices can download RL model\nfrom edge nodes and train the local model using own data, including the remained energy resources and the\nworkload of IoT device, etc. The edge node aggregates the updated private model into the shared model.\nAlthough this method considers privacy protection issues, it requires further evaluation regarding the cost\nof communication resources by the model exchange. In addition, the work [59] proposes a federated\ndeep-reinforcement-learning-based framework (FADE) for edge caching. Edge devices, including Base\nStations (BSs), can cooperatively learn a predictive model using the ﬁrst round of training parameters\nfor local learning, and then upload the local parameters tuned to the next round of global training. By\nkeeping the training on local devices, the FADE can enable fast training and decouple the learning process\nbetween the cloud and data owner in a distributed-centralized manner. More HFRL-based applications\nwill be classiﬁed and summarized in the next section.\nPrior to HFRL, a variety of distributed RL algorithms have been extensively investigated, which\nare closely related to HFRL. In general, distributed RL algorithms can be divided into two types:\nsynchronized and asynchronous. In synchronous RL algorithms, such as Sync-Opt Synchronous Stochastic\nOptimization (Sync-Opt) [60] and Parallel Advantage Actor Critic (PAAC) [3], the agents explore their\nown environments separately, and after a number of samples are collected, the global parameters are\nupdated synchronously. On the contrary, the coordinator will update the global model immediately after\nreceiving the gradient from an arbitrary agent in asynchronous RL algorithms, rather than waiting for\nother agents. Several asynchronous RL algorithms are presented, including A3C [61], Impala [62], Ape-X\n[63] and General Reinforcement Learning Architecture (Gorila) [1]. From the perspective of technology\ndevelopment, HFRL can also be considered security-enhanced parallel RL. In parallel RL, multiple agents\ninteract with a stochastic environment to seek the optimal policy for the same task [1], [2]. By building\na closed loop of data and knowledge in parallel systems, parallel RL helps determine the next course\nof action for each agent. The state and action representations are fed into a designed neural network\nto approximate the action value function [64]. However, parallel RL typically transfers the experience of\nagent without considering privacy protection issues [7]. In the implementation of HFRL, further restrictions\naccompany privacy protection and communication consumption to adapt to special scenarios, such as IoT\napplications [59]. In addition, another point to consider is Non-IID data. In order to ensure convergence\nof the RL model, it is generally assumed in parallel RL that the states transitions in the environment\nfollow the same distribution, i.e., the environments of different agents are IID. But in actual scenarios,\nthe situation faced by agents may differ slightly, so that the models of environments for different agents\nare not identically distributed. Therefore, HFRL needs to improve the generalization ability of the model\ncompared with parallel RL to meet the challenges posed by Non-IID data.\nBased on the potential issues faced by the current RL technology, the advantages of HFRL can be\nsummarized as follows.\n• Enhancing training speed. In the case of a similar target task, multiple agents sharing training\nexperiences gained from different environments can expedite the learning process. The local model\nrapidly evolves through aggregation and update algorithms to assess the unexplored environment.\nMoreover, the data obtained by different agents are independent, reducing correlations between the\nobserved data. Furthermore, this also helps to solve the issue of unbalanced data caused by various\nrestrictions.\n• Improving the reliability of model. When the dimensions of the state of the environment are enormous\nor even uncountable, it is difﬁcult for a single agent to train an optimal strategy for situations with\nextremely low occurrence probabilities. Horizontal agents are exploring independently while building\na cooperative model to improve the local model’s performance on rare states.\n• Mitigating the problems of devices heterogeneity. Different devices deploying RL agents in the HFRL\narchitecture may have different computational and communication capabilities. Some devices may\nnot meet the basic requirements for training, but strategies are needed to guide actions. HFRL makes\nit possible for all agents to obtain the shared model equally for the target task.\n• Addressing the issue of non-identical environment. Considering the differences in the environment\n19\ndynamics for the different agents, the assumption of IID data may be broken. Under the HFRL\narchitecture, agents in not identically-distributed environment models can still cooperate to learn a\nfederated model. In order to address the difference in environment dynamics, a personalized update\nalgorithm of local model could be designed to minimize the impact of this issue.\n• Increasing the ﬂexibility of the system. The agent can decide when to participate in the cooperative\nsystem at any time, because HFRL allows asynchronous requests and aggregation of shared models.\nIn the existing HFRL-based application, new agents also can apply for membership and beneﬁt from\ndownloading the shared model.\nC. Vertical federated reinforcement learning\nIn VFL, samples of multiple data sets have different feature spaces but these samples may belong to the\nsame groups or common users. The training data of each participant are divided vertically according to their\nfeatures. More general and accurate models can be generated by building heterogeneous feature spaces\nwithout releasing private information. VFRL applies the methodology of VFL to RL and is suitable for\nPOMDP scenarios where different RL agents are in the same environment but have different interactions\nwith the environment. Speciﬁcally, different agents could have different observations that are only part of\nthe global state. They could take actions from different action spaces and observe different rewards, or\nsome agents even take no actions or cannot observe any rewards. Since the observation range of a single\nagent to the environment is limited, multiple agents cooperate to collect enough knowledge needed for\ndecision making. The role of FL in VFRL is to aggregate the partial features observed by various agents.\nEspecially for those agents without rewards, the aggregation effect of FL greatly enhances the value of\nsuch agents in their interactions with the environment, and ultimately helps with the strategy optimization.\nIt is worth noting that in VFRL the issue of privacy protection needs to be considered, i.e., private data\ncollected by some agents do not have to be shared with others. Instead, agents can transmit encrypted\nmodel parameters, gradients, or direct mid-product to each other. In short, the goal of VFRL is for agents\ninteracting with the same environment to improve the performance of their policies and the effectiveness\nin learning them by sharing experiences without compromising the privacy.\nMore formally, we denote {Fi}N\ni=1 as N agents in VFRL, which interact with a global environment E.\nThe i-th agent Fi is located in the environment Ei = E, obtains the local partial observation Oi, and can\nperform the set of actions Ai. Different from HFRL, the state/observation and action spaces of two agents\nFi and Fj may be not identical, but the aggregation of the state/observation spaces and action spaces of\nall the agents constitutes the global state and action spaces of the global environment E. The conditions\nfor VFRL can be deﬁned as i.e.,\nOi ̸= Oj, Ai ̸= Aj, Ei = Ej = E,\nN\n[\ni=1\nOi =S,\nN\n[\ni=1\nAi =A, ∀i, j ∈{1,2,...,N} , i ̸= j,\nwhere S and A denote the global state space and action space of all participant agents respectively. It\ncan be seen that all the observations of the N agents together constitute the global state space S of the\nenvironment E. Besides, the environments Ei and Ej are the same environment E. In most cases, there is\na great difference between the observations of two agents Fi and Fj.\nFigure 11 shows the architecture of VFRL. The dataset and feature space in VFL are converted to\nthe environment and state space respectively. VFL divides the dataset vertically according to the features\nof samples, and VFRL divides agents based on the state spaces observed from the global environment.\nGenerally speaking, every agent has its local state which can be different from that of the other agents and\nthe aggregation of these local partial states corresponds to the entire environment state [65]. In addition,\nafter interacting with the environment, agents may generate their local actions which correspond to the\nlabels in VFL.\nTwo types of agents can be deﬁned for VFRL, i.e., decision-oriented agents and support-oriented agents.\nDecision-oriented agents {Fi}K\ni=1 can interact with the environment E based on their local state {Si}K\ni=1\n20\nActions\nStates\nGlobal\nEnvironment\nAgent A\nAgent B\nAgent N\nActions A\nActions B\nActions N\nObserved by N\nObserved by A\nObserved by B\nVertical Federated \nReinforcement Learning\nFig. 11. Illustration of vertical federated reinforcement learning (VFRL).\nand action {Ai}K\ni=1. Meanwhile, support-oriented agents {Fi}N\ni=K+1 take no actions and receive no rewards\nbut only the observations of the environment, i.e., their local states {Si}N\ni=K+1. In general, the following\nsix steps, as shown in Figure 12, are the basic procedure for VFRL, i.e.,\n• Step 1: Initialization is performed for all agent models.\n• Step 2: Agents obtain states from the environment. For decision-oriented agents, actions are obtained\nbased on the local models, and feedbacks are obtained through interactions with the environment,\ni.e., the states of the next time step and rewards. The data tuple of state-action-reward-state (SARS)\nis used to train the local models.\n• Step 3: All agents calculate the mid-products of the local models and then transmit the encrypted\nmid-products to the federated model.\n• Step 4: The federated model performs the aggregation calculation for mid-products and trains the\nfederated model based on the aggregation results.\n• Step 5: Federated model encrypts model parameters such as weight and gradient and passes them\nback to other agents.\n• Step 6: All agents update their local models based on the received encrypted parameters.\nFig. 12. An example of vertical federated reinforcement learning (VFRL) architecture.\n21\nAs an example of VFRL, consider a microgrid (MG) system including household users, the power\ncompany, and the photovoltaic (PV) management company as the agents. All the agents observe the\nsame MG environment while their local state spaces are quite different. The global states of the MG\nsystem generally consist of several dimensions/features, i.e., State-of-Charge (SOC) of the batteries, load\nconsumption of the household users, power generation from PV, etc. The household agents can obtain\nthe SOC of their own batteries and their own load consumption, the power company can know the load\nconsumption of all the users, and PV management company can know the power generation of PV. As\nto the action, the power company needs to make decisions on the power dispatch of the diesel generators\n(DG), and the household users can make decisions to manage their electrical utilities with demand response.\nFinally, the power company can observe rewards such as the cost of DG power generation, the balance\nbetween power generation and consumption, and the household users can observe rewards such as their\nelectricity bill that is related to their power consumption. In order to learn the optimal policies, these\nagents need to communicate with each other to share their observations. However, PV managers do not\nwant to expose their data to other companies, and household users also want to keep their consumption\ndata private. In this way, VFRL is suitable to achieve this goal and can help improve policy decisions\nwithout exposing speciﬁc data.\nCompared with HFRL, there are currently few works on VFRL. Zhuo et al. [65] present the federated\ndeep reinforcement learning (FedRL) framework. The purpose of this paper is to solve the challenge\nwhere the feature space of states is small and the training data are limited. Transfer learning approaches\nin DRL are also solutions for this case. However, when considering the privacy-aware applications, directly\ntransferring data or models should not be used. Hence, FedRL combines the advantage of FL with RL,\nwhich is suitable for the case when agents need to consider their privacy. FedRL framework assumes\nagents cannot share their partial observations of the environment and some agents are unable to receive\nrewards. It builds a shared value network, i.e., MultiLayer Perceptron (MLP), and takes its own Q-network\noutput and encryption value as input to calculate a global Q-network output. Based on the output of global\nQ-network, the shared value network and self Q-network are updated. Two agents are used in the FedRL\nalgorithm, i.e., agent α and β, which interact with the same environment. However, agent β cannot build\nits own policies and rewards. Finally, FedRL is applied in two different games, i.e., Grid-World and\nText2Action, and achieves better results than the other baselines. Although the VFRL model in this paper\nonly contains two agents, and the structure of the aggregated neural network model is relatively simple,\nwe believe that it is a great attempt to ﬁrst implement VFRL and verify its effectiveness.\nMulti-agent RL (MARL) is very closely related to VFRL. As the name implies, MARL takes into\naccount the existence of multiple agents in the RL system. However, the empirical evaluation shows that\napplying the simple single-agent RL algorithms directly to scenarios of multiple agents cannot converge\nto the optimal solution, since the environment is no longer static from the perspective of each agent [66].\nIn speciﬁc, the action of each agent will affect the next state, thus affecting all agents in the future time\nstep [67]. Besides, the actions performed by one certain agent will yield different rewards depending on\nthe actions taken by other agents. This means that agents in MARL correlate with each other, rather than\nbeing independent of each other. This challenge, called as the non-stationarity of the environment, is the\nmain problem to be solved in the development of an efﬁcient MARL algorithm [68].\nMARL and VFRL both study the problem of multiple agents learning concurrently how to solve a task\nby interacting with the same environment [69]. Since MARL and VFRL have a large range of similarities,\nthe review of MARL’s related works is a very useful guide to help researchers summarize the research\nfocus and better understand VFRL. There is abundant literature related to MARL. However, most MARL\nresearch [70], [71], [72], [73] is based on a fully observed Markov Decision Process (MDP), where\neach agent is assumed to have the global observation of the system state [68]. These MARL algorithms\nare not applicable to the case of POMDP where the observations of individual agents are often only a\npart of the overall environment [74]. Partial observability is a crucial consideration for the development\nof algorithms that can be applied to real-world problems [75]. Since VFRL is mainly oriented towards\nPOMDP scenarios, it is more important to analyze the related works of MARL based on POMDP as the\n22\nguidance of VFRL.\nAgents in the above scenarios partially observe the system state and make decisions at each step\nto maximize the overall rewards for all agents, which can be formalized as a Decentralized Partially\nObservable Markov Decision Process (Dec-POMDP) [76]. Optimally addressing a Dec-POMDP model is\nwell known to be a very challenging problem. In the early works [77], Omidshaﬁei et al. proposes a two-\nphase MT-MARL approach that concludes the methods of cautiously-optimistic learners for action-value\napproximation and Concurrent Experience Replay Trajectories (CERTs) as the experience replay targeting\nsample-efﬁcient and stable MARL. The authors also apply the recursive neural network (RNN) to estimate\nthe non-observed state and hysteretic Q-learning to address the problem of non-stationarity in Dec-POMDP.\nHan et al. [78] designs a neural network architecture, IPOMDP-net, which extends QMDP-net planning\nalgorithm [79] to MARL settings under POMDP. Besides, Mao et al. [80] introduces the concept of\ninformation state embedding to compress agents’ histories and proposes an RNN model combining the\nstate embedding. Their method, i.e., embed-then-learn pipeline, is universal since the embedding can be fed\ninto any existing partially observable MARL algorithm as the black-box. In the study from Mao et al. [81],\nthe proposed Attention MADDPG (ATT-MADDPG) has several critic networks for various agents under\nPOMDP. A centralized critic is adopted to collect the observations and actions of the teammate agents.\nSpeciﬁcally, the attention mechanism is applied to enhance the centralized critic. The ﬁnal introduced work\nis from Lee et al. [82]. They present an augmenting MARL algorithm based on pretraining to address\nthe challenge in disaster response. It is interesting that they use behavioral cloning (BC), a supervised\nlearning method where agents learn their policy from demonstration samples, as the approach to pretrain\nthe neural network. BC can generate a feasible Dec-POMDP policy from demonstration samples, which\noffers advantages over plain MARL in terms of solution quality and computation time.\nSome MARL algorithms also concentrate on the communication issue of POMDP. In the study from\nSukhbaatar [83], communication between the agents is performed for a number of rounds before their\naction is selected. The communication protocol is learned concurrently with the optimal policy. Foerster\net al. [84] proposes a deep recursive network architecture, i.e., Deep Distributed Recurrent Q-Network\n(DDRQN), to address the communication problem in a multi-agent partially-observable setting. This\nwork makes three fundamental modiﬁcations to previous algorithms. The ﬁrst one is last-action inputs,\nwhich let each agent access its previous action as an input for the next time-step. Besides, inter-agent\nweight sharing allows diverse behavior between agents, as the agents receive different observations and\nthus evolve in different hidden states. The ﬁnal one is disabling experience replay, which is because\nthe non-stationarity of the environment renders old experiences obsolete or misleading. Foerster et al.\n[84] considers the communication task of fully cooperative, partially observable, sequential multi-agent\ndecision-making problems. In their system model, each agent can receive a private observation and take\nactions that affect the environment. In addition, the agent can also communicate with its fellow agents\nvia a discrete limited-bandwidth channel. Despite the partial observability and limited channel capacity,\nauthors achieved the task that the two agents could discover a communication protocol that enables them\nto coordinate their behavior based on the approach of deep recurrent Q-networks.\nWhile there are some similarities between MARL and VFRL, several important differences have to be\npaid attention to, i.e.,\n• VFRL and some MARL algorithms are able to address similar problems, e.g., the issues of POMDP.\nHowever, there are differences between the solution ideas between two algorithms. Since VFRL is\nthe product of applying VFL to RL, the FL component of VFRL focuses more on the aggregation\nof partial features, including states and rewards, observed by different agents since VFRL inception.\nSecurity is also an essential issue in VFRL. On the contrary, MARL may arise as the most natural\nway of adding more than one agent in a RL system [85]. In MARL, agents not only interact with the\nenvironment, but also have complex interactive relationships with other agents, which creates a great\nobstacle to the solution of policy optimization. Therefore, the original intentions of two algorithms\nare different.\n• Two algorithms are slightly different in terms of the structure. The agents in MARL must surely have\n23\nthe reward even some of them may not have their own local actions. However, in some cases, the\nagents in VFRL are not able to generate a corresponding operation policy, so in these cases, some\nagents have no actions and rewards [65]. Therefore, VFRL can solve more extensive problems that\nMARL is not capable of solving.\n• Both two algorithms involve the communication problem between agents. In MARL, information\nsuch as the states of other agents and model parameters can be directly and freely propagated among\nagents. During communication, some MARL methods such as DDRQN in the work of Foerster et\nal. [84] consider the previous action as an input for the next time-step state. Weight sharing is also\nallowed between agents. However, VFRL assumes states cannot be shared among agents. Since these\nagents do not exchange experience and data directly, VFRL focuses more on security and privacy\nissues of communication between agents, as well as how to process mid-products transferred by other\nagents and aggregate federated models.\nIn summary, as a potential and notable algorithm, VFRL has several advantages as follows, i.e.,\n• Excellent privacy protection. VFRL inherits the FL algorithm’s idea of data privacy protection, so\nfor the task of multiple agents cooperation in the same environment, information interaction can\nbe carried out conﬁdently to enhance the learning efﬁciency of RL model. In this process, each\nparticipant does not have to worry about any leakage of raw real-time data.\n• Wide application scenarios. With appropriate knowledge extraction methods, including algorithm\ndesign and system modeling, VFRL can solve more real-world problems compared with MARL\nalgorithms. This is because VFRL can consider some agents that cannot generate rewards into the\nsystem model, so as to integrate their partial observation information of the environment based on\nFL while protecting privacy, train a more robust RL agent, and further improve learning efﬁciency.\nD. Other types of FRL\nThe above HFRL or VFRL algorithms borrow ideas from FL for federation between RL agents.\nMeanwhile, there are also some existing works on FRL that are less affected by FL. Hence, they do\nnot belong to either HFRL or VFRL, but federation between agents is also implemented.\nThe study from Hu et al. [86] is a typical example, which proposes a reward shaping based general FRL\nalgorithm, called Federated Reward Shaping (FRS). It uses reward shaping to share federated information\nto improve policy quality and training speed. FRS adopts the server-client architecture. The server includes\nthe federated model, while each client completes its own tasks based on the local model. This algorithm\ncan be combined with different kinds of RL algorithms. However, it should be noted that FRS focuses on\nreward shaping, this algorithm cannot be used when there is no reward in some agents in VFRL. In addition,\nFRS performs knowledge aggregation by sharing high-level information such as reward shaping value or\nembedding between client and server instead of sharing experience or policy directly. The convergence\nof FRS is also guaranteed since only minor changes are made during the learning process, which is the\nmodiﬁcation of the reward in the replay buffer.\nAs another example, Anwar et al. [87] achieves federation between agents by smoothing the average\nweight. This work analyzes the Multi-task FRL algorithms (MT-FedRL) with adversaries. Agents only\ninteract and make observations in their environment, which can be featured by different MDPs. Different\nfrom HFRL, the state and action spaces do not need to be the same in these environments. The goal\nof MT-FedRL is to learn a uniﬁed policy, which is jointly optimized across all of the environments.\nMT-FedRL adopts policy gradient methods for RL. In other words, policy parameter is needed to learn\nthe optimal policy. The server-client architecture is also applied and all agents should share their own\ninformation with a centralized server. The role of non-negative smoothing average weights is to achieve\na consensus among the agents’ parameters. As a result, they can help to incorporate the knowledge from\nother agents as the process of federation.\n24\nV. APPLICATIONS OF FRL\nIn this section, we provide an extensive discussion of the application of FRL in a variety of tasks, such\nas edge computing, communications, control optimization, attack detection, etc. This section is aimed at\nenabling readers to understand the applicable scenarios and research status of FRL.\nA. FRL for edge computing\nIn recent years, edge equipment, such as BSs and Road Side Units (RSUs), has been equipped with\nincreasingly advanced communication, computing and storage capabilities. As a result, edge computing\nis proposed to delegating more tasks to edge equipment in order to reduce the communication load and\nreduce the corresponding delay. However, the issue of privacy protection remains challenging since it may\nbe untrustworthy for the data owner to hand off their private information to a third-party edge server [4].\nFRL offers a potential solution for achieving privacy-protected intelligent edge computing, especially in\ndecision-making tasks like caching and ofﬂoading. Additionally, the multi-layer processing architecture of\nedge computing is also suitable for implementing FRL through the server-client model. Therefore, many\nresearchers have focused on applying FRL to edge computing.\nThe distributed data of large-scale edge computing architecture makes it possible for FRL to provide\ndistributed intelligent solutions to achieve resource optimization at the edge. For mobile edge networks,\na potential FRL framework is presented for edge system [88], named as “In-Edge AI”, to address\noptimization of mobile edge computing, caching, and communication problems. The authors also propose\nsome ideas and paradigms for solving these problems by using DRL and Distributed DRL. To carry out\ndynamic system-level optimization and reduce the unnecessary transmission load, “In-Edge AI” framework\ntakes advantage of the collaboration among edge nodes to exchange learning parameters for better training\nand inference of models. It has been evaluated that the framework has high performance and relatively low\nlearning overhead, while the mobile communication system is cognitive and adaptive to the environment.\nThe paper provides good prospects for the application of FRL to edge computing, but there are still many\nchallenges to overcome, including the adaptive improvement of the algorithm, and the training time of\nthe model from scratch etc.\nEdge caching has been considered a promising technique for edge computing to meet the growing\ndemands for next-generation mobile networks and beyond. Addressing the adaptability and collaboration\nchallenges of the dynamic network environment, Wang et al. [89] proposes a Device-to-device (D2D)-\nassisted heterogeneous collaborative edge caching framework. User equipment (UE) in a mobile network\nuses the local DQN model to make node selection and cache replacement decisions based on network status\nand historical information. In other words, UE decides where to fetch content and which content should\nbe replaced in its cache list. The BS calculates aggregation weights based on the training evaluation\nindicators from UE. To solve the long-term mixed-integer linear programming problem, the attention-\nweighted federated deep reinforcement learning (AWFDRL) is presented, which optimizes the aggregation\nweights to avoid the imbalance of the local model quality and improve the learning efﬁciency of the DQN.\nThe convergence of the proposed algorithm is veriﬁed and simulation results show that the AWFDRL\nframework can perform well on average delay, hit rate, and ofﬂoad trafﬁc.\nA federated solution for cooperative edge caching management in fog radio access networks (F-RANs)\nis proposed [90]. Both edge computing and fog computing involve bringing intelligence and processing\nto the origins of data. The key difference between the two architectures is where the computing node is\npositioned. A dueling deep Q-network based cooperative edge caching method is proposed to overcome\nthe dimensionality curse of RL problem and improve caching performance. Agents are developed in fog\naccess points (F-APs) and allowed to build a local caching model for optimal caching decisions based\non the user content request and the popularity of content. HFRL is applied to aggregate the local models\ninto a global model in the cloud server. The proposed method outperforms three classical content caching\nmethods and two RL algorithms in terms of reducing content request delays and increasing cache hit\nrates.\n25\nFor edge-enabled IoT, Majidi et al. [91] proposes a dynamic cooperative caching method based on\nhierarchical federated deep reinforcement learning (HFDRL), which is used to determine which content\nshould be cached or evicted by predicting future user requests. Edge devices that have a strong relationship\nare grouped into a cluster and one head is selected for this cluster. The BS trains the Q-value based local\nmodel by using BS states, content states, and request states. The head has enough processing and caching\ncapabilities to deal with model aggregation in the cluster. By categorizing edge devices hierarchically,\nHFDRL improves the response time delay to keeps both small and large clusters from experiencing the\ndisadvantages they could encounter. Storage partitioning allows content to be stored in clusters at different\nlevels using the storage space of each device. The simulation results show the proposed method using\nMovieLens datasets improves the average content access delay and the hit rate.\nConsidering the low latency requirements and privacy protection issue of IoV, the study of efﬁcient and\nsecure caching methods has attracted many researchers. An FRL-empowered task caching problem with\nIoV has been analyzed by Zhao et al. [92]. The work proposes a novel cooperative caching algorithm\n(CoCaRL) for vehicular networks with multi-level FRL to dynamically determine which contents should\nbe replaced and where the content requests should be served. This paper develops a two-level aggregation\nmechanism for federated learning to speed up the convergence rate and reduces communication overhead,\nwhile DRL task is employed to optimize the cooperative caching policy between RSUs of vehicular\nnetworks. Simulation results show that the proposed algorithm can achieve a high hit rate, good adaptability\nand fast convergence in a complex environment.\nApart from caching services, FRL has demonstrated its strong ability to facilitate resource allocation in\nedge computing. In the study from Zhu et al. [93], the authors speciﬁcally focus on the data ofﬂoading task\nfor Mobile Edge Computing (MEC) systems. To achieve joint collaboration, the heterogeneous multi-agent\nactor-critic (H-MAAC) framework is proposed, in which edge devices independently learn the interactive\nstrategies through their own observations. The problem is formulated as a multi-agent MDP for modeling\nedge devices’ data allocation strategies, i.e., moving the data, locally executing or ofﬂoading to a cloud\nserver. The corresponding joint cooperation algorithm that combines the edge federated model with the\nmulti-agent actor-critic RL is also presented. Dual lightweight neural networks are built, comprising\noriginal actor/critic networks and target actor/critic networks.\nBlockchain technology has also attracted lot attention from researchers in edge computing ﬁelds since it\nis able to provide reliable data management within the massive distributed edge nodes. In the study from\nYu et al. [94], the intelligent ultra-dense edge computing (I-UDEC) framework is proposed, integrating\nwith blockchain and RL technologies into 5G ultra-dense edge computing networks. In order to achieve\nlow overhead computation ofﬂoading decisions and resource allocation strategies, authors design a two-\ntimescale deep reinforcement learning (2Ts-DRL) approach, which consists of a fast-timescale and a slow-\ntimescale learning process. The target model can be trained in a distributed manner via FL architecture,\nprotecting the privacy of edge devices.\nAdditionally, to deal with the different types of optimization tasks, variants of FRL are being studied.\nZhu et al. presents a resource allocation method for edge computing systems, called concurrent federated\nreinforcement learning (CFRL) [95]. The edge node continuously receives tasks from serviced IoT devices\nand stores those tasks in a queue. Depending on its own resource allocation status, the node determines\nthe scheduling strategy so that all tasks are completed as soon as possible. In case the edge host does\nnot have enough available resources for the task, the task can be ofﬂoaded to the server. Contrary to the\ndeﬁnition of the central server in the basic FRL, the aim of central server in CFRL is to complete the\ntasks that the edge nodes cannot handle instead of aggregating local models. Therefore, the server needs\nto train a special resource allocation model based on its own resource status, forwarded tasks and unique\nrewards. The main idea of CFRL is that edge nodes and the server cooperatively participate in all task\nprocessing in order to reduce total computing time and provide a degree of privacy protection.\n26\nB. FRL for communication networks\nIn parallel with the continuous evolution of communication technology, a number of heterogeneous\ncommunication systems are also being developed to adapt to different scenarios. Many researchers are also\nworking toward intelligent management of communication systems. The traditional ML-based management\nmethods are often inefﬁcient due to their centralized data processing architecture and the risk of privacy\nleakage [5]. FRL can play an important role in services slicing and access controlling to replace centralized\nML methods.\nIn communication network services, Network Function Virtualization (NFV) is a critical component\nof achieving scalability and ﬂexibility. Huang et al. proposes a novel scalable service function chains\norchestration (SSCO) scheme for NFV-enabled networks via FRL[96]. In the work, a federated-learning-\nbased framework for training global learning, along with a time-variant local model exploration, is designed\nfor scalable SFC orchestration. It prevents data sharing among stakeholders and enables quick convergence\nof the global model. To reduce communication costs, SSCO allows the parameters of local models to be\nupdated just at the beginning and end of each episode through distributed clients and the cloud server.\nA DRL approach is used to map Virtual Network Functions (VNFs) into networks with local knowledge\nof resources and instantiation cost. In addition, the authors also propose a loss-weight-based mechanism\nfor generation and exploitation of reference samples for training in replay buffers, avoiding the strong\nrelevance of each sample. Simulation results demonstrate that SSCO can signiﬁcantly reduce placement\nerrors and improve resource utilization ratios to place time-variant VNFs, as well as achieving desirable\nscalability.\nNetwork slicing (NS) is also a form of virtual network architecture to support divergent requirements\nsustainably. The work from Liu et al. [97] proposes a device association scheme (such as access control\nand handover management) for Radio Access Network (RAN) slicing by exploiting a hybrid federated\ndeep reinforcement learning (HDRL) framework. In view of the large state-action space and variety of\nservices, HDRL is designed with two layers of model aggregations. Horizontal aggregation deployed on\nBSs is used for the same type of service. Generally, data samples collected by different devices within\nthe same service have similar features. The discrete-action DRL algorithm, i.e., DDQN, is employed to\ntrain the local model on individual smart devices. BS is able to aggregate model parameters and establish\na cooperative global model. Vertical aggregation developed on the third encrypted party is responsible\nfor the services of different types. In order to promote collaboration between devices with different\ntasks, authors aggregate local access features to form a global access feature, in which the data from\ndifferent ﬂows is strongly correlated since different data ﬂows are competing for radio resources with\neach other. Furthermore, the Shapley value [98], which represents the average marginal contribution of a\nspeciﬁc feature across all possible feature combinations, is used to reduce communication cost in vertical\naggregation based on the global access feature. Simulation results show that HDRL can improve network\nthroughput and communication efﬁciency.\nThe Open Radio Access Network (O-RAN) has emerged as a paradigm for supporting multi-class\nwireless services in 5G and beyond networks. To deal with the two critical issues of load balance and\nhandover control, Cao et al. proposes a federated DRL-based scheme to train the model for user access\ncontrol in the O-RAN [99]. Due to the mobility of UEs and the high cost of the handover between\nBSs, it is necessary for each UE to access the appropriate BS to optimize its throughput performance.\nAs independent agents, UEs make access decisions with assistance from a global model server, which\nupdates global DQN parameters by averaging DQN parameters of selected UEs. Further, the scheme\nproposes only partially exchanging DQN parameters to reduce communication overheads, and using the\ndueling structure to allow convergence for independent agents. Simulation results demonstrate that the\nscheme increases long-term throughput while avoiding frequent handovers of users with limited signaling\noverheads.\nThe issue of optimizing user access is important in wireless communication systems. FRL can provide\ninteresting solutions for enabling efﬁcient and privacy-enhanced management of access control. Zhang et\n27\nal. [100] studies the problem of multi-user access in WIFI networks. In order to mitigate collision events\non channel access, an enhanced multiple access mechanism based on FRL is proposed for user-dense\nscenarios. In particular, distributed stations train their local q-learning networks through channel state,\naccess history and feedback from central Access Point (AP). AP uses the central aggregation algorithm to\nupdate the global model every period of time and broadcast it to all stations. In addition, a Monte Carlo\n(MC) reward estimation method for the training phase of local model is introduced, which allocates more\nweight to the reward of that current state by reducing the previous cumulative reward.\nFRL is also studied for Intelligent Cyber-Physical Systems (ICPS), which aims to meet the requirements\nof intelligent applications for high-precision, low-latency analysis of big data. In light of the heterogeneity\nbrought by multiple agents, the central RL-based resource allocation scheme has non-stationary issues and\ndoes not consider privacy issues. Therefore, the work from Xu et al. [101] proposes a multi-agent FRL\n(MA-FRL) mechanism which synthesizes a good inferential global policy from encrypted local policies\nof agents without revealing private information. The data resource allocation and secure communication\nproblems are formulated as a Stackelberg game with multiple participants, including near devices (NDs),\nfar devices (FDs) and relay devices (RDs). Take into account the limited scope of the heterogeneous\ndevices, the authors model this multi-agent system as a POMDP. Furthermore, it is proved that MA-FRL\nis µ-strongly convex and β-smooth and derives its convergence speed in expectation.\nZhang et al. [102] pays attention to the challenges in cellular vehicle-to-everything (V2X) commu-\nnication for future vehicular applications. A joint optimization problem of selecting the transmission\nmode and allocating the resources is presented. This paper proposes a decentralized DRL algorithm\nfor maximizing the amount of available vehicle-to-infrastructure capacity while meeting the latency\nand reliability requirements of vehicle-to-vehicle (V2V) pairs. Considering limited local training data\nat vehicles, the federated learning algorithm is conducted on a small timescale. On the other hand, the\ngraph theory-based vehicle clustering algorithm is conducted on a large timescale.\nThe development of communication technologies in extreme environments is important, including deep\nunderwater exploration. The architecture and philosophy of FRL are applied to smart ocean applications in\nthe study of Kwon [103]. To deal with the nonstationary environment and unreliable channels of underwater\nwireless networks, the authors propose a multi-agent DRL-based algorithm that can realize FL computation\nwith Internet-of-Underwater-Things (IoUT) devices in the ocean environment. The cooperative model is\ntrained by MADDPG for cell association and resource allocation problems. As for downlink throughput,\nit is found that the proposed MADDPG-based algorithm performed 80% and 41% better than the standard\nactor-critic and DDPG algorithms, respectively.\nC. FRL for control optimization\nReinforcement learning based control schemes are considered as one of the most effective ways to\nlearn a nonlinear control strategy in complex scenarios, such as robotics. Individual agent’s exploration\nof the environment is limited by its own ﬁeld of vision and usually needs a great deal of training to\nobtain the optimal strategy. The FRL-based approach has emerged as an appealing way to realize control\noptimization without exposing agent data or compromising privacy.\nAutomated control of robots is a typical example of control optimization problems. Liu et al. [57]\ndiscusses robot navigation scenarios and focuses on how to make robots transfer their experience so\nthat they can make use of prior knowledge and quickly adapt to changing environments. As a solution,\na cooperative learning architecture, called LFRL, is proposed for navigation in cloud robotic systems.\nUnder the FRL-based architecture, the authors propose a corresponding knowledge fusion algorithm to\nupgrade the shared model deployed on the cloud. In addition, the paper also discusses the problems and\nfeasibility of applying transfer learning algorithms to different tasks and network structures between the\nshared model and the local model.\nFRL is combined with autonomous driving of robotic vehicles in the study of Liang et al. [104]. To\nreach rapid training from a simulation environment to a real-world environment, Liang et al. presents a\n28\nFederated Transfer Reinforcement Learning (FTRL) framework for knowledge extraction where all the\nvehicles make corresponding actions with the knowledge learned by others. The framework can potentially\nbe used to train more powerful tasks by pooling the resources of multiple entities without revealing raw\ndata information in real-life scenarios. To evaluate the feasibility of the proposed framework, authors\nperform real-life experiments on steering control tasks for collision avoidance of autonomous driving\nrobotic cars and it is demonstrated that the framework has superior performance to the non-federated\nlocal training process. Note that the framework can be considered an extension of HFRL, because the\ntarget tasks to be accomplished are highly-relative and all observation data are pre-aligned.\nFRL also appears as an attractive approach for enabling intelligent control of IoT devices without\nrevealing private information. Lim et al. proposes a FRL architecture which allows agents working on\nindependent IoT devices to share their learning experiences with each other, and transfer the policy model\nparameters to other agents [105]. The aim is to effectively control multiple IoT devices of the same type\nbut with slightly different dynamics. Whenever an agent meets the predeﬁned criteria, its mature model\nwill be shared by the server with all other agents in training. The agents continue training based on the\nshared model until the local model converges in the respective environment. The actor-critical proximal\npolicy optimization (Actor-Critic PPO) algorithm is integrated into the control of multiple rotary inverted\npendulum (RIP) devices. The results show that the proposed architecture facilitates the learning process\nand if more agents participate the learning speed can be improved. In addition, Lim et al. [106] uses\nFRL architecture based on a multi-agent environment to solve the problems and limitations of RL for\napplications to the real-world problems. The proposed federation policy allows multiple agents to share\ntheir learning experiences to get better learning efﬁcacy. The proposed scheme adopts Actor-Critic PPO\nalgorithm for four types of RL simulation environments from OpenAI Gym as well as RIP in real control\nsystems. Compared to a previous real-environment study, the scheme enhances learning performance by\napproximately 1.2 times.\nD. FRL for attack detection\nWith the heterogeneity of services and the sophistication of threats, it is challenging to detect these\nattacks using traditional methods or centralized ML-based methods, which have a high false alarm rate\nand do not take privacy into account. FRL offers a powerful alternative to detecting attacks and provides\nsupport for network defense in different scenarios.\nBecause of various constraints, IoT applications have become a primary target for malicious adversaries\nthat can disrupt normal operations or steal conﬁdential information. In order to address the security issues\nin ﬂying ad-hoc network (FANET), Mowla et al. proposes an adaptive FRL-based jamming attack defense\nstrategy for unmanned aerial vehicles (UAVs) [107]. A model-free Q-learning mechanism is developed and\ndeployed on distributed UAVs to cooperatively learn detection models for jamming attacks. According to\nthe results, the average accuracy of the federated jamming detection mechanism, employed in the proposed\ndefense strategy, is 39.9% higher than the distributed mechanism when veriﬁed with the CRAWDAD\nstandard and the ns-3 simulated FANET jamming attack dataset.\nAn efﬁcient trafﬁc monitoring framework, known as DeepMonitor, is presented in the study of Nguyen\net al. [108] to provide ﬁne-grained trafﬁc analysis capability at the edge of Software Deﬁned Network\n(SDN) based IoT networks. The agents deployed in edge nodes consider the different granularity-level\nrequirements and their maximum ﬂow-table capacity to achieve the optimal ﬂow rule match-ﬁeld strategy.\nThe control optimization problem is formulated as the MDP and a federated DDQN algorithm is developed\nto improve the learning performance of agents. The results show that the proposed monitoring framework\ncan produce reliable trafﬁc granularity at all levels of trafﬁc granularity and substantially mitigate the\nissue of ﬂow-table overﬂows. In addition, the Distributed Denial Of Service (DDoS) attack detection\nperformance of an intrusion detection system can be enhanced by up to 22.83% by using DeepMonitor\ninstead of FlowStat.\nIn order to reduce manufacturing costs and improve production efﬁciency, the Industrial Internet of\nThings (IIoT) is proposed as a potentially promising research direction. It is a challenge to implement\n29\nanomaly detection mechanisms in IIoT applications with data privacy protection. Wang et al. proposes a\nreliable anomaly detection strategy for IIoT using FRL techniques[109]. In the system framework, there\nare four entities involved in establishing the detection model, i.e., the Global Anomaly Detection Center\n(GADC), the Local Anomaly Detection Center (LADC), the Regional Anomaly Detection Center (RADC),\nand the users. The anomaly detection is suggested to be implemented in two phases, including anomaly\ndetection for RADC and users. Especially, the GADC can build global RADC anomaly detection models\nbased on local models trained by LADCs. Different from RADC anomaly detection based on action\ndeviations, user anomaly detection is mainly concerned with privacy leakage and is employed by RADC\nand GADC. Note that the DDPG algorithm is applied for local anomaly detection model training.\nE. FRL for other applications\nDue to the outstanding performance of training efﬁciency and privacy protection, many researchers are\nexploring the possible applications of FRL.\nFL has been applied to realize distributed energy management in IoT applications. In the revolution\nof smart home, smart meters are deployed in the advanced metering infrastructure (AMI) to monitor and\nanalyze the energy consumption of users in real-time. As an example [110], the FRL-based approach\nis proposed for the energy management of multiple smart homes with solar PVs, home appliances, and\nenergy storage. Multiple local home energy management systems (LHEMSs) and a global server (GS)\nmake up FRL architecture of the smart home. DRL agents for LHEMSs construct and upload local models\nto the GS by using energy consumption data. The GS updates the global model based on local models\nof LHEMSs using the federated stochastic gradient descent (FedSGD) algorithm. Under heterogeneous\nhome environments, simulation results indicate that the proposed approach outperforms others when it\ncomes to convergence speed, appliance energy consumption, and the number of agents.\nMoreover, FRL offers an alternative to share information with low latency and privacy preservation. The\ncollaborative perception of vehicles provided by IoV can greatly enhance the ability to sense things beyond\ntheir line of sight, which is important for autonomous driving. Region quadtrees have been proposed as\na storage and communication resource-saving solution for sharing perception information [111]. It is\nchallenging to tailor the number and resolution of transmitted quadtree blocks to bandwidth availability.\nIn the framework of FRL, Mohamed et al. presents a quadtree-based point cloud compression mechanism\nto select cooperative perception messages[112]. Speciﬁcally, over a period of time, each vehicle covered\nby an RSU transfers its latest network weights with the RSU, which then averages all of the received\nmodel parameters and broadcasts the result back to the vehicles. Optimal sensory information transmission\n(i.e., quadtree blocks) and appropriate resolution levels for a given vehicle pair are the main objectives of\na vehicle. The dueling and branching concepts are also applied to overcome the vast action space inherent\nin the formulation of the RL problem. Simulation results show that the learned policies achieve higher\nvehicular satisfaction and the training process is enhanced by FRL.\nF. Lessons Learned\nIn the following, we summarize the major lessons learned from this survey in order to provide a\ncomprehensive understanding of current research on FRL applications.\n1) Lessons learned from the aggregation algorithms: The existing FRL literature usually uses classical\nDRL algorithms, such as DQN and DDPG, at the participants, while the gradients or parameters of the\ncritic and/or actor networks are periodically reported synchronously or asynchronously by the participants\nto the coordinator. The coordinator then aggregates the parameters or gradients and sends the updated\nvalues to the participants. In order to meet the challenges presented by different scenarios, the aggregation\nalgorithms have been designed as a key feature of FRL. In the original FedAvg algorithm [12], the number\nof samples in a participant’s dataset determines its inﬂuence on the global model. In accordance with this\nidea, several papers propose different methods to calculate the weights in the aggregation algorithms\naccording to the requirement of application. In the study from Lim et al. [106], the aggregation weight is\n30\nderived from the average of the cumulative rewards of the last ten episodes. Greater weights are placed\non the models of those participants with higher rewards. In contrast to the positive correlation of reward,\nHuang et al. [96] takes the error rate of action as an essential factor to assign weights for participating\nin the global model training. In D2D -assisted edge caching, Wang et al. [89] uses the reward and some\ndevice-related indicators as the measurement to evaluate the local model’s contribution to the global model.\nMoreover, the existing FRL methods based on ofﬂine DRL algorithms, such DQN and DDPG, usually\nuse experience replay. Sampling random batch from replay memory can break correlations of continuous\ntransition tuples and accelerate the training process. To arrive at an accurate evaluation of the participants,\nthe paper [102] calculates the aggregation weight based on the size of the training batch in each iteration.\nThe above aggregation methods can effectively deal with the issue of data imbalance and performance\ndiscrepancy between participants, but it is hard for participants to cope with subtle environmental dif-\nferences. According to the paper [105], as soon as a participant reaches the predeﬁned criteria in its\nown environment, it should stop learning and send its model parameters as a reference to the remaining\nindividuals. Exchanging mature network models (satisfying terminal conditions) can help other participants\ncomplete their training quickly. Participants in other similar environments can continue to use FRL for\nfurther updating their parameters to achieve the desired model performance according to their individual\nenvironments. Liu et al. [57] also suggests that the sharing global model in the cloud is not the ﬁnal policy\nmodel for local participants. An effective transfer learning should be applied to resolve the structural\ndifference between the shared network and private network.\n2) Lessons learned from the relationship between FL and RL: In most of the literature on FRL,\nFL is used to improve the performance of RL. With FL, the learning experience can be shared among\ndecentralized multiple parties while ensuring privacy and scalability without requiring direct data ofﬂoading\nto servers or third parties. Therefore, FL can expand the scope and enhance the security of RL. Among\nthe applications of FRL, most researchers focus on the communication network system due to its robust\nsecurity requirements, advanced distributed architecture, and a variety of decision-making tasks. Data\nofﬂoading [93] and caching [89] solutions powered by distributed AI are available from FRL. In addition,\nwith the ability to detect a wide range of attacks and support defense solutions, FRL has emerged as\na strong alternative for performing distributed learning for security-sensitive scenarios. Enabled by the\nprivacy-enhancing and cooperative features, detection and defense solutions can be learned quickly where\nmultiple participants join to build a federated model [107], [109]. FRL can also provide viable solutions to\nrealize intelligence for control systems with many applied domains such as robotics [57] and autonomous\ndriving [104] without data exchange and privacy leakage. The data owners (robot or vehicle) may not\ntrust the third-party server and therefore hesitate to upload their private information to potentially insecure\nlearning systems. Each participant of FRL runs a separate RL model for determining its own control policy\nand gains experience by sharing model parameters, gradients or losses.\nMeanwhile, RL may have the potential to optimize FL schemes and improve the efﬁciency of training.\nDue to the unstable network connectivity, it is not practical for FL to update and aggregate models\nsimultaneously across all participants. Therefore, Wang et al. [113] proposes a RL-based control framework\nthat intelligently chooses the participants to participate in each round of FL with the aim to speed up\nconvergence. Similarly, Zhang et al. [114] applies RL to pre-select a set of candidate edge participants, and\nthen determine reliable edge participants through social attribute perception. In IoT or IoV scenarios, due\nto the heterogeneous nature of participating devices, different computing and communication resources\nare available to them. RL can speed up training by coordinating the allocation of resources between\nparticipants. Zhan et al. [115] deﬁnes the L4L (Learning for Learning) concept, i.e., use RL to improve\nFL. Using the heterogeneity of participants and dynamic network connections, this paper investigates a\ncomputational resource control problem for FL that simultaneously considers learning time and energy\nefﬁciency. An experience-driven resource control approach based on RL is presented to derive the near-\noptimal strategy with only the participants’ bandwidth information in the previous training rounds. In\naddition, as with any other ML algorithm, FL algorithms are vulnerable to malicious attacks. RL has\nbeen studied to defend against attacks in various scenarios, and it can also enhance the security of FL.\n31\nThe paper [116] proposes a reputation-aware RL (RA-RL) based selection method to ensure that FL is\nnot disrupted. The participating devices’ attributes, including computing resources and trust values, etc,\nare used as part of the environment in RL. In the aggregation of the global model, devices with high\nreputation levels will have a greater chance of being considered to reduce the effects of malicious devices\nmixed into FL.\n3) Lessons learned from categories of FRL: As discussed above, FRL can be divided into two main\ncategories, i.e., HFRL and VFRL. Currently, most of the existing research is focused on HFRL, while\nlittle attention is devoted to VFRL. The reason for this is that HFRL has obvious application scenarios,\nwhere multiple participants have similar decision-making tasks with individual environments, such as\ncaching allocation [59], ofﬂoading optimization [58], and attack monitoring [108]. The participants and\ncoordinator only need to train a similar model with the same state and action spaces. Consequently, the\nalgorithm design can be implemented and the training convergence can be veriﬁed relatively easily. On\nthe other hand, even though VFRL has a higher degree of technical difﬁculty at the algorithm design level,\nit also has a wide range of possible applications. In a multi-agent scenario, for example, a single agent is\nlimited by its ability to observe only part of the environment, whereas the transition of the environment\nis determined by the behavior of all the agents. Zhuo et al. [65] assumes agents cannot share their partial\nobservations of the environment and some agents are unable to receive rewards. The federated Q-network\naggregation algorithm between two agents is proposed for VFRL. The paper [97] speciﬁcally applies both\nHFRL and VFRL for radio access network slicing. For the same type of services, similar data samples are\ntrained locally at participating devices, and BSs perform horizontal aggregation to integrate a cooperative\naccess model by adopting an iterative approach. The terminal device also can optimize the selection of\nbase stations and network slices based on the global model of VFRL, which aggregates access features\ngenerated by different types of services on the third encrypted party. The method improves the device’s\nability to select the appropriate access points when initiating different types of service requests under\nrestrictions regarding privacy protection. The feasible implementation of VFRL also provides guidance\nfor future research.\nVI. OPEN ISSUES AND FUTURE RESEARCH DIRECTIONS\nAs we presented in the previous section, FRL serves an increasingly important role as an enabler of\nvarious applications. While the FRL-based approach possesses many advantages, there are a number of\ncritical open issues to consider for future implementation. Therefore, this section focuses on several key\nchallenges, including those inherited from FL such as security and communication issues, as well as those\nunique to FRL. Research on tackling these issues offers interesting directions for the future.\nA. Learning convergence in HFRL\nIn realistic HFRL scenarios, while the agents perform similar tasks, the inherent dynamics for the\ndifferent environments in which the agents reside are usually not exactly identically distributed. The\nslight difference in the stochastic properties of the transition models for multiple agents could cause the\nlearning convergence issue. One possible method to address this problem is by adjusting the frequency of\nglobal aggregation, i.e., after each global aggregation, a period of time is left for each agent to ﬁne-tune\nits local parameters according to its own environment. Apart from the non-identical environment problem,\nanother interesting and important problem is how to leverage FL to make RL algorithms converge better\nand faster. It is well-known that DRL algorithms could be unstable and diverge, especially when off-policy\ntraining is combined with function approximation and bootstrapping. In FRL, the training curves of some\nagents could diverge while others converge although the agents are trained in the exact replicas of the\nsame environment. By leveraging FL, it is envisioned that we could expedite the training process as well\nas increase the stability. For example, we could selectively aggregate the parameters of a subset of agents\nwith a larger potential for convergence, and later transfer the converged parameters to all the agents. To\ntackle the above problems, several possible solutions proposed for FL algorithm contains certain reference\n32\nsigniﬁcance. For example, server operators could account for heterogeneity inherent in partial information\nby adding a proximal term [117]. The local updates submitted by agents are constrained by the tunable\nterm and have a different effect on the global parameters. In addition, a probabilistic agent selection\nscheme can be implemented to select the agents whose local FL models have signiﬁcant effects on the\nglobal model to minimize the FL convergence time and the FL training loss [118]. Another problem is\ntheoretical analysis of the convergence bounds. Although some existing studies have been directed at this\nproblem [119], the convergence can be guaranteed since the loss function is convex. How to analyze and\nevaluate the non-convex loss functions in HFRL is also an important research topic in the future.\nB. Agents without rewards in VFRL\nIn most existing works, all the RL agents have the ability to take part in full interaction with the\nenvironment and can generate their own actions and rewards. Even though some MARL agents may not\nparticipate in the policy decision, they still generate their own reward for evaluation. In some scenarios,\nspecial agents in VFRL take the role of providing assistance to other agents. They can only observe\nthe environment and pass on the knowledge of their observation, so as to help other agents make more\neffective decisions. Therefore, such agents do not have their own actions and rewards. The traditional RL\nmodels cannot effectively deal with this thorny problem. Many algorithms either directly use the states\nof such agents as public knowledge in the system model or design corresponding action and reward for\nsuch agents, which may be only for convenience of calculation and have no practical signiﬁcance. These\napproaches cannot fundamentally overcome the challenge, especially when privacy protection is also an\nessential objective to be complied with. Although the FedRL algorithm [65] is proposed to deal with the\nabove problem, which has demonstrated good performance, there are still some limitations. First of all,\nthe number of agents used in experiments and algorithms is limited to two, which means the scalability\nof this algorithm is not high and VFRL algorithms for a large number of agents need to be designed.\nSecondly, this algorithm uses Q-network as the federated model, which is a relatively simple algorithm.\nTherefore, how to design VFRL models based on other more complex and changeable networks remains\nan open issue.\nC. Communications\nIn FRL, the agents need to exchange the model parameters, gradients, intermediate results, etc., between\nthemselves or with a central server. Due to the limited communication resources and battery capacity,\nthe communication cost is an important consideration when implementing these applications. With an\nincreased number of participants, the coordinator has to bear more network workload within the client-\nserver FRL model [120]. This is because each participant needs to upload and download model updates\nthrough the coordinator. Although the distributed peer-to-peer model does not require a central coordinator,\neach agent may have to exchange information with other participants more frequently. In current research\nfor distributed models, there are no effective model exchange protocols to determine when to share\nexperiences with which agents. In addition, DRL involves updating parameters in deep neural networks.\nSeveral popular DRL algorithms, such as DQN [121] and DDPG [122], consist of multiple layers or\nmultiple networks. Model updates contain millions of parameters, which isn’t feasible for scenarios with\nlimited communication resources. The research directions for the above issues can be divided into three\ncategories. First, it is necessary to design a dynamic update mechanism for participants to optimize the\nnumber of model exchanges. A second research direction is to use model compression algorithms to\nreduce the amount of communication data. Finally, aggregation algorithms that allow participants to only\nsubmit the important parts of local model should be studied further.\nD. Privacy and Security\nAlthough FL provides privacy protection that allows the agents to exchange information in a secure\nmanner during the learning process, it still has several privacy and security vulnerabilities associated\n33\nwith communication and attack [123]. As FRL is implemented based on FL algorithms, these problems\nalso exist in FRL in the same or variant form. It is important to note that the data poisoning attack is\na different attack mode between FL and FRL. In the existing classiﬁcation tasks of FL, each piece of\ntraining data in the dataset corresponds to a respective label. The attacker ﬂips the labels on training\nexamples in one category onto another while the features of the examples are kept unchanged, misguiding\nthe establishment of a target model [124]. However, in the decision-making task of FRL, the training data\nis continuously generated from the interaction between the agent and the environment. As a result, the\ndata poisoning attack is implemented in another way. For example, the malicious agent tampers with the\nreward, which causes the evaluative function to shift. An option is to conduct regular safety assessments\nfor all participants. Participants whose evaluation indicator falls below the threshold are punished to reduce\nthe impact on the global model [125]. Apart form the insider attacks which are launched by the agents in\nthe FRL system, there may be various outsider attacks which are launched by intruders or eavesdroppers.\nIntruders may hide in the environment where the agent is and manipulate the transitions of environment\nto achieve speciﬁc goals. In addition, by listening to the communication between the coordinator and the\nagent, the eavesdropper may infer sensitive information from exchanging parameters and gradients [126].\nTherefore, the development of technology that detects and protects against attacks and privacy threats\ndoes have great potential and is urgently needed.\nE. Join and exit mechanisms design\nOne overlooked aspect of FRL-based research is the join and exit process of participants. In practice,\nthe management of participants is essential to the normal progression of cooperation. As mentioned earlier\nin the security issue, the penetration of malicious participants severely impacts the performance of the\ncooperative model and the speed of training. The joining mechanism provides participants with the legal\nstatus to engage in federated cooperation. It is the ﬁrst line of defense against malicious attackers. In\ncontrast, the exit mechanism signiﬁes the cancellation of the permission for cooperation. Participant-\ndriven or enforced exit mechanisms are both possible. In particular, for synchronous algorithms, ignoring\nthe exit mechanism can negatively impact learning efﬁciency. This is because the coordinator needs to wait\nfor all participants to submit their information. In the event that any participant is ofﬂine or compromised\nand unable to upload, the time for one round of training will be increased indeﬁnitely. To address the\nbottleneck, a few studies consider updating the global model using the selected models from a subset of\nparticipants [127], [113]. Unfortunately, there is no comprehensive consideration of the exit mechanism,\nand the communication of participants is typically assumed to be reliable. Therefore, research gaps of\nFRL still exist in joining and exiting mechanisms. It is expected that the coordinator or monitoring system,\nupon discovering a failure, disconnection, or malicious participant, will use the exit mechanism to reduce\nits impact on the global model or even eliminate it.\nF. Incentive mechanisms\nFor most studies, the agents taking part in the FRL process are assumed to be honest and voluntary.\nEach agent provides assistance for the establishment of the cooperation model following the rules and\nfreely shares the masked experience through encrypted parameters or gradients. An agent’s motivation for\nparticipation may come from regulation or incentive mechanisms. The FRL process within an organization\nis usually governed by regulations. For example, BSs belonging to the same company establish a joint\nmodel for ofﬂoading and caching. Nevertheless, because participants may be members of different orga-\nnizations or use disparate equipment, it is difﬁcult for regulation to force all parties to share information\nlearned from their own data in the same manner. If there are no regulatory measures, participants prone\nto selﬁsh behavior will only beneﬁt from the cooperation model but not submit local updates. Therefore,\nthe cooperation of multiple parties, organizations, or individuals requires a fair and efﬁcient incentive\nmechanism to encourage their active participation. In this way, agents providing more contributions can\nbeneﬁt more and selﬁsh agents unwilling to share there learning experience will receive less beneﬁt. As\n34\nan example, Google Keyboard [128] users can choose whether or not to allow Google to use their data,\nbut if they do, they can beneﬁt from more accurate word prediction. Although an incentive mechanism in\na context-aware manner among data owners is proposed in the study from Yu et al. [129], it is not suitable\nfor the RL problems. There is still no clear plan of action regarding how the FRL-based application can\nbe designed to create a reasonable incentive mechanism for inspiring agents to participate in collaborative\nlearning. To be successful, future research needs to propose a quantitative standard for evaluating the\ncontribution of agents in FRL.\nG. Peer-to-peer cooperation\nFRL applications have the option of choosing between a central server-client model as well as a\ndistributed peer-to-peer model. A distributed model can not only eliminate the single point of failure, but\nit can also improve energy efﬁciency signiﬁcantly by allowing models to be exchanged directly between\ntwo agents. In a typical application, two adjacent cars share experience learned from road condition\nenvironment in the form of models with D2D communications to assist autonomous driving. However, the\ndistributed cooperation increases the complexity of the learning system and imposes stricter requirements\nfor application scenarios. This research should include, but not be limited to, the agent selection method\nfor the exchange model, the mechanism for triggering the model exchange, the improvement of algorithm\nadaptability, and the convergence analysis of the aggregation algorithm.\nVII. CONCLUSION\nAs a new and potential branch of RL, FL can make learning safer and more efﬁcient while leveraging\nthe beneﬁts of FL. We have discussed the basic deﬁnitions of FL and RL as well as our thoughts on their\nintegration in this paper. In general, FRL algorithms can be classiﬁed into two categories, i.e., HFRL and\nVFRL. Thus, the deﬁnition and general framework of these two categories have been given. Speciﬁcally,\nwe have highlighted the difference between HFRL and VFRL. Then, a lot of existing FRL schemes have\nbeen summarized and analyzed according to different applications. Finally, the potential challenges in the\ndevelopment of FRL algorithms have been explored. Several open issues of FRL have been identiﬁed,\nwhich will encourage more efforts toward further research in FRL.\nREFERENCES\n[1] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D. Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen,\nS. Legg, V. Mnih, K. Kavukcuoglu, and D. Silver, “Massively parallel methods for deep reinforcement learning,” CoRR, vol.\nabs/1507.04296, 2015. [Online]. Available: http://arxiv.org/abs/1507.04296\n[2] M. Grounds and D. Kudenko, “Parallel reinforcement learning with linear function approximation,” in Adaptive Agents and Multi-Agent\nSystems III. Adaptation and Multi-Agent Learning, K. Tuyls, A. Nowe, Z. Guessoum, and D. Kudenko, Eds.\nBerlin, Heidelberg:\nSpringer Berlin Heidelberg, 2008, pp. 60–74.\n[3] A. V. Clemente, H. N. C. Mart´ınez, and A. Chandra, “Efﬁcient parallel methods for deep reinforcement learning,” CoRR, vol.\nabs/1705.04862, 2017. [Online]. Available: http://arxiv.org/abs/1705.04862\n[4] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang, D. Niyato, and C. Miao, “Federated learning in mobile edge\nnetworks: A comprehensive survey,” IEEE Communications Surveys Tutorials, vol. 22, no. 3, pp. 2031–2063, 2020.\n[5] D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, and H. Vincent Poor, “Federated learning for internet of things: A\ncomprehensive survey,” IEEE Communications Surveys Tutorials, vol. 23, no. 3, pp. 1622–1658, 2021.\n[6] L. U. Khan, W. Saad, Z. Han, E. Hossain, and C. S. Hong, “Federated learning for internet of things: Recent advances, taxonomy,\nand open challenges,” IEEE Communications Surveys Tutorials, vol. 23, no. 3, pp. 1759–1799, 2021.\n[7] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, and H. Yu.\nMorgan & Claypool, 2019.\n[8] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept and applications,” ACM Transactions on Intelligent\nSystems and Technology (TIST), vol. 10, no. 2, pp. 1–19, 2019.\n[9] L. Qinbin, W. Zeyi, and H. Bingsheng, “Federated learning systems: Vision, hype and reality for data privacy and protection,”\nCoRR, vol. abs/1907.09693, 2019. [Online]. Available: http://arxiv.org/abs/1907.09693\n[10] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning: Challenges, methods, and future directions,” IEEE Signal Processing\nMagazine, vol. 37, no. 3, pp. 50–60, 2020.\n[11] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan, “Adaptive federated learning in resource constrained\nedge computing systems,” IEEE Journal on Selected Areas in Communications, vol. 37, no. 6, pp. 1205–1221, 2019.\n[12] H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Communication-efﬁcient learning of deep networks from decentralized\ndata,” CoRR, vol. abs/1602.05629, 2016. [Online]. Available: http://arxiv.org/abs/1602.05629\n35\n[13] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “Privacy-preserving deep learning via additively homomorphic encryption,”\nIEEE Transactions on Information Forensics and Security, vol. 13, no. 5, pp. 1333–1345, 2018.\n[14] H. Zhu and Y. Jin, “Multi-objective evolutionary federated learning,” IEEE Transactions on Neural Networks and Learning Systems,\nvol. 31, no. 4, pp. 1310–1322, 2020.\n[15] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. A. Bonawitz, Z. Charles, G. Cormode, R. Cummings,\nR. G. L. D’Oliveira, S. E. Rouayheb, D. Evans, J. Gardner, Z. Garrett, A. Gasc´on, B. Ghazi, P. B. Gibbons, M. Gruteser,\nZ. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi, G. Joshi, M. Khodak, J. Koneˇcn´y, A. Korolova,\nF. Koushanfar, S. Koyejo, T. Lepoint, Y. Liu, P. Mittal, M. Mohri, R. Nock, A. ¨Ozg¨ur, R. Pagh, M. Raykova, H. Qi, D. Ramage,\nR. Raskar, D. Song, W. Song, S. U. Stich, Z. Sun, A. T. Suresh, F. Tram`er, P. Vepakomma, J. Wang, L. Xiong, Z. Xu, Q. Yang,\nF. X. Yu, H. Yu, and S. Zhao, “Advances and open problems in federated learning,” CoRR, vol. abs/1912.04977, 2019. [Online].\nAvailable: http://arxiv.org/abs/1912.04977\n[16] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp.\n1345–1359, 2010.\n[17] Y. Li, “Deep reinforcement learning: An overview,” CoRR, vol. abs/1701.07274, 2017. [Online]. Available: http://arxiv.org/abs/1701.\n07274\n[18] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang, “Experience-driven networking: A deep reinforcement learning\nbased approach,” in IEEE INFOCOM 2018-IEEE Conference on Computer Communications.\nIEEE, 2018, pp. 1871–1879.\n[19] M. Mohammadi, A. Al-Fuqaha, M. Guizani, and J.-S. Oh, “Semisupervised deep reinforcement learning in support of iot and smart\ncity services,” IEEE Internet of Things Journal, vol. 5, no. 2, pp. 624–635, 2018.\n[20] F. Bu and X. Wang, “A smart agriculture iot system based on deep reinforcement learning,” Future Generation Computer Systems,\nvol. 99, pp. 500–507, 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0167739X19307277\n[21] X. Xiong, K. Zheng, L. Lei, and L. Hou, “Resource allocation based on deep reinforcement learning in iot edge computing,” IEEE\nJournal on Selected Areas in Communications, vol. 38, no. 6, pp. 1133–1146, 2020.\n[22] L. Lei, J. Qi, and K. Zheng, “Patent analytics based on feature vector space model: A case of iot,” IEEE Access, vol. 7, pp. 45 705–\n45 715, 2019.\n[23] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement learning for autonomous driving,” CoRR, vol.\nabs/1610.03295, 2016. [Online]. Available: http://arxiv.org/abs/1610.03295\n[24] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement learning framework for autonomous driving,” Electronic\nImaging, vol. 2017, no. 19, pp. 70–76, 2017. [Online]. Available: 10.2352/ISSN.2470-1173.2017.19.AVM-023\n[25] M. E. Taylor, “Teaching reinforcement learning with mario: An argument and case study,” in Second AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, 2011. [Online]. Available: https://www.aaai.org/ocs/index.php/EAAI/EAAI11/paper/viewPaper/3515\n[26] S. D. Holcomb, W. K. Porter, S. V. Ault, G. Mao, and J. Wang, “Overview on deepmind and its alphago zero ai,” in Proceedings of\nthe 2018 international conference on big data and education, 2018, pp. 67–71.\n[27] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no. 3-4, pp. 279–292, 1992. [Online]. Available:\nhttps://link.springer.com/content/pdf/10.1007/BF00992698.pdf\n[28] T. L. Thorpe, “Vehicle trafﬁc light control using sarsa,” in Online]. Available: citeseer. ist. psu. edu/thorpe97vehicle. html.\nCiteseer,\n1997. [Online]. Available: https://citeseer.ist.psu.edu/thorpe97vehicle.html\n[29] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Deterministic policy gradient algorithms,” in\nProceedings of the 31st International Conference on Machine Learning, ser. Proceedings of Machine Learning Research,\nE. P. Xing and T. Jebara, Eds., vol. 32, no. 1.\nBejing, China: PMLR, 22–24 Jun 2014, pp. 387–395. [Online]. Available:\nhttps://proceedings.mlr.press/v32/silver14.html\n[30] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine learning, vol. 8,\nno. 3, pp. 229–256, 1992.\n[31] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algorithms,” in Advances in neural information processing systems, 2000, pp.\n1008–1014. [Online]. Available: https://proceedings.neurips.cc/paper/1786-actor-critic-algorithms.pdf\n[32] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, “Deep reinforcement learning that matters,”\nin\nProceedings\nof\nthe\nAAAI\nconference\non\nartiﬁcial\nintelligence,\nvol.\n32,\nno.\n1,\n2018.\n[Online].\nAvailable:\nhttps:\n//ojs.aaai.org/index.php/AAAI/article/view/11694\n[33] L. Lei, Y. Tan, G. Dahlenburg, W. Xiang, and K. Zheng, “Dynamic energy dispatch based on deep reinforcement learning in iot-driven\nsmart isolated microgrids,” IEEE Internet of Things Journal, vol. 8, no. 10, pp. 7938–7953, 2021.\n[34] L. Lei, H. Xu, X. Xiong, K. Zheng, W. Xiang, and X. Wang, “Multiuser resource control with deep reinforcement learning in iot\nedge computing,” IEEE Internet of Things Journal, vol. 6, no. 6, pp. 10 119–10 133, 2019.\n[35] S. Ohnishi, E. Uchibe, Y. Yamaguchi, K. Nakanishi, Y. Yasui, and S. Ishii, “Constrained deep q-learning gradually approaching\nordinary q-learning,” Frontiers in neurorobotics, vol. 13, p. 103, 2019.\n[36] J. Peng and R. J. Williams, “Incremental multi-step q-learning,” in Machine Learning Proceedings 1994. Elsevier, 1994, pp. 226–232.\n[37] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015.\n[38] L. Lei, Y. Tan, K. Zheng, S. Liu, K. Zhang, and X. Shen, “Deep reinforcement learning for autonomous internet of things: Model,\napplications and challenges,” IEEE Communications Surveys Tutorials, vol. 22, no. 3, pp. 1722–1760, 2020.\n[39] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double q-learning,” in Proceedings of the AAAI\nconference on artiﬁcial intelligence, vol. 30, no. 1, 2016. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/view/10295\n[40] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience replay,” arXiv preprint arXiv:1511.05952, 2015. [Online].\nAvailable: https://arxiv.org/abs/1511.05952\n[41] S. Gu, T. P. Lillicrap, Z. Ghahramani, R. E. Turner, and S. Levine, “Q-prop: Sample-efﬁcient policy gradient with an off-policy\ncritic,” CoRR, vol. abs/1611.02247, 2016. [Online]. Available: http://arxiv.org/abs/1611.02247\n36\n[42] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning\nwith a stochastic actor,” in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of\nMachine Learning Research, J. Dy and A. Krause, Eds., vol. 80.\nPMLR, 10–15 Jul 2018, pp. 1861–1870. [Online]. Available:\nhttps://proceedings.mlr.press/v80/haarnoja18b.html\n[43] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine\nLearning Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48.\nNew York, New York, USA: PMLR, 20–22 Jun 2016, pp.\n1928–1937. [Online]. Available: https://proceedings.mlr.press/v48/mniha16.html\n[44] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep\nreinforcement learning,” arXiv preprint arXiv:1509.02971, 2015. [Online]. Available: https://arxiv.org/abs/1509.02971\n[45] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, D. TB, A. Muldal, N. Heess, and T. P. Lillicrap, “Distributed\ndistributional deterministic policy gradients,” CoRR, vol. abs/1804.08617, 2018. [Online]. Available: http://arxiv.org/abs/1804.08617\n[46] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approximation error in actor-critic methods,” in Proceedings of the\n35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds.,\nvol. 80.\nPMLR, 10–15 Jul 2018, pp. 1587–1596. [Online]. Available: https://proceedings.mlr.press/v80/fujimoto18a.html\n[47] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in Proceedings of the 32nd\nInternational Conference on Machine Learning, ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37.\nLille, France: PMLR, 07–09 Jul 2015, pp. 1889–1897. [Online]. Available: https://proceedings.mlr.press/v37/schulman15.html\n[48] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” CoRR, vol.\nabs/1707.06347, 2017. [Online]. Available: http://arxiv.org/abs/1707.06347\n[49] P. Zhu, X. Li, and P. Poupart, “On improving deep reinforcement learning for pomdps,” CoRR, vol. abs/1704.07978, 2017. [Online].\nAvailable: http://arxiv.org/abs/1704.07978\n[50] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially observable mdps,” in 2015 aaai fall symposium series, 2015.\n[Online]. Available: https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/viewPaper/11673\n[51] N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver, “Memory-based control with recurrent neural networks,” CoRR, vol.\nabs/1512.04455, 2015. [Online]. Available: http://arxiv.org/abs/1512.04455\n[52] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr, P. Kohli, and S. Whiteson, “Stabilising experience replay for deep\nmulti-agent reinforcement learning,” in Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of\nMachine Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70.\nPMLR, 06–11 Aug 2017, pp. 1146–1155. [Online]. Available:\nhttps://proceedings.mlr.press/v70/foerster17b.html\n[53] E. Van der Pol and F. A. Oliehoek, “Coordinated deep reinforcement learners for trafﬁc light control,” Proceedings of Learning,\nInference and Control of Multi-Agent Systems (at NIPS 2016), 2016. [Online]. Available: https://www.elisevanderpol.nl/papers/\nvanderpolNIPSMALIC2016.pdf\n[54] J.\nFoerster,\nG.\nFarquhar,\nT.\nAfouras,\nN.\nNardelli,\nand\nS.\nWhiteson,\n“Counterfactual\nmulti-agent\npolicy\ngradients,”\nin\nProceedings\nof\nthe\nAAAI\nConference\non\nArtiﬁcial\nIntelligence,\nvol.\n32,\nno.\n1,\n2018.\n[Online].\nAvailable:\nhttps:\n//ojs.aaai.org/index.php/AAAI/article/view/11794\n[55] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed cooperative-competitive\nenvironments,” CoRR, vol. abs/1706.02275, 2017. [Online]. Available: http://arxiv.org/abs/1706.02275\n[56] C. Nadiger, A. Kumar, and S. Abdelhak, “Federated reinforcement learning for fast personalization,” in 2019 IEEE Second International\nConference on Artiﬁcial Intelligence and Knowledge Engineering (AIKE), 2019, pp. 123–127.\n[57] B. Liu, L. Wang, M. Liu, and C. Xu, “Lifelong federated reinforcement learning: A learning architecture for navigation in cloud\nrobotic systems,” CoRR, vol. abs/1901.06455, 2019. [Online]. Available: http://arxiv.org/abs/1901.06455\n[58] J. Ren, H. Wang, T. Hou, S. Zheng, and C. Tang, “Federated learning-based computation ofﬂoading optimization in edge computing-\nsupported internet of things,” IEEE Access, vol. 7, pp. 69 194–69 201, 2019.\n[59] X. Wang, C. Wang, X. Li, V. C. M. Leung, and T. Taleb, “Federated deep reinforcement learning for internet of things with decentralized\ncooperative edge caching,” IEEE Internet of Things Journal, vol. 7, no. 10, pp. 9441–9455, 2020.\n[60] J. Chen, R. Monga, S. Bengio, and R. J´ozefowicz, “Revisiting distributed synchronous SGD,” CoRR, vol. abs/1604.00981, 2016.\n[Online]. Available: http://arxiv.org/abs/1604.00981\n[61] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine\nLearning Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48.\nNew York, New York, USA: PMLR, 20–22 Jun 2016, pp.\n1928–1937. [Online]. Available: https://proceedings.mlr.press/v48/mniha16.html\n[62] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and\nK. Kavukcuoglu, “IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures,” in Proceedings of\nthe 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds.,\nvol. 80.\nPMLR, 10–15 Jul 2018, pp. 1407–1416. [Online]. Available: http://proceedings.mlr.press/v80/espeholt18a.html\n[63] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van Hasselt, and D. Silver, “Distributed prioritized experience\nreplay,” CoRR, vol. abs/1803.00933, 2018. [Online]. Available: http://arxiv.org/abs/1803.00933\n[64] T. Liu, B. Tian, Y. Ai, L. Li, D. Cao, and F.-Y. Wang, “Parallel reinforcement learning: a framework and case study,” IEEE/CAA\nJournal of Automatica Sinica, vol. 5, no. 4, pp. 827–835, 2018.\n[65] H. H. Zhuo, W. Feng, Q. Xu, Q. Yang, and Y. Lin, “Federated reinforcement learning,” CoRR, vol. abs/1901.08277, 2019. [Online].\nAvailable: http://arxiv.org/abs/1901.08277\n[66] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino, M. Re, and S. Span`o, “Multi-agent reinforcement\nlearning: A review of challenges and applications,” Applied Sciences, vol. 11, no. 11, p. 4948, 2021. [Online]. Available:\nhttps://doi.org/10.3390/app11114948\n37\n[67] L. Busoniu, R. Babuska, and B. De Schutter, “A comprehensive survey of multiagent reinforcement learning,” IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, no. 2, pp. 156–172, 2008.\n[68] K. Zhang, Z. Yang, and T. Bas¸ar, “Multi-agent reinforcement learning: A selective overview of theories and algorithms,” Handbook\nof Reinforcement Learning and Control, pp. 321–384, 2021.\n[69] P. Stone and M. Veloso, “Multiagent systems: A survey from a machine learning perspective,” Autonomous Robots, vol. 8, no. 3, pp.\n345–383, 2000.\n[70] C. Szepesv´ari and M. L. Littman, “A uniﬁed analysis of value-function-based reinforcement-learning algorithms,” Neural computation,\nvol. 11, no. 8, pp. 2017–2060, 1999.\n[71] M. L. Littman, “Value-function reinforcement learning in markov games,” Cognitive systems research, vol. 2, no. 1, pp. 55–66, 2001.\n[72] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooperative agents,” in Proceedings of the tenth international conference\non machine learning, 1993, pp. 330–337.\n[73] M. Lauer and M. Riedmiller, “An algorithm for distributed reinforcement learning in cooperative multi-agent systems,”\nin In Proceedings of the Seventeenth International Conference on Machine Learning.\nCiteseer, 2000. [Online]. Available:\nhttp://citeseerx.ist.psu.edu/viewdoc/summary\n[74] G. E. Monahan, “State of the art—a survey of partially observable markov decision processes: theory, models, and algorithms,”\nManagement science, vol. 28, no. 1, pp. 1–16, 1982.\n[75] A. Oroojlooyjadid and D. Hajinezhad, “A review of cooperative multi-agent deep reinforcement learning,” CoRR, vol. abs/1908.03963,\n2019. [Online]. Available: http://arxiv.org/abs/1908.03963\n[76] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The complexity of decentralized control of markov decision processes,”\nMathematics of operations research, vol. 27, no. 4, pp. 819–840, 2002.\n[77] S. Omidshaﬁei, J. Pazis, C. Amato, J. P. How, and J. Vian, “Deep decentralized multi-task multi-agent reinforcement learning under\npartial observability,” in Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine\nLearning Research, D. Precup and Y. W. Teh, Eds., vol. 70.\nPMLR, 06–11 Aug 2017, pp. 2681–2690. [Online]. Available:\nhttps://proceedings.mlr.press/v70/omidshaﬁei17a.html\n[78] Y. Han and P. Gmytrasiewicz, “Ipomdp-net: A deep neural network for partially observable multi-agent planning using interactive\npomdps,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, no. 01, 2019, pp. 6062–6069.\n[79] P. Karkus, D. Hsu, and W. S. Lee, “Qmdp-net: Deep learning for planning under partial observability,” 2017. [Online]. Available:\nhttps://arxiv.org/abs/1703.06692\n[80] W. Mao, K. Zhang, E. Miehling, and T. Bas¸ar, “Information state embedding in partially observable cooperative multi-agent\nreinforcement learning,” in 2020 59th IEEE Conference on Decision and Control (CDC), 2020, pp. 6124–6131.\n[81] H. Mao, Z. Zhang, Z. Xiao, and Z. Gong, “Modelling the dynamic joint policy of teammates with attention multi-agent DDPG,”\nCoRR, vol. abs/1811.07029, 2018. [Online]. Available: http://arxiv.org/abs/1811.07029\n[82] H.-R. Lee and T. Lee, “Multi-agent reinforcement learning algorithm to solve a partially-observable multi-agent problem in disaster\nresponse,” European Journal of Operational Research, vol. 291, no. 1, pp. 296–308, 2021.\n[83] S. Sukhbaatar, a. szlam, and R. Fergus, “Learning multiagent communication with backpropagation,” in Advances in Neural\nInformation Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29.\nCurran Associates,\nInc., 2016. [Online]. Available: https://proceedings.neurips.cc/paper/2016/ﬁle/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf\n[84] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, “Learning to communicate with deep multi-agent reinforcement\nlearning,” CoRR, vol. abs/1605.06676, 2016. [Online]. Available: http://arxiv.org/abs/1605.06676\n[85] L. Bus¸oniu, R. Babuˇska, and B. De Schutter, “Multi-agent reinforcement learning: An overview,” Innovations in multi-agent systems\nand applications-1, pp. 183–221, 2010.\n[86] Y. Hu, Y. Hua, W. Liu, and J. Zhu, “Reward shaping based federated reinforcement learning,” IEEE Access, vol. 9, pp. 67 259–67 267,\n2021.\n[87] A. Anwar and A. Raychowdhury, “Multi-task federated reinforcement learning with adversaries,” CoRR, vol. abs/2103.06473, 2021.\n[Online]. Available: https://arxiv.org/abs/2103.06473\n[88] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, “In-edge ai: Intelligentizing mobile edge computing, caching and\ncommunication by federated learning,” IEEE Network, vol. 33, no. 5, pp. 156–165, 2019.\n[89] X. Wang, R. Li, C. Wang, X. Li, T. Taleb, and V. C. M. Leung, “Attention-weighted federated deep reinforcement learning for\ndevice-to-device assisted heterogeneous collaborative edge caching,” IEEE Journal on Selected Areas in Communications, vol. 39,\nno. 1, pp. 154–169, 2021.\n[90] M. Zhang, Y. Jiang, F.-C. Zheng, M. Bennis, and X. You, “Cooperative edge caching via federated deep reinforcement learning in\nfog-rans,” in 2021 IEEE International Conference on Communications Workshops (ICC Workshops), 2021, pp. 1–6.\n[91] F. Majidi, M. R. Khayyambashi, and B. Barekatain, “Hfdrl: An intelligent dynamic cooperate cashing method based on hierarchical\nfederated deep reinforcement learning in edge-enabled iot,” IEEE Internet of Things Journal, pp. 1–1, 2021.\n[92] L. Zhao, Y. Ran, H. Wang, J. Wang, and J. Luo, “Towards cooperative caching for vehicular networks with multi-level federated\nreinforcement learning,” in ICC 2021 - IEEE International Conference on Communications, 2021, pp. 1–6.\n[93] Z. Zhu, S. Wan, P. Fan, and K. B. Letaief, “Federated multi-agent actor-critic learning for age sensitive mobile edge computing,”\nIEEE Internet of Things Journal, pp. 1–1, 2021.\n[94] S. Yu, X. Chen, Z. Zhou, X. Gong, and D. Wu, “When Deep Reinforcement Learning Meets Federated Learning: Intelligent\nMulti-Timescale Resource Management for Multi-access Edge Computing in 5G Ultra Dense Network,” arXiv:2009.10601 [cs], Sep.\n2020, arXiv: 2009.10601. [Online]. Available: http://arxiv.org/abs/2009.10601\n[95] Z. Tianqing, W. Zhou, D. Ye, Z. Cheng, and J. Li, “Resource allocation in iot edge computing via concurrent federated reinforcement\nlearning,” IEEE Internet of Things Journal, pp. 1–1, 2021.\n38\n[96] H. Huang, C. Zeng, Y. Zhao, G. Min, Y. Zhu, W. Miao, and J. Hu, “Scalable orchestration of service function chains in nfv-enabled\nnetworks: A federated reinforcement learning approach,” IEEE Journal on Selected Areas in Communications, vol. 39, no. 8, pp.\n2558–2571, 2021.\n[97] Y.-J. Liu, G. Feng, Y. Sun, S. Qin, and Y.-C. Liang, “Device association for ran slicing based on hybrid federated deep reinforcement\nlearning,” IEEE Transactions on Vehicular Technology, vol. 69, no. 12, pp. 15 731–15 745, 2020.\n[98] G. Wang, C. X. Dang, and Z. Zhou, “Measure contribution of participants in federated learning,” in 2019 IEEE International Conference\non Big Data (Big Data), 2019, pp. 2597–2604.\n[99] Y. Cao, S.-Y. Lien, Y.-C. Liang, and K.-C. Chen, “Federated deep reinforcement learning for user access control in open radio access\nnetworks,” in ICC 2021 - IEEE International Conference on Communications, 2021, pp. 1–6.\n[100] L. Zhang, H. Yin, Z. Zhou, S. Roy, and Y. Sun, “Enhancing wiﬁmultiple access performance with federated deep reinforcement\nlearning,” in 2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall), 2020, pp. 1–6.\n[101] M. Xu, J. Peng, B. B. Gupta, J. Kang, Z. Xiong, Z. Li, and A. A. A. El-Latif, “Multi-agent federated reinforcement learning for\nsecure incentive mechanism in intelligent cyber-physical systems,” IEEE Internet of Things Journal, pp. 1–1, 2021.\n[102] X. Zhang, M. Peng, S. Yan, and Y. Sun, “Deep-reinforcement-learning-based mode selection and resource allocation for cellular v2x\ncommunications,” IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6380–6391, 2020.\n[103] D. Kwon, J. Jeon, S. Park, J. Kim, and S. Cho, “Multiagent ddpg-based deep learning for smart ocean federated learning iot networks,”\nIEEE Internet of Things Journal, vol. 7, no. 10, pp. 9895–9903, 2020.\n[104] X. Liang, Y. Liu, T. Chen, M. Liu, and Q. Yang, “Federated Transfer Reinforcement Learning for Autonomous Driving,”\narXiv:1910.06001 [cs], Oct. 2019, arXiv: 1910.06001. [Online]. Available: http://arxiv.org/abs/1910.06001\n[105] H.-K. Lim, J.-B. Kim, J.-S. Heo, and Y.-H. Han, “Federated Reinforcement Learning for Training Control Policies on Multiple IoT\nDevices,” Sensors, vol. 20, no. 5, p. 1359, Mar. 2020. [Online]. Available: https://www.mdpi.com/1424-8220/20/5/1359\n[106] H.-K. Lim, J.-B. Kim, I. Ullah, J.-S. Heo, and Y.-H. Han, “Federated reinforcement learning acceleration method for precise control\nof multiple devices,” IEEE Access, vol. 9, pp. 76 296–76 306, 2021.\n[107] N. I. Mowla, N. H. Tran, I. Doh, and K. Chae, “Afrl: Adaptive federated reinforcement learning for intelligent jamming defense in\nfanet,” Journal of Communications and Networks, vol. 22, no. 3, pp. 244–258, 2020.\n[108] T. G. Nguyen, T. V. Phan, D. T. Hoang, T. N. Nguyen, and C. So-In, “Federated deep reinforcement learning for trafﬁc monitoring\nin sdn-based iot networks,” IEEE Transactions on Cognitive Communications and Networking, pp. 1–1, 2021.\n[109] X. Wang, S. Garg, H. Lin, J. Hu, G. Kaddoum, M. J. Piran, and M. S. Hossain, “Towards accurate anomaly detection in industrial\ninternet-of-things using hierarchical federated learning,” IEEE Internet of Things Journal, pp. 1–1, 2021.\n[110] S. Lee and D.-H. Choi, “Federated reinforcement learning for energy management of multiple smart homes with distributed energy\nresources,” IEEE Transactions on Industrial Informatics, pp. 1–1, 2020.\n[111] H. Samet, “The quadtree and related hierarchical data structures,” ACM Comput. Surv., vol. 16, no. 2, p. 187–260, Jun. 1984.\n[Online]. Available: https://doi.org/10.1145/356924.356930\n[112] M. K. Abdel-Aziz, S. Samarakoon, C. Perfecto, and M. Bennis, “Cooperative perception in vehicular networks using multi-agent\nreinforcement learning,” in 2020 54th Asilomar Conference on Signals, Systems, and Computers, 2020, pp. 408–412.\n[113] H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing Federated Learning on Non-IID Data with Reinforcement Learning,” in IEEE\nINFOCOM 2020 - IEEE Conference on Computer Communications.\nToronto, ON, Canada: IEEE, Jul. 2020, pp. 1698–1707.\n[Online]. Available: https://ieeexplore.ieee.org/document/9155494/\n[114] P. Zhang, P. Gan, G. S. Aujla, and R. S. Batth, “Reinforcement learning for edge device selection using social attribute perception in\nindustry 4.0,” IEEE Internet of Things Journal, pp. 1–1, 2021.\n[115] Y. Zhan, P. Li, W. Leijie, and S. Guo, “L4l: Experience-driven computational resource control in federated learning,” IEEE Transactions\non Computers, pp. 1–1, 2021.\n[116] Y. Dong, P. Gan, G. S. Aujla, and P. Zhang, “Ra-rl: Reputation-aware edge device selection method based on reinforcement learning,”\nin 2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM), 2021, pp.\n348–353.\n[117] A. K. Sahu, T. Li, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith, “On the convergence of federated optimization in\nheterogeneous networks,” CoRR, vol. abs/1812.06127, 2018. [Online]. Available: http://arxiv.org/abs/1812.06127\n[118] M. Chen, H. V. Poor, W. Saad, and S. Cui, “Convergence time optimization for federated learning over wireless networks,” IEEE\nTransactions on Wireless Communications, vol. 20, no. 4, pp. 2457–2471, 2021.\n[119] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence of fedavg on non-iid data,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/1907.02189?context=stat.ML\n[120] K. A. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Koneˇcn´y, S. Mazzocchi, H. B.\nMcMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards federated learning at scale: System design,” CoRR,\nvol. abs/1902.01046, 2019. [Online]. Available: http://arxiv.org/abs/1902.01046\n[121] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,\nG. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,\n“Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015. [Online]. Available:\nhttps://doi.org/10.1038/nature14236\n[122] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep\nreinforcement learning,” 2019. [Online]. Available: https://arxiv.org/abs/1509.02971\n[123] L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A survey,” CoRR, vol. abs/2003.02133, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2003.02133\n[124] C. Fung, C. J. M. Yoon, and I. Beschastnikh, “Mitigating sybils in federated learning poisoning,” CoRR, vol. abs/1808.04866, 2018.\n[Online]. Available: http://arxiv.org/abs/1808.04866\n[125] A. Anwar and A. Raychowdhury, “Multi-task federated reinforcement learning with adversaries,” 2021.\n39\n[126] L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” CoRR, vol. abs/1906.08935, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1906.08935\n[127] T. Nishio and R. Yonetani, “Client selection for federated learning with heterogeneous resources in mobile edge,” in ICC 2019 - 2019\nIEEE International Conference on Communications (ICC), 2019, pp. 1–7.\n[128] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage, and F. Beaufays, “Applied federated learning: Improving\ngoogle keyboard query suggestions,” CoRR, vol. abs/1812.02903, 2018. [Online]. Available: http://arxiv.org/abs/1812.02903\n[129] H. Yu, Z. Liu, Y. Liu, T. Chen, M. Cong, X. Weng, D. Niyato, and Q. Yang, “A fairness-aware incentive scheme for federated\nlearning,” in Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES ’20.\nNew York, NY, USA: Association\nfor Computing Machinery, 2020, p. 393–399. [Online]. Available: https://doi.org/10.1145/3375627.3375840\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-08-26",
  "updated": "2021-10-24"
}