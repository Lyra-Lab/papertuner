{
  "id": "http://arxiv.org/abs/2108.12711v1",
  "title": "Learning to Track Objects from Unlabeled Videos",
  "authors": [
    "Jilai Zheng",
    "Chao Ma",
    "Houwen Peng",
    "Xiaokang Yang"
  ],
  "abstract": "In this paper, we propose to learn an Unsupervised Single Object Tracker\n(USOT) from scratch. We identify that three major challenges, i.e., moving\nobject discovery, rich temporal variation exploitation, and online update, are\nthe central causes of the performance bottleneck of existing unsupervised\ntrackers. To narrow the gap between unsupervised trackers and supervised\ncounterparts, we propose an effective unsupervised learning approach composed\nof three stages. First, we sample sequentially moving objects with unsupervised\noptical flow and dynamic programming, instead of random cropping. Second, we\ntrain a naive Siamese tracker from scratch using single-frame pairs. Third, we\ncontinue training the tracker with a novel cycle memory learning scheme, which\nis conducted in longer temporal spans and also enables our tracker to update\nonline. Extensive experiments show that the proposed USOT learned from\nunlabeled videos performs well over the state-of-the-art unsupervised trackers\nby large margins, and on par with recent supervised deep trackers. Code is\navailable at https://github.com/VISION-SJTU/USOT.",
  "text": "Learning to Track Objects from Unlabeled Videos\nJilai Zheng1\nChao Ma1*\nHouwen Peng2\nXiaokang Yang1\n1 MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University\n2 Microsoft Research\n{zhengjilai,chaoma,xkyang}@sjtu.edu.cn, houwen.peng@microsoft.com\nAbstract\nIn this paper, we propose to learn an Unsupervised Sin-\ngle Object Tracker (USOT) from scratch. We identify that\nthree major challenges, i.e., moving object discovery, rich\ntemporal variation exploitation, and online update, are the\ncentral causes of the performance bottleneck of existing\nunsupervised trackers. To narrow the gap between unsu-\npervised trackers and supervised counterparts, we propose\nan effective unsupervised learning approach composed of\nthree stages. First, we sample sequentially moving objects\nwith unsupervised optical ﬂow and dynamic programming,\ninstead of random cropping.\nSecond, we train a naive\nSiamese tracker from scratch using single-frame pairs.\nThird, we continue training the tracker with a novel cy-\ncle memory learning scheme, which is conducted in longer\ntemporal spans and also enables our tracker to update on-\nline. Extensive experiments show that the proposed USOT\nlearned from unlabeled videos performs well over the state-\nof-the-art unsupervised trackers by large margins, and on\npar with recent supervised deep trackers. Code is available\nat https://github.com/VISION-SJTU/USOT.\n1. Introduction\nVisual object tracking is one of the most fundamental\ncomputer vision tasks with numerous applications, such\nas autonomous driving, intelligent surveillance, robot, and\nhuman-computer interaction.\nThe past few years have\nwitnessed considerable progress in visual object tracking,\nthanks to the powerful representation of deep learning. In\nspite of the success, the state-of-the-art deep tracking algo-\nrithms are data-hungry, requiring a huge number of anno-\ntated data for supervised training. As manually labeled data\nare expensive and time-consuming, exploiting ubiquitous\nunlabeled videos for visual tracking has drawn increasing\nattention recently. Following the classic pipeline of unsu-\npervised learning, existing unsupervised trackers randomly\ncrop template regions on unlabeled videos and employ ei-\n* Corresponding author.\nFigure 1: Comparison on the VOT2017/18 benchmark with\nrecent deep trackers.\nThe proposed trackers, USOT and\nUSOT*, perform well over the state-of-the-art unsupervised\ndeep trackers, including LUDT [38], LUDT+ [38], and\nS2SiamFC [34], and on par with recent supervised trackers.\nNotation: SiamFC [2], SiamDW [47], SiamRPN [26], C-\nRPN [12], DaSiamRPN [50], ATOM [8], SiamRPN++ [25],\nDiMP [3], KYS [4].\nther self consistency [34] or cycle consistency [37] as a pre-\ntext task for learning to track. Despite the promising results,\nthere still exists a large performance gap between unsuper-\nvised and supervised trackers. In view of the great success\nof unsupervised learning on a number of other vision tasks,\nsuch as video object segmentation [23], optical ﬂow [28]\nand depth estimation [14], it is of great interest to narrow\nthe gap between unsupervised and supervised trackers.\nWe identify three critical challenges that cause the per-\nformance bottleneck of unsupervised trackers.\n1) Mov-\ning object discovery. As ground truth bounding boxes are\nnot available, existing unsupervised trackers randomly sam-\nple regions in frames as pseudo templates [37, 34]. Ran-\ndom samples are far from precisely locating objects, not to\nmention learning to distinguish between objects and back-\nground. Moreover, as random samples do not contain clear\nedges of objects, they are not suitable for bounding box re-\narXiv:2108.12711v1  [cs.CV]  28 Aug 2021\ngression learning. The lack of bounding box regression for\nscale change estimation heavily limits the performance of\nunsupervised trackers. 2) Rich temporal variation exploita-\ntion. Due to the lack of labels in the temporal span, exist-\ning unsupervised trackers struggle to learn from rich motion\nclues. For example, UDT [37] performs forward and back-\nward tracking within at most 10 frames. In such a short\nclip, the foreground objects show highly correlated appear-\nances with little variations, causing a failure to exploit rich\ntemporal variations over a long span for training. 3) Online\nupdate. Online update helps to exploit the temporal smooth-\nness and has demonstrated great success in leading super-\nvised tracking methods [35, 3, 46, 48]. While supervised\ntrackers usually collect multiple object samples in separated\nframes for learning online modules [3, 13], it is more chal-\nlenging to train online branches for unsupervised trackers,\ndue to lack of even coarse object locations in videos.\nTo address these challenges, we propose to train a robust\ntracker from unlabeled videos. First, for data preparation,\nwe develop a sequential box sampling algorithm to coarsely\ndiscover moving objects from unlabeled videos. Speciﬁ-\ncally, we use unsupervised optical ﬂow to detect moving ob-\njects and apply dynamic programming to sequentially link\ncandidate boxes.\nSecond, we naively train from scratch\nan unsupervised Siamese tracker using single-frame pairs.\nThat is, we train with each Siamese pair cropped based on\nthe sampled box in a single frame. Despite its simplicity,\nwe show that this strategy provides a great initialization for\nthe unsupervised tracker, thus beneﬁcial to future training\nin longer temporal spans. Third, we propose a cycle mem-\nory learning scheme to continue training the naive tracker.\nSpeciﬁcally, we divide the whole video into a number of\nfragments according to the detected moving object trajec-\ntory. We then conduct forward tracking from a single frame\nto several other frames in the same fragment, and store all\nintermediate tracking results in a memory queue. We then\ntrack backward to the initial frame to compute the consis-\ntency loss. Since the length of video fragments is quite\nlong (averaged 64.6 frames on VID [33]) compared with\nUDT [37] (<10 frames), our tracker can capture large mo-\ntion and appearance variations. More importantly, the pro-\nposed cycle memory scheme allows updating the memory\nqueue online for model update (see Sec. 4.3).\nWe evaluate the proposed unsupervised tracker on six\nlarge-scale benchmarks. Extensive experiments show that\nour proposed tracker performs well over the state-of-the-art\nunsupervised trackers by large margins, and on par with the\nrecent supervised trackers (see Fig. 1). The main contribu-\ntions of this work are summarized as follows:\n• We coarsely discover moving objects from unlabeled\nvideos for unsupervised learning.\n• We train a naive Siamese tracker with single-frame\npairs and gradually extend it to longer temporal spans.\n• We propose a cycle memory learning scheme, allowing\nunsupervised trackers to update online.\n2. Related Work\nSupervised Tracking. Deep learning has revolutionized\nthe ﬁeld of visual tracking by powerful representation.\nIn the past few years, template-based deep trackers with\nSiamese networks have received increasing attention due to\nthe promising results on benchmark datasets. These track-\ners regard the target object as a template and search over\na cropped window to locate the target. SiamFC [2] ﬁrst\nutilizes the same backbone network to extract deep fea-\ntures from both the template patch and the search patch,\nand computes the response map with cross-correlation.\nSince then, considerable efforts have been made to ex-\ntend Siamese trackers. SiamRPN [26] incorporates a re-\ngion proposal network (RPN) [31] into the Siamese frame-\nwork.\nDiMP [3] proposes to attach an online module\nto Siamese trackers for template update.\nOther notice-\nable improvements in the Siamese framework involve ad-\nvanced backbone network [47], correlation method [25],\nattention mechanism [45], re-detection module [36], mask\ngeneration [40], feature alignment [48], anchor-free regres-\nsor [6, 15], etc. With these efforts, Siamese trackers have\nshown the superior tracking performance thus far. However,\ntraining Siamese trackers requires a huge number of labeled\ntraining data. In this work, we aim at a novel unsupervised\nlearning scheme, which helps to learn template-based track-\ners from unlabeled videos in the wild.\nUnsupervised Tracking. The pioneer unsupervised deep\ntracker (UDT) [37] suggests that a robust tracker is able to\ntrack an object forward and backward in a video and ﬁnally\nreturn to its initial location in the starting frame. UDT de-\nvelops a tracker based on DCFNet [39], and computes a\ncycle consistency loss between forward and backward tra-\njectories in the training phase. The contemporary method of\nUDT is TimeCycle [41], which proposes cycle consistency\nto generate unsupervised video representation. JSLTC [27]\nproposes to calculate an inter-frame afﬁnity matrix to model\nthe transitions between video frames and use such corre-\nspondence to track objects. S2SiamFC [34] adopts the self-\nSiamese pipeline to train a foreground/background classi-\nﬁer like SiamFC [2] from single-frame pairs, showing com-\nparable results to the supervised counterpart. Despite the\npromising results, there exists a large performance gap be-\ntween the state-of-the-art unsupervised trackers and the top-\nperforming supervised ones. We identify three critical chal-\nlenges, moving object discovery, rich temporal variation ex-\nploitation, and online update, that cause the performance\nbottleneck of unsupervised trackers. By effectively tackling\nthese challenges, the proposed unsupervised tracker out-\nperforms the state-of-the-art unsupervised trackers by large\nmargins, and is on par with recent supervised trackers.\n3. Proposed Method\nIn this section, we present the proposed unsupervised\ntracker in detail. The unsupervised training scheme involves\nthree stages. The ﬁrst stage in Sec. 3.1 aims to produce a\ntrajectory of moving objects from unlabeled videos. The\nsecond stage in Sec. 3.2 learns a naive Siamese tracker us-\ning single-frame pairs. The third stage in Sec. 3.3 continues\ntraining the naive tracker by means of cycle memory learn-\ning, which is performed in longer temporal spans and also\nenables the unsupervised tracker to update online.\n3.1. Moving Object Discovery\nInstead of randomly cropping objects, we propose to\ngenerate a smooth bounding box sequence for moving fore-\nground objects in unlabeled videos. For discovering moving\nobjects, we have two key observations:\n• Foreground objects tend to have distinguishing motion\npatterns in contrast to the background surroundings.\nThis inspires us to discover candidate foreground ob-\njects by means of unsupervised optical ﬂow.\n• The trajectories of moving objects tend to be smooth.\nThis motivates us to employ dynamic programming\n(DP) to get temporally reliable box sequences.\nCandidate Box Generation. Let an arbitrary video be I\nincluding L successive frames with the same size W × H,\nnamely I = {It | 1 ≤t ≤L}, where It is the tth frame\nin I. To locate the potential foreground object in frame It,\nwe ﬁrst compute the optical ﬂow map Ft with frame It and\nframe It+Tf , i.e., Ft = Flowt→t+Tf . Tf is an interval for\ncomputing optical ﬂow. As is illustrated in Fig. 2, we obtain\nFt from frame It and frame It+Tf using the off-the-shelf\nunsupervised ARFlow [28] algorithm, and then transform\nFt to a distance map Dt. We binarize Dt to get a mask Mt\nas follows:\nM i\nt =\n\u001a\n1\nif Di\nt ≥α · maxj(Dj\nt) + (1 −α) · meanj(Dj\nt)\n0\no.w.\nwhere Di\nt =\n\r\r\rF i\nt −meanj(F j\nt )\n\r\r\r\n2 ,\n(1)\nand α ∈(0, 1) is a hyper-parameter. Here superscript de-\nnotes pixel-wise index.\nThe maximum value and mean\nvalue within the spatial dimension are respectively indicated\nby max and mean.\nEvery connected area with all internal pixels satisfying\nM i\nt = 1 corresponds to an area which has distinguishing\nmotion compared with background in It, and this area is\nmore likely to cover a foreground object. To further ﬁlter\nout unreliable areas from these candidates, we take the cir-\ncumscribed rectangles of all these areas as initial candidate\nboxes, and score the boxes according to their sizes and po-\nsitions. Due to center bias, larger bounding boxes in the\nFt\nDt\nMt\nIt\nBt\nIt+Tf\nFigure 2: Candidate box generation via optical ﬂow. The\nﬂow map Ft contains the distinguishing motion patterns of\nmoving objects. We binarize the ﬂow map Ft using a dis-\ntance metric Dt to generate the candidate box Bt.\nmiddle of the image should have higher quality scores. Let\nB = (x0, y0, x1, y1) denote the top-left and bottom-right\ncorners of a box. The quality score Sc of the box B is de-\nﬁned as:\nSc(B) =(x1 −x0)(y1 −y0)+\nβ · min(x0, W −x1) min(y0, H −y1),\n(2)\nwhere β is a weight parameter. The box with the highest\nscore is selected as the ﬁnal candidate box Bt for frame It.\nWe denote the set of all these selected candidate boxes in\nvideo I as B = {Bt | 1 ≤t ≤L}.\nBox Sequence Generation.\nThe generated candidate\nbounding boxes B may contain noisy boxes due to camera\nshake, occlusion, etc. To remove unreliable boxes, we ap-\nply dynamic programming to create a more reliable bound-\ning box sequence B′. According to the second observation\nthat the trajectory of a moving object in a video should be\nsmooth, we select a subset of candidate bounding boxes\nfrom B, where the trajectory of the selected boxes is as\nsmooth as possible. For dynamic programming, the most\ncritical issue is how to measure the reward of transition in\nthe box trajectory from one bounding box to another. We\nmodify the DIoU [49] metric, which originally considers\nthe overlap and distance between two boxes. Formally, the\nreward Rdp for dynamic programming is deﬁned as:\nRdp(Bt, Bt′) = IoU(Bt, Bt′) −γ · RDIoU(Bt, Bt′), (3)\nwhere γ is a hyper-parameter.\nTo encourage a smooth\ntrajectory, we set γ\n>\n1 for the distance penalty on\nRDIoU [49]. Note that dynamic programming aims at dis-\ncovering an optimal path in the box sequence B with the\nhighest reward accumulation (see the supplementary docu-\nment for the complete algorithm). As shown in Fig. 3, for\nthe frames whose candidate boxes are not selected by DP,\nwe use linear interpolation to generate pseudo boxes based\non their adjacent candidate boxes selected by DP.\nTrajectory via dynamic programming\nLinear interpolation\nCandidate boxes\nIt\nIt+3\nIt+6\nIt+9\nFigure 3: Box sequence generation. We use dynamic programming to generate a smooth and reliable box trajectory from\ncandidate boxes in yellow. Pseudo boxes in green in the remaining frames are generated through linear interpolation.\n3.2. Naive Siamese Tracker\nWith the generated box sequences, we train a naive\nSiamese tracker using single-frame pairs from scratch. This\npretext task is based on a simple observation that an image\nand any of its sub-region naturally form a training pair of the\nSiamese network [34]. However, randomly sampled pseudo\nboxes as in [34] fail to cover foreground objects for effec-\ntively training Siamese networks. Moreover, random sam-\nples are not suitable for learning bounding box regression.\nThis signiﬁcantly hinders the performance of unsupervised\ntrackers. We propose to utilize the reliable box sequence\nB′ as training data. To ensure that the most precise bound-\ning boxes in B′ are sampled by the data loader, we adopt a\ntwo-level scoring mechanism to ﬁlter out low-quality boxes\nat both the sequence and frame levels. We ﬁnd that denser\nframe selection by DP in video I tends to imply a more\nsuccessful moving object discovery. As such, we deﬁne the\nquality score Qv of video I = {It | 1 ≤t ≤L} as:\nQv(I) = Ndp\nL ,\n(4)\nwhere Ndp indicates the number of frames in video I se-\nlected by DP.\nSimilarly, the frame quality score evaluating the box B′\nt\nin frame It can be measured by the percentage of frames\nselected by DP within all its adjacent frames. Let Ts be a\nﬁxed frame interval. We formally deﬁne the frame quality\nscore Qf as:\nQf(B′\nt) =\nN ′\ndp\n2Ts + 1,\n(5)\nwhere N ′\ndp indicates the number of frames between frame\nIt−Ts and frame It+Ts selected by DP.\nWhen loading training pairs, we sequentially conduct\nvideo sampling and frame sampling. We sample a video\nonly if its quality score Qv(I) ≥θ1, where θ1 is a thresh-\nold. During frame sampling, we randomly sample several\nframes with their total number positively correlated with\n1/Qv(I) from the selected video, and then select the frame\nwith the highest frame quality score Qf(B′\nt) for training.\nWe follow the conventional training paradigm as in\nSiamFC [2]. The input template zt and the search area xt\nare respectively of size 127 × 127 and 255 × 255, both\ncropped from It based on B′\nt. After extracting deep fea-\ntures from the input pair, we adopt PrPool [19] to pool the\ntemplate feature, and then compute the multi-scale corre-\nlation map [48]. The output response map Rcls is of size\n25 × 25 × 1 for foreground/background classiﬁcation. The\nother output response map Rreg is of size 25 × 25 × 4 for\nregressing the distances from the center location to the four\nsides of the bounding box. The loss function Lnaive is the\nsum of both the regression and classiﬁcation losses:\nLnaive = Lreg + λ1Lcls,\n(6)\nwhere Lreg and Lcls are respectively the IoU loss [44] and\nthe binary cross-entropy (BCE) loss [10]. λ1 is a weight\nparameter.\n3.3. Cycle Memory Training\nWe view the above unsupervised Siamese tracker as a\nnaive tracker as it incurs two limitations. First, as the tem-\nplate and search area are cropped in the same frame, the\ntracker is not learned with large motion and appearance\nvariations. Second, this tracker cannot update itself online,\nthus fails to track objects in long temporal spans or under\ncomplex scenes.\nWe propose to continue training the naive tracker us-\ning a cycle memory learning scheme, aiming to enable the\ntracker to handle large variations as well as update the mem-\nory queue online. The main idea of cycle memory can be\nsummarized as in Fig. 4. In brief, we ﬁrst conduct for-\nward tracking from a template zt to Nmem adjacent mem-\nory frames, then store features of all intermediate tracking\nresults as a memory queue, and ﬁnally conduct backward\ntracking to the original search area xt. A cycle memory loss\nLmem is computed using the same ground truth as Lcls.\nSpeciﬁcally, at every training step, we simultaneously\ncrop a training pair zt and xt in frame It (the same\nas training the naive Siamese tracker), as well as Nmem\nmemory search areas sampled from {xt | Tl ≤t ≤Tu}.\nThese memory search areas are cropped from Nmem\nadjacent frames of It according to the box sequence\n{B′\nt | Tl ≤t ≤Tu}. Here Tl and Tu are the lower and up-\nper frame indices for sampling memory frames.\nSelect-\ning these two indices is quite important.\nTo learn from\nlong-term variations, the frame interval between Tl and Tu\nForward \ntrack\nMemory \nqueue\nBackward \ntrack\nSelf \ntrack\nTemplate\nSearch area\nPooling features\nMemory\nsearch\nareas\n*\n*\n*\nconv\n3 × 3\nconv\n3 × 3\nw\nNaive Siamese Tracker\nCycle Memory Learning \nTraining Pipeline\nSame \nframe\nconv\nconv\nconv\nCcorr\nCconf\nCval\nMemory  \n queue\nSearch\narea\nmem\ncls\nreg\nFigure 4: Overview of the proposed unsupervised tracking framework. Left: The overall training pipeline. Right: The\ndetailed structure of the naive Siamese tracker for self tracking and forward tracking, and the online module learned with the\ncycle memory scheme. The naive tracker is trained with a template and a search area cropped from the same frame, while\nthe online module aims to track backward from the memory search areas to the template frame following the cycle learning\npipeline. The circle notations with ∗denote multi-scale correlation [48] for deep features, where the same color indicates\nweight sharing. The circle with W refers to the conﬁdence-value module for integrating the correlation maps (Eqn. 8).\nshould be large enough. However, excessive frame interval\ndoes harm to the learning process as the target object may\ndisappear in frames far from It. In practice, we dynami-\ncally set Tl and Tu at frame It. Since they are two mirror\nvariables, we formally deﬁne Tu as follows:\nTu(It) = max\nt≤k≤L {k}\ns.t.\n∀t < t′ ≤k, Rdp(B′\nt′−1, B′\nt′) ≥θ2\n∀t < t′ ≤k, Qf(B′\nt′) ≥θ3,\n(7)\nwhere θ2 and θ3 are two thresholds. The main idea of Eqn. 7\nis that, as long as a box B′\nk can be connected to B′\nt through a\nsmooth and reliable box sequence in B′, search area cropped\nfrom B′\nk can be used for cycle memory training. In other\nwords, we take step changes in B′ to divide I into frag-\nments, and the pseudo boxes of all frames in the same frag-\nment tend to locate the same object. This scheme helps our\ntracker exploit long-term variations, while still ensuring the\nreliability of pseudo bounding boxes in the memory frames\n(see Sec. 4.3 for quantitative analysis).\nLet Nmem denote the number of memory frames. We\nﬁrst utilize the tracker to predict Nmem intermediate bound-\ning boxes in the memory frames for the template zt. We\nadopt PrPool [19] to pool Nmem features based on the in-\ntermediate boxes. Then we use the pooled features as tem-\nplates to conduct multi-scale correlation [48] with the deep\nfeature of xt. Note that the original classiﬁcation branch\nand the memory branch share the same weights in terms\nof the multi-scale correlation module. All Nmem correla-\ntion maps, denoted as {Cu\ncorr | 1 ≤u ≤Nmem}, are inte-\ngrated together by a conﬁdence-value strategy. Speciﬁcally,\ngiven a correlation map Cu\ncorr, we utilize two 3 × 3 con-\nvolution layers to generate a conﬁdence map Cu\nconf and a\nvalue map Cu\nval with the same dimension. We then nor-\nmalize Cu\nconf element-wise across all conﬁdence maps as\nweights on Cu\nval. The ﬁnally integrated correlation map C\nis formulated as follows:\nC =\nX\n1≤u≤Nmem\nsoftmax (Cu\nconf) ⊙Cu\nval,\n(8)\nwhere ⊙denotes Hadamard product. As shown in Fig. 4,\nthe integrated map C is converted to 25 × 25 × 1 via con-\nvolution, yielding the response map Rmem of the object in\nsearch area xt. The total loss function L for training is:\nL = Lreg + λ1Lcls + λ2Lmem,\n(9)\nwhere λ1 and λ2 are weight parameters. We use the BCE\nloss [10] as the cycle memory loss Lmem, which shares the\nsame pseudo ground truth label as in Lcls.\nTable 1: Results on the VOT benchmarks. The proposed trackers perform well over the state-of-the-art unsupervised trackers.\nBoldface denotes the best performance among all trackers built without video labels for ofﬂine training (the same below).\nTracker\nUnsup.\nVOT2016\nVOT2017/18\nVOT2020\nA ↑\nR ↓\nEAO ↑\nA ↑\nR ↓\nEAO ↑\nA ↑\nR ↑\nEAO ↑\nSiamFC [2]\nNo\n0.532\n0.461\n0.235\n0.503\n0.585\n0.188\n0.418\n0.502\n0.179\nDaSiamRPN [50]\nNo\n0.61\n0.22\n0.411\n0.56\n0.34\n0.326\n-\n-\n-\nATOM [8]\nNo\n-\n-\n-\n0.590\n0.204\n0.401\n0.462\n0.734\n0.271\nDiMP [3]\nNo\n-\n-\n-\n0.597\n0.153\n0.440\n0.457\n0.740\n0.274\nIVT [32]\nYes\n0.419\n1.109\n0.115\n0.400\n1.639\n0.076\n0.345\n0.244\n0.092\nMIL [1]\nYes\n0.407\n0.727\n0.165\n0.394\n1.011\n0.118\n0.367\n0.322\n0.113\nKCF [17]\nYes\n0.489\n0.569\n0.192\n0.447\n0.773\n0.135\n0.407\n0.432\n0.154\nECO [7]\nYes\n0.55\n0.20\n0.375\n0.48\n0.27\n0.280\n-\n-\n-\nS2SiamFC [34]\nYes\n0.493\n0.639\n0.215\n0.463\n0.782\n0.180\n-\n-\n-\nLUDT [38]\nYes\n0.544\n0.422\n0.232\n0.463\n0.693\n0.154\n-\n-\n-\nLUDT+ [38]\nYes\n0.570\n0.331\n0.299\n0.490\n0.412\n0.230\n-\n-\n-\nUSOT (Ours)\nYes\n0.593\n0.336\n0.351\n0.564\n0.435\n0.290\n0.458\n0.600\n0.222\nUSOT* (Ours)\nYes\n0.600\n0.233\n0.402\n0.578\n0.304\n0.344\n0.448\n0.600\n0.219\n4. Experiments\nThis section presents the results of our unsupervised\ntracker on multiple benchmarks, with comparisons to the\nstate-of-the-art tracking algorithms.\nExtensive ablation\nstudies are provided to analyze the effectiveness of our de-\nsign choices.\n4.1. Implementation Details\nTraining. Our tracker is trained on the data collected from\nthe training sets of four datasets including GOT-10k [18],\nImageNet VID [33], LaSOT [11] and YouTube-VOS [43].\nNote that the ground-truth labels of these training sets are\nnot available in our method.\nThe hyper-parameters for\nextracting the video box sequences are set as α = 0.3,\nβ = 0.5, γ = 4.1 respectively. Our network adopts ResNet-\n50 [16] as the backbone network, and uses the third convo-\nlutional block to extract deep features for input images. We\nnotice that existing CNN backbones pretrained on the Ima-\ngeNet dataset [33] contain information from manual labels.\nFor the sake of solid comparisons, we conduct all the ex-\nperiments in two settings (i.e., w/o and w/ supervised back-\nbone pretraining on ImageNet). During training, we use\nsynchronized SGD [24] on 4 NVIDIA GeForce RTX 3090\nGPUs. Each GPU hosts 12 groups of training instances.\nThe whole end-to-end training phase takes 30 epochs in to-\ntal, in which cycle memory is conducted only within the\nlast 25 epochs. We start with a warm-up learning rate from\n2.5 × 10−3 to 5 × 10−3 in the ﬁrst 6 epochs, while the re-\nmaining epochs adopt an exponentially decreasing learning\nrate from 5 × 10−3 down to 2 × 10−5.\nAt each training step, we sample a template bounding\nbox from B′ with θ1 = 0.4 and crop a template patch and\na search area as SiamFC [2]. This image pair is augmented\nwith horizontal and vertical ﬂips. For training the online\nmodule with cycle memory, we input extra Nmem = 4\nmemory search areas together with the template-search pair.\nOther hyper-parameters for cycle memory are set to γ =\n2.5, θ2 = 0.45 and θ3 = 0.40. The trade-off parameter\nin the loss function is ﬁxed to λ1 = 0.2 for naive Siamese\ntraining, and for cycle memory we keep λ1 + λ2 = 0.9 and\ngradually decrease λ1 from 0.3, in order to make the tracker\ngently suit to the tracking task in longer temporal spans.\nInference. The inference is performed on both the ofﬂine\nand online branches. The ofﬂine branch follows the conven-\ntional inference methodology of Siamese networks [25, 48].\nBased on the template feature from frame I1, the ofﬂine re-\nsponse maps Rcls and bounding boxes Rreg in subsequent\nframes are generated by the ofﬂine classiﬁer and regressor.\nOn the other hand, the online branch maintains a memory\nqueue of templates and dynamically updates this queue with\nnew predictions. In practice, when tracking the object in\nframe It, the memory queue consists of totally Nq tem-\nplates, including two ground-truth templates from frame I1\n(i.e., the original template and its horizontal ﬂip), the latest\npredicted template from frame It−1, and Nq −3 histori-\ncal templates with the highest scores in R from frame I2 to\nIt−2. The ﬁnal response map R is a weighted sum of Rcls\nand Rmem, namely R = (1 −w)Rcls + wRmem, where\nthe weight w is set to 0.7 throughout our experiments.\n4.2. State-of-the-art Comparison\nWe compare our method with the state-of-the-art un-\nsupervised and supervised trackers. The comparisons are\nconducted on six benchmarks, including VOT2016 [20],\nVOT2017/18 [22],\nVOT2020 [21],\nTrackingNet [29],\nOTB2015 [42] and LaSOT [11]. We denote by USOT the\nTable 2: Results on the TrackingNet [29] dataset. The pro-\nposed unsupervised trackers, USOT and USOT*, perform\nwell over the state-of-the-arts.\nTrackers\nUnsup.\nSucc. ↑\nPrec. ↑\nNorm P. ↑\nSiamFC [2]\nNo\n57.1\n53.3\n66.3\nMDNet [30]\nNo\n61.4\n55.5\n71.0\nUpdateNet [46]\nNo\n67.7\n62.5\n75.2\nATOM [8]\nNo\n70.3\n64.8\n77.1\nSiamRPN++ [25]\nNo\n73.3\n69.4\n80.0\nKCF [17]\nYes\n44.7\n41.9\n54.6\nDSST [9]\nYes\n46.4\n46.0\n58.8\nECO [7]\nYes\n56.1\n48.9\n62.1\nLUDT [38]\nYes\n54.3\n46.9\n59.3\nLUDT+ [38]\nYes\n56.3\n49.5\n63.3\nUSOT (Ours)\nYes\n59.9\n55.1\n68.2\nUSOT* (Ours)\nYes\n61.5\n56.6\n69.1\nproposed tracker trained with the unsupervised backbone\ninitialization [5], while denoting by USOT* that with su-\npervised ImageNet pretraining. It is worth mentioning that\nwe keep all the hyper-parameters ﬁxed throughout the ex-\nperiments to report results on all the datasets.\nVOT2016.\nThe VOT2016 dataset contains 60 video se-\nquences. We adopt the Accuracy (A), Robustness (R) and\nExpected Average Overlap (EAO) [20] as the VOT-toolkit\nto evaluate the overall performance. Tab. 1 shows that the\nproposed trackers, both USOT and USOT*, signiﬁcantly\noutperform the state-of-the-art unsupervised trackers, with\nrespectively 5.2 and 10.3 points increase in EAO over the\ntop-performing unsupervised tracker LUDT+.\nVOT2017/18. The VOT2017/18 dataset consists of 60 more\nchallenging video sequences. Tab. 1 shows that our USOT*\nobtains 8.8 and 11.4 points increase respectively in Accu-\nracy and EAO compared with LUDT+. Our USOT without\nsupervised backbone still outperforms LUDT+ by respec-\ntively 7.4 and 6.0 points in Accuracy and EAO.\nVOT2020.\nThe VOT2020 dataset encodes targets with\nsegmentation masks, and updates the calculation of Accu-\nracy (A), Robustness (R) and Expected Average Overlap\n(EAO) [21]. Tab. 1 shows that our USOT* outperforms\nSiamFC by 3.0, 9.8 and 4.0 points respectively in A, R and\nEAO. Our USOT even achieves better performance com-\npared with USOT* with an EAO of 0.222.\nTrackingNet.\nThe TrackingNet dataset contains over\n30000 videos, with 511 videos for testing. Tab. 2 shows that\nour USOT* outperforms LUDT+ with 5.2 points in Success\nand 7.1 points in Precision. Our USOT is comparable with\nUSOT* on the TrackingNet dataset. This suggests that un-\nsupervised representation learning has the potential to per-\nform as well as supervised ImageNet pretraining.\nOTB2015.\nThe OTB2015 dataset contains 100 videos.\nTab. 3 shows that our USOT outperforms USOT*. This\nFigure 5: Success plot and Precision plot on the LaSOT\ntesting set [11].\nafﬁrms the potential of unsupervised representation learn-\ning from scratch. Furthermore, the proposed unsupervised\ntrackers achieve comparable performance with LUDT and\nSiamFC. Our trackers perform slightly worse than LUDT+,\nbecause LUDT+ adopts some online tracking techniques in\n[7] over LUDT for better performance.\nLaSOT. The LaSOT testing set consists of 280 videos with\nan average length over 2000. LaSOT is important for mea-\nsuring long-term tracking performance. Fig. 5 shows that\nour USOT* signiﬁcantly outperforms LUDT+ with an in-\ncrease of 5.3 and 5.2 on Success and Precision, respectively.\nOur USOT achieves comparable results to the supervised\ntrackers SiamFC [2] and SiamDW [47].\n4.3. Ablation Studies\nTraining Stage Indispensability. We do extensive ablation\nstudies on the importance of different stages in the train-\ning phase. Experiments are conducted on the VOT2017/18\nbenchmark with USOT*.\nTab. 4 shows that training a\nnaive tracker from single-frame pairs with random cropping\ncauses a signiﬁcant accuracy drop compared with our pro-\nposed box sequence generation. In addition, directly con-\nducting cycle memory training without the naive Siamese\ntracker initialization also causes a large performance drop.\nVideo Utilization Rate. We use the video quality score\nQv(I) in Eqn. 4 to ﬁlter out noisy sequences during unsu-\npervised training. On the GOT-10k dataset and the VID\ndataset, we utilize 50.8% and 56.3% videos respectively\nwithin all videos available for training, covering rich va-\nrieties of unlabeled videos.\nFrame Interval. Our proposed training method can learn\nthe appearance information across long intervals. This fa-\ncilitates unsupervised trackers to adapt to temporal appear-\nance changes. Compared to the very short frame intervals\nin previous deep unsupervised trackers S2SiamFC [34] (i.e.,\n0 frame) and UDT [37] (i.e., < 10 frames), the training in-\nstances sampled by our method possess an averaged long\nframe interval of 41.1 and 64.6 respectively on the GOT-\n10k and VID datasets.\nTable 3: Results on the OTB2015 [42] dataset. Our unsupervised trackers achieve comparable results with the previous\nsupervised and unsupervised trackers.\nTracker\nDCFNet\nSiamFC\nSiamRPN\nATOM\nDiMP\nDSST\nKCF\nLUDT\nLUDT+\nUSOT\nUSOT*\n[39]\n[2]\n[26]\n[8]\n[3]\n[9]\n[17]\n[38]\n[38]\n(Ours)\n(Ours)\nAUC success\n58.0\n58.2\n63.7\n66.7\n68.6\n51.8\n48.5\n60.2\n63.9\n58.9\n57.4\nDistance precision\n-\n77.1\n85.1\n87.9\n89.9\n68.9\n69.6\n76.9\n84.3\n80.6\n77.5\nTable 4: Ablation studies on our pseudo box generation\nmodule and naive Siamese tracker initialization before cy-\ncle memory training on the VOT2017/18 dataset.\nOperations\nA ↑\nR ↓\nEAO ↑\nRandom boxes\n0.488\n0.646\n0.195\nOur gnereated boxes\n0.567\n0.520\n0.263\nw/o naive Siamese learning\n0.575\n0.389\n0.306\nw/ naive Siamese learning\n0.578\n0.304\n0.344\nTable 5: Quantitative results on the IoU success rates of the\npseudo boxes in template frames and memory frames.\nDataset \\ IoU\nTemplate\nMemory\n0.3\n0.5\n0.3\n0.5\nGOT-10k\n63.2%\n45.5%\n62.0%\n43.8%\nVID\n64.4%\n42.1%\n63.9%\n42.0%\nPseudo Bounding Box Generation. To better investigate\nthe precision of the pseudo bounding boxes, we collect over\n104 training instances and compute the IoU scores between\nthe output pseudo bounding boxes and the ground truth\nbounding boxes on both the GOT-10k and VID datasets.\nTab. 5 shows the success rates of the pseudo bounding boxes\nover different IoU scores in both template frames and mem-\nory frames. On both datasets, over 63% sampled boxes in\ntemplate frames cover at least parts of the foreground ob-\njects (IoU > 0.3), while over 42% sampled boxes in tem-\nplate frames are precise enough to cover approximately the\nentire objects (IoU > 0.5). Besides, from the small dif-\nference between the IoU success rates of pseudo boxes in\ntemplate frames and memory frames on both datasets, we\nconclude that using large frame intervals for cycle memory\ntraining only slightly decreases the reliability of memory\nframes compared to template frames. This explains why\nour unsupervised tracker can learn from large motions.\nTraining Dataset. Since most existing unsupervised deep\ntrackers are trained on the VID dataset, we investigate the\nimpact of training data on USOT* on the VOT2017/18\nbenchmark. As is shown in Tab. 6, when only using VID\nas the training set, the proposed tracker still achieves 0.315\nin EAO, with an 8.5 points increase over the state-of-the-art\nunsupervised tracker LUDT+ (i.e., 0.230 in EAO). Besides,\nour tracker beneﬁts from training on more unlabeled videos,\ninferring the great potential of unsupervised tracking.\nTable 6: Ablation studies on training data.\nWith more\nunlabeled videos used for training, the proposed USOT*\nachieves better results on the VOT2017/18 dataset.\nTraining Data\nA ↑\nR ↓\nEAO ↑\nVID\nGOT-10k\nLaSOT\nYT-VOS\n\"\n0.576\n0.337\n0.315\n\"\n\"\n0.587\n0.323\n0.320\n\"\n\"\n\"\n0.579\n0.328\n0.337\n\"\n\"\n\"\n\"\n0.578\n0.304\n0.344\nTable 7: Parameter sensitivity of the length and weight of\nthe online memory queue on the VOT2017/18 dataset.\nNq \\ w\n0.3\n0.5\n0.6\n0.7\n0.8\n5\n0.289\n0.302\n0.323\n0.313\n0.323\n6\n0.294\n0.312\n0.312\n0.329\n0.322\n7\n0.310\n0.318\n0.336\n0.344\n0.331\n8\n0.302\n0.300\n0.319\n0.341\n0.338\nOnline Update. We study the parameter sensitivity of Nq\nand w in the online memory module. Nq indicates the num-\nber of memorized features collected online in the memory\nqueue, while w indicates the weight for Rmem.\nTab. 7\nreports the EAO scores of USOT* on the VOT2017/18\ndataset. The cooperation of ofﬂine and online modules with\nw = 0.7 beneﬁts the proposed tracker most, and setting the\nlength of the memory queue Nq to 7 is most suitable.\n5. Concluding Remarks\nIn this paper, we propose learning a robust tracker from\nunlabeled videos from scratch. We ﬁrst generate candidate\nbox sequences to cover moving objects in videos. We then\ntrain a naive Siamese tracker using single-frame pairs. We\nﬁnally continue training the naive tracker in longer tem-\nporal spans with a novel cycle memory scheme, enabling\nthe tracker to update online. Extensive experiments demon-\nstrate that the proposed unsupervised tracker sets new state-\nof-the-art unsupervised tracking results, and even performs\non par with recent supervised deep trackers. This work un-\nveils the power of unsupervised learning for object tracking.\nAcknowledgements. This work was supported by NSFC\n(61906119,\nU19B2035),\nShanghai Municipal Science\nand Technology Major Project (2021SHZDZX0102), and\nShanghai Pujiang Program.\nReferences\n[1] Boris Babenko, Ming-Hsuan Yang, and Serge J. Belongie.\nRobust object tracking with online multiple instance learn-\ning. TPAMI, 2011.\n[2] Luca Bertinetto, Jack Valmadre, Jo˜ao F. Henriques, Andrea\nVedaldi, and Philip H. S. Torr. Fully-convolutional siamese\nnetworks for object tracking. In ECCV Workshop, 2016.\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Learning discriminative model prediction for track-\ning. In ICCV, 2019.\n[4] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Know your surroundings: Exploiting scene infor-\nmation for object tracking. In ECCV, 2020.\n[5] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\nHe. Improved baselines with momentum contrastive learn-\ning. arXiv:2003.04297, 2020.\n[6] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang,\nand Rongrong Ji. Siamese box adaptive network for visual\ntracking. In CVPR, 2020.\n[7] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. ECO: efﬁcient convolution operators for\ntracking. In CVPR, 2017.\n[8] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. ATOM: accurate tracking by overlap max-\nimization. In CVPR, 2019.\n[9] Martin Danelljan, Gustav H¨ager, Fahad Shahbaz Khan, and\nMichael Felsberg. Accurate scale estimation for robust vi-\nsual tracking. In BMVC, 2014.\n[10] Pieter-Tjerk de Boer, Dirk P. Kroese, Shie Mannor, and\nReuven Y. Rubinstein.\nA tutorial on the cross-entropy\nmethod. Ann. Oper. Res., 2005.\n[11] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLasot: A high-quality benchmark for large-scale single ob-\nject tracking. In CVPR, 2019.\n[12] Heng Fan and Haibin Ling. Siamese cascaded region pro-\nposal networks for real-time visual tracking. In CVPR, 2019.\n[13] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang.\nStmtrack:\nTemplate-free visual tracking with space-time\nmemory networks. In CVPR, 2021.\n[14] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-\ntow.\nUnsupervised monocular depth estimation with left-\nright consistency. In CVPR, 2017.\n[15] Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and\nShengyong Chen.\nSiamcar: Siamese fully convolutional\nclassiﬁcation and regression for visual tracking. In CVPR,\n2020.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016.\n[17] Jo˜ao F. Henriques, Rui Caseiro, Pedro Martins, and Jorge\nBatista. High-speed tracking with kernelized correlation ﬁl-\nters. TPAMI, 2015.\n[18] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A\nlarge high-diversity benchmark for generic object tracking in\nthe wild. TPAMI, 2019.\n[19] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yun-\ning Jiang. Acquisition of localization conﬁdence for accurate\nobject detection. In ECCV, 2018.\n[20] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg,\nRoman P. Pﬂugfelder, Luka Cehovin, Tom´as Voj´ır, Gustav\nH¨ager, Alan Lukezic, Gustavo Fern´andez, et al. The visual\nobject tracking VOT2016 challenge results. In ECCV Work-\nshop, 2016.\n[21] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-\nberg, Roman P. Pﬂugfelder, Joni-Kristian K¨am¨ar¨ainen, Mar-\ntin Danelljan, Luka Cehovin Zajc, Alan Lukezic, Ondrej Dr-\nbohlav, Linbo He, et al. The eighth visual object tracking\nVOT2020 challenge results. In ECCV Workshop, 2020.\n[22] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-\nberg, Roman P. Pﬂugfelder, Luka Cehovin Zajc, Tom´as\nVoj´ır, Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey,\nGustavo Fern´andez, ´Alvaro Garc´ıa-Mart´ın, ´Alvaro Iglesias-\nArias, et al. The sixth visual object tracking VOT2018 chal-\nlenge results. In ECCV Workshop, 2018.\n[23] Zihang Lai, Erika Lu, and Weidi Xie. MAST: A memory-\naugmented self-supervised tracker. In CVPR, 2020.\n[24] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie\nHenderson, Richard E. Howard, Wayne E. Hubbard, and\nLawrence D. Jackel. Backpropagation applied to handwrit-\nten zip code recognition. Neural Comput., 1989.\n[25] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\nand Junjie Yan.\nSiamrpn++: Evolution of siamese visual\ntracking with very deep networks. In CVPR, 2019.\n[26] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In CVPR, 2018.\n[27] Xueting Li, Sifei Liu, Shalini De Mello, Xiaolong Wang,\nJan Kautz, and Ming-Hsuan Yang. Joint-task self-supervised\nlearning for temporal correspondence. In NeurIPS, 2019.\n[28] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao\nWang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and\nFeiyue Huang. Learning by analogy: Reliable supervision\nfrom transformations for unsupervised optical ﬂow estima-\ntion. In CVPR, 2020.\n[29] Matthias M¨uller, Adel Bibi, Silvio Giancola, Salman Al-\nSubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nECCV, 2018.\n[30] Hyeonseob Nam and Bohyung Han. Learning multi-domain\nconvolutional neural networks for visual tracking. In CVPR,\n2016.\n[31] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015.\n[32] David A. Ross, Jongwoo Lim, Ruei-Sung Lin, and Ming-\nHsuan Yang. Incremental learning for robust visual tracking.\nIJCV, 2008.\n[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Fei-Fei Li. Imagenet large scale visual recognition chal-\nlenge. IJCV, 2015.\n[34] Chon-Hou Sio, Yu-Jen Ma, Hong-Han Shuai, Jun-Cheng\nChen, and Wen-Huang Cheng. S2siamfc: Self-supervised\nfully convolutional siamese network for visual tracking. In\nACM MM, 2020.\n[35] Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao\nBao, Wangmeng Zuo, Chunhua Shen, Rynson W. H. Lau,\nand Ming-Hsuan Yang. VITAL: visual tracking via adver-\nsarial learning. In CVPR, 2018.\n[36] Paul Voigtlaender, Jonathon Luiten, Philip H. S. Torr, and\nBastian Leibe. Siam R-CNN: visual tracking by re-detection.\nIn CVPR, 2020.\n[37] Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei\nLiu, and Houqiang Li.\nUnsupervised deep tracking.\nIn\nCVPR, 2019.\n[38] Ning Wang, Wengang Zhou, Yibing Song, Chao Ma, Wei\nLiu, and Houqiang Li.\nUnsupervised deep representation\nlearning for real-time tracking. IJCV, 2021.\n[39] Qiang Wang, Jin Gao, Junliang Xing, Mengdan Zhang, and\nWeiming Hu. Dcfnet: Discriminant correlation ﬁlters net-\nwork for visual tracking. arXiv:1704.04057, 2017.\n[40] Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and\nPhilip H. S. Torr. Fast online object tracking and segmenta-\ntion: A unifying approach. In CVPR, 2019.\n[41] Xiaolong Wang, Allan Jabri, and Alexei A. Efros. Learn-\ning correspondence from the cycle-consistency of time. In\nCVPR, 2019.\n[42] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object track-\ning benchmark. TPAMI, 2015.\n[43] Ning Xu,\nLinjie Yang,\nYuchen Fan,\nJianchao Yang,\nDingcheng Yue, Yuchen Liang, Brian L. Price, Scott Cohen,\nand Thomas S. Huang. Youtube-vos: Sequence-to-sequence\nvideo object segmentation. In ECCV, 2018.\n[44] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and\nThomas S. Huang. Unitbox: An advanced object detection\nnetwork. In ACM MM, 2016.\n[45] Yuechen Yu, Yilei Xiong, Weilin Huang, and Matthew R.\nScott. Deformable siamese attention networks for visual ob-\nject tracking. In CVPR, 2020.\n[46] Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer,\nMartin Danelljan, and Fahad Shahbaz Khan. Learning the\nmodel update for siamese trackers. In ICCV, 2019.\n[47] Zhipeng Zhang and Houwen Peng.\nDeeper and wider\nsiamese networks for real-time visual tracking. In CVPR,\n2019.\n[48] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and\nWeiming Hu. Ocean: Object-aware anchor-free tracking. In\nECCV, 2020.\n[49] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\nYe, and Dongwei Ren. Distance-iou loss: Faster and better\nlearning for bounding box regression. In AAAI, 2020.\n[50] Zheng Zhu, Qiang Wang, Bo Li, Wei Wu, Junjie Yan, and\nWeiming Hu. Distractor-aware siamese networks for visual\nobject tracking. In ECCV, 2018.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-08-28",
  "updated": "2021-08-28"
}