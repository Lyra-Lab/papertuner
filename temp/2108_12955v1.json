{
  "id": "http://arxiv.org/abs/2108.12955v1",
  "title": "Unsupervised Learning of Deep Features for Music Segmentation",
  "authors": [
    "Matthew C. McCallum"
  ],
  "abstract": "Music segmentation refers to the dual problem of identifying boundaries\nbetween, and labeling, distinct music segments, e.g., the chorus, verse, bridge\netc. in popular music. The performance of a range of music segmentation\nalgorithms has been shown to be dependent on the audio features chosen to\nrepresent the audio. Some approaches have proposed learning feature\ntransformations from music segment annotation data, although, such data is time\nconsuming or expensive to create and as such these approaches are likely\nlimited by the size of their datasets. While annotated music segmentation data\nis a scarce resource, the amount of available music audio is much greater. In\nthe neighboring field of semantic audio unsupervised deep learning has shown\npromise in improving the performance of solutions to the query-by-example and\nsound classification tasks. In this work, unsupervised training of deep feature\nembeddings using convolutional neural networks (CNNs) is explored for music\nsegmentation. The proposed techniques exploit only the time proximity of audio\nfeatures that is implicit in any audio timeline. Employing these embeddings in\na classic music segmentation algorithm is shown not only to significantly\nimprove the performance of this algorithm, but obtain state of the art\nperformance in unsupervised music segmentation.",
  "text": "UNSUPERVISED LEARNING OF DEEP FEATURES FOR MUSIC SEGMENTATION\nMatthew C. McCallum\nGracenote Inc.\nABSTRACT\nMusic segmentation refers to the dual problem of identifying bound-\naries between, and labeling, distinct music segments, e.g., the cho-\nrus, verse, bridge etc.\nin popular music.\nThe performance of a\nrange of music segmentation algorithms has been shown to be de-\npendent on the audio features chosen to represent the audio. Some\napproaches have proposed learning feature transformations from\nmusic segment annotation data, although, such data is time con-\nsuming or expensive to create and as such these approaches are\nlikely limited by the size of their datasets. While annotated mu-\nsic segmentation data is a scarce resource, the amount of available\nmusic audio is much greater. In the neighboring ﬁeld of semantic\naudio unsupervised deep learning has shown promise in improving\nthe performance of solutions to the query-by-example and sound\nclassiﬁcation tasks.\nIn this work, unsupervised training of deep\nfeature embeddings using convolutional neural networks (CNNs) is\nexplored for music segmentation. The proposed techniques exploit\nonly the time proximity of audio features that is implicit in any\naudio timeline.\nEmploying these embeddings in a classic music\nsegmentation algorithm is shown not only to signiﬁcantly improve\nthe performance of this algorithm, but obtain state of the art perfor-\nmance in unsupervised music segmentation.\nIndex Terms— Music information retrieval, Acoustic signal\nprocessing, Convolutional neural network, Deep learning\n1. INTRODUCTION\nMusic segmentation refers to the task of labeling distinct segments\nof music in a way that is similar to a human annotation. For exam-\nple the chorus, verse, intro, outro and bridge in popular music.. The\nboundary between such segments may be due to a number of factors,\nfor example, a change in melody or chord progression, a change in\nrhythm, changes in instrumentation, dynamics, key or tempo. This\ntask is generally evaluated with two classes of metrics. The ﬁrst\nclass, boundary detection, refers to the ability of the algorithm to\nlocate the locations of such boundaries in time. The second class,\nsegment labelling, refers to the labelling of segments where two seg-\nments that are disconnected in time are labelled as same or different\nbased on their perceptual similarity [1–3].\n1.1. Prior Work\nA number of techniques exist in the literature that address either the\nboundary detection [4–6], segment labelling [7] problems, or both\nsimultaneously [8–11]. Methods addressing solely the former met-\nric typically involve the generation of a novelty function via a self\nsimilarity matrix (SSM) representation. Other approaches address-\ning both of the aforementioned metrics focus on clustering audio\nfeatures based on characteristics that are expected to remain homo-\ngeneous within a given musical segment. Such clustering has been\nperformed on the basis of timbral and harmonic features in the con-\ntext of spectral clustering [9], or in the context of time-translation\ninvariant features such as HMM state histograms [8]. In addition, a\nnumber of feature transformations have been considered with respect\nto clustering approaches, including non-negative matrix factoriza-\ntions (NMFs) of audio features [11] or self-similarity matrices [12],\nas well as learned features from labelled segmentation data [5].\nIn the context of supervised deep learning some work has ad-\ndressed the problem of music segmentation [13–15], where perfor-\nmance is likey bound by the limited annotated music segmentation\ndata available. The largest effort to collect labeled music segmenta-\ntion data is in the SALAMI dataset [16], providing 2246 annotations\nof music structure in 1360 audio ﬁles. This amount of data is small\nin the context of deep learning, but promising results were obtained.\nRecently, in the neighboring ﬁeld of semantic audio, unsuper-\nvised approaches towards embedding audio features have attracted\nattention in the literature [17–19]. In particular, the beneﬁts of learn-\ning audio feature embeddings in an unsupervised manner have be-\ncome apparent in [17]. Because there is no requirement for labeled\ndata in training such embeddings, they can be trained on much larger\ndatasets and in turn, this has achieved impressive results when such\npre-trained embeddings are employed in sound classiﬁcation and\nquery by example tasks [17]. Despite the promising results obtained\nfor tasks related to the identiﬁcation of events in audio, little to no\ninvestigation has been made into the use of such unsupervised audio\nfeature embeddings for music content speciﬁcally.\n1.2. Contributions\nThis work focuses on the unsupervised training of CNNs to obtain\nmeaningful features for music segmentation methods. This is a natu-\nral progression of the application of modern machine learning meth-\nods to the problem of music segmentation for three reasons.\nFirstly, previous literature pays careful attention to obtaining\nfeatures that are representative of musical characteristics that are\nperceptually important in identifying segment boundaries such as\ntimbre or harmonic repetition [4–6, 9, 10, 20–24]. Whether or not\na given feature is important in identifying a segment boundary or la-\nbel may be genre or song speciﬁc and so a data based approach may\nbe promising in either producing a representation that better general-\nizes across genres or in the least may be arbitrarily learned for each\ngenre speciﬁcally. Several data based approaches have been inves-\ntigated for the music segmentation problem [5,10,13], with little to\nno work in the context of unsupervised deep learning.\nSecondly, musical content has a structure that may be exploited\nto further improve the machine learning methodologies employed in\nworks such as [17–19]. The characteristics that deﬁne music such as\nrhythm and harmonicity have not been exploited in these previous\nworks. warranting some investigation in the context of music.\nFinally, labeled data for the problem of music segmentation is\nnotoriously time consuming and/or expensive to produce [25], as\narXiv:2108.12955v1  [cs.SD]  30 Aug 2021\nsuch an unsupervised machine learning approach that can exploit\nlarge amounts of unlabeled music data is highly desirable. The ap-\nproach in this paper investigates deep learning in an unsupervised\nframework, where only the time locality and a comparative analy-\nsis of time local data is exploited. Such an approach overcomes the\nnecessity to hand annotate data, and hence may be scaled to the full\nextent of the available music data. No longer limited by the size\nof the training dataset, it is expected to generalize better than hand\ncrafted features that rely on aspects such as timbre or repetition that\nmay be speciﬁc to certain music genres.\n2. AUDIO FEATURE EMBEDDING\nThe approach of here is to create an embedding for audio features by\ntransforming an audio representation via a CNN into a domain that\nis representative of musical structure. These embeddings may be\ntrained by employing a loss function, such as contrastive loss [26],\nor triplet loss [27], that observes positive (similar) or negative (dis-\nsimilar) pairs or triplets of examples, where a triplet represents an\nanchor and both a positive example and a negative example. The\ntriplet loss function is often shown to result in superior performance\nto contrastive loss, which has been argued to be due to the its relative\nnature [28]. That is, the triplet loss function is designed to provide\na positive gradient with respect to increasing distance between posi-\ntive pairs or the anchor and positive example of a triplet, relative to\nthe distance between negative pairs, up to a given margin. As such,\nit is employed in this research.\nIf a transformation from an input feature, x[q, k] (or equivalently\nx for notational simplicity) to an embedding space, e.g., via a CNN,\nis described as f : RK×Q →RD, then a Euclidean triplet loss with\na given margin, α, and a mini-batch of training data consisting of the\nset T =\n\b\nxc\na, xc\np, xc\nn\n\tc=C−1\nc=0\nmay be described as,\nL(T ) =\nC−1\nX\nc=0\nh\r\rf (xc\na) −f\n\u0000xc\np\n\u0001\r\r2\n2 −∥f (xc\na) −f (xc\nn)∥2\n2 + α\ni\n+ ,\n(1)\nwhere subscript a, p and n represent an anchor, positive and negative\ninput example respectively.\nIdeally, in the context of music segmentation positive examples\nwill be composed of audio feature representations from the same\nmusic segment as the anchor, while negative examples will represent\naudio features from distinct music segments. Thus, once trained,\ninput features could be transformed via that CNN into a space where\ndistinct clusters with respect to a Euclidean distance metric would\nrepresent distinct music segments. In an unsupervised context with\nno prior information about the music segmentation, exact selection\nof such examples is not possible, however strategies are described in\nSection 3 that may improve this selection.\nThe properties that are signiﬁcant in forming clusters represent-\ning music segments in the embedded space are learned from the data\nat the CNN input. These embedded features may be representative\nof any of the musical qualities mentioned in Section 1, provided\nthat these qualities are observable from the input features. Here,\na constant-Q transform (CQT) [29] is used due to its ability to accu-\nrately represent transient and harmonic audio qualities, its translation\ninvariant representation of harmonic structures, and the promising\nresults observed for this feature speciﬁcally in [30].\nA time window of CQT data providing K frequency bins across\nQ time windows forms a time frequency representation that may\ncompose any of xc\na, xc\np or xc\nn. In this work it is found signiﬁcantly\nadvantageous to synchronize the CQT analysis windows with the\nbeat of the music. That is, if a beat at index i occurs at time bi, then\nR CQT windows are analyzed centered at times bi +r(bi+1 −bi)/R\nfor integers, r ∈{0..R −1}. These beat synchronized CQT repre-\nsentations are then aggregated across B beats providing Q = BR\ntime indices in the representation x[q, k] at the input to the CNN.\n3. SAMPLING\nMotivated by the unreasonable effectiveness of data in deep learn-\ning [31], methods are proposed here that create noisy positive / neg-\native examples exploiting the facts that a) musical segments form\ncontiguous regions in a song’s timeline, and b) each distinct musical\nsegment label typically occurs for the minority of a song’s timeline.\nSpeciﬁcally, it is proposed to use the time proximity information\nimplicit in a song’s features - sampling features that occur close\ntogether or at a minimum distance apart for positive and negative ex-\namples respectively. That is, an anchor beat index, ia, is selected via\na uniform sampling of the beat indices in a given song, {0..L −1}.\nThereafter, a positive beat index, ip, is chosen uniformly sampled\nfrom beat indices {max(ia −δp, 0).. min(ia + δp, L −1)}, and\na negative beat index in is chosen as uniformly sampled across\ntwo regions, {max(ia −δn,max, 0).. max(ia −δn,min, 0)}, and\n{min(ia + δn,min, L −1).. min(ia + δn,max, L −1)}. An exam-\nple of the positive and negative sampling distributions is shown in\nFig. 1. Intuitively, this results in an embedding in which clusters\nrepresent features that frequently occur close together. This is useful\nfor the problem of the structural segmentation of music as segment\nboundaries are typically infrequent enough to be described as rare\nevents, at least in comparison to lengths of the segments themselves.\nIt is interesting to consider the rates of false positives and nega-\ntives that result from the aforementioned sampling paradigm. Upon\nselection of ia it may be denoted to fall somewhere in the nth musi-\ncal segment of class s and length ls,n. If δp < lsep, where lsep is the\nminimum number of indices between s and any identically labeled\nsegment, then the probability of the positive example being selected\nfrom a distinct segment, i.e., a false positive, is,\nP(FP|ls,n; δp) =\n\n\n\n\n\n\n\n2δp−ls,n\nls,n\nls,n ≤δp\nδ2\np\n2l2s,n −\n3δp\n4ls,n + 1\n2\nδp < ls,n < 2δp\nδp\n4ls,n\n2δp ≤ls,n\n.\nSimilarly, if δn,min < lsep and δn,max ≥L, then the probabillity\nof a false negative is simply\nP\nm ls,m\nL\n−(1 −P(FP|ls,n; δn,min)).\nIn practice the aforementioned assumptions are realistic in many\nscenarios, but do not hold under all conditions. An empirical analy-\nsis of the rate of false positives is shown in Fig. 2. It is clear that with\nan increasing δp, an increasing rate of false positives is observed, al-\nthough it is important to note that smaller δp restricts the maximum\nobserved time separation between the anchor and positive example.\nWith musical phrases often lasting 16 beats, it is reasonable to set\nδp ≥16 to discourage distinct clustering of features within a sin-\ngle phrase. An empirical analysis false negatives is shown in Fig. 3.\nThere it can be seen that for the datasets shown δn,min > 28 and\nδn,max > 116 result in relatively low false negative rates.\nWhile structural annotations are not available under the scope\nof unsupervised learning, it is interesting to ask whether analysis of\nsignal features may be employed to decrease the false-negative or\nfalse-positive rate. It was shown in [7] that 2D Fourier magnitude\ncoefﬁcients of HPCPs can be a useful feature in segment labeling.\nIt is proposed here to use a similar feature to inform the sampling\nof positive and negative examples. That is, for every selected ia,\nδn,max\nδn,min\nia\nP(in)\nP(ip)\nin\nL\nδp\n0\n0\nip\nFig. 1. Sampling distributions for in and ip, for a given choice of ia.\nδp\n%FP\nFig. 2. Observed false positive (FP) rates in the BeatlesTUT (⋄), and\nSALAMI (□), datasets for varying δp. Dashed lines show results\nwith unbiased sampling, and solid lines employ the comparitive 2D\nFFT technique described here.\na comparison of the log-amplitude of the 2D Fourier transform of\nthe log-amplitude of 8 beat long CQT segments is considered. The\nEuclidean distances between these CQT segments centered at two\ntimes before ia, i.e., ia −4 and ia −16, and two times after ia,\ni.e., ia + 4 and ia + 16 are considered. The side (“before” where\ni < ia or “after” where i > ia) with minimum Euclidean distance\nis then assumed to be more likely in the same musical segment as ia\nand is chosen from which to sample ip while the opposite side may\nbe chosen from which to sample in, in addition to the constraints,\nδp, δn,min and δn,max above. The changes in false negative and\nfalse positive rates employing this sampling paradigm can be seen in\nFigures 2 and 3, respectively, where some improvement is observed.\n4. BOUNDARY DETECTION\nTo evaluate the effectiveness of the music embeddings described in\nthis work, the problem of music segment boundary detection is con-\nsidered. Perhaps the simplest and most well known method for mu-\nsic boundary detection is that of Foote [4]. While this has been sur-\npassed in performance by several methods, e.g., [6,9], its simplicity\nmakes it effective in demonstrating the utility of the audio embed-\ndings proposed here, as will be seen in the results in Section 5.\nThe SSM of the proposed features is computed as,\nS[i, j] = ∥f (xi[q, k]) −f (xj[q, k])∥2\n2\n(2)\nwhere xi[q, k] and xj[q, k] correspond to beat synchronous CQT\nsegments centered at beats i and j, respectively.\nNote that this work endeavors to create features that are close\nwith respect to Euclidean distance at any point within a music seg-\nment. If successful, the SSM of embedded features typically con-\ntains block structures as opposed to the path structures typically rep-\nresentative of repetition. These structures are evident in Fig. 4. It was\nfound beneﬁcial in practice to perform median ﬁltering on S[i, j] to\nproduce ¯S[i, j] which reduces noise in the distances between em-\nbedded features while maintaining the aforementioned structures.\nTo detect segment boundaries in ¯S[i, j], a checkerboard kernel,\nδn,max\n%FN\nΔ%FN\nδn,max\nδn,min\nδn,min\nSALAMI\nBeatlesTUT\na)\nb)\nFig. 3. Observed false negative (FN) rates for various δn,min and\nδn,max. Row (a) shows FN raters resulting from unbiased sampling,\nwhile row (b) represents the change in FN rates when employing the\ncomparitive 2D FFT technique described in this paper.\ng[i, j] = sgn(i)sgn(j)e\n−(i−j)2\nσ\n(3)\nfor −κ ≤i, j ≤κ is convolved along the diagonal of the SSM,\nη[ν] =\nκ\nX\ni=−κ\nκ\nX\nj=−κ\n¯S[ν + i, ν + j]g[ν + i, ν + j]\n(4)\nproducing the novelty function η[ν]. Note that it was found advanta-\ngeous to set g[i, j] to 0 where |i| ≤1 or |j| ≤1. Finally, boundaries\nare detected as peaks in the novelty function. Speciﬁcally, peaks at\nwhich the peak-to-mean ratio exceeds a given threshold, τ, are se-\nlected as segment boundaries, i.e., where,\n(2T + 1)η[ν]\nPT\nt=−T η[ν + t]\n> τ.\n(5)\n5. RESULTS\nFor evaluation, two datasets are considered: the BeatlesTUT dataset\nconsisting of 174 hand annotated tracks from The Beatles cata-\nlogue [20], and the internet archive portion of the SALAMI dataset\n(SALAMI-IA) consisting of 375 hand annotated recordings [16].\nThe former dataset is perhaps the most widely evaluated in the\nmusic segmentation literature. The latter is employed here for two\nreasons, ﬁrstly the complete SALAMI dataset audio is not avail-\nable to the author due to copyright restrictions, and secondly, the\nSALAMI-IA dataset is particularly interesting as it consists pri-\nmarily of live recordings with many imperfections. It provides a\ndataset that is indicative of segmentation performance when there\nare mistakes either by musicians or recording engineers, resulting in\nimperfect repetitions and distorted or noisy audio in many cases.\nFor comparison, two baseline algorithms are included in the re-\nsults as speciﬁed and measured in [30], note that these algorithms\ntoo included beat-synchronized features. Firstly, the method of [4]\nis included as it most closely mirrors the algorithm of Section 4.\nSecondly, the algorithm of [6] is widely evaluated as having the best\nBeats\n0\n100\n200\n300\n400\nFig. 4. Example of median ﬁltered SSM and novelty function for the\nsong ”Starlight” by Muse. On both, detected boundaries are marked\nwith yellow lines.\nConvolutional\n72  x  512  x  64\nMax Pooling 2x4\nMax Pooling 3x4\nConvolutional\n36  x  128  x  128\nMax Pooling 2x4\nConvolutional\n12  x  32  x  256\nDense Layer 128\nDense Layer 128\nLinear Layer 128\nL2 Normalization\nFig. 5. Architecture of the CNN employed in experiments. For each\nconvolutional layer, dimensions represent time, frequency and chan-\nnel respectively. Each convolutional and dense layer employ a ReLU\nactivation. All convolutional layers employ 6 × 4 kernels.\nperformance with respect to unsupervised boundary detection in mu-\nsic segmentation. CQT features have been evaluated to provide su-\nperior performance in [30], and are used at the input to all algo-\nrithms, proposed and benchmark, in the results presented here. For\nthe proposed algorithms, three methods are investigated, ”Unsyn-\nchronized”, employing a constant hop size of 3.6 ms between suc-\ncessive CQT windows; ”Beat-Synchronized”, employing 128 CQT\nwindows centered at times linearly interpolated between successive\nbeat markers (estimated during training and inference by an algo-\nrithm similar to [32]); and ”Biased”, employing the same features as\nthe ”Beat-Synchronized” approach, but using the 2D Fourier Trans-\nform comparison sampling described in Section 3. The CQT features\nuse a minimum frequency of 40 Hz, 12 bins per octave and 6 octaves.\nThe structure of CNNs employed in this paper is chosen to be\nsimple enough so that it may be trained on most modern high perfor-\nmance GPUs. The architecture described in Fig. 5 was found to be\neffective and is employed in the experiments here. During training,\nmini-batches of size C = 96 consisting of 16 triplets from each of 6\nrandomly selected tracks were formed in real-time by randomly se-\nlecting from 28,345 songs, excluding all songs in the SALAMI-IA\nand BeatlesTUT dataset. 256 min-batches form 1 epoch, and train-\ning took place over 240 epochs taking approximately 8 hours. The\ntriplet margin was set to α = 0.1. Despite the observed error rates\nin Figures 3 and 3, [27,28] argue that it is the difﬁculty of separation\nbetween examples that is important, and so not all false positives\nand negatives are equal. In practice it was found that δp = 16,\nδn,min = 1 and δn,max = 96 provide optimal results.\nFor boundary detection, embedded features were observed at ev-\nTable 1. Performance metrics for the BeatlesTUT dataset\nAlgorithm\nF-Measure\nPrecision\nRecall\n[4]\n0.503 ± 0.18\n0.579 ± 0.21\n0.461 ± 0.17\n[6]\n0.651 ± 0.17\n0.622 ± 0.19\n0.708 ± 0.19\nUnsynchronized\n0.597 ± 0.17\n0.589 ± 0.19\n0.625 ± 0.17\nBeat-Synchronized\n0.648 ± 0.17\n0.647 ± 0.20\n0.677 ± 0.18\nBiased Sampling\n0.662 ± 0.17\n0.663 ± 0.20\n0.691 ± 0.19\nTable 2. Performance metrics for the SALAMI-A dataset\nAlgorithm\nF-Measure\nPrecision\nRecall\n[4]\n0.446 ± 0.17\n0.457 ± 0.21\n0.483 ± 0.19\n[6]\n0.493 ± 0.17\n0.454 ± 0.20\n0.595 ± 0.19\nUnsynchronized\n0.497 ± 0.16\n0.429 ± 0.18\n0.653 ± 0.15\nBeat-Synchronized\n0.535 ± 0.15\n0.491 ± 0.20\n0.660 ± 0.16\nBiased Sampling\n0.533 ± 0.16\n0.491 ± 0.21\n0.656 ± 0.16\nery beat for beat-synchronized approaches, or once every 0.2484 sec-\nonds for ”Unsynchronized” (this is approximately twice per beat at a\ntypical 120 BPM to ensure this method is not disadvantaged by any\nshortcoming in time resolution). SSM representations were median\nﬁltered using an 8x8 window. The checkerboard kernel was conﬁg-\nured with σ = 18.5 and κ = 40 and for peak picking, the crest\nfactor window size T = 10 and threshold τ = 1.35 was employed.\nFor evaluation, the trimmed F-measure, precision and recall of\nthe boundary detection hit rate at the 3 second tolerance level are\nemployed [1].\nThe evaluation of the proposed and reference al-\ngorithms for the BeatlesTUT and SALAMI-IA dataset are shown\nin Table 1 and Table 2, respectively. It should be noted here that\nfor the proposed algorithms, any selection of parameters was per-\nformed by observing results on the BeatlesTUT dataset only, and so\nthe SALAMI-IA dataset displays the boundary detection algorithm’s\nability to generalize to unseen data.\nIt is interesting to see that simply by employing the proposed\ndeep features in an algorithm similar to that of Foote [4], such a\nmethod becomes competitive with the state of the art in unsupervised\nmusic segmentation. Furthermore, on the SALAMI-IA dataset, a\nsigniﬁcant performance improvement over the state of the art is ob-\nserved without any additional parameter adjustment.\nThis result\nmight be postulated to be due to the poor quality of audio / music\ndata in this portion of the SALAMI dataset. Because the algorithm\nof [6] is designed to detect changes in repetition patterns, when these\npatterns become imperfect, or corrupted by noise, a performance\ndrop might be expected. In the proposed embedding, clustering of\nfeatures is performed simply based on the time proximity of features\nobserved from the training data, which contains many of the afore-\nmentioned imperfections providing some robustness.\n6. CONCLUSION\nIn this work, methods for the unsupervised training of music em-\nbeddings were investigated with respect to their utility in the task\nof music segmentation. In particular, it was shown that by employ-\ning such embeddings in a traditional music segmentation algorithm,\nthe performance of this algorithm can obtain state of the art perfor-\nmance. It was found that a common musical feature, rhythm, may\nbe exploited in beat-synchronized sampling (and in the 2D Fourier\nTransform comparitive sampling of Section 3) to further improve\nperformance.\n7. REFERENCES\n[1] Colin Raffel, Brian McFee, Eric J Humphrey, Justin Salamon,\nOriol Nieto, Dawen Liang, Daniel PW Ellis, and C Colin Raf-\nfel, “mir eval: A transparent implementation of common MIR\nmetrics,” in ISMIR, 2014, pp. 367–372.\n[2] Oriol Nieto, Morwaread M Farbood, Tristan Jehan, and\nJuan Pablo Bello, “Perceptual analysis of the F-measure for\nevaluating section boundaries in music,” in ISMIR, 2014, pp.\n265–270.\n[3] Hanna M Lukashevich,\n“Towards quantitative measures of\nevaluating song segmentation,” in ISMIR, 2008, pp. 375–380.\n[4] Jonathan Foote, “Automatic audio segmentation using a mea-\nsure of audio novelty,” in Multimedia and Expo (ICME). IEEE\nInternational Conference on, 2000, pp. 452–455.\n[5] Brian McFee and Daniel PW Ellis, “Learning to segment songs\nwith ordinal linear discriminant analysis,” in Acoustics, Speech\nand Signal Processing (ICASSP), IEEE International Confer-\nence on, 2014, pp. 5197–5201.\n[6] Joan Serra, Meinard M¨uller, Peter Grosche, and Josep Ll Ar-\ncos, “Unsupervised music structure annotation by time series\nstructure features and segment similarity,” IEEE Transactions\non Multimedia, vol. 16, no. 5, pp. 1229–1240, 2014.\n[7] Oriol Nieto and Juan Pablo Bello, “Music segment similarity\nusing 2D-Fourier magnitude coefﬁcients,” in Acoustics, Speech\nand Signal Processing (ICASSP), IEEE International Confer-\nence on, 2014, pp. 664–668.\n[8] Mark Levy and Mark Sandler, “Structural segmentation of mu-\nsical audio by constrained clustering,” IEEE transactions on\naudio, speech, and language processing, vol. 16, no. 2, pp.\n318–326, 2008.\n[9] Brian McFee and Dan Ellis, “Analyzing song structure with\nspectral clustering,” in ISMIR, 2014, pp. 405–410.\n[10] Ron J Weiss and Juan Pablo Bello, “Unsupervised discovery of\ntemporal structure in music,” IEEE Journal of Selected Topics\nin Signal Processing, vol. 5, no. 6, pp. 1240–1251, 2011.\n[11] Oriol Nieto and Tristan Jehan, “Convex non-negative matrix\nfactorization for automatic music structure identiﬁcation,” in\nAcoustics, Speech and Signal Processing (ICASSP), IEEE In-\nternational Conference on, 2013, pp. 236–240.\n[12] Florian Kaiser and Thomas Sikora, “Music structure discovery\nin popular music using non-negative matrix factorization,” in\nISMIR, 2010, pp. 429–434.\n[13] Karen Ullrich, Jan Schl¨uter, and Thomas Grill, “Boundary de-\ntection in music structure analysis using convolutional neural\nnetworks,” in ISMIR, 2014, pp. 417–422.\n[14] Jan Schl¨uter and Sebastian B¨ock,\n“Improved musical onset\ndetection with convolutional neural networks,” in Acoustics,\nSpeech and Signal Processing (ICASSP), IEEE International\nConference on, 2014, pp. 6979–6983.\n[15] Thomas Grill and Jan Schluter,\n“Music boundary detection\nusing neural networks on spectrograms and self-similarity lag\nmatrices,” in EUSIPCO, 2015, pp. 1296–1300.\n[16] Jordan Bennett Louis Smith, John Ashley Burgoyne, Ichiro Fu-\njinaga, David De Roure, and J Stephen Downie, “Design and\ncreation of a large-scale database of structural annotations,” in\nISMIR, 2011, pp. 555–560.\n[17] Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel PW Ellis,\nShawn Hershey, Jiayang Liu, R Channing Moore, and Rif A\nSaurous, “Unsupervised learning of semantic audio represen-\ntations,” in Acoustics, Speech and Signal Processing (ICASSP),\nIEEE International Conference on, 2018, pp. 126–130.\n[18] Justin Salamon and Juan Pablo Bello, “Unsupervised feature\nlearning for urban sound classiﬁcation,” in Acoustics, Speech\nand Signal Processing (ICASSP), IEEE International Confer-\nence on, 2015, pp. 171–175.\n[19] Justin Salamon and Juan Pablo Bello, “Feature learning with\ndeep scattering for urban sound analysis,” in EUSIPCO, 2015,\npp. 724–728.\n[20] Jouni Paulus and Anssi Klapuri, “Music structure analysis by\nﬁnding repeated parts,” in Proceedings of the 1st ACM work-\nshop on Audio and music computing multimedia, 2006, pp. 59–\n68.\n[21] Wei Chai, “Semantic segmentation and summarization of mu-\nsic: methods based on tonality and recurrent structure,” IEEE\nSignal Processing Magazine, vol. 23, no. 2, pp. 124–132, 2006.\n[22] Matija Marolt, “A mid-level melody-based representation for\ncalculating audio similarity,” in ISMIR, 2006, pp. 280–285.\n[23] Kristoffer Jensen, “Multiple scale music segmentation using\nrhythm, timbre, and harmony,” EURASIP Journal on Applied\nSignal Processing, vol. 2007, no. 1, pp. 159–159, 2007.\n[24] Johan Pauwels, Florian Kaiser, and Geoffroy Peeters, “Com-\nbining harmony-based and novelty-based approaches for struc-\ntural segmentation,” in ISMIR, 2013, pp. 601–606.\n[25] Cheng-i Wang, Gautham J Mysore, and Shlomo Dubnov, “Re-\nvisiting the music segmentation problem with crowdsourcing,”\nin ISMIR, 2017, pp. 738–744.\n[26] Raia Hadsell, Sumit Chopra, and Yann LeCun, “Dimension-\nality reduction by learning an invariant mapping,” in In Proc.\nComputer Vision and Pattern Recognition Conference (CVPR),\n2006, pp. 1735–1742.\n[27] Florian Schroff, Dmitry Kalenichenko, and James Philbin,\n“Facenet: A uniﬁed embedding for face recognition and clus-\ntering,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2015, pp. 815–823.\n[28] Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp\nKr¨ahenb¨uhl, “Sampling matters in deep embedding learning,”\nin Proc. IEEE International Conference on Computer Vision\n(ICCV), 2017.\n[29] Christian Sch¨orkhuber and Anssi Klapuri, “Constant-Q trans-\nform toolbox for music processing,” in 7th Sound and Music\nComputing Conference, Barcelona, Spain, 2010, pp. 3–64.\n[30] Oriol Nieto and Juan Pablo Bello, “Systematic exploration of\ncomputational music structure research.,” in ISMIR, 2016, pp.\n547–553.\n[31] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav\nGupta, “Revisiting unreasonable effectiveness of data in deep\nlearning era,” in Proc. IEEE International Conference on Com-\nputer Vision (ICCV), 2017, pp. 843–852.\n[32] Anssi P Klapuri, Antti J Eronen, and Jaakko T Astola, “Anal-\nysis of the meter of acoustic musical signals,” IEEE Transac-\ntions on Audio, Speech, and Language Processing, vol. 14, no.\n1, pp. 342–355, 2006.\n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "cs.MM",
    "eess.AS"
  ],
  "published": "2021-08-30",
  "updated": "2021-08-30"
}