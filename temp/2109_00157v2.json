{
  "id": "http://arxiv.org/abs/2109.00157v2",
  "title": "A Survey of Exploration Methods in Reinforcement Learning",
  "authors": [
    "Susan Amin",
    "Maziar Gomrokchi",
    "Harsh Satija",
    "Herke van Hoof",
    "Doina Precup"
  ],
  "abstract": "Exploration is an essential component of reinforcement learning algorithms,\nwhere agents need to learn how to predict and control unknown and often\nstochastic environments. Reinforcement learning agents depend crucially on\nexploration to obtain informative data for the learning process as the lack of\nenough information could hinder effective learning. In this article, we provide\na survey of modern exploration methods in (Sequential) reinforcement learning,\nas well as a taxonomy of exploration methods.",
  "text": "A Survey of Exploration Methods in Reinforcement Learning\nA Survey of Exploration Methods in Reinforcement Learning\nSusan Amin\nsusan.amin@mail.mcgill.ca\nDepartment of Computer Science, McGill University\nMila- Québec Artiﬁcial Intelligence Institute\nMontréal, Québec, Canada\nMaziar Gomrokchi ∗\ngomrokma@mila.quebec\nDepartment of Computer Science, McGill University\nMila- Québec Artiﬁcial Intelligence Institute\nMontréal, Québec, Canada\nHarsh Satija ∗\nharsh.satija@mail.mcgill.ca\nDepartment of Computer Science, McGill University\nMila- Québec Artiﬁcial Intelligence Institute\nMontréal, Québec, Canada\nHerke van Hoof ∗\nh.c.vanhoof@uva.nl\nInformatics Institute, University of Amsterdam\nAmsterdam, the Netherlands\nDoina Precup\ndprecup@cs.mcgill.ca\nDepartment of Computer Science, McGill University\nMila- Québec Artiﬁcial Intelligence Institute\nMontréal, Québec, Canada\nAbstract\nExploration is an essential component of reinforcement learning algorithms, where agents\nneed to learn how to predict and control unknown and often stochastic environments. Rein-\nforcement learning agents depend crucially on exploration to obtain informative data for the\nlearning process as the lack of enough information could hinder eﬀective learning. In this\narticle, we provide a survey of modern exploration methods in (Sequential) reinforcement\nlearning, as well as a taxonomy of exploration methods.\nKeywords:\nExploration, Reinforcement Learning, Exploration-Exploitation Trade-oﬀ,\nMarkov Decision Processes, Sequential Decision Making\n1. Introduction\nWhen a reinforcement learning (RL) agent starts acting in an environment, it usually does\nnot have any prior knowledge regarding the task which it needs to tackle. The agent must\ninteract with the environment, by taking actions and observing their consequences (in the\n∗. These authors contributed equally to the work\n1\narXiv:2109.00157v2  [cs.LG]  2 Sep 2021\nxxxx\nform of rewards and next states), and then it can use this data to improve its behavior, as\nmeasured by the expected long-term return. This reliance on data that it gathers by itself\ndiﬀerentiates RL agents from those performing either supervised or unsupervised learning,\nand it is often a limiting factor in terms of the agent’s ultimate success at mastering the envi-\nronment. Speciﬁcally, if the agent only manages to visit a limited portion of the environment,\nits knowledge will be limited, leading to sub-optimal decision making (Wiering, 1999). How-\never, if the agent focuses on acquiring information regarding parts of the environment that\nit has not seen suﬃciently, it can lose the chance of gaining immediate reinforcement. This\nproblem is referred to as exploration-exploitation trade-oﬀ, and is a crucial open problem in\nreinforcement learning (alongside generalization). Handling this trad-eoﬀis inﬂuenced by\nseveral factors, including the dynamics of the environment (i.e. transition probability and\nreward distribution), properties of the state/action spaces (e.g. discrete/continuous, number\nof states/actions, . . . ), and the available number of interactions with the environment that\nthe agent is allowed while it is training.\nOur goal in this survey is to provide a broad high-level overview on the types of explo-\nration methods employed by RL agents, by reviewing literature from the last three decades.\nExploration studies have evolved during this time from simple ideas such as pure random-\nization, to increasingly eﬀective methods which have interesting theoretical guarantees, or\nimpressive empirical performance in large problems.\nExploration techniques have been categorized generally into undirected and directed\nmethods (Thrun, 1992) based on the choice of information considered by the exploration\nalgorithm. While in undirected exploration strategies, reinforcement learning agents select\nexploratory actions at random, without using any exploration-speciﬁc knowledge, directed\nexploration methods use the obtained information to pursue the exploration of less- visited\nstate-action pairs, or of state-action pairs that are deemed to be more informative for the\nagent. However, this is not the only relevant diﬀerentiation between current exploration\nmethods, as the ﬁeld has expanded drastically and more nuances have developed.\nIn this survey, we present a more detailed categorization of exploration methods in rein-\nforcement learning (see Figure 1). We attempt to group existing algorithmic approaches into\nthese categories in order to provide a more detailed understanding of the current landscape\nof methods, in terms of both the goals and the information they employ. Of course, a few\nof the methods do not fall squarely into one category, and hence are included in multiple\ncategories.\nWe note that some of the discussed techniques in this survey were originally proposed and\ndesigned for bandit settings, and only later applied in the reinforcement learning problems.\nHowever, this survey is not intended to cover exploration methods speciﬁcally designed for\nbandits, as many references exist in this area, including the excellent recent textbook Lat-\ntimore and Szepesvári (2020).\nWe will focus only on sequential decision making, where\nexploration has an even larger impact, as it controls not only the immediate information re-\nceived by the agent, but also the potential interesting information in its future data stream.\nIn addition, the very large number of publications in this ﬁeld, especially in recent years,\nrequires making hard choices regarding which papers to discuss in this survey. In some cases,\nwe have picked particular representative methods for certain approaches, rather than listing\nall instances of a particular approach. Finally, in order to provide a concise and easy-to-\nunderstand overview of the categories and methods, we do not focus on the mathematical\n2\nA Survey of Exploration Methods in Reinforcement Learning\ndetails and theoretical results in the ﬁeld. Our main goal is to provide an entry point into\nthe ﬁeld, which is accessible to readers who may want to understand the types of algorithms\nand empirical evaluations that have been provided, and to practitioners who want to build\nsuccessful applications, and hence have to tackle this really diﬃcult problem.\nThe survey is organized as follows. In the following section, the notation is introduced\nand a brief RL background is provided. The exploration categories are subsequently pre-\nsented in Section 3. Section 4 presents exploration methods that do not use reward infor-\nmation at all. Section 5 presents methods that rely mainly on randomizing action choices.\nSection 6 presents methods that are based on optimism in the face of uncertainty. Section 7\nintroduces methods which start with the formulation of the optimal exploration-exploitation\ntrade-oﬀ, and then approximate its solution. Section 8 discusses probability matching meth-\nods, including posterior sampling. Finally, Section 9 provides some conclusions and perspec-\ntives.\n2. Notation and Background\nIn reinforcement learning problems, at each discrete time step t = 0, 1, 2, . . . , the system\nis at state st and the agent interacts with the environment by selecting action at. Conse-\nquently, the system transitions from the state st to st+1, determined by the transition model\nof the system P and returns a numerical reward rt. Upon receiving the reward, the agent\ndecides on the next action at+1 and the process continues. Depending on the type of the\nproblem, the decision making process either stops when it reaches a terminal state in an\nepisodic task or continues in a continuing task with an inﬁnite horizon. In the following\nparagraphs, we introduce the commonly used notation in this survey. Note that capital\nand calligraphic letters denote random variables and sets, respectively, unless stated other-\nwise. For a comprehensive introduction to reinforcement learning, refer to Sutton and Barto\n(1998a).\n2.1 Markov Decision Processes\nIn this survey, the problems are modeled as Markov decision processes (MDPs) M =\n⟨S, A, P, r⟩, where S and A denote sets of all possible states and actions in the system,\nrespectively. MDPs assume that the environment is Markovian; i.e. the transition prob-\nability distribution P : S × A →D (S) determines the next state from the probability\ndistribution over the set of states D (S) as a function of the current state-action pair only.\nIn other words, in MDPs we have,\nP\n\u0000St+1 = s′ | st, at\n\u0001\n= P\n\u0000St+1 = s′ | st, at, st−1, at−1, . . . , s0, a0\n\u0001\n.\n(1)\nIf the agent starts from state s and takes an action a, it transitions to the state s′ with the\nprobability,\nP\n\u0000s, a, s′\u0001\n= P\n\u0000St+1 = s′ | St = s, At = a\n\u0001\n.\n(2)\nThe expected rewards can be written as the reward function r : S × A →R, which maps\nthe current state-action pair (s, a) to the immediate reward obtained from the set of real\nnumbers R in the system,\nr (s, a) = E [Rt+1 | St = s, At = a] ,\n(3)\n3\nxxxx\nwhere the expectation is with respect to the randomness induced by the reward function r.\nAll the sequence of observations, actions and any kind of information the agent obtains\nduring its lifetime is called history,\nHt = S0, A0, R0, S1, A1, R1, . . . , St, At, Rt.\n(4)\nA trajectory τ is deﬁned as the sequence of information extracted from the history HT\nfor the horizon T.\n2.2 Reinforcement Learning Setting\nIn reinforcement learning setting, agent behaves according to a policy π ∈Π, where Π\nrepresents the set of all possible policies. A deterministic policy π : S →A at time step\nt, also represented as at = π (st), returns a particular action at, while a stochastic policy\nπ (at | st) gives a probability distribution over a set of actions (π : S →D (A)), deﬁned as\nπ (at | st) = P (At = at | St = st) .\n(5)\nThe ultimate goal of reinforcement learning is to ﬁnd an optimal policy π⋆∈Π, which\nmaps states to actions that lead to the maximization of the expected (discounted) cumulative\nfuture reward Jπ\nJπ = EP,π [G] ,\n(6)\nwhere G denotes the return. For continuing tasks with inﬁnite horizon, where the task never\nends, G is deﬁned as the discounted cumulative reward\nG =\n∞\nX\nt=0\nγtRt,\n(7)\nwhere γ ∈[0, 1) is the discount factor, and is used to determine and control the importance\nof the future rewards. In the cases with ﬁnite horizon T (episodic tasks), the return G can\nbe modiﬁed to the undiscounted version\nG =\nT\nX\nt=0\nRt.\n(8)\nThe expected return for action selection policy π given the initial state S0 = s, is called the\nstate value function V π(s), written as\nV π (s) := EP,π [G | S0 = s] = EP,π\n\" ∞\nX\nt=0\nγtRt | S0 = s\n#\n,\n(9)\nand is related to the expected return as Jπ = EP [V π (S)].\nAn alternative to state value function is action value function Qπ (s, a), which considers\nstate-action pair (s, a) instead of state s only. In fact, it gives the value of taking action a\nin state s under a given policy π, and is deﬁned as\nQπ (s, a) = EP,π [G | s, a] = EP,π\n\" ∞\nX\nt=0\nγtRt | s, a\n#\n.\n(10)\n4\nA Survey of Exploration Methods in Reinforcement Learning\nAmong all possible true value functions V π for diﬀerent policies π ∈Π, there exists an\noptimal value function V ⋆corresponding to an optimal policy π⋆, which is at least as large\nas the others deﬁned as,\nV ⋆(s) := max\nπ\nV π (s)\n(11)\nfor all s ∈S. The optimal policy π⋆(s) for all s ∈S is thus a solution to maxπ V π (s).\nSimilarly, an optimal action value function Q⋆can be deﬁned for taking action a while\nbeing in state s. The optimal state value function V ⋆and optimal action value function Q⋆\nare related as\nV ⋆(s) = max\na\nQ⋆(s, a) .\n(12)\nThe optimal policy π⋆(s) can thus be written as\nπ⋆(s) = arg max\na\nQ⋆(s, a) .\n(13)\nHere, we introduced the general notation used throughout this survey. The notation speciﬁc\nto each section will be introduced in its respective category. In the next section, we propose\na method of categorization for exploration techniques in reinforcement learning.\n3. Categorization of Exploratory Techniques\nEﬃcient exploration has been acknowledged as an important problem in adaptive control\nfor quite a few decades, starting with the literature on bandit problems, eg. Thompson\n(1933). In this survey, however, we will not discuss the bandit literature, which is vast\nand has been the topic of the recent book Lattimore and Szepesvári (2020). Instead, we\nfocus on sequential decision making.\nSome of the early studies that acknowledged the\nimportance of eﬃcient exploration in this context were delivered by Mozer and Bachrach\n(1990); Sutton (1990); Moore (1990); Schmidhuber (1990) and Barto et al. (1991). A study\nby Mozer and Bachrach (1990) showed that eﬃcient learning of tasks modeled by a ﬁnite-\nstate automaton is achievable using simple random exploration, provided that the number\nof states is small enough, while learning more complex tasks requires a more intelligent\nexploration technique that accelerates the coverage of the state space. In another study,\nSutton (1990) demonstrated that the selection of sub-optimal actions (exploration) in non-\nstationary maze domains is essential for eﬃcient learning, even though it may negatively\naﬀect the short-run acquisition of rewards.\nExploration techniques have been categorized in a few studies mainly based on the choice\nof inclusion of information in pursuing exploration.\nFor instance, in a technical report,\nMoore (1990) emphasizes the interplay between the exploration-exploitation balance and\neﬃcient learning. He categorizes exploratory moves based on the action selection method\ninto entirely random, local random, and sceptical categories. His proposed categorization\nstates that while in the entirely-random method the agent chooses the exploratory actions\ntotally at random, in the local-random experimentation it selects actions from the perturbed\nbest known actions. As the very ﬁrst action in the local-random method is chosen completely\nat random, the performance of this method is very sensitive to the quality of the ﬁrst chosen\naction. Thus, a wrong decision at the initial step may lead to much larger learning times.\nThe sceptical exploration method chooses actions depending on the prediction for the best\n5\nxxxx\nknown action. If it is predicted to be unsuccessful, the agent explores other actions, and\nselects the best known action otherwise. Although this categorization of exploration methods\ncan explain the similarities and diﬀerences between some of the early proposed approaches,\nit does not provide a general foundation for classifying exploration techniques.\nOne of the ﬁrst general categorization of the exploration methods was introduced by\nThrun (1992). He grouped exploration techniques into two general categories: undirected\nand directed methods. The undirected or uninformed exploration methods do not use any\nsort of exploration-speciﬁc knowledge in order to perform the exploration task. These meth-\nods generally rely solely on randomness in selecting actions. Random walk is the simplest\nmethod in this category, which is believed to be ﬁrst utilized in action selection mechanisms\nby Anderson (1986), Munro (1987), Mozer and Bachrach (1990), Jordan (1989), Nguyen and\nWidrow (1990) and Thrun et al. (1991). Other examples of undirected methods consist of\nthe exploration techniques related to Boltzmann distribution (based on utility and temper-\nature parameter for controlling the exploration-exploitation trade-oﬀ) (Barto et al., 1991;\nWatkins, 1989; Lin, 1992; Singh, 1992; Sutton, 1990; Lin, 1990) and random action selec-\ntion with a certain probability (Whitehead and Ballard, 1991; Mahadevan and Connell, 1992,\n1991). On the contrary, directed or informed exploration methods utilize exploration-speciﬁc\nknowledge of the learning process to direct the agent towards exploring the environment.\nThese exploration techniques are more eﬃcient and beneﬁcial compared with the undirected\nexploration methods in terms of complexity and cost (Thrun, 1992; Whitehead, 1991). In\nparticular, random exploration methods may lead to an increase in the learning time as well\nas safety issues due to the random selection of unsafe actions repeatedly (especially in real\ncases, such as in robotics). Consequently, in the following years, exploration-related studies\nfocused increasingly on “directed” exploration methods. The consequent diversity in these\napproaches necessitates the provision of a new basis for a reﬁned categorization of these\nmethods.\nTo address the issues corresponding to the existing categorization methods, we propose\na new classiﬁcation approach based on the type of information the agent uses to explore the\nworld (Figure 1). In particular, we categorize the RL exploration methods into the two gen-\neral classes, namely “Reward-Free Exploration”, where the included exploration techniques\ndo not use the extrinsic rewards in their action selection process, and “Reward-Based Ex-\nploration”, in which extrinsic rewards aﬀect the choice of exploratory actions. These two\nclasses are further divided into two groups “Memory-Free” and “Memory-Based” techniques,\ndepending on the reliance of the exploratory movements on the agent’s memory of the ob-\nserved space. The categories in each class of exploration techniques are described below and\ndetailed in the following sections.\n• Reward-Free Exploration - The general property of the exploration methods in-\ncluded in this category is that rewards (or value functions) do not aﬀect the choice\nof actions in their action-selection criteria. In other words, actions are selected with-\nout regard to the obtained rewards or the value functions. The methods belonging in\nthis section can either act completely blindly, which we call blind exploration, or utilize\nsome sort of information (other than extrinsic rewards) in the form of intrinsic rewards\nin order to encourage exploration. This type of exploration methods are referred to\n6\nA Survey of Exploration Methods in Reinforcement Learning\nExploration Categories \nReward-Free \nReward-Based \nMemory-Free \nMemory-Based \nRandomized  \nAction Selection \nValue-Based \nPolicy-Search \nBased \nDeliberate \nExploration \nBayes-Adaptive \nMeta Learning \nMemory-Free \nMemory-Based \nBlind \nIntrinsically-\nMotivated \nOptimism/Bonus-\nBased \nProbability \nMatching \nOptimistic \nInitialization \nCount-Based \nError-Based \nFigure 1: Exploration Categories- The exploration methods are categorized into two main\ngroups reward-free and reward-based exploration techniques, depending on their utilization\nof extrinsic rewards. Each group is further divided to memory-based and memory-free cat-\negories based on the reliance of the exploratory decisions on the agent’s memory of the\nobserved space.\nas Intrinsically-Motivated Exploration techniques. The details regarding this category\nand its subcategories are provided in section 4.\n• Randomized Action Selection - The exploration methods in this category induce\nexploratory behaviour via assigning action selection probabilities to the admissible ac-\ntions based on the estimated value functions or rewards (Value-Based Exploration),\nor the learned policies (Policy-Search Based Exploration). The exploration methods\nincluded in the former group use the reward-based feedback in order to handle the\nexploration-exploitation trade-oﬀ. The list of the exploration techniques in this cate-\ngory as well as the detailed explanation of each method are provided in section 5.1. In\nthe latter group, exploration methods explore the environment via performing search\nin the space of policies.\nThey learn a stochastic policy, whose stochasticity helps\nthe agent balance the trade-oﬀbetween exploration and exploitation in the system.\nThese methods explicitly represent policies, and aim to update them to maximize the\nexpected extrinsic rewards (section 5.2). Note that the policy-search based methods\nthat do not utilize extrinsic rewards in their exploratory decision making are listed\nand discussed in section 4.\n• Optimism/Bonus-Based Exploration - The exploration techniques in this cate-\ngory function based on the principle of optimism in the face of uncertainty, where\nactions with uncertain values are preferred over the rest of the possible actions. In\nthis category of exploration methods, the methods usually involve a form of bonus,\nwhich is added to the extrinsic reward, leading to a directed search in the spaces of\nstate-action. The methods included in this category and the details are provided in\nsection 6. The main diﬀerence between bonus-based techniques and the intrinsically-\nmotivated exploration methods, discussed in section 4, is that the latter does not utilize\nextrinsic reward for motivating the exploration of the environment. Diﬀerent forms\n7\nxxxx\nof optimism/bonus-based exploration approaches are discussed in section 6, including\ncount-based exploration methods and prediction-error based approaches. Finally, the\nclear distinction between the methods in this category and those in the Stochastic\nAction Selection methods (5) is that the techniques considered as Optimism/Bonus-\nBased direct the agent’s moves with the use of bonuses, while the methods in the other\ngroup rely solely on the extrinsic rewards.\n• Deliberate Exploration - This category includes exploration methods that operate\nbased on solving the exploration-exploitation tradeoﬀoptimally and is discussed in\nsection 7.\nThis category consists of Bayes-Adaptive exploration methods that are\nrealized with a Bayesian model-based set-up, where the posterior distribution over\nmodels is computed and updated assuming a prior over the transition dynamics. This\ngroup also consists of Meta-Learning Based Exploration techniques, via which the\nagent learns to adapt quickly using the prior given tasks.\n• Probability Matching - This category of exploration techniques uses a heuristic to\ndecide the next action based on sampling a single instance from the posterior belief over\nenvironments or value functions, and solving for that sampled environment exactly.\nThe agent can then act according to that solution, e.g. for the duration of one episode.\nEach action is thus taken with the according to the probability the agent considers it to\nbe the optimal action. This heuristic eﬀectively directs exploration eﬀort to promising\nactions.\nIn terms of the categorization proposed by Thrun (1992), we can classify our proposed\ncategories into the two general groups of directed and undirected exploration techniques.\nIn this regard, in the reward-free exploration category, blind exploration methods (section\n4.1) are undirected, while the intrinsically-motivated exploration techniques (section 4.2)\nare considered directed exploration approaches. The stochastic action selection exploration\ncategory (section 5.1) consists of undirected techniques. The rest of the categories fall under\nthe directed exploration category.\nIn the following sections, the above mentioned groups of exploration techniques are dis-\ncussed in more detail and the methods under each group are explained. In a few cases among\nthe exploration techniques, there exist some approaches that belong to two of the proposed\ncategories, leading to a small overlap between the groups. Whenever such situation is en-\ncountered, the respective exploration method is noted as shared between the corresponding\ncategories.\n4. Reward-Free Exploration\nWe use the notion of reward-free exploration in reinforcement learning to describe any\nmethod of exploration that does not incorporate extrinsic reward in their exploratory ac-\ntion selection criteria. This type of exploration methods was ﬁrst introduced and utilized\nwith the name pure exploration in multi-armed bandits, a set of sequential decision-making\ntasks where at each time step, an agent pulls an arm and receives a random reward drawn\nfrom the reward distribution of that speciﬁc arm (Bubeck et al., 2009).\nIn particular,\nthese exploration techniques do not incorporate the rewards obtained from the environment\n8\nA Survey of Exploration Methods in Reinforcement Learning\n(i.e. extrinsic rewards) in measuring the cost of picking bandit arms. Instead, they uti-\nlize other available resources in a limited budget, such as CPU time or cost, in order to\nacquire knowledge. Similarly, there have been reward-free exploration methods proposed\nfor the reinforcement learning framework, which do not rely on the extrinsic rewards the\nagent receives from the environment. These exploration techniques can be: 1) completely\nblind, where the exploratory agent selects actions in the absence of any sort of information\nobtained as the result of its interaction with the environment; or 2) driven by some form of\nintrinsic motivation and curiosity. These two forms of reward-free exploration methods are\nfurther explained, and the corresponding proposed methods are discussed in the following\nparagraphs and listed in Table 1. Note that in neither of the aforementioned cases, the agent\nuses extrinsic reward as a source of knowledge. Thus, bonus-based methods (i.e. exploration\nmethods that rely on a sort of bonus reward in addition to extrinsic reward) do not belong\nin this category and are discussed in Optimism/Bonus-Based Exploration section (section\n6).\n4.1 Blind Exploration\nExploration techniques in the blind exploration category explore environments solely on the\nbasis of random action selection. In other words, these agents are not guided through their\nexploratory path by any form of information, thus are uninformed or blind. This category\nof exploration techniques is indeed the most basic type of reward-free exploration, and\nincludes random-walk as the simplest exploration method (Thrun, 1992). Some examples\nof the early uses of random walk in exploring the eﬀect of various actions on diﬀerent states\nwere the studies performed by Anderson (1986); Mozer and Bachrach (1990) and Jordan\n(1989). In the random walk method, the agent chooses actions randomly regardless of the\ninformation it has obtained so far and thus, due to the uniformly random probability of\nselecting actions, there is a chance that the picked action takes the agent away from the\ngoal rather than taking it closer (i.e. exploration). On the other hand, it leads to large\ncomplexity that grows exponentially with the size of the environment (Whitehead, 1991),\nwhich makes random-walk an ineﬃcient exploration technique.\nAnother simple yet eﬀective exploration method in this category is known as the method\nof ϵ-greedy (Sutton, 1996), also known as max-random or pseudo-stochastic (Caironi and\nDorigo, 1994; Watkins, 1989). In the ϵ-greedy approach, the parameter ϵ ∈[0, 1] controls the\nbalance between exploration and exploitation. The action at at every time step t is chosen\nsuch that,\nat =\n(\na⋆\nt ,\nwith probability 1-ϵ\nrandom action\nwith probability ϵ,\n(14)\nwhere a⋆\nt is the greedy action taken at time t with respect to the greedy policy (exploitation).\nThe ϵ-greedy method has been found to be quite eﬀective in diﬀerent RL settings (Sutton\nand Barto, 1998b). In particular, it is eﬃcient in the sense that it does not need to cache any\ndata to perform exploration, and the only hyperparameter to adjust is ϵ. However, despite\nthe fact that the ϵ-greedy method guarantees that at inﬁnite time horizon every state-action\npair is visited inﬁnitely often, it stays sub-optimal in the sense that it asymptotically prevents\nthe agent from selecting the best action (Vermorel and Mohri, 2005). Another problem an\n9\nxxxx\nagent may encounter while using ϵ-greedy is the lack of decisiveness in the exploration phase,\nwhich might in turn lead to getting stuck in local optima. To address this issue, a temporally\nextended form of ϵ-greedy, called ϵz-greedy (Dabney et al., 2020), has been proposed, where\nrandom exploratory actions are replaced by temporally-extended sequence of actions. In\nparticular, the ϵz-greedy agent exploits with probability 1−ϵ and explores via repeating the\nsame action for a certain number of steps n ∼z, where z(n) is a distribution over the action-\nrepeat duration n. Dabney et al. (2020) perform experiments in tabular as well as deep RL\nframeworks in tasks with discrete-action spaces, and tabular or discretized continuous state\nspaces.\nAlthough ϵ is usually hand-tuned depending on the type of the problem, there are other\nproposed extensions of the ϵ-greedy method, such as ϵ-ﬁrst (Tran-Thanh et al., 2010) (where\nexploration is done during the ﬁrst ϵT time steps- T is the total number of steps) and\ndecreasing-ϵ in bandits (Caelen and Bontempi, 2007), where ϵ is a decreasing function of\ntime, as well as the derandomization of ϵ-greedy in RL tasks (Even-Dar and Mansour,\n2002). Another extension of the ϵ-greedy approach is the Value-Diﬀerence Based Exploration\n(VDBE) method (Tokic (2010) in Bandits and Tokic and Palm (2011) in reinforcement\nlearning), which adjusts the exploration rate ϵ based on the changes in the state-action\nvalue functions.\nThe methods Even-Dar and Mansour (2002); Tokic (2010); Tokic and\nPalm (2011), which incorporate extrinsic rewards in their exploratory decision making, are\ndiscussed in detail in section 5.1. There are other blind exploration methods mainly proposed\nin the ﬁeld of Robotics, for instance the spiral search technique (Burlington and Dudek,\n1999), which ensures visiting new locations in planar environments via expanding the search\nin the space with the use of logarithmic spirals. However, due to the study limit of these\nmethods to Robotics and planar environments, we are not going to cover them here.\n4.2 Intrinsically-Motivated Exploration\nThe second exploration type in the reward-free exploration category is the intrinsically mo-\ntivated exploration, which is composed of the methods that utilize a form of intrinsic mo-\ntivation in the absence of external rewards to promote exploring the unexplored parts of\nthe environment. In contrast to blind exploration, intrinsically-motivated exploration tech-\nniques utilize some form of intrinsic information to encourage exploring the state-action\nspaces. The idea of employing internal incentives in exploratory tasks is borrowed from\nintrinsically-motivated behaviour in humans, which has been studied and discussed exten-\nsively in education and psychology literature (Deci, 1971, 1975; Amabile et al., 1976; Benware\nand Deci, 1984; Deci and Ryan, 1985; Grolnick and Ryan, 1987). In psychology, the distinc-\ntion between an extrinsically and an intrinsically motivated behaviour is made based on the\ntypes of the stimuli that “move” the person to perform a task (Ryan and Deci, 2000). While\nintrinsic motivation leads to an inherent satisfaction of performing a job, extrinsic motiva-\ntion sets an external regulation, which encourages a person to do a task in order to obtain\nsome separable outcome (e.g. reward or reinforcement). Studies show that internalization\nof the external regulations, also known as self-regularization or self-determination (Deci and\nRyan, 1985), helps children attain higher achievements (Benware and Deci, 1984; Grolnick\nand Ryan, 1987) in terms of learning or completing complex tasks, in contrast to using ex-\ntrinsic motivations in the form of rewards or reinforcement. Analogs of intrinsic motivation\n10\nA Survey of Exploration Methods in Reinforcement Learning\nin human can be employed in order to promote exploration in the RL framework. Here, we\nreview some of these studies and discuss diﬀerent forms of intrinsic motivation that have\nbeen used in exploration techniques in the reinforcement learning tasks.\nIn the context of reinforcement learning, curiosity takes various interpretations and forms\ndepending on the types of problems and the deﬁned goals as well as the approaches taken\ntoward understanding and solving the problems. In general, we can deﬁne curiosity as a\nway or desire to explore new situations that may help the agent with pursuing goals in the\nfuture. As agents are encountered with deceptive and/or sparse rewards in many RL set-\nups, exploratory agents that rely on extrinsic rewards might end up in local optima because\nof deceptive rewards or get stuck due to zero gradient in the received rewards. Thus, the\ntechniques that intrinsically motivate the agent to explore the environment and do not rely\non the extrinsic reinforcement are eﬀective in learning of such tasks.\nMany of the reward-free intrinsically-motivated exploration strategies aim at minimizing\nthe agent’s uncertainty or error in its predictions (Schmidhuber, 1991a,b; Pathak et al.,\n2017).\nIn order to evaluate the precision of the agent’s predictions of the environment\nbehavior, a model of the environment dynamics is required, such that given the current\nstate and the chosen action, the model predicts the next state. Minimization of the resulting\nerror in the model prediction encourages the exploration of the underlying space. There are\nother reward-free techniques that pursue the maximization of space coverage (new states\nor state-action pairs visitation), which utilize a form of intrinsic motivation to govern the\nagent’s exploratory behaviour (Hazan et al., 2019; Amin et al., 2020). More space coverage\nessentially means visiting more unexplored states in a shorter amount of time and in turn,\nlearning more about the environment. In this section, we survey and discuss some of the\nreward-free intrinsically-motivated exploration approaches that seek either of the above-\nmentioned goals. Note that the notion of intrinsic motivation in RL tasks has been also\nused in combination with external rewards, which is not the subject of our discussion in\nthis section and will be elaborated in the “Bonus/Optimism-Based Exploration” category\n(section 6).\nThe early use of intrinsic motivation in computational framework dates back to 1976,\nwhen Lenat (1976) used the notion of “interestingness” in mathematics to encourage new\nconcepts and hypotheses. Scott and Markovitch (1989) introduced DIDO, a curiosity-driven\nlearning strategy that can help the agent explore initially unknown domains in an unsuper-\nvised set-up using the notion of Shannon’s uncertainty function deﬁned as\nsh = −\nn\nX\ni=1\n(pi × log2(pi)) ,\n(15)\nwhere pi is an estimate of the probability of the outcome Oi, and the summation is taken\nover all outcomes.\nMinimization of uncertainty in their proposed formalism leads to a\nbroader searching span, as well as more eﬀective learning of the search space. DIDO, in\nfact, promotes the idea of using an experience generator that provides experiences, which\nare novel compared to the previous ones and are related to them at the same time. The\nobtained experiences help the agent search for a better representation in the space of pos-\nsible representations while DIDO’s representation generator is employed in ﬁnding more\ninformative experiences. Scott and Markovitch (1989) performed DIDO in several discrete\n11\nxxxx\ndomains, which showed that their exploratory algorithm enables the agent to select sensible\nexperiences and eventually leads to a good representation of the domain.\nIn the following years, another form of curiosity-driven exploration was introduced\n(Schmidhuber, 1991a,b) based on the improvement in the reliability of the RL agent’s pre-\ndictions of the world model. In particular, Schmidhuber (1991b) proposed a model-building\ncontrol system that could provide an adaptive model of the environmental dynamics. He\nfurther proposed the notion of dynamic curiosity and boredom, described as “the explicit\ndesire to improve the world model”, as a potential means of increasing the knowledge of\nthe animat about the world in the exploration phase. In his work, curiosity aims at mini-\nmization of the agent’s ignorance and is triggered when the agent comes to the realization\nthat it does not have enough knowledge of something. It provides a source of reinforcement\nfor the agent and is deﬁned as the Euclidean distance between the real and the predicted\nmodel network. A failure in the correct prediction of the environment leads to a positive\nreinforcement that encourages the agent to further explore the corresponding actions. Im-\nprovement in the world model predictions with time leads to less reinforcement and thus,\ndiscouragement of exploring the corresponding actions, also referred to as boredom.\nWhile the idea of using curiosity was not implemented in Schmidhuber (1991b), Schmid-\nhuber later utilized the notion of adaptive curiosity (Schmidhuber, 1991a) to encourage\nexploration of the unpredictable parts of the environment. In particular, he proposed a\ncurious model-building control system, where the notion of adaptive conﬁdence was used\nfor modeling the reliability of a predictor’s predictions, and adaptive curiosity was utilized\nto reward the agent for encountering hard but learnable states and thus improve the ex-\nploration phase by reducing the extra time spent on the non-useful or well-modelled parts\nof the environment. The strength of his proposed approach in comparison to its prede-\ncessors including his previous study (Schmidhuber, 1991b), lies in its ability to work in\nuncertain non-deterministic environments by adaptively modeling the reliability of a pre-\ndictor’s predictions and learning to predict cumulative error changes in the model network.\nThis goal can be achieved via maximization of the expectation of cumulative changes in\nprediction reliability. Schmidhuber (1991a) tested his proposed curiosity-driven algorithm\nbased on Watkin’s Q-learning in two-dimensional discrete-state toy environments with over\n100 states and compared the results to the ones obtained using random search as the ex-\nploratory approach. Utilizing an adaptive curious agent led to a decrease of an order of\nmagnitude in the learning time.\nAnother similar yet diﬀerent exploration approach was proposed by Thrun and Möller\n(1992) around the same time, which suggested using the notion of competence map for\nguiding exploration via estimating the controller’s accuracy. In particular, Thrun and Möller\n(1992) introduced a notion of energy\nE = (1 −Γ)Eexplore + ΓEexploit,\n(16)\nwhere gain parameter 0 < Γ < 1 controls the exploration-exploitation trade-oﬀand is a\nfunction of the change in the exploration energy Eexplore and the exploitation energy Eexploit.\nA competence network system is trained to estimate the upper bound of the “model network\nerror”; minimization of the “expected competence” leads to the exploration of the world.\nThrun and Möller (1992) employed “competence map” in a continuous two-dimensional\n12\nA Survey of Exploration Methods in Reinforcement Learning\nrobot navigation task, which revealed that their suggested exploration method can perform\nbetter compared with “random walk” and “purely greedy” approaches.\nAn exploration approach was later proposed by Storck et al. (1995) as an extension of\nprevious similar studies, such as Schmidhuber (1991a,b); Thrun and Möller (1992). Their\nproposed exploration method, called Reinforcement Driven Information Acquisition (RDIA),\nis devised for non-deterministic environments and utilizes the notion of information gain,\nwhich is used as an intrinsic motivation to govern the agent’s exploratory movements. In\nparticular, the RDIA agent models the environment via estimating the transition probability\np⋆\nijk(t) at each time step t as the ratio of the number of times so far that the pair (si, aj)\nhas led to the state sk over the number of times the agent has experienced (si, aj). The\ninformation gain is then deﬁned as the diﬀerence between the agent’s current estimation of\nthe transition probability p⋆\nijk(t) and p⋆\nijk(t + 1) at time t + 1. Information gain represents\nthe information that the agent has acquired upon performing the respective action, which\nconsequently leads to an increase in the estimator’s accuracy. Storck et al. (1995) assess\ntheir proposed method in simple discrete environments with certain numbers of states and\nactions using two diﬀerent information gain measures, namely the entropy diﬀerence and\nthe Kullback-Leibler (KL) distance, between the probability distributions, and show that\nthe results obtained by RDIA surpass those of simple random search.\nInformation gain as intrinsic motivation has been used in other exploration strategies\nsuch as the studies performed by Little and Sommer (2013) and Mobin et al. (2014). In\nparticular, the exploration method proposed by Little and Sommer (2013) takes a Bayesian\napproach, where the agent builds an internal model of the environment, and upon taking\nan action and observing the next state, calculates the KL-divergence of its current internal\nmodel from the one it had predicted prior to taking the action. The resulting unweighted\nsum of the KL-divergences yields the missing information IM, which is utilized as a measure\nof inaccuracy in the agent’s internal model.\nThe agent subsequently takes actions that\nmaximize the expected (predicted) information gain (PIG), deﬁned as the expected decrease\nin the missing information IM between the internal models. Another similar exploration\nmethod (Mobin et al., 2014) extends the application of PIG (Little and Sommer, 2013) to\nperform in environments with unbounded discrete state spaces. In particular, Mobin et al.\n(2014) utilize the Chinese Restaurant Process (CRP)(Aldous, 1985) to ﬁnd the probability\nof revisiting a state or discovering a new one, and use the obtained results to calculate the\nagent’s internal model. The subsequent steps are similar to those presented and discussed by\nLittle and Sommer (2013). Another study by Shyam et al. (2019) introduces the Bayesian\nModel-based Active eXploration (MAX) method, which utilizes the novelty of transitions as\na learning signal, and is applicable in discrete and continuous environments. In particular,\nMAX agent calculates the Jenson-Shannon divergence and the Jensen-Rényi divergence\n(Rényi et al., 1961) of the predicted space of distributions from the resulting one in discrete\nand continuous environments, respectively. Maximization of the resulting novelty measure\ngoverns the agent’s exploratory behaviour. The evaluation of MAX performance in several\ndiscrete and continuous tasks presents promising results compared with those obtained from\nMAX counterparts and other baselines.\nAnother curiosity-driven approach is “Intrinsic Curiosity Module” (ICM) (Pathak et al.,\n2017), where curiosity is deﬁned as “the error in an agent’s ability to predict the consequence\nof its own actions”. In their set-up, the agent interacts with high-dimensional continuous\n13\nxxxx\nApproach\nIntrinsic\nRemarks\nMotivation\nAnderson (1986)\nNone (Blind)\nEarly use of random walk\nMozer and Bachrach (1990)\nNone (Blind)\nEarly use of random walk\nJordan (1989)\nNone (Blind)\nEarly use of random walk\nSutton (1996)\nNone (Blind)\nϵ-greedy\nCaironi and Dorigo (1994)\nNone (Blind)\nmax-random (ϵ-greedy)\nDabney et al. (2020)\nNone (Blind)\nϵz-greedy (Temporally-extended actions)\nBurlington and Dudek (1999)\nNone (Blind)\nSpiral search (For planar environments\nonly)\nSchmidhuber (1991b)\nUncertainty\nAdaptive model of the environment dy-\nnamics\nScott and Markovitch (1989)\nUncertainty\nMinimization of Shannon’s uncertainty\nfunction\nSchmidhuber (1991a)\nUncertainty\nPrediction of cumulative error changes\nThrun and Möller (1992)\nUncertainty\nCompetence map\nStorck et al. (1995)\nUncertainty\nMaximization of information gain\nLittle and Sommer (2013)\nUncertainty\nMaximization\nof\nexpected\ninformation\ngain\nMobin et al. (2014)\nUncertainty\nMaximization\nof\nexpected\ninformation\ngain\nShyam et al. (2019)\nUncertainty\nUses an ensemble of forward dynamics\nmodels\nPathak et al. (2017)\nUncertainty\nMinimization of predicted error in feature\nrepresentation\nHazan et al. (2019)\nSpace coverage\nMaximization of entropy of the distribu-\ntion over the visited states\nAmin et al. (2020)\nSpace coverage\nGeneration of correlated trajectories in the\nstate and action spaces\nForestier et al. (2017)\nSelf-generated\ngoals\nCoverage maximization of the space of\ngoals\nColas et al. (2018)\nSelf-generated\ngoals\nCombines Forestier et al. (2017) with\nDDPG\nMachado et al. (2017)\nSpace coverage\nMaximization of eigenpurposes\nMachado et al. (2018b)\nSpace coverage\nExtension of Machado et al. (2017) to\nstochastic environments\nJinnai et al. (2019)\nSpace coverage\nMinimization of cover time\nJinnai et al. (2020)\nSpace coverage\nExtension of Jinnai et al. (2019) to large\nor continuous state spaces\nHong et al. (2018)\nSpace coverage\nEncouraging new policies using a distance\nmeasure between the policies\nTable 1: Examples of some reward-free exploration approaches.\n14\nA Survey of Exploration Methods in Reinforcement Learning\nstate spaces (images in this case). The authors show that ICM helps the agent to learn and\nimprove its exploration policy in the presence of sparse extrinsic rewards as well as in the\nabsence of any sort of environmental rewards. Moreover, they show that the curious agent\ncan apply its gained knowledge and skills in new scenarios and still achieve improved results.\nThe main idea behind ICM is that instead of targeting learnable states and rewarding the\nagent for detecting them (Schmidhuber, 1991b,a), ICM (Pathak et al., 2017) focuses only\non a feature representation that reﬂects the parts of the environment that either aﬀect\nthe agent or get aﬀected by the agent’s choice of actions. Intuitively, by focusing on the\ninﬂuential feature space instead of the state space, ICM is able to avoid the unpredictable\nor unlearnable parts of the environment.\nNote that in some of the fore-mentioned proposed algorithms (Schmidhuber, 1991b,a;\nPathak et al., 2017), while the external reinforcement is not a necessary component, the\nexternal reward, if exists, can be added to the curious reinforcement. Thus, these studies\nare included in the list of “Bonus/Optimism-Based Exploration” category as well (section\n6).\nSome of the intrinsically-motivated exploration techniques utilize the analogy between\nthe dynamical and physical systems, and thus propose the notion of entropy maximization\nto encourage exploration of the search space. In this regard, an early utilization of entropy in\nintelligent adaptation of search control was in the context of search eﬀort allocation problem\nin genetic search procedures (Rosca, 1995), where entropy was used as a measure of diversity.\nAround the same time, Wyatt (1998) deﬁned a notion of entropy for the case of bandits as\na measure of the uncertainty regarding the identity of the optimal action. In his proposed\nexploration algorithm, the agent selects the more informative action, which is the one that\non expectation will lead to a larger entropy reduction. In the ﬁeld of reinforcement learning\n(which is the main focus of the current survey), there are several studies that utilize notion\nof entropy in their proposed exploration techniques (Achbany et al., 2006; Lee et al., 2018;\nYin, 2002; Hazan et al., 2019). In this section, however, we discuss the method proposed\nby Hazan et al. (2019) as it is the only reward-free technique among the studies that utilize\nthe notion of entropy in guiding exploration. Hazan et al. (2019) introduce an exploration\napproach, which targets environments that do not provide the agent with extrinsic reward. In\ntheir proposed method, the intrinsic objective is to maximize the entropy of the distribution\nover the visited states. They introduce an algorithm, which optimizes objectives that are\nonly functions of the state-visitation frequencies. In particular, it generates and optimizes a\nsequence of intrinsic reward signals, which consequently leads to the entropy maximization\nof the distribution that the policy induces over the visited states. The reward signals form\na concave reward functional R(dπ), which is a function of the entropy of the induced state\ndistribution dπ given policy π. The obtained optimal policy is referred to as maximum-\nentropy (MaxEnt) exploration policy, which is deﬁned as π⋆∈arg maxπ R(dπ).\nAnother intrinsically-motivated exploration approach that encourages space coverage is\nthe method of PolyRL (Amin et al., 2020), which is designed for tasks with continuous\nstate and action spaces and sparse reward structures. PolyRL is inspired by the statistical\nmodels used in the ﬁeld of polymer physics to explain the behaviour of simpliﬁed polymer\nmodels. In particular, PolyRL exploration policy selects orientationally correlated actions\nin the action space and induces persistent trajectories of visited states (locally self-avoiding\n15\nxxxx\nwalks) in the state space using a measure of spread known as the radius of gyration squared,\nU2\ng (τS) :=\n1\nTe −1\nX\ns∈τS\nd2(s, ¯τS).\n(17)\nIn equation 17, Te denotes the number of exploratory steps taken so far in the current ex-\nploratory trajectory, τS is the trajectory of the visited states, and d(s, ¯τS) is a measure of\ndistance between a visited state s and the empirical mean of all visited states ¯τS. At each\ntime step, the exploratory agent computes U2\ng (τS), and subsequently compares it with the\nvalue obtained from the previous step. In addition, it calculates the high-probability conﬁ-\ndence bounds on the radius of gyration squared, within which the stiﬀness of the trajectory\nis maintained. If the change in U2\ng (τS) is within the conﬁdence interval, the agent continues\nto explore, otherwise it selects the subsequent action using the target policy. Amin et al.\n(2020) assess the performance of PolyRL in 2D continuous navigation tasks as well as sev-\neral high-dimensional sparse MuJoCo tasks and show improved results compared with those\nobtained from several other exploration techniques.\nPolicy-Search Based Exploration without Extrinsic Reward - Policy-search\nmethods search in the parameter space θ for the appropriate parameterized policy πθ. As\npolicy-search methods typically do not learn value functions, the choice of a proper param-\neter θ is essential for ensuring an eﬃcient, stable and robust learning. This calls for an\neﬃcient exploration strategy in order to provide the policy evaluation step in the policy\nsearch methods with new trajectories and thus new information, which is subsequently used\nfor policy update (Deisenroth et al., 2013). The exploration approaches performed in policy\nsearch methods use stochastic policies, and they can either employ rewards obtained from\nthe environment to guide the exploratory trajectories or function in a completely reward-free\nmanner. In the current section, we review the policy-search methods that do not incorpo-\nrate extrinsic rewards in their exploratory decision making. We provide a more thorough\nintroduction to policy-search methods in section 5.2, where we discuss the policy-search\napproaches that utilize extrinsic rewards.\nOne of the approaches to solving problems autonomously is breaking the problem/goal\ninto smaller sub-problems/sub-goals (Forestier et al., 2017).\nThis idea is inspired from\nthe way children tend to select their objectives such that they are not too easy or too\nhard for them to handle. These intermediate learned goals facilitate learning more com-\nplex goals, which ultimately lead to building up more skills required to achieve bigger\ngoals.\nBased on this intuition, Forestier et al. (2017) propose a curiosity-driven explo-\nration algorithm called “Intrinsically Motivated Goal Exploration Process” (IMGEP). The\nIMGEP approach structure relies on assuming that the agent is capable of choosing goal\np from the space of RL problem and is able to calculate the corresponding reward r us-\ning the reward function R (p, c, θ, oτ) given the parameterized policy πθ, context c (which\ngives the current state of the environment) and the observed outcome oτ in the trajec-\ntory τ = {st0, at0, st1, at1, . . . , stend, atend}. The reward function R (p, c, θ, oτ) is thus non-\nMarkovian and can be calculated at any time during or after performing the tasks. Using\nthe computed rewards, the agent samples the interesting goal p, which is a self-generated\ngoal that leads to faster learning progress. In the exploration phase, the agent uses the meta\npolicy Πϵ (θ|p, c) to ﬁnd the parameter θ for goal p, which is subsequently utilized in a goal-\n16\nA Survey of Exploration Methods in Reinforcement Learning\nparameterized policy search process. The obtained outcome is then used in computing the\nintrinsic reward r, which in turn provides useful information regarding the interestingness\nof the samples goal p. The goal sampling strategy and the meta-policy are subsequently\nupdated. The performance of IMGEP in the case of a real humanoid robot shows that the\nIMGEP robot can eﬀectively explore high-dimensional spaces through discovering skills with\nincreasing complexity.\nOne of the exploration methods proposed based on the “Goal Exploration Processes”\n(GEPs) (Forestier et al., 2017) is the “Goal Exploration Process- Policy Gradient” (GEP-\nPG) (Colas et al., 2018), which combines the intrinsically-motivated exploration processes\nGEPs with the deep reinforcement learning method DDPG in order to improve exploration\nin continuous state-action spaces and learn the tasks. Colas et al. (2018) perform GEP-PG in\nthe low-dimensional “Continuous Mountain Car” and the higher-dimensional “Half-Cheetah”\ntasks. GEP-PG is tested in the fore-mentioned problems with diﬀerent variants of DDPG.\nThe authors show that speciﬁcally in the Half-Cheetah task, the performance, variability\nand sample eﬃciency of their proposed method surpasses those of DDPG.\nAnother policy-search based exploration strategy proposed by Machado et al. (2017)\nutilizes the notion of proto-value functions (PVFs) to discover options that lead the agent\ntowards eﬃcient exploration of the state space. PVFs, ﬁrst introduced by Mahadevan (2005),\nare the basis functions used for approximating value functions through incorporating topo-\nlogical properties of the state space. In particular, using the MDP’s transition matrix, a\ndiﬀusion model is generated, whose diagonalized form subsequently gives rise to PVFs. The\ndiﬀusion model provides the diﬀusion information ﬂow in the environment. This feature\nallows the PVFs to provide useful information regarding the geometry of the environment,\nincluding the bottlenecks. Machado et al. (2017) deﬁne an intrinsic reward function (a.k.a.\neigenpurpose) as,\nre\nin\n\u0000s, s′\u0001\n= e⊺\u0000φ\n\u0000s′\u0001\n−φ (s)\n\u0001\n,\n(18)\nwhere e ∈R|S| is the proto-value function and φ(s) is the feature representation of state s,\nwhich can be replaced by the state s itself in tabular cases. Machado et al. utilize eigen-\npurpose re\ni to discover options (a.k.a. eigenoptions) and their corresponding eigenbehaviors.\nThey subsequently use policy iteration to solve the problem for an optimal policy.\nAlthough the exploration technique introduced by Machado et al. (2017) is applicable\nto discrete domains only, one of its major advantages is that it provides a dense intrinsic\nreward function, which facilitates exploration tasks with sparse-extrinsic-reward structures.\nMoreover, since it is equipped with options, their proposed method can cover a relatively\nlarger span in the state space compared with that of a simple random walk. Finally, the\nauthors show that their method with options is eﬀective for exploration tasks with the goal\nof maximizing the cumulative rewards. Later work by Machado et al. (2018b) proposed an\nimproved version of eigenoption discovery, extending it to stochastic environments with non-\ntabular states. In particular, using the notion of successor representation Dayan (1993), in\ntheir proposed method the agent learns the non-linear representation of the states which in\nturn gives the diﬀusive information ﬂow (DIF) model, and subsequently, the eigenpurposes\nand eigenoptions are obtained.\nIn the recent years, Jinnai et al. (2019) introduced the method of covering options,\nwhere options are generated with the goal of minimizing cover time. Their proposed method\n17\nxxxx\nencourages the agent to visit less-explored regions of the state space by generating options for\nthose parts, without using the information obtained from extrinsic rewards. Their empirical\nevaluation in discrete sparse-reward domains present reduced learning time in comparison\nwith that of some of their predecessors. The method of deep covering options (Jinnai et al.,\n2020) extends covering options to large or continuous state spaces while minimizing the\nagent’s expected cover time in the state space. The authors have successfully shown the\nbehaviour of deep covering options in challenging sparse-reward tasks, including Pinball, as\nwell as some MuJoCo and Atari domains.\nIn a study by Hong et al. (2018), the authors apply a diversity-driven method to oﬀ- and\non-policy DRL algorithms and improve their performances in large state spaces with sparse\nor deceptive rewards via encouraging the agent to try new policies. In particular, the agent\nuses a distance measure to evaluate the novelty of π in comparison to the prior ones and\nsubsequently modiﬁes the loss function,\nLD = L −Eπ′∈Π′ \u0002\nαD\n\u0000π, π′\u0001\u0003\n,\n(19)\nwhere L denotes the loss function of the deep RL algorithm, α is a scaling factor, and D\nis a distance measure between the current policy π and the policy π′ sampled from a set of\nmost recent policies Π′. Equation 19, encourages the agent to try new policies and explore\nunvisited states without relying on the extrinsic rewards received from the environment.\nThe agent will consequently overcome the problem of getting stuck in local optima due\nto the presence of deceptive rewards or failing to learn tasks with sparse rewards or large\nstate spaces. The authors apply their proposed exploration method in 2D grid worlds with\ndeceptive or sparse rewards, Atari 2600 as well as MuJoCo, and show that it enhances the\nperformance of DRL algorithms through a more eﬀective exploration strategy.\n5. Randomized Action Selection\nSo far, we have introduced and discussed exploration methods that do not acquire informa-\ntion in the form of extrinsic rewards in the process of exploratory action selection. In this\nsection and the rest of the survey, we focus on the exploration techniques that make deci-\nsions using extrinsic rewards with or without other forms of information obtained from the\nlearned process. In this section, we speciﬁcally target the exploration methods that assign\naction selection probabilities to the admissible actions based on value functions/rewards or\nthe learned policies. The two groups of exploration methods are introduced in sections 5.1\nand 5.2, respectively.\n5.1 Value-Based Methods\nA simple way to deal with the exploration-exploitation trade-oﬀis to induce exploratory\nbehaviour via assigning action selection probabilities to the admissible actions based on the\nestimated value functions. In the early phase of learning, the agent should be able to try\ndiﬀerent actions in each state. Later in the intermediate learning phase, if the agent’s target\npolicy takes the control of the action selection process, it may lead to a partial visitation\nof the state space, and thus a sub-optimal policy and value function. To tackle this issue,\nthere are exploration approaches that select the stochastic actions based on the feedback\n18\nA Survey of Exploration Methods in Reinforcement Learning\nthey receive, in the form of value function or rewards, from the environment. Using these\nfeedback, they balance exploration and exploitation via deciding between acquiring more\nknowledge from the environment and maximizing the obtained rewards. In this section,\nsome of the exploration methods that perform action selection according to the above-\nmentioned criteria are discussed below. Examples of some value-based randomized action\nselection exploration methods are provided in Table 2.\nOne of the most important exploration methods in this category is the Softmax action\nselection method (Bridle, 1990). In this method, the greedy action at the current state is\nselected with the highest probability, while the other actions are given the probability of\nbeing selected according to their estimated values. The most commonly used formalism\nfor performing Softmax action selection is the Boltzmann distribution. The early use of\nBoltzmann distribution in exploration was by Watkins (1989); Lin (1992) and Barto et al.\n(1991). In the Boltzmann exploration approach, the value function Qt(s, a) for the current\nstate St = s and action at = a assigns the probability of selecting action a as,\nπt (a|s) =\neQt(s,a)/Tm\nPN\ni=1 eQt(s,ai)/Tm ,\n(20)\nwhere Tm > 0 is called the temperature and controls how frequent the agent will choose\nrandom actions as opposed to the best actions a⋆\nt = arg maxa Qt(s, a). If Tm decreases, the\nprobability of generating the action with the highest expected reward a⋆\nt increases, leading\nto a decrease in the probability of selecting other actions and hence a lower probability of\nexploring the environment. In the limit of zero temperature Tm →0, the agent uses the\ntarget policy to select greedy actions in order to maximize the obtained rewards. At very\nlarge temperatures Tm →∞, all of the actions have almost the same probability and the\naction selection process approaches random walk. Setting the value for the temperature T\nis not straightforward, but it is generally reduced during the experiments, leading to more\nexploitation over time.\nOne of the extended applications of the Softmax exploration method is in the multiobjec-\ntive reinforcement learning tasks (Vamplew et al., 2017), where scalar rewards are replaced\nwith vector-valued rewards. Each element in the vectors represent the reward corresponding\nto an objective. The action-value function Q(s, a) is consequently presented in the form\nof vector ¯Q(s, a). In order to use the vector representation of the value functions in the\nBoltzmann formalism, the vectors must be mapped to scalar values using a scalarization\nfunction f( ¯Q(s, a)) (Liu et al., 2015), which can have either linear (Vamplew et al., 2017;\nGuo et al., 2009; Perez et al., 2009) or non-linear (Gábor et al., 1998; Van Moﬀaert et al.,\n2013a,b) representations. The Boltzmann formalism is consequently written as,\nπ (a|s) =\nef( ¯Qt(s,a))/T\nPN\ni=1 ef( ¯Qt(s,ai))/T .\n(21)\nAnother extension of Softmax exploration is the Max-Boltzmann rule (Wiering, 1999),\nwhich is a combination of the ϵ-greedy or Max-random approach (explained in detail in\nsection 4.1) and the Boltzmann exploration method. In the Max-Boltzmann method, similar\nto the ϵ-greedy approach, the action that gives the maximum Q-value is chosen with the\nprobability 1−ϵ. With the probability ϵ, the Boltzmann distribution (equation 20) is used for\n19\nxxxx\nApproach\nRemarks\nBridle (1990)\nSoftmax\nWatkins (1989)\nEarly use of Boltzmann distribution\nLin (1992)\nEarly use of Boltzmann distribution\nBarto et al. (1991)\nEarly use of Boltzmann distribution\nVamplew et al. (2017)\nVectorization of rewards in multi-objective\ntasks\nWiering (1999)\nMax-Boltzmann (ϵ-greedy + Boltzmann)\nTokic (2010)\nValue-Diﬀerence\nBased\nExploration\n(VDBE)\nTokic and Palm (2011)\nExtension of VDBE (VDBE + Max-\nBoltzmann)\nTijsma et al. (2016)\nControls and increases the probability of\ngreedy action selection with time\nEven-Dar and Mansour (2002)\nDerandomization of ϵ-greedy\nTable 2: Examples of value-based randomized action selection exploration methods.\naction selection. A drawback for the Max-Boltzmann exploration method compared to the\ntwo approaches, Max-Random and Boltzmann, is the need for tuning two hyperparameters\nT and ϵ instead of one. However, the Max-Boltzmann method has been shown to reduce\nthe weight of exploration in comparison with exploitation, and thus avoid over-exploration.\nAnother exploration technique in this category is the incremental Q-learning algorithm\n(Even-Dar and Mansour, 2002), which is an extension of the ϵ-greedy method (discussed in\nsection 4). In particular, in their proposed method, Even-Dar and Mansour (2002) deran-\ndomize the ϵ-greedy method by adding a promotion term to the estimated Q-value for each\nstate-action pair. If action a is not taken in state s at time t, the promotion term of that\nspeciﬁc state-action pair at time t+1 is increased by a value called promotion function, and\nzeroed otherwise. The promotion function plays the role of a decreasing ϵ in the ϵ-greedy\napproach, and decreases with the number of times action a′ ̸= a has been taken in state s.\nConsequently, the values of actions that have not been taken are promoted and the fraction\nof time the sub-optimal actions are chosen decreases with time and vanishes in the limit.\nAnother extension of the ϵ-greedy method is the Value-Diﬀerence Based Exploration\n(VDBE) (Tokic, 2010), where the ϵ parameter in the ϵ-greedy approach is replaced with a\nstate-dependent exploration probability ϵt(s) instead of being hand-tuned. In VDBE, the\ninitialization of the exploration probability ϵt(s) is done with ϵo(s) = 1 for all states. At each\ntime-step, the TD-error is computed, which serves as a measure for the agent’s uncertainty.\nA larger TD-error for a state corresponds to a larger uncertainty, which consequently triggers\na higher chance of exploration by assigning a larger value to the exploration rate ϵt(s) for\nthat speciﬁc state.\nAlthough Tokic (2010) assesses VDBE in multi-armed bandit problems, he argues that\nthe method of VDBE is also applicable in multi-state MDPs. In another study, Tokic and\nPalm (2011) introduce an extension of VDBE, namely VDBE-Softmax, for solving rein-\nforcement learning problems with multiple states. The VDBE-Softmax method combines\n20\nA Survey of Exploration Methods in Reinforcement Learning\nVDBE (introduced by Tokic (2010)) with Max-Boltzmann exploration (proposed by Wiering\n(1999)) in order to overcome the shortcomings of the two former approaches. A disadvan-\ntage of VDBE, as stated by Tokic and Palm (2011), is that it does not discriminate between\nexploratory actions, which leads to equal probability of selecting the actions that yield high\nand low Q-values. Another disadvantage of VDBE is its divergence in the cases where oscil-\nlations exist in the value functions caused by stochastic rewards or function approximators.\nIn the VDBE-Softmax approach, with probability ϵt(s), the agent selects the exploratory\nactions using equation 20, and chooses the argmax of Q-value (greedy action) with the\nprobability 1 −ϵt(s). The authors show that in general, their proposed variation of VDBE\nperforms better than the exploration methods ϵ-greedy, Softmax and pure VDBE in the\nenvironments with deterministic rewards (cliﬀ-walking problem and bandit-world tasks) as\nwell as the ones with stochastic rewards (bandit-world tasks).\nSimilar to the method of VDBE (Tokic, 2010), there exist other exploration strategies,\nwhich were originally proposed for and applied in the bandit problems, but were later per-\nformed in multi-state MDPs as well. One of these approaches is called Pursuit (Thathachar\nand Sastry, 1984), which maintains the probability of selecting greedy action as well as the\nvalue of diﬀerent actions at each time step. As described in Sutton and Barto (1998b), in\nk-armed bandit problems, Pursuit algorithms initialize the probability of choosing an arm\nwith pt=0(a) = 1/k for all actions a = 1, . . . , k. At each time step t, the probability of\nselecting action a is calculated as\npt+1(a) =\n(\npt(a) + α (1 −pt(a))\nif a = arg maxi Qt(i)\npt(a) + α (0 −pt(a))\nOtherwise\n(22)\nwhere α ∈(0, 1) is the learning rate. According to equation 22, the probability of selecting\nthe greedy action increases with time, leading to fewer exploratory moves and more exploita-\ntion. The equivalence of equation 22 for the case of multi-state MDPs is given as follows\n(Tijsma et al., 2016)\nπt+1(st, at) =\n(\nπt(st, at) + α (1 −πt(st, at))\nif at = a⋆\nt\nπt(st, at) + α (0 −πt(st, at))\nOtherwise\n(23)\nwhere a⋆\nt = arg maxa Q(st, a). Tijsma et al. (2016) perform pursuit as well as other ex-\nploration techniques including Softmax and ϵ-greedy in random discrete stochastic mazes\nwith one optimal goal and two sub-optimal goal states. Their results show that Softmax\noutperforms the other exploration strategies and that the ϵ-greedy method has the worst\nperformance of all.\n5.2 Policy-Search Based Methods\nAfter having discussed methods that implicitly represent a policy using value function, we\nturn our attention to policy search methods. Policy search methods explicitly represent a\npolicy, instead of, or in addition to, a value function. In the latter case, these methods are\nmore speciﬁcally referred to as actor-critic methods. Most policy search methods learn a\nstochastic policy. The stochasticity in the policy is usually also the main driver of exploration\nin such methods. The diﬀerent ways in which such perturbations can be applied will be the\n21\nxxxx\nfocus of the next several sections1: after this short introduction, Section 5.2.1 will introduce\nthe organizational principle for the section, with Sections 5.2.2 and 5.2.3 explaining the\nindividual methods in detail.\nMany policy search methods belong to the policy gradient family. These methods aim\nto update the policy in the direction of the gradient of the expected return ∇Jπ. A basic\nway to do so is by calculating a ﬁnite-diﬀerence approximation of the gradient.\nIn this\napproach, rollouts are performed for one or more perturbations of the original parameter\nvector, which are then used to estimate the gradient. When the system is non-deterministic,\nthese estimates are extremely noisy, although better estimates can be obtained in simulation\nwhen the stochasticity of the environment is controlled by ﬁxing the sequence of pseudo-\nrandom numbers (Ng and Jordan, 2000).\nMore sophisticated approaches are based on the log-ratio policy gradient (Williams,\n1992). The log-ratio policy gradient relies on stochastic policies, and exploits the knowl-\nedge of the policy’s score function (gradient of the log-likelihood). Stochastic policies for\ncontinuous actions based on the Gaussian distribution (Williams, 1992) are still frequently\nused (Deisenroth et al., 2013).\nFor discrete action spaces, a Gibbs distributions with a\nlearned energy function (Sutton et al., 2000) can be used instead.\nThe initialization of the exploration policy can be freely chosen.\nIn some policy ar-\nchitectures, the amount of exploration is ﬁxed to some constant or decreased according to\na set schedule. In other architectures, the amount of exploration is controlled by learned\nparameters, possibly separate from other parameters (such as those controlling the ‘mean\naction’ for any given state). Policy search methods typically maximize the expected return,\nand thus probability mass tends to slowly be shifted towards a more greedy policy (usually\nresulting in a decreasing amount of exploration). These and more advanced strategies will\nbe discussed in more detail in Sec. 5.2.4.\nThe discussed approaches diﬀer in an important aspect: while in ﬁnite-diﬀerence methods\n(Ng and Jordan, 2000) the parameters of the policy are perturbed, the method proposed\nby Williams (1992) selects the actions stochastically. Where perturbations are applied at\nthe level of parameters, they often aﬀect an entire episode (episode-based perturbations). In\ncontrast, classically action-space perturbations are often only applied for a single time step\n(independent perturbations).\nIn this section, we will focus on research in the area of policy search methods that intro-\nduce new exploration strategies or that explicitly evaluate the eﬀects of diﬀerent exploration\nstrategies. We will focus on such policy search methods that are trained on a single task\nand where the policy has its own representation. Policies that are deﬁned only in terms of\nvalue function are covered in Section 5.1. Policies explicitly optimizing over a distribution of\ntasks are covered with Bayesian and meta-learning approaches in Section 7. An overview of\nthe methods we will cover in this section is given in Table 3. The table groups the method\nby type and coherence of perturbation that, like Deisenroth et al. (2013), we consider to be\nkey characteristics of exploration strategies in policy search. The following subsection will\ngive a more detailed explanation of these characteristics.\n1. A more general overview of the properties of policy search methods is given in Deisenroth et al. (2013).\n22\nA Survey of Exploration Methods in Reinforcement Learning\nApproach\nPerturbed space\nTemporal\ncoherence∗\nRemarks\nBarto et al. (1983)\nAction-space\nIndependent\n-\nGullapalli (1990)\nAction-space\nIndependent\n-\nWilliams (1992)\nAction-space\nIndependent\n-\nMorimoto and Doya (2001)\nAction-space\nCorrelated\nMulti-modal\n(hierarchy)\nNachum et al. (2019)\nAction-space\nCorrelated\n-\nWawrzynski (2015)\nAction-space\nCorrelated\n-\nLillicrap et al. (2016)\nAction-space\nCorrelated\n-\nHaarnoja et al. (2017)\nAction-space\nIndependent\nMulti-modal\nXu et al. (2018)\nAction-space\nIndependent\n-\nKohl and Stone (2004)\nParameter-space\nEpisode-based\n-\nSehnke et al. (2010)\nParameter-space\nEpisode-based\n-\nRückstiess et al. (2010)\nParameter-space\nEpisode-based\n-\n\"\nAction-space\nEpisode-based\n-\nTheodorou et al. (2010)\nParameter-space\nEpisode-based\n-\nStulp and Sigaud (2012)\nParameter-space\nEpisode-based\nCorrelated\nparameters\nSalimans et al. (2017)\nParameter-space\nEpisode-based\n-\nConti et al. (2018)\nParameter-space\nEpisode-based\n-\nvan Hoof et al. (2017)\nParameter-space†\nCorrelated†\n-\nPlappert et al. (2018)\nParameter-space†\nEpisode-based†\n-\nFortunato et al. (2018)\nParameter-space†\nEpisode-based†\n-\nMahajan et al. (2019)\nParameter-space\nEpisode-based\nMulti-agent\nTable 3: Diﬀerent exploration approaches proposed in the context of policy search algo-\nrithms. The ﬁrst section of the table lists methods that mainly perturb the policy in the\naction space, these methods will be discussed in Sec. 5.2.2. The second section lists methods\nthat mainly perturb the policy in the parameter space, that will be discussed in Sec. 5.2.3.\nWithin these two broad categories, papers are ordered roughly chronologically, although\npapers within a similar line of work are kept together. Multiple entries for the same paper\nrefer to diﬀerent variants.\n∗Denotes whether perturbations are applied independently at each timestep, don’t change\nat all throughout an episode, or have an intermediate correlation structure.\nDetails in\nSec. 5.2.1.\n† These methods have additional step-based action-space noise for numeric reasons or to\nensure a diﬀerentiable objective.\n23\nxxxx\n5.2.1 Perturbed space and coherence\nIn policy based methods, exploratory behavior is usually obtained by applying random\nperturbations.\nOne of the main characteristics that diﬀerentiate exploration methods is\nwhere those perturbations are applied. Within policy gradient techniques, there are two\nmain candidates: either the actions or the parameters are perturbed (although we will\ndiscuss some approaches beyond these two shortly). These possibilities reﬂect two views on\nthe learning problem. On the one hand, the actions that are executed on the system are what\nactually aﬀects the reward and the next state, no matter which parameter vector generated\nthe actions. From this perspective, it is more straightforward to start with ﬁnding good\nactions, and subsequently ﬁnd a parametric policy that can generate them. From another\nperspective, parametrized policies have limited capacity, and the resulting inductive bias\nmight mean that the true optimal policy is excluded from the set of representable policies.\nWe are then looking for parameters with which the overall policy behavior is best across\nall states. Also, if the structure of the policy is chosen to reﬂect prior information about\nthe structure of the solution (e.g. policies linear in hand-picked features or policies with a\nhierarchical structure), perturbing the policy parameters ensure that explorative behavior\nfollows the same structure.\nThe space in which explorative behaviors are applied is usually closely linked to the coher-\nence of behavior. Coherence here refers to the question of whether (and how closely) pertur-\nbations in subsequent time steps depend on one another. Exploration with a low coherence\n(e.g., perturbations chosen independently at every time step) has the advantage that many\ndiﬀerent strategies might be tried within a single episode. On the other hand, exploration\nwith a high coherence (e.g., perturbing the policy at the beginning of each episode only) has\nthe advantage that the long-term eﬀect of following a certain policy can be evaluated (Sehnke\net al., 2010). Whereas independent perturbations can result in ineﬃcient random walk be-\nhavior, following a perturbed policy consistently could result in reaching a greater variety of\nstates (Machado et al., 2017). Intermediate strategies between the extremes of completely\nidentical perturbation across an entire episode and completely independent perturbation per\ntime step are also possible (Morimoto and Doya, 2001; Wawrzynski, 2015), and can be used\nto compromise between the advantages of the more extreme strategies.\nMost exploration approaches which perturb the policy in action space have focused\non independent perturbations in each time step, as applying the same perturbation at all\ntime steps would not cover the space of possible policies well (see Table 3). In contrast,\nparameter-space exploration tends to go together with episode-based exploration, because\ncertain parameters might only inﬂuence behavior in certain states, so such a perturbation\nhas to be evaluated across multiple states to give a good indication of its merit (Rückstiess\net al., 2010). However, exceptions to this pattern exist, especially in the area of exploration\nstrategies of intermediate coherence.\nIn the following paragraphs, papers presenting or\nanalyzing speciﬁc exploration strategies will be discussed in more detail. We will start by\ndiscussing exploration strategies that apply explorative perturbations in the action space,\nbefore turning our attention to strategies that perturb the policy parameters. Finally, we\nwill discuss the issue of what distribution these perturbations are sampled from.\n24\nA Survey of Exploration Methods in Reinforcement Learning\n5.2.2 Action-space perturbing strategies\nUsing stochastic policies to generate action-space perturbations has been used at least since\nthe early 80’s. Barto et al. (1983) proposed an early actor critic-type algorithm, that per-\nturbed the output of a simple neural network before applying the thresholding activation\nfunction. This resulted in Bernouilli-distributed outputs (Williams, 1992).\nSubsequent work also investigated the use of Gaussian noise in continuous action do-\nmains. Gullapalli (1990) introduced speciﬁc learning rules for the mean and standard devi-\nation of Gaussian policies, which was introduced to learn policies with continuous actions.\nWilliams (1992) provided a more general algorithm that provides an update rule for a general\nclass of policy functions including stochastic and deterministic operations. Williams (1992)\nnotes that because a Gaussian distribution has separate parameters controlling location and\nscale, such random units have the potential to control both the degree of exploration as well\nas where to explore. These early approaches all perturbed the action chosen at each time\nstep independently.\nWhile independent perturbation is still a popular method, attention has also turned to\nstrategies that attempt to correlate behavior in subsequent time steps. An interesting early\nexample can be found in Morimoto and Doya (2001). That paper describes a hierarchical\npolicy, where an upper-level policy sets a sub-goal which a lower-level policy then tries to\nachieve. Exploration on the upper-level by itself causes some consistency in the exploration\nbehavior, as a perturbation in the goal-picking strategy will consistently perturb the system’s\nbehavior until the subgoal is reached. Additionally, the lower-level learner itself applies a\nlow-pass ﬁlter on the action perturbations. As a result, similar perturbations will typically\nbe applied on subsequent time steps. The authors applied this algorithm to learn a stand-up\nbehavior for a real robot.2\nNachum et al. (2019) studied hierachical methods in more detail, among others focusing\non their exploration behavior. In their experiments, they investigate two simple exploration\nheuristics that share certain properties with hierarchical policies. The ﬁrst heuristic, Explore\n& Exploit, randomly sets ’goals’ for a separately trained explore policies analogous to the\nhigh-level actions in a goal-conditioned hierarchical method. Goals stay active for one or\nmultiple time steps. Their second method, Switching Ensembles, trains several separate\nnetworks that individually attempt to optimize rewards. During training, the active policy\nis periodically switched, and when the policies are diﬀerent, this switching leads again to\nexploration behavior that is coherent over several time steps. Nachum et al. ﬁnd that both\nmethods beneﬁt from temporal coherence, and their results suggest that setting goals in a\nmeaningful space might additionally beneﬁt exploration.\nSimilar to the approach by Morimoto and Doya (2001), Wawrzynski (2015) investigated\nperforming explorative perturbations on physical robots.\nAs the authors note, applying\nperturbations independently at each time step (e.g., independent draws from a stochastic\npolicy) causes jerkiness in the trajectories, which damages the robot. As an alternative,\nthe paper proposes to apply an auto correlated noise signal. This signal is generated in a\nslightly diﬀerent way than the previously discussed approach, as it is generated by summing\n2. Other work has speciﬁcally evaluated the potential of lower-level sub-policies to decrease the diﬀusion\ntime during the exploration process in the context of pure exploration (Machado et al., 2017). This\npaper is explained in more detail in Sec. 4.2.\n25\nxxxx\nup independent perturbations from the last M time steps. The authors explicitly evaluated\nthe suggested strategy on various continuous robot control problems. Their experiments\nsuggest that the proposed strategy leads to equivalent asymptotic performance (although\nsometimes a slower learning speed), while causing less stress to the robot’s joints by reducing\nthe jerkiness of trajectories.\nCorrelating the perturbations over several time steps, however, complicates the calcula-\ntion of log-ratio policy gradients, as the policy is no longer Markov (as the selected action is\nno longer independent of earlier events given the state). Lillicrap et al. (2016) instead apply\nauto-correlated noise for their deep deterministic policy gradient (DDPG). Since DDPG is\nan oﬀ-policy algorithm, the generating distribution of the behavior policy does not need\nto be known, simplifying the use of various kinds of auto-correlated noise. They proposed\ngenerating this noise using an Ornstein-Uhlenbeck process, which generates noise with the\nsame properties as that used by Morimoto and Doya (2001). This paper did not focus on\nreal-robot experiments, thus motor strain was not a major concern. They did, however,\ndetermine that auto-correlated noise does help learn in (simulated) ‘physical environments\nthat have momentum’, in other words, environments where a sequence of similar actions\nneed to be performed to cause appreciable movement in high-inertia objects.\nMore recently, attention seems to have swung back to independent action perturbations,\nwith recent work attempting to make the distribution from which actions are drawn more\nexpressive, or the resulting explorative actions more informative. While classically, simple\nparametric distributions have been used as stochastic policies, these typically cannot rep-\nresent multi-modal policies. Haarnoja et al. (2017) point out that it is useful to maintain\nprobability mass on all good policies during the learning process, even if this results in a\nmulti-modal distribution. In particular, a seemingly slightly-suboptimal mode might in a\nlater stage of learning be discovered to actually be optimal, which would be hard to uncover\nif this mode was discarded earlier in the learning process. The authors deﬁne the exploration\npolicy as maximizing an objective composed of a reward term and an entropy term. The\nsolution to this maximization problem is an energy-based policy. As one cannot generally\nsample from such distributions, an approximating neural-network policy is ﬁt to it instead.\nThe authors show that with certain (initially) multi-modal reward distributions the method\noutperforms exploration using single-modal exploration policies. They also show empirically\nthat a multi-modal policy learned on an initial task can provide a useful bias for exploring\nmore reﬁned tasks later.\nAlthough maintaining a high entropy can be a useful strategy to obtain more informative\ndata, it might be even more eﬀective to directly maximize the amount of improvement to a\ntarget policy caused by data gathered using the exploratory behavior policy. This was the\napproach proposed by Xu et al. (2018), who study the optimization of the behavior policy in\noﬀ-policy reinforcement learning methods, where the exploration policy can be fundamen-\ntally diﬀerent from the target policy. The authors’ insight is that good exploration policies\nmight indeed be quite diﬀerent from good target policies, and thus might not be centered\non the current target policy, but instead have a separate parametrization. While the target\npolicy is adapted in an oﬀ-policy manner in the direction of maximum reward, the behavior\npolicy is separately updated by an on-policy algorithm towards greater improvements to the\ntarget policy. This is achieved by using an estimate of the improvement of the target policy\nas the reward of a ‘meta-MDP’. Experiments show that learned variance, and even more\n26\nA Survey of Exploration Methods in Reinforcement Learning\nso a learned mean function, results in faster learning and better average rewards compared\nto conventional exploration strategies centered on the target policy. The authors attribute\nthis performance gain to more diversity in the exploration samples leading to more global\nexploration.\n5.2.3 Parameter-space perturbing strategies\nInstead of using action-space perturbation, one might directly perturb the parameters of\nthe policy. Especially where the parametrization of the policy can be restricted due to prior\nknowledge about the problem, it might be advantageous to do so. For example, if we know\nor assume that the optimal action will be directly proportional to the deviation from a set\npoint, it is not informative to perturb the policy at the set point, because the policy will\nchoose an action of 0 at the set-point regardless of the parameter value. This information\nis implicitly taken into account if parameters, rather than actions, are perturbed.\nPerturbing parameters rather than actions also ensure that any explorative action can\nalso, in fact, be reproduced by some policy in hypothesis space (Deisenroth et al., 2013).\nReferring back to the previous example, a non-zero action perturbation at the set-point, for\nexample, would not be reproducible by any of the considered controllers.\nPerturbing parameters also works very well together with temporally coherent explo-\nration. A parameter vector might simply be perturbed only at the beginning of an episode,\nand then kept constant for the rest of the episode.\nContrast this to action-perturbing\nschemes, where keeping the action perturbation constant for a whole episode in general\nwould not yield behavior that covers the state-action space well.\nThe most straightforward way to ﬁnd out what parameter perturbations improve a policy\nis to treat the entire interaction between the policy parameters and environment as a black-\nbox system and calculate a ﬁnite-diﬀerence estimate of the policy gradient, keeping the\npolicy perturbation ﬁxed during the episode.\nAn example of this approach is given by\nKohl and Stone (2004). For each policy roll-out, each policy parameter randomly chosen\nto be either left as is or to be perturbed by a adding a small positive or negative constant.\nAfter obtaining a set of such roll-outs, the policy gradient is then estimated using a ﬁnite-\ndiﬀerence calculation. This method was demonstrated to able to optimize walking behaviors\non four-legged robots better than earlier hand-tuned or learned gaits.\nA potential problem with this approach is that stochasticity (from the policy or envi-\nronmental transitions) can make gradient estimates extremely high-variance. In simulation,\nwhere stochasticity can be controlled, this can be addressed by using common random num-\nbers, as was proposed by Ng and Jordan (2000) in their PEGASUS algorithm to learn\npolicies for gridworld problems and a bicycle simulator.\nRatio-likelihood policy gradient estimators exploit the knowledge of the parametric form\nof the policy to calculate more informed estimates of the policy gradient. Sehnke et al.\n(2010) proposed a stochastic policy gradient with parameter-based exploration by posit-\ning a parameterized distribution π(θ|ρ) over the parameters θ of a (deterministic) low-level\npolicy, and learning the hyper-parameters ρ. Their experiments showed that the result-\ning parameter-perturbing, episode-based exploration strategy outperformed conventional\naction-perturbing strategies on several simulated dynamical systems tasks, including robotic\nlocomotion and manipulation tasks. Rückstiess et al. (2010) extended the idea of parameter-\n27\nxxxx\nbased exploration to several other policy-search algorithms, including a new method called\nstate-dependent exploration. In that approach, perturbations are deﬁned in the action-space,\nbut are generated based on an ‘exploration function’ that is a deterministic function of a\nrandomly generated vector and the current state. The authors show that state-dependent\nexploration is equal to parameter-based exploration in the special case of linear policies, and\nargue that in other cases it combines some of the advantages of parameter-based and action-\nbased exploration. The resulting data was then used to perform REINFORCE updates. As\nthese updates do not fully account for the dependency between actions, they might thus\nhave an increased variance.\nTheodorou et al. (2010) proposed ‘per-basis’ exploration, which is a variant parameter-\nbased exploration scheme where the perturbation is only applied to the parameter corre-\nsponding to the basis function with the highest activation and kept constant as long as\nthat basis function had the highest activation. Theodorou et al. (2010) noted that they\nemperically observed this trick improved the learning speed.\nThe eﬀect of step-based (independent) versus correlated or episode-based exploration\nwas further studied by Stulp and Sigaud (2012). They investigated connections between\nCMA-ES (Covariance matrix adaptation evolutionary strategies) from the stochastic search\nliterature (Hansen and Ostermeier, 2001) and PI2, a reinforcement learning algorithm with\nroots in the control community (Theodorou et al., 2010). Stulp and Sigaud (2012) found\nthat episode-based exploration indeed outperformed per time-step exploration on a simu-\nlated reaching task. Furthermore, it also outperformed per-basis exploration proposed by\nTheodorou et al. (2010).\nSalimans et al. (2017) applied the idea of episode-level log-ratio policy gradients (as used\nin earlier work by e.g. Sehnke et al. 2010) to complex neural network policies. They proposed\nthe use of virtual batch normalization to increase the methods sensitivity to small initial\ndiﬀerence. The authors connect this method to approaches from the evolutionary strategies\ncommunity (e.g.\nKoutnik et al. 2010).\nSalimans et al. (2017) found their approach to\ncompare favorably to Trust Region Policy Optimization (TRPO, Schulman et al. 2015), an\naction-space perturbing method, on simulated robotic tasks. Furthermore, the approach\nperformed competitively with the Asynchronous Advantage Actor Critic (A3C, Mnih et al.\n2016), while training an order of magnitude faster.\nConti et al. (2018) combined similar ideas from the evolutionary strategies community\nwith directed exploration strategies. This combination results in two new hybrid approaches,\nwhere evolutionary strategies are used to maximize a scalarization of the original reward-\nmaximization objective with a term encoding novelty and diversity. A heuristic strategy for\nadapting the scalarization constant is proposed. The proposed approach is evaluated on a\nsimulated locomotion task and a benchmark of Atari games, where it outperforms a regular\nevolutionary strategy approach and performs competitively with the deep RL approach by\nFortunato et al. (2018; described below). In this experiment, the evolutionary strategies\nwere given more frames, but still ran faster due to better parallelizability.\nVan Hoof et al. (2017) apply auto-correlated noise in parameter space. This noise is dis-\ntributed similarly to that proposed by Morimoto and Doya (2001) and Lillicrap et al. (2016),\nbut applied to the parameters rather than the actions. As a result, intermediate trade-oﬀs\nbetween independent and episode-based perturbations are obtained. The latent parameter\nvector violates the independence assumption of step-based log-ratio policy gradients meth-\n28\nA Survey of Exploration Methods in Reinforcement Learning\nods, which is resolved by explicitly using the joint log-probability of the entire sequence\nof actions. Expressing this log-probability in closed form requires the use of a restricted\npolicy class (e.g., linear policies). Note that the auto-correlated action-space noise used by\nLillicrap et al. (2016) was applied in an oﬀ-policy setting, avoiding this problem. Auto-\ncorrelated parameter-space noise was compared to several baselines, including action-space\nperturbations as well as episode-based and independent parameter-space perturbations. On\nvarious simulated and real continuous control tasks, intermediate trade-oﬀs between inde-\npendent and episode-based perturbation led to faster learning and a way to control state\nspace coverage and motor strain.\nTwo methods (Plappert et al., 2018; Fortunato et al., 2018) independently proposed\nstrategies to use parameter-based perturbations for reinforcement learning approaches based\non deep neural networks. Both of these works consider both value-networks and policy-based\napproaches. Here, we will discuss the policy-based variants. Both of the approaches also\nbuild on the principle of episode-based perturbation of the parameter space, but better ex-\nploit the temporal structure of roll-outs than previous studies (Sehnke et al., 2010; Salimans\net al., 2017) that largely ignored it.\nBy making the perturbations ﬁxed over an entire episode and using the reparametrization\ntrick, these methods allow the use of a wide range of policies, but possibly increase variance.\nSubsequent actions are now conditioned on a shared sample from the noise distribution, and\ntheir conditional independence means the trajectory likelihood can be factored as usual. As\na result, this methods are applicable to non-linear neural network policies. Plappert et al.\n(2018) used a pre-determined amount of noise that was decreased over time according to a\npre-set schedule. On the other hand, Fortunato et al. (2018) learned the magnitude of the\nparameter noise together with the policy mean\nApplying this principle in an oﬀ-policy setting is relatively easy: since any behavior policy\ncould be used to generate data, this could easily be the current deterministic target policy\nwith noise added to the parameters. Plappert et al. (2018) modiﬁed the Deep Deterministic\nPolicy Gradient (DDPG, Lillicrap et al. 2016) algorithm in this manner. With on-policy\nalgorithms, this is more challenging. On-policy algorithms with per time-step updates tend\nto also require stochastic action selection in each time step. For the on-policy Asynchronous\nAdvantage Actor Critic (A3C, Mnih et al. 2016) in Fortunato et al. (2018); and for Trust\nRegion Policy Optimization (TRPO, Schulman et al. 2015) in Plappert et al. (2018); this\nproblem was resolved by using a combination of parameter perturbations and stochastic\naction selection.\nThe NoisyNet-A3C method by Fortunato et al. (2018) compared favorably to the base-\nline A3C variant on a majority of 57 Atari games. The parameter-exploring variants of\nDDPG and TRPO proposed by Plappert et al. (2018) compared favorably to baselines with\n(correlated or uncorrelated) action-space noise on several simulated robot environment. 3\nIncoherent behavior causes particular problems in multi-agent learning, as pointed out\nby Mahajan et al. (2019). In cooperative decentralized execution scenarios, uncoordinated\nexploration between the agent can lead to state visitation frequencies for which the factorized\n3. Colas et al. (2018), whose exploration method is discussed in Sec. 4, verify the performance diﬀerence\nbetween action- and parameter space noise for DDPG, and compare their methods to the exploration\nmethods by Lillicrap et al. (2016) and Plappert et al. (2018). Like these methods, their proposal beneﬁts\nfrom the ﬂexibility of DDPG as oﬀ-policy method to work with data from any behavior policy.\n29\nxxxx\nq-function approximation is catastrophically bad. Such failure traps the agents in suboptimal\nbehavior. Mahajan et al. (2019) remedy the situation by making exploration at train time\ncoherent across both time and the individual agents, by conditioning on a common latent\nvariable generated by a high-level policy4. A separate variational network is used to estimate\na mutual information term which avoids collapse of the high-level policy on constant low-\nlevel behavior. The proposed approach is compared on both a toy task as well as challenging\nscenarios from the StarCraft Multi-Agent Challenge (Samvelyan et al., 2019).\n5.2.4 The distribution of perturbations\nSeparate from the issue of how perturbations are applied, is the issue of what distribution\nthese perturbations are sampled from. Often, these are Gaussian distributions centered on\nthe policy mean, leaving the choice of standard deviation open. Other parametric policies\nmay have diﬀerent parameters controlling the amount of exploration. Sometimes, such pa-\nrameters are treated as additional hyperparameters (Silver et al., 2014) or governed by a\nspeciﬁc heuristic (Gullapalli, 1990). More commonly, they can also be adapted like the other\nparameters during learning. The most straightforward way is to adapt the parameters con-\ntrolling this standard deviation using the same policy gradient (Williams, 1992). Without\nadditional regularization, policy gradient methods will tend to reduce the uncertainty, lead-\ning to a loss of exploration that is hard to control and might result in premature convergence\nto a suboptimal solution (Williams and Peng, 1991; Peters et al., 2010).\nTo address this problem, several approaches of them have been proposed.\nMany of\nthem involve regularization using the entropy of the policy or the relative entropy from a\nreference policy. These entropies can either be constrained or added as regularization term\nto the optimization objective. A uniﬁed view on regularized MDPs is presented by Neu\net al. (2017); Geist et al. (2019).\nAs an example, Bagnell and Schneider (2003) studied natural policy gradients through\nthe lens of limiting the divergence between successive policies. They found the natural gra-\ndient can be derived from a bound on the approximate Kullback-Leibler divergence between\ntrajectory distributions. Dynamic policy programming (Azar et al., 2011) uses a similar\nformulation but instead uses the relative entropy as penalty term. Schulman et al. (2015)\nprovides a more exact method, focused on limiting the equivalent expected Kullback-Leibler\ndivergence between policies. They connects this update to the idea of trust region optimiza-\ntion, and provides several steps to make scale this type of network to deep reinforcement\nlearning architectures with tens of thousands of parameters.\nThe ‘relative entropy policy search’ method proposed by Peters et al. (2010) bounds the\nKL divergence in the joint state-action distribution to avoid a loss of exploration during train-\ning. Their bound on the joint KL is a stricter condition than a bound on the expected KL\ndivergence of state-action conditionals which has theoretically attractive properties (Zimin\nand Neu, 2013). However, the method is more complex and seems harder to scale to deep\narchitectures (Duan et al., 2016a). Bas-Serrano et al. (2020) propose a method building on\nrelative entropy policy search, that combines regularization terms on the joint- and expected\n4. Note that although the perturbed parameters are of a value network, this hybrid value- and policy\nbased approach was covered here as the novelty in the exploration strategy stems from the high-level\nparametrized policy.\n30\nA Survey of Exploration Methods in Reinforcement Learning\nKL. A large beneﬁt is that the resulting algorithm can be faithfully implemented in deep\nRL frameworks, and thus does need further approximations of the policy.\nAn alternative used early on was to add the derivative of the policy entropy to the policy\nupdates. Williams and Peng (1991) found this strategy to improve exploration open on a\ntoy example and several optimization problems. This strategy has also proved fruitful in\npractice in deep learning approaches: Mnih et al. (2016) applied this strategy and informally\nobserved it lead to improved exploration by discouraging premature convergence. Such an\nentropy term can be seen as a special case of the expected relative entropy objective, with\nthe reference policy being the maximum entropy distribution. Neu et al. (2017) studied such\nregularized objectives in detail, and conclude that a entropy penalty based on samples from\nthe previous policy distribution distribution can lead to optimization problems.\nThe entropy regularization methods in the previous paragraph only took the instanta-\nneous policy entropy into account. Haarnoja et al. (2017, 2018) instead propose two methods\nthat explicitly encourages policies that reaching high-entropy states in the future. They note\ntheir method improves exploration by acquiring diverse behaviors.\n6. Bonus-Based/Optimism-Based Exploration\nA popular category of exploration methods commonly used in domains with weak or sparse\nexplicit reward structure is the bonus-based methods. In this category, extrinsic reward\nr(s, a) is augmented with a bonus term that is often demonstrated as a form of intrinsic\nreward (Oudeyer et al., 2007) to encourage better exploration. The term bonus was ﬁrst\nintroduced in the early nineties by Sutton (1990) in the tabular setting. Algorithms that\nadopt the bonus-based exploration approach employ diﬀerent bonus calculation techniques\nto encourage the choice of action that leads to a higher level of uncertainty and consequently,\nnovel or informative states.\nIn an environment with underlying MDP M, upon selection of the state-action pair\n(s, a), the explicit reward r(s, a) is observed by the RL agent, and the bonus term B(s, a) is\ncomputed by the RL agent. Thus, the total reward obtained by the agent by taking action\na at state s is deﬁned as,\nr+(s, a) := r(s, a) ⊕B(s, a),\n(24)\nwhere the operator ⊕denotes the aggregation between the two sources of environment\n(extrinsic) reward and bonus term.\nIn designing bonus-based exploration algorithms, two main questions arise:\n1. How should the bonus function B(s, a) be speciﬁed to yield an eﬀective exploration\nbehavior?\n2. How should we combine the two separately acquired sources of information, exploration\nbonus denoted by B(s, a) and extrinsic reward denoted by r(s, a)?\nExploration methods described in this section adopt three diﬀerent approaches to com-\npute the additive bonus term, namely optimism-based, count-based, and prediction error-\nbased. In the optimism-based methods, the bonus term is implicitly embedded in the value\nfunction initial value estimates. In the count-based methods, where the novel state-action\n31\nxxxx\npairs are the ones that are less frequently visited, the bonus term is calculated based on\nsome form of state-action visitation counts. Another way of calculating the bonus term is\nthrough a prediction model of environment dynamics and measuring the induced prediction\nerror. The interplay between the optimism-based approach and the count-based method in\ndesigning the bonus term is subtle, and it is often hard to distinguish between these two\nparadigms. Thus, in the optimism section, we only discuss the optimism-based approaches\nthat do not explicitly use any notion of count in their choice of bonus. In the count-based\nsection, we only discuss the methods that explicitly employ a notion of count in their bonus\ndeﬁnitions. In this survey, we choose to interpret the optimistic initialization heuristic as a\ntype of bonus assigned to unseen state-action pairs to encourage exploration.\nAll these three categories are divided into two sub-categories of tabular and function ap-\nproximation methods. In the tabular setting, the state and action spaces are small enough\nthat the value function estimate can be presented as a lookup table. In the function approx-\nimation setting, on the other hand, due to the inﬁnite or large nature of state and action\npairs, the value function is represented as a parameterized function rather than a table. At\nthe beginning of each section, we provide a table that summarises the papers discussed.\n6.1 Optimism-based methods\nOptimistic exploration is a category of exploration methods that adopt the philosophy of\nOptimism in the Face of Uncertainty (OFU) principle, which was ﬁrst introduced as an\nad-hoc technique by Kaelbling et al. (1996); Kaelbling (1993); Sutton (1991a); Thrun and\nMöller (1992).\nFrom an optimistic perspective, a state is considered a good state if it\ninduces a higher level of uncertainty in the state-value estimate and greater return potential.\nOptimistic exploration methods are typically realized by implicitly utilizing an exploration\nbonus either in the form of optimistic initialization (Even-Dar and Mansour, 2002; Sutton\nand Barto, 1998b; Szita and Lőrincz, 2008) or Upper Conﬁdence Bounds (UCBs)(Strehl\nand Littman, 2008; Jaksch et al., 2010). In the optimistic initialization approach, the key\nassumption is that the unvisited state-action pairs yield the best outcome, whereas in the\nUCB-based methods, the unvisited state-action pairs are assumed to collect the outcome\nproportional to the largest statistically possible reward. In this section, we only focus on\nthe methods that do not employ count-based approaches to implement the OFU principle\nas we address the count-based methods in a separate section.\n6.1.1 Optimism-based methods: tabular\nA model-based approach proposed as a generalization of E3 algorithm (Kearns and Singh,\n2002) (discussed in detail in Section 6.2.1) is the R-max algorithm (Brafman and Tennen-\nholtz, 2002), which models agents’ interactions in the context of zero-sum stochastic games\n(SG) instead of MDPs.\nIn R-max (Brafman and Tennenholtz, 2002), the agent always\nmaintains maximum likelihood estimates of environment dynamics and reward function if\nthe observed data is suﬃciently rich. The algorithm uses the approximate model estimates\nfor a state-action pair if its visitation count exceeds a certain threshold. The optimistic\napproach is adopted at the initialization phase of the model, where all actions in all states\nare assumed to return maximum reward Rmax. R-max beneﬁts from a built-in mechanism\nfor resolving the exploration vs.\nexploitation dilemma because of the model estimation\n32\nA Survey of Exploration Methods in Reinforcement Learning\nReference\nApproach\nPerformance measure\nTabular Methods\nBrafman and Tennenholtz (2002)\noptimistic initialization\nPAC bound\nAuer and Ortner (2007)\nUCB-based\nPAC bound\nSzita and Lőrincz (2008)\noptimistic initialization\nPAC\nbound/Empirical-\nGrid\nand\nEmpirical-Toy\nMDP\nFunction\nApproxima-\ntion Methods\nJong and Stone (2007)\noptimistic initialization\nEmpirical-Grid\nworld/Mountain Car\nNouri and Littman (2009)\noptimistic initialization\nPAC bound/Empirical\nOrtner and Ryabko (2012)\nUCB action selection\nRegret bound\nKumaraswamy et al. (2018)\nUCB action selection\nEmpirical-MuJoCo\nand\nPuddle\nWorld/\nPAC\nBound\nCiosek et al. (2019)\noptimistic initialization\nEmpirical-MuJoCo\nRashid et al. (2020)\nUCB action selection\nRegret\nBound/Empirical-\nAtari\nSeyde et al. (2020)\nUCB action selection\nEmpirical-MuJoCo\nTable 4: Exploration methods that implement an optimism-based bonus mechanism.\nand optimism. That is, taking the optimal action according to the learned model results\nin either exploring a previously unknown state or obtaining the near-optimal reward. In\nR-max, a state is marked as known if the number of states reachable (based on the learned\nmodel) from that state passes a ﬁxed threshold; therefore, it is no longer considered a novel\nstate. The sample complexity analysis conducted by Brafman and Tennenholtz (2002) shows\nnear-optimal performance in a polynomial number of time-steps (assuming the state-space\nis ﬁnite). The R-max algorithm also attains near-optimal polynomial time average reward\nin |S|, |A| and mixing time T.\nIn a similar approach, authors in Szita and Lőrincz (2008) propose a new sample eﬃcient\nand model-based exploration algorithm called Optimistic Initial Model (OIM) in an MDP\nframework with ﬁnite state and action spaces. The proposed method assigns an optimistic\nvalue to unknown areas, and if the sampled state is among the set of ’Garden of Eden’\nstates, it receives the maximum reward of Rmax.\nThe RL agent in this method builds\nan approximate model of environment dynamics and updates the value functions using\ndynamic programming. To handle the full update sweep complexity imposed by dynamic\nprogramming, authors adopt the prioritized sweeping algorithm of Wiering and Schmidhuber\n(1998). Theoretically, the authors show almost sure convergence of the proposed method\nto a near-optimal policy in polynomial time under a lower-bound assumption on Rmax.\nExperimentally, OMI’s performance is assessed against ϵ-greedy, Boltzmann and some other\nexploration methods in three environments of RiverSwim and SixArms (Strehl et al., 2006),\nMaze (Wiering and Schmidhuber, 1998) and Chain, Loop and FlagMaze (Meuleau and\nBourgine, 1999; Strens, 2000; Dearden et al., 1999).\n33\nxxxx\nThe seminal idea of employing optimistic upper conﬁdence bound (UCB) to encourage\noptimistic exploration policies in RL setting was ﬁrst introduced by Strehl and Littman\n(2004). Auer and Ortner (2007) proposed UCRL algorithm as the ﬁrst near-optimal explo-\nration method that extends the idea of optimistic UCBs in RL. UCRL computes a count-\nbased upper bound on the empirical estimates of reward and transition probability after\neach visit and then switches between policies based on the observed gap and calculates\nand a new policy. Later, Jaksch et al. (2010) proposed UCRL2 as an extension to Auer\nand Ortner (2007). Apart from optimistic initialization, both UCRL and URCL2 imple-\nment count-based UCBs to encourage exploration. Thus, we postpone further explanation\nregarding these methods to the count-based section.\n6.1.2 Optimism-based methods: function approximation\nAfter the successful application of OFU to RL with ﬁnite state-action MDPs, which we\naddressed in Section 6.1.1, some recent approaches extended this idea to MDPs with large\nor inﬁnite state-action spaces (Azar et al., 2017; Bartlett and Tewari, 2012; Fruit et al.,\n2018; Filippi et al., 2010; Jaksch et al., 2010; Tossou et al., 2019). This section provides a\ncomprehensive overview of the methods that employ OFU in the MDPs with large or inﬁnite\nstate-action spaces.\nThe study presented by Jong and Stone (2007) is a model-based exploration-exploitation\ntrade-oﬀalgorithm for continuous state spaces, termed Fitted R-max. The proposed algo-\nrithm is a combination of R-max (Brafman and Tennenholtz, 2002) with ﬁtted value itera-\ntion (Gordon, 1995). The algorithm ﬁrst updates environment models at each time-step and\nthen applies the value iteration step to solve their proposed Bellman optimality equation. In\ndiscrete setting, the proposed method simply implements the optimistic value function pro-\nposed by Brafman and Tennenholtz (2002) and control the exploration-exploitation trade-\noﬀthrough a visitation count threshold. In continuous setting, the authors propose a new\ncounting method based on the sum of the unnormalized kernel values based in the estimated\nenvironment dynamics. The performance of the Fitted R-max algorithm is experimentally\ntested in the two environments Mountain Car (Sutton and Barto, 1998b) and Puddle World\n(Kearns and Singh, 2002).\nAnother line of work that implements the OFU principle in continuous state space en-\nvironments is Nouri and Littman (2009), where the authors combine their proposed Multi-\nresolution Exploration (MRE) algorithm with ﬁtted Q-iteration (Antos et al., 2008). Their\nproposed model-based method is built upon Kakade et al. (2003) and introduces a method\nthat measures the uncertainty associated with the visited states through building regression\ntrees, termed as knownness-tree. Knownness-tree is used to model the environment transi-\ntion dynamics and optimistically update its model at each time step. Theoretically, Nouri\nand Littman (2009) show, under some smoothness assumption on transition dynamics, the\nnear-optimal performance of the proposed algorithm, and assess the performance of MRE\nagainst ϵ-greedy algorithm empirically in continuous Mountain Car environment (Sutton\nand Barto, 1998a).\nKumaraswamy et al. (2018) propose a model-free computationally eﬃcient exploration\nstrategy based upon computing Upper-Conﬁdence Least-Squares (UCLS), which are UCBs\nfor least-squares temporal diﬀerence learning (LSTD). Since LSTD maintains the agent’s\n34\nA Survey of Exploration Methods in Reinforcement Learning\npast interactions eﬃciently, the computed upper conﬁdence bounds induce context-dependent\nvariance, which encourages the exploration of states with higher variance. This study pro-\nvides the ﬁrst theoretical results that obtain UCBs for policy evaluation using function\napproximation. Empirically, UCLS algorithm shows outperformance over DGPQ (Grande\net al., 2014), UCBootstrap (White and White, 2010), and RLSVI (Osband et al., 2016b) in\nSparse Mountain Car, Puddle World and RiverSwim environments.\nCiosek et al. (2019) provide an optimistic actor-critic algorithm to resolve two phenom-\nena: pessimistic under-exploration, that is deviating the algorithm from sampling actions\nthat result in improvement on the critic estimates, and directionally uninformed action sam-\npling, which is uniform sampling of actions that are lying in two opposite sides of mean in\nGaussian policies. They state that these two phenomena prevent state-of-the-art actor-critic-\nbased algorithms such as SAC (Haarnoja et al., 2018) from performing eﬃcient exploration.\nTo tackle these challenges, they calculate approximate upper conﬁdence bounds on the value\nfunction estimates to encourage directed exploration and lower conﬁdence bound to prevent\noverestimation of the value function estimates. They benchmark their proposed algorithm\n(OAC) in high-dimensional MuJoCo tasks, and the plotted results demonstrate marginal\nimprovement against the SAC algorithm.\nTo avoid the pessimistic initialization phenomenon, commonly used in deep network\ninitialization schemes, Rashid et al. (2020) propose an optimistic initialization algorithm,\ntermed Optimistic Pessimistically Initialised Q-Learning (OPIQ) that decouples optimistic\ninitialization of Q function from network initialization.\nRashid et al. (2020) propose a\nsimple count-based bonus augmented with the Q-value estimates. In the tabular setting,\ntheir proposed algorithm is based on Jin et al. (2018). In the Deep RL setting, they adopt\ncommonly employed methods of calculating pseudo counts such as Bellemare et al. (2016);\nOstrovski et al. (2017) to compute the additive bonus term.\nOPIQ is evaluated in the\nthree domains toy randomized Markov chain, Maze and Montezuma’s Revenge against the\nnaive extension of UCB-H (Jin et al., 2018) to the deep RL and some variations of DQN\naugmented with some state-of-the-art pseudo-count estimate methods.\nWhen both the environment dynamics and task objective are unknown to the RL agent,\nSeyde et al. (2020) propose a model-based exploration algorithm, termed Deep Optimistic\nValue Exploration (DOVE), to encourage deep exploration through adopting optimistic\nvalue function. Throughout each episode, DOVE learns a transition function and reward\nfunction using supervised learning. The initial conditions are applied to the learning pol-\nicy generated by perturbing locally observed states fetched from the replay memory. The\nlocal perturbation performed throughout each episode is employed to ensure information\npropagation. Empirically, Seyde et al. (2020) benchmark DOVE in some high-dimensional\ncontinuous control MujuCo tasks.\n6.2 Count-based bonus\nOne way to model the intrinsic reward is to measure how surprising a state-action pair is. An\nintuitive approach to measuring surprise is to count how frequently a particular state-action\npair is visited. In the count-based setting, the notion of bonus is deﬁned as a function of\nstate-action pair visitation count. In table 5, we provide a list of exploration methods that\n35\nxxxx\nReference\nBonus Type\nPerformance measure\nTabular Methods\nSutton (1991b)\ncount-based\nEmpirical-Grid world\nMoore and Atkeson (1993)\ncount-based\nthresh-\nold/optimistic initialization\nEmpirical-Grid world\nKaelbling (1993)\ncount-based\nEmpirical-Toy MDP\nDayan and Sejnowski (1996)\ncount-based\nEmpirical-Grid world\nTadepalli and Ok (1998)\ncount-based/optimistic\nini-\ntialization\nEmpirical-Grid\nworld/AGV-\nscheduling\nKearns and Singh (2002)\ncount-based/optimistic\nini-\ntialization\nPAC bound\nKakade et al. (2003)\ndistance-based count\nPAC bound\nStrehl et al. (2006)\nUCB-based\nPAC bound\nAuer and Ortner (2007)\nUCB-based\nPAC bound\nJaksch et al. (2010)\nUCB-based\nPAC/Regret bound\nAzar et al. (2017)\nUCB-based\nRegret bound\nKolter and Ng (2009)\ncount-based\nPAC bound\nPazis and Parr (2013)\noptimistic\ninitialization\n/\ndistance-based bonus\nPAC bound/Empirical\nGuo and Brunskill (2015)\ncount-based threshold\nPAC bound\nJin et al. (2018)\ncount-based/UCB-based\nPAC bound\nWang et al. (2020)\ncount-based/UCB-based\nPAC bound\nFunction\nApproximation\nMethods\nBellemare et al. (2016)\ndensity-based\nEmpirical\nFu et al. (2017)\ndensity-based\nEmpirical\nMartin et al. (2017)\ndensity-based\nEmpirical\nTang et al. (2017)\ncount-based\nEmpirical\nMachado et al. (2018a)\ncount-based\nEmpirical\nRashid et al. (2020)\noptimistic\ninitialization/\ncount-based\nEmpirical\nTable 5: Count-based methods.\nexplicitly employ a form of visitation count to control the exploration-exploitation trade-oﬀ.\n36\nA Survey of Exploration Methods in Reinforcement Learning\n6.2.1 Count-based bonus: tabular\nIn the tabular setting, counting visited states or state-action pairs is a trivial problem and\nthe bonus term is typically used in one of the following forms,\nB(s, a) or B(s) ∝\n\n\n\n\n\n\n\n\n\nq\nln n\nN(s,a) or\nq\nln n\nN(s),\n1\n√\nN(s,a) or\n1\n√\nN(s),\n1\nN(s,a) or\n1\nN(s).\n(25)\nwhere n denotes the total number of time-steps taken by the RL agent.\nAn intuitive interpretation of equation (25) is that the bonus for visiting a state-action\npair (s, a) is highest when (s, a) is novel, and decays each time the pair (s, a) is revisited.\nThe main limitation of such deﬁnitions of bonus is that they are mainly applicable in tabular\nsettings, where the set of state-action pairs is countable and ﬁnite. Although bonus-based\nmethods employed in the tabular settings are not necessarily suitable for large state-and-\naction space settings, they still provide useful intuitions. The ﬁrst study that employed\nthe notion of bonus in the context of exploration algorithms was Sutton (1991b), where a\nnew RL architecture Dyna-Q+ was proposed. Dyna-Q+, which is a combination of Dyna\narchitecture (Sutton, 1991a) and Watkins Q-learning (Watkins and Dayan, 1992) uses an\nadditional explicit count-based exploration bonus assigned to state-action pair (s, a). The\nbonus term used in Dyna-Q+ is proportional to the square root of the number of time steps\nthat have elapsed after the last trial of action a at state s. This exploration bonus is added\nto the update rule designed to update Q-value. The main advantage of using such bonuses\nis to increase the chance of visiting the state-action pairs that have not been frequently\nvisited. Sutton (1991a) tests his proposed model in the two RL environments Blocking and\nShortcut tasks, designed as a small 2D maze environment against two other variations of\nDyna-Q that do not use exploration bonus to encourage exploration. Sutton (1991a) shows\nthat in both experiments, Dyna-Q+ outperforms other variations of Dyna-Q in the setting,\nwhere the performance is measured with respect to the collected reward. To address the\ntwo issues, namely high computational expense raised by Kaelbling (1993), and instability\nof the bonus raised by Sutton (1991a) in large state and action spaces, Moore and Atkeson\n(1993) proposed the prioritized sweeping algorithm that uses a preset threshold parameter\nTboard to determine whether the state-action pair is worth exploring more or not. Prior to\nreaching the visitation threshold, the bonus parameter is set to the max return value in the\ndiscounted reward setting Rmax/(1 −γ). Once the visitation count exceeds the optimistic\nthreshold, the algorithm uses the non-optimistic true discounted return. Apart from the\ntabular nature of this approach, its main bottleneck is that the key hyperparameter Tboard is\npreﬁxed and set manually. The proposed algorithm’s performance is experimentally tested\nagainst Dyna-Q in two deterministic and stochastic grid world environments.\nAn early study that adopts the OFU principle in a model-based setting for exploration is\nTadepalli and Ok (1998). The proposed H-learning algorithm is designed in the context of\nthe average-reward RL setting, where the RL agent’s goal at each time-step t is to optimize\nthe average expected reward. At each time-step t, an empirical model of the environment is\ncomputed, and consequently, a set of greedy actions accessible from the current state with\nrespect to the Bellman equation for average-reward RL is reported, as well as the expected\n37\nxxxx\nlong-term advantage function h(s). The long-term advantage function h(s) reﬂects the long-\nterm impact of start state s on the obtained expected average reward. Eventually, the ﬁnal\nexpected average reward associated with actions in the set of greedy actions at the current\nstate s is calculated with respect to h(s), a temperature parameter α and expected average\nreward computed at time t −1. Experimentally, the proposed H-learning algorithm is com-\npared with four other exploration methods, random exploration, counter-based exploration,\nBoltzmann exploration, and recency-based exploration in a two-dimensional grid world with\ndiscrete state and action spaces, termed Delivery domain. The non-tabular version of the H-\nlearning algorithm is proposed based on local linear regression as the function approximator\nand Bayesian network representation of the action space. The extended H-learning method\ncalled LBH-learning is tested in three AGV-scheduling tasks (Maxwell and Muckstadt, 1982)\nand compared to six diﬀerent H-learning baselines.\nThe Explicit Explore or Exploit algorithm (known as E3) (Kearns and Singh, 2002)\nadopts a model-based approach that initiates the exploration phase by dividing the set of\nstates into two categories of known and unknown states. A state is considered to be known\nif the number of state visitations passes a certain threshold such that the learned dynamics\nare suﬃciently close to the true one. If the current state is unknown, the algorithm calls the\nprocedure of balanced wandering, in which the algorithm chooses the least frequent action at\nthe unknown state and assigns the max reward to the unknown state. When the algorithm\nis not engaged in the balanced wandering phase, it performs two oﬄine optimal policy\ncomputations sub-routines. Later, Kakade et al. (2003) proposed Metric-E3 algorithm as a\ngeneralization of E3algorithm. Metric-E3 provides the time complexity bound on ﬁnding a\nnear-optimal policy that depends on the covering numbers of the state space rather than the\nsize of the state space as presented by Kearns and Singh (2002). This diﬀerence is mainly\ndue to the diﬀerence in their deﬁnition of a \"known\" state.\nIn the context of undiscounted RL, Auer and Ortner (2007) use the notion of upper\nconﬁdence bounds to manage exploration-exploitation trade-oﬀ. In their study, count-based\nconﬁdence bound proportional to\nq\n1\nN(s,a) is updated at each step and, together with empir-\nical estimates of reward and transition functions, help the agent to control the exploration-\nexploitation trade-oﬀ.\nThe regret analysis performed by Auer and Ortner (2007) shows\nlogarithmic performance in the number of time steps taken by the algorithm based on the\noptimal policy.\nKolter and Ng (2009) provide an explicit notion of bonus, called Bayesian Exploration\nBonus, to manage exploration-exploitation trade-oﬀ. Their proposed algorithm focuses on\nthe Bayes-adaptive RL setting with a tabular representation of state-action space.\nThe\nbonus term is proportional to\n1\n1+N(s,a), where N(s, a) is calculated based on the number of\nvisitation counts implied by the prior. Kolter and Ng (2009) provide a template for count-\nbased bonus terms in the form of a theorem stating that any algorithm that adopts an\nexploration bonus of the form\n1\n(N(s,a))p with p ≤1/2 is not a PAC-MDP. In their proposed\nalgorithm, called BEB, the action-selection is performed with respect to the mean of the\ncurrent learned belief over transition model, with an additional Bayesian bonus.\nIn the\nmain theorem of this work, the authors show that their proposed algorithm, while allowing\na higher rate of exploration, provides a near-optimal sample complexity bound, which is\n38\nA Survey of Exploration Methods in Reinforcement Learning\npolynomial in |S|, |A|, and time horizon T, where the optimality is deﬁned in the Bayesian\nsense.\nIn the continuous state space setting, Pazis and Parr (2013) introduce C-PACE as a\nPAC-optimal exploration algorithm for continuous state MDPs. C-PACE adopts the OFU\nprinciple in the estimation of the Bellman equation. At each time step, from the k-nearest\nneighbours, the action that maximizes the optimistic Q value function is selected.\nThe\noptimistic Q value function is deﬁned based on the knowledge of k-neighbouring state-\naction pairs (the bonus term) and the immediate reward obtained upon transiting to any\nneighbouring pairs. C-PACE assumes the existence of a Lipschitz continuous distance metric\nover the set of state-action pairs. The main result of this paper provides a PAC bound that\nshows the near-optimal C-PACE performance with respect to the covering number of the\nstate-action space. Finally, the authors evaluate the performance of C-PACE in a simulated\nHIV treatment environment.\nGuo and Brunskill (2015) propose a conﬁdence-based exploration algorithm called PAC-\nEXPLORE in a model-based setting, which is operationally very similar to the E3 algorithm\n(Kearns and Singh, 2002), with the diﬀerence in the conﬁdence bounds used to compute poli-\ncies that are practically more eﬃcient. The PAC-EXPLORE algorithm takes a state-action\npair visitation threshold and divides the space of state-action pairs into two clusters of known\nand unknown pairs. If the state falls into the set of known states, the algorithm applies the\nsame technique as in Wiering (1999) to estimate conﬁdence bounds on transition probability.\nAuthors in this work show that by sharing the experience of concurrently running agents\non top of Wiering (1999), one can achieve linear improvement on the algorithm’s sample\ncomplexity.\nDelayed Q-learning (Strehl et al., 2006) is one of the ﬁrst papers that study model-free\nPAC optimal algorithm. At each time-step t, the agent keeps track of three values for each\nvisited state-action pairs (s, a), the value function Qt(s, a), the Boolean ﬂag LEARNt(s, a)\nthat indicates whether or not the change has occurred to the Q estimate for (s, a), and a\nvisitation counter N(s, a). The exploration-exploitation trade-oﬀis managed based on a\nvisitation count threshold and the value LEARNt(s, a). When the visitation count for (s, a)\nis larger than a pre-set threshold and the LEARNt(s, a) is true, the Qt(s, a) estimate is\nupdated. At the initial phase, the Boolean ﬂag LEARN(s, a) is set to TRUE for all state-\naction pairs and N(s, a) is set to zero. Strehl et al. (2006) under certain assumptions prove\nthat their proposed algorithm is PAC-MDP in the tabular setting.\nJin et al. (2018) provide two types of upper conﬁdence-based bonuses for Q-learning\nin the episodic tabular MDP setting: 1) Hoeﬀding-style bonus, 2) Bernstein-style bonus.\nBy employing the Hoeﬀding-style bonus, the authors show O(\n√\nT) regret dependency with\nrespect to the total number of time-step T. They also show\n√\nH improvement by using\nBernstein-style bonus over the Hoeﬀding-style bonus exploration algorithm, where T denotes\nthe time horizon.\nWang et al. (2020) introduce another method that addresses the sample eﬃciency of\nmodel-free algorithms by adopting UCB-exploration bonus in Q-learning. Their proposed\nUCB-based algorithm maintains two types of visitation counts, Nt(s, a) that denotes the\nnumber of times the pair (s, a) has been visited up to time-step t, and τ(s, a, k) that records\nthe number of time steps that state-action pair (s, a) has occurred for the k-th time. If\n(s, a) has not been visited k times, then τ(s, a, k) = ∞. At each time-step, a bonus term\n39\nxxxx\nproportional to\nq\n|S||A| ln(Nt(s,a))\nNt(s,a)\nis added to the discounted value estimate, and the action-\nvalue function gets updated accordingly.\nTo assess the PAC eﬃciency of the proposed\nalgorithm, the authors ﬁrst propose a learning instance illustrating Ω(1/ϵ3) lower bound\nincurred by Delayed Q-learning, which leaves a quadratic gap in 1/ϵ from the best known\nlower bound in the class of UCB-based exploration algorithms.\n6.2.2 Count-based: function approximation\nDespite the near-optimal performance guarantees often achieved in the tabular setting, these\nmethods are mostly not suitable for environments with large or inﬁnite state spaces. This\nsection summarizes exploration methods that adopt a notion of visitation count to design\nexploration algorithms for environments with large state and action spaces.\nBellemare et al. (2016) revisit the problem of extending count-based exploration to non-\ntabular setting and propose a density model that hinges upon ideas from the intrinsic moti-\nvation literature (refer to section 4) and propose an algorithm that measures state novelty\nfor any choice of action given an arbitrary density model. The key contribution of their\nstudy is drawing a connection, called pseudo-count, between intrinsic motivation and count-\nbased exploration. Pseudo-count quantity is derived from an arbitrary density model over\nthe state space. The density model proposed in Bellemare et al. (2016) models a marginal\ndistribution in which states are independently distributed. For any given choice of density\nmodel ρ, the paper draws a connection between two unknowns: 1) pseudo-count function\nand 2) pseudo-count total. The paper also introduces a connection between the conditional\nprobability assigned to state s using ρ after observing its new occurrence conditioning based\non its prior observations, pseudo-count function and pseudo-count total. The notion of in-\nformation gain as a popular measure of novelty and curiosity is then shown to be related\nto pseudo-count, which leads to the main theorem in the paper that suggests using pseudo-\ncount bonus leads to an exploratory behaviour as good as when the information gain bonus\nis used. Under two major assumptions: 1) the given density model asymptotically behaves\nsimilarly to the limiting empirical distribution, and 2) the learning rate at which ρ changes\nwith respect to the true state distribution µ is positive in the asymptotic sense, the limit of\nratios of pseudo-counts to empirical counts is ﬁnite and exists for all states. Bellemare et al.\n(2016) test their proposed method in comparison with two state-of-the-art RL algorithms,\nDouble DQN (van Hasselt et al., 2016) and A3C (Asynchronous Advantage Actor-Critic)\n(Mnih et al., 2016) on some of the Atari 2600 games.\nIn a subsequent work, Ostrovski et al. (2017) answer two questions regarding the mod-\neling assumptions raised in Bellemare et al. (2016): 1) what is the impact of the quality\nof density model on exploration? 2) To what extent do Monte-Carlo updates inﬂuence ex-\nploration? To address the ﬁrst question, Ostrovski et al. (2017) adopt an advanced neural\ndensity model PixelCNN (Van den Oord et al., 2016), and discuss the challenges involved in\nthis approach in terms of model choice, model training and model use. PixelCNN is a con-\nvolutional neural network that models a probability distribution over pixels conditioned on\nthe previous pixels. The paper provides a list of properties that the density model requires\nand subsequently suggests a suitable notion of pseudo-count for DQN agents that leads to\nstate-of-the-art results in diﬃcult Atari games like Montezuma’s Revenge.\n40\nA Survey of Exploration Methods in Reinforcement Learning\nFollowing the pseudo-count technique proposed by Bellemare et al. (2016) and Ostrovski\net al. (2017), Martin et al. (2017) propose a new density model to measure the similar-\nity between states and, consequently, a generalized visitation count method. Even though\nBellemare et al. (2016) construct the density model over raw state visitations, the method\nproposed by Martin et al. (2017) relies on the feature map used in value function approxima-\ntion to construct the density model. The bonus-based exploration algorithm φ-Exploration\nBonus proposed by Martin et al. (2017) augments the extrinsic reward with the bonus\nterm proportional to the inverse of the square root of the pseudo-count calculated based\non the proposed feature-based density model. Empirically, φ-Exploration Bonus algorithm\nis evaluated against the ϵ-greedy, A3C (Mnih et al., 2016), Double DQN (Hasselt et al.,\n2016), Double DQN with pseudo-count (Bellemare et al., 2016), TRPO (Schulman et al.,\n2015), Gorila (Nair et al., 2015), and Dueling Network (Wang et al., 2016b) baselines in ﬁve\ndiﬀerent games from the Arcade Learning Environment (ALE).\nFu et al. (2017) introduce another study that uses count-based bonuses to conduct ex-\nploration in high-dimensional domains using the notions of curiosity and novelty. Eﬀective\nexploration methods that are based on a notion of visitation novelty typically require either\na tabular representation of states and actions or a generative model over state and actions,\nwhich can be diﬃcult to train in high-dimensional and continuous settings. Fu et al. (2017)\npropose an approach to approximate state visitation densities using a discriminative model\n(exemplar model) over the complex model of states using deep neural networks, where the\nclassiﬁer assigns reward bonuses if the recently visited state is novel. The authors of this\nwork show that discriminative modeling is equivalent to implicit density estimation. They\nargue that learning a discriminative model using standard convolutional classiﬁer networks\nin the case of rich sensory inputs like images is typically easier than learning the generative\nmodel of the environment. Their proposed model is inspired by the concept of Genera-\ntive Adversarial Networks (Goodfellow et al., 2014) and employs the intuition that novel\nstates are typically more easily distinguished from all other visited states. The main idea is\nto maintain a density estimator using exemplar models based on a discriminatively trained\nclassiﬁer instead of maintaining explicit counts. To train the proposed discriminator, a cross-\nentropy loss is employed. Fu et al. (2017) evaluate their proposed method in sparse-reward\ncontinuous high-dimensional control tasks in MuJoCo (Todorov et al., 2012) and Vizdoom\n(Kempka et al., 2016). They compare the algorithm’s performance with the two state-of-\nthe-art baseline by Houthooft et al. (2016) (discussed in Section 6.3.2) and Schulman et al.\n(2015).\nAs an extension of count-based exploration to high-dimensional and continuous deep RL\nbenchmarks, Tang et al. (2017) use a hashing mechanism to map novel states and visited\nstates to hash codes and subsequently count the state visitations using the corresponding\nhash table. In the simple domains, authors propose a static hashing approach, in which\nthe state space is discretized using a hash function such as SimHash (Charikar, 2002), and\nsubsequently the bonus term is set to be proportional to the inverse of the square root of\nstate count with respect to the hash code. In environments with complex structures, the\nauthors adopt the Learned Hashing mechanism that implements an autoencoder to learn the\nappropriate hash codes. Like the static hash mechanism, the Learned Hashing mechanism\nalso employs a bonus term proportional to the inverse of the square root of count on the\n41\nxxxx\nhash codes. This approach outperforms the method presented by Houthooft et al. (2016) in\nsome rllab benchmark tasks, as well as the vanilla DQN agent in some Atari 2600 games.\nInspired by the results from Wu et al. (2018) and Machado et al. (2017), authors of\nMachado et al. (2020) show that the inverse of l1 norm of successor representation (Dayan,\n1993) can be interpreted as an exploration bonus in both tabular and function approxi-\nmation setting. Successor representation (Dayan, 1993) can be interpreted as an implicit\nestimator of the transition dynamics of the environment. In the tabular setting, they aug-\nment the Sarsa (Sutton, 1992) update rule with the inverse of the norm of the successor\nrepresentation of the visited states. Their proposed algorithm is empirically compared with\ntraditional Sarsa in traditional PAC-MDP domains SixArms and RiverSwim (Strehl and\nLittman, 2008). In the function approximation case, the bonus used is similar to the one\nutilized in tabular setting and is the inverse of l1 norm of the parameterized successor feature\nvector. Machado et al. (2020) evaluate their proposed algorithm empirically in the Arcade\nLearning Environments (Bellemare et al., 2013) with sparse reward structure, including\nFreeway, Gravitar, Montezuma’s Revenge, Private Eye, Solaris, and Venture.\n6.3 Prediction error-based bonus\nIn this category of exploration methods, the bonus term is computed based on the change\nin the agent’s knowledge about the environment dynamics. The agent’s knowledge about\nthe environment is often measured through a prediction model of environment dynamics.\nThis section focuses on the exploration techniques that use Prediction Error (PE) as an\nexploration bonus. PE is a term used to measure the diﬀerence between the true environment\nmodel parameters and their estimates that are used to predict transition dynamics. The\nmethods that fall into this category use the discrepancy between the induced prediction\nfrom the learned model of environment dynamics and state-action representation models,\nand the real observation to assess the novelty of the visited states. The states that lead to\nmore considerable discrepancy are considered more informative than those with a smaller\ndiscrepancy. The ﬁrst two studies that employ PE as an exploration bonus to encourage\ncuriosity are Schmidhuber (1991a) and Schmidhuber (1991b), which are explained in detail\nin Section 4 (due to the fact that they can also function in environments that do not provide\nextrinsic rewards).\nFormally, let Ht be the history of observations until time-step t, at denote the action\ntaken at time t, and Mφ be the predictive model of transition parameterized by the feature\nfunction φ. Then, the prediction error at time t is proportional to,\ne(Ht−1, at, st) ∝∥st −Mφ(Ht−1, at)∥p,\n(26)\nwhere ∥.∥p denotes the p-norm of a given vector.\n6.3.1 Prediction error-based bonus: Tabular\nDayan and Sejnowski (1996) consider the problem of exploration in a non-stationary ab-\nsorbing ﬁnite POMDP setting and provide a systematic approach to designing exploration\nbonuses in such setting. Their algorithm borrows the certainty equivalence approximation\ntechnique from the dual control literature and provides a statistical model of the environ-\nment’s uncertainty in a ﬁnite state space setting and subsequently incorporates the uncer-\n42\nA Survey of Exploration Methods in Reinforcement Learning\nReference\nApproach\nPerformance measure\nTabular Methods\nSchmidhuber (1991a)\nConﬁdence-based\nEmpirical-Grid World\nDearden et al. (1998)\nInformation gain-based\nConvergence/Empirical\nWiering (1999)\nConﬁdence-based\nPAC bound\nIshii et al. (2002)\nConﬁdence-based/Entropy-\nbased\nEmpirical-Grid\nStrehl and Littman (2004)\nConﬁdence-based\nEmpirical\nStrehl and Littman (2008)\nConﬁdence-based\nPAC bound/Empirical\nLopes et al. (2012)\ndensity-based\nPAC bound/Empirical\nFunction\nApproximation\nMethods\nWhite and White (2010)\nConﬁdence based\nConvergence/Empirical\nStadie et al. (2015)\nPE-based bonus\nEmpirical\nHouthooft et al. (2016)\nInformation gain\nEmpirical\nPathak et al. (2017)\nPE-based bonus\nEmpirical\nBurda et al. (2018a)\nPE-based bonus\nEmpirical\nHong et al. (2018)\nPE-based bonus\nEmpirical\nBurda et al. (2018b)\ndensity-based\nEmpirical\nKim et al. (2019)\nInformation gain-based\nEmpirical\nTable 6: Prediction Error-based methods\ntainty estimates into the systematic design of exploration bonuses. The exploration bonus\nin Dayan and Sejnowski (1996) is proportional to the amount of uncertainty induced by the\nagent’s belief system and the true model of the environment. Dayan and Sejnowski (1996)\nassess their proposed method against DAYNA (Sutton, 1991a) in a two-dimensional maze\nwith movable barriers.\nThe concept of Interval Estimation (IE) was ﬁrst introduced by Kaelbling (1993) in\nthe bandit setting to employ conﬁdence intervals during the exploration phase. The agent\nchooses the action that induces the highest upper conﬁdence bound. A few years later, Wier-\ning (1999) provided a theoretical extension to Kaelbling (1993) and discussed a new variation\nof Model-Based Interval Estimation (MBIE) by augmenting the Bellman optimally equation\nwith a bonus term proportional to 1/\np\nN(s, a). The author also provide a formal PAC-style\nguarantee for their proposed algorithm and theoretically analyze the eﬀect of additive bonus\nterm on the number of time steps required to achieve the sub-optimal performance.\nIn the stream of model-based approaches to exploration, Ishii et al. (2002) compute\nthe exploration bonus using the entropy of the posterior distribution of the state-transition\nkernel. The reward associated with the state-action pair is composed of the obtained imme-\ndiate reward, and the entropy of the visited state-action pairs. The action sampling policy is\nbased on a softmax action selection algorithm combined with an entropic bonus term. Small\nentropy means that the amount of information acquired given the agent’s current model of\nthe environment is expected to be small, and therefore, the probability of taking action given\nthe current state of the agent is small. Ishii et al. (2002) assess their proposed method in\na small 2D maze environment with ﬁxed and moving barriers and a zig-zag maze. In both\n43\nxxxx\nexperiments, the eﬀect of exploration bonus on the required number of steps for reaching\nthe shortest path is shown.\nLopes et al. (2012) improve the traditional model-based exploration techniques based on\nOFU principle such as R-Max (Brafman and Tennenholtz, 2002) and Bayesian exploration\nbonus (Kolter and Ng, 2009), and empirically estimate the learner’s accuracy and the learn-\ning progress. Lopes et al. (2012) use the change in the loss of prediction error (both mean\nand variance) as the bonus term, and subsequently use it to modify R-Max and Bayesian ex-\nploration bonus methods. This approach is particularly useful in scenarios where the agent\nhas an incorrect prior knowledge of the transition model, or when it changes over time. The\nlearning progress is measured with respect to the empirical estimate of predictive error using\nthe leave-one-out cross-validation estimator.\n6.3.2 Prediction error-based bonus: Function Approximation\nIn this section, we focus on the exploration techniques that use PE as a measure of uncer-\ntainty to design exploration bonuses in domains with large or inﬁnite state spaces, where\nmethods that focus on tabular settings fail to generalize.\nStadie et al. (2015) propose a method of exploration that hinges upon a model of system\ndynamics trained using past experiences and observations. A state is considered novel and\naccordingly receives an exploration bonus based on its disagreement with the environment’s\nlearned model. Formally, given the state encoding function σ, the prediction error with\nrespect to σ at time t is denoted by et(σ) and thus the bonus term is proportional to\net(σ)/t. The authors benchmark their proposed algorithm on Atari tasks and show that it\nis an eﬃcient method of assigning exploration bonuses for large and complex RL domains.\nThe predictive model introduced by Stadie et al. (2015) is a simple two-layer neural network,\nand the prediction error is measured with respect to the l2 Euclidean norm. They evaluate\ntheir proposed method in 14 Arcade Learning Environments (ALE) against Boltzmann,\nDQN and Thompson sampling methods.\nIn another diversity-driven method, Hong et al. (2018) augment a diversity-based bonus\nwith the loss function and encourage the agent to explore diverse behaviours during the\ntraining phase. The modiﬁed loss function is computed by subtracting the current policy’s\nexpected deviation or distance from a set of recently adopted policies. They use a clipping\nthreshold in the case of observing extraordinary deviation in the computed empirical expec-\ntation. The authors evaluate the performanc of their proposed method in Mujoco and Atari\nenvironments against vanilla DDPG and the Parameter-Noise exploration method (Plappert\net al., 2018).\nBurda et al. (2018a) conducted a large set of experiments on curiosity-driven learning\nalgorithms that work with intrinsic reward mechanisms across 54 diﬀerent environments.\nInterestingly, the results presented show the impact of feature learning on better gener-\nalizability while using prediction error as a deriving force for exploration.\nThrough the\nconducted experiments, they also demonstrate the limitations of prediction-based bonus\nmechanisms.\nSome studies measure the observed state’s novelty based on the amount of PE the\nobserved state induces on the network that is trained using the agent’s past experience.\nFor example, Burda et al. (2018b) introduce a simple notion of exploration bonus, which\n44\nA Survey of Exploration Methods in Reinforcement Learning\nis based on the PE induced by the features of observed states and the prediction of the\nrandomly initialized network when the environment is stochastic.\nBurda et al. (2018b)\ncount four factors as the primary sources of error in prediction, 1) the amount of training\ndata, 2) stochasticity of environment, 3) model misspeciﬁcation, and 4) learning dynamics.\nThe uncertainty factor considered in their study is based upon the uncertainty quantiﬁcation\nmethod proposed initially by Osband et al. (2018). Burda et al. (2018b) assess their proposed\nexploration method in the diﬃcult Atari game Montezuma’s Revenge and outperforms the\nstate-of-the-art baselines.\nA line of research uses information gain (IG) as an exploration bonus (For a more detailed\nexplanation regarding information gain, refer to section 4.2). For instance, Houthooft et al.\n(2016) propose a curiosity-driven strategy, which uses information gain as a driving force\nto encourage exploration of actions that lead to states that cause a larger change in the\nagent’s internal model of environment dynamics. The state and action spaces are considered\nto be continuous.\nThe paper proposes a variational approach to approximate the true\nposterior, and therefore, the information gain is measured using the KL-divergence between\nthe agent’s internal belief over environment dynamics at diﬀerent time steps. The main\nchallenge in their proposed model is the computation of variational lower-bound. The way\nHouthooft et al. (2016) compute variational lower-bound, requires the calculation of the\nposterior probability, which is generally considered to be computationally intractable. The\ncomputed variational lower-bound is used to measure the agent’s curiosity. Houthooft et al.\n(2016) assess their proposed algorithm in continuous Mujoco domains, and compare its\nperformance with TRPO, ERWR and REINFORCE.\nAnother study that uses IG as an exploration bonus is introduced by Kim et al. (2019).\nThe authors apply the information bottleneck (IB) principle (Tishby et al., 2000) to design\nan exploration bonus to handle the exploration-exploitation trade-oﬀ, particularly in distrac-\ntive environments. The bonus term in Curiosity-Bottleneck (CB) objective (inspired by IB\nprinciple) appears in the form of mutual information, measured by KL-divergence between\nthe latent representation of the environment and the input observation.\nTo inspect the\nperformance of the proposed CB method, Kim et al. (2019) perform experiments on three\nenvironments: 1) Novelty detection on MNIST and Fashion-MNIST, 2) Treasure Hunt in a\ngrid-world environment, and 3) Atari’s Gravitar, Montezuma’s Revenge, and Solaris games.\nThe CB performance is compared with the work of Burda et al. (2018b).\n7. Deliberate Exploration\nThe optimal solution to the exploration-exploitation problem is a strategy that yields the\nhighest expected total reward over the entire duration of an agent’s interaction with the\nenvironment. The problem of ﬁnding such an optimal strategy is a meta-problem in itself,\nwhere a notion of optimality can be deﬁned with respect to a distribution over the models\nof the environments the agent is likely to encounter. Which exploration strategy works best\ndepends on the range of environments considered plausible by the agent. If a probability\ndistribution over the environment is known, we can deﬁne the optimal exploration strat-\negy as the strategy that yields the highest expected total reward in expectation over this\ndistribution.\n45\nxxxx\nIf unknown transition and reward model parameters are considered as the unobservable\nstates of the system, then the entire problem can be deﬁned as a speciﬁc kind of POMDP\nwhere the hidden part is a set of environment parameters and the observable part is the\nstate of the sampled MDP. However, this formulation can have too many belief states and\ncan thus in general not be solved exactly within practicable time. Various sub-ﬁelds have\nfocused on diﬀerent avenues of solving this problem approximately. The Bayesian approach\nfocuses on tackling the full problem, termed Bayes-adaptive MDP, by introducing diﬀerent\napproximations to the problem; for example by making prior assumptions about the form of\nthe uncertainty over the unknown model parameters either via function approximation or\nrepresenting the distribution over environment using a set of samples. On the other hand,\nmeta-learning approaches assume the agent does not directly have access to a distribution\nover environments but can be trained on samples from the relevant distribution. Various\nmethods, e.g. from model-free reinforcement learning, can then be used to ﬁnd policies that\ncan eﬀectively embed interaction histories and map them to actions to be taken. These\nmethods tend to aim at ﬁnding (locally) optimal solutions to the Bayes-adaptive MDP\nwithin a parametrized family of policies.\n7.1 Solving the exploration-exploitation trade-oﬀoptimally\nIn the Bayesian approach for RL, a posterior is maintained over the possible models of the\nenvironment given the observations so far. As the agent collects more observations, the\nbelief is updated to reﬂect this new information. Consequently, learning bayes-optimally\nin an MDP is equivalent to solving for an optimal action selection strategy in a meta-level\nMarkov decision process deﬁned by these belief states. In the following sections, we will\nfocus on a particular formulation of this meta-level problem: the Bayes-Adaptive MDP\n(Duﬀ, 2002) formulation, where the belief state is given by a current base-level state as well\nas a posterior distribution over the base-level transition and reward models.\nWhen the dynamics are unknown, the Bayesian RL formulation assumes that the transi-\ntion model P is a latent variable according to some prior distribution P(P). Let Ht denote\nthe history of observations up to time t, then the dynamics model is updated according\nto the Bayes rule P(P|Ht) ∝P(Ht|P)P(P), in response to the observed transitions. The\nuncertainty of the model dynamics is handled by transforming it into uncertainty into the\naugmented state and history space. The actual state of the agent, S, together with the belief\nstate that that consists of parameters deﬁning uncertainty distributions over the transition\nmodel, X, comprise the hyperstate, S+ = S × X. The new dynamics model in this new\naugmented space is deﬁned by P+(s′, x′, a, s, x), that denotes the transition model for hy-\nperstates, conditioned on action a being taken in hyperstate ⟨s, x⟩. The reward function for\nthe aurgmented MDP is given by R+(s, x, a) = R(s, a). The new MDP is deﬁned by the tu-\nple of M+ = ⟨S+, A, P+, R+, γ⟩and is known as the Bayes-Adaptive MDP (BAMDP)\n(Duﬀ, 2002).\nThe hyperstate is a suﬃcient statistics for the process evolving under uncertainty, and the\nBellman equations formalism used for MDPs also holds true for the generalized hyperstate\nMDP. The solution to the Bellman equations gives the value function of the hyperstate,\nand an optimal value function implicitly deﬁnes the optimal learning policy, which maps\nhyperstates to actions. The value function given by Bellman equation for the BAMDP is\n46\nA Survey of Exploration Methods in Reinforcement Learning\n−1\n+1\np1\n12\np1\n11\np1\n21\np1\n22\n(a)\n−1\n+1\np2\n12\np2\n11\np2\n21\np2\n22\n(b)\nFigure 2: A 2-state MDP with uncertain transition probabilities under (a) action 1 and (b)\naction 2. Rewards are denoted by ±1 in the states.\ngiven by:\nV ⋆(s, x) = max\na∈A\n\nR+(x, a) + γ\nX\nx′∈X,\ns′∈S\nP+(s′, x′ | s, x, a)V ⋆(s′, x′)\n\n.\n(27)\nThe agent starts in the belief state corresponding to its prior and, by executing the\ngreedy policy in the BAMDP while updating its posterior, acts optimally (with respect to\nits beliefs) in the original MDP. The Bayes-optimal policy for the unknown environment is\nthe optimal policy of the BAMDP, thereby providing an elegant solution to the exploration-\nexploitation trade-oﬀ. Through this framework, rich prior knowledge about the environment\ncan be naturally incorporated into the planning process, potentially leading to more eﬃcient\nexploration and exploitation of the uncertain world.\nExample:\nWe use the following example from Duﬀ(2003) to highlight how the Bayesian\nformulation allows to solve the exploration-exploitation optimally. Consider an MDP with\n2 states and 2 actions as shown in Figure 2. The hyperstate in this case is deﬁned by a\ntuple (s; x), where the physical state s is the state the agent currently is in as given by the\nMDP, and the information state x, which is the collection of distributions describing uncer-\ntainty in the transition probabilities. The rewards (±1) are deterministic and are received\nwhen the agent lands in the corresponding state. The unknown transition probabilities are\ndenoted by the labelled arcs. For this particular example, the authors propose using an\nappropriate conjugate family of distributions, such as Dirichlet, to model the uncertainty.\nFor instance, if the uncertainty in p1\n11 (transition from state 1 under action 1) is represented\nas beta distribution parameterized by (α1\n1, β1\n1), then hyper state for s = 1 can be written\nas: (1;\n\u0014α1\n1\nβ1\n1\nβ1\n2\nα1\n2\n\u0015\n,\n\u0014α2\n1\nβ2\n1\nβ2\n2\nα2\n2\n\u0015\n). The new augmented transition function can be written as\nP(s′, x′|s, x, a) = P(s′|s, x, a)P(x′|s, x, a, s′).\nThe ﬁrst factor can be estimated from samples by taking expectation over the possible\ntransition function to corresponding to a given hyperstate. Therefore, for a transition from\ns under action a, the corresponding terms for s′ = 1 and s′ = 2 will be\nαa\nαas+βas and\nβa\nαas+βas\nrespectively. For the second factor, the parameter of the dirichlet is given by the number of\n’eﬀective’ transitions of each type observed in transit from the initial hyperstate to the given\n47\nxxxx\nhyperstate. For this particular example, the form of the posterior update for the information\nstate parameters, given an observation is also a beta distribution, but with parameters that\nare incremented to reﬂect the observed data. As such, for transition from s under action a,\nthe update information state terms for s′ = 1 and s′ = 2 will be αa\ns + 1 and βa\ns + 1. An\noptimality equation can be written for the local transitions and the corresponding successor\nhyperstates. For the above example, the optimal value function has the form:\nV\n\u0012\n1;\n\u0014α1\n1\nβ1\n1\nβ1\n2\nα1\n2\n\u0015\n,\n\u0014α2\n1\nβ2\n1\nβ2\n2\nα2\n2\n\u0015\u0013\n= max\n(\nα1\n1\nα1\n1 + β1\n1\n\u0014\nr1\n11 + V\n\u0012\n1;\n\u0014α1\n1 + 1\nβ1\n1\nβ1\n2\nα1\n2\n\u0015\n,\n\u0014α2\n1\nβ2\n1\nβ2\n2\nα2\n2\n\u0015\u0013\u0015\n+\nβ1\n1\nα1\n1 + β1\n1\n\u0014\nr1\n12 + V\n\u0012\n2;\n\u0014α1\n1\nβ1\n1 + 1\nβ1\n2\nα1\n2\n\u0015\n,\n\u0014α2\n1\nβ2\n1\nβ2\n2\nα2\n2\n\u0015\u0013\u0015\n,\nα2\n1\nα2\n1 + β2\n1\n\u0014\nr2\n11 + V\n\u0012\n1;\n\u0014α1\n1\nβ1\n1\nβ1\n2\nα1\n2\n\u0015\n,\n\u0014α2\n1 + 1\nβ2\n1\nβ2\n2\nα2\n2\n\u0015\u0013\u0015\n+\nβ2\n1\nα2\n1 + β2\n1\n\u0014\nr2\n12 + V\n\u0012\n2;\n\u0014α1\n1\nβ1\n1\nβ1\n2\nα1\n2\n\u0015\n,\n\u0014α2\n1\nβ2\n1 + 1\nβ2\n2\nα2\n2\n\u0015\u0013\u0015 )\n.\nAn optimal policy can be computed using the dynamic programming on the augmented\nMDP. However, as each transition can increment any of the information state parameters\nwith every time step, there is an exponential increase in number of distinct reachable hyper-\nstates with the time horizon (4depth hyperstates at a given depth for the above example).\nHence, in the Bayesian RL formulation the exploration-exploitation trade-oﬀis handled\nin a principled manner, because the agent does not heuristically choose between exploiting\nor exploring, rather, it takes an optimal action (that might lead to a mixture of exploration\nand exploitation) with respect to its full Bayesian model of the uncertain sequential decision\nprocess (Duﬀ, 2002; Poupart et al., 2006). In Bayesian decision theory, the optimal action\nis the one that, over the entire time horizon considered, maximizes the expected reward,\naveraged over the possible world models. Any gain in reducing the uncertainty over the\nposterior transition models is not valued just for its own sake, but rather is driven by the\npotential gain in the future reward that it oﬀers.\nThe major problem with the BAMDPs is that the number of hyperstates grows exponen-\ntially with the time-horizon. This exponential growth in hyperstates limits the scalability\nof this approach as it can make solving the problem intractable when either the size of state\nand action spaces increases or the planning horizon increases. In the next section we will\ngo over some of the works that provide tractable computational procedures that retain the\nBayesian formulation but sidesteps the intractability by employing various approximation\nand sampling techniques.\n7.2 Bayesian Methods\nTo recap, in the Bayes-Adaptive setting, a prior over MDPs is given to the agent, and\n(approximately) optimal decisions are made based on the posterior over MDPs, where the\nposterior is a function of the interaction history with the MDP. In this section we review a\nfew notable works that are based on the diﬀerent BAMDP formulations, to illustrate how\nthe explosion of the hyperstate space is handled to provide tractable approximate solutions.\n48\nA Survey of Exploration Methods in Reinforcement Learning\nFor a thorough insight into diﬀerent techniques for tractability in Bayesian methods for RL\nwe refer the reader to the survey by Ghavamzadeh et al. (2015, Chapter 4).\nBayesian Q-Learning (Dearden et al., 1998) adopts a Bayesian approach to the Q-\nlearning by maintaining and propagating the probability distributions to represent the un-\ncertainty over the agent’s estimates of Q-values. Under certain modeling assumptions, the\nauthors show that Dirichlet distributions can be used to maintain such a posterior over the\nQ-values. Instead of solving the BAMDP using dynamic programming as in the previous\nSection 7.1, the authors propose an action selection procedure based on the ‘value of in-\nformation’ - the expected gain in future decision quality that might arise from information\nacquired from the current action choice. Intuitively, this notion considers the gain that can\nbe acquired learning the true value of a particular Q-value. Formally, let a1 and a2 denote\nthe actions with best and second best expected values respectively, and q⋆\ns,a denote a ran-\ndom variable representing the a possible value of Q⋆(s, a) in some MDP, then the gain from\nlearning the true value is deﬁned as:\nGains,a(q⋆\ns,a) =\n\n\n\n\n\nE[q(s, a2)] −q⋆\ns,a\nif a = a1 and q⋆\ns,a < E[qs,a2],\nq⋆\ns,a −E[q(s, a1)]\nif a ̸= a1 and q⋆\ns,a > E[qs,a1],\n0\notherwise\n(28)\nAs the agent doesn’t know the true value of q⋆\ns,a, the expected gain is computed using the\nprior beliefs to estimate the Value of Perfect Information (VPI):\nVPI(s, a) =\nZ +∞\n−∞\nGains,a(x)Pr(qs,a = x)dx,\n(29)\nThe value of perfect information gives an upper bound on the myopic (1-step) expected value\nof information for exploring with action a. In order to take into account the exploitation\naspect, the expected reward is also added to the action selection criteria. Therefore, the\ngoal is to select action that maximizes (E[qs,a] + VPI(s, a)).\nOnce the action is taken and transitions are observed, the authors propose two ways of\nestimating the distribution of the Q-value. The ﬁrst one is based on Moment matching that\nleads to a closed-form update but can become overly conﬁdent. The second technique is\nbased on mixture updates that are more cautious but require numerical integration. Finally,\nthey provide some theoretical results on the convergence of the algorithm and then they\nconclude with some experimental results on three toy problems (a 5-state chain MDP, an\n8-state loop MDP and a 2D-maze of size 8x8) and compare their work with three other\nmethods.\nOptimal Probe (Duﬀ, 2003) retains the full Bayesian framework but proposes to sidestep\nthe intractable calculations by using a novel actor-critic architecture and proposing a corre-\nsponding policy-gradient based update for it. The policies and value functions are approxi-\nmated by functions involving linear combinations of the information state components. The\nmain assumption is that the value function is a relatively smooth function of the information\nstate, x. For the feature set, they propose using the components of the information state.\nAs such, the critic is parameterized as: V (s, x) ≈Vs(x) = P\nl θl[s]xl, where Vs represents\nthe function approximator associated with the state in the original MDP s, and θl repre-\nsents the parameters corresponding to the l-th information state component. For policies,\n49\nxxxx\nApproach\nChoice of approximation\nSolution method\nDearden et al. (1998)\nOnline myopic value function\nDynamic programming\nDuﬀ(2002)\nOﬄine value function\nPolicy gradient\nWang et al. (2005)\nOnline tree search\nTree backups, myopic\nheuristic at leaves\nPoupart et al. (2006)\nOﬄine value function\nPoint-based POMDP methods\nGuez et al. (2012)\nOnline tree search\nQ-learning with rollout policy\nTable 7: Overview of the main techniques covered in Section 7.2\nthey propose using a separate parameterized function approximator for each original state\ns and possible action. This is because the stochastic policies that map from hyperstates to\nactions are required to be relatively smooth over the information state x, but should allow\narbitrary and discontinuous variation with respect to the original state and the prospective\naction. This reduces the size of the class of stochastic policies that the actor can model,\nbut the hope is that this parameterized family of policies will be rich enough to represent\nnear-optimal policies.\nFor updating the policy, a Monte-Carlo based Policy gradient update rule is proposed\nthat uses a single hyperstate trajectory for providing an unbiased estimate of the gradient\ncomponents with respect to the actor’s parameters. In conclusion, the assumed function class\nfor the actor introduces a bias but makes the complexity independent of the (exponential)\nnumber of hyperstates. Policy gradients can be expressed in terms of a matrix representing\nthe steady-state probability of hyperstates. This matrix is again computationally intractable\nbut can be approximated using sampled roll-outs. They test their approach a 2-states and\n2 actions toy MDP with a horizon of 25 and were the ﬁrst to show a tractable algorithm for\nthat case. This method however is only computationally feasible for domains with a small\nnumber of information states where the proposed architecture works.\nSparse sampling (Kearns et al., 2002) is a sample-based tree search algorithm, where\nthe agent samples the next possible tree nodes from each state and then applies Bellman\nbackup to propagate the values of child nodes to the parent node.\nIn Bayesian Sparse\nSampling (Wang et al., 2005) the authors apply the sparse sampling technique to search\nover BAMDPs, where the task is to use lookahead search to estimate the long term value of\npossible actions in a given belief state. The key idea is to exploit information in the Bayesian\nposterior to make intelligent action selection decisions during the look-ahead simulation,\nrather than simply enumerating over all the actions or selecting the actions myopically.\nThe search tree is expanded adaptively in a non-uniform manner, instead of building a\nuniformly balanced look-ahead tree. The intuition is that the agent only needs to investigate\nactions that are potentially optimal, and in this way can save computation resources on\nsub-optimal actions. At each decision node, a promising action is selected using a heuristic\nbased on Thompson sampling (Thompson, 1933) to preferentially expand the tree below\nactions that appear to be locally promising. At each branch node, a successor belief-state\nis sampled from the transition dynamics of the belief-state MDP. Once chosen, the action\nis executed, and a new belief state is entered. As the focus of the work is to demonstrate\naction selection improvements, they compare their approach with other selection schemes\nlike standard Sparse sampling, Thompson sampling, Interval Estimation, and Boltzmann\n50\nA Survey of Exploration Methods in Reinforcement Learning\nexploration techniques in the continuous 2-dimensional action space Gaussian processes\ntask. The results show that their approach yields improved action selection quality whenever\nBayesian posteriors can be conveniently calculated.\nPoupart et al. (2006), focus on the problem of discrete Bayesian model-based RL in\nthe online setting, and propose the BEETLE (Bayesian Exploration Exploitation Tradeoﬀ\nin LEarning), a point-based value iteration algorithm, that takes into account exploration\nduring the exploitation step itself using the belief states mechanism. The main contribution\nof the work is that they present an analytical derivation of the optimal value functions for\nthe discrete Bayesian RL problem where the optimal value function is parameterized by a\nset of multivariate polynomials. This analytical form then allows them to build an eﬃcient\npoint-based value iteration algorithm that exploits the particular form of parameterization.\nAs a result, they have a computationally eﬃcient oﬄine policy optimization technique and\nresults that show the optimization remains eﬃcient as long as the number of unknown\ntransition dynamics parameters remains small.\nThe results are based on the argument\nthat the transition dynamics of many problems can be encoded with few parameters by\neither tying the parameters together or using a factored model. The algorithm achieves\nonline eﬃciency, by moving the policy optimization oﬄine and doing only action selection\nand belief monitoring at run time. The authors compare their proposed method with two-\nheuristics based exploration approaches on two discrete toy MDPs benchmarks: a toy-chain\nwith 5 states and 2 actions and an assistive technology scenario MDP with 9 stats and 6\nactions.\nThe Bayes-Adaptive Monte Carlo Planning (BAMCP) algorithm Guez et al. (2012)\nprovides a sample-based method for approximate Bayes-optimal planning for discrete MDPs\nthat exploits Monte-Carlo tree search (MCTS). The core idea is to use the UCT algorithm\n(Kocsis and Szepesvári, 2006) in a computationally eﬃcient manner for BAMDPs, where the\nbelief state is approximated using the samples sampled only at the root node of the tree. The\nauthors propose Bayes-Adaptive UCT, where instead of integration over all the transition\nmodels, or even approximating this expectation using an average of sampled transition\nmodel, only a single transition model Pi is sampled from the agent’s current belief (posterior\nat the root of the search tree) and is used to simulate all the necessary samples during this\nepisode. A tree policy then treats the forward search as a meta-exploration problem, in a\nsimilar manner as the vanilla UCT problem. The goal of the tree policy is to exploit regions\nof the search tree that appear better than the others while continuing to explore the less\nknown parts of the tree. For the exploitation part, a rollout policy is learned in a model-\nfree manner, using Q-learning, from the samples collected by the agent as a result of the\ninteraction with the environment. In order for further computational eﬃciency, the authors\npropose a novel lazy sampling scheme for the partial transition models. The intuition is that\nif the transition parameters for diﬀerent states and actions are independent, then instead of\nsampling a complete P, only the parameters necessary for individual state-action pairs can\nbe sampled. The returns from each episode are then used to update the value of each node\nin the search tree during the planning. By integrating over many simulations, and therefore\nmany sampled MDPs, the optimal value of each future sequence is obtained with respect to\nthe agent’s belief.\nThe authors compare their method with BOSS (Asmuth et al., 2009) and BEB (Kolter\nand Ng, 2009) in the discrete grid-world domain (10 x 10 states) and loop domain with 9\n51\nxxxx\nstates and show their method outperforms the others. They also test their method on an\ninﬁnite 2D grid-world domain where they show that their method greatly outperforms the\nother baselines. In the inﬁnite 2D grid domain the baselines can not handle the large state\nspace but as BAMCP limits the posterior inference to the root of the search tree it is not\ndirectly aﬀected by the size of the state space, but instead is limited by the planning time.\n7.3 Meta-learning\nSome exploration strategies are based on, or emerge from, a meta learning perspective.\nMeta learning focuses on learning an appropriate bias from a collection of tasks that allows\nmore eﬃcient learning on new, similar tasks (Vilalta and Drissi, 2002). The ﬁeld of meta-\nreinforcement learning uses this approach in reinforcement learning settings. As such, the\nagent interacts with multiple train MDPs, allowing it to learn a strategy for interacting\nwith eventual novel test MDPs from the same family. At ‘meta-train time’ the agent learns\ngeneral patterns from a set of train tasks, that can be exploited to learn more eﬃciently at\n‘meta-test time’. Usually, the train and test tasks are assumed to be drawn from the same\ndistribution. Thus, the agent can tailor the learning strategy employed at ‘meta-test time’\nusing inductive biases extracted at ‘meta-train time’. As a simple example, the training\nMDPs might be used to optimize the learning rate used on testing MDPs. However, much\nmore complex meta-strategies might also be learned, including the update rule itself.\nBefore we give a more detailed breakdown of meta-reinforcement learning methods and\nexploration strategies used with or emerging from those methods, it is good to realize the\nconnection between meta-reinforcement learning and Bayes-adaptive reinforcement learning.\nIn Bayes-adaptive learning, a prior over MDPs is known to the agent, and (approximately)\noptimal decisions are made based on the posterior over MDPs. The posterior is a function\nof the interaction history with the MDP. Compare this set-up with a common set-up for\nmeta-reinforcement learning where train and test tasks are assumed to be drawn from the\nsame distribution. An optimized mapping from the interaction history to the next action to\nbe taken can thus be seen to target the same objective as Bayes-adaptive learning, with the\nprior represented by a ﬁnite set of sampled train MDPs. Thus, meta-reinforcement learning\nstrategies have the potential to learn an approximately optimal exploration-exploitation\ntrade-oﬀwith respect to the distribution of training MDPs. Whereas the Bayes-adaptive\nliterature has typically focused on discrete MDPs and tabular representations, most work on\nmeta reinforcement learning focuses on the function approximation case, using deep neural\nnetworks to represent policies or value functions.\nMeta-reinforcement learning techniques can be classiﬁed based on the amount of struc-\nture used in deﬁning the mapping from interaction history to actions (Finn, 2018). At the\nextreme, such policies can be black boxes directly mapping from interaction histories to\nactions. We can think of this black box as combining two functions usually implemented\nby separate functions: updating the policy, and executing the policy in the current state.\nThus, black box meta-reinforcement learning approaches are sometimes described as learning\na reinforcement learning algorithm (Wang et al., 2016a; Duan et al., 2016b).\nOther classes of meta-reinforcement learning tasks impose more structure on the mapping\nfrom interaction histories to actions.\nOne common type of structure is that the policy\nupdate is given by gradient ascent. The agent’s objective then becomes ﬁnding prior policy\n52\nA Survey of Exploration Methods in Reinforcement Learning\nBlack box methods\nMeta-learned object\nHeess et al. (2015)\nRecurrent policy\nDuan et al. (2016b)\nRecurrent policy\nWang et al. (2016a)\nRecurrent policy\nGarcia and Thomas (2019)\nAdviser policy\nAlet et al. (2020)\nBonus mechanism\nGradient-based methods\nExploration characteristics\nFinn et al. (2017)\nEarly gradient-based approach\nStadie et al. (2018)\nMore credit to pre-update policies\nRothfuss et al. (2019)\nLow-variance, improved action-level credit assignment.\nFrans et al. (2018)\nEﬃcient exploration through meta-learning sub-policies\nInference-based methods\nInference type\nUse of posterior\nGupta et al. (2018)\nTest-time approximate inference\nPosterior sampling\nRakelly et al. (2019)\nAmortized inference\nPosterior sampling\nZintgraf et al. (2019b)\nAmortized inference\nConditioning on varia-\ntional parameters\nTable 8: Overview of the main techniques covered in this section.\nparameters such that a few gradient steps result in good ‘post-update’ policies (Finn et al.,\n2017). Another common type of structure is when, similar to Bayes-adaptive approaches,\nthe inference of the posterior distribution over MDPs is decoupled from the action selection\nmechanism (Rakelly et al., 2019; Zintgraf et al., 2019a).\nPer category, we will now discuss how exploration can be done. In our review, we will\nfocus on papers that explicitly introduce exploration methods or explicitly discuss or analyze\nexploration behavior in these methods. Table 8 provides an overview of the main methods\ndiscussed and some of their characteristics.\nBlack-box methods\nIn black-box models, a mapping from interaction history to actions\nis optimized directly. The interaction history could comprise states or observations, actions,\nand rewards at all previous time steps, or even previous episodes in the same MDP. Since\ninteraction histories are sequences without a ﬁxed size, recurrent neural networks are a\npopular architecture to represent policies or value functions. Conceptually, these methods\nare quite straightforward: in theory any well-known policy search method could be used,\nusing a recurrent architecture and with interaction histories rather than single states as\ninput. A generic policy for a black-box method is of the functional form\nπθ(At|St, Ht),\nHt = [S0, A0, R0, . . . , St−1, At−1, Rt−1, St],\nwith πθ the (usually recurrent) policy architecture parametrized by θ and Ht the interaction\nhistory up to time t.\nEarly work on this approach was performed by Heess et al. (2015), who looked at dif-\nferent kinds of partial observability, including a BAMDP-like setting where the agent had\nto explore a tank of water for a hidden platform, and thereafter exploit this knowledge to\nﬁnd the platform again. Work by Duan et al. (2016b) and Wang et al. (2016a) further\ninvestigated this type of approach. These works also formalized the idea of “learning a re-\n53\nxxxx\ninforcement learning algorithm”. Although the framework is roughly similar, the methods\ndiﬀer in design choices such as which base reinforcement learning method is used (A2C/A3C\n(Mnih et al., 2016) versus TRPO (Schulman et al., 2015)). Both papers look at the explo-\nration/exploitation trade-oﬀin bandits and visual navigation tasks compared to classical\nexploration methods based on Thompson sampling or exploration bonuses. On the discrete\ntasks with known dynamics, the meta-learning approach performs competitively with clas-\nsical methods, but it is applicable to the challenging visual exploration task where these\ntabular methods are not applicable. In addition, Duan et al. (2016b) investigate exploration\nbehavior on tabular MDPs, and Wang et al. (2016a) investigate multiple tasks inspired by\nbehavioral science and neuroscience paradigms. Their ‘dependent arms’ bandit experiments\nreveal that the method successfully learns the optimal exploration/exploitation strategy for\nthis particular family of bandits, where after one exploration action without any reward the\nagent switches to pure exploitation behavior. This exploration behavior is not matched by\nthe considered classical bandit algorithms. Overall, the black-box model is very general as\nwell as conceptually straightforward, however, training the recurrent models tends to take\na lot of samples and training time.\nAn interesting approach is that by Garcia and Thomas (2019). Here, an ‘advisor’ policy\nis learned in a black-box fashion. Since the black box environment contains a base reinforce-\nment learning algorithm (such as REINFORCE (Williams, 1992) or PPO Schulman et al.\n(2017)), the meta state becomes a combination of the current MDP, the MDP state, and\nthe base learner’s internal state. This structure is reminiscent of gradient-based methods\n(covered in the next paragraph), although unlike in those methods, gradients are not taken\nthrough the internal update of the base learner. Actions executed in the environment are\nmostly based on those of the base learner, but in a fraction ϵ of time steps an explorative\naction from the ‘advisor’ (meta-policy) is executed (rather than an action chosen uniformly\nas in ϵ-greedy strategies). The authors provide the theoretical result that solving the meta\nMDP indeed results in optimal exploration policies in the sense of maximizing total return\nover a set number of episodes averaged over the prior MDP distribution. Furthermore, they\nshow strong empirical performance in the function approximation setting compared to the\nbase algorithms without advisor, random exploration, and the MAML meta-learning ap-\nproach (Finn et al., 2017, covered in the next section) on continuous control tasks such as\nthe ‘Ant’ task from the Roboschool environment5.\nWhere the methods covered above all meta-learned a policy, Alet et al. (2020) proposed\na diﬀerent approach. Their method meta-learns curiosity mechanisms or bonuses that gen-\neralize across very diﬀerent reinforcement-learning domains. They formulate the problem\nof ﬁnding exploration bonuses as an outer loop that will search over a space of bonuses\n(meta-learned), and an inner loop that will perform standard reinforcement learning us-\ning the adapted reward signal. They propose to do the meta-learning of the bonus in the\ncombinatorial space of programs instead of transferring neural network weights resulting\nin an approach similar to neural architecture search. The programs are represented in a\ndomain-speciﬁc language that includes modular building blocks like neural networks that\ncan be updated with gradient-descent mechanisms, ensembles, buﬀers, etc. They show that\nsearching through a rich space of programs yields novel designs that work better than human-\n5. https://openai.com/blog/roboschool/\n54\nA Survey of Exploration Methods in Reinforcement Learning\ndesigned methods such as those proposed by Pathak et al. (2017); Burda et al. (2018b). At\nthe same time, the proposed approach generalizes across environments with diﬀerent state\nand action spaces, for instance, image-based 2D gridworld games and Mujoco environments\nlike Acrobot.\nGradient-based methods\nGradient-based methods aim to introduce more structure in\nthe meta reinforcement learning problems compared to the discussed black-box methods.\nThese black-box methods in essence learn a RL algorithm speciﬁc to the current distribution\nover MDPs. Thus, it should be no wonder that updates take many steps at meta-train time.\nGradient-based methods introduce prior knowledge about typical reinforcement learning\nupdates in the learning process. These methods try to learn a policy in such a way that a\nfew updates (or even a single one) results in a ‘post-update’ policy that is able to attain high\nexpected returns. As a function of the interaction history, the policy is then of the form\nπθ′(Ht)(At|St),\nθ′(Ht) ←θ + α∇θE\n\"t−1\nX\nu=0\nR(Su, Au)\n#\n,\nwith θ′ the post-update parameters, that result from an inner update using an estimate of\nthe policy gradient, and su, au with u < t taken from the interaction history ht (Finn et al.,\n2017).\nStadie et al. (2018) extended earlier work on gradient-based meta-RL (Finn et al., 2017)\nwith the explicit aim to improve exploration. Where the earlier implementation of model-\nagnostic meta learning (MAML) by Finn et al. (2017) did not properly assign credit to\npre-update trajectories, the proposed algorithm was hypothesized to explore better and\nthus called E-MAML. The proposed approach was tested on benchmark problems including\nmazes and ‘Krazy World’, an environment speciﬁcally designed to test exploration in meta\nlearning. The proposed approach indeed learned faster on these benchmarks. Furthermore,\na separate analysis looked at the diﬀerence in exploration metrics (such as the number of\ngoal states visited) and conﬁrmed the proposed algorithm scored higher. Similar results\nheld for E-RL2, an approach based on RL2 (Duan et al., 2016b), inspired by E-MAML,\nwhich attempts to promote exploration behavior in black-box methods by ignoring rewards\nobtained during exploratory roll-outs.\nMAML and E-MAML were analyzed in more detail by Rothfuss et al. (2019). They\nfound that the MAML formulation takes the internal structure of the policy update better\ninto account. When all terms of the gradient of this formulation are taken into account, one\nshould thus expect better performance. To do so, they propose a low-variance estimator\nof the required Hessian. Their experiments on various locomotion benchmarks conﬁrm this\nperformance improvement. Furthermore, they explicitly analyze exploration behavior. This\nqualitative analysis shows that the original MAML implementation does not learn a good\nexploration strategy, with E-MAML doing better but having a hard time assigning credit\nto individual actions. The proposed LVC estimator, on the other hand, developed good\nexploration behavior and was able to exploit the gleaned information.\nThe approach proposed by Frans et al. (2018) meta-learns a shared hierarchy, meaning\nthat a common set of sub-policies is learned while a master policy that selects between the\nsub-policies is adapted to the meta-test task at hand. This approach can be compared to\nmethods such as the previously discussed MAML (Finn et al., 2017), although here the\n55\nxxxx\nparameters of the shared hierarchy are not updated in the ‘inner’ update, and second order\ngradients are not passed back to the ‘outer’ meta-level updates of the shared hierarchy. Frans\net al. (2018) ﬁnd that the proposed method can successfully learn a meta-structure where\nexploration takes place eﬃciently on the level of the master policy. Furthermore, they ﬁnd\nthat sub-policies learned on small mazes can be transferred eﬀectively to a more challenging\nsparse-reward navigation task.\nInference-based methods\nIn inference-based methods, the inference of the properties\nof the current MDP is decoupled from taking actions in that MDP. This structure mirrors\nclassical strategies for solving POMDPs (Monahan, 1982) or BAMDPs (Strens, 2000; Duﬀ,\n2002). Since meta-learning methods are often applied to problems with continuous states\nand non-linear dynamics, inference over tasks can generally not be performed in closed form.\nInstead, inference-based meta-reinforcement learning algorithms tend to use a learned latent\nembedding space and often employ some form of approximate inference (Gupta et al., 2018;\nRakelly et al., 2019; Zintgraf et al., 2019a). A generic inference-based policy architecture\ncould thus be described as\nπθ(At|St, φ∗),\nφ∗= arg min\nφ KL(qφ(Z)||p(Z|Ht)),\nwith φ∗the optimal parameters for a variational distribution q over latent context variables\nZ.\nModel agnostic exploration with structured noise (MAESN), ﬁnds an approximate pos-\nterior using gradient ascent at meta-test time (Gupta et al., 2018). Latent context variables\ncan then be drawn in a posterior-sampling like manner. These latent variables are constant\nduring an episode and thus allow structured exploration behavior. The experience with\nmeta-train tasks is thus used both to initialize a policy and to acquire a latent exploration\nspace that can inject structured stochasticity into a policy. The method is compared experi-\nmentally to MAML, RL2, and conventional (non-meta) learning strategies. The authors ﬁnd\nthat MAESN strongly outperforms baseline strategies in terms of learning speed and per-\nformance after 100 learning iterations. Furthermore, qualitative analysis shows MAESN is\nable to learn a well-structured latent space that eﬀectively explores in the space of coherent\nstrategies for the trained family of environments.\nBoth other methods perform inference in this latent space by training an amortized\ninference network. Rakelly et al. (2019) optimize this network to directly minimize a chosen\nloss function while staying close to a prior. The resulting posterior is then used for posterior\nsampling. The authors ﬁnd improved results compared to earlier meta-learning methods,\npresumably thanks to the structured and eﬃcient exploration as well as the ability to use\noﬀ-policy data oﬀered by the decoupling of inference and acting.\nThe approach by Zintgraf et al. (2019a), instead, explicitly optimizes the embedding\nspace to decode transition and reward information. Also, instead of posterior sampling,\nthe policy is conditioned on the mean and co-variance of the full variational posterior. In\npreliminary experiments, the authors show that the approximate posterior can be used to\nstrategically and systematically explore gridworlds.\nIn these experiments, the proposed\nmethod outperformed black-box meta-learning methods by a large margin.\n56\nA Survey of Exploration Methods in Reinforcement Learning\n8. Probability Matching\nAn entire body of algorithms for eﬃcient exploration is inspired by the Probability Matching\napproach, also known as Thompson Sampling (Thompson, 1933). Probability matching (or\nThompson Sampling) is a heuristic for balancing the exploration-exploitation dilemma in\nthe Multi-Arm Bandit setting (Li and Chapelle, 2012; Agrawal and Goyal, 2012). In this\nsetting, the agent maintains a posterior distribution over its beliefs regarding the optimal\naction, but instead of selecting the action with the highest expected return according to\nthe belief posterior, the agent selects the action randomly according to the probability with\nwhich it deems that action to be optimal. This approach uses the variance of the posterior\nto induce randomization and incentivizes the exploration of uncertain states and actions.\nAs more experience is gathered, the variance of the posterior will decrease and concentrate\non the true value. Thompson sampling is provably eﬃcient for the bandit setting (Russo\nand Van Roy, 2013).\nWe use the setting from Agrawal and Goyal (2012) to give an example of the Thompson\nSampling algorithm for Bernoulli bandit setting, i.e. when the agent gets a binary reward\n(0 or 1) for selecting an arm i, and the probability of success is µi. The algorithm maintains\nBayesian priors on the Bernoulli means µi. The algorithm initially assumes arm i to have a\nuniform prior on µi (Beta(1, 1)). At time t, having observed Si(t) successes and Fi(t) failures\nplays of the arm i, the algorithm corresponding updates distribution on µi as Beta(Si(t) +\n1, Fi(t) + 1).\nThe algorithm then samples the model of the means from these posterior\ndistributions of the µi’s and plays an arm according to the probability of its mean being the\nlargest.\nPosterior Sampling for Reinforcement Learning (PSRL)\nBayesian dynamic pro-\ngramming was ﬁrst introduced in Strens (2000) and is more recently known as posterior\nsampling for reinforcement learning (PSRL) (Osband et al., 2013). PSRL can be thought\nof as an extension of the Thompson Sampling algorithm to the RL setting with ﬁnite state\nand action spaces. Compared to Thompson Sampling, where a model is re-sampled at ev-\nery time-step6, PSRL samples a single model for an episode and follows this policy for the\nduration of the episode.\nIn PSRL, the agent starts with a prior belief over the model of the MDP and then pro-\nceeds to update its full posterior distribution over models with the newly observed samples.\nFor each episode, a model hypothesis is then sampled from this distribution, and traditional\nplanning techniques are used to solve the MDP and obtain the optimal value function. For\nthe current episode, the agent follows the greedy policy with respect to the optimal value\nfunction. They evaluate their approach on a 6-state chain MDP with 3 actions and a random\nMDP with 10 state and 5 actions. They show that PSRL outperforms UCRL2 by a large\nmargin in both the above domains.\nAlthough, both categories of the methods maintain a distribution over the rewards and\ntransition dynamics obtained using a Bayesian modeling approach, PSRL based methods\nemploy the posterior sampling exploration algorithm that requires solving for an optimal\npolicy for a single MDP in each iteration. As such, PSRL is more computationally eﬃcient\ncompared to typical Bayesian-Adaptive algorithms that ﬁnd optimal exploration strategy\n6. In the bandit setting the length of an episode is 1 time-step.\n57\nxxxx\nvia either dynamic programming or tree look-ahead in the Bayesian belief state space over\na set of a prior distribution over MDPs.\nBest of Sampled Set (BOSS) (Asmuth et al., 2009) drives exploration by sampling mul-\ntiple models from the posterior and combining them to select actions optimistically. The\nproposed algorithm resembles RMAX (Brafman and Tennenholtz, 2002) in the sense that\nit samples multiple models from the posterior only when the number of transitions from a\nstate-action pair exceeds a certain threshold. The sampled models are then merged into an\noptimistic MDP which is solved to select the best action. They show that this approach leads\nto suﬃcient exploration to guarantee ﬁnite-sample performance guarantees. They compare\ntheir approach against BEETLE and RMAX and show superior results on the 5-state chain\nproblem and 6x6 grid-world.\nAgrawal and Jia (2017) propose an algorithm based on posterior sampling that achieves\nnear-optimal worst-case regret bounds when the underlying MDP is communicating with\n(unknown) ﬁnite diameter. The diameter D is deﬁned as an upper bound on the time it\ntakes to move from any state s to any other s′ using an appropriate policy, for each pair of\ns, s′. The algorithm combines the optimism in the face of uncertainty principle (Section 6)\nwith the posterior sampling heuristic.\nThe algorithm proceeds in epochs, where, at the\nbeginning of each epoch the algorithm generates ψ = ˜O(S) sample transition probability\nvectors from a posterior distribution for every state and action. It then proceeds to solve\nthe extended MDP with ψA actions and S states formed using these samples. The optimal\npolicy found from the extended MDP is then used for the entire epoch. This algorithm can\nbe viewed as a combination of methodologies from BOSS and PSRL algorithms described\nabove. The main contribution of this work is providing tighter regret bounds, and as such,\nthey do not provide any experimental results for their algorithm.\nRandomized Value Functions\nThe PSRL approach is limited to the ﬁnite state and\naction setting, where learning the model and planning might be tractable. For the rest of\nthe section, we will look at the approaches that aren’t based on modeling the MDP tran-\nsition and rewards explicitly, but instead focus on estimating distributions over the value\nfunctions directly. The underlying assumption of these approaches is that approximating\nthe posterior distribution for the value function is more statistically and computationally\neﬃcient than learning the MDP. Osband et al. (2016c, 2017) proposed a family of methods\ncalled Randomized Value Functions (RVFs) in order to improve the scalability of PSRL. At\nan abstract level, RVFs can be interpreted as a model-free version of PSRL. These methods\ndirectly model a distribution over the value functions instead of over MDPs. The agent then\nworks by sampling a randomized value function at the beginning of each episode and follow-\ning that for the rest of the episode. Exploring with dithering strategies (Sec. 4.1, 5, 5.2), is\nineﬃcient as the agent may oscillate back and forth, it might not be able to discover tem-\nporally extended interesting behaviours. On the other hand, exploring with Randomized\nValue Functions, the agent is committed to a randomized but internally consistent strategy\nfor the entire length of the episode. The switch to value function modelling also facilitates\nthe use of function approximation.\nIn order to scale posterior sampling approach to large MDPs with linear function approx-\nimation, Osband et al. (2016b) introduce Randomized Least Square Value Iteration (RLSVI)\nthat involves using Bayesian linear regression for learning the value function. The goal is to\n58\nA Survey of Exploration Methods in Reinforcement Learning\nextend PSRL to value function learning, that would involve maintaining a belief distribution\nover candidates for the optimal value function. Before each episode, the agent would then\nsample a value function from its posterior distribution and then apply the associated greedy\npolicy throughout the episode.\nLeast Square Value Iteration (LSVI) (Sutton and Barto, 1998a; Szepesvári, 2010) per-\nforms a linear regression for the Bellman error at each timestep - similar to Fitted Q-Iteration\n(Riedmiller, 2005). As the value function learned from LSVI has no notion of uncertainty,\nalgorithms based on just LSVI have to rely on other exploration strategies, like blind ex-\nploration (Sec. 4.1). RLSVI also performs linear regression for one-step Bellman error but\nit incorporates a Gaussian uncertainty estimate for the resultant value function. This is\nequivalent to replacing the linear regression step of LSVI with a Bayesian linear regression\nas if the one-step Bellman error was sampled from a Gaussian distribution. Even though\nthis is not the correct Bayesian distribution, Osband et al. (2016b) show that it is still useful\nfor approximating the uncertainty. As the RLSVI updates the value function based on a\nrandom sample from this distribution, the resultant value function is also a random sample\nfrom the approximate posterior. RLSVI is a provably eﬃcient algorithm for exploration in\nlarge MDPs with linear value function approximators (Osband et al., 2017). The authors\ncompare their approach with the dithering based strategies in a didactic chain environment\nwhere RLVSI is able to scale up to 100 state length chain. They also show better learning\nperformance of RLVSI compared LSPI and LSVI on learning to play Tetris task, and a rec-\nommendation engine task, both of which have exponential state space but they have access\nto the appropriate basis functions for the task.\nOne of the problems with this approach is that the distributions over the value functions\ncan be as complex to represent as distributions over transition model, and exact Bayesian\ninference might not be computationally tractable. RLSVI does not explicitly maintain and\nupdate belief distributions, as a coherent Bayesian method would, but still serves as a\ncomputationally tractable method for sampling value functions.\nOsband et al. (2016a) propose bootstrapped Q-learning, an RVF based approach, as\nan extension of RLSVI to nonlinear function approximators. Bootstrapped-DQN consists\nof a simple non-parametric bootstrap7 with random initialization to generate approximate\nposterior samples over Q-values. This technique helps in the scenario where exact Bayesian\ninference is intractable, such as in deep networks. Bootstrapped-DQN consists of a network\nwith K bootstrapped estimates of the Q-function, trained in parallel. This means that each\nQ1, . . . , QK provide a temporally extended (and consistent) estimate of the value uncertainty.\nAt the start of each episode, the agent samples one head, which it follows for the duration\nof the episode. Bootstrapped-DQN is a non-parametric approach to uncertainty estimation.\nThe authors also show that, when used with deep neural networks, the bootstrap can produce\nreasonable estimates of uncertainty. They compare their method against DQN Mnih et al.\n(2015) on the didactic chain MDP with 100 states, and on the Atari domain on the Arcade\nLearning Environment (Bellemare et al., 2013), where they show that Bootstrapped-DQN\nis able to learn faster and also improves the ﬁnal score in most of the games.\nMany other exploration methods, such as (Azizzadenesheli et al., 2018; Touati et al.,\n2019) can be interpreted as combining the concept of RVF with neural network function\n7. Bootstrap uses the empirical distribution of a sampled dataset as an estimate of the population statistic.\n59\nxxxx\nApproach\nPosterior Sampling\nTheoretical properties\nStrens (2000); Osband\net al. (2013); Osband\nand Van Roy (2017)\nSample 1 MDP model per episode\nBounded Expected regret\n˜O(HS\n√\nAT),\nBounded Bayesian regret\n˜O(H\n√\nSAT)\nAsmuth et al. (2009)\nSample K MDP models per step\nPAC-MDP\nOsband et al. (2016b)\nSample 1 value function per episode\nBounded Expected regret\n˜O(\n√\nH3SAT)\nAgrawal and Jia (2017)\nSample ˜O(S) transition probability\nvectors per epoch\nBounded Worst-case regret\n˜O(D\n√\nSAT)\nOsband et al. (2016a)\nSample 1 head Q-network from the\nbootstrap ensemble per episode\n-\nAzizzadenesheli et al.\n(2018)\nSample weights for last layer of Q-\nnetwork per episode\n-\nTouati et al. (2019)\nSample noise variables for the nor-\nmalizing ﬂow per episode\n-\nJanz et al. (2019)\nSample weights for the Q-network\nbased on the successor features for\nevery episode\n-\nTable 9: Overview of the main techniques covered in Section 8. For the theoretical properties\ncolumn, S and A denote the cardinalities of the state and action spaces, T denotes time\nelapsed, H denotes the episode duration, and D denotes the diameter.\n60\nA Survey of Exploration Methods in Reinforcement Learning\napproximation. It allows these methods to scale to high-dimensional problems, such as Atari\ndomain (Bellemare et al., 2013), that otherwise might be too computationally extensive\nfor PSRL. However, the approximations introduced in these works come with trade-oﬀs\nthat are not present in the original PSRL work. Speciﬁcally, because a value function is\ndeﬁned with respect to a particular policy, constructing posterior over the value functions\nrequires selection of a reference policy or distribution over policies. However, in practice,\nthe above methods do not enforce any explicit structure or dependencies. Janz et al. (2019)\npropose Successor Uncertainties (SU), a scalable and computationally cheaper (compared to\nBootstrapped DQN) model-free exploration algorithm that retains the key elements of the\nPSRL. SU models the posterior over rewards and state transitions directly and derives the\nposterior over the value functions analytically, thereby ensuring the posterior over the values\nestimates matches the posterior sampling policy. Empirically, SU performs much better on\nhard tabular didactic chain problem where the algorithm scales up to chains of length 200\nstates. On the Atari domain, SU outperforms the closest RVF algorithm - Bootstrapped-\nDQN on 36 of 49 games.\n9. Conclusion and Perspectives\nIn this survey, we have proposed a categorization for reinforcement learning exploration\nstrategies based on the information that the agent uses in its exploratory action selection.\nWe divided exploration techniques into two general classes: “Reward-Free Exploration” and\n“Reward-Based Exploration”.\nThe reward-free techniques either select actions totally at\nrandom, or utilize some notion of intrinsic information in order to guide exploration, without\ntaking into account extrinsic rewards. This can be useful in environments where the reward\nsignal is very sparse, and therefore not immediately available to the agent. Reward-based\nexploration methods leverage the information related to the reward signal and can further be\ndivided into the “memory-free”, which only take into account the state of the environment,\nand “memory-based\", which consider additional information about the history of the agent’s\ninteraction with the environment. In each of these categories, methods which share similar\nproperties are clustered together in one subcategory. The basis for this clustering is mainly\nthe type of information used, as well as the way it is used in the selection of exploratory\nactions. We have discussed these exploration methods and have pointed out their strengths\nand limitations, as well as improvements that have been made and some which are still\npossible. We would like to emphasize that our goal was not to review the theoretical sample\ncomplexity results, which are abundant in the ﬁeld. Rather, we wanted to provide a big\npicture which captures the current “lay of the land in terms of methods\", and which is\nuseful for practitioners in their choice of methods. We note that theoretical results often\nneed to rely on assumptions about the environment and the RL algorithm, for example\ntabular or linear representations of the value function, or smooth dynamics, which are often\nnot satisﬁed in practice. Nonetheless, exploration methods can still provide useful empirical\nresults even if their theoretical assumptions are not satisﬁed.\nIn this study, we have limited ourselves to sequential decision making in MDPs. We\nhave not covered in detail strategies for POMDPs and bandits, although these settings have\nprovided inspiration for some of methods proposed for the MDP setting. An emerging trend\nis the study of safe exploration methods, but as relatively little is written on them so far, we\n61\nxxxx\nhave not focused on this topic. Finally, considering the large number of yearly publications in\nthis ﬁeld, we have excluded some methods that are similar to the major classes of approaches\nwe discussed.\nOwing to the improved and more accessible computational power in the recent years, the\nnewly proposed exploration techniques have contributed signiﬁcantly to the improvement in\nthe exploration-exploitation dilemma. However, there are major concerns and issues that\nhave not been resolved yet, mainly due to the absence of a consensus over the ways explo-\nration methods can be assessed. For instance, diﬀerent techniques are evaluated according to\ndiﬀerent measures of eﬃciency and performance, such as state coverage, information gain,\nsample eﬃciency, or regret. Furthermore, there is no set of standard experimental tasks\nthat all proposed exploration techniques are evaluated on. This diversity in the methods\nof assessment complicates the comparison of exploration techniques together. Finally, there\nis often some sort of a discrepancy between the theoretical guarantees that a method pro-\nvides and the experimental condition the agent encounters. Consequently, there is often no\nreliable guarantee for the performance of these techniques in more involved environments.\nAddressing these issues could be the focus of future work.\n10. Acknowledgement\nThe authors would like to thank Scott Fujimoto for providing valuable feedback on the early\ndraft of this manuscript. Funding is provided by Natural Sciences and Engineering Research\nCouncil of Canada (NSERC).\n62\nA Survey of Exploration Methods in Reinforcement Learning\nReferences\nYoussef Achbany, Francois Fouss, Luh Yen, Alain Pirotte, and Marco Saerens. Optimal tun-\ning of continual online exploration in reinforcement learning. In International Conference\non Artiﬁcial Neural Networks, pages 790–800. Springer, 2006.\nShipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit\nproblem. In Conference on Learning Theory, pages 39–1, 2012.\nShipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:\nworst-case regret bounds. In Advances in Neural Information Processing Systems, pages\n1184–1194, 2017.\nDavid J Aldous.\nExchangeability and related topics.\nIn École d’Été de Probabilités de\nSaint-Flour XIII—1983, pages 1–198. Springer, 1985.\nFerran Alet, Martin F Schneider, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Meta-\nlearning curiosity algorithms. In International Conference on Learning Representations,\n2020.\nTeresa M Amabile, William DeJong, and Mark R Lepper. Eﬀects of externally imposed\ndeadlines on subsequent intrinsic motivation. Journal of personality and social psychology,\n34(1):92, 1976.\nSusan Amin, Maziar Gomrokchi, Hossein Aboutalebi, Harsh Satija, and Doina Precup.\nLocally persistent exploration in continuous control tasks with sparse rewards.\narXiv\npreprint arXiv:2012.13658, 2020.\nCharles W Anderson. Learning and problem solving with multilayer connectionist systems.\nPhD thesis, University of Massachusetts at Amherst, 1986.\nAndrás Antos, Csaba Szepesvári, and Rémi Munos. Fitted q-iteration in continuous action-\nspace mdps. In Advances in neural information processing systems, pages 9–16, 2008.\nJohn Asmuth, Lihong Li, Michael L Littman, Ali Nouri, and David Wingate. A Bayesian\nsampling approach to exploration in reinforcement learning. In Conference on Uncertainty\nin Artiﬁcial Intelligence, pages 19–26, 2009.\nPeter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted rein-\nforcement learning. In Advances in Neural Information Processing Systems, pages 49–56,\n2007.\nMohammad Gheshlaghi Azar, Vicenç Gómez, and Bert Kappen. Dynamic policy program-\nming with function approximation. In Proceedings of the Fourteenth International Con-\nference on Artiﬁcial Intelligence and Statistics, pages 119–127, 2011.\nMohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for\nreinforcement learning. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 263–272. JMLR. org, 2017.\n63\nxxxx\nKamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar.\nEﬃcient ex-\nploration through bayesian deep q-networks.\nIn Information Theory and Applications\nWorkshop (ITA), pages 1–9. IEEE, 2018.\nJ Andrew Bagnell and JeﬀSchneider. Covariant policy search. In Proceedings of the Inter-\nnational Joint Conference on Artiﬁcial Intelligence, 2003.\nPeter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforce-\nment learning in weakly communicating mdps. arXiv preprint arXiv:1205.2661, 2012.\nAndrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements\nthat can solve diﬃcult learning control problems. IEEE transactions on systems, man,\nand cybernetics, 13(5):834–846, 1983.\nAndrew Gehret Barto, Steven J Bradtke, and Satinder P Singh. Real-time learning and con-\ntrol using asynchronous dynamic programming. University of Massachusetts at Amherst,\nDepartment of Computer and Information Science, 1991.\nJoan Bas-Serrano, Sebastian Curi, Andreas Krause, and Gergely Neu. Logistic q-learning.\nTechnical Report arXiv:2010.11151, arXiv, 2020.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi\nMunos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural\nInformation Processing Systems, pages 1471–1479, 2016.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\nCarl A Benware and Edward L Deci.\nQuality of learning with an active versus passive\nmotivational set. American Educational Research Journal, 21(4):755–765, 1984.\nRonen I Brafman and Moshe Tennenholtz.\nR-max-a general polynomial time algorithm\nfor near-optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):\n213–231, 2002.\nJohn S Bridle. Training stochastic model recognition algorithms as networks can lead to\nmaximum mutual information estimation of parameters. In Advances in neural informa-\ntion processing systems, pages 211–217, 1990.\nSébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in multi-armed ban-\ndits problems. In International conference on Algorithmic learning theory, pages 23–37.\nSpringer, 2009.\nYuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A\nEfros.\nLarge-scale study of curiosity-driven learning.\nIn International Conference on\nLearning Representations, 2018a.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random\nnetwork distillation. arXiv preprint arXiv:1810.12894, 2018b.\n64\nA Survey of Exploration Methods in Reinforcement Learning\nScott Burlington and Gregory Dudek. Spiral search as an eﬃcient mobile robotic search\ntechnique. In Proceedings of the 16th National Conf. on AI, Orlando Fl, 1999.\nOlivier Caelen and Gianluca Bontempi. Improving the exploration strategy in bandit al-\ngorithms. In International Conference on Learning and Intelligent Optimization, pages\n56–68. Springer, 2007.\nPierguido VC Caironi and Marco Dorigo. Training q-agents, 1994.\nMoses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings\nof the thiry-fourth annual ACM symposium on Theory of computing, pages 380–388, 2002.\nKamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with\noptimistic actor critic.\nIn Advances in Neural Information Processing Systems, pages\n1785–1796, 2019.\nCédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. GEP-PG: Decoupling exploration\nand exploitation in deep reinforcement learning algorithms. In Proceedings of the 35th\nInternational Conference on Machine Learning, volume 80 of Proceedings of Machine\nLearning Research, pages 1039–1048, 2018.\nEdoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O Stanley,\nand JeﬀClune.\nImproving exploration in evolution strategies for deep reinforcement\nlearning via a population of novelty-seeking agents. In Advances in Neural Information\nProcessing Systems, 2018.\nWill Dabney, Georg Ostrovski, and André Barreto. Temporally-extended {\\epsilon}-greedy\nexploration. arXiv preprint arXiv:2006.01782, 2020.\nPeter Dayan.\nImproving generalization for temporal diﬀerence learning: The successor\nrepresentation. Neural Computation, 5(4):613–624, 1993.\nPeter Dayan and Terrence J Sejnowski. Exploration bonuses and dual control. Machine\nLearning, 25(1):5–22, 1996.\nRichard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In AAAI National\nConference on Artiﬁcial Intelligence, pages 761–768, 1998.\nRichard Dearden, Nir Friedman, and David Andre. Model based Bayesian exploration. In\nConference on Uncertainty in Artiﬁcial Intelligence, pages 150–159. Morgan Kaufmann\nPublishers Inc., 1999.\nEdward Deci and Richard M Ryan. Intrinsic motivation and self-determination in human\nbehavior. Springer Science & Business Media, 1985.\nEdward L Deci. Eﬀects of externally mediated rewards on intrinsic motivation. Journal of\npersonality and Social Psychology, 18(1):105, 1971.\nEdward L Deci. Intrinsic motivation. Plenum Press., New York, NY, US, 1975. ISBN\n978-1-4613-4448-3.\n65\nxxxx\nMarc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for\nrobotics. Foundations and Trends® in Robotics, 2(1–2):1–142, 2013.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.\nBenchmark-\ning deep reinforcement learning for continuous control. In International Conference on\nMachine Learning, pages 1329–1338, 2016a.\nYan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel.\nRL2:\nFast reinforcement learning via slow reinforcement learning.\narXiv preprint\narXiv:1611.02779, 2016b.\nMichael O Duﬀ. Design for an optimal probe. In International Conference on Machine\nLearning, pages 131–138, 2003.\nMichael O’Gordon Duﬀ. Optimal Learning: Computational procedures for Bayes-adaptive\nMarkov decision processes. PhD thesis, University of Massachusetts at Amherst, 2002.\nEyal Even-Dar and Yishay Mansour. Convergence of optimistic and incremental q-learning.\nIn Advances in neural information processing systems, pages 1499–1506, 2002.\nSarah Filippi, Olivier Cappé, and Aurélien Garivier. Optimism in reinforcement learning and\nkullback-leibler divergence. In 2010 48th Annual Allerton Conference on Communication,\nControl, and Computing (Allerton), pages 115–122. IEEE, 2010.\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-agnostic meta-learning for fast\nadaptation of deep networks. In International Conference on Machine Learning, pages\n1126–1135, 2017.\nChelsea B Finn. Learning to Learn with Gradients. PhD thesis, University of California,\nBerkeley, 2018.\nSébastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal ex-\nploration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190,\n2017.\nMeire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian\nOsband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin,\nCharles Blundell, and Shane Legg.\nNoisy networks for exploration.\nIn International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\nid=rywHCPkAW.\nKevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning\nshared hierarchies. In International Conference on Learning Representations, 2018.\nRonan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Eﬃcient bias-span-\nconstrained exploration-exploitation in reinforcement learning. In Proceedings of the In-\nternational Conference on Machine Learning, volume 80, pages 1573–1581, 2018.\nJustin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models\nfor deep reinforcement learning. In Advances in Neural Information Processing Systems,\npages 2577–2587, 2017. URL https://arxiv.org/abs/1703.01260.\n66\nA Survey of Exploration Methods in Reinforcement Learning\nZoltán Gábor, Zsolt Kalmár, and Csaba Szepesvári. Multi-criteria reinforcement learning.\nIn ICML, volume 98, pages 197–205, 1998.\nFrancisco M Garcia and Philip S Thomas. A meta-mdp approach to exploration for lifelong\nreinforcement learning. In Advances in neural information processing systems, 2019.\nMatthieu Geist, Bruno Scherrer, and Olivier Pietquin.\nA theory of regularized markov\ndecision processes. Technical Report arXiv:1901.11275, arXiv, 2019.\nMohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian rein-\nforcement learning: A survey. Foundations and Trends® in Machine Learning, 8(5-6):\n359–483, 2015.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems, pages 2672–2680, 2014.\nGeoﬀrey J Gordon. Stable function approximation in dynamic programming. In Machine\nLearning Proceedings 1995, pages 261–268. Elsevier, 1995.\nRobert Grande, Thomas Walsh, and Jonathan How. Sample eﬃcient reinforcement learning\nwith gaussian processes. In International Conference on Machine Learning, pages 1332–\n1340, 2014.\nWendy S Grolnick and Richard M Ryan. Autonomy in children’s learning: An experimental\nand individual diﬀerence investigation. Journal of personality and social psychology, 52\n(5):890, 1987.\nArthur Guez, David Silver, and Peter Dayan. Eﬃcient bayes-adaptive reinforcement learning\nusing sample-based search. In Advances in Neural Information Processing Systems, pages\n1025–1033, 2012.\nVijaykumar Gullapalli.\nA stochastic reinforcement learning algorithm for learning real-\nvalued functions. Neural networks, 3(6):671–692, 1990.\nYing Guo, Astrid Zeman, and Rongxin Li. A reinforcement learning approach to setting\nmulti-objective goals for energy demand management. International Journal of Agent\nTechnologies and Systems (IJATS), 1(2):55–70, 2009.\nZhaohan Guo and Emma Brunskill. Concurrent PAC RL. In AAAI, pages 2624–2630, 2015.\nAbhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. Meta-\nreinforcement learning of structured exploration strategies. In Advances in Neural Infor-\nmation Processing Systems, pages 5302–5311, 2018.\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learn-\ning with deep energy-based policies. In Proceedings of the International Conference on\nMachine Learning, 2017.\n67\nxxxx\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In Interna-\ntional Conference on Machine Learning, pages 1861–1870, 2018.\nNikolaus Hansen and Andreas Ostermeier.\nCompletely derandomized self-adaptation in\nevolution strategies. Evolutionary computation, 9(2):159–195, 2001.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double\nq-learning. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,\npages 2094–2100, 2016.\nElad Hazan, Sham M Kakade, Karan Singh, and Abby Van Soest. Provably eﬃcient maxi-\nmum entropy exploration. In International Conference on Machine Learning, pages 2681–\n2691, 2019.\nNicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and David Silver. Memory-based\ncontrol with recurrent neural networks. arXiv preprint arXiv:1512.04455, 2015.\nZhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, and Chun-\nYi Lee. Diversity-driven exploration strategy for deep reinforcement learning. Advances\nin Neural Information Processing Systems, 31:10489–10500, 2018.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel.\nVime: Variational information maximizing exploration. In Advances in Neural Informa-\ntion Processing Systems, pages 1109–1117, 2016.\nShin Ishii, Wako Yoshida, and Junichiro Yoshimoto. Control of exploitation–exploration\nmeta-parameter in reinforcement learning. Neural networks, 15(4-6):665–687, 2002.\nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforce-\nment learning. Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.\nDavid Janz, Jiri Hron, Przemysław Mazur, Katja Hofmann, José Miguel Hernández-Lobato,\nand Sebastian Tschiatschek. Successor uncertainties: exploration and uncertainty in tem-\nporal diﬀerence learning. In Advances in Neural Information Processing Systems, pages\n4509–4518, 2019.\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably\neﬃcient? In Advances in Neural Information Processing Systems, pages 4863–4873, 2018.\nYuu Jinnai, Jee Won Park, David Abel, and George Konidaris. Discovering options for\nexploration by minimizing cover time. arXiv preprint arXiv:1903.00606, 2019.\nYuu Jinnai, Jee Won Park, Marlos C Machado, and George Konidaris. Exploration in rein-\nforcement learning with deep covering options. In International Conference on Learning\nRepresentations, 2020.\nNicholas K Jong and Peter Stone.\nModel-based exploration in continuous state spaces.\nIn International Symposium on Abstraction, Reformulation, and Approximation, pages\n258–272. Springer, 2007.\n68\nA Survey of Exploration Methods in Reinforcement Learning\nMichael I Jordan. Generic constraints on underspeciﬁed target trajectories. In International\nJoint Conference on Neural Networks, volume 1, pages 217–225. IEEE Press New York,\n1989.\nLeslie Pack Kaelbling. Learning in embedded systems. MIT press, 1993.\nLeslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning:\nA survey. Journal of artiﬁcial intelligence research, 4:237–285, 1996.\nSham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In\nProceedings of the 20th International Conference on Machine Learning (ICML-03), pages\n306–312, 2003.\nMichael Kearns and Satinder Singh.\nNear-optimal reinforcement learning in polynomial\ntime. Machine learning, 49(2-3):209–232, 2002.\nMichael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-\noptimal planning in large markov decision processes. Machine learning, 49(2-3):193–208,\n2002.\nMichał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski.\nVizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016\nIEEE Conference on Computational Intelligence and Games (CIG), pages 1–8. IEEE,\n2016.\nYoungjin Kim, Wontae Nam, Hyunwoo Kim, Ji-Hoon Kim, and Gunhee Kim. Curiosity-\nbottleneck: Exploration by distilling task-speciﬁc novelty. In International Conference on\nMachine Learning, pages 3379–3388, 2019.\nLevente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European\nconference on machine learning, pages 282–293. Springer, 2006.\nNate Kohl and Peter Stone. Policy gradient reinforcement learning for fast quadrupedal\nlocomotion. In Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE Inter-\nnational Conference on, volume 3, pages 2619–2624. IEEE, 2004.\nJ Zico Kolter and Andrew Y Ng. Near-Bayesian exploration in polynomial time. In Inter-\nnational Conference on Machine Learning, pages 513–520. ACM, 2009.\nJan Koutnik, Faustino Gomez, and Jürgen Schmidhuber. Evolving neural networks in com-\npressed weight space. In Proceedings of the 12th annual conference on Genetic and evolu-\ntionary computation, pages 619–626. ACM, 2010.\nRaksha Kumaraswamy, Matthew Schlegel, Adam White, and Martha White.\nContext-\ndependent upper-conﬁdence bounds for directed exploration. In Advances in Neural In-\nformation Processing Systems, pages 4779–4789, 2018.\nTor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.\n69\nxxxx\nKyungjae Lee, Sungjoon Choi, and Songhwai Oh. Sparse markov decision processes with\ncausal sparse tsallis entropy regularization for reinforcement learning. IEEE Robotics and\nAutomation Letters, 3(3):1466–1473, 2018.\nDouglas B Lenat. AM: An artiﬁcial intelligence approach to discovery in mathematics as\nheuristic search. Technical report, Stanford university, department of computer science,\n1976.\nLihong Li and Olivier Chapelle. Open problem: Regret bounds for thompson sampling. In\nConference on Learning Theory, pages 43–1, 2012.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement\nlearning. In International Conference on Learning Representations, 2016.\nLong-Ji Lin. Self-improving reactive agents: Case studies of reinforcement learning frame-\nworks. In Proceedings of the International Conference on Simulation of Adaptive Behavior:\nFrom Animals to Animats, 1990.\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine learning, 8(3-4):293–321, 1992.\nDaniel Ying-Jeh Little and Friedrich Tobias Sommer. Learning and exploration in action-\nperception loops. Frontiers in neural circuits, 7:37, 2013.\nChunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A compre-\nhensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45(3):\n385–398, 2015.\nManuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer.\nExploration in\nmodel-based reinforcement learning by empirically estimating learning progress. In Ad-\nvances in Neural Information Processing Systems, pages 206–214, 2012.\nMarlos C Machado, Marc G Bellemare, and Michael Bowling.\nA Laplacian framework\nfor option discovery in reinforcement learning. In Proceedings of the 34th International\nConference on Machine Learning, pages 2295–2304, 2017.\nMarlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with\nthe successor representation. Technical Report arXiv:1807.11622, arXiv, 2018a.\nMarlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro,\nand Murray Campbell.\nEigenoption discovery through the deep successor representa-\ntion.\nIn International Conference on Learning Representations, 2018b.\nURL https:\n//openreview.net/forum?id=Bk8ZcAxR-.\nMarlos C Machado, Marc G Bellemare, and Michael Bowling.\nCount-based exploration\nwith the successor representation. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 34, pages 5125–5133, 2020.\n70\nA Survey of Exploration Methods in Reinforcement Learning\nSridhar Mahadevan. Proto-value functions: Developmental reinforcement learning. In Pro-\nceedings of the 22nd international conference on Machine learning, pages 553–560. ACM,\n2005.\nSridhar Mahadevan and Jonathan Connell. Scaling reinforcement learning to robotics by\nexploiting the subsumption architecture. In Machine Learning Proceedings 1991, pages\n328–332. Elsevier, 1991.\nSridhar Mahadevan and Jonathan Connell.\nAutomatic programming of behavior-based\nrobots using reinforcement learning. Artiﬁcial intelligence, 55(2-3):311–365, 1992.\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-\nagent variational exploration. In Advances in Neural Information Processing Systems,\npages 7611–7622, 2019.\nJarryd Martin, S Suraj Narayanan, Tom Everitt, and Marcus Hutter. Count-based explo-\nration in feature space for reinforcement learning. In Proceedings of the 26th International\nJoint Conference on Artiﬁcial Intelligence, pages 2471–2478, 2017.\nWilliam L Maxwell and Jack A Muckstadt. Design of automatic guided vehicle systems. Iie\nTransactions, 14(2):114–124, 1982.\nNicolas Meuleau and Paul Bourgine. Exploration of multi-state environments: Local mea-\nsures and back-propagation of uncertainty. Machine Learning, 35(2):117–154, 1999.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning.\nNature, 518(7540):529–533,\n2015.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu.\nAsynchronous methods for deep\nreinforcement learning.\nIn International conference on machine learning, pages 1928–\n1937, 2016.\nShariq A Mobin, James A Arnemann, and Fritz Sommer. Information-based learning by\nagents in unbounded state spaces. In Advances in Neural Information Processing Systems,\npages 3023–3031, 2014.\nGeorge E Monahan.\nState of the art—a survey of partially observable Markov decision\nprocesses: theory, models, and algorithms. Management Science, 28(1):1–16, 1982.\nAndrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning\nwith less data and less time. Machine learning, 13(1):103–130, 1993.\nAndrew William Moore.\nEﬃcient memory-based learning for robot control. PhD thesis,\nUniversity of Cambridge, 1990.\nJun Morimoto and Kenji Doya.\nAcquisition of stand-up behavior by a real robot using\nhierarchical reinforcement learning. Robotics and Autonomous Systems, 36(1):37–51, 2001.\n71\nxxxx\nMichael C Mozer and Jonathan Bachrach.\nDiscovering the structure of a reactive envi-\nronment by exploration. In Advances in neural information processing systems, pages\n439–446, 1990.\nPaul Munro. A dual back-propagation scheme for scalar reward learning. In Annual Con-\nference of the Cognitive Science Society, pages 165–176, 1987.\nOﬁr Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, and Sergey Levine. Why\ndoes hierarchy (sometimes) work so well in reinforcement learning?\nTechnical Report\n1909.10618, arXiv, 2019.\nArun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessan-\ndro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Pe-\ntersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint\narXiv:1507.04296, 2015.\nGergely Neu, Anders Jonsson, and Vicenç Gómez. A uniﬁed view of entropy-regularized\nMarkov decision processes. Technical Report 1705.07798, arXiv, 2017.\nAndrew Y Ng and Michael Jordan.\nPegasus: A policy search method for large MDPs\nand POMDPs.\nIn Proceedings of the Sixteenth conference on Uncertainty in artiﬁcial\nintelligence, pages 406–415. Morgan Kaufmann Publishers Inc., 2000.\nDerrick Nguyen and Bernard Widrow. The truck backer-upper: An example of self-learning\nin neural networks. In Advanced neural computers, pages 11–19. Elsevier, 1990.\nAli Nouri and Michael L Littman. Multi-resolution exploration in continuous spaces. In\nAdvances in neural information processing systems, pages 1209–1216, 2009.\nRonald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous rein-\nforcement learning. In Advances in Neural Information Processing Systems, pages 1763–\n1771, 2012.\nIan Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for\nreinforcement learning? In International Conference on Machine Learning, pages 2701–\n2710, 2017.\nIan Osband, Daniel Russo, and Benjamin Van Roy. (More) eﬃcient reinforcement learning\nvia posterior sampling. In Advances in Neural Information Processing Systems, pages\n3003–3011, 2013.\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration\nvia bootstrapped DQN.\nIn Advances in neural information processing systems, pages\n4026–4034, 2016a.\nIan Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran-\ndomized value functions. In International Conference on Machine Learning, pages 2377–\n2386, 2016b.\n72\nA Survey of Exploration Methods in Reinforcement Learning\nIan Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via ran-\ndomized value functions.\nIn Proceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning - Volume 48, ICML’16, page 2377–2386.\nJMLR.org, 2016c.\nIan Osband, Daniel Russo, Zheng Wen, and Benjamin Van Roy.\nDeep exploration via\nrandomized value functions. arXiv preprint arXiv:1703.07608, 2017.\nIan Osband, John Aslanides, and Albin Cassirer.\nRandomized prior functions for deep\nreinforcement learning.\nIn Advances in Neural Information Processing Systems, pages\n8617–8629, 2018.\nGeorg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Rémi Munos. Count-based\nexploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.\nPierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for\nautonomous mental development. IEEE transactions on evolutionary computation, 11(2):\n265–286, 2007.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven ex-\nploration by self-supervised prediction. In International Conference on Machine Learning\n(ICML), volume 2017, 2017.\nJason Pazis and Ronald Parr. Pac optimal exploration in continuous space Markov decision\nprocesses. In AAAI, 2013.\nJulien Perez, Cécile Germain-Renaud, Balázs Kégl, and Charles Loomis. Responsive elastic\ncomputing. In Proceedings of the 6th international conference industry session on Grids\nmeets autonomic computing, pages 55–64. ACM, 2009.\nJan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In\nTwenty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2010.\nMatthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen,\nXi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz.\nParameter space\nnoise for exploration. In International Conference on Learning Representations, 2018.\nPascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan. An analytic solution to dis-\ncrete Bayesian reinforcement learning. In Proceedings of the International Conference on\nMachine Learning, pages 697–704. ACM, 2006.\nKate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Eﬃcient\noﬀ-policy meta-reinforcement learning via probabilistic context variables. arXiv preprint\narXiv:1903.08254, 2019.\nTabish Rashid, Bei Peng, Wendelin Boehmer, and Shimon Whiteson. Optimistic explo-\nration even with a pessimistic initialisation.\nIn International Conference on Learning\nRepresentations, 2020.\n73\nxxxx\nAlfréd Rényi et al. On measures of entropy and information. In Proceedings of the Fourth\nBerkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions\nto the Theory of Statistics. The Regents of the University of California, 1961.\nMartin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data eﬃcient neural\nreinforcement learning method.\nIn European Conference on Machine Learning, pages\n317–328. Springer, 2005.\nJustinian P Rosca. Entropy-driven adaptive representation. In Proceedings of the workshop\non genetic programming: From theory to real-world applications, volume 9, pages 23–32.\nCiteseer, 1995.\nJonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp:\nProximal meta-policy search. In International Conference on Learning Representations,\n2019.\nThomas Rückstiess, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi Sun, and Jürgen Schmid-\nhuber. Exploring parameter space in reinforcement learning. Paladyn, 1(1):14–24, 2010.\nDaniel Russo and Benjamin Van Roy.\nEluder dimension and the sample complexity of\noptimistic exploration.\nIn Advances in Neural Information Processing Systems, pages\n2256–2264, 2013.\nRichard M Ryan and Edward L Deci. Intrinsic and extrinsic motivations: Classic deﬁnitions\nand new directions. Contemporary educational psychology, 25(1):54–67, 2000.\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strate-\ngies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864,\n2017.\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas\nNardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon\nWhiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International\nConference on Autonomous Agents and MultiAgent Systems, pages 2186–2188. Interna-\ntional Foundation for Autonomous Agents and Multiagent Systems, 2019.\nJürgen Schmidhuber. Making the world diﬀerentiable: On using self-supervised fully recur-\nrent neural networks for dynamic reinforcement learning and planning in non-stationary\nenvironments. Technical Report FKI-126-90, Technische Universität München, 1990.\nJürgen Schmidhuber. Curious model-building control systems. In Neural Networks, 1991.\n1991 IEEE International Joint Conference on, pages 1458–1463. IEEE, 1991a.\nJürgen Schmidhuber.\nA possibility for implementing curiosity and boredom in model-\nbuilding neural controllers.\nIn Proc. of the international conference on simulation of\nadaptive behavior: From animals to animats, pages 222–227, 1991b.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust\nregion policy optimization. In International Conference on Machine Learning, pages 1889–\n1897, 2015.\n74\nA Survey of Exploration Methods in Reinforcement Learning\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. Technical Report 1707.06347, arXiv, 2017.\nPaul D Scott and Shaul Markovitch. Learning novel domains through curiosity and conjec-\nture. In IJCAI, pages 669–674, 1989.\nFrank Sehnke, Christian Osendorfer, Thomas Rückstieß, Alex Graves, Jan Peters, and Jür-\ngen Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551–559,\n2010.\nTim Seyde, Wilko Schwarting, Sertac Karaman, and Daniela L Rus.\nLearning to plan\nvia deep optimistic value exploration. In Alexandre M. Bayen, Ali Jadbabaie, George\nPappas, Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors,\nConference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine\nLearning Research, pages 815–825. PMLR, 10–11 Jun 2020.\nPranav Shyam, Wojciech Jaśkowski, and Faustino Gomez. Model-based active exploration.\nIn International Conference on Machine Learning, pages 5779–5788, 2019.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-\nmiller. Deterministic policy gradient algorithms. In International Conference on Interna-\ntional Conference on Machine Learning, 2014.\nSatinder Pal Singh. Transfer of learning by composing solutions of elemental sequential\ntasks. Machine Learning, 8(3-4):323–339, 1992.\nBradly C. Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforce-\nment learning with deep predictive models. Technical Report 1507.00814, arXiv, 2015.\nURL http://arxiv.org/abs/1507.00814.\nBradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel,\nand Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement\nlearning. In ICLR Workshop track, 2018.\nJan Storck, Sepp Hochreiter, and Jürgen Schmidhuber. Reinforcement driven information\nacquisition in non-deterministic environments. In Proceedings of the international confer-\nence on artiﬁcial neural networks, Paris, volume 2, pages 159–164. Citeseer, 1995.\nAlexander Strehl and Michael Littman. Exploration via model based interval estimation. In\nInternational Conference on Machine Learning. Citeseer, 2004.\nAlexander L Strehl and Michael L Littman. An analysis of model-based interval estimation\nfor Markov decision processes. Journal of Computer and System Sciences, 74(8):1309–\n1331, 2008.\nAlexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac\nmodel-free reinforcement learning. In Proceedings of the 23rd international conference on\nMachine learning, pages 881–888, 2006.\n75\nxxxx\nMalcolm Strens. A Bayesian framework for reinforcement learning. In International Con-\nference on Machine Learning, pages 943–950, 2000.\nFreek Stulp and Olivier Sigaud. Path integral policy improvement with covariance matrix\nadaptation. In International Conference on Machine Learning, 2012.\nRichard S Sutton. Integrated architectures for learning, planning, and reacting based on\napproximating dynamic programming. In Machine Learning Proceedings 1990, pages 216–\n224. Elsevier, 1990.\nRichard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting.\nACM SIGART Bulletin, 2(4):160–163, 1991a.\nRichard S Sutton. Integrated modeling and control based on reinforcement learning and\ndynamic programming. In Advances in neural information processing systems, pages 471–\n478, 1991b.\nRichard S Sutton. Reinforcement learning architectures. In Proceedings ISKIT’92 Interna-\ntional Symposium on Neural Information Processing. Citeseer, 1992.\nRichard S Sutton.\nGeneralization in reinforcement learning: Successful examples using\nsparse coarse coding. In Advances in neural information processing systems, pages 1038–\n1044, 1996.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Cam-\nbridge, MA: MIT Press, 1998a.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: an introduction mit press.\nCambridge, MA, 1998b.\nRichard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy\ngradient methods for reinforcement learning with function approximation. In Advances\nin neural information processing systems, pages 1057–1063, 2000.\nCsaba Szepesvári. Algorithms for reinforcement learning. Synthesis lectures on artiﬁcial\nintelligence and machine learning, 4(1):1–103, 2010.\nIstván Szita and András Lőrincz. The many faces of optimism: a unifying approach. In\nProceedings of the 25th international conference on Machine learning, pages 1048–1055,\n2008.\nPrasad Tadepalli and DoKyeong Ok. Model-based average reward reinforcement learning.\nArtiﬁcial intelligence, 100(1-2):177–224, 1998.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan,\nJohn Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-\nbased exploration for deep reinforcement learning. In Advances in Neural Information\nProcessing Systems, pages 2750–2759, 2017.\n76\nA Survey of Exploration Methods in Reinforcement Learning\nMandayam A.L. Thathachar and P.S. Sastry. A class of rapidly converging algorithms for\nlearning automata. In IEEE International Conference on Cybernetics and Society, pages\n602–606, 1984.\nEvangelos Theodorou, Jonas Buchli, and Stefan Schaal. A generalized path integral control\napproach to reinforcement learning.\nJournal of Machine Learning Research, 11(Nov):\n3137–3181, 2010.\nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in\nview of the evidence of two samples. Biometrika, 1933.\nSebastian Thrun, Knut Möller, and Alexander Linden. Planning with an adaptive world\nmodel. In Advances in neural information processing systems, pages 450–456, 1991.\nSebastian B Thrun. Eﬃcient exploration in reinforcement learning. Technical Report CMU-\nCS-92-102, Carnegie-Mellon University, 1992.\nSebastian B Thrun and Knut Möller.\nActive exploration in dynamic environments.\nIn\nAdvances in neural information processing systems, pages 531–538, 1992.\nArryon D Tijsma, Madalina M Drugan, and Marco A Wiering.\nComparing exploration\nstrategies for q-learning in random stochastic mazes. In 2016 IEEE Symposium Series on\nComputational Intelligence (SSCI), pages 1–8. IEEE, 2016.\nNaftali Tishby, Fernando C Pereira, and William Bialek.\nThe information bottleneck\nmethod. arXiv preprint physics/0004057, 2000.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,\npages 5026–5033. IEEE, 2012.\nMichel Tokic. Adaptive ε-greedy exploration in reinforcement learning based on value dif-\nferences. In Annual Conference on Artiﬁcial Intelligence, pages 203–210. Springer, 2010.\nMichel Tokic and Günther Palm. Value-diﬀerence based exploration: adaptive control be-\ntween epsilon-greedy and softmax. In Annual Conference on Artiﬁcial Intelligence, pages\n335–346. Springer, 2011.\nAristide Tossou,\nDebabrota Basu,\nand Christos Dimitrakakis.\nNear-optimal opti-\nmistic reinforcement learning using empirical bernstein inequalities.\narXiv preprint\narXiv:1905.12425, 2019.\nAhmed Touati, Harsh Satija, Joshua Romoﬀ, Joelle Pineau, and Pascal Vincent. Random-\nized value functions via multiplicative normalizing ﬂows. UAI, 2019.\nLong Tran-Thanh, Archie Chapman, Enrique Munoz de Cote, Alex Rogers, and Nicholas R\nJennings. Epsilon–ﬁrst policies for budget–limited multi-armed bandits. In Twenty-Fourth\nAAAI Conference on Artiﬁcial Intelligence, 2010.\nPeter Vamplew, Richard Dazeley, and Cameron Foale. Softmax exploration strategies for\nmultiobjective reinforcement learning. Neurocomputing, 263:74–86, 2017.\n77\nxxxx\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al.\nConditional image generation with pixelcnn decoders. In Advances in neural information\nprocessing systems, pages 4790–4798, 2016.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double\nq-learning. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.\nHerke van Hoof, Daniel Tanneberg, and Jan Peters. Generalized exploration in policy search.\nMachine Learning, 106(9-10):1705–1724, 2017.\nKristof Van Moﬀaert, Madalina M Drugan, and Ann Nowé.\nHypervolume-based multi-\nobjective reinforcement learning.\nIn International Conference on Evolutionary Multi-\nCriterion Optimization, pages 352–366. Springer, 2013a.\nKristof Van Moﬀaert, Madalina M Drugan, and Ann Nowé. Scalarized multi-objective rein-\nforcement learning: Novel design techniques. In 2013 IEEE Symposium on Adaptive Dy-\nnamic Programming and Reinforcement Learning (ADPRL), pages 191–199. IEEE, 2013b.\nJoannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical eval-\nuation. In European conference on machine learning, pages 437–448. Springer, 2005.\nRicardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artiﬁcial\nintelligence review, 18(2):77–95, 2002.\nJane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi\nMunos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to rein-\nforcement learn. arXiv preprint arXiv:1611.05763, 2016a.\nTao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans. Bayesian sparse sam-\npling for on-line reward optimization. In Proceedings of the 22nd international conference\non Machine learning, pages 956–963. ACM, 2005.\nYuanhao Wang, Kefan Dong, Xiaoyu Chen, and Liwei Wang. Q-learning with ucb explo-\nration is sample eﬃcient for inﬁnite-horizon mdp. In International Conference on Learning\nRepresentations, 2020.\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.\nDueling network architectures for deep reinforcement learning. In International conference\non machine learning, pages 1995–2003. PMLR, 2016b.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292,\n1992.\nChristopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis,\nKing’s College, Cambridge, 1989.\nPawel Wawrzynski. Control policy with autocorrelated noise in reinforcement learning for\nrobotics. International Journal of Machine Learning and Computing, 5(2):91, 2015.\n78\nA Survey of Exploration Methods in Reinforcement Learning\nMartha White and Adam White. Interval estimation for reinforcement-learning algorithms\nin continuous-state domains.\nIn Advances in Neural Information Processing Systems,\npages 2433–2441, 2010.\nSteven D Whitehead. A complexity analysis of cooperative mechanisms in reinforcement\nlearning. In AAAI, pages 607–613, 1991.\nSteven D Whitehead and Dana H Ballard. Learning to perceive and act by trial and error.\nMachine Learning, 7(1):45–83, 1991.\nMarco Wiering and Jürgen Schmidhuber. Eﬃcient model-based exploration. In Proceedings\nof the Sixth International Conference on Simulation of Adaptive Behavior: From Animals\nto Animats, volume 6, pages 223–228, 1998.\nMarco A Wiering. Explorations in eﬃcient reinforcement learning. PhD thesis, University\nof Amsterdam, 1999.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist rein-\nforcement learning. Machine learning, 8(3-4):229–256, 1992.\nRonald J Williams and Jing Peng. Function optimization using connectionist reinforcement\nlearning algorithms. Connection Science, 3(3):241–268, 1991.\nYifan Wu, George Tucker, and Oﬁr Nachum. The laplacian in rl: Learning representations\nwith eﬃcient approximations. arXiv preprint arXiv:1810.04586, 2018.\nJeremy Wyatt. Exploration and inference in learning from reinforcement. PhD thesis, Uni-\nversity of Edinburgh. College of Science and Engineering. School of Informatics., 1998.\nTianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to explore with meta-policy\ngradient. In Proceedings of the International Conference on Machine Learning, volume 80,\npages 5459–5468, 2018.\nPeng-Yeng Yin. Maximum entropy-based optimal threshold selection using deterministic\nreinforcement learning with controlled randomization. Signal Processing, 82(7):993–1006,\n2002.\nAlexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes\nby relative entropy policy search. In Advances in Neural Information Processing Systems\n26, pages 1583–1591, 2013.\nLuisa Zintgraf, Maximilian Igl, Kyriacos Shiarlis, Anuj Mahajan, Katja Hofmann, and Shi-\nmon Whiteson. Variational task embeddings for fast adaptation in deep reinforcement\nlearning. In Workshop on “Structure & Priors in Reinforcement Learning” at ICLR, 2019a.\nLuisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast\ncontext adaptation via meta-learning. In International Conference on Machine Learning,\nvolume 97, pages 7693–7702, Long Beach, California, USA, 2019b. PMLR.\n79\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-09-01",
  "updated": "2021-09-02"
}