{
  "id": "http://arxiv.org/abs/2109.01533v1",
  "title": "UnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry",
  "authors": [
    "Yiming Tu",
    "Jin Xie"
  ],
  "abstract": "Extensive research efforts have been dedicated to deep learning based\nodometry. Nonetheless, few efforts are made on the unsupervised deep lidar\nodometry. In this paper, we design a novel framework for unsupervised lidar\nodometry with the IMU, which is never used in other deep methods. First, a pair\nof siamese LSTMs are used to obtain the initial pose from the linear\nacceleration and angular velocity of IMU. With the initial pose, we perform the\nrigid transform on the current frame and align it closer to the last frame.\nThen, we extract vertex and normal features from the transformed point clouds\nand its normals. Next a two-branches attention modules are proposed to estimate\nresidual rotation and translation from the extracted vertex and normal\nfeatures, respectively. Finally, our model outputs the sum of initial and\nresidual poses as the final pose. For unsupervised training, we introduce an\nunsupervised loss function which is employed on the voxelized point clouds. The\nproposed approach is evaluated on the KITTI odometry estimation benchmark and\nachieves comparable performances against other state-of-the-art methods.",
  "text": "arXiv:2109.01533v1  [cs.CV]  3 Sep 2021\nUnDeepLIO: Unsupervised Deep Lidar-Inertial\nOdometry\nYiming Tu1,2 and Jin Xie 1,2(B)\n1 PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional\nInformation of Ministry of Education, Nanjing University of Science and Technology,\nNanjing, China\n{tymstudy,csjxie}@njust.edu.cn\n2 Jiangsu Key Lab of Image and Video Understanding for Social Security,\nSchool of Computer Science and Engineering,\nNanjing University of Science and Technology, Nanjing, China\nAbstract. Extensive research eﬀorts have been dedicated to deep learn-\ning based odometry. Nonetheless, few eﬀorts are made on the unsuper-\nvised deep lidar odometry. In this paper, we design a novel framework for\nunsupervised lidar odometry with the IMU, which is never used in other\ndeep methods. First, a pair of siamese LSTMs are used to obtain the ini-\ntial pose from the linear acceleration and angular velocity of IMU. With\nthe initial pose, we perform the rigid transform on the current frame and\nalign it to the last frame. Then we extract vertex and normal features\nfrom the transformed point clouds and its normals. Next a two-branch\nattention module is proposed to estimate residual rotation and transla-\ntion from the extracted vertex and normal features, respectively. Finally,\nour model outputs the sum of initial and residual poses as the ﬁnal pose.\nFor unsupervised training, we introduce an unsupervised loss function\nwhich is employed on the voxelized point clouds. The proposed approach\nis evaluated on the KITTI odometry estimation benchmark and achieves\ncomparable performances against other state-of-the-art methods.\nKeywords: Unsupervised · Deep learning · Lidar-inertial odometry\n1\nIntroduction\nThe task of odometry is to estimate 3D translation and orientation of au-\ntonomous vehicles which is one of key steps in SLAM. Autonomous vehicles\nusually collect information by perceiving the surrounding environment in real\ntime and use on-board sensors such as lidar, Inertial Measurement Units (IMU),\nor camera to estimate their 3D translation and orientation. Lidar can provide\nhigh-precision 3D measurements but also has no requirement for light. The point\nclouds generated by the lidar can provide high-precision 3D measurements, but if\nit has large translation or orientation in a short time, the continuously generated\npoint clouds will only get few matching points, which will aﬀect the accuracy of\nodometry. IMU has advantages of high output frequency and directly outputting\n2\nY. Tu et al.\nthe 6DOF information to predict the initial translation and orientation that the\nlocalization failure phenomenon can be reduced when lidar has large translation\nor orientation.\nThe traditional methods [1,19,24,20] are mainly based on the point regis-\ntration and work well in ideal environments. However, due to the sparseness\nand irregularity of the point clouds, these methods are diﬃcult to obtain enough\nmatching points. Typically, ICP [1] and its variants [19,14] iteratively ﬁnd match-\ning points which depend on nearest-neighbor searching and optimize the transla-\ntion and orientation by matching points. This optimization procedure is sensitive\nto noise and dynamic objects and prone to getting stuck into the local minima.\nThanks to the recent advances in deep learning, many approaches adopt deep\nneural networks for lidar odometry, which can achieve more promising perfor-\nmance compared to traditional methods. But most of them are supervised meth-\nods [12,25,22,13,21,10]. However, supervised methods require ground truth pose,\nwhich consumes a lot of manpower and material resources. Due to the scarcity\nof the ground truth, recent unsupervised methods are proposed [5,15,23], but\nsome of them obtain unsatisfactory performance, and some need to consume a\nlot of video memory and time to train the network.\nTwo issues exist in these methods. First, these methods ignore IMU, which\noften bring fruitful clues for accurate lidar odometry. Second, those methods do\nnot make full use of the normals, which only take the point clouds as the inputs.\nNormals of point clouds can indicate the relationship between a point and its\nsurrounding points. And even if those approaches [12] who use normals as net-\nwork input, they simply concatenate points and normals together and put them\ninto network, but only orientation between two point clouds relates to normals,\nso normals should not be used to estimate translation.\nTo circumvent the dependence on expensive ground truth, we propose a novel\nframework termed as UnDeepLIO, which makes full use of the IMU and normals\nfor more accurate odometry. We compare against various baselines using point\nclouds from the KITTI Vision Benchmark Suite [7] which collects point clouds\nusing a 360◦Velodyne laser scanner.\nOur main contributions are as follows:\n• We present a self-supervised learning-based approach for robot pose esti-\nmation. our method can outperform [5,15].\n• We use IMU to assist odometry. Our IMU feature extraction module can\nbe embedded in most network structures [12,22,5,15].\n• Both points and its normals are used as network inputs. We use feature of\npoints to estimate translation and feature of both of them to estimate orienta-\ntion.\nUnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry\n3\n2\nRelated Work\n2.1\nModel-based Odometry Estimation\nGauss-Newton iteration methods have a long-standing history in odometry\ntask. Model-based methods solve odometry problems generally by using New-\nton’s iteration method to adjust the transformation between frames so that the\n”gap” between frames keeps getting smaller. They can be categorized into two-\nframe methods [1,19,14] and multi-frame methods [24,20].\nPoint registration is the most common skill for two-frame methods, where\nICP [1] and its variants [19,14] are typical examples. The ICP iteratively search\nkey points and its correspondences to estimate the transformation between two\npoint clouds until convergence. Moreover, most of these methods need multiple\niterations with a large amount of calculation, which is diﬃcult to meet the real-\ntime requirements of the system.\nMulti-frame algorithms [24,20,2] often relies on the two-frame based estima-\ntion. They improve the steps of selecting key points and ﬁnding matching points,\nand use additional mapping step to further optimize the pose estimation. Their\ncalculation process is generally more complicated and runs at a lower frequency.\n2.2\nLearning-based Odometry Estimation\nIn the last few years, the development of deep learning has greatly aﬀected\nthe most advanced odometry estimation. Learning-based model can provide a\nsolution only needs uniformly down sampling the point clouds without manually\nselecting key points. They can be classiﬁed into supervised methods and unsu-\npervised methods.\nSupervised methods appear relatively early, Lo-net [12] maps the point clouds\nto 2D ”image” by spherical projection. Wang et al. [22] adopt a dual-branch ar-\nchitecture to infer 3-D translation and orientation separately instead of a single\nnetwork. Velas et al. [21] use point clouds to assist 3D motion estimation and\nregarded it as a classiﬁcation problem. Diﬀerently, Li et al. [13] do not simply es-\ntimate 3D motion with fully connected layer but Singular Value Decomposition\n(SVD). Use Pointnet [18] as base net, Zheng et al. [25] propose a new approach\nfor extracting matching keypoint pairs(MKPs) of consecutive LiDAR scans by\nprojecting 3D point clouds into 2D spherical depth images where MKPs can be\nextracted eﬀectively and eﬃciently.\nUnsupervised methods appear later. Cho et al. [5] ﬁrst apply unsupervised\napproach on deep-learning-based LiDAR odometry which is an extension of their\nprevious approach [4]. The inspiration of its loss function comes from point-to-\nplane ICP [14]. Then, Nubert et al. [15] report methods with similarly models\nand loss function, but they use diﬀerent way to calculate normals of each point\nin point clouds and ﬁnd matching points between two continuous point clouds.\n4\nY. Tu et al.\n3\nMethods\n3.1\nData Preprocess\nData input At every timestamp k ∈R+, we can obtain one point clouds Pk\nof N ∗3 dimensions and between every two timestamps we can get S frames\nIMU Ik,k+1 of S ∗6 dimensions including 3D angular velocity and 3D linear\nacceleration . we take above data as the inputs.\nVertex map In order to circumvent the disordered nature of point clouds, we\nproject the point clouds into the 2D image coordinate system according to the\nhorizontal and vertical angle. We employ projection function Π : R3 7→R2. Each\n3D point p = (px, py, pz) in a point clouds Pk is mapped into the 2D image plane\n(w, h) represented as\n\u0012\nw\nh\n\u0013\n=\n\u0012(fw −arctan( py\npx ))/ηw\n(fh −arcsin( pz\nd ))/ηh\n\u0013\n,\nH > h ≥0, W > w ≥0,\n(1)\nwhere depth is d =\np\npx2 + py2 + pz2. fw and fh are the maximum horizontal\nand vertical angle. H and W are shape of vertex map. fh depends on the type\nof the lidar. ηw and ηh control the horizontal and vertical sampling density. If\nseveral 3D points correspond the same pixel values, we choose the point with\nminimum depth as the ﬁnal result. If one pixel coordinate has no matching 3D\npoints, the pixel value is set to (0, 0, 0). We deﬁne the 2D image plane as vertex\nmap V .\nNormal map The normal vector of one point includes its relevance about the\nsurrounding points, so we compute a normal map N which consists of normals\nn and has the same shape as corresponding vertex map V . We adopt similar\noperations with Cho et al. [5] and Li et al. [12] to calculate the normal vectors.\nEach normal vector n corresponds to a vertex v with the same image coordinate.\nDue to sparse and discontinuous characteristics of point clouds, we pay more\nattention on the vertex with small Euclidean distance from the surrounding pixel\nvia a pre-deﬁned weight, which can be expressed as wa,b = e{−0.5|d(va)−d(vb)|}.\nEach normal vector n is represented as\nnp =\nX\ni∈[0,3]\nwpi,p(vpi −vp) × wpi+1,p(vpi+1 −vp),\n(2)\nwhere pi represents points in 4 directions of the central vertex p (0-up, 1-right,\n2-down, 3-left).\nUnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry\n5\nvertex map\nnormal map\n[2,3,H,W]\n[2,3,H,W]\nIMU angular velocity\n[S,3]\nvetertex remapped map\n[2,3,H,W]\n[2,3,H,W]\nnormal remapped map\n[M]\n[M]\n[M]\n[M]\n[S,3]\nIMU linear acceleration\n෠ܶସ×ସ\n෠ܴଷ×ଷ\nƸݐଷ\nොݍଷ\nLSTM+FC\nResNet\nEncoder\n+Avgpool\n+FC\nvertex\nfeature\nnormal\nfeature\nLSTM+FC\nrota_feat\ntrans_feat\nAttention\n+FC\nAttention \n+FC\nߜݍଷ\nߜݐଷ\nߜܴଷ×ଷ\nߜܶସ×ସ\nResNet\nEncoder\n+Avgpool\n+FC\nܶସ×ସ\nw1ଵ\nଵ,ଶ\nw2ଵ\nଵ,ଶ\nw3ଵ\nଵ,ଶ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nw1ௌ\nଵ,ଶ\nw2ௌ\nଵ,ଶ\nw3ௌ\nଵ,ଶ\nܽͳଵ\nଵ,ଶ\nܽʹଵ\nଵ.ଶ\nܽ͵ଵ\nଵ,ଶ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nܽͳௌ\nଵ,ଶ\nܽʹௌ\nଵ,ଶ\nܽ͵ௌ\nଵ,ଶ\nTransform\nand 2D remap\n×\n+\nFig. 1. The proposed network and our unsupervised training scheme. FC represents\nfully connected layer. t means translation and q means Euler angle of orientation.\nLSTM takes continuous frames of IMU as inputs and output initial relative pose ˆT .\nˆT are used to transform two maps of current frame to last frame. Then we send the\nremapped maps into ResNet Encoder, which outputs feature maps, including vertex\nand normal features. From the features, we propose an attention layer to estimate\nresidual pose δT . The ﬁnal output is their sum T = δT ˆT.\n3.2\nNetwork Structure\nNetwork input Our pipeline is shown in the Fig. 1. Each point clouds asso-\nciates with a vertex/normal map of (3, H, W) dimensions, so we concatenate\nthe vertex/normal map of k and k + 1 timestamp to get vertex/normal pair\nof (2, 3, H, W) dimensions. We take a pair of vertex/normal maps and IMU be-\ntween k and k+1 timestamp as the inputs of our model, where the IMU consists\nof the linear acceleration and angular velocity both of (S, 3) dimensions, and S\nis the length of IMU sequence. Our model outputs relative pose Tk,k+1, where\nRk,k+1 is orientation and tk,k+1 is translation.\nT 4×4\nk,k+1 =\n\u0014R3×3\nk,k+1 t3×1\nk,k+1\n0\n1\n\u0015\n,\n(3)\nEstimating initial relative pose from IMU Linear acceleration is used to\nestimate translation and angular velocity is used to estimate orientation. We\nemploy LSTM on IMU to extract the features of IMU. Then the features are\nforwarded into the FC layer to estimate initial relative translation or orientation.\nMapping the point clouds of current frame to the last frame Each\nvertex/normal pair consists of last and current frames. They are not in the same\ncoordinate due to the transformation. The initial relative pose can map current\n6\nY. Tu et al.\n[64,H,W/4]\n[2*3,H,W]\n[2*3,H,W]\n[128,H,W/8]\n[256,H/2,W/16]\n[512]\nConvolution Layer\nMaxPool Layer\nResNet Block\nResNet Downsample Block\nAdaptAvgPool Layer\n[M]\nFull Connected Layer\nfeat\nure\n[64,H,W/2]\n[512,H/4,W/32]\nFig. 2. The detail structure of ResNet Encoder + Avgpool + FC part.\nframe in current coordinate to last coordinate, then we can obtain the remapped\ncurrent map with the same size as the old one. The relationship between two\nmaps are shown as formula (4). Take the vk\nk+1,p for example, it is the mapped\nvertex at timestamp k from timestamp k + 1 via the initial pose.\nvk\nk+1,p = Rk,k+1vk+1\nk+1,p + tk,k+1,\n(4)\nnk\nk+1,p = Rk,k+1nk+1\nk+1,p.\n(5)\nEstimating residual relative pose from the remapped maps We use\nResNet Encoder (see Fig. 2) as our map feature extractor. ResNet [8] is used in\nimage recognition. Its input is the 2D images similar to us. Therefore, this struc-\nture can extract feature in our task as well. we send the remapping vertex/normal\nmap pair into siamese ResNet Encoder, which outputs feature maps, including\nvertex and normal features. From the features, we propose an attention layer (by\nformula (6), x is input) which is inspired by LSTM [9] to estimate residual pose\nδT between last frame and the remapped current frame. . Among them, vertex\nand normal features are combined to estimate orientation, but only vertex is\nused to estimate translation because the change of translation does not cause\nthe change of normal vectors. Together with initial relative pose, we can get ﬁnal\nrelative pose T .\ni = σ(Wix + bi),\ng = tanh(Wgx + bg),\no = σ(Wox + bo),\nout = o ∗tanh(i ∗g).\n(6)\n3.3\nLoss Function\nFor unsupervised training, we use a combination of geometric losses in our\ndeep learning framework. Unlike Cho et al. [5] who use pixel locations as cor-\nrespondence between two point clouds, we search correspondence on the whole\nUnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry\n7\norg\nremove most of ground-point \nvoxel down-sample\ncalculate normal\nremove most of ground-point \nvoxel down-sample\nFig. 3. Point downsample process, including point (up) and normal (down).\npoint clouds. For speeding up calculation, we ﬁrst calculate the normals NPi of\nwhole point clouds Pi by plane ﬁtting Φ [17], and then remove its ground points\nby RANSAC [6], at last perform voxel grid ﬁltering ⇓(the arithmetic average of\nall points in voxel as its representation. The normal vectors of voxel are processed\nin the same way and standardized after downsample.) to downsample to about\nK points (The precess is shown in Fig. 3). Given the predicted relative pose\nTk,k+1, we apply it on preprocessed current point clouds DPk+1 and its normals\nNPk+1. For the correspondence search, we use KD-Tree [3] to ﬁnd the nearest\npoint in the last point clouds DPk of each point in the transformed current point\nclouds DP k+1.\nDPi =⇓(RANSAC(Pi)),\nNPi =⇓(RANSAC(Φ(Pi)),\n(7)\nNP k+1 = Rk,k+1NPk+1,\nDP k+1 = Rk,k+1DPk+1 + tk,k+1.\n(8)\nPoint-to-plane ICP loss We use every point dpk+1 in current point clouds\nDP k+1, corresponding point of dpk and normal vector of npk in last point clouds\nDP k to compute the distance between point and its matching plane. The loss\nfunction Lpo2pl is represented as\nLpo2pl =\nX\ndpk+1∈DP k+1\n|npk \u0005 (dpk+1 −dpk)|1,\n(9)\nwhere \u0005 denotes element-wise product.\nPlane-to-plane ICP loss Similarly to point-to-plane ICP, we use normal\nnpk+1 of every point in NP k+1, corresponding normal vector of npk in NP k\n8\nY. Tu et al.\nto compute the angle between a pair of matching plane. The loss function Lpl2pl\nis represented as\nLpl2pl =\nX\nnpk+1∈NP k+1\n|npk+1 −npk|2\n2.\n(10)\nOverall loss Finally, the overall unsupervised loss is obtained as\nL = αLpo2pl + λLpo2pl,\n(11)\nwhere α and λ are balancing factors.\n4\nExperiments\nIn this section, we ﬁrst introduce implementation details of our model and\nbenchmark dataset used in our experiments and the implementation details of\nthe proposed model. Then, comparing to the existing lidar odometry methods,\nour model can obtain competitive results. Finally, we conduct ablation studies\nto verify the eﬀectiveness of the innovative part of our model.\n4.1\nImplementation Details\nThe proposed network is implemented in PyTorch [16] and trained with a\nsingle NVIDIA Titan RTX GPU. We optimize the parameters with the Adam\noptimizer [11] whose hyperparameter values are β1 = 0.9, β2 = 0.99 and wdecay =\n10−5. We adopt step scheduler with a step size of 20 and γ = 0.5 to control the\ntraining procedure, the initial learning rate is 10−4 and the batch size is 20.\nThe length S of IMU sequence is 15. The maximum horizontal and vertical\nangle of vertex map are fw = 180◦and fh = 23◦, and density of them are\nηw = ηh = 0.5. The shapes of input maps are H = 52 and W = 720. The loss\nweight of formula (11) is set to be α = 1.0 and λ = 0.1. The initial side length\nof voxel downsample is set to 0.3m, it is adjusted according to the number of\npoints after downsample, if points are too many, we increase the side length,\notherwise reduce. The adjustment size is 0.01m per time. The number of points\nafter downsample is controlled within K ± 100 and K = 10240.\n4.2\nDatasets\nThe KITTI odometry dataset [7] has 22 diﬀerent sequences with images, 3D\nlidar point clouds, IMU and other data. Only sequences 00-10 have an oﬃcial\npublic ground truth. Among them, only sequence 03 does not provide IMU.\nTherefore, we do not use sequence 03 when there exists the IMU assist in our\nmethod.\nUnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry\n9\nnoimu-09\nnoimu-10\nimu-09\nimu-10\nFig. 4. 2D estimated trajectories of our method on sequence 09 and 10.\n4.3\nEvaluation on the KITTI Dataset\nWe compare our method with the following methods which can be divided\ninto two types. Model-based methods are: LOAM [24] and LeGO-LOAM [20].\nLearning-based methods are: Nubert et al. [15], Cho et al. [5] and SelfVoxeLO\n[23].\nIn model-based methods, we show the lidar odometry results of them with\nmapping and without mapping.\nIn learning-based methods, we use two ways to divide the train and test set.\nFirst, we use sequences 00-08 for training and 09-10 for testing, as Cho et al. [5]\nand Nubert et al. [15] use Sequences 00-08 as their training set. We name it as\n”Ours-easy”. Then, we use sequences 00-06 for training and 07-10 for testing, to\ncompare with SelfVoxeLO which uses Sequences 00-06 as training set. We name\nit as ”Ours-hard”.\nTable. 1 contains the details of the results: trel means average translational\nRMSE (%) on length of 100m-800m and rrel means average rotational RMSE\n(◦/100m) on length of 100m-800m. LeGO-LOAM is not always more precise by\nadding imu, traditional method is more sensitive to the accuracy of imu (In\nsequence 00, there exists some lack of IMU), which is most likely the reason for\nits accuracy drop. Even if the accuracy of the estimation is improved by the\nIMU, the eﬀect is not obvious, especially after the mapping step. Our method\ngains a signiﬁcant improvement by using IMU in test set, and has a certain\nadvantage over traditional method without mapping, and is not much lower than\nwith mapping. In the easy task (For trajectories results, see Fig. 4), our method\nwithout imu assist is also competitive compared to Cho et al. [5] and Nubert\net al. [15] which also project the point clouds into the 2D image coordinate\nsystem. Our method can acquire a lot of improvements with imu. In the hard\ntask, comparing to the most advanced method SelfVoxeLO [23] which uses 3D\nconvolutions on voxels and consumes much video memory and training time, our\nmethod also can get comparable results with IMU. Since they did not publish\nthe code, we are unable to conduct experiments on their method with imu.\n4.4\nAblation Study\nIMU As mentioned earlier, IMU can greatly improve the accuracy of odometry,\nbut the role played by diﬀerent IMU utilization methods is also diﬀerent. If only\n10\nY. Tu et al.\nTable 1. KITTI odometry evaluation.\ntrel(%)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nLeGO-LOAM(w/ map)[20]\n1.44 21.12 2.69 1.73 1.70 0.98 0.87 0.77\n1.35 1.46 1.84\n3.27\nLeGO-LOAM(w/ map)+imu\n7.24 20.07 2.56\nx\n1.68 0.82 0.86 0.67 1.29 1.49 1.75\n3.84\nLeGO-LOAM(w/o map)\n6.98 26.52 6.92 6.16 3.64 4.57 5.16 4.05\n6.01 5.22 7.73\n7.54\nLeGO-LOAM(w/o map)+imu 10.46 22.38 6.05\nx\n2.04 1.98 2.98 2.99\n3.23 3.29 2.74\n5.81\nLOAM(w/ map)[24]\n1.10 2.79 1.54 1.13 1.45 0.75 0.72 0.69 1.18 1.20 1.51\n1.28\nLOAM(w/o map)\n15.99 3.43 9.40 18.18 9.59 9.16 8.91 10.87 12.72 8.10 12.67\n10.82\nNubert et al.[15]\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n6.05 6.44\n3.00\n6.25\nCho et al.[5]\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4.87 5.02\n3.68\n4.95\nOurs-easy\n1.33 3.40 1.53 1.43 1.26 1.22 1.19 0.97 1.92 3.87 2.69\n1.58\n3.28\nOurs-easy+imu\n1.50\n3.44 1.33\nx\n0.94 0.98 0.90 1.00 1.63 2.24 1.83\n1.46\n2.03\nSelfVoxelLO[23]\nNA\nNA\nNA\nNA\nNA\nNA\nNA 3.09 3.16 3.01 3.48\n2.50\n3.19\nOurs-hard\n1.58 3.42 2.27 2.53 0.96 1.36 0.99 6.58\n6.89 5.77 4.04\n1.87\n5.82\nOurs-hard+imu\n1.38 3.46 1.42\nx\n0.98 1.26 0.94 3.45\n4.05 2.77 2.16\n1.57\n3.11\nrrel(◦/100m)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nLeGO-LOAM(w/ map)\n0.65\n2.17 0.99 0.99 0.69 0.47 0.45 0.51\n0.58 0.64 0.74\n0.81\nLeGO-LOAM(w/ map)+imu\n2.44\n0.61 0.91\nx\n0.59 0.38 0.43 0.38\n0.53 0.58 0.63\n0.75\nLeGO-LOAM(w/o map)\n3.27\n4.61 3.10 3.42 2.98 2.38 2.24 2.41\n2.85 2.61 4.03\n3.08\nLeGO-LOAM(w/o map)+imu 3.72\n1.79 2.12\nx\n0.88 0.88 1.24 1.64\n1.23 1.75 1.57\n1.68\nLOAM(w/ map)\n0.53 0.55 0.55 0.65 0.50 0.38 0.39 0.50 0.44 0.48 0.57\n0.50\nLOAM(w/o map)\n6.25\n0.93 3.68 9.91 4.57 4.10 4.63 6.76\n5.77 4.30 8.79\n5.43\nNubert et al.\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2.15 3.00\n1.38\n2.58\nCho et al.\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.95 1.83\n0.87\n1.89\nOurs-easy\n0.69 0.97 0.68 1.04 0.73 0.66 0.64 0.58 0.78 1.67 1.97\n0.75\n1.82\nOurs-easy+imu\n0.70\n0.99 0.59\nx\n0.78 0.56 0.45 0.54 0.78 1.13 1.14\n0.67\n1.14\nSelfVoxelLO\nNA\nNA\nNA\nNA\nNA\nNA\nNA 1.81 1.14 1.14 1.11\n1.11\n1.30\nOurs-hard\n0.91\n1.09 1.19 1.42 0.61 0.78 0.64 4.56\n2.86 2.34 2.89\n0.95\n3.16\nOurs-hard+imu\n0.62 0.98 0.67\nx\n0.67 0.64 0.56 2.17\n1.63 1.25 1.11\n0.69\n1.54\nNA: The result of other papers do not provide.\nx: Do not use this sequence in method.\nTrainavg and testavg of traditional methods are the average results of all 00-10 sequences.\nIMU angular velocity\nIMU linear acceleration\nvertex  map\nnormal  map\nvertex\nfeature\nnormal\nfeature\nangvel\nfeature\nlinacc\nfeature\nrota_feat\ntrans_feat\nLSTM+FC\nLSTM+FC\nResNet\nEncoder\n+Avgpool\n+FC\nResNet\nEncoder\n+Avgpool\n+FC\nw1ଵ\nଵ,ଶ\nw2ଵ\nଵ,ଶ\nw3ଵ\nଵ,ଶ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nw1ௌ\nଵ,ଶ\nw2ௌ\nଵ,ଶ\nw3ௌ\nଵ,ଶ\nܽͳଵ\nଵ,ଶ\nܽʹଵ\nଵ.ଶ\nܽ͵ଵ\nଵ,ଶ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nڭ\nܽͳௌ\nଵ,ଶ\nܽʹௌ\nଵ,ଶ\nܽ͵ௌ\nଵ,ଶ\n+\n+\nFig. 5. Use IMU only as feature.\nuse IMU to extract features through the network, and directly merge with the\nfeature of the point clouds, the eﬀect is limited (see Fig. 5). Our method uses IMU\nUnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry\n11\nTable 2. Comparison among diﬀerent ways to preprocess imu and whether using imu.\ntrel(%)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w preprocess)\n1.50 3.44 1.33\nx\n0.94 0.98 0.90 1.00 1.63 2.24 1.83\n1.58\n2.03\nimu(w/o preprocess) 1.35 3.56 1.57\nx\n1.07 1.21 1.03 0.90 1.59 2.46 1.87\n1.66\n2.17\nnoimu\n1.33 3.40 1.53 1.43 1.26 1.22 1.19 0.97 1.92 3.87 2.69\n1.89\n3.28\nrrel(◦/100m)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w preprocess)\n0.70 0.99 0.59\nx\n0.78 0.56 0.45 0.54 0.78 1.13 1.14\n0.76\n1.14\nimu(w/o preprocess) 0.67 1.01 0.69\nx\n0.63 0.68 0.56 0.60 0.71 1.13 1.28\n0.80\n1.20\nnoimu\n0.69 0.97 0.68 1.04 0.73 0.66 0.64 0.58 0.78 1.67 1.97\n0.95\n1.82\nvertex\nfeature\nnormal\nfeature\nrota_feat\ntrans_feat\nvertex\nfeature\nrota_feat\ntrans_feat\n+\nFig. 6. The network structure of learning translation and rotation features from con-\ncatenated vetex and normal features simultaneously (left) and the network structure\nwithout the normal feature (right).\nand LSTM network to estimate a relative initial pose, project vertex image and\nnormal vector image of the original current frame, and then send the projection\nimages into the point clouds feature extraction network, so that the IMU can\nnot only have a direct connection with the ﬁnal odometry estimate network, but\nalso make the coordinate of two consecutive frames closer. The comparison is\nshown in Table. 2.\nDiﬀerent operations to obtain the rotation and translation features\nThe normal vector contains the relationship between a point and its surrounding\npoints, and can be used as feature of pose estimation just like the point itself.\nThrough the calculation formula of the normal vector, we can know that the\nchange of the normal vector is only related to the orientation, and the translation\nwill not bring about the change of the normal vector. Therefore, we only use the\nfeature of the point to estimate the translation. We compare the original method\nwith the two strategies of not using normal vectors as the network input and not\ndistinguishing feature of the normals and points (see Fig. 6). The comparison is\nshown in Table. 3.\nAttention After extracting the features of the vertex map and the normal\nmap, we add an additional self-attention module to improve the accuracy of\npose estimation. The attention module can self-learn the importance of features,\nand give higher weight to more important features. We verify its eﬀectiveness by\n12\nY. Tu et al.\nTable 3. Comparison among whether distinguishing features (dist) and whether using\nnormal.\ntrel(%)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w normal, w dist)\n1.50 3.44 1.33\nx\n0.94 0.98 0.90 1.00 1.63 2.24 1.83\n1.58\n2.03\nimu(w normal, w/o dist)\n1.45 3.68 2.03\nx\n0.72 1.11 1.15 0.68 1.67 3.44 1.86\n1.78\n2.65\nimu(w/o normal, w/o dist)\n2.54 3.81 4.13\nx\n0.95 1.77 0.99 1.25 1.93 2.72 2.21\n2.23\n2.47\nnoimu(w normal, w dist)\n1.33 3.40 1.53 1.43 1.26 1.22 1.19 0.97 1.92 3.87 2.69\n1.89\n3.28\nnoimu(w normal, w/o dist)\n1.49 3.95 2.49 2.27 0.88 1.19 0.90 1.47 2.02 4.93 4.34\n2.36\n4.64\nnoimu(w/o normal, w/o dist) 1.63 4.96 2.99 2.36 2.15 1.31 1.31 1.51 1.89 5.75 6.11\n2.91\n5.93\nrrel(◦/100m)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w normal, w dist)\n0.70 0.99 0.59\nx\n0.78 0.56 0.45 0.54 0.78 1.13 1.14\n0.76\n1.14\nimu(w normal, w/o dist)\n0.65 1.04 0.96\nx\n0.53 0.56 0.58 0.46 0.64 1.45 1.15\n0.80\n1.30\nimu(w/o normal, w/o dist)\n1.31 1.05 1.60\nx\n0.52 0.88 0.48 0.87 0.93 1.15 1.21\n1.00\n1.18\nnoimu(w normal, w dist)\n0.69 0.97 0.68 1.04 0.73 0.66 0.64 0.58 0.78 1.67 1.97\n0.95\n1.82\nnoimu(w normal, w/o dist)\n0.88 1.24 1.20 1.38 0.66 0.70 0.58 1.03 0.95 1.92 2.06\n1.15\n1.99\nnoimu(w/o normal, w/o dist) 0.90 1.48 1.37 1.49 1.38 0.79 0.73 1.08 0.93 2.31 2.73\n1.38\n2.52\nTable 4. Comparison among whether using attention module.\ntrel(%)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w attention)\n1.50 3.44 1.33\nx\n0.94 0.98 0.90 1.00 1.63 2.24 1.83\n1.58\n2.03\nimu(w fc+activation)\n1.19 3.49 1.48\nx\n0.83 0.95 0.64 0.91 1.49 3.21 1.54\n1.57\n2.38\nnoimu(w attention)\n1.33 3.40 1.53 1.43 1.26 1.22 1.19 0.97 1.92 3.87 2.69\n1.89\n3.28\nnoimu(w fc+activation) 1.65 3.59 1.67 1.88 0.87 1.34 1.10 1.23 1.76 6.64 3.25\n2.27\n4.95\nrrel(◦/100m)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w attention)\n0.70 0.99 0.59\nx\n0.78 0.56 0.45 0.54 0.78 1.13 1.14\n0.76\n1.14\nimu(w fc+activation)\n0.62 0.97 0.64\nx\n1.02 0.54 0.42 0.55 0.70 1.20 1.07\n0.77\n1.13\nnoimu(w attention)\n0.69 0.97 0.68 1.04 0.73 0.66 0.64 0.58 0.78 1.67 1.97\n0.95\n1.82\nnoimu(w fc+activation) 0.77 0.99 0.67 1.10 0.70 0.67 0.48 0.80 0.80 2.36 2.07\n1.04\n2.21\nmatch\nnot match\npixel-to-pixel\npoint-to-point\nFig. 7. Matching points search strategy of Cho et al.(pixel-to-pixel), our and Nubert\net al.(point-to-point).\ncomparing the result of the model which replaces the self-attention module with\na single FC layer with activation function (as formula (12)). The comparison is\nshow in Table. 4.\nout = tanh(W2(tanh(W1x + b1)) + b2).\n(12)\nLoss function Cho et al. [5] adopt the strategy of using the points with the\nsame pixel in last and current vertex map as the matching points. Although the\ncalculation speed is fast, the matching points found in this way are likely to be\nUnDeepLIO: Unsupervised Deep Lidar-Inertial Odometry\n13\nTable 5. Comparison among diﬀerent loss functions and matching point search strat-\negy.\ntrel(%)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w point-to-plane))+point-to-point\n1.50 3.44 1.33\nx\n0.94 0.98 0.90 1.00 1.63 2.24 1.83\n1.58\n2.03\nimu(w/o point-to-plane)+point-to-point\n2.27 4.33 2.24\nx\n1.59 1.70 1.26 1.29 1.87 2.04 2.07\n2.07\n2.06\nimu(w point-to-plane)+pixel-to-pixel\n2.14 4.36 2.29\nx\n1.65 1.66 1.17 1.36 1.73 2.95 2.28\n2.16\n2.61\nnoimu(w point-to-plane)+point-to-point\n1.33 3.40 1.53 1.43 1.26 1.22 1.19 0.97 1.92 3.87 2.69\n1.89\n3.28\nnoimu(w/o point-to-plane)+point-to-point 1.46 3.44 1.67 1.91 0.92 1.00 1.11 1.36 1.81 4.72 2.78\n2.02\n3.75\nnoimu(w point-to-plane))+pixel-to-pixel\n2.76 4.43 2.73 2.07 1.71 1.50 1.32 1.32 1.95 3.68 3.65\n2.47\n3.67\nrrel(◦/100m)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\ntrainavg testavg\nimu(w point-to-plane))+point-to-point\n0.70 0.99 0.59\nx\n0.78 0.56 0.45 0.54 0.78 1.13 1.14\n0.76\n1.14\nimu(w/o point-to-plane)+point-to-point\n1.01 1.12 0.98\nx\n0.96 0.82 0.62 0.78 0.86 1.14 1.19\n0.95\n1.16\nimu(w point-to-plane)+pixel-to-pixel\n0.96 1.11 0.96\nx\n0.98 0.83 0.58 0.83 0.87 1.52 1.27\n0.99\n1.39\nnoimu(w point-to-plane)+point-to-point\n0.69 0.97 0.68 1.04 0.73 0.66 0.64 0.58 0.78 1.67 1.97\n0.95\n1.82\nnoimu(w/o point-to-plane)+point-to-point 0.73 0.99 0.70 1.34 0.69 0.58 0.49 0.85 0.76 1.85 1.84\n0.98\n1.85\nnoimu(w point-to-plane))+pixel-to-pixel\n1.10 1.16 1.11 1.40 1.03 0.76 0.62 0.78 0.89 1.56 2.05\n1.13\n1.80\nincorrect. Therefore, we and Nubert et al. [15] imitate ICP algorithm, using the\nnearest neighbor as the matching point(see Fig. 7). Although we use the same\nloss functions and the same matching point search strategy (nearest neighbor)\nas Nubert et al. [15], we search in the entire point clouds space, and maintain the\nnumber of points in search space not too large by removing most of the ground\npoints and operating voxel grids downsample on point clouds. The number of\npoints even is only 1/3 of the points sampled by the 2D projection which used\nin [15]. Table. 5 shows the necessity of two loss parts and strategy of searching\nmatching points in the entire point clouds.\n5\nConclusion\nIn this paper, we proposed UnDeepLIO, an unsupervised learning-based\nodometry network. Diﬀerent from other unsupervised lidar odometry methods,\nwe additionally used IMU to assist odometry task. There have been already\nmany IMU and lidar fusion algorithms in the traditional ﬁeld for odometry, and\nit has become a trend to use the information of both at the same time. Moreover,\nwe conduct extensive experiments on kitti dataset and experiments verify that\nour method is competitive with the most advanced methods. In ablation study,\nwe validated the eﬀectiveness of each component of our model. In the future, we\nwill study how to incorporate mapping steps into our network framework and\nconduct online tests.\nReferences\n1. Arun, K.S., Huang, T.S., Blostein, S.D.: Least-squares ﬁtting of two 3-d point sets.\nTPAMI 9(5), 698–700 (1987)\n2. Behley, J., Stachniss, C.: Eﬃcient surfel-based slam using 3d laser range data in\nurban environments. In: Robotics: Science and Systems. vol. 2018 (2018)\n14\nY. Tu et al.\n3. Bentley, J.L.: Multidimensional binary search trees used for associative searching.\nCommunications of the ACM 18(9), 509–517 (1975)\n4. Cho, Y., Kim, G., Kim, A.: Deeplo: Geometry-aware deep lidar odometry. arXiv\npreprint arXiv:1902.10562 (2019)\n5. Cho, Y., Kim, G., Kim, A.: Unsupervised geometry-aware deep lidar odometry. In:\nICRA. pp. 2145–2152. IEEE (2020)\n6. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model\nﬁtting with applications to image analysis and automated cartography. Communi-\ncations of the ACM 24(6), 381–395 (1981)\n7. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti\nvision benchmark suite. In: CVPR. pp. 3354–3361. IEEE (2012)\n8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. pp. 770–778 (2016)\n9. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n9(8), 1735–1780 (1997)\n10. Javanmard-Gh.,\nA.:\nDeeplio:\nDeep\nlidar\ninertial\nodometry.\nhttps://github.com/ArashJavan/DeepLIO (2020)\n11. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR\n(2015)\n12. Li, Q., Chen, S., Wang, C., Li, X., Wen, C., Cheng, M., Li, J.: Lo-net: Deep real-\ntime lidar odometry. In: CVPR. pp. 8473–8482 (2019)\n13. Li, Z., Wang, N.: Dmlo: Deep matching lidar odometry. In: IROS (2020)\n14. Low, K.L.: Linear least-squares optimization for point-to-plane icp surface regis-\ntration. Chapel Hill, University of North Carolina 4(10), 1–3 (2004)\n15. Nubert, J., Khattak, S., Hutter, M.: Self-supervised learning of lidar odometry for\nrobotic applications. In: ICRA (2021)\n16. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-\nperformance deep learning library. In: NIPS (2019)\n17. Pauly, M.: Point primitives for interactive modeling and processing of 3D geometry.\nHartung-Gorre (2003)\n18. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for\n3d classiﬁcation and segmentation. In: CVPR. pp. 652–660 (2017)\n19. Segal, A., Haehnel, D., Thrun, S.: Generalized-icp. Robotics: science and systems\n2(4), 435 (2009)\n20. Shan, T., Englot, B.: Lego-loam: Lightweight and ground-optimized lidar odometry\nand mapping on variable terrain. In: IROS. pp. 4758–4765. IEEE (2018)\n21. Velas, M., Spanel, M., Hradis, M., Herout, A.: Cnn for imu assisted odometry\nestimation using velodyne lidar. In: ICARSC. pp. 71–77. IEEE (2018)\n22. Wang, W., Saputra, M.R.U., Zhao, P., Gusmao, P., Yang, B., Chen, C., Markham,\nA., Trigoni, N.: Deeppco: End-to-end point cloud odometry through deep parallel\nneural network. In: IROS (2019)\n23. Xu, Y., Huang, Z., Lin, K.Y., Zhu, X., Shi, J., Bao, H., Zhang, G., Li, H.: Selfvoxelo:\nSelf-supervised lidar odometry with voxel-based deep neural networks. In: CoRL\n(2020)\n24. Zhang, J., Singh, S.: Loam: Lidar odometry and mapping in real-time. Robotics:\nScience and Systems 2(9) (2014)\n25. Zheng, C., Lyu, Y., Li, M., Zhang, Z.: Lodonet: A deep neural network with 2d\nkeypoint matching for 3d lidar odometry estimation. In: ACM MM. pp. 2391–2399\n(2020)\n",
  "categories": [
    "cs.CV",
    "cs.RO"
  ],
  "published": "2021-09-03",
  "updated": "2021-09-03"
}