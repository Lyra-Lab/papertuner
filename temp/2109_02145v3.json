{
  "id": "http://arxiv.org/abs/2109.02145v3",
  "title": "Temporal Shift Reinforcement Learning",
  "authors": [
    "Deepak George Thomas",
    "Tichakorn Wongpiromsarn",
    "Ali Jannesari"
  ],
  "abstract": "The function approximators employed by traditional image-based Deep\nReinforcement Learning (DRL) algorithms usually lack a temporal learning\ncomponent and instead focus on learning the spatial component. We propose a\ntechnique, Temporal Shift Reinforcement Learning (TSRL), wherein both temporal,\nas well as spatial components are jointly learned. Moreover, TSRL does not\nrequire additional parameters to perform temporal learning. We show that TSRL\noutperforms the commonly used frame stacking heuristic on both of the Atari\nenvironments we test on while beating the SOTA for one of them. This\ninvestigation has implications in the robotics as well as sequential\ndecision-making domains.",
  "text": "Temporal Shift Reinforcement Learning\nDeepak George Thomas, Tichakorn Wongpiromsarn, Ali Jannesari *†‡§\nOctober 28, 2021\nAbstract\nThe function approximators employed by traditional image-based Deep Rein-\nforcement Learning (DRL) algorithms usually lack a temporal learning compo-\nnent and instead focus on learning the spatial component. We propose a technique,\nTemporal Shift Reinforcement Learning (TSRL), wherein both temporal, as well\nas spatial components are jointly learned. Moreover, TSRL does not require addi-\ntional parameters to perform temporal learning. We show that TSRL outperforms\nthe commonly used frame stacking heuristic on all of the Atari environments we\ntest on while beating the SOTA for all except one of them. This investigation has\nimplications in the robotics as well as sequential decision-making domains.\n1\nIntroduction\nDeveloping Reinforcement Learning (RL) algorithms that can make effective decisions\nusing high dimensional observations such as images is quite challenging. In addition,\nit consumes a lot of time and energy. In recent months researchers have worked on\ndeveloping sample efﬁcient RL, plug and play algorithms, that can directly learn from\npixels. Srinivas et al. incorporated Contrastive Learning, into off-policy algorithms, to\nlearn relevant features from image based inputs. Laskin et al. investigated developing\ndata efﬁcient and generalizable algorithms, by introducing a generic data augmentation\nmodule for RL algorithms; Laskin et al. (2020); Shang et al. (2021); Srinivas et al.\n(2020).\nWhile a lot of work has been devoted to extracting positional information from\nimage inputs, very little investigation has been done on learning from temporal infor-\nmation. Shang et al. performed experiments using DMControl (Tassa et al. (2018))\nto highlight the importance of temporal information in RL. They compared two Soft-\nActor-Critic (SAC) RL algorithms, wherein one had access to pose and temporal infor-\nmation and the other only had access to pose. It was found that the former algorithm\n**This work was not supported by any organization\n†Deepak George Thomas is with the Department of Computer Science, Iowa State University, Ames, IA\n50011 dgthomas@iastate.edu\n‡Dr. Ali Jannesari is with the Department of Computer Science, Iowa State University, Ames, IA 50011\njannesar@iastate.edu\n§Tichakorn (Nok) Wongpiromsarn is with the Department of Computer Science, Iowa State University,\nAmes, IA 50011 nok@iastate.edu\n1\narXiv:2109.02145v3  [cs.LG]  27 Oct 2021\nswiftly learned the optimal policy, while the latter failed to do so. Furthermore, a recur-\nring heuristic used by many papers is to stack sequential observations together while\ninputting it to a neural network; Mnih et al. (2015). This heuristic combines frames,\nwithout processing it and therefore can be considered analogous to early fusion; Karpa-\nthy et al. (2014). Recently, Shang et al. approached this as a video classiﬁcation prob-\nlem. This is a lucid approach, as considering a DRL state equivalent to a video, will\nhelp improve the prediction capabilities of the underlying neural network. Successful\nvideo recognition architectures use late fusion where all frames are processed, using\nneural networks, before they are combined together; Shang et al. (2021), Laskin et al.\n(2020).\nMoreover, a video stream consists of both spatial and temporal aspects. The for-\nmer contains information about the video frame including objects and its surroundings,\nwhile the movement of the frame and its associated objects can be learned from the\ntemporal portion; Simonyan & Zisserman (2014). While learning the spatial aspect\nis enough for image recognition, video recognition requires learning both spatial and\ntemporal components. Enabling agents to extract temporal information from a given\nset of frames will result in the DRL agent making better Q-value predictions and there-\nfore result in improved data efﬁciency. Furthermore, it will contribute to the agent\nunderstanding the differences between seemingly similar actions, such as opening and\nclosing objects; Lin et al. (2019), Shang et al. (2021).\nThere has been a plethora of work related to video recognition using 3D and stan-\ndard 2D CNNs. 3D CNNs have the ability to simultaneously extract out spatial and\ntemporal features from videos. However, they are computationally costly, which makes\nthem hard to implement in real-time situations. Incorporating similar architectures\nwith vision-based DRL exacerbates this problem, as many applications require fast\npredictions during training, and having latency is infeasible. Furthermore, the extra\nparameters could make the model more prone to overﬁtting without large amounts of\ndata. This once again poses a roadblock to the development of sample efﬁcient RL;Lin\net al. (2019); Tran et al. (2015); Carreira & Zisserman (2017). 2D CNNs, although\nrelatively efﬁcient however fail to extract out temporal information;Simonyan & Zis-\nserman (2014).\nAmiranashvili et al. (2018) incorporated optical ﬂow in their RL algorithm, al-\nthough their technique required state variables in addition to pixel observations during\ntraining. Modeling temporal information in RL using simply pixel inputs was inves-\ntigated by Shang et al. (2021), and it brought a new approach to efﬁciently reducing\nsample complexity in reinforcement learning. We intend to further optimize this tech-\nnique by leveraging recent work in the ﬁeld of video action prediction and therefore\npropose the Temporal Shift Reinforcement Learning (TSRL) algorithm.\nThe contributions of our work 1 are presented here:\n1) We propose a plug and play architecture works with any generic vision-based\nDRL algorithms.\n2) We augment a video recognition; Lin et al. (2019) that does not require any\nadditional parameters to model temporal information in DRL.\n1Our code is available at https://anonymous.4open.science/r/TSM_RL-85F5/README.\nmd\n2\n2\nRelated Work\n2.1\nLatent Flow\nSimonyan et al. investigated the use of optical ﬂow techniques to perform video classi-\nﬁcation and was able to achieve SOTA performance by a signiﬁcant amount over previ-\nous work in video classiﬁcation. They developed a dual-stream architecture using Con-\nvNets, consisting of spatial and temporal recognition components. The spatial stream\nwas learned using a pre-trained ConvNet, wherein each frame was sent to the network\nas input. The input to the temporal stream was stacked optical ﬂow displacement ﬁelds\ngenerated from consecutive frames. Movement among frames can be obtained from\noptical ﬂow ﬁelds, thereby eliminating the need for the network to learn it. This tech-\nnique achieved high accuracies without requiring a lot of data. More importantly, they\nestablished that training a temporal CNN using optical ﬂow was a drastically better\ntechnique compared to training on a stacked bunch of images; Simonyan & Zisserman\n(2014); Karpathy et al. (2014). The downside of this algorithm is that it is computa-\ntionally costly both during inference and training and therefore cannot be combined\nwith RL algorithms; Shang et al. (2021).\n2.2\nFlow of Latents\nShang et al. looked for a computationally feasible technique to integrate RL with op-\ntical ﬂow. They were inspired by late fusion techniques; wherein every frame was run\nthrough a CNN before fusion was applied. Late fusion provides improved performance\nwith lesser parameters and also allows multi-modal data Jain et al. (2019); Chebotar\net al. (2017). They presented a structured late fusion architecture, wherein each im-\nage frame was encoded using a neural network. The encodings at each time step were\nsubtracted from their prior, and this difference was fused with the latent encodings,\nwhich was then used by the RL algorithm. This technique was analogous to the work\ndone by Simonyan & Zisserman (2014). The optical ﬂow was approximated using the\ndifference in encodings, which provided temporal information. The spatial component\nwas obtained by encoding each of the frames. This technique provided the CNN with\na necessary inductive bias. They chose Rainbow DQN, and RAD; Laskin et al. (2020)\nto be their base algorithm and found that it outperformed SOTA algorithms in perfor-\nmance and sample efﬁciency. Also, they showed that their algorithm reached optimal\nperformance in state-based RL despite only being provided positional state information\nand no state velocity.\nThey also separately investigated encoding frames and then stacking the encodings\ntogether instead of the raw images. This technique yielded sub-par results, and the\nauthors hypothesized that stacking high dimensional image frames would allow CNNs\nto learn temporal information. However, by stacking latent frames, the temporal infor-\nmation was lost and thereby causing the difference in results.\n3\n2.3\nTemporal Shift Module\nWhile working with video model activations consisting of N frames, C channels and\nH height and W width,\nA ∈RNXCXT XHXW , 2D CNNs don’t consider the temporal dimension T thereby\nignoring it. Lin et al. (2019) addressed this by shifting channels, thereby mixing in-\nformation from neighboring frames through the temporal dimension and referred to it\nas the Temporal Shift Module (TSM). Therefore the current frame contains informa-\ntion that was obtained from its surroundings. They leveraged the concepts of shifts\nand multiply-accumulate, which are the basic principles of a convolution operation.\nThey extended it by shifting one step forward and backward along the temporal dimen-\nsion. Furthermore, the multiply-accumulate was folded from the channel dimension to\nthe temporal dimension. However, for online video recognition, only previous frames\ncould be shifted forward and not the other way around. Therefore in such cases, a\nuni-directional TSM was implemented.\nWhile this process doesn’t require extra parameters, they found that this technique\nhad drawbacks - 1) The data movement generated due to the shift strategy was not efﬁ-\ncient and would increase the latency, especially since 5D activation of videos results in\nlarge memory usage. This implied that moving all channels would result in inference\nlatency and large memory footprint on the hardware hosting the model. 2) Moving\nchannels directly across the temporal dimension, referred to as in-place shift, would\naffect the accuracy of model since the spatial model is distorted. This is because the\ncurrent channel would have some of its frames (or feature maps) shifted, and there-\nfore, the 2D CNN would lose that information during the classiﬁcation process. The\nauthors obtained a 2.6 % accuracy drop relative to their baseline; Wang et al. (2016)\nwhile naively shifting channels. The former issue was mitigated by shifting only a par-\ntial number of channels, thereby reducing the amount of data movement and latency\nincurred. For the latter problem, the TSM module was inserted within the residual\nbranch of a Res-Net, thereby enabling the 2D CNN to learn spatial features without\ndegrading. The authors claimed that this method, namely residual shift, allows the in-\nformation present within the original activation to be retained after channel shifting due\nto identity mapping. Therefore, the TSM module is a simple modiﬁcation to the 2D\nCNN. After encoding images, it shifts frames in the temporal dimension by +1, -1, and\n0. However, shifting frames by -1, i.e. backward, is only possible for ofﬂine problems.\nFor online image classiﬁcation problems, the frames are moved +1; Lin et al. (2019).\nA major advantage of online TSM was that it enabled multi-level temporal fusion.\nOther online methods are generally limited to late and mid-level temporal fusion. The\nauthors found multi-level temporal fusion to signiﬁcantly inﬂuence temporal problems;\nZhou et al. (2018); Lin et al. (2019); Zolfaghari et al. (2018).\n2.4\nPrioritized Deep Q Network\nMnih et al. (2015) combined Q Networks with CNNs in order to obtain an approxima-\ntion of the Q values -\nQ∗(s, a) = maxπ E[rt + γrt+1 + γ2rt+2 + ...|st = s, at = a, π]\nThe above expression maximizes the sum of discounted rewards for an agent fol-\n4\nFigure 1: A schematic of Temporal Shift Reinforcement Learning algorithm\nlowing a policy, π = P(a|s), r using a discount factor, γ during every time step t.\nIt was known to be the ﬁrst RL algorithm that could be integrated into various envi-\nronments with raw pixels as inputs. They addressed the learning instabilities that RL\npresented when coupled with a deep neural network using a replay buffer and target\nnetwork. They found that the sequential observations were highly correlated with each\nother and also that minimal changes to Q would drastically affect the policy. The use\nof a replay buffer mitigated this issue by randomizing the data during the training pro-\ncess. This was done by storing the transitions as a tuple (s, at, st+1, rt + 1) of state,\naction, next states and rewards within a cyclic buffer. This provided a two-fold ben-\neﬁt. The replay buffer reduced the number of environments needed for the agent to\nlearn since the agent could always resample from the buffer. Furthermore, this reduces\nthe variance during gradient descent since batches are sampled. The target network\ntakes the weight from the current network but updates it only after a ﬁxed duration of\ntime. The target network’s weights are then used to compute the TD error, which is\nthe difference between the Q value and the TD target. If we use the parameters from\nthe current network to estimate both these values, they’ll become correlated and will\nresult in instability. Hasselt (2010) suggested using dual instead of single estimators to\nestimate the expected return since the latter led to over-estimated values and introduced\nthe Double-Q learning algorithm (DDQN). A later investigation by Van Hasselt et al.\n(2016) showed that rather than learning a separate function, the target network could\nbe used to obtain the estimate; Mnih et al. (2015); Arulkumaran et al. (2017).\nIn addition, Schaul et al. (2015) modiﬁed the experience replay process so that,\ninstead of the conventional uniform sampling process, important samples were given\na higher priority. The Prioritized Experience Replay (PER) technique was found to\ndouble the learning speed and also achieve SOTA scores on Atari games.\n5\n3\nApproach\nThe motivation behind TSRL was to introduce an efﬁcient algorithm that did not re-\nquire any additional parameters, leveraging the beneﬁts of multi-level temporal fusion.\nThe architecture developed by Lin et al. (2019) for online Temporal Shift was modi-\nﬁed and incorporated into a Double DQN with Prioritized Experience Replay (DDQN-\nPER). Lin et al. (2019) used a ResNet model for their experiments, however going with\nthe conventional CNN models used by the vision RL community, we used a shallow\nthree layer CNN.\nAlso, we used in-place shift instead of residual shift wherein the channels were di-\nrectly moved across the temporal dimension. We assumed that the accuracy improve-\nments obtained, in predicting the Q values, while modeling the temporal aspect would\ncompensate for the loss obtained due to spatial degradation. Furthermore, the online\nTSM algorithm Lin et al. (2019) cached the features in memory and then replaced it\nwith those in the next time step. Our approach was to directly roll the features across\ntime steps.\nFinally, the authors of the TSM paper found that the highest accuracy for the online\nmodel was obtained by shifting 1/8th of channels for each layer of the neural network.\nHowever, while testing our algorithm, we found that the best results were obtained\nwhen we shifted around 1/5 to 1/3 of our channels.\nA schematic of our algorithm has been given in Figure 1 and a PyTorch based\npseudocode for our algorithm has been presented here -\nAlgorithm 1 TSRL\nFor\neach\ns t e p\nt\ndo\nFor\neach\nc o n v o l u t i o n\ns t e p\ndo\nx =\ns e l f . r e l u 1 ( s e l f . conv1 ( x ) )\nn , c , h ,w = x . shape\nx = x . reshape ( n / / T , T ,\nc ,\nh , w)\ncopy = t o r c h . clone ( x )\nx [ : , : ,\n: c / / 8 ,\n: ,\n: ]\n= t o r c h . r o l l ( x [ : , : ,\n: c / / 8 ,\n: ,\n: ] ,\ns h i f t s\n= 1 ,\ndims = 1)\nx [ : , 0 ,\n: c / / 8 ,\n: ,\n: ]\n= copy [ : , 0 ,\n: c / / 8 ,\n: ,\n: ]\nz t = FullyConnected ( x )\nEnd For\nEnd For\n4\nExperiments\nWe tested our algorithm using OpenAI Gym Atari environments with visual images as\ninput. An open-sourced implementation of DDQN (https://github.com/higgsﬁeld/RL-\nAdventure) combined with PER was used. The images were converted to grayscale to\nspeed up the learning process. To gauge the sample efﬁciency of TSRL we compared\n6\nit with a generic DDQN-PER getting stacked images as input. Also, we used our own\nimplementation of the algorithm developed by Shang et al. (2021) and referred to it as\nFlare, in order to compare against state of the art. The number of stacked images was\nkept equal to the timesteps considered by TSRL both for DDQN-PER and Flare. Also,\nall algorithms were run for 1.4M time steps using 5 different trials. The performance\nof the algorithm was gauged by averaging the trials and then summing over all rewards\nobtained; Brockman et al. (2016); Bellemare et al. (2013); Mott & Team (1996).\n(a) Freeway Atari Environment\n(b) Asterix Atari Environment\n(c) River Raid Atari Environment\n(d) Pong Atari Environment\nFigure 2: OpenAI Gym environments used for training\n7\n4.1\nResults\nTable 4.1 shows the sum of average rewards obtained across the ﬁve runs for each\nenvironment. The shift parameter, s column, shows the ratio of channels that were\nshifted. For instance, if s = 3, then the ﬁrst 1/3rd channels would be shifted across the\ntemporal dimension for every layer of the CNN.\nFigure 3 shows the reward obtained per episode. In some cases, an algorithm may\nhave large step sizes relatively early. This would lead to a lower number of episodes\nand vice versa.\nTSRL outperforms both DDQN-PER and Flare in all environments except Asterix,\nwherein it only defeats the DDQN-PER.\nFigure 3: Plots of episode vs reward for different Atari environments\nTable 1: Sum of average rewards obtained.\nEnvironment\nShift\nTSRL\nDDQN-PER\nFLARE\nFreeway\n3\n18291.5\n17807.6\n14686.19\nAsterix\n5\n22854.25\n20702.0\n33496.93\nRiverraid\n5\n41850.3\n34849.2\n34966.0\nPong\n5\n7892.17\n7221.80\n-36528.20\n4.2\nDiscussion\nA major difference between our algorithm and other RL algorithms taking temporal\naspects into account is that we provide multi-level temporal fusion. Most RL algo-\nrithms implement early fusion Mnih et al. (2015) and the recent ones Amiranashvili\n8\net al. (2018); Shang et al. (2021) have experimented with late fusion. However, our\napproach enables RL to have temporal fusion across all levels. This type of fusion was\nfound to signiﬁcantly help difﬁcult temporal modeling problems Lin et al. (2019).\nIt is interesting to note that instead of a single shift hyperparameter being optimal\nfor all tasks, it varies across environments. We hypothesize that this is caused due\nto the trade-off between spatial and temporal learning. Some environments might not\nrequire a higher number of feature maps and therefore could work with a lower shift\nhyperparameter. This would permit a larger number of channels to be moved, leading\nto improved temporal learning. However, this might not be the case in complicated\nenvironments, and such situations might require the shift hyperparameter to be higher.\nFinally, we see that TSRL is able to beat the baseline and SOTA for almost all the\nenvironments. 2 Since Flare concatenates latent ﬂow with features, we feel that this\nincreases the number of parameters and, therefore, the relative training time compared\nto TSRL. Furthermore, the latent ﬂow is obtained by subtracting the current frame from\nthe immediately preceding frame while ignoring the frames before that. This might not\nprovide much information in situations when the difference between immediate frames\nis minute. This problem is mitigated by the multi-level fusion abilities of our algorithm.\n5\nConclusions\nWe present a facile shifting technique for learning temporal features in DRL problems\nwithout the requirement of additional parameters. After testing our algorithm on Ope-\nnAI Atari environments, we ﬁnd that our algorithm outperforms the commonly used\nframe-stacking heuristic.\nA major drawback of our algorithm is the requirement to ﬁnd a suitable shift hy-\nperparameter. Future work could include either learning the optimal value of this hy-\nperparameter online or changing how the shift is performed (such as residual shift Lin\net al. (2019)) so that the spatial features aren’t disturbed.\nReferences\nArtemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, and Thomas Brox. Mo-\ntion perception in reinforcement learning with dynamic objects. In Conference on\nRobot Learning, pp. 156–168. PMLR, 2018.\nKai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath.\nA brief survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866,\n2017.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade\nlearning environment: An evaluation platform for general agents. Journal of Artiﬁ-\ncial Intelligence Research, 47:253–279, 2013.\n2We used our own implementation of the Flare algorithm.\n9\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,\nJie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540,\n2016.\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and\nthe kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 6299–6308, 2017.\nYevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, and\nSergey Levine.\nPath integral guided policy search.\nIn 2017 IEEE international\nconference on robotics and automation (ICRA), pp. 3381–3388. IEEE, 2017.\nHado Hasselt. Double q-learning. Advances in neural information processing systems,\n23:2613–2621, 2010.\nDivye Jain, Andrew Li, Shivam Singhal, Aravind Rajeswaran, Vikash Kumar, and\nEmanuel Todorov. Learning deep visuomotor policies for dexterous hand manip-\nulation. In 2019 International Conference on Robotics and Automation (ICRA), pp.\n3636–3643. IEEE, 2019.\nAndrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar,\nand Li Fei-Fei. Large-scale video classiﬁcation with convolutional neural networks.\nIn Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,\npp. 1725–1732, 2014.\nMichael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Ar-\navind Srinivas.\nReinforcement learning with augmented data.\narXiv preprint\narXiv:2004.14990, 2020.\nJi Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efﬁcient video\nunderstanding. In Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision, pp. 7083–7093, 2019.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. Human-level control through deep reinforcement learning. nature,\n518(7540):529–533, 2015.\nBradford W Mott and Stella Team. Stella: a multiplatform atari 2600 vcs emulator,\n1996.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience\nreplay. arXiv preprint arXiv:1511.05952, 2015.\nWenling Shang, Xiaofei Wang, Aravind Srinivas, Aravind Rajeswaran, Yang Gao,\nPieter Abbeel, and Michael Laskin. Reinforcement learning with latent ﬂow. arXiv\npreprint arXiv:2101.01857, 2021.\nKaren Simonyan and Andrew Zisserman. Two-stream convolutional networks for ac-\ntion recognition in videos. arXiv preprint arXiv:1406.2199, 2014.\n10\nAravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised\nrepresentations for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas,\nDavid Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind\ncontrol suite. arXiv preprint arXiv:1801.00690, 2018.\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\nLearning spatiotemporal features with 3d convolutional networks. In Proceedings of\nthe IEEE international conference on computer vision, pp. 4489–4497, 2015.\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with\ndouble q-learning. In Proceedings of the AAAI conference on artiﬁcial intelligence,\nvolume 30, 2016.\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc\nVan Gool. Temporal segment networks: Towards good practices for deep action\nrecognition. In European conference on computer vision, pp. 20–36. Springer, 2016.\nBolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational\nreasoning in videos. In Proceedings of the European Conference on Computer Vision\n(ECCV), pp. 803–818, 2018.\nMohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efﬁcient con-\nvolutional network for online video understanding. In Proceedings of the European\nconference on computer vision (ECCV), pp. 695–712, 2018.\n11\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-09-05",
  "updated": "2021-10-27"
}