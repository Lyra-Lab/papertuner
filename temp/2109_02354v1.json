{
  "id": "http://arxiv.org/abs/2109.02354v1",
  "title": "Method for making multi-attribute decisions in wargames by combining intuitionistic fuzzy numbers with reinforcement learning",
  "authors": [
    "Yuxiang Sun",
    "Bo Yuan",
    "Yufan Xue",
    "Jiawei Zhou",
    "Xiaoyu Zhang",
    "Xianzhong Zhou"
  ],
  "abstract": "Researchers are increasingly focusing on intelligent games as a hot research\narea.The article proposes an algorithm that combines the multi-attribute\nmanagement and reinforcement learning methods, and that combined their effect\non wargaming, it solves the problem of the agent's low rate of winning against\nspecific rules and its inability to quickly converge during intelligent wargame\ntraining.At the same time, this paper studied a multi-attribute decision making\nand reinforcement learning algorithm in a wargame simulation environment, and\nobtained data on red and blue conflict.Calculate the weight of each attribute\nbased on the intuitionistic fuzzy number weight calculations. Then determine\nthe threat posed by each opponent's chess pieces.Using the red side\nreinforcement learning reward function, the AC framework is trained on the\nreward function, and an algorithm combining multi-attribute decision-making\nwith reinforcement learning is obtained. A simulation experiment confirms that\nthe algorithm of multi-attribute decision-making combined with reinforcement\nlearning presented in this paper is significantly more intelligent than the\npure reinforcement learning algorithm.By resolving the shortcomings of the\nagent's neural network, coupled with sparse rewards in large-map combat games,\nthis robust algorithm effectively reduces the difficulties of convergence. It\nis also the first time in this field that an algorithm design for intelligent\nwargaming combines multi-attribute decision making with reinforcement\nlearning.Attempt interdisciplinary cross-innovation in the academic field, like\ndesigning intelligent wargames and improving reinforcement learning algorithms.",
  "text": "Method for making multi-attribute decisions in wargames by combining intuitio\nnistic fuzzy numbers with reinforcement learning\nYuxiang Sun 1,*, Bo Yuan 2, Yufan Xue 1, Jiawei Zhou 1, Xiaoyu Zhang 1 and Xianzhong\nZhou 1,*\n1\nSchool of Management and Engineering, Nanjing University, Nanjing 210023, China;\nyufanxue1@163.com (Y.X.); JiaweiZhou163@163.com (J.Z.); zhangxy0903@126.com (X.Z.)\n2\nSchool of Electronics, Computing and Mathematics, University of Derby, Kedleston Rd, Derby DE22\n1GB, UK; b.yuan@derby.ac.uk\n* Correspondence: sunyuxiangsun@126.com (Y.S.); zhouxz@nju.edu.cn (X.Z.); Tel.:\n+86-1882-705-9351(Y.S.)\nAbstract\nResearchers are increasingly focusing on intelligent games as a hot research area.The article\nproposes an algorithm that combines the multi-attribute management and reinforcement learning\nmethods, and that combined their effect on wargaming, it solves the problem of the agent's low\nrate of winning against specific rules and its inability to quickly converge during intelligent\nwargame training.At the same time, this paper studied a multi-attribute decision making and\nreinforcement learning algorithm in a wargame simulation environment, and obtained data on red\nand blue conflict.Calculate the weight of each attribute based on the intuitionistic fuzzy number\nweight calculations. Then determine the threat posed by each opponent's chess pieces.Using the\nred side reinforcement learning reward function, the AC framework is trained on the reward\nfunction, and an algorithm combining multi-attribute decision-making with reinforcement learning\nis\nobtained.\nA\nsimulation\nexperiment\nconfirms\nthat\nthe\nalgorithm\nof\nmulti-attribute\ndecision-making combined with reinforcement learning presented in this paper is significantly\nmore intelligent than the pure reinforcement learning algorithm.By resolving the shortcomings of\nthe agent's neural network, coupled with sparse rewards in large-map combat games, this robust\nalgorithm effectively reduces the difficulties of convergence. It is also the first time in this field\nthat an algorithm design for intelligent wargaming combines multi-attribute decision making with\nreinforcement learning.Attempt interdisciplinary cross-innovation in the academic field, like\ndesigning intelligent wargames and improving reinforcement learning algorithms.\nKey words: Wargame, Reinforcement learning, Multiple attribute decision making, Intelligent\ngame\n1、Introduction\nArtificial\nintelligence\nand\nmachine\nlearning\nare\nbecoming\nmore\ncommon\nin\nreal-world applications, and games are increasingly fighting against humans through training\nagents. AlphaGo, an artificial intelligence that has achieved success in the field of Go, and\nAlphastar, an artificial intelligence that has achieved success in the man-machine conflict\nof\n’StarCraft’\nare\ntwo\ntypical\nexamples.\n[1-2].\nIn RTS games, artificial intelligence methods are increasingly being integrated. In the King Glory\nGame, Ye D used his improved PPO algorithm to train the hero AI, with positive results. [3]. By\nusing\nreinforcement\nlearning\nalgorithms,\nSilver\nD\ndeveloped\na\ntraining\nframework that requires no human knowledge other than the rules of the game, allowing AlphaGo\nto train itself, and achieving high levels of intelligence in the process [4]. Using deep\nreinforcement learning and supervised strategy learning, Barrigan improves the AI performance of\nRTS games, and defeats the built-in AI [5]. AI has become a hot research topic in recent years,\nshowing a wide variety of applications such as deduction and analysis [6-7]. There are still\ninsufficient effective solutions to the problem of convergence and convergence rate under a variety\nof conditions, especially when it comes to confrontation.\nIndexes measure the value of things or the parameter of an evaluation system. It is the scale\nof the effectiveness of things to the subject. As an attribute value, it provides the subjective\nconsciousness or the objective facts expressed in numbers or words. It is important to select a\nscientifically valid target threat assessment (TA) index and evaluate that index scientifically[8].\nTarget threat assessment contributes to intelligence wargame decision-making as part of current\nintelligent wargames. It is mainly based on rules, decision trees, reinforcement learning, and other\ntechnologies in the current mainstream game intelligent decision-making field, but rarely\nincorporates multi-attribute decision-making theory and methods of management into the\nintelligent\ndecision-making\nfield.\nThe\nactual\nwargame\ndata\nobtained\nthrough\nwargame\nenvironments is presented in this paper, as well as the multi-attribute threat assessment indicators\nthat are effectively transformed and presented as a unified expression. Using the three expression\nforms of real number, interval number, and intuitionistic fuzzy number, the multi-attribute\ndecision-making theory and method are used to analyze the target threat degree, and then the\nreward function in reinforcement learning is established to train more effective intelligent\ndecision-making\nalgorithm.For\nthe\nfirst\ntime,\nthis\nmethod\ncombines\nthe\nmulti-attribute\ndecision-making of management science and reinforcement learning of control science. This\nmethod appears to be effective based on a wargame experiment.\nTo this end, this article conducts the following research:\n（1）In this article we attempt to combine the multi-attribute decision-making method in\nmanagement with reinforcement learning in cybernetics for the first time by extracting data from\nthe environment in wargames and combining real numbers, interval numbers, and intuitionistic\nfuzzy numbers for data expression.Perform the multi-attribute decision analysis on this basis,\nanalyze the threat of the opponent, and then feed back the reward function. The reward function is\ntrained in an algorithm of reinforcement learning to obtain a more ideal result.\n（2）Increase the training convergence speed. To solve the sparsity problem involved in\nagent training, one that leads to the emergence of agent strategies that are non-convergent or\nslowly convergent. The main solution is to use multi-attribute decision making（MADM）to figure\nout the threat of the opponent in advance and determine it in real time in a certain step, then attack\nbased on the threat in order to obtain additional rewards and add it to the training process.\n（3）Increase the winning rate of chess piece agent training.The intelligence of the agent\nimproves by predicting the opponent's most threatening chess piece and beating it as a result of\npractice, which is directly reflected in the agent's winning rate against a regular opponent.\n2、Related theories\n2.1 The reinforcement learning\nReinforcement learning is a large category of machine learning that is based on solving\ninteraction problems using Bellman equations [9], helping to improve and ultimately achieve the\ngoal.Ultimately, reinforcement learning causes the agent to form a strategy that maximizes the\nreward value in order to achieve the goal [10]. During the 1990s, Littman proposed multi-agent\nreinforcement learning based on the MDP as the framework and applied the ideas and algorithms\nof reinforcement learning to multi-agent systems, frequently considering both competition\nbetween agents and cooperation between them [11]. Reinforcement learning starts with a Markov\nProcess (MDP), which describes the interaction process between the agent and the environment\nthrough state and action models. As a general rule, MDP is a quadruple of < S, A, R, T >\nconsisting of 4 elements:\n(1) S is a finite State Space, which represents all the states of the Agent in the environment;\n(2) A is a limited Action Space, which contains the actions that the Agent is capable of taking\nin each state;\n(3) Rss'\na\nmeans that the agent performs a action in the s state, and the agent is rewarded by\nthe environment interaction;\n(4) The state transition function (STF) of an environment is Pss'\na = ℙSt+1 = s'∣St = s, At =\na , which represents the probability of performing action a on state s and transitioning to state s'.\nIn MDP, the agent interacts with the environment in the way shown in Figure 1.\nFigure 1 Schematic diagram of reinforcement learning and environment\nIt interprets the current environment state st and selects at action from its action space A;\nsubsequently, the environment sends the corresponding reward rt+1 to the agent, and transfers it\nto the new environment state st+1. Wait for the agent to make the next new decision [12].When an\nagent interacts with its environment, there are two uncertainties: one is what kind of action to\nchoose in state S. The strategy π(a|s) is used to represent a certain strategy of the agent (i.e. the\nprobability distribution from state to action). And the other is the probability Pss'\na\nof a state\ntransition generated by the environment. The goal of reinforcement learning is to find an optimal\nstrategy π(a|s) so that it can obtain the maximum long-term cumulative reward in any state s\nand any time step t.\n*\n0\nargmax\nk\nt k\nt\nk\nr\ns\ns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n∣\nE\nAmong these, ॱπ\nrepresents the expected value given the strategy, γ ∈[0，1) the\ndiscount rate , k the next time period, and rk+t the instant reward the agent receives in the time\nperiod (t + k).\nIn reinforcement learning, we mostly learn the optimal strategy π∗by finding the optimal\nstate value function V∗s\nor the optimal state action value function Q∗(s，a) . Here are the\nformulas for V∗s\nand Q∗(s，a).\n*\n0\n( )\nmax\nk\nt k\nt\nk\nV\ns\nr\ns\ns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n∣\nE\n*\n0\n( , )\nmax\n,\nk\nt k\nt\nt\nk\nQ s a\nr\ns\ns a\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n∣\nE\nReinforcement learning has attracted researchers' attention in the field of RTS games in the\nrecent years.In this paper, we use reinforcement learning algorithms to make intelligent decision\nabout wargame, establish an intelligent environment for playing wargames based on reinforcement\nlearning algorithms, use reinforcement learning algorithms to select chess pieces, and create a\nstrong AI for game intelligence.\n2.2 Making multi-attribute decisions\nThe field of decision-making has been a research hotspot in management, economics, and\ninformation science for a long time, and research on decision-making theory and methods has also\nbeen fruitful.Multiple attribute decision-making is a limited option selection problem based on\nmultiple attributes that has a wide practical background [13][14][15]. For the multi-attribute utility\ntheory, the value function is used to express the preference theory based on the concept of ordinal\ncomparison and preference strength, and the utility function is used to express the preference\ntheory based on the concept of risk choice [16]. Multi-attribute decision making (MADM) is\ncommonly known as finite-scheme multi-objective decision-making, and it is an essential\ncomponent of decision-making theory and method research [17]. The first problem of\nmulti-attribute decision making is to determine the scheme set and the attribute set. Let A = {A1,\nA2,..., An} be the scheme set and G = {G1, G2, G3,..., GM} be the attribute set of multi-attribute\ndecision making. The attribute value of scheme AI to attribute Gj is Yij (i = 1,2,..., n, j = 1,2,..., m).\nThe decision matrix Y (nxm) is composed of Yij. The plan set is the object of decision making. The\ndecision matrix provides information needed to build the decision plan. Different types of analysis\nuse the decision matrix to make decisions.\nWith a system with multiple targets, one target may be considered a decision-making plan, a\nplan set consisting of all the opponent's targets, such as tanks, helicopters, and infantry.The\ndecision criterion is the threat level our opponent poses to our defending target. It includes a\nnumber of attributes that affect the extent of threat, such as target type, target distance, target\nspeed, target attack ability, target defense ability, target environment, and visibility.Based on the\nactual wargame data obtained, apply reasonable methods to evaluate target threats.\n2.3 The intuitionistic fuzzy number\nWhile the application of fuzzy number theory to information processing technology has\ngradually matured, its limitations have slowly emerged[18]. The fuzzy number A has a unique real\nnumber in [0,1] corresponding to each of its elements. However, fuzzy number theory cannot be\napplied to problems of either one or the other nature at the same time. Therefore, scholars have\ndeveloped fuzzy number theory. In 1986, Atanassov devised intuitionistic fuzzy numbers and\nexplored the nature and theorems of their operations.\nThe intuitionistic fuzzy number definition:\nIn the universe of\n\n\n0\n1\nA\nix\n\n\n\n, the intuitionistic fuzzy set A on X has the following\nform:\n\n\n\n\n\n\n,\n,\ni\nA\ni\nA\ni\ni\nA\nx\nx\nv\nx\nx\nX\n\n\n\n∣\nIn the formula,\n\n:\n[0,1]\nA\nix\nX\n\n\nand\n\n:\n[0,1]\nA\ni\nv\nx\nX \nare the membership\nfunction\nand\nnon-membership\nfunction\nof\nA,\nrespectively,\nand\nthey\nhold\nfor\nall\n\n\n\n\n,0\n1\ni\nA\ni\nA\ni\nx\nX\nx\nv\nx\n\n\n\n\n\non A.\n\n\n\n\n\n\n1\nA\ni\nA\ni\nA\ni\nx\nx\nv\nx\n\n\n\n\nis the hesitation in A,\nand it is a measure of the hesitation of xi with A. Clearly,\n\n\n0\n1\nA\nix\n\n\n\n.\nAs a convenience, call\n\n\n,v\n\n\n\n\n\nthe Intuitionistic Fuzzy Number (IFN),There is\n[0,1]\n\n\n,\n[0,1]\nv\n, and\n1\nv\n\n\n\n\n.\nDefinition 2: The algorithm of intuitionistic fuzzy numbers, set any intuitionistic fuzzy\nnumber to\n\n\n\n\n,\n,\n,\nv\nv\n\n\n\n\n\n\n\n\n\n\n, then\n1）\n\n\n,v v\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n；\n2）\n\n\n,v\nv\nv v\n\n\n\n\n\n\n\n\n\n\n\n\n\n；\n3）\n\n\n\n\n\n\n1\n1\n,\n,\n0\nv\n\n\n\n\n\n\n\n\n\n\n\n；\n4）\n\n\n\n\n\n\n( )\n,1\n1\n,\n0\nv\n\n\n\n\n\n\n\n\n\n\n\n\n；\nDefinition 3: In this calculation we assume\n,\n,\n1,2,\n,\ni\ni\ni\na\na\na\nv\ni\nn\n\n\n\n\nis an IFN,\n\n\n1\n2\n,\n,\n,\nn\nw w\nw\n\nW\n\nis a weighted vector,\n1\n1,\n[0,1],\n1,2,\n,\nn\nj\nj\nj\nw\nw\nj\nn\n\n\n\n\n\n\nintuitionistic\nfuzzy weighted chess pieces satisfy IFWA, and it's a\nn\nmapping:\n\n\n\n\n\n\n1\n2\n1 1\n2\n2\n1\n1\nIFWA\n,\n,\n,\n1\n1\n,\nj\nj\nj\nj\nn\nn\nw\nw\nn\nn\nn\na\na\nj\nj\na a\na\nw a\nw a\nw a\nv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4:\n\n\n,\n,\nL\nU\nL\nU\nL\nU\na\na\na\nx a\nx\na a\na\n\n\n\n\n\n\n\n\n\nR\n\n∣\nis called an interval number,\nL\na\nis the lower limit of the interval, and\nU\na\nis the upper limit. When\nL\nU\na\na\n\n, a\ndegenerate into a real number, the interval number can be regarded as an extension of the real\nnumber. Here are the interval numbers\n,\nL\nU\na\na\na\n\n\n\n\n\nand\n,\nL\nU\nb\nb\nb\n\n\n\n\n\n, and the real number\n0\n\n. The interval number arithmetic rules are as follows:\n1）Addition:\n,\nL\nL\nU\nU\na\nb\na\nb\na\nb\n\n\n\n\n\n\n\n\n\n\n2）Subtraction:\n,\nL\nU\nU\nL\na\nb\na\nb\na\nb\n\n\n\n\n\n\n\n\n\n\n3）Number multiplication:\n,\nL\nU\na\na\na\n\n\n\n\n\n\n\n\n，particularly when\n0\n\n,\n0\na\n\n\n\n；\n4）Multiplication:\n\n\n\n\nmin\n,\n,\n,\nmax\n,\n,\n,\nL\nL\nL\nU\nU\nL\nU\nU\nL\nL\nL\nU\nU\nL\nU\nU\na b\na b\na b\na b\na b\na b\na b\na b\na b\n\n\n\n\n\n\n\n;\n5）Division:\n/\n,\n1/\n,1/\n,\n,\n0\nL\nU\nU\nL\nL\nU\na b\na\na\nb\nb\nb\nb\n\n\n\n\n\n\n\n\n\n\n3、Wargaming multiple attribute index threat quantification\nObtaining scientific evaluation results requires a reasonable quantification of indicators. An\nimportant aspect of decision-making assistance in wargames is target threat assessment, and the\nevaluation result directly affects the effectiveness of wargame AI [19]. The aim of this section is to\nintroduce threat quantification methods for different types of indicators. By combining the target\ntype, this section divides the target into target distance threat, target attack threat, target speed\nthreat, terrain visibility threat, environmental indicator threat, and target defense value. The\nacquired confrontation data are incorporated into different indicator types, and then the\ncorresponding comprehensive threat value is calculated.In the table 1 are the attributes and\nmeanings of specific indicators.\nTable 1 A list of indicator attributes and their meanings\nIndicator\nAttribute\nMeaning\nTarget distance threat\nCost type\nDistance between the two parties will influence the kill probability\nTarget attack threat\nBenefit type\nThreat degrees should be determined by the opponent's type, range,\nand lethality of the weapon\nTarget speed threat\nBenefit type\nThe threat of speed from our opponents\nTerrain visibility\nthreat\nIntervisibility >\nno intervisibility\nWhether or not the terrain is visible will directly impact the threat\nEnvironmental\nindicator threat\nBenefit type\nWhile the opponent's environment is conducive to concealment,\nmobility is more dangerous.\nTarget defense value\nCost type\nThe stronger the opponent's armor, the harder it is to destroy it\n3.1 Quantification of threat distance indicators\nTarget distance is an important parameter to evaluate the threat degree of the target [20]. In\nthe wargame environment, the distance between entities can be calculated using the ranged tool.\nTo simplify the process, real numbers are used to determine the distance between entities.The\ntraditional threat quantification method uses only the target distance, whereas this article considers\nthe distance from the control point in the wargame environment. The player who reaches the\ncontrol point first wins, regardless of who reaches it first. This consideration will also be a factor\nin the target distance indicator.\nFor a red tank i and a blue tank j in wargame, calculate the target distance j threat to the red\ntank i.The coordinates of the control point are\n( , )\nO x y , and\ncommon\n\nis the strength of the tank\npiece needed to pass over a grid of ordinary terrain,\n( , )\nx y\n\nis the stamina consumed by the\nvehicle through special terrain, D( ,\n)\nJ O\nis the distance from tank\nmax\nD\nto the control point, E\nis the maximum distance from the imaginary boundary, and\nij\nD\nis the grid distance between\ntanks i and j. Based on a comprehensive analysis of the terrain, it can be seen that for the red tank i,\nthe closer the blue tank j is to the control point and the closer it is to the red tank, the greater the\nthreat of the distance indicator.\nUsing the following formula, we can calculate the target distance threat index of red tank i\nand blue tank j. The threat quantification of the target distance between the blue tank j and the red\ntank i is designated as\n( , )\ni x y\n\n, the control threat value is designated as\n( , )\nj x y\n\n, and the\ncomprehensive target distance quantitative value of the blue tank j as compared to the red tank i is\ndesignated as\n( , )\nij x y\n\n, where i and j are the number of tanks on the red side and blue side\nrespectively.\ncommon\nmax\nD( ,\n)\n( , )\n1\n( , )\nj\nJ O\nx y\nx y\nD\n\n\n\n\n\n\n\n\n\n\n\nmax\n( , )\ni\nij\nx y\nD\nD\n\n\n\n\n\n1\n( , )\n+\n2\nij\nj\ni\nx y\n\n\n\n\n3.2 Quantification of threat indicators for target speeds\nThreat levels of the target speed are a function of target motion state. The faster the target\nmoves, the faster its position and environment change, and the harder it is for us to hit it, so the\ngreater the threat level [21]. Therefore, the target speed threat degree can be calculated based on\nthe benefit index, that is, the greater the target speed, the greater the threat. As a reference for the\ntarget combat intention estimation, the speed direction information can be used. Here, only the\nscalar of the target speed is considered. As for different types of targets, their speed determines\ntheir quantified value of threat. For example, composite armored tanks, heavy tanks, and light\ntanks all have different speeds. For the composite armor target, Vcomposite-max, the heavy tank target\nVheavy-max, the light tank target Vlight-max, and the relative speed between the opponent's tank target\nTj, and our evaluation node Wi, Vij, quantitatively according to the target type, and the speed threat\ndegree Tvij as\n1\n2\n3\n,\n C\n \n \n,\n H\n \n,\n \nij\ncomposite max\nij\nvij\nheavy max\nij\nlight max\nv\nomposite armored tanks\nV\nv\nT\neavy tanks\nV\nv\night tanks\nV\nL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs part of this formula,\n1\n2\n3\n,\n,\n[0,1]\n\n\nis the threat factor for composite armored tanks,\nheavy tanks, and light tanks, which represents the speed threat characteristics for different types of\ntargets, respectively. The speed threats of general composite armored tanks and light tanks are\ngreater than those of heavy tanks. The value of\n1\n2\n3\n,\n,\n\n\ncan be calculated in advance by\nprofessionals according to the game characteristics of a particular target, in order to effectively\ncharacterize the target's speed characteristics.\n3.3 Quantification of threat indicators for target attacks\nIn calculating the attack capability of a target tank, the attack capability threat function is\nmainly considered:\n\n\n\n\n1\n2\n1\n2\n3\n4\nln\nln\n1\nln\nC\nB\nA\nA\n\n\n\n\n\n\n\n\n\n\n\nAssuming B is the maneuverability of the tank pawn; A1 is the weapon attack capability of\nthe tank pawn; A2 is the detection capability of the tank pawn; By calculating the threat value of\nthe target tank chess piece based on its attack capability threat function,\n1\nis the forward firing\ncapability,\n2\n\nis the bomb-carrying capability,\n3\nis the electronic countermeasure capability,\nand\n4\n\nis the missile offensive capability of the tank pawn.\n3.4 Quantification of terrain visibility threats\nIn the assessment of threats, whether the targets can see each other is a significant factor. In\nparticular, tanks, which are direct-pointing weapons, are important. To aim and track a target, one\nmust see the target. A simplified view of the blue tank targets Tj and Wi is shown in the figure 2.\n(a)\n(b)\nFigure 2 The visibility of the red and blue tanks.(a) Both blue and red have the same terrain\nelevation, and both are visible. (b) Because the terrain between the red and blue is high and the\nelevation on both sides is low, the red and blue will not be visible\nDetermine whether both parties' location information can be connected through the visibility\ninterface of the wargame environment. A visibility interface uses the elevation between the direct\nconnections between the two parties as a reference standard. If the direct elevation between the\ntwo parties is higher than the location of the two parties, inter-view is not possible; otherwise, it is\npossible. Based on the visibility interface and the locations of both parties, evaluate the terrain\nvisibility threat fij of the target blue side Tj and red side Wi.\n\n\n\n\n\n\n\n\n2\n0,\n0,\n0,\n0,\n1\n0,\n0,\n0,\n1 ,\nmax\n( )\n0\nmax\n( )\n0\n0,\n,\nmax\n( )\n0\nmax\n( )\n0\n0,\n,\nmax\n( )\n0\nmax\n( )\n0\n1,1 ,\nmax\n( )\n0\n,\n0\nij\nij\nij\nij\nij\nij\nij\nij\nj\nij\nl\nL\nl\nL\nij\nj\nij\nl\nL\nl\nL\nij\nij\nj\nij\nl\nL\ni\ni\ni\nl\nL\nij\nj\nl\nL\nt\nH\nl\nH\nH\nl\nH\nH\nl\nH\nH\nl\nH\nf\nt\nH\nl\nH\nH\nl\nH\nH\nl\nH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n且\n且\n且\n\n\n0,\n1\n2\nmax\n( )\n0\n,\n,\nij\ni\nij\nl\nL\nH\nl\nH\nt t\nother\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n且\n1\n2\n,\n(0,1)\nt t \nis a quantitative parameter,\n1\n2\nt\nt\n\nis expressed by the number of intervals, Lij\nis the linear distance between the blue tank target Tj and the red tank Wi, and l is the horizontal\ndistance from the actual point to the target Tj. Hij (l) is the distance between red and blue targets.Tj\nrepresents the actual terrain height of l, Hi and Hj represent the actual terrain height of the\npositions of tank Wi and tank Tj.\nIf the evaluation node and target are both fully visible and are above the mid-elevation, the\nterrain visibility threat degree is [t2,1]; when neither can communicate, the threat degree is [0,0];if\nthe red-sided Wi is situated at a high point and higher than the middle elevation, and the blue-sided\nTj is situated at a low point and lower than the middle elevation, then the threat degree is\n[0,t1];whenever the blue side of Tj is higher than the middle elevation, and the red side of Wi is\nlower than the middle elevation, the threat degree is [1,1]; As the shooting can be completed\nwithout a cross-view during indirect shooting, the degree of threat is uniformly set to [t1,t2]. By\ncombining the target's position and elevation with the wargame environment, it is possible to\nevaluate the visibility of the target to the red tank Wi by combining the visibility of target with\nenvironment.\n3.5 Quantification of environmental indicators\nThe effects of the wargame confrontation environment are an important determinant of\ncombat effectiveness. The better the environmental indicators, the better the equipment\neffectiveness of the chess pieces, and the worse the environmental indicators, the worse the\ncombat effectiveness of the chess pieces[22-23]. Due to the fact that the simulation is carried out\nin a wargame environment, this article extracts the two major influencing factors for the wargame\nenvironment, urban residential areas and highways, for the quantification of environmental\nindicators.Tanks and infantry personnel can be concealed in urban residential areas so that the\nenemy cannot detect our targets, which is conducive to our defense. Highways can speed up the\nchess pieces' moving speed, and there are first-level and second-level highways in the wargame\nenvironment. The higher the level of the road, the better the chess pieces' speed.Therefore,\nenvironmental indicators can be quantified according to the environment where chess pieces are\nlocated, considering surrounding urban residential areas and road conditions. Determine whether\nthere are first-class roads, second-class roads and residential areas in the two squares around the\nred tank Wi. Gaining threat degree Tei through judgment.\n1 1\n2\n2\n3\ni\nTe\nw h\nw h\nw r\n\n\n\nh1, h2, and r represent first-class roads, second-class roads, and urban residential areas,\nrespectively, and w1, w2, and w3 represent weight vectors. If the above terrain environment is\nlocated around the chess piece, the associated value is assigned.\n3.6 Quantification of target defense\nGenerally, the defense quantification of chess pieces includes the armor protection attributes.\nThe different types of armor include composite armor, heavy armor, medium armor, light armor,\nand unarmored. This article assigns different armors different defense values, based on their armor\nprotection capabilities.The table 2 below shows.\nTable 2 Quantified value of target defense\nArmored attributes\nQuantified value\ncomposite armor\n1\nheavy armor\n0.7\nmedium armor\n0.5\nlight armor\n0.3\nunarmored\n0\n4 、Establishment\nof\na\nmulti-attribute\nquantitative\nthreat\nmodel\nbased\non\nintuitionistic fuzzy numbers\nBy using the interval number method, this article indicates whether visibility is possible, and\ndifferent threats are generated. Nevertheless, the quantified values of other threat targets are real\nnumbers. To unify the problem solving method, this paper converts all interval numbers and real\nnumbers to intuitionistic fuzzy numbers, and calculates the size of the threat by calculating the\nintuitionistic fuzzy numbers.\n4.1 Principles of conversion between different description forms\nThe quantitative multi-attribute index set represented by different forms is X, and the\nquantitative index is x,\nx\nX\n\n; the intuitionistic fuzzy set is Y, and the intuitionistic fuzzy\nnumber of the index is y,\ny\nY\n\n. When converting indicators of different representations,\nconsidering the following principles should be followed in order to ensure that the form of the\nconversion is scientific and reasonable:\n1) Range limitations\nIn mapping x\ny\n\n, if x\nX\n\nis present, then y\nY\n\n.\n2）Characteristics of boundary\nIf x is the upper bound of its representation, then\n1,0\ny \n. And if x is the lower bound\nof its representation, then\n0,1\ny \n.\n3）Monotonic map\nWhen\n1\n2\nx\nx\n\n, then\n1\n2\ny\ny\n\n; if\n1\n2\nx\nx\n\n, then\n1\n2\ny\ny\n\n, if\n1\n2\nx\nx\n\n, then\n1\n2\ny\ny\n\n.\n4.2 The interval number is converted into an intuitionistic fuzzy number\nDuring the wargame confrontation, it is difficult to draw accurate conclusions about the level\nof visibility. This article, therefore, uses the interval number method to express this type of\ndifficult-to-determine information. To uniformly express that all the numbers in this article are\ntransformed into intuitionistic fuzzy numbers for representation, here is a conversion method\nbetween interval numbers and intuitionistic fuzzy numbers. The interval number\n,\nL\nU\ni\ni\ni\na\na\na\n\n\n\n\n\nis transformed into intuitionistic fuzzy number\n,\ni\ni\ni\nf\n\n\nusing the following formula.\n\n\n\n\n1,2,\n,\n1,2,\n,\nmax\n1\nmax\ni\ni\nU\ni\nn\ni\nU\ni\ni\nU\ni\nn\ni\na L\na\na\na\n\n\n\n\n\n\n\n\n\n\n\n\n\nProof: 1) Range restrictions\nIf\n,\nL\nU\ni\ni\ni\na\na\na\nR\n\n\n\n\n\n\n\n，then\n\n\n1,2,,\nmax\nL\ni\ni\nU\ni\nn\ni\na\na\n\n\n\n，\n\n\n1,2,\n,\n[0,1]\nmax\nL\ni\ni\nU\ni\nn\ni\na\na\n\n\n\n\n\n，\n\n\n1,2,\n,\n1\n[0,1]\nmax\nU\ni\ni\nU\ni\nn\ni\na\na\n\n\n\n\n\nso\n\n\n0\n1\n1\nmax\nU\nL\ni\ni\ni\ni\nU\ni\ni\na\na\na\n\n\n\n\n\n\n\n2) Boundary characteristics\nia\ntakes\nthe\nlower\nbound\n[0,0]\n,\nand\n\n\n1,2,\n,\nmax\n0\nU\ni\nn\nia\n\n\n\n.\nApparently\n\n\n1,2,\n,\n0\nmax\nL\ni\ni\nU\ni\nn\ni\na\na\n\n\n\n\n\nfollows.\n3) Mapping monotonic\nSet\n1\n1\n1\n,\nL\nU\na\na\na\n\n\n\n\n\nand\n2\n2\n2\n,\nL\nU\na\na\na\n\n\n\n\n\nintervals. The degree of uncertainty between these\nnumbers is usually expressed as a probability relation. In order to judge the monotonicity of the\nmapping, it is important to use the determined relationship (possibility is 1).So, when determining\nthe size relationship, this article is mainly concerned with three situations:\n1a\nis smaller than\n\n\n2\n1\n2\na\na\na\n\n\n\n\nby the full amount，\n1a\nequal\n\n\n2\n1\n2\na\na\na\n\n\n\n\nand\n1a\nis completely greater than\n\n\n2\n1\n2\na\na\na\n\n\n\n\nA）If\n1\n2\na\na\n\n\n\nand\n1\n1\n2\n2\nL\nU\nL\nU\na\na\na\na\n\n\n\n, then\n\n\n\n\n1\n2\n1\n2\n1,2,\n,\n1,2,\n,\n0\nmax\nmax\nU\nU\ni\nn\ni\ni\nn\ni\na L\na L\na\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n2\n1\n1\n2\n1,2,\n,\n1,2,\n,\n1,2,\n,\n1\n1\n0\nmax\nmax\nmax\nU\nU\nU\nU\nU\nU\nU\ni\nn\ni\ni\nn\ni\ni\nn\ni\na\na\na\na\na\na\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo\n1\n2\nf\nf\n\nB）If\n1\n2\na\na\n\n\n, obviously\n1\n2\n\n\n\n,\n1\n2\n\n\n\n, so\n1\n2\nf\nf\n\nC）If\n1\n2\na\na\n\n\n，and\n2\n2\n1\n1\nL\nU\nL\nU\na\na\na\na\n\n\n\n。Then\n\n\n\n\n1\n2\n1\n2\n1,2,\n,\n1,2,\n,\n0\nmax\nmax\nL\nU\nU\ni\nn\ni\ni\nn\ni\na L\na\na\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n2\n1\n1\n2\n1,2,\n,\n1,2,\n,\n1,2,\n,\n1\n1\n0\nmax\nmax\nmax\nU\nU\nU\nU\nU\nU\ni\nn\ni\ni\nn\ni\ni\nn\ni\naU\na\na\na\na\na\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo\n1\n2\nf\nf\n\n。\n4.3 Convert real numbers into intuitionistic fuzzy numbers\nTo unify the index representation form, the real number type index representation must also\nbe converted into intuitionistic fuzzy numbers. The following is the formula for calculating the\nmembership degree and non-membership degree of the conversion of real numbers into IFN.\nBeneficial index\n\n\n1,2,\n,\nmax\n1\nmax\n1,2,\n,\ni\ni\ni\nn\ni\ni\ni\ni\na\na\na\na\ni\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe cost index\n\n\n1,2,\n,\nmin\nmin\n1,2,\n,\n1\ni\nn\ni\ni\ni\ni\ni\ni\na\na\ni\nn a\na\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the formula, real numbers are considered special cases of interval numbers. The function\nof factor\n[0,1]\n\nis to prevent the deterministic characteristics of real numbers from affecting\nother uncertainty indicators. The proof: can treat real numbers as special cases of interval numbers,\nso it can prove that real numbers are converted into intuitionistic fuzzy numbers via the same\nmethod used to prove interval numbers are converted into direct fuzzy numbers.\n4.4 Quantitative calculation of fuzzy numbers intuitively based on multiple\nattributes\n（1）This intuitionistic fuzzy entropy describes the degree of fuzzy judgment information\nprovided by an intuitionistic fuzzy set. The larger the intuitionistic fuzzy entropy of an evaluation\ncriterion, the smaller the weight should be given; otherwise, the larger needs to be. Based on\nformulas from the literature[24], we calculated the entropy weights for each intuitionistic fuzzy.\nAmong them, ideal solution\niS is a conceived optimal solution (scheme), and its attribute values\nhit the best value among the alternatives; and the negative ideal solution\niS is the worst\nconceived solution (scheme), and its attribute values hit the worst value among the alternatives.\nip\nis generated by comparing each alternative scheme with the ideal solution and negative ideal\nsolution. If one of the solutions is closest to the ideal solution, but at the same time far from the\nnegative ideal solution, then it is the best solution among the alternatives.\n\n\n\n\n\n1\n1\nln\nln\nln\n1\nln 2\nln 2\nm\nj\nij\nij\nij\nij\nij\nij\nij\nij\nij\nij\ni\nH\nv\nv\nv\nv\nv\nn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf\n0\nij\n\n，\n0\nijv \n，\n0\nij\nijv\n\n\n，then\nln\n0\nij\nij\n\n\nln\n0\nij\nij\nv\nv \n,\n\n\n\nln\n0\nij\nij\nij\nij\nv\nv\n\n\n\n\n\nThe entropy weight of the j attribute is defined as:\n1\n1\nj\nj\nn\nj\nj\nH\nw\nn\nH\n\n\n\n\nAmong\n1\n0,\n1,2,\n, ,\n1\nn\nj\nj\nj\nw\nj\nn\nw\n\n\n\n\n\n\n（2 ）Determine the optimal solution A+ and the worst solution A- using the following\nformula:\n\n\n\n\n1\n1\n2\n2\n1\n1\n2\n2\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\n,\nn\nn\nn\nn\nA\nA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn which\n\n\n\n12\n1,2,\n,\nmax\n,\nmin\ni\nj\nm\nij\ni\nj\nm\nij\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1,2,\n,\n1,2,\n,\nmin\n,\nmax\ni\nj\nm\nij\ni\nj\nm\nij\n\n\n\n\n\n\n\n\n\n\n\n\n（3）Calculate the similarity between the fuzzy intuitionistic A and B as follows:\n\n\n\n\n\n\n\n\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n2\n2\n2\n2\n,\n,\n,\n1\n1\n3\n2\n3\n2\ns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn which，\n1\n1\n1\n2\n2\n2\n1\n,\n1\n\n\n\n\n\n\n\n\n\n（4）Calculate the similarity\niS \nand\niS between each solution and the optimal solution\nand the worst solution based on the following formula:\n\n\n\n\n1\n1\n,\n,\n,\n,\n,\n,\nn\ni\nk\nk\nk\nik\nik\nk\nn\ni\nk\nk\nk\nik\nik\nk\nS\nw\ns\nS\nw\ns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(5) Then calculate the relative closeness\n\n\n/\ni\ni\ni\ni\np\nS\nS\nS\n\n\n\n\n\nComparing threat levels of opponents based on their closeness to the target depends on the\nlevel of threat assessment performed.\n5、Multi-attribute threat quantitative simulation experiment using intuitionistic\nfuzzy numbers\nThe threat assessment problem is transformed into a multi-attribute decision-making problem,\nwhile the combat intention of the target is incorporated into the evaluation system to make the\nevaluation system more reasonable and the results more reliable. A simulation scene depicts ten\ntanks of the red and blue sides fighting each other, and ten opposite are found as chess pieces in\nthe wargame. By using the war game deduction environment, each tank piece's attribute data will\nbe derived in real time, including the opposite piece's position (hexagon number), elevation, type,\ndistance, range, strike power and control point distance, as well as armor thickness. A table 3 is\nshown below.\nTable 3 Data about each piece in wargaming\nTime\nNumber of\ntarget\nPosting\nElevation\nType\nDistance\nRange\nTaking Fire\nControl point's distance\nThickness of armor\nt1\n1\n(15, 40)\n110\nTank\n34\n16\n5\n18\nUnarmored\n2\n(16, 41)\n110\nTank\n34\n20\n2\n19\nUnarmored\n3\n(17, 41)\n110\nTank\n34\n17\n3\n20\nLight armor\n4\n(18, 40)\n110\nTank\n33\n17\n5\n19\nMedium armor\n5\n(18, 41)\n110\nTank\n34\n20\n3\n20\nComposite armor\n6\n(17, 40)\n110\nTank\n33\n20\n4\n19\nUnarmored\n7\n(16, 40)\n110\nTank\n33\n20\n5\n18\nUnarmored\n8\n(15, 41)\n110\nTank\n35\n16\n4\n19\nLight armor\n9\n(18, 42)\n110\nTank\n35\n16\n2\n21\nComposite armor\n10\n(17, 42)\n110\nTank\n35\n17\n3\n21\nHeavy armor\nt2\n1\n(15, 39)\n110\nTank\n32\n16\n5\n17\nUnarmored\n2\n(16, 40)\n110\nTank\n32\n20\n2\n18\nUnarmored\n3\n(17, 41)\n110\nTank\n33\n17\n3\n20\nLight armor\n4\n(18, 40)\n110\nTank\n32\n17\n5\n19\nMedium armor\n5\n(18, 40)\n110\nTank\n32\n20\n3\n19\nComposite armor\n6\n(17, 40)\n110\nTank\n32\n20\n4\n19\nUnarmored\n7\n(16, 39)\n110\nTank\n31\n20\n5\n17\nUnarmored\n8\n(15, 41)\n110\nTank\n34\n16\n4\n19\nLight armor\n9\n(18, 41)\n110\nTank\n33\n16\n2\n20\nComposite armor\n10\n(17, 41)\n110\nTank\n33\n17\n3\n20\nHeavy armor\nt3\n1\n(15, 39)\n110\nTank\n31\n16\n5\n17\nUnarmored\n2\n(16, 40)\n110\nTank\n31\n20\n2\n18\nUnarmored\n3\n(17, 41)\n110\nTank\n32\n17\n3\n20\nLight armor\n4\n(18, 40)\n110\nTank\n31\n17\n5\n19\nMedium armor\n5\n(18, 40)\n110\nTank\n31\n20\n3\n19\nComposite armor\n6\n(17, 40)\n110\nTank\n31\n20\n4\n19\nUnarmored\n7\n(16, 39)\n110\nTank\n30\n20\n5\n17\nUnarmored\n8\n(15, 41)\n110\nTank\n33\n16\n4\n19\nLight armor\n9\n(18, 40)\n110\nTank\n31\n16\n2\n19\nComposite armor\n10\n(17, 41)\n110\nTank\n32\n17\n3\n20\nHeavy armor\nt4\n1\n(15, 39)\n110\nTank\n31\n16\n5\n17\nUnarmored\n2\n(16, 39)\n110\nTank\n30\n20\n2\n17\nUnarmored\n3\n(17, 41)\n110\nTank\n33\n17\n3\n20\nLight armor\n4\n(18, 39)\n110\nTank\n31\n17\n5\n18\nMedium armor\n5\n(18, 40)\n110\nTank\n32\n20\n3\n19\nComposite armor\n6\n(17, 39)\n110\nTank\n31\n20\n4\n18\nUnarmored\n7\n(16, 39)\n110\nTank\n30\n20\n5\n17\nUnarmored\n8\n(15, 41)\n110\nTank\n33\n16\n4\n19\nLight armor\n9\n(18, 40)\n110\nTank\n32\n16\n2\n19\nComposite armor\n10\n(17, 41)\n110\nTank\n33\n17\n3\n20\nHeavy armor\nt5\n1\n(15, 39)\n110\nTank\n31\n16\n5\n17\nUnarmored\n2\n(16, 38)\n110\nTank\n30\n20\n2\n16\nUnarmored\n3\n(17, 40)\n110\nTank\n33\n17\n3\n19\nLight armor\n4\n(18, 38)\n110\nTank\n31\n17\n5\n17\nMedium armor\n5\n(18, 40)\n110\nTank\n33\n20\n3\n19\nComposite armor\n6\n(17, 39)\n110\nTank\n32\n20\n4\n18\nUnarmored\n7\n(16, 39)\n110\nTank\n31\n20\n5\n17\nUnarmored\n8\n(15, 41)\n110\nTank\n33\n16\n4\n19\nLight armor\n9\n(18, 40)\n110\nTank\n33\n16\n2\n19\nComposite armor\n10\n(17, 41)\n110\nTank\n34\n17\n3\n20\nHeavy armor\nBy integrating real numbers and interval numbers to form intuitionistic fuzzy number\nformulas, the target distance is quantified according to the distance threat quantification method in\nSection 3.1, and the distance threat degree is then turned into a fuzzy number using the conversion\nmethod of real numbers and fuzzy numbers in Section 4.The calculated result for the target\ndistance threat degree is provided in the table 4. Consider the threat measurement at T1 as an\nexample of a decision matrix.\nTable 4\nCalculation of target distance threat\nTar-\nget\nValue of normal\nterrain stamina\nconsumption\nValue of special\nterrain energy\nconsumption\nDistance between\nthe blue tank and the\ncontrol point\nConsider the\nmaximum distance\nbetween two borders\nDistance\nbetween\ntanks i and j\nRepresentati\non of real\nnumbers\nRepresentation\nof fuzzy numbers\nintuitively\n1\n3\n6\n18\n50\n34\n8.16\n[0.187378998,\n0.012621002]\n2\n3\n6\n19\n50\n34\n8.155\n[0.18749387,\n0.01250613]\n3\n3\n6\n20\n50\n34\n8.15\n[0.187608882,\n0.012391118]\n4\n3\n6\n19\n50\n33\n8.655\n[0.176663586,\n0.023336414]\n5\n3\n6\n20\n50\n34\n8.15\n[0.187608882,\n0.012391118]\n6\n3\n6\n19\n50\n33\n8.655\n[0.176663586,\n0.023336414]\n7\n3\n6\n18\n50\n33\n8.66\n[0.176561598,\n0.023438402]\n8\n3\n6\n19\n50\n35\n7.655\n[0.199738767,\n0.000261233]\n9\n3\n6\n21\n50\n35\n7.645\n[0.2, 0.0]\n10\n3\n6\n21\n50\n35\n7.645\n[0.2, 0.0]\nBased on the real number conversion intuitionistic fuzzy number formula, the target speed is\nquantified according to the 3.2 section speed threat quantification method, and the speed threat\ndegree is converted into an intuitionistic fuzzy number according to the 4th section of the\nconversion method of real numbers and intuitionistic fuzzy numbers. Calculation of target velocity\nthreat degree. Create a matrix showing the target velocity threat measurement at T1, for example,\nas shown in the table 5.\nTable 5 Calculation of target speed threat\nTarg\net\nThe target\nspeed\nThe relative speed between the opposite\nand the us\nThe real number\nrepresentation\nIntuitive fuzzy number\nrepresentation\n1\n125\n325\n2.6\n[0.153863899, 0.046136101]\n2\n150\n350\n2.333333333\n[0.171440811, 0.028559189]\n3\n200\n400\n2\n[0.2, 0.0]\n4\n200\n400\n2\n[0.2, 0.0]\n5\n200\n400\n2\n[0.2, 0.0]\n6\n200\n400\n2\n[0.2, 0.0]\n7\n200\n400\n2\n[0.2, 0.0]\n8\n150\n350\n2.333333333\n[0.171440811, 0.028559189]\n9\n175\n375\n2.142857143\n[0.186672886, 0.013327114]\n10\n150\n350\n2.333333333\n[0.171440811, 0.028559189]\nThe real number conversion intuitionistic fuzzy number formula is applied to determine the\ntarget's attack ability using the attack threat quantification method in Section 3.3, and then the\nattack threat degree is converted into an intuitionistic fuzzy number using the conversion method\nof real numbers and intuitionistic fuzzy numbers in Section 4. The calculated solution result of the\ntarget attack threat. Determine the decision matrix using the target attack threat measurement at T1\nas an example, as shown in the table 6.\nTable 6 Calculation of target attack threat\nT\nar\nge\nt\nMano\neuvra\nbility\nWeapon\nsystem attack\ncapability\nReconnaissan\nce capability\nCapability\nof indirect\nshots\nThe\nammunition\ncarrying\ncapacity\nECM\ncapab\nility\nOffensive\nmissile\ncapability\nThe real\nnumber\nrepresentati\non\nIntuitive\nfuzzy number\nrepresentation\n1\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n2\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n3\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n4\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n5\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n6\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n7\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n8\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n9\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\n10\n6\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n{'type1': 1,\n'type2': 0.5,\n'type3': 1.5}\n1\n3\n1\n1\n10.8730228\n[0.2, 0.0]\nBy combining the interval number conversion intuitionistic fuzzy number formula with the\nterrain visibility threat quantification method in section 3.4, the target visibility is quantified, and\nthen the terrain visibility threat degree is converted into intuitionistic fuzzy numbers. Solution\nresult of the visibility threat degree of the target terrain. Create a decision matrix using the metric\nof the visibility threat of T1 as an example, as shown in the table 7.\nTable 7 Calculated threat level for terrain visibility\nTar\nget\nThe red square\nelevation\nThe blue square\nelevation\nThe highest altitude between\nred and blue\nThe interval number\nrepresentation\nIntuitive fuzzy number\nrepresentation\n1\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n2\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n3\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n4\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n5\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n6\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n7\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n8\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n9\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\n10\n130\n110\n150\n[0, 0.2]\n[0.0, 0.0]\nAs described in section 3.5, environmental indicators are quantified and converted into\nintuitionistic fuzzy numbers using the real number conversion intuitionistic fuzzy number formula,\nthen converted into real numbers using the conversion method of real numbers and intuitionistic\nfuzzy numbers in section 4. The calculation result of the target environment index. Construct a\ndecision matrix using the environmental indicator threat measurement at T1 as an example, as\nshown in the table 8.\nTable 8 Calculation of the target environment index\nTar\nget\nThe first class road\n(weight W1)\nThe secondary road\n(weight W2)\nThe urban residential\narea (weight W3)\nThe real number\nrepresentation\nIntuitive fuzzy number\nrepresentation\n1\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n2\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n3\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n4\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n5\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n6\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n7\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n8\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\n9\n1(3)\n0(2)\n0(1)\n3\n[6.6644e-05,\n0.199933356]\n10\n0(3)\n0(2)\n0(1)\n0\n[0.2, 0.0]\nUsing the real number conversion intuitionistic fuzzy number formula, the target defense is\nquantified following the target defense quantification method in Section 3.6, and then the target\ndefense quantified value is converted into intuitionistic fuzzy numbers following the conversion\nmethod of real numbers and intuitionistic fuzzy numbers in Section 4. Calculate the quantitative\nresults of target defense in the table. Build a decision matrix by using the quantification of defense\nindicators at T1 as an example, as shown in the table 9.\nTable 9 Calculation of the target defense quantitatively\nTarget\nArmored type\nThe real number representation\nIntuitive fuzzy number representation\n1\nUnarmored\n0\n[0.2, 0.0]\n2\nUnarmored\n0\n[0.2, 0.0]\n3\nLight armor\n0.3\n[0.000664452, 0.199335548]\n4\nMedium armor\n0.5\n[0.000399202, 0.199600798]\n5\nComposite armor\n1\n[0.0001998, 0.1998002]\n6\nUnarmored\n0\n[0.2, 0.0]\n7\nUnarmored\n0\n[0.2, 0.0]\n8\nLight armor\n0.3\n[0.000664452, 0.199335548]\n9\nComposite armor\n1\n[0.0001998, 0.1998002]\n10\nHeavy armor\n0.7\n[0.000285307, 0.199714693]\nA unified intuitiveistic fuzzy number representation has been created for all multi-attribute\nindicators. An example of an intuitionistic fuzzy number representation of threat assessment\nindicators is illustrated in the following table 10.\nTable 10 Information decision table for threat target parameters (intuitionistic fuzzy number)\nTank1\nTank2\nTank3\nTank4\nTank5\nTank6\nTank7\nTank8\nTank9\nTank10\nQuantification\nof target\ndistance\nthreats\n[0.1873\n78998,\n0.01262\n1002]\n[0.1874\n9387,\n0.01250\n613]\n[0.1876\n08882,\n0.01239\n1118]\n[0.1766\n63586,\n0.02333\n6414]\n[0.1876\n08882,\n0.01239\n1118]\n[0.1766\n63586,\n0.02333\n6414]\n[0.1765\n61598,\n0.02343\n8402]\n[0.1997\n38767,\n0.00026\n1233]\n[0.2,\n0.0]\n[0.2,\n0.0]\nQuantification\nof target speed\nthreats\n[0.1538\n63899,\n0.04613\n6101]\n[0.1714\n40811,\n0.02855\n9189]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.1714\n40811,\n0.02855\n9189]\n[0.1866\n72886,\n0.01332\n7114]\n[0.1714\n40811,\n0.02855\n9189]\nQuantifying\nthe threat from\ntarget attacks\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\nQuantifying\nthe threat\nposed by\nterrain\nvisibility\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\n[0.0,\n0.0]\nQuantification\nof\nenvironmental\nindicators of\nthreat\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[6.6644\ne-05,\n0.19993\n3356]\n[0.2,\n0.0]\nQuantification\nof target\ndefense\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.0006\n64452,\n0.19933\n5548]\n[0.0003\n99202,\n0.19960\n0798]\n[0.0001\n998,\n0.19980\n02]\n[0.2,\n0.0]\n[0.2,\n0.0]\n[0.0006\n64452,\n0.19933\n5548]\n[0.0001\n998,\n0.19980\n02]\n[0.0002\n85307,\n0.19971\n4693]\nBy obtaining data represented by the intuitionistic vagueness of the threat assessment\nindicators shown in the table, formulae in 4.4 may be used to obtain the intuitionistic vague target\nthreat assessment based on multiple attribute decision-making approaches. As shown in the table\n11, determine the target threat assessment results.\nTable 11 Threat assessment for target\niS\n[0.9900131572106283, 0.9930194457658972, 0.9713249517102417,\n0.9694274902547305, 0.9712630240082707, 0.9960298049584839,\n0.9960124538670997, 0.9685356920167532, 0.9447732710194203,\n0.9685296037271114]\niS\n[0.9451975215527424, 0.9421912329974735, 0.963885727053129,\n0.9657831885086402, 0.9639476547551001, 0.9391808738048868,\n0.9391982248962711, 0.9666749867466174, 0.9904374077439504,\n0.9666810750362593]\niP\n[0.5115790069137391, 0.5131324752716081, 0.5019220710020746,\n0.5009415775207532, 0.5018900705058751, 0.5146880470889931,\n0.5146790810929003, 0.5004807500523212, 0.4882017660336315,\n0.500477603991942]\nRa\nnk\nin\ng\nT6>T7>T2>T1>T3>T5>T4>T8>T10>T9\nIn the table 12, the opposite target at T1 is shown as a threat.\nTable 12 Ranking of opposite targets at time T\nType of piece\nIndicator\ncomprehensive\nRanking\nTank 1\n0.511579007\n4\nTank 2\n0.513132475\n3\nTank 3\n0.501922071\n5\nTank 4\n0.500941578\n7\nTank 5\n0.501890071\n6\nTank 6\n0.514688047\n1\nTank 7\n0.514679081\n2\nTank 8\n0.50048075\n8\nTank 9\n0.488201766\n10\nTank 10\n0.500477604\n9\nFigure 3\nThe threat value on the ordinate, and the threat of the opponent's ten tanks at time\nT represented by ten colors on the abscissa.\nBased on the evaluation results, it can be concluded that the blue T6 tank is the most harmful\nand the T7 tank is the second most harmful, this is shown in figure 3. This article does not limit\nitself to subjective analysis of experts, but also introduces reinforcement learning, associates the\nreinforcement learning intelligent algorithm through the reward function and analyzes the\nscientific nature of the method through an improvement of the actual intelligent AI algorithm's\nwinning rate.\n6、Reinforcement learning and multi-attribute threat fusion model\n6.1 Reinforcement learning algorithm and multi-attribute model formulation\nPrior work in this article has described the quantied value of multi-attribute decision-making\nthreat based on the entropy weight method. The subsequent chapters deal with how to associate\nthis value with reinforcement learning. This part of the work is also the goal of this article. Its\nessence is to establish a multi-attribute decision-making mechanism that is based on reinforcement\nlearning, and then select the entity with the highest threat to establish the return value and threat\ndegree. The higher the threat degree, the greater the return value, this is shown in figure 4.\nFigure 4\nA fusion model of reinforcement learning and multi-attribute threat estimation based on\nAC framework.The module mainly consists of a reinforcement learning pre-training experience\nstorage module that integrates multi-attribute decision-making, Critic evaluation network update\nmodule, and a new and old strategy network module\nA reinforcement learning algorithm is built using the AC framework to achieve intelligent\ndecision-making. It includes a reinforcement learning pre-training module that integrates\nmulti-attribute decision-making, Critic evaluation network update module and a new and old\nstrategy network update module.In the intensive learning pre-training experience storage module,\nmulti-attribute decision making mainly uses state data obtained from the wargame environment,\nsuch as elevation, distance, armor thickness, etc., to make multi-attribute decisions. By\nnormalizing the data, calculating the threat of each piece of the opponent by using the entropy\nmethod, and then setting the reward function and storing it in the experience store, further actions\nin the environment will be taken to obtain the next state and action rewards. The Critic network\ncalculates the value from the reward value determined during the last step of the action. combines\nthe experience store data with the value calculated by the Critic network, slashes it from the\nreward value determined during the last action, then returns to update the Critic network\nparameters.\nAs the advantage value guides the calculation of the actor network value, the\nnetwork outputs the action value according to the old and new networks, and the distribution\nprobability overall, and selects and outputs the action from the network. As a result, the advantage\nvalue is corrected, the actor loss is calculated, and the actor network is updated in the reverse\ndirection.\n6.2 Setting reward function value\nAs a core challenge of deep reinforcement learning in solving practical tasks, the sparse\nreward problem relates to the fact that the training environment cannot supervise the updating of\nagent parameters in the process of reinforcement learning [25]. When supervised learning is used,\nthe training process is supervised by humans, while in reinforcement learning, rewards are used to\nsupervise the training process, and the agent optimizes strategies based on rewards [19]. In the\nexperimental environment in this article, the pawn will only transmit a victory message if it\nreaches the control point or annihilates the opposite pawn, or if it reaches the control point or if we\nare all annihilated. Both cases have no rewards at every step of the training process, that is, the\nsparse reward problem can affect algorithm convergence. So, this article tries to use multi-attribute\ndecision-making to judge the eopposite's threat at every step, and then set the reward function\naccording to the threat to get a higher reward value by hitting the threatening pieces.Furthermore,\nan additional reward mechanism is introduced. The reward value is determined according to the\nstate of the chess piece and the distance from the control point. Additionally, in order to prevent\nthe agent from falling into an optimal local situation, one reward is added every time the agent\nwins. A table of specific additional rewards is shown in Table 13.\nTable 13\nAdditional award\nSituation\nReward\nThe state is now closer to the control point than\nthe previous state\nReward+0.5\nThis state is nearly as far from the control point\nas the previous state\nReward-0.3\nThe map boundary has been reached\nReward-1\nConsumption per step (to avoid falling into\nlocal optimum)\nReward-0.005\nThe opposite piece was hit\nReward+（5*Risk of being hit by a piece）\nHit by an opposite round\nReward-（5*Risk of being hit by a piece）\nAn opposite piece is annihilated\nReward+10\nTaking out one of the opposite's pieces will lead\nto victory\nReward+20\nDefeat an opposite piece leading to failure\n(other opposite pieces reach the control point)\nReward-10\nGet to the control point\nReward+10\nopposite wins\nReward-10\nWhen the above additional rewards are added to the training process, the convergence speed\ncan be significantly accelerated, and the likelihood that the agent falls into the local optimum is\nsignificantly reduced.\n7、An experiment simulation of wargames\n7.1 Experiment environment introduction\nIt is the objective of this article to conduct experimental verification in the self-developed\nwargame environment. Figure 5 shows the situation that exists during the initial red and blue\nconfrontation deployment. There are two tank pawns on each side, and the center is the point of\ncontention.In a confrontation, both sides compete for control points, and the party that reaches the\nmiddle red flag first wins.At the same time, both red and blue parties can shoot at each other,\nwhile they can hide in urban residential areas. By concealing, it is difficult for our opponents to\nfind our targets. Each hexagon has its own number and elevation. The higher the elevation, the\ndarker the hexagon. On the highway, the tanks move faster than on the secondary roads. The red\nstraight line represents the secondary road and the black straight line represents the primary\nroad.As the cross symbol represents aiming and shooting, the destroyed target disappears from the\nmap.\nFigure 5\nGaming environment display. The red and blue pawns fight separately, the red flag in\nthe middle is the control point, and the first player to reach the control point wins.When all the\nchess pieces on one side are destroyed, the other side wins.\n7.2 Results and analysis of the experiment\nIn this article, the PPO algorithm and the PPO algorithm combined with multi-attribute\ndecision-making are used to compare and analyze the winning rate. MADM-PPO and PPO are\ntrained for 24 hours, and this article uses the MADM-PPO algorithm as the red side and the\nrule-based blue side algorithm to fight. At the same time, the second round uses the PPO algorithm\nas the red side, and the blue side fights according to rules. Next, this article observes the winning\npercentage of both algorithms in 100 games. Experiments have shown that the agents using the\nPPO reinforcement learning algorithm combined with the multi-attribute decision-making method\nperformed better than the agents using the PPO algorithm based on the threat of the opponent.As\ncan be seen in the figure 6 and figure 7, the threat-based multi-attribute decision-making method\ndesigned in this paper, combined with PPO algorithm of reinforcement learning, proves to\neffectively improve the effectiveness of intelligent wargame decision-making. A winning\npercentage chart is presented in the table 14, table 15.\n(a)\n(b)\nFigure 6. (a) Win rate: the red side is the AI of MADM-PPO intelligent algorithm and\nthe blue side is rule-based AI; (b) Win times: the red side is the AI of MADM-PPO\nintelligent algorithm and the blue side is rule-based AI; The winning rate and the\nnumber of wins for the red and blue sides. The first round wins so one side starts from 1\nand the other from 0.\nTable 14. Comparison of the number of winning matches between red and blue teams\nafter swapping positions.\nAlgorithm\nVictory Number\nRounds\nMADM-PPO\n78\n100\nRule\n22\n100\n(a)\n(b)\nFigure 7. (a) Win rate: the red side is the AI of PPO intelligent algorithm and the blue\nside is rule-based AI; (b) Win times: the red side is the AI of PPO intelligent algorithm\nand the blue side is rule-based AI; The winning rate and the number of wins for the red\nand blue sides. The first round wins so one side starts from 1 and the other from 0.\nTable 15. Comparison of winning matches between red and blue.\nAlgorithm\nVictory Number\nRounds\nPPO\n62\n100\nRule\n38\n100\nThe experimental results show that the MADM-PPO model can reduce the number of times\nto explore during training, and improve the problem that the PPO algorithm takes too long to train.\nIt shows that the introduction of prior knowledge improves the performance of the PPO algorithm,\nand has a certain theoretical significance for improving the efficiency of the algorithm, the detail\nscore is shown in Figure 8.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 8. (a) The get goal score of both sides (Red: PPO); (b) the kill score of both\nsides (Red: PPO); (c) the survive score of both sides(Red: PPO); (d) the get goal score\nof both sides (Red: MADM-PPO); (e) the kill score of both sides (Red: MADM-PPO);\n(f) the survive score of both sides (Red: MADM-PPO). The x-axis is the training\nepisodes, and the y-axis is the score. Red and blue represent two teams in the\nconfrontation environment.\n8、Conclusion\nUsing the wargaming environment, the paper proposes an entity that combines multi-attribute\ndecision-making and reinforcement learning to solve the problem that the reinforcement learning\nalgorithm cannot quickly converge in war game training and the agent has a low winning rate\nagainst the algorithm.As part of this study, this paper conducts experiments on the multi-attribute\ndecision-making and reinforcement learning algorithms in a wargame simulation environment,\nand obtains red and blue confrontation data from the wargame environment. Calculate the weight\nof each attribute based on the intuitionistic fuzzy number weight calculations. Then determine the\nthreat posed by each opponent's chess pieces. On the basis of the degree of threat, the red side\nreinforcement learning reward function is constructed and the AC framework is trained with the\nreward function, and the algorithm combines multi-attribute decision-making with reinforcement\nlearning. A study demonstrated that the algorithm can gradually increase the reward value of the\nagent when exploring an environment over a short training period, while the final victory rate of\nthe agent against specific rules and strategies reached 78%, which is significantly higher than that\nof a pure reinforcement learning algorithm, which is 62%. Solved the convergence difficulties of\nthe state-space wargame's sparse rewards caused by the randomization of an agent's neural\nnetwork. For the algorithm design of intelligent wargaming, this is the first attempt in this field to\ncombine the multi-attribute decision-making method in management with the reinforcement\nlearning algorithm in cybernetics. An interdisciplinary approach to cross-innovation in academia\ncould lead to improvements in the design of intelligent wargames and even improvements in\nreinforcement learning algorithms.\nReference\n[1] Pang Z J, Liu R Z, Meng Z Y, et al. On reinforcement learning for full-length game of starcraft[C]//Proceedings of the AAAI\nConference on Artificial Intelligence. 2019, 33(01): 4691-4698.\n[2] Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J]. nature,\n2016, 529(7587): 484-489.\n[3] Ye D, Liu Z, Sun M, et al. Mastering complex control in moba games with deep reinforcement learning[C]//Proceedings of\nthe AAAI Conference on Artificial Intelligence. 2020, 34(04): 6672-6679.\n[4] Silver D, Schrittwieser J, Simonyan K, et al. Mastering the game of go without human knowledge[J]. nature, 2017,\n550(7676): 354-359.\n[5] Barriga N A, Stanescu M, Besoain F, et al. Improving rts game ai by supervised policy learning, tactical search, and deep\nreinforcement learning[J]. IEEE Computational Intelligence Magazine, 2019, 14(3): 8-18.\n[6] Schrittwieser J, Antonoglou I, Hubert T, et al. Mastering atari, go, chess and shogi by planning with a learned model[J].\nNature, 2020, 588(7839): 604-609.\n[7]\nBarriga\nN,\nStanescu\nM,\nBuro\nM.\nCombining\nstrategic\nlearning\nwith\ntactical\nsearch\nin\nreal-time\nstrategy\ngames[C]//Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. 2017, 13(1).\n[8] Kong depeng, Chang Tianqing, Hao Na, Zhang Lei, Guo Libin. Multi attribute index processing method of ground combat\ntarget threat assessment [J]. Acta automatica Sinica, 2021,47 (01): 161-172\n[9] Zhong S, Tan J, Dong H, et al. Modeling-Learning-Based Actor-Critic Algorithm with Gaussian Process Approximator[J].\nJournal of Grid Computing, 2020, 18(2): 181-195.\n[10] Littman M L. Reinforcement learning improves behaviour from evaluative feedback[J]. Nature, 2015, 521(7553): 445-451.\n[11] Littman M L. Markov games as a framework for multi-agent reinforcement learning[M]//Machine learning proceedings\n1994. Morgan Kaufmann, 1994: 157-163.\n[12] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018.\n[13] Hussain A, Chun J, Khan M. A novel multicriteria decision making (MCDM) approach for precise decision making under a\nfuzzy environment[J]. Soft Computing, 2021, 25(7): 5645-5661.\n[14] Opricovic S, Tzeng G H. Compromise solution by MCDM methods: A comparative analysis of VIKOR and TOPSIS[J].\nEuropean journal of operational research, 2004, 156(2): 445-455.\n[15] Kou G, Lu Y, Peng Y, et al. Evaluation of classification algorithms using MCDM and rank correlation[J]. International\nJournal of Information Technology & Decision Making, 2012, 11(01): 197-225.\n[16] Von Winterfeldt D, Fischer G W. Multi-attribute utility theory: models and assessment procedures[J]. Utility, probability,\nand human decision making, 1975: 47-85.\n[17] Tzeng G H, Huang J J. Multiple attribute decision making: methods and applications[M]. CRC press, 2011.\n[18] Zhu Y, Tian D, Yan F. Effectiveness of entropy weight method in decision-making[J]. Mathematical Problems in\nEngineering, 2020, 2020.\n[19] Zhang Zhen, Huang Yanyan, Zhang Yongliang, Chen Tiande. Game confrontation algorithm of combat entity based on\nnear end strategy optimization [J]. Journal of Nanjing University of technology, 2021,45 (01): 77-83\n[20] Li Chen, Huang Yanyan, Zhang Yongliang, Chen Tiande. Multi agent decision making method based on actor critical\nframework and its application in wargame [J]. Systems engineering and electronic technology, 2021,43 (03): 755-762\n[21] Liu man, Zhang Hongjun, Hao Wenning, Cheng Kai, Wang Jiayin. Intelligent decision making method of tactical wargame\nentity combat action [J]. Control and decision, 2020,35 (12): 2977-2985\n[22] Dorton S L, Maryeski L A R, Ogren L, et al. A wargame-augmented knowledge elicitation method for the agile\ndevelopment of novel systems[J]. Systems, 2020, 8(3): 27.\n[23] Zhang J, Xue Q, Chen Q, et al. Intelligent Battlefield Situation Comprehension Method Based On Deep Learning in\nWargame[C]//2019 IEEE 1st International Conference on Civil Aviation Safety and Information Technology (ICCASIT). IEEE,\n2019: 363-368.\n[24] Vlachos I K, Sergiadis G D. Intuitionistic fuzzy information–applications to pattern recognition[J]. Pattern Recognition\nLetters, 2007, 28(2): 197-206.\n[25] Kaelbling L P, Littman M L, Moore A W. Reinforcement learning: A survey[J]. Journal of artificial intelligence research,\n1996, 4: 237-285.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2021-09-06",
  "updated": "2021-09-06"
}