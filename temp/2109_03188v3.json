{
  "id": "http://arxiv.org/abs/2109.03188v3",
  "title": "Optimizing Quantum Variational Circuits with Deep Reinforcement Learning",
  "authors": [
    "Owen Lockwood"
  ],
  "abstract": "Quantum Machine Learning (QML) is considered to be one of the most promising\napplications of near term quantum devices. However, the optimization of quantum\nmachine learning models presents numerous challenges arising from the\nimperfections of hardware and the fundamental obstacles in navigating an\nexponentially scaling Hilbert space. In this work, we evaluate the potential of\ncontemporary methods in deep reinforcement learning to augment gradient based\noptimization routines in quantum variational circuits. We find that\nreinforcement learning augmented optimizers consistently outperform gradient\ndescent in noisy environments. All code and pretrained weights are available to\nreplicate the results or deploy the models at:\nhttps://github.com/lockwo/rl_qvc_opt.",
  "text": "Optimizing Quantum Variational Circuits with\nDeep Reinforcement Learning\nOwen Lockwood\nDepartment of Computer Science\nRensselaer Polytechnic Institute, Troy NY, USA\nE-mail: lockwo@rpi.edu\nApril 2022\nAbstract.\nQuantum Machine Learning (QML) is considered to be one of the most\npromising applications of near term quantum devices.\nHowever, the optimization\nof quantum machine learning models presents numerous challenges arising from\nthe imperfections of hardware and the fundamental obstacles in navigating an\nexponentially scaling Hilbert space.\nIn this work, we evaluate the potential of\ncontemporary methods in deep reinforcement learning to augment gradient based\noptimization routines in quantum variational circuits.\nWe ﬁnd that reinforcement\nlearning augmented optimizers consistently outperform gradient descent in noisy\nenvironments. All code and pretrained weights are available to replicate the results or\ndeploy the models at: github.com/lockwo/rl qvc opt .\n1. Introduction\nIn the last decade the spectre of quantum computing has begun to materialize. Recently,\nthere have been a number of claims of hardware that represents some form of a quantum\nadvantage [1, 2, 3, 4]. While it is still debated whether “Quantum Supremacy” has\nbeen reached [5, 6, 7], there is no doubt that these advances represent substantial\nimprovements in quantum computing hardware.\nThere are a number of challenges\nwhen working with these devices, such as noise [8], decoherence [9] and even cosmic rays\n[10]. In addition to these common problems, the supposed “killer app” of near term\nquantum hardware, quantum machine learning, faces additional challenges [11]. Parallel\nto the developments of quantum computing in the past decade, deep reinforcement\nlearning (RL) has achieved a number of impressive results. From reaching superhuman\nperformance in games such as Chess [12], Poker [13] and Dota 2 [14], to robotic control\n[15] and chip design [16].\nQuantum Machine Learning (QML) seeks to apply the potential advantages of\nquantum computing for machine learning problems.\nQuantum machine learning\npresents a number of signiﬁcant (both polynomial and exponential) theoretical speedups\n[17, 18, 19]. QML algorithms have been developed for supervised learning [20, 21, 22],\narXiv:2109.03188v3  [cs.LG]  14 May 2022\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n2\nunsupervised learning [23, 24, 25], and reinforcement learning [26, 27, 28, 29]. A shared\nproblem among many of these techniques is the optimization routine. Gradient [30]\nand gradient-free [31] optimization techniques decrease in eﬃcacy exponentially as the\nnumber of qubits grows. Additionally, independent of these phenomena, the presence of\nnoise also induces exponential diﬃculties optimization [32]. While there has been some\nwork in mitigating the eﬀects of barren plateaus [33, 34, 35], it remains a pervasive\nproblem in optimizing QML models.\nIn this work, we propose a reinforcement learning based approach to the problems of\noptimizing QML systems. Speciﬁcally, we train a deep reinforcement learning agent to\nminimize the loss of random quantum variational circuits of random sizes with random\nobjectives.\nThere have been several previous applications of reinforcement learning\nto aid with some of the challenges of optimizing QML systems [36, 37, 38, 39, 40, 41].\nHowever, many of these works are limited in their applicable problem space, e.g. to only\ncombinatorial/QAOA [42] routines or only to certain ansatzs/circuit structures. In this\nwork, we shift towards a more general setup. Speciﬁcally, we create an ansatz, depth,\nqubit number, and cost function agnostic training routine (speciﬁed up to a maximum in\neach of these categories). We ﬁnd that this optimizer can be used to eﬀectively augment\ngradient based routines in noisy circuit simulations, increasing the performance across\na variety of tasks without increasing the complexity of the circuit sampling.\n2. Background\n2.1. Reinforcement Learning\nDeep Reinforcement learning (RL) is one of the three main branches of contemporary\ndeep learning. The goal of RL is to have an agent learn to interact with an environment\nso as to maximize a reward signal [43]. The framework of RL is often formalized as\na Markov Decision Process (MDP) with states S, actions A, and rewards R.\nThe\nobjective of this RL optimization problem is J(π) = maxπ\nPH\nt=0\n\u0002\nE(st,at)∼πr(st, at)\n\u0003\nwith\nhorizon (the number of timesteps in the environment) H, state at time t st, action at\ntime t at, and reward function r. In other words, the goal is to ﬁnd the policy, π,\nwhich maximizes the expected return. In this work, we employ entropy maximizing\nRL algorithms, a state of the art class of algorithms that have shown to be especially\nrobust [44]. Experiments were conducted with other SotA model free algorithm such\nas Proximal Policy Optimization (PPO) [45] and Twin Delayed Deep Deterministic\nPolicy Gradient (TD3) [46], however, we found them to be consistently outperformed by\nentropy based methods. These entropy maximizing algorithms have a slightly modiﬁed\nobjective function, J(π) = maxπ\nPH\nt=0\n\u0002\nE(st,at)∼πr(st, at) + αH(π(·|st))\n\u0003\n[47], i.e. these\nalgorithms seek to maximize the expected reward and the expected entropy of the policy.\nAt α = 0, this is the same as the previous objective. Note that for continuous functions,\nH(π(·|st)) = −\nR\nA π(a|st)logπ(a|st)da.\nIn order to maximize this objective, we utilize Soft-Actor Critic (SAC) [48]. SAC\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n3\nis a model free, oﬀ-policy, actor critic algorithm. The algorithm is composed of ﬁve\ntotal neural networks, one policy network, two Q networks and two target Q networks.\nThe Q networks use neural networks to approximate the Q function [49], which is an\nestimation of the expected reward given a state action pair. In the case of maximum\nentropy RL this takes the form of Qθ(st, at) = Eθ\n\u0002\nrt + γ log\nR\nA eQ(st+1,a)da\n\u0003\n. The Q\nfunctions are updated via the Soft Mean Squared Bellman Error:\nLSMSBE(θ) = [Qθ(st, at) −(r(st, at) + γ(Qθ′(st+1, at+1)\n−α log(π(at+1|st+1))))]2\nThe target Q networks (with parameters denoted by θ′) serve to prevent premature\nnumerical overestimation of the Q value and are updated via Polyak averaging [50].\nThe policy network is updated in a similar manner to DDPG [51], via estimation of the\ngradient from the Q function: ∇φα log(πφ(st, at)) + ∇atα log(πφ(st, at)) −∇atQθ(st, at).\nAdditionally, the temperature parameter α can be automatically adjusted.\n2.2. Quantum Machine Learning\nQuantum machine learning is built upon both advancements in quantum computing\nand in machine learning. Quantum computing advantages often stem from the ability\nof quantum computers to represent and operate on information that scales exponentially\nwith the number of qubits. In this work, we focus on the Quantum Variational Circuit\n(QVC) as the machine learning model [52]. QVCs are a type of quantum circuit with\nlearnable parameters. Any number of QVC setups are possible, in this work we consider\narbitrary structure QVCs with the gate set {CNOT, H, Rx, Ry, Rz}. Note that this\nis a universal gateset, meaning any circuit can be represented via these gates. The\nPauli rotations gates, Rx(θ), Ry(θ), Rz(θ), rotate around the speciﬁed axis θ radians,\nRν(θ) = e−i θ\n2 σν, where ν = X, Y, Z. The controlled NOT (CNOT) gate is a two qubit\ngate that can induce entanglement in qubits. The aforementioned θ are the learnable\nparameters [53]. The measurement operator (from which the cost function is calculated)\nwe utilize is the Pauli ˆZ operator, or the ‘computational basis’.\nThe gradients of these quantum circuits can be calculated using the parameter\nshift rule [54]. The rotation gates, Rα(θ) = e−i θ\n2 σα, can be diﬀerentiated via\n∂\n∂θi =\nf(θi+s)−f(θi−s)\n2sin(s)\ngiven s ∈R, s ̸= kπ, k ∈Z [55] where f(θ) = ⟨0|U †(θ) ˆZU(θ)|0⟩and U(θ)\nis composed of these single qubit rotation gates. A common choice for s is π/2 [56].\nGradients can also be calculated in simulations using adjoint diﬀerentiation [57, 58]\nwhich requires no parameter perturbations (hence making it substantially faster for\nclassical simulations), but is not feasible on quantum hardware.\n3. Approach\nTo work with this QVC optimization problem, we must reconceptualize it into an\nenvironment compatible with RL agents. To this end we must consider the how states,\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n4\nactions, and rewards can be represented and numerically fed to the agent’s neural\nnetwork. Here we discuss our approach to each of these. First we have the state space\nproblem, i.e. how do we convert the information from the QVC into an useful format\nfor a RL agent to work with? Additionally, how can we eﬀectively convey information\nabout the structure, inputs, etc. that may vary? In this work we utilize two distinct\napproaches to convert the QVC information into inputs to the RL agent. Both encoding\ntechniques rely on no simulation/statevector information and are fully compatible with\nany future or existing hardware. The ﬁrst approach, which we call “feature” encoding, is\ninspired by the FLIP [59] algorithm. This encoding takes the QVC and returns a matrix\nwith dimensions [max parameters, 8], where each row contains the following information\nabout the parameterized gate: current circuit error, current parameter value, gate type,\nqubit number, qubit layer, max qubits, max depth and input type. This input is then\nﬂatten and processed by a MLP in the RL agent. The second encoding scheme we call\n“block” encoding and is inspired by [60]. Block encoding takes the QVC and returns\na 3D array with dimensions [max qubits, max depth, 5]. Similar to other RL encoding\nschemes [12], each sheet/plane in this array represents information about a certain\nfeature. The ﬁrst 3 sheets contain the parameters of the associated Pauli rotation gates\n(i.e. sheet 1 corresponds with Rx gates, and so on), the fourth represents the input\ntype and the ﬁnal layer is the current error. This matrix is then fed into a CNN for the\nagent to process. Prior to training the agent, a maximum number of parameters must be\nenforced (due to the static size of the weight matrices in the RL neural networks) but can\nbe arbitrarily large. Now we consider the action space. The output of the RL agent is\na vector that has the same length as the maximum number of parameters, representing\nthe new value for each possible parameter. Not all parameters would necessarily be\nused, in cases where the number of parameters are less than the maximum the output\nis simply clipped to match this size. Finally, we consider the reward function. Due to\nthe brittle nature of RL, constructing an eﬀective reward function can be challenging\nand important for the agent to succeed [61]. Our reward function is the negative mean\nsquared error between the QVC with the parameters provided by the RL agent and the\ntarget value (which is randomly chosen during training). This is not required to be the\nloss function used in deployment, and we conduct experiments with a variety of cross\nentropy based losses by simply feeding this loss into the agent as the current error.\nWith the basics of the RL agent established, we can detail the training setup. At the\nbeginning of a training routine, the maximum qubits, depth, and optimization timesteps\nare speciﬁed. Implicit within these is the maximum number of parameters (max qubits\ntimes max depth).\nThe values used in this work (and provided in the pretrained\nexamples) are 20, 20, and 150, respectively. This limits the number of parameters to\nbe at most 400. At the beginning of each environment iteration a circularly entangled\ncircuit is created with random Pauli rotation gates, random depth and a random number\nof qubits.\nAn input type (either ground state or equal superposition state) is then\nselected. Finally a readout cost function is chosen. The possible functions are a product\nof the Z readout values on each qubit, the Z readout on the ﬁrst qubit, and the sum of\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n5\nZ readout values on each qubit. A target value is then selected and the agent learns to\nmaximize the negative mean squared error between the target value and the output of\nQVC.\nThe agents were then trained using 2 RTX 6000 24 GB GPUs with 12 vCPUs.\nWith this, the total training time was approximately 150 hours. The evaluation time\nwas done on the same hardware and took approximately 70 hours (largely dominated by\nsome slower noisy simulations). The circuit simulation environment code was generated\nwith TensorFlow-Quantum [62] and Stable Baselines 3 [63] was used as the RL package.\nWhile the agent is trained to directly optimize the circuit, given the scope and\ncomplexity of the environment (with no restrictions on the hyperparameters of the\ncircuit), the agent is not meant to be used in this capacity. Rather, the agent is meant\nto augment traditional approaches by providing an alternative set of parameters at\neach optimization iteration, as outlined in the following algorithm. Essentially, each\noptimization step is done by taking the parameters that minimize the loss the most out\nof the two RL agents and gradient descent as shown in the algorithm below.\nAlgorithm 1: Augmented Algorithm\nwhile iteration < max iterations do\nL = L (f (θ))\nθg = θ −∇L\nθMLP = SACMLP (L)\nθCNN = SACCNN (L)\nLg = L (f (θg))\nLMLP = L (f (θMLP))\nLCNN = L (f (θCNN))\nθ = minL {θg, θMLP, θCNN}\nreturn θ\n4. Results\nWe evaluate our RL techniques on six diﬀerent problems, four classical and two quantum.\nThe four classical problems include two binary classiﬁcation problems, one multi-class\nclassiﬁcation and one regression (the Boston Housing Dataset).\nThe classiﬁcation\nproblems can be visualized in Figure 1. The two binary classiﬁcation problems utilize 2\nqubits, blobs uses 7 qubits, and the regression problem makes use of 13 qubits. The two\nquantum problems are the optimization routines of the Variational Quantum Eigensolver\n(VQE) [64] and the Quantum Approximate Optimization Algorithm (QAOA) [42]. For\nVQE and QAOA we consider 3 diﬀerent problems sizes of 5, 10, and 20 qubits. Note\nthat the other hyperparameters for VQE and QAOA are constant. The QAOA problem\nis MAX-CUT and the random graph has regularity two and p = 10. The VQE problem\ngenerates random Hamiltonians that are decomposed into 10 Pauli sums and utilizes\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n6\n5 layers of a hardware eﬃcient ansatz [65]. The results are presented in three tables,\neach experiment was repeated 3 times (with diﬀerent random initializations) and the ±\nindicates one standard deviation. Note that one cannot compare the numerical values\nacross tables as the problems are randomly generated for each table.\nEach column\nrepresents an optimization technique to the same problem. The SAC MLP and SAC\nCNN columns represent using just the speciﬁed RL agent for every optimization step\n(MLP corresponds with feature encoding and CNN with block). These results of these\noptimizers used solo can be found in Appendix A. For all experiments the gradient\ndescent optimizer is Adam [66] . In Table 1, we show the results for simulations with zero\nnoise (shot or depolarizing). The gradient descent makes use of adjoint diﬀerentiation\nto enable larger simulations. Table 2 shows the results for simulations with only shot\nnoise. This table is shorter as the larger simulations are less feasible to conduct while\nusing parameter shift diﬀerentiation techniques. The noisy simulation results are shown\nin Table 3. In addition to shot noise, these circuits are simulated with depolarizing\nnoise, modifying the density operator via ρ →(1 −p) ρ +\np\n4n−1\nP\ni PiρPi where p is\nthe probability and Pi are the Pauli gates.\nIn these experiments p = 0.075.\nThe\nresults tend to show consistent advantages for the augmented optimizer in the “real\nworld” regimes (i.e. at least noise from expectation approximation) and inconsistent\nrelative performance in completely noiseless simulations (achieving the best results in\napproximately 1/3 of the experiments).\n(a) Binary Classiﬁcation of Circles\n(b) Binary Classiﬁcation of Moons\n(c) Multi-Class Classiﬁcation of Blobs\nFigure 1: Classical Data Classiﬁcation Problems\nWe also brieﬂy present the present the potential of this augmented approach to aid\nwith the barren plateaus problem. We consider a toy example of two layers of RX, RY\nrotations on 6 qubits. Traditional gradient descent approaches are unable to succeed\n(even with 10000 shots to reduce shot noise). However, we present two approaches that\nmay help to alleviate this problem. First is the exact same algorithm as above, these\nresults can be seen in Figure 2. We are also able to achieve better results in this case\nby keeping slightly more history and instead of performing gradient descent on θt we\nalso perform gradient descent on θt−1 and simply add this into the greedy selection of\nminima for the next θ. This is able to achieve the results shown in Figure 3.\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n7\nEvaluation\nGradient Descent\nAugmented\nCircle Train Loss\n0.5834 ± 2 ∗10−6\n0.5834 ± 5 ∗10−6\nCircle Validation Loss\n0.5761 ± 0.0001\n0.57048 ± 0.0017\nMoons Train Loss\n0.354218 ± 0.000379\n0.3538 ± 3 ∗10−5\nMoons Validation Loss\n0.36714 ± 0.0042747\n0.3586 ± 0.00143\nBlobs Train Loss\n1.5677 ± 0.03122\n1.578 ± 0.01315\nBlobs Validation Loss\n1.56367 ± 0.02969\n1.5798 ± 0.0155\nRegression Train Loss\n0.0254 ± 0.00502\n0.0719 ± 0.0182\nRegression Validation\n0.0291 ± 0.00513\n0.0747 ± 0.0131\nLoss\n5 Qubit QAOA Cost\n−1.50 ± 8 ∗10−7\n−1.50 ± 9 ∗10−7\n10 Qubit QAOA Cost\n−2.9998 ± 0.000205\n−2.9997 ± 0.000262\n20 Qubit QAOA Cost\n−7.8989 ± 0.0877\n−7.667 ± 0.461\n5 Qubit VQE Cost\n−2.51396 ± 0.1517\n−2.5595 ± 0.1471\n10 Qubit VQE Cost\n−1.2362 ± 0.0109\n−1.2345 ± 0.0195\n20 Qubit VQE Cost\n−0.05 ± 0.000838\n−0.05 ± 0.000827\nTable 1: Noiseless\nEvaluation\nGradient Descent\nAugmented\nCircle Train Loss\n0.5674 ± 0.00032\n0.5676 ± 0.0003\nCircle Validation Loss\n0.6087 ± 0.001076\n0.605 ± 0.0005565\nMoons Train Loss\n0.344 ± 0.0002\n0.3439 ± 0.00042\nMoons Validation Loss\n0.3844 ± 0.000683\n0.3835 ± 0.0008\n5 Qubit QAOA Cost\n−1.561 ± 0.0061\n−1.5627 ± 0.0034\n10 Qubit QAOA Cost\n−4.8 ± 0.2382\n−4.687 ± 0.2756\n5 Qubit VQE Cost\n−3.575 ± 0.198\n−3.5626 ± 0.076\n10 Qubit VQE Cost\n−1.0922 ± 0.0813\n−1.1215 ± 0.0599\nTable 2: Only shot noise\nEvaluation\nGradient Descent\nAugmented\nCircle Train Loss\n0.6739 ± 0.000303\n0.6724 ± 0.00012\nCircle Validation Loss\n0.656 ± 0.000278\n0.6547 ± 0.00106\nMoons Train Loss\n0.644 ± 0.005\n0.6347 ± 0.00307\nMoons Validation Loss\n0.6406 ± 0.0058\n0.631 ± 0.0043\n5 Qubit QAOA Cost\n−1.52 ± 0.00998\n−1.5773 ± 0.01062\n10 Qubit QAOA Cost\n−4.805 ± 0.683\n−5.2257 ± 0.0889\n5 Qubit VQE Cost\n−0.1634 ± 0.0297\n−0.1716 ± 0.0242\n10 Qubit VQE Cost\n−1.0683 ± 0.06495\n−1.10984 ± 0.0272\nTable 3: Shot and depolarizing noise\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n8\nFigure 2: Loss vs. Iterations of 6 Qubit System with Global Cost Function\nFigure 3: Loss vs. Iterations of 6 Qubit System with Global Cost Function With Rolling\nGradients\n5. Discussion\nHere it is important to highlight and clarify a potential confusion. It may not be clear as\nto why the augmented optimizer is not upper bounded by the other optimizers and does\nnot always perform as well. This is primarily because it does not necessarily explore the\nsame region of the cost landscape. At every iteration, the augmented optimizer takes\nthe biggest step in the negative direction, hence it does not follow the same trajectories\nas the other optimizers. As the RL optimizers’ predictions are dependent on the current\nparameters, these diﬀering steps result in diﬀerent inputs to the agent and therefore\ndiﬀerent output. This is likely why the gradient descent approach out performs the\naugmented optimizer in completely noise free situation, i.e.\nthe greedy augmented\napproach leads to diﬀering local minima. Secondly, although the RL agents are trained\nas solo optimizers (without any augmentation), they seem to consistently be the worst\nperforming. This is likely because the evaluations are so diﬀerent from the training.\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n9\nGiven the variety of hyperparameters of current QVC systems (ansatz, cost function,\netc.) not only is it impossible to directly train on all combinations, but the goal is to be\nsomewhat generalizable to situations not directly in the training data. Thus, excellent\nperformance on a limited set of tasks is traded for worse performance on a larger set of\nproblems.\n5.1. Future Work\nThere are a number of potential future directions for this line of work. A clear extension\nis to expand training and validation for larger systems, up to and beyond 30 qubit\nsimulations which would require substantially more computational power.\nAnother\nimportant step would be to verify and experiment with these optimizers on actual\nquantum hardware. Additionally, this approach can be compared and combined with\nother gradient (and non-gradient) based optimizers to provide further insight into this\napproach. Finally, the potential to alleviate barren plateaus seems substantial but is\nlimited to a toy problem. Expanding on this analysis is important and potentially very\nimpactful.\n6. Conclusion\nIn this work, we presented and experimented with an approach to train and evaluate deep\nreinforcement learning to optimizer quantum variational circuits. These agents have the\npotential to take advantage of recent advancements in deep learning and reinforcement\nlearning to aid with the diﬃcult task of quantum circuit optimization.\nWe trained\n(and provided) models on large systems of up to 20 qubit, 400 parameter quantum\nvariational circuits. We analyzed these agents capabilities to augment existing gradient\nbased approaches to optimization on a variety of quantum machine learning tasks of\nvarious sizes and various levels of noise. The empirical ﬁndings suggest the augmentation\ncan help even in the absence of noise, and consistently helps in the presence of shot or\ndepolarizing noise. Our work is indicative that contemporary deep learning research can\nhelp to alleviate some challenges of working with and optimizing on current and near\nterm quantum hardware.\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n10\nReferences\n[1] Arute F, Arya K, Babbush R, Bacon D, Bardin J C, Barends R, Biswas R, Boixo S, Brandao F G,\nBuell D A et al. 2019 Nature 574 505–510\n[2] Zhong H S, Wang H, Deng Y H, Chen M C, Peng L C, Luo Y H, Qin J, Wu D, Ding X, Hu Y\net al. 2020 Science 370 1460–1463\n[3] Arrazola J, Bergholm V, Br´adler K, Bromley T, Collins M, Dhand I, Fumagalli A, Gerrits T,\nGoussev A, Helt L et al. 2021 Nature 591 54–60\n[4] Wu Y, Bao W S, Cao S, Chen F, Chen M C, Chen X, Chung T H, Deng H, Du Y, Fan D et al.\n2021 arXiv preprint arXiv:2106.14734\n[5] Pednault E, Gunnels J A, Nannicini G, Horesh L and WisnieﬀR 2019 arXiv preprint\narXiv:1910.09534\n[6] Huang C, Zhang F, Newman M, Cai J, Gao X, Tian Z, Wu J, Xu H, Yu H, Yuan B et al. 2020\narXiv preprint arXiv:2005.06787\n[7] Pan F and Zhang P 2021 arXiv preprint arXiv:2103.03074\n[8] Steane A M 1998 Fortschritte der Physik: Progress of Physics 46 443–457\n[9] Pellizzari T, Gardiner S A, Cirac J I and Zoller P 1995 Physical Review Letters 75 3788\n[10] McEwen M, Faoro L, Arya K, Dunsworth A, Huang T, Kim S, Burkett B, Fowler A, Arute F,\nBardin J C et al. 2021 arXiv preprint arXiv:2104.05219\n[11] McClean J R, Boixo S, Smelyanskiy V N, Babbush R and Neven H 2018 Nature communications\n9 1–6\n[12] Silver D, Hubert T, Schrittwieser J, Antonoglou I, Lai M, Guez A, Lanctot M, Sifre L, Kumaran\nD, Graepel T et al. 2018 Science 362 1140–1144\n[13] Brown N and Sandholm T 2019 Science 365 885–890\n[14] Berner C, Brockman G, Chan B, Cheung V, Debiak P, Dennison C, Farhi D, Fischer Q, Hashme\nS, Hesse C et al. 2019 arXiv preprint arXiv:1912.06680\n[15] Haarnoja T, Ha S, Zhou A, Tan J, Tucker G and Levine S 2018 arXiv preprint arXiv:1812.11103\n[16] Mirhoseini A, Goldie A, Yazgan M, Jiang J W, Songhori E, Wang S, Lee Y J, Johnson E, Pathak\nO, Nazi A et al. 2021 Nature 594 207–212\n[17] Biamonte J, Wittek P, Pancotti N, Rebentrost P, Wiebe N and Lloyd S 2017 Nature 549 195–202\n[18] Huang H Y, Kueng R, Torlai G, Albert V V and Preskill J 2021 arXiv preprint arXiv:2106.12627\n[19] Liu Y, Arunachalam S and Temme K 2021 Nature Physics 1–5\n[20] Schuld M and Petruccione F 2018 Supervised learning with quantum computers vol 17 (Springer)\n[21] Havl´ıˇcek V, C´orcoles A D, Temme K, Harrow A W, Kandala A, Chow J M and Gambetta J M\n2019 Nature 567 209–212\n[22] P´erez-Salinas A, Cervera-Lierta A, Gil-Fuster E and Latorre J I 2020 Quantum 4 226\n[23] Otterbach J, Manenti R, Alidoust N, Bestwick A, Block M, Bloom B, Caldwell S, Didier N, Fried\nE S, Hong S et al. 2017 arXiv preprint arXiv:1712.05771\n[24] Kerenidis I, Landman J, Luongo A and Prakash A 2018 arXiv preprint arXiv:1812.03584\n[25] Wiebe N, Kapoor A and Svore K M 2015 Quantum information and computation 15 318–358\n[26] Chen S Y C, Yang C H H, Qi J, Chen P Y, Ma X and Goan H S 2020 IEEE Access 8 141007–141024\n[27] Lockwood O and Si M 2020 Reinforcement learning with quantum variational circuit Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment vol 16\npp 245–251\n[28] Lockwood O and Si M 2021 Playing atari with hybrid quantum-classical reinforcement learning\nNeurIPS 2020 Workshop on Pre-registration in Machine Learning (Proceedings of Machine\nLearning Research vol 148) (PMLR) pp 285–301 URL http://proceedings.mlr.press/v148/\nlockwood21a.html\n[29] Jerbi S, Gyurik C, Marshall S, Briegel H J and Dunjko V 2021 arXiv preprint arXiv:2103.05577\n[30] Cerezo M, Sone A, VolkoﬀT, Cincio L and Coles P J 2020 arXiv e-prints arXiv–2001\n[31] Arrasmith A, Cerezo M, Czarnik P, Cincio L and Coles P J 2020 arXiv preprint arXiv:2011.12245\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n11\n[32] Wang S, Fontana E, Cerezo M, Sharma K, Sone A, Cincio L and Coles P J 2021 Nature\ncommunications 12 1–11\n[33] Pesah A, Cerezo M, Wang S, VolkoﬀT, Sornborger A T and Coles P J 2020 arXiv preprint\narXiv:2011.02966\n[34] Grant E, Wossnig L, Ostaszewski M and Benedetti M 2019 Quantum 3 214\n[35] Larocca M, Czarnik P, Sharma K, Muraleedharan G, Coles P J and Cerezo M 2021 arXiv preprint\narXiv:2105.14377\n[36] Sørdal V B and Bergli J 2019 arXiv preprint arXiv:1904.04712\n[37] Yao J, K¨ottering P, Gundlach H, Lin L and Bukov M 2020 arXiv preprint arXiv:2012.06701\n[38] Yao J, Bukov M and Lin L 2020 Policy gradient based quantum approximate optimization\nalgorithm Mathematical and Scientiﬁc Machine Learning (PMLR) pp 605–634\n[39] Wauters M M, Panizon E, Mbeng G B and Santoro G E 2020 Physical Review Research 2 033446\n[40] Khairy S, Shaydulin R, Cincio L, Alexeev Y and Balaprakash P 2020 Learning to optimize\nvariational quantum circuits to solve combinatorial problems Proceedings of the AAAI\nConference on Artiﬁcial Intelligence vol 34 pp 2367–2375\n[41] Baum Y, Amico M, Howell S, Hush M, Liuzzi M, Mundada P, Merkh T, Carvalho A R and Biercuk\nM J 2021 arXiv preprint arXiv:2105.01079\n[42] Farhi E, Goldstone J and Gutmann S 2014 arXiv preprint arXiv:1411.4028\n[43] Sutton R S and Barto A G 2018 Reinforcement learning: An introduction (MIT press)\n[44] Eysenbach B and Levine S 2021 arXiv preprint arXiv:2103.06257\n[45] Schulman J, Wolski F, Dhariwal P, Radford A and Klimov O 2017 arXiv preprint arXiv:1707.06347\n[46] Fujimoto S, Hoof H and Meger D 2018 Addressing function approximation error in actor-critic\nmethods International Conference on Machine Learning (PMLR) pp 1587–1596\n[47] Haarnoja T, Zhou A, Abbeel P and Levine S 2018 Soft actor-critic: Oﬀ-policy maximum entropy\ndeep reinforcement learning with a stochastic actor International conference on machine learning\n(PMLR) pp 1861–1870\n[48] Haarnoja T, Zhou A, Hartikainen K, Tucker G, Ha S, Tan J, Kumar V, Zhu H, Gupta A, Abbeel\nP et al. 2018 arXiv preprint arXiv:1812.05905\n[49] Watkins C J and Dayan P 1992 Machine learning 8 279–292\n[50] Polyak B T and Juditsky A B 1992 SIAM journal on control and optimization 30 838–855\n[51] Lillicrap T P, Hunt J J, Pritzel A, Heess N, Erez T, Tassa Y, Silver D and Wierstra D 2015 arXiv\npreprint arXiv:1509.02971\n[52] Benedetti M, Lloyd E, Sack S and Fiorentini M 2019 Quantum Science and Technology 4 043001\n[53] McClean J R, Romero J, Babbush R and Aspuru-Guzik A 2016 New Journal of Physics 18 023023\n[54] Schuld M, Bergholm V, Gogolin C, Izaac J and Killoran N 2019 Physical Review A 99 032331\n[55] Mari A, Bromley T R and Killoran N 2021 Physical Review A 103 012405\n[56] Bergholm V, Izaac J, Schuld M, Gogolin C, Alam M S, Ahmed S, Arrazola J M, Blank C, Delgado\nA, Jahangiri S et al. 2018 arXiv preprint arXiv:1811.04968\n[57] Plessix R E 2006 Geophysical Journal International 167 495–503\n[58] Luo X Z, Liu J G, Zhang P and Wang L 2020 Quantum 4 341\n[59] Sauvage F, Sim S, Kunitsa A A, Simon W A, Mauri M and Perdomo-Ortiz A 2021 arXiv preprint\narXiv:2103.08572\n[60] F¨osel T, Niu M Y, Marquardt F and Li L 2021 arXiv preprint arXiv:2103.07585\n[61] Henderson P, Islam R, Bachman P, Pineau J, Precup D and Meger D 2018 Deep reinforcement\nlearning that matters Proceedings of the AAAI conference on artiﬁcial intelligence vol 32\n[62] Broughton M, Verdon G, McCourt T, Martinez A J, Yoo J H, Isakov S V, Massey P, Halavati\nR, Niu M Y, Zlokapa A, Peters E, Lockwood O, Skolik A, Jerbi S, Dunjko V, Leib M, Streif\nM, Dollen D V, Chen H, Cao S, Wiersema R, Huang H Y, McClean J R, Babbush R, Boixo S,\nBacon D, Ho A K, Neven H and Mohseni M 2021 arXiv preprint arXiv:2003.02989\n[63] Raﬃn A, Hill A, Gleave A, Kanervisto A, Ernestus M and Dormann N 2021 Journal of Machine\nLearning Research 22 1–8 URL http://jmlr.org/papers/v22/20-1364.html\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n12\n[64] Peruzzo A, McClean J, Shadbolt P, Yung M H, Zhou X Q, Love P J, Aspuru-Guzik A and O’brien\nJ L 2014 Nature communications 5 1–7\n[65] Kandala A, Mezzacapo A, Temme K, Takita M, Brink M, Chow J M and Gambetta J M 2017\nNature 549 242–246\n[66] Kingma D P and Ba J 2014 arXiv preprint arXiv:1412.6980\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n13\nAppendix A. Additional Results\nEvaluation\nSAC MLP\nSAC CNN\nCircle Train Loss\n0.588 ± 0.00115\n0.5993 ± 0.0056\nCircle Validation Loss\n0.5657 ± 0.00148\n0.576 ± 0.01\nMoons Train Loss\n0.4148 ± 0.0201\n0.4266 ± 0.02148\nMoons Validation Loss\n0.39574 ± 0.01771\n0.4098 ± 0.03125\nBlobs Train Loss\n1.854 ± 0.00951\n1.856 ± 0.007386\nBlobs Validation Loss\n1.852 ± 0.00759\n1.859 ± 0.00538\nRegression Train Loss\n0.1349 ± 0.01003\n0.1428 ± 0.00777\nRegression Validation\n0.1526 ± 0.00251\n0.1572 ± 0.00395\nLoss\n5 Qubit QAOA Cost\n−1.312 ± 0.09788\n−1.3387 ± 0.0979\n10 Qubit QAOA Cost\n−2.4763 ± 0.3418\n−2.237 ± 0.319\n20 Qubit QAOA Cost\n−5.3057 ± 0.194\n−4.8765 ± 0.0498\n5 Qubit VQE Cost\n−1.604 ± 0.00368\n−0.6838 ± 0.0863\n10 Qubit VQE Cost\n−0.1667 ± 0.031\n−0.1598 ± 0.00192\n20 Qubit VQE Cost\n−0.00457 ± 0.0004\n−0.00479 ± 0.0007\nTable A1: Noiseless\nEvaluation\nSAC MLP\nSAC CNN\nCircle Train Loss\n0.5743 ± 0.00063\n0.5876 ± 0.0103\nCircle Validation Loss\n0.6094 ± 0.0023\n0.6138 ± 0.00868\nMoons Train Loss\n0.4401 ± 0.0159\n0.4172 ± 0.0229\nMoons Validation Loss\n0.4563 ± 0.008\n0.4493 ± 0.0219\n5 Qubit QAOA Cost\n−1.293 ± 0.0451\n−1.2663 ± 0.0404\n10 Qubit QAOA Cost\n−2.84 ± 0.4335\n−2.634 ± 0.3651\n5 Qubit VQE Cost\n−0.9572 ± 0.2028\n−0.8533 ± 0.078\n10 Qubit VQE Cost\n−0.209 ± 0.00234\n−0.226 ± 0.0293\nTable A2: Only shot noise\nOptimizing Quantum Variational Circuits with Deep Reinforcement Learning\n14\nEvaluation\nSAC MLP\nSAC CNN\nCircle Train Loss\n0.676 ± 0.000288\n0.681 ± 0.00131\nCircle Validation Loss\n0.6571 ± 0.00022\n0.6679 ± 0.0019\nMoons Train Loss\n0.6599 ± 0.00176\n0.6657 ± 0.00204\nMoons Validation Loss\n0.6564 ± 0.000924\n0.6645 ± 0.00253\n5 Qubit QAOA Cost\n−1.364 ± 0.0936\n−1.368 ± 0.095\n10 Qubit QAOA Cost\n−3.1087 ± 0.4198\n−2.7073 ± 0.811\n5 Qubit VQE Cost\n−0.1751 ± 0.0226\n−0.1889 ± 0.0267\n10 Qubit VQE Cost\n−0.829 ± 0.00707\n−0.7524 ± 0.02768\nTable A3: Shot and depolarizing noise\n",
  "categories": [
    "cs.LG",
    "quant-ph"
  ],
  "published": "2021-09-07",
  "updated": "2022-05-14"
}