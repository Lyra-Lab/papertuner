{
  "id": "http://arxiv.org/abs/2109.03702v2",
  "title": "Unsupervised clothing change adaptive person ReID",
  "authors": [
    "Ziyue Zhang",
    "Shuai Jiang",
    "Congzhentao Huang",
    "Richard YiDa Xu"
  ],
  "abstract": "Clothing changes and lack of data labels are both crucial challenges in\nperson ReID. For the former challenge, people may occur multiple times at\ndifferent locations wearing different clothing. However, most of the current\nperson ReID research works focus on the benchmarks in which a person's clothing\nis kept the same all the time. For the last challenge, some researchers try to\nmake model learn information from a labeled dataset as a source to an unlabeled\ndataset. Whereas purely unsupervised training is less used. In this paper, we\naim to solve both problems at the same time. We design a novel unsupervised\nmodel, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person\nReID problem. We developer a purely unsupervised clothing change person ReID\npipeline with person sync augmentation operation and same person feature\nrestriction. The person sync augmentation is to supply additional same person\nresources. These same person's resources can be used as part supervised input\nby same person feature restriction. The extensive experiments on clothing\nchange ReID datasets show the out-performance of our methods.",
  "text": "Unsupervised clothing change adaptive person ReID\nZiyue Zhang, Shuai Jiang, Congzhentao Huang, Richard YiDa Xu\nUniversity of Technology Sydney\nAbstract\nClothing changes and lack of data labels are both crucial chal-\nlenges in person ReID. For the former challenge, people may\noccur multiple times at different locations wearing different\nclothing. However, most of the current person ReID research\nworks focus on the benchmarks in which a person’s cloth-\ning is kept the same all the time. For the last challenge, some\nresearchers try to make model learn information from a la-\nbeled dataset as a source to an unlabeled dataset. Whereas\npurely unsupervised training is less used. In this paper, we\naim to solve both problems at the same time. We design\na novel unsupervised model, Sync-Person-Cloud ReID, to\nsolve the unsupervised clothing change person ReID prob-\nlem. We developer a purely unsupervised clothing change\nperson ReID pipeline with person sync augmentation oper-\nation and same person feature restriction. The person sync\naugmentation is to supply additional same person resources.\nThese same person’s resources can be used as part supervised\ninput by same person feature restriction. The extensive ex-\nperiments on clothing change ReID datasets show the out-\nperformance of our methods.\n1\nIntroduction\nPerson re-identiﬁcation (ReID) (Ye et al. 2021) is designed\nto match speciﬁc pedestrians in images or video sequences.\nThe main challenge of ReID is that the ReID features vari-\nations of the same person in different situations are usu-\nally signiﬁcant because of viewpoint or situation differences,\nwhich makes it challenging to identify the same person.\nMeanwhile, the lack of ReID features variations of different\npeople in the same situation or wearing the same clothing\nalso inﬂuence ReID performance. Current works in person\nReID are mostly to learn discriminative features of person\nidentity by a speciﬁcally designed backbone model (Chang,\nHospedales, and Xiang 2018; Wang et al. 2019a). There\nare also works focusing on problems of occlusions (Hou\net al. 2019), different poses (Qian et al. 2018), illumination\nchanges (Zhang et al. 2020) and resolution changes (Li et al.\n2019).\nThe clothing change plays a crucial character in person\nReID. However, the researchers assume the same person\nwears the same clothes, which means the above works can-\nnot handle the clothing change variations in person ReID. In\ndaily life, people usually change their clothes, which means\nFigure 1: Example of clothing change person ReID dataset\nReal28 (Wan et al. 2020). The query images include ﬁve per-\nsons’ images. The images of each row in the query are from\nthe same person with different clothes. The gallery images\nconsist of different person images with different clothes.\nthat the same person captured by the camera at different\ntimes may wear different clothes. As shown in the ﬁgure 1,\nclothing change ReID datasets consist of a person with dif-\nferent clothing and researchers try to ﬁnd the same person\nunder such settings, which is more in line with the real-life\nscene. The existing conventional methods tend to fail in such\na scenario because of the unreliability of clothing texture in-\nformation and the lack of the same person’s ID information\nwith different clothing.\nAnother key problem is that it is challenging to get person\nReID labels. Because it is expensive to record person images\nacross multiple cameras, many previous works try to address\nperson ReID problem by unsupervised or semi-supervised\nlearning (Lin et al. 2019; Yu et al. 2019; Wang et al. 2018b).\nMost of these methods use unsupervised domain adaptation.\nThey ﬁrst pretrain a model on the source labeled dataset and\nthen ﬁne-tune it on the target unlabeled dataset. However,\nthese methods still need at least one label dataset and suffer\nfrom the variation between the source and target domain.\nHence, we choose to use purely unsupervised learning for\nour method.\narXiv:2109.03702v2  [cs.CV]  14 Sep 2021\nIn this paper, we propose a pure unsupervised model\ncalled Sync-Person-Cloud ReID to solve the unsupervised\nclothing change person ReID problem. The whole pipeline\nfollows the below procedure. The person image features of\nthe training data are extracted by the CNN backbone. Then\nwe use a clustering algorithm to cluster image features and\nproduce pseudo labels. At last, the backbone is trained with a\ncontrastive loss such as InfoNCE loss using the storage per-\nson image features. This pipeline can initially obtain pedes-\ntrian features by purely unsupervised learning. However, it\nis still not able to separate the noise information caused by\ndifferent clothes and the unsupervised features are not robust\nenough. Because people always change clothes on daily life,\nit is not possible to create a simple disentanglement classi-\nﬁer. So to better bridge the gap between the same person\nfeatures with different clothing and improve the unsuper-\nvised performance, we have added several innovations sum-\nmarised as follows:\n1. We use a clothing change person augmentation module to\ngenerate synthetic person images with different clothes.\nA person image is sent to the augmentation module to\nget multiple synthetic person images with person pars-\ning and clothing template. By using this scheme, we can\nget semi-supervised information from synthetic images\nwhich can be seemed as the same person.\n2. To constrain the synthetic same person features, we adopt\na self cluster loss to reduce the distance between these\nfeatures. This will let the later cluster algorithm perform\nbetter by constraining the same person images with dif-\nferent clothes as the same pseudo label. The augmenta-\ntion images can use the semi-supervised information to\nmake the ReID backbone more robust.\n3. Both the average and batch hard sampling are used on\nthe cluster image features. It takes much fewer GPU re-\nsources than using all image features. With the average\nsampling, the model can be trained by the whole dataset\nfeature to get high performance on a whole large dataset.\nWith batch hard sampling, the model can be trained faster\nand more accurately.\nThe rest of the article is organized as follows. Section Re-\nlated work illustrates related works on conventional person\nRe-ID, unsupervised person ReID and clothing change per-\nson ReID. Section Method describes our pipeline structure\nfor getting unsupervised clothing change person ReID fea-\ntures in detail. Our experiment results and details are shown\nin section Experiments. We ﬁnally conclude our work in\nsection Conclusion.\n2\nRelated work\n2.1\nConventional Person ReID\nMost researchers focused on traditional person ReID. In the\nearly research, one primary method of person ReID is met-\nric learning, which is to formalize the problem as super-\nvised metric learning where a projection matrix is sought\nout (Yang, Wang, and Tao 2017; Yi et al. 2014; Ding et al.\n2015). Beneﬁting from the advances of convolutional neural\nnetwork (CNN) architectures, another primary method is to\nlearn appropriate features associated with the same ID using\nfeatures distance information (Hermans, Beyer, and Leibe\n2017) on a backbone module (Chang, Hospedales, and Xi-\nang 2018; Si et al. 2018), such as Resnet50 (He et al. 2016).\nOur work can seem like the second one. We also use a mod-\niﬁed Resnet50 as our ReID backbone.\nAfter getting the ReID features, the person re-id is to train\nthe backbone with contrastive loss. The contrastive loss is\nto reduce the distances between image features of the same\nperson and to increase the distances between the image fea-\ntures of different persons. Several methods employed triplet\nloss (Hermans, Beyer, and Leibe 2017; Chen et al. 2017;\nSong et al. 2018) to constrain the distances between im-\nage triplets. Some researchers also use identity classiﬁcation\nloss (Zheng, Zheng, and Yang 2017; Zhong et al. 2018) to\nmake the ReID problem an image classiﬁcation problem. Al-\nthough these works focus on person ReID, they may fail in\nsome circumstances, e.g., when person images are with dif-\nferent clothing. They cannot handle clothing change ReID\ndata, which will lead to low performance.\n2.2\nUnsupervised Person ReID\nEarly unsupervised Re-ID works are mainly to learn invari-\nant components, i.e., dictionary or metric, whose discrim-\ninability or scalability is relatively insufﬁcient. For deeply\nunsupervised methods, there are two main types of methods.\nThe ﬁrst is unsupervised domain adaptation (UDA),\nwhich transfers the knowledge on a labeled source dataset\nto the unlabeled target dataset. Due to the powerful su-\npervision in the source dataset, it can learn enough infor-\nmation for person ReID. For example, (Deng et al. 2018)\npresents a baseline to translate the labeled images from\nsource to target domain in an unsupervised manner with\nself-similarity and domain-dissimilarity. (Lin et al. 2018)\ndeveloped a Multi-task Mid-level Feature Alignment net-\nwork that can be jointly optimised under the person’s\nidentity classiﬁcation and the attribute learning task with\na cross-dataset mid-level feature alignment regularisation\nterm. (Wang et al. 2018b) proposed a model named Trans-\nferable Joint Attribute-Identity Deep Learning for simulta-\nneously learning an attribute-semantic and identity discrim-\ninative feature representation space transferable to any new\ntarget domain without the need for collecting new labeled\ntraining data from the target domain. Although these works\ncan learn a new unlabeled dataset, the large domain shift is\nstill a challenge for the UDA problem.\nAnother method is end-to-end purely unsupervised learn-\ning, which generally generates pseudo labels from the com-\npletely unlabeled data using a clustering algorithm. (Fan\net al. 2018) proposed an effective baseline named the pro-\ngressive unsupervised learning (PUL) method to transfer\npretrained deep representations to unseen domains. (Fu et al.\n2019) proposed a self-similarity grouping approach to get\nthe pseudo identities by exploiting the potential similarity of\nunlabeled samples to build multiple clusters from different\nviews automatically. (Lin et al. 2019) proposed a bottom-up\nclustering approach to jointly optimize a convolutional neu-\nral network and the relationship among the individual sam-\nples. (Dai et al. 2021) proposed a cluster contrast that stores\nFigure 2: The whole pipeline of our unsupervised clothing change ReID model (best viewed in color). For conciseness, we do\nnot show the augmentation module PISE details here.\nfeature vectors and computes contrast loss in the cluster level\nto solve the inconsistency problem for cluster feature repre-\nsentation.\nIn this paper, we choose to use end-to-end purely unsu-\npervised learning as our initial pipeline.\n2.3\nClothing Change Person ReID\nIn a practical surveillance scenario, there are a large number\nof persons with changing clothes. Some researchers use the\nface and body appearance as the supplement information to\naddress clothing change ReID. (Xue et al. 2018) proposed a\ncloth-Clothing Change Aware Network to address the cloth-\ning change ReID by separately extracting the face and body\ncontext representation. However, the face and body appear-\nance might be unavailable, which may cause the model to\nfail in some scenarios. The infrared or depth images can\nalso be the supplement information to help address clothing\nchange ReID (Barbosa et al. 2012; Wu et al. 2017). How-\never, the depth information is only applicable in indoor en-\nvironments, which is not suitable for all scenarios. And the\ninformation of the infrared image is much less than RGB\nimages’, which makes the ReID less effective.\nFor purely RGB clothing change person ReID, the meth-\nods mostly work on reducing the domain gap between\nthe image features of the same person with different\nclothes. (Hong et al. 2021) proposed a Fine-grained Shape-\nAppearance Mutual learning framework that learns ﬁne-\ngrained discriminative body shape knowledge in a shaped\nstream and transfers it to an appearance stream to comple-\nment the cloth-unrelated knowledge in the appearance fea-\ntures. (Wan et al. 2020) proposed a new clothing change\ndataset with both real and synthetic images. (Shu et al. 2021)\nproposed a semantic-guided pixel sampling model to auto-\nmatically learn cloth-irrelevant cues. (Yang, Wu, and Zheng\n2019) proposed a new dataset and present a spatial polar\ntransformation to learn cross-cloth invariant representation.\nIn this paper, we propose a clothing change augmentation\nand self identity image constrain to help our unsupervised\nmodel solve the clothing change ReID problem\n2.4\nData Augmentation in Person ReID\nOrdinary data augmentations such as random resizing, crop-\nping, random erasing and horizontal ﬂipping are widely used\nin Re-ID. Besides, using GAN to generates augmented im-\nages is also applied in some methods. (Zheng, Zheng, and\nYang 2017) ﬁrst attempted to use the GAN for person Re-\nID. It improves the supervised feature representation learn-\ning with the generated person images. (Liu et al. 2018) in-\ntroduced pose constraints to improve the quality of the gen-\nerated person images, generating the different pose person\nimages. (Zhong et al. 2018) add camera style information\nin the image generation process to address the cross cam-\nera adaptation problem. Some works also use GAN to gen-\nerate synthetic person images from one domain to another.\n(Wang et al. 2019b; Zhang et al. 2021b) used GAN to gen-\nerate the same content person image from RGB modality to\nIR modality. (Wang et al. 2018c) use GAN to transfer low\nresolution images to high resolution versions.\nIn our work, we use a pretrained Decoupled GAN pro-\nposed by (Zhang et al. 2021a) to generate different clothes\nperson images. These images are then seemed as same per-\nson images by the same person constrain loss.\n3\nMethod\n3.1\nOverview of Pipeline\nTo get the unsupervised clothing change person ReID fea-\nture, inspired by (Dai et al. 2021), we proposed our unsu-\npervised pipeline for clothing change ReID in a similar way.\nWe show our whole model pipeline in Figure 2.\nFirst, we use a modiﬁed resnet-50 (He et al. 2016) as our\nbackbone module E for ReID feature extraction, which is\npretrained on ImageNet (Deng et al. 2009). All train dataset\nimages Itrain will be sent to E to get unlabeled training\ndataset ReID features F unlabeled\ntrain\n.\nF unlabeled\ntrain\n= E(Itrain)\n(1)\nThen, we use a clustering algorithm DBScan (Ester et al.\n1996) to generate pseudo labels for each input image feature.\nDBScan is a density-based clustering method, whose clus-\ntering results almost independently depend on the sequence\nof nodes. It can discover the cluster of any shape and effec-\ntively discover noise points, which is more suitable in per-\nson ReID problem scenario than other clustering algorithms\nsuch as Kmeans (MacQueen et al. 1967) and agglomerative\nclustering (Day and Edelsbrunner 1984). DBScan requires\ntwo hyper-parameters. The ﬁrst is the maximum distance ε,\nwhich represents the neighbor radius of the deﬁnition den-\nsity. In other words, ε is the distance between two samples\nfor one to be considered as in the neighborhood of the other.\nAnother is the cluster neighbor density threshold M, which\ndonates the minimum number of samples in a neighborhood\nfor an instance to be considered as a core instance. We set ε\nto 0.4 and M to 4 in all our experiments. Through DBScan\nclustering, the cluster ID is assigned to each training image\nas the pseudo label to get the pseudo-labeled training dataset\nReID features.\nF labeled\ntrain\n= DBScanM\nε (F unlabeled\ntrain\n).\n(2)\nFinally, a contrastive loss with the global and average\nsample is used to compute the loss values between the query\ninstances and the memory dictionary. The details of the com-\nputation and other components will be illustrated later.\n3.2\nClothing Change Person Image Augmentation\nTo get the same person features with different clothes, we in-\ntroduce a pretrained Person Image Synthesis module PISE\n(Zhang et al. 2021a) here. PISE is a two-stage generative\nmodel for Person Image Synthesis and Editing, which is able\nto generate realistic person images with desired poses, tex-\ntures, or semantic layouts. The effectiveness and function of\nPISE are shown in Figure 3.\nWe use PISE to get the synthetic person images Isync\nfrom each input query image Iquery.\nIsync = PISE(Iquery).\n(3)\nWe use S different clothing style template for each input\nquery image to get the corresponding S synthetic images.\nIn our settings, S is set to 4. These synthetic images can be\nassumed to have the same identity but different clothes.\n3.3\nCluster Contrast and Update Procedure\nIn the very ﬁrst step of each epoch training, we random sam-\nple one single image feature from each clustered pseudo-\nlabeled features F labeled\ntrain\nusing uniform sample U.\nFsample(i) = U(F labeled\ntrain (i)).\n(4)\nWe donate the N as the cluster number and Fsample(i) as\na single feature from ith cluster features F labeled\ntrain (i). We\nuse these sampled features as the initial memory features\nfor F average\nmemory and F hard\nmemory.\nDuring the model training stage, we set the number for\nquery image Iquery as P ∗K, in which P is person identity\nnumber and K is the instance number for each person iden-\ntity. We send these K person images to the PISE module to\nget the corresponding S ∗K synthetic person images Isync\nwith different clothes for each query person. All these im-\nages are sent to the ReID backbone to get the query features\nFquery.\nFquery = E(Iquery ∪Isync).\n(5)\nFor updating the feature memory, inspired by (Concha\net al. 2011) which handle the variant dimensions of prob-\nability features using normalization, we select the hardest\ninstance and average instance for each person identity with\nmomentum m. We maintain two features memory, which is\nFigure 3: The function of the PISE module. It takes two in-\nputs which are original input query person image and target\nclothing style image. It will generate a image with the same\ncontent of input query person image and same clothing style\nof input target clothing style image. The synthetic image can\nbe assumed as the same person identity with original input\nquery person image.\naverage feature memory F average\nmemory and hardest feature mem-\nory F hard\nmemory. For a certain cluster with person identity i in\nhardest feature memory F hard\nmemory, its feature vector is up-\ndated by:\nf hard\nquery = arg max\nfquery\nKL(fquery, F hard\nmemory(i)),\nfquery ∈Fquery(i),\nF hard\nmemory(i) = m · F hard\nmemory(i) + (1 −m) · f hard\nquery,\n(6)\nwhere the batch hard instance f hard\nquery is the instance with the\nminimum similarity to the cluster feature. We measure the\nsimilarity with KL divergence. m is the momentum updating\nfactor. Fquery(i) is the instance features set with cluster ID\ni in the current mini batch.\nFor a certain cluster with person identity i in average fea-\nture memory F average\nmemory, its feature vector is updated by:\nf average\nquery\n=\nP fquery\n(S + 1) × K ,\nfquery ∈Fquery(i),\nF average\nmemory(i) = m · F average\nmemory(i) + (1 −m) · f average\nquery ,\n(7)\nwhere the batch hard instance f average\nquery is the average instance\nof all query features Fquery(i) from identity i.\nThe query image features (including the original input and\nsynthetic image features) are compared to all cluster features\n(including average memory and hardest memory) with In-\nfoNCE loss (Oord, Li, and Vinyals 2018). The InfoNCE loss\ncan be formulated as follows:\nLq(Fquery, Fmemory) = −log\nexp (fq · f +\nm) /τ\nPK\ni=0 exp (fq, fm(i)) /τ\n,\nfq ∈Fquery, fm ∈F hard\nmemory ∪F average\nmemory.\n(8)\nwhere f +\nm is the positive memory cluster feature vector to\nquery instance fq. τ is a temperature hyper-parameter (Wu\net al. 2018). The loss tries to classify fq as f +\nm through low\nvalue when fq is similar to its positive cluster feature and\ndissimilar to all other cluster features.\n3.4\nSelf Identity Constrain\nSince the synthetic clothing change images can seem as the\nsame person, we use a self identity constrain loss to reduce\nthe variance between all synthetic person image features\ngenerated from the same input person image. We deﬁne this\nloss as follows:\nLs =\nPP\ni=0\nPK\nj=0\nPS+1\na,b=0 KL(f a\nquery, f b\nquery)\nP × K × (S + 1)\n,\nfquery ∈Fquery.\n(9)\nThe loss tries to classify all synthetic and corresponding\noriginal input person image features as the same person by\nreducing the KL divergence between them.\nAlgorithm 1: Whole training process for our method\nRequire: Unlabeled training images Itrain\nRequire: Backbone E Pretrained on Imagenet\nRequire: Pretrained clothing change person image synthe-\nsis module PISE\nRequire: Hyper parameters P, K, S, m, τ, M, ε\n1: for n ∈[1, max epochs] do\n2:\nSend Itrain to E to get all features F unlabeled\ntrain\n.\n3:\nCluster F unlabeled\ntrain\nto get F labeled\ntrain\nusing Eq.2.\n4:\nInitialize the memory F average\nmemory and F hard\nmemory with\nFsample using Eq.4.\n5:\nfor i ∈[1, max iter] do\n6:\nSample P × K person images Iquery from Itrain\n7:\nSent Iquery to PISE to get Isync using Eq.3.\n8:\nSent Iquery and Isync to E to get query features\nFquery using Eq.5.\n9:\nCompute InfoNCE loss and self identity loss using\nEq.8 and Eq.9 and compute the total loss L = Lq+\nαLs.\n10:\nBackwards the total loss functions to optimize our\nencoder E.\n11:\nUpdate hardest memory and average memory us-\ning Eq.6 and Eq.7.\n12:\nend for\n13: end for\n3.5\nWhole Train Process\nWe show our whole unsupervised training process in Al-\ngorithm 1. As shown in the pseudo code, the entire training\nprocess of each epoch consists of three stages, which are\n1) cloud for pseudo label and memory initialization, 2) loss\ncomputation for updating backbone and 3) memory update.\n4\nExperiments\n4.1\nImplementation Details\nWe implement our model with Pytorch ((Paszke et al.\n2019)). We conduct our model based on the unsupervised\nlearning baseline (Dai et al. 2021). We adopt the ResNet-\n50 ((He et al. 2016)) as our backbone. We modify the last\nlayer stride to be 1 in the backbone Resnet50 to make ﬁnal\noutput features have more abundant information. Then we\nextract 2048d features for all images from the GAP layer\nin our backbone. During testing, we take the 2048d features\nto calculate the distance. For the beginning of each epoch,\nwe use DBScan as our clustering method to generate pseudo\nlabels for unlabeled input images.\nThe input image is resized to 256×128. The random crop-\nping and horizontal ﬂipping are performed as the augmenta-\ntion methods. We haven’t used random erasing and image\npadding because we want to keep the content of the input\nimage to get synthetic clothing change person images. The\nbatch size P × K is set as 32, in which K is set to 4 as\nperson images of identity and P is set to 8 as pseudo per-\nson identities. The synthetic person image number for each\ninput image S is set to 4. So we process 160 images per\nmini-batch. We set the weight factor of loss α as 0.3. We set\nTable 1: Comparison on PRCC and VC-Clothes datasets.The R1 is Rank-1 CMC accuracies (%). The mAP denotes mean\nAverage Precision score (%).\nMethod\nPRCC\nVC-Clothes\nClothing Change\nSame Clothing\nClothing Change\nSame Clothing\nR1\nmAP\nR1\nmAP\nR1\nmAP\nR1\nmAP\nLOMO(Liao et al. 2015) + KISSME(Koestinger et al. 2012)\n18.55\n-\n47.40\n-\n-\n-\n-\n-\nLOMO(Liao et al. 2015) + XQDA(Liao et al. 2015)\n14.53\n-\n29.41\n-\n34.5\n30.9\n86.2\n83.3\nPCB(Sun et al. 2018)\n41.8\n38.7\n86.88\n-\n62.0\n62.2\n94.7\n94.3\nRGA-SC(Zhang et al. 2020)\n42.3\n-\n98.4\n-\n71.1\n67.4\n95.4\n94.8\nMGN(Wang et al. 2018a)\n33.8\n35.9\n99.5\n98.4\n-\n-\n-\n-\nLTCC(Qian et al. 2020)\n34.38\n-\n64.2\n-\n-\n-\n-\n-\nFSAM(Hong et al. 2021)\n54.5\n-\n98.8\n-\n78.6\n78.9\n94.7\n94.8\nSGPS(Shu et al. 2021)\n65.8\n61.2\n99.5\n96.7\n-\n-\n-\n-\nSync-Person-Cloud\n43.7\n39.8\n87.4\n82.1\n67.4\n62.5\n91.9\n89.3\nthe momentum for update the memory instance m as 0.3.\nWe use Adam optimizer and set both the weight decay\nfactor and weight decay bias factor as 0.0005. The base\nlearning rate is 0.00035. We use a linear warming up strat-\negy for adjustment of learning rate at train stage. For the ﬁrst\nEwarmup epoch with start learning rate Rstart, the learn-\ning rate will linearly increase from\nRstart\nEwarmup to Rstart. We\nset warm-up iteration number Ewarmup as 20 in our exper-\niments. The total training epoch number is 120. We set the\nmaximum distance ε between two samples as 0.4 and the\nminimal number of neighbors in a core point M as 4 for our\nclustering method DBScan hyperparameters.\n4.2\nDatasets and Evaluation Protocol\nWe mainly evaluated our method on two cloth changing Re-\nID datasets: PRCC (Yang, Wu, and Zheng 2019) and VC-\nClothes (Wan et al. 2020).\nPRCC is named Person Re-id under the moderate Cloth-\ning Change dataset. It was collected for the task of cloth-\nchanging person re-ID. PRCC consists of 221 identities\ncaptured by three camera views. VC-Clothes is a synthetic\ndataset rendered by the GTA5 game engine, which contains\n19060 images of 512 identities captured from 4 cameras.\nFor evaluation protocol, we adopted the mean average\nprecision (mAP) and ard Cumulative Match-ing Charac-\nteristics (CMC) rank-k accuracy. For both cloth-changing\ndatasets PRCC and VC-Clothes, we followed their evalua-\ntion protocols and evaluated the performance. For PRCC,\nwe used single-shot matching by randomly choosing one im-\nage of each identity as the gallery. The cloth-changing set-\nting in PRCC means there are all clothing change samples\nin the test set. For VC-Clothes, we used multi-shot match-\ning and the clothing change setting is the same as that of\nPRCC. We also do the experiments on both datasets un-\nder the same clothing settings, whose test sets are all cloth-\nconsistent samples in the test set.\n4.3\nComparison With the State-of-The-Art\nAs there is no existing unsupervised clothing change per-\nson ReID method, we compare our method with the state-of-\nthe-art supervised clothing change person ReID method. As\nshown in Table 1, we can see that our method is not the high-\nest solution overall methods. The reason is that although we\nuse person augmentation information and a strong unsuper-\nvised deep ReID pipeline, our model still lacks some super-\nvised information to converge the ReID backbone. Through\nself identity constrain loss for synthetic clothing change per-\nson image feature and using a clustering algorithm to get\npseudo labels, we can get some identity information from\nthe dataset. However, the pseudo label is inaccurate and the\nperson number keeps change so that we can’t use classiﬁca-\ntion loss. The synthetic clothing change image is generated\nfrom a pretrained PISE module, whose image information\nis from other texture style transfer datasets. Hence, the qual-\nity of the supervised information restricts our model’s abil-\nity.\nHowever, our method can perform as well as some su-\npervised person ReID model, which is not design for cloth-\ning change scenarios. As shown in the table 1, our method\nhave higher Rank 1 accuracy and mAP than PCB (Sun\net al. 2018), RGA-SC (Zhang et al. 2020) and LOMO (Liao\net al. 2015). Hence, our method is effective for unsupervised\nclothing change ReID.\n4.4\nAblation Study\nComponent Effectiveness\nIn this section, we study the\neffectiveness of key components of our proposed method.\nWe take PRCC dataset under clothing change settings as an\nexample. The baseline is a pure unsupervised person ReID\npipeline. The components include Ca, Ci and Cs. Ca is the\nPISE person clothing change image augmentation module,\nCi is the self identity constrain loss and Cs is the cluster av-\nerage and hardest sample operation. Because the Ci must be\nbased on Ca, we need to at least use both components when\nwe want to analyze the effectiveness of Ci. The results of\nour proposed different components’ effectiveness are shown\nin Table 2.\nAs shown in Table 2, our proposed model with all com-\nponents signiﬁcantly outperformed the baseline model. The\nbaseline gets 29.6% R1 accuracy, which is much lower than\nthe most of state of the art method. By using Ca, we can\nget much more information from the augmented clothing\nchange person images. Through training these images, the\nTable 2: Self comparison on PRCC dataset.\nMethod\nComponent\nPRCC\nCa\nCi\nCs\nR1\nmAP\nBaseline\n29.6\n27.4\nBaseline\n✓\n30.3\n27.9\nBaseline\n✓\n32.4\n30.8\nBaseline\n✓\n✓\n35.1\n33.3\nBaseline\n✓\n✓\n37.2\n35.3\nSync-Person-Cloud\n✓\n✓\n✓\n43.7\n39.8\nmodel can improve 0.7% R1 accuracy, which is not so sig-\nniﬁcant. The reason for the low improvement is that al-\nthough we use the augmentation module, the identity in-\nformation is still not be constrained and with more images\ninformation it is hard for the model to converge. By us-\ning Cs, we can get 2.8% R1 accuracy improvement. The\nhardest and average sampling can help the model with bet-\nter and faster convergence. The average sampling can help\nthe model learn more global information in the dataset and\nthe hardest sampling can help the model learn from an ex-\ntreme instance. Hence, by combining Ca and Cs together,\nwe can get 35.1% in R1 accuracy. Through using Ci to con-\nstrain the synthetic person image features generated from the\nsame original input person image, the model can get 37.2%\nR1 accuracy. This signiﬁcant improvement shows that the\naugmentation images have abundant person identity infor-\nmation for model disentanglement between people’s cloth-\ning and identity, which helps our model to adapt the cloth-\ning change person ReID scenario. Finally, our method with\nall components can reach the highest R1 accuracy 43.7%,\nwhich means that the Ca, Ci and Cs can work well corpo-\nrately.\nSample Method\nIn this section, we want to discuss the ef-\nfectiveness of our sampling method. We take PRCC dataset\nunder clothing change settings as an example.\nTable 3: Sampling method comparison on PRCC dataset.\nSampling Method\nPRCC\nR1\nmAP\nNo Sample\n37.2\n35.3\nAverage Sample\n36.1\n34.7\nHardest Sample\n41.2\n37.4\nBoth\n43.7\n39.8\nThe comparison results are shown in Table 3. All the set-\ntings are based on our model with image augmentation and\nself identity constrain. The ﬁrst setting is that we don’t use\nsampling in our contrast module, which means the query\nfeatures and memory need to update one by one. In this case,\nthe feature memory is not a single instance for each identity.\nIt will record multiple instances in the memory to decrease\nthe update frequency for each instance. It can achieve 37.2%\nin Rank1 accuracy. Using the average sampling method de-\ncreases the accuracy by 1.1% than ﬁrst settings. The rea-\nson is mainly that although we use the average sample to\nreduce the memory load and utilize the global information\nof the same pseudo person, the model is harder to converge\nthan use all the instances one by one. Another reason is that\nthe pseudo label is not stable for update global information,\nwhich also causes low performance. By using the hardest\nsampling method, we can get 41.2% Rank1 accuracy. The\nhardest sampling can help the model better converge and\nlearn from the extreme image features. By using both av-\nerage and hardest sampling, our model can get the highest\nRank1 accuracy 43.7%. In this situation, using global infor-\nmation is not the obstruction of the model convergence with\nthe help from the hardest sampling. The hardest sampling\nwill lead the gradient to the extreme instance and average\nsampling will help the model keep the global information of\nall images (including the synthetic clothing change images).\nDBScan Hyper-parameters\nIn the DBScan clustering al-\ngorithm, ε is the maximum distance between two samples\nfor one to be considered as in the neighborhood of the other.\nIt greatly affects the performance of clustering by deciding\nthe ﬁnal number of clusters (person pseudo identity numbers\nin our case).\nWhen ε is too small, the cluster with a small density\nwill be divided into multiple clusters with similar properties.\nHowever, when ε is too large, clusters with a closer distance\nand a larger density will be merged into one cluster. In the\ncase of high-dimensional data in the person ReID problem,\nit is more difﬁcult to select the ε value due to the curse of\ndimensionality. We analyze ε inﬂuence on the PRCC dataset\nunder clothing change settings using our whole model.\nTable 4: Analysis of ε value on PRCC dataset.\nε Value\nPRCC\nR1\nmAP\n0.2\n38.4\n33.1\n0.3\n41.2\n36.5\n0.4\n43.7\n39.8\n0.5\n40.8\n35.9\n0.6\n40.2\n35.8\n0.7\n40.4\n34.1\nAs shown in the Table 4, we can see that our method\nworks best when ε equals 0.4.\n5\nConclusion\nIn this paper, to solve the lack of person identity label and\nclothing change problem in person ReID, we propose an un-\nsupervised clothing change person ReID model called Sync-\nPerson-Cloud. To our best knowledge, we are the ﬁrst to use\nunsupervised learning in the clothing change ReID problem.\nTo improve the ability of extraction clothing change invari-\nant information of our model, we propose a self identity\nconstrain for synthetic clothing change person image and a\nhardest and average sampling method for cluster contrast.\nExperiments show the performance of our model compared\nwith the state-of-the-art supervised clothing change person\nReID methods.\nReferences\nBarbosa, I. B.; Cristani, M.; Del Bue, A.; Bazzani, L.;\nand Murino, V. 2012.\nRe-identiﬁcation with rgb-d sen-\nsors. In European Conference on Computer Vision, 433–\n442. Springer.\nChang, X.; Hospedales, T. M.; and Xiang, T. 2018. Multi-\nlevel factorisation net for person re-identiﬁcation. In CVPR,\n2109–2118.\nChen, W.; Chen, X.; Zhang, J.; and Huang, K. 2017. Be-\nyond triplet loss: a deep quadruplet network for person re-\nidentiﬁcation. In CVPR, 403–412.\nConcha, O. P.; Da Xu, R. Y.; Moghaddam, Z.; and Piccardi,\nM. 2011. HMM-MIO: an enhanced hidden Markov model\nfor action recognition. In CVPR 2011 WORKSHOPS, 62–\n69. IEEE.\nDai, Z.; Wang, G.; Yuan, W.; Zhu, S.; and Tan, P. 2021.\nCluster Contrast for Unsupervised Person Re-Identiﬁcation.\narXiv preprint arXiv:2103.11568.\nDay, W. H.; and Edelsbrunner, H. 1984. Efﬁcient algorithms\nfor agglomerative hierarchical clustering methods. Journal\nof classiﬁcation, 1(1): 7–24.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDeng, W.; Zheng, L.; Ye, Q.; Kang, G.; Yang, Y.; and\nJiao, J. 2018.\nImage-image domain adaptation with pre-\nserved self-similarity and domain-dissimilarity for person\nre-identiﬁcation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 994–1003.\nDing, S.; Lin, L.; Wang, G.; and Chao, H. 2015. Deep fea-\nture learning with relative distance comparison for person\nre-identiﬁcation. Pattern Recognition, 48(10): 2993–3003.\nEster, M.; Kriegel, H.-P.; Sander, J.; Xu, X.; et al. 1996.\nA density-based algorithm for discovering clusters in large\nspatial databases with noise. In kdd, volume 96, 226–231.\nFan, H.; Zheng, L.; Yan, C.; and Yang, Y. 2018.\nUnsu-\npervised person re-identiﬁcation: Clustering and ﬁne-tuning.\nACM Transactions on Multimedia Computing, Communica-\ntions, and Applications (TOMM), 14(4): 1–18.\nFu, Y.; Wei, Y.; Wang, G.; Zhou, Y.; Shi, H.; and Huang, T. S.\n2019. Self-similarity grouping: A simple unsupervised cross\ndomain adaptation approach for person re-identiﬁcation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 6112–6121.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nHermans, A.; Beyer, L.; and Leibe, B. 2017. In defense of\nthe triplet loss for person re-identiﬁcation. arXiv preprint\narXiv:1703.07737.\nHong, P.; Wu, T.; Wu, A.; Han, X.; and Zheng, W.-S.\n2021. Fine-Grained Shape-Appearance Mutual Learning for\nCloth-Changing Person Re-Identiﬁcation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 10513–10522.\nHou, R.; Ma, B.; Chang, H.; Gu, X.; Shan, S.; and Chen,\nX. 2019.\nVRSTC: Occlusion-Free Video Person Re-\nIdentiﬁcation. In CVPR, 7183–7192.\nKoestinger, M.; Hirzer, M.; Wohlhart, P.; Roth, P. M.; and\nBischof, H. 2012. Large scale metric learning from equiv-\nalence constraints. In 2012 IEEE conference on computer\nvision and pattern recognition, 2288–2295. IEEE.\nLi, Y.-J.; Chen, Y.-C.; Lin, Y.-Y.; Du, X.; and Wang, Y.-C. F.\n2019. Recover and identify: A generative dual model for\ncross-resolution person re-identiﬁcation.\nIn ICCV, 8090–\n8099.\nLiao, S.; Hu, Y.; Zhu, X.; and Li, S. Z. 2015. Person re-\nidentiﬁcation by local maximal occurrence representation\nand metric learning. In CVPR, 2197–2206.\nLin, S.; Li, H.; Li, C.-T.; and Kot, A. C. 2018.\nMulti-\ntask mid-level feature alignment network for unsuper-\nvised cross-dataset person re-identiﬁcation. arXiv preprint\narXiv:1807.01440.\nLin, Y.; Dong, X.; Zheng, L.; Yan, Y.; and Yang, Y. 2019.\nA bottom-up clustering approach to unsupervised person re-\nidentiﬁcation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, 8738–8745.\nLiu, J.; Ni, B.; Yan, Y.; Zhou, P.; Cheng, S.; and Hu, J. 2018.\nPose transferrable person re-identiﬁcation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 4099–4108.\nMacQueen, J.; et al. 1967. Some methods for classiﬁcation\nand analysis of multivariate observations. In Proceedings of\nthe ﬁfth Berkeley symposium on mathematical statistics and\nprobability, volume 1, 281–297. Oakland, CA, USA.\nOord, A. v. d.; Li, Y.; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. In NeurIPS, 8026–8037.\nQian, X.; Fu, Y.; Xiang, T.; Wang, W.; Qiu, J.; Wu, Y.; Jiang,\nY.-G.; and Xue, X. 2018. Pose-normalized image generation\nfor person re-identiﬁcation. In ECCV, 650–667.\nQian, X.; Wang, W.; Zhang, L.; Zhu, F.; Fu, Y.; Xiang, T.;\nJiang, Y.-G.; and Xue, X. 2020. Long-term cloth-changing\nperson re-identiﬁcation. In Proceedings of the Asian Con-\nference on Computer Vision.\nShu, X.; Li, G.; Wang, X.; Ruan, W.; and Tian, Q. 2021.\nSemantic-guided Pixel Sampling for Cloth-Changing Person\nRe-identiﬁcation. IEEE Signal Processing Letters.\nSi, J.; Zhang, H.; Li, C.-G.; Kuen, J.; Kong, X.; Kot,\nA. C.; and Wang, G. 2018. Dual attention matching net-\nwork for context-aware feature sequence based person re-\nidentiﬁcation. In CVPR, 5363–5372.\nSong, C.; Huang, Y.; Ouyang, W.; and Wang, L. 2018.\nMask-guided contrastive attention model for person re-\nidentiﬁcation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 1179–1188.\nSun, Y.; Zheng, L.; Yang, Y.; Tian, Q.; and Wang, S. 2018.\nBeyond part models: Person retrieval with reﬁned part pool-\ning (and a strong convolutional baseline). In ECCV, 480–\n496.\nWan, F.; Wu, Y.; Qian, X.; and Fu, Y. 2020. When Person\nRe-identiﬁcation Meets Changing Clothes. arXiv preprint\narXiv:2003.04070.\nWang, G.; Lai, J.; Huang, P.; and Xie, X. 2019a. Spatial-\ntemporal person re-identiﬁcation.\nIn AAAI, volume 33,\n8933–8940.\nWang, G.; Yuan, Y.; Chen, X.; Li, J.; and Zhou, X. 2018a.\nLearning discriminative features with multiple granularities\nfor person re-identiﬁcation. In ACMMM, 274–282. ACM.\nWang, G.; Zhang, T.; Cheng, J.; Liu, S.; Yang, Y.; and\nHou, Z. 2019b. RGB-Infrared Cross-Modality Person Re-\nIdentiﬁcation via Joint Pixel and Feature Alignment.\nIn\nICCV, 3623–3632.\nWang, J.; Zhu, X.; Gong, S.; and Li, W. 2018b. Transferable\njoint attribute-identity deep learning for unsupervised per-\nson re-identiﬁcation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 2275–2284.\nWang, Z.; Ye, M.; Yang, F.; Bai, X.; and Satoh, S. 2018c.\nCascaded SR-GAN for scale-adaptive low resolution person\nre-identiﬁcation. In IJCAI, volume 1, 4.\nWu, A.; Zheng, W.-S.; Yu, H.-X.; Gong, S.; and Lai, J.\n2017. Rgb-infrared cross-modality person re-identiﬁcation.\nIn ICCV, 5380–5389.\nWu, Z.; Xiong, Y.; Yu, S. X.; and Lin, D. 2018. Unsuper-\nvised feature learning via non-parametric instance discrimi-\nnation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 3733–3742.\nXue, J.; Meng, Z.; Katipally, K.; Wang, H.; and van Zon,\nK. 2018. Clothing change aware person identiﬁcation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, 2112–2120.\nYang, Q.; Wu, A.; and Zheng, W.-S. 2019.\nPerson re-\nidentiﬁcation by contour sketch under moderate clothing\nchange. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence.\nYang, X.; Wang, M.; and Tao, D. 2017.\nPerson re-\nidentiﬁcation with metric learning using privileged informa-\ntion. IEEE Transactions on Image Processing, 27(2): 791–\n805.\nYe, M.; Shen, J.; Lin, G.; Xiang, T.; Shao, L.; and Hoi, S. C.\n2021. Deep learning for person re-identiﬁcation: A survey\nand outlook. IEEE Transactions on Pattern Analysis and\nMachine Intelligence.\nYi, D.; Lei, Z.; Liao, S.; and Li, S. Z. 2014. Deep metric\nlearning for person re-identiﬁcation. In 2014 22nd Interna-\ntional Conference on Pattern Recognition, 34–39. IEEE.\nYu, H.-X.; Zheng, W.-S.; Wu, A.; Guo, X.; Gong, S.; and\nLai, J.-H. 2019. Unsupervised person re-identiﬁcation by\nsoft multilabel learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2148–2157.\nZhang, J.; Li, K.; Lai, Y.-K.; and Yang, J. 2021a.\nPISE:\nPerson Image Synthesis and Editing With Decoupled GAN.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 7982–7990.\nZhang, Z.; Da Xu, R. Y.; Jiang, S.; Li, Y.; Huang, C.; and\nDeng, C. 2020. Illumination Adaptive Person Reid Based on\nTeacher-Student Model and Adversarial Training. In ICIP,\n2321–2325.\nZhang, Z.; Jiang, S.; Huang, C.; Li, Y.; and Da Xu, R. Y.\n2021b.\nRGB-IR cross-modality person ReID based on\nteacher-student GAN model. Pattern Recognition Letters,\n150: 155–161.\nZhang, Z.; Lan, C.; Zeng, W.; Jin, X.; and Chen, Z. 2020.\nRelation-aware global attention for person re-identiﬁcation.\nIn Proceedings of the ieee/cvf conference on computer vision\nand pattern recognition, 3186–3195.\nZheng, Z.; Zheng, L.; and Yang, Y. 2017.\nUnlabeled\nSamples Generated by GAN Improve the Person Re-\nidentiﬁcation Baseline in vitro. In ICCV.\nZhong, Z.; Zheng, L.; Zheng, Z.; Li, S.; and Yang, Y. 2018.\nCamera style adaptation for person re-identiﬁcation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 5157–5166.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-09-08",
  "updated": "2021-09-14"
}