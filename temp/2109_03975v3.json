{
  "id": "http://arxiv.org/abs/2109.03975v3",
  "title": "Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning",
  "authors": [
    "Maziar Gomrokchi",
    "Susan Amin",
    "Hossein Aboutalebi",
    "Alexander Wong",
    "Doina Precup"
  ],
  "abstract": "While significant research advances have been made in the field of deep\nreinforcement learning, there have been no concrete adversarial attack\nstrategies in literature tailored for studying the vulnerability of deep\nreinforcement learning algorithms to membership inference attacks. In such\nattacking systems, the adversary targets the set of collected input data on\nwhich the deep reinforcement learning algorithm has been trained. To address\nthis gap, we propose an adversarial attack framework designed for testing the\nvulnerability of a state-of-the-art deep reinforcement learning algorithm to a\nmembership inference attack. In particular, we design a series of experiments\nto investigate the impact of temporal correlation, which naturally exists in\nreinforcement learning training data, on the probability of information\nleakage. Moreover, we compare the performance of \\emph{collective} and\n\\emph{individual} membership attacks against the deep reinforcement learning\nalgorithm. Experimental results show that the proposed adversarial attack\nframework is surprisingly effective at inferring data with an accuracy\nexceeding $84\\%$ in individual and $97\\%$ in collective modes in three\ndifferent continuous control Mujoco tasks, which raises serious privacy\nconcerns in this regard. Finally, we show that the learning state of the\nreinforcement learning algorithm influences the level of privacy breaches\nsignificantly.",
  "text": "Membership Inference Attacks Against Temporally\nCorrelated Data in Deep Reinforcement Learning\nMaziar Gomrokchi§\nDepartment of Computer Science\nMcGill University / Mila\ngomrokma@mila.quebec\nSusan Amin§\nDepartment of Computer Science\nMcGill University / Mila\nsusan.amin@mail.mcgill.ca\nHossein Aboutalebi§\nCheriton School of Computer Science\nUniversity of Waterloo / VIP Lab\nhaboutal@uwaterloo.ca\nAlexander Wong\nDepartment of Systems Design Engineering\nUniversity of Waterloo / VIP Lab / DarwinAI\na28wong@uwaterloo.ca\nDoina Precup\nDepartment of Computer Science\nMcGill University / Mila / DeepMind\ndprecup@cs.mcgill.ca\nI. ABSTRACT\nWhile signiﬁcant research advances have been made in\nthe ﬁeld of deep reinforcement learning, there have been\nno concrete adversarial attack strategies in literature tailored\nfor studying the vulnerability of deep reinforcement learning\nalgorithms to membership inference attacks. In such attacking\nsystems, the adversary targets the set of collected input data\non which the deep reinforcement learning algorithm has been\ntrained. To address this gap, we propose an adversarial attack\nframework designed for testing the vulnerability of a state-of-\nthe-art deep reinforcement learning algorithm to a membership\ninference attack. In particular, we design a series of experiments\nto investigate the impact of temporal correlation, which\nnaturally exists in reinforcement learning training data, on the\nprobability of information leakage. Moreover, we compare\nthe performance of collective and individual membership\nattacks against the deep reinforcement learning algorithm.\nExperimental results show that the proposed adversarial attack\nframework is surprisingly effective at inferring data with an\naccuracy exceeding 84% in individual and 97% in collective\nmodes in three different continuous control Mujoco tasks, which\nraises serious privacy concerns in this regard. Finally, we show\nthat the learning state of the reinforcement learning algorithm\ninﬂuences the level of privacy breaches signiﬁcantly.\nII. INTRODUCTION\nDespite the recent advancements in the design and perfor-\nmance of deep reinforcement learning (deep RL) algorithms in\ncomplex domains ( [1]–[3]), the vulnerability of these models\nto privacy breaches has only begun to be explored in the\nliterature. In particular, while there have been a few studies\non the vulnerability of deep RL models to adversarial attacks\n[4]–[6], there has been no study on the potential membership\nleakage of the data directly employed in training deep RL\nmodels, which is known as membership inference attacks\n(MIAs). The potential success of such MIAs can have serious\n§Equal contribution\nsecurity ramiﬁcations in the deployment of models resulting\nfrom deep RL.\nOne of the major challenges in the implementation of\nMIAs in deep RL settings is the sequential and correlated\nnature of deep RL data points. Unlike in deep supervised\nsettings, a data point in deep RL algorithms may consist\nof hundreds of correlated components in the form of tuples,\nall together forming a single trajectory. A successful MIA\nalgorithm against a deep RL model should be able to learn not\nonly the relation between the training and output trajectories but\nalso the correlation between the tuples within each trajectory\n(data point). Another complication in this regard concerns the\nrelationship between the training and prediction data points.\nIn deep RL settings, batches of collected input data are used\nfor training the deep RL policy. Thus, each output data point\ncorresponds to every single data point in the training batches.\nThis feature is in contrast with, for instance, that of data\npoints in text generation problems (e.g. machine translation or\ndialogue generation systems), where there is a direct (usually\none-to-one) correspondence between the input and output\nsequential data points. Finally, RL algorithms are learning\nsystems where the concept of labels is not deﬁned as it is in\nsupervised learning methods. Instead, during the learning phase,\nthe deep RL agent receives reinforcement (aka rewards) from\nthe environment as the outcome of the selected action. The\ndeep RL agent uses the obtained rewards to learn the task and\noptimize its learning policy. These factors lead to complications\nin deﬁning input-output pairs in training attack classiﬁers and\nsubsequently establishing a meaningful relationship between\nthe pair constituents.\nDeep RL methods have a unique structural difference\ncompared to deep supervised or unsupervised methods, i.e.\nlearning based on temporal correlation between the tuples\nin each trajectory and partial reinforcements the model re-\nceives upon interaction with the underlying environment. Even\nthough deep RL models decorrelate input trajectories through\nan intermediate mechanism called replay buffer (for more\ninformation, refer to the Background section), the inherent\n1\narXiv:2109.03975v3  [cs.LG]  16 Nov 2022\ncorrelation between transition tuples still plays a signiﬁcant\nrole in the feature representation learned by the deep RL model\n[7], hence the behaviour of the output policy. In this regard,\ntwo natural questions arise:\n1) How much information (concerning the training data\npoints) can an adversary extract from the output of a\ntrained deep RL model?\n2) To what extent can an adversary beneﬁt from feature\ncorrelation in the learned policy?\nThis study presents the ﬁrst black-box MIA against a deep\nRL agent to address these two questions. In our proposed\nadversarial attack framework, the target model is considered a\nblack box; thus, the attacker does not have access to the internal\nstructure of the deep RL agent. In particular, the attacker can\nonly access the model output in the form of trajectories τ out\nT\nresulting from the trained policy πf. We use batch off-policy\nreinforcement learning setting, where the common practice is\nthat an (unknown) exploration policy (behaviour policy) πb\ncollects private data points in the form of a batch of trajectories.\nThe batch data is thereafter delivered to the deep RL algorithm\nin the form of independent trajectories (Markov chains) to train\nthe target policy. In this setting, the RL agent decouples the\ndata collection phase from the policy training phase (i.e. off-\npolicy). In the off-policy setting, the learning system is not tied\nto a particular exploration algorithm and ensures disjointedness\nbetween the training data sets provided for the RL algorithm in\ndifferent settings. Off-policy setup is particularly preferred in\ndesigning MIA frameworks in black-box settings, where neither\nthe internal structure of the target model nor the exploration\npolicy used to collect the training trajectories is known to the\nadversary.\nOur proposed attack framework tests the vulnerability of a\nstate-of-the-art off-policy deep RL model to MIA in two modes:\nindividual and collective. In the individual mode, the attacker’s\ngoal is to train a probabilistic model that infers the membership\nprobability of a single trajectory τ in\nT given the trained policy\nπf and the initial state s0. In this case, the goal is to measure\nthe extent to which the adversary can exploit trajectory-level\ntemporal correlation to reveal the presence of a trajectory in\nthe training set. In the collective mode, the attacker’s target\nis to predict the membership probability of a collection of\ndata points. In this mode, the goal is to measure the extent\nto which the adversary is capable of exploiting not only the\ntrajectory-level temporal correlation but also the batch-level\ncorrelation to reveal the presence of a trajectory in the training\nset. We show that the deep RL model is more vulnerable to\ncollective MIA as in this mode, the attack classiﬁer has access\nto more information.\nMoreover, we assess the vulnerability of the RL algorithm to\nMIA in terms of the learning state of the algorithm. Our results\nshow that the cumulative amount of reinforcement the RL agent\nobtains in the course of training the policy is proportional to\nthe level of its vulnerability to MIAs. Finally, to determine\nthe role of data correlation in the vulnerability of the deep RL\nmodel to MIA, we disturb the correlation within the data points\nused to train the attack classiﬁer and subsequently compare the\nimpact of training the attacker with the resulting decorrelated\ntrajectories on the performance of the attack classiﬁer. We\nobserve that the presence of correlation within the trajectories\nhelps the adversary discern between the member and non-\nmember data points with higher probability compared with the\nresults obtained from the decorrelated case.\nBACKGROUND\nIn this section, we provide the background information in\ntwo parts: i) a general introduction to reinforcement learning\nsystems, and ii) membership inference attacks.\nReinforcement Learning\nIn reinforcement learning (RL) systems, an agent learns\na task through a sequence of trial and error and receives\nrewards through environmental interactions. The agent’s task is\nformalized as a stochastic process that is described by a Markov\nDecision Process (MDP). An MDP is a tuple ⟨S, A, P, R, p0⟩\nconsisting of a set of states S, a set of actions A, a transition\nprobability kernel P : S × A →Pr(S), a reward function\nR : S × A →R, and an initial state distribution p0 that\ncharacterizes the initial state of each episode. At each time\nstep t = 0, 1, 2, . . . , T −1, the agent is at the environment\nstate st ∈S and selects action at ∈A according to the\npolicy π(at|st). The policy π : S →Pr(A) is the agent’s\naction-selection strategy, which maps the current state to a\ndistribution over actions and is updated throughout the learning\nprocess. Upon taking action at, the environment determines\nthe agent’s next state st+1 via the transition probability kernel\nP(st+1|st, at) and returns the reward rt computed by the\nreward function R(st, at).\nThe RL agent’s goal is to maximize the rewards re-\nceived in the long run. The cumulative reward that the RL\nagent receives after time step t is called return, deﬁned as\nGπ\nt := P∞\nk=0 γkrt+k+1, where the discount factor γ ∈[0, 1]\ndetermines the weight of the future rewards. The value of each\nstate at time t under policy π is called the state-value function\nV π(st), and is deﬁned as the expected return when the agent\nstarts at st and follows the policy π:\nV π(st) = Eπ{Gt|st}.\n(1)\nSimilarly, we can determine the value of a state st and action\nat taken at time t (on the condition that we follow the policy π\nafterwards) using the notion of action-value function Qπ(st, at),\ndeﬁned as\nQπ(st, at) = Eπ{Gt|st, at}.\n(2)\nThe ultimate goal of the RL agent is to learn an effective policy\nthat maximizes the value functions.\nIn the context of RL, a data point in a batch of data is\na sequence of temporally correlated tuples (st, at, rt, st+1)\nthat denote the history of the RL agent’s interaction with the\nenvironment. This sequence of tuples is often referred to as\ntrajectory, and is denoted as\n2\nτT =(s0, a0, r1, s1), (s1, a1, r2, s2), . . .\n(3)\n, (sT −2, aT −2, rT −1, sT −1).\nRL agents do not have knowledge of the environment at\nthe very initial stage of learning and acquire the necessary\nexperience through continued interactions with the environment.\nAn RL agent can acquire the necessary information in two\nways: on-policy and off-policy. In the on-policy setting, the\nagent uses the target policy that is trained so far to obtain\ndata through interaction with the environment. In the off-policy\nsetting, on the other hand, the agent receives an input batch of\ndata in the form of trajectories provided by an exploratory agent\n(i.e. behaviour policy πb), and subsequently uses the acquired\ndata to train the target policy πf (exploitation). The output of\nthe trained target policy consists of data points (trajectories)\nproduced as a result of the interaction between the target\npolicy and the environment (Figure 1). From the privacy point\nof view, since the private data is assumed to exist a priori,\noff-policy methods are natural choices to be analyzed in this\nregard. Figure 1 presents a schematic of off-policy deep RL\narchitecture.\nThe fact that the input trajectories in off-policy deep RL\nmodels are temporally correlated necessitates the use of a\nmechanism that converts the input data to i.i.d. samples before\npassing it to the deep network. A widespread and fundamental\ndata management mechanism that has become an inevitable\npart of the existing off-policy deep RL models is experience\nreplay buffer or replay buffer. The main intuition behind the\napplication of replay buffer in deep RL lies at the heart of\nRL theory. It is well-studied that fundamental RL algorithms\n(e.g. Q-learning) easily diverge in the case of linear function\napproximation [8], [9]. The solution that replay buffer offers to\nthe divergence problem of these algorithms is to decorrelate the\ninput trajectories and subsequently treat each transition tuple as\nan i.i.d. sample point (Figure 1). This intermediate decorrelation\nstep signiﬁcantly improves data efﬁciency and helps the deep\nRL algorithm converge to the optimal policy according to\nthe law of large numbers. Moreover, it allows the deep RL\nalgorithm to beneﬁt from mini-batch training and shufﬂing\ntechniques, which are proven to improve the performance of\ndeep RL algorithms signiﬁcantly [3], [10]–[13].\nMembership Inference Attack\nIn machine learning, a membership inference attack (MIA)\nor tracing attack [14], [15] is a form of adversarial attack that\nis designed to infer the presence of a particular data point\nin the training set of a target model. The central intuition in\nthe design of MIAs is that publicly available trained models\ntend to exhibit higher conﬁdence in their predictions of the\nindividuals who participated in the training data. Consequently,\nthe members of training sets are vulnerable to privacy threats\n[15], [16]. The main challenge for the adversary in MIAs is\nto design a classiﬁer compatible with the target model domain\nsetting and decide whether a particular data point was part\nof the training set given the training target model’s output.\nAttackers employ different MIA design strategies based on: i)\nthe adversary’s knowledge level of the parameters in the target\nmodel (Label-only strategy) and ii) the adversary’s knowledge\nlevel of the training data (Shadow model technique).\nIn the Label-only strategy [17], [18], the attacker only relies\non model predictions and discards the model’s conﬁdence\nscores. In this technique, the attacker uses the generalization\ngap (the difference between the train and test accuracy) in the\nattack model as the main driver in inferring the membership\nof individuals used in training the target model. The label-only\ntechnique was ﬁrst introduced by Yeom et al. [17] and was\nsubsequently extended by Choquette et al. [18] to show how the\nlabel-only technique can improve the existing attack baselines.\nAs the notion of the label is not deﬁned in the general RL\nsetting, the label-only technique cannot be applied here in\ndevising MIAs against RL models.\nShadow model technique [15] is known as an effective and\npractical approach for designing MIA models. Shadow models\nare parallel local models trained on data sets often sampled\nfrom the same distribution as the underlying distribution of the\nprivate data. In this method, the adversary trains the shadow\nmodels with complete knowledge of the training set. Thus,\nusing the auxiliary membership information and the trained\nshadow models, the adversary can build a membership classiﬁer\nthat identiﬁes whether an individual has participated in the\ntraining of similarly trained models.\nIn the training phase in both label-only and shadow model\ntechniques, the adversary should have access to the model\noutput labels and the training data true labels. However, the\nsequential nature of the training and output data points and the\ntemporal nature of model training make the design of MIAs\nfor RL models fundamentally different. Moreover, the presence\nof replay buffer as an inevitable part of off-policy deep RL\nmodels adds another level of complexity to the design of MIAs,\nas this intermediate transformation phase adds a new source\nof noise to the data from the attacker’s perspective.\nRELATED WORK\nMIAs were used for the ﬁrst time against machine learning\nsystems by Shokri et al. [15]. In the following years, extensive\nstudies were performed on the application of MIAs against\nsupervised ( [15]–[17], [19], [20]) and unsupervised ( [21]–\n[23]) machine learning models, surveyed comprehensively by\nHu et al. [24], and Rigaki and Garcia [25]. This section reviews\nthe existing attack models against supervised and unsupervised\nmodels trained on sequential data.\nMIAs have been executed against aggregate location time-\nseries [26]–[28]. For the ﬁrst time, Pyrgelis et al. [28]\nstudied the impact of different spatial-temporal factors that\ncontribute to the vulnerability of time-series-based algorithms\nto MIAs. MIAs have also been studied in the context of text\ngeneration problems [29], [30], where the attacker’s goal is\nto identify whether or not a speciﬁc sequence-to-sequence or\nsequence-to-word pair is part of the input training data of a\nmachine translation engine, a dialogue system or a sentimental\nrecommendation system. The structure of machine learning\n3\nFig. 1. Batch off-policy RL learning architecture. An external behaviour policy generates a batch of trajectories composed of transition tuples (state, action,\nreward, new state). The trajectories are subsequently passed to the deep RL model. The replay buffer mechanism, as an internal part of the model, decorrelates\neach trajectory into a collection of i.i.d. transition tuples and then uses them in the form of mini-batches to train the target policy.\nalgorithms with sequential data differs from that of classic\nclassiﬁcation tasks in the input and prediction types. While\ninputs and outputs in standard classiﬁcation problems have\nﬁxed sizes, they are chains of correlated elements with variable\nlengths in sequence generation tasks. This difference poses a\nfundamentally different approach to designing MIAs against\nsequence generation tasks. The knowledge of output space\ndistribution is no longer valid for the attack classiﬁer since the\noutput length may vary from one model to another. To tackle\nthis challenge, Song and Shmatikov [29] assume access to a\nprobability distribution over output-space vocabularies. They\n[29] split their proposed attack model into two phases, shadow\nmodel training and audit model training. In the shadow model\ntraining phase, the attacker trains multiple shadow models\nassuming that the attacker has access to a generative model\nthat generates a sequence of vocabularies. In the audit training\nphase, the attacker uses the rank of the words produced by\nthe target model instead of the output probability distribution.\nThe central assumption is that the gap observed between the\ntrained model rank predictions depends on word frequencies in\nthe training and test sequences. In a similar study, Hisamoto et\nal. [30] address MIA against sequence-to-sequence models\nin a setting where the adversary is agnostic to the word\nsequence distribution. In their work, the attacker is equipped\nwith a generative model for different translation subcorpora,\nan alternative for output word sequence distribution.\nWhile in models trained on sequential data, the input-output\nrelation is well deﬁned and deterministic, in deep RL models,\nthe output data are generated through the trained policy; thus,\neach output sequence can be considered as evidence for the\nentire input dataset. Therefore, one requires a fundamentally\ndifferent approach in designing MIAs against RL algorithms.\nTo the best of our knowledge, there is no prior work in the\ncontext of deep RL that addresses the problem of membership\ninference at a microscopic level, where the attacker infers the\nmembership of a particular data point in the training set of\ndeep RL models [24], [25].\nMETHODS AND EXPERIMENTS\nIn our proposed adversarial attack framework, we success-\nfully conduct MIA against deep RL in a black-box setting,\nwhere only the model output is accessible to external users.\nThe deep RL model interacts with an environment whose\ndistribution of initial states, state space S and action space A\nare common knowledge, an assumption widely accepted in the\nRL community [31]–[33]. In this section, we ﬁrst explain the\ngeneral setting of the problem and subsequently introduce our\nattack platform and our proposed method of data formatting\nfor training the attack models. We further mention the different\nsettings we have considered in our experimental design. Finally,\nwe provide our choices of performance measures to assess the\nbehaviour of the attack model.\nGeneral Setup\nWe propose an adversarial attack method for studying the\nvulnerability of the deep RL algorithm to MIA in a black-box\nsetting, where the attacker’s access to the model is limited to\nthe output trajectories of the model trained on a private batch\nof input trajectories. Figure 2 depicts the general framework\nof our proposed black-box attack against deep RL algorithms.\nThe two important oracles that should accompany the end-\nto-end design of a black-box attack model in off-policy deep\nRL are i) the data oracle Odata, and ii) the model trainer oracle\nOtrain. The data oracle interacts with the environment and\nreturns a set of independent and identically distributed (i.i.d.)\ntraining trajectories (Markov chains) for the model trainer\noracle Otrain (see Figures 2 (a, b)). The data oracle is a black\nbox which is equipped with a set of unknown exploration\npolicies. To train the target model, whose training input is of\nthe adversary’s interest, the data oracle is initialized privately\n(see Figure 2 (a)), leading to the generation of a batch of\nprivate training data points in the form of trajectories. The\nmodel trainer oracle is agnostic to the exploration policy used\nfor the data collection. The training data batch is passed to\nthe deep RL trainer oracle, and the resulting trained model\n4\nFig. 2. The proposed black-box MIA architecture. (a) Private deep RL model training: the black-box exploration engine (data oracle) interacts with the\nenvironment and provides private training trajectories for the black-box deep RL model trainer. The trained deep RL model is subsequently used to output\ntarget trajectories via interaction with the environment. (b) Shadow training: the data oracle is used to produce non-private training trajectories to be used as\ninput to train the deep RL trainer oracle. The output trajectories are subsequently generated by the trained deep RL model. (c) Training the attack classiﬁer: the\ninput and output trajectories obtained in part (b) are paired together in data formatter to provide positive training pairs. Another set of trajectories, which has\nnot been used in training the shadow model, is used with the output trajectories from part (b) to create negative training pairs. The attack model is subsequently\ntrained using the paired trajectories with the corresponding positive and negative labels. (d) Membership inference attack: the target output trajectories from\npart (a) are paired with sample test trajectories in the data formatter. The trained attack model subsequently uses the pairs to infer the test set trajectories that\nwere used to train the private deep RL model.\n5\nis made publicly available for data query. Our experimental\nframework can adopt any of the existing off-policy batches\ndeep RL models as the deep RL trainer oracle. In this study,\nwe choose to work with the state-of-the-art Batch-Constrained\ndeep Q-learning (BCQ) [34] model, which is widely used as\nthe basis of other deep RL algorithms and exhibits remarkable\nperformance in complex control tasks. Structurally, BCQ trains\na generative model on the input trajectories such that the model\nlearns the relationship between the visited states in the input\ntrajectories and the corresponding actions taken. The BCQ\nalgorithm subsequently uses the developed generative model\nto train a deep Q-network, which ultimately learns to sample\nthe highest-valued actions similar to the ones in the input\ntrajectories.\nWe use the shadow model [15] training technique to acquire\nthe data needed for training the attack classiﬁer. In this method,\nthrough the data oracle, the attacker provides the deep RL\ntrainer oracle with a set of non-private training trajectories\n(Figure 2 (b)), on which the deep RL model is trained. The\nattacker subsequently queries output trajectories from the\ntrained deep RL model and passes the training and output\ntrajectories to the data formatter (Figure 2 (c)). In this step,\nthe training-output trajectories are augmented into pairs and\nare subsequently labelled as 1 in positive pairs and 0 in\nnegative ones depending on whether or not the trajectories\nbelong to the same trained model. Finally, the attack trainer\ntrains a probabilistic classiﬁer that takes as input the pairs of\ntrajectories prepared by the data formatter and returns a trained\nprobabilistic attack classiﬁer that is subsequently used to infer\nthe membership of target input trajectories (Figure 2 (d)).\nSince the attack training data collected by the data oracle\nOdata and prepared by the data formatter is of a sequential\nnature, we need to adopt an attack model that is compatible with\ntime-series data. The classiﬁer should minimize the expected\nloss, deﬁned as\nED [l(AD,θ(., πf), g(.))] ≈\n1\n|D|\nX\nτ∈D\nl(Aθ(τ, πf), g(τ, πf)),\n(4)\nwhere g(.) is the function that assigns labels to the formatted\npairs, A(.) is the parameterized classiﬁer, and l(.) is the loss\nfunction adopted by A. The dataset D contains a set of i.i.d.\ntrajectories drawn from D, and πf denotes the policy trained\non D. The goal of the attacker is to train a classiﬁer that learns\na parameter vector (or network) θ∗that minimizes the loss\nfunction. The following sections provide more details regarding\nthe data formatter and attack classiﬁer.\nExperimental Setup\nIn our experimental design, we study the vulnerability of\nthe deep RL model to MIAs in terms of the following factors:\n1) the membership inference mode (collective vs. individual\nMIA) - In the individual mode, the adversary’s goal is to infer\nthe membership of single training data points (trajectories),\nwhile in the collective mode, the adversary’s target is a batch of\ntrajectories used in the training of the deep RL model. In this\nexperimental setup, we aim to address two scenarios in which\na participant’s identity is revealed. In the individual mode, a\ntrajectory reveals a user’s identity, while in the collective mode,\na collection of trajectories represents the user’s identity.\n2) the maximum trajectory length Tmax within each episode -\nThe value of Tmax is determined and ﬁxed by the environment\nduring data collection and model training. In particular, the RL\nagent’s trajectory in each episode ends when either the agent\narrives at an absorbing state at T < Tmax or the number of time\nsteps T = Tmax. Larger Tmax corresponds to larger values of\nreturn (cumulative reward), thus an improved deep RL policy.\n3) the level of correlation within the input trajectories used\nto train the attack classiﬁer - In the case of individual MIA,\nwe study the performance of our proposed attack classiﬁer in\ntwo modes: 1) correlated mode, where the adversary is trained\non pairs with undisturbed input trajectories, 2) decorrelated\nmode, where in the attack training phase, the input trajectory\nis formed by sampling tuples at random from the whole batch.\nThis set of experiments provides useful information regarding\nthe effect of the correlation level within the input trajectories\non the performance of the attack model.\nBelow is a detailed description of the environments used in\nour experimental design, the data formatting technique, and\nthe attack architecture.\nEnvironments and RL Setting- We assess the algorithm\non OpenAI Gym environments [35] powered by MuJoCo\nphysics engine [36], which are standard tasks adopted by\nmany recent RL studies [37]–[41]. Gym provides a variety of\nsimulated locomotion tasks with different action and state space\ndimensionalities. Here, we train the deep RL agent on three\nhigh-dimensional continuous control tasks: Hopper-v2 (A ⊂R3\nand S ⊂R11), Half Cheetah-v2 (A ⊂R6 and S ⊂R17), and\nAnt-v2 (A ⊂R8 and S ⊂R111). Starting from virtually zero\nknowledge of how each task works, the deep RL model’s goal\nis to teach the Hopper how to hop, the HalfCheetah how to\nrun, and the Ant how to walk as fast as possible. We use the\nDeep Deterministic Policy Gradient (DDPG) algorithm [37] as\nthe data oracle Odata and Batch-Constrained Deep Q-Learning\n(BCQ) [34] as the batch off-policy deep RL method used in\nthe trainer oracle Otrain.\nData Augmentation- Each trajectory starts with an initial\nstate s0 drawn from the available distribution of initial states in\nthe environment, followed by action a0 selected and taken by\nthe RL agent. The environment subsequently takes the agent\nto the next state s1 and returns the reward r1. The agent’s next\nchoice of action is based on s1, and this cycle continues until\nthe trajectory ends at sT . In other words, the initial state s0\nplays a signiﬁcant role in determining the sequence of actions\ntaken by the RL policy and the consequent states and rewards.\nThus, to prepare training pairs to train the attack classiﬁer, we\npair the training and output trajectories that have the same initial\nstates, ﬁxing the starting point of the two trajectories in a pair.\nMoreover, as the RL agent interacts with MDP, the resulting\ntrajectory is a Markov chain, i.e. every state and reward in\nthe trajectory is the direct consequence of the previous state\nand action. Therefore, we choose to remove states and rewards\n6\nFig. 3. The network architecture of TCN (a) and ResNet (b) used in the individual and collective MIAs, respectively.\nfrom the trajectories, keep the actions in the trajectory, and\nuse them in the pairing process.\nEach task is equipped with a set of absorbing states B ∈S.\nAn absorbing state is a state that leads to the termination of an\nagent’s chain of interactions with an environment. Due to the\npresence of absorbing states in the environment, the generated\ntrajectories have different lengths. To pair the training and\noutput action trajectories obtained from the deep RL model, we\nneed to either increase the length of shorter action trajectories\nto match that of the longest one or clip longer action trajectories\nto a pre-determined length. Based on the desired length, we\nchoose to repeat the last action in shorter action trajectories\nfor the required number of times and trim longer trajectories.\nEach action trajectory is a dA × T dimensional array, where\ndA is the dimension of action space, and T is the total number\nof actions in the trajectory. Every output action trajectory is\nconcatenated with a training trajectory such that the resulting\npair is a 2dA×T dimensional array. The pairs are subsequently\npassed to the attack classiﬁers in multi-dimensional arrays\nR2dA×T and R2dA×T ×m in individual and collective modes,\nrespectively. The value m refers to the number of pairs in each\nbatch in the collective mode.\nAttack Classiﬁer Architecture- We use Temporal Convo-\nlutional Networks (TCNs) [42] as the classiﬁer for individual\nMIA, and Residual Network (ResNet) [43] deep architecture\nfor collective MIA. Figure 3 shows a schematic of TCN (Figure\n3(a)) and ResNet (Figure 3(b)) architectures.\nIndividual-Mode Attack Classiﬁer Architecture-\nAs both\ntraining and output trajectories of RL models are composed\nof temporally correlated transition tuples, the choice of attack\nclassiﬁer must utilize the input-level temporal correlation in\nits feature representation. TCNs are structurally designed to\nutilize the inherent temporal correlation in the training data\nthrough a hierarchy of temporal convolutions architecture. In\nthis regard, TCN employs a 1D fully-convolutional network\n(FCN) architecture [44], where each of its hidden layers has\nthe same length as the input layer (Figure 3(a)). The main\nadvantage of TCN is its ability to use dilation in convolution\nlayers to keep the long-range temporal dependency and increase\nthe receptive ﬁeld of the convolutional layers. In the individual\nMIA mode, since the input data to the classiﬁer is a pair of\ntemporally correlated tensors (i.e. R2dA×T ), the long-range\ncorrelation between input tuples within each trajectory is well-\naligned with the input structure of TCNs. For more information\non the internal structure of TCN architecture, refer to the\nAppendix.\nCollective-Mode Attack Classiﬁer Architecture- In this case,\nwhile more information is accessible to the attacker, it requires\na more complex learning architecture and more sophisticated\nhyper-parameter tuning to exploit the cross-correlation among\nthe training trajectories and the temporal correlation within a\ntrajectory. In the collective mode, our input is in the form\nof three-dimensional tensor (e.g. R2dA×T ×m). Unlike the\nindividual MIA mode, which involves 2-dimensional inputs, in\nthe collective MIA mode, we have another dimension m for\nthe number of trajectories in each batch of trajectories, similar\nto the data structure used in image classiﬁcation problems [43],\n[45], [46]. Thus, we use the deep residual network (ResNet)\narchitecture [43] because of its inherent compatibility with data\nsets with temporally deep structures. ResNets are popular for\nsolving standard computer vision problems [43], [47], [48].\nPerformance Metrics\nWe adopt the standard performance metrics used in the\nclassiﬁcation literature [49] to evaluate the performance of our\nproposed attack models. We measure the performance of the\nattack classiﬁer with the following metrics:\nOverall accuracy (ACC), which captures the overall perfor-\n7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nACC\nIndividual\nCollective\n(a) Hopper-v2\nTmax\nIndividual\nCollective\n10\n50\n100\n200\n500\nMCC\nIndividual\nCollective\n(b) HalfCheetah-v2\n10\n30\n50\n100\n200\nIndividual\nCollective\n10\n30\n50\n100\n200\nTmax\n(c) Ant-v2\nIndividual\nCollective\n10\n50\n100\n200\nTmax\nIndividual\nCollective\nFig. 4. The performance of the attack classiﬁers in tasks Hopper-v2 (a), HalfCheetah-v2 (b), and Ant-v2 (c) in individual and collective attack modes. Each\ndata point is determined from the average result of 5 separate runs. The error bars depict the error on the mean for ACC (top) and MCC (bottom) in the\ncorresponding runs. The batch size m = 50 in the collective mode.\nmance of the attack classiﬁer and is calculated as follows,\nACC =\nTP + TN\nTP + TN + FP + FN,\n(5)\nwhere TP (true positives) denotes the number of correctly\nrecognized positives, and TN (true negatives) shows the number\nof correctly recognized negative ones. The two other quantities,\nfalse positives FP and false negatives FN indicate the number\nof incorrectly recognized positives and negatives, respectively.\nPrecision (PR), which shows the fraction of pairs classiﬁed\nas matching pairs that are indeed coming from the same model,\nand is written as, PR = TP/TP + FP.\nRecall (RE), which measures the fraction of matching pairs\nthat the attack classiﬁer can infer correctly, and is computed\nas, RE = TP/TP + FN.\nF1 score (F1), which is the harmonic mean of the pre-\ncision (PR) and recall (RE), and is calculated as\nF1 =\n(2.PR.RE)/(PR + RE).\nMatthews Correlation Coefﬁcient (MCC) [50], which cal-\nculates the correlation between the predicted and the true\nclassiﬁcation labels, and is deﬁned as,\nMCC =\nTP.TN −FP.FN\np\n(TP + FP)(TP + FN)(TN + FP)(TN + FN)\n. (6)\nMCC is an effective and meaningful combination of all\nfour quantities TP, TN, FP, and FN, and ranges from −1 to 1.\nThe closer MCC is to 1, the better the model performs [51].\nMCC= 0 shows that the model is a random guesser. The other\nevaluation metrics ACC, PR, RE, and F1 vary in the [0, 1] range.\nIn a well-performing model, all of these evaluation metrics\nhave values close to 1. Finally, to show the performance of our\nproposed MIA classiﬁers in individual and collective modes at\ndifferent classiﬁcation thresholds θ, we plot receiver operating\ncharacteristic (ROC) curve, which shows the changes of recall\nRE as a function of False Positive Rate FPR = FP/(FP + TN)\nfor different values of θ.\nRESULTS AND DISCUSSION\nThis section presents and discusses the results of different\nexperimental scenarios to capture the interdependence between\ndifferent parameters that affect the accuracy of membership\ninference in deep RL settings.\nCollective vs. Individual MIAs\nUsing different classiﬁcation metrics, we assess the behaviour\nof TCN and ResNet attack classiﬁers in predicting the mem-\nbership probability of individual and collective data points,\nrespectively. Figure 4 presents the performance of the classiﬁers\nTCN and ResNet in Hopper-v2 (Figure 4(a)), HalfCheetah-v2\n(Figure 4(b)), and Ant-v2 (Figure 4(c)) in terms of ACC and\nMCC for different maximum trajectory lengths Tmax. The full\nreport of their performance in these three tasks is provided in\nTables 1-3 in the Appendix (Section B). The results show that\nour proposed attack framework can infer the RL model training\ndata points with high accuracy (e.g. > 0.8 in the individual and\n> 0.9 in the collective mode for Tmax ≥100 in Hopper-v2),\nindicating a high risk of privacy invasion. Moreover, the results\nreveal that for a ﬁxed Tmax, the adversary infers collective data\npoints with signiﬁcantly higher accuracy than the accuracy\nvalue in the individual mode. For example, in the Hopper-v2\ntask, the membership inference accuracies in the collective\nmode for Tmax are more than 12% higher than those in the\nindividual mode. This observation shows that the deep RL\nalgorithm is more vulnerable to MIA in the collective mode,\nwhich is expected since more information is provided to the\nattack classiﬁer through a batch of data points instead of one.\n8\nFig. 5. The MIA accuracy in Hopper-v2 (a), HalfCheetah-v2 (b), and Ant-v2 (c) in the collective attack mode for different batch sizes. Each data point is\ndetermined from the average result of 5 separate runs. The error bars depict the error on the mean. The maximum trajectory length Tmax = 100.\nIn particular, in the collective mode, the adversary can capture\nthe collective properties of the training data points and their\nrelationship with the output trajectories, which could be veiled\nin one individual trajectory.\nTo further study the effect of batch size on the performance\nof MIA in the collective mode, we conduct MIAs against the\ndeep RL agent for different batch sizes (Figure 5). A closer\nanalysis of the two ﬁgures (Figure 4 and Figure 5) reveals that\nwhile larger batch sizes correspond to higher level of deep RL\ntraining members’ vulnerability to MIA, batch sizes m ≤5 in\nHopper-v2, m < 25 in HalfCheetah-v2, and m ≤10 in Ant-v2\nlead to smaller values of inference accuracy compared with\nthose in the individual mode. We believe that this difference\nin the performance of the adversary between the two cases\ncorresponds to the different structures used in the individual\nand collective modes (i.e. TCN and ResNet). In particular, our\nresults show that the effectiveness of the ResNet classiﬁer in\ninferring the membership of the data points surpasses that of\nTCN in larger batch sizes.\nThe Impact of Tmax\nWe test the performance of attack classiﬁers against the\ntarget model for different values of Tmax in a set of experiments.\nAs the environment is unvarying, the value of Tmax remains\nunchanged throughout each experiment. Our observations\npresented in Figure 4 show that as Tmax increases, the accuracy\nACC of the attack classiﬁers in inferring the training data points\nin both individual and collective modes improves. Moreover,\nour results show consistent improvement of MCC as a function\nof Tmax in all three environments Hopper-v2, Half Cheetah-v2,\nand Ant-v2, which is consistent with the changes in ACC. Note\nthat as MCC utilizes all four values in the confusion matrix, it\nprovides a more reliable and robust measure compared to the\nother metrics (for additional results and comparison, refer to\nthe tables provided in the Appendix).\nMaximum trajectory length Tmax plays a signiﬁcant role in\nthe performance of deep RL models. Figure 6 illustrates the\nlearning curves for the deep RL model in Hopper-v2 (Figure\n6(a)), HalfCheetah-v2 (Figure 6(b)), and Ant-v2 (Figure 6(c))\nfor different values of Tmax. The plots show that as Tmax\nincreases, the deep RL policy presents a consistently improved\nbehaviour. As RL policy is a function that maps the visited\nstates to the selected actions, a closer deep RL policy to the\noptimal policy corresponds to a more predictable relationship\nbetween the training and the output trajectories. We argue that\nthis feature of deep RL policies contributes to the higher level\nof vulnerability of the deep RL models that are trained with\nlarger values of Tmax.\nAs the attack classiﬁers output membership probabilities, we\ndetermine the predicted binary label for a range of acceptance\nthresholds θ = 0.1, 0.2, . . . , 0.9, and subsequently choose\nthe threshold θ, at which the classiﬁer shows the highest\nFig. 6. Deep RL curves in three high-dimensional locomotion tasks Hopper-v2 (a), HalfCheetah-v2 (b), and Ant-v2 (c). The graphs depict the performance of\nthe deep RL model as a function of time for different maximum trajectory lengths Tmax. The plots are averaged over 5 random seeds. The performance of the\ndeep RL policy is assessed every 5000 step over 1000000 time steps.\n9\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRE\n(a) Individual\nTmax = 50\nTmax = 100\nTmax = 200\nRandom Guesser\n(b) Collective\nFPR\nFPR\nTmax = 50\nTmax = 100\nTmax = 200\nRandom Guesser\nFig. 7. The receiver operator characteristic (ROC) curves of the MIA in HalfCheetah-v2 in the individual (a) and collective (b) modes for different values of\nTmax.\nperformance. Figure 7 depicts the sample ROC curves for\nHalfCheetah-v2 in individual (Figure 7(a)) and collective\n(Figure 7(b)) modes. The plots show that larger values of\nTmax lead to better performance of the attack classiﬁers. The\nbest result is obtained at Tmax = 200 in both individual\nand collective modes. We ﬁnd that the acceptance threshold\nθ = 0.5 yields the highest performance throughout all of our\nexperiments.\nTemporal Correlation\nThe results presented so far exhibit the performance of\nthe MIAs against deep RL as a result of training the attack\nclassiﬁers on the temporally correlated data collected from the\ntraining set and the output of the deep RL model. Considering\nthat the training trajectories are decorrelated in the replay buffer\nas the ﬁrst step after entering the deep RL trainer oracle, a\nquestion arises as to what role the temporal correlation in the\ndata set plays in the vulnerability of deep RL models to MIAs.\nTo answer this question, we have performed a set of\nexperiments, where prior to the data augmentation phase, the\ntemporal correlation between the deep RL training trajectories\nis broken. In particular, we decorrelate the training trajectories\nby shufﬂing their constituent tuples. We subsequently store\nthe decorrelated transition tuples in an auxiliary buffer. In the\nnext step, we generate trajectories of the desired length by\nsampling actions uniformly at random from the buffer. Finally,\nwe pass the collection of decorrelated training trajectories\ntogether with output trajectories to the data augmentation\nmechanism and train the attack classiﬁers with the paired\ntrajectories in the individual and collective modes. Figure 8\ncompares the accuracy of the MIA in the correlated mode with\nthat in the decorrelated mode for the three tasks. The plots\ndepict that the adversary’s accuracy in inferring RL training\nmembers decreases signiﬁcantly upon decorrelating the training\ntrajectories. The results show that despite the inevitable input\ndecorrelation imposed by the replay buffer mechanism in the\ntraining phase of off-policy deep RL models, the temporal\ncorrelation in the training trajectories is channelled to the\nmodel output data points. Thus, the attack classiﬁers trained\non temporally correlated training data points exhibit higher\naccuracy than those trained on decorrelated trajectories.\nIII. CONCLUSION\nIn this study, we design and evaluate the ﬁrst membership\ninference attack (MIA) framework against off-policy deep\nRL in collective and individual membership inference modes\nby exploiting the inherent and structural temporal correla-\ntion present in deep RL data points. We demonstrate the\nperformance of the proposed adversarial attack framework\nin complex high-dimensional locomotion tasks for different\nIndividual - Correlated\nCollective - Correlated\nIndividual - Decorrelated\nCollective - Decorrelated\n0.4\n0.6\n0.8\n1.0\nTmax\nACC\n10\n50\n100\n200\n500\n(a) Hopper-v2\nTmax\n10\n30\n50\n100\n200\n(b) HalfCheetah-v2\n(c) Ant-v2\nTmax\n10\n50\n100\n200\nFig. 8. Comparison of the MIA accuracy between correlated and decorrelated settings for Hopper-v2 (a), HalfCheetah-v2 (b), and Ant-v2 (c).\n10\nmaximum trajectory lengths. Our proposed attack framework\nreveals the substantial vulnerability of a state-of-the-art off-\npolicy deep RL model to the black-box MIAs. We show that\nit is signiﬁcantly more vulnerable to MIA in the collective\nsetting when compared to its vulnerability in individual MIAs.\nMoreover, our results demonstrate a consistent increase in the\naccuracy of the membership inference as a function of batch\nsize in the collective mode. Furthermore, our experimental\nresults reveal that the maximum trajectory length (in the\nepisodic RL setting), which is set by the environment, plays a\nsigniﬁcant role in the vulnerability of the training data used\nin the deep RL model to the MIA. We show that a longer\nmaximum trajectory length leads to an improved deep RL\npolicy, thus a more deﬁned relationship between the training\nand output trajectories, and consequently less private training\ndata. Moreover, our results reveal the determinative role of\ntemporal correlation in obtaining high MIA performance, which\nthe attacker can utilize to design high-accuracy MIAs against\ndeep RL. Despite the existence of replay memory as an\nintermediate data decorrelation mechanism at the heart of deep\nRL models, the trained policy still fully exploits the inherent\ncorrelation in learning feature representation, which poses a\nsigniﬁcant privacy concern in the deployment of trained RL\npolicies at the industrial scale. Finally, the results from this\nstudy highlight serious privacy concerns in the widespread\ndeployment of similar deep RL models, which demand more\ninvestigation of this matter to offer solutions in future studies.\nThe tasks employed in the current study are under the umbrella\nof robotics simulation tasks that motivate the extension of\nexperiments to real-world robot learning tasks. Moreover,\ndialogue systems such as Amazon Alexa, Apple Siri, and\nGoogle Assistant are other interesting future platforms to apply\nRL-based MIAs on. In virtual dialogue systems, a data point is\npresented by a collection of interaction trajectories between the\nchatbot and the end user. A chatbot in this setting is the trained\nRL policy, and the user interactions with the bot form the\ntraining trajectories. In such settings, the collective mode is the\nnatural inference setting since a collection of user interactions\nwith the bot represents individual identity in the training set.\nIn other words, the user’s presence in the training set can be\ninferred by the adversary if and only if the attacker correctly\ninfers a batch of trajectories representing the individual in\nthe training set. Another extension to this line of research is\nto investigate MIAs against Deep RL models in a white-box\nsetting, where the internal structure of the target policy is also\nknown to the adversary.\nACKNOWLEDGEMENTS\nThe authors would like to thank Hamidreza Ghafghazi and\nSpencer Main for their valuable contribution to the design\nand development of the preliminary version of the codebase.\nComputing resources were provided by Compute Canada,\nCalcul Qu´ebec, and VIP Lab at the University of Waterloo\nthroughout the project, which the authors appreciate. Funding\nwas provided by the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC).\n11\nREFERENCES\n[1] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger,\nK. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko et al., “Highly\naccurate protein structure prediction with alphafold,” Nature, vol. 596,\nno. 7873, pp. 583–589, 2021.\n[2] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian,\nT. J. Walsh, R. Capobianco, A. Devlic, F. Eckert, F. Fuchs et al.,\n“Outracing champion gran turismo drivers with deep reinforcement\nlearning,” Nature, vol. 602, no. 7896, pp. 223–228, 2022.\n[3] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering the\ngame of go without human knowledge,” nature, vol. 550, no. 7676, pp.\n354–359, 2017.\n[4] X. Pan, W. Wang, X. Zhang, B. Li, J. Yi, and D. Song, “How you act\ntells a lot: Privacy-leakage attack on deep reinforcement learning,” arXiv\npreprint arXiv:1904.11082, 2019.\n[5] A. Gleave, M. Dennis, N. Kant, C. Wild, S. Levine, and S. Russsell,\n“Adversarial policies: Attacking deep reinforcement learning,” In Proc.\nICLR-20, 2020.\n[6] X. Wu, W. Guo, H. Wei, and X. Xing, “Adversarial policy training against\ndeep reinforcement learning,” in 30th USENIX Security Symposium\n(USENIX Security 21), 2021, pp. 1883–1900.\n[7] B. Mavrin, H. Yao, and L. Kong, “Deep reinforcement learning with\ndecorrelation,” arXiv preprint arXiv:1903.07765, 2019.\n[8] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 1998.\n[9] M. Fairbank and E. Alonso, “The divergence of reinforcement learning\nalgorithms with value-iteration and function approximation,” in The 2012\nInternational Joint Conference on Neural Networks (IJCNN).\nIEEE,\n2012, pp. 1–8.\n[10] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al.,\n“Human-level control through deep reinforcement learning,” nature, vol.\n518, no. 7540, pp. 529–533, 2015.\n[11] S. Zhang and R. S. Sutton, “A deeper look at experience replay,” arXiv\npreprint arXiv:1712.01275, 2017.\n[12] R. Liu and J. Zou, “The effects of memory replay in reinforcement\nlearning,” in 2018 56th Annual Allerton Conference on Communication,\nControl, and Computing (Allerton).\nIEEE, 2018, pp. 478–485.\n[13] W. Fedus, P. Ramachandran, R. Agarwal, Y. Bengio, H. Larochelle,\nM. Rowland, and W. Dabney, “Revisiting fundamentals of experience\nreplay,” in International Conference on Machine Learning.\nPMLR,\n2020, pp. 3061–3071.\n[14] C. Dwork, A. Smith, T. Steinke, and J. Ullman, “Exposed! a survey of\nattacks on private data,” Annual Review of Statistics and Its Application,\nvol. 4, pp. 61–84, 2017.\n[15] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership\ninference attacks against machine learning models,” in 2017 IEEE\nSymposium on Security and Privacy (SP).\nIEEE, 2017, pp. 3–18.\n[16] A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes,\n“Ml-leaks: Model and data independent membership inference attacks\nand defenses on machine learning models,” in NCSS, 2019.\n[17] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in\nmachine learning: Analyzing the connection to overﬁtting,” in 2018\nIEEE 31st Computer Security Foundations Symposium (CSF).\nIEEE,\n2018, pp. 268–282.\n[18] C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot, “Label-\nonly membership inference attacks,” in International Conference on\nMachine Learning.\nPMLR, 2021, pp. 1964–1974.\n[19] Y. Long, L. Wang, D. Bu, V. Bindschaedler, X. Wang, H. Tang, C. A.\nGunter, and K. Chen, “A pragmatic approach to membership inferences\non machine learning models,” in 2020 IEEE European Symposium on\nSecurity and Privacy (EuroS P), 2020, pp. 521–534.\n[20] L. Song and P. Mittal, “Systematic evaluation of privacy risks of machine\nlearning models,” in 30th {USENIX} Security Symposium ({USENIX}\nSecurity 21), 2021.\n[21] J. Hayes, L. Melis, G. Danezis, and E. D. Cristofaro, “Logan:\nMembership inference attacks against generative models,” Proceedings\non Privacy Enhancing Technologies, vol. 2019, no. 1, pp. 133–152,\n2019. [Online]. Available: https://doi.org/10.2478/popets-2019-0008\n[22] B. Hilprecht, M. H¨arterich, and D. Bernau, “Monte carlo and reconstruc-\ntion membership inference attacks against generative models.” Proc. Priv.\nEnhancing Technol., vol. 2019, no. 4, pp. 232–249, 2019.\n[23] D. Chen, N. Yu, Y. Zhang, and M. Fritz, “Gan-leaks: A taxonomy of\nmembership inference attacks against generative models,” in Proceedings\nof the 2020 ACM SIGSAC conference on computer and communications\nsecurity, 2020, pp. 343–362.\n[24] H. Hu, Z. Salcic, G. Dobbie, and X. Zhang, “Membership inference\nattacks on machine learning: A survey,” arXiv preprint arXiv:2103.07853,\n2021.\n[25] M. Rigaki and S. Garcia, “A survey of privacy attacks in machine\nlearning,” arXiv preprint arXiv:2007.07646, 2020.\n[26] A. Pyrgelis, C. Troncoso, and E. De Cristofaro, “What does the crowd say\nabout you? evaluating aggregation-based location privacy,” Proceedings\non Privacy Enhancing Technologies, vol. 4, pp. 76–96, 2017.\n[27] A. Pyrgelis, C. Troncoso, and E. D. Cristofaro, “Knock knock, who’s\nthere? membership inference on aggregate location data,” in 25th Annual\nNetwork and Distributed System Security Symposium, NDSS 2018, San\nDiego, California, USA, February 18-21, 2018.\nThe Internet Society,\n2018.\n[28] A. Pyrgelis, C. Troncoso, and E. De Cristofaro, “Measuring membership\nprivacy on aggregate location time-series,” Proceedings of the ACM on\nMeasurement and Analysis of Computing Systems, vol. 4, no. 2, pp. 1–28,\n2020.\n[29] C. Song and V. Shmatikov, “Auditing data provenance in text-generation\nmodels,” in Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 2019, pp. 196–206.\n[30] S. Hisamoto, M. Post, and K. Duh, “Membership inference attacks on\nsequence-to-sequence models: Is my data in your machine translation\nsystem?” Transactions of the Association for Computational Linguistics,\nvol. 8, pp. 49–63, 2020.\n[31] R. S. Sutton, “Temporal credit assignment in reinforcement learning,”\nPh.D. dissertation, University of Massachusetts Amherst, 1984.\n[32] G. Vietri, B. Balle, A. Krishnamurthy, and S. Wu, “Private reinforcement\nlearning with pac and regret guarantees,” in International Conference on\nMachine Learning.\nPMLR, 2020, pp. 9754–9764.\n[33] C. Szepesv´ari, Algorithms for reinforcement learning.\nMorgan &\nClaypool Publishers, 2010.\n[34] S. Fujimoto, D. Meger, and D. Precup, “Off-policy deep reinforcement\nlearning without exploration,” in International Conference on Machine\nLearning.\nPMLR, 2019, pp. 2052–2062.\n[35] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-\nman, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint\narXiv:1606.01540, 2016.\n[36] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-\nbased control,” in Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ\nInternational Conference on.\nIEEE, 2012, pp. 5026–5033.\n[37] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,\nand D. Wierstra, “Continuous control with deep reinforcement learning,”\narXiv preprint arXiv:1509.02971, 2015.\n[38] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International Conference on Machine Learning, 2018, pp.\n1861–1870.\n[39] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation\nerror in actor-critic methods,” in International Conference on Machine\nLearning, 2018, pp. 1587–1596.\n[40] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger,\n“Deep reinforcement learning that matters,” in Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[41] V. Franc¸ois-Lavet, P. Henderson, R. Islam, M. G. Bellemare, and\nJ. Pineau, “An introduction to deep reinforcement learning,” arXiv\npreprint arXiv:1811.12560, 2018.\n[42] S. Bai, J. Z. Kolter, and V. Koltun, “An empirical evaluation of generic\nconvolutional and recurrent networks for sequence modeling,” arXiv\npreprint arXiv:1803.01271, 2018.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[44] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015, pp. 3431–3440.\n[45] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[46] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2017, pp. 4700–4708.\n12\n[47] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and\nD. Terzopoulos, “Image segmentation using deep learning: A survey,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n[48] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep\nlearning: A review,” IEEE transactions on neural networks and learning\nsystems, vol. 30, no. 11, pp. 3212–3232, 2019.\n[49] M. Sokolova and G. Lapalme, “A systematic analysis of performance\nmeasures for classiﬁcation tasks,” Information processing & management,\nvol. 45, no. 4, pp. 427–437, 2009.\n[50] B. W. Matthews, “Comparison of the predicted and observed secondary\nstructure of t4 phage lysozyme,” Biochimica et Biophysica Acta (BBA)-\nProtein Structure, vol. 405, no. 2, pp. 442–451, 1975.\n[51] D. Chicco, N. T¨otsch, and G. Jurman, “The matthews correlation\ncoefﬁcient (mcc) is more reliable than balanced accuracy, bookmaker\ninformedness, and markedness in two-class confusion matrix evaluation,”\nBioData mining, vol. 14, no. 1, pp. 1–22, 2021.\n[52] D.\nP.\nKingma\nand\nJ.\nBa,\n“Adam:\nA\nmethod\nfor\nstochastic\noptimization,” CoRR, vol. abs/1412.6980, 2014. [Online]. Available:\nhttp://arxiv.org/abs/1412.6980\n13\nAPPENDIX\nIn Section A, we provide additional information regarding the attack network architectures we use to design attack classiﬁer\nin the individual and collective modes. In Section B, we present the detailed results on the performance of the two attack\nclassiﬁers in three standard continuous control locomotion MuJoCo tasks Hopper-v2 (Table I), HalfCheetah-v2 (Table II), and\nAnt-v2 (Table III).\nA. NETWORK ARCHITECTURES\nIndividual-Mode Attack Classiﬁer Architecture- In the individual mode, we use Temporal Convolutional Network (TCN)\narchitecture [42], which takes temporally correlated data as input and uses a hierarchy of dilated (causal) convolutional\nlayers to capture the inherent temporal correlation in the input data. In this study, since the input data to the classiﬁer in\nthe individual mode is a pair of temporally correlated tensors (i.e. R2dA×T ), the size of input channel in TCN architecture\nrepresents twice the dimension of the action space. In the design of the TCN architecture, we use a two-layer network\nand set the number of hidden layers to 600. We set the kernel size to 3 and use dropout with a threshold of 0.45. For\nnetwork training, we employ Adam optimizer [52] with the initial learning rate set at 0.0003 and gradient clipping at\n0.35 to avoid exploding gradients. We change the initial learning rate during the training phase using the learning rate\nscheduler to avoid local minimum. In this regard, we use a learning rate scheduler at 100 and 200 epochs with gamma\nrate 0.1 to decay the learning rate with time. The batch size is set to 16 during training, and the network is trained for 300 epochs.\nCollective-Mode Attack Classiﬁer Architecture- In the collective mode, we use deep Residual Networks (ResNets) as the\nchoice of attack classiﬁer. In particular, we employ ResNet-18 architecture, where in addition to an input channel, there is a\nwidth dimension, which we use to input a collection of trajectories as a batch. Our input is in the form of a three-dimensional\ntensor (e.g. R2dA×T ×m), where m denotes the number of trajectories in each batch, similar to the data structure used in image\nclassiﬁcation problems [43], [45], [46]. Moreover, we use Adam optimizer with learning rate 0.0008 and weight decay 1.0, and\nwe set the clipping size to 0.35. Finally, we use batch size 256 and train the network for 300 epochs.\nB. ATTACK PERFORMANCE\nIn Table I, we show the results obtained for the attack performance on the deep RL algorithm in the three MuJoCo\nenvironments in terms of ﬁve performance metrics Accuracy, Precision, recall, F1 score, and Matthews correlation coefﬁcient.\nTABLE I\nTHE PERFORMANCE OF THE ATTACK CLASSIFIERS IN HOPPER-V2 FOR DIFFERENT MAXIMUM TRAJECTORY LENGTHS TMAX IN TERMS OF ACCURACY\n(ACC), PRECISION (PR), RECALL (RE), F1 SCORE (F1), AND MATTHEWS CORRELATION COEFFICIENT (MCC). THE VALUES IN PARENTHESES SHOW THE\nRESULTS FOR THE COLLECTIVE ATTACK MODE.\n14\nTABLE II\nTHE MIA PERFORMANCE RESULTS FOR INDIVIDUAL AND COLLECTIVE MODES IN HALFCHEETAH-V2 FOR DIFFERENT MAXIMUM TRAJECTORY LENGTHS\nTMAX IN TERMS OF ACCURACY (ACC), PRECISION (PR), RECALL (RE), F1 SCORE (F1), AND MATTHEWS CORRELATION COEFFICIENT (MCC). THE\nVALUES IN PARENTHESES PRESENT THE RESULTS FOR THE COLLECTIVE ATTACK MODE.\nTABLE III\nTHE PERFORMANCE OF THE ATTACK CLASSIFIERS IN ANT-V2 FOR DIFFERENT MAXIMUM TRAJECTORY LENGTHS TMAX IN TERMS OF ACCURACY (ACC),\nPRECISION (PR), RECALL (RE), F1 SCORE (F1), AND MATTHEWS CORRELATION COEFFICIENT (MCC). THE VALUES IN PARENTHESES SHOW THE RESULTS\nFOR THE COLLECTIVE ATTACK MODE.\n15\n",
  "categories": [
    "cs.LG",
    "cs.CR"
  ],
  "published": "2021-09-08",
  "updated": "2022-11-16"
}