{
  "id": "http://arxiv.org/abs/2109.04081v1",
  "title": "DeepEMO: Deep Learning for Speech Emotion Recognition",
  "authors": [
    "Enkhtogtokh Togootogtokh",
    "Christian Klasen"
  ],
  "abstract": "We proposed the industry level deep learning approach for speech emotion\nrecognition task. In industry, carefully proposed deep transfer learning\ntechnology shows real results due to mostly low amount of training data\navailability, machine training cost, and specialized learning on dedicated AI\ntasks. The proposed speech recognition framework, called DeepEMO, consists of\ntwo main pipelines such that preprocessing to extract efficient main features\nand deep transfer learning model to train and recognize. Main source code is in\nhttps://github.com/enkhtogtokh/deepemo repository",
  "text": "DeepEMO: Deep Learning for Speech Emotion\nRecognition\nEnkhtogtokh Togootogtokh\nTechnidoo Solutions Lab\nTechnidoo Solutions Germany and Mongolian University of Science and Technology\nBavaria, Germany\nenkhtogtokh.java@gmail.com, togootogtokh@technidoo.com\nChristian Klasen\nTechnidoo Solutions Lab\nTechnidoo Solutions Germany\nBavaria, Germany\nklasen@technidoo.com\nAbstract—We proposed the industry level deep learning ap-\nproach for speech emotion recognition task. In industry, care-\nfully proposed deep transfer learning technology shows real\nresults due to mostly low amount of training data availability,\nmachine training cost, and specialized learning on dedicated\nAI tasks. The proposed speech recognition framework, called\nDeepEMO, consists of two main pipelines such that prepro-\ncessing to extract efﬁcient main features and deep transfer\nlearning model to train and recognize. Main source code is in\nhttps://github.com/enkhtogtokh/deepemo repository.\nIndex Terms—Speech Emotion Recognition, Deep learning for\nSpeech Emotion Recognition, DeepEMO, Emotion Recognition\nI. INTRODUCTION\nAbility to understand and manage emotions, called emo-\ntional intelligence, has been shown to play an important role\nin decision-making. Some researchers suggest that emotional\nintelligence can be learned and strengthened it refers to the\nability to perceive, control, and evaluate emotions. For ma-\nchine intelligence, it is also important role to understand and\neven generate such emotional intelligence. Here we proposed\nthe simple yet effective speech emotional recognition modern\ndeep learning technique called DeepEMO framework. It has\nbeen conceived considering the main AI pipeline (from sen-\nsors to results) together with modern technology trends. The\nDeepEMO has two main pipelines which are to extract strong\nspeech features and deep transfer learning for the emotion\nrecognition task. We applied them on english emotional speech\ncase. Generally it is possible to apply them on any natural\nlanguage. There are inevitable demands to recognize the\nspeech emotion with advanced technology.\nConcretely, the key contributions of the proposed work are:\n• The industry level AI technology for speech emotion\nrecognition\n• The speech recognition general modern deep learning\nframework for similar tasks\nSystematic experiments conducted on real-world acquired\ndata have shown as:\n• It is possible to be common framework for many type of\nspeech recognition task.\n• It is possible to achieve 99.9% accuracy on well prepared\ntraining data to recognize.\n• It is possible to later generate realistic enough synthetic\nemotional speech data generation with multiple variations\nThe rest of the paper is organized as follows. The proposed\nframework is described in Section II. The recognition deep\nconvolutional model is explained in Section II-B. The details\nabout the experimental results are presented in Section III.\nFinally, Section IV provides the conclusions and future work.\nII. THE PROPOSED METHOD (DEEPEMO)\nFig. 1. The DeepEMO framework\nIn this section, we discuss the proposed DeepEMO model\nfor speech emotion recognition AI applications as shown in\nFigure 1. The DeepEMO has two main pipelines which are\nthe preprocessing to extract efﬁcient features and deep transfer\nlearning mechanism. We discuss them in detail with coming\nsections.\nA. The preprocessing feature extraction\nAudio signiﬁcant feature extraction is the important part of\nmodern deep learning. There are many mechanisms to do it.\nHere we extract melspectrogram audio feature later to train\nmachine with high accuracy.\nSpeciﬁcally, it consists of following general steps:\n• To compute fast Fourier transform (FFT)\n• To generate mel scale\n• To generate spectrogram\nThe FFT is an algorithm which efﬁciently computes the\nFourier transform. The Fourier transform is a mathematical\nformula which decomposes a signal into it’s individual fre-\nquencies and the frequency’s amplitude. In other words, it\narXiv:2109.04081v1  [cs.SD]  9 Sep 2021\nconverts the signal from the time domain into the frequency\ndomain. The result is called a spectrum. The mathematical\noperation converts frequencies to the mel scale. Researchers\nproposed a unit of pitch such that equal distances in pitch\nsounded equally distant to the listener, which is called the\nmel scale. When signal’s frequency content varies over time\nas non periodic signals, it needs a right representation. As\nwe can compute several spectrums by performing FFT on\nseveral windowed segments. It is called the short-time Fourier\ntransform. The FFT is computed on overlapping windowed\nsegments of the signal, which is called the spectrogram.\nB. The deep transfer learning\nSpeciﬁcally, we deﬁne the transfer learning model by [1]:\n• To prepare the pre-trained model\n• To re-deﬁne the last output layer as n (in case, n=8)\nneurons layer for the new task\n• To train the network\nThis is called transfer learning, i.e. we have a model trained\non another task, and we need to tune it for the new dataset\nwe have in hand.\nTo recognize speech emotion, we propose the deep convo-\nlutional transfer neural network. Since after melspectrogram\nfeature extraction preprocessing, it is now generally computer\nvision problem. The deep convolutional backbone model is\nResNet18 [2] which consists of assemble of convolutional lay-\ners and batch norms as shown in Algorithm 1. For simplicity,\nthe Pytorch [3] style pseudo code is provided. Cross Entropy\nLoss function (CE) and Adam optimize implemented for the\nmodel.\nAlgorithm 1: Transfer Learning Model\n1: import torch\n2: import torch.nn as nn\n3: import torchvision\n4: model = torchvision.models.resnet18(pretrained =\nTrue)\n5: loss = torch.nn.CrossEntropyLoss()\n6: model.fc = torch.nn.Linear(infeatures =\n512, outfeatures = 8)\n7: optimizer =\ntorch.optim.Adam(model.parameters(), lr = 3e −5)\nIII. EXPERIMENTAL RESULTS\nIn this section, we discuss ﬁrst about the setup, and then\nevaluate the deep transfer learning recognition and melspec-\ntrogram generation results are experimented in systematic\nscenarios.\nA. Setup\nWe train and test on ubuntu 18 machine with capacity of\n(CPU: Intel(R) Xeon(R) CPU @ 2.20GHz, RAM:16GB, GPU:\nNVidia GeForce GTX 1070, 16 GB).\nB. The dataset\nWe use the Ryerson Audio-Visual Database of Emotional\nSpeech and Song (RAVDESS) [4] dataset. It has 8 speech label\nemotions as neutral, calm, happy, sad, angry, fearful, disgust,\nand surprised speeches.\nC. The recognition results\nTable I shows the accuracy of training and validation on\nnumber of epochs. After 42 epochs, we achieved enough\naccuracy as loss, training, and validation are 100%, 0,009,\nand 100%, correspondingly.\nNumber of epoch\nTraining accuracy\nLoss\nValidation accuracy\n10\n0.88\n0.470\n0.970\n20\n0.991\n0.011\n1.000\n42\n1.000\n0.009\n1.000\n50\n1.000\n0.006\n1.000\nTABLE I\nTHE DEEP CONVOLUTIONAL NEURAL NETWORK RECOGNITION MODEL\nTRAINING AND VALIDATION ACCURACY ON EPOCHS.\nFigure 2, 3, 4, and 5 show the recognition results of testing\ndata happy, calm, sad, and surprised emotional speeches,\naccordingly. We printed out top-8 probability classes to show\ncases with corresponding melspectrogram.\nFig. 2. The recognition result for happy emotional speech\nFig. 3. The recognition result for calm emotional speech\nFig. 4. The recognition result for sad emotional speech\nFig. 5. The recognition result for surprised emotional speech\nIV. CONCLUSION\nWe proposed the modern AI deep learning framework as\nDeepEMO for speech emotional recognition and application\nfor industry use case. Modern state-of-the-art deep learning\napproaches implemented to recognize the typical emotional\nspeeches. Main algorithm is directly provided in this research\nto develop ﬁrst phase of emotion recognition. The real visual\nresults and some important evaluation accuracy scores are pre-\nsented. In future works, we will publish next series of research\nto apply on emotional speech generation deep learning tasks.\nREFERENCES\n[1] Togootogtokh, Enkhtogtokh, and Amarzaya Amartuvshin. ”Deep learn-\ning approach for very similar objects recognition application on chi-\nhuahua and mufﬁn problem.” arXiv preprint arXiv:1801.09573 (2018).\n[2] He, Kaiming, et al. ”Deep residual learning for image recognition.”\nProceedings of the IEEE conference on computer vision and pattern\nrecognition. 2016.\n[3] Paszke, Adam, et al. ”Pytorch: An imperative style, high-performance\ndeep learning library.” Advances in neural information processing sys-\ntems. 2019.\n[4] Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database\nof Emotional Speech and Song (RAVDESS): A dynamic, multimodal\nset of facial and vocal expressions in North American English. PLoS\nONE 13(5): e0196391.\n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS"
  ],
  "published": "2021-09-09",
  "updated": "2021-09-09"
}