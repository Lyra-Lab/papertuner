{
  "id": "http://arxiv.org/abs/2109.04876v1",
  "title": "Integrating Approaches to Word Representation",
  "authors": [
    "Yuval Pinter"
  ],
  "abstract": "The problem of representing the atomic elements of language in modern neural\nlearning systems is one of the central challenges of the field of natural\nlanguage processing. I present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.",
  "text": "arXiv:2109.04876v1  [cs.CL]  10 Sep 2021\nIntegrating Approaches to Word Representation\nYuval Pinter *\nSeptember 2021\nThe problem of representing the atomic elements of language in modern\nneural learning systems is one of the central challenges of the ﬁeld of natural\nlanguage processing.\nI present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.\n1\nIntroduction\nThe mission of natural language processing (NLP) as a computational research ﬁeld is\nto enable machines to function in human-oriented environments where language is the\nmedium of communication. We want them to understand our utterances, to connect\nthese utterances with the objects and concepts of the surrounding world, to produce\nlanguage which is meaningful to us and helps us navigate a task or satisfy an emo-\ntional need. Over the years of its existence, the mainstream of NLP has known shifts\nmotivated by developments in computation, in linguistics, in foundational artiﬁcial\nintelligence, and in learning theory. Since the mid-2010’s, the clear dominant frame-\nwork for tackling NLP tasks, and an undeniably powerful one, has been that of deep\nneural networks (DNNs). This connectionist approach was originally motivated by\nthe workings of the human brain, but has since developed its own characteristics, and\nformed a well-deﬁned landscape for exploration which includes constraints stemming\nfrom the fundamental properties of its design.\nThis survey focuses on one of these built-in constraints, which I believe to be\ncentral to DNNs in the context of natural language, and speciﬁcally of text process-\ning, namely that of representations. DNNs “live” in metric space: their operation\nmanipulates real numbers organized into vectors and matrices, propagating function\napplications and calculated values within instantiations of pre-deﬁned architectures.\n*uvp@cs.bgu.ac.il\n1\nYuval Pinter\nIntegrating Approaches to Word Representation\nThis mode of existence is very well-suited to problem domains that inhabit their own\nmetric space, like the physical realms of vision and sound. In stark contrast to these,\nthe textual form of linguistic communication is built atop a discrete alphabet and\nhinges on notions such as symbolic semantics, inconsistent compositionality, and the\narbitrariness of the sign (de Saussure, 1916). The example in (1) exhibits all of these:\nthe symbol dog refers to two distinct objects bearing no semantic resemblance; large\nand white each describe the (canine) dog’s physical properties, while dining categorizes\nthe table based on its function, and hot does not modify (the second) dog at all, but\nrather joins it to denote a distinctive atomic concept.\n(1) The large white dog ate the hot dog left on the dining table.\nGiven these properties of language, it is far from straightforward to decide the\nmeans by which to transform raw text into an input for a neural NLP system tasked\nwith a goal which requires a grasp on the overall communicative intent of the text, such\nthat this initial representation does not lose basic semantics essential to the eventual\noutcome. This transformation process is known as embedding, after which its artifacts\nare themselves known as embeddings, often used synonymously in context with “vec-\ntors” or “distributed representations”. Indeed, the choice for default representations\nhas known several shifts within the short DNN era, motivated in part by advances in\ncomputational power but also by a collective coming to terms with the limitations of\nthe preceding methods.\nThe great challenge of representation is compounded by the unboundedness of it\nall — human concept space is ever-expanding, and each new concept may be assigned\nan arbitrary sign (e.g., zoomer); within an existing concept space, associations capable\nof inspiring new utterances occupy a combinatorial magnitude which is essentially\ninﬁnite; and even the form-meaning relationship itself exhibits malleability by humans’\ninteraction with text input devices and various cognitive biases.1 Each of these sources\nof expansion weighs any proposed representationalmethod with theadditionalburden\nof generalizing to novel inputs while maintaining consistency in the manner by which\nthey are represented in the system. In the NLP literature, the surface manifestation\nof the expanding spaces of concept and form, and of the more locally-constrained\ndisparity between text available at diﬀerent points in time of a model’s training and\ndeployment, is known as the out-of-vocabulary problem,and the unseen surface forms\nthemselves are termed OOVs.\nIn this survey, I consider three central approaches to representing the funda-\nmental units of natural language text in its input stage and the consequences of each\napproach’s selection on the goals of the systems they are applied in. The ﬁrst, most\npopular, and most successful one when used in isolation, is the distributional ap-\nproach where the representation function is trained to embed textual units which\n1As a case in point, over the course of writing this survey I have manually added dozens of new terms\nto the Overleaf editor’s spell-check dictionary, two in the referring sentence alone.\n2\nYuval Pinter\nIntegrating Approaches to Word Representation\nappear in similar contexts close to each other in vector space. The second is the com-\npositional approach which seeks to assemble embeddings for workable textual units\nby breaking them down into more fundamental elements and applying functions over\ntheir own representations, less committed to semantic guarantees. The last is the rela-\ntional approach which makes use of large semantic structures curated manually or in\na semi-supervised fashion, leveraging known connections between text and concepts\nand among concepts in order to create embeddings manifesting humans’ notions of\n“meaning”. The OOV problem features heavily in the motivation and analysis of the\nwork presented, as it presents challenges to each of the approaches described, yet the\nexact deﬁnition of vocabularies and OOV-ness themselves are challenged by the ad-\nvent of NLP systems that have become mainstream following the processed described\nin this work, namely contextualized subword embeddings.\n2\nThe Atoms of Language\nNatural language is ultimately a system for conveying meaning, information, and so-\ncial cues from the realm of human experience into a discrete linear form by encoding\nthem as auditory, visual, and/or textual symbols, which are then iteratively composed\ninto more complex units. In order to process such a system’s outputs by computa-\ntional means, it seems ﬁtting to identify those symbols which carry the basic units of\nmeaning, and then ﬁnd the proper ways to map those meanings into representations\nfor a program which can compose them. The ﬁrst step, that of identifying linguistic\natoms, proves to be a formidable challenge. From the surface output perspective, the\ncommon wisdom is that the basic semantic unit of language is what is known as a mor-\npheme. The English word unbelievable, for example, is composed of a stem morpheme\nbelieve, a semantic-syntactic suﬃx -able recasting the verb into an adjective pertaining\nto potential, and a semantic preﬁx un- denoting negation. But this morpheme = atom\nstipulation is not unassailable. Processes below the morpheme level have been doc-\numented across languages, for example the sound symbolism phenomenon known\nas phonaesthesia, where arbitrary sound patterns correlate with a concept or concep-\ntual properties, such as /gl/ in the English light/shine-related words glow, glitter, and\nglare (Blake, 2017). Less arbitrarily, patterns and even individual sounds in names\nare known to evoke semantic qualities based on their acoustic properties (Köhler,\n1947; Bergh et al., 1984). In English-language informal communication modes, writers\nsometimes employ the practice of expressive lengthening, where a single character\nin a word is repeated in order to amplify its referent’s extension on some scale. For\nexample, looooong would be used to describe a particularly long object or period of\ntime. In addition to these sub-morpheme phenomena, the morpheme symbolism and\nthe atoms of our conceptual space relate at neither a univalent nor a one-to-one rela-\ntion. Certain stem morphemes, like star, denote multiple types of concepts or objects\n(polysemy and homonymy), while some concepts may be referred to using diﬀerent\nmorphemes like the relevant meanings of room and space (synonymy). The suﬃx -s\n3\nYuval Pinter\nIntegrating Approaches to Word Representation\ncan denote both a third-person present verb or a plural noun (polyexponence), and\nboth are replaced by -es under certain local conditions (ﬂexivity).\nTheoretical quibbles notwithstanding, NLP is a practical ﬁeld, and from its\nnascence it was clear that ﬁnding the most appropriate way to break text down to\nits purest elements should not set back our eﬀorts to perform sequence-level tasks and\ndevelop useful applications. Thus, concessions must be made in the form of selecting\na unit easily extractable from text and working with it. This necessity coincides with\nthe reality of having English as the overwhelmingly central target of NLP applications\nand easiest source of data. The focus on a language with mostly isolating morphology,\nwhere morphemes often occupy distinct word forms that are related through sentence-\nlevel syntax, conspired with the technical ease of detecting whitespace in text and led\nto an inevitable starting point for the community in using the space-delimited word\nas the basic unit of text analysis.2 The very name of the fundamental bag-of-words\napproach (BoW) illustrates the implicit synonymity of “word” and “basic unit of rep-\nresentation” in NLP jargon. Although subword- and multiword-level systems were\ndesigned and developed outside this paradigm, mostly citing a non-English motiva-\ntion, when the neural revolution came the predominant methods again anchored the\nﬁeld to the space-delimited word as the atom.\nThe most obvious advantage of this approach is its simplicity, considering how\ndiﬃcult it is in practice to extract correct sub-word morphemes directly from text.\nHistorically-entrenched orthographic conventions and local-context phonological pro-\ncesses lead to phenomena such as variance in morpheme form at diﬀerent instanti-\nations, such as the disappearance of the stem’s ﬁnal e in the unbelievable or the s-t\nalteration in derivations like Mars-Martian, making a deterministic mapping from sur-\nface form to morpheme sequence impossible. The lack of overt textual marking of\nmorpheme boundaries (except for the uncommon case of hyphenation) also leads\nto ambiguous segmentation in words like unionize, and the general property of our\nsound and writing systems’ inventory being relatively small leads to the incidence of\naﬃx-identical sequences in single-morpheme words like reply (cf. shortly) and bring\n(cf. lying). Automatic detection of morphemes can be achieved today by unsupervised\ndata-driven systems like Morfessor (Creutz and Lagus, 2002, 2007), which rely on large\namounts of training data and provide no guarantee to ﬁnding the true morphemes in\nall cases or downstream applications.\n2I will continue throughout to use “space-delimited” to describe a family of simple string tokenization\ntechniques which typically also include minimal heuristics for punctuation separation and a handful\nof language-speciﬁc rules like separating English contractions based on a short closed list, in partial\naccommodation of the diﬀerence between grammatical words and orthographic words (Dixon et al.,\n2002).\n4\nYuval Pinter\nIntegrating Approaches to Word Representation\n3\nNeural Representations\nThe idea of breaking down concepts in language into numerically-valued axes has\nplayed a role in the formation of the modern research landscape in linguistics. Osgood\n(1952) proposed a low-dimensional space in which nominal objects and concepts are\nrepresented by values associated with characteristics which may describe them, such\nthat “eager” and “burning” share a value along the weak ⇔strong dimension, while\ndiﬀering along the cold⇔hot dimension. The values were elicited from human subjects.\nScaling this very linguistically-motivated approach manually over an entire lan-\nguage is at the very least impractical, and over the years some relaxations of this\nscheme to deﬁne representations for words which are distributed along dimensions\ngave rise to more automation-friendly processing techniques. Most crucial was the\nrealization that the individual dimensions in the representation space do not have to be\nmeaningful in and of themselves. Liberating the dimensions from their labels allowed\nthe number of dimensions to be governed by concerns of data availability and com-\nputational memory and power, rather than by the precision of our semantic theory\nand ontological thoroughness; it allows for the discovery of unnamed but possibly\nuseful similarities and distinctions between concepts; and it “leaves room” for new\nproperties to be learned if, for example, a domain shift occurs during the process of\napplying an embedding-based system to a downstream task.\nEmbedding concepts into a “blank” vector space using learning methods turns\nthe implied causal direction that motivated Osgood’s framework on its head: instead\nof creating the embeddings based on what we know about language and the relations\nbetween concepts,the latter become the proxy target by which we can measure whether\nor not the embeddings learned by our model are useful to us. Starting with an arbitrary\nmetric space with well-known properties such as Rd becomes a great advantage, as the\nspace comes with metrics and operations which are easy to conceptualize and imagine\nas the necessary proxies.3 As the formative instance of this realization served the ability\nto score the relative directionality of two vectors using the cosine similarity function,\nwhich can be compared to annotations in word similarity resources such as WordSim-\n65 (Rubenstein and Goodenough, 1965), where human subjects were asked to score\nword pairs without the hassle of decomposing them into their semantic properties\nﬁrst. Metric space also aﬀords the intuitive parallelogram metaphor of word analogy,\nhaunting every introductory text and presentation on embeddings with the equation\nking −man + woman ≈queen.\n3One heroic departure from the shackles of euclidean space is the line of work on embeddings\nin hyperbolic space (Nickel and Kiela, 2017), touted as a more suitable representation framework for\nhierarchical structures, including the semantic structure of a language.\n5\nYuval Pinter\nIntegrating Approaches to Word Representation\n4\nDistributional Semantics\nThe development of the distributed view of representation for linguistic objects ac-\ncompanied the rise of methodologies making use of the distributional hypothesis,\ntraditionally attributed to Harris (1954) and framed as “you shall know a word by the\ncompany it keeps”. The maximalist interpretation of this adage as “a word is deﬁned\nby applying a combination function to the set of its contexts”, used pre-modern-\nneurally in inﬂuential methods such as Brown Clustering (Brown et al., 1992), is an\nappealing principle to the embedding movement for good reason: breaking words\ndown into contexts provides us with just the distributed ﬁxed dimensions we seek.\nOnce we decide exactly what “context” means to us, we can programatically extract\nall contexts for all target words given only a corpus, and base our latent dimensions\n(whose number is limited to hundreds or thousandsfor practical reasons) on them. The\ntwo methods which ended up dominating the distributional embeddings landscape\nshare a deﬁnition of context, essentially “words that appear near the target word”,\nbut translate this decision into embedding diﬀerently. In SkipGram (Mikolov et al.,\n2013a), dimension signiﬁcance is built “bottom-up” from a random initialization and\na traversal of the corpus; in GloVe (Pennington et al., 2014), dimensions are the result\nof an implicit reduction of the full V × V co-occurrence matrix, where V is the number\nof words in our vocabulary. The former approach was inspired by early embedding\nsystems (Bengio et al., 2003) developed around the task of language modeling, which\nis deﬁned with an expectation based in distributional signals, while the latter has ori-\ngins in latent semantic analysis (LSA; Deerwester et al., 1990). Evaluation on intrinsic\ntasks such as similarity datasets and analogy benchmarks (e.g., Finkelstein et al., 2001;\nMikolov et al., 2013b; Hill et al., 2015) cemented distributional word embeddings as\nthe representation go-to and an accessible replacement to one-hot encodings for a host\nof applications, while performance on downstream tasks within deep learning systems\nadvanced the understanding of the utility that pre-training can aﬀord end-to-end sys-\ntems which include an embedding layer (Collobert and Weston, 2008; Collobert et al.,\n2011).\n5\nOut-of-Vocabulary Words\nThe choice of space-delimited words as the basic unit for representation, and the large\nresource investment necessary to pre-train a distributional model over a large corpus,\nin both money and time, create a situation where vectors can mostly be trusted as long\nas the words they represent are present in the pre-training corpus. The models so far\ndiscussed have no intrinsic ability to represent words not present in their lookup table,\nor out-of-vocabulary, or OOVs (Brill, 1995; Brants, 2000; Plank, 2016; Heigold et al.,\n2017; Young et al., 2018). Empirical analyses such as the one in Pinter et al. (2017)\nshow that indeed, the overwhelming majority of downstream datasets contain words\nnot present in the pre-training corpora. Pinter et al. (2020a) present a diachronical\n6\nYuval Pinter\nIntegrating Approaches to Word Representation\ndataset showcasing the volume of novelterms entering alarge, steady daily publication\nin English over time; but even a snapshot of a language at a given moment contains\nunlimited domain-speciﬁc terms, morphological derivations, named entities, potential\nloanwords, typographical errors, and other sources of OOVs which would appear very\nreasonably in text analysis tasks and which the downstream model should be given\nthe faculty to handle. In fact, according to Kornai (2002), statistical reasoning leads\nus to conclude that languages have an inﬁnite vocabulary. But even if a language’s\nword set were ﬁnite, and all present in some corpus, practical memory and lookup\nconstraints would still limit embedding tables to non-exhaustive vocabularies.\nTo overcome the intrinsic limits of corpus-learned embedding tables, the distri-\nbutional system has begotten some heuristics that try and initialize embeddings for\nOOVs beyond the trivial random initialization fallback. If one were to stay true to\nFirth’s maxim, one possible strategy would be to keep SkipGram’s context embedding\ntable as well as the main table (for “target” words), and initialize OOV embeddings\nbased on the context in which they are ﬁrst encountered (Horn, 2017). This approach\nhas not caught on, and instead most practitioners took to the use of a special <UNK> em-\nbedding, named as an abbreviation of unknown (Bengio et al., 2003). In a pre-training\nstage, such an embedding is learned by replacing a small percentage of the corpus with\na dedicated <UNK> token, thus gaining at least some prior for an initialization, in some\nsense an average over possible contexts for encountering any word. This approach is\nbrutally simplistic; it assumes not only that all novel words are representable using\nthe same approximation technique, but that they are all exactly the same. The ﬁrst\nassumption alone is easy to dispute: a careful observation of any taxonomy of word\nformation processes (Lieber, 2005; Plag, 2018) suggests that embedding new words\ninto an existing space must involve considering multiple approaches in parallel.\n• Words created by processes at the multi-word level, such as compounding or\nblending, require means of extracting the underlying constructed words and\ncomposing the semantic contribution from each word. For example, brunch is\na blend of breakfast and lunch; a reasonable initial embedding can be the mean\nvector for these two words, hopefully keeping it at a high similarity with other\nmeals and the appropriate time of day.\n• Words that are inﬂections of known words, for example ameliorating, can beneﬁt\nfrom a morphological analysis which ﬁnds its stem and syntactic suﬃx, placing\nthe new vector at the sum of the verb ameliorate and the generalized notion of\n-ing verbs, if one is realized in the embedding space (arguably, in a good space it\nshould at least be reliably extractable).\n• Novel named entities such as Lyft or SARS-COV-2, more often than not, reﬂect\narbitrary naming practices and cultural primitives, and even recognition of their\ntype (person / organization / location, etc.) might well be impossible without\naccess to knowledge bases covering the appropriate domain, noting explicitly\nwhere in concept space the novel word should be embedded.\n7\nYuval Pinter\nIntegrating Approaches to Word Representation\n• Some OOVs are the result of unpredictable subword processes such as typo-\ngraphical errors (typos) and stylistic variation, like the aforementioned expres-\nsive lengthening. In such cases, it is sometimes best to opt out of creation of a\nnew embedding at all and simply map the new form to the existing embedding\nof its intended canonical word form. This choice will depend on the intended\napplication; in certain cases like sentiment analysis, the stylistic information itself\nis essential.\n• Loanwords like vespa originate in a diﬀerent language than the one the em-\nbedding was produced for, but in some cases we have access to an embedding\nspace for the origin language and a function which translates between the two\nlanguages’ space. A system which can detect the word and its origin, perhaps\novercoming processes like writing-system transliteration and phonological adap-\ntation, can start by embedding the target language word in a position projected\nfrom the source language’s embedding for the equivalent word form.\nThis is not acomprehensive list. More types of novelwords are identiﬁed in Pinter et al.\n(2020a), and not all suggestions in the taxonomy above correspond to actual existing\nwork. Limiting this discussion to a strict interpretation of written-form uniqueness\nalso prevents us from considering as OOVs concepts which are spelled in the same\nway as other words, either by chance (homography, for example row as a line or a\nﬁght), by naming (e.g., Space Force), or by processes such as zero-derivation (the verb\nsmoke, derived from the noun). In languages other than English, some OOV-creating\nforces may be more dominant in word formation than in English. Morphologically-\nrich languages, as one edge case, feature large percentages of OOVs in novel texts for a\ngiven task’s text size compared to English, and this property is often compounded by\nthe fact that many of these are low-resource languages, possessing a relatively small\ncorpus-extracted vocabulary to begin with.\nThe richness and unpredictability of the OOV problem calls for complementing\nthe word representationsystems obtained distributionally with additional approaches,\nwhich is the focus of this survey.\n6\nSubword Compositionality\nThe ﬁrst approach considered is an attempttobreak thespace-delimited word paradigm\nand get at the ﬁner atomic units of meaning, which can then either be used as the fun-\ndamental representation layer, or induce better representations at the word level. This\nperspective, known as the compositional approach, is inspired mostly by the cases\nwhere insuﬃcient generalizations are made for cases of morphological word formation\nprocesses. Under the compositional framework, an ideal representationfor unbelievable\ncan be obtained by (1) detecting its three morphological components un-, believe, and\n-able, (2) querying reliable representations learned for each of them, distributionally or\n8\nYuval Pinter\nIntegrating Approaches to Word Representation\notherwise, and (3) properly assembling them via some appropriate function.4\nEach of these three steps is a challenge in itself and open to various implemen-\ntational approaches. Learning representations for subword units is usually done by\nconsidering the subword elements in unison with the full word while applying a distri-\nbutional method (e.g., Bojanowski et al., 2017), but some have opted for pre-processing\nthe pre-training corpus such that only lemma forms exist as raw text and the other\ntokens are explicit representations of the morphological attributes attached to each\nlemma (Avraham and Goldberg, 2017; Tan et al., 2020), inducing the production of\nmore consistent vocabularies. Others yet leave the learning to the downstream task it-\nself, feeding oﬀthe backpropagated signal from the training instances (Sutskever et al.,\n2011; Ling et al., 2015; Lample et al., 2016; Garneau et al., 2019); while others train a\ncompositional network based on the word embedding table in an intermediate phase\nbetween pre-training and downstream application (Pinter et al., 2017; Zhao et al., 2018).\nThe composition function from subwords to the word level is also open to many diﬀer-\nent approaches: prior work has opted for construction techniques as diverse as using\nthe subword strings as one-hot entries to represent the words themselves (Huang et al.,\n2013); summing morpheme embeddings to producewordembeddings(Botha and Blunsom,\n2014); traversing a possibly deep morphological parse tree using a recursive neural\nnetwork (Luong et al., 2013); positing probabilistic word embeddings for which the\nmorpheme embeddings act as a prior distribution (Bhatia et al., 2016); side-by-side\ntraining of both word-level and character-level modules followed by concatenating\nthe resulting representations, to allow the downstream model to learn from both levels\nindependently and control the interaction terms directly (Plank et al., 2016); assem-\nbling a hierarchical recurrent net that progressively encodes longer portions of text in\neach layer (Chung et al., 2019); or dispensing with the word level altogether and just\nrepresenting text with a single atomic layer of characters or subwords (Sennrich et al.,\n2016).\nMost challenging of all is the detection of the subwords themselves. As noted\nabove, morphemes are hard to detect from the surface form of a word. For the de-\nfault setting where no curated resources exist to allow correct morpheme extraction\nfrom a word’s form, as is the case in nearly all languages in the world, the main-\nstream of compositional representation research has centered on the raw character\nsequence, the unarguable atom of text,5 which is used either via direct operation\nor as a basis for heuristics that deﬁne subword units based on statistical objectives.\nThe great advantage of using characters or primitive character n-grams as the atomic\n4I will use the term subword to denote textual units which are largely between the character level\nand the word level, when no guarantee of their morphological soundness is attempted. In appropriate\ncontexts, this can also denote word-long or character-long elements which are nevertheless obtained by\na subword tokenizer.\n5At least in languages using the Latin script, like English. Chinese text analysis has beneﬁtted from\ndecomposing characters into strokes or radicals; Hebrew and Arabic include diacritical marks that are\nnot character-intrinsic; and elsewhere, treatment of individual bytes from the Unicode representation of\ncharacters has also shown merit.\n9\nYuval Pinter\nIntegrating Approaches to Word Representation\nunit for the model (Santos and Zadrozny, 2014; Kim et al., 2016; Wieting et al., 2016;\nBojanowski et al., 2017; Peters et al., 2018) is that it rids us of the need to explicitly\ndesignate morphemes altogether; the challenge is to still capture the information they\nconvey, somehow.\nIn contrast, heuristically learning a subword vocabulary from\ninformation-theoretic notions (Sennrich et al., 2016; Kudo and Richardson, 2018) or\ncharacter-sequence unigram distribution (Kudo, 2018) may ﬁnd us many true mor-\nphemes, but there is no guarantee of either precision or recall: corpus collection\neﬀects are signiﬁcant in determining the ultimate vocabulary, orthographic norms\nmay still obfuscate many useful generalized morphemes, and many frequent charac-\nter sequences may enter the subword vocabulary as the result of coincidental quirks.\nFor example, the character sequence eva might contribute to the representation of un-\nbelievable, passing along signals learned from unrelated words such as Eva or evaluate.\nThe ever-growing popularity of systems which use such vocabularies in conjunction\nwith the null composition function that ignores sub-word hierarchy and passes the\ndownstream model embeddings corresponding to the raw subword sequence (see §8)\nprevents any possibility of correcting incorrect subword tokens at the word level: in\nthis scenario, the next processing layer of the model will use the embedding for eva as\nif it were part of the input equally important to a frequent word like house.\n7\nRelational Semantics\nAnother way to complement distributionally-trained embeddings is to incorporate\nsignals from curated type-level relational resources.\nThe prominent category of\nsuch resources is semantic graphs, such as WordNet (Fellbaum, 1998) and Babel-\nNet (Navigli and Ponzetto, 2010), which encode the structural qualities of language as\na representation of human knowledge. The core goal of semantic graphs is to describe\nconnections between referents in the perceived and conceived world, and to this end\nthey make an explicit distinction between words as character sequences and an inter-\nnal semantic primitive which we can call concepts. Concepts form the chief node type\nin the semantic graph, connected by individual edges typed into relations such as hy-\npernymy (elm “is a” tree) or meronymy (branch “is part of a” tree), as well as linguistic\nfacts about concept names (shop.verb “is derivationally related to” shop.noun) which\nmake use of the word-form partition of the graph’s node set. In similar vein, relations\nwhich straddle the divide between form and function, like synonymy, are extractable\nfrom the bipartite subgraph relating word forms and their available meanings.\nIn the context of language representation, these structures oﬀer a notion of atom-\nicity stemming from our conceptual primitives, an attractive premise. They may not\nanswer all needs arising from inﬂectional morphology (since syntactic properties do\nnot explicitly denote concepts) or some of the other word formation mechanisms,\nbut the rich ontological scaﬀolding oﬀered by the graph and the prospects of assign-\ning separate embeddings for homonyms in a model-supported manner, assuming\nsense can be disambiguated in usage, seems much “cleaner” than relying on large\n10\nYuval Pinter\nIntegrating Approaches to Word Representation\ncorpora and heuristics to statistically extract linguistic elements and their meaning.\nIn addition to this conceptual shift, as it were, the graph structure itself provides a\nlearning signal not present in linear corpus text, relating the basic units to each other\nthrough various types of connections and placing all concepts within some quan-\ntiﬁable relation of each other (within each connected component, although lack of\nany relation path is also a useful signal). The structure can also occupy the place\nof the fragile judgment-based word similarity and analogy benchmarks, allowing\nmore exact, reﬁned, well-deﬁned relations to be used for both learning the repre-\nsentations and evaluating them. Methods which embed nodes and relations from\ngeneral graph structures before even considering any semantics attached to individual\nnodes and edges, like Node2vec (Grover and Leskovec, 2016) and graph convolutional\nnets (Schlichtkrull et al., 2017), indeed serve as a basis and inspiration for many of the\nworks in this space.\nThe fundamentally diﬀerent manner in which the relational paradigm is comple-\nmentary to the distributional one in contrast with the compositional one has bearing on\nthe OOV problem, which can be viewed from several perspectives. First is the poten-\ntial of semantic graphs to improve representation of words that are rare or not present\nin a large corpus used to initialize distributional embeddings. This has proven to be\na powerful direction by methods such as retroﬁtting (Faruqui et al., 2015), where em-\nbeddings of related concepts are pushed together in a post-processing learning phase,\nshowcasing WordNet’s impressive coverage of English domain-speciﬁc taxonomies\nsuch as classical natural sciences. Elsewhere, properly modelling hypernymy, for ex-\nample, has been found to help understand text with rare words whose hypernyms\nare well-represented in the pre-training corpus (Shwartz et al., 2017).6 Still, semantic\ngraphs provide only a partial solution to the overall goal of OOV impact mitigation,\ngiven their limited scope and heavy reliance on expert annotation.\nFrom the other direction, systems relying on semantic graphs for applications\nsuch as question answering and dialogue generation are likely to encounter “OOVs”\nof their own, i.e. words and concepts not present in the underlying graph. Unlike\nthe corpus-OOV problem, which cannot be quantiﬁed convincingly without selecting\na speciﬁc downstream task ﬁrst, coping with graph-OOVs can be examined through\ntasks intrinsic to the graph structure itself. One such task is relation prediction, where\nwe assume a concept has a known connection with some other concept, and need to\nﬁgure out which one. Depending on our perspective, either the source or target of\nthe relation may be the OOV concept; for example, on ﬁrst encounter of the concept\nindian lettuce, we wish to know its hypernym from our set of known concepts. This\ntask is also useful for a similar class of graphs known as knowledge graphs (KGs),\n6A tangential but noteworthy approach considers relations that are not curated in large graphs, but\nrather corpora annotated for inter-word relations such as syntactic dependencies (Madhyastha et al.,\n2016). Their system creates a mapping between a distributionally-obtained embedding table and one\ntrained on the annotated parses, and generalizes this mapping to words which are now out-of-vocabulary\nfor a further downstream task (e.g., sentiment analysis). In this case, the reference vocabulary (for deﬁning\nOOV-ness) is not the unsupervised corpus, but rather an intermediate downstream task.\n11\nYuval Pinter\nIntegrating Approaches to Word Representation\nsuch as Freebase (Bollacker et al., 2008)7 and WikiData (Vrandeˇci´c and Krötzsch, 2014),\nwhich diﬀer from semantic graphs in several aspects. While WordNet curates connec-\ntions between semantic concepts and dictionary entries, including certain aspects of\nthe physical world (e.g. “an elm is a tree”), KGs focus on real-world entities and\noften time-sensitive encyclopedic knowledge (e.g.\n“Satya Nadella is the CEO of\nMicrosoft”). WordNet is a manually-crafted resource created by language and do-\nmain experts, whereas many KGs are either crowdsourced or automatically extracted\nfrom databases and large text corpora. As a result, KGs are typically disconnected,\nshallow, and sparse, boasting areas of hubness and areas of isolation; this contrasts\nwith semantic graphs, where systematic connectedness and hierarchy have been ob-\nserved (Sigman and Cecchi, 2002). KGs are also distinguished by the richness of their\nrelation type variety, in the hundreds or thousands, compared to WordNet’s 18 relation\ntypes (including seven pairs of relations reciprocal to each other). Nevertheless, much\nof the work on the relation prediction problem has been developed and evaluated on\nboth semantic and knowledge graphs, as well as on derived tasks like graph comple-\ntion, where the entirety of a node’s connections are to be inferred at once, imitating\nreal-world scenarios of knowledge discovery.\nOver the years, distributional methods have been used to feed increasingly com-\nplex neural nets predicting relations by embedding both concept nodes and relation\nedges based on corpus-trained tables, to a large degree of success (e.g. Nickel et al.,\n2011; Socher et al., 2013; Bordes et al., 2013; Yang et al., 2014; Toutanova and Chen,\n2015; Neelakantan et al., 2015; Ji et al., 2015; Shi and Weninger, 2017; Dettmers et al.,\n2018; Nathani et al., 2019). The basic idea calls for embedding concepts into a metric\nspace and modeling relations by some operator that induces a score for an embedding\npair input, either by translating the concept vectors, combining them via bilinear op-\nerators, projecting them onto a “scoring scale”, or designing an intricate deep system\nthat ﬁnds complex relationships. While these systems achieve impressive results, they\nall build on an implicit assumption that relation prediction is a strictly local task: the\nﬁt of an edge can be estimated from the nodes it connects and the intended label alone.\nIn KGs, where structure is of secondary concern, this assumption may go a long way\nbefore its limitations stress out performance; in the much more structure-crucial se-\nmantic graphs, it is increasingly likely that connections are predicted which should not\nbe permissible from enforceable structural constraints alone, e.g. that the hypernym\ngraph cannot contain cycles. Some systems indeed go beyond the individual edge to\nembed and predict relations, for example the idea of a path prediction task (Guu et al.,\n2015) which demands more structure reliance, or embedding methods leveraging local\nneighborhoods of relation interactions and automatic detection of relations from syn-\ntactically parsed text in an iterative manner (Riedel et al., 2013; Toutanova et al., 2015;\nSchlichtkrull et al., 2017). Others have constructed prediction models where an adver-\nsary produces examples which violate structural constraints such as symmetry and\ntransitivity (Minervini and Riedel, 2018). Pinter and Eisenstein (2018) present a sys-\n7Now defunct.\n12\nYuval Pinter\nIntegrating Approaches to Word Representation\ntem which improves WordNet prediction by augmenting the distributionally-obtained\nsignal with features (motifs) representing the global structure of the semantic ediﬁce.\nIn addition to the task beneﬁt, the emerging feature weights lead to discovery of some\ngeneral properties of English semantics.\n8\nContextualized Representations\nRecent developments in NLP have brought about a shift in the balance depicted so far\nwith respect to the atomic level chosen to represent language in applications and the\napproaches taken to create these representations. Advances in multi-task learning and\ntransfer learning, both in non-neural NLP and in non-NLP deep methods, matured\nwellenough to allow deep NLP to use them eﬀectively as well. The increase of available\ncomputation power and the extreme utility found to lie in recurrent nets, most notably\nthe Long Short-Term Memory cell (LSTM; Hochreiter and Schmidhuber, 1997), led to\na series of works suggesting the incorporation of instance-speciﬁc context into the fea-\nture extraction part of a model, before applying any task-speciﬁc elements, beginning\nwith simple prediction tasks (Melamud et al., 2016), followed by near-full coverage\nof core NLP (Peters et al., 2018). The next step was to continue training the shared-\narchitecture context learner, which we can now safely call a language model, during\nthe downstream step, in a process known as ﬁne-tuning (Howard and Ruder, 2018).\nDesign and processing power considerations, but also downstream performance, fu-\neled the shift (Radford et al., 2018) from recurrent net infrastructure to transformer\nmodels (Vaswani et al., 2017), which in turn facilitated another major conceptual in-\nnovation where autoregressive token prediction was replaced by masked language\nmodeling, where sequence-medial tokens are hidden from the representation layer\nand must be predicted based on the remaining context (Devlin et al., 2019; Liu et al.,\n2019). Throughout this evolution, one main principle remained stable: the language\nprediction task acts as the pre-training step, providing a scaﬀolding model which is\ncapable of representing tokens within a sequence at a level of eﬀectiveness that allows\ndownstream tasks to begin training with meaningful contextualized representations.\nThe heart of contextualization lies in the distributional approach.\nThe design of these pre-training tasks meant they can no longer tolerate OOV\ntokens at the rate encountered by static embedding algorithms, as that might render\nthe models unusable for any words that appear in context with OOVs downstream,\nrather than just the OOVs themselves. On the other hand, the prediction layer creates\na computational bottleneck which scales with the size of the vocabulary, since every\ntoken must be available for prediction at all model steps. Therefore, these models\nresorted to compositional techniques for the bottom layer where the input sequence is\nprocessed into tokens. The character convolution net selected for ELMo (Peters et al.,\n2018) did not gain traction, possibly because it didn’t provide an adequate method\nfor predicting text from the output layer, and so subsequent models, particularly\nthose relying on transformers, operate over a sequence of equal-status tokens, each\n13\nYuval Pinter\nIntegrating Approaches to Word Representation\nrepresenting a word or a subword, from a mid-size vocabulary (tens of thousands)\nbuilt in a pre-pre-training phase using statistical heuristic techniques mentioned in §6.\nThese models inherit the problems endemic to these methods like inadequacy for\ncertain OOV classes, morphological unsoundness, and length-imbalance; as well as\nissues like the added burden they impose on already limited-length token sequences.\nCommon wisdom seems to hold that they make up for these shortcomings within\nthe depths of their fully-connected transformer layers, and end up with satisfactory\ntop-layer representations. Recent work challenging these models with truly novel\nword forms suggest otherwise (Pinter et al., 2020a,b), while work on either incorporat-\ning the compositional signal into subword-vocabulary transformers (Ma et al., 2020;\nAguilar et al., 2020; El Boukkouri et al., 2020; Pinter et al., 2021), or replacing the sub-\nwords with characters or bytes altogether (Clark et al., 2021; Xue et al., 2021), is rapidly\ngaining traction as well.\nAcknowledgments\nThis survey is an adapted version of the introduction my PhD thesis. I thank my\ncommittee for helping to shape it: my advisor, Jacob Eisenstein; Mark Riedl, Dan Roth,\nWei Xu, and Diyi Yang.\nReferences\nAguilar, G., McCann, B., Niu, T., Rajani, N., Keskar, N., and Solorio, T. (2020).\nChar2subword: Extending the subword embedding space from pre-trained models\nusing robust character compositionality. arXiv preprint arXiv:2010.12730.\nAvraham, O. and Goldberg, Y. (2017). The interplay of semantics and morphology\nin word embeddings. In Proceedings of the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Volume 2, Short Papers, pages 422–426,\nValencia, Spain. Association for Computational Linguistics.\nBengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic\nlanguage model. The journal of machine learning research, 3:1137–1155.\nBergh, B. G. V., Collins, J., Schultz, M., and Adler, K. (1984). Sound advice on brand\nnames. Journalism Quarterly, 61(4):835–840.\nBhatia, P., Guthrie, R., and Eisenstein, J. (2016). Morphological priors for probabilistic\nneural word embeddings. In Proceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 490–500, Austin, Texas. Association for\nComputational Linguistics.\nBlake, B. J. (2017). Sound symbolism in english: Weighing the evidence. Australian\nJournal of Linguistics, 37(3):286–313.\n14\nYuval Pinter\nIntegrating Approaches to Word Representation\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). Enriching word vectors\nwith subword information. Transactions of the Association for Computational Linguistics,\n5:135–146.\nBollacker, K., Evans, C., Paritosh, P., Sturge, T., and Taylor, J. (2008). Freebase: a\ncollaboratively created graph database for structuring human knowledge. In Pro-\nceedings of the 2008 ACM SIGMOD international conference on Management of data,\npages 1247–1250.\nBordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013).\nTranslating embeddings for modeling multi-relational data.\nIn Burges, C. J. C.,\nBottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. Q., editors, Advances in\nNeural Information Processing Systems 26, pages 2787–2795. Curran Associates, Inc.\nBotha, J. A. and Blunsom, P. (2014). Compositional morphology for word representa-\ntions and language modelling. In Proceedings of the International Conference on Machine\nLearning (ICML).\nBrants, T. (2000). Tnt: a statistical part-of-speech tagger. In Proceedings of the sixth\nconference on Applied natural language processing, pages 224–231. Association for Com-\nputational Linguistics.\nBrill, E. (1995). Transformation-based error-driven learning and natural language pro-\ncessing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–\n565.\nBrown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. (1992). Class-\nbased n-gram models of natural language. Computational Linguistics, 18(4):467–480.\nChung, J., Ahn, S., and Bengio, Y. (2019). Hierarchical multiscale recurrent neural\nnetworks. In 5th International Conference on Learning Representations, ICLR 2017.\nClark, J. H., Garrette, D., Turc, I., and Wieting, J. (2021).\nCanine:\nPre-training\nan eﬃcient tokenization-free encoder for language representation. arXiv preprint\narXiv:2103.06874.\nCollobert, R. and Weston, J. (2008). A uniﬁed architecture for natural language pro-\ncessing: Deep neural networks with multitask learning. In Proceedings of the 25th\ninternational conference on Machine learning, pages 160–167.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011).\nNatural language processing (almost) from scratch.\nJournal of Machine Learning\nResearch, 12(76):2493–2537.\nCreutz, M. and Lagus, K. (2002). Unsupervised discovery of morphemes. In Proceed-\nings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 21–30.\nAssociation for Computational Linguistics.\n15\nYuval Pinter\nIntegrating Approaches to Word Representation\nCreutz, M. and Lagus, K. (2007).\nUnsupervised models for morpheme segmenta-\ntion and morphology learning. ACM Transactions on Speech and Language Processing\n(TSLP), 4(1):1–34.\nde Saussure, F. (1916). Cours de linguistique générale (roy harris, trans.). London:\nDuckworth.\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990).\nIndexing by latent semantic analysis. Journal of the American society for information\nscience, 41(6):391–407.\nDettmers, T., Pasquale, M., Pontus, S., and Riedel, S. (2018). Convolutional 2d knowl-\nedge graph embeddings.\nIn Proceedings of the 32th AAAI Conference on Artiﬁcial\nIntelligence.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\nDixon, R. M., Aikhenvald, A. Y., et al. (2002). Word: a typological framework. Word:\nA cross-linguistic typology, 1:41.\nEl Boukkouri, H., Ferret, O., Lavergne, T., Noji, H., Zweigenbaum, P., and Tsu-\njii, J. (2020). CharacterBERT: Reconciling ELMo and BERT for word-level open-\nvocabulary representations from characters. In Proceedings of the 28th International\nConference on Computational Linguistics, pages 6903–6915, Barcelona, Spain (Online).\nInternational Committee on Computational Linguistics.\nFaruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., and Smith, N. A. (2015).\nRetroﬁtting word vectors to semantic lexicons. In Proceedings of the 2015 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 1606–1615, Denver, Colorado. Association for Compu-\ntational Linguistics.\nFellbaum, C. (1998). WordNet: An Electronic Lexical Database. Bradford Books.\nFinkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., and\nRuppin, E. (2001). Placing search in context: The concept revisited. In Proceedings of\nthe 10th international conference on World Wide Web, pages 406–414.\nGarneau, N., Leboeuf, J.-S., Pinter, Y., and Lamontagne, L. (2019). Attending form\nand context to generate specialized out-of-vocabularywords representations. arXiv\npreprint arXiv:1912.06876.\nGrover, A. and Leskovec, J. (2016). node2vec: Scalable feature learning for networks.\nIn Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery\nand data mining, pages 855–864.\n16\nYuval Pinter\nIntegrating Approaches to Word Representation\nGuu, K., Miller, J., and Liang, P. (2015). Traversing knowledge graphs in vector space.In\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npages 318–327, Lisbon, Portugal. Association for Computational Linguistics.\nHarris, Z. S. (1954). Distributional structure. Word, 10(2-3):146–162.\nHeigold, G., Neumann, G., and van Genabith, J. (2017). How robust are\ncharacter-based word embeddings in tagging and mt against wrod scramlbing or\nranddm nouse? arXiv preprint arXiv:1704.04441.\nHill, F., Reichart, R., and Korhonen, A. (2015). SimLex-999: Evaluating semanticmodels\nwith (genuine) similarity estimation. Computational Linguistics, 41(4):665–695.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory.Neural computation,\n9(8):1735–1780.\nHorn, F. (2017). Context encoders as a simple but powerful extension of word2vec.\nIn Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 10–14,\nVancouver, Canada. Association for Computational Linguistics.\nHoward, J. and Ruder, S. (2018). Universal language model ﬁne-tuning for text classi-\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia. Association\nfor Computational Linguistics.\nHuang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. (2013). Learning deep\nstructured semantic models for web search using clickthrough data. In Proceedings of\nthe 22nd ACM international conference on Information & Knowledge Management, pages\n2333–2338.\nJi, G., He, S., Xu, L., Liu, K., and Zhao, J. (2015). Knowledge graph embedding via\ndynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages 687–696, Beijing, China. Association\nfor Computational Linguistics.\nKim, Y., Jernite, Y., Sontag, D., and Rush, A. M. (2016).\nCharacter-aware neural\nlanguage models. In Proceedings of the National Conference on Artiﬁcial Intelligence\n(AAAI).\nKöhler, W. (1947). Gestalt psychology, 2nd edn new york. NY: Liveright Publishing\nCorporation.[Google Scholar].\nKornai, A. (2002). How many words are there? Glottometrics, 4:61–86.\nKudo, T. (2018). Subword regularization: Improving neural network translation mod-\nels with multiple subword candidates. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 66–75,\nMelbourne, Australia. Association for Computational Linguistics.\n17\nYuval Pinter\nIntegrating Approaches to Word Representation\nKudo, T. and Richardson, J. (2018). SentencePiece: A simple and language indepen-\ndent subword tokenizer and detokenizer for neural text processing. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium. Association for Computational Lin-\nguistics.\nLample, G., Ballesteros, M., Subramanian, S., Kawakami, K., and Dyer, C. (2016).\nNeural architectures for named entity recognition. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 260–270, San Diego, California. Association for\nComputational Linguistics.\nLieber, R. (2005). English word-formation processes. In Handbook of\nword-formation, pages 375–427. Springer.\nLing, W., Dyer, C., Black, A. W., Trancoso, I., Fermandez, R., Amir, S., Marujo, L.,\nand Luís, T. (2015).\nFinding function in form: Compositional character models\nfor open vocabulary word representation. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, pages 1520–1530, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., and Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692.\nLuong, T., Socher, R., and Manning, C. (2013). Better word representations with recur-\nsive neural networks for morphology. In Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning, pages 104–113, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nMa, W., Cui, Y., Si, C., Liu, T., Wang, S., and Hu, G. (2020). CharBERT: Character-\naware pre-trained language model. In Proceedings of the 28th International Conference\non Computational Linguistics, pages 39–50, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nMadhyastha, P. S., Bansal, M., Gimpel, K., and Livescu, K. (2016).\nMapping un-\nseen words to task-trained embedding spaces. In Proceedings of the 1st Workshop\non Representation Learning for NLP, pages 100–110, Berlin, Germany. Association for\nComputational Linguistics.\nMelamud, O., Goldberger, J., and Dagan, I. (2016). context2vec: Learning generic\ncontext embedding with bidirectional LSTM. In Proceedings of The 20th SIGNLL\nConference on Computational Natural Language Learning, pages 51–61, Berlin, Germany.\nAssociation for Computational Linguistics.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Eﬃcient estimation of word\nrepresentations in vector space. In Proceedings of International Conference on Learning\nRepresentations.\n18\nYuval Pinter\nIntegrating Approaches to Word Representation\nMikolov, T., Yih, W.-t., and Zweig, G. (2013b). Linguistic regularities in continuous\nspace word representations. In Proceedings of the 2013 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 746–751, Atlanta, Georgia. Association for Computational Linguistics.\nMinervini, P. and Riedel, S. (2018). Adversarially regularising neural NLI models to\nintegrate logical background knowledge. In Proceedings of the 22nd Conference on Com-\nputational Natural Language Learning, pages 65–74, Brussels, Belgium. Association for\nComputational Linguistics.\nNathani, D., Chauhan, J., Sharma, C., and Kaul, M. (2019). Learning attention-based\nembeddings for relation prediction in knowledge graphs. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pages 4710–4723, Flo-\nrence, Italy. Association for Computational Linguistics.\nNavigli, R. and Ponzetto, S. P. (2010). BabelNet: Building a very large multilingual\nsemantic network. In Proceedings of the 48th Annual Meeting of the Association for\nComputational Linguistics, pages 216–225, Uppsala, Sweden. Association for Compu-\ntational Linguistics.\nNeelakantan, A., Roth, B., and McCallum, A. (2015).\nCompositional vector space\nmodels for knowledge base inference. In 2015 aaai spring symposium series.\nNickel, M. and Kiela, D. (2017). Poincaré embeddings for learning hierarchical rep-\nresentations. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems, pages 6341–6350.\nNickel, M., Tresp, V., and Kriegel, H.-P. (2011).\nA three-way model for collective\nlearning on multi-relational data. In ICML, volume 11, pages 809–816.\nOsgood, C. E. (1952). The nature and measurement of meaning. Psychological bulletin,\n49(3):197.\nPennington, J., Socher, R., and Manning, C. (2014). GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for\nComputational Linguistics.\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer,\nL. (2018).\nDeep contextualized word representations. In Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans,\nLouisiana. Association for Computational Linguistics.\nPinter, Y. and Eisenstein, J. (2018). Predicting semantic relations using global graph\nproperties. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 1741–1751, Brussels, Belgium. Association for Computational\nLinguistics.\n19\nYuval Pinter\nIntegrating Approaches to Word Representation\nPinter, Y., Guthrie, R., and Eisenstein, J. (2017). Mimicking word embeddings us-\ning subword RNNs. In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, pages 102–112, Copenhagen, Denmark. Association for\nComputational Linguistics.\nPinter, Y., Jacobs, C. L., and Bittker, M. (2020a). NYTWIT: A dataset of novel words\nin the New York Times. In Proceedings of the 28th International Conference on Compu-\ntational Linguistics, pages 6509–6515, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nPinter, Y., Jacobs, C. L., and Eisenstein, J. (2020b). Will it unblend?\nIn Findings of\nthe Association for Computational Linguistics: EMNLP 2020, pages 1525–1535, Online.\nAssociation for Computational Linguistics.\nPinter, Y., Stent, A., Dredze, M., and Eisenstein, J. (2021). Learning to look inside:\nAugmenting token-based encoders with character-level information. arXiv preprint\narXiv:2108.00391.\nPlag, I. (2018). Word-formation in English. Cambridge University Press.\nPlank, B. (2016). What to do about non-standard (or non-canonical) language in nlp.\narXiv preprint arXiv:1608.07836.\nPlank, B., Søgaard, A., and Goldberg, Y. (2016). Multilingual part-of-speech tagging\nwith bidirectional long short-term memory models and auxiliary loss. In Proceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pages 412–418, Berlin, Germany. Association for Computational\nLinguistics.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving lan-\nguage understanding by generative pre-training. Technical report, OpenAI.\nRiedel, S., Yao, L., McCallum, A., and Marlin, B. M. (2013). Relation extraction with\nmatrix factorization and universal schemas. In Proceedings of the 2013 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 74–84, Atlanta, Georgia. Association for Computational\nLinguistics.\nRubenstein, H. and Goodenough, J. B. (1965). Contextual correlates of synonymy.\nCommunications of the ACM, 8(10):627–633.\nSantos, C. D. and Zadrozny, B. (2014). Learning character-level representations for part-\nof-speech tagging. In Proceedings of the International Conference on Machine Learning\n(ICML), pages 1818–1826.\nSchlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., and Welling, M.\n(2017). Modeling relational data with graph convolutional networks. stat, 1050:17.\n20\nYuval Pinter\nIntegrating Approaches to Word Representation\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare\nwords with subword units. In Proceedings of the 54th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin,\nGermany. Association for Computational Linguistics.\nShi, B. and Weninger, T. (2017). Proje: Embedding projection for knowledge graph\ncompletion. In AAAI, volume 17, pages 1236–1242.\nShwartz, V., Santus, E., and Schlechtweg, D. (2017).\nHypernyms under siege:\nLinguistically-motivated artillery for hypernymy detection. In Proceedings of the\n15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 1, Long Papers, pages 65–75, Valencia, Spain. Association for Computational\nLinguistics.\nSigman, M. and Cecchi, G. A. (2002). Global organization of the wordnet lexicon.\nProceedings of the National Academy of Sciences, 99(3):1742–1747.\nSocher, R., Chen, D., Manning, C. D., and Ng, A. (2013).\nReasoning with neural\ntensor networks for knowledge base completion. In Advances in Neural Information\nProcessing Systems, pages 926–934.\nSutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent\nneural networks. In ICML.\nTan, S., Joty, S., Varshney, L., and Kan, M.-Y. (2020). Mind your inﬂections! Improving\nNLP for non-standard Englishes with Base-Inﬂection Encoding. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n5647–5663, Online. Association for Computational Linguistics.\nToutanova, K. and Chen, D. (2015). Observed versus latent features for knowledge\nbase and text inference.\nIn Proceedings of the 3rd Workshop on Continuous Vector\nSpace Models and their Compositionality, pages 57–66, Beijing, China. Association for\nComputational Linguistics.\nToutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., and Gamon, M. (2015).\nRepresenting text for joint embedding of text and knowledge bases. In Proceedings\nof the 2015 Conference on Empirical Methods in Natural Language Processing, pages\n1499–1509, Lisbon, Portugal. Association for Computational Linguistics.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł.,\nand Polosukhin, I. (2017). Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998–6008.\nVrandeˇci´c, D. and Krötzsch, M. (2014). Wikidata: A free collaborative knowledgebase.\nCommun. ACM, 57(10):78–85.\nWieting, J., Bansal, M., Gimpel, K., and Livescu, K. (2016). Charagram: Embedding\nwords and sentences via character n-grams. In Proceedings of the 2016 Conference on\n21\nYuval Pinter\nIntegrating Approaches to Word Representation\nEmpirical Methods in Natural Language Processing, pages 1504–1515, Austin, Texas.\nAssociation for Computational Linguistics.\nXue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and\nRaﬀel, C. (2021). Byt5: Towards a token-free future with pre-trained byte-to-byte\nmodels. arXiv preprint arXiv:2105.13626.\nYang, B., Yih, W.-t., He, X., Gao, J., and Deng, L. (2014). Embedding entities and rela-\ntions for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575.\nYoung, T., Hazarika, D., Poria, S., and Cambria, E. (2018).\nRecent trends in deep\nlearning based natural language processing. ieee Computational intelligenCe magazine,\n13(3):55–75.\nZhao, J., Mudgal, S., and Liang, Y. (2018). Generalizing word embeddings using bag of\nsubwords. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 601–606, Brussels, Belgium. Association for Computational\nLinguistics.\n22\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-09-10",
  "updated": "2021-09-10"
}