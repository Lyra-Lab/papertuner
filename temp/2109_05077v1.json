{
  "id": "http://arxiv.org/abs/2109.05077v1",
  "title": "Data Generation Method for Learning a Low-dimensional Safe Region in Safe Reinforcement Learning",
  "authors": [
    "Zhehua Zhou",
    "Ozgur S. Oguz",
    "Yi Ren",
    "Marion Leibold",
    "Martin Buss"
  ],
  "abstract": "Safe reinforcement learning aims to learn a control policy while ensuring\nthat neither the system nor the environment gets damaged during the learning\nprocess. For implementing safe reinforcement learning on highly nonlinear and\nhigh-dimensional dynamical systems, one possible approach is to find a\nlow-dimensional safe region via data-driven feature extraction methods, which\nprovides safety estimates to the learning algorithm. As the reliability of the\nlearned safety estimates is data-dependent, we investigate in this work how\ndifferent training data will affect the safe reinforcement learning approach.\nBy balancing between the learning performance and the risk of being unsafe, a\ndata generation method that combines two sampling methods is proposed to\ngenerate representative training data. The performance of the method is\ndemonstrated with a three-link inverted pendulum example.",
  "text": "arXiv:2109.05077v1  [eess.SY]  10 Sep 2021\n1\nData Generation Method for Learning a\nLow-dimensional Safe Region in Safe\nReinforcement Learning\nZhehua Zhou1, Ozgur S. Oguz2, Yi Ren3, Marion Leibold1 and Martin Buss1\nAbstract—Safe reinforcement learning aims to learn a control\npolicy while ensuring that neither the system nor the environment\ngets damaged during the learning process. For implementing safe\nreinforcement learning on highly nonlinear and high-dimensional\ndynamical systems, one possible approach is to ﬁnd a low-\ndimensional safe region via data-driven feature extraction meth-\nods, which provides safety estimates to the learning algorithm. As\nthe reliability of the learned safety estimates is data-dependent,\nwe investigate in this work how different training data will affect\nthe safe reinforcement learning approach. By balancing between\nthe learning performance and the risk of being unsafe, a data gen-\neration method that combines two sampling methods is proposed\nto generate representative training data. The performance of the\nmethod is demonstrated with a three-link inverted pendulum\nexample.\nIndex Terms—Safe Reinforcement Learning, Data Generation,\nData-driven Feature Extraction\nI. INTRODUCTION\nDeep reinforcement learning (RL) approaches have demon-\nstrated impressive achievements in various control tasks of\ndynamical systems, e.g., humanoid control [1] or robotic\nmanipulator control [2]. However, most of the RL algorithms\nare currently applied only in simulations [3], as during the\nexploration process for an optimal policy, the system may\nencounter an unsafe intermediate policy that is harmful to the\nsystem itself or to the environment. Hence, for employing RL\napproaches on real-world dynamical systems, it is critical to\ninclude safety guarantees in the learning process.\nSafe reinforcement learning (SRL) in dynamical systems\nwith continuous action space has become a popular topic\nin recent researches [4]. For model-free scenarios, safety is\nusually achieved via solving a constraint satisfaction prob-\nlem. For example, constrained policy optimization [5] adds\na constraint to the learning process that the expected return of\ncost functions should be restricted within certain predeﬁned\nlimits. Alternatively, introducing an additional risk term in the\nreward function, e.g. risk-sensitive RL [6], can also increase\nthe safety of RL algorithms. However, as no exact system\nmodel is considered in these approaches, there exists still a\n1Chair of Automatic Control Engineering, Technical University of Munich,\nMunich, Germany (e-mail: zhehua.zhou@tum.de; marion.leibold@tum.de;\nmb@tum.de).\n2Max Planck Institute for Intelligent Systems and University of Stuttgart,\nStuttgart, Germany (e-mail: ozgur.oguz@ipvs.uni-stuttgart.de).\n3Tencent\nRobotics\nX\nLab,\nTencent,\nShenzhen,\nChina\n(e-mail:\nevanyren@tencent.com).\nhigh probability that safety conditions are violated, especially\nin the early phase of the learning process.\nWhen an approximation of the system model is available,\nmore reliable safety guarantees can be realized by combining\ntechniques from model-based nonlinear control with RL ap-\nproaches. For example in [7], [8], Lyapunov functions are em-\nployed to construct a safe subregion of the state space, referred\nto as a safe region, such that by limiting the learning process\nwithin the safe region, safety conditions will never be violated.\nHowever, ﬁnding suitable candidates of Lyapunov functions\nis challenging if the system dynamics is highly nonlinear and\nhigh-dimensional. Besides, robust model predictive control can\nalso be used to provide safety and stability guarantees to\nRL algorithms [9], [10]. However, the performance of such\napproaches in general highly relies on the accuracy of the\nused system model.\nTo relax requirements on the quality of the system model,\nrecent research introduces data-driven methods for designing\na SRL approach that is based on probabilistic safety estimates.\nFor example in [11], safety in learning is modelled as a differ-\nential game. By approximating unknown external disturbances\nwith Gaussian Process models, a probabilistic safe region is\ncomputed via reachability analysis. Similarly in [12], [13], a\nsafe region is represented by the region of attraction (RoA) and\nis estimated through modeling the unknown part of system dy-\nnamics with a Gaussian Process model. The exploration of RL\nalgorithms is restricted in such a forward invariant safe region,\nsuch that safety is preserved as long as a corrective controller\nis applied when the system approaches the boundary of the\nsafe region. However, ﬁnding the safe region becomes difﬁcult\nwhen the system dynamics is highly nonlinear and high-\ndimensional (referred to as complex dynamical systems), as\nin this case performing the reachability analysis or estimating\nthe RoA via sampling are both computationally infeasible [14].\nIn order to implement SRL on complex dynamical systems,\nwhich are often the systems of interest for applying deep RL\nalgorithms, we propose a SRL framework that is based on ﬁnd-\ning a low-dimensional representation of the safe region [15].\nFor each high-dimensional system state, a low-dimensional\ncorresponding state is computed and is considered as the safety\nfeature that predicts the safety of the system. Then these\nlow-dimensional states are used to construct a reduced-order\nsafe region that approximates the original high-dimensional\nsafe region in a probabilistic form. However, determining a\nreliable low-dimensional representation of the safe region is\nchallenging, especially when a thorough understanding about\n2\nsystem dynamics is lacking. To overcome this problem, we\nintroduce a data-driven feature extraction method in [16] to\nconstruct a well-performed low-dimensional safe region. It\nassumes that, although the exact full system dynamics might\nbe unknown, a nominal system model that provides at least\nrough estimates about the system behaviour is available. Then\nby collecting data about safety of different system states\nfrom the nominal system, a low-dimensional safety feature\nis derived via learning the probabilistic similarities between\ntraining data points. The mismatch between the nominal\nand the real systems is later compensated through an online\nadaptation method. However, as the learned safety feature is\ndata-dependent, how to generate training data points that are\nmost useful to the SRL framework is an important problem to\nbe solved.\nThe contribution of this work is two-fold. First, we propose\na data generation method that is able to generate representative\ntraining data points by considering their potential inﬂuence on\nthe performance of the SRL framework. The low-dimensional\nrepresentation of the safe region learned from those data points\nachieves a satisfying balance between exploring the state space\nfor ﬁnding an optimal policy and keeping the system safe.\nSecond, we investigate how different training data points will\naffect the reliability of the derived safety estimates. Taking the\nused SRL framework as an example, we provide an insight\nabout what could be a useful way to generate training data\npoints when any other data-driven method is employed to\npredict the safety of a dynamical system.\nII. PRELIMINARIES\nIn this section, we outline a SRL framework for complex dy-\nnamical systems that is proposed in [15], [16]. The framework\nrelies on the construction of a low-dimensional representation\nof the safe region, i.e., a subregion of the state space from\nwhere the system can be controlled back to a safe state, for\nestimating the safety of different system states.\nA. SRL based on RoA\nWe consider the nonlinear control afﬁne dynamical system\nwith partially unknown dynamics as\n˙x = f(x) + g(x)u + d(x)\n(1)\nwhere x ∈X ⊆Rn is the n-dimensional system state and\nu ∈U ⊆Rm is the m-dimensional control input to the system.\nd(x) represents the unknown part of the system dynamics. For\nbrevity, we refer to this system as the real system in this work.\nSimilar as in [12], we assume that the origin of the real\nsystem (1) is a known safe state and is locally asymptotically\nstable under a given corrective controller u = K(x). A system\nstate x is said to be a safe state if the system can be controlled\nback to the origin by using the corrective controller K(x)\nwhen starting from this state. According to this, a safe region\nis deﬁned by using the RoA R of the origin with respect to\nthe corrective controller K(x) as follows.\nDeﬁnition 1. A safe region S is a closed positive invariant\nsubset of the RoA R.\nAs long as the system state x is inside the safe region S, the\nsystem can always be controlled back to a safe state, i.e., the\norigin, by applying the corrective controller K(x). Therefore,\nwe categorize system states into safe (safety label z = 1) and\nunsafe (z = 0) classes with a labeling function l(x) : X →\nZ = {1, 0} as\nl(x) = z =\n(\n1,\nif x ∈S\\{∂S}\n0,\nelse\n(2)\nwhere S\\{∂S} is the interior of the safe region S that excludes\nthe boundary ∂S. Apparently, if the safe region S is known,\nthen an ideal SRL framework can be designed based on a\nsupervisory control strategy that switches between a learning-\nbased controller π(x) and the corrective controller K(x) as\nu =\n(\nπ(x),\nif t < t∗\nK(x),\nelse\n(3)\nwhere t∗is the ﬁrst time that the system state x is considered\nas unsafe, i.e., l(x) = 0. In each learning trial, the system starts\ninside the safe region S with time t = 0 and the learning-based\ncontroller π(x) is ﬁrst applied. For keeping the system safe, the\nsupervisor (3) activates the corrective controller K(x) at time\nt = t∗. After the safety recovery, the learning environment\nis reset and the next learning trial starts again with time\nt = 0. However, in general it is computationally infeasible to\ncalculate the safe region S directly for a complex dynamical\nsystem [14], [17].\nB. SRL with a Low-dimensional Representation of the Safe\nRegion\nFor implementing the SRL framework on complex dynami-\ncal systems, an approach that is based on estimating the safety\nwith a low-dimensional representation of the safe region is\nintroduced in [15].\nBy mapping each system state x to a low-dimensional\nsimpliﬁed state, i.e., the safety feature, y ∈Y ⊆Rny, ny ≪n\nwith a state mapping y = Ψ(x), the safety of system state x is\nestimated through safety of the corresponding simpliﬁed state\ny in a probabilistic form as\nP(l(x) = 1) = Γ(y)|y=Ψ(x) ∼[0, 1]\n(4)\nwhere Γ(y) is a safety assessment function deﬁned over the\nsimpliﬁed state space Y. In [15], the state mapping y = Ψ(x)\nand the safety assessment function Γ(y) is determined by using\na physically inspired model order reduction technique.\nBy using a predeﬁned probability threshold pt, a low-\ndimensional representation of the safe region, denoted as the\nsimpliﬁed safe region Sy, is thus given in the simpliﬁed state\nspace Y as\nSy = {y ∈Y | Γ(y) > pt}\n(5)\nwhich approximates the high-dimensional safe region S. It\nleads to a SRL framework for complex dynamical systems\nthat is based on the following modiﬁed supervisor\nu =\n(\nπ(x),\nif t < t\n′\nK(x),\nelse\n(6)\n3\nwhere t\n′ is the ﬁrst time point that the system state x is\npredicted to be unsafe, i.e., Ψ(x) = y /∈Sy.\nC. Data-driven Feature Extraction\nTo overcome the limitation of physically inspired model\norder reduction, we employ a data-driven feature extraction\nmethod for identifying the simpliﬁed safe region Sy in [16].\nIt assumes that, the available knowledge about the system\ndynamics formulates a nominal system\n˙x = f(x) + g(x)u\n(7)\nDue to the highly nonlinear and high-dimensional dynamics,\ncalculating the safe region of the nominal system, denoted\nas Sn, is still computationally infeasible. However, the safety\nof each individual system state x of the nominal system can\nbe examined directly by simulating the nominal system with\nrespect to the corrective controller K(x). Hence, a dataset that\nreﬂects the safe region of the nominal system Sn is obtainable.\nBy expecting that the behavior of the nominal system will\nat least provide a prediction about the behaviour of the real\nsystem, a data-driven method is thus implemented to derive\nan initial estimate of the simpliﬁed safe region Sy for the real\nsystem.\nWe refer to the dataset obtained from the nominal system as\nthe training dataset Dtr with |Dtr| = k data points. Each data\npoint contains both the system state x at which the corrective\ncontroller is initially activated and the corresponding safety\nlabel of this state. By comparing the pairwise similarities\nbetween training data points, a method called t-Distributed\nStochastic Neighbor Embedding (t-SNE) [18] is adopted to\ncompute a realization of simpliﬁed states {y1, . . . , yk} that\nbest represents the training dataset Dtr. These simpliﬁed states\nare then used to approximate the state mapping y = Ψ(x) and\nthe safety assessment function Γ(y) for calculating an initial\nestimate of the simpliﬁed safe region Sy. See [16] for more\ndetails about learning with t-SNE.\nIII. TRAINING DATA GENERATION METHOD\nThe estimate of the simpliﬁed safe region Sy obtained from\nthe aforementioned data-driven method provides a hypothesis\nh(x) : X →Z for predicting the safety label of different real\nsystem states as\nh(x) = z =\n(\n1,\nif Ψ(x) = y ∈Sy\n0,\nif Ψ(x) = y /∈Sy\n(8)\nThe reliability of the hypothesis h(x) depends, on the one\nhand, on the magnitude of discrepancy between the nominal\nand the real systems. On the other hand, as the state mapping\ny = Ψ(x) and the safety assessment function Γ(y) are derived\nfrom the training dataset Dtr, the quality of training data points\nalso affects the performance of the hypothesis h(x). Therefore,\nwe ﬁrst investigate in this section the inﬂuence of the choice of\ntraining data on the classiﬁcation error and the performance of\nSRL framework. Then we propose a data generation method\nthat combines a uniform distribution and a multivariate normal\ndistribution to generate a training dataset Dtr that is most\nuseful to the SRL framework.\nA. Training Data vs. Classiﬁcation Error\nPredicting safety of real system states can be treated as a\nbinary classiﬁcation problem. Hence, we consider the classiﬁ-\ncation error as the ﬁrst criterion when generating the training\ndataset Dtr.\nWe assume that during the learning, all visited system states\nof the real system are drawn from an unknown distribution\nD. Meanwhile, the distribution of the nominal system states\ncontained in the training dataset Dtr is denoted as Dn. Then\nfor the learned hypothesis h(x), its classiﬁcation error on the\nreal system ǫ(h, l) (referred to as the generalization error) and\non the nominal system ǫn(h, ln) (referred to as the source\nerror) are\nǫ(h, l) = Ex∼D [I(h(x) ̸= l(x))]\n(9)\nǫn(h, ln) = Ex∼Dn [I(h(x) ̸= ln(x))]\n(10)\nwhich represent the probability that according to the distribu-\ntion D or Dn, the hypothesis h(x) disagrees with the labeling\nfunction l(x) given by the safe region S, or the labeling\nfunction ln(x) given by the safe region of the nominal system\nSn, respectively. By extending Theorem 1 given in [19] based\non the H∆H-divergence, the following theorem that bounds\nthe generalization error holds for the hypothesis h(x).\nTheorem 1. The generalization error ǫ(h, l) of hypothesis\nh(x) satisﬁes\nǫ(h, l) ≤ǫn(h, ln) + 1\n2dH∆H(D, Dn) + min{E1, E2}\n(11)\nwhere dH∆H is the H∆H-distance and we have E1\n=\nEx∼Dn [I(l(x) ̸= ln(x))] , E2 = Ex∼D [I(l(x) ̸= ln(x))].\nProof. See Appendix B.\nThe upper bound of the generalization error given in (11)\ncontains three terms: the ﬁrst term is the source error; the\nsecond term represents the divergence in distributions; the\nthird term is the difference in labeling functions and cannot\nbe changed, as it is affected only by the discrepancy between\nthe nominal and the real systems. Hence for achieving a low\ngeneralization error, Theorem 1 suggests to move closer the\ntwo distributions D and Dn while keeping the source error\nsmall. Based on this, it is motivated to generate the training\ndataset Dtr by using an accurate estimate of the unknown\ndistribution D.\nIn [16], the training dataset Dtr is generated by sampling\nsystem states with a uniform distribution (UD) Dud among\nthe entire state space. However when controlling a dynamical\nsystem, the probability that a system state x will be visited\nis affected by the system dynamics (see Section IV-B and in\nparticular Fig. 2a for an example). Hence, in general the UD\nDud is expected not to be close to the real distribution D.\nAlthough the distribution D is unknown prior to the learning\nprocess on the real system, it can be approximated by simu-\nlating the nominal system. To do this, we ﬁrst set the initial\nstate of the nominal system as the origin. Then, we control the\nnominal system with a random policy and record all system\n4\nstates observed in the system trajectory. Repeating this multi-\nple times results in a dataset X of system states that reﬂects the\nprobability that different states will be visited during control.\nA multivariate normal distribution (MND) Dmnd(µ, Σ) is then\nﬁtted to the dataset X and is considered as an approximation\nof the distribution D. Apparently, the accuracy of such an\napproximation is affected by the magnitude of discrepancy\nbetween the nominal and the real systems.\nB. Classiﬁcation Error and SRL\nThe motivation of using the MND Dmnd for generating\ntraining data points is from the prospective of reducing the\ngeneralization error. However, if the data generation is decided\nonly based on the generalization error, the performance of the\nSRL framework might be affected due to the following reason.\nThe generalization error consists of two parts\nǫ(h, l)\n=\nEx∼D [I(h(x) = 1, l(x) = 0)]\n+\nEx∼D [I(h(x) = 0, l(x) = 1)]\n(12)\nWhile the ﬁrst error type (false positive) will cause unsafe\nbehaviours of the dynamical system, the second error type\n(false negative) only means conservativeness in the SRL pro-\ncess. The purpose of SRL is to ﬁnd a satisfying learning-based\npolicy π(x) while keeping a high probability that the system is\nsafe. Hence, conservativeness is acceptable as long as a well-\nperformed policy can be learned within the subregion of state\nspace restricted by the supervisor. In that regard, considering\nonly the generalization error in the training data generation is\nlikely to deteriorate the performance of the SRL framework,\nas reducing the conservativeness usually also means a higher\nchance of encountering an unsafe system behavior.\nTherefore, for taking the performance of the SRL frame-\nwork into consideration, we in general would like to keep a\ncertain degree of conservativeness during the learning process.\nThis can be achieved by reducing our conﬁdence in con-\nsidering a system state x as safe unless enough evidence is\nprovided. In that sense, using the UD Dud for generating the\ntraining dataset Dtr is helpful. The reason is that, compared to\nthe MND Dmnd that has a majority of data points being close\nto the origin, the training data points are now placed among\nthe entire state space (see Section IV-B for an example). Thus\nthe proportion of safe data points is reduced. Note that, the\nunderlying principle of using data-driven method for making\nsafety predictions is to use known data points for estimating\nthe safety of unseen data points. Therefore, the hypothesis\nh(x) learned from the UD Dud tends to make an unsafe\nprediction, since it is less likely to ﬁnd a nearby safe training\ndata point. As a result, although the UD Dud gives a higher\ngeneralization error, it preserves the conservativeness in the\nSRL framework, which then ensures a higher probability that\nthe system is safe during the learning process.\nC. Combined Data Generation\nWhile conservativeness is able to results in a safer learning\nprocess, a satisfying policy that completes the given control\ntask might not be found if the RL algorithm is overly restricted.\nFig. 1: Three-link inverted pendulum with a target circle given in the Cartesian\nspace. The connection point between the pendulum and the ground is the\norigin of the Cartesian coordinate system. When the pendulum is at the zero\nconﬁguration, the end-effector point locates at (0, 3). The target circle has its\ncentre at (0, 2.7) and its radius as r = 0.3.\nHence for achieving a good balance between the learning\nperformance and the probability of being safe, we propose\nto divide the training dataset Dtr into two parts\nDtr = Dud + Dmnd\n(13)\nwith |Dud| = αk, |Dmnd| = (1−α)k and 0 ≤α ≤1. The sub-\ndatasets Dud and Dmnd are generated by using the UD Dud and\nthe MND Dmnd, respectively. The coefﬁcient α determines\nthe size of sub-datasets as well as the tendency of the SRL\nframework to perform exploration or to keep the safety. If the\nunknown dynamics d(x) is assumed to be small, or failure of\nthe corrective controller K(x) is considered as less critical, it\nis suggested to use a small value of α for ensuring a satisfying\nperformance of the RL algorithm, where training data points\nare mostly sampled by using known knowledge about the\nsystem trajectories. On the contrary, if it is more important\nto avoid unsafe behaviours, then a large value of α should\nbe used to keep the conservativeness in learning, i.e., most\ntraining data points are drawn from the UD.\nIV. EXPERIMENTAL RESULTS\nIn this section, we examine the inﬂuence of the combined\ndata generation method on the performance of the SRL frame-\nwork with a three-link inverted pendulum example.\nA. Experimental Setup\nWe consider a three-link inverted pendulum given as in\nFig. 1. The learning task is to ﬁnd a control policy π(x) that\nmakes the end-effector point of the pendulum track a trajectory\ngiven as a circle in the Cartesian space with an angular velocity\nwith respect to the centre of the circle as π rad/ sec. During\nthe learning, we attempt to keep the system safe by preventing\nthe ﬁrst link from hitting the ground.\nThe system state is 6-dimensional and consists of three\njoint angles and three joint angular velocities as x\n=\n[θ1, θ2, θ3, ˙θ1, ˙θ2, ˙θ3]T . The inputs u = [u1, u2, u3]T are the\ntorques applied on the three joints, where the maximal and\nminimal allowed torques are umax = 100 N m and umin =\n−100 N m for all three joints. The lengths of the links are\nset to l1 = l2 = l3 = 1 m. We assume that the masses\nare concentrated on the centre of masses that are located\nat the middle point of each link. For the nominal system,\n5\n-2\n-1\n0\n1\n2\n-10\n-5\n0\n5\n10\nSafe\nUnsafe\n(a)\n-2\n-1\n0\n1\n2\n-10\n-5\n0\n5\n10\n(b)\n-2\n-1\n0\n1\n2\n-10\n-5\n0\n5\n10\n(c)\n-20\n-10\n0\n10\n20\n-20\n-10\n0\n10\n20\n(d)\n-20\n-10\n0\n10\n20\n-20\n-10\n0\n10\n20\n(e)\n-20\n-10\n0\n10\n20\n-20\n-10\n0\n10\n20\n(f)\n(g)\n(h)\n(i)\nFig. 2: (a)-(c) Distribution of joint angle and angular velocity of the ﬁrst link in the sampled system states for α = 0, α = 0.5 and α = 1, respectively. (d)-(f)\nThe derived realization of simpliﬁed states. (g)-(i) The learned simpliﬁed safe region Sy. The output of the safety assessment function Γ(y) is represented\nby different colors.\nwe consider the masses as m1 = m2 = m3 = 1 kg.\nThe discrepancy between the nominal and the real systems\nis assumed to be caused by the mismatch in the masses of\nthe ﬁrst and the second links as m1 = m2 = ∆· 1 kg. We\nuse the Proximal Policy Optimization (PPO) [20] algorithm\nas the learning-based controller, and the corrective controller\nK(x) is a LQG controller that is derived from the nominal\nsystem. When activated, the corrective controller K(x) tries\nto control the system back to the upright conﬁguration. For the\nSRL framework, the probability threshold is set to pt = 0.8.\nEach learning condition is trained with three different seeds,\nand the averaged results are presented. The parameters used\nin the PPO algorithm are given in Appendix C.\nB. Estimate of the Simpliﬁed Safe Region\nWe ﬁrst examine the inﬂuence of the parameter α on the\nlearned simpliﬁed safe region Sy. We consider the training\ndataset Dtr with size k = 1000 and generate it with three\ndifferent values of α: α = 0 (using only the MND Dmnd), α =\n0.5, and α = 1 (using only the UD Dud). The corresponding\nresults are presented in Fig. 2.\nFig. 2a-2c show the distributions of sampled system states\ncontained in the generated training dataset Dtr by displaying\nthe joint angle and angular velocity of the ﬁrst link. As the\nlearning starts at the origin, it is more likely to observe a\nsystem state in the neighbourhood of the origin. Hence, the\nsystem states generated from the MND Dmnd are more dense\nin the subregions near the origin (see Fig. 2a). As a result,\nthe proportion of safe data points increases, since the closer\nto the origin, the higher the probability that a system state\ncan be controlled back to the upright position. Moreover,\nlimited by the natural dynamics, there exists no feasible control\nsequence to control the system to a state that simultaneously\nhas a large positive angle and a large negative angular velocity\n(right-bottom of Fig. 2a), or a large negative angle and a\nlarge positive angular velocity (left-top of Fig. 2a) of the\nﬁrst link. Therefore, no system states are sampled in those\nsubregions. For the UD Dud, the sampled system states are\nplaced among the entire state space, and as a consequence, a\nsmaller proportion of safe data points is obtained (see Fig. 2c).\nFig.\n2d-2c\nare\nthe\nrealizations\nof\nsimpliﬁed\nstates\n{y1, . . . , yk} derived by using t-SNE. The safe and unsafe\ntraining data points are clearly separated in the simpliﬁed state\nspace. The corresponding learned simpliﬁed safe region Sy\nare presented in Fig. 2g-2i. With less observed safe training\ndata points, the initial estimate of the simpliﬁed safe region\n6\n(a)\n(b)\n(c)\n1637\n 856\n  30\n8465\n 8537\n 9474\n10196\n = 0\n=0.5\n=1\nnone\n0\n2000\n4000\n6000\n8000\n10000\n12000\ntimes\nFailures\nActivated times\n(d)\n2205\n1115\n  47\n7768\n8481\n8946\n9552\n = 0\n=0.5\n=1\nnone\n0\n2000\n4000\n6000\n8000\n10000\n12000\ntimes\n(e)\n 4397\n 2381\n  452\n10028\n 9804\n 9987\n10258\n = 0\n=0.5\n=1\nnone\n0\n2000\n4000\n6000\n8000\n10000\n12000\ntimes\n(f)\nFig. 3: (a)-(c) Learning performance of the SRL framework with different simpliﬁed safe regions obtained with α = 0, α = 0.5, α = 1 as well as the case\nthat no supervisor is implemented (none), for ∆= 1.1, ∆= 1.5 and ∆= 4, respectively. (d)-(f) The total number of times that the corrective controller\nK(x) is activated and the corresponding number of failures for ∆= 1.1, ∆= 1.5 and ∆= 4, respectively.\nSy becomes more conservative and tends to make an unsafe\nprediction. Note that, the axes y1 and y2 obtained with\ndifferent training datasets have different meanings and cannot\nbe directly compared, as they are the outputs of different t-\nSNE computations.\nC. Data Generation vs. Learning Performance\nWe then examine the inﬂuence of simpliﬁed safe region Sy\nlearned from different training datasets Dtr on the performance\nof the SRL framework. We compare the performance with\nthree different levels of discrepancy between the nominal\nand the real systems as ∆= 1.1, ∆= 1.5 and ∆= 4.\nNote that, since in this work we focus only on the training\ndata generation, we use the initially learned hypothesis h(x)\nthroughout the entire learning process, such that its inﬂuence\nis better illustrated. As a baseline for comparisons, we also\ninvestigate the learning performance of implementing the PPO\ndirectly without the supervisor. The corresponding results are\nshown in Fig. 3.\nAs illustrated in Fig. 3a-3c, for all three levels of discrep-\nancy, the training dataset that uses only the UD, i.e., α = 1,\nresults in a ﬁnal policy with a lower reward, since the learning\nprocess is overly restricted. For training dataset that uses only\nthe MND (α = 0) or that combines the UD and the MND\n(α = 0.5), the SRL framework is able to ﬁnd a satisfying\nﬁnal policy. It is also worth noting that, as each learning trial is\nterminated when an unsafe behaviour is predicted to occur, an\nearly-stop functionality is introduced to the learning process\nby using the simpliﬁed safe region, which then helps with\nsearching for an optimal policy. This effect becomes more\nsigniﬁcant when the system is hard to control due to heavier\nmasses (∆= 4), where compared to the free learning case, a\nbetter ﬁnal policy is found when using the SRL framework.\nFig. 3d-3f show the total number of times that the corrective\ncontroller K(x) is activated during the entire learning process\nas well as the corresponding number of failures among these\nsafety recoveries. As a comparison, the total number of\ntimes that the safety constraint is violated during the free\nlearning case is also given. When the discrepancy is small,\ni.e., ∆= 1.1, the MND Dmnd results in a success rate of\nthe corrective controller K(x) (80.8%) that is close to the\nprobability threshold pt = 0.8. While increasing the value of α\nmakes the learning process more conservative, it also ensures\na higher probability that the system is safe, e.g. 91.0% for\nα = 0.5 and 99.7% for α = 1. Similar behaviours can also be\nobserved for ∆= 1.5 and ∆= 4, though the success rate of\nthe corrective controller K(x) decreases according to the level\nof the discrepancy. In general, the supervisor learned from the\nUD Dud is more robust to the mismatches due to its higher\nconservativeness.\nV. CONCLUSION\nIn this work, we propose a data generation method that is\nable to provide representative training data for increasing the\nperformance of a SRL framework. The method divides the\ntraining dataset into two parts and use a multivariate normal\ndistribution and a uniform distribution to generate the sub-\ndatasets, respectively. By adjusting the sizes of the two sub-\ndatasets, a balance between ﬁnding a satisfying policy and\nkeeping the system safe is achieved. The proposed data gen-\neration method gives an insight about how different training\ndata will affect the reliability of the safety estimates made\nvia data-driven methods. For future work, we intend to ﬁnd\na metric for quantifying the discrepancy between the nominal\nand the real systems, such that it can be used to guide the\ntraining data generation process.\n7\nREFERENCES\n[1] X. B. Peng, G. Berseth, K. Yin, and M. Van De Panne, “Deeploco: Dy-\nnamic locomotion skills using hierarchical deep reinforcement learning,”\nACM Trans. Graphics, vol. 36, no. 4, pp. 1–13, Jul. 2017.\n[2] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of\ndeep visuomotor policies,” J. Mach. Learn. Res., vol. 17, no. 39, pp.\n1–40, Apr. 2016.\n[3] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Bench-\nmarking deep reinforcement learning for continuous control,” in Proc.\n33rd Int. Conf. Mach. Learn. (ICML), Jun. 2016, pp. 1329–1338.\n[4] J. Garcıa and F. Fern´andez, “A comprehensive survey on safe reinforce-\nment learning,” J. Mach. Learn. Res., vol. 16, no. 42, pp. 1437–1480,\nAug. 2015.\n[5] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy\noptimization,” in Proc. 34th Int. Conf. Mach. Learn. (ICML), Aug. 2017,\npp. 22–31.\n[6] Y. Shen, M. J. Tobia, T. Sommer, and K. Obermayer, “Risk-sensitive\nreinforcement learning,” Neural Comput., vol. 26, no. 7, pp. 1298–1328,\nJul. 2014.\n[7] T. J. Perkins and A. G. Barto, “Lyapunov design for safe reinforcement\nlearning,” J. Mach. Learn. Res., vol. 3, no. Dec, pp. 803–832, Dec. 2002.\n[8] Y. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh, “A\nlyapunov-based approach to safe reinforcement learning,” in Proc. Adv.\nNeural Inf. Process. Syst. (NeurIPS), Dec. 2018, pp. 8103–8112.\n[9] C. J. Ostafew, A. P. Schoellig, and T. D. Barfoot, “Robust constrained\nlearning-based nmpc enabling reliable mobile robot path tracking,” Int.\nJ. Robot. Res., vol. 35, no. 13, pp. 1547–1563, May 2016.\n[10] M. Zanon and S. Gros, “Safe reinforcement learning using robust mpc,”\nIEEE Trans. Autom. Control, early access, Sep. 2020.\n[11] J. F. Fisac, A. K. Akametalu, M. N. Zeilinger, S. Kaynama, J. Gillula,\nand C. J. Tomlin, “A general safety framework for learning-based control\nin uncertain robotic systems,” IEEE Trans. Autom. Control, vol. 64,\nno. 7, pp. 2737–2752, Jul. 2019.\n[12] F. Berkenkamp, R. Moriconi, A. P. Schoellig, and A. Krause, “Safe\nlearning of regions of attraction for uncertain, nonlinear systems with\ngaussian processes,” in Proc. IEEE 55th Conf. Decision Control (CDC),\nDec. 2016, pp. 4661–4666.\n[13] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, “Safe model-\nbased reinforcement learning with stability guarantees,” in Proc. Adv.\nNeural Inf. Process. Syst. (NeurIPS), Dec. 2017, pp. 908–919.\n[14] J. F. Fisac, N. F. Lugovoy, V. Rubies-Royo, S. Ghosh, and C. J. Tomlin,\n“Bridging hamilton-jacobi safety analysis and reinforcement learning,”\nin Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2019, pp. 8550–\n8556.\n[15] Z. Zhou, O. S. Oguz, M. Leibold, and M. Buss, “A general framework\nto increase safety of learning algorithms for dynamical systems based\non region of attraction estimation,” IEEE Trans. Robot., vol. 36, no. 5,\npp. 1472–1490, Oct. 2020.\n[16] Z. Zhou, O. S. Oguz, M. Leibold, and M. Buss, “Learning a low-\ndimensional representation of a safe region for safe reinforcement\nlearning on dynamical systems,” IEEE Trans. Neural Netw. Learn. Syst.,\nearly access, Sep. 2021.\n[17] A. A. Ahmadi and A. Majumdar, “Dsos and sdsos optimization: more\ntractable alternatives to sum of squares and semideﬁnite optimization,”\nSIAM J. Appl. Algebra Geom., vol. 3, no. 2, pp. 193–230, Apr. 2019.\n[18] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” J. Mach.\nLearn. Res., vol. 9, no. 86, pp. 2579–2605, Nov. 2008.\n[19] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W.\nVaughan, “A theory of learning from different domains,” Mach. learn.,\nvol. 79, no. 1, pp. 151–175, Oct. 2010.\n[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.\n[Online]. Available: https://arxiv.org/abs/1707.06347\nAPPENDIX A\nLIST OF NOTATIONS\nu\nm-dimensional input\nx\nn-dimensional system state\ny\nny-dimensional simpliﬁed state\nz\nsafety label\nk\nnumber of training data points in Dtr\npt\nprobability threshold for the SRL framework\nd(x)\nunknown part of the system dynamics\nK(x)\ncorrective controller\nπ(x)\nlearning-based controller\nR\nRoA of the real system under the corrective controller\nS\nsafe region of the real system\nSn\nsafe region of the nominal system\nSy\nsimpliﬁed safe region\nl(x)\nlabeling function of the real system given by S\nln(x)\nlabeling function of the nominal system given by Sn\nh(x)\nhypothesis for predicting the safety label of real system states\ngiven by the initial estimate of Sy\nΨ(x)\nstate mapping\nΓ(y)\nsafety assessment function\nDtr\ntraining dataset\nDud\nsub-dataset generated by using Dud\nDmnd\nsub-dataset generated by using Dmnd\nX\ndataset of the observed system states for learning Dmnd\nD\ndistribution of system states of the real system\nDn\ndistribution of system states of the nominal system\nDud\nuniform distribution among the system state space\nDmnd\nmultivariate normal distribution of system states for approx-\nimating D\nAPPENDIX B\nPROOF OF THEOREM 1\nLet ǫn(h, l) = Ex∼Dn [I(h(x) ̸= l(x))], we have\nǫ(h, l)\n=\nǫ(h, l) + ǫn(h, ln) −ǫn(h, ln)\n+ǫn(h, l) −ǫn(h, l)\n≤\nǫn(h, ln) + |ǫn(h, l) −ǫn(h, ln)|\n+|ǫ(h, l) −ǫn(h, l)|\n≤\nǫn(h, ln) + Ex∼Dn [I(l(x) ̸= ln(x))]\n+|ǫ(h, l) −ǫn(h, l)|\n(14)\nAccording to Lemma 3 in [19], the following holds for any\ntwo hypothesis h1 and h2\n|ǫ(h1, h2) −ǫn(h1, h2)| ≤1\n2dH∆H(D, Dn)\n(15)\nBy considering the labeling function l(x) as a hypothesis, (14)\nbecomes\nǫ(h, l) ≤ǫn(h, ln)+Ex∼Dn [I(l(x) ̸= ln(x))]+1\n2dH∆H(D, Dn)\n(16)\nIf in the ﬁrst line we use ǫ(h, ln) = Ex∼D [I(h(x) ̸= ln(x))]\ninstead of ǫn(h, l), we have\nǫ(h, l) ≤ǫn(h, ln)+Ex∼D [I(l(x) ̸= ln(x))]+1\n2dH∆H(D, Dn)\n(17)\nCombining (16) and (17) hence gives the Theorem 1.\n8\nAPPENDIX C\nEXPERIMENTAL SETUP\nFor generating the training dataset Dtr, the sample ranges\nof each state variable used in the UD are chosen as: −π\n2 rad <\nθ1 <\nπ\n2 rad, −πrad ≤θ2, θ3 ≤πrad, −10rad/ sec ≤\n˙θ1 ≤10rad/ sec, −20rad/ sec ≤˙θ2, ˙θ3 ≤20rad/ sec. The\nfollowing reward function is used for the PPO algorithm\nR(t) = Rc −10 · ||pe(t) −pd(t)||\n(18)\nwhere Rc = 2 is a constant reward for being safe, ||pe(t) −\npd(t)|| is the Euclidean distance between the current end-\neffector position pe and the desired position pd given by\nthe trajectory of the target circle. Following parameters are\nused for the PPO: the number of steps per update is 2048,\nthe value function loss coefﬁcient is 1, the gradient norm\nclipping coefﬁcient is 10, the learning rate is 1e−4, the number\nof training epochs per update is 10, the number of training\nminibatches per update is 128, the discounting factor is 0.99,\nthe advantage estimation discounting factor is 0.95, the policy\nentropy coefﬁcient is 0, the number of hidden layers of the\nneural network is 2 with 128 neurons in each layer.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.RO",
    "cs.SY"
  ],
  "published": "2021-09-10",
  "updated": "2021-09-10"
}