{
  "id": "http://arxiv.org/abs/2109.05205v2",
  "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval",
  "authors": [
    "Jinpeng Wang",
    "Ziyun Zeng",
    "Bin Chen",
    "Tao Dai",
    "Shu-Tao Xia"
  ],
  "abstract": "The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.\nCode and configurations are publicly available at\nhttps://github.com/gimpong/AAAI22-MeCoQ.",
  "text": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval\nJinpeng Wang1,2,4, Ziyun Zeng1,4, Bin Chen2*, Tao Dai3, Shu-Tao Xia1,4\n1Tsinghua Shenzhen International Graduate School, Tsinghua University\n2Harbin Institute of Technology, Shenzhen\n3Shenzhen University\n4Research Center of Artiﬁcial Intelligence, Peng Cheng Laboratory\n{wjp20, zengzy21}@mails.tsinghua.edu.cn, chenbin2021@hit.edu.cn, daitao.edu@gmail.com, xiast@sz.tsinghua.edu.cn\nAbstract\nThe high efﬁciency in computation and storage makes hash-\ning (including binary hashing and quantization) a common\nstrategy in large-scale retrieval systems. To alleviate the re-\nliance on expensive annotations, unsupervised deep hashing\nbecomes an important research problem. This paper provides\na novel solution to unsupervised deep quantization, namely\nContrastive Quantization with Code Memory (MeCoQ).\nDifferent from existing reconstruction-based strategies, we\nlearn unsupervised binary descriptors by contrastive learn-\ning, which can better capture discriminative visual seman-\ntics. Besides, we uncover that codeword diversity regu-\nlarization is critical to prevent contrastive learning-based\nquantization from model degeneration. Moreover, we intro-\nduce a novel quantization code memory module that boosts\ncontrastive learning with lower feature drift than conven-\ntional feature memories. Extensive experiments on bench-\nmark datasets show that MeCoQ outperforms state-of-the-art\nmethods. Code and conﬁgurations are publicly available at\nhttps://github.com/gimpong/AAAI22-MeCoQ.\nIntroduction\nHashing (Wang et al. 2017) plays a key role in Ap-\nproximate Nearest Neighbor (ANN) search and has been\nwidely applied in large-scale systems to improve search\nefﬁciency. There are two technical branches in hashing,\nnamely binary hashing and quantization. Binary hashing\nmethods (Charikar 2002; Heo et al. 2012) transform data\ninto the Hamming space such that distances are measured\nquickly with bitwise operations. Quantization methods (Je-\ngou, Douze, and Schmid 2010) divide real data space into\ndisjoint cells. Then the data points in each cell are approx-\nimately represented as the centroid. Since the inter-centroid\ndistances can be pre-computed as a lookup table, quantiza-\ntion methods can efﬁciently calculate pairwise distance.\nWith the progress in deep learning, the past few years\nhave seen many deep hashing methods (Yuan et al. 2020; Liu\net al. 2016) with impressive performance. Unfortunately, an-\nnotating tons of data in real-world applications is expensive,\nmaking it hard to apply these supervised methods. Recent\nresearch interests have arisen in unsupervised deep hashing\n*Corresponding author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nto address this issue, but existing works are not satisfactory\nenough. On the one hand, most existing studies in unsuper-\nvised deep hashing focus on preserving the information from\ncontinuous features. They mostly use quantization loss (Lin\net al. 2016; Chen, Cheung, and Wang 2018) and similarity\nreconstruction loss (Shen et al. 2018) as the learning objec-\ntives, resulting in heavy reliance on the quality of extracted\nfeatures from pre-trained backbones (Krizhevsky, Sutskever,\nand Hinton 2012; Simonyan and Zisserman 2015; He et al.\n2016). If an adopted backbone generalizes poorly in the tar-\nget domain, the unsatisfactory features will degrade the out-\nput binary codes fundamentally. On the other hand, Lin et al.\n(2016); Huang et al. (2017) introduced rotation invariance\nof images to learn deep hashing, but weak negative samples\nand ineffective training schemes led to inferior performance.\nThis paper focuses on unsupervised deep quantization. To\nmake better use of unlabeled training data, we perform Con-\ntrastive Learning (CL) (Hadsell, Chopra, and LeCun 2006)\nthat learns representations by mining visual-semantic in-\nvariance from inputs (Chen et al. 2020; He et al. 2020).\nCL has become a promising direction toward deep unsu-\npervised representation, but CL-based deep quantization re-\nmains non-trivial. Speciﬁcally, we ﬁnd three challenges in\nthis task: (i) Sampling bias. Without label supervision, a\nrandomly sampled batch may contain positive samples that\nare falsely taken as negatives. (ii) Model degradation. We\nobserve that quantization codewords of the same codebook\ntend to get closer during CL, which gradually degrades the\nrepresentation ability and harms the model. (iii) The con-\nﬂict between effect and efﬁciency in training. CL beneﬁts\nfrom a large batch size that ensures enough negative sam-\nples, while a single GPU can afford a limited batch size.\nTraining by CL often requires multi-GPU synchronization,\nwhich is complex in engineering and less efﬁcient. To im-\nprove the efﬁciency, some recent studies (Wu et al. 2018;\nMisra and Maaten 2020) enable small-batch CL by caching\nembeddings in a memory bank and reusing them as nega-\ntives in later iterations. However, as the encoder updates, the\ncached embeddings will expire and affect the effect of CL.\nTo tackle the problems, we propose Contrastive Quantiza-\ntion with Code Memory (MeCoQ) that combines memory-\nbased CL and deep quantization in a mutually beneﬁcial\nframework. Speciﬁcally, (i) MeCoQ is bias-aware. We adopt\na debiased framework (Chuang et al. 2020) that can correct\narXiv:2109.05205v2  [cs.CV]  8 Mar 2022\nthe sampling bias in CL. (ii) MeCoQ avoids degeneration.\nWe ﬁnd that codeword diversity is critical to prevent the\nCL-based quantization model from degeneration. Hence, we\ndesign a codeword regularization to reinforce MeCoQ. (iii)\nMeCoQ boosts CL effectively and efﬁciently. We propose a\nnovel memory bank for quantization codes that shows lower\nfeature drift (Wang et al. 2020) than existing feature mem-\nories. Thus, it can retain cached negatives valid for a longer\nperiod and enhance the effect of CL, without heavy compu-\ntations from the momentum encoder (He et al. 2020).\nOur contributions can be summarized as follows.\n• We provide a novel solution to unsupervised deep quan-\ntization, which combines contrastive learning and deep\nquantization in a mutually beneﬁcial framework.\n• We show that codeword diversity is critical to prevent\ncontrastive deep quantization from model degeneration.\n• We propose a quantization code memory to enhance the\neffect of memory-based contrastive learning.\n• Extensive experiments on public benchmarks show that\nMeCoQ outperforms state-of-the-art methods.\nRelated Work\nUnsupervised Hashing\nTraditional unsupervised hashing\nincludes binary hashing (Charikar 2002; Weiss, Torralba,\nand Fergus 2008; Salakhutdinov and Hinton 2009; Heo et al.\n2012; Gong et al. 2012) and quantization (Jegou, Douze, and\nSchmid 2010; Ge et al. 2013; Zhang, Du, and Wang 2014;\nMorozov and Babenko 2019). Limited by the hand-designed\nrepresentations (Oliva and Torralba 2001; Lowe 2004), these\nmethods reach suboptimal performance.\nDeep hashing methods with Convolutional Neural Net-\nworks (CNNs) (Krizhevsky, Sutskever, and Hinton 2012; Si-\nmonyan and Zisserman 2015; He et al. 2016) usually per-\nform better than non-deep hashing methods. Existing deep\nhashing methods can be categorized into generative (Dai\net al. 2017; Duan et al. 2017; Zieba et al. 2018; Song et al.\n2018; Dizaji et al. 2018; Shen, Liu, and Shao 2019; Shen\net al. 2020; Li and van Gemert 2021; Qiu et al. 2021) or dis-\ncriminative (Lin et al. 2016; Huang et al. 2017; Su et al.\n2018; Chen, Cheung, and Wang 2018; Yang et al. 2018,\n2019; Tu, Mao, and Wei 2020) series. Most of them impose\nvarious constraints (i.e., loss or regularization terms) such\nas pointwise constraints: (i) quantization error (Duan et al.\n2017; Chen, Cheung, and Wang 2018), (ii) even bit distri-\nbution (Zieba et al. 2018; Shen, Liu, and Shao 2019), (iii)\nbit irrelevance (Dizaji et al. 2018), (iv) maximizing mutual\ninformation between features and codes (Li and van Gemert\n2021; Qiu et al. 2021); and pairwise constraints: (v) preserv-\ning similarity among continuous feature vectors (Su et al.\n2018; Yang et al. 2018, 2019; Tu, Mao, and Wei 2020). They\nmerely explore statistical characteristics of hash codes or fo-\ncus on preserving the semantic information from continuous\nfeatures, which leads to heavy dependence on high-quality\npre-trained features. Due to limited generalization ability,\nthe adopted CNNs may extract unsatisfactory features for\nimages from new domains, which harms generated hash\ncodes fundamentally. On the other hand, DeepBit (Lin et al.\n2016) and UTH (Huang et al. 2017) introduce rotation in-\nvariance of images to improve deep hashing. Unfortunately,\nrotation itself is not enough to construct informative negative\nsamples and the training schemes of DeepBit and UTH are\nless effective, which leads to inferior performances. Most re-\ncently, Qiu et al. (2021) combined contrastive learning with\ndeep binary hashing. By taking effective data augmentations\nand engaging more negative samples, it shows promising re-\nsults. Different from them, we explore the combination of\ncontrastive learning and deep quantization that is more chal-\nlenging. We propose a codeword diversity regularization to\nprevent model degeneration. Besides, we adopt a debiasing\nmechanism and propose a quantization code memory to en-\nhance contrastive learning, yielding better results.\nContrastive Learning\nContrastive Learning (CL) (Had-\nsell, Chopra, and LeCun 2006) based representation learn-\ning has drawn increasing attention. Wu et al. (2018) pro-\nposed an instance discrimination method that combines a\nnon-parametric classiﬁer (i.e., a memory bank) with a cross-\nentropy loss (aka InfoNCE (Oord, Li, and Vinyals 2018) or\ncontrastive loss (He et al. 2020)). Positive samples from the\nsame image are pulled closer and negative samples from\nother images are pushed apart. The subsequent instance-\nwise CL methods focus on designing end-to-end (Chen et al.\n2020), memory bank-based (Misra and Maaten 2020), or\nmomentum encoder-based (He et al. 2020) architectures. Be-\nsides, cluster-wise contrastive methods (Caron et al. 2020;\nLi et al. 2021) integrate the clustering objective into CL,\nwhich shows promising results. Moreover, there are some\nworks addressing sampling bias (Chuang et al. 2020) or ex-\nploring effective negative sampling (Robinson et al. 2021)\nin CL, which improve the training effect. We uncover that\ncodeword diversity is critical to enable CL in deep quantiza-\ntion. Besides, we propose a novel memory that stores quan-\ntization codes. Interestingly, without needing a momentum\nencoder, it can show lower feature drift (Wang et al. 2020)\nthan existing feature memories (Wu et al. 2018; Misra and\nMaaten 2020) and thus boosts CL effectively.\nModeling Framework\nProblem Formulation and Model Overview\nGiven an unlabeled training set D of ND images where each\nimage x can be ﬂattened as a P-dimensional vector, the\ngoal of unsupervised deep quantization is to learn a neu-\nral quantizer Q : RP 7→{0, 1}B that encodes images as\nB-bit semantic quantization codes (aka the binary represen-\ntations) for efﬁcient image retrieval. To this end, we propose\nContrastive Quantization with Code Memory (MeCoQ) in\nan end-to-end deep learning architecture. As shown in Fig-\nure 1, MeCoQ consists of: (i) Two operators sampled from\nthe same data augmentation family (T ∼T and T ′ ∼T ),\nwhich are applied to each training image to obtain two corre-\nlated views (xq and xk ∈RP ). (ii) A deep embedding mod-\nule h combined with a standard CNN (Simonyan and Zis-\nserman 2015) and a transform layer, which produces a con-\ntinuous embedding z ∈RD for an input view x ∈RP . (iii)\nA trainable quantization module that produces a soft quan-\ntization code vector p and the reconstruction ˆz ∈RD for\nz ∈RD. In inference, it directly outputs the hard quantiza-\ntion code vector b ∈{0, 1}B for image x. (iv) A code mem-\nBias-aware Contrastive Learning\n···\n···\n···\n(shared)\n(shared)\n(shared)\na query image \nkey images \n···\n···\n···\n···\nConvolution Neural Network \n(shared)\nTransform \nLayer \n(shared)\n···\nCode Memory\nquantized \nreconstruction\nContinuous Feature Extraction\nData Augmentation\nstack \ntogether\n<latexit sha1_base64=\"2aZ+4AFTORXmwUzfCu8qYzibI0=\">ACx3icjVHLSsNAFD2Nr1pfVZdugkVwVRJRdFl0o7sK9gFtkcl02oamSUgmxVJc+ANu9c/EP9C/8M4BbWITkhy5tx7zsy914sDP5WO85qzFhaXlfyq4W19Y3NreL2Tj2NsoSLGo+CKGl6LBWBH4qa9GUgmnEi2MgLRMbXqh4YyS1I/CGzmJRWfE+qHf8zmTimpLlt0WS07Z0cueB64BJZhVjYovaKOLCBwZRhAIQkHYEjpacGFg5i4DqbEJYR8HRe4R4G0GWUJymDEDunbp13LsCHtlWeq1ZxOCehNSGnjgDQR5SWE1Wm2jmfaWbG/eU+1p7rbhP6e8RoRKzEg9i/dLPO/OlWLRA9nugafao1o6rjxiXTXVE3t79UJckhJk7hLsUTwlwrZ32tSbVtaveMh1/05mKVXtucjO8q1vSgN2f45wH9aOye1J2ro9LlXMz6jz2sI9DmucpKrhEFTXyHuART3i2rqzIGlt3n6lWzmh28W1ZDx+tfpDN</latexit>τ\nBias-aware Contrastive \nLearning Loss\n<latexit sha1_base64=\"FhUEDpXl42aFOt/Nhxb8/g+1fWo=\">AB23icpVC7TsNAENwLr2BeBkoaiwiJKrIRgjaChjJISYgUW9H5soRTzufT3ZmHLFd0iJaf4U/oaOErcIKbkJKZrQ7u5rdWAlurO9/kNrS8srqWn3d2djc2t5xd/d6Js0wy5LRar7MTUouMSu5VZgX2mkSzwJp5cTvs396gNT2XHPimMEjqW/JYzasvS0O2EmRyhjVlmIfelF74TxZDt+E3/Rm8RFUogEV2kNXhaOUZQlKywQ1ZhD4ykY51ZYzgYUTZgYVZRM6xkEpJU3QRPns+sI7UuUzPE9lmYCMz3nFjzGcovEKJf4YB8Tau8Kx3HKiMHfQIuid9IMzpr+9WmjdVGFrcMBHMIxBHAOLbiCNnSBwTt8whd8k4g8kxfy+mutkWpmH+ZA3n4AvdB+tQ=</latexit>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!$\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!$\n<latexit sha1_base64=\"HjYy5MZtLqt31TfJnec8ED9WbY=\">AB23icpVC7TsNAEDzDOYVoKQ5ESFRWeTQNJF0FAGKS8ptqLzZQmnM/W3ZmHLFd0iJaf4U/oaOErcEIoUlAx04x2Z1ezGyaCa0PIu7W0vLK6tl7asDe3tnd2y3v7XR2nikGHxSJW/ZBqEFxCx3AjoJ8oFEoBdOLqf93h0ozWPZNo8JBEdS37DGTVFaVhu+6kcgQoVZD5eErs/5P5sFwhTqNBqtUaJk6NeJ5XLwQ59eoNF7sOmaGC5mgNy4k/ilkagTRMUK0HLklMkFlOBOQ236qIaFsQscwKSkEegm12f4+OkeAYWFLM4FZCqBbfgIRbJASZhHvzEFzm9u2XUT8zYH/Fl3Pc8cl2tNC/mYUvoEB2hE+Sic9REV6iFOoihN/SBPtGXFVhP1rP18mNdsuYzB2gB1us3C9J+/g=</latexit>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!$\nQuantization\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!$\nStep 1: Contrastive Quantization Learning\n(Best viewed in color.)\n···\n(shared)\n(shared)\ndequeue\nenqueue\nStep 2: Code Memory Update\n···\nquantization \ncodes\nCode Memory \n(after update)\nCode Memory\nfalse  \nnegative \nkeys \ntrue  \nnegative  \nkeys \npositive \nkey\nquery\nsampled images\ntraining images\nX t ∼D\nx ∈X t\nT ∼T\nT ′ ∼T\n˜\nX t\n<latexit sha1_base64=\"S+cvTyRPBqH1k1UG6yJa/uaKgSI=\">AC0nicjVHLSsNAFD2Nr1ofjbp0EyCq5KoqMuiG5cV+oK2lCSd1mBeZiZiLS7ErX/gVn9J8A/0L7wzpqAW0QmZOXPuOXfmznVi3+PCNF9z2szs3PxCfrGwtLyWtTX1hs8ShOX1d3Ij5KWY3PmeyGrC0/4rBUnzA4cnzWdixMZb16xhHtRWBOjmHUDexh6A8+1BVE9vdhxIr/PRwEtxnXvsqeXzLKphjENrAyUkI1qpL+gz4iuEgRgCGEIOzDBqevDQsmYuK6GBOXEPJUnOEWBfKmpGKksIm9oHlIu3bGhrSXOblyu3SKT39CTgPb5IlIlxCWpxkqnqrMkv0t91jlHcb0epkuQJiBc6J/cs3Uf7XJ2sRGOBI1eBRTbFiZHVuliVryJvbnypSlCGmDiJ+xRPCLvKOXlnQ3m4ql2+ra3ib0opWbl3M2Kd3Bqr/WzmdOgsVu2Dsp7Z/ulynHW6Dw2sYUd6uYhKjhFXV8Uc84VmraTfanXb/KdVymWcD34b28AFBrZWw</latexit>xq\nT ∼T\nT ′ ∼T\nT ∼T\nT ′ ∼T\nMt\n<latexit sha1_base64=\"+ic+DTe+0bLdky9juNT8F3vdQJk=\">AC0nicjVHLSsNAFD2Nr1ofjbp0EyCq5KoqMuiG5cV+oK2lCSd1tC8yEyEtrgQt/6BW/0lwT/Qv/DOmIJaRCdk5sy59yZO9eJfY8L03zNaQuLS8sr+dXC2vrGZlHf2m7wKE1cVncjP0pajs2Z74WsLjzhs1acMDtwfNZ0Rhcy3rxhCfeisCbGMesG9jD0Bp5rC6J6erHjRH6fjwNajElv1NLZtlUw5gHVgZKyEY10l/QR8RXKQIwBCEPZhg9PXhgUTMXFdTIlLCHkqznCLAnlTUjFS2MSOaB7Srp2xIe1lTq7cLp3i05+Q08A+eSLSJYTlaYaKpyqzZH/LPVU5d3GtDpZroBYgWti/LNlP/1yVoEBjhTNXhU6wYWZ2bZUnVq8ibG1+qEpQhJk7iPsUTwq5yzt7ZUB6uapdva6v4m1JKVu7dTJviHZza/1s5jxoHJatk/LR1XGpcp41Oo9d7OGAunmKCi5RV1/BFPeNZq2kS70+4/pVou8+zg29AePgA4OZWs</latexit>zk\nzq\n<latexit sha1_base64=\"Z14+0vCWK9ZKxRqO9jQSyq+QxSs=\">AC0nicjVHLSsNAFD2Nr1ofjbp0EyCq5KoqMuiG5cV+oJWSpJO29C8zEyEWlyIW/Arf6S4B/oX3hnTEtohMyc+bc+7MnevEvseFab7mtLn5hcWl/HJhZXVtvahvbDZ4lCYuq7uRHyUtx+bM90JWF57wWStOmB04Pms6ozMZb16zhHtRWBPjmF0G9iD0+p5rC6K6erHjRH6PjwNajGH3quXzLKphjELrAyUkI1qpL+gx4iuEgRgCGEIOzDBqevDQsmYuIuMSEuIeSpOMtCuRNScVIYRM7onlAu3bGhrSXOblyu3SKT39CTgO75IlIlxCWpxkqnqrMkv0t90TlHcb0+pkuQJiBYbE/uWbKv/rk7UI9HGiavCoplgxsjo3y5KqV5E3N75UJShDTJzEPYonhF3lnL6zoTxc1S7f1lbxN6WUrNy7mTbFOzi1/rZzFnQ2C9bR+WDi8NS5TRrdB7b2MEedfMYFZyjirq+COe8KzVtBvtTrv/lGq5zLOFb0N7+AbrZWg</latexit>hq\n<latexit sha1_base64=\"GIjf/fp8o2lNDPXHrNmDz4xUTYU=\">AC0nicjVHLSsNAFD2Nr1ofjbp0EyCq5KoqMuiG5cV+oK2lCSdtqF5kZkItbgQt/6BW/0lwT/Qv/DOmIJaRCdk5sy59yZO9eJfY8L03zNaQuLS8sr+dXC2vrGZlHf2m7wKE1cVncjP0pajs2Z74WsLjzhs1acMDtwfNZ0xhcy3rxmCfeisCYmMesG9jD0Bp5rC6J6erHjRH6fTwJajFv3NLZtlUw5gHVgZKyEY10l/QR8RXKQIwBCEPZhg9PXhgUTMXFdTIlLCHkqznCLAnlTUjFS2MSOaR7Srp2xIe1lTq7cLp3i05+Q08A+eSLSJYTlaYaKpyqzZH/LPVU5d0mtDpZroBYgRGxf/lmyv/6ZC0CA5ypGjyqKVaMrM7NsqTqVeTNjS9VCcoQEydxn+IJYVc5Z+9sKA9Xtcu3tVX8TSklK/dupk3xDk7tX42cx40DsvWSfno6rhUOc8ancu9nBA3TxFBZeoq46/ognPGs17Ua70+4/pVou8+zg29AePgANeZWa</latexit>hk\npk\npq\nˆzq\nˆzMt\nˆzk\nLMeCoQ\nQuantization Module \n(soft mode)\n<latexit sha1_base64=\"+ic+DTe+0bLdky9juNT8F3vdQJk=\">AC0nicjVHLSsNAFD2Nr1ofjbp0EyCq5KoqMuiG5cV+oK2lCSd1tC8yEyEtrgQt/6BW/0lwT/Qv/DOmIJaRCdk5sy59yZO9eJfY8L03zNaQuLS8sr+dXC2vrGZlHf2m7wKE1cVncjP0pajs2Z74WsLjzhs1acMDtwfNZ0Rhcy3rxhCfeisCbGMesG9jD0Bp5rC6J6erHjRH6fjwNajElv1NLZtlUw5gHVgZKyEY10l/QR8RXKQIwBCEPZhg9PXhgUTMXFdTIlLCHkqznCLAnlTUjFS2MSOaB7Srp2xIe1lTq7cLp3i05+Q08A+eSLSJYTlaYaKpyqzZH/LPVU5d3GtDpZroBYgWti/LNlP/1yVoEBjhTNXhU6wYWZ2bZUnVq8ibG1+qEpQhJk7iPsUTwq5yzt7ZUB6uapdva6v4m1JKVu7dTJviHZza/1s5jxoHJatk/LR1XGpcp41Oo9d7OGAunmKCi5RV1/BFPeNZq2kS70+4/pVou8+zg29AePgA4OZWs</latexit>zk\nzq\nˆzq\nˆzk\npk\npq\nMt\nMt+1\n<latexit sha1_base64=\"rYWeEUnouZFfCFd7yJcCLYOK0SI=\">AC0nicjVHLSsNAFD2Nr1ofjbp0EyCq5KoqMuiG5cV+oK2lCSd1tC8yEzEWlyIW/Arf6S4B/oX3hnTEtohMyc+bc+7MnevEvseFab7mtLn5hcWl/HJhZXVtvahvbDZ4lCYuq7uRHyUtx+bM90JWF57wWStOmB04Pms6ozMZb16xhHtRWBPjmHUDexh6A8+1BVE9vdhxIr/PxwEtxnVv1NLZtlUw5gFVgZKyEY10l/QR8RXKQIwBCEPZhg9PXhgUTMXFdTIhLCHkqznCLAnlTUjFS2MSOaB7Srp2xIe1lTq7cLp3i05+Q08AueSLSJYTlaYaKpyqzZH/LPVE5d3GtDpZroBYgUti/JNlf/1yVoEBjhRNXhU6wYWZ2bZUnVq8ibG1+qEpQhJk7iPsUTwq5yTt/ZUB6uapdva6v4m1JKVu7dTJviHZza/1s5ixo7Jeto/LBxWGpcpo1Oo9t7GCPunmMCs5RV1/BFPeNZq2o12p91/SrVc5tnCt6E9fAzeZWq</latexit>xk\nFigure 1: The framework of MeCoQ. After augmentation, we set one view for an image as the training query xq and leave the\nother view xk along with the views of other images as the keys in contrastive learning. Then, we extract the embeddings for\nthese images and forward them to the quantization module to get quantized reconstructions. Embeddings reconstructed from the\ncode memory bank serve as additional negative keys that boost contrastive learning. Next, we maximize the similarity between\nthe query and the positive key (i.e., the other view from the same image) and minimize the similarities of negative query-key\npairs. Finally, we update the code memory bank with the quantization codes of the views of current image batch.\nory bank M to cache the quantization code vectors of im-\nages, which serves as an additional source of negative train-\ning keys and is not involved in inference.\nDebiased Contrastive Learning for Quantization\nTrainable Quantization\nIt is hard to integrate traditional\nquantization (Jegou, Douze, and Schmid 2010) into the\ndeep learning framework because the codeword assignment\nstep is clustering-based and can not be trained by back-\npropagation. To enable end-to-end learning in MeCoQ, we\napply a trainable quantization scheme. Denote the quanti-\nzation codebooks as C = C1 × C2 × · · · × CM, where\nthe m-th codebook Cm ∈RK×d consists of K codewords\ncm\n1 , cm\n2 , · · · , cm\nK ∈Rd. Assume that z ∈RD can be di-\nvided into M equal-length d-dimensional segments, i.e., z ∈\nRD ≡[z1, · · · , zM], zm ∈Rd, d = D/M, 1 ≤m ≤M.\nGiven a vector, each codebook is used to quantize one seg-\nment respectively. In the m-th d-dimensional subspace, the\nsegment and codewords are ﬁrst normalized:\nzm ←zm/ ∥zm∥2 , cm\ni ←cm\ni / ∥cm\ni ∥2 .\n(1)\nThen each segment is quantized with codebook attention by\nˆzm = Attention(zm, Cm, Cm) =\nK\nX\ni=1\npm\ni cm\ni ,\n(2)\nwhere attention score pm\ni is computed with the α-softmax:\npm\ni = softmaxα\n\u0010\nzm⊤cm\ni\n\u0011\n=\nexp\n\u0000α · zm⊤cm\ni\n\u0001\nPK\nj=1 exp\n\u0000α · zm⊤cm\nj\n\u0001.\n(3)\nThe α-softmax is a differentiable alternative to the argmax\nthat relaxes the discrete optimization of hard codeword as-\nsignment to a trainable form. Finally, we get soft quantiza-\ntion code and the soft quantized reconstruction of z as\np = concatenate\n\u0000p1, p2, · · · , pM\u0001\n∈RKM,\n(4)\nˆz = concatenate\n\u0000ˆz1, ˆz2, · · · , ˆzM\u0001\n.\n(5)\nDebiased Contrastive Learning\nWe conduct Contrastive\nLearning (CL) based on the soft quantized reconstruction\nvectors. Because negative keys for a training query are ran-\ndomly sampled from unlabeled training set, there are un-\navoidably some false-negative keys that harm CL. To tackle\nthis problem, we adopt a bias-aware framework (Chuang\net al. 2020). The debiased CL loss is deﬁned as\nLDCL = −\n2N\nX\nq=1\nlog\nexp(\nsq,k+\nτ\n)\nexp(\nsq,k+\nτ\n) + NIn-Batch\n,\n(6)\nwhere N is the batch size, q and k+ denote the indices of the\ntraining query and the positive key in the augmented batch, τ\nis the temperature hyper-parameter. The similarity between a\nquery q and a key k is deﬁned as sq,k ≜ˆz⊤\nq ˆzk. The debiased\nin-batch negative term in Eq.(6) is deﬁned as\nNIn-Batch ≜\n2N\nX\nk−=1,\nk−/∈{q,k+}\n\"\nexp(\nsq,k−\nτ\n)\n1 −ρ+\n−ρ+ · exp(\nsq,k+\nτ\n)\n1 −ρ+\n#\n,\n(7)\nwhere k−denotes the indices of negative key and ρ+ is the\npositive prior for bias correction.\nRegularization to Avoid Model Degeneration\nModel Degeneration\nWe ﬁnd that deep quantization is\nprone to degenerate as CL goes on. To show the isuue, we re-\nduce irrelevant factors1 and train a simpliﬁed 32-bit MeCoQ\n1We have done pretest about the effect of debiasing mechanism\nto the issue. As the results showed that it does not change the phe-\nnomenon, we exclude it for a concise interpretation in this section.\n(a) Loss and MAP Curves.\n(b) Learning Rate and Average Inter-\ncodeword Similarity Curves.\n0\n500 1000 1500 2000\nTraining Iteration\n0\n5\n10\n15\nContr. Learn. Loss, LCL\n0.5\n0.6\n0.7\n0.8\nTest mAP\nw/o reg., loss\nw/ reg., loss\nw/o reg., mAP\nw/ reg., mAP\n0\n500 1000 1500 2000\nTraining Iteration\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAvg. Inter-codeword Sim.\n0\n5\n10\nLearning Rate / 10−3\nFigure 2: MeCoQ degenerates during contrastive learning\nbecause codewords in the same codebook are getting closer.\nThe codeword diversity regularization can avoid it.\nby vanilla CL on the Flickr25K dataset. As shown in Fig-\nure 2(a), the model performance (solid red line) declines\nwhile the loss (dash red line) rises with ﬂuctuation. It seems\nthat the success of CL for continuous representation doesn’t\ngeneralize to quantization directly. Considering the differ-\nence in models, we investigate the behavior of quantization\ncodebooks during CL. Intuitively, we observe the changes of\naverage inter-codeword similarity, namely,\nΩC =\n1\nMK2\nM\nX\nm=1\nK\nX\ni=1\nK\nX\nj=1\ncm\ni\n⊤cm\nj .\n(8)\nFigure 2(b) shows a monotonic increase of ΩC, which slows\ndown as the learning rate decreases. It suggests that the op-\ntimization leads to a degenerated solution.\nHere we discuss what may cause this phenomenon. Recall\nthat vanilla CL loss w.r.t. a training query xq is\nLCL(xq) = −log\nexp(\nsq,k+\nτ\n)\nexp(\nsq,k+\nτ\n) + P2N\nk−=1,\nk−/∈{q,k+}\nexp(\nsq,k−\nτ\n)\n.\n(9)\nProposition 1. Suppose that xk+ is the positive training key\nand xk−is a negative key to xq. ˆzk+ and ˆzk−are the recon-\nstructed embeddings w.r.t. xk+ and xk−, then we have\n\r\r\r\r\n∂LCL(xq)\n∂ˆzk+\n\r\r\r\r >\n\r\r\r\r\n∂LCL(xq)\n∂ˆzk−\n\r\r\r\r > 0.\n(10)\nWe prove Proposition 1 in Appendix. It suggests that the\nscale of loss gradient w.r.t. the positive key is greater than\nthat w.r.t. any negative key. Engaging more negatives leads to\na larger gap between such scales. Besides, to reduce the de-\nviation between soft quantization in training and hard quan-\ntization in inference, we take a relatively large α (10 by\ndefault) in the codeword attention (Eq.(2)). It makes each\nsoft reconstruction and assigned codeword approximate at\nthe forward step. In the back-propagation, their gradients are\nalso approximate. Thus, it can hold when replacing ˆzk+ and\nˆzk−in Proposition 1 with assigned codewords. If the query\nand the positive key are assigned to different codewords,\nthere will be a large gradient to pull these codewords closer.\nWe ﬁnd it irreversible without explicit control, because of\nthe insufﬁcient frequency of the same assignment for neg-\native pairs along with subtle gradients to push the same of\n(a) Feature Drift Estimations.\n(b) Quantization Errors.\n0\n500 1000 1500 2000\nTraining Iteration\n0\n3\n6\n9\n12\nFeature Drift / 10−2\n∆t = 1, feat\n∆t = 1, soft\n∆t = 1, hard\n∆t = 5, feat\n∆t = 5, soft\n∆t = 5, hard\n∆t =10, feat\n∆t =10, soft\n∆t =10, hard\n∆t =20, feat\n∆t =20, soft\n∆t =20, hard\n0\n250\nTraining Iteration\n1\n2\n3\nFeature Drift / 10−2\n0\n500 1000 1500 2000\nTraining Iteration\n1\n2\n3\n4\nQuant. Err., 1\nDkz −ˆzk2\n2 / 10−2\nsoft\nhard\nFigure 3: We estimate the feature drifts of original features,\nsoft quantized features, and hard quantized features. The soft\nfeatures show lower drift than the original features because\nmoderate quantization error can compensate for the drift.\ncodewords away. As a result, the representation ability of\ncodebooks degrades and the model degenerates.\nCodeword Diversity Regularization\nTo avoid degenera-\ntion, we regularize the optimization by imposing ΩC ≤ϵ,\nwhere ϵ is a ﬁxed bound. In practice, we set it as a loss term\nthat encourages codeword diversity and guides deep quanti-\nzation model to pay more attention to proper codeword as-\nsignment rather than violently moving the codewords. The\nblue lines in Figure 2(a) show that the regularization effec-\ntively avoids the issue and also helps to calm the loss down.\nQuantization Code Memory to Boost Training\nEffective CL methods rely on sufﬁcient negative samples\nto learn discriminative representations. Therefore, existing\nmemory-based methods cache the image embeddings and\nserve them as the negatives in the later training. However,\nas the model keeps updating, the early cached embeddings\nexpire and become noises to CL. To enhance the effect of\nmemory, MoCo (He et al. 2020) introduces a momentum\nencoder that mitigates the embedding aging issue at a higher\ncomputation cost. In contrast, we ﬁnd an elegant solution\nthat achieves a similar effect more efﬁciently.\nFeature Drift\nWe follow Wang et al. (2020) to investigate\nthe embedding aging issue. We deﬁne the feature drift of a\nneural embedding model h by\nDrift(X ′, t; ∆t) ≜\n1\n|X ′|\nX\nx∈X ′\n\r\rh(x; θt\nh) −h(x; θt−∆t\nh\n)\n\r\r2\n2 ,\n(11)\nwhere X ′ is a given image set for estimation, θh is the pa-\nrameters of h, t and ∆t denote the number and the inter-\nval of training iterations (i.e., batches) respectively. We train\na 32-bit CL-based quantization model and compute its fea-\nture drift based on original embeddings, soft quantized re-\nconstructions, and hard quantized reconstructions.\nAs shown in Figure 3(a), the embeddings violently change\nat the early stage, after which they become relatively stable.\nMoreover, it is surprising that the soft quantized embeddings\nshow even lower feature drifts than the original embeddings.\nWe realize that undesirable quantization error (illustrated in\nFigure 3(b)) that used to be eliminated in quantization mod-\nels partially2 offsets the feature drift, keeping the cached in-\nformation valid for a longer time.\nMemory-augmented Training\nBase on the above facts,\nwe start using a memory bank M with NM slots to\nstore soft quantization codes after the warm-up stage.\nIn each training iteration, we fetch the cached codes\npM1, pM2, · · · , pMNM from the memory bank. Then,\nwe forward them to the quantization module and respec-\ntively reconstruct the embeddings ˆzM1, ˆzM2, · · · , ˆzMNM\nby Eq.(2) and (5). Finally, we integrate these embeddings to\nEq.(6) and formulate the memory-augmented loss as\nLMeCoQ = −\n2N\nX\nq=1\nlog\nexp(\nsq,k+\nτ\n)\nexp(\nsq,k+\nτ\n) + NIn-Batch + NMemory\n,\n(12)\nwhere the added negative term about code memory is\nNMemory ≜\nNM\nX\ni=1\n\"\nexp(\nsq,Mi\nτ\n)\n1 −ρ+\n−ρ+ · exp(\nsq,k+\nτ\n)\n1 −ρ+\n#\n.\n(13)\nAt the end of each training iteration, we update the mem-\nory as a queue, i.e., the current batch is enqueued and an\nequal number of the oldest slots are removed. To simplify\nengineering, we set the queue size NM to a multiple of the\nbatch size N so that we update the memory bank by batches.\nLearning Algorithm\nThe learning objective of MeCoQ is\nmin\nθh,C E LMeCoQ + β∥θh∥2\n2 + γΩC,\n(14)\nwhere LMeCoQ is formulated as Eq.(12), ΩC is deﬁned as\nEq.(8). θh denotes the network parameters of the embedding\nmodule h, β and γ are the trade-off hyper-parameters. The\ntraining process is quite efﬁcient as we formulate the whole\nproblem in a deep learning framework. Many off-the-shelf\noptimizers can be applied within a few code lines.\nEncoding and Retrieval\nIn inference, we encode the database with hard quantization:\nim\ndb = arg max\n1≤i≤K\nzm\ndb\n⊤cm\ni ,\nˆzm\ndb = cm\nim\ndb.\n(15)\nWe can aggregate the indices\n\u0002\ni1\ndb, i2\ndb, · · · , iM\ndb\n\u0003\nand con-\nvert it into a code vector bdb for the database image xdb.\nGiven a retrieval query image xq, we extract its deep em-\nbedding and cut the vector into M equal-length segments,\ni.e., zq = [z1\nq; z2\nq; · · · ; zM\nq ], zm\nq ∈Rd. We adopt Asymmet-\nric Quantized Similarity (AQS) (Jegou, Douze, and Schmid\n2010) as the metric, which computes the similarity between\nzq and the reconstruction of a database point ˆzdb by\nAQS(xq, xdb) =\nM\nX\nm=1\nzm\nq\n⊤ˆzm\ndb\n\r\rzm\nq\n\r\r\n2\n=\nM\nX\nm=1\nzm\nq\n⊤cm\nim\ndb\n\r\rzm\nq\n\r\r\n2\n.\n(16)\n2Note that hard quantization shows higher feature drift. It\ndoesn’t beneﬁt from the offset because of large quantization error.\nWe can set up a query-speciﬁc lookup table Ξq ∈RM×K\nfor each xq, which stores the pre-computed similarities be-\ntween the segments of xq and all codewords. Speciﬁcally,\nΞm\nq,im\ndb = zm\nq\n⊤cm\nim\ndb/∥zm\nq ∥2. Hence, the AQS can be efﬁ-\nciently computed by summing chosen items from the lookup\ntable according to the quantization code, i.e.,\nAQS(xq, xdb) =\nM\nX\nm=1\nΞm\nq,im\ndb,\n(17)\nwhere im\ndb is the index of codeword in the m-th codebook.\nExperiments\nSetup\nDatasets\n(i) Flickr25K (Huiskes and Lew 2008) con-\ntains 25k images from 24 categories. We follow Li and van\nGemert (2021) to randomly pick 2,000 images as the testing\nqueries, while another 5,000 images are randomly selected\nfrom the rest of the images as the training set. (ii) CIFAR-\n10 (Krizhevsky and Hinton 2009) contains 60k images from\n10 categories. We consider two typical experiment proto-\ncols. CIFAR-10 (I): We follow Li and van Gemert (2021)\nto use 1k images per class (totally 10k images) as the test\nquery set, and the remaining 50k images are used for train-\ning. CIFAR-10 (II): Following Qiu et al. (2021) we randomly\nselect 1,000 images per category as the testing queries and\n500 per category as the training set. All images except those\nin the query set serve as the retrieval database. (iii) NUS-\nWIDE (Chua et al. 2009) is a large-scale image dataset con-\ntaining about 270k images from 81 categories. We follow\nLi and van Gemert (2021) to use the 21 most popular cate-\ngories for evaluation. 100 images per category are randomly\nselected as the testing queries while the remaining images\nform the database and the training set.\nMetrics\nWe adopt the typical metric, Mean Average Preci-\nsion (MAP), from previous works (Yang et al. 2018; Li and\nvan Gemert 2021; Qiu et al. 2021). It is deﬁned as\nMAP@N =\n1\n|Q|\nX\nxq∈Q\n PN\nn=1 Precq(n) · I{xn∈Rq}\n|Rq|\n!\n,\n(18)\nwhere Q is test query image set and n is the index of a\ndatabase image in a returned rank list. Precq(n) is the pre-\ncision at cut-off n in the rank list w.r.t. xq. Rq is the set of\nall relevant images w.r.t. xq. I{·} is an indicator function. We\nfollow previous works to adopt MAP@1000 for CIFAR-10\n(I) and (II), MAP@5000 for Flickr25K and NUS-WIDE.\nModels\nWe compare the retrieval performance of MeCoQ\nwith 24 classic or state-of-the-art unsupervised baselines,\nincluding: (i) 5 shallow hashing methods: LSH (Charikar\n2002), SpeH (Weiss, Torralba, and Fergus 2008), SH\n(Salakhutdinov and Hinton 2009), SphH (Heo et al. 2012)\nand ITQ (Gong et al. 2012). (ii) 2 shallow quantization\nmethods: PQ (Jegou, Douze, and Schmid 2010) and OPQ\n(Ge et al. 2013). (iii) 15 deep binary hashing methods: Deep-\nBit (Lin et al. 2016), UTH (Huang et al. 2017), SAH (Do\nTable 1: Mean Average Precision (MAP, %) results for different number of bits on Flickr25K, CIFAR-10 (I and II) and NUS-\nWIDE datasets. ‘D’, ‘Q’ and ‘BH’ indicate ‘Deep’, ‘Quantization’ and ‘Binary Hashing’ for short in ‘Type’ column.\nDataset →\nFlickr25K\nCIFAR-10 (I)\nCIFAR-10 (II)\nNUS-WIDE\nMethod ↓\nVenue ↓\nType ↓\n16 bits\n32 bits\n64 bits\n16 bits\n32 bits\n64 bits\n16 bits\n32 bits 64 bits\n16 bits\n32 bits 64 bits\nLSH+VGG\nSTOC’02\nBH\n56.11\n57.08\n59.26\n14.38\n15.86\n18.09\n12.55\n13.76\n15.07\n38.52\n41.43\n43.89\nSpeH+VGG\nNeurIPS’08\nBH\n59.77\n61.36\n64.08\n27.09\n29.44\n32.65\n27.20\n28.50\n30.00\n51.70\n51.10\n51.00\nSH+VGG\nIJAR’09\nBH\n60.02\n63.30\n64.17\n28.28\n28.86\n28.51\n24.68\n25.34\n27.16\n46.85\n53.63\n56.28\nSphH+VGG\nCVPR’12\nBH\n61.32\n62.47\n64.49\n26.90\n31.75\n35.25\n25.40\n29.10\n33.30\n49.50\n55.80\n58.20\nITQ+VGG\nTPAMI’12\nBH\n63.30\n65.92\n68.86\n34.41\n35.41\n38.82\n30.50\n32.50\n34.90\n62.70\n64.50\n66.40\nPQ+VGG\nTPAMI’10\nQ\n62.75\n66.63\n69.40\n27.14\n33.30\n37.67\n28.16\n30.24\n30.61\n65.39\n67.41\n68.56\nOPQ+VGG\nCVPR’13\nQ\n63.27\n68.01\n69.86\n27.29\n35.17\n38.48\n32.17\n33.50\n34.46\n65.74\n68.38\n69.12\nDeepBit\nCVPR’16\nDBH\n62.04\n66.54\n68.34\n19.43\n24.86\n27.73\n20.60\n28.23\n31.30\n39.20\n40.30\n42.90\nUTH\nACMMMW’17\nDBH\n-\n-\n-\n-\n-\n-\n-\n-\n-\n45.00\n49.50\n54.90\nSAH\nCVPR’17\nDBH\n-\n-\n-\n41.75\n45.56\n47.36\n-\n-\n-\n-\n-\n-\nSGH\nICML’17\nDBH\n72.10\n72.84\n72.83\n34.51\n37.04\n38.93\n43.50\n43.70\n43.30\n59.30\n59.00\n60.70\nHashGAN\nCVPR’18\nDBH\n72.11\n73.25\n75.46\n44.70\n46.30\n48.10\n42.81\n47.54\n47.29\n68.44\n70.56\n71.71\nGreedyHash\nNeurIPS’18\nDBH\n69.91\n70.85\n73.03\n44.80\n47.20\n50.10\n45.76\n48.26\n53.34\n63.30\n69.10\n73.10\nBinGAN\nNeurIPS’18\nDBH\n-\n-\n-\n-\n-\n-\n47.60\n51.20\n52.00\n65.40\n70.90\n71.30\nBGAN\nAAAI’18\nDBH\n-\n-\n-\n-\n-\n-\n52.50\n53.10\n56.20\n68.40\n71.40\n73.00\nSSDH\nIJCAI’18\nDBH\n75.65\n77.10\n76.68\n36.16\n40.37\n44.00\n33.30\n38.29\n40.81\n58.00\n59.30\n61.00\nDVB\nIJCV’19\nDBH\n-\n-\n-\n-\n-\n-\n40.30\n42.20\n44.60\n60.40\n63.20\n66.50\nDistillHash\nCVPR’19\nDBH\n-\n-\n-\n-\n-\n-\n-\n-\n-\n62.70\n65.60\n67.10\nTBH\nCVPR’20\nDBH\n74.38\n76.14\n77.87\n54.68\n58.63\n62.47\n53.20\n57.30\n57.80\n71.70\n72.50\n73.50\nMLS3RDUH\nIJCAI’20\nDBH\n-\n-\n-\n-\n-\n-\n-\n-\n-\n71.30\n72.70\n75.00\nBi-half Net\nAAAI’21\nDBH\n76.07\n77.93\n78.62\n56.10\n57.60\n59.50\n49.97\n52.04\n55.35\n76.86\n78.31\n79.94\nCIBHash\nIJCAI’21\nDBH\n77.21\n78.43\n79.50\n59.39\n63.67\n65.16\n59.00\n62.20\n64.10\n79.00\n80.70\n81.50\nDBD-MQ\nCVPR’17\nDQ\n-\n-\n-\n21.53\n26.50\n31.85\n-\n-\n-\n-\n-\n-\nDeepQuan\nIJCAI’18\nDQ\n-\n-\n-\n39.95\n41.25\n43.26\n-\n-\n-\n-\n-\n-\nMeCoQ (Ours)\nAAAI’22\nDQ\n81.31\n81.71\n82.68\n68.20\n69.74\n71.06\n62.88\n64.09\n65.07\n80.18\n82.16\n83.24\net al. 2017), SGH (Dai et al. 2017), HashGAN (Dizaji et al.\n2018), GreedyHash (Su et al. 2018), BinGAN (Zieba et al.\n2018), BGAN (Song et al. 2018), SSDH (Yang et al. 2018),\nDVB (Shen, Liu, and Shao 2019), DistillHash (Yang et al.\n2019), TBH (Shen et al. 2020), MLS3RDUH (Tu, Mao,\nand Wei 2020), Bi-half Net (Li and van Gemert 2021) and\nCIBHash (Qiu et al. 2021). (iv) 2 deep quantization meth-\nods: DBD-MQ (Duan et al. 2017) and DeepQuan (Chen,\nCheung, and Wang 2018). We carefully collect their results\nfrom related literature. When results about some baselines\non a certain benchmark are not available (e.g. CIBHash on\nFlickr25K dataset), we try to run their open-sourced codes\n(if available and executable) and report the results.\nImplementation Settings\nWe implement MeCoQ with\nPytorch (Paszke et al. 2019). We follow the standard eval-\nuation protocol (Qiu et al. 2021; Li and van Gemert 2021)\nof unsupervised deep hashing to use the VGG16 (Simonyan\nand Zisserman 2015). Speciﬁcally, for shallow models, we\nextract 4096-dimensional deep fc7 features as the model in-\nput. For deep models, we directly use raw image pixels as in-\nput and adopt the pre-trained VGG16 (conv1 ∼fc7) as the\nbackbone network. We use the data augmentation scheme\nin Qiu et al. (2021) that combines random cropping, hori-\nzontal ﬂipping, image graying, and randomly applied color\njitter and blur. The default hyper-parameter settings are as\nfollows. (i) We set the batch size as 128 and the maximum\nepoch as 50. (ii) The queue length (i.e., the memory bank\nsize), NM = 384. (iii) The smoothness factor of codeword\nassignment in Eq.(3), α = 10. (iv) The codeword number of\neach codebook, K = 256 such that each image is encoded\nby B = M log2 K = 8M bits (i.e., M bytes). (v) The pos-\nitive prior, ρ+ = 0.1 for CIFAR-10 (I and II), ρ+ = 0.15\nfor Flickr25K and NUS-WIDE. (vi) The starting epoch for\nthe memory module are set to 5 on Flickr25K, 10 on NUS-\nWIDE and 15 on CIFAR-10 (I and II).\nResults and Analysis\nComparison with Existing Methods\nThe MAP results in\nTables 1 show that MeCoQ substantially outperforms all the\ncompared methods. Speciﬁcally, compared with CIBHash,\na latest and strong baseline, MeCoQ achieves average MAP\nincreases of 3.52, 6.92, 2.24 and 1.46 on Flickr25K, CIFAR-\n10 (I), (II) and NUS-WIDE datasets, respectively. Besides,\nwe can get two ﬁndings from the MAP results. (i) Deep\nmethods do not always outperform shallow methods with\nCNN features. For instance, DeepBit and UTH do not out-\nperform PQ and OPQ with CNN features on NUS-WIDE.\nIt implies that without label supervision, some deep hashing\nmethods are less effective to take good advantage of pre-\ntrained CNNs. (ii) Contrastive Learning (CL) is effective to\nlearn deep hashing models. The two CL-based methods in\nTable 1, CIBHash and MeCoQ, perform best on all datasets.\nMoreover, with debiasing mechanism and code memory,\nMeCoQ shows notable improvements over CIBHash.\nComponent Analysis\nWe set 6 MeCoQ variants to anal-\nyse the contributions of components: (i) MeCoQ w/o debiasing\nremoves debiasing mechanism by setting ρ+ = 0 in Eq.(7)\nand (13); (ii) MeCoQ w/o ΩC removes codeword regulariza-\ntion, ΩC; (iii) MeCoQ w/o M removes the code memory, M;\n(iv) MeCoQ feature M replaces soft code memory by feature\nTable 2: Mean Average Precision (MAP, %) results for different MeCoQ variants with different number of bits on Flickr25K,\nCIFAR-10 (I and II) and NUS-WIDE datasets. The subscript results are the MAP drops compared with full MeCoQ.\nDataset →\nFlickr25K\nCIFAR-10 (I)\nCIFAR-10 (II)\nNUS-WIDE\nMethod ↓\n16 bits\n32 bits\n64 bits\n16 bits\n32 bits\n64 bits\n16 bits\n32 bits\n64 bits\n16 bits\n32 bits\n64 bits\nMeCoQ\n81.31\n81.71\n82.68\n68.20\n69.74\n71.06\n62.88\n64.09\n65.07\n80.18\n82.16\n83.24\nMeCoQ w/o debiasing\n80.03(↓1.28) 80.19(↓1.52) 80.67(↓2.01)\n66.59(↓1.61) 67.08(↓2.66) 68.77(↓2.29)\n60.51(↓2.37) 62.81(↓1.28) 62.85(↓2.22)\n79.64(↓0.54) 80.68(↓1.48) 82.30(↓0.94)\nMeCoQ w/o ΩC\n54.00(↓27.31) 57.53(↓24.18) 58.84(↓23.84)\n39.03(↓29.17) 36.20(↓33.54) 42.75(↓28.31)\n32.54(↓30.34) 35.67(↓28.42) 36.40(↓28.67)\n51.33(↓28.85) 57.29(↓24.87) 61.01(↓22.23)\nMeCoQ w/o M\n78.77(↓2.54) 79.50(↓2.21) 80.76(↓1.92)\n64.82(↓3.38) 67.64(↓2.10) 69.36(↓1.70)\n59.78(↓3.10) 61.22(↓2.87) 63.80(↓1.27)\n76.70(↓3.48) 79.13(↓3.03) 81.43(↓1.81)\nMeCoQ feature M\n78.39(↓2.92) 79.61(↓2.10) 80.13(↓2.55)\n65.39(↓2.81) 67.62(↓2.12) 70.57(↓0.49)\n62.55(↓0.33) 63.32(↓0.77) 64.84(↓0.23)\n77.71(↓2.47) 79.90(↓2.26) 81.64(↓1.60)\nMeCoQ hard code M\n60.77(↓20.54) 63.10(↓18.61) 68.49(↓14.19)\n33.95(↓34.25) 40.65(↓29.09) 46.76(↓24.30)\n34.40(↓28.48) 37.55(↓26.54) 44.90(↓20.17)\n69.46(↓10.72) 72.23(↓9.93) 74.95(↓8.29)\nMeCoQ w/o delaying M\n79.02(↓2.29) 78.44(↓3.27) 78.17(↓4.51)\n66.76(↓1.44) 67.82(↓1.92) 69.04(↓2.02)\n59.79(↓3.09) 61.42(↓2.67) 62.60(↓2.47)\n79.72(↓0.46) 81.39(↓0.77) 82.67(↓0.57)\n16 bits\n32 bits\n64 bits\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nPositive Prior for Debiasing, ρ+\n66\n68\n70\n72\nTest mAP (%)\n(a)\n0\n128\n384\n640\n896\n1920\nCode Memory Size, NM\n64\n66\n68\n70\n72\nTest mAP (%)\n(b)\nFigure 4: Sensitivities of ρ+ and NM on CIFAR-10 (I). The\ndotted lines indicate the MAP results of default settings.\nmemory; (v) MeCoQ hard code M replaces soft code mem-\nory by hard code memory; (vi) MeCoQ w/o delaying M begins\nmemory-augmented training at the very start of the learning.\nWe can make the following summaries based on the Table 2.\nDebiasing improves performance. MeCoQ outperforms\nMeCoQ w/o debiasing by 1.60, 2.19, 1.96 and 0.99 of average\nMAPs on Flickr25K, CIFAR-10 (I), (II) and NUS-WIDE,\nwhich shows that correcting the sampling bias can improve\nmodel training. Figure 4(a) shows that the optimal ρ+ on\nCIFAR-10 (I) is about 0.1, which means that we may ran-\ndomly drop a false-negative sample from the training set\nwith a 10% probability. It is consistent with the property of\nCIFAR-10 (I) that each category accounts for 10%.\nThe codeword diversity regularization avoids model\ndegeneration.\nMeCoQ\noutperforms\nMeCoQ w/o ΩC\nby\n25.11, 30.34, 29.14 and 25.32 of average MAPs on 4\ndatasets. It demonstrates the importance of regularization.\nSoft code memory is effective and efﬁcient to enhance\ncontrastive learning. MeCoQ outperforms MeCoQ w/o M\nby 2.22, 2.39, 2.41 and 2.77 of average MAPs on 4 datasets,\nwhich veriﬁes the worth of using M. Besides, Figure 4(b)\nshows that enlarging memory to cache more negatives im-\nproves MeCoQ, while the improvement tends to drop as\nmemory size exceeds a certain range. The reason is that the\nlow feature drift phenomenon only holds within a limited\nperiod. We can also learn that fewer bits with larger quan-\ntization errors allow a slightly larger memory because the\nquantization error can partially offset the feature drift. More-\nover, as shown in Table 3, using code memory is efﬁcient in\nGPU memory and computation. It can achieve better results\nwith much less GPU memory than enlarging batch size. The\nmarginal increase of time is caused by similarity computa-\ntions between training queries and cached negatives.\nTable 3: Model proﬁling results on CIFAR-10 (I) dataset, in-\ncluding the number of negative samples (‘#Neg.’) per train-\ning query, average training time per epoch in seconds (‘Time\n/ Ep. / sec’), GPU memory demand in megabytes (‘GPU\nMem. / MB’) and MAP (%) for 32 bits, under different batch\nsize and memory size settings. N and NM denote the batch\nsize and memory size respectively. We do these experiments\non a single NVIDIA GeForce GTX 1080 Ti (11GB) and In-\ntel® Xeon® CPU E5-2650 v4 @ 2.20GHz (48 cores).\nMethod\n#Neg.\nTime / Ep. / sec\nGPU Mem. / MB\nMAP(%)@32 bits\nN=128, w/o M\n128\n528\n5925\n67.64\nN=256, w/o M\n256\n550(↑22)\n10927(↑5002)\n68.66(↑1.02)\nN=128, NM=128\n256\n536(↑8)\n5927(↑2)\n68.39(↑0.75)\nN=128, NM=384\n512\n547(↑19)\n5933(↑8)\n69.74(↑2.10)\nN=128, NM=896\n1024\n563(↑35)\n5947(↑22)\n69.17(↑1.53)\nSoft code memory is better than feature memory and\nhard code memory. MeCoQ outperforms MeCoQ feature M\nby 2.52, 1.81, 0.44 and 2.11 of average MAPs on 4 datasets,\nbecause the soft code memory has lower feature drift than\nfeature memory. Surprisingly, MeCoQ hard code M fails. It\nseems that reconstructed features from hard codes become\nadverse noises rather than valid negatives because the large\nerror of hard quantization leads to an over-large feature drift.\nIt is better to delay the usage of memory in the learn-\ning process. MeCoQ outperforms MeCoQ w/o delaying M by\n3.36, 1.79, 2.74 and 0.6 of average MAPs on 4 datasets. It\nimplies that using code memory from the very beginning\nleads to sub-optimal solutions because reusing unstable rep-\nresentations in initial training stage is not recommended.\nConclusions\nIn this paper, we propose Contrastive Quantization with\nCode Memory (MeCoQ) for unsupervised deep quanti-\nzation. Different from existing reconstruction-based unsu-\npervised deep hashing methods, MeCoQ learns quantiza-\ntion by contrastive learning. To avoid model degeneration\nwhen optimizing MeCoQ, we introduce a codeword diver-\nsity regularization. We further improve the memory-based\ncontrastive learning by designing a novel quantization code\nmemory, which shows lower feature drift than existing fea-\nture memories without using momentum encoder. Extensive\nexperiments show the superiority of MeCoQ over the state-\nof-the-art methods. More importantly, MeCoQ sheds light\non a promising future direction to unsupervised deep hash-\ning. Potentials of contrastive learning remain to be explored.\nAcknowledgments\nThis work is supported in part by the National Key\nResearch and Development Program of China under\nGrant\n2018YFB1800204,\nthe\nNational\nNatural\nSci-\nence Foundation of China under Grant 61771273 and\n62171248, the R&D Program of Shenzhen under Grant\nJCYJ20180508152204044, and the PCNL KEY project\n(PCL2021A07).\nReferences\nCaron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.;\nand Joulin, A. 2020. Unsupervised Learning of Visual Fea-\ntures by Contrasting Cluster Assignments. In NeurIPS.\nCharikar, M. S. 2002. Similarity estimation techniques from\nrounding algorithms. In STOC, 380–388.\nChen, J.; Cheung, W. K.; and Wang, A. 2018. Learning Deep\nUnsupervised Binary Codes for Image Retrieval. In IJCAI,\n613–619.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA simple framework for contrastive learning of visual repre-\nsentations. In ICML, 1597–1607. PMLR.\nChua, T.-S.; Tang, J.; Hong, R.; Li, H.; Luo, Z.; and Zheng,\nY. 2009. Nus-wide: a real-world web image database from\nnational university of singapore. In Proceedings of the ACM\ninternational conference on image and video retrieval, 1–9.\nChuang, C.-Y.; Robinson, J.; Lin, Y.-C.; Torralba, A.; and\nJegelka, S. 2020.\nDebiased Contrastive Learning.\nIn\nNeurIPS, volume 33, 8765–8775.\nDai, B.; Guo, R.; Kumar, S.; He, N.; and Song, L. 2017.\nStochastic generative hashing. In ICML, 913–922. PMLR.\nDizaji, K. G.; Zheng, F.; Sadoughi, N.; Yang, Y.; Deng, C.;\nand Huang, H. 2018. Unsupervised deep generative adver-\nsarial hashing network. In CVPR, 3664–3673.\nDo, T.-T.; Le Tan, D.-K.; Pham, T. T.; and Cheung, N.-M.\n2017.\nSimultaneous feature aggregating and hashing for\nlarge-scale image search. In CVPR, 6618–6627.\nDuan, Y.; Lu, J.; Wang, Z.; Feng, J.; and Zhou, J. 2017.\nLearning deep binary descriptor with multi-quantization. In\nCVPR, 1183–1192.\nGe, T.; He, K.; Ke, Q.; and Sun, J. 2013. Optimized product\nquantization for approximate nearest neighbor search.\nIn\nCVPR, 2946–2953.\nGong, Y.; Lazebnik, S.; Gordo, A.; and Perronnin, F. 2012.\nIterative quantization: A procrustean approach to learning\nbinary codes for large-scale image retrieval. TPAMI, 35(12):\n2916–2929.\nHadsell, R.; Chopra, S.; and LeCun, Y. 2006. Dimension-\nality reduction by learning an invariant mapping. In CVPR,\nvolume 2, 1735–1742. IEEE.\nHe, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In CVPR, 9729–9738.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nHeo, J.-P.; Lee, Y.; He, J.; Chang, S.-F.; and Yoon, S.-E.\n2012. Spherical hashing. In CVPR, 2957–2964. IEEE.\nHuang, S.; Xiong, Y.; Zhang, Y.; and Wang, J. 2017. Un-\nsupervised triplet hashing for fast image retrieval. In ACM\nMM Workshops, 84–92.\nHuiskes, M. J.; and Lew, M. S. 2008. The mir ﬂickr retrieval\nevaluation.\nIn Proceedings of the 1st ACM international\nconference on Multimedia information retrieval, 39–43.\nJegou, H.; Douze, M.; and Schmid, C. 2010. Product quanti-\nzation for nearest neighbor search. TPAMI, 33(1): 117–128.\nKrizhevsky, A.; and Hinton, G. 2009. Learning multiple lay-\ners of features from tiny images. Technical report, Univer-\nsity of Toronto, Toronto, Ontario.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. In NeurIPS, volume 25, 1097–1105.\nLi, J.; Zhou, P.; Xiong, C.; and Hoi, S. 2021. Prototypical\nContrastive Learning of Unsupervised Representations. In\nICLR.\nLi, Y.; and van Gemert, J. 2021. Deep Unsupervised Image\nHashing by Maximizing Bit Entropy. In AAAI.\nLin, K.; Lu, J.; Chen, C.-S.; and Zhou, J. 2016. Learning\ncompact binary descriptors with unsupervised deep neural\nnetworks. In CVPR, 1183–1192.\nLiu, H.; Wang, R.; Shan, S.; and Chen, X. 2016. Deep super-\nvised hashing for fast image retrieval. In CVPR, 2064–2072.\nLowe, D. G. 2004. Distinctive image features from scale-\ninvariant keypoints. IJCV, 60(2): 91–110.\nMisra, I.; and Maaten, L. v. d. 2020. Self-supervised learning\nof pretext-invariant representations. In CVPR, 6707–6717.\nMorozov, S.; and Babenko, A. 2019. Unsupervised neural\nquantization for compressed-domain similarity search. In\nICCV, 3036–3045.\nOliva, A.; and Torralba, A. 2001. Modeling the shape of\nthe scene: A holistic representation of the spatial envelope.\nIJCV, 42(3): 145–175.\nOord, A. v. d.; Li, Y.; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. PyTorch: An imperative style, high-performance\ndeep learning library. In NeurIPS, 8024–8035.\nQiu, Z.; Su, Q.; Ou, Z.; Yu, J.; and Chen, C. 2021. Unsuper-\nvised Hashing with Contrastive Information Bottleneck. In\nIJCAI.\nRobinson, J. D.; Chuang, C.-Y.; Sra, S.; and Jegelka, S.\n2021. Contrastive Learning with Hard Negative Samples.\nIn ICLR.\nSalakhutdinov, R.; and Hinton, G. 2009.\nSemantic hash-\ning. International Journal of Approximate Reasoning, 50(7):\n969–978.\nShen, F.; Xu, Y.; Liu, L.; Yang, Y.; Huang, Z.; and Shen,\nH. T. 2018.\nUnsupervised deep hashing with similarity-\nadaptive and discrete optimization. TPAMI, 40(12): 3034–\n3044.\nShen, Y.; Liu, L.; and Shao, L. 2019.\nUnsupervised bi-\nnary representation learning with deep variational networks.\nIJCV, 127(11): 1614–1628.\nShen, Y.; Qin, J.; Chen, J.; Yu, M.; Liu, L.; Zhu, F.; Shen, F.;\nand Shao, L. 2020. Auto-encoding twin-bottleneck hashing.\nIn CVPR, 2818–2827.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nICLR.\nSong, J.; He, T.; Gao, L.; Xu, X.; Hanjalic, A.; and Shen,\nH. T. 2018. Binary generative adversarial networks for im-\nage retrieval. In AAAI.\nSu, S.; Zhang, C.; Han, K.; and Tian, Y. 2018. Greedy hash:\nTowards fast optimization for accurate hash coding in cnn.\nIn NeurIPS, 806–815.\nTu, R.-C.; Mao, X.-L.; and Wei, W. 2020. MLS3RDUH:\nDeep Unsupervised Hashing via Manifold based Local Se-\nmantic Similarity Structure Reconstructing. In IJCAI, 3466–\n3472.\nWang, J.; Zhang, T.; Sebe, N.; Shen, H. T.; et al. 2017. A\nsurvey on learning to hash. TPAMI, 40(4): 769–790.\nWang, X.; Zhang, H.; Huang, W.; and Scott, M. R. 2020.\nCross-batch memory for embedding learning.\nIn CVPR,\n6388–6397.\nWeiss, Y.; Torralba, A.; and Fergus, R. 2008. Spectral hash-\ning. In NeurIPS, 1753–1760.\nWu, Z.; Xiong, Y.; Yu, S. X.; and Lin, D. 2018. Unsuper-\nvised feature learning via non-parametric instance discrimi-\nnation. In CVPR, 3733–3742.\nYang, E.; Deng, C.; Liu, T.; Liu, W.; and Tao, D. 2018. Se-\nmantic structure-based unsupervised deep hashing. In IJ-\nCAI, 1064–1070.\nYang, E.; Liu, T.; Deng, C.; Liu, W.; and Tao, D. 2019. Dis-\ntillhash: Unsupervised deep hashing by distilling data pairs.\nIn CVPR, 2946–2955.\nYuan, L.; Wang, T.; Zhang, X.; Tay, F. E.; Jie, Z.; Liu, W.;\nand Feng, J. 2020. Central similarity quantization for efﬁ-\ncient image and video retrieval. In CVPR, 3083–3092.\nZhang, T.; Du, C.; and Wang, J. 2014. Composite quanti-\nzation for approximate nearest neighbor search. In ICML,\n838–846. PMLR.\nZieba, M.; Semberecki, P.; El-Gaaly, T.; and Trzcinski, T.\n2018. BinGAN: learning compact binary descriptors with a\nregularized GAN. In NeurIPS, 3612–3622.\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.IR"
  ],
  "published": "2021-09-11",
  "updated": "2022-03-08"
}