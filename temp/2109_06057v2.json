{
  "id": "http://arxiv.org/abs/2109.06057v2",
  "title": "Unsupervised Person Re-Identification: A Systematic Survey of Challenges and Solutions",
  "authors": [
    "Xiangtan Lin",
    "Pengzhen Ren",
    "Chung-Hsing Yeh",
    "Lina Yao",
    "Andy Song",
    "Xiaojun Chang"
  ],
  "abstract": "Person re-identification (Re-ID) has been a significant research topic in the\npast decade due to its real-world applications and research significance. While\nsupervised person Re-ID methods achieve superior performance over unsupervised\ncounterparts, they can not scale to large unlabelled datasets and new domains\ndue to the prohibitive labelling cost. Therefore, unsupervised person Re-ID has\ndrawn increasing attention for its potential to address the scalability issue\nin person Re-ID. Unsupervised person Re-ID is challenging primarily due to\nlacking identity labels to supervise person feature learning. The corresponding\nsolutions are diverse and complex, with various merits and limitations.\nTherefore, comprehensive surveys on this topic are essential to summarise\nchallenges and solutions to foster future research. Existing person Re-ID\nsurveys have focused on supervised methods from classifications and\napplications but lack detailed discussion on how the person Re-ID solutions\naddress the underlying challenges. This survey review recent works on\nunsupervised person Re-ID from the perspective of challenges and solutions.\nSpecifically, we provide an in-depth analysis of highly influential methods\nconsidering the four significant challenges in unsupervised person Re-ID: 1)\nlacking ground-truth identity labels to supervise person feature learning; 2)\nlearning discriminative person features with pseudo-supervision; 3) learning\ncross-camera invariant person feature, and 4) the domain shift between\ndatasets. We summarise and analyse evaluation results and provide insights on\nthe effectiveness of the solutions. Finally, we discuss open issues and suggest\nsome promising future research directions.",
  "text": "Unsupervised Person Re-Identiﬁcation: A Systematic Survey of\nChallenges and Solutions\nXiangtan Lina, Pengzhen Renb, Chung-hsing Yeha, Lina Yaoc, Andy Songd and Xiaojun Changd,∗\naMonash University, Wellington Road, Clayton, 3800, Victoria, Australia\nbNorthwest University, Xi’an, Shaanxi, China\ncUniversity of New South Wales, Sydney, 2052, New South Wales, Australia\ndRMIT University, 124 La Trobe St, Melbourne, 3000, Victoria, Australia\nA R T I C L E I N F O\nKeywords:\nPerson re-identiﬁcation\nUnsupervised person re-identiﬁcation\nDeep learning\nFeature representation learning\nA B S T R A C T\nPerson re-identiﬁcation (Re-ID) has been a signiﬁcant research topic in the past decade due to its\nreal-world applications and research signiﬁcance. While supervised person Re-ID methods achieve\nsuperior performance over unsupervised counterparts, they can not scale to large unlabelled datasets\nand new domains due to the prohibitive labelling cost. Therefore, unsupervised person Re-ID\nhas drawn increasing attention for its potential to address the scalability issue in person Re-ID.\nUnsupervised person Re-ID is challenging primarily due to lacking identity labels to supervise person\nfeature representation learning. The corresponding solutions are diverse and complex, with various\nmerits and limitations. Therefore, comprehensive surveys on this topic are essential to summarise\nchallenges and solutions to foster future research. Existing person Re-ID surveys have focused on\nsupervised methods from classiﬁcations and applications. Still, they lack detailed discussion on how\nthe person Re-ID solutions address the underlying person Re-Id challenges. This survey review recent\nworks on unsupervised person Re-ID from the perspective of challenges and solutions. Speciﬁcally, we\nprovide an in-depth analysis of highly inﬂuential methods considering the four signiﬁcant challenges\nin unsupervised person Re-ID: 1) lacking ground-truth identity labels to supervise person feature\nlearning; 2) learning discriminative person features with pseudo-supervision; 3) learning cross-\ncamera invariant person features and 4) the domain gap between datasets. We summarise and analyze\nevaluation results and provide insights on the eﬀectiveness of the solutions. Finally, we discuss open\nissues and suggest some promising future research directions.\n1. Introduction\nPerson re-identiﬁcation (Re-ID) aims to ﬁnd the query\nperson from a collection of person images or videos captured\nby non-overlapping cameras in a distributed multi-camera\nsystem [1, 2, 3]. Person Re-ID has wide real-life appli-\ncations, such as criminals search, multi-camera tracking,\nmissing person search etc. The essential task of person Re-\nID is to learn the discriminative person features and associate\nthe query with the best matching person in the gallery images\nor videos. Historically, early person Re-ID models exploited\nhandcrafted person features such as colours and textures\nand focused on metric learning to align the query person\nwith the most similar person in the gallery images [4, 5,\n6]. However, handcrafted features are ineﬀective in large-\nscale applications because they can not capture the dynamic\nperson features in an extensive collection of gallery images.\nTherefore, the handcrafted feature engineering approach was\nsuperseded by CNN-based deep learning since 2014 [7] as\nthe latter dominated computer vision research.\nMost deep learning person Re-ID models are trained\nwith labelled data in a single domain. With CNN-based deep\nlearning, supervised person Re-ID has achieved impressive\naccuracy in the commonly used benchmark datasets. In\nsupervised person Re-ID, with suﬃcient pairwise labelled\n* Corresponding author\nxiangtan.lin@gmail.com (X. Lin); pzhren@foxmail.com (P. Ren);\nChung-hsing.Yeh@monash.edu (C. Yeh); lina.yao@unsw.edu.au (L. Yao);\nandy.song@rmit.edu.au (A. Song); xiaojun.chang@rmit.edu.au (X. Chang)\ndata, researchers have focused on designing novel network\nstructures and eﬃcient loss functions to learn cross-camera\ndiscriminative feature representation, which greatly im-\nproves person Re-ID performance. For instance, the Rank-1\nbenchmark of the single query search on the Market-1501 [8]\ndataset has increased from 44.4% [8] at the time of release\nto 98.5% [9] in 2020. The Rank-1 evaluation result of the\nDukeMTMC-Re-ID [10] dataset has been lifted from only\n30.8% [10] in 2017 to over 95% [9] in 2020.\nThe deep learning-based methods are data-driven and\nrequire many labelled data, which incur signiﬁcant labour\nand computational costs for annotating data. Therefore su-\npervised person Re-ID can not scale to large-scale datasets\nwith prohibitive labelling costs. Moreover, those costly mod-\nels trained on one dataset can not be directly deployed to\nother environments on the unlabeled datasets due to signiﬁ-\ncant photometric and geometric variations between datasets,\nhindering the person Re-ID performance. To address the\nscalability issue of supervised person Re-ID, in recent years,\nresearchers have been focusing on unsupervised person Re-\nID to train Re-ID models with abundant unlabelled data.\nUnsupervised person Re-Id is not a new research topic\nin the computer vision community. Early attempts using\nhandcrafted features and silence learning\n[11] failed to\nproduce satisfactory Re-ID results due to limited discrimi-\nnative power in the person feature descriptors. It is the deep\nlearning-based approach that delivers steady improvement\nin unsupervised person Re-ID performance. Therefore, this\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 1 of 20\narXiv:2109.06057v2  [cs.CV]  2 Oct 2021\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nsurvey focuses on deep learning-based methods in unsuper-\nvised person Re-Id.\n1.1. Motivation and Related Surveys\nThe performance of unsupervised person Re-ID is gener-\nally inferior to supervised person Re-ID due to lacking pair-\nwise labelled data to learn camera-invariant person feature\nrepresentation. In recent years, unsupervised person Re-ID\nhas remarkably closed the performance gap with supervised\nperson Re-ID. Therefore, unsupervised person Re-ID has\ndrawn increasing attention in the last couple of years due\nto its scalable applications. A d diversity of solutions are\nproposed with various merits and limitations. To foster fu-\nture research in this area, systematic surveys of unsupervised\nperson Re-ID methods are essential. Recent person Re-ID\nsurveys have paid little attention to unsupervised person Re-\nID methods and lack comprehensive and in-depth analysis\nof the challenges and solutions. Only a few image-based\nunsupervised person Re-ID works are covered in recent\nperson Re-ID surveys. To ﬁll the gap, we conduct a com-\nprehensive review of the diverse solutions of unsupervised\nperson Re-ID covering both image-based and video-based\nmethods. Speciﬁcally, We surveyed recently published and\npre-print publications of unsupervised person Re-Id from\ntop conferences and journals. Most current Re-ID surveys\nreview methods from the perspective of applications and\nsolutions that lack in-depth analysis of the rationals be-\nhind the solutions. To inspire new ideas to further advance\nunsupervised person Re-ID, we discuss the common chal-\nlenges of unsupervised person Re-ID and provide insights\non how the challenges are addressed in various solutions\nwith convincing evaluation results. We summarise the main\ndiﬀerences between this survey and related deep learning-\nbased person Re-ID surveys in Table 1.\n1.2. Contribution\nMost existing Re-ID surveys emphasize the classiﬁca-\ntion of diverse person Re-ID methods but lack cohesive\nanalysis of how the solutions correspond to the underlying\nchallenges. This survey aims to analyze the unsupervised\nperson Re-ID methods from the perspective of challenges\nand solutions. Therefore, the rationals behind the solutions\ncan be easily grasped to inspire new ideas. Speciﬁcally,\nWe surveyed recently published and pre-print publications\nof unsupervised person Re-ID from top conferences and\njournal articles. We discuss the methods from the challenges\nand solutions perspective. We summarise and analyze eval-\nuation results accordingly. We discuss open issues that lead\nto promising future research directions with an in-depth\njudgment of the challenges and corresponding solutions. The\nmain contributions of this survey are summarised as follows:\n• We emphasize unsupervised person Re-ID, which ad-\ndresses the scalability issue of person Re-ID. This is\nthe ﬁrst survey dedicated to unsupervised person Re-\nID, as far as we know.\nTable 1\nSummary of the main diﬀerences between the related person\nRe-ID surveys and this survey.\nSurvey Covering\nAnalysis\n[12]\nHandcrafted\nperson Re-ID\nDeep learning\nperson Re-ID\nApplications and methods\n[13,\n14, 15,\n16, 17,\n18, 19,\n20]\nDeep learning\nperson Re-ID\n(mainly\nsupervised)\nApplications and methods\nOurs\nUnsupervised\nperson Re-ID\nImage-based\nand\nvideo-\nbased\n(Challenges: Solutions)\nLacking ground-truth identity labels:\nPseudo-label estimation\nLearning discriminative person features:\nDeep feature representation learning\nLearning camera-invariant person fea-\ntures\nCamera-aware invariance learning\nDomain gap between datasets:\nUnsupervised domain adaptation\n• We analyze unsupervised person Re-ID methods from\nthe perspective of challenges and solutions. Most cur-\nrent Re-ID surveys review methods from the per-\nspective of applications and solutions that lack in-\ndepth analysis of the rationals behind the ideas. To\nfoster new ideas, we discuss the main challenges of\nunsupervised person Re-ID and provide insights on\nhow the challenges are addressed.\n• We summarise and analyze existing unsupervised per-\nson Re-ID methods’ performance and provide insights\non promising future research directions.\n1.3. Organisation of the Survey\nIn this survey, we ﬁrst overview the person Re-ID prob-\nlem in Section 2 to set the context and highlight the essence\nof unsupervised person Re-ID. We then conduct a compre-\nhensive analysis of unsupervised person Re-ID from the per-\nspective of challenges and solutions in Section 3. After that,\nin Section 4, we analyze the performance of representative\nsolutions in response to the challenges. In Section 5, we\ndiscuss open issues that lead to promising future directions\nof unsupervised person Re-ID. Finally, we conclude the\nsurvey with the key takeaway messages in Section 6.\n2. Overview of Person Re-Identiﬁcation\nPerson re-identiﬁcation has been a signiﬁcant research\ntopic for over a decade due to its real-life applications,\nsuch as criminals searches, missing person searches etc.\nPerson Re-ID heavily relies on visually similar person ap-\npearance from disjoint camera views to match the query\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 2 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nFigure 1:\nA typical person re-identiﬁcation system. A person Re-ID dataset is typically collected from a multi-camera system\nand curated for training and testing a Re-ID model. During training, the Re-ID model is to learn person feature representation.\nDuring testing, a query person is given to ﬁnd the matching person in the gallery images. The output is a ranked list of the best\nmatching person.\nperson with the person images. Photometric and geometric\nvariations in person appearance across cameras pose the\ngreatest challenge. Most person Re-ID works have concen-\ntrated on supervised learning in various settings on small\nto large datasets. Occluded person re-identiﬁcation (Re-\nID) aims to match partial and whole person images across\ncamera views [21, 22, 23, 24, 25]. Long-term person Re-\nId [26] aims to ﬁnd the query person over a long period\nwith changing clothes [26]. Homogeneous person Re-ID\nfor single modality data such as Image-based and video-\nbased person Re-ID have been wildly studied due to their\nsimplicity. Heterogeneous person Re-ID aims to ﬁnd the\nquery person in cross-modality data [27, 28], such as RGB-\ninfrared person Re-ID [29].\nPerson Re-ID aims to ﬁnd the query person from a\ncollection of person images or videos captured by multi-\nple disjoint cameras. Person Re-ID is an application of a\nmulti-camera system from where the training and testing\nimages/videos are sourced. The core part of the Re-ID\nsystem is to learn a Re-ID model to extract discriminative\nperson features, which minimizes the distance with the query\nperson. A typical person Re-ID system is illustrated in Figure\n1. The overall objective of the Re-ID model can be expressed\nas:\nmin\n휃,푤\n푁\n∑\n푖=1\n퓁(푓(푤; 휙(휃; 푥푖)), 푦푖).\n(1)\nwhere 푥푖is the person feature vector, 휙(.) is a function to\nlearn the feature representation of 푥푖, 휃are the learnable\nparameters in 휙(.). 푓(.) is the classiﬁcation function that\ntakes input the person feature representation 휙(.) and outputs\nthe classiﬁcation label. 푤are trainable weights in 푓(.). 푦푖\nis the ground-truth identity label to supervise learning the\nfeature extraction function 휙(.). 퓁is the loss between the\nproject label 푓(.) and the ground-truth 푦푖. Therefore, the\noverall objective is to minimise the classiﬁcation losses\nbetween predicted and ground-truth identity labels.\nAs illustrated in Equation 1, the core part of the person\nRe-ID model system is to learn person feature representa-\ntions so that two people have similar features can be regarded\nas the same identity. To learn suitable person feature repre-\nsentations, historically, prior to CNN-based deep learning,\nnumerous hand-crafted features are exploited for person Re-\nID, in particular the color and texture features, such as\nHSV color histogram [30, 31], LAB color histogram [32],\nLBP histogram [31], Gabor features [31] and SIFT [32].\nIn a handcrafted Re-ID system, such as LOMO [33] and\nBOW [8], it’s critical to learn a suitable distance metric to\nclose the gap between the query person and the handcrafted\nfeatures since the handcrafted features can not capture the\ndynamic feature variance in the sample space. Therefore,\nthe handcrafted methods are only suitable for small datasets\nand fail to fully exploit the data distribution to learn the\nappropriate feature representations for large-scale datasets.\nDeep learning-based methods [34, 35, 36, 37, 38] for\nfeature extraction have shown substantial advantage over\nhard-crafted features because they fully exploit the data\ndistribution of dynamic person features. Since 2014, deep\nlearning-based supervised Re-ID methods have matured and\nhave achieved over 95% Rank-1 accuracy in several Re-ID\ndatasets. Researchers have been working on novel network\nstructures and eﬀective training algorithms, coupled with ef-\nfective global and local person features [39, 40] and eﬃcient\nloss functions etc. For example, a Domain Guided Dropout\nlayer\n[41] is proposed to illuminate useless neurons to\nlearn global person representations from multiple domains.\nA multi-scale triplet CNN [42] is proposed to extract multi-\nscale person features with a comparative similarity loss on\ntriplet sample. [43] proposed an RNN architecture to exploit\nspatial and temporal information in videos to learn person\nfeatures for video-based person Re-ID. [38] jointly learns\nlocal and global features with multiple classiﬁcation losses.\nAlthough these methods perform well on the single\ndomain dataset when directly testing a model trained with\nthe source dataset on the new target dataset, performance\nwould drop dramatically, simply because the model hasn’t\nseen the variations of the person features in the new domain.\nFor a Re-ID model to generalize to a new environment, some\nworks such as DIMN [44] is trained with several publicly\nlabelled datasets. Person representation is averaged over\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 3 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nFigure 2: Timeline of person Re-ID with representative papers. Above the timeline are some representative supervised person\nRe-ID works. Below the line are unsupervised person Re-ID works. Shallow dots indicate hand-crafted person Re-ID. Solid dots\nare deep learning-based person Re-Id.\nmultiple representations hoping that it would work in the\nunseen domain. DIMN deliberately uses large datasets as\nthe source domains and the smaller ones as target domains.\nHowever, this doesn’t address the labelled data scarcity\nproblem. Testing on small datasets may work well as the\nmodel has seen similar features in the large dataset. Still, it\ndoesn’t guarantee that the model would generalize to other\nlarge datasets with signiﬁcant variance in feature space.\nThis generalization performance gap has led many re-\nsearchers to unsupervised person Re-ID. Unsupervised per-\nson Re-ID has been investigated around the same time\nas supervised person Re-ID. Early works before the deep\nlearning era mainly focus on how to construct eﬀective\nfeature representations manually. Traditional unsupervised\nperson re-id studies have mainly focused on feature engi-\nneering [30, 45, 32], which design appropriate handcrafted\nfeatures using prior expert knowledge. Deep learning-based\nunsupervised person Re-ID has advanced signiﬁcantly over\nthe past few years, and several notable works have greatly\nadvanced unsupervised person Re-ID performance. For in-\nstance, GLT [46] combines the pseudo-label prediction and\nRe-ID representation learning in one uniﬁed optimization\nobjective. The holistic and immediate interaction between\nthese two steps in the training process can signiﬁcantly\nhelp the unsupervised person Re-ID task. The historical\nevolvement of person Re-ID is presented in Figure 2 with\nseveral representative works.\n3. Unsupervised Person Re-Identiﬁcation\nIn this section, we review unsupervised person Re-ID\nmethods from the challenges and solutions perspective. We\nﬁrst discuss the application scenario of unsupervised person\nRe-ID in unsupervised cross-domain person Re-ID and fully\nunsupervised person Re-ID based on whether a labelled\nsource dataset is utilized. We then summarise the main\nchallenges in unsupervised person Re-ID and analyze the\ncorresponding solutions. Unsupervised person Re-ID has\nbeen studied in both image-based and video-based person\nRe-ID. Image-based unsupervised person Re-ID has been\nthe mainstream research which is an extension of the im-\nage retrieval problem. Video-based unsupervised Re-ID has\nbeen gaining popularity in the past few years due to the rich\ntemporal information in the video, which provides additional\nguidance in camera-invariant feature learning.\nCNN-based deep learning person Re-ID works [47, 48]\nhave achieved impressive Re-ID accuracy, but their success\nis largely dependent on suﬃcient annotated data that incur\nhigh labelling costs. In contrast, unlabeled person images\nare easy to collect from video systems, fostering the study of\nunsupervised Re-ID. In the current literature, unsupervised\nperson Re-ID has two settings depending on whether using\nextra labelled data. It is generally perceived that it’s diﬃcult\nto learn camera-invariant person features without pairwise\nlabels for the person identities appear in multiple camera\nviews. Therefore, most works pre-train a Re-ID model on\nthe labelled source dataset and adapt to the unlabelled tar-\nget dataset, namely unsupervised cross-domain person re-\nidentiﬁcation [49, 50, 51] as illustrated in Figure 3. Another\ngroup of unsupervised person Re-ID works directly with the\nunlabelled dataset without utilizing any labelled data, which\nis named fully unsupervised person Re-ID [52, 53, 54] as\nillustrated Figure 4.\nUnsupervised cross-domain person re-identiﬁcation\nIn Unsupervised cross-domain person Re-Id, a labelled\nsource domain and an unlabeled target domain are used to\ntrain a Re-ID model for the target domain. The Re-ID model\nis pre-trained on the labelled source data in a supervised\nmanner then adapts to the target domain with unlabelled\ndata [55, 56]. The domain knowledge of the labelled dataset\nis transferred to the unlabelled domain via transfer learning.\nIn the current literature, unsupervised cross-domain person\nRe-ID is studied with the image dataset. A number of diverse\nmethods utilize transfer learning to improve unsupervised\nperson Re-ID [57, 58, 59, 49, 60, 61, 50, 62, 63, 64, 65, 66].\nSome works\n[57, 58] utilize extra attribute annotations\nto minimize the mid-level feature discrepancy. GAN is\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 4 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nFigure 3: Illustration of unsupervised cross-domain person Re-ID. 1. Pre-train model with labelled source data. 2. Use the\npre-trained model to extract and cluster features from unlabelled target data to obtain pseudo-labels. 3. Adapt the pre-trained\nmodel to target domain with pseudo-labels.\nanother popular choice for transfer learning, in which the\nlabelled source images are transferred into images with\nthe target image styles while preserving source identity\nlabels. The translated images with the source identity labels\nare then used to ﬁne-tune the Re-ID model for the target\ndomain [49, 60, 61, 50, 62]. Clustering-based methods has\nshown an advantage over the others by exploiting dynamic\nperson features in unlabelled data. Although the knowledge\ntransfer approach can adapt the source domain to the target\ndomain, it assumes that the two domains have many similar\nfeatures. The Domain adaptation approach may not work\nwell when the source and the target dataset have signiﬁcant\nfeature variations.\nFully unsupervised person re-identiﬁcation. The fully\nunsupervised Re-ID is more challenging since only un-\nlabeled data are provided to train a Re-ID model. Thus\nits performance is limited without pairwise labelled data\nto learn invariant person features across cameras views.\nTherefore it is less researched as it is more challenging to\nlearn robust representation without labelled training sam-\nples. Before deep learning-based person Re-ID, the person\nRe-ID models are generically using handcrafted features\n[33, 8] and focused on metric learning to associate the\nquery person with the hard-crafted person representation.\nThere are few attempts of unsupervised person Re-ID with\nhandcrafted features in which performance is far behind\ndeep learning-based methods. Unlike unsupervised domain\nadaptation, fully unsupervised person Re-ID doesn’t require\na pre-trained model on labelled data. It fully explores the\ndistribution of unlabeled data to learn discriminative person\nfeatures. It’s commonly perceived that without any label\nguidance it’s diﬃculty to learn robust discriminative per-\nson features across camera views [67, 11, 68, 69]. A few\nnotable achievements have greatly closed the performance\ngap between unsupervised cross-domain Re-ID and fully su-\npervised Re-ID. Fully unsupervised person re-identiﬁcation\nhas been studied with both image and video person Re-\nID datasets. In reviewing papers of unsupervised cross-\ndomain person Re-ID and fully unsupervised person Re-ID,\nwe summarise four signiﬁcant challenges that unsupervised\nperson Re-ID solutions need to address as follows:\n• Lacking ground-truth identity labels to supervise\nfeature representation learning. A person Re-ID\nmodel is required to learn discriminative feature rep-\nresentation, which is trained as a classiﬁcation task.\nWithout ground-truth identity labels, the model has\nto predetermine the pseudo-identity labels associated\nwith the training data, which may hinder the repre-\nsentation learning if the estimated identity labels are\nincorrect. Therefore, lacking identity labels is the ﬁrst\nchallenge to deal with for unsupervised person Re-ID.\n• Learning discriminative person feature represen-\ntation with noisy pseudo-labels. A person Re-ID\nmodel needs to learn discriminative person features\nfrom diverse person images considering occlusion,\nview point, illumination etc. Estimated pseudo-labels\nare noisy and may mislead the feature learning pro-\ncess. How to minimize the impact of noisy pseudo-\nlabels and maximize the model’s discriminative power\nis a big challenge.\n• Learning camera-invariant features without pair-\nwise samples. Person Re-ID is a multi-camera re-\ntrieval application. A Re-ID model is required to learn\ncamera-invariant person features to identify the person\nwith the cross-camera appearance in the gallery im-\nages. Without pairwise identity labels across camera\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 5 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nFigure 4: Illustration of fully-unsupervised person Re-ID. 1. Extract and cluster features from unlabelled target data to obtain\npseudo-labels. 2. Train model with unlabeled data and pseudo-labels.\nviews, it is challenging to learn a Re-ID model to\nmatch cross-camera person images.\n• Domain gap between datasets. The domain gap chal-\nlenge is speciﬁc to unsupervised cross-domain person\nRe-ID in which the person appearance show signif-\nicant variation in diﬀerent datasets due to diﬀerence\nviewpoint, light condition, background clutters etc.\nTherefore the domain gap needs to be considered\nwhen adapting a pre-trained model from the source to\nthe target domain.\nIn the following section, we analyze unsupervised person\nRe-ID methods regarding the above challenges and corre-\nsponding solutions from the following aspects:\n• Pseudo-label estimation. Addressing the challenge\nof absent ground-truth identity labels by estimating\npseudo-labels for unlabelled samples. The pseudo-\nlabels are then used to supervise the model training\non the unlabeled dataset. Estimated pseudo-labels are\ngenerated in an unsupervised manner and may be\nnoisy due to false labels. We discuss how various\nsolutions reﬁne noisy pseudo-labels.\n• Deep feature representation learning. Addressing\nthe challenge of learning discriminative person feature\nrepresentation with pseudo-labels from person images\nconcerning background clutter, occlusion and poses\netc.\n• Camera-invariant feature representation learning.\nAddressing the challenge of learning cross-camera\ninvariance person features. Person appearances vary\nin diﬀerent camera views due to viewpoint, lighting\ncondition, background etc. Commonly, an inter-class\nperson looks similar from the same camera view,\nwhile same person appearances vary across multi-\nple camera views. We highlight solutions that learn\ncamera-invariant features which lead to superior per-\nformance.\n• Unsupervised domain adaptation. Addressing the\nchallenge of mitigating the domain gap when adapt-\ning a source model to a target domain using various\nunsupervised domain adaptation (UDA) strategies.\n3.1. Pseudo-Label Estimation\nThe primary task of the person Re-ID problem is to learn\na Re-ID model to extract discriminative person features,\nwhich minimizes the distance with the query person. As\ndeﬁned in Equation 1, this is generally formulated as a\nclassiﬁcation task where the objective is to associate sample\nimages with identity labels and minimize the classiﬁcation\nlosses between predicted identity labels and the ground-truth\nidentity labels.\nIn unsupervised person Re-ID, the ground-truth identity\nlabels for the training samples are not provided, and the num-\nber of identities is unknown. Therefore, the ﬁrst challenge\nin unsupervised person Re-ID is to estimate pseudo-labels\nfor the unlabelled training samples. Apart from few early\nworks in unsupervised cross-domain person re-identiﬁcation\nwhich preserve source domain identities [49, 60] to train\nthe target model, diverse solutions have been proposed to\nestimate identity labels based on data distribution [55, 70,\n71, 72, 50, 73]. The graph matching [70] method proposes to\nuse graph model to represent samples and perform dynamic\ngraph matching for cross-camera labeling. Clustering-based\nmethods employ clustering algorithms such as K-means\nand DBSCAN to progressively group training samples into\nclusters and use the cluster IDs as the pseudo-labels to train\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 6 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\na Re-ID model. Clustering operation is performed progres-\nsively until the model converges and the clustering results\nare satisfactory [55, 74, 66, 56]. For example, PUL [55]\nproposes a progressive clustering and ﬁne-tuning method to\ntrain a Re-ID model using K-means clustering and the IDE\nmodel [12].\nIn the clustering approach, pseudo-label estimation ex-\nploits the target domain’s data distribution characteristics\nand generates labels from clustering results. In the current\nliterature, pseudo-labels are estimated in two ﬂavours: hard\nlabelling and soft labelling. A hard label is a single label\npredicted for a sample, such as cluster IDs in the clustering-\nbased approach are ﬁxed hard labels for training samples. A\nsoft label is a probability that a label belongs to a sample. A\nsample could have multiple soft labels, such as in TSSL [75]\nwhere a video tracklet can be associated with multiple iden-\ntities at various probabilities.\nA common problem in clustering-based pseudo-label\nestimation is the label noises so that incorrect labels are\npredicted to guide model training. To reﬁne the pseudo-\nlabels, NRMT [76] employs two networks in training to\nselect samples and perform collaborative clustering with the\nselected samples and is similar to the ACT [77] method,\nwhich use two networks to distinguish pure and diverse\nsamples. MEB-Net [78] train multiple expert models on the\nsource dataset and utilizes mutual learning to learn the best\nmodel from the multiple experts using a regularization term\nto determine the expert models’ authority.\nIn an unlabelled dataset, the number of identities is\nunknown. Since the number of classes is critical in the\nclassiﬁcation task, the pseudo label estimation process needs\nto predetermine or estimate the number of identities in the\ntarget domain and associate identiﬁes with the unlabeled\ntraining images. K-means is a popular clustering algorithm\n[55] and is very sensitive to the k value. The K-means\nalgorithm unavoidably generates false labels. Diﬀerent from\nthe K-means algorithm, BUC [52] uses bottom-up clustering\nto generate pseudo-labels. Bottom-up clustering is essen-\ntially hierarchical clustering which can visit all samples and\ndetermine the similarity of samples. Based on the similarity\nmetric deﬁned for cluster merging, hierarchical clustering\nestimate labels based on visual similarity distribution. Hard\nsamples such as the visually similar person with diﬀerent\nidentities can’t be distinguished in hierarchical clustering,\nthus merge hard samples into the same cluster. These false\nlabels eventually mislead the model training and hamper the\noverall Re-ID performance. To better separate the positive\nsamples from negative samples, GDS [79] employs a global\ndistance-distribution constraint to separate positive and neg-\native samples, therefore achieve better pseudo-labels.\nTo reduce the impact of the hard samples, HCT [54]\nuses PK (K samples out of P identities) sampling to form a\nnew dataset and employs a hard-batch triplet loss to similar\nsamples closer and separate diﬀerent samples further from\neach other, therefore reduce the inﬂuence of hard samples.\nDSCE[80] uses DBSCAN for pseudo-label generation and\ndoesn’t deal with hard samples, which pose greater feature\nvariation and could be used to learn invariant features across\nscenes. SSG [56] builds clusters from diﬀerent camera views\nfor similar global bodies and local parts. The cluster IDs are\nthen used to supervise the model training. MAR [59] ad-\ndresses the hard samples by using comparative consistency\nbetween soft labels. It learns soft labels from the source\ndataset for Re-ID model training to reduce the impact of the\nhard samples. MMT [51] reﬁne hard labels oﬄine and soft\nlabels online in an alternative training manner.\nBUC [52] and HTC [54] are essentially hierarchical\nclustering algorithms. The hierarchical structure indicates\nthat the distance criteria used for cluster merging or division\nare crucial. In hierarchical clustering, clustering and cluster\nmerging depends on the distance criteria. BUC exploit a\nminimum distance criterion to merge clusters by taking the\nshortest distance between images so that if two images are\nvery similar, their clusters will be merged. BUC introduces\na diversity regularisation term into the distance criterion to\nboost diversity and avoid dominant clusters as express in\nEquation 2.\n퐷(퐴, 퐵) =\nmin\n푥푎∈퐴,푥푏∈퐵푑(푥푎, 푥푏) + 휆(|퐴| + |퐵|).\n(2)\nwhere 푑(푥푎, 푥푏) is the Euclidean distance between the two\nimages. The minimum criteria work well to merge visually\nsimilar samples into the same cluster. However, it doesn’t\nuse other samples and fails to consider outliers in the sample\nspace, such as hard samples that are similar in appearance\nbut with diﬀerent identities.\nTo reduce the impact of outliers and false labels in the\nclustering process, HCT uses Euclidean distance to measure\nthe sample distance, which can be expressed as:\n퐷푎푏= −1\n푛푎푛푏\n∑\n푖∈퐶푎,푗∈퐶푏\n퐷(퐶푎푖, 퐶푏푗).\n(3)\nwhere 퐶푎푖,퐶푏푗are two samples in the cluster 퐶푎and 퐶푏re-\nspectively. 푛푎and 푛푏are sample numbers in 퐶푎and 퐶푏. 퐷(⋅)\nis the Euclidean distance. It considers all the pairwise dis-\ntances between clusters. All pairwise distances are weighted\nequally. Therefore, it eﬀectively alleviate the impact of the\nhard samples, resulting in better clustering results.\nTo reﬁne pseudo-labels in unsupervised video-based\nperson Re-ID, SMP [71] uses a step-wise metric promotion\nmethod to predict tracklet labels progressively. SMP method\nuses reciprocal nearest neighbour search to eliminate the\nhard samples which generate false labels. An OIM\n[81]\nloss function instead of the cross-entropy loss is used in the\nclassiﬁcation operation to speed up model convergence with\na large number of identities.\n3.2. Deep Feature Representation Learning\nThe core part of the Re-ID system is to learn a Re-ID\nmodel to extract discriminative person feature representa-\ntion, which minimizes the distance with the query person\n(Eq.1). With estimated pseudo-labels 푦푖, unsupervised per-\nson Re-Id models are trained in a supervised manner to\nlearn discriminative feature representation. However, due\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 7 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nto the noisy pseudo-labels, the feature learning process is\nbiased inevitably. The general approach to alleviating this\nimpact is to exploit the training data distributions rather\nthan solely relying on the pseudo-labels to guide the feature\nlearning [82, 83, 84, 54].\nDeep feature representation learning in unsupervised\nmodels exploit global and local person features similar to\nsupervised person Re-ID models. Global feature learning,\nsuch as image-level features, has been well studies tradition-\nally. Discriminative local features have been proved eﬀective\nin supervised person Re-ID [36, 85, 86]. For Unsupervised\nperson Re-ID, PAUL [87] proposes to learn local discrim-\ninative features from unlabeled patches rather than from\nthe global images in an unsupervised manner. ADTC [88]\nemploys an attention mechanism to strengthen the infor-\nmative parts of person images. SSG [56] mines the visual\nsimilarities from the global body level as well as the local\nlevel from upper and lower body parts. PAST [66] exploits\nlocal features of target data with triplet losses, leading to\nimproved feature representations. Unlike the PAST methods,\nwhich use ﬁxed samples, DCML [89] emphasizes credible\nsamples for feature representation learning to reduce the\nimpact of the false labels. The DCML method proposes two\nmetrics, the KNN similarity and the prototype similarity.\nThe KNN similarity measures the neighbourhood density\nbetween the sample and its neighbours. The prototype sim-\nilarity estimates the sample and the class prototype’s simi-\nlarity. The DCML discards hard samples that provide more\ndiscriminate information across cameras.\nEﬃcient loss functions play critical roles in guiding deep\nfeature learning. The general purpose is to close the intra-\nclass samples in the feature space and drive them away\nfrom the inter-class samples. Euclidean distance and cosine\nsimilarity are common metrics to evaluate the similarity\nlevel among samples. The Re-ID model is generally formu-\nlated as a classiﬁcation problem where conventional cross-\nentropy loss trains the classiﬁer. BUC [52] uses a repelled\nloss to maximize the sample balance in identities. To close\nintra-class features while separating them from other classes,\nthe triplet loss [90] is a popular choice in guiding feature\nlearning for a large number of classes [82, 83, 84]. The\ntriplet loss is formulated as:\n퐿푡푟푖=\n푃\n∑\n푖=1\n[\n푚+ 퐷(푥푖\n푎, 푥푗\n푝) −퐷(푥푖\n푎, 푥푗\n푛)\n]\n.\n(4)\nwhere 푥푖\n푎a is the anchor, 푥푗\n푝is the person features with\nthe same identity as the anchor. 푥푗\n푛is the person feature\nwith diﬀerent identity as the anchor. 퐷(⋅) is the Euclidean\ndistance, and 푚is the hyper-parameter margin. This loss\nensures that intra-class features are close in feature space\nwhile away from the inter-class features. To reducing the\nimpact of outliers in unsupervised clustering, HCT [54] uses\na hard-batch triplet loss (Eq.5) to guide feature learning.\n퐿ℎ푡푟푖=\n푃∑\n푖=1\n퐾\n∑\n푎=1\n[\n푚+ max\n푝=1...퐾퐷(푥푖\n푎, 푥푗\n푝) −min\n푗=1...푃\n푛=1...푁\n퐷(푥푖\n푎, 푥푗\n푛)\n]\n. (5)\nHard-batch triplet loss ensures that give an anchor 푥푖\n푎, 푥푗\n푝\nis closer to 푥푖\n푎than 푥푗\n푛. As a result, a person with the same\nidentity will be closer to each other and separate from others.\nUnsupervised person Re-ID is also applicable in video-\nbased person Re-ID applications, where a video sequence\nwith multiple frames (tracklet) represents a person. Due to\nthe rich appearance and temporal information, video-based\nunsupervised Re-ID has increased interest in the computer\nvision community, bringing additional challenges in video\nfeature representation learning with tracklets. In video-based\nperson Re-ID, temporal information is widely used in feature\nlearning. JVTC [91] computes the temporal consistency\nbased on the distribution of time interval between cameras.\nThe intuition is easy to follow. For example, when a person\nappears in camera 퐴at time 푡푎, according to the temporal\ndistribution, the person would be likely recorded by camera\n퐵at time 푡푏, and is less likely be recorded by camera 퐶at\ntime 푡푐. This temporal information is useful to determine\nvisually similar hard samples, thus improve representation\nlearning.\nTracklets are useful in learning invariant feature repre-\nsentation simply because that same tracklet frames with var-\nious person appearances represent the same person. Person\ntracklets can be obtained by tracking algorithms [92, 93, 94,\n95, 96] without prohibitive labelling cost. TAUDL [97] and\nUTAL [98] learns intra-camera person tracklet features and\ncorrelate inter-camera tracklets to learn better person fea-\ntures. TSSL [75] proposes a self-supervised learning method\nthat directly learn features from unlabelled tracklet data for\nboth video-based and image-based unsupervised person Re-\nID. In addition, TSSL predicts soft labels for images to\nminimize the impact of hard samples.\n3.3. Camera-Aware Invariance Learning\nPerson Re-ID is a cross-camera retrieval process aiming\nto learn a model to discriminate samples from diﬀerent\ncamera views. Suppose a model trained with samples from\nsome cameras can also generalize to distinguish samples\nfrom the other cameras. In that case, we could obtain a\nmodel that can extract the intrinsic feature without camera-\nspeciﬁc bias and is robust to camera changes. However,\ndue to lacking pairwise ground-truth information for cross-\ncamera samples, unsupervised Re-ID models ability to learn\ncamera-aware invariant features is limited.\nTo learn camera-aware invariant features, SCCT [83]\ncalculates the colour statistics of diﬀerent camera views\nand apply a linear transformation to match the statistics of\ntransformed images same as target images, thus improves the\ncamera-aware invariance. Cross-domain Mixup [99] con-\nducts interpolation on the data manifold, which is similar\nto GAN-based image style transfer, and achieve good cross-\ncamera invariant features. DSCE[80] introduces a camera-\naware meta-learning (MetaCam) aiming to learn camera-\ninvariant representations by simulating the cross-camera Re-\nID process during training. Precisely, MetaCam separates\nthe training data into meta-train and meta-test subsets, en-\nsuring that they belong to entirely diﬀerent cameras. Then\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 8 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nenforce the model to learn camera-invariant features under\nboth camera settings by updating the model with meta-train\nand validating the updated model with meta-test. UGA [100]\nmines the cross-camera association and alleviates the impact\nof noisy labels. Speciﬁcally, UGA trains the model in two\nstages. The intra-camera learning stage is to learn person\nrepresentations considering the camera information, which\nhelps to reduce false cross-camera relationships in the inter-\ncamera learning stage. UCDA [65] utilizes the unique char-\nacteristics of camera-level images and learn camera-aware\ninvariant features.\nIn unsupervised clustering, each sample is allocated a\nlabel the same as its nearest neighbours. Utilizing a memory\nbank (e.g. lookup table), a sample’s probability sharing the\nsame identity with its neighbours can be calculated as:\n푝푖푗=\nexp(푠.푚푇\n푗푥푡\n푖)\n∑푁푡\n푘=1 exp(푠.푚푇\n푘푥푡\n푖)\n.\n(6)\nwhere 푠is a scaling factor 푚푇\n푖is the person features for the\n푖푡ℎtarget identity in the lookup table (LUT). 푚푇\n푗is the 푗푡ℎ\nperson features in the LUT. The objective is to minimise the\nloss:\n= −log(푝푡).\n(7)\nDue to the cross-camera scene variation, the diﬀerent\nintra-camera people may share more signiﬁcant similarities\nthan those in the inter-camera scene. HHL [61] learns\ncamera-aware person features in the unlabeled domain.\nHowever, HHL overﬁts the visually similar pairs from the\ntarget domain. Consequently, the Re-ID model is sensitive\nto pose and background variations in the target domain. To\nalleviate the impact of the hard samples, the Mixup [99]\nmethod considers the camera information when calculating\nthe probability that a sample feature shares the same identity\nas another feature. The probability that two features of\nthe same identify are from the same camera view can be\nexpressed as:\n푝푖푛푡푟푎\n푖푗\n=\nexp(푠.푚푇\n푗푥푡\n푖)\n∑\n푘∈푂푖푛푡푟푎\n푖\nexp(푠.푚푇\n푘푥푡\n푖)\n.\n(8)\nSimilarly, the probability that two samples of the same\nidentity are from diﬀerent camera views is formulated as:\n푝푖푛푡푒푟\n푖푗\n=\nexp(푠.푚푇\n푗푥푡\n푖)\n∑\n푘∈푂푖푛푡푒푟\n푖\nexp(푠.푚푇\n푘푥푡\n푖)\n.\n(9)\nwhere 푠is a scaling factor, 푚푇\n푖is the person features for the\n푖푡ℎtarget identity in the lookup table (LUT). 푚푇\n푗is the 푗푡ℎ\nperson features in the LUT. The objective is to minimise the\nthe combination of intra-camera and inter-camera losses:\n= −\n∑\n푤푖,푗log(푝푖푛푡푟푎\n푖,푗\n) −\n∑\n푤푖,푡log(푝푖푛푡푒푟\n푖,푡\n).\n(10)\nwhere 푗denotes the 푗푡ℎintra-camera neighbor of 푥푡\n푖, and 푡\ndenotes the 푡푡ℎinter-camera neighbor of 푥푡\n푖. By taking con-\nsideration of inter-camera neighbor in calculating identity\nprobability, the learnt feature achieve camera-aware invari-\nance.\nTo learn camera-aware invariant feature in video-based\nunsupervised person Re-ID, TAUDL [97] learns intra-camera\nperson tracklet features ﬁrst, then jointly learn all the intra-\ncamera tracklet features to propagate feature learning from\nintra-camera learning to inter-camera feature learning, there-\nfore achieving camera-aware invariance.\n3.4. Unsupervised Domain Adaption\nUnsupervised domain adaptation addresses the domain\ngap challenge in unsupervised cross-domain person Re-ID.\nIn this setting, a model is ﬁrst pre-trained on the labelled\nsource domain and then adapted to the unlabelled target\ndomain. In unsupervised cross-domain person Re-ID, there\nis an unlabeled target domain 푇\n=\n{푡푖}푁푇\n푖=1 containing\n푁푇person images. Additionally, a labelled source domain\n푆\n= {푠푖, 푦푖}푁푆\n푖=1 containing 푁푆labelled person images\nis available to pre-train the Re-ID model, where 푦푖is the\nground-truth identity label for the training sample 푠푖. The\nobjective of the cross-domain person Re-ID is to learn a\nfeature extractor 푓(푡) for the unlabeled target domain 푇,\nusing both source domain 푆and target domain 푇. The\ntraining cost of 푓(푡) is the sum of the training loss on both\npre-trained and adapted models. With ground-truth identity\nlabels, the training on source domain 푆is a classiﬁcation\ntask with the conventional cross-entropy loss, i.e.,\n= −1\n푁푆\n푁푆\n∑\n푖=1\nlog 푃(푦푖|푠푖),\n(11)\nwhere 푃(푦푖|푠푖) is the probability of sample 푠푖being identity\n푦푖. This supervised learning on source domain ensures the\noptimal performance of 푓(푡) on source domain thus ensure\nthe feature extraction quality on the target domain for adap-\ntation.\nGround-truth identity labels are absent in training the the\ntarget domain, instead, pseudo-labels are used to supervise\nthe training. The training loss can be expressed as:\n= −1\n푁푇\n푁푇\n∑\n푖=1\nlog 푃( ̂푦푖|푡푖),\n(12)\nwhere ̂푦푖is the estimated pseudo-label for 푡푖. 푃( ̂푦푖|푡푖) is the\nprobability of sample 푡푖being identity ̂푦푖.\nThe pre-trained Re-ID model on the source domain\nis sensitive to the feature variations in the target domain.\nThe domain gap has a high impact on the cross-domain\nRe-Id performance. Therefore, the image variations must\nbe considered when adapting the pre-trained model to the\ntarget domain. The existing unsupervised domain adaptation\nsolutions can be grouped into three categories, mid-level\nfeature alignment, image style transfer and clustering-based\napproach.\n3.4.1. Mid-Level Feature Alignment\nThe feature alignment approach aims to reduce the do-\nmain gap at the feature and image levels and assumes that\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 9 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nFigure 5: Illustration of feature alignment approach for unsupervised domain adaptation. Mid-level attribute features are aligned\nbetween source and target domains in a joint learning pipeline. 푎푡푡푟represents the attribute alignment loss between the source\nattributes and the target attributes.\nthe source and target datasets share a common mid-level\nfeature space, and the common mid-level features can be\nused to infer person identities cross domain [57, 58]. [58,\n61, 101] exploit auxiliary information to improve the model\ngeneralization capability. TFusion [101] exploits Spatio-\ntemporal information to learn better feature representation.\nTJ-AIDL [58] and MMFA [57] utilize annotated attributes\ninformation to learn discriminative feature representation.\nAn example of the attribute alignment approach is illustrated\nin Figure 5. However, strictly speaking, annotated attributes\nare considered supervised learning and incur labelling costs.\nD-MMD [102] loss is proposed to align features by closing\npairwise distances using small batch sizes. In PAUL [87],\nit is assumed that if two images are similar, then their\nlocal patches are similar. So that instead of learning the\nimage level features, PAUL resorts to patch features for\nunsupervised person Re-ID. Diﬀer from the above direct\nfeature alignment methods, 푝MR-SADA [103] proposes to\nreduce the domain gap at the image level by mixing the\ncamera semantic information with the original images.\nTo alleviate the negative transfer caused by the domain\ndivergence, DAAM [104] proposes an attention model based\non residual mechanism. It transfers knowledge from the\nlabelled dataset to the unlabeled dataset by jointly modelling\nthe domain-shared and domain-speciﬁc features. Moreover,\nit diﬀers signiﬁcantly from existing methods in that a soft\nlabel loss is proposed to alleviate the negative eﬀect of\ninaccuracy pseudo labels.\n3.4.2. Image Style Transfer\nImage style transfer, in particular, GAN-based image\nstyle transfer has been an popular approach to transfer knowl-\nedge between the source domain and the target domain for\nunsupervised cross-domain person Re-ID [64, 105, 61, 49,\n60, 105]. For example, PTGAN\n[49] and SPGAN [60]\ndirectly use CycleGAN [106] to reduce the domain gap\nproblem. They ﬁrst transfer images from source datasets\nto the target style while preserving source identity labels,\nthen uses transferred images and preserved source domain\nidentity labels to train a Re-ID model for the target domain.\nAn example of GAN-based image transfer approach is il-\nlustrated in Figure 6. HHL\n[61] generates images under\ndiﬀerent cameras and trains networks using triplet loss,\nthus improves model performance with the camera-aware\ninvariant features. ECN [50] utilizes transfer learning and\nminimizes the target invariance using an exemplar memory\nto learning invariant features. CR-GAN[62] exploits con-\ntextual styles, in particular the background. It masks the\nperson in target images to retain background clutter and use\nthe source person and the target background as transferred\nimages to train the Re-ID model. DAS [107] employs a new\nsynthetic dataset simulating various lighting conditions and\ntransfer the synthetic dataset to the target styles using Cy-\ncleGAN [106]. PDA-Net [64] exploits PatchGAN [108] and\nposes disentanglement in transferring images from source to\ntarget. Based on SPGAN [60], CR-GAN [62] PTGAN [49]\nuse person segmentation masks as extra information to pre-\nserve the discriminative person features.\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 10 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nFigure 6: Illustration of GAN-based image transfer for unsupervised domain adaptation. Source images are transferred to target\nimage styles using a GAN while preserving source identity labels. The transferred images with the source identity labels are used\nto train the Re-Id model as a classiﬁcation task.\n3.4.3. Clustering-Based Approach\nClustering-based methods adapt the pre-trained model\nin the source domain in two steps. First, they use the pre-\ntrained model to extract and estimate pseudo-labels based on\nunlabeled image cluster IDs. Then they use the images with\npseudo-labels to ﬁne-tune the pre-trained model on the target\ndataset. The pseudo-labels are generated by clustering all\ncross-camera samples based on visual similarity. The model\nis then ﬁne-tuned as a classiﬁcation task. These two stages\nare executed alternately to optimize the Re-ID performance\nwith intra-camera and inter-camera losses. An example of\nclustering-based UDA is illustrated in Figure 3. Since the\nRe-ID model is trained as a classiﬁcation task, one key\nparameter in the clustering-based approach is to determine\nthe number of identities for the ﬁnal classiﬁcation output.\nCDS [73] adopts the K-means algorithm to cluster target\nsamples, where K is pre-deﬁned empirically. CDS uses a\ndynamic sampling strategy to determine whether a sample\nbelongs to a cluster by calculating the cosine similarity\nbetween the sample and the cluster centroid.\nClustering results based on sample visual similarity lead\nto data imbalance in clusters. To deal with data imbalance,\nDBC [109] learns the data distribution then uses pairwise\nsample relationships to achieve better cluster balance in the\nclustering process. MMCL [82] formulates a multi-label\nclassiﬁcation task to progressively estimate soft pseudo-\nlabels. The predicted labels are veriﬁed based on visual\nsimilarity and cycle consistency. AD-Cluster[84] employs\nan image generator to augment samples in the density-based\nclustering and improve the discriminative capability with the\naugmented clusters.\nIn unsupervised video-based person Re-Id, temporal in-\nformation can be easily obtained in a multi-camera system.\nJVTC [91] tackles the domain gap challenge by enforcing\nvisual and temporal consistency in a multi-class classiﬁ-\ncation task. Visual similarity and temporal consistency are\nused in multi-class prediction to optimize the intra-class and\ninter-class association. Two samples are considered the same\nidentity only when they share considerable visual similarity\nand satisfy the temporal consistency. To accurately estimate\nlabels from noisy frames, RACE\n[110] proposes robust\nanchor embedding and top-푘counts label prediction to learn\nfull feature embeddings. SSL [53] oﬀers a soft label classi-\nﬁcation approach to exploit the unlabeled image similarity\nfully. DAL [111] proposes two margin-based association\nlosses to eﬀectively constrain the intra-camera and inter-\ncamera frame association, thus achieve better discrimina-\ntive cross-camera person representation. ACT [77] design\nan asymmetric co-teaching framework with two models to\nselect samples from each other. One model focus on training\nwith true positive sample and another model address the hard\nsamples with false-positive samples.\n4. Datasets and Evaluation\n4.1. Datasets\nUnsupervised person Re-ID aims to address the data\nscalability issue in conventional supervised person Re-ID.\nMost models are trained and evaluated on large-scale person\nRe-ID datasets sourced from the multi-camera video sys-\ntem. Person Re-ID is generally perceived as a multi-camera\napplication. Therefore, the person appearing in more than\none camera views are used to train and test a person Re-ID\nmodel. Dataset statistics are summarised in Table 2.\nCUHK03 [7] has 1,360 identities in 13,164 images from\n6 camera views. This dataset has both manually annotated\nbounding boxes and auto-detected bounding boxes. 1,160\nidentities are used for training, 200 identities are selected for\ntesting.\nMarket-1501 [8] is a large-scale benchmark dataset for\nimage-based person Re-ID. It has 1,501 identities and 32,668\nannotated person using the DPM [112] detector. On average,\neach person has 3.6 images from each camera. Among the\n1501 identities, 750 identities are used for training, and the\nrest 751 identities are used for evaluation. The evaluation set\nhas 19,732 images. 3,368 images are used as probe queries.\nDukeMTMC-Re-ID [10] is extracted from the Duke-\nMTMC [93] tracking dataset for image-based person Re-\nID. The person images are manually cropped from high-\nresolution videos from 8 non-overlapping cameras. It has\n34,183 person images and 1,404 identities that appear in at\nleast two cameras. The training set has 16,522 images of\n702 identities. 17,661 images and 702 identities are used for\nevaluation. In evaluation, 2,228 images are used as queries.\nMSMT17 [49] is the largest dataset for image-based per-\nson Re-ID. It contains videos captured by 15 cameras at 12\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 11 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nTable 2\nCommonly used benchmark datasets for unsupervised person\nre-identiﬁcation. The top four datasets are for image-based\nperson Re-ID, the bottom four are for video-based person Re-\nID.\nDataset\n#boxes\n#identity\n#cameras\ndetector\nCUHK03 [7]\n13,164\n1,360\n6\nhand/detector\nMarket-1501 [8]\n32,668\n1,501\n6\nhand\nDukeMTMC-Re-ID [10]\n34,183\n1,404\n8\nhand\nMSMT17 [49]\n126,441\n4,101\n15\nFaster R-CNN\nPRID2011 [113]\n40,033\n200\n2\nhand\nMARS [8]\n1,067,516\n1,261\n6\nDPM\nDukeMTMC-VideoRe-ID [72]\n815,420\n1,404\n8\nhand\niLIDS-VID [114]\n42,460\n300\n2\nhand\ntime intervals. The videos are 180 hours in length and pose\ncomplex photometric variations. For image-based person\nRe-ID, the dataset has 4,101 identities and 126,441 person\nimages.\nPRID2011 [113] is an early video dataset that contains\nperson videos from only two cameras. The ﬁrst camera cap-\ntured 385 person identities, 749 person identities were from\nthe second camera. 200 person appeared in both cameras.\nThe videos are curated to tracklets (sequence of frames) with\n5 to 675 frames.\nMARS [8] is a large-scale dataset for video-based person\nRe-ID. Videos were captured by 6 cameras. 1,261 person\nidentities appeared in at least 2 cameras. Lighting conditions\nand image quality vary in the dataset. To mimic the real-\nlife viewing condition, the dataset contains 3,248 distractors.\nTracklets are generated by DPM and GMMCP [115] tracker.\nDukeMTMC-VideoRe-ID [72] is a subset of the Duke-\nMTMC [93] tracking dataset for video-based person Re-\nID. It contains 1,812 identities and 4,832 tracklets from 8\ncameras. On average, a tracklet has 168 consecutive frames.\nThe person boxes in the frames are manually cropped and\nlabelled.\niLIDS-VID [114] is for video-based person Re-ID. It con-\ntains 300 person identities captured by 2 cameras. 600 track-\nlets are hand-annotated, covering 300 person identities from\n2 camera views. Tracklet lengths vary from 23 to 192 frames.\n4.2. Evaluation Metrics\nPerson Re-ID methods are commonly evaluated with\nmean averaged precision (mAP) and cumulative matching\ncharacteristics (CMC Rank-K). Inherited from the object\ndetection problem, the mAP is a popular evaluation metric\nin person Re-ID. With mAP, an averaged precision (AP) is\ncalculated for each probe image, and then the ﬁnal evaluation\nmAP is the average of all APs. With CMC, the top K similar\nperson in the gallery images are ranked according to the\nintersection-over-union (IoU@0.5) overlap with the ground-\ntruth person identity. Most evaluation protocols using single\nperson image as the query person.\n4.3. Performance Analysis\nThis section summarises and analyses the evaluation re-\nsults considering the signiﬁcant challenges in unsupervised\nperson Re-ID. We aim to present the inﬂuential factors that\ncontribute to the overall performance of relevant models.\nIn person Re-ID, the pre-trained ResNet on ImageNet is\nwildly used as the initial model and updated with various\nobjectives. ImageNet has many person images, which make\nthe pre-trained ResNet model suitable for initial person\nfeature extraction. Therefore, in this survey, we don’t specif-\nically discuss CNN backbones and network architectures.\nInstead, we highlight how the innovative solutions address\nthe underlying challenges.\nAs discussed in Section 3, unsupervised person Re-ID\nhas two main settings, unsupervised cross-domain person\nRe-ID and fully unsupervised person Re-ID. Unsupervised\ncross-domain person Re-ID has been solely investigated in\nimage-based person Re-ID, while fully unsupervised person\nRe-Id has both image-based and video-based applications.\nTherefore, we summarise and compare unsupervised person\nRe-ID methods performance in three groups: image-based\nunsupervised cross-domain person Re-ID, image-based fully\nunsupervised person Re-ID, and video-based fully unsu-\npervised person Re-ID. We compare unsupervised domain\nadaptation methods for image-based person Re-ID in Ta-\nble 3 on four source to target settings, i.e. Duke→Market,\nMarket→Duke, Market→MSMT, and Duke→MSMT. We\nsummarise image-based fully unsupervised person Re-ID\nmethods performance in Table 4 on four datasets, Market-\n1501, DueMTMC-Re-ID, MSMT-17 and CUHK03. Unsu-\npervised video-based person Re-ID evaluation results on\nMARS, DukeMTMC-VideoRe-ID, PRID2011 and iLIDS-\nVID are summarised in Table 5.\nIn image-based unsupervised cross-domain person Re-\nID, features alignment methods and GAN-based image\ntransfer methods can’t compete with the clustering-based\ndomain adaptation methods. Features alignment methods\nare more suitable for closed-set application scenarios, where\nthe source and target domains have the same identity labels.\nHowever, this is not the case for unsupervised person Re-\nID, where the identity labels are unknown in the source\nand the target domains. The feature alignment approach\nalso assumes that the mid-level features overlap in source\nand target domains. MMFA and TJ-AIDL rely on mid-\nlevel attributes to improve feature transfer from the source\nto the target domain. However, arbitrary attributes can’t\ncapture the dynamics of both source and target domains.\nFor instance, the Market1501 dataset has 27 annotated\nattributes, while the DukeMTMC-Re-ID dataset only has 23\nannotated attributes. Those attributes cannot fully capture\nmany mid-level visual cues. Therefore, UDA models using\nthe mid-level attributes alignment approach can’t learn good\nperson feature representations and result in poor evaluation\nperformance. Moreover, attribute annotation incurs labelling\ncosts and shouldn’t be considered as unsupervised learning.\nOn the other hand, PAUL [87] learns mid-level features\nthrough body patches and fully exploits the sample space and\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 12 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nTable 3\nImage-based unsupervised cross-domain person Re-ID evaluation results. Adaptation approach is annotated as described in section\n3.4. FA: mid-level feature alignment approach. IT: image style transfer approach. Cluster: clustering-based approach.\nDuke →Market-1501\nMarket-1501 →Duke\nMarket-1501 →MSMT17\nDuke →MSMT17\nMethod\nYear\nApproach\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nCAMEL [69]\n2017\nCluster\n54.50\n-\n-\n26.30\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nPUL [55]\n2018\nCluster\n44.70\n59.10\n65.60\n20.10\n30.40\n44.50\n50.70\n16.40\n-\n-\n-\n-\n-\n-\n-\n-\nMMFA [57]\n2018\nFA\n56.70\n75.00\n81.80\n27.40\n45.30\n59.80\n66.30\n24.70\n-\n-\n-\n-\n-\n-\n-\n-\nTJ-AIDL [58]\n2018\nFA\n58.20\n74.80\n81.10\n26.50\n44.30\n59.60\n65.00\n23.00\n-\n-\n-\n-\n-\n-\n-\n-\nHHL (g) [61]\n2018\nIT\n62.20\n78.80\n84.00\n31.40\n46.90\n61.00\n66.70\n27.20\n-\n-\n-\n-\n-\n-\n-\n-\nPTGAN [49]\n2018\nIT\n38.60\n-\n66.10\n-\n27.40\n-\n50.70\n-\n10.20\n-\n24.40\n2.90\n11.80\n-\n27.40\n3.30\nSPGAN [60]\n2018\nIT\n58.10\n76.00\n82.70\n26.90\n46.90\n62.60\n68.50\n26.40\n-\n-\n-\n-\n-\n-\n-\n-\nDAS* [107]\n2018\nIT\n65.70\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDAR (c) [116]\n2018\nCluster\n75.80\n89.50\n93.20\n53.70\n68.40\n80.10\n83.50\n49.00\n-\n-\n-\n-\n-\n-\n-\n-\nPAUL [87]\n2019\nFA\n68.50\n82.40\n87.40\n40.10\n72.00\n82.70\n86.00\n53.20\n-\n-\n-\n-\n-\n-\n-\n-\nCDS [73]\n2019\nCluster\n71.60\n81.20\n84.70\n39.90\n67.20\n75.90\n79.40\n42.70\n-\n-\n-\n-\n-\n-\n-\n-\nCamStyle [117]\n2019\nIT\n64.70\n80.20\n85.30\n30.40\n51.70\n67.00\n72.80\n27.70\n-\n-\n-\n-\n-\n-\n-\n-\nATNet [105]\n2019\nIT\n55.70\n73.20\n79.40\n25.60\n45.10\n59.50\n64.20\n24.90\n-\n-\n-\n-\n-\n-\n-\n-\nCR-GAN [62]\n2019\nIT\n77.70\n89.70\n92.70\n54.00\n68.90\n80.20\n84.70\n48.60\n-\n-\n-\n-\n-\n-\n-\n-\nCASCL [63]\n2019\nFA\n64.70\n80.20\n85.60\n35.60\n51.50\n66.70\n71.70\n30.50\n-\n-\n-\n-\n-\n-\n-\n-\nPDA-Net [64]\n2019\nIT\n75.20\n86.30\n90.20\n47.60\n63.20\n77.00\n82.50\n45.10\n-\n-\n-\n-\n-\n-\n-\n-\nUCDA [65]\n2019\nCluster\n64.30\n-\n-\n34.50\n55.40\n-\n-\n36.70\n-\n-\n-\n-\n-\n-\n-\n-\nMAR [59]\n2019\nCluster\n67.70\n81.90\n-\n40.00\n67.10\n79.80\n-\n48.00\n-\n-\n-\n-\n-\n-\n-\n-\nECN [50]\n2019\nCluster\n75.10\n87.60\n91.60\n43.00\n63.30\n75.80\n80.40\n40.40\n25.30\n36.30\n42.10\n8.50\n30.20\n41.50\n46.80\n10.20\nSSG (c) [56]\n2019\nCluster\n86.20\n94.60\n96.50\n68.70\n76.00\n85.80\n89.30\n60.30\n31.60\n-\n49.60\n13.20\n32.20\n-\n51.20\n13.30\nDG-Net++ [118]\n2019\nIT\n82.10\n90.20\n92.70\n61.70\n78.90\n87.80\n90.40\n63.80\n48.40\n60.90\n66.10\n22.10\n48.80\n60.90\n65.90\n22.10\nPAST [66]\n2019\nCluster\n78.38\n-\n-\n54.62\n72.35\n-\n-\n54.26\n-\n-\n-\n-\n-\n-\n-\n-\nDCML [89]\n2020\nCluster\n88.20\n94.90\n96.40\n72.30\n79.30\n86.70\n89.50\n63.5\n-\n-\n-\n-\n-\n-\n-\n-\nD-MMD [102]\n2020\nFA\n70.60\n87.00\n91.50\n48.80\n63.50\n78.80\n83.90\n46.00\n29.10\n46.30\n54.10\n13.50\n34.40\n51.10\n58.50\n15.30\nADTC [88]\n2020\nCluster\n79.30\n90.80\n94.10\n59.70\n71.90\n84.10\n87.50\n52.50\n-\n-\n-\n-\n-\n-\n-\n-\nACT [77]\n2020\nCluster\n80.50\n-\n-\n60.60\n72.40\n-\n-\n54.50\n-\n-\n-\n-\n-\n-\n-\n-\npMR-SADA [103]\n2020\nFA\n83.00\n91.80\n94.10\n59.80\n74.50\n85.30\n88.70\n55.80\n-\n-\n-\n-\n-\n-\n-\n-\nDAAM [104]\n2020\nFA\n86.40\n-\n-\n67.80\n77.60\n-\n-\n63.90\n44.50\n-\n-\n20.80\n46.70\n-\n-\n21.60\nMMCL [82]\n2020\nCluster\n84.40\n92.80\n95.00\n60.40\n72.40\n82.90\n85.00\n51.40\n-\n-\n-\n-\n43.60\n54.30\n58.90\n16.20\nSCCT [83]\n2020\nCluster\n86.60\n94.50\n96.90\n67.90\n76.00\n85.00\n88.90\n60.40\n23.60\n36.00\n42.10\n8.30\n37.80\n51.20\n57.00\n13.20\nAD-Cluster [84]\n2020\nCluster\n86.70\n94.40\n96.50\n68.30\n72.60\n82.50\n85.50\n54.10\n-\n-\n-\n-\n-\n-\n-\n-\nJVTC [91]\n2020\nCluster\n86.80\n95.20\n97.10\n67.20\n80.40\n89.90\n92.20\n66.50\n48.60\n65.30\n68.20\n25.10\n52.90\n70.50\n75.90\n27.50\nMMT [51]\n2020\nCluster\n87.70\n94.90\n96.90\n71.20\n78.00\n88.80\n92.50\n63.10\n49.20\n63.10\n68.80\n22.90\n50.10\n63.90\n69.80\n23.30\nNRMT [76]\n2020\nCluster\n87.80\n94.60\n96.50\n71.70\n77.80\n86.90\n89.50\n62.20\n-\n-\n-\n-\n-\n-\n-\n-\nMixup [99]\n2020\nIT\n88.10\n94.40\n96.20\n79.50\n79.50\n88.30\n91.40\n65.20\n43.70\n56.10\n61.90\n20.40\n51.70\n64.00\n68.90\n24.30\nGDS [79]\n2020\nCluster\n89.30\n-\n-\n72.50\n76.70\n-\n-\n59.70\n-\n-\n-\n-\n-\n-\n-\n-\nMEB-Net [78]\n2020\nCluster\n89.90\n96.00\n97.50\n76.00\n79.60\n88.30\n92.20\n66.10\n-\n-\n-\n-\n-\n-\n-\n-\nSPCL [119]\n2020\nCluster\n-\n-\n-\n-\n-\n-\n-\n-\n53.70\n65.00\n69.80\n26.80\n-\n-\n-\n-\nDSCE[80]\n2021\nCluster\n90.10\n-\n-\n76.50\n79.50\n-\n-\n65.00\n-\n-\n-\n-\n-\n-\n-\n-\nGLT [46]\n2021\nCluster\n92.20\n96.50\n97.80\n79.50\n82.00\n90.20\n92.80\n69.20\n-\n-\n-\n-\n-\n-\n-\n-\nyields much better performance than MMFA and TJ-AIDL.\nCASCL [63] aligns the mid-level features via intra-camera\nand inter-camera consistency which further improve the Re-\nID performance. D-MMD [102] and GDS [79] align fea-\ntures through data distribution. pMR-SADA [103] also uses\ncamera information to alight source feature to target feature\nand achieve R-1 rate 83% for Duke→Market. DAAM [104]\nachieves the best evaluation results in this category by using\nan attention mechanism only to align source domain-speciﬁc\nfeatures to the target domain while maintaining the target\ndomain-speciﬁc features. Therefore, common knowledge is\nshared between the source and the target while the model is\nrobust on speciﬁc target features.\nSimilarly to the feature alignment approach, image trans-\nfer methods such as GAN-based methods preserve source\nidentities in transferred images to supervise model training.\nThe success relies on a great level of similarity in the label\nspace. In really, person identities diﬀer between domains,\nsuch as the number of person identities are diﬀerent across\ndatasets. In addition, the GAN-based image transfer ap-\nproach only exploits styling diﬀerence between the source\nand target domain without exploiting the full feature dy-\nnamics in abundant unlabelled data, therefore, resulting in\nlow performance compare to clustering-based UDA meth-\nods, which exploit the full visual similarity in unlabelled\ndata to guide the adaptation procedure. Transferred images\nare limited to image level discriminative features, which is\ninferior to local discriminative features. In a multi-camera\nenvironment, a person appearance varies signiﬁcantly due to\nviewpoint, occlusion etc. therefore, local discriminative fea-\ntures play an essential role in distinguishing a person across\nviews. The GAN-based domain adaptation methods such as\nPTGAN, SPGAN, CR-GAN, etc. focus on image transfer\naccuracy from the source to the target domain, ignoring the\nvisual cues in the target domain. The performance of such\ndomain adaptation methods heavily depends on the similar\ndata distribution and the label space.\nClustering-based methods with pseudo-label reﬁnement\ncoupled with camera variation constraint achieve the best\nRe-ID accuracy, which explains that camera variations\nmainly cause domain shift. GLT [46] combines the pseudo-\nlabel prediction and Re-ID representation learning in one\nuniﬁed optimization objective. The holistic and immediate\ninteraction between these two steps in the training process\ncan signiﬁcantly help the UDA person Re-ID task.\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 13 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nTable 4\nImage-based fully unsupervised person Re-ID evaluation results.\nMarket-1501\nDukeMTMC-Re-ID\nMSMT-17\nCUHK03\nMethod\nYear\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nTAUDL [97]\n2018\n63.70\n-\n-\n41.20\n61.70\n-\n-\n43.50\n-\n-\n-\n-\n44.70\n-\n-\n31.20\nBUC [52]\n2019\n66.20\n79.60\n84.50\n38.30\n47.40\n62.60\n68.40\n27.50\n-\n-\n-\n-\n-\n-\n-\n-\nUGA [100]\n2019\n87.20\n-\n-\n70.30\n75.00\n-\n-\n53.30\n49.50\n-\n-\n21.70\n56.50\n-\n-\n68.20\nDBC [109]\n2019\n69.20\n83.00\n87.80\n41.30\n51.50\n64.60\n70.10\n30.00\n-\n-\n-\n-\n-\n-\n-\nUTAL [98]\n2020\n69.20\n-\n-\n46.20\n62.30\n-\n-\n44.60\n31.40\n-\n-\n13.10\n56.30\n-\n-\n42.30\nTSSL [75]\n2020\n71.20\n-\n-\n43.30\n45.10\n-\n-\n38.50\n-\n-\n-\n-\n-\n-\n-\n-\nSSL [53]\n2020\n71.70\n83.80\n87.40\n37.80\n52.50\n63.50\n68.90\n28.60\n-\n-\n-\n-\n-\n-\n-\n-\nJVTC [91]\n2020\n79.50\n89.20\n91.90\n47.50\n74.60\n82.90\n85.30\n50.70\n43.10\n53.80\n59.40\n17.30\n-\n-\n-\n-\nHCT [54]\n2020\n80.00\n91.60\n95.20\n56.40\n69.60\n83.40\n-\n87.40\n50.70\n-\n-\n-\n-\n-\n-\n-\nMMCL [82]\n2020\n80.30\n89.40\n92.30\n45.50\n65.20\n75.90\n80.00\n40.20\n35.40\n44.80\n49.80\n11.20\n-\n-\n-\n-\nSpCL [119]\n2020\n88.10\n95.10\n97.00\n73.10\n-\n-\n-\n-\n42.30\n55.60\n61.20\n19.10\n-\n-\n-\n-\nDSCE [80]\n2021\n83.90\n92.30\n-\n61.70\n73.80\n84.20\n-\n53.80\n35.20\n48.30\n-\n15.50\n-\n-\n-\n-\nIICS [120]\n2021\n89.50\n95.20\n97.00\n72.90\n80.00\n89.00\n91.60\n64.40\n56.40\n68.80\n73.40\n26.90\n-\n-\n-\n-\nTable 5\nVideo-based fully unsupervised person Re-ID evaluation results.\nMARS\nDukeMTMC-VideoRe-ID\nPRID2011\niLIDS-VID\nMethod\nYear\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nR-1\nR-5\nR-10\nmAP\nSMP [71]\n2017\n23.59\n35.81\n-\n10.54\n-\n-\n-\n-\n80.90\n95.60\n98.80\n-\n41.70\n66.30\n64.40\n-\nRACE [110]\n2018\n43.20\n57.10\n62.10\n24.50\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDAL [111]\n2018\n49.30\n65.90\n72.20\n23.00\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nTAUDL [97]\n2018\n43.80\n59.90\n-\n29.10\n-\n-\n-\n-\n49.40\n78.70\n-\n-\n26.70\n51.30\n-\n-\nBUC [52]\n2019\n61.10\n75.10\n80.00\n38.00\n69.20\n81.10\n85.80\n61.90\n-\n-\n-\n-\n-\n-\n-\n-\nUGA [100]\n2019\n58.10\n73.40\n-\n39.30\n-\n-\n-\n-\n80.90\n94.40\n-\n-\n57.30\n72.00\n-\n-\nUTAL [98]\n2020\n49.90\n66.40\n-\n35.20\n-\n-\n-\n-\n54.70\n83.10\n-\n-\n35.10\n59.00\n-\n-\nTSSL [75]\n2020\n56.30\n-\n-\n30.50\n73.90\n-\n-\n64.60\n-\n-\n-\n-\n-\n-\n-\n-\nSSL [53]\n2020\n62.80\n77.20\n80.10\n43.60\n76.40\n88.70\n91.00\n69.30\n-\n-\n-\n-\n-\n-\n-\n-\nWe compare image-based fully unsupervised person Re-\nID methods performance on the Market-1501, DukeMTMC-\nRe-ID, MSMT-17 and CUHK03 datasets in Table 4. Fully\nunsupervised person Re-ID is more challenging than un-\nsupervised cross-domain person Re-ID as there is no la-\nbelled source data. The most common approach is to cluster\nunlabelled data into many clusters based on visual simi-\nlarity to obtain pseudo-labels to supervise model tanning.\nClustering on visual similarity is not suﬃcient to learn\ncross-camera invariant features because intra-camera people\nalways share greater similarity due to common background,\nlighting conditions, etc. To enhance cross-camera invari-\nance, several methods exploit camera information to achieve\nbetter results. In particular, IICS leverages the intra-camera\nand inter-camera similarities in learning cross-camera in-\nvariance, which achieves the state-of-the-art evaluation re-\nsults on Market-1501, DukeMTMC-Re-ID and MSMT17\ndatasets.\nIn Table 3 and Table 4, performance on the MSMT17\ndataset is signiﬁcantly worse than on the Market1501 and\nDukeMTMC-Re-ID datasets in both unsupervised cross-\ndomain and fully unsupervised person Re-ID settings. The\nMSMT17 dataset is more challenging than the Market1501\nand DukeMTMC-Re-ID datasets because of more complex\nlighting, scene variations, random distracting samples. In\naddition, MSMT17 has a much larger number of identities\n(4,101) than Market (1,501) and Duke (1,404). Unsuper-\nvised cross-domain person Re-Id models learnt on source\ndomains are sensitive to person feature variations in un-\nknown target domains, which explains why UDA methods\nachieve better evaluation results on the Duke and Market\ndatasets. Similarly, image-based fully unsupervised person\nRe-ID models perform better on the Market and the Duke\ndatasets than on the MSMT17 datasets because it is more\nchallenging to learn person feature representations for 4,101\nidentities with more variations than for 1,501 and 1,404\nidentities with fewer variations.\nVideo-based fully unsupervised person Re-ID methods\nperformance is summarised in Table 5. Video-based un-\nsupervised person Re-ID is a less investigated area. Most\nvideo-based unsupervised learning uses temporary infor-\nmation in tracklets to associate person identities to track-\nlets. The clustering approach is commonly used to generate\npseudo-labels for the tracklets. To address the impact of\ncross-camera hard samples, SSL [53] learns soft labels for\ntraining samples, which is particularly helpful with the hard\nsamples, which yields the top performance.\n5. Discussion and Future Directions\nThis survey reviews the recent Re-ID advances focusing\non unsupervised methods that address the data scalability\nissue. Despite the fact that remarkable achievements have\nbeen made in the past few years, it remains an open ques-\ntion on addressing the signiﬁcant person Re-ID challenges,\nnamely the identity label estimation, discriminative person\nfeature learning, camera-invariant person feature learning\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 14 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nand the domain gap between datasets. Next, we highlight a\nfew promising future research directions.\nUnsupervised end-to-end person Re-ID. While most\nperson Re-ID works use hand-annotated or detected bound-\ning boxes to train the Re-ID, it is more practical to learn a\nperson Re-Id model end-to-end in which raw images and\nvideo are used directly to train the Re-ID model.\n[121]\nproposed to combine the person detection and person Re-\nID in a uniﬁed process. Since then, several supervised end-\nto-end person re-identiﬁcation methods have been proposed\nand proved that jointly considering person detection and\nperson Re-ID leads to higher retrieval accuracy than using\nthem separately due to the detection-identiﬁcation inconsis-\ntency issue [122]. However, there is still a gap in utilizing\nlarge unlabelled images and videos to learn Re-ID mod-\nels in an unsupervised end-to-end manner, which is more\npractical in real-world applications. Therefore, we suggest\nthere is a great research opportunity in unsupervised end-\nto-end person re-identiﬁcation, in particular, leveraging the\nevolutionary vision transformers [123, 124, 125, 126].\nUnsupervised generalizable person Re-ID. Unsuper-\nvised cross-domain person Re-Id models assume that the\nsource and target domains share considerable data similarity.\nThus the knowledge learnt in the source domain can be\nadapted into the target domain. However, this adaptation\nprocess is excessive and is required on each target domain,\nwhich restricts their applicability in arbitrarily domains [97].\nThe success of unsupervised cross-domain person Re-Id\nmodels depends on the source domain’s prior knowledge\nof the target domain. Therefore, when the source domain\ndata are diverse enough that cover the target domain, the\npre-trained Re-ID model on the source domain may be\nable to deploy to the target domain without domain adap-\ntation. Namely, unsupervised generalizable person Re-ID\nis achieved. A generalizable Re-Id model must be robust\nand discriminative in multiple environments. Therefore the\nmodel needs to be trained with suﬃcient annotated and syn-\nthesized data covering diverse photometric and geometric\nperson variations. DIMN [44] and OsNet [127] attempted to\nlearn one-ﬁt-all person feature representations out of public\nlabelled data in a supervised manner. Person representations\nare averaged over multiple representations hoping that they\nwould work in the unseen domains. However, these models\ndidn’t perform well as they didn’t see the data distributions\nin the target domains explored. Labelling cost is the biggest\nhurdle to collect suﬃcient annotated data to train models that\ngeneralize well to unknown domains. Instead, unlabelled\ndata are easy to collect and will cover more variations for\nthe Re-ID purpose. Therefore, unsupervised generalizable\nperson re-identiﬁcation is a promising direction due to the\nbeneﬁt of zero deployment cost to arbitrary domains.\nUnsupervised text-based cross-domain person Re-ID.\nText-based person re-identiﬁcation\n[128] aims to ﬁnd a\ntarget person from a gallery of images or videos for a\ngiven textual description. Text-based Re-ID is handy when\nthe query image is absent while the textual description of\nthe target person is available. It’s precious in an emer-\ngency environment such as the missing person search. While\ntraining a text-based Re-ID model for every environment\nis prohibitive, it’s reasonable to pre-train a model with\nannotated data and adapt to unlabeled domains. Like the\nimage-based unsupervised cross-domain person Re-ID, the\nabundant unlabelled data can be exploited to learn person\nfeature representations for the target domains. Instead of pre-\ndicting global pseudo-labels, the adaptation process needs to\nalign word-level features between the source and the target\ndomains for text-based domain adaptation. The attention\nmechanism and various distribution distance losses can be\nemployed to facilitate feature representation learning.\n6. Conclusion\nThis survey delivers a comprehensive review of the re-\ncent works on unsupervised person re-identiﬁcation, which\naddress the data scalability problem. Unsupervised person\nRe-ID has attracted much attention due to its practical appli-\ncations and research signiﬁcance. For the ﬁrst time, we sur-\nveyed unsupervised person Re-ID methods in both image-\nbased and video-based applications. We discuss highly re-\ngarded methods from the perspective of challenges and\nsolutions to inspire new ideas. We summarise and analyze\nunsupervised person Re-ID methods performance. We pro-\nvide insights that unsupervised person Re-ID methods need\nto address the joint challenge of identity label estimation,\ndiscriminative person feature representation learning, cross-\ncamera invariant feature learning and domain gap between\nsource and target datasets. We ﬁnally suggest several promis-\ning future research directions to inspire new research in the\nﬁeld.\nCRediT authorship contribution statement\nXiangtan Lin: Conceptualization, Investigation, Formal\nanalysis, Writing - original draft. Pengzhen Ren: Formal\nanalysis, Writing - review & editing. Chung-hsing Yeh:\nSupervision, Writing - review & editing. Lina Yao: Writing\n- review & editing. Andy Song: Writing - review & editing.\nXiaojun Chang: Conceptualization, Supervision, Funding\nacquisition, Resources, Writing - review & editing.\nDeclaration of competing interest\nThe authors declare that they have no known competing\nﬁnancial interests or personal relationships that could have\nappeared to inﬂuence the work reported in this paper.\nAcknowledgement\nThis work is partially supported by Australian Research\nCouncil (ARC) Discovery Early Career Researcher Award\n(DECRA) under DE190100626.\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 15 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nReferences\n[1] W. Liu, X. Chang, L. Chen, D. Phung, X. Zhang, Y. Yang, A. G.\nHauptmann, Pair-based uncertainty and diversity promoting early\nactive learning for person re-identiﬁcation, ACM Trans. Intell. Syst.\nTechnol. 11 (2) (2020) 21:1–21:15.\n[2] Z. Li, W. Liu, X. Chang, L. Yao, M. Prakash, H. Zhang, Domain-\naware unsupervised cross-dataset person re-identiﬁcation, in: J. Li,\nS. Wang, S. Qin, X. Li, S. Wang (Eds.), Advanced Data Mining and\nApplications - 15th International Conference, ADMA 2019, Dalian,\nChina, November 21-23, 2019, Proceedings, Vol. 11888 of Lecture\nNotes in Computer Science, Springer, 2019, pp. 406–420.\n[3] D. Cheng, Y. Gong, X. Chang, W. Shi, A. G. Hauptmann, N. Zheng,\nDeep feature learning via structured graph laplacian embedding for\nperson re-identiﬁcation, Pattern Recognit. 82 (2018) 94–104.\n[4] W. Liu, X. Chang, L. Chen, Y. Yang, Semi-supervised bayesian\nattribute learning for person re-identiﬁcation, in: Proceedings of the\nThirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, AAAI Press, 2018, pp. 7162–7169.\n[5] D. Cheng, X. Chang, L. Liu, A. G. Hauptmann, Y. Gong, N. Zheng,\nDiscriminative dictionary learning with ranking metric embedded\nfor person re-identiﬁcation, in: Proceedings of the Twenty-Sixth In-\nternational Joint Conference on Artiﬁcial Intelligence, IJCAI 2017,\nMelbourne, Australia, August 19-25, 2017, 2017, pp. 964–970.\n[6] W. Liu, X. Chang, L. Chen, Y. Yang, Early active learning with\npairwise constraint for person re-identiﬁcation, in: Machine Learn-\ning and Knowledge Discovery in Databases - European Conference,\nECML PKDD 2017, Skopje, Macedonia, September 18-22, 2017,\nProceedings, Part I, Vol. 10534 of Lecture Notes in Computer\nScience, Springer, 2017, pp. 103–118.\n[7] W. Li, R. Zhao, T. Xiao, X. Wang, DeepReID: Deep Filter Pairing\nNeural Network for Person Re-Identiﬁcation, 2014, pp. 152–159.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2014/html/Li_\nDeepReID_Deep_Filter_2014_CVPR_paper.html\n[8] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, Q. Tian, Scalable\nPerson Re-Identiﬁcation: A Benchmark, 2015, pp. 1116–1124.\nURL\nhttps://www.cv-foundation.org/openaccess/content_iccv_\n2015/html/Zheng_Scalable_Person_Re-Identification_ICCV_2015_\npaper.html\n[9] C. Liu, X. Chang, Y.-D. Shen, Unity Style Transfer for Person\nRe-Identiﬁcation, 2020, pp. 6887–6896.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Liu_\nUnity_Style_Transfer_for_Person_Re-Identification_CVPR_2020_\npaper.html\n[10] Z. Zheng, L. Zheng, Y. Yang, Unlabeled Samples Generated by\nGAN Improve the Person Re-Identiﬁcation Baseline in Vitro, 2017,\npp. 3754–3762.\nURL\nhttps://openaccess.thecvf.com/content_iccv_2017/html/\nZheng_Unlabeled_Samples_Generated_ICCV_2017_paper.html\n[11] H. Wang, S. Gong, T. Xiang, Unsupervised Learning of Generative\nTopic Saliency for Person Re-identiﬁcation, in: Proceedings of the\nBritish Machine Vision Conference 2014, British Machine Vision\nAssociation, Nottingham, 2014, pp. 48.1–48.11. doi:10.5244/C.28.\n48.\nURL http://www.bmva.org/bmvc/2014/papers/paper019/index.html\n[12] L. Zheng, Y. Yang, A. G. Hauptmann, Person Re-identiﬁcation: Past,\nPresent and Future, arXiv:1610.02984 [cs]ArXiv: 1610.02984 (Oct.\n2016).\nURL http://arxiv.org/abs/1610.02984\n[13] H. Chahar, N. Nain, A Study on Deep Convolutional Neural Network\nBased Approaches for Person Re-identiﬁcation, in: B. U. Shankar,\nK. Ghosh, D. P. Mandal, S. S. Ray, D. Zhang, S. K. Pal (Eds.), Pattern\nRecognition and Machine Intelligence, Lecture Notes in Computer\nScience, Springer International Publishing, Cham, 2017, pp. 543–\n548. doi:10.1007/978-3-319-69900-4_69.\n[14] K. Wang, H. Wang, M. Liu, X. Xing, T. Han, Survey on person re-\nidentiﬁcation based on deep learning, CAAI Trans. Intell. Technol.\n(2018). doi:10.1049/TRIT.2018.1001.\n[15] B. Lavi, M. F. Serj, I. Ullah, Survey on Deep Learning Techniques\nfor Person Re-Identiﬁcation Task, arXiv:1807.05284 [cs]ArXiv:\n1807.05284 (Jul. 2018).\nURL http://arxiv.org/abs/1807.05284\n[16] D. Wu, S.-J. Zheng, X.-P. Zhang, C.-A. Yuan, F. Cheng,\nY. Zhao, Y.-J. Lin, Z.-Q. Zhao, Y.-L. Jiang, D.-S. Huang,\nDeep learning-based methods for person re-identiﬁcation: A\ncomprehensive review, Neurocomputing 337 (2019) 354–371.\ndoi:10.1016/j.neucom.2019.01.079.\nURL\nhttps://www.sciencedirect.com/science/article/pii/\nS0925231219301225\n[17] M. O. Almasawa, L. A. Elrefaei, K. Moria, A Survey on Deep\nLearning-Based Person Re-Identiﬁcation Systems, IEEE Access 7\n(2019) 175228–175247, conference Name: IEEE Access. doi:10.\n1109/ACCESS.2019.2957336.\n[18] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, S. C. H. Hoi, Deep\nLearning for Person Re-identiﬁcation: A Survey and Outlook, IEEE\nTransactions on Pattern Analysis and Machine Intelligence (2021)\n1–1Conference Name: IEEE Transactions on Pattern Analysis and\nMachine Intelligence. doi:10.1109/TPAMI.2021.3054775.\n[19] Z. Wang, Z. Wang, Y. Zheng, Y. Wu, W. Zeng, S. Satoh, Be-\nyond Intra-modality: A Survey of Heterogeneous Person Re-\nidentiﬁcation, arXiv:1905.10048 [cs]ArXiv: 1905.10048 (Apr.\n2020).\nURL http://arxiv.org/abs/1905.10048\n[20] Q. Leng, M. Ye, Q. Tian, A Survey of Open-World Person Re-\nIdentiﬁcation, IEEE Transactions on Circuits and Systems for Video\nTechnology 30 (4) (2020) 1092–1108, conference Name: IEEE\nTransactions on Circuits and Systems for Video Technology. doi:\n10.1109/TCSVT.2019.2898940.\n[21] G. Wang, S. Yang, H. Liu, Z. Wang, Y. Yang, S. Wang, G. Yu,\nE. Zhou, J. Sun, High-Order Information Matters: Learning Relation\nand Topology for Occluded Person Re-Identiﬁcation, 2020, pp.\n6449–6458.\nURL https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_\nHigh-Order_Information_Matters_Learning_Relation_and_Topology_\nfor_Occluded_Person_CVPR_2020_paper.html\n[22] S. Gao, J. Wang, H. Lu, Z. Liu, Pose-Guided Visible Part Matching\nfor Occluded Person ReID, 2020, pp. 11744–11752.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Gao_\nPose-Guided_Visible_Part_Matching_for_Occluded_Person_ReID_\nCVPR_2020_paper.html\n[23] J. Miao, Y. Wu, P. Liu, Y. Ding, Y. Yang, Pose-Guided Feature\nAlignment for Occluded Person Re-Identiﬁcation, 2019, pp.\n542–551.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/\nMiao_Pose-Guided_Feature_Alignment_for_Occluded_Person_\nRe-Identification_ICCV_2019_paper.html\n[24] H. Huang, D. Li, Z. Zhang, X. Chen, K. Huang, Adversarially\nOccluded Samples for Person Re-Identiﬁcation, 2018, pp. 5098–\n5107.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2018/html/\nHuang_Adversarially_Occluded_Samples_CVPR_2018_paper.html\n[25] L. He, W. Liu, Guided Saliency Feature Learning for Person\nRe-identiﬁcation in Crowded Scenes, in: A. Vedaldi, H. Bischof,\nT. Brox, J.-M. Frahm (Eds.), Computer Vision – ECCV 2020, Vol.\n12373, Springer International Publishing, Cham, 2020, pp. 357–\n373, series Title: Lecture Notes in Computer Science. doi:10.1007/\n978-3-030-58604-1_22.\nURL https://link.springer.com/10.1007/978-3-030-58604-1_22\n[26] X. Qian, W. Wang, L. Zhang, F. Zhu, Y. Fu, T. Xiang, Y.-G. Jiang,\nX. Xue, Long-Term Cloth-Changing Person Re-identiﬁcation,\n2020.\nURL\nhttps://openaccess.thecvf.com/content/ACCV2020/html/Qian_\nLong-Term_Cloth-Changing_Person_Re-identification_ACCV_2020_\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 16 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\npaper.html\n[27] S. Choi, S. Lee, Y. Kim, T. Kim, C. Kim, Hi-CMD: Hierarchical\nCross-Modality\nDisentanglement\nfor\nVisible-Infrared\nPerson\nRe-Identiﬁcation, 2020, pp. 10257–10266.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/\nChoi_Hi-CMD_Hierarchical_Cross-Modality_Disentanglement_for_\nVisible-Infrared_Person_Re-Identification_CVPR_2020_paper.html\n[28] Y. Lu, Y. Wu, B. Liu, T. Zhang, B. Li, Q. Chu, N. Yu, Cross-Modality\nPerson Re-Identiﬁcation With Shared-Speciﬁc Feature Transfer,\n2020, pp. 13379–13389.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Lu_\nCross-Modality_Person_Re-Identification_With_Shared-Specific_\nFeature_Transfer_CVPR_2020_paper.html\n[29] A. Wu, W.-S. Zheng, H.-X. Yu, S. Gong, J. Lai, RGB-Infrared\nCross-Modality Person Re-Identiﬁcation, 2017, pp. 5380–5389.\nURL\nhttps://openaccess.thecvf.com/content_iccv_2017/html/Wu_\nRGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.html\n[30] M. Farenzena, L. Bazzani, A. Perina, V. Murino, M. Cristani, Person\nre-identiﬁcation by symmetry-driven accumulation of local features,\nin: 2010 IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition, 2010, pp. 2360–2367, iSSN: 1063-6919.\ndoi:10.1109/CVPR.2010.5539926.\n[31] W. Li, X. Wang, Locally Aligned Feature Transforms across Views,\n2013, pp. 3594–3601.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2013/html/Li_\nLocally_Aligned_Feature_2013_CVPR_paper.html\n[32] R. Zhao, W. Ouyang, X. Wang, Unsupervised Salience Learning for\nPerson Re-identiﬁcation, 2013, pp. 3586–3593.\nURL\nhttps://www.cv-foundation.org/openaccess/content_cvpr_\n2013/html/Zhao_Unsupervised_Salience_Learning_2013_CVPR_paper.\nhtml\n[33] S. Liao, Y. Hu, X. Zhu, S. Z. Li, Person Re-Identiﬁcation by Local\nMaximal Occurrence Representation and Metric Learning, 2015, pp.\n2197–2206.\nURL https://openaccess.thecvf.com/content_cvpr_2015/html/Liao_\nPerson_Re-Identification_by_2015_CVPR_paper.html\n[34] W. Li, X. Zhu, S. Gong, Harmonious Attention Network for Person\nRe-Identiﬁcation, 2018, pp. 2285–2294.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2018/html/Li_\nHarmonious_Attention_Network_CVPR_2018_paper.html\n[35] J. Si, H. Zhang, C.-G. Li, J. Kuen, X. Kong, A. C. Kot, G. Wang,\nDual Attention Matching Network for Context-Aware Feature\nSequence Based Person Re-Identiﬁcation, 2018, pp. 5363–5372.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2018/html/Si_\nDual_Attention_Matching_CVPR_2018_paper.html\n[36] L. Zhao, X. Li, Y. Zhuang, J. Wang, Deeply-Learned Part-Aligned\nRepresentations for Person Re-Identiﬁcation, 2017, pp. 3219–3228.\nURL https://openaccess.thecvf.com/content_iccv_2017/html/Zhao_\nDeeply-Learned_Part-Aligned_Representations_ICCV_2017_paper.\nhtml\n[37] S. Zhou, J. Wang, J. Wang, Y. Gong, N. Zheng, Point to Set Similar-\nity Based Deep Feature Learning for Person Re-Identiﬁcation, 2017,\npp. 3741–3750.\nURL https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_\nPoint_to_Set_CVPR_2017_paper.html\n[38] W. Li, X. Zhu, S. Gong, Person Re-Identiﬁcation by Deep Joint\nLearning of Multi-Loss Classiﬁcation, in: Proceedings of the\nTwenty-Sixth International Joint Conference on Artiﬁcial Intelli-\ngence, International Joint Conferences on Artiﬁcial Intelligence\nOrganization, Melbourne, Australia, 2017, pp. 2194–2200.\ndoi:\n10.24963/ijcai.2017/305.\nURL https://www.ijcai.org/proceedings/2017/305\n[39] D. Li, X. Chen, Z. Zhang, K. Huang, Learning Deep Context-Aware\nFeatures Over Body and Latent Parts for Person Re-Identiﬁcation,\n2017, pp. 384–393.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2017/html/Li_\nLearning_Deep_Context-Aware_CVPR_2017_paper.html\n[40] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang, X. Tang,\nSpindle Net: Person Re-Identiﬁcation With Human Body Region\nGuided Feature Decomposition and Fusion, 2017, pp. 1077–1085.\nURL https://openaccess.thecvf.com/content_cvpr_2017/html/Zhao_\nSpindle_Net_Person_CVPR_2017_paper.html\n[41] T. Xiao, H. Li, W. Ouyang, X. Wang, Learning Deep Feature\nRepresentations With Domain Guided Dropout for Person Re-\nIdentiﬁcation, 2016, pp. 1249–1258.\nURL https://openaccess.thecvf.com/content_cvpr_2016/html/Xiao_\nLearning_Deep_Feature_CVPR_2016_paper.html\n[42] J. Liu, Z.-J. Zha, Q. Tian, D. Liu, T. Yao, Q. Ling, T. Mei, Multi-\nScale Triplet CNN for Person Re-Identiﬁcation, in: Proceedings of\nthe 24th ACM international conference on Multimedia, MM ’16,\nAssociation for Computing Machinery, New York, NY, USA, 2016,\npp. 192–196. doi:10.1145/2964284.2967209.\nURL https://doi.org/10.1145/2964284.2967209\n[43] N. McLaughlin, J. M. del Rincon, P. Miller, Recurrent Convolutional\nNetwork for Video-Based Person Re-Identiﬁcation, 2016, pp. 1325–\n1334.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2016/html/\nMcLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.html\n[44] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, T. M. Hospedales,\nGeneralizable\nPerson\nRe-Identiﬁcation\nby\nDomain-Invariant\nMapping Network, 2019, pp. 719–728.\nURL https://openaccess.thecvf.com/content_CVPR_2019/html/Song_\nGeneralizable_Person_Re-Identification_by_Domain-Invariant_\nMapping_Network_CVPR_2019_paper.html\n[45] D. Gray, H. Tao, Viewpoint Invariant Pedestrian Recognition with\nan Ensemble of Localized Features, in: D. Forsyth, P. Torr, A. Zis-\nserman (Eds.), Computer Vision – ECCV 2008, Lecture Notes in\nComputer Science, Springer, Berlin, Heidelberg, 2008, pp. 262–275.\ndoi:10.1007/978-3-540-88682-2_21.\n[46] K. Zheng, W. Liu, L. He, T. Mei, J. Luo, Z.-J. Zha, Group-\naware Label Transfer for Domain Adaptive Person Re-identiﬁcation,\narXiv:2103.12366 [cs]ArXiv: 2103.12366 (Mar. 2021).\nURL http://arxiv.org/abs/2103.12366\n[47] Y. Sun, L. Zheng, Y. Yang, Q. Tian, S. Wang, Beyond Part\nModels: Person Retrieval with Reﬁned Part Pooling (and A Strong\nConvolutional Baseline), 2018, pp. 480–496.\nURL\nhttps://openaccess.thecvf.com/content_ECCV_2018/html/\nYifan_Sun_Beyond_Part_Models_ECCV_2018_paper.html\n[48] G. Wang, Y. Yuan, X. Chen, J. Li, X. Zhou, Learning Discriminative\nFeatures with Multiple Granularities for Person Re-Identiﬁcation,\nProceedings of the 26th ACM international conference on Multi-\nmedia (2018) 274–282ArXiv: 1804.01438.\ndoi:10.1145/3240508.\n3240552.\nURL http://arxiv.org/abs/1804.01438\n[49] L. Wei, S. Zhang, W. Gao, Q. Tian, Person Transfer GAN to Bridge\nDomain Gap for Person Re-Identiﬁcation, 2018, pp. 79–88.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2018/html/Wei_\nPerson_Transfer_GAN_CVPR_2018_paper.html\n[50] Z. Zhong, L. Zheng, Z. Luo, S. Li, Y. Yang, Invariance Matters:\nExemplar Memory for Domain Adaptive Person Re-Identiﬁcation,\n2019, pp. 598–607.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2019/html/\nZhong_Invariance_Matters_Exemplar_Memory_for_Domain_Adaptive_\nPerson_Re-Identification_CVPR_2019_paper.html\n[51] Y. Ge, D. Chen, H. Li, Mutual Mean-Teaching: Pseudo Label\nReﬁnery for Unsupervised Domain Adaptation on Person Re-\nidentiﬁcation, 2020.\nURL https://iclr.cc/virtual_2020/poster_rJlnOhVYPS.html\n[52] Y. Lin, X. Dong, L. Zheng, Y. Yan, Y. Yang, A Bottom-Up Cluster-\ning Approach to Unsupervised Person Re-Identiﬁcation, Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence 33 (01)\n(2019) 8738–8745, number: 01. doi:10.1609/aaai.v33i01.33018738.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/4898\n[53] Y. Lin, L. Xie, Y. Wu, C. Yan, Q. Tian, Unsupervised Person\nRe-Identiﬁcation via Softened Similarity Learning, 2020, pp.\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 17 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\n3390–3399.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/Lin_\nUnsupervised_Person_Re-Identification_via_Softened_Similarity_\nLearning_CVPR_2020_paper.html\n[54] K. Zeng, M. Ning, Y. Wang, Y. Guo, Hierarchical Clustering With\nHard-Batch Triplet Loss for Person Re-Identiﬁcation, 2020, pp.\n13657–13665.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/\nZeng_Hierarchical_Clustering_With_Hard-Batch_Triplet_Loss_for_\nPerson_Re-Identification_CVPR_2020_paper.html\n[55] H. Fan, L. Zheng, C. Yan, Y. Yang, Unsupervised Person Re-\nidentiﬁcation: Clustering and Fine-tuning, ACM Transactions on\nMultimedia Computing, Communications, and Applications 14 (4)\n(2018) 83:1–83:18. doi:10.1145/3243316.\nURL https://doi.org/10.1145/3243316\n[56] Y. Fu, Y. Wei, G. Wang, Y. Zhou, H. Shi, T. S. Huang, Self-\nSimilarity Grouping: A Simple Unsupervised Cross Domain\nAdaptation Approach for Person Re-Identiﬁcation, 2019, pp.\n6112–6121.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/Fu_\nSelf-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_\nAdaptation_Approach_for_ICCV_2019_paper.html\n[57] S. Lin, Multi-task Mid-level Feature Alignment Network for Unsu-\npervised Cross-Dataset Person Re-Identiﬁcation (2018) 13.\n[58] J. Wang, X. Zhu, S. Gong, W. Li, Transferable Joint Attribute-\nIdentity Deep Learning for Unsupervised Person Re-Identiﬁcation,\n2018, pp. 2275–2284.\nURL https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_\nTransferable_Joint_Attribute-Identity_CVPR_2018_paper.html\n[59] H.-X. Yu, W.-S. Zheng, A. Wu, X. Guo, S. Gong, J.-H. Lai,\nUnsupervised Person Re-Identiﬁcation by Soft Multilabel Learning,\n2019, pp. 2148–2157.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2019/html/\nYu_Unsupervised_Person_Re-Identification_by_Soft_Multilabel_\nLearning_CVPR_2019_paper.html\n[60] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, J. Jiao, Image-Image\nDomain Adaptation With Preserved Self-Similarity and Domain-\nDissimilarity for Person Re-Identiﬁcation, 2018, pp. 994–1003.\nURL https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_\nImage-Image_Domain_Adaptation_CVPR_2018_paper.html\n[61] Z. Zhong, L. Zheng, S. Li, Y. Yang, Generalizing A Person Retrieval\nModel Hetero- and Homogeneously, 2018, pp. 172–188.\nURL https://openaccess.thecvf.com/content_ECCV_2018/html/Zhun_\nZhong_Generalizing_A_Person_ECCV_2018_paper.html\n[62] Y. Chen, X. Zhu, S. Gong, Instance-Guided Context Rendering for\nCross-Domain Person Re-Identiﬁcation, 2019, pp. 232–242.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/\nChen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_\nRe-Identification_ICCV_2019_paper.html\n[63] A. Wu, W.-S. Zheng, J.-H. Lai, Unsupervised Person Re-\nIdentiﬁcation by Camera-Aware Similarity Consistency Learning,\n2019, pp. 6922–6931.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/\nWu_Unsupervised_Person_Re-Identification_by_Camera-Aware_\nSimilarity_Consistency_Learning_ICCV_2019_paper.html\n[64] Y.-J. Li, C.-S. Lin, Y.-B. Lin, Y.-C. F. Wang, Cross-Dataset Person\nRe-Identiﬁcation via Unsupervised Pose Disentanglement and\nAdaptation, 2019, pp. 7919–7929.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/Li_\nCross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_\nDisentanglement_and_Adaptation_ICCV_2019_paper.html\n[65] L. Qi, L. Wang, J. Huo, L. Zhou, Y. Shi, Y. Gao, A Novel\nUnsupervised Camera-Aware Domain Adaptation Framework for\nPerson Re-Identiﬁcation, 2019, pp. 8080–8089.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/Qi_\nA_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_\nfor_Person_Re-Identification_ICCV_2019_paper.html\n[66] X. Zhang, J. Cao, C. Shen, M. You, Self-Training With Progressive\nAugmentation\nfor\nUnsupervised\nCross-Domain\nPerson\nRe-\nIdentiﬁcation, 2019, pp. 8222–8231.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/\nZhang_Self-Training_With_Progressive_Augmentation_for_\nUnsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_\npaper.html\n[67] E. Kodirov, T. Xiang, S. Gong, Dictionary Learning with Iterative\nLaplacian Regularisation for Unsupervised Person Re-identiﬁcation,\nin: Procedings of the British Machine Vision Conference 2015,\nBritish Machine Vision Association, Swansea, 2015, pp. 44.1–44.12.\ndoi:10.5244/C.29.44.\nURL http://www.bmva.org/bmvc/2015/papers/paper044/index.html\n[68] H. Wang, X. Zhu, T. Xiang, S. Gong, Towards unsupervised open-set\nperson re-identiﬁcation, 2016, pp. 769–773. doi:10.1109/ICIP.2016.\n7532461.\n[69] H.-X. Yu, A. Wu, W.-S. Zheng, Cross-View Asymmetric Metric\nLearning for Unsupervised Person Re-Identiﬁcation, 2017, pp.\n994–1002.\nURL\nhttps://openaccess.thecvf.com/content_iccv_2017/html/Yu_\nCross-View_Asymmetric_Metric_ICCV_2017_paper.html\n[70] M. Ye, A. Ma, L. Zheng, J. Li, P. C. Yuen, Dynamic Label Graph\nMatching for Unsupervised Video Re-Identiﬁcation (Sep. 2017).\ndoi:10.1109/ICCV.2017.550.\n[71] Z. Liu, D. Wang, H. Lu, Stepwise Metric Promotion for Unsuper-\nvised Video Person Re-Identiﬁcation, 2017, pp. 2429–2438.\nURL\nhttps://openaccess.thecvf.com/content_iccv_2017/html/Liu_\nStepwise_Metric_Promotion_ICCV_2017_paper.html\n[72] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Ouyang, Y. Yang, Exploit\nthe Unknown Gradually: One-Shot Video-Based Person Re-\nIdentiﬁcation by Stepwise Learning, 2018, pp. 5177–5186.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2018/html/Wu_\nExploit_the_Unknown_CVPR_2018_paper.html\n[73] J. Wu, S. Liao, Z. lei, X. Wang, Y. Yang, S. Z. Li, Clustering and\nDynamic Sampling Based Unsupervised Domain Adaptation for\nPerson Re-Identiﬁcation, in: 2019 IEEE International Conference\non Multimedia and Expo (ICME), 2019, pp. 886–891, iSSN: 1945-\n788X. doi:10.1109/ICME.2019.00157.\n[74] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, Q. Tian, MARS:\nA Video Benchmark for Large-Scale Person Re-Identiﬁcation, in:\nB. Leibe, J. Matas, N. Sebe, M. Welling (Eds.), Computer Vision\n– ECCV 2016, Lecture Notes in Computer Science, Springer In-\nternational Publishing, Cham, 2016, pp. 868–884.\ndoi:10.1007/\n978-3-319-46466-4_52.\n[75] G. Wu, X. Zhu, S. Gong, Tracklet Self-Supervised Learning for\nUnsupervised Person Re-Identiﬁcation, Proceedings of the AAAI\nConference on Artiﬁcial Intelligence 34 (07) (2020) 12362–12369,\nnumber: 07. doi:10.1609/aaai.v34i07.6921.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/6921\n[76] F. Zhao, S. Liao, G.-S. Xie, J. Zhao, K. Zhang, L. Shao, Unsu-\npervised Domain Adaptation with Noise Resistible Mutual-Training\nfor Person Re-identiﬁcation, in: A. Vedaldi, H. Bischof, T. Brox,\nJ.-M. Frahm (Eds.), Computer Vision – ECCV 2020, Vol. 12356,\nSpringer International Publishing, Cham, 2020, pp. 526–544, se-\nries Title: Lecture Notes in Computer Science.\ndoi:10.1007/\n978-3-030-58621-8_31.\nURL https://link.springer.com/10.1007/978-3-030-58621-8_31\n[77] F. Yang, K. Li, Z. Zhong, Z. Luo, X. Sun, H. Cheng, X. Guo,\nF. Huang, R. Ji, S. Li, Asymmetric Co-Teaching for Unsupervised\nCross-Domain Person Re-Identiﬁcation, Proceedings of the AAAI\nConference on Artiﬁcial Intelligence 34 (07) (2020) 12597–12604,\nnumber: 07. doi:10.1609/aaai.v34i07.6950.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/6950\n[78] Y. Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, Y. Tian, Multiple Expert\nBrainstorming for Domain Adaptive Person Re-Identiﬁcation, in:\nA. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer\nVision – ECCV 2020, Vol. 12352, Springer International Publishing,\nCham, 2020, pp. 594–611, series Title: Lecture Notes in Computer\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 18 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nScience. doi:10.1007/978-3-030-58571-6_35.\nURL https://link.springer.com/10.1007/978-3-030-58571-6_35\n[79] X. Jin, C. Lan, W. Zeng, Z. Chen, Global Distance-Distributions\nSeparation\nfor\nUnsupervised\nPerson\nRe-identiﬁcation,\nin:\nA. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer\nVision – ECCV 2020, Vol. 12352, Springer International Publishing,\nCham, 2020, pp. 735–751, series Title: Lecture Notes in Computer\nScience. doi:10.1007/978-3-030-58571-6_43.\nURL https://link.springer.com/10.1007/978-3-030-58571-6_43\n[80] F. Yang, Z. Zhong, Z. Luo, Y. Cai, Y. Lin, S. Li, N. Sebe, Joint\nNoise-Tolerant Learning and Meta Camera Shift Adaptation for Un-\nsupervised Person Re-Identiﬁcation, arXiv:2103.04618 [cs]ArXiv:\n2103.04618 (Mar. 2021).\nURL http://arxiv.org/abs/2103.04618\n[81] T. Xiao, S. Li, B. Wang, L. Lin, X. Wang, Joint Detection and\nIdentiﬁcation Feature Learning for Person Search, in: 2017 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR),\nIEEE, Honolulu, HI, 2017, pp. 3376–3385. doi:10.1109/CVPR.2017.\n360.\nURL http://ieeexplore.ieee.org/document/8099843/\n[82] D. Wang, S. Zhang, Unsupervised Person Re-Identiﬁcation via\nMulti-Label Classiﬁcation, 2020, pp. 10981–10990.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/\nWang_Unsupervised_Person_Re-Identification_via_Multi-Label_\nClassification_CVPR_2020_paper.html\n[83] W. Xiang, H. Yong, J. Huang, X.-S. Hua, L. Zhang, Second-order\nCamera-aware Color Transformation for Cross-domain Person\nRe-identiﬁcation, 2020.\nURL\nhttps://openaccess.thecvf.com/content/ACCV2020/html/\nXiang_Second-order_Camera-aware_Color_Transformation_for_\nCross-domain_Person_Re-identification_ACCV_2020_paper.html\n[84] Y. Zhai, S. Lu, Q. Ye, X. Shan, J. Chen, R. Ji, Y. Tian, AD-Cluster:\nAugmented Discriminative Clustering for Domain Adaptive Person\nRe-Identiﬁcation, 2020, pp. 9021–9030.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/\nZhai_AD-Cluster_Augmented_Discriminative_Clustering_for_Domain_\nAdaptive_Person_Re-Identification_CVPR_2020_paper.html\n[85] W. Xiang, J. Huang, X.-S. Hua, L. Zhang, Part-aware Attention\nNetwork for Person Re-Identiﬁcation, 2020.\nURL https://openaccess.thecvf.com/content/ACCV2020/html/Xiang_\nPart-aware_Attention_Network_for_Person_Re-Identification_ACCV_\n2020_paper.html\n[86] J. Guo, Y. Yuan, L. Huang, C. Zhang, J.-G. Yao, K. Han, Beyond\nHuman Parts: Dual Part-Aligned Representations for Person\nRe-Identiﬁcation, 2019, pp. 3642–3651.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/\nGuo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_\nPerson_Re-Identification_ICCV_2019_paper.html\n[87] Q.\nYang,\nH.-X.\nYu,\nA.\nWu,\nW.-S.\nZheng,\nPatch-Based\nDiscriminative Feature Learning for Unsupervised Person Re-\nIdentiﬁcation, 2019, pp. 3633–3642.\nURL https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_\nPatch-Based_Discriminative_Feature_Learning_for_Unsupervised_\nPerson_Re-Identification_CVPR_2019_paper.html\n[88] Z. Ji, X. Zou, X. Lin, X. Liu, T. Huang, S. Wu, An Attention-\nDriven Two-Stage Clustering Method for Unsupervised Person Re-\nidentiﬁcation, in: A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm\n(Eds.), Computer Vision – ECCV 2020, Vol. 12373, Springer In-\nternational Publishing, Cham, 2020, pp. 20–36, series Title: Lecture\nNotes in Computer Science. doi:10.1007/978-3-030-58604-1_2.\nURL https://link.springer.com/10.1007/978-3-030-58604-1_2\n[89] G. Chen, Y. Lu, J. Lu, J. Zhou, Deep Credible Metric Learning\nfor Unsupervised Domain Adaptation Person Re-identiﬁcation, in:\nA. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer\nVision – ECCV 2020, Vol. 12353, Springer International Publishing,\nCham, 2020, pp. 643–659, series Title: Lecture Notes in Computer\nScience. doi:10.1007/978-3-030-58598-3_38.\nURL https://link.springer.com/10.1007/978-3-030-58598-3_38\n[90] A. Hermans, L. Beyer, B. Leibe, In Defense of the Triplet Loss for\nPerson Re-Identiﬁcation, arXiv:1703.07737 [cs]ArXiv: 1703.07737\n(Nov. 2017).\nURL http://arxiv.org/abs/1703.07737\n[91] J. Li, S. Zhang, Joint Visual and Temporal Consistency for Unsu-\npervised Domain Adaptive Person Re-identiﬁcation, in: A. Vedaldi,\nH. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer Vision – ECCV\n2020, Vol. 12369, Springer International Publishing, Cham, 2020,\npp. 483–499, series Title: Lecture Notes in Computer Science. doi:\n10.1007/978-3-030-58586-0_29.\nURL https://link.springer.com/10.1007/978-3-030-58586-0_29\n[92] L. Leal-Taixé, A. Milan, I. Reid, S. Roth, K. Schindler, MOTChal-\nlenge 2015: Towards a Benchmark for Multi-Target Tracking,\narXiv:1504.01942 [cs]ArXiv: 1504.01942 (Apr. 2015).\nURL http://arxiv.org/abs/1504.01942\n[93] E. Ristani, F. Solera, R. Zou, R. Cucchiara, C. Tomasi, Perfor-\nmance Measures and a Data Set for Multi-target, Multi-camera\nTracking, in: G. Hua, H. Jégou (Eds.), Computer Vision – ECCV\n2016 Workshops, Lecture Notes in Computer Science, Springer\nInternational Publishing, Cham, 2016, pp. 17–35.\ndoi:10.1007/\n978-3-319-48881-3_2.\n[94] J. Gao, T. Zhang, C. Xu, Graph Convolutional Tracking, in: 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), IEEE, Long Beach, CA, USA, 2019, pp. 4644–4654. doi:\n10.1109/CVPR.2019.00478.\nURL https://ieeexplore.ieee.org/document/8953448/\n[95] J. Gao, I Know the Relationships: Zero-Shot Action Recognition\nvia Two-Stream Graph Convolutional Networks and Knowledge\nGraphs. (2019) 8303–8311doi:10.1609/aaai.v33i01.33018303.\n[96] Z. Zhang, H. Peng, Deeper and Wider Siamese Networks for Real-\nTime Visual Tracking, arXiv:1901.01660 [cs]ArXiv: 1901.01660\n(Mar. 2019).\nURL http://arxiv.org/abs/1901.01660\n[97] M. Li, X. Zhu, S. Gong, Unsupervised Person Re-identiﬁcation\nby Deep Learning Tracklet Association, in: V. Ferrari, M. Hebert,\nC. Sminchisescu, Y. Weiss (Eds.), Computer Vision – ECCV 2018,\nVol. 11208, Springer International Publishing, Cham, 2018, pp.\n772–788, series Title: Lecture Notes in Computer Science.\ndoi:\n10.1007/978-3-030-01225-0_45.\nURL http://link.springer.com/10.1007/978-3-030-01225-0_45\n[98] M. Li, X. Zhu, S. Gong, Unsupervised Tracklet Person Re-\nIdentiﬁcation, IEEE Transactions on Pattern Analysis and Machine\nIntelligence 42 (7) (2020) 1770–1782, conference Name: IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\ndoi:\n10.1109/TPAMI.2019.2903058.\n[99] C. Luo, C. Song, Z. Zhang, Generalizing Person Re-Identiﬁcation\nby Camera-Aware Invariance Learning and Cross-Domain Mixup,\nin: A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer\nVision – ECCV 2020, Vol. 12360, Springer International Publishing,\nCham, 2020, pp. 224–241, series Title: Lecture Notes in Computer\nScience. doi:10.1007/978-3-030-58555-6_14.\nURL https://link.springer.com/10.1007/978-3-030-58555-6_14\n[100] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, S. Z. Li, Unsupervised Graph\nAssociation for Person Re-Identiﬁcation, 2019, pp. 8321–8330.\nURL\nhttps://openaccess.thecvf.com/content_ICCV_2019/html/Wu_\nUnsupervised_Graph_Association_for_Person_Re-Identification_\nICCV_2019_paper.html\n[101] J. Lv, W. Chen, Q. Li, C. Yang, Unsupervised Cross-Dataset\nPerson Re-Identiﬁcation by Transfer Learning of Spatial-Temporal\nPatterns, 2018, pp. 7948–7956.\nURL\nhttps://openaccess.thecvf.com/content_cvpr_2018/html/Lv_\nUnsupervised_Cross-Dataset_Person_CVPR_2018_paper.html\n[102] D. Mekhazni, A. Bhuiyan, G. Ekladious, E. Granger, Unsuper-\nvised Domain Adaptation in the Dissimilarity Space for Person Re-\nidentiﬁcation, in: A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm\n(Eds.), Computer Vision – ECCV 2020, Vol. 12372, Springer Inter-\nnational Publishing, Cham, 2020, pp. 159–174, series Title: Lecture\nNotes in Computer Science. doi:10.1007/978-3-030-58583-9_10.\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 19 of 20\nUnsupervised Person Re-Identiﬁcation: A Systematic Survey of Challenges and Solutions\nURL https://link.springer.com/10.1007/978-3-030-58583-9_10\n[103] G. Wang, J.-H. Lai, W. Liang, G. Wang, Smoothing Adversarial\nDomain Attack and P-Memory Reconsolidation for Cross-Domain\nPerson Re-Identiﬁcation, 2020, pp. 10568–10577.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2020/html/\nWang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_\nReconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.html\n[104] Y. Huang, P. Peng, Y. Jin, Y. Li, J. Xing, Domain Adaptive Attention\nLearning for Unsupervised Person Re-Identiﬁcation, Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence 34 (07) (2020)\n11069–11076, number: 07. doi:10.1609/aaai.v34i07.6762.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/6762\n[105] J. Liu, Z.-J. Zha, D. Chen, R. Hong, M. Wang, Adaptive Transfer\nNetwork for Cross-Domain Person Re-Identiﬁcation, 2019, pp.\n7202–7211.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2019/html/\nLiu_Adaptive_Transfer_Network_for_Cross-Domain_Person_\nRe-Identification_CVPR_2019_paper.html\n[106] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired Image-to-Image\nTranslation Using Cycle-Consistent Adversarial Networks, in: 2017\nIEEE International Conference on Computer Vision (ICCV), 2017,\npp. 2242–2251, iSSN: 2380-7504. doi:10.1109/ICCV.2017.244.\n[107] S. Bak, P. Carr, J.-F. Lalonde, Domain Adaptation through Synthesis\nfor Unsupervised Person Re-identiﬁcation, 2018, pp. 189–205.\nURL\nhttps://openaccess.thecvf.com/content_ECCV_2018/html/\nSlawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.html\n[108] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-Image Translation\nwith Conditional Adversarial Networks, in: 2017 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), IEEE, Hon-\nolulu, HI, 2017, pp. 5967–5976. doi:10.1109/CVPR.2017.632.\nURL http://ieeexplore.ieee.org/document/8100115/\n[109] G. Ding, S. Khan, Z. Tang, J. Zhang, F. Porikli, Towards bet-\nter Validity: Dispersion based Clustering for Unsupervised Person\nRe-identiﬁcation, arXiv:1906.01308 [cs]ArXiv: 1906.01308 (Jun.\n2019).\nURL http://arxiv.org/abs/1906.01308\n[110] M. Ye, X. Lan, P. C. Yuen, Robust Anchor Embedding for Unsu-\npervised Video Person Re-Identiﬁcation in the Wild, 2018, pp. 170–\n186.\nURL https://openaccess.thecvf.com/content_ECCV_2018/html/Mang_\nYE_Robust_Anchor_Embedding_ECCV_2018_paper.html\n[111] Y. Chen, X. Zhu, S. Gong, Deep Association Learning for Unsuper-\nvised Video Person Re-identiﬁcation, arXiv:1808.07301 [cs]ArXiv:\n1808.07301 (Aug. 2018).\nURL http://arxiv.org/abs/1808.07301\n[112] R. Girshick, F. Iandola, T. Darrell, J. Malik, Deformable Part Models\nare Convolutional Neural Networks, arXiv:1409.5403 [cs]ArXiv:\n1409.5403 (Oct. 2014).\nURL http://arxiv.org/abs/1409.5403\n[113] M. Hirzer, C. Beleznai, P. M. Roth, H. Bischof, Person Re-\nidentiﬁcation by Descriptive and Discriminative Classiﬁcation, in:\nA. Heyden, F. Kahl (Eds.), Image Analysis, Lecture Notes in Com-\nputer Science, Springer, Berlin, Heidelberg, 2011, pp. 91–102. doi:\n10.1007/978-3-642-21227-7_9.\n[114] T. Wang, S. Gong, X. Zhu, S. Wang, Person Re-identiﬁcation by\nVideo Ranking, in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars\n(Eds.), Computer Vision – ECCV 2014, Lecture Notes in Computer\nScience, Springer International Publishing, Cham, 2014, pp. 688–\n703. doi:10.1007/978-3-319-10593-2_45.\n[115] A. Dehghan, S. M. Assari, M. Shah, GMMCP tracker: Globally\noptimal Generalized Maximum Multi Clique problem for multiple\nobject tracking, in: 2015 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), IEEE, Boston, MA, USA, 2015, pp.\n4091–4099. doi:10.1109/CVPR.2015.7299036.\nURL http://ieeexplore.ieee.org/document/7299036/\n[116] L. Song, C. Wang, L. Zhang, B. Du, Q. Zhang, C. Huang, X. Wang,\nUnsupervised Domain Adaptive Re-Identiﬁcation: Theory and Prac-\ntice, arXiv:1807.11334 [cs]ArXiv: 1807.11334 (Jul. 2018).\nURL http://arxiv.org/abs/1807.11334\n[117] Z. Zhong, L. Zheng, Z. Zheng, S. Li, Y. Yang, CamStyle: A\nNovel Data Augmentation Method for Person Re-Identiﬁcation,\nIEEE Transactions on Image Processing 28 (3) (2019) 1176–1190,\nconference Name: IEEE Transactions on Image Processing.\ndoi:\n10.1109/TIP.2018.2874313.\n[118] Z. Zheng, X. Yang, Z. Yu, L. Zheng, Y. Yang, J. Kautz,\nJoint Discriminative and Generative Learning for Person Re-\nIdentiﬁcation, 2019, pp. 2138–2147.\nURL\nhttps://openaccess.thecvf.com/content_CVPR_2019/html/\nZheng_Joint_Discriminative_and_Generative_Learning_for_Person_\nRe-Identification_CVPR_2019_paper.html\n[119] Y. Ge, F. Zhu, D. Chen, R. Zhao, H. Li, Self-paced Contrastive\nLearning with Hybrid Memory for Domain Adaptive Object Re-ID,\nAdvances in Neural Information Processing Systems 33 (2020)\n11309–11321.\nURL\nhttps://proceedings.neurips.cc/paper/2020/hash/\n821fa74b50ba3f7cba1e6c53e8fa6845-Abstract.html\n[120] S. Xuan, S. Zhang, Intra-Inter Camera Similarity for Unsupervised\nPerson Re-Identiﬁcation, arXiv:2103.11658 [cs]ArXiv: 2103.11658\n(Mar. 2021).\nURL http://arxiv.org/abs/2103.11658\n[121] Y. Xu, B. Ma, R. Huang, L. Lin, Person Search in a Scene by Jointly\nModeling People Commonness and Person Uniqueness, in: Proceed-\nings of the 22nd ACM international conference on Multimedia, MM\n’14, Association for Computing Machinery, New York, NY, USA,\n2014, pp. 937–940. doi:10.1145/2647868.2654965.\nURL https://doi.org/10.1145/2647868.2654965\n[122] X. Lin, P. Ren, Y. Xiao, X. Chang, A. Hauptmann, Person search\nchallenges and solutions: A survey, in: Z.-H. Zhou (Ed.), Proceed-\nings of the Thirtieth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-21, International Joint Conferences on Artiﬁcial\nIntelligence Organization, 2021, pp. 4500–4507, survey Track. doi:\n10.24963/ijcai.2021/613.\nURL https://doi.org/10.24963/ijcai.2021/613\n[123] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, M. Shah,\nTransformers in Vision: A Survey, arXiv:2101.01169 [cs]ArXiv:\n2101.01169 (Sep. 2021).\nURL http://arxiv.org/abs/2101.01169\n[124] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nS. Zagoruyko, End-to-end object detection with transformers, in:\nA. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer\nVision – ECCV 2020, Springer International Publishing, Cham,\n2020, pp. 213–229.\n[125] X.-B. Nguyen, D. T. Bui, C. N. Duong, T. D. Bui, K. Luu,\nClusformer:\nA\nTransformer\nBased\nClustering\nApproach\nto\nUnsupervised Large-Scale Face and Visual Landmark Recognition,\n2021, pp. 10847–10856.\nURL\nhttps://openaccess.thecvf.com/content/CVPR2021/html/\nNguyen_Clusformer_A_Transformer_Based_Clustering_Approach_to_\nUnsupervised_Large-Scale_Face_CVPR_2021_paper.html\n[126] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo,\nSwin Transformer: Hierarchical Vision Transformer using Shifted\nWindows, arXiv:2103.14030 [cs]ArXiv: 2103.14030 (Aug. 2021).\nURL http://arxiv.org/abs/2103.14030\n[127] K. Zhou, Y. Yang, A. Cavallaro, T. Xiang, Omni-Scale Feature\nLearning for Person Re-Identiﬁcation, 2019, pp. 3702–3712.\nURL https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_\nOmni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_\n2019_paper.html\n[128] S. Li, T. Xiao, H. Li, B. Zhou, D. Yue, X. Wang, Person Search\nwith Natural Language Description, in: 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), IEEE, Honolulu,\nHI, 2017, pp. 5187–5196. doi:10.1109/CVPR.2017.551.\nURL http://ieeexplore.ieee.org/document/8100034/\nX. Lin, P. Ren et al.: Preprint submitted to Elsevier\nPage 20 of 20\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-09-01",
  "updated": "2021-10-02"
}