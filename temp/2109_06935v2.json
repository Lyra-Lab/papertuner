{
  "id": "http://arxiv.org/abs/2109.06935v2",
  "title": "On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning",
  "authors": [
    "Marc Tanti",
    "Lonneke van der Plas",
    "Claudia Borg",
    "Albert Gatt"
  ],
  "abstract": "Recent work has shown evidence that the knowledge acquired by multilingual\nBERT (mBERT) has two components: a language-specific and a language-neutral\none. This paper analyses the relationship between them, in the context of\nfine-tuning on two tasks -- POS tagging and natural language inference -- which\nrequire the model to bring to bear different degrees of language-specific\nknowledge. Visualisations reveal that mBERT loses the ability to cluster\nrepresentations by language after fine-tuning, a result that is supported by\nevidence from language identification experiments. However, further experiments\non 'unlearning' language-specific representations using gradient reversal and\niterative adversarial learning are shown not to add further improvement to the\nlanguage-independent component over and above the effect of fine-tuning. The\nresults presented here suggest that the process of fine-tuning causes a\nreorganisation of the model's limited representational capacity, enhancing\nlanguage-independent representations at the expense of language-specific ones.",
  "text": "On the Language-speciﬁcity of Multilingual BERT\nand the Impact of Fine-tuning\nMarc Tanti1\nLonneke van der Plas2\nClaudia Borg3\nAlbert Gatt4\n1University of Malta, Institute of Linguistics and Language Technology\n2Idiap Research Institute\n3University of Malta, Department of AI\n4Utrecht University, Information and Computing Sciences\n{marc.tanti,claudia.borg}@um.edu.mt\nlonneke.vanderplas@idiap.ch, a.gatt@uu.nl\nAbstract\nRecent work has shown evidence that the\nknowledge acquired by multilingual BERT\n(mBERT) has two components: a language-\nspeciﬁc and a language-neutral one. This pa-\nper analyses the relationship between them,\nin the context of ﬁne-tuning on two tasks –\nPOS tagging and natural language inference –\nwhich require the model to bring to bear differ-\nent degrees of language-speciﬁc knowledge.\nVisualisations reveal that mBERT loses the\nability to cluster representations by language\nafter ﬁne-tuning, a result that is supported by\nevidence from language identiﬁcation experi-\nments. However, further experiments on ‘un-\nlearning’ language-speciﬁc representations us-\ning gradient reversal and iterative adversarial\nlearning are shown not to add further improve-\nment to the language-independent component\nover and above the effect of ﬁne-tuning. The\nresults presented here suggest that the process\nof ﬁne-tuning causes a reorganisation of the\nmodel’s limited representational capacity, en-\nhancing language-independent representations\nat the expense of language-speciﬁc ones.\n1\nIntroduction\nSince the introduction of transformer architectures\nand the demonstration that they improve the state\nof the art on tasks such as machine translation\nand parsing (Vaswani et al., 2017), there has been\na decisive turn in NLP towards the development\nof large, pre-trained transformer models, such as\nBERT (Devlin et al., 2019). Such models are pre-\ntrained on tasks such as masked language mod-\nelling (MLM) and next-sentence prediction (NSP)\nand are intended to be task-agnostic, facilitating\ntheir transfer to new tasks following ﬁne-tuning\nwith limited amounts of data.\nExtending such models to multiple languages\nis a natural next step, as evidenced by the recent\nproliferation of multilingual transformers, includ-\ning multilingual BERT (mBERT), XLM (Con-\nneau and Lample, 2019), and XLM-R (Conneau\net al., 2020a). These follow from a line of earlier\nwork which sought to achieve transferable mul-\ntilingual representations using recurrent network-\nbased methods (e.g. Artetxe et al., 2019, inter\nalia), as well as work on developing multilingual\nembedding representations (Ruder et al., 2017).\nThe considerable capacity of these multilin-\ngual models and their success in cross-lingual\ntasks has motivated a lot of research into the na-\nture of the representations learned during pre-\ntraining.\nOn the one hand, there is a signiﬁ-\ncant amount of research suggesting that models\nsuch as mBERT acquire robust language-speciﬁc\nrepresentations (Wu and Dredze, 2019; Libovick´y\net al., 2020; Choenni and Shutova, 2020).\nOn\nthe other hand, it has been suggested that in\naddition to language-speciﬁc information, mod-\nels like mBERT also have language-neutral rep-\nresentations, which cut across linguistic distinc-\ntions and enable the model to handle aspects of\nmeaning language-independently.\nThis also al-\nlows the model to be ﬁne-tuned on a monolin-\ngual labelled data set and achieve good results in\nother languages, a process known as cross-lingual\nzero-shot learning (Pires et al., 2019; Libovick´y\net al., 2020; Conneau et al., 2018; Hu et al., 2020).\nThese results have motivated researchers to try and\ndisentangle the language-speciﬁc and language-\nneutral components of mBERT (e.g. Libovick´y\net al., 2020; Gonen et al., 2020).\nThis background provides the motivation for the\nwork presented in this paper. We focus on the rela-\ntionship between language-speciﬁc and language-\nneutral representations in mBERT. However, our\nmain goal is to study the impact of ﬁne-tuning\non the balance between these two types of rep-\nresentations. More speciﬁcally, we measure the\narXiv:2109.06935v2  [cs.CL]  26 Dec 2021\neffect of ﬁne-tuning on mBERT’s representations\nin the context of two different tasks – part-of-\nspeech (POS) tagging and natural language infer-\nence (NLI) – which lay different demands on the\nmodel’s semantic and language-speciﬁc knowl-\nedge. While NLI involves reasoning about deep\nsemantic relations between texts, POS tagging re-\nquires a model to bring to bear knowledge of a lan-\nguage’s morphosyntactic features. Though many\nlanguages share such features as a result of ty-\npological relations (which mBERT is known to\nexploit; see, e.g. Pires et al., 2019; Choenni and\nShutova, 2020; Rama et al., 2020), there are also\nlanguage-speciﬁc features to which, we hypothe-\nsise, mBERT needs to dedicate a greater share of\nits representational capacity, compared to the NLI\ntask.\nWe\nshow\nthat\nthe\nmodel\naccommodates\nlanguage-speciﬁc and language-neutral represen-\ntations to different degrees as a function of the\ntask it is ﬁne-tuned on. This is supported by re-\nsults from language identiﬁcation (LID) experi-\nments, conducted both on task-speciﬁc data and\non a new data set extracted from Wikipedia. We\nthen consider two alternative strategies that force\nthe model to ‘unlearn’ language-speciﬁc represen-\ntations, via gradient reversal or iterative adver-\nsarial learning.\nThese are shown not to further\nimprove the language-independent component for\ncross-lingual transfer, over and above the effect\nof ﬁne-tuning. Thus, we conclude that the reor-\nganisation of mBERT’s representations that hap-\npens with ﬁne-tuning is already taking on this role.\nNote that our goal is not to improve mBERT’s\nmultilinguality but to acquire a better understand-\ning of it, extending previous work along these\nlines.\nOur main contributions are (a) to provide fur-\nther support for the distinction between language-\nspeciﬁc and language-neutral representation in\nmBERT; (b) to show that ﬁne-tuning results in a\nreorganisation of mBERT’s representations in a\nway that destroys existing language clusters; (c) to\nstudy two methods to enhance language-neutrality\nin mBERT, both of which are shown not to im-\nprove performance on ﬁne-tuned tasks; (d) a new\nWikipedia-based language identiﬁcation data set.\n2\nTasks and data\nTo perform cross-lingual zero-shot learning, we\nﬁne-tune mBERT on English only and evalu-\nate it on multiple languages at once.\nWe fo-\ncus on two tasks: the low-level structured pre-\ndiction task of cross-lingual POS tagging and the\nhigh-level semantic task of cross-lingual NLI. We\nchose these two tasks, because each task requires\na different type of linguistic knowledge. Cross-\nlingual POS tagging requires a model to bring to\nbear language-speciﬁc knowledge related to mor-\nphosyntactic properties, word order, etc., in de-\ntermining the correct part-of-speech to assign to\na token in context. Previous work, for example\nby Pires et al. (2019), showed good results on this\ntask for mBERT in a cross-lingual zero-shot set-\nting. In contrast, NLI requires a model to deter-\nmine the semantic relationship between two texts,\ndetermining whether it is one of entailment, con-\ntradiction or neutrality (Sammons, 2015; Bowman\net al., 2015). We expected this task to require more\nlanguage-neutral, semantic knowledge, compared\nto POS tagging.1\nIn addition to results on these two tasks, we re-\nport results on language identiﬁcation (LID) ex-\nperiments. These are reported both on the test data\nfor the tasks themselves, as well as on an inde-\npendent data set consisting of Wikipedia texts, de-\nscribed below.\nIn all the experiments reported, we reserve a\ndevelopment set for hyperparameter tuning and\na validation set to monitor progress during train-\ning.\nData set statistics are provided in the Ap-\npendix.\nFollowing the practice of Hu et al.\n(2020) for the XTREME benchmark, all texts\nwere truncated to their ﬁrst 128 tokens (with\nXNLI having a combined premise-hypothesis\nlength of 128).\nAll the data was tokenised us-\ning the bert-base-multilingual-cased\ntokeniser.2 Data containing unknown tokens (ac-\ncording to the tokeniser) was omitted. All our ex-\nperiments are conducted on data for 33 languages,\nthe same set of languages included in the UD-\nPOS task of the XTREME benchmark (Hu et al.,\n2020).3 The exception is XNLI, for which cross-\nlingual test data exists for only 15 of these lan-\n1We note that one task is word-level and one is sentence-\nlevel. Ideally the two tasks would have the same level of gran-\nularity, but, to our knowledge, no two tasks conform to this\ngoal while at the same time addressing the morphosyntax-\nversus-semantics requirements.\n2https://huggingface.co/\nbert-base-multilingual-cased\n3ISO 639-1 language codes used: af, ar, bg, de, el, en, es,\net, eu, fa, ﬁ, fr, he, hi, hu, id, it, ja, kk, ko, mr, nl, pt, ru, ta, te,\nth, tl, tr, ur, vi, yo, and zh.\nguages (Conneau et al., 2018).\nUDPOS\nFor POS tagging, we use data from\nthe Universal Dependencies Treebank (UDPOS;\nMarneffe et al., 2020) v2.7, using the train/dev/test\nsplits provided. A validation set is randomly sam-\npled from the training set. Since we are interested\nin cross-lingual zero-shot learning, we removed all\nnon-English data from the train/val splits.\nXNLI\nFor NLI, we use the monolingual English\nMultiNLI data set (Williams et al., 2018) as a\ntraining set, and the Cross-lingual Natural Lan-\nguage Inference data (XNLI; Conneau et al., 2018)\nfor the development set and test set. Again, a val-\nidation set is randomly sampled from the training\nset.\nWikipedia\nAn independent data set for LID was\nextracted from Wikipedia. For each language, we\nrandomly selected 5 000 paragraphs which are at\nleast 100 characters in length. Further details are\nprovided in the Appendix.\n3\nModel architecture\nThe general architecture used in our experiments\nis shown in Figure 1. We use a pre-trained mBERT\nmodel to encode the input using its ﬁnal hidden\nlayer, because we are interested in the linguistic\ncapabilities of mBERT’s typical use case, which is\nalso the practice in the XTREME benchmark (Hu\net al., 2020). The same mBERT model is shared\nbetween two single-layer softmax classiﬁers, both\ntrained using a categorical cross-entropy loss. One\nof these assigns a task-speciﬁc label (a POS tag\nor an NLI class). This is trained on UDPOS or\nXNLI data. We will sometimes refer to this as the\ntarget task. The other is a language classiﬁer that\npredicts which of the 33 languages (see above) the\ninput text is written in. The language classiﬁer is\ntrained on the Wikipedia data.\nWe conduct separate experiments for each tar-\nget task (UDPOS and XNLI). In the case of UD-\nPOS, the classiﬁcation is for individual tokens.\nFor XNLI, the classiﬁcation is for sentence pairs,\nrepresented as a single text consisting of the con-\ncatenation of the premise and hypothesis, sepa-\nrated by a [SEP] token.\nThe language classiﬁer is also trained to either\npredict the language of each token or of an entire\ntext according to the target task. In this way, we\nare also able to test this classiﬁer for predictions\nboth on the independent Wikipedia data and on\nmBERT\nlabel \nclassifier\nlanguage \nclassifier\nFigure 1: The basic model architecture.\nthe test data from the target task (which is also la-\nbelled by language).\nUnless otherwise speciﬁed, gradients from the\nlanguage classiﬁer are not propagated to the pre-\ntrained mBERT model. Thus, the mBERT model\nparameters are only ﬁne-tuned on the target task\ndata set whilst the language classiﬁer is ﬁne-\ntuned in isolation. This allows us to monitor how\nmuch language-speciﬁc information exists in the\nmBERT encoding, without directly inﬂuencing it.\nMore information about how the model was\ntrained together with the hyperparameters used\ncan be found in the Appendix.4\n3.1\nModifying mBERT’s language-speciﬁc\nrepresentations\nTo explain the rationale for our experiments\non language-speciﬁcity, we distinguish language-\nneutrality from language confusion.\nLet TL1 be a text in language L1, and its trans-\nlation TL2 in language L2, and let E(·) be an en-\ncoding extracted from a model M. We say that\nE(·) is language-neutral if E(TL1) is close to, or\nidentical with, E(TL2). Thus, M treats seman-\ntically equivalent texts in different languages in\nthe same way.\nIn contrast, language confusion\nplaces a weaker requirement on a model: here,\nE(TL1) and E(TL2) need not be identical, but the\nencoding itself does not support language identiﬁ-\ncation. In short, language confusion arises when\nlanguage-speciﬁc information is missing from an\nencoding.\nHere,\nwe\nfocus\non\nreducing\nlanguage-\nspeciﬁcity using methods to enhance language\nconfusion, inspired by work on domain adapta-\ntion. In a domain adaptation setting, Ganin and\nLempitsky (2015) successfully enhanced domain\ninvariance by enhancing domain confusion. We\nwant to see if this works for language as well,\nthat is, whether reducing the language-speciﬁcity\n4Our\nimplementation\ncan\nbe\nfound\non\nthe\nGitHub\nrepo\nhttps://github.com/mtanti/\nmbert-language-specificity.\nof representations leads to better cross-lingual\ngeneralisation on target tasks.\nTo this end, we\nconsider two methods which are intended to\nenhance the model’s language confusion: gradient\nreversal and iterative entropy maximisation.5\nGradient reversal\nGradient reversal has been\nused to train a classiﬁer in a multi-domain setting,\nusing only labelled data in a single domain (Ganin\nand Lempitsky, 2015). A small labelled data set\nin one domain and a large unlabelled data set in\nanother domain can be used to encourage the fea-\ntures learned by the model to be domain-invariant\nby training the model to confuse a domain clas-\nsiﬁer, thus avoiding any features that are speciﬁc\nto the labelled data set’s domain. We make use\nof Ganin and Lempitsky’s gradient reversal layer,\nwhich multiplies gradients from the language clas-\nsiﬁer by a factor −λ before backpropagating them\nto the mBERT model. In effect, this updates the\nlanguage classiﬁer layer itself to perform better\nat LID whilst the mBERT parameters are updated\nto make the language of an encoded text progres-\nsively harder to classify. A similar strategy was\nproposed by Libovick´y et al. (2020), who used\ngradient reversal from LID during additional pre-\ntraining of mBERT using masked language mod-\nelling. Here, we consider its use in the context\nof ﬁne-tuning on a target task, such as UDPOS or\nNLI. Hyperparameters, including λ, are listed in\nthe Appendix.\nEntropy maximisation\nIterative entropy max-\nimisation is closer in spirit to the strategies\nemployed by generative adversarial networks\n(GANs), in that training is performed iteratively in\ntwo alternating phases, each lasting one epoch. In\nthe ﬁrst phase, the language classiﬁcation layer is\ntrained in isolation for one epoch on the Wikipedia\ndata set. In the second phase, the target classiﬁer is\ntrained together with mBERT with two goals: (a)\nmaximise the accuracy of the label classiﬁer and\n(b) maximise the entropy of the language classi-\nﬁer’s output probabilities. The entropy maximisa-\ntion step is intended to make the language classi-\nﬁer approach a uniform distribution, thus training\nthe mBERT model to make the encodings as con-\nfusing as possible to the language classiﬁer.\nThe loss in the second phase is a combination\nof the label classiﬁer’s loss and the language clas-\n5Note that we do not change the architecture or the pre-\ntrained mBERT model in these experiments.\nsiﬁer’s negative entropy. This loss is calculated as\nfollows:\nL = (1 −w)(XE(ya)) + w\n\u0010\n−\nX\nln yb\n\u0011\n(1)\nwhere ya is the label classiﬁer output, yb is the\nlanguage classiﬁer output, XE is the cross entropy\nfunction, and w is the weight assigned to the loss\nfor the language classiﬁer. Hyperparameters, in-\ncluding w, are listed in the Appendix.\n4\nExperiments\nIn what follows, we ﬁrst discuss the language clus-\ntering and LID capabilities of mBERT both in its\nunmodiﬁed form, and after ﬁne-tuning on UD-\nPOS and XNLI. Then, we consider the impact of\ngradient reversal and entropy maximisation, both\nof which seek to shift mBERT representations to-\nwards greater language neutrality.\n4.1\nThe effect of ﬁne-tuning on\nlanguage-speciﬁc representations\nIn this section we focus on how much language-\nspeciﬁc information is readily available in mBERT\nrepresentations before and after ﬁne-tuning it. We\nmeasure this in two steps.\nFirst, we train the label and language classiﬁers\non the unmodiﬁed pre-trained mBERT model,\nwithout modifying mBERT’s parameters.\nWe\nmeasure the model’s performance on the target\ntask using the label classiﬁer and on the LID task\nusing the language classiﬁer.\nWe measure LID\nperformance on both the target task test data and\non the Wikipedia test data.\nSecond, we reinitialise both classiﬁers and\ntrain the label classiﬁer together with the pre-\ntrained mBERT model, after which we freeze\nmBERT’s parameters and train the language clas-\nsiﬁer. Again, we measure the label and language\nclassiﬁers’ performance as explained above.\nIn addition, we show 2D t-SNE projections\nof mBERT’s representations of a sample of data\nitems (tokens or texts) from the target task test set\nand from the Wikipedia test set. Sample sizes are\ngiven in the Appendix. We colour the 2D points\naccording to label (target test set) or language (tar-\nget test set and Wikipedia test set). This allows\nus to compare the way in which mBERT organises\nrepresentations prior to and after ﬁne-tuning. As a\nnumerical measure of organisation we cluster the\nfull representations (prior to t-SNE compression)\nusing k-means and measure the agreement of the\nUDPOS\nXNLI\nInit.\nFine-T.\nInit.\nFine-T.\nTarget task\n51.2\n59.6\n29.7\n66.3\nLang. ID (Target)\n78.3\n0.3\n49.8\n39.2\nLang. ID (Wiki)\n59.3\n0.5\n97.0\n97.2\nTable 1: Macro F1 scores (%) for target tasks (UDPOS\nand XNLI) and language identiﬁcation before (Init.)\nand after ﬁne-tuning (Fine-T.). Note that ‘Lang. ID\n(Target)’ refers to language classiﬁcation on the target\ndata set.\nclusters for labels/languages using the V-measure\n(Rosenberg and Hirschberg, 2007). We take the\naverage V-measure from ten independent runs of\nk-means clustering (from scratch).\nTable 1 gives the macro F1 scores of the classi-\nﬁer predictions. For both target tasks, performance\nimproves after ﬁne-tuning, as expected. This oc-\ncurs to a greater extent on XNLI than on UDPOS.6\nFor LID, we observe the reverse trend: on both\nthe target test sets and Wikipedia, the classiﬁer’s\nperformance in detecting the language of the input\ndrops signiﬁcantly after ﬁne-tuning for UDPOS,\nless so for XNLI (with Wikipedia LID remaining\nunchanged on XNLI).\nUDPOS\nXNLI\nGrad.\nEnt.\nGrad.\nEnt.\nTarget task\n53.5\n56.8\n62.2\n62.1\nLang. ID (Target)\n0.1\n5.5\n1.3\n3.4\nLang. ID (Wiki)\n0.1\n3.1\n1.5\n54.3\nTable 2: Macro F1 scores (%) for target tasks (UDPOS\nand XNLI) and language identiﬁcation after training\nusing gradient reversal (Grad.) and entropy\nmaximisation (Ent.). Note that ‘Lang. ID (Target)’\nrefers to language classiﬁcation on the target data set.\nFigure 3 and Figure 4 show the t-SNE projec-\ntions of the token-based UDPOS representations\nand the text-based XNLI representations respec-\ntively, together with the corresponding V-measure.\nFor labels in both target tasks, mBERT starts off\nwith no discernible structure, whereas ﬁne-tuning\nresults in clear clusters by label (compare Figures\n3a vs 3b; and 4a vs 4b).7\n6Note that we report macro F1 scores. The results re-\nported in related work, such as the XTREME benchmark (Hu\net al., 2020), report accuracies for NLI and micro F1 scores\nfor POS. They are comparable to those in Table 1: we obtain\nmicro F1 for POS of 73.4%, compared to 70.3% reported by\nHu et al. (2020); and accuracy of 66.3% on XNLI, where the\nequivalent result reported by Hu et al. (2020) is 65.4%.\n7Note that ﬁne-tuning was done on English data, while\nthe plots are generated on the multilingual test data, mean-\ning that data for languages that were not used for ﬁne-tuning\nnevertheless cluster by labels in line with the English data.\nOn the other hand, the opposite is observed for\nlanguages.\nHere, mBERT starts off with fairly\nclear-cut language clusters which then become\nmixed (compare Figures 3c vs 3d; and 4c vs 4d).\nThis is less evident in the Wikipedia data set. In\ngeneral, we observe a loss of language-speciﬁc\nrepresentational capacity following ﬁne-tuning on\nboth tasks, in line with the drop in LID perfor-\nmance in Table 1.\nIn the case of the Wikipedia plots for XNLI\n(Figures 4e and 4f), there seems to be negligible\nclustering present (even according to V-measure)\nbut a very high LID performance. Given that the\nlanguage classiﬁer is single layer deep, this in-\ndicates that, although the representations are not\nclustered, they are still linearly separable (before\nbeing compressed by t-SNE).\nWith the exception of LID on Wikipedia after\nXNLI ﬁne-tuning, there is a drop in LID perfor-\nmance, which we take as evidence that as mBERT\nis ﬁne-tuned on a task, its representations become\nmore language invariant.\nPut differently, ﬁne-\ntuning requires mBERT’s ﬁnite representational\ncapacity to be dedicated to the task requirements,\nat the expense of accurately distinguishing be-\ntween languages. In this sense, language-speciﬁc\nand language-neutral representations are compet-\ning with each other in the context of ﬁne-tuning.\nHowever, the results also show interesting dif-\nferences between tasks: ﬁne-tuning results in a\nsteeper increase in F1 score for XNLI compared to\nUDPOS, suggesting that cross-lingual transfer on\nsemantic tasks may beneﬁt more if language speci-\nﬁcity decreases, compared to tasks involving mor-\nphosyntax. In a related vein, Lauscher et al. (2020)\nshow that POS tagging and dependency parsing\nare impacted by structural language similarity.\n4.2\nThe impact of increasing language\nconfusion\nIn this section, we address whether, by using\nexplicit means to enforce language confusion in\nmBERT representations, we can observe better\nperformance on the target tasks in a cross-lingual\nzero-shot transfer setting. Unlike the ﬁne-tuning\nexperiments, the language classiﬁer in this case\nis not trained in isolation, but is trained together\nwith the rest of the system in order to allow the\nmBERT model to learn to confuse the language\nclassiﬁer. The language classiﬁer is then retrained\nfrom scratch in isolation in order to be able to mea-\naf\nar\nbg\nde\nel\nen\nes\net\neu\nfa\nfi\nfr\nhe\nhi\nhu\nid\nit\nja\nkk\nko\nmr\nnl\npt\nru\nta\nte\nth\ntl\ntr\nur\nvi\nyo\nzh\n(a) Colour legend for languages.\nADJ\nADP\nADV\nAUX\nCCONJ\nDET\nINTJ\nNOUN\nNUM\nPART\nPRON\nPROPN\nPUNCT\nSCONJ\nSYM\nVERB\nX\n(b) Colour legend for UDPOS\nlabels.\ncontradiction\nentailment\nneutral\n(c) Colour legend for XNLI\nlabels.\nFigure 2: Legends for t-SNE visualisations.\n(a) UDPOS labels with initial mBERT (macro F1:\n51.2%, V-measure: 13.0%).\n(b) UDPOS labels with ﬁne-tuned mBERT (macro F1:\n59.6%, V-measure: 47.5%).\n(c) UDPOS languages with initial mBERT (macro F1:\n78.3%, V-measure: 49.2%).\n(d) UDPOS languages with ﬁne-tuned mBERT (macro\nF1: 0.3%, V-measure: 10.4%).\n(e) Wikipedia languages with initial mBERT (macro F1:\n59.3%, V-measure: 22.3%).\n(f) Wikipedia languages with ﬁne-tuned mBERT (macro\nF1: 0.5%, V-measure: 11.6%).\nFigure 3: 2D t-SNE projections before and after ﬁne-tuning, with UDPOS as target task. Macro F1 scores for\nlabel/language classiﬁcation of mBERT embeddings are also given. Points represent tokens. See Figure 2a\n(languages) and Figure 2b (labels) for colour legends.\n(a) XNLI labels with initial mBERT (macro F1: 29.7%,\nV-measure: 0.1%).\n(b) XNLI labels with ﬁne-tuned mBERT (macro F1:\n66.3%, V-measure: 20.3%).\n(c) XNLI languages with initial mBERT (macro F1:\n49.8%, V-measure: 35.1%).\n(d) XNLI languages with ﬁne-tuned mBERT (macro F1:\n39.2%, V-measure: 6.8%).\n(e) Wikipedia languages with initial mBERT (macro F1:\n97.0%, V-measure: 11.7%).\n(f) Wikipedia languages with ﬁne-tuned mBERT (macro\nF1: 97.2%, V-measure: 8.9%).\nFigure 4: 2D t-SNE projections before and after ﬁne-tuning, with XNLI as target task. Macro F1 scores for\nlabel/language classiﬁcation of mBERT embeddings are also given. Points represent pairs of sentences. See\nFigure 2a (languages) and Figure 2c (labels) for colour legends.\nsure the amount of language sensitive information\nin the mBERT representations.\nTable 2 gives the macro F1 scores of the clas-\nsiﬁer predictions. Compared to the simple ﬁne-\ntuning setup in Table 1, it is clear that both gradi-\nent reversal and entropy maximisation have a neg-\native impact on target task performance.\nt-SNE plots for the mBERT representations af-\nter language unlearning are shown in the Ap-\npendix, but we show the V-measure values in Ta-\nble 3. In general, gradient reversal and entropy\nmaximisation yield a comparable degree of clus-\ntering to that observed above after ﬁne-tuning.\nThe exception is XNLI, where the V-measure is\nhigher and suggests more well-deﬁned clusters\ncompared to the ﬁne-tuned case.\nThese results suggest that ﬁne-tuning of mul-\nUDPOS\nXNLI\nGrad.\nEnt.\nGrad.\nEnt.\nTarget labels\n22.8\n30.1\n1.1\n12.2\nLanguage (Target)\n12.5\n12.1\n18.8\n20.0\nLanguage (Wiki)\n7.9\n9.6\n12.2\n14.7\nTable 3: V-measures (%) of full vector test set samples\nafter clustering for target labels (UDPOS and XNLI)\nand languages after training using gradient reversal\n(Grad.) and entropy maximisation (Ent.). Note that\n‘Language (Target)’ refers to languages in the target\ndata set.\ntilingual models such as mBERT involves an in-\nterplay between language-speciﬁc and language-\nneutral representations. They also indicate that the\ntask matters: classifying tokens with morphosyn-\ntactic information results in greater loss of lan-\nguage speciﬁcity and in the ability to identify lan-\nguages than classifying entire texts. Furthermore,\nstrategies to enforce language confusion in repre-\nsentations result in a deterioration of performance,\ninterfering with the ability of the model to balance\nthe two sources of information as a function of the\ntask it is being ﬁne-tuned on.\n5\nRelated work\nSeveral studies have shown that mBERT performs\nwell in zero-shot transfer settings in low-level\nstructured prediction tasks (Pires et al., 2019) and\nin comparison to models that use explicit mul-\ntilingual information for a variety of tasks and\nlanguages (Wu and Dredze, 2019). Another fo-\ncus of research has been the correlation between\ncross-lingual performance and shared vocabulary\n(e.g. as measured by wordpiece overlap). K et al.\n(2020) ﬁnd no such correlation, suggesting that\nmBERT’s success in zero-shot transfer must be\ndue to cross-lingual mappings at a deeper linguis-\ntic level. However, it is not clear how this ﬁnding\nshould be interpreted in light of the further ﬁnding\nthat mBERT performs well (ca. 96%) at language\nidentiﬁcation on the WiLI data set8.\nOther work has taken this further by focusing\non the hypothesis that mBERT encodings contain\nboth a language-speciﬁc and a language-neutral\ncomponent (Libovick´y et al., 2020). Gonen et al.\n(2020) set out to disentangle both components and\nﬁnd that in ‘language identity subspace’, t-SNE\nprojections show large improvement in cluster-\ning with respect to language. In language-neutral\nspace, semantic representations are largely intact.\nDufter and Sch¨utze (2020) show that smaller mod-\nels result in better cross-lingual zero-shot learning.\nThis could be due to there being less representa-\ntional space allocated to language-speciﬁc infor-\nmation.\nSome previous work has also shed light on the\nimpact of training strategies on cross-lingual zero-\nshot learning. Tan and Joty (2021) show that train-\ning on code-switched data improves cross-lingual\ntransfer, while Phang et al. (2020) show that inter-\nmediate task training, prior to ﬁne-tuning proper,\nalso results in better transfer with XLM-R (Con-\nneau et al., 2020b). This is related to the present\nﬁnding that mBERT representations become less\nlanguage-speciﬁc after ﬁne-tuning.\nIn this paper, we also experiment with tech-\nniques that speciﬁcally enforce language confu-\n8https://zenodo.org/record/841984\nsion, using entropy maximisation or gradient re-\nversal. The latter has been used successfully for\ndomain adaptation (Ganin and Lempitsky, 2015),\nwhile Libovick´y et al. (2020) use it for additional\npre-training.\n6\nConclusions\nThis\npaper\nexplored\nthe\ninterplay\nbetween\nlanguage-speciﬁc and language-neutral represen-\ntations in the multilingual transformer model\nmBERT. Our results show that ﬁne-tuning on spe-\nciﬁc tasks has a differential impact on how much\nlanguage-speciﬁc information is retained. On the\nother hand, gradient reversal and iterative entropy\nmaximisation interfere with ﬁne-tuning and do not\nimprove task performance.\nFine-tuning for tasks requiring identiﬁcation\nof morphosyntactic properties, such as POS tag-\nging, can result in greater loss of language-speciﬁc\ninformation, compared to deeper semantically-\noriented tasks such as NLI9.\nHowever, there are also differences in the in-\nput data of these two tasks which could have im-\npacted the results reported here. The data sets dif-\nfer in their homogeneity (unlike XNLI, UDPOS is\nconstructed from multiple sources) and size. The\ntasks also differ in their granularity, in that NLI is\ntext-level, while POS is token-level. Furthermore,\nNLI is known to be susceptible to ‘shortcut learn-\ning’, whereby models rely on recurrent biases in\ntraining to solve the task (D’Amour et al.) whereas\nshortcuts in part of speech tagging are less known.\nThese considerations are worthy of further inves-\ntigation.\nWe believe these results contribute towards\ngreater understanding of ﬁne-tuning on multilin-\ngual model representations. This results in a reor-\nganisation of the representation space, suggesting\nthat language-speciﬁc and language-independent\nsubspaces are dependent on task.\n7\nEthical considerations\nThe experiments reported here rely on previously\navailable data sets and/or data extracted from pub-\n9It could be argued that the model’s loss of language-\nspeciﬁc information is not due to the nature of ﬁne-tuning,\nbut due to the fact that ﬁne-tuning is carried out using mono-\nlingual data (English). This could, in principle, imply that the\nmodel is not being given any indication that language-speciﬁc\ninformation should be retained. However, we ﬁnd this an un-\nlikely explanation since it does not account for the substantial\ndifference in LID performance after ﬁne-tuning on different\ntasks.\nlicly available sources. To ensure reproducibility,\nwe will release all code, including scripts to regen-\nerate the Wikipedia data set for language identiﬁ-\ncation.\nAcknowledgements\nThis research was supported by a Malta Enterprise\nResearch and Development grant. We thank our\ncollaborators, CityFalcon Ltd.10 Comments and\nquestions by four anonymous reviewers are also\ngratefully acknowledged.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings ofthe 58th\nAnnual Meeting ofthe Association for Computa-\ntional Linguistics (ACL’20), pages 4623–4637. As-\nsociation for Computational Linguistics.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nRochelle Choenni and Ekaterina Shutova. 2020. What\ndoes it mean to be language-agnostic? probing mul-\ntilingual sentence encoders for typological proper-\nties. CoRR, abs/2009.12862.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzman, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings ofthe 58th Annual Meeting ofthe As-\nsociation for Computational Linguistics (ACL’20),\npages 8440–8451.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual Language Model Pretraining. In Proceed-\nings of the 2019 Conference on Advances in Neural\nInformation Processing Systems (NeurIPS’19), On-\nline. Curran Associates, Inc.\n10https://www.cityfalcon.com/\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018.\nXNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAlexander D’Amour, Katherine Heller, Dan Moldovan,\nBen\nAdlam,\nBabak\nAlipanahi,\nAlex\nBeutel,\nChristina Chen, Jonathan Deaton, Jacob Eisen-\nstein, Matthew D. Hoffman, Farhad Hormozdi-\nari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel,\nAlan Karthikesalingam, Mario Lucic, Yian Ma,\nCory McLean, Diana Mincu, Akinori Mitani, An-\ndrea Montanari, Zachary Nado, Vivek Natarajan,\nChristopher Nielson, Thomas F. Osborne, Rajiv\nRaman, Kim Ramasamy, Rory Sayres, Jessica\nSchrouff, Martin Seneviratne, Shannon Sequeira,\nHarini Suresh, Victor Veitch, Max Vladymyrov,\nXuezhi Wang, Kellie Webster, Steve Yadlowsky,\nTaedong Yun, Xiaohua Zhai, and D. Sculley. Un-\nderspeciﬁcation presents challenges for credibility\nin modern machine learning. CoRR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nPhilipp Dufter and Hinrich Sch¨utze. 2020.\nIdenti-\nfying elements essential for BERT’s multilingual-\nity.\nIn Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP), pages 4423–4437, Online. Associa-\ntion for Computational Linguistics.\nYaroslav Ganin and Victor Lempitsky. 2015.\nUnsu-\npervised domain adaptation by backpropagation. In\nProceedings of the 32nd International Conference\non Machine Learning, volume 37 of Proceedings\nof Machine Learning Research, pages 1180–1189,\nLille, France. PMLR.\nHila Gonen, Shauli Ravfogel, Yanai Elazar, and Yoav\nGoldberg. 2020. It’s not Greek to mBERT: Induc-\ning word-level translations from multilingual BERT.\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 45–56, Online. Association for Compu-\ntational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. CoRR.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In International Con-\nference on Learning Representations.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli´c, and\nGoran Glavaˇs. 2020.\nFrom zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nJindˇrich Libovick´y,\nRudolf Rosa,\nand Alexander\nFraser. 2020.\nOn the language neutrality of pre-\ntrained multilingual representations.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 1663–1674, Online. Associa-\ntion for Computational Linguistics.\nMarie-Catherine de Marneffe, Filip Ginter, Yoav Gold-\nberg, Jan Hajiˇc, Christopher Manning, Ryan Mc-\nDonald, Joakim Nivre, Slav Petrov, Sampo Pyysalo,\nSebastian Schuster, Natalia Silveira, Reut Tsarfaty,\nFrancis Tyers, and Dan Zeman. 2020. Universal de-\npendencies v2.7.\nJason Phang, Iacer Calixto, Phu Mon Htut, Yada\nPruksachatkun, Haokun Liu, Clara Vania, Katha-\nrina Kann, and Samuel R. Bowman. 2020. English\nintermediate-task training improves zero-shot cross-\nlingual transfer too. In Proceedings of the 1st Con-\nference of the Asia-Paciﬁc Chapter of the Associa-\ntion for Computational Linguistics and the 10th In-\nternational Joint Conference on Natural Language\nProcessing, pages 557–575, Suzhou, China. Associ-\nation for Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT?\nIn Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nTaraka Rama, Lisa Beinborn, and Steffen Eger. 2020.\nProbing multilingual BERT for genetic and typo-\nlogical signals.\nIn Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 1214–1228, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nAndrew Rosenberg and Julia Hirschberg. 2007.\nV-\nmeasure: A conditional entropy-based external clus-\nter evaluation measure. In Proceedings of the 2007\nJoint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural\nLanguage Learning (EMNLP-CoNLL), pages 410–\n420, Prague, Czech Republic. Association for Com-\nputational Linguistics.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard.\n2017. A Survey Of Cross-lingual Word Embedding\nModels. CoRR, pages 1–55.\nMark Sammons. 2015.\nRecognizing textual entail-\nment. In Chris Lappin, Shalom and Fox, editor, The\nHandbook of Contemporary Semantic Theory, 2 edi-\ntion, chapter 17, pages 523–557. John Wiley & Sons\nLtd.\nSamson Tan and Shaﬁq Joty. 2021. Code-mixing on\nsesame street: Dawn of the adversarial polyglots.\nIn Proceedings of the Fifth Workshop on Compu-\ntational Approaches to Linguistic Code-Switching,\npage 141, Online. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, and\nŁukasz Kaiser. 2017. Attention Is All You Need.\nIn Proceedings of the 31st Conference on Neural\nInformaton Processing Systems (NeurIPS’17), Long\nBeach, CA.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nAppendix\nA\nData set details\nWhen splitting data sets, a stratiﬁed split was used\nto preserve the proportions of different languages\nbetween splits.\nUDPOS\nAfter ﬁltering data so that only the 33\nselected languages are used, we split the initial\ntraining set into a smaller training set (90%) and\na validation set (10%). Non-English data was re-\nmoved from the training set and validation set.\nThe training set contains 18 906 sentences, the\nvalidation set contains 2 100 sentences, develop-\nment set contains 66 062, and the test set contains\n95 065 sentences.\nXNLI\nWe split the training set (which contains\nonly English text) into a smaller training set (90%)\nand a validation set (10%). The training set con-\ntains 351 340 pairs of sentences, the validation set\ncontains 39 037 pairs, development set contains\n33 484, and the test set contains 67 459 pairs.\nWikipedia\nAfter\nrandomly\nsampling\n5 000\nparagraphs\nper\nlanguage\nfrom\nthe\n33\nse-\nlected languages, we split the paragraphs into\ntrain/val/dev/test splits using 70/10/10/10% splits.\nThe training set contains 115 500 paragraphs,\nthe validation set contains 16 500 paragraphs,\ndevelopment set contains 16 500, and the test set\ncontains 16 500 paragraphs. Text was extracted\nfrom the 20200420 Wikimedia dump using\nwikiextractor.11\nB\nt-SNE/V-measure data sample\nFor t-SNE plots and V-measure computation, we\nuse a random sample of data points. The sam-\nples were chosen such that a maximum number\nof points is shown from each label-language pair\nin the target data set, or from each language in the\nWikipedia data set. Since different data sets have\ndifferent numbers of label-language pairs, a differ-\nent maximum is chosen for each data set in order\nto keep them similarly sized in total.\nFor UDPOS, which has the largest number of\nlanguage-label pairs, only a maximum of 10 points\nare shown from each pair. This results in 5 013\npoints in total.\n11https://github.com/attardi/\nwikiextractor.\nXNLI has three different labels, so a maximum\nof 50 points is shown from each pair. This results\nin 2 100 points in total.\nWikipedia has no labels and a maximum of 100\npoints is shown from each language. This results\nin 3 300 points in total.\nC\nModel training details\nAll experiments are conducted on a 64GB RAM\nserver with an Intel(R) Xeon W-2123 8-core CPU\n(3.60GHz) and a GeForce Titan RTX2080 Ti\nGPU.\nExperiments are conducted using PyTorch and\nthe implementation of Multilingual BERT in the\ntransformers Python library.12\nWe use the\nAdam optimiser with a different learning rate for\nthe mBERT model and for the classiﬁcation lay-\ners, although both classiﬁcation layers use the\nsame learning rate.\nThe classiﬁcation layers are initialised ran-\ndomly using a normal distribution with mean\nzero. The mBERT encodings are passed through\na dropout layer with a dropout rate of 0.1 before\nbeing passed to the classiﬁcation layers.\nWe train the model for 5 epochs. A validation\nset is used to measure the model’s performance af-\nter each epoch; we reserve the model at the best\nepoch out of the ﬁve.\nWhen optimising the label classiﬁer, the macro\nF1 score performance on the English data of the\ntarget task validation set is measured (only En-\nglish is considered in order to be faithful to a cross-\nlingual zero-shot learning setting).\nOn the other hand, when optimising the lan-\nguage classiﬁer, the macro F1 score performance\non all of the Wikipedia validation set is measured\n(language labels are assumed to be available dur-\ning zero-shot learning).\nIn the gradient reversal experiments, only the\ntarget task validation set is used for determining\nwhich epoch gave the best model.\nTwo mini-\nbatches of the same size from the labelled data and\nthe language data are used for each parameter up-\ndate.\nD\nHyperparameter tuning\nHyperparameter tuning is conducted via random\nsearch over a ﬁxed set of values per hyperparam-\neter. Due to time constraints, only 20 random sets\n12https://huggingface.co/\nbert-base-multilingual-cased\nHyperparameter\nValues\nInit. standard deviation\n1e-1, 1e-2, 1e-3\nMinibatch size\n64, 32, 16\nClassiﬁer learning rate\n1e-1, 1e-2, 1e-3, 1e-4\nmBERT learning rate\n1e-3, 1e-4, 1e-5, 1e-6\nGradient reversal lambda\n0.1, 0.3, 0.5, 0.7\nEntropy max. weighting\n0.1, 0.3, 0.5, 0.7\nTable 4: Sets of hyperparameter values sampled from\nduring hyperparameter tuning.\nof hyperparameters are sampled for each experi-\nment. The development set is used to evaluate hy-\nperparameters during hyperparameter tuning and\nonly the label classiﬁer is evaluated. Hyperparam-\neters that best ﬁt the label classiﬁer are also used\nin the language classiﬁer.\nThe hyperparameter values used in the random\nsearch are given in Table 4. Selected hyperparam-\neter values for each model are given in Table 5.\nE\nResults on XNLI after language\nunlearning\nIn Figure 5 and Figure 6 we reproduce the visu-\nalisations after gradient reversal and entropy max-\nimisation, when the model is ﬁne-tuned on UD-\nPOS and XNLI respectively.\nExperiment\nHyperparameters\nUDPOS frozen\ninit. stddev.: 1e-01, minibatch size: 16, class. learning rate: 1e-03\nUDPOS ﬁne-tuned\ninit. stddev.: 1e-02, minibatch size: 64, mBERT learning rate: 1e-04,\nclass. learning rate: 1e-01\nUDPOS grad. rev.\ninit. stddev.: 1e-03, minibatch size: 32, mBERT learning rate: 1e-06,\nclass. learning rate: 1e-03, grad. rev. λ: 0.1\nUDPOS ent. max.\ninit. stddev.: 1e-02, minibatch size: 32, mBERT learning rate: 1e-06,\nclass. learning rate: 1e-02, entropy max. w: 0.7\nXNLI frozen\ninit. stddev.: 1e-02, minibatch size: 64, class. learning rate: 1e-02\nXNLI ﬁne-tuned\ninit. stddev.: 1e-03, minibatch size: 64, mBERT learning rate: 1e-05,\nclass. learning rate: 1e-02\nXNLI grad. rev.\ninit. stddev.: 1e-03, minibatch size: 32, mBERT learning rate: 1e-06,\nclass. learning rate: 1e-03, grad. rev. λ: 0.1\nXNLI ent. max.\ninit. stddev.: 1e-01, minibatch size: 32, mBERT learning rate: 1e-06,\nclass. learning rate: 1e-04, entropy max. w: 0.1\nTable 5: Selected hyperparameter values used in training.\n(a) UDPOS labels with gradient reversal (macro F1:\n53.5%, V-measure: 22.8%).\n(b) UDPOS labels with entropy maximisation (macro\nF1: 56.8%, V-measure: 30.1%).\n(c) UDPOS language clusters after gradient reversal\n(macro F1: 0.1%, V-measure: 12.5%).\n(d) UDPOS language clusters after entropy\nmaximisation learned mBERT (macro F1: 5.5%,\nV-measure: 12.1%).\n(e) Wikipedia language clusters after gradient reversal\n(macro F1: 0.1%, V-measure: 7.9%).\n(f) Wikipedia language clusters after entropy\nmaximisation (macro F1: 3.1%, V-measure: 9.6%).\nFigure 5: Token based t-SNE plots of target task label and language clusters, after ﬁne-tuning on UDPOS with\ngradient reversal or entropy maximisation. Each point is a randomly sampled token. Macro F1 scores for\nclassiﬁcation of mBERT embeddings is also given.\n(a) XNLI labels with gradient reversal (macro F1:\n62.2%, V-measure: 1.1%).\n(b) XNLI labels with entropy maximisation (macro F1:\n62.1%, V-measure: 12.2%).\n(c) XNLI languages with gradient reversal (macro F1:\n1.3%, V-measure: 18.8%).\n(d) XNLI languages with entropy maximisation (macro\nF1: 3.4%, V-measure: 20.0%).\n(e) Wikipedia languages with gradient reversal learned\nmBERT (macro F1: 1.5%, V-measure: 12.2%).\n(f) Wikipedia languages with entropy maximisation\nlearned mBERT (macro F1: 54.3%, V-measure: 14.7%).\nFigure 6: Text-based t-SNE plots of language clusters, after ﬁne-tuning on XNLI with gradient reversal or entropy\nmaximisation. Macro F1 scores for classiﬁcation of mBERT embeddings is also given.\n",
  "categories": [
    "cs.CL",
    "cs.NE"
  ],
  "published": "2021-09-14",
  "updated": "2021-12-26"
}