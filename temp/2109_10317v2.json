{
  "id": "http://arxiv.org/abs/2109.10317v2",
  "title": "Introduction to Neural Network Verification",
  "authors": [
    "Aws Albarghouthi"
  ],
  "abstract": "Deep learning has transformed the way we think of software and what it can\ndo. But deep neural networks are fragile and their behaviors are often\nsurprising. In many settings, we need to provide formal guarantees on the\nsafety, security, correctness, or robustness of neural networks. This book\ncovers foundational ideas from formal verification and their adaptation to\nreasoning about neural networks and deep learning.",
  "text": "Introduction to Neural Network Veriﬁcation\nAws Albarghouthi\nUniversity of Wisconsin–Madison\nAuthor’s name in native alphabet: ú\n\u0011Gñ\t«Q\u001e.Ë@ ð\r@\narXiv:2109.10317v2  [cs.LG]  4 Oct 2021\ni\nAbout This Book\nWhy This Book?\nOver the past decade, a number of hardware and software advances have con-\nspired to thrust deep learning and neural networks to the forefront of computing.\nDeep learning has created a qualitative shift in our conception of what software\nis and what it can do: Every day we’re seeing new applications of deep learn-\ning, from healthcare to art, and it feels like we’re only scratching the surface of a\nuniverse of new possibilities.\nIt is thus safe to say that deep learning is here to stay, in one form or another.\nThe line between software 1.0 (that is, manually written code) and software 2.0\n(learned neural networks) is getting fuzzier and fuzzier, and neural networks are\nparticipating in safety-critical, security-critical, and socially critical tasks. Think,\nfor example, healthcare, self-driving cars, malware detection, etc. But neural net-\nworks are fragile and so we need to prove that they are well-behaved when ap-\nplied in critical settings.\nOver the past few decades, the formal methods community has developed\na plethora of techniques for automatically proving properties of programs, and,\nwell, neural networks are programs. So there is a great opportunity to port ver-\niﬁcation ideas to the software 2.0 setting. This book offers the ﬁrst introduction\nof foundational ideas from automated veriﬁcation as applied to deep neural net-\nworks and deep learning. I hope that it will inspire veriﬁcation researchers to\nexplore correctness in deep learning and deep learning researchers to adopt veri-\nﬁcation technologies.\nii\nWho Is This Book For?\nGiven that the book’s subject matter sits at the intersection of two pretty much\ndisparate areas of computer science, one of my main design goals was to make it\nas self-contained as possible. This way the book can serve as an introduction to the\nﬁeld for ﬁrst-year graduate students or senior undergraduates, even if they have\nnot been exposed to deep learning or veriﬁcation. For a comprehensive survey of\nveriﬁcation algorithms for neural networks, along with implementations, I direct\nthe reader to Liu et al. (2021).\nWhat Does This Book Cover?\nThe book is divided into three parts:\nPart 1 deﬁnes neural networks as data-ﬂow graphs of operators over real-\nvalued inputs. This formulation will serve as our basis for the rest of the book.\nAdditionally, we will survey a number of correctness properties that are desir-\nable of neural networks and place them in a formal framework.\nPart 2 discusses constraint-based techniques for veriﬁcation. As the name sug-\ngests, we construct a system of constraints and solve it to prove (or disprove)\nthat a neural network satisﬁes some properties of interest. Constraint-based\nveriﬁcation techniques are also referred to as complete veriﬁcation in the litera-\nture.\nPart 3 discusses abstraction-based techniques for veriﬁcation. Instead of execut-\ning a neural network on a single input, we can actually execute it on an inﬁnite\nset and show that all of those inputs satisfy desirable correctness properties.\nAbstraction-based techniques are also referred to as approximate veriﬁcation in\nthe literature.\nParts 2 and 3 are disjoint; the reader may go directly from Part 1 to Part 3 without\nlosing context.\niii\nAcknowledgements\nThanks to the best focus group ever: the CS 839 students and TA, Swati Anand,\nat the University of Wisconsin–Madison. A number of insightful people sent me\ncomments that radically improved the presentation: Frantisek Plasil, Georg Weis-\nsenbacher, Sayan Mitra, Benedikt Böing, Vivek Garg, Guy Van den Broeck, Matt\nFredrikson, in addition to some anonymous folks.\niv\nTable of Contents\nI\nNeural Networks & Correctness\n1\nA New Beginning 2\n1.1\nIt Starts With Turing 2\n1.2\nThe Rise of Deep Learning 3\n1.3\nWhat do We Expect of Neural Networks? 4\n2\nNeural Networks as Graphs 7\n2.1\nThe Neural Building Blocks 7\n2.2\nLayers and Layers and Layers 9\n2.3\nConvolutional Layers 11\n2.4\nWhere are the Loops? 12\n2.5\nStructure and Semantics of Neural Networks 14\n3\nCorrectness Properties 19\n3.1\nProperties, Informally 19\n3.2\nA Speciﬁcation Language 23\n3.3\nMore Examples of Properties 25\nII Constraint-Based Veriﬁcation\n4\nLogics and Satisﬁability 32\n4.1\nPropositional Logic 32\n4.2\nArithmetic Theories 36\n5\nEncodings of Neural Networks 40\nCONTENTS\nv\n5.1\nEncoding Nodes 40\n5.2\nEncoding a Neural Network 43\n5.3\nHandling Non-linear Activations 46\n5.4\nEncoding Correctness Properties 49\n6\nDPLL Modulo Theories 54\n6.1\nConjunctive Normal Form (CNF) 54\n6.2\nThe DPLL Algorithm 55\n6.3\nDPLL Modulo Theories 59\n6.4\nTseitin’s Transformation 62\n7\nNeural Theory Solvers 67\n7.1\nTheory Solving and Normal Forms 67\n7.2\nThe Simplex Algorithm 69\n7.3\nThe Reluplex Algorithm 77\nIII Abstraction-Based Veriﬁcation\n8\nNeural Interval Abstraction 83\n8.1\nSet Semantics and Veriﬁcation 84\n8.2\nThe Interval Domain 85\n8.3\nBasic Abstract Transformers 88\n8.4\nGeneral Abstract Transformers 89\n8.5\nAbstractly Interpreting Neural Networks 92\n9\nNeural Zonotope Abstraction 96\n9.1\nWhat the Heck is a Zonotope? 97\n9.2\nBasic Abstract Transformers 101\n9.3\nAbstract Transformers of Activation Functions 103\n9.4\nAbstractly Interpreting Neural Networks with Zonotopes 106\n10 Neural Polyhedron Abstraction 109\n10.1\nConvex Polyhedra 110\n10.2\nComputing Upper and Lower Bounds 112\nCONTENTS\nvi\n10.3\nAbstract Transformers for Polyhedra 112\n10.4\nAbstractly Interpreting Neural Networks with Polyhedra 116\n11 Verifying with Abstract Interpretation 118\n11.1\nRobustness in Image Recognition 119\n11.2\nRobustness in Natural-Language Processing 125\n12 Abstract Training of Neural Networks 129\n12.1\nTraining Neural Networks 129\n12.2\nAdversarial Training with Abstraction 134\n13 The Challenges Ahead 138\nReferences 141\nPart I\nNeural Networks & Correctness\n2\nChapter 1\nA New Beginning\nHe had become so caught up in building sentences\nthat he had almost forgotten the barbaric days when\nthinking was like a splash of color landing on a page.\n—Edward St. Aubyn, Mother’s Milk\n1.1 It Starts With Turing\nThis book is about verifying that a neural network behaves according to some set of\ndesirable properties. These ﬁelds of study, veriﬁcation and neural networks, have\nbeen two distinct areas of computing research with almost no bridges connecting\nthem, until very recently. Intriguingly, however, both ﬁelds trace their genesis to\na two-year period of Alan Turing’s tragically short life.\nIn 1949, Turing wrote a little-known paper titled Checking a Large Routine (Alan,\n1949). It was a truly forward-looking piece of work. In it, Turing asks how can\nwe prove that the programs we write do what they are supposed to do? Then, he\nproceeds to provide a proof of correctness of a program implementing the factorial\nfunction. Speciﬁcally, Turing proved that his little piece of code always terminates\nand always produces the factorial of its input. The proof is elegant; it breaks\ndown the program into single instructions, proves a lemma for every instruction,\nQuote found in William Finnegan’s Barbarian Days.\nCHAPTER 1. A NEW BEGINNING\n3\nand ﬁnally stitches the lemmas together to prove correctness of the full program.\nUntil this day, proofs of programs very much follow Turing’s proof style from\n1949. And, as we shall see in this book, proofs of neural networks will, too.\nJust a year before Turing’s proof of correctness of factorial, in 1948, Turing\nwrote a perhaps even more farsighted paper, Intelligent Machinery, in which he\nproposed unorganized machines.1 These machines, Turing argued, mimic the infant\nhuman cortex, and he showed how they can learn using what we now call a ge-\nnetic algorithm. Unorganized machines are a very simple form of what we now\nknow as neural networks.\n1.2 The Rise of Deep Learning\nThe topic of training neural networks continued to be studied since Turing’s 1948\npaper. But it has only exploded in popularity over the past decade, thanks to a\ncombination algorithmic insights, hardware developments, and a ﬂood of data\nfor training.\nModern neural networks are called deep neural networks, and the approach to\ntraining these neural networks is deep learning. Deep learning has enabled incred-\nible improvements in complex computing tasks, most notably in computer vision\nand natural-language processing, for example, in recognizing objects and people\nin an image and translating between languages. Everyday, a growing research\ncommunity is exploring ways to extend and apply deep learning to more chal-\nlenging problems, from music generation to proving mathematical theorems and\nbeyond.\nThe advances in deep learning have changed the way we think of what soft-\nware is, what it can do, and how we build it. Modern software is increasingly\nbecoming a menagerie of traditional, manually written code and automatically\ntrained—sometimes constantly learning—neural networks. But deep neural net-\nworks can be fragile and produce unexpected results. As deep learning becomes\nused more and more in sensitive settings, like autonomous cars, it is imperative\nthat we verify these systems and provide formal guarantees on their behavior.\n1Intelligent Machinery is reprinted in Turing (1969).\nCHAPTER 1. A NEW BEGINNING\n4\nLuckily, we have decades of research on program veriﬁcation that we can build\nupon, but what exactly do we verify?\n1.3 What do We Expect of Neural Networks?\nIn Turing’s proof of correctness of his factorial program, Turing was concerned\nthat we will be programming computers to perform mathematical operations, but\nwe could be getting them wrong. So in his proof he showed that his implementa-\ntion of factorial is indeed equivalent to the mathematical deﬁnition. This notion\nof program correctness is known as functional correctness, meaning that a program\nis a faithful implementation of some mathematical function. Functional correct-\nness is incredibly important in many settings—think of the disastrous effects of a\nbuggy implementation of a cryptographic primitive or an aircraft controller.\nIn the land of deep learning, proving functional correctness is an unrealistic\ntask. What does it mean to correctly recognize cats in an image or correctly trans-\nlate English to Hindi? We cannot mathematically deﬁne such tasks. The whole\npoint of using deep learning to do tasks like translation or image recognition is\nbecause we cannot mathematically capture what exactly they entail.\nSo what now? Is veriﬁcation out of the question for deep neural networks?\nNo! While we cannot precisely capture what a deep neural network should do,\nwe can often characterize some of its desirable or undesirable properties. Let’s\nlook at some examples of such properties.\nRobustness\nThe most-studied correctness property of neural networks is robustness, because\nit is generic in nature and deep learning models are infamous for their fragility\n(Szegedy et al., 2014). Robustness means that small perturbations to inputs should\nnot result in changes to the output of the neural network. For example, changing\na small number of pixels in my photo should not make the network think that I\nam a cupboard instead of a person, or adding inaudible noise to a recording of my\nlecture should not make the network think it is a lecture about the Ming dynasty\nin the 15th century. Funny examples aside, lack of robustness can be a safety and\nCHAPTER 1. A NEW BEGINNING\n5\nsecurity risk. Take, for instance, an autonomous vehicle following trafﬁc signs\nusing cameras. It has been shown that a light touch of vandalism to a stop sign\ncan cause the vehicle to miss it, potentially causing an accident (Eykholt et al.,\n2018). Or consider the case of a neural network for detecting malware. We do\nnot want a minor tweak to the malware’s binary to cause the detector to suddenly\ndeem it safe to install.\nSafety\nSafety is a broad class of correctness properties stipulating that a program should\nnot get to a bad state. The deﬁnition of bad depends on the task at hand. Consider\na neural-network-operated robot working in some kind of plant. We might be\ninterested in ensuring that the robot does not exceed certain speed limits, to avoid\nendangering human workers, or that it does not go to a dangerous part of the\nplant. Another well-studied example is a neural network implementing a collision\navoidance system for aircrafts (Katz et al., 2017). One property of interest is that\nif an intruding aircraft is approaching from the left, the neural network should\ndecide to turn the aircraft right.\nConsistency\nNeural networks learn about our world via examples, like images. As such, they\nmay sometimes miss basic axioms, like physical laws, and assumptions about re-\nalistic scenarios. For instance, a neural network recognizing objects in an image\nand their relationships might say that object A is on top of object B, B is on top of\nC, and C is on top of A. But this cannot be! (At least not in the world as we know\nit.)\nFor another example, consider a neural network tracking players on the soccer\nﬁeld using a camera. It should not in one frame of video say that Ronaldo is on\nthe right side of the pitch and then in the next frame say that Ronaldo is on the left\nside of the pitch—Ronaldo is fast, yes, but he has slowed down in the last couple\nof seasons.\nCHAPTER 1. A NEW BEGINNING\n6\nLooking Ahead\nI hope that I have convinced you of the importance of verifying properties of\nneural networks. In the next two chapters, we will formally deﬁne what neural\nnetworks look like (spoiler: they are ugly programs) and then build a language\nfor formally specifying correctness properties of neural networks, paving the way\nfor veriﬁcation algorithms to prove these properties.\n7\nChapter 2\nNeural Networks as Graphs\nThere is no rigorous deﬁnition of what deep learning is and what it is not. In fact,\nat the time of writing this, there is a raging debate in the artiﬁcial intelligence com-\nmunity about a clear deﬁnition. In this chapter, we will deﬁne neural networks\ngenerically as graphs of operations over real numbers. In practice, the shape of\nthose graphs, called the architecture, is not arbitrary: Researchers and practition-\ners carefully construct new architectures to suit various tasks. For example, at\nthe time of writing, neural networks for image recognition typically look different\nfrom those for natural language tasks.\nFirst, we will informally introduce graphs and look at some popular architec-\ntures. Then, we will formally deﬁne graphs and their semantics.\n2.1 The Neural Building Blocks\nA neural network is a graph where each node performs an operation. Overall,\nthe graph represents a function from vectors of real numbers to vectors of real\nnumbers, that is, a function in Rn →Rm. Consider the following very simple\ngraph.\nx\nv\ny\nFigure 2.1 A very simple neural network\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n8\nThe red node is an input node; it just passes input x, a real number, to node v.\nNode v performs some operation on x and spits out a value that goes to the output\nnode y. For example, v might simply return 2x + 1, which we will denote as the\nfunction fv : R →R:\nfv(x) = 2x + 1\nIn our model, the output node may also perform some operation, for example,\nfy(x) = max(0, x)\nTaken together, this simple graph encodes the following function f : R →R:\nf (x) = fy( fv(x)) = max(0, 2x + 1)\nTransformations and Activations\nThe function fv in our example above is afﬁne: simply, it multiplies inputs by\nconstant values (in this case, 2x) and adds constant values (in this case, 1). The\nfunction fy is an activation function, because it turns on or off depending on its\ninput. When its input is negative, fy outputs 0 (off), otherwise it outputs its input\n(on). Speciﬁcally, fy, illustrated in Figure 2.2, is called a rectiﬁed linear unit (ReLU),\nand it is a very popular activation function in modern deep neural networks (Nair\nand Hinton, 2010). Activation functions are used to add non-linearity into a neural\nnetwork.\nx\nrelu(x)\nFigure 2.2 Rectiﬁed linear unit\nThere are other popular activation functions, for example, sigmoid,\nσ(x) =\n1\n1 + exp(−x)\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n9\n−4 −2\n2\n4\n0.5\nFigure 2.3 Sigmoid function\nwhose output is bounded between 0 and 1, as shown in Figure 2.3.\nOften, in the literature and practice, the afﬁne functions and the activation\nfunction are composed into a single operation. Our graph model of neural net-\nworks can capture that, but we usually prefer to separate the two operations on to\ntwo different nodes of the graph, as it will simplify our life in later chapters when\nwe start analyzing those graphs.\nUniversal Approximation\nWhat is so special about these activation functions? The short answer is they work\nin practice, in that they result in neural networks that are able to learn complex\ntasks. It is also very interesting to point out that you can construct a neural net-\nwork comprised of ReLUs or sigmoids and afﬁne functions to approximate any\ncontinuous function. This is known as the universal approximation theorem (Hornik\net al., 1989), and in fact the result is way more general than ReLUs and sigmoids—\nnearly any activation function you can think of works, as long as it is not polyno-\nmial (Leshno et al., 1993)! For an interactive illustration of universal approxima-\ntion, I highly recommend Nielsen (2018, Ch.4).\n2.2 Layers and Layers and Layers\nIn general, a neural network can be a crazy graph, with nodes and arrows pointing\nall over the place. In practice, networks are usually layered. Take the graph in\nFigure 2.4. Here we have 3 inputs and 3 outputs, denoting a function in R3 →R3.\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n10\nx1\nx2\nx3\na1\na2\na3\ny1\ny2\ny3\nFigure 2.4 A multilayer perceptron\nx1\nx2\nx3\nv1\nv2\nv3\nv4\nv5\nv6\ny1\ny2\ny3\nFigure 2.5 A multilayer perceptron with two hidden layers\nNotice that the nodes of the graph form layers, the input layer, the output layer,\nand the layer in the middle which is called the hidden layer. This form of graph—\nor architecture—has the grandiose name of multilayer perceptron (MLP). Usually,\nwe have a bunch of hidden layers in an MLP; Figure 2.5 shows a MLP with two\nhidden layers. Layers in an MLP are called fully connected layers, since each node\nreceives all outputs from the preceding layer.\nNeural networks are typically used as classiﬁers: they take an input, e.g., pixels\nof an image, and predict what the image is about (the image’s class). When we\nare doing classiﬁcation, the output layer of the MLP represents the probability\nof each class, for example, y1 is the probability of the input being a chair, y2 is\nthe probability of a TV, and y3 of a couch. To ensure that the probabilities are\nnormalized, that is, between 0 and 1 and sum up to 1, the ﬁnal layer employs a\nsoftmax function. Softmax, generically, looks like this for an output node yi, where\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n11\nn is the number of classes:\nfyi(x1, . . . , xn) =\nexp(xi)\n∑n\nk=1 exp(xk)\nWhy does this work? Imagine that we have two classes, i.e., n = 2. First, we\ncan verify that\nfy1, fy2 ∈[0, 1]\nThis is because the numerators and denominators are both positive, and the nu-\nmerator is ⩽than the denominator. Second, we can see that fy1(x1, x2) + fy2(x1, x2) =\n1, because\nfy1(x1, x2) + fy2(x1, x2) =\nex1\nex1 + ex2 +\nex2\nex1 + ex2 = 1\nTogether, these two facts mean that we have a probability distribution. For an in-\nteractive visualization of softmax, please see the excellent online book by Nielsen\n(2018, Chapter 3).\nGiven some outputs (y1, . . . , yn) of the neural network, we will use\nclass(y1, . . . , yn)\nto denote the index of the largest element (we assume no ties), i.e., the class with\nthe largest probability. For example, class(0.8, 0.2) = 1, while class(0.3, 0.7) = 2.\n2.3 Convolutional Layers\nAnother kind of layer that you will ﬁnd in a neural network is a convolutional\nlayer. This kind of layer is widely used in computer-vision tasks, but also has uses\nin natural-language processing. The rough intuition is that if you are looking at an\nimage, you want to scan it looking for patterns. The convolutional layer gives you\nthat: it deﬁnes an operation, a kernel, that is applied to every region of pixels in\nan image or every sequence of words in a sentence. For illustration, let’s consider\nan input layer of size 4, perhaps each input deﬁnes a word in a 4-word sentence,\nas shown in Figure 2.6. Here we have a kernel, nodes {v1, v2, v3}, that is applied\nto every pair of consecutive words, (x1, x2), (x2, x3), and (x3, x4). We say that this\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n12\nkernel has size 2, since it takes an input in R2. This kernel is 1-dimensional, since\nits input is a vector of real numbers. In practice, we work with 2-dimensional\nkernels or more; for instance, to scan blocks of pixels of a gray-scale image where\nevery pixel is a real number, we can use kernels that are functions in R10×10 →R,\nmeaning that the kernel is applied to every 10 × 10 sub-image in the input.\nx1\nx2\nx3\nx4\nv1\nv2\nv3\ny1\ny2\ny3\nFigure 2.6 1-dimensional convolution\nTypically, a convolutional neural network (CNN) will apply a bunch of kernels to\nan input—and many layers of them—and aggregate (pool) the information from\neach kernel. We will meet these operations in later chapters when we verify prop-\nerties of such networks.1\n2.4 Where are the Loops?\nAll of the neural networks we have seen so far seem to be a composition of a\nnumber mathematical functions, one after the other. So what about loops? Can we\nhave loops in neural networks? In practice, neural network graphs are really just\ndirected acyclic graphs (DAG). This makes training the neural network possible\nusing the backpropagation algorithm.\nThat said, there are popular classes of neural networks that appear to have\nloops, but they are very simple, in the sense that the number of iterations of the\n1Note that there are many parameters that are used to construct a CNN, e.g., how many kernels\nare applied, how many inputs a kernel applies to, the stride or step size of a kernel, etc. These are\nnot of interest to us in this book. We’re primarily concerned with the core building blocks of the\nneural network, which will dictate the veriﬁcation challenges.\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n13\nloop is just the size of the input. Recurrent neural networks (RNN) is the canonical\nclass of such networks, which are usually used for sequence data, like text. You\nwill often see the graph of an RNN rendered as follows, with the self loop on node\nv.\nx\nv\ny\nFigure 2.7 Recurrent neural network\nEffectively, this graph represents an inﬁnite family of acyclic graphs that unroll\nthis loop a ﬁnite number of times. For example, Figure 2.8 is an unrolling of length\n3. Notice that this is an acyclic graph that takes 3 inputs and produces 3 outputs.\nThe idea is that if you receive a sentence, say, with n words, you unroll the RNN\nto length n and apply it to the sentence.\nx1\nv1\ny1\nx2\nv2\ny2\nx3\nv3\ny3\nFigure 2.8 Unrolled recurrent neural network\nThinking of it through a programming lens, given an input, we can easily stat-\nically determine—i.e., without executing the network—how many loop iterations\nit will require. This is in contrast to, say, a program where the number of loop iter-\nations is a complex function of its input, and therefore we do not know how many\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n14\nloop iterations it will take until we actually run it. That said, in what follows, we\nwill formalize neural networks as acyclic graphs.\n2.5 Structure and Semantics of Neural Networks\nWe’re done with looking at pretty graphs. Let’s now look at pretty symbols. We\nwill now formally deﬁne neural networks as directed acyclic graphs and discuss\nsome of their properties.\nNeural Networks as DAGs\nA neural network is a directed acyclic graph G = (V, E), where\n• V is a ﬁnite set of nodes,\n• E ⊆V × V is a set of edges,\n• Vin ⊂V is a non-empty set of input nodes,\n• Vo ⊂V is a non-empty set of output nodes, and\n• each non-input node v is associated with a function fv : Rnv →R, where nv\nis the number of edges whose target is node v. The vector of real values Rnv\nthat v takes as input is all of the outputs of nodes v′ such that (v′, v) ∈E.\nNotice that we assume, for simplicity but without loss of generality, that a\nnode v only outputs a single real value.\nTo make sure that a graph G does not have any dangling nodes and that se-\nmantics are clearly deﬁned, we will assume the following structural properties:\n• All nodes are reachable, via directed edges, from some input node.\n• Every node can reach an output node.\n• There is ﬁxed total ordering on edges E and another one on nodes V.\nWe will use x ∈Rn to denote an n-ary (row) vector, which we represent as a\ntuple of scalars (x1, . . . , xn), where xi is the ith element of x.\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n15\nSemantics of DAGs\nA neural network G = (V, E) deﬁnes a function in Rn →Rm where\nn = |Vin| and m = |Vo|\nThat is, G maps the values of the input nodes to those of the output nodes.\nSpeciﬁcally, for every non-input node v ∈V, we recursively deﬁne the value\nin R that it produces as follows. Let (v1, v), . . . , (vnv, v) be an ordered sequence\nof all edges whose target is node v (remember that we’ve assumed an order on\nedges). Then, we deﬁne the output of node v as\nout(v) = fv(x1, . . . , xnv)\nwhere xi = out(vi), for i ∈{1, . . . , nv}.\nThe base case of the above deﬁnition (of out) is input nodes, since they have\nno edges incident on them. Suppose that we’re given an input vector x ∈Rn. Let\nv1, . . . , vn be an ordered sequence of all input nodes. Then,\nout(vi) = xi\nA Simple Example\nLet’s look at an example graph G:\nv1\nv2\nv3\nWe have Vin = {v1, v2} and Vo = {v3}. Now assume that\nfv3(x1, x2) = x1 + x2\nand that we’re given the input vector (11, 79) to the network, where node v1 gets\nthe value 11 and v2 the value 79. Then, we have\nout(v1) = 11\nout(v2) = 79\nout(v3) = fv3(out(v1), out(v2)) = 11 + 79 = 90\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n16\nData Flow and Control Flow\nThe graphs we have deﬁned are known in the ﬁeld of compilers and program\nanalysis as data-ﬂow graphs; this is in contrast to control-ﬂow graphs.2 Control-\nﬂow graphs dictate the order in which operations need be performed—the ﬂow\nof who has control of the CPU. Data-ﬂow graphs, on the other hand, only tell us\nwhat node needs what data to perform its computation, but not how to order the\ncomputation. This is best seen through a small example.\nConsider the following graph\nv1\nv2\nv3\nv4\nv5\nViewing this graph as an imperative program, one way to represent it is as follows,\nwhere ←is the assignment symbol.\nout(v3) ←fv3(out(v1))\nout(v4) ←fv4(out(v2))\nout(v5) ←fv5(out(v3), out(v4))\nThis program dictates that the output value of node v3 is computed before that\nof node v4. But this need not be, as the output of v3 does not depend on that of\nv4. Therefore, an equivalent implementation of the same graph can swap the ﬁrst\ntwo operations:\nout(v4) ←fv4(out(v2))\nout(v3) ←fv3(out(v1))\nout(v5) ←fv5(out(v3), out(v4))\nFormally, we can compute the values out(·) in any topological ordering of graph\nnodes. This ensures that all inputs of a node are computed before its own opera-\ntion is performed.\n2In deep learning frameworks like TensorFlow, they call data-ﬂow graphs computation graphs.\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n17\nProperties of Functions\nSo far, we have assumed that a node v can implement any function fv it wants over\nreal numbers. In practice, to enable efﬁcient training of neural networks, these\nfunctions need be differentiable or differentiable almost everywhere. The sigmoid\nactivation function, which we met earlier in Figure 2.3, is differentiable. How-\never, the ReLU activation function, Figure 2.2, is differentiable almost everywhere,\nsince at x = 0, there is a sharp turn in the function and the gradient is undeﬁned.\nMany of the functions we will be concerned with are linear or piecewise linear.\nFormally, a function f : Rn →R is linear if it can be deﬁned as follows:\nf (x) =\nn\n∑\ni=1\ncixi + b\nwhere ci, b ∈R. A function is piecewise linear if it can be written in the form\nf (x) =\n\n\n\n\n\n\n\n∑i c1\ni xi + b1,\nx ∈S1\n...\n∑i cm\ni xi + bm,\nx ∈Sm\nwhere Si are mutually disjoint subsets of Rn and ∪iSi = Rn. ReLU, for instance,\nis a piecewise linear function, as it is of the form:\nrelu(x) =\n(\n0,\nx < 0\nx,\nx ⩾0\nAnother important property that we will later exploit is monotonicity. A func-\ntion f : R →R is monotonically increasing if for any x ⩾y, we have f (x) ⩾f (y).\nBoth activation functions we saw earlier in the chapter, ReLUs and sigmoids, are\nmonotonically increasing. You can verify this in Figures 2.2 and 2.3: the functions\nnever decrease with increasing values of x.\nLooking Ahead\nNow that we have formally deﬁned neural networks, we’re ready to pose ques-\ntions about their behavior. In the next chapter, we will formally deﬁne a language\nCHAPTER 2. NEURAL NETWORKS AS GRAPHS\n18\nfor posing those questions. Then, in the chapters that follow, we will look at algo-\nrithms for answering those questions.\nMost discussions of neural networks in the literature use the language of linear\nalgebra—see, for instance, the comprehensive book of Goodfellow et al. (2016).\nLinear algebra is helpful because we can succinctly represent the operation of\nmany nodes in a single layer as a matrix A that applies to the output of the previ-\nous layer. Also, in practice, we use fast, parallel implementations of matrix multi-\nplication to evaluate neural networks. Here, we choose a lower-level presentation,\nwhere each node is a function in Rn →R. While this view is non-standard, it will\nhelp make our presentation of different veriﬁcation techniques much cleaner, as\nwe can decompose the problem into smaller ones that have to do with individual\nnodes.\nThe graphs of neural networks we presented are lower-level versions of the\ncomputation graphs of deep-learning frameworks like Tensorﬂow (Abadi et al.,\n2016) and PyTorch (Paszke et al., 2019)\nNeural networks are an instance of a general class of programs called differen-\ntiable programs. As their name implies, differentiable programs are ones for which\nwe can compute derivatives, a property that is needed for standard techniques for\ntraining neural networks. Recently, there have been interesting studies of what it\nmeans for a program to be differentiable (Abadi and Plotkin, 2020; Sherman et al.,\n2021). In the near future, it is likely that people will start using arbitrary differen-\ntiable programs to deﬁne and train neural networks. Today, this is not the case,\nmost neural networks have one of a few prevalent architectures and operations.\n19\nChapter 3\nCorrectness Properties\nIn this chapter, we will come up with a language for specifying properties of neu-\nral networks. The speciﬁcation language is a formulaic way of making statements\nabout the behavior of a neural network (or sometimes multiple neural networks).\nOur concerns in this chapter are solely about specifying properties, not about au-\ntomatically verifying them. So we will take liberty in specifying complex proper-\nties, ridiculous ones, and useless ones. In later parts of the book, we will constrain\nthe properties of interest to ﬁt certain veriﬁcation algorithms—for now, we have\nfun.\n3.1 Properties, Informally\nRemember that a neural network deﬁnes a function f : Rn →Rm. The properties\nwe will consider here are of the form:\nfor any input x, the neural network produces an output that ...\nIn other words, properties dictate the input–output behavior of the network, but\nnot the internals of the network—how it comes up with the answer.\nSometimes, our properties will be more involved, talking about multiple in-\nputs, and perhaps multiple networks:\nfor any inputs x, y, ... that ... the neural networks produce outputs\nthat ...\nCHAPTER 3. CORRECTNESS PROPERTIES\n20\nFigure 3.1 Left: Handwritten 7 from mnist dataset. Middle: Same digit with increased\nbrightness. Right: Same digit but with a dot added in the top left.\nThe ﬁrst part of these properties, the one talking about inputs, is called the\nprecondition; the second part, talking about outputs, is called the postcondition. In\nwhat follows, we will continue our informal introduction to properties using ex-\namples.\nImage Recognition\nLet’s say we have a neural network f that takes in an image and predicts a label\nfrom dog, zebra, etc. An important property that we may be interested in ensuring\nis robustness of such classiﬁer. A classiﬁer is robust if its prediction does not change\nwith small variations (or perturbations) of the input. For example, changing the\nbrightness slightly or damaging a few pixels should not change classiﬁcation.\nLet’s ﬁx some image c that is classiﬁed as dog by f. To make sure that c is not\nan adversarial image of a dog that is designed to fool the neural network, we will\ncheck—prove or verify—the following property:\nfor any image x that is slightly brighter or darker than c, f (x) predicts\ndog\nNotice here that the precondition speciﬁes a set of images x that are brighter or\ndarker than c, and the postcondition speciﬁes that the classiﬁcation by f remains\nunchanged.\nCHAPTER 3. CORRECTNESS PROPERTIES\n21\nRobustness is a desirable property: you don’t want classiﬁcation to change\nwith a small movement in the brightness slider. But there are many other prop-\nerties you desire—robustness to changes in contrast, rotations, Instagram ﬁlters,\nwhite balance, and the list goes on. This hits at the crux of the speciﬁcation prob-\nlem: we often cannot specify every possible thing that we desire, so we have to\nchoose some. (More on this later.)\nFor a concrete example, see Figure 3.1. The MNIST dataset (LeCun et al., 2010) is\na standard dataset for recognizing handwritten digits. The ﬁgure shows a hand-\nwritten 7 along with two modiﬁed versions, one where brightness is increased\nand one where a spurious dot is added—perhaps a drip of ink. We would like our\nneural network to classify all three images as 7.\nNatural-Language Processing\nSuppose now that f takes an English sentence and decides whether it represents a\npositive or negative sentiment. This problem arises, for example, in automatically\nanalyzing online reviews or tweets. We’re also interested in robustness in this\nsetting. For example, say we have ﬁxed a sentence c with positive sentiment, then\nwe might specify the following property:\nfor any sentence x that is c with a few spelling mistakes added, f (x)\nshould predict positive sentiment\nFor another example, instead of spelling mistakes, imagine replacing words\nwith synonyms:\nfor any sentence x that is c with some words replaced by synonyms,\nthen f (x) should predict positive sentiment\nFor instance, a neural network should classify both of these movie reviews as\npositive reviews:\nThis movie is delightful\nThis movie is enjoyable\nCHAPTER 3. CORRECTNESS PROPERTIES\n22\nWe could also combine the two properties above to get a stronger property\nspecifying that prediction should not change in the presence of synonyms or spelling\nmistakes.\nSource Code\nSay that our neural network f is a malware classiﬁer, taking a piece of code and\ndeciding whether it is malware or not. A malicious entity may try to modify a\nmalware to sneak it past the neural network by fooling it into thinking that it’s\na benign program. One trick the attacker may use is adding a piece of code that\ndoes not change the malware’s operation but that fools the neural network. We\ncan state this property as follows: Say we have piece of malware c, then we can\nstate the following property:\nfor any program x that is equivalent to c and syntactically similar, then\nf (x) predicts malware\nControllers\nAll of our examples so far have been robustness problems. Let’s now look at a\nslightly different property. Say you have a controller deciding on the actions of a\nrobot. The controller looks at the state of the world and decides whether to move\nleft, right, forward, or backward. We, of course, do not want the robot to move\ninto an obstacle, whether it is a wall, a human, or another robot. As such, we\nmight specify the following property:\nfor any state x, if there is an obstacle to the right of the robot, then f (x)\nshould not predict right\nWe can state one such property per direction.\nCHAPTER 3. CORRECTNESS PROPERTIES\n23\n3.2 A Speciﬁcation Language\nOur speciﬁcations are going to look like this:\n{ precondition }\nr ←f (x)\n{ postcondition }\nThe precondition is a Boolean function (predicate) that evaluates to true or false. The\nprecondition is deﬁned over a set of variables which will be used as inputs to the\nneural networks we’re reasoning about. We will use xi to denote those variables.\nThe middle portion of the speciﬁcation is a number of calls to functions deﬁned by\nneural networks; in this example, we only see one call to f, and the return value\nis stored in a variable r. Generally, our speciﬁcation language allows a sequence\nof such assignments, e.g.:\n{ precondition }\nr1 ←f (x1)\nr2 ←g(x2)\n...\n{ postcondition }\nFinally, the postcondition is a Boolean predicate over the variables appearing in\nthe precondition xi and the assigned variables rj.\nThe way to read a speciﬁcation, informally, is as follows:\nfor any values of x1, . . . , xn that make the precondition true, let r1 =\nf (x1), r2 = g(x2), . . .. Then the postcondition is true.\nIf a correctness property is not true, i.e., the postcondition yields false, we will\nalso say that the property does not hold.\nExample 3.A Recall our image brightness example from the previous section, and\nsay c is an actual grayscale image, where each element of c is the intensity of a\npixel, from 0 to 1 (black to white). For example, in our MNIST example in Fig-\nure 3.1, each digit is represented by 784 pixels (28 × 28), where each pixel is a\nnumber between 0 and 1. Then, we can state the following speciﬁcation, which\nCHAPTER 3. CORRECTNESS PROPERTIES\n24\ninformally says that changing the brightness of c should not change the classiﬁca-\ntion (recall the deﬁnition of class from Section 2.2):\n{ |x −c| ⩽0.1 }\nr1 ←f (x)\nr2 ←f (c)\n{ class(r1) = class(r2) }\nLet’s walk through this speciﬁcation:\nPrecondition Take any image x where each pixel is at most 0.1 away from its\ncounterpart in c. Here, both x and c are assumed to be the same size, and the ⩽\nis deﬁned pointwise.1\nAssignments Let r1 be the result of computing f (x) and r2 be the result of\ncomputing f (c).\nPostcondition Then, the predicted labels in vectors r1 and r2 are the same. Re-\ncall that in a classiﬁcation setting, each element of vector ri refers to the proba-\nbility of a speciﬁc label. We use class as a shorthand to extract the index of the\nlargest element of the vector.\n■\nCounterexamples\nA counterexample to a property is a valuation of the variables in the precondition\n(the xis) that falsiﬁes the postcondition. In Example 3.A, a counterexample would\nbe an image x whose classiﬁcation by f is different than that of image c and whose\ndistance from c, i.e., |x −c|, is less than 0.1.\n1The pointwise operation | · | is known as the ℓ∞norm, which we formally discuss in Chap-\nter 11 and compare it to other norms.\nCHAPTER 3. CORRECTNESS PROPERTIES\n25\nExample 3.B Here’s a concrete example (not about image recognition, just a simple\nfunction that adds 1 to the input):\n{ x ⩽0.1 }\nr ←x + 1\n{ r ⩽1 }\nThis property does not hold. Consider replacing x with the value 0.1. Then, r ←\n1 + 0.1 = 1.1. Therefore, the postcondition is falsiﬁed. So, setting x to 0.1 is a\ncounterexample.\n■\nA Note on Hoare Logic\nOur speciﬁcation language looks like speciﬁcations written in Hoare logic (Hoare,\n1969). Speciﬁcations in Hoare logic are called Hoare triples, as they are composed\nof three parts, just like our speciﬁcations. Hoare logic comes equipped with de-\nduction rules that allows one to prove the validity of such speciﬁcations. For our\npurposes in this book, we will not deﬁne the rules of Hoare logic, but many of\nthem will crop up implicitly throughout the book.\n3.3 More Examples of Properties\nWe will now go through a bunch of example properties and write them in our\nspeciﬁcation language.\nEquivalence of Neural Networks\nSay you have a neural network f for image recognition and you want to replace\nit with a new neural network g. Perhaps g is smaller and faster, and since you’re\ninterested in running the network on a stream of incoming images, efﬁciency is\nvery important. One thing you might want to prove is that f and g are equivalent;\nCHAPTER 3. CORRECTNESS PROPERTIES\n26\nhere’s how to write this property:\n{ true }\nr1 ←f (x)\nr2 ←g(x)\n{ class(r1) = class(r2) }\nNotice that the precondition is true, meaning that for any image x, we want the\npredicted labels of f and g to be the same. The true precondition indicates that the\ninputs to the neural networks (x in this case) are unconstrained. This speciﬁcation\nis very strong: the only way it can be true is if f and g agree on the classiﬁcation\non every possible input, which is highly unlikely in practice.\nOne possible alternative is to state that f and g return the same prediction on\nsome subset of images, plus or minus some brightness, as in our above example.\nSay S is a ﬁnite set of images, then:\n{ x1 ∈S, |x1 −x3| ⩽0.1, |x1 −x2| ⩽0.1 }\nr1 ←f (x2)\nr2 ←g(x3)\n{ class(r1) = class(r2) }\nThis says the following: Pick an image x1 and generate two variants, x2 and x3,\nwhose brightness differs a little bit from x1. Then, f and g should agree on the\nclassiﬁcation of the two images.\nThis is a more practical notion of equivalence than our ﬁrst attempt. Our ﬁrst\nattempt forced f and g to agree on all possible inputs, but keep in mind that most\nimages (combinations of pixels) are meaningless noise, and therefore we don’t\ncare about their classiﬁcation. This speciﬁcation, instead, constrains equivalence\nto an inﬁnite set of images that look like those in the set S.\nCollision Avoidance\nOur next example is one that has been a subject of study in the veriﬁcation liter-\nature, beginning with the pioneering work of Katz et al. (2017). Here we have a\ncollision avoidance system that runs on an autonomous aircraft. The system de-\ntects intruder aircrafts and decides what to do. The reason the system is run on\nCHAPTER 3. CORRECTNESS PROPERTIES\n27\na neural network is due to its complexity: The trained neural network is much\nsmaller than a very large table of rules. In a sense, the neural network compresses\nthe rules into an efﬁciently executable program.\nThe inputs to the neural network are the following:\n• vown: the aircraft’s velocity\n• vint: the intruder aircraft’s velocity\n• aint: the angle of the intruder with respect to the current ﬂying direction\n• aown: the angle of the aircraft with respect to the intruder.\n• d: the distance between the two aircrafts\n• prev: the previous action taken.\nGiven the above values, the neural network decides how to steer: left/right,\nstrong left/right, or nothing. Speciﬁcally, the neural network assigns a score to\nevery possible action, and the action with the lowest score is taken.\nAs you can imagine, many things can go wrong here, and if they do—disaster!\nKatz et al. (2017) identify a number of properties that they verify. These properties\ndo not account for all possible scenarios, but they are important to check. Let’s\ntake a look at one that says if the intruder aircraft is far away, then the score for\ndoing nothing should be below some threshold.\n{ d ⩾55947, vown ⩾1145, vint ⩽60 }\nr ←f (d, vown, vint, . . .)\n{ score of nothing in r is below 1500 }\nNotice that the precondition speciﬁes that the distance between the two aircrafts is\nmore than 55947 feet, that the aircraft’s velocity is high, and the intruder’s velocity\nis low. The postcondition speciﬁes that doing nothing should have a low score,\nbelow some threshold. Intuitively, we should not panic if the two aircrafts are\nquite far apart and have moving at very different velocities.\nKatz et al. (2017) explore a number of such properties, and also consider ro-\nbustness properties in the collision-avoidance setting. But how do we come up\nCHAPTER 3. CORRECTNESS PROPERTIES\n28\nwith such speciﬁc properties? It’s not straightforward. In this case, we really need\na domain expert who knows about collision-avoidance systems, and even then,\nwe might not cover all corner cases. A number of people in the veriﬁcation com-\nmunity, the author included, argue that speciﬁcation is harder than veriﬁcation—\nthat is, the hard part is asking the right questions!\nPhysics Modeling\nHere is another example due to Qin et al. (2019). We want the neural network\nto internalize some physical laws, such as the movement of a pendulum. At any\npoint in time, the state of the pendulum is a triple (v, h, w), its vertical position\nv, its horizontal position h, and its angular velocity w. Given the state of the\npendulum, the neural network is to predict the state in the next time instance,\nassuming that time is divided into discrete steps.\nA natural property we may want to check is that the neural network’s un-\nderstanding of how the pendulum moves adheres to the law of conservation of\nenergy. At any point in time, the energy of the pendulum is the sum of its po-\ntential energy and its kinetic energy. (Were you paying attention in high school\nphysics?) As the pendulum goes up, its potential energy increases and kinetic en-\nergy decreases; as it goes down, the opposite happens. The sum of the kinetic and\npotential energies should only decrease over time. We can state this property as\nfollows:\n{ true }\nv′, h′, w′ ←f (v, h, w)\n{ E(h′, w′) ⩽E(h, w) }\nThe expression E(h, w) is the energy of the pendulum, which is its potential\nenergy mgh, where m is the mass of the pendulum and g is the gravitational con-\nstant, plus its kinetic energy 0.5ml2w2, where l is the length of the pendulum.\nNatural-Language Processing\nLet’s recall the natural language example from earlier in the chapter, where we\nwanted to classify a sentence into whether it expresses a positive or negative sen-\ntiment. We decided that we want the classiﬁcation not to change if we replaced\nCHAPTER 3. CORRECTNESS PROPERTIES\n29\na word by a synonym. We can express this property in our language: Let c be a\nﬁxed sentence of length n. We assume that each element of vector c is a real num-\nber representing a word—called an embedding of the word. We also assume that\nwe have a thesaurus T, which given a word gives us a set of equivalent words.\n{ 1 ⩽i ⩽n, w ∈T(ci), x = c[i 7→w] }\nr1 ←f (x)\nr2 ←f (c)\n{ class(r1) = class(r2) }\nThe precondition speciﬁes that variable x is just like the sentence c, except that\nsome element i is replaced by a word w from the thesaurus. We use the notation\nc[i 7→w] to denote c with the ith element replaced with w and ci to denote the ith\nelement of c.\nThe above property allows a single word to be replaced by a synonym. We can\nextend it to two words as follows (I know, it’s very ugly, but it works):\n{ 1 ⩽i, j ⩽n , i ̸= j , wi ∈T(ci) , wj ∈T(cj) , x = c[i 7→wi, j 7→wj] }\nr1 ←f (x)\nr2 ←f (c)\n{ class(r1) = class(r2) }\nMonotonicity\nA standard mathematical property that we may desire of neural networks is mono-\ntonicity (Sivaraman et al., 2020), meaning that larger inputs should lead to larger\noutputs. For example, imagine you’re one of those websites that predict house\nprices using machine learning. You’d expect the machine-learning model used is\nmonotonic with respect to square footage—if you increase the square footage of a\nhouse, its price should not decrease, or perhaps increase. Or imagine a model that\nestimates the risk of complications during surgery. You’d expect that increasing\nthe age of the patient should not decrease the risk. (I’m not a physician, but I like\nCHAPTER 3. CORRECTNESS PROPERTIES\n30\nthis example.) Here’s how you could encode monotonicity in our language:\n{ x > x′ }\nr ←f (x)\nr′ ←f (x′)\n{ r′ ⩾r′ }\nIn other words, pick any pair of inputs such that x > x′, we want f (x) ⩾f (x′).\nOf course, we can strengthen the property by making the postcondition a strict\ninequality—that completely depends on the problem domain we’re working with.\nLooking Ahead\nWe’re done with the ﬁrst part of the book. We have deﬁned neural networks and\nhow to specify their properties. In what follows, we will discuss different ways of\nverifying properties automatically.\nThere has been an insane amount of work on robustness problems, particularly\nfor image recognition. Lack of robustness was ﬁrst observed by Szegedy et al.\n(2014), and since then many approaches to discover and defend against robustness\nviolations (known as adversarial examples) have been proposed. We will survey\nthose later. The robustness properties for natural-language processing we have\ndeﬁned follow those of Ebrahimi et al. (2018) and Huang et al. (2019).\nPart II\nConstraint-Based Veriﬁcation\n32\nChapter 4\nLogics and Satisﬁability\nIn this part of the book, we will look into constraint-based techniques for veriﬁca-\ntion. The idea is to take a correctness property and encode it as a set of constraints.\nBy solving the constraints, we can decide whether the correctness property holds\nor not.\nThe constraints we will use are formulas in ﬁrst-order logic (FOL). FOL is a very\nbig and beautiful place, but neural networks only live in a small and cozy corner\nof it—the corner that we will explore in this chapter.\n4.1 Propositional Logic\nWe begin with the purest of all, propositional logic. A formula F in propositional\nlogic is over Boolean variables (traditionally given the names p, q, r, . . .) and de-\nﬁned using the following grammar:\nF :=\ntrue\nfalse\nvar\nVariable\n| F ∧F\nConjunction (and)\n| F ∨F\nDisjunction (or)\n| ¬F\nNegation (not)\nEssentially, a formula in propositional logic deﬁnes a circuit with Boolean vari-\nables, AND gates (∧), OR gates (∨), and not gates (¬). Negation has the highest\nCHAPTER 4. LOGICS AND SATISFIABILITY\n33\noperator precedence, followed by conjunction and then disjunction. At the end of\nthe day, all programs can be deﬁned as circuits, because everything is a bit on a\ncomputer and there is a ﬁnite amount of memory, and therefore a ﬁnite number\nof variables.\nWe will use fv(F) to denote the set of free variables appearing in the formula.\nFor our purposes, this is the set of all variables that are syntactically present in the\nformula;\nExample 4.A As an example, here is a formula\nF ≜(p ∧q) ∨¬r\nObserve the use of ≜; this is to denote that we’re syntactically deﬁning F to be\nthe formula on the right of ≜, as opposed to saying that the two formulas are\nsemantically equivalent (more on this in a bit). The set of free variables in F is\nfv(F) = {p, q, r}.\n■\nInterpretations\nLet F be a formula over a set of variables fv(F). An interpretation I of F is a map\nfrom variables fv(F) to true or false. Given an interpretation I of a formula F, we\nwill use I(F) to denote the formula where we have replaced each variable in fv(F)\nwith its interpretation in I.\nExample 4.B Say we have the formula\nF ≜(p ∧q) ∨¬r\nand the interpretation\nI = {p 7→true, q 7→true, r 7→false}\nNote that we represent I as a set of pairs of variables and their interpretations.\nApplying I to F, we get\nI(F) ≜(true ∧true) ∨¬false\n■\nCHAPTER 4. LOGICS AND SATISFIABILITY\n34\nEvaluation Rules\nWe will deﬁne the following evaluation, or simpliﬁcation, rules for a formula. The\nformula on the right of ≡is an equivalent, but syntactically simpler, variant of the\none on the left:\ntrue ∧F\n≡\nF\nConjunction\nF ∧true\n≡\nF\nfalse ∧F\n≡\nfalse\nF ∧false\n≡\nfalse\nfalse ∨F\n≡\nF\nDisjunction\nF ∨false\n≡\nF\ntrue ∨F\n≡\ntrue\nF ∨true\n≡\ntrue\n¬true\n≡\nfalse\nNegation\n¬false\n≡\ntrue\nIf a given formula has no free variables, then by applying these rules repeat-\nedly, you will get true or false. We will use eval(F) to denote the simplest form of\nF we can get by repeatedly applying the above rules.\nSatisﬁability\nA formula F is satisﬁable (SAT) if there exists an interpretation I such that\neval(I(F)) = true\nin which case we will say that I is a model of F and denote it\nI |= F\nWe will also use I ̸|= F to denote that I is not a model of F. It follows from our\ndeﬁnitions that I ̸|= F iff I |= ¬F.\nEquivalently, a formula F is unsatisﬁable (UNSAT) if for every interpretation I\nwe have eval(I(F)) = false.\nExample 4.C Consider the formula F ≜(p ∨q) ∧(¬p ∨r). This formula is satisﬁ-\nable; here is a model I = {p 7→true, q 7→false, r 7→true}.\n■\nCHAPTER 4. LOGICS AND SATISFIABILITY\n35\nExample 4.D Consider the formula F ≜(p ∨q) ∧¬p ∧¬q. This formula is unsat-\nisﬁable.\n■\nValidity and Equivalence\nTo prove properties of neural networks, we will be asking validity questions. A\nformula F is valid if every possible interpretation I is a model of F. It follows that\na formula F is valid if and only if ¬F is unsatisﬁable.\nExample 4.E Here is a valid formula F ≜(¬p ∨q) ∨p. Pick any interpretation I\nthat you like; you will ﬁnd that I |= F.\n■\nWe will say that two formulas, A and B, are equivalent if and only if every\nmodel I of A is a model of B, and vice versa. We will denote equivalence as A ≡B.\nThere are many equivalences that are helpful when working with formulas. For\nany formulas A, B, and C, we have commutativity of conjunction and disjunction,\nA ∧B\n≡\nB ∧A\nA ∨B\n≡\nB ∨A\nWe can push negation inwards:\n¬(A ∧B)\n≡\n¬A ∨¬B\n¬(A ∨B)\n≡\n¬A ∧¬B\nMoreover, we have distributivity of conjunction over disjunction (DeMorgan’s laws),\nand vice versa:\nA ∨(B ∧C)\n≡\n(A ∨B) ∧(A ∨C)\nA ∧(B ∨C)\n≡\n(A ∧B) ∨(A ∧C)\nImplication and Bi-implication\nWe will often use an implication A ⇒B to denote the formula\n¬A ∨B\nSimilarly, we will use a bi-implication A ⇔B to denote the formula\n(A ⇒B) ∧(B ⇒A)\nCHAPTER 4. LOGICS AND SATISFIABILITY\n36\n4.2 Arithmetic Theories\nWe can now extend propositional logic using theories. Each Boolean variable now\nbecomes a more complex Boolean expression over variables of different types.\nFor example, we can use the theory of linear real arithmetic (LRA), where a Boolean\nexpression is, for instance,\nx + 3y + z ⩽10\nAlternatively, we can use the theory of arrays, and so an expression may look like:\na[10] = x\nwhere a is an array indexed by integers. There are many other theories that people\nhave studied, including bitvectors (to model machine arithmetic) and strings (to\nmodel string manipulation). The satisﬁabilty problem is now called satisﬁability\nmodulo theories (SMT), as we check satisﬁability with respect to interpretations of\nthe theory.\nIn this section, we will focus on the theory of linear real arithmetic (LRA), as it\nis (1) decidable and (2) can represent a large class of neural-network operations,\nas we will see in the next chapter.\nLinear Real Arithmetic\nIn LRA, each propositional variable is replaced by a linear inequality of the form:\nn\n∑\ni=1\ncixi + b ⩽0\nor\nn\n∑\ni=1\ncixi + b < 0\nwhere ci, b ∈R and {xi}i is a ﬁxed set of variables. For example, we can have a\nformula of the form:\n(x + y ⩽0 ∧x −2y < 10) ∨x > 100\nNote that > and ⩾can be rewritten into < and ⩽. Also note that when a\ncoefﬁcient ci is 0, we simply drop the term cixi, as in the inequality x > 100 above,\nCHAPTER 4. LOGICS AND SATISFIABILITY\n37\nwhich does not include the variable y. An equality x = 0 can be written as the\nconjunction x ⩾0 ∧x ⩽0. Similarly, a disequality x ̸= 0 can be written as\nx < 0 ∨x > 0.\nModels in LRA\nAs with propositional logic, the free variables fv(F) of a formula F in LRA is the\nset of variables appearing in the formula.\nAn interpretation I of a formula F is an assignment of every free variable to a\nreal number. An interpretation I is a model of F, i.e., I |= F, iff eval(I(F)) = true.\nHere, the extension of the simpliﬁcation rules to LRA formulas is straightforward:\nall we need is to add standard rules for evaluating arithmetic inequalities, e.g.,\n2 ⩽0 ≡false.\nExample 4.F As an example, consider the following formula:\nF ≜x −y > 0 ∧x ⩾0\nA model I for F is\n{x 7→1, y 7→0}\nApplying I to F, i.e., I(F), results in\n1 −0 > 0 ∧1 ⩾0\nApplying the evaluation rules, we get true.\n■\nReal vs. Rational\nIn the literature, you might ﬁnd LRA being referred to as linear rational arithmetic.\nThere are two interrelated reasons for that: First, whenever we write formulas\nin practice, the constants in those formulas are rational values—we can’t really\nrepresent π, for instance, in computer memory. Second, let’s say that F contains\nonly rational coefﬁcients. Then, it follows that, if F is satisﬁable, there is a model\nof F that assigns all free variables to rational values.\nCHAPTER 4. LOGICS AND SATISFIABILITY\n38\nExample 4.G Let’s consider a simple formula like x < 10. While {x 7→π} is\na model of x < 10, it also has satisfying assignments that assign x to a rational\nconstant, like {x 7→1/2}. This will always be the case: we cannot construct\nformulas that only have irrational models, unless the formulas themselves contain\nirrational constants, e.g., x = π.\n■\nNon-Linear Arithmetic\nDeciding satisﬁability of formulas in LRA is an NP-complete problem. If we extend\nour theory to allow for polynomial inequalities, then the best known algorithms\nare doubly exponential in the size of the formula in the worst case (Caviness and\nJohnson, 2012). If we allow for transcedental functions—like exp, cos, log, etc.—\nthen satisﬁability becomes undecidable (Tarski, 1998). Thus, for all practical pur-\nposes, we stick to LRA. Even though it is NP-complete (a term that sends shivers\ndown the spines of theoreticians), we have very efﬁcient algorithms that can scale\nto large formulas.\nConnections to MILP\nFormulas in LRA, and the SMT problem for LRA, is equivalent to the mixed integer\nlinear programming (MILP) problem. Just as there are many SMT solvers, there are\nmany MILP solvers out there, too. So the natural question to ask is why don’t\nwe use MILP solvers? In short, we can, and maybe sometimes they will actually\nbe faster than SMT solvers. However, the SMT framework is quite general and\nﬂexible. So not only can we write formulas in LRA, but we can (1) write formulas\nin different theories, as well as (2) formulas combining theories.\nFirst, in practice, neural networks do not operate over real or rational arith-\nmetic. They run using ﬂoating point, ﬁxed point, or machine-integer arithmetic.\nIf we wish to be as precise as possible at analyzing neural networks, we can opt\nfor a bit-level encoding of its operations and use bitvector theories employed by\nSMT solvers. (Machine arithmetic, surprisingly, is practically more expensive to\nsolve than linear real arithmetic, so most of the time we opt for a real-arithmetic\nencoding of neural networks.)\nCHAPTER 4. LOGICS AND SATISFIABILITY\n39\nSecond, as we move forward and neural networks start showing up every-\nwhere, we do not want to verify them in isolation, but in conjunction with other\npieces of code that the neural network interacts with. For example, think of a\npiece of code that parses text and puts it in a form ready for the neural network to\nconsume. Analyzing such piece of code might require using string theories, which\nallow us to use string concatenation and other string operations in formulas. SMT\nsolvers employ theorem-proving techniques for combining theories, and so we can\nwrite formulas, for example, over strings and linear arithmetic.\nThese are the reasons why in this book we use SMT solvers as the target of our\nconstraint-based veriﬁcation: they give us many ﬁrst-order theories and allow us\nto combine them. However, it is important to note that, at the time of writing this,\nmost research on constraint-based veriﬁcation focuses on linear real arithmetic\nencodings.\nLooking Ahead\nIn the next chapter, we will look at how to encode neural-network semantics, and\ncorrectness properties, as formulas in LRA, thus enabling automated veriﬁcation\nusing SMT solvers. After that, we will spend some time studying the algorithms\nunderlying SMT solvers.\nIn veriﬁcation, we typically use fragments of ﬁrst-order logic to encode pro-\ngrams. FOL has a long and storied history. FOL is a very general logic, and its\nsatisﬁability is undecidable, thanks to a proof by Church (1936).\nSMT solvers,\nwhich have been heavily studied over the past twenty years or so aim at solving\nfragments of FOL, like LRA and other theories. I encourage the interested reader to\nconsult the Handbook of Satisﬁability for an in-depth exposition (Biere et al., 2009).\n40\nChapter 5\nEncodings of Neural Networks\nOur goal in this chapter is to translate a neural network into a formula in lin-\near real arithmetic (LRA). The idea is to have the formula precisely (or soundly)\ncapture the input–output relation of the neural network. Once we have such a\nformula, we can use it to verify correctness properties using SMT solvers.\n5.1 Encoding Nodes\nWe begin by characterizing a relational view of a neural network. This will help\nus establish the correctness of our encoding.\nInput-output Relations\nRecall that a neural network is represented as a graph G that deﬁnes a function\nfG : Rn →Rm. We deﬁne the input–output relation of fG as the binary relation RG\ncontaining every possible input and its corresponding output after executing fG.\nFormally, the input–output relation of fG is:\nRG = {(a, b) | a ∈Rn, b = fG(a)}\nWe will similarly use Rv to deﬁne the input–output relation of the function fv of a\nsingle node v in G.\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n41\nExample 5.A Consider the simple function fG(x) = x + 1. Its input–output relation\nis\nRG = {(a, a + 1) | a ∈R}\n■\nEncoding a Single Node, Illustrated\nWe begin by considering the case of a single node v and the associated function\nfv : R →R. A node with a single input is illustrated as follows (recall that, by\ndeﬁnition, a node in our neural network can only produce a single real-valued\noutput):\nv\nSay fv(x) = x + 1. Then, we can construct the following formula in LRA to\nmodel the relation Rv = {(a, a + 1) | a ∈R}:\nFv ≜vo = vin,1 + 1\nwhere vo and vin,1 are real-valued variables. The symbol vo denotes the output of\nnode v and vin,1 denotes its ﬁrst input (it only has a single input).\nConsider the models of Fv; they are all of the form:\n{vin,1 7→a, vo 7→a + 1}\nfor any real number a. We can see a clear one-to-one correspondence between\nelements of Rv and models of Fv.\nLet’s now take a look at a node v with two inputs; assume that fv(x) = x1 +\n1.5x2.\nv\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n42\nThe encoding Fv is as follows:\nFv ≜vo = vin,1 + 1.5vin,2\nObserve how the elements of the input vector, x1 and x2, correspond to the two\nreal-valued variables vin,1 and vin,2.\nEncoding a Single Node, Formalized\nNow that we have seen a couple of examples, let’s formalize the process of en-\ncoding the operation fv of some node v. We will assume that fv : Rnv →R is\npiecewise-linear, i.e., of the form\nf (x) =\n\n\n\n\n\n\n\n\n\n∑j c1\nj · xj + b1\nif S1\n...\n∑j cl\nj · xj + bl\nif Sl\nwhere j ranges from 1 to nv. We will additionally assume that each condition Si is\ndeﬁned as a formula in LRA over the elements of the input x. Now, the encoding\nis as follows:\nFv ≜\nl^\ni=1\n\"\nSi ⇒\n \nvo =\nnv\n∑\nj=1\nci\nj · vin,j + bi\n!#\nThe way to think of this encoding is as a combination of if statements: if Si is true,\nthen vo is equal to the ith inequality. The implication (⇒) gives us the ability to\nmodel a conditional, where the left side of the implication is the condition and the\nright side is the assignment. The big conjunction on the outside of the formula,\nVl\ni=1, essentially combines if statements: “if S1 then ... AND if S2 then ... AND if S3\n...”\nExample 5.B The above encoding is way too general with too many superscripts\nand subscripts. Here’s a simple and practical example, the ReLU function:\nrelu(x) =\n(\nx\nif x > 0\n0\nif x ⩽0\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n43\nA node v such that fv is ReLU would be encoded as follows:\nFv ≜(vin,1 > 0\n|\n{z\n}\nx>0\n⇒vo = vin,1) ∧(vin,1 ⩽0\n|\n{z\n}\nx⩽0\n⇒vo = 0)\n■\nSoundness and Completeness\nThe above encoding precisely captures the semantics of a piecewise-linear node.\nLet’s formally capture this fact: Fix some node v with a piecewise-linear function\nfv. Let Fv be its encoding, as deﬁned above.\nFirst, our encoding is sound: any execution of the node is captured by a model\nof the formula Fv. Informally, soundness means that our encoding does not miss\nany behavior of fv. Formally, let (a, b) ∈Rv and let\nI = {vin,1 7→a1, . . . , vin,n 7→an, vo 7→b}\nThen, I |= Fv.\nSecond, our encoding is complete: any model of Fv maps to a behavior of fv.\nInformally, completeness means that our encoding is tight, or does not introduce\nnew behaviors not exhibited by fv. Formally, let the following be a model of Fv:\nI = {vin,1 7→a1, . . . , vin,n 7→an, vo 7→b}\nThen, (a, b) ∈Rv.\n5.2 Encoding a Neural Network\nWe have shown how to encode a single node of a neural network. We’re now\nready to encode the full-blown graph. The encoding is broken up into two pieces:\n(1) a formula encoding semantics of all nodes, and (2) a formula encoding the\nconnections between them, i.e., the edges.\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n44\nEncoding the Nodes\nRecall that a neural network is a graph G = (V, E), where the set of nodes V\ncontains input nodes Vin, which do not perform any operations. The following\nformula combines the encodings of all non-input nodes in G:\nFV ≜\n^\nv∈V\\Vin\nFv\nAgain, the big conjunction essentially says, “the output of v1 is ... AND the output of\nnode v2 is ... AND ...” This formula, however, is meaningless on its own: it simply\nencodes the input–output relation of every node, but not the connections between\nthem!\nEncoding the Edges\nLet’s now encode the edges. We will do this for every node individually, encoding\nall of its incoming edges. Fix some node v ∈V \\ Vin. Let (v1, v), . . . (vn, v) be\nan ordered sequence of all edges whose target is v. Recall that in Section 2.5,\nwe assumed that there is a total ordering on edges. The reason for this ordering\nis to be able to know which incoming edges feed into which inputs of a node.\nInformally, the edge relation E gives us a bunch of wires to be plugged into node\nv; the ordering tells us where to plug those wires—the ﬁrst wire in the ﬁrst socket,\nthe second wire in the second socket, and so on.\nWe can now deﬁne a formula for edges of v:\nF◦→v ≜\nn^\ni=1\nvin,i = vo\ni\nIntuitively, for each edge (vi, v), we connect the output of node vi with the ith\ninput of v. We can now deﬁne FE as the conjunction of all incoming edges of all\nnon-input nodes:\nFE ≜\n^\nv∈V\\Vin\nF◦→v\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n45\nPutting it all Together\nNow that we have shown how to encode nodes and edges, there is nothing left to\nencode! So let’s put things together. Given a graph G = (V, E), we will deﬁne its\nencoding as follows:\nFG ≜FV ∧FE\nJust as for the single-node encoding, we get soundness and completeness. Let\nRG be the input–output relation of G. Soundness means that FG does not miss any\nof the behaviors in RG. Completeness means that every model of FG maps to an\ninput–output behavior of G.\nNote that the size of the encoding is linear in the size of the neural network\n(number of nodes and edges). Simply, each node gets a formula and each edge\ngets a formula. The formula of node v is of size linear in the size of the (piecewise)\nlinear function fv.\nCorrectness of the Encoding\nAssume we that have the following ordered input nodes in Vin\nv1, . . . , vn\nand the following output nodes in Vo\nvn+1, . . . , vn+m\nOur encoding is sound and complete. First, let’s state soundness: Let (a, b) ∈\nRG and let\nI = {vo\n1 7→a1, . . . , vo\nn 7→an} ∪{vo\nn+1 7→b1, . . . , vo\nn+m 7→bm}\nThen, there exists I′ such that I ∪I′ |= FG.\nNotice that, unlike the single-node setting, the model of FG not only con-\ntains assignments to inputs and outputs of the network, but also the intermediate\nnodes. This is taken care of using I, which assigns values to the outputs of input\nand output nodes, and I′, which assigns the inputs and outputs of all nodes and\ntherefore its domain does not overlap with I.\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n46\nSimilarly, completeness is stated as follows: Let the following be a model of\nFG:\nI = {vo\n1 7→a1, . . . , vo\nn 7→an} ∪{vo\nn+1 7→b1, . . . , vo\nn+m 7→bm} ∪I′\nThen, (a, b) ∈RG.\nAn Example Network and its Encoding\nEnough abstract mathematics. Let’s look at a concrete example neural network G.\nv1\nv2\nv3\nv4\nAssume that fv3(x) = 2x1 + x2 and fv4(x) = relu(x).\nWe begin by constructing formulas for non-input nodes:\nFv3 ≜vo\n3 = 2vin,1\n3\n+ vin,2\n3\nFv4 ≜(vin,1\n4\n> 0 =⇒vo\n4 = vin,1\n4\n) ∧(vin,1\n4\n⩽0 =⇒vo\n4 = 0)\nNext, we construct edge formulas:\nF◦→v3 ≜(vin,1\n3\n= vo\n1) ∧(vin,2\n3\n= vo\n2)\nF◦→v4 ≜vin,1\n4\n= vo\n3\nFinally, we conjoin all of the above formulas to arrive at the complete encoding of\nG:\nFG ≜Fv3 ∧Fv4\n|\n{z\n}\nFV\n∧F◦→v3 ∧F◦→v4\n|\n{z\n}\nFE\n5.3 Handling Non-linear Activations\nIn the above, we have assumed that all of our nodes are associated with piecewise-\nlinear functions, allowing us to precisely capture their semantics in linear real\narithmetic. How can we handle non-piecewise-linear activations, like sigmoid\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n47\n−4 −2\n2\n4\n0.5\n1\nFigure 5.1 Sigmoid function\nand tanh? One way to encode them is by overapproximating their behavior, which\ngives us soundness but not completeness. As we will see, soundness means that\nour encoding can ﬁnd proofs of correctness properties, and completeness means\nthat our encoding can ﬁnd counterexamples to correctness properties. So, by over-\napproximating an activation function, we give up on counterexamples.\nHandling Sigmoid\nLet’s begin with the concrete example of the sigmoid activation:\nσ(x) =\n1\n1 + exp(−x)\nwhich is shown in Figure 5.1. The sigmoid function is (strictly) monotonically\nincreasing, so if we have two points a1 < a2, we know that σ(a1) < σ(a2). We can\nas a result overapproximate the behavior of σ by saying: for any input between a1\nand a2, the output of the function can be any value between σ(a1) and σ(a2).\nConsider Figure 5.2. Here we picked three points on the sigmoid curve, shown\nin red, with x coordinates −1, 0, and 1. The red rectangles deﬁne the lower and\nupper bound on the output of the sigmoid function between two values of x.\nFor example, for inputs between 0 and 1, the output of the function is any value\nbetween 0.5 and 0.73. For inputs more than 1, we know that the output must be\nbetween 0.73 to 1 (the range of σ is upper bounded by 1).\nSay for some node v, fv is a sigmoid activation. Then, one possible encoding,\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n48\nfollowing the approximation in Figure 5.2, is as follows:\nFv ≜(vin,1 ⩽−1 =⇒0 < vo ⩽0.26)\n∧(−1 < vin,1 ⩽0 =⇒0.26 < vo ⩽0.5)\n∧(0 < vin,1 ⩽1 =⇒0.5 < vo ⩽0.73)\n∧(vin,1 > 1 =⇒0.73 < vo < 1)\nEach conjunct speciﬁes a range of inputs (left of implication) and the possible out-\nputs in that range (right of implication). For example, the ﬁrst conjunct speciﬁes\nthat, for inputs ⩽−1, the output can be any value between 0 and 0.26.\nHandling any Monotonic Function\nWe can generalize the above process to any monotonically (increasing or decreas-\ning) function fv.\nLet’s assume that fv is monotonically increasing. We can pick a sequence of\nreal values c1 < · · · < cn. Then, we can construct the following encoding:\nFv ≜(vin,1 ⩽c1 =⇒lb < vo ⩽fv(c1))\n∧(c1 < vin,1 ⩽c2 =⇒fv(c1) < vo ⩽fv(c2))\n...\n∧(cn < vin,1 =⇒fv(cn) < vo ⩽ub)\nwhere lb and ub are the lower and upper bounds of the range of fv; for example,\nfor sigmoid, they are 0 and 1, respectively. If a function is unbounded, then we\ncan drop the constraints lb ⩽vo and vo ⩽ub.\nThe more points ci we choose and the closer they are to each other, the better\nour approximation is. This encoding is sound but incomplete, because it captures\nmore behaviors than conceivable from the activation function. In our sigmoid\nexample, for input ⩾1, the encoding says that the output of the sigmoid is any\nvalue between 0.73 and 1, as indicated by the right-most shaded area in Figure 5.2.\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n49\n−10 −8 −6 −4 −2\n2\n4\n6\n8\n10\n1\n(0, 0.5)\n(1, 0.73)\n(−1, 0.26)\nx\nσ(x)\nFigure 5.2 Sigmoid function with overapproximation\n5.4 Encoding Correctness Properties\nNow that we have shown how to encode the semantics of neural networks as\nlogical constraints, we’re ready for the main dish: encoding entire correctness\nproperties.\nChecking Robustness Example\nWe begin with a concrete example before seeing the general form. Say we have a\nneural network G deﬁning a binary classiﬁer fG : Rn →R2. The neural network\nfG takes a grayscale image as a vector of reals, between 0 and 1, describing the in-\ntensity of each pixel (black to white), and predicts whether the image is of a cat or\na dog. Say we have an image c that is correctly classiﬁed as cat. We want to prove\nthat a small perturbation to the brightness of c does not change the prediction. We\nformalize this as follows:\n{ |x −c| ⩽0.1 }\nr ←fG(x)\n{ r1 > r2 }\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n50\nwhere the ﬁrst output, r1, is the probability of cat, while r2 is the probability of dog.\nThe high-level intuition for the encoding of this correctness property follows\nhow the property is written. The formula that we generate to check this statement,\ncalled the veriﬁcation condition (VC), looks roughly like this:\n(precondition ∧neural network) =⇒postcondition\nIf this formula is valid, then the correctness property holds.\nLet’s assume for our example that the input nodes of the neural network are\n{v1, . . . , vn} and the output nodes are {vn+1, vn+2}. Assume also that the formula\nFG encodes the network, as described earlier in this chapter. We encode the cor-\nrectness property as follows:\n n^\ni=1\n|xi −ci| ⩽0.1\n!\n|\n{z\n}\nprecondition\n∧\nFG\n|{z}\nnetwork\n∧\n n^\ni=1\nxi = vo\ni\n!\n|\n{z\n}\nnetwork input\n∧\n\u0000r1 = vo\nn+1 ∧r2 = vo\nn+2\n\u0001\n|\n{z\n}\nnetwork output\n=⇒\nr1 > r2\n| {z }\npostcondition\nHere’s the breakdown:\n• The precondition is directly translated to an LRA formula. Since LRA for-\nmulas don’t natively support vector operations, we decompose the vector\ninto its constituent scalars. Note that the absolute-value operation | · | is\nnot present natively in LRA, but, fear not, it is actually encodable: A lin-\near inequality with absolute value, like |x| ⩽5, can be written in LRA as\nx ⩽5 ∧−x ⩽5.\n• The network is encoded as a formula FG, just as we saw earlier in Section 5.2.\nThe trick is that we now also need to connect the variables of FG with the\ninputs x and output r. This is captured by the two subformulas labeled\n“network input” and “network output”.\n• The postcondition is encoded as is.\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n51\nEncoding Correctness, Formalized\nA correctness property is of the form\n{ P }\nr1 ←fG1(x1)\nr2 ←fG2(x2)\n...\nrl ←fGl(xl)\n{ Q }\nWhile most examples we see in the book involve a single neural network fG, recall\nfrom Chapter 3 that our properties allow us to consider a collection of networks.\nAssume that the precondition and postcondition are encodable in LRA. We\nthen encode the veriﬁcation condition as follows:\n \nP ∧\nl^\ni=1\nFi\n!\n=⇒Q\nwhere Fi is the encoding of the ith assignment ri ←fGi(xi). The assignment en-\ncoding Fi combines the encoding of the neural network FGi along with connections\nwith inputs and outputs, xi and ri, respectively:\nFi ≜FGi ∧\n\n\nn^\nj=1\nxi,j = vo\ni\n\n∧\n\n\nm\n^\nj=1\nri,j = vo\nn+j\n\n\nHere we make two assumptions:\n• The input and output variables of the encoding of Gi are v1, . . . , vn and\nvn+1, . . . , vn+m, respectively.\n• Each graph Gi has unique nodes and therefore input–output variables.\nInformally, we can think of our encoding,\n\u0010\nP ∧Vl\ni=1 Fi\n\u0011\n=⇒Q, as saying the\nfollowing: “if the precondition is true AND we execute all l networks, then the postcon-\ndition should be true”\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n52\nSoundness and Completeness\nSay we have a correctness property that we have encoded as a formula F. Then,\nwe have the following soundness guarantee: If F is valid, then the correctness\nproperty is true.\nCompleteness depends on whether all functions are encodable in LRA. Assum-\ning all functions are encodable in LRA, then, if F is invalid, we know that there is a\nmodel I |= ¬F. This model is a counterexample to the correctness property. From\nthis model, we can read values for the input variables that result in outputs that\ndo not satisfy the postcondition. This is best seen through an example:\nExample 5.C Take the following simple correctness property, where f (x) = x:\n{ |x −1| ⩽0.1 }\nr ←f (x)\n{ r ⩾1 }\nThis property is not true.\nLet x = 0.99; this satisﬁes the precondition.\nBut,\nf (0.99) = 0.99, which is less than 1. If we encode a formula F for this property,\nthen we will have a model I |= ¬F such that x is assigned 0.99.\n■\nLooking Ahead\nAhh, this chapter was tiring! Thanks for sticking around. We have taken neural\nnetworks, with all their glory, and translated them into formulas. In the coming\nchapters, we will study algorithms for checking satisﬁability of these formulas.\nTo my knowledge, the ﬁrst encoding of neural networks as constraints for ver-\niﬁcation is due to Pulina and Tacchella (2010), predating the current explosion in\ninterest. Bastani et al. (2016) were the ﬁrst to view robustness veriﬁcation as a\nconstraint-solving problem.\nOur encoding of sigmoid follows that of Ehlers (2017). A number of papers\nhave considered MILP encodings that are similar to the ones we give (Tjeng et al.,\n2019a). In MILP, we don’t have disjunction, so we simulate disjunction with an\nCHAPTER 5. ENCODINGS OF NEURAL NETWORKS\n53\ninteger that can take the values {0, 1}. The main issue with LRA and MILP encod-\nings is disjunction; with no disjunction, the problem is polynomial-time solvable.\nDisjunctions mostly arise due to ReLUs. We say that a ReLU is active if its out-\nput is > 0 and inactive otherwise. If a ReLU is active or inactive for all possible\ninputs to the neural network, as prescribed by the precondition, then we can get\nrid of the disjunction, and treat it as the function f (x) = 0 (inactive) or f (x) = x\n(active). With this idea in mind, there are two tricks to simplify veriﬁcation: (1)\nWe can come up with lightweight techniques to discover which ReLUs are ac-\ntive or inactive. The abstraction-based veriﬁcation techniques discussed in Part\nIII of the book can be used. (2) Typically, when we train neural networks, we\naim to maximize accuracy on some training data; we can additionally bias the\ntraining towards neural networks where most of the ReLUs are always active/in-\nactive (Tjeng et al., 2019b).\nIn practice, neural networks are implemented using ﬁnite-precision arithmetic,\nwhere real numbers are approximated as ﬂoating-point numbers, ﬁxed-point num-\nbers, or even machine integers. Some papers carefully ensure that veriﬁcation re-\nsults hold for a ﬂoating-point implementation of the network (Katz et al., 2017).\nA recent paper has shown that veriﬁed neural networks in LRA may not really be\nrobust when one considers the bit-level behavior (Jia and Rinard, 2020b). A num-\nber of papers have also considered bit-level veriﬁcation of neural networks, using\npropositional logic instead of LRA (Jia and Rinard, 2020a; Narodytska et al., 2018).\n54\nChapter 6\nDPLL Modulo Theories\nIn the previous chapter, we saw how to reduce the veriﬁcation problem to that\nof checking satisﬁability of a logical formula. But how do we actually check sat-\nisﬁability? In this chapter, we will meet the DPLL (Davis–Putnam–Logemann–\nLoveland) algorithm, which was developed decades ago for checking satisﬁabil-\nity of Boolean formulas. Then we will see an extension of DPLL that can handle\nﬁrst-order formulas over theories.\nThese algorithms underlie modern SAT and SMT solvers. I’ll give a complete\ndescription of DPLL in the sense that you can follow the chapter and implement a\nworking algorithm. But note that there are numerous data structures, implemen-\ntation tricks, and heuristics that make DPLL really work in practice, and we will not\ncover those here. (At the end of the chapter, I point you to additional resources.)\n6.1 Conjunctive Normal Form (CNF)\nThe goal of DPLL is to take a Boolean formula F and decide whether it is SAT or\nUNSAT. In case F is SAT, DPLL should also return a model of F. We begin by talking\nabout the shape of the formula DPLL expects as input.\nDPLL expects formulas to be in conjunctive normal form (CNF).\nLuckily, all\nBoolean formulas can be rewritten into equivalent formulas in CNF. (We will see\nhow later in this chapter.) A formula F in CNF is of the following form:\nC1 ∧· · · ∧Cn\nCHAPTER 6. DPLL MODULO THEORIES\n55\nwhere each subformula Ci is called a clause and is of the form\nℓ1 ∨· · · ∨ℓmi\nwhere each ℓi is called a literal and is either a variable, e.g., p, or its negation, e.g.,\n¬p.\nExample 6.A The following is a CNF formula with two clauses, each of which\ncontains two literals:\n(p ∨¬r) ∧(¬p ∨q)\nThe following formula is not in CNF:\n(p ∧q) ∨(¬r)\n■\n6.2 The DPLL Algorithm\nThe completely naïve way to decide satisﬁability is by trying every possible in-\nterpretation and checking if it is a model of the formula. Of course, there are ex-\nponentially many interpretations in the number of variables. DPLL tries to avoid\ndoing a completely blind search, but, on a bad day, it will just devolve into an\nexponential enumeration of all possible interpretations—after all, satisﬁability is\nthe canonical NP-complete problem.\nDPLL alternates between two phases: deduction and search. Deduction tries\nto simplify the formula using the laws of logic. Search simply searches for an\ninterpretation.\nDeduction\nThe deduction part of DPLL is called Boolean constant propagation (BCP). Imagine\nthat you have the following formula in CNF:\n(ℓ) ∧C2 ∧· · · Cn\nCHAPTER 6. DPLL MODULO THEORIES\n56\nNotice that the ﬁrst clause consists of a single literal—we call it a unit clause.\nClearly, any model of this formula must assign ℓthe value true: Speciﬁcally, if\nℓis a variable p, then p must be assigned true; if ℓis a negation ¬p, then p must be\nassigned false.\nThe BCP phase of DPLL will simply look for all unit clauses and replace their\nliterals with true. BCP is enabled by the fact that formulas are in CNF, and it can be\nquite effective at proving SAT or UNSAT.\nExample 6.B Consider the following formula\nF ≜(p) ∧(¬p ∨r) ∧(¬r ∨q)\nBCP ﬁrst ﬁnds the unit clause (p) and assigns p the value true. This results in the\nfollowing formula:\n(true) ∧(¬true ∨r) ∧(¬r ∨q)\n≡(r) ∧(¬r ∨q)\nClearly BCP’s job is not done: the simpliﬁcation has produced another unit clause,\n(r). BCP sets r to true, resulting in the following formula:\n(true) ∧(¬true ∨q)\n≡(q)\nFinally, we’re left with a single clause, the unit clause (q). Therefore BCP assigns\nq the value true, resulting in the ﬁnal formula true. This means that F is SAT, and\n{p 7→true, q 7→true, r 7→true} is a model.\n■\nIn the above example, BCP managed to show satisﬁability of F. Note that BCP\ncan also prove unsatisﬁability, by simplifying the formula to false. But BCP might\nget stuck if it cannot ﬁnd unit clauses. This is where search is employed by DPLL.\nDeduction + Search\nAlgorithm 1 shows the entire DPLL algorithm. The algorithm takes a formula F in\nCNF.\nCHAPTER 6. DPLL MODULO THEORIES\n57\nAlgorithm 1: DPLL\nData: A formula F in CNF form\nResult: I |= F or UNSAT\n▷Boolean constant propagation (BCP)\nwhile there is a unit clause (ℓ) in F do\nLet F be F[ℓ7→true]\nif F is true then return SAT\n▷Search\nfor every variable p in F do\nIf DPLL(F[p 7→true]) is SAT then return SAT\nIf DPLL(F[p 7→false]) is SAT then return SAT\nreturn UNSAT\n▷The model I that is returned by DPLL when the result is SAT is\nmaintained implicitly in the sequence of assignments to variables (of the\nform [l 7→·] and [p 7→·])\nThe ﬁrst part performs BCP: the algorithm keeps simplifying the formula until\nno more unit clauses exist. We use the notation F[ℓ7→true] to mean replace all\noccurrences of ℓin F with true and simplify the resulting formula. Speciﬁcally, if\nℓis a variable p, then all occurrences of p are replaced with true; if ℓis a negation\n¬p, then all occurrences of p are replaced with false.\nAfter BCP is done, the algorithm checks if the formula is true, which means that\nBCP has proven that the formula is SAT.\nIf BCP is unsuccessful in proving SAT, then DPLL moves to the search phase:\nit iteratively chooses variables and tries to replace them with true or false, calling\nDPLL recursively on the resulting formula. The order in which variables are cho-\nsen in search is critical to DPLL’s performance. There is a lot of research on variable\nselection. One of the popular heuristics maintains a continuously updated score\nsheet, where variables with higher scores are chosen ﬁrst.\nAlgorithm 1, as presented, returns SAT when the input formula is satisﬁable,\nbut does not return a model. The model I that is returned by DPLL when the result\nCHAPTER 6. DPLL MODULO THEORIES\n58\nis SAT is maintained implicitly in the sequence of assignments to variables (of the\nform [l 7→·] and [p 7→·]) made by BCP and search that led to SAT being returned.\nThe algorithm returns UNSAT when it has exhausted all possible satisfying assign-\nments.\nExample 6.C Consider the following formula given to DPLL\nF ≜(p ∨r) ∧(¬p ∨q) ∧(¬q ∨¬r)\nFirst level of recursion DPLL begins by attempting BCP, which cannot ﬁnd any\nunit clauses. Then, it proceeds to search. Suppose that search chooses variable\np, setting it to true by invoking DPLL recursively on\nF1 = F[p 7→true] = q ∧(¬q ∨¬r)\nSecond level of recursion Next, DPLL attempts BCP on F1. First, it sets q to true,\nresulting in the formula\nF2 = F1[q 7→true] = (¬r)\nThen, it sets r to false, resulting in\nF3 = F2[r 7→false] = true\nSince F3 is true, DPLL returns SAT. Implicitly, DPLL has built up a model of F:\n{p 7→true, q 7→true, r 7→false}\n■\nPartial Models\nNote that DPLL may terminate with SAT but without assigning every variable in\nthe formula. We call the resulting model a partial model. You can take a partial\nmodel and extend it by assigning the remaining variables in any way you like, and\nyou’ll still have a model of the formula.\nCHAPTER 6. DPLL MODULO THEORIES\n59\nExample 6.D Consider this simple formula:\nF ≜p ∧(q ∨p ∨¬r) ∧(p ∨¬q)\nThe ﬁrst thing DPLL will do is apply BCP, which will set the unit clause p to true.\nThe rest of the formula then simpliﬁes to true. This means that q and r are useless\nvariables—give them any interpretation and you’ll end up with a model, as long\nas p is assigned true. Therefore, we call I = {p →true} a partial model of F.\nFormally, eval(I(F)) = true.\n■\n6.3 DPLL Modulo Theories\nWe have seen how DPLL can decide satisﬁability of Boolean formulas. We now\npresent DPLL modulo theories, or DPLLT, an extension of DPLL that can handle for-\nmulas over, for example, arithmetic theories like LRA. The key idea of DPLLT is to\nstart by treating a formula as if it is completely Boolean, and then incrementally\nadd more and more theory information until we can conclusively say that a for-\nmula is SAT or UNSAT. We begin by deﬁning the notion of Boolean abstraction of\na formula.\nBoolean Abstraction\nFor illustration, we assume that we’re dealing with formulas in LRA, as with the\nprevious chapters. Say we have the following formula in LRA:\nF ≜(x ⩽0 ∨x ⩽10) ∧(¬x ⩽0)\nThe Boolean abstraction of F, denoted FB, is the formula where every unique linear\ninequality in the formula is replaced with a special Boolean variable, as follows:\nFB ≜(p ∨q) ∧(¬p)\nThe inequality x ⩽0 is abstracted as p and x ⩽10 is abstracted as q. Note that\nboth occurrences of x ⩽0 are replaced by the same Boolean variable, though this\nCHAPTER 6. DPLL MODULO THEORIES\n60\nAlgorithm 2: DPLLT\nData: A formula F in CNF form over theory T\nResult: I |= F or UNSAT\nLet FB be the abstraction of F\nwhile true do\nIf DPLL(FB) is UNSAT then return UNSAT\nLet I be the model returned by DPLL(FB)\nAssume I is represented as a formula\nif IT is satisﬁable (using a theory solver) then\nreturn SAT and the model returned by theory solver\nelse\nLet FB be FB ∧¬I\nneed not be the case for the correctness of our exposition. We will also use the\nsuperscript T to map Boolean formulas back to theory formulas, e.g., (FB)T is F.\nWe call this process abstraction because constraints are lost in the process; namely,\nthe relation between different inequalities is obliterated. Formally speaking, if FB\nis UNSAT, then F is UNSAT. But the converse does not hold: if FB is SAT, it does\nnot mean that F is SAT.\nExample 6.E Consider the formula F ≜x ⩽0 ∧x ⩾10. This formula is clearly\nUNSAT. However, its abstraction, p ∧q, is SAT.\n■\nLazy DPLL Modulo Theories\nThe DPLLT algorithm takes a formula F, over some theory like LRA, and decides\nsatisﬁability.\nDPLLT assumes access to a theory solver. The theory solver takes\na conjunction of, for example, linear inequalities, and checks their satisﬁability.\nIn a sense, the theory solver takes care of conjunctions and the DPLL algorithm\ntakes care of disjunctions. In the case of LRA, the theory solver can be the Simplex\nalgorithm, which we will see in the next chapter.\nDPLLT, shown in Algorithm 2, works as follows: First, using vanilla DPLL, it\nchecks if the abstraction FB is UNSAT, in which case it can declare that F is UNSAT,\nCHAPTER 6. DPLL MODULO THEORIES\n61\nfollowing the properties of abstraction discussed above. The tricky part comes\nwhen dealing with the case where FB is SAT, because that does not necessarily\nmean that F is SAT. This is where the theory solver comes into play. We take\nthe model I returned by DPLL(FB) and map it to a formula IT in the theory; for\nexample, if the theory we’re working with is LRA, IT is a conjunction of linear\ninequalities. If the theory solver deems IT satisﬁable, then we know that F is\nsatisﬁable and we’re done. Otherwise, DPLLT learns the fact that I is not a model.\nSo it negates I and conjoins it to FB. In a sense, the DPLLT lazily learns more and\nmore facts, reﬁning the abstraction, until it can decide SAT or UNSAT.\nExample 6.F Consider the following LRA formula F:\nx ⩾10 ∧(x < 0 ∨y ⩾0)\nand its abstraction FB:\np ∧(q ∨r)\nwhere p denotes x ⩾10, q denotes x < 0, and r denotes y ⩾0.\nFirst iteration DPLLT begins by invoking DPLL on FB. Suppose DPLL returns\nthe partial model\nI1 = {p 7→true, q 7→true}\nWe will represent I1 as a formula\np ∧q\nNext, we check if I1 is indeed a model of F. We do so by invoking the theory\nsolver on IT\n1 , which is\nx ⩾10\n| {z }\np\n∧x < 0\n| {z }\nq\nThe theory solver will say that IT\n1 is UNSAT, because x cannot be ⩾10 and < 0.\nTherefore, DPLLT blocks this model by conjoining ¬I1 to FB. This makes FB the\nfollowing formula, which is still in CNF, because ¬I1 is a clause:\np ∧(q ∨r) ∧(¬p ∨¬q)\n|\n{z\n}\n¬I1\nCHAPTER 6. DPLL MODULO THEORIES\n62\nIn other words, we’re saying that we cannot have a model that sets both x ⩾10\nand x < 0 to true.\nSecond iteration In the second iteration, DPLLT invokes DPLL on the updated\nFB. DPLL cannot give us the same model I1. So it gives us another one, say\nI2 = p ∧¬q ∧r. The theory solver checks IT\n2 , which is satisﬁable, and returns\nits own theory-speciﬁc model, e.g., {x 7→10, y 7→0}. We’re now done, and we\nreturn the model.\n■\n6.4 Tseitin’s Transformation\nWe’ve so far assumed that formulas are in CNF. Indeed, we can take any formula\nand turn it into an equivalent formula in CNF. We can do this by applying De-\nMorgan’s laws (see Chapter 4), by distributing disjunction over conjunction. For\nexample, r ∨(p ∧q) can be rewritten into the equivalent (r ∨p) ∧(r ∨q). This\ntransformation, unfortunately, can lead to an exponential explosion in the size\nof the formula. It turns out that there’s a simple technique, known as Tseitin’s\ntransformation, that produces a formula of size linear in the size of the non-CNF\nformula.\nTseitin’s transformation takes a formula F and produces a CNF formula F′. The\nset of variables of F is a subset of the variables in F′; i.e., Tseitin’s transformation\ncreates new variables. Tseitin’s transformation guarantees the following proper-\nties:\n1. Any model of F′ is also a model of F, if we disregard the interpretations of\nnewly added variables.\n2. If F′ is UNSAT, then F is UNSAT.\nTherefore, given a non-CNF formula F, to check its satisﬁability, we can simply\ninvoke DPLL on F′.\nCHAPTER 6. DPLL MODULO THEORIES\n63\nIntuition\nTseitin’s transformation is pretty much the same as rewriting a complex arithmetic\nexpression in a program into a sequence of instructions where every instruction is\nan application of a single unary or binary operator. (This is roughly what a com-\npiler does when compiling a high-level program to assembly or an intermediate\nrepresentation.) For example, consider the following function (in Python syntax):\ndef f(x,y,z):\nreturn x + (2*y + 3)\nThe return expression can be rewritten into a sequence of operations, each oper-\nating on one or two variables, as follows, where t1, t2, and t3 are temporary\nvariables:\ndef f(x,y,z):\nt1 = 2 * y\nt2 = t1 + 3\nt3 = x + t2\nreturn t3\nIntuitively, each subexpression is computed and stored in a temporary variable:\nt1 contains the expression 2*y, t2 contains the expression 2*y + 3, and t3 con-\ntains the entire expression x + (2*y + 3).\nTseitin Step 1: NNF\nThe ﬁrst thing that Tseitin’s transformation does is to push negation inwards so\nthat ¬ only appears next to variables. E.g., ¬(p ∧r) is rewritten into ¬p ∨¬r. This\nis known as negation normal form (NNF). Any formula can be easily translated to\nNNF by repeatedly applying the following rewrite rules until we can’t rewrite the\nformula any further:\n¬(F1 ∧F2) →¬F1 ∨¬F2\n¬(F1 ∨F2) →¬F1 ∧¬F2\n¬¬F →F\nCHAPTER 6. DPLL MODULO THEORIES\n64\nIn other words, (sub)formulas that match the patterns on the left of →are trans-\nformed into the patterns on the right. In what follows we assume formulas are in\nNNF.\nTseitin Step 2: Subformula Rewriting\nWe deﬁne a subformula of F to be any subformula that contains a disjunction or\nconjunction—i.e., we don’t consider subformulas at the level of literals.\nExample 6.G The following formula F is decomposed into 4 subformulas:\nF ≜(p ∧q)\n| {z }\nF1\n∨(q ∧¬r\n| {z }\nF2\n∧s)\n|\n{z\n}\nF3\n|\n{z\n}\nF4\nF1 and F2 are in the deepest level of nesting, while F4 is in the shallowest level.\nNotice that F2 is a subformula of F3, and all of Fi are subformulas of F4.\n■\nNow for the transformation steps, we assume that F has n subformulas:\n1. For every subformula Fi of F, create a fresh variable ti. These variables are\nanalogous to the temporary variables ti we introduced to our Python program above.\n2. Next, starting with those most-deeply nested subformula, for every subfor-\nmula Fi, create the following formula: Let Fi be of the form ℓi ◦ℓ′\ni, where ◦\nis ∧or ∨and ℓi, ℓ′\ni are literals. Note that one or both of ℓi and ℓ′\ni may be the\nnew variable tj denoting a subformula Fj of Fi. Create the formula\nF′\ni ≜ti ⇔(ℓi ◦ℓ′\ni)\nThese formulas are analogous to the assignments to the temporary variables in our\nPython program above, where ⇔is the logical analogue of variable assignment (=).\nExample 6.H\nContinuing our running example, for subformula F1, we create the\nformula\nF′\n1 ≜t1 ⇔(p ∧q)\nCHAPTER 6. DPLL MODULO THEORIES\n65\nFor subformula F2, we create\nF′\n2 ≜t2 ⇔(q ∧¬r)\nFor subformula F3, we create\nF′\n3 ≜t3 ⇔(t2 ∧s)\n(Notice that q ∧¬r is replaced by the variable t2.) Finally, for subformula F4, we\ncreate\nF′\n4 ≜t4 ⇔(t1 ∨t3)\n■\nNotice that each F′\ni can be written in CNF. This is because, following the deﬁ-\nnition of ⇔and DeMorgan’s laws, we have:\nℓ1 ⇔(ℓ2 ∨ℓ3) ≡(¬ℓ1 ∨ℓ2 ∨ℓ3) ∧(ℓ1 ∨¬ℓ2) ∧(ℓ1 ∨¬ℓ3)\nand\nℓ1 ⇔(ℓ2 ∧ℓ3) ≡(¬ℓ1 ∨ℓ2) ∧(¬ℓ1 ∨ℓ3) ∧(ℓ1 ∨¬ℓ2 ∨¬ℓ3)\nFinally, we construct the following CNF formula:\nF′ ≜tn ∧\n^\ni\nF′\ni\nBy construction, in any model of F′, each ti is assigned true if and only if the\nsubformula Fi evaluates to true. Therefore, the constraint tn in F’ says that F must\nbe true. You can think of tn as the return statement in our transformed Python program.\nExample 6.I Continuing our running example, we ﬁnally construct the CNF for-\nmula\nF′ ≜t4 ∧F′\n1 ∧F′\n2 ∧F′\n3 ∧F′\n4\nSince all of the formulas F′\ni can be written in CNF, as described above, F′ is in CNF.\n■\nAt this point, we’re done. Given a formula in some theory and a theory solver,\nwe ﬁrst rewrite the formula in CNF, using Tseitin’s tranformation, and invoke\nDPLLT.\nCHAPTER 6. DPLL MODULO THEORIES\n66\nLooking Ahead\nI gave a trimmed down version of DPLLT. A key idea in modern SAT and SMT\nsolvers is conﬂict-driven clause learning, a graph data structure that helps us cut\nthe search space by identifying sets of interpretations that do not make satisfying\nassignments. I encourage the interested reader to consult Biere et al. (2009) for a\ndetailed exposition of clause learning and other ideas.\nI also encourage you to play with popular SAT and SMT solvers. For example,\nMiniSAT (Een, 2005), as the name suggests, has a small and readable codebase.\nFor SMT solvers, I recommend checking out Z3 (de Moura and Bjørner, 2008) and\nCVC4 (Barrett et al., 2011). One of the interesting ideas underlying SMT solvers is\ntheory combination (Nelson and Oppen, 1979), where you can solve formulas com-\nbining different theories. This is useful when doing general program veriﬁcation,\nwhere programs may manipulate strings, arrays, integers, etc. In the future, I\nstrongly suspect that theory combination will be needed for neural-network veri-\nﬁcation, because we will start looking at neural networks as components of bigger\npieces of software.\n67\nChapter 7\nNeural Theory Solvers\nIn the previous chapter, we discussed DPLLT for solving formulas in FOL. In this\nchapter, we study the Simplex algorithm for solving conjunctions of literals in linear\nreal arithmetic. Then, we extend the solver to natively handle rectiﬁed linear units\n(ReLUs), which would normally be encoded as disjunctions and thus dealt with\nusing the SAT solver.\n7.1 Theory Solving and Normal Forms\nThe Problem\nThe theory solver for LRA receives a formula F as a conjunction of linear inequal-\nities:\nn^\ni=1\n \nm\n∑\nj=1\ncij · xj ⩾bi\n!\nwhere cij, bi ∈R. The goal is to check satisﬁability of F, and, if satisﬁable, discover\na model I |= F.\nNotice that our formulas do not have strict inequalities (>). The approach\nwe present here can be easily generalized to handle strict inequalities, but for\nsimplicity, we stick with inequalities.1\n1See Dutertre and De Moura (2006) for how to handle strict inequalities. In most instances\nof verifying properties of neural networks, we do not need strict inequalities to encode network\nsemantics or properties.\nCHAPTER 7. NEURAL THEORY SOLVERS\n68\nSimplex Form\nThe Simplex algorithm, discussed in the next section, expects formulas to be in a\ncertain form (just like how DPLL expected propositional formulas to be in CNF).\nSpeciﬁcally, Simplex expects formulas to be conjunctions of equalities of the form\n∑\ni\nci · xi = 0\nand bounds of the form\nli ⩽xi ⩽ui\nwhere ui, li ∈R ∪{∞, −∞}. We use ∞(resp. −∞) to indicate that a variable has\nno upper (resp. lower) bound.\nTherefore, given a formula F, we need to translate it into an equivalent formula\nof the above form, which we will call the Simplex form (also known as slack form).\nIt turns out that translating a formula into Simplex form is pretty simple. Suppose\nthat\nF ≜\nn^\ni=1\n \nm\n∑\nj=1\ncij · xj ⩾bi\n!\nThen, we take every inequality and translate it into two conjuncts, an equality and\na bound. From the ith inequality,\nm\n∑\nj=1\ncij · xj ⩾bi\nwe generate the equality\nsi =\nm\n∑\nj=1\ncij · xj\nand the bound\nsi ⩾bi\nwhere si is a new variable, called a slack variable. Slack variables are analogous to\nthe temporary variables introduced by Tseitin’s transformation (Chapter 6).\nCHAPTER 7. NEURAL THEORY SOLVERS\n69\nExample 7.A Consider the formula F, which we will use as our running example:\nx + y ⩾0\n−2x + y ⩾2\n−10x + y ⩾−5\nFor clarity, we will drop the conjunction operator and simply list the inequalities.\nWe convert F into a formula Fs in Simplex form:\ns1 = x + y\ns2 = −2x + y\ns3 = −10x + y\ns1 ⩾0\ns2 ⩾2\ns3 ⩾−5\n■\nThis transformation is a simple rewriting of the original formula that maintains\nsatisﬁability. Let Fs be the Simplex form of some formula F. Then, we have the\nfollowing guarantees (again, the analogue of Tseitin’s transformation for non-CNF\nformulas):\n1. Any model of Fs is a model of F, disregarding assignments to slack variables.\n2. If Fs is UNSAT, then F is UNSAT.\n7.2 The Simplex Algorithm\nWe’re now ready to present the Simplex algorithm. This is a very old idea due\nto George Dantizg, who developed it in 1947 (see Dantzig’s recollection of the\norigins of Simplex (Dantzig, 1990)). The goal of the algorithm is to ﬁnd a satisfying\nassignment that maximizes some objective function. Our interest in veriﬁcation is\ntypically to ﬁnd any satisfying assignment, and so the algorithm we will present\nis a subset of Simplex.\nCHAPTER 7. NEURAL THEORY SOLVERS\n70\nIntuition\nOne can think of the Simplex algorithm as a procedure that simultaneously looks\nfor a model and a proof of unsatisﬁability. It starts with some interpretation and\ncontinues to update it in every iteration, until it ﬁnds a model or discovers a proof\nof unsatisﬁability. We start from the interpretation I that sets all variables to 0.\nThis assignment satisﬁes all the equalities, but may not satisfy the bounds. In\nevery iteration of Simplex, we pick a bound that is not satisﬁed and we modify I\nto satisfy it, or we discover that the formula is unsatisﬁable. Let’s see this process\npictorially on a satisﬁable example before we dig into the math.\n-3\n-2\n-1\n0\n2\n3\n-2\n-1\n2\n3\n4\nx\ny\nx + y ⩾0\n−10x + y ⩾−5\n−2x + y ⩾2\nI0\nI1\nI2\n-3\n-2\n-1\n0\n2\n3\n-2\n-1\n2\n3\n4\nFigure 7.1 Simplex example\nExample 7.B Recall the formula F from our running example, illustrated in Fig-\nure 7.1, where the satisfying assignments are the shaded region. Each inequality\ndeﬁnes halfspace, i.e., splits R2 in half. Taking all of the inequalities, we get the\nshaded region—the intersection of the all halfspaces.\nx + y ⩾0\n−2x + y ⩾2\n−10x + y ⩾−5\nCHAPTER 7. NEURAL THEORY SOLVERS\n71\nSimplex begins with the initial interpretation\nI0 = {x 7→0, y 7→0}\nshown in Figure 7.1, which is not a model of the formula.\nSimplex notices that\nI0 ̸|= −2x + y ⩾2\nand so it decreases the interpretation of x to −1, resulting in I1. Then, Simplex\nnotices that\nI1 ̸|= x + y ⩾0\nand so it increases the interpretation of y from 0 to 2/3, resulting in the satisfying\nassignment I2. (Notice that x also changes in I2; we will see why shortly.) In a\nsense, Simplex plays Whac-A-Mole, trying to satisfy one inequality only to break\nanother, until it arrives at an assignment that satisﬁes all inequalities. Luckily, the\nalgorithm actually terminates.\n■\nBasic and Non-basic Variables\nRecall that Simplex expects an input formula to be in Simplex form. The set of\nvariables in the formula are broken into two subsets:\nBasic variables are those that appear on the left hand side of an equality; ini-\ntially, basic variables are the slack variables.\nNon-basic variables are all other variables.\nAs Simplex progresses, it will rewrite the formula, thus some basic variables will\nbecome non-basic and vice versa.\nExample 7.C In our running example, initially the set of basic variables is {s1, s2, s3}\nand non-basic variables is {x, y}.\n■\nTo ensure termination of Simplex, we will ﬁx a total ordering on the set of all\n(basic and non-basic) variables. So, when we say “the ﬁrst variable that...”, we’re\nCHAPTER 7. NEURAL THEORY SOLVERS\n72\nreferring to the ﬁrst variable per our ordering. To easily refer to variables, we will\nassume they are of the form x1, . . . , xn. Given a basic variable xi and a non-basic\nvariable xj, we will use cij to denote the coefﬁcient of xj in the equality\nxi = . . . + cij · xj + . . .\nFor a variable xi, we will use li and ui to denote its lower bound and upper bound,\nrespectively. If a variable does not have an upper bound (resp. lower bound), its\nupper bound is ∞(resp. −∞). Note that non-slack variables have no bounds.\nSimplex in Detail\nWe’re now equipped to present the Simplex algorithm, shown in Algorithm 3.\nThe algorithm maintains the following two invariants:\n1. The interpretation I always satisﬁes the equalities, so only the bounds may\nbe violated. This is initially true, as I assigns all variables to 0.\n2. The bounds of non-basic variables are all satisﬁed. This is initially true, as\nnon-basic variables have no bounds.\nIn every iteration of the while loop, Simplex looks for a basic variable whose\nbounds are not satisﬁed by the current interpretation, and attempts to ﬁx the in-\nterpretation. There are two symmetric cases, encoded as two branches of the if\nstatement, xi < li or xi > ui.\nLet’s consider the ﬁrst case, xi < li. Since xi is less than li, we need to increase\nits assignment in I. We do this indirectly by modifying the assignment of a non-\nbasic variable xj. But which xj should we pick? In principle, we can pick any xj\nsuch that the coefﬁcient cij ̸= 0, and adjust the interpretation of xj accordingly. If\nyou look at the algorithm, there are a few extra conditions. If we cannot ﬁnd an\nxj that satisﬁes these conditions, then the problem is UNSAT. We will discuss the\nunsatisﬁability conditions shortly. For now, assume we have found an xj. We can\nincrease its current interpretation by li−I(xi)\ncij\n; this makes the interpretation of xi\nincrease by li −I(xi), thus barely satisfying the lower bound, i.e., I(xi) = li. Note\nthat the interpretations of basic variables are assumed to change automatically\nCHAPTER 7. NEURAL THEORY SOLVERS\n73\nwhen we change the interpretation of non-basic variables. This maintains the ﬁrst\ninvariant of the algorithm.2\nAfter we have updated the interpretation of xj, there is a chance that we have\nviolated one of the bounds of xj. Therefore, we rewrite the formulas such that xj\nbecomes a basic variable and xi a non-basic variable. This is known as the pivot\noperation, and it is mechanically done as follows: Take the following equality,\nwhere N is the set of indices of non-basic variables:\nxi = ∑\nk∈N\ncikxk\nand rewrite it by moving xj to the left-hand side:\nxj = −xi\ncij\n+\n∑\nk∈N\\{j}\ncik\ncij\nxk\n|\n{z\n}\nreplace xj with this\nNow, replace xj in all other equalities with the expression above. This operation\nresults in a set of equalities where xj only appears once, on the left-hand side. And\nso, after pivoting, xj becomes a basic variable and xi a non-basic one.\nExample 7.D Let’s now work through our running example in detail. Recall that\nour formula is:\ns1 = x + y\ns2 = −2x + y\ns3 = −10x + y\ns1 ⩾0\ns2 ⩾2\ns3 ⩾−5\nSay the variables are ordered as follows:\nx, y, s1, s2, s3\n2Basic variables are sometimes called dependent variables and non-basic variables independent\nvariables, indicating that the assignments of basic variables depend on those of non-basic vari-\nables.\nCHAPTER 7. NEURAL THEORY SOLVERS\n74\nAlgorithm 3: Simplex\nData: A formula F in Simplex form\nResult: I |= F or UNSAT\nLet I be the interpretation that sets all variables fv(F) to 0\nwhile true do\nif I |= F then return I\nLet xi be the ﬁrst basic variable s.t. I(xi) < li or I(xi) > ui\nif I(xi) < li then\nLet xj be the ﬁrst non-basic variable s.t.\n(I(xj) < uj and cij > 0) or (I(xj) > lj and cij < 0)\nif If no such xj exists then return UNSAT\nI(xj) ←I(xj) + li−I(xi)\ncij\nelse\nLet xj be the ﬁrst non-basic variable s.t.\n(I(xj) > lj and cij > 0) or (I(xj) < uj and cij < 0)\nif If no such xj exists then return UNSAT\nI(xj) ←I(xj) + ui−I(xi)\ncij\nPivot xi and xj\nInitially, the bounds of s1 and s3 are satisﬁed, but s2 is violated, because s2 ⩾2 but\nI0(s2) = 0, as all variables are assigned 0 initially.\nFirst iteration In the ﬁrst iteration, we pick the variable x to ﬁx the bounds\nof s2, as it is the ﬁrst one in our ordering. Note that x is unbounded (i.e., its\nbounds are −∞and ∞), so it easily satisﬁes the conditions. To increase the\ninterpretation of s2 to 2, and satisfy its lower bound, we can decrease I0(x) to\n−1, resulting in the following satisfying assignment:\nI1 = {x 7→−1, y 7→0, s1 7→−1, s2 7→2, s3 7→10}\n(Revisit Figure 7.1 for an illustration.) We now pivot s2 and x, producing the\nCHAPTER 7. NEURAL THEORY SOLVERS\n75\nfollowing set of equalities (the bounds always remain the same):\nx = 0.5y −0.5s2\ns1 = 1.5y −0.5s2\ns3 = −4y + 5s2\nSecond iteration The only basic variable not satisfying its bounds is now s1,\nsince I1(s1) = −1 < 0. The ﬁrst non-basic variable that we can tweak is y.\nWe can increase the value of I(y) by 1/1.5 = 2/3, resulting in the following\ninterpretation:\nI2 = {x 7→−2/3, y 7→2/3, s1 7→0, s2 7→2, s3 7→7/3}\nAt this point, we pivot y with s1.\nThird iteration Simplex terminates since I2 |= F.\n■\nWhy is Simplex Correct?\nFirst, you may wonder, why does Simplex terminate? The answer is due to the\nfact that we order variables and always look for the ﬁrst variable violating bounds.\nThis is known as Bland’s rule (Bland, 1977). Bland’s rule ensures that we never\nrevisit the same set of basic and non-basic variables.\nSecond, you may wonder, is Simplex actually correct? If Simplex returns an\ninterpretation I, it is easy to see that I |= F, since Simplex checks that condition\nbefore it terminates. But what about the case when it says UNSAT? To illustrate\ncorrectness in this setting, we will look at an example.\nCHAPTER 7. NEURAL THEORY SOLVERS\n76\nExample 7.E Consider the following formula in Simplex form:\ns1 = x + y\ns2 = −x −2y\ns3 = −x + y\ns1 ⩾0\ns2 ⩾2\ns3 ⩾1\nThis formula is UNSAT—use your favorite SMT solver to check this. Imagine an\nexecution of Simplex that performs the following two pivot operations: (1) s1 with\nx and (2) s2 with y.\nThe ﬁrst pivot results in the following formula:\nx = s1 −y\ns2 = −s1 −y\ns3 = −s1 + 2y\nThe second pivot results in the following formula:\nx = 2s1 + s2\ny = −s2 −s1\ns3 = −3s1 −2ss\nThe algorithm maintains the invariant that all non-basic variables satisfy their\nbounds. So we have s1 ⩾0 and s2 ⩾2. Say s3 violates its bound, i.e.,\n−3s1 −2s2 < 1\nThe only way to ﬁx this is by decreasing the interpretations of s1 and s2. But even\nif we assign s1 and s2 the values 0 and 2 (their lower bounds), respectively, we\ncannot make s3 ⩾1. Contradiction! So Simplex ﬁgures out the formula is UNSAT.\nThe conditions for choosing variable xj in Algorithm 3 encode this argument.\n■\nAs the above example illustrates, we can think of Simplex as constructing a\nproof by contradiction to prove that a set of linear inequalities is unsatisﬁable.\nCHAPTER 7. NEURAL THEORY SOLVERS\n77\n7.3 The Reluplex Algorithm\nUsing the Simplex algorithm as the theory solver within DPLLT allows us to solve\nformulas in LRA. So, at this point in our development, we know how to algo-\nrithmically reason about neural networks with piecewise-linear activations, like\nReLUs. Unfortunately, this approach has been shown to not scale to large net-\nworks. One of the reasons is that ReLUs are encoded as disjunctions, as we saw in\nChapter 5. This means that the SAT-solving part of DPLLT will handle the disjunc-\ntions, and may end up considering every possible case of the disjunction—ReLU\nbeing active (output = input) or inactive (output = 0)—leading to many calls to\nSimplex, exponential in the number of ReLUs.\nTo ﬁx those issues, the work of Katz et al. (2017) developed an extension of\nSimplex, called Reluplex, that natively handles ReLU constraints in addition to\nlinear inequalities. The key idea is to try to delay case splitting on ReLUs. In the\nworst case, Reluplex may end up with an exponential explosion, just like DPLLT\nwith Simplex, but empirically it has been shown to be a promising approach for\nscaling SMT solving to larger neural networks. In what follows, we present the\nReluplex algorithm.\nReluplex Form\nJust like with Simplex, Reluplex expects formulas to be in a certain form. We\nwill call this form Reluplex form, where formulas contain (1) equalities (same as\nSimplex), (2) bounds (same as Simplex), and (3) ReLU constraints of the form\nxi = relu(xj)\nGiven a conjunction of inequalities and ReLU constraints, we can translate them\ninto Reluplex form by translating the inequalities into Simplex form. Additionally,\nfor each ReLU constraint xi = relu(xj), we can add the bound xi ⩾0, which is\nimplied by the deﬁnition of a ReLU.\nCHAPTER 7. NEURAL THEORY SOLVERS\n78\nExample 7.F Consider the following formula:\nx + y ⩾2\ny = relu(x)\nWe translate it into the following Reluplex form:\ns1 = x + y\ny = relu(x)\ns1 ⩾2\ny ⩾0\n■\nReluplex in Detail\nWe now present the Reluplex algorithm. The original presentation by Katz et al.\n(2017) is a set of rules that can be applied non-deterministically to arrive at an\nanswer. Here, we present a speciﬁc schedule of the Reluplex algorithm.\nThe key idea of Reluplex is to call Simplex on equalities and bounds, and then\ntry to massage the interpretation returned by Simplex to satisfy all ReLU con-\nstraints. Reluplex is shown in Algorithm 4.\nInitially, Simplex is invoked on the formula F′, which is the original formula F\nbut without the ReLU constraints. If Simplex returns UNSAT, then we know that F\nis UNSAT—this is because F ⇒F′ is valid. Otherwise, if Simplex returns a model\nI |= F′, it may not be the case that I |= F, since F′ is a weaker (less constrained)\nformula.\nIf I ̸|= F, then we know that one of the ReLU constraints is not satisﬁed. We\npick one of the violated ReLU constraints xi = relu(xj) and modify I to make sure\nit is not violated. Note that if any of xi and xj is a basic variable, we pivot it with\na non-basic variable. This is because we want to modify the interpretation of one\nof xi or xj, which may affect the interpretation of the other variable if it is a basic\nCHAPTER 7. NEURAL THEORY SOLVERS\n79\nAlgorithm 4: Reluplex\nData: A formula F in Reluplex form\nResult: I |= F or UNSAT\nLet I be the interpretation that sets all variables fv(F) to 0\nLet F′ be the non-ReLU constraints of F\nwhile true do\n▷Calling Simplex (note that we supply Simplex with a reference to the\ninitial interpretation and it can modify it)\nr ←Simplex(F′, I)\nIf r is UNSAT then return UNSAT\nr is an interpretation I\nif I |= F then return I\n▷Handle violated ReLU constraint\nLet ReLU constraint xi = relu(xj) be s.t. I(xi) ̸= relu(I(xj))\nif xi is basic then\npivot xi with non-basic variable xk, where k ̸= j and cik ̸= 0\nif xj is basic then\npivot xj with non-basic variable xk, where k ̸= i and cjk ̸= 0\nPerform one of the following operations:\nI(xi) ←relu(I(xj)) or\nI(xj) ←I(xi)\n▷Case splitting (ensures termination)\nif uj > 0, lj < 0, and xi = relu(xj) considered more than τ times then\nr1 ←Reluplex(F ∧xj ⩾0 ∧xi = xj)\nr2 ←Reluplex(F ∧xj ⩽0 ∧xi = 0)\nif r1 = r2 = UNSAT then return UNSAT\nif r1 ̸= UNSAT then return r1\nreturn r2\nvariable and cij ̸= 0.3 Finally, we modify the interpretation of xi or xj, ensuring\nthat I |= xi = relu(xj). Note that the choice of xi or xj is up to the implementation.\nThe problem is that ﬁxing a ReLU constraint may end up violating a bound,\n3These conditions are not explicit in Katz et al. (2017), but their absence may lead to wasted\niterations (or Update rules in Katz et al. (2017)) that do not ﬁx violations of ReLU constraints.\nCHAPTER 7. NEURAL THEORY SOLVERS\n80\nand so Simplex need be invoked again. We assume that the interpretation I in\nReluplex is the same one that is modiﬁed by invocations of Simplex.\nCase Splitting\nIf we simply apply Reluplex without the last piece of the algorithm—case splitting—\nit may not terminate. Speciﬁcally, it may get into a loop where Simplex satisﬁes\nall bounds but violates a ReLU, and then satisfying the ReLU causes a bound to\nbe violated, and on and on.\nThe last piece of Reluplex checks if we are getting into an inﬁnite loop, by\nensuring that we do not attempt to ﬁx a ReLU constraint more than τ times,\nsome ﬁxed threshold. If this threshold is exceeded, then the ReLU constraint\nxi = relu(xj) is split into its two cases:\nF1 ≜xj ⩾0 ∧xi = xj\nand\nF2 ≜xj ⩽0 ∧xi = 0\nReluplex is invoked recursively on two instances of the problem, F ∧F1 and F ∧F2.\nIf both instances are UNSAT, then the formula F is UNSAT. If any of the instances\nis SAT, then F is SAT. This is due to the fact that\nF ≡(F ∧F1) ∨(F ∧F2)\nLooking Ahead\nWe’re done with constraint-based veriﬁcation. In the next part of the book, we\nwill look at different approaches that are more efﬁcient at the expense of failing to\nprovide proofs in some cases.\nThere are a number of interesting problems that we didn’t cover. A critical one\nis soundness with respect to machine arithmetic. Our discussion has assumed\nreal-valued variables, but, of course, that’s not the case in the real world—we use\nmachine arithmetic. A recent paper has shown that veriﬁed neural networks in\nCHAPTER 7. NEURAL THEORY SOLVERS\n81\nLRA may not really be robust when one considers the bit-level behavior (Jia and\nRinard, 2020b).\nAnother issue is scalability of the analysis. Using arbitrary-precision rational\nnumbers can be very expensive, as the size of the numerators and denominators\ncan blow up due to pivot operations. Reluplex (Katz et al., 2017) ends up us-\ning ﬂoating-point approximations, and carefully ensures the results are sound by\nkeeping track of round-off errors.\nAt the time of writing, constraint-based veriﬁcation approaches have only\nmanaged to scale to neural networks with around a hundred thousand ReLUs (Tjeng\net al., 2019a), which is small compared to state-of-the-art neural networks. This is\nstill a triumph, as the veriﬁcation problem is NP-hard (Katz et al., 2017). It is, how-\never, unclear how much further we can push this technology. Scaling constraint-\nbased veriﬁcation further requires progress along two fronts: (1) developing and\ntraining neural networks that are friendlier to veriﬁcation (less ReLUs is better),\nand (2) advancing the algorithms underlying SMT solvers and MILP solvers.\nPart III\nAbstraction-Based Veriﬁcation\n83\nChapter 8\nNeural Interval Abstraction\nIn the previous part of the book, we described how to precisely capture the seman-\ntics of a neural network by encoding it, along with a correctness property, as a for-\nmula in ﬁrst-order logic. Typically, this means that we’re solving an NP-complete\nproblem, like satisﬁability modulo linear real arithmetic (equivalently, mixed in-\nteger linear programming). While we have fantastic algorithms and tools that\nsurprisingly work well for such hard problems, scaling to large neural networks\nremains an issue.\nIn this part of the book, we will look at approximate techniques for neural-\nnetwork veriﬁcation. By approximate, we mean that they overapproximate—or\nabstract—the semantics of a neural-network, and therefore can produce proofs of\ncorrectness, but when they fail, we do not know whether a correctness property\nholds or not. The approach we use is based on abstract interpretation (Cousot and\nCousot, 1977), a well-studied framework for deﬁning program analyses. Abstract\ninterpretation is a very rich theory, and the math can easily make you want to\nquit computer science and live a monastic life in the woods, away from anything\nthat can be considered technology. But fear not, it is a very simple idea, and we\nwill take a pragmatic approach here in deﬁning it and using it for neural-network\nveriﬁcation.\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n84\n8.1 Set Semantics and Veriﬁcation\nLet’s focus on the following correctness property, deﬁning robustness of a neural\nnetwork f : Rn →Rm on an input grayscale image c whose classiﬁcation label is\n1.\n{ |x −c| ⩽0.1 }\nr ←f (x)\n{ class(r) = 1 }\nConcretely, this property makes the following statement: Pick any image x that\nis like c but is slightly brighter or darker by at most 0.1 per pixel, assuming each\npixel is some real number encoding its brightness. Now, execute the network on\nx. The network must predict that x is of class 1.\nThe issue in checking such statement is that there are inﬁnitely many possible\nimages x. Even if there are ﬁnitely many images—because, at the end of the day,\nwe’re using bits—the number is still enormous, and we cannot conceivably run\nall those images through the network and ensure that each and every one of them\nis assigned class 1. But let’s just, for the sake of argument, imagine that we can lift\nthe function f to work over sets of images. That is, we will deﬁne a version of f of\nthe form:\nf s : P(Rn) →P(Rm)\nwhere P(S) is the powerset of set S. Speciﬁcally,\nf s(X) = {y | x ∈X, y = f (x)}\nArmed with f s, we can run it with the following input set:\nX = {x | |x −c| ⩽0.1}\nwhich is the set of all images x deﬁned above in the precondition of our correct-\nness property. By computing f s(X), we get the predictions of the neural network\nf for all images in X. To verify our property, we simply check that\nf s(X) ⊆{y | class(y) = 1}\nIn other words, all runs of f on every image x ∈X result in the network predicting\nclass 1.\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n85\nThe above discussion may sound like crazy talk: we cannot simply take a neu-\nral network f and generate a version f s that takes an inﬁnite set of images. In this\nchapter, we will see that we actually can, but we will often have to lose precision:\nwe will deﬁne an abstract version of our theoretical f s that may return more an-\nswers. The trick is to deﬁne inﬁnite sets of inputs using data structures that we\ncan manipulate, called abstract domains.\nIn this chapter, we will meet the interval abstract domain. We will focus our\nattention on the problem of executing the neural network on an inﬁnite set. Later,\nin Chapter 11, we come back to the veriﬁcation problem.\n8.2 The Interval Domain\nLet’s begin by considering a very simple function\nf (x) = x + 1\nI would like to evaluate this function on a set of inputs X; that is, I would like to\nsomehow evaluate\nf s(X) = {x + 1 | x ∈X}\nWe call f s the concrete transformer of f.\nAbstract interpretation simpliﬁes this problem by only considering sets X that\nhave a nice form. Speciﬁcally, the interval abstract domain considers an interval of\nreal numbers written as [l, u], where l, u ∈R and l ⩽u. An interval [l, u] denotes\nthe potentially inﬁnite set\n{x | l ⩽x ⩽u}\nSo we can now deﬁne a version of our function f s that operates over an interval,\nas follows:\nf a([l, u]) = [l + 1, u + 1]\nWe call f a an abstract transformer of f. In other words, f a takes a set of real numbers\nand returns a set of real numbers, but the sets are restricted to those that can be\ndeﬁned as intervals. Observe how we can mechanically evaluate this abstract\ntransformer on an arbitrary interval [l, u]: add 1 to l and add 1 to u, arriving at\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n86\nl\nu\nl + 1\nu + 1\nFigure 8.1 Illustration of an abstract transformer of f (x) = x + 1.\nthe interval [l + 1, u + 1]. Geometrically, if we have an interval on the number line\nfrom l to u, and we add 1 to every point in this interval, then the whole interval\nshifts to the right by 1. This is illustrated in Figure 8.1. Note that the interval\n[l, u] is an inﬁnite set (assuming l < u), and so f a adds 1 to an inﬁnite set of real\nnumbers!\nExample 8.A Continuing our example,\nf a([0, 10]) = [1, 11]\nIf we pass a singleton interval, e.g., [1, 1], we get f a([1, 1]) = [2, 2]—exactly the\nbehavior of f.\n■\nGenerally, we will use the notation ([l1, u1], . . . , [ln, un]) to denote an n-dimensional\ninterval, or a hyperrectangular region in Rn, i.e., the set of all n-ary vectors\n{x ∈Rn | li ⩽xi ⩽ui}\nSoundness\nWhenever we design an abstract transformer f a, we need to ensure that it is a\nsound approximation of f s. This means that its output is a superset of that of the\nconcrete transformer, f s. The reason is that we will be using f a for veriﬁcation,\nso to declare that a property holds, we cannot afford to miss any behavior of the\nneural network.\nFormally, we deﬁne soundness as follows: For any interval [l, u], we have\nf s([l, u]) ⊆f a([l, u])\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n87\nEquivalently, we can say that for any x ∈[l, u], we have\nf (x) ∈f a([l, u])\nIn practice, we will often ﬁnd that\nf s([l, u]) ⊂f a([l, u])\nfor many functions and intervals of interest. This is expected, as our goal is to\ndesign abstract transformers that are easy to evaluate, and so we will often lose\nprecision, meaning overapproximate the results of f s. We will see some simple\nexamples shortly.\nThe Interval Domain is Non-relational\nThe interval domain is non-relational, meaning that it cannot capture the relations\nbetween different dimensions. We illustrate this fact with an example.\nExample 8.B Consider the set\nX = {(x, x) | 0 ⩽x ⩽1}\nWe cannot represent this set precisely in the interval domain. The best we can do\nis the square between (0, 0) and (1, 1), denoted as the 2-dimensional interval\n([0, 1], [0, 1])\nand illustrated as the gray region below:\nThe set X deﬁnes points where higher values of the x coordinate associate with\nhigher values of the y coordinate. But our abstract domain can only represent\nrectangles whose faces are parallel to the axes. This means that we can’t capture\nthe relation between the two dimensions: we simply say that any value of x in\n[0, 1] can associate with any value of y in [0, 1].\n■\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n88\n8.3 Basic Abstract Transformers\nWe now look at examples of abstract transformers for basic arithmetic operations.\nAddition\nConsider the binary function: f (x, y) = x + y. The concrete transformer f s :\nP(R2) →P(R) is deﬁned as follows:\nf s(X) = {x + y | (x, y) ∈X}\nWe deﬁne f a as a function that takes two intervals, i.e., a rectangle, one represent-\ning the range of values of x1 and the other of x2:\nf a([l, u], [l′, u′]) = [l + l′, u + u′]\nThe deﬁnition looks very much like f, except that we perform addition on the\nlower bounds and the upper bounds of the two input intervals.\nExample 8.C Consider\nf a([1, 5], [100, 200]) = [101, 205]\nThe lower bound, 101, results from adding the lower bounds of x and y (1 + 100);\nthe upper bound, 205, results from adding the upper bounds of x and y (5 + 200).\n■\nIt is simple to prove soundness of our abstract transformer f a. Take any\n(x, y) ∈([l, u], [l′, u′])\nBy deﬁnition, l ⩽x ⩽u and l′ ⩽y ⩽u′. So we have\nl + l′ ⩽x + y ⩽u + u′\nBy deﬁnition of an interval, we have\nx + y ∈[l + l′, u + u′]\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n89\nMultiplication\nMultiplication is a bit trickier. The reason is that the signs might ﬂip, making the\nlower bound an upper bound. So we have to be a bit more careful.\nLet f (x, y) = x ∗y. If we only consider positive inputs, then we can deﬁne f a\njust like we did for addition:\nf a([l, u], [l′, u′]) = [l ∗l′, u ∗u′]\nBut consider\nf a([−1, 1], [−3, −2]) = [3, −2]\nWe’re in trouble: [3, −2] is not even an interval as per our deﬁnition—the upper\nbound is less than the lower bound!\nTo ﬁx this issue, we need to consider every possible combination of lower and\nupper bounds as follows:\nf a([l, u], [l′, u′]) = [min(B), max(B)]\nwhere\nB = {l ∗l′, l ∗u′, u ∗l′, u ∗u′}\nExample 8.D Consider the following abstract multiplication of two intervals:\nf a([−1, 1], [−3, −2]) = [min(B), max(B)]\n= [−3, 3]\nwhere B = {3, 2, −3, −2}.\n■\n8.4 General Abstract Transformers\nWe will now deﬁne general abstract transformers for classes of operations that\ncommonly appear in neural networks.\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n90\nAﬃne Functions\nFor an afﬁne function\nf (x1, . . . , xn) = ∑\ni\ncixi\nwhere ci ∈R, we can deﬁne the abstract transformer as follows:\nf a([l1, u1], . . . , [ln, un]) =\n\"\n∑\ni\nl′\ni, ∑\ni\nu′\ni\n#\nwhere l′\ni = min(cili, ciui) and u′\ni = max(cili, ciui).\nNotice that the deﬁnition looks pretty much like addition: sum up the lower\nbounds and the upper bounds. The difference is that we also have to consider the\ncoefﬁcients, ci, which may result in ﬂipping an interval’s bounds when ci < 0.\nExample 8.E Consider f (x, y) = 3x + 2y. Then,\nf ([5, 10], [20, 30]) = [3 · 5 + 2 · 20, 3 · 10 + 2 · 30]\n= [55, 90]\n■\nMonotonic Functions\nMost activation functions used in neural networks are mono-\ntonically increasing, e.g., ReLU and sigmoid. It turns out that it’s easy to deﬁne\nan abstract transformer for any monotonically increasing function f : R →R, as\nfollows:\nf a([l, u]) = [ f (l), f (u)]\nSimply, we apply f to the lower and upper bounds.\nExample 8.F Figure 8.2 illustrates how to apply ReLU to an interval [3, 5]. The\nshaded region shows that any value y in the interval [3, 5] results in a value\nrelu(3) ⩽relu(y) ⩽relu(5)\nthat is, a value in the interval [relu(3), relu(5)].\n■\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n91\nl = 3\nu = 5\nrelu(l) = 3\nrelu(u) = 5\nx\nrelu(x)\nFigure 8.2 ReLU function over an interval of inputs [l, u]\nComposing Abstract Transformers\nSay we have a function composition f ◦g—this notation means ( f ◦g)(x) =\nf (g(x)). We don’t have to deﬁne an abstract transformer for the composition:\nwe can simply compose the two abstract transformers of f and g, as f a ◦ga, and\nthis will be a sound abstract transformer of f ◦g.\nComposition is very important, as neural networks are a composition of many\noperations.\nExample 8.G Let\ng(x) = 3x\nf (x) = relu(x)\nh(x) = f (g(x))\nThe function h represents a very simple neural network, one that applies an\nafﬁne function followed by a ReLU on an input in R.\nWe deﬁne\nha([l, u]) = f a(ga([l, u]))\nwhere f a and ga are as deﬁned earlier for monotonic functions and afﬁne func-\ntions, respectively. For example, on the input interval [2, 3], we have\nha([2, 3]) = f a(ga([2, 3]))\n= f a([6, 9])\n= [6, 9]\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n92\n■\n8.5 Abstractly Interpreting Neural Networks\nWe have seen how to construct abstract transformers for a range of functions and\nhow to compose abstract transformers. We now direct our attention to construct-\ning an abstract transformer for a neural network.\nRecall that a neural network is deﬁned as a graph G = (V, E), giving rise to\na function fG : Rn →Rm, where n = |Vin| and m = |Vo|. Recall that Vin are\ninput nodes and Vo are output nodes of G. We would like to construct an abstract\ntransformer f a\nG that takes n intervals and outputs m intervals.\nWe deﬁne f a\nG([l1, u1], . . . , [ln, un]) as follows:\n• First, for every input node vi, we deﬁne\nouta(vi) = [li, ui]\nRecall that we assume a ﬁxed ordering of nodes.\n• Second, for every non-input node v, we deﬁne\nouta(v) = f a\nv(outa(v1), . . . , outa(vk))\nwhere f a\nv is the abstract transformer of fv, and v has the incoming edges\n(v1, v), . . . , (vk, v),\n• Finally, the output of f a\nG is the set of intervals outa(v1), . . . , outa(vm), where\nv1, . . . , vm are the output nodes.\nExample 8.H Consider the following simple neural network G:\nv1\nv2\nv3\nv4\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n93\nAssume that fv3(x) = 2x1 + x2 and fv4(x) = relu(x).\nSay we want to evaluate f a\nG([0, 1], [2, 3]). We can do this as follows, where f a\nv3\nand f a\nv4 follow the deﬁnitions we discussed above for afﬁne and monotonically\nincreasing functions, respectively.\nouta(v1) = [0, 1]\nouta(v2) = [2, 3]\nouta(v3) = [2 ∗0 + 2, 2 ∗1 + 3] = [2, 5]\nouta(v4) = [relu(2), relu(5)] = [2, 5]\nIt’s nice to see the outputs of every node written on the edges of the graph as\nfollows:\nv1\nv2\nv3\nv4\n[0, 1]\n[2, 3]\n[2, 5]\n[2, 5]\n■\nLimitations of the Interval Domain\nThe interval domain, as described, seems infallible. We will now see how it can,\nand often does, overshoot: compute wildly overapproximating solutions. The\nprimary reason for this is that the interval domain is non-relational, meaning it\ncannot keep track of relations between different values, e.g., the inputs and out-\nputs of a function.\nExample 8.I Consider the following, admittedly bizarre, neural network:\nv1\nv2\nv3\n[0, 1]\n[0, 1]\n[−1, 0]\n[−1, 1]\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n94\nwhere\nfv2(x) = −x\nfv3(x) = x1 + x2\nClearly, for any input x, fG(x) = 0. Therefore, ideally, we can deﬁne our abstract\ntransformer simply as f a\nG([l, u]) = [0, 0] for any interval [l, u].\nUnfortunately, if we follow the recipe above, we get a much bigger interval\nthan [0, 0]. For example, on the input [0, 1], f a\nG returns [−1, 1], as illustrated on\nthe graph above. The reason this happens is because the output node, v3, receives\ntwo intervals as input, not knowing that one is the negation of the other. In other\nwords, it doesn’t know the relation between, or provenance of, the two intervals.\n■\nExample 8.J\nHere’s another simple network, G, where fv2 and fv3 are ReLUs.\nTherefore, fG(x) = (x, x) for any positive input x.\nv1\nv2\nv3\n[0, 1]\n[0, 1]\n[0, 1]\n[0, 1]\nFollowing our recipe, we have f a\nG([0, 1]) = ([0, 1], [0, 1]). In other words, the\nabstract transformer tells us that, for inputs between 0 and 1, the neural network\ncan output any pair (x, y) where 0 ⩽x, y ⩽1. But that’s too loose an approx-\nimation: we should expect to see only outputs (x, x) where 0 ⩽x ⩽1. Again,\nwe have lost the relation between the two output nodes. They both should return\nthe same number, but the interval domain, and our abstract transformers, are not\nstrong enough to capture that fact.\n■\nCHAPTER 8. NEURAL INTERVAL ABSTRACTION\n95\nLooking Ahead\nWe’ve seen how interval arithmetic can be used to efﬁciently evaluate a neural\nnetwork on a set of inputs, paying the price of efﬁciency with precision. Next, we\nwill see more precise abstract domains.\nThe abstract interpretation framework was introduced by Cousot and Cousot\n(1977) in their seminal paper. Abstract interpretation is a general framework,\nbased on lattice theory, for deﬁning and reasoning about program analyses. In our\nexposition, we avoided the use of lattices, because we do not aim for generality—\nwe just want to analyze neural networks. Nonetheless, the lattice-based formal-\nization allows us to easily construct the most-precise abstract transformers for any\noperation.\nInterval arithmetic is an old idea that predates program analysis, even com-\nputer science: it is a standard tool in the natural sciences for measuring accu-\nmulated measurement errors. For neural-network veriﬁcation, interval arithmetic\nﬁrst appeared in a number of papers starting in 2018 (Gehr et al., 2018; Gowal\net al., 2018; Wang et al., 2018). To implement interval arithmetic for real neural\nnetworks efﬁciently, one needs to employ parallel matrix operations (e.g., using a\nGPU). Intuitively, an operation like matrix addition can be implemented with two\nmatrix additions for interval arithmetic, one for upper bounds and one for lower\nbounds.\nThere are also powerful techniques that employ the interval domain (or any\nmeans to bound the output of various nodes of the network) with search. We did\nnot cover this combination here but I would encourage you to check out FastLin\napproach (Weng et al., 2018) and its successor, CROWN (Zhang et al., 2018a). (Both\nare nicely summarized by Li et al. (2019))\nOne interesting application of the interval domain is as a quick-and-dirty way\nfor speeding up constraint-based veriﬁcation. Tjeng et al. (2019b) propose using\nsomething like the interval domain to bound the interval of values taken by a\nReLU for a range of inputs to the neural network. If the interval of inputs of a\nReLU is above or below 0, then we can replace the ReLU with a linear function,\nf (x) = x or f (x) = 0, respectively. This simpliﬁes the constraints for constraint-\nbased veriﬁcation, as there’s no longer a disjunction.\n96\nChapter 9\nNeural Zonotope Abstraction\nIn the previous chapter, we deﬁned the interval abstract domain, which allows us\nto succinctly capture inﬁnite sets in Rn by deﬁning lower and upper bounds per\ndimension. In R2, an interval deﬁnes a rectangle; in R3, an interval deﬁnes a box;\nin higher dimensions, it deﬁnes hyperrectangles.\nThe issue with the interval domain is that it does not relate the values of var-\nious dimensions—it just bounds each dimension. For example, in R2, we cannot\ncapture the set of points where x = y and 0 ⩽x ⩽1. The best we can do is\nthe square region ([0, 1], [0, 1]). Syntactically speaking, an abstract element in the\ninterval domain is captured by constraints of the form:\n^\ni\nli ⩽xi ⩽ui\nwhere every inequality involves a single variable, and therefore no relationships\nbetween variables are captured. So the interval domain is called non-relational. In\nthis chapter, we will look at a relational abstract domain, the zonotope domain, and\ndiscuss its application to neural networks.\nFigure 9.1 Examples of zonotopes in R2\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n97\n9.1 What the Heck is a Zonotope?\nLet’s begin with deﬁning a 1-dimensional zonotope. We assume we have a set of\nm real-valued generator variables, denoted ϵ1, . . . , ϵm. A 1-dimensional zonotope\nis the set of all points\n(\nc0 +\nm\n∑\ni=1\nci · ϵi\n\f\f\f\f\f ϵi ∈[−1, 1]\n)\nwhere ci ∈R.\nIf you work out a few examples of the above deﬁnition, you’ll notice that a\n1-dimensional zonotope is just a convoluted way of deﬁning an interval. For ex-\nample, if we have one generator variable, ϵ, then a zonotope is the set\n{c0 + c1ϵ | ϵ ∈[−1, 1]}\nwhich is the interval [c0 −c1, c0 + c1], assuming c1 ⩾0. Note that c0 is the center\nof the interval.\nZonotopes start being more expressive than intervals in R2 and beyond. In\nn-dimensions, a zonotope with m generators is the set of all points\n\n\n\n\n\n\n\n\n\n\n\n\n\nc10 +\nm\n∑\ni=1\nc1i · ϵi\n|\n{z\n}\nﬁrst dimension\n. . . , cn0 +\nm\n∑\ni=1\ncni · ϵi\n|\n{z\n}\nnth dimension\n\n\n\n\n\n\f\f\f\f\f\f\f\f\f\nϵi ∈[−1, 1]\n\n\n\n\n\n\n\n\n\nThis is best illustrated through a series of examples in R2.1\nExample 9.A Consider the following two-dimensional zonotope with two genera-\ntors.\n(1 + ϵ1, 2 + ϵ2)\nwhere we drop the set notation for clarity. Notice that in the ﬁrst dimension the\ncoefﬁcient of ϵ2 is 0, and in the second dimension the coefﬁcient of ϵ1 is 0. Since the\ntwo dimensions do not share generators, we get the following box shape whose\ncenter is (1, 2).\n1In the VR edition of the book, I take the reader on a guided 3D journey of zonotopes; since\nyou cheaped out and just downloaded the free pdf, we’ll have to do with R2.\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n98\n1\n2\n1\n2\nObserve that the center of the zonotope is the vector of constant coefﬁcients of\nthe two dimensions, (1, 2), as illustrated below:\n( 1\n|{z} + ϵ1,\n2\n|{z} + ϵ2)\n■\nExample 9.B Now consider the following zonotope with 1 generator:\n(2 + ϵ1, 2 + ϵ1)\nSince the two dimensions share the same expression, this means that two dimen-\nsions are equal, and so we get we get a line shape centered at (2, 2):\n1\n2\n3\n1\n2\n3\nThe reason ϵ1 is called a generator is because we can think of it as a constructor\nof a zonotope. In this example, starting from the center point (2,2), the generator\nϵ1 stretches the point (2,2) to (3,3), by adding (1,1) (the two coefﬁcients of ϵ1) and\nstretches the center to (1,1) by subtracting (1,1). See the following illustration:\n1\n2\n3\n1\n2\n3\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n99\n■\nExample 9.C Now consider the following zonotope with 2 generators,\n(2 + ϵ1, 3 + ϵ1 + ϵ2)\nwhich is visualized as follows, with the center point (2,3) in red.\n1\n2\n3\n4\n1\n2\n3\n4\n5\nLet’s see how this zonotope is generated in two steps, by considering one gen-\nerator at a time. The coefﬁcients of ϵ1 are (1,1), so it stretches the center point (2,3)\nalong the (1,1) vector, generating a line:\n1\n2\n3\n4\n1\n2\n3\n4\n5\nNext, the coefﬁcients of ϵ2 are (0,1), so it stretches all points along the (0, 1)\nvector, resulting in the zonotope we plotted earlier:\n1\n2\n3\n4\n1\n2\n3\n4\n5\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n100\n■\nYou may have deduced by now that adding more generators adds more faces\nto the zonotope. For example, the right-most zonotope in Figure 9.1 uses three\ngenerators to produce the three pairs of parallel faces.\nA Compact Notation\nGoing forward, we will use a compact notation to describe an n-dimensional\nzonotope with m generator variables:\n( \nc10 +\nm\n∑\ni=1\nc1i · ϵi, . . . , cn0 +\nm\n∑\ni=1\ncni · ϵi\n!\f\f\f\f\f ϵi ∈[−1, 1]\n)\nSpeciﬁcally, we will deﬁne it as a tuple of vectors of coefﬁcients:\n(⟨c10, . . . , c1m⟩, . . . , ⟨cn0, . . . , cnm⟩)\nFor an even more compact presentation, will also use\n(⟨c1i⟩i, . . . , ⟨cni⟩i)\nwhere i ranges from 0 to m, the number of generators; we drop the index i when\nit’s clear from context.\nWe can compute the upper bound of the zonotope (the largest possible value)\nin the j dimension by solving the following optimization problem:\nmax cj0 +\nm\n∑\ni=1\ncjiϵi\ns.t. ϵi ∈[−1, 1]\nThis can be easily solved by setting ϵi to 1 if cji > 0 and −1 otherwise.\nSimilarly, we can compute the lower bound of the zonotope in the jth dimen-\nsion by minimizing instead of maximizing, and solving the optimization problem\nby setting ϵi to −1 if cji > 0 and 1 otherwise.\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n101\nExample 9.D Recall our parallelogram from Example 9.C:\n(2 + ϵ1, 3 + ϵ1 + ϵ2)\nIn our compact notation, we write this as\n(⟨2, 1, 0⟩, ⟨3, 1, 1⟩)\nThe upper bound in the vertical dimension, 3 + ϵ1 + ϵ2, is\n3 + 1 + 1 = 5\nwhere ϵ1 and ϵ2 are set to 1.\n■\n9.2 Basic Abstract Transformers\nNow that we have seen zonotopes, let’s deﬁne some abstract transformers over\nzonotopes.\nAddition\nFor addition, f (x, y) = x + y, we will deﬁne the abstract transformer f a that takes\na two-dimensional zonotope deﬁning a set of values of (x, y). We will assume a\nﬁxed number of generators m. So, for addition, its abstract transformer is of the\nform\nf a(⟨c10, . . . , c1m⟩, ⟨c20, . . . , c2m⟩)\nCompare this to the interval domain, where f a([l1, u1], [l2, u2])\nIt turns out that addition over zonotopes is straightforward: we just sum up\nthe coefﬁcients:\nf a(⟨c10, . . . , c1m⟩, ⟨c20, . . . , c2m⟩) = ⟨c10 + c20, . . . , c1m + c2m⟩\nExample 9.E Consider the simple zonotope (0 + ϵ1, 1 + ϵ2). This represents the\nfollowing box:\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n102\n−1\n1\n1\n2\nThe set of possible values we can get by adding the x and y dimensions in\nthis box is the interval between −1 and 3. Following the deﬁnition of the abstract\ntransformer for addition:\nf a(⟨0, 1, 0⟩, ⟨1, 0, 1⟩) = ⟨1, 1, 1⟩\nThat is the output zonotope is the set\n{1 + ϵ1 + ϵ2 | ϵ1, ϵ2 ∈[−1, 1]}\nwhich is the interval [−1, 3].\n■\nAﬃne Functions\nFor an afﬁne function\nf (x1, . . . , xn) = ∑\nj\najxj\nwhere aj ∈R, we can deﬁne the abstract transformer as follows:\nf a(⟨c1i⟩, . . . ⟨cni⟩) =\n*\n∑\nj\najcj0, . . . ,∑\nj\najcjm\n+\nIntuitively, we apply f to the center point and coefﬁcients of ϵ1, ϵ2, etc.\nExample 9.F Consider f (x, y) = 3x + 2y. Then,\nf a(⟨1, 2, 3⟩, ⟨0, 1, 1⟩) = ⟨f (1, 0), f (2, 1), f (3, 1)⟩\n= ⟨3, 8, 11⟩\n■\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n103\n9.3 Abstract Transformers of Activation Functions\nWe now discuss how to construct an abstract transformer for the ReLU activation\nfunction.\nLimitations of the Interval Domain\nLet’s ﬁrst recall the interval abstract transformer of ReLU:\nrelua([l, u]) = [relu(l), relu(u)]\nThe issue with the interval domain is we don’t know how points in the output\ninterval relua([l, u]) relate to the input interval [l, u]—i.e., which inputs are re-\nsponsible for which outputs.\nGeometrically, we think of the interval domain as approximating the ReLU\nfunction with a box as follows:\nu\nl\nrelu(u)\nu\nl\nrelu(u)\nrelu(l)\nThe ﬁgure on the left shows the case where the lower bound is negative and the\nupper bound is positive; the right ﬁgure shows the case where the lower bound is\npositive.\nA Zonotope Transformer for ReLU\nLet’s slowly build the ReLU abstract transformer for zonotopes. We’re given a\n1-dimensional zonotope ⟨ci⟩i as input. We will use u to denote the upper bound\nof the zonotope and l the lower bound.\nrelua(⟨ci⟩i) =\n\n\n\n\n\n\n\n⟨ci⟩i\nfor l ⩾0\n⟨0⟩i\nfor u ⩽0\n?\notherwise\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n104\nIf l ⩾0, then we simply return the input zonotope back; if u ⩽0, then the\nanswer is 0; when the zonotope has both negative and positive values, there are\nmany ways to deﬁne the output, and so I’ve left it as a question mark. The easy\napproach is to simply return the interval [l, u] encoded as a zonotope. But it turns\nout that we can do better: since zonotopes allow us to relate inputs and outputs,\nwe can shear a box into a parallelogram that ﬁts the shape of ReLU more tightly,\nas follows:\nu\nl\nrelu(u)\nu\nl\nrelu(u)\nThe approximation on the right has a smaller area than the approximation af-\nforded by the interval domain on the left. The idea is that a smaller area results in\na better approximation, albeit an incomparable one, as the parallelogram returns\nnegative values, while the box doesn’t. Let’s try to describe this parallelogram as\na zonotope.\nThe bottom face of the zonotope is the line\ny = λx\nfor some slope λ. It follows that the top face must be\ny = λx + u(1 −λ)\nIf we set λ = 0, we get two horizontal faces, i.e., the interval approximation shown\nabove. The higher we crank up λ, the tighter the parallelogram gets. But, we can’t\nincrease λ past u/(u −l); this ensures that the parallelogram covers the ReLU\nalong the input range [l, u]. So, we will set\nλ =\nu\nu −l\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n105\nIt follows that the distance between the top and bottom faces of the parallelo-\ngram is u(1 −λ),. Therefore, the center of the zonotope (in the vertical axis) must\nbe the point\nη = u(1 −λ)\n2\nWith this information, we can complete the deﬁnition of relua as follows:\nrelua(⟨c1, . . . , cm⟩) =\n\n\n\n\n\n\n\n⟨ci⟩i,\nfor l ⩾0\n⟨0⟩i,\nfor u ⩽0\n⟨λc1, . . . , λcm, 0⟩+ ⟨η, 0, 0, . . . , η⟩\notherwise\nThere are two non-trivial things we do here:\n• First, we add a new generator, ϵm+1, in order to stretch the parallelogram in\nthe vertical axis; its coefﬁcient is η, which is half the hight of the parallelo-\ngram.\n• Second, we add the input zonotope scaled by λ with coefﬁcient 0 for the new\ngenerator; this ensures that we capture the relation between the input and\noutput.\nLet’s look at an example for clarity:\nExample 9.G Say we invoke relua with the interval between l = −1 and u = 1, i.e.,\nrelua(⟨0, 1⟩)\nHere, λ = 0.5 and η = 0.25. So the result of relua is the following zonotope:\n⟨0, 0.5, 0⟩+ ⟨0.25, 0, 0.25⟩= ⟨0.25, 0.5, 0.25⟩\nThe 2-dimensional zonotope composed of the input and output zonotopes of relua\nis\n(⟨0, 1, 0⟩, ⟨0.25, 0.5, 0.25⟩)\nor, explicitly,\n(0 + ϵ1, 0.25 + 0.5ϵ1 + 0.25ϵ2)\nThis zonotope, centered at (0, 0.25), is illustrated below:\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n106\n1\n−1\n1\nη = 0.25\n■\nOther Abstract Transformers\nWe saw how to design an abstract transformer for ReLU. We can follow a similar\napproach to design abstract transformers for sigmoid. It is indeed a good exercise\nto spend some time designing a zonotope transformer for sigmoid or tanh—and\ndon’t look at the literature (Singh et al., 2018)!\nIt is interesting to note that as the abstract domain gets richer—allowing cra-\nzier and crazier shapes—the more incomparable abstract transformers you can\nderive (Sharma et al., 2014). With the interval abstract domain, which is the sim-\nplest you can go without being trivial, the best you can do is a box to approximate\na ReLU or a sigmoid. But with zonotopes, there are inﬁnitely many shapes that\nyou can come up with. So designing abstract transformers becomes an art, and\nit’s hard to predict which transformers will do well in practice.\n9.4 Abstractly Interpreting Neural Networks with Zonotopes\nWe can now use our zonotope abstract transformers to abstractly interpret an en-\ntire neural network in precisely the same way we did intervals. We review the\nprocess here for completeness.\nRecall that a neural network is deﬁned as a graph G = (V, E), giving rise to\na function fG : Rn →Rm, where n = |Vin| and m = |Vo|. We would like to\nconstruct an abstract transformer f a\nG that takes an n-dimensional zonotope and\noutputs an m-dimensional zonotope.\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n107\nWe deﬁne f a\nG(⟨c1j⟩, . . . ⟨cnj⟩) as follows:\n• First, for every input node vi, we deﬁne\nouta(vi) = ⟨cij⟩j\nRecall that we assume a ﬁxed ordering of nodes.\n• Second, for every non-input node v, we deﬁne\nouta(v) = f a\nv(outa(v1), . . . , outa(vk))\nwhere f a\nv is the abstract transformer of fv, and v has the incoming edges\n(v1, v), . . . , (vk, v),\n• Finally, the output of f a\nG is the m-dimensional zonotope\n(outa(v1), . . . , outa(vm))\nwhere v1, . . . , vm are the output nodes.\nOne thing to note is that some abstract transformers (for activation functions)\nadd new generators. We can assume that all of these generators are already in the\ninput zonotope but with coefﬁcients set to 0, and they only get non-zero coefﬁ-\ncients in the outputs of activation function nodes.\nExample 9.H Consider the following neural network, which we saw in the last\nchapter,\nv1\nv2\nv3\nwhere\nfv2(x) = −x\nfv3(x) = x1 + x2\nCHAPTER 9. NEURAL ZONOTOPE ABSTRACTION\n108\nClearly, for any input x, fG(x) = 0. Consider any input zonotope ⟨ci⟩. The output\nnode, v3, receives the two-dimensional zonotope\n(⟨−ci⟩, ⟨ci⟩)\nThe two dimensions cancel each other out, resulting in the zonotope ⟨0⟩, which is\nthe singleton set {0}.\nIn contrast, with the interval domain, given input interval [0, 1], you get the\noutput interval [−1, 1].\n■\nLooking Ahead\nWe’ve seen the zonotope domain, an efﬁcient extension beyond simple interval\narithmetic. Next, we will look at full-blown polyhedra.\nTo my knowledge, the zonotope domain was ﬁrst introduced by Girard (2005)\nin the context of hybrid-system model checking. In the context of neural-network\nveriﬁcation, Gehr et al. (2018) were the ﬁrst to use zonotopes, and introduced\nprecise abstract transformers (Singh et al., 2018), one of which we covered here.\nIn practice, we try to limit the number of generators to keep veriﬁcation fast. This\ncan be done by occasionally projecting out some of the generators heuristically as\nwe’re abstractly interpreting the neural network.\nA standard optimization in program analysis is to combine program opera-\ntions and construct more precise abstract transformers for the combination. This\nallows us to extract more relational information. In the context of neural networks,\nthis amounts to combining activation functions in a layer of the network. Singh\net al. (2019a) showed how to elegantly do this for zonotopes.\n109\nChapter 10\nNeural Polyhedron Abstraction\nIn the previous chapter, we saw the zonotope abstract domain, which is more\nexpressive than the interval domain. Speciﬁcally, instead of approximating func-\ntions using a hyperrectangle, the zonotope domain allows us to approximate func-\ntions using a zonotope, e.g., a parallelogram, capturing relations between differ-\nent dimensions.\nIn this section, we look at an even more expressive abstract domain, the poly-\nhedron domain. Unlike the zonotope domain, the polyhedron domain allows us\nto approximate functions using arbitrary convex polyhedra. A polyhedron in Rn\nis a region made of straight (as opposed to curved) faces; a convex shape is one\nwhere the line between any two points in the shape is completely contained in the\nshape. Convex polyhedra can be speciﬁed as a set of linear inequalities. Using\nconvex polyhedra, we approximate a ReLU as follows:\nu\nl\nrelu(u)\nThis is the smallest convex polyhedron that approximates ReLU. You can vi-\nsually check that it is convex. This approximation is clearly more precise than\nthat afforded by the interval and zonotope domains, as it is fully contained in the\napproximations of ReLU in those domains:\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n110\nu\nl\nrelu(u)\nu\nl\nrelu(u)\n10.1 Convex Polyhedra\nWe will deﬁne a polyhedron in a manner analogous to a zonotope, using a set\nof m generator variables, ϵ1, . . . , ϵm. With zonotopes the generators are bounded\nin the interval [−1, 1]; with polyhedra, generators are bounded by a set of linear\ninequalities.\nLet’s ﬁrst revisit and generalize the deﬁnition of a zonotope. A zonotope in Rn\nis a set of points deﬁned as follows:\n( \nc10 +\nm\n∑\ni=1\nc1i · ϵi, . . . , cn0 +\nm\n∑\ni=1\ncni · ϵi\n!\f\f\f\f\f F(ϵ1, . . . , ϵm)\n)\nwhere F is a Boolean function that evaluates to true iff all of its arguments are\nbetween −1 and 1.\nWith polyhedra, we will deﬁne F as a set (conjunction) of linear inequalities\nover the generator variables, e.g.,\n0 ⩽ϵ1 ⩽5 ∧ϵ1 = ϵ2\n(equalities are deﬁned as two inequalities). We will always assume that F deﬁnes\na bounded polyhedron, i.e., gives a lower and upper bound for each generator;\ne.g., ϵ1 ⩽0 is not allowed, because it does not enforce a lower bound on ϵ1.\nIn the 1-dimensional case, a polyhedron is simply an interval. Let’s look at\nhigher dimensional examples:\nExample 10.A Consider the following 2-dimensional polyhedron:\n{(ϵ1, ϵ2) | F(ϵ1, ϵ2)}\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n111\nwhere\nF ≡0 ⩽ϵ1 ⩽1 ∧ϵ2 ⩽ϵ1 ∧ϵ2 ⩾0\nThis polyhedron is illustrated as follows:\nClearly, this shape is not a zonotope, because its faces are not parallel.\n■\nExample 10.B In 3 dimensions, a polyhedron may look something like this1\nOne can add more faces by adding more linear inequalities to F.\n■\nFrom now on, given a polyhedron\n( \nc10 +\nm\n∑\ni=1\nc1i · ϵi, . . . , cn0 +\nm\n∑\ni=1\ncni · ϵi\n!\f\f\f\f\f F(ϵ1, . . . , ϵm)\n)\nwe will abbreviate it as the tuple:\n(⟨c1i⟩i, . . . ⟨cni⟩i, F)\n1Adapted from Westburg (2017).\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n112\n10.2 Computing Upper and Lower Bounds\nGiven a polyhedron (⟨c1i⟩i, . . . ⟨cni⟩i, F), we will often want to compute the lower\nand upper bounds of one of the dimensions. Unlike with the interval and zono-\ntope domains, this process is not straightforward. Speciﬁcally, it involves solving\na linear program, which takes polynomial time in the number of variables and\nconstraints.\nTo compute the lower bound of the jth dimension, we solve the following lin-\near programming problem:\nmin cj0 +\nm\n∑\ni=1\ncjiϵi\ns.t. F\nSimilarly, we compute the upper bound of the jth dimension by maximizing in-\nstead of minimizing.\nExample 10.C Take our triangle shape from Example 10.A, deﬁned using two gen-\nerators:\n(⟨0, 1, 0⟩, ⟨0, 0, 1⟩, F)\nwhere\nF ≡0 ⩽ϵ1 ⩽1 ∧ϵ2 ⩽ϵ1 ∧ϵ2 ⩾0\nTo compute the upper bound of ﬁrst dimension, we solve\nmax ϵ1\ns.t. F\nThe answer here is 1, which is obvious from the constraints.\n■\n10.3 Abstract Transformers for Polyhedra\nWe’re now ready to go over some abstract transformers for polyhedra.\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n113\nAﬃne Functions\nFor afﬁne functions, it is really the same transformer as the one for the zonotope\ndomain, except that we carry around the set of linear inequalities F—for the zono-\ntope domain, F is ﬁxed throughout.\nSpeciﬁcally, for an afﬁne function\nf (x1, . . . , xn) = ∑\nj\najxj\nwhere aj ∈R, we can deﬁne the abstract transformer as follows:\nf a(⟨c1i⟩, . . . ⟨cni⟩, F) =\n *\n∑\nj\najcj0, . . . ,∑\nj\najcjm\n+\n, F\n!\nNotice that the set of linear inequalities does not change between the input and\noutput of the function—i.e., there are no new constraints added.\nExample 10.D Consider f (x, y) = 3x + 2y . Then,\nf a(⟨1, 2, 3⟩, ⟨0, 1, 1⟩, F) = (⟨3, 8, 11⟩, F)\n■\nRectiﬁed Linear Unit\nLet’s now look at the abstract transformer for ReLU, which we illustrated earlier\nin the chapter:\nu\nl\nrelu(u)\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n114\nThis is the tightest convex polyhedron we can use to approximate the ReLU\nfunction. We can visually verify that tightening the shape any further will either\nmake it not an approximation or not convex—e.g., by bending the top face down-\nwards, we get a better approximation but lose convexity.\nLet’s see how to formally deﬁne relua. The key point is that the top face is the\nline\ny = u(x −l)\nu −l\nThis is easy to check using vanilla geometry. Now, our goal is to deﬁne the shaded\nregion, which is bounded by y = 0 from below, y = x from the right, and y =\nu(x−l)\nu−l\nfrom above.\nWe therefore deﬁne relua as follows:\nrelua(⟨ci⟩i, F) = (⟨0, 0, . . . , 0\n|\n{z\n}\nm\n, 1⟩, F′)\nwhere\nF′ ≡F ∧ϵm+1 ⩽u(⟨ci⟩−l)\n(u −l)\n∧ϵm+1 ⩾0\n∧ϵm+1 ⩾⟨ci⟩\nThere are a number of things to note here:\n• l and u are the lower and upper bounds of the input polyhedron, which can\nbe computed using linear programming.\n• ⟨ci⟩i is used to denote the full term c0 + ∑m\ni=1 ciϵi.\n• Observe that we’ve added a new generator, ϵm+1. The new set of constraints\nF′ relate this new generator to the input, effectively deﬁning the shaded re-\ngion.\nExample 10.E Consider the 1-dimensional polyhedron\n(⟨0, 1⟩, −1 ⩽ϵ1 ⩽1)\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n115\nwhich is the interval between −1 and 1. Invoking\nrelua(⟨0, 1⟩, −1 ⩽ϵ1 ⩽1)\nresults in (⟨0, 0, 1⟩, F′), where\nF′ ≡−1 ⩽ϵ1 ⩽1\n∧ϵ2 ⩽ϵ1 + 1\n2\n∧ϵ2 ⩾0\n∧ϵ2 ⩾ϵ1\nIf we plot the region deﬁned by F′, using ϵ1 as the x-axis and ϵ2 as the y-axis, we\nget the shaded region\n1\n−1\n1\n■\nOther Activation Functions\nFor ReLU, the transformer we presented is the most precise. For other activation\nfunctions, like sigmoid, there are many ways to deﬁne abstract transformers for\nthe polyhedron domain. Intuitively, one can keep adding more and more faces to\nthe polyhedron to get a more precise approximation of the sigmoid curve.\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n116\n10.4 Abstractly Interpreting Neural Networks with Polyhedra\nWe can now use our abstract transformers to abstractly interpret an entire neural\nnetwork, in precisely the same way we did for zonotopes, except that we’re now\ncarrying around a set of constraints. We review the process here for completeness.\nRecall that a neural network is deﬁned as a graph G = (V, E), giving rise to\na function fG : Rn →Rm, where n = |Vin| and m = |Vo|. We would like to\nconstruct an abstract transformer f a\nG that takes an n-dimensional polyhedron and\noutputs an m-dimensional polyhedron.\nWe deﬁne f a\nG(⟨c1j⟩, . . . ⟨cnj⟩, F) as follows:\n• First, for every input node vi, we deﬁne\nouta(vi) = (⟨cij⟩j, F)\nRecall that we assume a ﬁxed ordering of nodes.\n• Second, for every non-input node v, we deﬁne\nouta(v) = f a\nv\n \np1, . . . , pk,\nk^\ni=1\nFk\n!\nwhere f a\nv is the abstract transformer of fv, v has the incoming edges (v1, v), . . . , (vk, v),\nand\nouta(vi) = (pi, Fi)\nObserve what is happening here: we’re combining (with ∧) the constraints\nfrom the incoming edges. This ensures that we capture the relations between\nincoming values.\n• Finally, the output of f a\nG is the m-dimensional polyhedron\n \np1, . . . , pm,\nm\n^\ni=1\nFi\n!\nwhere v1, . . . , vm are the output nodes and outa(vi) = (pi, Fi)\nCHAPTER 10. NEURAL POLYHEDRON ABSTRACTION\n117\nSome abstract transformers (for activation functions) add new generators. We\ncan assume that all of these generators are already in the input polyhedron but\nwith coefﬁcients set to 0, and they only get non-zero coefﬁcients in the outputs of\nactivation function nodes.\nLooking Ahead\nWe looked at the polyhedron abstract domain, which was ﬁrst introduced by\nCousot and Halbwachs (1978). To minimize the size of the constraints, Singh\net al. (2019b) use a specialized polyhedron restriction that limits the number of\nconstraints, and apply it to neural-network veriﬁcation. Another representation\nof polyhedra, with specialized abstract transformers for convolutional neural net-\nworks, is ImageStars (Tran et al., 2020b). For a good description of efﬁcient poly-\nhedron domain operations and representations, for general programs, please con-\nsult Singh et al. (2017).\n118\nChapter 11\nVerifying with Abstract Interpretation\nWe have seen a number of abstract domains that allow us to evaluate a neural\nnetwork on an inﬁnite set of inputs. We will now see how to use this idea for\nveriﬁcation of speciﬁc properties. While abstract interpretation can be used, in\nprinciple, to verify any property in our language of correctness properties, much\nof the work in the literature is restricted to speciﬁc properties of the form:\n{ precondition }\nr ←f (x)\n{ postcondition }\nwhere the precondition deﬁnes a set of possible values for x, the inputs of the neu-\nral network, and the postcondition deﬁnes a set of possible correct values of r, the\noutputs of the neural network. To verify such properties with abstract interpreta-\ntion, we need to perform three tasks:\n1. Soundly represent the set of values of x in the abstract domain.\n2. Abstractly interpret the neural network f on all values of x, resulting in an\noverapproximation of values of r.\n3. Check that all values of r satisfy the postcondition.\nWe’ve seen how to do (2), abstractly interpreting the neural network. We will now\nsee how to do (1) and (3) for speciﬁc correctness properties from the literature.\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n119\n11.1 Robustness in Image Recognition\nIn image recognition, we’re often interested in ensuring that all images similar to\nsome image c have the same prediction as the label of c. Let’s say that the label of\nc is y. Then we can deﬁne robustness using the following property:\n{ ∥x −c∥p ⩽ϵ }\nr ←f (x)\n{ class(r) = y }\nwhere ∥x∥p is the ℓp norm of a vector and ϵ > 0. Typically we use the ℓ2 (Eu-\nclidean) or the ℓ∞norm as the distance metric between two images:\n∥z∥2 =\nr\n∑\ni\n|zi|2\n∥z∥∞= max\ni\n|zi|\nIntuitively, the ℓ2 norm is the length of the straight-line between two images in\nRn, while ℓ∞is the largest discrepancy between two images. For example, if each\nelement of an image’s vector represents one pixel, then the ℓ∞norm tells us the\nbiggest difference between two corresponding pixels.\nExample 11.A\n∥(1, 2) −(2, 4)∥2 = ∥(−1, −2)∥2\n=\n√\n3\n∥(1, 2) −(2, 4)∥∞= ∥(−1, −2)∥∞\n= 2\n■\nExample 11.B Consider an image c where every element of c represents the bright-\nness of a grayscale pixel, from black to white, say from 0 to 1. If we want to repre-\nsent the set of all images that are like c but where each pixel differs by a brightness\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n120\namount of 0.2, then we can use the ℓ∞norm in the precondition, i.e., the set of im-\nages x where\n∥x −c∥∞⩽0.2\nThis is because the ℓ∞norm captures the maximum discrepancy a pixel in c can\nwithstand. As an example, consider the handwritten 7 digit on the left and a\nversion of it on the right where each pixel’s brightness was changed by up to 0.2\nrandomly:\nNow consider the case where we want to represent all images that are like c\nbut where a small region has a very different brightness. For example, on the left\nwe see the handwritten 7 and on the right we see the same handwritten digit but\nwith a small bright dot:\nTo characterize a set of images that have such noise, like the dot above, we\nshouldn’t use ℓ∞norm, because ℓ∞bounds the brightness difference for all pixels,\nbut not some pixels, and here the brightness difference that results in the white dot\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n121\nis extreme—from 0 (black) to white (1). Instead, we can use the ℓ2 norm. For the\nabove pair of images, their ℓ∞-norm distance is 1; their ℓ2-norm distance is also 1,\nbut the precondition\n{∥x −c∥∞⩽1}\nincludes the images that are all black or all white, which are clearly not the digit\n7. The precondition\n{∥x −c∥2 ⩽1}\non the other hand, only allows a small number of pixels to signiﬁcantly differ in\nbrightness.\n■\nFor veriﬁcation, we will start by focusing on the ℓ∞-norm case and the interval\ndomain.\nAbstracting the Precondition\nOur ﬁrst goal is to represent the precondition in the interval domain. The precon-\ndition is the set of the following images:\n{x | ∥x −c∥∞⩽ϵ}\nExample 11.C Say c = (0, 0) and ϵ = 1. Then the above set is the following region:\n1\n−1\n1\n■\nAs the illustration above hints, it turns out that we can represent the set {x |\n∥x −c∥∞⩽ϵ} precisely in the interval domain as\nI = ([c1 −ϵ, c1 + ϵ], . . . , [cn −ϵ, cn + ϵ])\nInformally, this is because the ℓ∞norm allows us to take any element of c and\nchange it by ϵ independently of other dimensions.\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n122\nChecking the Postcondition\nNow that we have represented the set of values that x can take in the interval do-\nmain as I, we can go ahead and evaluate the abstract transformer f a(I), resulting\nin an output of the form\nI′ = ([l1, u1], . . . , [lm, um])\nrepresenting all possible values of r, and potentially more.\nThe postcondition speciﬁes that class(r) = y. Recall that class(r) is the index of\nthe largest element of r. To prove the property, we have to show that for all r ∈I′,\nclass(r) = y. We make the observation that\nif ly > ui for all i ̸= y,\nthen for all r ∈I′, class(r) = y\nIn other words, if the yth interval is larger than all others, then we know that\nthe classiﬁcation is always y. Notice that this is a one-sided check: if ly ⩽ui\nfor some i ̸= y, then we can’t disprove the property. This is because the set I′\noverapproximates the set of possible predictions of the neural network on the\nprecondition. So I′ may include spurious predictions.\nExample 11.D Suppose that\nf a(I) = I′ = ([0.1, 0.2], [0.3, 0.4])\nThen, class(r) = 2 for all r ∈I′. This is because the second interval is strictly larger\nthan the ﬁrst interval.\nNow suppose that\nI′ = ([0.1, 0.2], [0.15, 0.4])\nThese two intervals overlap in the region 0.15 to 0.2. This means that we cannot\nconclusively say that class(r) = 2 for all r ∈I′, and so veriﬁcation fails.\n■\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n123\nVerifying Robustness with Zonotopes\nLet’s think of how to check the ℓ∞-robustness property using the zonotope do-\nmain. Since the precondition is a hyperrectangular set, we can precisely represent\nit as a zonotope Z. Then, we evaluate the abstract transformer f a(Z), resulting in\na zonotope Z′.\nThe fun bit is checking the postcondition.\nWe want to make sure that di-\nmension y is greater than all others. The problem boils down to checking if a\n1-dimensional zonotope is always > 0. Consider the zonotope\nZ′ = (⟨c1i⟩, . . . ⟨cmi⟩)\nTo check that dimension y is greater than dimension j, we check if the lower bound\nof the 1-dimensional zonotope\n⟨cyi⟩−⟨cji⟩\nis > 0.\nExample 11.E Suppose that\nZ′ = (2 + ϵ1, 4 + ϵ1 + ϵ2)\nwhich is visualized as follows, with the center point (2,4) in red:\n1\n2\n3\n4\n1\n2\n3\n4\n5\nClearly, for any point (x, y) in this region, we have y > x. To check that y > x\nmechanically, we subtract the x dimension from the y dimension:\n(4 + ϵ1 + ϵ2) −(2 + ϵ1) = 2 + ϵ2\nThe resulting 1-dimensional zonotope (2 + ϵ2) denotes the interval [1, 3], which is\ngreater than zero.\n■\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n124\nVerifying Robustness with Polyhedra\nWith the polyhedron domain, the story is analogous to zonotopes but requires in-\nvoking a linear-program solver. We represent the precondition as a hyperrectan-\ngular polyhedron Y. Then, we evaluate the abstract transformer, f a(Y), resulting\nin the polyhedron\nY′ = (⟨c1i⟩, . . . ⟨cmi⟩, F)\nTo check if dimension y is greater than dimension j, we ask a linear-program\nsolver if the following constraints are satisﬁable\nF ∧⟨cyi⟩> ⟨cji⟩\nRobustness in ℓ2 Norm\nLet’s now consider the precondition with the set of images within an ℓ2 norm of\nc:\n{x | ∥x −c∥2 ⩽ϵ}\nExample 11.F Say c = (0, 0) and ϵ = 1. Then the above set is the following circular\nregion:\n1\n−1\n1\n■\nThis set cannot be represented precisely in the interval domain. To ensure that\nwe can verify the property, we need to overapproximate the circle with a box. The\nbest we can do is using the tightest box around the circle, i.e., ([−1, 1], [−1, 1]),\nshown below in red:\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n125\n1\n−1\n1\nThe zonotope and polyhedron domains also cannot represent the circular set\nprecisely. However, there isn’t a tightest zonotope or polyhedron that overap-\nproximates the circle. For example, with polyhedra, one can keep adding more\nand more faces, getting a better and better approximation, as illustrated below:\nIn practice, there is, of course, a precision–scalability tradeoff: more faces mean\nmore complex constraints and therefore slower veriﬁcation.\n11.2 Robustness in Natural-Language Processing\nWe will now take a look at another robustness property from natural-language\nprocessing. The goal is to show that replacing words with synonyms does not\nchange the prediction of the neural network. For instance, a common task is sen-\ntiment analysis, where the neural network predicts whether, say, a movie review\nis positive or negative. Replacing “amazing” with “outstanding” should not fool\nthe neural network into thinking a positive review is a negative one.\nWe assume that the input to the neural network is a vector where element i is\na numerical representation of the ith word in the sentence, and that each word w\nhas a ﬁnite set of possible synonyms Sw, where we assume w ∈Sw. Just as with\nimages, we assume a ﬁxed sentence c with label y for which we want to show\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n126\nrobustness. We therefore deﬁne the correctness property as follows:\n{ xi ∈Sci for all i }\nr ←f (x)\n{ class(r) = y }\nIntuitively, the precondition deﬁnes all vectors x that are like c but where some\nwords are replaced by synonyms.\nThe set of possible vectors x is ﬁnite, but it is exponentially large in the length\nof the input sentence. So it is not wise to verify the property by evaluating the\nneural network on every possible x. We can, however, represent an overapproxi-\nmation of the set of possible sentences in the interval domain. The idea is to take\ninterval between the largest and smallest possible numerical representations of\nthe synonyms of every word, as follows:\n([min Sc1, max Sc1], . . . , [min Scn, max Scn])\nThis set contains all the values of x, and more, but it is easy to construct, since\nwe need only go through every set of synonyms Sci individually, avoiding an\nexponential explosion.\nThe rest of the veriﬁcation process follows that of image robustness. In prac-\ntice, similar words tend to have close numerical representations, thanks to the\npower of word embeddings (Mikolov et al., 2013). This ensures that the interval\nis pretty tight. If words received arbitrary numerical representations, then our\nabstraction can be arbitrarily bad.\nLooking Ahead\nWe saw examples of how to verify properties via abstract interpretation. The an-\nnoying thing is that for every abstract domain and every property of interest, we\nmay need custom operations. Most works that use abstract interpretation so far\nhave focused on the properties I covered in this chapter. Other properties from\nearlier in the book can also be veriﬁed via the numerical domains we’ve seen. For\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n127\nexample, the aircraft controller from Chapter 3 has properties of the form:\n{ d ⩾55947, vown ⩾1145, vint ⩽60 }\nr ←f (d, vown, vint, . . .)\n{ score of nothing in r is below 1500 }\nNote that the precondition can be captured precisely in the interval domain.\nAt the time of writing, abstraction-based techniques have been applied suc-\ncessfully to relatively large neural networks, with up to a million neurons along\nmore than thirty layers (Müller et al., 2020; Tran et al., 2020a). Achieving such\nresults requires performant implementations, particularly for more complicated\ndomains like the zonotope and polyhedron domain. For instance, Müller et al.\n(2020) come up with data-parallel implementations of polyhedron abstract trans-\nformers that run on a GPU. Further, there are heuristics that can be employed to\nminimize the number of generators in the zonotope domain—limiting the number\nof generators reduces precision while improving efﬁciency. It is also important to\nnote that thus far most of the action in the abstraction-based veriﬁcation space,\nand veriﬁcation of neural networks at large, has been focused on ℓp-robustness\nproperties for images. (We’re also starting to see evidence that the ideas can ap-\nply to natural-language robustness (Zhang et al., 2021).) So it’s unclear whether\nveriﬁcation will work for more complex perceptual notions of robustness—e.g.,\nrotating an image or changing the virtual background on a video—or other more\ncomplex properties and domains, e.g., malware detection.\nThe robustness properties we discussed check if a ﬁxed region of inputs sur-\nrounding a point lead to the same prediction. Alternatively, we can ask, how big\nis the region around a point that leads to the same prediction? Naïvely, we can do this\nby repeatedly performing veriﬁcation with larger and larger ℓ2 or ℓ∞bounds un-\ntil veriﬁcation fails. Some techniques exploit the geometric structure of a neural\nnetwork—induced by ReLUs—to grow a robust region around a point (Zhang\net al., 2018b; Fromherz et al., 2021).\nAs we discussed throughout this part of the book, abstract-interpretation tech-\nniques can make stupid mistakes due to severe overapproximations. However,\nabstract interpretation works well in practice for veriﬁcation. Why? Two recent\npapers shed light on this question from a theoretical perspective (Baader et al.,\nCHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION\n128\n2020; Wang et al., 2020). The papers generalize the universal approximation prop-\nerty of neural networks (Section 2.1) to veriﬁcation with the interval domain or\nany domain that is more precise. Speciﬁcally, imagine that we have a neural net-\nwork that is robust as per the ℓ∞norm; i.e., the following property is true for a\nbunch of inputs of interest:\n{ ∥x −c∥∞⩽ϵ }\nr ←f (x)\n{ class(r) = y }\nBut suppose that abstract interpretation using the interval domain fails to prove\nrobustness for most (or all) inputs of interest. It turns out that we can always con-\nstruct a neural network f ′, using any realistic activation function (ReLU, sigmoid,\netc.), that is very similar to f—as similar as we like—and for which we can prove\nrobustness using abstract interpretation. The bad news, as per Wang et al. (2020),\nis that the construction of f ′ is likely exponential in the size of the domain.\n129\nChapter 12\nAbstract Training of Neural Networks\nYou have reached the ﬁnal chapter of this glorious journey. So far on our jour-\nney, we have assumed that we’re given a neural network that we want to verify.\nThese neural networks are, almost always, constructed by learning from data. In\nthis chapter, we will see how to train a neural network that is more amenable to\nveriﬁcation via abstract interpretation for a property of interest.\n12.1 Training Neural Networks\nWe begin by describing neural network training from a data set. Speciﬁcally, we\nwill focus throughout this chapter on a classiﬁcation setting.\nOptimization Objective\nA dataset is of the form\n{(x1, y1), . . . , (xm, ym)}\nwhere each xi ∈Rn is an input to the neural network, e.g., an image or a sentence,\nand yi ∈{0, 1} is a binary label, e.g., indicating if a given image is that of a cat\nor if a sentence has a positive or negative sentiment. Each item in the dataset is\ntypically assumed to be sampled independently from a probability distribution,\ne.g., the distribution of all images of animals.\nGiven a dataset, we would like to construct a function in Rn →R that makes\nthe right prediction on most of the points in the dataset. Speciﬁcally, we assume\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n130\nthat we have a family of functions represented as a parameterized function fθ,\nwhere θ is a vector of weights. We would like to ﬁnd the best function by searching\nthe space of θ values. For example, we can have the family of afﬁne functions\nfθ(x) = θ1 + θ2x1 + θ3x2\nTo ﬁnd the best function in the function family, we effectively need to solve an\noptimization problem like this one:\nargmin\nθ\n1\nm\nm\n∑\ni=1\n1[ fθ(xi) = yi]\nwhere 1[b] is 1 if b is true and 0 otherwise. Intuitively, we want the function that\nmakes the smallest number of prediction mistakes on our dataset {(x1, y1), . . . , (xm, ym)}.\nPractically, this optimization objective is quite challenging to solve, since the\nobjective is non-differentiable—because of the Boolean 1[·] operation, which isn’t\nsmooth. Instead, we often solve a relaxed optimization objective like mean squared\nerror (MSE), which minimizes how far fθ’s prediction is from each yi. MSE looks like\nthis:\nargmin\nθ\n1\nm\nm\n∑\ni=1\n( fθ(xi) −yi)2\nOnce we’ve ﬁgured out the best values of θ, we can predict the label of an input x\nby computing fθ(x) and declaring label 1 iff fθ(x) ⩾0.5.\nWe typically use a general form to describe the optimization objective. We\nassume that we’re given a loss function L(θ, x, y) which measures how bad is the\nprediction fθ(x) is compared to the label y. Formally, we solve\nargmin\nθ\n1\nm\nm\n∑\ni=1\nL(θ, xi, yi)\n(12.1)\nSquared error is one example loss function, but there are others, like cross-\nentropy loss. For our purposes here, we’re not interested in what loss function is\nused.\nLoss Function as a Neural Network\nThe family of functions fθ is represented as a neural network graph Gθ, where\nevery node v’s function fv may be parameterized by θ. It turns out that we can we\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n131\nrepresent the loss function L also as a neural network; speciﬁcally, we represent L\nas an extension of the graph Gθ by adding a node at the very end that computes,\nfor example, the squared difference between fθ(x) and y. By viewing the loss\nfunction L as a neural network, we can abstractly interpret it, as we shall see later\nin the chapter.\nSuppose that fθ : Rn →R has a graph of the form\nv1\nvn\n...\nvo\nwhere the dotted arrows indicate potentially intermediate nodes. We can con-\nstruct the graph of a loss function L(θ, x, y) by adding an input node vy for the\nlabel y and creating a new output node vL that compares the output of fθ (the\nnode vo) with y.\nv1\nvn\nvy\n...\nvo\nvL\nHere, input node vy takes in the label y and fvL encodes the loss function, e.g.,\nmean squared error ( f (x) −y)2.\nGradient Descent\nHow do we ﬁnd values of θ that minimize the loss? Generally, this is a hard\nproblem, so we just settle for a good enough set of values. The simplest thing to\ndo is to randomly sample different values of θ and return the best one after some\nnumber of samples. But this is a severely inefﬁcient approach.\nTypically, neural-network training employs a form of gradient descent. Gradient\ndescent is a very old algorithm, due to Cauchy in the mid 1800s. It works by\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n132\nstarting with a random value of θ and iteratively nudging it towards better values\nby following the gradient of the optimization objective. The idea is that starting\nfrom some point x0, if we want to minimize g(x0), then our best bet is to move in\nthe direction of the negative gradient at x0.\nThe gradient of a function g(θ) with respect to inputs θ, denoted ∇g, is the\nvector of partial derivatives1\n\u0012 ∂g\n∂θ1\n, . . . , ∂g\n∂θn\n\u0013\nThe gradient at a speciﬁc value θ0, denoted (∇g)(θ0), is\n\u0012 ∂g\n∂θ1\n(θ0), . . . , ∂g\n∂θn\n(θ0)\n\u0013\nIf you haven’t played with partial derivatives in a while, I recommend Deisenroth\net al. (2020) for a machine-learning-speciﬁc refresher.\nGradient descent can be stated as follows:\n1. Start with j = 0 and a random value of θ, called θ0.\n2. Set θj+1 to θj −η((∇g)(θi)).\n3. Set j to j + 1 and repeat.\nHere η > 0 is the learning rate, which constrains the size of the change of θ: too\nsmall a value and you’ll make baby steps towards a good solution; too large a\nvalue and you’ll bounce wildly around unable to catch a good region of solu-\ntions for θ, potentially even diverging. The choice of η is typically determined\nempirically by monitoring the progress of the algorithm for a few iterations. The\nalgorithm is usually terminated when the loss has been sufﬁciently minimized or\nwhen it starts making tiny steps, asymptotically converging to a solution.\nIn our setting, our optimization objective is\n1\nm\nm\n∑\ni=1\nL(θ, xi, yi)\n1The gradient is typically a column vector, but for simplicity of presentation we treat it as a\nrow vector here.\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n133\nFollowing the beautiful properties of derivatives, the gradient of this function is\n1\nm\nm\n∑\ni=1\n∇L(θ, xi, yi)\nIt follows that the second step of gradient descent can be rewritten as\nSet θj+1 to θj −η\nm ∑m\ni=1 ∇L(θj, xi, yi).\nIn other words, we compute the gradient for every point in the dataset indepen-\ndently and take the average.\nStochastic Gradient Descent\nIn practice, gradient descent is incredibly slow. So people typically use stochastic\ngradient descent (SGD). The idea is that, instead of computing the average gradient\nin every iteration for the entire dataset, we use a random subset of the dataset to\napproximate the gradient. SGD is also known as mini-batch gradient descent. Speciﬁ-\ncally, here’s how SGD looks:\n1. Start with j = 0 and a random value of θ, called θ0.\n2. Divide the dataset into a random set of k batches, B1, . . . , Bk.\n3. For i from 1 to k,\nSet θj+1 to θj −η\nm\n∑\n(x,y)∈Bi\n∇L(θj, x, y)\nSet j to j + 1\n4. Go to step 2.\nIn practice, the number of batches k (equivalently size of the batch) is typically a\nfunction of how much data you can cram into the GPU at any one point.2\n2To readers from the future: In the year 2021, graphics cards and some specialized accelera-\ntors used to be the best thing around for matrix multiplication. What have you folks settled on,\nquantum or DNA computers?\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n134\n12.2 Adversarial Training with Abstraction\nThe standard optimization objective for minimizing loss (Equation (12.1)) is only\ninterested in, well, minimizing the average loss for the dataset, i.e., getting as\nmany predictions right. So there is no explicit goal of generating robust neural\nnetworks, for any deﬁnition of robustness. As expected, this translates to neural\nnetworks that are generally not very robust to perturbations in the input. Fur-\nthermore, even if the trained network is robust on some inputs, veriﬁcation with\nabstract interpretation often fails to produce a proof. This is due to the over-\napproximate nature of abstract interpretation. One can always rewrite a neural\nnetwork—or any program for that matter—into one that fools abstract interpreta-\ntion, causing it to loose a lot of precision and therefore fail to verify properties of\ninterest. Therefore, we’d like to train neural networks that are friendly for abstract\ninterpretation.\nWe will now see how to change the optimization objective to produce robust\nnetworks and how to use abstract interpretation within SGD to solve this opti-\nmization objective.\nRobust Optimization Objective\nLet’s consider the image-recognition-robustness property from the previous chap-\nter: For every (x, y) in our dataset, we want the neural network to predict y on all\nimages z such that ∥x −z∥∞⩽ϵ. We can characterize this set as\nR(x) = {z | ∥x −z∥∞⩽ϵ}\nUsing this set, we will rewrite our optimization objective as follows:\nargmin\nθ\n1\nm\nm\n∑\ni=1\nmax\nz∈R(xi) L(θ, z, yi)\n(12.2)\nIntuitively, instead of minimizing the loss for (xi, yi), we minimize the loss for\nthe worst-case perturbation of xi from the set R(xi). This is known as a robust-\noptimization problem (Ben-Tal et al., 2009). Training the neural network using such\nobjective is known as adversarial training—think of an adversary (represented\nusing the max) that’s always trying to mess with your dataset to maximize the\nloss as you are performing the training (Madry et al., 2018).\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n135\nSolving Robust Optimization via Abstract Interpretation\nWe will now see how to solve the robust-optimization problem using SGD and\nabstract interpretation!\nLet’s use the interval domain. The set R(x) can be deﬁned in the interval do-\nmain precisely, as we saw in the last chapter, since it deﬁnes a set of images within\nan ℓ∞-norm bound. Therefore, we can overapproximate the inner maximization\nby abstractly interpreting L on the entire set R(xi). (Remember that L, as far as\nwe’re concerned, is just a neural network.) Speciﬁcally, by virtue of soundness of\nthe abstract transformer La, we know that\n\u0012\nmax\nz∈R(xi) L(θ, z, yi)\n\u0013\n⩽u\nwhere\nLa(θ, R(xi), yi) = [l, u]\nIn other words, we can overapproximate the inner maximization by abstractly\ninterpreting the loss function on the set R(xi) and taking the upper bound.\nWe can now rewrite our robust-optimization objective as follows:\nargmin\nθ\n1\nm\nm\n∑\ni=1\nupper bound of La(θ, R(xi), yi)\n(12.3)\nInstead of thinking of La as an abstract transformer in the interval domain, we can\nthink of it as a function that takes a vector of inputs, denoting lower and upper\nbounds of R(x), and returns the pair of lower and upper bounds. We call this idea\nﬂattening the abstract transformer; we illustrate ﬂattening with a simple example:\nExample 12.A Consider the ReLU function relu(x) = max(0, x). The interval ab-\nstract transformer is\nrelua([l, u]) = [max(0, l), max(0, u)]\nWe can ﬂatten it into a function reluaf : R2 →R2 as follows:\nreluaf(l, u) = (max(0, l), max(0, u))\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n136\nNotice that reluaf returns a pair in R2 as opposed to an interval.\n■\nWith this insight, we can ﬂatten the abstract loss function La into Laf. Then, we\njust invoke SGD on the following optimization problem,\nargmin\nθ\n1\nm\nm\n∑\ni=1\nLaf\nu (θ, li1, ui1, . . . , lin, uin, yi)\n(12.4)\nwhere Laf\nu is only the upper bound of the output of Laf, i.e., we throw away the\nlower bound (remember Equation (12.3)), and R(xi) = ([li1, ui1], . . . , [lin, uin]).\nSGD can optimize such objective because all of the abstract transformers of the\ninterval domain that are of interest for neural networks are differentiable (almost\neverywhere). The same idea can be adapted to the zonotope domain, but it’s a tad\nbit uglier.\nExample 12.B Given a function f : R →R, its zonotope abstract transformer f a\nis one that takes as input a 1-dimensional input zonotope with m generator vari-\nables, ⟨c0, . . . , cm⟩, and outputs a 1-dimensional zonotope also with m generators,\n⟨c′\n0, . . . , c′\nm⟩. We can ﬂatten f a by treating it as a function in\nf af : Rm+1 →Rm+1\nwhere the m + 1 arguments and outputs are the coefﬁcients of the m generator\nvariables and the center point.\n■\nFlattening does not work for the polyhedron domain, because it invokes a\nblack-box linear-programming solver for activation functions, which is not dif-\nferentiable.\nLooking Ahead\nWe saw how to use abstract interpretation to train (empirically) more robust neu-\nral networks. It has been shown that neural networks trained with abstract inter-\npretation tend to be (1) more robust to perturbation attacks and (2) are veriﬁably\nCHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS\n137\nrobust using abstract interpretation. The second point is subtle: You could have a\nneural network that satisﬁes a correctness property of interest, but that does not\nmean that an abstract domain will succeed at verifying that the neural network\nsatisﬁes the property. By incorporating abstract interpretation into training, we\nguide SGD towards neural networks that are amenable to veriﬁcation.\nThe ﬁrst use of abstract interpretation within the training loop came in 2018 (Mir-\nman et al., 2018; Gowal et al., 2018). Since then, many approaches have used ab-\nstract interpretation to train robust image-recognition as well as natural-language-\nprocessing models (Zhang et al., 2020, 2021; Jia et al., 2019; Xu et al., 2020; Huang\net al., 2019). Robust optimization is a rich ﬁeld (Ben-Tal et al., 2009); to my knowl-\nedge, Madry et al. (2018) were the ﬁrst to pose training of ℓp-robust neural net-\nworks as a robust-optimization problem.\nThere are numerous techniques for producing neural networks that are amenable\nto veriﬁcation. For instance, Sivaraman et al. (2020) use constraint-based veriﬁca-\ntion to verify that a neural network is monotone. Since constraint-based tech-\nniques can be complete, they can produce counterexamples, which are then used\nto retrain the neural network, steering it towards monotonicity. Another interest-\ning direction in the constraint-based world is to train neural networks towards\nReLUs whose inputs are always positive or always negative (Xiao et al., 2019).\nThis ensures that the generated constraints have as few disjunctions as possible,\nbecause the encoding of the ReLU will be linear (i.e., no disjunction).\nℓp-robustness properties are closely related to the notion of Lipschitz continuity.\nFor instance, a network f : Rn →Rm is K-Lipschitz under the ℓ2 norm if\n∥f (x) −f (y)∥2 ⩽K ∥x −y∥2\nThe smallest K satisfying the above is called the Lipschitz constant of f. If we\ncan bound K, then we can prove ℓ2-robustness of f. A number of works aim to\nconstruct networks with constrained Lipschitz constants, e.g., by adding special\nlayers to the network architecture or modifying the training procedure (Trockman\nand Kolter, 2021; Leino et al., 2021; Li et al., 2019)\n138\nChapter 13\nThe Challenges Ahead\nMy goal with this book is to give an introduction to two salient neural-network\nveriﬁcation approaches. But, as you may expect, there are many interesting ideas,\nissues, and prospects that we did not discuss.\nCorrectness Properties\nIn Part I of the book, we saw a general language of correctness properties, and\nsaw a number of interesting examples across many domains. One of the hardest\nproblems in the ﬁeld veriﬁcation—and the one that is least discussed—is how to\nactually come up with such properties (also known as speciﬁcations). For instance,\nwe saw forms of the robustness property many times throughout the book. Ro-\nbustness, at a high level, is very desirable. You expect an intelligent system to be\nrobust in the face of silly transformations to its input. But how exactly do we\ndeﬁne robustness? Much of the literature focuses on ℓp norms, which we saw in\nChapter 11. But one can easily perform transformations that lie outside ℓp norms,\ne.g., rotations to an image, or work in domains where ℓp norms don’t make much\nsense, e.g., natural language, source code, or other structured data.\nTherefore, coming up with the right properties to verify and enforce is a chal-\nlenging, domain-dependent problem requiring a lot of careful thought.\nVeriﬁcation Scalability\nEvery year, state-of-the-art neural networks blow up in size, gaining more and\nmore parameters. We’re talking about billions of parameters. There is no clear\nCHAPTER 13. THE CHALLENGES AHEAD\n139\nend in sight. This poses incredible challenges for veriﬁcation. Constraint-based\napproaches are already not very scalable, and abstraction-based approaches tend\nto lose precision with more and more operations. So we need creative ways to\nmake sure that veriﬁcation technology keeps up with the parameter arms race.\nVeriﬁcation Across the Stack\nVeriﬁcation research has focused on checking properties of neural networks in\nisolation. But neural networks are, almost always, a part of a bigger more com-\nplex system. For instance, a neural network in a self-driving car receives a video\nstream from multiple cameras and makes decisions on how to steer, speed up,\nor brake. These video streams run through layers of encoding, and the decisions\nmade by the neural network go through actuators with their own control software\nand sensors. So, if one wants to claim any serious correctness property of a neural-\nnetwork-driven car, one needs to look at all of the software components together\nas a system. This makes the veriﬁcation problem challenging for two reasons: (1)\nThe size of the entire stack is clearly bigger than just the neural network, so scala-\nbility can be an issue. (2) Different components may require different veriﬁcation\ntechniques, e.g., abstract domains.\nAnother issue with veriﬁcation approaches is the lack of emphasis on the train-\ning algorithms that produce neural networks. For example, training algorithms\nmay themselves not be robust: a small corruption to the data may create vastly\ndifferent neural networks. For instance, a number of papers have shown that poi-\nsoning the dataset through minimal manipulation can cause a neural network to\npick up on spurious correlations that can be exploited by an attacker. Imagine a\nneural network that detects whether a piece of code is malware. This network can\nbe trained using a dataset of malware and non-malware. By adding silly lines of\ncode to some of the non-malware code in the dataset, like print(\"LOL\"), we can\nforce the neural network to learn a correlation between the existence of this print\nstatement and the fact that a piece of code is not malware (Ramakrishnan and Al-\nbarghouthi, 2020). This can then be exploited by an attacker. This idea is known\nas installing a backdoor in the neural network.\nSo it’s important to prove that our training algorithm is not susceptible to small\nperturbations in the input data. This is a challenging problem, but researchers\nCHAPTER 13. THE CHALLENGES AHEAD\n140\nhave started to look at it for simple models (Drews et al., 2020; Rosenfeld et al.,\n2020).\nVeriﬁcation in Dynamic Environments\nOften, neural networks are deployed in a dynamic setting, where the neural net-\nwork interacts with the environment, e.g., a self-driving car. Proving correctness\nin this setting is rather challenging. First, one has to understand the interaction\nbetween the neural network and the environment—the dynamics. This is typically\nhard to pin down precisely, as real-world physics may not be as clean as textbook\nformulas. Further, the world can be uncertain, e.g., we have to somehow reason\nabout other crazy drivers on the road. Second, in such settings, one needs to verify\nthat a neural-network-based controller maintains the system in a safe state (e.g.,\non the road, no crash, etc.). This requires an inductive proof, as one has to reason\nabout arbitrarily many time steps of control. Third, sometimes the neural network\nis learning on-the-go, using reinforcement learning, where the neural network tries\nthings to see how the environment responds, like a toddler stumbling around.\nSo we have to ensure that the neural network does not do stupid things as it is\nlearning.\nRecently, there have been a number of approaches attempting to verify prop-\nerties of neural networks in dynamic and reinforcement-learning settings (Bastani\net al., 2018; Zhu et al., 2019; Ivanov et al., 2019; Anderson et al., 2020).\nProbabilistic Approaches\nThe veriﬁcation problems we covered are hard, yes-or-no problems. A recent ap-\nproach, called randomized smoothing (Cohen et al., 2019; Lécuyer et al., 2019), has\nshown that one can get probabilistic guarantees, at least for some robustness prop-\nerties (Ye et al., 2020; Bojchevski et al., 2020). Instead of saying a neural network\nis robust or not around some input, we say it is robust with a high probability.\n141\nBibliography\nMartín Abadi and Gordon D. Plotkin. A simple differentiable programming lan-\nguage.\nProc. ACM Program. Lang., 4(POPL):38:1–38:28, 2020.\ndoi: 10.1145/\n3371106. URL https://doi.org/10.1145/3371106.\nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jef-\nfrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gor-\ndon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete War-\nden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.\nTensorﬂow:\nA sys-\ntem for large-scale machine learning.\nIn Kimberly Keeton and Timothy\nRoscoe, editors, 12th USENIX Symposium on Operating Systems Design and Im-\nplementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016, pages 265–\n283. USENIX Association, 2016. URL https://www.usenix.org/conference/\nosdi16/technical-sessions/presentation/abadi.\nTuring Alan. On checking a large routine. In Report of a Conference on 11i9h Speed\nAutomatic Calculating Machines, pages 67–69, 1949.\nGreg Anderson, Abhinav Verma, Isil Dillig, and Swarat Chaudhuri. Neurosym-\nbolic reinforcement learning with formally veriﬁed exploration.\nIn Hugo\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\nHsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual, 2020.\nURL https://proceedings.neurips.cc/\npaper/2020/hash/448d5eda79895153938a8431919f4c9f-Abstract.html.\nBIBLIOGRAPHY\n142\nMaximilian Baader, Matthew Mirman, and Martin T. Vechev. Universal approxi-\nmation with certiﬁed networks. In 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020. URL https://openreview.net/forum?id=B1gX8kBtPr.\nClark W. Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan\nJovanovic, Tim King, Andrew Reynolds, and Cesare Tinelli. CVC4. In Ganesh\nGopalakrishnan and Shaz Qadeer, editors, Computer Aided Veriﬁcation - 23rd In-\nternational Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceed-\nings, volume 6806 of Lecture Notes in Computer Science, pages 171–177. Springer,\n2011. doi: 10.1007/978-3-642-22110-1\\_14. URL https://doi.org/10.1007/\n978-3-642-22110-1_14.\nOsbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,\nAditya V. Nori, and Antonio Criminisi.\nMeasuring neural net robust-\nness with constraints.\nIn Daniel D. Lee, Masashi Sugiyama, Ulrike von\nLuxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neu-\nral Information Processing Systems 29:\nAnnual Conference on Neural Informa-\ntion Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages\n2613–2621, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/\n980ecd059122ce2e50136bda65c25e07-Abstract.html.\nOsbert Bastani, Yewen Pu, and Armando Solar-Lezama.\nVeriﬁable reinforce-\nment learning via policy extraction. In Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, edi-\ntors, Advances in Neural Information Processing Systems 31: Annual Conference on\nNeural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,\nMontréal, Canada, pages 2499–2509, 2018. URL https://proceedings.neurips.\ncc/paper/2018/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html.\nAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization,\nvolume 28 of Princeton Series in Applied Mathematics. Princeton University Press,\n2009. ISBN 978-1-4008-3105-0. doi: 10.1515/9781400831050. URL https://doi.\norg/10.1515/9781400831050.\nBIBLIOGRAPHY\n143\nArmin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh, editors. Handbook\nof Satisﬁability, volume 185 of Frontiers in Artiﬁcial Intelligence and Applications.\nIOS Press, 2009. ISBN 978-1-58603-929-5.\nRobert G. Bland. New ﬁnite pivoting rules for the simplex method. Math. Oper.\nRes., 2(2):103–107, 1977. doi: 10.1287/moor.2.2.103. URL https://doi.org/10.\n1287/moor.2.2.103.\nAleksandar Bojchevski, Johannes Klicpera, and Stephan Günnemann. Efﬁcient\nrobustness certiﬁcates for discrete data: Sparsity-aware randomized smoothing\nfor graphs, images and more. In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of\nProceedings of Machine Learning Research, pages 1003–1013. PMLR, 2020. URL\nhttp://proceedings.mlr.press/v119/bojchevski20a.html.\nBob F Caviness and Jeremy R Johnson. Quantiﬁer elimination and cylindrical alge-\nbraic decomposition. Springer Science & Business Media, 2012.\nAlonzo Church. A note on the entscheidungsproblem. J. Symb. Log., 1(1):40–41,\n1936. doi: 10.2307/2269326. URL https://doi.org/10.2307/2269326.\nJeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial ro-\nbustness via randomized smoothing.\nIn Kamalika Chaudhuri and Ruslan\nSalakhutdinov, editors, Proceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of\nProceedings of Machine Learning Research, pages 1310–1320. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v97/cohen19c.html.\nPatrick Cousot and Radhia Cousot.\nAbstract interpretation: A uniﬁed lattice\nmodel for static analysis of programs by construction or approximation of ﬁx-\npoints. In Robert M. Graham, Michael A. Harrison, and Ravi Sethi, editors,\nConference Record of the Fourth ACM Symposium on Principles of Programming Lan-\nguages, Los Angeles, California, USA, January 1977, pages 238–252. ACM, 1977.\ndoi: 10.1145/512950.512973. URL https://doi.org/10.1145/512950.512973.\nPatrick Cousot and Nicolas Halbwachs. Automatic discovery of linear restraints\namong variables of a program.\nIn Alfred V. Aho, Stephen N. Zilles, and\nBIBLIOGRAPHY\n144\nThomas G. Szymanski, editors, Conference Record of the Fifth Annual ACM Sym-\nposium on Principles of Programming Languages, Tucson, Arizona, USA, January\n1978, pages 84–96. ACM Press, 1978. doi: 10.1145/512760.512770. URL https:\n//doi.org/10.1145/512760.512770.\nGeorge B Dantzig. Origins of the simplex method. In A history of scientiﬁc comput-\ning, pages 141–151. 1990.\nLeonardo Mendonça de Moura and Nikolaj Bjørner. Z3: an efﬁcient SMT solver.\nIn C. R. Ramakrishnan and Jakob Rehof, editors, Tools and Algorithms for the\nConstruction and Analysis of Systems, 14th International Conference, TACAS 2008,\nHeld as Part of the Joint European Conferences on Theory and Practice of Software,\nETAPS 2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings, volume 4963\nof Lecture Notes in Computer Science, pages 337–340. Springer, 2008. doi: 10.1007/\n978-3-540-78800-3\\_24. URL https://doi.org/10.1007/978-3-540-78800-3_\n24.\nMarc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. Mathematics for ma-\nchine learning. Cambridge University Press, 2020.\nSamuel Drews, Aws Albarghouthi, and Loris D’Antoni. Proving data-poisoning\nrobustness in decision trees. In Alastair F. Donaldson and Emina Torlak, editors,\nProceedings of the 41st ACM SIGPLAN International Conference on Programming\nLanguage Design and Implementation, PLDI 2020, London, UK, June 15-20, 2020,\npages 1083–1097. ACM, 2020. doi: 10.1145/3385412.3385975. URL https://\ndoi.org/10.1145/3385412.3385975.\nBruno Dutertre and Leonardo De Moura. Integrating simplex with dpll (t). Com-\nputer Science Laboratory, SRI International, Tech. Rep. SRI-CSL-06-01, 2006.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip: White-box\nadversarial examples for text classiﬁcation.\nIn Iryna Gurevych and Yusuke\nMiyao, editors, Proceedings of the 56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2:\nShort Papers, pages 31–36. Association for Computational Linguistics, 2018. doi:\n10.18653/v1/P18-2006. URL https://www.aclweb.org/anthology/P18-2006/.\nBIBLIOGRAPHY\n145\nNiklas Een. Minisat: A sat solver with conﬂict-clause minimization. In Proc. SAT-\n05: 8th Int. Conf. on Theory and Applications of Satisﬁability Testing, pages 502–518,\n2005.\nRüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural net-\nworks. In Deepak D’Souza and K. Narayan Kumar, editors, Automated Technol-\nogy for Veriﬁcation and Analysis - 15th International Symposium, ATVA 2017, Pune,\nIndia, October 3-6, 2017, Proceedings, volume 10482 of Lecture Notes in Computer\nScience, pages 269–286. Springer, 2017.\ndoi: 10.1007/978-3-319-68167-2\\_19.\nURL https://doi.org/10.1007/978-3-319-68167-2_19.\nKevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei\nXiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world\nattacks on deep learning visual classiﬁcation. In 2018 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\n22, 2018, pages 1625–1634. IEEE Computer Society, 2018. doi: 10.1109/CVPR.\n2018.00175. URL http://openaccess.thecvf.com/content_cvpr_2018/html/\nEykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.html.\nAymeric Fromherz, Klas Leino, Matt Fredrikson, Bryan Parno, and Corina S.\nPasareanu.\nFast geometric projections for local robustness certiﬁcation.\nIn\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/\nforum?id=zWy1uxjDdZJ.\nTimon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat\nChaudhuri, and Martin T. Vechev. AI2: safety and robustness certiﬁcation of\nneural networks with abstract interpretation. In 2018 IEEE Symposium on Secu-\nrity and Privacy, SP 2018, Proceedings, 21-23 May 2018, San Francisco, California,\nUSA, pages 3–18. IEEE Computer Society, 2018. doi: 10.1109/SP.2018.00058.\nURL https://doi.org/10.1109/SP.2018.00058.\nAntoine Girard. Reachability of uncertain linear systems using zonotopes. In\nManfred Morari and Lothar Thiele, editors, Hybrid Systems: Computation and\nControl, 8th International Workshop, HSCC 2005, Zurich, Switzerland, March 9-11,\nBIBLIOGRAPHY\n146\n2005, Proceedings, volume 3414 of Lecture Notes in Computer Science, pages 291–\n305. Springer, 2005. doi: 10.1007/978-3-540-31954-2\\_19. URL https://doi.\norg/10.1007/978-3-540-31954-2_19.\nIan J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. Deep Learning. Adap-\ntive computation and machine learning. MIT Press, 2016. ISBN 978-0-262-03561-\n3. URL http://www.deeplearningbook.org/.\nSven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli\nQin, Jonathan Uesato, Relja Arandjelovic, Timothy A. Mann, and Pushmeet\nKohli. On the effectiveness of interval bound propagation for training veriﬁ-\nably robust models. CoRR, abs/1810.12715, 2018. URL http://arxiv.org/abs/\n1810.12715.\nC. A. R. Hoare. An axiomatic basis for computer programming. Commun. ACM,\n12(10):576–580, 1969. doi: 10.1145/363235.363259. URL https://doi.org/10.\n1145/363235.363259.\nKurt Hornik, Maxwell B. Stinchcombe, and Halbert White. Multilayer feedfor-\nward networks are universal approximators.\nNeural Networks, 2(5):359–366,\n1989.\ndoi: 10.1016/0893-6080(89)90020-8.\nURL https://doi.org/10.1016/\n0893-6080(89)90020-8.\nPo-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama,\nSven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving ver-\niﬁed robustness to symbol substitutions via interval bound propagation.\nIn\nKentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4081–4091. Associ-\nation for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1419. URL\nhttps://doi.org/10.18653/v1/D19-1419.\nRadoslav Ivanov, James Weimer, Rajeev Alur, George J. Pappas, and Insup Lee.\nVerisig: verifying safety properties of hybrid systems with neural network\ncontrollers. In Necmiye Ozay and Pavithra Prabhakar, editors, Proceedings of\nBIBLIOGRAPHY\n147\nthe 22nd ACM International Conference on Hybrid Systems: Computation and Con-\ntrol, HSCC 2019, Montreal, QC, Canada, April 16-18, 2019, pages 169–178. ACM,\n2019. doi: 10.1145/3302504.3311806. URL https://doi.org/10.1145/3302504.\n3311806.\nKai Jia and Martin Rinard.\nEfﬁcient exact veriﬁcation of binarized neural\nnetworks, 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/\n1385974ed5904a438616ff7bdb3f7439-Abstract.html.\nKai Jia and Martin Rinard. Exploiting veriﬁed neural networks via ﬂoating point\nnumerical error. CoRR, abs/2003.03021, 2020b. URL https://arxiv.org/abs/\n2003.03021.\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang. Certiﬁed robust-\nness to adversarial word substitutions. In Kentaro Inui, Jing Jiang, Vincent Ng,\nand Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,\n2019, pages 4127–4140. Association for Computational Linguistics, 2019. doi:\n10.18653/v1/D19-1423. URL https://doi.org/10.18653/v1/D19-1423.\nGuy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochen-\nderfer.\nReluplex:\nAn efﬁcient SMT solver for verifying deep neural net-\nworks. In Rupak Majumdar and Viktor Kuncak, editors, Computer Aided Ver-\niﬁcation - 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-\n28, 2017, Proceedings, Part I, volume 10426 of Lecture Notes in Computer Sci-\nence, pages 97–117. Springer, 2017. doi: 10.1007/978-3-319-63387-9\\_5. URL\nhttps://doi.org/10.1007/978-3-319-63387-9_5.\nYann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database.\nATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.\nMathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman\nJana. Certiﬁed robustness to adversarial examples with differential privacy. In\n2019 IEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA,\nBIBLIOGRAPHY\n148\nMay 19-23, 2019, pages 656–672. IEEE, 2019. doi: 10.1109/SP.2019.00044. URL\nhttps://doi.org/10.1109/SP.2019.00044.\nKlas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks.\nIn Marina Meila and Tong Zhang, editors, Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol-\nume 139 of Proceedings of Machine Learning Research, pages 6212–6222. PMLR,\n2021. URL http://proceedings.mlr.press/v139/leino21a.html.\nMoshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multi-\nlayer feedforward networks with a nonpolynomial activation function can ap-\nproximate any function.\nNeural Networks, 6(6):861–867, 1993.\ndoi: 10.1016/\nS0893-6080(05)80131-5.\nURL\nhttps://doi.org/10.1016/S0893-6080(05)\n80131-5.\nQiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B. Grosse, and Jörn-\nHenrik Jacobsen. Preventing gradient attenuation in lipschitz constrained con-\nvolutional networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-\nimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances\nin Neural Information Processing Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 15364–15376, 2019. URL https://proceedings.neurips.cc/\npaper/2019/hash/1ce3e6e3f452828e23a0c94572bef9d9-Abstract.html.\nChangliu Liu, Tomer Arnon, Christopher Lazarus, Christopher A. Strong,\nClark W. Barrett, and Mykel J. Kochenderfer. Algorithms for verifying deep\nneural networks.\nFound. Trends Optim., 4(3-4):244–404, 2021.\ndoi: 10.1561/\n2400000035. URL https://doi.org/10.1561/2400000035.\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. Towards deep learning models resistant to adversarial attacks.\nIn 6th International Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,\n2018. URL https://openreview.net/forum?id=rJzIBfZAb.\nBIBLIOGRAPHY\n149\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\nEfﬁcient estima-\ntion of word representations in vector space. In Yoshua Bengio and Yann Le-\nCun, editors, 1st International Conference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013. URL\nhttp://arxiv.org/abs/1301.3781.\nMatthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract inter-\npretation for provably robust neural networks. In Jennifer G. Dy and Andreas\nKrause, editors, Proceedings of the 35th International Conference on Machine Learn-\ning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages 3575–3583. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/mirman18b.html.\nChristoph Müller, Gagandeep Singh, Markus Püschel, and Martin T. Vechev. Neu-\nral network robustness veriﬁcation on gpus. CoRR, abs/2007.10868, 2020. URL\nhttps://arxiv.org/abs/2007.10868.\nVinod Nair and Geoffrey E. Hinton.\nRectiﬁed linear units improve restricted\nboltzmann machines. In Johannes Fürnkranz and Thorsten Joachims, editors,\nProceedings of the 27th International Conference on Machine Learning (ICML-10),\nJune 21-24, 2010, Haifa, Israel, pages 807–814. Omnipress, 2010. URL https:\n//icml.cc/Conferences/2010/papers/432.pdf.\nNina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv,\nand Toby Walsh. Verifying properties of binarized deep neural networks. In\nSheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on\nEducational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana,\nUSA, February 2-7, 2018, pages 6615–6624. AAAI Press, 2018. URL https://\nwww.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16898.\nGreg Nelson and Derek C. Oppen. Simpliﬁcation by cooperating decision proce-\ndures. ACM Trans. Program. Lang. Syst., 1(2):245–257, 1979. doi: 10.1145/357073.\n357079. URL https://doi.org/10.1145/357073.357079.\nBIBLIOGRAPHY\n150\nMichael A. Nielsen. Neural Networks and Deep Learning. Determination Press, 2018.\nURL http://neuralnetworksanddeeplearning.com/.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-\ngory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Al-\nban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. Pytorch: An imperative style, high-performance deep learn-\ning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence\nd’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/\nhash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html.\nLuca Pulina and Armando Tacchella.\nAn abstraction-reﬁnement approach to\nveriﬁcation of artiﬁcial neural networks. In Tayssir Touili, Byron Cook, and\nPaul B. Jackson, editors, Computer Aided Veriﬁcation, 22nd International Confer-\nence, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings, volume 6174 of\nLecture Notes in Computer Science, pages 243–257. Springer, 2010. doi: 10.1007/\n978-3-642-14295-6\\_24. URL https://doi.org/10.1007/978-3-642-14295-6_\n24.\nChongli Qin, Krishnamurthy (Dj) Dvijotham, Brendan O’Donoghue, Rudy Bunel,\nRobert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, and Push-\nmeet Kohli. Veriﬁcation of non-linear speciﬁcations for neural networks. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/\nforum?id=HyeFAsRctQ.\nGoutham Ramakrishnan and Aws Albarghouthi. Backdoors in neural models of\nsource code. CoRR, abs/2006.06841, 2020. URL https://arxiv.org/abs/2006.\n06841.\nElan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and J. Zico Kolter.\nCerti-\nﬁed robustness to label-ﬂipping attacks via randomized smoothing.\nIn Pro-\nBIBLIOGRAPHY\n151\nceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-\n18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Re-\nsearch, pages 8230–8241. PMLR, 2020. URL http://proceedings.mlr.press/\nv119/rosenfeld20b.html.\nRahul Sharma, Aditya V. Nori, and Alex Aiken. Bias-variance tradeoffs in pro-\ngram analysis.\nIn Suresh Jagannathan and Peter Sewell, editors, The 41st\nAnnual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan-\nguages, POPL ’14, San Diego, CA, USA, January 20-21, 2014, pages 127–138. ACM,\n2014. doi: 10.1145/2535838.2535853. URL https://doi.org/10.1145/2535838.\n2535853.\nBenjamin Sherman, Jesse Michel, and Michael Carbin. λs: computable seman-\ntics for differentiable programming with higher-order functions and datatypes.\nProc. ACM Program. Lang., 5(POPL):1–31, 2021. doi: 10.1145/3434284. URL\nhttps://doi.org/10.1145/3434284.\nGagandeep Singh, Markus Püschel, and Martin T. Vechev. Fast polyhedra abstract\ndomain. In Giuseppe Castagna and Andrew D. Gordon, editors, Proceedings\nof the 44th ACM SIGPLAN Symposium on Principles of Programming Languages,\nPOPL 2017, Paris, France, January 18-20, 2017, pages 46–59. ACM, 2017. doi:\n10.1145/3009837.3009885. URL https://doi.org/10.1145/3009837.3009885.\nGagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Mar-\ntin T. Vechev.\nFast and effective robustness certiﬁcation.\nIn Samy Ben-\ngio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-\nBianchi, and Roman Garnett, editors, Advances in Neural Information Pro-\ncessing Systems 31:\nAnnual Conference on Neural Information Processing Sys-\ntems 2018,\nNeurIPS 2018,\nDecember 3-8,\n2018,\nMontréal,\nCanada,\npages\n10825–10836, 2018.\nURL https://proceedings.neurips.cc/paper/2018/\nhash/f2f446980d8e971ef3da97af089481c3-Abstract.html.\nGagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin T. Vechev. Be-\nyond the single neuron convex barrier for neural network certiﬁcation.\nIn\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-\nBuc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Informa-\nBIBLIOGRAPHY\n152\ntion Processing Systems 32: Annual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 15072–15083, 2019a.\nURL https://proceedings.neurips.cc/paper/\n2019/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html.\nGagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. An ab-\nstract domain for certifying neural networks.\nProc. ACM Program. Lang., 3\n(POPL):41:1–41:30, 2019b. doi: 10.1145/3290354. URL https://doi.org/10.\n1145/3290354.\nAishwarya Sivaraman, Golnoosh Farnadi, Todd D. Millstein, and Guy Van den\nBroeck. Counterexample-guided learning of monotonic neural networks. In\nHugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.\ncc/paper/2020/hash/8ab70731b1553f17c11a3bbc87e0b605-Abstract.html.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In\nYoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning\nRepresentations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track\nProceedings, 2014. URL http://arxiv.org/abs/1312.6199.\nAlfred Tarski. A decision method for elementary algebra and geometry. In Quanti-\nﬁer elimination and cylindrical algebraic decomposition, pages 24–84. Springer, 1998.\nVincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural\nnetworks with mixed integer programming. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-\nReview.net, 2019a. URL https://openreview.net/forum?id=HyGIdiRqtm.\nVincent Tjeng, Kai Yuanqing Xiao, and Russ Tedrake. Evaluating robustness of\nneural networks with mixed integer programming.\nIn 7th International Con-\nference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-\nBIBLIOGRAPHY\n153\n9, 2019. OpenReview.net, 2019b.\nURL https://openreview.net/forum?id=\nHyGIdiRqtm.\nHoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veriﬁca-\ntion of deep convolutional neural networks using imagestars. In Shuvendu K.\nLahiri and Chao Wang, editors, Computer Aided Veriﬁcation - 32nd International\nConference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part\nI, volume 12224 of Lecture Notes in Computer Science, pages 18–42. Springer,\n2020a. doi: 10.1007/978-3-030-53288-8\\_2. URL https://doi.org/10.1007/\n978-3-030-53288-8_2.\nHoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veriﬁca-\ntion of deep convolutional neural networks using imagestars. In Shuvendu K.\nLahiri and Chao Wang, editors, Computer Aided Veriﬁcation - 32nd International\nConference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part\nI, volume 12224 of Lecture Notes in Computer Science, pages 18–42. Springer,\n2020b. doi: 10.1007/978-3-030-53288-8\\_2. URL https://doi.org/10.1007/\n978-3-030-53288-8_2.\nAsher Trockman and J. Zico Kolter. Orthogonalizing convolutional layers with\nthe cayley transform. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL\nhttps://openreview.net/forum?id=Pbj8H_jEHYv.\nAlan Turing. Intelligent machinery. 1948. The Essential Turing, page 395, 1969.\nShiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. For-\nmal security analysis of neural networks using symbolic intervals. In William\nEnck and Adrienne Porter Felt, editors, 27th USENIX Security Symposium,\nUSENIX Security 2018, Baltimore, MD, USA, August 15-17, 2018, pages 1599–\n1614. USENIX Association, 2018. URL https://www.usenix.org/conference/\nusenixsecurity18/presentation/wang-shiqi.\nZi Wang, Aws Albarghouthi, Gautam Prakriya, and Somesh Jha. Interval uni-\nversal approximation for neural networks. CoRR, abs/2007.06093, 2020. URL\nhttps://arxiv.org/abs/2007.06093.\nBIBLIOGRAPHY\n154\nTsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca\nDaniel, Duane S. Boning, and Inderjit S. Dhillon.\nTowards fast computa-\ntion of certiﬁed robustness for relu networks. In Jennifer G. Dy and Andreas\nKrause, editors, Proceedings of the 35th International Conference on Machine Learn-\ning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages 5273–5282. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/weng18a.html.\nJimmy\nWestburg.\nhttps://tex.stackexchange.com/questions/356121/\nhow-to-draw-these-polyhedrons, 2017.\nKai Y. Xiao, Vincent Tjeng, Nur Muhammad (Mahi) Shaﬁullah, and Aleksander\nMadry. Training for faster adversarial robustness veriﬁcation via inducing relu\nstability. In 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://\nopenreview.net/forum?id=BJfIVjAcKm.\nKaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie\nHuang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturba-\ntion analysis for scalable certiﬁed robustness and beyond. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neu-\nral Information Processing Systems, volume 33, pages 1129–1141. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf.\nMao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for\ncertiﬁed robustness to adversarial word substitutions. In Dan Jurafsky, Joyce\nChai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th An-\nnual Meeting of the Association for Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 3465–3475. Association for Computational Linguistics,\n2020. doi: 10.18653/v1/2020.acl-main.317. URL https://doi.org/10.18653/\nv1/2020.acl-main.317.\nHuan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.\nEfﬁcient neural network robustness certiﬁcation with general activation func-\ntions.\nIn Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grau-\nBIBLIOGRAPHY\n155\nman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural\nInformation Processing Systems 31: Annual Conference on Neural Information Pro-\ncessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages\n4944–4953, 2018a. URL https://proceedings.neurips.cc/paper/2018/hash/\nd04863f100d59b3eb688a11f95b0ae60-Abstract.html.\nXin Zhang, Armando Solar-Lezama, and Rishabh Singh.\nInterpreting neu-\nral network judgments via minimal, stable, and symbolic corrections.\nIn\nSamy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò\nCesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information\nProcessing Systems 31:\nAnnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages\n4879–4890, 2018b. URL https://proceedings.neurips.cc/paper/2018/hash/\n300891a62162b960cf02ce3827bb363c-Abstract.html.\nYuhao Zhang, Aws Albarghouthi, and Loris D’Antoni.\nRobustness to pro-\ngrammable string transformations via augmented abstract training. In Proceed-\nings of the 37th International Conference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research,\npages 11023–11032. PMLR, 2020. URL http://proceedings.mlr.press/v119/\nzhang20b.html.\nYuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. Certiﬁed robustness to\nprogrammable transformations in LSTMs. CoRR, abs/2102.07818, 2021. URL\nhttps://arxiv.org/abs/2102.07818.\nHe Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan.\nAn induc-\ntive synthesis framework for veriﬁable reinforcement learning. In Kathryn S.\nMcKinley and Kathleen Fisher, editors, Proceedings of the 40th ACM SIGPLAN\nConference on Programming Language Design and Implementation, PLDI 2019,\nPhoenix, AZ, USA, June 22-26, 2019, pages 686–701. ACM, 2019. doi: 10.1145/\n3314221.3314638. URL https://doi.org/10.1145/3314221.3314638.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.PL"
  ],
  "published": "2021-09-21",
  "updated": "2021-10-04"
}