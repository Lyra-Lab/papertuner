{
  "id": "http://arxiv.org/abs/2109.10900v2",
  "title": "Towards Multi-Agent Reinforcement Learning using Quantum Boltzmann Machines",
  "authors": [
    "Tobias Müller",
    "Christoph Roch",
    "Kyrill Schmid",
    "Philipp Altmann"
  ],
  "abstract": "Reinforcement learning has driven impressive advances in machine learning.\nSimultaneously, quantum-enhanced machine learning algorithms using quantum\nannealing underlie heavy developments. Recently, a multi-agent reinforcement\nlearning (MARL) architecture combining both paradigms has been proposed. This\nnovel algorithm, which utilizes Quantum Boltzmann Machines (QBMs) for Q-value\napproximation has outperformed regular deep reinforcement learning in terms of\ntime-steps needed to converge. However, this algorithm was restricted to\nsingle-agent and small 2x2 multi-agent grid domains. In this work, we propose\nan extension to the original concept in order to solve more challenging\nproblems. Similar to classic DQNs, we add an experience replay buffer and use\ndifferent networks for approximating the target and policy values. The\nexperimental results show that learning becomes more stable and enables agents\nto find optimal policies in grid-domains with higher complexity. Additionally,\nwe assess how parameter sharing influences the agents behavior in multi-agent\ndomains. Quantum sampling proves to be a promising method for reinforcement\nlearning tasks, but is currently limited by the QPU size and therefore by the\nsize of the input and Boltzmann machine.",
  "text": "Towards Multi-Agent Reinforcement Learning using\nQuantum Boltzmann Machines\nTobias M¨uller, Christoph Roch, Kyrill Schmid and Philipp Altmann\nMobile and Distributed Systems Group, LMU Munich, Germany\ntobias.mueller1@campus.lmu.de, {christoph.roch, kyrill.schmid, philipp.altmann}@iﬁ.lmu.de\nKeywords:\nMulti-Agent; Reinforcement Learning; D-Wave; Boltzmann Machines; Quantum Annealing; Quantum\nArtiﬁcial Intelligence\nAbstract:\nReinforcement learning has driven impressive advances in machine learning.\nSimultaneously, quantum-\nenhanced machine learning algorithms using quantum annealing underlie heavy developments. Recently, a\nmulti-agent reinforcement learning (MARL) architecture combining both paradigms has been proposed. This\nnovel algorithm, which utilizes Quantum Boltzmann Machines (QBMs) for Q-value approximation has out-\nperformed regular deep reinforcement learning in terms of time-steps needed to converge. However, this\nalgorithm was restricted to single-agent and small 2x2 multi-agent grid domains. In this work, we propose\nan extension to the original concept in order to solve more challenging problems. Similar to classic DQNs,\nwe add an experience replay buffer and use different networks for approximating the target and policy values.\nThe experimental results show that learning becomes more stable and enables agents to ﬁnd optimal policies\nin grid-domains with higher complexity. Additionally, we assess how parameter sharing inﬂuences the agents’\nbehavior in multi-agent domains. Quantum sampling proves to be a promising method for reinforcement\nlearning tasks, but is currently limited by the Quantum Processing Unit (QPU) size and therefore by the size\nof the input and Boltzmann machine.\n1\nIntroduction\nRecently, adiabatic quantum computing has proven\nto be a useful extension to machine learning tasks\n(Benedetti et al., 2018; Biamonte et al., 2017; Li et al.,\n2018; Neukart et al., 2017a). Especially hard com-\nputational tasks with high data volume and dimen-\nsionality have beneﬁtted from the possibility of using\nquantum devices with manufactured spins to speed-\nup computational bottlenecks (Neven et al., 2008;\nRebentrost et al., 2014; Wiebe et al., 2012).\nOne speciﬁc type of machine learning is Rein-\nforcement Learning (RL), where an interacting entity,\ncalled agent, aims to learn an optimal state-action pol-\nicy through trial and error (Sutton and Barto, 2018).\nReinforcement Learning has gained the public atten-\ntion by defeating the 9-dan Go grandmaster Lee Sedol\n(Silver et al., 2016), which has been thought to be\nimpossible for a machine.\nIn the latest years, re-\ninforcement learning has seen many improvements,\ngained a large variety of application ﬁelds like eco-\nnomics (Charpentier et al., 2020), autonomous driv-\ning (Kiran et al., 2020), biology (Mahmud et al.,\n2018) and even achieved superhuman performance\nin chip design (Mirhoseini et al., 2020). Reinforce-\nment Learning has only seen quantum speed-ups for\nspecials models (Levit et al., 2017; Neukart et al.,\n2017a; Neukart et al., 2017b; Paparo et al., 2014).\nEspecially multi-agent domains have rarely been re-\nsearched (Neumann et al., 2020).\nReal-world reinforcement learning frameworks\npredominantly use deep neural networks (DNNs) as\nfunction approximators. Since DNNs are powerful -\nsee the latest prominent example AlphaFold2 (Jumper\net al., 2020) - and can be run efﬁciently for large\ndatasets on classical computers, deep reinforcement\nlearning is able to tackle complex problems in large\ndata spaces. Hence, there was little need for improve-\nments.\nHowever, since recent work has proved speed-\nups for classical RL by leveraging quantum comput-\ning (Levit et al., 2017; Neumann et al., 2020) and\nthe application ﬁeld gets more and more complex,\nit could be beneﬁcial to explore quantum RL algo-\nrithms. These inspiring studies considered Boltzmann\nmachines (Ackley et al., 1985) as function approxi-\nmator - instead of traditionally used DNNs. Boltz-\nmann machines are stochastic neural networks, which\narXiv:2109.10900v2  [cs.AI]  22 Nov 2021\nare mainly avoided due to the fact, that their training\ntimes are exponential to the input size. Since ﬁnd-\ning the energy minimum of Boltzmann machines can\nbe formulated as a ”Quadratic Unconstrained Binary\nOptimization” (QUBO) problem, simulated anneal-\ning respectively quantum annealing is well suited to\naccelerate training time.\nNevertheless, the combination of RL and Boltz-\nmann machines using (simulated) quantum annealing\nonly worked properly for small single-agent environ-\nments and reached its limit at a simple 3 × 3 multi-\nagent domain. This work proposes an architecture in-\nspired by DQNs (Mnih et al., 2015) to enable more\ncomplex domains and stabilize learning by using ex-\nperience replay buffer and separating policy and tar-\nget networks. We thoroughly evaluate the effects of\nthese augmentations on learning.\nLately, an inspiring novel method to speed-up\nquantum reinforcement learning for large state and\naction spaces by proposing a combination of reg-\nular NNs and DBMs/QBMs, namely Deep Energy\nBased Networks (DEBNs) was proposed (Jerbi et al.,\n2020). More speciﬁcally, these architectures are con-\nstructed with an input layer consisting of action and\nstate units, which are connected with the ﬁrst hidden\nlayer through directed weights. This is followed by\na single undirected stochastic layer. The remaining\nlayers are linked with directed deterministic connec-\ntions. Lastly, a ﬁnal output layer returns the negative\nfree energy −F(s,a).\nIn contrast to QBMs, DEBNs therefore only com-\nprise one stochastic layer, return an output similar to\ntraditional deep neural networks and can be trained\nthrough backpropagation. DEBNs also use an expe-\nrience replay buffer and separate the policy and tar-\nget network.\nAdditionally, they allow to trade off\nlearning performance for efﬁciency of computation.\nJerbi et al.\nbrieﬂy stated, that QBMs are applica-\nble. Unfortunately, no numerical results were given\nfor purely stochastic, energy-based QBM agents or\ndomains with multiple agents. We aim to build on\nthis.\nSummarized, our contribution is three-fold:\n• We provide a Quantum Reinforcement Learn-\ning (Q-RL) framework, which stabilizes learning\nleading to more optimal policies\n• Based on single- and multi-agent domains, we\nprovide a thorough evaluation on the effects of an\nExperience Replay Buffer and an additional Tar-\nget Network compared to traditional QBM agents\n• Additionally, we demonstrate and discuss limita-\ntions to the concept\nWe ﬁrst describe the preliminaries about rein-\nforcement learning and quantum Boltzmann ma-\nchines underlying the proposed architectures. After-\nwards, the state-of-the-art algorithm and extensions\nmade to it will be explained. We test and evaluate the\napproach and ﬁnally discuss restrictions and potential\ngrounds for future work.\n2\nPreliminaries\nThis chapter describes the basics needed to under-\nstand our proposed architecture. First, reinforcement\nlearning and the underlying Markov Decision Process\nwill be explained followed by Boltzmann Machines\nand the process of quantum annealing.\n2.1\nReinforcement Learning\nWe ﬁrst describe Markov Decision Processes as the\nunderlying problem formulation which is followed by\nan introduction to reinforcement learning in general.\nThe subsequent sections specify independent and co-\noperative multi-agent reinforcement learning.\nMarkov Decision Processes\nThe problem formula-\ntion is based on the notion of Markov Decision Pro-\ncesses (MDP) (Puterman, 1994). MDPs are a class\nof sequential decision processes and described via the\ntuple M = ⟨S,A,P,R⟩, where\n• S is a ﬁnite set of states and st ∈S the state of the\nMDP at time step t.\n• A is the set of actions and at ∈A the action the\nMDP takes at time step t.\n• P(st+1|st,at) is the probability transition function.\nIt describes the transition that occurs when action\nat is executed in state st. The resulting state st+1\nis chosen according to P.\n• R(st,at) is the reward, when the MDP takes action\nat in state st. We assume R(st,at) ∈R\nConsequently, the cost and transition function\nonly depend on the current state and action of the\nsystem. Eventually, the MDP should ﬁnd a policy\nπ : S →A in the space of all possible policies Π, which\nmaximizes the return Gt at state st over an inﬁnite\nhorizon via:\nGt =\n∞\n∑\nk=0\nγk ·R(st+k,at+k),\n(1)\nwith γ ∈[0,1] as the discount factor. This policy is\ncalled the optimal policy π.\nReinforcement\nLearning\nModel-free\nreinforce-\nment learning (Strehl et al., 2006) is considered to\nsearch the policy space Π in order to ﬁnd the opti-\nmal policy π∗. The interacting reinforcement learn-\ning agent executes an action at for every time step\nt ∈[1,..] in the MDP environment. In model-free al-\ngorithms, the agent acts without any knowledge of the\nenvironment and the algorithm only keeps informa-\ntion of the value-function. Therefore, the agent knows\nits current state st and the action space A, but neither\nthe reward nor the next state st+1 of any action at in\nany state st.\nConsequently, the agent needs to learn from delayed\nrewards without having a model of the environment.\nA popular value-based approach to solve this problem\nis Q-learning (Peng and Williams, 1994). In this ap-\nproach, the action-value function Qπ : SxA →R,π ∈Π\ndescribes the accumulated reward Qπ(st,at) for an ac-\ntion at in state st. The optimal Q-learning function Q∗\nis approximated by starting from an initial guess for\nQ and updating the function via:\nQ(st,at) ←Q(st,at)+α[rt +γmaxa Q(st+1,a)−Q(st,at)]\n(2)\nThe learned Q-function will eventually converge\nto Q∗, which then implies an optimal policy. In the\ntraditional experiments a deep neural network is used\nas a parameterized function approximator to calculate\nthe optimal action for a given state.\nIndependent Multi-Agent Learning\nWhen mul-\ntiple agents interact with the environment, A fully\ncooperative multi-agent task can be described as a\nstochastic game G, deﬁned as in (Foerster et al., 2017)\nvia the tuple G = ⟨S,A,P,R,Z,O,n,γ⟩, where:\n• S is a ﬁnite set of states. At each time step t, the\nenvironment has a true state st ∈S.\n• A is the set of actions. At each time step t each\nagent ag simultaneously chooses an action aag ∈\nA, forming a joint action a ∈A ≡An.\n• P(st+1|st,at) is the probability transition function\nas previously deﬁned.\n• R(st,at) is the reward as previously deﬁned. All\nagents share the same reward function.\n• Z is a set of observations of a partially or fully\nobservable environment.\n• O(s,ag) is the observation function. Each agent\ndraws observations z ∈Z according to O(s,ag).\n• n is the number of agents identiﬁed by ag ∈AG ≡\n{1,...,n}.\n• γ ∈[0,1) is the discount factor.\nFigure 1: A Deep Quantum Boltzmann Machine with seven\ninput state neurons and ﬁve input action neurons. The QBM\nadditionally consists of three hidden layers with four neu-\nrons each. The state and action are given as ﬁxed input\nand the conﬁguration of the hidden neurons are sampled via\n(simulated) quantum annealing. The weights between two\nneurons are updated in the Q-learning step as described in\nsection 3.\nIn independent multi-agent learning algorithms, each\nagent learns from its own action-observation history\nand is trained independently. This means, every agent\nsimultaneously learns its own Q-function (Tan, 1993).\n2.2\nBoltzmann Machines\nThe structure of a Boltzmann machine (BM) (Ack-\nley et al., 1985) is similar to Hopﬁeld networks and\ncan be described as a stochastic energy-based neural\nnetwork. A traditional BM consists of a set of visi-\nble nodes V and a set of hidden nodes H, where ev-\nery node represents a binary random variable. The\nbinary nodes are connected through real-valued, bidi-\nrected, weighted edges of the underlying undirected\ngraph. The global energy conﬁguration is generally\ngiven by the energy level of Hopﬁeld networks. Since\nclamped BMs ﬁx the assignment of the visible binary\nvariables, these nodes are removed from the under-\nlying graph and contribute as constant coefﬁcients to\nthe associated energy. Therefore the formula, which\nwe aim to minimize, is given as the energy level of\nHopﬁeld networks with constant visible nodes:\nE(h) = −∑\ni\nwiivi −∑\nj\nw j jhj −∑\ni ∑\nj\nviwijhj,\n(3)\nwith vi as the visible nodes, hj as the hidden nodes\nand weights w.\nFor this work, we implemented a Deep Boltzmann\nMachine (DBM) as trainable state-action approxima-\ntor, which is constructed with multiple hidden layers,\none visible input layer for the state and one visible ac-\ntion input layer. Finally, we modiﬁed the DBM to get\na Quantum Boltzmann Machine (QBM), where qubits\nare associated to each node of the network instead of\nrandom binary variables (Crawford et al., 2019; Neu-\nmann et al., 2020; Levit et al., 2017). A visualization\nof a QBM for seven state neurons and ﬁve input action\nneurons can be seen in ﬁgure 1. For any QBM with\nv ∈V and h ∈H, the energy function is described by\nthe quantum Hamiltonian Hv:\nHv = −∑\nv,h\nwvhvσz\nh −∑\nv,v′\nwvv′vv′\n−∑\nh,h′\nwhh′σz\nhσz\nh′ −Γ∑\nh\nσx\nh\n(4)\nFurthermore, Γ is the annealing parameter, while\nσz\ni and σxi are spin-values of node i in the z−and\nx−direction.\nBecause measuring the state of one\ndirection destroys the state of the other, we follow\nthe architecture of Neumann et al. (2020) (Neumann\net al., 2020) and replace all σx\ni by σz by using replica\nstacking based on the Suzuki-Trotter expansion of the\nHamiltonian Hv. The BM is replicated r times in to-\ntal and connections between corresponding nodes in\nadjacent replicas are added. By this, we obtain a new\neffective Hamiltonian H e f f\nv=(s,a) in its clamped version\ngiven by:\nH ef f\nv=(s,a) = −∑\nh∈H\nh−s ad j\nr\n∑\nk=1\nwsh\nr σh,k −∑\nh∈H\nh−a ad j\nr\n∑\nk=1\nwah\nr σh,k\n−∑\n(h,h′)⊆H\nr\n∑\nk=1\nwhh′\nr σh,kσh′,k −Γ ∑\nh∈H\nr\n∑\nk=0\nσh,kσh,k+1\n(5)\nFor each evaluation of the Hamiltonian, we get a\nspin conﬁguration ˆh. After nreads reads for a ﬁxed\ncombination of s and a, we get a multi-set ˆhs,a =\n{ˆh1,..., ˆhnreads}. We average over this multi-set to gain\na single spin conﬁguration Cˆhs,a, which will be used\nfor updating the network. If a node is +1 or −1 de-\npends on the global energy conﬁguration:\npnode i=1 =\n1\n1+exp(−∆Ei\nT )\n,\n(6)\nwith T as the current temperature.\nSince the structure of Boltzmann Machines are\ninherent to Ising models, we sample spin values\nfrom the Boltzmann distribution by using simulated\nquantum annealing, which simulates the effect of\ntransverse-ﬁeld Ising model by slowly reducing the\ntemperature or strength of the transverse ﬁeld at ﬁnite\ntemperature to the desired target value (Levit et al.,\n2017). As proven in (Morita and Nishimori, 2008),\nspin system deﬁned by simulated quantum annealing\nconverges to quantum Hamiltonian. Therefore it is\nstraightforward to use simulated quantum annealing\n(SQA) to ﬁnd a spin conﬁguration for h ∈H - given\ns ∈S - which minimizes the free energy.\n3\nQuantum Reinforcement Learning\nRecently, quantum reinforcement learning algorithms\n(QRL) using boltzmann machines and quantum an-\nnealing of single agent (Crawford et al., 2019) and\nmulti-agent domains (Neumann et al., 2020) for learn-\ning grid-traversal policies have been proposed. Al-\nthough, these architectures were able to learn opti-\nmal policies in less time steps compared to classic\ndeep reinforcement learners (DRL), they could only\nbe applied to single-agent or small multi-agent do-\nmains. Unfortunately, already 3 × 3 domains with 2\nagents could not be solved optimally (Neumann et al.,\n2020). QRL seems to be unstable for more complex\ndomains. We intuitively assume that BMs underlie\nsimilar instability problems as traditional neural net-\nworks. Hence, by correlations present in the sequence\nof observations and how small updates to the Q-values\nchange the policy, data distribution and therefore the\ncorrelations between free energy F(sn,an) and target\nenergy F(sn+1,an+1). Inspired by Deep Q-Networks\n(Mnih et al., 2015), we propose to enhance the state-\nof-the-art architecture as described in section 3.1 by\nadding an experience replay buffer (see section 3.2)\nto randomize over transitions and by separating the\nnetwork calculating the policy and the network ap-\nproximating the target value (see section 3.3) in order\nto reduce correlations with the target.\n3.1\nState of the Art\nTraditionally, single-agent reinforcement learning us-\ning quantum annealing and QBMs is an adaption\nof Sallans and Hintons (2004) (Sallans and Hinton,\n2004) RBM RL algorithm and structured as follows:\nInitialization.\nThe weights of the QBM are initial-\nized by setting the weights using Gaussian zero-mean\nvalues with a standard deviation of 1.00. The topol-\nogy of the hidden layers is set beforehand.\nPolicy.\nAt the beginning of each episode, every\nagent is set randomly onto the grid and receives its\ncorresponding observation. At each time step t, ev-\nery agent i independently chooses an action ai\nt accord-\ning to its policy πi\nt. To enable exploration, we imple-\nmented an ε-greedy policy, where the agent acts ran-\ndom with probability ε, which decreases by εdecay =\n0.0008 with each training step until εmin = 0.01 is\nreached. When the agent follows its learned policy,\nwe sweep across all possible actions and choose the\naction which maximizes the Q-value for state si\nt. The\nQ-function of state s and action a is deﬁned as the\ncorresponding negative free-energy −F:\nQ(s,a) ≈−F(s,a) = −F(s,a;w),\n(7)\nwith w as the vector of weights of a QBM and F(s,a)\nas:\nF(s,a) = ⟨He f f\nv=(s,a)⟩−1\nβP(st+1|st,at)logP(st+1|st,at)\n(8)\nSummarized, the agent acts via:\nπ =\n\u001a\nrandom,\nif p ≥ε\nargmax Q(s,a),\nif p < ε\n(9)\nfor a ∈A and random variable p.\nWeight Update.\nThe environment returns a reward\nri\nt+1 and second state si\nt+1 ←ai\nt(si\nt) for each agent i.\nBased on this transition, the QBM is trained. The\nused update rules are an adaption of the state-action-\nreward-state-action (SARSA) rule by Rummery et al.\n(1994) (Rummery and Niranjan, 1994) with negative\nfree energy instead of Q-values (Levit et al., 2017)\ndeﬁned as:\n∆wvh = µ(rn(sn,an)−γF(sn+1,an+1)\n+F(sn,an))v⟨σz\nh⟩\n(10)\n∆whh′ = µ(rn(sn,an)−γF(sn+1,an+1)\n+F(sn,an))v⟨σz\nhσz\nh′⟩,\n(11)\nwith γ as the discount factor and µ as the learning\nrate. The free energy and conﬁgurations of the hid-\nden neurons are gained by applying simulated quan-\ntum annealing respectively quantum annealing to the\nformulation of the effective Hamiltonian He f f\nv=(s,a) as\ndescribed in the previous section. At each episode,\nthis process is repeated for a deﬁned number of steps\nor until the episode ends.\n3.2\nExperience Replay Buffer\nThe ﬁrst extension is a biologically inspired mech-\nanism named experience replay (Mcclelland et al.,\n1995; O’Neill et al., 2010). O’Neill et al. (2010)\nfound, that the human brain stabilizes memory traces\nfrom short- to long-term memory by replaying mem-\nories during sleep and rest. The reactivation of brain-\nwide memory traces could underlie memory consoli-\ndation. Similar to the human brain, experience replay\nbuffers used in deep Q-networks (DQN) store experi-\nenced transitions and provides randomized data dur-\ning updating neural connections. Hence, correlations\nof observation sequences are removed and changes in\nthe data distribution are smoothed. Furthermore, due\nto the random choice of training samples, one transi-\ntion can be used multiple times to consolidate experi-\nences.\nTo enable experience replay, at each time step t we\nstore the each agents’ experience et = (st,at,rt,st+1)\nin a data set Dt = (e1,...,et).\nFor every training\nstep, we randomly sample mini-batches from Dt from\nwhich to Q-learning updates are performed.\nThis means, instead of updating the weights on\nstate-action pairs as they occur, we store discovered\ndata and perform training on random mini-batches\nfrom a pool of random transitions.\n3.3\nPolicy and Target Network\nIn order to perform a training step, it is necessary to\ncalculate the policy value F(sn,an) and target value\nF(sn+1,an+1).\nCurrently, policies and target val-\nues are approximated by the same network. Conse-\nquently, Q-values and target values are highly corre-\nlated. Small updates to Q-values may signiﬁcantly\nchange the policy, data distribution and target.\nTo counteract, we separate policy network calcu-\nlating F(sn,an) from the target network approximat-\ning F(sn+1,an+1). Both networks are initialized simi-\nlarly. The policy network is updated with every train-\ning step, whereas the target network is only periodi-\ncally updated. Every m steps, the weights of the pol-\nicy network are simply adopted by the target network.\n3.4\nMulti-Agent Quantum\nReinforcement Learning\nIn this work, we explore independent quantum learn-\ning in cooperative and non-cooperative settings. The\nexplicit requirement for cooperation is communica-\ntion (Binmore, 2007).\nWe enable communication\nvia parameter sharing as proposed by Foerster et al.\n(2016) (Foerster et al., 2016).\nIn this case, every\nagents’ transition is stored in a centralized experi-\nence replay buffer and only one BM is trained. Each\nagent receives its own observation and the centralised\nnetwork approximates the agents’ Q-value indepen-\ndently. Whereas in non-cooperative settings, every\nagent keeps and updates its own BM solely with its\nown experiences without any information exchange.\nThe policy and weight updates are performed as de-\nscribed in the previous section.\n4\nEvaluation\n4.1\nDomain\nTo evaluate our approach, we implemented a discrete\nn × m multi-agent grid-world domain with i deter-\nministic rewards and i agents. At every time step t\neach agent independently chooses an action from ac-\ntion space A = {up, down, left, right, stand still}\ndepending on the policy π.\nMore speciﬁcally, the\ngoal of every agent is to collect corresponding balls\nwhile avoiding obstacles (e.g. walls and borders) and\npenalty states (e.g. pits and others’ balls). The envi-\nronment size, number of agents, balls and obstacles\ncan be easily modiﬁed. Reaching a target location is\nrewarded by a value of 220, whereas penalty states are\npenalized by -220 and an extra penalty of -10 is given\nfor every needed step. An agent is done, when all\nits corresponding balls were collected. Consequently,\nwe consider the domain as solved, when every agent\nis done. The main goal lies in efﬁciently navigating\nthrough the grid. Two example domains can be seen\nin ﬁgure 2.\nThe starting position of all agents are chosen ran-\ndomly at the beginning of each episode whereas the\nlocations of their goals are ﬁxed. The observation is\none-hot-encoded and divided into two layers. One\nlayer describes the agents’ position and its goal and\nthe other layer details the position of all other agents\nand their goals. This observation is issued as input for\nthe algorithm. Therefore, the input shape is n×m×2.\nTo asses the learned policies, we use the accumulated\nepisode rewards as quality measure.\n(a) 3x3 grid\n(b) 5x3 grid\nFigure 2: Example ﬁgures of two single-agent domains.\nPicture a) shows a 3 × 3 grid domain with one reward,\nwhereas b) illustrates a bigger 5 × 3 grid domain with an\nadditional penalty state.\n4.2\nSingle-Agent Results\nFirst, we evaluate how adding an experience replay\nbuffer (ERB) and separating policy and target net-\nwork inﬂuences the learning process and performance\nof a single agent. We started by running the tradi-\ntional Q-RL algorithm as proposed by Neumann et\nal. (2020) (Neumann et al., 2020) including their pa-\nrameter setting. Then, we only added an experience\nreplay buffer (ERB) respectively solely the target net-\nwork.\nFinally, we extended the original algorithm\nwith a combination of both, an ERB and target net-\nwork. The resulting rewards on running all four ar-\nchitecture on the 3 × 3 domain (see ﬁgure 2) can be\nseen in ﬁgure 3 a) and the corresponding learned pol-\nicy in ﬁgure 3 b).\nAll graphs have been averaged\nover ten runs. The traditional Q-RL agent without\nany extensions (blue line) learns unstable with occa-\nsional high swings down to -1700 and -1000 reward\npoints. Extended versions seem to be show less out-\nliers. This observation gets more evident, when con-\nducting the same experiment on a bigger 5 × 3 en-\nvironment. As seen in ﬁgure 3 c) - d) the achieved\nrewards of non-extended agents (blue) collapses fre-\nquently. The ERB (black) respectively target network\n(green) alone stabilize learning, but the combination\nof both (red) yields smoothest training curve. Hence,\nthese enhancements are getting more important with\nbigger state space and more complex environments.\nAfter training, we evaluate the resulting policies\nfor 100 episodes without further training.\nThe av-\nerage rewards of ten test-runs on the 3 × 3 domain\ncan be seen in ﬁgure 3 b). As already described, an\nagent is rewarded +220 points for reaching its goal\nand -10 for each taken step. So, when considering\nan optimal policy, the agent would be awarded +190\nfor the 3 × 3 domain (respectively +170 for 5 × 3) if\nthe agent is spawned furthest from its goal and +220\nfor the best starting position. Assuming the starting\npositions over all episodes are distributed evenly, the\noptimal median reward would be at +205 for the 3×3\ndomain and +195 for the 5×3 environment.\nThe traditional QBM agent shows multiple out-\nliers and a higher spread of rewards throughout the\nevaluation episodes compared to the other architec-\ntures. As it can be seen, adding only one of the ex-\ntensions leads to a better median reward and a seem-\ningly optimal policy is gained through a combination\nof both. Again, this observation gets more distinct\nwith bigger domains, see ﬁgure 3 d). Even though\nERB or target network alone signiﬁcantly enhance\nthe median reward, the plots still show outliers. The\ncombined architecture is free of outliers with less in-\nterquartile range and lower overall span indicating re-\n(a) Training: Reward per Episode (3x3)\n(b) Evaluation: Boxplot of Rewards (3x3)\n(c) Training: Reward per Episode (5x3)\n(d) Evaluation: Boxplot of Rewards (5x3)\nFigure 3: Performance of a single agent with different architectures. a) Shows the gained reward per episode on a 3 × 3\ndomain of different architectures, whereas b) displays the corresponding achieved rewards of the learned policy on 400 test\nepisodes. c) illustrates the reward of the same experiment on a 5×3 domain and d) the corresponding learned policy of test\nepisodes.\nduced variance of training performance and nearly\noptimal policy. In summary, alleviating data correla-\ntion and the problems non-stationary distributions by\nrandomly sampling previous transitions and separat-\ning target and policy network increases stable learning\nleading to robust and more optimal policies. Compar-\ning the results for 3 × 3 with the 3 × 5 gridworld, a\ncorrelation of impact through the extensions and in-\nput size can be suspected.\n4.3\nMulti-Agent Results\nTraditional Q-RL was limited so 2×2 multi-agent do-\nmains and bigger domains could not be solved ratio-\nnally (Neumann et al., 2020). This section explores,\nif the proposed architecture enables multi-agent rein-\nforcement learning. We modify the known environ-\nments by adding one agent and one corresponding\ngoal. If an agents picks up the others goal, it is pe-\nnalized with -220. The averaged results over 10 runs\ncan be seen in ﬁgure 4.\nThe graphs suggest, that 3 × 3 domain (blue) can\nbe solved in contrast to the bigger environment (red).\nLooking at ﬁgure 4 b), the median reward of the\nlearned policy on the smaller domain is around +350,\nwhich is near optimum. Unfortunately, the bigger do-\nmain could not be solved with a median reward of\n-450. Additionally, the 5 × 3 learning curve does not\nseem to converge. Therefore, we can conclude, that\nit is possible to solve bigger domains with the pro-\nposed architecture, but Q-RL with ERB and extra tar-\nget network still fails in somewhat larger multi-agent\ndomains.\nLastly, we explore if the cooperation method of\nparameter sharing enhances quantum multi-agent re-\ninforcement learning.\nWith parameter sharing no\nexplicit communication is necessary since only one\ncentralized entity is trained and shared between the\nagents.\nMore speciﬁcally, the experience of every\nagent is stored in a centralized ERB. At each train-\ning step, one QBM is trained with a randomized sam-\nple from the ERB similar to the single-agent case.\nBoth agents use this network to independently calcu-\nlate their Q-values based their observation. By this,\nwe additionally smooth the data distribution hoping\nto achieve a more general policy and not two speciﬁc\npolicies adjusted to particular observations.\nThe results with and without parameter sharing\nare illustrated in ﬁgure 5. Unfortunately, parameter\nsharing seems to have a negative effect on the small\n3 × 3 domain. In this case, the agents seem to have\nlearned a worse policy with this adaption. Rewards\non the bigger environments have increased. However,\nthe 5 × 3 domain can still not be considered solved.\na) Learning Process on Both Domains\nb) Learned Policy on Both Domains\nFigure 4: Performance of two agents on the 3×3 (blue) re-\nspectively 3×5 (red) domain. Figure a) shows the learning\nprocess over 500 episodes, whereas ﬁgure b) displays the\nlearned policy over 100 testing episodes.\nHence, parameter sharing is sub-optimal for the eval-\nuated use case.\nThe complexity of the task and size of the in-\nput did not increase, so this observation is counter-\nintuitive. Since the centralized entity is simultane-\nously learning two independent behaviors, it might be\npossible that in this case two independently optimal\naction-state probability distributions (as learned with-\nout parameter sharing) cancel out each other when\nlearned together. To proof this assumption, more ex-\nperiments must be conducted.\n5\nDiscussion\nIn summary, adding an ERB and additional target\nnetwork alleviates data correlation and the problem\nof non-stationary distribution resulting in stabilized\nlearning and a more optimal policies. With the pro-\nposed architecture, we were able to solve bigger\nenvironments compared to traditional MARL using\nQBMs. However, this architecture is still limited to\nrelatively small domains.\na) Learned Policy (3x3)\nb) Learned Policy (5x3)\nFigure 5: Performance of a two agents with and without pa-\nrameter sharing. a) Shows the gained reward of the learned\npolicy of 100 testing episodes on a 3 × 3 domain, whereas\nb) displays the same experiment on the bigger environment.\nEven though it is possible to coordinate a single\nagent in the 5 × 3 domain and multiple agents in a\nsmaller domain. The question remains why the 5×3\nmulti-agent domain fails. The QBM-agent receives\nan input of 15 neurons on 5 × 3 single-agent domain\nsince only one input layer is needed. When adding\nmore agents to the environment, there is another in-\nput layer necessary in order to distinguish between\nthe acting agent and other opposing agents. Hence,\nthe 3 × 3 multi-agent domain returns an observation\nsize of 18 and bigger multi-agent domain of size 30.\nThe input are considered in the QUBO formulation,\nwhich therefore increases. Hence, simulated quan-\ntum annealing is applied to a bigger formulation. A\nbigger formulation demands more qubits, which may\nlimit the accuracy, variation and stability of the quan-\ntum annealing algorithm. This is only an assumption\nand needs to be examined more closely. Neumann et\nal. (2020) also already stated, that Q-RL is limited\nby the current Quantum Processing Unit (QPU) size.\nHowever, with the extension of an Experience Replay\nBuffer and Target Network, we are able to stabilize\nlearning and therefore may reduce the needed QPU\nsize compare to previous approaches.\nQuantum sampling has been proven to be a\npromising method to enhance reinforcement learn-\ning tasks to speed-up learning in relation to needed\ntime steps (Neumann et al., 2020). Further work con-\ncerning the relation between QPU size and domain\ncomplexity (respectively state input) would needed to\nstrictly determine current limitations.\nACKNOWLEDGEMENTS\nThis work was funded by the BMWi project PlanQK\n(01MK20005I).\nREFERENCES\nAckley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). A\nlearning algorithm for boltzmann machines. Cognitive\nScience, 9(1):147 – 169.\nBenedetti, M., Realpe-G´omez, J., and Perdomo-Ortiz, A.\n(2018).\nQuantum-assisted helmholtz machines: A\nquantum–classical deep learning framework for in-\ndustrial datasets in near-term devices. Quantum Sci-\nence and Technology, 3(3):034007.\nBiamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe,\nN., and Lloyd, S. (2017). Quantum machine learning.\nNature, 549(7671):195–202.\nBinmore, K. (2007). Game Theory: A Very Short Introduc-\ntion. Oxford University Press.\nCharpentier, A., Elie, R., and Remlinger, C. (2020). Rein-\nforcement learning in economics and ﬁnance.\nCrawford, D., Levit, A., Ghadermarzy, N., Oberoi, J. S.,\nand Ronagh, P. (2019). Reinforcement learning using\nquantum boltzmann machines.\nFoerster, J. N., Assael, Y. M., de Freitas, N., and Whiteson,\nS. (2016). Learning to communicate with deep multi-\nagent reinforcement learning.\nFoerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and\nWhiteson, S. (2017). Counterfactual multi-agent pol-\nicy gradients. In AAAI.\nJerbi, S., Trenkwalder, L. M., Nautrup, H. P., Briegel, H. J.,\nand Dunjko, V. (2020). Quantum enhancements for\ndeep reinforcement learning in large spaces.\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov,\nM., Tunyasuvunakool, K., Ronneberger, O., Bates,\nR., ˇZ´ıdek, A., Bridgland, A., Meyer, C., Kohl, S.\nA. A., Potapenko, A., Ballard, A. J., Cowie, A.,\nRomera-Paredes, B., Nikolov, S., Jain, R., Adler,\nJ., Back, T., Petersen, S., Reiman, D., Steinegger,\nM., Pacholska, M., Silver, D., Vinyals, O., Senior,\nA. W., Kavukcuoglu, K., Kohli, P., and Hassabis, D.\n(2020). High accuracy protein structure prediction us-\ning deep learning. In Fourteenth Critical Assessment\nof Techniques for Protein Structure Prediction (Ab-\nstract Book), 14.\nKiran, B. R., Sobh, I., Talpaert, V., Mannion, P., Sallab, A.\nA. A., Yogamani, S., and P´erez, P. (2020). Deep rein-\nforcement learning for autonomous driving: A survey.\nLevit, A., Crawford, D., Ghadermarzy, N., Oberoi, J. S.,\nZahedinejad, E., and Ronagh, P. (2017). Free energy-\nbased reinforcement learning using a quantum proces-\nsor.\nLi, R. Y., Di Felice, R., Rohs, R., and Lidar, D. A. (2018).\nQuantum annealing versus classical machine learning\napplied to a simpliﬁed computational biology prob-\nlem. npj Quantum Information, 4(1).\nMahmud, M., Kaiser, M. S., Hussain, A., and Vassanelli,\nS. (2018).\nApplications of deep learning and rein-\nforcement learning to biological data. IEEE Trans-\nactions on Neural Networks and Learning Systems,\n29(6):2063–2079.\nMcclelland, J., Mcnaughton, B., and O’Reilly, R. (1995).\nWhy there are complementary learning systems in the\nhippocampus and neocortex: Insights from the suc-\ncesses and failures of connectionist models of learning\nand memory. Psychological review, 102:419–57.\nMirhoseini, A., Goldie, A., Yazgan, M., Jiang, J., Songhori,\nE., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Bae,\nS., Nazi, A., Pak, J., Tong, A., Srinivasa, K., Hang, W.,\nTuncer, E., Babu, A., Le, Q. V., Laudon, J., Ho, R.,\nCarpenter, R., and Dean, J. (2020). Chip placement\nwith deep reinforcement learning.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A., Veness, J.,\nBellemare, M., Graves, A., Riedmiller, M., Fidjeland,\nA., Ostrovski, G., Petersen, S., Beattie, C., Sadik,\nA., Antonoglou, I., King, H., Kumaran, D., Wierstra,\nD., Legg, S., and Hassabis, D. (2015). Human-level\ncontrol through deep reinforcement learning. Nature,\n518:529–33.\nMorita, S. and Nishimori, H. (2008). Mathematical founda-\ntion of quantum annealing. Journal of Mathematical\nPhysics, 49(12):125210.\nNeukart, F., Compostella, G., Seidel, C., von Dollen, D.,\nYarkoni, S., and Parney, B. (2017a). Trafﬁc ﬂow opti-\nmization using a quantum annealer.\nNeukart, F., Dollen, D. V., Seidel, C., and Compostella, G.\n(2017b). Quantum-enhanced reinforcement learning\nfor ﬁnite-episode games with discrete state spaces.\nNeumann, N., Heer, P., Chiscop, I., and Phillipson, F.\n(2020). Multi-agent reinforcement learning using sim-\nulated quantum annealing.\nNeven, H., Denchev, V. S., Rose, G., and Macready, W. G.\n(2008). Training a binary classiﬁer with the quantum\nadiabatic algorithm.\nO’Neill,\nJ.,\nPleydell-Bouverie,\nB.,\nDupret,\nD.,\nand\nCsicsvari, J. (2010).\nPlay it again: Reactivation of\nwaking experience and memory.\nTrends in neuro-\nsciences, 33:220–9.\nPaparo, G. D., Dunjko, V., Makmal, A., Martin-Delgado,\nM. A., and Briegel, H. J. (2014). Quantum speedup\nfor active learning agents. Physical Review X, 4(3).\nPeng, J. and Williams, R. J. (1994).\nIncremental multi-\nstep q-learning. In Cohen, W. W. and Hirsh, H., edi-\ntors, Machine Learning Proceedings 1994, pages 226\n– 232, San Francisco (CA). Morgan Kaufmann.\nPuterman, M. L. (1994). Markov Decision Processes: Dis-\ncrete Stochastic Dynamic Programming. John Wiley\n& Sons, Inc., New York, NY, USA, 1st edition.\nRebentrost, P., Mohseni, M., and Lloyd, S. (2014). Quan-\ntum support vector machine for big data classiﬁcation.\nPhysical Review Letters, 113(13).\nRummery, G. and Niranjan, M. (1994).\nOn-line q-\nlearning using connectionist systems. Technical Re-\nport CUED/F-INFENG/TR 166.\nSallans, B. and Hinton, G. E. (2004). Reinforcement learn-\ning with factored states and actions. J. Mach. Learn.\nRes., 5:1063–1088.\nSilver, D., Huang, A., Maddison, C., Guez, A., Sifre, L.,\nDriessche, G., Schrittwieser, J., Antonoglou, I., Pan-\nneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lill-\nicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\nand Hassabis, D. (2016). Mastering the game of go\nwith deep neural networks and tree search. Nature,\n529:484–489.\nStrehl, A. L., Li, L., Wiewiora, E., Langford, J., and\nLittman, M. L. (2006). Pac model-free reinforcement\nlearning.\nIn Proceedings of the 23rd International\nConference on Machine Learning, ICML ’06, pages\n881–888, New York, NY, USA. ACM.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learn-\ning: An Introduction. A Bradford Book, Cambridge,\nMA, USA.\nTan, M. (1993). Multi-agent reinforcement learning: In-\ndependent vs. cooperative agents. In In Proceedings\nof the Tenth International Conference on Machine\nLearning, pages 330–337. Morgan Kaufmann.\nWiebe, N., Braun, D., and Lloyd, S. (2012). Quantum algo-\nrithm for data ﬁtting. Physical Review Letters, 109.\n",
  "categories": [
    "cs.AI",
    "cs.MA"
  ],
  "published": "2021-09-22",
  "updated": "2021-11-22"
}