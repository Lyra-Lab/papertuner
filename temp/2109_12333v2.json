{
  "id": "http://arxiv.org/abs/2109.12333v2",
  "title": "Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification",
  "authors": [
    "Zheng Hu",
    "Chuang Zhu",
    "Gang He"
  ],
  "abstract": "Unsupervised person re-identification (Re-ID) is a promising and very\nchallenging research problem in computer vision. Learning robust and\ndiscriminative features with unlabeled data is of central importance to Re-ID.\nRecently, more attention has been paid to unsupervised Re-ID algorithms based\non clustered pseudo-label. However, the previous approaches did not fully\nexploit information of hard samples, simply using cluster centroid or all\ninstances for contrastive learning. In this paper, we propose a Hard-sample\nGuided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss\nwith instance-level loss for unsupervised person Re-ID. Our approach applies\ncluster centroid contrastive loss to ensure that the network is updated in a\nmore stable way. Meanwhile, introduction of a hard instance contrastive loss\nfurther mines the discriminative information. Extensive experiments on two\npopular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms\nprevious state-of-the-art methods and significantly improves the performance of\nunsupervised person Re-ID. The code of our work is available soon at\nhttps://github.com/bupt-ai-cz/HHCL-ReID.",
  "text": "Hard-sample Guided Hybrid Contrast Learning\nfor Unsupervised Person Re-Identification\nZheng Hu1, Chuang Zhu1*, Gang He1\n1Beijing Laboratory of Advanced Information Networks\nBeijing Key Laboratory of Network System Architecture and Convergence\nBeijing University of Posts and Telecommunications\nBeijing 100876, China\n{huzheng95, czhu, brianhe}@bupt.edu.cn\nAbstract\nUnsupervised person re-identification (Re-ID) is a\npromising and very challenging research problem in com-\nputer vision. Learning robust and discriminative features\nwith unlabeled data is of central importance to Re-ID. Re-\ncently, more attention has been paid to unsupervised Re-\nID algorithms based on clustered pseudo-label. However,\nthe previous approaches did not fully exploit information\nof hard samples, simply using cluster centroid or all in-\nstances for contrastive learning. In this paper, we propose\na Hard-sample Guided Hybrid Contrast Learning (HHCL)\napproach combining cluster-level loss with instance-level\nloss for unsupervised person Re-ID. Our approach applies\ncluster centroid contrastive loss to ensure that the network\nis updated in a more stable way. Meanwhile, introduction\nof a hard instance contrastive loss further mines the dis-\ncriminative information. Extensive experiments on two pop-\nular large-scale Re-ID benchmarks demonstrate that our\nHHCL outperforms previous state-of-the-art methods and\nsignificantly improves the performance of unsupervised per-\nson Re-ID. The code of our work and dataset are avail-\nable soon at https://github.com/bupt-ai-cz/\nHHCL-ReID.\nKeywords: Unsupervised Learning; Person Re-ID; Hard\nSample; Contrastive Learning, Pseudo Label.\n1. Introduction\nPerson Re-ID aims to identify the same person under dif-\nferent cameras views. It has been used extensively in large-\nscale surveillance systems. Though great progress has been\nmade in supervised person Re-ID tasks, the reliance on ex-\ntensive manual annotation greatly constrains its application.\n*the corresponding author: Chuang Zhu (czhu@bupt.edu.cn)\nCluster centroid\nHard Sample\nMemory Sample\n（b）Hard Instance-level Contrastive Loss\nPull\nPush\n（a）Cluster-level Contrastive Loss\nNew Sample\nFigure 1. Hard-sample guided hybrid contrast learning. Accord-\ning to the features saved in the memory bank, we calculate cluster-\nlevel contrastive loss and hard instance-level contrastive loss, re-\nspectively. (a) Cluster centroid leads the optimization trend of\nfeatures, resulting in features belonging to the same cluster being\nmore compact and strengthen identity similarity. (b) Hard instance\ncontrastive loss compares input sample with hard positive that be-\nlong to the same cluster and hard negative instances from other\nclusters, thereby learning to distinguish easily confusing samples.\n(Best viewed in color)\nNevertheless, collecting pedestrian images without annota-\ntion is much cheaper and easier. Thus, increasing research\nattention has been drawn to unsupervised person Re-ID, di-\nrectly learning from unlabeled data, which is more scalable\nand has more potential to deployments in the real world.\nThe extant unsupervised person re-ID methods can be\nbroadly divided into two categories, unsupervised domain\nadaptation Re-ID methods and purely unsupervised Re-ID\nmethods. The first type methods are based on unsupervised\ndomain adaption (UDA) where the source domain dataset\nis fully annotated and the target domain is an unlabeled\ndataset. Most of these UDA-based methods address this\ntask by learning the knowledge in the labeled source do-\nmain dataset and transferring them to the unlabeled target\ndomain dataset [32, 1, 8].\nThe second type of unsuper-\nvised Re-ID method is pseudo-label-based fully unsuper-\nvised learning that directly learn from unlabeled data in the\ntarget domain and use representation features to estimate\npseudo labels [23, 29, 9]. This method does not require any\nannotations and is more challenging. Existing fully unsu-\npervised Re-ID works mainly aim to exploit pseudo labels\nfrom clustering and apply contrastive learning which has\nshown excellent performance in unsupervised representa-\ntion learning [27, 3, 11].\nThe performance of the unsupervised methods relies on\nfeature representation learning. More recently, the State-\nof-the-art method [11] using a memory bank unit [28] to\nstore all instance features, treats each image as an individual\nclass, and learns the representation by matching features of\nthe same instance in different augmented views. However,\neach class usually contains more than one positive instance\nin Re-ID datasets. SpCL [9] method alleviates this problem\nby matching an instance with the centroid of the multiple\npositives. To further ensure each positive converges to its\ncentroid at a uniform pace, cluster contrast learning [4] up-\ndates the memory dictionary and computes contrastive loss\nin the cluster level.\nAlthough cluster contrast learning [4] has achieved im-\npressive performance, the method of applying contrastive\nlearning only in the cluster level does not consider the the\nrelationship between hard instances in the instance level.\nIn fact, previous works in deep metric learning have fo-\ncused on hard sample mining to lay more emphasis on hard\nsamples inside a class. These methods aim to distinguish\nsamples from different categories and bring samples from\nthe same category closer together. However, these methods\nusually adopt a mini-batch-based deep metric loss, such as\nhard triplet loss [13] and multi-similarity loss [25]. Mean-\nwhile, these losses only utilized a small portion of data\nwithout considering the information of all categories.\nTo learn discriminative feature representation for Re-ID\nand address the lack of adequately exploring information\nof hard samples, this paper introduces a novel hard-sample\nmining strategy and proposes a simple and effective method\nof hard-sample guided hybrid contrast learning for unsuper-\nvised Re-ID. In summary, this paper makes the following\ncontributions:\n• We propose a hybrid contrast learning framework\nfor unsupervised person Re-ID which combines both\ncluster-level contrastive loss and instance-level con-\ntrastive loss.\n• We introduce a novel hard instance mining strategy,\nwhich is based on an instance memory bank, to explore\nmore discriminative information by selecting global\nhard samples online for each input instance.\n• Extensive experiments on two popular large-scale Re-\nID benchmarks demonstrate that our HHCL outper-\nforms previous state-of-the-art methods and signifi-\ncantly improves the performance of unsupervised per-\nson Re-ID.\n2. Related Works\n2.1. Unsupervised Re-ID\nThe domain adaptation strategy has been widely used for\nunsupervised person Re-ID tasks [1, 8]. The transfer-based\nmethods follow the strategy of UDA, which uses the pre-\ntrained model in the labeled source domain dataset as the\ninitialization of the target domain, or uses the style trans-\nfer method to transfer labeled images to the target domain.\nHowever, the UDA approach can be very challenging when\nthe categories in the two domains are quite different. The\ndrawback with pseudo-labels is that if the domains are not\nsimilar enough, it is not easy for us to obtain high quality\npseudo labels, because the labeling noise might be too high\nto hurt the performance.\nMore recently, researchers have given more attention to\npseudo-label-based methods that do not require source do-\nmain data. The pseudo labels can be generated by a pre-\ntrained classifier or by a feature similarity-based cluster-\ning algorithm, such as K-means, DB-SCAN [6]. In this\nway, the pseudo labels are applied to fine-tuning the Re-ID\nmodel in a supervised manner. HCT [31] combined hier-\narchical clustering with hard-batch triplet loss to improve\nthe quality of pseudo labels. MMCL [22] formulated unsu-\npervised person re-ID as a multi-label classification task to\nprogressively seek true labels. SpCL [9] adopted the self-\npaced contrastive learning strategy to form more reliable\nclusters. CACL [16] designed an asymmetric contrastive\nlearning framework to help the siamese network effectively\nmine the invariance in feature learning.\n2.2. Mining Schemes\nSampling is a fundamental operation for reducing bias\nduring model learning.\nRandom sampling is one of the\ncommonly used approaches, and different sampling meth-\nods are proposed to facilitate the learning of various loss\nfunctions. For the person re-ID task, identity sampling is\nwidely used during the training stage, such as pair-wise\nsampling for contrastive loss and semi-hard negative min-\ning method for triplet loss.\nHard sample mining is considered as a vital component\nof many deep metric learning algorithms [28] to acceler-\nate network convergence or to improve the final discrimi-\nnative ability of the neural network because hard samples\nare more informative for training. The training should fo-\ncus more on hard samples than easy samples. However, ex-\nisting hard mining schemes of deep metric learning based\non mini-batch training data often suffer from slow conver-\ngence, because they employ only one negative or partial\nnegative example in mini-batch while not interacting with\nthe other negative classes that have not been sampled into\nthe current mini-batch in each update. In this paper, we pro-\npose a new strategy selecting the global hard samples from\na memory bank for each input feature, to improve the model\nperformance. Our hard mining strategy considers the rela-\ntionship between each query instance and other clusters of\ndifferent pseudo labels rather than taking into account only\nthe inter-instance relationship with a small fraction of the\ncategories.\n3. Preliminaries\nGiven an unlabeled training set X = {x1, x2, ..., xn}\nconsisting of n image samples, the goal is to learn\nϕ(·; θ)—an encoder parameterized by θ used to extract fea-\ntures from input images. For inference, this encoder is ap-\nplied to the gallery set X g = {xg\n1, xg\n2, ..., xg\nng} and query\nset X q = {xq\n1, xq\n2, ..., xq\nnq}. The gallery set contains the\ntotal collection of retrieval images in the database and rep-\nresentations of the query images ϕ(xq\ni ; θ) are used to search\nthe gallery set to retrieve the most similar matches to xq\ni ac-\ncording to Euclidean distance between the query and gallery\nembeddings, d(xq, xg) = ||ϕ(xq; θ) −ϕ(xg; θ)||, where a\nsmaller distance implies increased similarity between the\nimages. Thus, feature representations of the same person\nare supposed to be as close as possible.\n4. Method\n4.1. Architecture\nOur hybrid contrast learning framework for fully unsu-\npervised Re-ID consists of two main components: Cluster\nCentroid Contrastive Loss (CCCL) and Hard Instance Con-\ntrastive Loss (HICL). As shown in Fig.2.\n4.2. Hybrid Contrast Learning\nTo increase intra-class compactness and inter-class sepa-\nrability, state-of-the-art contrastive learning methods mini-\nmize the distance between samples of the same category and\nmaximize the distance between samples of different cate-\ngories with InfoNCE loss [21].\n  \\ mat hca\nl {L} _ {q}=\\\nma\nthb b {E} [  -\\lo\ng \\frac {\\exp (q \\cdot k^{+})/\\tau }{\\sum _{i=1}^{K}\\exp (q \\cdot k^i)/\\tau }], \\tag {1} \\label {eq:InfoNCE-Loss} \n(1)\nwhere q is an encoded query and k+ is a positive feature\nwhich has the same label with q selected from a set of can-\ndidates k1, k2, ...kK. τ is a temperature hyper-parameter\nthat controls the scale of similarities.\nComparing the non-parametric loss functions of different\napproaches based on the memory dictionary, the SSL [17]\nconsiders each image as an individual instance and com-\nputes the loss and updates the memory dictionary both in the\ninstance level so that all features of the training data need to\nbe saved. To decrease memory usage and take full advan-\ntage of clustering outliers, SPCL [9] computes the loss in\ncluster level but updates the memory dictionary in the in-\nstance level. However, the updating progress for each clus-\nter is inconsistent due to the varying cluster size and ran-\ndomness of sampling. ClusterNCE loss [4] updates the fea-\nture vectors and computes the loss both in the cluster level.\nAlthough only a smaller storage space needs to be created to\nhold a cluster size amount of features for ClusterNCE, a sin-\ngle feature vector is not enough for a cluster representation.\nThe averaged momentum representations calculated from\nall instances belonging to one cluster may lose the intra-\nclass diversity. If updating cluster representation with only\nan instance feature, would introduce more biases because of\nnoisy pseudo labels generated by unsupervised clustering.\nThus, we proposed a new unsupervised Re-ID frame-\nwork that combines cluster-level loss with instance-level\nloss. The overall loss function of our method is as follow:\n  \\ma t hcal { L} _ {ReID}=\\mu \\mathcal {L}_{cls} + (1 - \\mu )\\mathcal {L}_{ins}, \\tag {2} \\label {eq:Total-Loss} \n(2)\nwhere µ is a balancing factor and we set µ = 0.5 by de-\nfault. In the following, we will detail the objective function\nEq.(2).\nCluster Centroid Contrastive Loss Some instance-\nlevel memory dictionary techniques, such as [22, 8] main-\ntaining each instance feature of the dataset and update corre-\nsponding memory dictionary with its own instance features\nin each mini-batch, have the problem of memory updating\nconsistency [4]. Since different instances within the same\ncluster will have different updating states. In every training\niteration, due to the unbalanced distribution of cluster size,\na smaller cluster could have a higher proportion of instances\nupdated than a larger cluster. Unlike the previous instance-\nlevel memory dictionary, we use cluster-level memory dic-\ntionary Mcls to keep one cluster feature for each cluster in-\nstead of preserving every instance feature. The correspond-\ning memory dictionary is updated regardless of whether the\nclusters are large or small, ensuring updating consistency of\nfeatures within the same cluster.\n  \\m a thc al \n{L}_{ c l s} =\\mat\nhb\nb { E}[ - \\ l og  \\fra\nc {\\exp (<q \\cdot c^{+}>)/\\tau _c}{\\sum _{i=1}^{C}\\exp (<q \\cdot c^i>)/\\tau _c}], \\tag {3} \\label {eq:CC-Loss} \n(3)\nwhere C is the number of clusters in a training epoch and\nτc is a temperature hyper-parameter. Different from unified\ncontrastive loss, outliers are dropped out.\nWe calculate cluster centroids c1, c2, ...cCand store them\nin a memory for the cluster centroid contrastive loss. We\nupdate the cluster memory bank as follows:\n  { c^i  \\l e ftarrow \\alpha c^i + (1-\\alpha ) \\bar {c}^i}, \\tag {4} \\label {eq:Update Cluster Memory} \n(4)\nwhere ¯ci is the average of i-th class instance features in the\nmini-batch.\nFeature\nembedding \nCluster Centroid Memory\nInstance Memory\nBack Propagate\nBack Propagate\nUpdate\nCluster Centroid Feature\nInstance Embedding  Feature\nFlow\nBack Propagate\nHard Samples\nSelect\nPseudo Labels\nUpdate\nMini Batch Samples\nFigure 2. Hybrid Contrast Learning Framework. 1) Initialization: clustering algorithm divides all features extracted from the training\nset into different clusters as pseudo labels and initialize instance memory bank and cluster centroid memory bank; 2) forward propagate:\ncalculate cluster contrastive loss between the input and the clustering centroids and the hard instance contrastive loss of the hard samples\nselected by hard mining strategy respectively; 3) back propagate and update the encoder model; 4) update instance memory bank and\ncluster centroid memory bank.\n4.3. Memory Based Hard Mining Scheme\nTo further distinguish easily confused sample pairs and\nexplore the inter-instance relationship, we propose a novel\nhard sample mining strategy based on a memory dictionary.\nWe construct another memory-based dictionary Mins to\nstore P × C instance features, which contains C pseudo\nidentities and each identity has P instances. As shown in\nFig.1, unlike traditional hard mining strategies such as hard\ntriplet loss [13], which is based on pairwise loss calculating\nthe distance of the hardest positive and the hardest nega-\ntive instances within a mini-batch, our proposed method is\nbased on all pseudo-labeled categories and contains C −1\nnegative samples for each query. Our hard mining strat-\negy considers the relationship between each query instance\nand other clusters of different pseudo labels rather than tak-\ning into account only the inter-instance relationship with a\nsmall fraction of the categories.\nFor the same query, we construct C sample pairs which\ninclude one positive pair and C −1 hard negative pairs. We\ndefine hard instance contrastive loss as follows:\n  \\m a thc al \n{L}_{ i n s}\n=\\ma thbb {E\n}[\n -\\ log \\ f r ac\n {\\e xp (<q \n\\cdot z^{+}_{hard}>)/\\tau _{ins}}{\\sum _{i=1}^{C}\\exp (<q \\cdot z^i_{hard}>)/\\tau _{ins}}], \\tag {5} \\label {eq:HIC-Loss} \n(5)\nwhere τins is an instance temperature hyper-parameter,\nz+\nhard is the hard positive instance feature that has the lowest\ncosine similarity with query q within the same cluster, and\nzi\nhard is hard negative instance feature that has the highest\ncosine similarity that belongs to i-th class. They are respec-\ntively defined as\n  \n{z^+ _ {hard} =  a rg\nm in( < q  \\ cdot  z^+_{k}>), {k = 1,...,K}}; \\tag {6} \\label {eq:Select Hard Pos Instance} \n(6)\n  \n{z^i _ {hard} = argmax(<q \\cdot z^i_{k}>), {k = 1,...,K}}. \\tag {7} \\label {eq:Select Hard Neg Instance} x\n(7)\nSimilarly, to ensure memory updating consistency, all in-\nstance features of the corresponding K identities in the mini-\nbatch are updated in each training iteration. We update the\ninstance memory bank as follows:\n  \n{ m ^i\n_{k} \\leftarrow z^i_k}, \\tag {8} \\label {eq:Update Hard Memory} \n(8)\n5. Experiments\n5.1. Data and Metrics\nWe evaluate our approach on two large-scale bench-\nmark datasets: Market1501 [33], and DukeMTMC-reID\n[35] which are widely used real-world person Re-ID tasks.\nMarket1501 contains 1,501 person identities with\n32,668 images which are captured by 6 cameras in front\nof the Tsinghua University campus. It contains 12,936 im-\nages of 751 identities for training and 19,732 images of 750\nidentities for testing. All of the images were cropped by a\npedestrian detector which inevitably introduced little mis-\nalignment, part missing and false positives.\nAlgorithm 1: Hard-sample Guided Hybrid Con-\ntrast Learning for Unsupervised Re-Identification\nData: An unlabeled training set X\nInput: ImageNet pre-trained model ϕ(·; θ), the\niteration number N, the training batch size\nB.\nResult: trained model ϕ(·; θ)\n1 for epoch = 1 to N do\n2\nExtract feature embedding x from X by ϕ(·; θ);\n3\nClustering x into c clusters with DB-Scan;\n4\nInitialize cluster memory bank Mcls and hard\ninstance memory bank Mins with Eq.1;\n5\nfor iter = 1 to B do\n6\nSample P × K mini-batch images from X;\n7\nForward to extract the features of the\nsamples;\n8\nCompute the total loss LReID in Eq.(2)\nwhich combines cluster centroid\ncontrastive loss Lcls Eq.(3) and hard\ninstance contrastive loss Lins Eq.(5);\n9\nBackward to update model ϕθ;\n10\nUpdate cluster memory Mcls and instance\nmemory Mins via Eq.(4) and Eq.(8);\n11\nend\n12 end\nDukeMTMC-reID consists a total of 36,411 images of\npeople from 1404 different identities collected by 8 cam-\neras. Specifically, The dataset is split by randomly select-\ning 702 identities as the training set and 702 identities as\nthe testing set. it contains 16,522 images for training, 2,228\nquery images and 17,661 gallery images for testing.\nEvaluation Metrics. We followed the standard train-\ning/test split and evaluation protocol to evaluate the per-\nformance of our method. For the evaluation metrics, we\nused the Rank-k (for k = 1, 5, and 10) matching accuracy,\nwhich means the query picture has the match in the top-k\nlist. And we use the mean Average Precision (mAP), which\nis computed from the Cumulated Matching Characteristics\n(CMC) [10]. Moreover, results reported in this paper are\nunder the single-query setting, and no post-processing tech-\nnique is applied.\n5.2. Implementation\nWe adopt ResNet-50 [12] as the backbone of the fea-\nture extractor and initialize the model with the parame-\nters pre-trained on ImageNet [5].\nAfter layer-4, we re-\nmove all sub-module layers and add global average pooling\n(GAP) followed by batch normalization layer [14] and L2-\nnormalization layer, which will produce 2048-dimensional\nfeatures. During testing, we take the features of the global\naverage pooling layer to calculate the distance. For the be-\nginning of each epoch, we use DB-SCAN [6] for cluster-\ning to generate pseudo labels. The input image is resized\n256 × 128. For training images, we perform random hor-\nizontal flipping, padding with 10 pixels, random cropping,\nand random erasing. Each mini-batch contains 256 images\nof 16 pseudo person identities (16 instances for each per-\nson). We adopt Adam optimizer to train the Re-ID model\nwith weight decay 5e-4. The initial learning rate is set to\n3.5e-4, and is reduced to 1/10 of its previous value every 20\nepoch in a total of 50 epoch. As the same with the cluster\nmethod of [9] paper, we use DB-SCAN and Jaccard dis-\ntance [36] to cluster with k nearest neighbors, where k = 30.\nFor DB-SCAN, the maximum distance d between two sam-\nples is set as 0.45 and the minimal number of neighbors in\na core point is set as 4.\n5.3. Results\n5.3.1\nComparison with unsupervised method\nWe compare our proposed method with state-of-the-\nart ReID methods including:\n1) the unsupervised do-\nmain adaptation methods for person Re-ID(e.g.\nECN\n[37], MAR[37], SSG[7], MMCL[22], JVTC[15], DG-\nNet++[40], ECN+[38], MMT[8], DCML[1], MEB[32],\nSpCL [9]; 2) the purely unsupervised methods for per-\nson Re-ID SSL[9], MMCL[22], JVTC[15], HCT[31],\nCycAs[26], SpCL[9], CAP[24], CACL [16], CCL [4] and\nICE[2]). The comparison results of the state-of-the-art un-\nsupervised domain adaptation methods and purely unsuper-\nvised methods on Market-1501 and DukeMTMC-reID are\nreported in Tab. 1.\nAs shown in Tab.1, we observe our method is compet-\nitive with all the state-of-the-art methods.\nOn the three\ndatasets, our proposed HHCL without any identity annota-\ntion achieves better performance than all of UDA methods\nthat use of the additional labeled source dataset. It can be\nfound that we not only perform better than all unsupervised\ndomain adaptation methods and also achieve competitive\nperformance with purely unsupervised methods. Under the\nfully unsupervised setting, HHCL achieves 84.2% in mAP\nand 93.4% in rank-1 accuracy on Market-1501, which is\n1.9% higher than the current state of the art (ICE [4]). On\nDukeMTMC-reID, our method also achieves a high perfor-\nmance of 73.3/85.1% in mAP/rank-1. These results indi-\ncate that our method is effective for unsupervised person\nRe-ID learning.\n5.3.2\nComparison with supervised method\nOur HHCL method can be easily implemented as a su-\npervised approach when we replace the pseudo-labels with\nground truth. We further find that our proposed unsuper-\nvised method is already comparable to some excellent su-\nMethod\nReference\nMarket1501\nDukeMTMC-reID\nmAP\nR1\nR5\nR10\nmAP\nR1\nR5\nR10\nUnsupervised Domain Adaptation\nECN [37]\nCVPR’19\n43.0\n75.1\n87.6\n91.6\n40.4\n63.3\n75.8\n80.4\nMAR[30]\nCVPR’19\n40.0\n67.7\n81.9\n-\n48.0\n67.1\n79.8\n-\nSSG[7]\nICCV’19\n58.3\n80.0\n90.0\n92.4\n53.4\n73.0\n80.6\n83.2\nMMCL [22]\nCVPR’20\n60.4\n84.4\n92.8\n95.0\n51.4\n72.4\n82.9\n85.0\nJVTC [15]\nECCV’20\n61.1\n83.8\n93.0\n95.2\n56.2\n75.0\n85.1\n88.2\nDG-Net++ [40]\nECCV’20\n61.7\n82.1\n90.2\n92.7\n63.8\n78.9\n87.8\n90.4\nECN+ [38]\nPAMI’20\n63.8\n84.1\n92.8\n95.4\n54.4\n74.0\n83.7\n87.4\nMMT [8]\nICLR’20\n71.2\n87.7\n94.9\n96.9\n65.1\n78.0\n88.8\n92.5\nDCML [1]\nECCV’20\n72.6\n87.9\n95.0\n96.7\n63.3\n79.1\n87.2\n89.4\nMEB [32]\nECCV’20\n76.0\n89.9\n96.0\n97.5\n66.1\n79.6\n88.3\n92.2\nSpCL [9]\nNeurIPS’20\n76.7\n90.3\n96.2\n97.7\n68.8\n82.9\n90.1\n92.5\nFully Unsupervised\nSSL [17]\nCVPR’20\n37.8\n71.7\n83.8\n87.4\n28.6\n52.5\n63.5\n68.9\nJVTC [15]\nECCV’20\n41.8\n72.9\n84.2\n88.7\n42.2\n67.6\n78.0\n81.6\nMMCL [22]\nCVPR’20\n45.5\n80.3\n89.4\n92.3\n40.2\n65.2\n75.9\n80.0\nHCT [31]\nCVPR’20\n56.4\n80.0\n91.6\n95.2\n50.7\n69.6\n83.4\n87.4\nCycAs [26]\nECCV’20\n64.8\n84.8\n-\n-\n60.1\n77.9\n-\n-\nSpCL [9]\nNeurIPS’20\n73.1\n88.1\n95.1\n97.0\n65.3\n81.2\n90.3\n92.2\nCAP [24]\nAAAI’21\n79.2\n91.4\n96.3\n97.7\n67.3\n81.1\n89.3\n91.8\nCACL [16]\nArxiv’21\n80.9\n92.7\n97.4\n98.5\n69.6\n82.6\n91.2\n93.8\nICE[2]\nICCV’ 21\n82.3\n93.8\n97.6\n98.4\n69.9\n83.3\n91.5\n94.1\nCCL[4]\nArxiv’21\n82.6\n93.0\n97.0\n98.1\n72.8\n85.7\n92.0\n93.5\nHHCL\nThis paper\n84.2\n93.4\n97.7\n98.5\n73.3\n85.1\n92.4\n94.6\nSupervised\nPCB[20]\nECCV’18\n81.6\n93.8\n97.5\n98.5\n69.2\n83.3\n90.5\n92.5\nOSNet [39]\nICCV’ 19\n84.9\n94.8\n-\n-\n73.5\n88.6\n-\n-\nDG-Net [34]\nCVPR’19\n86.0\n94.8\n-\n-\n74.8\n86.6\n-\n-\nICE [2] (w/ GT)\nICCV’ 21\n86.6\n95.1\n98.3\n98.9\n76.5\n88.2\n94.1\n95.7\nHHCL(w/ GT)\nThis paper\n87.2\n94.6\n98.5\n99.1\n80.0\n89.8\n95.2\n96.7\nTable 1. Experimental results of the proposed HHCL and state-of-the-art methods on Market-1501 and DukeMTMC-reID. Note that the\nbest results are bolded.\npervised methods, such as PCB [20] and DG-Net [34],\nwhen ground truth is not used.\nAnd our HHCL even\nachieves a better performance under supervised setting.\nThis result shows that our proposed method achieves bet-\nter results when using the ground truth to avoid introducing\nnoisy pseudo-labels. And it also further demonstrates the\neffectiveness of our method for the person Re-ID problem,\nboth unsupervised and supervised.\n5.4. Ablation Study\nInfluence of Hyper-Parameter µ Tab. 2 reports the ex-\nperiment result under different value of hyper-parameterµ.\nAs mentioned in 2, µ is a balancing factor between 0 and\n1, which plays an important role in affecting the weights\nof the cluster-level loss and instance-level loss. When µ is\nequal to 0, the loss function contains only the hard instance\ncontrastive loss. From the fig.3. we can find that the model\nconverges very slowly in the early stage of the training pro-\ncess, and using only the hard samples for comparison is not\nbenefit for learning generalized features and obtaining bet-\nter clustering pseudo labels. On the contrary, when µ=1 and\ncluster-level loss only is used, although a faster convergence\ncan be achieved, only one feature is retained for each clus-\nter, which loses the diversity of intra class and is still not\nconducive to facilitating the network to learn more discrim-\ninative features. It can be seen that combining both kind\nµ\nMarket1501\nmAP\nR1\nR5\nR10\n0(hard)\n78.5\n90.5\n96.0\n97.4\n0.25\n82.7\n92.9\n97.0\n98.2\n0.5\n84.2\n93.4\n97.7\n98.5\n0.75\n81.7\n92.1\n97.1\n98.2\n1.(mean)\n80.8\n91.3\n96.3\n97.6\nTable 2. Evaluation of parameter µ on Market1501.\nof contrastive loss leads to better performance obviously.\nAnd when µ = 0.5, we get the best performance 84.2% in\nmAP, indicating that our proposed hybrid contrastive learn-\ning method has a distinct advantage over others during the\ntraining process.\nFigure 3. Ablation study on Market1501: Result com-\nparisons of different settings in mAP and Rank-1.\nMethod\nMarket1501\nmAP\nR1\nR5\nR10\nResNet50\n84.2\n93.4\n97.7\n98.5\nIBN + GeM\n87.8\n95.1\n98.2\n98.8\nIBN + GeM + LS\n88.2\n94.9\n98.3\n98.9\nMethod\nDukeMTMC-reID\nmAP\nR1\nR5\nR10\nResNet50\n73.3\n85.1\n92.4\n94.6\nIBN + GeM\n76.8\n87.9\n93.4\n94.9\nIBN + GeM + LS\n77.3\n87.7\n93.5\n95.1\nTable 3. Comparison of HHCL with other tricks on Market1501\nand DukeMTMC-reID datasets. ’IBN’ denotes that the backbone\napplies IBN-ResNet50. ’GeM’ and ’LS’ represent GeM pooling\nlayer and label smoothing respectively.\nInstance-batch normalization (IBN) [18] and General-\nized Mean Pooling (GeM) [19] has been proved effective\nin both supervised and UDA based Re-ID methods. We\ncompare the performance of HCCL under different settings\nin Tab.3. The performance of our proposed HHCL can be\nfurther improved with an IBN-ResNet50 backbone network\nand GeM pooling layer.\n6. Conclusion\nIn this paper, we propose a novel method for the fully\nunsupervised person re-ID. The new concepts and tech-\nniques introduced include a more efficient hybrid contrast\nlearning framework and a memory based hard sample min-\ning scheme. Specifically, our proposed HHCL approach\ncomprehensively consider both of cluster level and instance\nlevel information. For effectively exploiting the invariance\nwithin and between clusters, HHCL leverages hard samples\nto guide network to learn more robust and discriminative\nfeatures. Extensive experiments on two benchmark datasets\ndemonstrated that HHCL achieves the best results compar-\ning with all existing purely unsupervised and UDA-based\nRe-ID methods.\nAcknowledgements\nThis work was supported in part by 111 Project of China\n(B17007), and in part by the National Natural Science\nFoundation of China (61602011).\nReferences\n[1] Guangyi Chen, Yuhao Lu, Jiwen Lu, and Jie Zhou. Deep\ncredible metric learning for unsupervised domain adaptation\nperson re-identification. In ECCV, 2020.\n[2] Hao Chen, Benoit Lagadec, and F. Br´emond.\nIce: Inter-\ninstance contrastive encoding for unsupervised person re-\nidentification. ArXiv, abs/2103.16364, 2021.\n[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey E. Hinton. A simple framework for contrastive learn-\ning of visual representations. ArXiv, abs/2002.05709, 2020.\n[4] Zuozhuo Dai, Guangyuan Wang, Siyu Zhu, Weihao Yuan,\nand P. Tan.\nCluster contrast for unsupervised person re-\nidentification. ArXiv, abs/2103.11568, 2021.\n[5] Jia Deng, Wei Dong, R. Socher, Li-Jia Li, K. Li, and Li Fei-\nFei. Imagenet: A large-scale hierarchical image database. In\nCVPR, 2009.\n[6] Martin Ester, H. Kriegel, J. Sander, and Xiaowei Xu.\nA\ndensity-based algorithm for discovering clusters in large spa-\ntial databases with noise. In KDD, 1996.\n[7] Yang Fu,\nYunchao Wei,\nGuanshuo Wang,\nXi Zhou,\nHumphrey Shi, and Thomas S. Huang. Self-similarity group-\ning: A simple unsupervised cross domain adaptation ap-\nproach for person re-identification. 2019 IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 6111–\n6120, 2019.\n[8] Yixiao Ge, Dapeng Chen, and Hongsheng Li.\nMu-\ntual mean-teaching:\nPseudo label refinery for unsuper-\nvised domain adaptation on person re-identification. ArXiv,\nabs/2001.01526, 2020.\n[9] Yixiao Ge, Dapeng Chen, Feng Zhu, Rui Zhao, and Hong-\nsheng Li. Self-paced contrastive learning with hybrid mem-\nory for domain adaptive object re-id. ArXiv, abs/2006.02713,\n2020.\n[10] Douglas Gray, Shane Brennan, and Hai Tao. Evaluating ap-\npearance models for recognition, reacquisition, and tracking.\n2007.\n[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. Momentum contrast for unsupervised vi-\nsual representation learning.\n2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n9726–9735, 2020.\n[12] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 770–778, 2016.\n[13] Alexander Hermans, L. Beyer, and B. Leibe.\nIn de-\nfense of the triplet loss for person re-identification. ArXiv,\nabs/1703.07737, 2017.\n[14] S. Ioffe and Christian Szegedy. Batch normalization: Accel-\nerating deep network training by reducing internal covariate\nshift. ArXiv, abs/1502.03167, 2015.\n[15] Jianing Li and Shiliang Zhang.\nJoint visual and tempo-\nral consistency for unsupervised domain adaptive person re-\nidentification. In ECCV, 2020.\n[16] Mingkun Li, Chun-Guang Li, and Jun Guo. Cluster-guided\nasymmetric contrastive learning for unsupervised person re-\nidentification. ArXiv, abs/2106.07846, 2021.\n[17] Yutian Lin, Lingxi Xie, Yu Wu, C. Yan, and Qi Tian. Un-\nsupervised person re-identification via softened similarity\nlearning. 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 3387–3396, 2020.\n[18] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two\nat once: Enhancing learning and generalization capacities\nvia ibn-net. In ECCV, 2018.\n[19] Filip Radenovi´c, Giorgos Tolias, and O. Chum.\nFine-\ntuning cnn image retrieval with no human annotation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n41:1655–1668, 2019.\n[20] Yifan Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang. Beyond\npart models: Person retrieval with refined part pooling. In\nECCV, 2018.\n[21] A¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. ArXiv,\nabs/1807.03748, 2018.\n[22] Dongkai Wang and Shiliang Zhang. Unsupervised person re-\nidentification via multi-label classification. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10981–10990, 2020.\n[23] Hanxiao Wang, Xiatian Zhu, T. Xiang, and S. Gong. Towards\nunsupervised open-set person re-identification. 2016 IEEE\nInternational Conference on Image Processing (ICIP), pages\n769–773, 2016.\n[24] Menglin Wang, B. Lai, Jianqiang Huang, Xiaojin Gong, and\nXiansheng Hua.\nCamera-aware proxies for unsupervised\nperson re-identification. In AAAI, 2021.\n[25] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and\nM. Scott.\nMulti-similarity loss with general pair weight-\ning for deep metric learning. 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5017–5025, 2019.\n[26] Zhongdao Wang, Jingwei Zhang, Liang Zheng, Yixuan Liu,\nYifan Sun, Yali Li, and Shengjin Wang.\nCycas:\nSelf-\nsupervised cycle association for learning re-identifiable de-\nscriptions. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceed-\nings, Part XI 16, pages 72–88. Springer, 2020.\n[27] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3733–3742, 2018.\n[28] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance-\nlevel discrimination. ArXiv, abs/1805.01978, 2018.\n[29] Hong-Xing Yu, Ancong Wu, and W. Zheng.\nCross-view\nasymmetric metric learning for unsupervised person re-\nidentification. 2017 IEEE International Conference on Com-\nputer Vision (ICCV), pages 994–1002, 2017.\n[30] Hong-Xing Yu, W. Zheng, Ancong Wu, X. Guo, S. Gong,\nand J. Lai.\nUnsupervised person re-identification by soft\nmultilabel learning. 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2143–\n2152, 2019.\n[31] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo.\nHierarchical clustering with hard-batch triplet loss for per-\nson re-identification. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n13657–13665, 2020.\n[32] Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Ron-\ngrong Ji, and Yonghong Tian. Multiple expert brainstorm-\ning for domain adaptive person re-identification.\nArXiv,\nabs/2007.01546, 2020.\n[33] L. Zheng, Liyue Shen, Lu Tian, S. Wang, Jingdong Wang,\nand Q. Tian.\nScalable person re-identification: A bench-\nmark.\n2015 IEEE International Conference on Computer\nVision (ICCV), pages 1116–1124, 2015.\n[34] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, L. Zheng,\nY. Yang, and J. Kautz.\nJoint discriminative and genera-\ntive learning for person re-identification. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 2133–2142, 2019.\n[35] Zhedong Zheng, L. Zheng, and Y. Yang. Unlabeled sam-\nples generated by gan improve the person re-identification\nbaseline in vitro. 2017 IEEE International Conference on\nComputer Vision (ICCV), pages 3774–3782, 2017.\n[36] Zhun Zhong, L. Zheng, Donglin Cao, and Shaozi Li. Re-\nranking person re-identification with k-reciprocal encoding.\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 3652–3661, 2017.\n[37] Zhun Zhong, L. Zheng, Zhiming Luo, Shaozi Li, and Y.\nYang.\nInvariance matters: Exemplar memory for domain\nadaptive person re-identification. 2019 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 598–607, 2019.\n[38] Zhun Zhong, L. Zheng, Zhiming Luo, Shaozi Li, and Yezhou\nYang. Learning to adapt invariance in memory for person re-\nidentification. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 43:2723–2738, 2021.\n[39] Kaiyang Zhou, Yongxin Yang, A. Cavallaro, and T. Xi-\nang. Omni-scale feature learning for person re-identification.\n2019 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 3701–3711, 2019.\n[40] Yang Zou, Xiaodong Yang, Zhiding Yu, B. Kumar, and J.\nKautz. Joint disentangling and adaptation for cross-domain\nperson re-identification. ArXiv, abs/2007.10315, 2020.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-09-25",
  "updated": "2022-05-08"
}