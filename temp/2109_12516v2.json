{
  "id": "http://arxiv.org/abs/2109.12516v2",
  "title": "Prioritized Experience-based Reinforcement Learning with Human Guidance for Autonomous Driving",
  "authors": [
    "Jingda Wu",
    "Zhiyu Huang",
    "Wenhui Huang",
    "Chen Lv"
  ],
  "abstract": "Reinforcement learning (RL) requires skillful definition and remarkable\ncomputational efforts to solve optimization and control problems, which could\nimpair its prospect. Introducing human guidance into reinforcement learning is\na promising way to improve learning performance. In this paper, a comprehensive\nhuman guidance-based reinforcement learning framework is established. A novel\nprioritized experience replay mechanism that adapts to human guidance in the\nreinforcement learning process is proposed to boost the efficiency and\nperformance of the reinforcement learning algorithm. To relieve the heavy\nworkload on human participants, a behavior model is established based on an\nincremental online learning method to mimic human actions. We design two\nchallenging autonomous driving tasks for evaluating the proposed algorithm.\nExperiments are conducted to access the training and testing performance and\nlearning mechanism of the proposed algorithm. Comparative results against the\nstate-of-the-art methods suggest the advantages of our algorithm in terms of\nlearning efficiency, performance, and robustness.",
  "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nPrioritized Experience-based Reinforcement\nLearning with Human Guidance for Autonomous\nDriving\nJingda Wu, Student Member, IEEE Zhiyu Huang, Student Member, IEEE Wenhui Huang, and Chen Lv, Senior\nMember, IEEE\nAbstractâ€”Reinforcement learning (RL) requires skillful deï¬-\nnition and remarkable computational efforts to solve optimiza-\ntion and control problems, which could impair its prospect.\nIntroducing human guidance into reinforcement learning is a\npromising way to improve learning performance. In this paper,\na comprehensive human guidance-based reinforcement learning\nframework is established. A novel prioritized experience replay\nmechanism that adapts to human guidance in the reinforcement\nlearning process is proposed to boost the efï¬ciency and per-\nformance of the reinforcement learning algorithm. To relieve\nthe heavy workload on human participants, a behavior model\nis established based on an incremental online learning method\nto mimic human actions. We design two challenging autonomous\ndriving tasks for evaluating the proposed algorithm. Experiments\nare conducted to access the training and testing performance and\nlearning mechanism of the proposed algorithm. Comparative re-\nsults against the state-of-the-art methods suggest the advantages\nof our algorithm in terms of learning efï¬ciency, performance,\nand robustness.\nIndex Termsâ€”Reinforcement learning, priority experience re-\nplay, human demonstration, autonomous driving.\nI. INTRODUCTION\nR\nEINFORCEMENT learning (RL) has substantially con-\ntributed to numerous ï¬elds [1]â€“[4] by solving control\nand optimization problems. As a branch of machine learning\nmethods, RL improves the capability of controlling agents in\nblack-box environments through the exploratory trial-and-error\nprinciple [5]. Recent popular RL algorithms, e.g., rainbow\ndeep Q-learning [6], proximal policy optimization (PPO) [7],\nand soft actor-critic (SAC) [8], have shown ability in handling\nhigh-dimensional environment representation and generaliza-\ntion, due to the introduction of deep neural networks. Albeit\nRL can achieve good performance in complex tasks, its draw-\nback emerges that their interactions with the environment are\nvery inefï¬cient [9]. Thus, using RL to solve a problem needs\nskillful deï¬nitions and settings and consumes remarkable\ncomputational resources [10].\nCombining human guidance with RL can be a promising\nway to mitigate the above drawback [11]. First, human inter-\nJ. Wu, Z. Huang, W. Huang and C. Lv are with the School of Me-\nchanical and Aerospace Engineering, Nanyang Technological University, Sin-\ngapore, 639798. (E-mail: {jingda001, zhiyu001, wenhui001}@e.ntu.edu.sg,\nlyuchen@ntu.edu.sg)\nCorresponding author: Chen Lv\nThis paper has been published in IEEE Transactions on Neural Networks\nand Learning Systems. DOI: 10.1109/TNNLS.2022.3177685\nThe code associated with this paper is available at this link\nvention has been used to improve RL performance. Interven-\ntion is triggered by unfavorable actions and should be avoided\nby RL. Then, the human demonstration is a powerful tool\nto enhance RLâ€™s ability [12]. In this context, the objective\nfunctions are generally reshaped compatible with supervised\nlearning to improve efï¬ciency [13]. Despite the above hu-\nman guidance-based methods, RL needs to process numerous\ndata from its self-explorations. The existing methods do not\nparticularly optimize the utilization of human guidance data;\nconsequently, they still need great human workloads to avoid\nsubmersion of guidance in exploratory data. Additionally,\nhuman guidance, which is variant to proï¬ciency, mental and\nphysical status of participants, should not be equally treated\nsince some low-quality guidance can even impair the RL\nperformance.\nWe propose a priority-based experience replay method\non human guidance and put forward the associated human\nguidance-based RL algorithm to bridge the abovementioned\ngap. Our approach is off-policy, which leverages the experi-\nence replay mechanism [14] to maximize the utilization efï¬-\nciency of self-exploratory data. The proposed priority replay\nmechanism can further improve the utilization efï¬ciency of\nhuman guidance data by quantifying their values and weighing\ntheir utilized probability, which ultimately augments the RL\nperformance. As a result, the efï¬ciency can be improved\nby over seven times under the adopted task. The schematic\ndiagram of our algorithm is depicted in Fig. 1. To evaluate\nthe training and testing performance of our proposed method,\nwe design two challenging autonomous driving scenarios.\nThe experimental results suggest the advance of the proposed\nalgorithm compared to state-of-the-art baselines in learning\nefï¬ciency, practical performance, and robustness.\nThe contribution of this report can be summarized into\nthree aspects. 1) We propose a novel prioritized experience\nutilization mechanism regarding human guidance in the RL\nprocess to improve performance. 2) We establish a com-\nprehensive and holistic framework of human guidance-based\nRL by integrating the human-RL action switch scheme, be-\nhavior cloning-based objective function, human-demonstration\nreplay method, and human-intervention reward shaping mech-\nanism.3) We validate the superior performance of the proposed\nalgorithm in solving challenging autonomous driving tasks\ncomprehensively.\nThe remainder is organized as follows: a review of related\nwork is provided in Section II, preliminaries for the proposed\narXiv:2109.12516v2  [cs.LG]  29 Nov 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nHuman-RL authority transfer\nRL algorithm\n(shaped for human guidance)\nEnvironment \n(controlled object)\nTDQA \npriority calculation\nHuman guidance\nRL exploratory \nexperience\nHuman-guidance \nexperience\nReplay for \nlearning\nState transition\nAction\nğ‘ğ‘ğ‘…ğ‘…ğ‘…ğ‘…\nAction\nğ‘ğ‘ğ»ğ»\nObservation\nObservation\nInteraction \ninformation (tuple)\nHuman guidance-based priority \nexperience replay mechanism\nEventual action ğ‘ğ‘\nUpdate after use\nSorted storing \n1\n2\n3\n4\nFig. 1. Framework of the proposed human-guided reinforcement learning. The\nRL algorithm in this report is shaped in multiple aspects to adapt to human\nguidance. In the proposed human guidance-based priority experience replay\nmechanism, TDQA represents the proposed priority calculation scheme, and\nthe number 1-4 indicates the ï¬‚ow sequence of data. The dotted line of the\naction signal represents that the framework allows intermittent human-in-the-\nloop guidance.\nalgorithm is introduced in Section III, Section IV provides\nthe proposed human guidance-based reinforcement learning\nalgorithm, a human behavior model for substituting real human\nparticipant is established in Section V. Section VI presents\nthe problem formulation for the adopted autonomous driving\ntasks, Section VII provides the experimental results, and the\nconclusion is drawn in Section VIII.\nII. RELATED WORK\nSample efï¬ciency bottlenecks the training and performance\nof RL. Combining human guidance with RL is a promising\nway to mitigate the challenge. Three categories of human\nguidance have been integrated into RL.\nThe ï¬rst one is human feedback, where the human expertâ€™s\nprior knowledge about the task could be used to qualitatively\nor quantitatively score the RL behaviors [15]. In this man-\nner, an RL-based unmanned ground vehicle was guided to\nrun through a maze [16]. However, the feedback is high-\ndemanding on human ability and thus is no longer popular\nin recent studies.\nThe second branch is human intervention. Intervention is\na more direct manifestation of human knowledge than giving\nfeedback. RL agents are devised to reduce their conï¬dence\nin adopted actions if intervention occurs [17]. [18] employed\nreal humans to detect catastrophic actions of DQN in playing\nAtari games, where humans were required to intervene in\nthe training process to block the risk. It punished the human\nintervened scenes through the reward-shaping technique to\nprevent RL from reaching the unfavorable situations again.\nWith a similar idea, [19] devised a reward shaping-based PPO\nalgorithm and made the RL agent complete the drone driving\ntasks under human interventions. In this report, the above-\nmentioned reward shaping scheme is also adopted, and more\nimportantly, we provide a theoretical derivation and related\ndiscussion on the optimality of the human intervention-based\nreward shaping method.\nThe human demonstration is the other way to enhance RL\nperformance. For discrete-action RL, the DQfD algorithm [12]\nshaped the value function of DQN using human demonstration.\n[20] presented a double experience buffer setting to separately\nstore the RL data and human demonstrations. For more com-\nplicated RL with actor-critic architecture, the policy function is\nusually modiï¬ed to be compatible with learning from demon-\nstration. The behavior cloning objective has been added to the\nobjective of the policy function to greatly improve learning\nefï¬ciency, which is a milestone in the ï¬eld. In this way,\ndexterous manipulations of high degree-of-freedom robotic\narms [21]â€“[23] and human-level game operation [17] were\nachieved based on the state-of-the-art RL algorithms. In this\nreport, the behavior cloning objective and its associated human\nguidance-based actor-critic framework is also integrated into\nour method. However, it is not reasonable for equal treat-\nment on various demonstrations, which is adopted in existing\nmethods. First, without optimizing the utilization, small-scale\nhuman demonstrations would be submerged in the numerous\nRL-generated data. Second, human guidance is variant due to\nthe proï¬ciency and status of participants, and some low-quality\nguidance can even impair RL performance. Noticeably, these\ndrawbacks are to be overcome by the proposed prioritized\nexperience utilization mechanism.\nIII. PRELIMINARIES\nIn this section, we ï¬rst introduce the notation and concept of\noff-policy actor-critic RL, and we then illustrate the prioritized\nexperience replay mechanism. All three parts in this section\nare the base for the proposed human-guidance-based RL\nalgorithm.\nA. Notation\nWe consider a standard RL setting where an RL agent\ninteracts with the controlled environment. Such an interaction\ncan be formulated as a discrete-time Markov decision process\n(MDP), deï¬ned by the tuple (S, A, R, p). The state-space S\nconsists of continuous state variables s and the action space\nconstitutes continuous action variables a. R(Â·|s, a) : S Ã—A â†’\nr is a reward function mapping the state-action pair (s, a)\nto a deterministic reward value r. The environment dynamics\ngenerates state transition probability p(Â·|s, a) : SÃ—A â†’P(sâ€²)\nmapping the state-action pair (s, a) to the probability distri-\nbution over the next state sâ€².\nAt each time step t, the agent observes the state st âˆˆS\nand sends the action at âˆˆA to the environment, receiving\nthe feedback of a scalar reward rt and next state st+1. The\nagentâ€™s behavior is determined by a policy Ï€(at|st) : S â†’\nP(at), which maps a state to the probability distribution over\ncandidate actions. We utilize ÏÏ€ to represent the state-action\ndistribution induced by the policy Ï€.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nB. Off-policy Actor-critic Architecture\nThe goal of RL is to optimize the policy which maximizes\nthe expected value V over the environment dynamics. A\nBellman value function (also called critic) is established to\nestimate V in a bootstrapping way. This value function is\nusually called Q. Under an arbitrary policy Ï€, Q is deï¬ned\nas:\nQÏ€(st, at) = rt + Î³\nE\n(st+1,at+1)âˆ¼ÏÏ€[QÏ€(st+1, at+1)],\n(1)\nwhere Î³ âˆˆ(0, 1) is the discount factor. Then the policy func-\ntion (also called actor) can be obtained concerning maximized\nQ, represented as:\nÏ€ = arg max\nÏ€\n\u0014\nE\n(s,a)âˆ¼ÏÏ€ [QÏ€ (s, a)]\n\u0015\n,\n(2)\nIn practice, value function pursues the evaluation regarding\nonly the optimal policy Ï€â‹†, regardless of the policy executing\nthe interaction. Therefore, RL decouples the policy evaluation\nprocess and the policyâ€™s behavior, which makes the agent\nupdate in an off-policy manner.\nWe use neural networks as the function approximator to\nformulate the actor and critic, the objectives are then reached\nthrough the loss functions. Speciï¬cally, the loss function of\nthe critic LQ, and the actor LÏ€ can be expressed as:\nLQ (Î¸) = rt + Î³E [Q (st+1, Ï€ (st+1; Ï†) ; Î¸)] âˆ’Q (st, at; Î¸) ,\n(3)\nLÏ€ (Ï†) = âˆ’Q (st, Ï€ (Â·|st; Ï†) ; Î¸) ,\n(4)\nwhere Q(Â·; Î¸) represents the parameterized critic function and\nÎ¸ represents the parameters of the critic network, Ï€(Â·; Ï†)\nrepresents the parameterized actor function and Ï† represents\nthe parameters of the actor network. Hereinafter, the parameter\nÎ¸ and Ï† can be omitted if no ambiguity exists.\nC. Prioritized Experience Replay Mechanism\nThe experience replay mechanism establishes an experience\nbuffer to store the data at each interaction. Accordingly, the\nRL agent can retrieve data generated by previous policies from\nthe buffer for policy evaluation and improvement.\nGiven an arbitrary time step t, the interaction between the\nRL agent and the environment generates a transition tuple,\nwhich is stored into the experience replay buffer as:\nB â†Î¶t = (st, at, rt, st+1).\n(5)\nConventionally, the experience in the buffer is retrieved from\nthe buffer using uniform random sampling. In a more efï¬cient\nmethod, prioritized experience replay mechanism (PER) [24],\nthe data sample is subjected to a nonuniform distribution I,\nand its probability mass function pI âˆ¼I can be expressed as:\npI (i) =\npÎ±\ni\nP\nk pÎ±\nk\n,\n(6)\nwhere Î± âˆˆ[0, 1] is the scaling coefï¬cient, p represents the\npriority of each tuple i, which is determined by the temporal\ndifference (TD) error Î´T D and expressed as:\npi = |Î´T D\ni\n| + Îµ\n= |ri + Î³ Â· Q (si+1, Ï€ (Â·|si+1; Ï†) ; Î¸) âˆ’Q (si, ai; Î¸) | + Îµ,\n(7)\nwhere Îµ âˆˆR+ is a small positive constant to guarantee\nthe probability larger than zero. A larger TD error indicates\nan experience worth learning to a higher extent. Thus, the\nTD error-based prioritized experience replay mechanism can\nimprove the RL training efï¬ciency.\nIV. HUMAN-IN-THE-LOOP REINFORCEMENT LEARNING\nIn this section, we ï¬rst summarize the human behaviors\nin the RL training process which can be leveraged in the\nalgorithm design. Based on that, we establish an actor-\ncritic framework adapting to human guidance. Then, two\nmodules are proposed to further improve RL in the context\nof human guidance: a novel prioritized experience replay\nmechanism concerning human demonstration, and a reward\nshaping technique concerning human intervention. Finally, a\nholistic human-in-the-loop RL algorithm is instantiated using\nthe above components.\nA. Human Guidance Behavior in RL Training\nWe deï¬ne two useful human guidance behaviors in the RL\ntraining process: intervention and demonstration.\nIntervention: Human participants recognize RL interaction\nscenes and identify whether a guidance behavior should be\nconducted based on their prior knowledge and reasoning\nabilities. If human participants decide to intervene, they can\nmanipulate the equipment to get the control authority (partially\nor totally) from the RL agent. The intervention generally\nhappens when the RL agent conducts catastrophic actions or\nis stuck in local optima traps. Thus, RL could learn to avoid\nunfavorable situations from the intervention.\nDemonstration: Human participants perform their actions\nwhen an intervention event happens, which generates the\ncorresponded reward signal and next-step state. The generated\ntransition tuple can be seen as a piece of demonstration data\nsince it is induced by human policy instead of the RLâ€™s\nbehavior policy. RL algorithm could learn human behavior\nfrom the demonstration.\nState-of-the-art human-guidance-based RL algorithms have\nbeen integrating learning from intervention (LfI) [18], and\nlearning from demonstration (LfD) [25]. In this report, both\nLfI and LfD will be employed in the proposed architecture.\nSpeciï¬cally, LfI based on the reward shaping technique is\nutilized in the reward function deï¬nition, while LfD plays its\nrole in the underlying principles of the algorithm.\nB. Human-guidance-based Actor-critic Framework\nIn this section, we elaborate on the interaction mechanism\nand learning objective of the proposed human-guidance-based\nactor-critic RL algorithm.\nFirst, we focus on the interaction mechanism. In the stan-\ndard interaction between RL and environment, RLâ€™s behavior\npolicy will output actions to explore the environment. Given\nan off-policy actor-critic RL, the above process is shown as:\naRL\nt\n= Ï€(Â·|st; Ï†) + Î¾a âŠ™astd\nt ,\n(8)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nwhere astd\nt\nâˆˆRdim(A) is a training-dependent variable that\nscales the exploration noise, âŠ™represents the Hadamard\nproduct and Î¾a âˆ¼N(0, Idim(A))\nWe give full authority to human participants whenever they\ndecided to take control in the training loop of RL. Thus, the\neventual action is ï¬ltered by a mask as:\nat = (Idim(A) âˆ’âˆ†t) Â· aRL\nt\n+ âˆ†t Â· aH\nt ,\n(9)\nwhere aH\nt\nrepresents the action from the human participantâ€™s\npolicy, âˆ†t âˆˆRdim(A) is a demonstration mask: it is an identity\nmatrix when human demonstration happens and a zero matrix\nin the non-demonstrated step.\nThe interaction transition tuple Î¶ will be recorded and stored\ninto the experience replay buffer once the action is sent to the\nenvironment. In particular, actions from the human policy and\nthe RL policy are stored in the same buffer. For this context,\nthe new transition tuple Î¶ is deï¬ned to discriminate human\ndemonstrations from normal RL experiences as:\nÎ¶i = (si, ai, ri, si+1, âˆ†i).\n(10)\nThen, we focus on the learning objective. Given a batch of\ntransition tuples with batch size N, there could exist data Î¶N1\nfrom the RL policy and Î¶N2=Nâˆ’N1 from the human policy.\nThe critic network, based on the optimal value function, can\nlearn from both policies. Thus, its loss function is calculated\nas:\nLQ(Î¸) =\n1\nN1\nN1\nX\ni\nâˆ¥ri + Î³Q(si+1, Ï€(Â·|si+1); Î¸) âˆ’Q(si, aRL\ni\n; Î¸)âˆ¥2\n2\n+ 1\nN2\nN2\nX\nj\nâˆ¥rj + Î³Q(sj+1, Ï€(Â·|sj+1); Î¸) âˆ’Q(sj, aH\nj ; Î¸)âˆ¥2\n2.\n(11)\nGiven the data from the human policy, the actor should\nlearn from these demonstrations in addition to maximizing the\ncriticâ€™s value. Hence, we devise the loss function of the actor\nnetwork considering behavior cloning as:\nLÏ€ (Ï†) = 1\nN1\nN1\nX\ni\n[âˆ’Q(si, Ï€(Â·|si; Ï†); Î¸)]\n+ 1\nN2\nN2\nX\nj\n[Ï‰ Â· âˆ¥aH\nj âˆ’Ï€(Â·|sj; Ï†)âˆ¥2\n2],\n(12)\nwhere Ï‰ is a manually determined constant that weighs the\nimportance of behavior cloning.\nIt is noticeable that the mean squared error (MSE) losses\ninvolved in the above formulas are for exempliï¬ed calculation,\nmeaning that they can be alternated by any loss functions.\nC. Prioritized Human Demonstration Replay\nIn this section, we put forward a novel PER mechanism for\nhuman demonstration.\nHuman demonstrations are generally more critical than most\nexploration from RLâ€™s behavior policy due to prior knowledge\nand reasoning ability. Thus, a more effective method is needed\nto weigh human demonstrations among the buffer. We propose\nan advantage-based metric instead of TD-error of the normal\nPER to establish the prioritized replay mechanism.\nFirst, we deï¬ne an advantage measure regarding the human\ndemonstration against the RLâ€™s behavior policy. Since the\ncritic, i.e., value function, can evaluate the policy, we calculate\nthe difference between the Q value of the human action and\nthat of the RL action. Given a human-demonstration transition\ntuple (si, ai = aH\ni , ri, si+1), the priority level p is deï¬ned as:\npi =\nâˆ†|Î´T D\ni\n|+Îµ+exp\n\u0002\nQ(si, aH\ni ; Î¸) âˆ’Q(si, Ï€(Â·|si); Î¸)\n\u0003\n, (13)\nwhere exp is the exponential function to guarantee the non-\nnegative advantage value.\nWe call the last term of the Eq. 13 the Q-advantage term,\nwhich evaluates to what extent should a speciï¬c human-\ndemonstration tuple be retrieved except the TD-error metric.\nThrough the RL training process, the RL agentâ€™s ability\nvaries and the priority level of one human-demonstration tuple\nchanges accordingly, which gives rise to a dynamic priority\nmechanism. We abbreviate Q-advantage as QA and call the\nabove mechanism TDQA to illustrate it combines two metrics\nas the measurement of human guidance. The QA term is\nremoved for non-demonstration tuples when calculating the\nabove equation, thus, the priority levels of non-demonstration\ndata are aligned with those in the conventional PER.\nIn this manner, the experience in the buffer B subjects to\na distribution Iâ€², and the probability mass function of the\nexperience distribution pIâ€² âˆ¼Iâ€² can be expressed as:\npIâ€²(i) =\npÎ±\ni\nP\nk pÎ±\nk\n.\n(14)\nWe inherent the optimization trick of the conventional PER\nby using a sum-tree structure to store transition data, and the\nupdating and sampling can be conducted with a complexity of\nO(log N).\nThe priority mechanism introduces the bias to the estimation\nof the expectation of the value function since it changes the\nexperience distribution in the buffer. Biased value network Q\ncould have little impact on the RL asymptotic performance, yet\nit may affect the stability and robustness of the mature policy\nin some situations. As an optional operation, we can anneal\nthe bias by introducing the importance-sampling weight to the\nloss function of the value network. The importance-sampling\nweight of a transition i is calculated as:\nwIS(i) = [pIâ€²(i)]âˆ’Î² .\n(15)\nwhere Î²\nâˆˆ[0, 1] is a coefï¬cient: the fully non-uniform\nsampling occurs if Î² = 1, and fully uniform sampling occurs\nif Î² = 0. Î² will gradually decrease to zero along with the\ntraining process.\nThe importance-sampling weight can be added to the loss\nfunction of the value network, expressed as:\nLQ(Î¸) =\nE\nÎ¶iâˆ¼Iâ€²wIS(i) (ri + Î³Q(si+1, Ï€(Â·|si+1); Î¸) âˆ’Q (si, ai; Î¸))].\n(16)\nThrough the proposed PER, we prioritize human guidance\nover RL experiences. Moreover, high-quality demonstrations\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nare prioritized to more extents, and the utilization efï¬ciency\nof human demonstrations can be enhanced.\nD. Human-intervention-based Reward Shaping\nIn this section, we introduce the human-intervention-based\nreward shaping technique. Naturally, there is no need for\nhumans to provide guidance if the being-trained RL agent is\nexecuting a good policy. Therefore, to minimize the human\nworkload, we assume human participants would intervene in\nthe training process only when RLâ€™s behaviors are unfavorable.\nIn this context, the intervention event can be seen as a negative\nsignal and the corresponding state should be avoided by RL.\nThis negative feedback can be realized by reward shaping,\nwhich will be detailed in this section.\nğ‘ğ‘0\nğ‘ğ‘1\nğ‘ğ‘2\nğ‘ğ‘3\nğ‘ğ‘4\nğ‘ğ‘5\nğ‘ ğ‘ 0\nğ‘ ğ‘ 1\nğ‘ ğ‘ 2\nğ‘ ğ‘ 3\nğ‘ ğ‘ 4\nğ‘ ğ‘ 5\nğ‘ ğ‘ 6\nState \nRL action (ğš«ğš«= ğŸğŸ)\nHuman demonstration action (ğš«ğš«= ğˆğˆ)\nIntervention time ( (ğš«ğš«ğ‘¡ğ‘¡= ğˆğˆ) âˆ§(ğš«ğš«ğ‘¡ğ‘¡âˆ’1 = ğŸğŸ) )\nFig. 2. Illustration of the intervention time step. In a time-sequential MDP,\nthe ï¬rst time step which is controlled by human demonstration is taken as the\nintervention time.\nWe ï¬rst identify the intervention event. Recall Eq. 9 deï¬nes\na mask âˆ†t, which is a time-sequential variable recording if\nthe action at is conducted by human demonstration. Hence,\nthe intervention time, i.e., the start time of a period of human\ndemonstrations, can be represented by (âˆ†t = I)âˆ§(âˆ†tâˆ’1 = 0)\nin a time-sequential training process of RL, as illustrated in\nFig. 2. It is noted that only the intervention time is to be\npunished by the reward shaping, since the states after humans\nintervention will be substituted by human demonstrations and\ncannot be seen as unfavorable. For instance, in Fig. 2, s2 is\npenalized while s3 and s4 are not.\nThen, we can shape the vanilla reward function with an\nadditional penalized function:\nrshape\nt\n= rt + rpen[(âˆ†t = Idim(A) âˆ§(âˆ†tâˆ’1 = 0dim(A))], (17)\nwhere rshape\nt\nis the reward after shaping, rpen is a scalar that\nweighs the intervention penalty.\nThe theoretical performance of this reward shaping scheme\nis analyzed in Appendix A.\nE. Prioritized human-in-the-loop RL algorithm\nIn this section, we integrate all the above components and\npropose a holistic RL algorithm considering human guidance.\nIt is noted that although the human guidance-based actor-critic\nframework in Section IV-B and reward shaping in Section\nIV-D are components of the algorithm, they are not the major\nnovelty of this report. To highlight our core idea of the\nprioritized human-demonstration replay mechanism of Section\nIV-C, we name the proposed algorithm as Prioritized Human-\nIn-the-Loop (PHIL) RL.\nSpeciï¬cally, we obtain the holistic human-in-the-loop RL\nconï¬guration through equipping the human-guidance-based\nactor-critic framework with prioritized human-demonstration\nreplay and intervention-based reward shaping mechanisms. We\ninstantiate the PHIL algorithm based on one of the state-of-the-\nart off-policy RLs, i.e., twin delayed deep deterministic policy\ngradient (TD3) [26]. We also remind the above components\nare adaptive to various off-policy actor-critic RL algorithms.\nIn TD3, the target networks, namely, the target critic Qâ€²\nwith parameter Î¸â€² and target actor Ï€â€² with parameter Ï†â€² are\nutilized to stabilize the algorithm update. And the actorâ€™s\noutput becomes a deterministic value instead of a sample from\nthe probability distribution.\nConsidering the role of human participants in the RL\ninteraction process, the eventual action in the time step t can\nbe expressed as:\nat = (Idim(A) âˆ’âˆ†t) Â· aRL\nt\n+ âˆ†t Â· aH\nt ,\n(18a)\naRL\nt\n= Ï€(Â·|st) + clip (Ïµ, âˆ’c, c) , Ïµ âˆ¼N (0, Î£) ,\n(18b)\nwhere Ïµ is a noise coefï¬cient vector dependent on the training\nproceed, c is the bounding of the exploratory action, Î£ is the\ncovariance matrix of the Gaussian distribution N.\nA transition tuple is obtained through the above interaction\nstep and stored into the proposed human-demonstration expe-\nrience buffer as:\nB â†Î¶t = (st, at, rt, st+1, âˆ†t).\n(19)\nStored experience tuples will be retrieved for the training of\nthe value and policy networks. An arbitrary transition tuple Î¶\nwith index i would be retrieved by the probability p, which is\ncalculated by:\np(i) =\npÎ±\ni\nP\nk pÎ±\nk\n,\n(20)\nwherein the priority level p is:\npt = |Î´T D\nt\n| + Îµ + (âˆ†t = Idim(A)) Â· QA,\n(21a)\nQA = exp [Qâ€² (st, at; Î¸â€²) âˆ’Qâ€² (st, Ï€(Â·|st; Ï†); Î¸â€²)] ,\n(21b)\nIt is noticeable that Q-advantage is calculated by the target\ncritic network Qâ€² to avoid unstable updates.\nSupposing a tuple with size N contains N1 amount of non\ndemonstration tuples and N2 = N âˆ’N1 human demonstration\nones, the loss function of the critic can be expressed as:\nLQk(Î¸) =\n1\nN1\nN1\nX\ni\nâˆ¥ri + Î³Qâ€²\nl (si+1, Ï€â€²(Â·|si+1)) âˆ’Qk\n\u0000si, aRL\ni\n\u0001\nâˆ¥2\n2\n+ 1\nN2\nN2\nX\nj\nâˆ¥rj + Î³Qâ€²\nl (sj+1, Ï€â€² (Â·|sj+1)) âˆ’Qk\n\u0000sj, aH\nj\n\u0001\nâˆ¥2\n2\n(22)\nwhere k = 1, 2 represents the index of two Q networks. Note\nthe double Q network trick, which utilizes the smaller Q\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nAlgorithm 1: PHIL-TD3\nInput: maximum episode number E, episode duration\nT, batch size N, policy network Ï€(Â·; Ï†), value\nnetworks Q1(Â·; Î¸1), Q2(Â·; Î¸2), target networks,\nempty buffer B, learning rate lrQ, lrÏ€, priority\ncoefï¬cient Î±, policy update factor d, soft\nupdate factor Ï„.\nfor episode=1 to E do\nObserve the initial state s1;\nfor t=1 to T do\nif human intervene then\nAdopt human action at = aH\nt , set âˆ†t = I;\nelse\nSelect RL action\nat = aRL\nt\n= Ï€(Â·|st; Ï†) + Ïµ, set âˆ†t = 0 ;\nend\nObserve reward rt and new state st+1 ;\nShape reward\nrt = rt + rpen Â· [(âˆ†t = I) âˆ§(âˆ†tâˆ’1 = 0)];\nStore tuple\n\u0000st, at, rt, sâ€²\nt+1, âˆ†t\n\u0001\nin B with\npriority pt = maxi<t pi ;\nSample N tuples from B with probability\np(i) = pÎ±\ni /(P\nk pÎ±\nk);\nUpdate priority by Eq. 13 ;\nUpdate value networks by\nÎ¸k=1,2 â†Î¸k=1,2 âˆ’lrQk=1,2 Â· âˆ‡Î¸LQ(Î¸);\nif t mod d then\nUpdate policy network by\nÏ† â†Ï† âˆ’lrÏ€ Â· âˆ‡Ï†LÏ€(Ï†);\nUpdate target networks ;\nend\nend\nend\nvalue of two networks (l = min{1, 2}), is introduced here\nto eliminate the value overestimation effect.\nThe loss function of the actor is calculated as:\nLÏ€(Ï†) = 1\nN1\nN1\nX\ni\n[âˆ’Q1 (si, Ï€ (Â·|si; Ï†) ; Î¸)]\n+ 1\nN2\nN2\nX\nj\n\u0002\nÏ‰ Â· âˆ¥aH\nj âˆ’Ï€ (Â·|sj; Ï†) âˆ¥2\n2\n\u0003\n.\n(23)\nIt is noticeable that the training of the policy network can\nbe delayed stabilizing the algorithm, that is, the actor would\nbe updated once given the critic updating d times.\nLumping all factors, the complete version of the proposed\nalgorithm is provided in Algorithm 1.\nV. HUMAN POLICY MODEL\nIn this section, a human policy model is established in\nconjunction with PHIL-RL. The model can relieve human\nworkload in the human-in-the-loop RL process by imitating\nthe behavior policy of actual human participants.\nWe train a regression model to imitate human policy si-\nmultaneously with RL, and this policy model can substitute\nhumans when necessary. Consider human behaviors in the RL\ntraining process: the human participant is required to intervene\nin the control process when he/she believes the agent poorly\nbehaves. Human interventions are usually imposed to the loop\nin an intermittent way and demonstrations are incrementally\nsupplemented into the training set (buffer). Thus, we train the\nhuman policy model leveraging an online- and incremental-\nbased imitation learning algorithm, i.e., the Data Aggregation\n(DAgger) [27], which is free from ofï¬‚ine large-scale collection\nof the demonstration data.\nIt is noted that the human policy model does not aim\nto accurately mimic expert-level humans. In practice, the\ncommon situation is humans who cooperate with RL are\nnon-proï¬cient, and humansâ€™ performance can ï¬‚uctuate with\nmental and physical status. Thus, we do not require the model\nto achieve expert-level performance. In essence, the human\npolicy model is to provide roughly correct demonstrations for\nthe RL agent.\nDenoting the human policy model with H, the objective\nis to ï¬nd a policy Ï€H minimizing its difference d with the\nhuman policy Ï€H:\nÏ€H = arg min\nÏ€\n\u0002\nEsi\n\u0002\nd(si, Ï€H)\n\u0003\u0003\n.\n(24)\nWe initialize model H by replicating an untrained RL policy\nnetwork. After the ï¬rst human-intervention event, model H is\nestablished as:\nÏ€H\n0 (Ï•) â†Ï€(Ï†).\n(25)\nIn subsequent episodes, we retrieve human demonstrations to\nconduct incremental learning with the loss function:\nLH(Ï•) = E(si,aH\ni )\n\u0002\nâˆ¥aH\ni âˆ’Ï€H(Â·|si; Ï†)âˆ¥2\n2\n\u0003\n,\n(26)\nand update the model with the gradient method as:\nÏ€H\ne+1 â†Ï€H\ne âˆ’lrÏ€H Â· âˆ‡Ï• LH (Ï•),\n(27)\nwhere e is the episode number of the RL process.\nThrough the above update, model H would gradually be\ncompetent to accurately mimic human policy, and accordingly,\nsubstitute human participants to assist RL. It is noticeable that\nif using this human policy model to cooperate with PHIL, the\nactivation conditions of model H shall be manually deï¬ned\nvarying to speciï¬c environments.\nVI. PROBLEM FORMULATION\nThe proposed PHIL-TD3, like most RLs, can be universally\nadapted to any continuous-action decision and control tasks.\nHere we choose the end-to-end autonomous driving problem\nas the object, evaluating our algorithm in two challenging\ndriving scenarios. Note that the RL-based autonomous driving\nproblem can be solved by numerous reasonable settings, while\nthe problem formulation in this section is to provide a fair\nenvironment for algorithm evaluation and comparison.\nIn this section, two challenging autonomous driving scenar-\nios are introduced to evaluate the control and optimization\nperformance of the proposed algorithm, then the standard\noptimization setting is established.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nA. Autonomous Driving Scenarios\nRL is better suited to the challenging driving tasks compared\nto rule-based or model optimization-based approaches due to\nits high representational and generalization capabilities. We\nchoose two scenarios, shown in Fig. 3, to evaluate the RL\nperformance. These scenarios are challenging to conventional\nautonomous driving strategies due to complex combinatorial\nrelationships.\nUnprotected left-turn: This scenario is illustrated in Figs.\n3(a-b). The ego vehicle, i.e., the controlled vehicle, in the side\nroad is trying to make a left turn and merge into the main\nroad. No trafï¬c signals guide the vehicles in the intersection.\nWe assume the lateral path of the ego vehicle is planned by\nother techniques, while the longitudinal control is assigned to\nthe RL agent. Surrounding vehicles are initialized with varying\nrandom velocities ranging from [4, 6] m/s and controlled by\nthe intelligent driver model (IDM) [28] to execute lane-keeping\nbehaviors. All surrounding drivers are set with aggressive\ncharacteristics, meaning that they would not yield to the ego\nvehicle. The control interval for all vehicles is set as 0.1\nseconds.\nHighway congestion: This scenario is illustrated in Figs.\n3(c-d). The ego vehicle is stuck in severe congestion and\ntightly surrounded by other vehicles; thus, it is trying to shrink\nthe gap with its leading vehicles and conduct the car-following\ntask with the target velocity. We assume the longitudinal\ncontrol is completed by IDM with a target velocity of 6\nm/s, while the lateral control is assigned to the RL agent.\nSurrounding vehicles are initialized with the velocity ranging\nfrom [4, 6] m/s and controlled by IDM to execute car-\nfollowing behaviors. The control interval for all vehicles is\nset as 0.1 seconds. The crowded surrounding vehicles cover\nthe lane markings and no speciï¬c one leading vehicle in the\nego lane, which can lead the conventional lateral-planning\napproaches to be invalid in such a scenario.\nEgo vehicle\nSurrounding\nvehicles\na\nb\nEgo vehicle\nSurrounding\nvehicles\nc\ndd\nFig. 3. Task environment conï¬guration. a, the devised unprotected left-turn\nscenario in T-intersection, established in CARLA. b, the bird-view of the\nleft-turn scenario, where the dotted line indicates a left-turn trajectory. c,\nthe devised congestion scenario in the highway, established in CARLA. d,\nthe bird-view of the congestion scenario, where the dotted line shows a car-\nfollowing trajectory.\na\nb\nFig. 4. State space illustration: birdâ€™s-eye-view semantic graph. a, the left-turn\nscenario, b, the congestion scenario.\nB. RL-based problem deï¬nition\nState: : The bird-view semantic graphs are taken as the\nstate information for the RL agent, shown in Fig. 4. Two\nconsecutive frame images are used to constitute one state\nvariable to enable temporal perception. We scale the camera-\ncaptured image to a smaller size to relieve the computational\nburden. The state variable can be expressed as:\nst = {ptâˆ’1, pt|p âˆˆ[0, 1]},\n(28)\nwhere p âˆˆR45Ã—80 is a pixel matrix of which the elements are\nnormalized.\nAction: : The action variable can be either lateral or\nlongitudinal commands adaptive to different requirements. For\nthe lateral control task in the congestion scenario, we choose\nthe angle of the steering wheel as the action, expressed as:\nat = [Î´t|Î´ âˆˆ[âˆ’5Î»Ï€, 5Î»Ï€]] ,\n(29)\nwhere Î´ âˆˆR1 is the continuous steering command, of which\nthe negative value indicates a left-turn command and the\npositive value corresponds to a right-turn command, and Î»\nis the scaling factor that limits the steering range.\nFor the longitudinal control in the left-turn scenario, we\nchoose the accelerating/braking pedal aperture, expressed as:\nat = [Î·t |Î· âˆˆ[âˆ’1, 1]] ,\n(30)\nwhere Î· âˆˆR1 is the continuous pedal aperture, of which the\nnegative value indicates a braking command and the positive\nvalue corresponds to an accelerating command.\nReward: : The goal of an autonomous vehicle is to rapidly\ncomplete trafï¬c scenarios through safe and smooth driving\nbehaviors. RL-based driving strategy achieves this by an\nappropriate reward function deï¬nition. The reward schemes\nof the two tasks in Fig.3 can be respectively deï¬ned as:\nRleft-turn (Â·|st, at) =rgoal Â· 1 (st âˆˆSgoal)\n+rfail Â· 1 (st âˆˆSfail) + rspeed (st) ,\n(31)\nRcongestion (Â·|st, at) =rgoal Â· 1 (st âˆˆSgoal)\n+rfail Â· 1 (st âˆˆSfail) + rsteer (st) ,\n(32)\nwhere rgoal = 10 and Sgoal is the set of goal states where the\nego vehicle successfully completes the scenario; rfail = âˆ’10\nand Sfail is the set of failure states where the collision\noccurs; while rspeed = âˆ’âˆ¥vego âˆ’vtargetâˆ¥is the reward that\nencourages the target speed, i.e., 5m/s set in this section;\nrsteer = âˆ¥Î´t âˆ’Î´tâˆ’1âˆ¥is the reward that discourages frequent\nsteering behaviors. It is noticeable that both rspeed and rsteer\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\na\nb\nğ‘ğ‘\nConvolutional \nlayers\nFlatten\nDense \nlayers\nState: consecutive images\nLatent feature\nOutput action\nğ‘„ğ‘„\nConvolutional layers\nDense \nlayers\nState: consecutive images\nLatent feature\nOutput Q value\nEmbedding layer\nAction: steer/pedal signals\nFlatten\nFig. 5.\nNeural network approximator illustration. a, the policy function\narchitecture achieved by the neural network, where the target value network\nowning the same structure is omitted for brevity. b, the value network\narchitecture achieved by the neural network, where the target value network\nowning the same structure is omitted for brevity.\ncan implicitly play a role in promoting smooth driving. Ad-\nditionally, we set the penalty term rpen in Eq. 17 the same\nas rfail and incremented it to the above reward when human\nintervention occurs.\nFunction approximator: : The function approximators of the\nvalue and policy functions are concrete by deep convolutional\nnetworks, as shown in Fig. 5.\nAuxiliary functions: : We deï¬ne some auxiliary control\nfunctions independent of the RL action to achieve a complete\ncontrol suit. When RL manipulates the steering wheel, the\nlongitudinal control is achieved by an IDM. When RL ma-\nnipulates the pedal aperture, the lateral motion target is to\ntrack the planned waypoints through a proportional-integral\n(PI) controller.\nVII. EXPERIMENTAL VALIDATION\nA. Baseline Algorithms\nWe employ state-of-the-art in the domain of human-involved\nRL algorithms as baselines and compare their performance\nagainst the proposed algorithm.\nIA-TD3: This baseline is derived from Intervention Aided\nReinforcement Learning (IARL) , which is a representative\ncombination of a continuous-action RL algorithm and human\ndemonstration. The RLâ€™s policy network is modiï¬ed to adapt\nto human demonstrated actions by introducing the behavior\ncloning objective. Once human intervention happens, the hu-\nman demonstration will substitute the RLâ€™s exploratory action,\nand a penalty signal will impose on the reward value. In\nthis study, we devise a modiï¬ed IARL by replacing the on-\npolicy base algorithm with TD3, which essentially augmented\nthe algorithm by improving the sample efï¬ciency. We also\nimplement the prioritized experience replay (PER) in this\nbaseline for a fair comparison.\nHI-TD3: This baseline is derived from Human Intervention\nReinforcement Learning (HIRL) , which is a combination of a\ndiscrete-action RL algorithm and human demonstration. Once\nintervention happens, the human demonstration will substitute\nthe RLâ€™s exploratory action, and a penalty signal will take on\nthe reward signal. In this study, we devise a modiï¬ed HIRL by\nreplacing the discrete-action base algorithm with TD3, which\naugmented the algorithm by improving the representation and\ncontrol precision. We also implement the PER in this baseline\nfor a fair comparison.\nRD2-TD3: This baseline is derived from Recurrent Replay\nDistributed Demonstration-based DQN (R2D3), which is a\nrepresentative combination of PER mechanism and human\ndemonstration. In this study, we devise a modiï¬ed algorithm\nby replacing DQN with TD3. The original R2D3 utilizes the\nrecurrent neural network to augment performance, which is not\nthe concerned technique in the context of this report, thus, we\nremove the recurrent network structure and only focus on its\nreplay distributed character regarding human demonstrations.\nThus, we devise a Replay Distributed Demonstration-based\n(RD2) TD3 algorithm, which distributes human demonstration\nand RL exploratory experience into two experience buffers\nrespectively and retrieves experiences by PER. The probability\nof utilizing human guidance instead of RL exploratory experi-\nence is aligned with the ratio of human guidance amount and\ntotal data amount.\nFurthermore, we use the vanilla PER+TD3 that is shielded\nfrom human guidance as an ablated baseline.\nB. Experimental Setting\nMultiple experiments are to evaluate the comprehensive\nperformance of PHIL-TD3 against baselines. First, the training\nefforts of involved algorithms are comparatively evaluated in\nthe two autonomous driving scenarios. Then the well-trained\nautonomous driving strategies are tested regarding control\nperformance with several metrics. Last, a series of experiments\ninvolving both training and testing stages are conducted to\nanalyze the mechanism of PHIL-TD3.\nThe training hardware comprises a driving simulator and\na high-performance workstation. The driving simulator is\nutilized to collect human data to train the human policy model\ncomplying with Section IV, and the workstation is dedicated\nto processing RL training. A high-ï¬delity autonomous driving\nsimulation platform, CARLA [29], is employed to implement\ndriving scenarios and generate RL-environment interaction\ninformation. The schematic diagram of the RL training stage\nis illustrated in Fig. 6(a).\nThe testing hardware is a robotic vehicle. The post-trained\nRL policy is implemented on the computation platform of\nthe vehicle, which can communicate with the CARLA server\nthrough the wireless network. The on-board RL policy receives\nstate information from CARLA and sends its control command\nback to remotely complete autonomous driving tasks. The\nrobotic vehicle aims to test whether the RL policy is well-\nworked under the current onboard calculation and communi-\ncation situations. The schematic diagram of the RL testing\nstage is demonstrated in Fig. 6(b).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nThe detailed conï¬guration of the above experimental plat-\nform is provided in table A1. The algorithms are concreted\nbased on neural networks, of which the architecture is il-\nlustrated in Appendix A. And the hyperparameters of the\nalgorithms are also given in Appendix A.\nC. Evaluation of RL Training Performance\nIn this section, we explore whether human guidance can\nindeed improve the RL training, and further, which algorithm\ncan achieve the best learning performance given the same\nhuman guidance. Additionally, we also investigate the effects\nof human guidance in dealing with RL tasks of different\ndifï¬culties.\nTo eliminate the deviation brought by participant random-\nness and obtain repeatable results, we use the identical human\nmodel (see Section V) to mimic human guidance behaviors\nin RL training processes. We ï¬xate the sequence of random\nseeds and make the triggering conditions of human inter-\nventions invariant in all training attempts, which achieves\na fair comparison across different algorithms. Two metrics\nare employed: the average reward of the training episode\n(excluding intervention-based shaping term), and the surviving\ndistance of the ego vehicle in the training episode before a goal\nstate or failure state in Eq. 31 occurs. A higher value of both\nmetrics indicates a better learning performance.\nFig. 7 visualizes the learning performance through curves,\nrepresented with a solid line of the mean value and an error\nband of the standard deviation. We run each algorithm ï¬ve\ntimes in the unprotected left-turn scenario and demonstrate\ntheir learning processes in Figs. 7(a-b). The vanilla TD3\nis struggling to improve its policy, while the other three\nalgorithms achieve higher rewards and survive distances in\na much shorter time, which indicates the effectiveness of\nHigh-fidelity \nscenario server\nPost-trained RL \ndriving policy\nğ‘ğ‘\nReal-time onboard \nenvironment\nHigh-fidelity \nscenario server\nAction \ncommand\nState \ninformation\nBeing-trained RL \ndriving policy\nğ‘ğ‘\nğ‘„ğ‘„\nPretreat human \nmodel before RL\nğ‘ğ‘\nHuman policy\nInteraction information (tuple)\nUpdate \nstrategy\nHuman-in-the-loop \nmodel training\nWorkstation \nenvironment\nState \ninformation\nState \ninformation\nAction \ncommand\nAction command\n(when needed)\nRL training stage\nRL testing stage\na\nb\nFig. 6.\nExperimental workï¬‚ow. a, the experimental workï¬‚ow in the RL\ntraining stage. The dotted line represents the human policy model that is not\nalways sending commands. b, the experimental workï¬‚ow in the RL testing\nstage.\ne\nf\n0\n100\n200\n300\n400\nTraining episode\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\nReward\nPHIL-TD3(ours)\nIA-TD3\nHI-TD3\nVanilla TD3\n0\n100\n200\n300\n400\nTraining episode\n0\n20\n40\n60\n80\nSurviving distance(m)\nPHIL-TD3(ours)\nIA-TD3\nHI-TD3\nVanilla TD3\n0\n100\n200\n300\n400\nTraining episode\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nReward\nPHIL-TD3(ours)\nIA-TD3\nHI-TD3\nVanilla TD3\n0\n100\n200\n300\n400\nTraining episode\n10\n15\n20\nSurviving distance (m)\nPHIL-TD3(ours)\nIA-TD3\nHI-TD3\nVanilla TD3\na\nb\nd\nc\nFig. 7.\nLearning efforts of different RL algorithms. a-b, curves of training\nrewards and surviving distances in the left-turn scenario, respectively. c-d,\ncurves of training rewards and surviving distances in the congestion scenario,\nrespectively.\nhuman guidance. Among the human-involved algorithms, HI-\nTD3 performs the slowest learning process suggested by\neither reward or surviving distance, and IA-TD3 exhibits a\nfaster convergence but with limited asymptotic performance.\nIn opposite, PHIL rapidly seizes the opportunity of human\nguidance and learns the best asymptotic policy. It is noticeable\nthat PHIL-TD3 achieves the best asymptotic average reward\nof the baselines in less than 50 episodes, improving the\nlearning efï¬ciency by over 700%. We also run the congestion\nscenario ï¬ve times for each algorithm and plot the learning\ncurves in Figs. 7(c-d). The comparable PHIL and IA-TD3\nperform better than the other two baselines when considering\nthe reward. While the metric of surviving distance further\nconï¬rms this advantage and proï¬tably differentiates the algo-\nrithm abilities. Speciï¬cally, PHIL wins the highest eventual\nscore. IA-TD3 and HI-TD3 manifest comparable levels of\nasymptotic performance while IA-TD3 has an advantage in\n100\n150\n200\n10\n20\n30\n40\n50\n60\nPHIL-TD3 IA-TD3\nHI-TD3\nVanilla\nTD3\nMean of average surviving distance (m)\nNormal setting\nTough setting\nRatio\n100\n150\n200\n10\n15\n20\nPHIL-TD3 IA-TD3\nHI-TD3\nVanilla\nTD3\nMean of average surviving distance (m)\nNormal setting\nTough setting\nRatio\nb\na\nRatio (%)\nRatio (%)\nFig. 8. Learning efforts of different RL algorithms in different task difï¬culties.\na-b, curves of training rewards and surviving distances in the left-turn scenario,\nrespectively. c-d, curves of training rewards and surviving distances in the\ncongestion scenario, respectively.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nTABLE I\nCOMPARISON OF TIME EFFICIENCY OF DIFFERENT RL ALGORITHMS\nAlgorithm\nTime consumption (s) per 10000 steps\nPHIL-TD3\n360.70\nIA-TD3\n348.71\nHI-TD3\n328.93\nVanilla-TD3\n329.39\nlearning efï¬ciency. In this scenario, PHIL-TD3 achieves the\nbest asymptotic average surviving distance of the baselines\nin 220 episodes, improving the learning efï¬ciency by over\n120%. Overall, the results in this training session highlight the\nsigniï¬cant superiority of the proposed algorithm in learning\nperformance.\nWe further explore the learning performance of RLs with\ndifferent task difï¬culties, which gives rise to Fig. 8. The nor-\nmal setting complies with the problem deï¬nition in Section VI,\nwhich is adopted throughout the report, while the tough setting\nchanges consecutive-frame input of Eq. 28 into a single frame\ninput, impairing the temporal perception ability of RL agents.\nAt the high level, the statistical results of the normal setting\nare aligned with the trends of Figs. 7(a-d). And it is indicated\nthat the tough setting does not change the performance ranking\nof algorithms despite the degradations in different degrees. At\nthe detail level, the performance difference between the normal\nand the tough settings, i.e., the ratios in Fig. 8, can manifest\nmore algorithmic characteristics. Speciï¬cally, PHIL-TD3 and\nIA-TD3, which own the behavior-cloning objective, are less\naffected by the incomplete problem deï¬nition of the tough\nsetting, whereas HI-TD3, and vanilla TD3, which less or not\nrely on human guidance, are signiï¬cantly degraded in the same\ncondition. Despite the single-frame state in the autonomous\ndriving task is not fairly reasonable, the ï¬ndings through this\ncomparison are useful. Since numerous complex real-world\ntasks are intractable to be well-deï¬ned or are only partially\nobservable, the strong integration of human guidance into RL,\ne.g., behavior-cloning, can play a more remarkable role than\npure RL algorithms.\nThen, we investigate the contributions of different compo-\nnents in improving the performance of the proposed PHIL\nalgorithm. The results are provided in Appendix 1. Three com-\nponents, the behavior cloning objective of Eq. 12 of Section\nIV-B, the proposed prioritized experience replay mechanism\nof Section IV-C, and the intervention-based reward shaping\nmechanism of Section IV-D, are validated to be effective,\nrespectively. The results show that the proposed prioritized\nhuman-demonstration replay mechanism plays a crucial role\nin improving the ultimate performance.\nLast, we evaluate the computational efï¬ciency. The CPU\nclock time of different algorithms is compared in table I. It\nis shown that the training time consumption of the proposed\nalgorithm is similar to that of IA-TD3. This is because the\nproposed priority calculation scheme consumes very few com-\nputational resources. In all, the proposed PHIL-TD3 greatly\nimproves the training efï¬ciency and performance without\nrequiring signiï¬cantly higher computational resources.\n30\n50\n70\n90\nCongestion\nNoise-injected\nCongestion\nVariant\nCongestion\n**\n12\n16\n20\n24\nLeft-turn\nNoise-injected\nLeft-turn\nVariant  Left-turn\nPHIL-TD3 (ours)\nIA-TD3\nHI-TD3\nVanilla TD3\n***\n*\nSurviving distance (m)\nSurviving distance (m)\nb\nc\n1\n2\n3\n4\n1\n2\n3\n4\n5\n6\n74\n78\n78\n72\n66\n100\n100\n100\n100\n96\n90\n88\n82\n98\n94\n88\n94\n94\n90\n90\n98\n92\n84\n86\n70\n75\n80\n85\n90\n95\n100\nC\nNC\nVC\nL\nNL\nVL\nPHIL-TD3\n(ours)\nIA-TD3\nHI-TD3\nVanilla TD3\nSuccess rate (%)\n**\n*\n*\na\nFig. 9.\nHigh-level driving performance of different RL strategies under\nsix autonomous driving scenarios. The two noise-injected scenarios and two\nvariant scenarios are different with the two training scenarios, which can\nexamine the robustness and adaptiveness, respectively. â€œCâ€ and â€œLâ€ refer to\ncongestion scene and left-turn scene, respectively, while â€œNâ€ and â€œVâ€ denote\nnoise-injected and variant scene, respectively. a, the heatmap of success rate.\nb, the barplot of surviving distance in the left-turn scenarios. The theoretical\nmaximum surviving distance of the scenario is 21 meters. The error bar\ndescribes the standard deviation. c, the barplot of surviving distance in the\ncongestion scenarios. The theoretical maximum surviving distance of the\nscenario is 80 meters. The error bar describes the standard deviation. The\npaired t-test is adopted for the statistical test.\nD. Evaluation of Testing Performance of Driving Strategies\nIn this section, the post-trained driving strategies are tested\nin terms of autonomous driving performance, adaptiveness,\nand robustness, which can further evaluate the practicality of\nthe above algorithms.\nThe zero-mean Gaussian noises, of which the standard devi-\nation is 5% of the whole control domain, are injected to output\ncommands of the driving strategies to test the robustness.\nMore types and amounts of surrounding vehicles are added to\nconstruct variant scenarios to test the adaptiveness. We conduct\n50 runs with the same sequence of random seeds for each\npost-trained strategy in each scenario. The success rate, which\nis deï¬ned as the number of completed runs divided by the\ntotal attempts in the same scenario, is taken as the metric for\nevaluating the safety performance in Fig. 9(a). Our PHIL-TD3\nachieves the highest success rate in all scenarios, showing its\nsuperior task-completeness abilities. The vanilla TD3, albeit\nwith its unstable training performance, performs competitively\nlike IA-TD3 and HI-TD3 in the testing stage. Considering the\ntwo trained scenes (rows 1, 4) and noise-injected scenes (rows\n2, 5), three baseline strategies behave acceptably, nevertheless,\nthe scenario variants (rows 3, 6) signiï¬cantly degrade their\nsafety. Our PHIL, instead, maintains the highest ability regard-\nless of varying testing conditions, manifesting itself with good\nrobustness and adaptiveness. In Figs. 9(b-c), PHIL-TD3 once\nagain shows its superiority in safety by the highest average\nsurviving distance, and importantly, its performance stability\nis conï¬rmed due to the lowest variance.\nFig. 10 can further evaluate the detailed performance of\ndriving strategies. Time consumption of the episode is the\nsecondary target of RL optimization in the left-turn task,\nwhich is implied in the reward function of Eq. 31; thus,\nthe related boxplot is illustrated in Fig. 10(a) to access this\nobjective. It is found that the proposed strategy enjoys minimal\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\nTABLE II\nCOMPARISON OF THE SURVIVING DISTANCE OF HUMAN-RELATED DRIVING STRATEGIES IN THREE LEFT-TURN SCENARIOS.\nSurviving distance, meter, â†‘\nLeft-turn\nNoise-injected Left-turn\nVariant Left-turn\nPHIL-TD3 (ours)\n21.28Â±0.02\n21.27Â±0.02\n21.29Â±0.02\nIA-TD3\n20.37Â±2.58\n19.75Â±3.05\n19.46Â±3.03\nHI-TD3\n20.87Â±1.76\n20.63Â±1.74\n19.90Â±2.65\nHuman policy model\n20.70Â±1.82\n20.88Â±1.32\n20.90Â±1.21\nTABLE III\nCOMPARISON OF THE SURVIVING DISTANCE OF HUMAN-RELATED DRIVING STRATEGIES IN THREE CONGESTION SCENARIOS.\nSurviving distance, meter, â†‘\nCongestion\nNoise-injected Congestion\nVariant Congestion\nPHIL-TD3 (ours)\n80.15Â±0.08\n79.26Â±4.30\n77.29Â±11.55\nIA-TD3\n79.26Â±5.90\n78.64Â±6.11\n78.39Â±7.66\nHI-TD3\n76.27Â±16.00\n76.02Â±13.72\n73.57Â±18.49\nHuman policy model\n80.11Â±0.07\n77.66Â±12.27\n75.15Â±15.20\nLeft-turn\nNoise-injected\nLeft-turn\nVariant \nLeft-turn\nCongestion\nNoise-injected \nCongestion\nVariant \nCongestion\n****\n****\n****\n****\n*\nb\na\nPHIL-RD3 (ours)\nIA-TD3\nHI-TD3\nVanilla-TD3\n****\n****\n****\n****\n****\n****\n*\nFig. 10.\nLow-level driving performance of different RL strategies under\nsix autonomous driving scenarios.a, the boxplot of time consumption of the\nepisode without failure in the left-turn scenarios. b, the boxplot of average\nlateral acceleration of the episode in the congestion scenarios. The paired\nt-test is adopted for the statistical test.\ntime consumption, which is signiï¬cantly different from other\ncandidates. In congestion tasks, smoothness is the secondary\ntarget of the reward function of Eq. 32; thus, we choose the\nlateral acceleration as the smoothness measure and provide\nthe associated boxplot in Fig. 10(b). The comparable human-\ninvolved strategies show their superior smoothness to vanilla\nTD3 in the training and noise-injected scenes, while the\nvariant congestion scenario proï¬tably validates the advantage\nof PHIL-TD3.\nAdditionally, we compare the performance of three human\nguidance-based RL algorithms to the human guidance itself.\nSpeciï¬cally, the surviving distance of these human-involved\nRLs are compared with the human policy model, and the\nresults are provided in Tables II-III, where the results (mean\nand standard deviation) are calculated by 50 evaluation seeds.\nThe results suggest the superiority of the proposed PHIL-TD3\nover the human policy model.\nOverall, our PHIL-TD3 perpetuates its predominance of\ntraining performance and takes the top spot in the testing stage.\nE. Discussion on Prioritized Human Experience Mechanism\nIn this section, we explore the effect of PHIL-RL from three\naspects: the performance improvement by the TDQA mecha-\nnism, the merit of the single-buffer experience replay structure,\nand the algorithmic robustness to bad demonstrations.\nTDQA, as the crucial innovation of PHIL-TD3, can im-\nprove learning performance in the context of human guidance-\nbased RLs, as suggested in Sections VII-C and VII-D. More\nspeciï¬cally, it establishes a novel priority indicator to deal with\nvarious human guidance. Thus, we ï¬rst evaluate TDQA by\ncomparing different priority schemes. â€œQ-advâ€ represents the\nscheme in which the priority of human guidance is calculated\nbased only on Q-advantage. â€œTDâ€, i.e., temporal difference,\nthe scheme is inherited from the original PER method, but\nthe TD weights of human demonstrations in it are doubled to\nhighlight the human guidance in the replay buffer.\nFive learning attempts are conducted with the same se-\nquence of random seeds for each candidate, and the corre-\nsponding learning curves are in Fig. 11. We ï¬nd scheme\ncomparison in two training scenarios shows similar trends\nwhen observing results in Figs. 11(a-b) and Figs. 11(c-d). The\npure TD scheme learns faster than pure Q-advantage in both\nscenarios, yet its asymptotic scores (both reward and surviving\ndistance) are signiï¬cantly lower than those of the Q-advantage\nscheme. To be more speciï¬c, we evaluate different weigh of\nâ€œTDâ€ and â€œQ-advâ€ and provide the learning performance\nin Fig. 12. Under the same TD, Q-advantage is weighted\nwith three importance levels. In particular, the equal weighting\nscheme, i.e., w = 1e0, is the adopted default scheme in the\nreport, whereas the other two variants are for comparison. It is\nshown that a larger TD (w = 1eâˆ’1) makes faster convergence\nbut can lead to unfavorable asymptotic performance, while\na larger Q-advantage (w = 1e2) can achieve the same-level\nperformance as the default setting, despite sometimes slower\nlearning process. The above results, reveal the same perfor-\nmance trends as Fig. 11. That is, TD error accelerates the\nconvergence speed and Q-advantage contributes to improving\nconvergence performance.\nEssentially, these two schemes score human guidance based\non different indicators, and a better indicator can provide RL\nwith more high-quality guidance to improve learning efï¬-\nciency. Thus, we ï¬nd TD indicator, as proved in conventional\nPER, is indeed beneï¬cial to rapidly improve performance,\nnonetheless, the Q-advantage indicator is superior to the\nTD indicator in the later stage of the training process. The\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n0.1\n0.2\n0.3\n0.4\nReward\nPHIL(TD+Q-adv)\nQ-adv\nTD\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n0\n20\n40\n60\n80\nSurviving distance (m)\nPHIL(TD+Q-adv)\nQ-adv\nTD\nc\nd\na\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n-1.5\n-1\n-0.5\n0\nReward\nPHIL(TD+Q-adv)\nQ-adv\nTD\nb\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n12\n16\n20\nSurviving distance (m)\nPHIL(TD+Q-adv)\nQ-adv\nTD\nFig. 11.\nLearning efforts of experience replay mechanisms with different\npriority indicators.. a-b, training rewards in left-turn and congestion scenario,\nrespectively. c-d, surviving distances in left-turn and congestion scenario,\nrespectively.\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n0.1\n0.2\n0.3\n0.4\nReward\nPHIL-TD3(adopted, w=1e0)\nPHIL-TD3(w=1e-1)\nPHIL-TD3(w=1e2)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n10\n15\n20\nSurviving distance (m)\nPHIL-TD3(adopted, w=1e0)\nPHIL-TD3(w=1e-1)\nPHIL-TD3(w=1e2)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n0\n20\n40\n60\n80\nSurviving distance(m)\nPHIL-TD3(adopted, w=1e0)\nPHIL-TD3(w=1e2)\nPHIL-TD3(w=1e-1)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n-2\n-1.5\n-1\n-0.5\n0\nReward\nPHIL-TD3(adopted, w=1e0)\nPHIL-TD3(w=1e2)\nPHIL-TD3(w=1e-1)\nc\nd\na\nb\nFig. 12.\nLearning efforts of experience replay mechanisms with different\nweighting schemes. a-b, training rewards in left-turn and congestion scenario,\nrespectively. c-d, surviving distances in left-turn and congestion scenario,\nrespectively.\ndelayed superiority of Q-advantage complies with intuition\nsince unlike the direct indicator as TD, the evaluation ability\nof the Q network, i.e., the source of Q-advantage, also needs\nto be trained. The proposed PHIL, which smartly combines\nboth indicators, achieves the most favorable performance in\nthe two scenarios, showing the effectiveness of the TDQA\nmechanism.\nPHIL puts the human guidance and exploratory experience\nof RL into the same experience replay buffer. This structure\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n0\n0.1\n0.2\n0.3\n0.4\nReward\nPHIL\nRD2-PHIL\nRD2-TD3\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n-3\n-2\n-1\n0\nReward\nPHIL\nRD2-PHIL\nRD2-TD3\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n5\n10\n15\n20\nSurviving distance (m)\nPHIL\nRD2-PHIL\nRD2-TD3\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTraining episode\n0\n20\n40\n60\n80\nSurviving distance (m)\nPHIL\nRD2-PHIL\nRD2-TD3\nc\nd\na\nb\nFig. 13. Learning efforts of experience replay mechanism with different buffer\nstructure. a-b, the training rewards of algorithms with different experience\nreplay structures in the left-turn scenario and congestion, respectively. c-d,\nthe training surviving distances of algorithms with different experience replay\nstructures in the left-turn and congestion scenario, respectively.\n0%\n50%\n100%\nVL\nNL\nL\nVC\nNC\nC\nVL\nNL\nL\nVC\nNC\nC\nVL\nNL\nL\nVC\nNC\nC\nPHIL-TD3\n(ours)\nIA-TD3\nHI-TD3\nMean of average surviving \ndistance (Good guidance)\nMean of average surviving \ndistance (Poor guidance)\n50%\nFig. 14. Stacked barplot of the surviving distance of different human-guided\nRL strategies under good/poor guidance in all scenarios.\ndiffers from the double distributed scheme which is repre-\nsented by R2D3. To evaluate the performance of these two\nschemes under the devised autonomous driving tasks, RD2-\nTD3 is developed which utilizes TD as the indicator to\nrespectively retrieve data from two buffers. Additionally, the\nTDQA priority mechanism is ported to the RD2-TD3 setting\nforming the other variant, RD2-PHIL. Five learning attempts\nwith the same sequence of random seeds are conducted by\nRD2-TD3 and RD2-PHIL. Through learning curves in Figs.\n13(a-d), it is found that the double distributed buffer scheme,\ni.e., RD2-PHIL, fails to achieve the same level of learning\nefï¬ciency as the proposed PHIL. A possible reason behind\nthis is that human guidance can only be utilized in a chunk\nway under the double-buffer setting, whereas the single buffer\nscheme of PHIL is more ï¬‚exible and friendly to small-scale\nhuman guidance data. The conventional RD2-TD3 is least\nfavorable, which is within expectation due to the lack of\nthe TDQA mechanism. To sum up, the results in Fig. 13\nsupport the single-buffer structure utilized in the PHIL-TD3,\nand proï¬tably suggest the effectiveness of the proposed TDQA\nmechanism.\nA general situation occurs that human guidance is not per-\nfect, and thus an unqualiï¬ed human participant can sometimes\nconduct actions that are harmful to the task. We test if the un-\nfavorable guidance of the unqualiï¬ed human would impair the\nlearning process, that is, evaluating the robustness to harmful\nguidance. It is noticeable that the robustness discussed here\nis distinguished from that in Section VII-D: we discuss how\nthe algorithms are affected by poor guidance instead of the\nanti-noise ability of post-trained driving strategies. The human\nintervention condition of the training stage keeps the same as\nforegoing experiments, while one-third of the demonstrations\nfrom the human model are replaced with random actions to\nsimulate non-proï¬cient human behaviors.\nPost-trained driving strategies under poor guidance are\ntested to conduct 50 runs in each scenario and are compared\nwith those under the good guidance of Fig. 9. The stacked\nbarplots in Fig. 14 provide the adversarial testing performance\nof three human-guidance-based RL algorithms under good\nand poor guidance. We take the average surviving distance\nas the metric and the less performance deterioration by poor\nguidance suggests better robustness. Our PHIL-TD3 exhibits\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\n0\n5\n10\n15\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\nPHIL-TD3 (ours)\nIA-TD3\nHI-TD3\n0\n5\n10\n15\n20\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n0\n5\n10\n15\n20\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n0\n5\n10\n15\n20\n25\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\nPHIL-TD3 (ours)\nIA-TD3\nHI-TD3\n0\n5\n10\n15\n20\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n0\n20\n40\n60\n80\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23 25\na\nMean of absolute lateral acceleration (m/s2) \n0.520\n0.796\n1.072\nMean of absolute lateral acceleration (m/s2) \n1.50\n1.95\n2.40\n0.40\n1.00\n1.60\nMean of absolute longitudinal acceleration (m/s2) \n1.30\n1.90\n2.50\nMean of absolute longitudinal acceleration (m/s2) \n1.300\n1.972\n2.644\nMean of absolute longitudinal acceleration (m/s2) \n1.34\n2.06\n2.78\nPercentage\nPercentage\nPercentage\nPercentage\nPercentage\nPercentage\nb\nc\nd\ne\nf\nMean of absolute lateral acceleration (m/s2) \nFig. 15. Acceleration distributions of different human-guided RL strategies.\na-c, the frequency distribution plot of the average absolute value of the\nlongitudinal acceleration in the left-turn scene, noise-injected left-turn scene,\nand variant left-turn scene, respectively. The smaller acceleration indicates a\nbetter driving smoothness. d-f, t he frequency distribution plot of the average\nabsolute value of the lateral acceleration in the congestion scene, noise-\ninjected congestion scene, and variant congestion scene, respectively. The\nsmaller acceleration indicates a better driving smoothness.\ngood performance since a nearly 50:50 situation occurs in all\nsix scenarios. IA-TD3 falls behind with a 2.1% degradation\non average in poor guidance context, while HI-TD3 is even\nimproved by an average of 3.6% extent given poor guidance.\nIntuitively, poor guidance would remarkably degrade PHIL\nand IA-TD3 since they utilize the behavior-cloning objective\nto learn from human guidance, while HI-TD3, which only\nsubstitutes partial RL explorations with human guidance, can\nbe less affected. The not-degraded HI-TD3 and most-degraded\nIA-TD3 support the above idea. Our PHIL defeating IA-TD3\nis attributed to the TDQA mechanism: Q-advantage well\naccess the quality of human demonstrations and feed more\nhigh-quality demonstrations to the RL agent; accordingly,\nthe agent learns greater from good guidance than negative\nguidance. The secondary optimization target of RL, i.e.,\ndriving smoothness, is evaluated in Fig. 15 by acceleration\ndistribution. The proposed PHIL-TD3 wins all scenarios by\nthe most favorable smoothness which further conï¬rms the\nabovementioned superiority.\nOverall, the TDQA mechanism, as the core innovation\nof the PHIL-RL algorithm, contributes to the preponderant\nlearning performance through its unique discriminatory power\non the quality of human guidance. It also improves the robust-\nness to poor guidance, which can relieve the requirement on\nthe qualiï¬cation of human guidance. Additionally, the single\nbuffer setting is more favorable than the double distributed\nbuffer scheme under autonomous driving tasks of this report.\nVIII. CONCLUSIONS\nIn this paper, we establish a human-guidance-based rein-\nforcement learning framework and propose a novel experience\nutilization mechanism of human guidance. Based on that, we\nput forward an algorithm, PHIL-TD3, aiming at improving\nalgorithmic abilities in the context of human-in-the-loop RL.\nWe also introduce a human behavior modeling mechanism\nto relieve the human workload. PHIL-TD3 is employed to\nsolve two challenging autonomous driving tasks, and its\nperformance is comparatively evaluated against state-of-the-\nart human-guidance-based RLs as well as the non-guidance\nbaseline. Three main points are obtained through experimental\nresults:\n1) The proposed PHIL-TD3 can improve the learning efï¬-\nciency by over 700% and 120% under the adopted two situa-\ntions, respectively, and achieve remarkably higher asymptotic\nperformance compared to state-of-the-art human-guidance-\nbased RLs.\n2) The proposed PHIL-TD3 achieves the most favorable per-\nformance, robustness, and adaptiveness in a series of metrics\nunder the adopted two challenging autonomous driving tasks.\n3) The proposed TDQA mechanism prominently con-\ntributes to the advance of PHIL-TD3 and can well discriminate\nthe quality of various human guidance to relieve humans by\nless requiring on human proï¬ciency.\nIn future works, the proposed algorithm is expected to\nbe transferred to a real-world ground vehicle, whereby the\neffect of human guidance on RLâ€™s optimization and control\nperformance can be further examined in real life.\nREFERENCES\n[1] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, and T. Graepel, â€œA general rein-\nforcement learning algorithm that masters chess, shogi, and go through\nself-play,â€ Science, vol. 362, no. 6419, pp. 1140â€“1144, 2018.\n[2] X. Gao, J. Si, Y. Wen, M. Li, and H. Huang, â€œReinforcement learning\ncontrol of robotic knee with human-in-the-loop by ï¬‚exible policy iter-\nation,â€ IEEE Transactions on Neural Networks and Learning Systems,\n2021.\n[3] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura,\nâ€œNavigating occluded intersections with autonomous vehicles using deep\nreinforcement learning,â€ in 2018 IEEE International Conference on\nRobotics and Automation (ICRA).\nIEEE, Conference Proceedings, pp.\n2034â€“2039.\n[4] J. Wu, Z. Wei, W. Li, Y. Wang, Y. Li, and D. U. Sauer, â€œBattery\nthermal-and health-constrained energy management for hybrid electric\nbus based on soft actor-critic drl algorithm,â€ IEEE Transactions on\nIndustrial Informatics, vol. 17, no. 6, pp. 3751â€“3761, 2020.\n[5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[6] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dab-\nney, D. Horgan, B. Piot, M. Azar, and D. Silver, â€œRainbow: Combining\nimprovements in deep reinforcement learning,â€ in Thirty-second AAAI\nconference on artiï¬cial intelligence, Conference Proceedings.\n[7] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProx-\nimal policy optimization algorithms,â€ arXiv preprint arXiv:1707.06347,\n2017.\n[8] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,â€ in International conference on machine learning.\nPMLR,\nConference Proceedings, pp. 1861â€“1870.\n[9] E. O. Neftci and B. B. Averbeck, â€œReinforcement learning in artiï¬cial\nand biological systems,â€ Nature Machine Intelligence, vol. 1, no. 3, pp.\n133â€“143, 2019.\n[10] M. L. Littman, â€œReinforcement learning improves behaviour from eval-\nuative feedback,â€ Nature, vol. 521, no. 7553, pp. 445â€“451, 2015.\n[11] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,\nT. RothÂ¨orl, T. Lampe, and M. Riedmiller, â€œLeveraging demonstrations\nfor deep reinforcement learning on robotics problems with sparse\nrewards,â€ arXiv preprint arXiv:1707.08817, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\n[12] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,\nD. Horgan, J. Quan, A. Sendonaris, and I. Osband, â€œDeep q-learning\nfrom demonstrations,â€ in Thirty-second AAAI conference on artiï¬cial\nintelligence, Conference Proceedings.\n[13] G. Libardi, G. De Fabritiis, and S. Dittert, â€œGuided exploration with\nproximal policy optimization using a single demonstration,â€ in In-\nternational Conference on Machine Learning.\nPMLR, Conference\nProceedings, pp. 6611â€“6620.\n[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski,\nâ€œHuman-level control through deep reinforcement learning,â€ nature, vol.\n518, no. 7540, pp. 529â€“533, 2015.\n[15] S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell, M. Riedl, and\nA. Thomaz, â€œLearning from explanations using sentiment and advice in\nrl,â€ IEEE Transactions on Cognitive and Developmental Systems, vol. 9,\nno. 1, pp. 44â€“55, 2017.\n[16] J. MacGlashan, M. K. Ho, R. Loftin, B. Peng, G. Wang, D. L. Roberts,\nM. E. Taylor, and M. L. Littman, â€œInteractive learning from policy-\ndependent human feedback,â€ in International Conference on Machine\nLearning.\nPMLR, Conference Proceedings, pp. 2285â€“2294.\n[17] B. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei,\nâ€œReward learning from human preferences and demonstrations in atari,â€\nAdvances in Neural Information Processing Systems, vol. 31, 2018.\n[18] W. Saunders, G. Sastry, A. StuhlmÂ¨uller, and O. Evans, â€œTrial without\nerror: Towards safe reinforcement learning via human intervention,â€\nin Proceedings of the 17th International Conference on Autonomous\nAgents and MultiAgent Systems. Richland, SC: International Foundation\nfor Autonomous Agents and Multiagent Systems, 2018, Conference\nProceedings, p. 2067â€“2069.\n[19] F. Wang, B. Zhou, K. Chen, T. Fan, X. Zhang, J. Li, H. Tian, and J. Pan,\nâ€œIntervention aided reinforcement learning for safe and practical policy\noptimization in navigation,â€ in Proceedings of The 2nd Conference\non Robot Learning, ser. Proceedings of Machine Learning Research,\nvol. 87.\nPMLR, 2018, pp. 410â€“421.\n[20] C. Gulcehre, T. Le Paine, B. Shahriari, M. Denil, M. Hoffman, H. Soyer,\nR. Tanburn, S. Kapturowski, N. Rabinowitz, and D. Williams, â€œMaking\nefï¬cient use of demonstrations to solve hard exploration problems,â€\nin International Conference on Learning Representations, Conference\nProceedings.\n[21] E. Senft, S. Lemaignan, P. E. Baxter, M. Bartlett, and T. Belpaeme,\nâ€œTeaching robots social autonomy from in situ human guidance,â€ Science\nRobotics, vol. 4, no. 35, 2019.\n[22] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel,\nâ€œOvercoming exploration in reinforcement learning with demonstra-\ntions,â€ in 2018 IEEE International Conference on Robotics and Au-\ntomation (ICRA).\nIEEE, Conference Proceedings, pp. 6292â€“6299.\n[23] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman,\nE. Todorov, and S. Levine, â€œLearning complex dexterous manipulation\nwith deep reinforcement learning and demonstrations,â€ in Proceedings\nof Robotics: Science and Systems, Conference Proceedings.\n[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, â€œPrioritized experience\nreplay,â€ arXiv preprint arXiv:1511.05952, 2015.\n[25] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, â€œA survey of\nrobot learning from demonstration,â€ Robotics and autonomous systems,\nvol. 57, no. 5, pp. 469â€“483, 2009.\n[26] S. Fujimoto, H. Hoof, and D. Meger, â€œAddressing function approxi-\nmation error in actor-critic methods,â€ in International Conference on\nMachine Learning.\nPMLR, Conference Proceedings, pp. 1587â€“1596.\n[27] S. Ross, G. Gordon, and D. Bagnell, â€œA reduction of imitation learning\nand structured prediction to no-regret online learning,â€ in Proceedings\nof the fourteenth international conference on artiï¬cial intelligence and\nstatistics.\nJMLR Workshop and Conference Proceedings, Conference\nProceedings, pp. 627â€“635.\n[28] M. Liebner, M. Baumann, F. Klanner, and C. Stiller, â€œDriver intent\ninference at urban intersections using the intelligent driver model,â€\nin 2012 IEEE Intelligent Vehicles Symposium.\nIEEE, Conference\nProceedings, pp. 1162â€“1167.\n[29] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, â€œCarla:\nAn open urban driving simulator,â€ in Conference on robot learning.\nPMLR, Conference Proceedings, pp. 1â€“16.\n[30] A. Y. Ng, D. Harada, and S. Russell, â€œPolicy invariance under reward\ntransformations: Theory and application to reward shaping,â€ in Icml,\nvol. 99, Conference Proceedings, pp. 278â€“287.\nAPPENDIX\nTheorem 1 (Policy Optimality Invariance of the Human\nIntervention-based Reward Shaping): Let the intervention-\nbased reward shaping function F : S Ã— A Ã— S â†’R add a\nnegative constant to the human intervened state as Eq. 17, if\nthe human intervention will certainly occur at state st when\nthe next state st+1 is unacceptable, then the reward shaping\nfunction F does not change the policy optimality.\nProof 1: According to [30], potential-based reward shaping\nfunction F : S Ã—AÃ—S is proven to be the only form that can\npreserve policy optimality. Speciï¬cally, F is represented as:\nF(st, at, st+1) = Î³Î¦(st+1)) âˆ’Î¦(st),\n(33)\nwhere Î¦ : S â†’R is called the potential function deï¬ned over\nthe state space.\nThus, the proof converts to construct potential function Î¦.\nDeï¬ne the potential function Î¦ as:\nÎ¦(st) =\n( rpen\nÎ³ ,\nif st is unacceptable\n0,\notherwise.\n(34)\nThen, when humans intervene in the state st (meaning st+1\nis unacceptable), F becomes:\nF(st, at, st+1) = Î³Î¦(st+1) âˆ’Î¦(st)\n= rpen\nÎ³\nÂ· Î³ âˆ’0 = rpen.\n(35)\nAnd when humans do not intervene the state, F becomes:\nF(st, at, st+1) = Î³Î¦(st+1) âˆ’Î¦(st) = 0 âˆ’0 = 0.\n(36)\nLumping Eqs. 35 and 36, F turns into the reward-shaping\nterm of Eq. 17, shown as:\nrshape\nt\n=rt + F(st, at, st+1)\n=rt + rpen Â· [(âˆ†t = I) âˆ§(âˆ†tâˆ’1 = 0)],\n(37)\nwhere [(âˆ†t = I)âˆ§(âˆ†tâˆ’1 = 0)] refers to the intervention event\nof the human.\nHence, we complete the proof.\nRemark 1: Theorem 1 is established on the below assump-\ntions: humans are considered to owe invariant judgment on\nthe environment state. In this manner, the Î¦ can be seen as a\nstable function deï¬ned in the state space.\nRemark 2: The assumption of Remark 1 is hard to be\nmaintained in practice. This is because 1) the varying mental\nand physical status of one speciï¬c human participant would\naffect its accurate judgment on the environment state; 2) the\njudgment on the environment will be varying across different\nhuman participants; 3) the state space in the context of\ndeep networks (the image-based one in our manuscript) is\nintractable to be identiï¬ed by humans accurately.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n15\nTABLE A1\nCONFIGURATION OF THE EXPERIMENTAL PLATFORM.\nType\nDescription\nDetails\nWorkstation\nOperation system\nUbuntu 20.04\nWorkstation\nCPU + RAM\nAMD Ryzen 3900X + 32GB\nWorkstation\nGPU\nNVIDIA RTX 2080S\nDriving simulator\nScenario software\nCARLA\nDriving simulator\nSteering wheel suit\nLogitech G29\nDriving simulator\nDisplays\nJoint heads-up monitorsÃ—3\nDriving simulator\nOther equipment\nDriver seat suit\nRobotic vehicle\nVehicle brand\nWheeled UGV-Hunter\nRobotic vehicle\nSize dimension\n1000mm Ã— 740mm Ã— 400mm\nRobotic vehicle\nCommunication type\nROS publisher-subscriber\nRobotic vehicle\nCalculation board\nXavier NX Dev Kit\nOther\nProgramming\nPython\nOther\nNeural network toolbox\nPytorch\nTABLE A2\nARCHITECTURE AND DETAILS OF VALUE NEURAL NETWORK (CRITIC)\nParameter\nValue\nInput (state + action) shape\n[80,45,2] + [1]\nNetwork convolution Filter feature\n[6,16] (kernel size 6 Ã— 6)\nNetwork pooling feature\nMaxpooling (Stride 2)\nNetwork fully connected layer feature\n[256,128,64]\nTABLE A3\nARCHITECTURE AND DETAILS OF POLICY NEURAL NETWORK (ACTOR)\nParameter\nValue\nInput (state) shape\n[80,45,2]\nNetwork convolution Filter feature\n[6,16] (kernel size 6 Ã— 6)\nNetwork pooling feature\nMaxpooling (Stride 2)\nNetwork fully connected layer feature\n[256,128,64]\nTABLE A4\nARCHITECTURE AND DETAILS OF DAGGER-BASED HUMAN POLICY\nMODEL\nParameter\nValue\nInput (state) shape\n[80,45,1]\nNetwork convolution Filter feature\n[6,16] (kernel size 6 Ã— 6)\nNetwork pooling feature\nMaxpooling (Stride 2)\nNetwork fully connected layer feature\n[256,128,64]\nTABLE A5\nHYPERPARAMETERS FOR RL TRAINING\nType\nDescription\nDetails\nMaximum episode\nCutoff episode number of the training process\n400\nMinibatch size (N)\nCapacity of minibatch\n128\nActor learning rate\nInitial learning rate (policy/actor networks)\n5e-4\nCritic learning rate\nInitial learning rate (value/critic networks)\n2e-4\nLearning rate decay\nDelay of learning rate (per episode)\n0.996\nActivation function\nActivation function of the networks\nRelu\nInitial exploration\nInitial exploration rate of noise in Ïµ greedy\n1\nFinal exploration\nCutoff exploration rate of noise in Ïµ greedy\n0.05\nGamma (Î³)\nDiscount factor of the Bellman equation\n0.95\nSoft updating factor\nParameter update frequency to target networks\n1e-3\nNoise scale (Ïµ)\nNoise amplitude of action in TD3\n0.2\nBounding box (c)\nBounding of the exploratory action in TD3\n1\nPolicy delay (d)\nUpdate frequency of critic over actor\n1\nTABLE A6\nHYPERPARAMETERS FOR THE PER MECHANISM\nType\nDescription\nDetails\nReplay buffer size\nCapacity of PER buffer\n1e5\nPriority factor (Î±)\nPriority scaling factor\n0.6\nSample factor (Î²)\nImportance sampling correlation\n1\nOffset factor (Îµ)\nTiny constant avoiding zero retrieving probability\n1e-3\nTABLE A7\nHYPERPARAMETERS FOR DAGGER-BASED HUMAN POLICY MODEL.\nType\nDescription\nDetails\nLearning rate\nInitial learning rate\n1e-4\nActivation function\nActivation function of the network\nRelu\nMaximum episode\nCutoff episode number of the training process\n50\nBatch size\nCapacity of minibatch\n128\n0\n100\n200\n300\n400\nTraining episode\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\nReward\nPHIL-TD3(ours)\nWithout Q-adv\nWithout reward shaping\nWithout behavior cloning\n0\n100\n200\n300\n400\nTraining episode\n0\n20\n40\n60\n80\nSurviving distance(m)\nPHIL-TD3(ours)\nWithout Q-adv\nWithout reward shaping\nWithout behavior cloning\n0\n100\n200\n300\n400\nTraining episode\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nReward\nPHIL-TD3(ours)\nWithout Q-adv\nWithout reward shaping\nWithout behavior cloning\n0\n100\n200\n300\n400\nTraining episode\n10\n15\n20\nSurviving distance (m)\nPHIL-TD3(ours)\nWithout Q-adv\nWithout reward shaping\nWithout behavior cloning\na\nb\nd\nc\nFig. 1.\nAblation study of the proposed algorithm. a-b, curves of training\nrewards and surviving distances in the left-turn scenario, respectively. c-d,\ncurves of training rewards and surviving distances in the congestion scenario,\nrespectively.\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2021-09-26",
  "updated": "2022-11-29"
}