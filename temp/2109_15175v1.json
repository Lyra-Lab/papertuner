{
  "id": "http://arxiv.org/abs/2109.15175v1",
  "title": "Coordinated Reinforcement Learning for Optimizing Mobile Networks",
  "authors": [
    "Maxime Bouton",
    "Hasan Farooq",
    "Julien Forgeat",
    "Shruti Bothe",
    "Meral Shirazipour",
    "Per Karlsson"
  ],
  "abstract": "Mobile networks are composed of many base stations and for each of them many\nparameters must be optimized to provide good services. Automatically and\ndynamically optimizing all these entities is challenging as they are sensitive\nto variations in the environment and can affect each other through\ninterferences. Reinforcement learning (RL) algorithms are good candidates to\nautomatically learn base station configuration strategies from incoming data\nbut they are often hard to scale to many agents. In this work, we demonstrate\nhow to use coordination graphs and reinforcement learning in a complex\napplication involving hundreds of cooperating agents. We show how mobile\nnetworks can be modeled using coordination graphs and how network optimization\nproblems can be solved efficiently using multi- agent reinforcement learning.\nThe graph structure occurs naturally from expert knowledge about the network\nand allows to explicitly learn coordinating behaviors between the antennas\nthrough edge value functions represented by neural networks. We show\nempirically that coordinated reinforcement learning outperforms other methods.\nThe use of local RL updates and parameter sharing can handle a large number of\nagents without sacrificing coordination which makes it well suited to optimize\nthe ever denser networks brought by 5G and beyond.",
  "text": "Coordinated Reinforcement Learning\nfor Optimizing Mobile Networks\nMaxime Bouton, Hasan Farooq, Julien Forgeat, Shruti Bothe, Meral Shirazipour, Per Karlsson\nEricsson Research, Santa Clara CA, USA\n{maxime.bouton, hasan.farooq, julien.forgeat, shruti.bothe}@ericsson.com\nAbstract\nMobile networks are composed of many base stations and for each of them many\nparameters must be optimized to provide good services. Automatically and dynam-\nically optimizing all these entities is challenging as they are sensitive to variations\nin the environment and can affect each other through interferences. Reinforcement\nlearning (RL) algorithms are good candidates to automatically learn base station\nconﬁguration strategies from incoming data but they are often hard to scale to\nmany agents. In this work, we demonstrate how to use coordination graphs and\nreinforcement learning in a complex application involving hundreds of cooperating\nagents. We show how mobile networks can be modeled using coordination graphs\nand how network optimization problems can be solved efﬁciently using multi-\nagent reinforcement learning. The graph structure occurs naturally from expert\nknowledge about the network and allows to explicitly learn coordinating behaviors\nbetween the antennas through edge value functions represented by neural networks.\nWe show empirically that coordinated reinforcement learning outperforms other\nmethods. The use of local RL updates and parameter sharing can handle a large\nnumber of agents without sacriﬁcing coordination which makes it well suited to\noptimize the ever denser networks brought by 5G and beyond.\n1\nIntroduction\nOne of the major factors inﬂuencing the quality of experience in mobile networks is the conﬁguration\nof base station antennas, notably the tilt angle. Incorrectly conﬁgured base stations antennas in\na mobile network could interfere with neighboring antennas and deteriorate the signal of users\nin a cell that would otherwise have good coverage. To appropriately conﬁgure a network before\ndeployment, engineers have to anticipate many possible trafﬁc conditions as well as possible sources\nof interference from the environment and the network itself. With a constantly growing demand in\nhigh quality services, an increasing network complexity, and highly dynamic environments, relying\non human interventions to update network conﬁgurations leads to a suboptimal use of the network\nresources and is often very costly.\nInstead, one could automate the optimization procedure under the context of Self-Organizing Net-\nworks (SONs) [1]. Existing approaches for network optimization rely on hand-engineered strategies\nwhich are suboptimal and hard to scale [2]–[4]. Methods relying on mathematical models [5] or\nreinforcement learning (RL) are also used for network optimization [6]–[11]. They are more robust\nand principled. However, due to the large scale and the complex interactions between network\nentities they often consider them as independent agents and do not leverage the beneﬁt of cooperation.\nThe resulting solution is suboptimal despite its scalability. On the other hand optimizing all agents\ntogether can yield better solutions at the expense of scalability.\nIn this work, we propose to derive a coordinated RL approach for dynamic network optimization\ntaking into account local interactions between the base stations. We propose a generic approach to\nPreprint. Under review.\narXiv:2109.15175v1  [cs.LG]  30 Sep 2021\nmodel cellular networks as coordination graphs. The graph representation, provides a support for\nencoding prior knowledge on a network deployment and can be obtained from standard network\nspeciﬁcations. Leveraging the structure of the graph allows using a scalable distributed optimization\nto ﬁnd an approximately optimal joint antenna conﬁguration through message passing. In order\nto dynamically adapt to the environment, we intertwine the optimization procedure with local RL\nupdates that learn value functions for each edge of the graph, taking advantage of the network\ntopology. To improve the sample efﬁciency of the RL algorithm and enabling its scalability to\nhundreds of learning agents we share network parameters across the graph edges which allows using\ndata from many pairs of agents to train the same network. The proposed method can scale to very\nlarge networks and is well suited to handle the expected increase in density of the networks brought\nby 5G and beyond. We empirically show that coordinated RL scales to more than two hundred\nagents and, when augmented with parameter sharing, consistently outperforms other multi-agent\nRL algorithms on an antenna tilt control problem. This work demonstrates how multi-agent RL\nalgorithms can be applied beyond simple games, to a variety of large scale network optimization\nproblems involving coordination between agents.\n2\nRelated Work\nMost prior works on network optimization often address the problem by controlling one antenna\nindependently of each other thus ignoring potential coordination beneﬁts. They rely on rule-based\nmethods [2], [3] or optimization techniques using mathematical models [5]. These methods are often\nsuboptimal and hard to scale to many agents [4] because the search space increases exponentially\nwith the number of agents. Instead, one can focus on reinforcement learning to automatically learn\ncontrol strategies from incoming network data.\nWhen using Reinforcement learning, the problem of coordination still arises. Considering each\nantenna as independent learning agents have been used in the past to address the problem of optimizing\nmobile networks [6], [8], [9], [11], [12], hence failing to capture phenomena like interference.\nLearning algorithms leveraging coordination can use a centralized controller [7], [13] which does\nnot scale to a large number of agents. In contrast, our approach beneﬁts from recent advances\nin multi-agent reinforcement learning to enable scalable coordination achieving an approximately\noptimal joint optimization of the whole network.\nPrior work using the mean-ﬁeld multi-agent RL algorithm [14] has shown promising results for\nantenna tuning problems [8]. This method requires aggregating the inﬂuence of all the neighbors\nthrough a resulting action. This action is not easy to estimate in practice and does not distinguish\nwhich neighbor has the most inﬂuence. Our work demonstrates that multi-agent RL techniques\nthrough value function factorizations [15] is a scalable approach for learning cooperative behavior\nwith hundreds of controlled agents. A key enabler is to model interactions using a coordination graph.\nContrary to other techniques like QMIX [16], it allows encoding prior knowledge through the graph\nstructure. We show empirically that the learned edge value functions can be interpreted.\nThe procedure of using coordination graphs for RL was ﬁrst introduced by Guestrin et al.[17]. Update\nequations for learning the edge value functions are very well described and justiﬁed by Kok and\nVlassis for tabular representations of the value function [19]. Our proposed algorithm combines\ncoordinated RL with neural network approximation and parameter sharing. The core of the algorithm\nis similar to Boehmer et al. [20] while some aspects are simpliﬁed in order to allow for a fully\ndistributed implementation in a real mobile networks. Our method does not make any requirements\non the graph structure and we propose an approach to derive coordination graphs from existing mobile\nnetwork deployments.\n3\nCollaborative Multi-agent Reinforcement Learning\nCollaborative sequential decision making problems can be modeled by a multi-agent Markov decision\nprocess where several decision agents are trying to maximize a common reward function. Formally, it\nis deﬁned by a tuple (N, S, A, T, R, γ) where N is a set of n agents, S = S1 × . . . × Sn a joint state\nspace, A = A1 × . . . × An a joint action space, T a transition function, R a global reward function\nand γ ∈[0, 1) a discount factor. We make an additional assumption that the reward function can be\n2\nadditively decomposed into local reward signals: R(s, a) = Pn\ni=1 ri(s, a). Note that the individual\nreward still depends on the joint state and action as agents might inﬂuence each other’s performance.\nIn this work, we seek to ﬁnd a joint policy mapping a joint state to an action that\nmaximizes the accumulated global reward.\nWe deﬁne the value function associated to\na policy π as Qπ(s, a) = E[P∞\nt=0 γt Pn\ni=1 ri,t|s0 = s, a0 = a].\nGiven a joint value func-\ntion, a joint policy is given by π(s) = arg maxa Q(s, a).\nThe value function can be ap-\nproximated by a parameteric model learned by minimizing the following Bellman loss:\nJ(θ) = Es′[(r + γ maxa′ Q(s′, a′; θ) −Q(s, a; θ))2] where θ represents learned parameters and\n(s, a, r, s′) is an experience tuple obtained by interacting with the environment.\nThe size of the joint state, and action spaces grows exponentially with the number of agents, making\nthe value function difﬁcult to learn and represent even with powerful function approximators such\nas neural networks. Computing the argmax to extract the action from the value function suffers\nfrom combinatorial explosion. Previous works in MARL often attempts to learn value function\ndecomposition such as in VDN (linear) or QMIX [16], [21] to make this problem tractable. If\nknowledge about dependencies between agents is available, such decomposition can directly be used\nthrough a coordination graph\nA coordination graph G is deﬁned by a set of vertices V and a set of undirected edges E. Each\nvertex corresponds to an agent. Each edge (i, j) ∈E is associated to a value function Qij :\nSi × Sj × Ai × Aj →R representing the pairwise value function of agent i and j. The topology of\nthe graph allows for capturing coordination behavior between connected agents. For each edge, (i, j),\nwe initialize a value function Qij as a function parameterized by θ. With knowledge of the graph\ntopology, the joint value function can then be expressed as follows:\nQ(s, a) =\nX\n(i,j)∈E\nQij(si, sj, ai, aj; θ)\n(1)\nTo address high dimensional and continuous observation spaces, we represent the value functions by\nneural networks, Qij(si, sj, ai, aj; θ) for all (i, j) ∈E. The parameters are shared across all edges.\nTo learn these pairwise value functions agents will keep interacting with the environment in a loop\nalternating between action selection and learning.\nAction selection. The action selection procedure consists of solving the following problem:\na∗= arg max\n(a1,...,an)\nX\n(i,j)∈E\nQij(si, sj, ai, aj)\n(2)\nwhere n is the number of agents and E is the set of edges of the coordination graph. Note that\nwithout any assumptions on the interaction between the agents, solving this problem would be\nintractable as it would require searching the whole action space. Instead, this problem can be solved\napproximately using the max-plus (or message passing) algorithm [19]. Each agent i computes a\nmessage, µij ∈R|Aj|, for each of its neighbors. This message depends on the value function of the\nedges connected to agent i and on the messages received from its neighbors.It is computed as follows:\nµij(aj) = max\nai [Qij(si, sj, ai, aj) +\nX\nk∈N(i),k̸=j\nµki(ai)] + cij\n(3)\nfor all aj ∈Aj, where N(j) is the set of neighbors of j, and cij is a normalizing term. Qij is\nparameterized by a neural network and is a function of the states. However, during the message\npassing procedure, the state does not change. Hence, Qij can be treated as a 2D matrix of size\n|Ai| × |Aj|. The agents keep sending, receiving, and recalculating messages until the value of the\nmessages converges. After convergence, individual actions are given by:\na∗\ni = arg max\nai\nX\nj∈N(i)\nµji(ai)\n(4)\nExploration techniques like ϵ-greedy can be applied for each agent.\nLearning. The second step of the algorithm consists of learning the value functions at the edges of\nthe graph. Those value functions represent the pairwise inﬂuence of agents among each other. Two\nsubtleties occur in deriving an update rule for the value function: the credit assignment and the best\naction. In standard RL we could use ri + rj as a reward signal. However, agent i is connected to\n3\nBase Station\nConnected user\nSignal\nInterferences\nNon connected \nuser\nCell\nDown tilt \nangle \nFigure 1: Illustration of a network optimization problem where the variable is antenna tilt. Each base\nstation consists of three cells. We can see two undesirable events caused by poor conﬁguration and\nlack of coordination. To the left, two antennas interfere with each other because of low tilt values in\nthe middle base station (causing the signal to beam upward). In the bottom right, a high tilt value\ncauses users to lose coverage.\nagent j but also to other neighbors. Hence, the performance of agent i must be distributed among all\nof its neighbors as they might all contribute to its value (whether it is a good or bad inﬂuence). The\nreward used to learn Qij is then given by\nri\nN(i) +\nrj\nN(j).\nThe\nsecond\naspect\nto\nconsider\nis\nhow\nto\nevaluate\nthe\ntarget\nvalue.\nIf\nwe\nuse\narg maxai,aj Qij(s′\ni, s′\nj, ., .), the value function would ignore that the action selection procedure is\ninﬂuenced by the whole graph and not only the pair of interest. To correct this effect, the target value\nis given by Qij(s′\ni, s′\nj, a∗\ni , a∗\nj) where a∗\ni , and a∗\nj are the results from the message passing algorithm\ndescribed in the previous section. We can now derive a generic update rule for coordinated RL with\nfunction approximation applied for all i, j:\nθ ←θ + α[ ri\nN(i) +\nrj\nN(j) + γQij(s′\ni, s′\nj, a∗\ni , a∗\nj; θ) −Qij(si, sj, ai, aj; θ)]∇θQij(si, sj, ai, aj; θ)\n(5)\nwhere α is the learning rate. This update rule can be carried with only local information about agents\ni and j. Similarly as in DQN, the parameters can be updated using batches of experience samples,\nand we can use additional improvements like double Q learning and target networks [22]. Relying on\nthe discussion above, we could generalize the update rule to online learning methods which are often\nmore sample efﬁcient [23] and would be more suitable for training in the real world. A discussion\nand theoretical justiﬁcation of the update rule in the tabular case is provided in previous work [18].\n4\nProblem Formulation\nThe problem of optimizing network performance can be modeled as a multi-agent MDP. We consider\na mobile telecommunication network consisting of multiple base stations each of them with multiple\ncells. Each cell is associated to an antenna as illustrated in Fig. 1. User equipments (users) are\nassociated to a cell if they receive a sufﬁciently strong signal from the antenna of that cell. The signal\nstrength is affected by tunable parameters such as the tilt or the power of the antennas along with\nenvironmental aspects and ultimately affects the quality of experience of users. In this work, we\nfocus on downlink performance only and it is assumed that all users are active all the time.\nIn this paper, we choose to optimize antenna tilt conﬁgurations. However, our approach is applicable\nto any network parameter optimization. Each base station consist of three cells with directional\nantennas. Nevertheless, our approach can generalize to other types of deployments with different\nnumber of cells per base station and even heterogeneous base stations. Each cell is associated to a\nreinforcement learning agent.\nFigure 1 illustrates one possible interaction between agents with the appearance of interference in\nthe red area. Two nearby antennas have low down tilt causing interference between their signals.\nThe agents must coordinate to alleviate the situation. On the other hand, the antenna in the middle\nfacing down has very high down tilt, causing a lack of coverage for users at the extremity of the cell.\nLearning control policies for these agents requires coordination and collaboration to maximize the\noverall network quality for all users. The multi-agent MDP is further speciﬁed as follows:\n4\n−2\n0\n2\n−2\n0\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n6\n7\n18\n19\n20\n21\n22\n23\n24\n25\n26\nLatitudinal position (km)\nLongitudinal position (km)\n−2\n0\n2\n−2\n0\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nLatitudinal position (km)\n−2\n0\n2\n−2\n0\n2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n6\n7\n18\n19\n20\n21\n22\n23\n24\n25\n26\nLatitudinal position (km)\nFigure 2: Example of a coordination graph constructed using interference patterns (middle). The\nposition of the nodes is offset for the clarity of the visualization. The nodes close to a base station cor-\nresponds to antennas located at that base station. On the left, we consider a dense graph construction\nscheme. On the right we consider the minimum spanning tree of the graph.\nObserved state. Each agent observes the signal to interference and noise ratio (SINR), in dB, of\nall the users it covers. Since the number of users in a cell can vary over time, we transform the\nSINR list to a ﬁxed vector representation by extracting four percentiles of the distribution: 10 %,\n25 %, 50 %, 75 %. We found that this representation captured well different characteristics of the cell\ncoverage without making the observation space too complex. The observation space is then a four\ndimensional vector of real values. The SINR is directly inﬂuenced by a change in antenna tilt and\ncaptures interference between agents. Further details on how SINR is computed and how antenna\ngain is modeled can be found in appendix.\nAction space. We consider that the antenna in each cell can set its electrical down tilt, α, to one of\nsixteen possible values in the set {0°, 1°, . . . , 15°}. The action space of one agent is then a discrete\nspace with sixteen possible actions.\nReward function. The goal of our algorithm is to maximize the downlink throughput (in megabits\nper second) for every UE. Each cell can measure its contribution to this global objective by measuring\nthe average throughput in this cell. We thus have one local reward signal per agent. Mathematically\nthe overall objective is expressed as follows: R(s, a) = Pn\ni=1 Ti. Where Ti is the logarithm of the\naverage throughput in cell i. The throughput is affected by the signal strength and the load of a given\ncell, if too many users are connected to a cell the throughput drops as they have to share resources.\nMaximizing throughput globally in the network requires some coordination between all the agents.\nFurther details on the modeling is provided in appendix, including information about antenna models\nand formulas used to compute SINR and throughput. Although we only demonstrate the performance\nof the algorithm on the problem of antenna tilt control, it can generalize to other problems and\nsupports heterogeneous deployments where each agent has a different action space.\n5\nMobile Networks as Coordination Graphs\nTo take full advantage of the topology of the cellular networks we propose to represent it using a\ncoordination graph. Given a deployment of base stations, we construct a graph where each cell is a\nnode. An edge is assigned between cells that can inﬂuence the received signal of each other’s users.\nTo determine such relations we can use automated procedure using domain knowledge and heuristics:\nbased on the geographic distance between cells, using the radiation patterns of the antennas [4],\nusing automatic neighbor relations (ANR) as deﬁned in international standards [1], or using network\nplanning tools computing coverage prediction.\nAll these methods could be used to automate the construction of an undirected graph from a given\ndeployment. In addition, domain knowledge can be used to reﬁne the graph topology by pruning\nor adding edges based on key feature of a city or knowledge about the terrain (if there is a natural\nobstacle for example). Reducing the number of edges can speed up the message passing procedure\ndescribed in the previous section. In this paper we construct the graph based on the radiation pattern\nof the antennas, that is antennas that can interfere with each other are connected by an edge. An\nexample of such graph is illustrated on a 27 cells deployment in Fig. 2 (middle).\n5\nIn case where the graph construction leads to a disconnected graph, then our algorithm can run in\neach connected component independently. A disconnected graph would mean that there is no a priori\ninteraction between its connected components. Without loss of generality we assume a connected\ngraph for the remaining of the article. In Section 6 we propose a comparison with other graph\nconstruction techniques leading to a dense graph or a tree as illustrated in Fig. 2. We found that\nadding edges did not yield any gain in performance.\nGiven a network deployment we can extract a graph in a simple way, yet capturing a lot of information\nabout the interaction between agents. Finding the optimal graph representation of a telecommunica-\ntion network is an interesting problem and is left as future work. As explained in the previous section,\nthe graph representation can directly be used in a multi-agent reinforcement learning algorithm to\nperform network optimization at scale.\nMessage Passing. In our experiments, the graphs contain cycles and hence the message passing\nalgorithm is only proven to converge empirically. We implemented two stopping criteria for the\nalgorithm, leading to an approximately optimal solution. The ﬁrst criterion is to stop the algorithm\nafter a limited number of iterations. Our second criterion detects the occurrence of oscillation in the\nvalues of the message, and compare the global value of each solution in the oscillation cycle. The\nalgorithm stops and assigns the action found by the best possible conﬁguration from the detected\ncycle. This early stopping criteria allows speeding up the message passing algorithm while making\nsure we stop at the best possible conﬁguration found throughout the message exchange. Future work\nwould be needed in studying the performance gap between solving Eq. (2) exactly or approximately.\nWe show in our experiment that even with the approximated method, the overall coordinated RL\nprocedure is able to outperform many baselines. An example of what messages look like in the\nantenna tilt use case is given in Appendix D.\nImplementation considerations. In this paper we rely on a simulated environment and on a fully\ncentralized implementation of the message passing algorithm. However, an implementation in a\nreal network could use the property that the message passing algorithm can be implemented in a\nfully distributed and anytime way [19]. The messages could be sent through existing interfaces for\ncommunicating between the base stations. Since they only rely on local information, the RL updates\ncould be carried in hardware located at the station. In such a setting, the data would stay local to the\nnetwork, alleviating privacy concern related to sending user data to a central server.\n6\nExperiments\nWe empirically demonstrate the advantage of coordinated RL over state-of-the art multi-agent RL\nalgorithms and against heuristics in a high ﬁdelity mobile network simulator. By comparing the\nperformance in networks of various sizes we validate the scalability of the approach. Then we analyze\nthe effect of the coordination graph topology on the ﬁnal performance. Finally, we show that the\nlearned edge value functions match intuitive coordination behavior between neighboring antennas.\nSimulation Environment. We evaluate our approach using a high ﬁdelity proprietary network\nsimulator that handles multiple cells and multiple antennas. The simulator performs all the path gain\ncalculations using advanced radio propagation models as well as trafﬁc calculations for each user.\nFrom that simulator we can get different key performance indicators for each user such as SINR and\nthroughput. Using the simulator, we generate different deployments from 6 to 207 cells. The base\nstations are positioned according to a Poisson process with a minimum intersite distance of 1.5 km.\nThe simulated area increases as well such that the average intersite distance stays the same. More\ndetails on the environment are provided in appendix.\nBaselines. Our approach is referred to as PS-CRL for coordinated RL with parameter sharing. We\ncompare it to six different baselines:\n• Coordinated RL (CRL): our approach without parameter sharing. A different neural network\nis used for each edge of the graph.\n• Independent DQN (I-DQN): this algorithm assumes that all cells are independent and\nassociate a Q-network for each of them [24]. Each Q-network is learned independently.\n• DQN with parameter sharing (S-DQN) this algorithm associates the same Q-network for\neach cell. It is similar to I-DQN but the weights of the network are the same for each cell. It\nhas been shown to greatly help in problem with homogenous cooperating agents [25].\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\n·104\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nTraining steps\nReward per cell\nCRL\nPS-CRL\nI-DQN\nS-DQN\nQMIX\n0\n50\n100\n150\n200\n0\n1\n2\nNumber of cells\nBest reward per cell\nCRL\nPS-CRL\nI-DQN\nS-DQN\nQMIX\nsweep\nC-sweep\nFigure 3: (left) Training performance of the RL methods on a network with 27 cells. (right) Reward\nper cells as a function of the total number of cells for the best trained models.\n• QMIX: this algorithm attempts to capture agent coordination by learning a non-linear\ncombination of individual value functions, modeled by a neural network [16].\n• sweep: This algorithm is not a learning based method. For each cell, we sweep through\nall the tilt values while maintaining other cells ﬁxed, and choose the best tilt value. It is\nagnostic of the state of the environment.\n• C-sweep: We perform a tilt sweep for each pair of connected agent according to the same\ngraph topology as in CRL. When doing the sweeping we populate a table of size |Ai| × |Aj|\nfor each edge. Message passing is then used to choose the best action.\n6.1\nTraining Performance and Scalability\nWe measure the performance of the trained model in terms of average reward (evaluated across\n10 different user distributions for every point) which is directly related to the average throughput.\nFigure 3 left illustrates the training performance of our method and the baseline. Each experiment is\nrepeated with ﬁve different random seeds. During training, we use ϵ-greedy exploration but we report\nthe evaluated reward (without ϵ-greedy) at different checkpoints. We show that on this network with\n27 agents, both PS-CRL and CRL outperforms all the baselines in terms of ﬁnal reward, convergence\nspeed and show less variance.\nFigure 3 shows the performance of the best performing models for different network sizes. We choose\nthe models with the best average reward throughout the training process (not necessarily the latest).\nWe are reporting the average reward per cell as a function of the number of cells in the environment.\nOne cell corresponds to one RL agent. We notice that there is a monotonic trend, as the number of\ncell increases, the reward per cell seems to increase. This matches our intuition since more cells\nshould imply better coverage. However, adding cells can also create interference, hence the small\ndegradation for I-DQN, sweep, and C-sweep around 78 cells. We can see that all the way to 78 cells,\nCRL and PS-CRL are outperforming all the baselines. As the number of cell increases we observe a\nsimilar fact than in the previous section. PS-CRL reaches the best performance, and CRL is getting\ncaught up by S-DQN and QMIX. In addition, it seems like the beneﬁt of using coordination increases\nwith the number of cells as the gap between the performance of PS-CRL and the other method is\nincreasing as well.\nFinally, we introduced two non-learning based method in this plot, sweep and C-sweep. They are\ngenerally outperformed by the RL baseline, until the environment becomes large enough that the\nconvergence of the RL policy is affected. These empirical results show the beneﬁt of CRL and\nPS-CRL in order to model coordination between cells and train multi-agent RL algorithms at scale\non telecommunication networks. Our largest deployments have an order of magnitude more agents\nthan in standard multi-agent RL algorithms benchmarks.\n6.2\nInterpreting Edge Value Functions\nFigure 4 illustrates the value functions for certain edges in a given state. Each axis on the heatmap\ncorresponds to the action of one of the two connected agents. The bottom left value function (edge\n7\n0.2\n0.4\n0.6\n0.8\n1\n·104\n0\n0.5\n1\n1.5\nTraining steps\nReward per sector\ndense\ncomplete\nsparse\ntree\n3\n6\n7\n8\n10\n11\n0\n16\na10\n16\n0\na6\n0\n16\na7\n16\n0\na6\n0\n16\na8\n16\n0\na3\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFigure 4: (left) Performance of PS-CRL for different types of coordination graphs on a scenario with\n78 agents. (right) Illustration of the learned value functions for the three solid edges in a subset of the\nsparse graph from Fig. 2.\n(3,8)), shows that there is low value when both agents have low tilt (bottom left corner). In that case,\nthe antennas are likely to interfere with each other. The right most value function (edge (6,10)) also\nshows that when both antennas have high tilt (top right corner), the value is low. In that case, antennas\nare tilted down, there is no interference, but the coverage is poor. The learned value functions align\nwith the intuition that for two neighboring antennas both should not be tilted up or down at the same\ntime. The ability to interpret the value function by visualizing them as a heatmap gives great value to\nthe coordination graph representation. Experts can look at the learned value function across the graph\nand diagnose which antennas have strong correlations and use it as a network planning tool.\n6.3\nGraph Structure\nDifferent methods for creating a coordination graphs are considered: sparse, dense and tree as\nillustrated in Fig. 2, and a fully connected graph (complete). We analyze the performance of PS-CRL\nfor these different graph structures. We can see that for the dense, sparse, and tree methods, PS-CRL\nis converging to similar values. When there are too many connections, it fails to learn a good solution.\nIn problems where graph structure can be identiﬁed PS-CRL is a good solution even if the graph\nmodel is not accurate. In the tree representation, information can still ﬂow between distant agents\nthrough the message passing procedure even if they are not directly connected.\n7\nConclusions\nCoordinated RL with parameter sharing allows learning control strategies for a large number of\ncooperating agents. Its ability to exploit graph structures makes it a good candidate for mobile network\noptimization problem. We presented a principled way to model mobile networks as coordination\ngraphs and learn a value function for each edge of this graph. By leveraging the topology of the graph,\nan efﬁcient message passing algorithm can be used to coordinate all the learning agents. We showed\nthat the proposed algorithm outperforms a variety of RL and non RL baselines. The learned value\nfunctions at the edge of the graph can be interpreted and were shown to capture intuitive phenomena\nabout the network. When augmented with parameter sharing, CRL has better sample efﬁciency and\nscales to hundreds of learning agents.\nOne of the main challenges of applying RL to telecommunication networks is that it requires a lot\nof data. Even though simulators are available, they are often slow. Future work involves looking\nat speeding up simulation and mixing simulated and real world data. Another interesting direction\ninvolves transferring learned policies from one graph to another.\nReferences\n[1]\nO. G. Aliu, A. Imran, M. A. Imran, and B. G. Evans, “A survey of self organisation in future cellular\nnetworks,” IEEE Communications Surveys & Tutorials, vol. 15, no. 1, pp. 336–361, 2013.\n8\n[2]\nH. Eckhardt, S. Klein, and M. Gruber, “Vertical antenna tilt optimization for LTE base stations,” in IEEE\nVehicular Technology Conference, 2011.\n[3]\nA. Eisenblätter and H. Geerdes, “Capacity optimization for UMTS: bounds and benchmarks for interfer-\nence reduction,” in IEEE International Symposium on Personal, Indoor and Mobile Radio Communica-\ntions (PIMRC), 2008.\n[4]\nA. Saeed, O. G. Aliu, and M. A. Imran, “Controlling self healing cellular networks using fuzzy logic,” in\nIEEE Wireless Communications and Networking Conference, 2012.\n[5]\nB. Partov, D. J. Leith, and R. Razavi, “Utility fair optimization of antenna tilt angles in LTE networks,”\nIEEE/ACM Transactions On Networking, vol. 23, no. 1, pp. 175–185, 2015.\n[6]\nH. Farooq, A. Imran, and M. Jaber, “AI empowered smart user association in LTE relays hetnets,” in\nIEEE International Conference on Communications Workshops, 2019.\n[7]\nN. Dandanov, H. Al-Shatri, A. Klein, and V. Poulkov, “Dynamic self-optimization of the antenna tilt for\nbest trade-off between coverage and capacity in mobile networks,” Wireless Personal Communications,\nvol. 92, no. 1, pp. 251–278, 2017.\n[8]\nE. Balevi and J. G. Andrews, “Online antenna tuning in heterogeneous cellular networks with deep\nreinforcement learning,” vol. 5, no. 4, pp. 1113–1124, 2019.\n[9]\nR. Shaﬁn, H. Chen, Y. Nam, S. Hur, J. Park, J. Zhang, J. H. Reed, and L. Liu, “Self-tuning sectorization:\nDeep reinforcement learning meets broadcast beam optimization,” IEEE Transactions on Wireless\nCommunications, vol. 19, no. 6, pp. 4038–4053, 2020.\n[10]\nA. Galindo-Serrano and L. Giupponi, “Distributed q-learning for aggregated interference control in\ncognitive radio networks,” IEEE Transactions on Vehicular Technology, vol. 59, no. 4, pp. 1823–1834,\n2010.\n[11]\nF. Vannella, J. Jeong, and A. Proutière, “Off-policy learning for remote electrical tilt optimization,” arXiv\npreprint, vol. abs/2005.10577, 2020. arXiv: 2005.10577.\n[12]\nY. S. Nasir and D. Guo, “Multi-agent deep reinforcement learning for dynamic power allocation in\nwireless networks,” IEEE Journal on Selected Areas in Communications, vol. 37, no. 10, pp. 2239–2250,\n2019.\n[13]\nF. D. Calabrese, L. Wang, E. Ghadimi, G. Peters, L. Hanzo, and P. Soldati, “Learning radio resource\nmanagement in rans: Framework, opportunities, and challenges,” IEEE Communications Magazine,\nvol. 56, no. 9, pp. 138–145, 2018.\n[14]\nY. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean ﬁeld multi-agent reinforcement learning,”\nin International Conference on Machine Learning, 2018.\n[15]\nA. Oroojlooyjadid and D. Hajinezhad, “A review of cooperative multi-agent deep reinforcement learning,”\nCoRR, vol. abs/1908.03963, 2019. arXiv: 1908.03963.\n[16]\nT. Rashid, M. Samvelyan, C. S. de Witt, G. Farquhar, J. N. Foerster, and S. Whiteson, “QMIX: monotonic\nvalue function factorisation for deep multi-agent reinforcement learning,” in International Conference on\nMachine Learning, 2018.\n[17]\nC. Guestrin, M. G. Lagoudakis, and R. Parr, “Coordinated reinforcement learning,” in International\nConference on Machine Learning, 2002.\n[18]\nJ. R. Kok and N. A. Vlassis, “Collaborative multiagent reinforcement learning by payoff propagation,”\nJournal of Machine Learning Research, vol. 7, pp. 1789–1828, 2006.\n[19]\n——, “Using the max-plus algorithm for multiagent decision making in coordination graphs,” in RoboCup\n2005: Robot Soccer World Cup IX, 2005.\n[20]\nW. Boehmer, V. Kurin, and S. Whiteson, “Deep coordination graphs,” in International Conference on\nMachine Learning, 2020.\n[21]\nP. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat,\nJ. Z. Leibo, K. Tuyls, and T. Graepel, “Value-decomposition networks for cooperative multi-agent learning\nbased on team reward,” in Autonomous Agents and Multiagent Systems, 2018.\n[22]\nH. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double q-learning,” in AAAI\nConference on Artiﬁcial Intelligence, 2016.\n[23]\nE. Bargiacchi, T. Verstraeten, D. M. Roijers, A. Nowé, and H. van Hasselt, “Learning to coordinate with\ncoordination graphs in repeated single-stage multi-agent decision problems,” in International Conference\non Machine Learning, 2018.\n[24]\nM. Tan, “Multi-agent reinforcement learning: Independent versus cooperative agents,” in International\nConference on Machine Learning, 1993.\n[25]\nJ. K. Gupta, M. Egorov, and M. J. Kochenderfer, “Cooperative multi-agent control using deep reinforce-\nment learning,” in Autonomous Agents and Multiagent Systems, 2017.\n[26]\nA. Asghar, H. Farooq, and A. Imran, “Concurrent optimization of coverage, capacity, and load balance in\nhetnets through soft and hard cell association parameters,” IEEE Transactions on Vehicular Technology,\nvol. 67, no. 9, pp. 8781–8795, 2018.\n9\n[27]\nE. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. Gonzalez, M. I. Jordan, and I. Stoica,\n“Rllib: Abstractions for distributed reinforcement learning,” in International Conference on Machine\nLearning, 2018.\n[28]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A. Riedmiller,\nA. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D.\nWierstra, S. Legg, and D. Hassabis, “Human-level control through deep reinforcement learning,” Nature,\nvol. 518, no. 7540, pp. 529–533, 2015.\n[29]\nT. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience replay,” in International\nConference on Learning Representations, 2016.\nA\nAdditional Details on the Simulation Environment\nThe base stations we simulate are LTE base stations, operating at 2 GHz. Antennas are at a 32 m\nheight, have a maximum transmitting power of 40 W, a horizontal beamwidth of 65° and a vertical\nbeamwidth of 6.5°. The radiation pattern of the antenna is ﬁtted from real base station antenna data.\nUsing the simulator, we generate different deployments from 6 to 207 cells. The base stations are\npositioned according to a Poisson process with a minimum intersite distance of 1.5 km. The simulated\narea increases as well such that the average intersite distance stays the same. The environment\nis generated randomly such that 50 % of the area corresponds to indoor environments and the\nrest outdoor. There are 1000 users uniformly distributed in the environment and their position is\nre-sampled after every change in tilt conﬁguration.\nObserved state\nEach agent observes the signal to interference and noise ratio (SINR), in dB, of all the users it covers.\nSince the number of users in a cell can vary over time, we transform the SINR list to a ﬁxed vector\nrepresentation by extracting four percentiles of the distribution: 10 %, 25 %, 50 %, 75 %. We found\nthat this representation captured well different characteristics of the cell coverage without making the\nobservation space too complex. The observation space is then a four dimensional vector of real values.\nThe choice of SINR is motivated by the fact that it is directly inﬂuenced by a change in antenna tilt\nand captures interference from other cells. The downlink SINR, ρc,u, of a user u connected to cell c\nis expressed as the ratio of the received power measured by u from cell c and the sum of the received\npower from all other cells, and the noise power σ [26], assuming full load.\nρc,u =\nPcGc,uLc,u\nPNcells\ni=1,i̸=c PiGi,uLi,u + σ\n(6)\nPc, Gc,u, Lc,u are respectively the transmitted power, gains of the transmitter antenna, and path\nloss for UE u connected to cell c. The denominator accounts for the effect of other cells, the gain\nG is inﬂuenced by antenna parameters such as tilt and azimuth, and the path loss accounts for the\ntransmission medium and obstacles (building, air, trees, ...). We use a prioprietary antenna model\nwhich is ﬁtted using real antenna data.\nAction space\nEach cell contains an antenna. The signal emission pattern of this antenna can be adjusted by a variety\nof parameters. Changing those parameters will increase or degrade the capacity and coverage of the\ncell. In this paper we focus on electrical down tilt as it has one of the biggest effect on network key\nperformance indicators and can easily be controlled by an automated system (contrary to mechanical\ntilt). We consider that the antenna in each cell can set its electrical down tilt, α, to one of sixteen\npossible values in the set {0°, 1°, . . . , 15°}. The action space of one agent is then a discrete space\nwith sixteen possible actions.\nReward\nThe goal of our algorithm is to maximize coverage and capacity. Our algorithm aims at maximizing\nthroughput. We consider the throughput of a UE to be deﬁned by the bitrate of that user divided by\nthe load of the cell. The bitrate is the maximum amount of data per second that a user can receive\ngiven the SINR produced by its associated antenna. The load of the cell is equal to the number of\n10\nusers covered by a given cell. Since the bitrate can take very large values spread across a wide range,\nwe use the logarithm of the average bitrate in a cell in our reward deﬁnition. The overall objective of\nour cooperative multi-agent RL problem is expressed mathematically as follows:\nR(s, a) =\nNcells\nX\ni=1\n[log( 1\n|Ui|\nX\nu∈Ui\nTi,u)]\n(7)\nwhere Ncells is the number of cells in the network, Ti,u is the throughput of user u connected to cell\ni, and Ui is the set of all users in cell i. The throughput is expressed as a function of the SINR and\nbandwidth available to user u:\nTi,u = ni,uωBf(ρi,u)\n(8)\nwhere f(ρi,u) is the spectral efﬁciency of the user link for a given SINR, ni,u is the number of\nphysical resource blocks (PRBs) allocated to user u in cell i and ωB is the bandwidth per PRB\n(180 kHz). Considering round robin scheduling (equal number of PRB allocated to each user) and\nthe Shanon rate capacity, Eq. (8) becomes:\nTi,u = nBωB\n|Ui| log2(1 + ρi,u)\n(9)\nwhere nB is the total number of PRBs in cell i. The number of users connected to cell i is also\naffected by tilt variation since we assume users connect to the cell from which they get maximum\nreference signal received power (RSRP). Given a tilt value α, the users in cell i are deﬁned by:\nUi = {∀u ∈U | i = arg max\n∀c∈C\nPcGc,u(α)Lc,u}\n(10)\nB\nAdditional Details on Training Methodology\nThe observation and reward are the same for all the methods considered. In order to have good\nnumerical conditions we normalize the observation vector by dividing by the maximum SINR value.\nThe reward function is normalized to have zero mean and unit variance. The mean and variance of\nthe reward, along with the maximum SINR, are estimated through 1000 random tilt conﬁguration.\nAll the baselines are part of the RLlib python package and our method is implemented using the\nsame abstractions [27]. This library allows us to distribute the experience collection across multiple\nCPUs. We performed hyperparameter search on the learning rate, batch size, and target network\nupdate frequency and the exploration schedule for each method. The network architecture was tuned\nusing the I-DQN algorithm and we then use networks of equivalent sizes for all the algorithms. All\nthe hyperparameters are reported in Table 1, parameters that are not speciﬁed are set to the default\nprovided by RLlib. Each method is allowed a budget of 10 000 interactions with the simulation\nenvironment and each training is repeated with 5 different random seeds.\nFor all baselines, the experience collection strategy is similar to the deep Q-learning algorithm with\nprioritized experience replay [28], [29]. For exploration, we rely on an ϵ-greedy policy. In the case of\nshared DQN, we use the same replay buffer for all the agents.\nC\nComparison of Inference Time for Different Graph Models\nThe inference time of CRL depends on the size of the graph in two ways. First once must compute\nthe value function for each edge at a given state which is linear in the number of edges. Then the\nmessage passing procedure also depends on the graph size. One round of message passing is given\nby Eq. (3), which is linear in the degree of a node. The more complex the graph, the more message\npassing iteration might be needed at each action selection step as well. A theoretical analysis is out\nof the scope of this work. In Appendix C we illustrate the inﬂuence of the coordination graph on the\ninference time. As expected, the less edges the faster (for the same number of nodes). Since dense,\nsparse, and tree provides similar performance, a tree like graph would be the most efﬁcient since it\nhas a shorter inference time.\n11\nTable 1: Hyperparameters of the deep RL algorithms\nCommon hyperparameters\nγ\n0.0\nlearning rate\n1 × 10−4\ninitial ϵ\n1.0\nﬁnal ϵ\n0.01\nϵ decrease steps\n5000\nhidden layers\n32 × 32\nactivation function\nReLU\nbatch size\n32\nDQN speciﬁc\ntarget network update frequency\n2000\nCRL speciﬁc\nmessage passing max iterations\n40\ntarget network update frequency\n500\nQMIX speciﬁc\nmixing embedding dimension\n32\ndouble Q learning\nyes\n0.2\n0.4\n0.6\n0.8\n1\n·104\n500\n1,000\n1,500\n2,000\n2,500\nTraining steps\nInference time (ms)\ndense,sparse,complete,tree tick align\nFigure 5: Inference time for PS-CRL throughout training for different types of coordination graphs.\nD\nBackground on Message Passing\nIn the case of trees, the max-plus procedure is shown to converge to the optimal solutions. When\ngraphs contain cycles, they only converge to approximately optimal solutions. Another approach is to\nuse variable elimination to solve Eq. (2) but it is not as scalable. For a detail treatment of the message\npassing algorithm we refer the reader to the work of Kok and Vlassis [19]. The message passing\nprocedure is illustrated in Appendix D at a speciﬁc node from the twenty-seven cells deployment\nillustrated in Fig. 2. We provide an illustration of the message passing procedure in Appendix D that\nshows the messages after convergence for our network optimization use case.\nLearning the value function of an edge (i, j) can be considered as a standalone, value-based, off-\npolicy RL problem involving two agents. The equivalent of the state and action spaces for a pair of\nagent is (si, sj), and (ai, aj).\nE\nMore on the Training Performance\nAnother metric to look at is the throughput CDF as illustrated in Fig. 7. It shows the distribution\nof throughput across the users. For the tail of the distribution (below 10 %), the algorithms have\n12\n6\n7\n8\n10\nμ6, 7\nμ6, 8\nμ6, 10\nμ10, 6\nμ8, 6\nμ7, 6\n1.150\n1.175\n1.200\n1.225\n1.250\n1.275\n1.300\n1.325\nFigure 6: Illustration of the message passing procedure from the point of view of node 6. The\nmessages sent and received have the size of the action space of the receiving neighbor. In this work,\nit always corresponds to sixteen tilt values. Node 6 aggregates the received messages using Eq. (3) to\nrecompute its outgoing messages for the next round, until convergence.\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n0\n50\n100\nThroughput (Mbps)\nPercentage of UEs\nCRL\nPS-CRL\nI-DQN\nS-DQN\nQMIX\nsweep\nC-sweep\nFigure 7: Cumulative density function plot of the throughput for the deployment with 27 cells. The\nmore to the right the better\nsimilar performance. For 20 % of the users and above, CRL and PS-CRL signiﬁcantly improve\nthe throughput compared to other methods. We can also see that the sweeping techniques are less\nperformant than RL based methods. An explanation is that they cannot adapt well to different user\ndistributions.\nThe training performance on two different deployment is shown in Fig. 8. On the larger environment\nwith 207 cells, PS-CRL outperforms all the methods by a large margin, it shows better sample\nefﬁciency and less variance as well. A notable fact is that the CRL method seems to still be improving\nat the end of the training. I-DQN and QMIX also seems to be still improving. They show more\ninstability throughout the training. We suspect that a larger training budget would have yielded better\nperformance for CRL. However, for those large scenarios, the computational burden of the simulation\nis very high. The beneﬁt of parameter sharing is also seen when comparing I-DQN and S-DQN.\n13\n0\n0.2\n0.4\n0.6\n0.8\n1\n·104\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nTraining steps\nReward per cell\nCRL\nPS-CRL\nI-DQN\nS-DQN\nQMIX\n0\n0.2\n0.4\n0.6\n0.8\n1\n·104\n1\n1.5\n2\n2.5\nTraining step\nReward per cell\nCRL\nPS-CRL\nI-DQN\nS-DQN\nQMIX\nFigure 8: Training performance on two deployments one with 27 cells (left) and 207 cells (right).\nOur approach is CRL and PS-CRL, the other algorithms are common multi-agent RL techniques.\n14\n",
  "categories": [
    "cs.LG",
    "cs.NI"
  ],
  "published": "2021-09-30",
  "updated": "2021-09-30"
}