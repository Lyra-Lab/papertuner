{
  "id": "http://arxiv.org/abs/2109.15317v2",
  "title": "Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation",
  "authors": [
    "Jay Patravali",
    "Gaurav Mittal",
    "Ye Yu",
    "Fuxin Li",
    "Mei Chen"
  ],
  "abstract": "We present MetaUVFS as the first Unsupervised Meta-learning algorithm for\nVideo Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled\nvideos to train a two-stream 2D and 3D CNN architecture via contrastive\nlearning to capture the appearance-specific spatial and action-specific\nspatio-temporal video features respectively. MetaUVFS comprises a novel\nAction-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on\nthe action-oriented video features in relation to the appearance features via\nexplicit few-shot episodic meta-learning over unsupervised hard-mined episodes.\nOur action-appearance alignment and explicit few-shot learner conditions the\nunsupervised training to mimic the downstream few-shot task, enabling MetaUVFS\nto significantly outperform all unsupervised methods on few-shot benchmarks.\nMoreover, unlike previous few-shot action recognition methods that are\nsupervised, MetaUVFS needs neither base-class labels nor a supervised\npretrained backbone. Thus, we need to train MetaUVFS just once to perform\ncompetitively or sometimes even outperform state-of-the-art supervised methods\non popular HMDB51, UCF101, and Kinetics100 few-shot datasets.",
  "text": "Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned\nMeta-Adaptation\nJay Patravali‚ãÜ‚Ä°\nGaurav Mittal‚ãÜ‚Ä†\nYe Yu‚Ä†\nFuxin Li‚Ä°\nMei Chen‚Ä†\n‚Ä†Microsoft\n‚Ä°Oregon State University\n{gaurav.mittal, yu.ye, mei.chen}@microsoft.com\n{patravaj,lif}@oregonstate.edu\nAbstract\nWe present MetaUVFS as the first Unsupervised Meta-\nlearning algorithm for Video Few-Shot action recogni-\ntion. MetaUVFS leverages over 550K unlabeled videos to\ntrain a two-stream 2D and 3D CNN architecture via con-\ntrastive learning to capture the appearance-specific spatial\nand action-specific spatio-temporal video features respec-\ntively. MetaUVFS comprises a novel Action-Appearance\nAligned Meta-adaptation (A3M) module that learns to fo-\ncus on the action-oriented video features in relation to the\nappearance features via explicit few-shot episodic meta-\nlearning over unsupervised hard-mined episodes.\nOur\naction-appearance alignment and explicit few-shot learner\nconditions the unsupervised training to mimic the down-\nstream few-shot task, enabling MetaUVFS to significantly\noutperform all state-of-the-art unsupervised methods on\nfew-shot benchmarks. Moreover, unlike previous few-shot\naction recognition methods that are supervised, MetaUVFS\nneeds neither base-class labels nor a supervised pretrained\nbackbone.\nThus, we need to train MetaUVFS just once\nto perform competitively or sometimes even outperform\nstate-of-the-art supervised methods on popular HMDB51,\nUCF101, and Kinetics100 few-shot datasets.\n1. Introduction\nFew-shot learning [36, 53, 61, 17, 51, 48, 36, 14, 10,\n27, 66] has emerged as a school of approaches that train\na model to transfer-learn or adapt quickly on novel, of-\nten out-of-domain, classes using as few labeled samples\nas possible to mitigate the lack of large-scale supervision\nfor these novel classes. Few-shot learning is highly rele-\nvant for videos because collecting large-scale labeled video\ndata is extra challenging with the additional temporal di-\nmension. There has been work utilizing both 2D and 3D\nCNNs [74, 5, 15, 68, 4, 71] to achieve strong results on few-\n‚ãÜAuthors with equal contribution.\nWork done when Jay Patravali was a full-time intern at Microsoft.\nOnly 3D \nAction\nOnly 2D \nAppearance\n3D Action\n2D \nAppearance\nClassifier\nClassifier\nClassifier\nExisting Unsupervised Methods\nJump Rope\nGolf Swing\nDirect finetuning Classifier\nTennis\nSwing\nMetaUVFS (Ours)\nFew-shot Action-Appearance Aligned \nMeta-adaptation (A3M) + Classifier\n‚ùå\n‚ùå\n‚úì\nStill Rings\nJump Rope\nSkydiving\nGolf Swing\nTennis Swing\nTennis Swing\nSupport\nQuery\nQuery\nValue\n‚äô\nKey\n‚®Ç\nA3M Module\nFigure 1. The above example shows a 1-support 5-way few-shot\nvideo action recognition task to classify a query sample of novel\nTennis Swing class. Using only appearance with a 2D CNN in-\ncorrectly predicts Jump Rope as it relies on only frame-level spa-\ntial cues. Using only action with a 3D CNN incorrectly predicts\nGolf Swing as it matches based on just the swinging action with-\nout paying attention to the spatial cues. Whereas MetaUVFS pre-\ndicts the correct class via the Action-Appearance Aligned Meta-\nadaptation (A3M) module that learns to align and relate the ac-\ntion with the appearance attributes via few-shot meta-learning. All\nthree methods are trained using unlabeled videos.\nshot action recognition in videos. However, these are super-\nvised approaches and require large amounts of labeled base-\nclass data and/or large-scale supervised pretrained back-\nbones [5, 4, 15, 68] that are not only prohibitively expen-\nsive to scale but also oftentimes unattainable. Meanwhile,\nthere is virtually infinite unlabeled video data at our disposal\nthrough the rise of multi-media social networking. This mo-\ntivates us to address the question, ‚ÄúCan we develop models\nfor video action recognition that perform competitively on\nfew-shot benchmarks without the use of either base-class\nlabels or any external supervision?‚Äù\nExisting unsupervised video representation learning\nmethods [47, 55, 24] provide task-agnostic representations\nthat apply to various downstream tasks. However, as we\nshow in later sections, these methods are not specialized for\nthe few-shot learning task with novel classes and therefore\nperform sub-optimally on them.\nTo this end, we propose MetaUVFS as the first method\nfor unsupervised meta-learning for few-shot video action\nrecognition. MetaUVFS leverages large-scale (over half a\nmillion) unlabeled video data to learn video representations\nvia contrastive learning and then trains an explicit few-shot\nmeta-learner using episodes that are hard-mined over the\nlearned representations. The episodic meta-learning helps\nmimic the episodic few-shot meta-testing during the train-\ning phase. This imposes a downstream task-specific prior\non the learned video representations and reduces the knowl-\nedge gap between training and testing.\nWe introduce an unsupervised two-stream action-\nappearance network in MetaUVFS to learn fine-grained\nspatio-temporal 3D features over video segments via an ac-\ntion stream and spatial 2D features over video frames via\nan appearance stream. Direct finetuning of either feature\nalone can be sub-optimal in a challenging few-shot sce-\nnario as illustrated in Fig. 1. Instead, we design an Action-\nAppearance Aligned Meta-adaptation module (A3M) in the\nfew-shot meta-learner of MetaUVFS that combines the two\nstreams by learning a spatio-temporal alignment of appear-\nance over action features. A3M learns an attention map\nconditioned on the action and appearance features to bet-\nter focus on the action-specific features in the frame-level\nappearance embeddings. This helps to improve intra-class\nsimilarity and reduce inter-class confusion for few-shot.\nConsequently, MetaUVFS outperforms all state-of-the-\nart (SoTA) unsupervised video learning methods on multi-\nple benchmark datasets and also outperforms or performs\ncompetitively against the SoTA few-shot action recognition\nmethods. To summarize, our main contributions are,\n1. We propose MetaUVFS as the first unsupervised meta-\nlearning algorithm for few-shot video action recogni-\ntion.\n2. MetaUVFS uses a two-stream network to learn action\nand appearance-specific features via contrastive learn-\ning over 550K unlabeled videos. It employs a novel\nAction-Appearance Aligned Meta-adaptation (A3M)\nmodule that is episodically trained via hard-mined\nepisodes to specialize for few-shot downstream tasks.\n3. MetaUVFS outperforms all SoTA unsupervised meth-\nods across multiple few-shot benchmarks and per-\nforms competitively to or even outperforms some of\nthe SoTA few-shot action recognition methods.\n2. Related Work\nSupervised Few-shot Learning A typical supervised\nfew-shot learning setting has a set of base-classes with a\nlarge number of labeled samples and a set of novel classes\nwith few labeled samples (not enough for plain finetuning).\nIt is evaluated in a meta-testing phase where it classifies\nsamples (query) from the novel classes based on a few, e.g.\n1 or 5 labeled examples (support).\nFor images,\nfew-shot learning approaches include\nmetric-learning based [51, 61, 53] that learn to minimize\nthe distance between support and query embeddings or op-\ntimization based [14, 48] that develop rapidly learnable\nmodels for efficient adaptation on novel classes.\nUsing\njust the base-class data inhibits generalization to novel\nclasses. There are, therefore, approaches using data aug-\nmentation/hallucination [66, 27] or simply training larger\nsupervised models with larger dataset with non-episodic\nfew-shot learning [65, 10, 68]. There are also few-shot ap-\nproaches using some form of attention/alignment module\nfor improved performance [17, 31, 12] but these are image-\nspecific and are not compatible with the action-appearance\nfeatures aligned by our A3M module.\nTo the best of our knowledge, existing few-shot learn-\ning work for videos are all supervised approaches. Proto-\nGAN [38] uses GANs [20] to synthesize addition exam-\nples for novel classes, CMN [74] uses memory augmented\nnetworks [50] to store video features for query match-\ning, and R-3DFSV [68] uses a large pretrained 3D CNN\nalong with weak labels to augment novel class support sam-\nples. There is also work using different forms of cross-\nattention/alignment such as TARN and ARN [3, 71] captur-\ning spatio-temporal dependencies via attention, OTAM [5]\nmatching query-support pairs via metric-learning based\ntemporal alignment, RVN [4] aligning support-query fea-\ntures via LSTMs, and AMeFu-Net [15] aligning appearance\nand motion by fusing depth with RGB. Some methods also\nleverage auxiliary self-supervision to boost few-shot perfor-\nmance [16, 49, 12, 71]. However, unlike previous formula-\ntions that either align support and query or use additional\nmodality along with being supervised, our A3M module in\nMetaUVFS learns to align 2D and 3D features using hard-\nmined episodes in a purely unsupervised manner.\nSupervised Action Recognition Previous methods use\neither 2D CNNs with frame-level features [19, 13] or 3D\nCNNs [25, 57] with spatio-temporal features for super-\nvised action recognition. 2D models suffer from the lack\nof long-term temporal reasoning while 3D models tend to\noverfit due to larger parameter count.\nTo mitigate this,\nrecent methods introduce self-attention [64], temporal re-\nlation [73], factorized 3D convolutions [58], 2D replace-\nments [69], multi-grid scheduler [67] and slow-fast net-\nworks [13]. There are also two-stream networks using both\n2D and 3D CNNs [63, 7, 54] exploiting optic flow or frame\nresiduals with RGB that we take inspiration from to design\nour novel action-appearance two-stream network to learn\nfrom unlabeled videos.\nUnsupervised Few-Shot Learning Recently, unsuper-\nvised meta-learning approaches for few-shot image classi-\nfication [34, 42, 32] have shown competitive performance\nwithout using base-class labels or external supervision. Our\nMetaUVFS drew inspiration from these works to leverage\nunlabeled videos for few-shot action recognition.\nUnsupervised Video Representation Learning Solv-\ning pretext tasks in images [11, 72, 18, 44] has inspired\nmethods to learn from unlabeled videos [33, 35] via pre-\ntext tasks such as sorting frames and predicting video\nspeed [43, 39, 70, 62, 2, 41, 22, 46]. Recently, methods us-\ning contrastive learning (InfoNCE) [45] have been the most\neffective in harnessing large-scale unlabeled data [28, 8, 47,\n55, 24] and perform comparably to supervised methods on\nvision tasks. Although these methods have shown low-shot\nlearning capabilities, it is primarily limited to being able to\nfinetune on in-distribution training classes with a tiny frac-\ntion of full-labeled dataset. Unlike the proposed MetaU-\nVFS, without any dedicated few-shot meta-learning mech-\nanism during training, the existing unsupervised methods\nstill require a full-size labeled dataset to optimally transfer\nto a downstream task with out-of-distribution novel classes.\n3. Method\nWe first describe the unsupervised training of the two-\nstream network in MetaUVFS. We then explain the unsu-\npervised few-shot meta-training and testing of MetaUVFS.\n3.1. Two-stream Video Networks\nAs shown in Fig. 2a, MetaUVFS has a 2D CNN-based\nappearance stream f ap(¬∑) that captures the high-level spa-\ntial semantics of the video. f ap(¬∑) encodes a sequence of F\nframes, Xap = [xap\nt ]F\nt=1, into embeddings hap = [hap\nt ]F\nt=1\nwhere hap\nt\n= f ap(xap\nt ). hap are averaged to obtain ¬Øhap .\nMetaUVFS also has a 3D CNN-based action stream f act(¬∑)\nthat captures the spatio-temporal semantics of the video.\nf act(¬∑) encodes another F ‚Ä≤ frames, Xact = [xact\nt\n]F ‚Ä≤\nt=1, into\na single embedding hact, where hact = f act(Xact). In-\nductive biases of using 2D and 3D convolutional kernels\nin the appearance and action streams respectively enable\nthe streams to specialize in capturing the appearance and\naction-related video information.\n3.2. Two-stream Unsupervised Training Objective\nThe training objective of our two-streams network is\nbased on the multi-view InfoNCE contrastive loss formu-\nlation [8, 28, 45, 56] of the InfoMax principle [40] which\nmaximizes the mutual information between embeddings of\nmultiple views of x, xi and xj. In contrastive learning, the\nnetwork is trained to correctly match each input sample with\nan augmented version of itself among a large training batch\nof other samples and their respective augmentations. We\nuse the NT-Xent loss [8] defined as,\n  \\footnotes ize  \\ mat\nhcal {L}^{\\te\nxt \n{NT -Xent}} (x_i, x_j) = - \\log \\frac {e^{sim(z_i, z_j)/\\tau }}{\\sum _{k=1}^{2N} \\mathbbm {1}_{[k\\neq i]} e^{sim(z_i,z_k)/\\tau }} \\label {eq:nt_xent} \n(1)\nwhere sim(zi, zk) is the cosine similarity between zi and\nzk, œÑ is a temperature scalar and zk = g(xk). N is the\nsize of the mini-batch of distinct samples where each sam-\nple x has xi and xj as positive augmentations. As shown in\nEqn. 1, the NT-Xent loss maximizes the agreement between\ntwo augmented views xi and xj of the same input sample x\nin a low-dimension representation space encoded by g.\nFor xact\ni\nand xap\ni1 , . . . , xap\niF , the action and appearance\nstream encodings: hact\ni\nand ¬Øhap\ni\nare fed to MLP projection\nheads to obtain zact\ni\nand zap\ni . Similarly we obtain zap\nj\nand\nzact\nj\nfor another augmentation set xact\nj\nand xap\nj1, . . . , xap\njF .\nzap\ni\nand zap\nj\nare used to compute Lap\nNCE contrastive loss to\ntrain the appearance encoder, while Lact\nNCE is computed us-\ning zact\ni\nand zact\nj\nto train the action encoder.\n3.3. Unsupervised Meta-learning for Video Few-\nShot (MetaUVFS)\nMetaUVFS explicitly trains a few-shot meta-learner via\nepisodic training to improve performance on the down-\nstream few-shot tasks having novel classes. MetaUVFS first\ngenerates episodes at video instance level using noise-\ncontrastive embeddings without any supervision and im-\nposes a hardness threshold to boost few-shot meta-learning.\nUsing these generated episodes, MetaUVFS trains a novel\nAction-Appearance Aligned Meta-adaptation (A3M) mod-\nule to align and relate action and appearance features, and\noutput an embedding that can more effectively generalize to\nnovel classes in few-shot testing. The episodic training of\nMetaUVFS imposes a downstream task-specific prior on the\nunsupervised model features that reduces the gap between\ntrain and test settings, thereby improving performance.\n3.3.1\nUnsupervised Hard Episodes Generation\nTo simulate the meta-testing episodic setting during train-\ning, we leverage the unlabeled video data to generate mean-\ningful episodes for meta-training the A3M module. We gen-\nerate 1-shot, 5-way classification episodes (similar to the\ndownstream few-shot task) where the support and query for\neach class are formed using spatio-temporal augmentations\n(Sec. 4.2) of an unlabeled video sample. In this way, the\nclassification happens at the instance level (i.e. each video\nbehaves as its own class) and the task is to classify a query\naugmentation belonging to the correct video sample.\nA\nsimple approach would be to randomly sample unlabeled\nvideos and process their augmentations into episodes. How-\never, the InfoNCE contrastive learning pushes the embed-\ndings, hact\ni\nand ¬Øhap\ni , for the augmentations of a video xi\nalready very close to each other compared to embeddings\n*\n*\nùë•!\n\"#\nùë•!\n\"$%\nA3M\nClassifier\n2D\n3D\nFrozen\nFrozen\n‚Ñé#!\n\"#\n‚Ñé#&\n\"#\n‚Ñé!\n\"$%\n‚Ñé&\n\"$%\n2048\n128\n2048\n128\nùëß!\n\"#\nùëß&\n\"#\nùëß!\n\"$%\nùëß&\n\"$%\n‚Ñí'()\n\"#\n‚Ñí'()\n\"$%\nAppearance \nEncoder\n2D \nCNN\nUnlabeled Video\nData\nAction \nEncoder\nùë•!\n\"$%\nùë•&\n\"$%\n(a) Unsupervised Pre-Training via InfoNCE\nMLP\nMLP\n3D \nCNN\nx\n*\nx\n*\nx\nx\n*\n*\n*\n‚Ä¶\n‚Ä¶\n‚Ä¶\nSupport Query\nEpisode 1\n*\n*\n‚Ä¶\n‚Ä¶\nSupport Query\nEpisode 2\n‚Ä¶\n‚Ä¶\nSupport Query\nEpisode E\n‚Ä¶\nPick n videos with least cosine similarity \nSet of videos from \naction and\nappearance stream + \nfew randomly\nselected videos\nEpisode \nGeneration\n‚Ñé#!\n\"#\n‚Ñé#&\n\"#\n‚Ñé!\n\"$%\n‚Ñé&\n\"$%\n2D \nCNN\n3D \nCNN\nx\n*\n‚Ä¶\nLarge \nSampled \nVideo Batch\nùë•!\n\"#\nùë•&\n\"#\nùë•!\n\"$%\nùë•&\n\"$%\n(b) Unsupervised Hard Episodes Generation\nAction Embedding Space\nAppearance Embedding Space\n‚Ä¶\nùë•!*\n\"#, ùë•!+\n\"#, ‚Ä¶, ùë•!,\n\"#\n2D \nCNN\n‚Ä¶\nùë•!\n\"$%\n3D \nCNN\n‚Ä¶\n‚Ñé!*\n\"#\n‚Ñé!+\n\"#\n‚Ñé!,\n\"#\n2048\n2D embeddings \nfor F frames\n3D action \nembedding\n‚Ñé!\n\"$%\n‚Ä¶\n‚Ä¶\nF\nValue \nhead\nKey \nhead\nQuery \nhead\n‚®Ç\nùëë-\nùëë.\nùëë.\n2048\n(c) Action-Appearance Aligned Meta-adaptation (A3M)\n(d) Few-shot Meta Testing\nQuery\nSupport\n‚Ä¶\nFine-tuning on support \nsamples from novel classes\nTest on query samples from \nnovel classes\nClassifier\n‚Ä¶\nùë•!*\n\"#, ùë•!+\n\"#, ‚Ä¶, ùë•!,\n\"#\n‚Ä¶\nùë•!*\n\"#, ùë•!+\n\"#, ‚Ä¶, ùë•!,\n\"#\nSpatio-temporal\nAugmentation\nx\n*\n‚Ä¶\nFrozen\nFrozen\nSoftmax\n‚äô\nFrozen\nFrozen\nùë•!\n\"#\nùë•!\n\"$%\nA3M\nClassifier\n2D\n3D\nFrozen\nFrozen\nùëë-\nFigure 2. MetaUVFS: Model Overview. (a) Two-stream Training: using a large unlabeled video dataset subjected to our sampling and\naugmentation scheme (see Section 4.2). (b) Hard Episode Generation hard episodes are mined from the two-stream networks‚Äôs feature\nspace. (c) A3M module learns to align appearance over action features through episodic meta-adaptation. (d) Meta-testing Meta-trained\nA3M as a specialized few-shot classifier finetunes on novel class support to classify query videos.\nfor augmentations for other videos. Thus, randomly sam-\npled videos will provide A3M module episodes that can be\ntrivially solved and will impede any meaningful learning.\nAs shown in Fig. 2b, to incentivize learning and generate\nmeaningful episodes, we mine episodes where we select\nhard video instances whose augmentations lie far from each\nother in the embedding space of the trained two-stream en-\ncoders. We feedforward a large batch of video augmenta-\ntions through the trained and frozen action and appearance\nencoders. For each encoder, we select n videos which have\nthe lowest cosine similarity among its augmentations. We\npool the set of the videos collected this way from the two\nencoders and extend this video set by another 10% with ran-\ndomly sampled videos for exploration and to cover all video\nsamples on expectation. We then sample E episodes from\nthis set of videos for one training iteration of few-shot train-\ning. Selecting n videos for both action and appearance en-\nables the few-shot meta-learner to reduce confusion from\nboth action and appearance perspectives (Fig. 1).\n3.3.2\nA3M:\nAction-Appearance\nAligned\nMeta-\nadaptation\nAs shown in Figure 1, it is important for the model to\nattend to both action and appearance-related aspects of a\nvideo in correspondence to each other to enhance intra-\nclass relationship and avoid inter-class confusion, particu-\nlarly when learning from very few labeled samples. To this\nend, we design a novel cross-attention module for action-\nappearance aligned meta-adaptation, A3M, that is trained\nusing episodic few-shot learning to meta-learn to cross-\nalign action with appearance-related features.\nThe A3M module learns to establish a soft correspon-\ndence between the action and appearance features using\nattention-based Transformers [60]. As shown in Fig. 2c, we\nparameterize three linear mappings, key-head K : RD ‚Üí\nRdk, value-head V : RD ‚ÜíRdv and query-head Q :\nRD ‚ÜíRdk for this purpose where dk and dv are the size\nof the key and value embeddings, respectively. We generate\nkey-value pairs using K and V for the frame-level repre-\nsentations, hap\ni1 , . . . , hap\niF , from the 2D appearance encoder.\nLet km = K ¬∑ hap\nm and vm = V ¬∑ hap\nm form the key-value\npair for the mth frame-level representation for unlabeled xi.\nWe also generate a query embedding, q = Q ¬∑ hact, for the\nspatio-temporal feature, hact, from the 3D action encoder\nusing Q. We then compute the dot-product attention scores\nbetween the keys and the query, and normalize the scores\nvia softmax over all key embeddings as,\n  \\\nfootno t esize \na\n_ m = \\f r ac {\\exp (\\mathbf {k}_m \\cdot \\mathbf {q}) / \\sqrt {d_k}}{\\sum _{t} \\exp (\\mathbf {k}_t \\cdot \\mathbf {q} / \\sqrt {d_k})} \\label {eq:attn_ma2m} \n(2)\nwhere am is the attention score for the mth frame embed-\nding. These attention scores provide a soft correspondence\nthat align and relate the action information with the appear-\nance of the video. The attention scores are then combined\nwith the value head embeddings and aggregated via sum to\nobtain a single feature embedding, hA3M = P\nm amvm.\nAs the attention scores are computed via a combination of\naction and appearance features, they weigh the appearance\nfeatures to focus on the most action-relevant parts. The ag-\ngregated embedding hA3M, conditioned on both action and\nappearance information, is therefore better equipped than\nnaive concatenation for few-shot tasks.\n3.3.3\nFew-Shot Meta-Training\nWe leverage Model-Agnostic Meta Learning (MAML) [14]\nto train the network to learn to adapt to a new task of novel\naction classes with few labeled samples. Once we train the\naction and appearance streams, we freeze the two back-\nbones and train fŒ∏ comprising of the A3M module along\nwith a classifier layer during the few-shot episodic meta-\ntraining. The action-appearance aligned feature embedding\nfrom the A3M module is l2-normalized before being fed\nto the classifier.\nFor each generated episode e ‚ààE in\na training iteration, we generate s support augmentations\nfor sampled videos and compute adapted parameters with\ngradient descent of the cross-entropy classification loss L\nover fŒ∏ as Œ∏‚Ä≤\ne = Œ∏ ‚àíŒ±‚àáŒ∏Le(fŒ∏) where Œ± is the adaptation\nlearning rate. We then generate q query augmentations for\nvideos in episode e to compute the loss L using adapted pa-\nrameters Œ∏‚Ä≤\ne as Le(fŒ∏‚Ä≤e). We repeat this for all E episodes\nand finally update Œ∏ at the end of the training iteration as\nŒ∏ ‚ÜêŒ∏ ‚àíŒ≤‚àáŒ∏\nPE\ne Le(fŒ∏‚Ä≤e) where Œ≤ is the learning rate for\nthe meta-learner optimizer.\n3.3.4\nFew-Shot Meta-Testing\nOnce trained, we test MetaUVFS by finetuning on mul-\ntiple few-shot test episodes. As can be seen in Fig. 2d, for\neach episode, we freeze the action-appearance encoders and\nfinetune the A3M and classifier layers which has been meta-\ntrained. After every episode, we refresh the parameters of\nA3M and classifier layers for the next episode.\n4. Experiments and Results\n4.1. Datasets\nWe evaluate MetaUVFS on three publicly-available few-\nshot datasets:\nKinetics100 [6, 74], UCF101 [52] and\nHMDB51 [37].\nFollowing [74], we obtain the few-shot\ntrain/validation/test splits with 64/12/24 non-overlapping\nclasses for Kinetics100. For UCF101 and HMDB51, we\nfollow the few-shot split from [71]. UCF101 contains 100\nclasses split into 70/10/20 and HMDB51 contains 51 classes\nsplit as 31/10/10. The test splits of each dataset are used for\nnovel class evaluation in the meta-testing phase. For the un-\nsupervised training of MetaUVFS‚Äôs two-stream networks,\nwe leverage Kinetics700 [6] without using any labels. Ki-\nnetics700 is a large-scale video classification dataset that\ncovers 700 human action classes including human-object\nand human-human interactions. To increase the size of our\nunlabeled training data, we also include the videos from the\nbase-classes of Kinetics100, UCF101, and HMDB51, with-\nout the labels. Altogether, we obtain around 550K video\nclips with a duration of around 10s each (25 FPS). We take\nextra precaution to ensure that there is no video in the train-\ning dataset belonging to the union of all the novel classes\nacross all three evaluation datasets. This is to ensure that\nour testing is truly on a disjoint set of unseen classes.\n4.2. Implementation Details\nData Sampling and Augmentation We develop a spatio-\ntemporal sampling protocol that is most optimal for the un-\nsupervised two-stream training and A3M-based few-shot\ntraining/testing.\nFor an input video, the 2D appearance\nstream encodes 8 input frames where 1 frame is randomly\nsampled from each of 8 segments equally-partitioned along\nthe video length. With focus on spatial information, we use\na higher frame resolution of 224 √ó 224. We refer to this as\n8 √ó 1. For the 3D-action stream, with the goal of encod-\ning fine-grained spatio-temporal action information across\nvideo segments, we sample 4 clips across 4 equidistant seg-\nments of the video to form a 16 frame input. To balance the\nspatio-temporal information, we use a lower frame resolu-\ntion of 112 √ó 112. We refer to this as 4 √ó 4. We follow\nSimCLR‚Äôs protocol for spatial augmentation [8]: a compo-\nsition of Random crops, Random horizontal flips, Random\nColor Jitter, Random grayscale, Gaussian blur. The spatial\naugmentation is clip-wise consistent, i.e., the random seed\nis fixed across all frames of a video augmentation [70, 22].\nMetaUVFS Training\nWe use ResNet50 [29] backbone\nto train the 2D appearance stream and its 3D counterpart,\nResNet50-3D [26], for the 3D action stream. The dimen-\nsion of zap and zact obtained from the MLP projection head\nis 128 (similar to [8]). We first train the action and appear-\nance streams individually using losses Lap\nNCE and Lact\nNCE\nrespectively.\nWe use a batch size of 512 and train both\nmodels for 300 epochs on 64 NVIDIA P100 GPUs. Fol-\nlowing [30, 21], we do a gradual learning rate (LR) warmup\nfor 5 epochs followed by a half-period cosine learning rate\ndecay with SGD optimizer and 0.9 momentum. With 0.001\nper-gpu LR, we also linearly scale the LR to 0.064.\nFor hard-mining episodes, n is set to 32. We set dk =\n128 and dv = 2048 for the A3M module. For MAML, we\nset E = 10, Œ± = 0.001 and Œ≤ = 10. We train for 20, 000\niterations using Adam optimizer and cosine annealing [1]\nfor a total of 200K unsupervised hard-mined episodes. For\nmore details, please refer to the supplementary material.\nFew-shot Evaluation\nWe evaluate MetaUVFS on all\nthree datasets based on 5-way, 1-shot and 5-way, 5-shot\nsettings as is standard in few-shot learning literature. For\neach episode, 5 classes are randomly sampled from the set\nMethods\nSupervision\nUCF101\nHMDB51\nKinetics100\nPretraining\nBase-Class\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nMatching Net [74]\nImagenet-2D\nYes\n-\n-\n-\n-\n53.3\n74.6\nMAML [74]\nImagenet-2D\nYes\n-\n-\n-\n-\n54.2\n75.3\nCMN [74]\nImagenet-2D\nYes\n-\n-\n-\n-\n60.5\n78.9\nTARN [3]\nSports-1M\nYes\n-\n-\n-\n-\n66.6\n80.7\nOTAM [5]\nImagenet-2D\nYes\n-\n-\n-\n-\n73.0\n85.8\nR-3DFSV [68]\nSports-1M\nYes\n-\n-\n-\n-\n95.3\n97.8\nProtoGAN [38]\nSports-1M\nYes\n57.8 ¬± 3.0\n80.2 ¬± 1.3\n34.7 ¬± 9.20\n54.0 ¬± 3.90\n-\n-\nAmeFu-Net [15]\nImagenet-2D\nYes\n85.1\n95.5\n60.2\n75.5\n74.1\n86.8\nRVN [4]\nKinetics-400\nYes\n88.71 ¬± 0.19\n96.78 ¬± 0.08\n63.43 ¬± 0.28\n79.69 ¬± 0.20\n-\n-\nARN [71]\nNo\nYes\n66.32 ¬± 0.99\n83.12 ¬± 0.70\n45.15 ¬± 0.96\n60.56 ¬± 0.86\n63.7\n82.4\n3DRotNet [33]\nNo\nNo\n39.43 ¬± 0.48\n33.61 ¬± 0.34\n32.35 ¬± 0.42\n27.84 ¬± 0.40\n27.53 ¬± 0.36\n25.54 ¬± 0.39\nVCOP [70]\nNo\nNo\n32.91 ¬± 0.42\n39.11 ¬± 0.37\n27.80 ¬± 0.37\n31.56 ¬± 0.35\n26.48 ¬± 0.37\n28.87 ¬± 0.36\nIIC [55]\nNo\nNo\n56.81 ¬± 0.46\n78.74 ¬± 0.37\n34.66 ¬± 0.41\n49.57 ¬± 0.44\n37.73 ¬± 0.43\n51.11 ¬± 0.43\nPace Prediction [62]\nNo\nNo\n25.58 ¬± 0.33\n26.58 ¬± 0.31\n26.21 ¬± 0.33\n27.09 ¬± 0.31\n22.42 ¬± 0.33\n22.94 ¬± 0.30\nMemDPC [23]\nNo\nNo\n49.27 ¬± 0.44\n67.38 ¬± 0.45\n30.33 ¬± 0.40\n41.15 ¬± 0.42\n42.01 ¬± 0.41\n53.90 ¬± 0.43\nCoCLR [24]\nNo\nNo\n51.99 ¬± 0.46\n72.17 ¬± 0.42\n31.29 ¬± 0.40\n44.92 ¬± 0.45\n37.59 ¬± 0.42\n51.11 ¬± 0.43\nCVRL [47]\nNo\nNo\n63.00 ¬± 0.41\n87.80 ¬± 0.30\n44.21 ¬± 0.45\n60.35 ¬± 0.45\n53.26 ¬± 0.48\n71.39 ¬± 0.44\nMetaUVFS (Ours)\nNo\nNo\n76.38 ¬± 0.40\n92.50 ¬± 0.24\n47.55 ¬± 0.45\n66.13 ¬± 0.33\n62.80 ¬± 0.45\n79.55 ¬± 0.39\nTable 1. Results on UCF101, HMDB51 and Kinetics100 datasets for 5-way, 1-shot and 5-shot few-shot action recogniton. Our method\nMetaUVFS outperforms SoTA methods on unsupervised video representations by large margins on few-shot benchmarks. We also show\ncompetitive performance w.r.t. supervised few-shot video approaches. Moreover, on UCF101 and HMDB51, MetaUVFS is able to out-\nperform ARN that uses only base-class supervision. MetaUVFS also outperforms ProtoGAN on UCF101 and HMDB51, and CMN on\nKinetics100. Values in blue represent SoTA across all levels of supervision.\nof novel classes for classification and training happens on 1\nand 5 support samples per class respectively. In all settings,\nTop-1 accuracy is reported on 1 query sample per class. In\neach experiment, we randomly sample 10,000 episodes for\nfew-shot meta-testing and report the average accuracy at the\n95% confidence interval. Finetuning is done at a constant\nlearning rate of 10 for 50 epochs for all experiments.\n4.3. Compare to SoTA Unsupervised Approaches\nTable 1 compares MetaUVFS with various state-of-the-\nart supervised and unsupervised methods on different few-\nshot settings and datasets. We categorize the different tech-\nniques based on the amount of supervision in terms of base-\nclass data (‚ÄòYes‚Äô in Base-Class) and surrogate supervision,\ni.e., initializing the network using the weights pretrained\non a large-scale supervised image/video data (‚ÄòYes‚Äô in Pre-\ntrained Weights). Cells are left blank if there are no publicly\navailable results for that setting.\nThe second part of Table 1 compares MetaUVFS with\nvarious state-of-the-art methods that leverage unlabeled\nvideos for representation learning. To the best of our knowl-\nedge, MetaUVFS is the first approach that specializes in\nfew-shot action recognition in a purely unsupervised man-\nner. Hence, there is no publicly available benchmark for\nthe performance of existing video-based unsupervised tech-\nniques on few-shot action recognition. We took the ini-\ntiative to assess these approaches on our few-shot test-bed\nusing the same hyperparameters for few-shot meta-testing\nas MetaUVFS. Many of these approaches are originally\ntrained on a relatively small unlabeled dataset. Therefore,\nfor a fair comparison, we train these methods on our large-\nscale unlabeled dataset using their publicly available code.\nAs shown in Table 1, MetaUVFS is able to clearly\noutperform all state-of-the-art unsupervised methods on\nthe task of few-shot action recognition by at least\n13.38%, 3.34% and 9.54% (absolute increase) on UCF101,\nHMDB51 and Kinetics100 1-shot, 5-way benchmark re-\nspectively.\nAmong the methods we compare, IIC [55],\nCVRL [47] and CoCLR (RGB only) [24] also use con-\ntrastive loss for unsupervised training. The superior perfor-\nmance of MetaUVFS in comparison to these methods indi-\ncate that our approach of jointly leveraging and aligning ac-\ntion and appearance along with meta-training episodically\nfor few-shot plays an integral role in performing effectively\nwhen the downstream task lies in the low-shot regime.\n4.4. Compare to SoTA Supervised Few-shot Works\nThe first part of Table 1 compares MetaUVFS with var-\nious state-of-the-art supervised few-shot action recognition\nmethods. We can observe that compared to ARN [71] that\nuses only base-class data as supervision, MetaUVFS sig-\nnificantly outperforms on UCF101 and HMDB51, and per-\nforms competitively on Kinetics100. Furthermore, MetaU-\nVFS is even able to outperform some of the supervised\nmethods that use both pretrained weights and base-class la-\nbels for supervision such as ProtoGAN [38] on UCF101 and\nHMDB51, and CMN [74] on Kinetics100. It is worth noting\nthat, unlike these methods that need to train separate models\nto obtain results on the different datasets, MetaUVFS trains\na single unsupervised model to achieve all results. This\nsingle model either outperforms or performs competitively\ncompared to supervised methods across all three datasets.\nAction\nAppearance\nA3M\nHard Episodes\nUCF101\nHMDB51\nKinetics100\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\n‚úì\n‚úì\n66.97 ¬± 0.44\n88.64 ¬± 0.30\n44.56 ¬± 0.45\n61.03 ¬± 0.45\n53.56 ¬± 0.48\n71.31 ¬± 0.44\n‚úì\n‚úì\n66.10 ¬± 0.45\n84.58 ¬± 0.34\n39.97 ¬± 0.46\n58.20 ¬± 0.46\n50.91 ¬± 0.48\n69.47 ¬± 0.46\n‚úì\n‚úì\n71.82 ¬± 0.42\n89.93 ¬± 0.27\n44.64 ¬± 0.46\n62.40 ¬± 0.45\n57.52 ¬± 0.46\n75.21 ¬± 0.41\n‚úì\n‚úì\n‚úì\n73.02 ¬± 0.42\n91.38 ¬± 0.26\n44.89 ¬± 0.46\n64.96 ¬± 0.44\n59.16 ¬± 0.44\n77.42 ¬± 0.40\n‚úì\n‚úì\n‚úì\n73.97 ¬± 0.41\n91.50 ¬± 0.26\n45.84 ¬± 0.45\n64.68 ¬± 0.41\n59.88 ¬± 0.45\n77.64 ¬± 0.39\n‚úì\n‚úì\n‚úì\n‚úì\n76.38 ¬± 0.40\n92.50 ¬± 0.24\n47.55 ¬± 0.45\n66.13 ¬± 0.33\n62.80 ¬± 0.45\n79.55 ¬± 0.39\nTable 2. Ablation study of MetaUVFS highlights the superior few-shot performance of meta-training two-stream feature representations\nover individual action and appearance streams. The performance is further boosted by action-appearance feature alignment by the A3M\nmodule. Moreover, mining unsupervised hard episodes is crucial for effectively training the A3M module.\n4.5. MetaUVFS: Ablation Study\nWe conduct an ablation study where we isolate individ-\nual aspects of MetaUVFS and quantify their impact on the\nfew-shot performance. Table 2 summarizes the results. We\ntrain all ablation experiments using MAML as the few-shot\nalgorithm.\nWe first conduct experiments without the A3M module\nwhere the network consists of only the action stream, only\nthe appearance stream and dual action-appearance stream\n(Table 2, Rows 1, 2, 4). Without the A3M module, for\nthe one-stream setting, we directly feed the features from\nthe available stream (averaging appearance features over 8\nframes) to the classifier layer for few-shot episodic meta-\ntraining and later for meta-testing; for the two-stream set-\nting, we simply concatenate the action features and appear-\nance features (averaged over 8 frames) and feed them to the\nclassifier for few-shot episodic meta-training. All three ex-\nperiments use unsupervised hard-mined episodes. In the ab-\nsence of either the action or the appearance stream, only the\nfeatures of the available stream are used to mine episodes.\nWe can observe from Table 2 (Rows 1, 2, 4) that the few-\nshot performance is significantly worse when either action\nor appearance stream is missing compared to when both are\npresent. This is because when only a few support samples\nare available to learn for a set of novel classes, the likeli-\nhood of the model to make mistakes reduces sharply in the\npresence of both streams as it allows the network more ways\nto activate and respond to the representative features neces-\nsary for correct classification. We can also compare Row\n1 (Action stream only) with CVRL [47] in Table 1. CVRL\nbackbone is similar to our 3D action stream. However, due\nto an explicit few-shot training phase, our Action only base-\nline performs consistently better than CVRL.\nRows 4 and 6 in Table 2 highlight the impact of the A3M\nmodule by aligning action-appearance as part of few-shot\ntraining. Our proposed A3M module in MetaUVFS results\nin an average absolute improvement of 3.22% and 1.47%\non 5-way, 1-shot and 5-way, 5-shot benchmarks across all\ndatasets. Aligning the action and appearance features dur-\ning few-shot episodic training significantly improves the\nmodel‚Äôs ability to attend to the most representative video\naspects while leveraging the inductive biases of both 2D\nand 3D CNNs to learn complementary representation that\nboosts few-shot performance.\nWe then perform an ablation where we train our method\nepisodically without mining hard episodes based on noise-\ncontrastive embeddings (Table 2, Row 5). Comparing Rows\n5 and 6, we can observe a significant reduction in perfor-\nmance without hard episodes, underlining the importance\nof mining hard episodes to the few-shot episodic training of\nMetaUVFS. This is because in the absence of hard episodes,\nthe randomly sampled videos in a training episode are such\nthat the action and appearance embeddings fed for support\nand query augmentation samples to the A3M module during\ntraining are already easily separable. This severely compro-\nmises the training of A3M and makes it behave close to\nan identity function, as evident from Row 5‚Äôs only slightly\nhigher performance than Row 4 where A3M is not present.\nWe additionally perform an experiment where both A3M\nand hard episodes are absent during training (Table 2, Row\n3).\nWe can observe that this setting results in a statis-\ntically significant reduction in performance compared to\nwhen A3M and/or hard episodes are employed for training\n(Rows 4-6).\n4.6. Discussion\nImpact of Frame Sampling.\nSince the two-streams in\nMetaUVFS specialize both in terms of architecture and their\nfunction, we observe that the sampling strategy in choos-\ning the frames as input to both streams along with their\nframe resolution make a difference in the performance. Ta-\nble 3 provides an analysis of the few-shot performance\nfor 5-way, 1-shot settings on Kinetics100 across different\nsampling strategies for both 3D action and 2D appearance\nstreams first individually and then in combination. We ob-\nserve that for the 3D stream, choosing a 4√ó4 sampling, i.e.,\nsampling 4 segments of 4 frames uniformly over the entire\nvideo length provides a 3.3% improvement over sampling\n16 frames from 32 consecutive frames with a stride of 2.\nSimilarly, for the 2D stream, we find that 16 √ó 1 and 8 √ó 1\nsampling, i.e., sampling 1 frame from 16 or 8 segments over\nthe entire video length as most effective. In our two-stream\nsetting, we find 8√ó1, 4√ó4 as the optimal sampling scheme.\nMeta-learning Algorithm. We further validate the choice\nof MAML as our few-shot meta-learning algorithm by as-\nAppearance only (224√ó224)\nAction only (112√ó112)\nSampling\n4√ó1\n8√ó1\n16√ó1\n32‚àí\n‚Üí16\n4√ó4\n8‚àí\n‚Üí4√ó4\n1-shot\n50.54 ¬± 0.48\n50.91 ¬± 0.48\n51.0 ¬± 0.42\n50.26 ¬±0.52\n53.56 ¬± 0.48\n53.09 ¬± 0.42\nMetaUVFS(Appearance + Action + A3M, ours)\nSampling\n4√ó1, 4√ó4\n4√ó1, 8‚àí\n‚Üí4√ó4\n8√ó1, 4√ó4\n8√ó1, 8‚àí\n‚Üí4√ó4\n16√ó1, 4√ó4\n16√ó1, 8‚àí\n‚Üí4√ó4\n1-shot\n60.76 ¬± 0.44\n60.69 ¬± 0.45\n62.80 ¬± 0.45\n61.34 ¬±0.48\n60.30 ¬±0.41\n60.91 ¬± 0.47\nTable 3. Comparing sampling strategies evaluated on Kinetics100 for 1-shot, 5-way. ‚àí‚Üídenotes downsampling from A to B value.\nFigure 3.\nComparison of MetaUVFS\nwith CVRL [47] on\nUCF101 and Kinetics100 dataset on 1-shot few-shot for N-way\nclassification where N ranges from 5 to over 20.\nFew-shot Algorithm\n1-shot\n5-shot\nProtoNet [51]\n31.20 ¬± 0.39\n55.96 ¬± 0.44\nBaseline++ [9]\n56.10 ¬± 0.47\n73.37 ¬± 0.43\nProtoMAML [59]\n62.13 ¬± 0.46\n78.65 ¬± 0.40\nMAML [14]\n62.80 ¬± 0.45\n79.55 ¬± 0.39\nTable 4. Comparison between different few-shot meta-learning al-\ngorithms on Kinetics100 5-way, 1/5-shot dataset.\nsessing other popular few-shot approaches in the literature.\nTo compare with ProtoNet [51] and ProtoMAML [59], we\nrepurpose the output of A3M module to compute proto-\ntypes across support samples (augmentations) and compare\nagainst query samples to compute the loss during training.\nFor few-shot testing on novel classes, ProtoNet matches the\nquery samples with prototypes that are computed from sup-\nport samples to assign class label based on the best matched\nprototype.\nFor ProtoMAML, we reshape the prototypes\ncomputed from support samples as parameters of the clas-\nsifier layer which is finetuned along with the A3M module\nas per Sec. 3.3.4. We also compare with Baseline++ [9]\nwhere we use meta-trained parameters for A3M and clas-\nsifier but change the few-shot finetuning during testing to\nBaseline++. As shown in Table 4, we find that our few-shot\nmeta-testing protocol of using MAML with l2-normalized\nembedding as input to the classifier significantly outper-\nforms other few-shot learning methods. We also observe\nthat, in general, few-shot methods employing finetuning\nduring few-shot testing tend to perform better. We believe\nthis is due to extra adaptation steps needed during testing\nbecause of the absence of supervision during training.\nSignificance of Action-Appearance We conduct an exper-\niment where both the streams are either 3D CNNs (action)\nor 2D CNNs (appearance). This delineates the impact of\nhaving complementary action and appearance streams on\nfew-shot performance from the impact of increase in the\nStreams\n1-shot\n5-shot\nAppearance + Appearance\n55.75 ¬± 0.46\n72.23 ¬± 0.43\nAction + Action\n54.25 ¬± 0.47\n73.21 ¬± 0.42\nAction + Appearance\n59.16 ¬± 0.44\n77.42 ¬± 0.40\nTable 5. Comparison of using Action and Appearance streams in\nMetaUVFS with using two Action or two Appearance streams.\nResults are without A3M module for fair comparison.\nparameter count due to an additional backbone. We train\nthem using MAML with hard episodes without A3M for\nfair comparison. We can observe from Table 5 that although\nhaving more parameters with either two appearance or two\naction streams improves the performance compared to sin-\ngle stream, the improvement is significantly higher when\na combination of action and appearance streams is used\nthat helps to leverage more diverse 2D/3D representations\nto learn more generalizable few-shot video representations.\nMany-Way Few Shot. We go beyond the 5-way 1-shot set-\nting by increasing the number of novel classes to evaluate\nMetaUVFS on a more challenging and in-the-wild few-shot\nmany-way classification task on UCF101 and Kinetics100.\nFig. 3 shows a plot for this experiment. Although, as ex-\npected, the performance reduces with increasing N, MetaU-\nVFS is still able to outperform the best-performing unsu-\npervised baseline method, CVRL, by a significant margin\nfor all N-way 1-shot classification tasks considered. This\nproves that MetaUVFS is more robust even in extreme few-\nshot scenarios where the inter-class confusion is higher.\n5. Conclusion\nWe propose a novel unsupervised meta-learning al-\ngorithm, MetaUVFS, for few-shot video action recogni-\ntion. It leverages large-scale unlabeled video data to learn\nunsupervised video features from a two-stream action-\nappearance network.\nIt further performs explicit few-\nshot episodic meta-learning over unsupervised hard-mined\nepisodes using a novel Action-Appearance Aligned Meta-\nadaptation (A3M) module.\nThe A3M module learns to\nalign the 3D action with 2D appearance features to learn\nan embedding that is more effective in focusing on the\naction-specific features of a video for the few-shot down-\nstream task. Through extensive experiments, we demon-\nstrate that using an explicit few-shot learner and action-\nappearance aligned features makes MetaUVFS significantly\nbetter suited for downstream few-shot tasks compared to all\nstate-of-the-art unsupervised methods. Moreover, MetaU-\nVFS performs competitively and sometimes even outper-\nforms SoTA supervised few-shot methods.\nAcknowledgement\nAt Oregon State University, Jay Patravali is supported by\nDARPA grant N66001-19-2-4035.\nReferences\n[1] Antreas Antoniou, Harrison Edwards, and Amos Storkey.\nHow to train your maml. 2019. 5\n[2] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri,\nWilliam T Freeman, Michael Rubinstein, Michal Irani, and\nTali Dekel. Speednet: Learning the speediness in videos.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9922‚Äì9931, 2020. 3\n[3] Mina Bishay, Georgios Zoumpourlis, and Ioannis Pa-\ntras.\nTarn: Temporal attentive relation network for few-\nshot and zero-shot action recognition.\narXiv preprint\narXiv:1907.09021, 2019. 2, 6\n[4] Congqi Cao, Yajuan Li, Qinyi Lv, Peng Wang, and Yanning\nZhang. Few-shot action recognition with implicit temporal\nalignment and pair similarity optimization. arXiv preprint\narXiv:2010.06215, 2020. 1, 2, 6\n[5] Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, and\nJuan Carlos Niebles. Few-shot video classification via tem-\nporal alignment. In The IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020. 1, 2, 6\n[6] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zis-\nserman.\nA short note on the kinetics-700 human action\ndataset. arXiv preprint arXiv:1907.06987, 2019. 5\n[7] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299‚Äì6308, 2017. 2\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020. 3, 5\n[9] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nfication. In International Conference on Learning Represen-\ntations, 2018. 8\n[10] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nfication. arXiv preprint arXiv:1904.04232, 2019. 1, 2\n[11] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, pages 1422‚Äì1430, 2015. 3\n[12] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrosstransformers: spatially-aware few-shot transfer. arXiv\npreprint arXiv:2007.11498, 2020. 2\n[13] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 6202‚Äì6211, 2019. 2\n[14] Chelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-\nagnostic meta-learning for fast adaptation of deep networks.\nIn Proceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 1126‚Äì1135. JMLR. org,\n2017. 1, 2, 5, 8\n[15] Yuqian Fu, Li Zhang, Junke Wang, Yanwei Fu, and Yu-Gang\nJiang. Depth guided adaptive meta-fusion network for few-\nshot video recognition.\nIn Proceedings of the 28th ACM\nInternational Conference on Multimedia, pages 1142‚Äì1151,\n2020. 1, 2, 6\n[16] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nP¬¥erez, and Matthieu Cord. Boosting few-shot visual learn-\ning with self-supervision. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 8059‚Äì8068,\n2019. 2\n[17] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4367‚Äì4375, 2018. 1, 2\n[18] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. arXiv preprint arXiv:1803.07728, 2018. 3\n[19] Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic,\nand Bryan Russell. Actionvlad: Learning spatio-temporal\naggregation for action classification. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 971‚Äì980, 2017. 2\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. Advances\nin Neural Information Processing Systems, 27:2672‚Äì2680,\n2014. 2\n[21] Priya Goyal, Piotr Doll¬¥ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He.\nAccurate, large mini-\nbatch sgd: Training imagenet in 1 hour.\narXiv preprint\narXiv:1706.02677, 2017. 5\n[22] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-\naugmented dense predictive coding for video representation\nlearning. arXiv preprint arXiv:2008.01065, 2020. 3, 5\n[23] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-\naugmented dense predictive coding for video representation\nlearning. In ECCV, 2020. 6\n[24] Tengda Han, Weidi Xie, and Andrew Zisserman.\nSelf-\nsupervised co-training for video representation learning. In\nNeurips, 2020. 1, 3, 6\n[25] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Learn-\ning spatio-temporal features with 3d residual networks for\naction recognition. In Proceedings of the IEEE International\nConference on Computer Vision Workshops, pages 3154‚Äì\n3160, 2017. 2\n[26] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can\nspatiotemporal 3d cnns retrace the history of 2d cnns and\nimagenet? In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 6546‚Äì\n6555, 2018. 5\n[27] Bharath Hariharan and Ross Girshick.\nLow-shot visual\nrecognition by shrinking and hallucinating features. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 3018‚Äì3027, 2017. 1, 2\n[28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729‚Äì9738, 2020. 3\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 5\n[30] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of tricks for image classification\nwith convolutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 558‚Äì567, 2019. 5\n[31] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan,\nand Xilin Chen. Cross attention network for few-shot classi-\nfication. arXiv preprint arXiv:1910.07677, 2019. 2\n[32] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised\nlearning via meta-learning. In International Conference on\nLearning Representations, 2018. 3\n[33] Longlong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian.\nSelf-supervised spatiotemporal feature learning via video ro-\ntation prediction. arXiv preprint arXiv:1811.11387, 2018. 3,\n6\n[34] Siavash Khodadadeh, Ladislau Boloni, and Mubarak Shah.\nUnsupervised meta-learning for few-shot image classifica-\ntion. In Advances in Neural Information Processing Systems,\npages 10132‚Äì10142, 2019. 3\n[35] Dahun Kim, Donghyeon Cho, and In So Kweon.\nSelf-\nsupervised video representation learning with space-time cu-\nbic puzzles. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, volume 33, pages 8545‚Äì8552, 2019. 3\n[36] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.\nSiamese neural networks for one-shot image recognition. In\nICML deep learning workshop, volume 2. Lille, 2015. 1\n[37] Hildegard Kuehne, Hueihan Jhuang, Est¬¥ƒ±baliz Garrote,\nTomaso Poggio, and Thomas Serre. Hmdb: a large video\ndatabase for human motion recognition. In 2011 Interna-\ntional Conference on Computer Vision, pages 2556‚Äì2563.\nIEEE, 2011. 5\n[38] Sai Kumar Dwivedi, Vikram Gupta, Rahul Mitra, Shuaib\nAhmed, and Arjun Jain. Protogan: Towards few shot learn-\ning for action recognition. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision Workshops, pages\n0‚Äì0, 2019. 2, 6\n[39] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-\nHsuan Yang. Unsupervised representation learning by sort-\ning sequences.\nIn Proceedings of the IEEE International\nConference on Computer Vision, pages 667‚Äì676, 2017. 3\n[40] Ralph Linsker. Self-organization in a perceptual network.\nComputer, 21(3):105‚Äì117, 1988. 3\n[41] William Lotter, Gabriel Kreiman, and David Cox. Deep pre-\ndictive coding networks for video prediction and unsuper-\nvised learning. arXiv preprint arXiv:1605.08104, 2016. 3\n[42] Carlos Medina, Arnout Devos, and Matthias Grossglauser.\nSelf-supervised prototypical transfer learning for few-shot\nclassification. arXiv preprint arXiv:2006.11325, 2020. 3\n[43] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-\nfle and learn: unsupervised learning using temporal order\nverification. In European Conference on Computer Vision,\npages 527‚Äì544. Springer, 2016. 3\n[44] Mehdi Noroozi and Paolo Favaro.\nUnsupervised learning\nof visual representations by solving jigsaw puzzles.\nIn\nEuropean Conference on Computer Vision, pages 69‚Äì84.\nSpringer, 2016. 3\n[45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 3\n[46] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo.\nEvolving losses for unsupervised video representation learn-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 133‚Äì142, 2020.\n3\n[47] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang,\nHuisheng Wang, Serge Belongie, and Yin Cui. Spatiotempo-\nral contrastive video representation learning. arXiv preprint\narXiv:2008.03800, 2020. 1, 3, 6, 7, 8\n[48] Sachin Ravi and Hugo Larochelle. Optimization as a model\nfor few-shot learning. 2016. 1, 2\n[49] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell,\nKevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and\nRichard S Zemel. Meta-learning for semi-supervised few-\nshot classification. arXiv preprint arXiv:1803.00676, 2018.\n2\n[50] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra, and Timothy Lillicrap.\nMeta-learning with\nmemory-augmented neural networks. In International con-\nference on machine learning, pages 1842‚Äì1850. PMLR,\n2016. 2\n[51] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical\nnetworks for few-shot learning. In Advances in neural in-\nformation processing systems, pages 4077‚Äì4087, 2017. 1, 2,\n8\n[52] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 5\n[53] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1199‚Äì1208, 2018. 1, 2\n[54] Li Tao, Xueting Wang, and Toshihiko Yamasaki. Rethinking\nmotion representation: Residual frames with 3d convnets for\nbetter action recognition. arXiv preprint arXiv:2001.05661,\n2020. 2\n[55] Li Tao, Xueting Wang, and Toshihiko Yamasaki.\nSelf-\nsupervised video representation learning using inter-intra\ncontrastive framework.\nIn Proceedings of the 28th ACM\nInternational Conference on Multimedia, pages 2193‚Äì2201,\n2020. 1, 3, 6\n[56] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive multiview coding. arXiv preprint arXiv:1906.05849,\n2019. 3\n[57] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 4489‚Äì4497,\n2015. 2\n[58] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In Proceedings of the\nIEEE conference on Computer Vision and Pattern Recogni-\ntion, pages 6450‚Äì6459, 2018. 2\n[59] Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal\nLamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, et al.\nMeta-dataset: A dataset of datasets for learning to learn from\nfew examples. In International Conference on Learning Rep-\nresentations, 2019. 8\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems, 30:5998‚Äì6008, 2017. 4\n[61] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. In\nAdvances in neural information processing systems, pages\n3630‚Äì3638, 2016. 1, 2\n[62] Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu.\nSelf-\nsupervised video representation learning by pace prediction.\narXiv preprint arXiv:2008.05861, 2020. 3, 6\n[63] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment net-\nworks: Towards good practices for deep action recognition.\nIn European conference on computer vision, pages 20‚Äì36.\nSpringer, 2016. 2\n[64] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794‚Äì7803, 2018. 2\n[65] Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Lau-\nrens van der Maaten.\nSimpleshot:\nRevisiting nearest-\nneighbor classification for few-shot learning. arXiv preprint\narXiv:1911.04623, 2019. 2\n[66] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath\nHariharan. Low-shot learning from imaginary data. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 7278‚Äì7286, 2018. 1, 2\n[67] Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Fe-\nichtenhofer, and Philipp Krahenbuhl. A multigrid method\nfor efficiently training video models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 153‚Äì162, 2020. 2\n[68] Yongqin Xian,\nBruno Korbar,\nMatthijs Douze,\nBernt\nSchiele, Zeynep Akata, and Lorenzo Torresani.\nGeneral-\nized many-way few-shot video classification. arXiv preprint\narXiv:2007.04755, 2020. 1, 2, 6\n[69] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking spatiotemporal feature learning:\nSpeed-accuracy trade-offs in video classification.\nIn Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 305‚Äì321, 2018. 2\n[70] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and\nYueting Zhuang. Self-supervised spatiotemporal learning via\nvideo clip order prediction. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n10334‚Äì10343, 2019. 3, 5, 6\n[71] Hongguang Zhang, Li Zhang, X Qui, Hongdong Li,\nPhilip HS Torr, and Piotr Koniusz. Few-shot action recog-\nnition with permutation-invariant attention. In Proceedings\nof the European Conference on Computer Vision (ECCV),\n2020. 1, 2, 5, 6\n[72] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization.\nIn European conference on computer\nvision, pages 649‚Äì666. Springer, 2016. 3\n[73] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-\nralba.\nTemporal relational reasoning in videos.\nIn Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 803‚Äì818, 2018. 2\n[74] Linchao Zhu and Yi Yang. Compound memory networks for\nfew-shot video classification. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), pages 751‚Äì\n766, 2018. 1, 2, 5, 6\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-09-30",
  "updated": "2021-10-11"
}