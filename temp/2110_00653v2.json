{
  "id": "http://arxiv.org/abs/2110.00653v2",
  "title": "Sparse Deep Learning: A New Framework Immune to Local Traps and Miscalibration",
  "authors": [
    "Yan Sun",
    "Wenjun Xiong",
    "Faming Liang"
  ],
  "abstract": "Deep learning has powered recent successes of artificial intelligence (AI).\nHowever, the deep neural network, as the basic model of deep learning, has\nsuffered from issues such as local traps and miscalibration. In this paper, we\nprovide a new framework for sparse deep learning, which has the above issues\naddressed in a coherent way. In particular, we lay down a theoretical\nfoundation for sparse deep learning and propose prior annealing algorithms for\nlearning sparse neural networks. The former has successfully tamed the sparse\ndeep neural network into the framework of statistical modeling, enabling\nprediction uncertainty correctly quantified. The latter can be asymptotically\nguaranteed to converge to the global optimum, enabling the validity of the\ndown-stream statistical inference. Numerical result indicates the superiority\nof the proposed method compared to the existing ones.",
  "text": "arXiv:2110.00653v2  [stat.ML]  2 Dec 2021\nSparse Deep Learning: A New Framework Immune\nto Local Traps and Miscalibration\nYan Sun\nPurdue University\nWest Lafayette, IN 47906\nsun748@purdue.edu\nWenjun Xiong\nGuangxi Normal University & Purdue University\nWest Lafayette, IN 47906\nxiong90@purdue.edu\nFaming Liang∗\nPurdue University\nWest Lafayette, IN 47906\nfmliang@purdue.edu\nAbstract\nDeep learning has powered recent successes of artiﬁcial intelligence (AI). How-\never, the deep neural network, as the basic model of deep learning, has suffered\nfrom issues such as local traps and miscalibration. In this paper, we provide a new\nframework for sparse deep learning, which has the above issues addressed in a\ncoherent way. In particular, we lay down a theoretical foundation for sparse deep\nlearning and propose prior annealing algorithms for learning sparse neural net-\nworks. The former has successfully tamed the sparse deep neural network into the\nframework of statistical modeling, enabling prediction uncertainty correctly quan-\ntiﬁed. The latter can be asymptotically guaranteed to converge to the global op-\ntimum, enabling the validity of the down-stream statistical inference. Numerical\nresult indicates the superiority of the proposed method compared to the existing\nones.\nKeywords: Asymptotic Normality, Posterior Consistency, Prior Annealing, Struc-\nture Selection, Uncertainty Quantiﬁcation\n1\nIntroduction\nDuring the past decade, deep neural networks (DNNs) have achieved the state-of-the-art perfor-\nmance in many machine learning tasks such as computer vision and natural language processing.\nHowever, the DNN suffers from a training-prediction dilemma from the perspective of statistical\ninference: A small DNN model can be well calibrated, but tends to get trapped into a local optimum;\non the other hand, an over-parameterized DNN model can be easily trained to a global optimum\n(with zero training loss), but tends to be miscalibrated [Guo et al., 2017]. In consequence, it is often\nunclear whether a DNN is guaranteed to have a desired property after training instead of getting\ntrapped into an arbitrarily poor local minimum, or whether its decision/prediction is reliable. This\ndifﬁculty makes the trustworthiness of AI highly questionable.\nTo resolve this difﬁculty, researchers have attempted from two sides of the training-prediction\ndilemma. Towards understanding the optimization process of the DNN training, a line of researches\nhave been done. For example, Gori and Tesi [1992] and Nguyen and Hein [2017] studied the train-\ning loss surface of over-parameterized DNNs. They showed that for a fully connected DNN, al-\nmost all local minima are globally optimal, if the width of one layer of the DNN is no smaller\n∗To whom correspondence should be addressed: Faming Liang\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\nthan the training sample size and the network structure from this layer on is pyramidal. Recently,\nAllen-Zhu et al. [2019], Du et al. [2019], Zou et al. [2020] and Zou and Gu [2019] explored the\nconvergence theory of the gradient-based algorithms in training over-parameterized DNNs. They\nshowed that the gradient-based algorithms with random initialization can converge to global min-\nima provided that the width of the DNN is polynomial in training sample size.\nTo improve calibration of the DNN, different methods have been developed, see e.g., Monte Carlo\ndropout [Gal and Ghahramani, 2016] and deep ensemble [Lakshminarayanan et al., 2017]. However,\nthese methods did not provide a rigorous study for the asymptotic distribution of the DNN prediction\nand thus could not correctly quantify its uncertainty. Recently, researchers have attempted to address\nthis issue with sparse deep learning. For example, for Bayesian sparse neural networks, Liang et al.\n[2018], Polson and Roˇcková [2018] and Sun et al. [2021] established the posterior consistency, and\nWang and Rocková [2020] further established the Bernstein-von Mises (BvM) theorem for linear\nand quadratic functionals. The latter guarantees in theory that the Bayesian credible region has\na faithful frequentist coverage. However, since the theory by Wang and Rocková [2020] does not\ncover the point evaluation functional, the uncertainty of the DNN prediction still cannot be correctly\nquantiﬁed. Moreover, their theory is developed with the spike-and-slab prior (i.e., each weight or\nbias of the DNN is subject to a spike-and-slab prior), whose discrete nature makes the resulting pos-\nterior distribution extremely hard to simulate. To facilitate computation, Sun et al. [2021] employed\na mixture Gaussian prior. However, due to nonconvexity of the loss function of the DNN, a direct\nMCMC simulation still cannot be guaranteed to converge to the right posterior distribution even\nwith the mixture Gaussian prior.\nIn this paper, we provide a new framework for sparse deep learning, which successfully resolved the\ntraining-prediction dilemma. In particular, we propose two prior annealing algorithms, one from the\nfrequentist perspective and one from the Bayesian perspective, for learning sparse neural networks.\nThe algorithms start with an over-parameterized deep neural network and then have its structure\ngradually sparsiﬁed. We provide a theoretical guarantee that the training procedures are immune\nto local traps, the resulting sparse structures are consistent, and the predicted values are asymptot-\nically normally distributed. The latter enables the prediction uncertainty correctly quantiﬁed. Our\ncontribution in this paper is two-fold:\n• We provide a new framework for sparse deep learning, which is immune to local traps and\nmiscalibration.\n• We lay down the theoretical foundation for how to make statistical inference with sparse\ndeep neural networks.\nThe remaining part of the paper is organized as follows. Section 2 lays down the theoretical founda-\ntion for sparse deep learning. Section 3 describes the proposed prior annealing algorithms. Section\n4 presents some numerical results. Section 5 concludes the paper.\n2\nTheoretical Foundation for Sparse Deep Learning\nAs mentioned previously, sparse deep learning has received much attention as a promising way\nfor addressing the miscalibration issue of the DNN. Theoretically, the approximation power of the\nsparse DNN has been studied for various classes of functions [Schmidt-Hieber, 2017, Bölcskei et al.,\n2019]. Under the Bayesian setting, posterior consistency has been established in Liang et al. [2018],\nPolson and Roˇcková [2018], Sun et al. [2021]. In particular, the work Sun et al. [2021] has achieved\nimportant progress toward taming sparse DNNs into the framework of statistical modeling. They\nprovide a neural network approximation theory fundamentally different from the existing ones. In\nthe existing theory, no data is involved and a small network can potentially achieve an arbitrarily\nsmall approximation error by allowing connection weights to take values in an unbounded space?.\nIn contrast, the theory by Sun et al. [2021] links the network approximation error, the network size,\nand the bound of connection weights to the training sample size. They prove that for a given training\nsample size n, a sparse DNN of size O(n/ log(n)) has been large enough to approximate many types\nof functions, such as afﬁne functions and piecewise smooth functions, arbitrarily well as n →∞.\nMoreover, they prove that the sparse DNN possesses many theoretical guarantees. For example, its\nstructure is more interpretable, from which the relevant variables can be consistently identiﬁed for\nhigh-dimensional nonlinear systems; and its generalization error bound is asymptotically optimal.\n2\nFrom the perspective of statistical inference, some gaps remain toward taming sparse DNNs into\nthe framework of statistical modeling. This paper bridges the gap by establishing (i) asymptotic\nnormality of the connection weights, and (ii) asymptotic normality of the prediction.\n2.1\nPosterior consistency and structure selection consistency\nThis subsection provides a brief review of the sparse DNN theory developed in Sun et al. [2021]\nand gives the conditions that we will use in the followed theoretical developments. Without loss of\ngenerality, we let Dn = (x(i), y(i))i=1,...,n denote a dataset of n i.i.d observations, where x(i) ∈\nRpn and y(i) ∈R. Consider a generalized linear model with the distribution of y given by\nf(y|µ∗(x)) = exp{A(µ∗(x))y + B(µ∗(x)) + C(y)},\nwhere µ∗(x) is a nonlinear function of x and A(·), B(·) and C(·) are appropriately deﬁned functions.\nFor example, for normal regression, we have A(µ∗) = µ∗/σ2, B(µ∗) = −µ∗2/2σ2, C(y) =\n−y2/2σ2 −log(2πσ2)/2, and σ2 is a constant. We approximate µ∗(x) using a fully connected\nDNN with Hn −1 hidden layers. Let Lh denote the number of hidden units at layer h with LHn = 1\nfor the output layer and L0 = pn for the input layer. Let wh ∈RLh×Lh−1 and bh ∈RLh×1,\nh ∈{1, 2, ..., Hn} denote the weights and bias of layer h, and let ψh : RLh×1 →RLh×1 denote\na coordinate-wise and piecewise differentiable activation function of layer h. The DNN forms a\nnonlinear mapping\nµ(β, x) = wHnψHn−1 \u0002\n· · · ψ1 \u0002\nw1x + b1\u0003\n· · ·\n\u0003\n+ bHn,\n(1)\nwhere β = (w, b) =\n\b\nwh\nij, bh\nk : h ∈{1, . . . , Hn}, i, k ∈{1, . . . , Lh}, j ∈{1, . . . , Lh−1}\n\t\ndenotes the collection of all weights and biases, consisting of Kn = PHn\nh=1 (Lh−1 × Lh + Lh)\nelements. For convenience, we treat bias as a special connection and call each element in β a\nconnection weight. In order to represent the structure for a sparse DNN, we introduce an indicator\nvariable for each connection weight. Let γwh and γbh denote the indicator variables associated\nwith wh and bh, respectively. Let γ = {γwh\nij , γbh\nk\n: h ∈{1, . . . , Hn}, i, k ∈{1, . . . , Lh} , j ∈\n{1, . . . , Lh−1}}, which speciﬁes the structure of the sparse DNN. With slight abuse of notation, we\nwill also write µ(β, x) as µ(β, x, γ) to include the information of the network structure. We assume\nµ∗(x) can be well approximated by a parsimonious neural network with relevant variables, and call\nthis parsimonious network as the true DNN model. More precisely, we deﬁne the true DNN model\nas\n(β∗, γ∗) =\narg min\n(β,γ)∈Gn, ∥µ(β,γ,x)−µ∗(x)∥L2(Ω)≤̟n\n|γ|,\n(2)\nwhere Gn := G(C0, C1, ε, pn, Hn, L1, L2, . . . , LHn) denotes the space of valid sparse networks\nsatisfying condition A.2 (given below) for the given values of Hn, pn, and Lh’s, and ̟n is some\nsequence converging to 0 as n →∞. For any given DNN (β, γ), the error µ(β, γ, x) −µ∗(x)\ncan be generally decomposed as the network approximation error µ(β∗, γ∗, x) −µ∗(x) and the\nnetwork estimation error µ(β, γ, x) −µ(β∗, γ∗, x). The L2 norm of the former is bounded by ̟n,\nand the order of the latter will be given in Lemma 2.1. For the sparse DNN, we make the following\nassumptions:\nA.1 The input x is bounded by 1 entry-wisely, i.e. x ∈Ω= [−1, 1]pn, and the density of x is\nbounded in its support Ωuniformly with respect to n.\nA.2 The true sparse DNN model satisﬁes the following conditions:\nA.2.1 The network structure satisﬁes: rnHn log n +rn log L + sn log pn ≤C0n1−ε, where\n0 < ε < 1 is a small constant, rn = |γ∗| denotes the connectivity of γ∗, L =\nmax1≤j≤Hn−1 Lj denotes the maximum hidden layer width, sn denotes the input\ndimension of γ∗.\nA.2.2 The network weights are polynomially bounded: ∥β∗∥∞≤En, where En = nC1 for\nsome constant C1 > 0.\nA.3 The activation function ψ is Lipschitz continuous with a Lipschitz constant of 1.\n3\nRefer to Sun et al. [2021] for explanations and discussions on these assumptions. We let each con-\nnection weight and bias be subject to a mixture Gaussian prior, i.e.,\nwh\nij ∼λnN(0, σ2\n1,n) + (1 −λn)N(0, σ2\n0,n),\nbh\nk ∼λnN(0, σ2\n1,n) + (1 −λn)N(0, σ2\n0,n),\n(3)\nwhere λn ∈(0, 1) is the mixture proportion, σ2\n0,n is typically set to a very small number, while σ2\n1,n\nis relatively large.\nPosterior Consistency\nLet P ∗and E∗denote the respective probability measure and expectation\nwith respect to data Dn. Let d(p1, p2) denote the Hellinger distance between two densities p1(x, y)\nand p2(x, y). Let π(A | Dn) be the posterior probability of an event A.\nLemma 2.1. (Theorem 2.1 of Sun et al. [2021]) Suppose Assumptions A.1-A.3 hold. If the mix-\nture Gaussian prior (3) satisﬁes the conditions: λn = O(1/{Kn[nHn(Lpn)]τ}) for some con-\nstant τ\n> 0, En/{Hn log n + log L}1/2 ≲σ1,n ≲nα for some constant α > 0, and\nσ0,n ≲min\n\b\n1/{√nKn(n3/2σ1,0/Hn)Hn}, 1/{√nKn(nEn/Hn)Hn}\n\t\n, then there exists an er-\nror sequence ǫ2\nn = O(̟2\nn) + O(ζ2\nn) such that limn→∞ǫn = 0 and limn→∞nǫ2\nn = ∞, and the\nposterior distribution satisﬁes\nP ∗n\nπ[d(pβ, pµ∗) > 4ǫn|Dn] ≥2e−cnǫ2\nn\no\n≤2e−cnǫ2\nn,\nE∗\nDnπ[d(pβ, pµ∗) > 4ǫn|Dn] ≤4e−2cnǫ2\nn,\n(4)\nfor sufﬁciently large n, where c denotes a constant, ζ2\nn = [rnHn log n+rn log L+sn log pn]/n, pµ∗\ndenotes the underlying true data distribution, and pβ denotes the data distribution reconstructed by\nthe Bayesian DNN based on its posterior samples.\nStructure Selection Consistency\nThe DNN is generally nonidentiﬁable due to the symmetry of\nnetwork structure. For example, µ(β, γ, x) can be invariant if one permutes certain hidden nodes or\nsimultaneously changes the signs or scales of certain weights. As in Sun et al. [2021], we deﬁne a\nset of DNNs by Θ such that any possible DNN can be represented by one and only one DNN in Θ via\nnodes permutation, sign changes, weight rescaling, etc. Let ν(γ, β) ∈Θ be an operator that maps\nany DNN to Θ via appropriate weight transformations. To serve the purpose of structure selection in\nΘ, we consider the marginal inclusion posterior probability (MIPP) approach proposed in Liang et al.\n[2013]. For each connection, we deﬁne its MIPP by qi = R P\nγ ei|ν(γ,β)π(γ|β, Dn)π(β|Dn)dβ\nfor i = 1, 2, . . ., Kn, where ei|ν(γ,β) is the indicator of connection i. The MIPP approach is to\nchoose the connections whose MIPPs are greater than a threshold ˆq, i.e., setting ˆγ ˆq = {i : qi >\nˆq, i = 1, 2, . . . , Kn} as an estimator of γ∗∈Θ. Let A(ǫn) = {β : d(pβ, pµ∗) ≥ǫn} and deﬁne\nρ(ǫn) = max1≤i≤Kn\nR\nA(ǫn)c\nP\nγ |ei|ν(γ,β) −ei|ν(γ∗,β∗)|π(γ|β, Dn)π(β|Dn)dβ, which measures\nthe structure difference between the true and sampled models on A(ǫn)c. Then we have:\nLemma 2.2. (Theorem 2.2 of Sun et al. [2021]) If the conditions of Lemma 2.1 hold and ρ(ǫn) →0\nas n →∞and ǫn →0, then (i) max1≤i≤Kn{|qi −ei|ν(γ∗,β∗)|}\np→0; (ii) (sure screening) P(γ∗⊂\nˆγ ˆq)\np→1 for any pre-speciﬁed ˆq ∈(0, 1); (iii) (consistency) P(γ∗= ˆγ0.5)\np→1.\nLemma 2.2 implies consistency of variable selection for the true DNN model as deﬁned in (2).\n2.2\nAsymptotic Normality of Connection Weights\nIn this section, we establish the asymptotic normality of the network parameters and predictions.\nLet nln(β) = Pn\ni=1 log(pβ(xi, yi)) denote the log-likelihood function, and let π(β) denote the\ndensity of the mixture Gaussian prior (3). Let hi1,i2,...,id(β) denote the d-th order partial deriva-\ntives\n∂dln(β)\n∂βi1 ∂βi2 ···∂βid . Let Hn(β) denote the Hessian matrix of ln(β). Let hij(β) and hij(β)\ndenote the (i, j)-th component of Hn(β) and H−1\nn (β), respectively. Let ¯λn(β) and λn(β) de-\nnotes the maximum and minimum eigenvalue of the Hessian matrix Hn(β), respectively. Let\nBλ,n = ¯λ1/2\nn (β∗)/λn(β∗) and bλ,n =\np\nrn/nBλ,n, where rn is the connectivity of γ∗. For a\nDNN parameterized by β, we deﬁne the weight truncation at the true model γ∗: (βγ∗)i = βi\nfor i ∈γ∗and (βγ∗)i = 0 otherwise. For the mixture Gaussian prior (3), let Bδn(β∗) = {β :\n|βi −β∗\ni | < δn, ∀i ∈γ∗, |βi −β∗\ni | < 2σ0,n log(\nσ1,n\nλnσ0,n ), ∀i /∈γ∗}. We follow the deﬁnition of\nasymptotic normality in Castillo et al. [2015] and Wang and Rocková [2020]:\n4\nDeﬁnition 2.1. Denote by dβ the bounded Lipschitz metric for weak convergence and by φn the\nmapping φn : β →√n(g(β) −g∗). We say that the posterior distribution of the functional g(β) is\nasymptotically normal with the center g∗and variance G if dβ(π[· | Dn] ◦φ−1\nn , N(0, G)) →0 in\nP ∗-probability as n →∞. We will write this more compactly as π[· | Dn] ◦φ−1\nn\n⇝N(0, G).\nTheorem 2.1 establishes the asymptotic normality of ˜ν(β), where ˜ν(β) denotes a transformation of\nβ which is invariant with respect to µ(β, γ, x) while minimizing ∥˜ν(β) −β∗∥∞.\nTheorem 2.1. Assume the conditions of Lemma 2.2 hold with ρ(ǫn) = o( 1\nKn ) and C1 >\n2\n3 in\nCondition A.2.2. For some δn s.t.\nrn\n√n ≲δn ≲\n1\n3√nrn , let A(ǫn, δn) = {β : maxi∈γ∗|βi −β∗\ni | >\nδn, d(pβ, pµ∗) ≤ǫn}, where ǫn is the posterior contraction rate as deﬁned in Lemma 2.1. Assume\nthere exists some constants C > 2 and M > 0 such that\nC.1 β∗\n=\n(β∗\n1, β∗\n2, . . . , β∗\nKn) is generic Feng and Simon [2017], Fefferman [1994],\nmini∈γ∗|β∗\ni | > Cδn and π(A(ǫn, δn) | Dn) →0 as n →∞.\nC.2 |hi(β∗)| < M, |hj,k(β∗)| < M, |hj,k(β∗)| < M, |hi,j,k(β)| < M, |hl(β)| < M hold for\nany i, j, k ∈γ∗, l /∈γ∗and β ∈B2δn(β∗).\nC.3 sup\n\b\n|Eβ(aT U)3| : ∥βγ∗−β∗∥≤1.2bλ,n, ∥a∥= 1\n\t\n≤\n0.1\np\nn/rnλ2\nn(β∗)/¯λ1/2\nn (β∗)\nand Bλ,n = O(1), where U = Z −Eβγ∗(Z), Z denotes a random variable drawn from a\nneural network model parameterized by βγ∗, and Eβγ∗(Z) denotes the mean of Z.\nThen π[√n(˜ν(β) −β∗) | Dn] ⇝N(0, V ) in P ∗-probability as n →∞, where V = (vij), and\nvi,j = E(hi,j(β∗)) if i, j ∈γ∗and 0 otherwise.\nCondition C.1 is essentially an identiﬁability condition, i.e., when n is sufﬁciently large, the DNN\nweights cannot be too far away from the true weights if the DNN produces approximately the same\ndistribution as the true data. Condition C.2 gives typical conditions on derivatives of the DNN.\nCondition C.3 ensures consistency of the MLE of β∗for the given structure γ∗Portnoy [1988].\n2.2.1\nAsymptotic Normality of Prediction\nTheorem 2.2 establishes asymptotic normality of the prediction µ(β, x0) for a test data point x0,\nwhich implies that a faithful prediction interval can be constructed for the learnt sparse neural net-\nwork. Refer to Appendix A.4 for how to construct the prediction interval based on the theorem. Let\nµi1,i2,...,id(β, x0) denote the d-th order partial derivative\n∂dµ(β,x0)\n∂βi1∂βi2 ···∂βid .\nTheorem 2.2. Assume the conditions of Theorem 2.1 and the following condition hold:\n|µi(β∗, x0)| < M, |µi,j(β, x0)| < M, |µk(β, x0)| < M hold for any i, j ∈γ∗, k /∈γ∗and\nβ ∈B2δn(β∗), where M is as deﬁned in Theorem 2.1. Then π[√n(µ(β, x0)−µ(β∗, x0)) | Dn] ⇝\nN(0, Σ), where Σ = ∇γ∗µ(β∗, x0)T H−1∇γ∗µ(β∗, x0) and H = E(−∇2\nγ∗ln(β∗)) is the Fisher\ninformation matrix.\nThe asymptotic normality for general smooth functional has been established in Castillo et al. [2015].\nFor linear and quadratic functional of deep ReLU network with a spike-and-slab prior, the asymp-\ntotic normality has been established in Wang and Rocková [2020]. The DNN prediction µ(β, x0)\ncan be viewed as a point evaluation functional over the neural network function space. However,\nin general, this functional is not smooth with respect to the locally asymptotic normal (LAN) norm.\nThe results of Castillo et al. [2015] and Wang and Rocková [2020] are not directly applicable for the\nasymptotic normality of µ(β, x0).\n3\nPrior Annealing Algorithms for Sparse DNN Computation\nAs implied by Theorems 2.1 and 2.2, a consistent estimator of (γ∗, β∗) is essential for statistical\ninference of the sparse DNN. Toward this goal, Sun et al. [2021] proved that the marginal inclusion\nprobabilities qi’s can be estimated using Laplace approximation at the mode of the log-posterior.\nBased on this result, they proposed a multiple-run procedure. In each run, they ﬁrst maximize the\nlog-posterior by an optimization algorithm, such as SGD or Adam; then sparsify the DNN structure\n5\nby truncating the weights less than a threshold to zero, where the threshold is calculated from the\nprior (3) based on the Laplace approximation theory; and then reﬁne the weights of the sparsiﬁed\nDNN by running an optimization algorithm for a few iterations. Finally, they select a sparse DNN\nmodel from those obtained in the multiple runs according to their Bayesian evidence or BIC values.\nThe BIC is suggested when the size of the sparse DNN is large.\nAlthough the multiple-run procedure works well for many problems, it is hard to justify that it will\nlead to a consistent estimator of the true model (γ∗, β∗). To tackle this issue, we propose two prior\nannealing algorithms, one from the frequentist perspective and one from the Bayesian perspective.\n3.1\nPrior Annealing: Frequentist Computation\nIt has been shown in Nguyen and Hein [2017], Gori and Tesi [1992] that the loss of an over-\nparameterized DNN exhibits good properties:\n(S∗) For a fully connected DNN with an analytic activation function and a convex loss function\nat the output layer, if the number of hidden units of one layer is larger than the number of\ntraining points and the network structure from this layer on is pyramidal, then almost all\nlocal minima are globally optimal.\nMotivated by this result, we propose a prior annealing algorithm, which is immune to local traps\nand aims to ﬁnd a consistent estimate of (β∗, γ∗) as deﬁned in (2). The detailed procedure of the\nalgorithm is given in Algorithm 1.\nAlgorithm 1 Prior annealing: Frequentist\n(i) (Initial training) Train a DNN satisfying condition (S*) such that a global optimal solu-\ntion β0 = arg maxβ ln(β) is reached, which can be accomplished using SGD or Adam\nKingma and Ba [2015].\n(ii) (Prior annealing) Initialize β at β0 and simulate from a sequence of distributions\nπ(β|Dn, τ, η(k), σ(k)\n0,n) ∝enln(β)/τπη(k)/τ\nk\n(β) for k = 1, 2, . . . , m, where 0 < η(1) ≤\nη(2) ≤· · · ≤η(m) = 1, πk = λnN(0, σ2\n1,n) + (1 −λn)N(0, (σ(k)\n0,n)2), and σinit\n0,n\n=\nσ(1)\n0,n ≥σ(2)\n0,n ≥· · · ≥σ(m)\n0,n = σend\n0,n . The simulation can be done in an annealing manner\nusing a stochastic gradient MCMC algorithm [Welling and Teh, 2011, Chen et al., 2014,\nMa et al., 2015, Nemeth and Fearnhead, 2019]. After the stage m has been reached, con-\ntinue to run the simulated annealing algorithm by gradually decreasing the temperature τ\nto a very small value. Denote the resulting DNN by ˆβ = (ˆβ1, ˆβ2, . . . , ˆβKn).\n(iii) (Structure sparsiﬁcation) For each connection i ∈{1, 2, . . ., Kn}, set ˜γi = 1 if |ˆβi| >\n√\n2σ0,nσ1,n\n√\nσ2\n1,n−σ2\n0,n\nr\nlog\n\u0010\n1−λn\nλn\nσ1,n\nσ0,n\n\u0011\nand 0 otherwise, where the threshold value of |ˆβi| is ob-\ntained by solving π(γi = 1|βi) > 0.5 based on the mixture Gaussian prior as in Sun et al.\n[2021]. Denote the yielded sparse DNN structure by ˜γ.\n(iv) (Nonzero-weights reﬁning) Reﬁne the nonzero weights of the sparsiﬁed DNN by maximiz-\ning ln(β). Denote the resulting estimate by ˜β˜γ, which represents the MLE of β∗.\nFor Algorithm 1, the consistency of (˜γ, ˜β˜γ) as an estimator of (γ∗, β∗) can be proved based on\nTheorem 3.4 of Nguyen and Hein [2017] for global convergence of β0, the property of simulated\nannealing (by choosing an appropriate sequence of ηk and a cooling schedule of τ), Theorem 2.2\nfor consistency of structure selection, Theorem 2.3 of Sun et al. [2021] for consistency of structure\nsparsiﬁcation, and Theorem 2.1 of Portnoy [1988] for consistency of MLE under the scenario of\ndimension diverging. Then we can construct the conﬁdence intervals for neural network predictions\nusing (˜γ, ˜β˜γ). The detailed procedure is given in supplementary material.\nIntuitively, the initial training phase can reach the global optimum of the likelihood function. In the\nprior annealing phase, as we slowly add the effect of the prior, the landscape of the target distribu-\ntion is gradually changed and the MCMC algorithm is likely to hit the region around the optimum\n6\nof the target distribution. More explanations on the effect of the prior can be found in the supple-\nmentary material. In practice, let t denote the step index, a simple implementation of the initial\ntraining and prior annealing phases of Algorithm 1 can be given as follows: (i) for 0 < t < T1,\nrun initial training; (ii) for T1 ≤t ≤T2, ﬁx σ(t)\n0,n = σinit\n0,n and linearly increase ηt by setting\nη(t) =\nt−T1\nT2−T1 ; (iii) for T2 ≤t ≤T3, ﬁx η(t) = 1 and linearly decrease\n\u0010\nσ(t)\n0,n\n\u00112\nby setting\n\u0010\nσ(t)\n0,n\n\u00112\n=\nT3−t\nT3−T2\n\u0000σinit\n0,n\n\u00012 +\nt−T2\nT3−T2\n\u0000σend\n0,n\n\u00012; (iv) for t > T3, ﬁx η(t) = 1 and σ(t)\n0,n = σend\n0,n and\ngradually decrease the temperature τ, e.g., setting τt =\nc\nt−T3 for some constant c.\n3.2\nPrior Annealing: Bayesian Computation\nFor certain problems the size (or #nonzero elements) of γ∗is large, calculation of the Fisher infor-\nmation matrix is difﬁcult. In this case, the prediction uncertainty can be quantiﬁed via posterior\nsimulations. The simulation can be started with a DNN satisfying condition (S*) and performed us-\ning a SGMCMC algorithm Ma et al. [2015], Nemeth and Fearnhead [2019] with an annealed prior\nas deﬁned in step (ii) of Algorithm 1 (For Bayesian approach, we may ﬁx the temperature τ = 1).\nThe over-parameterized structure and annealed prior make the simulations immune to local traps.\nTo justify the Bayesian estimator for the prediction mean and variance, we study the deviation of the\npath averaging estimator 1\nT\nPT\nt=1 φ(β(t)) and the posterior mean\nR\nφ(β)π(β|Dn, η∗, σ∗\n0,n)dβ for\nsome test function φ(β). For simplicity, we will focus on SGLD with prior annealing. Our analysis\ncan be easily generalized to other SGMCMC algorithms Chen et al. [2015].\nFor a test function φ(·), the difference between φ(β) and\nR\nφ(β)π(β|Dn, η∗, σ∗\n0,n)dβ can be char-\nacterized by the Poisson equation:\nLψ(β) = φ(β) −\nZ\nφ(β)π(β|Dn, η∗, σ∗\n0,n)dβ,\nwhere ψ(·) is the solution of the Poisson equation and L is the inﬁnitesimal generator of the Langevin\ndiffusion. i.e. for the following Langevin diffusion dβ(t) = ∇log(π(β|Dn, η∗, σ∗\n0,n))dt+\n√\n2IdWt,\nwhere I is identity matrix and Wt is Brownian motion, we have\nLψ(β) := ⟨∇ψ(β), ∇log(π(β|Dn, η∗, σ∗\n0,n)) + tr(∇2ψ(β)).\nLet Dkψ denote the kth-order derivatives of ψ. To control the perturbation of φ(β), we need the\nfollowing assumption about the function ψ(β):\nAssumption 3.1. For k ∈{0, 1, 2, 3}, Dkψ exists and there exists a function V, s.t. ||Dkψ|| ≲Vpk\nfor some constant pk > 0. In addition, V is smooth and the expectation of Vp on β(t) is bounded for\nsome p ≤2 maxk{pk}, i.e. supt E(Vp(β(t))) < ∞, P\ns∈(0,1) Vp(sβ1 + (1 −s)β2) ≲Vp(β1) +\nVp(β2).\nIn step t of the SGLD algorithm, the drift term is replaced by ∇β log π(β(t)|D(t)\nm,n, η(t), σ(t)\n0,n), where\nD(t)\nm,n is used to represent the mini-batch data used in step t. Let Lt be the corresponding inﬁnitesi-\nmal generator. Let δt = Lt −L. To quantify the effect of δt, we introduce the following assumption:\nAssumption 3.2. β(t) has bounded expectation and the expectation of log-prior is Lipschitz contin-\nuous with respect to σ0,n, i.e. there exists some constant M s.t. supt E(|β(t)|) ≤M < ∞. For all t,\n|E log(π(β(t)|λn, σ(t1)\n0,n , σ1,n)) −E log(π(β(t)|λn, σ(t2)\n0,n , σ1,n))| ≤M|σ(t1)\n0,n −σ(t2)\n0,n |.\nThen we have the following theorem:\nTheorem 3.1. Suppose the model satisfy assumption 3.2, and a constant learning rate of ǫ is used.\nFor a test function φ(·), if the solution of the Poisson equation ψ(·) satisfy assumption 3.1, then\nE\n \n1\nT\nT −1\nX\nt=1\nφ(β(t)) −\nZ\nφ(β)π(β|Dn, η∗, σ∗\n0,n)dβ\n!\n= O\n \n1\nT ǫ +\nPT −1\nt=0 (|η(t) −η∗| + |σ(t)\n0,n −σ∗\n0,n|)\nT\n+ ǫ\n!\n,\n(5)\nwhere σ∗\n0,n is treated as a ﬁxed constant.\n7\nTheorem 3.1 shows that with prior annealing, the path averaging estimator can still be used for\nestimating the mean and variance of the prediction and constructing the conﬁdence interval. The\ndetailed procedure is given in supplementary material. For the case that a decaying learning rate is\nused, a similar theorem can be developed as in Chen et al. [2015].\n4\nNumerical Experiments\nThis section illustrates the performance of the proposed method on synthetic and real data exam-\nples.2 For the synthetic example, the frequentist algorithm is employed to construct prediction\nintervals. The real data example involves a large network, so both the frequentist and Bayesian\nalgorithms are employed along with comparisons with some existing network pruning methods.\n4.1\nSynthetic Example\nWe consider a high-dimensional nonlinear regression problem, which shows that our method can\nidentify the sparse network structure and relevant features as well as produce prediction intervals\nwith correct coverage rates. The datasets were generated as in Sun et al. [2021], where the explana-\ntory variables x1, . . . , xpn were simulated by independently generating e, z1, . . . , zpn from N(0, 1)\nand setting xi = e+zi\n√\n2 . The response variable was generated from a nonlinear regression model:\ny =\n5x2\n1 + x2\n1\n+ 5 sin(x3x4) + 2x5 + 0x6 + · · · + 0x2000 + ǫ,\nwhere ǫ ∼N(0, 1). Ten datasets were generated, each consisting of 10000 samples for training and\n1000 samples for testing. This example was taken from Sun et al. [2021], through which we show\nthat the prior annealing method can achieve similar results with the multiple-run method proposed\nin Sun et al. [2021].\nWe modeled the data by a DNN of structure 2000-10000-100-10-1 with tanh activation function.\nHere we intentionally made the network very wide in one hidden layer to satisfy the condition\n(S*). Algorithm 1 was employed to learn the model. The detailed setup for the experiments were\ngiven in the supplementary material. The variable selection performance were measured using the\nfalse selection rate FSR =\nP10\ni=1 | ˆSi\\S|\nP10\ni=1 | ˆSi|\nand negative selection rate NSR =\nP10\ni=1 |S\\ ˆSi|\nP10\ni=1 |S| , where\nS is the set of true variables, ˆSi is the set of selected variables from dataset i and | ˆSi| is the size\nof ˆSi. The predictive performance is measured by mean square prediction error (MSPE) and mean\nsquare ﬁtting error (MSFE). We compare our method with the multiple-run method (BNN_evidence)\nSun et al. [2021] as well as other existing variable selection methods including Sparse input neu-\nral network(Spinn) Feng and Simon [2017], Bayesian adaptive regression tree (BART) Bleich et al.\n[2014], linear model with lasso penalty (LASSO) Tibshirani [1996], and sure independence screen-\ning with SCAD penalty (SIS)Fan and Lv [2008]. To demonstrate the importance of selecting correct\nvariables, we also compare our method with two dense model with the same network structure: DNN\ntrained with dropout(Dropout) and DNN trained with no regularization(DNN). Detailed setups for\nthese methods were given in the supplementary material as well. The results were summarized in\nTable 1. With a single run, our method BNN_anneal achieves similar result with the multiple-run\nmethod. The latter trained the model for 10 times and selected the best one using Bayesian evidence.\nWhile for Spinn (with LASSO penalty), even with over-parametrized structure, it performs worse\nthan the sparse BNN model.\nTo quantify the uncertainty of the prediction, we conducted 100 experiments over different training\nsets as generated previously. We constructed 95% prediction intervals over 1000 test points. Over\nthe 1000 test points, the average coverage rate of the prediction intervals is 94.72%(0.61%), where\n(0.61%) denote the standard deviation. Figure 1 shows the prediction intervals constructed for 20 of\nthe testing points. Refer to the supplementary material for the detail of the computation.\n4.2\nReal Data Example\nAs a different type of applications of the proposed method, we conducted unstructured network\npruning experiments on CIFAR10 datasetKrizhevsky et al. [2009]. Following the setup in Lin et al.\n2The code for running these experiments can be found in https://github.com/sylydya/Sparse-Deep-Learning-A-New-Framewor\n8\nTable 1: Simulation Result: MSFE and MSPE were calculated by averaging over 10 datasets, and\ntheir standard deviations were given in the parentheses.\nMethod\n| ˆS|\nFSR\nNSR\nMSFE\nMSPE\nBNN_anneal\n5(0)\n0\n0\n2.353(0.296)\n2.428(0.297)\nBNN_Evidence\n5(0)\n0\n0\n2.372(0.093)\n2.439(0.132)\nSpinn\n10.7(3.874)\n0.462\n0\n4.157(0.219)\n4.488(0.350)\nDNN\n-\n-\n-\n1.1701e-5(1.1542e-6)\n16.9226(0.3230)\nDropout\n-\n-\n-\n1.104(0.068)\n13.183(0.716)\nBART50\n16.5(1.222)\n0.727\n0.1\n11.182(0.334)\n12.097(0.366)\nLASSO\n566.8(4.844)\n0.993\n0.26\n8.542(0.022)\n9.496(0.148)\nSIS\n467.2(11.776)\n0.991\n0.2\n7.083(0.023)\n10.114(0.161)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nindex\n−10\n−5\n0\n5\n10\nY\nConfidence Interval\nTrue Value\nPredicted Value\nFigure 1: Prediction intervals of 20 testing points, where the y-axis is the response value, the x-axis\nis the index, and the blue point represents the true observation.\n[2020], we train the residual networkHe et al. [2016] with different networks size and pruned the net-\nwork to different sparsity levels. The detailed experimental setup can be found in the supplementary\nmaterial.\nWe compared the proposed methods, BNN_anneal (Algorithm 1) and BNN_average (averaged\nover last 75 networks simulated by the Bayesian version of the prior annealing algorithm), with\nseveral state-of-the-art unstructured pruning methods, including Consistent Sparse Deep Learn-\ning (BNN_BIC) Sun et al. [2021], Dynamic pruning with feedback (DPF) Lin et al. [2020], Dy-\nnamic Sparse Reparameterization (DSR) Mostafa and Wang [2019] and Sparse Momentum (SM)\nDettmers and Zettlemoyer [2019]. The results of the baseline methods were taken from Lin et al.\n[2020] and Sun et al. [2021]. The results of prediction accuracy for different models and target spar-\nsity levels were summarized in Table 2. Due to the threshold used in step (iii) of Algorithm 1, it is\nhard for our method to make the pruning ratio exactly the same as the targeted one. We intentionally\nmake the pruning ratio smaller than the target ratio, while our method still achieve better test accu-\nracy. Compared to BNN_BIC, the test accuracy is very close, but the result of BNN_BIC is obtained\nby running the experiment 10 times while our method only run once. To further demonstrate that the\nproposed method result in better model calibration, we followed the setup of Maddox et al. [2019]\nand compared the proposed method with DPF on several metrics designed for model calibration,\nincluding negtive log likelihood (NLL), symmetrized, discretized KL distance between in and out of\nsample entropy distributions (JS-Distance), and expected calibration error (ECE). For JS-Distance,\nwe used the test data of SVHN data set3 as out-of-distribution samples. The results were summarized\nin Table 3. As discussed in Maddox et al. [2019], Guo et al. [2017], a well calibrated model tends\nto have smaller NLL, larger JS-Distance and smaller ECE. The comparison shows that the proposed\n3The Street View House Numbers (SVHN) Dataset: http://ufldl.stanford.edu/housenumbers/\n9\nTable 2: ResNet network pruning results for CIFAR-10 data, which were calculated by averaging\nover 3 independent runs with the standard deviation reported in the parentheses.\nResNet-20\nResNet-32\nMethod\nPruning Ratio\nTest Accuracy\nPruning Ratio\nTest Accuracy\nDNN_dense\n100%\n92.93(0.04)\n100%\n93.76(0.02)\nBNN_average\n19.85%(0.18%)\n92.53(0.08)\n9.99%(0.08%)\n93.12(0.09)\nBNN_anneal\n19.80%(0.01%)\n92.30(0.16)\n9.97%(0.03%)\n92.63(0.09)\nBNN_BIC\n19.67%(0.05%)\n92.27(0.03)\n9.53%(0.04%)\n92.74(0.07)\nSM\n20%\n91.54(0.16)\n10%\n91.54(0.18)\nDSR\n20%\n91.78(0.28)\n10%\n91.41(0.23)\nDPF\n20%\n92.17(0.21)\n10%\n92.42(0.18)\nBNN_average\n9.88%(0.02%)\n91.65(0.08)\n4.77%(0.08%)\n91.30(0.16)\nBNN_anneal\n9.95%(0.03%)\n91.28(0.11)\n4.88%(0.02%)\n91.17(0.08)\nBNN_BIC\n9.55%(0.03%)\n91.27(0.05)\n4.78%(0.01%)\n91.21(0.01)\nSM\n10%\n89.76(0.40)\n5%\n88.68(0.22)\nDSR\n10%\n87.88(0.04)\n5%\n84.12(0.32)\nDPF\n10%\n90.88(0.07)\n5%\n90.94(0.35)\nTable 3: ResNet network pruning results for CIFAR-10 data, which were calculated by averaging\nover 3 independent runs with the standard deviation reported in the parentheses.\nMethod\nModel\nPruning Ratio\nNLL\nJS-Distance\nECE\nDNN_dense\nResNet20\n100%\n0.2276(0.0021)\n7.9118(0.9316)\n0.02627(0.0005)\nBNN_average\nResNet20\n9.88%(0.02%)\n0.2528(0.0029)\n9.9641(0.3069)\n0.0113(0.0010)\nBNN_anneal\nResNet20\n9.95%(0.03%)\n0.2618(0.0037)\n10.1251(0.1797)\n0.0175(0.0011)\nDPF\nResNet20\n10%\n0.2833(0.0004)\n7.5712(0.4466)\n0.0294(0.0009)\nBNN_average\nResNet20\n19.85%(0.18%)\n0.2323(0.0033)\n7.7007(0.5374)\n0.0173(0.0014)\nBNN_anneal\nResNet20\n19.80%(0.01%)\n0.2441(0.0042)\n6.4435(0.2029)\n0.0233(0.0020)\nDPF\nResNet20\n20%\n0.2874(0.0029)\n7.7329(0.1400)\n0.0391(0.0001)\nDNN_dense\nResNet32\n100%\n0.2042(0.0017)\n6.7699(0.5253)\n0.02613(0.00029)\nBNN_average\nResNet32\n9.99%(0.08%)\n0.2116(0.0012)\n9.4549(0.5456)\n0.0132(0.0001)\nBNN_anneal\nResNet32\n9.97%(0.03%)\n0.2218(0.0013)\n8.5447(0.1393)\n0.0192(0.0009)\nDPF\nResNet32\n10%\n0.2677(0.0041)\n7.8693(0.1840)\n0.0364(0.0015)\nBNN_average\nResNet32\n4.77%(0.08%)\n0.2587(0.0022)\n7.0117(0.2222)\n0.0100(0.0002)\nBNN_anneal\nResNet32\n4.88%(0.02%)\n0.2676(0.0014)\n6.8440(0.4850)\n0.0149(0.0006)\nDPF\nResNet32\n5%\n0.2921(0.0067)\n6.3990(0.8384)\n0.0276(0.0019)\nmethod outperforms DPF in most cases. In addition to the network pruning method, we also train\na dense model with the standard training set up. Compared to the dense model, the sparse network\nhas worse accuracy, but it tends to outperform the dense network in terms of ECE and JS-Distance,\nwhich indicates that sparsiﬁcation is also a useful way for improving calibration of the DNN.\n5\nConclusion\nThis work, together with Sun et al. [2021], has built a solid theoretical foundation for sparse deep\nlearning, which has successfully tamed the sparse deep neural network into the framework of sta-\ntistical modeling. As implied by Lemma 2.1, Lemma 2.2, Theorem 2.1, and Theorem 2.2, the\nsparse DNN can be simply viewed as a nonlinear statistical model which, like a traditional statistical\nmodel, possesses many nice properties such as posterior consistency, variable selection consistency,\nand asymptotic normality. We have shown how the prediction uncertainty of the sparse DNN can\nbe quantiﬁed based on the asymptotic normality theory, and provided algorithms for training sparse\nDNNs with theoretical guarantees for its convergence to the global optimum. The latter ensures the\nvalidity of the down-stream statistical inference.\n10\nAcknowledgment\nFunding in direct support of this work: NSF grant DMS-2015498, NIH grants R01-GM117597 and\nR01-GM126089, and Liang’s startup fund at Purdue University.\nReferences\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\nparameterization. In ICML, 2019.\nJustin Bleich, Adam Kapelner, Edward I George, and Shane T Jensen. Variable selection for bart:\nan application to gene regulation. The Annals of Applied Statistics, pages 1750–1781, 2014.\nHelmut Bölcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approximation with\nsparsely connected deep neural networks. CoRR, abs/1705.01714, 2019.\nIsmaël Castillo and Judith Rousseau. Supplement to “a bernstein–von mises theorem for smooth\nfunctionals in semiparametric models”. Annals of Statistics, 43(6):2353–2383, 2015.\nIsmaël Castillo, Judith Rousseau, et al. A bernstein–von mises theorem for smooth functionals in\nsemiparametric models. The Annals of Statistics, 43(6):2353–2383, 2015.\nChangyou Chen, Nan Ding, and Lawrence Carin. On the convergence of stochastic gradient mcmc\nalgorithms with high-order integrators. In Proceedings of the 28th International Conference on\nNeural Information Processing Systems-Volume 2, pages 2278–2286, 2015.\nTianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In\nInternational conference on machine learning, pages 1683–1691, 2014.\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing\nperformance. arXiv preprint arXiv:1907.04840, 2019.\nSimon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds\nglobal minima of deep neural networks. In ICML, 2019.\nJianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849–911, 2008.\nCharles Fefferman. Reconstructing a neural net from its output. Revista Matemática Iberoameri-\ncana, 10(3):507–555, 1994.\nJean Feng and Noah Simon. Sparse-input neural networks for high-dimensional nonparametric\nregression and classiﬁcation. arXiv preprint arXiv:1711.07592, 2017.\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model un-\ncertainty in deep learning. In Proceedings of the 33rd International Conference on International\nConference on Machine Learning - Volume 48, ICML’16, page 1050–1059. JMLR.org, 2016.\nM. Gori and A. Tesi. On the problem of local minima in backpropagation. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 14(1):76–86, 1992.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\nnetworks. In Proceedings of the 34th International Conference on Machine Learning - Volume\n70, ICML’17, page 1321–1330. JMLR.org, 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n770–778, 2016.\nD.P. Kingma and J.L. Ba. Adam: a method for stochastic optimization. In International Conference\non Learning Representations, 2015.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nTechnical report, Citeseer, 2009.\n11\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive\nuncertainty estimation using deep ensembles. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, NIPS’17, page 6405–6416, Red Hook, NY, USA,\n2017. Curran Associates Inc. ISBN 9781510860964.\nF. Liang, Q. Song, and K. Yu. Bayesian subset modeling for high dimensional generalized linear\nmodels. Journal of the American Statistical Association, 108:589–606, 2013.\nF. Liang, Q. Li, and L. Zhou. Bayesian neural networks for selection of drug sensitive genes. Journal\nof the American Statistical Association, 113(523):955–972, 2018.\nTao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi.\nDynamic model\npruning with feedback. In International Conference on Learning Representations, 2020. URL\nhttps://openreview.net/forum?id=SJem8lSFwB.\nYi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. In\nAdvances in Neural Information Processing Systems, pages 2917–2925, 2015.\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson.\nA simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information\nProcessing Systems, pages 13153–13164, 2019.\nHesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural networks\nby dynamic sparse reparameterization. In International Conference on Machine Learning, pages\n4646–4655, 2019.\nChristopher Nemeth and Paul Fearnhead. Stochastic gradient markov chain monte carlo. arXiv\npreprint arXiv:1907.06986, 2019.\nQuynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In ICML,\n2017.\nNicholas G. Polson and Veronika Roˇcková. Posterior concentration for sparse deep learning. In\nProceedings of the 32nd International Conference on Neural Information Processing Systems,\nNIPS’18, page 938–949, Red Hook, NY, USA, 2018. Curran Associates Inc.\nS. Portnoy. Asymptotic behavior of likelihood methods for exponential families when the number\nof parameters tend to inﬁnity. The Annals of Statistics, 16(1):356–366, 1988.\nJohannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation\nfunction. arXiv:1708.06633, 2017.\nY. Sun, Q. Song, and F. Liang. Consistent sparse deep learning: Theory and computation. Journal\nof the American Statistical Association, page in press, 2021.\nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical\nSociety. Series B (Methodological), pages 267–288, 1996.\nYuexi Wang and V. Rocková. Uncertainty quantiﬁcation for sparse deep learning. In AISTATS, 2020.\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In\nProceedings of the 28th international conference on machine learning (ICML-11), pages 681–\n688, 2011.\nDifan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural\nnetworks. In NuerIPS, 2019.\nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.\nGradient descent optimizes over-\nparameterized deep relu networks. Machine Learning, 109:467 – 492, 2020.\n12\nA\nSupplementary material of \"Sparse Deep Learning: A New Framework\nImmune to Local Traps and Miscalibration\"\nA.1\nProof of Theorem 2.1\nProof. We ﬁrst deﬁne the equivalent class of neural network parameters. Given a parameter vector\nβ and the corresponding structure parameter vector γ, its equivalent class is given by\nQE(β, γ) = {(˜β, ˜γ) : νg(˜β, ˜γ) = (β, γ), µ(˜β, ˜γ, x) = µ(β, γ, x), ∀x},\nwhere νg(·) denotes a generic mapping that contains only the transformations of node permutation\nand weight sign ﬂipping. Speciﬁcally, we deﬁne\nQ∗\nE = QE(β∗, γ∗),\nwhich represents the equivalent class of true DNN model.\nLet Bδn(β∗) = {β : |βi −β∗\ni | < δn, ∀i ∈γ∗, |βi −β∗\ni | < 2σ0,n log(\nσ1,n\nλnσ0,n ), ∀i /∈γ∗}. By\nassumption C.1, β∗is generic (i.e. QE(β∗) contains only reparameterizations of weight sign-\nﬂipping or node permutations as deﬁned in Feng and Simon [2017] and Fefferman [1994]) and\nmini∈γ∗|β∗\ni |−δn > (C−1)δn > δn, then for any β∗(1), β∗(2) ∈Q∗\nE, Bδn(β∗(1))∩Bδn(β∗(2)) = ∅,\nand thus {β : ˜ν(β) ∈Bδn(β∗)} = ∪β∈Q∗\nEBδn(β).\nIn what follows, we will ﬁrst show\nπ(∪β∈Q∗\nEBδn(β) | Dn) →1 as n →∞, which means the most posterior mass falls in the neigh-\nbourhood of true parameter.\nRemark on the notation: ˜ν(·) is similar to ν(·) deﬁned in Section 2.1 of the main text. They both\nmap the set QE(β, γ) to a unique network. The difference between them is that ∥ν(β)−β∗∥∞may\nbe arbitrary, but ∥˜ν(β)−β∗∥∞is minimized. In other words, ν(β, γ) is to ﬁnd an arbitrary network\nin QE(β, γ) as the representative of the equivalent class, while ˜ν(β, γ) is to ﬁnd a representative in\nQE(β, γ) such that the distance between β∗and the representative is minimized. In what follows,\nwe will use ˜ν(β) and ˜ν(γ) to denote the connection weight and network structure of ˜ν(β, γ), re-\nspectively. With a slight abuse of notation, we will use ˜ν(β)i to denote the ith component of ˜ν(β),\nand use ˜ν(γ)i to denote the ith component of ˜ν(γ).\nRecall that the marginal posterior inclusion probability is given by\nqi =\nZ X\nγ\nei|˜ν(β,γ)π(γ|β, Dn)π(β|Dn)dβ =\nZ\nπ(˜ν(γ)i = 1|β)π(β|Dn)dβ.\nFor the mixture Gaussian prior,\nπ(γi = 1|β) =\n1\n1 + σ1,n(1−λn)\nσ0,nλn\ne\n−(\n1\n2σ2\n0,n\n−\n1\n2σ2\n1,n\n)β2\ni ,\nwhich increases with respect to |βi|. In particular, if |βi| > 2σ0,n log(\nσ1,n\nλnσ0,n ), then π(γi = 1|β) >\n1\n2.\nFor the mixture Gaussian prior,\nπ(β /∈∪β∈Q∗\nEBδn(β) | Dn)\n≤π(∃i /∈γ∗, |˜ν(β)i| > 2σ0,n log( σ1,n\nλnσ0,n\n) | Dn) + π(∃i ∈γ∗, |˜ν(β)i −β∗\ni | > δn | Dn).\nFor the ﬁrst term, note that for a given i /∈γ∗,\nπ(|˜ν(β)i| > 2σ0,n log( σ1,n\nλnσ0,n\n) | Dn) ≤π(π(˜ν(γ)i = 1|β) > 1\n2 | Dn)\n≤2\nZ\nπ(˜ν(γ)i = 1|β)π(β|Dn)dβ\n≤2ρ(ǫn) + 2π(d(pβ, pµ∗) ≥ǫn | Dn) →0.\n13\nThen we have\nπ(∃i /∈γ∗, |˜ν(β)i| > 2σ0,n log( σ1,n\nλnσ0,n\n) | Dn) =π(max\ni/∈γ∗|˜ν(β)i| > 2σ0,n log( σ1,n\nλnσ0,n\n) | Dn)\n≤π(max\ni/∈γ∗π(˜ν(γ)i = 1|β) > 1\n2 | Dn)\n≤\nX\ni/∈γ∗\nπ(π(˜ν(γ)i = 1|β) > 1\n2 | Dn)\n≤2Knρ(ǫn) + 2Knπ(d(pβ, pµ∗) ≥ǫn | Dn) →0.\nFor the second term, by condition C.1 and Lemma 2.1,\nπ(∃i ∈γ∗, |˜ν(β)i −β∗\ni | > δn | Dn) = π(max\ni∈γ∗|˜ν(β)i −β∗\ni | > δn | Dn)\n=π(max\ni∈γ∗|˜ν(β)i −β∗\ni | > δn, d(pβ, pµ∗) ≤ǫn | Dn)\n+ π(max\ni∈γ∗|˜ν(β)i −β∗\ni | > δn, d(pβ, pµ∗) ≥ǫn | Dn)\n≤π(A(ǫn, δn) | Dn) + π(d(pβ, pµ∗) ≥ǫn | Dn) →0.\nSummarizing the above two terms, we have π(∪β∈Q∗\nEBδn(β) | Dn) →1.\nLet Qn = |Q∗\nE| be the number of equivalent true DNN model. By the generic assumption of β∗, for\nany β∗(1), β∗(2) ∈Q∗\nE, Bδn(β∗(1)) ∩Bδn(β∗(2)) = ∅. Then in Bδn(β∗), the posterior density of\n˜ν(β) is Qn times the posterior density of β. Then for a function f(·) of ˜ν(β), by changing variable,\nZ\n˜ν(β)∈Bδn(β∗)\nf(˜ν(β))π(˜ν(β)|Dn)d˜ν(β) = Qn\nZ\nBδn(β∗)\nf(β)π(β|Dn)dβ.\nThus, we only need to consider the integration on Bδn(β∗). Deﬁne\nˆβi =\n(\nβ∗\ni −P\nj∈γ∗hi,j(β∗)hj(β∗),\n∀i ∈γ∗,\n0,\n∀i ̸∈γ∗.\nWe will ﬁrst prove that for any real vector t,\nE(e\n√ntT (˜ν(β)−ˆβ) | Dn, Bδn(β∗)) :=\nR\nBδn(β∗) e\n√ntT (˜ν(β)−ˆβ)π(˜ν(β)|Dn)d˜ν(β)\nR\nBδn(β∗) π(˜ν(β)|Dn)d˜ν(β)\n=\nR\nBδn(β∗) e\n√ntT (β−ˆβ)enln(β)π(β)dβ\nR\nBδn(β∗) enln(β)π(β)dβ\n=e\n1\n2 tT V t+oP ∗(1).\n(6)\nFor any β ∈Bδn(β∗), we have\n|√n(tT (β −βγ∗))| ≤√nKn||t||∞2σ0,n log( σ1,n\nλnσ0,n\n) = o(1),\n|n(ln(β) −ln(βγ∗))| = |n\nX\ni/∈γ∗\nβi(hi(˜β))| ≤nKnM2σ0,n log( σ1,n\nλnσ0,n\n) = o(1).\nThen, we have\n√ntT (β −ˆβ) =√ntT (β −βγ∗+ βγ∗−β∗) + √n\nX\ni,j∈γ∗\nhi,j(β∗)tjhi(β∗))\n=o(1) + √n\nX\ni∈γ∗\n(βi −β∗\ni )ti + √n\nX\ni,j∈γ∗\nhi,j(β∗)tjhi(β∗),\n(7)\n14\nnln(β) −nln(β∗) =n(ln(β) −ln(βγ∗) + ln(βγ∗) −nln(β∗))\n=o(1) + n\nX\ni∈γ∗\n(βi −β∗\ni )hi(β∗) + n\n2\nX\ni,j∈γ∗\nhi,j(β∗)(βi −β∗\ni )(βj −β∗\nj)\n+ n\n6\nX\ni,j,k∈γ∗\nhi,j,k(ˇβ)(βi −β∗\ni )(βj −β∗\nj)(βk −β∗\nk),\n(8)\nwhere ˇβ is a point between βγ∗and β∗. Note that for β ∈Bδn(β∗), |βi −β∗\ni | ≤δn ≲\n1\n3√nrn , we\nhave n\n6\nP\ni,j,k∈γ∗hi,j,k(ˇβ)(βi −β∗\ni )(βj −β∗\nj)(βk −β∗\nk) = o(1).\nLet β(t) be network parameters satisfying β(t)\ni\n= βi +\n1\n√n\nP\nj∈γ∗hi,j(β∗)tj, ∀i ∈γ∗and β(t)\ni\n=\nβi, ∀i /∈γ∗. Note that\n1\n√n\nP\nj∈γ∗hi,j(β∗)tj ≤rn||t||∞M\n√n\n≲δn, for large enough n, |β(t)\ni | < 2δn\n∀i ∈γ∗. Thus, we have\nnln(β(t)) −nln(β∗) =n(ln(β(t)) −ln(β(t)\nγ∗) + ln(β(t)\nγ∗) −nln(β∗))\n=o(1) + n\nX\ni∈γ∗\n(β(t)\ni\n−β∗\ni )hi(β∗) + n\n2\nX\ni,j∈γ∗\nhi,j(β∗)(β(t)\ni\n−β∗\ni )(β(t)\nj\n−β∗\nj)\n=o(1) + n\nX\ni∈γ∗\n(βi −β∗\ni )hi(β∗) + n\n2\nX\ni,j∈γ∗\nhi,j(β∗)(βi −β∗\ni )(βj −β∗\nj)\n+ √n\nX\ni,j∈γ∗\nhi,j(β∗)tjhi(β∗) + √n\nX\ni∈γ∗\n(βi −β∗\ni )ti + 1\n2\nX\ni,j∈γ∗\nhi,j(β∗)titj\n=o(1) + √ntT (β −ˆβ) + nln(β) −nln(β∗) + 1\n2\nX\ni,j∈γ∗\nhi,j(β∗)titj,\n(9)\nwhere the last equality is derived by replacing appropriate terms by √ntT (β −ˆβ) and nln(β) −\nnln(β∗) based on (7) and (8), respectively; and the third equality is derived based on the following\ncalculation:\nn\n2\nX\ni,j∈γ∗\nhi,j(β∗)(β(t)\ni\n−β∗\ni )(β(t)\nj\n−β∗\nj)\n=n\n2\nX\ni,j∈γ∗\nhi,j(β∗)(βi −β∗\ni +\n1\n√n\nX\nk∈γ∗\nhi,k(β∗)tk)(βj −β∗\nj +\n1\n√n\nX\nk∈γ∗\nhj,k(β∗)tk)\n=n\n2\nX\ni,j∈γ∗\nhi,j(β∗)(βi −β∗\ni )(βj −β∗\nj) + 2 × n\n2\nX\ni,j∈γ∗\nhi,j(β∗) 1\n√n\nX\nk∈γ∗\nhi,k(β∗)tk(βj −β∗\nj)\n+ n\n2\nX\ni,j∈γ∗\nhi,j(β∗)( 1\n√n\nX\nk∈γ∗\nhi,k(β∗)tk)( 1\n√n\nX\nk∈γ∗\nhj,k(β∗)tk)\n=n\n2\nX\ni,j∈γ∗\nhi,j(β∗)(βi −β∗\ni )(βj −β∗\nj) + √n\nX\ni∈γ∗\n(βi −β∗\ni )ti + 1\n2\nX\ni,j∈γ∗\nhi,j(β∗)titj,\n(10)\nwhere the second and third terms in the last equality are derived based on the relation\nP\ni∈γ∗hi,j(β∗)hi,k(β∗) = δj,k, where δj,k = 1 if j = k, δj,k = 0 if j ̸= k.\nBy rearranging the terms in (9), we have\nZ\nBδn(β∗)\nexp{√ntT (β −ˆβ) + nln(β)}π(β)dβ\n= exp\n\n\n−1\n2\nX\ni,j∈γ∗\nhi,j(β∗)titj + o(1)\n\n\n\nZ\nBδn(β∗)\nenln(β(t))π(β)dβ.\n15\nFor β ∈Bδn(β∗), i ∈γ∗, by Assumption C.1, there exists a constant C > 2 such that\n|β(t)\ni | ≥|βi| −rn||t||∞M\n√n\n≥|β∗\ni | −2δn ≥(C −2)δn ≳rn\n√n\n≳\nv\nu\nu\nt\n \n1\n2σ2\n0,n\n−\n1\n2σ2\n1,n\n!−1\nlog\n\u0012rn(1 −λn)σ1,n\nσ0,nλn\n\u0013\n.\nThen we have\nσ1,n(1 −λn)\nσ0,nλn\ne\n−(\n1\n2σ2\n0,n\n−\n1\n2σ2\n1,n\n)(β(t)\ni\n)2\n≲1\nrn\n.\nIt is easy to see that the above formula also holds if we replace β(t)\ni\nby βi. Note that the mixture\nGaussian prior of βi can be written as\nπ(βi) =\nλn\n√\n2πσ1,n\ne\n−\nβ2\ni\n2σ2\n1,n\n\u0012\n1 + σ1,n(1 −λn)\nσ0,nλn\ne\n−(\n1\n2σ2\n0,n\n−\n1\n2σ2\n1,n\n)β2\ni \u0013\n.\nSince |βi −β(t)\ni | ≲δn ≲\n1\n3√nrn , |βi + β(t)\ni | < 2En + 3δn ≲En, and\n1\nσ2\n1,n ≲Hn log(n)+log(¯L)\nE2n\n, we\nhave\nrn\nσ2\n1,n\n(βi −β(t)\ni )(βi + β(t)\ni ) = Hn log(n) + log(¯L)\nnC1+1/3\n= o(1),\nby the condition C1 > 2/3 and Hn log(n) + log(¯L) ≺n1−ǫ. Thus,\nπ(β)\nπ(β(t)) = Q\ni∈γ∗\nπ(βi)\nπ(β(t)\ni\n) =\n1 + o(1), and\nZ\nBδn(β∗)\nenln(β(t))π(β)dβ =(1 + o(1))\nZ\nβ(t)∈Bδn(β∗)\nenln(β(t))π(β(t))dβ(t)\n=(1 + o(1))CNπ(β(t) ∈Bδn(β∗) | Dn),\n(11)\nwhere CN is the normalizing constant of the posterior. Note that ||β(t) −β||∞≲δn, we have\nπ(β(t) ∈Bδn(β∗) | Dn) →π(β ∈Bδn(β∗) | Dn). Moreover, since −1\n2\nP\ni,j∈γ∗hi,j(β∗)titj →\n1\n2tT V t, we have\nE(e\n√ntT (˜ν(β)−ˆβ) | Dn, Bδn(β∗)) =\nR\nBδn(β∗) e\n√ntT (β−ˆβ)enhn(β)π(β)dβ\nR\nBδn(β∗) enhn(β)π(β)dβ\n= e\ntT V t\n2\n+oP ∗(1).\nCombining the above result with the fact that π(˜ν(β) ∈Bδn(β∗) | Dn) →1, by section 1 of\nCastillo and Rousseau [2015], we have\nπ[√n(˜ν(β) −ˆβ) | Dn] ⇝N(0, V ).\nWe will then show that ˆβ will converge to β∗, then essentially we can replace ˆβ by β∗in the above\nresult. Let Θγ∗= {β : βi = 0, ∀i /∈γ∗} be the parameter space given the model γ∗, and let ˆβγ∗\nbe the maximum likelihood estimator given the model γ∗, i.e.\nˆβγ∗= arg max\nβ∈Θγ∗ln(β).\nGiven condition C.3 and by Theorem 2.1 of Portnoy [1988], we have ||ˆβγ∗−β∗|| = O(p rn\nn ) =\no(1).\nNote that hi(ˆβγ∗) = 0 as ˆβγ∗is maximum likelihood estimator. Then for any i ∈γ∗, |hi(β∗)| =\n|hi(ˆβγ∗) −hi(β∗)| = | P\nj∈γ∗hij(˜β)((ˆβγ∗)j −β∗\nj)| ≤M||ˆβγ∗−β∗||1 = O(p rn\nn ).\nThen for any i, j ∈γ∗, we have P\nj∈γ∗hi,j(β∗)hj(β∗) = O(\nq\nr3\nn\nn ) = o(1). By the deﬁnition of\nˆβ, we have ˆβ −β∗= o(1). Therefore, we have\nπ[√n(˜ν(β) −β∗) | Dn] ⇝N(0, V ).\n16\nA.2\nProof of Theorem 2.2\nProof. The proof of Theorem 2.2 can be done using the same strategy as that used in proving Theo-\nrem 2.1. Here we provide a simpler proof using the result of Theorem 2.1. The notations we used in\nthis proof are the same as in the proof of Theorem 2.1. In the proof of Theorem 2.1, we have shown\nthat π(˜ν(β) ∈Bδn(β∗) | Dn) →1. Note that µ(β, x0) = µ(˜ν(β), x0). We only need to consider\nβ ∈Bδn(β∗). For β ∈Bδn(β∗), we have\n√n(µ(β, x0) −µ(β∗, x0))\n=√n(µ(β, x0) −µ(βγ∗, x0) + µ(˜ν(βγ∗), x0) −µ(β∗, x0)).\nSince β ∈Bδn(β∗), for i /∈γ∗, |βi| < 2σ0,n log(\nσ1,n\nλnσ0,n ); and for i ∈γ∗, |˜ν(β)i −β∗\ni | < δ ≲\n1\n3√nrn . Therefore,\n|√nµ(β, x0) −µ(βγ∗, x0))| = |√n\nX\ni/∈γ∗\nβi(µi(˜β, x0))| ≤√nKnM2σ0,n log( σ1,n\nλnσ0,n\n) = o(1),\nwhere µi(β, x0) denotes the ﬁrst derivative of µ(β, x0) with respect to the ith component of β, and\n˜β denotes a point between β and βγ∗. Further,\nµ(˜ν(βγ∗), x0) −µ(β∗, x0)\n=√n\nX\ni∈γ∗\n(˜ν(β)i −β∗\ni )µi(β∗, x0) + √n\nX\ni∈γ∗\nX\nj∈γ∗\n(˜ν(β)i −β∗\ni )µi,j(ˇβ, x0)(˜ν(β)j −β∗\nj)\n=√n\nX\ni∈γ∗\n((˜ν(β)i −β∗\ni )µi(β∗, x0) + o(1),\nwhere µi,j(β, x0) denotes the second derivative of µ(β, x0) with respect to the ith and jth compo-\nnents of β and ˇβ is a point between ˜ν(β) and β∗. Summarizing the above two equations, we have\n√nµ(β, x0) −µ(β∗, x0)) = √n\nX\ni∈γ∗\n((˜ν(βi) −β∗\ni )µi(β∗, x0) + o(1).\nBy Theorem 2.1, π[√n(˜ν(β) −β∗) | Dn] ⇝N(0, V ), where V = (vij), and vi,j = E(hi,j(β∗))\nif i, j ∈γ∗and 0 otherwise. Then we have π[√n(µ(β, x0) −µ(β∗, x0)) | Dn] ⇝N(0, Σ), where\nΣ = ∇γ∗µ(β∗, x0)T H−1∇γ∗µ(β∗, x0) and H = E(−∇2\nγ∗ln(β∗)).\nA.3\nTheory of Prior Annealing: Proof of Theorem 3.1\nOur proof follows the proof of Theorem 2 in Chen et al. [2015]. SGLD use the ﬁrst order integrator\n(see Lemma 12 of Chen et al. [2015] for the detail). Then we have\nE(ψ(β(t+1))) =ψ(β(t)) + ǫtLtψ(β(t)) + O(ǫ2\nt )\n=ψ(β(t)) + ǫt(Lt −L)ψ(β(t)) + ǫtLψ(β(t)) + O(ǫ2\nt ).\nNote that by Poisson equation, Lψ(β) = φ(β) −\nR\nφ(β)π(β|Dn, η∗, σ∗\n0,n)dβ. Taking expectation\non both sides of the equation, summing over t = 0, 1, . . . , T −1, and dividing ǫT on both sides of\nthe equation, we have\nE\n \n1\nT\nT −1\nX\nt=1\nφ(β(t)) −\nZ\nφ(β)π(β|Dn, η∗, σ∗\n0,n)\n!\n= 1\nT ǫ(E(ψ(β(T ))) −ψ(β(0))) −1\nT\nT −1\nX\nt=0\nE(δtψ(β(t))) + O(ǫ).\nTo characterize the order of δt = Lt −L, we ﬁrst study the difference of the drift term\n∇log(π(β(t)|D(t)\nm,n, η(t), σ(t)\n0,n)) −∇log(π(β(t)|Dn, η∗, σ∗\n0,n))\n=\nn\nX\ni=1\n∇log(pβ(t)(xi, yi)) −n\nm\nm\nX\nj=1\n∇log(pβ(t)(xij, yij))\n+ η(t)∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n)) −η∗∇log(π(β(t)|λn, σ∗\n0,n, σ1,n)).\n17\nUse of the mini-batch data gives an unbiased estimator of the full gradient, i.e.\nE(\nn\nX\ni=1\n∇log(pβ(t)(xi, yi)) −n\nm\nm\nX\nj=1\n∇log(pβ(t)(xij, yij))) = 0.\nFor the prior part, let p(σ) denote the density function of N(0, σ). Then we have\n∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n))\n= −\n(1 −λn)p(σ(t)\n0,n)\n(1 −λn)p(σ(t)\n0,n) + λnp(σ1,n)\nβ(t)\nσ(t)\n0,n\n2 −\nλnp(σ1,n)\n(1 −λn)p(σ(t)\n0,n) + λnp(σ1,n)\nβ(t)\nσ2\n1,n\n,\nand thus E|∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n))| ≤2E|β(t)|\nσ∗\n0,n\n2 . By Assumption 5.2, we have\nE(|η(t)∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n)) −η∗∇log(π(β(t)|λn, σ∗\n0,n, σ1,n))|)\n=E(|η(t)∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n)) −η∗∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n))|)\n+ E(|η∗∇log(π(β(t)|λn, σ(t)\n0,n, σ1,n)) −η∗∇log(π(β(t)|λn, σ∗\n0,n, σ1,n))|)\n≤2M\nσ∗\n0,n\n2 |η(t) −η∗| + η∗M|σ(t)\n0,n −σ∗\n0,n|.\nBy Assumption 5.1, E(ψ(β(t))) ≤∞. Then\n1\nT\nT −1\nX\nt=0\nE(δtψ(β(t))) = O\n \n1\nT\nT −1\nX\nt=0\n(|η(t) −η∗| + |σ(t)\n0,n −σ∗\n0,n|)\n!\n.\nNote that by assumption 5.1, |(ψ(β(T ))) −ψ(β(0))| is bounded. Then\nE\n \n1\nT\nT −1\nX\nt=1\nφ(Xt) −\nZ\nφ(β)π(β|Dn, η∗, σ∗\n0,n)\n!\n= O\n \n1\nT ǫ +\nPT −1\nt=0 (|η(t) −η∗| + |σ(t)\n0,n −σ∗\n0,n|)\nT\n+ ǫ\n!\n.\nA.4\nConstruct Conﬁdence Interval\nTheorem 2.2 implies that a faithful prediction interval can be constructed for the sparse neu-\nral network learned by the proposed algorithms. In practice, for a normal regression problem\nwith noise N(0, σ2), to construct the prediction interval for a test point x0, the terms σ2 and\nΣ = ∇γ∗µ(β∗, x0)T H−1∇γ∗µ(β∗, x0) in Theorem 2.2 need to be estimated from data. Let\nDn = (x(i), y(i))i=1,...,n be the training set and µ(β, ·) be the predictor of the network model\nwith parameter β. We can follow the following procedure to construct the prediction interval for the\ntest point x0:\n• Run algorithm 1, let ˆβ be an estimation of the network parameter at the end of the algorithm\nand ˆγ be the correspoding network structure.\n• Estimate σ2 by\nˆσ2 = 1\nn\nn\nX\ni=1\n(µ(ˆβ, x(i)) −y(i))2.\n• Estimate Σ by\nˆΣ = ∇ˆγµ(ˆβ, x0)T (−∇2\nˆγln(ˆβ))−1∇ˆγµ(ˆβ, x0).\n• Construct the prediction interval as\n \nµ(ˆβ, x0) −1.96\nr\n1\nn\nˆΣ + ˆσ2, µ(ˆβ, x0) + 1.96\nr\n1\nn\nˆΣ + ˆσ2\n!\n.\n18\nHere, by the structure selection consistency (Lemma 2.2) and consistency of the MLE for the learnt\nstructure Portnoy [1988], we replace β∗and γ∗in Theorem 2.2 by ˆβ and ˆγ.\nIf the dimension of the sparse network is still too high and the computation of ˆΣ becomes prohibitive,\nthe following Bayesian approach can be used to construct conﬁdence intervals.\n• Running SGMCMC algorithm to get a sequence of posterior samples: β(1), . . . , β(m).\n• Estimating σ2 by ˆσ2 = 1\nn\nPn\ni=1(y(i) −µ(i))2, where\nµ(i) = 1\nm\nm\nX\nj=1\nµ(β(j), x(i)), i = 1, . . . , n,\n• Estimate the prediction mean by\nˆµ = 1\nm\nm\nX\ni=1\nµ(β(i), x0).\n• Estimate the prediction variance by\nˆV = 1\nm\nm\nX\ni=1\n(µ(β(i), x0) −ˆµ)2 + ˆσ2.\n• Construct the prediction interval as\n(µ −1.96\n√\nV , µ + 1.96\n√\nV ).\nA.5\nPrior Annealing\nIn this section, we give some graphical illustration of the prior annealing algorithm. In practice,\nthe negative log-prior puts penalty on parameter weights. The mixture Gaussian prior behaves like\na piecewise L2 penalty with different weights on different regions. Figure 2 shows the shape of a\nnegative log-mixture Gaussian prior. In step (iii) of Algorithm 1, the condition π(γi = 1|βi) >\n0.5 splits the parameters into two parts. For the βi’s with large magnitudes, the slab component\nN(0, σ2\n1,n) plays the major role in the prior, imposing a small penalty on the parameter. For the βi’s\nwith smaller magnitudes, the spike component N(0, σ2\n0,n) plays the major role in the prior, imposing\na large penalty on the parameters to push them toward zero in training.\nFigure 3 shows the shape of negative log-prior and π(γi = 1|βi) for different choices of σ2\n0,n and\nλn. As we can see from the plot, σ2\n0,n plays the major role in determining the effect of the prior.\nLet α be the threshold in step (iii) of Algorithm 1 of the main body, i.e. the positive solution to\nπ(γi = 1|βi) = 0.5. In general, a smaller σ2\n0,n will result in a smaller α. If a very small σ2\n0,n\nis used in the prior from the beginning, then most of βi’s at initialization will have a magnitude\nlarger than α and the slab component N(0, σ2\n1,n) of the prior will dominate most parameters. As a\nresult, it will be difﬁcult to ﬁnd the desired sparse structure. Following the proposed prior annealing\nprocedure, we can start with a larger σ2\n0,n, i.e. a larger threshold α and a relatively smaller penalty\nfor those |βi| < α. As we gradually decrease the value of σ2\n0,n, α decreases, and the penalty\nimposed on the small weights increases and drives them toward zero. The prior annealing allows us\nto gradually sparsify the DNN and impose more and more penalties on the parameters close to 0.\nA.6\nExperimental Setups\nA.6.1\nSimulated examples\nPrior annealing\nWe follow simple implementation of Algorithm given in section 3.1. We run\nSGHMC for T = 80000 iterations with constant learning rate ǫt = 0.001, momentum 1 −α = 0.9\nand subsample size m = 500. We set λn = 1e −7, σ2\n1,n = 1e −2, (σinit\n0,n )2 = 5e −5, (σend\n0,n )2 =\n1e −6 and T1 = 5000, T2 = 20000, T3 = 60000. We set temperature τ = 0.1 for t < T3 and for\nt > T3, we gradually decrease temperature τ by τ =\n0.1\nt−T3 . After structure selection, the model is\nﬁne tuned for 40000 iterations. The number of iteration setup is the same as Sun et al. [2021].\n19\n−1.00\n−0.75\n−0.50\n−0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nβ\n−5\n0\n5\n10\n15\n20\n25\n30\nNegtive Log Prior\nFigure 2: Negative logarithm of the mixture Gaussian prior.\n−0.100\n−0.075\n−0.050\n−0.025\n0.000\n0.025\n0.050\n0.075\n0.100\nβ\n−5\n0\n5\n10\n15\nNegtive Log Prior\nσ2\n1 = 0.04, σ2\n0 = 1.5e −5, λn = 1e −8\nσ2\n1 = 0.04, σ2\n0 = 1.5e −4, λn = 1e −8\nσ2\n1 = 0.04, σ2\n0 = 1.5e −5, λn = 1e −7\n−0.100\n−0.075\n−0.050\n−0.025\n0.000\n0.025\n0.050\n0.075\n0.100\nβ\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP(γ = 1|β)\nσ2\n1 = 0.04, σ2\n0 = 1.5e −5, λn = 1e −8\nσ2\n1 = 0.04, σ2\n0 = 1.5e −4, λn = 1e −8\nσ2\n1 = 0.04, σ2\n0 = 1.5e −5, λn = 1e −6\nFigure 3: Negative log-prior and π(γ = 1|β) for different choices of σ2\n0,n and λn.\n20\nOther Methods\nSpinn, Dropout and DNN are trained with the same network structure using SGD\nwith momentum. Same as our method, we use constant learning rate 0.001, momentum 0.9, sub-\nsample size 500 and traing the model for 80000 iterations. For Spinn, we use LASSO penalty and\nthe regularization parameter is selected from {0.05, 0.06, . . ., 0.15} according to the performance\non validation data set. For Dropout, the dropout rate is set to be 0.2 for the ﬁrst layer and 0.5 for\nthe other layers. Other baseline methods BART50, LASSO, SIS are implemented using R-package\nrandomForest, glmnet, BART and SIS respectively with default parameters.\nA.6.2\nCIFAR10\nWe follow the standard training procedure as in Lin et al. [2020], i.e. we train the model with\nSGHMC for T = 300 epochs, with initial learning rate ǫ0 = 0.1, momentum 1 −α = 0.9, temper-\nature τ = 0.001, mini-batch size m = 128. The learning rate is divided by 10 at 150th and 225th\nepoch. We follow the implementation given in section 3.1 and use T1 = 150, T2 = 200, T3 = 225,\nwhere Tis are number of epochs. We set temperature τ = 0.01 for t < T3 and gradually decrease\nτ by τ =\n0.01\nt−T3 for t > T3. We set σ2\n1,n = 0.04 and (σinit\n0,n )2 = 10 × (σend\n0,n )2 and use different\nσend\n0,n , λn for different network size and target sparsity level. The detailed settings are given below:\n• ResNet20 with target sparsity level 20%: (σend\n0,n )2 = 1.5e −5, λn = 1e −8\n• ResNet20 with target sparsity level 10%: (σend\n0,n )2 = 6e −5, λn = 1e −9\n• ResNet32 with target sparsity level 10%: (σend\n0,n )2 = 3e −5, λn = 2e −9\n• ResNet32 with target sparsity level 5%: (σend\n0,n )2 = 1e −4, λn = 2e −8\n21\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2021-10-01",
  "updated": "2021-12-02"
}