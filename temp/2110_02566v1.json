{
  "id": "http://arxiv.org/abs/2110.02566v1",
  "title": "Adaptive control of a mechatronic system using constrained residual reinforcement learning",
  "authors": [
    "Tom Staessens",
    "Tom Lefebvre",
    "Guillaume Crevecoeur"
  ],
  "abstract": "We propose a simple, practical and intuitive approach to improve the\nperformance of a conventional controller in uncertain environments using deep\nreinforcement learning while maintaining safe operation. Our approach is\nmotivated by the observation that conventional controllers in industrial motion\ncontrol value robustness over adaptivity to deal with different operating\nconditions and are suboptimal as a consequence. Reinforcement learning on the\nother hand can optimize a control signal directly from input-output data and\nthus adapt to operational conditions, but lacks safety guarantees, impeding its\nuse in industrial environments. To realize adaptive control using reinforcement\nlearning in such conditions, we follow a residual learning methodology, where a\nreinforcement learning algorithm learns corrective adaptations to a base\ncontroller's output to increase optimality. We investigate how constraining the\nresidual agent's actions enables to leverage the base controller's robustness\nto guarantee safe operation. We detail the algorithmic design and propose to\nconstrain the residual actions relative to the base controller to increase the\nmethod's robustness. Building on Lyapunov stability theory, we prove stability\nfor a broad class of mechatronic closed-loop systems. We validate our method\nexperimentally on a slider-crank setup and investigate how the constraints\naffect the safety during learning and optimality after convergence.",
  "text": "Adaptive control of a mechatronic system using\nconstrained residual reinforcement learning\nTom Staessens, Tom Lefebvre and Guillaume Crevecoeur\nAbstract—We propose a simple, practical and intuitive\napproach to improve the performance of a conventional\ncontroller in uncertain environments using deep reinforce-\nment learning while maintaining safe operation. Our ap-\nproach is motivated by the observation that conventional\ncontrollers in industrial motion control value robustness\nover adaptivity to deal with different operating conditions\nand are suboptimal as a consequence. Reinforcement\nlearning on the other hand can optimize a control signal\ndirectly from input-output data and thus adapt to opera-\ntional conditions, but lacks safety guarantees, impeding its\nuse in industrial environments. To realize adaptive control\nusing reinforcement learning in such conditions, we follow\na residual learning methodology, where a reinforcement\nlearning algorithm learns corrective adaptations to a base\ncontroller’s output to increase optimality. We investigate\nhow constraining the residual agent’s actions enables to\nleverage the base controller’s robustness to guarantee safe\noperation. We detail the algorithmic design and propose to\nconstrain the residual actions relative to the base controller\nto increase the method’s robustness. Building on Lyapunov\nstability theory, we prove stability for a broad class of\nmechatronic closed-loop systems. We validate our method\nexperimentally on a slider-crank setup and investigate how\nthe constraints affect the safety during learning and opti-\nmality after convergence.\nIndex Terms—mechatronics, servo systems, motion con-\ntrol, reinforcement learning, uncertain systems.\nI. INTRODUCTION\nM\nECHATRONIC drivetrain systems face increasingly de-\nmanding performance and efﬁciency requirements in\nindustrial and manufacturing applications. They furthermore\nneed to become more autonomous while interacting with\na varying environment. Adequate motion control needs to\naddress these challenges. When designing, implementing and\ntuning controllers, having knowledge on the dynamics of\nthe mechatronic system is key. Accurate position and speed\ncontrol of servo drive systems for instance require detailed\nknowledge on the inertia and friction [1]. Based on ab initio\nphysical modelling principles it is possible to approximate the\nreal system behavior. However capturing the full mechatronic\nsystem dynamics is often cumbersome and challenging as\nmechatronic systems are plagued by nonlinear and complex\ndynamic behavior due to interacting components [1]–[3]. Pa-\nrameter identiﬁcation procedures can subsequently be engaged\nto closer align the model to the real mechatronic system [4].\nNonetheless, despite tremendous engineering modeling efforts,\nuncertainties may still be present.\nUnfortunately the optimality of control system design is\nstrongly affected by the modelling ﬁdelity [5]. When de-\nsigned off-line, controllers are approximate due to the inherent\nuncertainties in the real-world that are not incorporated in\nthe modelling. They furthermore require tremendous tuning\nThis research received funding from the Flemish Government under\nthe ”Onderzoeksprogramma Artiﬁci¨ele Intelligentie (AI) Vlaanderen” pro-\ngramme.\nT. Staessens, T. Lefebvre and G. Crevecoeur are with the De-\npartment of Electrical Energy, Metals, Mechanical Constructions and\nSystems, Ghent University, 9000 Ghent, Belgium, and also with\nEEDT-DC, Flanders Make (e-mail: {tom.staessens, tom.lefebvre, guil-\nlaume.crevecoeur}@ugent.be).\nefforts, e.g. ﬁnding gains in PID controllers to track setpoints.\nA wide range of adaptive control strategies have been designed\nto alleviate this issue on-line: adapting PID gains [6], online\ngravity compensation [7] or, starting from an approximate\nmodel, adapting the parameters in linearly parameterized\nmodel predictive control (MPC) [8] or of a fuzzy approxima-\ntion of the remaining unknown system inﬂuences in sliding-\nmode control [9]. In the stochastic optimal control framework,\nstochastic uncertainties are introduced to the deterministic\noptimal control resulting in linear quadratic Gaussian [10]\nand stochastic model predictive control formalisms [11], that\nare variants of the LQR and deterministic MPC, respectively.\nIn the framework of MPC, off-line design strategies include\nopen-loop and feedback minimax formulations [12] and tube-\nbased formulations. Initially, robust tube-based MPC has been\nused for linear systems in process control [13] and more\nrecently has been further elaborated for mechatronic systems\n[14]. Tube-based MPC approaches the control problem by ﬁrst\nsolving the MPC problem for the nominal system. In a second\nstage the ancillary state feedback control law is designed to\nconﬁne the error between nominal and actual states within an\ninvariant tube.\nStrategies such as the tube-based MPC are able to cope\nwith uncertainties but do not aim to reduce them by learning\nintelligently from the input-output behavior of the controlled\nsystem. Learning-based techniques with iterative learning\ncontrol techniques [15] have been devised that adapt with\nrespect to the tracking error. These iterative learning control\nalgorithms can work under a model-free assumption [16],\nbut can only compensate for periodic disturbances. Also to\nadapt for repetitive tasks, a recent MPC strategy was proposed\nrelying solely on a data-driven model [17]. Reinforcement\nlearning (RL) methods on the other hand adapt to the actual\nbehavior of the mechatronic system, interacting with a variable\nenvironment, and directly learn an optimal feedback policy\n(referred to as the agent). As opposed to the aformentioned\ntechniques, the learned policy is state-dependent, where state\ncan be interpreted broadly as any measurement information,\ne.g. camera images, and is independent of any time-, state- or\ntrajectory-dependent periodicity or underlying dynamics.\nHence RL opens up interesting new perspectives on adap-\ntivity. Assuming the parametrized policy is sufﬁciently expres-\nsive, the training procedure of RL is capable of generalizing\nto variable operating conditions and changing environmental\nsettings [18]. Moreover, RL can handle control problems that\nare difﬁcult to approach with conventional controllers because\nthe control goal can be speciﬁed indirectly as a term in a\nreward function with no explicit requirements on its form or\ndependencies. The main disadvantage of RL is that it can only\nfashion new insights by interacting with the system and the\nenvironment in real-time, leveraging the actions it takes to\nprobe and explore the optimality landscape. Since the former\nprocess relies on the stochasticity inherent to the system or\non deliberate perturbation, this may lead to unsafe situations\nlimiting the usage of RL to non safety critical situations [19].\nRecently, related work for the adaptive and robust control of\nnonlinear systems using actor critic RL has been developed.\nThese methods however still require an approximate model or\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\narXiv:2110.02566v1  [eess.SY]  6 Oct 2021\nsystem identiﬁcation, are designed for reference tracking only\n[20], [21] or rely on the convergence of the critic function\napproximators and as such the quality of the collected dataset,\nfor robustness guarantees. [22].\nNext to aforementioned stability issues, the amount of\nexplorative trials to learn and ﬁnd optimal control actions is\nsigniﬁcant. To face the issue of data-efﬁcient training the litera-\nture proposes the use of off-policy algorithms [23]. As opposed\nto on-policy algorithms [24], off-policy methods train the agent\nbased on data generated with another, canonical controller.\nAs a consequence, off-policy algorithms strongly improve\nthe data-efﬁciency, however they still require a signiﬁcant\namount of trials which may not always be feasible for real-\nworld applications [19]. Nevertheless, the simple observation\nthat off-policy algorithms can be merged with conventional\ncontrollers is of particular interest and sheds a new light on\nthe adaptivity issue raised earlier.\nIn this paper we explore the possibilities of a residual ar-\nchitecture to cater the limitations bothering the straightforward\napplication of traditional RL in an industrial setting. On the\none hand we will rely on a traditional suboptimal but stabilis-\ning control law. On the other hand we superpose a residual\nRL agent that may adapt the control output in an attempt\nto optimize an auxiliary objective. This architecture, coined\nresidual reinforcement learning (RRL), has been explored in\nearlier research and results into an efﬁcient, safe and optimal\ncontrol design. RRL has been introduced recently to alleviate\nthe exploration needs and increase tractability in terms of data-\nefﬁciency for data-driven robot control [25], [26]. By applying\nthe reinforcement learning algorithm residually on a base con-\ntroller that roughly approaches the control objective, the base\ncontroller ‘guides’ the reinforcement learning algorithm to an\napproximate solution, accelerating training. The constraints\nimposed on the residual agent were absolute, determined by\nthe limits of the controlled system’s inputs.\nOriginally used to accelerate learning in robot control, RRL\ncan be engaged to increase the optimality in mechatronic\nmotion control. The situation differs however for the setting of\nindustrial motion control of mechatronic systems, where the\nchallenge shifts to maintaining safe operation while realising\nan adaptive motion control. By leveraging a base controller,\nresidual RL improves the data-efﬁciency that prohibits tradi-\ntional RL from being used in such real-world mechatronic\nsystems. However the safety concerns, being the main reason\nimpeding its in industrial settings, remain unaddressed.\nThis paper explores the possibilities of adding RL next to an\nexisting control law in a safe and robust manner. We do this by\nfollowing a residual approach for which we introduce relative\nconstraints on the residual agent. The ensuing objective of this\npaper is to realize an adaptive motion control for mechatronic\nsystems using RL, applicable in a real-world setting. This is\napproached in a twofold manner. First, we detail the design\nof a stable constrained residual learning methodology. We\nintroduce an algorithmic adaptation to residual RL, employing\nrelative constraints and prove the stability of both methods\nusing the Lyapunov method. Second, we validate the method’s\nability to achieve adaptive control, improving the performance\nfor motion control of a mechatronic system compared to the\ntraditional controller. We provide implementation details on\nthe presented methodology and demonstrate the results by\napplying it on a slider crank setup and evaluating the Mean\nAbsolute Error (MAE) of the objective. The contributions of\nthis paper are as follows:\n• Extension of the Residual Reinforcement Learning frame-\nwork to Constrained RRL. This allows the use of RL al-\ngorithms in industrial, safety-critical settings and as such\nenables online adaptive control without any assumptions\nor prerequisites of the system dynamics, control objective\nor form of the controller inputs.\n• Theoretical analysis of the developed method using Lya-\npunov stability theory, proving stability for a broad class\nof mechatronic systems even under worst case conditions.\n• Experimental validation on a slider-crank, a non-linear\nsystem with applications in many industrial systems.\nIn conclusion, where pure RL does not work, we show that this\nframework enables the use of these algorithms in real-world\nmechatronic settings.\nII. METHODOLOGY\nA. Reinforcement Learning\nReinforcement learning operates within a standard (Partially\nObservable) Markov Decision Process (MDP) framework. An\nMDP is a tuple M = (S, A, r, p) where s\n∈\nS are\nstates, a\n∈\nA are actions, r(s, a) is the reward for taking\naction a in state s and p(st+1|st, at) is the probability of\ntransitioning to state st+1 following state st and action at.\nWe deﬁne a trajectory τ as a sequence of states and actions,\nτ = (s0, a0, s1, a1, ...). The return is deﬁned as the inﬁnite\ndiscounted sum of rewards R(τ) = limT →∞\nPT\nt=0 γtr(st, at)\nwith 0 ⩽γ ⩽1 the temporal discount factor. Given an\ninitial state s0, the objective of any RL method is to solve\nthe following stochastic optimal control problem by ﬁnding an\noptimal policy π∗(at|st) that maximizes the expected return\nJ(π) = Eτ∼p(τ|π)[R(τ)]\nπ∗= argmax\nπ\nJ(π)\n(1)\nwith p(τ|s0, π) = ΠT −1\nt=0 p(st+1|st, at)π(at|st). In this paper,\nthe Soft Actor-Critic (SAC) algorithm is employed as RL\nmethod for all experiments. SAC is a state-of-the-art actor-\ncritic RL method. This implies that both estimates of the\nstate-action value function and policy are approximated us-\ning a neural network. Based on temporal differencing these\nestimates are iterated until they satisfy the Bellman equation.\nSAC is unique with regard to other actor-critic methods as it\nmaintains a stochastic actor. Actions are realized by sampling\nfrom a Gaussian distribution whose mean and variance are out-\nputted by the network. This has the advantage of encouraging\nexploration during training and achieving a higher stability\nafter convergence. For further details we refer to [27], [28].\nB. Constrained Residual Reinforcement Learning\nRecently introduced for robot control, residual reinforce-\nment learning trains an RL controller residually on top of\nan imperfect, traditional controller [25], [26]. The RL algo-\nrithm leverages the traditional controller as an initialization\nto enable data-efﬁcient reinforcement learning for tasks where\ntraditional RL is intractable, such as robotic insertion tasks\nwhere rewards are sparse [29]. Starting from a suboptimal, but\nadequate and robust controller, as often present in the motion\ncontrol of industrial applications, we introduce the Constrained\nResidual Reinforcement Learning (CRRL) architecture.\nTwo advantages are principle to the concept of CRRL.\nFirstly, the architecture leverages the traditional controller to\nguarantee robust exploration of the RL agent during operation.\nThe robust controller can be tuned so that the exploration\nof the residual policy remains within the principle region\nof attraction. Secondly, as with basic RRL, the traditional\ncontroller provides a good initialization for the reinforcement\nlearning algorithm which may further improve the steepness\nof the learning curve.\nIn this contribution we study two variants of the basic RRL\narchitecture, absolute and relative CRRL.\n1) Absolute CRRL: For Absolute CRRL, we simply super-\npose the residual policy, parametrized by the parameters θ, to\nthe traditional controller. This produces a control input uθ(s)\nuθ(s) = u(s) + βaπθ(y(s))\n(2)\nwhere u(s) is the traditional control algorithm, πθ(y(s)) is the\nreinforcement learning policy, y(s) is a preprocessing feature\nextraction map and βa is a parameter determining the scale\nof the residual actions. Besides the state s, the map y(s) can\ncontain e.g. u(s) or any other feature that seems interesting1.\nNote that (2) corresponds mathematically with basic RRL\nemploying a RL algorithm which uses a tanh to conﬁne its\nactions, such as SAC [27], with special care taken to tune βa.\nIn robot control, e.g. connector insertion tasks [29], choos-\ning βa as an absolute constraint so as to conﬁne the total\nactions within the feasible action space, such as the torque\nlimits of the controlled actuators, sufﬁces to learn a tractable\npolicy that succeeds in its task. For industrial motion control\nhowever, safety during all phases of the learning process\nis required. Determining bounds for the residual agent that\nguarantee safety is non-trivial. In Section II-C, we establish\nsafety conditions that guarantee that regardless of the residual\nagent the system stays within the principle region of attraction\nof the traditional controller. However the detailed modelling\nrequirements may make it unfeasible for some real-world\napplications in the industry. Alternatively, for cyclic processes,\none can take the output of the base controller during one\ncycle as a reference. This can however not be done in a\nstraightforward manner when facing non-cyclic processes.\nFurthermore, it requires the scale of the base actions to remain\nconsistent within one cycle to provide a safe constraint tube.\n2) Relative CRRL: To realize a residual agent maintaining\nsafe operation irrespective of such considerations regarding\nthe underlying process, we propose to constrain its actions\nrelative to the base controller’s outputs. By constraining the\nreinforcement learning algorithm to a percentage of the clas-\nsical control action and adding it to said action, we effectively\ncreate a tube around the classical control actions where the\nreinforcement learning algorithm is allowed to explore and\nlearn corrective adaptations to the conventional controller’s\noutput that improve its performance. Both during the learning\nphase, when exploration is the dominant behavior, and when\nencountering inputs that deviate from the training distribution\nafter convergence, the classical control algorithm determines\nthe bulk of the control input and thereby ensures a safe and\nrobust operation. The resulting relative residual policy is:\nuθ(s) = u(s)(1 + βrπθ(y(s)))\n(3)\nwith βr > 0 a parameter constraining the actions of the neural\nnetwork relative to the actions of the base control algorithm.\nFig. 1 shows an overview of the relative CRRL structure.\n3) Algorithm: The implementation of the absolute CRRL\narchitecture is straightforward. That of the relative CRRL\narchitecture is more subtle. As the actor network is trained\nthrough gradient descent to minimize its loss [27], multiplying\nthe actor output by βru(s) directly scales the gradient of\nall actor network parameters by this fraction of the base\ncontroller’s output. This gives more weight to situations where\n1Generally speaking the feature map may even contain additional\nmeasurement information such as camera images etc.\nFig. 1: Overview of the proposed relative CRRL structure.\nthe base controller’s output is large during the training of\nthe actor network. To alleviate this imbalance, one can opt\nto use the unscaled output for training and only scale the\naction when applying it on the system itself. The input state for\nthe networks can then be extended with the base controller’s\noutput to again have full state information. However, in\npractice, we have found the residual controller’s performance\nto beneﬁt from the scaling during training as well. Therefore\nthe former option is used throughout the experiments.\n4) Convergence: For the convergence condition, we rely on\nthe proof given in [27] for SAC in the theoretical tabular case,\nwhich is approximated for practical use in continuous domains\nby using the neural networks as function approximators. By\nconsidering the base controller as a part of the system on which\nthe residual SAC agent acts and assuming its robustness, the\nsame conditions for convergence hold. The adapted algorithm\nof [28], which automatically balances the stochasticity of the\nactor as a function of the reward, further promotes convergence\nin practice by lowering the variance of the policy in states\nwhere the policy achieves high rewards.\nC. Closed-loop tracking stability guarantees for mechani-\ncal systems using a PD base-line controller\nHere we analyse the closed-loop stability of the proposed\nlearning approach. We consider a CRRL controller both with\nabsolute (2) and relative constraints (3). The experimental\nperformance of both controllers is investigated further in\nSection III-B3. Our synthesis focusses on closed-loop tracking\nstability guarantees for mechanical systems using a PI base-\nline controller f(s). Such provides a generic setting that meets\nthe requirements of many practical examples from industry.\nWe aim to establish safety guarantees when worst-case condi-\ntions are met during exploration, not to make claims about\nits near optimal behaviour during convergence. Therefore,\nwe treat the residual agent as a disturbance whose actions\ndestabilize the system. Our analysis allows us to determine\nrobust gain values for the base-line controller that guarantee\nstability regardless of the actions taken by the residual agent.\nOur analysis is based on classic Lyapunov stability theory\nand can be summarized in the following two theorems. We\nnote that the proofs of both theorems rely on a particular\nchoice of Lyapunov function. Therefore, these theorems are\nillustrative to the fact that it is possible to obtain conditions\nfor the base-line control settings corresponding speciﬁc safety\nguarantees in the context of CRRL. On the other hand, our\nconditions might be overly conservative and possibly weaker\nconditions exist based on other Lyapunov functions.\nTheorem 1. Consider a mechanical system with generalised\ncoordinates q ∈Rn, input τ ∈Rn and reference trajectory\nqr ∈Rn so that ∥˙qr∥≤ω0 and ∥¨qr∥≤α0 whose dynamics\nare governed by the equation\nM(q)¨q + B(q, ˙q) ˙q + g(q) = u\n(4)\nWith e = q −qr let the absolute CRRL policy be deﬁned as\nu(q, ˙q) = −kP ˙e −kIe −βaπθ(q, ˙q, . . . )\n(5)\nThen closed-loop error trajectories x = (e, ˙e) are bounded by\n∥x(t)∥≤e−1\n2\nα3\nα2 t α2\n√α1\n∥x(0)∥2 + δ,\nlim\nt→∞∥x(t)∥≤δ\n(6)\nwith α1, α2, α3 and α4 deﬁned as in theorem 4 and where\nδ =\nµη(P )\nr\n1\n4 k2\nP+λ2µη(M)2\n\u0012\nα0+\n1\nνη(M) (µη(B)ω0+g0+βa)\n\u0013\nνη(P )νη( ˇ\nQ)\n(7)\nwhere g0 = sup ∥g(q)∥and matrices P and ˇQ deﬁned as in\ntheorem 4 if also λ >\nk2\nP\n2kIν(M), kP >\n2µη(Lq)ω0\n1−1\nλ −\nr\nϵ2µη(P )µη(B)\nλ2νη(P )νη(M)\nand\nkI > µη(M)µη(B)2ω2\n0\n2ν(M)2\n.\nFor the proof, deﬁnitions of the norms we refer to appx. B.\nTheorem 2. Considering the same controlled system as in\ntheorem 1. Let the relative CRRL policy be deﬁned as\nu(q, ˙q) = −(kP ˙e + kIe) (1 + βrπθ(q, ˙q, . . . ))\n(8)\nThen closed-loop error trajectories x are bounded by\n∥x(t)∥≤e−1\n2\nα3\nα2 t α2\n√α1\n∥x(0)∥2 + δ,\nlim\nt→∞∥x(t)∥≤δ\n(9)\nwhere\nδ =\nµη(P )\nr\n1\n4 k2\nP+λ2µη(M)2\n\u0012\nα0+\n1\nνη(M) (µη(B)ω0+g0)\n\u0013\nνη(P )νη( ˇ\nQ)\n(10)\nif it also holds that λ >\nk2\nP\n2(1−2βr)kIν(M),\nkP >\n2µη(Lq)ω0\n1−1\nλ −2βrkI\nλ\n−\nr\nϵ2µη(P )µη(B)\nλ2νη(P )νη(M)\nand kI > µη(M)µη(B)2ω2\n0\n2(1−βr)ν(M)2 .\nFor the proof we refer to appx. B.\nThese two theorems suggest that if the closed-loop system\nis initiated in a state ∥x(0)∥= 0, the error will never grow\nbeyond a magnitude δ. Provided a desired value for δ and\ndepending on the CRRL architecture, it is possible to choose\nvalues for kP, kI and λ and therefore for βa or βr, so that the\nconditions in either theorem 1 or 2 are satisﬁed. For practical\ncalculation, note that all variables in Theorems 1 and 2 depend\nonly on the norms of the matrices, deﬁned in appx. B. These\nnorms can be calculated with knowledge of the initial state\nbounds as per the proof’s assumptions. For the slider crank\nsystem deﬁned in Table I and a controller with parameters\nkp = 1.1 and ki = 2.3, the conditions for βr are 0.36, 35.28\nand 0.999 respectively to ensure stable convergence.\nIII. RESULTS AND DISCUSSION\nIn this contribution we study the CRRL methodology on a\nphysical slider-crank setup. A PI controller is chosen to obtain\na stable system. Combining RL with more complex controllers\nsuch as MPC controllers for online parameter tuning [30] or\nstate-feedback controllers is possible, but is out of the scope\nof this paper: to improve upon and be directly modular with\na commonly used controller in an industrial setting. Note that\nthe CRRL framework allows for the use of any base controller\nnonetheless.\nFig. 2: Schematic overview of a slider-crank [31] and picture\nof the experimental setup.\nA. Experimental setup\n1) Slider-crank linkage: Present in many industrial appli-\ncations, a slider-crank provides reciprocating linear motion\nthrough a rotary motor in combination with a bar linkage\nsystem. Fig. 2 shows a schematic overview as well as a\npicture of the experimental setup. The dimensions of the\nsetup are detailed in Table I. This system exhibits highly\nnonlinear behavior [31] and is often plagued by unidentiﬁed\nload disturbances and unknown interactions with the environ-\nment. To achieve an adequate control under these conditions,\ntypically some form of PID controller is used. This strategy is\nadequate in applications with low requirements on precision,\nbut suffers from suboptimality for systems with varying loads\nor environment conditions. Coping with these uncertainties\nrequires either re-tuning the controller or having knowledge\nof the disturbances interacting with the system which is often\nunfeasible in practice. This application is of direct relevance\nin various industrial systems, e.g. compressors [32], hydraulic\npumps [33], weaving looms [34] and presses [35].\nTABLE I: Slider-crank setup details.\nParameter\nValue\nLength of link 1 (l1)\n0.05 m\nLength of link 2 (l2)\n0.275 m\nDistance to link 1 center of mass (r1)\n0.33 m\nDistance to link 2 center of mass (r2)\n0.1375 m\nInertia of link 1\n0.0038 kg · m2\nInertia of link 2\n0.002193 kg · m2\nMass of link 1\n0.223 kg\nMass of link 2\n0.348 kg\nMass of sliding block\n0.795 kg\nMotor friction coefﬁcient\n0.0047\nB. Experiments\nThe control problem that we consider is tracking of an\nangular speed reference through torque control of the sys-\ntem. Due to the nonlinear behavior of the system and large\ninﬂuence of friction on its dynamics, this requires a nonlinear\ncontrol signal that is sensitive to external inﬂuences, making\nit a challenging task for a PID controller to achieve high\nperformance. All results shown in this section are averaged\nover at least ﬁve runs with mean and min-max shown. The\nSAC policy, which is implemented customly to allow for the\nalgorithmic changes, is trained using the mean squared error\n(MSE) of the instantaneously measured deviation from the\nrequired rotational velocity, 1\n2(ωd −ω)2. In the discussion of\nthe results, the Mean Absolute Error (MAE) is used to allow to\ninterpret the algorithm’s properties intuitively as this focuses\nless on the outliers. In the ﬁgures however, both metrics are\nshown.\nThe sampling of the action is kept on for the entire\nexperiment here, since the stochasticity is limited after\nconvergence as described in Section II-B4. The state used\nis composed of the crank’s rotational velocity and the sine\nand cosine of its angle, requiring no extra sensors but the\nencoder standardly used for the base controller. As with any\nRL algorithm, tuning of the hyperparameters is a necessary\nstep to obtain the desired performance for CRRL as well.\nNonetheless, we have found the robustness to hyperparameter\nsettings of SAC with automatic temperature adjustment [28]\nto hold for the CRRL employing SAC as well, with only the\nbatch size and learning rate having a notable effect on the\noutcome, provided no unconventional values for the other\nparameters are set. To illustrate this robustness and to allow\neasy comparison, the same SAC hyperparameters, listed in\nTable II, are used throughout all CRRL experiments in this\npaper. The computation time for 1 epoch on a computer with\na 6-core Intel i7-8700 CPU with 8GB RAM is 0.87 seconds.\nNote that this is only necessary for training the network.\nDuring deployment, only a forward pass of the actor network\nat each timestep is needed, which consists of only 2436\nFLOPS. Section III-B1 discusses the general behavior of a\nCRRL controller followed by a more in-depth examination of\nits different features in the subsequent subsections.\nTABLE II: SAC parameters.\nParameter\nCRRL\nRL\nOptimizer (all networks)\nAdam\nAdam\nLearning rate (all networks)\n3e-4\n1e-5\nDiscount (γ)\n0.97\n0.9\nBatch size (randomly sampled from replay buffer)\n256\n256\nReplay buffer size\n1e6\n1e6\nNumber of hidden layers - Actor network\n2\n2\nNumber of hidden layers - Critic network\n3\n2\nNumber of neurons per hidden layer - Actor network\n32\n32\nNumber of neurons per hidden layer - Critic network\n128\n32\nNonlinearity\nReLU\nReLU\nTarget smoothing coefﬁcient\n0.005\n0.005\n1) Learning process: Fig. 3 shows the performance of a\nrelative CRRL controller with βr = 0.2 and an averagely\nwell tuned PI controller (kp = 1.4, kI = 0.1) as base policy\ntracking a constant angular reference of 60 rpm. This PI\ncontroller, as well as the optimally and poorly tuned ones\nemployed later have been tuned through a grid search on the\nsystem for each reference signal. The grid ranges from 0.1 to\n1.2 and 2.6 for ki and kp respectively in steps of 0.1. These\nbounds are determined empirically. ωd denotes the desired and\nω the actual crank angular velocity. The general form of the\nlearning process displayed by this conﬁguration is illustrative\nfor all other conﬁgurations mentioned hereafter.\nThe performance of a PI controller on the slider crank setup\nvaries slightly from run to run despite lab conditions with\nlimited external disturbances. Therefore each experiment starts\nwith an initial run-in phase in which only the PI controller acts\non the system to benchmark the results (the blue shaded region\nin Fig. 3). This variability of the PI controller’s performance\nover different runs is illustrative for the difﬁculties in optimally\ntuning a controller for all conditions. Note that the reward\nshown is not the instantaneous reward of one timestep, but\nthe average reward over one revolution. As such, the error\noffset of the PI controller does not illustrate a steady-state\nerror, but the inability of the PI controller to compensate for\nthe non-linearities throughout one revolution which amounts to\nthe mean error shown. After this phase, the residual controller\nis activated at epoch 65. For our experiments, an epoch is\ndeﬁned as 500 measurement points sent to the PC. In the\nbeginning of training, the Q-values are small and the entropy\nterm dominates in the objective [27]. This leads to actions\nthat are sampled nearly uniformly from the distribution output\nby the SAC policy [27], resulting in random and therefore\npossibly unsafe controller outputs. In Fig. 1 one can see that\nthe drop in performance caused by the exploration phase in\nCRRL is, although unavoidably still present, strongly limited.\nThis intuitively corresponds to assuming a robustness of the\nbase controller to limited disturbances, i.e. the residual actions\nconstrained by the parameter βr, as can often be assumed of an\nindustrially employed controller and conﬁrms the theoretical\nﬁndings of Section II-C. The effect of constraining the residual\nactions to the base policy is examined further in Section\nIII-B3. After the exploration phase, the performance of the RL\nalgorithm improves until it converges to a residual policy that\ngives a stable improvement of approximately 13% compared\nto the base PI controller.\n2) RL benchmark: A standalone SAC controller was pre-\ntrained to mimic an optimally tuned base controller and\n0\n50\n100\n150\n200\n250\n300\nEpoch\n0.00\n1.00\n2.00\n3.00\n4.00\n5.00\n6.00\n|ωd −ω| [rad/s]\nAverage MAE over one revolution\nOur method\nRL\n0\n100\n200\n300\n0.32\n0.36\n0.40\nCRRL curve\nFig. 3: CRRL and pretrained RL controller loss, averaged over\n5 runs with min and max shown. During the blue and red\nshaded region respectively, only the base controller acts.\nsubsequently trained to further optimize the learning objective.\nA suitable hyparparameter combination, listed in Table II,\nwas obtained after a two week period of manual tuning.\nThe resulting loss curve is shown on Fig. 3 with the red\nshaded region indicating the pretraining period. For this best\nperforming set of parameters, the algorithm converges to a\nMAE of approximately 0.55 rad/s after 200 epochs. During\nexploration, the error unavoidably reaches up to 6.28 rad/s\noccasionally, i.e. standstill, due to the full freedom of the SAC\nalgorithm. This indicates that the residual controller beneﬁts\nfrom the base controller both for limiting the unsafe behaviour\nduring exploration and converging to a high performing policy.\nHaving reached the limit of performance possible by tuning\nthe base algorithm, it also demonstrates the modularity of\nCRRL to existing controllers without needing system speciﬁc\nadaptations such as policy constraints, speciﬁc reward shaping\n... which would be required to further decrease the standalone\nRL algorithm’s error.\n3) The importance of constraining residual actions: In\nrelative CRRL (3), the actions are constrained to a tube with\nwidth a percentage relative to the base controller’s output. Fig.\n4 compares CRRL with relative constraints to a residual policy\nwith absolute constraints (2), tracking a constant angular\nvelocity reference of 60 rpm. To ease the comparison, the\nbounds of the absolute tube are expressed as a percentage\nof the largest base controller output during a cycle without\nresidual controller. The base controller is an averagely tuned\nPI controller (kp = 1.4, kI = 0.1). Fig. 4 on the left shows\nthe average performance improvement after convergence of\nthe CRRL controller relative to the average PI controller\nperformance during the ﬁrst 65 epochs. The right side shows\na boxplot of the relative decrease in performance of all epochs\nafter activating the residual controller where the reward was\nlower than the average PID reward. The dotted red line\nindicates the largest negative deviation by the PID controller\nitself from its average reward during the ﬁrst 65 epochs. These\nconventions are maintained for the remainder of the results.\nFor a residual controller within an absolute tube of 20%,\nboth the decrease in performance during exploration and the\nimprovement after convergence are substantially larger than\nfor a relative tube of 20%, as is to be expected due to the\nincreased freedom given to the residual controller. In the\nnext paragraph, this trade-off is discussed in more detail. To\ncompare with the improvement obtained by a relative bound,\nwe experimentally found a residual controller with an absolute\ntube of 7.5% to have a similar decrease in performance as a\nrelatively constrained controller of 20%. The ﬁnal increase\nin performance however reaches only approximately 54% of\nthe increase reached by a relatively constrained controller.\nThis indicates that the relative constraint method of (3) is\nadvantageous to achieve a higher optimality while maintaining\nsafe operation. For all experiments throughout the remainder\nof this paper, this method is employed.\nβa = 0.2\nβa = 0.075\nβr = 0.2\nConstraints\n0\n10\n20\n30\n26.1\n6.5\n12.0\n0\n16\n32\n48\n45.9\n20.6\n26.4\nPerformance improvement\nafter convergence [%]\nMAE\nMSE\nβa = 0.2\nβa = 0.075βr = 0.2\nConstraints\n0\n10\n20\nMAE\nMSE\n0\n23\n47\nPerformance decrease\nduring exploration [%]\nFig. 4: Comparison of the performance after convergence\nand during exploration for a CRRL controller with different\nconstraints.\nThe relative constraints are regulated by the parameter βr.\nFig. 5 shows the inﬂuence of βr by comparing both the con-\nvergence and exploration performance of a CRRL controller\nrelative to the allowed deviation. As a larger allowed deviation\ngives the residual controller more freedom, determining βr\nis a trade-off between the eventual increase in performance\nattainable and the decrease possible during exploration.\n0.05\n0.1\n0.2\n0.5\nβr\n0\n10\n20\n30\n2.3\n5.4\n12.0\n24.9\n0\n15\n30\n44\n8.6\n14.8\n26.4\n47.4\nPerformance improvement\nafter convergence [%]\nMAE\nMSE\n0.05\n0.1\n0.2\n0.5\nβr\n0\n10\n20\nMAE\nMSE\n0\n20\n41\nPerformance decrease\nduring exploration [%]\nFig. 5: Comparison of the performance after convergence and\nduring exploration for different values of βr, i.e. the allowed\ndeviation from the base controller’s signal caused by the\nresidual controller.\n4) CRRL performance compared to base policy perfor-\nmance: A desirable key feature of CRRL is to not reduce\nthe performance of an already optimal controller. The best\nPI parameters for the current setup and a constant angular\nvelocity reference of 60 rpm were found to be kp = 2.4\nand kI = 0.1. Fig. 6 compares both the performance after\nconvergence and during exploration of a CRRL controller\nrelative to the base policy for this PI controller as well as\nan average (kp = 1.4, kI = 0.1) and a poorly (kp = 0.2,\nkI = 0.1) tuned one.\nThe CRRL controller attains a similar increase in perfor-\nmance for both non-optimally tuned base controllers and it\nsucceeds in improving the best base policy by 5% on average.\nNote that the reinforcement learning algorithm sometimes\ndoesn’t succeed in ﬁnding an improvement for this base\ncontroller, in which case it learns to give nearly 0 output to not\ndecrease the performance as the min-max interval line indi-\ncates. This result allows to deploy the CRRL methodology on\n0.2\n1.4\n2.4\nkp\n0\n5\n10\n15\n11.6\n12.0\n5.0\n0\n9\n19\n29\n19.3\n26.4\n9.6\nPerformance improvement\nafter convergence [%]\nMAE\nMSE\n0.2\n1.4\n2.4\nkp\n0\n20\n40\nMAE\nMSE\n0\n40\n81\nPerformance decrease\nduring exploration [%]\nFig. 6: Comparison of the performance after convergence\nand during exploration for a relative CRRL controller with\ndifferent base controllers.\ncontrollers that are likely to be optimal as well, as the residual\ncontroller learns to refrain from adapting if no improvement\nis found, instead of diminishing performance. We also want\nto emphasize that no additional hyperparameter tuning was\ncarried out to optimize the results for each experiment. For\nall conﬁgurations, the median of the decrease in performance\ncaused by the exploration is less than the maximum decrease\nthat was observed for the base controller itself. Notably for\nboth non-optimally tuned base controllers, the largest observed\nPI performance decrease is larger than or close to 75% of\nthe decreases observed after activating the CRRL controller.\nAlthough the relative decrease in performance increases as the\nperformance of the base policy increases, larger deviations are\nseldom and statistical outliers.\nFig. 7 on the left shows the residual actions taken by a\nCRRL controller after convergence with an optimally tuned\nbase PI controller as well as the bounds in between which\nit is allowed to operate. The residual policy has learned to\neither add a speciﬁc signal, maximally reinforce or maximally\ncounteract the base policy to increase optimality. As the\nconstraints are relative, the residual actions are constrained\nto 0 at times when the base controller outputs zero as well.\nThe regions where the residual signal reaches the limits of\nits allowed deviation suggest that a looser bound might result\nin a more optimal policy after convergence. This is a trade-\noff with the possible performance loss during the exploration\nphase, as investigated in Subsection III-B3. Fig. 7 on the right\nillustrates the base policy’s control signal and the total control\nsignal with the residual added. Note the limited changes to the\nbase signal that result in a 5% performance improvement.\n0\nπ\n2\nπ\n3π\n2\n2π\nCrank angle\n−0.10\n−0.05\n0.00\n0.05\n0.10\nTorque [Nm]\nβru(s)πθ(s)\n±βru(s)\n0\nπ\n2\nπ\n3π\n2\n2π\nCrank angle\n−0.50\n−0.25\n0.00\n0.25\nTorque [Nm]\nu(s)\nuθ(s)\nFig. 7: Left: exemplary residual control signal and its bounds.\nRight: exemplary base (red) and total control signal (blue).\n5) CRRL adaptivity to different references: In Fig. 8, the\nperformance is shown when tracking either a constant refer-\nence of 60 or 90 rpm or a sine reference in function of the\ncrank angle, ωd = 15 sin(ψ) + 60 [rpm], with ψ the crank\nangle. The base controllers are the ones that were found to\nhave the best performance for each reference, respectively\n(kp = 2.4, kI = 0.1), (kp = 0.7, kI = 0.1) and (kp = 1.4,\nkI = 0.1). Note that for the higher reference speed, excessive\nshaking of the setup caused by a high kp gain limits the practi-\ncally feasible values. The ﬁgure shows how the same residual\ncontroller succeeds in learning improvements for different base\ncontrollers tracking different references, demonstrating the\nadaptivity to operating conditions. Note that for the sinusoidal\nreference, which is more challenging for the base PI controller\nto track, only some outlier cycles over all runs have a larger\ntemporary decrease in performance than the largest decrease\nobserved when only the base controller acts. In line with pre-\nvious results, the difference in base controller for the constant\nreferences causes a large difference in relative decrease in\nperformance observed during the exploration phase.\n60 rpm\n90 rpm\nSine\nωd\n0\n3\n7\n10\n5.0\n6.4\n5.0\n0\n6\n12\n18\n9.6\n15.5\n13.9\nPerformance improvement\nafter convergence [%]\nMAE\nMSE\n60 rpm 90 rpm\nSine\nωd\n0\n20\n40\nMAE\nMSE\n0\n40\n81\nPerformance decrease\nduring exploration [%]\nFig. 8: Comparison of the performance after convergence and\nduring exploration for a relative CRRL controller tracking\ndifferent references.\nIV. CONCLUSIONS AND FUTURE WORK\nIn this paper, we proposed CRRL, a method to improve\nthe optimality of conventional controllers that are robust, but\nsuffer from suboptimality when faced with uncertain operat-\ning conditions. In CRRL, a reinforcement learning algorithm\nlearns corrective adaptations to the conventional controller’s\noutput, directly from the controlled system’s operating data.\nBy adding the adaptation residually on top of and constraining\nit by the base controller’s output, the robustness of the latter is\nleveraged to limit the possible performance decrease during the\nlearning process of the residual agent. The Lyapunov stability\ntheory was used to establish safety guarantees of the proposed\nmethod even when worst-case conditions are met for a broad\nclass of mechatronic systems. The performance of CRRL was\nvalidated experimentally on a slider crank setup tracking a\nspeed reference with a PI base controller. The method is shown\nto improve the performance for different conﬁgurations of the\nbase controller tracking different references substantially after\nconvergence, while maintaining safe operation at all times.\nThe structure of the constraints applied on the residual agent’s\nactions was investigated and it was shown experimentally that\nconstraints relative to the base controller’s output are beneﬁcial\nto limit the possible performance decrease during training\nwhile achieving a substantial improvement after convergence.\nIn future work we will focus on expanding the method with\nadaptive, state-dependent constraints for the residual agent. A\nconsideration is that even though the CRRL architecture limits\nthe performance decrease during exploration greatly, the nearly\nuniformly sampled actions during exploration inherent to SAC\nwould nonetheless be too brisk for e.g. a position controlled\nsystem. As a next step, we will explore how to design the\nexploration process for different situations to increase the\ngeneral applicability of CRRL.\nAPPENDIX A\nTRACKING STABILITY OF MECHANICAL SYSTEMS\nWe start by recollecting some ingredients from Lyapunov\nStability theory.\nDeﬁnition 1. A strict Lyapunov function V for ˙x = f(x)\nat x∗is quadratic if it is analytic and there exist three\npositive constants α1, α2 and α3 so that α1∥e∥2 ≤V ≤\nα2∥e∥2 and ˙V ≤−α3∥e∥2, e = x−x∗, ∀t ∈R+ and x ∈X.\nTheorem 3. Consider a disturbed dynamical system ˙x =\nA(t, x)x + d(t). Let V satisfy deﬁnition 1 at 0 for the undis-\nturbed system (i.e. d ≡0) on D and assume there exists a posi-\ntive constant α4 such that |∇xV d| ≤α4∥x∥then the response\nof the disturbed system from any initial condition x(t0) ∈D\nis bounded by ∥x(t)∥≤e−1\n2\nα3\nα2 t α2\n√α1 ∥x(t0)∥2 + α2α4\nα1α3 .\nProof. See reference [36], Theorem 2.\nFurther we are interested in the stability of the closed-\nloop system dynamics of a mechanical system governed by\nM(q)¨q + B(q, ˙q) ˙q + g(q) = u, M ≻0. We use the feedback\nlinearisation control policy, u = M(q)¨qr +B(q, ˙q) ˙qr +g(q)−\nkP ˙e −kIe, where e = q −qr is deﬁned as the tracking error\nw.r.t. a reference qr ∈D. Further assume that the reference\nsatisﬁes ∥qr∥≤θ0, ∥˙qr∥≤ω0, ∥¨qr∥≤α0. Finally we assume\nthe system is initialised so that ∥e(0)∥2 + ∥˙e(0)∥2 ≤ϵ and\n∥q(0)∥2 ≤η. The closed-loop system dynamics are given.\nNote that the dynamics correspond with those in Theorem 3.\n\u0014\n˙e\n¨e\n\u0015\n=\n\u0014\n0\nI\n−kIM −1\n−M −1 (B + kPI)\n\u0015 \u0014\ne\n˙e\n\u0015\n(11)\nThe following theorem allows to make claims about the\nstability of the system in (11).\nTheorem 4. Let r : R 7→J be a smooth curve, with ∥qr∥≤\nθ0 and ∥˙qr∥≤ω0. Further, let V : X 7→R be deﬁned as\nV = 1\n2x⊤P(q)x, P(q) =\n\u0014\nλkII\n1\n2kPI\n1\n2kPI\nλM(q)\n\u0015\n(12)\nwith x = (e, ˙e) and where λ >\nk2\nP\n2kIν(M), then V is quadratic\nfor system (11) with α1 = νη(P), α2 = µη(P), α3 =\nνη( ˇQ) where ˇQ =\nkP\n2µη(M)\n\"\nν(M)kI\n2µη(M)\n1\n2kP\n1\n2kP\n(λ −1)ν(M)\n#\nif kI >\nµη(M)µη(B)2ω2\n0\n2ν(M)2\nand kP >\n2µη(Lq)ω0\n1−1\nλ −\nr\nϵ2µη(P )µη(B)\nλ2νη(P )νη(M)\n. The norms are\ndeﬁned as µη(A) = sup∥q∥≤η sup∥x∥=1 |x⊤A(q)x|, µ(A) =\nlimη→∞µη(A) and νη(A) = inf∥q∥≤η inf∥x∥=1 x⊤A(q)x.\nNote that from the deﬁnition of the norms it follows\nthat sup∥q∥≤η x⊤A(q)x ≤µη(A)∥x∥2 and νη(A)∥x∥2 ≤\ninf∥q∥≤η x⊤A(q)x.\nProof. See reference [36], Proposition 10, Corollary 11 and\nProposition 12.\nAPPENDIX B\nPROOF OF THEOREM 1 AND 2\nProof. We can analyse the tracking stability of an absolute\nCRRL agent deﬁned as u = −kP ˙e −kIe −βaπθ(q, ˙q, . . . ) by\nanalysing the disturbance term\nd = −\n\u0014\n0\n¨qr + M −1 (B(q, ˙q) ˙qr + g(q) + βaπθ(q, ˙q, . . . ))\n\u0015\n(13)\nand adopting the Lyapunov function from Theorem 4\n∇xV d =\nx⊤P\n\u0014\n0\n¨qr + M −1 (B(q, ˙q) ˙qr + g(q) + βaπθ(q, ˙q, . . . ))\n\u0015\n. . .\n≤\nq\n1\n4k2\nP + λ2µη(M)2(α0 + µη(B)ω0+g0+βa\nνη(M)\n)∥x∥\n(14)\nwhere g0 = sup ∥g(q)∥. Further we rely on the results from\nTheorem 3, Deﬁnition 1 and Theorem 4 to show that ∥x(t)∥≤\ne−1\n2\nα3\nα2 t α2\n√α1 ∥x(0)∥2 + ∆, limt→∞∥x(t)∥≤∆with α1, α2\nand α3 as in Theorem 4, α4 as deﬁned above and\n∆=\nµη(P )\nr\n1\n4 k2\nP +λ2µη(M)2\n\u0012\nα0+\n1\nνη(M) (µη(B)ω0+g0+βa)\n\u0013\nνη(P )νη( ˇ\nQ)\n(15)\nwhere µη(B)\n=\nsup∥q∥≤η sup∥x∥=1 sup∥y∥=1 ∥B(q, x)y∥.\nNote that it follows that ∥B(q, x)y∥≤µη(B)∥x∥∥y∥.\nAnalogously we can analyse the tracking stability of a\nrelative CRRL agent deﬁned as u = −(kP ˙e + kIe) (1 +\nβrπθ(q, ˙q, . . . )). Here we could either perform a similar\nanalysis as in the previous proof, identifying the associated\ndisturbance and determining the corresponding value for α4.\nHowever, this disturbance would depend on the error and\nmay therefore be overly conservative. Alternatively, we could\ntry and analyse the undisturbed closed-loop dynamics corre-\nsponding with the control policy u = M(q)¨qr + B(q, ˙q) ˙qr +\ng(q) −(kP ˙e + kIe) (1 + βrπθ(q, ˙q, . . . )). Then the closed-loop\nsystem dynamics are as in (11) but with k′\nP and k′\nI substituted\nfor kP and kI respectively, where k′\nP = (1 + βrπθ)kP and\nk′\nI = (1 + βrπθ)kI. If we can derive bounds for γ, kP and kI\nin this context and so that V , as deﬁned in theorem 4, satisﬁes\ndeﬁnition 1 for the resulting closed-loop system, we can rely\nagain on theorem 3 to establish a bound on the error ∥x∥.\nProof. The derivative of the Lyapunov function along the\nmotion of the system is given by\n˙V = −1\n2kPx⊤\n\u0014\nk′\nIM −1\n1\n2k′\nPM −1\nk′\nIM −1\nλ(1 + 2πθ)\n\u0015\nx −1\n2kP(λ −1)∥˙e∥2\n−1\n2e⊤M −1B ˙e −βrπθkPkIe⊤˙ekP + λ˙e⊤\u0010\n1\n2 ˙M −B\n\u0011\n˙e\n(16)\nFor the last term it holds that λ˙e⊤\u0010\n1\n2 ˙M −B\n\u0011\n˙e\n=\nλ 1\n2 ˙e⊤Lq( ˙qr)˙e ≤λµη(Lq)ω0∥˙e∥2 where Lq(x) =\n˙M(qx) −\nPn\ni=1 xi∂qiM and qx : R 7→D with ˙qx = x. After factoring\nout, ∥˙e∥2, for the middle term\n−1\n2kP∥˙e∥2\u0010\nλ −1 + e⊤M −1B(q, ˙e)\n˙e\n∥˙e∥2 + . . .\ne⊤M −1B(q, ˙qr)\n˙e\n∥˙e∥2 + 2βrπθkI e⊤˙e\n∥˙e∥2\n\u0011\n< −1\n2kP∥˙e∥2 × . . .\n\u0012\nλ −1 −2βrkI −ϵ\nq\nµη(P )µη(B)\nνη(P )νη(M)\n\u0013\n−1\n2kPe⊤M −1B(q, ˙qr)˙e\n(17)\nsince\ne⊤M −1B(q, ˙e)\n˙e\n∥˙e∥2\n≤\n−ϵ\nq\nµη(P )µη(B)\nνη(P )νη(M)\nand\n2βrπθkI e⊤˙e\n∥˙e∥2 ≤−2βrkI.\nSubstituting these inequalities into the equation for ˙V yields\n˙V ≤−1\n2kPx⊤\n \u0014\nk′\nIM −1\n1\n2k′\nPM −1\nk′\nIM −1\nλ(1 + 2βrπθ)\n\u0015\n−. . .\n\u0014\n0\n1\n2M −1B(q, ˙qr)\n1\n2B(q, ˙qr)⊤M −1\n0\n\u0015 !\nx −1\n2kP∥˙e∥⊤× . . .\n \n1 −1\nλ −2βrkI\nλ\n−\nr\nϵ2µη(P )µη(B)\nλ2νη(P )νη(M) −2µη(Lq)ω0\nkP\n!\n(18)\nThe second term is < 0 if kP >\n2µη(Lq)ω0\n1−1\nλ −2βrkI\nλ\n−\nr\nϵ2µη(P )µη(B)\nλ2νη(P )νη(M)\n.\nFinally we can rewrite the matrix difference as\n\u0014 1\n2\n\u0000k′\nIM −1 −1\n2B⊤M−2B\n\u0001\n0\n0\n0\n\u0015\n+\n\u0014 1\n2k′\nIM −1\n1\n2k′\nPM −1\nk′\nIM −1\nλ(1 + 2βrπθ)\n\u0015\n+\n\u0014 1\n2B⊤M −1\nI\n\u0015 \u0002 1\n2M −1B\nI\n\u0003\n(19)\nThe\nﬁrst\nmatrix\nis\npositive\nsemi-deﬁnite\nif\nkI\n>\nµη(M)µη(B)2ω2\n0\n2(1−βr)ν(M)2\nand the second if λ >\nk2\nP\n2(1−2βr)kIν(M).\nREFERENCES\n[1] S. Kim, “Moment of inertia and friction torque coefﬁcient identiﬁcation\nin a servo drive system,” IEEE Transactions on Industrial Electronics,\nvol. 66, no. 1, pp. 60–70, 2018.\n[2] X. Wang, W. Wang, L. Li, J. Shi, and B. Xie, “Adaptive control of\ndc motor servo system with application to vehicle active steering,”\nIEEE/ASME Transactions on Mechatronics, vol. 24, no. 3, pp. 1054–\n1063, 2019.\n[3] D. Papageorgiou, M. Blanke, H. H. Niemann, and J. H. Richter,\n“Robust backlash estimation for industrial drive-train systems-theory and\nvalidation,” IEEE Transactions on Control Systems Technology, vol. 27,\nno. 5, pp. 1847–1861, 2018.\n[4] T. B. Sch¨on, A. Wills, and B. Ninness, “System identiﬁcation of\nnonlinear state-space models,” Automatica, vol. 47, no. 1, pp. 39–49,\n2011.\n[5] D. Q. Mayne, J. B. Rawlings, C. V. Rao, and P. O. Scokaert, “Con-\nstrained model predictive control: Stability and optimality,” Automatica,\nvol. 36, no. 6, pp. 789–814, 2000.\n[6] T.-Y. Kuc and W.-G. Han, “An adaptive pid learning control of robot\nmanipulators,” Automatica, vol. 36, no. 5, pp. 717–725, 2000.\n[7] T. Yang, N. Sun, Y. Fang, X. Xin, and H. Chen, “New adaptive control\nmethods for n-link robot manipulators with online gravity compensation:\nDesign and experiments,” IEEE Transactions on Industrial Electronics,\n2021.\n[8] V. Adetola and M. Guay, “Robust adaptive mpc for constrained uncertain\nnonlinear systems,” International Journal of Adaptive Control and\nSignal Processing, vol. 25, no. 2, pp. 155–167, 2011.\n[9] T. Yang, N. Sun, and Y. Fang, “Adaptive fuzzy control for a class\nof mimo underactuated systems with plant uncertainties and actuator\ndeadzones: Design and experiments,” IEEE Transactions on Cybernetics,\n2021.\n[10] M. Athans, “The role and use of the stochastic linear-quadratic-gaussian\nproblem in control system design,” IEEE transactions on automatic\ncontrol, vol. 16, no. 6, pp. 529–552, 1971.\n[11] A. Mesbah, “Stochastic model predictive control: An overview and\nperspectives for future research,” IEEE Control Systems Magazine,\nvol. 36, no. 6, pp. 30–44, 2016.\n[12] J. L¨ofberg, Minimax approaches to robust model predictive control, vol.\n812.\nLink¨oping University Electronic Press, 2003.\n[13] D. Lim´on, I. Alvarado, T. Alamo, and E. F. Camacho, “Robust tube-\nbased mpc for tracking of constrained linear systems with additive\ndisturbances,” Journal of Process Control, vol. 20, no. 3, pp. 248–260,\n2010.\n[14] Z. Yan, X. Le, and J. Wang, “Tube-based robust model predictive control\nof nonlinear systems via collective neurodynamic optimization,” IEEE\nTransactions on Industrial Electronics, vol. 63, no. 7, pp. 4377–4386,\n2016.\n[15] Y. Wang, F. Gao, and F. J. Doyle III, “Survey on iterative learning\ncontrol, repetitive control, and run-to-run control,” Journal of Process\nControl, vol. 19, no. 10, pp. 1589–1600, 2009.\n[16] R. Chi, Z. Hou, S. Jin, and B. Huang, “An improved data-driven point-\nto-point ilc using additional on-line control inputs with experimental\nveriﬁcation,” IEEE Transactions on Systems, Man, and Cybernetics:\nSystems, vol. 49, no. 4, pp. 687–696, 2017.\n[17] U. Rosolia, X. Zhang, and F. Borrelli, “Data-driven predictive control\nfor autonomous systems,” Annual Review of Control, Robotics, and\nAutonomous Systems, vol. 1, pp. 259–286, 2018.\n[18] F. L. Lewis and D. Vrabie, “Reinforcement learning and adaptive\ndynamic programming for feedback control,” IEEE circuits and systems\nmagazine, vol. 9, no. 3, pp. 32–50, 2009.\n[19] G. Dulac-Arnold, D. Mankowitz, and T. Hester, “Challenges of real-\nworld reinforcement learning,” arXiv preprint arXiv:1904.12901, 2019.\n[20] H. Fu, X. Chen, W. Wang, and M. Wu, “Mrac for unknown discrete-time\nnonlinear systems based on supervised neural dynamic programming,”\nNeurocomputing, vol. 384, pp. 130–141, 2020.\n[21] J. Na, Y. Lv, K. Zhang, and J. Zhao, “Adaptive identiﬁer-critic-based\noptimal tracking control for nonlinear systems with experimental vali-\ndation,” IEEE Transactions on Systems, Man, and Cybernetics: Systems,\n2020.\n[22] M.-B. Radac and T. Lala, “Robust control of unknown observable\nnonlinear systems solved as a zero-sum game,” IEEE Access, vol. 8,\npp. 214 153–214 165, 2020.\n[23] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu,\nand N. de Freitas, “Sample efﬁcient actor-critic with experience replay,”\narXiv preprint arXiv:1611.01224, 2016.\n[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[25] T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea,\nE. Solowjow, and S. Levine, “Residual reinforcement learning for robot\ncontrol,” in 2019 International Conference on Robotics and Automation\n(ICRA), pp. 6023–6029.\nIEEE, 2019.\n[26] T. Silver, K. Allen, J. Tenenbaum, and L. Kaelbling, “Residual policy\nlearning,” arXiv preprint arXiv:1812.06298, 2018.\n[27] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” arXiv preprint arXiv:1801.01290, 2018.\n[28] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Ku-\nmar, H. Zhu, A. Gupta, P. Abbeel et al., “Soft actor-critic algorithms\nand applications,” arXiv preprint arXiv:1812.05905, 2018.\n[29] G. Schoettler, A. Nair, J. Luo, S. Bahl, J. A. Ojea, E. Solowjow,\nand S. Levine, “Deep reinforcement learning for industrial inser-\ntion tasks with visual inputs and natural rewards,” arXiv preprint\narXiv:1906.05841, 2019.\n[30] M. Zanon and S. Gros, “Safe reinforcement learning using robust mpc,”\nIEEE Transactions on Automatic Control, 2020.\n[31] W. De Groote, E. Kikken, E. Hostens, S. Van Hoecke, and G. Creve-\ncoeur, “Neural network augmented physics models for systems with\npartially unknown dynamics: Application to slider-crank mechanism,”\narXiv preprint arXiv:1910.12212, 2019.\n[32] Z. Gao, R. S. Colby, L. Turner, and B. Leprettre, “Filter design for\nestimating parameters of induction motors with time-varying loads,”\nIEEE Transactions on Industrial Electronics, vol. 58, no. 5, pp. 1518–\n1529, 2010.\n[33] M. Li, R. Foss, K. A. Stelson, E. J. Barth et al., “Design, dynamic\nmodeling, and experimental validation of a novel alternating ﬂow\nvariable displacement hydraulic pump,” IEEE/ASME Transactions on\nMechatronics, vol. 24, no. 3, pp. 1294–1305, 2019.\n[34] R. Eren, G. Ozkan, and M. Karahan, “Comparison of heald frame motion\ngenerated by rotary dobby and crank & cam shedding motions,” Fibres\nand textiles in eastern Europe, vol. 13, no. 4, p. 78, 2005.\n[35] E. Zheng and X. Zhou, “Modeling and simulation of ﬂexible slider-\ncrank mechanism with clearance for a closed high speed press system,”\nMechanism and Machine theory, vol. 74, pp. 10–30, 2014.\n[36] D. E. Koditschek, Quadratic Lyapunov functions for mechanical systems.\nYale University, 1987.\nTom Staessens obtained the M.Sc. in elec-\ntromechanical and control engineering from\nGhent University, Belgium, in 2018. In Septem-\nber 2018, he joined the Department of Elec-\ntromechanical, Systems and Metal Engineering,\nwhere he is currently pursuing the Ph.D. degree.\nHis current research interest includes optimal\nand safe control by combining traditional con-\ntrol algorithms and data-driven techniques. Tom\nStaessens is afﬁliate member of Flanders Make,\nthe strategic research centre for the manufactur-\ning industry in Flanders, Belgium.\nTom Lefebvre received the M.Sc. in electrome-\nchanical and control engineering from Ghent\nUniversity, Belgium, in 2015. From 2016 till 2019\nhe pursued the Ph.D. degree at the same Uni-\nversity. Since 2019 he is a postdoctoral research\nassistant. His main research interest includes\nfoundational work on numerical methods for\nstochastic optimal control, gradient and stochas-\ntic trajectory optimization, and uncertainty quan-\ntiﬁcation. Tom Lefebvre is afﬁliate member of\nFlanders Make, the strategic research centre for\nthe manufacturing industry in Flanders, Belgium.\nGuillaume Crevecoeur received the MSc and\nthe Ph.D. degree in Engineering Physics from\nGhent University in 2004 and 2009, respec-\ntively. In 2009 he became a postdoctoral fellow\nof the Research Foundation Flanders and in\n2014 he was appointed Associate Professor at\nthe Faculty of Engineering and Architecture of\nGhent University. He is active member of Flan-\nders Make, the strategic research center for the\nmanufacturing industry. His research interests\nare the modelling, optimization and control of\nmechatronic and industrial robotic applications.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY"
  ],
  "published": "2021-10-06",
  "updated": "2021-10-06"
}