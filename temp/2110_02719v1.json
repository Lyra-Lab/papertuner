{
  "id": "http://arxiv.org/abs/2110.02719v1",
  "title": "The Information Geometry of Unsupervised Reinforcement Learning",
  "authors": [
    "Benjamin Eysenbach",
    "Ruslan Salakhutdinov",
    "Sergey Levine"
  ],
  "abstract": "How can a reinforcement learning (RL) agent prepare to solve downstream tasks\nif those tasks are not known a priori? One approach is unsupervised skill\ndiscovery, a class of algorithms that learn a set of policies without access to\na reward function. Such algorithms bear a close resemblance to representation\nlearning algorithms (e.g., contrastive learning) in supervised learning, in\nthat both are pretraining algorithms that maximize some approximation to a\nmutual information objective. While prior work has shown that the set of skills\nlearned by such methods can accelerate downstream RL tasks, prior work offers\nlittle analysis into whether these skill learning algorithms are optimal, or\neven what notion of optimality would be appropriate to apply to them. In this\nwork, we show that unsupervised skill discovery algorithms based on mutual\ninformation maximization do not learn skills that are optimal for every\npossible reward function. However, we show that the distribution over skills\nprovides an optimal initialization minimizing regret against\nadversarially-chosen reward functions, assuming a certain type of adaptation\nprocedure. Our analysis also provides a geometric perspective on these skill\nlearning methods.",
  "text": "THE INFORMATION GEOMETRY OF\nUNSUPERVISED REINFORCEMENT LEARNING\nBenjamin Eysenbach1 2\nRuslan Salakhutdinov1\nSergey Levine2 3\n1Carnegie Mellon University,\n2Google Brain,\n3UC Berkeley\nbeysenba@cs.cmu.edu\nABSTRACT\nHow can a reinforcement learning (RL) agent prepare to solve downstream tasks if\nthose tasks are not known a priori? One approach is unsupervised skill discovery, a\nclass of algorithms that learn a set of policies without access to a reward function.\nSuch algorithms bear a close resemblance to representation learning algorithms\n(e.g., contrastive learning) in supervised learning, in that both are pretraining\nalgorithms that maximize some approximation to a mutual information objective.\nWhile prior work has shown that the set of skills learned by such methods can\naccelerate downstream RL tasks, prior work offers little analysis into whether\nthese skill learning algorithms are optimal, or even what notion of optimality\nwould be appropriate to apply to them. In this work, we show that unsupervised\nskill discovery algorithms based on mutual information maximization do not\nlearn skills that are optimal for every possible reward function. However, we\nshow that the distribution over skills provides an optimal initialization minimizing\nregret against adversarially-chosen reward functions, assuming a certain type of\nadaptation procedure. Our analysis also provides a geometric perspective on these\nskill learning methods.\n1\nINTRODUCTION\nThe high sample complexity of reinforcement learning (RL) algorithms has prompted a large body of\nprior work to study pretraining of RL agents. During the pretraining stage, the agent collects unsu-\npervised experience from the environment that is not labeled with any rewards. Prior methods have\nused this pretraining stage to learn representations of the environment that might assist the learning\nof downstream tasks. For example, some methods learn representations of the observations [32, 52]\nor representations of the dynamics model [11, 22, 53]. In this work, we focus on methods that learn\na set of potentially-useful policies, often known as skills [1, 12, 20, 41, 51]. That is, the learned\nrepresentation corresponds to a reparametrization of policies. The aim of this unsupervised pretrain-\ning is to learn skills that, when a reward function is given, can quickly be combined or composed\nto maximize this reward. Prior work has demonstrated that this general approach does accelerate\nlearning downstream RL [1, 12, 20]. However, prior work offers little analysis about when and where\nsuch methods are provably effective. Even simple questions, such as what it means for a set of skills\nto be optimal, remain unanswered.\nAlgorithms for unsupervised skill learning are conceptually related to the representation learning\nmethods used to improve supervised learning [4, 21, 26, 28, 44, 60]. Both typically maximize a\nlower bound on mutual information, and the learned representations are often combined linearly\nto solve downstream tasks [28, 44]. However, whereas prior work in supervised learning has\nprovided thorough analysis of when and where these representation learning methods produce useful\nfeatures [31, 37, 56], there has been comparatively little analysis into when unsupervised skill learning\nmethods produce skills that are useful for solving downstream RL tasks.\nIn this paper, we analyze when and where existing skill learning methods based on mutual information\nmaximization are (or are not) optimal for preparing to solve unknown, downstream tasks. On the\none hand, we show that the skills learned by these methods are not complete, in that they cannot\nbe used to represent the solution to every RL problem. This result implies that using the learned\nskills for hierarchical RL may result in suboptimal performance, and suggests new opportunities\n1\narXiv:2110.02719v1  [cs.LG]  6 Oct 2021\nfor better skill learning algorithms. One the other hand, we show that existing methods acquire the\noptimal initialization for learning downstream tasks, under some assumptions about how adaptation\nis performed. To the best of our knowledge, this is the ﬁrst result showing that unsupervised skill\nlearning methods are optimal in any sense.\nOur analysis also illuminates a number of properties of these methods. For example, we show that\nevery skill is optimal for some reward function, and we provide a nontrivial upper bound on the\nnumber of unique skills learned. This result implies that these methods cannot learn an inﬁnite\nnumber of unique skills, and instead will learn duplicate copies of some skills. The key to our analysis\nis to view RL algorithms and skill learning algorithms as geometric operations (see Fig. 1). Points\ncorrespond to distributions over states, and the set of all possible distributions is a convex polytope\nthat lies on a probability simplex. We show that all reward-maximizing policies lie at vertices of\nthis polytope and that maximizing mutual information corresponds to solving a facility assignment\nproblem on the simplex.\nThe main contribution of this paper is a proof that skill learning algorithms based on mutual informa-\ntion are optimal for minimizing regret against unknown reward functions, assuming that adaptation is\nperformed using a certain procedure. Our proof of optimality relies on certain problem assumptions,\nleaving the door open for future skill learning algorithms to perform better under different problem\nassumptions. This contribution provides a rigorous notion of what it means for an unsupervised RL\nalgorithm to be optimal, and also answers additional questions about unsupervised skill learning\nalgorithms, such as whether the skills correspond to reward-maximizing policies and how many\nunique skills will be learned.\n2\nPRIOR WORK\nIn the unsupervised RL setting, an agent interacts with the environment without access to a reward\nfunction, with the aim of learning some representation of the environment that will assist in learning\ndownstream tasks. Prior work has proposed many approaches for this problem, including learning a\ndynamics model [11, 22, 53], learning compact representations of observations [32, 52], performing\ngoal-conditioned RL without hand-speciﬁed rewards [7, 52], doing pure exploration [25, 35, 41, 45,\n51, 54] or learning collections of skills [1, 6, 12, 20, 23, 55, 59]. These prior methods are similar in\nthat they all learn representations, with different methods learning representation of observations,\ndynamics, or policies. While each approach works well in some setting (see Laskin et al. [33] for\na recent benchmark), the precise connection between these pretraining methods and success on\ndownstream tasks remains unclear. We will focus on unsupervised skill learning methods.\nThe problem of unsupervised pretraining for RL has also been studied in the RL theory community [2,\n39, 40], where it is referred to as the reward-free setting. The algorithms proposed in these prior\nworks use a version of goal-conditioned RL for pretraining. However, the aim of these methods is\ndifferent from ours: these methods aim to collect a sufﬁcient breadth of data, whereas we focus on\nlearning good representations.\nOur analysis uses the dual of the RL problem, which is written in terms of the policy’s state\ndistribution [46, Eq. 6.9.2]. In contrast to prior work that proposes algorithms for solving the dual\nproblem [42, 43, 58], we use the dual problem purely as an analytical tool. Our paper is similar\nto prior work that uses a geometric perspective to analyze RL algorithms [5, 10]. Whereas those\nprior works study the geometry of value functions and representations, ours will study the geometry\nof policies.\n3\nPRELIMINARIES\nWe focus on inﬁnite-horizon MDPs with discrete states S and actions A, initial state distribu-\ntion p0(s0), dynamics p(st+1 | st, at), and discount factor γ ∈[0, 1]. We assume a Marko-\nvian policy π(a | s), and deﬁne the discounted state occupancy measure of this policy as\nρπ(s) = (1 −γ) P∞\nt=0 γtP π\nt (s), where P π\nt (s) is the probability that policy π visits state s at time\nt. We deﬁne C to be the set of state marginal distributions that are feasible under the environment\ndynamics. The RL objective can be expressed using the reward function r(s) and the state marginal\ndistribution:\n(1 −γ)Eπ\n\" ∞\nX\nt=0\nγtr(st)\n#\n= Eρπ(s) [r(s)] .\n2\nThe factor of 1 −γ accounts for the fact that the sum of discount factors 1 + γ + γ2 + · · · =\n1\n1−γ .\nWithout loss of generality, we focus on state-dependent reward functions; action-dependent reward\nfunctions can be handled by modifying the state to include the previous action. While our analysis\nwill ignore function approximation error, it applies to any MDP, including MDPs where observations\ncorrespond to features (e.g., activations of a frozen neural network) rather than original states.\n3.1\nUNSUPERVISED SKILL LEARNING ALGORITHMS\nPrior skill learning algorithms learn a policy πθ(a | s, zinput) with parameters θ and conditioned on an\nadditional input zinput ∼p(zinput). Let ρπθ(s | zinput) denote the state marginal distribution of policy\nπθ(a | s, zinput). We will focus on methods [12, 16] that maximize the mutual information between\nthe representation zinput and the states s visited by policy πθ(a | s, zinput):\nmax\nθ,p(zinput) I(s; zinput) ≜Ep(zinput)Eρπθ (s|zinput)[log ρπθ(s | zinput) −log ρπθ(s)].\n(1)\nWe will refer to such methods as mutual information skill learning (MISL), noting that our analysis\nmight be extended to other mutual information objectives [1, 41, 51, 55]. Because these methods\nhave two learnable components, θ and p(zinput), analyzing them directly can be challenging. Instead,\nwe will use a simpliﬁed abstract model of skill learning where both the policy parameters θ and\nthe latent variable zinput are part of a single representation, z = (θ, zθ). Then, we can deﬁne a skill\nlearning algorithm as learning a single distribution p(z):\nmax\np(z) I(s; z) ≜Ep(z)Eρ(s|z)[log ρ(s | z) −log ρ(s)].\n(2)\nWe can recover Eq. 1 by choosing a distribution (z) that factors as p(z = (θ, zinput) = δ(θ)p(zinput).\nWhereas skills learning algorithms are often viewed as optimizing individual skills, this simpliﬁed\nabstract model lets us think about these algorithms as assigning different probabilities to different\npolicies a probability of zero to almost every skill. We will refer to the small number of policies that\nare given non-zero probability as the “skills.” We will show that, in general, the learned skills fail to\ncover all possible optimal policies. However, we show that the distribution over skills that maximizes\nmutual information, when converted into a distribution over states, provides the best state distribution\nfor learning policies to optimize an adversarially-chosen reward function (under some assumptions).\n4\nOVERVIEW OF MAIN RESULTS\nOur main results analyze the relationship between the skills learned via the mutual information\nobjective in Equation 2 and optimal policies. We will show that, in the general case, maximizing\nmutual information alone does not result in skills that cover the set of all optimal policies, no matter\nhow many skills are learned. Perhaps surprisingly, when the number of skills is large enough,\nadditional skills no longer differentiate from the existing ones, despite some potentially optimal\npolicies not being covered. However, we will show that the the distribution over skills learned by\nMISL is optimal in a different sense: when this distribution over skills is converted into a distribution\nover states, it provides a prior over states that is close to the state distributions of reward-maximizing\npolicies. This prior is optimal in the sense that, when combined with a particular adaptation procedure,\nit minimizes regret against the worst-case reward function. We formally state this result below:\nTheorem 4.1. The optimal initialization for adapting to an unknown reward function is the average\nstate marginal given by maximizing mutual information (Eq. 2):\nmin\nρ(s)∈C\nmax\nr(s)∈R|S| ADAPTATIONOBJECTIVE(ρ(s), r(s)) = max\np(z) I(s; z),\nwhere the adaptation objective is deﬁned as\nADAPTATIONOBJECTIVE(ρ(s), r(s)) ≜\nmin\nρ∗(s)∈C max\nρ+(s)∈C Eρ+(s)[r(s)] −Eρ∗(s)[r(s)]\n|\n{z\n}\nregret\n+DKL(ρ∗(s) ∥ρ(s)).\n(3)\nMoreover, the optimal prior can be written is the average state distribution of the skills: ρ∗(s) =\nEp∗(z)[ρ(s | z)].\nNote that the optimal prior, ρ∗(s) is not hard to compute: it corresponds to sampling one of the skills\nz ∼p∗(z) and then execute the corresponding policy, π(a | s, z).\n3\nFigure 1: Polytope of achievable policies: (Left) For a 3-state MDP, the set of feasible state marginal\ndistributions is described by a triangle [(1, 0, 0), (0, 1, 0), (0, 0, 1)] in R3. We plot the state occupancy measure\nof 6 different policies. (Center) We plot those same policies, but remove the coordinate axes to improve visual\nclarity. (Right) For any convex combination of valid state occupancy measures, there exists a Markovian policy\nthat has this state occupancy measure. Thus, policy search happens inside a convex polytope.\nTable 1: Notation for analysis.\nr(s)\nReward function.\nρ(s)\nInitialization.\nρ+(s)\nOptimal state marginal distribution for\nreward r(s). Used to deﬁne regret.\nρ∗(s)\nOptimal state marginal distribution af-\nter adaptation, which maximizes reward\nand minimizes information cost.\nThe regularized regret objective, ADAPTA-\nTIONOBJECTIVE, can be interpreted in several\ndifferent ways. The regularization can be moti-\nvated as preventing overﬁtting to a limited num-\nber of samples of a noisy reward function. We\ncould also interpret this term as reﬂecting the\ncost of adapting from to the new task, but un-\nder a somewhat idealized model of the learning\nprocess. Precisely, the adaptation objective is\nequivalent to performing one step of natural gra-\ndient descent [3, 30, 36], where the underlying\nmetric is the state marginal distribution [48, Theorem 1]. This natural gradient is different from\nthe standard natural policy gradient [30], which does not depend on the environment dynamics and\nis much easier to estimate. This equivalence suggests that our notion of adaptation is idealized: it\nassumes that this adaptation step can be performed exactly, without considering how data is collected,\nhow state marginal distributions are estimated, or how the underlying policy parameters should\nbe updated. Nonetheless, this paper is (to the best of our knowledge) the ﬁrst to provide any for-\nmal connection between the mutual information objective optimized by unsupervised skill learning\nalgorithms and performance on downstream tasks.\n5\nRL AS GEOMETRY ON A SIMPLEX\nIn order to prove our main result and further analyze the kinds of policies learned by unsupervised\nskill learning methods, we will ﬁrst introduce a geometric perspective on RL. This perspective will\nallow us to better understand how unsupervised skill learning relates to the space of potentially\noptimal policies (for state-dependent reward functions).\nWe visualize policies by their discounted state occupancy measure, |S|-dimensional point lying on\nthe probability simplex (see Fig. 1). Note that this is a distribution over states, not actions, so a policy\nthat chooses actions uniformly will not necessarily lie at the center of the simplex. Of course, not all\nstate marginals are feasible, as the MDP dynamics might preclude certain distributions. We will be\ninterested in where skill learning algorithms place skills on this probability simplex, and whether these\nskills are optimal for learning downstream tasks. Note that not all points on the simplex correspond\nto realizable policies, and multiple policies could map to the same state occupancy measure.\n5.1\nWHICH STATE MARGINALS ARE ACHIEVABLE?\nUnderstanding which state marginals are achievable is important for analyzing what unsupervised\nskill learning algorithms do, and how the learned skills relate to downstream state-dependent reward\nfunctions. While the set of achievable state marginal distributions can be described by a set of linear\nequations [46, Eq. 6.9.2] (see Appendix A.1), we offer a different and complementary description\nbased on the following property [61, Theorem 2.8]:\n4\nProposition 1. Given a set of feasible state marginals, any convex combination is also feasible.\nThis property is illustrated in Fig. 1 (right): if we plot the state marginals for six policies, then\nany point inside the convex hull of those state marginals is also feasible. That is, there exists a\n(Markovian) policy that achieves that state marginal distribution. In general, we can obtain the set\nof all feasible state marginals by taking the convex hull of the state marginals of all deterministic\npolicies. We will refer to this set of achievable state marginals as a polytope. Every vertex of the state\nmarginal polytope will contain a deterministic policy,1 and the state marginal of every policy can be\nrepresented as a convex combination of the state marginals at the vertices.\n5.2\nWHICH POLICIES ARE OPTIMAL?\nFigure 2: Maximizing Rewards: We vi-\nsualize reward functions as vectors starting\nat the origin. Maximizing expected return\ncorresponds to maximizing the dot product\nbetween the state marginal distribution and\nthis reward vector.\nWe now analyze where reward-maximizing policies lie\nwithin this state marginal polytope. Again taking a geomet-\nric perspective, we visualize reward functions as vectors\nstarting from the origin, so that the expected return objec-\ntive can be expressed as the inner product between the state\nmarginal distribution and the reward vector (see Fig. 2).\nThis visualization helps illustrate two facts about the state\nmarginal distributions of reward-maximizing policies:\nFact 1. For every state-dependent reward function, among\nthe set of policies that maximize that reward function is\none that lies at a vertex of the state marginal polytope.\nFact 2. For every vertex of the state marginal polytope,\nthere exists a reward function for which that vertex corre-\nsponds to a reward-maximizing policy.\nFact 1 is an application of the maximum principle [49,\nChpt. 32]; Fact 2 is an application of the strong separation\ntheorem [27, Theorem 3.2.2]. These observations suggest\nthat, for the purpose of RL, it is sufﬁcient to search among\npolicies whose state marginals lie at vertices.2 These observations also suggests an objective for\nunsupervised skill learning:\nDeﬁnition 5.1 (Vertex discovery problem). Given a controlled Markov process (i.e., an MDP without\na reward function), ﬁnd the smallest set of policies such that every vertex of the state marginal\npolytope contains at least one policy.\nWhile we are unaware of any tractable algorithm for exactly solving this problem, the next section\ndiscusses how a prior skill learning algorithm provides an efﬁcient approximation to this problem.\n6\nTHE GEOMETRY OF SKILL LEARNING\nWe now analyze unsupervised skill discovery algorithms using the tools discussed in the previous\nsection. We view the skills learned by these methods as a set of points on the state marginal polytope.\nWe will show that the average state distribution of the skills provides an optimal initialization for\nsolving downstream tasks using a certain adaptation procedure. However, we then show that the skills\ndo not cover every vertex, and are thus insufﬁcient for representing the solution to every downstream\nRL task. Before presenting these results, we introduce a classic result from information geometry,\nwhich is the keystone to our analysis.\n6.1\nWHERE DO SKILLS LEARNED BY MUTUAL INFORMATION FALL ON THE POLYTOPE?\nWe analyze where the skills learned by MISL lie on the state marginal polytope by dissecting the\nmutual information objective. The mutual information can be written as the average divergence\n1Note, however, that some deterministic policies may not lie at vertices of this polytope.\n2While it is well known that any reward function has a deterministic optimal policy [46, Sec. 4.4], our\nvisualization adds nuance to the story, suggesting that searching in the space of all deterministic policies can be\nwasteful, at least in the case of rewards that depend only on state.\n5\n(a) Examples where MISL recovers 3 = |S| skills.\n(b) Example where MISL learns\nonly two unique skills.\nFigure 3: Information Geometry of Skill Learning: Maximizing mutual information is equivalent to mini-\nmizing the maximum divergence between a prior distribution over states p(s) (green square) and any achievable\nstate marginal distribution (Lemma 6.5). The opaque circles are skills discovered by MISL, and the dashed line\ndenotes state marginals that are equidistant from the green square.\nbetween each policy’s state distribution ρ(s | z) and the average state distribution, ρ(s):\nI(s; z) = Ep(z) [DKL(ρ(s | z) ∥ρ(s))] .\nTo optimize this mutual information, a skill learning algorithm would choose to assign higher\nprobability to some policies and lower probability (perhaps zero) to other policies. In particular,\nthis mutual information suggests that skill learning algorithms should assign higher probabilities to\npolicies whose state distributions are further from the average state distribution. In fact, if one policy\nhas a state distribution that is further to the average state distribution than another policy, then the\nskill learning algorithm should assign a probability of zero to this second policy. Thus, the net result\nof optimizing mutual information is to place probability only on policies that are as far away from\ntheir average as possible:\nLemma 6.1. The MISL skills must lie at vertices of the state marginal polytope, and thus are optimal\nfor some reward functions.\nThis result, which we conﬁrm experimentally3 in Fig. 3, answers the question of where the skills\nlearned by mutual information fall on the polytope. Recalling that the vertices of the state marginal\npolytope correspond to optimal policies (Fact 1), this result implies that the skills learned by MISL\nare a subset of the set of optimal policies. Thus,\nCorollary 6.1.1. MISL skills are all optimal for some downstream state-dependent reward functions.\n6.2\nHOW MANY UNIQUE SKILLS ARE LEARNED?\nA simple way to use skills acquired during pretraining to solve a downstream RL task is to simply\nsearch among the skills to ﬁnd the one that achieves the highest reward. So, it is natural to ask whether\nskill learning algorithms learn skills that are optimal for every possible reward function. Indeed, this\nis a very natural notion of what it might mean for a skill learning algorithm to be optimal. Prior\nwork motivates the use of larger dimensional z or continuous-valued z, arguing that these design\ndecisions will allow the method to learn a larger (perhaps inﬁnite) number of skills [12, 24], based on\nthe intuition that these skills may be optimal for solving downstream tasks. However, it has remained\nunclear whether such design decisions are useful and whether the goal of acquiring an inﬁnite or\nexponential number of skills is even possible. Among prior skill learning methods, we are unaware\nof prior work that characterizes the number of skills that these methods will learn. In this section we\nshow that existing skill learning algorithms based on mutual information are not optimal in this sense.\nOur proof will follow a simple counting argument based on the number of unique skills.\nTo count the number of unique skills, we start by analyzing the distance between each skill and the\nstate distribution averaged over skills, measured as a KL divergence between state marginals. We can\nuse the KKT conditions to prove that all skills with non-zero support (i.e., for which p(z) > 0) have\nan equal divergence from the average state marginal:\n3Code to reproduce: https://github.com/ben-eysenbach/info_geometry/blob/main/experiments.ipynb\n6\nLemma 6.2. Let ρ(s | z) be given. Let p(z) be the solution to maximizing mutual information (Eq. 2)\nand let ρ(s) = Ep(z)[ρ(s | z)] be the average state marginal. Then the following holds:\np(z) > 0 =⇒DKL(ρ(s | z)∥ρ(s)) = max\nz∗DKL(ρ(s | z∗)∥ρ(s)) ≜dmax.\nWe visualize this result in Fig. 3: the dashed line denotes a state marginals ρ(s | z) that are equidistant\nfrom the average state marginal (green square). Note that all skills where MISL places non-zero\nprobability mass (denoted by solid orange dots instead of transparent orange dots) lie on this dashed\ncircle. This Lemma allows us to count how many unique skills MISL acquires, under a certain\nregularity condition.\nLemma 6.3. Assume that the vertices of the state marginal polytope are not concyclic, as deﬁned\nusing the KL divergence. Then MISL will recover at most |S| distinct skills.\nProof. The proof follows a simple counting argument. Without loss of generality, we only consider\nvertices of the state marginal polytope as candidate skills (Lemma 6.1). The location of the average\nstate marginal (ρ(s)) on the probability simplex is determined by the location of the skills and the\ndistance to the skills. The average state marginal is a vector in |S|-dimensional space with |S| −1\ndegrees of freedom; we subtract one to account for the constraint that the state marginal sum to 1.\nConsider adding skills one by one.\nWhen we add a new skill (i.e., set p(z) > 0 for some\nz), we add an additional constraint to the location of this average state marginal; namely, that\nDKL(ρ(s | z) ∥p(z)) = dmax. The value of dmax adds one additional degree of freedom. Thus, after\nadding (|S| −1) + 1 skills, the average state marginal is fully speciﬁed. Adding additional skills\nwould make the average state marginal ill-deﬁned, as it would have to violate one of the distance\nconstraints. Thus, we can have at most |S| unique skills.\nMISL need not learn exactly |S| skills. In some MDPs, such as the one illustrated in Fig. 3b, MISL\nacquires only 2 unique skills for a 3-state MDP. If an additional skill (at a vertex) were added, then\nthe average distance from the skills to the average state marginal would decrease, worsening the\nmutual information objective. Additionally, when the regularity assumption is violated, there could\nbe more than |S| skills that all have a KL divergence of dmax from the initialization ρ(s), in which\ncase MISL could learn more than |S| skills.\nTo summarize, our analysis shows that MISL only acquires optimal policies (i.e., those at vertices\nof the state marginal polytope. However, the fact that there can be up to |A||S| vertices (see\nAppendix A.6) and the fact that MISL acquires at most |S| skills leads us to conclude that MISL does\nnot solve the vertex discovery problem:\nTheorem 6.4. In general, skill learning algorithms that maximize the mutual information I(s; z)\nwill not recover the set of all optimal policies.\nThis result is important for determining how to use the output of an unsupervised skill learning\nalgorithm to solve downstream tasks. One approach to adaptation is to ﬁnd whichever skill receives\nthe highest return. This result indicates that this approach will produce suboptimal performance on a\nlarge number of reward functions. In fact, for every reward maximizing policy which a skill learning\nalgorithm can represent, there can be an exponential number that the skill learning algorithm can fail\nto represent. This result casts doubt on the use of the skills themselves for solving downstream tasks,\nat least those skills learned by the existing generation of skill learning algorithms.\nMight other simple algorithms for skill discovery recover the entire set of optimal policies? We are\ncurrently unaware of any algorithm that does solve the vertex discovery problem. In Appendix A.4,\nwe show that (1) including actions in the mutual information objective or (2) maximizing a collection\nof indicator reward functions also fail to discover all optimal policies.\nUnsupervised skill learning algorithms learn a full distribution over skills. Instead of just looking at\nthe set of policies that are given non-zero probability (i.e., skills), do the numerical values of these\nprobabilities give us useful information to help solve downstream tasks? In the next section, we\ndiscuss how our main result (Theorem 4.1) helps to address this equestion.\n7\n6.3\nAN OPTIMAL INITIALIZATION FOR AN UNKNOWN REWARD FUNCTION\nIn this section we present a proof sketch of our main result, Theorem 4.1, and then discuss several\ninterpretations of this result. Intuitively, the difﬁculty of solving downstream tasks depends on how\nmuch the agent must adapt its initialization to learn the optimal policy for downstream tasks. Different\ninitializations will make some tasks easier and some tasks more difﬁcult.\nIt may seem counterintuitive that unsupervised skill learning algorithms would learn a good ini-\ntialization. Maximizing mutual information pushes skills away from one another, not as ﬁnding a\nskill or policy that is close to every other policy. We now discuss a different perspective, thinking\nabout how the average state distribution over the skills changes during optimization. It turns out that\nmaximizing mutual information corresponds to pulling the average state distribution closer to other\npolicies. Precisely, the average state distribution is pulled closer to the furthest policy:\nLemma 6.5 (Theorem 13.11 from Cover & Thomas [9], based on Gallager [18], Ryabko [50]). Let\nρ(s | z) be given. Maximizing mutual information is equivalent to minimizing the divergence between\nthe average state distribution ρ(s) and the state distribution of the furthest skill:\nmax\np(z) I(s; z) = min\nρ(s) max\nz\nDKL(ρ(s | z) ∥ρ(s))\nThis characterization can be understood as a facility location problem [13] or a 1-center problem [19,\n38]. Since maximizing mutual information learns an average state distribution that is close to other\npolicies, we expect that this average state distribution will be a good initialization for learning\nthese other policies. This result provides the intuition behind our main result, which we present in\nAppendix A.5.\nRelationship with meta-learning.\nWe now discuss a close connection between this result and meta-\nlearning. The pretraining objective (Eq. 3) bears a resemblance to the objective for meta-learning\nalgorithms [17, 34, 47]: both aim to ﬁnd a parameter initialization for which an inner optimization\nproblem can be efﬁciently solved. Whereas these methods assume access to a training distribution,\nskill learning algorithms used for pretraining do not have information about the downstream task.\nMoreover, meta-learning typically studies the average performance at test time, our objective measures\nthe worst-case performance at test time. From this perspective, unsupervised skill learning could be\ninterpreted as optimal pretraining for an idealized downstream adaptation procedure that searches\nover policies in the space of state marginals. Of course, this is an abstraction of practical RL methods,\nwhich must search in the space of policy parameters. This connection suggests that it might be\npossible to design meta-learning algorithms using the tools of mutual information skill learning, but\nthe underlying mutual information objective might need to be modiﬁed so that adaptation can happen\nin the space of policy parameters, rather than state marginal distributions.\n6.4\nIMPLICATIONS FOR FUTURE SKILL LEARNING ALGORITHMS\nOur analysis sheds light on fundamental limitations of skill learning algorithms based on mutual\ninformation. First, as explained in Sec. 6.2, existing algorithms based on mutual information do not\nlearn skills that are sufﬁcient for representing every potentially optimal policy. Future skill learning\nalgorithms that can represent every policy may prove better at adapting to new tasks.\nThe second limitation is the adaptation procedure for which these methods produce the optimal\ninitialization, under an assumption of how adaptation is performed. The natural gradient used for\nadaptation is not the standard natural policy gradient [30], which is performed in policy parameter\nspace, but rather the natural gradient with respect to the state marginal distribution. This is a very\nstrong (hypothetical) reinforcement learning algorithm, because it explores directly in the space of\nstate marginals. This gradient implicitly depends on the environment dynamics, and computing this\ngradient would likely require exploring the environment to determine how changes in a policy’s\nparameters affect changes in the states visited. Said in other words, our analysis suggests that existing\nunsupervised skill learning methods are optimal for a somewhat unrealistic and idealized model of how\ndownstream policy learning is performed. This suggests an opportunity to develop signiﬁcantly more\npowerful algorithms that serve as effective initializations for more realistic adaptation procedures,\nsuch as those studied in meta-RL [15, 29, 57], and this is likely an exciting and promising direction\nfor future work.\n8\n6.5\nLIMITATIONS OF ANALYSIS\nIn our analysis, we used a simpliﬁed model of skill learning algorithms, one that viewed policy\nparameters and latent codes as one monolithic representation, with different skills using entirely\ndifferent sets of parameters. Practical implementations of skill learning algorithms are less expressive,\nwhich may change the geometry of the state marginal polytope. For example, a limited policy\nclass may be insufﬁcient to represent some state marginal distributions, introducing “holes” in the\nstate marginal polytope, or splitting the polytope into two disjoint regions. It remains to be seen\nwhether these practical implementations actually learn fewer unique skills, and how these expressivity\nconstraints affect the average state distribution.\n7\nCONCLUSION\nWe have used a geometric perspective to answer a number of open questions in the study of skill dis-\ncovery algorithms and unsupervised RL. We also showed that the skills learned by unsupervised skill\nlearning algorithms are not sufﬁcient for maximizing every downstream reward function. However,\nwe do show that the average state distribution of the skills provides an initialization that is optimal\nfor solving unknown downstream tasks using a particular adaptation procedure. Our analysis showed\nthat the number of skills is bounded by the number of states, suggesting that scaling these methods\nto learn more skills will hit a theoretical upper bound. Taken together, we believe that this analysis\nsheds light on what prior methods are doing, when and where they should work, and what limitations\ncould be addressed in the next generation of skill learning algorithms.\nAcknowledgements.\nWe thank Abhishek Gupta, Alex Alemi, Archit Sharma, Jonathan Ho, Michael Janner,\nRuosong Wang, and Shane Gu for discussions and feedback on the paper. This material is supported by the\nFannie and John Hertz Foundation and the NSF GRFP (DGE1745016).\nREFERENCES\n[1] Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery\nalgorithms. arXiv preprint arXiv:1807.10299, 2018.\n[2] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and\nrepresentation learning of low rank mdps. arXiv preprint arXiv:2006.10814, 2020.\n[3] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):251–276,\n1998.\n[4] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron\nCourville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and Andreas Krause\n(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings\nof Machine Learning Research, pp. 531–540. PMLR, 10–15 Jul 2018. URL https://proceedings.\nmlr.press/v80/belghazi18a.html.\n[5] Marc Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux,\nDale Schuurmans, Tor Lattimore, and Clare Lyle. A geometric perspective on optimal representations for\nreinforcement learning. Advances in neural information processing systems, 32:4358–4369, 2019.\n[6] Víctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giró-i Nieto, and Jordi Torres.\nExplore, discover and learn: Unsupervised discovery of state-covering skills. In International Conference\non Machine Learning, pp. 1317–1327. PMLR, 2020.\n[7] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Ben-\njamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised ofﬂine reinforcement\nlearning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\n[8] John Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey Levine.\nSelf-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In\nInternational Conference on Machine Learning, pp. 1009–1018. PMLR, 2018.\n9\n[9] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications\nand Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.\n[10] Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G. Bellemare. The value\nfunction polytope in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),\nProceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of\nMachine Learning Research, pp. 1486–1495. PMLR, 09–15 Jun 2019. URL https://proceedings.\nmlr.press/v97/dadashi19a.html.\n[11] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual fore-\nsight: Model-based deep reinforcement learning for vision-based robotic control.\narXiv preprint\narXiv:1812.00568, 2018.\n[12] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning\nskills without a reward function. In International Conference on Learning Representations, 2018.\n[13] Reza Zanjirani Farahani and Masoud Hekmatfar. Facility location: concepts, models, algorithms and case\nstudies. Springer Science & Business Media, 2009.\n[14] Eugene A Feinberg and Adam Shwartz. Handbook of Markov decision processes: methods and applications,\nvolume 40. Springer Science & Business Media, 2012.\n[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017.\n[16] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforcement\nlearning. arXiv preprint arXiv:1704.03012, 2017.\n[17] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel\nprogramming for hyperparameter optimization and meta-learning. In International Conference on Machine\nLearning, pp. 1568–1577. PMLR, 2018.\n[18] Robert G Gallager. Source coding with side information and universal coding. 1979.\n[19] RS Garﬁnkel, AW Neebe, and MR Rao. The m-center problem: Minimax facility location. Management\nScience, 23(10):1133–1142, 1977.\n[20] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv preprint\narXiv:1611.07507, 2016.\n[21] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for\nunnormalized statistical models. In Proceedings of the thirteenth international conference on artiﬁcial\nintelligence and statistics, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.\n[22] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\n[23] Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, and Volodymyr\nMnih. Fast task inference with variational intrinsic successor features. In International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=BJeAHkrYDS.\n[24] Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an\nembedding space for transferable robot skills. In International Conference on Learning Representations,\n2018.\n[25] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum entropy\nexploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019.\n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9729–9738, 2020.\n[27] Jean-Baptiste Hiriart-Urruty and Claude Lemaréchal. Fundamentals of convex analysis. Springer Science\n& Business Media, 2004.\n[28] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler,\nand Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.\nIn International Conference on Learning Representations, 2018.\n[29] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and Nicolas\nHeess. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424, 2019.\n10\n[30] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems, 14, 2001.\n[31] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical\nreview E, 69(6):066138, 2004.\n[32] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for\nreinforcement learning. In International Conference on Machine Learning, pp. 5639–5650. PMLR, 2020.\n[33] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto,\nand Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. 2021.\n[34] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differen-\ntiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 10657–10665, 2019.\n[35] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdinov.\nEfﬁcient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.\n[36] James Martens. New insights and perspectives on the natural gradient method. Journal of Machine\nLearning Research, 21:1–76, 2020.\n[37] David McAllester and Karl Stratos. Formal limitations on the measurement of mutual information. In\nInternational Conference on Artiﬁcial Intelligence and Statistics, pp. 875–884. PMLR, 2020.\n[38] Edward Minieka. The m-center problem. Siam Review, 12(1):138–139, 1970.\n[39] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstraction\nand provably efﬁcient rich-observation reinforcement learning. In International conference on machine\nlearning, pp. 6961–6971. PMLR, 2020.\n[40] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free represen-\ntation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035, 2021.\n[41] Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically\nmotivated reinforcement learning. Advances in Neural Information Processing Systems, 28:2125–2133,\n2015.\n[42] Oﬁr Nachum and Bo Dai.\nReinforcement learning via fenchel-rockafellar duality.\narXiv preprint\narXiv:2001.01866, 2020.\n[43] Andrew Y Ng, Ronald Parr, and Daphne Koller. Policy search via density estimation. In NIPS, pp.\n1022–1028, 1999.\n[44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n[45] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by\nself-supervised prediction. In International conference on machine learning, pp. 2778–2787. PMLR, 2017.\n[46] Martin L Puterman. Markov decision processes. Handbooks in operations research and management\nscience, 2:331–434, 1990.\n[47] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit\ngradients. Advances in Neural Information Processing Systems, 32:113–124, 2019.\n[48] Garvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. IEEE Transactions\non Information Theory, 61(3):1451–1457, 2015.\n[49] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press, 2015.\n[50] Boris Yakovlevich Ryabko. Coding of a source with unknown but ordered probabilities. Problems of\nInformation Transmission, 15(2):134–138, 1979.\n[51] Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment–an introduction. In Guided\nSelf-Organization: Inception, pp. 67–114. Springer, 2014.\n[52] Max Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon\nHjelm, Philip Bachman, and Aaron Courville. Pretraining representations for data-efﬁcient reinforcement\nlearning. arXiv preprint arXiv:2106.04799, 2021.\n11\n[53] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.\nPlanning to explore via self-supervised world models. In Hal Daumé III and Aarti Singh (eds.), Proceedings\nof the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine\nLearning Research, pp. 8583–8592. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.\npress/v119/sekar20a.html.\n[54] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy\nmaximization with random encoders for efﬁcient exploration. arXiv preprint arXiv:2102.09430, 2021.\n[55] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware\nunsupervised discovery of skills. In International Conference on Learning Representations, 2019.\n[56] Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information\nestimators. In International Conference on Learning Representations, 2019.\n[57] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles\nBlundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint\narXiv:1611.05763, 2016.\n[58] Tao Wang, Michael Bowling, and Dale Schuurmans. Dual representations for dynamic programming and\nreinforcement learning. 2007 IEEE International Symposium on Approximate Dynamic Programming and\nReinforcement Learning, pp. 44–51, 2007.\n[59] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr\nMnih. Unsupervised control through non-parametric discriminative rewards. In International Conference\non Learning Representations, 2018.\n[60] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 3733–3742, 2018.\n[61] Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy.\nCarnegie Mellon University, 2010.\nA\nPROOFS AND ADDITIONAL ANALYSIS\nA.1\nMATHEMATICAL DEFINITION OF THE STATE MARGINAL POLYTOPE\nWe provide a formal description of the constraint set. Precisely, the set of achievable state marginal\ndistributions are those for which the state-action marginal distribution ρ(s, a) satisﬁes the following\nlinear constraints [46, Eq. 6.9.2]:\nX\na′∈A\nρ(s′, a′) = (1 −γ)p1(s′) + γ\nX\ns∈S,a∈A\np(s′ | s, a)ρ(s, a)\n∀s′ ∈S\nρ(s, a) ≥0\n∀s ∈S, a ∈A.\nThe constraint that ρ be a valid probability distribution (i.e., P\ns,a ρ(s, a) = 1) is redundant with the\nconstraints above. These constraints can also be written in matrix form as (˜I −γP)⃗ρs,a = (1 −γ)⃗p1,\nwhere ˜I is a binary matrix that sums the action probabilities for each state (i.e., ˜I⃗ρs,a = ⃗ρs), P is the\ntransition matrix, and ⃗p1 is the initial state distribution.\nA.2\nADDITIONAL INTUITION FOR THE STATE MARGINAL POLYTOPE\nFirst, let two policies π1(a | s) and π2(a | s) be given and consider their corresponding state\nmarginals, ρ1(s) and ρ2(s). We illustrate these marginals in Fig. 1 (center). Any state marginal\non the line between the two state marginals ρ1 and ρ2 is also achievable. We can express all such\nstate marginals as ρλ = λρ1 + (1 −λ)ρ2. One way to construct the policy that achieves the state\nmarginal ρλ is to form a non-Markovian mixture policy: sample i ∼BERNOULLI(λ) at the start of\neach episode and then sample actions a ∼πi(a | s) for each step of that episode. By construction,\nthis mixture policy will have a state occupancy measure of ρλ.\n12\nIt turns out that there also exists a Markovian policy πλ that achieves the state marginal ρλ:\nπλ(a | s) = λ(s)π1(a | s) + (1 −λ(s))π2(a | s)\nwhere\nλ(s) ≜\nλρ1(s)\nλρ1(s) + (1 −λ)ρ2(s)\n(4)\nWhile the existence proof is somewhat involved [61, Theorem 2.8][14, Theorem 6.1], the construction\nof the policy is surprisingly simple. First, note that it is sufﬁcient to ensure ρλ(s, a) = λρ1(s, a) +\n(1 −λ)ρ2(s, a). Then, we apply Bayes’ rule to determine the corresponding policy:\nπλ(a | s) = ρλ(s, a)\nρλ(s)\n= λρ1(s, a) + (1 −λ)ρ2(s, a)\nλρ1(s) + (1 −λ)ρ2(s)\n= λπ1(a | s)ρ1(s) + (1 −λ)π2(a | s)ρ2(s)\nλρ1(s) + (1 −λ)ρ2(s)\n= λ(s)π1(a | s) + (1 −λ(s))π2(a | s)\nwhere\nλ(s) ≜\nλρ1(s)\nλρ1(s) + (1 −λ)ρ2(s).\nBy induction, one can also show that convex combinations of multiple state marginals also yields\nachievable state marginal distributions:\nFact 3. The convex hull of achievable state marginal distributions is also achievable.\nWe illustrate this observation in Fig. 1 (right). The construction of the corresponding policy is\nanalogous to Eq. 4.\nA.3\nPROOF OF LEMMA 6.1\nWe now prove Lemma 6.1:\nProof. If p(z) were non-zero for a policy where the state marginal ρ(s | z) did not lie at a vertex,\nthen the mutual information objective could be improved by shifting p(z) to place probability mass\non the vertex rather than interior point ρ(s | z).\nA.4\nALTERNATIVE STRATEGIES ALSO FAIL TO DISCOVERY ALL VERTICES\nIndicator reward functions.\nAnother simple method for skill discovery, based in prior work [39,\nSec. 3.1], is to deﬁne a sparse reward function for visiting a particular state and action: rs,a(s′, a′) ≜\n1(s′ = s, a′ = a). This method also fails to learn all vertices of the state marginal polytope, as we\nshow in the following worked example.\nWe deﬁne the dynamics for a 3-state, 2-action MDP, set γ = 0.5, and deﬁne the initial state distribution\nto be uniform over the three states. This counterexample has deterministic dynamics, represented by\narrows below:\n1\n3\n2\nNone of the policies that are optimal for reward functions {rs,a} are optimal for the reward r(s, a) =\n1(s ∈{1, 2}). The intuition behind this counterexample is that there are multiple ways to maximize\nthis reward function: remaining at state 1 and remaining at state 2. However, none of the optimal\npolicies for {rs,a)} employ this strategy; rather, all these policies deterministically attempt to repeat\na single transition.\nIncluding actions in the mutual information (I((s, a); z)).\nPrior work on skill learning has\nconsidered many alternative deﬁnitions of the mutual information objective, experimenting with\nconditioning on actions [12], initial states [20], and entire trajectories [8]. We refer the reader\nto Achiam et al. [1] for a survey of these methods. Consider, for instance, the objective I((s, a); z).\nOur reasoning from Lemma 6.3 suggests that this alternative objective would result in learning at\nmost |S| · |A| skills, more than the |S| skills learned when maximizing I(s; z). This objective results\n13\n(a) I(s; z)\n(b) I((s, a); z)\nFigure 4: Counterexample 1: Maximizing I((s, a); z) does not result in discovering more vertices\nof the state marginal polytope.\nin discovering vertices of the state-action marginal polytope. However, these additional skills do\nnot necessarily cover additional vertices of the state marginal polytope. We now present a worked\nexample where maximizing I((s, a); z) only covers 3 of 6 vertices, the same number as covered by\nmaximizing I(s; z).\nWe describe Counterexample 1 from Sec. 6 in more detail. This example has 3 states, 4 actions, and 7\nskills. The matrix describing the state-action marginals p(s, a | z) is given as follows:\nPsa =\n\n\n0.0716829\n0.18424628\n0.1795034\n0.13672623\n0.00295358\n0.09510334\n0.03845856\n0.05906211\n0.08669371\n0.01741351\n0.02568876\n0.10246764\n0.11750427\n0.14607\n0.11480248\n0.00287273\n0.00954288\n0.10701075\n0.02102705\n0.02261918\n0.14991443\n0.23005368\n0.06343457\n0.01514798\n0.13386153\n0.06171002\n0.09080011\n0.16650628\n0.05625721\n0.06271223\n0.10353768\n0.09575361\n0.02849315\n0.00634354\n0.13433545\n0.05968919\n0.05926209\n0.10606773\n0.04541879\n0.05518233\n0.14630736\n0.12098995\n0.05692926\n0.0745452\n0.04428352\n0.12954613\n0.0261651\n0.13530253\n0.13704367\n0.01333569\n0.10392952\n0.04119388\n0.11826303\n0.12777541\n0.12092377\n0.0703527\n0.08563638\n0.08155578\n0.04348018\n0.05650998\n0.06520597\n0.00354202\n0.11542116\n0.09079959\n0.14099273\n0.05765374\n0.04477772\n0.03235234\n0.07636344\n0.00824066\n0.09351822\n0.27113241\n0.20516233\n0.19498386\n0.03909264\n0.11229717\n0.06929778\n0.09957651\n0.02888635\n0.08654659\n0.00456412\n0.10995985\n0.0328871\n0.0167457\n\n\nThe matrix describing the state marginals is computed as follows:\nPa = Psa\n\n\n1.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n1.\n0.\n0.\n1.\n\n\nWe illustrate the skills found by maximizing I(s; z) and I((s, a); z) in Fig. 4. Note that including the\nactions in the mutual information does not result in discovering more vertices of the state marginal\npolytope.\n14\nA.5\nPROOF OF THEOREM 4.1\nProof. Our proof proceeds in two steps. First, we will solve the adaptation problem, assuming that\nboth the reward function r(s) and the initialization ρ(s) are given. We then substitute the optimal\npolicy ρ∗(s) into the pretraining problem and solve for the optimal initialization ρ(s).\nFirst, we solve for the optimal policy ρ∗(s) of the adaptation step. Using calculus of variations, the\noptimal policy can be written as\nρ∗(s) =\nρ(s)er(s)\nR\nρ(s′)er(s′)ds′ .\nWe can then express the cost of adaptation as follows:\nADAPTATIONOBJECTIVE(ρ(s), r(s))\n=\nmax\nρ+(s)∈C Eρ+(s)[r(s)] −Eρ∗(s)[r(s)] + DKL(ρ∗(s) ∥ρ(s))\n=\nmax\nρ+(s)∈C Eρ+(s)[r(s)] −\nR\nr(s)ρ(s)er(s)ds\nR\nρ(s′)er(s′)ds′\n+\nR\nρ(s)er(s)\nR\nρ(s′)er(s′)ds′\n\u0012\n\u0018\u0018\u0018\n\u0018\nlog ρ(s) + r(s) −log\nZ\nρ(s′)er(s′)ds′ −\u0018\u0018\u0018\n\u0018\nlog ρ(s)\n\u0013\nds\n=\nmax\nρ+(s)∈C Eρ+(s)[r(s)] −\u0018\u0018\u0018\u0018\u0018\u0018\u0018\n\u0018\nR\nr(s)ρ(s)er(s)ds\nR\nρ(s′)er(s′)ds′\n+\n\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\nR\nρ(s)er(s)\nR\nρ(s′)er(s′)ds′ r(s)ds −log\nZ\nρ(s)er(s)ds\n=\nmax\nρ+(s)∈C Eρ+(s)[r(s)] −log\nZ\nρ(s)er(s)ds.\nWe can note write the pretraining problem (Eq. 3) as follows:\nmin\nρ(s)∈C\nmax\nr(s)∈R|S| ADAPTATIONOBJECTIVE(ρ(s), r(s)) = min\nρ(s)∈C\nmax\nr(s)∈R|S|\nmax\nρ+(s)∈C Eρ+(s)[r(s)] −log\nZ\nρ(s)er(s)ds.\nWe now aim to solve this optimization problem for ρ(s). We start by noting that the max operator is\ncommutative, so we can swap the order of the maximization of r(s) and ρ+(s). That is, instead of\nthe adversary ﬁrst choosing an reward function and then choosing an optimal policy for that reward\nfunction, we can consider the adversary ﬁrst choosing an optimal policy and then choosing a reward\nfunction for which that policy is optimal.\nmin\nρ(s)∈C\nmax\nr(s)∈R|S| ADAPTATIONOBJECTIVE(ρ(s), r(s)) = min\nρ(s)∈C max\nρ+(s)∈C\nmax\nr(s)∈R|S| Eρ+(s)[r(s)] −log\nZ\nρ(s)er(s)ds.\nUsing calculus of variations, we determine that the optimal reward function satisﬁes\nr(s) = log ρ+(s) −log ρ(s) + b,\nwhere b ∈R is a constant scalar. There are many optimal reward functions, one for each choice of b.\nHowever, as the scalar b will cancel out in the next step of our proof, this multiplicity is not a concern.\nSubstituting this worst-case reward function, we can write the overall pretraining objective as\nmin\nρ(s)∈C\nmax\nr(s)∈R|S| ADAPTATIONOBJECTIVE(ρ(s), r(s))\n= min\nρ(s)∈C max\nρ+(s)∈C Eρ+(s)[log ρ+(s) −log ρ(s) + b] −log\nZ\n\b\b\nρ(s)ρ+(s)\n\b\b\nρ(s) ebds\n= min\nρ(s)∈C max\nρ+(s)∈C Eρ+(s)[log ρ+(s) −log ρ(s) + \u0001b] −\b\b\b\nlog eb\n= min\nρ(s)∈C max\nρ+(s)∈C Eρ+(s)[log ρ+(s) −log ρ(s)]\n= min\nρ(s)∈C max\nρ+(s)∈C DKL(ρ+(s) ∥ρ(s)).\nApplying Lemma 6.5 completes the proof.\nA.6\nEXAMPLE WHERE ALL DETERMINISTIC POLICIES ARE UNIQUE VERTICES\nB\nADDITIONAL PLOTS\nFig. 6 plots the same experiment as Fig. 3 in the main text, repeated for different randomly-sampled\ndynamics.\n15\ns1\ns2\ns3\ns4\ns5\na1\na2\na1\na2\na1\na2\na1\na2\nFigure 5\nFigure 6: Information Geometry of Skill Learning: More examples of the skills learned by MISL.\n16\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-10-06",
  "updated": "2021-10-06"
}