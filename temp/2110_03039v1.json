{
  "id": "http://arxiv.org/abs/2110.03039v1",
  "title": "Optimized Recommender Systems with Deep Reinforcement Learning",
  "authors": [
    "Lucas Farris"
  ],
  "abstract": "Recommender Systems have been the cornerstone of online retailers.\nTraditionally they were based on rules, relevance scores, ranking algorithms,\nand supervised learning algorithms, but now it is feasible to use reinforcement\nlearning algorithms to generate meaningful recommendations. This work\ninvestigates and develops means to setup a reproducible testbed, and evaluate\ndifferent state of the art algorithms in a realistic environment. It entails a\nproposal, literature review, methodology, results, and comments.",
  "text": "Universitat Oberta de Catalunya (UOC)\nM´aster Universitario en Ciencia de Datos (Data Science)\nTRABAJO FINAL DE M´ASTER\n´Area: 2\nOptimized Recommender Systems with Deep\nReinforcement Learning\n—————————————————————————–\nAutor: Lucas Farris\nTutor: Luis Esteve Elfau\nProfesor: Jordi Casas Roma\n—————————————————————————–\nBarcelona, October 8, 2021\narXiv:2110.03039v1  [cs.IR]  6 Oct 2021\nCr´editos/Copyright\nEsta obra est´a sujeta a una licencia de Reconocimiento - NoComercial - SinObraDerivada\n3.0 Espa˜na de CreativeCommons.\ni\nFICHA DEL TRABAJO FINAL\nT´ıtulo del trabajo:\nOptimized e-commerce recommendations with\ndeep reinforcement learning\nNombre del autor:\nLucas Farris\nNombre del colaborador/a docente:\nLuis Esteve Elfau\nNombre del PRA:\nJordi Casas Roma\nFecha de entrega (mm/aaaa):\nOctober 8, 2021\nTitulaci´on o programa:\nM´aster Universitario en Ciencia de Datos\n´Area del Trabajo Final:\nM2.879 - ´Area 2\nIdioma del trabajo:\nIngl´es\nPalabras clave\ndeep reinforcement learning, recommender systems,\ne-commerce recommendations\nii\nDedication\nThis is for my aunt, Maria Giuliana Farris, may she rest in peace. She helped me get the\ncomputer, and courage, with which these words were typed.\niii\nAcknowledgements\nI would like to thank my advisor, Luis Esteve Elfau, for his guidance, trust, and for helping\nme prioritize my time in this work. Also, to thank the Universitat Oberta de Catalunya and\nits staﬀfor the opportunity.\nFinally I would like to thank my brother Pedro, my mother Lilian, and my beloved Natalia,\nfor supporting me throughout the years.\niv\nAbstract\nRecommender Systems have been the cornerstone of online retailers. Traditionally they were\nbased on rules, relevance scores, ranking algorithms, and supervised learning algorithms, but\nnow it is feasible to use reinforcement learning algorithms to generate meaningful recommenda-\ntions. This work investigates and develops means to setup a reproducible testbed, and evaluate\ndiﬀerent state of the art algorithms in a realistic environment. It entails a proposal, literature\nreview, methodology, results, and comments.\nResumen\nEl Sistema de Recomendaciones es parte importante de un comercio electr´onico. Tradicional-\nmente estos sistemas se basaban en algoritmos de reglas, m´etricas de relevancia, clasiﬁcaciones\ny aprendizaje supervisado. Actualmente se pueden usar algoritmos de aprendizaje por refuerzo\npara crear recomendaciones m´as ´utiles. Este trabajo investiga y desarrolla maneras de producir\nun sistema de prueba de agentes, y comparar diferentes algoritmos de ´ultima generaci´on en un\nentorno de simulaci´on basado en datos reales. Se describe una propuesta, una revisi´on de la\nliteratura, la metodolog´ıa de investigaci´on, los resultados obtenidos y comentarios.\nKeywords: deep reinforcement learning, recommender systems, e-commerce recommenda-\ntions\nv\nContents\nAbstract\nv\nIndex\nvi\nList of Figures\nviii\nList of Tables\nx\nGlossary\nxi\nAcronyms\nxi\n1\nIntroduction\n1\n1.1\nTopic Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nPersonal Motivation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.3\nMain and Secondary Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.4\nMethodology\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.5\nProject Schedule\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nLiterature Review\n5\n2.1\nInvestigation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nHistorical Background\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nStandard Deﬁnitions and Techniques\n. . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3.1\nRecommender Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3.3\nReinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.4\nDeep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3.5\nRL-Based Recommender Systems . . . . . . . . . . . . . . . . . . . . . .\n15\n2.4\nCurrent Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.4.1\nDeep Learning And Recommender Systems . . . . . . . . . . . . . . . . .\n16\nvi\nCONTENTS\nvii\n2.4.2\nReinforcement Learning And Recommender Systems\n. . . . . . . . . . .\n17\n2.4.3\nSimulation Environments . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.4.4\nThe ACM RecSys Conference . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3\nMaterials And Methods\n21\n3.1\nLearning Environment Selection . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.2\nHow RecSim Simulates Recommender Systems . . . . . . . . . . . . . . . . . . .\n22\n3.3\nComparison Metrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.4\nEvaluation Agents\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.5\nEvaluation Methods\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n3.6\nImplementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4\nResults And Discussion\n30\n4.1\nPreliminary Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.2\nHyperparameter Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.3\nBenchmark Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n4.4\nRelevance Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5\nConclusions\n35\n6\nSuggestions for Future Research\n36\nBibliography\n37\nAppendices\n46\nA Network Architectures\n46\nB Hyperparameter Search Results\n50\nList of Figures\n1.1\nAdaptation of CRISP-DM focusing on investigation, rather than commercial\napplication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nTimeline of development of the thesis.\n. . . . . . . . . . . . . . . . . . . . . . .\n4\n2.1\nTaxonomy of Recommender Systems. . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2\nExample architecture of a Dueling Q-Network. Notice how value and advantage\nare modeled separately. Only one neuron is used to estimate v(s), and to estimate\nA and Q we need as many neurons as there are actions available. The last layer\nis not a regular linear layer, but rather a calculation, as explained in eq. (3.1).\n.\n15\n3.1\nClass diagram covering the most important parts of the implementation.\n. . . .\n28\n4.1\nComparison of an agent that recommends movies randomly (orange) and an\nagent that learns from user reviews (blue). . . . . . . . . . . . . . . . . . . . . .\n30\n4.2\nAverage ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the\nREINFORCE agent. Notice how γ = 0.95 had the highest average, but also the\nhighest variance. Therefore it would be wiser to choose 0.9 in this case. . . . . .\n31\n4.3\nAverage ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the\nActor-Critic agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.4\nAverage ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the\nDueling DQN agent.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n4.5\nComparison of training of the agents with the best hyper-parameters found. The\nhorizontal line at return 32, symbolizing the performance of a random agent, was\nadded to help compare them. The area around the lines is the 95% conﬁdence\ninterval. The Dueling DQN agent was trained for 3 seeds, while the others were\ntrained for 5. In this scenario, the maximum possible reward is 50, and it would\nmean that the user rated 5/5 for every single recommended movie. . . . . . . . .\n33\nA.1 Dueling DQN gradient graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nviii\nLIST OF FIGURES\nix\nA.2 Value estimator gradient graph. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nA.3 Policy estimator gradient graph. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nList of Tables\n2.1\nExample of a N × M Utility Matrix. Cell values represent utility or relevance,\nfor instance how many times the user at that row clicked on the product at that\ncolumn. Note that in practice, this is often a very sparse matrix. . . . . . . . . .\n7\n3.1\nComparison of diﬀerent Recommender System environments. . . . . . . . . . . .\n22\n3.2\nComparison of recent studies that train RL agents using the MovieLens dataset.\nNote the diversity of metrics.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.1\nComparison of relevance metrics for an agent making random recommendations\nand a Dueling DQN agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nB.1 Actor-Critic and REINFORCE hyperparameter search results. . . . . . . . . . .\n50\nB.2 Dueling DQN hyperparameter search results. . . . . . . . . . . . . . . . . . . . .\n51\nx\nGlossary\nCase-Based are knowledge-based systems that let users provide a sample item, and recom-\nmend similar ones. 7, 35\nCold-Start Problem is a problem of recommendation systems that emerges when recom-\nmendations need to be made for a user who we don’t have enough data about. 7, 18, 33,\n35\nCollaborative Filtering in recommender systems is the concept of using one user’s prefer-\nences to infer other’s. 6, 35\nKnowledge-Based are recommendation systems that rely on user created constraints, and\ndomain-speciﬁc product matching rules . 7, 8\nSupervised Learning are a class of machine learning algorithms that address regression and\nclassiﬁcation problems. 7, 10, 19, 20\nxi\nAcronyms\nA2C Advantage Actor-Critic Networks. 15, 33, 37\nCTR Click-through Rate. 1, 17–19, 24\nDDQN Double Deep Q-Networks. 14, 18, 19\nDL Deep Learning. 10, 11, 13, 16, 20, 35\nDP Dynamic Programming. 13\nDQN Deep Q-Networks. 13, 15, 16, 35\nDRL Deep Reinforcement Learning. 1, 2, 13, 16, 18\nERB Experience Replay Buﬀer. 14, 17, 19\ni.i.d. independent and identically distributed. 12–14\nLTV Life-Time Value. 15, 18\nMDP Markov Decision Process. 2, 12, 13, 15–17\nML Machine Learning. 4, 7, 9–11, 19\nNDCG Normalized Discounted Cumulative Gain. 9, 35\nPER Prioritized Experience Replay. 25, 27\nReLU Rectiﬁer Linear Unit. 10, 25, 27\nRL Reinforcement Learning. 1, 2, 5, 9, 11, 12, 15–19, 23–25, 35, 36\nRS Recommender System. 1, 4, 15–17, 19, 22, 24, 33–37\nxii\nAcronyms\nxiii\nTD Temporal-Diﬀerence. 13, 14, 25, 27\nUCB Upper Conﬁdence Bound. 11, 15\nChapter 1\nIntroduction\n1.1\nTopic Relevance\nDue to recent developments in large scale industrial production, improvements in logistics, and\nscalable technological infrastructure, e-commerce platforms are now able to oﬀer thousands\nof products to its users. One of the challenges that arise in this scenario is how to properly\nrecommend products, given such large variety but little space for recommendations. A solution\nthat addresses this problem is commonly known as a Recommender System (RS).\nSearch engines like Apache Lucene rely on users typing or selecting ﬁlters that approximately\nmatch what they are looking for. They usually calculate relevance scores that estimate how\nwell each document matches each search term, and order results based on it [see 1, chap. 3].\nSome RS leverage features such as user-made evaluations, search history, demographic data,\nand geographic data. Then, they may apply a family of supervised learning ranking algorithms\nlike Learning-To-Rank (LTR) [see 2, chap. 13]. Such systems may even estimate which products\nhave the highest probability of being chosen by the end user.\nAlternatively, there are methods based on Reinforcement Learning (RL) that may take into\naccount a user’s journey.\nThey make recommendations not only based on search ﬁlters or\nuser data, but also on a user’s interaction and behavior (i.e., what buttons were clicked and\nwhen). The usual goal of such agents is to optimize for metrics like Click-through Rate (CTR),\nconversion rate, or dropout rate. In other words, to increase the percentage of users that are\nclicking on products or buying them, or to decrease the percentage of users that leave without\nselecting recommendations.\nThe proposal for this work is to leverage interaction data from large retailers, use them to\ngenerate a RL environment, and measure how diﬀerent Deep Reinforcement Learning (DRL)\nalgorithms perform under these circumstances.\nThere has been research on ways generate\nfaithful user simulations from website interaction data, and how to use such simulations to create\n1\n2\nIntroduction\nrealistic learning environments. The goal is not to improve the simulations or environments,\nbut rather perform a comprehensive evaluation on which methods work best.\nThe relevance of this project can be understood from three points of view. There is the\nscientiﬁc relevance of applying the latest advancements in DRL to new problems and publish the\nresults. There is the social beneﬁt of developing technologies that help humans ﬁnd what they\nneed more easily - especially in the information era of overwhelming amounts of data. Lastly,\nthere is the commercial relevance, since online shopping has been on the rise, and retailers often\nstruggle to help consumers ﬁnd what they need.\n1.2\nPersonal Motivation\nPersonally, as many other online consumers, I have had trouble ﬁnding products online. More\noften than not I knew almost exactly what I was looking for but couldn’t ﬁnd relevant products\nin the recommendations. In many cases it was only after exhaustive searching through pages\nof recommendations that I found what I was looking for.\nFrom a professional point of view, I’m also very intrigued by this problem. In 2018 I started\nworking as an engineer in an Elasticsearch partner ﬁrm. I’ve had the chance to study and\nlearn how modern search engines estimate relevance and can scale to millions of users. In 2019\nI’ve worked on improving marketing campaigns by predicting psychological proﬁles from users,\nbased on their data. In 2020 I’ve had the opportunity to work on a solution based on RL that\nused interaction data to dynamically customize the user interface, with the goal of improving\nengagement.\nAnother personal motivation for me was the book Persuasion Proﬁling by Kaptein [3].\nIt explains, among other things, why online stores fail to reach the conversion rates of their\nphysical counterparts. The author describes this issue from a decision making standpoint, and\ndiscusses concepts that could be implemented to improve the performance of online retailers. I\nbelieve, that there is a lot of potential in modeling the recommendation problems as a Markov\nDecision Process (MDP), and solving it using DRL.\n1.3\nMain and Secondary Goals\nThe main goals of this work are to use an existing RL environment (containing an interaction\ndataset and a user simulator), apply state of the art DRL algorithms to it, thoroughly compare\ndiﬀerent algorithms, and evaluate which ones work best under which conditions.\nIn terms the environments, algorithms, and metrics, we are still evaluating which are more\nrelevant in the literature. Currently the best potential environment we found was MARS-gym\n1.4. Methodology\n3\n[4], which is based data from the ACM Trivago RecSys 2019 Challenge [5]. A few potential\nalgorithms are: Deep Q-Learning (DQN), Double Deep Q-Learning (DDQN), Categorical DQN,\nRainbow DQN, REINFORCE, and Advantage Actor-Critic (A2C). For the metrics we would\nlike to compare standard RL metrics such as the cumulative mean reward, and more traditional\nRecSys metrics like catalog coverage [6]. This metric measures what percentage of the product\ncatalog was displayed to a user during a session.\nThe partial goals for the project are setting up the user simulations, setting up the envi-\nronments, implementing each of the RL algorithms, and creating a testbed for reproducible\ncomparison. If the main goals are reached in suﬃcient time, it would be interesting to eval-\nuate how trained RL models perform after perturbations in the user simulator (to simulate\nseasonality in e-commerce websites). Another potential direction of investigation would be to\nevaluate if a model, pre-trained in a certain environment, performs well in a new but similar\nenvironment (to simulate adaptation to new demographics).\n1.4\nMethodology\nPopular methodologies such as CRISP-DM [7] focus on the development of commercial and\nstable products. Therefore, it would be pragmatic to use a simpliﬁed version of them in an\ninvestigation eﬀort. We propose a reduced version of CRISP-DM, without the Business Under-\nstanding and Deploy steps. A sequential diagram of this methodology can be found in ﬁg. 1.1.\nThe scope of the remaining steps would be:\nData Understanding\nData Preparation\nModeling\nEvaluation\nFigure 1.1: Adaptation of CRISP-DM focusing on investigation, rather than commercial appli-\ncation.\n1. Data Understanding: to obtain and understand the Trivago tracking data\n2. Data Preparation: to use the tracking data to generate a simulator, a RL environment,\nand a testbed\n3. Modeling: to implement several RL agents to be trained in the environment\n4. Evaluation: to run several variations of each agent, measure, and compare them.\n4\nIntroduction\n1.5\nProject Schedule\nThe planned schedule for the development of the thesis is represented in ﬁg. 1.2. It speciﬁes\nthe order and dependencies of the multiple eﬀorts that need to take place for the work to be\ncompleted.\nResearch & \nPreparation\nFebruary\n1\nMarch\n2\nApril\n3\nMay\n4\nJune\n5\n- extensive research of \nsimilar works in the \nliterature\n- download of the data, and \nsetup of the environment\nProposal \nSubmission\n- refinement of goals for \nmasters thesis\nModeling & \nEvaluation\n- Implementation and \ncomparison of different \nagents\nPresentation & \nDefense\n- preparation of \npresentations\n- thesis defense\nSecondary goals\n& Writing\n- work on secondary goals if \ntime permits\n- finalize writing of the thesis\nFigure 1.2: Timeline of development of the thesis.\nThe month of February is dedicated to ﬁnalizing this proposal, by deﬁning goals and reﬁning\nwhat we want to achieve. Then, during the ﬁrst three weeks of March (until the 20th), the focus\nwill be in researching the state of the art of using Machine Learning (ML) in RS, and ﬁnding\nresearch related to the scope of this thesis. The most signiﬁcant challenges for me in this step\nare to get familiar with the recommender system literature, and to ﬁnd the best sources to\nreview in the ﬁeld.\nOnce the state of the art and related works are researched, we will concentrate on the\nimplementation of the goals, until May 23rd. This phase will consist in setting up the source\ndataset, the user simulations, the environments, and the algorithms. It will also include the\nexecution and measurement of all comparisons. I have experience implementing and evaluating\nDRL models and using Gym environments, but I lack experience with training recommender\nsystems on oﬄine data, and specially in building user simulations from tracking data. I’m also\nnot very familiar with the metrics commonly used in the RS literature.\nThe next step of the project will be ﬁnalizing the writing of thesis, by appropriately doc-\numenting the methodology and ﬁndings. This step will also account for the preparation of\npresentations, and will ﬁnish on June 6th. Finally the last days of the project will contain the\nformal presentations, and defenses.\nChapter 2\nLiterature Review\n2.1\nInvestigation Method\nFor the purpose of reviewing the literature on applying RL to address the recommendation\nproblem, and analyzing the state-of-the-art, the methodology described by Silyn-Roberts [8,\nchap. 4] was used. The literature review will ﬁrst cover a short historical overview of the ﬁelds\nof reinforcement learning and recommender systems. Secondly, based on some of the most\ninﬂuential textbooks oﬀeach ﬁeld, a review of standard techniques will be provided. Then\nwe’ll explore a few recent review articles to have a panorama view of the ﬁeld. Finally, the key\npapers of the area will be analyzed, along with some more recent fringe papers.\n2.2\nHistorical Background\nRecommender Systems grew in popularity during the 1990s. One of the ﬁrst such systems was\nGroupLens which was also responsible for publicly releasing large datasets that signiﬁcantly\nhelped researchers in the area [9, chap. 1.2]. In more recent years, thanks to tech giants like\nNetﬂix and Amazon this ﬁeld has gained a lot of attention, and one of its hallmarks is the\nannual ACM RecSys challenge.\nThe Reinforcement Learning ﬁeld as we know it came together in the 1980s, thanks to a\ncombination of optimal control research (that started in the 1950s with Bellman) and psycho-\nlogical research of animal learning. A richly detailed history of the ﬁeld can be found in [10,\nchap. 1.7].\nArtiﬁcial neural networks research started in the 1940s with cybernetics researchers at-\ntempting to generate computational models of the brain. Then in the 80s under the name\nconnectionism there was the development of back-propagation, and multi-layer networks. Deep\nLearning research started in 2006 [11, chap. 1.2.1].\n5\n6\nLiterature Review\n2.3\nStandard Deﬁnitions and Techniques\nMost technical information described in this section was summarized from inﬂuential textbooks\nin the ﬁelds of Recommender Systems [9], Reinforcement Learning [10], and Deep Learning [11].\n2.3.1\nRecommender Systems\nThe problem that Recommender Systems solve can be generalized as in deﬁnition 2.3.1. Using\nthis generic deﬁnition, we can explore all the types of recommender systems under a uniﬁed\nview.\nDeﬁnition 2.3.1 (The Recommender System Problem). To deliver a set of items, to a set of\nusers, optimizing a set of goals.\nSome systems achieve this by predicting certain user-item metrics. Examples of metrics that\ncould be predicted are click-throughs, user ratings, or relevance ranking. The most usual ones\nthough are ratings of user-product tuples, and the top-k highest ranked items for a given user.\nSystems that use the latter are sometimes known as Top-N Recommender Systems. Common\ngoals for such systems include accuracy, relevance, novelty, serendipity, and diversity.\nThe predicted items could be movies in a streaming platform, books in an online store, ads\nin a marketing platform, or friend connections in a social network. The entities that receive\npredictions are usually users of an online system, but could also be businesses or even software\nagents.\nMetrics, items, and users can be modeled in diﬀerent ways. For instance, product ratings\ncould be measured as a like or dislike (binary), a 5-star rating (numerical), a Likert scale\n(categorical), or even a button click (implicit). Items could be wine brands, and be modeled\nby the features that make them unique, for instance acidity, sweetness, bitterness, aroma,\ncolor, and aftertaste. Users could be modeled using demographic data, geographic data, their\npurchase history, their user interactions, and so on.\nOne way of presenting these concepts is by using a Utility Matrix [see 12, chap. 9]. Some\nstudies refer to the modeling of user-item goals as utility, and this matrix representation places\nusers in rows, items in columns, and measurements for the cells. The table 2.1 contains an\nexample of this concept.\n2.3.1.1\nRecommender System Categorization\nIf the main features of a recommendation system are based on user-item interactions, it is known\nas a Collaborative Filtering system. For example, a collaborative ﬁltering wine recommendation\nsystem could recommend you a bottle because this particular brand has recently received high\n2.3. Standard Deﬁnitions and Techniques\n7\nItem 1\nItem 2\nItem 3\n...\nItem M\nUser 1\n1\n0\n0\n...\n2\nUser 2\n0\n2\n0\n...\n1\n...\n...\n...\n...\n...\n...\nUser N\n1\n3\n0\n...\n0\nTable 2.1: Example of a N × M Utility Matrix. Cell values represent utility or relevance, for\ninstance how many times the user at that row clicked on the product at that column. Note\nthat in practice, this is often a very sparse matrix.\nratings from other users with similar interests as you. Such systems are challenging because\nuser-interaction data is usually sparse. The two main methods of collaborative ﬁltering are\nMemory-Based, and Model-Based, according to how predictions are made.\nMemory-Based methods make predictions based on the neighborhood of the current user.\nIn other words, a distance metric is chosen (e.g., Euclidean distance, Manhattan distance), and\ncomputed pairwise between users. These methods are again subdivided into two categories,\naccording to how predictions are made. User-Based Collaborative Filtering is a special case of\nmemory-based methods that ﬁnd products that likeminded users chose in the past. Meanwhile\nItem-Based Collaborative Filtering ﬁnds items similar to what a particular user liked in the\npast.\nModel-Based methods, in contrast, use ML to predict item metrics for users. It has been\nshown that combinations of Memory-Based and Model-Based systems can provide very accurate\nresults. It has also been shown that Supervised Learning can be generalized to eﬀectively tackle\nthe recommendation problem.\nIf the main features are modeled after product attributes, it is called a Content-Based\nsystem. In such systems user interaction is combined with item data into features, and the\nrating is predicted as the target of a Supervised Learning algorithm.\nSuch systems often\nprovide more obvious recommendations (this issue is known as low serendipity), and are not\neﬀective in predicting items for newer users - this is known as the Cold-Start Problem. In\ncontrast, they perform better when predicting ratings for new products (because many models\nare able to generalize knowledge based on product metadata).\nSystems that are modeled after user-speciﬁed constraints (e.g., search ﬁelds, ﬁlters, ranges)\nare called Knowledge-Based systems. They are usually applied when there’s limited domain\nknowledge, when user ratings tend to change over time, or due to seasonality. Such systems\nusually contain business-speciﬁc rules for determining product similarities, and they allow users\nto explicitly query what they’re looking for. These systems are labeled according to how users\ninput requirements. Constraint-Based systems allow users to specify certain values or ranges\nfor speciﬁc attributes of items (e.g., ﬁltering real state by square meter price). In Case-Based\n8\nLiterature Review\nsystems users provide sample items, or speciﬁcations, and it will ﬁnd similar items (e.g., ﬁltering\nreal state by desired number of bedrooms).\nDepending on a Knowledge-Based system’s user interface, it can be categorized diﬀerently.\nIn Conversational Systems, users are questioned about their needs via natural language dia-\nlogue (e.g., a chatbot), in Search-Based Systems users answer a preset number of questions\n(e.g., a quiz). In Navigation-Based Systems, also known as Critiquing-Based Recommender\nSystems, users iteratively request changes to an example product (e.g., a car dealership website\nthat allows users to ﬁnd similar cars with lower carbon emission). Finally, a special case of\nKnowledge-Based systems are called Utility-Based, for when the relevance (i.e., item utility)\nformula is known a priori.\nHybrid Recommender Systems are the ones that combine aspects of some of the diﬀerent\ntechniques already mentioned. Other subcategories of recommender systems include: demo-\ngraphic recommender systems (when user demographic data is used), context-based or context-\naware systems (when time, location, social data are used), time-sensitive systems (ratings evolve\nor depend on seasonality), location-based systems (user locations are used). Special types of\nrecommender systems are social recommenders. These models recommend using social cues\nlike social graph connections (i.e., using PageRank algorithms), social inﬂuence (also known\nas viral marketing or inﬂuence analysis), trustworthiness (by having users explicitly provide\ntrust/distrust ratings), and social tagging (e.g., hashtags).\nRecommendation systems that model users in groups are known as Group Recommender\nSystems. Systems that allow each item attribute to have its own rating (for instance hotel\nbooking websites, that rate hotels on cleanliness or hospitality) are known as Multi-Criteria\nRecommender Systems. The aspect of some recommendation systems to actively encourage\nusers to rate less popular items is known as Active-Learning.\nThere are some less popular deﬁnitions that are not present in the textbook, but are part of\nthe more recent literature. Some researches refer to systems that directly receive user feedback\non recommendations as Interactive Recommender Systems. Some researchers refer to systems\nthat actively leverage user interaction to make predictions as Sequential Recommender Systems.\nFinally, some make a distinction between how soon users are expected to accept recommen-\ndations. In this sense, Long-Term Prediction systems are optimized to have users select items\nat any time in the future. Meanwhile Short-Term Prediction Systems expect users to select\nrecommendations immediately.\nA diagram with the most important types of Recommender Systems can be found in ﬁg. 2.1\n2.3. Standard Deﬁnitions and Techniques\n9\nRecommender\nSystem\nCollaborative\nFiletering\nContent-Based\nKnowledge-Based\nLearns from like-\nminded users\nLearns from user\ninteractions and item\nattributes\nUser specifies\nconstraints\nMemory-Based\nModel-Based\nPredictions based on\ndistance metrics\nUser-Based\nItem-Based\nPredictions based on\nuser neighborhood\nPredictions based on\nitem neighborhood\nPredictions based on\nML models\nConstraint-Based\nCase-Based\nUser specifies ranges\nand value filters\nUser provides sample\nitems\nHybrid\nCombination of other\ntechniques\nFigure 2.1: Taxonomy of Recommender Systems.\n2.3.1.2\nRecommender System Evaluation\nBefore discussing evaluation metrics, it is important to make a distinction between how eval-\nuations are made.\nOnline Evaluation methods present real users with diﬀerent versions of\nrecommender systems, and measure how each one performs. It is often the case that there are\ntwo versions and users are randomly split 50/50, and this setting is called A/B Testing. An\nautomated generalization of this test setting can be implemented with RL, using Multi-Armed\nBandit algorithms. Research using online methods is often limited, due to the commercial\nimpact of testing unsuccessful recommenders, the lack of generalization of the domain of spe-\nciﬁc models, and commercial secrets. In contrast, Oﬄine Evaluation methods use historical\ndatasets, often public, to train RS on recorded data.\nThese are ideal for research because\nﬁndings are comparable and reproducible.\nThere are several goals that can be used to compare Recommender Systems, and the fol-\nlowing deﬁnitions are purposefully generic, since many researchers decide to calculate these\nconcepts diﬀerently, based on the goals of their work. Accuracy measures the percentage of\nthe recommendations that would’ve been selected by a user. Coverage measures the percent-\nage of items that were recommended, out of all the items available. Conﬁdence measures the\naverage statistical conﬁdence or conﬁdence interval (which some ML models provide) in the\nrecommendations. Novelty measures the percentage of the current recommendations that has\nnever been recommended to this user before. Serendipity measures the percentage of the rec-\nommendations that would not have been selected by an obvious recommender (e.g., a simplistic\nmodel). Diversity measures the similarity between the items that were recommended. Robust-\nness or Stability are measurements of how a recommendation system can be aﬀected by (i.e.,\nhow attack-resistant it is to) false reviews or fake ratings. Lastly, Scalability measures how an\nincreased amount of items impact learning time, prediction time, and memory consumption.\nAn example of a concrete goal often used in research is to maximize Normalized Discounted\nCumulative Gain (NDCG). This metric averages how relevant each item in the recommendations\n10\nLiterature Review\nis to the user (Cumulative Gain). The averages are weighed by how early each item appears\non the list (Discounted Cumulative Gain), and normalized by how relevant each item is in\ngeneral (i.e., the Ideal Relevance). Given a user u, a slate of products P = {p1, p2, ..., pN}, and\na relevance metric taken from user feedback, we can calculate the discounted cumulative gain\nas in eq. (2.1).\nDCGu(P) =\nN\nX\ni=1\nrelevanceu(pi)\nlog2(i + 1)\n(2.1)\nTypically relevance is the expected rating of a user to an item [9, chap. 7.5.3]. Using ground-\ntruth data (raw ratings) we can compute an ideal DCG metric, called IDCG, representing the\nmaximum possible DCG. And ﬁnally, we have eq. (2.2):\nNDCGu(P) = DCGu(P)\nIDCGu(P)\n(2.2)\nWhich outputs a value in range (0, 1). Normally these metrics are reported as NDCG@N,\nwhere N is the number of products in the recommendation slate, or |P|.\n2.3.2\nDeep Learning\nMachine Learning is a ﬁeld of Artiﬁcial Intelligence (like Natural Language Processing, or\nComputer Vision) that aims at building solutions to problems that involve pattern recognition,\nclustering, regression, classiﬁcation, and decision-making. Regression and classiﬁcation prob-\nlems are commonly addressed by speciﬁc types of learning algorithms, and are referred to as\nSupervised Learning. Neural Networks are a class of the latter, and Deep Learning (DL) refers\nto some speciﬁc types of those networks (i.e., when there are multiple hidden layers).\nIn Artiﬁcial Neural Networks (ANN), the main logic units are called neurons, and are\ncharacterized by an activation function that combines a set of weights (and often biases) and\nproduces an output signal [11]. A perceptron is a neuron in which the output is 1 if the dot\nproduct of the weights is greater than 0, and 0 otherwise. This is often called a linear function,\nand can be represented as in eq. (2.3):\nf(x) =\n\n\n\n1\nif w · x + b > 0\n0\notherwise\n(2.3)\nwhere w is the weight vector, x is the input, and b is the bias vector. Other examples of\nactivations include ReLU, the sigmoid function, and the softmax function.\nThe training of such learning models requires adjusting the weights in order to minimize\n2.3. Standard Deﬁnitions and Techniques\n11\na speciﬁc cost function, which often is an aggregation of the loss that occurred during train-\ning. Examples of cost functions include the Mean Square Error (MSE) or the Cross-Entropy\nfunction. The logic that tells us how much to adjust the weights is called an optimizer, and\nis often based on gradient descent optimization. The degree to which the optimizer moves in\nthe direction of the gradient is referred to as learning rate. Examples of optimizers include\nMomentum and Adam. The algorithm that optimizes all the neurons in a network accordingly\nis called backpropagation.\nWhen several layers of neurons are stacked together, the models are called Deep Learning.\nThe layers can be fully connected or not, and architecture refers to how connections are setup,\nhow layers are created, the activation functions chosen for each neuron, and so on. There are\nseveral utilities for these methods, and they are discussed in more detail in section 2.4.1.\n2.3.3\nReinforcement Learning\nRL refers to a class of ML algorithms in which an agent interacts with an environment. The\nformer tells the latter which action should be taken next, and receives a reward signal along with\nthe next state. Commonly these interactions are measured by steps, and after some conditions\nare met the interaction episode ends (ﬁnite-horizon environments). There are RL problems\nthough, in which episodes go on continuously. In such settings it is customary to discount\nrewards, so that agents can learn actions that are good in the long-term, but so that rewards\nin the distant future tend to zero.\nA classic example of a RL problem is the task of balancing a pole on a cart in two dimensions\n[10]. In this case, the environment would be a physics model, that would estimate the position,\nangle, and velocity of the pole. The agent would apply force to the cart, either left or right. For\nevery step that the pendulum remains upright, the agent receives a +1 reward. If the pole gets\ntoo inclined, or the cart moves too far away from the starting position, the episode ends. By\nrandomly exploring diﬀerent strategies, many RL algorithms are able to learn how to balance\nthe pole appropriately.\nThere is a notable class of algorithms, that seek to make optimal decisions without con-\nsidering long-term agent-environment interactions (non-associative tasks), called Multi-Armed\nBandits. Some examples of these include k-Armed Bandits, Upper Conﬁdence Bound (UCB)\naction selection, and Gradient Bandits. Bandit algorithms with environment context are called\nContextual Bandits.\nGenerally these algorithms start by randomly picking actions and observing the reward from\neach action. They estimate expectations of the reward, and greedily start choosing actions that\nmaximize these expectations (this is called ϵ-greedy exploration).\nUCB methods have the\nadvantage of modeling uncertainty in the reward expectation. Gradient Bandits go beyond and\n12\nLiterature Review\nmodel the rewards as Boltzmann (soft-max) distributions.\nThe more advanced RL algorithms rely on the problems having certain attributes, speciﬁ-\ncally that they are Markov Decision Processes (MDP). They are extensions of Markov Chains,\nand therefore require the Markov Property (memorylessness). In other words, if we are at a\nparticular state, the probabilities of transitioning to other states depend solely on information\nof the current state, not the past ones (i.e., a game of chess). Environments that don’t pro-\nvide agents with the full state are a special type of MDP, called Partially Observable Markov\nDecision Processes (POMDP).\nAgents may learn policies, value functions, and models. Policies are a mapping between\nstates and actions, that tells the agent which is the best action to take in each possible state.\nPolicies can be deterministic or stochastic. Value functions predict the sum of all expected\nfuture rewards, if a particular policy is followed in the future (known as Return). Models can\nbe estimations of the transition probability between all the states, or estimated reward for all\nstate-action pairs. RL agents that don’t create models are referred to as model-free. Some agents\nattempt to learn all value functions, some attempt to learn an optimal policy (sometimes known\nas Control), and some attempt to do both. These last ones are called Actor-Critic methods.\nRL algorithms face several challenges that supervised models are not equipped to handle.\nThe ﬁrst one is that data is not independent and identically distributed (i.i.d.). The second\nproblem is the trade-oﬀbetween exploring new action or state scenarios, and exploiting previ-\nously obtained knowledge. In some cases actions the agent takes will only payoﬀlater on, in\nother words, the expected reward can be delayed. Some problems also display the property of\nnonstationarity, which means that the transition matrix or the reward distribution may change\nover time.\nWe can represent mathematically the value of being in a particular given state, using the\nBellman equation. It calculates the immediate reward, plus the discounted reward of all future\ninteractions. Given a state s, the value vπ(s) of being in this state if following a policy π is\ndeﬁned as in eq. (2.4):\nvπ(s) .=\nX\na\nπ(a|s)\nX\ns′,r\np(s′, r|s, a)[ r + γvπ(s′)]\n(2.4)\nHere π(a|s) is the probability of this policy choosing an action a given state s, while\np(s′, r|s, a) is the probability of receiving reward r and transitioning to state s′ when tak-\ning action a in state s. γ is the discount factor for future rewards, and vπ(s′) is the recursive\nrepresentation of the value of the following state s′ (often called a trajectory). This equation is\nsolvable numerically, albeit often unfeasible due to the high dimensionality of the state space.\nRL algorithms are more eﬃcient solutions to this problem.\n2.3. Standard Deﬁnitions and Techniques\n13\nTraditional RL algorithms seek optimal policies by storing a map of the value of every\nknown state-action pair.\nThese solutions are called Tabular Solution methods.\nThese are\nsubdivided into Dynamic Programming (DP), Monte Carlo methods, and Temporal-Diﬀerence\n(TD) learning.\nDP methods are applied on environments that can be perfectly modeled, and they iteratively\nimprove policies until reaching a mathematically optimal one. Examples of this collection of\nalgorithms include Policy Iteration and Value Iteration. Monte Carlo methods don’t require\nenvironment modeling, and are able to learn optimal policies from experience, by simulating\ndiﬀerent scenarios.\nAn example of such a strategy is the On-policy MC control algorithm.\nTD learning is a combination of the two previous ones, in the sense that it also learns from\nexperience like Monte Carlo methods, and that experiences aﬀect other related estimates, like\nin DP. Examples of these algorithms include SARSA and Q-Learning.\n2.3.4\nDeep Reinforcement Learning\nTabular methods are very limited, when applied to real world problems. The ﬁrst limitation\nis the need to model the state using discrete variables.\nA tabular solution would consider\nvelocities of 14.999 and 15 to be two completely unrelated states. Another limitation is the\nunfeasibility of storing large numbers of states. A chess board for instance, has 10120 possible\nconﬁgurations, which is much more than we can currently store.\nThese problems can be solved with Approximate solutions, that attempt to approximate\ntowards an optimal policy, although convergence is not guaranteed in most cases. These so-\nlutions use supervised learning algorithms, and although other models could be used, there\nare several beneﬁts of using DL. There’s the natural correlation between the data from RL\nenvironments, the changes in the value functions that happen when the policy changes, and\nthe nonstationarity of many problems.\nA very straightforward example of such models is a neuron with linear activation, in which\ninputs are the state features, and the output models the value of that particular state. The cost\nfunction is the mean squared error between the observed and predicted values. This algorithm\nis called Monte Carlo Gradient Descent.\n2.3.4.1\nDeep Q-Learning\nIf we model the value of choosing each action a in a particular state s, known as Q(s, a), as an\noutput neuron this algorithm is called DQN. However, adding more neurons and layers intro-\nduces some challenges like the lack of i.i.d. data, the value function changing after every update\nof weights, and the lack of exploration strategies. Several techniques have been developed by\n14\nLiterature Review\nthe Deep Mind team at Google to make DRL useful under MDP problems. We will now discuss\na number of the most important ones.\nTo simulate the i.i.d. property researchers have developed the Experience Replay Buﬀer\n(ERB), which stores a large amount of episode experiences prior to learning, and the agent\nsamples randomly from it in small batches.\nWe can use the loss (TD-error) from certain\nexperiences to learn which among them moves the network the most along the gradient. We\ncan use this information, known as priority, to assign these experiences a higher probability\nof being sampled. When combining it with Importance Sampling (to avoid bias towards high\npriority experiences) we have a Prioritized Experience Replay, developed in 2016 by Schaul\net al. [13].\nTo handle the problem of the Q-value functions changing after every training, the concept\nof a target network was introduced, which is a duplicate of the original network, but that gets\nupdated every k iterations. This means decisions are made using the target network, but the\noriginal one gets updated during learning. If we obtain the Q-value from the target network\nwhen learning we have Double Deep Q-Networks (DDQN), developed in 2016 by Hasselt et al.\n[14].\nTo handle the exploration problem, the ϵ-greedy approach with a decaying exploration\nprobability is often used. A more recent approach, called Noisy Nets, is to introduce random\nnoise in the hidden layers, developed in 2019 by Fortunato et al. [15].\nA recent approach\nto improve the estimations of the Q-value has been to split it into two diﬀerent estimators:\nthe value v(s) of being in a state s and the advantage of choosing an action a in this state\nA(a, s). This setup is called a Dueling Q-Network, developed in 2016 by Wang et al. [16] and\nan illustration of this concept can be found in ﬁg. 2.2. The way Q is calculated from A and v is\nby adding them, and subtracting the average advantage (to avoid the identiﬁability problem).\n2.3.4.2\nPolicy Gradients & Actor-Critic Methods\nOne of the limitations of DQN is that the action space needs to be necessarily discrete, since we\nestimate the value of each action using a neuron. A diﬀerent approach is to model the output\nas a probability distribution of the expected return, estimated separately for each action. This\nobviously also works on discrete action spaces, by using softmax distributions. Modeling this\nway means we will want to maximize the value outputs, instead of minimizing the cost. This\n2014 algorithm is a policy gradient version of REINFORCE by Silver et al. [17].\nThere are advantages and disadvantages of modeling this way. On one hand it is simpler,\nsince there’s no need for exploration, replay buﬀers, or target networks. On the other hand,\nthe correlations between states of the same episode may introduce bias, the high variance of\nthe reward signals can cause slow convergence.\n2.3. Standard Deﬁnitions and Techniques\n15\nInput\nv(s)\ns\nA(s, a)\nQ(s, a)\nHidden\nlayers\nValue\nsubnet\nAdvantage\nsubnet\nFigure 2.2: Example architecture of a Dueling Q-Network. Notice how value and advantage\nare modeled separately. Only one neuron is used to estimate v(s), and to estimate A and Q\nwe need as many neurons as there are actions available. The last layer is not a regular linear\nlayer, but rather a calculation, as explained in eq. (3.1).\nApproximate Actor-Critic methods are a combination of the previous two approaches. The\nname comes from the fact that this method uses two distinct neural networks, an actor that\ndecides what action to take at each step, and a critic that evaluates the action taken by\nestimating Q. Advantage Actor-Critic Networks (A2C) are an example of this approach, but\nmodeling the advantage of the state-action pair separately. They were developed in 2016 by\nMnih et al. [18].\n2.3.5\nRL-Based Recommender Systems\nIn the RecSys literature, there are two main ways in which RL is used. The ﬁrst one is to\ncreate an automated way to decide which recommender system (out of a pool of options)\nshould be used with which user. The second way is to use item features to deliver personalized\nrecommendations to each user. In both cases, most of the literature focuses on tabular methods\nsuch as Contextual Bandits, the ϵ-greedy Algorithm, or the Upper Conﬁdence Bound algorithm\n(UCB). Research in solving the RS problem using RL is very limited, and there is a lot of\nopportunity in this ﬁeld [see 9, chap. 13.3].\nThe RL literature refers to the second usage as Personalized Web Services [see 10, chap. 16.7].\nIn such setting agents use feedback directly (or via user clicks) to iteratively improve a recom-\nmendation policy. This is often referred to as Associative Reinforcement Learning, and it has\nhad success in improving clickout-rates of online sellers, and has been shown to have better\nresults when modeled as a MDP. One of the potentials of using RL in this scenario is the long\nterm eﬀect (also known as Life-Time Value or LTV) of a policy, having the potential to convince\n16\nLiterature Review\na user to purchase something through a sort of recommendation funnel. Researchers testing RL\nmodels on commercial solutions often use oﬀ-policy evaluation to avoid ﬁnancial risks.\n2.4\nCurrent Technologies\n2.4.1\nDeep Learning And Recommender Systems\nSince the application of RL in the context of Recommender System is fairly new, there are not\nmany review papers available. Nevertheless, two important review articles are worth discussing.\nThe ﬁrst article from 2019 reviews the most important research in using DL to build RS, by\nZhang et al. [19], and brieﬂy discusses some research that used DRL. It explains how several\ntech companies use DL to enhance recommendation quality. Some examples of such systems are\nrecommendations in the form of YouTube videos, Google Play apps, and Yahoo news articles.\nAll of these having served billions of user sessions over the years. Naturally, the amount of\nacademic research around DL and RS grows steadily.\nThere are several beneﬁts of using DL. Firstly the ability to model problems nonlinearly,\nwhich is a property that enables these models to recognize more complex patterns. Secondly\nthere’s representational learning, which allows large number of features, as well as less tra-\nditional ones like images, audio, or text, to be used in the learning process. Finally there’s\nthe ability to model signals sequentially, which in some cases allow Recurrent Neural Net-\nworks (RNN) to represent some temporal dynamics. Despite these beneﬁts, DL is not without\nsome setbacks. These models often make the interpretability of recommendations very diﬃcult,\nrequire large amounts of data to be trained, and extensive hyperparameter tuning.\nIt is important to look at the latest advances in DL because there are many important\nconcepts of it that could be successfully applied in the ﬁeld of DRL. The concept of autoencoders,\nfor instance, could be used for dimensionality reduction in ﬁnding latent variables in RL state\nvectors.\nAttention mechanisms could be used to improve recommendation interpretability.\nConvolutional layers could be used to introduce images to environments.\nIn spite of all this potential, DL by itself has no means of optimizing recommendations in\nthe long term, nor it has the ability to perform well on nonstationary scenarios (user behavior\nchanges over time), nor the ability to handle mutating product catalogues. All of these are\noften requirements in recommender systems, and they’re where RL becomes very relevant.\nThe second review article worth mentioning is an overview of RL (speciﬁcally MDP) tech-\nniques applied to RS [20]. These studies validate the usage of partially observable environments,\noﬀ-policy training, nonlinear functions, the ϵ-greedy algorithm, SARSA, value iteration, and\nDeep Q-Networks. The most important reviewed works are discussed in section 2.4.2.\n2.4. Current Technologies\n17\n2.4.2\nReinforcement Learning And Recommender Systems\nWe’ll review chronologically some the most cited journal articles and conference papers, in the\nﬁeld of applying Reinforcement Learning to the RS problem. Then, we look at some interesting\nRL environments, and details about the most recent ACM RS conference.\nWe start by analyzing the 2007 work of Mahmood and Ricci [21]. They showed how Con-\nversational Recommender Systems could be modeled as MDP. Their system was based in users\nsearching travel products, and relied on them specifying constraints. In this case, the RL agent\nsuggests improvements to the constraints when too many or too little recommendations were\nreturned. In their model, there is a very limited number of states an agent can be in, and a\nlimited number of actions it can take. They show how an agent could improve recommendation\npolicies by using the Policy Iteration algorithm, and evaluate it using simulations. In 2009 [22]\nthey validate their model in an online setting, by comparing it with a ﬁxed recommendation\npolicy. They found that users that received recommendations from the RL agent reached their\ngoal faster, more often, and adopted more query improvement suggestions.\nNext we analyze the research from Yahoo! Labs in recommending news articles. In 2010 Li\net al. [23] modeled a news recommender system as a contextual bandit problem that maximizes\nuser clicks on articles. They show that it’s possible to reliably train RL models oﬄine with\ntracking data, recorded by recommending articles randomly. They deploy the trained model\nonline, and show that it performs better than a multi-armed bandit agent without user context.\nIn 2011 Li et al. [24] addressed some problems of training agents oﬄine, speciﬁcally the modeling\nbias often present in user simulations. It was accomplished by introducing a replay method-\nology (analogous to an Experience Replay Buﬀer), and research shows how training oﬄine by\nreplaying dataset experiences can produce comparable results to online training. Results were\nmeasured in terms of CTR.\nTwo more theoretical works from Yahoo! Labs helped prove regret bounds for a few models.\nIn 2010 Kale et al. [25] showed how a multi-armed bandit model could be adapted to provide\na set of actions per step. They analyze scenarios in which the actions need to be ordered, and\nscenarios that don’t require ordering. They provide theoretical a bound for the regret metric\nof such agents. In 2011 Chu et al. [26] provided a theoretical analysis of a variant of LinUCB.\nThey prove a high-probability bound for the regret of their agent. There is a discussion about\nhow to deal with scenarios when these linear models are not guaranteed to accurately estimate\nthe expected reward of recommendations. It may be the case that in real online systems often\nit’s not possible to perfectly obtain the expected reward, especially in non-stationary settings.\nThis is exactly the problem that Bouneﬀouf et al. [27] address in 2012. They develop an\nϵ-greedy model in which the exploration probably ϵ depends on the user’s situation. The precise\ndeﬁnition of situation is the similarity between a user’s feature vector, and the closest vector\n18\nLiterature Review\nin its history of features. They evaluate their model oﬄine, using a dataset from a commercial\nsystem, and measuring CTR. They compare it to a standard ϵ-greedy agent, and one with\ndecreasing exploration probability, and show that their model performs best.\nNow we review important work from Adobe Research in the area of ad recommendation.\nIn 2015 Thomas et al. [28] studied eﬀective ways to evaluate recommendation policies oﬄine.\nThey discuss the motivation, and the high risks of using a poor recommendation strategy in\nlive systems. Their evaluation technique accurately estimates a lower conﬁdence bound on the\nexpected return of a given policy. Their methodology is evaluated using real recorded Adobe\nMarketing Cloud data, and training a user simulation from it. It’s shown how the conﬁdence\ninterval for predictions narrows down when the volume of training data increases, as would\nbe expected.\nAlso in 2015, Theocharous et al. [29] investigate the beneﬁts of training RL\nagents to increase long-term user interactions (known as Life-Time Value or LTV), as opposed\nto maximizing CTR. They also provide means of evaluating such models oﬄine, to enable\nhyperparameter optimization, and to guarantee safe deploys of these policies.\nThey use a\nFitted-Q Iteration RL agent, that in turn uses a combination of ϵ-greedy exploration and a\nRandom Forest classiﬁer. Even though this model handles highly dimensional states, it does\nnot solve the problem of a large action space.\nThe work that Choi et al. [30] did in 2018 was really important in addressing highly dimen-\nsional data, both in the state space and in the action space. They introduce a biclustering step\nbefore training, that drastically reduces the amount of states and actions of RL agents. Two\nadded beneﬁts are that clustering actions allows some explainability of recommendations, and\nclustering users allows some generalization of recommendations, which eﬀectively handles the\nCold-Start Problem. They test Q-Learning and SARSA oﬄine, on the MovieLens movie recom-\nmendation datasets. It would have been interesting to see comparisons of other dimensionality\nreduction techniques, and non-tabular agents.\nLastly, we look at recent works that used DRL. Zheng et al. [31] in 2018 used a dueling\nDDQN to create a news recommendation system. With their model, they wanted to improve\nagents that optimized for short-term goals, that relied solely on CTR, and that had low Diver-\nsity. They evaluate that their model achieves these goals both with oﬄine training, and online.\nThese works give us an idea on how a utility matrix can be adapted, and neural networks can\nbe used to predict its metrics. By modeling the network input (the state) as a combination\nof features (user, item, interaction, context), the output can be, for instance, the probability\nof this user clicking on this item at some point in the future (in other words, the expected\nclick-out rate).\nChen et al. [32] in 2018 address the problem of high variance and biased expectation of\nrewards, typical of recommender systems. To improve the variance they introduce a modiﬁca-\n2.4. Current Technologies\n19\ntion to the ERB, by stratifying stable user features (like age or gender). This means sampling\nexperiences will draw similar amounts of experiences from these strata. To improve the expec-\ntation bias, they propose the concept of Approximate Regretted Reward, in which the reward\nof an agent learning online is penalized by the reward of an agent that learned with previously\navailable oﬄine data. They apply these concepts to a DDQN model, and test it on a live search\nengine. Their model had over 20 thousand possible actions (user suggestions) and the states\nwere modeled after user features. It’s shown to perform better (in terms of CTR and conversion\nrate) than a system based on a Supervised Learning algorithm, on an A/B test.\nZhao et al. published two important works in 2018. The ﬁrst one [33] is about a model that\nexplicitly requires users to vote if a recommendation was useful or not. The agent learns directly\nfrom these reward signals.\nThey tested it with data from an online e-commerce platform.\nThe second one [34] addresses some problems of learning with real-time user feedback, and\nrecommending pages of products. They develop a page-wise Actor-Critic RL agent that is able\nto maximize clicks on a 2D grid of product recommendations. The researchers train and test\nthe model oﬄine, but also perform online evaluations.\n2.4.3\nSimulation Environments\nWith the recent advances in Reinforcement Learning and ML in general, certain tools and\nframeworks start to become standards. One of the most inﬂuential RL benchmark tools is\nthe 2016 OpenAI Gym by Brockman et al. [35]. We will therefore explore research that was\ndone in designing gym environments that speciﬁcally simulate the dynamics of recommendation\nsystems.\nWe start by reviewing Google Research’s 2019 RecSim by Ie et al. [36]. It consists of a\ngym environment in which it’s possible to ﬂexibly deﬁne models for users, documents (items),\nand user-choice interactions. User models refer to the features of a user (e.g., demographic\ndata, context), and its probability distributions. The same modeling (sampling features from\na probability distribution) is done for documents. The probability of a user selecting a recom-\nmendation is also sampled from a distribution, and so are the transitions that may occur in a\nuser’s state when it selects a document (e.g., a user’s engagement may increase or decrease).\nThe agent’s task is to optimally recommend documents to users. RecSim comes with several\nalready prepared environments.\nA diﬀerent approach called PyRecGym was researched in 2019 by Shi et al. [37]. Also a set\nof gym environments, this framework uses some traditional RS datasets (e.g., MovieLens, Yelp)\nto create simulations of recommender systems. The tool divides datasets into initialization data\nused to train user simulators, and interaction data used to teach the RL agent. User and item\nfeatures are combined, and recommendation selections by the simulated user happen only if\n20\nLiterature Review\nthey occurred in the initialization dataset. This is a naive way of modeling human-computer\ninteractions, and this environment would have beneﬁted from a probability distribution when\nsimulating the interactions.\nAnother gym environment that is worth reviewing is the 2020 MARSGym by Santana et al.\n[38]. This environment is designed speciﬁcally for simulating marketplace dynamics, in which\nrecommendation systems not only need to optimize click-throughs, but also maintain healthy\nlevels of fairness (recommendation diversity). It bears similarities with PyRecGym in the sense\nthat it learns from tracking data, and that user-interactions simulations are exact replicas of\nwhat happened in the historical data. The researchers go into detail about the problems and\nbeneﬁts of oﬄine training, and how counterfactual estimators can be used to reduce the bias.\nThey demonstrate the eﬀectiveness of the environment using data from Trivago.\n2.4.4\nThe ACM RecSys Conference\nTo conclude this review, it’s important to investigate not only in academic and commercial\nproduction, but also in competitions. In 2020 Jannach et al. [39] did an extensive exploration\nof algorithms used in competitions, and share their ﬁndings and hypothesis. From the top 5\nwinning teams of the ACM RecSys Conference from 2017 to 2019, only one used DL. All the\nothers used extensive feature engineering and more traditional Supervised Learning algorithms.\nA similar situation happens in other related competitions. The authors argue how competition\ndatasets are more massive than the ones used in academic research, and how the time available\nto train models in competitions is often short. DL-based models often require more training\ntime, more expensive hardware, and more extensive hyperparameter optimization. Another\nimportant aspect is that competition datasets tend to be much more sparse, which causes\noverﬁtting in many DL models.\nSome of the authors of this paper were part of the winning team of the 2020 RecSys challenge,\nand their methodology is described in Schiﬀerer et al. [40]. The greatest contributions of this\npaper are their eﬀorts in improving the performance of preprocessing, feature selection, and\nmodel training on huge datasets. They are able to run all these steps in the GPU, and speed\nup computations 280 times, reducing processing time from several hours to a couple minutes.\nThe authors describe how they compared several deep learning models and other supervised\nones, but XGBoost was the one that performed best.\nChapter 3\nMaterials And Methods\n3.1\nLearning Environment Selection\nIn the previous chapter we proposed to evaluate three modern gym environments found in the\nliterature: PyRecGym [37], RecSim (the interest evolution environment) [36], and MARS-Gym\n(the Trivago Rio environment) [4]. Unfortunately, it proved impossible to ﬁnd an open-source\nimplementation of PyRecGym, therefore we will replace it in this comparison with one of\nGoogle’s ML Fairness Gym [41] recommender environments. ML Fairness Gym is based oﬀ\nRecSim, and we’ll be discussing speciﬁcally the experimental source code based on the 1M\nMovieLens dataset [42].\nWhen comparing these environments there are many desirable traits we are looking for.\nThe usage of real-world data, for instance, makes the environment more relevant to realistic\nproblems. The availability of large amounts of data makes it possible to properly train deep\nneural networks. The documentation of the environment is also important, to enable us to\nunderstand the features of the states. The time it takes for a time-step to be simulated is also\nparamount, because smaller execution times allow for further experimentation. In table 3.1\nthese traits are displayed in comparison.\nMARS-gym only has one possible episode because its deﬁnition of an episode is a sequential\niteration over its ground-truth dataset (exactly like a supervised learning training routine).\nMeanwhile on the others, episodes represent a user’s journey. The state of an environment’s\ndocumentation speciﬁcally relates to the existence of explanations regarding the RL variables\n(i.e., state/observations, actions, reward, and the done ﬂag). We benchmarked the time taken\nto simulate a time-step, averaged over 50 episodes, and taken using the same hardware (2.4GHz\nx 16 Intel i9). Diﬀerent machines will of course output diﬀerent simulation times.\nSince none of the studied environments are suitable for this investigation, our decision will\nbe to use the ML Fairness environment, but extending it to incorporate MovieLens user features\n21\n22\nMaterials And Methods\nMARS Gym\nRecSim\nML Fairness Gym\nSimulatable episodes\n1\nInﬁnite\n6040\nReal-world data\nYes\nNo\nYes\nDocumentation\nLacking\nComprehensive\nLacking\nTime-step avg. duration\n0.14 ms\n1.16 ms\n0.33 ms\nModels user interest\nNo\nYes\nYes\nUser features\nNone\nNumeric (20)\nNone\nItem features\nNumeric (148)\nNumeric (20)\nBinary (19)\nInteraction Features\nBinary (779)\nMiscellaneous (5)\nCategorical (1)\nTable 3.1: Comparison of diﬀerent Recommender System environments.\nsuch as sex, age, occupation, and zip code. Another beneﬁt of this environment is the extensive\nusage of the MovieLens dataset in RS literature, which allows for comparison with other works.\nThere’s also the possibility of implementing environments using other recommendation datasets\n(i.e., a Trivago-based environment).\n3.2\nHow RecSim Simulates Recommender Systems\nAs previously discussed, the RecSim framework (which our environment is based on) contains\n4 key abstractions of recommender systems, that need to be conﬁgured:\n• A User Model that deﬁnes ways in which users are generated. This logic speciﬁes which\nfeatures are part of the user, and how they are generated (sampled from a distribution,\nor read from a dataset). In our case, this includes user features from the MovieLens data\n(i.e., sex, age, occupation, zip code).\n• A Document Model that deﬁnes ways in which documents are generated. Like the user\nmodel, this logic speciﬁes which features describe an item. In our case it includes features\nfrom the MovieLens dataset, merged with violence scores from the Genome project [43]\n(i.e., title, genres, year, violence score).\n• A User-Choice Model that simulates a user’s response to a particular document (i.e.,\na ﬁve-star rating to a movie). Like the user model, this logic speciﬁes which features\ndescribe an item. This can be generated for example by sampling from a distribution, or\nby reading a matrix factorization of a user-interaction dataset. User-Choice models may\nalso contain features like timestamps, user interactions, and so on. In our case this data\ncomes from a matrix factorization of the MovieLens ratings.\n• A User Transition Model that determines how user features are aﬀected after each user-\ndocument interaction. For instance, a user’s interest in a certain retail category may\n3.2. How RecSim Simulates Recommender Systems\n23\ndecrease after interacting with it. In our case, we’ve conﬁgured users to slowly get addicted\nto certain movie categories; this is simulated by slowly increasing user aﬃnities (their\nembeddings obtained by the matrix factorization).\n.\nAdditionally, it allows the conﬁguration of recommendation slates. A slate is a subset of\nall the available documents that is presented to the user, and the RL agent is responsible for\ndeciding which documents are included in the slate. This type of strategy is often seen in\ne-commerce websites under sections that display items similar to the one currently being seen\nby the user (e.g., “You may also like”). During the execution of the episodes, documents and\nusers can be sampled randomly, with or without resampling, or divided into pools (i.e., users\nmay be divided into train, test, and validation pools).\nScores are simulated using a matrix decomposition strategy, from the utility matrix. Given\nthe utility-matrix U of shape M × N, where element Ui,j represents the rating, in range (0, 5),\nprovided by a user Mi to a movie Nj. Since U is often sparse, and we need to simulate any\nuser-movie recommendation, we can use a non-negative Single-Value Decomposition (SVD)\nalgorithm that replaces missing ratings with the average before training. This is a traditional\nrecommendation strategy that has gained a lot of attention since the 2006 Netﬂix challenge\n[44]. The technique consists of estimating matrices (W, H) such that W ∗H ≈U. We can then\nestimate a user’s rating of an item by the dot product of a row of W (user embeddings) and a\ncolumn of H (document embeddings). The larger the number of components the more accurate\nthe factorization, and since the ML-Fairness project uses 55 components, we will use the same\namount.\nIn RL terms, the action space of this environment comprises of the 3883 movies that can\nbe recommended at any moment. The observation space, just like in RecSim environments, is\ncomposed of the document’s (movies) features, the user’s response to the last seen item, and\nthe user’s features. The movies in this environment have 19 binary features, that represent the\nmovie’s category (e.g., drama, adventure). The features of the response are a simulated 5-star\nrating, and a violence score (extracted from the Genome dataset). The reward signal is in the\nrange (0, 1) and relates to the user’s simulated rating of the recommended movie. The reward\nis a multi-objective model, and represent a trade-oﬀbetween interest and violence. A user’s\njourney (history) Ht at time-step t can be represented as:\nHt = [O0, A0, O1, R1, A1, ..., At−1, Ot, Rt]\nwhere Oi contains the features of available documents, current features of the user, and features\nof the latest user-document interaction at time-step i. Ai represents the recommendation of a\n24\nMaterials And Methods\nslate of movies at time-step i, and Ri represents the reward received by the agent at time-step i.\nThe embeddings used to generate the reward are not observable by the agent, as this is partly\nwhat the RL agent tries to estimate.\n3.3\nComparison Metrics\nAs described in section 3.2 the RL models will be maximizing the expected return (i.e., the\nsimulated user ratings). Several other studies measure diﬀerent metrics, that may for instance\nhelp assess the health of a recommendation ecosystem, such as diversity. Modeling a return\nthat balances these goals, performing a counterfactual evaluation, is a ﬁeld under active re-\nsearch. Martin Mladenov (personal communication, May 5 2021), one of the authors of Rec-\nSim, explained there are hard methodological challenges involved in counterfactually accurate\nsimulations.\nOther studies measure the eﬃciency of a RS using traditional metrics like NDCG from\neq. (2.2), but depending on how relevance is modeled, there are no guarantees that the highest\nexpected reward will output a high NDCG. Some researchers, instead of the expected reward,\nsimplify the simulations using clickthrough-rates or hit-rates. Arguably, simulating reviews\nprovides more accurate simulations, given the higher cardinality of simulated user evaluation.\nBelow is a brief description of recent studies that use RL and the MovieLens dataset, and\ntable 3.2 summarizes them.\nAfkhamizadeh et al. [45], measured their agent using cumulative custom regret metric, but\nit proved hard to understand how it’s calculated and draw comparisons. Their best model\nachieved a cumulative regret of approximately 30 after 100 time-steps.\nThe 2021 study by Huang et al. [46], measured not the expected return, but percentage of\nrecommendation slates that contained a movie the user saw (i.e., Hit-Rate, or HR@N where\nN is the slate size). Their best results are approximately 45% HR@10 for the MovieLens1M\ndataset, after training for an unknown number of steps. Sanz-Cruzado et al. [47] in 2019 showed\non the same dataset a cumulative recall of approximately 0.6 after 3 million time-steps, for ϵ-\ngreedy, k-NN with k = 10, and kNN-based bandits. Another 2019 study by Zou et al. [48] found\nNDCG@3 (from equation 2.2) of 75% using REINFORCE on the same dataset after training\non 90% of the dataset (around 900k time-steps).\nAnother study from 2020 that uses the same style of measurement (but they call it CTR)\nby Yu et al. [49] reported 35% in 30000 time-steps, using a Trust Region Policy Optimization\n(TRPO) algorithm. They experimented with Q-Learning but found TRPO to wield better\nresults.\nThe 2020 study by Liu et al. [50] also used a similar metric, although they called it Precision,\n3.4. Evaluation Agents\n25\nand tested several models. One of the best performing models was DQN, and they found 65%\nPrecision@20 after training on 80% of the user interactions (around 800k time-steps). Very\nsimilar previous works by the same authors [51] [52], on the same dataset, and using the same\nDQN model, had found 54% Precision@20.\nYear\nMetric\nBest Result\nTimesteps\nReference\n2017\nCum. Regret\n30\n100\n[45]\n2019\nCum. Recall\n0.6\n3M\n[47]\n2019\nNDCG@3\n0.75\n900K\n[48]\n2020\nCTR\n0.35\n30K\n[49]\n2020\nPrecision@20\n0.8\n800K\n[50]\n2020\nPrecision@20\n0.54\n800K\n[51]\n2021\nHR@10\n0.45\nN/A\n[46]\nTable 3.2: Comparison of recent studies that train RL agents using the MovieLens dataset.\nNote the diversity of metrics.\n3.4\nEvaluation Agents\nThe evaluation of the performance of our RL agents will be done using three distinct agents:\na Dueling DQN implementation that uses PER buﬀers and noisy networks, a REINFORCE\nimplementation using discounted episode rewards as a baseline, and an Actor-Critic implemen-\ntation that uses a value estimator as the critic. The source code will be written in Python,\nwith tools such as Jupyter notebooks, pandas, and Pytorch. The code is made publicly avail-\nable on the source-control management platform Github1 and the notebooks are provided for\nreproducibility.\nThe Dueling DQN neural network is fully connected with one input neuron for each obser-\nvation variable (there are 25), hidden noisy layers with linear neurons and ReLU activation,\na noisy value-estimator subnetwork with ReLU activation, a noisy advantage-estimator sub-\nnetwork with ReLU activation, and the noisy output layer with one neuron for each movie\n(currently there are 3883). The hidden layers are model parameters. The network uses the\nAdam optimizer, and the learning rate η is a parameter. The noisy layers use gaussian noise\nthat helps with exploration, and the starting noise σ is a parameter. The discount factor γ\nused to calculate the TD-error is also a parameter. The PER is also parametrized with the\nbuﬀer size, the burn-in, the batch size, the priority importance α, the weight eﬀect β, the\nβ annealing coeﬃcient, and the minimum allowable priority ϵ. The frequencies used to train\n1github.com/luksfarris/pydeeprecsys\n26\nMaterials And Methods\nthe main network, and to update the target network, are also parameters of the model. The\npseudo-code is demonstrated in algorithm 1.\nAlgorithm 1: Dueling DQN with Prioritized Experience Buﬀer and Noisy Layers\nLet Qθ be the main network with weights θ and noisy layers with noise σ;\nLet Q′\nθ′ be the target network;\nLet PER be the prioritized experience buﬀer of size ND, burn-in Nin, batch size Nbatch,\npriority importance α, weight eﬀect β, annealing βa, and minimum priority ϵ;\nLet P be the priorities of the experiences of PER;\nLet Ftrain be the frequency at which the main network is trained;\nLet Fsync be the frequency at which the target network is updated;\nLet c be the step count;\nLet η be the learning rate;\nfor each Time-Step t, St, γt ∈(0, 1) do\nLet ready ←(|PER| > Nin);\nLet At be argmax Q′\nθ′(St) if ready else a random valid action;\nLet Rt, St+1 be the environment’s reward and next state for At;\nif ready then\nc ←c + 1;\nif c mod Ftrain = 0 then\nLet Π ←P α;\nLet b be a randomly sampled batch of size Nbatch from PER using\nprobabilities Π;\n// Importance Sampling\nLet wb ←(\n1\nNbatch\n1\nPb)β be their weights;\n// Weight annealing\nβ ←min(β + βa, 1);\n// TD Error following the DDQN update\nLet δ ←Qθ(Sb, Ab) −γt Q′\nθ′(S′b, argmax Qθ(Sb)) + Rb;\n// Weighed mean square errors\nApply gradient descent θ ←θ η (δ2 wb)\n|b|\n∇θ Qθ(Sb, Ab);\nPi ←δi + ϵ for every experience i in b;\nend\nif c mod Fsync = 0 then\n// Target network update, copy the weights θ over to Q′\nθ′ ←θ;\nend\nend\nStore (St, St+1, At, Rt) in PER with priority max(P);\nend\nThe REINFORCE model does not use noisy layers, uses tanh activation, and softmax output\nneurons. The hidden layers and learning rate η are parameters of this model. The discount\n3.5. Evaluation Methods\n27\nfactor γ used to calculate the baseline is also a parameter. The network is trained at the end\nof each episode. Predictions are made sampling from the predicted multinomial probability\ndistribution, meaning the policy is stochastic.\nThe Actor-Critic agent includes the same parameters from the REINFORCE agent, namely\nthe learning rate ηactor, the actor hidden layers, and the discount factor γ. Additionally, it\nincludes the critic’s hidden layers and learning rate ηcritic. The critic’s network is fully connected\nand it uses ReLU activation. Both networks are trained at the end of each episode.\nRegarding the loss calculations and subsequent backpropagation, the Dueling DQN agent\nuses the mean of the squared TD-errors of the batch, multiplied by their PER weights. For more\ndetailed representations of the network architectures and gradient calculations, please refer to\nthe Appendix A. As in DDQN agents, the TD-error is calculated using the target network\nto estimate the expected Q-value of the next state. The feedforward logic that combines the\nestimated value and advantage is detailed in eq. (3.1).\nQ(s, a) = V (s) + Adv(s, a) −1\n|A|\nX\na′∈A\nAdv(s, a′)\n(3.1)\nThe subtraction of the average advantage is needed due to the identiﬁability problem (i.e.,\ndecomposing Q into V and Adv).\nWhile the critic error is the TD-error, the REINFORCE and Actor loss functions are the\napplication of the Policy Gradient Theorem, as expected, and are described in eq. (3.2).\n∇θJ(θ) = Eπ[Gt∇θlnπθ(a|s)]\n(3.2)\n3.5\nEvaluation Methods\nDue to the large number of parameters in each agent, performing an exhaustive hyperparameter\nspace search would not be eﬃcient. To that eﬀect, following the methodology from [10], we\nwill test one parameter at a time, freezing the remaining ones. Each combination is tested 3\ntimes for 500 episodes, and the average of the ﬁnal moving mean (100 episode window) of the\nReturn will be used for comparison. The best value found for each parameter is chosen in the\nﬁnal model.\nOnce models have the best parameters found, we run the comparison for 5 diﬀerent seeds\nand 2000 episodes. The moving mean will be used to compare the agents, along with its 95%\nconﬁdence interval. A random recommendation baseline is included for comparison. After the\nbest agent is chosen, we run a training with slates of size 10 to compute the NDCG metric.\nThis runs again for 5 seeds, and the agent is trained for 500 episodes (25K time-steps).\n28\nMaterials And Methods\n3.6\nImplementation Details\nThe source-code used to run the tests was developed using the git source control technology,\nand stored online and made available publicly on the Github website. The needed changes made\nto the MLFairness repository were made in a fork, available as a submodule. The repository is\nlicensed under the MIT license, which allows the code to be used privately and commercially, but\nwithout liabilities or warranties. The repository contains wiki pages with detailed instructions\non installation and execution.\nAdditionally, the repository contains jupyter notebooks with\nexample usage of the environment.\nThe source code was designed using the Object Oriented Programming (OOP) paradigm.\nUsing inheritance to create component abstractions allowed for quicker development with less\nneed for duplication. A class diagram containing the main classes, attributes, and functions is\navailable in ﬁg. 3.1. The main base classes created encapsulate common aspects of environments,\nneural networks, RL agents, and the experience buﬀers.\nReinforcementLearning\n+ action_for_state \n + top_k_actions_for_state \n + store_experience\nReinforceAgent\npolicy_estimator\n+ discounted_rewards \n + learn_from_experiences\nActorCriticAgent\npolicy_estimator \n value_estimator\n+ learn_from_experiences\nRainbowDQNAgent\nnetwork \n target_network \n buffer\n+ check_update_network\nPolicyEstimator\n+ predict \n + action_probabilities \n + update\nValueEstimator\n+ predict \n + update\nDuelingDDQN\n+ get_q_values \n + top_k_actions_for_state \n + learn_with\nPrioritizedExperienceReplayBuffer\nalpha \n beta\n+ priorities \n + update_priorities\nBaseNetwork\n+ run_backpropagation \n + save \n + load\nNoisyLayer\nsigma_weigth \n sigma_bias\nExperienceBuffer\n+ ready_to_predict \n + sample_batch \n + store_experience\nExperienceReplayBuffer\nbatch_size\nFigure 3.1: Class diagram covering the most important parts of the implementation.\nThe ReinforcementLearning abstract class deﬁnes what functions agents are expected\n3.6. Implementation Details\n29\nto have. Namely, to predict the next action to be taken on a given state, the next k best\nactions (for slate environments), and to store a particular experience (i.e., a state, action,\nreward, done, next observed state) tuple.\nThe BaseNetwork base class implements helper\nfunctions like saving and loading from a ﬁle, freezing speciﬁc parameters, running backward\npropagation of a loss, plotting the gradient graph, conﬁguring the hardware device (i.e., CPU or\nGPU). The ExperienceBuffer interface expects implementations for experiences to be stored,\nsampled, and for the buﬀer to tell if there are enough experiences to start making predictions.\nAdditionally, a LearningStatistics module was developed to help collect diﬀerent metrics\nthat agents may output while training, providing ways to retrieve, plot, and aggregate them\non many levels (i.e., model, episode, time-step, environment). Finally, the Manager module\ncoordinates the sending and receiving of actions and states.\nManagers help with training\nagents, hyperparameter search, executing episodes, and printing overviews of environments.\nThe project was developed using modern best-practices, such as automatic code formatting\nand linting (using python packages black and ﬂake8) that help with the readability of the code.\nThe project also uses a dependency-management tool to download, version, and update the\nseveral libraries needed to run the project (using the poetry package), which makes the project\nusable with just a few commands. Unit tests and coverage measurement were implemented to\nhelp provide a stable and reliable project (using pytest and coverage). Finally, it is possible to\ngenerate HTML documentation of all the modules (using pdoc). All these tools are easy to use,\nand described in the project’s documentation.\nChapter 4\nResults And Discussion\n4.1\nPreliminary Results\nUsing the already discussed environment, we can run a comparison of a random agent and a\nDueling DQN agent (as shown in ﬁg. 2.2), to see how they fare in terms of rewards obtained.\nFor the comparison, agents learned for 500 episodes, and each episode contained 50 recom-\nmendations to a randomly selected user. This test was executed 20 times for each agent, and\nﬁg. 4.1 shows the average moving reward of the previous 100 iterations, along with a conﬁdence\ninterval of 95% conﬁdence. It is possible to see, at the last episode’s reward average, that the\nDQN agent had users rating around 3.8/5. Meanwhile, the random agent had ratings around\n3.2/5.\n0\n100\n200\n300\n400\n500\nEpisodes\n26\n28\n30\n32\n34\n36\n38\nEpisode Total Reward Moving Average\nmodel\nDuelingDQN\nRandomAgent\nFigure 4.1: Comparison of an agent that recommends movies randomly (orange) and an agent\nthat learns from user reviews (blue).\nThis is already a signiﬁcant diﬀerence in terms of user satisfaction, and the plot indicates\nthat further learning iterations would only increase the diﬀerence. Also, no hyperparameter\n30\n4.2. Hyperparameter Search\n31\noptimization was done in this model. This preliminary result approximates the 2018 ﬁndings\nof Choi et al. [30], whose models (Q-Leaning and SARSA) approximate return 8 out of 10 after\n200 episodes.\n4.2\nHyperparameter Search\nWhen comparing the results of the executions, it was found that taking only the means was not\nenough to make a decision on the optimal parameters. The standard deviation also provided\nvaluable data to choose the best hyperparameters. To that end, the following plots contain the\n95% conﬁdence interval of the results of the executions, along with the means. The complete\nexecution results can be found in Appendix B.\nThe results for the hyperparameter search with the REINFORCE model can be found in\nﬁg. 4.2. We found that the policy estimator network with two hidden layers of 128 neurons\nworked best. Additionally, we found highest returns with a discount factor γ of 90% and a\nlearning rate η of 0.001.\n[64, 64]\n[128, 128]\n[256, 256]\nparam_value\n31.5\n32.0\n32.5\n33.0\n33.5\nReturn\nhidden_layers\n0.9\n0.95\n0.99\nparam_value\n30\n31\n32\n33\n34\n35\ndiscount_factor\n1e-05\n0.0001\n0.001\nparam_value\n31\n32\n33\n34\nlearning_rate\nFigure 4.2: Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the\nREINFORCE agent.\nNotice how γ = 0.95 had the highest average, but also the highest\nvariance. Therefore it would be wiser to choose 0.9 in this case.\nSince the policy estimator is similar to the Actor network in the Actor-Critic agent, we\nfocused its tests on the critic parameters. Results can be found in ﬁg. 4.3. The best critic\nnetwork had hidden layers of 128 and 64 neurons, a γ of 0.99, and ηcritic of 0.0001.\n[64, 32]\n[128, 64]\n[256, 128]\nparam_value\n22\n24\n26\n28\nReturn\ncritic_hidden_layers\n0.9\n0.95\n0.99\nparam_value\n22\n24\n26\n28\n30\n32\ndiscount_factor\n1e-05\n0.0001\n0.001\nparam_value\n22\n24\n26\n28\ncritic_learning_rate\nFigure 4.3: Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the\nActor-Critic agent.\n32\nResults And Discussion\nThe results from the Dueling DQN parameter search can be found in ﬁg. 4.4. It was found\nthat the best network update frequency was 3, the target network sync frequency was 300, the\npriority importance was 0.4, the priority weight growth (β-annealing) was 0.01, the buﬀer size\nwas 10000, the buﬀer burn-in was 1000, the batch size was 32, the gaussian noise σ was 0.017,\nand the best architecture was with two hidden layers of 512 neurons, and 128 neurons for the\nadvantage and value sub-networks. During the tests we used a γ of 0.95, and η of 0.001.\n0.0001\n0.001\n0.01\n26\n28\n30\n32\n34\n36\nReturn\npriority_weigth_growth\n5000\n10000\n30\n32\n34\nbuffer_size\n500\n1000\n1500\n30\n32\n34\n36\n38\nbuffer_burn_in\n3\n5\n7\n27.5\n30.0\n32.5\n35.0\n37.5\nReturn\nnetwork_update_frequency\n100\n200\n300\n22.5\n25.0\n27.5\n30.0\n32.5\nnetwork_sync_frequency\n0.4\n0.6\n0.8\n27.5\n30.0\n32.5\n35.0\n37.5\npriority_importance\n16\n32\n48\nparam_value\n28\n30\n32\n34\n36\nReturn\nbatch_size\n0.1\n0.017\n0.25\nparam_value\n26\n28\n30\n32\n34\nnoise_sigma\n[128, 128, 32, 32]\n[256, 256, 64, 64]\n[512, 512, 128, 128]\nparam_value\n32.5\n35.0\n37.5\n40.0\nhidden_layers\nFigure 4.4: Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the\nDueling DQN agent.\n4.3\nBenchmark Results\nThe results of the benchmark for all the agents can be found in ﬁg. 4.5. The Dueling DQN agent\nwas trained for 3 seeds, while the others were trained for 5. Inspecting the plot allows to identify\nthat the DQN agent clearly outperformed the others. Moreover, it seems that the Actor-Critic\nagent hasn’t managed to get results better than random. There may be some extra challenges\nin modeling the recommendation choices as probability distributions. Given that those agents\nhave stochastic policies, it may be that small diﬀerences in the output nodes cause sub-optimal\n4.3. Benchmark Results\n33\nmovies to be sampled too often. It may also be that it needed more training episodes, due\nto the high variance of the reward signal. A combination of more sophisticated Actor-Critic\nagents (e.g., Soft Actor-Critic, A2C) could potentially bring powerful improvements as well.\n30\n40\nReturn Mov. Avg.\nRandom Agent Baseline\nDuelingDQN\n25\n30\n35\n40\nReturn Mov. Avg.\nREINFORCE\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nepisode\n20\n30\n40\nReturn Mov. Avg.\nActor-Critic\nFigure 4.5: Comparison of training of the agents with the best hyper-parameters found. The\nhorizontal line at return 32, symbolizing the performance of a random agent, was added to\nhelp compare them. The area around the lines is the 95% conﬁdence interval. The Dueling\nDQN agent was trained for 3 seeds, while the others were trained for 5. In this scenario, the\nmaximum possible reward is 50, and it would mean that the user rated 5/5 for every single\nrecommended movie.\nAn interesting factor to analyze in this sort of setup is that the agent starts oﬀwithout any\nprevious knowledge of the environment. In the RS literature this is referred to as the Cold-\nStart Problem. The ability of an agent to deliver as little “bad” recommendations as possible\nis very valuable in commercial situations, when the stakes are higher. From this perspective,\nand reviewing the results from ﬁg. 4.5, the REINFORCE agent seems to make less errors in\nthe short term. In the long term, however, it fails to deliver predictions as good as the ones\nfrom the DQN agent.\n34\nResults And Discussion\n4.4\nRelevance Measurements\nUsing the best model found (the DQN agent), we can execute simulations with slates (i.e.,\nwhen an agent recommends multiple movies at each time-step). This allows us to calculate\nmore traditional RS relevance metrics. Results can be found in table 4.1.\nDCG@10 Random\nDCG@10 DQN\nNDCG@10 Random\nNDCG@10 DQN\nMean\n17.5560\n19.5429\n0.7727\n0.8602\nStd\n4.5842\n2.2787\n0.2017\n0.1003\nMin\n9.2098\n13.1055\n0.4054\n0.5768\nMedian\n19.2187\n20.1940\n0.8459\n0.8889\nMax\n21.5615\n22.7177\n0.9491\n1.0\nTable 4.1: Comparison of relevance metrics for an agent making random recommendations and\na Dueling DQN agent.\nIt is important to note that the results from other researchers mentioned before had diﬀerent\nways to model relevance. While in their case a relevant movie is a movie that was reviewed\nby a user, in our case relevance is the simulated rating (from 1 to 5).\nAdditionally, when\ncalculating the ideal relevance, we used ratings of 5 (i.e., an ideal slate is one in which the\nchosen recommended movie receives a 5/5 review). Since our agent maximizes the simulated\nrating (part of our deﬁnition of reward), it was expected that the agent would perform better\nthan random.\nIf we measured other interesting RS metrics, like the diversity of recommended movie genres,\nit is possible that we would not see great results. This sort of investigation, optimizing for many\nobjectives, is beyond the scope of this work.\nChapter 5\nConclusions\nA comprehensive search was performed to ﬁnd proper RL environments that simulate realistic\nproblems of a Recommender System. Having found a most suitable one, improvements were\nmade to ﬁx the problems it had. Agents using diﬀerent learning strategies were tested in this\nnovel environment, and the results were comparable to those found by other recent studies.\nThe code was made available and open-source at Github1.\nThe usage of RL allowed for an eﬀective build of a hybrid RS approach, that combines\nfeatures of model-based Collaborative Filtering, and Case-Based systems. It is shown how even\nthough the learning models start with zero information about the items and users, some are\nable to provide meaningful recommendations after around 200 simulated user sessions, thus\neﬀectively handling the Cold-Start Problem.\nThree diﬀerent RL models were implemented, and some hyperparameter search was con-\nducted to conﬁgure them as well as possible. It was found that the model based on estimating\nQ-values using DL worked better consistently. Particularly, the models that had stochastic\npolicies did not perform as well.\nResults were compared based on the ﬁnal episode reward sum average, to demonstrate\nthat models eﬀectively outperform random recommendation strategies. The NDCG@10 metric\nwas computed for the DQN-based model, to demonstrate that it eﬀectively returns relevant\nrecommendation slates.\n1github.com/luksfarris/pydeeprecsys\n35\nChapter 6\nSuggestions for Future Research\nOne of the biggest challenges of researching this topic was ﬁnding suitable learning environ-\nments. Open-source environments were scarce, not actively maintained, only executable on\nspeciﬁc Operational Systems (i.e., Linux), only executable on speciﬁc Python versions (i.e.,\n3.6), and poorly documented. This represents a technical barrier, that prevents the ﬁeld from\nadvancing faster. A great example of learning environments that work out-of-the-box on most\noperational systems and python versions is Open AI Gym, and it should serve as an inspira-\ntion for more RS environments. Additionally, more heterogeneous and larger datasets could be\nincluded, such as the MovieLens1B dataset or the ones from the ACM conferences.\nA second important branch of investigation is on the simulation of users. In other words,\ngiven a ground-truth dataset of user-journeys, ﬁnding a strategy that optimally generalizes\nhow users behave in those particular conditions. While matrix factorization strategies are fast,\npractical, and relatively accurate, their performance might not scale as well as neural-network\nbased approaches. In particular, it seems that Recurrent Neural Networks would be a powerful\ncandidate in this particular problem.\nA third interesting, and maybe the hardest, problem suitable for future research is ﬁnding\nappropriate multi-objective rewards, that simultaneously keep users satisﬁed with the recom-\nmendations, but without compromising the user, nor the items being recommended. For in-\nstance, an optimized food recommender system could recommend only chocolates to a user if\nhealth factors are not considered in the reward. Alternatively, an optimized clothing recom-\nmender system could recommend only pieces from the three major stores, eﬀectively killing\nthe competition. Encoding counterfactual elements in the reward signal could prevent such\nproblems.\nA fourth, very interesting path of research is regarding the Reinforcement Learning agents.\nMore sophisticated Actor-Critic methods seem to have been gaining traction recently on other\napplications, such as games and robotics. In particular, it would be interesting to understand\n36\n37\nif Advantage Actor-Critic Networks or Soft-Actor-Critic (SAC) agents could outperform the\nDueling DQN model, both with deterministic and stochastic policies. A more thorough com-\nparison would also be very interesting, checking if tabular methods would be feasible, and if\ntheir performance would be comparable.\nLastly, there is a lot of room for research regarding the explainability of recommendations.\nParticularly in RS environments, the power of being able to explain why certain recommenda-\ntions are being made is invaluable. While explainability in supervised learning problems has\nbeen explored extensively, there is still a gap in the Deep Reinforcement Learning application\nﬁeld. Speciﬁcally, users could be interested in what is the expected relevance of a particular\ndocument, what factors were relevant in estimating it, and how each factor weighs in.\nBibliography\n[1] Otis Gospodnetic, Erik Hatcher, and Michael McCandless. Lucene in Action. Manning\nPublications, 2nd edition, 2010.\n[2] Kim Falk. Practical Recommender Systems. Manning Publications, ﬁrst edition, January\n2019. ISBN 978-1-61729-270-5. URL https://learning.oreilly.com/library/view/\npractical-recommender-systems/9781617292705/. Accessed on 2021-02-27.\n[3] Maurits Kaptein. Persuasion Proﬁling. Business Contact Publishers, 2015.\n[4] Marlesson R. O. Santana, Luckeciano C. Melo, Fernando H. F. Camargo, Bruno\nBrand˜ao, Anderson Soares, Renan M. Oliveira, and Sandor Caetano. MARS-Gym: A\nGym framework to model, train, and evaluate Recommender Systems for Marketplaces.\narXiv:2010.07035 [cs, stat], September 2020.\n[5] RecSys Challenge ’19:\nProceedings of the Workshop on ACM Recommender Systems\nChallenge, New York, NY, USA, 2019. Association for Computing Machinery.\nISBN\n9781450376679.\n[6] Mouzhi Ge, Carla Delgado, and Dietmar Jannach. Beyond accuracy: Evaluating recom-\nmender systems by coverage and serendipity. In Proceedings of the Fourth ACM Conference\non Recommender Systems, pages 257–260, January 2010. doi: 10.1145/1864708.1864761.\n[7] Pete Chapman, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin\nShearer, and Rudiger Wirth. Crisp-dm 1.0 step-by-step data mining guide. Technical\nreport, The CRISP-DM consortium, August 2000.\n[8] Heather Silyn-Roberts. Writing For Science And Engineering. Elsevier, 2000. ISBN 978-0-\n7506-4636-9. URL https://doi.org/10.1016/B978-0-7506-4636-9.X5000-9. Accessed\non 2021-03-13.\n[9] Charu C. Aggarwal. Recommender Systems. Springer International Publishing, Cham,\n2016. ISBN 978-3-319-29657-9 978-3-319-29659-3. doi: 10.1007/978-3-319-29659-3. URL\nhttp://link.springer.com/10.1007/978-3-319-29659-3. Accessed on 2021-03-13.\n38\nBIBLIOGRAPHY\n39\n[10] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. Adap-\ntive computation and machine learning. MIT Press, Cambridge, Mass, 2 edition, 2018.\nISBN 978-0-262-19398-6.\n[11] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\nURL http://www.deeplearningbook.org. Accessed on 2021-03-03.\n[12] Jure Leskovec, Anand Rajaraman, and JeﬀUllman. Mining Of Massive Datasets. Cam-\nbridge University Press, 3 edition, 2019. URL http://www.mmds.org. Accessed on 2021-\n03-03.\n[13] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience\nReplay. arXiv:1511.05952 [cs], February 2016. URL http://arxiv.org/abs/1511.05952.\narXiv: 1511.05952. Accessed on 2021-03-18.\n[14] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with\nDouble Q-Learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 30(1),\nMarch 2016. ISSN 2374-3468. URL https://ojs.aaai.org/index.php/AAAI/article/\nview/10295. Number: 1. Accessed on 2021-03-18.\n[15] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband,\nAlex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell,\nand Shane Legg. Noisy Networks for Exploration. arXiv:1706.10295 [cs, stat], July 2019.\nURL http://arxiv.org/abs/1706.10295. arXiv: 1706.10295. Accessed on 2021-03-18.\n[16] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Fre-\nitas. Dueling Network Architectures for Deep Reinforcement Learning. In International\nConference on Machine Learning, pages 1995–2003. PMLR, June 2016.\nURL http:\n//proceedings.mlr.press/v48/wangf16.html. ISSN: 1938-7228. Accessed on 2021-03-\n18.\n[17] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-\nmiller. Deterministic Policy Gradient Algorithms. In International Conference on Machine\nLearning, pages 387–395. PMLR, January 2014. URL http://proceedings.mlr.press/\nv32/silver14.html. ISSN: 1938-7228. Accessed on 2021-03-18.\n[18] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep\nReinforcement Learning. In International Conference on Machine Learning, pages 1928–\n1937. PMLR, June 2016.\nURL http://proceedings.mlr.press/v48/mniha16.html.\nISSN: 1938-7228. Accessed on 2021-03-18.\n40\nBIBLIOGRAPHY\n[19] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep Learning Based Recommender Sys-\ntem: A Survey and New Perspectives. ACM Computing Surveys, 52(1):5:1–5:38, February\n2019. ISSN 0360-0300. doi: 10.1145/3285029. URL https://doi.org/10.1145/3285029.\nAccessed on 2021-03-15.\n[20] G. Gupta and R. Katarya. A Study of Recommender Systems Using Markov Decision\nProcess. In 2018 Second International Conference on Intelligent Computing and Control\nSystems (ICICCS), pages 1279–1283, June 2018. doi: 10.1109/ICCONS.2018.8663161.\n[21] Tariq Mahmood and Francesco Ricci. Learning and adaptivity in interactive recommender\nsystems.\nIn Proceedings of the ninth international conference on Electronic commerce,\nICEC ’07, pages 75–84, New York, NY, USA, August 2007. Association for Computing\nMachinery. ISBN 978-1-59593-700-1. doi: 10.1145/1282100.1282114. URL https://doi.\norg/10.1145/1282100.1282114. Accessed on 2021-03-13.\n[22] Tariq Mahmood and Francesco Ricci.\nImproving recommender systems with adaptive\nconversational strategies. In Proceedings of the 20th ACM conference on Hypertext and\nhypermedia, HT ’09, pages 73–82, New York, NY, USA, June 2009. Association for\nComputing Machinery. ISBN 978-1-60558-486-7. doi: 10.1145/1557914.1557930. URL\nhttps://doi.org/10.1145/1557914.1557930. Accessed on 2021-03-14.\n[23] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach\nto personalized news article recommendation.\nIn Proceedings of the 19th international\nconference on World wide web, WWW ’10, pages 661–670, New York, NY, USA, April\n2010. Association for Computing Machinery.\nISBN 978-1-60558-799-8.\ndoi: 10.1145/\n1772690.1772758. URL https://doi.org/10.1145/1772690.1772758. Accessed on 2021-\n03-13.\n[24] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased oﬄine evaluation of\ncontextual-bandit-based news article recommendation algorithms. In Proceedings of the\nfourth ACM international conference on Web search and data mining, WSDM ’11, pages\n297–306, New York, NY, USA, February 2011. Association for Computing Machinery.\nISBN 978-1-4503-0493-1.\ndoi: 10.1145/1935826.1935878.\nURL https://doi.org/10.\n1145/1935826.1935878. Accessed on 2021-03-14.\n[25] Satyen Kale, Lev Reyzin, and Robert E. Schapire. Non-stochastic bandit slate problems.\nIn Proceedings of the 23rd International Conference on Neural Information Processing\nSystems - Volume 1, NIPS’10, pages 1054–1062, Red Hook, NY, USA, December 2010.\nCurran Associates Inc.\nBIBLIOGRAPHY\n41\n[26] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual Bandits with Linear\nPayoﬀFunctions. In Proceedings of the Fourteenth International Conference on Artiﬁcial\nIntelligence and Statistics, pages 208–214. JMLR Workshop and Conference Proceedings,\nJune 2011. URL http://proceedings.mlr.press/v15/chu11a.html. ISSN: 1938-7228.\nAccessed on 2021-03-14.\n[27] Djallel Bouneﬀouf, Amel Bouzeghoub, and Alda Lopes Gan¸carski. A Contextual-Bandit\nAlgorithm for Mobile Context-Aware Recommender System. In Tingwen Huang, Zhigang\nZeng, Chuandong Li, and Chi Sing Leung, editors, Neural Information Processing, Lecture\nNotes in Computer Science, pages 324–331, Berlin, Heidelberg, 2012. Springer. ISBN 978-\n3-642-34487-9. doi: 10.1007/978-3-642-34487-9 40.\n[28] Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-Conﬁdence\nOﬀ-Policy Evaluation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 29\n(1), February 2015. ISSN 2374-3468. URL https://ojs.aaai.org/index.php/AAAI/\narticle/view/9541. Number: 1. Accessed on 2021-03-13.\n[29] Georgios Theocharous, Philip S. Thomas, and Mohammad Ghavamzadeh. Ad Recommen-\ndation Systems for Life-Time Value Optimization. In Proceedings of the 24th International\nConference on World Wide Web, WWW ’15 Companion, pages 1305–1310, New York,\nNY, USA, May 2015. Association for Computing Machinery. ISBN 978-1-4503-3473-0.\ndoi: 10.1145/2740908.2741998. URL https://doi.org/10.1145/2740908.2741998. Ac-\ncessed on 2021-03-13.\n[30] Sungwoon Choi, Heonseok Ha, Uiwon Hwang, Chanju Kim, Jung-Woo Ha, and Sungroh\nYoon. Reinforcement Learning based Recommender System using Biclustering Technique.\nJanuary 2018. URL https://arxiv.org/abs/1801.05532v1. Accessed on 2021-03-16.\n[31] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie,\nand Zhenhui Li. DRN: A Deep Reinforcement Learning Framework for News Recommenda-\ntion. In Proceedings of the 2018 World Wide Web Conference, WWW ’18, pages 167–176,\nRepublic and Canton of Geneva, CHE, April 2018. International World Wide Web Confer-\nences Steering Committee. ISBN 978-1-4503-5639-8. doi: 10.1145/3178876.3185994. URL\nhttps://doi.org/10.1145/3178876.3185994. Accessed on 2021-03-15.\n[32] Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong Tang.\nStabilizing Reinforcement Learning in Dynamic Environment with Application to Online\nRecommendation. In Proceedings of the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, KDD ’18, pages 1187–1196, New York, NY,\n42\nBIBLIOGRAPHY\nUSA, July 2018. Association for Computing Machinery. ISBN 978-1-4503-5552-0. doi:\n10.1145/3219819.3220122. URL https://doi.org/10.1145/3219819.3220122. Accessed\non 2021-03-15.\n[33] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. Rec-\nommendations with Negative Feedback via Pairwise Deep Reinforcement Learning.\nIn\nProceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining, KDD ’18, pages 1040–1048, New York, NY, USA, July 2018. Association\nfor Computing Machinery. ISBN 978-1-4503-5552-0. doi: 10.1145/3219819.3219886. URL\nhttps://doi.org/10.1145/3219819.3219886. Accessed on 2021-03-15.\n[34] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep\nreinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM\nConference on Recommender Systems, RecSys ’18, pages 95–103, New York, NY, USA,\nSeptember 2018. Association for Computing Machinery. ISBN 978-1-4503-5901-6. doi:\n10.1145/3240323.3240374. URL https://doi.org/10.1145/3240323.3240374. Accessed\non 2021-03-09.\n[35] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba. Openai gym, 2016.\n[36] Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui\nWu, and Craig Boutilier. RecSim: A Conﬁgurable Simulation Platform for Recommender\nSystems. arXiv:1909.04847 [cs, stat], September 2019. URL http://arxiv.org/abs/\n1909.04847. arXiv: 1909.04847. Accessed on 2021-03-17.\n[37] Bichen Shi, Makbule Gulcin Ozsoy, Neil Hurley, Barry Smyth, Elias Z. Tragos, James\nGeraci, and Aonghus Lawlor. PyRecGym: a reinforcement learning gym for recommender\nsystems. In Proceedings of the 13th ACM Conference on Recommender Systems, RecSys\n’19, pages 491–495, New York, NY, USA, September 2019. Association for Computing\nMachinery. ISBN 978-1-4503-6243-6. doi: 10.1145/3298689.3346981. URL https://doi.\norg/10.1145/3298689.3346981. Accessed on 2021-03-17.\n[38] Marlesson R. O. Santana, Luckeciano C. Melo, Fernando H. F. Camargo, Bruno\nBrand˜ao, Anderson Soares, Renan M. Oliveira, and Sandor Caetano. MARS-Gym: A\nGym framework to model, train, and evaluate Recommender Systems for Marketplaces.\narXiv:2010.07035 [cs, stat], September 2020. URL http://arxiv.org/abs/2010.07035.\narXiv: 2010.07035. Accessed on 2021-03-17.\nBIBLIOGRAPHY\n43\n[39] Dietmar Jannach, Gabriel de Souza P. Moreira, and Even Oldridge.\nWhy Are Deep\nLearning Models Not Consistently Winning Recommender Systems Competitions Yet?\nA Position Paper.\nIn Proceedings of the Recommender Systems Challenge 2020, Rec-\nSysChallenge ’20, pages 44–49, New York, NY, USA, September 2020. Association for\nComputing Machinery. ISBN 978-1-4503-8835-1. doi: 10.1145/3415959.3416001. URL\nhttps://doi.org/10.1145/3415959.3416001. Accessed on 2021-03-17.\n[40] Benedikt Schiﬀerer, Gilberto Titericz, Chris Deotte, Christof Henkel, Kazuki Onodera,\nJiwei Liu, Bojan Tunguz, Even Oldridge, Gabriel De Souza Pereira Moreira, and Ah-\nmet Erdem.\nGPU Accelerated Feature Engineering and Training for Recommender\nSystems.\nIn Proceedings of the Recommender Systems Challenge 2020, RecSysChal-\nlenge ’20, pages 16–23, New York, NY, USA, September 2020. Association for Com-\nputing Machinery.\nISBN 978-1-4503-8835-1.\ndoi:\n10.1145/3415959.3415996.\nURL\nhttps://doi.org/10.1145/3415959.3415996. Accessed on 2021-03-09.\n[41] Alexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D. Sculley, and\nYoni Halpern.\nFairness is not static: Deeper understanding of long term fairness via\nsimulation studies. In Proceedings of the 2020 Conference on Fairness, Accountability,\nand Transparency, FAccT ’20, page 525–534, New York, NY, USA, 2020. Association\nfor Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3372878. URL\nhttps://doi.org/10.1145/3351095.3372878. Accessed on 2021-03-18.\n[42] F. Maxwell Harper and Joseph A. Konstan. The MovieLens Datasets: History and Con-\ntext. ACM Transactions on Interactive Intelligent Systems, 5(4):19:1–19:19, December\n2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/2827872.\nAccessed on 2021-05-05.\n[43] Jesse Vig, Shilad Sen, and John Riedl. The Tag Genome: Encoding Community Knowledge\nto Support Novel Interaction. ACM Transactions on Interactive Intelligent Systems, 2\n(3):13:1–13:44, September 2012. ISSN 2160-6455. doi: 10.1145/2362394.2362395. URL\nhttps://doi.org/10.1145/2362394.2362395. Accessed on 2021-05-05.\n[44] Yehuda Koren, Robert Bell, and Chris Volinsky.\nMatrix Factorization Techniques for\nRecommender Systems.\nComputer, 42(8):30–37, August 2009.\nISSN 1558-0814.\ndoi:\n10.1109/MC.2009.263. Conference Name: Computer.\n[45] Mostafa Afkhamizadeh, Alexei Avakov, and Reza Takapoui. Automated Recommendation\nSystems. page 5, 2017.\n44\nBIBLIOGRAPHY\n[46] Liwei Huang, Mingsheng Fu, Fan Li, Hong Qu, Yangjun Liu, and Wenyu Chen. A deep\nreinforcement learning based long-term recommender system. Knowledge-Based Systems,\n213:106706, February 2021. ISSN 09507051. doi: 10.1016/j.knosys.2020.106706. URL\nhttps://linkinghub.elsevier.com/retrieve/pii/S0950705120308352.\nAccessed on\n2021-03-09.\n[47] Javier Sanz-Cruzado, Pablo Castells, and Esther L´opez. A simple multi-armed nearest-\nneighbor bandit for interactive recommendation. In Proceedings of the 13th ACM Con-\nference on Recommender Systems, pages 358–362, Copenhagen Denmark, September\n2019. ACM.\nISBN 978-1-4503-6243-6.\ndoi:\n10.1145/3298689.3347040.\nURL https:\n//dl.acm.org/doi/10.1145/3298689.3347040. Accessed on 2021-03-09.\n[48] Lixin Zou, Long Xia, Zhuoye Ding, Dawei Yin, Jiaxing Song, and Weidong Liu. Rein-\nforcement Learning to Diversify Top-N Recommendation. In Guoliang Li, Jun Yang, Joao\nGama, Juggapong Natwichai, and Yongxin Tong, editors, Database Systems for Advanced\nApplications, Lecture Notes in Computer Science, pages 104–120, Cham, 2019. Springer\nInternational Publishing. ISBN 978-3-030-18579-4. doi: 10.1007/978-3-030-18579-4 7.\n[49] Yang Yu, Zhenhao Gu, Rong Tao, Jingtian Ge, and Kenglun Chang. Interactive Search\nBased on Deep Reinforcement Learning. arXiv:2012.06052 [cs], December 2020. URL\nhttp://arxiv.org/abs/2012.06052. arXiv: 2012.06052. Accessed on 2021-02-22.\n[50] Feng Liu, Ruiming Tang, Huifeng Guo, Xutao Li, Yunming Ye, and Xiuqiang He. Top-\naware reinforcement learning based recommendation. Neurocomputing, 417:255–269, De-\ncember 2020. ISSN 0925-2312. doi: 10.1016/j.neucom.2020.07.057. URL https://www.\nsciencedirect.com/science/article/pii/S0925231220311656. Accessed on 2021-03-\n09.\n[51] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng\nGuo, Yuzhou Zhang, and Xiuqiang He.\nState representation modeling for deep rein-\nforcement learning based recommendation. Knowledge-Based Systems, 205:106170, Oc-\ntober 2020.\nISSN 0950-7051.\ndoi: 10.1016/j.knosys.2020.106170.\nURL https://www.\nsciencedirect.com/science/article/pii/S095070512030407X. Accessed on 2021-03-\n09.\n[52] Feng Liu, Huifeng Guo, Xutao Li, Ruiming Tang, Yunming Ye, and Xiuqiang He. End-to-\nEnd Deep Reinforcement Learning based Recommendation with Supervised Embedding.\nIn Proceedings of the 13th International Conference on Web Search and Data Mining,\nWSDM ’20, pages 384–392, New York, NY, USA, January 2020. Association for Computing\nBIBLIOGRAPHY\n45\nMachinery. ISBN 978-1-4503-6822-3. doi: 10.1145/3336191.3371858. URL https://doi.\norg/10.1145/3336191.3371858. Accessed on 2021-03-09.\nAppendix A\nNetwork Architectures\n46\n47\n (3883)\nAddBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nReluBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nReluBackward0\nUnsqueezeBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nReluBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nTBackward\nAccumulateGrad\nfully_connected_1.weight\n (256, 25)\nAccumulateGrad\nfully_connected_1.bias\n (256)\nTBackward\nAddBackward0\nAccumulateGrad\nfully_connected_2.weight\n (256, 256)\nMulBackward0\nAccumulateGrad\nfully_connected_2.sigma_weight\n (256, 256)\nAddBackward0\nAccumulateGrad\nfully_connected_2.bias\n (256)\nMulBackward0\nAccumulateGrad\nfully_connected_2.sigma_bias\n (256)\nTBackward\nAddBackward0\nAccumulateGrad\nvalue_subnet.0.weight\n (64, 256)\nMulBackward0\nAccumulateGrad\nvalue_subnet.0.sigma_weight\n (64, 256)\nAddBackward0\nAccumulateGrad\nvalue_subnet.0.bias\n (64)\nMulBackward0\nAccumulateGrad\nvalue_subnet.0.sigma_bias\n (64)\nTBackward\nAccumulateGrad\nvalue_subnet.2.weight\n (1, 64)\nAccumulateGrad\nvalue_subnet.2.bias\n (1)\nSubBackward0\nAddBackward0\nMeanBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nReluBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nTBackward\nAddBackward0\nAccumulateGrad\nadvantage_subnet.0.weight\n (64, 256)\nMulBackward0\nAccumulateGrad\nadvantage_subnet.0.sigma_weight\n (64, 256)\nAddBackward0\nAccumulateGrad\nadvantage_subnet.0.bias\n (64)\nMulBackward0\nAccumulateGrad\nadvantage_subnet.0.sigma_bias\n (64)\nTBackward\nAccumulateGrad\nadvantage_subnet.2.weight\n (3883, 64)\nAccumulateGrad\nadvantage_subnet.2.bias\n (3883)\nFigure A.1: Dueling DQN gradient graph.\n48\nNetwork Architectures\n (1)\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nReluBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nReluBackward0\nAddBackward0\nSqueezeBackward3\nMmBackward\nTBackward\nAccumulateGrad\nmodel.0.weight\n (50, 25)\nAccumulateGrad\nmodel.0.bias\n (50)\nTBackward\nAccumulateGrad\nmodel.2.weight\n (12, 50)\nAccumulateGrad\nmodel.2.bias\n (12)\nTBackward\nAccumulateGrad\nmodel.4.weight\n (1, 12)\nAccumulateGrad\nmodel.4.bias\n (1)\nFigure A.2: Value estimator gradient graph.\n49\n (3883)\nSoftmaxBackward\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nTanhBackward\nAddBackward0\nSqueezeBackward3\nMmBackward\nUnsqueezeBackward0\nTanhBackward\nAddBackward0\nSqueezeBackward3\nMmBackward\nTBackward\nAccumulateGrad\nmodel.0.weight\n (50, 25)\nAccumulateGrad\nmodel.0.bias\n (50)\nTBackward\nAccumulateGrad\nmodel.2.weight\n (50, 50)\nAccumulateGrad\nmodel.2.bias\n (50)\nTBackward\nAccumulateGrad\nmodel.4.weight\n (3883, 50)\nAccumulateGrad\nmodel.4.bias\n (3883)\nFigure A.3: Policy estimator gradient graph.\nAppendix B\nHyperparameter Search Results\nAgent\nParameter\nValue\nSeed 1\nSeed 2\nSeed 3\nActorCritic\ncritic hidden layers\n[64 32]\n25.1849\n23.7950\n23.8909\nActorCritic\ncritic hidden layers\n[128 64]\n26.6424\n28.3380\n28.6798\nActorCritic\ncritic hidden layers\n[256 128]\n26.9030\n20.4505\n24.0356\nActorCritic\ndiscount factor\n0.9\n32.5063\n24.4924\n22.0036\nActorCritic\ndiscount factor\n0.95\n24.3296\n24.5951\n22.5244\nActorCritic\ndiscount factor\n0.99\n26.2057\n27.1427\n24.5305\nActorCritic\ncritic learning rate\n1e-05\n21.1772\n27.8518\n22.9904\nActorCritic\ncritic learning rate\n0.0001\n23.8572\n25.0933\n23.0319\nActorCritic\ncritic learning rate\n0.001\n24.1299\n22.4342\n21.4234\nReinforce\nhidden layers\n[64 64]\n32.0943\n31.8088\n31.5308\nReinforce\nhidden layers\n[128 128]\n33.8032\n33.6644\n31.4792\nReinforce\nhidden layers\n[256 256]\n31.4428\n32.6030\n33.3391\nReinforce\ndiscount factor\n0.9\n32.0863\n30.8048\n31.5693\nReinforce\ndiscount factor\n0.95\n35.0739\n31.8631\n30.0414\nReinforce\ndiscount factor\n0.99\n31.0604\n31.3833\n29.9745\nReinforce\nlearning rate\n1e-05\n33.4242\n32.1575\n31.0293\nReinforce\nlearning rate\n0.0001\n33.5311\n32.7911\n32.7734\nReinforce\nlearning rate\n0.001\n32.9169\n32.7536\n33.8805\nTable B.1: Actor-Critic and REINFORCE hyperparameter search results.\n50\n51\nAgent\nParameter\nValue\nSeed 1\nSeed 2\nSeed 3\nDuelingDQN\nnetwork update freq.\n3\n30.4253\n34.0480\n39.1959\nDuelingDQN\nnetwork update freq.\n5\n33.2199\n28.9370\n29.6506\nDuelingDQN\nnetwork update freq.\n7\n26.3449\n31.2439\n37.4598\nDuelingDQN\nnetwork sync freq.\n100\n31.7379\n21.0853\n29.4332\nDuelingDQN\nnetwork sync freq.\n200\n31.3731\n29.3101\n27.8519\nDuelingDQN\nnetwork sync freq.\n300\n29.8941\n31.2043\n33.5061\nDuelingDQN\npriority importance\n0.4\n30.5110\n31.8823\n36.6239\nDuelingDQN\npriority importance\n0.6\n27.9894\n30.5443\n33.4694\nDuelingDQN\npriority importance\n0.8\n29.5587\n29.5457\n39.1650\nDuelingDQN\npriority weight growth\n0.0001\n29.8128\n29.3818\n35.5804\nDuelingDQN\npriority weight growth\n0.001\n31.8397\n26.3807\n35.7185\nDuelingDQN\npriority weight growth\n0.01\n30.0311\n32.2882\n33.9274\nDuelingDQN\nbuﬀer size\n5000\n28.9867\n30.3517\n32.8997\nDuelingDQN\nbuﬀer size\n10000\n34.8314\n30.9879\n31.6142\nDuelingDQN\nbuﬀer burn in\n500\n29.2160\n29.6616\n29.5514\nDuelingDQN\nbuﬀer burn in\n1000\n30.7920\n38.3978\n37.7760\nDuelingDQN\nbuﬀer burn in\n1500\n30.5391\n29.8369\n30.7158\nDuelingDQN\nbatch size\n16\n27.9476\n30.5927\n31.7432\nDuelingDQN\nbatch size\n32\n33.9847\n34.2276\n32.9230\nDuelingDQN\nbatch size\n48\n28.9723\n33.2276\n38.9923\nDuelingDQN\nnoise sigma\n0.1\n30.4463\n29.7764\n33.7494\nDuelingDQN\nnoise sigma\n0.017\n33.6226\n31.9494\n33.0891\nDuelingDQN\nnoise sigma\n0.25\n28.7930\n25.2711\n27.8358\nDuelingDQN\nhidden layers\n[128 128 32 32]\n30.6985\n33.7101\n36.7923\nDuelingDQN\nhidden layers\n[256 256 64 64]\n31.1062\n33.7865\n32.8018\nDuelingDQN\nhidden layers\n[512 512 128 128]\n34.3929\n40.4306\n41.7473\nTable B.2: Dueling DQN hyperparameter search results.\n",
  "categories": [
    "cs.IR",
    "cs.LG"
  ],
  "published": "2021-10-06",
  "updated": "2021-10-06"
}