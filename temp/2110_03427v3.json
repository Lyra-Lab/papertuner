{
  "id": "http://arxiv.org/abs/2110.03427v3",
  "title": "Is Attention always needed? A Case Study on Language Identification from Speech",
  "authors": [
    "Atanu Mandal",
    "Santanu Pal",
    "Indranil Dutta",
    "Mahidas Bhattacharya",
    "Sudip Kumar Naskar"
  ],
  "abstract": "Language Identification (LID) is a crucial preliminary process in the field\nof Automatic Speech Recognition (ASR) that involves the identification of a\nspoken language from audio samples. Contemporary systems that can process\nspeech in multiple languages require users to expressly designate one or more\nlanguages prior to utilization. The LID task assumes a significant role in\nscenarios where ASR systems are unable to comprehend the spoken language in\nmultilingual settings, leading to unsuccessful speech recognition outcomes. The\npresent study introduces convolutional recurrent neural network (CRNN) based\nLID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)\ncharacteristics of audio samples. Furthermore, we replicate certain\nstate-of-the-art methodologies, specifically the Convolutional Neural Network\n(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with\nattention), and conduct a comparative analysis with our CRNN-based approach. We\nconducted comprehensive evaluations on thirteen distinct Indian languages and\nour model resulted in over 98\\% classification accuracy. The LID model exhibits\nhigh-performance levels ranging from 97% to 100% for languages that are\nlinguistically similar. The proposed LID model exhibits a high degree of\nextensibility to additional languages and demonstrates a strong resistance to\nnoise, achieving 91.2% accuracy in a noisy setting when applied to a European\nLanguage (EU) dataset.",
  "text": "Natural Language Engineering (2019), 1–00\ndoi:10.1017/xxxxx\nARTICLE\nIs Attention always needed? A Case Study on\nLanguage Identification from Speech\nAtanu Mandal1⋆, Santanu Pal2, Indranil Dutta3, Mahidas Bhattacharya3, and Sudip Kumar Naskar1\n1Department of Computer Science and Engineering, Jadavpur University, Kolkata\n2Wipro AI Lab, Wipro India Limited, Bengaluru\n3School of Languages and Linguistics, Jadavpur University, Kolkata\n⋆Corresponding author. E-mails: atanumandal0491@gmail.com, santanu.pal2@wipro.com,\nindranildutta.lnl@jadavpuruniversity.in, languagemahib@gmail.com, sudipkumar.naskar@jadavpuruniversity.in\nCompeting interests: The author(s) declare none\n(Received xx xxx xxx; revised xx xxx xxx; accepted xx xxx xxx)\nAbstract\nLanguage Identification (LID) is a crucial preliminary process in the field of Automatic Speech\nRecognition (ASR) that involves the identification of a spoken language from audio samples.\nContemporary systems that can process speech in multiple languages require users to expressly desig-\nnate one or more languages prior to utilization. The LID task assumes a significant role in scenarios\nwhere ASR systems are unable to comprehend the spoken language in multilingual settings, leading to\nunsuccessful speech recognition outcomes. The present study introduces convolutional recurrent neural\nnetwork (CRNN) based LID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC) char-\nacteristics of audio samples. Furthermore, we replicate certain state-of-the-art methodologies, specifically\nthe Convolutional Neural Network (CNN) and Attention-based Convolutional Recurrent Neural Network\n(CRNN with attention), and conduct a comparative analysis with our CRNN-based approach. We con-\nducted comprehensive evaluations on thirteen distinct Indian languages and our model resulted in over\n98% classification accuracy. The LID model exhibits high-performance levels ranging from 97% to 100%\nfor languages that are linguistically similar. The proposed LID model exhibits a high degree of extensibil-\nity to additional languages and demonstrates a strong resistance to noise, achieving 91.2% accuracy in a\nnoisy setting when applied to a European Language (EU) dataset.\n1. Introduction\nIn the era of the Internet of Things, smart and intelligent assistants (e.g., Alexa a; Siri b; Cortana\nc; Google Assistant d; etc.) can interact with humans with some default language settings (mostly\nin English) and these smart assistants rely heavily on ASR. The motivation for our work stems\nfrom the inadequacy of virtual assistants in providing support in multilingual settings. In order\nto enhance the durability of intelligent assistants, LID can be implemented to enable automatic\nrecognition of the speaker’s language, thereby facilitating appropriate language setting adjust-\nments. Psychological behaviour exhibits that Humans have an inherent capability to determine\nahttps://developer.amazon.com/en-US/alexa/alexa-voice-service\nbhttps://www.apple.com/in/siri/\nchttps://www.microsoft.com/en-in/windows/cortana\ndhttps://assistant.google.com/\n© Cambridge University Press 2019\narXiv:2110.03427v3  [cs.LG]  25 Oct 2023\n2\nNatural Language Engineering\nthe language of a statement nearly instantly. Automatic LID seeks to classify a speaker’s language\nusage from their speech utterances.\nWe focus our study of LID on Indian Languages since India is the world’s second most popu-\nlated and seventh largest country in landmass and a linguistically diverse country. Currently, India\nhas 28 states and 8 Union Territories, where each state and Union Territories has its own language,\nbut none of the languages is recognised as the national language of the country. Only, English and\nHindi are used as official languages according to the Constitution of India Part XVII Chapter 1\nArticle 343e. Currently, the Eighth Schedule of the Constitution consists of 22 languages. Table\n1 describes the recognised 22 languages according to the Eighth Schedule of the Constitution of\nIndia, as of 1 December 2007.\nTable 1. : List of languages as per the Eighth Schedule of the Constitution of India, as of 1\nDecember 2007 with their language family & states spoken in.\nSl. No.\nLanguage\nFamily\nSpoken in\n1\nAssamese\nIndo-Aryan\nAssam\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n2\nBengali\nIndo-Aryan\nAssam, Jharkhand, Tripura, West Bengal\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n3\nBodo\nSino-Tibetan\nAssam\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n4\nDogri\nIndo-Aryan\nJammu & Kashmir\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n5\nGujarati\nIndo-Aryan\nGujrat, Dadra & Nagar Haveli & Daman & Diu\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n6\nHindi\nIndo-Aryan\nAndaman & Nicobar Islands, Bihar, Chhattisgarh,\nDadra & Nagar Haveli & Daman & Diu, Delhi, Haryana,\nHimachal Pradesh, Jammu & Kashmir, Jharkhand,\nLadakh, Madhya Pradesh, Mizoram, Rajasthan,\nUttar Pradesh, Uttarakhand\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n7\nKannada\nDravidian\nKarnataka\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n8\nKashmiri\nIndo-Aryan\nJammu & Kashmir\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n9\nKonkani\nIndo-Aryan\nDadra & Nagar Haveli & Daman & Diu, Goa\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n10\nMaithili\nIndo-Aryan\nJharkhand\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n11\nMalayalam\nDravidian\nKerala, Lakshadweep, Puducherry\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n12\nManipuri\nSino-Tibetan\nManipur\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n13\nMarathi\nIndo-Aryan\nDadra & Nagar Haveli & Daman & Diu, Goa,\nMaharashtra\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n14\nNepali\nIndo-Aryan\nSikkim, West Bengal\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n15\nOdia\nIndo-Aryan\nJharkhand, Odisha\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n16\nPunjabi\nIndo-Aryan\nDelhi, Haryana, Punjab\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n17\nSanskrit\nIndo-Aryan\nHimachal Pradesh\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n18\nSantali\nAustroasiatic\nJharkhand\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n19\nSindhi\nIndo-Aryan\nRajasthan\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n20\nTamil\nDravidian\nTamil Nadu\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n21\nTelugu\nDravidian\nAndhra Pradesh, Puducherry, Telangana\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\n22\nUrdu\nIndo-Aryan\nBihar, Delhi, Jammu & Kashmir,\nJharkhand, Telangana, Uttar Pradesh\nehttps://www.mea.gov.in/Images/pdf1/Part17.pdf\nMandal et al\n3\nMost of the Indian languages originated from the Indo-Aryan and Dravidian language fami-\nlies. It can be seen from Table 1 that different languages are spoken in different states, however,\nlanguages do not obey geographical boundaries. Therefore, many of these languages, particu-\nlarly in the neighbouring regions, have multiple dialects which are amalgamations of two or more\nlanguages.\nSuch enormous linguistic diversity makes it difficult for citizens to communicate in different\nparts of the country. Bilingualism and multilingualism are the norms in India. In this context,\na LID system becomes a crucial component for any speech-based smart assistant. The biggest\nchallenge and hence an area of active innovation for the Indian language is the reality that most of\nthese languages are under-resourced.\nEvery spoken language has its underlying lexical, speaker, channel, environment, and other\nvariations. The likely differences among various spoken languages are in their phoneme invento-\nries, frequency of occurrence of the phonemes, acoustics, the span of the sound units in different\nlanguages, and intonation patterns at higher levels. The overlap between the phoneme set of\ntwo or more familial languages makes it a challenge for recognition. The low-resource status\nof these languages makes the training of machine learning models doubly difficult. The idea\nbehind our methodology is interesting on account of the aforementioned limitations. Our method-\nology involves forecasting the accurate spoken language, irrespective of the limitations mentioned\nearlier.\nCNN has been heavily utilized by Natural Language Processing (NLP) researchers from the\nvery beginning due to their efficient use of local features. While Recurrent Neural Networks\n(RNNs) have been shown to be effective in a variety of NLP tasks in the past, recent work\nwith Attention-based methods have outperformed all previous models and architectures because\nof their ability to capture global interactions. Yamada et al (2020) were able to achieve bet-\nter results than BERT (Devlin et al 2019), SpanBERT (Joshi et al 2020), XLNet (Yang et al\n2019), and ALBERT (Zhenzong et al 2020) using their Attention-based methods in the Question-\nAnswering domain. Researchers (Takase et al 2021; Gu et al 2019; Chen et al 2020) have\nemployed Attention-based methods to achieve state-of-the-art (SOTA) performance in Machine\nTranslation. Transformers (Vaswani et al 2017), which utilize a self-attention mechanism, have\nfound extensive application in almost all fields of NLP such as language modelling, text clas-\nsification, topic modelling, emotion classification, sentiment analysis, etc., and produced SOTA\nperformance.\nIn this work, we present LID for Indian languages using a combination of CNN, RNN, and\nAttention-based methods. Our LID methods cover 13 Indian languagesf. Additionally, our method\nis language agnostic. The main contributions of this work can be summarized as follows:\n• We carried out exhaustive experiments using CNN, CRNN, and attention-based CRNN for\nthe LID task on 13 Indian languages and achieved state-of-the-art results.\n• The model exhibits exceptional performance in languages that are part of the same language\nfamily, as well as in diverse language sets under both normal and noisy conditions.\n• We empirically proved that CRNN framework achieves better or similar results com-\npared to CRNN with Attention framework although CRNN without Attention requires less\ncomputational overhead.\n2. Related Works\nExtraction of language-dependent features for example prosody and phonemes was widely used\nto classify spoken languages (Zissman 1996; Martinez et al 2020; Ferrer et al 2010). Following\nfThe study was limited to the number of Indian languages for which datasets were available\n4\nNatural Language Engineering\nthe success of speaker verification systems, identity vectors (i-vectors) have also been used as fea-\ntures in various classification frameworks. Use of i-vectors requires significant domain knowledge\n(Dehak et al 2011; Martinez et al 2020). In recent trends, researchers rely on neural networks for\nfeature extraction and classification (Lopez-Moreno et al 2014; Ganapathy et al 2014). Researcher\nRevay and Teschke (2019) used the ResNet50 (He et al 2016) framework for classifying languages\nby generating the log-Mel spectra for each raw audio. The framework uses a cyclic learning rate\nwhere the learning rate increases and then decreases linearly. The maximum learning rate for a\ncycle is set by finding the optimal learning rate using fastai (Howard et al 2020).\nResearcher Gazeau et al (2018) established the use of a Neural Network, Support Vector\nMachine, and Hidden Markov Model (HMM) to identify different languages. Hidden Markov\nmodels convert speech into a sequence of vectors and are used to capture temporal features in\nspeech. Established LID systems (Dehak et al 2011; Martinez et al 2020; Plchot et al 2016; Zazo\net al 2016) are based on identity vector (i-vectors) representations for language processing tasks.\nIn Dehak et al (2011), i-vectors are used as data representations for a speaker verification task and\nfed to the classifier as the input. Dehak et al (2011) applied Support Vector Machines (SVM) with\ncosine kernels as the classifier, while Martinez et al (2020) used logistic regression for the actual\nclassification task. Recent years have found the use of feature extraction with neural networks,\nparticularly with Long Short Term Memory (LSTM) (Zazo et al 2016; Gelly et al 2016; Lozano-\nDiez et al 2015). These neural networks produce better accuracy while being simpler in design\ncompared to the conventional LID methods (Dehak et al 2011; Martinez et al 2020; Plchot et al\n2016). Recent trends in developing LID systems are mainly focused on different forms of LSTMs\nwith DNNs. Plchot et al (2016) used a 3-layered CNN where i-vectors were the input layer and\nsoftmax activation function was the output layer. Zazo et al (2016) used MFCC with Shifted Delta\nCoefficient features as information to a unidirectional layer that is directly connected to a softmax\nclassifier. Gelly et al (2016) used audio transformed to Perceptual Linear Prediction (PLP) coeffi-\ncients and their 1st and 2nd order derivatives as information for a Bidirectional LSTM in forward\nand backward directions. The forward and backward sequences generated from the Bidirectional\nLSTM were joined together and used to classify the language of the input samples. Lozano-Diez\net al (2015) used CNNs for their LID system. They transformed the input data into an image con-\ntaining MFCCs with Shifted Delta Coefficient features. The image represents the time domain for\nthe x-axis and frequency bins for the y-axis.\nLozano-Diez et al (2015) used CNN as the feature extractor for the identity vectors. They\nachieved better performance when combining both the CNN features and identity vectors. Revay\nand Teschke (2019) used ResNet (He et al 2016) framework for language classification by gen-\nerating spectrograms of each audio. Cyclic Learning (Smith 2018) was used where the learning\nrate increases and decreases linearly. Venkatesan et al (2018) utilised MFCCs to infer aspects of\nspeech signals from Kannada, Hindi, Tamil, and Telugu. They obtained an accuracy of 76% and\n73% using Support Vector Machines and Decision Tree classifiers, respectively, on 5 hours of\ntraining data. Mukherjee et al (2019) used CNNs for language identification in German, Spanish,\nand English. They used Filter Banks to extract features from frequency domain representations\nof the signal. Aarti et al (2017) experimented with several auditory features in order to determine\nthe optimal feature set for a classifier to detect Indian Spoken Language. Sisodia et al (2020)\nevaluated Ensemble Learning models for classifying spoken languages such as German, Dutch,\nEnglish, French, and Portuguese. Bagging, Adaboosting, random forests, gradient boosting, and\nadditional trees were used in their ensemble learning models.\nHeracleous et al (2018) presented a comparative study of Deep Neural Networks (DNN) and\nCNNs for Spoken LID, with Support Vector Machines (SVM) as the baseline. They also pre-\nsented the performance of the fusion of the mentioned methods. The NIST 2015 i-vector Machine\nLearning Challenge dataset was used to assess the system’s performance with the goal of detect-\ning 50 in-set languages. Bartz et al (2017) tackled the problem of Language Identification in the\nimage domain rather than the typical acoustic domain. A hybrid CRNN is employed for this,\nMandal et al\n5\nwhich acts on spectrogram images of the provided audio clips. Draghichi et al (2020) tried to\nsolve the task of Language Identification while using Mel-spectrogram images as input features.\nThis strategy was employed in CNNs and CRNN in terms of performance. This work is charac-\nterized by a modified training strategy that provides equal class distribution and efficient memory\nutilisation. Ganapathy et al (2014) reported how they used bottleneck features from a CNN for the\nLID task. Bottleneck features were used in conjunction with conventional acoustic features, and\nperformance was evaluated. Experiments revealed that when a system with bottleneck features is\ncompared to a system without them, average relative improvements of up to 25% are achieved.\nZazo et al (2016) proposed an open-source, end-to-end, LSTM-RNN system that outperforms a\nmore recent reference i-vector system by up to 26% when both are tested on a subset of the NIST\nLanguage Recognition Evaluation with 8 target languages.\nOur research differs from the previous works on LID in the following aspects:\n• Comparison of performance of CNN, CRNN, as well as CRNN with Attention.\n• Extensive experiments with our proposed model show its applicability both for close\nlanguage as well as noisy speech scenarios.\n3. Model Framework\nOur proposed framework consists of three models.\n• CNN-based framework\n• CRNN-based framework\n• CRNN with Attention-based framework\nWe made use of the capacity of CNNs to capture spatial information to identify languages from\naudio samples. In a CNN-based framework, our network uses four convolution layers, where each\nlayer is followed by the ReLU (Nair and Geoffrey 2010) activation function and max pooling with\na stride of 3 and a pool size of 3. The kernel sizes and the number of filters for each convolution\nlayer are (3, 512), (3, 512), (3, 256), and (3, 128), respectively.\nFigure 1 provides a schematic overview of the framework. The CRNN framework passes the\noutput of the Convolutional Module to a Bi-Directional LSTM consisting of a single LSTM with\n256 output units. The LSTM’s activation function is tanh, and its recurrent activation is sigmoid.\nThe Attention Mechanism used in our framework is based on Hierarchical Attention Networks\n(Yang et al 2016). In the Attention Mechanism, contexts of features are summarized with a\nbidirectional LSTM by going forward and backwards.\n−→\nan = −−−→\nLSTM(an), n ∈[1, L]\n(1a)\n←−\nan = ←−−−\nLSTM(an), n ∈[L, 1]\n(1b)\nai = [−→\nan, ←−\nan]\n(1c)\nIn equation 1, L is the number of audio specimens. an is the input sequence for the LSTM\nnetwork. −→\nan and ←−\nan provide the learned vectors from LSTM forward direction and backward\ndirection, respectively. The vector, ai, builds the base for the attention mechanism. The goal of the\nattention mechanism is to learn the model through training with randomly initialized weights and\nbiases. The layer also ensures with the tanh function that the network does not stall. The function\nkeeps the input values between -1 and 1 and maps zeros to near-zero values. The layer with tanh\nfunction is again multiplied by trainable context vector ui. The trainable context vector refers to a\nvector learned during the training process and used as a fixed-length representation of the entire\ninput document. In our framework, the attention mechanism is used to compute a weighted sum of\n6\nNatural Language Engineering\nConvolution\nBlock\nConv1D\nConv1D\nMaxPooling1D\nConv1D\nMaxPooling1D\nConv1D\nLSTM Block\nBackward LSTM\nForward LSTM\nLinear Layer\nFigure 1: The figure presents our CRNN framework consisting of a Convolution Block and LSTM\nBlock denoted in different blocks. The convolution block extracts feature from the input audio.\nThe output of the final convolution layer is provided to the Bi-Directional LSTM network as the\ninput which is further connected to a Linear Layer with softmax classifier.\nthe sequences for each speech utterance, where the weights are learned based on the relevance of\neach sequence to the speech utterances. This produces a fixed-length vector for each utterance that\ncaptures the most salient information in the sequences. The context weight vector ui is randomly\ninitiated and jointly learned during the training process. Improved vectors are represented by a\n′\ni as\nshown in equation 2.\na\n′\ni = tanh(ai · wi + bi) · ui\n(2)\nContext vectors are finally calculated by providing a weight to each Wi by dividing the expo-\nnential values of the previously generated vectors with the summation of all exponential values\nMandal et al\n7\nof previously generated vectors as shown in equation 3. To avoid division by zero, an epsilon is\nadded to the denominator.\nWi =\nexp(a\n′\ni)\n∑i exp(a\n′\ni) + ε\n(3)\nThe sum of these importance weights concatenated with the previously calculated context\nvectors is fed to a linear layer with 13 output units serving as a classifier for the 13 languages.\nwi\nbi\ntanh()\nui\nFigure 2: Schematic diagram of the Attention Module.\nFigure 2 presents the schematic diagram of the Attention Module where ai is the input to the\nmodule and output of the Bi-Directional LSTM layers.\n4. Experiments\n4.1 Feature Extraction\nFor feature extraction of spoken utterances, we used MFCCs. For calculating MFCCs we used\npre emphasis, frame size represented as f size, frame stride represented as f stride, N-point Fast\nFourier transform represented as NFFT, low-frequency mel represented as l f, the number of fil-\nters represented as nfilt, the number of cepstral coefficients represented as ncoe f and cepstral\nlifter represented lifter of values 0.97, 0.025 (25ms), 0.015 (15ms overlapping), 512, 0, 40, 13,\nand 22, respectively. We used a frame size of 25 ms as typically frame sizes in the speech pro-\ncessing domain use 20ms to 40ms with 50% (in our case 15ms) overlapping between consecutive\nframes.\nhf = 2595 × log10(1 + 0.5 × sr\n700\n)\n(4)\nWe used low-frequency mel (lf) as 0 and high-frequency mel (hf) is calculated using the equation\n4. lf and hf are used to generate the non-linear human ear perception of sound, by being more\ndiscriminative at lower frequencies and less discriminative at higher frequencies.\nemphasized signal = [sig[0], sig[1 :] −pre emphasis ∗sig[: −1]]\n(5)\nAs shown in equation 5 emphasized signal is calculated by using a pre-emphasis filter applied\non the signal (sig) using the first-order filter. The number of frames is calculated by taking the\nceiling value of the division of the absolute value of the difference between signal length (sig len)\nand product of filter size (f size) and sample rate (sr) with the product of frame stride ( f stride)\nand sample rate (sr) as shown in equation 6. Signal length is the length of emphasized signal\ncalculated in equation 5.\nn frames = ⌈|sig len −(f size × sr)|\n(f stride × sr)\n⌉\n(6)\nUsing equation 7 pad signal is generated from concatenation of emphasized signal and zero\nvalue array of dimension (pad signal length −signal length)×1, where, pad signal length is\n8\nNatural Language Engineering\ncalculated by n frames × (f stride × sr) + (f size × sr).\npad signal = [emphasized signal, [0]((n frames×(f stride×sr)+(f size×sr))−sig len)×1]\n(7)\nFrames are calculated as shown in equation 8 from the pad signal elements where elements\nare the addition of an array of positive natural numbers from 0 to f size × sr repeated n frames\nand the transpose of the array of size of num frames where each element is the difference of\n(f stride × sr).\nframes = pad signal[({x ∈Z+ : 0 < x < (f size × sr)}n)(n frames,1)\nn=0\n+\n(({r : r = (f stride × sr) × (i −1), i ∈{0, . . . , n frames × (f stride × sr)}}n)((f size×sr),1)\nn=0\n)T]\n(8)\nPower frames shown in equation 9 are calculated as the square of the absolute value of the\nDiscrete Fourier Transform (DFT) of the product of hamming window and frames of each element\nwith NFFT.\npf =\n|DFT((frames × (0.54 −(∑\n(f size×sr)−1\nN=0\n0.46 × cos\n2πN\n(f size×sr)−1))), NFFT)|2\nNFFT\n(9)\nmel points = {r : r = l f +\nhf −l f\n(nfilt + 2) −1 × i, i ∈{l f, . . . , h f}}\n(10)\nMel points are the array where elements are calculated as shown in the equation 10, where i is the\nvalues belonging from lf to hf.\nbins = ⌊(NFFT + 1) × (700 × (10\nmel points\n2595\n−1))\nsample rate\n⌋\n(11)\nFrom equation 11, bins are calculated where the floor value of the elements are taken which is the\nproduct of hertz points and NFFT + 1 divided by the sample rate. Hertz points are calculated by\nmultiplying 700 by subtraction of 1 from 10 power of mel points\n2595\n.\nfbankm(k) =\n\n\n\n\n\n\n\n\n\n\n\n0\nk < bins(m −1)\nk−bins(m−1)\nbins(m)−bins(m−1)\nbins(m −1) ≤k ≤bins(m)\nbins(m+1)−k\nbins(m+1)−bins(m)\nbins(m) ≤k ≤bins(m + 1)\n0\nk > bins(m + 1)\n(12)\nBins calculated from equation 11 are used to calculate filter banks shown in equation 12. Each\nfilter in the filter bank is triangular, with a response of 1 at the central frequency and a linear drop\nto 0 till it meets the central frequencies of the two adjacent filters, where the response is 0.\nFinally, mfcc is calculated shown in equation 13 by decorrelating the filter bank coeffi-\ncients using Discrete Cosine Transform (DCT) to get a compressed representation of the filter\nbanks. Sinusoidal liftering is applied to the mfcc to de-emphasize higher mfccs which improves\nclassification in noisy signals.\nmfcc = DCT(20 log10(pf · fbankT)) ×\n\u0014\n1 + li fter\n2\nsin {π ⊙n : n ∈Z+, n ≤ncoe f}\nli fter\n\u0015\n(13)\nMFCCs features of shape (1000, 13) generated from equation 13 is provided as input to the\nneural network which expects the same dimension followed by convolution layers as mentioned\nin section 3. Raw speech signal cannot be provided input to the framework as it contains lots\nof noise data therefore extracting features from the speech signal and using it as input to the\nmodel will produce better performance than directly considering raw speech signal as input. Our\nMandal et al\n9\nmotivation to use MFCC features as the feature count is small enough to force us to learn the\ninformation of the sample. Parameters are related to the amplitude of frequencies and provide us\nwith frequency channels to analyze the speech specimen.\n4.2 Data\n4.2.1 Benchmark Data\nThe Indian language (IL) dataset was acquired from the Indian Institute of Technology, Madrasg.\nThe dataset includes 13 widely used Indian languages. Table 2 presents the statistics of this dataset\nwhich we used for our experiments.\nTable 2. : Statistics of the Indian Language (IN) Dataset\nLanguage\nLabel\nGender\nSamples\nTotal Samples\nAverage Duration\n(in seconds)\nAssamese\nas\nF\n8,713\n17,654\n5.587\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n8,941\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nBengali\nbn\nF\n3,253\n9,440\n5.743\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n6,187\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nBodo\nbd\nF\n571\n571\n25.219\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nGujarati\ngu\nF\n2,396\n5,684\n13.459\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n3,288\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nHindi\nhi\nF\n2,318\n4,636\n8.029\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n2,318\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nKannada\nkn\nF\n1,289\n2,578\n10.264\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n1,289\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nMalayalam\nml\nF\n5,650\n11,300\n5.699\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n5,650\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nManipuri\nmn\nF\n9,487\n17,917\n4.169\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n8,430\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nMarathi\nmr\nF\n2,448\n2,448\n7.059\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nOdia\nor\nF\n3,578\n7,151\n4.4\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n3,573\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nRajasthani\nrj\nF\n4,346\n9,125\n7.914\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n4,779\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nTamil\nta\nF\n3,243\n6,960\n10.516\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n3,717\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nTelugu\nte\nF\n4,043\n6,524\n15.395\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nM\n2,481\nghttps://www.iitm.ac.in/donlab/tts/database.php\n10\nNatural Language Engineering\n4.2.2 Experimental Data\nIn the past two decades, the development of LID methods has been largely fostered through NIST\nLanguage Evaluations (LREs). As a result, the most popular benchmark for evaluating new LID\nmodels and methods is the NIST LRE evaluation dataset (Sadjadi et al 2018). The NIST LREs\ndataset mostly contains narrow-band telephone speech. Datasets are typically distributed by the\nLinguistic Data Consortium (LDC) and cost thousands of dollars. For example, the standard Kaldi\n(Povey et al 2011) recipe for LRE072 relies on 18 LDC SLR datasets that cost $15000 (approx)\nto LDC non-members. This makes it difficult for new research groups to enter the academic field\nof LID. Furthermore, the NIST LRE evaluations focus mostly on telephone speech.\nTable 3. : Statistics of the EU Dataset\nLanguage\nLabel\nTotal Samples\nAverage Duration\n(in seconds)\nEnglish\nen\n43,269\n684.264\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nFrench\nfr\n67,689\n492.219\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nGerman\nde\n48,454\n1,152.916\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nSpanish\nes\n57,869\n798.169\nAs the NIST LRE dataset is not freely available we used the EU Dataset (Bartz et al 2017)\nwhich is open source. The (EU) dataset contains YouTube News data for 4 major European lan-\nguages – English (en), French (fr), German (de) and Spanish (es). Statistics of the dataset are\ngiven in Table 3.\n4.3 Environment\nWe implemented our framework using Tensorflow (Abadi et al 2016) backend. We split the Indian\nlanguage dataset into training, validation, and testing set, containing 80%, 10%, and 10% of the\ndata, respectively, for each language and gender.\nFor regularization, we apply dropout (Srivastava et al 2014) after the Max-Pooling layer and\nBi-Directional LSTM layer. We use the rate of 0.1. A l2 regularization with 10−6 weight is also\nadded to all the trainable weights in the network. We train the model with Adam (Kingma et al\n2014) optimizer with β1 = 0.9, β2 = 0.98, and ε = 10−9 and learning rate schedule (Vaswani et al\n2017), with 4k warm-up steps and peak learning rate of 0.05/\n√\nd where d is 128. A batch size of\n64 with “Sparse Categorical Crossentropy” as the loss function was used.\n4.4 Result on Indian Language Dataset\nThe proposed framework was assessed against Kulkarni et al (2022) using identical datasets. Both\nCRNN and CRNN with Attention exhibited superior performance compared to the results reported\nby Kulkarni et al (2022), as shown in Table 4. They used 6 Linear layers where units are 256, 256,\n128, 64, 32, and 13, respectively in the CNN framework, whereas the DNN framework uses 3\nLSTM layers having units 256, 256, and 128, respectively followed by dropout layer followed by\n3 Time Distributed layer followed by a Linear layer of 13 as units.\nWe evaluated system performance using the following evaluation metrics – Recall (TPR),\nPrecision (PPV), f1-score, and Accuracy. Since one of our major objectives was to measure the\naccessibility of the network to new languages, we introduced Data Balancing of training data for\neach class, as the number of samples available for each class may vary drastically. This is the\nMandal et al\n11\nTable 4. : Comparative evaluation results (in terms of Accuracy) of our model and the model of\nKulkarni et al (2022) on the Indian Language dataset\nAccuracy\nDNN (Kulkarni et al 2022)\n0.9834\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nRNN (Kulkarni et al 2022)\n0.9843\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nCNN\n0.983\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nCRNN\n0.987\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nCRNN with Attention\n0.987\ncase for the Indian Language Dataset as shown in Table 2 in which Kannada, Marathi and partic-\nularly Bodo have a limited amount of data compared to the rest of the languages. To alleviate this\ndata imbalance problem, we used class weight balancing as a dynamic method using scikit-learn\n(Pedregosa et al 2011).\nTable 5. : Experimental Results for Indian Languages\nLanguage\nCRNN with Attention\nCRNN\nCNN\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nas\n0.989\n0.998\n0.993\n0.987\n0.995\n0.989\n0.992\n0.987\n0.991\n0.988\n0.989\n0.983\nbn\n1\n0.9\n0.948\n1\n0.904\n0.949\n1\n0.888\n0.941\nbd\n0.966\n1\n0.983\n0.966\n1\n0.983\n0.966\n1\n0.983\ngu\n0.997\n0.997\n0.997\n0.951\n0.998\n0.974\n0.996\n0.991\n0.994\nhi\n0.987\n0.991\n0.989\n0.991\n0.991\n0.991\n0.991\n0.974\n0.983\nkn\n0.977\n0.996\n0.987\n0.996\n0.996\n0.996\n0.973\n0.992\n0.983\nml\n0.996\n0.988\n0.992\n0.997\n0.99\n0.994\n0.99\n0.993\n0.991\nmn\n0.987\n0.999\n0.993\n0.973\n0.999\n0.986\n0.972\n0.998\n0.985\nmr\n1\n1\n1\n1\n1\n1\n1\n0.996\n0.998\nor\n1\n1\n1\n1\n0.999\n0.999\n0.986\n0.999\n0.992\nrj\n0.999\n0.993\n0.996\n0.995\n1\n0.997\n0.986\n0.996\n0.991\nta\n0.929\n0.991\n0.959\n0.975\n0.997\n0.986\n0.946\n0.989\n0.967\nte\n0.979\n0.998\n0.989\n0.982\n1\n0.991\n0.975\n0.998\n0.986\nPPV, TPR, f1-score, and Accuracy scores are reported in Table 5 for the three frameworks\n- CNN, CRNN, and CRNN with Attention. From Table 5 it is clearly visible that both CRNN\nframework and CRNN with Attention provide competitive results of 0.987 accuracy. Table 6,\nTable 7, and Table 8 shows the confusion matrix for CNN, CRNN, and CRNN with Attention.\nTable 6. : Confusion matrix for CRNN with Attention framework\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n1762\n0\n0\n0\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0.989\n0.998\n0.993\nbn\n10\n850\n0\n1\n0\n0\n0\n18\n0\n0\n0\n53\n12\n1\n0.9\n0.948\nbd\n0\n0\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.966\n1\n0.983\ngu\n1\n0\n0\n566\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0.997\n0.997\n0.997\nhi\n0\n0\n0\n0\n460\n0\n4\n0\n0\n0\n0\n0\n0\n0.987\n0.991\n0.989\nkn\n0\n0\n1\n0\n0\n257\n0\n0\n0\n0\n0\n0\n0\n0.977\n0.996\n0.987\nml\n0\n0\n1\n0\n6\n5\n1116\n1\n0\n0\n0\n0\n1\n0.996\n0.988\n0.992\nmn\n0\n0\n0\n1\n0\n0\n0\n1790\n0\n0\n0\n0\n0\n0.987\n0.999\n0.993\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n245\n0\n0\n0\n0\n1\n1\n1\nor\n0\n0\n0\n0\n0\n0\n0\n0\n0\n716\n0\n0\n0\n1\n1\n1\nrj\n4\n0\n0\n0\n0\n1\n1\n0\n0\n0\n906\n0\n0\n0.999\n0.993\n0.996\nta\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n690\n0\n0.929\n0.991\n0.959\nte\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n652\n0.979\n0.998\n0.989\n12\nNatural Language Engineering\nTable 7. : Confusion matrix for CRNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n1746\n0\n0\n0\n0\n0\n0\n19\n0\n0\n0\n1\n0\n0.995\n0.989\n0.992\nbn\n8\n853\n0\n28\n0\n0\n0\n28\n0\n0\n0\n17\n10\n1\n0.904\n0.949\nbd\n0\n0\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.966\n1\n0.983\ngu\n0\n0\n0\n567\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0.951\n0.998\n0.974\nhi\n0\n0\n0\n0\n460\n0\n3\n0\n0\n0\n1\n0\n0\n0.991\n0.991\n0.991\nkn\n0\n0\n1\n0\n0\n257\n0\n0\n0\n0\n0\n0\n0\n0.996\n0.996\n0.996\nml\n0\n0\n1\n0\n4\n1\n1119\n1\n0\n0\n4\n0\n0\n0.997\n0.99\n0.994\nmn\n0\n0\n0\n1\n0\n0\n0\n1789\n0\n0\n0\n0\n1\n0.973\n0.999\n0.986\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n245\n0\n0\n0\n0\n1\n1\n1\nor\n0\n0\n0\n0\n0\n0\n0\n1\n0\n715\n0\n0\n0\n1\n0.999\n0.999\nrj\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n912\n0\n0\n0.995\n1\n0.997\nta\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n694\n0\n0.975\n0.997\n0.986\nte\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n653\n0.982\n1\n0.991\nTable 8. : Confusion matrix for CNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n1744\n0\n1\n0\n0\n3\n1\n13\n0\n0\n0\n4\n0\n0.991\n0.988\n0.989\nbn\n9\n838\n0\n1\n1\n0\n0\n35\n0\n8\n5\n34\n13\n1\n0.888\n0.941\nbd\n0\n0\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.966\n1\n0.983\ngu\n2\n0\n0\n563\n0\n0\n0\n0\n0\n1\n0\n0\n2\n0.996\n0.991\n0.994\nhi\n0\n0\n0\n0\n452\n0\n10\n0\n0\n0\n2\n0\n0\n0.991\n0.974\n0.983\nkn\n0\n0\n0\n0\n1\n256\n0\n0\n0\n0\n1\n0\n0\n0.973\n0.992\n0.983\nml\n0\n0\n1\n0\n2\n2\n1122\n0\n0\n0\n3\n0\n0\n0.99\n0.993\n0.991\nmn\n1\n0\n0\n0\n0\n0\n0\n1788\n0\n0\n0\n1\n1\n0.972\n0.998\n0.985\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n244\n1\n0\n0\n0\n1\n0.996\n0.998\nor\n0\n0\n0\n0\n0\n0\n0\n1\n0\n715\n0\n0\n0\n0.986\n0.999\n0.992\nrj\n1\n0\n0\n0\n0\n2\n1\n0\n0\n0\n908\n0\n0\n0.986\n0.996\n0.991\nta\n3\n0\n0\n1\n0\n0\n0\n1\n0\n0\n2\n688\n1\n0.946\n0.989\n0.967\nte\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n652\n0.975\n0.998\n0.986\nFrom Table 6, Table 7, and Table 8 it can be observed that Assamese gets confused with\nManipuri; Bengali gets confused with Assamese, Manipuri, Tamil, and Telugu; and Hindi gets\nconfused with Malayalam.\nAssamese and Bengali have originated from the same language family and they share approx-\nimately the same phoneme set. However, Bengali and Tamil are from different language families\nbut share a similar phoneme set. For example, in Bengali cigar is churut and star is nakshatra\nwhile cigar in Tamil is charuttu and star in Tamil is natsattira, which is quite similar. Similarly,\nManipuri and Assamese share similar phonemes. On close study, we observed that Hindi and\nMalayalam have also similar phoneme sets as both languages borrowed most of the vocabu-\nlary from Sanskrit. For example, ‘arrogant’ is Ahankar in Hindi and Ahankaram in Malayalam.\nSimilarly, Sathyu or commonly spoken as Satya in Hindi means ‘Truth’, which is Sathyam in\nMalayalam. Also, the word Sundar in Hindi is Sundaram in Malayalam, which means ‘beautiful’.\nTable 9 shows the most common classification errors encountered during evaluation.\nTable 9. : Most common errors\nAssamese\n→\nManipuri\nBengali\n→\nAssamese\nBengali\n→\nManipuri\nBengali\n→\nTamil\nHindi\n→\nMalayalam\nMandal et al\n13\n4.5 Result on same language families on Indian Language Dataset\nA deeper study into these 13 Indian languages led us to define five clusters of languages\nbased on their phonetic similarity. Cluster internal languages are phonetically similar, close, and\ngeographically contiguous, hence difficult to differentiate.\n• Cluster 1: Assamese, Bengali, Odia\n• Cluster 2: Gujarati, Hindi, Marathi, Rajasthani\n• Cluster 3: Kannada, Malayalam, Tamil, Telugu\n• Cluster 4: Bodo\n• Cluster 5: Manipuri\nBodo and Manipuri are phonetically very much distant from any of the rest of the languages,\nthus they form singleton clusters. We carried out separate experiments for the identification of\nthe cluster internal languages for Cluster 1, 2 and 3, and the experimental results are presented in\nTable 10.\nTable 10. : Experimental Results of LID for close languages.\nCluster\nLanguage\nCRNN\nwith\nAttention\nCRNN\nCNN\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\n1\nas\n0.962\n1\n0.981\n0.98\n0.953\n1\n0.976\n0.974\n0.953\n1\n0.976\n0.971\nbn\n1\n0.926\n0.961\n1\n0.907\n0.951\n1\n0.894\n0.944\nor\n1\n1\n1\n1\n1\n1\n0.982\n1\n0.991\n2\ngu\n1\n0.998\n0.999\n0.999\n1\n0.998\n0.999\n0.999\n1\n0.993\n0.996\n0.996\nhi\n1\n1\n1\n1\n0.998\n0.999\n0.991\n0.996\n0.994\nmr\n1\n1\n1\n1\n1\n1\n1\n0.996\n0.998\nrj\n0.999\n1\n0.999\n0.998\n1\n0.999\n0.995\n0.998\n0.996\n3\nkn\n1\n0.996\n0.998\n0.999\n1\n1\n1\n1\n0.992\n0.988\n0.99\n0.996\nml\n0.999\n1\n0.999\n1\n1\n1\n0.996\n0.996\n0.996\nta\n1\n1\n1\n1\n1\n1\n0.996\n0.997\n0.996\nte\n1\n1\n1\n1\n1\n1\n0.995\n0.997\n0.996\nIt can be clearly observed from Table 10 that both CRNN framework and CRNN with Attention\nprovide competitive results for every language cluster. For cluster-1 CRNN framework and\nCRNN with Attention provides an accuracy of 0.98/0.974, for cluster-2 0.999/0.999, and for\ncluster-3 0.999/1, respectively. CNN framework also provides comparable results to the other\ntwo frameworks.\nTable 11, Table 12 and Table 13 presents the confusion matrix for cluster 1, cluster 2, and\ncluster 3, respectively. From Table 11, we observed that Bengali gets confused with Assamese\nand Odia, which is quite expected since these two languages are spoken in neighbouring states\nand both of them share almost the same phonemes. For example, in Odia rice is pronounced\nas bhata whereas in Bengali pronounced as bhat, similarly fish in odia as machha whereas in\nBengali it is machh. Both CRNN and CRNN with Attention perform well to discriminate between\nBengali and Odia. It can be observed from Table 13 that CNN creates a lot of confusion when\ndiscriminating between these four languages. Both CRNN and CRNN with Attention prove to be\nbetter at discriminating among these languages. From the results in Table 10, 11, 12 and 13, it is\npretty clear that CRNN (Bi-Directional LSTM over CNN) and CRNN with Attention are more\neffective for Indian language identification and they perform almost at par. Another important\nobservation is that it is harder to classify the languages in cluster 1 than in the other two clusters.\n14\nNatural Language Engineering\nTable 11. : Confusion matrix for Cluster 1\nCRNN and Attention\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nor\nActual\nas\n1766\n0\n0\n0.962\n1\n0.981\nbn\n70\n874\n0\n1\n0.926\n0.961\nor\n0\n0\n716\n1\n1\n1\nCRNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nor\nActual\nas\n1766\n0\n0\n0.953\n1\n0.976\nbn\n88\n856\n0\n1\n0.907\n0.951\nor\n0\n0\n716\n1\n1\n1\nCNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nor\nActual\nas\n1766\n0\n0\n0.953\n1\n0.976\nbn\n87\n844\n13\n1\n0.894\n0.944\nor\n0\n0\n716\n0.982\n1\n0.991\nTable 12. : Confusion matrix for Cluster 2\nCRNN and Attention\nPredicted\nPPV\nTPR\nf1\nScore\ngu\nhi\nmr\nrj\nActual\ngu\n567\n0\n0\n1\n1\n0.998\n0.999\nhi\n0\n464\n0\n0\n1\n1\n1\nmr\n0\n0\n245\n0\n1\n1\n1\nrj\n0\n0\n0\n912\n0.999\n1\n0.999\nCRNN\nPredicted\nPPV\nTPR\nf1\nScore\ngu\nhi\nmr\nrj\nActual\ngu\n567\n0\n0\n1\n1\n0.998\n0.999\nhi\n0\n463\n0\n1\n1\n0.998\n0.999\nmr\n0\n0\n245\n0\n1\n1\n1\nrj\n0\n0\n0\n912\n0.998\n1\n0.999\nCNN\nPredicted\nPPV\nTPR\nf1\nScore\ngu\nhi\nmr\nrj\nActual\ngu\n564\n2\n0\n2\n1\n0.993\n0.996\nhi\n0\n462\n0\n2\n0.991\n0.996\n0.994\nmr\n0\n0\n244\n1\n1\n0.996\n0.998\nrj\n0\n2\n0\n910\n0.995\n0.998\n0.996\nTable 13. : Confusion matrix for Cluster 3\nCRNN and Attention\nPredicted\nPPV\nTPR\nf1\nScore\nkn\nml\nta\nte\nActual\nkn\n257\n1\n0\n0\n1\n0.996\n0.998\nml\n0\n1130\n0\n0\n0.999\n1\n0.999\nta\n0\n0\n696\n0\n1\n1\n1\nte\n0\n0\n0\n653\n1\n1\n1\nCRNN\nPredicted\nPPV\nTPR\nf1\nScore\nkn\nml\nta\nte\nActual\nkn\n258\n0\n0\n0\n1\n1\n1\nml\n0\n1130\n0\n0\n1\n1\n1\nta\n0\n0\n696\n0\n1\n1\n1\nte\n0\n0\n0\n653\n1\n1\n1\nCNN\nPredicted\nPPV\nTPR\nf1\nScore\nkn\nml\nta\nte\nActual\nkn\n255\n3\n0\n0\n0.992\n0.988\n0.99\nml\n1\n1125\n2\n2\n0.996\n0.996\n0.996\nta\n1\n0\n694\n1\n0.996\n0.997\n0.996\nte\n0\n1\n1\n651\n0.995\n0.997\n0.996\n4.6 Results on European Language\nWe evaluated our model in two environments – No Noise and White Noise. According to our\nintuition, in real-life scenarios during prediction of language chances of capturing background\nnoise of chatter and other sounds may happen. For the white noise evaluation setup, we mixed\nwhite noise into each test sample which has an audible solid presence but retains the identity of\nthe language.\nTable 14 compares the results of our models on the EU dataset with state-of-the-art models\npresented by Bartz et al (2017). The model proposed by Bartz et al (2017) consists of CRNN\nand uses Google’s Inception-v3 framework (Szegedy et al 2016). The feature extractor performs\nconvolutional operations on the input image through multiple stages, resulting in the production\nof a feature map that possesses a height of one. The feature map is partitioned horizontally along\nthe x-axis, and each partition is employed as a temporal unit for the subsequent Bidirectional\nLSTM network. The network employs a total of five convolutional layers, with each layer being\nsucceeded by the ReLU activation function, Batch Normalization, and 2 × 2 max pooling with\na stride of 2. The convolutional layers in question are characterized by their respective kernel\nsizes and the number of filters, which are as follows: (7 × 7, 16), (5 × 5, 32), (3 × 3, 64), (3 × 3,\nMandal et al\n15\nTable 14. : Comparative evaluation results (in terms of Accuracy) of our model and the model of\nBartz et al (2017) on the EU dataset\nNo Noise\nWhite Noise\nCRNN (Bartz et al 2017)\n0.91\n0.63\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nInception-v3 CRNN (Bartz et al 2017)\n0.96\n0.91\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nCNN\n0.948\n0.871\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nCRNN\n0.967\n0.912\n.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..\nCRNN with Attention\n0.966\n0.888\n128), and (3 × 3, 256). The Bidirectional LSTM model comprises a pair of individual LSTM\nmodels, each with 256 output units. The concatenation of the two outputs is transformed into a\n512-dimensional vector, which is then input into a fully-connected layer. The layer has either 4 or\n6 output units, which function as the classifier. They experimented in four different environments\n– No Noise, White Noise, Cracking Noise, and Background Noise. All our evaluation results are\nrounded to 3 digits after the decimal point.\nThe CNN model failed to achieve competitive results; it provided an accuracy of 0.948/0.871 in\nNo Noise/White Noise. In the CRNN framework, our model provides an accuracy of 0.967/0.912\non the No Noise/White Noise scenario outperforming the state-of-the-art results of Bartz et\nal (2017). Use of Attention improves over the Inception-v3 CRNN in the No Noise scenario,\nhowever, it does not perform well on White Noise.\n4.7 Ablation Studies\n4.7.1 Convolution Kernel Size\nTo study the effect of kernel sizes in the convolution layers, we sweep the kernel size with 3, 7, 17,\n32, and 65 of the models. We found that performance decreases with larger kernel sizes, as shown\nin Table 15. On comparing the accuracy up to the second decimal place kernel size 3 performs\nbetter than the rest.\nTable 15. : Ablation study on convolution kernel sizes\nKernel size\nAccuracy\n3\n98.7%\n7\n98.68%\n17\n98.65%\n32\n98.13%\n65\n93.56%\n4.7.2 Automatic Class Weight vs Manual Class Weight\nBalancing the data using class weights gives better accuracy for CRNN with Attention (98.7%)\nand CRNN (98.7%), compared to CNN (98.3%) shown in Table 5. We study the efficacy of the\nframeworks by manually balancing the datasets using 100 samples, 200 samples, and 571 samples\n16\nNatural Language Engineering\ndrawn randomly from the dataset and the results of these experiments are presented in Table 16,\nTable 17 and Table 18, respectively.\nTable 16. : Experimental Results for Manually Balancing the Samples for each category to 100.\nLanguage\nCRNN with Attention\nCRNN\nCNN\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nas\n0.766\n0.72\n0.742\n0.883\n0.839\n0.94\n0.887\n0.932\n0.617\n0.58\n0.598\n0.72\nbn\n0.875\n0.7\n0.778\n0.957\n0.9\n0.928\n0.816\n0.8\n0.808\nbd\n1\n1\n1\n0.962\n1\n0.98\n0.843\n0.86\n0.851\ngu\n0.943\n1\n0.971\n1\n0.98\n0.99\n0.731\n0.76\n0.745\nhi\n0.959\n0.94\n0.95\n0.957\n0.9\n0.928\n0.778\n0.7\n0.737\nkn\n0.961\n0.98\n0.97\n0.94\n0.94\n0.94\n0.725\n0.74\n0.733\nml\n0.958\n0.92\n0.939\n0.923\n0.96\n0.941\n0.774\n0.82\n0.796\nmn\n0.878\n0.72\n0.791\n0.935\n0.86\n0.896\n0.691\n0.76\n0.724\nmr\n0.906\n0.96\n0.932\n0.98\n0.96\n0.97\n0.857\n0.84\n0.848\nor\n0.959\n0.94\n0.949\n0.943\n1\n0.971\n0.811\n0.86\n0.835\nrj\n0.782\n0.86\n0.819\n0.894\n0.84\n0.866\n0.605\n0.52\n0.559\nta\n0.677\n0.88\n0.765\n0.898\n0.88\n0.889\n0.564\n0.62\n0.590\nte\n0.878\n0.86\n0.869\n0.906\n0.96\n0.932\n0.532\n0.5\n0.515\nTable 17. : Experimental Results for Manually Balancing the Samples for each Category to 200.\nLanguage\nCRNN with Attention\nCRNN\nCNN\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nas\n0.941\n0.96\n0.95\n0.975\n1\n0.94\n0.969\n0.971\n0.8\n0.88\n0.838\n0.883\nbn\n0.909\n1\n0.952\n1\n0.96\n0.98\n0.92\n0.92\n0.92\nbd\n0.98\n0.96\n0.97\n0.98\n0.98\n0.98\n0.94\n0.94\n0.94\ngu\n1\n1\n1\n1\n1\n1\n0.918\n0.9\n0.909\nhi\n1\n0.98\n0.99\n1\n0.98\n0.99\n0.956\n0.86\n0.905\nkn\n1\n0.98\n0.99\n1\n0.98\n0.99\n0.878\n0.86\n0.869\nml\n0.962\n1\n0.98\n0.893\n1\n0.943\n0.896\n0.86\n0.878\nmn\n0.979\n0.92\n0.948\n0.907\n0.98\n0.942\n0.754\n0.92\n0.829\nmr\n0.98\n0.98\n0.98\n0.98\n0.96\n0.97\n0.956\n0.86\n0.905\nor\n0.98\n1\n0.99\n1\n1\n1\n0.941\n0.96\n0.95\nrj\n0.96\n0.96\n0.96\n1\n0.96\n0.98\n0.86\n0.86\n0.86\nta\n1\n0.96\n0.98\n0.904\n0.94\n0.922\n0.784\n0.8\n0.792\nte\n1\n0.98\n0.99\n0.979\n0.94\n0.959\n0.935\n0.86\n0.896\nTable 18. : Experimental Results for Manually Balancing the Samples for each category to 571.\nLanguage\nCRNN with Attention\nCRNN\nCNN\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nPPV\nTPR\nf1\nScore\nAccuracy\nas\n1\n1\n1\n0.988\n0.983\n0.983\n0.983\n0.985\n0.967\n1\n0.983\n0.945\nbn\n1\n1\n1\n1\n1\n1\n0.983\n1\n0.991\nbd\n1\n1\n1\n1\n1\n1\n1\n1\n1\ngu\n1\n1\n1\n0.983\n1\n0.991\n0.982\n0.931\n0.956\nhi\n0.983\n0.983\n0.983\n1\n1\n1\n0.893\n0.862\n0.877\nkn\n1\n1\n1\n1\n1\n1\n0.903\n0.966\n0.933\nml\n1\n0.966\n0.982\n0.983\n1\n0.991\n0.914\n0.914\n0.914\nmn\n0.983\n1\n0.991\n1\n0.983\n0.991\n0.931\n0.931\n0.931\nmr\n1\n1\n1\n0.982\n1\n0.991\n0.965\n0.982\n0.973\nor\n1\n1\n1\n1\n0.983\n0.991\n1\n0.966\n0.982\nrj\n0.919\n0.983\n0.95\n0.918\n0.966\n0.941\n0.9\n0.931\n0.915\nta\n0.964\n0.931\n0.947\n0.964\n0.914\n0.938\n0.879\n0.879\n0.879\nte\n1\n0.983\n0.991\n1\n0.983\n0.991\n0.982\n0.931\n0.956\nMandal et al\n17\nThe objective of the study was to observe the performance of the frameworks in increasing\nthe sample size. Since the Bodo language has the minimum data (571 samples) among all the\nlanguages in the dataset, we performed our experiments on 571 samples.\nA comparison of the results in Table 16, Table 17, and Table 18 reveals the following\nobservations.\n• All the models perform consistently better with more training data.\n• CRNN and CRNN with attention perform consistently better than CNN.\n• CRNN is less data hungry among the 3 models and it performs the best in the lowest data\nscenario.\nFigure 3: Comparison of model results for varying dataset size.\nFigure 3 graphically shows the performance improvement over increasing data samples. The\nconfusion matrices for the three frameworks for the 3 datasets are presented in Table A.1, A.2,\nA.3, B.1, B.2, B.3, C.1, C.2, C.3 in the Appendix.\n4.7.3 Additional performance and parameter size analysis of our frameworks\nTable 19. : A comprehensive performance analysis of our various proposed frameworks.\nFramework\nCNN\nCRNN\nCRNN\nwith\nAttention\nParameters\n1,355,917\n2,094,477\n2,357,645\nIndian Dataset\n0.983\n0.987\n0.987\nClose Language Cluster\nCluster 1\n0.971\n0.974\n0.980\nCluster 2\n0.996\n0.999\n0.999\nCluster 3\n0.996\n1\n0.999\nEuropean Language Dataset\nNo Noise\n0.948\n0.967\n0.966\nWhite Noise\n0.871\n0.912\n0.888\nTable 19 demonstrates that both CRNN and CRNN with attention perform better compared to\nthe CNN-based framework. At the same time, CRNN itself produces better or equivalent perfor-\nmance compared to CRNN with an Attention-based mechanism. CRNN with Attention performs\n18\nNatural Language Engineering\nbetter only for Cluster 1 of the Indian dataset; CRNN itself produces the best results in all other\ntasks, sometimes jointly with CRNN with Attention. This is despite the fact that the Attention-\nbased framework has more parameters than the other models. The underlying intuition is that\nthe attention-based framework generally suffers from overfitting problems due to its additional\nparameter count. An attention-based framework needs to learn how to assign importance to dif-\nferent parts of the input sequence, which may require a large number of training instances to\nproduce a generalized performance. Thus, CRNN with Attention makes the experimental set-up\ntime-consuming and resource-intensive, but still, it is not able to improve over CRNN.\n5. Conclusion and future work\nIn this work, we proposed a language identification method using CRNN that works on MFCC fea-\ntures of speech signals. Our framework efficiently identifies the language both in close language\nand noisy scenarios. We carried out extensive experiments and our framework produced state-\nof-the-art results. Through our experiments, we have also shown our framework’s robustness to\nnoise and its extensibility to new languages. The model exhibits the overall best accuracy of 98.7%\nwhich improves over the traditional use of CNN (98.3%). CRNN with attention performs almost\nat par with CRNN, however, the attention mechanism which incurs additional computational\noverhead does not result in improvement over CRNN in most cases.\nIn future, we would like to extend our work by increasing the language classes with speech\nspecimens recorded in different environments. We would also like to extend our work to check\nthe usefulness of the proposed framework on smaller time speech samples through which we can\ndeduce the optimal time required to classify the languages with high accuracy. We would also like\nto test our method on language dialect identification.\nAcknowledgements\nThis research was supported by the TPU Research Cloud (TRC) program, a Google Research\ninitiative and funded by Rashtriya Uchchatar Shiksha Abhiyan 2.0 [grant number R-11/828/19].\nReferences\nB. Aarti and S. K. Kopparapu, ”Spoken Indian language classification using artificial neural network — An experimental\nstudy,” 2017 4th International Conference on Signal Processing and Integrated Networks (SPIN), 2017, pp. 424-430, doi:\n10.1109/SPIN.2017.8049987.\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,\nGeoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit\nSteiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow:\na system for large-scale machine learning. In Proceedings of the 12th USENIX conference on Operating Systems Design\nand Implementation (OSDI’16). USENIX Association, USA, 265–283.\nBartz, C., Herold, T., Yang, H., Meinel, C. (2017). Language Identification Using Deep Convolutional Recurrent Neural\nNetworks. In: Liu, D., Xie, S., Li, Y., Zhao, D., El-Alfy, ES. (eds) Neural Information Processing. ICONIP 2017. Lecture\nNotes in Computer Science(), vol 10639. Springer, Cham. https://doi.org/10.1007/978-3-319-70136-3 93\nChen, Pinzhen & Heafield, Kenneth. (2020). Approaching Neural Chinese Word Segmentation as a Low-Resource Machine\nTranslation Task.\nN. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel and P. Ouellet, ”Front-End Factor Analysis for Speaker Verification,”\nin IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788-798, May 2011, doi:\n10.1109/TASL.2010.2064307.\nDehak, N., Torres-Carrasquillo, P.A., Reynolds, D., Dehak, R. (2011) Language recognition via i-vectors and dimensionality\nreduction. Proc. Interspeech 2011, 857-860, doi: 10.21437/Interspeech.2011-328\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages\nMandal et al\n19\n4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\nAlexandra Draghici, Jakob Abeßer, and Hanna Lukashevich. 2020. A study on spoken language identification using deep\nneural networks. In Proceedings of the 15th International Audio Mostly Conference (AM ’20). Association for Computing\nMachinery, New York, NY, USA, 253–256. https://doi.org/10.1145/3411109.3411123\nL. Ferrer, N. Scheffer and E. Shriberg, ”A comparison of approaches for modeling prosodic features in speaker recog-\nnition,” 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, 2010, pp. 4414-4417, doi:\n10.1109/ICASSP.2010.5495632.\nGanapathy, S., Han, K., Thomas, S., Omar, M., Segbroeck, M.V., Narayanan, S.S. (2014) Robust language identification\nusing convolutional neural network features. Proc. Interspeech 2014, 1846-1850, doi: 10.21437/Interspeech.2014-419\nValentin Gazeau, Cihan Varol, ”Automatic Spoken Language Recognition with Neural Networks”, International Journal of\nInformation Technology and Computer Science(IJITCS), Vol.10, No.8, pp.11-17, 2018. DOI: 10.5815/ijitcs.2018.08.02\nGelly, G., Gauvain, J.-L., Le, V.B., Messaoudi, A. (2016) A Divide-and-Conquer Approach for Language Identification Based\non Recurrent Neural Networks. Proc. Interspeech 2016, 3231-3235, doi: 10.21437/Interspeech.2016-180\nJiatao Gu, Changhan Wang, and Jake Zhao Junbo. 2019. Levenshtein transformer. Proceedings of the 33rd International\nConference on Neural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 1003,\n11181–11191.\nK. He, X. Zhang, S. Ren and J. Sun, ”Deep Residual Learning for Image Recognition,” 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.\nP. Heracleous, K. Takai, K. Yasuda, Y. Mohammad and A. Yoneyama, ”Comparative Study on Spoken Language Identification\nBased on Deep Learning,” 2018 26th European Signal Processing Conference (EUSIPCO), 2018, pp. 2265-2269, doi:\n10.23919/EUSIPCO.2018.8553347.\nHoward,\nJ.;\nGugger,\nS.\nFastai:\nA\nLayered\nAPI\nfor\nDeep\nLearning.\nInformation\n2020,\n11,\n108.\nhttps://doi.org/10.3390/info11020108\nJoshi, M., Chen, D., Liu, Y., Weld, D., Zettlemoyer, L., & Levy, O. (2020). SpanBERT: Improving Pre-training by\nRepresenting and Predicting Spans. Transactions of the Association for Computational Linguistics, 8, 64-77. Retrieved\nfrom https://transacl.org/ojs/index.php/tacl/article/view/1853\nKingma, Diederik P., and Jimmy Ba. ”Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980 (2014).\nKulkarni, R., Joshi, A., Kamble, M., Apte, S. (2022). Spoken Language Identification for Native Indian Languages Using\nDeep Learning Techniques. In: Chen, J.IZ., Wang, H., Du, KL., Suma, V. (eds) Machine Learning and Autonomous\nSystems. Smart Innovation, Systems and Technologies, vol 269. Springer, Singapore. https://doi.org/10.1007/978-981-\n16-7996-4 7\nZhenzhong, Lan., Mingda, Chen., Sebastian, Goodman., Kevin, Gimpel., Piyush, Sharma., Radu, Soricut. ALBERT: A Lite\nBERT for Self-supervised Learning of Language Representations. (2020).\nI. Lopez-Moreno, J. Gonzalez-Dominguez, O. Plchot, D. Martinez, J. Gonzalez-Rodriguez and P. Moreno, ”Automatic lan-\nguage identification using deep neural networks,” 2014 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2014, pp. 5337-5341, doi: 10.1109/ICASSP.2014.6854622.\nLozano-Diez, A., Zazo-Candil, R., Gonzalez-Dominguez, J., Toledano, D.T., Gonzalez-Rodriguez, J. (2015) An end-to-end\napproach to language identification in short utterances using convolutional neural networks. Proc. Interspeech 2015, 403-\n407, doi: 10.21437/Interspeech.2015-164\nMart´ınez, D., Plchot, O., Burget, L., Glembek, O., Matˇejka, P. (2011) Language recognition in ivectors space. Proc.\nInterspeech 2011, 861-864, doi: 10.21437/Interspeech.2011-329\nS. Mukherjee, N. Shivam, A. Gangwal, L. Khaitan and A. J. Das, ”Spoken Language Recognition Using CNN,” 2019\nInternational Conference on Information Technology (ICIT), 2019, pp. 37-41, doi: 10.1109/ICIT48102.2019.00013.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of\nthe 27th International Conference on International Conference on Machine Learning (ICML’10). Omnipress, Madison,\nWI, USA, 807–814.\nFabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel,\nPeter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu\nBrucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res.\n12, null (2/1/2011), 2825–2830.\nPlchot, O., Matejka, P., Glembek, O., Fer, R., Novotny, O., Pesan, J., Burget, L., Brummer, N., Cumani, S. (2016) BAT System\nDescription for NIST LRE 2015. Proc. The Speaker and Language Recognition Workshop (Odyssey 2016), 166-173, doi:\n10.21437/Odyssey.2016-24\nPovey, Daniel & Ghoshal, Arnab & Boulianne, Gilles & Burget, Luk´aˇs & Glembek, Ondrej & Goel, Nagendra & Hannemann,\nMirko & Motl´ıˇcek, Petr & Qian, Yanmin & Schwarz, Petr & Silovsk´y, Jan & Stemmer, Georg & Vesel, Karel. (2011). The\nKaldi speech recognition toolkit. IEEE 2011 Workshop on Automatic Speech Recognition and Understanding.\nRevay, S., & Teschke, M. (2019). Multiclass Language Identification using Deep Learning on Spectral Images of Audio\nSignals. ArXiv, abs/1905.04348.\n20\nNatural Language Engineering\nSadjadi, S.O., Kheyrkhah, T., Greenberg, C., Singer, E., Reynolds, D., Mason, L., Hernandez-Cordero, J. (2018)\nPerformance Analysis of the 2017 NIST Language Recognition Evaluation. Proc. Interspeech 2018, 1798-1802, doi:\n10.21437/Interspeech.2018-69\nD. S. Sisodia, S. Nikhil, G. S. Kiran and P. Sathvik, ”Ensemble Learners for Identification of Spoken Languages using Mel\nFrequency Cepstral Coefficients,” 2nd International Conference on Data, Engineering and Applications (IDEA), 2020, pp.\n1-5, doi: 10.1109/IDEA49133.2020.9170720.\nSmith, L.N. (2018). A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum,\nand weight decay. ArXiv, abs/1803.09820.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way\nto prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1 (January 2014), 1929–1958.\nC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna, ”Rethinking the Inception Architecture for Computer\nVision,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2818-2826, doi:\n10.1109/CVPR.2016.308.\nTakase, S., & Kiyono, S. (2021). Lessons on Parameter Sharing across Layers in Transformers. ArXiv, abs/2104.06022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems (NIPS’17). Curran Associates Inc., Red Hook, NY, USA, 6000–6010.\nH. Venkatesan, T. V. Venkatasubramanian and J. Sangeetha, ”Automatic Language Identification using Machine learning\nTechniques,” 2018 3rd International Conference on Communication and Electronics Systems (ICCES), 2018, pp. 583-588,\ndoi: 10.1109/CESYS.2018.8724070.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. LUKE: Deep Contextualized Entity\nRepresentations with Entity-aware Self-attention. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6442–6454, Online. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: general-\nized autoregressive pretraining for language understanding. Proceedings of the 33rd International Conference on Neural\nInformation Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 517, 5753–5763.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical Attention Networks\nfor Document Classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 1480–1489, San Diego, California. Association for\nComputational Linguistics.\nZazo R, Lozano-Diez A, Gonzalez-Dominguez J, T. Toledano D, Gonzalez-Rodriguez J (2016) Language Identification in\nShort Utterances Using Long Short-Term Memory (LSTM) Recurrent Neural Networks. PLOS ONE 11(1): e0146917.\nhttps://doi.org/10.1371/journal.pone.0146917\nM. A. Zissman, ”Comparison of four approaches to automatic language identification of telephone speech,” in IEEE\nTransactions on Speech and Audio Processing, vol. 4, no. 1, pp. 31-, Jan. 1996, doi: 10.1109/TSA.1996.481450.\nMandal et al\n21\nAppendix A. CNN framework\nTable A.1. : Confusion matrix of Manually Balancing the Samples for each category to 100 with\nCNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n29\n2\n0\n1\n0\n3\n0\n7\n0\n0\n3\n2\n3\n0.617\n0.58\n0.598\nbn\n2\n40\n0\n0\n0\n1\n1\n1\n1\n0\n0\n3\n1\n0.816\n0.8\n0.808\nbd\n0\n0\n43\n1\n0\n2\n0\n0\n2\n0\n1\n1\n0\n0.843\n0.86\n0.851\ngu\n1\n0\n0\n38\n4\n0\n0\n0\n0\n0\n0\n3\n4\n0.731\n0.76\n0.745\nhi\n2\n0\n0\n2\n35\n3\n3\n0\n0\n2\n0\n2\n1\n0.778\n0.7\n0.737\nkn\n0\n0\n1\n0\n1\n37\n6\n1\n0\n0\n1\n0\n3\n0.725\n0.74\n0.733\nml\n0\n1\n0\n1\n0\n3\n41\n0\n0\n0\n2\n1\n1\n0.774\n0.82\n0.796\nmn\n2\n2\n0\n0\n0\n0\n1\n38\n0\n1\n0\n1\n5\n0.691\n0.76\n0.724\nmr\n0\n0\n1\n0\n0\n0\n0\n0\n42\n3\n4\n0\n0\n0.857\n0.84\n0.848\nor\n0\n1\n1\n0\n0\n0\n0\n0\n3\n43\n1\n1\n0\n0.811\n0.86\n0.835\nrj\n2\n2\n3\n1\n3\n0\n1\n2\n1\n2\n26\n7\n0\n0.605\n0.52\n0.559\nta\n5\n0\n0\n3\n2\n0\n0\n3\n0\n0\n2\n31\n4\n0.564\n0.62\n0.590\nte\n4\n1\n2\n5\n0\n2\n0\n3\n0\n2\n3\n3\n25\n0.532\n0.5\n0.515\nTable A.2. : Confusion matrix of Manually Balancing the Samples for each category to 200 with\nCNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n44\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1\n3\n0\n0.8\n0.88\n0.838\nbn\n1\n46\n0\n0\n0\n0\n0\n1\n0\n0\n0\n2\n0\n0.92\n0.92\n0.92\nbd\n0\n0\n47\n0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0.94\n0.94\n0.94\ngu\n1\n1\n0\n45\n1\n0\n0\n0\n0\n0\n0\n0\n2\n0.918\n0.9\n0.909\nhi\n0\n1\n0\n1\n43\n1\n1\n2\n0\n0\n1\n0\n0\n0.956\n0.86\n0.905\nkn\n0\n0\n1\n1\n0\n43\n2\n1\n0\n0\n0\n1\n1\n0.878\n0.86\n0.869\nml\n1\n0\n0\n0\n0\n1\n43\n2\n1\n2\n0\n0\n0\n0.896\n0.86\n0.878\nmn\n2\n0\n0\n0\n0\n0\n1\n46\n0\n0\n0\n1\n0\n0.754\n0.92\n0.829\nmr\n0\n0\n2\n0\n0\n0\n1\n1\n43\n1\n2\n0\n0\n0.956\n0.86\n0.905\nor\n0\n0\n0\n0\n1\n0\n0\n0\n0\n48\n1\n0\n0\n0.941\n0.96\n0.95\nrj\n2\n1\n0\n0\n0\n1\n0\n1\n1\n0\n43\n1\n0\n0.86\n0.86\n0.86\nta\n2\n0\n0\n1\n0\n2\n0\n4\n0\n0\n1\n40\n0\n0.784\n0.8\n0.792\nte\n2\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n3\n43\n0.935\n0.86\n0.896\nTable A.3. : Confusion matrix of Manually Balancing the Samples for each category to 571 with\nCNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.967\n1\n0.983\nbn\n0\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.983\n1\n0.991\nbd\n0\n0\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\ngu\n0\n0\n0\n54\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0.982\n0.931\n0.956\nhi\n0\n0\n0\n0\n50\n0\n2\n1\n0\n0\n0\n5\n0\n0.893\n0.862\n0.877\nkn\n0\n0\n0\n0\n0\n56\n2\n0\n0\n0\n0\n0\n0\n0.903\n0.966\n0.933\nml\n0\n0\n0\n0\n0\n4\n53\n0\n0\n0\n1\n0\n0\n0.914\n0.914\n0.914\nmn\n1\n0\n0\n0\n0\n1\n0\n54\n0\n0\n0\n1\n1\n0.931\n0.931\n0.931\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n55\n0\n1\n0\n0\n0.965\n0.982\n0.973\nor\n0\n0\n0\n0\n1\n0\n0\n0\n1\n56\n0\n0\n0\n1\n0.966\n0.982\nrj\n1\n1\n0\n0\n0\n0\n1\n1\n0\n0\n54\n0\n0\n0.9\n0.931\n0.915\nta\n0\n0\n0\n1\n1\n0\n0\n1\n1\n0\n3\n51\n0\n0.879\n0.879\n0.879\nte\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n54\n0.982\n0.931\n0.956\n22\nNatural Language Engineering\nAppendix B. CRNN framework\nTable B.1. : Confusion matrix of Manually Balancing the Samples for each category to 100 with\nCRNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n47\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0.839\n0.94\n0.887\nbn\n0\n45\n0\n0\n0\n1\n0\n0\n0\n0\n0\n3\n1\n0.957\n0.9\n0.928\nbd\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.962\n1\n0.98\ngu\n0\n0\n0\n49\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0.98\n0.99\nhi\n0\n0\n0\n0\n45\n2\n2\n0\n0\n0\n0\n0\n1\n0.957\n0.9\n0.928\nkn\n1\n0\n0\n0\n0\n47\n2\n0\n0\n0\n0\n0\n0\n0.94\n0.94\n0.94\nml\n0\n0\n0\n0\n1\n0\n48\n0\n0\n0\n1\n0\n0\n0.923\n0.96\n0.941\nmn\n1\n2\n0\n0\n0\n0\n0\n43\n0\n1\n1\n0\n2\n0.935\n0.86\n0.896\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n48\n0\n2\n0\n0\n0.98\n0.96\n0.97\nor\n0\n0\n0\n0\n0\n0\n0\n0\n0\n50\n0\n0\n0\n0.943\n1\n0.971\nrj\n4\n0\n1\n0\n0\n0\n0\n0\n1\n1\n42\n1\n0\n0.894\n0.84\n0.866\nta\n2\n0\n0\n0\n1\n0\n0\n2\n0\n1\n0\n44\n0\n0.898\n0.88\n0.889\nte\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n48\n0.906\n0.96\n0.932\nTable B.2. : Confusion matrix of Manually Balancing the Samples for each category to 200 with\nCRNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n47\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n1\n0\n1\n0.94\n0.969\nbn\n0\n48\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n1\n0.96\n0.98\nbd\n0\n0\n49\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0.98\n0.98\n0.98\ngu\n0\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nhi\n0\n0\n0\n0\n49\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0.98\n0.99\nkn\n0\n0\n0\n0\n0\n49\n1\n0\n0\n0\n0\n0\n0\n1\n0.98\n0.99\nml\n0\n0\n0\n0\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0.893\n1\n0.943\nmn\n0\n0\n0\n0\n0\n0\n1\n49\n0\n0\n0\n0\n0\n0.907\n0.98\n0.942\nmr\n0\n0\n1\n0\n0\n0\n0\n1\n48\n0\n0\n0\n0\n0.98\n0.96\n0.97\nor\n0\n0\n0\n0\n0\n0\n0\n0\n0\n50\n0\n0\n0\n1\n1\n1\nrj\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n48\n1\n0\n1\n0.96\n0.98\nta\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n47\n1\n0.904\n0.94\n0.922\nte\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n1\n47\n0.979\n0.94\n0.959\nTable B.3. : Confusion matrix of Manually Balancing the Samples for each category to 571 with\nCRNN\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n57\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0.983\n0.983\n0.983\nbn\n0\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nbd\n0\n0\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\ngu\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.983\n1\n0.991\nhi\n0\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nkn\n0\n0\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nml\n0\n0\n0\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0\n0.983\n1\n0.991\nmn\n0\n0\n0\n0\n0\n0\n0\n57\n0\n0\n1\n0\n0\n1\n0.983\n0.991\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n56\n0\n0\n0\n0\n0.982\n1\n0.991\nor\n0\n0\n0\n0\n0\n0\n0\n0\n1\n57\n0\n0\n0\n1\n0.983\n0.991\nrj\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n56\n0\n0\n0.918\n0.966\n0.941\nta\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n4\n53\n0\n0.964\n0.914\n0.938\nte\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n57\n1\n0.983\n0.991\nMandal et al\n23\nAppendix C. CRNN with Attention framework\nTable C.1. : Confusion matrix of Manually Balancing the Samples for each category to 100 with\nCRNN and Attention\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n36\n0\n0\n0\n0\n0\n0\n1\n0\n0\n6\n6\n1\n0.766\n0.72\n0.742\nbn\n1\n35\n0\n0\n0\n0\n0\n1\n0\n0\n0\n13\n0\n0.875\n0.7\n0.778\nbd\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\ngu\n0\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.943\n1\n0.971\nhi\n0\n0\n0\n0\n47\n1\n1\n0\n0\n0\n0\n0\n1\n0.959\n0.94\n0.95\nkn\n0\n0\n0\n0\n0\n49\n0\n0\n0\n0\n0\n1\n0\n0.961\n0.98\n0.97\nml\n0\n0\n0\n0\n1\n1\n46\n0\n0\n0\n2\n0\n0\n0.958\n0.92\n0.939\nmn\n5\n3\n0\n1\n0\n0\n0\n36\n0\n1\n1\n0\n3\n0.878\n0.72\n0.791\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n48\n0\n2\n0\n0\n0.906\n0.96\n0.932\nor\n0\n0\n0\n0\n0\n0\n0\n0\n3\n47\n0\n0\n0\n0.959\n0.94\n0.949\nrj\n2\n0\n0\n0\n0\n0\n1\n0\n2\n1\n43\n1\n0\n0.782\n0.86\n0.819\nta\n2\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n44\n1\n0.677\n0.88\n0.765\nte\n1\n1\n0\n2\n0\n0\n0\n3\n0\n0\n0\n0\n43\n0.878\n0.86\n0.869\nTable C.2. : Confusion matrix of Manually Balancing the Samples for each category to 200 with\nCRNN and Attention\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n48\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.941\n0.96\n0.95\nbn\n0\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.909\n1\n0.952\nbd\n0\n0\n48\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0.98\n0.96\n0.97\ngu\n0\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nhi\n0\n0\n0\n0\n49\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0.98\n0.99\nkn\n0\n0\n0\n0\n0\n49\n1\n0\n0\n0\n0\n0\n0\n1\n0.98\n0.99\nml\n0\n0\n0\n0\n0\n0\n50\n0\n0\n0\n0\n0\n0\n0.962\n1\n0.98\nmn\n3\n0\n0\n0\n0\n0\n0\n46\n0\n1\n0\n0\n0\n0.979\n0.92\n0.948\nmr\n0\n0\n1\n0\n0\n0\n0\n0\n49\n0\n0\n0\n0\n0.98\n0.98\n0.98\nor\n0\n0\n0\n0\n0\n0\n0\n0\n0\n50\n0\n0\n0\n0.980\n1\n0.99\nrj\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n48\n0\n0\n0.96\n0.96\n0.96\nta\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n48\n0\n1\n0.96\n0.98\nte\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n49\n1\n0.98\n0.99\nTable C.3. : Confusion matrix of Manually Balancing the Samples for each category to 571 with\nCRNN and Attention\nPredicted\nPPV\nTPR\nf1\nScore\nas\nbn\nbd\ngu\nhi\nkn\nml\nmn\nmr\nor\nrj\nta\nte\nActual\nas\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nbn\n0\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nbd\n0\n0\n56\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\ngu\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nhi\n0\n0\n0\n0\n57\n0\n0\n0\n0\n0\n0\n1\n0\n0.983\n0.983\n0.983\nkn\n0\n0\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\nml\n0\n0\n0\n0\n1\n0\n56\n0\n0\n0\n1\n0\n0\n1\n0.966\n0.982\nmn\n0\n0\n0\n0\n0\n0\n0\n58\n0\n0\n0\n0\n0\n0.983\n1\n0.991\nmr\n0\n0\n0\n0\n0\n0\n0\n0\n56\n0\n0\n0\n0\n1\n1\n1\nor\n0\n0\n0\n0\n0\n0\n0\n0\n0\n58\n0\n0\n0\n1\n1\n1\nrj\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n57\n0\n0\n0.919\n0.983\n0.95\nta\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n54\n0\n0.964\n0.931\n0.947\nte\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n57\n1\n0.983\n0.991\n",
  "categories": [
    "cs.LG",
    "cs.CL",
    "cs.SD",
    "eess.AS",
    "eess.SP"
  ],
  "published": "2021-10-05",
  "updated": "2023-10-25"
}