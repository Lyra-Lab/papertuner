{
  "id": "http://arxiv.org/abs/2110.04442v2",
  "title": "A Primer on Deep Learning for Causal Inference",
  "authors": [
    "Bernard Koch",
    "Tim Sainburg",
    "Pablo Geraldo",
    "Song Jiang",
    "Yizhou Sun",
    "Jacob Gates Foster"
  ],
  "abstract": "This review systematizes the emerging literature for causal inference using\ndeep neural networks under the potential outcomes framework. It provides an\nintuitive introduction on how deep learning can be used to estimate/predict\nheterogeneous treatment effects and extend causal inference to settings where\nconfounding is non-linear, time varying, or encoded in text, networks, and\nimages. To maximize accessibility, we also introduce prerequisite concepts from\ncausal inference and deep learning. The survey differs from other treatments of\ndeep learning and causal inference in its sharp focus on observational causal\nestimation, its extended exposition of key algorithms, and its detailed\ntutorials for implementing, training, and selecting among deep estimators in\nTensorflow 2 available at github.com/kochbj/Deep-Learning-for-Causal-Inference.",
  "text": "A Primer on Deep Learning for Causal Inference\nBernard J. Koch∗1,2,3, Tim Sainburg4, Pablo Geraldo Bast´ıas1, Song Jiang5, Yizhou\nSun5 and Jacob Foster1,6\n1UCLA Department of Sociology\n2Northwestern Kellogg School of Management\n3University of Chicago Department of Sociology\n4Harvard Medical School\n5UCLA Department of Computer Science\n6Santa Fe Institute\nApril 2023\nAbstract\nThis primer systematizes the emerging literature on causal inference using deep neural net-\nworks under the potential outcomes framework. It provides an intuitive introduction on building\nand optimizing custom deep learning models and shows how to adapt them to estimate/predict\nheterogeneous treatment effects. It also discusses ongoing work to extend causal inference to set-\ntings where confounding is non-linear, time-varying, or encoded in text, networks, and images.\nTo maximize accessibility, we also introduce prerequisite concepts from causal inference and deep\nlearning. The primer differs from other treatments of deep learning and causal inference in its\nsharp focus on observational causal estimation, its extended exposition of key algorithms, and its\ndetailed tutorials for implementing, training, and selecting among deep estimators in Tensorflow\n2 and PyTorch.\n∗bernardkoch@ucla.edu\n1\narXiv:2110.04442v2  [cs.LG]  28 Nov 2023\nContents\n1\nIntroduction\n4\n2\nDeep Learning Fundamentals\n6\n2.1\nArtificial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nDeep Learning in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.1\nSet Up and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.2\nTraining and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.2.3\nModel Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.3\nRepresentation Learning and Multitask Learning . . . . . . . . . . . . . . . . . . . . .\n14\n3\nCausal Identification and Estimation Strategies\n15\n3.1\nIdentification of Causal Effects\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.2\nEstimation of Causal Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.2.1\nOutcome Modeling: Regression . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.2.2\nTreatment Modeling: Non-Parametric Matching\n. . . . . . . . . . . . . . . . .\n23\n3.2.3\nTreatment Modeling: Inverse Propensity Score Weighting . . . . . . . . . . . .\n23\n3.2.4\nDouble Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4\nThree Different Approaches to Deep Causal Estimation\n25\n4.1\nDeep Outcome Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.2\nBalancing through Representation Learning . . . . . . . . . . . . . . . . . . . . . . . .\n28\n4.3\nDouble Robustness with Inverse Propensity Score Weighting . . . . . . . . . . . . . . .\n30\n4.3.1\nSemi-parametric Theory of Causal Inference . . . . . . . . . . . . . . . . . . . .\n31\n4.3.2\nFrom TMLE to Targeted Regularization . . . . . . . . . . . . . . . . . . . . . .\n33\n5\nConfidence and Interpretation\n35\n5.1\nAssessing Confidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n5.2\nInterpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.3\nWhat’s in the tutorials? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n6\nBeyond Traditional Data: Text, Networks, Images, and Treatment over Time\n39\n6.1\nCausal Inference from Text\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n6.2\nCausal Inference from Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n6.3\nCausal Inference from Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n6.4\nCausal Inference from Time-varying Data . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n7\nConclusion: Deep Causal Estimation in Context\n44\n8\nAuthor’s Note\n46\n2\nA Balancing Using Integral Probability Metrics\n58\nA.1 Wasserstein Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\nA.2 Extending Representation Balancing with IPMs . . . . . . . . . . . . . . . . . . . . . .\n59\nA.2.1\nExtending Representation Balancing with Matching\n. . . . . . . . . . . . . . .\n62\nB Model Selection Using the PEHE\n62\nC Recurrent Neural Networks (RNN)\n63\nD Generative Modeling through Adversarial Training\n64\nD.1 GANs as Generative Models of Treatment Effect Distributions (GANITE) . . . . . . .\n65\nD.2 Adversarial Representation Balancing\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nBoxes\n1\nBox 1: Example Scenarios for Causal Inference with Non-Traditional Data . . . . . . .\n6\n2\nBox 2: Basic Introduction to Supervised Learning\n. . . . . . . . . . . . . . . . . . . .\n6\n3\nBox 3: Reading Machine Learning Papers: Computational Graphs and Loss Functions\n8\n4\nBox 4: Basic Introduction to Causal Inference . . . . . . . . . . . . . . . . . . . . . . .\n16\n5\nBox 5: Applied Causal Inference Example: The Infant Health and Development Study\n17\n6\nBox 6: Notation for Causal Inference and Estimation . . . . . . . . . . . . . . . . . . .\n20\n7\nBox 7: TARNet in Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n8\nBox 8: Graph Neural Networks and Transformers . . . . . . . . . . . . . . . . . . . . .\n40\n9\nBox 9: Generative Adversarial Networks (GAN)\n. . . . . . . . . . . . . . . . . . . . .\n64\n3\n1\nIntroduction\nThis primer aims to introduce social science readers to an exciting literature exploring how deep neural\nnetworks can be used to estimate causal effects. In recent years, both causal inference frameworks and\ndeep learning have seen rapid adoption across science, industry, and medicine. Causal inference has a\nlong tradition in the social sciences, and social scientists are increasingly exploring the use of machine\nlearning for causal inference (Athey and Imbens, 2016; Wager and Athey, 2018; Chernozhukov et al.,\n2018). Nevertheless, deep learning remains conspicuously underutilized by social scientists compared\nto other ML approaches, both for causal inference and more generally.\nThe deep learning revolution has been spurred by the flexibility and expressiveness of these models.\nNeural networks are nearly non-parametric and can theoretically approximate any continuous function\n(Cybenko, 1989), making them well suited for both classification and regression tasks. Furthermore,\nthey can be configured with different architectures and objectives to learn from a variety of quantitative\ndata as well as text, images, video, networks, and speech.\nThese advantages allow them to learn\nvector “representations” of complex data with emergent properties. Simple examples of representation\nlearning include the Word2Vec algorithm that discovers semantic relationship between words in texts,\nor face classification models that learn vectors describing facial features (Mikolov et al., 2013). More\nrecently, generative models like DALL-E, Stable Diffusion, and ChatGPT have shown how coherent\ntext passages and life-like images can be reconstructed from learned representations.\nHere we explore the potential for leveraging these advantages to estimate causal effects. Causal\ninference frameworks are non-parametric, but the linear models traditionally used to estimate causal\neffects require strong parametric assumptions. In contrast, the nearly non-parametric nature of neural\nnetworks allows us to estimate smooth response surfaces that capture heterogeneous treatment effects\nfor individual units with low bias.1 The ability of these models to learn from complex data means\nwe can extend causal inference to new settings where confounding is complicated, time-varying, or\neven encoded in texts, graphs, or images (see Box 1 for hypothetical examples). Lastly given the right\nobjectives, neural networks promise to learn deconfounded representations of data, presenting a new\n1Neural networks can have hundreds to billions of parameters making them effectively non-parametric. The risks of\noverparameterization of neural networks are discussed in Section 2.\n4\nstrategy for treatment modeling.\nThis primer synthesizes existing literature on deep causal estimators, but it is not a review; its\ngoals are fundamentally pedagogical and prospective rather than retrospective.\nIn Section 2, we\nintroduce social scientists to the fundamental concepts of deep learning, and the basic workflow for\nbuilding and training their own deep neural networks within a supervised learning framework. For\nreaders unfamiliar with causal inference, Section 3 introduces the assumptions of causal identification\nand three fundamental estimation strategies within the selection on observables design: matching,\noutcome modeling, and inverse propensity score weighting. Machine learning models often perform\npoorly in both theory and practice when only one of these strategies is employed, so we also introduce\nthe concept of double robustness.\nSection 4 is the main body of the article. Here we introduce four related deep learning models for\nthe estimation of heterogeneous treatment effects: the S-learner, T-learner, TARNet and Dragonnet\n(Shalit et al., 2017; Shi et al., 2019). Although this literature is rapidly evolving, these four models\nare sufficient to illustrate how traditional estimation strategies can be used in creative ways that\nleverage the key strengths of neural networks (i.e., deconfounding through representation learning,\nsemi-parametric inference). Section 5 deals with the practical considerations of building confidence\nintervals and interpreting neural networks. These guidelines are concretized in the companion online\ntutorials, which show readers how to implement and interpret the models described in Section 4 in\nTensorflow 2 and PyTorch.\nIn Section 6, we focus on the future of deep causal information: estimators that can disentange\ncounfounding relationships embedded within texts, images, graphs, or time-varying data.\nIn the\ninterest of clarity, we give hypothetical examples of the types of questions social scientists might\nanswer with these models, and briefly describe ongoing research on each of these modalities. For fuller\ntreatments of some of these models, see the appendix. We conclude with a discussion of how neural\nnetworks fit into the broader literature on machine learning for causal inference (Section 7).\nThe primer makes multiple contributions. First, it is one of the first pieces in the sociological\nliterature to introduce the fundamentals of deep learning not only at a conceptual level (e.g, back-\npropagation, representation learning), but at a practical one (e.g., validation, hyperparameter tuning).\n5\nOur recommendations for training and interpreting neural networks are supported by heavily anno-\ntated tutorials that teach readers without prior familiarity with deep learning how to build their own\ncustom models in Tensorflow 2 and PyTorch. Second, we use this foundation and select examples\nto build intuition on how the core strengths of deep learning can be leveraged for causal inference.\nFinally, we highlight future directions for this literature and argue why the future of causal estimation\nruns through deep learning.\nBox 1: Example Scenarios for Causal Inference with Non-Traditional Data\nText. As a motivating example, Veitch et al. (2020) consider the effect of the author’s\nreported gender (T) on the number of upvotes a Reddit post receives (Y ). However gender\nmay also “affect the text of the post, e.g., through tone, style, or topic choices, which also\naffects its score [(X)].” Controlling for a representation of the text would allow the analyst to\nmore accurately estimate the direct effect of gender.\nImages. Todorov et al. (2005) showed that split second-judgments of a politician’s compe-\ntence (T) from pictures (X) of their face is predictive of their electability (Y ). When attempting\nto replicate this study using machine learning classifiers rather than human classifiers, Joo et al.\n(2015) suggest that the age of the face (Z) is a not-so-obvious confounder: while older individ-\nuals are more likely to appear competent, they are also more likely to be incumbents. Even\nif age is unknown, using neural networks to control for confounders implicitly encoded in the\nimage (like age) could reduce bias.\nNetworks. Nagpal et al. (2020) explore the question of which types of prescription opioids\n(e.g., natural, semi-synthetic, synthetic) (T) are most likely to cause long term addiction (Y ).\nBecause of predisposition to different injuries, type of employment (X) could be a common cause\nof both treatment and outcome. Suppose job type is unobserved, but we know that patients\nare likely to associate with coworkers through homophily. To capture some of the effects of\nthis latent unobserved confounder, analysts might choose to control for a representation of the\npatient’s position in their social network when estimating the causal effect.\n2\nDeep Learning Fundamentals\n2.1\nArtificial Neural Networks\nBox 2: Basic Introduction to Supervised Learning\nDeep learning algorithms have most commonly been adapted for causal inference using\nsupervised machine learning, the most popular learning framework within the field.a The goal\nof supervised learning is teach a model a non-linear function that transforms covariates/features\n6\nX into predicted outcomes ˆY in unseen data. The model learns this function from labeled\nexamples of Xtr and Ytr in a training dataset.\nAs in traditional statistical analyses, the function is learned by optimizing the model’s\nparameters such that they minimize the error between its predictions ˆ\nYtr and the true values\nYtr using a loss function (e.g., a likelihood). In a traditional social science analysis focused on\ninference, we would stop here and interpret these parameters. In supervised machine learning\nwhere the focus is on generalization to unseen data, the model is ultimately used to predict\noutcomes Yte in a test dataset of previously unseen covariates/features Xte. This framework\ncan be generically applied to cases where Y is categorical (called classification problems), and\nwhere Y is continuous (called regression problems).\nStatistical learning theory articulates the central challenge of supervised learning as a bal-\nance between overfitting and underfitting the training dataset.\nThis is also called the\nbias-variance” tradeoff. In a regression context, bias error is the difference between the\nexpected value of Y and the expected value of the mapping function learned by the model.b\nHigh bias typically results from an algorithm that has not sufficiently learned the relationships\nin the training dataset (i.e., underfit the data). In contrast, an algorithm that has learned the\ntraining dataset so closely that it is fitting noise in the sample (i.e., overfitting) is likely to\ngeneralize poorly, producing out-of-sample predictions with high variance. Underfitting can be\neasily diagnosed and addressed by increasing the complexity of the model. In the case of deep\nlearning, model complexity can be increased by adding additional layers or parameters/neurons.\nDiagnosing and addressing overfitting is a more challenging problem. In supervised learning,\noverfitting is diagnosed after training (but before testing) by assessing predictive performance\nin a reserved portion of the training set called the validation set. If the model fits the training\ndataset well but performs poorly in the validation set, it is likely to generalize poorly to the\ntest set as well. To prevent overfitting, regularization techniques can be used to simplify the\ncomplexity of the model. Training and regularization of neural networks is discussed in detail\nin Section 2.2. For a full treatment of supervised learning and statistical learning theory, see\nHastie et al. (2009).\naThe other two prominent paradigms are unsupervised learning and reinforcement learning.\nbNote that bias in statistical learning theory is not equivalent to bias of a statistical estimator.\nArtificial neural networks (ANN) are statistical models inspired by the human brain (Brand et al.,\n2020; Goodfellow et al., 2016). In an ANN, each “neuron” in the network takes the weighted sum\nof its inputs (the outputs of other neurons) and transforms them using a differentiable, non-linear\nfunction (e.g. sigmoid, rectified linear unit) that outputs a value between 0 and 1 if the transformed\nvalue is above some threshold. Neurons are arrayed in layers where an input layer takes the raw data,\nand neurons in subsequent layers take the weighted sum of outputs in previous layers as input. An\n“output” layer contains a neuron for each of the predicted outcomes with transformation functions\nappropriate to those outcomes. For example, a regression network that predicts one outcome will\n7\nhave a single output neuron without a transformation function so that it produces a real number. A\nregression network without any hidden layers corresponds exactly to a generalized linear model (Fig.\n1A). When additional “hidden” layers are added between the input and output layers, the architecture\nis called a feed-forward network or multi-layer perceptron (Fig. 1B). A neural network with\nmultiple hidden layers is called a “deep” network, hence the name “deep learning” (LeCun et al.,\n2015). A neural network with a single, large enough hidden layer can theoretically approximate any\ncontinuous function (Cybenko, 1989).\nBox 3: Reading Machine Learning Papers: Computational Graphs and Loss Func-\ntions\nWithin the machine learning literature, novel algorithms are often presented in terms of\ntheir computational graph and loss function. A computational graph (not to be confused with\na causal graph) uses arrows to depicts the flow of data from the inputs of a neural network,\nthrough parameters, to the outputs. Layers of neurons or specialized sub-architectures are often\ngenerically abstracted as shapes. In our diagrams, we use rounded purple shapes to represent\nobservables, orange rectangles for representation layers of the network, rounded white shapes\nfor produced outputs, and textured rectangles for outcome modeling layers. Operations that\nare computed after prediction (i.e., for which an error gradient is not calculated) are shown\nwith dashed lines (e.g., plug-in estimation of causal estimands).\nAlong with the architecture, the loss function of a neural network is the primary means for\nthe analyst to dictate what types of representations a neural network learns and what types of\noutputs it produces. In multi-task learning settings, we denote joint loss functions for an entire\nnetwork as a weighted sum of the losses for substituent tasks and modules. These specific losses\nare weighted by hyperparameters. For example, we might weight the joint loss for a network\nthat predicts outcomes and propensity scores as:\narg min\nh,π\nL = Lh + λLπ = MSE(Y, h(X, T)) + λBCE(T, π(X, T))\nwhere h(X, T) is the predicted potential outcome, π(X, T) is the predicted propensity score, λ\nis a hyperparameter and MSE and BCE stand for mean squared error and binary cross entropy\n(i.e., log loss), common losses for regression and binary classification respectively (Box 6).\nNeural networks are trained to predict their outcomes by optimizing a loss function (also called\nan objective or cost function).\nDuring training, the backpropagation algorithm uses calculus’s\nchain rule to assign portions of the total error in the loss function to each neuron in the network.\nAn optimizer, such as the stochastic gradient descent algorithm or the currently popular ADAM\nalgorithm (Kingma and Ba, 2015), then moves each parameter in the opposite direction of this error\n8\nFigure 1: A: Generalized linear model represented as a computational graph. Observable\ncovariates X1, X2, X3 and treatment status T depicted in rounded purple boxes. Each of the lines\nbetween the rounded purple inputs and the textured box represents a parameter (i.e., a β in a gen-\neralized linear model equation). The textured box is an “output neuron” that sums it’s weighted\ninputs, performs a transformation g (the link function in GLM; in this case the identity function), and\npredicts the conditional outcome ˆY (T). Instead of theoretically interpreting these parameters from\nan inferential statistics perspective, machine learning approaches typically use the predicted observed\nand unobserved potential outcomes for plug-in estimation of causal estimands (e.g., the conditional\naverage treatment effect\nˆ\nCATE). After training, setting T to 1 −T for each observation can predict\nthe unobserved potential outcome ˆY (1 −T). Because this operation occurs after prediction and does\nnot feed a gradient back to the network to optimize the parameters, it is depicted here with a dotted\nline. Plug-in calculation of\nˆ\nCATE is similarly shown with a dotted line.\nB: Feed-forward neural network (S-learner). In a feed-forward neural network, additional fully\nconnected (parameterized) layers of neurons are added between the inputs (rounded purple) and out-\nput neuron. The size of the input covariates and hidden layers are generically abstracted as boxes\n(orange). The final hidden layer before the output neuron is denoted Φ because the hidden layers\ncollectively encode a representation function (see section 2.3). In causal inference settings, this archi-\ntecture is sometimes called a S(ingle)-learner because one feed-forward network learns to predict both\npotential outcomes.\n9\ngradient. Neural networks first rose to popularity in the 1980s but fell out of favor compared to other\nmachine learning model families (e.g., support vector machines) due to their expense of training. By\nthe late 2000s, improvements to backpropagation, advances in computing power (i.e., graphic cards),\nand access to larger datasets collectively enabled a deep learning revolution where ANNs began to\nsignificantly outperform other model families. Today, deep learning is the hegemonic machine learning\napproach in industries and fields other than social science.\n2.2\nDeep Learning in Practice\nThis section focuses on the practice of training neural networks within a supervised learning framework.\nWhile the principles behind supervised machine learning are universal, the workflow for neural networks\ndiffers substantially from other ML approaches (e.g., random forests, support vector machines) in\npractice. Figure 2 presents this workflow in four different parts: Set Up, Training, Model Evaluation,\nand Interpretation. We delve into each of these topics in more detail below. Box 2 contains a basic\nintroduction to supervised learning for unfamiliar readers.\n2.2.1\nSet Up and Hyperparameters\nThe first step in training a neural network, as in other types of supervised machine learning, is to split\nyour dataset into training, validation, and testing datasets (Fig. 2A). If the network is being used for\nstatistical inference, as here, the testing dataset is optional, and inference may be conducted on just\nthe validation set or the full dataset.\nWhile the computational graph and loss function define a deep learning architecture (Box 3), actual\nimplementations can vary significantly due to the choice of hyperparameters. In supervised machine\nlearning, hyperparameters are parameters that are not learned automatically when training the\nmodel, but must be specified by the analyst. In deep learning, architectural hyperparameters include\nthe number of layers to use for each section of the computational graph, the number of neurons to use\nin each layer, and the activation functions to be used by neurons. While some basic rules of thumb\napply (e.g., use fewer layers than neurons), these choices remain poorly understood theoretically2;\n2For some interesting work on understanding neural networks theoretically from a statistical physics perspective see\n10\nFigure 2: Supervised Deep Learning Workflow. 1) Set Up: The first step in training a deep\nlearning model is splitting the data into a training set, validation set, and optionally a test set. Initial\nhyperparameters are then selected from a set of choices specified by the user.\n2) Training: In\neach iteration of the training process (called an epoch), the training set is randomly divided into\nsmall minibatches For each minibatch, the network makes predictions for all units, and calculates\nthe error gradients to be assigned to each neuron in the network based on those predictions. An\noptimizer then move the network’s parameters in the opposite direction of the error gradient. After all\nminibatches have been trained (one epoch), error is calculated on the entire validation set. This whole\nprocess is repeated up until the validation error stops decreasing (to avoid overfitting). 3) Model\nEvaluation: A criterion (typically the validation error) is used to evaluate the performance of this\nhyperparameterization. New hyperparameters are then selected using a hyperparameter optimization\nalgorithm (eg. Grid search, Bayesian hyperparameter optimization, genetic algorithms) and steps 1\nand 2 are repeated. Once the hyperparameter optimization algorithm has completed its search, the\n“best” model is selected for inference. 4) Inference and interpretation: With a model selected,\nthe analyst is now ready to apply it to their test data (or in the case of statistical inference, potentially\nthe full dataset). Predictions of the outcomes and/or propensity score can then be used to compute\nthe CATE and calculate confidence intervals. Feature importance algorithms like SHAP or Integrated\nGradients can also be used to interpret the CATE estimates.\n11\nDecisions are generally made by comparing empirical performance on the validation set, a practice\ncalled hyperparameter tuning.\n2.2.2\nTraining and Regularization\nNeural networks are trained by repeatedly making predictions from the training set, calculating error\ngradients for each parameter, and backpropagating small fractions of those error gradients. (Fig. 2 B).\nA full pass through examples in the training set is called a training loop or epoch. At the beginning of\neach epoch, the training set is divided into mini-batches of 2 to 1000 units, randomly sampled without\nreplacement. This practice not only aids in memory management, it also improves optimization. Using\nsmall random samples reduces the risk of large “exploding” error gradients, particularly early in the\ntraining, that could cause the model to overshoot optimal solutions and instead get stuck in local\nminima.\nThe size of mini-batches can be considered a hyperparameter.3 Because a mini-batch of data is\nonly a sample of a sample (the training dataset), the optimizer only adjusts weight parameters by a\nfraction of the error gradient (the learning rate) to avoid overfitting. The learning rate is also a\nhyperparameter, that typically varies between 0.0001 and 0.01.\nThe non-convex nature of most loss functions4 means that optimization often requires hundreds\nto potentially millions of epochs of training.\nMoreover, neural networks are highly susceptible to\noverfitting because it is easy to overparameterize them with excessive neurons/layers. To ward against\noverfitting, error metrics on the complete validation set are computed at the end of every epoch.\nIn a regularization practice called “early stopping,” analysts usually stop training once validation\nmetrics stop improving.\nOther common regularization techniques include weight decay (i.e., ℓ2\nnorm, ridge, or Tikhonov) penalties on the parameters, dropout of neurons during training, and batch\nnormalization.\nRoberts et al. (2022).\n3In the specific context of causal inference, we recommend not having mini-batches that are too small such that the\nmodel can learn from both treated and control units with sufficient overlap.\n4In convex functions (e.g. the OLS loss), there is a single minimum, so optimizing the function means that you will\nalways converge at the same parameter weights. This is not the case for non-convex functions which may have many\nlocal minima.\n12\nDropout is a regularization technique in deep learning where certain nodes are randomly silenced\nfrom training during a given epoch (Srivastava et al., 2014). The general idea of dropout is to force\ntwo neurons in the same layer to learn different aspects of the covariate/feature space and reduce\noverfitting. Batch normalization is another regularization technique applied to a layer of neurons\n(Ioffe and Szegedy, 2015). By standardizing (i.e. z-scoring) the inputs to a layer on a per-batch basis\nand then rescaling them using trainable parameters, batch normalization smooths the optimization of\nthe loss function. The addition and extent of each of these regularization techniques can be treated\nas hyperparemeters.\n2.2.3\nModel Selection\n(Tutorial 2\n)\nAfter the model has been trained, the analyst compares models assembled with different hyper-\nparameterizations or initial parameter values (Fig. 2C). Hyperparameterizations can be chosen using\nrandom search, an exhaustive grid search of all possible combinations, or strategic search algorithms\nlike Bayesian hyperparameter optimization or evolutionary optimization (Snoek et al., 2012). Valida-\ntion loss metrics on the final epoch are commonly used for these comparisons.\nModel selection for causal estimators is complicated by the fundamental problem of causal inference:\nwe are not actually interested in the observed “factual” outcomes and propensity scores, but the CATE\nand ATE. In the case of algorithms like Dragonnet 4.3 where the validation loss explicitly targets a\ncausal quantity, we use that as the model selection criterion. In cases where the algorithm is only\ntrained for outcome modeling or propensity modeling, other solutions are needed. In the Appendix,\nwe describe Johansson et al. (2020)’s proposal to use matching on a nearest neighbor approximation of\nthe Precision in Estimated Heterogeneous Effects (PEHE), a measure of CATE bias, as an alternative\nmodel selection metric (Appendix A.B).\nThe development of more sophisticated methods for model selection of causal estimators through\ndata simulation is an active area of research within this literature.5\nFor example, Parikh et al.\n5We note that crossfitting (Zivich and Breskin, 2021), another approach that has emerged for model selection of other\ntypes of machine learning causal estimators may work for the models discussed here, but is likely data-inefficient.\n13\n(2022) use deep generative models to approximate the data generating distribution under weak, non-\nparametric assumptions. Alaa and Van Der Schaar (2019) independently model each outcome and the\npropensity score before using influence functions to assess model error.\n2.3\nRepresentation Learning and Multitask Learning\nOne comparative advantage of deep learning over other machine learning approaches has been the\nability of ANNs to encode and automatically compress informative features from complex data into\nflexible, relevant “representations” or “embeddings” that make downstream supervised learning\ntasks easier (Goodfellow et al., 2016; Bengio, 2013). While other machine learning approaches may\nalso encode representations, they often require extensive pre-processing to create useful features for\nthe algorithm (i.e., feature engineering). Through the lens of representation learning, a geometric\ninterpretation of the role of each layer in a supervised neural network is to transform its inputs (either\nraw data or output of previous layers) into a typically lower (but possibly higher) dimensional vector\nspace. As a means to share statistical power, encoded representations can also be jointly learned for\ntwo tasks at once in multi-task learning.\nThe simplest example of a representation might be the final layer in a feed-forward network,\nwhere the early layers of the network can be understood as non-linearly encoding the inputs into an\narray of latent linear features for the output neuron (Goodfellow et al., 2016) (Fig. 1B). A famous\nexample of representation learning is the use of neural networks for face detection. Examining the\nrepresentations produced by each layer of these networks shows that each subsequent layer seems to\ncapture increasingly abstract features of a face (first edges, then noses and eyes, and finally whole\nfaces) (LeCun et al., 2015). A more familiar example of representation learning to social scientists\nmight be word vector models like Word2Vec (Mikolov et al., 2013). Word2Vec is a neural network with\none hidden layer and one output layer where words that are semantically similar are closer together\nin the representation space created by the hidden layer of the network.\nThe novel contribution of deep learning to causal estimation is the proposal that a neural network\ncan learn a function Φ that produces representations of the covariates decorrelated from the treatment.\n14\nFigure 3: Balancing through representation learning. The promise of deep learning for causal\ninference is that a neural network encoding function Φ can transform the treated and control covariate\ndistributions into a representation space such that they are indistinguishable. Used with permission\nfrom Johansson and Shen (2018).\nFundamentally, the idea is that Φ can transform the treated and control covariate distributions into a\nrepresentation space such that they are indistinguishable (Fig. 3). To ensure that these representations\nare also still predictive of the outcome (multi-task learning), multiple loss functions are generally\napplied simultaneously to balance these objectives. This approach is applied in a majority of the\nalgorithms presented in section 4.\n3\nCausal Identification and Estimation Strategies\n3.1\nIdentification of Causal Effects\nThe papers described in this primer are primarily framed within the Potential Outcomes causal frame-\nwork (Neyman-Rubin causal model) (Rubin, 1974; Imbens and Rubin, 2015). This framework is con-\ncerned with identifying the “potential outcomes” of each unit i in the sample, had it received treatment\n(Y (1)) or not received treatment (Y (0)). However, because each unit can only receive one treatment\n15\nregime in reality (being treated or remaining untreated), it is not possible to observe both potential\noutcomes for each individual (often termed “the fundamental problem of causal inference”) (Holland,\n1986). While we cannot thus identify individual treatment effects τi = Yi(1) −Yi(0) for each unit,\ncausal inference frameworks allow us to probabilistically estimate average treatment effects (ATE)\nand average treatment effects conditional on select covariates (CATE) across samples of treated and\ncontrol units. Within this literature, the motivation of many papers is to present algorithms that\ncan both infer CATEs from observational data, but also predict them for out-of-sample units where\ntreatment status is unknown. For readers unfamiliar with causal inference, a short introduction is\nglossed in Box 4 with a concrete example, used in the tutorials, in Box 5.\nBox 4: Basic Introduction to Causal Inference\nCorrelation does not equal causation, and causal inference is concerned with the identifica-\ntion of causal relationships between random variables. Many causal questions we would like to\nask about social data (“What is the causal effect of T on Y for units with characteristics X?”)\ncan be unpacked as counterfactual questions with the general format: “What would have been\nthe outcome Y for a unit with X characteristics, if T had happened or not happened?”.\nRandomized control trials (RCTs, also known as A/B testing in data science and industry\napplications) are usually understood to be the ideal approach to answering this type of question:\neach unit with covariates or features X is randomly assigned to the treatment or control groups\nand outcome Y is subsequently measured. But in many scenarios it is prohibitively expensive\nor unethical (e.g., randomly assigning students to attend college or not) to collect experimental\ndata. In these cases, we can statistically adjust observational data (e.g., survey data on college\nattendance) to approximate the experimental ideal.\nThe methods described in this paper\nare designed to answer counterfactual questions with primarily non-experimental observational\ndata.\nThere are at least three different schools of causal inference that have been introduced\nin social statistics and econometrics (Rubin, 1974; Imbens and Rubin, 2015), epidemiology\n(Robins, 1986, 1987; Hern´an and Robins, 2020), and computer science (Goldszmidt and Pearl,\n1996; Pearl, 2009). The goal of these causal frameworks is to describe and correct for biases in\ndata or study design that would prevent one from making a true causal claim. If these biases\nare correctable and the causal effect can be uniquely expressed in terms of the distribution of\nobserved data, then we say that the causal effect is identifiable (Kennedy, 2016). Only if a\ncausal effect is identifiable, we can use statistical tools to correct for biases and estimate the\ncausal effect (e.g., inverse propensity score weighting, g-computation, deep learning). Other\nforms of bias, like sample selection bias, are possible to correct for, but are outside the focus\nof this article.\nThe algorithms presented in this paper focus on estimating causal effects primarily by\ncorrecting for confounding bias. Loosely speaking, a confounding covariate/feature is one that is\ncorrelated with both the treatment and the outcome, misleadingly suggesting that the treatment\n16\nhas a causal effect on the outcome, or obscuring a true causal relationship between the treatment\nand outcome. Often times, the confounder is a cause of the treatment and outcome. As an\nexample of confounding bias, estimating the causal effect of attending college (treatment) on\nadult income (outcome) requires controlling for the fact that parental income may be a common\ncause of both college attendance and adult income.\nThe ATE is defined as:\nATE = E[Yi(1) −Yi(0)] = E[τi]\nwhere Yi(1) and Yi(0) are the potential outcomes had the unit i received or not received the\ntreatment, respectively. The CATE is defined as,\nCATE = E[Yi(1) −Yi(0)|Xi = x] = E[τi|Xi = x]\nwhere X is the set of selected, observable covariates, and x ∈X.\nBox 5: Applied Causal Inference Example: The Infant Health and Development\nStudy\nTo make this problem setting more concrete for readers unfamiliar with causal inference,\nconsider simulations based on the 1985-1988 Infant Health and Development Study that are\nwidely used as benchmarks within this literature.\nIn this experiment, premature children\nwere randomly assigned to intensive, high-quality childcare (T), and their cognitive test scores\nwere measured later (Y ). The authors also measured numerous other covariates X including\n“pregnancy complications, child’s birth weight and gestation age, birth order, child’s gender,\nhousehold composition, day care arrangements, source of health care, quality of the home\nenvironment, parents’ race and ethnicity, and maternal age, education, IQ, and employment”\n(Ramey et al., 1992). The ATE would be the effect of intensive child care on cognitive scores\nacross all children, while various CATEs might be formulated to better understand how the\neffects of child care vary for female children, children born to teenage mothers, or children with\nunemployed parents.\nHill (2011) turns this experimental data into an observational benchmark by re-simulating\nthe outcome such that the covariates X induce confounding bias between the treatment and\noutcome. While the simulations don’t preserve the names of the covariates, we can imagine\nsome confounding relationships that might be present in an observational study. For example,\nsuppose that affluent (X1) parents are more likely able to afford high-quality child care (T), but\nthere is actually a weak association between childcare and premature babies’ cognitive ability\n(Y ). We also know affluent parents are more likely to engage in breastfeeding (X2), which is\n17\npositively associated with higher cognitive ability (Heck et al., 2006; Kramer et al., 2008). If\nwe do not account for the correlation between income and childcare (X1 →T), or income and\ncognitive ability (X1 →X2 →Y ), we may have bias in our ATE/CATE estimates, or worse,\nerroneously interpret the correlation between childcare and cognitive ability as causal. This\nexample is depicted in a causal graph below.\nThe hypothetical confounding bias presented here can be adjusted for either through treat-\nment modeling (e.g., inverse propensity score weighting, non-parametric, deep representation\nlearning) to block the path X1 →T, outcome modeling (e.g., generalized linear models, deep\nregression) to block the path X1 →X2 →Y , or both (see Section 3.2). For coded examples\nusing many of these approaches in Tensorflow and Pytorch on the IHDP benchmark, please see\nthe tutorials.\nWithin the machine learning literature on causal inference treated here, the primary strategy for\ncausal identification is selection on observables. A challenge to identifying causal effects is the\npresence of confounding relationships between covariates associated with both the treatment and the\noutcome.\nThe key assumptions allowing the identification of causal effects in the presence of confounding is:\n1. Conditional Ignorability/Exchangability The potential outcomes Y (0), Y (1) and the treat-\n18\nment T are conditionally independent given X,\nY (0), Y (1) ⊥⊥T|X\nConditional Ignorability specifies that there are no unmeasured confounders that affect both treatment\nand outcome outside of those in the observed covariates/features X. Additionally X may contain\npredictors of the outcome (helping precision), but should not contain instrumental variables (hurting\nprecision and potentially amplifying residual bias) or colliders within the conditioning set.6\nOther standard assumptions invoked to justify causal identification are:\n2. Consistency/Stable Unit Treatment Value Assumption (SUTVA). Consistency speci-\nfies that when a unit receives treatment, their observed outcome is exactly the corresponding potential\noutcome (and the same goes for the outcomes under the control condition). Moreover, the response\nof any unit does not vary with the treatment assignment to other units (i.e., no network or spillover\neffects), and the form/level of treatment is homogeneous and consistent across units (no multiple ver-\nsions of the treatment). Note that this is an identification assumption, based on our understanding of\nthe data generating process, and independent of the model chosen for estimation. More formally,\nT = t →Y = Y (T)\n3. Overlap. For all x ∈X (i.e., any observed covariate value), all treatments t ∈{0, 1} have a\nnon-zero probability of being observed in the data, within the “strata” defined by such covariates,\n1 > p(T = t|X = x) > 0\n4. An additional assumption sometimes invoked at the interface of identification and estimation\nusing neural networks is:\n6A variable is a collider if it is caused by two other variables. Controlling for colliding variables, or descendants\nof colliding variables, will induce a spurious correlation between the parents. In the case of adjusting for confounding,\ncontrolling for a collider variable can (re-)open a confounding path that would otherwise be closed, introducing additional\nbias.\n19\nInvertability\nΦ−1(Φ(X)) = X\nIn words, there must exist an inverse function of the representation function Φ encoded by a neural\nnetwork that can reproduce X from representation space. This is required for the Conditional Ignor-\nability assumption to hold when using representation learning. From a practical perspective, it also\nmeans that the representation we created is rich enough to capture the causal relationships we are\ninterested in.\nFor reference, we describe the full notation used within the review in Box 6.\nBox 6: Notation for Causal Inference and Estimation\nWe use uppercase to denote general quantities (e.g., random variables) and lowercase to\ndenote specific quantities for individual units (e.g., observed variable values).\nCausal identification\n• Observed covariates/features: X\n• Potential outcomes: Y (0) and Y (1)\n• Treatment: T\n• Unobservable Individual Treatment Effect: τi = Yi(1) −Yi(0)\n• Average Treatment Effect: ATE = E[Yi(1) −Yi(0)] = E[τi]\n• Conditional Average Treatment Effect: CATE(x) = E[Yi(1) −Yi(0)|Xi = x] = E[τi|Xi = x]\nDeep learning estimation\n• Predicted potential outcomes: ˆY (0) and ˆY (1)\n• Outcome modeling functions: ˆY (T) = h(X, T)\n• Propensity score function: π(X, T) = P(T|X) (where π(X, 0) = 1 −π(X, 1))\n• Representation functions: Φ(X) (producing representations ϕ)\n• Loss functions: L(true, predicted)\n• Loss abbreviations: MSE (mean squared error), BCE (binary cross-entropy), CCE (categorical\ncross-entropy)\n• Loss hyperparameters: λ, α, β\n• Estimated CATE*:\nˆ\nCATEi = ˆτi = ˆYi(1) −ˆYi(0) = h(X, 1) −h(X, 0)\n• Estimated ATE:\nˆ\nATE =\n1\nN\nPN\ni=1 ˆτi\nBeyond the ATE and CATE there is an additional metric commonly used in the machine learn-\ning literature, first introduced by Hill (2011) called the Precision in Estimated Heterogeneous\nEffects (PEHE). PEHE is the average error across the predicted CATEs.\n• Precision in Estimated Heterogeneous Effects: PEHE =\n1\nN\nPN\ni=1(τi −ˆτi)2\n20\nBeyond being a metric for simulations with known counterfactuals, the PEHE has theoretical\nsignificance in the formulation of generalization bounds within this literature (Shalit et al.,\n2017; Johansson et al., 2018, 2020; Zhang et al., 2020).\n*Note that we use ˆτ to refer to the estimated CATE because truly individual treatment\neffects cannot be described only by the observed covariates X.\n3.2\nEstimation of Causal Effects\nOnce a strategy for identifying causal effects from available data has been developed (arguably the\nharder and more important part of causal inference), statistical methods can be used to estimate causal\neffects by controlling for confounding bias, selection bias, and/or measurement error. There are two\nfundamental approaches to estimation: treatment modeling to control for correlations between the\ncovariates X and the treatment T, and outcome modeling to control for correlations between the\ntreatment X and the outcome Y (Fig. 4). Below we briefly review three traditional techniques for\nremoving confounding bias to motivate our systematization of deep learning models. First, we discuss\noutcome modeling through regression. Next, we consider treatment modeling through non-parametric\nmatching. Finally, we discuss treatment modeling through inverse propensity score weighting (IPW)\nand introduce the concept of double robustness.\n3.2.1\nOutcome Modeling: Regression\nAssuming the treatment effect is constant across covariates/features or the probability of treatment\nis constant across all covariates/features (both improbable assumptions), the simplest consistent ap-\nproach to estimating the ATE is to regress the outcome on the treatment indicator and covariates\nusing a linear model.7 The ATE is then the coefficient of the treatment indicator. Without loss of\ngenerality, we call outcome models of this nature, linear or non-linear, h:\nˆYi(T) = h(Xi, T)\n7Another outcome modeling approach that could be used to estimate the outcome, not discussed here, is g-\ncomputation (Robins, 1986; Hern´an and Robins, 2020).\n21\nFigure 4: Two fundamental approaches to deconfounding. Blunted arrows indicate blocked\ncausal paths. Treatment modeling approaches like inverse propensity weighting, balancing, and repre-\nsentation learning adjust for the association between the covariates X and the treatment T. Outcome\nmodeling approaches like generalized linear models or machine learning regressors adjust for the asso-\nciation between X and the outcome Y .\n22\nA slightly more sophisticated semi-parametric approach to outcome modeling, used widely in\nthe application of machine learning to causal inference, is to use h(X, T) to impute ˆY (1) and ˆY (0),\nand calculate the CATE for each unit as a plug-in estimator:\n\\\nCATEi = ˆτi =\nˆ\nYi(1) −\nˆ\nYi(0) = h(Xi, 1) −h(Xi, 0)\nand the ATE as:\n[\nATE = 1\nN\nN\nX\ni=1\nˆτi\n3.2.2\nTreatment Modeling: Non-Parametric Matching\nA common treatment-modeling strategy is balancing the treated and control covariate distributions\nthrough matching.\nMatching requires the analyst to select a distance measure that captures the\ndifference in observed covariate distributons between a treated and untreated unit (Austin, 2011).\nUnits with treatment status T can then be matched with one or more counterparts with treatment\nstatus 1−T using a variety of algorithms (Stuart, 2010). In a one-to-one matching scenario where each\ntreated unit has an otherwise identical untreated counterpart, the covariate distribution of treated and\ncontrol units is indistinguishable.\n3.2.3\nTreatment Modeling: Inverse Propensity Score Weighting\nAnother common approach is inverse propensity score weighting (IPW). In IPW, units are\nweighted on their inverse propensity to receive treatment.\nWithout loss of generality, we call the\npropensity function π. The propensity score is calculated as the probability of receiving treatment\nconditional on covariates:\nπ(X, T) = P(T|X)\nThe simplest IPW estimator of the ATE is then:\n23\n[\nATE = 1\nN\nN\nX\ni=1\n\u001a\nTiYi\nˆπ(Xi, 1) + (1 −Ti)Yi\nˆπ(Xi, 0)\n\u001b\n(1)\nNote that only one of the two terms is active for any given unit. Furthermore, this presentation\nlooks different than how the IPW is generally presented because we use π as a function with different\noutputs depending on the value of T rather than a scalar (Box 6).8\nIPW weighting is attractive because if the propensity score π is specified correctly, it is an unbiased\nestimator of the ATE. Moreover, the IPW is consistent if π is estimated consistently (Rosenbaum and\nRubin, 1983; Glynn and Quinn, 2010).\n3.2.4\nDouble Robustness\nBecause different models make different assumptions, it is not uncommon to combine outcome modeling\nwith propensity modeling or matching estimators to create doubly-robust estimators. For example,\none of the most widely used doubly-robust estimators is the Augmented-IPW (AIPW) estimator.\nˆ\nATE = 1\nN\nN\nX\ni=1\n[(\nT\nπ(Φ(X), 1) −\n1 −T\nπ(Φ(X), 0))\n|\n{z\n}\nTreatment Modeling\n× [Y −h(Φ(X), T)]\n|\n{z\n}\nResidual Confounding\n|\n{z\n}\nAdjustment\n+ [h(Φ(X), 1) −h(Φ(X), 0)]\n|\n{z\n}\nOutcome Modeling\n(2)\nThe first term is the difference in prediction from two outcome models, one for treated and one for\ncontrol units, while the last terms is a “corrected” IPW estimator replacing the raw outcome by the\nresiduals from the regression models. As expected, this estimator is unbiased if the IPW and regression\nestimators are consistently estimated. However, the model is attractive because it will be consistent\nif either the propensity score π(X, T) is correctly specified or the regression model h is consistently\nspecified (Glynn and Quinn, 2010). The model also provide efficiency gains with respect to the use of\neach model separately, and especially with respect to weighting alone.\n8To de-emphasize the contribution of units with extreme weights due to sparse data, sometimes a “stabilized” IPW\nis used (Glynn and Quinn, 2010).\n24\nDoubly robust estimation is especially important for causal estimation using machine learning.\nWhen using simple outcome plug-in estimators, bias is directly dependent on estimation error, which\nmay be different for each potential outcome depending on the modeling strategy (Kennedy, 2020).\nMachine learning estimation of the propensity score can also rely heavily on non-confounding predic-\ntors, giving rise to extreme weights (Schnitzer et al., 2016). More generally, there are no asymptotic\nlinearity guarantees for machine learning estimators which may converge at a slow rate, leading to\nmisleading confidence intervals (Naimi et al., 2021; Zivich and Breskin, 2021). For these reasons, plug-\nin machine learning estimation often has poor empirical performance when not using double robust\nestimators (Benkeser et al., 2017; Kennedy, 2020; Zivich and Breskin, 2021).\nThe growth of machine learning for causal inference literature has thus been largely driven by\nthe introduction of semi-parametric frameworks. Semi-parametric frameworks address these issues\nby using machine learning only to estimate the nuissance parameters (i.e., potential outcomes and\npropensity score) of influence functions for causal parameters like the ATE and CATE (Chernozhukov\net al., 2018; Kennedy, 2016; Van der Laan and Rose, 2011). In these approaches, the estimation of\ncausal parameters is only-second order dependent on machine learning error, there is double-robustness\nagainst inconsistent estimation, and guarantees of fast convergence and asymptotically-valid confidence\nintervals even if the machine learning models converge slowly (Benkeser et al., 2017; Kennedy, 2020;\nNaimi et al., 2021; Zivich and Breskin, 2021). We use the final algorithm introduced below, Dragonnet,\nas an opportunity to provide an intuitive introduction to semi-parametric theory and how it can be\nused for doubly robust estimation (Shi et al., 2019).\n4\nThree Different Approaches to Deep Causal Estimation\nThe architectures proposed in the deep learning literature for causal estimation build upon the core idea\ndiscussed above. First, we introduce “S-Learners” and “T-Learners” to show how neural networks can\nbe used to estimate non-linearities in potential outcomes. Second, given the right objectives, a neural\nnetwork can learn representations of the treated and control distributions that are deconfounded (Fig.\n3). This approach, which can be related theoretically to non-parametric matching, is illustrated by the\n25\nfoundational TARNet algorithm in section 4.3 (Shalit et al., 2017). Finally, the machine learning for\ncausal inference literature has been largely driven by the introduction of semi-parametric frameworks\nthat allow predictive machine learning models to be plugged-in to doubly robust estimation equations\n(Van der Laan and Rose, 2011; Chernozhukov et al., 2018, 2021). In section 4.3, we introduce the\nconcept of influence functions and the targeted maximum likelihood estimator to explain the Dragonnet\nalgorithm. For clarity the algorithms presented here all share a familial resemblence to the TARNet\nalgorithm. However, we note that there are many other approaches to using deep learning for causal\ninference (e.g., the generative models described in Appendix A.D).\n4.1\nDeep Outcome Modeling\nS-Learners and T-Learners (Tutorial 1\n)\nBecause at most one potential outcome is unobserved, it is not possible to apply supervised models\nto directly learn treatment effects. Across econometrics, biostatistics, and machine learning, a common\napproach to this challenge has been to instead use machine learning to model each potential outcome\nseparately and use plug-in estimators for treatment effects (Chernozhukov et al., 2018; Van der Laan\nand Rose, 2011; Wager and Athey, 2018). As with linear models, a single neural model can be trained\nto learn both potential outcomes (“S[ingle]-learner”) (Fig. 1B), or two independent models can be\ntrained to learn each potential outcome (a “T-learner”) (Johansson et al., 2020) (Fig. 5A). In both\ncases, the neural network estimators would be feed-forward networks tasked with minimizing the MSE\nin the prediction of observed outcomes. In a slight abuse of notation, the joint loss function for a\nT-learner can be written as:\nL(Y, h(X, T)) = MSE(Ti(Yi, h1(Xi, 1)) + (1 −Ti)(Yi, h0(Xi, 0))\n(3)\nwhere h1 and h0 represent separate networks for each potential outcome.\nAfter training, inputting the same unit into both networks of a T-learner will produce predictions\nfor both potential outcomes: ˆY (T) and ˆY (1 −T). We can plug-in these predictions to estimate the\n26\nFigure 5: A. T-learner. In a T-learner, separate feed-forward networks are used to model each out-\ncome (textured boxes). We denote the function encoded by these outcome modelers h. B. TARNet.\nTARNet extends the T-learner with shared representation layers (orange). The motivation behind\nTARNet (and further elaborations of this model) is that the multi-task objective of accurately pre-\ndicting both the treated and control potential outcomes forces the representation layers to learn a\nbalancing function Φ such that the Φ(X|T = 0) and Φ(X|T = 1) are overlapping distributions in\nrepresentation space. For a code implementation, see Box 7. C. Dragonnet Dragonnet also adds\na propensity score head to TARNet (black textured box) and a free “nudge” parameter ϵ.\nIn an\nadaptation of Targeted Maximum Likelihood Estimation, ˆπ and ϵ are used to re-weight the outcomes\nto provide lower biased estimates of the ATE.\n27\nCATE for each unit,\nˆτi = (1 −2Ti)( ˆYi(1 −Ti) −ˆYi(Ti))\nwhere the first term is a switch to make sure the treated potential outcome comes first. The average\ntreatment effect as,\n[\nATE = 1\nN\nN\nX\ni=1\nˆτi\nNearly all of the models described below combine this plug-in outcome modeling approach with\nother forms of treatment adjustment.\n4.2\nBalancing through Representation Learning\nTARNet (Tutorial 1\n)\nBalancing is a treatment adjustment strategy that aims to deconfound the treatment from outcome\nby forcing the treated and control covariate distributions closer together (Johansson et al., 2016). The\nnovel contribution of deep learning to the selection on observables literature is the proposal that a\nneural network can transform the covariates into a representation space Φ such that the treated and\ncontrol covariate distributions are indistinguishable (Fig. 3).\nTo encourage a neural network to learn balanced representations, the seminal paper in this liter-\nature, Shalit et al. (2017), proposes a simple two-headed neural network called Treatment Agnostic\nRegression Network (TARNet) that extends the outcome modeling T-learner with shared representa-\ntion layers (Fig. 5B). Each head models a separate potential outcome: one head learns the function\nˆY (1) = h1(Φ(X), 1), and the other head learns the function ˆY (0) = h0(Φ(X), 0). During training,\nonly one head will receive error gradients at a time (the one predicting the observed outcome). How-\never, both heads backpropagate their gradients to shared representation layers that learn Φ(X). The\nidea is that these representation layers must learn to balance the data because they are tasked with\npredicting both outcomes. The authors of this algorithm have subsequently extended TARNet with\nadditional losses in an algorithm called CFRNET that explicitly encourage balancing by minimizing\na statistical distance between the two covariate distributions in representation space (see Appendix\n28\nA.A for details) (Johansson et al., 2018, 2020).\nThe complete objective for the network is to fit the parameters of h and Φ for all n units in the\ntraining sample such that,\narg min\nh,Φ\n1\nN\nN\nX\ni=1\n(Yi −(Ti(Yi, h1(Φ(Xi), 1)\n|\n{z\n}\nˆ\nYi(1)\n) + (1 −Ti)(Yi, h0(Φ(Xi), 0)\n|\n{z\n}\nˆ\nYi(0)\n)2 + λ R(h)\n| {z }\nL2\n(4)\nor more compactly,\narg min\nh,Φ\nMSE(Yi, h(Φ(Xi), Ti)\n|\n{z\n}\nˆ\nYi(Ti)\n) + λ R(h)\n| {z }\nL2\n(5)\nwhere R(h) is a model complexity term (e.g., for L2 regularization) and λ is a hyperparameter chosen\nthrough model selection. For coded versions of TARNet in Tensorflow and Pytorch, see Box 7.\nBox 7: TARNet in Code\nBelow we show simple implementations of TARNet in Python Tensorflow 2 and Pytorch.\nFor more explanation on this implementation and to run this code on the IHDP data, see the\ntutorials.\nTensorflow 2 Functional API (Keras)\ndef make_tarnet(input_dim):\n#The argument is the number of X covariates.\nx = Input(shape=(input_dim,), name=’input’)\n#In TF fxnl API, stack layers by feeding output of prev layer to next\n#Make 2 representation layers\n#units is the output dim of layer\n#elu is \\\"exponentiated linear unit\" activation fxn\nphi = Dense(units=200, activation=’elu’)(x)\nphi = Dense(units=200, activation=’elu’)(phi)\n#Begin separate outcome modeling heads\ny0_hidden = Dense(units=100, activation=’elu’)(phi)\ny1_hidden = Dense(units=100, activation=’elu’)(phi)\n# Add second layers\ny0_hidden = Dense(units=100, activation=’elu’)(y0_hidden)\ny1_hidden = Dense(units=100, activation=’elu’)(y1_hidden)\n# Output predictions\ny0_pred = Dense(units=1, activation=None)(y0_hidden)\ny1_pred = Dense(units=1, activation=None)(y1_hidden)\n#Bundle outputs\nconcat_pred = Concatenate(1)([y0_pred, y1_pred])\n29\n#instantiate model\nmodel = Model(inputs=x, outputs=concat_pred)\nreturn model\nPytorch\nclass TARNet(nn.Module):\ndef __init__(self,input_dim):\nsuper(TARNet,self).__init__()\nself.phi = nn.Sequential(\n#both input and output dims are specified in torch\nnn.Linear(input_dim, 200),\nnn.ELU(), #activations are discrete from layers\nnn.Linear(200,200),\nnn.ELU())\nself.y0_hidden = nn.Sequential(\nnn.Linear(200, 100),\nnn.ELU(),\nnn.Linear(100,100),\nnn.ELU())\nself.y1_hidden = nn.Sequential(\nnn.Linear(200, 100),\nnn.ELU(),\nnn.Linear(100,100),\nnn.ELU())\nself.y0_pred =nn.Linear(100,1)\nself.y1_pred = nn.Linear(100,1)\n#the flow of data/gradients in torch is declared in a forward fxn\ndef forward(self,X):\nrep = self.phi(X)\ny0_rep=self.y0_hidden(rep)\ny0_hat=rep=self.y0_pred(y0_rep)\ny1_rep=rep=self.y1_hidden(rep)\ny1_hat=rep=self.y1_pred(y1_rep)\nreturn y0_hat, y1_hat\n4.3\nDouble Robustness with Inverse Propensity Score Weighting\nRather than applying losses directly to the representation function, IPW methods estimate propensity\nscores from representations using the function π(Φ(X), T) = P(T|Φ(X)).\nAs in traditional IPW\nestimators, these methods exploit the sufficiency of correctly-specified propensity scores to reweight\nthe plugged-in outcome predictions and provide unbiased estimates of the ATE (Rosenbaum and\n30\nRubin, 1983). Because these models combine outcome modeling with IPW, they retain the attractive\nstatistical properties of doubly robust estimators discussed in section 3.2.2 (Atan et al., 2018). In this\nsection we focus on Shi et al. (2019)’s Dragonnet model, which adapts semi-parametric estimation\ntheory for batch-wise neural network training in a procedure they call “Targeted Regularization”\n(TarReg) (Kennedy, 2016). Given the increasing importance of semi-parametric theory and “double\nmachine learning” across the causal estimation literature, we include a brief introduction to semi-\nparametric theory and targeted maximum likelihood estimation (TMLE) before diving into the details\nof the Dragonnet algorithm Van der Laan and Rose (2011); Chernozhukov et al. (2018).\nDragonnet (Tutorial 3\n/ Tutorial 4\n)\nA trivial extension to TARNet is to add a third head to predict the propensity score. This third\nhead could use multiple neural network layers or just a single neuron, as proposed in Dragonnet (Fig.\n5C) (Shi et al., 2019). Dragonnet uses this additional head to develop a training procedure called\n“Targeted Regularization” for semi-parametric causal estimation, inspired by “Targeted Maxmimum\nLikelihood Estimation” (TMLE)(Van der Laan and Rose, 2011).\nWith three heads, the basic loss function for this network looks like:\narg min\nΦ,π,h\nMSE(Yi, h(Φ(Xi), Ti)\n|\n{z\n}\nOutcome Loss\n+α BCE(Ti, π(Φ(Xi), Ti))\n|\n{z\n}\nπ Loss\n+λ R(h)\n| {z }\nL2\n(6)\nwith α being a hyperparameter to balance the two objectives. The mean squared error and binary cross-\nentropy are standard objective functions in machine learning for regression and binary classification,\nrespectively. Note that the first term is simply an expansion of the first term in equation 4.2\nBelow, we explore how the authors add a second loss on top of this one to allow for semi-parametric\nestimation.\n4.3.1\nSemi-parametric Theory of Causal Inference\nIn recent years, semi-parametric theory has emerged as a dominant theoretical framework for applying\nmachine learning algorithms, including neural networks, to causal estimation (Chernozhukov et al.,\n31\n2018, 2021, 2022; Farrell et al., 2021; Kennedy, 2016; Nie and Wager, 2021; Van der Laan and Rose,\n2011; Wager and Athey, 2018).\nThe great appeal of these frameworks is that they allow for ma-\nchine learning algorithms to be plugged-in for non-linear estimates of outcomes and propensity score,\nwhile still providing attractive statistical guarantees (e.g., consistency, efficiency, asymptotically-valid\nconfidence intervals).\nAt a very intuitive level, semi-parametric causal estimation is focused on estimating a target pa-\nrameter of a distribution P (the ATE) of treatment effects T(P) (Fisher and Kennedy, 2021). While\nwe do not know the true distribution of treatment effects because we lack counterfactuals, we do know\nsome parameters of this distribution (e.g., the treatment assignment mechanism). We can encode\nthese constraints in the form of a likelihood that parametrically defines a set of possible approximate\ndistributions P from our existing data P. Within this set there is a sample-inferred distribution ˜P ∈P,\nthat can be used to estimate T(P) using T( ˜P).\nRegardless of ˜P chosen, ˜P ̸= P →T( ˜P) ̸= T(P). We do not know how to pick ˜P with finite data\nto get the best estimate T( ˜P). We can maximize a likelihood function to pick ˜P, but there may be\n“nuisance” parameters in the likelihood that are not the target and we do not care about estimating\naccurately. Maximum likelihood optimization may provide lower-biased estimates of these nuissance\nterms at the cost of better estimates of T(P).\nTo sharpen the likelihood’s focus on T(P), we define a “nudge” parameter ϵ that moves ˜P closer\nto P (thus moving T( ˜P) closer to T(P)). An influence curve of T(P) tells us how changes in ϵ will\ninduce changes in T(P +ϵ( ˜P −P)). We’ll use this influence curve to fit ϵ to get a better approximation\nof T(P) within the likelihood framework. In particular, there is a specific efficient influence curve\n(EIC) that provides us with the lowest variance estimates of T(P). In causal estimation, solving\nthe EIC for the ATE yields estimates that are asymptotically unbiased, efficient, and have confidence\nintervals with (asymptotically) correct coverage.\nThe EIC for the ATE is,\n32\nEICAT E = 1\nN\nN\nX\ni=1\n[(\nTi\nπ(Xi, 1) −1 −Ti\nπ(Xi, 0))\n|\n{z\n}\nTreatment Modeling\n× (Yi −h(Xi, T))\n|\n{z\n}\nResidual Confounding\n|\n{z\n}\nAdjustment\n] + [h(Xi, 1) −h(Xi, 0)]\n|\n{z\n}\nOutcome Modeling\n] −ATE\n(7)\nSetting EICAT E to it’s mean of 0,\nATE = 1\nN\nN\nX\ni=1\n[(\nTi\nπ(Xi, 1) −1 −Ti\nπ(Xi, 0))\n|\n{z\n}\nTreatment Modeling\n× (Yi −h(Xi, T))\n|\n{z\n}\nResidual Confounding\n|\n{z\n}\nAdjustment\n] + [h(Xi, 1) −h(Xi, 0)]\n|\n{z\n}\nOutcome Modeling\n]\n(8)\nThe underbraces illustrate how EICAT E resembles a doubly robust estimator.\nWhen the EIC is\nminimized (set to 0) as in equation 8, the ATE is equal to the outcome modeling estimate plus a\ntreatment modeling estimate proportional to the residual error.\n4.3.2\nFrom TMLE to Targeted Regularization\nTargeted Regularization (TarReg) is closely modeled after “Targeted Maxmimum Likelihood Estima-\ntion” (TMLE) (Van der Laan and Rose, 2011). TMLE is an iterative procedure where a nuissance\nparameter ϵ is used to nudge the outcome models towards sharper estimates of the ATE when mini-\nmizing the EIC as in Equation 8.9\n1. Fit h by predicting outcomes (e.g., using TARNet) and minimizing MSE(Y, h(Φ(X), T))\n2. Fit π by predicting treatment (e.g., using logistic regression) and BCE(T, π(Φ(X), T))\n3. Plug-in h and π functions to fit ϵ and estimate h∗(X, T) where,\nh∗(Xi, Ti)\n|\n{z\n}\nY ∗\n= h(Φ(Xi), Φ(Ti))\n|\n{z\n}\nˆY\n+\n\u0012\nTi\nπ(Φ(Xi), 1) −\n1 −Ti\nπ(Φ(Xi), 0)\n\u0013\n|\n{z\n}\nTreatment Modeling Adjustment\n×\nϵ\n|{z}\n“nudge”\nby minimizing MSE(Y, h∗(Φ(X), T)). This is equivalent to minimizing the “Adjustment” part\nin equation 8.\n9For a deeper dive on targeted learning, we recommend (Benkeser and Chambaz, 2020).\n33\n4. Plug-in h∗(X, T) to estimate\nˆ\nATE:\n[\nATET MLE = 1\nN\nN\nX\ni=1\nh∗(Xi, 1)\n|\n{z\n}\nY ∗\ni (1)\n−h∗(Xi, 0)\n|\n{z\n}\nY ∗\ni (0)\nTargeted Regularization takes TMLE and adapts it for a neural network loss function. The main\ndifference is that steps 1 and 2 above are done concurrently by Dragonnet, and that the loss functions\nfor the first three steps are combined into a single loss applied to the whole network at the end of each\nbatch. It requires adding a single free parameter to the Dragonnet network for ϵ.\nAt a very intuitive level, Targeted Regularization is appealing because it introduces a loss function\nto TARNet that explicitly encourages the network to learn the mean of the treatment effect distri-\nbution, and not just the outcome distribution. The Targeted Regularization procedure proceeds as\nfollows:\nIn each epoch:\n1.\n(a) Use Dragonnet to predict h(Φ(X), T) and π(Φ(X), T).\n(b) Calculate the standard ML loss for the network using a hyperparameter α:\narg min\nΦ,π,h\nMSE(Yi, h(Φ(Xi), Ti))\n|\n{z\n}\nOutcome Loss\n+α BCE(Ti, π(Φ(Xi), Ti))\n|\n{z\n}\nπ Loss\n+λ R(h)\n| {z }\nL2\n2.\n(a) Compute h∗(Φ(Xi), Ti) as above,\nh∗(Φ(Xi), Ti)\n|\n{z\n}\nY ∗\n= h(Φ(Xi), Ti)\n|\n{z\n}\nˆ\nYi\n+ (\nTi\nπ(Φ(Xi), 1) −\n1 −Ti\nπ(Φ(Xi), 0))\n|\n{z\n}\nTreatment Modeling Adjustment\n×\nϵ\n|{z}\n“nudge”\n(b) Calculate the targeted regularization loss: MSE(Y, h∗(Φ(X), T))\n3. Combine and minimize the losses from 1 and 2 using a hyperparameter β,\narg min\nΦ,h,ϵ\n= MSE(Y, h(Φ(X), T))\n|\n{z\n}\nOutcome Loss\n+α · BCE(T, π(Φ(X), T))\n|\n{z\n}\nπ Loss\n+λ R(h)\n| {z }\nL2\n+β · MSE(Y, h∗(Φ(X), T))\n|\n{z\n}\nTargeted Regularization Loss\nStep 3 of Targeted Regularization is exactly equivalent to minimizing the EIC up to a constant β.\nAt the end of training, we can thus estimate the targeted regularization estimate of the ATE\n34\nˆ\nATET R as in TMLE:\nˆ\nATET R = 1\nN\nN\nX\ni=1\nh∗(Φ(Xi), 1)\n|\n{z\n}\nY ∗\ni (1)\n−h∗(Φ(Xi), 0)\n|\n{z\n}\nY ∗\ni (0)\nCompared to S-learners, T-learners, and TARNet, the Dragonnet algorithm is particularly attrac-\ntive because of the statistical guarantees afforded by its semiparametric framework. It is doubly robust,\nunbiased, converges at a rate of\n1\n√n, and the sampling distribution is asymptotically normal. Below\nwe describe how to create assymptotically-valid confidence intervals for this estimator.\n5\nConfidence and Interpretation\nIn this section, we move from theory to practice, and treat best practices for building confidence\nintervals and interpreting heterogeneous treatment effects. Both of these topics are active areas of\ndevelopment, not only within the causal inference literature, but across machine learning research.\nHere we specifically focus on recommendations that can be easily implemented by analysts.\n5.1\nAssessing Confidence\n(Tutorial 4\n)\nIn this paper, we feature Dragonnet over other approaches because of its attractive statistical\nproperties. Because the Targeted Regularization procedure in Dragonnet is essentially a variant of\nTMLE, an asymptotically valid standard error can be calculated as the sample corrected variance of\nthe efficient influence curve σ\nˆ\nAT E, where\nσ\nˆ\nAT ET R =\ns\nV ar(EIC\nˆ\nAT ET R)\nN\n(9)\nand,\n35\nV ar(EIC\nˆ\nAT ET R) = V ar[(\nT\nπ(X, 1) −1 −T\nπ(X, 0))(Y −h∗(X, T)) + (h∗(X, 1) −h∗(X, 0)) −\nˆ\nATET R] (10)\n((Van der Laan and Rose, 2011), pp. 96)\nIn Tutorial 5, we show how σ\nˆ\nAT E can be used to calculate a Wald confidence interval for Dragonnet.\nWhile not featured in this review, asymptotically valid conference intervals can also be calculated\nusing RieszNet, a variant of Dragonnet introduced in Chernozhukov et al. (2022) that connects neural\nnetwork estimation to the automatically debiased machine learning literature currently popular in\ncausal econometrics (Chernozhukov et al., 2018, 2021).\n5.2\nInterpretation\n(Tutorial 4\n)\nA lack of interpretability has been a barrier to the adoption of machine learning methods like\nneural networks and random forests in social science settings. However, the literature on post-hoc\ninterpretability techniques has matured considerably over the past five years, and several techniques\nfor identifying important features/covariates such as permutation importance, LIME scores, SHAP\nscores, Individual Conditional Expectation plots etc... are in widespread usage today (Altmann et al.,\n2010; Goldstein et al., 2015; Lundberg and Lee, 2017; Ribeiro et al., 2016). For a broad and accessible\ntreatment on interpreting machine learning models, see Molnar (2022).\nBuilding on criteria used to evaluate other explainable AI methods, Crabb´e et al. (2022) note four\ndesirable properties of a feature importance technique for the interpretation of deep causal estima-\ntors: sensitivity, completeness, linearity, and implementation invariance (Sundararajan et al., 2017). A\nmethod that is ’sensitive’ can distinguish between features that are simply predictive of the outcome,\nand those that actually influence CATE heterogeneity. A method that is ’complete’ identifies all fea-\ntures that, together, explain all effect heterogeneity compared to a baseline. A ’linear’ method is one\nwhere the feature importance scores additively describe the prediction. Lastly, the approach should be\nagnostic to both the model architecture (e.g., TARNet, Dragonnet) and different architectural hyper-\n36\nparameterizations (i.e., invariant to implementation). Of the feature importance methods surveyed,\nthey identify two that manifest all four of these qualities: SHAP scores, and integrated gradients.\nSHAP (SHapley Additive exPlanations) scores have emerged as one of the most popular methods\nfor evaluating machine learning models in recent years (Lundberg and Lee, 2017). SHAP is what is\ncalled a “local” interpretability method: it provides feature importance estimates for each individual\ndatum. Theoretically, SHAP frames feature importance estimation as a cooperative (game-theoretic)\ngame between covariates to predict a specific outcome. Under the hood, the algorithm exhaustively\ncompares all possible “coalitions” of covariates and their ability to predict the outcome (win the game).\nPredictions from this powerset of coalitions are used to calculate the additive marginal contributions\nof each feature in prediction using Shapley values. The disadvantage of SHAP is that, even with\ncomputational tricks, calculating scores for every unit can become computationally intractable in high\ndimensional datasets. SHAP scores are interpreted in comparison to a causal baseline of the ATE.\nBecause of the computational expense of SHAP scores, Crabb´e et al. (2022) also recommend another\nlocal-interpretability method called “Integrated Gradients” (Sundararajan et al., 2017). Intuitively,\nthis algorithm draws a straight-line, linear path in feature space between the target input (individual\nunit) and a baseline (i.e., a hypothetical unit who is exactly average on all covariates). A feature\nimportance score can then be constructed by calculating the gradient in prediction error along this\npath with respect to the feature of interest. Note that SHAP scores can also be understood theoretically\nwithin the path framework. From this perspective, coalitions are paths in which each feature is turned\non sequentially, and the SHAP score is the expectation across these paths. This interpretation leads\nto a gradient-based algorithm for calculating SHAP scores specifically for neural networks, which is\nalso in the SHAP package. In practice, we recommend that analysts experiment with both integrated\ngradients and SHAP scores.\n5.3\nWhat’s in the tutorials?\nTo move from theory to empirics, the online tutorials show how to implement many of the ideas\npresented throughout this primer. The tutorials are hosted in notebooks in the Google Colaboratory\n37\nenvironment. When users open a Colab notebook, Google immediately provides a free virtual machine\nwith standard Python machine learning packages available. This means that readers need not install\nanything on their own computers to experiment with these models. The tutorials are written in the\nPython programming language and provide examples in both Tensorflow2 and Pytorch, the two most\npopular deep learning frameworks. We note that both Tensorflow2 and Pytorch have implementations\nin R. However, we strongly recommend that readers interested in getting into deep learning work in\nPython, which has a much richer ecosystem of third-party packages for machine learning.\nCurrently there are five tutorials:\n•\nTutorial 1 introduces S-learners, and T-learners before TARNet as a way to get familiar\nwith building custom Tensorflow models.\n•\nTutorial 2 focuses on causal inference metrics and hyperparameter optimization. Be-\ncause we do not observe counterfactual outcomes, it’s not obvious how to optimize supervised\nlearning models for causal inference. This tutorial introduces some metrics for evaluating model\nperformance. In the first part, you learn how to assess performance on these metrics in Tensor-\nboard. In the second part, we hack Keras Tuner to do hyperparameter optimization for TARNet,\nand discuss considerations for training models as estimators rather than predictors.\n•\nTutorial 3 highlights the semi-parametric extension to TARNet featured in Shi et al.\n(2019). We add treatment modeling to our TARNet model, and build an augmented inverse\npropensity score estimator. We then briefly describe the algorithm for Targeted Maximum Like-\nlihood Estimation to introduce and build a Dragonnet with Shi et al.’s Targeted Regularization.\n•\nTutorial 4 reimplements Dragonnet in Pytorch and shows how to calculate asymptotically-\nvalid confidence intervals for the average treatment effect. We also interpret the features con-\ntributing to different heterogeneous CATEs using Integrated Gradients and SHAP scores. This\ntutorial is a good tutorial if you also just want to learn how to interpret SHAP scores, indepen-\ndent of the context of causal inference.\n•\nTutorial 5 features the Counterfactual Regression Network (CFRNet) and propensity-\n38\nweighted CFRNet in Shalit et al. (2017); Johansson et al. (2018, 2020) (Appendix A.A). This\napproach relies on integral probability metrics to bound the counterfactual prediction loss and\nforce the treated and control distributions closer together. The weighted variant adds adaptive\npropensity-based weights that provide a consistency guarantee, relax overlap assumptions, and\nideally reduce bias.\n6\nBeyond Traditional Data:\nText, Networks, Images, and\nTreatment over Time\nAs exciting as neural networks are for heterogeneous treatment effect estimation from quantitative\ndata, a great promise of deep causal estimation is inference when treatments, confounders, and media-\ntors are encoded in high-dimensional data (e.g., text, images, social networks, speech, and video) or are\ntime-varying. This is a strong advantage of neural networks over other machine learning approaches,\nwhich do not generalize competitively to non-quantitative data. In these scenarios, multi-task ob-\njectives and tailored architectures can be used to learn representations that are simultaneously rich,\ncapture information about causal quantities, and disentangle their relationships. Moreover, the inher-\nent flexibility of neural networks means that, in many cases, the TARNet-style models presented above\ncan serve as the foundations to inference on text and graphs with some architectural modifications,\nadditional losses, and new identification assumptions.\nThis literature is rapidly evolving, so readers should treat this section of the primer as funda-\nmentally prospective.\nTo maintain accessibility, our primary goal here is to introduce readers to\nhypothetical scenarios where they might perform causal inference on text, network, or image data.\nSecond, we selectively review contemporary, theoretically-motivated literature on deep causal estima-\ntion in these settings. The identification assumptions for different data types differ substantially, so we\ngenerally leave those to the interested reader. Finally, we briefly discuss approaches for dealing with\ntime-varying confounding. We also take this section as an opportunity to introduce the Transformer\nor Graph Neural Network, an architecture now used in most contemporary deep learning models to\nlearn from complex data (Box 8).\n39\n6.1\nCausal Inference from Text\nIn recent years, an interdisciplinary community across both social science and computer science has co-\nalesced around causal inference from text (see Keith et al. (2020) and Feder et al. (2021) for exhaustive\nreviews). Broadly speaking, texts may capture information about any causal quantity (treatments,\noutcomes, confounders, mediators) we might be interested in. For example, in an exit-polling exper-\niment, analysts might want to measure toxicity (Y ) in text responses to political prompts. In an\nobservational study of e-mail response times (Y ), analysts might want to measure the effects of the\ntone of the email (T). In this scenario, the analyst might also want to control for confounders like sub-\nject matter (X). Each of these scenarios presents distinct identification challenges (Feder et al., 2021).\nBut in all cases, we can use low-dimensional representations of the high dimensional text to extract,\nquantify, and disentangle relationships between nuanced qualities like tone and subject matter.\nThe ability of neural networks to automatically extract features makes them particularly suited for\nthe last scenario when both treatment information and confounding covariates are encoded in text.\nIn many cases, we may not have explicitly identified, quantified, or labeled all of the confounders in\ntext (e.g., subject matter and tone of emails), but we would still like to control for them. Pryzant\net al. (2021),Veitch et al. (2020), and Gui and Veitch (2022) address this problem by prepending\nTransformer-layers (Box 8) for reading text to the beginning of TARNet or Dragonnet. Veitch et al.\n(2020) demonstrate the viability of this approach on a Science of Science question testing the causal\neffect of equations on getting papers accepted to computer science conferences. Pryzant et al. (2021);\nGui and Veitch (2022) explore the more complicated scenario not in which the treatment is explicitly\nknown (e.g., equations in papers, gender of authors), but is instead externally perceived upon reading\n(e.g., politeness/rudeness of an email or toxicity of a social media post). In these models, an additional\nloss function is also added for learning text representations concurrently with the causal inference losses\ndiscussed above.\nBox 8: Graph Neural Networks and Transformers\nGraph neural networks (GNNs) are the current state-of-the-art approach for creating\nrepresentations for nodes in graphs. Compared to previous approaches that relied on “shallow”\n40\nembeddings based only on a node’s local context (e.g., random walks to nearby nodes), GNNs\nare attractive because their node representations are aggregated from the structural position\nand covariates of all nodes n degrees away from the target node, where n is the number of\ngraph neural network layers.\nThe most intuitive understanding of how graph neural networks work is as a message passing\nsystem (Gilmer et al., 2017). We use one of the first GNN papers, the Graph Convolutional\nNetwork as an example (Kipf and Welling, 2017).\nIn this interpretation, each node has a\nmessage that it passes to it’s neighbors through a graph convolution operation. In the first layer\nof a GNN this message would consist of the node’s covariates/features. In consecutive layers of\nthe network, these messages are actually representations of the node produced by the previous\nlayer. During graph convolution, each node multiplies incoming messages by it’s own set of\nweights and combines these weighted inputs using an aggregation function (e.g., summation).\nBy the n-th GNN layer, these messages will contain structure and covariate information from\nall nodes n degrees away. For interested readers, there is also a spectral interpretation of this\nprocess. Typically GNNs are trained to produce representations of graphs by predicting the\nprobability that two nodes are linked in the network, and then used for something else. One\nvariant of the GNN uses an “attention” mechanism to vary the extent that nodes value messages\nfrom different neighbors (the graph attention network or GAT) (Veliˇckovi´c et al., 2018).\nAs of 2023, Transformers are the hegemonic architecture used in natural language process-\ning. After their introduction in 2017, these models improved performance on many high-profile\nNLP tasks across the board. Several enterprise-scale transformers have been featured in the\nmedia for their impressive performance in text generation and question answering (e.g. Ope-\nnAI’s GPT-3 and ChatGPT, Google’s Bard). Smaller models in broad use are based on the\nBERT architecture (Devlin et al., 2019).\nThe connection between GNNs, and specifically GATs, is the focus on attention mechanisms.\nFrom this perspective, words in sentences are akin to nodes in networks, with their relative\npositions to each other being analogous to their structural positions in the graph. Transformers\nimproved on previous sequential approaches to text analysis (i.e. recurrent neural networks) by\nhaving each word (or representation of a word) receive messages from not just adjacent words,\nbut all words heterogeneously. Attention mechanisms throughout the architecture allow each\nlayer of a transformer to attend to words or aggregated representation mechanisms heteroge-\nneously. Architectures such as BERT or GPT stack transformer layers to create models with\nhundreds of millions of parameters. These models are expensive to train, both computationally\nand with respect to data, so they are often pretrained on large datasets and then “fine-tuned”\n(lightly re-trained) with smaller datasets for specific tasks or to align with certain goals.\n6.2\nCausal Inference from Networks\nA smaller literature has leveraged relational data for causal inference in two distinct scenarios. In the\nfirst traditional selection on observable settings, we wish to control for information about unobserved\nconfounding inferable from homophilous ties. For example, age or gender might be unmeasured in our\ndata, but we might expect people to develop friendship ties with those of the same gender identity or\n41\nage cohort.\nThis scenario suggests estimation strategies similar to those when confounders are encoded in text.\nMuch like Transformer layers can be prepended to TARNet-style estimators to learn from text, graph\nneural networks (an analog of the Transformer) can be preprended to learn from graphs. Guo et al.\n(2020) provides a first pass at this problem by adding GNN layers to CFRNet Shalit et al. (2017).\nVeitch et al. (2019) instead adapt Dragonnet in a semi-parametric framework to allow for consistent\nestimates of the treatment and outcome, assuming the network representation encodes significant\ninformation about confounders.\nThe second, more challenging scenario is estimating the causal effect of social influence on outcomes\nfrom observational data. For example, Cristali and Veitch (2022) introduce the problem of measuring\nthe effects of vaccination (T) on peer vaccination choice (Y ). This is a hard problem because a) SUTVA\nis a fundamental assumption of all causal inference frameworks and b) it is hard to disentangle whether\nchanges in the outcome result from the treatment via peer effects (e.g, person A pressuring person B to\nvaccinate), or from homophily (e.g., person A and person B having similar political leanings). In other\nwords, contagion and homophily are generically confounded (Shalizi and Thomas, 2011). McFowland\nand Shalizi (2021) are the first to tackle this problem by making strong parametric assumptions about\nthe generation of network ties and the outcome model. Cristali and Veitch (2022) instead propose an\napproach using neural network-learned representations of the graph.\n6.3\nCausal Inference from Images\nWhile ideas from causal inference have been leveraged extensively to improve image classification,\nto our knowledge there are no papers that explore causal inference where treatments, confounders,\nmediators, or predictors are encoded in images.10\nThat being said, some scenarios proposed for\ncausal text analysis should apply here as well.\nFor example, consider the conjoint experiment by\nTodorov et al. (2005) where both the treatment (e.g. incumbency of a politician) and potential latent\nconfounders (e.g., party, age, gender, race) are encoded in an image.\nIn this setting, a TARNet-\n10Jesson et al. (2021) introduce a simulation where the MNIST digit dataset serves as covariates X as toy example of\nhigh-dimensional confounding, but not a possible application.\n42\nlike model adapted to learn and condition on image representations could improve treatment effect\nestimation by controlling for confounders such as the politician’s age. Causal inference on images is\nan area ripe for exploration, and we hope to see more work here in the future.\n6.4\nCausal Inference from Time-varying Data\nOne natural extension of deep causal estimation is to scenarios where treatments are administered\nover time and confounding may be time-varying.\nWhile “g-methods” developed by Robins et al.\nfor estimating effects with time-varying treatments and confounding have existed for decades, the\nstatistical assumptions encoded in these models are quite strong (Robins, 1994; Robins et al., 2000,\n2009). Due to their reliance on generalized linear models to define the “structural” component, they\nassume that the outcome is a linear function of all covariates and treatment. Second, for identification,\nthey make strong assumptions about which previous timesteps confound the current one. Third, they\nrequire different coefficients to be estimated at each time steps. Transformers (Box 8) and recurrent\nneural networks, a simpler model for sequential data (Appendix A.C), should be able to capture long-\nterm dependencies and non-linearities in ways that marginal structural models and g-computation\ncannot.\nSeveral papers have begun to explore these possibilities in the context of personalized medicine. Lim\net al. (2018) build a marginal structural model using a recurrent neural network, and Bica et al. (2020a)\nextend this framework with an additional loss to more explicitly deal with time varying confounding by\nforcing the model to “unlearn” information about the previous time steps. Melnychuk et al. (2022) go\none step further by adapting Bica et al. (2020a)’s approach with a transformer. Inspired by longitudinal\ntargeted maximum likelihood, Frauen et al. (2022) add a semi-parametric targeting layer to their RNN\nto create a g-computation algorithm that is doubly robust and asymptotically efficient. Li et al. (2021)\ninstead propose an RNN framework for g-computation that allows for dynamic treatment regimes. All\nof these papers use simulations of tumor growth dynamics, naturalistic simulations based on vital signs\nfrom intensive care unit visits, or factual datasets exploring treatment response to physical therapy\nfor back pain.\n43\n7\nConclusion: Deep Causal Estimation in Context\nIn this primer we introduce social scientists to the emerging machine learning literature on deep\nlearning for causal inference.\nTo set the stage, we first provide both an intuitive introduction to\nfundamental deep learning concepts like representation and multi-task learning, as well as practical\nguidelines for training neural networks. In the main body of the article, we show how ML researchers\nhave adapted core treatment and outcome modeling strategies to leverage the particular strengths\nof neural networks for heterogeneous treatment effect estimation.\nWe follow with a discussion on\ninference (e.g., model selection, confidence intervals, interpretation), and closed with a prospective\nlook at algorithms for inference from text, social networks, images, and time varying data.\nDeep learning is not the only potential tool for heterogeneous treatment effect inference, and there\nare robust literatures exploring the usage of other methods in both the econometrics and biostatistics\ncommunities (Van der Laan and Rose, 2011; Chernozhukov et al., 2018; Wager and Athey, 2018).\nWhile these literatures are certainly more mature, below we discuss reasons why we think the use gap\nbetween neural networks and other machine learning methods will continue to narrow, a change that\nwe must prepare for.\nFirst, neural networks are better at modeling non-linear heterogeneity (e.g., in treatment responses)\nthan other machine learning methods. In extensive simulations, Curth et al. (2021) found that when\nthe data-generating process for treatment heterogeneity includes exponential relationships, neural\nnetworks outperformed random forests, but tree-based methods are robust when the data-generating\nprocess is built on linear functions. Neural networks were also consistently better at predicting outlier\ntreatment effects than forests. These differences result from how the two methods model functions.\nWhile neural networks can approximate any continuous function with enough neurons, random forests\nmust build non-linear or non-orthogonal decision boundaries using piecewise functions and average\npredictions.\nConsistent with these differences, Curth et al. (2021) also find that neural networks\ndo better when variables are constructed as continuous covariates, and vice versa when they are\ndichotomized.\nFrom a statistical perspective, the rise of semi-parametric and double machine learning frameworks\n44\nhas also narrowed the gap between neural networks and other types of machine learning in terms of the-\noretical guarantees. For example, the TMLE-inspired Dragonnet algorithm featured here is unbiased,\nplausibly consistent, and converges to the target estimand at a fast rate of\n1\n√n. The closely-related\nRiezsnet double machine learning model (not featured) boasts similar guarantees (Chernozhukov et al.,\n2022). Beyond these algorithms, there is a growing adjacent literature of model-agnostic plug-in learn-\ners (e.g., X-learner, R-learner) that can leverage the strengths of neural networks (Nie and Wager,\n2021; K¨unzel et al., 2019).\nThird, folk beliefs about the data-hungriness and uninterpretability of neural networks are over-\nstated. Neural networks are data-hungry when over-parameterized or learning from high-dimensional\ndata like images, but we show in the tutorials that modest-sized, well-regularized neural networks\ncan successfully infer heterogeneous treatment effects in a naturalistic simulation of quantitative data\nwith less than 800 units. In Section 5, we also highlight the considerable progress in machine learning\ninterpretability over the past five years, much of which has been on model-agnostic approaches that\nbenefit all black-box algorithms equally.11\nIn our opinion, the most pressing limitation of current deep learning approaches is the difficulty\nof optimizing neural networks. Theoretically, this stems from a) the complexity of the loss functions\nwhich are often non-convex, and b) the ease of over-parameterizing these models to fit these functions.\nIf neural networks are to be used as statistical estimators, statistical guarantees must be backed by\noptimization guarantees and/or more rigorous methods for model selection.\nOutside of statistical\nestimation, this limitation has largely been addressed through empirical testing on test data and\nstrategic model selection. Within the statistical estimation context, this gap will likely need to be\naddressed by simulation-based sensitivity analyses and, in the short term, comparisons to other model\nfamilies.\nMoreover, there has been a lack of mature tools and empirical applications of these models. A\nmajor goal of this primer, and the tutorials in particular, is to synthesize the theoretical literature,\npractical training and interpretation guidelines, and annotated code so that social scientists in one place\n11Critics often point to out-of-bag feature importances as a particular strength of random forests, but this approach\nhas been shown to be less accurate than model-agnostic permutation importances anyways (Altmann et al., 2010).\n45\ncan start using these models. Deep learning frameworks like Tensorflow and Pytorch are becoming\nmore accessible every year, but we note that canned Python packages like Uber’s causalML exist for\ninterested readers who just want to experiment with a few of these models (Chen et al., 2020).\nDespite current limitations, we believe the future of causal estimation runs through deep learning.\nAs causal inference ventures into new settings, the flexibility of neural networks will become essential\nfor learning from text, graph, image, video, and speech data. For time-varying settings, we believe\nthe ability of neural networks to model non-linearities and long-range temporal dependencies will\nultimately lead to solutions with net weaker assumptions than current approaches. Overall, we are\noptimistic and excited to see where deep causal estimation heads over the next few years.\n8\nAuthor’s Note\nThe accompanying tutorials are available at https://github.com/kochbj/Deep-Learning-for-Causal-\nInference. The tutorials use the IHDP naturalistic simulation introduced in Hill (2011) as an example.\nThe 25 covariates/features for the 747 units (139 treated) in the dataset were taken from an experiment,\nbut Hill simulated the outcomes to create known counterfactuals. The data are available from Fredrik\nJohansson’s website https://www.fredjo.com.\nReferences\nAlaa, Ahmed and Mihaela Van Der Schaar. 2019. “Validating Causal Inference Models via Influence\nFunctions.” In International Conference on Machine Learning, volume 36, pp. 191–201. Association\nfor Computing Machinery.\nAltmann, Andr´e, Laura Tolo¸si, Oliver Sander, and Thomas Lengauer. 2010. “Permutation importance:\na corrected feature importance measure.” Bioinformatics 26:1340–1347.\nArjovsky, Martin, Soumith Chintala, and L´eon Bottou. 2017. “Wasserstein Generative Adversarial\nNetworks.” In International Conference on Machine Learning, volume 34, pp. 214–223. Association\nfor Computing Machinery.\n46\nAtan, Onur, James Jordon, and Mihaela Van Der Schaar. 2018.\n“Deep-Treat: Learning Optimal\nPersonalized Treatments from Observational Data Using Neural Networks.” In Association for the\nAdvancement of Artificial Intelligence Conference on Artificial Intelligence, volume 32, p. 2071–2078.\nAssociation for the Advancement of Artificial Intelligence.\nAthey, Susan and Guido Imbens. 2016.\n“Recursive partitioning for heterogeneous causal effects.”\nProceedings of the National Academy of Sciences 113:7353–7360.\nAustin, Peter C. 2011. “An Introduction to Propensity Score Methods for Reducing the Effects of\nConfounding in Observational Studies.” Multivariate Behavioral Research 46:399–424.\nBengio, Yoshua. 2013.\n“Deep Learning of Representations: Looking Forward.”\nIn International\nConference on Statistical Language and Speech Processing, pp. 1–37. Association for Computing\nMachinery.\nBenkeser, D, M Carone, M J Van Der Laan, and P B Gilbert. 2017. “Doubly robust nonparametric\ninference on the average treatment effect.” Biometrika 104:863–880.\nBenkeser, David and Antoine Chambaz. 2020. “A Ride in Targeted Learning Territory.” Journal de\nla soci´et´e fran¸caise de statistique 161:201–286.\nBica, Ioana, Ahmed M Alaa, James Jordon, and Mihaela van der Schaar. 2020a. “Estimating Coun-\nterfactual Treatment Outcomes Over Time through Adversarially Balanced Representations.” In\nInternational Conference on Learning Representations, volume 37. Association for Computing Ma-\nchinery.\nBica, Ioana, James Jordon, and Mihaela van der Schaar. 2020b. “Estimating the Effects of Continuous-\nvalued Interventions using Generative Adversarial Networks.” In Neural Information Processing\nSystems, volume 33, pp. 16434–16445.\nBrand, Jennie E, Bernard Koch, and Jiahui Xu. 2020. “Machine Learning.” In Sage Research Methods\nFoundations. SAGE.\n47\nChen, Huigang, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. 2020. “CausalML:\nPython Package for Causal Machine Learning.”\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney\nNewey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural\nparameters.” The Econometrics Journal 21:C1–C68.\nChernozhukov, Victor, Whitney Newey, Victor M Quintas-Martinez, and Vasilis Syrgkanis. 2022.\n“Riesznet and forestriesz:\nAutomatic debiased machine learning with neural nets and random\nforests.” In International Conference on Machine Learning, pp. 3901–3914. PMLR.\nChernozhukov, Victor, Whitney K Newey, Victor Quintas-Martinez, and Vasilis Syrgkanis. 2021. “Au-\ntomatic debiased machine learning via neural nets for generalized linear regression.” arXiv preprint\narXiv:2104.14737 .\nCho, Kyunghyun, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations using RNN Encoder–Decoder\nfor Statistical Machine Translation.”\nIn Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1724–1734. Association for Computational Linguistics.\nCrabb´e, Jonathan, Alicia Curth, Ioana Bica, and Mihaela van der Schaar. 2022.\n“Benchmark-\ning heterogeneous treatment effect models through the lens of interpretability.”\narXiv preprint\narXiv:2206.08363 .\nCristali, Irina and Victor Veitch. 2022. “Using Embeddings for Causal Estimation of Peer Influence\nin Social Networks.” ArXiv abs/2205.08033.\nCurth, Alicia, David Svensson, Jim Weatherall, and Mihaela van der Schaar. 2021. “Really Doing\nGreat at Estimating CATE? A Critical Look at ML Benchmarking Practices in Treatment Effect\nEstimation.” In Proceedings of the Neural Information Processing Systems Track on Datasets and\nBenchmarks, edited by J. Vanschoren and S. Yeung, volume 1.\nCuturi, Marco. 2013. “Sinkhorn Distances: Lightspeed Computation of Optimal Transport.” In Neural\nInformation Processing Systems, volume 27, pp. 2292–2300. Curran Associates, Inc.\n48\nCybenko, George. 1989. “Approximation by superpositions of a sigmoidal function.” Mathematics of\nControl, Signals and Systems 5:455.\nDaza, Daniel. 2019. “Approximating Wasserstein Distances with PyTorch.” https://dfdazac.\ngithub.io/sinkhorn.html. Last accessed 2019-08-01.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding.”\nIn Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\npp. 4171–4186. Association for Computational Linguistics.\nDu, Xin, Lei Sun, Wouter Duivesteijn, Alexander Nikolaev, and Mykola Pechenizkiy. 2021. “Adversar-\nial Balancing-based Representation Learning for Causal Effect Inference with Observational Data.”\nData Mining and Knowledge Discovery 35:1713–1738.\nFarrell, Max H, Tengyuan Liang, and Sanjog Misra. 2021. “Deep Neural Networks for Estimation and\nInference.” Econometrica 89:181–213.\nFeder, Amir, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-\nDoughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E Roberts, et al. 2021. “Causal\nInference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond.”\narXiv preprint arXiv:2109.00725 .\nFisher, Aaron and Edward H Kennedy. 2021. “Visually Communicating and Teaching Intuition for\nInfluence Functions.” The American Statistician 75:162–172.\nFrauen, Dennis, Tobias Hatt, Valentyn Melnychuk, and Stefan Feuerriegel. 2022. “Estimating average\ncausal effects from patient trajectories.” arXiv preprint arXiv:2203.01228 .\nGilmer, Justin, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017.\n“Neural message passing for quantum chemistry.” In International Conference on Machine Learning,\nvolume 34, pp. 1263–1272. Association for Computing Machinery.\n49\nGlynn, Adam N and Kevin M Quinn. 2010. “An introduction to the Augmented Inverse Propensity\nWeighted Estimator.” Political Analysis 18:36–56.\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black\nBox: Visualizing Statistical Learning With Plots of Individual Conditional Expectation.” Journal\nof Computational and Graphical Statistics 24:44–65.\nGoldszmidt, Mois´es and Judea Pearl. 1996. “Qualitative probabilities for default reasoning, belief\nrevision, and causal modeling.” Artificial Intelligence 84:57–112.\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press. http:\n//www.deeplearningbook.org.\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In Neural Information\nProcessing Systems, volume 27, pp. 2672–2680. Association for Computing Machinery.\nGretton, Arthur, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola.\n2012. “A Kernel Two-sample Test.” Journal of Machine Learning Research 13:723–773.\nGui, Lin and Victor Veitch. 2022. “Causal Estimation for Text Data with (Apparent) Overlap Viola-\ntions.” arXiv preprint arXiv:2210.00079 .\nGulrajani, Ishaan, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. 2017.\n“Improved Training of Wasserstein GANs.”\nIn International Conference on Neural Information\nProcessing Systems, volume 31, p. 5769–5779. Curran Associates Inc.\nGuo, Ruocheng, Jundong Li, and Huan Liu. 2020. “Counterfactual Evaluation of Treatment Assign-\nment Functions with Networked Observational Data.” In SIAM International Conference on Data\nMining, pp. 271–279. Society for Industrial and Applied Mathematics.\nHastie, T., R. Tibshirani, and J.H. Friedman. 2009.\nThe Elements of Statistical Learning: Data\nMining, Inference, and Prediction. Springer series in statistics. Springer.\n50\nHeck, Katherine E, Paula Braveman, Catherine Cubbin, Gilberto F Ch´avez, and John L Kiely. 2006.\n“Socioeconomic status and breastfeeding initiation among California mothers.” Public health reports\n121:51–59.\nHern´an, Miguel A. and James M. Robins. 2020. Causal Inference: What If. 2020. Chapman & Hall.\nHill, Jennifer L. 2011. “Bayesian Nonparametric Modeling for Causal Inference.” Journal of Compu-\ntational and Graphical Statistics 20:217–240.\nHochreiter, Sepp and J¨urgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation\n9:1735–1780.\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American statistical Asso-\nciation 81:945–960.\nHuszar, Ferenc. 2015. “Another Favourite Machine Learning Paper: Adversarial Networks vs Kernel\nScoring Rules.” Last accessed 2019-08-01.\nImbens, Guido W and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical\nSciences. Cambridge University Press.\nIoffe, Sergey and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training\nby Reducing Internal Covariate Shift.” In International Conference on Machine Learning, volume 32,\npp. 448–456. Association for Computing Machinery.\nJesson, Andrew, S¨oren Mindermann, Yarin Gal, and Uri Shalit. 2021.\n“Quantifying Igno-\nrance in Individual-Level Causal-Effect Estimates under Hidden Confounding.”\narXiv preprint\narXiv:2103.04850 .\nJohansson,\nFredrik\nand\nMax\nShen.\n2018.\n“Causal\nInference\n&\nDeep\nLearning.”\nhttps://github.com/maxwshen/iap-cidl. MIT IAP.\nJohansson, Fredrik D, Nathan Kallus, Uri Shalit, and David Sontag. 2018. “Learning Weighted Rep-\nresentations for Generalization Across Designs.” Unpublished .\n51\nJohansson, Fredrik D., Uri Shalit, Nathan Kallus, and David A. Sontag. 2020. “Generalization Bounds\nand Representation Learning for Estimation of Potential Outcomes and Causal Effects.”\narXiv\nabs/2001.07426.\nJohansson, Fredrik D, Uri Shalit, and David Sontag. 2016. “Learning Representations for Counter-\nfactual Inference.” In International Conference on Machine Learning, volume 48. Association for\nComputing Machinery.\nJoo, Jungseock, Francis F Steen, and Song-Chun Zhu. 2015. “Automated Facial Trait Judgment and\nElection Outcome Prediction: Social Dimensions of Face.” In International Conference on Computer\nVision, pp. 3712–3720. IEEE.\nKallus, Nathan. 2020. “Generalized Optimal Matching Methods for Causal Inference.” Journal of\nMachine Learning Research 21:62–1.\nKeith, Katherine, David Jensen, and Brendan O’Connor. 2020. “Text and Causal Inference: A Review\nof Using Text to Remove Confounding from Causal Estimates.” In Annual Meeting of the Association\nfor Computational Linguistics, volume 58, pp. 5332–5344, Online. Association for Computational\nLinguistics.\nKennedy, Edward H. 2016. “Semiparametric Theory and Empirical Processes in Causal Inference.” In\nStatistical Causal Inferences and their Applications in Public Health Research, pp. 141–167. Springer.\nKennedy, Edward H. 2020. “Towards optimal doubly robust estimation of heterogeneous causal ef-\nfects.”\nKingma, Diederik P. and Jimmy Ba. 2015.\n“Adam: A Method for Stochastic Optimization.”\nIn\nInternational Conference on Learning Representations, volume 3. OpenReview.\nKipf, Thomas N and Max Welling. 2017. “Semi-supervised classification with graph convolutional\nnetworks.” International Conference on Learning Representations 5.\nKramer, Michael S, Frances Aboud, Elena Mironova, Irina Vanilovich, Robert W Platt, Lidia Matush,\nSergei Igumnov, Eric Fombonne, Natalia Bogdanovich, Thierry Ducruet, et al. 2008. “Breastfeeding\n52\nand child cognitive development: new evidence from a large randomized trial.” Archives of general\npsychiatry 65:578–584.\nK¨unzel, S¨oren R, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. “Metalearners for estimating\nheterogeneous treatment effects using machine learning.” Proceedings of the national academy of\nsciences 116:4156–4165.\nLeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. “Deep learning.” Nature 521:436.\nLi, Rui, Stephanie Hu, Mingyu Lu, Yuria Utsumi, Prithwish Chakraborty, Daby M. Sow, Piyush\nMadan, Jun Li, Mohamed Ghalwash, Zach Shahn, and Li-wei Lehman. 2021. “G-Net: a Recurrent\nNetwork Approach to G-Computation for Counterfactual Prediction Under a Dynamic Treatment\nRegime.” In Proceedings of Machine Learning for Health, edited by Subhrajit Roy, Stephen Pfohl,\nEmma Rocheteau, Girmaw Abebe Tadesse, Luis Oala, Fabian Falck, Yuyin Zhou, Liyue Shen,\nGhada Zamzmi, Purity Mugambi, Ayah Zirikly, Matthew B. A. McDermott, and Emily Alsentzer,\nvolume 158 of Proceedings of Machine Learning Research, pp. 282–299. PMLR.\nLim, Bryan, Ahmed M Alaa, and Mihaela van der Schaar. 2018. “Forecasting Treatment Responses\nOver Time Using Recurrent Marginal Structural Networks.”\nIn Neural Information Processing\nSystems, volume 18, pp. 7483–7493. Curran Associates Inc.\nLundberg, Scott M and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In\nAdvances in Neural Information Processing Systems, edited by I. Guyon, U. Von Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, volume 30. Curran Associates, Inc.\nMcFowland, Edward and Cosma Rohilla Shalizi. 2021.\n“Estimating Causal Peer Influence in Ho-\nmophilous Social Networks by Inferring Latent Locations.”\nJournal of the American Statistical\nAssociation 0:1–12.\nMelnychuk, Valentyn, Dennis Frauen, and Stefan Feuerriegel. 2022. “Causal Transformer for Estimat-\ning Counterfactual Outcomes.” In Proceedings of the 39th International Conference on Machine\nLearning, edited by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu,\n53\nand Sivan Sabato, volume 162 of Proceedings of Machine Learning Research, pp. 15293–15329.\nPMLR.\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Rep-\nresentations of Words and Phrases and Their Compositionality.” In Neural Information Processing\nSystems, volume 26, p. 3111–3119. Curran Associates Inc.\nMolnar, C. 2022. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.\nChristoph Molnar.\nM¨uller, Alfred. 1997.\n“Integral Probability Metrics and Their Generating Classes of Functions.”\nAdvances in Applied Probability 29:429–443.\nNagpal, Chirag, Dennis Wei, Bhanukiran Vinzamuri, Monica Shekhar, Sara E. Berger, Subhro Das, and\nKush R. Varshney. 2020. “Interpretable Subgroup Discovery in Treatment Effect Estimation with\nApplication to Opioid Prescribing Guidelines.” In Conference on Health, Inference, and Learning,\np. 19–29. Association for Computing Machinery.\nNaimi, Ashley I, Alan E Mishler, and Edward H Kennedy. 2021. “Challenges in Obtaining Valid\nCausal Effect Estimates With Machine Learning Algorithms.” American Journal of Epidemiology\n192:1536–1544.\nNie, Xinkun and Stefan Wager. 2021. “Quasi-oracle Estimation of Heterogeneous Treatment Effects.”\nBiometrika 108:299–319.\nParikh, Harsh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. 2022.\n“Validating Causal\nInference Methods.” In Proceedings of the 39th International Conference on Machine Learning,\nedited by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan\nSabato, volume 162 of Proceedings of Machine Learning Research, pp. 17346–17358. PMLR.\nPearl, Judea. 2009. Causality. Cambridge University Press.\nPryzant, Reid, Dallas Card, Dan Jurafsky, Victor Veitch, and Dhanya Sridhar. 2021. “Causal Effects of\nLinguistic Properties.” In Conference of the North American Chapter of the Association for Compu-\n54\ntational Linguistics: Human Language Technologies, pp. 4095–4109. Association for Computational\nLinguistics.\nRamey, Craig T, Donna M Bryant, Barbara H Wasik, Joseph J Sparling, Kaye H Fendt, and Lisa M\nLa Vange. 1992. “Infant Health and Development Program for low birth weight, premature infants:\nProgram elements, family participation, and child intelligence.” Pediatrics 89:454–465.\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “” Why should i trust you?” Ex-\nplaining the predictions of any classifier.” In Proceedings of the 22nd ACM SIGKDD international\nconference on knowledge discovery and data mining, pp. 1135–1144.\nRoberts, Daniel A., Sho Yaida, and Boris Hanin. 2022. The Principles of Deep Learning Theory.\nCambridge University Press. https://deeplearningtheory.com.\nRobins, James. 1986. “A New Approach to Causal Inference in Mortality Studies with a Sustained\nExposure Period—Application to Control of the Healthy Worker Survivor Effect.” Mathematical\nModelling 7:1393–1512.\nRobins, James. 1987. “A Graphical Approach to the Identification and Estimation of Causal Pa-\nrameters in Mortality Studies with Sustained Exposure Periods.”\nJournal of Chronic Diseases\n40:139S–161S.\nRobins, James M. 1994. “Correcting for non-compliance in randomized trials using structural nested\nmean models.” Communications in Statistics-Theory and Methods 23:2379–2412.\nRobins, James M, Miguel Angel Hernan, and Babette Brumback. 2000. “Marginal Structural Models\nand Causal Inference in Epidemiology.” Epidemiology .\nRobins, James M, Miguel A Hern´an, G Fitzmaurice, M Davidian, G Verbeke, and G Molenberghs.\n2009. “Longitudinal Data Analysis.” Handbooks of Modern Statistical Methods pp. 553–599.\nRosenbaum, Paul R and Donald B Rubin. 1983. “The Central Role of the Propensity Score in Obser-\nvational Studies for Causal Effects.” Biometrika 70:41–55.\n55\nRubin, Donald B. 1974. “Estimating Causal Effects of Treatments in Randomized and Non-randomized\nStudies.” Journal of Educational Psychology 66:688.\nSchnitzer, Mireille E, Judith J Lok, and Susan Gruber. 2016. “Variable selection for confounder control,\nflexible modeling and collaborative targeted minimum loss-based estimation in causal inference.”\nThe international journal of biostatistics 12:97–115.\nSchwab, Patrick, Lorenz Linhardt, and Walter Karlen. 2018. “Perfect Match: A Simple Method for\nLearning Representations For Counterfactual Inference With Neural Networks.” arXiv:1810.07406v1\n.\nShalit, Uri, Fredrik D Johansson, and David Sontag. 2017. “Estimating Individual Treatment Effect\n: Generalization Bounds and Algorithms.”\nIn International Conference on Machine Learning.\nAssociation for Computing Machinery.\nShalizi, Cosma Rohilla and Andrew C. Thomas. 2011. “Homophily and Contagion Are Generically\nConfounded in Observational Social Network Studies.” Sociological Methods & Research 40:211–239.\nShi, Claudia, David Blei, and Victor Veitch. 2019. “Adapting Neural Networks for the Estimation of\nTreatment Effects.” Nneural Information Processing Systems 32.\nSnoek, Jasper, Hugo Larochelle, and Ryan P Adams. 2012.\n“Practical Bayesian Optimization of\nMachine Learning Algorithms.” In Advances in Neural Information Processing Systems, edited by\nF. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, volume 25. Curran Associates, Inc.\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n2014. “Dropout: a simple way to prevent neural networks from overfitting.” Journal of Machine\nLearning Research 15:1929–1958.\nStock, Michiel. 2017.\n“Notes on optimal transport.”\nhttps://michielstock.github.io/\nOptimalTransport/. Last accessed 2019-08-01.\nStuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.”\nStatistical Science 25:1.\n56\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic attribution for deep networks.”\nIn International conference on machine learning, pp. 3319–3328. PMLR.\nTodorov, Alexander, Anesu N Mandisodza, Amir Goren, and Crystal C Hall. 2005. “Inferences of\nCompetence from Faces Predict Election Outcomes.” Science 308:1623–1626.\nVan der Laan, Mark J and Sherri Rose. 2011. Targeted Learning: Causal Inference for Observational\nand Experimental Data. Springer Science & Business Media.\nVeitch, Victor, Dhanya Sridhar, and David Blei. 2020. “Adapting Text Embeddings for Causal Infer-\nence.” In Conference on Uncertainty in Artificial Intelligence, pp. 919–928. Association for Uncer-\ntainty in Artificial Intelligence.\nVeitch, Victor, Yixin Wang, and David Blei. 2019. “Using Embeddings to Correct for Unobserved\nConfounding in Networks.” Neural Information Processing Systems 32.\nVeliˇckovi´c, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Ben-\ngio. 2018. “Graph Attention Networks.” In International Conference for Learning Representations,\nvolume 6. OpenReview.\nWager, Stefan and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects\nusing Random Forests.” Journal of the American Statistical Association 113:1228–1242.\nYao, Liuyi, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2018. “Representa-\ntion Learning for Treatment Effect Estimation from Observational Data.” In Neural Information\nProcessing Systems. Curran Associates, Inc.\nYoon, Jinsung, James Jordon, and Mihaela Van Der Schaar. 2018. “GANITE: Estimation of Indi-\nvidualized Treatment Effects using Generative Adversarial Nets.” In International Conference on\nLearning Representations. OpenReview.\nZhang, Yao, Alexis Bellot, and Mihaela Schaar. 2020. “Learning Overlapping Representations for\nthe Estimation of Individualized Treatment Effects.”\nIn International Conference on Artificial\nIntelligence and Statistics, pp. 1005–1014. Association for Artificial Intelligence and Statistics.\n57\nZivich, Paul N and Alexander Breskin. 2021. “Machine learning for causal inference: on the use of\ncross-fit estimators.” Epidemiology (Cambridge, Mass.) 32:393.\nAppendix A\nBalancing Using Integral Probability Metrics\nA.1\nWasserstein Distance\nFollowing (Stock, 2017; Daza, 2019), suppose we have two discrete distributions (treated and control)\nwith marginal densities p(x) and q(x) captured as vectors t and c, with dimensions n and m respectively.\nTo compute the Wasserstein distance, we must define a ”mapping matrix” P that defines the mapping\nof “earth” in p(x) to corresponding piles in q(x). Let U(t, c) be the set of positive, n × m mapping\nmatrices where the sum of the rows is t and the sum of the columns is c.\nU(t, c) = P ⊂Rnxm\n>0 |P · 1m = t, P T · 1n = c\n(11)\nIn words, this matrix maps the probability mass from points in the support of p(x) (i.e, the elements\nof t) to points in the support of q(x) (the elements of c) (note that the mapping need not be one-to-\none). We also have a “cost” matrix C that describes the cost of applying P (i.e. the cost of shoveling\ndirt according to the map described in P). The cost matrix can be computed using a norm ℓ(most\ncommonly ℓ2) between the points in t being mapped to c in the mapping matrix P. Finally, the ℓ-norm\nWasserstein distance dWℓcan be defined as\ndWℓ= minP ⊂(t,c)\nX\ni,j\nPijCij\n(12)\nIn other words, the Wasserstein distance is the smallest Frobenius inner product of a mapping matrix\nP that fits the above constraints, and its associated cost matrix C. Although this problem can be\nsolved via linear programming, the Wasserstein distance is often implemented in a different form that\nworks with continuous distributions and can be optimized by gradient descent (Arjovsky et al., 2017;\nGulrajani et al., 2017). There is also a variant of the Wasserstein distance that imposes an entropy-\n58\nbased regularization on the coupling matrix to make it smoother or sparser called the Sinkhorn distance\n(Cuturi, 2013).\nA.2\nExtending Representation Balancing with IPMs\nDeep Dive: CFRNet (Shalit et al. (2017); Johansson et al. (2018, 2020))\nBeyond receiving outcome modeling gradients for both potential outcomes, the authors have sub-\nsequently extended TARNet with additional losses that explicitly encourage balancing by minimizing\na statistical distance between the two covariate distributions in representation space. These distances\nare called integral probability metrics (M¨uller, 1997).12 Johansson et al. (2016); Shalit et al. (2017);\nJohansson et al. (2018) propose two possible IPMs, the Wasserstein distance and the maximum mean\ndiscrepancy distance (MMD) for use in these architectures.\nThe Wasserstein or “Earth Mover’s” distance fits an interpretable “map” (i.e. a matrix) showing\nhow to efficiently convert from one probability mass distribution to another. The Wasserstein distance\nis most easily understood as an optimal transport problem (i.e., a scenario where we want to transport\none distribution to another at minimum cost). The nickname “Earth mover’s distance” comes from\nthe metaphor of shoveling dirt to terraform one landscape into another. In the idealized case in which\none distribution can be perfectly transformed into another, the Wasserstein map corresponds exactly\nto a perfect one-to-one matching on covariates strategy (Kallus, 2020).\nThe MMD is the normed distance between the means of two distributions, after a kernel function\nϕ has transformed them into a high-dimensional space called a reproducing kernel Hibbert Space\n(RKHS) (Gretton et al., 2012). The MMD with an L2 norm in RKHS H can be specified as:\nMMD(P, Q) = ||EX∼P ϕ(X) −EX∼Qϕ(X)||2\nH\n(13)\nThe metric is built on the idea that there is no function that would have differing Expected Values\nfor P and Q in this high-dimensional space if P and Q are the same distribution (Huszar, 2015). The\n12Zhang et al. (2020) criticize the usage of IPMs because they make no restrictions on the moments of the transformed\ndistributions. Thus while the covariate distributions may have a high percentage of overlap in representation space, this\noverlap may be substantially biased in unknown ways.\n59\nMMD is inexpensive to calculate using the “kernel trick” where the inner product between two points\ncan be calculated in the RKHS without first transforming each point into the RKHS.13\nWhen an IPM loss is applied to the representation layers in TARNet, the authors call the resulting\nnetwork “CounterFactual Regression Network” (CFRNet) (Fig. 6A) (Shalit et al., 2017). The loss\nfunction for this network is\nmin\nh,Φ,IP M\n1\nN\nN\nX\ni=1\nMSE(Yi, h(Φ(Xi), Ti))\n|\n{z\n}\nOutcome Loss\n+λ IPM(Φ(X|T = 1), Φ(X|T = 0))\n|\n{z\n}\nDist. b/w T & C covar. distributions\n+α R(h)\n| {z }\nL2\n(14)\nwhere R(h) is a model complexity term and λ and α are hyperparameters.\nThese two papers also make important theoretical contributions by providing bounds on the gener-\nalization error for the PEHE (Hill, 2011). In Shalit et al. (2017), they show that the PEHE is bounded\nby the sum of the factual loss, counterfactual loss, and the variance of the conditional outcome.\nIn Johansson et al. (2020), the authors introduce estimated IPW weights π(Φ(X), T) to CFRNet\nthat are used within the IPM calculation to provide consistency guarantees (Fig. 6B). Theoretically,\nthey also use these weights to relax the overlap assumption as long as the weights themselves obey the\npositivity assumption. From a practical standpoint, adding weights that are optimized smoothly across\nthe whole dataset each epoch reduces noise created by calculating the IPM score in small batches.\nWeighted CFRNet minimizes the following loss function:\narg min\nh,Φ,IP M,π,λh,λw\n1\nN\nN\nX\ni=1\nˆP(Ti)\nπ(Φ(Xi), Ti)\n|\n{z\n}\nIP W\n· MSE(Yi, h(Φ(Xi), Ti))\n|\n{z\n}\nOutcome Loss\n+λh\nR(h)\n| {z }\nL2 Outcome\n+\nα · IPM(\nˆP(1)\nπ(Φ(X, 1))\n|\n{z\n}\nIP W\n·Φ(X|T = 1),\nˆP(0)\nπ(Φ(X, 0))\n|\n{z\n}\nIP W\n·Φ(X|T = 0))\n|\n{z\n}\nDistance between IP W weighted T & C covar. distributions\n+λπ\n||π||2\nN\n| {z }\nL2V ar(π)\n(15)\nwhere R(h) is a model complexity term and λh, λπ and α are hyperparameters. The final term is a\n13This kernel trick is also what makes support vector machines computationally tractable.\n60\nFigure 6: A. CFRNet architecture originally introduced in Shalit et al. (2017). CFRNet adds an\nadditional integral probability metric (IPM) loss to TARNet to explicitly force representations of the\ntreated and control covariates closer in representation space.\nB. Weighted CFRNet adds a propensity score head to CFRNet to predict IPW-weighted outcomes.\nDuring training, the propensity score is used to reweight both the predicted outcomes ˆY (0) and ˆY (1),\nas well as the represented covariate distributions in calculation of the IPM loss. This allows the authors\nto provide consistency guarantees and relax the overlap assumption. Figures adapted from Johansson\net al. (2020).\n61\nregularization term on the variance of the weight parameters.\nA.2.1\nExtending Representation Balancing with Matching\nBeyond IPMs, other approaches have directly embraced matching as a balancing strategy. Yao et al.\n(2018) train their TARNet on six point mini-batches of propensity score-matched units with additional\nreconstruction losses designed to preserve the relative distances between these points when projecting\nthem into representation space. Schwab et al. (2018) takes an even simpler approach by feeding random\nbatches of propensity-matched units to the TarNet outcome structure.\nAppendix B\nModel Selection Using the PEHE\nIn order to select hyperparameters in real data, Johansson et al. (2020) propose to use a matching vari-\nant of PEHE with the nearest Euclidean neighbor of each unit i from the other treatment assignment\ngroup ynn\ni\nas a counterfactual. If we identify the nearest neighbor j of each unit i in representation\nspace such that tj ̸= ti as\nynn\ni (1 −ti) =\nmin\nj∈(1−T ) ||Φ(xi|ti) −Φ(xj|1 −ti)||2\nthen,\nPEHEnn = 1\nN\nN\nX\ni=1\n((1 −2ti)(yi(ti) −ynn\ni (1 −ti)\n|\n{z\n}\nCAT Enn\n−(h(Φ(x), 1) −h(Φ(x), 0)))\n|\n{z\n}\nˆ\nCAT E\n2\nIf we take the square root of the PEHEnn then we get an approximation of the unit-level error.\nThe intuition behind √PEHEnn is solid. If our representation function Φ is truly learning to\nbalance the treated and control distributions, CATEnn should coarsely measure it.\n62\nFigure 7: Recurrent neural network.\nAppendix C\nRecurrent Neural Networks (RNN)\nRecurrent neural networks are a specialized architecture created for learning outcomes from sequential\ndata (e.g.\ntime series, biological sequences, text) (Fig.\n7).\nIn a classic RNN, each “unit” u in\nthe network takes as input its own covariates X (or possibly a representation) and a representation\nproduced by the previous unit, encoding cumulative information about earlier states in the sequence.\nThese units are not just simple hidden layers: there is a set of weights within each unit for its raw\ninputs, the representation from the previous time step, and its outputs.\nDifferent RNN variants\nhave different operations for integrating past representations with present inputs. Recurrent neural\nnetworks may be directed acyclic graphs or feedback on themselves. Commonly used variants include\nGated Recurrent Unit networks (GRU) and Long-term Short-term memory networks (LSTM) (Cho\net al., 2014; Hochreiter and Schmidhuber, 1997).\n63\nAppendix D\nGenerative Modeling through Adversarial Train-\ning\nAdversarial training approaches include a wide variety of architectures where two networks or loss\nfunctions compete against each other. Adversarial approaches are inspired by Generative Adversarial\nNetworks (GANs) (Box 9) (Goodfellow et al., 2014). In the machine learning literature on causal\ninference, adversarial training has been applied both to trade off outcome modeling and treatment\nmodeling tasks during representation learning, as well as to trade off estimation and regularization\nof IPW weights.\nGANs have also been used directly as generative models for counterfactual and\ntreatment effect distributions.\nBox 9: Generative Adversarial Networks (GAN)\nIn GANs, two networks, a discriminator network D and a generator network G, play a\nzero-sum game like cops and robbers. The generator network’s job is to learn a distribution\nfrom which the training data X could have credibly been generated. In each training batch, the\ngenerator produces a new outcome (originally images, but could be IPW weights, counterfactu-\nals or treatment effects) by drawing a random noise sample from a known distribution Z (e.g.\nGaussian) and transforming it into outcomes with the function G(Z) = ˆX. The discriminator’s\njob is to learn a function D(X) = P(X is real) that can distinguish whether the outcome is\nfrom the training data X, or whether it is a “fake” ˆX created by the generator. The generator\nthen receives a negative version of the discriminator’s loss, a penalty that is proportional to\nhow well it was able to “deceive” the discriminator. The discriminator’s loss can be the log\nloss, Jensen-Shannon divergence (Goodfellow et al., 2014), the Wasserstein distance (Arjovsky\net al., 2017; Gulrajani et al., 2017), or any number of divergences and IPMs. Formally, the\ngenerator attempts to minimize the following loss function,\narg min\nG\n=\nEX\n|{z}\nreal dist.\n[ L(D(X)\n|\n{z\n}\nP (X is real)\n] +\nEZ\n|{z}\nfake dist.\n[1 −L(D(G(Z))\n|\n{z\n}\nP ( ˆ\nXis real)\n]\nwhere the first quantity is the discriminator’s estimated probability data from X is indeed real,\nand the second quantity is the discriminator’s estimate that a generated quantity from the\ndistribution Z is real.\nBecause the discriminator is trying to catch the generator, its objective is to maximize the\nsame function,\narg max\nD\n=\nEX\n|{z}\nreal dist.\n[ L(D(X)\n|\n{z\n}\nP (Xis real)\n] +\nEZ\n|{z}\nfake dist.\n[1 −L(D(G(Z))\n|\n{z\n}\nP ( ˆ\nXis real)\n]\n64\nIn practice, the discriminator and the generator are trained either alternatingly or simultane-\nously, with the discriminator increasing its ability to discern between real and fake outcomes\nover time, and the generator increasing its ability to deceive the discriminator. The idea is that\nthe adaptive loss function created by the discriminator can coax the generator out of local min-\nima to generate superior outcomes. Results by these models have been impressive, and many\nof the fake portraits and “deepfake” videos circulating online in recent years are generated by\nthis architectures. The advantage of GANs is that they can impressively learn very complex\ngenerative distributions with limited modeling assumptions. The disadvantage of GANs is that\nthey are difficult and unreliable to train, often plateauing in local optima.\nD.1\nGANs as Generative Models of Treatment Effect Distributions (GAN-\nITE)\nDeep Dive: GANITE (Yoon et al. (2018)) Although a generative model of the treatment effect\ndistribution is generally unknown, a natural application of GANs is to try to machine learn this\ndistribution from data. GANITE uses two GANs: GAN1, consisting of generator G1 and discriminator\nD1, to model the counterfactual distribution and GAN2, consisting of generator G2 and discriminator\nD2, to model the CATE distribution (Yoon et al., 2018) (Fig. 8). The training procedure for GAN1\nis as follows:\n1. Taking X,T, and generative noise Z as input, generator G1 generates both potential outcomes\n{ ˜Y (T), ˜Y (1 −T)}. A factual loss MSE(Y (T), ˜Y (T)) is applied.\n2. Create a new vector C = {Y (T), ˜Y (1 −T)} by combining the observed potential outcome and\nthe counterfactual predicted by G1.\n3. Taking X and C as inputs, the discriminator D1 rates each value in C for the probability that\nit is the observed outcome using the categorical cross entropy loss:\nL(D1) = CCE({ P(C0 = Y (T))\n|\n{z\n}\nProb first idx is real\n, P(C1 = Y (T))\n|\n{z\n}\nProb sec idx is real\n}, {C0 == Y (T)\n|\n{z\n}\n1 if idx 0 is real\n, C1 == Y (T)\n|\n{z\n}\n1 if idx 1 is real\n})\n(16)\n4. This loss is then fed back to G1 such that the total loss for the generator is now\narg min\nG1\n= MSE(Y (T), ˜Y (T)) −λL(D1)\n(17)\nAfter generator G1 is trained to completion, the authors use C as a “complete dataset” containing\nboth a factual outcome and a counterfactual outcome to train GAN2, which generates treatment\n65\neffects:\n1. Taking only X and generative noise Z as input, G2 generates a new potential outcome vector\nR = { ˆY (T), ˆY (1−T)}. G2 receives an MSE loss to minimize the difference between its predictions\nand the “complete dataset” C: MSE(C, R).\n2. Discriminator D2 takes X, C, and R as inputs and estimates a probability that C is the “com-\nplete” dataset, and that R is the “complete dataset”:\nL(D2) = CCE({\nP(C = C)\n|\n{z\n}\nProb C is “CD”\n, P(R = C)\n|\n{z\n}\nProb R is “CD”\n}, { C == C\n|\n{z\n}\n1 if idx 0 is C\n, C1 == Y (T)\n|\n{z\n}\n1 if idx 1 is C\n})\n(18)\n3. This loss is then fed back to the generator G2 such that the total loss for the generator is now\narg min\nG2\n= MSE(C, R) −λL(D2)\n(19)\nAt the end of training, G2 should be able to predict treatment effects with only covariates X and noise\nZ as inputs. An evolution of GANITE, SCIGAN, extends this framework to settings with more than\none treatment and continuous dosages (Bica et al., 2020b).\n66\nFigure 8: GANITE has two GANs. The first generator G1 generates counterfactuals ˜Y (T). The\ndiscriminator D1 attempts to discriminate between these predictions and real data (Y (T)).\nThe\nsecond generator proposes pairs of potential outcomes ˆY (0) and ˆY (1) (i.e., treatment effects), a vector\nwe call R. Discriminator D2 attempts to discern between R and a “complete dataset” C created by\npairing each observed/factual outcome Y (T) with a synthetic outcome ˜Y (1 −T) proposed by G1.\nAlthough we do not show gradients in other figures, we make an exception for GANs (solid red line).\n67\nD.2\nAdversarial Representation Balancing\nThe use of the IPM loss in CFRNet (Shalit et al., 2017) may also be viewed as an adversarial ap-\nproach in that the representation layers are forced to maximize performance on two competing tasks:\npredicting outcomes and minimizing an IPM. Rather than using an IPM loss, other authors have\ntrained propensity score estimators that send positive (rather than negative) gradients back to the\nrepresentation layers (Atan et al., 2018; Du et al., 2021).\nBica et al. (2020a) extend this approach to settings with treatment over time using a recurrent\nneural network. In their medical setting, decorrelating treatment from patient covariates and history\nallows them to estimate treatment effects at each individual snapshot.\n68\n",
  "categories": [
    "cs.LG",
    "econ.EM",
    "stat.ME",
    "stat.ML"
  ],
  "published": "2021-10-09",
  "updated": "2023-11-28"
}