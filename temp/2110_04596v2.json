{
  "id": "http://arxiv.org/abs/2110.04596v2",
  "title": "Deep Long-Tailed Learning: A Survey",
  "authors": [
    "Yifan Zhang",
    "Bingyi Kang",
    "Bryan Hooi",
    "Shuicheng Yan",
    "Jiashi Feng"
  ],
  "abstract": "Deep long-tailed learning, one of the most challenging problems in visual\nrecognition, aims to train well-performing deep models from a large number of\nimages that follow a long-tailed class distribution. In the last decade, deep\nlearning has emerged as a powerful recognition model for learning high-quality\nimage representations and has led to remarkable breakthroughs in generic visual\nrecognition. However, long-tailed class imbalance, a common problem in\npractical visual recognition tasks, often limits the practicality of deep\nnetwork based recognition models in real-world applications, since they can be\neasily biased towards dominant classes and perform poorly on tail classes. To\naddress this problem, a large number of studies have been conducted in recent\nyears, making promising progress in the field of deep long-tailed learning.\nConsidering the rapid evolution of this field, this paper aims to provide a\ncomprehensive survey on recent advances in deep long-tailed learning. To be\nspecific, we group existing deep long-tailed learning studies into three main\ncategories (i.e., class re-balancing, information augmentation and module\nimprovement), and review these methods following this taxonomy in detail.\nAfterward, we empirically analyze several state-of-the-art methods by\nevaluating to what extent they address the issue of class imbalance via a newly\nproposed evaluation metric, i.e., relative accuracy. We conclude the survey by\nhighlighting important applications of deep long-tailed learning and\nidentifying several promising directions for future research.",
  "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nDeep Long-Tailed Learning: A Survey\nYifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, Fellow, IEEE, and Jiashi Feng\nAbstract—Deep long-tailed learning, one of the most challenging problems in visual recognition, aims to train well-performing deep\nmodels from a large number of images that follow a long-tailed class distribution. In the last decade, deep learning has emerged as a\npowerful recognition model for learning high-quality image representations and has led to remarkable breakthroughs in generic visual\nrecognition. However, long-tailed class imbalance, a common problem in practical visual recognition tasks, often limits the practicality of\ndeep network based recognition models in real-world applications, since they can be easily biased towards dominant classes and perform\npoorly on tail classes. To address this problem, a large number of studies have been conducted in recent years, making promising\nprogress in the ﬁeld of deep long-tailed learning. Considering the rapid evolution of this ﬁeld, this paper aims to provide a comprehensive\nsurvey on recent advances in deep long-tailed learning. To be speciﬁc, we group existing deep long-tailed learning studies into three main\ncategories (i.e., class re-balancing, information augmentation and module improvement), and review these methods following this\ntaxonomy in detail. Afterward, we empirically analyze several state-of-the-art methods by evaluating to what extent they address the issue\nof class imbalance via a newly proposed evaluation metric, i.e., relative accuracy. We conclude the survey by highlighting important\napplications of deep long-tailed learning and identifying several promising directions for future research.\nIndex Terms—Long-tailed Learning, Deep Learning, Imbalanced Learning\n!\n1\nINTRODUCTION\nD\nEEP learning allows computational models, composed of\nmultiple processing layers, to learn data representations with\nmultiple levels of abstraction [1], [2] and has made incredible\nprogress in computer vision [3], [4], [5], [6], [7], [8]. The key\nenablers of deep learning are the availability of large-scale datasets,\nthe emergence of GPUs, and the advancement of deep network\narchitectures [9]. Thanks to the strong ability of learning high-\nquality data representations, deep neural networks have been\napplied with great success to many visual discriminative tasks,\nincluding image classiﬁcation [6], [10], object detection [7], [11]\nand semantic segmentation [8], [12].\nIn real-world applications, training samples typically exhibit a\nlong-tailed class distribution, where a small portion of classes have\na massive number of sample points but the others are associated\nwith only a few samples [13], [14], [15], [16]. Such class imbalance\nof training sample numbers, however, makes the training of deep\nnetwork based recognition models very challenging. As shown in\nFig. 1, the trained model can be easily biased towards head classes\nwith massive training data, leading to poor model performance\non tail classes that have limited data [17], [18], [19]. Therefore,\nthe deep models trained by the common practice of empirical risk\nminimization [20] cannot handle real-world applications with long-\ntailed class imbalance, e.g., face recognition [21], [22], species\nclassiﬁcation [23], [24], medical image diagnosis [25], urban scene\nunderstanding [26] and unmanned aerial vehicle detection [27].\nTo address long-tailed class imbalance, massive deep long-\ntailed learning studies have been conducted in recent years [15],\n[16], [28], [29], [30]. Despite the rapid evolution in this ﬁeld, there\nis still no systematic study to review and discuss existing progress.\nTo ﬁll this gap, we aim to provide a comprehensive survey for\n•\nY. Zhang and B. Hooi are with School of Computing, National University\nof Singapore. E-mail: yifan.zhang@u.nus.edu, dcsbhk@nus.edu.sg.\n•\nB.\nKang\nand\nJ.\nFeng\nare\nwith\nByteDance\nAI\nLab.\nE-mail:\nbingykang@gmail.com, jshfeng@bytedance.com.\n•\nS. Yan is with SEA AI Lab. E-mail: yansc@sea.com.\nNumber of training samples\nSorted class index\nTail\nHead\nDecision\nboundary\nFig. 1. The label distribution of a long-tailed dataset (e.g., the iNaturalist\nspecies dataset [23] with more than 8,000 classes). The head-class\nfeature space learned on these sampled is often larger than tail classes,\nwhile the decision boundary is usually biased towards dominant classes.\nrecent long-tailed learning studies conducted before mid-2021.\nAs shown in Fig. 2, we group existing methods into three main\ncategories based on their main technical contributions, i.e., class\nre-balancing, information augmentation and module improvement;\nthese categories can be further classiﬁed into nine sub-categories:\nre-sampling, class-sensitive learning, logit adjustment, transfer\nlearning, data augmentation, representation learning, classiﬁer\ndesign, decoupled training and ensemble learning. According to this\ntaxonomy, we provide a comprehensive review of existing methods,\nand also empirically analyze several state-of-the-art methods by\nevaluating their abilities of handling class imbalance using a new\nevaluation metric, namely relative accuracy. We conclude the\nsurvey by introducing several real-world application scenarios of\ndeep long-tailed learning and identifying several promising research\ndirections that can be explored by the community in the future.\narXiv:2110.04596v2  [cs.CV]  15 Apr 2023\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nLong-tailed \nLearning\nClass \nRe-balancing \n(Sec. 3.1)\nInformation \nAugmentation\n(Sec. 3.2)\nModule\nImprovement\n(Sec. 3.3)\nRe-sampling (Sec. 3.1.1)\nClass-sensitive Learning (Sec. 3.1.2)\nLogit Adjustment (Sec. 3.1.3)\nTransfer Learning (Sec. 3.2.1)\nData Augmentation (Sec. 3.2.2)\nRepresentation Learning (Sec. 3.3.1)\nClassifier Design (Sec. 3.3.2)\nEnsemble Learning (Sec. 3.3.4)\nDecoupled Training (Sec. 3.3.3)\nFig. 2. Taxonomy of existing deep long-tailed learning methods.\nWe summarize the key contributions of this survey as follows.\n•\nTo the best of our knowledge, this is the ﬁrst comprehensive\nsurvey of deep long-tailed learning, which will provide a\nbetter understanding of long-tailed visual learning with\ndeep neural networks for researchers and the community.\n•\nWe provide an in-depth review of advanced long-tailed\nlearning studies, and empirically study state-of-the-art\nmethods by evaluating to what extent they handle long-\ntailed class imbalance via a new relative accuracy metric.\n•\nWe identify four potential directions for method innovation\nas well as eight new deep long-tailed learning task settings\nfor future research.\nThe rest of this survey will be organized as follows: Section 2\npresents the problem deﬁnition and introduces widely-used datasets,\nmetrics and applications. Section 3 provides a comprehensive\nreview of advanced long-tailed learning methods and Section 4\nempirically analyzes several state-of-the-art methods based on\na new evaluation metric. Section 5 identiﬁes future research\ndirections. We conclude the survey in Section 6.\n2\nPROBLEM OVERVIEW\n2.1\nProblem Deﬁnition\nDeep long-tailed learning seeks to learn a deep neural network\nmodel from a training dataset with a long-tailed class distribution,\nwhere a small fraction of classes have a massive number of samples,\nand the rest of the classes are associated with only a few samples\n(c.f. Fig. 1). Let {xi, yi}n\ni=1 be the long-tailed training set, where\neach sample xi has a corresponding class label yi. The total\nnumber of training set over K classes is n = PK\nk=1 nk, where\nnk denotes the data number of class k; let π denote the vector of\nlabel frequencies, where πk = nk/n indicates the label frequency\nof class k. Without loss of generality, a common assumption in\nlong-tailed learning [31], [32] is that the classes are sorted by\ncardinality in decreasing order (i.e., if i1 < i2, then ni1 ≥ni2,\nand n1 ≫nK), and then the imbalance ratio is deﬁned as n1/nK.\nThis task is challenging due to two difﬁculties: (1) imbalanced\ndata numbers across classes make deep models biased to head\nclasses and performs poorly on tail classes; (2) lack of tail-class\nsamples makes it further challenging to train models for tail-class\nclassiﬁcation. Such a task is fundamental and may occur in various\nvisual recognition tasks, such as image classiﬁcation [15], [32],\ndetection [19], [33] and segmentation [26], [34], [35].\nTABLE 1\nStatistics of long-tailed datasets. “Cls.” indicates image classiﬁcation;\n“Det.” represents object detection; “Seg.” means instance segmentation.\nTask\nDataset\n# classes\n# training data\n# test data\nCls.\nImageNet-LT [15]\n1,000\n115,846\n50,000\nCIFAR100-LT [18]\n100\n50,000\n10,000\nPlaces-LT [15]\n365\n62,500\n36,500\niNaturalist 2018 [23]\n8,142\n437,513\n24,426\nDet./Seg.\nLVIS v0.5 [36]\n1,230\n57,000\n20,000\nLVIS v1 [36]\n1,203\n100,000\n19,800\nMulti-label Cls.\nVOC-LT [37]\n20\n1,142\n4,952\nCOCO-LT [37]\n80\n1,909\n5,000\nVideo Cls.\nVideoLT [38]\n1,004\n179,352\n51,244\n2.2\nDatasets\nIn recent years, a variety of visual datasets have been released for\nlong-tailed learning, differing in tasks, class numbers and sample\nnumbers. In Table 1, we summarize nine visual datasets that are\nwidely used in the deep long-tailed learning community.\nIn long-tailed image classiﬁcation, there are four benchmark\ndatasets: ImageNet-LT [15], CIFAR100-LT [18], Places-LT [15],\nand iNaturalist 2018 [23]. The previous three are sampled from\nImageNet [39], CIFAR100 [40] and Places365 [41] following\nPareto distributions, respectively, while iNaturalist is a real-world\nlong-tailed dataset. The imbalance ratio of ImageNet-LT, Places-LT\nand iNaturalist are 256, 996 and 500, respectively; CIFAR100-LT\nhas three variants with various imbalance ratios {10, 50, 100}.\nIn long-tailed object detection and instance segmentation,\nLVIS [36], providing precise bounding box and mask annotations,\nis the widely-used benchmark. In multi-label image classiﬁcation,\nthe benchmarks are VOC-LT [37] and COCO-LT [37], which are\nsampled from PASCAL VOC 2012 [42] and COCO [43], respec-\ntively. Recently, a large-scale “untrimmed” video dataset, namely\nVideoLT [38], was released for long-tailed video recognition.\n2.3\nEvaluation Metrics\nLong-tailed learning seeks to train a well-performing model on the\ndata with long-tailed class imbalance. To evaluate how well class\nimbalance is resolved, the model performance on all classes and\nthe performance on class subsets (i.e., head, middle and tail classes)\nare usually reported. Note that the evaluation metrics should treat\neach class equally. Following this principle, top-1 accuracy or\nerror rate is often used for balanced test sets, where every test\nsample is equally important. When the test set is not balanced,\nmean Average Precision (mAP) or macro accuracy is often adopted\nsince the two metrics treat each class equally. For example, in\nprevious studies, top-1 accuracy or error rate was widely used for\nlong-tailed image classiﬁcation, in which the test set is usually\nassumed to be near-balanced. Meanwhile, mAP was adopted for\nlong-tailed object detection, instance segmentation and multi-label\nimage classiﬁcation, where the test set is usually not balanced.\n2.4\nApplications\nThe main applications of deep long-tailed learning include image\nclassiﬁcation, detection segmentation, and visual relation learning.\nImage Classiﬁcation. The most common applications of\nlong-tailed learning are multi-class classiﬁcation [15], [32], [44],\n[45] and multi-label classiﬁcation [37], [46]. As mentioned in\nSection 2.2, there are many artiﬁcially sampled long-tailed datasets\nfrom widely-used multi-class classiﬁcation datasets (i.e., ImageNet,\nCIFAR, and Places) and multi-label classiﬁcation datasets (i.e.,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\nVOC and COCO). Based on these datasets, various long-tailed\nlearning methods have been proposed, as shown in Section 3.\nBesides these artiﬁcial tasks, long-tailed learning is also applied\nto real-world applications, including species classiﬁcation [23],\n[24], [47], face recognition [21], [22], [48], [49], face attribute\nclassiﬁcation [50], cloth attribute classiﬁcation [50], age classiﬁ-\ncation [51], rail surface defect detection [52], and medical image\ndiagnosis [25], [53]. These real applications usually require more\nﬁne-grained discrimination abilities, since the differences among\ntheir classes are more subtle. Due to this new challenge, existing\ndeep long-tailed learning methods tend to fail in these applications,\nsince they only focus on addressing the class imbalance and\ncannot essentially identify subtle class differences. Therefore, when\nexploring new methods to handle these applications, it is worth\nconsidering how to tackle the challenges of class imbalance and\nﬁne-grained information identiﬁcation, simultaneously.\nImage Detection / Segmentation. Object detection and in-\nstance segmentation has attracted increasing attention in the long-\ntailed learning community [54], [55], [56], [57], [58], [59], where\nmost existing studies are conducted based on LVIS and COCO. In\naddition to these widely-used benchmarks, many other applications\nhave also been explored, including urban scene understanding [26],\n[60] and unmanned aerial vehicle detection [27]. Compared to\nartiﬁcial tasks on LVIS and COCO, these real applications are\nmore challenging due to more complex environments in the\nwild. For example, the images may be collected from different\nweather conditions or different times in a day, which may lead\nto multiple image domains with different data distributions and\ninconsistent class skewness. When facing these new challenges,\nexisting deep long-tailed learning methods tend to fail. Hence, it is\nworth exploring how to simultaneously resolve the challenges of\nclass imbalance and domain shifts for handling these applications.\nVisual Relation Learning. Visual relation learning is impor-\ntant for image understanding and is attracting rising attention in\nthe long-tailed learning community. Important applications include\nlong-tailed scene graph generation [61], [62], long-tailed visual\nquestion answering and image captioning [63], [64]. Most existing\nlong-tailed studies focus on discriminative tasks, so they cannot be\napplied to the aforementioned applications that require modeling\nrelations between objects or those between images and texts. Even\nso, it is interesting to explore the high-level ideas (e.g., class\nre-balancing) in existing long-tailed studies to design application-\ncustomized approaches for visual relation learning.\n2.5\nRelationships with Related Tasks\nWe then brieﬂy discuss several related tasks, including non-deep\nlong-tailed learning, class-imbalanced learning, few-shot learning,\nand out-of-domain generalization.\nNon-deep long-tailed learning. There are a lot of non-deep\nlearning approaches for long-tailed problems [65], [66], [67]. They\nusually explore prior knowledge to enhance classic machine learn-\ning algorithms for handling the long-tailed problem. For example,\nthe prior of similarity among categories is used to regularize kernel\nmachine algorithm for long-tailed object recognition [65]. More-\nover, the prior of a long-tailed power-law distribution produced by\nthe Pitman-Yor Processes (PYP) method [68] is applied to enhance\nthe Bayesian non-parametric framework for long-tailed active\nlearning [66]. An artiﬁcial distribution prior is adopted to construct\ntail-class data augmentation to enhance KNN and SVM for long-\ntailed scene parsing [67]. Almost all these approaches extract image\nfeatures based on Scale Invariant Feature Transform (SIFT) [69],\nHistogram of Gradient Orientation (HOG) [70], or RGB color\nhistogram [71]. Such representation approaches, however, cannot\nextract highly informative and discriminative features for real\nvisual applications [1] and thus lead to limited performance in\nlong-tailed learning. Recently, in light of the powerful abilities of\ndeep networks for image representation, deep long-tailed methods\nhave achieved signiﬁcant performance improvement for long-tailed\nlearning. More encouragingly, the use of deep networks also\ninspires plenty of new solution paradigms for long-tailed learning,\nsuch as transfer learning, decoupled training and ensemble learning,\nwhich will be introduced in the next section.\nClass-imbalanced learning [5], [72] also seeks to train models\nfrom class-imbalanced samples. In this sense, long-tailed learning\ncan be regarded as a challenging sub-task of class-imbalanced\nlearning. The dominant distinction is that the classes of long-\ntailed learning follow a long-tailed class distribution, which is\nnot necessary for class-imbalanced learning. More differences\ninclude that in long-tailed learning the number of classes is usually\nlarge and the tail-class samples are often very scarce, whereas the\nnumber of minority-class samples in class-imbalanced learning is\nnot necessarily small in an absolute sense. These extra challenges\nlead long-tailed learning to be a more challenging task than class-\nimbalanced learning. Despite these differences, both seek to resolve\nthe class imbalance, so some high-level solution ideas (e.g., class\nre-balancing) are shared between them.\nFew-shot learning [73], [74], [75], [76] aims to train models\nfrom a limited number of labeled samples (e.g., 1 or 5) per class.\nIn this regard, few-shot learning can be regarded as a sub-task of\nlong-tailed learning, in which the tail classes generally have a very\nsmall number of samples.\nOut-of-domain Generalization [77], [78] indicates a class of\ntasks, in which the training distribution is inconsistent with the\nunknown test distribution. Such inconsistency includes inconsistent\ndata marginal distributions (e.g., domain adaptation [79], [80], [81],\n[82], [83], [84] and domain generalization [85], [86]), inconsistent\nclass distributions (e.g., long-tailed learning [15], [28], [32], open-\nset learning [87], [88]), and the combination of the previous two\nsituations. From this perspective, long-tailed learning can be viewed\nas a speciﬁc task within out-of-domain generalization.\n3\nCLASSIC METHODS\nAs shown in Fig. 2, we divide existing deep long-tailed learning\nmethods into three main categories according to their main\ntechnical characteristics, including class re-balancing, information\naugmentation, and module improvement. More speciﬁcally, class\nre-balancing consists of three sub-categories: re-sampling, class-\nsensitive learning (CSL), and logit adjustment (LA). Information\naugmentation comprises transfer learning (TL) and data augmenta-\ntion (Aug). Module improvement includes representation learning\n(RL), classiﬁer design (CD), decoupled training (DT) and ensemble\nlearning (Ensemble). According to this taxonomy, we sort out\nexisting methods in Table 2 and review them in detail as follows.\n3.1\nClass Re-balancing\nClass re-balancing, a mainstream paradigm in long-tailed learning,\nseeks to re-balance the negative inﬂuence brought by the class\nimbalance in training sample numbers. This type of methods has\nthree main sub-categories: re-sampling, class-sensitive learning,\nand logit adjustment. We begin with re-sampling based methods,\nfollowed by class-sensitive learning and logit adjustment.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nTABLE 2\nSummary of existing deep long-tailed learning methods published in the top-tier conferences before mid-2021. There are three main categories:\nclass re-balancing, information augmentation and module improvement. In this table, “CSL” indicates class-sensitive learning; “LA” indicates logit\nadjustment; “TL” represents transfer learning; “Aug” indicates data augmentation; “RL” indicates representation learning; “CD” indicates classiﬁer\ndesign, which seeks to design new classiﬁers or prediction schemes for long-tailed recognition; “DT” indicates decoupled training, where the feature\nextractor and the classiﬁer are trained separately; “Ensemble” indicates ensemble learning based methods. In addition, “Target Aspect” indicates\nfrom which aspect an approach seeks to resolve the class imbalance. We also make our codebase and our collected long-tailed learning resources\navailable at https://github.com/Vanint/Awesome-LongTailed-Learning.\nMethod\nYear\nClass Re-balancing\nAugmentation\nModule Improvement\nTarget Aspect\nRe-sampling\nCSL\nLA\nTL\nAug\nRL\nCD\nDT\nEnsemble\nLMLE [89]\n2016\n\u0013\nfeature\nHFL [90]\n2016\n\u0013\nfeature\nFocal loss [54]\n2017\n\u0013\nobjective\nRange loss [21]\n2017\n\u0013\nfeature\nCRL [50]\n2017\n\u0013\nfeature\nMetaModelNet [91]\n2017\n\u0013\nDSTL [92]\n2018\n\u0013\nDCL [93]\n2019\n\u0013\nsample\nMeta-Weight-Net [94]\n2019\n\u0013\nobjective\nLDAM [18]\n2019\n\u0013\nobjective\nCB [16]\n2019\n\u0013\nobjective\nUML [95]\n2019\n\u0013\nfeature\nFTL [96]\n2019\n\u0013\n\u0013\nfeature\nUnequal-training [48]\n2019\n\u0013\nfeature\nOLTR [15]\n2019\n\u0013\nfeature\nBalanced Meta-Softmax [97]\n2020\n\u0013\n\u0013\nsample, objective\nDecoupling [32]\n2020\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nfeature, classiﬁer\nLST [98]\n2020\n\u0013\n\u0013\nsample\nDomain adaptation [28]\n2020\n\u0013\nobjective\nEqualization loss (ESQL) [19]\n2020\n\u0013\nobjective\nDBM [22]\n2020\n\u0013\nobjective\nDistribution-balanced loss [37]\n2020\n\u0013\nobjective\nUNO-IC [99]\n2020\n\u0013\nprediction\nDe-confound-TDE [45]\n2020\n\u0013\n\u0013\nprediction\nM2m [100]\n2020\n\u0013\n\u0013\nsample\nLEAP [49]\n2020\n\u0013\n\u0013\n\u0013\nfeature\nOFA [101]\n2020\n\u0013\n\u0013\n\u0013\nfeature\nSSP [102]\n2020\n\u0013\n\u0013\nfeature\nLFME [103]\n2020\n\u0013\n\u0013\nsample, model\nIEM [104]\n2020\n\u0013\nfeature\nDeep-RTC [105]\n2020\n\u0013\nclassiﬁer\nSimCal [34]\n2020\n\u0013\n\u0013\nsample, model\nBBN [44]\n2020\n\u0013\nsample, model\nBAGS [56]\n2020\n\u0013\nsample, model\nVideoLT [38]\n2021\n\u0013\nsample\nLOCE [33]\n2021\n\u0013\n\u0013\nsample, objective\nDARS [26]\n2021\n\u0013\n\u0013\n\u0013\nsample, objective\nCReST [106]\n2021\n\u0013\n\u0013\nsample\nGIST [107]\n2021\n\u0013\n\u0013\n\u0013\nclassiﬁer\nFASA [58]\n2021\n\u0013\n\u0013\nfeature\nEqualization loss v2 [108]\n2021\n\u0013\nobjective\nSeesaw loss [109]\n2021\n\u0013\nobjective\nACSL [110]\n2021\n\u0013\nobjective\nIB [111]\n2021\n\u0013\nobjective\nPML [51]\n2021\n\u0013\nobjective\nVS [112]\n2021\n\u0013\nobjective\nLADE [31]\n2021\n\u0013\n\u0013\nobjective, prediction\nRoBal [113]\n2021\n\u0013\n\u0013\n\u0013\nobjective, prediction\nDisAlign [29]\n2021\n\u0013\n\u0013\n\u0013\nobjective, classiﬁer\nMiSLAS [114]\n2021\n\u0013\n\u0013\n\u0013\nobjective, feature, classiﬁer\nLogit adjustment [14]\n2021\n\u0013\nprediction\nConceptual 12M [115]\n2021\n\u0013\nDiVE [116]\n2021\n\u0013\nMosaicOS [117]\n2021\n\u0013\nRSG [118]\n2021\n\u0013\n\u0013\nfeature\nSSD [119]\n2021\n\u0013\n\u0013\nRIDE [17]\n2021\n\u0013\n\u0013\nmodel\nMetaSAug [120]\n2021\n\u0013\nsample\nPaCo [121]\n2021\n\u0013\nfeature\nDRO-LT [122]\n2021\n\u0013\nfeature\nUnsupervised discovery [35]\n2021\n\u0013\nfeature\nHybrid [123]\n2021\n\u0013\nfeature\nKCL [13]\n2021\n\u0013\n\u0013\nfeature\nDT2 [61]\n2021\n\u0013\nfeature, classiﬁer\nLTML [46]\n2021\n\u0013\nsample, model\nACE [124]\n2021\n\u0013\nsample, model\nResLT [125]\n2021\n\u0013\nsample, model\nSADE [30]\n2021\n\u0013\nobjective, model\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\n3.1.1\nRe-sampling\nConventional training of deep networks is based on mini-batch\ngradient descent with random sampling, i.e., each sample has\nan equal probability of being sampled. Such a sampling manner,\nhowever, ignores the imbalance issue in long-tailed learning, and\nnaturally samples more head-class samples than tail-class samples\nin each sample mini-batch. This makes the resulting deep models\nbiased towards head classes and perform poorly on tail classes. To\naddress this issue, re-sampling [126], [127], [128], [129] has been\nexplored to re-balance classes by adjusting the number of samples\nper class in each sample batch for model training.\nIn the non-deep learning era, the most classic re-sampling\napproaches are random over-sampling (ROS) and random under-\nsampling (RUS). Speciﬁcally, ROS randomly repeats the samples\nfrom minority classes to re-balance classes before training, while\nRUS randomly discards the samples from majority classes. When\napplying them to deep long-tailed learning where the classes\nare highly skewed, ROS with duplicated tail-class data might\nlead to overﬁtting over tail classes, while RUS might discard\nprecious head-class samples and degrade model performance on\nhead classes [44]. Instead of using random re-sampling, recent deep\nlong-tailed studies have developed various class-balanced sampling\nmethods for mini-batch training of deep models.\nWe begin with Decoupling [32], in which four sampling\nstrategies were evaluated for representation learning of long-tailed\ndata, including random sampling, class-balanced sampling, square-\nroot sampling and progressively-balanced sampling. Speciﬁcally,\nclass-balanced sampling means that each class has an equal\nprobability of being selected. Square-root sampling [130] is a\nvariant of class-balanced sampling, where the sampling probability\nof each class is related to the square root of the sample size in\nthe corresponding class. Progressively-balanced sampling [32]\ninterpolates progressively between random and class-balanced\nsampling. Based on empirical results, Decoupling [32] found\nthat square-root sampling and progressively-balanced sampling\nare better strategies for standard model training in long-tailed\nrecognition. The two strategies, however, require knowing the\ntraining sample frequencies of different classes in advance, which\nmay be unavailable in real applications.\nTo address the above issue, recent studies proposed vari-\nous adaptive sampling strategies. Dynamic Curriculum Learning\n(DCL) [93] developed a new curriculum strategy to dynamically\nsample data for class re-balancing. The basic idea is that the more\ninstances from one class are sampled as training proceeds, the lower\nprobability of this class would be sampled in later stages. Following\nthis idea, DCL ﬁrst conducts random sampling to learn general\nrepresentations, and then samples more tail-class instances based\non the curriculum strategy to handle the imbalance. In addition to\nusing the accumulated sampling times, Long-tailed Object Detector\nwith Classiﬁcation Equilibrium (LOCE) [33] proposed to monitor\nmodel training on different classes via the mean classiﬁcation\nprediction score (i.e., running prediction probability), and used this\nscore to guide the sampling rates for different classes. Furthermore,\nVideoLT [38], focusing on long-tailed video recognition, introduced\na new FrameStack method that dynamically adjusts the sampling\nrates of different classes based on running model performance\nduring training, so that it can sample more video frames from tail\nclasses (generally with lower running performance).\nBesides using the statistics computed during model training,\nsome re-sampling approaches resorted to meta learning [131].\nBalanced Meta-softmax [97] developed a meta-learning-based\nsampling method to estimate the optimal sampling rates of\ndifferent classes for long-tailed learning. Speciﬁcally, the developed\nmeta learning method seeks to learn the best sample distribution\nparameter by optimizing the model classiﬁcation performance on a\nbalanced meta validation set. Similarly, Feature Augmentation and\nSampling Adaptation (FASA) [58] explored the model classiﬁcation\nloss on a balanced meta validation set as a score, which is used\nto adjust the sampling rate for different classes so that the under-\nrepresented tail classes can be sampled more.\nNote that some long-tailed visual tasks may have multiple levels\nof imbalance. For example, long-tailed instance segmentation is\nimbalanced in terms of both images and instances (i.e., the number\nof instances per image is also imbalanced). To address this task,\nSimple Calibration (SimCal) [34] proposed a new bi-level class-\nbalanced sampling strategy that combines image-level and instance-\nlevel re-sampling for class re-balancing.\nDiscussions. Re-sampling methods seek to address the class\nimbalance issue at the sample level. When the label frequencies\nof different classes are known a priori, progressively-balanced\nsampling [32] is recommended. Otherwise, using the statistics of\nmodel training to guide re-sampling [33] is a preferred solution\nfor real applications. For meta-learning-based re-sampling, it may\nbe difﬁcult to construct a meta validation set in real scenarios.\nNote that if one re-sampling strategy has already addressed class\nimbalance well, further using other re-sampling methods may\nnot bring extra beneﬁts. Moreover, the high-level ideas of these\nre-sampling methods can be applied to design multi-level re-\nsampling strategies if there are multiple levels of imbalance in\nreal applications.\n3.1.2\nClass-sensitive Learning\nConventional training of deep networks is based on the softmax\ncross-entropy loss (c.f. Table 3). This loss ignores the class\nimbalance in data sizes and tends to generate uneven gradients for\ndifferent classes. That is, each positive sample of one class can be\nseen as a negative sample for other classes in cross-entropy, which\nleads head classes to receive more supporting gradients (as they\nusually are positive samples) and causes tail classes to receive more\nsuppressed gradients (as they usually are negative samples) [19],\n[55]. To address this, class-sensitive learning seeks to particularly\nadjust the training loss values for various classes to re-balance\nthe uneven training effects caused by the imbalance issue [132],\n[133], [134], [135], [136], [137]. There are two main types of\nclass-sensitive strategies, i.e., re-weighting and re-margining. We\nbegin with class re-weighting as follows.\nRe-weighting. To address the class imbalance, re-weighting\nattempts to adjust the training loss values for different classes by\nmultiplying them with different weights. The most intuitive method\nis to directly use the label frequencies of training samples for loss\nre-weighting to re-balance the uneven positive gradients among\nclasses. For example, weighted softmax (c.f. Table 3) directly\nmultiplies the loss values of different classes by the inverse of train-\ning label frequencies. However, simply multiplying by its inverse\nmay not be the optimal solution. Recent studies thus proposed to\ntune the inﬂuence of training label frequencies based on sample-\naware inﬂuences [111]. Moreover, Class-balanced loss (CB) [16]\nintroduced a novel concept of effective number to approximate\nthe expected sample number of different classes, which is an\nexponential function of their training label number. Following this,\nCB loss enforces a class-balanced re-weighting term, inversely\nproportional to the effective number of classes, to address the class\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\nTABLE 3\nSummary of losses. In this table, z and p indicate the predicted logits\nand the softmax probability of the sample x, where zy and py\ncorrespond to the class y. Moreover, n indicates the total number of\ntraining data, where ny is the sample number of the class y. In addition,\nπ denotes the vector of sample frequencies, where πy=ny/n\nrepresents the label frequency of the class y. The class-wise weight is\ndenoted by ω and the class-wise margin is denoted by ∆, if no more\nspeciﬁc value is given. Loss-related parameters include γ.\nLoss\nFormulation\nType\nSoftmax loss\nLce = −log(py)\n-\nFocal loss [54]\nLﬂ= −(1 −py)γ log(py)\nre-weighting\nWeighted Softmax loss\nLwce = −1\nπy log(py)\nre-weighting\nClass-balanced loss [16]\nLcb = −\n1−γ\n1−γny log(py)\nre-weighting\nBalanced Softmax loss [97]\nLbs = −log(\nπy exp(zy)\nP\nj πj exp(zj))\nre-weighting\nEqualization loss [19]\nLeq = −log(\nexp(zy)\nP\nj ωj exp(zj))\nre-weighting\nLDAM loss [18]\nLldam = −log(\nexp(zy−∆y)\nP\nj exp(zj−∆j))\nre-margining\nimbalance (c.f. Table 3). Besides the aforementioned re-weighting\nat the level of log probabilities, we can also use the training label\nfrequencies to re-weight prediction logits. Balanced Softmax [97]\nproposed to adjust prediction logits by multiplying by the label\nfrequencies, so that the bias of class imbalance can be alleviated by\nthe label prior before computing ﬁnal losses. Afterwards, Vector-\nscaling loss (VS) [112] intuitively analyzed the distinct effects of\nadditive and multiplicative logit-adjusted losses, leading to a novel\nVS loss to combine the advantages of both forms of adjustment.\nInstead of using training label frequencies, Focal loss [54]\nexplored class prediction hardness for re-weighting. This is inspired\nby the observation that class imbalance usually increases the\nprediction hardness of tail classes, whose prediction probabilities\nwould be lower than those of head classes. Following this, Focal\nloss uses the prediction probabilities to inversely re-weight classes\n(c.f. Table 3), so that it can assign higher weights to the harder tail\nclasses but lower weights to the easier head classes. Besides using\na pre-deﬁned weighting function, the class weights can also be\nlearned from data. For instance, Meta-Weight-Net [94] proposed\nto learn an MLP-approximated weighting function based on a\nbalanced validation set for class-sensitive learning.\nSome recent studies [19], [37] also seek to address the negative\ngradient over-suppression issue of tail classes. For example,\nEqualization loss [19] directly down-weights the loss values of\ntail-class samples when they serve as negative labels for head-class\nsamples. However, simply down-weighting negative gradients may\nharm the discriminative abilities of deep models. To address this,\nAdaptive Class Suppression loss (ACSL) [110] uses the output\nconﬁdence to decide whether to suppress the gradient for a negative\nlabel. Speciﬁcally, if the prediction probability of a negative label\nis larger than a pre-deﬁned threshold, it means that the model\nis confused about this class so the weight for this class is set to\n1 to improve model discrimination; otherwise, the weight is set\nto 0 to avoid negative over-suppression. Moreover, Equalization\nloss v2 [108] extended the equalization loss [19] by introducing a\nnovel gradient-guided re-weighting mechanism that dynamically\nup-weights the positive gradients and down-weights the negative\ngradients for different classes. Similarly, Seesaw loss [109] re-\nbalances positive and negative gradients for each class with two re-\nweighting factors, i.e., mitigation and compensation. Speciﬁcally, to\naddress gradient over-suppression, the mitigation factor alleviates\nthe penalty to tail classes based on a dynamically cumulative\nsampling number of different classes. Meanwhile, if a false positive\nsample is observed, the compensation factor up-weights the penalty\nto the corresponding class for improving model discrimination.\nRe-margining. To handle the class imbalance, re-margining\nattempts to adjust losses by subtracting different margin factors for\ndifferent classes, so that they have a different minimal margin (i.e.,\ndistance) between features and the classiﬁer. Directly using existing\nsoft margin losses [138], [139] is unfeasible, since they ignore the\nissue of class imbalance. To address this, Label-Distribution-Aware\nMargin (LDAM) [18] enforces class-dependent margin factors for\ndifferent classes based on their training label frequencies, which\nencourages tail classes to have larger margins.\nHowever, the training label frequencies may be unknown in\nreal applications, and simply using them for re-margining also\nignores the status of model training on different classes. To address\nthis, recent studies explored various adaptive re-margining methods.\nUncertainty-based margin learning (UML) [95] found that the class\nprediction uncertainty is inversely proportional to the training label\nfrequencies, i.e., tail classes are more uncertain. Inspired by this,\nUML proposed to use the estimated class-level uncertainty to re-\nmargin losses, so that the tail classes with higher class uncertainty\nincur a higher loss value and thus have a larger margin between\nfeatures and the classiﬁer. Moreover, LOCE [33] proposed to use\nthe mean class prediction score to monitor the learning status of\ndifferent classes and apply it to guide class-level margin adjustment\nfor enhancing tail classes. Domain balancing [22] introduced a\nnovel frequency indicator based on the inter-class compactness\nof features, and uses this indicator to re-margin the feature space\nof tail domains. Despite effectiveness, the above re-margining\nmethods for encouraging large tail-class margins may degrade\nthe feature learning of head classes. To address this, RoBal [113]\nfurther enforces a margin factor to also enlarge head-class margins.\nDiscussions. These class-sensitive learning methods aim to\nresolve the class imbalance issue at the objective level. We\nsummarize some of them in Table 3. Both re-weighting and re-\nmargining methods have a similar effect on re-balancing classes.\nIf the negative inﬂuence of class imbalance can be addressed\nby one class-sensitive approach well, it is unnecessary to further\napply other class-sensitive methods, which would not bring further\nperformance gain and even harm performance. More speciﬁcally,\nif the training label frequencies are available, directly using them\nfor re-weighting (e.g., Balanced Softmax [97] and VS [112]) or\nre-margining (e.g., LDAM [18]) provides a simple and generally\neffective solution for real applications. If not, it is preferred to use\nthe mean class prediction score to guide class-sensitive learning\n(e.g., ACSL [110] and LOCE [33]) thanks to its simplicity. One\ncan also consider other guidance, like intra-class compactness.\nHowever, inter-class compactness of features [22] may be not that\ninformative when the feature dimensions are very high, while the\nprediction uncertainty [95] may be difﬁcult to estimate accurately\nin practice. Moreover, using prediction hardness for re-weighting\nin Focal loss performs well when the number of classes is not large,\nbut may fail when facing a large number of classes. Furthermore,\nEqualization loss v2, Seesaw loss and RoBal can also be considered\nif the challenges that they try to resolve appear in real applications.\n3.1.3\nLogit Adjustment\nLogit adjustment [14], [140] seeks to resolve the class imbalance\nby adjusting the prediction logits of a class-biased deep model.\nOne recent study [14] comprehensively analyzed logit adjustment\nvia training label frequencies of different classes in long-tailed\nrecognition, and theoretically showed that logit adjustment is Fisher\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\nconsistent to minimize the average per-class error. Following this\nidea, RoBal [113] applied a post-processing strategy to adjust the\ncosine classiﬁer based on training label frequencies.\nHowever, the above methods tent to fail when the training label\nfrequencies are unavailable. To address this this, UNO-IC [99]\nproposed to learn the logit offset based on a balanced meta\nvalidation set and use it to calibrate the biased model predictions.\nInstead of using a meta validation set, DisAlign [29] applied\nan adaptive calibration function for logit adjustment, where the\ncalibration function is learned by matching the calibrated prediction\ndistribution to a pre-deﬁned relatively balanced class distribution.\nThe idea of logit adjustment naturally suits agnostic test class\ndistributions. If the test label frequencies are available, LADE [31]\nproposed to use them to post-adjust model outputs so that the\ntrained model can be calibrated for arbitrary test class distributions.\nHowever, the test label frequencies are usually unavailable, which\nmakes LADE less practical in real scenarios.\nDiscussions. To summarize, these logit adjustment methods\naddress the class imbalance at the prediction level. If the training\nlabel frequencies are known, directly using them to post-adjust the\npredictions of biased deep models is recommended [14], [113]. If\nsuch information is unknown, it is preferred to exploit the idea\nof DisAlign [29] to learn an adaptive calibration function. These\nlogit adjustment methods are exclusive to each other, so using a\nwell-performing one is enough for real applications.\n3.1.4\nSummary\nClass re-balancing is relatively simple among the three main\nmethod types of long-tailed learning, but it can achieve comparable\nor even better performance. Some methods, especially class-\nsensitive learning, are theoretically inspired or guaranteed to handle\nlong-tailed problems [16], [18], [31]. These advantages enable class\nre-balancing to be a good candidate for real-world applications.\nThe ultimate goal of its three sub-categories (i.e., re-sampling,\nclass-sensitive learning and logit adjustment) are the same, i.e., re-\nbalancing classes. Hence, when the class imbalance is not addressed\nwell, combining them may achieve better performance. However,\nthese subtypes are sometimes exclusive to each other. For example,\nif we have trained a class-balanced deep model via class-sensitive\nlearning, then further using logit adjustment methods to post-adjust\nmodel inference will instead lead to biased predictions and suffer\npoor performance. Therefore, if one wants to combine them, the\npipeline should be designed carefully.\nOne drawback of class re-balancing is that most methods\nimprove tail-class performance at the cost of lower head-class\nperformance, which is like playing on a performance seesaw.\nAlthough the overall performance is improved, it cannot essentially\nhandle the issue of lacking information, particularly on tail classes\ndue to limited data sizes. To address this limitation, one feasible\nsolution is to conduct information augmentation as follows.\n3.2\nInformation Augmentation\nInformation augmentation seeks to introduce additional information\ninto model training, so that the model performance can be improved\nfor long-tailed learning. There are two kinds of methods in this\nmethod type: transfer learning and data augmentation.\n3.2.1\nTransfer Learning\nTransfer learning [91], [101], [118], [141], [142] seeks to transfer\nthe knowledge from a source domain (e.g., datasets) to enhance\nmodel training on a target domain. In long-tailed learning, there\nare four main transfer schemes, i.e., model pre-training, knowledge\ndistillation, head-to-tail model transfer, and self-training.\nModel pre-training is a popular scheme for deep model\ntraining [143], [144], [145], [146], [147] and has also been explored\nin long-tailed learning. For example, Domain-Speciﬁc Transfer\nLearning (DSTL) [92] ﬁrst pre-trains the model with all long-\ntailed samples for representation learning, and then ﬁne-tunes the\nmodel on a more class-balanced training subset. In this way, DSTL\nslowly transfers the learned features to tail classes, obtaining more\nbalanced performance among all classes. Rather than supervised\npre-training, Self-supervised Pre-training (SSP) [102] proposed to\nﬁrst use self-supervised learning (e.g., contrastive learning [148]\nor rotation prediction [149]) for model pre-training, followed by\nstandard training on long-tailed data. Empirical results show self-\nsupervised learning helps to learn a balanced feature space for\nlong-tailed learning [13]. Such a scheme has also been explored to\nhandle long-tailed data with noisy labels [150].\nKnowledge distillation seeks to train a student model based\non the outputs of a well-trained teacher model [151], [152]. Recent\nstudies have explored knowledge distillation for long-tailed learn-\ning. For example, Learning from Multiple Experts (LFME) [103]\nﬁrst trains multiple experts on several less imbalanced sample\nsubsets (e.g., head, middle and tail sets), and then distills these\nexperts into a uniﬁed student model. Similarly, Routing Diverse\nExperts (RIDE) [17] introduced a knowledge distillation method\nto reduce the parameters of the multi-expert model by learning\na student network with fewer experts. Instead of multi-expert\nteachers, Distill the Virtual Examples (DiVE) [116] showed that\nlearning a class-balanced model as the teacher is also beneﬁcial\nfor long-tailed learning. Following DiVE, Self-Supervision to\nDistillation (SSD) [119] developed a new self-distillation scheme\nto enhance decoupled training (c.f. Section 3.3.3). Speciﬁcally,\nSSD ﬁrst trains a calibrated model based on supervised and self-\nsupervised information via the decoupled training scheme, and then\nuses the calibrated model to generate soft labels for all samples.\nFollowing that, both the generated soft labels and original long-\ntailed hard labels are used to distill a new student model, followed\nby a new classiﬁer ﬁne-tuning stage.\nHead-to-tail model transfer seeks to transfer the model\nknowledge from head classes to enhance model performance on tail\nclasses. For example, MetaModelNet [91] proposed to learn a meta-\nnetwork that can map few-shot model parameters to many-shot\nmodel parameters. To this end, MetaModelNet ﬁrst trains a many-\nshot model on the head-class training set, and trains a fake few-shot\nmodel on a sampled subset from these classes with a very limited\nnumber of data to mimic tail classes. Then, the meta-network is\nlearned by mapping the learned fake few-shot model to the many-\nshot model. Following that, the learned meta-network on head\nclasses is applied to map the true few-shot model trained on tail\nclasses for obtaining better tail-class performance. Instead of model\nmapping, Geometric Structure Transfer (GIST) [107] proposed\nto conduct head-to-tail transfer at the classiﬁer level. Speciﬁcally,\nGIST uses the relatively large classiﬁer geometry information of\nhead classes to enhance the tail-class classiﬁer weights, so that the\nperformance of tail classes can be improved.\nSelf-training aims to learn well-performing models from a\nsmall number of labeled samples and massive unlabeled sam-\nples [153], [154], [155]. To be speciﬁc, it ﬁrstly uses labeled\nsamples to train a supervised model, which is then applied to\ngenerate pseudo labels for unlabeled data. Following that, both the\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\nlabeled and pseudo-labeled samples are used to re-train models.\nIn this way, self-training can exploit the knowledge from massive\nunlabeled samples to enhance long-tailed learning performance.\nSuch a paradigm, however, cannot be directly used to handle long-\ntailed problems, because both labeled and unlabeled datasets may\nfollow long-tailed class distributions with different degrees. In such\ncases, the trained model on labeled samples may be biased to head\nclasses and tends to generate more head-class pseudo labels for\nunlabeled samples, leading to a more skewed degree of imbalance.\nTo address this issue, Distribution Alignment and Random\nSampling (DARS) [26] proposed to regard the label frequencies\nof labeled data as a reference and enforce the label frequencies of\nthe generated pseudo labels to be consistent with the labeled ones.\nInstead of using training label frequencies, Class-rebalancing Self-\ntraining (CReST) [106] found that the precision of the supervised\nmodel on tail classes is surprisingly high, and thus proposed to\nselect more tail-class samples for online pseudo labeling in each\niteration, so that the re-trained model can obtain better performance\non tail classes. Beyond classiﬁcation tasks, MosaicOS [117]\nresorted to other object-centric images to boost long-tailed object\ndetection. Speciﬁcally, it ﬁrst pre-trains the model with labeled\nscene-centric images from the original detection dataset, and then\nuses the pre-trained model to generate pseudo bounding boxes\nfor object-centric images, e.g., ImageNet-1K [39]. After that,\nMosaicOS ﬁne-tunes the pre-trained model in two stages, i.e.,\nﬁrst ﬁne-tuning with the pseudo-labeled object-centric images and\nthen ﬁne-tuning with the original labeled scene-centric images.\nIn this way, MosaicOS alleviates the negative inﬂuence of data\ndiscrepancies and effectively improves long-tailed performance.\nDiscussions. These transfer learning methods are complemen-\ntary to each other, which brings additional information from\ndifferent perspectives to long-tailed learning. Most of them can\nbe used together for real applications if the resources permit and\nthe combination pipeline is designed well. More concretely, when\nusing model pre-training, the trade-off between supervised discrim-\nination learning and self-supervised class-balanced learning should\nbe tuned [13], which contributes to better long-tailed learning\nperformance. In addition, knowledge distillation with multi-experts\ncan usually achieve better performance than distillation with a\nsingle teacher. In head-to-tail model transfer, GIST is a better\ncandidate than MetaModelNet due to its simplicity. Lastly, the use\nof self-training methods depends on task requirements and what\nunlabeled samples are available at hand.\n3.2.2\nData Augmentation\nData Augmentation aims to enhance the size and quality of datasets\nby applying pre-deﬁned transformations to each data/feature for\nmodel training [156], [157]. In long-tailed learning, there are\ntwo types of augmentation methods that have been explored, i.e.,\ntransfer-based augmentation and non-transfer augmentation.\nHead-to-tail transfer augmentation seeks to transfer the\nknowledge from head classes to augment tail-class samples. For\nexample, Major-to-Minor translation (M2m) [100] proposed to\naugment tail classes by translating head-class samples to tail-\nclass ones via perturbation-based optimization, which is essentially\nsimilar to adversarial attack. The translated tail-class samples are\nused to construct a more balanced training set for model training.\nBesides the data-level transfer in M2m, most studies explore\nfeature-level transfer. For instance, Feature Transfer Learning\n(FTL) [96] found that tail-class samples have much smaller\nintra-class variance than head-class samples, leading to biased\nfeature spaces and decision boundaries. To address this, FTL\nexploits the knowledge of intra-class variance from head classes\nto guide feature augmentation for tail-class samples, so that\nthe tail-class features have higher intra-class variance. Similarly,\nLEAP [49] constructs “feature cloud” for each class, and transfers\nthe distribution knowledge of head-class feature clouds to enhance\nthe intra-class variation of tail-class feature clouds. As a result,\nthe distortion of the intra-class feature variance among classes is\nalleviated, leading to better tail-class performance.\nInstead of using the intra-class variation information, Rare-\nclass Sample Generator (RSG) [118] proposed to dynamically\nestimate a set of feature centers for each class, and use the\nfeature displacement between head-class sample features and their\nnearest intra-class feature center to augment each tail sample\nfeature for enlarging the tail-class feature space. Moreover, Online\nFeature Augmentation (OFA) [101] proposed to use class activation\nmaps [158] to decouple sample features into class-speciﬁc and\nclass-agnostic ones. Following that, OFA augments tail classes by\ncombining the class-speciﬁc features of tail-class samples with\nclass-agnostic features from head-class samples.\nNon-transfer augmentation seeks to improve or design\nconventional data augmentation methods to address long-tailed\nproblems. SMOTE [159], a classic over-sampling method for\nnon-deep class imbalance, can be applied to deep long-tailed\nproblems to generate tail-class samples by mixing several intra-\nclass neighbouring samples. Recently, MiSLAS [114] further\ninvestigated data mixup in deep long-tailed learning, and found that\n(1) data mixup helps to remedy model over-conﬁdence; (2) mixup\nhas a positive effect on representation learning but a negative or\nnegligible effect on classiﬁer learning in the decoupled training\nscheme [32]. Following these observations, MiSLAS proposed to\nuse data mixup to enhance representation learning in the decoupled\nscheme. In addition, Remix [160] also resorted to data mixup for\nlong-tailed learning and introduced a re-balanced mixup method to\nparticularly enhance tail classes.\nInstead of using data mixup, FASA [58] proposed to generate\nnew data features for each class, based on class-wise Gaussian\npriors with their mean and variance estimated from previously\nobserved samples. Here, FASA exploits the model classiﬁcation\nloss on a balanced validation set to adjust feature sampling rates\nfor different classes, so that the under-represented tail classes can\nbe augmented more than head classes. With a similar idea, Meta\nSemantic Augmentation (MetaSAug) [120] proposed to augment\ntail classes with a variant of implicit semantic data augmentation\n(ISDA) [161]. Speciﬁcally, ISDA estimates the class-conditional\nstatistics (i.e., covariance matrices from sample features) to obtain\nsemantic directions, and generates diversiﬁed augmented samples\nby translating sample features along with diverse semantically\nmeaningful directions. To better estimate the covariance matrices\nfor tail classes, MetaSAug explored meta learning to guide the\nlearning of covariance matrices for each class with the class-\nbalanced loss [16], leading to more informative synthetic features.\nDiscussions. Data augmentation based methods attempt to\naddress the class imbalance at the sample or feature levels. The\ngoals of these methods are consistent, so they can be used\nsimultaneously if the combination pipeline is constructed well.\nAmong its two subtypes, head-to-tail transfer augmentation is more\nintuitive than non-transfer augmentation. More speciﬁcally, head-\nto-tail transfer at the feature level (e.g., RSG) seems to perform\nbetter than transfer at the sample level (e.g., M2m). In the feature-\nlevel transfer augmentation, RSG is preferred thanks to its easy-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nto-use source code, whereas the intra-class variation in FTL and\nLEAP may be less informative for augmentation when the feature\ndimension is very high. In non-transfer augmentation, mixup-\nbased strategies are usually used thanks to their simplicity, where\nMiSLAS has demonstrated promising performance. In contrast, the\nclass-wise Gaussian priors in FASA and the covariance matrices in\nMetaSAug may be difﬁcult to estimate in various real scenarios.\n3.2.3\nSummary\nInformation augmentation addresses the long-tailed problems by\nintroducing additional knowledge, and thus is compatible with and\ncomplementary to other two method types, i.e., class re-balancing\nand module improvement. For the same reason, its two method\nsubtypes, i.e., transfer learning and data augmentation, are also\ncomplementary to each other. More concretely, both the subtypes\nare able to improve tail-class performance without sacriﬁcing\nhead-class performance if designed carefully. Considering that\nall classes are important in long-tailed learning, this type of\nmethod is worth further exploring. Moreover, data augmentation\nis a very fundamental technique and can be used for a variety\nof long-tailed problems, which makes it more practical than\nother paradigms in real-world applications. However, simply using\nexisting class-agnostic augmentation techniques for improving\nlong-tailed learning is unfavorable, since they ignore the class\nimbalance and inevitably augment more head-class samples than\ntail-class samples. How to better conduct data augmentation for\nlong-tailed learning is still an open question.\n3.3\nModule Improvement\nBesides re-balancing and information augmentation, researchers\nalso explored methods to improve network modules in long-tailed\nlearning. These methods can be divided into four categories: (1)\nrepresentation learning improves the feature extractor; (2) classiﬁer\ndesign enhances the model classiﬁer; (3) decoupled training aims\nto boost the learning of both the feature extractor and the classiﬁer;\n(4) ensemble learning improves the whole architecture.\n3.3.1\nRepresentation Learning\nExisting long-tailed learning methods improve representation\nlearning based on three main paradigms, i.e., metric learning,\nprototype learning, and sequential training.\nMetric learning aims at designing task-speciﬁc distance\nmetrics for establishing similarity or dissimilarity between data.\nIn deep long-tailed learning, metric learning based methods seek\nto explore various distance-based losses to learn a discriminative\nfeature space for long-tailed data. One example is Large Margin\nLocal Embedding (LMLE) [89], which introduced a quintuplet\nloss to learn representations that maintain both inter-cluster and\ninter-class margins. Unlike the triplet loss [162] that samples two\ncontrastive pairs, LMLE presented a quintuplet sampler to sample\nfour contrastive pairs, including a positive pair and three negative\npairs. The positive pair is the most distant intra-cluster sample,\nwhile the negative pairs include two inter-clusters samples from the\nsame class (one is the nearest and one is the most distant within\nthe same cluster) and the nearest inter-class sample. Following\nthat, LMLE introduced a quintuplet loss to encourage the sampled\nquintuplet to follow a speciﬁc distance order. In this way, the\nlearned representations preserve not only locality across intra-class\nclusters but also discrimination between classes. Moreover, each\ndata batch contains the same number of samples from different\nclasses for class re-balancing. However, LMLE does not consider\nthe sample differences among head and tail classes. To address this,\nClass Rectiﬁcation Loss (CRL) [50] explored hard pair mining\nand proposed to construct more hard-pair triplets for tail classes,\nso that tail-class features can have a larger degree of intra-class\ncompactness and inter-class distances.\nRather than sampling triplets or quintuplets, range loss [21]\ninnovated representation learning by using the overall distances\namong all sample pairs within one mini-batch. In other words, the\nrange loss uses statistics over the whole batch and thus alleviates\nthe bias of data number imbalance over classes. Speciﬁcally, range\nloss enlarges the inter-class distance by maximizing the distances\nof any two class centers within the mini-batch, and reduces the\nintra-class variation by minimizing the largest distances between\nintra-class samples. In this way, the range loss obtains features with\nbetter discriminative abilities and less imbalanced bias.\nRecent studies also explored contrastive learning for long-\ntailed problems. KCL [13] proposed a k-positive contrastive loss\nto learn a balanced feature space, which helps to alleviate the\nclass imbalance and improve model generalization. Parametric\ncontrastive learning (PaCo) [121] further innovated supervised\ncontrastive learning by adding a set of parametric learnable class\ncenters, which plays the same role as a classiﬁer if regarding the\nclass centers as the classiﬁer weights. Following that, Hybrid [123]\nintroduced a prototypical contrastive learning strategy to enhance\nlong-tailed learning. DRO-LT [122] extended the prototypical\ncontrastive learning with distribution robust optimization [163],\nwhich makes the learned model more robust to distribution shift.\nPrototype learning based methods seek to learn class-speciﬁc\nfeature prototypes to enhance long-tailed learning performance.\nOpen Long-Tailed Recognition (OLTR) [15] innovatively explored\nthe idea of feature prototypes to handle long-tailed recognition in\nan open world, where the test set also includes open classes that do\nnot appear in training data. To address this task, OLTR maintains a\nvisual meta memory containing discriminative feature prototypes,\nand uses the features sampled from the visual memory to augment\nthe original features for better discrimination. Meanwhile, the\nsample features from novel classes are enforced to be far away from\nthe memory and closer to the origin point. In this way, the learned\nfeature space enables OLTR to classify all seen classes and detect\nnovel classes. However, OLTR only maintains a static prototype\nmemory and each class has only one prototype. Such a single\nprototype per class may fail to represent the real data distribution.\nTo address this issue, Inﬂated Episodic Memory (IEM) [104] further\ninnovated the meta-embedding memory by a dynamical update\nscheme, in which each class has independent and differentiable\nmemory blocks. Each memory block is updated to record the most\ndiscriminative feature prototypes of the corresponding categories,\nthus leading to better performance than OLTR.\nSequential training based methods learn data representation\nin a continual way. For example, Hierarchical Feature Learning\n(HFL) [90] took inspiration from that each class has its individuality\nin discriminative visual representation. Therefore, HFL hierarchi-\ncally clusters objects into visually similar class groups, forming\na hierarchical cluster tree. In this cluster tree, the model in the\noriginal node is pre-trained on ImageNet-1K; the model in each\nchild node inherits the model parameters from its parent node\nand is then ﬁne-tuned based on samples in the cluster node. In\nthis way, the knowledge from the groups with massive classes\nis gradually transferred to their sub-groups with fewer classes.\nSimilarly, Unequal-training [48] proposed to divide the dataset into\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\nhead-class and tail-class subsets, and treat them differently in the\ntraining process. First, unequal-training uses the head-class samples\nto train relatively discriminative and noise-resistant features with\na new noise-resistant loss. After that, it uses tail-class samples to\nenhance the inter-class discrimination of representations via hard\nidentity mining and a novel center-dispersed loss.\nDiscussions. These representation learning methods seek to\naddress the class imbalance at the feature level. The methods\nwithin each subtype are competing with each other (e.g., KCL [13]\nvs PaCo [121] and OLTR [15] vs IEM [104]), while the methods\nfrom different subtypes may be complementary to each other (e.g.,\nKCL [13] and Unequal-training [48]). Therefore, the pipeline must\nbe carefully designed, if one wants to combine them together.\nMoreover, when handling real long-tailed applications, PaCo [121]\nis recommended to use thanks to its promising performance and\nopen-source code. If there are open classes in test data, IEM [104]\nis preferred. Other methods, like Unequal-training [48], can also\nbe considered if they suit real scenarios.\n3.3.2\nClassiﬁer Design\nIn addition to representation learning, researchers also explored\ndifferent types of classiﬁers to address long-tailed problems. In\ngeneric visual problems [10], [148], the common practice of\ndeep learning is to use linear classiﬁer p = φ(w⊤f+b), where\nφ denotes the softmax function and the bias term b can be\ndiscarded. However, long-tailed class imbalance often results in\nlarger classiﬁer weight norms for head classes than tail classes [96],\nwhich makes the linear classiﬁer easily biased to dominant classes.\nTo address this, recent studies [49], [113] proposed to use the\nscale-invariant cosine classiﬁer p = φ((\nw⊤f\n∥w∥∥f∥)/τ + b), where\nboth the classiﬁer weights and sample features are normalized.\nHere, the temperature τ should be chosen reasonably [164],\nor the classiﬁer performance would be negatively inﬂuenced.\nHowever, normalizing the feature space may harm its representation\nabilities. Therefore, the τ-normalized classiﬁer [32] rectiﬁes the\nimbalance by only adjusting the classiﬁer weight norms through\na τ-normalization procedure. Formally, let ˜w =\nw\n∥w∥τ\n2 , where τ\nis the temperature factor for normalization. When τ = 1, the τ-\nnormalization reduces to L2 normalization, while when τ = 0, no\nscaling is imposed. Note that, the hyper-parameter τ can also be\ntrained with class-balanced sampling, and the resulting classiﬁer\nis named the learnable weight scaling classiﬁer [32]. Another\napproach to address classiﬁer weight imbalance is to use the nearest\nclass mean classiﬁer [32], which ﬁrst computes the mean features\nfor each class on the training set as the classiﬁer, and then conducts\nprediction based on the nearest neighbor algorithm [165].\nThere are also some more complicated classiﬁer designs\nbased on hierarchical classiﬁcation, causal inference or classiﬁer\nknowledge transfer. For example, Realistic Taxonomic Classiﬁer\n(RTC) [105] proposed to address class imbalance with hierarchical\nclassiﬁcation by mapping images into a class taxonomic tree\nstructure, where the hierarchy is deﬁned by a set of classiﬁcation\nnodes and node relations. Different samples are adaptively classiﬁed\nat different hierarchical levels, where the level at which the\nprediction is made depends on the sample classiﬁcation difﬁculty\nand the classiﬁer conﬁdence. Such a design favors correct decisions\nat intermediate levels rather than incorrect decisions at the leaves.\nCausal classiﬁer [45] resorted to causal inference for keeping\nthe good and removing the bad momentum causal effects in long-\ntailed learning. The good causal effect indicates the beneﬁcial\nfactor that stabilizes gradients and accelerates training, while the\nbad causal effect indicates the accumulated long-tailed bias that\nleads to poor tail-class performance. To better approximate the\nbias information, the causal classiﬁer applies a multi-head strategy\nto divide the channel (or dimensions) of model weights and data\nfeatures equally into K groups. Formally, the causal classiﬁer\ncalculates the original logits by p = φ( τ\nK\nPK\nk=1\n(wk)⊤f k\n(∥wk∥+γ)∥f k∥),\nwhere τ is the temperature factor and γ is a hyper-parameter.\nThis classiﬁer is essentially the cosine classiﬁer when γ = 0.\nIn inference, the causal classiﬁer removes the bad causal ef-\nfect by subtracting the prediction when the input is null, i.e.,\np = φ( τ\nK\nPK\nk=1\n(wk)⊤f k\n(∥wk∥+γ)∥f k∥−α cos(xk, ˆdk)(wk)⊤ˆdk\n∥wk∥+γ\n), where ˆd\nis the unit vector of the exponential moving average features, and\nα is a trade-off parameter to control the direct and indirect effects.\nMore intuitively, the classiﬁer records the bias by computing\nthe exponential moving average features during training, and\nthen removes the bad causal effect by subtracting the bias from\nprediction logits during inference.\nGIST classiﬁer [107] seeks to transfer the classiﬁer geometric\nstructure of head classes to tail classes. Speciﬁcally, the GIST\nclassiﬁer consists of a class-speciﬁc weight center (for encoding\nthe class location) and a set of displacements (for encoding the\nclass geometry). By exploiting the relatively large displacements\nfrom head classes to enhance tail-class weight centers, the GIST\nclassiﬁer is able to obtain better performance on tail classes.\nDiscussions. These methods address the imbalance at the\nclassiﬁer level. Note that these classiﬁers are exclusive to each\nother, and the choice of classiﬁers also inﬂuences other long-tailed\nmethods. For example, the effects of data mixup are different for\nthe linear classiﬁer and the cosine classiﬁer. Hence, when exploring\nnew long-tailed approaches, it is better to ﬁrst determine which\nclassiﬁer is used. Generally, the cosine classiﬁer or the learnable\nweight-scaling classiﬁer are recommended, as they are empirically\nrobust to the imbalance and also easy to use. Moreover, when\ndesigning feature prototype-based methods, the nearest class mean\nclassiﬁer is a good choice. More complicated classiﬁer designs (e.g.,\nRTC, Causal and GIST) can also be considered if real applications\nare complex and hard to handle.\n3.3.3\nDecoupled Training\nDecoupled training decouples the learning procedure into repre-\nsentation learning and classiﬁer training. Here, decoupled training\nrepresents a general paradigm for long-tailed learning instead of\na speciﬁc approach. Decoupling [32] was the pioneering work\nto introduce such a two-stage decoupled training scheme. It\nempirically evaluated different sampling strategies (mentioned\nin Section 3.1.1) for representation learning in the ﬁrst stage,\nand then evaluated different classiﬁer training schemes by ﬁxing\nthe trained feature extractor in the second stage. In the classiﬁer\nlearning stage, there are also four methods, including classiﬁer\nre-training with class-balanced sampling, the nearest class mean\nclassiﬁer, the τ-normalized classiﬁer, and the learnable weight-\nscaling classiﬁer. The main observations are twofold: (1) random\nsampling is surprisingly the best strategy for representation\nlearning in decoupled training; (2) re-adjusting the classiﬁer leads\nto signiﬁcant performance improvement in long-tailed recognition.\nFollowing this scheme, KCL [13] empirically observed that\na balanced feature space is beneﬁcial to long-tailed learning.\nTherefore, it innovated the decoupled training scheme by devel-\noping a k-positive contrastive loss to learn a more class-balanced\nand class-discriminative feature space, which leads to better long-\ntailed learning performance. Moreover, MiSLAS [114] empirically\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\nTraining Class\nDistributions\n(a) Standard training\nTraining Class\nDistributions\n(b) BBN [44], TLML [46], SimCAL [34]\nTraining Class\nDistributions\n(c) BAGS [56], LFME [103]\nTraining Class\nDistributions\n(d) ACE [124], ResLT [125]\nTraining Class\nDistributions\n(e) RIDE [17]\nTraining Class\nDistributions\n(f) SADE [30]\nFig. 3. Illustrations of existing ensemble-based long-tailed methods. Compared to standard training (a), the trained experts by ensemble-based\nmethods (b-f) may have different expertise, e.g., being skilled in different class distributions or different class subsets (indicated by different colors).\nFor example, BBN and SimCAL train two experts for simulating the original long-tailed and uniform distributions so that they can address the two\ndistributions well. BAGS, LFME, ACE, and ResLT train multiple experts by sampling class subsets, so that different experts can particularly handle\ndifferent sets of classes. SADE directly trains multiple experts to separately simulate long-tailed, uniform and inverse long-tailed class distributions\nfrom a stationary long-tailed distribution, which enables it to handle test sets with agnostic class distributions based on self-supervised aggregation.\nobserved that data mixup is beneﬁcial to features learning but has\na negative/negligible effect on classiﬁer training under the two-\nstage decoupled training scheme. Therefore, MiSLAS proposed\nto enhance the representation learning with data mixup in the ﬁrst\nstage, while applying a label-aware smoothing strategy for better\nclassiﬁer generalization in the second stage.\nSeveral recent studies particularly enhanced the classiﬁer\ntraining stage. For example, OFA [101] innovated the classiﬁer\nre-training through tail-class feature augmentation. SimCal [34] en-\nhanced the classiﬁer training stage by calibrating the classiﬁcation\nhead with a novel bi-level class-balanced sampling strategy for long-\ntailed instance segmentation. DisAlign [29] innovated the classiﬁer\ntraining with a new adaptive logit adjustment strategy. Very recently,\nDT2 [61] applied the scheme of decoupled training to the scene\ngraph generation task, which demonstrates the effectiveness of\ndecoupled training in handling long-tailed visual relation learning.\nDiscussions. Decoupled training methods resolve the class\nimbalance issue at both the feature and classiﬁer levels. Under ideal\nconditions, combining different methods can lead to better long-\ntailed performance, e.g., using self-supervised pre-training [13]\nand mixup augmentation [114] together for better representation\nlearning, and applying label-aware smoothing [114] and tail-class\nfeature augmentation [101] together for better classiﬁer tuning.\nTherefore, it is recommended to use MiSLAS [114] as a base\nmethod and use different tricks on it. Note that some representation\nmethods are also competing to each other, e.g., different sampling\nmethods for representation learning [32].\nThe classiﬁer learning stage does not introduce too many\ncomputation costs but can lead to signiﬁcant performance gains.\nThis makes decoupled training attract increasing attention. One\ncritique is that the accumulated training stages make decoupled\ntraining less practical to be integrated with existing well-formulated\nmethods for other long-tailed problems like object detection and\ninstance segmentation. Despite this, the idea of decoupled training\nis conceptually simple and thus can be easily used to design new\nmethods for resolving various long-tailed problems, like DT2 [61].\n3.3.4\nEnsemble Learning\nEnsemble learning based methods strategically generate and\ncombine multiple network modules (namely, multiple experts)\nto solve long-tailed visual learning problems. We summarize the\nmain schemes of existing ensemble-based methods in Fig. 3, which\nwill be detailed as follows.\nBBN [44] proposed to use two network branches, i.e., a conven-\ntional learning branch and a re-balancing branch (cf. Table 3(b)),\nto handle long-tailed recognition. To be speciﬁc, the conventional\nlearning branch applies uniform sampling to simulate the original\nlong-tailed training distribution, while the re-balancing branch\napplies a reversed sampler to sample more tail-class samples in each\nmini-batch for improving tail-class performance. The predictions of\ntwo branches are dynamically combined during training, so that the\nlearning focus of BBN gradually changes from head classes to tail\nclasses. Following BBN, LTML [46] applied the bilateral-branch\nnetwork scheme to solve long-tailed multi-label classiﬁcation. To\nbe speciﬁc, LTML trains each branch using the sigmoid cross-\nentropy loss for multi-label classiﬁcation and enforces a logit\nconsistency loss to improve the consistency of the two branches.\nSimilarly, SimCal [34] explored a dual classiﬁcation head scheme,\na conventional classiﬁcation head and a calibrated classiﬁcation\nhead, to address long-tail instance segmentation. Based on a new\nbi-level sampling strategy, the calibrated classiﬁcation head is able\nto improve the performance on tail classes, while the original head\naims to maintain the performance on head classes.\nInstead of bilateral branches, BAGS [56] explored a multi-\nhead scheme to address long-tailed object detection. Speciﬁcally,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\nBAGS took inspiration from an observation that learning a more\nuniform distribution with fewer samples is sometimes easier than\nlearning a long-tailed distribution with more samples. Therefore,\nBAGS divides classes into several groups, where the classes in each\ngroup have a similar number of training data. Then, BAGS applies\nmultiple classiﬁcation heads for prediction, where different heads\nare trained on different class groups (cf. Table 3(c)). In this way,\neach classiﬁcation head performs the softmax operation on classes\nwith a similar number of training data, thus avoiding the negative\ninﬂuence of class imbalance. Moreover, BAGS also introduces a\nlabel of “other classes” into each group to alleviate the contradiction\namong different heads. Similar to BAGS, LFME [103] divides the\nlong-tailed dataset into several subsets with smaller class imbalance\ndegrees, and trains multiple experts with different sample subsets.\nBased on these experts, LFME then learns a uniﬁed student model\nusing adaptive knowledge distillation from multiple teachers.\nInstead of division into several balanced sub-groups, ACE [124]\ndivides classes into several skill-diverse subsets: one subset contains\nall classes; one contains middle and tail classes; another one has\nonly tail classes (cf. Table 3(d)). ACE then trains multiple experts\nwith various class subsets, so that different experts have speciﬁc and\ncomplementary skills. Moreover, considering that various subsets\nhave different sample numbers, ACE also applies a distributed-\nadaptive optimizer to adjust the learning rate for different experts.\nA similar idea of ACE was also explored in ResLT [125].\nInstead of dividing the dataset, RIDE [17] uses all training\nsamples to train multiple experts with softmax loss respectively\n(cf. Table 3(e)), and enforces a KL-divergence based loss to improve\nthe diversity among various experts. Following that, RIDE applies\nan expert assignment module to improve computing efﬁciency.\nNote that training each expert with the softmax loss independently\nboosts the ensemble performance on long-tailed learning a lot.\nHowever, the learned experts by RIDE are not diverse enough.\nSelf-supervised Aggregation of Diverse Experts (SADE) [30]\nexplored a new multi-expert scheme to handle test-agnostic\nlong-tailed recognition, where the test class distribution can be\neither uniform, long-tailed or even inversely long-tailed. To be\nspeciﬁc, SADE developed a novel spectrum-spanned multi-expert\nframework (cf. Table 3(f)), and innovated the expert training\nscheme by introducing diversity-promoting expertise-guided losses\nthat train different experts to handle different class distributions,\nrespectively. In this way, the learned experts are more diverse than\nRIDE, leading to better ensemble performance, and integratedly\nspan a wide spectrum of possible class distributions. In light of this,\nSADE further introduced a self-supervised learning method, namely\nprediction stability maximization, to adaptively aggregate experts\nat test time for better handling unknown test class distribution.\nDiscussions. These ensemble-based methods address the class\nimbalance at the model level. As they require particular manners\nfor multi-model design and training (cf. Fig. 3), they are exclusive\nto each other and usually cannot be used together. More speciﬁcally,\nthe methods with bilateral branches like BBN and TLML only have\ntwo experts, whose empirical performance has been shown worse\nthan the approaches with more experts. Moreover, the methods with\nexperts trained on class subsets like BAGS and ACE may suffer\nfrom expert inconsistency in terms of different label spaces, which\nmakes the aggregation of experts difﬁcult and may lead to poor\nperformance in real applications. Instead, RIDE trains multiple\nexperts with all samples but the resulting multiple experts are not\ndiverse enough. In contrast, SADE is able to train skill-diverse\nexperts with the same label space, and thus is recommended for\nreal applications. One concern of these ensemble-based methods is\nthat they generally lead to higher computational costs due to the\nuse of multiple experts. Such a concern, however, can be alleviated\nby using a shared feature extractor. Moreover, efﬁciency-oriented\nexpert assignment and knowledge distillation strategies [17], [103]\ncan also reduce computational complexity.\n3.3.5\nSummary\nModule improvement based methods seek to address long-tailed\nproblems by improving network modules. Speciﬁcally, represen-\ntation learning and classiﬁer design are fundamental problems\nof deep learning, being worth further exploring for long-tailed\nproblems. Both representation learning and classiﬁer design are\ncomplementary to decoupled training. The scheme of decoupled\ntraining is conceptually simple and can be easily used to design\nnew methods for resolving real long-tailed applications. In addition,\nensemble-based methods, thanks to the aggregation of multiple\nexperts, are able to achieve better long-tailed performance without\nsacriﬁcing the performance on any class subsets, e.g., head classes.\nSince all classes are important, such a superiority enables ensemble-\nbased methods to be a better choice for real applications compared\nto existing class re-balancing methods that usually improve tail-\nclass performance at the cost of lower head-class performance.\nHere, both ensemble-based methods and decoupled training require\nspeciﬁc model training and design manners, so it is not easy to use\nthem together unless very careful design.\nNote that most module improvement methods are developed\nbased on fundamental class re-balancing methods. Moreover,\nmodule improvement methods are complementary to information\naugmentation methods. Using them together can usually achieve\nbetter performance for real-world long-tailed applications.\n4\nEMPIRICAL STUDIES\nThis section empirically analyzes existing long-tailed learning\nmethods. To begin with, we introduce a new evaluation metric.\n4.1\nNovel Evaluation Metric\nThe key goal of long-tailed learning is to handle the class imbalance\nfor better model performance. Therefore, the common evaluation\nprotocol [13], [22] is directly using the top-1 test accuracy (denoted\nby At) to judge how well long-tailed methods perform and which\nmethod handles class imbalance better. Such a metric, however,\ncannot accurately reﬂect the relative superiority among different\nmethods when handling class imbalance, as the top-1 accuracy\nis also inﬂuenced by other factors apart from class imbalance.\nFor example, long-tailed methods like ensemble learning (or data\naugmentation) also improve the performance of models, trained\non a balanced training set. In such cases, it is hard to tell if the\nperformance gain is from the alleviation of class imbalance or from\nbetter network architectures (or more data information).\nTo better evaluate the method effectiveness in handling class\nimbalance, we explore a new metric, namely relative accuracy\nAr, to alleviate the inﬂuence of unnecessary factors in long-\ntailed learning. To this end, we ﬁrst compute an empirically upper\nreference accuracy Au = max(Av, Ab), which is the maximal\nvalue between the vanilla accuracy Av of the backbone trained\non a balanced training set with cross-entropy and the balanced\naccuracy Ab of the model trained on a balanced training set with\nthe corresponding long-tailed method. Here, the balanced training\nset is a variant of the long-tailed training set, where the total data\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n13\nnumber is similar but each class has the same number of data. This\nupper reference accuracy, obtained from the balanced training set, is\nused to alleviate the inﬂuence apart from class imbalance, and then\nthe relative accuracy is deﬁned by Ar = At\nAu . Note that this metric\nis mainly designed for empirical understanding, i.e., to evaluate to\nwhat extent existing methods resolve the class imbalance. We\nconduct this analysis based on the ImageNet-LT dataset [15],\nwhere a corresponding balanced training set variant can be built by\nsampling from the original ImageNet following [13].\n4.2\nExperimental Settings\nWe then introduce the experimental settings.\nDatasets. We adopt the widely-used ImageNet-LT [15] and\niNaturalist 2018 [23] as the benchmark long-tailed dataset for\nempirical studies. Their dataset statistics can be found in Table 1.\nBesides the performance regarding all classes, we also report\nperformance on three class subsets: Head (more than 100 images),\nMiddle (20∼100 images) and Tail (less than 20 images).\nBaselines. We select long-tailed methods via two criteria: (1)\nthe source codes are publicly available or easy to re-implement; (2)\nthe methods are evaluated on ImageNet-LT in the corresponding\npapers. As a result, more than 20 methods are empirically eval-\nuated in this paper, including baseline (Softmax), class-sensitive\nlearning (Weighted Softmax, Focal loss [54], LDAM [18],\nESQL [19], Balanced Softmax [97], LADE [31]), logit ad-\njustment (UNO-IC [99]), transfer learning (SSP [102]), data\naugmentation (RSG [118]) representation learning (OLTR [15],\nPaCo [121]). classiﬁer design (De-confound [45]), decoupled\ntraining (Decouple-IB-CRT [32], CB-CRT [32], SR-CRT [32],\nPB-CRT [32], MiSLAS [114]), ensemble learning (BBN [44],\nLFME [103], RIDE [17], ResLT [125], SADE [30]).\nImplementation details. We implement all experiments in\nPyTorch. Following [17], [31], [32], we use ResNeXt-50 for\nImageNet-LT and and ResNet-50 for iNaturalist 2018 as the\nnetwork backbones for all methods. We conduct model training\nwith the SGD optimizer based on batch size 256, momentum 0.9\nand weight decay factor 0.0005, and learning rate 0.1 (linear LR\ndecay). For method-related hyper-parameters, we set the values by\neither directly following the original papers or manual tuning if the\ndefault values perform poorly. Moreover, we use the same basic\ndata augmentation (i.e., random resize and crop to 224, random\nhorizontal ﬂip, color jitter, and normalization) for all methods.\n4.3\nResults on ImageNet-LT\nObservations on all classes. Table 4 and Fig. 4 report the average\nperformance of ImageNet-LT over all classes. From these results,\nwe have several observations on the overall method progress and\ndifferent method types. As shown in Table 4, almost all long-\ntailed methods perform better than the Softmax baseline in terms\nof accuracy, which demonstrates the effectiveness of long-tailed\nlearning. Even so, there are two methods performing slightly worse\nthan Softmax, i.e., Decouple-CB-CRT [32] and BBN [44]. We\nspeculate that the poor performance of Decouple-CB-CRT results\nfrom poor representation learning by class-balanced sampling in the\nﬁrst stage of decoupled training (refer to [32] for more empirical\nobservations). The poor results of BBN (based on the ofﬁcial codes)\nmay come from the cumulative learning strategy, which gradually\nadjusts the learning focus from head classes to tail classes; at the\nend of the training, however, it may put too much focus on the\ntail ones. As a result, despite the better tail-class performance, the\n46\n48\n50\n52\n54\n56\n58\n60\nAccuracy\n80\n82\n84\n86\n88\n90\n92\n94\n96\nRelative Accuracy\nSoftmax\nWeighted Softmax\nBalanced Softmax\nLADE\nSSP\nRSG\nPaCo\nMISLAS\nRIDE\nSADE\nFig. 4. Performance trends of long-tailed learning methods in terms\nof accuracy and relative accuracy under 200 epochs. Here, the shape\nof ◦indicates the softmax baseline; □indicates class re-balancing; △\nand ♦are information augmentation and module improvement methods,\nrespectively. Different colors represent different methods.\nmodel accuracy on head classes drops signiﬁcantly (c.f. Table 5),\nleading to worse average performance.\nIn addition to accuracy, we also evaluate long-tailed methods\nbased on upper reference accuracy (UA) and relative accuracy\n(RA). Table 4 shows that most methods have the same UA as the\nbaseline model, but there are still some methods having higher\nUA, e.g., SSP, MiSLAS, and SADE. For these methods, the\nperformance improvement comes not only from the alleviation of\nclass imbalance, but also from other factors, like data augmentation\nor better network architectures. Therefore, simply using accuracy\nfor evaluation is not comprehensive enough, while the proposed RA\nmetric provides a good complement as it alleviates the inﬂuences of\nfactors apart from class imbalance. For example, MiSLAS, based\non data mixup, has higher accuracy than Balanced Softmax under\n90 training epochs, but it also has higher UA. As a result, the\nrelative accuracy of MiSLAS is lower than Balanced Softmax,\nwhich means that Balanced Softmax alleviates class imbalance\nbetter than MiSLAS under 90 training epochs.\nAlthough some recent high-accuracy methods have lower RA,\nthe overall development trend of long-tailed learning is still positive,\nas shown in Fig. 4. Such a performance trend demonstrates that\nrecent studies of long-tailed learning make real progress. Moreover,\nthe RA of the state-of-the-art SADE is 93.0, which implies that\nthere is still room for improvement in the future.\nWe also evaluate the inﬂuence of different training epochs (i.e.,\n90 and 200) in Table 4. Overall, training with 200 epochs leads to\nbetter performance for most long-tailed methods, because sufﬁcient\ntraining enables deep models to ﬁt data better and learn better\nvisual representations. However, there are also some methods that\nperform better when only training 90 epochs, e.g., De-confound\nand Decouple-CB-CRT. We speculate that, for these methods,\n90 epochs are enough to train models well, while training more\nepochs does not bring additional beneﬁts but increases the training\ndifﬁculties since it also inﬂuences the learning rate decay scheme.\nObservations on different method types. We next analyze\ndifferent method types in Table 4. To begin with, almost all class\nre-balancing (CR) methods all beneﬁcial to long-tailed learning\nperformance, compared to the baseline model. Among them, LADE,\nBalanced Softmax and LDAM achieve state-of-the-art. Moreover,\nFocal loss was initially proposed to handle object detection [54].\nHowever, when handling a highly large number of classes (e.g.,\n1,000 in ImageNet-LT), Focal loss cannot perform well and only\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n14\nTABLE 4\nResults on ImageNet-LT in terms of accuracy (Acc), upper reference\naccuracy (UA), relative accuracy (RA) under 90 or 200 training epochs. In\nthis table, CR, IA and MI indicate class re-balancing, information\naugmentation and module improvement, respectively.\nType\nMethod\n90 epochs\n200 epochs\nAcc\nUA\nRA\nAcc\nUA\nRA\nBaseline\nSoftmax\n45.5\n57.3\n79.4\n46.8\n57.8\n81.0\nCR\nWeighted Softmax\n47.9\n57.3\n83.6\n49.1\n57.8\n84.9\nFocal loss [54]\n45.8\n57.3\n79.9\n47.2\n57.8\n81.7\nLDAM [18]\n51.1\n57.3\n89.2\n51.1\n57.8\n88.4\nESQL [19]\n47.3\n57.3\n82.5\n48.0\n57.8\n83.0\nUNO-IC [99]\n45.7\n57.3\n81.4\n46.8\n58.6\n79.9\nBalanced Softmax [97]\n50.8\n57.3\n88.7\n51.2\n57.8\n88.6\nLADE [31]\n51.5\n57.8\n89.1\n51.6\n57.8\n89.3\nIA\nSSP [102]\n53.1\n59.6\n89.1\n53.3\n59.9\n89.0\nRSG [118]\n49.6\n57.3\n86.7\n52.9\n57.8\n91.5\nMI\nOLTR [15]\n46.7\n57.3\n81.5\n48.0\n58.4\n82.2\nPaCo [121]\n52.7\n58.7\n89.9\n54.4\n59.6\n91.3\nDe-confound [45]\n51.8\n57.7\n89.8\n51.3\n57.8\n88.8\nDecouple-IB-CRT [32]\n49.9\n57.3\n87.1\n50.3\n58.1\n86.6\nDecouple-CB-CRT [32]\n44.9\n57.3\n78.4\n43.0\n57.8\n74.4\nDecouple-SR-CRT [32]\n49.3\n57.3\n86.0\n48.5\n57.8\n83.9\nDecouple-PB-CRT [32]\n48.4\n57.3\n84.5\n48.1\n57.8\n83.2\nMiSLAS [114]\n51.4\n58.3\n88.2\n53.4\n59.7\n89.4\nBBN [44]\n41.2\n57.3\n71.9\n44.7\n57.8\n77.3\nLFME [103]\n47.0\n57.3\n82.0\n48.0\n57.8\n83.0\nResLT [125]\n51.6\n57.3\n90.1\n53.2\n58.1\n91.6\nRIDE [17]\n55.5\n60.2\n92.2\n56.1\n60.9\n92.1\nSADE [30]\n57.3\n61.9\n92.6\n58.8\n63.2\n93.0\nTABLE 5\nAccuracy results on ImageNet-LT regarding head, middle and tail\nclasses under 90 or 200 training epochs. In this table, WS indicates\nweighed softmax and BS indicates balanced softmax. The types of\nmethods are the same to Table 4.\nMethod\n90 epochs\n200 epochs\nHead\nMiddle\nTail\nHead\nMiddle\nTail\nSoftmax\n66.5\n39.0\n8.6\n66.9\n40.4\n12.6\nWS\n66.3\n42.2\n15.6\n57.9\n46.2\n34.0\nFocal loss [54]\n66.9\n39.2\n9.2\n67.0\n41.0\n13.1\nLDAM [18]\n62.3\n47.4\n32.5\n60.0\n49.2\n31.9\nESQL [19]\n62.5\n44.0\n15.7\n63.1\n44.6\n17.2\nUNO-IC [99]\n66.3\n38.7\n9.3\n67.0\n40.3\n12.7\nBS [97]\n61.7\n48.0\n29.9\n62.4\n47.7\n32.1\nLADE [31]\n62.2\n48.6\n31.8\n63.1\n47.7\n32.7\nSSP [102]\n65.6\n49.6\n30.3\n67.3\n49.1\n28.3\nRSG [118]\n68.7\n43.7\n16.2\n65.0\n49.4\n31.1\nOLTR [15]\n58.2\n45.5\n19.5\n62.9\n44.6\n18.8\nPaCo [121]\n59.7\n51.7\n36.6\n63.2\n51.6\n39.2\nDe-confound [45]\n63.0\n48.5\n31.4\n64.9\n46.9\n28.1\nIB-CRT [32]\n62.6\n46.2\n26.7\n64.2\n46.1\n26.0\nCB-CRT [32]\n62.4\n39.3\n14.9\n60.9\n36.9\n13.5\nSR-CRT [32]\n64.1\n43.9\n19.5\n66.0\n42.3\n18.0\nPB-CRT [32]\n63.9\n45.0\n23.2\n64.9\n43.1\n20.6\nMiSLAS [114]\n62.1\n48.9\n32.6\n65.3\n50.6\n33.0\nBBN [44]\n40.0\n43.3\n40.8\n43.3\n45.9\n43.7\nLFME [103]\n60.6\n43.5\n22.0\n64.1\n42.3\n22.8\nResLT [125]\n57.8\n50.4\n40.0\n61.6\n51.4\n38.8\nRIDE [17]\n66.9\n52.3\n34.5\n67.9\n52.3\n36.0\nSADE [30]\n65.3\n55.2\n42.0\n67.2\n55.3\n40.0\nleads to marginal improvement. In LDAM, there is a deferred\nre-balancing optimization schedule in addition to the LDAM loss.\nSimply learning with the LDAM loss without the deferred scheme\ncannot achieve promising results. In addition, as shown in Table 4,\nthe upper reference accuracy of most class-sensitive methods is\nthe same, so their relative accuracy is positively correlated to\naccuracy. Hence, the accuracy improvement in this method type\ncan accurately reﬂect the alleviation of class imbalance.\nIn information augmentation (IA), both SSP (transfer learning)\nand RSG (data augmentation) help to handle long-tailed imbalance.\nAlthough SSP also improves upper reference accuracy, its relative\naccuracy is increased more signiﬁcantly, implying that the perfor-\nmance gain mostly comes from handling the class imbalance. In\nmodule improvement (MI), all methods contribute to addressing\nthe imbalance. By now, the state of the art is ensemble-based long-\ntailed methods, i.e., SADE and RIDE, in terms of both accuracy\nand relative accuracy. Although ensemble learning also improves\nupper reference accuracy, the performance gain from handling\nimbalance is more signiﬁcant, leading to higher relative accuracy.\nResults on different class subsets. We then report the\nperformance in terms of different class subsets. As shown in\nTable 5, almost all methods improve tail-class and middle-class\nperformance at the cost of lower head-class performance. The\nhead classes, however, are also important in long-tailed learning,\nso it is necessary to improve long-tailed performance without\nsacriﬁcing the performance of the head. Potential solutions include\ninformation augmentation and ensemble learning, e.g., SSP and\nSADE. By comparing both Tables 4 and 5, one can ﬁnd that the\noverall performance gain largely depends on the improvement of\nmiddle and tail classes; hence, how to improve their performance\nis still the most important goal of long-tailed learning in the future.\nBy now, SADE [30] achieves the best overall performance\nin terms of accuracy and RA (c.f. Table 4), but SADE does not\nperform state-of-the-art on all class subsets (c.f. Table 5). For\nTABLE 6\nResults on iNaturalist 2018 in terms of accuracy under 200 training\nepochs. In this table, CR, IA and MI indicate class re-balancing,\ninformation augmentation and module improvement, respectively.\nType\nMethod\nHead\nMiddle\nTail\nAll\nBaseline\nSoftmax\n75.3\n66.4\n60.4\n64.9\nCR\nWeighted Softmax\n66.5\n68.0\n69.2\n68.3\nFocal loss [54]\n58.8\n66.5\n66.8\n66.6\nLDAM [18]\n57.4\n62.7\n63.8\n62.8\nBalanced Softmax [97]\n70.9\n70.7\n70.4\n70.6\nLADE [31]\n70.1\n69.5\n69.9\n69.7\nIA\nSSP [102]\n72.0\n68.9\n66.3\n68.2\nRSG [118]\n70.7\n69.9\n69.3\n70.0\nMI\nPaCo [121]\n68.5\n72.0\n71.8\n71.6\nDecouple-IB-CRT [32]\n73.2\n68.8\n65.1\n67.8\nDecouple-IB-LWS [32]\n71.3\n69.2\n68.1\n69.0\nMiSLAS [114]\n71.7\n71.5\n69.7\n70.7\nResLT [125]\n67.5\n69.2\n70.1\n69.4\nRIDE [17]\n71.5\n70.0\n71.6\n71.8\nSADE [30]\n74.4\n72.5\n73.1\n72.9\nexample, when training 200 epochs, the head-class performance of\nSADE is worse than RIDE and its tail-class performance is worse\nthan BBN. To summarize, the higher average performance of SADE\nimplies that the key to obtaining better long-tailed performance is\na better trade-off among all classes. In summary, the current best\npractice for deep long-tailed learning is using ensemble learning\nand class re-balancing, simultaneously.\n4.4\nResults on iNaturalist 2018\niNaturalist 2018 is not a synthetic dataset sampled from a larger\ndata pool, so we cannot build a corresponding balanced training\nset with a similar data size for it through sampling. As a result, it\nis infeasible to compute relative accuracy for it, so we only report\nthe performance in terms of accuracy. As shown in Table 6, most\nobservations are similar to those on ImageNet-LT. For example,\nmost long-tailed methods outperform Softmax. Although LDAM\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n15\nTABLE 7\nAnalysis of class re-balancing on ImageNet-LT based on ResNeXt-50.\nLA indicates logit post-adjustment, while re-sampling indicates\nclass-balance re-sampling [32]. BS indicates Balanced Softmax [97].\nLoss\nLA\nRe-sampling\nHead\nMiddle\nTail\nAll\nSoftmax\n66.9\n40.4\n12.6\n46.8\nBS [97]\n62.4\n47.7\n32.1\n51.2\n47.2\n45.5\n48.5\n46.6\n57.6\n47.5\n30.6\n49.1\n42.6\n46.6\n43.6\n44.6\nTABLE 8\nAnalysis of whether transfer-based methods (e.g., SSP\npre-training [102]) are beneﬁcial to other types of long-tailed learning.\nHere, we use ResNet-50 as the backbone since SSP provides an\nopen-source self-supervised pre-trained ResNet-50.\nMethod\nSSP pre-training [102]\nHead\nMiddle\nTail\nAll\nSoftmax\n64.7\n35.9\n7.1\n43.1\nRe-sampling [32]\n51.7\n48.2\n32.4\n47.4\n63.5\n45.3\n20.5\n49.0\nBS [97]\n61.7\n47.8\n28.5\n50.5\n62.9\n50.0\n30.4\n52.3\nDecouple [32]\n64.2\n46.1\n26.0\n50.3\n67.3\n49.1\n28.3\n53.3\nSADE [30]\n66.0\n56.1\n41.0\n57.8\n66.3\n56.9\n42.4\n58.6\n(based on the ofﬁcial codes) performs slightly worse, its tail-class\nperformance is better than the baseline, which demonstrates that\nLDAM can alleviate the class imbalance. However, its head-class\nperformance drops signiﬁcantly due to the head-tail trade-off, thus\nleading to poor overall performance. In addition, the current state-\nof-the-art method is SADE [30] in terms of accuracy, which further\ndemonstrates the superiority of ensemble-based methods over other\ntypes of methods. All these baselines, except data augmentation\nbased methods, adopt only basic augmentation operations. If we\nadopt stronger data augmentation and longer training, their model\nperformance can be further improved.\n4.5\nAnalysis\nWe next analyze the relationship between various types of methods.\nDiscussions on class re-balancing. Class re-balancing has\nthree subtypes of methods, i.e., re-sampling, class-sensitive learning\nand logit adjustment. Although they have the same goal for re-\nbalancing classes, they are exclusive to each other to some degree.\nAs shown in Table 7, Balanced Softmax (class-sensitive learning)\nalone greatly outperforms Softmax. However, when further using\nlogit adjustment, it performs only comparably to Softmax. The\nreason is that the trained model by class-sensitive learning is\nalready relatively class-balanced, so further using logit adjustment\nto post-adjust model inference will cause the predictions to become\nbiased again and result in inferior performance. The performance\nis even worse when further combining class-balanced re-sampling.\nTherefore, simply combining existing class re-balancing without a\ncareful design cannot lead to better performance.\nDiscussions on the relationship between pre-training and\nother long-tailed methods. As mentioned in Section 3.2, model\npre-training is a transfer-based scheme for long-tailed learning.\nIn this experiment, we analyze whether it is beneﬁcial to other\nlong-tailed paradigms. As shown in Table 8, SSP pre-training\nbrings consistent performance gains to class re-balancing (class-\nbalanced sampling [32] and BS [97]) and module improvement\nTABLE 9\nAnalysis of whether augmentation methods (e.g., RandAugment) are\nbeneﬁcial to other types of long-tailed learning, based on ResNeXt-50.\nMethod\nRandAugment [166]\nHead\nMiddle\nTail\nAll\nSoftmax\n66.9\n40.4\n12.6\n46.8\nBS [97]\n62.4\n47.7\n32.1\n51.2\n64.1\n50.4\n32.3\n53.2\nPaCo [121]\n63.2\n51.6\n39.2\n54.4\n63.7\n56.6\n39.2\n57.0\nDe-confound [45]\n64.9\n46.9\n28.1\n51.3\n66.1\n50.5\n32.2\n54.0\nSADE [30]\n67.2\n55.3\n40.0\n58.8\n67.3\n60.4\n46.4\n61.2\nTABLE 10\nThe decoupled training performance of various class-sensitive losses\nunder 200 training epochs on ImageNet-LT. Here, “Joint” indicates\none-stage end-to-end joint training; “NCM” is the nearest class mean\nclassiﬁer [32]; “CRT” represents class-balanced classiﬁer re-training [32];\n“LWS” means learnable weight scaling [32]. Moreover, BS indicates the\nbalanced softmax method [97].\nTest Dist.\nAccuracy on all classes\nAccuracy on head classes\nJoint\nNCM\nCRT\nLWS\nJoint\nNCM\nCRT\nLWS\nSoftmax\n46.8\n50.2\n50.2\n50.8\n66.9\n63.5\n65.0\n64.6\nFocal loss [54]\n47.2\n50.7\n50.7\n51.5\n67.0\n62.6\n64.5\n64.3\nESQL [19]\n48.0\n49.8\n50.6\n50.5\n63.1\n60.2\n64.0\n63.3\nBS [97]\n51.2\n50.4\n50.6\n51.1\n62.4\n62.4\n64.9\n64.3\nTest Dist.\nAccuracy on middle classes\nAccuracy on tail classes\nJoint\nNCM\nCRT\nLWS\nJoint\nNCM\nCRT\nLWS\nSoftmax\n40.4\n45.8\n45.3\n46.1\n12.6\n28.1\n25.5\n28.2\nFocal loss [54]\n41.0\n47.0\n46.4\n47.3\n13.1\n30.1\n26.9\n30.2\nESQL [19]\n44.6\n46.6\n46.5\n46.1\n17.2\n31.1\n27.1\n29.5\nBS [97]\n47.7\n46.8\n46.1\n46.7\n32.1\n29.1\n26.2\n29.4\n(Decouple [32] and SADE [30]). We thus conclude that transfer-\nbased methods are complementary to other long-tailed paradigms.\nDiscussions on the relationship between data augmentation\nand other long-tailed methods. We then analyze whether data\naugmentation methods are beneﬁcial to other long-tailed paradigms.\nAs shown in Table 9, RandAugment [166] brings consistent\nperformance improvement to BS (a class re-balancing method),\nPaCo (representation learning), De-confound (classiﬁer design)\nand SADE (ensemble learning). Such a result demonstrates\nthat augmentation-based methods are complementary to other\nparadigms of long-tailed learning.\nDiscussions on class-sensitive losses in the decoupled train-\ning scheme. We further evaluate the performance of different class-\nsensitive learning losses on the decoupled training scheme [32].\nIn the ﬁrst stage, we use different class-sensitive learning losses\nto train the model backbone for learning representations, while\nin the second stage, we use four different strategies for classiﬁer\ntraining [32], i.e., joint training without re-training, the nearest\nclass mean classiﬁer (NCM), class-balanced classiﬁer re-training\n(CRT), and learnable weight scaling (LWS). As shown in Table 10,\ndecoupled training can further improve the overall performance of\nmost class-sensitive methods with joint training, except BS. Among\nthese methods, BS performs the best under joint training, but the\nothers perform comparably to BS under decoupled training. Such\nresults are particularly interesting, as they imply that although these\nclass-sensitive losses perform differently under joint training, they\nessentially learn the similar quality of feature representations. The\nworse overall performance of BS under decoupled training than\njoint training may imply that BS has conducted class re-balancing\nvery well; further using classiﬁer re-training for re-balancing does\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16\nnot bring additional beneﬁts but even degenerates the consistency\nof network parameters by end-to-end joint training.\n4.6\nSummary of Empirical Observations\nWe then summarize main take-home messages from our empirical\nstudies. First, we analyze to what extent existing long-tailed\nmethods resolve the class imbalance in terms of relative accuracy,\nand conﬁrm that existing research is making positive progress in\nresolving class imbalance instead of just chasing state-of-the-art\nperformance through tricks. Second, we determine the relative\nperformance of existing long-tailed methods in a uniﬁed setup,\nand ﬁnd that ensemble-based methods are the current state-of-\nthe-art. Third, we analyze method performance on various class\nsubsets, and ﬁnd that most methods improve tail-class performance\nat the cost of lower head-class performance. Considering that\nall classes are important in long-tailed learning, it is worth\nexploring how to improve all classes at the same time in the\nfuture. Fourth, we empirically show that the three subtypes of class\nre-balancing are exclusive to each other to some degree. Moreover,\ninformation augmentation methods are complementary to other\nlong-tailed paradigms. Lastly, by evaluating class-sensitive learning\non the decoupled training scheme, we ﬁnd class re-balancing and\ndecoupled training play an interchangeable role in resolving class\nimbalance. Moreover, the representations learned by different class-\nsensitive losses perform similarly under decoupled training.\n5\nFUTURE DIRECTIONS\nIn this section, we identify several future research directions for\ndeep long-tailed learning.\nTest-agnostic long-tailed learning. Existing long-tailed learn-\ning methods generally hypothesize a balanced test class distribution.\nThe practical test distribution, however, often violates this hypoth-\nesis (e.g., being long-tailed or even inversely long-tailed), which\nmay lead existing methods to fail in real-world applications. To\novercome this limitation, LADE [31] relaxes this hypothesis by\nassuming that the test class distribution can be skewed arbitrarily\nbut the prior of test distribution is available. Afterward, SADE [30]\nfurther innovates the task, in which the test class distribution is not\nonly arbitrarily skewed but also unknown. Besides class imbalance,\nthis task poses another challenge, i.e., unidentiﬁed class distribution\nshift between the training and test samples.\nOpen-set long-tailed learning. Real-world samples often have\na long-tailed and open-ended class distribution. Open-set long-\ntailed learning [15], [104] seeks to learn from long-tailed data and\noptimize the classiﬁcation accuracy over a balanced test set that\nincludes head, tail and open classes. There are two main challenges:\n(1) how to share visual knowledge between head and tail classes;\n(2) how to reduce confusion between tail and open classes.\nFederated long-tailed learning. Existing long-tailed studies\ngenerally assume that all training samples are accessible during\nmodel training. However, in real applications, long-tailed training\ndata may be distributed on numerous mobile devices or the Internet\nof Things [167], which requires decentralized training of deep\nmodels. Such a task is called federated long-tailed learning, which\nhas two key challenges: (1) long-tail class imbalance; (2) unknown\nclass distribution shift among the local data of different clients.\nClass-incremental long-tailed learning. In real-world appli-\ncations, long-tailed data may come in a continual and class-\nincremental manner [98], [168], [169]. To deal with this scenario,\nclass-incremental long-tailed learning aims to learn deep models\nfrom class-incremental long-tailed data, suffering two key chal-\nlenges: (1) how to handle long-tailed class imbalance when different\nclasses come sequentially, and the model has no information about\nthe future input regarding classes as well as label frequencies;\n(2) how to overcome catastrophic forgetting of previous class\nknowledge when learning new classes. Such a task setting can also\nbe named continual long-tailed learning.\nMulti-domain long-tailed learning. Current long-tailed meth-\nods generally assume that all long-tailed samples come from the\nsame data marginal distribution. However, in practice, long-tailed\ndata may also get from different domains with distinct data distribu-\ntions [28], [170], e.g., the DomainNet dataset [171]. Motivated by\nthis, multi-domain long-tailed learning seeks to handle both class\nimbalance and domain distribution shift, simultaneously. One more\nchallenging issue may be the inconsistency of class imbalance\namong different domains. In other words, various domains may\nhave different class distributions, which further enlarges the domain\nshift in multi-domain long-tailed learning.\nRobust long-tailed learning. Real-world long-tailed samples\nmay also suffer image noise [113], [172] or label noise [150], [155].\nMost long-tailed methods, however, assume all images and labels\nare clean, leading to poor model robustness in practical applications.\nThis issue would be particularly severe for tail classes, as they have\nvery limited training samples. Inspired by this, robust long-tailed\nlearning seeks to handle the class imbalance and improve model\nrobustness, simultaneously.\nLong-tailed regression. Most existing studies of long-tailed\nvisual learning focus on classiﬁcation, detection and segmentation,\nwhich have discrete labels with class indices. However, many tasks\ninvolve continuous labels, where hard classiﬁcation boundaries\namong classes do not exist. Motivated by this, long-tailed regres-\nsion [173] aims to deal with long-tailed learning with continuous\nlabel space. In such a task, how to simultaneously resolve long-\ntailed class imbalance and handle potential missing data for certain\nlabels remains an open question.\nLong-tailed video learning. Most existing deep long-tailed\nlearning studies focus on the image level, but ignore that the video\ndomain also suffers from the issue of long-tail class imbalance.\nConsidering the additional temporal dimension in video data, long-\ntailed video learning should be more difﬁcult than long-tailed image\nlearning. Thanks to the recent release of a VideoLT dataset [38],\nlong-tailed video learning can be explored in the near future.\n6\nCONCLUSION\nIn this survey, we have extensively reviewed classic deep long-\ntailed learning methods proposed before mid-2021, according to\nthe taxonomy of class re-balancing, information augmentation\nand module improvement. We have empirically analyzed several\nstate-of-the-art long-tailed methods by evaluating to what extent\nthey address the issue of class imbalance, based on a newly\nproposed relative accuracy metric. Following that, we discussed the\nmain application scenarios of long-tailed learning, and identiﬁed\npotential innovation directions for methods and task settings.\nWe expect that this timely survey not only provides a better\nunderstanding of long-tailed learning for researchers and the\ncommunity, but also facilitates future research.\nACKNOWLEDGEMENTS\nThis work was partially supported by NUS ODPRT Grant A-\n0008067-00-00.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n17\nREFERENCES\n[1]\nY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[2]\nI. Goodfellow, Y. Bengio, and A. Courville, Deep learning.\nMIT press,\n2016.\n[3]\nA. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis,\n“Deep learning for computer vision: A brief review,” Computational\nIntelligence and Neuroscience, 2018.\n[4]\nC. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using\ndeep convolutional networks,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 38, no. 2, pp. 295–307, 2015.\n[5]\nZ. Wang, J. Chen, and S. C. Hoi, “Deep learning for image super-\nresolution: A survey,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2020.\n[6]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁca-\ntion with deep convolutional neural networks,” Advances in Neural\nInformation Processing Systems, vol. 25, pp. 1097–1105, 2012.\n[7]\nS. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time\nobject detection with region proposal networks,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 39, no. 6, pp. 1137–1149,\n2016.\n[8]\nE. Shelhamer, J. Long, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation.” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 39, no. 4, pp. 640–651, 2016.\n[9]\nY. Bengio, Y. LeCun, and G. Hinton, “Deep learning for ai,” Communi-\ncations of the ACM, vol. 64, no. 7, pp. 58–65, 2021.\n[10]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Computer Vision and Pattern Recognition, 2016.\n[11]\nC. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\ndetection,” 2013.\n[12]\nR. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\nhierarchies for accurate object detection and semantic segmentation,” in\nComputer Vision and Pattern Recognition, 2014, pp. 580–587.\n[13]\nB. Kang, Y. Li, S. Xie, Z. Yuan, and J. Feng, “Exploring balanced\nfeature spaces for representation learning,” in International Conference\non Learning Representations, 2021.\n[14]\nA. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar,\n“Long-tail learning via logit adjustment,” in International Conference on\nLearning Representations, 2021.\n[15]\nZ. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu, “Large-\nscale long-tailed recognition in an open world,” in Computer Vision and\nPattern Recognition, 2019, pp. 2537–2546.\n[16]\nY. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “Class-balanced loss\nbased on effective number of samples,” in Computer Vision and Pattern\nRecognition, 2019, pp. 9268–9277.\n[17]\nX. Wang, L. Lian, Z. Miao, Z. Liu, and S. X. Yu, “Long-tailed recog-\nnition by routing diverse distribution-aware experts,” in International\nConference on Learning Representations, 2021.\n[18]\nK. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning\nimbalanced datasets with label-distribution-aware margin loss,” in\nAdvances in Neural Information Processing Systems, 2019.\n[19]\nJ. Tan, C. Wang, B. Li, Q. Li, W. Ouyang, C. Yin, and J. Yan,\n“Equalization loss for long-tailed object recognition,” in Computer Vision\nand Pattern Recognition, 2020, pp. 11 662–11 671.\n[20]\nV. Vapnik, “Principles of risk minimization for learning theory,” in\nAdvances in Neural Information Processing Systems, 1992, pp. 831–838.\n[21]\nX. Zhang, Z. Fang, Y. Wen, Z. Li, and Y. Qiao, “Range loss for deep face\nrecognition with long-tailed training data,” in International Conference\non Computer Vision, 2017, pp. 5409–5418.\n[22]\nD. Cao, X. Zhu, X. Huang, J. Guo, and Z. Lei, “Domain balancing: Face\nrecognition on long-tailed domains,” in Computer Vision and Pattern\nRecognition, 2020, pp. 5671–5679.\n[23]\nG. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard,\nH. Adam, P. Perona, and S. Belongie, “The inaturalist species classiﬁca-\ntion and detection dataset,” in Computer Vision and Pattern Recognition,\n2018, pp. 8769–8778.\n[24]\nZ. Miao, Z. Liu, K. M. Gaynor, M. S. Palmer, S. X. Yu, and W. M.\nGetz, “Iterative human and automated identiﬁcation of wildlife images,”\narXiv:2105.02320, 2021.\n[25]\nL. Ju, X. Wang, L. Wang, T. Liu, X. Zhao, T. Drummond, D. Mahapatra,\nand Z. Ge, “Relational subsets knowledge distillation for long-tailed\nretinal diseases recognition,” arXiv:2104.11057, 2021.\n[26]\nR. He, J. Yang, and X. Qi, “Re-distributing biased pseudo labels for\nsemi-supervised semantic segmentation: A baseline investigation,” in\nInternational Conference on Computer Vision, 2021.\n[27]\nW. Yu, T. Yang, and C. Chen, “Towards resolving the challenge of\nlong-tail distribution in uav images for object detection,” in IEEE Winter\nConference on Applications of Computer Vision, 2021, pp. 3258–3267.\n[28]\nM. A. Jamal, M. Brown, M.-H. Yang, L. Wang, and B. Gong, “Rethinking\nclass-balanced methods for long-tailed visual recognition from a domain\nadaptation perspective,” in Computer Vision and Pattern Recognition,\n2020, pp. 7610–7619.\n[29]\nS. Zhang, Z. Li, S. Yan, X. He, and J. Sun, “Distribution alignment: A\nuniﬁed framework for long-tail visual recognition,” in Computer Vision\nand Pattern Recognition, 2021, pp. 2361–2370.\n[30]\nY. Zhang, B. Hooi, L. Hong, and J. Feng, “Self-supervised aggregation\nof diverse experts for test-agnostic long-tailed recognition,” in Advances\nin Neural Information Processing Systems, 2022.\n[31]\nY. Hong, S. Han, K. Choi, S. Seo, B. Kim, and B. Chang, “Disentangling\nlabel distribution for long-tailed visual recognition,” in Computer Vision\nand Pattern Recognition, 2021.\n[32]\nB. Kang, S. Xie, M. Rohrbach, Z. Yan, A. Gordo, J. Feng, and\nY. Kalantidis, “Decoupling representation and classiﬁer for long-tailed\nrecognition,” in International Conference on Learning Representations,\n2020.\n[33]\nC. Feng, Y. Zhong, and W. Huang, “Exploring classiﬁcation equilibrium\nin long-tailed object detection,” in International Conference on Computer\nVision, 2021.\n[34]\nT. Wang, Y. Li, B. Kang, J. Li, J. Liew, S. Tang, S. Hoi, and J. Feng,\n“The devil is in classiﬁcation: A simple framework for long-tail instance\nsegmentation,” in European Conference on Computer Vision, 2020.\n[35]\nZ. Weng, M. G. Ogut, S. Limonchik, and S. Yeung, “Unsupervised\ndiscovery of the long-tail in instance segmentation using hierarchical\nself-supervision,” in Computer Vision and Pattern Recognition, 2021.\n[36]\nA. Gupta, P. Dollar, and R. Girshick, “Lvis: A dataset for large vocabulary\ninstance segmentation,” in Computer Vision and Pattern Recognition,\n2019, pp. 5356–5364.\n[37]\nT. Wu, Q. Huang, Z. Liu, Y. Wang, and D. Lin, “Distribution-balanced\nloss for multi-label classiﬁcation in long-tailed datasets,” in European\nConference on Computer Vision, 2020, pp. 162–178.\n[38]\nX. Zhang, Z. Wu, Z. Weng, H. Fu, J. Chen, Y.-G. Jiang, and L. Davis,\n“Videolt: Large-scale long-tailed video recognition,” in International\nConference on Computer Vision, 2021.\n[39]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in Computer Vision and\nPattern Recognition, 2009, pp. 248–255.\n[40]\nA. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\nfrom tiny images,” 2009.\n[41]\nB. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning\ndeep features for scene recognition using places database,” Advances in\nNeural Information Processing Systems, vol. 27, pp. 487–495, 2014.\n[42]\nM. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,\nand A. Zisserman, “The pascal visual object classes challenge: A\nretrospective,” International Journal of Computer Vision, vol. 111, no. 1,\npp. 98–136, 2015.\n[43]\nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European Conference on Computer Vision, 2014.\n[44]\nB. Zhou, Q. Cui, X.-S. Wei, and Z.-M. Chen, “Bbn: Bilateral-branch\nnetwork with cumulative learning for long-tailed visual recognition,” in\nComputer Vision and Pattern Recognition, 2020, pp. 9719–9728.\n[45]\nK. Tang, J. Huang, and H. Zhang, “Long-tailed classiﬁcation by keeping\nthe good and removing the bad momentum causal effect,” in Advances\nin Neural Information Processing Systems, vol. 33, 2020.\n[46]\nH. Guo and S. Wang, “Long-tailed multi-label visual recognition\nby collaborative training on uniform and re-balanced samplings,” in\nComputer Vision and Pattern Recognition, 2021, pp. 15 089–15 098.\n[47]\nM. R. Keaton, R. J. Zaveri, M. Kovur, C. Henderson, D. A. Adjeroh, and\nG. Doretto, “Fine-grained visual classiﬁcation of plant species in the wild:\nObject detection as a reinforced means of attention,” arXiv:2106.02141,\n2021.\n[48]\nY. Zhong, W. Deng, M. Wang, J. Hu, J. Peng, X. Tao, and Y. Huang,\n“Unequal-training for deep face recognition with long-tailed noisy data,”\nin Computer Vision and Pattern Recognition, 2019, pp. 7812–7821.\n[49]\nJ. Liu, Y. Sun, C. Han, Z. Dou, and W. Li, “Deep representation learning\non long-tailed data: A learnable embedding augmentation perspective,”\nin Computer Vision and Pattern Recognition, 2020.\n[50]\nQ. Dong, S. Gong, and X. Zhu, “Class rectiﬁcation hard mining for\nimbalanced deep learning,” in International Conference on Computer\nVision, 2017, pp. 1851–1860.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n18\n[51]\nZ. Deng, H. Liu, Y. Wang, C. Wang, Z. Yu, and X. Sun, “Pml: Progressive\nmargin loss for long-tailed age classiﬁcation,” in Computer Vision and\nPattern Recognition, 2021, pp. 10 503–10 512.\n[52]\nZ. Zhang, S. Yu, S. Yang, Y. Zhou, and B. Zhao, “Rail-5k: a real-world\ndataset for rail surface defects detection,” arXiv:2106.14366, 2021.\n[53]\nA. Galdran, G. Carneiro, and M. A. G. Ballester, “Balanced-mixup\nfor highly imbalanced medical image classiﬁcation,” in International\nConference on Medical Image Computing and Computer-Assisted\nIntervention, 2021.\n[54]\nT.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss for\ndense object detection,” in International Conference on Computer Vision,\n2017, pp. 2980–2988.\n[55]\nT.-I. Hsieh, E. Robb, H.-T. Chen, and J.-B. Huang, “Droploss for long-\ntail instance segmentation,” in AAAI Conference on Artiﬁcial Intelligence,\nvol. 35, no. 2, 2021, pp. 1549–1557.\n[56]\nY. Li, T. Wang, B. Kang, S. Tang, C. Wang, J. Li, and J. Feng,\n“Overcoming classiﬁer imbalance for long-tail object detection with\nbalanced group softmax,” in Computer Vision and Pattern Recognition,\n2020, pp. 10 991–11 000.\n[57]\nT. Weyand, A. Araujo, B. Cao, and J. Sim, “Google landmarks dataset\nv2-a large-scale benchmark for instance-level recognition and retrieval,”\nin Computer Vision and Pattern Recognition, 2020, pp. 2575–2584.\n[58]\nY. Zang, C. Huang, and C. C. Loy, “Fasa: Feature augmentation\nand sampling adaptation for long-tailed instance segmentation,” in\nInternational Conference on Computer Vision, 2021.\n[59]\nJ. Wu, L. Song, T. Wang, Q. Zhang, and J. Yuan, “Forest r-cnn: Large-\nvocabulary long-tailed object detection and instance segmentation,” in\nACM International Conference on Multimedia, 2020, pp. 1570–1578.\n[60]\nJ. Mao, M. Niu, C. Jiang, H. Liang, X. Liang, Y. Li, C. Ye, W. Zhang,\nZ. Li, J. Yu et al., “One million scenes for autonomous driving: Once\ndataset,” in NeurIPS 2021 Datasets and Benchmarks Track, 2021.\n[61]\nA. Desai, T.-Y. Wu, S. Tripathi, and N. Vasconcelos, “Learning of\nvisual relations: The devil is in the tails,” in International Conference on\nComputer Vision, 2021.\n[62]\nN. Dhingra, F. Ritter, and A. Kunz, “Bgt-net: Bidirectional gru trans-\nformer network for scene graph generation,” in Computer Vision and\nPattern Recognition, 2021, pp. 2150–2159.\n[63]\nJ. Chen, A. Agarwal, S. Abdelkarim, D. Zhu, and M. Elhoseiny,\n“Reltransformer: Balancing the visual relationship detection from local\ncontext, scene and memory,” arXiv:2104.11934, 2021.\n[64]\nZ. Li, E. Stengel-Eskin, Y. Zhang, C. Xie, Q. Tran, B. Van Durme,\nand A. Yuille, “Calibrating concepts and operations: Towards symbolic\nreasoning on real images,” in International Conference on Computer\nVision, 2021.\n[65]\nG. Wang, D. Forsyth, and D. Hoiem, “Comparative object similarity for\nimproved recognition with few or no examples,” in Computer Vision and\nPattern Recognition, 2010, pp. 3525–3532.\n[66]\nC. C. Loy, T. M. Hospedales, T. Xiang, and S. Gong, “Stream-based\njoint exploration-exploitation active learning,” in Computer Vision and\nPattern Recognition, 2012, pp. 1560–1567.\n[67]\nJ. Yang, B. Price, S. Cohen, and M.-H. Yang, “Context driven scene\nparsing with attention to rare classes,” in Computer Vision and Pattern\nRecognition, 2014, pp. 3294–3301.\n[68]\nJ. Pitman and M. Yor, “The two-parameter poisson-dirichlet distribution\nderived from a stable subordinator,” The Annals of Probability, pp.\n855–900, 1997.\n[69]\nD. G. Lowe, “Distinctive image features from scale-invariant keypoints,”\nInternational Journal of Computer Vision, vol. 60, no. 2, pp. 91–110,\n2004.\n[70]\nN. Dalal and B. Triggs, “Histograms of oriented gradients for human\ndetection,” in Computer Vision and Pattern Recognition, vol. 1, 2005,\npp. 886–893.\n[71]\nM. J. Swain and D. H. Ballard, “Color indexing,” International Journal\nof Computer Vision, vol. 7, no. 1, pp. 11–32, 1991.\n[72]\nH. He and E. A. Garcia, “Learning from imbalanced data,” IEEE\nTransactions on Knowledge and Data Engineering, vol. 21, no. 9, pp.\n1263–1284, 2009.\n[73]\nJ. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot\nlearning,” Advances in Neural Information Processing Systems, 2017.\n[74]\nF. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales,\n“Learning to compare: Relation network for few-shot learning,” in\nComputer Vision and Pattern Recognition, 2018, pp. 1199–1208.\n[75]\nQ. Sun, Y. Liu, T.-S. Chua, and B. Schiele, “Meta-transfer learning for\nfew-shot learning,” in Computer Vision and Pattern Recognition, 2019.\n[76]\nY. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a few\nexamples: A survey on few-shot learning,” ACM Computing Surveys,\nvol. 53, no. 3, pp. 1–34, 2020.\n[77]\nD. Krueger, E. Caballero et al., “Out-of-distribution generalization via\nrisk extrapolation,” in International Conference on Machine Learning,\n2021, pp. 5815–5826.\n[78]\nZ. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui, “Towards\nout-of-distribution generalization: A survey,” arXiv:2108.13624, 2021.\n[79]\nS. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via\ntransfer component analysis,” IEEE Transactions on Neural Networks,\nvol. 22, no. 2, pp. 199–210, 2010.\n[80]\nE. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discrimi-\nnative domain adaptation,” in Computer Vision and Pattern Recognition,\n2017, pp. 7167–7176.\n[81]\nY. Zhang, H. Chen, Y. Wei, P. Zhao, J. Cao, X. Fan, X. Lou, H. Liu,\nJ. Hou, X. Han et al., “From whole slide imaging to microscopy:\nDeep microscopy adaptation network for histopathology cancer image\nclassiﬁcation,” in International Conference on Medical Image Computing\nand Computer-Assisted Intervention, 2019, pp. 360–368.\n[82]\nY. Zhang, Y. Wei et al., “Collaborative unsupervised domain adaptation\nfor medical image diagnosis,” IEEE Transactions on Image Processing,\n2020.\n[83]\nZ. Qiu, Y. Zhang, H. Lin, S. Niu, Y. Liu, Q. Du, and M. Tan, “Source-free\ndomain adaptation via avatar prototype generation and adaptation,” in\nInternational Joint Conference on Artiﬁcial Intelligence, 2021.\n[84]\nH. Wu, H. Zhu, Y. Yan, J. Wu, Y. Zhang, and M. K. Ng, “Heterogeneous\ndomain adaptation by information capturing and distribution matching,”\nIEEE Transactions on Image Processing, vol. 30, pp. 6364–6376, 2021.\n[85]\nD. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales, “Deeper, broader and\nartier domain generalization,” in International Conference on Computer\nVision, 2017, pp. 5542–5550.\n[86]\nH. Li, S. J. Pan, S. Wang, and A. C. Kot, “Domain generalization\nwith adversarial feature learning,” in Computer Vision and Pattern\nRecognition, 2018, pp. 5400–5409.\n[87]\nL. Neal, M. Olson, X. Fern, W.-K. Wong, and F. Li, “Open set learning\nwith counterfactual images,” in European Conference on Computer\nVision, 2018, pp. 613–628.\n[88]\nY. Fu, X. Wang, H. Dong, Y.-G. Jiang, M. Wang, X. Xue, and\nL. Sigal, “Vocabulary-informed zero-shot and open-set learning,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 42,\nno. 12, pp. 3136–3152, 2019.\n[89]\nC. Huang, Y. Li, C. C. Loy, and X. Tang, “Learning deep represen-\ntation for imbalanced classiﬁcation,” in Computer Vision and Pattern\nRecognition, 2016.\n[90]\nW. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\ndeep model for object detection with long-tail distribution,” in Computer\nVision and Pattern Recognition, 2016, pp. 864–873.\n[91]\nY.-X. Wang, D. Ramanan, and M. Hebert, “Learning to model the tail,”\nin Advances in Neural Information Processing Systems, 2017.\n[92]\nY. Cui, Y. Song, C. Sun, A. Howard, and S. Belongie, “Large scale\nﬁne-grained categorization and domain-speciﬁc transfer learning,” in\nComputer Vision and Pattern Recognition, 2018, pp. 4109–4118.\n[93]\nY. Wang, W. Gan, J. Yang, W. Wu, and J. Yan, “Dynamic curriculum\nlearning for imbalanced data classiﬁcation,” in International Conference\non Computer Vision, 2019, pp. 5017–5026.\n[94]\nJ. Shu, Q. Xie, L. Yi, Q. Zhao, S. Zhou, Z. Xu, and D. Meng,\n“Meta-weight-net: Learning an explicit mapping for sample weighting,”\nAdvances in Neural Information Processing Systems, 2019.\n[95]\nS. Khan, M. Hayat, S. W. Zamir, J. Shen, and L. Shao, “Striking the right\nbalance with uncertainty,” in Computer Vision and Pattern Recognition,\n2019, pp. 103–112.\n[96]\nX. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker, “Feature transfer\nlearning for face recognition with under-represented data,” in Computer\nVision and Pattern Recognition, 2019, pp. 5704–5713.\n[97]\nR. Jiawei, C. Yu, X. Ma, H. Zhao, S. Yi et al., “Balanced meta-softmax\nfor long-tailed visual recognition,” in Advances in Neural Information\nProcessing Systems, 2020.\n[98]\nX. Hu, Y. Jiang, K. Tang, J. Chen, C. Miao, and H. Zhang, “Learning to\nsegment the tail,” in Computer Vision and Pattern Recognition, 2020.\n[99]\nJ. Tian, Y.-C. Liu, N. Glaser, Y.-C. Hsu, and Z. Kira, “Posterior re-\ncalibration for imbalanced datasets,” in Advances in Neural Information\nProcessing Systems, 2020.\n[100] J. Kim, J. Jeong, and J. Shin, “M2m: Imbalanced classiﬁcation via major-\nto-minor translation,” in Computer Vision and Pattern Recognition, 2020.\n[101] P. Chu, X. Bian, S. Liu, and H. Ling, “Feature space augmentation for\nlong-tailed data,” in European Conference on Computer Vision, 2020.\n[102] Y. Yang and Z. Xu, “Rethinking the value of labels for improving class-\nimbalanced learning,” in Advances in Neural Information Processing\nSystems, 2020.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n19\n[103] L. Xiang, G. Ding, and J. Han, “Learning from multiple experts: Self-\npaced knowledge distillation for long-tailed classiﬁcation,” in European\nConference on Computer Vision, 2020, pp. 247–263.\n[104] L. Zhu and Y. Yang, “Inﬂated episodic memory with region self-attention\nfor long-tailed visual recognition,” in Computer Vision and Pattern\nRecognition, 2020, pp. 4344–4353.\n[105] T.-Y. Wu, P. Morgado, P. Wang, C.-H. Ho, and N. Vasconcelos, “Solving\nlong-tailed recognition with deep realistic taxonomic classiﬁer,” in\nEuropean Conference on Computer Vision, 2020, pp. 171–189.\n[106] C. Wei, K. Sohn, C. Mellina, A. Yuille, and F. Yang, “Crest: A class-\nrebalancing self-training framework for imbalanced semi-supervised\nlearning,” in Computer Vision and Pattern Recognition, 2021.\n[107] B. Liu, H. Li, H. Kang, G. Hua, and N. Vasconcelos, “Gistnet: a\ngeometric structure transfer network for long-tailed recognition,” in\nInternational Conference on Computer Vision, 2021.\n[108] J. Tan, X. Lu, G. Zhang, C. Yin, and Q. Li, “Equalization loss v2: A new\ngradient balance approach for long-tailed object detection,” in Computer\nVision and Pattern Recognition, 2021, pp. 1685–1694.\n[109] J. Wang, W. Zhang, Y. Zang, Y. Cao, J. Pang, T. Gong, K. Chen,\nZ. Liu, C. C. Loy, and D. Lin, “Seesaw loss for long-tailed instance\nsegmentation,” in Computer Vision and Pattern Recognition, 2021.\n[110] T. Wang, Y. Zhu, C. Zhao, W. Zeng, J. Wang, and M. Tang, “Adaptive\nclass suppression loss for long-tail object detection,” in Computer Vision\nand Pattern Recognition, 2021, pp. 3103–3112.\n[111] S. Park, J. Lim, Y. Jeon, and J. Y. Choi, “Inﬂuence-balanced loss\nfor imbalanced visual classiﬁcation,” in International Conference on\nComputer Vision, 2021.\n[112] G. R. Kini, O. Paraskevas, S. Oymak, and C. Thrampoulidis, “Label-\nimbalanced and group-sensitive classiﬁcation under overparameteriza-\ntion,” in Advances in Neural Information Processing Systems, vol. 34,\n2021, pp. 18 970–18 983.\n[113] T. Wu, Z. Liu, Q. Huang, Y. Wang, and D. Lin, “Adversarial robust-\nness under long-tailed distribution,” in Computer Vision and Pattern\nRecognition, 2021, pp. 8659–8668.\n[114] Z. Zhong, J. Cui, S. Liu, and J. Jia, “Improving calibration for long-tailed\nrecognition,” in Computer Vision and Pattern Recognition, 2021.\n[115] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m:\nPushing web-scale image-text pre-training to recognize long-tail visual\nconcepts,” in Computer Vision and Pattern Recognition, 2021.\n[116] Y.-Y. He, J. Wu, and X.-S. Wei, “Distilling virtual examples for long-\ntailed recognition,” in International Conference on Computer Vision,\n2021.\n[117] C. Zhang, T.-Y. Pan, Y. Li, H. Hu, D. Xuan, S. Changpinyo, B. Gong,\nand W.-L. Chao, “Mosaicos: A simple and effective use of object-centric\nimages for long-tailed object detection,” in International Conference on\nComputer Vision, 2021.\n[118] J. Wang, T. Lukasiewicz, X. Hu, J. Cai, and Z. Xu, “Rsg: A simple but\neffective module for learning imbalanced datasets,” in Computer Vision\nand Pattern Recognition, 2021, pp. 3784–3793.\n[119] T. Li, L. Wang, and G. Wu, “Self supervision to distillation for long-\ntailed visual recognition,” in International Conference on Computer\nVision, 2021.\n[120] S. Li, K. Gong, C. H. Liu, Y. Wang, F. Qiao, and X. Cheng, “Metasaug:\nMeta semantic augmentation for long-tailed visual recognition,” in\nComputer Vision and Pattern Recognition, 2021, pp. 5212–5221.\n[121] J. Cui, Z. Zhong, S. Liu, B. Yu, and J. Jia, “Parametric contrastive\nlearning,” in International Conference on Computer Vision, 2021.\n[122] D. Samuel and G. Chechik, “Distributional robustness loss for long-tail\nlearning,” in International Conference on Computer Vision, 2021.\n[123] P. Wang, K. Han, X.-S. Wei, L. Zhang, and L. Wang, “Contrastive\nlearning based hybrid networks for long-tailed image classiﬁcation,” in\nComputer Vision and Pattern Recognition, 2021, pp. 943–952.\n[124] J. Cai, Y. Wang, and J.-N. Hwang, “Ace: Ally complementary experts for\nsolving long-tailed recognition in one-shot,” in International Conference\non Computer Vision, 2021.\n[125] J. Cui, S. Liu, Z. Tian, Z. Zhong, and J. Jia, “Reslt: Residual learning\nfor long-tailed recognition,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2022.\n[126] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,\n“Smote: synthetic minority over-sampling technique,” Journal of artiﬁcial\nintelligence research, vol. 16, pp. 321–357, 2002.\n[127] A. Estabrooks, T. Jo, and N. Japkowicz, “A multiple resampling method\nfor learning from imbalanced data sets,” Computational Intelligence,\nvol. 20, no. 1, pp. 18–36, 2004.\n[128] X.-Y. Liu, J. Wu, and Z.-H. Zhou, “Exploratory undersampling for\nclass-imbalance learning,” IEEE Transactions on Systems, Man, and\nCybernetics, vol. 39, no. 2, pp. 539–550, 2008.\n[129] Z. Zhang and T. Pﬁster, “Learning fast sample re-weighting without\nreward data,” in International Conference on Computer Vision, 2021.\n[130] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li,\nA. Bharambe, and L. Van Der Maaten, “Exploring the limits of weakly\nsupervised pretraining,” in European conference on computer vision,\n2018, pp. 181–196.\n[131] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning\nin neural networks: A survey,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 44, no. 9, pp. 5149–5169, 2021.\n[132] C. Elkan, “The foundations of cost-sensitive learning,” in International\nJoint Conference on Artiﬁcial Intelligence, 2001.\n[133] Z.-H. Zhou and X.-Y. Liu, “Training cost-sensitive neural networks with\nmethods addressing the class imbalance problem,” IEEE Transactions\non Knowledge and Data Engineering, vol. 18, no. 1, pp. 63–77, 2005.\n[134] P. Zhao, Y. Zhang, M. Wu, S. C. Hoi, M. Tan, and J. Huang, “Adaptive\ncost-sensitive online classiﬁcation,” IEEE Transactions on Knowledge\nand Data Engineering, vol. 31, no. 2, pp. 214–228, 2018.\n[135] Y. Zhang, P. Zhao, J. Cao, W. Ma, J. Huang, Q. Wu, and M. Tan, “Online\nadaptive asymmetric active learning for budgeted imbalanced data,” in\nSIGKDD International Conference on Knowledge Discovery & Data\nMining, 2018, pp. 2768–2777.\n[136] Y. Zhang, P. Zhao, S. Niu, Q. Wu, J. Cao, J. Huang, and M. Tan,\n“Online adaptive asymmetric active learning with limited budgets,” IEEE\nTransactions on Knowledge and Data Engineering, 2019.\n[137] Y. Sun, M. S. Kamel, A. K. Wong, and Y. Wang, “Cost-sensitive boosting\nfor classiﬁcation of imbalanced data,” Pattern Recognition, vol. 40,\nno. 12, pp. 3358–3378, 2007.\n[138] F. Wang, J. Cheng, W. Liu, and H. Liu, “Additive margin softmax for\nface veriﬁcation,” IEEE Signal Processing Letters, vol. 25, no. 7, pp.\n926–930, 2018.\n[139] V. Koltchinskii and D. Panchenko, “Empirical margin distributions and\nbounding the generalization error of combined classiﬁers,” The Annals\nof Statistics, vol. 30, no. 1, pp. 1–50, 2002.\n[140] F. Provost, “Machine learning from imbalanced data sets 101,” in AAAI\nWorkshop on Imbalanced Data Sets, vol. 68, no. 2000, 2000, pp. 1–3.\n[141] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions\non Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359,\n2009.\n[142] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on\ndeep transfer learning,” in International Conference on Artiﬁcial Neural\nNetworks, 2018, pp. 270–279.\n[143] D. Erhan, A. Courville, Y. Bengio, and P. Vincent, “Why does unsuper-\nvised pre-training help deep learning?” in International Conference on\nArtiﬁcial Intelligence and Statistics, 2010, pp. 201–208.\n[144] K. He, R. Girshick, and P. Doll´ar, “Rethinking imagenet pre-training,” in\nInternational Conference on Computer Vision, 2019, pp. 4918–4927.\n[145] D. Hendrycks, K. Lee, and M. Mazeika, “Using pre-training can improve\nmodel robustness and uncertainty,” in International Conference on\nMachine Learning, 2019, pp. 2712–2721.\n[146] B. Zoph, G. Ghiasi, T.-Y. Lin, Y. Cui, H. Liu, E. D. Cubuk, and\nQ. Le, “Rethinking pre-training and self-training,” Advances in Neural\nInformation Processing Systems.\n[147] Y. Zhang, B. Hooi, D. Hu, J. Liang, and J. Feng, “Unleashing the power\nof contrastive self-supervised visual models via contrast-regularized\nﬁne-tuning,” in Advances in Neural Information Processing Systems,\n2021.\n[148] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for\nunsupervised visual representation learning,” in Computer Vision and\nPattern Recognition, 2020.\n[149] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation\nlearning by predicting image rotations,” in International Conference on\nLearning Representations, 2018.\n[150] S. Karthik, J. Revaud, and C. Boris, “Learning from long-tailed data\nwith noisy labels,” arXiv:2108.11096, 2021.\n[151] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” arXiv:1503.02531, 2015.\n[152] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A\nsurvey,” International Journal of Computer Vision, vol. 129, no. 6, pp.\n1789–1819, 2021.\n[153] X. J. Zhu, “Semi-supervised learning literature survey,” 2005.\n[154] C. Rosenberg, M. Hebert, and H. Schneiderman, “Semi-supervised self-\ntraining of object detection models,” 2005.\n[155] T. Wei, J.-X. Shi, W.-W. Tu, and Y.-F. Li, “Robust long-tailed learning\nunder label noise,” arXiv:2108.11569, 2021.\n[156] L. Perez and J. Wang, “The effectiveness of data augmentation in image\nclassiﬁcation using deep learning,” arXiv:1712.04621, 2017.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n20\n[157] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmen-\ntation for deep learning,” Journal of Big Data, vol. 6, no. 1, pp. 1–48,\n2019.\n[158] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning\ndeep features for discriminative localization,” in Computer Vision and\nPattern Recognition, 2016, pp. 2921–2929.\n[159] H. Han, W.-Y. Wang, and B.-H. Mao, “Borderline-smote: a new over-\nsampling method in imbalanced data sets learning,” in International\nConference on Intelligent Computing, 2005, pp. 878–887.\n[160] H.-P. Chou, S.-C. Chang, J.-Y. Pan, W. Wei, and D.-C. Juan, “Remix:\nRebalanced mixup,” in European Conference on Computer Vision\nWorkshop, 2020, pp. 95–110.\n[161] Y. Wang, X. Pan, S. Song, H. Zhang, G. Huang, and C. Wu, “Implicit\nsemantic data augmentation for deep networks,” in Advances in Neural\nInformation Processing Systems, vol. 32, 2019, pp. 12 635–12 644.\n[162] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss for\nperson re-identiﬁcation,” arXiv:1703.07737, 2017.\n[163] J. Goh and M. Sim, “Distributionally robust optimization and its tractable\napproximations,” Operations Research, vol. 58, no. 4-part-1, pp. 902–\n917, 2010.\n[164] H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, “Identifying\nand compensating for feature deviation in imbalanced deep learning,”\narXiv:2001.01385, 2020.\n[165] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE\ntransactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.\n[166] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, “Randaugment: Practical\nautomated data augmentation with a reduced search space,” in Advances\nin Neural Information Processing Systems, vol. 33, 2020.\n[167] M. Luo, F. Chen, D. Hu, Y. Zhang, J. Liang, and J. Feng, “No fear of\nheterogeneity: Classiﬁer calibration for federated learning with non-iid\ndata,” in Advances in Neural Information Processing Systems, 2021.\n[168] C. D. Kim, J. Jeong, and G. Kim, “Imbalanced continual learning with\npartitioning reservoir sampling,” in European Conference on Computer\nVision, 2020, pp. 411–428.\n[169] S. Niu, J. Wu, G. Xu, Y. Zhang, Y. Guo, P. Zhao, P. Wang, and\nM. Tan, “Adaxpert: Adapting neural architecture for growing data,”\nin International Conference on Machine Learning, 2021, pp. 8184–8194.\n[170] Y. Zhang, S. Niu, Z. Qiu, Y. Wei, P. Zhao, J. Yao, J. Huang, Q. Wu, and\nM. Tan, “Covid-da: Deep domain adaptation from typical pneumonia to\ncovid-19,” arXiv:2005.01577, 2020.\n[171] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang,\n“Moment matching for multi-source domain adaptation,” in International\nConference on Computer Vision, 2019, pp. 1406–1415.\n[172] K. Cao, Y. Chen, J. Lu, N. Arechiga, A. Gaidon, and T. Ma, “Het-\neroskedastic and imbalanced deep learning with adaptive regularization,”\nin International Conference on Learning Representations, 2021.\n[173] Y. Yang, K. Zha, Y.-C. Chen, H. Wang, and D. Katabi, “Delving into\ndeep imbalanced regression,” in International Conference on Machine\nLearning, 2021.\nYifan Zhang is working toward the Ph.D. degree\nin computer science at National University of\nSingapore. His research interests are broadly in\nmachine learning, now with high self-motivation\nto solve domain shifts problems for deep learning.\nHe has published papers in top venues, including\nNeurIPS, ICML, ICLR, SIGKDD, ECCV, IJCAI,\nTPAMI, TIP, and TKDE. He has been invited as\na reviewer for top-tier conferences and journals,\nincluding NeurIPS, ICML, ICLR, CVPR, ECCV,\nAAAI, IJCAI, TPAMI, TIP, IJCV, and JMLR.\nBingyi Kang is currently a research scientist at\nTikTok. Before joining TikTok, got his Ph.D degree\nin Electronic and Computer Engineering from\nNational University of Singapore. He received\nhis B.E. degree in automation from Zhejiang\nUniversity, Hangzhou, Zhejiang in 2016. His cur-\nrent research interest focuses on sample-efﬁcient\nlearning and reinforcement learning.\nBryan Hooi is an assistant professor in the\nSchool of Computing and the Institute of Data\nScience in National University of Singapore. He\nreceived his PhD degree in Machine Learning\nfrom Carnegie Mellon University, USA in 2019.\nHis research interests include methods for learn-\ning from graphs and other complex or multimodal\ndatasets, with the goal of developing efﬁcient\nand practical approaches for applications such as\nthe detection of anomalies or malicious behavior,\nand automatic monitoring of medical, trafﬁc, and\nenvironmental sensor data.\nShuicheng Yan is currently the director of Sea\nAI Lab and group chief scientist of Sea. He is an\nIEEE Fellow, ACM Fellow, IAPR Fellow, and Fel-\nlow of Academy of Engineering, Singapore. His\nresearch areas include computer vision, machine\nlearning and multimedia analysis. Till now, he has\npublished over 1,000 papers in top international\njournals and conferences, with Google Scholar\nCitation over 93,000 times and H-index 137. He\nhad been among “Thomson Reuters Highly Cited\nResearchers” in 2014, 2015, 2016, 2018, 2019.\nHis team has received winner or honorable-mention prizes for 10 times\nof two core competitions, Pascal VOC and ImageNet (ILSVRC), which\nare deemed as “World Cup” in the computer vision community. Also, his\nteam won over 10 best paper or best student paper prizes and especially,\na grand slam in ACM MM, the top conference in multimedia, including\nBest Paper Award, Best Student Paper Award and Best Demo Award.\nJiashi Feng is currently a research manager at\nTikTok and is leading a fundamental research\nteam. Before TikTok, he was an assistant profes-\nsor with Department of Electrical and Computer\nEngineering at National University of Singapore\nand a postdoc researcher in the EECS depart-\nment and ICSI at the University of California,\nBerkeley. He received his Ph.D. degree from NUS\nin 2014. His research areas include deep learning\nand their applications in computer vision. He has\nauthored/co-authored more than 300 technical\npapers on deep learning, image classiﬁcation, object detection, machine\nlearning theory. His recent research interest focuses on foundation\nmodels, transfer learning, 3D vision and deep neural networks. He\nreceived the best technical demo award from ACM MM 2012, best paper\naward from TASK-CV ICCV 2015, best student paper award from ACM\nMM 2018. He is also the recipient of Innovators Under 35 Asia, MIT\nTechnology Review 2018. He served as the area chairs for NeurIPS,\nICML, CVPR, ICLR, WACV, ACM MM and program chair for ICMR 2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-10-09",
  "updated": "2023-04-15"
}