{
  "id": "http://arxiv.org/abs/2110.05732v1",
  "title": "Guided-GAN: Adversarial Representation Learning for Activity Recognition with Wearables",
  "authors": [
    "Alireza Abedin",
    "Hamid Rezatofighi",
    "Damith C. Ranasinghe"
  ],
  "abstract": "Human activity recognition (HAR) is an important research field in ubiquitous\ncomputing where the acquisition of large-scale labeled sensor data is tedious,\nlabor-intensive and time consuming. State-of-the-art unsupervised remedies\ninvestigated to alleviate the burdens of data annotations in HAR mainly explore\ntraining autoencoder frameworks. In this paper: we explore generative\nadversarial network (GAN) paradigms to learn unsupervised feature\nrepresentations from wearable sensor data; and design a new GAN\nframework-Geometrically-Guided GAN or Guided-GAN-for the task. To demonstrate\nthe effectiveness of our formulation, we evaluate the features learned by\nGuided-GAN in an unsupervised manner on three downstream classification\nbenchmarks. Our results demonstrate Guided-GAN to outperform existing\nunsupervised approaches whilst closely approaching the performance with fully\nsupervised learned representations. The proposed approach paves the way to\nbridge the gap between unsupervised and supervised human activity recognition\nwhilst helping to reduce the cost of human data annotation tasks.",
  "text": "Guided-GAN: Adversarial Representation Learning\nfor Activity Recognition with Wearables\nAlireza Abedin∗, Hamid Rezatoﬁghi† and Damith C. Ranasinghe∗\n∗The School of Computer Science, The University of Adelaide, SA 5005 Australia\n{alireza.abedinvaramin, damith.ranasinghe}@adelaide.edu.au\n†Monash University, Melbourne, VIC 3800, Australia\nhamid.rezatoﬁghi@monash.edu\nAbstract—Human activity recognition (HAR) is an important\nresearch ﬁeld in ubiquitous computing where the acquisition of\nlarge-scale labeled sensor data is tedious, labor-intensive and time\nconsuming. State-of-the-art unsupervised remedies investigated\nto alleviate the burdens of data annotations in HAR mainly\nexplore training autoencoder frameworks. In this paper: we\nexplore generative adversarial network (GAN) paradigms to\nlearn unsupervised feature representations from wearable sen-\nsor data; and design a new GAN framework—Geometrically-\nGuided GAN or Guided-GAN—for the task. To demonstrate\nthe effectiveness of our formulation, we evaluate the features\nlearned by Guided-GAN in an unsupervised manner on three\ndownstream classiﬁcation benchmarks. Our results demonstrate\nGuided-GAN to outperform existing unsupervised approaches\nwhilst closely approaching the performance with fully supervised\nlearned representations. The proposed approach paves the way\nto bridge the gap between unsupervised and supervised human\nactivity recognition whilst helping to reduce the cost of human\ndata annotation tasks.\nIndex Terms—GAN; Recurrent GAN; BiGAN; Bi-directional\nGenerative Adversarial Networks; Activity Recognition; Deep\nlearning; Generative Adversarial Networks; Wearable Sensors\nI. INTRODUCTION\nThe proliferation of low-cost sensing technologies coupled\nwith rapid advances in machine learning techniques are en-\nabling automatic human activity recognition (HAR) using\nwearables to realise a multitude of applications, especially in\nthe healthcare domain [1]–[8]. However, the growth and per-\nformance of wearable based activity recognition applications\nare impeded by the demand for annotated sensor data col-\nlection, a process both expensive and laborious to undertake,\nfor supervised learning methods. Consequently, investigating\nunsupervised HAR to avoid the heavy reliance on labeled\ndata, has sparked signiﬁcant interest. But, despite advances in\nunsupervised learning methods, investigations of unsupervised\nrepresentation learning for HAR applications using wearable\nsensor data remains little explored.\nThe state-of-the-art methods for unsupervised representation\nlearning have largely explored autoencoder frameworks [9],\n[10]; an encoder ﬁrst projects the data into a compact latent\nrepresentation and a decoder exploits the representation to\nsequentially re-generate the original sensor data. Within this\nframework, the network weights are encouraged to learn\nfeature representations that minimize the element-wise recon-\nstruction error. However, it is not clear whether the pretext task\nof element-wise reconstruction alone sufﬁces for extracting\nenriched activity features. In contrast, in the vision domain,\nexploration of the latent spaces in generative adversarial\nnetworks (GANs) [11] built upon deep convolutional neural\nnetworks has resulted in promising frameworks for unsuper-\nvised learning of enriched feature representations [12]–[17].\nOur Motivations. Despite the demonstrated strength of GANs\nin the visual domain to capture semantic variations of data\ndistributions, the adoption of GANs for the challenging task of\nunsupervised representation learning for sequential wearable\ndata remains. In contrast to vision tasks, multimodal sequential\ndata, typical of those from wearables, are uniquely charac-\nterized by the inherent sample dependencies across time and\ndesire architecture designs beyond convolutional operators for\ntemporal modeling. But, examination of GANs for temporal\nHAR data are predominantly conﬁned to synthesizing artiﬁcial\nsequences that resemble the original data [18]–[23], while\ninvestigation of GAN’s latent feature space for unsupervised\nlearning remains largely unexplored. Further, GANs are no-\ntorious for their unstable training process and sensitivity to\nhyper-parameter selections. Although, the immense commu-\nnity effort in computer vision has resulted in established\nguidelines for architectural designs—weight initializations and\nhyper-parameter settings [24]–[27]—the same exploration is\nlacking in the sensor-based HAR domain. Motivated by these\nfactors and grounded on the immense success of GANs in\nthe visual domain, we believe exploring the GAN’s latent\nfeature space offers an appealing alternative to the de-facto\nautoencoder-based frameworks [9], [10], [28] for wearable\nsensor representation learning.\nOur Approach. We design a generative adversarial framework\nfor unsupervised representation learning from wearables. Our\nintuitive approach measures and minimises the errors asso-\nciated with reconstructing data and latent samples and is\nefﬁciently implemented through re-using existing components\nwith weight sharing as illustrated in Figure 1. In summary:\n• We present a ﬁrst rigorous study of generative adversarial\nframeworks for unsupervised representation learning from\nsequential multi-modal data from wearable sensors.\n• We develop a novel unsupervised representation learning\nframework with a generative adversarial network architec-\narXiv:2110.05732v1  [cs.LG]  12 Oct 2021\nData Reconstruction\nLatent Reconstruction\nReal/Fake?\nweight sharing\nFig. 1: Guided-GAN. Overview of our proposed adversarial game for unsupervised representation learning from sequential\nwearable data. In relation to [12] (gray box), we propose: i) incorporating gradient feedback from geometric distance\nminimization in both data and latent manifolds; ii) efﬁcient implementation architecture through parameter sharing (dashed\nlines); and iii) integration of our recurrent block designs in Fig. 4 for temporal modeling.\nture. Our framework augments adversarial feedback with\ngeometric distance guidance to encourage the encoder to\ninvert the generator mappings with symmetrically orches-\ntrated recurrent generator and encoder components. Ex-\nploiting the symmetry, we craft an efﬁcient implementation\narchitecture with weight sharing.\n• We conduct a series of systematic experiments to demon-\nstrate the effectiveness of our proposed approach for un-\nsupervised representation learning from sequential multi-\nmodal data through downstream classiﬁcation tasks.\nII. BACKGROUND\nMulti-modal sensing platforms continuously record mea-\nsurements through different sensor channels over time and\ngenerate sequential multi-modal data. The acquired stream is\nthen partitioned into segments x ∈RD×W using a sliding\nwindow, where D denotes the number of sensing modalities\nused for data acquisition and W represents the choice for\nthe window duration. Here, the goal is to learn unsupervised\nrepresentations enriched with distinctive features that can\nsubsequently beneﬁt classiﬁcation of generated sequences. In\nwhat follows, we ﬁrst discuss how existing GAN frameworks\ncan be adopted for unsupervised representation learning of\nsuch sequences (Section II). Highlighting the existing chal-\nlenges, we then introduce our novel framework to uncover\nunsupervised representations with higher correspondence to\nclass semantics (Section III).\nA. Recurrent Generative Adversarial Networks\nThe standard GAN [11] comprises of two parameterized\nfeed-forward neural networks—a generator Gφ and a dis-\ncriminator Dω—competing against one another in a minimax\ngame. Ultimately, the goal is for the generator to capture the\nunderlying data distribution px. To this end, the generator\nexploits a simple prior distribution pz to produce realistic\nsamples that trick the discriminator. On the contrary, the\ndiscriminator is trained to distinguish between the real and the\ngenerated samples. The resulting adversarial game optimises\nmin\nGφ max\nDω Ex∼px[log Dω(x)] + Ez∼pz[log(1 −Dω(Gφ(z)))],\n(1)\nwhere, x ∼px represent the mini-batch training samples and\nz ∼pz denote the drawn latents.\nExtending the vanilla GAN to generate sequences of real-\nvalued data, [19] substitutes both the generator and discrimina-\ntor with recurrent neural networks and develops the Recurrent\nGAN (RGAN) for medical time-series generation. Within the\nresulting framework depicted in Fig. 2-a, the generator Gφ\ntakes a latent sample z and sequentially generates multi-\nchannel data for each time-step. Similarly, the discriminator\nDω consumes an input sequence and delivers per time-step\nclassiﬁcation decisions. We visualize the internal structure of\nthese components in Fig. 3.\nWhile the focus in [19] is solely on sequence generation, in\nour experimental study, re-investigate the framework for the\nReal\nor\nFake?\nReal\nor\nFake?\n(a) Recurrent GAN [Esteban et al,. 2017]\n(b) Recurrent FAAE [Zhang et al,. 2018]\nFig. 2: Generative framework pipelines for (a) RGAN [19], and (b) Recurrent adaptation of ﬂipped adversarial autoencoder\nproposed in [16].\nLatent Space\nShared Linear \nLSTM\nCopied T times\nGenerator \nShared Linear \nLSTM\nDiscriminator \nReal or Fake?\n(a) Discriminator \n(c) Encoder \nLatent Space\nLinear \nLSTM\nEncoder \n(b) Generator  \nInput Latent\nInferred Latent\nInput Sequence\nGenerated Sequence\n(or    )\nInput Sequence\nFig. 3: An illustration of baselines’ recurrent building blocks: (a) Discriminator functions in the data space to produce real vs.\nfake classiﬁcation scores at each time-step. (b) Generator consumes a latent input repeated to the sequence length and produces\na synthetic sample in the sequential data space. (c) Encoder serves as the inference machine and projects an input sequence\ninto its corresponding latent representation. Notably, we depict a slightly modiﬁed version of [19]; i.e.: i) a shared linear layer\nis added on top of the recurrent networks; and ii) instead of sampling independent latents, a single latent is sampled and\nreplicated to the sequence length.\npurpose of unsupervised feature learning for sequences; the\nintermediate representations from the trained discriminator of\na GAN are found to capture useful feature representations for\nrelated supervised tasks [24]. Intuitively, these set of features\nare attained free of cost and encoded in the discriminator\nweights when distinguishing real sequences from generated\nsequences during training. Notably, RGAN provides the ar-\nguably most straightforward extension of a regular GAN for\nthe sequential domain and thus, we base our own investigations\nby building upon this framework.\nB. Recurrent Flipped Adversarial AutoEncoder\nDespite their empirical strength to model arbitrary data\ndistributions, the vanilla GAN and in turn the RGAN, nat-\nurally lack an inference mechanism to directly infer the latent\nrepresentation z for a given data sample x. Accordingly,\n[16] proposes a natural extension of GANs to jointly train\nan encoder network that embodies the inverse mapping and\ncoins the name Flipped Adversarial AutoEncoder (FAAE).\nVisualized in Fig. 2-b, the resulting framework exploits the\nadversarial guidance of a discriminator in the data space\nexactly identical to a regular GAN. In order to train the\nencoder, it additionally minimizes the reconstruction error\nassociated with reproducing the latent representations. The\ntraining objective thus translates to:\nmin\nGφ,Eθ max\nDω Ex∼px[log Dω(x)]\n+ Ez∼pz[log(1 −Dω(Gφ(z)))]\n+ ∥z −Eθ(Gφ(z))\n|\n{z\n}\nzrec\n∥2\n2,\n(2)\nwhere, Eθ denotes the parameterized encoder network and\nzrec = Eθ(Gφ(z)) is the reconstructed latent representation.\nSince there exists no previous exploration of FAAE frame-\nwork for the sequential domain, we design and augment\nRGAN with a recurrent encoder Eθ depicted in Fig. 3. In\nparticular, the encoder reads through the generated sequences\nfrom the generator and updates its internal hidden state accord-\ning to the received measurements at each time step. Ultimately,\nthe ﬁnal hidden state after processing the entire sequence is\nexploited to regress the latent representations. In addition, the\nencoder and generator receive adversarial feedback from the\ndiscriminator for parameter updates during training.\nC. Bi-directional Generative Adversarial Networks\nWhile the FAAE framework paves the way for learning\nthe inverse mapping function, the encoder Eθ performance\nis heavily reliant on the quality and diversity of generator’s\nproduced samples. Essentially, the encoder is never exposed\nto the original data from the training set and thus, its learned\nfeature representations are handicapped by the generator’s\nperformance. Accordingly, [12], [13] propose the BiGAN with\na novel approach to integrate efﬁcient inference. We illustrate\nBiGAN while contrasting it with our proposed formulation,\nGuided-GAN, in the gray shaded area in Fig. 1; i.e., the\ndiscriminator is modiﬁed to discriminate not only in the\ndata space, but rather in the joint data-latent space between\n(x, Eθ(x)) and (Gφ(z), z) pairs. Hence, the corresponding\nminimax objective is deﬁned as\nmin\nGφ,Eθ max\nDω Ex∼px[log Dω(x, Eθ(x))]\n+ Ez∼pz[log(1 −Dω(Gφ(z), z))].\n(3)\nTo satisfy the objective, the generator is motivated to\nproduce samples resembling the real data distribution and\n(or    )\n(or    )\n(or    )\nLatent Space\nShared Linear \nLSTM\nCopied T times\nShared Linear \nLSTM\nReal or Fake?\n(c) Encoder \nLatent Space\nLinear \nLSTM\n(b) Generator \nInput Latent\nInferred Latent\nInput Sequence\nGenerated Sequence\nInput Latent\nCopied T times\nLatent Space\nLinear \nInput Sequence\n(a) Joint Discriminator \nConcat\n(or        )\n(or    )\n(or        )\nFig. 4: Our developed recurrent components: (a) Discriminator receives pairs of data-latents, learns a sequence of aggregated\nfeature representations and delivers per time-step classiﬁcation scores; (b) Generator and (c) Encoder leverage the same\narchitecture design as RGAN and FAAE. But, they are trained to minimize the associated reconstruction errors in data and\nlatent spaces, respectively. L represents the concatenation operation of projected data and latent features.\nthe encoder is incentivized to output latent representations\nmatching with the prior latent distribution. It is shown in [12]\nthat the theoretical optimal solution to this adversarial game\nleads to the encoder and generator inverting one another while\nthe joint distributions are aligned. Importantly, the encoder in\nBiGAN has the luxury of directly learning from real samples\nx ∼px.\nIII. OUR PROPOSED FRAMEWORK\nWe formulate a novel framework, as illustrated in Fig. 1, to\nuncover unsupervised representations with higher correspon-\ndence to class semantics by drawing inspiration from BiGANs.\nWe contrast our proposed formulation, Guided-GAN with the\nBiGAN components in Fig. 1; speciﬁcally, our discriminator\ndesign not only discriminates in the data space, but in the joint\ndata-latent space between (x, Eθ(x)) and (Gφ(z), z) pairs. Our\ndesign: i) realises a state-of-the art GAN for representation\nlearning from sequential multi-modal data; and ii) alleviates\nthe convergence problem [13], [16]. We explain our design\nand the efﬁcient implementation architecture next.\nGeometrically-Guided Adversarial Feedback. We observe,\nthat the generator and encoder within the BiGAN framework\ndo not directly communicate. Thus, the discriminator alone\nbears the burden of matching the joint data-latent distributions\nand guiding the encoder and generator components towards\ninverting one another at the optimal solution. Unfortunately,\nconverging to the optimal theoretical solution is difﬁcult to\nachieve in practice; thus, the encoder and the generator do not\nnecessarily invert one another [13], [16]. Hence, in addition\nto the adversarial feedback provided by the discriminator to\nmatch the joint data-latent distribution, we optimize geometric\ndistance functions to match the marginal manifolds indepen-\ndently; i.e., we receive gradients from aligning: i) the origi-\nnal data manifold with generator’s induced output manifold;\nand ii) the prior latent manifold with the encoder’s output\nmanifold. In particular for early stages of training, geometric\ndistance optimization usually provides much easier training\ngradients [29]. We discovered this to be a vital necessity for\nsuccessful training of a BiGAN in the sequential domain where\nGAN heuristics may be missing. Notably, our attempts to train\nrecurrent BiGANs without our proposed manifold distance\nminimization terms were unsuccessful—see Appendix VII.\nSpeciﬁcally, the encoder did not learn useful representations\nand resulted in extremely low downstream classiﬁcation per-\nformance. To this end, we measure and minimize the re-\nconstruction errors associated with reproducing both the data\nand the latent representations. Hence, our proposed minimax\nadversarial game is formulated as:\nmin\nGφ,Eθ max\nDω Ex∼px[log Dω(x, Eθ(x))]\n+ Ez∼pz[log(1 −Dω(Gφ(z), z))]\n+ λx∥x −Gφ(Eθ(x))\n|\n{z\n}\nxrec\n∥2\n2 + λz∥z −Eθ(Gφ(z))\n|\n{z\n}\nzrec\n∥2\n2,\nwhere ∥.∥2\n2 imposes the ℓ2 reconstruction in data and latent\nfeature spaces. Notably, the reconstruction errors are efﬁciently\ncomputed at no extra model complexity cost through weight\nsharing depicted in Fig. 1. Here, λx and λz denote the loss\nbalance coefﬁcients.\nRecurrent Symmetrical Adversarial Framework. In order\nto exploit the sequential nature of multi-modal wearable data\nstreams, the core building blocks of our framework are em-\npowered by recurrent units with memory cells, as depicted in\nFig. 4. Moreover, the generator and encoder communicate in a\nsymmetrical orchestration to measure the manifold distances\nin data and latent spaces.\nRecurrent Joint Discriminator—The discriminator Dω op-\nerates in the joint data-latent space attempting to differentiate\njoint input samples of (x, Eθ(x)) against (Gφ(z), z) pairs.\nInternally, the multi-modal stream is initially processed by an\nLSTM network yielding a sequence of hidden state represen-\ntations. Simultaneously, the latent input is linearly projected\nand concatenated to the LSTM hidden states at each time-\nstep. The resulting sequence aggregates the learned features\nfrom both data and latent spaces, and is used to construct per\ntime-step classiﬁcation decisions.\nSymmetrical Generator and Encoder—The generator Gφ\nand encoder Eθ of our framework are symmetrically connected\nand serve augmented responsibilities: i) unlike a recurrent\nGAN, the generator is additionally exposed to encoded latents\nfrom the encoder’s posterior distribution (ˆz = Eθ(x); x ∼px)\nand is trained to reconstruct the input sequence; and ii) the\nencoder observes the generator’s outputs (ˆx = Gφ(z); z ∼pz)\nand learns to regress the corresponding latent representation.\nNotably, given the symmetrical architecture design, our en-\ncoder now has access to both the original data samples\n(in contrast to FAAE) as well as the newly generated data\nsamples (as opposed to BiGAN) to uncover generalizable\nfeature representations and serve as a fully ﬂedged feature\nextractor.\nIV. EXPERIMENTAL EVALUATIONS\nTo validate our framework, we employ HAR benchmark\ndatasets UCI HAR [30] and USC-HAD [31] exhibiting di-\nversity in terms of the sensing modalities used and activi-\nties. To allow easy visual interpretation of the results, we\nfurther re-purpose the popular MNIST hand-written digits\ndataset [32] and explore sequential digit classiﬁcation, as\nin [19]. We follow the standard evaluation protocol in [33]\nto assess the quality of unsupervised representations achieved\nfor downstream supervised classiﬁcation tasks. We detail the\nfair evaluation protocol, parameter settings and input-data. In\nsummary, we train all network modules using the unlabeled\ntraining split sequences. Subsequently, we freeze the feature\nextractor parameters and leverage the training labels to train a\nsingle linear classiﬁer on the learned representations. Except\nfor the RGAN baseline [19], the encoder network E within\nthe frameworks serves as the feature extractor. In particular:\n• We describe the adopted representation learning baselines\n(§IV-A) and the fair evaluation protocol following previ-\nous studies(§IV-B).\n• We evaluate the effectiveness of the learned representa-\ntions by transferring them for use in downstream classi-\nﬁcation tasks (§IV-C).\n• We assess the generalization of the the feature representa-\ntions learned by the investigated unsupervised approaches\nfor each classiﬁcation task by training a single linear\nclassiﬁer on the frozen learned representations(§IV-D).\n• To share further insights through extensive experiments;\nWe assess the ability of the unconditional generator\ntrained through Guided-GAN to produce diverse and re-\nalistic multi-modal sequences (§IV-E) as well as evaluate\nand visualize the faithfulness of sequential multi-modal\ndata reconstructions by our Guided-GAN (§IV-F).\nA. Unsupervised Activity Representation Learning Baselines\nWe brieﬂy introduce the alternative approaches that serve\nas concrete baselines for the task of unsupervised activity\nrepresentation learning for HAR applications:\nRecurrent Autoencoder (RAE) [9]. The framework comprises\nof a deterministic encoder E and a decoder G trained directly\nto minimize ℓ1 or ℓ2 element-wise reconstruction error in the\ndata space.\nMotion2Vector (M2V) [10].\nThis framework includes a de-\ncoder G, however with a stochastic encoder E parameterizing\nan Isotropic Gaussian N(0, I). The framework is trained with\nreconstruction error in the data space as well as KL-divergence\nadditionally optimized for the latent space; KL-divergence is\nincorporate to match the encoder output distribution with the\nstandard Gaussian prior.\nRecurrent Generative Adversarial Network (RGAN) [19]. The\nframework trains a generator G and a discriminator D by\noptimizing the standard adversarial loss in the data space\naccording to Eq. (1). in main manuscript.\nRecurrent Flipped Adversarial Autoencoder (RFAAE). We\nadapt the approach proposed in [16] to a recurrent framework.\nIn addition to the generator G and discriminator D of a stan-\ndard GAN, this baseline jointly trains an additional encoder E\nto regress the latents; according to Eq. (2), the adversarial loss\nis optimized in the data space, and the ℓ2 reconstruction error\nis minimized in the latent space between the encoder outputs\nand the sampled priors.\nB. Evaluation Protocol\nFollowing [9], sensory data are down-sampled to 33 Hz,\nand per-channel normalization is adopted using the training\ndata statistics to scale the values into the range [−1, 1].\nSubsequently, the data-streams are partitioned into segments\nusing a sliding window of 30 samples (i.e., W=30).\nWe also follow the standard evaluation protocol in [33]\nto assess the quality of unsupervised learned sequence rep-\nresentations of different baselines. All network parameters are\ntrained end-to-end for 500 epochs by back-propagating the\ngradients of the corresponding loss functions averaged over\nmini-batches of size 64 and using the Adam [34] update rule.\nThe learning rate for Adam is set to 10-3 and the beta values\nβ1 = 0.5, β2 = 0.999 are used. The ﬁxed prior distribution\npz for deep generative models is set to be a 100-dimensional\nisotropic Gaussian N(0, I). To ensure a fair comparison, the\nencoder E, generator (interchangeably decoder in autoencoder\nframeworks) G, and discriminator D constitute a single-layer\nuni-directional LSTM with 100 hidden neurons to process the\ninput sequences. For our Guided-GAN, the loss weighting\ncoefﬁcients λz = 1 and λx = 0.01 are kept constant across\nthe sensory datasets.\nFirst, we train all network modules using the unlabeled\ntraining split sequences. Subsequently, we freeze the feature\nextractor parameters and leverage the training labels to train a\nsingle linear classiﬁer on the learned representations. Except\nfor the RGAN baseline [19], the encoder network E within the\nframeworks serves as the feature extractor. For the RGAN\nbaseline lacking an encoder module, the penultimate repre-\nsentations from the discriminator network D are used as the\nunsupervised features. The trained classiﬁer is then evaluated\non the held-out test split sequences and we report the achieved\nclassiﬁcation performance.\nC. Downstream Sequence Classiﬁcation\nIn Table I, we summarize the downstream classiﬁcation\nperformance by reporting the classiﬁcation accuracy and class-\naveraged f1-score (Fm). Given the potential imbalanced class\nTABLE I: Comparison of unsupervised learned representations when transferred for use on downstream classiﬁcation tasks.\nAll methods employ recurrent neural networks. (∗) RFAAE is our recurrent adaptation of [16].\nRecurrent Representation\nSeq. MNIST\nUCI HAR\nUSC-HAD\nLearning Method\nAcc.\nf1-score\nAcc.\nf1-score\nAcc.\nf1-score\nRAND\n51.4%\n48.8%\n51.6%\n44.3%\n34.3%\n21.2%\nM2V\n82.8%\n82.6%\n70.9%\n69.8%\n54.1%\n44.0%\nRGAN\n80.2%\n79.8%\n76.3%\n75.6%\n50.7%\n42.8%\nRFAAE (See˜∗)\n94.0%\n93.9%\n88.5%\n88.4%\n64.4%\n57.2%\nRAE-ℓ1\n95.5%\n95.5%\n87.1%\n87.1%\n63.8%\n54.8%\nRAE-ℓ2\n95.7%\n95.7%\n87.2%\n87.2%\n65.2%\n56.0%\nOurs (Guided-GAN)\n97.3%\n97.3%\n89.0%\n88.9\n67.2%\n59.9%\nSUP (fully supervised)\n99.1%\n99.1%\n91.1%\n91.0%\n68.6%\n62.6%\nWalk-Forward\nWalk-Left\nWalk-Right\nUpstairs\nDownstairs\nRun\nJump\nSit\nStand\nSleep\nElevator-Up\nElevator-Down\nPredicted label\nWalk-Forward\nWalk-Left\nWalk-Right\nUpstairs\nDownstairs\nRun\nJump\nSit\nStand\nSleep\nElevator-Up\nElevator-Down\nTrue label\n0.55 0.17 0.16 0.09 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00\n0.06 0.86 0.02 0.04 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.11 0.04 0.72 0.12 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.02 0.12 0.00 0.62 0.07 0.00 0.01 0.01 0.02 0.00 0.05 0.07\n0.04 0.10 0.03 0.08 0.64 0.00 0.01 0.01 0.01 0.00 0.03 0.04\n0.03 0.01 0.01 0.08 0.01 0.61 0.24 0.00 0.01 0.00 0.00 0.00\n0.02 0.04 0.01 0.03 0.09 0.02 0.60 0.00 0.04 0.01 0.06 0.09\n0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.75 0.12 0.00 0.04 0.06\n0.00 0.03 0.01 0.05 0.00 0.00 0.00 0.01 0.44 0.00 0.17 0.29\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.44 0.10 0.01 0.25 0.19\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.39 0.11 0.00 0.24 0.26\nUSC-HAD Dataset\nWalking\nUpstairs\nDownstairs\nSitting\nStanding\nLaying\nPredicted label\nWalking\nUpstairs\nDownstairs\nSitting\nStanding\nLaying\nTrue label\n0.90\n0.04\n0.04\n0.01\n0.01\n0.00\n0.08\n0.84\n0.07\n0.00\n0.01\n0.00\n0.03\n0.05\n0.91\n0.00\n0.01\n0.00\n0.00\n0.02\n0.00\n0.79\n0.18\n0.01\n0.01\n0.00\n0.00\n0.11\n0.88\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\nUCI HAR Dataset\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPredicted label\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTrue label\n0.99\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.99\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.97\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.96\n0.00\n0.01\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n0.97\n0.00\n0.01\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.01\n0.00\n0.96\n0.01\n0.00\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.99\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n0.97\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.01\n0.00\n0.00\n0.97\n0.01\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.01\n0.97\nSequential MNIST Dataset\nFig. 5: Class-speciﬁc recognition results from the Guided-GAN’s unsupervised features. The confusion matrices highlighting the\nclass-speciﬁc recognition performance for the testing splits of Sequential MNIST, UCI HAR, and USC-HAD benchmarks.\nThe vertical axis represents the ground-truth labels and the horizontal axis denotes the predicted labels.\ndistributions, the latter metric reﬂects the ability of the HAR\nmodel to recognize every activity category regardless of its\nprevalence in the collected data. In Table I, RAE [9] and\nM2V [10] respectively employ recurrent autoencoder and\nvariational autoencoder frameworks. We further present results\nfrom a fully supervised trained feature extractor (SUP) and a\nrandomly initialized feature extractor (RAND) for reference.\nAcross the three datasets, the large performance gap be-\ntween RGAN (80.2%, 76.3% and 50.7% respectively) against\nRFAAE (94%, 88.5% and 64.4%) highlights the signiﬁcance\nof incorporating an inference network for effective representa-\ntion learning in generative adversarial frameworks. While the\ndiscriminator’s delegated task of distinguishing between real\nand generated sequences beneﬁts the penultimate represen-\ntations (see the superiority of RGAN over RAND baseline),\nlearning an inverse mapping to the latent feature space\nthrough encoder results in signiﬁcantly more effective features.\nHowever, the encoder in RFAAE is only trained on synthetic\nsequences and never encounters real data samples. Accord-\ningly, its performance as a feature extractor is heavily reliant\non the quality and diversity of the generator’s sequences. In\ncontrast, the encoder in our framework, exposed to both real\ndata sequences as well as generated ones evidently offers\nfeature representations of higher quality, achieving 3.51%,\n0.56% and 4.35% relative improvements, respectively, on\nSequential MNIST, UCI HAR and USC-HAD datasets.\nComparing the lower performance levels of M2V against\nits non-variational counterparts (RAE-ℓ1 and RAE-ℓ2), we\nobserve that its ability to sample new sequences comes at the\ncost of harming the feature representation qualities. However,\nour proposed framework bridges this shortcoming by allowing\nthe generation of realistic synthetic data while simultane-\nously achieving higher quality representations. Notably, our\napproach not only outperforms existing unsupervised baselines\nwith a large margin but it also closely approaches the fully\nsupervised baseline–SUP–performance.\nWe summarise the number of parameters corresponding to\nthe feature extractor (frozen), classiﬁer (trainable) and the\nratio of trainable parameters to the total number of network\nparameters (trainable ratio) in Table II. We can observe the\nproposed Guided-GAN to achieve comparable classiﬁcation\nperformance to that of the fully supervised baseline (SUP)\nwhilst having access to only 1.6%, 1.1%, and 2.2% of the\nparameters for training on the three datasets, respectively.\nFig. 6: Effectiveness of learned sequence representations when building a classiﬁer with varying sizes of labelled training\ndata. We freeze the feature extractor parameters and employ training labels to train a single linear classiﬁer on the learned\nrepresentations. We train 1.6%, 1.1%, and 2.2% of the parameters, respectively, on the three datasets and report mean values\nover the entire held-out test splits in ﬁve runs with different subsets of training data.\nTABLE II: A detailed description on the number of trainable\nand frozen network parameters employed for downstream\nclassiﬁcation evaluation on the three benchmarks.\nParameter Count\nSequential MNIST\nUCI HAR\nUSC-HAD\nTotal\n63110\n55106\n54512\nFrozen\n62100\n54500\n53300\nTrainable\n1010\n606\n1212\n(Trainable Ratio)\n(1.6%)\n(1.1%)\n(2.2%)\nFor reference, we summarise the class-speciﬁc recognition\nresults from the Guided-GAN’s unsupervised features by pre-\nsenting confusion matrices for the downstream classiﬁcation\ntasks in Fig. 5.\nD. Generalisation of the Feature Representations\nTo gain further insights into the generalization capability\nof feature representations learned by the investigated unsuper-\nvised approaches, we analyze the classiﬁcation performance\non the entire testing splits for the three datasets while varying\nthe amount of available labeled data for supervised classiﬁer\ntraining in Fig. 6. The reported results are averaged over ﬁve\nruns with different subsets of training data.\nWe observe the unsupervised baselines to provide an ef-\nfective means to learn useful feature representations by ex-\nploiting unlabeled data in the absence of large amounts of\nannotated training data, resulting in substantial performance\ngains over the RAND and SUP baselines; supervised classiﬁer\ntraining on top of a randomly initialized feature extractor\n(RAND baseline) fails to learn clear classiﬁcation boundaries\nto discriminate different classes regardless of the amount of\navailable labeled data and the fully supervised baseline (SUP\nbaseline) struggles to generalize to the unseen test sequences\nwhen trained on low volumes of annotated samples. But: i) our\napproach consistently offers better generalization to unseen\ndata compared with existing unsupervised remedies in the\npresence of extremely limited labeled training data; and ii)\nGuided-GAN achieves higher classiﬁcation performance when\nleveraging all labeled training data and is comparable with the\nsupervised baselines trained end-to-end with full supervision.\nE. Assessment of Multi-modal Sequence Generation\nWhile unsupervised feature learning constitutes the main\nfocus of our study, we qualitatively demonstrate the ability\nof the unconditional generator trained through Guided-GAN\nin producing diverse and realistic sequences. To this end, we\nvisualise the data spaces for both the real and generated\ndatasets in 2D using t-SNE [35] in Fig. 7 and Fig. 8 for\nSequential MNIST and UCI HAR respectively.\nWe can observe that the generated data distribution closely\nfollows the real data distribution, as indicated by the dense\noverlap between their corresponding sequence samples. In\naddition, we observe a smooth interpolation in the space be-\ntween different categories for the generated sequences; e.g., the\ngenerated samples interconnecting the Lying and Sitting\nactivity categories in Fig. 8-b. Further, we depict a set of gen-\nerated sequences where visual inspection of generated data for\nSequential MNIST clearly demonstrates a conformance to\nthe class label semantics.\nF. Faithfulness of Reconstructions\nIt has been reported in [13], [16] that the reconstructions of\ndata with Bidirectional GAN (BiGAN) [12] and Adversarially\nLearned Inference (ALI) [13] are not always faithful reproduc-\ntions of the inputs; in extreme cases deviating entirely from the\nsemantic labels.\nWe conduct a set of experiments to quantitatively measure\nthe veracity of the sequential multi-modal data reconstruc-\ntions by our Guided-GAN. To this end, we rigorously ex-\nplore the downstream classiﬁcation tasks on Sequential\nMNIST and UCI HAR while considering different develop-\nment datasets (denoting the dataset used to train the supervised\nclassiﬁer) and evaluation datasets (denoting the dataset used\nfor evaluation):\n• Train—The standard training split data and labels.\n• Test—The standard testing split data and labels.\n• Reconstructed Train—The reconstructed training data\nattained by applying xrec = Gφ(Eθ(x)) for every sequence\nin the original Train split whilst retaining the original\nlabels.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nReal Data\nGenerated Data\n(a) Real Data \n(b) Generated Data overlaid on Real Data \nFig. 7: Sequential model added for ease of visual interpretation. For Sequential MNIST dataset, we present t-SNE\nvisualisation of: (a) real data sequences colour-coded with semantic labels, and (b) generated sequences overlaid on real data\nsamples. Evidently, our Guided-GAN’s generator successfully captures semantic variations in the data and aligns with the real\ndata distribution.\nTABLE III: We quantitatively assess the faithfulness of data\nreconstructions through rigorous evaluation on downstream\nclassiﬁcation tasks. We report classiﬁcation accuracy together\nwith class-averaged F-score (value in parenthesis) on the\nholdout evaluation datasets.\nDevelopment\nDataset\nEvaluation\nDataset\nSequential\nMNIST\nUCI HAR\nTrain\nTest\n97.3 (97.3)\n89.0 (88.9)\nReconstructed Train\nTest\n95.1 (95.0)\n84.2 (84.0)\nTrain\nReconstructed Test\n93.9 (93.8)\n83.4 (83.1)\nReconstructed Train\nReconstructed Test\n94.6 (94.6)\n84.9 (84.7)\n• Reconstructed Test—The reconstructed test data at-\ntained by applying xrec = Gφ(Eθ(x)) for every sequence\nin the original Test split whilst maintaining the original\nlabels.\nWe summarise the corresponding classiﬁcation perfor-\nmances in Table III. From the results, across both datasets,\nwe observe that Guided-GAN demonstrates reconstructions of\nreasonable faithfulness to their original semantic categories;\ni.e., substituting the original data splits—e.g. Train—with\ntheir corresponding reconstructions—Reconstructed Train—\nstill allows learning a classiﬁer with comparable performance\nto the original data splits. We further substantiate this by\nincluding qualitative samples of data reconstructions in Fig.\n9 for reference.\nV. RELATED WORK\nUnsupervised Learning in HAR. A thread of studies in\nubiquitous computing leverage unsupervised learning as a\nmeans for pre-training deep activity recognition models prior\nto supervised ﬁne-tuning with labeled data [5], [36]–[39].\nExploiting the easily accessible unlabeled activity datasets\nin the pre-training stage guides the following discriminative\ntraining phase to better generalization performance on unseen\nwearable data. In this regard, [36] pre-trains weights of a deep\nbelief network using unlabeled spectrograms extracted from\nacceleration measurements. In another work, [37] replaces\nthe layer-wise pretraining with end-to-end optimization of a\nconvolutional autoencoder applied on raw multi-modal sensor\ndata; while [38] explores a self-supervised pre-training objec-\ntive. Yet, other studies explore semi-supervised methods [39].\nHowever, these studies involve a subsequent supervised ﬁne-\ntuning of learned parameters and thus, no isolated analysis\nis presented towards assessing the quality of unsupervised\nactivity features.\nAnother line of research explores unsupervised representa-\ntion learning of human activities through sensor data captured\nby wearables. Early efforts in this area witnessed adoption\nof Restricted Boltzmann Machines (RBMs) [40] and vanilla\nautoencoders [41], [42] comprised of deep feed-forward neural\nnetworks. These approaches lack recurrent structures and thus,\ninherently fail to capture the temporal aspect of sensory data.\nRecently, [9] proposed recurrent autoencoders to account for\nthe temporal dependencies among sequential sensor samples.\nA LSTM encoder ﬁrst processes the input multi-channel\nsequence into a compact latent representation. The extracted\nrepresentation is then sequentially manipulated by a LSTM\ndecoder to reproduce the sensor data. As training progresses,\nthe encoder is expected to capture salient features required\nfor satisfying the reconstruction task. In another recent work,\n[10] designed a recurrent variational auto-encoder, namely\nMotion2Vector, by enforcing the latent representations of the\nGenerated Data\n(a) Real Data \n(b) Generated Data overlaid on Real Data \nWalking\nUpstairs\nDownstairs\nSitting\nStanding\nLying\nDownstairs\nWalking\nUpstairs\nLying\nSitting\nStanding\nReal Data\nFig. 8: For UCI HAR benchmark, we present t-SNE visualisation of: (a) real data sequences colour-coded with semantic activity\nlabels, and (b) generated sequences overlaid on real data samples. Our Guided-GAN’s generator is observed to successfully\ncapture semantic variations embedded in the multi-modal motion sequences.\n(a) Sequential MNIST\n(b) UCI HAR\nFig. 9: We present qualitative results for data reconstructions with Guided-GAN for (a) Sequential MNIST and (b) UCI\nHAR datasets, where the odd rows represents the original data x and the even rows are the corresponding reconstructions\nxrec = Gφ(Eθ(x)). Interestingly, we observe reconstructions for UCI HAR sequences reﬂecting different phases and variations\nof activity sequences whilst preserving the semantics of the reconstructed sequences.\nencoder to follow the standard multivariate Gaussian. The ef-\nfectiveness of the proposed unsupervised deep learning model\nis studied on epileptic patient activity data.\nUnsupervised Representation Learning with Generative Ad-\nversarial Networks. GANs have demonstrated great success\nin approximating arbitrary complex data distributions and\nthus, have emerged as the state-of-the-art for realistic data\ngeneration on variety of benchmarks [24], [43], [44]. Beyond\ndata generation, recent interests investigate GAN’s latent space\nfor unsupervised feature learning through the addition of an\nencoder network. In [15], mutual information maximization\nis adopted to infer and gain control over a subset of latent\nfeatures. In order to achieve full inference on the latents in\n[14], [16], the generator output is directly fed to an encoder\nnetwork that is trained to reconstruct the latents entirely.\nTaking an alternative approach in learning the generator’s\ninverse mappings, the encoder and the generator do not\ndirectly communicate in [12], [13]. However, the discriminator\nreceives pairs of data and latents to conduct the discrimination\ntask in the joint space. The resulting adversarially learned\nrepresentations demonstrate state-of-the-art performance when\ntransferred for auxiliary supervised discrimination tasks on\nnatural images.\nGenerative Adversarial Networks for Multi-modal Sequen-\ntial Data. Interest in the adoption of GANs for sequen-\ntial multi-modal data has predominantly focused on realistic\nsequence generation. In a preliminary work, [18] proposes\na generative adversarial model that operates on continuous\nsequential data and applies the framework on a collection of\nclassical music. The resulting GAN adopts LSTM networks\nfor generator and discriminator to generate polyphonic mu-\nsic. Applying architectural modiﬁcations, the authors in [19]\ndevelop a recurrent GAN to produce synthetic medical time-\nseries data. The approach is further extended to exploit data\nannotations for conditional generation in order to substitute\nsensitive patient records. Particularly for HAR applications,\nauthors in [20] employ GANs to generate synthetic sensor\ndata preserving statistics of smartphone accelerometer sensor\ntraces. Similarly in [21], authors attempt to synthesize sen-\nsory data captured by wearable sensors for human activity\nrecognition. Due to the challenging nature and heterogeneity\nof sequential sensor data, the authors faced difﬁculties devel-\noping a uniﬁed GAN to cover the multi-modal distribution\nof human actions. Accordingly, the framework leverages data\nannotations to train independent activity-speciﬁc GANs and\nachieve stable training.\nIn another work, [22] trains a GAN with pre-processed\nWiFi signals and generates new patterns with similar statistical\nfeatures to expand the available training data. The augmented\ndataset is subsequently used to train a LSTM recognition\nmodel to infer human actions in indoor environments. Both\ndiscussed studies operate on pre-processed input features in-\nstead of raw multi-channel sensor data. Besides synthetic se-\nquence generation, recent semi-supervised activity recognition\nhas also beneﬁted from adversarial training. In this regard, [45]\nassumes availability of limited labeled data during training and\ndevelops a convolutional adversarial autoencoder to encode\nmulti-channel sequences into discrete class representations as\nwell as continuous latent variables. The resulting framework is\nshown to achieve additional performance boost by exploiting\nunlabeled data. Similarly in a semi-supervised context, [46]\ninvestigates the problem of cross-subject adversarial knowl-\nedge transfer using GANs in the domain of human activity\nrecognition.\nSummary. In the realm of unsupervised learning for HAR\napplications, training variants of autoencoders with element-\nwise reconstruction objective constitutes the dominant unsu-\npervised representation learning pipeline. While in the visual\ndomain, deep generative models have demonstrated great\npotential in unsupervised learning of enriched features for\nnatural images, their application for human activity recognition\nwith sequential sensor data is mainly limited to synthesizing\nartiﬁcial sequences that resemble wearable sensor sequences.\nAccordingly, this paper presents a ﬁrst rigorous study of\nGAN-based alternatives for unsupervised learning of activity\nrepresentations in our attempts to bridge the gap between un-\nsupervised representation learning and generative adversarial\nnetworks for human activity recognition with wearables.\nVI. CONCLUSIONS\nWe propose Guided-GAN in our efforts to abridge un-\nsupervised representation learning and generative adversarial\nnetworks for advancing the ﬁeld of unsupervised learning\nfor wearable-HAR. Recognizing the inherent temporal de-\npendencies within the captured sequential data, we design a\nnew recurrent network architecture incorporating in a bidi-\nrectional GAN framework. We share the key insight that the\ndiscriminator adversarial feedback alone may be insufﬁcient to\nuncover the generator’s inverse mapping. Hence, Guided-GAN\nleverages an intuitive formulation to alleviate the burden on the\ndiscriminator in achieving inverting generators and encoders.\nWhen evaluated on three downstream sequence classiﬁcation\nbenchmarks, our learned sequence representations outperform\nexisting unsupervised approaches whilst closely approaching\nthe performance of fully supervised learned features.\nVII. APPENDIX\nWe present our results from experimental evaluations\ndemonstrating the difﬁculty in convergence observed in train-\ning a recurrent BiGAN [44].\nComparing Training of Guided-GAN with Recurrent Bi-\nGAN.\nAs mentioned in Section III, our attempts to train\nrecurrent BiGANs without our proposed manifold distance\nminimization terms were unsuccessful. For reference, we\npresent empirical results obtained from training recurrent Bi-\nGAN as well as our Guided-GAN on Sequential MNIST\nfor ease of visual inspections.\nTo observe the behavior of generators and encoders, we\nvisualize randomly generated samples ˆx = Gφ(z) at different\nstages of the training process together with their corresponding\nreconstructions xrec = Gφ(Eθ(ˆx)) for both recurrent BiGAN\nand our proposed Guided-GAN in Fig. 10. In particular, we\nobserved that the sole discriminator in BiGAN was not able to\nguide the recurrent encoder towards uncovering the generator’s\ninverse mapping function. Thus, no useful representations\nwere obtained and accordingly, extremely low downstream\nclassiﬁcation performance was achieved. In contrast, our pro-\nposed Guided-GAN successfully inverts the generator and\nencoder at the very early stages of training process. We further\nprovide the downstream classiﬁcation performance achieved\nby both approaches in Fig. 11 at set epochs in the training\nprocess.\nREFERENCES\n[1] A. Mannini, M. Rosenberger, W. L. Haskell, A. M. Sabatini, and S. S.\nIntille, “Activity recognition in youth using single accelerometer placed\nat wrist or ankle,” Medicine and Science in Sports and Exercise, vol. 49,\nno. 4, p. 801, 2017.\n[2] R. L. Shinmoto Torres, R. Visvanathan, D. Abbott, K. D. Hill, and\nD. C. Ranasinghe, “A battery-less and wireless wearable sensor system\nfor identifying bed and chair exits in a pilot trial in hospitalized older\npeople,” PLOS ONE, vol. 12, no. 10, pp. 1–25, 10 2017.\n[3] M. G¨overcin, Y. K¨oltzsch, M. Meis, S. Wegel, M. Gietzelt, J. Spehr,\nS. Winkelbach, M. Marschollek, and E. Steinhagen-Thiessen, “Deﬁning\nthe user requirements for wearable and optical fall prediction and fall\ndetection devices for home use,” Informatics for Health and Social Care,\nvol. 35, no. 3-4, pp. 177–187, 2010.\n[4] J. Frank, S. Mannor, and D. Precup, “Activity and gait recognition with\ntime-delay embeddings,” in AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2010.\n[5] N. Y. Hammerla, J. Fisher, P. Andras, L. Rochester, R. Walker, and\nT. Pl¨otz, “PD disease state assessment in naturalistic environments using\ndeep learning,” in AAAI Conference on Artiﬁcial Intelligence (AAAI),\n2015.\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\nEpoch 1\nEpoch 5\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\nEpoch 50\nG(z)\nG(E(G(z)))\nGuided-GAN\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\nEpoch 1\nEpoch 5\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\nEpoch 50\nG(z)\nG(E(G(z)))\nRecurrent BiGAN\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\ndata\nrec\nFig. 10: Training comparisons between our proposed Guided-GAN and recurrent BiGAN. As illustrated, we observed through\nmultiple experiments, the inability of the recurrent BiGAN to uncover the generator’s inverse mapping.\n0\n20\n40\n60\n80\nEpoch\n0\n20\n40\n60\n80\n100\nClassification Accuracy\nGuided-GAN\nRecurrent BiGAN\nFig. 11: A comparison of classiﬁcation accuracy achieved\non holdout test split for Guided-GAN and recurrent BiGAN\nevaluated at set epocs during training process.\n[6] A. Jayatilaka, Q. H. Dang, S. J. Chen, R. Visvanathan, C. Fumeaux,\nand D. C. Ranasinghe, “Designing batteryless wearables for hospital-\nized older people,” in Proceedings of the International Symposium on\nWearable Computers (ISWC), 2019, p. 91–95.\n[7] A. Fellger, G. Sprint, D. Weeks, E. Crooks, and D. J. Cook, “Wearable\ndevice-independent next day activity and next night sleep prediction for\nrehabilitation populations,” IEEE J. Transl. Eng. Health Med., vol. 8,\npp. 1–9, 2020.\n[8] M. Chesser, A. Jayatilaka, R. Visvanathan, C. Fumeaux, A. Sample, and\nD. C. Ranasinghe, “Super low resolution RF powered accelerometers\nfor alerting on hospitalized patient bed exits,” in IEEE International\nConference on Pervasive Computing and Communications (PerCom),\n2019, pp. 1–10.\n[9] H. Haresamudram, D. V. Anderson, and T. Pl¨otz, “On the role of features\nin human activity recognition,” in Proceedings of the International\nSymposium on Wearable Computers (ISWC), 2019, pp. 78–88.\n[10] L. Bai, C. Yeung, C. Efstratiou, and M. Chikomo, “Motion2Vector:\nUnsupervised learning in human activity recognition using wrist-sensing\ndata,” in Proceedings of the ACM International Symposium on Wearable\nComputers (ISWC), 2019, p. 537–542.\n[11] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nin Conference on Neural Information Processing Systems (NIPS), 2014,\npp. 2672–2680.\n[12] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell, “Adversarial feature learn-\ning,” in The International Conference on Learning Representations\n(ICLR), 2017.\n[13] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Ar-\njovsky, and A. Courville, “Adversarially learned inference,” in The\nInternational Conference on Learning Representations (ICLR), 2017.\n[14] G. Perarnau, J. van de Weijer, B. Raducanu, and J. M.\n´Alvarez,\n“Invertible Conditional GANs for image editing,” in NIPS Workshop\non Adversarial Training, 2016.\n[15] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and\nP. Abbeel, “InfoGAN: Interpretable representation learning by informa-\ntion maximizing generative adversarial nets,” in Conference on Neural\nInformation Processing Systems (NIPS), 2016, p. 2180–2188.\n[16] J. Zhang, H. Dang, H. K. Lee, and E.-C. Chang, “Flipped-adversarial\nautoencoders,” arXiv preprint arXiv:1802.04504, 2018.\n[17] J. Donahue and K. Simonyan, “Large scale adversarial representation\nlearning,” in Conference on Neural Information Processing Systems\n(NIPS), 2019, pp. 10 542–10 552.\n[18] O. Mogren, “C-RNN-GAN: Continuous recurrent neural networks with\nadversarial training,” arXiv preprint arXiv:1611.09904, 2016.\n[19] C. Esteban, S. L. Hyland, and G. R¨atsch, “Real-valued (medical)\ntime series generation with recurrent conditional gans,” arXiv preprint\narXiv:1706.02633, 2017.\n[20] M. Alzantot, S. Chakraborty, and M. B. Srivastava, “SenseGen: A deep\nlearning architecture for synthetic sensor data generation,” in IEEE\nInternational Conference on Pervasive Computing and Communications\nWorkshops (PerCom Workshops), 2017.\n[21] J. Wang, Y. Chen, Y. Gu, Y. Xiao, and H. Pan, “SensoryGANs: an\neffective generative adversarial framework for sensor-based human activ-\nity recognition,” in International Joint Conference on Neural Networks\n(IJCNN), 2018, pp. 1–8.\n[22] P. F. Moshiri, H. Navidan, R. Shahbazian, S. A. Ghorashi, and D. Win-\ndridge, “Using GAN to enhance the accuracy of indoor human activity\nrecognition,” arXiv preprint arXiv:2004.11228, 2020.\n[23] J. Yoon, D. Jarrett, and M. van der Schaar, “Time-series generative\nadversarial networks,” in Conference on Neural Information Processing\nSystems (NIPS), 2019, pp. 5508–5518.\n[24] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation\nlearning with deep convolutional generative adversarial networks,” arXiv\npreprint arXiv:1511.06434, 2015.\n[25] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral nor-\nmalization for generative adversarial networks,” in The International\nConference on Learning Representations (ICLR), 2018.\n[26] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and\nX. Chen, “Improved techniques for training gans,” in Conference on\nNeural Information Processing Systems (NIPS), 2016, pp. 2234–2242.\n[27] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of\ngans for improved quality, stability, and variation,” The International\nConference on Learning Representations (ICLR), 2018.\n[28] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and\nB. Schuller, “audeep: Unsupervised learning of representations from au-\ndio with deep recurrent neural networks,” Journal of Machine Learning\nResearch, vol. 18, no. 173, pp. 1–5, 2018.\n[29] T. Che, Y. Li, A. P. Jacob, Y. Bengio, and W. Li, “Mode regularized\ngenerative adversarial networks,” in The International Conference on\nLearning Representations (ICLR), 2017.\n[30] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A public\ndomain dataset for human activity recognition using smartphones.” in\nEuropean Symposium on Artiﬁcial Neural Networks (ESANN), 2013.\n[31] M. Zhang and A. A. Sawchuk, “Usc-had: a daily activity dataset for\nubiquitous activity recognition using wearable sensors,” in Proceedings\nof the ACM Conference on Ubiquitous Computing (UbiComp), 2012,\npp. 1036–1043.\n[32] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,” Proceedings of the IEEE, vol. 86,\nno. 11, pp. 2278–2324, 1998.\n[33] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in\nEuropean Conference on Computer Vision (ECCV), 2016, pp. 649–666.\n[34] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin The International Conference on Learning Representations (ICLR),\n2015.\n[35] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” in Journal\nof Machine Learning Research, 2008.\n[36] M. A. Alsheikh, A. Selim, D. Niyato, L. Doyle, S. Lin, and H.-P.\nTan, “Deep activity recognition models with triaxial accelerometers,”\nin Workshops at the AAAI Conference on Artiﬁcial Intelligence, 2016.\n[37] A. A. Varamin, E. Abbasnejad, Q. Shi, D. C. Ranasinghe, and\nH. Rezatoﬁghi, “Deep auto-set: A deep auto-encoder-set network for\nactivity recognition using wearables,” in Proceedings of the EAI Inter-\nnational Conference on Mobile and Ubiquitous Systems: Computing,\nNetworking and Services (MobiQuitous), 2018, p. 246–253.\n[38] H. Haresamudram, A. Beedu, V. Agrawal, P. L. Grady, I. Essa,\nJ. Hoffman, and T. Pl¨otz, “Masked reconstruction based self-supervision\nfor human activity recognition,” in Proceedings of the International\nSymposium on Wearable Computers (ISWC), 2020, p. 45–49.\n[39] C. I. Tang, I. Perez-Pozuelo, D. Spathis, S. Brage, N. Wareham, and\nC. Mascolo, “SelfHAR: Improving human activity recognition through\nself-training with unlabeled data,” Proc. ACM Interact. Mob. Wearable\nUbiquitous Technol., vol. 5, no. 1, 2021.\n[40] B. Almaslukh, J. AlMuhtadi, and A. Artoli, “An effective deep autoen-\ncoder approach for online smartphone-based human activity recogni-\ntion,” Int. J. Comput. Sci. Netw. Secur, vol. 17, no. 4, pp. 160–165,\n2017.\n[41] T. Pl¨otz, N. Y. Hammerla, and P. L. Olivier, “Feature learning for activity\nrecognition in ubiquitous computing,” in International Joint Conference\non Artiﬁcial Intelligence (IJCAI), 2011.\n[42] J. Wang, X. Zhang, Q. Gao, H. Yue, and H. Wang, “Device-free wireless\nlocalization and activity recognition: A deep learning approach,” IEEE\nTrans. Veh. Technol., vol. 66, no. 7, pp. 6258–6267, 2016.\n[43] E. Denton, S. Chintala, A. Szlam, and R. Fergus, “Deep generative\nimage models using a laplacian pyramid of adversarial networks,” in\nConference on Neural Information Processing Systems (NIPS), 2015, p.\n1486–1494.\n[44] A. Brock, J. Donahue, and K. Simonyan, “Large scale GAN training for\nhigh ﬁdelity natural image synthesis,” in The International Conference\non Learning Representations (ICLR), 2019.\n[45] D. Balabka, “Semi-supervised learning for human activity recognition\nusing adversarial autoencoders,” in Adjunct Proceedings of the ACM\nInternational Joint Conference on Pervasive and Ubiquitous Computing\nand Proceedings of the ACM International Symposium on Wearable\nComputers (ISWC), pp. 685—-688.\n[46] E. Soleimani and E. Nazerfard, “Cross-subject transfer learning in hu-\nman activity recognition systems using generative adversarial networks,”\narXiv preprint arXiv:1903.12489, 2019.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-10-12",
  "updated": "2021-10-12"
}