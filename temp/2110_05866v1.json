{
  "id": "http://arxiv.org/abs/2110.05866v1",
  "title": "MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech",
  "authors": [
    "Szu-Wei Fu",
    "Cheng Yu",
    "Kuo-Hsuan Hung",
    "Mirco Ravanelli",
    "Yu Tsao"
  ],
  "abstract": "Most of the deep learning-based speech enhancement models are learned in a\nsupervised manner, which implies that pairs of noisy and clean speech are\nrequired during training. Consequently, several noisy speeches recorded in\ndaily life cannot be used to train the model. Although certain unsupervised\nlearning frameworks have also been proposed to solve the pair constraint, they\nstill require clean speech or noise for training. Therefore, in this paper, we\npropose MetricGAN-U, which stands for MetricGAN-unsupervised, to further\nrelease the constraint from conventional unsupervised learning. In MetricGAN-U,\nonly noisy speech is required to train the model by optimizing non-intrusive\nspeech quality metrics. The experimental results verified that MetricGAN-U\noutperforms baselines in both objective and subjective metrics.",
  "text": "METRICGAN-U: UNSUPERVISED SPEECH ENHANCEMENT/ DEREVERBERATION \nBASED ONLY ON NOISY/ REVERBERATED SPEECH \n \nSzu-Wei Fu1*, Cheng Yu1, Kuo-Hsuan Hung1, Mirco Ravanelli2, Yu Tsao1 \n \n1 Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan \n2 Mila-Quebec AI Institute, Montreal, Canada \n \nABSTRACT \n \nMost of the deep learning-based speech enhancement \nmodels are learned in a supervised manner, which implies \nthat pairs of noisy and clean speech are required during \ntraining. Consequently, several noisy speeches recorded in \ndaily life cannot be used to train the model. Although \ncertain unsupervised learning frameworks have also been \nproposed to solve the “pair” constraint, they still require \nclean speech or noise for training. Therefore, in this paper, \nwe propose MetricGAN-U, which stands for MetricGAN-\nunsupervised, to further release the constraint from \nconventional unsupervised learning. In MetricGAN-U, only \nnoisy speech is required to train the model by optimizing \nnon-intrusive speech quality metrics. The experimental \nresults verified that MetricGAN-U outperforms baselines in \nboth objective and subjective metrics. \n \nIndex Terms—Unsupervised speech enhancement, \nMetricGAN \n \n1. INTRODUCTION \n \nRecently, deep learning-based speech enhancement models \nhave gained significant improvements compared to \ntraditional methods [1-8]. However, the success is mainly \nbased on a large amount of training data, which includes \nseveral different clean and noisy speech pairs. In general, \nthe noisy speech is synthesized by adding clean speech with \nnoise; hence, both clean speech and noise are required for \nmodel training. Because both are very difficult to obtain in \ndaily life, they are usually recorded in an anechoic chamber. \nThis implies that unlike noisy speech, a lot of time and \neffort is needed to collect them. Therefore, to solve this \nissue, \nin \nthis \nstudy, \nwe \npropose \na \nspeech \nenhancement/dereverberation framework whose training \ndata are based only on noisy/reverberated speech. \nIn the field of speech enhancement, unsupervised \ntraining is usually defined as pairs of noisy and clean speech \nsignals that are not required during model training [9]. This \ncan be further divided into different levels of “unsupervised,” \naccording to what training data are actually needed, as \nfollows: \n \n1) Level 1 of unsupervised learning (clean speech or noise \nis needed):  \nAlthough noisy and clean speech pairs are not required, \nclean speech or noise may still be needed. Most \nunsupervised speech enhancement methods belong to this \ncategory. Bie et al. [9] used clean speech to first pre-train a \nvariational \nauto-encoder \nand \napplied \nvariational \nexpectation-maximization to fine-tune the encoder part \nduring inference. Another method to achieve non-parallel \nspeech enhancement is through a cycle-consistent generative \nadversarial network (CycleGAN) [10, 11]. Through the \nframework of a GAN and cycle-consistent loss, only non-\npaired clean and noisy speech was used during training. \nFujimura et al. [12] proposed noisy-target training (NyTT) \nby adding noise to noisy speech. The noise-added signal and \noriginal noisy speech are considered as the model input and \ntarget, respectively. In summary, although the above-\nmentioned methods do not require parallel data, clean \nspeech or noise may still be needed. \n2) Level 2 of unsupervised learning (noisy speech is \nneeded):  \n    In this study, we train a speech enhancement/ \ndereverberation model based on noisy/reverberated speech \nonly. Because the proposed method follows the MetricGAN \n[13] framework, we call it MetricGAN-U, which is short for \nMetricGAN-unsupervised. The basic idea is to optimize a \nnon-intrusive speech quality metric so that no clean speech \nis required during training. We selected DNSMOS [14] and \nspeech-to-reverberation modulation energy ratio (SRMR) \n[15] as the quality metrics for speech enhancement and \ndereverberation, respectively. To foster reproducibility, \nMetricGAN-U is available within the SpeechBrain1 [16]. \n3) Level 3 of unsupervised learning (no training data is \nneeded):  \n    At this level of unsupervised learning, no training data is \nrequired (because it does not even learn anything!). Most of \nthese methods are based on traditional signal processing \nmethods, such as the MMSE [17] and Wiener filter [18]. \nNote that the extra effort needed to go from level 3 to \nlevel 2 is not as large as level 2 to level 1, because noisy \nspeech is very easy to obtain in our daily life. \n \n                                                 \n1 Code is available at https://speechbrain.github.io/ \n*Currently affiliated with Microsoft. \n2. INTRODUCTION TO METRICGAN \n \nThe training framework of MetricGAN [13] is very similar \nto a conventional generative adversarial network (GAN) \n[19], except that the goal of the discriminator is to mimic the \nbehavior of the target evaluation function (e.g., perceptual \nevaluation of speech quality (PESQ) function [20]). The \ndiscriminator (surrogate of the metric of interest) is learned \nfrom raw metric scores by treating the target evaluation \nfunction as a black box. Hence, the surrogate can be used as \na loss function for the generator (speech enhancement model) \nto optimize the target metric. We recently proposed an \nimproved version of MetricGAN called MetricGAN+ [21], \nwhich includes some more advanced training techniques. \nWe briefly introduce the training algorithm here. \nLet 𝑄′(𝐼)  be a function that represents the target \nevaluation metric normalized between 0 and 1, where I \ndenotes the input of the metric. For example, for intrusive \nmetrics such as PESQ, I denotes a pair of enhanced speech, \n𝐺(𝑥) (or noisy speech, 𝑥) that we want to evaluate, and its \ncorresponding clean speech, y. To ensure that the \ndiscriminator network (D) behaves similar to 𝑄′ , the \nobjective function of D is \n𝐿D(MetricGAN+) = 𝔼𝑥,𝑦[(𝐷(𝑦, 𝑦) − 𝑄′(𝑦, 𝑦))2 + \n                    (𝐷(𝐺(𝑥), 𝑦) − 𝑄′(𝐺(𝑥), 𝑦))\n2 + \n(𝐷(𝑥, 𝑦) − 𝑄′(𝑥, 𝑦))2]         (1) \nThe three terms were used to minimize the difference \nbetween 𝐷(.) and 𝑄′ (.) for clean, enhanced, and noisy \nspeech, respectively. Note that, 𝑄′(𝑦, 𝑦) = 1 , 0 ≤ \n𝑄′(𝐺(𝑥), 𝑦) ≤ 1 and 0 ≤ 𝑄′(𝑥, 𝑦) ≤ 1. \nThe training of the generator network (G) can \ncompletely rely on the adversarial loss \n𝐿G(MetricGAN+) = 𝔼𝑥[(𝐷(𝐺(𝑥), 𝑦) − 𝑠)2]           (2) \nwhere s denotes the desired assigned score. For example, to \ngenerate clean speech, we can simply assign 1 to s.  \n \n3. UNSUPERVISED SPEECH ENHANCEMENT \n/DEREVERBERATION USING METRICGAN \nFRAMEWORK \n \n3.1. MetricGAN-U \nTo \nachieve \nlevel \n2 \nof \nunsupervised \nspeech \nenhancement/dereverberation, the clean speech, y, used in (1) \nand (2) have to be removed. Therefore, the training of the \ndiscriminator network is modified as: \n𝐿D(MetricGAN−U) = 𝔼𝑥[(𝐷(G(𝑥)) − 𝑄′(G(𝑥)))2 + \n(𝐷(𝑥) − 𝑄′(𝑥))2]         (3) \nFrom (3), it is obvious that the target evaluation metric 𝑄′(.) \nmust be nonintrusive (no clean reference is needed). Certain \nwell-known non-intrusive speech quality metrics include \nITU-T P.563 [22] (designed for 3.1 kHz narrow-band \ntelephony applications), SRMR [15], and DNSMOS [14]. In \nthis study, we apply DNSMOS as 𝑄′ (.) for speech \nenhancement and SRMR for speech dereverberation. The \noverall training flow for MetricGAN-U is shown in Fig. 1. \nQuality-Net \n(D)\nPredicted \nmetric score\nNoisy or enhanced \nspeech\nTrue \nmetric score\nMSE loss\nQuality-Net \n(D)\nEnhancement \nmodel \nPredicted \nmetric score\nFixed weights\n Maximum \nscore =1\nMSE loss\nBack-propagation\nMask\nBack-propagation\nNoisy speech\nTraining D\nAlternately \nupdated\nAlternately \nupdated\nTraining G\n(G)\nEvaluation \nmetric\n(    )\nMSE loss\nBlack-box\n Fig. 1. Training flow of MetricGAN-U. \n \n3.2. Introduction to DNSMOS and SRMR \nDNSMOS is a neural network based quality estimation \nmetric that can be used to evaluate different deep noise \nsuppression (DNS) methods based on mean opinion score \n(MOS) estimates [14]. It is trained using ground truth \nhuman ratings obtained using ITU-T P.808 [23]. Although \ntheoretically we can just directly concatenate the DNSMOS \nmodel after a speech enhancement model and use \nbackpropagation to increase its score, the DNSMOS model \nis not publicly released, and hence, we can only obtain the \nscores through an API provided by the authors. Therefore, \nthis also falls within the application scope of MetricGAN as \na black-box metric optimization. \nIn contrast to DNSMOS, SRMR is a handcrafted \nmetric design by speech experts. The basic idea of SRMR is \nbased on the observation that reverberation generates high-\nfrequency modulation energy. Hence, its definition is simply \nthe ratio between the low-frequency modulation energy \n(speech component) and high-frequency one (reverberation \ncomponent). For more details, please refer to [15]. \n \n4. EXPERIMENTS \n \n4.1. Model Structure \nThe generator used in this experiment is a BLSTM [24] with \ntwo bidirectional LSTM layers, with 200 neurons each. The \nLSTM is followed by two fully connected layers, each with \n300 LeakyReLU nodes and 257 sigmoid nodes for mask \nestimation, respectively. The discriminator herein is a CNN \nwith four two-dimensional (2-D) convolutional layers with \n15 filters and a kernel size of (5, 5). To handle the variable \nlength input, a 2-D global average pooling layer was added \nsuch that the features could be fixed at 15 dimensions. Three \nfully connected layers were subsequently added, each with \n50 and 10 LeakyReLU neurons, and 1 linear node. \nTable 1. Comparison of MetricGAN-U with other methods for speech enhancement on the VoiceBank-DEMAND test set.\n \n4.1. Speech enhancement \n4.1.1. Dataset \nTo compare the proposed MetricGAN-U with other existing \nmethods, we used the publicly available VoiceBank-\nDEMAND dataset [26]. The original training sets (11572 \nutterances) consisted of 28 speakers with four signal-to-\nnoise ratios (SNRs) (15, 10, 5, and 0 dB). We randomly \nselected two speakers (p226 and p287) from this set to form \na validation set (770 utterances). The test sets (824 \nutterances) consisted of two speakers with four SNRs (17.5, \n12.5, 7.5, and 2.5 dB). Details of the data can be found in \nthe original paper. We mainly evaluated the performance \nwith the PESQ and DNSMOS scores. Although the other \nthree metrics, CSIG, CBAK, and COVL, predict the MOS \nof the signal distortion, background noise interference, and \noverall speech quality, respectively, they are all based on the \nPESQ [18] and may not be highly correlated to MOS [27] \n(e.g., CBAK is very sensitive to the loudness of speech). \n \n4.1.2. Experimental results \nTable 1 shows the results of the proposed MetricGAN-U \nwith other supervised and unsupervised baselines. Note that, \nalthough the criterion to distinguish between supervised and \nunsupervised training here is based on whether clean and \nnoisy training pairs are needed, the degree of unsupervised \ntraining \ncan \nbe \nfurther \ninvestigated. \nThe \n“most” \nunsupervised methods are MMSE [17] and Wiener filter [18] \nwhere no training data is required. MetricGAN-U only \nrequires noisy speech for training, and it can be real \nrecordings (does not have to be synthetic). Finally, although \nnoisy-target training (NyTT) [12] also does not require clean \nspeech, noise is required to generate more noisy signals as \nmodel input (the original noisy speech is used as the target). \nThe difference between MetricGAN-U (half) and \nMetricGAN-U (full) was the number of training epochs. The \nformer is based on the early stopping [28] strategy with the \ncriterion that the average PESQ score on the validation set \nreaches a maximum, while the latter is trained with full 600 \nepochs. Fig. 2 shows the learning process of the DNSMOS \nscores on the validation set. \nIn Table 1, although the Wiener filter can improve the \nPESQ score, its DNSMOS score is almost the same as that \nof noisy speech. Similarly, our MetricGAN-U (half) can \nobtain the highest PESQ score among unsupervised methods,  \n \n \nFig. 2. Learning curve of MetricGAN-U on the validation \nset of VoiceBank-DEMAND dataset. \n \nbut its DNSMOS score is the lowest (it seems that the \ncorrelation between the two metrics does not always remain \npositive). On the other hand, MetricGAN-U (full) seems to \nachieve a good trade-off between all metrics. In summary, it \ncan increase 0.16 and 0.103 for PESQ and DNSMOS, \nrespectively (Note that, considering the DNSMOS score for \nthe clean test set is only 3.567, an improvement of 0.103 is \nsignificant). \nTo evaluate the perceptual quality of the enhanced speech, \nwe conducted subjective listening tests to compare the \nproposed MetricGAN-U (full) with Wiener filter and noisy \nspeech (because it has been shown DNSMOS have higher \ncorrelation with MOS than PESQ [14], we selected \nMetricGAN-U (full) instead of MetricGAN-U (half)). 40 \nsamples were randomly selected from the test set; therefore, \nthere were a total of 40 × 3 (enhancement methods) = 120 \nutterances that each listener had to take. For each signal, the \nlistener rated the overall quality based on signal distortion \nand noise intrusiveness, using a scale from 1 to 5. (e.g., 5 \nrepresents excellent quality, and 1 represents very poor \nquality). 15 listeners participated in the study. In Table 2, it \ncan be observed that MetricGAN-U (full) is preferred over \nboth the noisy and Wiener baseline (with p-value <0.001 \nand <0.05, respectively). An example of a spectrogram \npresentation is shown in Fig. 3. It can be found that \nMetricGAN-U (full) can remove more noise compared with \nthe Wiener filter enhanced speech. \n0\n100\n200\n300\n400\n500\n600\n2.7\n2.75\n2.8\n2.85\n2.9\n2.95\n3\n3.05\n3.1\nNumber of iterations (x100)\nDNSMOS\nValidation set of VoiceBank-DEMAND dataset\nDNSMOS score \nof noisy speech\n \nUnsupervised? \nNeed clean (c), noise (n), \nor noisy (N) for training? \nPESQ \nCSIG \nCBAK \nCOVL \nDNSMOS \nNoisy \n- \n- \n1.97 \n3.35 \n2.44 \n2.63 \n3.048 \nSEGAN [25] \n \nN(n+c) & c \n2.16 \n3.48 \n2.94 \n2.80 \n- \nBLSTM (MSE) \n \nN(n+c) & c \n2.71 \n3.94 \n3.28 \n3.32 \n3.367 \nMMSE [17] \n (Level 3) \nNo \n2.19 \n3.16 \n2.55 \n2.61 \n2.978 \nWiener [18] \n (Level 3) \nNo \n2.22 \n3.23 \n2.68 \n2.67 \n3.050 \nNyTT [12] \n (Level 1) \nN & n \n2.30 \n3.19 \n3.01 \n2.72 \n- \nMetricGAN-U (half) \n (Level 2) \nN \n2.45 \n3.47 \n2.63 \n2.91 \n2.891 \nMetricGAN-U (full) \n (Level 2) \nN \n2.13 \n3.22 \n2.42 \n2.63 \n3.151 \nTable 2. Subjective evaluation results for speech \nenhancement. \n \nNoisy \nWiener \nMetricGAN-U (full) \nMOS \n3.083 \n3.387 \n3.556 \n \n   \n \n(a) Clean speech  \n(b) Noisy speech \n  \n \n(c) Wiener enhanced                (d) MetricGAN-U (full) \nFig. 3. Spectrograms of an example (2.5 dB) in the \nVoiceBank-DEMAND test set. \n \n4.2. Speech dereverberation \n4.2.1. Dataset \nBecause there is no publicly available free dataset for \nspeech dereverberation, we prepared a dataset based on the \nclean speech from VoiceBank-DEMAND [26] (discard the \nnoisy speech) and convolved them with the room impulse \nresponse (RIR) from OpenSLR2. We used the list3 provided \nin the downloaded files to select 325 real RIR data for our \ndataset generation. Among them, 315 RIR were used to \ngenerate the training set, and another 10 RIR were randomly \nchosen for the test set. We applied the AddReverb function \nprovided in the SpeechBrain [16] toolkit to generate the \nreverberated speech. The rir_scale_factor is randomly \nselected from {1.2, 1.1, 1.0, 0.9, 0.8} for the training set and \n{1.15, 1.05, 0.95, 0.85, 0.75} for the testing set. If 0 < \nrir_scale_factor < 1, the impulse response is compressed \n(less reverb), whereas if rir_scale_factor > 1, it is dilated \n(more reverb). Each clean utterance is convolved with only \none RIR and rir_scale_factor. As in the case of VoiceBank-\nDEMAND, two speakers (p226 and p287) were randomly \nchosen from the training set to form a validation set (770 \nutterances). The prepared dataset is called VoiceBank-SLR \nand can be publicly available4 for a fair comparison. \n \n4.2.2. Experimental results \nIn the experiment for speech dereverberation, MetricGAN-\nU was applied to maximize the SRMR score. Preliminary \nexperiments show that the SRMR score can be easily \nincreased to a very high level (i.e., larger than 10) by \nincreasing the low-frequency modulation energy (speech \ncomponent) and decreasing the high-frequency component \n                                                 \n2 http://www.openslr.org/resources/28/rirs_noises.zip \n3 In the path: RIRS_NOISES/real_rirs_isotropic_noises / rir_list \n4 https://bio-asplab.citi.sinica.edu.tw/Opensource.html#VB-SLR \nTable 3. Comparison of MetricGAN-U with other methods \nfor speech dereverberation on the VoiceBank-SLR test set. \n \nUnsupervised? \nPESQ \nSRMR \nReverb \n- \n1.98 \n6.039 \nBLSTM (MSE) \n \n2.35 \n8.039 \nWPE [29] \n \n2.01 \n6.259 \nMetricGAN-U \n \n2.07 \n8.265 \n \nTable 4. Subjective evaluation results for speech \ndereverberation. \n \nReverb \nWPE \nMetricGAN-U \nMOS \n2.920 \n2.957 \n3.140 \n \n(reverberation component). Because we only want to \nremove the reverberation component and keep the speech \ncomponent unchanged, a self-reconstruction constraint term \nis added to the loss function of the generator: \n𝐿G(MetricGAN−U(SRMR)) = \n𝔼𝑥[(𝐷(𝐺(𝑥)) − 𝑠)\n2] + 𝛼|𝐺(𝑥) −𝑥|2\n2           (4) \nwhere 𝛼 is set to 0.6, decided on the performance of the \nvalidation \nset. \nTable \n3 \nshows \nthat \nMetricGAN-U \noutperforms the well-known weighted prediction error \n(WPE) [29]. \n    We also conducted subjective listening tests on the \ndereverberation task to compare MetricGAN-U with WPE \nand reverb speech. 20 samples were randomly selected from \nthe test set; therefore, there were a total of 20 × 3 (dereverb \nmethods) = 60 utterances that each participant had to listen. \nFor each signal, the listener rated the overall quality of \ndereverberation using a scale from 1 to 5. 15 listeners \nparticipated in the study. In Table 4, it can be observed that \nMetricGAN-U is preferred over both the reverb speech and \nWPE (with p-value <0.001 and <0.01, respectively). \n \n5. DISCUSSION \nAlthough it seems that there is a gap between the \nperformance of supervised methods and the proposed \nunsupervised method, the difference can be easily reduced \nby increasing the amount of training data, which is very \neasy to obtain for level 2 of unsupervised learning. In \naddition, our method can also be used to boost the \nperformance of supervised models by 1) applying the \nweights of the unsupervised model as the weight \ninitialization for the supervised model or 2) as in [30], train \nthe generator to maximize the discriminator score using real \nnoisy data together with the synthetic one. \n \n6. CONCLUSION \nCompared to conventional unsupervised training, this paper \nproposed MetricGAN-U, which only requires noisy speech \nas training data. Because noisy speech is much easier to \nobtain than clean speech and noise, this significantly \nreduces the effort required for training data collection. By \napplying the proposed framework, a large amount of real \nnoisy data can also be utilized during model training. \n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n7. REFERENCES \n \n[1] \nD. Wang and J. Chen, \"Supervised speech separation \nbased on deep learning: An overview,\" IEEE/ACM \nTransactions \non \nAudio, \nSpeech, \nand \nLanguage \nProcessing, 2018. \n[2] \nY. Xu, J. Du, L.-R. Dai, and C.-H. Lee, \"An \nexperimental study on speech enhancement based on \ndeep neural networks,\" IEEE Signal Processing Letters, \nvol. 21, pp. 65-68, 2014. \n[3] \nH. Erdogan, J. R. Hershey, S. Watanabe, and J. Le Roux, \n\"Phase-sensitive \nand \nrecognition-boosted \nspeech \nseparation using deep recurrent neural networks,\" in \nProc. ICASSP, 2015, pp. 708-712. \n[4] \nK. Tan, J. Chen, and D. Wang, \"Gated residual networks \nwith \ndilated \nconvolutions \nfor \nmonaural \nspeech \nenhancement,\" IEEE/ACM Transactions on Audio, \nSpeech, and Language Processing, 2018. \n[5] \nT. Gao, J. Du, L.-R. Dai, and C.-H. Lee, \"Densely \nconnected progressive learning for lstm-based speech \nenhancement,\" in Proc. ICASSP, 2018, pp. 5054-5058. \n[6] \nF.-A. Chao, S.-W. F. Jiang, B.-C. Yan, J.-w. Hung, and \nB. Chen, \"TENET: A time-reversal enhancement \nnetwork \nfor \nnoise-robust \nASR,\" \narXiv \npreprint \narXiv:2107.01531, 2021. \n[7] \nH. Li and J. Yamagishi, \"Noise tokens: Learning neural \nnoise \ntemplates \nfor \nenvironment-aware \nspeech \nenhancement,\" arXiv preprint arXiv:2004.04001, 2020. \n[8] \nH. Li, S.-W. Fu, Y. Tsao, and J. Yamagishi, \n\"iMetricGAN: Intelligibility enhancement for speech-in-\nnoise using generative adversarial network-based metric \nlearning,\" in Proc. Interspeech, 2020. \n[9] \nX. Bie, S. Leglaive, X. Alameda-Pineda, and L. Girin, \n\"Unsupervised speech enhancement using dynamical \nvariational \nauto-encoders,\" \narXiv \npreprint \narXiv:2106.12271, 2021. \n[10] \nY. Xiang and C. Bao, \"A parallel-data-free speech \nenhancement method using multi-objective learning \ncycle-consistent \ngenerative \nadversarial \nnetwork,\" \nIEEE/ACM Transactions on Audio, Speech, and \nLanguage Processing, vol. 28, pp. 1826-1838, 2020. \n[11] \nG. Yu, Y. Wang, C. Zheng, H. Wang, and Q. Zhang, \n\"CycleGAN-based non-parallel speech enhancement \nwith an adaptive attention-in-attention mechanism,\" \narXiv preprint arXiv:2107.13143, 2021. \n[12] \nT. Fujimura, Y. Koizumi, K. Yatabe, and R. Miyazaki, \n\"Noisy-target training: a training strategy for dnn-based \nspeech enhancement without clean speech,\" arXiv \npreprint arXiv:2101.08625, 2021. \n[13] \nS.-W. Fu, C.-F. Liao, Y. Tsao, and S.-D. Lin, \"Metricgan: \nGenerative adversarial networks based black-box metric \nscores optimization for speech enhancement,\" in Proc. \nICML, 2019. \n[14] \nC. K. Reddy, V. Gopal, and R. Cutler, \"DNSMOS: A \nnon-intrusive perceptual objective speech quality metric \nto \nevaluate \nnoise \nsuppressors,\" \narXiv \npreprint \narXiv:2010.15258, 2020. \n[15] \nT. H. Falk, C. Zheng, and W.-Y. Chan, \"A non-intrusive \nquality and intelligibility measure of reverberant and \ndereverberated speech,\" IEEE Transactions on Audio, \nSpeech, and Language Processing, vol. 18, pp. 1766-\n1774, 2010. \n[16] \nM. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. \nCornell, L. Lugosch, et al., \"SpeechBrain: A general-\npurpose \nspeech \ntoolkit,\" \narXiv \npreprint \narXiv:2106.04624, 2021. \n[17] \nY. Ephraim and D. Malah, \"Speech enhancement using a \nminimum-mean \nsquare \nerror \nshort-time \nspectral \namplitude estimator,\" IEEE Transactions on Acoustics, \nSpeech and Signal Processing, vol. 32, pp. 1109-1121, \n1984. \n[18] \nP. C. Loizou, Speech enhancement: theory and practice: \nCRC press, 2013. \n[19] \nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. \nWarde-Farley, S. Ozair, et al., \"Generative adversarial \nnets,\" in Advances in neural information processing \nsystems, 2014, pp. 2672-2680. \n[20] \nA. Rix, J. Beerends, M. Hollier, and A. Hekstra, \n\"Perceptual evaluation of speech quality (PESQ), an \nobjective \nmethod \nfor \nend-to-end \nspeech \nquality \nassessment of narrowband telephone networks and \nspeech codecs,\" ITU-T Recommendation, p. 862, 2001. \n[21] \nS.-W. Fu, C. Yu, T.-A. Hsieh, P. Plantinga, M. Ravanelli, \nX. Lu, et al., \"MetricGAN+: An improved version of \nMetricGAN \nfor \nspeech \nenhancement,\" \nin \nProc. \nInterspeech, 2021. \n[22] \nL. Malfait, J. Berger, and M. Kastner, \"P. 563—The \nITU-T \nstandard \nfor \nsingle-ended \nspeech quality \nassessment,\" IEEE Transactions on Audio, Speech, and \nLanguage Processing, vol. 14, pp. 1924-1934, 2006. \n[23] \nB. \nNaderi \nand \nR. \nCutler, \n\"An \nopen \nsource \nimplementation of itu-t recommendation p. 808 with \nvalidation,\" arXiv preprint arXiv:2005.08138, 2020. \n[24] \nF. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Le \nRoux, J. R. Hershey, et al., \"Speech enhancement with \nLSTM recurrent neural networks and its application to \nnoise-robust ASR,\" in Proc. LVA/ICA, 2015, pp. 91-99. \n[25] \nS. Pascual, A. Bonafonte, and J. Serra, \"SEGAN: Speech \nenhancement generative adversarial network,\" in Proc. \nInterspeech, 2017. \n[26] \nC. Valentini-Botinhao, X. Wang, S. Takaki, and J. \nYamagishi, \n\"Investigating \nRNN-based \nspeech \nenhancement methods for noise-robust Text-to-Speech,\" \nin SSW, 2016, pp. 146-152. \n[27] \nC. K. Reddy, V. Gopal, and R. Cutler, \"DNSMOS P.835: \nA non-intrusive perceptual objective speech quality \nmetric to evaluate noise suppressors,\" arXiv preprint \narXiv: 2110.01763, 2021. \n[28] \nL. Prechelt, \"Early stopping-but when?,\" in Neural \nNetworks: Tricks of the trade, ed: Springer, 1998, pp. 55-\n69. \n[29] \nT. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and \nB.-H. Juang, \"Speech dereverberation based on variance-\nnormalized \ndelayed \nlinear \nprediction,\" \nIEEE \nTransactions \non \nAudio, \nSpeech, \nand \nLanguage \nProcessing, vol. 18, pp. 1717-1731, 2010. \n[30] \nZ. Xu, M. Strake, and T. Fingscheidt, \"Deep noise \nsuppression with non-intrusive pesqnet supervision \nenabling the use of real training data,\" arXiv preprint \narXiv:2103.17088, 2021. \n \n \n",
  "categories": [
    "cs.SD",
    "cs.CL",
    "eess.AS"
  ],
  "published": "2021-10-12",
  "updated": "2021-10-12"
}