{
  "id": "http://arxiv.org/abs/2110.06632v2",
  "title": "Unsupervised Contrastive Learning with Simple Transformation for 3D Point Cloud Data",
  "authors": [
    "Jincen Jiang",
    "Xuequan Lu",
    "Wanli Ouyang",
    "Meili Wang"
  ],
  "abstract": "Though a number of point cloud learning methods have been proposed to handle\nunordered points, most of them are supervised and require labels for training.\nBy contrast, unsupervised learning of point cloud data has received much less\nattention to date. In this paper, we propose a simple yet effective approach\nfor unsupervised point cloud learning. In particular, we identify a very useful\ntransformation which generates a good contrastive version of an original point\ncloud. They make up a pair. After going through a shared encoder and a shared\nhead network, the consistency between the output representations are maximized\nwith introducing two variants of contrastive losses to respectively facilitate\ndownstream classification and segmentation. To demonstrate the efficacy of our\nmethod, we conduct experiments on three downstream tasks which are 3D object\nclassification (on ModelNet40 and ModelNet10), shape part segmentation (on\nShapeNet Part dataset) as well as scene segmentation (on S3DIS). Comprehensive\nresults show that our unsupervised contrastive representation learning enables\nimpressive outcomes in object classification and semantic segmentation. It\ngenerally outperforms current unsupervised methods, and even achieves\ncomparable performance to supervised methods. Our source codes will be made\npublicly available.",
  "text": "The Visual Comptuer 2022\nUnsupervised Contrastive Learning with Simple\nTransformation for 3D Point Cloud Data\nJincen Jiang1, Xuequan Lu2, Wanli Ouyang3 and Meili Wang1*\n1College of Information Engineering, Northwest A&F University, China.\n2School of Information Technology, Deakin University, Australia.\n3School of Electrical and Information Engineering, The University of Sydney, Australia.\n*Corresponding author(s). E-mail(s): wml@nwsuaf.edu.cn;\nContributing authors: jinec@nwsuaf.edu.cn; xuequan.lu@deakin.edu.au;\nwanli.ouyang@sydney.edu.au;\nAbstract\nThough a number of point cloud learning methods have been proposed to handle unordered points, most of them\nare supervised and require labels for training. By contrast, unsupervised learning of point cloud data has received\nmuch less attention to date. In this paper, we propose a simple yet effective approach for unsupervised point cloud\nlearning. In particular, we identify a very useful transformation which generates a good contrastive version of an\noriginal point cloud. They make up a pair. After going through a shared encoder and a shared head network, the\nconsistency between the output representations are maximized with introducing two variants of contrastive losses\nto respectively facilitate downstream classiﬁcation and segmentation. To demonstrate the efﬁcacy of our method,\nwe conduct experiments on three downstream tasks which are 3D object classiﬁcation (on ModelNet40 and Mod-\nelNet10), shape part segmentation (on ShapeNet Part dataset) as well as scene segmentation (on S3DIS). Com-\nprehensive results show that our unsupervised contrastive representation learning enables impressive outcomes\nin object classiﬁcation and semantic segmentation. It generally outperforms current unsupervised methods, and\neven achieves comparable performance to supervised methods. Our source codes will be made publicly available.\nKeywords: unsupervised contrastive learning; point cloud; 3D object classiﬁcation; semantic segmentation\n1 Introduction\nPoint cloud, as an effective representation for 3D geo-\nmetric data, has attracted noticeable attention recently.\nIt has been used for learning based segmentation, clas-\nsiﬁcation, object detection, etc. Promising results have\nbeen achieved among those application ﬁelds. In this\nwork, we focus on the use of 3D point clouds for clas-\nsiﬁcation and segmentation tasks. They respectively\ntarget to automatically recognize 3D objects and pre-\ndict segment labels, which are crucial in multimedia\ncomputing, robotics, etc.\nMost of existing methods for 3D point cloud anal-\nysis [1–9] use annotated data for training. Neverthe-\nless, annotation is time-consuming and costly, espe-\ncially for a considerable amount of data. In the real\nworld, it is particularly challenging to have annotated\ndata for training all the time. Unsupervised learning is\na good alternative [10–13]. For example, Latent-GAN\n[10] used a deep architecture of Autoencoder (AE),\nand trained a minimal GAN in the AE’s latent space\nfor learning representations of point clouds. Fold-\ningNet [11] proposed a new AE to get the codeword\n1\narXiv:2110.06632v2  [cs.CV]  24 Jan 2023\nThe Visual Comptuer 2022\nwhich can represent the high dimensional embed-\nding point cloud, and the fully-connected decoder\nwas replaced with the folding-based decoder. MAP-\nVAE [12] conducted half-to-half predictions (splitting\npoint cloud into a front half and a back half with\nseveral angles), and then combined them with global\nself-supervision to capture the geometry and struc-\nture of the point cloud. 3D-PointCapsNet [13] used\nthe encoder-decoder structure, and concatenated the\nfeatures from the encoder to form the point capsules.\nThese methods usually employ the AE as the back-\nbone, and often suffer from the curse of less quality\nrepresentations. As a result, they may still induce\nless desired performance on downstream tasks (e.g.\nclassiﬁcation and segmentation).\nMotivated by the above analysis, we propose an\nunsupervised representation learning method which\ncan facilitate the downstream 3D object classiﬁca-\ntion and semantic segmentation. Our core idea is to\nmaximize the agreement or consistency between the\nrepresentations of the original point cloud and its\ntransformed version (i.e. contrastive version).\nWe simply utilize one transformation to generate\nthe transformed version point cloud and pair it with\nthe original ones. And then feed them into a shared\nbase encoder network (e.g. former part of PointNet [7]\nwith global feature), followed by a subsequent projec-\ntion head network (e.g. latter part of PointNet: several\nmlp layers). The agreement maximization is imposed\non the outputs of the projection head network, to facil-\nitate the training efﬁciency and better preserve the rich\nrepresentations output from the encoder. Since there\nare no labels involved in training, it is unsupervised\nrepresentation learning for 3D point cloud data.\nTo validate our unsupervised method, we conduct\nexperiments for the object classiﬁcation task on Mod-\nelNet40 and ModelNet10, the shape part segmentation\ntask on ShapeNet Part dataset, and the scene segmen-\ntation task on the S3DIS dataset. Extensive results\nshow that our unsupervised contrastive representa-\ntive learning enables impressive outcomes in terms\nof the three tasks. Our method generally outperforms\nstate-of-the-art unsupervised techniques, and is even\ncomparable to certain supervised counterparts.\nThe contributions of this paper are:\n• an unsupervised representation learning approach\nwhich is simple yet effective on 3D point cloud\ndata,\n• a simple transformation in generating a good con-\ntrastive version of an original point cloud, which is\nbetter than other complex transformations,\n• two variants of point cloud based contrastive losses\nfor downstream classiﬁcation and segmentation,\nrespectively,\n• experiments and analysis on three tasks (classiﬁca-\ntion, shape part segmentation and scene segmen-\ntation), achieving promising performance with our\nsimple contrastive learning.\n2 Related Work\nUnlike 2D images, which consist of regular and uni-\nform pixels, point cloud data are often irregular,\nsparse and contaminated with noise/outliers during\nthe obtaining procedure of scanning and processing\n[14–17]. 3D point cloud learning techniques can be\ngenerally classiﬁed into three categories: (1) voxel\nbased [1–3], (2) view based [4–6, 18, 19] and (3)\npoint based [7–9, 20–27]. Voxel based methods often\ninvolve resolution and memory issues, and view based\napproaches are often criticized for the tedious pre-\nprocessing, i.e. projecting each 3D object onto 2D\nimage planes. Point based techniques are capable of\nlearning features from point cloud data straightfor-\nwardly. In fact, most of these methods are supervised.\nVoxel based techniques. 3D volumetric CNNs\n(Convolutional Neural Network) imitates classical 2D\nCNNs by performing voxelization on the input point\ncloud. 3D ShapeNets was designed for learning volu-\nmetric shapes [1]. Riegler et al. proposed OctNet for\ndeep learning with sparse 3D data [2]. Wang et al. pre-\nsented an Octree-based CNN for 3D shape analysis,\nwhich was called O-CNN [3]. These methods are pro-\nposed to improve 3D volumetric CNNs and reach high\nvolume resolutions.\nView based methods. View based methods are to\nproject 3D point cloud data onto the regular image\nplanes. For example, MVCNNs used multiple images\nrendered from the 3D shapes to ﬁt classical 2D CNNs\n[4]. Su et al. proposed to utilize a sparse set of sam-\nples in a high-dimensional lattice as the representation\nof a collection of points [18]. Zhou et al. proposed\nthe multi-view saliency guided deep neural network\n(MVSG-DNN) which contains three modules to cap-\nture and extract the features of individual views to\ncompile 3D object descriptors for 3D object retrieval\nand classiﬁcation [19]. Xu et al. used a LSTM-based\nnetwork to recurrently aggregate the 3D objects shape\nThe Visual Comptuer 2022\n3\nembedding from an image sequence and estimate\nimages of unseen viewpoints, aiming at the fusion of\nmultiple views’ features [28]. Huang et al. devised a\nview mixture model (VMM) to decompose the mul-\ntiple views into a few latent views for the descriptor\nconstruction [29]. Li et al. presented an end-to-end\nframework to learn local multi-view descriptors for\n3D point clouds [5]. Lyu et al. projected 3D point\nclouds into 2D image space by learning the topology-\npreserving graph-to-grid mapping [6].\nPoint based methods. PointNet is a seminal work\non point based learning [7]. In PointNet, max-pooling\noperation is used to learn permutation-invariant fea-\ntures. The original authors introduced PointNet++,\na hierarchical neural network that applied PointNet\nrecursively on a nested partitioning of the input point\nset [8]. It achieved better learning outcomes than\nPointNet. Later, pointCNN was introduced to learn an\nX-transformation from the input points, to promote\nthe weighting of the input features and the permuta-\ntion of the points into a latent order [9]. PointConv,\na density re-weighted convolution, was proposed to\nfully approximate the 3D continuous convolution on\nany set of 3D points [20]. Xu et al. proposed Spider-\nCNN to extract geometric features from point clouds\n[21]. Liu et al. designed a Relation-Shape Convolu-\ntional Neural Network to learn the geometric topology\nconstraints among points [22]. Simonovsky et al. gen-\neralized the convolution operator from regular grids\nto arbitrary graphs and applied it to point cloud clas-\nsiﬁcation [30]. Parametric Continuous Convolution\nwas introduced to exploit parameterized kernel func-\ntions that spanned the full continuous vector space\n[31]. Li et al. came up with a self-organizing net-\nwork which applied hierarchical feature aggregation\nusing self-organizing map [32]. It included a point\ncloud auto-encoder as pre-training to improve network\nperformance. Komarichev et al. presented an annu-\nlar convolution operator to better capture the local\nneighborhood geometry of each point by specify-\ning the (regular and dilated) ring-shaped structures\nand directions in the computation [23]. Zhao et al.\nput forwarded PointWeb to enhance local neighbor-\nhood features for point cloud processing [33]. Xie et\nal. developed a new representation by adopting the\nconcept of shape context as the building block and\ndesigned a model (ShapeContextNet) for point cloud\nrecognition [34]. Wang et al. designed a new neu-\nral network module dubbed EdgeConv which acts\non graphs dynamically computed in each layer [24].\nMore recently, Fujiwara et al. proposed to embed\nthe distance ﬁeld to neural networks [35]. Lin et al.\ndeﬁned learnable kernels with a graph max-pooling\nmechanism for their 3D Graph Convolution Networks\n(3D-GCN) [25]. Yan et al. presented the adaptive sam-\npling and the local-nonlocal modules for robust point\ncloud processing [36]. Qiu et al. proposed a network\nconsidering both low-level geometric information of\n3D space points explicitly and high-level local geo-\nmetric context of feature space implicitly [37]. Chen\net al. presented a hierarchical attentive pooling graph\nnetwork (HAPGN) for segmentation which includes\nthe gated graph attention network to get a better rep-\nresentation of local features and hierarchical graph\npooling module to learn hierarchical features [38].\nLiu et al. devised a point context encoding module\n(PointCE) and a semantic context encoding loss (SCE-\nloss) to capture the rich semantic context of a point\ncloud adaptively, achieving improved segmentation\nperformance [39].\nUnsupervised representation learning. Yang et\nal. proposed an autoencoder (AE), referred to as Fold-\ningNet, for unsupervised learning on point cloud data\n[11]. MAP-VAE was proposed to enable the learn-\ning of global and local geometry by jointly leverag-\ning global and local self-supervision [12]. Rao et al.\npresented bidirectional reasoning between the local\nstructures and the global shape for unsupervised rep-\nresentation learning of point clouds [40]. It used a\nmuch larger RSCNN as backbone (4×RSCNN) [22].\nZhang et al. presented an explainable machine learn-\ning method for point cloud classiﬁcation by build-\ning local-to-global features through iterative one-hop\ninformation exchange, and feeding the feature vec-\ntor to a random forest classiﬁer for classiﬁcation [41].\nDifferent from them, we create a contrastive pair\nfor each point cloud, and our framework simply con-\nsists of an encoder network and a head network. The\nencoder outputs global representations (features) for\ndownstream networks and the head outputs projection\nfeatures (a smaller size) for calculating the loss.\nMore recently, Xie et al. presented an unsuper-\nvised pre-training framework called PointContrast\nfor high-level scene understanding tasks [42]. Their\nﬁndings demonstrated that the learned representation\ncould generalize across domains. [42] focused on 3D\nscenes (pretrained on a very large-scale generated\ndataset (about 1 terabyte), and sophisticatedly con-\nsidered matched points (i.e. common points) of two\ndifferent views (at least 30% overlap) as pairs. Unlike\nthat, our point cloud level based approach simply\nThe Visual Comptuer 2022\ncontrastive\ntransformation\ninput point cloud\ncontrastive pair\nbase\nencoder\nnetwork\nm\nm\nglobal feature\ncontrastive\nvector\noriginal point cloud\ntransformed\npoint cloud\nMLP\nprojection\nhead\nprojection\nfeature\ncontrastive\nvector\ncontrastive\nloss\n(+)\n(-)\n(-)\nfeature space\nk\nk\nFig. 1 Overview of our unsupervised contrastive representation learning method. Given a point cloud, the transformation (rotation in this\nwork) is used to the get transformed version of the original point cloud, which deﬁnes a pair. Then, the pairs are input to the base encoder\nnetwork (e.g. PointNet or DGCNN) to learn the global feature of each model. The projection head is further used to reduce the global feature\ndimension and for effective loss degradation. The contrastive loss encourages a pair of point clouds to be consistent in the feature space.\nuses a rotational transformation to generate a trans-\nformed version of an original point cloud. It can easily\nget a great pose discrepancy, without requiring point\ncloud overlap to satisfy the demand of obtaining a\ncertain number of matched points. In essence, a pair\nof matched points are treated as a pair in [42] to\nlearn point-level features, while a pair of point clouds\n(a point cloud consisting of a series of points) are\nregarded as a pair in our work. Treating the point\nclouds as the pair in our method has the advantage\nof learning better global representations when com-\npared with [42]. It is also intuitive and straightforward\nto use point cloud level, while PointContrast [42]\ncan hardly obtain point cloud representations directly\nand is suitable for point-wise tasks, e.g., scene seg-\nmentation. In comparison, our global feature of point\ncloud level can be easily used in both point cloud\nlevel and point-wise tasks (e.g., classiﬁcation and seg-\nmentation). Meanwhile, for unsupervised learning we\nderive two variants of contrastive losses based on\npoint clouds, which respectively facilitate two dif-\nferent types of downstream tasks (i.e., classiﬁcation\nand segmentation). Finally, the backbone networks are\ndifferent: they use a Sparse Residual U-Net which\nrequires voxelization of point clouds, while we use a\nsimple encoder-head structure.\nDifference from SimCLR [43]: It is derived on top\nof SimCLR [43], but is largely different from Sim-\nCLR: (1) SimCLR is designed for 2D images, and our\ncontrastive learning is for 3D point cloud data, which\ninvolves irregular point distribution, and poses more\nchallenges than 2D images with regular grids. (2)\nSimCLR uses multiple transformations for an original\nimage, and these two different versions of images act\nas a pair. By contrast, we only generate a transformed\nversion of the original point cloud with a simple trans-\nformation (e.g. rotation), thus forming a contrastive\npair of this point cloud (i.e. a pair in point cloud\nlevel). (3) SimCLR typically uses TPU resources with\na large batch size for training, while we demonstrate\nan elegant approach for 3D point cloud representa-\ntion learning, which is simple yet effective, without\nrequiring TPU computing resources.\n3 Method\nIn this work, we take 3D object classiﬁcation and\nsemantic segmentation (shape and scene) as the down-\nstream tasks of our unsupervised contrastive repre-\nsentation learning. To clearly elaborate our method,\nwe take downstream object classiﬁcation as an exam-\nple when designing the unsupervised stage, and we\nwill later explain how to extend it to shape and scene\nsegmentation.\nGiven an unlabeled point cloud, we ﬁrst use a\ntransformation (i.e. rotation) to generate its trans-\nformed version, thus constructing a contrastive point\ncloud pair for this original point cloud. They are fed\ninto a base encoder network, in order to learn a pair of\nglobal features or representations. The global features\nare then passed to a projection head network, to obtain\nanother pair of representations. These two representa-\ntions from the projection head network target to reach\na maximum agreement with the aid of a contrastive\nloss function. Figure 1 shows the framework of our\nunsupervised contrastive representation learning.\nThe Visual Comptuer 2022\n5\n3.1 Unsupervised Contrastive\nRepresentation Learning\nContrastive transformation. Unlike 2D images,\npoint cloud data often have an irregular distribution\nin 3D space, and have a complex degree of freedom.\nGiven this, it is more difﬁcult to identify the practi-\ncally useful transformations for constructing a good\ncontrastive pair of a point cloud. SimCLR [43] uti-\nlized different types of transformations for a single\nimage (e.g. cropping, rotation), and generated two\ntransformed versions. In contrast, we only use one\ntransformation for simplicity, and pair the original\npoint cloud with the transformed version, that is, a pair\nincluding the original point cloud and its transformed\ncounterpart. Common transformations in 3D space are\nrotation, cutout, crop, scaling, smoothing, noise cor-\nruption, etc. Since heavy noise corruption will destroy\nthe object shapes, we exclude this for transformations\napplied here. Jittering is analogous to light noise, and\nwe use jittering for data augmentation, following the\nprotocol of the state-of-the-art point based methods.\nIn this work, we select rotation as the transformation,\nand use it to generate a transformed version for the\noriginal point cloud. We provide the discussion of the\nchoice in ablation studies (Section 4.6).\nBase encoder network. Point based networks,\nsuch as PointNet [7], DGCNN [24], Pointﬁlter [14],\noften involve a pooling layer to output the global fea-\nture for an input point cloud. The former part of a point\nbased network before this layer (inclusive) can be nat-\nurally viewed as a based encoder in our framework. In\nother words, the input point cloud can be encoded into\na latent representation vector (i.e. the global feature).\nIn this sense, we can simply extract this former part of\nany such point based networks as a base encoder net-\nwork in our unsupervised contrastive representation\nframework. In this work, we select some state-of-\nthe-art point based networks including PointNet and\nDGCNN as the backbone, and extract their former\nparts as our base encoder accordingly. It is interest-\ning to discover that the encoders involving T-Net (i.e.\ntransformation net) will hinder the learning of unsu-\npervised contrastive representations. We deduce that\nT-Net accounts for various rotated point cloud aug-\nmentations, which degrades the ability of capturing\na large contrast between the input pair. As such, we\nremove the original T-Net (i.e. transformation net) in\nthese encoders, if involved. We show the results of\ndifferent encoders in Section 4.\nProjection head network. Point based networks\nusually have several fully connected layers to bridge\nthe global feature with the ﬁnal k-class vector. Simi-\nlar to the encoder, we can also simply extract the latter\npart of a point based network as the projection head.\nAlternatively, it is also ﬂexible to customize a projec-\ntion head network by designing more or fewer fully\nconnected layers.\nMathematically, the ﬁnal k-class vector (or repre-\nsentation vector) can be formulated as\nzi = H(E(P)),\nzj = H(E(P′)),\n(1)\nwhere P is an original point cloud and P′ is its\ntransformed counterpart. E and H denote the encoder\nnetwork and the projection head network, respectively.\nContrastive loss function. We ﬁrst randomly\nselect n samples, and use the selected transforma-\ntion (i.e. rotation) to generate another n corresponding\ntransformed counterparts, resulting in n pairs (2n\nsamples) constituting the minibatch. Analogous to\nSimCLR [43], we also do not explicitly deﬁne positive\nor negative pairs. Instead, we select a pair as the posi-\ntive pair, and the remaining (n−1) pairs (i.e. 2(n−1)\nsamples) are simply regarded as negative pairs.\nAs for the unsupervised loss function, InfoNCE\n[44] is a widely-used loss function for unsupervised\nrepresentation learning of 2D images. More recently,\n[42] also utilized a similar loss for contrastive scene\nrepresentation learning. Inspired by them, we also\nintroduce a variant as our unsupervised loss function,\nwhich is deﬁned as\nL = −1\nS\nX\n(i,j)∈S\nlog\nexp(zi · zj/τ)\nP\n(·,t)∈S,t̸=j exp(zi · zt/τ),\n(2)\nwhere S is the set of all positive pairs (point cloud\nlevel), and τ is a temperature parameter. denotes the\ncardinality of the set. The loss is computed using all\nthe contrastive pairs, and is equivalent to applying\nthe cross entropy with pseudo labels (e.g. 0 ∼15\nfor 16 pairs). We found it works very well in our\nunsupervised contrastive representation learning.\n3.2 Downstream 3D Object Classiﬁcation\nWe take 3D object classiﬁcation as the ﬁrst down-\nstream task in this work, to validate our unsupervised\nrepresentation learning. The above designed scheme\nThe Visual Comptuer 2022\nis immediately ready for the unsupervised represen-\ntation learning to facilitate the downstream classiﬁca-\ntion task. In particular, we will utilize two common\nschemes for validation here. One is to train a linear\nclassiﬁcation network by taking the learned represen-\ntations of our unsupervised learning as input. Here, the\nlearned representation is the global feature. We did not\nchoose the k-class representation vector as it had less\ndiscriminative features than the global feature in our\nframework, and it induced a poor performance (see\nSection 4.6). The other validation scheme is to initial-\nize the backbone with the unsupervised trained model\nand perform a supervised training. We will demon-\nstrate the classiﬁcation results for these two validation\nschemes in Section 4.\n3.3 Downstream Semantic Segmentation\nTo further demonstrate the effectiveness of our unsu-\npervised representation learning, we also ﬁt the above\nunsupervised learning scheme to the downstream\nsemantic segmentation, including shape part segmen-\ntation and scene segmentation. Since it is a different\ntask from 3D object classiﬁcation, we need to design\na new scheme to facilitate unsupervised training. We\nstill use the rotation to generate a transformed ver-\nsion of an original point cloud (e.g. a shape point\ncloud or a split block from the scene), and view them\nas a contrastive pair (i.e. point cloud level). As for\nsegmentation, each point in the point cloud has a fea-\nture representation. For unsupervised representation\nlearning, we compute the mean of all point-wise cross\nentropy to evaluate the overall similarity within the\nmini-batch. We therefore deﬁne a loss function for\nsemantic segmentation as:\nL = −1\nS\nX\n(a,b)∈S\n1\nP(a,b)\nX\n(i,j)∈P(a,b)\nlog\nexp(zi · zj/τ)\nP\n(·,t)∈P(a,b),t̸=j exp(zi · zt/τ),\n(3)\nwhere S is the set of all positive pairs (i.e. point\ncloud a and b), and P(a,b) is the set of all point pairs\n(i.e. the same point id) of the point cloud a and b. Sim-\nilarly, we apply the cross entropy with pseudo labels\nwhich match the point indices (e.g. 0 ∼2047 for 2048\npoints).\n4 Experimental Results\n4.1 Datasets\nObject classiﬁcation. We utilize ModelNet40 and\nModelNet10 [1] for 3D object classiﬁcation. We fol-\nlow the same data split protocols of PointNet-based\nmethods [7, 8, 24] for these two datasets. For Mod-\nelNet40, the train set has 9, 840 models and the test\nset has 2, 468 models, and the datset consists of 40\ncategories. For ModelNet10, 3, 991 models are for\ntraining and 908 models for testing. It contains 10 cat-\negories. For each model, we use 1, 024 points with\nonly (x, y, z) coordinates as the input, which is also\nconsistent with previous works.\nNote that some methods [10, 11, 13, 45] are pre-\ntrained under the ShapeNet55 dataset [47]. We also\nconduct a version of ShapeNet55 training for the clas-\nsiﬁcation task. We used the same dataset as [13],\nwhich has 57, 448 models with 55 categories, and all\nmodels will be used for unsupervised training. Follow-\ning the same setting of previous work, we use 2, 048\npoints as input.\nWe provide comparison experiments with Point-\nContrast [42] for the classiﬁcation task, and they use\nthe ShapeNetCore [47] for ﬁnetuning. The dataset\ncontains 51, 127 pre-aligned shapes from 55 cate-\ngories, which has 35, 708 models for training, 5, 158\nmodels for validation and 10, 261 models for testing.\nWe use 1, 024 points as input which is the same as\nPointContrast [42].\nShape part segmentation. We use the ShapeNet\nPart dataset [48] for shape part segmentation, which\nconsists of 16, 881 shapes from 16 categories. Each\nobject involves 2 to 6 parts, with a total number of\n50 distinct part labels. We follow the ofﬁcial dataset\nsplit and the same point cloud sampling protocol as\n[47]. Only the point coordinates are used as input. Fol-\nlowing [8, 24], we use mean Intersection-over-Union\n(mIoU) as the evaluation metric.\nScene segmentation. We also evaluate our model\nfor scene segmentation on Stanford Large-Scale 3D\nIndoor Spaces Dataset (S3DIS) [49]. This dataset con-\ntains 3D scans of 271 rooms and 6 indoor areas,\ncovering over 6, 000m2. We follow the same setting as\n[8, 24]. Each room is split with 1m×1m area into little\nblocks, and we sampled 4, 096 points of each block.\nEach point is represented as a 9D vector, which means\nthe point coordinates, RGB color and normalized loca-\ntion for the room. Each point is annotated with one of\nthe 13 semantic categories. We also follow the same\nThe Visual Comptuer 2022\n7\nTable 1\nClassiﬁcation results of unsupervised methods and our method (Linear Classiﬁer), on the datasets of ModelNet40 and ModelNet10.\nBoth ShapeNet55 (upper part) and ModelNet40 (bottom part) pretrained datasets are provided.\nMethods\nPretrained Dataset\nInput Data\nResolution\ne.g. # Points\nModelNet40\nAccuracy\nModelNet10\nAccuracy\nLatent-GAN [10]\nShapeNet55\nxyz\n2k\n85.70\n95.30\nFoldingNet [11]\nShapeNet55\nxyz\n2k\n88.40\n94.40\nMRTNet [45]\nShapeNet55\nxyz\nmulti-resolution\n86.40\n-\n3D-PointCapsNet [13]\nShapeNet55\nxyz\n2k\n88.90\n-\nOurs (DGCNN)\nShapeNet55\nxyz\n2k\n89.37\n-\nVIPGAN [46]\nModelNet40\nviews\n12\n91.98\n94.05\nLatent-GAN [10]\nModelNet40\nxyz\n2k\n87.27\n92.18\nFoldingNet [11]\nModelNet40\nxyz\n2k\n84.36\n91.85\n3D-PointCapsNet [13]\nModelNet40\nxyz\n1k\n87.46\n-\nPointHop [41]\nModelNet40\nxyz\n1k\n89.10\n-\nMAP-VAE [12]\nModelNet40\nxyz\n2k\n90.15\n94.82\nGLR (RSCNN-Large) [40]\nModelNet40\nxyz\n1k\n92.9\n-\nOurs (PointNet [7])\nModelNet40\nxyz\n1k\n88.65\n90.64\nOurs (DGCNN [24])\nModelNet40\nxyz\n1k\n90.32\n95.09\nprotocol of adopting the six-fold cross validation for\nthe six area.\nPlease refer to the Appendices for additional\ninformation and visual results.\n4.2 Experimental Setting\nWe use Adam optimizer for our unsupervised rep-\nresentation training. We implemented our work with\nTensorFlow, and use a single TITAN V GPU for\ntraining (DGCNN using multiple GPUs).\nFor downstream 3D object classiﬁcation on Mod-\nelNet40, ModelNet10, ShapeNet55 and ShapeNet-\nCore, we use a batch size of 32 (i.e. 16 contrastive\npairs) for training and testing. Temperature hyper-\nparameter τ is set as 1.0. We use the same dropouts\nwith the original methods accordingly, i.e. 0.7 for\nPointNet as backbone, 0.5 for DGCNN as backbone.\nThe initial decay rate of batch normalization is 0.5,\nand will be increased no lager than 0.99. The training\nstarts with a 0.001 learning rate, and is decreased to\n0.00001 with an exponential decay.\nWe employ DGCNN as the backbone for seman-\ntic segmentation. As for shape part segmentation on\nShapeNet Part dataset, we utilize a batch size of 16\n(i.e. 8 constrasive pairs) for training. We use a batch\nsize of 12 (i.e. 6 constrasive pairs) for scene segmen-\ntation on S3DIS. For the two tasks, we simply use a\nbatch size of 1 during testing, and the other settings\nfollow DGCNN.\n4.3 3D Object Classiﬁcation\nWe conduct two kinds of experiments to evaluate\nthe learned representations of our unsupervised con-\ntrastive learning. We ﬁrst train a simple linear classiﬁ-\ncation network with the unsupervised representations\nas input. Secondly, we take our unsupervised rep-\nresentation learning as pretraining, and initialize the\nweights of the backbone before supervised training.\nTables 1 and 3 show 3D object classiﬁcation results\nfor our method and a wide range of state-of-the-art\ntechniques.\nLinear classiﬁcation evaluation. In this part, we\nuse the former part of PointNet [7] and DGCNN [24]\nas the base encoder, and use the latter mlp layers\nas the projection head. The learned features are used\nas the input for training the linear classiﬁcation net-\nwork. We use the test accuracy as the evaluation of\nour unsupervised contrastive learning. Comparisons\nare reported in Table 1. Regarding linear classiﬁcation\nevaluation, our method with DGCNN as backbone\nalways performs better than our method using Point-\nNet as backbone, for example, 95.09% versus 90.64%\nfor ModelNet10, 90.32% versus 88.65% for Model-\nNet40. This is due to a more complex point based\nstructure of DGCNN. Our method with DGCNN as\nbackbone also outperforms most unsupervised tech-\nniques, like two recent methods PointHop (1.22%\ngain) and MAP-VAE (0.17% gain), and is comparable\nto some supervised methods in Table 3, for exam-\nple, 90.32% versus 90.6% (O-CNN) and 90.9% (SO-\nNet with xyz) on ModelNet40, 95.09% versus 93.9%\nThe Visual Comptuer 2022\n(supervised 3D-GCN) on ModelNet10. The GLR [40]\nmined rich semantic and structural information, and\nused a larger RSCNN as backbone (4×RSCNN) [22],\nresulting in a better accuracy than our method.\nNotice\nthat\nsome\nmethods\nused\na\nlarger\nShapeNet55 dataset for training [10, 11, 13, 45].\nAlthough the previous work [12] re-implemented\nthem by training on ModelNet40, they use 2048 points\nrather than our 1024. To provide additional insights,\nwe re-implement and train a state-of-the-art method\n(3D-PointCapsNet [13]) on ModelNet40 with 1024\npoints, and train a linear classiﬁer for evaluation. We\nchoose this method since its code is publicly available\nand it is recent work. From Table 1, it is obvious that\nour method (DGCNN as backbone) still outperforms\n3D-PointCapsNet by a 2.86% margin.\nTo show our learned representations have the\ntransfer capability, we also train our method (DGCNN\nas backbone) on ShapeNet55 dataset for unsuper-\nvised contrastive learning and then feed the Mod-\nelNet40 dataset to the trained model to get point\ncloud features. We use these features to train a lin-\near classiﬁer on ModelNet40 for evaluation. From\nTable 1 we can see that our method achieves the best\nresult compared with other state-of-art methods on the\nsame settings (Latent-GAN [10], FoldingNet [11], and\n3D-PointCapsNet [13]), exceeding them by 3.67%,\n0.97%, and 0.47%, respectively.\nWe also conduct an experiment for training with\nlimited data to verify the capability of our pretrained\nmodel with linear classiﬁer. The results can be seen\nin Table 2. Our pretrained model achieves 86.6% with\n30% data, which is 2.4% higher compared with Fold-\ningNet’s [11] 84.2%. With less data, e.g. 10% of the\ndata, our result is 0.6% lower than FoldingNet. We\nsuspect that with far fewer data, our contrastive learn-\ning method could less effectively learn the features\nof each sample with such simple transformation, and\nresult in less improvement.\nTable 2\nComparison results of classiﬁcation accuracy with\nlimited training data (different ratios).\nMethods\n10%\n20%\n30%\nFoldingNet [11])\n81.2\n83.6\n84.2\nOurs (DGCNN as backbone)\n80.6\n85.4\n86.6\nPretraining evaluation. In addition to the above\nevaluation using a linear classiﬁer, we further utilize\nthe pre-training evaluation to demonstrate the efﬁ-\ncacy of our unsupervised contrastive representation\nlearning. Speciﬁcally, we also select PointNet and\nDGCNN as the backbone, in which the part before and\nincluding the global feature is regarded as the base\nencoder, and the remaining classiﬁcation branch (i.e.\nseveral mlp layers) as the projection head. After our\nunsupervised representation training, we initialize the\ncorresponding network with the unsupervised trained\nmodel, and then perform the supervised training. Table\n3 shows the comparison results of our method and\nthe state-of-the-art 3D object classiﬁcation techniques\nwith supervised training.\nThe pretraining evaluation based on our unsuper-\nvised representation learning sees an improvement\nover the original backbone network, increased from\n89.2% to 90.44% (1.24% increase) with PointNet\nand from 92.2% to 93.03% (0.83% increase) with\nDGCNN on ModelNet40. Regarding ModelNet10, the\naccuracy of PointNet as our backbone for pretraining\nevaluation is 94.38%, which is on par with the super-\nvised 3D-GCN (93.9%). It is interesting to see that\nour method (DGCNN as backbone) is the best one on\nModelNet10 in the pretraining evaluation, while the\nsecond best is achieved by two very recent supervised\nmethods (Neural Implicit [35] and PointASNL [36]).\n[35] even used a large weight matrix as the input for\nclassiﬁcation training. For ModelNet40, our method\n(DGCNN as backbone) achieves 93.03%, outperform-\ning almost all techniques including both supervised\nand unsupervised ones. For example, our method in\nthis case outperforms the very recent supervised meth-\nods including 3D-GCN [25], Neural Implicit [35] and\nPointASNL [36].\nCompared to using PointNet as backbone, taking\nDGCNN as backbone achieves a better classiﬁca-\ntion accuracy, for example, 95.93% versus 94.38%,\n93.03% versus 90.44%. Similarly, we believe this is\nmainly because DGCNN exploits richer information\nthan PointNet.\nPointContrast [42] also presented an unsupervised\ncontrastive learning approach, which is based on point\nlevel while ours is based on point cloud level. They\nvalidated their effectiveness on some datasets using\nthe pretrain-ﬁnetuning strategy. In order to provide\na potential comparison with it, we also used the\nShapeNetCore dataset for the classiﬁcation task with\npretraining evaluation. The comparison results are\nshown in Table 4, and we can see that our method\n(DGCNN as backbone) outperforms them by 0.5%,\nthough PointContrast is pretrained on a rather larger\ndataset (ScanNet). Note that our method is not suitable\nto be pretrained on ScanNet since this downstream\nThe Visual Comptuer 2022\n9\nTable 3\nClassiﬁcation results of other supervised methods and our method (Pretraining), on the datasets of ModelNet40 and ModelNet10.\nWe distinguish the results of other methods from our method by the line.\nMethods\nInput Data\nResolution\ne.g. # Points\nModelNet40\nAccuracy\nModelNet10\nAccuracy\nKd-Net (depth=10) [50]\ntree\n210 × 3\n90.6\n93.3\nPointNet++ [8]\nxyz\n1k\n90.7\n-\nKCNet [51]\nxyz\n1k\n91.0\n94.4\nMRTNet [45]\nxyz\n1k\n91.2\n-\nSO-Net [32]\nxyz\n2k\n90.9\n94.1\nKPConv [52]\nxyz\n6.8k\n92.9\n-\nPointNet++ [8]\nxyz, normal\n5k\n91.9\n-\nSO-Net [32]\nxyz, normal\n5k\n93.4\n-\nO-CNN [3]\nxyz, normal\n-\n90.6\n-\nPointCNN [9]\nxyz\n1k\n92.2\n-\nPCNN [53]\nxyz\n1k\n92.3\n94.9\nPoint2Sequence [54]\nxyz\n1k\n92.6\n95.3\nRS-CNN (voting) [22]\nxyz\n1k\n93.6\n-\nNeural Implicit [35]\nweights\n1024 × 256\n92.2\n95.7\nPointASNL [36]\nxyz\n1k\n92.9\n95.7\n3D-GCN [25]\nxyz\n1k\n92.1\n93.9\nHAPGN [38]\nxyz\n1k\n91.7\n-\nMVSG-DNN [19]\nviews\n12\n92.3\n94.0\nScratch (PointNet [7])\nxyz\n1k\n89.2\n-\nOurs (PointNet [7])\nxyz\n1k\n90.44 (+1.24)\n94.38\nScratch (DGCNN [24])\nxyz\n1k\n92.2\n-\nOurs (DGCNN [24])\nxyz\n1k\n93.03 (+0.83)\n95.93\ntask is for classiﬁcation (requiring point cloud level\nfeatures for classiﬁcation) while ScanNet has point-\nwise labels. The good performance of our method\nis mainly due to the proper design of point cloud\nlevel based contrastive pairs and contrastive learn-\ning, so that we can directly obtain the global fea-\nture from contrastive representation learning. We also\nre-implement DGCNN [24] on the ShapeNetCore\ndataset, which further demonstrates the effectiveness\nof our method by increasing from 84.0% (original\nDGCNN) to 86.2%. In comparison with PointContrast\nwhich improved 0.6% from the version of training\nfrom scratch, we achieve 2.2% increase.\nTable 4\nComparison results of PointContrast and our method on\nthe dataset of ShapeNetCore with Pretraining evaluation. Note that\n* represents that the model is trained on ScanNet.\nMethods\nAccuracy\nTrained from scratch (PointContrast [42])\n85.1\nPointContrast* [42]\n85.7\nTrained from scratch (original DGCNN [24])\n84.0\nOurs (DGCNN as backbone)\n86.2\n4.4 Shape Part Segmentation\nIn addition to the 3D object classiﬁcation, we also\nverify our method on shape part segmentation. The\nsegmentation results are listed in Table 5. Here we take\nDGCNN as the backbone of our approach and simply\nemploy the linear classiﬁer evaluation setting. It can\nbe seen from the table that our method in linear clas-\nsiﬁcation evaluation achieves 79.2% instance mIOU\nand 75.5% class mIOU, which are remarkably better\nthan state-of-the-art unsupervised techniques includ-\ning MAP-VAE [12] and Multi-task [55]. Speciﬁcally,\nour method outperforms MAP-VAE [12] and Multi-\ntask [55] by a margin of 7.55% and 3.4%, respectively,\nin terms of class mIOU. Figure 2 illustrates some\nexamples of our method (Linear Classiﬁer setting) on\nthe task of shape part segmentation.\n4.5 Scene Segmentation\nWe also test our method for the scene segmentation\ntask on the S3DIS dataset, which typically appears\nto be more challenging than the shape part segmen-\ntation. Similarly, we utilize DGCNN as the backbone\nThe Visual Comptuer 2022\nTable 5 Shape part segmentation results of our method (Linear Classiﬁer) and state-of-the-art techniques on ShapeNet Part dataset. We\ndistinguish between supervised and unsupervised learning methods by the line.\nMethods\nSupervised\nclass\nmIOU\ninstance\nmIOU\nair.\nbag\ncap\ncar\nchair\near.\nguit.\nkni.\nlam.\nlap.\nmot.\nmug\npist.\nrock.\nska.\ntab.\nKd-Net [50]\nyes\n77.4\n82.3\n80.1\n74.6\n74.3\n70.3\n88.6\n73.5\n90.2\n87.2\n81.0\n84.9\n87.4\n86.7\n78.1\n51.8\n69.9\n80.3\nMRTNet [45]\nyes\n79.3\n83.0\n81.0\n76.7\n87.0\n73.8\n89.1\n67.6\n90.6\n85.4\n80.6\n95.1\n64.4\n91.8\n79.7\n87.0\n69.1\n80.6\nPointNet [7]\nyes\n80.4\n83.7\n83.4\n78.7\n82.5\n74.9\n89.6\n73.0\n91.5\n85.9\n80.8\n95.3\n65.2\n93.0\n81.2\n57.9\n72.8\n80.6\nKCNet [51]\nyes\n82.2\n84.7\n82.8\n81.5\n86.4\n77.6\n90.3\n76.8\n91.0\n87.2\n84.5\n95.5\n69.2\n94.4\n81.6\n60.1\n75.2\n81.3\nRS-Net [56]\nyes\n81.4\n84.9\n82.7\n86.4\n84.1\n78.2\n90.4\n69.3\n91.4\n87.0\n83.5\n95.4\n66.0\n92.6\n81.8\n56.1\n75.8\n82.2\nSO-Net [32]\nyes\n81.0\n84.9\n82.8\n77.8\n88.0\n77.3\n90.6\n73.5\n90.7\n83.9\n82.8\n94.8\n69.1\n94.2\n80.9\n53.1\n72.9\n83.0\nPointNet++ [8]\nyes\n81.9\n85.1\n82.4\n79.0\n87.7\n77.3\n90.8\n71.8\n91.0\n85.9\n83.7\n95.3\n71.6\n94.1\n81.3\n58.7\n76.4\n82.6\nDGCNN [24]\nyes\n82.3\n85.2\n84.0\n83.4\n86.7\n77.8\n90.6\n74.7\n91.2\n87.5\n82.8\n95.7\n66.3\n94.9\n81.1\n63.5\n74.5\n82.6\nKPConv [52]\nyes\n85.1\n86.4\n84.6\n86.3\n87.2\n81.1\n91.1\n77.8\n92.6\n88.4\n82.7\n96.2\n78.1\n95.8\n85.4\n69.0\n82.0\n83.6\nNeural Implicit [35]\nyes\n-\n85.2\n84.0\n80.4\n88.0\n80.2\n90.7\n77.5\n91.2\n86.4\n82.6\n95.5\n70.0\n93.9\n84.1\n55.6\n75.6\n82.1\n3D-GCN [25]\nyes\n82.1\n85.1\n83.1\n84.0\n86.6\n77.5\n90.3\n74.1\n90.9\n86.4\n83.8\n95.6\n66.8\n94.8\n81.3\n59.6\n75.7\n82.8\nHAPGN [38]\nyes\n87.1\n89.3\n87.1\n85.7\n90.1\n86.2\n91.7\n78.3\n94.3\n85.9\n82.6\n95.2\n77.9\n94.3\n90.1\n73.9\n90.3\n90.6\nMAP-VAE [12]\nno\n67.95\n-\n62.7\n67.1\n73.0\n58.5\n77.1\n67.3\n84.8\n77.1\n60.9\n90.8\n35.8\n87.7\n64.2\n45.0\n60.4\n74.8\nMulti-task [55]\nno\n72.1\n77.7\n78.4\n67.7\n78.2\n66.2\n85.5\n52.6\n87.7\n81.6\n76.3\n93.7\n56.1\n80.1\n70.9\n44.7\n60.7\n73.0\nOurs (DGCNN)\nno\n75.5\n79.2\n76.3\n76.6\n82.5\n65.8\n85.9\n67.1\n86.6\n81.3\n79.2\n93.8\n55.8\n92.8\n73.5\n53.1\n61.3\n76.6\nairplane\nbag\ncap\ncar\nchair\nguitar\nknife\nlamp\nmotorbike\nmug\nskateboard\ntable\nFig. 2\nSome examples of shape part segmentation using our method (Linear Classiﬁer setting).\nand adopt the Linear Classiﬁer evaluation setting. We\nare not able to compare our method with unsupervised\nmethods like MAP-VAE [12] and Multi-task [55],\nsince they did not provide scene segmentation results\nand their source codes are not publicly available. Table\n6 lists the comparisons of 1 fold testing on Area 5. It is\nobserved that our method even outperforms the super-\nvised PointNet in terms of mean accuracy. Due to the\nunsupervised property, our method is inferior to the\nsupervised PointCNN and ﬁne-tuned PointContrast.\nOur method has relatively smaller mean IOU, which\nis probably due to the imbalanced categories and the\nlimited minibatch size. The performance could be fur-\nther improved if more powerful computing resources\nare allowed. Figure 3 shows a visual example for scene\nsegmentation.\nTable 6 Scene segmentation results of our method (Linear\nClassiﬁer) and some state-of-the-art techniques on testing Area 5\n(Fold 1) of the S3DIS dataset.\nMethods\nSupervised\nMean\naccuracy\nMean\nIOU\nPointNet [7]\nyes\n49.0\n41.1\nPointCE [39]\nyes\n-\n51.7\nPointCNN [9]\nyes\n63.9\n57.3\nPointContrast [42]\nyes\n76.9\n70.3\nOurs (DGCNN)\nno\n59.4\n32.6\n4.6 Ablation Studies\nTransformation. One of the key elements in our\nunsupervised representation learning is using 180◦\nrotation around the Y axis to get the transformation.\nTo comprehensively study the inﬂuence of transfor-\nmation on representations, we consider many common\ntransformations including rotation, cutout, crop, scale,\njittering and smoothing. Figure 4 visualizes different\ntransformations for a point cloud.\nThe Visual Comptuer 2022\n11\nGround truth\nOur result\nFig. 3 Visual result of scene segmentation.\nWe list the comparison results of the above trans-\nformations in Table 7. It can be clearly observed that\nour choice attains the best accuracy, which is unlike\nSimCLR [43] that utilizes two different transforma-\ntions of an image as the pair. We suspect that rotation\nis a very simple and effective transformation for 3D\npoint cloud data, and a larger valid rotation would gen-\nerate a greater pose discrepancy (i.e., contrast) in 3D\nspace. As such, our choice using 180◦rotation around\nthe Y axis is better than others.\nFurthermore, we apply two sequential transforma-\ntions on one point cloud and make the result as a\npair with the original point cloud. We chose the best\ntransformation (i.e. rotate 180◦around Y axis) as the\nﬁrst transformation, and then apply one of the rest of\nthe transformations as the second. For more complex\ntransformations, we group all the transformations into\nfour categories, including rotation, scaling, jittering/s-\nmoothing, and cropping/cutout, and apply these four\ncategories on one point cloud sequentially. We show\nthe results in Table 8. It can be seen that after applying\nthe complex transformation, it is still not as good as\nthe best choice shown above. We suspect that the com-\nplex transformation may damage the information on\nthe point cloud, thus leading to inferior results. Again,\nthis veriﬁes that our choice is the best transformation\nfor 3D point cloud data in generating contrastive pairs.\nOutput of encoder versus output of projection\nhead. We also compare the choices of using the output\nof the base encoder (i.e. global feature) and the output\nof the projection head for subsequent linear classiﬁ-\ncation. Table 9 shows the comparison results of the\ntwo choices on ModelNet40 and ModelNet10. We see\nthat the former choice is better than the latter choice.\nWe think the output of the base encoder involves more\ndiscriminative features for the training of the linear\nclassiﬁer.\nCross validation. In addition to the above evalua-\ntions, we further test the abilities of our unsupervised\ncontrastive representation learning in a crossed eval-\nuation setting. To achieve this, we use the learned\nOriginal\nRotate 180◦\n(Y axis)\nRotate 90◦\n(Y axis)\nRotate 45◦\n(Y axis)\nRotate 180◦\n(X axis)\nRotate 90◦\n(X axis)\nRotate 45◦\n(X axis)\nCutout\nCrop\nScale\nJitter\nSmooth\nFig. 4 Illustration for transformations used in Table 7.\nTable 7 Comparison of different contrastive transformation on\nModelNet10. DGCNN [24] is the backbone. We use linear\nclassiﬁcation evaluation for comparisons. % is used for\nclassiﬁcation accuracy.\nTransformation\nMean class\naccuracy\nOverall\naccuracy\nrotate 180◦(Y axis)\n94.88\n95.09\nrotate 90◦(Y axis)\n94.12\n94.53\nrotate 45◦(Y axis)\n94.09\n94.20\nrotate 180◦(X axis)\n93.21\n93.53\nrotate 90◦(X axis)\n93.30\n93.42\nrotate 45◦(X axis)\n93.71\n93.97\ncutout\n94.01\n93.97\ncrop\n93.80\n94.31\nscale\n94.10\n94.20\njitter\n93.95\n93.97\nsmooth\n93.93\n94.08\nrepresentations from the unsupervised trained model\non ModelNet40 to further train a linear classiﬁer on\nModelNet10, and vice versa. Classiﬁcation outcomes\nare reported in Table 10. It can be observed that our\nunsupervised representation learning is indeed work-\ning in the cross-dataset setting. It also reveals that our\nunsupervised method trained on a large dataset would\nprobably beneﬁt the testing on another dataset greatly.\nIn here, our method trained on ModelNet40 enables a\nbetter cross-test accuracy, compared to unsupervised\ntraining on ModelNet10 and testing on ModelNet40.\nPretraining evaluation: initializing projection\nhead. Projection head is very useful in maximizing the\nagreement between the contrastive pair. However, it\nThe Visual Comptuer 2022\nTable 8 Comparison of more complex contrastive transformation\non ModelNet10. We distinguish between the results of using only\ntwo transformations and those of using four transformations (more\ncomplex) by the line. “Rotate” means 180◦rotation around the Y\naxis. DGCNN [24] is the backbone. We use linear classiﬁcation\nevaluation for comparisons. % is used for classiﬁcation accuracy.\nTransformation\nMean class\naccuracy\nOverall\naccuracy\nrotate + cutout\n93.43\n93.64\nrotate + crop\n93.90\n93.97\nrotate + scale\n94.11\n94.08\nrotate + jitter\n94.33\n94.42\nrotate + smooth\n93.41\n93.64\nrotate + scale + jitter + cutout\n94.07\n94.42\nrotate + scale + jitter + crop\n93.92\n94.31\nrotate + scale + smooth + cutout\n93.56\n93.86\nrotate + scale + smooth + crop\n93.78\n94.20\nrotate 180◦(Y axis)\n94.88\n95.09\nTable 9 Comparison of using the output of encoder and projection\nhead for linear classiﬁcation evaluation. PointNet [7] is the\nbackbone. % is used for classiﬁcation accuracy.\nComponent\nDataset\nMean class\naccuracy\nOverall\naccuracy\nencoder\nModelNet40\n83.81\n88.65\nhead\nModelNet40\n68.55\n75.81\nencoder\nModelNet10\n90.55\n90.64\nhead\nModelNet10\n81.57\n82.59\nTable 10 Cross validation for ModelNet40 and ModelNet10. We\nperform unsupervised learning on one dataset and conduct the\nclassiﬁcation task on another dataset. PointNet [7] is the backbone.\n% is used for classiﬁcation accuracy.\nUnsupervised\ndataset\nClassiﬁcation\ndataset\nMean class\naccuracy\nOverall\naccuracy\nModelNet40\nModelNet10\n90.00\n90.51\nModelNet10\nModelNet40\n77.13\n82.87\nmay hinder pretraining evaluation, if the correspond-\ning part is initialized with the projection head of the\nunsupervised model. Table 11 shows that initializing\nencoder only produces better classiﬁcation accuracy\nfor PointNet/DGCNN on ModelNet10/ModelNet40,\nwhich conﬁrms the judgement that initializing encoder\nonly is a better choice.\nT-SNE Visualization. We utilize the t-SNE to\nvisualize the features learned on ModelNet40 in\nFigure 5. It can be seen that using only a linear clas-\nsiﬁer can separate different features to some extent.\nIt is worth noting that our model using unsupervised\ncontrastive learning can better separate features after\ntraining a linear classiﬁer, which implies that our con-\ntrastive learning is useful and effectively facilitates the\nclassiﬁcation task.\nTable 11 Pretraining validation to determine whether using\npreojection head for initialization. ModelNet40 and ModelNet10\nare used for datasets. PointNet [7] and DGCNN[24] are the\nbackbone. % is used for classiﬁcation accuracy.\nBackbone\nDataset\nHead\ninitialization\nMean class\naccuracy\nOverall\naccuracy\nPointNet\nModelNet10\nyes\n93.80\n93.97\nPointNet\nModelNet10\nno\n94.23\n94.38\nPointNet\nModelNet40\nyes\n86.64\n90.22\nPointNet\nModelNet40\nno\n86.80\n90.44\nDGCNN\nModelNet10\nyes\n95.05\n95.09\nDGCNN\nModelNet10\nno\n95.78\n95.93\nDGCNN\nModelNet40\nyes\n88.58\n91.96\nDGCNN\nModelNet40\nno\n89.52\n93.03\nFig. 5 T-SNE visualization of features. (a) without contrastive\nlearning, (b) with contrastive learning.\n5 Conclusion\nWe have presented an unsupervised representation\nlearning method for 3D point cloud data. We identi-\nﬁed that rotation is a very useful transformation for\ngenerating a contrastive version of an original point\ncloud. Unsupervised representations are learned via\nmaximizing the correspondence between paired point\nclouds (i.e. an original point cloud and its contrastive\nversion). Our method is simple to implement and does\nnot require expensive computing resources like TPU.\nWe evaluate our unsupervised representations for\nthe downstream tasks including 3D object classiﬁca-\ntion, shape part segmentation and scene segmentation.\nExperimental results demonstrate that our method\ngenerates impressive performance. In the future, We\nwould like to exploit semi-supervised techniques like\n[57] to improve the performance. We would also like\nto extend our approach to other interesting applica-\ntions such as 3D object detection.\nConﬂicts of Interests\nThe authors declare that the work is original and has\nnot been submitted elsewhere.\nThe Visual Comptuer 2022\n13\ncontrastive\ntransformation\ninput point cloud\ncontrastive pair\nbase\nencoder\nnetwork\nglobal feature\ncontrastive\nvector\noriginal point cloud\ntransformed\npoint cloud\nMLP\nprojection\nhead\nprojection feature\ncontrastive\nvector\ncontrastive\nloss\n(+)\n(-)\nfeature space\nn x m\n(+)\n(-)\n(-)\npoint cloud\nfeature\npoint feature\nn x m\nn x k\nn x k\nFig. A1\nOverview of our unsupervised contrastive learning for the downstream segmentation task. All the point clouds in the minibatch will\nbe mapped into the feature space. The designed contrastive loss (shown in the main paper) encourages a pair of point clouds (original point\ncloud and its transformed point cloud) to be consistent in the feature space, and the point-wise features of the same point ID also tend to be\nconsistent.\nAppendix A\nOverview of\nSegmentation\nWe also achieve the task of point cloud semantic\nsegmentation, including shape part segmentation and\nscene segmentation. Different from the 3D object clas-\nsiﬁcation task, we need to gain all the point-wise fea-\ntures in the point cloud, which is the key to solve the\nsegmentation task. For our unsupervised contrastive\nlearning, as shown in Figure A1, we still consider the\noriginal point cloud and its transformed point cloud as\na contrastive pair. However, in order to ensure that the\nfeature of each point in the point cloud will be learned,\nwe use the mean of point-wise cross entropy to evalu-\nate the point cloud similarity, and try to maximize the\nsimilarity of the positive pair (all other pairs of point\nclouds in the minibatch are viewed as negative pairs).\nIn this unsupervised manner, our framework can learn\nthe feature of each point in the point cloud.\nAppendix B\nAdditional Visual\nResults on Scene\nSegmentation\nIn this section, we show more visual results on scene\nsegmentation. Similarly, we utilize the Linear Clas-\nsiﬁer setting for this downstream task. Figure B2\nshows the visual results of several scenes. We can\nobserve from the ﬁgure that our method produces\nclose segmentation results to the ground truth. This\ndemonstrates the capability of our unsupervised rep-\nresentation learning method.\nGround truth\nOur result\nFig. B2 Visual result of scene segmentation.\nAppendix C\nAdditional Visual\nResults on Shape\nPart Segmentation\nIn this section, we put more visual results of our\nmethod on the downstream shape part segmentation.\nWe simply employ the Linear Classiﬁer setting for this\ndownstream task. Figure C3 shows the visual results\nof 32 models of 16 categories, involving 2 models\nper category. As we can see from the ﬁgure, with\nour unsupervised learned representations, a simple\nlinear classiﬁer for the downstream task can gener-\nate very similar visual results to the ground truth\nsegmentation. It further conﬁrms the effectiveness of\nour unsupervised method in learning distinguishable\nrepresentations.\nThe Visual Comptuer 2022\nGround truth\nOur result\nGround truth\nOur result\nGround truth\nOur result\nGround truth\nOur result\nairplane\nbag\ncap\ncar\nchair\nearphone\nguitar\nknife\nlamp\nlaptop\nmotorbike\nmug\npistol\nrocket\nskateboard\ntable\nFig. C3 Some examples of all 16 categories in ShapeNet Part dataset.\nThe Visual Comptuer 2022\n15\nReferences\n[1] Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L.,\nTang, X., Xiao, J.: 3d shapenets: A deep repre-\nsentation for volumetric shapes. In: Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1912–1920 (2015)\n[2] Riegler, G., Osman Ulusoy, A., Geiger, A.: Oct-\nnet: Learning deep 3d representations at high\nresolutions. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recogni-\ntion, pp. 3577–3586 (2017)\n[3] Wang, P.-S., Liu, Y., Guo, Y.-X., Sun, C.-Y.,\nTong, X.: O-cnn: Octree-based convolutional\nneural networks for 3d shape analysis. ACM\nTransactions on Graphics (TOG) 36(4), 1–11\n(2017)\n[4] Su, H., Maji, S., Kalogerakis, E., Learned-Miller,\nE.: Multi-view convolutional neural networks\nfor 3d shape recognition. In: Proceedings of\nthe IEEE International Conference on Computer\nVision, pp. 945–953 (2015)\n[5] Li, L., Zhu, S., Fu, H., Tan, P., Tai, C.-L.: End-to-\nend learning local multi-view descriptors for 3d\npoint clouds. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, pp. 1919–1928 (2020)\n[6] Lyu, Y., Huang, X., Zhang, Z.: Learning to seg-\nment 3d point clouds in 2d image space. In:\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp.\n12255–12264 (2020)\n[7] Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet:\nDeep learning on point sets for 3d classiﬁca-\ntion and segmentation. In: Proceedings of the\nIEEE Conference on Computer Vision and Pat-\ntern Recognition, pp. 652–660 (2017)\n[8] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Point-\nnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In: Advances in Neu-\nral Information Processing Systems, pp. 5099–\n5108 (2017)\n[9] Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.:\nPointcnn: Convolution on x-transformed points.\nIn: Advances in Neural Information Processing\nSystems, pp. 820–830 (2018)\n[10] Achlioptas, P., Diamanti, O., Mitliagkas, I.,\nGuibas, L.: Learning representations and genera-\ntive models for 3d point clouds. In: International\nConference on Machine Learning, pp. 40–49\n(2018). PMLR\n[11] Yang, Y., Feng, C., Shen, Y., Tian, D.: Fold-\ningnet: Point cloud auto-encoder via deep grid\ndeformation. In: Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recog-\nnition, pp. 206–215 (2018)\n[12] Han, Z., Wang, X., Liu, Y.-S., Zwicker, M.:\nMulti-angle point cloud-vae: unsupervised fea-\nture learning for 3d point clouds from multiple\nangles by joint self-reconstruction and half-to-\nhalf prediction. In: 2019 IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV),\npp. 10441–10450 (2019). IEEE\n[13] Zhao, Y., Birdal, T., Deng, H., Tombari, F.: 3d\npoint capsule networks. In: Proceedings of the\nIEEE Conference on Computer Vision and Pat-\ntern Recognition, pp. 1009–1018 (2019)\n[14] Zhang, D., Lu, X., Qin, H., He, Y.: Pointﬁlter:\nPoint cloud ﬁltering via encoder-decoder mod-\neling. IEEE Transactions on Visualization and\nComputer Graphics, 1–1 (2020). https://doi.org/\n10.1109/TVCG.2020.3027069\n[15] Lu, D., Lu, X., Sun, Y., Wang, J.: Deep\nfeature-preserving\nnormal\nestimation\nfor\npoint\ncloud\nﬁltering.\nComputer-\nAided\nDesign\n125,\n102860\n(2020).\nhttps://doi.org/10.1016/j.cad.2020.102860\n[16] Lu, X., Schaefer, S., Luo, J., Ma, L., He, Y.: Low\nrank matrix approximation for 3d geometry ﬁl-\ntering. IEEE Transactions on Visualization and\nComputer Graphics, 1–1 (2020). https://doi.org/\n10.1109/TVCG.2020.3026785\n[17] Lu, X., Wu, S., Chen, H., Yeung, S., Chen,\nW., Zwicker, M.: Gpf: Gmm-inspired feature-\npreserving point set ﬁltering. IEEE Transac-\ntions on Visualization and Computer Graph-\nics 24(8), 2315–2326 (2018). https://doi.org/10.\n1109/TVCG.2017.2725948\nThe Visual Comptuer 2022\n[18] Su, H., Jampani, V., Sun, D., Maji, S., Kaloger-\nakis, E., Yang, M.-H., Kautz, J.: Splatnet: Sparse\nlattice networks for point cloud processing. In:\nProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pp. 2530–\n2539 (2018)\n[19] Zhou, H.-Y., Liu, A.-A., Nie, W.-Z., Nie, J.:\nMulti-view saliency guided deep neural network\nfor 3-d object retrieval and classiﬁcation. IEEE\nTransactions on Multimedia 22(6), 1496–1506\n(2019)\n[20] Wu, W., Qi, Z., Fuxin, L.: Pointconv: Deep con-\nvolutional networks on 3d point clouds. In: Pro-\nceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 9621–9630\n(2019)\n[21] Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y.: Spi-\ndercnn: Deep learning on point sets with param-\neterized convolutional ﬁlters. In: Proceedings of\nthe European Conference on Computer Vision\n(ECCV), pp. 87–102 (2018)\n[22] Liu, Y., Fan, B., Xiang, S., Pan, C.: Relation-\nshape convolutional neural network for point\ncloud analysis. In: Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recog-\nnition, pp. 8895–8904 (2019)\n[23] Komarichev, A., Zhong, Z., Hua, J.: A-cnn:\nAnnularly convolutional neural networks on\npoint clouds. In: Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recog-\nnition, pp. 7421–7430 (2019)\n[24] Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bron-\nstein, M.M., Solomon, J.M.: Dynamic graph cnn\nfor learning on point clouds. Acm Transactions\nOn Graphics (tog) 38(5), 1–12 (2019)\n[25] Lin, Z.-H., Huang, S.-Y., Wang, Y.-C.F.: Convo-\nlution in the cloud: Learning deformable kernels\nin 3d graph convolution networks for point cloud\nanalysis. In: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recog-\nnition, pp. 1800–1809 (2020)\n[26] Jiang, L., Shi, S., Tian, Z., Lai, X., Liu, S., Fu,\nC.-W., Jia, J.: Guided point contrastive learn-\ning for semi-supervised point cloud semantic\nsegmentation. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision,\npp. 6423–6432 (2021)\n[27] Du, B., Gao, X., Hu, W., Li, X.: Self-contrastive\nlearning with hard negative sampling for self-\nsupervised point cloud learning. In: Proceedings\nof the 29th ACM International Conference on\nMultimedia, pp. 3133–3142 (2021)\n[28] Xu,\nC.,\nLeng,\nB.,\nChen,\nB.,\nZhang,\nC.,\nZhou, X.: Learning discriminative and gener-\native shape embeddings for three-dimensional\nshape retrieval. IEEE Transactions on Multime-\ndia 22(9), 2234–2245 (2019)\n[29] Huang, J., Yan, W., Li, T.H., Liu, S., Li, G.:\nLearning the global descriptor for 3d object\nrecognition based on multiple views decomposi-\ntion. IEEE Transactions on Multimedia (2020)\n[30] Simonovsky, M., Komodakis, N.: Dynamic\nedge-conditioned ﬁlters in convolutional neural\nnetworks on graphs. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern\nRecognition, pp. 3693–3702 (2017)\n[31] Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A.,\nUrtasun, R.: Deep parametric continuous con-\nvolutional neural networks. In: Proceedings of\nthe IEEE Conference on Computer Vision and\nPattern Recognition, pp. 2589–2597 (2018)\n[32] Li, J., Chen, B.M., Hee Lee, G.: So-net: Self-\norganizing network for point cloud analysis. In:\nProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pp. 9397–\n9406 (2018)\n[33] Zhao, H., Jiang, L., Fu, C.-W., Jia, J.: Pointweb:\nEnhancing local neighborhood features for point\ncloud processing. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern\nRecognition, pp. 5565–5573 (2019)\n[34] Xie, S., Liu, S., Chen, Z., Tu, Z.: Attentional\nshapecontextnet for point cloud recognition. In:\nProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pp. 4606–\n4615 (2018)\n[35] Fujiwara, K., Hashimoto, T.: Neural implicit\nThe Visual Comptuer 2022\n17\nembedding for point cloud analysis. In: Proceed-\nings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11734–\n11743 (2020)\n[36] Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S.:\nPointasnl: Robust point clouds processing using\nnonlocal neural networks with adaptive sam-\npling. In: Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recogni-\ntion, pp. 5589–5598 (2020)\n[37] Qiu, S., Anwar, S., Barnes, N.: Geometric back-\nprojection network for point cloud classiﬁcation.\nIEEE Transactions on Multimedia (2021)\n[38] Chen, C., Qian, S., Fang, Q., Xu, C.: Hapgn:\nHierarchical attentive pooling graph network for\npoint cloud segmentation. IEEE Transactions on\nMultimedia (2020)\n[39] Liu, H., Guo, Y., Ma, Y., Lei, Y., Wen, G.:\nSemantic context encoding for accurate 3d point\ncloud segmentation. IEEE Transactions on Mul-\ntimedia (2020)\n[40] Rao, Y., Lu, J., Zhou, J.: Global-local bidirec-\ntional reasoning for unsupervised representation\nlearning of 3d point clouds. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 5376–5385 (2020)\n[41] Zhang, M., You, H., Kadam, P., Liu, S., Kuo, C.-\nC.J.: Pointhop: An explainable machine learn-\ning method for point cloud classiﬁcation. IEEE\nTransactions on Multimedia 22(7), 1744–1755\n(2020)\n[42] Xie, S., Gu, J., Guo, D., Qi, C.R., Guibas,\nL., Litany, O.: Pointcontrast: Unsupervised pre-\ntraining for 3d point cloud understanding. In:\nEuropean Conference on Computer Vision, pp.\n574–591 (2020). Springer\n[43] Chen, T., Kornblith, S., Norouzi, M., Hinton,\nG.: A simple framework for contrastive learning\nof visual representations. In: International Con-\nference on Machine Learning, pp. 1597–1607\n(2020). PMLR\n[44] Oord, A.v.d., Li, Y., Vinyals, O.: Representa-\ntion learning with contrastive predictive coding.\narXiv preprint arXiv:1807.03748 (2018)\n[45] Gadelha, M., Wang, R., Maji, S.: Multiresolu-\ntion tree networks for 3d point cloud processing.\nIn: Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 103–118 (2018)\n[46] Han, Z., Shang, M., Liu, Y.-S., Zwicker, M.:\nView inter-prediction gan: Unsupervised repre-\nsentation learning for 3d shapes by learning\nglobal shape memories to support local view\npredictions. In: Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 33, pp.\n8376–8384 (2019)\n[47] Chang, A.X., Funkhouser, T., Guibas, L., Han-\nrahan, P., Huang, Q., Li, Z., Savarese, S.,\nSavva, M., Song, S., Su, H., et al.: Shapenet:\nAn information-rich 3d model repository. arXiv\npreprint arXiv:1512.03012 (2015)\n[48] Yi, L., Kim, V.G., Ceylan, D., Shen, I.-C., Yan,\nM., Su, H., Lu, C., Huang, Q., Sheffer, A.,\nGuibas, L.: A scalable active framework for\nregion annotation in 3d shape collections. ACM\nTransactions on Graphics (ToG) 35(6), 1–12\n(2016)\n[49] Armeni, I., Sax, S., Zamir, A.R., Savarese,\nS.: Joint 2d-3d-semantic data for indoor scene\nunderstanding. arXiv preprint arXiv:1702.01105\n(2017)\n[50] Klokov, R., Lempitsky, V.: Escape from cells:\nDeep kd-networks for the recognition of 3d point\ncloud models. In: Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pp.\n863–872 (2017)\n[51] Shen, Y., Feng, C., Yang, Y., Tian, D.: Mining\npoint cloud local structures by kernel correla-\ntion and graph pooling. In: Proceedings of the\nIEEE Conference on Computer Vision and Pat-\ntern Recognition, pp. 4548–4557 (2018)\n[52] Thomas, H., Qi, C.R., Deschaud, J.-E., Mar-\ncotegui, B., Goulette, F., Guibas, L.J.: Kpconv:\nFlexible and deformable convolution for point\nclouds. In: Proceedings of the IEEE International\nConference on Computer Vision, pp. 6411–6420\n(2019)\nThe Visual Comptuer 2022\n[53] Atzmon, M., Maron, H., Lipman, Y.: Point con-\nvolutional neural networks by extension oper-\nators. ACM Transactions on Graphics (TOG)\n37(4), 1–12 (2018)\n[54] Liu, X., Han, Z., Liu, Y.-S., Zwicker, M.:\nPoint2sequence: Learning the shape representa-\ntion of 3d point clouds with an attention-based\nsequence to sequence network. In: Proceedings\nof the AAAI Conference on Artiﬁcial Intelli-\ngence, vol. 33, pp. 8778–8785 (2019)\n[55] Hassani, K., Haley, M.: Unsupervised multi-task\nfeature learning on point clouds. In: Proceed-\nings of the IEEE International Conference on\nComputer Vision, pp. 8160–8171 (2019)\n[56] Huang, Q., Wang, W., Neumann, U.: Recur-\nrent slice networks for 3d segmentation of point\nclouds. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition,\npp. 2626–2635 (2018)\n[57] Feng, Z., Zhou, Q., Gu, Q., Tan, X., Cheng,\nG., Lu, X., Shi, J., Ma, L.: Dmt: Dynamic\nmutual training for semi-supervised learning.\narXiv preprint arXiv:2004.08514 (2020)\n",
  "categories": [
    "cs.CV",
    "cs.CG"
  ],
  "published": "2021-10-13",
  "updated": "2023-01-24"
}