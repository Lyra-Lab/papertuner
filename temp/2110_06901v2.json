{
  "id": "http://arxiv.org/abs/2110.06901v2",
  "title": "A Survey on Deep Learning for Skeleton-Based Human Animation",
  "authors": [
    "L. Mourot",
    "L. Hoyet",
    "F. Le Clerc",
    "François Schnitzler",
    "Pierre Hellier"
  ],
  "abstract": "Human character animation is often critical in entertainment content\nproduction, including video games, virtual reality or fiction films. To this\nend, deep neural networks drive most recent advances through deep learning and\ndeep reinforcement learning. In this article, we propose a comprehensive survey\non the state-of-the-art approaches based on either deep learning or deep\nreinforcement learning in skeleton-based human character animation. First, we\nintroduce motion data representations, most common human motion datasets and\nhow basic deep models can be enhanced to foster learning of spatial and\ntemporal patterns in motion data. Second, we cover state-of-the-art approaches\ndivided into three large families of applications in human animation pipelines:\nmotion synthesis, character control and motion editing. Finally, we discuss the\nlimitations of the current state-of-the-art methods based on deep learning\nand/or deep reinforcement learning in skeletal human character animation and\npossible directions of future research to alleviate current limitations and\nmeet animators' needs.",
  "text": "Published in COMPUTER GRAPHICS Forum (CGF)\nhttps://doi.org/10.1111/cgf.14426\nA Survey on Deep Learning for Skeleton-Based Human Animation\nL. Mourot1,2\n, L. Hoyet1\n, F. Le Clerc2\n, F. Schnitzler2\nand P. Hellier2\n1Inria, Univ Rennes, CNRS, IRISA\n2InterDigital, Inc\nAbstract\nHuman character animation is often critical in entertainment content production, including video games, virtual reality or\nﬁction ﬁlms. To this end, deep neural networks drive most recent advances through deep learning and deep reinforcement\nlearning. In this article, we propose a comprehensive survey on the state-of-the-art approaches based on either deep learning\nor deep reinforcement learning in skeleton-based human character animation. First, we introduce motion data representations,\nmost common human motion datasets and how basic deep models can be enhanced to foster learning of spatial and temporal\npatterns in motion data. Second, we cover state-of-the-art approaches divided into three large families of applications in human\nanimation pipelines: motion synthesis, character control and motion editing. Finally, we discuss the limitations of the current\nstate-of-the-art methods based on deep learning and/or deep reinforcement learning in skeletal human character animation and\npossible directions of future research to alleviate current limitations and meet animators’ needs.\nCCS Concepts\n• General and reference →Surveys and overviews; • Applied computing →Media arts; • Computing methodologies →Mo-\ntion processing; Physical simulation; Neural networks; Supervised learning; Unsupervised learning; Reinforcement learning;\n1. Introduction\nHumans and their representations are ubiquitous in culture. In the\npast decades digital technologies brought more and more tools to\nassist designers, animators and artists, with the goal of increasing\ntheir creative capabilities and the realism of their artworks while re-\nducing production costs. For instance, digital doubles have brought\nto life non-human ﬁctional creatures like the Na’vi in Avatar or\neven visual identity of long time deceased actors like Grand Moff\nTarkin in Rogue One in 2016 portrayed by the British actor Peter\nCushing, even though he passed away in 1994. Within this context,\ncomputer-assisted human character animation plays a central role,\nsuch as for the synthesis of the motion of digital doubles. Another\nexample is the video game production, where animation and con-\ntrol of the characters directly condition the success of a game.\nHowever, animation of human characters is challenging: the way\nhumans move is very diverse and is inﬂuenced by many factors in-\ncluding the mood, the intentions, the activity or even individual\ncharacteristics. In addition, Newton’s second law of motion inher-\nently makes human motion a dynamic process, while our under-\nstanding of biomechanics is far from being comprehensive. In prac-\ntice, compromises therefore need to be made to balance between\nproduction costs, the realism, the amount of manual work, and the\nlevel of expertise of the animator. The research area of character\nanimation has been active for decades to mitigate these compro-\nmises and make animation more accessible, starting from pioneer-\ning works such as editing and deforming motion examples [WP95],\nretargeting motions to new characters [Gle98], controlling charac-\nters using motion graphs [KGP02; MC12], etc.\nOver the past few years, Deep Neural Networks (DNNs) have\nemerged as a powerful means to enhance the performance and ca-\npabilities of character animation, as evidenced by the growing num-\nber of publications on the topic leveraging Deep Learning (DL) and\nDeep Reinforcement Learning (DRL) (see Figure 1). These two\ntechniques have shown an unprecedented ability to address com-\nplex tasks in a wide variety of domains not restricted to animation,\nsuch as computer vision, Natural Language Processing (NLP) and\nmany more. Humans are outperformed by artiﬁcial intelligence al-\ngorithms in a growing number of tasks such as image classiﬁca-\ntion or playing Go. This is notably due to the fact that DNNs are\npowerful function approximators, able to learn sophisticated pat-\nterns in complex real-world phenomena from data. Moreover, once\nthe training is complete, training data is discarded, leaving compact\nmodels that are able to meet performance requirements of real-time\napplications or to scale to embedded systems. In animation, DL &\nDRL based approaches attempt to handle the human motion com-\nplexity and provide promising perspectives for cheaper and faster\nanimation techniques with more and more ﬁdelity and creative ca-\npabilities.\nIn this article, we present an overview of the recent growing\ntrend of DL & DRL in skeletal character animation, focused on hu-\nmanoid characters. Skeletal here means using a representation de-\nrived from a skeleton, as commonly used in the movie and game in-\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n1\nhttps://doi.org/10.1111/cgf.14426\narXiv:2110.06901v2  [cs.GR]  23 Nov 2021\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n0\n5\n10\n15\n20\n25\n30\n35\n40\nDL-only\nDRL-only\nDL & DRL\nFigure 1: Histogram of the peer-reviewed publications over the\npast decade in human skeletal character animation using Deep\nLearning (DL) and/or Deep Reinforcement Learning (DRL) ad-\ndressed in this survey.\ndustries in combination with 3D skinned meshes (see Section 2.1).\nOn the one hand, DL is particularly effective at building compact\nmodels from such human motion data, e.g. for motion synthesis\nand editing. On the other hand, DRL, which is concerned with how\nagents ought to act in a simulated environment in order to maximise\nsome cumulative reward, is well-suited for character control where\ncharacters are agents, and their actuation model parameters are ac-\ntions. The goal of this review is therefore to provide researchers\ninvolved in character animation and related ﬁelds with an overview\nof existing DL & DRL based methods in that domain, best practices\nto represent and process motion data with DNNs and the most suc-\ncessful approaches for different well-studied families of problems.\nIn the last decade, various state-of-the-art reports have studied\ncharacter animation topics: Pejsa and Pandzic [PP10] covered mo-\ntion planning, motion graphs and parametric models for motion\nsynthesis in interactive applications from examples; Geijtenbeek\nand Pronost [GP12] reviewed the literature on physical simula-\ntion for interactive character animation; then Karg et al. [KSG*13]\nstudied the generation and recognition of motions based on af-\nfective expressions; ﬁnally, Wang et al. [WCW14] presented an\nanalysis of state-of-the-art techniques in 3D human motion synthe-\nsis while focusing on motion capture data-driven methods. How-\never, none of these reviews covered DL or DRL based methods\nbecause most recent advances appeared since 2015, as shown in\nFigure 1. More recently, Alemi and Pasquier [AP19] reviewed the\ntopic of data-driven movement generation with Machine Learning\n(ML). While their paper includes the review of some DL-based\nmethods, numerous other advances have emerged in very recent\nyears. In addition, some of the ML approaches presented by Alemi\nand Pasquier [AP19], such as Hidden Markov Models or Principal\nComponent Analysis, are clearly outperformed by DL as of today.\nIn contrast, our work addresses state-of-the-art of DL and DRL in\nskeletal human character animation.\nThis paper is organised as follows: in Section 2, we begin with an\noverview of low-level concerns encountered when processing hu-\nman motion data with DNNs, presenting pose representations (Sec-\ntion 2.1) and human motion datasets (Section 2.2) frequently used\nin the literature, as well as how to efﬁciently and successfully learn\nspatial (Section 2.3) and temporal (Section 2.4) features.\nNext, Section 3 covers motion synthesis, that we deﬁne as the\nprocess of creating perceptually plausible motion sequences with a\ndesired style or expressed emotion for instance. Motion synthesis\nmodels are thus capable of generating different motions depending\non inputs, including low-level parameters (e.g., latent variables),\nhigh-level parameters (e.g. trajectory or speciﬁc action to perform)\nor historical parameters (e.g. past motions to be extrapolated). In\nthis review, we divide motion synthesis approaches into three cate-\ngories: predictive in the short-term (Section 3.1), predictive in the\nlong-term (Section 3.2) and generative (Section 3.3). The ﬁrst fo-\ncuses on deterministically synthesising motion segments from few\npast frames while the second tries to generate longer plausible mo-\ntion continuations. The third category covers motion synthesis from\nother types of parameters, such as the trajectory to follow, and relies\non generative models.\nSection 4 deals with the task of controlling the motion of char-\nacters so that they react naturally to user inputs while accounting\nfor environment and biomechanical constraints. We divide these\napproaches into kinematic (Section 4.1), physical (Section 4.2)\nand biomechanical (Section 4.3) control. Kinematic approaches di-\nrectly produce motions as joint positions or angles. In contrast, both\nphysical and biomechanical approaches strive to obey the laws of\nphysics, while differing in the actuation model: physical models are\nactuated by forces and torques, while biomechanical models (a.k.a.\nmusculoskeletal models) are driven by muscle activations.\nFinally, Section 5 gathers motion editing approaches at large, i.e.\nmethods aiming to process or transform some aspects of existing\nmotion data. Motion cleaning (Section 5.1) improves motion data,\ne.g. by removing noise or ﬁlling in missing information such as\nmarker or joint positions. Note that we distinguish here markers\nand joints completion from motion prediction and in-betweening\nthat are addressed in Sections 3.1 and 3.2 respectively, although all\nthree might be formulated as completion from partial observations.\nThen, retargeting (Section 5.2) strives to transfer the motion from\na source character to a target character, while motion style transfer\n(Section 5.3) edits the style of a motion segment while preserving\nthe action performed and the character.\n2. Human Motion Representation, Data and Modelling\nIn both DL and DRL, choices of input and output spaces for DNNs\nare often impactful on the effectiveness of the learning and on what\nspeciﬁc aspects of the data will be retained. When dealing with\nhuman motion, the pose representation mainly determines these\ninput and output spaces. Moreover in DNNs, the computational\nworkﬂow can be structured around spatial and temporal aspects of\nmotion. In this section we explore the different human pose repre-\nsentations commonly encountered in deep animation and their key\nstrengths and weaknesses (Section 2.1), commonly used datasets\n(Section 2.2), as well as DNN architectures structured with respect\nto spatial (Section 2.3) and temporal (Section 2.4) domains.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n2\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n2.1. Pose Representation\nTraditional animation approaches typically use 3D rigged skele-\ntons with skinned meshes, which provides a good trade-off be-\ntween quality and complexity. Rigging offers convenient ways to\nmanipulate 3D models as strings do for a puppet, while skinning\nis the process of binding actual 3D meshes to animated characters.\nIn that framework, human motions are usually represented as se-\nquences of poses separated by constant time intervals whose rate\ngenerally ranges from 30 to 250 hertz. At each time step, the state\nof the human body is then represented as a set of links (i.e. bones)\nconnected by joints. This skeletal representation is a good compro-\nmise for the complexity and the diversity of human movements that\ncan be represented. When bone lengths are kept constant over time,\nthe Degrees of Freedom (DOFs) are the orientations of the bones,\ncommonly expressed relative to their parent. In the following, we\ncall such a representation an angular pose representation, as op-\nposed to a positional pose representation where the skeleton DOFs\nare the coordinates of the joint positions, which does not explicitly\nconstrain bone lengths to remain constant over time.\n2.1.1. Positional Pose Representations\nIn a positional pose representation, each joint is directly repre-\nsented by its position, generally expressed at each time step in\nthe body’s local coordinate system [HSKJ15; HSK16; WHSZ19;\nHAB20; DHS*19; HHS*17; ZLX*18; HHKK17; SCNW19;\nTCHG17; KAS*20], which allows the decomposition of the whole\nmotion into the local movements of limbs with respect to the body\nitself and the global movement of the body with respect to its en-\nvironment. Although different coordinate systems could be formu-\nlated to embed the set of joint positions, positional pose representa-\ntions almost always rely on the Cartesian coordinate system. It has\nneither discontinuities nor singularities and constitutes a convenient\nspace for interpolation, visualisation and optimisation. Moreover,\nwithin the framework described here, positional pose representa-\ntion does not present some of the limitations inherent to angular\nrepresentations presented in the following section. However, it suf-\nfers from some limitations related to the structure of human mo-\ntions. For instance, joint positions do not encode the information\nof bone orientations around themselves which is often needed for\nconcrete applications in animation, in order to display more natural\nmesh deformations. Positional representations also do not explic-\nitly constrain bone lengths to remain constant over time, therefore\nrequiring reliance on additional constraints to ensure that the skele-\nton does not break apart. For these reasons, communities closer\nto animation, such as computer graphics, rarely use purely posi-\ntional pose representations, while on the opposite, communities\nmore commonly involved in deep learning, such as computer vi-\nsion, are more prone to employing these representations.\n2.1.2. Angular Pose Representations\nAngular representations have been widely used in animation\nmainly because their hierarchical nature allows straightforward ori-\nentation of any joint together with all of its descendants, while\nkeeping bone lengths constant. Indeed, the position of each joint\nis described with respect to its parent as a 3D rigid transformation,\noften decomposed into a variable rotation and a ﬁxed translation,\ncorresponding to the joint orientation and the bone dimensions re-\nspectively. Main differences among angular pose representations\nare determined by the parameterisation of the rotations, however\nthere are also representations working at the level of rigid transfor-\nmation parameterisations.\nFormally, the set of all rotations of R3 equipped with the com-\nposition is the 3D rotation group often denoted SO(3), standing for\nspecial orthogonal group of dimension 3. SO(3) can be identiﬁed\nwith the group of orthogonal 3×3 matrices with determinant 1 un-\nder the matrix multiplication. Similarly, the 3D special Euclidean\ngroup whose elements are proper 3D rigid transformations (i.e. ex-\ncluding reﬂections) is SE(3) = SO(3)×R3. Both SO(3) and SE(3)\nare Lie groups, i.e. differentiable spaces that locally resemble Eu-\nclidean space. Furthermore, a Lie algebra is associated to every Lie\ngroup, called so(3) and se(3) for SO(3) and SE(3), respectively.\nLie algebras are vector spaces tangent to their Lie group at the iden-\ntity element completely capturing its local structure, making them\ncompelling as representation spaces.\nEuler Angles. The most intuitive parameterisation of SO(3) is\nprobably Euler angles, that represents a 3D orientation as three\nsuccessive rotations around different axes, e.g. yaw, pitch and roll.\nHowever, it suffers from the well-known gimbal lock when two of\nthe three rotation axes align, causing a DOF to be lost. Gimbal lock\ncan be avoided only if at least one rotation axis is limited to a range\nsmaller than 180◦, which is not always possible in practice. As a\nresult, Euler angles are unsuitable for Inverse Kinematics (IK), dy-\nnamics and spacetime optimisation [Gra98]. Moreover, they do not\nwork well for interpolations since the space of orientations is highly\nnonlinear [Gra98]. Finally, multiple conventions exist for the axes\nconsidered, including their order, which requires to deﬁne them for-\nmally in each application to avoid any ambiguity. For these reasons,\nthis representation is inappropriate for a lot of applications.\nLie Algebras. A popular angular pose representation in skeletal\ncharacter animation consists in representing each joint rotation as\nan element of so(3), where the direction and magnitude of the vec-\ntor correspond to the axis and angle of the rotation [Gra98], respec-\ntively. Since such a vector is an element of so(3), the parameterisa-\ntion is deﬁned by the exponential map from so(3) to SO(3) which\ncan be efﬁciently computed with the Rodrigues’ formula [Rod40].\nThis pose representation is often called exponential map, and is\nsometimes confused with the so-called axis-angle representation\nthat is equivalent but separates the vector into a unit vector and a\nscalar describing the axis and the magnitude of the rotation.\nSince Lie algebras are locally linearised versions of their Lie\ngroup, so(3) is a compelling space to work with elements of SO(3).\nHowever, as all parameterisations of SO(3) in R3, the exponen-\ntial map representation has singularities [Gra98] leading to losing\na DOF in some parts of the representation space, even though these\nare located on the spheres of radius 2kπ for k ∈N+, since a rota-\ntion of 2π about any axis is equivalent to no rotation. Therefore,\nthis representation is often well-suited in animation since control\nand simulation deal with small time steps and thus with small ro-\ntations that stay inside the sphere of radius 2π, far from the sin-\ngularities. It has been employed in early works in deep anima-\ntion [THR06; TH09], and broadly exploited for motion synthe-\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n3\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nsis [CM11; ALP15] and prediction [FLFM15; JZSS16; BBKK17;\nMBR17; LZLL18; TMLZ18; GWLM18; GMK*19; LWY*19;\nGC19; KGB19; WAC*19; CSY20; MLS20; LCZ*20; CSK*21;\nLCC*21], as well as in other topics [HKS17; AP17; MSZ*18;\nJL20]. However, as quality needs increase, long-term correlations\nare more and more important and inevitably imply larger rotations,\ngetting close to the singularities of the parameterisation.\nSimilarly\nto\nthe\nexponential\nmap\nrepresentation\nwhich\nuses so(3) to represent joint orientations w.r.t. their parents,\nLiu et al. [LWJ*19] proposed a pose representation using se(3) to\nrepresent rigid transformations of each joint w.r.t. its parent. The\nmain motivation for choosing such a representation is to explicitly\nencode both geometric constraints (i.e. bone lengths) and actual\nDOFs (i.e. joint orientations) together. Nevertheless, it still has\nthe same singularities as the exponential map representation. As\nwe will see in the following paragraphs, other parameterisations\nin higher-dimensional spaces than R3, i.e. over-parameterised\nrepresentations, are able to prevent such singularities.\nRotation Matrices. In computer graphics, rotation matrices are\nwidely used to represent 3D rotations. The corresponding param-\neterisation is the identity since elements of SO(3) are 3 × 3 ma-\ntrices. Rotation matrices have no singularity and can be integrated\ntogether with joint translations into a 4 × 4 homogeneous matrix,\nwhich is elegant and effective when involved in computations like\ncomposition or inverse. However, such a representation is particu-\nlarly difﬁcult to work with when its parameters must be estimated\nsince the representation is over-parameterised. Indeed, not all 3×3\nmatrices belong to SO(3). By deﬁnition, a matrix R ∈R3×3 must\nsatisfy R⊤R = I and det(R) = 1 to be a valid 3D rotation. For in-\nstance, predicting the orientation of a joint in matrix representation\nwould require to solve a constrained optimisation problem to en-\nsure the validity of the rotation, which can be tedious.\nUnit Quaternions. A more compact representation than rota-\ntion matrices are unit quaternions. Lying in R4, they are free\nof singularities, suitable for interpolation [Gra98], numerically\nstable and computationally efﬁcient [PGA18]. Like so(3), the\nspace of unit quaternions has the same local geometry and topol-\nogy as SO(3) [Gra98]. However, unit quaternions are also over-\nparameterised, but have only four parameters (in comparison to\nnine parameters for rotation matrices). Thus, they must be con-\nstrained to remain on the unit 4-sphere. This angular representa-\ntion has been popularised in DL-based animation with QuaterNet,\na quaternion-based framework for human motion prediction pro-\nposed by Pavllo et al. [PGA18; PFAG20] (see Section 3.1). In that\nframework, Pavllo et al. introduced a penalty term in the loss func-\ntion for all quaternions predicted by the network that minimises\ntheir divergence from the unit length. It encourages the network to\npredict valid rotations and leads to better training stability. More-\nover, the predicted quaternions are also normalised after computing\nthe penalty to enforce their validity. According to the authors, the\ndistribution of predicted quaternion norms converges to a Gaussian\nwith mean 1 during the training, suggesting that the model actu-\nally learns to represent valid rotations. Since Pavllo et al. [PGA18]\nshowed promising results using quaternions, their use is gain-\ning popularity [KPKH20; AWL*20; ASS*20; ALL*20; PFAG20;\nLCC19; VYCL18; HYNP20; GWE*20].\nGram-Schmidt-like. Zhou et al. [ZBL*19] recently pointed out\nthat all representations in Rn with n ≤4 have discontinuities which\ncan be unfavorable for DNNs training. They therefore introduced\na continuous representation of n-dimensional rotations SO(n) with\nn2 −n dimensions. The mapping from SO(n) to the representation\nspace simply drops the last column vector of the input n × n ma-\ntrix. The inverse mapping back to SO(n), called Gram-Schmidt-\nlike process, is a Gram-Schmidt process over the n −1 column\nvectors followed by the computation of the last column vector by\na generalisation to n dimensions of the cross product. In the case\nof SO(3), this Gram-Schmidt-like representation gives a 6D repre-\nsentation. Zhou et al. [ZBL*19] also provided a method to further\nreduce the dimensionality from 6D to 5D while still keeping a con-\ntinuous representation using a stereographic projection combined\nwith normalisation. However, they empirically found that nonlin-\nearities introduced by the projection can make the learning process\nmore difﬁcult. To the best of our knowledge, very few works have\ninvestigated this promising representation of 3D rotations.\nFigure 2: Illustration of the error accumulation problem with an-\ngular representations: even small angular errors along the kine-\nmatic chain can lead to large accumulated joint positioning errors\n(left-hand stick ﬁgure, see color gradient). This is problematic in\noptimization-based methods, e.g. DL/DRL, when penalizing joint\norientation deviations. This is not the case with positional represen-\ntations (right-hand stick ﬁgure) where joint positions are directly\noptimized.\nHierarchical Representation Limitations. In skeletal character\nanimation, a hierarchical modelling approach is used in conjunc-\ntion with angular pose representations, i.e. the representation of\nthe joint orientations relative to their parents. In that context, posi-\ntional errors over proximal joints (e.g. the shoulder) are propagated\nand accumulated down the kinematic chains. This is problematic in\noptimisation-based methods such as DL or DRL since equally dis-\ntributed joint orientation errors will result in growing joint position\nerrors along the kinematic chains as depicted in Figure 2, making\nit difﬁcult to accurately handle end effectors. This is especially true\nin motions sequences involving fast or ample movements, e.g. run-\nning.\nTo solve this problem, Pavllo et al. [PGA18; PFAG20] performed\nForward Kinematics (FK) to convert quaternion-based poses pre-\ndicted by their DNN into 3D joint positions, and then penalised\nabsolute position errors instead of angular errors. Since FK is a\ndifferentiable operation with respect to joint orientations, they can\ntrain their network end-to-end using a positional loss.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n4\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nWhile recent works have followed the same approach [ASS*20;\nALL*20], Ghorbani et al. [GWE*20] recently pointed out that ap-\nplying FK is computationally expensive especially for motion se-\nquences that are long or involve numerous joints. To this end, they\nproposed to hierarchically weight joint angle errors based on their\nimpact on the positions. They set the weight of a joint as its max-\nimum path length down to all of the connected end effectors in\nan average skeleton. While ablative studies showed improved per-\nformances when joint weighting was enabled, the results were not\ncompared to Pavllo et al.’s FK-based approach [PGA18; PFAG20],\nneither were the choice of the weights assessed.\n2.1.3. Hybrid Representations\nAs mentioned above, both angular and positional approaches for\nrepresenting human poses have advantages and drawbacks that\nsometimes depend on the application or viewpoint, dividing re-\nsearcher communities. For this reason, several works have pro-\nposed hybrid representations with the goal of mitigating drawbacks\nwhile keeping beneﬁts of both types of representations.\nAberman et al. [ALL*20] presented a novel data-driven ap-\nproach for retargeting motions between homeomorphic skeletons\n(see Section 5.2) along with an interesting and elegant represen-\ntation of human motion illustrated in Figure 3. In this work, both\nangular and positional information are combined: a static compo-\nnent S consisting of a set of 3D positional offsets describes the\nskeleton in some arbitrary pose (similar to a T-pose but speciﬁc to\na pose sequence), while a dynamic component Q speciﬁes the se-\nquence of orientations of each joint along time (with respect to S)\nrepresented using unit quaternions. The separation between static\nand dynamic partial representations enables the authors to design\nan architecture such that each component is processed in a separate\nbranch. In continuation of Pavllo et al.’s work [PGA18; PFAG20],\nAberman et al. [ALL*20] penalised errors in the positional space\nafter performing FK, while also penalising errors in the quater-\nnion space. Following this work, Shi et al. [SAA*20] addressed\nthe reconstruction of 3D kinematic skeletons from 2D keypoints\nestimated from monocular video while dividing the motion repre-\nsentation into static bone lengths and dynamic joint orientations.\nTo this end, a DNN called MotioNet learns to map 2D keypoints to\na symmetric static skeleton, represented by its bone lengths and a\ndynamic sequence of joint rotations (quaternions) which are then\ncombined through FK to get a full kinematic skeleton.\nFinally, the success of multiple recent methods in skeletal char-\nacter animation mixing different pose representations suggests that\nDNNs beneﬁt from redundant pose information. In addition to\njoint orientations, researchers fed their models with joint posi-\ntions [LLL18], joint positions and velocities [HKS17; MSZ*18;\nZSKS18; LZCvdP20; SZKZ20; SZKS19] or even joint positions\nand linear and angular velocities [HKPP20].\n2.2. Human Motion Datasets\nAnother crucial aspect of data-driven approaches is the choice of\ndataset, from which a DL-based model will learn a deep repre-\nsentation of human motion. In particular, large amounts of high-\nquality motion data are necessary to constitute so-called benchmark\nFigure 3:\nRepresentation of skeletal motion data as a graph\nin [ALL*20]. The nodes of the graph correspond to joints and the\nedges to armatures. Each of the J armatures holds a time-varying\ntensor Q modelling the temporal sequence of rotations at its cor-\nresponding joint, and a time-independent vector S modelling the\nbone offset to the parent joint. The global motion of the root joint R\nis processed separately. Figure from [ALL*20].\ndatasets and to provide robust assessment procedures. In this sec-\ntion we introduce a selection of human motion datasets that are the\nmost relevant for this survey. Although many datasets have been\nproposed, only a small number of them have been repeatedly ex-\nploited and even fewer can be considered as standard benchmarks\n(see Figure 4). Table 1 provides relevant information about these\ndatasets most commonly used in the works presented in this sur-\nvey.\nThe two most widely used databases in skeleton-based deep hu-\nman animation are CMU [Uni03] and Human3.6M [IPOS14]. Both\nare standard large-scale human motion datasets for learning and\nevaluation. Despite the fact that the CMU dataset was released\nabout a decade earlier than Human3.6M, they have a compara-\nble size (see Table 1). The main advantage of Human3.6M over\nCMU is the presence of RGB+D videos synchronized with human\npose sequences, making it sometimes more suitable for tasks closer\nto computer vision such as motion prediction, even though CMU\nis also often leveraged. To obtain convincing results, several re-\nsearchers provide evaluations of their work over both CMU and\nHuman3.6M data [BBKK17; LZLL18; LWY*19; GC19; KGB19;\nMLSL19; CSY20; LCZ*20; ZPK20; CHW*20; LKS*20; LCC*21;\nCSK*21; BGG*20; ASS*20].\nBeyond these two standard human motion databases, a few oth-\ners are noticeable, e.g. for the types of motion they contain, for\nthe annotations that are included, or even for the environment\nin which they were captured. A few years after the release of\nCMU, Müller et al. [MRC*07] proposed HDM05, a public well-\ndocumented database of systematically recorded motion capture\ndata. Complementary to CMU which contains a large number of di-\nverse motion sequences, HDM05 is composed of a limited number\nof speciﬁc motion sequences (one hundred) which were executed\nfrom 10 to 50 times by ﬁve actors. As an example, a cartwheel\nstarting with the left hand has been performed 21 times. More re-\ncently, Shahroudy et al. [SLNW16] proposed NTU RGB+D, one\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n5\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nTable 1: Summary of the main datasets presented in Section 2.2.\nDataset\nURL\nSize\nAvailability\nJoints\nRepresentation\nMiscalleneous\nCMU [Uni03]\n</>\n3.9 ∗106 frames @ 120 Hz\n→\n9.1 h\n29\nangular\nPublic\nHDM05 [MRC*07]\n</>\n3.6 ∗105 frames @ 120 Hz\n→\n0.8 h\n31\nangular\nPublic\nHuman3.6M [IPOS14]\n</>\n3.6 ∗106 frames @ 50 Hz\n→20.0 h\n32\nangular\nRGB+D\nOn request\nHolden et al. [HSK16]\n</>\n6.0 ∗106 frames @ 120 Hz\n→13.9 h\n21\npositional\nPublic\nNTU RGB+D [SLNW16]\n</>\n4.0 ∗106 frames @ 30 Hz\n→37.0 h\n25\npositional\nRGB+D+IR\nOn request\nNTU RGB+D 120 [LSP*20]\n</>\n8.0 ∗106 frames @ 30 Hz\n→74.1 h\n25\npositional\nRGB+D+IR\nOn request\n3DPW [vMHB*18]\n</>\n5.1 ∗104 frames @ 30 Hz\n→\n0.5 h\n23\nangular\nRGB\nPublic\nAMASS [MGT*19]\n</>\n1.8 ∗107 frames @ [60,250] Hz →41.5 h\n52\nangular\nBody mesh\nPublic\nMixamo [Ado]\n</>\n2.7 ∗105 frames @ 30 Hz\n→\n2.5 h\n52\nangular\nBody mesh\nPublic\nData\nHuman3.6M [IPOS14]\nCMU [Uni03]\nHolden et al. [HSK16]\nNTU RGB+D [SLNW16]\n3DPW [vMHB*18]\nHDM05 [MRC*07]\nMixamo [Ado]\nXia et al. [XWCC15]\nSFU [Uni19]\nAMASS [MGT*19]\nAlemi et al. [ALP15]\nHolden et al. [HKS17]\nHsu et al. [HPP05]\nEmilya [FP14]\nMHAD [OCK*13]\nHabibie et al. [HHS*17]\nDeepCap [HXZ*20]\nGTA-IM [CGM*20]\nHumanEva [SBB09]\nIkea FA [TCHG17]\nKinder-Gator 2.0 [DAS*20]\nLaFAN1 [HYNP20]\nMADS [ZLZ*17]\nMPI-INF-3DHP [MRC*17]\nMSR Action 3D [LZL10]\nPenn Action [ZZD13]\nPoseTrack [AII*18]\nPROX [HCTB19]\nWBHM [MTD*15]\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nFigure 4: Histogram of the number of papers using a given dataset\namong the works in skeleton-based deep human animation covered\nin this survey.\nof the largest datasets for 3D skeleton-based action recognition. It\ncontains 60 action classes as well as RGB and infrared (IR) videos\nand depth map sequences (D) synchronized with motion sequences.\nLater on, Liu et al. [LSP*20] added another 60 classes to constitute\nNTU RGB+D 120, roughly doubling the size of the dataset. How-\never, the motion sequences in these two datasets are represented\nonly by joint positions which limits their use in skeleton-based hu-\nman animation. Since motion capture systems need dedicated en-\nvironment e.g. for a multicamera setup, human motion datasets are\nmostly captured in the lab, resulting in a lack of in the wild data.\nTo this end, von Marcard et al. [vMHB*18] proposed a pose esti-\nmation method, leveraging inertial measurement units in addition\nto a hand-held camera, accurate enough to capture a new dataset\ncalled 3DPW consisting of human pose sequences in the wild syn-\nchronized with RGB videos. It contains challenging sequences in-\ncluding walking in the city, going up-stairs, or taking the bus for a\ntotal of more than 51000 frames.\nA special need of data occurs from the use-case of retargeting\n(see Section 5.2), which consists in transferring the movements of\na character to another one with a different morphology, i.e. motion\ndata with diversity among morphologies. Unfortunately, most hu-\nman motion datasets feature only a very limited number of subjects\nwith minor morphological differences. As a result the so-called\nMixamo Dataset [Ado] is often used to train or evaluate models\nfor retargeting. Indeed Mixamo is a computer graphics technol-\nogy company developing services for 3D character animation in-\ncluding an online animation store with downloadable animation se-\nquences performed by numerous 3D character models with varied\nmorphologies. This diversity of character models is crucial to the\ntask of retargeting.\nFinally, existing datasets were also gathered to build larger\nhuman motion databases. Holden et al. [HSK16] constructed\na dataset by collecting CMU [Uni03], HDM05 [MRC*07],\nMHAD [OCK*13] and Xia et al. [XWCC15] in addition to inter-\nnal motion capture sequences. This data was retargeted to a uniform\nskeleton structure and resampled to 120 frames per second. More\nrecently, Mahmood et al. [MGT*19] proposed AMASS, which\nuniﬁes 15 different optical marker-based motion capture datasets\n(including CMU [Uni03], HDM05 [MRC*07], SFU [Uni19], Hu-\nmanEva [SBB09]). The size of AMASS, initially around 42 hours\nof data, is still increasing. Motion sequences in AMASS are\nparameterised using the Skinned Multi-Person Linear (SMPL)\nmodel [LMR*15], a learnt model of human body shape and pose\nthat provides a parameter space from which the skeleton, the joint\norientations and the body mesh can be computed.\n2.3. Learning Spatial Features\nBesides the pose representation and the dataset, the network archi-\ntecture can also have a signiﬁcant impact on the deep representation\nlearned. In this section, we present different types of architectures\nleveraged to learn spatial correlations in motion data. While most\nDNNs designed to address tasks related to skeletal character ani-\nmation do not exploit the prior knowledge we have about geometric\nand structural aspects of the human skeleton, e.g. its symmetry or\nits hierarchical structure, a few methods proposed various clever ar-\nchitectures to beneﬁt from this prior knowledge. We present them in\nthe following sub-sections, divided into three categories: spatially-\nstructured architectures, Convolutional Neural Networks (CNNs)\nand Graph Convolutional Networks (GCNs).\n2.3.1. Spatially-Structured Architectures\nA ﬁrst group of approaches to help DNNs learn spatial correlations\nrely on network architectures structured around the human skele-\nton, such that the function computed by the network intrinsically\nencodes human skeleton characteristics. These approaches split the\nskeleton into body parts and process the corresponding data either\nin parallel network branches or hierarchically.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n6\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nIn parallel approaches, the main difference is usually related to\nthe targeted task, which conditions the architecture of individual\nbranches. Wang and Neff [WN15] extracted deep motion signa-\ntures with an independent autoencoder for each branch (i.e. limb\nor torso) and concatenated the outputs. Guo and Choi [GC19] re-\nlied instead on fully connected layers for each branch, whose out-\nputs are merged using a shared layer to predict the next frame.\nNakada et al. [NZC*18] similarly divided the body into sep-\narate modules responsible for controlling muscle activations of\nbody parts from preprocessed common visual information. Finally,\nJain et al. [JZSS16] predicted future poses with one Recurrent Neu-\nral Network (RNN) per body part, whose inputs are the neighbour-\ning RNNs predictions at the previous timestamps as well as their\nown previous predictions.\nHierarchical approaches can be either top-down or bottom-\nup. Wang et al. [WHSZ19] proposed a spatial encoder that sep-\narates the human pose into ﬁve body parts, then encodes and\nmerges them two by two recursively. Both Li et al. [LWY*19]\nand Bütepage et al. [BBKK17] used a similar top-down approach\nwith a ﬁner human pose split at the input. In contrast, Ak-\nsan et al. [AKH19] proposed a bottom-up scheme where the hu-\nman pose is predicted step by step from the root joint (e.g. pelvis)\nto the end effectors, i.e. the root is ﬁrst predicted and then the other\njoints are recursively predicted using neighbouring predictions as\nadditional inputs.\n2.3.2. Convolutional Neural Networks\nAnother type of architecture sometimes employed to model spa-\ntial correlations relies on 2D convolutions, i.e. in the spatial and\ntemporal domains. To this purpose, the skeleton graph is ﬂat-\ntened along the spatial dimension. CNNs are particularly efﬁcient\nat learning spatial correlations in data whose structure is regular\nsuch as images. However, learning the spatio-temporal dynamics\nof human joints remains challenging with CNNs because the graph\nstructure of the human skeleton cannot be meaningfully ﬂattened\nalong a single dimension. To capture the spatial correlations of\njoints from different limbs, Li et al. [LZLL18] proposed to en-\nlarge the convolutional kernels in the spatial domain. More recently,\nZang et al. [ZPK20] proposed to adaptively model the spatial cor-\nrelations with deformable convolutional kernels whose relative po-\nsitions of the entries are learned. The problem of learning patterns\nin irregular data structures like human poses with CNNs can be\novercome by extending convolutions to graph-structured data, as\nwe will see next.\n2.3.3. Graph Convolutional Networks\nTo leverage CNNs while properly handling the graph-structure typ-\nically used in animation to represent skeletons, Graph Convolu-\ntional Networks (GCNs), an extension of CNNs, have been re-\ncently considered in different frameworks working on human mo-\ntion data. GCNs come in two different ﬂavours [BBL*17]. Spa-\ntial approaches map neighbourhoods of each node in the graph to\nEuclidean patches on which a convolution is applied. Spectral ap-\nproaches operate in the Fourier domain of the feature signals sam-\npled on the graph, which depends on the graph Laplacian opera-\ntor [SNF*13]. By analogy with the convolution theorem, convolu-\ntion ﬁlters are deﬁned as spectral coefﬁcients that are multiplied by\nthe Fourier transforms of the signals.\nAberman et al. [ALL*20] resorted to a simple implementation\nof spatial GCNs. The supports of convolution kernels around each\njoint are deﬁned as d-ring neighbourhoods on the skeleton graph in\nthe spatial dimension and extended to the temporal axis to obtain\n2D skeleto-temporal convolutions. The operation of such GCNs\nis limited to skeletons sharing the same topology. However, the\nmotion retargeting network they proposed includes skeletal pool-\ning/unpooling layers, based on the fusion/duplication of the signals\nof adjacent edges. The pooling layers bring the input topology to\na common primal skeleton on which the core processing is per-\nformed. The result is transformed back to the original topology by\nthe unpooling layers. Such a network can cope with any topology\nthat is homeomorphic to the primal skeleton.\nThe other contributions leveraging GCNs [MLSL19; CSY20;\nLCZ*20; LCC*21] relied on the spectral approach of Kipf and\nWelling [KW17]. Here, the output Fl+1 of the graph convolu-\ntional layer l fed with a feature signal Fl is formulated as Fl+1 =\nσ( ˆAFlW l) where W l is the tensor of learnable convolution ﬁlter\nweights, ˆA depends only on the graph adjacency matrix and σ is a\nnon-linear activation function. In all of the proposed approaches the\nweights of the adjacency matrix are learnt in addition to the convo-\nlution ﬁlter. Mao et al. [MLSL19] built their adjacency matrix from\na fully connected graph of joints and thereby simultaneously learnt\nthe motion correlations between joints that are physically con-\nnected and joints that are far apart but whose motion are dependent,\ne.g. hands and feet during walking. Cui et al. [CSY20] objected that\nthis scheme may result in unstable training and separately learnt\ntwo graph adjacency matrices, one in which the weights of non-\nconnected joints in the skeleton are forced to 0 and another with full\njoint connectivity. Two papers by Li et al. [LCZ*20; LCC*21] pro-\nposed GCNs that operate at multiple scales, the ﬁnest scale corre-\nsponding to individual joints and the coarser scales to increasingly\nlarge body parts. While scales are analysed in parallel branches\nin [LCC*21], in [LCZ*20] the features at various scales are ad-\nditionally fused within each graph convolutional block.\n2.4. Learning Temporal Features\nThe temporal dimension of motion data is informative of the nature\nof the action being performed as well as the way it is performed.\nIn the following sub-sections we review the approaches taken to\nmodel human dynamics, most of which rely either on RNNs or\nCNNs.\n2.4.1. Recurrent Neural Networks\nRNNs are neural networks designed to process each timestep of a\ntime series one after another, and can thereby handle variable length\nsequences. They maintain an internal state that captures the tempo-\nral context of the signal. RNNs are most of the time based on Long\nShort-Term Memory (LSTM) or Gated Recurrent Unit (GRU).\nLong Short-Term Memory. A common LSTM is composed of a\nmemory cell that remembers values over arbitrary time intervals,\nand three gates – an input gate, an output gate and a forget gate – to\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n7\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nregulate the ﬂow of information into and out of the cell and to avoid\na common problem with RNNs known as the vanishing (or explod-\ning) gradient problem. In general, the problem is that the gradients\nused to update the network weights can become extremely small\n(or large), either preventing the network from further learning or\nmaking the network diverge, respectively. In the case of RNNs, the\nbackpropagation through time heavily relies on the chain rule to\ncompute gradients which exponentially decrease (vanishing prob-\nlem) or increase (exploding problem) if any weight is greater or\nsmaller than 1, respectively.\nThe memory cell remembers values over arbitrary time intervals,\nmaking LSTM effective at capturing both short-term and long-term\ntemporal dependencies. Indeed, LSTMs have proven to be power-\nful for learning temporal dependencies by achieving state-of-the-art\nperformance in key applications e.g. NLP, machine translation, etc.\nIn human motion related problems, LSTMs have been broadly em-\nployed, e.g. in motion prediction [FLFM15; CAW*19; LWJ*19;\nKGB19; JZSS16] and generation [HAB20; HYNP20; GSAH17;\nWYZ*20; WHSZ19; WACD20], in both physical [WMR*17;\nHTS*17; MAP*19; MTA*20] and kinematic [WCX17; LLL18;\nWCX21] character control, as well as in motion cleaning [ZLX*18]\nand style transfer [WCAD18].\nGated Recurrent Unit. As an alternative to LSTMs, GRUs have\nalso been widely used, such as in motion prediction [MBR17;\nTCHG17; GWLM18; GWRM18; PGA18; GMK*19; WAC*19;\nXLM19; GC19; CPAM20; AAR*20; LCZ*20], in motion gen-\neration [YK20; BKL18; ASS*20; GWE*20] or in motion edit-\ning [VYCL18; JL20]. GRUs rely on a gating mechanism similar\nto LSTMs in order to avoid the vanishing gradient problem but\nhave only two gates, a reset gate and an update gate. As a result,\nGRUs use fewer parameters and therefore less memory, are compu-\ntationally less expensive [GC19] and thus train faster than LSTMs.\nThey can process entire motion datasets [MBR17] instead of train-\ning action-speciﬁc models [FLFM15; JZSS16]. However, as shown\nby Weiss et al. [WGY18], the LSTM is strictly stronger than the\nGRU as it can easily perform unbounded counting, while the GRU\ncannot. Thus, LSTMs seem more accurate than GRUs on longer se-\nquences. In summary, the choice between LSTM and GRU depends\non the processed data and the considered application.\nBesides pure LSTM or GRU, extensions [TMLZ18] or combina-\ntions of both [WAC*19] have been used to model human dynamics.\nBidirectional LSTMs (BiLSTMs) stack two LSTMs running for-\nward and backward, respectively. As a result, temporal information\nis processed and preserved in both directions, i.e. past and future,\nwhich is helpful on certain tasks. For instance, BiLSTMs are lever-\naged to map an example motion into an embedding within an im-\nitation learning framework [WMR*17], to build a long plausible\nmotion sequence from a set of short motion clips [XXN*20] or\neven to reﬁne 3D motion data [LZZ*19; LZZL20].\n2.4.2. Temporal Convolutions\nCNNs provide an alternative to RNNs for learning temporal pat-\nterns in motion data. They can be either 1D along the tempo-\nral dimension, or use 2D spatio-temporal convolutions. Stacking\nseveral convolutional layers can efﬁciently capture both short and\nlong range temporal patterns since lower and higher layers will\ncapture dependencies between nearby and distant frames, respec-\ntively. CNN-based approaches are more computationally efﬁcient\nthan RNN-based ones because they process whole motion segments\nat once rather than frame by frame, allowing greater parallelism.\nHowever, CNN-based architectures often contain elements that do\nnot allow variable length inputs (e.g. a few fully connected layers\nafter convolutions) limiting their use in practice. As a result, they\nare quite rare in motion synthesis [HSK16; HGM19; DHS*19],\nprediction [BBKK17; PFAG20; CSY20; CSK*21] or character\ncontrol [HBM*20] with regard to RNNs but more frequent in other\ntasks where ﬁxed-length motion sequences are more suitable, e.g.\nmotion editing [LCC19; ZYC*20; SGXT20; SAA*20; KPKH20;\nALL*20; HHKK17; AWL*20; DAS*20; LAT21] (see Section 5).\n2.4.3. Miscellaneous\nOther approaches to better model the temporal ﬂow of human mo-\ntions include motion phase representation, spectral decomposition\nof motion data and spatio-temporal attention. For instance, several\nauthors investigated the representation and learning of the phase\nof movements in the context of kinematic character control, which\nis detailed in section 4.1. In the character controller network pro-\nposed by Holden et al. [HKS17], the network weights are com-\nputed as a spline function of the phase, whose control points, repre-\nsenting network weights conﬁgurations during the human locomo-\ntion cycle, are learned. Other works [ZSKS18; SZKS19; SZKZ20;\nLZCvdP20] use a gating network instead of the cyclic phase to\nblend expert weights, resulting in a mixture-of-experts scheme. In\nthe context of motion prediction, Mao et al. [MLSL19; MLS20] and\nCai et al. [CHW*20] learned temporal correlations in the frequency\ndomain by applying at the input and the output of their network a\nDiscrete Cosine Transform (DCT) and its inverse, respectively.\n3. Motion Synthesis\nSynthesising motion data is of strong practical interest to the media\nand entertainment industry. Besides generating realistic and diverse\nsequences, a key challenge is to be able to control various aspects\nof the motion using high-level parameters. In this section we dis-\ntinguish the tasks of short-term (Section 3.1) and long-term (Sec-\ntion 3.2) motion prediction, and motion generation (Section 3.3).\nThe ﬁrst focuses on the short-term deterministic extrapolation of\nmotion, i.e. up to one or two seconds, from a past conditioning\nclip, while the second operates beyond this temporal horizon, with\nthe purpose of generating plausible and diverse continuation of\nthe observed motion. Indeed, reproducing ground-truth animations\nquickly becomes impossible because of the stochastic nature of hu-\nman motion [FLFM15; PGA18; ZLX*18]. On the contrary, the\nability to generate diverse sequences is often a desired feature in\nlong-term motion prediction. In the third category, the goal is to\nsynthesise from non-historical parameters pose sequences that are\nboth consistent with the distribution of samples in a reference train-\ning dataset, and sufﬁciently diverse to capture its variations. We re-\nstrict our survey to general-purpose approaches and exclude works\ntargeting application-speciﬁc contexts such as the synthesis of ges-\ntures from speech or dance animations from music. We summarise\nthe methods presented in this section in Table 2.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n8\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nEncoder\nPose Sequence\nDecoder\nRecurrent Neural Network\nxt\nxt\nxt+1\nxt –2\nxt –1\nPredicted pose\n        at time t+1\nx t+1\nFigure 5: Illustration of the Encoder-Recurrent-Decoder (ERD) architecture originally proposed by Fragkiadaki et al. [FLFM15] and\nleveraged in both short and long term motion prediction in its original formulation and modiﬁed/extended versions. An RNN captures motion\ndynamics in a latent space. The encoder and decoder feedforward DNNs map skeletal poses to this latent representation and back.\n3.1. Short-term Prediction\nShort-term motion prediction consists in observing the motion on\na given temporal horizon and predicting the motion for the near\nfuture. Let us note Xt the 3D pose of a skeleton at time t: when ob-\nserving the set of poses Xt−Tp,...,Xt, we aim at predicting future\nposes Yt+1,...,Yt+Tf where Tp and Tf denote the past and future\ntemporal observation and prediction windows, respectively. Motion\nprediction is useful for many applications including robotic naviga-\ntion in crowds, human-robot interaction, video surveillance, virtual\nreality experiences or cloud gaming. In particular, the capability to\npredict the 3D positions of human end effectors rather than trajec-\ntory provides a much richer and helpful information.\nClassical approaches have been developed roughly between\n2000 and 2015 using techniques such as spatio-temporal autore-\ngressive models, Hidden Markov Models or Gaussian processes.\nThese “classical” techniques are out of the scope of this section.\nWith the development of DL techniques, as well as the production\nof larger mocap databases (as presented in section 2.2), it has be-\ncome a natural path to use DL for human motion prediction. There\nare multiple difﬁculties in this particular task of predicting human\nmotion with DL, namely:\n• When the motion is stationary and periodic, the repeating pat-\nterns can be discovered and used for future prediction. Unfortu-\nnately, human motion in real-world scenario is rarely stationary.\nThe ﬁrst challenge is thus to design deep models that account for\naperiodic and non-stationary motion.\n• Articulated human motion can only be efﬁciently predicted if\nspatial and temporal dependencies can be captured. For instance,\nmodelling the dependency between the right arm and the left\nleg during a walk cycle. The second challenge is to model these\nspatial and temporal dependencies.\n• DL offers the advantage of leveraging large quantities of data.\nHowever, the plausibility of the future prediction given the past\nobservations should be explicitly enforced. In particular, Mar-\ntinez et al. [MBR17] showed that many classical ML approaches\ncan be beaten by a simple baseline: the zero-motion prediction\n(this can partially be explained by the discontinuity of the ﬁrst\nframe prediction). The third challenge is therefore to ensure the\nplausibility and realism of the predicted motion.\n• Humans rarely act in an empty environment. Constraints such\nas objects, or other humans in the scene, should be taken into\naccount for prediction. The fourth challenge is to predict future\nmotion in a dynamic and constrained environment.\nWe ﬁrst review the existing methods, organised w.r.t. framework\nused. Then, we examine the existing answers that authors have pro-\nposed to the four aforementioned challenges.\n3.1.1. Classes of methods\nVarious approaches have been applied to the problem of short-term\nmotion prediction, and are presented according to the framework\nthey built on, namely: Sequence to Sequence (Seq2Seq), direct\nfeedforward networks, Graph Convolutional Networks (GCNs) and\nGenerative Adversarial Networks (GANs).\nSequence to Sequence. Popular and widely used in motion pre-\ndiction, Seq2Seq-based approaches generally consist in training an\nRNN in some latent space. Both the RNN and the hidden latent rep-\nresentation are trained jointly. Known issues of these techniques are\nthe convergence to a mean pose, as well as ﬁnding the trade-off in\nexposing the model to its own errors so that it can recover from de-\nviations. We describe hereafter the main solutions proposed in the\nliterature.\nFragkiadaki et al. [FLFM15] pioneered the Encoder-Recurrent-\nDecoder (ERD) architecture, in which an LSTM network is trained\nin the hidden layer of an autoencoder, enabling the network to learn\na representation suited for motion prediction, as illustrated in Fig-\nure 5. Papers that pursued this scheme generally opted for a GRU\nmodule afterwards.\nSome papers incorporated the skeletal structure in the network.\nJain et al. [JZSS16] learnt motion dynamics using a spatio-temporal\ngraph modelling body parts, using an RNN architecture, which can\nbe viewed as a structural-RNN architecture. Aksan et al. [AKH19]\nproposed a structured prediction mechanism, composed of a hierar-\nchy of sub-layers connected to the kinematic chain of the skeleton.\nThis enables an explicit modelling of body part motion prediction\nat a local and global scale. This structured prediction is compati-\nble with different ﬂavours of joint representation (exponential map\nor unit quaternions), as well as different types of prediction net-\nworks (simple RNN, Seq2Seq, etc.). Li et al. [LWY*19] proposed\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n9\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nan approach where the RNN is trained on a hierarchical decom-\nposition of joints, as well as on two different temporal horizons\nto maintain global consistency. Yang et al. [YKL21] used an RNN\nnetwork based on GRUs to predict the lower body motion, given\npast observations of the upper body.\nAlthough the prediction of joint positions is usually preferred,\nsome authors opted for different pose representations and the as-\nsociated loss function. To improve recent RNN approaches, Mar-\ntinez et al. [MBR17] proposed the following improvements: ﬁrst,\ninstead of feeding unrealistic noise at learning, they incorporated\nnetwork predictions to drive the network to recover from its own\nmistakes. Second, they proposed a residual architecture that takes\ninto account ﬁrst-order derivatives. Hence, they used a GRU in the\nlatent representation, forcing weight sharing between encoder and\ndecoder: motion continuity is enforced through residual connec-\ntions. Chiu et al. [CAW*19] learned a hierarchical and multi-scale\nlatent representation where an RNN is trained to predict human ve-\nlocities. Gui et al. [GWLM18] proposed a GRU-based Seq2Seq ar-\nchitecture, incorporating adversarial training to decrease the short-\nterm discontinuity and increase the long-range realism. In addi-\ntion, they advocated that the loss function should be based on a\ngeodesic loss, leveraging the properties of the Lie structure of or-\nthogonal matrices. Pavllo et al. [PGA18; PFAG20] proposed an au-\ntoregressive model based on two GRU layers in the space of quater-\nnions, therefore avoiding the gimbal lock issue at the expense of\nconstraining quaternions to remain on the unit 4-sphere (see Sec-\ntion 2.1.2). The total loss for the pose is composed of a quaternion\nloss and a positional loss obtained after FK.\nFinally, some methods took inspiration from the Seq2Seq frame-\nwork but replaced the RNN prediction layer: Li et al. [LZLL18]\nused two encoders to learn latent representations of both short and\nlong term horizons. A convolutional architecture is trained instead\nof an RNN to predict future poses. Xu et al. [XLM19] proposed\na hierarchical Seq2Seq model that encodes each sequence into a\nlatent representation. The prediction is then seen as a completion\nin this latent representation, obtained by vector addition. A sin-\ngle fully connected completion network is trained to this goal.\nGui et al. [GWRM18] leveraged a meta-learning scheme within a\nSeq2Seq approach using GRUs to adapt the model’s parameters to\nnew training examples. To the best of our knowledge, the work of\nWang et al. [WAC*19] is the only approach using DRL for motion\nprediction. The prediction problem is decomposed into K steps, and\nthe progressive prediction is trained using an imitation learning ap-\nproach.\nDirect Feedforward. Less used than Seq2Seq-based methods, di-\nrect feedforward techniques aim at computing the prediction net-\nwork F directly from the entire set of poses given the past obser-\nvations: Yt+1,...,Yt+Tf = F(Xt−Tp,...,Xt).\nBütepage et al. [BBKK17] trained such a feedforward network\nafter a temporal encoding of human motion. Feature learning is\nevaluated thanks to an action classiﬁer. Guo and Choi [GC19]\nproposed a combination of two DNNs for human motion predic-\ntion: the ﬁrst network, called SkelNet, assembles the prediction of\ndifferent body parts, while the second network, called Skel-TNet,\naccounts for temporal dependencies modelled through an RNN.\nZang et al. [ZPK20] proposed a deformable spatio-temporal con-\nvolution approach that captures in the past sequence the most rel-\nevant poses, through a masking mechanism. Based on this tem-\nporal attention, parameters for the prediction of the future mo-\ntion are generated on the ﬂy, as in few-shot learning methods.\nCai et al. [CHW*20] leveraged the increasingly popular trans-\nformer architecture [KNH*21]. Originally designed for processing\ndata sequences in NLP, it relies on attention mechanisms to bet-\nter account for long-range dependencies than RNNs. The attention\nweights learnt in a transformer model capture the relevance of data\nsequence items to other items that can be far apart. Because these\nweights modulate the input data prior to convolution, transformer\nnetworks can be viewed as extensions of CNNs and RNNs in which\nthe learnt convolution kernels are made data-dependent. This in-\ncreased expressiveness comes at the price of larger training data\nvolume requirements. More speciﬁcally, in [CHW*20] joints are\nconverted using a Discrete Cosine Transform (DCT). The decod-\ning of the future pose is done progressively in accordance with the\nskeleton topology.\nGraph Convolutional Networks. As described in Section 2.3.3,\nGraph Convolutional Networks (GCNs) leverage the graph nature\nof the human skeleton, and have therefore been used to design pre-\ndiction methods that act on such graphs. They were ﬁrst used in the\ncontext of short-term motion prediction by Mao et al. [MLSL19],\nwho proposed an approach where poses are encoded in a latent\nspace using DCT decomposition. This representation is then fed\nto a GCN for feedforward prediction, while the connectivity of the\ngraph is learned through the adjacency matrix, which enables to\nlearn the temporal and spatial dependencies between body joints.\nThey also proposed an attention mechanism applied to the DCT\ncoefﬁcients of the observed sequence [MLS20], where the main\nidea is to focus on relevant history information to predict the fu-\nture poses. Approaches based on GCNs have also been proposed\nto capture skeletal and motion information at different scales. For\ninstance, Li et al. [LCZ*20; LCC*21] used GCNs to capture the\nskeleton structure over different scales, ranging from individual\njoints to increasingly large body parts. A cross-scale fusion block\nthen merges the features over scales and feeds this representation\ninto a GRU-based decoder [LCZ*20]. As an alternative, scales can\nbe analysed in parallel branches to extract features used both for\naction recognition and motion prediction [LCC*21]. The predicted\nmotion category is then used in the motion prediction network.\nSimilarly, in order to capture relevant motion information at dif-\nferent scales, Lebailly et al. [LKS*20] proposed a temporal incep-\ntion module, using 1D temporal convolutions at different scales,\ncombined with a GCN to predict the future poses. Unlike previous\napproaches that represent skeletal connections with a single adja-\ncency matrix, Cui et al. [CSY20] relied on a double graph convo-\nlutional approach to better learn the dynamic relationships between\nskeletal joints, therefore learning a connective graph to represent\nthe natural kinematic links of the human skeleton and a global\ngraph to account for the dynamics of non-connected joints. Convo-\nlutions over these two graphs are then used to predict future poses.\nGCNs have also been used to explore the prediction of human-\nobject interactions: Corona et al. [CPAM20] built on the work of\nMartinez et al. [MBR17] and explicitly incorporated the modelling\nof such interactions. As adjacency matrices typically vary in time\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n10\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nin these dynamic interactions, the resulting graph representation is\nlearned to capture their dynamics.\n3.1.2. Spatial and Temporal Dependencies\nOne difﬁculty in motion prediction is to leverage the spatial and\ntemporal dependencies that exist in human motion. In the spatial\ndomain, human joints are directly related to their parents but indi-\nrectly related to other joints (e.g. symmetry or regularity in motion).\nIn the temporal domain, the repeating patterns (or information re-\ndundancy) can greatly help in predicting the future motion. Model-\ning the motion over various scales greatly helps for short-term mo-\ntion prediction. This is closely related to attention mechanisms that\nhave been well studied in the DL community. Researchers started to\ninvestigate spatial dependencies ﬁrst, then rapidly studied both spa-\ntial and temporal dependencies especially through temporal atten-\ntion mechanisms. We here review the different solutions that have\nbeen proposed in the literature.\nJain et al. [JZSS16] used a spatio-temporal graph that explic-\nitly models the body structure (spine, arm, and leg) and captures\nthe spatial dependencies. Bütepage et al. [BBKK17] learned a tem-\nporal encoding of motion using different time scales and convo-\nlutions, while a graph network is used to encode the hierarchical\nstructure of the skeleton. Li et al. [LZLL18] explicitly used two la-\ntent representations for short-range and long-range observation. By\ndoing so, they expected to beneﬁt from long-range motion consis-\ntency, as well as to improve the dynamics of short-term prediction.\nCui et al. [CSK*21] used a temporal attention mechanism, jointly\nwith dilated convolutions, to capture long-range motion features.\nCapturing temporal and spatial dependencies between body joints\nhas also been explored using GCNs [MLSL19; CSY20; LCZ*20;\nMLS20; LKS*20], as already detailed in Section 3.1.1, to match\nthe graph nature of the human skeleton. Cai et al. [CHW*20] lever-\naged the transformers concept that learns the spatial correlations as\nwell as the temporal smoothness of the predicted skeletons.\n3.1.3. Plausibility and Realism\nData-driven approaches to predict future motion can face the issue\nof predicting unrealistic poses in terms of biomechanics. In addi-\ntion, motion prediction can suffer from a lack of realism whatever\nthe temporal range of prediction. We present here the proposed so-\nlutions to these problems.\nTo improve plausibility, Gui et al. [GWLM18] included two dis-\ncriminators at training time: a ﬁdelity and a continuity discrimi-\nnator. The ﬁdelity discriminator assesses whether the prediction\nis smooth enough, while the continuity discriminator encourages\nthe prediction to be consistent with past observations, thus lim-\niting the discontinuities. It is also possible to rely on a discrim-\ninator to discriminate from real sequences, however results show\nthat this improves the prediction by a small margin only [LZLL18].\nCui et al. [CSY20] used the Gram matrix in the loss function to add\nmore consistency between the predicted and ground-truth poses.\nThey also included bone length preservation.\nPavllo et al. [PGA18] proposed to mix a teacher-forcing ap-\nproach (i.e. feeding the network with ground-truth motions) with\nan approach where the network is fed with its own predictions.\nThe network is ﬁrst trained with teacher-forcing and progressively\nswitches to its own predictions through a curriculum schedule. This\nprogressive training improves the error and the model stability.\nCai et al. [CHW*20] used a dictionary mechanism, similar to a\nmemory cell. This dictionary encodes motion that could be similar\nbut seen in slightly different contexts. Bourached et al. [BGG*20]\nproposed an out-of-distribution approach, where samples can be\naugmented thanks to a Variational Autoencoder (VAE) learned on\nHuman3.6 [IPOS14] and CMU [Uni03] datasets. Using graph con-\nvolutional layers, the VAE models connectivity, positions and tem-\nporal frequencies, which helps to limit the distribution shift and\nto regularise the training. More speciﬁcally, this mechanism helps\nproducing larger quantities of plausible training data.\n3.1.4. Context, Environment and Interactions\nModelling human motion in an empty environment, without ob-\njects or humans, is already a difﬁcult task. However, context, envi-\nronment and surrounding characters are crucial for predicting mo-\ntions in real-life scenarios, especially to avoid unrealistic situations,\ne.g. character collisions or incorrect interactions with the environ-\nment. This is an even more complex task, mainly due to the lack\nof databases and the large variety of situations that can be encoun-\ntered.\nTo better predict interactions between a human and all ob-\njects of the scene, Corona et al. [CPAM20] proposed to incor-\nporate the context and the interaction between humans and ob-\njects in a novel context-aware motion prediction architecture. To\ndo so, a convolutional method over the graph of objects and per-\nsons is learned, building on the RNN implementation proposed\nby Martinez et al. [MBR17]. The graph relationships are also\nlearned, initialised so that the prediction of one object only de-\npends on itself, and progressively accounting for interactions. Sim-\nilarly, to account for both scene and social contexts in the predic-\ntion, Adeli et al. [AAR*20] proposed a Seq2Seq approach with a\nGRU architecture, where the scene and social contexts are captured\nindependently through the analysis of monocular video. The spatio-\ntemporal features are ﬁrst extracted by a CNN, then fed to the de-\ncoder module to account for context.\n3.2. Long-term Prediction\nThe goal of long-term motion prediction is either to extrapolate\na past conditioning clip to the future (i.e. forecasting, see Sec-\ntion 3.2.1) or to interpolate between known past and future char-\nacter poses, often far apart both temporally and spatially (i.e. in-\nbetweening, see Section 3.2.2). Unlike short-term prediction ap-\nproaches, the time horizons considered are larger and the synthe-\nsised motions are not constrained to reproduce the ground truth.\nInstead, the goal at large is to generate a diversity of plausible con-\ntinuations of the original clip.\n3.2.1. Forecasting\nMost of the approaches in this category are based on the Encoder-\nRecurrent-Decoder (ERD) model originally proposed by Fragki-\nadaki et al. [FLFM15] for motion prediction and presented in sec-\ntion 3.1. The generation is conditioned by a past window of mo-\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n11\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\ntion frames. The temporal context is captured by an RNN operat-\ning in the latent space at the output of the encoder, as illustrated\nin Figure 5. As pointed out by many authors [HHS*17; ZLX*18;\nLWJ*19; WCX21; GWE*20], future motion predicted by ERD net-\nworks tend to converge to a motionless state or to diverge from hu-\nman motion. This is attributed to several issues that many of the\napproaches presented below aim at mitigating. Although some of\nthese issues are also relevant to short-term motion prediction (sec-\ntion 3.1), they are exacerbated when targeting longer prediction\nhorizons.\nA major downside of the ERD architecture is the accumulation\nof errors along time at the output of the RNN. As a result, the qual-\nity of the generated motion degrades as the prediction moves away\nfrom the past conditioning sequence. Wang et al. [WCX21] and\nKundu et al. [KGB19] treated the ERD model as the generator of a\nGAN and added a corresponding discriminator to improve the plau-\nsibility of the motion predictions. Kundu et al. [KGB19] concate-\nnated a stochastic component r to the output of the ERD encoder\nand complemented the discriminator with a critic network that re-\ngresses r from the predicted motion sequence. The regressed value\nis fed back to the ERD decoder to reﬁne the predicted sequence.\nThis encourages the learning of a one-to-one-mapping between the\nstochastic input to the GAN and the generated motion, thereby min-\nimising the risk that the output collapses to a single mode of the\nmotion distribution, a well-known issue with GANs [GPM*14].\nSeveral authors [HHS*17; GWE*20] ascribed the regression to-\nwards a static pose to the ill-posedness of long-term motion pre-\ndiction and advocated the use of external control signals to disam-\nbiguate the task. For instance, Ghorbani et al. [GWE*20] built the\nRNN cell of their network around a conditional VAE that is con-\nditioned on external constraints, such as action type and character\ngender, as well as on the hidden state of the RNN at the previous\ntime step. Cao et al. [CGM*20] conditioned their motion prediction\nscheme on the environment the character moves in, speciﬁed using\na 2D picture. The past motion history is provided as a sequence of\n2D joint heatmaps in the scene. A ﬁrst conditional VAE network\nsamples a plausible 2D target end location of the predicted motion\nsequence inside the environment, from which a second network\npredicts the 3D trajectory of the character center. Finally, a third\nnetwork promotes this point trajectory into a 3D pose sequence.\nThe RNNs at the core of the ERD model create other issues.\nTraining an RNN using ground-truth future samples rather than ex-\nposing it to its own predictions creates a difference in behaviour\nbetween the training and inference stages that is detrimental to per-\nformance, a phenomenon referred to as exposure bias [PGA18;\nGWE*20]. As a mitigation measure, Zhou et al. [ZLX*18] pro-\nposed an auto-conditioning network that is trained alternately in\nopen loop on ground-truth motion and in closed loop on its own\npredictions. Gopalakrishnan et al. [GMK*19] strike a balance be-\ntween the two training modes in their loss function and gradually\nincrease the weight of the closed-loop term. RNNs have a ten-\ndency to overly focus on recent poses and fail to capture long-\nterm temporal dependencies of human motion. To counter this ef-\nfect, Tang et al. [TMLZ18] augmented their RNN with a tempo-\nral attention mechanism that encourages pose predictions to align\nto previous poses in a pose embedding. To the same purpose,\nLiu et al. [LWJ*19] proposed a hierarchical RNN cell that cap-\ntures a global motion context in addition to the RNN hidden state\nat every frame.\nUnlike for short-term motion prediction where the goal is to min-\nimise deviations from the ground truth, diversity and randomness\nin the generated motion sequences is encouraged by complement-\ning the past sequence input with a stochastic component. In ERD\nnetworks this can be achieved by adding random noise to the la-\ntent pose representations at the output of the encoder [XLM19],\nor by stacking a random vector to the latent representation of the\nRNN [KGB19]. Stochasticity can also be embedded in the RNN\ncell, as in [GWE*20] where this cell is built around a conditional\nVAE whose random latent code drives motion generation.\nOther works proposed more expressive models of the skeleton or\nof the motion dynamics to improve the realism of synthesised ani-\nmations. For instance, representations of the articulated bone struc-\nture as quaternions [PGA18] or as 3D rigid transformations in the\nLie algebra se(3) [LWJ*19] were shown to generate more natural\nmotion sequences. Please refer to Section 2.1 for a detailed presen-\ntation of these approaches. Gopalakrishnan et al. [GMK*19] incor-\nporated short-term motion history in their representation of pose\nby complementing joint angles with their local temporal deriva-\ntives up to order 3. The prediction of motion dynamics is cast by\nWang et al. [WCX21] in a probabilistic framework. The transition\nof character pose xt at time t to the next frame is expressed by a\nprobability density function p(xt+1 | xt), modelled as a multivari-\nate Gaussian Mixture Model (GMM). Their network builds on two\nRNN cells that predict the GMM parameters at each time step by\nmaximising its likelihood.\nMany authors acknowledged that learning a motion manifold for\na large diversity of styles and movements is a difﬁcult problem. To\nfacilitate this task, several authors [GSAH17; LWJ*19; GMK*19;\nWHSZ19] learned separate sub-networks for modelling the spatial\nand temporal components of human motion. The prediction net-\nwork of Gopalakrishnan et al. [GMK*19] is a two-level hierarchi-\ncal RNN where the upper layer sketches a trajectory for future mo-\ntion while the lower layer synthesises poses on this trajectory. The\ndecoder in the ERD network architecture of Wang et al. [WHSZ19]\nfeatures a forward and a backward motion predictor, whose outputs\nare concatenated to reconstruct the full-length motion sequence.\nThe authors argue that since the backward predictor forces the de-\ncoder to pay attention to the last few frames output by the encoder,\nit encourages the model to ﬁrst learn short-term temporal correla-\ntions before accounting for longer-term dependencies.\nUnlike all previous approaches, Hernandez-Ruiz et al. [HGM19]\ndid not rely on an RNN. They instead reformulated motion predic-\ntion as a temporal inpainting task and built their model on a GAN.\nThey fed the generator with the past conditioning motion clip, and\nencoded it into frame-speciﬁc latent representations that are pro-\ncessed at multiple scales before being decoded into an output pose\nsequence.\n3.2.2. In-betweening\nThe task of interpolating motion between starting and ending char-\nacter poses, often far apart, is referred to as In-betweening. It can\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n12\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nbe viewed as a long-term prediction task conditioned on past and\nfuture contexts.\nYu et al. [YKK*19] proposed a simple scheme for interpolating a\nmotion sequence from starting and ending positions of end-effector\njoints. Their main objective is computational efﬁciency, in order to\ngenerate terrain-adaptive character motion in real time for video\ngames. They cascaded two fully connected networks: the ﬁrst in-\nterpolates the trajectories of the end effectors in time, the second\ninfers full poses at each frame.\nThe main purpose of other works is to generate plausible and\ndiverse motion over long transitions bounded by a starting and\nan ending keyframe. Kaufmann et al. [KAS*20] leveraged a deep\nconvolution denoising autoencoder, in which pooling layers en-\nsure large receptive ﬁelds to capture long-range spatial and tempo-\nral joint correlations. A curriculum learning scheme feeds the en-\ncoder with sequences containing increasingly large temporal gaps\nto improve the training. Xu et al. [XXN*20] proposed a temporally\nhierarchical scheme in which the transition segment is split into\nequal length sub-segments. Trajectory constraints are provided at\neach sub-segment endpoint. The transition sequence is initialised\nby sampling a motion clip from the training dataset for each sub-\nsegment. Next, the style of each clip is changed to match the style\nof a reference sequence throughout the whole transition sequence.\nThis stage leverages a motion autoencoder with separate content\nand style embeddings, that is trained in an unsupervised way. Style\ntransfer is performed by linearly combining the content latent code\nof the initial clips and the style latent code of the reference style se-\nquence. Finally, transitions between the endpoints of consecutive\nsub-segments are generated using forward and backward LSTM\nnetworks, and the plausibility of the generated sequence is en-\nhanced by combining the generation network with a discriminator\nin a GAN framework. Harvey et al. [HYNP20] pointed out that\nin-betweening between distant keyframes may result in stalling or\nteleportation artifacts if the temporal evolution of the motion is not\nmonitored during the generation process. To deal with this issue,\nthey proposed an ERD architecture that is fed with the pose repre-\nsentation deltas between the current and the target frame, in addi-\ntion to the character pose at the current timestep and the end pose.\nThe time-to-arrival is encoded in a sine wave and added, rather than\nstacked, to the latent code that is input to the RNN, forcing the net-\nwork to take this piece of information into account during training.\n3.3. Generative Synthesis\nUnlike motion prediction methods, approaches covered in this sec-\ntion synthesise human motion by processing a random seed with a\ndeep generative network, optionally conditioned on semantic cues\ntypically pertaining to the character trajectory or motion style. They\nare grouped in the following sub-sections based on the deep gen-\nerative framework they build on. As an exception, in the scheme\nproposed by Holden et al. [HSK16] the synthesis process is purely\ndeterministic and driven by high-level cues.\n3.3.1. Restricted Boltzmann Machines\nInitially proposed by Smolensky [Smo86], Restricted Boltzmann\nMachines (RBMs) build on a parametric expression of the prob-\nability density function of the data distribution from which new\nP ( H |V )\nV\nP(V | H )\nH\nV\n(0)\nH\n(0)∼P(H |V\n(0))\nV\n(1)∼P(V | H\n(0))\nFigure 6: A Restricted Boltzmann Machine (RBM) can be repre-\nsented by two fully-connected neural networks whose outputs pro-\nvide the conditional probabilities of hidden variables H given vis-\nible variables V and vice-versa. Sampling from these probabilities\nbuilds a chain of hidden H(j) and visible V (j) samples that con-\nverges to the model distribution. The chain is initiated by drawing\nV (0) from the training set.\nsamples are to be generated. This density depends on visible units\nV that correspond to the variables to be synthesised and hid-\nden units H. An RBM can be represented as a bipartite graph in\nwhich hidden units are conditionally independent given the visi-\nble units, and vice-versa. The form of the probability density func-\ntion P(H,V) in an RBM provides simple closed-form expressions\nfor P(H|V) and P(V|H) as fully connected layers with a sigmoid\nactivation. These layers are usually trained using an approximate\ngradient ascent scheme known as Contrastive Divergence [Hin02].\nOnce P(H|V) and P(V|H) have been determined, new samples\nare generated using a Markov Chain Monte Carlo (MCMC) sam-\npling process initiated by visible units V (0) drawn from the training\ndataset. Given V (i), P(H|V (i)) is computed and hidden units H(i)\nare drawn from that distribution. Next, evaluating and sampling\nfrom P(V|H(i)) yields V (i+1). This alternated sampling scheme be-\ntween V (i) and H(i) is stopped after K steps, V (K) providing the\nsought generated sample (see Figure 6).\nConditional RBMs. Taylor et al. [THR06] extended the RBM\ngenerative framework to the synthesis of time series by introduc-\ning an autoregressive model that conditions the visible and hidden\nvariables at the current time step on the visible variables at past\ntime steps. The formulation of this Conditional RBM (CRBM) dif-\nfers from the original RBM only by additional bias terms in the\nexpressions of P(H|V) and P(V|H). These biases are learnt during\ntraining. The optimisation and sampling processes are otherwise\nunchanged. Motion sequences can be generated using a CRBM by\nmapping character joint positions or rotations to its visible units.\nWhen the training dataset contains different categories of motion,\nthe generated category is dictated in principle by the selection of\nthe initial MCMC sample V (0), although transitions between dif-\nferent types of motion can be forced by adding noise to the hidden\nstates.\nFactored CRBMs. The same authors [TH09] proposed the Fac-\ntored CRBM (FCRBM) architecture to better control the motion\ncategory or style. It is essentially a gated CRBM consisting of three\nlayers: the visible units at the previous time steps form the input\nlayer, the same units at the current time step are the output layer,\nand the connections between these two layers are gated using mul-\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n13\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nEncoder\nDecoder\nGenerative\nModule\nFigure 7: In a Variational Autoencoder (VAE) a motion sequence\nis mapped by an encoder to a random latent code that is con-\nstrained to follow a prior Gaussian distribution. This code is\nmapped back by a decoder to the input motion data. Once the full\nnetwork is trained, feeding the decoder with samples drawn from\nthe prior generates stochastic motion sequences.\ntiplicative weights that form a third hidden layer. To obtain a more\nefﬁcient representation, the third-order interaction tensor between\nthe 3 layers is factored into three matrices of pairwise interactions.\nThe weights in the hidden layer are deﬁned as linear functions of\na one-hot vector of motion style labels that control motion synthe-\nsis. Transitions between motion styles can be introduced at will by\nappropriate settings of the motion style labels over the sequence.\nAlemi et al. [ALP15] applied the FCRBM model to the genera-\ntion of controlled affective variations of walking motion, based on\ntime-varying valence and arousal labels.\nHierarchical FCRBMs. Chiu and Marsella [CM11] noted that\nCRBM and FCRBM approaches are prone to overﬁtting when try-\ning to generalise to a rich set of styles and motion, because the\ntraining dataset can only sparsely sample the set of all possible\nstyle combinations and transitions. To mitigate this issue, they pro-\nposed a Hierarchical FCRBM (HFCRBM) model, corresponding\nto a Deep Belief Network with an FCRBM on top of a simpliﬁed\nCRBM where the temporal dependency on past visible states has\nbeen removed. The dynamics of the synthesised sequence is con-\ntrolled exclusively by the upper FCRBM. In their multi-path ap-\nproach, a separate FCRBM is learnt for each motion style, the out-\nput visible layers are linearly combined with predeﬁned weights\ndeﬁning the desired motion style, and the result is fed to the bot-\ntom layer of the HFCRBM. Thus, rather than relying on a unique\nFCRBM to represent all styles, the mixing of styles is performed\nby mixing individual FCRBM instances within the hidden layer of\nthe HFCRBM.\n3.3.2. Variational Autoencoders\nStructured as an encoder followed by a decoder, an autoencoder is a\nDNN whose input and output represent the same data. The encoder\noutput provides an intermediate latent code with a dimension of-\nten lower than the input data. Thus, autoencoders provide a scheme\nfor non-linear dimensionality reduction. As illustrated in Figure 7,\nthe distribution of the latent codes in a Variational Autoencoder\n(VAE) [KW14] is further constrained to follow a predeﬁned prior\ndistribution, typically a multivariate normal distribution, endowing\nthe autoencoder with a generative capability. Thus, VAEs provide\na convenient way to embed a stochastic component into the gener-\nation of human motion.\nVarious\napproaches\nhave\nbeen\nproposed\nto\nextend\nthe\nVAE\nframework\nto\nthe\nmodelling\nof\ntemporal\nsequences.\nToyer et al. [TCHG17] relied on a Deep Markov Model, essentially\na VAE in which the latent code is conditioned on its value at the pre-\nvious time step. Habibie et al. [HHS*17] combined a VAE and an\nRNN. Motion generation is driven by the random latent code sam-\nples, as well as by control variables that constrain the trajectory and\nvelocity of the character. The RNN is conditioned on encodings on\nthese control variables. At inference time, its cell state is initialised\nwith the latent code value. The concatenation of cell state outputs\nat each time step is fed to the decoder to produce the synthesised\nmotion sequence.\nDu et al. [DHS*19] built on the motion graph framework pro-\nposed by Min and Chai [MC12], in which motion sequences are\nrepresented as a graph of motion primitives. The segmentation of\nmotion sequences into primitives is dependent on the type of mo-\ntion and typically hand-crafted. Autoencoders learn embeddings\nfor each primitive, and the latent codes for these embeddings are\nfurther encoded by Conditional VAEs trained on dataset samples\nfor the considered primitives. The conditioning of the primitive-\nspeciﬁc VAEs ensures that they reproduce the style of the input mo-\ntion, which is encoded as a Gram matrix in the embedding space,\nfollowing prior work in motion style transfer [HSK16] (see Sec-\ntion 5.3). To synthesise a motion sequence, a path is determined\nin the motion graph based on user-deﬁned trajectory controls, and\nmotion primitives are generated along this path using the VAEs.\nYan et al. [YRV*18] and Aliakbarian et al. [ASS*20] relied on\nsimilar network architectures for stochastic motion prediction. Pose\nsequences are mapped to lower-dimensional features by an encoder\nand transformed back to motion data by a decoder. The VAE oper-\nates on the encoded features, its output is fed to the decoder to\nsynthesise the motion clips. Yan et al. [YRV*18] processed small\npose sequences called motion modes that capture short-term mo-\ntion features. During training, their VAE maps a pair of (past, fu-\nture) mode features to a random latent code (encoder) then to a\nprediction of the future mode feature (decoder). It thereby captures\nthe transition between the two modes. At inference time, the VAE\nand RNN decoders generate a stochastic prediction of the future\nmode, given a past conditioning mode and a draw of the VAE ran-\ndom latent code. Aliakbarian et al. [ASS*20] argued that stacking\nthe past sequence information and the random generating seed in a\nvector and feeding it to the decoding network leaves the possibility\nthat the stochastic component is assigned low weights during train-\ning and is thus effectively ignored by the network. This concern is\nconﬁrmed by experimental evidence. To avoid this, they proposed\na mix-and-match perturbation strategy and formed a vector by re-\nplacing randomly selected components of the past sequence feature\nby corresponding components of the VAE latent code. Feeding this\nvector to the decoder forces it to account for both the past context\nand the stochastic input.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n14\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nGenerator\nDiscriminator\nAdversarial\nLosses\nPrior\nDistribution\nTraining\nData\nGenerative\nModule\nreal /\nfake\nFigure 8: A Generative Adversarial Network (GAN) consists of\ntwo networks: the generator produces “fake” motion samples from\nrandom seeds, the discriminator tries to differentiate them from\n“real” ones drawn from the training dataset. The two modules are\ntrained jointly, aiming at an equilibrium where the generator out-\nputs cannot be distinguished from the training data.\n3.3.3. Generative Adversarial Networks\nGenerative Adversarial Networks (GANs) are a popular alternative\nto VAEs for generating samples from random seeds. In a GAN,\nnew samples are synthesised by a generator network that operates\nin conjunction with a discriminator network (see Figure 8). The\ngenerator transforms a random seed drawn from a known prior dis-\ntribution to a sample which aims to be similar to the contents of the\ntraining dataset, the discriminator assesses this similarity. The two\nnetworks are trained jointly with adversarial losses, the generator\nbeing driven to produce samples that the discriminator ultimately\nshould not be able to distinguish from the training dataset samples.\nBarsoum et al. [BKL18] pointed out that relying on a GAN\nfor long-term motion prediction has several intrinsic advantages.\nGANs do not suffer from the temporal error accumulation is-\nsues of RNNs and will produce one instance of all possible out-\nputs instead of averaging these possible outputs as ERD networks\ntend to do. GANs also natively embed a stochastic component.\nAny motion generation network can be wrapped into a GAN by\nadding a discriminator network to improve the plausibility of the\ngenerated sequences [BKL18; WCX21; HYNP20]. Other works\nfocused their contribution on optimising the architecture of the\nGAN generator network to facilitate and improve its training.\nWang et al. [WACD20] proposed an adversarial autoencoder ar-\nchitecture [MSJG15] where a GAN enforces a prior on the distri-\nbution of the latent code of an autoencoder. Yan et al. [YLX*19]\nfed the generator with a sequence of random samples drawn from\na Gaussian process with a predetermined covariance function.\nThrough a series of purely convolutional modules made up of a\nspatio-temporal upsampling layer followed by a graph convolution\non the skeleton features, they gradually increased the spatial and\ntemporal resolution of their output to produce a pose sequence.\nWang et al. [WYZ*20] split their generator into separate spatial\nand temporal sub-networks: the lower layer maps the input random\nseed and conditioning action label via an RNN to a sequence of\nlow-dimensional latent codes that model temporal transitions. The\nupper layer decodes each latent code into a skeletal pose. The gen-\nerator is regularised by a helper action classiﬁer network through\na cycle consistency constraint: the conditioning action label fed to\nthe generator should agree with the result of the classiﬁcation of\nthe motion sequence it produces.\n3.3.4. Normalising Flows\nGenerative models based on Normalising Flows [DKB14] synthe-\nsise samples by applying a composition of invertible elementary\ntransforms to a latent variable drawn from a known prior distri-\nbution. Unlike in GANs or VAEs, owing to the form of the ele-\nmentary transforms, the probability density function of the gener-\nated samples can be computed in closed form. Thus, the genera-\ntive network can be trained by maximum likelihood optimisation.\nHenter et al. [HAB20] adapted this framework to the generation of\nmotion sequences. Each transform layer in the generator network is\nconditioned by a control signal that encodes the past trajectory of\nthe root joint and holds an LSTM unit whose hidden state captures\ntemporal dependencies.\n3.3.5. Miscellaneous\nDeterministic Generation. Holden et al. [HSK16] conditioned\ntheir motion generation approach on purely deterministic con-\nstraints that specify the trajectory and velocity of the character.\nThey leveraged an autoencoder operating on temporal chunks of\nposes to learn a latent manifold of human motion. A separate\nfeedforward network maps the autoencoder embedding to high-\nlevel, semantically interpretable controls of the motion. Generating\nhigh-dimensional motion embedding samples from a set of low-\ndimensional control parameters is severely under-constrained. To\ndisambiguate the generation to the largest possible extent, the scope\nof the approach is restricted to human locomotion, and a compre-\nhensive set of trajectory and foot contact constraints is imposed.\nDeep Generative Models Sampling. After training a deep gen-\nerative model driven by an input random seed, motion sequences\nare often obtained by drawing independent samples from the seed\nand feeding them to the generator network. Yuan et Kitani [YK20]\nproposed a better sampling strategy to enforce diversity in the gen-\nerated sequences and cover minor modes of their distribution. To\nthis end, they computed correlated random seeds by applying a set\nof afﬁne transforms to a random variable drawn from a Gaussian\ndistribution. The parameters of the afﬁne transforms are learnt by a\nfront-end network that can be conditioned on the same conditioning\ndata used by the generative model. Each seed generates one motion\nsequence. Diversity among these sequences is enforced through a\nrepulsion loss term that maximises the dissimilarity between the\ngenerated sequences. Another loss term enforces consistency of the\nrandom seeds with the prior distribution of the generative model.\nThe relative weights of these two terms deﬁne a trade-off between\nthe conﬂicting goals of diversity and ﬁdelity to the training data\ndistribution.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n15\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nTable 2: Summary of the methods presented in Section 3. Miscellaneous data includes hand-crafted, synthetic, proprietary, unspeciﬁed or\nother public datasets.\nReference\nCode\nFramework\nRepresentation\n3.1 Short-term Prediction\n3.2 Long-term Prediction\n3.3 Generative Synthesis\n3DPW [vMHB*18]\nCMU [Uni03]\nHuman3.6M [IPOS14]\nHDM05 [MRC*07]\nHolden et al. [HSK16]\nNTU RGB+D[SLNW16]\nMiscellaneous\nFully Connected\nConvolutional\nGraph Convolutional\nRecurrent\nTransformer\nAutoencoder\nRBM\nVariational Autoencoder\nAdversarial\nNormalising Flows\n[FLFM15]\n×\nDL\n×\n×\nso(3)\n[JZSS16]\n</>\n×\nDL\n×\n×\nso(3)\n[BBKK17]\n×\nDL\n×\n×\n×\nso(3)\n[MBR17]\n</>\n×\nDL\n×\n×\nso(3)\n[GWLM18]\n×\nDL\n×\n×\n×\nso(3)\n[GWRM18]\n×\nDL\n×\n×\nUnknown\n[LZLL18]\n</>\n×\nDL\n×\n×\n×\n×\nso(3)\n[AKH19]\n</>\n×\nDL\n×\n×\nSO(3)\n[CAW*19]\n</>\n×\nDL\n×\n×\n×\n3D velocities\n[GC19]\n</>\n×\nDL\n×\n×\n×\nso(3)\n[LWY*19]\n×\nDL\n×\n×\n×\nso(3)\n[MLSL19]\n</>\n×\nDL\n×\n×\n×\n×\nDCT\n[WAC*19]\n×\nDRL\n×\n×\n×\nso(3)\n[AAR*20]\n×\nDL\n×\n×\n×\n×\n3D positions\n[CHW*20]\n×\nDL\n×\n×\n×\nDCT\n[CPAM20]\n×\nDL\n×\n×\n3D positions\n[CSY20]\n×\nDL\n×\n×\n×\n×\n×\n×\nso(3)\n[LCZ*20]\n</>\n×\nDL\n×\n×\n×\n×\n×\nso(3)\n[LKS*20]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions\n[MLS20]\n</>\n×\nDL\n×\n×\n×\n×\nDCT\n[PFAG20]\n×\nDL\n×\n×\n×\nQuaternions\n[ZPK20]\n×\nDL\n×\n×\n×\nUnknown\n[BGG*20]\n</>\n×\nDL\n×\n×\n×\n×\nDCT\n[CSK*21]\n×\nDL\n×\n×\n×\n×\n×\nso(3)\n[LCC*21]\n×\nDL\n×\n×\n×\n×\n×\n×\nso(3)\n[YKL21]\n×\nDL\n×\n×\n×\nAd hoc\n[PGA18]\n</>\n×\n×\nDL\n×\n×\n×\nQuaternions\n[XLM19]\n×\n×\nDL\n×\n×\nUnknown\n[GSAH17]\n×\nDL\n×\n×\n×\nUnknown\n[TMLZ18]\n×\nDL\n×\n×\nso(3)\n[ZLX*18]\n</>\n×\nDL\n×\n×\n3D positions\n[GMK*19]\n</>\n×\nDL\n×\n×\nso(3)\n[HGM19]\n</>\n×\nDL\n×\n×\n×\n3D positions\n[KGB19]\n</>\n×\nDL\n×\n×\n×\n×\n×\nso(3)\n[LWJ*19]\n</>\n×\nDL\n×\n×\n×\nse(3)\n[WHSZ19]\n×\nDL\n×\n×\n×\n×\n×\n3D positions\n[YKK*19]\n×\nDL\n×\n×\nUnknown\n[CGM*20]\n×\nDL\n×\n×\n×\n×\n3D positions\n[GWE*20]\n×\nDL\n×\n×\n×\n×\nQuaternions\n[KAS*20]\n×\nDL\n×\n×\n×\n3D positions\n[XXN*20]\n×\nDL\n×\n×\n×\nEuler Angles\n[HHS*17]\n×\n×\nDL\n×\n×\n×\n×\n3D positions\n[HYNP20]\n</>\n×\n×\nDL\n×\n×\n×\n×\nQuaternions\n[WCX21]\n×\n×\nDL\n×\n×\n×\n×\nUnknown\n[THR06]\n</>\n×\nDL\n×\n×\n×\nso(3)\n[TH09]\n×\nDL\n×\n×\n×\nso(3)\n[CM11]\n×\nDL\n×\n×\nso(3)\n[ALP15]\n×\nDL\n×\n×\nso(3)\n[HSK16]\n</>\n×\nDL\n×\n×\n×\n3D positions\n[TCHG17]\n×\nDL\n×\n×\n×\n2D positions\n[BKL18]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions\n[YRV*18]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions\n[DHS*19]\n×\nDL\n×\n×\n×\n×\n×\n3D positions\n[YLX*19]\n×\nDL\n×\n×\n×\n×\nUnknown\n[ASS*20]\n</>\n×\nDL\n×\n×\n×\n×\nQuaternions\n[HAB20]\n</>\n×\nDL\n×\n×\n×\n×\n×\n×\n3D positions\n[WACD20]\n</>\n×\nDL\n×\n×\n×\n×\nEuler Angles\n[WYZ*20]\n</>\n×\nDL\n×\n×\n×\n×\nUnknown\n[YK20]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions\nSection\nDataset\nArchitecture\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n16\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nGating Network\nExpert Pool\nMotion Prediction Network\nInputs:\nmotion, controls, \nactions, etc.\nα1 α2 α3 ... αn\nxt\nBlending \ncoefficients\nNetwork \nWeights\nBlending\nOptional\nEncoder\nPose at time\nt+1\nxt\nxt+1\nxt –2\nxt –1\nFigure 9: Overall idea of the mixture-of-experts scheme used in [ZSKS18; SZKS19; SZKZ20; LZZL20]: a gating network ﬁrst computes\nblending weights for a number of expert networks, based on a number of features extracted from the current frame xt. Each expert specialises\nin a particular movement. They are then blended together to dynamically compute the weights of a motion prediction network, which outputs\ninformation relevant to the next frame xt+1 including the pose of the animated character. This output is typically fed back into the network\n(autoregression) at time t +1.\n4. Character Control\nControlling character motions that react naturally to user inputs,\nwhile accounting for environment constraints as well as biome-\nchanical limitations, is another challenge involved in creating be-\nlievable virtual characters. In this section, we explore this topic\nthrough three main axes: how to make characters move and interact\nwith the virtual environment using kinematic (Section 4.1), phys-\nical (Section 4.2) or biomechanical (Section 4.3) control. Finally,\nwe summarise the methods presented in this section in Table 3.\n4.1. Kinematics-based\nKinematic approaches typically produce motions as joint angles,\nbased on a set of motion examples and high-level controls (e.g.\nuser inputs, interactions with the environment). Because of the re-\nquirement of generating motions in an online fashion when con-\ntrolling characters in video games or other interactive applications,\nRNNs and other autoregressive models are often considered to\nbe more appropriate than CNNs, as the future pose is predicted\nfrom the previous motion as well as a control signal. For instance,\nLee et al. [LLL18] used a four-layer LSTM model for controlling\ncharacters playing basketball and tennis. However, as mentioned\nin Section 3.2, such approaches often tend to fail in the long run,\nas errors in the prediction are fed back into the input and accumu-\nlate, eventually either converging to an average pose or introducing\nhigh frequency artifacts. According to Starke et al. [SZKS19], these\nmodels also often suffer from low responsiveness due to the large\nvariation of the memory state in the case of interactive character\ncontrol, as the internal memory state is high dimensional.\nTo overcome these limitations, Holden et al. [HKS17] proposed\nthe use of a specialised architecture called Phase-Functioned Neu-\nral Network (PFNN), which provides the phase variable to repre-\nsent the progression of the motion. In their seminal work, the phase\nis deﬁned based on alternating foot contacts, and used to generate\nthe weights of the regression network at each frame. A trade-off\nbetween compactness and runtime speed can then be achieved by\nprecomputing the phase-function for a number of ﬁxed intervals,\nthen interpolating the precomputed elements at runtime. One ma-\njor limitation of PFNN is that phase functions need to be manually\ndeﬁned, which can be in some cases extremely complex [ZSKS18;\nSZKZ20]. Zhang et al. [ZSKS18] therefore proposed to rely on a\nmixture-of-experts scheme to dynamically compute the weights of\na motion prediction network (see Figure 9 for an illustration of the\ngeneral concept). In their architecture, a gating network ﬁrst com-\nputes blending weights for a number of expert networks, each spe-\ncialising in a particular movement. This approach was ﬁrst demon-\nstrated for creating complex quadruped character controllers and\nthen extended by Starke et al. [SZKS19] to compute goal-directed\nseries of motions and transitions, while potentially interacting with\nthe environment. The idea was pushed one step further through the\nuse of local motion phases [SZKZ20], which are deﬁned based on\nhow each body part contacts external objects. Unlike previous ap-\nproaches where different actions are considered to be synchronised\nby a single global phase variable, their approach describes each\nmotion by a set of multiple independent and local phases for each\nbone. It then enables neural networks to learn asynchronous move-\nments of each bone, as well as its interaction with external elements\nof the virtual environment. While the previous approaches relied\non autoregressive DNNs to generate controlled character motions,\nLing et al. [LZCvdP20] demonstrated that a VAE using a similar\nmixture-of-experts scheme is also viable to produce stable high-\nquality human motions, while being usable in a DRL context to\nproduce goal-directed motions.\nSimultaneously, a few approaches explored the creation of con-\ntrollable expressive human motions from high-level semantic fac-\ntors. For instance, Alemi and Pasquier [AP17] trained a FCRBM on\na dataset of motion capture data containing movements from dif-\nferent subjects, expressions, and trajectories. This model can then\nbe used to generate modulated walking movements in real time.\nMason et al. [MSZ*18] explored a similar question from the per-\nspective of generating characters moving in different styles when\nthere is little data available for a new style and proposed a few-shot\nlearning approach. The goal of few-shot learning is to learn (part\nof) a model able to generalise out of a single or very few exam-\nples. In their work, Mason et al. [MSZ*18] adapted a pre-trained\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n17\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nPFNN (modelling style-independent components of the motions),\ncoupled with a set of residual adapters (modelling style-dependent\ncomponents) learned separately for each new style.\nWhile some of the approaches mentioned above have been\ndemonstrated to be compatible with multi-character control,\nsuch character interactions are typically handled by directly in-\ncluding information about the other characters’ relative posi-\ntions [SZKZ20], or indirectly through information about objects\nboth characters are interacting with and the action to be per-\nformed [LLL18; SZKZ20]. Wang et al. [WCX17] also proposed\nto generate character interactions based on the history motion data\nof both characters, relying on a variant of the ERD architecture to\nimprove animation stability, where multiple LSTM layers consti-\ntute the recurrent network.\nFinally, despite the impressive advances made by the aforemen-\ntioned methods, traditional animation approaches are still com-\nmonly used in animation pipelines to control human characters be-\ncause of the quality of the motions produced. However, novel ap-\nproaches, such as Learned Motion Matching [HKPP20], have re-\ncently begun to be explored with the goal of breaking down and\nreplacing individual components of animation algorithms by indi-\nvidual specialised neural networks. They balance the advantages of\nmore traditional approaches with the scalability of neural network\nbased models.\n4.2. Physics-based\nPhysics-based approaches generate animations in agreement with\nphysical laws. All but a few methods in this section rely\non multibody simulations for physical coherence. DL has also\nbeen combined with optimisation-based motion control to gen-\nerate physically coherent animations. Early works such as\nGrzeszczuk et al. [GTH98] or Peng et al. [PBvdP16] pioneered the\nuse of deep learning in physically realistic animation. While these\nwere limited to animals, humans were quickly considered as well.\nMultibody Simulations. The actuators of a multibody system are\ntypically driven by a DNN trained by Reinforcement Learning\n(RL), where human characters commonly have between 22 and 62\nDOFs. In a nutshell, Reinforcement Learning (RL) is a framework\nto learn to control a system by maximising a reward signal. There-\nfore designing the reward is an important aspect of RL algorithms.\nThe learned model (i.e. the agent) takes as inputs observations (i.e.\nthe states) from the system and outputs actions that impact the sys-\ntem. In the animation context, the states are often a combination\nof proprioceptive information (e.g. positions, angles, velocities and\nangular velocities of the joints and the root of the body, end-effector\ncontacts), the phase of the gait, as well as external information (e.g.\nterrain height, interacting object information) and task information\n(e.g. path to follow, direction of the goal). Similarly, the reward is\noften a combination of physics-based penalties (e.g. losing balance,\nusing more energy to produce a motion) and task-speciﬁc rewards\n(e.g. matching a reference motion, satisfying a desired velocity for\nlocomotion). Typically the actions are used to control the character\nand consist of either torques applied at the joints of the multibody\nsystem or target angles for said joints that are converted to torques\nby Proportional-Derivative (PD) controllers. The dynamics of the\nControl Policy\nPhysical\nstates\nPolicy Learning\nWeights update\nTarget pose or torques\nTorques\nLearning\nAlgorithm\nPhysical\nSimulation\nOptional\nPD Controller\nOptional\nUser Controls\nSimulation\nFigure 10: Illustration of the typical architecture for animation us-\ning multi-body simulations. A Deep Neural Network (DNN) takes\nas input information about the state of the character, its surround-\nings and optionally user inputs such as the desired direction. It then\ngenerates either joint torques or target poses fed to a Proportional-\nDerivative (PD) controller. Due to the feedback loop, the model is\ntrained by Reinforcement Learning (RL).\nmultibody system is then computed by a physical simulator (e.g.\nBullet or MuJoCo), as illustrated in Figure 10.\nTraining a DRL model is different from supervised DL. Rather\nthan using a labeled dataset, the model is typically trained by in-\nteracting with the environment and improves at the same time. The\nmost popular DRL algorithm for animation is Proximal Policy Op-\ntimisation (PPO) [SWD*17]. Common practices to improve train-\ning include imitating reference motion capture data; using two con-\ntrollers: a low-level one for locomotion or low-level actions and a\nhigher one for long term goals; enforcing symmetry of the motion;\nusing a curriculum, that is, learning increasingly difﬁcult tasks;\nclever state initialisation; early termination based on the lack of\nend-effector contacts or low rewards received.\nOptimisation-based Motion Control. These methods ﬁrst com-\npute a set of constraints (e.g. footstep positions) based on the en-\nvironment and a simpliﬁed model of the character dynamics (e.g.\nan inverted pendulum model). The animation is then generated by\nsolving a constrained optimisation problem combining these trajec-\ntory and character-level constraints. DL is used here to accelerate\nand approximate one or several computationally demanding steps.\n4.2.1. Character Locomotion\nLocomotion was the ﬁrst task addressed by multibody simulations\nand DRL. Peng and van de Panne [PvdP17] ﬁrst compared dif-\nferent action spaces for planar locomotion of bipeds and animals:\ntorques, target joint angles, target joint angle velocities and muscle-\nactivations. They observed that using target joint angles as action\nspace performs well and is the most robust in all cases. It is also\nthe fastest to train in 5 out of 7 cases. Biped locomotion was then\nquickly considered in 3D. Several works have developed two-level\ncontrollers. For instance, Peng et al. [PBYV17] used a high-level\ncontroller to output future footsteps, while a low-level controller\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n18\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\ntrained to imitate reference motions is used to actuate the character.\nMerel et al. [MTT*17] also constructed low-level controllers that\nimitate reference motion clips and control actuation. However, mul-\ntiple low-level controllers are trained using generative adversarial\nimitation learning. Each controller is trained on a speciﬁc skill such\nas walking, turning, running, and the high-level controller selects\nthe most appropriate instance. Similarly, Merel et al. [MAP*19]\ntrained a character that only perceives its environment through ego-\ncentric vision, where a LSTM-based vision-driven high-level con-\ntroller learns to select among a number of low-level controllers that\nhave learned to imitate short motion clips. Low-level policies can\nalso be combined [PCZ*19] by multiplying their distribution over\nactions and exponentially weighting them by a weight computed by\nanother neural network. Bergamin et al. [BCHF19] proposed to se-\nlect short motion capture clips using a motion matching approach.\nTarget angles are then extracted from the selected clip and a subset\nof these angles are corrected by a DRL model to stabilise the result-\ning motion. Chentanez et al. [CMM*18] used a similar approach to\nreproduce a reference clip provided as input, i.e. without including\na mechanism for automatic clip selection.\nImproving physical character locomotion has also been ex-\nplored through the use of curricula, training scenarii where the\nagent is requested to perform a sequence of typically increas-\ningly difﬁcult tasks. For instance, building a curriculum of en-\nvironments [HTS*17] can help to train an agent that is able\nto move forward and run, jump, crouch and turn as needed.\nXie et al. [XLKvdP20] also compared four environment generat-\ning curricula to learn to walk over stepping stones. The resulting\npolicy can then be used to walk over difﬁcult terrain where footstep\nlocations are given. The state includes the height of the pelvis with\nrespect to the lowest foot and the reward includes incentives for\nplacing the foot at the center of a stone and progressing towards the\nnext one. Curricula can also be constructed by assisting the charac-\nter. Yu et al. [YTL18] trained a model to generate symmetric and\nlow-energy motion to create more realistic motion without using\nmotion capture data. Symmetry is encouraged by an additional loss\nfunction. The curriculum is automatically created by including and\nreducing forces that help with forward motion and left-right bal-\nance, leading to different gaits emerging for different target veloc-\nities. Abdolhosseini et al. [ALX*19] also addressed learning sym-\nmetric gaits by either duplicating observed data by symmetry or by\nenforcing symmetry in the network. As the approaches discussed\nso far typically train one model for each morphology, Won and\nLee [WL19] developed a parametric DRL controller allowing the\nshape of the character to be modiﬁed during locomotion. At the\nheart of the training algorithm is an adaptive body shape sampling\nmechanism that progressively restricts the agent to body shapes it\nhas not mastered yet.\nOther types of approaches have also been explored in the com-\nmunity. Rajamäki and Hämäläinen [RH17; RH19] developed a\nsampling-based approach for locomotion: a Monte Carlo Tree\nSearch explores possible actions to maximise rewards over multiple\nfuture time steps. In the tree, each child node is one time step ahead\nof its parent. The impact of a sequence of candidate actions is eval-\nuated by physical simulations. The expansion of the tree (that is, the\ntrajectories explored) is driven in part by GAN models. After a set\nbudget, the most promising action is selected for the next time step.\nBabadi et al. [BNH19] later leveraged Rajamäki and Hämäläinen’s\napproach to train a DRL network that directly predicts the next ac-\ntion, without looking ahead, as most models in this section do. The\nMonte Carlo Tree Search approach is used to quickly produce ad-\nequate reference motions. Plausible states are extracted from these\nmotions and serve as initial states during training of a DRL model.\nThe authors highlighted a signiﬁcant reduction in training time.\nA few approaches also explored the use of DL-based opti-\nmisation techniques to physically animate walking characters.\nKwon et al. [KLvdP20] ﬁrst computed rough successive center\nof mass and footstep positions using an inverted pendulum on a\ncart model, reﬁned these estimates and added contact forces, tim-\nings and durations using a centroidal dynamics model, then mod-\niﬁed these estimates to take external forces into account and ﬁ-\nnally generated a full-body motion using an IK solver. A DNN\nwas trained to approximate these last two steps for real time per-\nformance. Mordatch et al. [MLA*15] trained an RNN model to\npredict the next pose of a character to generate physically realistic\ntrajectories. Their model was not trained to reproduce the output\nof a given dynamical system. Instead, they jointly generated phys-\nically realistic trajectories and trained the network on these trajec-\ntories, which were generated by solving an optimisation problem\nbalancing physical realism (equations of motion, non-penetration,\nand force complementarity) and the stability of the neural network.\n4.2.2. Other Motions\nMore complex types of motion have also been considered in addi-\ntion to or instead of locomotion. Peng et al. [PALvdP18] trained\nDRL models to imitate motion clips. Their approach is applied to\nlocomotion and various acrobatics and martial arts skills. The re-\nward of the training algorithm includes both an imitation term and\na task speciﬁc one. The model is trained incrementally, starting by\nthe end of the motion. Three approaches are proposed to sequence\nskills: 1) using the closest clip to compute the imitation reward,\n2) using a skill selection vector and 3) learning one policy per skill\nand using at every time step the policy expected to produce the\nhighest rewards based on the current state. Peng et al. [PKM*18]\ndeveloped a similar approach based on videos rather than motion\nclips. For each video clip, 2D and 3D poses are extracted and a 3D\nreference pose trajectory is reconstructed by optimising in the la-\ntent space of an autoencoder. Furthermore, the dataset is augmented\nby rotations and a curriculum is automatically constructed by an-\nother agent trained to propose initial states. Ma et al. [MYT*21]\nproposed to impose space-time bounds around samples of a trajec-\ntory rather than an imitation reward to learn from reference mo-\ntions. These bounds only allow small deviations in joint angles and\nin the positions of the center of mass and end effectors. Learn-\ning is possible using only a binary reward tied to the respect of\nthese bounds and early termination. Furthermore, to learn differ-\nent styles, they proposed different additional rewards to steer the\nmodel towards animations resulting in a different energy level or\nconvex hull volume. They also sampled initial states to favour states\nfrom which the current model achieves relatively lower rewards.\nRanganath et al. [RXKZ19] compressed the action space using\nprincipal or independent component analysis, reducing the size of\nthe output of the DRL model. Their method is otherwise similar\nto [PALvdP18].\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n19\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nWhile some approaches learn a different model for each motion\nclip, Merel et al. [MHG*19] trained a single controller able to ex-\necute many skills. They trained a VAE embedding current and fu-\nture states from motion capture clips and decoding the action from\nthe embedding and the current state. They then trained a high-level\ncontroller to set the value of the latent variable based on the task.\nOn a side note, Wang et al. [WMR*17] learned a similar embedding\n(although based on previous rather than future states), but used it\nonly for reproducing a given clip, not to automatically generate new\nanimations. Won et al. [WGH20] took as input a motion controller\nand trained a controller for a physically simulated character that\ncan reproduce all motions of the input controller. At runtime, the\ntrained controller is used to track such a motion. They used a two-\nlevel model by clustering generated motions, training one model by\ncluster and combining them in a mixture. They demonstrated their\nresults on walking, running, acting and performing various dance\nstyles. They also compared additive to multiplicative combination\nof different reward terms and chose the latter, arguing that it forces\nall reward terms to be high and that they obtained better results with\ncomplex motions such as dancing.\n4.2.3. Interactions\nDRL has also been used to animate characters interacting with\nother characters and complex objects. On a side note, some meth-\nods already described above have been applied to simple interac-\ntions such as pushing an object [PBYV17; PKM*18; PCZ*19].\nLiu and Hodgins [LH17] tackled animating a character walking\non a ball, balancing on a bongo board or skateboarding by gen-\nerating control fragments from motion capture data and training\na controller to select a generated control fragment at runtime. A\ncontrol fragment is a short (0.1s) segment of motion capture data\nand an associated linear control policy. The same authors [LH18]\nlater addressed basketball dribbling. One controller is ﬁrst learned\nfor the body and legs. Then, a second controller is trained for the\narms. Both are based on control fragments extracted from motion\ncapture data. For these two controllers, the actions are corrected\nby a linear model and a DRL model, respectively. Transitions be-\ntween control fragments are also learned. Basketball has also been\naddressed by Park et al. [PRL*19], together with obstacle racing,\nchicken hopping, ﬁghting with other characters and various other\nskills. A DRL controller is trained to correct target poses generated\nby a RNN and then fed to PD controllers. The RNN uses multi-\nobjective character control similar to [LLL18]. Pushing character\ninteraction much further, Haworth et al. [HBM*20] used multi-\nagent DRL to train a two-level model for crowd animation inspired\nby Peng et al. [PBYV17], where the low-level controls one charac-\nter based on two future footstep targets provided by the high-level\ncontroller. The latter is speciﬁc for each character whereas the for-\nmer is shared. The input of the high-level controller includes a ve-\nlocity ﬁeld of nearby objects and agents.\nMerel et al. [MTA*20] extended their previous work [MAP*19]\nto object manipulation tasks from egocentric vision. It in-\nvolves a hierarchical controller based on a latent space (similar\nto [MHG*19]), learning tasks step by step and task variations. No-\ntably, the object is not part of the state. The character can move\nobjects between shelves or catch and throw balls.\nInteractions with objects of the scene have also been explored\nwith respect to clothes, in order to animate an upper human body\ndressing up [CYT*18]. The key ideas include using haptic infor-\nmation in the state, dividing the task into subtasks, using the end\nstates of one subtask as initial states for training the next one and\nincluding cloth deformation in the reward.\n4.3. Biomechanics-based\nConcurrently to joint-actuated approaches for motion control, neu-\nral networks have also been explored in the context of muscu-\nloskeletal simulation. The general idea is that the complexity of\ntraditional musculoskeletal models usually leads to costly simula-\ntions, and that neural network controllers can provide an efﬁcient\nsolution by learning biomimicking controls using data simulated\nofﬂine with a detailed biomechanical model, to efﬁciently output\noptimal muscle activations at runtime.\nLee and Terzopoulos [LT06] originally applied this idea to de-\nsign novel neuromuscular control models of the human head-neck\nsystem (comprising of 7 cervical vertebrae and 72 neck muscles)\nlearned in supervised frameworks, ﬁrst using shallow neural net-\nworks generating both pose and tone control signals for the move-\nments of the head. Then, Nakada and Terzopoulos [NT15] im-\nproved head stability in a larger range of situations using deep\nstacked denoising autoencoders. Nakada et al. [NZC*18] further\ngeneralised the concept to full-body human biomechanical simu-\nlation (193 bone model actuated by a total of 823 muscles) by\ndesigning a sensorimotor control system made of 20 fully con-\nnected DNNs. In their framework, 5 DNNs per retina are respon-\nsible for extracting visual information required to direct eye/head,\narm and leg movements, while 10 additional DNNs are responsi-\nble for the neuromuscular control of the 216 neck muscles, of the\n29 muscles of each arm, and of the 39 muscles of each leg. More\nspeciﬁcally, the limbs and head are driven by one voluntary motor\nDNN each, responsible for generating efferent activation signals\nfor the corresponding muscles to execute realistic controlled move-\nments, and one reﬂex motor DNN each, responsible for computing\nmuscle activation adjustments due to low-level muscle dynamics.\nLee et al. [LPLL19] also explored full-body musculoskeletal sim-\nulation using DRL, with a two-level approach presented to learn\nsimultaneously trajectory mimicking and muscle coordination.\nFinally, motion realism brought by such musculoskeletal ap-\nproaches generally comes at the expense of complex modelling\nand costly simulations, which might explain to some extent why\njoint-actuation approaches are still most commonly used. To bridge\nthe gap between both worlds, and “generate human-like motion\ncomparable to muscle-actuation models, while retaining the ben-\neﬁt of simple modelling and fast computation offered by joint-\nactuation models”, Jiang et al. [JVDL19] proposed an approach to\nformulate the optimal control problem in the joint-actuation space\nwhile having an equivalent solution to the same problem in the\nmuscle-actuation space. Both the metabolic energy function and\nstate-dependent torque limits expressed in the joint-actuation space\nare approximated using fully connected DNNs and learned in a DL\nframework, and then used in conjunction with a policy learning al-\ngorithm built upon previous work [YTL18] (See section 4.2).\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n20\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nTable 3: Summary of the methods presented in Section 4. Miscellaneous data includes hand-crafted, synthetic, proprietary, unspeciﬁed or\nother public datasets.\nReference\nCode\nFramework\n4.1 Kinematics-based\n4.2 Physics-based\n4.3 Biomechanics-based\nCMU [Uni03]\nHolden et al. [HSK16]\nMiscellaneous\nFully Connected\nRecurrent\nRBM\nVariational Autoencoder\nAdversarial\nSimulator\nDimensionality\nHumanoid(s)\nOther biped(s)\nMonoped(s)\nQuadruped(s)\n[AP17]\n×\nDL\n×\n×\n3D\n×\n[HKS17]\n</>\n×\nDL\n×\n×\n3D\n×\n[WCX17]\n×\nDL\n×\n×\n×\n3D\n×\n[LLL18]\n×\nDL\n×\n×\n3D\n×\n[MSZ*18]\n</>\n×\nDL\n×\n×\n3D\n×\n[ZSKS18]\n</>\n×\nDL\n×\n×\n3D\n×\n[SZKS19]\n</>\n×\nDL\n×\n×\n3D\n×\n[HKPP20]\n×\nDL\n×\n×\n3D\n×\n[LZCvdP20]\n×\nDL & DRL\n×\n×\n×\n3D\n×\n[SZKZ20]\n×\nDL\n×\n×\n×\n3D\n×\n[MLA*15]\n×\nDL\n×\n×\nMuJoCo\n3D\n×\n×\n[HTS*17]\n×\nDRL\n×\nMuJoCo\n3D\n×\n×\n×\n[LH17]\n×\nDRL\n×\nODE\n3D\n×\n[MTT*17]\n×\nDRL\n×\n×\n×\nMuJoCo\n3D\n×\n[PBYV17]\n</>\n×\nDRL\n×\n×\nBullet\n3D\n×\n[PvdP17]\n×\nDRL\n×\n×\nUnknown\n2D\n×\n×\n[RH17]\n</>\n×\nDRL\n×\nODE\n3D\n×\n×\n×\n[WMR*17]\n×\nDRL\n×\n×\n×\n×\nMuJoCo\n3D\n×\n×\n[CMM*18]\n×\nDRL\n×\n×\nMuJoCo\n3D\n×\n[CYT*18]\n×\nDRL\n×\nDART\n3D\n×\n[LH18]\n×\nDRL\n×\n×\nODE\n3D\n×\n[PALvdP18]\n</>\n×\nDRL\n×\n×\nBullet\n3D\n×\n×\n×\n[PKM*18]\n</>\n×\nDRL\n×\n×\nBullet\n3D\n×\n[YTL18]\n</>\n×\nDRL\n×\nDART\n3D\n×\n×\n[ALX*19]\n</>\n×\nDRL\n×\n×\nBullet\n3D\n×\n×\n[BCHF19]\n×\nDRL\n×\n×\nBullet\n3D\n×\n[BNH19]\n</>\n×\nDRL\n×\n×\nODE\n3D\n×\n×\n×\n[MAP*19]\n×\nDRL\n×\n×\nMuJoCo\n3D\n×\n[MHG*19]\n×\nDRL\n×\n×\n×\nMuJoCo\n3D\n×\n[PCZ*19]\n×\nDRL\n×\n×\nBullet\n3D\n×\n×\n×\n[PRL*19]\n</>\n×\nDL & DRL\n×\n×\n×\nDART\n3D\n×\n[RH19]\n</>\n×\nDRL\n×\nODE\n3D\n×\n×\n×\n[RXKZ19]\n×\nDRL\n×\n×\n×\nUnknown\n3D\n×\n[WL19]\n×\nDRL\n×\nDART\n3D\n×\n×\n×\n[HBM*20]\n×\nDRL\n×\n×\nUnknown\n3D\n×\n[KLvdP20]\n</>\n×\nDL\n×\n×\nUnknown\n3D\n×\n×\n×\n×\n[MTA*20]\n</>\n×\nDRL\n×\n×\nMuJoCo\n3D\n×\n[WGH20]\n</>\n×\nDL & DRL\n×\n×\nBullet\n3D\n×\n[XLKvdP20]\n</>\n×\nDRL\n×\nBullet\n3D\n×\n×\n[MYT*21]\n×\nDRL\n×\n×\nBullet\n3D\n×\n[LT06]\n×\nDL\n×\n×\nUnknown\n3D\n×\n[NT15]\n×\nDL\n×\n×\nUnknown\n3D\n×\n[NZC*18]\n×\nDL\n×\n×\nUnknown\n3D\n×\n[JVDL19]\n</>\n×\nDL & DRL\n×\n×\nOpenSim\n3D\n×\n[LPLL19]\n</>\n×\nDL & DRL\n×\n×\nOpenSim\n3D\n×\nSection\nDataset\nArchitecture\nModel & Simulation\n5. Motion Editing\nSo far we looked at how motion data can be synthesised, either\nfor predictive or generative purposes, and how to build frameworks\nfor interactively controlling virtual characters. Besides these ap-\nplications, another important area of character animation is mo-\ntion editing, which diversiﬁes the creative capabilities of artists. In\nthis section, we focus on DL-based approaches for motion editing\nproblems divided into three topics: motion cleaning (Section 5.1),\nmotion retargeting (Section 5.2) and style transfer (Section 5.3).\nFinally, we summarise the methods presented in this section in Ta-\nble 4.\n5.1. Cleaning\nAs\nmentioned\nby\nseveral\nauthors,\nprojection\nto\nand\nin-\nverse projection from learnt manifolds of human motion can\nbe used to clean motion data, e.g. to perform operations\nsuch as denoising, ﬁxing corrupted information or ﬁlling in\nmissing motion sequences. Such problems have for instance\nbeen explored using RBMs [WN15], convolutional autoen-\ncoders [HSKJ15], temporal autoencoders [BBKK17; LAT21],\nspatio-temporal RNNs [WHSZ19], sequential RNNs [JL20], BiL-\nSTMs [LZZ*19], as well as RNN-based GANs [WCX21]. While\nmost approaches typically clean motion data through direct pro-\njection and inverse projection (e.g. [WN15; HSKJ15]), then ﬁx\nresidual errors as a post-process, it is also possible to include ad-\nditional constraints during training. For instance, it is possible to\nenforce bone length constraints and smoothness by including spe-\nciﬁc loss functions [LZZ*19; LZZL20], or to include an addi-\ntional perceptual loss measuring the difference in high-level fea-\ntures extracted by a pre-trained perceptual autoencoder [LZZL20],\nwhich improves overall visual quality at the cost of a slight in-\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n21\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\ncrease in reproduction error. Lohit et al. [LAT21] also proposed\nto optimise the latent representation to minimise the error between\nthe reconstructed sequence and the correct information of the in-\nput sequence, which they applied to ﬁlling missing joint trajecto-\nries. While most approaches focus on cleaning directly kinemat-\nics skeletal data, Holden [Hol18] proposed an approach producing\njoint transforms directly from raw marker data, in a way which is\nrobust to errors in the input data. This approach is based on learning\na deep denoising feedforward neural network using marker loca-\ntions synthetically reconstructed from motion capture data, where\nthe marker data is corrupted in terms of occlusions and positional\nshifts.\nSeveral authors also proposed to train neural networks to au-\ntomatically detect ground contact events (i.e. whether the feet\nare in contact with the ﬂoor or not), in order to prevent foots-\nliding artefacts. These approaches typically rely on motion data\naugmented with foot contact information to output foot contact\nprobabilities. To this end, foot contact can be either manually\nannotated [SCNW19], or automatically computed using different\nheuristics, usually based on empirical positional [YKL21] or ve-\nlocity [ZYC*20] thresholds. Using such data, different architec-\ntures have been proposed to automatically estimate foot contacts,\nsuch as relying on a fully connected neural network [SCNW19],\non a temporal CNN with residual connections [ZYC*20], or on\nan RNN with GRUs followed by linear layers and ReLU activa-\ntion [YKL21]. At runtime, the network estimates the ground con-\ntact information of each pose, which are then often used within an\nIK framework to remove footskate artifacts [SCNW19; YKL21].\nThis information can also be included as an additional zero veloc-\nity loss within a state-of-the-art method for pose and shape estima-\ntion [ZYC*20], or combined with other constraints into a physics-\nbased optimisation [SGXT20]. Shi et al. [SAA*20] also identiﬁed\nthe importance of foot contacts to mitigate footskating artifacts,\nand proposed a network predicting simultaneously joint rotations,\nglobal root positions, as well as foot contact labels from estimated\n2D joint positions, which are then fed into an integrated FK layer\nthat outputs 3D positions.\n5.2. Retargeting\nRetargeting [Gle98] refers to the task of transferring the movements\nof a source character in a skeletal animation sequence to a target\ncharacter with a different morphology, i.e. to a skeleton with differ-\nent bone lengths and possibly a different topology. For clarity, and\nconsistently with the literature, in the remainder of this section the\nsource character will be referred to as A and the target character\nas B. As pointed out by Aberman et al. [ALL*20], there is no for-\nmal speciﬁcation of the task. The purpose at large is to abstract out\nthe dynamics of the source sequence and to reproduce it on a char-\nacter whose body proportions differ. Effectively, the sought goal is\nto synthesise a retargeted sequence for the new morphology whose\nmotion mimics the source while remaining visually plausible and\nnatural. Since it is difﬁcult in practice to obtain ground-truth pairs\nof (source, target) sequences with exactly the same motion, most\nlearning approaches to retargeting, and all the schemes surveyed in\nthis section, rely on unpaired training data without motion corre-\nspondences across characters.\nIn the seminal work of Villegas et al. [VYCL18], the retarget-\ning network is built around two RNNs. An encoder RNN captures\nthe motion context of the source sequence in its hidden state and\nforwards it to a decoder RNN that outputs each frame of the retar-\ngeted sequence. A reference pose of the target skeleton is provided\nto the decoder. Besides regularisation loss terms, network training\nis driven by an adversarial loss and a cycle consistency loss. The ad-\nversarial loss attempts to minimise discrepancies between the joint\nvelocities of ground truth (true) and retargeted (fake) sequences.\nThe cycle consistency loss ensures that a motion sequence of char-\nacter A that is retargeted to B and then back to A remains as close\nas possible to the original.\nLim et al. [LCC19] and Kim et al. [KPKH20] reported that\nthe above approach tends to generate unrealistic motion. To mit-\nigate this issue, Lim et al. [LCC19] retargeted the motion of the\nroot joint separately from the poses at each time step, and com-\nbined the results to construct the output sequence. The retargeted\nposes are computed as joint rotations, represented as quaternions,\nto be applied to a reference pose of the target skeleton. The cy-\ncle consistency loss of Villegas et al. [VYCL18] is replaced by a\nreconstruction loss associated to self-retargeting to the same char-\nacter. Kim et al. [KPKH20] argued that CNNs are better suited than\nRNNs for retargeting because they can more accurately capture the\nshort-term motion dependencies that mostly condition the perfor-\nmance of the task. Accordingly, their scheme relies on a purely\nconvolutional network with temporally dilated convolutions that re-\ntargets whole motion sequences in one batch.\nAberman et al. [ALL*20] extended the scope of retargeting to\nskeletons with different topologies, subject to the condition that all\nconsidered topologies are homeomorphic. This implies that they\ncan all be reduced to a common primal skeleton by merging pairs of\nadjacent bones. Their representation of skeletal motion based on ar-\nmatures separates the temporally invariant bone offset vectors that\ndeﬁne the character morphology from the time-varying bone rota-\ntions at each joint that capture the motion dynamics (see Figure 3).\nThese two components are processed in distinct parallel branches\nof the retargeting network. Importantly, a motion sequence is mod-\nelled as a graph whose edges correspond to armatures. This pro-\nvides a principled formalism for processing motion data sampled\non the skeletal graph. The retargeting network includes three types\nof modules: space-time graph-convolutional operators acting on\nspatio-temporal joint neighbourhoods, graph pooling and graph un-\npooling operators. Graph pooling merges features of two adjacent\nedges (armatures) into a single feature. Each graph unpooling op-\nerator is designed as the inverse of a graph pooling operator, split-\nting one edge into two adjacent edges whose features are copied\nfrom the original feature. An autoencoder, composed of an encoder\nfollowed by a decoder, is learnt for every skeletal structure rep-\nresented in the training dataset. An encoder fed with motion data\nfor character A generates embeddings of the static bone offset and\ndynamic joint rotation components for this motion, expressed in\nthe common primal skeleton topology. A decoder for character B\nmaps these embeddings to the retargeted motion for this character.\nRetargeting is achieved by composing the encoder for the source\ncharacter with the decoder for the target character.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n22\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nTable 4: Summary of the methods presented in Section 5. Miscellaneous data includes hand-crafted, synthetic, proprietary, unspeciﬁed or\nother public datasets.\nReference\nCode\nFramework\nRepresentation\n5.1 Cleaning\n5.2 Retargeting\n5.3 Style Transfer\nCMU [Uni03]\nHuman3.6M [IPOS14]\nHDM05 [MRC*07]\nHolden et al. [HSK16]\nMixamo [Ado]\nNTU RGB+D [SLNW16]\nMiscellaneous\nFully Connected\nConvolutional\nRecurrent\nAutoencoder\nRBM\nVariational Autoencoder\nAdversarial\n[HSKJ15]\n×\nDL\n×\n×\n×\n3D positions\n[WN15]\n×\nDL\n×\n×\n×\nUnknown\n[BBKK17]\n×\nDL\n×\n×\n×\nso(3)\n[Hol18]\n×\nDL\n×\n×\n×\nEuler Angles\n[LZZ*19]\n×\nDL\n×\n×\n×\n3D positions\n[WHSZ19]\n×\nDL\n×\n×\n×\n×\n×\n3D positions\n[JL20]\n</>\n×\nDL\n×\n×\n×\n×\nso(3)\n[LZZL20]\n×\nDL\n×\n×\n×\n×\n3D positions\n[SAA*20]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions & Quaternions\n[SGXT20]\n×\nDL\n×\n×\n×\n2D positions\n[ZYC*20]\n</>\n×\nDL\n×\n×\n×\n2D positions\n[LAT21]\n×\nDL\n×\n×\n×\n×\n×\n2D positions\n[WCX21]\n×\nDL\n×\n×\n×\n×\nUnknown\n[YKL21]\n×\nDL\n×\n×\n×\nAd hoc\n[SCNW19]\n×\n×\nDL\n×\n×\n3D positions\n[VYCL18]\n</>\n×\nDL\n×\n×\n×\n×\nQuaternions\n[LCC19]\n</>\n×\nDL\n×\n×\n×\n×\nQuaternions\n[ALL*20]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions & Quaternions\n[KPKH20]\n</>\n×\nDL\n×\n×\n×\n×\nQuaternions\n[HSK16]\n</>\n×\nDL\n×\n×\n×\n3D positions\n[HHKK17]\n×\nDL\n×\n×\n×\n3D positions\n[WCAD18]\n×\nDL\n×\n×\n×\n×\nUnknown\n[AWL*20]\n</>\n×\nDL\n×\n×\n×\n×\n3D positions & Quaternions\n[DAS*20]\n×\nDL\n×\n×\n×\nQuaternions\n[WACD20]\n</>\n×\nDL\n×\n×\n×\n×\nEuler Angles\nSection\nDataset\nArchitecture\n5.3. Style Transfer\nNeural Style Transfer refers to algorithms manipulating data\nsuch as images, videos or human animations with DNNs to\nmake the stylistic components look like another data sample.\nGatys et al. [GEB16] ﬁrst introduced a method to perform neu-\nral style transfer on images, using the Gram matrix of the deep\nfeatures as the artistic style information of an image. In human an-\nimation, style transfer aims at transferring the style from one mo-\ntion sequence to another whose content is retained, called hereafter\nstyle and content motion sequences, respectively. For example, we\nmight want to edit a particular motion by affecting the state of mind\nof the character (e.g. enthusiastic, sad, angry) while preserving the\nperformed action, e.g. locomotion from point A to point B.\nIn\nthe\ncontext\nof\nDL-based\nskeletal\nanimation,\nHolden et al. [HSK16] pioneered human motion style trans-\nfer using their deterministic generation model (see Section 3.3).\nIn this framework, different types of constraints can be applied\non the generated motion, such as trajectory, bone lengths or\njoint positions, solving a constrained optimisation problem in a\nlearnt human motion manifold. Gradients are back-propagated\nthrough an autoencoder representing the manifold to optimise\nlatent representations. Style transfer constitutes a particular\ncase, where both joint positions and style are constrained with\nrespect to the content and style motion sequences, respectively.\nMoreover, Holden et al. [HSK16] followed prior work in image\nstyle transfer [GEB16] by using the Gram matrix in the latent\nrepresentation as a style similarity measure and thus do not\nrequire style annotations. They further improved this approach by\ntraining a feedforward network to perform style transfer thousands\nof times faster [HHKK17]. Gradients are back-propagated to\ntrain the feedforward network instead of optimising the latent\nrepresentation.\nAlternatively to the Gram matrix, style annotations can also\nbe used to guide the learning of a style transfer model.\nSmith et al. [SCNW19] divided the task of style transfer into spa-\ntial and temporal style variations networks, both taking as inputs\njoint positions as well as a one-hot style vector, where the networks\nare applied consecutively to predict corresponding style variations.\nHowever, this approach requires motion data registered with similar\nposes in different styles. Wang et al. [WCAD18; WACD20] lever-\naged an LSTM-based Sequential Adversarial Autoencoder whose\nencoder learns to map motions to separate content and style en-\ncodings: two additional discriminators are trained to recover style\nlabels from content and style encodings, respectively. The encoder\ntries to fool the former and helps the latter in order to free the con-\ntent encoding from the style information while preserving it in the\nstyle encoding. At runtime, both the content and style motion se-\nquences are encoded into separate content and style embeddings.\nThe decoder combines the content embedding of the content mo-\ntion sequence and the style embedding of the style motion sequence\nto produce the style transfer output. Following these works, Aber-\nman et al. [AWL*20] also extracted content and style encodings but\nwith two separate encoders. Furthermore, the style encoder learns\na common embedding from both 2D and 3D joint positions with a\ntriplet loss, which enables style extraction from videos. The output\nmotion is synthesised from the content encoding while the style\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n23\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nis controlled by the style encoding through temporally invariant\nAdaptive Instance Normalisation (AdaIN). Moreover, a multi-style\ndiscriminator assesses the output motion style.\nBeyond general-purpose style transfer, Dong et al. [DAS*20] fo-\ncused on translation from adult to child motions and vice versa.\nIn this particular case, retargeting is not sufﬁcient since it does\nnot capture the natural stylistic differences between adults and\nchildren [DAS*20]. This work leverages the capacity of Cycle-\nGAN [ZPIE17] to learn the mapping between adult and child mo-\ntion distributions without paired training data, which is critical due\nto the very limited availability of child data.\n6. Discussion\nIn this state-of-the-art review we explored the recent and promising\ntrends to address challenges in skeleton-based human animation\nwith DL and DRL. After a general overview of pose representa-\ntions and motion data processing with DNNs, we covered motion\nsynthesis, character control and motion editing based on DL and/or\nDRL. In this section, we discuss some of the limitations and poten-\ntial future work directions.\nFirst, a pose/motion representation should characterise the hu-\nman motion as precisely as possible, while being suitable for op-\ntimisation (e.g. avoid the error accumulation problem) and other\ncommon applications such as skinning and rigging. However, no\nsimple representation fulﬁlls all these requirements. The success\nof different approaches [PGA18; PFAG20; ALL*20] suggests that\nexpressing loss functions in the joint positions space is helpful, al-\nthough the joint orientations are more expressive to represent hu-\nman poses. To better animate human characters, it is necessary to\ndevelop more general pose/motion representation frameworks suit-\nable for learning both temporal and spatial patterns, probably com-\nbining the advantages of hierarchical orientations and absolute po-\nsitions, such as the approach proposed by Aberman et al. [ALL*20]\nfor skeletal convolutions in the case of motion retargeting (see Fig-\nure 3). Finally, the spectral domain of human motion remains as of\ntoday almost unexplored [MLSL19; MLS20; CHW*20] whereas,\ne.g. artifacts related to high-frequencies like noise or footskate\nmight be easier to prevent and/or remove in such a domain.\nSecond, approaches in short-term motion prediction attempt to\nforecast future motion exactly, with a ground truth being the only\nsolution. Many of them are autoregressive, i.e. (partial) outputs are\nfed back to the model to predict distant future motion frames. As a\nresult, they suffer from instability and tend to either diverge or con-\nverge to a mean pose. This is exacerbated when targeting longer\nhorizons since the problem of deterministic prediction becomes\nmore and more ambiguous. Ill-posedness can be mitigated either by\nadding contextual information or by modeling and sampling from a\ndistribution of possible future outcomes. Hopefully, both short and\nlong term motion prediction could be achieved by a single powerful\nstochastic model with a quasi-deterministic short-term behaviour,\nfrom which stochasticity could emerge in the long term.\nThird, important aspects of generative models in motion synthe-\nsis include the quality (e.g. perceptual plausibility, absence of ar-\ntifacts), the intermodal and intramodal diversity, and the level of\nrepresentation detail (e.g. ample arm movements versus subtle ﬁn-\nger manipulations), but also the ability to control high-level pa-\nrameters in synthesised motions. Currently, generative models still\nfail to fully represent the human motion distribution in the con-\nsidered scope, with all the diversity and modes it is made of. One\nreason for this is the lack of in-the-wild motion data, mainly be-\ncause reliable motion capture systems generally require markers to\nbe placed on the subject captured and a dedicated room for a multi-\ncamera setup. Future advances in marker-less motion capture from\nimage/video might help to collect more and more diverse motion\ndata, and hopefully to build strong human motion generative mod-\nels, maybe directly from image/video. Moreover, generative mod-\nels would also beneﬁt from motion data with higher skeleton res-\nolution, i.e. from capturing more joints. This would both improve\nthe quality of skinned character animation based on skeletal motion\ndata and enable models to account for more subtle details of human\nmotion, especially those expressed by the hands, the feet and the\nhead. Another promising opening to obtain favorable results with\nlimited amount of in-the-wild data might be self-supervised learn-\ning. In such a framework, a representation of the data would ﬁrst be\nlearnt over in-the-lab data (i.e. the pretext task) then ﬁne-tuned over\nin-the-wild data (i.e. the downstream task). Although advances in\nDL allow to model the human motion distribution with increasing\ndiversity and ﬁdelity, general-purpose approaches are most of the\ntime too general and lack ﬂexibility. Indeed, the difﬁculty to em-\nbed hard constraints in DL, e.g. desired precise action(s) or phys-\nical correctness, and the tendency of neural networks to be black\nboxes prevent animators from adjusting model behaviours to spe-\nciﬁc needs or from interactively editing the generated sequences.\nFourth, unlike with deep generative models used in motion syn-\nthesis, interactivity is at the core of character control with mod-\nels dynamically reacting to user input ﬂows while trying to pro-\nvide diverse motions that are accurate, physically realistic and per-\nformed like real humans would do in a similar situation. On the\none hand physics and biomechanics based approaches rely on the\nlaws of physics – often into multibody physical simulations – with\nactuation models, while on the other hand kinematics-based ap-\nproaches directly produce character motion without hard physi-\ncal constraints. As of today, the former provide humanoid con-\ntrollers that are mostly correct physically speaking since hard con-\nstraints are part of the models themselves. Although these con-\ntrollers e.g. manage locomotion, jumping and speciﬁc actions such\nas lifting, carrying, or pushing objects, they often fail to imi-\ntate human naturalness. It appears to us that the approaches clos-\nest to providing natural human motion rely more on real-world\ndata besides their physical model, e.g. the framework proposed\nby Peng et al. [PKM*18] which consists in imitating motions ex-\ntracted from video using deep pose estimation or the data-driven\ncontroller proposed by Bergamin et al. [BCHF19] where the policy\nnetwork computes corrective offsets added to reference motions.\nApproaches in kinematics-based character control are exclusively\ndata-driven, which makes it easier to obtain more natural results.\nHowever, the lack of physical models/simulation prevents produced\nanimations from being physically correct. In practice, real-world\nanimation pipelines still mostly prefer traditional rather than au-\ntomated methods due to their unpredictable behaviours and lower\nquality, although promising advances have been made with DL\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n24\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\nand DRL. Finally, we could expect approaches in character con-\ntrol to increasingly rely on (data-driven) deep generative models\ntogether with biomechanical or physical models (e.g. [JVDL19;\nLPLL19; PRL*19; WGH20]) to bring strengths from both types\nof approaches and converge to diverse, natural, high-quality and\nphysically-plausible controllers. Further advances on the ﬂexibility\nor the interactivity at runtime are also expected.\nFifth, motion editing has still not been explored very ex-\ntensively even though promising methods have been published\n(e.g. [HSKJ15; HSK16; ALL*20; AWL*20; DAS*20]). Yet this\ntopic is important to empower animators in other ﬁelds of human\nanimation. A signiﬁcant contribution in this respect is the motion\nediting framework proposed by Holden et al. [HSK16], where low-\nlevel motion parameters learnt by a motion modelling network are\nmapped to high-level, human understandable controls by means of\na disambiguation network. This latter network is trained separately\nand its inputs can be ﬁne-tuned to the editing task at hand. To the\nbest of our knowledge, this work has not been followed up. In our\nopinion, the main remaining challenges are the interpretability and\ncontrollability of the method, especially in a professional content\nproduction workﬂow. Interpretability refers to determining some\nrelationship between the latent representation and the physical con-\ntrol, while controllability refers to the explicit control of animations\ngiven some latent representation. To the best of our knowledge, no\nmethod has achieved these two goals at the moment. Removing or\npreventing artifacts is also a critical part of motion editing, e.g. for\nmotion data pre-processing or post-processing in animation work-\nﬂows. As an example, footskate artifacts are widespread but there\nis still no successful fully automated method to reliably ﬁx this ar-\ntifact in motion data. In the future we expect a growing number\nof works in motion editing at large, and maybe even new topics to\nemerge like retargeting did 20 years ago.\nLastly, most of the studies covered in this survey suffer from\na lack of perceptual and user studies, which are crucial. For in-\nstance, in motion prediction most works compare their results to\nothers using statistical metrics, such as the mean squared error over\npredicted joint angles, which are not suited to evaluate how plausi-\nble or natural predicted motions would be perceived. Other topics\nalso mostly skip perceptual and user studies although sometimes no\nground truth exists, e.g. character control. This is particularly strik-\ning in motion generation where ground truth is equivocal and the\ninception score is mainly used to assess the diversity of synthesised\nmotions. We therefore expect more perceptual and user studies as\nwell as performance metrics involving perceptual cues in the future.\nTo conclude this survey, we have presented a summary of the ad-\nvances made over the last few years in human animation based on\ndeep neural networks, either trained using DL or DRL. These ap-\nproaches are skyrocketing today in the ﬁeld of animation, and will\nprobably increasingly ﬁnd their place in real-world applications in\nthe entertainment industry to animate synthetic characters.\nAcknowledgements\nThis work was supported by the European Commission under\nEuropean Horizon 2020 Programme, grant number 951911 -\nAI4Media.\nAcronyms\nBiLSTM\nBidirectional LSTM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8, 21\nCNN\nConvolutional Neural Network . . . . . . . . . . . 6-8, 10, 11, 17, 22\nCRBM\nConditional RBM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13, 14\nDCT\nDiscrete Cosine Transform . . . . . . . . . . . . . . . . . . . . . . . . . . 8, 10\nDL\nDeep Learning . . . . . . . . . . . . . . . 1, 2, 4, 5, 9, 11, 18-20, 23-25\nDNN\nDeep Neural Network. . . . . .1, 2, 4-6, 8, 10, 14, 17-20, 22, 24\nDOF\nDegree of Freedom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-4, 18\nDRL\nDeep Reinforcement Learning. . . . . .1, 2, 4, 10, 17-20, 24, 25\nERD\nEncoder-Recurrent-Decoder . . . . . . . . . . . . . 8, 9, 11-13, 15, 18\nFCRBM\nFactored CRBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13, 14, 17\nFK\nForward Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . 4, 5, 10, 22\nGAN\nGenerative Adversarial Network . . . . . . . . 9, 12, 13, 15, 19, 21\nGCN\nGraph Convolutional Network . . . . . . . . . . . . . . . . . . . 6, 7, 9-11\nGMM\nGaussian Mixture Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12\nGRU\nGated Recurrent Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-11, 22\nHFCRBM\nHierarchical FCRBM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14\nIK\nInverse Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 19, 22\nLSTM\nLong Short-Term Memory . . . . . . . . . . . 7-9, 13, 15, 17, 18, 23\nMCMC\nMarkov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nML\nMachine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2, 9\nNLP\nNatural Language Processing . . . . . . . . . . . . . . . . . . . . . . 1, 8, 10\nPD\nProportional-Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18, 20\nPFNN\nPhase-Functioned Neural Network . . . . . . . . . . . . . . . . . . . . . .17\nRBM\nRestricted Boltzmann Machine. . . . . . . . . . . . . . . . . . . . . .13, 21\nRL\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nRNN\nRecurrent Neural Network . . . . . . . . . . . . . . . . . 6-15, 17, 19-22\nSeq2Seq\nSequence to Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-11\nVAE\nVariational Autoencoder . . . . . . . . . . . . . . 11, 12, 14, 15, 17, 19\nReferences\n[AAR*20] ADELI, VIDA, ADELI, EHSAN, REID, IAN, et al. “Socially\nand Contextually Aware Human Motion and Pose Forecasting”. IEEE\nRobotics and Automation Letters 5.4 (July 2020), 6033–6040. DOI: 10.\n1109/LRA.2020.3010742 8, 11, 16.\n[Ado] ADOBE. Mixamo. Accessed: 2021-09-16. URL: https://www.\nmixamo.com 6, 23.\n[AII*18] ANDRILUKA, MYKHAYLO, IQBAL, UMAR, INSAFUTDINOV,\nELDAR, et al. “PoseTrack: A Benchmark for Human Pose Estimation\nand Tracking”. Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition. IEEE Computer Society, June 2018, 5167–\n5176. DOI: 10.1109/CVPR.2018.00542 6.\n[AKH19] AKSAN, EMRE, KAUFMANN, MANUEL, and HILLIGES, OT-\nMAR. “Structured Prediction Helps 3D Human Motion Modelling”. Pro-\nceedings of the 2019 IEEE/CVF International Conference on Computer\nVision (ICCV). IEEE Computer Society, Oct. 2019, 7143–7152. DOI:\n10.1109/ICCV.2019.00724 7, 9, 16.\n[ALL*20] ABERMAN, KFIR, LI, PEIZHUO, LISCHINSKI, DANI, et al.\n“Skeleton-Aware Networks for Deep Motion Retargeting”. ACM Trans-\nactions on Graphics 39.4 (July 2020). DOI: 10 . 1145 / 3386569 .\n3392462 4, 5, 7, 8, 22–25.\n[ALP15] ALEMI, OMID, LI, WILLIAM, and PASQUIER, PHILIPPE.\n“Affect-expressive movement generation with factored conditional Re-\nstricted Boltzmann Machines”. Proceedings of the IEEE International\nConference on Affective Computing and Intelligent Interaction. IEEE\nComputer Society, Sept. 2015, 442–448. DOI: 10 . 1109 / ACII .\n2015.7344608 4, 6, 14, 16.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n25\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[ALX*19] ABDOLHOSSEINI, FARZAD, LING, HUNG YU, XIE, ZHAOM-\nING, et al. “On Learning Symmetric Locomotion”. Proceedings of the\nACM SIGGRAPH Conference on Motion, Interaction and Games. As-\nsociation for Computing Machinery, Oct. 2019. DOI: 10 . 1145 /\n3359566.3360070 19, 21.\n[AP17] ALEMI, OMID and PASQUIER, PHILIPPE. “WalkNet: A Neural-\nNetwork-Based Interactive Walking Controller”. Proceedings of the In-\nternational Conference on Intelligent Virtual Agents. Springer Interna-\ntional Publishing, Aug. 2017, 15–24. DOI: 10.1007/978-3-319-\n67401-8_2 4, 17, 21.\n[AP19] ALEMI, OMID and PASQUIER, PHILIPPE. “Machine Learning for\nData-Driven Movement Generation: a Review of the State of the Art”.\narXiv e-prints (Mar. 2019). eprint: 1903.08356 2.\n[ASS*20] ALIAKBARIAN, SADEGH, SALEH, FATEMEH SADAT, SALZ-\nMANN, MATHIEU, et al. “A Stochastic Conditioning Scheme for Di-\nverse Human Motion Prediction”. Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. June 2020, 5223–5232.\nDOI: 10.1109/CVPR42600.2020.00527 4, 5, 8, 14, 16.\n[AWL*20] ABERMAN, KFIR, WENG, YIJIA, LISCHINSKI, DANI, et al.\n“Unpaired Motion Style Transfer from Video to Animation”. ACM\nTransactions on Graphics 39.4 (2020). DOI: 10.1145/3386569.\n3392469 4, 8, 23, 25.\n[BBKK17] BÜTEPAGE, JUDITH, BLACK, MICHAEL J., KRAGIC, DAN-\nICA, and KJELLSTRÖM, HEDVIG. “Deep Representation Learning for\nHuman Motion Prediction and Classiﬁcation”. Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. IEEE Com-\nputer Society, July 2017, 1591–1599. DOI: 10.1109/CVPR.2017.\n173 4, 5, 7, 8, 10, 11, 16, 21, 23.\n[BBL*17] BRONSTEIN, MICHAEL, BRUNA, JOAN, LECUN, YANN, et al.\n“Geometric Deep Learning: Going beyond Euclidean data”. IEEE Signal\nProcessing Magazine 34.4 (Nov. 2017), 18–42. DOI: 10.1109/MSP.\n2017.2693418 7.\n[BCHF19] BERGAMIN, KEVIN, CLAVET, SIMON, HOLDEN, DANIEL,\nand FORBES, JAMES RICHARD. “DReCon: Data-Driven Responsive\nControl of Physics-Based Characters”. ACM Transactions on Graphics\n38.6 (Nov. 2019). DOI: 10.1145/3355089.3356536 19, 21, 24.\n[BGG*20] BOURACHED, ANTHONY, GRIFFITHS, RYAN-RHYS, GRAY,\nROBERT, et al. “Generative Model-Enhanced Human Motion Predic-\ntion”. NeurIPS Workshop on Interpretable Inductive Biases and Phys-\nically Structured Learning. Dec. 2020. eprint: 2010.11699 5, 11, 16.\n[BKL18] BARSOUM, EMAD, KENDER, JOHN, and LIU, ZICHENG. “HP-\nGAN: Probabilistic 3D Human Motion Prediction via GAN”. Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion Workshops. IEEE Computer Society, June 2018, 1499–1509. DOI:\n10.1109/CVPRW.2018.00191 8, 15, 16.\n[BNH19] BABADI, AMIN, NADERI, KOUROSH, and HÄMÄLÄINEN,\nPERTTU. “Self-Imitation Learning of Locomotion Movements through\nTermination Curriculum”. Proceedings of the ACM SIGGRAPH Con-\nference on Motion, Interaction and Games. Association for Computing\nMachinery, 2019. DOI: 10.1145/3359566.3360072 19, 21.\n[CAW*19] CHIU, HSU-KUANG, ADELI, EHSAN, WANG, BORUI, et al.\n“Action-Agnostic Human Pose Forecasting”. Proceedings of the IEEE\nWinter Conference on Applications of Computer Vision. IEEE Com-\nputer Society, Jan. 2019, 1423–1432. DOI: 10.1109/WACV.2019.\n00156 8, 10, 16.\n[CGM*20] CAO, ZHE, GAO, HANG, MANGALAM, KARTTIKEYA, et al.\n“Long-Term Human Motion Prediction with Scene Context”. Proceed-\nings of the European Conference on Computer Vision. Springer Inter-\nnational Publishing, Sept. 2020, 387–404. DOI: 10.1007/978- 3-\n030-58452-8_23 6, 12, 16.\n[CHW*20] CAI, YUJUN, HUANG, LIN, WANG, YIWEI, et al. “Learning\nProgressive Joint Propagation for Human Motion Prediction”. Proceed-\nings of the European Conference on Computer Vision. Springer Interna-\ntional Publishing, Sept. 2020. DOI: 10.1007/978-3-030-58571-\n6_14 5, 8, 10, 11, 16, 24.\n[CM11] CHIU, CHUNG-CHENG and MARSELLA, STACY. “A style con-\ntroller for generating virtual human behaviors”. Proceedings of the In-\nternational Conference on Autonomous Agents and Multiagent Systems.\nInternational Foundation for Autonomous Agents and Multiagent Sys-\ntems, May 2011, 1023–1030. URL: http://www.ifaamas.org/\nProceedings/aamas2011/papers/D7_G77.pdf 4, 14, 16.\n[CMM*18] CHENTANEZ, NUTTAPONG, MÜLLER, MATTHIAS, MACK-\nLIN, MILES, et al. “Physics-Based Motion Capture Imitation with Deep\nReinforcement Learning”. Proceedings of the ACM SIGGRAPH Con-\nference on Motion, Interaction and Games. Association for Computing\nMachinery, Nov. 2018. DOI: 10.1145/3274247.3274506 19, 21.\n[CPAM20] CORONA,\nENRIC,\nPUMAROLA,\nALBERT,\nALENYÀ,\nGUILLEM,\nand\nMORENO-NOGUER,\nFRANCESC.\n“Context-Aware\nHuman Motion Prediction”. Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. IEEE Computer Society, June\n2020, 6990–6999. DOI: 10.1109/CVPR42600.2020.00702 8, 10,\n11, 16.\n[CSK*21] CUI, QIONGJIE, SUN, HUAIJIANG, KONG, YUE, et al. “Efﬁ-\ncient human motion prediction using temporal convolutional generative\nadversarial network”. Information Sciences 545 (Feb. 2021), 427–447.\nDOI: 10.1016/j.ins.2020.08.123 4, 5, 8, 11, 16.\n[CSY20] CUI, QIONGJIE, SUN, HUAIJIANG, and YANG, FEI. “Learning\nDynamic Relationships for 3D Human Motion Prediction”. Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition. June 2020, 6519–6527. DOI: 10.1109/CVPR42600.2020.\n00655 4, 5, 7, 8, 10, 11, 16.\n[CYT*18] CLEGG, ALEXANDER, YU, WENHAO, TAN, JIE, et al. “Learn-\ning to Dress: Synthesizing Human Dressing Motion via Deep Reinforce-\nment Learning”. ACM Transactions on Graphics 37.6 (Dec. 2018). DOI:\n10.1145/3272127.3275048 20, 21.\n[DAS*20] DONG, YUZHU, ARISTIDOU, ANDREAS, SHAMIR, ARIEL, et\nal. “Adult2child: Motion Style Transfer using CycleGANs”. Proceed-\nings of the ACM SIGGRAPH Conference on Motion, Interaction and\nGames. Association for Computing Machinery, Oct. 2020. DOI: 10 .\n1145/3424636.3426909 6, 8, 23–25.\n[DHS*19] DU, HAN, HERRMANN, ERIK, SPRENGER, JANIS, et al.\n“Stylistic Locomotion Modeling and Synthesis Using Variational Gen-\nerative Models”. Proceedings of the ACM SIGGRAPH Conference on\nMotion, Interaction and Games. Association for Computing Machinery,\nOct. 2019. DOI: 10.1145/3359566.3360083 3, 8, 14, 16.\n[DKB14] DINH, LAURENT, KRUEGER, DAVID, and BENGIO, YOSHUA.\n“NICE: Non-linear Independent Components Estimation”. arXiv e-\nprints (Oct. 2014). eprint: 1410.8516 15.\n[FLFM15] FRAGKIADAKI, KATERINA, LEVINE, SERGEY, FELSEN,\nPANNA, and MALIK, JITENDRA. “Recurrent Network Models for Hu-\nman Dynamics”. Proceedings of the 2015 IEEE International Con-\nference on Computer Vision (ICCV). IEEE Computer Society, Dec.\n2015, 4346–4354. DOI: 10 . 1109 / ICCV . 2015 . 494 4, 8, 9, 11,\n16.\n[FP14] FOURATI, NESRINE and PELACHAUD, CATHERINE. “Emilya:\nEmotional Body Expression in Daily Actions Database”. Proceedings\nof the International Conference on Language Resources and Evaluation\n(LREC). European Language Resources Association, May 2014. URL:\nhttp://www.lrec-conf.org/proceedings/lrec2014/\nsummaries/334.html 6.\n[GC19] GUO, XIAO and CHOI, JONGMOO. “Human Motion Prediction via\nLearning Local Structure Representations and Temporal Dependencies”.\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 33.1 (July\n2019). DOI: 10.1609/aaai.v33i01.33012580 4, 5, 7, 8, 10, 16.\n[GEB16] GATYS, LEON A., ECKER, ALEXANDER S., and BETHGE,\nMATTHIAS. “Image Style Transfer Using Convolutional Neural Net-\nworks”. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. IEEE Computer Society, June 2016, 2414–2423.\nDOI: 10.1109/CVPR.2016.265 23.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n26\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[Gle98] GLEICHER, MICHAEL. “Retargetting Motion to New Characters”.\nProceedings of the Annual Conference on Computer Graphics and Inter-\nactive Techniques. Association for Computing Machinery, 1998, 33–42.\nDOI: 10.1145/280814.280820 1, 22.\n[GMK*19] GOPALAKRISHNAN, ANAND, MALI, ANKUR, KIFER, DAN,\net al. “A Neural Temporal Model for Human Motion Prediction”. Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE Computer Society, June 2019, 12108–12117. DOI:\n10.1109/CVPR.2019.01239 4, 8, 12, 16.\n[GP12] GEIJTENBEEK, THOMAS and PRONOST, NICOLAS. “Interactive\nCharacter Animation Using Simulated Physics: A State-of-the-Art Re-\nview”. Computer Graphics Forum 31.8 (Sept. 2012), 2492–2515. DOI:\n10.1111/j.1467-8659.2012.03189.x 2.\n[GPM*14] GOODFELLOW,\nIAN,\nPOUGET-ABADIE,\nJEAN,\nMIRZA,\nMEHDI, et al. “Generative Adversarial Nets”. Proceedings of the Inter-\nnational Conference on Neural Information Processing Systems. Cur-\nran Associates Inc., Dec. 2014, 2672–2680. URL: http://papers.\nnips.cc/paper/5423.pdf 12.\n[Gra98] GRASSIA, F. SEBASTIAN. “Practical Parameterization of Rota-\ntions Using the Exponential Map”. Journal of Graphics Tools 3.3 (Apr.\n1998), 29–48. DOI: 10.1080/10867651.1998.10487493 3, 4.\n[GSAH17] GHOSH, PARTHA, SONG, JIE, AKSAN, EMRE, and HILLIGES,\nOTMAR. “Learning Human Motion Models for Long-Term Predictions”.\nProceedings of the International Conference on 3D Vision. IEEE Com-\nputer Society, Oct. 2017, 458–466. DOI: 10 . 1109 / 3DV . 2017 .\n00059 8, 12, 16.\n[GTH98] GRZESZCZUK, RADEK, TERZOPOULOS, DEMETRI, and HIN-\nTON, GEOFFREY. “NeuroAnimator: Fast Neural Network Emulation and\nControl of Physics-Based Models”. Proceedings of the ACM Computer\nGraphics and Interactive Techniques. Association for Computing Ma-\nchinery, July 1998, 9–20. DOI: 10.1145/280814.280816 18.\n[GWE*20] GHORBANI, SAEED, WLOKA, CALDEN, ETEMAD, ALI, et al.\n“Probabilistic Character Motion Synthesis using a Hierarchical Deep La-\ntent Variable Model”. Computer Graphics Forum (Oct. 2020). DOI: 10.\n1111/cgf.14116 4, 5, 8, 12, 16.\n[GWLM18] GUI, LIANG-YAN, WANG, YU-XIONG, LIANG, XIAODAN,\nand MOURA, JOSÉ M. F. “Adversarial Geometry-Aware Human Motion\nPrediction”. Proceedings of the European Conference on Computer Vi-\nsion. Springer International Publishing, Sept. 2018, 823–842. DOI: 10.\n1007/978-3-030-01225-0_48 4, 8, 10, 11, 16.\n[GWRM18] GUI, LIANG-YAN, WANG, YU-XIONG, RAMANAN, DEVA,\nand MOURA, JOSÉ M. F. “Few-Shot Human Motion Prediction via\nMeta-Learning”. Proceedings of the European Conference on Computer\nVision. Springer International Publishing, Sept. 2018, 441–459. DOI:\n10.1007/978-3-030-01237-3_27 8, 10, 16.\n[HAB20] HENTER,\nGUSTAV\nEJE,\nALEXANDERSON,\nSIMON,\nand\nBESKOW, JONAS. “MoGlow: Probabilistic and controllable motion syn-\nthesis using normalising ﬂows”. ACM Transactions on Graphics 39.6\n(Nov. 2020). DOI: 10.1145/3414685.3417836 3, 8, 15, 16.\n[HBM*20] HAWORTH,\nBRANDON,\nBERSETH,\nGLEN,\nMOON,\nSEONGHYEON, et al. “Deep Integration of Physical Humanoid\nControl and Crowd Navigation”. Proceedings of the ACM SIGGRAPH\nConference on Motion, Interaction and Games. Association for Com-\nputing Machinery, Oct. 2020. DOI: 10.1145/3424636.3426894 8,\n20, 21.\n[HCTB19] HASSAN, MOHAMED, CHOUTAS, VASILEIOS, TZIONAS,\nDIMITRIOS, and BLACK, MICHAEL J. “Resolving 3D Human Pose\nAmbiguities With 3D Scene Constraints”. Proceedings of the 2019\nIEEE/CVF International Conference on Computer Vision (ICCV). IEEE\nComputer Society, Oct. 2019, 2282–2292. DOI: 10 . 1109 / ICCV .\n2019.00237 6.\n[HGM19] HERNANDEZ\nRUIZ,\nALEJANDRO,\nGALL,\nJÜRGEN,\nand\nMORENO, FRANCESC. “Human Motion Prediction via Spatio-Temporal\nInpainting”. Proceedings of the 2019 IEEE/CVF International Con-\nference on Computer Vision (ICCV). IEEE Computer Society, Oct.\n2019, 7133–7142. DOI: 10.1109/ICCV.2019.00723 8, 12, 16.\n[HHKK17] HOLDEN, DANIEL, HABIBIE, IKHSANUL, KUSAJIMA, IKUO,\nand KOMURA, TAKU. “Fast Neural Style Transfer for Motion Data”.\nIEEE Computer Graphics and Applications (CG&A) 37.4 (Aug.\n2017), 42–49. DOI: 10.1109/MCG.2017.3271464 3, 8, 23.\n[HHS*17] HABIBIE,\nIKHSANUL,\nHOLDEN,\nDANIEL,\nSCHWARZ,\nJONATHAN,\net\nal.\n“A\nRecurrent\nVariational\nAutoencoder\nfor\nHuman Motion Synthesis”. Proceedings of the British Machine\nVision Conference. BMVA Press, Sept. 2017, 119.1–119.12.\nDOI:\n10.5244/C.31.119 3, 6, 12, 14, 16.\n[Hin02] HINTON, GEOFFREY E. “Training products of experts by\nminimizing contrastive divergence”. Neural Computation 14.8 (Aug.\n2002), 1771–1800. DOI: 10.1162/089976602760128018 13.\n[HKPP20] HOLDEN, DANIEL, KANOUN, OUSSAMA, PEREPICHKA,\nMAKSYM, and POPA, TIBERIU. “Learned Motion Matching”. ACM\nTransactions on Graphics 39.4 (July 2020).\nDOI: 10 . 1145 /\n3386569.3392440 5, 18, 21.\n[HKS17] HOLDEN, DANIEL, KOMURA, TAKU, and SAITO, JUN. “Phase-\nFunctioned Neural Networks for Character Control”. ACM Transac-\ntions on Graphics 36.4 (July 2017). DOI: 10 . 1145 / 3072959 .\n3073663 4–6, 8, 17, 21.\n[Hol18] HOLDEN, DANIEL. “Robust Solving of Optical Motion Capture\nData by Denoising”. ACM Transactions on Graphics 37.4 (July 2018).\nDOI: 10.1145/3197517.3201302 22, 23.\n[HPP05] HSU, EUGENE, PILLI, KARI, and POPOVIC, JOVAN. “Style\nTranslation for Human Motion”. Proceedings of the Annual Confer-\nence on Computer Graphics and Interactive Techniques. Association\nfor Computing Machinery, July 2005, 1082–1089. DOI: 10 . 1145 /\n1186822.1073315 6.\n[HSK16] HOLDEN, DANIEL, SAITO, JUN, and KOMURA, TAKU. “A\nDeep Learning Framework for Character Motion Synthesis and Edit-\ning”. ACM Transactions on Graphics 35.4 (July 2016). DOI: 10.1145/\n2897824.2925975 3, 6, 8, 13–16, 21, 23, 25.\n[HSKJ15] HOLDEN, DANIEL, SAITO, JUN, KOMURA, TAKU, and JOYCE,\nTHOMAS. “Learning Motion Manifolds with Convolutional Autoen-\ncoders”. SIGGRAPH Asia Technical Briefs. Association for Computing\nMachinery, 2015. DOI: 10.1145/2820903.2820918 3, 21, 23, 25.\n[HTS*17] HEESS, NICOLAS, TB, DHRUVA, SRINIVASAN, SRIRAM, et al.\n“Emergence of Locomotion Behaviours in Rich Environments”. arXiv e-\nprints (July 2017). eprint: 1707.02286 8, 19, 21.\n[HXZ*20] HABERMANN,\nMARC,\nXU,\nWEIPENG,\nZOLLHÖFER,\nMICHAEL, et al. “DeepCap: Monocular Human Performance Capture\nUsing Weak Supervision”. Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. IEEE Computer Society, June\n2020, 5051–5062. DOI: 10.1109/CVPR42600.2020.00510 6.\n[HYNP20] HARVEY,\nFÉLIX\nGINGRAS,\nYURICK,\nMIKE,\nNOWROUZEZAHRAI,\nDEREK, and\nPAL,\nCHRISTOPHER. “Robust\nMotion In-betweening”. ACM Transactions on Graphics 39.4 (July\n2020). DOI: 10.1145/3386569.3392480 4, 6, 8, 13, 15, 16.\n[IPOS14] IONESCU, CATALIN, PAPAVA, DRAGOS, OLARU, VLAD, and\nSMINCHISESCU, CRISTIAN. “Human3.6M: Large Scale Datasets and\nPredictive Methods for 3D Human Sensing in Natural Environments”.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 36.7\n(July 2014), 1325–1339. DOI: 10.1109/TPAMI.2013.248 5, 6, 11,\n16, 23.\n[JL20] JANG, DEOK-KYEONG and LEE, SUNG-HEE. “Constructing Hu-\nman Motion Manifold With Sequential Networks”. Computer Graphics\nForum 39.2 (July 2020), 487–496. DOI: 10.1111/cgf.14028 4, 8,\n21, 23.\n[JVDL19] JIANG, YIFENG, VAN WOUWE, TOM, DE GROOTE, FRIEDL,\nand LIU, C. KAREN. “Synthesis of Biologically Realistic Human Mo-\ntion Using Joint Torque Actuation”. ACM Transactions on Graphics 38.4\n(July 2019). DOI: 10.1145/3306346.3322966 20, 21, 25.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n27\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[JZSS16] JAIN, ASHESH, ZAMIR, AMIR R., SAVARESE, SILVIO, and\nSAXENA, ASHUTOSH. “Structural-RNN: Deep Learning on Spatio-\nTemporal Graphs”. Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition. IEEE Computer Society, June\n2016, 5308–5317. DOI: 10 . 1109 / CVPR . 2016 . 573 4, 7–9, 11,\n16.\n[KAS*20] KAUFMANN, MANUEL, AKSAN, EMRE, SONG, JIE, et al.\n“Convolutional Autoencoders for Human Motion Inﬁlling”. Proceedings\nof the International Conference on 3D Vision. IEEE Computer Society,\nNov. 2020, 918–927. DOI: 10.1109/3DV50981.2020.00102 3,\n13, 16.\n[KGB19] KUNDU, JOGENDRA NATH, GOR, MAHARSHI, and BABU,\nR. VENKATESH. “BiHMP-GAN: Bidirectional 3D Human Motion Pre-\ndiction GAN”. Proceedings of the AAAI Conference on Artiﬁcial In-\ntelligence 33.1 (July 2019). DOI: 10 . 1609 / AAAI . V33I01 .\n33018553 4, 5, 8, 12, 16.\n[KGP02] KOVAR,\nLUCAS,\nGLEICHER,\nMICHAEL,\nand\nPIGHIN,\nFRÉDÉRIC. “Motion Graphs”. Proceedings of the Annual Confer-\nence on Computer Graphics and Interactive Techniques. Vol. 21. 3.\nAssociation for Computing Machinery, July 2002, 473–482.\nDOI:\n10.1145/566654.566605 1.\n[KLvdP20] KWON, TAESOO, LEE, YOONSANG, and van de PANNE,\nMICHIEL. “Fast and Flexible Multilegged Locomotion Using Learned\nCentroidal Dynamics”. ACM Transactions on Graphics 39.4 (July 2020).\nDOI: 10.1145/3386569.3392432 19, 21.\n[KNH*21] KHAN, SALMAN, NASEER, MUZAMMAL, HAYAT, MU-\nNAWAR, et al. “Transformers in Vision: A Survey”. arXiv e-prints\n(2021). eprint: 2101.01169 10.\n[KPKH20] KIM, SANGBIN, PARK, INBUM, KWON, SEONGSU, and HAN,\nJUNGHYUN. “Motion Retargetting based on Dilated Convolutions and\nSkeleton-speciﬁc Loss Functions”. Computer Graphics Forum 39.2 (July\n2020), 497–507. DOI: 10.1111/cgf.13947 4, 8, 22, 23.\n[KSG*13] KARG, MICHELLE, SAMADANI, ALI-AKBAR, GORBET, ROB,\net al. “Body Movements for Affective Expression: A Survey of Au-\ntomatic Recognition and Generation”. IEEE Transactions on Affective\nComputing 4.4 (Nov. 2013), 341–359. DOI: 10 . 1109 / T - AFFC .\n2013.29 2.\n[KW14] KINGMA, DIEDERIK P. and WELLING, MAX. “Auto-Encoding\nVariational Bayes”. Proceedings of the International Conference on\nLearning Representations. Apr. 2014. URL: https://openreview.\nnet/forum?id=33X9fd2-9FyZd 14.\n[KW17] KIPF, THOMAS N. and WELLING, MAX. “Semi-Supervised\nClassiﬁcation with Graph Convolutional Networks”. Proceedings of the\nInternational Conference on Learning Representations. Apr. 2017. URL:\nhttps://openreview.net/forum?id=SJU4ayYgl 7.\n[LAT21] LOHIT, SUHAS, ANIRUDH, RUSHIL, and TURAGA, PAVAN. “Re-\ncovering Trajectories of Unmarked Joints in 3D Human Actions Using\nLatent Space Optimization”. Proceedings of the IEEE Winter Confer-\nence on Applications of Computer Vision. IEEE Computer Society, Jan.\n2021, 2341–2350. DOI: 10.1109/WACV48630.2021.00239 8,\n21–23.\n[LCC*21] LI, MAOSEN, CHEN, SIHENG, CHEN, XU, et al. “Symbiotic\nGraph Neural Networks for 3D Skeleton-based Human Action Recog-\nnition and Motion Prediction”. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (Jan. 2021), 1–1. DOI: 10.1109/TPAMI.\n2021.3053765 4, 5, 7, 10, 16.\n[LCC19] LIM, JONGIN, CHANG, HYUNG JIN, and CHOI, JIN YOUNG.\n“PMnet: Learning of Disentangled Pose and Movement for Unsuper-\nvised Motion Retargeting”. Proceedings of the British Machine Vi-\nsion Conference. BMVA Press, Sept. 2019, 136. URL: https : / /\nbmvc2019 . org / wp - content / uploads / papers / 0997 -\npaper.pdf 4, 8, 22, 23.\n[LCZ*20] LI, MAOSEN, CHEN, SIHENG, ZHAO, YANGHENG, et al. “Dy-\nnamic Multiscale Graph Neural Networks for 3D Skeleton Based Hu-\nman Motion Prediction”. Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition. IEEE Computer Society, June\n2020, 211–220. DOI: 10.1109/CVPR42600.2020.00029 4, 5,\n7, 8, 10, 11, 16.\n[LH17] LIU, LIBIN and HODGINS, JESSICA K. “Learning to Sched-\nule Control Fragments for Physics-Based Characters Using Deep Q-\nLearning”. ACM Transactions on Graphics 36.3 (June 2017). DOI: 10.\n1145/3083723 20, 21.\n[LH18] LIU, LIBIN and HODGINS, JESSICA K. “Learning Basketball\nDribbling Skills Using Trajectory Optimization and Deep Reinforce-\nment Learning”. ACM Transactions on Graphics 37.4 (July 2018). DOI:\n10.1145/3197517.3201315 20, 21.\n[LKS*20] LEBAILLY, TIM, KICIROGLU, SENA, SALZMANN, MATHIEU,\net al. “Motion Prediction Using Temporal Inception Module”. Proceed-\nings of the Asian Conference on Computer Vision. Springer International\nPublishing, Nov. 2020, 651–665. DOI: 10 . 1007 / 978 - 3 - 030 -\n69532-3_39 5, 10, 11, 16.\n[LLL18] LEE, KYUNGHO, LEE, SEYOUNG, and LEE, JEHEE. “Inter-\nactive Character Animation by Learning Multi-Objective Control”.\nACM Transactions on Graphics 37.6 (Dec. 2018). DOI: 10 . 1145 /\n3272127.3275071 5, 8, 17, 18, 20, 21.\n[LMR*15] LOPER,\nMATTHEW,\nMAHMOOD,\nNAUREEN,\nROMERO,\nJAVIER, et al. “SMPL: A Skinned Multi-Person Linear Model”. ACM\nTransactions on Graphics 34.6 (Oct. 2015).\nDOI: 10 . 1145 /\n2816795.2818013 6.\n[LPLL19] LEE, SEUNGHWAN, PARK, MOONSEOK, LEE, KYOUNGMIN,\nand LEE, JEHEE. “Scalable Muscle-Actuated Human Simulation and\nControl”. ACM Transactions on Graphics 38.4 (July 2019). DOI: 10.\n1145/3306346.3322972 20, 21, 25.\n[LSP*20] LIU, JUN, SHAHROUDY, AMIR, PEREZ, MAURICIO, et al.\n“NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activ-\nity Understanding”. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 42.10 (2020), 2684–2701. DOI: 10.1109/TPAMI.2019.\n2916873 6.\n[LT06] LEE, SUNG-HEE and TERZOPOULOS, DEMETRI. “Heads up!\nBiomechanical Modeling and Neuromuscular Control of the Neck”.\nACM Transactions on Graphics 25.3 (July 2006), 1188–1198. DOI: 10.\n1145/1141911.1142013 20, 21.\n[LWJ*19] LIU, ZHENGUANG, WU, SHUANG, JIN, SHUYUAN, et al. “To-\nwards Natural and Accurate Future Motion Prediction of Humans and\nAnimals”. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. IEEE Computer Society, June 2019, 9996–10004.\nDOI: 10.1109/CVPR.2019.01024 4, 8, 12, 16.\n[LWY*19] LI, YANRAN, WANG, ZHAO, YANG, XIAOSONG, et al. “Efﬁ-\ncient convolutional hierarchical autoencoder for human motion predic-\ntion”. The Visual Computer 35.6 (June 2019), 1143–1156. DOI: 10 .\n1007/s00371-019-01692-9 4, 5, 7, 9, 16.\n[LZCvdP20] LING, HUNG YU, ZINNO, FABIO, CHENG, GEORGE, and\nvan de PANNE, MICHIEL. “Character Controllers using Motion VAEs”.\nACM Transactions on Graphics 39.4 (July 2020). DOI: 10 . 1145 /\n3386569.3392422 5, 8, 17, 21.\n[LZL10] LI, WANQING, ZHANG, ZHENGYOU, and LIU, ZICHENG. “Ac-\ntion recognition based on a bag of 3D points”. Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops.\nIEEE Computer Society, June 2010, 9–14. DOI: 10.1109/CVPRW.\n2010.5543273 6.\n[LZLL18] LI, CHEN, ZHANG, ZHEN, LEE, WEE SUN, and LEE, GIM\nHEE. “Convolutional Sequence to Sequence Model for Human Dynam-\nics”. Proceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition. IEEE Computer Society, June 2018, 5226–5234. DOI:\n10.1109/CVPR.2018.00548 4, 5, 7, 10, 11, 16.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n28\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[LZZ*19] LI, SHUJIE, ZHOU, YANG, ZHU, HAISHENG, et al. “Bidirec-\ntional recurrent autoencoder for 3D skeleton motion data reﬁnement”.\nComputers & Graphics 81 (2019), 92–103. DOI: 10.1016/j.cag.\n2019.03.010 8, 21, 23.\n[LZZL20] LI, SHU-JIE, ZHU, HAI-SHENG, ZHENG, LI-PENG, and LI,\nLIN. “A Perceptual-Based Noise-Agnostic 3D Skeleton Motion Data\nReﬁnement Network”. IEEE Access 8 (2020), 52927–52940. DOI: 10.\n1109/ACCESS.2020.2980316 8, 17, 21, 23.\n[MAP*19] MEREL, JOSH, AHUJA, ARUN, PHAM, VU, et al. “Hierarchical\nVisuomotor Control of Humanoids”. Proceedings of the International\nConference on Learning Representations. May 2019. URL: https://\nopenreview.net/forum?id=BJfYvo09Y7 8, 19–21.\n[MBR17] MARTINEZ, JULIETA, BLACK, MICHAEL J., and ROMERO,\nJAVIER. “On Human Motion Prediction Using Recurrent Neural Net-\nworks”. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. IEEE Computer Society, July 2017, 4674–4683.\nDOI: 10.1109/CVPR.2017.497 4, 8–11, 16.\n[MC12] MIN, JIANYUAN and CHAI, JINXIANG. “Motion Graphs++: A\nCompact Generative Model for Semantic Motion Analysis and Synthe-\nsis”. ACM Transactions on Graphics 31.6 (Nov. 2012). DOI: 10.1145/\n2366145.2366172 1, 14.\n[MGT*19] MAHMOOD, NAUREEN, GHORBANI, NIMA, TROJE, NIKO-\nLAUS F., et al. “AMASS: Archive of Motion Capture As Surface\nShapes”. Proceedings of the 2019 IEEE/CVF International Conference\non Computer Vision (ICCV). IEEE Computer Society, Oct. 2019, 5441–\n5450. DOI: 10.1109/ICCV.2019.00554 6.\n[MHG*19] MEREL, JOSH, HASENCLEVER, LEONARD, GALASHOV,\nALEXANDRE, et al. “Neural Probabilistic Motor Primitives for Hu-\nmanoid Control”. Proceedings of the International Conference on Learn-\ning Representations. May 2019. URL: https://openreview.net/\nforum?id=BJl6TjRcY7 20, 21.\n[MLA*15] MORDATCH, IGOR, LOWREY, KENDALL, ANDREW, GALEN,\net al. “Interactive Control of Diverse Complex Characters with Neural\nNetworks”. Proceedings of the International Conference on Neural In-\nformation Processing Systems. The MIT Press, Dec. 2015, 3132–3140.\nDOI: 10.5555/2969442.2969589 19, 21.\n[MLS20] MAO, WEI, LIU, MIAOMIAO, and SALZMANN, MATHIEU.\n“History Repeats Itself: Human Motion Prediction via Motion Atten-\ntion”. Proceedings of the European Conference on Computer Vision.\nSpringer International Publishing, Sept. 2020, 474–489. DOI: 10 .\n1007/978-3-030-58568-6_28 4, 8, 10, 11, 16, 24.\n[MLSL19] MAO, WEI, LIU, MIAOMIAO, SALZMANN, MATHIEU, and\nLI, HONGDONG. “Learning Trajectory Dependencies for Human Mo-\ntion Prediction”. Proceedings of the 2019 IEEE/CVF International Con-\nference on Computer Vision (ICCV). IEEE Computer Society, Oct.\n2019, 9488–9496. DOI: 10.1109/ICCV.2019.00958 5, 7, 8, 10,\n11, 16, 24.\n[MRC*07] MÜLLER, MEINARD, RÖDER, TIDO, CLAUSEN, MICHAEL,\net al. Documentation Mocap Database HDM05. Tech. rep. CG-2007-2.\nUniversität Bonn, June 2007. URL: https://resources.mpi-\ninf.mpg.de/HDM05/07_MuRoClEbKrWe_HDM05.pdf 5, 6, 16,\n23.\n[MRC*17] MEHTA, DUSHYANT, RHODIN, HELGE, CASAS, DAN, et al.\n“Monocular 3D Human Pose Estimation In The Wild Using Improved\nCNN Supervision”. Proceedings of the International Conference on 3D\nVision. IEEE Computer Society, Oct. 2017, 506–516. DOI: 10.1109/\n3dv.2017.00064 6.\n[MSJG15] MAKHZANI,\nALIREZA,\nSHLENS,\nJONATHON,\nJAITLY,\nNAVDEEP, and GOODFELLOW, IAN. “Adversarial Autoencoders”.\narXiv e-prints (Nov. 2015). eprint: 1511.05644 15.\n[MSZ*18] MASON, IAN, STARKE, SEBASTIAN, ZHANG, HE, et al. “Few-\nshot Learning of Homogeneous Human Locomotion Styles”. Computer\nGraphics Forum 37.7 (Oct. 2018), 143–153. DOI: 10.1111/cgf.\n13555 4, 5, 17, 21.\n[MTA*20] MEREL,\nJOSH,\nTUNYASUVUNAKOOL,\nSARAN,\nAHUJA,\nARUN, et al. “Catch & Carry: Reusable Neural Controllers for Vision-\nGuided Whole-Body Tasks”. ACM Transactions on Graphics 39.4 (July\n2020). DOI: 10.1145/3386569.3392474 8, 20, 21.\n[MTD*15] MANDERY, CHRISTIAN, TERLEMEZ, ÖMER, DO, MARTIN,\net al. “The KIT whole-body human motion database”. Proceedings of\nthe International Conference on Advanced Robotics. July 2015, 329–\n336. DOI: 10.1109/ICAR.2015.7251476 6.\n[MTT*17] MEREL, JOSH, TASSA, YUVAL, TB, DHRUVA, et al. “Learning\nhuman behaviors from motion capture by adversarial imitation”. arXiv e-\nprints (July 2017). eprint: 1707.02201 19, 21.\n[MYT*21] MA, LI-KE, YANG, ZESHI, TONG, XIN, et al. “Learning and\nExploring Motor Skills with Spacetime Bounds”. Computer Graphics\nForum 40.2 (June 2021), 251–263. DOI: 10.1111/cgf.142630 19,\n21.\n[NT15] NAKADA, MASAKI and TERZOPOULOS, DEMETRI. “Deep\nLearning of Neuromuscular Control for Biomechanical Human Anima-\ntion”. Advances in Visual Computing. Springer International Publishing,\nDec. 2015, 339–348. DOI: 10.1007/978- 3- 319- 27857- 5_\n31 20, 21.\n[NZC*18] NAKADA, MASAKI, ZHOU, TAO, CHEN, HONGLIN, et al.\n“Deep Learning of Biomimetic Sensorimotor Control for Biomechanical\nHuman Animation”. ACM Transactions on Graphics 37.4 (July 2018).\nDOI: 10.1145/3197517.3201305 7, 20, 21.\n[OCK*13] OFLI, FERDA, CHAUDHRY, RIZWAN, KURILLO, GREGORIJ,\net al. “Berkeley MHAD: A comprehensive Multimodal Human Action\nDatabase”. Proceedings of the IEEE Winter Conference on Applications\nof Computer Vision. IEEE Computer Society, Jan. 2013, 53–60. DOI:\n10.1109/WACV.2013.6474999 6.\n[PALvdP18] PENG, XUE BIN, ABBEEL, PIETER, LEVINE, SERGEY, and\nvan de PANNE, MICHIEL. “DeepMimic: Example-Guided Deep Rein-\nforcement Learning of Physics-Based Character Skills”. ACM Trans-\nactions on Graphics 37.4 (July 2018). DOI: 10 . 1145 / 3197517 .\n3201311 19, 21.\n[PBvdP16] PENG, XUE BIN, BERSETH, GLEN, and van de PANNE,\nMICHIEL. “Terrain-Adaptive Locomotion Skills Using Deep Reinforce-\nment Learning”. ACM Transactions on Graphics 35.4 (July 2016). DOI:\n10.1145/2897824.2925881 18.\n[PBYV17] PENG, XUE BIN, BERSETH, GLEN, YIN, KANGKANG, and\nVAN DE PANNE, MICHIEL. “DeepLoco: Dynamic Locomotion Skills\nUsing Hierarchical Deep Reinforcement Learning”. ACM Transac-\ntions on Graphics 36.4 (July 2017). DOI: 10 . 1145 / 3072959 .\n3073602 18, 20, 21.\n[PCZ*19] PENG, XUE BIN, CHANG, MICHAEL, ZHANG, GRACE, et\nal. “MCP: Learning Composable Hierarchical Control with Multiplica-\ntive Compositional Policies”. Proceedings of the International Confer-\nence on Neural Information Processing Systems. Curran Associates Inc.,\n2019, 3686–3697. URL: http : / / papers . nips . cc / paper /\n8626.pdf 19–21.\n[PFAG20] PAVLLO,\nDARIO,\nFEICHTENHOFER,\nCHRISTOPH,\nAULI,\nMICHAEL, and GRANGIER, DAVID. “Modeling Human Motion with\nQuaternion-Based Neural Networks”. International Journal of Com-\nputer Vision (IJCV) 128.4 (Apr. 2020), 855–872. DOI: 10 . 1007 /\ns11263-019-01245-6 4, 5, 8, 10, 16, 24.\n[PGA18] PAVLLO, DARIO, GRANGIER, DAVID, and AULI, MICHAEL.\n“QuaterNet: A Quaternion-based Recurrent Model for Human Motion”.\nProceedings of the British Machine Vision Conference. BMVA Press,\nSept. 2018. URL: https : / / dblp . org / rec / conf / bmvc /\nPavlloGA18 4, 5, 8, 10–12, 16, 24.\n[PKM*18] PENG, XUE BIN, KANAZAWA, ANGJOO, MALIK, JITENDRA,\net al. “SFV: Reinforcement Learning of Physical Skills from Videos”.\nACM Transactions on Graphics 37.6 (Dec. 2018). DOI: 10 . 1145 /\n3272127.3275014 19–21, 24.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n29\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[PP10] PEJSA, TOMISLAV and PANDZIC, IGOR. “State of the Art in\nExample-Based Motion Synthesis for Virtual Characters in Interactive\nApplications”. Computer Graphics Forum 29.1 (Feb. 2010), 202–226.\nDOI: 10.1111/j.1467-8659.2009.01591.x 2.\n[PRL*19] PARK, SOOHWAN, RYU, HOSEOK, LEE, SEYOUNG, et al.\n“Learning Predict-and-Simulate Policies from Unorganized Human Mo-\ntion Data”. ACM Transactions on Graphics 38.6 (Nov. 2019). DOI: 10.\n1145/3355089.3356501 20, 21, 25.\n[PvdP17] PENG, XUE BIN and van de PANNE, MICHIEL. “Learning Lo-\ncomotion Skills Using DeepRL: Does the Choice of Action Space Mat-\nter?”: Proceedings of the ACM SIGGRAPH / Eurographics Symposium\non Computer Animation. Association for Computing Machinery, July\n2017. DOI: 10.1145/3099564.3099567 18, 21.\n[RH17] RAJAMÄKI, JOOSE and HÄMÄLÄINEN, PERTTU. “Augmenting\nSampling Based Controllers with Machine Learning”. Proceedings of the\nACM SIGGRAPH / Eurographics Symposium on Computer Animation.\nAssociation for Computing Machinery, July 2017. DOI: 10 . 1145 /\n3099564.3099579 19, 21.\n[RH19] RAJAMÄKI, JOOSE and HÄMÄLÄINEN, PERTTU. “Continuous\nControl Monte Carlo Tree Search Informed by Multiple Experts”. IEEE\nTransactions on Visualization and Computer Graphics (TVCG) 25.8\n(Aug. 2019), 2540–2553. DOI: 10.1109/TVCG.2018.2849386 19,\n21.\n[Rod40] RODRIGUES, OLINDE. “Des lois géométriques qui régissent les\ndéplacements d’un système solide dans l’espace, et de la variation des\ncoordonnées provenant de ces déplacements considérés indépendants\ndes causes qui peuvent les produire”. Journal de Mathématiques Pures\net Appliquées 5 (1840), 380–440. URL: https://eudml.org/doc/\n234443 3.\n[RXKZ19] RANGANATH, AVINASH, XU, PEI, KARAMOUZAS, IOANNIS,\nand ZORDAN, VICTOR. “Low Dimensional Motor Skill Learning Us-\ning Coactivation”. Proceedings of the ACM SIGGRAPH Conference on\nMotion, Interaction and Games. Association for Computing Machinery,\nOct. 2019. DOI: 10.1145/3359566.3360071 19, 21.\n[SAA*20] SHI, MINGYI, ABERMAN, KFIR, ARISTIDOU, ANDREAS, et\nal. “MotioNet: 3D Human Motion Reconstruction from Monocular\nVideo with Skeleton Consistency”. ACM Transactions on Graphics 40.1\n(Sept. 2020). DOI: 10.1145/3407659 5, 8, 22, 23.\n[SBB09] SIGAL, LEONID, BALAN, ALEXANDRU O., and BLACK,\nMICHAEL J. “HumanEva: Synchronized Video and Motion Capture\nDataset and Baseline Algorithm for Evaluation of Articulated Human\nMotion”. International Journal of Computer Vision (IJCV) 87.1 (Aug.\n2009), 4. DOI: 10.1007/s11263-009-0273-6 6.\n[SCNW19] SMITH, HARRISON JESSE, CAO, CHEN, NEFF, MICHAEL,\nand WANG, YINGYING. “Efﬁcient Neural Networks for Real-Time Mo-\ntion Style Transfer”. Proceedings of the ACM Computer Graphics and\nInteractive Techniques 2.2 (July 2019). DOI: 10.1145/3340254 3,\n22, 23.\n[SGXT20] SHIMADA, SOSHI, GOLYANIK, VLADISLAV, XU, WEIPENG,\nand THEOBALT, CHRISTIAN. “PhysCap: Physically Plausible Monocu-\nlar 3D Motion Capture in Real Time”. ACM Transactions on Graphics\n39.6 (Dec. 2020). DOI: 10.1145/3414685.3417877 8, 22, 23.\n[SLNW16] SHAHROUDY, AMIR, LIU, JUN, NG, TIAN-TSONG, and\nWANG, GANG. “NTU RGB+D: A Large Scale Dataset for 3D Hu-\nman Activity Analysis”. Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition. IEEE Computer Society, June\n2016, 1010–1019. DOI: 10.1109/CVPR.2016.115 5, 6, 16, 23.\n[Smo86] SMOLENSKY, PAUL. “Information processing in dynamical sys-\ntems: Foundations of harmony theory”. Parallel distributed process-\ning: Explorations in the microstructure of cognition. The MIT Press,\nJan. 1986, 194–281. URL: https : / / apps . dtic . mil / sti /\ncitations/ADA620727 13.\n[SNF*13] SHUMAN, DAVID I, NARANG, SUNIL K., FROSSARD, PAS-\nCAL, et al. “The Emerging Field of Signal Processing on Graphs: Ex-\ntending High-Dimensional Data Analysis to Networks and Other Irregu-\nlar Domains”. IEEE Signal Processing Magazine 30.3 (Apr. 2013), 83–\n98. DOI: 10.1109/MSP.2012.2235192 7.\n[SWD*17] SCHULMAN, JOHN, WOLSKI, FILIP, DHARIWAL, PRA-\nFULLA, et al. “Proximal policy optimization algorithms”. arXiv e-prints\n(July 2017). eprint: 1707.06347 18.\n[SZKS19] STARKE, SEBASTIAN, ZHANG, HE, KOMURA, TAKU, and\nSAITO, JUN. “Neural State Machine for Character-Scene Interactions”.\nACM Transactions on Graphics 38.6 (Nov. 2019). DOI: 10 . 1145 /\n3355089.3356505 5, 8, 17, 21.\n[SZKZ20] STARKE, SEBASTIAN, ZHAO, YIWEI, KOMURA, TAKU, and\nZAMA, KAZI. “Local Motion Phases for Learning Multi-Contact Char-\nacter Movements”. ACM Transactions on Graphics 39.4 (July 2020).\nDOI: 10.1145/3386569.3392450 5, 8, 17, 18, 21.\n[TCHG17] TOYER, SAM, CHERIAN, ANOOP, HAN, TENGDA, and\nGOULD, STEPHEN. “Human Pose Forecasting via Deep Markov Mod-\nels”. Proceedings of the International Conference on Digital Image\nComputing: Techniques and Applications. IEEE Computer Society, Dec.\n2017, 1–8. DOI: 10.1109/DICTA.2017.8227441 3, 6, 8, 14, 16.\n[TH09] TAYLOR, GRAHAM W. and HINTON, GEOFFREY E. “Factored\nConditional Restricted Boltzmann Machines for Modeling Motion\nStyle”. Proceedings of the International Conference on Machine Learn-\ning. Association for Computing Machinery, June 2009, 1025–1032. DOI:\n10.1145/1553374.1553505 3, 13, 16.\n[THR06] TAYLOR, GRAHAM W., HINTON, GEOFFREY E., and ROWEIS,\nSAM. “Modeling Human Motion Using Binary Latent Variables”. Pro-\nceedings of the International Conference on Neural Information Pro-\ncessing Systems. The MIT Press, Sept. 2006, 1345–1352. DOI: 10 .\n7551/mitpress/7503.003.0173 3, 13, 16.\n[TMLZ18] TANG, YONGYI, MA, LIN, LIU, WEI, and ZHENG, WEI-\nSHI. “Long-Term Human Motion Prediction by Modeling Motion Con-\ntext and Enhancing Motion Dynamics”. Proceedings of the Interna-\ntional Joint Conferences on Artiﬁcial Intelligence. AAAI Press, July\n2018, 935–941. DOI: 10.24963/ijcai.2018/130 4, 8, 12, 16.\n[Uni03] UNIVERSITY, CARNEGIE-MELLON. CMU Graphics Lab Mo-\ntion Capture Database. Accessed: 2021-09-16. 2003. URL: http://\nmocap.cs.cmu.edu 5, 6, 11, 16, 21, 23.\n[Uni19] UNIVERSITY, SIMON FRASER. SFU Motion Capture Database.\nAccessed: 2021-09-16. 2019. URL: https : / / mocap . cs . sfu .\nca/ 6.\n[vMHB*18] Von MARCARD, TIMO, HENSCHEL, ROBERTO, BLACK,\nMICHAEL, et al. “Recovering Accurate 3D Human Pose in The Wild\nUsing IMUs and a Moving Camera”. Proceedings of the European Con-\nference on Computer Vision. Springer International Publishing, Sept.\n2018, 614–631. DOI: 10.1007/978- 3- 030- 01249- 6_37 6,\n16.\n[VYCL18] VILLEGAS, RUBEN, YANG, JIMEI, CEYLAN, DUYGU, and\nLEE, HONGLAK. “Neural Kinematic Networks for Unsupervised Mo-\ntion Retargetting”. Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition. IEEE Computer Society, June 2018, 8639–\n8648. DOI: 10.1109/CVPR.2018.00901 4, 8, 22, 23.\n[WAC*19] WANG, BORUI, ADELI, EHSAN, CHIU, HSU-KUANG, et al.\n“Imitation Learning for Human Pose Prediction”. Proceedings of the\n2019 IEEE/CVF International Conference on Computer Vision (ICCV).\nIEEE Computer Society, Oct. 2019, 7124–7133. DOI: 10 . 1109 /\nICCV.2019.00722 4, 8, 10, 16.\n[WACD20] WANG, QI, ARTIÈRES, THIERRY, CHEN, MICKAEL, and DE-\nNOYER, LUDOVIC. “Adversarial learning for modeling human motion”.\nThe Visual Computer 36.6–8 (June 2020), 141–160. DOI: 10.1007/\ns00371-018-1594-7 8, 15, 16, 23.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n30\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[WCAD18] WANG, QI, CHEN, MICKAEL, ARTIÈRES, THIERRY, and DE-\nNOYER, LUDOVIC. “Transferring Style in Motion Capture Sequences\nwith Adversarial Learning”. Proceedings of the European Symposium on\nArtiﬁcial Neural Network. ESANN, Apr. 2018. URL: https://hal.\narchives-ouvertes.fr/hal-02100672/ 8, 23.\n[WCW14] WANG, XIN, CHEN, QIUDI, and WANG, WANLIANG. “3D\nHuman Motion Editing and Synthesis: A Survey”. Computational and\nMathematical Methods in Medicine (June 2014). DOI: 10 . 1155 /\n2014/104535 2.\n[WCX17] WANG, YUMENG, CHE, WUJUN, and XU, BO. “Encoder-\nDecoder Recurrent Network Model for Interactive Character Animation\nGeneration”. The Visual Computer 33.6–8 (June 2017), 971–980. DOI:\n10.1007/s00371-017-1378-5 8, 18, 21.\n[WCX21] WANG, ZHIYONG, CHAI, JINXIANG, and XIA, SHIHONG.\n“Combining Recurrent Neural Networks and Adversarial Training for\nHuman Motion Synthesis and Control”. IEEE Transactions on Visual-\nization and Computer Graphics (TVCG) 27.1 (Jan. 2021), 14–28. DOI:\n10.1109/TVCG.2019.2938520 8, 12, 15, 16, 21, 23.\n[WGH20] WON, JUNGDAM, GOPINATH, DEEPAK, and HODGINS, JES-\nSICA K. “A Scalable Approach to Control Diverse Behaviors for Physi-\ncally Simulated Characters”. ACM Transactions on Graphics 39.4 (July\n2020). DOI: 10.1145/3386569.3392381 20, 21, 25.\n[WGY18] WEISS, GAIL, GOLDBERG, YOAV, and YAHAV, ERAN. “On\nthe Practical Computational Power of Finite Precision RNNs for Lan-\nguage Recognition”. Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2: Short Papers). As-\nsociation for Computational Linguistics, July 2018, 740–745. DOI: 10.\n18653/v1/P18-2117 8.\n[WHSZ19] WANG, HE, HO, EDMOND SHU-LIM, SHUM, HUBERT P. H.,\nand ZHU, ZHANXING. “Spatio-temporal Manifold Learning for Human\nMotions via Long-horizon Modeling”. IEEE Transactions on Visualiza-\ntion and Computer Graphics (TVCG) (Aug. 2019). DOI: 10.1109/\nTVCG.2019.2936810 3, 7, 8, 12, 16, 21, 23.\n[WL19] WON, JUNGDAM and LEE, JEHEE. “Learning Body Shape Varia-\ntion in Physics-Based Characters”. ACM Transactions on Graphics 38.6\n(Nov. 2019). DOI: 10.1145/3355089.3356499 19, 21.\n[WMR*17] WANG, ZIYU, MEREL, JOSH, REED, SCOTT, et al. “Robust\nImitation of Diverse Behaviors”. Proceedings of the International Con-\nference on Neural Information Processing Systems. Curran Associates\nInc., 2017, 5326–5335. URL: http://papers.nips.cc/paper/\n7116.pdf 8, 20, 21.\n[WN15] WANG, YINGYING and NEFF, MICHAEL. “Deep Signatures for\nIndexing and Retrieval in Large Motion Databases”. Proceedings of the\nACM SIGGRAPH Conference on Motion, Interaction and Games. Asso-\nciation for Computing Machinery, Nov. 2015, 37–45. DOI: 10.1145/\n2822013.2822024 7, 21, 23.\n[WP95] WITKIN, ANDREW and POPOVIC, ZORAN. “Motion Warping”.\nProceedings of the Annual Conference on Computer Graphics and In-\nteractive Techniques. Association for Computing Machinery, 1995, 105–\n108. DOI: 10.1145/218380.218422 1.\n[WYZ*20] WANG, ZHENYI, YU, PING, ZHAO, YANG, et al. “Learning\nDiverse Stochastic Human-Action Generators by Learning Smooth La-\ntent Transitions”. Proceedings of the AAAI Conference on Artiﬁcial In-\ntelligence 34.7 (Apr. 2020), 12281–12288. DOI: 10 . 1609 / aaai .\nv34i07.6911 8, 15, 16.\n[XLKvdP20] XIE, ZHAOMING, LING, HUNG YU, KIM, NAM HEE, and\nvan de PANNE, MICHIEL. “ALLSTEPS: Curriculum-driven Learning of\nStepping Stone Skills”. Computer Graphics Forum (Oct. 2020). DOI:\n10.1111/cgf.14115 19, 21.\n[XLM19] XU, YI TIAN, LI, YAQIAO, and MEGER, DAVID. “Human Mo-\ntion Prediction Via Pattern Completion in Latent Representation Space”.\nProceedings of the IEEE Conference on Computer and Robot Vision.\nIEEE Computer Society, 2019, 57–64. DOI: 10.1109/CRV.2019.\n00016 8, 10, 12, 16.\n[XWCC15] XIA, SHIHONG, WANG, CONGYI, CHAI, JINXIANG, and\nCHAI, JESSICA. “Realtime Style Transfer for Unlabeled Heterogeneous\nHuman Motion”. ACM Transactions on Graphics 34.4 (July 2015). DOI:\n10.1145/2766999 6.\n[XXN*20] XU, JINGWEI, XU, HUAZHE, NI, BINGBING, et al. “Hierar-\nchical Style-based Networks for Motion Synthesis”. Proceedings of the\nEuropean Conference on Computer Vision. Springer International Pub-\nlishing, Sept. 2020, 178–194. DOI: 10.1007/978-3-030-58621-\n8_11 8, 13, 16.\n[YK20] YUAN, YE and KITANI, KRIS. “DLow: Diversifying Latent Flows\nfor Diverse Human Motion Prediction”. Proceedings of the European\nConference on Computer Vision. Springer International Publishing, Sept.\n2020, 346–364. DOI: 10.1007/978-3-030-58545-7_20 8, 15,\n16.\n[YKK*19] YU, MOONWON, KWON, BYUNGJUN, KIM, JONGMIN, et al.\n“Fast Terrain-Adaptive Motion Generation Using Deep Neural Net-\nworks”. SIGGRAPH Asia Technical Briefs. Association for Computing\nMachinery, 2019, 57–60. DOI: 10.1145/3355088.3365157 13,\n16.\n[YKL21] YANG, DONGSEOK, KIM, DOYEON, and LEE, SUNG-HEE.\n“LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-\nbody Tracking Signals”. Computer Graphics Forum 40.2 (June\n2021), 265–275. DOI: 10.1111/cgf.142631 10, 16, 22, 23.\n[YLX*19] YAN, SIJIE, LI, ZHIZHONG, XIONG, YUANJUN, et al. “Con-\nvolutional Sequence Generation for Skeleton-Based Action Synthesis”.\nProceedings of the 2019 IEEE/CVF International Conference on Com-\nputer Vision (ICCV). IEEE Computer Society, Oct. 2019, 4393–4401.\nDOI: 10.1109/ICCV.2019.00449 15, 16.\n[YRV*18] YAN, XINCHEN, RASTOGI, AKASH, VILLEGAS, RUBEN, et al.\n“MT-VAE: Learning Motion Transformations to Generate Multimodal\nHuman Dynamics”. Proceedings of the European Conference on Com-\nputer Vision. Springer International Publishing, Sept. 2018, 276–293.\nDOI: 10.1007/978-3-030-01228-1_17 14, 16.\n[YTL18] YU, WENHAO, TURK, GREG, and LIU, C. KAREN. “Learn-\ning Symmetric and Low-Energy Locomotion”. ACM Transactions on\nGraphics 37.4 (July 2018). DOI: 10.1145/3197517.3201397 19–\n21.\n[ZBL*19] ZHOU, YI, BARNES, CONNELLY, LU, JINGWAN, et al. “On the\nContinuity of Rotation Representations in Neural Networks”. Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion. IEEE Computer Society, June 2019, 5738–5746. DOI: 10.1109/\nCVPR.2019.00589 4.\n[ZLX*18] ZHOU, YI, LI, ZIMO, XIAO, SHUANGJIU, et al. “Auto-\nConditioned Recurrent Networks for Extended Complex Human Motion\nSynthesis”. Proceedings of the International Conference on Learning\nRepresentations. May 2018. URL: https://openreview.net/\nforum?id=r11Q2SlRW 3, 8, 12, 16.\n[ZLZ*17] ZHANG, WEICHEN, LIU, ZHIGUANG, ZHOU, LIUYANG, et al.\n“Martial Arts, Dancing and Sports dataset: A challenging stereo and\nmulti-view dataset for 3D human pose estimation”. Image and Vision\nComputing 61 (2017), 22–39. DOI: 10.1016/j.imavis.2017.\n02.002 6.\n[ZPIE17] ZHU, JUN-YAN, PARK, TAESUNG, ISOLA, PHILLIP, and\nEFROS, ALEXEI A. “Unpaired Image-to-Image Translation Using\nCycle-Consistent Adversarial Networks”. Proceedings of the 2017 IEEE\nInternational Conference on Computer Vision (ICCV). IEEE Computer\nSociety, Oct. 2017, 2242–2251. DOI: 10.1109/ICCV.2017.244 24.\n[ZPK20] ZANG, CHUANQI, PEI, MINGTAO, and KONG, YU. “Few-shot\nHuman Motion Prediction via Learning Novel Motion Dynamics”. Pro-\nceedings of the International Joint Conferences on Artiﬁcial Intelligence.\nSept. 2020, 846–852. DOI: 10.24963/ijcai.2020/118 5, 7, 10,\n16.\n[ZSKS18] ZHANG, HE, STARKE, SEBASTIAN, KOMURA, TAKU, and\nSAITO, JUN. “Mode-Adaptive Neural Networks for Quadruped Motion\nControl”. ACM Transactions on Graphics 37.4 (July 2018). DOI: 10.\n1145/3197517.3201366 5, 8, 17, 21.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n31\nhttps://doi.org/10.1111/cgf.14426\nL. Mourot, L. Hoyet, F. Le Clerc, F. Schnitzler & P. Hellier / A Survey on Deep Learning for Skeleton-Based Human Animation\n[ZYC*20] ZOU, YULIANG, YANG, JIMEI, CEYLAN, DUYGU, et al. “Re-\nducing Footskate in Human Motion Reconstruction with Ground Contact\nConstraints”. Proceedings of the IEEE Winter Conference on Applica-\ntions of Computer Vision. IEEE Computer Society, Mar. 2020, 448–457.\nDOI: 10.1109/WACV45572.2020.9093329 8, 22, 23.\n[ZZD13] ZHANG, WEIYU, ZHU, MENGLONG, and DERPANIS, KON-\nSTANTINOS G. “From Actemes to Action: A Strongly-Supervised Rep-\nresentation for Detailed Action Understanding”. Proceedings of the 2013\nIEEE International Conference on Computer Vision (ICCV). IEEE Com-\nputer Society, Dec. 2013, 2248–2255. DOI: 10.1109/ICCV.2013.\n280 6.\nThis article has been published in COMPUTER GRAPHICS Forum (CGF).\n32\nhttps://doi.org/10.1111/cgf.14426\n",
  "categories": [
    "cs.GR"
  ],
  "published": "2021-10-13",
  "updated": "2021-11-23"
}