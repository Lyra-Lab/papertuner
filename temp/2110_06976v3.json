{
  "id": "http://arxiv.org/abs/2110.06976v3",
  "title": "Representational Continuity for Unsupervised Continual Learning",
  "authors": [
    "Divyam Madaan",
    "Jaehong Yoon",
    "Yuanchun Li",
    "Yunxin Liu",
    "Sung Ju Hwang"
  ],
  "abstract": "Continual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent CL advances are restricted\nto supervised continual learning (SCL) scenarios. Consequently, they are not\nscalable to real-world applications where the data distribution is often biased\nand unannotated. In this work, we focus on unsupervised continual learning\n(UCL), where we learn the feature representations on an unlabelled sequence of\ntasks and show that reliance on annotated data is not necessary for continual\nlearning. We conduct a systematic study analyzing the learned feature\nrepresentations and show that unsupervised visual representations are\nsurprisingly more robust to catastrophic forgetting, consistently achieve\nbetter performance, and generalize better to out-of-distribution tasks than\nSCL. Furthermore, we find that UCL achieves a smoother loss landscape through\nqualitative analysis of the learned representations and learns meaningful\nfeature representations. Additionally, we propose Lifelong Unsupervised Mixup\n(LUMP), a simple yet effective technique that interpolates between the current\ntask and previous tasks' instances to alleviate catastrophic forgetting for\nunsupervised representations.",
  "text": "Published as a conference paper at ICLR 2022\nREPRESENTATIONAL CONTINUITY FOR\nUNSUPERVISED CONTINUAL LEARNING\nDivyam Madaan1∗Jaehong Yoon2,3 † Yuanchun Li5,6 Yunxin Liu5,6 Sung Ju Hwang2,4\nNew York University1\nKAIST2\nMicrosoft Research3\nAITRICS4\nInstitute for AI Industry Research (AIR)5\nTsinghua University6\ndivyam.madaan@nyu.edu, {jaehong.yoon,sjhwang82}@kaist.ac.kr\nliyuanchun@air.tsinghua.edu.cn, liuyunxin@air.tsinghua.edu.cn\nABSTRACT\nContinual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent CL advances are restricted\nto supervised continual learning (SCL) scenarios. Consequently, they are not\nscalable to real-world applications where the data distribution is often biased and\nunannotated. In this work, we focus on unsupervised continual learning (UCL),\nwhere we learn the feature representations on an unlabelled sequence of tasks and\nshow that reliance on annotated data is not necessary for continual learning. We\nconduct a systematic study analyzing the learned feature representations and show\nthat unsupervised visual representations are surprisingly more robust to catastrophic\nforgetting, consistently achieve better performance, and generalize better to out-of-\ndistribution tasks than SCL. Furthermore, we ﬁnd that UCL achieves a smoother\nloss landscape through qualitative analysis of the learned representations and\nlearns meaningful feature representations. Additionally, we propose Lifelong\nUnsupervised Mixup (LUMP), a simple yet effective technique that interpolates\nbetween the current task and previous tasks’ instances to alleviate catastrophic\nforgetting for unsupervised representations. We release our code online.\n1\nINTRODUCTION\nRecently continual learning (Thrun, 1995) has gained a lot of attention in the deep learning community\ndue to its ability to continually learn on a sequence of non-stationary tasks (Kumar & Daume III,\n2012; Li & Hoiem, 2016) and close proximity to the human learning process (Flesch et al., 2018).\nHowever, the inability of the learner to prevent forgetting of the knowledge learnt from the previous\ntasks has been a long-standing problem (McCloskey & Cohen, 1989; Goodfellow et al., 2013). To\naddress this problem, a large body of methods (Rusu et al., 2016; Zenke et al., 2017; Yoon et al., 2018;\nLi et al., 2019; Aljundi et al., 2019; Buzzega et al., 2020) have been proposed; however, all these\nmethods focus on the supervised learning paradigm, but obtaining high-quality labels is expensive and\noften impractical in real-world scenarios. In contrast, CL for unsupervised representation learning\nhas received limited attention in the community. Although Rao et al. (2019) instantiated a continual\nunsupervised representation learning framework (CURL), it is not scalable for high-resolution tasks,\nas it is composed of MLP encoders/decoders and a simple MoG generative replay. This is evident in\ntheir limited empirical evaluation using digit-based gray-scale datasets.\nMeanwhile, a set of directions have shown huge potential to tackle the representation learning problem\nwithout labels (He et al., 2020; Chen et al., 2020a; Grill et al., 2020; Chen et al., 2020b; Chen &\nHe, 2021; Zbontar et al., 2021) by aligning contrastive pairs of training instances or maximizing\nthe similarity between two augmented views of each image. However, a common assumption for\nexisting methods is the availability of a large amount of unbiased and unlabelled datasets to learn\nthe feature representations. We argue that this assumption is not realistic for most of the real-time\napplications, including self-driving cars (Bojarski et al., 2016), medical applications (Kelly et al.,\n2019) and conversational agents (Li et al., 2020). The collected datasets are often limited in size\nduring the initial training phase (Finn et al., 2017), and datasets/tasks change continuously with time.\n∗Corresponding author.\n† The work was done while the author was an intern at Microsoft Research.\n1\narXiv:2110.06976v3  [cs.LG]  5 Apr 2022\nPublished as a conference paper at ICLR 2022\n…\n…\nClassifier 𝒉𝝍𝝉\n…\n𝑳𝟏\n𝑳𝑵\n𝓓𝝉:\n→𝒴𝒌,𝝉\n(       ,𝒴𝒌,𝝉)\n𝒉𝝍𝟏\n(        ,𝒴𝒋,𝟏)\n𝓤𝟏:\n→𝒴𝒋,𝟏\n…\n𝑳𝟏\n𝑳𝑵\n…\n𝑳𝟏\n𝑳𝑵\nSelf-Supervised\nloss\n…\n𝓤𝝉\nView1\nView2\n𝓤𝟏\nRevisit \ngeneric \nfeatures\n(𝓤𝟏)  Learned\nRepresentation\nSUPERVISED CONTINUAL LEARNING (SCL)\nUNSUPERVISED CONTINUAL LEARNING (UCL)\nFigure 1: Illustration of supervised and unsupervised continual learning. The objective of SCL is to learn\nthe ability to classify labeled images in the current task while preserving the past tasks’ knowledge, where the\ntasks are non-iid to each other. On the other hand, UCL aims to learn the representation of images without the\npresence of labels and the model learns general-purpose representations during sequential training.\nTo accommodate such continuous shifts in data distributions, representation learning models need to\nincrement the knowledge without losing the representations learned in the past. With this motivation,\nwe attempt to bridge the gap between unsupervised representation learning and continual learning to\naddress the challenge of learning the representations on a sequence of tasks. Speciﬁcally, we focus\non unsupervised continual learning (UCL), where the goal of the continual learner is to learn the\nrepresentations from a stream of unlabelled data instances without forgetting (see Figure 1). To this\nend, we extend various existing SCL strategies to the unsupervised continual learning framework and\nanalyze the performance of current state-of-the-art representation learning techniques: SimSiam (Chen\n& He, 2021) and BarlowTwins (Zbontar et al., 2021) for UCL. Surprisingly, we observe that the\nunsupervised representations are comparatively more robust to catastrophic forgetting across all\ndatasets and simply ﬁnetuning on the sequence of tasks can outperform various state-of-the-art\ncontinual learning alternatives. Furthermore, we show that UCL generalize better to various out of\ndistribution tasks and outperforms SCL for few-shot training scenarios (Section 5.2).\nWe demystify the robustness of unsupervised representations by investigating the feature similarity,\nmeasured by centered kernel alignment (CKA) (Kornblith et al., 2019) between two independent\nUCL and SCL models and between UCL and SCL models. We notice that two unsupervised model\nrepresentations have a relatively high feature similarity compared to two supervised representations.\nFurthermore, in all cases, two models have high similarity in lower layers indicating that they learn\nsimilar low-level features. Further, we measure the ℓ2 distance between model parameters (Neyshabur\net al., 2020) and visually compare the feature representations learned by different SCL and UCL\nstrategies. We observe that UCL obtains human perceptual feature patterns for previous tasks,\ndemonstrating their effectiveness to alleviate catastrophic forgetting (Section 5.3). We conjecture\nthat this is due to their characteristic ability to learn general-purpose features (Doersch et al., 2020),\nwhich makes them transfer better and comparatively more robust to catastrophic forgetting.\nTo gain further insights, we visualize the loss landscape (Li et al., 2018) of the UCL and SCL models\nand observe that UCL obtains a ﬂatter and smoother loss landscape than SCL. Additionally, we\npropose a simple yet effective technique coined Lifelong Unsupervised Mixup (LUMP), which utilizes\nmixup (Zhang et al., 2018) for unlabelled training instances. In particular, LUMP interpolates between\nthe current task examples and examples from previous instances to minimize catastrophic forgetting.\nWe emphasize that LUMP is easy to implement, does not require additional hyperparameters, and\nsimply trains on the interpolated instances. To this end, LUMP requires little, or no modiﬁcation to\nexisting rehearsal-based methods effectively minimizes catastrophic forgetting even with uniformly\nselecting the examples from replay buffer. We show that LUMP with UCL outperforms the state-of-\nthe-art supervised continual learning methods across multiple experimental settings with signiﬁcantly\nlower catastrophic forgetting. In summary, our contributions are as follows:\n• We attempt to bridge the gap between continual learning and representation learning and\ntackle the two crucial problems of continual learning with unlabelled data and representation\nlearning on a sequence of tasks.\n• Systematic quantitative analysis shows that UCL achieves better performance over SCL\nwith signiﬁcantly lower catastrophic forgetting on Sequential CIFAR-10, CIFAR-100, and\nTiny-ImageNet. Additionally, we evaluate out-of-distribution tasks and few-shot training\ndemonstrating the expressive power of unsupervised representations.\n• We provide visualization of the representations and loss landscapes, which show that UCL\nlearns discriminative, human perceptual patterns and achieves a ﬂatter and smoother loss\nlandscape. Furthermore, we propose Lifelong Unsupervised Mixup (LUMP) for UCL, which\neffectively alleviates catastrophic forgetting and provides better qualitative interpretations.\n2\nPublished as a conference paper at ICLR 2022\n2\nRELATED WORK\nContinual learning. We can partition the existing continual learning methods into three categories.\nThe regularization approaches (Li & Hoiem, 2016; Zenke et al., 2017; Schwarz et al., 2018; Ahn\net al., 2019) impose a regularization constraint to the objective that mitigates catastrophic forgetting.\nThe architectural approaches (Rusu et al., 2016; Yoon et al., 2018; Li et al., 2019) avoid this problem\nby including task-speciﬁc parameters and allowing the expansion of the network during continual\nlearning. The rehearsal approaches (Rebufﬁet al., 2017; Rolnick et al., 2019; Aljundi et al., 2019)\nallocate a small memory buffer to store and replay the examples from the previous task. However, all\nthese methods are conﬁned to supervised learning, which limits their application in real-life problems.\nRao et al. (2019); Smith et al. (2021) tackled the problem of continual unsupervised representation\nlearning; however, their methods are restricted to simple low-resolution tasks and not scalable to\nlarge-scale continual learning datasets.\nRepresentational learning. A large number of works have addressed the unsupervised learning prob-\nlem in the standard machine learning framework. Speciﬁcally, contrastive learning frameworks (He\net al., 2020; Chen et al., 2020a; Grill et al., 2020; Chen et al., 2020b;c) that learn the representations\nby measuring the similarities of positive and negative pairs have gained a lot of attention in the com-\nmunity. However, all these methods require large batches and negative sample pairs, which restrict\nthe scalability of these networks. Chen & He (2021) tackled these limitations and proposed SimSiam,\nthat use standard Siamese networks (Bromley et al., 1994) with the stop-gradient operation to prevent\nthe collapsing of Siamese networks to a constant. Recently, Zbontar et al. (2021) formulated an\nobjective that pushes the cross-correlation matrix between the embeddings of distorted versions of\na sample closer to the identity matrix. However, all these methods assume the presence of large\ndatasets for pre-training, which is impractical in real-world applications. In contrast, we tackle the\nproblem of incremental representational learning and learn the representations sequentially while\nmaximizing task adaptation and minimizing catastrophic forgetting.\n3\nPRELIMINARIES\n3.1\nPROBLEM SETUP\nWe consider the continual learning setting, where we learn on a continuum of data consisting of\nT tasks T1:T = (T1 . . . TT ). In supervised continual learning, each task consists a task descriptor\nτ ∈{1 . . . T} and its corresponding dataset Dτ = {(xi,τ, yi,τ)nτ\ni=1} with nτ examples. Each input-\npair (xi,τ, yi,τ) ∈Xτ × Yτ, where (Xτ, Yτ) is an unkown data distribution. Let us consider a\nnetwork fΘ : Xτ →RD parametrized by Θ = {wl}l=L\nl=1 , where RD and L denote D-dimensional\nembedding space and number of layers respectively. The classiﬁer is denoted by hψ : RD →Yτ.\nThe network error using cross entropy loss (CE) for SCL with ﬁnetuning can be formally deﬁned as:\nLFINETUNE\nSCL\n= CE (hψ (fΘ (xi,τ) , τ) , yi,τ) .\n(1)\nIn this work, we assume the absence of label supervision during training and focus on unsupervised\ncontinual learning. In particular, each task consists of Uτ = {(xi,τ)nτ\ni=1}, xi,τ ∈Xτ with nτ\nexamples. Our aim is to learn the representations fΘ : Xτ →RD on a sequence of tasks while\npreserving the knowledge of the previous tasks. We introduce the representation learning framework\nand propose LUMP in Section 4 to learn unsupervised representations while effectively mitigating\ncatastrophic forgetting.\n3.2\nLEARNING PROTOCOL AND EVALUATION METRICS\nCurrently, the traditional continual learning strategies follow the standard training protocol, where\nwe learn the network representations fΘ : Xτ →Yτ on a sequence of tasks. In contrast, our goal\nis to learn the feature representations fΘ : Xτ →RD, so we follow a two-step learning protocol\nto obtain the model predictions. First, we pre-train the representations on a sequence of tasks\nT1:T = (T . . . TT ) to obtain the representations. Next, we evaluate the quality of our pre-trained\nrepresentations using a K-nearest neighbor (KNN) classiﬁer (Wu et al., 2018) following the setup\nin Chen et al. (2020a); Chen & He (2021); Zbontar et al. (2021).\n3\nPublished as a conference paper at ICLR 2022\nTo validate knowledge transfer of the learned representations, we adopt the metrics from the SCL\nliterature (Chaudhry et al., 2019b; Mirzadeh et al., 2020). Let aτ,i denote the test accuracy of task i\nafter learning task Tτ using a KNN on frozen pre-trained representations on task Tτ. More formally,\nwe can deﬁne the metrics to evaluate the continually learned representations as follow:\n1. Average accuracy is the average test accuracy of all the tasks completed until the continual\nlearning of task τ: Aτ = 1\nτ\nPτ\ni=1 aτ,i\n2. Average Forgetting is the average performance decrease of each task between its maximum accu-\nracy and accuracy at the completion of training: F =\n1\nT −1\nPT −1\ni=1 maxτ∈{1,...,T } (aτ,i −aT,i)\n4\nUNSUPERVISED CONTINUAL LEARNING\n4.1\nCONTINUOUS REPRESENTATION LEARNING WITH SEQUENTIAL TASKS\nTo learn feature representations, contrastive learning (Chen et al., 2020a;b; He et al., 2020) maximizes\nthe similarity of representations between the images of the same views (positive pairs) and minimizes\nthe similarity between images of different views (negative pairs). However, these methods require\nlarge batches, negative sample pairs (Chen et al., 2020a;b), or architectural modiﬁcations (He et al.,\n2020; Chen et al., 2020c), or non-differentiable operators (Caron et al., 2020), which makes their\napplication difﬁcult for continual learning scenarios. In this work, we focus on SimSiam (Chen\n& He, 2021) and BarlowTwins (Zbontar et al., 2021), which tackle these limitations and achieve\nstate-of-the-art performance on standard representation learning benchmarks.\nSimSiam (Chen & He, 2021) uses a variant of Siamese networks (Bromley et al., 1994) for learning\ninput data representations. It consists of an encoder network fΘ, which is composed of a backbone\nnetwork, and is shared across a projection MLP and prediction MLP head h(·). Speciﬁcally, SimSiam\nminimizes the cosine-similarity between the output vectors of the projector and the predictor MLP\nacross two different augmentations for an image. Initially, we consider FINETUNE, which is a a naive\nCL baseline that minimizes the cosine-similarity between the projector output (z1\ni,τ = fΘ(x1\ni,τ)) and\nthe predictor output (p2\ni,τ = h(fΘ(x2\ni,τ)) on a sequence of tasks as follows:\nLFINETUNE\nUCL\n= 1\n2D(p1\ni,τ, stopgrad(z2\ni,τ)) + 1\n2D(p2\ni,τ, stopgrad(z1\ni,τ)),\n(2)\nwhere D(p1\ni,τ, z2\ni,τ) = −\np1\ni,τ\n∥p2\ni,τ∥2\n·\nz2\ni,τ\n∥z2\ni,τ∥2\n,\nx1\ni,τ and x2\ni,τ are two randomly augmented views of an input example xi,τ ∈Tτ and ∥·∥2 denotes\nthe ℓ2-norm. Note that, the stopgrad is a crucial component in SimSiam to prevent the trivial\nsolutions obtained by Siamese networks. Due to its simplicity and effectiveness, we chose Simsiam\nto analyze the performance of unsupervised representations for continual learning.\nBarlowTwins (Zbontar et al., 2021) minimizes the redundancy between the embedding vector\ncomponents of the distorted versions of an instance while conserving the maximum information\ninspired from Barlow (1961). In particular, the objective function eliminates the SimSiam stopgrad\ncomponent and instead makes the cross-correlation matrix computed between the outputs of two\nidentical networks closer to the identity matrix. Let C be the cross-correlation matrix between\nthe outputs of two Siamese branches along the batch dimension and Z1 and Z2 denote the batch\nembeddings of the distorted views for all images of a batch from the current task (xτ ∈Uτ). The\nobjective function for UCL with ﬁnetuning and BarlowTwins can then be deﬁned as:\nLFINETUNE\nUCL\n=\nX\ni\n(1 −Cii)2 + λ ·\nX\ni\nX\nj̸=i\nCij\n2, where Cij =\nP\nB z1\nB,iz2\nB,j\nqP\nB (z1\nB,i)2qP\nB (z2\nB,j)2 .\n(3)\nλ is a positive constant trading off the importance of the invariance and redundancy reduction terms\nof the loss, i and j denote the network’s output vector dimensions. Similar to SimSiam, BarlowTwins\nis simple, easy to implement, and can be applied to existing continual learning strategies with little or\nno modiﬁcation.\n4\nPublished as a conference paper at ICLR 2022\n4.2\nPRESERVING REPRESENTATIONAL CONTINUITY: A VIEW OF EXISTING SCL METHODS\nLearning feature representations from labelled instances on a sequence of tasks has been long\nstudied in continual learning. However, the majority of these learning strategies are not directly\napplicable to UCL. To compare with the regularization-based strategies, we extend Synaptic Intelli-\ngence (SI) (Zenke et al., 2017) to UCL and consider the online per-synapse consolidation during the\nentire training trajectory of the unsupervised representations. For architectural-based strategies, we in-\nvestigate Progressive Neural Networks (PNN) (Rusu et al., 2016) and learn the feature representations\nprogressively using the representations learning frameworks proposed in Section 4.1.\nWe also formulate Dark Experience Replay (DER) (Buzzega et al., 2020) for UCL. DER for SCL\nalleviates catastrophic forgetting by matching the network logits across a sequence of tasks during\nthe optimization trajectory. Notably, the loss for SCL-DER can be deﬁned as follow:\nLDER\nSCL = LFINETUNE\nSCL\n+ α · E(x,p)∼M\n\u0002\n∥softmax(p) −softmax(hψ(xi,τ))∥2\n2\n\u0003\n,\n(4)\nwhere p = hψτ (x), LFINETUNE\nSCL\ndenotes the cross-entropy loss on the current task (see Equation (1))\nand random examples are selected using reservoir sampling from the replay-buffer M. Since, we do\nnot have access to the labels for UCL, we cannot minimize the aforementioned objective.\nInstead, we utilize the output of the projected output by the backbone network to preserve the\nknowledge of the past tasks over the entire training trajectory. In particular, DER for UCL consists of\na combination of two terms. The ﬁrst term learns the representations using SimSiam from Equation (2)\nor BarlowTwins from Equation (3) and the second term minimizes the Euclidean distance between\nthe projected outputs to minimize catastrophic forgetting. More formally, UCL-DER minimizes the\nfollowing loss:\nLDER\nUCL = LFINETUNE\nUCL\n+ α · E(x)∼M\n\u0002\n∥fΘτ (x) −fΘ(xi,τ)∥2\n2\n\u0003\n(5)\nHowever, the performance of the rehearsal-based methods is sensitive to the choice of α and often\nrequires supervised training setup, task identities, and boundaries. To tackle this issue, we propose\nLifelong Unsupervised Mixup in the subsequent subsection, which interpolates between the current\nand past task instances to mitigate catastrophic forgetting effectively.\n4.3\nLIFELONG UNSUPERVISED MIXUP\nThe standard Mixup (Zhang et al., 2018) training constructs virtual training examples based on the\nprinciple of Vicinal Risk Minimization . In particular, let (xi, yi) and (xj, yj) denote two random\nfeature-target pairs sampled from the training data distribution and let (˜x, ˜y) denote the interpolated\nfeature-target pair in the vicinity of these examples; mixup then minimizes the following objective:\nLMIXUP(˜x, ˜y) = CE (hψ (fΘ (˜x)) , ˜y) ,\n(6)\nwhere ˜x = λ · xi + (1 −λ) · xj and ˜y = λ · yi + (1 −λ) · yj.\nλ ∼Beta(α, α), for α ∈(0, ∞). In this work, we focus on lifelong self-supervised learning and\npropose Lifelong Unsupervised Mixup (LUMP) that utilizes mixup for UCL by incorporating the\ninstances stored in the replay-buffer from the previous tasks into the vicinal distribution. In particular,\nLUMP interpolates between the examples of the current task (xi,τ) ∈Uτ and random examples\nselected using uniform sampling from the replay buffer, which encourages the model to behave\nlinearly across a sequence of tasks. More formally, LUMP minimizes the objective in Equation (2)\nand Equation (3) on the following interpolated instances ˜xi,τ for the current task τ:\n˜xi,τ = λ · xi,τ + (1 −λ) · xj,M,\n(7)\nwhere xj,M ∼M denotes the example selected using uniform sampling from replay buffer M.\nThe interpolated examples not only augments the past tasks’ instances in the replay buffer but also\napproximates a regularized loss minimization (Zhang et al., 2021). During UCL, LUMP enhances the\nrobustness of learned representation by revisiting the attributes of the past task that are similar to the\ncurrent task. Recently, Kim et al. (2020); Lee et al. (2021); Verma et al. (2021); Shen et al. (2022)\nalso employed mixup for contrastive learning. Our work is different from these existing works in\nthat our objective is different, and we focus on unsupervised continual learning. To this end, LUMP\nsuccessively mitigates catastrophic forgetting and learns discriminative & human-perceptual features\nover the current state-of-the-art SCL strategies (see Table 1 and Figure 4).\n5\nPublished as a conference paper at ICLR 2022\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nBaselines. We compare with multiple supervised and unsupervised continual learning baselines\nacross different categories of continual learning methods.\n1. Supervised continual learning. FINETUNE is a vanilla supervised learning method trained on\na sequence of tasks without regularization or episodic memory and MULTITASK optimizes the\nmodel on complete data. For regularization-based CL methods, we compare against SI (Zenke\net al., 2017) and AGEM (Chaudhry et al., 2019a). We include PNN (Rusu et al., 2016) for\narchitecture-based methods. Lastly, we consider GSS (Aljundi et al., 2019) that populates the\nreplay-buffer using solid-angle minimization and DER (Buzzega et al., 2020) matches the network\nlogits sampled through the optimization trajectory for rehearsal during continual learning.\n2. Unsupervised continual learning. We consider the unsupervised variants of various SCL base-\nlines to show the utility of the unsupervised representations for sequential learning. Speciﬁcally,\nwe use SIMSIAM (Chen & He, 2021) and BARLOWTWINS (Zbontar et al., 2021), which are\nthe state-of-the-art representational learning techniques for learning the unsupervised continual\nrepresentations. We compare with FINETUNE and MULTITASK following the supervised learning\nbaselines, and SI (Zenke et al., 2017), PNN (Rusu et al., 2016) for unsupervised regularization and\narchitecture CL methods respectively. For rehearsal-based method, we compare with the UCL\nvariant of DER (Buzzega et al., 2020) described in Section 4.2\nDatasets. We compare the performance of SCL and UCL on various continual learning benchmarks\nusing single-head ResNet-18 (He et al., 2016) architecture. Split CIFAR-10 (Krizhevsky, 2012)\nconsists of two random classes out of the ten classes for each task. Split CIFAR-100 (Krizhevsky,\n2012) consists of ﬁve random classes out of the 100 classes for each task. Split Tiny-ImageNet is\na variant of the ImageNet dataset (Deng et al., 2009) containing ﬁve random classes out of the 100\nclasses for each task with the images sized 64 × 64 pixels.\nTraining and evaluation setup. We follow the hyperparameter setup of Buzzega et al. (2020)\nfor all the SCL strategies and tune them for the UCL representation learning strategies. All the\nlearned representations are evaluated with KNN classiﬁer (Wu et al., 2018) across three independent\nruns. Further, we use the hyper-parameters obtained by SimSiam for training UCL strategies with\nBarlowTwins to analyze the sensitivity of UCL to hyper-parameters and for a fair comparison\nbetween different methods. We train all the UCL methods for 200 epochs and evaluate with the KNN\nclassiﬁer (Wu et al., 2018). We provide the hyper-parameters in detail in Table A.5.\n5.2\nQUANTITATIVE RESULTS\nEvaluation on SimSiam. Table 1 shows the evaluation results for supervised and unsupervised rep-\nresentations learnt by SimSiam (Chen & He, 2021) across various continual learning strategies. In all\ncases, continual learning with unsupervised representations achieves signiﬁcantly better performance\nthan supervised representations with substantially lower forgetting. For instance, SI with UCL obtains\nbetter performance and 68%, 54%, and 44% lower forgetting relative to the best-performing SCL\nstrategy on Split CIFAR-10, Split CIFAR-100, and Split Tiny-ImageNet, respectively. Surprisingly,\nFINETUNE with UCL achieves higher performance and signiﬁcantly lower forgetting in comparison\nto all SCL strategies except DER. Furthermore, LUMP improves upon the UCL strategies: 2.8%\nand 5.9% relative increase in accuracy and 15% and 57.1% relative decrease in forgetting on Split\nCIFAR-100 and Split Tiny-ImageNet, respectively.\nEvaluation on BarlowTwins. To verify that unsupervised representations are indeed more robust to\ncatastrophic forgetting, we train BarlowTwins (Zbontar et al., 2021) on a sequence of tasks. We notice\nthat the representations learned with BarlowTwins substantially improve the accuracy and forgetting\nover SCL: 71.4%, 69.7% and 73.2% decrease in forgetting with FINETUNE on Split CIFAR-10, Split\nCIFAR-100 and Split Tiny-ImageNet respectively. Similarly, we observe that SI, and DER are more\nrobust to catastrophic forgetting; however, PNN underperforms on complicated tasks since feature\naccumulation using adaptor modules is insufﬁcient to construct useful representations for current\ntask adaptation. Interestingly, representations learnt with BarlowTwins achieve lower forgetting for\nFINETUNE, DER and LUMP than SimSiam with comparable accuracy across all the datasets.\n6\nPublished as a conference paper at ICLR 2022\nTable 1: Accuracy and forgetting of the learnt representations on Split CIFAR-10, Split CIFAR-100 and Split\nTiny-ImageNet on Resnet-18 architecture with KNN classiﬁer (Wu et al., 2018). All the values are measured by\ncomputing mean and standard deviation across three trials. The best and second-best results are highlighted in\nbold and underline respectively.\nMETHOD\nSPLIT CIFAR-10\nSPLIT CIFAR-100\nSPLIT TINY-IMAGENET\nACCURACY FORGETTING ACCURACY FORGETTING ACCURACY FORGETTING\nSUPERVISED CONTINUAL LEARNING\nFINETUNE\n82.87 (± 0.47)\n14.26 (± 0.52)\n61.08 (± 0.04)\n31.23 (± 0.41)\n53.10 (± 1.37)\n33.15 (± 1.22)\nPNN (Rusu et al., 2016)\n82.74 (± 2.12)\n−\n66.05 (± 0.86)\n−\n64.38 (± 0.92)\n−\nSI (Zenke et al., 2017)\n85.18 (± 0.65)\n11.39 (± 0.77)\n63.58 (± 0.37)\n27.98 (± 0.34)\n44.96 (± 2.41)\n26.29 (± 1.40)\nA-GEM (Chaudhry et al., 2019a) 82.41 (± 1.24)\n13.82 (± 1.27)\n59.81 (± 1.07)\n30.08 (± 0.91)\n60.45 (± 0.24)\n24.94 (± 1.24)\nGSS (Aljundi et al., 2019)\n89.49 (± 1.75)\n7.50 (± 1.52)\n70.78 (± 1.67)\n21.28 (± 1.52)\n70.96 (± 0.72)\n14.76 (± 1.22)\nDER (Buzzega et al., 2020)\n91.35 (± 0.46)\n5.65 (± 0.35)\n79.52 (± 1.88)\n12.80 (± 1.47)\n68.03 (± 0.85)\n17.74 (± 0.65)\nMULTITASK\n97.77 (± 0.15)\n−\n93.89 (± 0.78)\n−\n91.79 (± 0.46)\n−\nUNSUPERVISED CONTINUAL LEARNING\nSIMSIAM\nFINETUNE\n90.11 (± 0.12)\n5.42 (± 0.08)\n75.42 (± 0.78)\n10.19 (± 0.37)\n71.07 (± 0.20)\n9.48 (± 0.56)\nPNN (Rusu et al., 2016)\n90.93 (± 0.22)\n−\n66.58 (± 1.00)\n−\n62.15 (± 1.35)\n−\nSI (Zenke et al., 2017)\n92.75 (± 0.06)\n1.81 (± 0.21)\n80.08 (± 1.30)\n5.54 (± 1.30)\n72.34 (± 0.42)\n8.26 (± 0.64)\nDER (Buzzega et al., 2020)\n91.22 (± 0.30)\n4.63 (± 0.26)\n77.27 (± 0.30)\n9.31 (± 0.09)\n71.90 (± 1.44)\n8.36 (± 2.06)\nLUMP\n91.00 (± 0.40)\n2.92 (± 0.53)\n82.30 (± 1.35)\n4.71 (± 1.52)\n76.66 (± 2.39)\n3.54 (± 1.04)\nMULTITASK\n95.76 (± 0.08)\n−\n86.31 (± 0.38)\n−\n82.89 (± 0.49)\n−\nBARLOWTWINS\nFINETUNE\n87.72 (± 0.32)\n4.08 (± 0.56)\n71.97 (± 0.54)\n9.45 (± 1.01)\n66.28 (± 1.23)\n8.89 (± 0.66)\nPNN (Rusu et al., 2016)\n87.52 (± 0.33)\n−\n57.93 (± 2.98)\n−\n48.70 (± 2.59)\n−\nSI (Zenke et al., 2017)\n90.21 (± 0.08)\n2.03 (± 0.22)\n75.04 (± 0.63)\n7.43 (± 0.67)\n56.96 (± 1.48)\n17.04 (± 0.89)\nDER (Buzzega et al., 2020)\n88.67 (± 0.24)\n2.41 (± 0.26)\n73.48 (± 0.53)\n7.98 (± 0.29)\n68.56 (± 1.47)\n7.87 (± 0.44)\nLUMP\n90.31 (± 0.30)\n1.13 (± 0.18)\n80.24 (± 1.04)\n3.53 (± 0.83)\n72.17 (± 0.89)\n2.43 (± 1.00)\nMULTITASK\n95.48 (± 0.14)\n−\n87.16 (± 0.52)\n−\n82.42 (± 0.74)\n−\n100\n200\n500\n2500\ndataset size per task\n20\n40\n60\n80\n100\naverage accuracy (%)\naccuracy over data size\nSCL-FT\nSCL-SI\nSCL-DER\nUCL-FT\nUCL-SI\nLUMP\n100\n200\n500\n2500\ndataset size per task\n0\n5\n10\naverage forgetting (%)\nforgetting over data size\nSCL-DER\nUCL-FT\nUCL-SI\nLUMP\nFigure 2: Evaluation on Few-shot training\nfor Split CIFAR-100 across different number\nof training instances per task. The results are\nmeasured across three independent trials.\n(a) FINETUNE\n(b) SI\n(c) DER\nFigure 3: CKA Feature similarity between two in-\ndependent UCL models (red), two independent SCL\nmodels (blue), and UCL and SCL model (green) for\ndifferent strategies on Split CIFAR-100 test distribution.\nEvaluation on Few-shot training. Figure 2 compares the effect of few-shot training on UCL and\nSCL, where each task has a limited number of training instances. Speciﬁcally, we conduct the\nexperimental evaluation using 100, 200, 500, and 2500 training instances for each task in split\nCIFAR-100 dataset. Surprisingly, we observe that the gap in average accuracy between SCL and\nUCL methods widens with a decrease in the number of training instances. Note that UCL decreases\nthe accuracy by 15.78%p on average with lower forgetting when the number of training instances\ndecreases from 2500 to 100; whereas, SCL obtains a severe 32.21%p deterioration in accuracy. We\nconjecture that this is an outcome of the discriminative feature embeddings learned by UCL, which\ndiscriminates all the images in the dataset and captures more than class-speciﬁc information as\nalso observed in Doersch et al. (2020). Furthermore, LUMP improves the performance over all the\nbaselines with a signiﬁcant margin across all few-shot experiments.\nEvaluation on OOD datasets. We evaluate the learnt representations on various out-of-distribution\n(OOD) datasets in Table 2 to measure their generalization to unseen data distributions. In particular,\nwe conduct the OOD evaluation on MNIST (LeCun, 1998), Fashion-MNIST (FMNIST) (Xiao et al.,\n2017), SVHN (Netzer et al., 2011), CIFAR-10 and CIFAR-100 (Krizhevsky, 2012) using a KNN\nclassiﬁer (Wu et al., 2018). We observe that unsupervised representations outperform the supervised\nrepresentations in all cases across all the datasets. In particular, the UCL representations learned with\nSimsiam, and SI on Split-CIFAR-10 improves the absolute performance over the best-performing\nSCL strategy by 4.58%, 6.09%, 15.26%, and 17.07% on MNIST, FMNIST, SVHN, and CIFAR-100\nrespectively. Further, LUMP trained on Split-CIFAR-100 outperforms SI across all datasets and\nobtains comparable performance with Split CIFAR-10 dataset.\n7\nPublished as a conference paper at ICLR 2022\nTable 2: Comparison of accuracy on out of distribution datasets using a KNN classiﬁer (Wu et al., 2018) on pre-\ntrained SCL and UCL representations. We consider MNIST (LeCun, 1998), Fashion-MNIST (FMNIST) (Xiao\net al., 2017), SVHN (Netzer et al., 2011) as out of distribution for Split CIFAR-100 and Split CIFAR-10. All the\nvalues are measured by computing mean and standard deviation across three trials. The best and second-best\nresults are highlighted in bold and underline respectively.\nIN-CLASS\nSPLIT CIFAR-10\nSPLIT CIFAR-100\nOUT-OF-CLASS\nMNIST\nFMNIST\nSVHN\nCIFAR-100\nMNIST\nFMNIST\nSVHN\nCIFAR-10\nSUPERVISED CONTINUAL LEARNING\nFINETUNE\n86.42 (± 1.11) 74.47 (± 0.84) 41.00 (± 0.85) 17.42 (± 0.96) 75.02 (± 3.97) 62.37 (± 3.20) 38.05 (± 0.73)\n39.18 (± 0.83)\nSI (Zenke et al., 2017)\n87.08 (± 0.79) 76.41 (± 0.81) 42.62 (± 1.31) 19.14 (± 0.91) 79.96 (± 2.63) 63.71 (± 1.36) 40.92 (± 1.64)\n40.41 (± 1.71)\nA-GEM (Chaudhry et al., 2019a) 86.07 (± 1.94) 74.74 (± 3.21) 37.77 (± 3.49) 16.11 (± 0.38) 77.56 (± 3.21) 64.16 (± 2.29) 37.48 (± 1.73)\n37.91 (± 1.33)\nGSS (Aljundi et al., 2019)\n70.36 (± 3.54) 69.20 (± 2.51) 33.11 (± 2.26) 18.21 (± 0.39) 76.54 (± 0.46) 65.31 (± 1.72) 35.72 (± 2.37)\n49.41 (± 1.81)\nDER (Buzzega et al., 2020)\n80.32 (± 1.91) 70.49 (± 1.54) 41.48 (± 2.76) 17.72 (± 0.25) 87.71 (± 2.23) 75.97 (± 1.29) 50.26 (± 0.95)\n59.07 (± 1.06)\nMULTITASK\n88.79 (± 1.13) 79.50 (± 0.52) 41.26 (± 1.95) 27.68 (± 0.66)\n92.29 (± 3.37) 86.12 (± 1.87)\n54.94 (± 1.77)\n54.04 (± 3.68)\nUNSUPERVISED CONTINUAL LEARNING\nSIMSIAM\nFINETUNE\n89.23 (± 0.99) 80.05 (± 0.34) 49.66 (± 0.81) 34.52 (± 0.12) 85.99 (± 0.86) 76.90 (± 0.11) 50.09 (± 1.41)\n57.15 (± 0.96)\nSI (Zenke et al., 2017)\n93.72 (± 0.58) 82.50 (± 0.51) 57.88 (± 0.16) 36.21 (± 0.69) 91.50 (± 1.26) 80.57 (± 0.93) 54.07 (± 2.73) 60.55 (± 2.54)\nDER (Buzzega et al., 2020)\n88.35 (± 0.82) 79.33 (± 0.62) 48.83 (± 0.55)) 30.68 (± 0.36) 87.96 (± 2.04) 76.21 (± 0.63) 47.70 (± 0.94)\n56.26 (± 0.16)\nLUMP\n91.03 (± 0.22) 80.78 (± 0.88) 45.18 (± 1.57) 31.17 (± 1.83) 91.76 (± 1.17) 81.61 (± 0.45) 50.13 (± 0.71)\n63.00 (± 0.53)\nMULTITASK\n90.69 (± 0.13) 80.65 (± 0.42) 47.67 (± 0.45) 39.55 (± 0.18)\n90.35 (± 0.24) 81.11 (± 1.86)\n52.20 (± 0.61) 70.19 (± 0.15)\nBARLOWTWINS\nFINETUNE\n86.86 (± 1.62) 78.37 (± 0.74) 44.64 (± 2.39) 28.03 (± 0.52) 76.08 (± 2.86) 76.82 (± 0.83) 42.95 (± 0.90)\n53.12 (± 0.13)\nSI (Zenke et al., 2017)\n90.31 (± 0.69) 80.58 (± 0.68) 49.18 (± 0.51)\n31.80 (± 0.4)\n85.24 (± 0.99) 78.82 (± 0.67) 45.18 (± 1.37)\n53.99 (± 0.56)\nDER (Buzzega et al., 2020)\n85.15 (± 2.19) 77.96 (± 0.59) 45.68 (± 0.93) 27.83 (± 0.86) 78.08 (± 1.95) 76.67 (± 0.68) 44.58 (± 1.01)\n53.24 (± 0.82)\nLUMP\n88.73 (± 0.54) 81.69 (± 0.45) 51.53 (± 0.41) 31.53 (± 0.36) 90.22 (± 1.39) 81.28 (± 0.91) 50.24 (± 0.95)\n60.76 (± 0.87)\nMULTITASK\n88.63 (± 1.38) 79.49 (± 0.29) 49.24 (± 2.44) 36.33 (± 0.29) 86.98 (± 1.70) 79.40 (± 1.10) 50.19 (± 0.81)\n49.50 (± 0.38)\n5.3\nQUALITATIVE ANALYSIS\nSimilarity in feature and parameter space. We analyze the similarity between the representations\nlearnt between (i) Two independent UCL models, (ii) Two independent SCL models (iii) SCL and UCL\nmodels using centered kernel alignment (CKA) (Kornblith et al., 2019) in Figure 3, which provides a\nscore between 0 and 1 measuring the similarity between a pair of hidden representations. For two\nrepresentations Θ1 : X →Rd1 and Θ2 : X →Rd1, CKA(Θ1, Θ2) =\n||Cov(Θ1(x),Θ2(x))||2\nF\n||Cov(Θ1(x))||F ·||Cov(Θ2(x))||F ,\nwhere covariances are with respect to the test distribution. Additionally, we measure the ℓ2 dis-\ntance (Neyshabur et al., 2020) between the parameters of two independent UCL models (see Table 3)\nand two independent SCL models (see Table 4). First, we observe that the representations learned\nby two independent UCL methods have a high feature similarity and lower ℓ2 distance compared\nto the two independent SCL methods, demonstrating UCL representations’ robustness. Second, we\nnote that the representations between any two independent models are highly similar in the lower\nlayers indicating that they learn similar high-level features, including edges and shapes; however, the\nfeatures are dissimilar for the higher modules. Lastly, we see that the representations between a UCL\nand SCL model are similar in the lower layers but diverge in the higher layers across all CL strategies.\nVisualization of feature space. Next, we visualize the learned features to dissect further the repre-\nsentations learned by UCL and SCL strategies. Figure 4 shows the visualization of the latent feature\nmaps for tasks T0 and T13 after the completion of continual learning. For T0, we observe that the SCL\nmethods are prone to catastrophic forgetting, as the features appear noisy and do not have coherent\npatterns. In contrast, the features learned by UCL strategies are perceptually relevant and robust to\ncatastrophic forgetting, with LUMP learning the most distinctive features. Similar to T0, we observe\nthat the UCL features are more relevant and distinguishable than SCL for T13. Note that we randomly\nselected the examples and feature maps for all visualizations.\nLoss landscape visualization. To gain further insights, we visualize the loss landscape of task T0\nafter the completion of training on task T0 and T19 for various UCL and SCL strategies in Figure 5.\nWe measure the cross-entropy loss for all methods with a randomly initialized linear classiﬁer for a\nfair evaluation of two different directions. We use the visualization tool from Li et al. (2018) that\nsearches the task loss surface by repeatedly adding random perturbations to model weights. We\nobserve that the loss landscape after T0 looks quite similar across all the strategies since the forgetting\ndoes not exist yet. However, after training T19, there is a clear difference with the UCL strategies\nobtaining a ﬂatter and smoother loss landscape because UCL methods are more stable and robust to\nthe forgetting, which hurts the loss landscapes of past tasks for SCL. It is important to observe that\nLUMP obtains a smoother landscape than other UCL strategies, demonstrating its effectiveness. We\ndefer further analyses for feature and loss landscape visualization to Appendix A.2.\n8\nPublished as a conference paper at ICLR 2022\nTable 3: ℓ2 distance between UCL parameters after\ncompletion of training.\nMODEL\nFINETUNE\nSI\nDER\nMULTITASK\nFINETUNE\n60.00 (± 1.70)\nSI\n76.46 (± 0.48)\n92.35 (± 0.61)\nDER\n55.60 (± 1.42)\n75.54 (± 0.97)\n48.76 (± 1.54)\nMULTITASK\n61.32 (± 0.59)\n79.95 (± 0.40)\n57.90 (± 0.86)\n61.42 (± 0.78 )\nTable 4: ℓ2 distance between SCL paraneters after\ncompletion of training.\nMODEL\nFINETUNE\nSI\nDER\nMULTITASK\nFINETUNE\n183.31 (± 0.10)\nSI\n206.16 (± 0.28)\n226.05 (± 0.13)\nDER\n202.61 (± 0.46)\n224.78 (± 0.75)\n219.06 (± 0.27)\nMULTITASK\n258.12 (± 0.26)\n277.30 (± 0.69)\n271.48 (± 0.45)\n314.84 (± 0.92)\nAPPLE (T0)\nSCL-FINETUNE\nAcc: 54.7 ± 0.2\nSCL-SI\nAcc: 58.9 ± 0.2\nSCL-GSS\nAcc: 78.4 ± 1.8\nSCL-DER\nAcc: 73.1 ± 0.4\nUCL-FINETUNE\nAcc: 70.8 ± 0.4\nUCL-SI\nAcc: 76.4 ± 1.6\nLUMP (OURS)\nAcc: 76.6 ± 2.7\nRACCOON (T13)\nSCL-FINETUNE\nAcc: 50.6 ± 1.4\nSCL-SI\nAcc: 48.4 ± 1.0\nSCL-GSS\nAcc: 59.9 ± 2.2\nSCL-DER\nAcc: 76.4 ± 2.2\nUCL-FINETUNE\nAcc: 74.6 ± 0.5\nUCL-SI\nAcc: 78.0 ± 1.6\nLUMP (OURS)\nAcc: 80.8 ± 0.5\nFigure 4: Visualization of feature maps for the second block representations learnt by SCL and UCL strategies\n(with Simsiam) for ResNet-18 architecture after the completion of CL for Split CIFAR-100 dataset (n = 20).\nT0\nT19\nSCL-FINETUNE\nAcc:54.73 ± 0.25\nSCL-SI\nAcc: 58.59 ± 0.20\nSCL-DER\nAcc: 73.13 ± 0.38\nUCL-FINETUNE\nAcc: 70.80 ± 0.40\nUCL-SI\nAcc: 76.39 ± 1.56\nLUMP (OURS)\nAcc:76.60 ± 2.70\nFigure 5: Loss landscape visualization of T0 after the completion of training on task T0 (top) and T19 (bottom)\nfor Split CIFAR-100 dataset on ResNet-18 architecture. We use Simsiam for UCL methods.\n6\nDISCUSSION AND CONCLUSION\nThis work attempts to bridge the gap between unsupervised representation learning and continual\nlearning. In particular, we establish the following ﬁndings for unsupervised continual learning.\nSurpassing supervised continual learning. Our empirical evaluation across various CL strategies\nand datasets shows that UCL representations are more robust to catastrophic forgetting than SCL\nrepresentations. Furthermore, we notice that UCL generalizes better to OOD tasks and achieves\nstronger performance on few-shot learning tasks. We propose Lifelong unsupervised mixup (LUMP),\nwhich interpolates the unsupervised instances between the current task and past task and obtains\nhigher performance with lower catastrophic forgetting across a wide range of tasks.\nDissecting the learned representations. We conduct a systematic analysis to understand the differ-\nences between the representations learned by UCL and SCL strategies. By investigating the similarity\nbetween the representations, we observe that UCL and SCL strategies have high similarities in the\nlower layers but are dissimilar in the higher layers. We also show that UCL representations learn\ncoherent and discriminative patterns and smoother loss landscape than SCL.\nLimitations and future work. In this work, we do not consider the high-resolution tasks for CL.\nWe intend to evaluate the forgetting of the learnt representations on ImageNet (Deng et al., 2009) in\nfuture work, since UCL shows lower catastrophic forgetting and representation learning has made\nsigniﬁcant progress on ImageNet over the past years. In follow-up work, we intend to conduct further\nanalysis to understand the behavior of UCL and develop sophisticated methods to continually learn\nunsupervised representations under various setups, such as class-incremental or task-agnostic CL.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENTS\nWe thank the anonymous reviewers for their insightful comments and suggestions. This work\nwas supported by Microsoft Research Asia, the Engineering Research Center Program through the\nNational Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-\n2018R1A5A1059921), Institute of Information & communications Technology Planning & Evalua-\ntion (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artiﬁcial Intelligence\nGraduate School Program (KAIST) and 2021-0-01696). Any opinions, ﬁndings, and conclusions or\nrecommendations expressed in this material are those of the authors and do not necessarily reﬂect the\nviews of the funding agencies.\nAUTHOR CONTRIBUTIONS\nDivyam Madaan conceived of the presented idea, developed the experimental framework, carried\nout OOD evaluation, CKA visualization and took the lead in writing the manuscript. Jaehong Yoon\nperformed the hyperparameter search, carried out the visualization of loss landscape and feature\nmaps and performed the few-shot training analysis. Yuanchun Li, Yunxin Liu, and Sung Ju Hwang\nsupervised the project.\nREFERENCES\nHongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup Moon. Uncertainty-based continual learning\nwith adaptive regularization. In Advances in Neural Information Processing Systems (NeurIPS),\n2019.\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection\nfor online continual learning. In Advances in Neural Information Processing Systems (NeurIPS),\n2019.\nHorace Barlow. Possible principles underlying the transformations of sensory messages. 1961.\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon\nGoyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,\nand Karol Zieba. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. Signature\nveriﬁcation using a \"siamese\" time delay neural network. 1994.\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark\nexperience for general continual learning: a strong, simple baseline. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020.\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient\nlifelong learning with a-gem. In Proceedings of the International Conference on Learning Repre-\nsentations (ICLR), 2019a.\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K\nDokania, Philip HS Torr, and M Ranzato. Continual learning with tiny episodic memories. arXiv\npreprint arXiv:1902.10486, 2019b.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In Proceedings of the International Conference on\nMachine Learning (ICML), 2020a.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big\nself-supervised models are strong semi-supervised learners. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2020b.\n10\nPublished as a conference paper at ICLR 2022\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020c.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Proceedings of the IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR), 2009.\nCarl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot\ntransfer. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In Proceedings of the International Conference on Machine Learning (ICML),\n2017.\nTimo Flesch, Jan Balaguer, Ronald Dekker, Hamed Nili, and Christopher Summerﬁeld. Comparing\ncontinual task learning in minds and machines. Proceedings of the National Academy of Sciences,\n2018.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investi-\ngation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211,\n2013.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\nBilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a\nnew approach to self-supervised learning. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE International Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE International Conference\non Computer Vision and Pattern Recognition (CVPR), 2020.\nChristopher J Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado, and Dominic King.\nKey challenges for delivering clinical impact with artiﬁcial intelligence. BMC medicine, 2019.\nSungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun. Mixco: Mix-up contrastive learning\nfor visual representation. arXiv preprint arXiv:2010.06300, 2020.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural\nnetwork representations revisited. In Proceedings of the International Conference on Machine\nLearning (ICML). PMLR, 2019.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05\n2012.\nAbhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. In\nProceedings of the International Conference on Machine Learning (ICML), 2012.\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\nKibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. i-mix: A\ndomain-agnostic strategy for contrastive representation learning. In ICLR, 2021.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape\nof neural nets. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n11\nPublished as a conference paper at ICLR 2022\nXilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual\nstructure learning framework for overcoming catastrophic forgetting. In Proceedings of the\nInternational Conference on Machine Learning (ICML), 2019.\nYuanpeng Li, Liang Zhao, Kenneth Church, and Mohamed Elhoseiny. Compositional language\ncontinual learning. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2020.\nZhizhong Li and Derek Hoiem. Learning without forgetting. In Proceedings of the European\nConference on Computer Vision (ECCV), 2016.\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation. 1989.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Under-\nstanding the role of training regimes in continual learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2020.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\ndigits in natural images with unsupervised feature learning. 2011.\nBehnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning?\nIn Advances in Neural Information Processing Systems (NeurIPS), 2020.\nDushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell.\nContinual unsupervised representation learning. In Advances in Neural Information Processing\nSystems, 2019.\nSylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:\nIncremental classiﬁer and representation learning. In Proceedings of the IEEE International\nConference on Computer Vision and Pattern Recognition (CVPR), 2017.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. In Advances in Neural Information Processing Systems (NeurIPS),\n2019.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint\narXiv:1606.04671, 2016.\nJonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame-\nwork for continual learning. In Proceedings of the International Conference on Machine Learning\n(ICML), 2018.\nZhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric Xing. Un-mix:\nRethinking image mixtures for unsupervised visual representation learning. In Proceedings of the\nAAAI National Conference on Artiﬁcial Intelligence (AAAI), 2022.\nJames Smith, Cameron Taylor, Seth Baer, and Constantine Dovrolis. Unsupervised progressive\nlearning and the stam architecture. In Proceedings of the International Joint Conference on\nArtiﬁcial Intelligence (IJCAI), 2021.\nSebastian Thrun. A Lifelong Learning Perspective for Mobile Robot Control. Elsevier, 1995.\nVikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. Towards domain-agnostic\ncontrastive learning. In Proceedings of the International Conference on Machine Learning (ICML),\n2021.\nZhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via\nnon-parametric instance discrimination. In Proceedings of the IEEE International Conference on\nComputer Vision and Pattern Recognition (CVPR), 2018.\n12\nPublished as a conference paper at ICLR 2022\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking\nmachine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically\nexpandable networks. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2018.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\nlearning via redundancy reduction. In Proceedings of the International Conference on Machine\nLearning (ICML), 2021.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\nIn Proceedings of the International Conference on Machine Learning (ICML), 2017.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2018.\nLinjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup\nhelp with robustness and generalization?\nIn Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\n13\nPublished as a conference paper at ICLR 2022\nA\nSUPPLEMENTARY MATERIAL\nOrganization. In the supplementary material, we provide the implementation details followed by\nthe hyper-parameter conﬁgurations in Appendix A.1. Further, we show the other experiments we\nconducted and additional visualizations and results in Appendix A.2.\nA.1\nEXPERIMENTAL DETAILS\nImplementations. We use the DER (Buzzega et al., 2020) open-source codebase1 for all the\nexperiments. In particular, we reproduce all their experimental results for supervised continual\nlearning and use various models with their set of hyper-parameters as our baselines. We follow the\noriginal representations for SimSiam2 and BarlowTwins3 for unsupervised continual learning. We\nverify our implementation by reproducing the reported results on CIFAR-10 in the original paper,\nwhere we train the representations on the complete CIFAR-10 dataset and evaluate on the test-set using\nKNN classiﬁer (Wu et al., 2018). In particular, (Wu et al., 2018) stores the features for each instance\nin the task-level training set in a discrete memory bank. The optimal feature-level embeddings are\nthen learned by instance-level discrimination, which maximally scatters the features of the training\nsamples. Following prior works in representation learning, we use the task-level training set without\nany augmentation in the task-incremental setup for the supervised and unsupervised KNN evaluation.\nHyperparameter conﬁgurations. We use the tuned hyper-parameters reported by Buzzega et al.\n(2020) for all the SCL experiments. On the other hand, we tune the hyper-parameters for continual\nlearning strategies for UCL. We provide the hyper-parameters setup for UCL for different datasets in\nTable A.5. We train all the UCL methods with a batch size of 256 for 200 epochs, while training the\nSCL methods with a batch size of 32 for 50 epochs following Buzzega et al. (2020). We observed that\ntraining the SCL methods further lead to a degredation in performance for all the methods. We use\nthe same set of augmentations for both SCL and UCL except that we use RandomResizedCrop\nwith scale in [0.2, 1.0] for UCL (Wu et al., 2018; Chen & He, 2021) and RandomCrop for SCL. For\nrehearsal-based methods, we use the buffer size 200 for Split CIFAR-10, Split CIFAR-100 and 256\nfor Split Tiny-ImageNet dataset. We use a learning rate of 0.03 for SGD optimizer with weight decay\n5e-4 and momentum 0.9.\nTable A.5: Hyperparameter conﬁgurations for all the datasets on ResNet-18 architecture.\nMETHOD\nSPLIT CIFAR-10\nSPLIT CIFAR-100\nSEQ. TINY-IMAGENET\nSI\nc : 100\nξ : 1\nc : 0.1\nξ : 1\nc : 0.01\nξ : 1\nPNN\nwd : 64\nwd : 12\nwd : 8\nDER\nα : 0.1\nα : 0.1\nα : 0.01\nLUMP\nλ : 0.1\nλ : 0.1\nλ : 0.4\nA.2\nADDITIONAL EXPERIMENTS\nWe provide additional loss landscape on Split CIFAR-100 in Figure A.6 and Figure A.7, Figure A.8\nshow the second and third block feature visualizations on Split CIFAR-100 respectively. Figure A.9\nshows the feature visualizations for Split Tiny-ImageNet on ResNet-18 architecture.\n1https://github.com/aimagelab/mammoth\n2https://github.com/facebookresearch/simsiam\n3https://github.com/facebookresearch/barlowtwins\n14\nPublished as a conference paper at ICLR 2022\nT0\nT17\nT18\nT19\nSCL-FINETUNE\nAcc: 54.73 ± 0.25\nSCL-SI\nAcc: 58.59 ± 0.20\nSCL-DER\nAcc: 73.13 ± 0.38\nUCL-FINETUNE\nAcc: 70.80 ± 0.40\nUCL-SI\nAcc: 76.39 ± 1.59\nLUMP (OURS)\nAcc: 76.60 ± 2.70\nFigure A.6: Loss landscape visualization of T0 after the completion of training on task T0, T17, T18, and T19 for\nSplit CIFAR-100 dataset on ResNet-18 architecture. We use Simsiam for UCL methods.\n15\nPublished as a conference paper at ICLR 2022\nRAW IMAGES\nT0, T1, T2, T13\nSCL-FINETUNE\nAcc:61.08 ± 0.04\nSCL-SI\nAcc:63.58 ± 0.37\nSCL-GSS\nAcc:70.78 ± 1.67\nSCL-DER\nAcc:79.52 ± 1.88\nUCL-FINETUNE\nAcc:75.42 ± 0.78\nUCL-SI\nAcc:80.08 ± 1.30\nLUMP (OURS)\nAcc:82.30 ± 1.35\nFigure A.7: Visualization of feature maps for the second block representations learnt by SCL and UCL strategies\n(with Simsiam) for Resnet-18 architecture after the completion of continual learning for Split CIFAR-100 dataset\n(n = 20). The accuracy is the mean across three runs for the corresponding task.\n16\nPublished as a conference paper at ICLR 2022\nRAW IMAGES\nT0, T1, T2, T13\nSCL-FINETUNE\nAcc:61.08 ± 0.04\nSCL-SI\nAcc:63.58 ± 0.37\nSCL-GSS\nAcc:70.78 ± 1.67\nSCL-DER\nAcc:79.52 ± 1.88\nUCL-FINETUNE\nAcc:75.42 ± 0.78\nUCL-SI\nAcc:80.08 ± 1.30\nLUMP (OURS)\nAcc:82.30 ± 1.35\nFigure A.8: Visualization of feature maps for the third block representations learnt by SCL and UCL strategies\n(with Simsiam) for Resnet-18 architecture after the completion of continual learning for Split CIFAR-100 dataset\n(n = 20). The accuracy is the mean across three runs for the corresponding task.\n17\nPublished as a conference paper at ICLR 2022\nRAW IMAGES\nT0, T1, T2, T10\nSCL-FINETUNE\nAcc:53.10 ± 1.37\nSCL-SI\nAcc:44.96 ± 2.41\nSCL-GSS\nAcc:70.96 ± 0.72\nSCL-DER\nAcc:68.03 ± 0.85\nUCL-FINETUNE\nAcc:71.07 ± 0.20\nUCL-SI\nAcc:72.34 ± 0.42\nLUMP (OURS)\nAcc:76.66 ± 2.39\nFigure A.9: Visualization of feature maps for the second block representations learnt by SCL and UCL strategies\n(with Simsiam) for Resnet-18 architecture after the completion of continual learning for Split Tiny-ImageNet dataset\n(n = 20). The accuracy is the mean across three runs for the corresponding task.\n18\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2021-10-13",
  "updated": "2022-04-05"
}