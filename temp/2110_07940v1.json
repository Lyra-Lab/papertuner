{
  "id": "http://arxiv.org/abs/2110.07940v1",
  "title": "Wasserstein Unsupervised Reinforcement Learning",
  "authors": [
    "Shuncheng He",
    "Yuhang Jiang",
    "Hongchang Zhang",
    "Jianzhun Shao",
    "Xiangyang Ji"
  ],
  "abstract": "Unsupervised reinforcement learning aims to train agents to learn a handful\nof policies or skills in environments without external reward. These\npre-trained policies can accelerate learning when endowed with external reward,\nand can also be used as primitive options in hierarchical reinforcement\nlearning. Conventional approaches of unsupervised skill discovery feed a latent\nvariable to the agent and shed its empowerment on agent's behavior by mutual\ninformation (MI) maximization. However, the policies learned by MI-based\nmethods cannot sufficiently explore the state space, despite they can be\nsuccessfully identified from each other. Therefore we propose a new framework\nWasserstein unsupervised reinforcement learning (WURL) where we directly\nmaximize the distance of state distributions induced by different policies.\nAdditionally, we overcome difficulties in simultaneously training N(N >2)\npolicies, and amortizing the overall reward to each step. Experiments show\npolicies learned by our approach outperform MI-based methods on the metric of\nWasserstein distance while keeping high discriminability. Furthermore, the\nagents trained by WURL can sufficiently explore the state space in mazes and\nMuJoCo tasks and the pre-trained policies can be applied to downstream tasks by\nhierarchical learning.",
  "text": "Wasserstein Unsupervised Reinforcement Learning\nShuncheng He\nDepartment of Automation\nTsinghua University\nBeijing, China\nhesc16@mails.tsinghua.edu.cn\nYuhang Jiang\nDepartment of Automation\nTsinghua University\nBeijing, China\njiangyh19@mails.tsinghua.edu.cn\nHongchang Zhang\nDepartment of Automation\nTsinghua University\nBeijing, China\nhc-zhang19@mails.tsinghua.edu.cn\nJianzhun Shao\nDepartment of Automation\nTsinghua University\nBeijing, China\nsjz18@mails.tsinghua.edu.cn\nXiangyang Ji\nDepartment of Automation\nTsinghua University\nBeijing, China\nxyji@mail.tsinghua.edu.cn\nAbstract\nUnsupervised reinforcement learning aims to train agents to learn a handful of\npolicies or skills in environments without external reward. These pre-trained poli-\ncies can accelerate learning when endowed with external reward, and can also be\nused as primitive options in hierarchical reinforcement learning. Conventional ap-\nproaches of unsupervised skill discovery feed a latent variable to the agent and shed\nits empowerment on agent’s behavior by mutual information (MI) maximization.\nHowever, the policies learned by MI-based methods cannot sufﬁciently explore the\nstate space, despite they can be successfully identiﬁed from each other. Therefore\nwe propose a new framework Wasserstein unsupervised reinforcement learning\n(WURL) where we directly maximize the distance of state distributions induced by\ndifferent policies. Additionally, we overcome difﬁculties in simultaneously training\nN(N > 2) policies, and amortizing the overall reward to each step. Experiments\nshow policies learned by our approach outperform MI-based methods on the metric\nof Wasserstein distance while keeping high discriminability. Furthermore, the\nagents trained by WURL can sufﬁciently explore the state space in mazes and\nMuJoCo tasks and the pre-trained policies can be applied to downstream tasks by\nhierarchical learning.\n1\nIntroduction\nAutonomous agents can learn to solve challenging tasks by deep reinforcement learning, including\nlocomotive manipulation (Lillicrap et al. [2015]; Haarnoja et al. [2018]) and game playing (Mnih\net al. [2015]; Silver et al. [2016]). The reward signal speciﬁed by the task plays an important role of\nsupervision in reinforcement learning. However, recent research reveals the possibilities that agents\ncan acquire diverse skills or policies in the absence of reward signal (Eysenbach et al. [2019]; Gregor\net al. [2016]; Achiam et al. [2018]). This setting is called unsupervised reinforcement learning.\nPreprint. Under review.\narXiv:2110.07940v1  [cs.LG]  15 Oct 2021\nFigure 1: Examples of Jensen-Shannon divergence and Wasserstein distance between q0 and q1. q0\nand q1 are uniform distributions over a round plate with radius r. The distance between the centers of\nq0, q1 varies from 0 to 3r.\nPractical applications of unsupervised reinforcement learning have been studied. The skills learned\nwithout reward can serve as primitive options for hierarchical RL in long horizon tasks (Eysenbach\net al. [2019]). Also the primitive options may be useful for transferring across different tasks. In\nmodel-based RL, the learned skills enables the agent to plan in the skill space (Sharma et al. [2020]).\nUnsupervised learning methods may alleviate the cost of supervision: in certain cases, designing\nreward function requires human supervision (Christiano et al. [2017]). The intrinsic reward derived\nfrom unsupervised learning can enhance exploration when combined with task reward (Houthooft\net al. [2016]; Gupta et al. [2018]).\nThe key point of unsupervised reinforcement learning is how to learn a set of policies that can\nsufﬁciently explore the state space. Previous methods make use of a latent variable and maximize\nthe mutual information (MI) between the latent variable and the behavior (Eysenbach et al. [2019]).\nConsequently the diversity in the latent space is cast into the state space. These methods are able to\nobtain different skills which are distinguishable from each other. However, limitations of MI-based\nmethods are pointed out as the diversity of learned skills is restricted by the Shannon entropy of the\nlatent variable. In addition, discriminability of skills does not always lead to the goal of sufﬁcient\nexploration of the environment.\nIn this paper, we propose a new approach of unsupervised reinforcement learning which is essentially\ndifferent form MI-based methods. The motivation of our method is to increase the discrepancy of\nlearned policies so that the agents can explore the state space extensively and reach the state as\n“far” as possible compared to other policies. This idea incentivizes us to employ a geometry-aware\nmetric to measure the discrepancy between the state distributions induced by different policies. In\nrecent literature of generative modeling, the optimal transport (OT) cost is a new direction to measure\ndistribution distance (Tolstikhin et al. [2018]) since it provides a more geometry-aware topology than\nf-divergences, including GAN (Nowozin et al. [2016]). Therefore, we choose Wasserstein distance,\na well-studied distance from optimal transport, to measure the distance between different policies in\nunsupervised reinforcement learning. By maximizing Wasserstein distance, the agents equipped with\ndifferent policies may drive themselves to enter different areas of state space and keep as “far” as\npossible from each other to earn greater diversity.\nOur contributions are four-fold. First, we propose a novel framework adopting Wasserstein distance\nas discrepancy measure for unsupervised reinforcement learning. This framework is well-designed to\nbe compatible with various Wasserstein distance estimation algorithms, both in primal form and in\ndual form. Second, as Wasserstein distance is deﬁned on two distributions, we extend our framework\nto multiple policy learning. Third, to address the problem of sparse reward provoked by Wasserstein\ndistance estimation, we devise an algorithm to amortize Wasserstein distance between two bunches\nof samples to stepwise intrinsic reward. Four, we empirically demonstrate our approach surpasses the\ndiversity of MI-based methods and can cover the state space by incremental learning.\n2\nDistribution discrepancy measure\nIn this Section, we brieﬂy review Wasserstein distance and its estimation methods.\n2\n2.1\nWasserstein distance and optimal transport\nMeasuring discrepancy or distance between two probability distributions can be seen as a transport\nproblem (Villani [2009]). Consider two distributions p, q on domains X ⊆Rn and Y ⊆Rm. Let\nΓ[p, q] be the set of all distributions on product space X × Y, with their marginal distributions on\nX and Y being p, q respectively. Therefore, given a proper cost function c(x, y) : X × Y →R for\nmoving mass from x to y, the Wasserstein distance is deﬁned as\nWc(p, q) =\ninf\nγ∈Γ[p,q]\nZ\nX×Y\nc(x, y)dγ.\n(1)\nThe joint distribution family Γ[p, q] essentially forms a family of bijective plans transporting probabil-\nity mass from p to q. Minimizing the transport cost is an optimal transport problem. The optimization\nproblem suffers from supercubic complexity, since the problem becomes linear programming when\nX and Y are ﬁnite discrete sets (Cuturi [2013]; Genevay et al. [2016]). To avoid suboptimal solutions,\na regularizer is added to the optimization objective. The smoothed Wasserstein distance is deﬁned as\n˜Wc(p, q) =\ninf\nγ∈Γ[p,q]\n\u0014Z\nX×Y\nc(x, y)dγ + βKL(γ|pq)\n\u0015\n.\n(2)\nMinimizing the cost together with the KL divergence encourages the joint distribution γ(x, y) to\nmove close to p(x)q(y). As β →0, the smoothed distance converges to Wc(p, q) (Pacchiano et al.\n[2020]).\nThe objective function is convex if c(x, y) is a proper cost function. Therefore the inﬁmum can be\ncalculated either by primal formulation or dual formulation. In Section 2.2 and 2.3, we introduce\npractical methods estimating Wasserstein distance from distribution samples.\n2.2\nPrimal form estimation\nSolving the optimal transport problem from the primal formulation is hard. However, the problem has\nanalytical solutions when the distributions are on one-dimensional Euclidean space and cost function\nis lp measure (p ≥0) (Rowland et al. [2019]). Inspired by 1-D Wasserstein distance estimation, we\nmay estimate Wasserstein distance in high dimensional Euclidean spaces by projecting distributions\nto R. Suppose p, q are probability distributions on Rd. For a vector v on the unit sphere Sd−1 in\nRd, the projected distribution Πv(p) is the marginal distribution along the vector by integrating p in\nthe orthogonal space of v. Estimating Wasserstein distance on 1-D space results sliced Wasserstein\ndistance (SWD) (Wu et al. [2019]; Kolouri et al. [2018]):\nSW(p, q) = Ev∼U(Sd−1) [W(Πv(p), Πv(q))] ,\n(3)\nwhere U(Sd−1) means the uniform distribution on unit sphere Sd−1. In practical use, the pro-\njected distribution Πv(ˆp) of empirical distribution ˆp =\n1\nN\nPN\nn=1 δxn can be written as Πv(ˆp) =\n1\nN\nPN\nn=1 δ⟨xn,v⟩, where ⟨·, ·⟩denotes inner product and δ is Dirac distribution.\nTo reduce estimation bias of SWD, Rowland et al. [2019] proposed projected Wasserstein distance\n(PWD) by disentangling coupling calculation and cost calculation. PWD obtains optimal coupling by\nprojecting samples to R, and calculates costs in original space Rd rather than the projected space.\n2.3\nDual form estimation\nDeﬁne set A = {(u, v)|∀(x, y) ∈X × Y : u(x) −v(y) ≤c(x, y)}. By Fenchel-Rockafellar duality,\nthe dual form of Wasserstein distance is (Villani [2009])\nWc(p, q) =\nsup\n(µ,ν)∈A\nEx∼p(x),y∼q(y) [µ(x) −ν(y)] ,\n(4)\nwhere µ : X →R and ν : Y →R are continuous functions on their domains. The dual formulation\nprovides us a neural approach to estimate Wasserstein distance, circumventing the difﬁculties to\nﬁnd the optimal transport plan between two probability distributions. The dual form of smoothed\nWasserstein distance shows more convenience since there are no constraints on µ, ν:\n˜Wc(p, q) = sup\nµ,ν Ex∼p(x),y∼q(y)\n\u0014\nµ(x) −ν(y) −β exp\n\u0012µ(x) −ν(y) −c(x, y)\nβ\n\u0013\u0015\n.\n(5)\n3\nAlternative dual formulation emerges when X = Y. It is the most common case that two distributions\nare deﬁned in the same space. Under this assumption, Kantorovich-Rubinstein duality gives another\ndual form objective in which only one function with Lipschitz constraint is optimized (Villani [2009]).\nWc(p, q) =\nsup\n∥f∥L≤1\nEx∼p(x),y∼q(y) [f(x) −f(y)] .\n(6)\nThe maxima of dual problem theoretically equals to the minima of primal problem. However, sliced\nWasserstein distance and projected Wasserstein distance have no such guarantee. Nonetheless, the\nprimal form estimation methods show competitive accuracy empirically.\n3\nWasserstein unsupervised reinforcement learning\n3.1\nMI-based unsupervised reinforcement learning\nTraditional unsupervised reinforcement learning adopts mutual information to seek diverse skills.\nMutual information between two random variables is popularly perceived as the degree of empower-\nment (Gregor et al. [2016]; Kwon [2021]): I(X; Y ) = H(X) −H(X|Y ) = H(Y ) −H(Y |X). For\ninstance, DIAYN (Eysenbach et al. [2019]) mainly aims to maximize I(S; Z), the mutual information\nbetween latent variables and states reached by agent. Conventionally, the prior of latent variable\np(z) is ﬁxed to uniform distribution which has maximal entropy. The maximization process of\nI(S; Z) broadcasts the diversity in Z to states S through policy π(a|s, z). However, estimating\nmutual information involves intractable posterior distribution p(z|s). With the tool of variational\ninference, a feasible lower bound is ready by approximating p(z|s) with qφ(z|s). We call qφ(z|s) a\nlearned discriminator trying to recognize the latent variable behind the policy from behavior. For\nexample, the discriminator is a neural net predicting labels as in classiﬁcation tasks when p(z) is a\ncategorical distribution.\nFrom the view of optimization, the agent and the discriminator are trained in the cooperative way\nto maximize the same objective. This learning process comes to the end immediately after the\ndiscriminator can successfully infer z behind the policy. However, the learned policy is not naturally\ndiverse enough. We will explain this claim by a simple example. Suppose the latent variable Z\nis randomly selected from {0, 1}. The mutual information I(S; Z) equals to the Jensen-Shannon\ndivergence between the two conditional probability distributions q0 = p(s|Z = 0) and q1 = p(s|Z =\n1). As illustrated in Fig. 1, the JS divergence reaches the maximal value when the supports of q0, q1\ndo not overlap. Actually, the decomposition of mutual information I(S; Z) = H(Z) −H(Z|S) also\nimplies that I(S; Z) is upper bounded by H(Z), which is ﬁxed by a predetermined distribution.\nTo address this issue, we propose our method that uses Wasserstein distance as intrinsic reward to\nencourage the agent to explore for farther states. In Fig. 1, Wasserstein distance provides information\nabout how far the two distributions are, while the JS divergence fails. Therefore, our method will\ndrive the agent to reach different areas as far as possible in the unknown space of valid states.\n3.2\nWasserstein distance as intrinsic reward\nThe Wasserstein distance can only measure discrepancy between two distributions. Therefore for\nthe most naive approach to Wasserstein unsupervised reinforcement learning (WURL), we train\na policy pair parameterized by πθ1, πθ2, with Wasserstein distance between the state distributions\npπθ1(s), pπθ2(s) as their intrinsic reward.\nAs Pacchiano et al. [2020] mentioned, dual form estimation allows us to assign reward at every step\nusing test functions. We adopt two manners of dual formulation, TF1 (Arjovsky et al. [2017]) and\nTF2 (Abdullah et al. [2018]). TF1 has one test function f with Lipschitz constraint optimizing the\nobjective in Eqn. 6. Meanwhile TF2 has two test functions µ, ν without any constraint. µ, ν are\ntrained according to Eqn. 5. As long as the test functions are optimal dual functions, the test functions\ngive scores of each state. By splitting the maximization objective in Eqn. 6, we can assign f(x)\nas reward for policy 1, and assign −f(y) as reward for policy 2 to push Wc(p, q) higher. Similar\ntreatment can be applied to TF2. Combining RL training and test function training, we obtain Alg. 1.\nHowever, primal form estimation can only be executed after policy rollout by collecting states in one\nepisode and compute the distance with states sampled from the replay buffer from another policy.\nWe refer this pattern of reward granting to Alg. 2. The challenge of sparse reward emerges in this\n4\nAlgorithm 1 Naive WURL (test function)\nInitialize two policy πθ1, πθ2, and replay buffers for each policy D1 = {}, D2 = {}. Intialize test\nfunctions.\nwhile Maximum number of episode is not reached do\nSelect policy l randomly or in turn. done = False.\nwhile Not done do\nSample action from πθl, execute, and receive s′ and done.\nif l = 1 then\nSet reward r = f(s) or r = µ(s).\nelse\nSet reward r = −f(s) or r = −ν(s).\nend if\nDl=Dl ∪{(s, a, s′, r)}.\nTrain πθl with SAC.\nTrain test functions by sampling D1, D2.\nend while\nend while\ntraining manner since the agent receives no reward until the episode ends. We will address this issue\nin Section 3.4.\nBackend training algorithm of RL can vary. Off-policy algorithms like Soft-Actor-Critic (SAC)\n(Haarnoja et al. [2018]) and on-policy algorithms like TRPO (Schulman et al. [2015]), PPO (Schulman\net al. [2017]) can all be deployed on WURL. Since SAC enjoys higher sample efﬁciency and suits\nenvironments with continuous action space, we choose SAC as our backend RL algorithm in our\nexperiments.\nAlgorithm 2 Naive WURL (ﬁnal reward)\nInitialize two policy πθ1, πθ2, and replay buffers for each policy D1 = {}, D2 = {}.\nwhile Maximum number of episode is not reached do\nSelect policy l randomly or in turn. Set trajectory S = {}. done = False.\nwhile Not done do\nSample action from πθl, execute, and receive s′ and done.\nS = S ∪{s′}. Set reward r = 0.\nif done then\nSample target batch of states T from D3−l.\nSet reward r = W(S, T ) by any Wasserstein distance estimation method.\nend if\nDl=Dl ∪{(s, a, s′, r)}.\nTrain πθl with SAC.\nTrain test functions if WDE algorithm is one of the dual form methods.\nend while\nend while\n3.3\nLearning with multiple policies\nLearning N(N > 2) policies at the same time requires the arbitrary policy i to keep distance with\nall other policies. We expect the policy to maximize the average Wasserstein distance between state\ndistribution pi induced by policy i, and other state distributions\n1\nN−1\nPN\nj=1,j̸=i W(pi, pj). Also we\ncan use the Wasserstein distance between pi and the average distribution of all others’. However this\nincurs underestimation due to the inequality\nW\n\npi,\n1\nN −1\nN\nX\nj=1,j̸=i\npj\n\n≤\n1\nN −1\nN\nX\nj=1,j̸=i\nW(pi, pj).\n(7)\nPractically we use minN\nj=1,j̸=i W(pi, pj) as reward to keep the current policy away from the nearest\npolicy. As the number of policies growing, the number of times of distance computing grows as\n5\nFigure 2: In the FreeRun environment, (a) and (b) show the results of learning 2 policies. (c) and\n(d) show the results of learning 10 policies at the same time. (a) and (c) show the success rate of the\ndiscriminator distinguishing policies. (b) and (d) show the mean Wasserstein distance between every\ntwo policies. Error bars represent standard deviations across 5 random runs.\nO(N 2). The dual form requires O(N 2) test functions which provokes memory and time consumption\nconcern since every test function is a neural network and needs training. The primal form reward\ncomputation complexity also rises to O(N 2). Fortunately, sliced Wasserstein distance or projected\nWasserstein distance gives us a faster and more lightweight solution without training and inferring\nthrough neural networks.\n3.4\nAmortized reward\nIn contrast to test functions, the primal form estimation of Wasserstein distance produces one ﬁnal\nreward at the end of an episode since we cannot estimate distribution distance from one sample. This\nnature of primal form estimation incurs sparse reward which imposes challenges on reinforcement\nlearning and may impair the performance of value-based RL algorithms (Andrychowicz et al. [2017]).\nNoting that the primal form estimation automatically yields an optimal matching plan, we could\ndecompose the overall Wasserstein distance into every sample. Formally speaking, suppose batch\nS = {xn}N\nn=1 is the set of states in one episode, and batch T = {ym}M\nm=1 is the state sample set\nof target distribution. We further suppose PN×M is the optimal matching matrix given cost matrix\nCN×M. Denote Pi as the ith row of matrix P, then the sample xi has its own credit by computing\nPiC⊤1 where 1 is N × 1 vector ﬁlled with ones. Combining with projected Wasserstein distance,\nwe have the following algorithm for crediting amortized reward. The matching matrix computation\nalgorithm is stated in Appendix A.2.\nAlgorithm 3 Amortized reward crediting\nGiven source batch S = {xn}N\nn=1 and target batch T = {ym}M\nm=1.\nCompute cost matrix CN×M\nfor k from 1 to K do\nSample vk from U(Sd−1)\nCompute projected samples ˆx(k)\nn\n= ⟨xn, vk⟩, ˆy(k)\nm = ⟨ym, vk⟩\nCompute matching matrix P (k)\nN×M from projected samples\nCompute reward vector r(k) = P (k)C⊤1\nend for\nReturn mean reward vector r = 1\nK\nPK\nk=1 r(k)\n3.5\nTraining schedule\nOur proposed method can either train N policies at the same time or train incrementally by introducing\nnew policies. Provided N diverse policies, a new policy is trained to maximize the average Wasserstein\ndistance from other policies by collecting state samples of policy 1, 2, . . . , N at the beginning.\n6\nFigure 3: Visualized policies in MuJoCo Ant environment. The left column shows the rendered\ntrajectories of 5 out of 10 total policies. The right column shows the X-Y position trajectories of 10\npolicies with different colors.\nAlgorithm\nMetric\nHalfCheetah\nAnt\nHumanoid\nAPWD(Ours)\nDSR\n0.981 ± 0.010\n0.989 ± 0.002\n0.998 ± 0.002\nWD\n39.21 ± 0.44\n8.57 ± 0.39\n556.96 ± 27.78\nDIAYN-I\nDSR\n0.978 ± 0.006\n0.985 ± 0.001\n0.999 ± 0.001\nWD\n13.01 ± 2.89\n4.50 ± 0.27\n279.94 ± 27.98\nDADS-I\nDSR\n0.994 ± 0.003\n0.793 ± 0.035\n0.999 ± 0.001\nWD\n11.75 ± 1.90\n7.53 ± 0.27\n420.67 ± 56.83\nTable 1: Policy diversity on three MuJoCo locomotion environments. Metric DSR represents the\ndiscriminator success rate and WD denotes the mean Wasserstein distance between two policies.\nThe incremental training schedule provides ﬂexibility of extending the number of diverse policies,\nespecially when we are agnostic about how many policies are suitable for a certain environment\nbeforehand. On the contrary, mutual information based unsupervised reinforcement learning is\nlimited by ﬁxed number of policies that cannot be easily extended due to the ﬁxed neural network\nstructure.\nSimilar idea appears in Achiam et al. [2018], in which skills are trained by a curriculum approach.\nThe objective of curriculum training is to ease the difﬁculty of classifying large number of skills.\nHowever, the maximum number of skills is still ﬁxed in advance therefore one cannot ﬂexibly add\nnew policies in.\n4\nExperiments\n4.1\nPolicy diversity\nWe ﬁrst examine our methods on a FreeRun environment where the particle agent spawns at the\ncenter. The agent can only control the acceleration on the X-Y plane and take the current position\nand velocity as observation. The particle runs freely on the plane with a velocity limit for a ﬁxed\nnumber of steps. We compare the performance of the two groups of algorithms:\n• Mutual information as intrinsic reward: DIAYN, DADS;\n• Wasserstein distance as intrinsic reward: TF1, TF2, PWD, APWD.\n7\nFigure 4: Results of training a bunch of policies incrementally. From left to right, new policies tend\nto reach new territory. As the number of policy n grows, the policies gradually ﬁll the state space.\nDADS (Sharma et al. [2020]) is another mutual information based algorithm that maximizes\nI(St+1; Z|St) utilizing model-based framework. It claims lower entropy of learned skills than\nDIAYN. TF1 and TF2 are two methods which adopt test function optimizing Eqn. 6 and Eqn. 5 re-\nspectively. PWD (Projected Wasserstein Distance) uses a primal form Wasserstein distance estimation\nmethod described in Section 2.2, and APWD is the amortized version of PWD to avoid sparse reward\nstated in Section 3.4. In the setting of learning 10 policies, TF1 and TF2 are unavailable since they are\nnot compatible with multiple policies as mentioned in Section 3.2. We examine these methods from\ntwo aspects. First, we train a neural network (discriminator) to distinguish each policy from rollout\nstate samples, and compare the success rate of the discriminator. Second, we estimate the mean\nWasserstein distance between every two state distributions of two different policies. Fig. 2 shows\npolicies learned by Wasserstein distance based algorithms generally has greater discriminability,\nand larger distance between the state distributions of every two policies. Fig. 2 also demonstrates\namortized reward improves performance.\nWe also examine our algorithms on several MuJoCo tasks, including three classical locomotion\nenvironments, and two customized point mass environments where a simpliﬁed ball agent wanders\nin different maps with various landscapes and movable objects (see Appendix for demonstrations).\nConsidering the signiﬁcantly larger action space and state space in MuJoCo environments, we replace\nthe shared actor network π(a|s, z), z = OneHot(i) in DIAYN and DADS with N individual networks\nπi(a|s), i = 1, . . . , N, while keep the discriminator network unchanged. DIAYN and DADS with\nindividual actors (DIAYN-I and DADS-I for short) enjoy greater parameter space and they are capable\nto learn a more diverse set of policies in MuJoCo tasks. Table 1 shows in the most cases, all three\nunsupervised RL approaches yield highly distinguishable policies. However, APWD achieves better\nperformance on Wasserstein distance, which means the policies learned by APWD are more distantly\ndistributed in the state space. Fig. 3 visualizes the differences of three policy sets in MuJoCo Ant\nenvironment and clearly shows that APWD encourages the policies to keep far from each other. The\nresults verify our hypothesis in Section 3.1, that mutual information based intrinsic reward is unable\nto drive the policies far from each other when JS divergence saturates.\n4.2\nIncremental learning\nOur method provides ﬂexibility to enlarge the policy set. New policies can be trained one by one,\nor trained based on policies in hand. We show the process of incremental learning to illustrate how\nthe newly learned policies gradually ﬁll the state space in Fig. 4. As new policies added in, the\nparticle agent in FreeRun and TreeMaze runs to new directions and new areas, and behaves differently\n(dithering around, turning, etc.).\n8\nAlgorithm\nFreeRun\nAnt\nAPWD(Ours)\n125.56 ± 12.63\n100.00 ± 18.03\nDIAYN-I\n15.06 ± 26.97\n35.00 ± 7.07\nDADS-I\n109.78 ± 27.08\n46.00 ± 10.84\nTable 2: Rewards of meta-policies on two hierarchical RL scenarios. Each meta-policy is trained with\na sub-policy set which contains 10 policies pre-trained with a speciﬁc unsupervised RL algorithm\nlisted above.\n4.3\nDownstream tasks\nIn previous unsupervised RL literature, the policies (or skills) learned without any task reward can be\nutilized either in hierarchical reinforcement learning or in planning (Eysenbach et al. [2019]; Sharma\net al. [2020]). Likewise, we examined our methods on downstream tasks including two navigation\ntasks based on the particle environment FreeRun and MuJoCo Ant. Both tasks require the agent\nto reach speciﬁc goals in a given order. The agent receives +50 reward for each goal reached. In\nFreeRun navigation task, we penalize each step with small negative reward to encourage the agent to\nﬁnish the task as quickly as possible.\nTo tackle these navigation tasks with pre-trained policies, we employ a meta-policy to choose one\nsub-policy to execute during H steps (H is ﬁxed in advance in our tasks). The meta-policy observes\nagent state every H steps , chooses an action corresponded to a sub-policy, and then receives the\nreward that the sub-policy collected during the successive H steps. Therefore, we can train the\nmeta-policy with any compatible reinforcement learning algorithms. In our experiments, we adopt\nPPO as meta-policy trainer (Schulman et al. [2017]).\nTable 2 presents the results on FreeRun and MuJoCo Ant navigation tasks. The pre-trained 10 policies\nwith DIAYN-I, DADS-I and our proposed method APWD serve as the sub-policies in the hierarchical\nframework. Since our method yields more diverse and distant sub-policies, APWD based hierarchical\npolicy achieves higher reward in both navigation tasks without doubt.\n5\nRelated work\nLearning in a reward-free environment has been attracting reinforcement learning researchers for\nlong. Early research takes mutual information as maximization objective. They explored the topics\non which variable of the policy should be controlled by the latent variable, and how to generate the\ndistribution of the latent variable. VIC (Gregor et al. [2016]) maximizes I(Z; Sf) to let the ﬁnal\nstate of a trajectory be controlled by latent code Z, while allowing to learn the prior distribution\np(z). VALOR (Achiam et al. [2018]) takes similar approach maximizing I(Z; τ) where τ denotes\nthe whole trajectory, but keeps p(z) ﬁxed by Gaussian distribution. Hausman et al. [2018] uses a\nnetwork to embed various tasks to the latent space. DIAYN (Eysenbach et al. [2019]) improves the\nperformance by maximizing I(Z; S), ﬁxing prior distribution, while minimizes I(Z; A|S). Recent\npapers show their interests on better adapting unsupervised skill discovery algorithm with MDP, on\nthe aspect of transition model and initial state. DADS (Sharma et al. [2020]) gives a model based\napproach by maximizing I(S′; Z|S) so that the learned skills can be employed in planning. Baumli\net al. [2020] alternates the objective to I(Sf; Z|S0) in order to avoid state partitioning skills in case\nof various start states.\nHowever, the nature of these methods restrict the diversity of learned skills since the mutual infor-\nmation is upper bounded by H(Z). Recent research on mutual information based methods tries to\nenlarge the entropy of the prior distribution through ﬁtting the uniform distribution on valid state space\nU(S). EDL (Campos et al. [2020]) takes three seperate steps by exploring the state space, encoding\nskills and learning skills. EDL ﬁrst uses state marginal matching algorithm to yield a sufﬁently\ndiverse distribution of states p(s), then a VQ-VAE is deployed to encode the state space to the latent\nspace, which creates p(z|s) as the discriminator to train the agent. Skew-ﬁt (Pong et al. [2019]) adopts\ngoal conditioned policies where the goal is sampled through importance sampling in the skewed\ndistribution of p(s) acquired by current policy. The agent can gradually extend their knowledge of\nstate space by ﬁtting the skewed distribution. Both methods claim they have state-covering skills.\n9\nWasserstein distance as an alternative distribution discrepancy measure is attracting machine learning\nresearchers recently (Ozair et al. [2019]). Especially in the literature of generative models (Arjovsky\net al. [2017]; Ambrogioni et al. [2018]; Patrini et al. [2020]; Tolstikhin et al. [2018]), Wasserstein\ndistance behaves well in the situations where distributions are degenerate on a sub-manifold in\npixel space. In reinforcement learning, Wasserstein distance is used to characterize the differences\nbetween policies instead of commonly used f-divergences, e.g., KL divergence (Zhang et al. [2018]).\nPacchiano et al. [2020] reports improvements in trust region policy optimization and evolution\nstrategies, and Dadashi et al. [2021] shows its efﬁcacy in imitation learning by minimizing Wasserstein\ndistance between behavioral policy and expert policy. Our proposed method inherits the motivations\nof using Wasserstein distance as a new distribution discrepancy measure in generative models and\npolicy optimization. To increase the diversity of policies in unsupervised reinforcement learning,\nWasserstein distance appears to be a more appropriate measure than f-divergences derived from\nmutual information based methods.\n6\nDiscussion\n6.1\nLimitations\nOur work uses Wasserstein distance as a new metric measuring discrepancy between probability\ndistributions. Although this new metric provides better performance on unsupervised reinforcement\nlearning in certain environments stated in Section 4, there are limitations or difﬁculties for further\napplication. First, Wasserstein distance depends on cost function c(x, y). In our experiments,\nwe choose l2-norm since the agent is conducting navigation tasks. However, choosing a proper\ncost function may be difﬁcult for other reinforcement learning environments. Second, different\ndimensions of state space may have different semantic intention such as position, force, angle.\nTherefore, implementing WURL in the full dimension of state space may not properly balance\nbetween different dimensions. Third, image-based observations could not be used for calculating\nWasserstein distance directly. Nevertheless, these limitations or difﬁculties in deploying WURL in a\nlarger range of applications may imply future research directions.\n6.2\nConclusion\nWe build a framework of Wasserstein unsupervised reinforcement learning (WURL) of training a\nset of diverse policies. In contrast to conventional methods of unsupervised skill discovery, WURL\nemploys Wasserstein distance based intrinsic reward to enhance the distance between different\npolicies, which has theoretical advantages to mutual information based methods (Arjovsky et al.\n[2017]). We overcome the difﬁculties of extending the WURL framework for multiple policy\nlearning. In addition, we devise a novel algorithm combining Wasserstein distance estimation and\nreinforcement learning, addressing reward crediting issue. Our experiments demonstrate WURL\ngenerates more diverse policies than mutual information based methods such as DIAYN and DADS,\non the metric of discriminability (MI-based metric) and Wasserstein distance. Furthermore, WURL\nexcites autonomous agents to form a set of policies to cover the state space spontaneously and\nprovides a good sub-policy set for sequential navigation tasks.\nReferences\nMohammed Amin Abdullah, Aldo Pacchiano, and Moez Draief. Reinforcement learning with\nwasserstein distance regularisation, with applications to multipolicy learning. arXiv preprint\narXiv:1802.03976, 2018.\nJoshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery\nalgorithms. arXiv preprint arXiv:1807.10299, 2018.\nLuca Ambrogioni, Umut Güçlü, Ya˘gmur Güçlütürk, Max Hinne, Marcel A. J. van Gerven, and Eric\nMaris. Wasserstein variational inference. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/\n2018/file/2c89109d42178de8a367c0228f169bf8-Paper.pdf.\n10\nMarcin Andrychowicz,\nFilip Wolski,\nAlex Ray,\nJonas Schneider,\nRachel Fong,\nPeter\nWelinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba.\nHindsight experience replay.\nIn Advances in Neural Information Processing Sys-\ntems, volume 30, 2017.\nURL https://proceedings.neurips.cc/paper/2017/file/\n453fadbd8a1a3af50a9df4df899537b5-Paper.pdf.\nMartin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In\nProceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings\nof Machine Learning Research, pages 214–223. PMLR, 06–11 Aug 2017.\nKate Baumli, David Warde-Farley, Steven Hansen, and Volodymyr Mnih. Relative variational\nintrinsic control. arXiv preprint arXiv:2012.07827, 2020.\nVíctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giro-i Nieto, and Jordi Tor-\nres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In International\nConference on Machine Learning, pages 1317–1327. PMLR, 2020.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In Advances in Neural Information Processing\nSystems, volume 30, 2017. URL https://proceedings.neurips.cc/paper/2017/file/\nd5e2c0adad503c91f91df240d0cd4e49-Paper.pdf.\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural\ninformation processing systems, 26:2292–2300, 2013.\nRobert Dadashi, Leonard Hussenot, Matthieu Geist, and Olivier Pietquin.\nPrimal wasserstein\nimitation learning. In International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=TtYSU29zgR.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.\nDiversity is all you\nneed: Learning skills without a reward function.\nIn International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.\nAude Genevay, Marco Cuturi, Gabriel Peyré, and Francis Bach.\nStochastic optimization\nfor large-scale optimal transport.\nIn Advances in Neural Information Processing Sys-\ntems, volume 29, 2016.\nURL https://proceedings.neurips.cc/paper/2016/file/\n2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf.\nKarol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv\npreprint arXiv:1611.07507, 2016.\nAbhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.\nMeta-\nreinforcement learning of structured exploration strategies. arXiv preprint arXiv:1802.07245,\n2018.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International Conference\non Machine Learning, pages 1861–1870. PMLR, 2018.\nKarol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller.\nLearning an embedding space for transferable robot skills.\nIn International Conference on\nLearning Representations, 2018.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:\nVariational information maximizing exploration. In Advances in Neural Information Processing\nSystems, pages 1109–1117, 2016.\nSoheil Kolouri, Gustavo K Rohde, and Heiko Hoffmann. Sliced wasserstein distance for learning\ngaussian mixture models. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3427–3436, 2018.\nTaehwan Kwon. Variational intrinsic control revisited. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.net/forum?id=P0p33rgyoE.\n11\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. nature, 518(7540):529–533, 2015.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-\nplers using variational divergence minimization. In Advances in Neural Information Processing\nSystems, volume 29, 2016. URL https://proceedings.neurips.cc/paper/2016/file/\ncedebb6e872f539bef8c3f919874e9d7-Paper.pdf.\nSherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre Sermanet.\nWasserstein dependency measure for representation learning. arXiv preprint arXiv:1903.11780,\n2019.\nAldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choromanska,\nand Michael Jordan. Learning to score behaviors for guided policy optimization. In International\nConference on Machine Learning, pages 7445–7454. PMLR, 2020.\nGiorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max\nWelling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertainty in Artiﬁcial\nIntelligence, pages 733–743. PMLR, 2020.\nVitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-ﬁt:\nState-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019.\nMark Rowland, Jiri Hron, Yunhao Tang, Krzysztof Choromanski, Tamas Sarlos, and Adrian Weller.\nOrthogonal estimation of wasserstein distances. In The 22nd International Conference on Artiﬁcial\nIntelligence and Statistics, pages 186–195. PMLR, 2019.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International conference on machine learning, pages 1889–1897. PMLR,\n2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware\nunsupervised discovery of skills. In International Conference on Learning Representations (ICLR),\n2020.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\nIlya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.\nIn International Conference on Learning Representations, 2018. URL https://openreview.\nnet/forum?id=HkL7n1-0b.\nCédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\nJiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and Luc Van\nGool. Sliced wasserstein generative models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 3713–3722, 2019.\nRuiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as Wasserstein\ngradient ﬂows.\nIn Proceedings of the 35th International Conference on Machine Learning,\nvolume 80 of Proceedings of Machine Learning Research, pages 5737–5746. PMLR, 10–15 Jul\n2018. URL http://proceedings.mlr.press/v80/zhang18a.html.\n12\nA\nAppendix\nA.1\nEvaluation of different estimation methods of Wasserstein distance\nIn this section we will compare different estimation methods of Wasserstein distance, including\ntwo dual form methods TF1, TF2 and two primal form methods SWD, PWD. As we mentioned in\nmain text, TF1 maximizes dual objective with one test function f subject to Lipschitz constraint. In\npractice, the Lipschitz constraint is implemented by clamping weights in f parameterized by a neural\nnetwork. We set this constant to 0.01. TF1 cannot give the exact number Wasserstein distance but\nthe distance multiplied by some underlying constant. Therefore, TF1 is still capable of measuring\nhow “far” two distributions are. TF2 maximizes dual objective plus a regularizer term with two test\nfunctions µ, ν subject to no constraint. TF2 can provide the exact number of Wasserstein distance.\nFig. 5 shows the typical training curves, convergence speed versus data dimension, and the linearity\nas the distribution distance grows. All experiments are evaluated on two set of samples sampled from\ntwo different Gaussian distributions.\nFigure 5: Comparisons of TF1 and TF2, on the aspects of iterations to convergence, typical training\ncurve and linearity.\nSince SWD projects samples on a real line and the distances are also projected, SWD tends to\nunderestimate the true Wasserstein distance. Table 3 characterizes the estimated numbers of each\nmethod. As we can see, TF2 and PWD give exact number of Wasserstein distance, and PWD consume\nless time of execution than TF2. Considering time consumption and accuracy, PWD is preferred in\nour algorithm.\nA.2\nMatching matrix computation\nAssume two 1-D (one dimensional) sample sets X = {xi}N\ni=1 and Y = {yj}N\nj=1. Further we assume\nthe cost function is C(x, y) = |x −y|. First we sort the two sets to ordered sets {xσx(i)}N\ni=1 and\n{yσy(j)}N\nj=1. The analytical solution of optimal matching in Wasserstein distance estimation is given\nby (xσx(i), yσy(i)). Exact Wasserstein distance between the two empirical distributions induced by\nsample sets is\nW(Px, Py) = 1\nN\nN\nX\ni=1\n|xσx(i) −yσy(i)| = 1\nN\nN\nX\ni=1\n|xi −yσ−1\ny\nσx(i)|.\n(8)\nThe matching matrix is instantly acquired from permutations calculated above.\nWhen two samples sets have different cardinalities, e.g., X = {xi}N\ni=1 and Y = {yj}M\nj=1. We\nduplicate the elements in X by M times and duplicate the elements in Y by N times. Then the two\n13\nDistance estimated\nExecution time (s)\nGround truth\n2.0\nN/A\nTF1\n0.00341±0.00045\n4.92±0.04\nTF2\n2.92±0.05\n1.43±0.05\nSWD\n0.791±0.026\n0.00409±0.00003\nPWD\n3.21±0.002\n0.00643±0.00003\nGround truth\n16.0\nN/A\nTF1\n0.0135±0.0022\n4.98±0.05\nTF2\n16.1±0.0\n1.39±0.02\nSWD\n6.31±0.22\n0.00409±0.00002\nPWD\n16.3±0.0\n0.00647±0.00004\nGround truth\n64.0\nN/A\nTF1\n0.0349±0.0053\n4.90±0.04\nTF2\n64.1±0.0\n1.40±0.03\nSWD\n25.9±0.75\n0.00410±0.00007\nPWD\n64.2±0.0\n0.00656±0.00015\nTable 3: Comparisons of distance estimation and execution time\nsets have the same cardinality N × M, and the aforementioned algorithm can be used. Nevertheless,\nfor computational convenience, we compute matching matrix in the following algorithm\nAlgorithm 4 Matching matrix computation\nSort two sets to {xσx(i)}N\ni=1 and {yσy(j)}M\nj=1\nInitialize matching matrix PN×M with zeros\nInitialize list A = [(1/N, σx(i))]N\ni=1 and list B = [(1/M, σy(j))]M\nj=1\nSet u = 0, v = 0\nwhile A ̸= ∅and B ̸= ∅do\nu, k = A.pop() if u = 0, v, l = B.pop() if v = 0\nw = min(u, v)\nPkl+ = w\nu−= w, v−= w\nend while\nA.3\nExperiment settings\nHardware:\n• 1x GeForce RTX 2080,\n• 1x Intel Core i7-7700 CPU @ 3.60GHz.\nSoftware:\n• Ubuntu 18.04.3 LTS,\n• torch 1.7.1+cu102,\n• python 3.7.5,\n• MuJoCo 150,\n• Gym 0.18.0.\nNetwork and optimizer:\n• Network type: MLP,\n• hidden layer size: 64 for environments in mujoco-maze, 256 for HalfCheetah, Ant and 1024\nfor Humanoid,\n• activation layer: ReLU,\n14\n• optimizer: Adam.\nPlease refer to source code for other details. Part of environments are customized, i.e., FreeRun,\nTreeMaze. Part of environments are modiﬁed from mujoco-maze 0.1.1, i.e., PointPush, PointBilliard.\nThe modiﬁed environments are presented in source code.\nA.4\nAdditional experiments\nPolicies trained in MuJoCo locomotion environments\nWe also investigate our proposed method in MuJoCo locomotion tasks, e.g., Walker2d, Swimmer,\nHalfCheetah. As Fig. 6 shows, under different policies, the agent acts with different gestures, or\nmoves to different directions. These trained models are provided in codes.\nFigure 6: Policies learned in Walker2d, Swimmer, HalfCheetah.\nMinimum distance as reward vs. mean distance as reward\nIn Section 3.3, we mentioned we use the minimum distance as reward minN\nj=1,j̸=i W(pi, pj) in\npractical use, not the mean distance\n1\nN−1\nPN\nj=1,j̸=i W(pi, pj). We compare this two methods and\nvisualize the learned trajectories in Fig. 7. Maximizing the minimum distance can lead the current\npolicy away from any other policies however maximizing the mean distance results in similar policies\nas demonstrated in the right column of Fig. 7.\nTrajectories of models from DIAYN and APWD\nWe visualize the policies learned by DIAYN and APWD. As we can see in Fig. 8, maximizing\nWasserstein distance keeps the policies far from each other. Although policies learned by DIAYN\ncan be distinguished by a discriminator, they would not keep far and explore the outer state space\nspontaneously.\nResults on customized MuJoCo environments\nAs mentioned in main text Section 4.1, we applied WURL on two customized MuJoCo environments,\nPointPush and PointBilliard. Fig. 9 shows selected policies learned by APWD algorithm. Not only\n15\nFigure 7: Left column: minimum distance as reward; right column: mean distance as reward. 24\npolicies learned in FreeRun and TreeMaze environments.\nFigure 8: Left column: DIYAN; right column: WURL with amortized PWD. 10 policies learned in\nFreeRun and TreeMaze environments.\nthe agent moves to different corner on the map, but the agent learns to interact with the movable\nobject as well, since this behaviour will enhance the diversity.\nTraining details of hierarchical reinforcement learning experiments\nWe compare the performance of our method APWD with DIAYN-I and DADS-I in downstream tasks\nas mentioned in main text Section 4.3. The 10 pre-trained policies from each method serve as the\nsub-policies in the hierarchical framework and we adopt PPO as the meta-controller trainer. Fig. 10\n16\nFigure 9: 5 Policies learned in customized MuJoCo environments, PointBilliard (upper) and PointPush\n(bottom). We visualize the position trajectories of the centroid of movable objects as well which\ndemonstrate the differences of the learned policies.\nis the training curve of the navigation task based on MuJoCo Ant environment and shows that the\npre-trained policies from APWD outperform policies from the other two methods in this hierarchical\nframework.\nFigure 10: The training curve of the navigation task based on MuJoCo Ant environment. The solid\nline is the average return across 5 random runs and the shadowed area denotes the standard deviation.\n17\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-10-15",
  "updated": "2021-10-15"
}