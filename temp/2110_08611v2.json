{
  "id": "http://arxiv.org/abs/2110.08611v2",
  "title": "Deep Active Learning by Leveraging Training Dynamics",
  "authors": [
    "Haonan Wang",
    "Wei Huang",
    "Ziwei Wu",
    "Andrew Margenot",
    "Hanghang Tong",
    "Jingrui He"
  ],
  "abstract": "Active learning theories and methods have been extensively studied in\nclassical statistical learning settings. However, deep active learning, i.e.,\nactive learning with deep learning models, is usually based on empirical\ncriteria without solid theoretical justification, thus suffering from heavy\ndoubts when some of those fail to provide benefits in real applications. In\nthis paper, by exploring the connection between the generalization performance\nand the training dynamics, we propose a theory-driven deep active learning\nmethod (dynamicAL) which selects samples to maximize training dynamics. In\nparticular, we prove that the convergence speed of training and the\ngeneralization performance are positively correlated under the ultra-wide\ncondition and show that maximizing the training dynamics leads to better\ngeneralization performance. Furthermore, to scale up to large deep neural\nnetworks and data sets, we introduce two relaxations for the subset selection\nproblem and reduce the time complexity from polynomial to constant. Empirical\nresults show that dynamicAL not only outperforms the other baselines\nconsistently but also scales well on large deep learning models. We hope our\nwork would inspire more attempts on bridging the theoretical findings of deep\nnetworks and practical impacts of deep active learning in real applications.",
  "text": "Deep Active Learning by Leveraging Training\nDynamics\nHaonan Wang1, Wei Huang2, Ziwei Wu1, Andrew Margenot1,\nHanghang Tong1, Jingrui He1\n1University of Illinois Urbana-Champaign\n2University of New South Wales\n1{haonan3,ziweiwu2,margenot,htong,jingrui}@illinois.edu\n2{weihuang.uts}@gmail.com\nAbstract\nActive learning theories and methods have been extensively studied in classical\nstatistical learning settings. However, deep active learning, i.e., active learning\nwith deep learning models, is usually based on empirical criteria without solid\ntheoretical justiﬁcation, thus suffering from heavy doubts when some of those fail\nto provide beneﬁts in real applications. In this paper, by exploring the connection\nbetween the generalization performance and the training dynamics, we propose a\ntheory-driven deep active learning method (dynamicAL) which selects samples to\nmaximize training dynamics. In particular, we prove that the convergence speed\nof training and the generalization performance are positively correlated under the\nultra-wide condition and show that maximizing the training dynamics leads to better\ngeneralization performance. Furthermore, to scale up to large deep neural networks\nand data sets, we introduce two relaxations for the subset selection problem and\nreduce the time complexity from polynomial to constant. Empirical results show\nthat dynamicAL not only outperforms the other baselines consistently but also\nscales well on large deep learning models. We hope our work would inspire more\nattempts on bridging the theoretical ﬁndings of deep networks and practical impacts\nof deep active learning in real applications.\n1\nIntroduction\nTraining deep learning (DL) models usually requires large amount of high-quality labeled data [1] to\noptimize a model with a massive number of parameters. The acquisition of such annotated data is\nusually time-consuming and expensive, making it unaffordable in the ﬁelds that require high domain\nexpertise. A promising approach for minimizing the labeling effort is active learning (AL), which\naims to identify and label the maximally informative samples, so that a high-performing classiﬁer can\nbe trained with minimal labeling effort [2]. Under classical statistical learning settings, theories of\nactive learning have been extensively studied from the perspective of VC dimension [3]. As a result,\na variety of methods have been proposed, such as (i) the version-space-based approaches, which\nrequire maintaining a set of models [4, 5], and (ii) the clustering-based approaches, which assume\nthat the data within the same cluster have pure labels [6].\nHowever, the theoretical analyses for these classical settings may not hold for over-parameterized\ndeep neural networks where the traditional wisdom is ineffective [1]. For example, margin-based\nmethods select the labeling examples in the vicinity of the learned decision boundary [7, 8]. However,\nin the over-parameterized regime, every labeled example could potentially be near the learned decision\nboundary [9]. As a result, theoretically, such analysis can hardly guide us to design practical active\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2110.08611v2  [cs.LG]  20 Nov 2022\nlearning methods. Besides, empirically, multiple deep active learning works, borrowing observations\nand insights from the classical theories and methods, have been observed unable to outperform their\npassive learning counterparts in a few application scenarios [10, 11].\nOn the other hand, the analysis of neural network’s optimization and generalization performance has\nwitnessed several exciting developments in recent years in terms of the deep learning theory [12–\n14]. It is shown that the training dynamics of deep neural networks using gradient descent can be\ncharacterized by the Neural Tangent Kernel (NTK) of inﬁnite [12] or ﬁnite [15] width networks.\nThis is further leveraged to characterize the generalization of over-parameterized networks through\nRademacher complexity analysis [13, 16]. We are therefore inspired to ask: How can we design a\npractical and generic active learning method for deep neural networks with theoretical justiﬁcations?\nTo answer this question, we ﬁrstly explore the connection between the model performance on testing\ndata and the convergence speed on training data for the over-parameterized deep neural networks.\nBased on the NTK framework [12, 13], we theoretically show that if a deep neural network converges\nfaster (“Train Faster”), then it tends to have better generalization performance (“Generalize Better”),\nwhich matches the existing observations [17–21]. Motivated by the aforementioned connection, we\nﬁrst introduce Training Dynamics, the derivative of training loss with respect to iteration, as a proxy\nto quantitatively describe the training process. On top of it, we formally propose our generic and\ntheoretically-motivated deep active learning method, dynamicAL, which will query labels for a subset\nof unlabeled samples that maximally increase the training dynamics. In order to compute the training\ndynamics by merely using the unlabeled samples, we leverage two relaxations Pseudo-labeling and\nSubset Approximation to solve this non-trivial subset selection problem. Our relaxed approaches\nare capable of effectively estimating the training dynamics as well as efﬁciently solving the subset\nselection problem by reducing the complexity from O(N b) to O(b).\nIn theory, we coin a new term Alignment to measure the length of the label vector’s projection on the\nneural tangent kernel space. Then, we demonstrate that higher alignment usually comes with a faster\nconvergence speed and a lower generalization bound. Furthermore, with the help of the maximum\nmean discrepancy [22], we extend the previous analysis to an active learning setting where the i.i.d.\nassumption may not hold. Finally, we show that alignment is positively correlated with our active\nlearning goal, training dynamics, which implies that maximizing training dynamics will lead to better\ngeneralization performance.\nRegarding experiments, we have empirically veriﬁed our theory by conducting extensive experiments\non three datasets, CIFAR10 [23], SVHN [24], and Caltech101 [25] using three types of network\nstructures: vanilla CNN, ResNet [26], and VGG [27]. We ﬁrst show that the result of the subset\nselection problem delivered by the subset approximation is close to the global optimal solution.\nFurthermore, under the active learning setting, our method not only outperforms other baselines but\nalso scales well on large deep learning models.\nThe main contributions of our paper can be summarized as follows:\n• We propose a theory-driven deep active learning method, dynamicAL, inspired by the observation\nof “train faster, generalize better”. To this end, we introduce the Training Dynamics, as a proxy to\ndescribe the training process.\n• We demonstrate that the convergence speed of training and the generalization performance is\nstrongly (positively) correlated under the ultra-wide condition; we also show that maximizing the\ntraining dynamics will lead to a lower generalization error in the scenario of active learning.\n• Our method is easy to implement. We conduct extensive experiments to evaluate the effectiveness\nof dynamicAL and empirically show that our method consistently outperforms other methods in a\nwide range of active learning settings.\n2\nBackground\nNotation. We use the random variable x ∈X to represent the input data feature and y ∈Y as the\nlabel where K is the number of classes and [K] := {1, 2, ..., K}. We are given non-degenerated\na data source D with unknown distribution p(x, y). We further denote the concatenation of x as\nX = [x1, x2, ..., xM]⊤and that of y as Y = [y1, y2, ..., yM]⊤. We consider a deep learning classiﬁer\nhθ(x) = argmax σ(f(x; θ)) : x →y parameterized by θ ∈Rp, where σ(·) is the softmax function\nand f is a neural network. Let ⊗be the Kronecker Product and IK ∈RK×K be an identity matrix.\n2\nActive learning. The goal of active learning is to improve the learning efﬁciency of a model with a\nlimited labeling budget. In this work, we consider the pool-based AL setup, where a ﬁnite data set\nS = {(xl, yl)}M\nl=1 with M points are i.i.d. sampled from p(x, y) as the (initial) labeled set. The AL\nmodel receives an unlabeled data set U sampled from p(x) and request labels according to p(y|x)\nfor any x ∈U in each query round. There are R rounds in total, and for each round, a query set Q\nconsisting of b unlabeled samples can be queried. The total budget size B = b × R.\nNeural Tangent Kernel. The Neural Tangent Kernel [12] has been widely applied to analyze the\ndynamics of neural networks. If a neural network is sufﬁciently wide, properly initialized, and\ntrained by gradient descent with inﬁnitesimal step size (i.e., gradient ﬂow), then the neural network is\nequivalent to kernel regression predictor with a deterministic kernel Θ(·, ·), called Neural Tangent\nKernel (NTK). When minimizing the mean squared error loss, at the iteration t, the dynamics of the\nneural network f has a closed-form expression:\ndf(X; θ(t))\ndt\n= −Kt(X, X) (f(X; θ(t)) −Y) ,\n(1)\nwhere θ(t) denotes the parameter of the neural network at iteration t, Kt(X, X) ∈R|X|×K×|X|×K\nis called the empirical NTK and Ki,j\nt (x, x′) = ∇θf i(x; θ(t))⊤∇θf j(x′; θ(t)) is the inner product of\nthe gradient of the i-th class probability and the gradient of the j-th class probability for two samples\nx, x′ ∈X and i, j ∈[K]. The time-variant kernel Kt(·, ·) is equivalent to the (time-invariant) NTK\nwith a high probability, i.e., if the neural network is sufﬁciently wide and properly initialized, then:\nKt(X, X) = Θ(X, X) ⊗IK.\n(2)\nThe ﬁnal learned neural network at iteration t, is equivalent to the kernel regression solution with\nrespect to the NTK [14]. For any input x and training data {X, Y } we have,\nf(x; θ(t)) ≈Θ(x, X)⊤Θ(X, X)−1(I −e−ηΘ(X,X)t)Y,\n(3)\nwhere η is the learning rate, Θ(x, X) is the NTK matrix between input x and all samples in training\ndata X.\n3\nMethod\nIn section 3.1, we introduce the notion of training dynamics which can be used to describe the training\nprocess. Then, in section 3.2, based on the training dynamics, we propose dynamicAL. In section 3.3,\nwe discuss the connection between dynamicAL and existing deep active learning methods.\n3.1\nTraining dynamics\nIn this section, we introduce the notion of training dynamics. The cross-entropy loss over the labeled\nset S is deﬁned as:\nL(S) =\nX\n(xl,yl)∈S\nℓ(f(xl; θ), yl)\n= −\nX\n(xl,yl)∈S\nX\ni∈[K]\nyi\nl log σi(f(xl; θ)),\n(4)\nwhere σi(f(x; θ)) =\nexp(f i(x;θ))\nP\nj exp(f j(x;θ)). We ﬁrst analyze the dynamics of the training loss, with respect\nto iteration t, on one labeled sample (derivation is in Appendix A.1):\n∂ℓ(f(x; θ), y)\n∂t\n= −\nX\ni\n\u0000yi −σi(f(x; θ))\n\u0001\n∇θf i(x; θ)∇⊤\nt θ.\n(5)\nFor neural networks trained by gradient descent, if the learning rate η is small, then ∇tθ = θt+1−θt =\n−η\n∂P\n(xl,yl)∈S ℓ(f(xl;θ),yl)\n∂θ\n. Taking the partial derivative of the training loss with respect to the\nparameters, we have (the derivation of the following equation can be found in Appendix A.2):\n∂ℓ(f(x; θ), y)\n∂θ\n=\nX\nj∈[K]\n\u0000σj(f(x; θ)) −yj\u0001∂f j(x; θ)\n∂θ\n.\n(6)\n3\nTherefore, we can further get the following result for the dynamics of training loss:\n∂ℓ(f(x; θ), y)\n∂t\n= −η\nX\ni\n\u0000σi(f(x; θ)) −yi\u0001 X\nj\nX\n(xl′ ,yl′ )∈S\n∇θf i(x; θ)⊤∇θf j(xl′ ; θ)\n\u0000σj(f(xl′ ; θ)) −yj\nl′\n\u0001\n.\n(7)\nFurthermore, we deﬁne di(X, Y ) = σi(f(X; θ)) −Y i and Y i is the label vector of all samples for\ni-th class. Then, the training dynamics (dynamics of training loss) over training set S, computed with\nthe empirical NTK Kij(X, X), is denoted by G(S) ∈R:\nG(S) = −1\nη\nX\n(xl,yl)∈S\n∂ℓ(f(xl; θ), yl)\n∂t\n=\nX\ni\nX\nj\ndi(X, Y )⊤Kij(X, X)dj(X, Y ).\n(8)\n3.2\nActive learning by activating training dynamics\nBefore we present dynamicAL, we state Proposition 1, which serves as the theoretical guidance for\ndynamicAL and will be proved in Section 4.\nProposition 1. For deep neural networks, converging faster leads to a lower worst-case generaliza-\ntion error.\nMotivated by the connection between convergence speed and generalization performance, we propose\nthe general-purpose active learning method, dynamicAL, which aims to accelerate the convergence by\nquerying labels for unlabeled samples. As we described in the previous section, the training dynamics\ncan be used to describe the training process. Therefore, we employ the training dynamics as a proxy\nto design an active learning method. Speciﬁcally, at each query round, dynamicAL will query labels\nfor samples which maximize the training dynamics G(S), i.e.,\nQ = argmaxQ⊆UG(S ∪Q), s.t. |Q| = b,\n(9)\nwhere Q is the corresponding data set for Q with ground-truth labels. Notice that when applying the\nabove objective in practice, we are facing two major challenges. First, G(S ∪Q) cannot be directly\ncomputed, because the label information of unlabeled examples is not available before the query.\nSecond, the subset selection problem can be computationally prohibitive if enumerating all possible\nsets with size b. Therefore, we employ the following two relaxations to make this maximization\nproblem to be solved with constant time complexity.\nPseudo labeling. To estimate the training dynamics, we use the predicted label ˆyu for sample\nxu in the unlabeled data set U to compute G. Note, the effectiveness of this adaptation has been\ndemonstrated in the recent gradient-based methods [11, 28], which compute the gradient as if the\nmodel’s current prediction on the example is the true label. Therefore, the maximization problem in\nEquation (9) is changed to,\nQ = argmaxQ⊆UG(S ∪bQ).\n(10)\nwhere bQ is the corresponding data set for Q with pseudo labels bYQ.\nSubset approximation. The subset selection problem of Equation (10) still requires enumerating all\npossible subsets of U with size b, which is O(nb). We simplify the selection problem to the following\nproblem without causing any change on the result,\nargmaxQ⊆UG(S ∪bQ) = argmaxQ⊆U∆( bQ|S),\n(11)\nwhere ∆( bQ|S) = G(S ∪bQ) −G(S) is deﬁned as the change of training dynamics. We approximate\nthe change of training dynamics caused by query set Q using the summation of the change of training\ndynamics caused by each sample in the query set. Then the maximization problem can be converted\nto Equation (12) which can be solved by a greedy algorithm with O(b).\nQ = argmaxQ⊆U\nX\n(x,by)∈b\nQ\n∆({(x, by)}|S), s.t. |Q| = b.\n(12)\nTo further show the approximated result is reasonably good, we decompose the change of training\ndynamics as (derivation in Appendix A.4):\n∆( bQ|S) =\nX\n(x,by)∈b\nQ\n∆({(x, by)}|S) +\nX\n(x,ˆy),(x′,ˆy′)∈b\nQ\ndi(x, ˆy)⊤Kij(x, x′)dj(x′, ˆy′),\n(13)\n4\nwhere Kij(x, x′) is the empirical NTK. The ﬁrst term in the right hand side is the approximated\nchange of training dynamics. Then, we further deﬁne the Approximation Ratio (14) which measures\nthe approximation quality,\nR( bQ|S) =\nP\n(x,by)∈b\nQ ∆({(x, by)}|S)\n∆( bQ|S)\n.\n(14)\nWe empirically measure the expectation of the Approximation Ratio on two data sets with two\ndifferent neural networks under three different batch sizes. As shown in Figure 4, the expectation\nEQ∼UR( bQ|S) ≈1 when the model is converged. Therefore, the approximated result delivered by\nthe greedy algorithm is close to the global optimal solution of the original maximization problem,\nEquation (10), especially when the model is converged.\nBased on the above two approximations, we present the proposed method dynamicAL in Algorithm 1.\nAs described below, the algorithm starts by training a neural network f(·; θ) on the initial labeled\nset S until convergence. Then, for every unlabeled sample xu, we compute pseudo label ˆyu and the\nchange of training dynamics ∆({(xu, byu)}|S). After that, dynamicAL will query labels for top-b\nsamples causing the maximal change on training dynamics, train the neural network on the extended\nlabeled set, and repeat the process. Note, to keep close to the theoretical analysis, re-initialization is\nnot used after each query, which also enables dynamicAL to get rid of the computational overhead of\nretraining the deep neural networks every time.\nAlgorithm 1 Deep Active Learning by Leveraging Training Dynamics\nInput: Neural network f(·; θ), unlabeled sample set U, initial labeled set S, number of query\nround R, query batch size b.\nfor r = 1 to R do\nTrain f(·; θ) on S with cross-entropy loss until convergence.\nfor xu ∈U do\nCompute its pseudo label ˆyu = argmaxf(xu; θ).\nCompute ∆({(xu, byu)}|S).\nend for\nSelect b query samples Q with the highest ∆values, and request their labels from the oracle.\nUpdate the labeled data set S = S ∪Q .\nend for\nreturn Final model f(·; θ).\n3.3\nRelation to existing methods\nAlthough existing deep active learning methods are usually designed based on heuristic criteria, some\nof them have empirically shown their effectiveness [11, 29, 30]. We surprisingly found that our\ntheoretically-motivated method dynamicAL has some connections with those existing methods from\nthe perspective of active learning criterion. The proposed active learning criterion in Equation (12)\ncan be explicitly written as (derivation in Appendix A.5):\n∆({(xu,byu)}|S) = ∥∇θℓ(f(xu; θ), ˆyu)∥2 + 2\nX\n(x,y)∈S\n∇θℓ(f(xu; θ), ˆyu)⊤∇θℓ(f(x; θ), y).\n(15)\nNote. The ﬁrst term of the right-hand side can be interpreted as the square of gradient length (2-\nnorm) which reﬂects the uncertainty of the model on the example and has been wildly used as an\nactive learning criterion in some existing works [30, 11, 31]. The second term can be viewed as\nthe inﬂuence function [32] with identity hessian matrix. And recently, [29] has empirically shown\nthat the effectiveness of using the inﬂuence function with identity hessian matrix as active learning\ncriterion. We hope our theoretical analysis can also shed some light on the interpretation of previous\nmethods.\n4\nTheoretical analysis\nIn this section, we study the correlation between the convergence rate of the training loss and the\ngeneralization error under the ultra-wide condition [12, 13]. We deﬁne a measure named alignment\n5\nto quantify the convergence rate and further show its connection with generalization bound. The\nanalysis provides a theoretical guarantee for the phenomenon of “Train Faster, Generalize Better” as\nwell as our active learning method dynamicAL with a rigorous treatment. Finally, we show that the\nactive learning proxy, training dynamics, is correlated with alignment, which indicates that increasing\nthe training dynamics leads to larger convergence rate and better generalization performance. We\nleave all proofs of theorems and details of veriﬁcation experiments in Appendix B and D respectively.\n4.1\nTrain faster provably generalize better\nGiven an ultra-wide neural network, the gradient descent can achieve a near-zero training error\n[12, 33] and its generalization ability in unseen data can be bounded [13]. It is shown that both the\nconvergence and generalization of a neural network can be analyzed using the NTK [13]. However,\nthe question what is the relation between the convergence rate and the generalization bound has\nnot been answered. We formally give a solution by introducing the concept of alignment, which is\ndeﬁned as follows:\nDeﬁnition 1 (Alignment). Given a data set S = {X, Y }, the alignment is a measure of correlation\nbetween X and Y projected in the NTK space. In particular, the alignment can be computed by\nA(X, Y ) = Tr[Y ⊤Θ(X, X)Y ] = PK\nk=1\nPn\ni=1 λi(⃗v⊤\ni Y k)2.\nIn the following, we will demonstrate why “Train Faster” leads to “Generalize Better” through\nalignment. In particular, the relation of the convergence rate and the generalization bound with\nalignment is analyzed. The convergence rate of gradient descent for ultra-wide networks is presented\nin following lemma:\nLemma 1 (Convergence Analysis with NTK, Theorem 4.1 of [13]). Suppose λ0 = λmin(Θ) > 0\nfor all subsets of data samples. For δ ∈(0, 1), if m = Ω(\nn7\nλ4\n0δ4ϵ2 ) and η = O( λ0\nn2 ), with probability\nat least 1 −δ, the network can achieve near-zero training error,\n∥Y −f(X; θ(t))∥2 =\nv\nu\nu\nt\nK\nX\nk=1\nn\nX\ni=1\n(1 −ηλi)2t(⃗v⊤\ni Y k)2 ± ϵ,\n(16)\nwhere n denotes the number of training samples and m denotes the width of hidden layers. The NTK\nΘ = V ⊤ΛV with Λ = {λi}n\ni=1 is a diagonal matrix of eigenvalues and V = {⃗vi}n\ni=1 is a unitary\nmatrix.\nIn this lemma, we take mean square error (MSE) loss as an example for the convenience of illustration.\nThe conclusion can be extended to other loss functions such as cross-entropy loss (see Appendix B.2\nin [14]). From the lemma, we ﬁnd the convergence rate is governed by the dominant term (16) as\nEt(X, Y ) =\nqPK\nk=1\nPn\ni=1(1 −ηλi)2t(⃗v⊤\ni Y k)2, which is correlated with the alignment:\nTheorem 1 (Relationship between the convergence rate and alignment). Under the same assumptions\nas in Lemma 1, the convergence rate described by Et satisﬁes,\nTr[Y ⊤Y ] −2tηA(X, Y ) ≤E2\nt (X, Y ) ≤Tr[Y ⊤Y ] −ηA(X, Y ).\n(17)\nRemark 1. In the above theorem, we demonstrate that the alignment can measure the convergence\nrate. Especially, we ﬁnd that both the upper bound and the lower bound of error Et(X, Y ) are\ninversely proportional to the alignment, which implies that higher alignment will lead to achieving\nfaster convergence.\nNow we analyze the generalization performance of the proposed method through complexity analysis.\nWe demonstrate that the ultra-wide networks can achieve a reasonable generalization bound.\nLemma 2 (Generalization bound with NTK, Theorem 5.1 of [13]). Suppose data S = {(xi, yi)}n\ni=1\nare i.i.d. samples from a non-degenerate distribution p(x, y), and m ≥poly(n, λ−1\n0 , δ−1). Consider\nany loss function ℓ: R × R →[0, 1] that is 1-Lipschitz, then with probability at least 1 −δ over the\nrandom initialization, the network trained by gradient descent for T ≥Ω(\n1\nηλ0 log n\nδ ) iterations has\npopulation risk Lp = E(x,y)∼p(x,y)[ℓ(fT (x; θ), y)] that is bounded as follows:\nLp ≤\nr\n2 Tr[Y ⊤Θ−1(X, X)Y ]\nn\n+ O\n\u0012s\nlog\nn\nλ0δ\nn\n\u0013\n.\n(18)\n6\nIn this lemma, we show that the dominant term in the generalization upper bound is B(X, Y ) =\nq\n2 Tr[Y ⊤Θ−1Y ]\nn\n. In the following theorem, we further prove that this bound is inversely proportional\nto the alignment A(X, Y ).\nTheorem 2 (Relationship between the generalization bound and alignment). Under the same assump-\ntions as in Lemma 2, if we deﬁne the generalization upper bound as B(X, Y ) =\nq\n2 Tr[Y ⊤Θ−1Y ]\nn\n,\nthen it can be bounded with the alignment as follows:\nTr2[Y ⊤Y ]\nA(X, Y ) ≤n\n2 B2(X, Y ) ≤λmax\nλmin\nTr2[Y ⊤Y ]\nA(X, Y ) .\n(19)\nRemark 2. Theorems 1 and 2 reveal that the cause for the correlated phenomenons “Train Faster”\nand “Generalize Better” is the projection of label vector on the NTK space (alignment).\n4.2\n“ Train Faster, Generalize Better ” for active learning\nFigure 1: Comparison between Em-\npirical Generalization Bound and\nMMD.\nIn the NTK framework [13], the empirical average requires\ndata in S is i.i.d. samples (Lemma 2). However, this assump-\ntion may not hold in the active learning setting with multiple\nquery rounds, because the training data is composed by i.i.d.\nsampled initial label set and samples queried by active learn-\ning policy. To extend the previous analysis principle to active\nlearning, we follow [34] to reformulate the Lemma 2 as:\nLp ≤(Lp −Lq) +\nr\n2 Tr[Y ⊤Θ−1(X, X)Y ]\nn\n+ O\n\u0012s\nlog\nn\nλ0δ\nn\n\u0013\n,\n(20)\nwhere Lq = E(x,y)∼q(x,y)[ℓ(f(x; θ), y)], q(x, y) denotes the\ndata distribution after query, and X, Y includes initial training\nsamples and samples after query. There is a new term in the upper bound, which is the difference\nbetween the true risk under different data distributions.\nLp −Lq =E(x,y)∼p(x,y)[ℓ(f(x; θ), y)] −E(x,y)∼q(x,y)[ℓ(f(x; θ), y)]\n(21)\nThough in active learning the data distribution for the labeled samples may be different from\nthe original distribution, they share the same conditional probability p(y|x). We deﬁne g(x) =\nR\ny ℓ(f(x; θ), y)p(y|x)dy, and then we have:\nLp −Lq =\nZ\nx\ng(x)p(x)dx −\nZ\nx\ng(x)q(x)dx.\n(22)\nTo measure the distance between two distributions, we employ the Maximum Mean Discrepancy\n(MMD) with neural tangent kernel [35] (derivation in Appendix B.3).\nLp −Lq ≤MMD(S0, S, HΘ) + O\n\u0010r\nC ln(1/δ)\nn\n\u0011\n.\n(23)\nSlightly overloading the notation, we denote the initial labeled set as S0, HΘ as the associated Repro-\nducing Kernel Hilbert Space for the NTK Θ, and ∀x, x′ ∈S, Θ(x, x′) ≤C. Note, MMD(S0, S, HΘ)\nis the empirical measure for MMD(p(x), q(x), HΘ). We empirically compute MMD and the dom-\ninant term of the generalization upper bound B under the active learning setting with our method\ndynamicAL. As shown in Figure 1, on CIFAR10 with a CNN target model (three convolutional layers\nwith global average pooling), the initial labeled set size |S| = 500, query round R = 1 and budget\nsize b ∈{250, 500, 1000}, we observe that, under different active learning settings, the MMD is\nalways much smaller than the B. Besides, we further investigate the MMD and B for R ≥2 and\nobserve the similar results. Therefore, the lemma 2 still holds for the target model with dynamicAL.\nMore results and discussions for R ≥2 are in Appendix E.4 and the computation details of MMD\nand NTK are in Appendix D.1.\n7\n4.3\nAlignment and training dynamics in active learning\nFigure 2: Relation between Align-\nment and Training Dynamics.\nIn this section, we show the relationship between the align-\nment and the training dynamics. To be consistent with the\nprevious theoretical analysis (Theorem 1 and 2), we use the\ntraining dynamics with mean square error under the ultra-\nwidth condition, which can be expressed as GMSE(S) =\nTr\n\u0002\n(f(X; θ) −Y )⊤Θ(X, X)(f(X; θ) −Y )\n\u0003\n. Due to the lim-\nited space, we leave the derivation in Appendix A.3. To further\nquantitatively evaluate the correlation between GMSE(S ∪Q)\nand A(X∥XQ, Y ∥YQ), we utilize the Kendall τ coefﬁcient [36]\nto empirically measure their relation. As shown in Figure 2,\nfor CNN on CIFAR10 with active learning setting, where\n|S| = 500 and |Q| = 250, there is a strong agreement between\nGMSE(S ∪Q) and A(X∥XQ, Y ∥YQ), which further indicates\nthat increasing the training dynamics will lead to a faster conver-\ngence and better generalization performance. More details about\nthis veriﬁcation experiment are in Appendix D.2.\n5\nExperiments\n5.1\nExperiment setup\nBaselines. We compare dynamicAL with the following eight baselines: Random, Corset, Conﬁdence\nSampling (Conf), Margin Sampling (Marg), Entropy, and Active Learning by Learning (ALBL),\nBatch Active learning by Diverse Gradient Embeddings (BADGE). Description of baseline methods\nis in Appendix E.1.\nData sets and Target Model. We evaluate all the methods on three benchmark data sets, namely,\nCIFAR10 [23], SVHN [24], and Caltech101 [25]. We use accuracy as the evaluation metric and\nreport the mean value of 5 runs. We consider three neural network architectures: vanilla CNN,\nResNet18 [26], and VGG11 [27]. For each model, we keep the hyper-parameters used in their ofﬁcial\nimplementations. More information about the implementation is in Appendix C.1.\nActive Learning Protocol. Following the previous evaluation protocol [11], we compare all those\nactive learning methods in a batch-mode setup with an initial set size M = 500 for all those three data\nsets, batch size b varying from {250, 500, 1000}. For the selection of test set, we use the benchmark\nsplit of the CIFAR10 [23], SVHN [24] and sample 20% from each class to form the test set for the\nCaltech101 [25].\n5.2\nResults and analysis\nThe main experimental results have been provided as plots due to the limited space. We also provide\ntables in which we report the mean and standard deviation for each plot in Appendix E.3.\nOverall results. The average test accuracy at each query round is shown in Figure 3. Our method\ndynamicAL can consistently outperform other methods for all query rounds. This suggests that\ndynamicAL is a good choice regardless of the labeling budget. And, we notice dynamicAL can work\nwell on data sets with a large class number, such as Caltech101. However, the previous state-of-the-art\nmethod, BADGE, cannot be scaled up to those data sets, because the required memory is linear\nwith the number of classes. Besides, because dynamicAL depends on pseudo labeling, a relatively\nlarge initial labeled set can provide advantages for dynamicAL. Therefore, it is important to examine\nwhether dynamicAL can work well with a small initial labeled set. As shown in Figure 3, dynamicAL\nis able to work well with a relatively small initial labeled set (M = 500). Due to the limited space,\nwe only show the result under three different settings in Figure 3. More evaluation results are in\nAppendix E.2. Moreover, although the re-initialization trick makes dynamicAL deviate from the\ndynamics analysis, we investigate the effect of it to dynamicAL and provide the empirical observations\nand analysis in Appendix E.5.\nEffect of query size and query round. Given the total label budget B, the increasing of query\nsize always leads to the decreasing of query round. We study the inﬂuence of different query size\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n32\n34\n36\n38\n40\n42\nT\nest Accuracy (%)\n CIF\nAR10, CNN, Query Batch Size:500, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\nBADGE\ndynamicAL\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n30\n40\n50\n60\n70\n80\n90\nT\nest Accuracy (%)\n SVHN, VGG, Query Batch Size:500, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\nBADGE\ndynamicAL\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n20\n25\n30\n35\n40\n45\n50\n55\n60\nT\nest Accuracy (%)\n Caltech101, ResNet, Query Batch Size:500, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\ndynamicAL\nFigure 3: Active learning test accuracy versus the number of query rounds for a range of conditions.\nand query round on dynamicAL from two perspectives. First, we study the expected approximation\nratio with different query batch sizes on different data sets. As shown in Figure 4, under different\nsettings the expected approximation ratio always converges to 1 with the increase of training epochs,\nwhich further indicates that the query set selected by using the approximated change of training\ndynamics is a reasonably good result for the query set selection problem. Second, we study inﬂuence\nof query round for actual performance of target models. The performance for different target models\non different data sets with total budge size B = 1000 is shown in Table 1. For certain query budget,\nour active learning algorithm can be further improved if more query rounds are allowed.\n10\n15\n20\n25\n30\n35\n40\n45\n50\nEpoch\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n\nQ\n∼\nU\nR(\n̂\nQ|S)\nCIF\nAR10, ResNet\nBatch Size b=250\nBatch Size b=500\nBatch Size b=1000\n10\n15\n20\n25\n30\n35\n40\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n\nQ\n∼\nU\nR(\n̂\nQ|S)\nSVHN, VGG\nBatch Size b=250\nBatch Size b=500\nBatch Size b=1000\nFigure 4: The Expectation of the Approximation Ratio with different query batch sizes b.\nTable 1: Accuracy of dynamicAL with different query batch size b.\nSetting\nCIFAR10+CNN\nCIFAR10+Resnet\nSVHN+VGG\nCaltech101+Resnet\nR = 10, b = 100\n36.84\n40.92\n76.34\n37.06\nR = 4, b = 250\n36.72\n40.78\n75.26\n36.48\nR = 2, b = 500\n36.71\n40.46\n74.10\n35.91\nR = 1, b = 1000\n36.67\n40.09\n70.04\n33.82\nComparison with different variants. The active learning criterion of dynamicAL can be written as\nP\n(x,y)∈S ∥∇θℓ(f(x; θu), ˆyu)∥2 + γ∇θℓ(f(xu; θ), ˆyu)⊤∇θℓ(f(x; θ), y). We empirically show the\nperformance for γ ∈{0, 1, 2, ∞} in Figure 5. With γ = 0, the criterion is close to the expected\ngradient length method [31]. And with γ = ∞, the selected samples are same with the samples\nselected by using the inﬂuence function with identity hessian matrix criterion [29]. As shown in\nFigure 5, the model achieves the best performance with γ = 2, which is aligned with the value\nindicated by the theoretical analysis (Equation 15). The result conﬁrms the importance of theoretical\nanalysis for the design of deep active learning methods.\n6\nRelated work\nNeural Tangent Kernel (NTK): Recent study has shown that under proper conditions, an inﬁnite-\nwidth neural network can be simpliﬁed as a linear model with Neural Tangent Kernel (NTK) [12].\nSince then, NTK has become a powerful theoretical tool to analyze the behavior of deep learning ar-\nchitecture (CNN, GNN, RNN) [33, 37, 38], random initialization [39], stochastic neural network [40],\nand graph neural network [41] from its output dynamics and to characterize the convergence and\ngeneralization error [13]. Besides, [15] studies the ﬁnite-width NTK, aiming at making the NTK\nmore practical.\n9\n0\n1\n2\n3\n4\n5\n# Query Round\n33\n34\n35\n36\n37\n38\n39\n40\nT\nest Accuracy (%)\n CIF\nAR10, CNN, Batch Siz : 500\nγ\n=\n0\nγ\n=\n1\nγ\n=\n∞\nγ\n=\n2\n0\n1\n2\n3\n4\n5\n# Query Round\n30\n40\n50\n60\n70\n80\nT\nest Accuracy (%)\n SVHN, VGG, Batc  Si)e: 500\nγ\n=\n0\nγ\n=\n1\nγ\n=\n∞\nγ\n=\n2\n0\n1\n2\n3\n4\n5\n# Query Round\n30\n35\n40\n45\nT\nest Accuracy (%)\n CIF\nAR10, ResNet, Batch Size: 500\nγ\n=\n0\nγ\n=\n1\nγ\n=\n∞\nγ\n=\n2\nFigure 5: Test Accuracy of different variants.\nActive Learning: Active learning aims at interactively query labels for unlabeled data points to\nmaximize model performances [2]. Among others, there are two popular strategies for active learning,\ni.e., diversity sampling [42–44] and uncertainty sampling [45–47, 11, 48, 49, 29]. Recently, several\npapers proposed to use gradient to measure uncertainty [49, 11, 29]. However, those methods need to\ncompute gradient for each class, and thus they can hardly be applied on data sets with a large class\nnumber. Besides, recent works [50, 51] leverage NTK to analyze contextual bandit with streaming\ndata, which are hard to be applied into our pool-based setting.\n7\nConclusion\nIn this work, we bridge the gap between the theoretic ﬁndings of deep neural networks and real-\nworld deep active learning applications. By exploring the connection between the generalization\nperformance and the training dynamics, we propose a theory-driven method, dynamicAL, which\nselects samples to maximize training dynamics. We prove that the convergence speed of training and\nthe generalization performance is (positively) strongly correlated under the ultra-wide condition and\nwe show that maximizing the training dynamics will lead to a lower generalization error. Empirically,\nour work shows that dynamicAL not only consistently outperforms strong baselines across various\nsetting, but also scales well on large deep learning models.\n8\nAcknowledgment\nThis work is supported by National Science Foundation (IIS-1947203, IIS-2117902, IIS-2137468,\nand IIS-2134079, and CNS-2125626), and by a joint ACES-ICGA funding initiative via USDA\nHatch ILLU-802-946, and Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-\n32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture.\nThe views and conclusions are those of the authors and should not be interpreted as representing the\nofﬁcial policies of the funding agencies or the government.\nReferences\n[1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. In International Conference on Learning\nRepresentations, 2017.\n[2] Burr Settles. Active learning literature survey. Technical report, University of Wisconsin–\nMadison, 2009.\n[3] Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends®\nin Machine Learning, 7(2-3):131–309, 2014.\n[4] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning.\nMachine learning, 15(2):201–221, 1994.\n[5] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. Journal\nof Computer and System Sciences, 75(1):78–89, 2009.\n[6] Sanjoy Dasgupta and Daniel Hsu. Hierarchical sampling for active learning. In International\nconference on Machine learning, pages 208–215, 2008.\n10\n[7] Maria-Florina Balcan, Andrei Z. Broder, and Tong Zhang. Margin based active learning. In\nConference on Learning Theory, volume 4539, pages 35–50, 2007.\n[8] Maria-Florina Balcan and Philip M. Long. Active and passive learning of linear separators\nunder log-concave distributions. In Conference on Learning Theory, volume 30, pages 288–316,\n2013.\n[9] Mina Karzand and Robert D. Nowak. Active learning in the overparameterized and interpolating\nregime. CoRR, abs/1905.12782, 2019.\n[10] Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efﬁcient and diverse batch\nacquisition for deep bayesian active learning. In Neural Information Processing Systems, pages\n7024–7035, 2019.\n[11] Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal.\nDeep batch active learning by diverse, uncertain gradient lower bounds. In International\nConference on Learning Representations, 2020.\n[12] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: convergence and\ngeneralization in neural networks. In Neural Information Processing Systems, pages 8580–8589,\n2018.\n[13] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analy-\nsis of optimization and generalization for overparameterized two-layer neural networks. In\nInternational Conference on Machine Learning, volume 97, pages 322–332, 2019.\n[14] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha\nSohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear\nmodels under gradient descent. In Neural Information Processing Systems, pages 8570–8581,\n2019.\n[15] Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In\nInternational Conference on Learning Representations, 2020.\n[16] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and\ndeep neural networks. Advances in Neural Information Processing Systems, 32:10836–10846,\n2019.\n[17] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of\nstochastic gradient descent. In International Conference on Machine Learning, volume 48,\npages 1225–1234, 2016.\n[18] Tongliang Liu, Gábor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and\nhypothesis complexity. In International Conference on Machine Learning, volume 70, pages\n2159–2167, 2017.\n[19] Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective\non training speed and model selection. In Neural Information Processing Systems, 2020.\n[20] Binxin Ru, Clare Lyle, Lisa Schut, Mark van der Wilk, and Yarin Gal.\nRevisiting the\ntrain loss: an efﬁcient performance estimator for neural architecture search. arXiv preprint\narXiv:2006.04492, 2020.\n[21] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph\nneural networks: Implicit acceleration by skip connections and more depth. In International\nConference on Machine Learning, volume 139, pages 11592–11602, 2021.\n[22] Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Schölkopf,\nand Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy.\nBioinformatics, 22(14):e49–e57, 2006.\n[23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n11\n[24] Yuval Netzer, Tao Wang, Adam Coates, Ro Bissacco, Bo Wu, and Andrew Y. Ng. Reading\ndigits in natural images with unsupervised feature learning, 2011.\n[25] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few\ntraining examples: An incremental bayesian approach tested on 101 object categories. In 2004\nconference on computer vision and pattern recognition workshop, pages 178–178. IEEE, 2004.\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778,\n2016.\n[27] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. In International Conference on Learning Representations, 2015.\n[28] Fangzhou Mu, Yingyu Liang, and Yin Li. Gradients as features for deep representation learning.\nIn International Conference on Learning Representations, 2020.\n[29] Zhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He. Inﬂuence\nselection for active learning. arXiv preprint arXiv:2108.09331, 2021.\n[30] Jiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh, and Adam Coates. Active\nlearning for speech recognition: the power of gradients. arXiv preprint arXiv:1612.03226, 2016.\n[31] Megh Shukla. Egl++: Extending expected gradient length to active learning for human pose\nestimation. arXiv preprint arXiv:2104.09493, 2021.\n[32] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions.\nIn International Conference on Machine Learning, volume 70, pages 1885–1894, 2017.\n[33] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.\nOn exact computation with an inﬁnitely wide neural net. In Neural Information Processing\nSystems, pages 8139–8148, 2019.\n[34] Zheng Wang and Jieping Ye. Querying discriminative and representative samples for batch\nmode active learning. ACM Transactions on Knowledge Discovery from Data, 9(3):1–23, 2015.\n[35] Sheng Jia, Ehsan Nezhadarya, Yuhuai Wu, and Jimmy Ba. Efﬁcient statistical tests: A neural\ntangent kernel approach. In International Conference on Machine Learning, volume 139, pages\n4893–4903, 2021.\n[36] Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.\n[37] Simon S Du, Kangcheng Hou, Barnabás Póczos, Ruslan Salakhutdinov, Ruosong Wang, and\nKeyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels.\narXiv preprint arXiv:1905.13192, 2019.\n[38] Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The recurrent\nneural tangent kernel. In International Conference on Learning Representations, 2020.\n[39] Wei Huang, Weitao Du, and Richard Yi Da Xu. On the neural tangent kernel of deep networks\nwith orthogonal initialization. arXiv preprint arXiv:2004.05867, 2020.\n[40] Wei Huang, Chunrui Liu, Yilan Chen, Tianyu Liu, and Richard Yi Da Xu. Demystify op-\ntimization and generalization of over-parameterized pac-bayesian learning. arXiv preprint\narXiv:2202.01958, 2022.\n[41] Wei Huang, Yayong Li, Weitao Du, Richard Yi Da Xu, Jie Yin, Ling Chen, and Miao Zhang.\nTowards deepening graph neural networks: A gntk-based optimization perspective. arXiv\npreprint arXiv:2103.03113, 2021.\n[42] Bo Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang, Wei Liu, Jialie Shen, and Dacheng Tao.\nExploring representativeness and informativeness for active learning. IEEE transactions on\ncybernetics, 47(1):14–26, 2015.\n12\n[43] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, and\nSilvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In Neural\nInformation Processing Systems, pages 5339–5349, 2018.\n[44] Fedor Zhdanov. Diverse mini-batch active learning. arXiv preprint arXiv:1901.05954, 2019.\n[45] Nicholas Roy and Andrew McCallum. Toward optimal active learning through sampling\nestimation of error reduction. In International Conference on Machine Learning, pages 441–\n448, 2001.\n[46] Jingbo Zhu and Matthew Ma. Uncertainty-based active learning with instability estimation for\ntext classiﬁcation. ACM Transactions on Speech and Language Processing (TSLP), 8(4):1–21,\n2012.\n[47] Yazhou Yang and Marco Loog. Active learning using uncertainty information. In International\nConference on Pattern Recognition, pages 2646–2651, 2016.\n[48] Donggeun Yoo and In So Kweon. Learning loss for active learning. In IEEE Conference on\nComputer Vision and Pattern Recognition, pages 93–102, 2019.\n[49] Burr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. In Neural\nInformation Processing Systems, pages 1289–1296, 2007.\n[50] Pranjal Awasthi, Christoph Dann, Claudio Gentile, Ayush Sekhari, and Zhilei Wang. Neural\nactive learning with performance guarantees. arXiv preprint arXiv:2106.03243, 2021.\n[51] Yikun Ban, Yuheng Zhang, Hanghang Tong, Arindam Banerjee, and Jingrui He. Improved\nalgorithms for neural active learning. 2022.\n[52] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.\nMIT press, 2018.\n[53] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander\nSmola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773,\n2012.\n[54] Felix Dangel, Frederik Kunstner, and Philipp Hennig. BackPACK: Packing more into backprop.\nIn International Conference on Learning Representations, 2020.\n[55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural information processing\nsystems, 32, 2019.\n[56] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for\nout-of-distribution generalization. In International Conference on Machine Learning, pages\n18347–18377. PMLR, 2022.\n[57] Pranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast differentially private\nsgd via just-in-time compilation and vectorization. Advances in Neural Information Processing\nSystems, 34:26409–26421, 2021.\n[58] Amir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Scaling\nneural tangent kernels via sketching and random features. arXiv preprint arXiv:2106.07880,\n2021.\n[59] R Botsch. Chapter 12: Signiﬁcance and measures of association. Scopes and Methods of\nPolitical Science, 2011.\n[60] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set\napproach. In International Conference on Learning Representations, 2018.\n[61] Dan Wang and Yi Shang. A new active labeling method for deep learning. In International\nJoint Conference on Neural Networks, pages 112–119, 2014.\n13\n[62] Dan Roth and Kevin Small. Margin-based active learning for structured output spaces. In\nMachine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin,\nGermany, September 18-22, 2006, Proceedings, volume 4212, pages 413–424, 2006.\n[63] Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In AAAI Conference on\nArtiﬁcial Intelligence, pages 2659–2665, 2015.\n[64] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,\nand Jascha Sohl-Dickstein. Finite versus inﬁnite neural networks: an empirical study. Advances\nin Neural Information Processing Systems, 33:15156–15172, 2020.\n[65] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy,\nand Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape\ngeometry and the time evolution of the neural tangent kernel. Advances in Neural Information\nProcessing Systems, 33:5850–5861, 2020.\n[66] Daniel S Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein. Towards\nnngp-guided neural architecture search. arXiv preprint arXiv:2011.06006, 2020.\n[67] Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in\nfour gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021.\n[68] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform\nresnets without pre-training or strong data augmentations. arXiv preprint arXiv:2106.01548,\n2021.\n[69] Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless\nFowlkes, Rahul Bhotika, Stefano Soatto, and Pietro Perona. A linearized framework and a new\nbenchmark for model selection for ﬁne-tuning. arXiv preprint arXiv:2102.00084, 2021.\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See appendix.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See\nappendix.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [Yes] See appendix.\n(b) Did you include complete proofs of all theoretical results? [Yes] See appendix.\n3. If you ran experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See supplemen-\ntal materials.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Section 5 and appendix.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] We report the standard deviation for all experiments. See\nappendix.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See appendix.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] See appendix.\n(b) Did you mention the license of the assets? [Yes]\n14\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [Yes]\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [Yes]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n15\nA\nAPPENDIX: Derivation of Objectives\nFor the notational convenience, we use f(x) to represent f(x; θ) in the Appendix.\nA.1\nTraining Dynamics for Cross-Entropy Loss\nThe partial derivative for softmax function can be deﬁned with the following,\n∂σi(f(x))\n∂f j(x)\n=\n\u001aσi(f(x))\n\u00001 −σi(f(x))\n\u0001\n,\ni = j,\n−σi(f(x))σj(f(x)),\ni ̸= j\n(24)\nThen, we have:\n∂ℓ(f(x), y)\n∂t\n= −\nX\ni\nyi ∂log σif(x)\n∂σi(f(x))\n∂σi(f(x))\n∂t\n= −\nX\ni\nyi\n1\nσi(f(x))\nX\nj\n∂σi(f(x))\n∂f j(x)\n∂f j(x)\n∂t\n= −\nX\ni\nyi X\nj\n\u00001[i == j] −σj(f(x))\n\u0001∂f j(x)\n∂t\n= −\nX\ni\n\u0000yi −σi(f(x))\n\u0001\n∇θf i(x)∇tθ\n(25)\nA.2\nDerivation for Cross-Entropy Loss\n∂ℓ(f(x), y)\n∂θ\n=\n∂ℓ\n∂f(x)\n∂f(x)\n∂θ\n= −\nX\ni\nyi\n1\nσi(f(x))\n∂σi(f(x))\nf(x)\n∂f(x)\n∂θ\n= −\nX\ni\nyi\n1\nσi(f(x))σi(f(x))\nX\nj\n\u00001[i == j] −σj(f(x))\n\u0001∂f j(x)\n∂θ\n=\nX\nj\n\u0000σj(f(x)) −yj\u0001∂f j(x)\n∂θ\n(26)\nA.3\nAPPENDIX: Training Dynamics for Mean Squared Error\nFor the labeled data set S, we deﬁne the Mean Squared Error(MSE) as:\nLMSE(S) =\nX\n(x,y)∈S\nℓMSE(f(x), y) = −\nX\n(x,y)∈S\nX\ni∈[K]\n1\n2(f i(x) −yi)2\nThen the training loss dynamics for each sample can be deﬁned as:\n∂ℓMSE(f(x), y)\n∂t\n= −\nX\ni\n\u0000yi −f i(x)\n\u0001\n∇θf i(x)∇tθ\nBecause neural networks are optimized by gradient descent, thus:\n∇tθ = θt+1 −θt =\nX\n(x,y)∈S\n∂ℓ(f(x), y)\n∂θ\n=\nX\n(x,y)∈S\nX\nj\n\u0000f j(x) −yj\u0001∂f j(x)\n∂θ\nTherefore, the training dynamics of MSE loss can be expressed as:\nGMSE(S) = −1\nη\n∂P\n(x,y)∈S ℓMSE(f(x), y)\n∂t\n= (f(X) −Y )⊤K(X, X)(f(X) −Y )\n16\nA.4\nAPPENDIX: Decomposition of the Change of Training Dynamics\nAccording to the deﬁnition of training dynamics ( Equation (8) ), we have,\nG(S) =\nX\ni,j\nX\n(xl,yl)∈S\n\u0000σi(f(xl; θ)) −yi\nl\n\u0001\nX\n(xl′ ,yl′ )∈S\n∇θf i(xl; θ)⊤∇θf j(xl′ ; θ)\n\u0000σj(f(xl′ ; θ)) −yj\nl′\n\u0001\nG(S ∪bQ) =\nX\ni,j\nX\n(x,y)∈S∪b\nQ\n\u0000σi(f(x; θ)) −yi\u0001\nX\n(x′,y′)∈S∪b\nQ\n∇θf i(x; θ)⊤∇θf j(x′; θ)\n\u0000σj(f(x′; θ)) −y′j\u0001\nThe change of training dynamics, ∆( bQ|S) = G(S ∪bQ) −G(S), can be further simpliﬁed as:\n∆( bQ|S) = G(S ∪bQ) −G(S)\n= 2\nX\ni,j\nX\n(xu,byu)∈b\nQ\n\u0000σi(f(xu; θ)) −byi\nu\n\u0001\nX\n(xl,yl)∈S\n∇θf i(xu; θ)⊤∇θf j(xl; θ)\n\u0000σj(f(xl; θ)) −yj\nl\n\u0001\n+\nX\ni,j\nX\n(xu,byu)∈b\nQ\n\u0000σi(f(xu; θ)) −byi\nu\n\u0001\n∇θf i(xu; θ)⊤∇θf j(xu; θ)\n\u0000σj(f(xu; θ)) −byj\nu\n\u0001\n+\nX\ni,j\nX\n(xu,byu)∈b\nQ\n\u0000σi(f(xu; θ)) −byi\nu\n\u0001\nX\n(xu′ ,byu′ )∈b\nQ,u′̸=u\n∇θf i(xu′; θ)⊤∇θf j(xu′; θ)\n\u0000σj(f(xu′; θ)) −byj\nu′\n\u0001\n=\nX\n(xu,byu)∈b\nQ\n∆({(xu, byu)}|S) +\nX\n(xu,byu),(xu′ ,byu′ )∈b\nQ\ndi(xu, byu)⊤Kij(xu, xu′)di(xu′, byu′)\nA.5\nAPPENDIX: Simpliﬁcation of the Change of Training Dynamics\n∆({(xu, byu)}|S) =2\nX\ni,j\nX\n(xu,byu)∈b\nQ\n\u0000σi(f(xu; θ)) −byi\nu\n\u0001\nX\n(xl,yl)∈S\n∇θf i(xu; θ)⊤∇θf j(xl; θ)\n\u0000σj(f(xl; θ)) −yj\nl\n\u0001\n+\nX\ni,j\nX\n(xu,byu)∈b\nQ\n\u0000σi(f(xu; θ)) −byi\nu\n\u0001\n∇θf i(xu; θ)⊤∇θf j(xu; θ)\n\u0000σj(f(xu; θ)) −byj\nu\n\u0001\nThe derivative of loss with respect to model parameters can be written as:\n∂P\n(x,y)∈S ℓ(f(x; θ), y)\n∂θ\n=\nX\n(x,y)∈S\nX\nj∈[K]\n\u0000σj(f(x; θ)) −yj\u0001\n∇θf j(x; θ)\nTherefore, the change of training dynamics caused by {(xu, byu)} can be written as:\n∆({(xu, byu)}|S) = ∥∇θℓ(f(xu; θ), ˆyu)∥2 + 2\nX\n(x,y)∈S\n∇θℓ(f(xu; θ), ˆyu)⊤∇θℓ(f(x; θ), y)\nB\nAPPENDIX: Proofs for Theoretical Analysis\nB.1\nProofs for Theorem 1\nLemma 1 (Convergence Analysis with NTK, Theorem 4.1 of [13]). Suppose λ0 = λmin(Θ) > 0\nfor all subsets of data samples. For δ ∈(0, 1), if m = Ω(\nn7\nλ4\n0δ4ϵ2 ) and η = O( λ0\nn2 ), with probability\nat least 1 −δ, the network can achieve near-zero training error,\n∥Y −ft(X; θ(t))∥2 =\nv\nu\nu\nt\nK\nX\nk=1\nn\nX\ni=1\n(1 −ηλi)2t(⃗v⊤\ni Y k)2 ± ϵ\n(27)\nwhere n denotes the number of training samples and m denotes the width of hidden layers. The NTK\nΘ = V ⊤ΛV with Λ = {λi}n\ni=1 is a diagonal matrix of eigenvalues and V = {⃗vi}n\ni=1 is a unitary\nmatrix.\n17\nProof. According to [13], if m\n=\nΩ(\nn7\nλ4\n0δ4ϵ2 ) and learning ratio η\n=\nO( λ0\nn2 ), then with\nprobability at least 1 −δ over the random initialization, we have, ∥Yl −ft(X; θ(t))∥2 =\nqPK\nk=1\nPn\ni=1(1 −ηλi)2t(v⊤\ni Y k\nl )2 ± ϵ.\nWe decompose the NTK using Θ = V ⊤ΛV with\nΛ = {λi}n\ni=1 a diagonal matrix of eigenvalues and V = {vi}n\ni=1 a unitary matrix. At each training\nstep in active learning, the labeled samples will be updated by S = S ∪Q. We can apply the\nconvergence result in each of this step and achieve near zero error.\nTheorem 1 (Relationship between convergence rate and alignment). Under the same assumptions as\nin Lemma 1, the convergence rate described by Et satisﬁes,\nTr[Y ⊤Y ] −2tηA(X, Y ) ≤E2\nt (X, Y ) ≤Tr[Y ⊤Y ] −ηA(X, Y )\n(28)\nProof. We ﬁrst prove the inequality on the right hand side. It is easy to see that (1−ηλi)2t ≤(1−ηλi)\nfor each λi and t ≥1, based on the fact that ∀λi, 0 ≤1 −ηλi ≤1. Then we can obtain,\nEt(X, Y ) =\nv\nu\nu\nt\nK\nX\nk=1\nn\nX\ni=1\n(1 −ηλi)2t(v⊤\ni Y k)2 ≤\nv\nu\nu\nt\nK\nX\nk=1\nn\nX\ni=1\n(1 −ηλi)(v⊤\ni Y k)2\n=\nq\nTr[Y ⊤(I −ηΘ)Y ] =\nq\nTr[Y ⊤Y ] −ηA(X, Y )\nThen we use Bernoulli’s inequality to prove the inequality on the left hand side. Bernoulli’s inequality\nstates that, (1 + x)r ≥1 + rx, for every integer r ≥0 and every real number x ≥−1. It is easy to\ncheck that (−ηλi) ≥−1, ∀λi. Therefore,\nEt(X, Y ) =\nv\nu\nu\nt\nK\nX\nk=1\nn\nX\ni=1\n(1 −ηλi)2t(v⊤\ni Y k)2 ≥\nv\nu\nu\nt\nK\nX\nk=1\nn\nX\ni=1\n(1 −2tηλi)(v⊤\ni Y k)2\n=\nq\nTr[Y ⊤(I −2tηΘ)Y ] =\nq\nTr[Y ⊤Y ] −2tηA(X, Y )\nB.2\nProof for Theorem 2\nLemma 2 (Generalization bound with NTK, Theorem 5.1 of [13]). Suppose data S = {(xi, yi)}n\ni=1\nare i.i.d. samples from a non-degenerate distribution p(x, y), and m ≥poly(n, λ−1\n0 , δ−1). Consider\nany loss function ℓ: R × R →[0, 1] that is 1-Lipschitz, with probability at least 1 −δ over the\nrandom initialization, the network trained by gradient descent for T ≥Ω(\n1\nηλ0 log n\nδ ) iterations has\npopulation risk Lp = E(x,y)∼p(x,y)[ℓ(fT (x), y)] that is bounded as follows:\nLp ≤\nr\n2 Tr[Y ⊤Θ−1(X, X)Y ]\nn\n+ O\n\u0012s\nlog\nn\nλ0δ\nn\n\u0013\n.\n(29)\nProof. We ﬁrst show that the generalization bound regrading our method on ultra-wide networks.\nThe distance between weights of trained networks and their initialization values can be bounded\nas, ∥wr(t) −wr(0)∥= O(\nn\n√mλ0\n√\nδ). We then give a bound on the ∥W(t) −W(0)∥F , where\nW = {w1, w2, . . . } is the set of all parameters. We deﬁnite Z =\n∂f(t)\n∂W (t), then the update function\nis given by W(t + 1) = W(t) −ηZ(Z⊤W(t) −Y ). Summing over all the time step t = 0, 1, . . . ,\nwe can obtain that W(∞) −W(0) = P∞\nt=0 ηZ(I −ηΘ)y = ZΘ−1Y . Thus the distance can be\nmeasured by ∥W(∞) −W(0)∥2\nF = Tr[Y ⊤Θ−1Y ].\nThen the key step is to apply Rademacher complexity. Given R > 0, with probability at least 1 −δ,\nsimultaneously for every B > 0, the function class FB,R = {f : ∥wr(t) −wr(0)∥≤R (∀r ∈\nm), ∥W(∞) −W(0)∥2\nF ≤B} has empirical Rademacher complexity bounded as,\nRS(FB,R) = 1\nnEϵi∈{±1}n\n\u0014\nsup\nf∈FB,R\nn\nX\ni=1\nϵif(xi)\n\u0015\n≤\nB\n√\n2n\n\u00001+(2 log 2\nδ\nm\n)1/4\u0001\n+2R2√m+R\nr\n2 log 2\nδ\n18\nwhere B =\nq\nTr[Y ⊤Θ−1(X, X)Y ], and R =\nn\n√mλ0\n√\nδ.\nFinally, Rademacher complexity directly gives an upper bound on generalization error [52],\nsupf∈F{Lp(f) −LS(f)} ≤2RS + 3c\nq\nlog(2/δ)\n2n\n, where LS(f) ≤\n1\n√n. Based on this, we ap-\nply a union bound over a ﬁnite set of different i’s. Then with probability at least 1 −δ/3 over\nthe sample S, we have supf∈FR,Bi{Lp(f) −LS(f)} ≤2RS(FBi,R) + O(\nq\nlog\nn\nλ0δ\nn\n), ∀i ∈\n{1, 2, . . . , O( n\nλ0 )}. Taking a union bound, we know that with probability at least 1 −2\n3δ over\nthe sample S, we have, fT ∈FB∗\ni ,R for some i∗, RS(FB∗\ni ,R) ≤\nq\nTr[Y ⊤Θ−1(X,X)Y ]\n2n\n+\n2\n√n and\nsupfT ∈FB∗\ni ,R{Lp(fT ) −LS(fT )} ≤2RS(FB∗\ni ,R) + O(\nq\nlog\nn\nλ0δ\nn\n). These together can imply,\nLp(f) ≤\n1\n√n + 2RS(FB∗\ni ,R) + O(\ns\nlog\nn\nλ0δ\nn\n) ≤\ns\n2 Tr[Y ⊤Θ−1(X, X)Y ]\nn\n+ O\n\u0012s\nlog\nn\nλ0δ\nn\n\u0013\n.\nMore proof details can be found in [13].\nTheorem 2 (Relationship between the generalization bound and alignment). Under the same assump-\ntions as in Lemma (2), if we deﬁne the generalization upper bound as B(X, Y ) =\nq\n2 Tr[Y ⊤Θ−1Y ]\nn\n,\nthen it can be bounded with the alignment as follows,\nTr2[Y ⊤Y ]\nA(X, Y ) ≤n\n2 B2(X, Y ) ≤λmax\nλmin\nTr2[Y ⊤Y ]\nA(X, Y )\n(30)\nProof. We ﬁrst expand the following expression:\nn\n2 B2(X, Y )A(X, Y ) =\nK\nX\nk=1\nn\nX\ni=1\nλi(v⊤\ni Y k)2\nK\nX\nk=1\nn\nX\ni=1\n1\nλi\n(v⊤\ni Y k)2\nThen we use this expansion to prove the inequality on the left hand side,\nK\nX\nk=1\nn\nX\ni=1\nλi(v⊤\ni Y k)2\nK\nX\nk=1\nn\nX\ni=1\n1\nλi\n(v⊤\ni Y k)2 =\nK\nX\nk=1\nK\nX\nk′=1\n\u0012\nn\nX\ni=1\nλi(v⊤\ni Y k)2\nn\nX\ni=1\n1\nλi\n(v⊤\ni Y k′)2\n\u0013\n≥\nK\nX\nk=1\nK\nX\nk′=1\n\u0012\nn\nX\ni=1\n(v⊤\ni Y k)2\nn\nX\ni=1\n(v⊤\ni Y k′)2\n\u0013\n=\n\u0000 K\nX\nk=1\nY k⊤V ⊤V Y k\u0001\u0000 K\nX\nk=1\nY k⊤V ⊤V Y k\u0001\n= Tr2[Y ⊤Y ]\nThe second line is due to quadratic mean is greater or equal to geometric mean. Finally, we prove the\ninequality on the right hand side,\nK\nX\nk=1\nn\nX\ni=1\nλi(v⊤\ni Y k)2\nK\nX\nk=1\nn\nX\ni=1\n1\nλi\n(v⊤\ni Y k)2 =\nK\nX\nk=1\nK\nX\nk′=1\n\u0012\nn\nX\ni=1\nλi(v⊤\ni Y k)2\nn\nX\ni=1\n1\nλi\n(v⊤\ni Y k′)2\n\u0013\n≤\nK\nX\nk=1\nK\nX\nk′=1\nλmax\nλmin\n\u0012\nn\nX\ni=1\n(v⊤\ni Y k)2\nn\nX\ni=1\n(v⊤\ni Y k′)2\n\u0013\n= λmax\nλmin\n\u0000 K\nX\nk=1\nY k⊤V ⊤V Y k\u0001\u0000 K\nX\nk=1\nY k⊤V ⊤V Y k\u0001\n= λmax\nλmin\nTr2[Y ⊤Y ]\nB.3\nDerivation for Maximum Mean Discrepancy\nThe difference between truth risk over p(x) and q(x) can be deﬁned as,\nLp −Lq =\nZ\nx\ng(x)p(x)dx −\nZ\nx\ng(x)q(x)dx\n19\nwhere g(x) =\nR\ny ℓ(f(x; θ), y)p(y|x)dy. Follow [34], we assume that the prediction functions\nhave bounded norm ∥f∥F . Thus, the function g is bounded. By given the loss function, g is also\nmeasurable. Then, ∃ˆg ∈C(x), such that,\nZ\nx\ng(x)p(x)dx −\nZ\nx\ng(x)q(x)dx =\nZ\nx\nˆg(x)p(x)dx −\nZ\nx\nˆg(x)q(x)dx\n≤sup\nˆg∈C(x)\nZ\nx\nˆg(x)p(x)dx −\nZ\nx\nˆg(x)q(x)dx = MMD\n\u0000p(x), q(x), C\n\u0001\nwhere C(x) is the function class of bounded and continuous functions of x. To make the MMD term\nbe measurable, we empirically restrict the MMD on a reproducing kernel Hilbert space (RKHS) with\nthe characteristic kernel HΘ. Following [53], we know that the relationship between the true MMD\nand the empirical MMD is,\nP\n\u0010\f\fMMD\n\u0000p(x), q(x), C\n\u0001\n−MMD(S0, S, HΘ)\n\f\f ≥ϵ + 2(\nr\nC\nn0\n+\nr\nC\nn )\n\u0011\n≤2e\n−ϵ2n0n\n2C(n0+n)\nwhere MMD(S0, S, HΘ) is the empirical measure for MMD(p(x), q(x), HΘ). Slightly overloading\nthe notation, we denote S ∼q(x), which may not be i.i.d., and the initial label set S0 ∼p(x).\nThen, in the active learning setting, S0 ⊆S. Further, we denote |S0| = n0, |S| = n and ∀x, x′ ∈\nS, Θ(x, x′) ≤C. Therefore, we have,\nq\nC\nn +\nq\nC\nn0 ≥2\nq\nC\nn . For constant factor γ =\nM\nM+B , we\nhave the following inequality,\nP\n\u0000MMD\n\u0000p(x), q(x), C\n\u0001\n≥MMD(S0, S, HΘ) + ϵ + 4\nr\nC\nn\n\u0001\n≤2e\n−γϵ2n\n4C\nDenoting 2e\n−γϵ2n\n4C\n= δ/2, then we have ϵ =\nq\n4C ln(4/δ)\nγn\n. Combining all the above results, we show\nthat with probability at least 1 −δ, the following inequality holds:\nLp −Lq ≤MMD(S0, S, HΘ) + 4\nr\nC\nn +\ns\n4C ln(4/δ)\nγn\nThen, we can get,\nLp −Lq ≤MMD(S0, S, HΘ) + O\n r\nC ln(1/δ)\nn\n!\nC\nAPPENDIX: More details of experimental settings\nC.1\nImplementation Detail\nFor simple CNN model, we utilize the same architecture used in Pytorch CIFAR10 Image Classi-\nﬁcation Tutorial 1. For ResNet model, we use the Pytorch Ofﬁcal implementation of ResNet-18 2\nand set the output dimension to the number of classes. For VGG model, we use the Pytorch Ofﬁcal\nimplementation of VGG-11 3. Besides, we leverage the library BackPACK [54] to collect the gradient\nof samples in batch.\nWe keep a constant learning rate of 0.001 for all three datasets and all three models. All the codes\nmentioned above use the MIT license. All experiments are done with four Tesla V100 SXM2 GPUs\nand a 12-core 2.2GHz CPU.\n1https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n2https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n3https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py\n20\nC.2\nComputation of Acquisition Function\nThe acquisition function employed by dynamicAL can be written as the Equation 15. Furthermore,\nwe simplify it into the following form:\n∆({(xu,byu)}|S) = ∥∇θℓ(f(xu; θ), ˆyu)∥2 + 2∇θℓ(f(xu; θ), ˆyu)⊤∇θℓ(f(XS; θ), YS).\n(31)\nwhere ∇θℓ(f(XS; θ), YS) = P\n(x,y)∈S ∇θℓ(f(x; θ), y). The computational requirement of the\nEquation 31 is mainly composed of two parts, the computation of gradient and the computation of\nthe inner product. While PyTorch [55] can compute efﬁciently batch gradients, BackPACK [54]\noptimizes the computation of individual gradient and compute the gradient norm, sample per sample,\nat almost no time overhead. Thus, the acquisition function can be computed at low computational\ncosts. Note, the efﬁciency of BackPACK has been veriﬁed by several recent works with extensive\nexperiments[56, 57].\nD\nAPPENDIX: Veriﬁcation Experiments under Ultra-wide Condition\nD.1\nExperiment Setting and Computational Detail for the Empirical Comparison between\nNTK and MMD\nExperiment Setting For the veriﬁcation experiment shown in Figure 1, we employ a simple CNN\nas the target model, in which there are three convolutional layers following with global average\npooling layer, on the CFAIR10 data set. Note, this CNN architecture is widely used in NTK analysis\nworks [33, 58]. To make the veriﬁcation experiment close to the application setting, we keep size of\ninitial labeled set and query batch size same as what we used in Section 5.\nComputational Detail We follow [35] to compute the MMD with NTK kernel. The MMD term,\nMMD(p(x), q(x), HΘ), can be simpliﬁed into the following form:\nMMD2(p(x), q(x)) = E[Θ(xi, xj) + Θ(x′\ni, x′\nj) −2Θ(xi, x′\nj)]\n(32)\nwhere xi, xj ∼p(x) and x′\ni, x′\nj ∼q(x). Then, we deﬁne set S0 as {x1, ..., xn0} ∼p(x) and\nset S as {x′\n1, ..., x′\nn} ∼q(x), where n0 ≤n. The MMD2(S0, S) is an unbiased estimation for\nMMD2(p(x), q(x)), can we explicitly computed by:\nMMD2(S0, S) =\n1\nm2 −ma +\n1\nn2 −nb −\n2\nm(n −1)c\na =\n m\nX\ni,j\nΘ(xi, xj) −\nm\nX\ni\nΘ(xi, xi)\n!\nb =\n n\nX\ni,j\nΘ(x′\ni, x′\nj) −\nn\nX\ni\nΘ(x′\ni, x′\ni)\n!\nc =\n m\nX\ni\nn\nX\nj\nΘ(xi, x′\nj) −\nm\nX\ni,j\nΘ(xi, x′\ni)\n!\n(33)\nTherefore, the MMD term of Equation (23), MMD(S0, S, HΘ) , can be empirically approximated\nby\nq\nMMD2(S0, S).\nD.2\nExperiment for the Correlation Study between Training Dynamics and Alignment\nExperiment Setting. For the veriﬁcation experiment shown in Figure 2, we also use the simple\nCNN on CIFAR10. And to keep consistent with the application setting, we set |S| = 500 and\n|Q| = 250. The Q is randomly sampled from the unlabeled set and the labeled set S is ﬁxed. We\nindependently sample Q 150 times to compute the correlation between between GMSE(S ∪Q) and\nA(X∥XQ, Y ∥YQ).\nCorrelation between Training Dynamics computed with pseudo-labels and Alignment.\n21\nFigure 6: Relation between Align-\nment and Training Dynamics com-\nputed with the pseudo-label.\nIn Figure 2, we compute the training dynamics with the\nground-truth label.\nTo study the effect of pseudo-labels,\nwe further provide the relation between training dynamics\ncomputed with pseudo-labels GMSE(S ∪Q) and alignment\nA(X∥XQ, Y ∥YQ), in which we compute the pseudo-labels\nwith Θ(XQ, X)⊤Θ(X, X)−1Y . The result is shown in the\nFigure 6. Note that we keep hyperparameters the same as pre-\nviously described. Compared with Figure 2, we ﬁnd that the\npositive relationship between A and the G computed with\nground-truth labels is stronger than the G computed with\npseudo-labels. The result is aligned with our expectations,\nbecause the extra noise is introduced by the pseudo-labels.\nBut, the Kendall τ coefﬁcient still achieves 0.46 for A and\nthe G computed with pseudo-labels which indicates the utility\nof using G calculating with pseudo-labels as the acquisition\nfunction to query samples.\nD.3\nCorrelation Study\nbetween Training Dynamics and Generalization Bound\nWe present the relation between the training dynamics and the\ngeneralization bound in Figure 7. Same as the previous, we set |S| = 500 and |Q| = 250 and the Q\nis randomly sampled from the unlabeled set. The result shows that with the increase of G, B will\ndecrease. This empirical observation is aligned with our expectation, because Theorem 2 indicates\nthat the alignment A is inverse proportional to B and Figure 2 tells us that the G is proportional to\nA. Besides, the τ achieves -0.253 which indicates that the A is moderately inverse proportional to\nB [59].\nFigure 7: Relationship between Training Dynamics and Generalization.\nE\nAPPENDIX: More details of experimental results\nE.1\nBaselines\n1. Random: Unlabeled data are randomly selected at each round.\n2. Coreset: This method performs a clustering over the last hidden representations in the\nnetwork, and calculates the minimum distance between each candidate sample’s embedding\nand embeddings of labeled samples. Then data samples with the maximum distances are\nselected. [60].\n3. Conﬁdence Sampling (Conf): The method selects b examples with smallest predicted class\nprobability maxK\ni f i(x; θ) [61].\n22\n4. Margin Sampling (Marg): The bottom b examples sorted according to the example’s multi-\nclass margin are selected. The margin is deﬁned as f i(x; θ) −f j(x; θ), where i and j are\nthe indices of the largest and second largest entries of f(x; θ) [62].\n5. Entropy: Top b samples are selected according to the entropy of the example’s predictive\nclass probability distribution, the entropy is deﬁned as H((f i(x; θ))K\ni=1), where H(p) =\nPK\ni pi ln 1\npi [61].\n6. Active Learning by Learning (ALBL): The bandit-style meta-active learning algorithm\ncombines Coreset and Conf [63].\n7. Batch Active learning by Diverse Gradient Embeddings (BADGE): b samples are selected by\nusing k-means++ seeding on the gradients of the ﬁnal layer, in order to query by uncertainty\nand diversity. [11].\nE.2\nExperiment Results\nThe results for ResNet18, VGG11, and vanilla CNN on CIFAR10, SVHN, and Caltech101 with\ndifferent batch sizes have been shown in the Figure 3 and 8. Note, for the large batch size setting\n(b = 1000) on Caltech101, we set the number of query round R = 4, in which 49.2% images will be\nlabeled after 4 rounds.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n32\n33\n34\n35\n36\n37\n38\n39\nT\nest Accuracy (%)\n CIF\nAR10, CNN, Query Batch Size:250, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\nBADGE\ndynamicAL\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n30\n35\n40\n45\n50\nT\nest Accuracy (%)\n CIF\nAR10, ResNet, Query Batch Size:500, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\nBADGE\ndynamicAL\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n30\n35\n40\n45\n50\n55\n60\nT\nest Accuracy (%)\n CIF\nAR10, ResNet, Query Batch Size:1000, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\nBADGE\ndynamicAL\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n30\n40\n50\n60\n70\n80\n90\nT\nest Accuracy (%)\n SVHN, VGG, Query Batch Size:1000, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\nBADGE\ndynamicAL\n0\n1\n2\n3\n4\n# Query Round\n20\n25\n30\n35\n40\n45\n50\n55\nT\nest Accuracy (%)\n Caltech101, ResNet, Query Batch Size:1000, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\ndynamicAL\nFigure 8: The evaluation results for different active learning methods under a range of conditions.\nE.3\nNumerical Result of Main Experiments\nFor the the main experiments, we report the means and standard deviations of active learning\nperformance under different settings in the the following tables.\nTable 2: CIFAR10, CNN, Query Batch Size:250, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\nBADGE\ndynamicAL\n0\n32.32%±1.308%\n32.48%±1.286%\n32.32%±1.269%\n32.41%±1.281%\n32.27%±1.266%\n32.50%±1.263%\n32.66%±1.182%\n32.57%±1.423%\n1\n33.00%±1.175%\n33.16%±1.165%\n32.74%±1.617%\n32.75%±1.306%\n33.00%±1.703%\n32.98%±1.184%\n33.45%±1.813%\n33.52%±1.311%\n2\n34.14%±1.322%\n34.41%±1.130%\n34.06%±1.546%\n33.77%±1.011%\n34.21%±1.426%\n34.02%±1.392%\n34.66%±1.483%\n34.70%±1.019%\n3\n35.05%±1.508%\n35.50%±1.301%\n35.16%±1.679%\n34.44%±0.937%\n35.25%±1.344%\n34.97%±1.227%\n35.75%±1.024%\n35.78%±1.115%\n4\n35.64%±1.945%\n36.55%±1.249%\n36.14%±1.646%\n35.08%±1.396%\n36.59%±1.508%\n35.58%±1.177%\n36.33%±0.791%\n36.72%±0.716%\n5\n36.28%±1.124%\n37.18%±1.547%\n36.77%±1.004%\n35.68%±1.390%\n37.19%±1.063%\n36.15%±1.311%\n37.29%±1.126%\n37.45%±1.573%\n6\n36.88%±1.568%\n37.73%±1.546%\n37.28%±1.983%\n36.18%±1.419%\n37.65%±2.062%\n36.65%±1.111%\n37.90%±1.988%\n37.95%±1.414%\n7\n37.29%±1.605%\n38.01%±0.874%\n37.67%±1.723%\n36.57%±1.346%\n38.09%±1.174%\n37.07%±1.731%\n38.28%±1.474%\n38.41%±1.295%\n8\n37.59%±1.848%\n38.43%±1.675%\n38.01%±1.601%\n36.98%±0.748%\n38.58%±1.556%\n37.35%±1.135%\n38.51%±1.091%\n38.70%±1.291%\n9\n37.85%±1.789%\n38.75%±1.550%\n38.29%±1.312%\n37.25%±1.527%\n38.91%±1.902%\n37.57%±1.170%\n38.78%±0.776%\n38.91%±1.358%\n23\nTable 3: CIFAR10, CNN, Query Batch Size:500, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\nBADGE\ndynamicAL\n0\n32.26%±1.164%\n32.31%±1.441%\n32.29%±1.397%\n32.54%±1.331%\n32.32%±1.288%\n32.41%±1.432%\n32.49%±1.320%\n32.37%±1.049%\n1\n34.87%±1.286%\n34.89%±1.575%\n34.58%±1.664%\n33.84%±1.368%\n34.75%±1.503%\n34.08%±1.368%\n34.44%±1.230%\n34.88%±1.557%\n2\n36.45%±0.842%\n36.69%±1.456%\n36.50%±1.463%\n35.96%±1.667%\n36.73%±1.744%\n35.62%±1.536%\n36.41%±1.175%\n36.78%±1.253%\n3\n37.16%±0.767%\n37.99%±1.356%\n37.30%±1.221%\n36.20%±1.086%\n38.12%±1.663%\n36.55%±1.327%\n37.56%±1.284%\n38.30%±1.152%\n4\n37.89%±0.880%\n39.15%±1.056%\n38.23%±0.878%\n36.73%±1.011%\n39.10%±1.336%\n37.20%±1.381%\n38.49%±1.238%\n39.37%±0.708%\n5\n38.59%±0.861%\n39.98%±1.562%\n39.01%±1.278%\n37.33%±1.373%\n39.81%±1.402%\n37.80%±1.560%\n39.61%±1.219%\n40.09%±0.940%\n6\n39.15%±1.108%\n40.70%±1.391%\n39.68%±1.315%\n37.97%±1.393%\n40.47%±1.126%\n38.47%±1.270%\n40.55%±1.066%\n40.75%±1.671%\n7\n39.51%±1.219%\n40.99%±1.217%\n40.09%±1.408%\n38.53%±1.600%\n41.05%±1.448%\n39.11%±1.385%\n40.97%±0.814%\n41.21%±1.433%\n8\n39.90%±0.807%\n41.39%±1.614%\n40.39%±1.357%\n39.06%±1.156%\n41.30%±1.865%\n39.55%±1.595%\n41.27%±1.409%\n41.59%±1.013%\n9\n40.17%±1.170%\n41.64%±1.287%\n40.71%±0.739%\n39.43%±0.892%\n41.55%±1.341%\n39.95%±1.299%\n41.41%±0.949%\n41.78%±0.645%\nTable 4: CIFAR10, ResNet, Query Batch Size:500, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\nBADGE\ndynamicAL\n0\n28.75%±1.780%\n28.75%±0.369%\n28.63%±1.394%\n28.63%±1.120%\n28.31%±1.011%\n28.75%±0.957%\n28.95%±1.040%\n28.52%±0.686%\n1\n34.11%±3.088%\n34.62%±2.022%\n34.42%±0.849%\n34.57%±0.992%\n33.49%±1.269%\n34.42%±2.077%\n34.26%±1.740%\n35.58%±2.858%\n2\n39.63%±2.157%\n39.63%±0.313%\n40.08%±1.022%\n38.94%±1.408%\n38.23%±1.454%\n40.16%±2.574%\n39.78%±1.384%\n40.46%±0.959%\n3\n41.38%±2.357%\n42.15%±0.810%\n42.18%±1.271%\n40.96%±0.961%\n40.87%±0.860%\n42.26%±2.347%\n41.74%±1.230%\n42.51%±0.799%\n4\n43.18%±1.809%\n44.09%±1.165%\n44.09%±1.150%\n42.60%±1.094%\n43.10%±1.325%\n43.52%±3.064%\n43.76%±1.364%\n44.36%±0.980%\n5\n44.73%±2.253%\n45.57%±1.115%\n45.00%±0.731%\n43.86%±1.369%\n44.83%±1.388%\n44.64%±3.097%\n44.73%±1.675%\n46.02%±0.754%\n6\n46.00%±2.193%\n47.17%±0.929%\n46.74%±1.118%\n45.08%±1.549%\n45.83%±1.426%\n46.22%±2.601%\n46.38%±1.607%\n47.34%±1.027%\n7\n46.80%±2.134%\n48.18%±1.230%\n47.69%±1.253%\n46.02%±1.589%\n47.47%±1.424%\n47.18%±2.384%\n47.17%±1.404%\n48.48%±1.452%\n8\n47.91%±1.722%\n49.26%±0.652%\n49.05%±1.113%\n47.14%±1.880%\n48.40%±1.178%\n48.18%±2.503%\n48.11%±1.049%\n49.58%±1.673%\n9\n48.84%±1.584%\n49.75%±1.341%\n49.46%±1.282%\n48.07%±1.480%\n49.35%±1.269%\n49.45%±2.529%\n49.06%±0.850%\n50.50%±1.301%\nTable 5: CIFAR10, ResNet, Query Batch Size:1000, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\nBADGE\ndynamicAL\n0\n28.34%±1.465%\n28.07%±2.604%\n28.24%±1.756%\n28.41%±0.722%\n29.05%±1.137%\n29.06%±0.847%\n28.43%±1.176%\n28.48%±2.062%\n1\n40.08%±0.329%\n39.57%±1.551%\n39.09%±2.180%\n38.95%±1.047%\n39.50%±2.340%\n39.67%±1.489%\n39.46%±3.020%\n40.09%±1.795%\n2\n45.63%±1.253%\n45.43%±0.444%\n44.48%±1.823%\n43.78%±0.986%\n45.62%±1.882%\n43.58%±1.329%\n44.55%±3.654%\n45.77%±2.290%\n3\n47.90%±1.257%\n47.96%±0.735%\n48.15%±2.509%\n45.93%±0.682%\n48.82%±1.797%\n47.24%±1.926%\n47.39%±4.189%\n49.22%±1.704%\n4\n50.13%±1.207%\n50.49%±0.807%\n49.97%±2.819%\n48.14%±0.566%\n50.79%±1.870%\n49.05%±1.831%\n49.13%±4.053%\n51.50%±1.925%\n5\n52.14%±1.517%\n52.24%±0.781%\n52.00%±2.762%\n49.85%±1.075%\n52.59%±2.202%\n50.59%±1.636%\n50.94%±3.628%\n53.24%±1.927%\n6\n53.33%±1.300%\n53.87%±0.635%\n53.57%±3.123%\n52.01%±0.772%\n53.99%±2.390%\n52.69%±1.599%\n52.36%±3.924%\n55.06%±1.697%\n7\n54.84%±1.238%\n55.19%±1.136%\n54.79%±3.144%\n52.99%±1.147%\n55.60%±2.002%\n54.20%±1.685%\n53.77%±3.985%\n56.33%±1.613%\n8\n55.86%±1.161%\n56.90%±0.732%\n56.23%±3.182%\n54.45%±0.821%\n56.79%±2.033%\n55.20%±1.868%\n54.91%±4.104%\n57.76%±1.796%\n9\n56.84%±0.979%\n57.73%±0.500%\n57.29%±3.225%\n55.42%±0.954%\n57.70%±2.042%\n56.67%±1.783%\n56.02%±3.935%\n58.56%±1.574%\nTable 6: SVHN, VGG, Query Batch Size:500, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\nBADGE\ndynamicAL\n0\n25.94%±7.158%\n26.15%±6.290%\n26.41%±8.994%\n25.83%±5.845%\n26.52%±7.489%\n25.31%±5.030%\n26.38%±9.100%\n26.30%±5.505%\n1\n61.23%±4.812%\n63.93%±3.127%\n59.02%±4.724%\n57.02%±3.672%\n61.99%±2.613%\n62.14%±5.531%\n58.70%±5.615%\n63.01%±12.293%\n2\n71.35%±2.364%\n74.08%±0.933%\n71.25%±1.459%\n67.95%±2.870%\n73.31%±2.828%\n71.75%±2.555%\n74.74%±2.978%\n74.10%±4.557%\n3\n76.34%±1.626%\n79.17%±1.064%\n76.74%±1.521%\n73.76%±2.844%\n78.02%±1.939%\n77.65%±1.518%\n77.75%±2.100%\n79.20%±2.651%\n4\n78.86%±1.378%\n82.18%±0.504%\n79.67%±0.809%\n78.14%±2.486%\n81.32%±1.901%\n81.09%±1.005%\n80.16%±1.353%\n82.33%±2.134%\n5\n80.56%±1.149%\n83.85%±0.750%\n81.87%±0.638%\n80.34%±2.339%\n83.31%±1.529%\n83.37%±1.225%\n82.94%±0.830%\n84.19%±1.940%\n6\n81.98%±1.334%\n85.61%±0.624%\n83.56%±0.541%\n82.32%±1.592%\n84.94%±0.858%\n85.19%±0.993%\n83.69%±0.975%\n85.80%±1.498%\n7\n83.00%±1.048%\n86.62%±0.607%\n84.94%±0.079%\n83.98%±1.394%\n85.97%±1.179%\n86.31%±0.977%\n85.15%±0.760%\n86.75%±1.426%\n8\n83.59%±0.945%\n87.57%±0.625%\n85.78%±0.068%\n85.26%±1.431%\n87.13%±0.679%\n87.55%±0.831%\n86.61%±0.478%\n87.91%±1.264%\n9\n84.42%±0.744%\n88.23%±0.600%\n87.11%±0.437%\n86.18%±0.886%\n87.87%±0.598%\n87.89%±0.780%\n87.29%±0.441%\n88.52%±1.240%\nTable 7: SVHN, VGG, Query Batch Size:1000, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\nBADGE\ndynamicAL\n0\n25.90%±3.479%\n26.20%±5.409%\n26.85%±4.403%\n26.18%±6.853%\n27.21%±8.721%\n26.60%±4.688%\n26.88%±6.248%\n26.43%±8.047%\n1\n70.26%±3.154%\n69.06%±3.646%\n68.72%±2.156%\n68.46%±1.111%\n69.85%±3.485%\n70.51%±3.487%\n70.09%±2.690%\n70.04%±1.650%\n2\n77.91%±1.061%\n78.24%±2.237%\n78.56%±0.492%\n77.66%±1.784%\n78.89%±2.809%\n78.14%±1.494%\n78.67%±1.799%\n78.86%±1.710%\n3\n81.25%±0.812%\n83.68%±1.657%\n82.83%±0.527%\n82.34%±1.461%\n83.75%±2.165%\n83.50%±1.669%\n83.07%±1.334%\n83.11%±1.269%\n4\n83.63%±0.746%\n86.12%±1.251%\n85.80%±0.744%\n85.34%±1.126%\n85.91%±1.128%\n86.18%±0.979%\n86.50%±1.087%\n85.70%±1.179%\n5\n85.17%±0.870%\n88.04%±1.022%\n87.66%±0.683%\n87.19%±0.928%\n87.61%±1.044%\n87.65%±1.031%\n88.03%±0.742%\n87.46%±1.054%\n6\n86.06%±0.822%\n89.13%±0.712%\n88.96%±0.395%\n88.65%±0.505%\n88.90%±0.845%\n88.93%±0.809%\n88.41%±0.783%\n88.89%±1.274%\n7\n87.30%±0.948%\n90.36%±0.532%\n90.00%±0.257%\n89.65%±0.486%\n90.18%±0.706%\n89.83%±0.747%\n90.53%±0.495%\n90.09%±1.149%\n8\n87.69%±0.890%\n90.95%±0.375%\n90.67%±0.385%\n90.15%±0.410%\n90.96%±0.677%\n90.75%±0.567%\n91.25%±0.432%\n90.95%±0.782%\n9\n88.28%±0.723%\n91.59%±0.417%\n91.25%±0.353%\n90.64%±0.311%\n91.66%±0.755%\n91.41%±0.665%\n91.76%±0.367%\n91.67%±0.840%\n24\nTable 8: Caltech101, ResNet, Query Batch Size:500, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\ndynamicAL\n0\n21.59%±1.431%\n21.98%±1.688%\n21.49%±1.681%\n21.39%±1.738%\n21.98%±1.459%\n21.48%±1.828%\n21.38%±1.323%\n1\n25.84%±1.112%\n28.42%±1.677%\n27.43%±0.760%\n28.61%±1.224%\n27.25%±1.155%\n28.02%±1.515%\n29.34%±2.111%\n2\n34.94%±0.635%\n34.76%±1.745%\n32.94%±1.224%\n35.37%±1.561%\n32.85%±0.849%\n35.86%±1.012%\n36.14%±1.234%\n3\n37.34%±1.088%\n39.70%±1.328%\n36.36%±0.636%\n40.81%±1.005%\n37.52%±1.250%\n40.20%±1.091%\n41.19%±0.789%\n4\n43.87%±0.867%\n43.26%±0.612%\n40.12%±0.805%\n45.38%±0.508%\n41.82%±1.104%\n45.27%±0.725%\n46.11%±1.138%\n5\n45.45%±1.672%\n46.25%±1.562%\n43.24%±1.617%\n47.81%±1.683%\n44.60%±1.295%\n48.35%±1.729%\n49.42%±1.298%\n6\n47.60%±1.383%\n49.20%±1.310%\n45.71%±1.047%\n50.60%±1.596%\n46.74%±0.760%\n51.20%±1.466%\n52.31%±1.739%\n7\n49.97%±0.530%\n51.40%±1.571%\n48.19%±0.928%\n52.80%±1.887%\n49.19%±0.885%\n53.90%±1.166%\n55.03%±1.098%\n8\n52.06%±1.476%\n53.56%±1.044%\n50.81%±0.943%\n55.31%±1.105%\n51.99%±1.383%\n56.22%±0.838%\n56.92%±1.153%\n9\n54.04%±0.898%\n55.92%±0.496%\n53.05%±0.554%\n56.93%±0.691%\n54.96%±0.981%\n57.99%±0.805%\n58.81%±1.040%\nTable 9: Caltech101, ResNet, Query Batch Size:1000, Initial Set Size:500\nRANDOM\nMARG\nENTROPY\nCORESET\nCONF\nALBL\ndynamicAL\n0\n22.13%±1.050%\n22.05%±1.011%\n21.83%±0.725%\n20.98%±0.631%\n22.03%±1.364%\n22.05%±0.633%\n21.42%±1.735%\n1\n33.91%±1.330%\n33.80%±1.002%\n31.98%±1.000%\n33.40%±0.962%\n32.43%±0.895%\n33.66%±2.174%\n33.83%±1.438%\n2\n42.08%±0.560%\n41.22%±0.730%\n39.23%±0.981%\n43.24%±0.960%\n40.05%±0.988%\n43.28%±2.360%\n43.27%±2.280%\n3\n47.43%±0.700%\n47.16%±0.659%\n46.26%±0.968%\n50.51%±0.706%\n47.87%±0.698%\n50.10%±2.082%\n50.43%±1.634%\n4\n52.77%±0.980%\n54.52%±1.288%\n54.11%±1.347%\n56.15%±1.284%\n53.76%±1.196%\n56.96%±1.733%\n57.52%±1.189%\nE.4\nMaximum Mean Discrepancy for Multiple Rounds\nAs shown in Figure 1, the MMD term is much smaller than the B at the ﬁrst query round. To better\nunderstand the relation between MMD and B for multiple query setting, we measure the MMD/B\nfor R ≥2. As shown in Figure 9, B is much larger than MMD even multiple query rounds. Besides,\nwe notice that, for the ﬁrst round, the larger query batch always leads to larger MMD/B, because the\nsampling bias introduced by the query policy will be ampliﬁed by using large batch size.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n# Query Round\n0.003\n0.004\n0.005\n0.006\n0.007\n0.008\n0.009\n0.010\nMMD/B\n100\n250\n500\n1000\nFigure 9: MMD/B for larger query round.\nFurthermore, we measure the MMD/B with a constant total budget size but different query rounds.\nThe result is shown in Table 10. As our expectation, spending the total query budget in one query\nround will induce the largest MMD/B. And, with more query rounds, the MMD/B will be lower.\nTable 10: MMD/B under constant budget size.\nSETTING\nR = 10, b = 100\nR = 4, b = 250\nR = 2, b = 500\nR = 1, b = 1000\nMMD/B\n0.004999\n0.005253\n0.005367\n0.005455\nE.5\nPerformance under the Re-initialization Setting\nTo study the effectiveness of dynamicALunder the re-initialization setting, we compare dynamicAL\nwith the strong baseline involving the re-initialization trick in its algorithm design, e.g., Coreset [60].\n25\nFollowing [11], we query samples when training accuracy is greater than 99% and the results\nare summarized in Table 11 and 12. The results show that dynamicAL can still be better than or\ncompetitive with the commonly used active learning methods. We notice that the improvement in\nthe non-retraining setting is more signiﬁcant. This is as our expectation. The dynamic analysis\n(Equation (8)), that dynamicAL is based on, considers the change of dynamics according to the model’s\ncurrent parameters. The re-initialization trick will not only causes the computational overhead of\nretraining, but also makes dynamicAL deviate from the analysis (Section 4).\nE.6\nPerformance with large query rounds\nWe provide the experiments with b = 500, r = 15 on Caltech101 data set with ResNet18 as the\nbackbone. We ignore the BACKGROUND Google label and then we have 8677 images in total.\nAt the last round, we run out of all images in the pool. As shown in the Figure 10, our method\nconsistently outperforms those baselines. Note, due to the non-retraining setting, the model will have\ndifferent performance even if all the samples are used for the training.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n# Query Round\n20\n30\n40\n50\n60\nT\nest Accuracy (%)\n Caltech101, ResNet, Query Batch Size:500, Initial Set Size:500\nRandom\nMarg\nEntropy\nCoreset\nConf\nALBL\ndynamicAL\nFigure 10: The evaluation result with larger query round on Caltech101.\nTable 11: CIFAR10, ResNet, Query Batch Size 500, Initial Set Size 500.\n#ROUND\nRANDOM\nCORESET\ndynamicAL\n0\n30.80±1.81\n30.77±0.92\n30.94±2.17\n1\n35.80±1.52\n36.62±2.10\n36.47±0.13\n2\n42.91±1.75\n43.16±1.79\n42.74±2.44\n3\n43.76±0.65\n44.35±2.25\n46.43±1.07\n4\n47.03±1.19\n48.74±1.94\n49.38±1.80\n5\n49.16±1.77\n50.20±1.25\n51.61±1.09\n6\n52.43±1.33\n53.44±1.37\n54.33±1.76\n7\n52.81±1.55\n53.89±0.78\n54.59±1.04\n8\n54.56±0.23\n57.12±1.11\n57.50±1.28\n9\n58.08±1.48\n59.62±1.50\n60.35±1.80\n26\nTable 12: SVHN, VGG, Query Batch Size 500, Initial Set Size 500.\n#ROUND\nRANDOM\nCORESET\ndynamicAL\n0\n52.68±1.97\n52.74±6.16\n52.59±3.73\n1\n67.64±1.99\n68.08±3.61\n66.48±4.10\n2\n73.46±1.51\n74.93±1.44\n74.34±2.22\n3\n77.30±1.08\n76.49±2.08\n76.73±2.65\n4\n79.27±0.78\n79.33±0.72\n80.19±0.78\n5\n79.97±1.28\n82.09±1.08\n82.08±1.39\n6\n83.97±0.42\n82.30±0.33\n83.80±1.30\n7\n83.44±0.57\n83.29±1.11\n84.85±1.12\n8\n86.24±0.52\n84.72±0.52\n86.59±1.25\n9\n85.75±1.23\n85.62±0.55\n86.57±0.74\nF\nDiscussion\nLimitation and Future Work.\nIn the work, we study the connection between generalization\nperformance and the training dynamics under the NTK regime. Although the relation between training\ndynamics and generalization performance has been veriﬁed by our experiments, the theoretical\nanalysis of the relation out of the NTK regime still needs study. Besides, in the experiments, we\nmainly focus on the classiﬁcation problem. Whether the proposed method is effective for the\nregression problem is under-explored. We would like to leave the study of the previously mentioned\ntwo problems in the future work.\nNTK Analysis for the Design of Practical Method.\nAlthough some works [64, 65] discussed\nthat the NTK assumption is hard to be strictly satisﬁed in some real-world models, we notice that\nsome recent works have shown that the high-level conclusions derived based on NTK is insightful\nand useful for the design of practical models. Some of their applications can achieve SOTA. For\nexample, Park et al. [66] used the NTK to predict the generalization performance of architectures in\nthe application of Neural Architecture Search (NAS). Chen et al. [67] used the condition number of\nNTK to predict a model’s trainability. Chen et al. [68] also used the NTK to evaluate the trainability\nof several ImageNet models, such as ResNet. Deshpande et al. [69] used the NTK for model selection\nin the ﬁne-tuning of pre-trained models on a target task. In our work, the empirical results in Figure 3\nand Appendix.E also show the effectiveness of the high-level conclusions derived from the theory\nstill hold.\nSocial Impacts.\nIn this work, we study the connection between the generalization performance\nand the training dynamics and try to bridge the gap between the theoretic ﬁndings of deep neural\nnetworks and deep active learning applications. We hope our work would inspire more attempts on\nthe design of deep active learning algorithms with theoretical justiﬁcation, which might have positive\nsocial impacts. We do not foresee any form of negative social impact induced by our work.\nLicense Privacy Information.\nWe use the commonly used datasets, CIFAR104, SVHN5, Cal-\ntech1016 in the experiments. Those datasets follow the MIT, CC0 1.0, CC BY 4.0 License respectively\nand are publicly accessible. No privacy information is included in those datasets.\n4https://www.cs.toronto.edu/~kriz/cifar.html\n5http://ufldl.stanford.edu/housenumbers/\n6https://data.caltech.edu/records/20086\n27\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-10-16",
  "updated": "2022-11-20"
}