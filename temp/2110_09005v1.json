{
  "id": "http://arxiv.org/abs/2110.09005v1",
  "title": "Unsupervised Learned Kalman Filtering",
  "authors": [
    "Guy Revach",
    "Nir Shlezinger",
    "Timur Locher",
    "Xiaoyong Ni",
    "Ruud J. G. van Sloun",
    "Yonina C. Eldar"
  ],
  "abstract": "In this paper we adapt KalmanNet, which is a recently pro-posed deep neural\nnetwork (DNN)-aided system whose architecture follows the operation of the\nmodel-based Kalman filter (KF), to learn its mapping in an unsupervised manner,\ni.e., without requiring ground-truth states. The unsupervised adaptation is\nachieved by exploiting the hybrid model-based/data-driven architecture of\nKalmanNet, which internally predicts the next observation as the KF does. These\ninternal features are then used to compute the loss rather than the state\nestimate at the output of the system. With the capability of unsupervised\nlearning, one can use KalmanNet not only to track the hidden state, but also to\nadapt to variations in the state space (SS) model. We numerically demonstrate\nthat when the noise statistics are unknown, unsupervised KalmanNet achieves a\nsimilar performance to KalmanNet with supervised learning. We also show that we\ncan adapt a pre-trained KalmanNet to changing SS models without providing\nadditional data thanks to the unsupervised capabilities.",
  "text": "UNSUPERVISED LEARNED KALMAN FILTERING\nGuy Revach, Nir Shlezinger, Timur Locher, Xiaoyong Ni, Ruud J. G. van Sloun, and Yonina C. Eldar\nABSTRACT\nIn this paper we adapt KalmanNet, which is a recently pro-\nposed deep neural network (DNN)-aided system whose ar-\nchitecture follows the operation of the model-based Kalman\nﬁlter (KF), to learn its mapping in an unsupervised man-\nner, i.e., without requiring ground-truth states.\nThe unsu-\npervised adaptation is achieved by exploiting the hybrid\nmodel-based/data-driven architecture of KalmanNet, which\ninternally predicts the next observation as the KF does. These\ninternal features are then used to compute the loss rather than\nthe state estimate at the output of the system. With the capa-\nbility of unsupervised learning, one can use KalmanNet not\nonly to track the hidden state, but also to adapt to variations\nin the state space (SS) model. We numerically demonstrate\nthat when the noise statistics are unknown, unsupervised\nKalmanNet achieves a similar performance to KalmanNet\nwith supervised learning. We also show that we can adapt a\npre-trained KalmanNet to changing SS models without pro-\nviding additional data thanks to the unsupervised capabilities.\nIndex Terms— Kalman ﬁlter, unsupervised learning.\n1. INTRODUCTION\nReal-time tracking of hidden state sequences from noisy ob-\nservations plays a major role in many signal processing sys-\ntems. Classic approaches are based on the Kalman ﬁlter (KF)\n[1] and its variants [2, Ch. 10]. These model-based (MB)\ntechniques rely on accurate knowledge of an underlying sta-\ntistical state space (SS) model capturing the system dynamics,\nwhich may not be available in some applications, and tend\nto notably degrade in the presence of model mismatch. To\ncope with missing model parameters, data is commonly used\nfor parameter estimation, followed by plugging in the missing\nparameters into the MB KF and its variants [3,4].\nThe unprecedented success of deep learning has spurred a\nmultitude of deep neural networks (DNNs) based approaches\nfor SS model related tasks, that are optimized in an end-to-end\nmanner. This allows to achieve improved accuracy compared\nG. Revach. T. Locher, and X. Ni are with the Institute for Signal and\nInformation Processing (ISI), D-ITET, ETH Z¨urich, Switzerland (e-mail:\n{grevach; tlocher}@ethz.ch, xiaoni@student.ethz.ch). N. Shlezinger is with\nthe School of ECE, Ben-Gurion University of the Negev, Beer Sheva, Israel\n(e-mail: nirshl@bgu.ac.il). R. J. G. van Sloun is with the EE Dpt., Eind-\nhoven University of Technology, and with Phillips Research, Eindhoven, The\nNetherlands (e-mail: r.j.g.v.sloun@tue.nl). Y. C. Eldar is with the Faculty of\nMath and CS, Weizmann Institute of Science, Rehovot, Israel (e-mail: yon-\nina@weizmann.ac.il). The authors thank Prof. Hans-Andrea Loeliger for his\nhelpful comments and discussion.\nwith MB algorithms when applied in complex, poorly under-\nstood, and partially known dynamics, by learning to carry out\nthe task directly from data. Notable approaches include DNN\nfeature extractors [5], variational inference techniques [6–11],\nand the usage of recurrent neural networks (RNNs) [12–15].\nWhen the SS model is partially known, one can beneﬁt from\nthe available knowledge by using the hybrid MB/data-driven\n(DD) KalmanNet architecture proposed in [16] for the real-\ntime ﬁltering task, as a learned KF via MB deep learning [17].\nA key challenge in applying an end-to-end DNN-based\nﬁlters, stems from their need to be trained using labeled data\ni.e., a large volume of pairs of noisy measurements and their\ncorresponding ground-truth hidden state sequences from the\nunderlying SS model. Obtaining such ground-truth sequences\nmay be costly, particularly in setups where the underlying dy-\nnamics, i.e., the SS model, change over time. Previous works\non DNN-based for SS model related tasks in the unsupervised\nsetup, focused mostly on the imputation task of ﬁlling in miss-\ning observations [7–10]. This task notably differs from real-\ntime state estimations, also known as ﬁltering [2, Ch. 4].\nIn this work we extend KalmanNet [16], to learn its\nmapping in an unsupervised fashion by building upon its\ninterpretable hybrid architecture, which learns to implement\nthe KF while preserving its structure. Speciﬁcally, we deﬁne\na loss measure that uses the noisy observations and their pre-\ndictions taken from an internal feature of KalmanNet. We\nalso propose a semi-supervised training method, which ﬁrst\ntrains KalmanNet ofﬂine, and then adapts in an unsupervised\nonline manner to dynamics that differ from the ofﬂine trained\nmodel without providing ground-truth data. This mechanism\nresults in KalmanNet tracking not only the latent state, but\nalso changes in the underlying SS model.\nOur numerical\nevaluations demonstrate that the unsupervised KalmanNet\nthat does not have access to the noise statistics, approaches\nthe KF with full domain knowledge. Furthermore, its semi-\nsupervised implementation allows to improve upon on its\nsupervised counterpart due to the newly added ability to track\nvariations in the SS model without requiring additional data.\nThe rest of this paper is organized as follows: Section 2\nformulates the SS model and the problem. Section 3 presents\nunsupervised KalmanNet, which is evaluated in Section 4.\n2. SYSTEM MODEL AND PRELIMINARIES\nWe review the SS model and brieﬂy recall the supervised\nKalmanNet. For simplicity, we focus on linear SS models,\nthough the derivations can also be used for non-linear models\narXiv:2110.09005v1  [eess.SP]  18 Oct 2021\nin the same manner as the extended KF is applied [2, Ch. 10],\nas we demonstrate in Section 4.\n2.1. Problem Formulation\nWe consider state estimation in discrete-time, linear, Gaussian\nSS models. Letting xt denote the m × 1 hidden state vector\nat time instance t ∈Z, which evolves in time via\nxt = F · xt−1 + wt,\nwt ∼N (0, Q) ,\nxt ∈Rm.\n(1)\nHere, F is the m × m state evolution matrix, while wt is\nadditive white Gaussian noise (AWGN) with covariance Q.\nThe corresponding observation yt, is related to xt via\nyt = H · xt + vt,\nvt ∼N (0, R) ,\nyt ∈Rn,\n(2)\nwhere H is the n × m measurement matrix, and vt is an\nAWGN with covariance R. We focus on the ﬁltering problem,\nwhere one needs to track the hidden state xt from a known ini-\ntial state x0. At each time instance t, the goal is to provide an\ninstantaneous estimate ˆxt, based on the observations seen so\nfar {yτ}t\nτ=1. We consider scenarios where one has partial do-\nmain knowledge, such that the statistics of the noises wt and\nvt are not known, while the matrices F and H are known.\nTo ﬁll the information gap we assume that we have access to\nan unlabeled data set containing a sequence of observations\nfrom which one has to learn to recover the hidden state.\n2.2. Supervised KalmanNet\nKalmanNet is a hybrid MB/DD implementation of the KF.\nThe latter utilizes full knowledge of the SS model to estimate\nxt, based on the current observed yt and the previous estimate\nˆxt−1. This is achieved by ﬁrst predicting the next state and\nobservation based solely on the previous estimate via\nˆxt|t−1 = F · ˆxt−1,\n(3a)\nˆyt|t−1 = H · ˆxt|t−1 ,\n(3b)\nwhile computing the second-order moments of these esti-\nmates as Σt|t−1\n= F · Σt−1 · F⊤+ Q, and St|t−1\n=\nH · Σt|t−1 · H⊤+ R. Next, the KF computes the Kalman\ngain (KG) Kt as Kt = Σt|t−1 ·H⊤·S−1\nt|t−1 , which is used to\nupdate the estimation covariance Σt = Σt|t−1 −Kt ·St|t−1 ·\nK⊤\nt , and provide the state estimate ˆxt via\nˆxt = ˆxt|t−1 + Kt · ∆yt,\n(4)\nwhere ∆yt is the innovation process computed as\n∆yt = yt −ˆyt|t−1 .\n(5)\nKalmanNet learns to implement the KF from labeled data\nin partially known SS models. This is achieved by noting\nthat the available knowledge allows to compute the predic-\ntions in (3), while the missing domain knowledge is needed\nF\nH\n•\nˆxt|t−1\n-1\n+\nˆyt|t−1\n×\nRecurrent Neural Network\n+\n•\nˆxt\nKt\nKalman Gain\n∆yt\n•\n•ˆxt−1\nZ−1\nZ−1\n+\n∆ˆxt−1\n-1\nyt\n∆yt\nx0\nt = 0\nt > 0\nˆxt\nFig. 1. KalmanNet block diagram.\nto compute the KG. Thus, KalmanNet augments the ﬂow of\nthe KF with an RNN, which estimates the KG and implic-\nitly tracks the second-order moments computed by the KF,\nwhile the state estimate is obtained via (4) (see [16] for a de-\ntailed description). The KalmanNet architecture, depicted in\nFig. 1, is trained to estimate the state in a supervised man-\nner based on labeled data. The data set comprises N pairs of\nhidden state trajectories and its corresponding observations of\nthe form D = {(Yi, Xi)}N\ni=1, where\nYi =\nh\ny(i)\n1 , . . . , y(i)\nTi\ni\n, Xi =\nh\nx(i)\n0 , x(i)\n1 , . . . , x(i)\nTi\ni\n,\n(6)\nand Ti is the length of the ith training trajectory. The training\nprocedure aims at minimizing the regularized ℓ2 loss. Letting\nΘ be the trainable parameters of the RNN and ˆx(i)\nt\n(Θ) be the\noutput of KalmanNet with parameters Θ at time t applied to\nYi, the loss is computed for the ith trajectory as\nli(Θ) = 1\nTi\nTi\nX\nt=1\n\r\r\rˆx(i)\nt\n(Θ)−x(i)\nt\n\r\r\r\n2\n+ γ · ∥Θ∥2 ,\n(7)\nwhere γ > 0 is a regularization coefﬁcient. The loss measure\n(7) is used to optimize Θ via stochastic gradient descent\n(SGD) optimization combined with the backpropagation\nthrough time (BPTT) algorithm [18].\n3. UNSUPERVISED KALMANNET\n3.1. Unsupervised Training Algorithm\nKalmanNet, described in Subsection 2.2, as well as other pre-\nviously proposed DNN-based state estimators such as [14],\nare designed to estimate xt, and thus are trained so that the\noutput approaches the ground-truth hidden state sequence.\nKalmanNet admits an interpretable architecture owing to\nits hybrid MB/DD design, which preserves the ﬂow of the\nKF. We exploit this fact to propose a training algorithm for\nKalmanNet that does not rely on ground-truth labels.\nUnsupervised loss:\nKalmanNet uses its state estimates\nto predict the next observation via (3b) as an internal feature.\nWhile the accuracy of this prediction, e.g., the squared magni-\ntude of the innovation process (5), depends on the accuracy in\nestimating xt and the observation noise, (5) can be computed\nbased solely on the observed sequence. Consequently, one\ncan adapt KalmanNet in an unsupervised manner by training\nit to minimize ∥∆yt∥2. This quantity can be used to compute\nthe gradient with respect to the parameters of the RNN, which\noutputs the learned KG, by the derivative chain rule. Indeed,\n∂∥∆yt∥2\n∂Kt−1\n(a)\n=\n∂\n∂Kt−1\n\r\rH · F · Kt−1 · ∆yt−1 −∆y−\nt\n\r\r2\n= 2 · H⊤· F⊤·\n\u0000Kt−1 · ∆yt−1 −∆y−\nt\n\u0001\n· ∆y⊤\nt−1,\n(8)\nwhere ∆y−\nt ≜yt −H · F · ˆxt−1|t−2 . In (8), (a) holds since\nˆyt|t−1 = H · F · ˆxt−1|t−1\n= H · F ·\n\u0000ˆxt−1|t−2 + Kt−1 · ∆yt−1\n\u0001\n.\n(9)\nThe gradient in (8) indicates that the ℓ2 norm of the in-\nnovation process can be used to learn the computation of the\nKG, which involves the trainable parameters of KalmanNet\nΘ. Similarly to (7), the resulting loss for the ith trajectory is\n˜li(Θ) = 1\nTi\nTi\nX\nt=1\n\r\r\rˆy(i)\nt|t−1 (Θ)−y(i)\nt\n\r\r\r\n2\n+ γ · ∥Θ∥2 .\n(10)\nUnsupervised KalmanNet is thus trained using solely ob-\nserved trajectories based on the loss measure (10) using SGD\nvariants combined with BPTT for gradient computation.\nOfﬂine versus online training:\nThe ability to train\nKalmanNet without providing ground-truth state sequences\ngives rise to two possible training approaches:\na purely\nunsupervised ofﬂine training scheme, and an online semi-\nsupervised strategy.\nThe ofﬂine approach follows conven-\ntional unsupervised learning using unlabeled data of the\nform ˜D = {(Yi)}N\ni=1.\nThis data set is used to optimize\nΘ via mini-batch SGD-based optimization, where for ev-\nery batch indexed by k, we choose M < N trajectories\nindexed by ik\n1, . . . , ik\nM, computing the mini-batch loss as\nLk (Θ) =\n1\nM\nPM\nj=1 ˜lik\nj (Θ).\nOnline training builds upon the ability to learn without\nlabels to adapt a pre-trained KalmanNet to dynamics that\ndiffer from those used during training. Pretraining can be\ndone using labeled data obtained by mathematical modelling\nand/or past measurements without altering the architecture of\nKalmanNet. Then, the deployed model is further adapted in\nan unsupervised manner using observations acquired during\noperation to form training trajectories from realizations of the\ndata. Such a training procedure provides KalmanNet with\nthe ability to be adaptive to changes in the distribution of the\ndata. Speciﬁcally, once every ˜T time steps, we compute the\nloss (10) over the last ˜T observations online and optimize the\nRNN parameters Θ accordingly.\n3.2. Discussion\nThe ability to train KalmanNet in an unsupervised manner\nwithout relying on ground-truth sequences, follows directly\nfrom the hybrid MB/DD architecture of KalmanNet, where\none can identify the observations innovation process as an in-\nternal feature and use it to compute the loss. The training pro-\ncedure does not affect the KalmanNet architecture, and one\ncan use the same supervised model designed in [16]. In the\ncurrent work, we focus on partially-known SS models where\nF and H are available from, e.g., a physical model. While\nsupervised KalmanNet was shown in [16] to operate reliably\nwhen using inaccurate approximations of F and H, we leave\nsuch a study in unsupervised setups for future work.\nThe proposed online semi-supervised technique allows\none to adapt a pre-trained KalmanNet state estimator after\ndeployment, coping with setups in which the original training\nis based on data that does not fully capture the true underlying\nSS model. This gain, numerically demonstrated in Section 4,\nbears some similarity to online training mechanisms proposed\nfor hybrid MB/DD communication receivers in [19–21]. De-\nspite the similarity, the proposed technique, obtained from\nthe interpretable operation of KalmanNet, is fundamentally\ndifferent from that proposed in [19–21], where structures\nin communication data were exploited to generate conﬁdent\nlabels from decisions. Nonetheless, both the current work\nand [19–21] demonstrate the potential of MB deep learning\nin enabling application-oriented, efﬁcient training algorithms.\n4. NUMERICAL EVALUATIONS\nIn this section we numerically1 evaluate unsupervised Kalman-\nNet on a linear SS model and on the non-linear Lorenz attrac-\ntor model, and compare it to the KF and extended KF.\nIn the linear setup, F and H take a canonical form, and\nQ and R are the diagonal matrices q2 · Im and r2 · In, re-\nspectively, while deﬁning ν ≜\nq2\nr2 . In Fig. 2 we compare\nthe performance of unsupervised KalmanNet to the MB KF,\nwhich achieves the minimum mean-squared error (MMSE)\nhere, for 2 × 2 and 5 × 5 SS models and trajectory length\nT = 80. We observe in Fig. 2 that the ofﬂine trained unsuper-\nvised KalmanNet learns to achieve the MMSE lower bound.\nNext, the previously trained model is evaluated on a longer\ntrajectory length of T = 10, 000. The results reported in Ta-\nble 1 show that KalmanNet does not overﬁt to the trajectory\nlength and that the unsupervised training of KalmanNet is not\ntailored to the trajectories presented during training, tuning\nthe ﬁlter with dependency only on the SS model.\nThe results so far indicate that for the considered SS\nmodel, unsupervised training does not degrade the perfor-\nmance of KalmanNet observed for supervised training in [16].\n1The source code used in our numerical study along with the complete set\nof hyper-parameters used in each numerical evaluation can be found online at\nhttps://github.com/KalmanNet/Unsupervised_ICASSP22.\n-10\n-5\n0\n5\n10\n15\n20\n25\n30\n-40\n-35\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\n10\nMSE [dB]\n2.75\n2.8\n2.85\n2.9\n2.95\n3\n3.05\n3.1\n3.15\n3.2\n3.25\n-14\n-12\n-10\n-8\n-6\n-4\n-2\nFig. 2. MSE vs\n1\nr2 for a 2 × 2 and a 5 × 5 linear systems,\nT = 80. An MSE offset is added to prevent overlapping.\n1\nr2 [dB]\n0\n3\n10\n20\n30\nKF MSE [dB]\n−2.31\n−5.31\n−12.3\n−22.3\n−32.3\nKalmanNet MSE [dB]\n−2.27\n−5.26\n−12.3\n−22.3\n−32.3\nTable 1. Generalizing for long trajectories, 2 × 2, ν = 0 [dB].\nTo understand the beneﬁts of supervision, we depict in Fig. 3\nthe MSE convergence of unsupervised KalmanNet compared\nwith its supervised counterpart. We observe in Fig. 3 that\nthe lack of labeled data in unsupervised KalmanNet and the\nfact that it is not explicitly encouraged to minimize the state\nestimation MSE, results in slower convergence to the MMSE\ncompared to supervised KalmanNet.\nNext, we train unsupervised KalmanNet for the non-linear\nSS model of the chaotic Lorenz attractor (see [16] for details).\nIn Fig. 4 we can see that we were able to train KalmanNet for\nthis challenging setup. Although the training test is bounded\nby the observation noise r2 = 0 [dB], the MSE achieved by\nunsupervised KalmanNet is within a minor gap of 0.5 [dB]\nfrom the MSE achieved by the extended KF which has full\nknowledge of the SS model. Furthermore, the DNN-aided\nKalmanNet is observed to require 0.15 seconds to infer for\n0\n10\n20\n30\n40\n50\n60\nTraining Iterations\n-6\n-4\n-2\n0\n2\n4\n6\n8\nMSE Loss [dB]\nTrain supervised\nTrain unsupervised\nCV supervised\nCV unsupervised\nTest supervised\nTest unsupervised\nKF\n20\n22\n24\n26\n28\n30\n-5.4\n-5.2\n-5\n-4.8\n-4.6\n-4.4\n-4.2\n-4\nFig. 3. MSE versus epoch of supervised and unsupervised\nKalmanNet for a 2 × 2 linear SS model, ν = 0 [dB].\nFig. 4. Learning curve of unsupervised KalmanNet, Lorenz\nattractor. r2 = 0 [dB], q2 = 0 [dB], T = 100.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000\n10000\nTime Step t\n16\n16.5\n17\n17.5\n18\n18.5\n19\n19.5\n20\n20.5\nState Prediction MSE [dB]\nOnline Moving Average Loss\nStart online learning\nKalmanNet before online training\nKalmanNet after online training\nKF partial information\nKF full information\nFig. 5. A pre-trained KalmanNet trained in an online unsu-\npervised manner. 2 × 2 linear SS model, q2 = 10 [dB]\neach trajectory, which is quicker compared to the extended\nKF, that involves matrix inversions and requires 0.2 seconds\nper trajectory. This indicates that KalmanNet may be prefer-\nable even when one can estimate the noise statistics.\nFinally, we evaluate the online training mechanism when\nthe testing distribution differs from the SS model from which\nthe training data is generated. We again consider a linear 2×2\nSS model where the true (testing) observation distribution is\ngenerated with r2 = 25 [dB], while the model is pre-trained\non data from an SS model with r2 = 10 [dB]. For online adap-\ntation, we train every incoming ˜T = 10 samples. In Fig. 5 we\ncan see that KalmanNet smoothly adapts to the test distribu-\ntion while training on the observed trajectory over multiple\ntime steps. This shows that the proposed training algorithm\nenables KalmanNet to track variations in the SS model.\n5. CONCLUSIONS\nIn this work we proposed an unsupervised training scheme\nthat enables KalmanNet to learn its mapping without requir-\ning ground-truth sequences.\nThe training scheme exploits\nthe interpretable nature of KalmanNet to formulate an unsu-\npervised loss based on an internal feature that predicts the\nnext observation. Our numerical evaluations demonstrate that\nthe proposed unsupervised training allows KalmanNet to ap-\nproach the MMSE, without access to the noise statistics.\n6. REFERENCES\n[1] R. E. Kalman, “A new approach to linear ﬁltering and\nprediction problems,” Journal of Basic Engineering,\nvol. 82, no. 1, pp. 35–45, 1960.\n[2] J. Durbin and S. J. Koopman, Time series analysis by\nstate space methods.\nOxford University Press, 2012.\n[3] P. Abbeel, A. Coates, M. Montemerlo, A. Y. Ng, and\nS. Thrun, “Discriminative training of Kalman ﬁlters.” in\nRobotics: Science and Systems, vol. 2, 2005, p. 1.\n[4] L. Xu and R. Niu, “EKFNet: Learning system noise\nstatistics from measurement data,” in Proc. IEEE\nICASSP, 2021, pp. 4560–4564.\n[5] L. Zhou, Z. Luo, T. Shen, J. Zhang, M. Zhen, Y. Yao,\nT. Fang, and L. Quan, “KFNet: Learning temporal cam-\nera relocalization using Kalman ﬁltering,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 4919–4928.\n[6] R. G. Krishnan, U. Shalit, and D. Sontag, “Deep Kalman\nﬁlters,” preprint arXiv:1511.05121, 2015.\n[7] M. Karl, M. Soelch, J. Bayer, and P. Van der Smagt,\n“Deep variational Bayes ﬁlters: Unsupervised learn-\ning of state space models from raw data,” preprint\narXiv:1605.06432, 2016.\n[8] M. Fraccaro, S. D. Kamronn, U. Paquet, and O. Winther,\n“A disentangled recognition and nonlinear dynamics\nmodel for unsupervised learning,” in Advances in Neu-\nral Information Processing Systems, 2017.\n[9] C. Naesseth, S. Linderman, R. Ranganath, and D. Blei,\n“Variational sequential Monte Carlo,” in International\nConference on Artiﬁcial Intelligence and Statistics.\nPMLR, 2018, pp. 968–977.\n[10] E. Archer, I. M. Park, L. Buesing, J. Cunningham, and\nL. Paninski, “Black box variational inference for state\nspace models,” arXiv preprint arXiv:1511.07367, 2015.\n[11] R. Krishnan, U. Shalit, and D. Sontag, “Structured in-\nference networks for nonlinear state space models,” in\nProceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, vol. 31, no. 1, 2017.\n[12] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Back-\nprop KF: Learning discriminative deterministic state es-\ntimators,” in Advances in Neural Information Process-\ning Systems, 2016, pp. 4376–4384.\n[13] X. Zheng, M. Zaheer, A. Ahmed, Y. Wang, E. P. Xing,\nand A. J. Smola, “State space LSTM models with\nparticle MCMC inference,” preprint arXiv:1711.11179,\n2017.\n[14] H. Coskun, F. Achilles, R. DiPietro, N. Navab, and\nF. Tombari, “Long short-term memory Kalman ﬁlters:\nRecurrent neural estimators for pose regularization,” in\nProceedings of the IEEE International Conference on\nComputer Vision, 2017, pp. 5524–5532.\n[15] P. Becker, H. Pandya, G. Gebhardt, C. Zhao, C. J. Tay-\nlor, and G. Neumann, “Recurrent Kalman networks:\nFactorized inference in high-dimensional deep feature\nspaces,” in International Conference on Machine Learn-\ning.\nPMLR, 2019, pp. 544–552.\n[16] G. Revach, N. Shlezinger, X. Ni, A. L. Escoriza, R. J.\nvan Sloun, and Y. C. Eldar, “KalmanNet: Neural net-\nwork aided Kalman ﬁltering for partially known dynam-\nics,” arXiv preprint arXiv:2107.10043, 2021.\n[17] N. Shlezinger, J. Whang, Y. C. Eldar, and A. G. Di-\nmakis, “Model-based deep learning,” arXiv preprint\narXiv:2012.08405, 2020.\n[18] P. J. Werbos, “Backpropagation through time: What it\ndoes and how to do it,” Proc. IEEE, vol. 78, no. 10, pp.\n1550–1560, 1990.\n[19] N. Shlezinger, N. Farsad, Y. C. Eldar, and A. J. Gold-\nsmith, “ViterbiNet: A deep learning based Viterbi algo-\nrithm for symbol detection,” IEEE Trans. Wireless Com-\nmun., vol. 19, no. 5, pp. 3319–3331, 2020.\n[20] N. Shlezinger, R. Fu, and Y. C. Eldar, “DeepSIC: Deep\nsoft interference cancellation for multiuser MIMO de-\ntection,” IEEE Trans. Wireless Commun., vol. 20, no. 2,\npp. 1349–1362, 2021.\n[21] C.-F. Teng and Y.-L. Chen, “Syndrome enabled unsuper-\nvised learning for neural network based polar decoder\nand jointly optimized blind equalizer,” IEEE Trans.\nEmerg. Sel. Topics Circuits Syst., 2020.\n",
  "categories": [
    "eess.SP",
    "cs.LG"
  ],
  "published": "2021-10-18",
  "updated": "2021-10-18"
}