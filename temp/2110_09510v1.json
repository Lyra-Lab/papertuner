{
  "id": "http://arxiv.org/abs/2110.09510v1",
  "title": "Unsupervised Finetuning",
  "authors": [
    "Suichan Li",
    "Dongdong Chen",
    "Yinpeng Chen",
    "Lu Yuan",
    "Lei Zhang",
    "Qi Chu",
    "Bin Liu",
    "Nenghai Yu"
  ],
  "abstract": "This paper studies \"unsupervised finetuning\", the symmetrical problem of the\nwell-known \"supervised finetuning\". Given a pretrained model and small-scale\nunlabeled target data, unsupervised finetuning is to adapt the representation\npretrained from the source domain to the target domain so that better transfer\nperformance can be obtained. This problem is more challenging than the\nsupervised counterpart, as the low data density in the small-scale target data\nis not friendly for unsupervised learning, leading to the damage of the\npretrained representation and poor representation in the target domain. In this\npaper, we find the source data is crucial when shifting the finetuning paradigm\nfrom supervise to unsupervise, and propose two simple and effective strategies\nto combine source and target data into unsupervised finetuning: \"sparse source\ndata replaying\", and \"data mixing\". The motivation of the former strategy is to\nadd a small portion of source data back to occupy their pretrained\nrepresentation space and help push the target data to reside in a smaller\ncompact space; and the motivation of the latter strategy is to increase the\ndata density and help learn more compact representation. To demonstrate the\neffectiveness of our proposed ``unsupervised finetuning'' strategy, we conduct\nextensive experiments on multiple different target datasets, which show better\ntransfer performance than the naive strategy.",
  "text": "Unsupervised Finetuning\nSuichan Li1,*, Dongdong Chen2,∗,†, Yinpeng Chen2, Lu Yuan2, Lei Zhang2, Qi Chu1, Bin Liu1, Nenghai Yu1,\n1University of Science and Technology of China\n2Microsoft Cloud & AI\n{lsc1230@mail., qchu@, ﬂowice@, ynh@}ustc.edu.cn, cddlyf@gmail.com,\n{ yiche, luyuan, leizhang}@microsoft.com\nAbstract\nThis paper studies “unsupervised ﬁnetuning”, the sym-\nmetrical problem of the well-known “supervised ﬁnetun-\ning”. Given a pretrained model and small-scale unlabeled\ntarget data, unsupervised ﬁnetuning is to adapt the repre-\nsentation pretrained from the source domain to the target\ndomain so that better transfer performance can be obtained.\nThis problem is more challenging than the supervised coun-\nterpart, as the low data density in the small-scale target\ndata is not friendly for unsupervised learning, leading to\nthe damage of the pretrained representation and poor rep-\nresentation in the target domain. In this paper, we ﬁnd the\nsource data is crucial when shifting the ﬁnetuning paradigm\nfrom supervise to unsupervise, and propose two simple and\neffective strategies to combine source and target data into\nunsupervised ﬁnetuning: “sparse source data replaying”,\nand “data mixing”. The motivation of the former strategy\nis to add a small portion of source data back to occupy their\npretrained representation space and help push the target\ndata to reside in a smaller compact space; and the moti-\nvation of the latter strategy is to increase the data density\nand help learn more compact representation. To demon-\nstrate the effectiveness of our proposed “unsupervised ﬁne-\ntuning” strategy, we conduct extensive experiments on mul-\ntiple different target datasets, which show better transfer\nperformance than the naive strategy.\n1. Introduction\nIn recent years, visual recognition has achieved tremen-\ndous success [30, 19, 31] from the development of deep\nneural networks and large-scale labeled data. However, in\nsome real applications, acquiring such large-scale data is\nso difﬁcult or even possible, let alone the intensive annota-\ntion. Therefore, ﬁnetuning from pretraining [20, 4, 13] has\nbecome the dominant training paradigm for such applica-\ntion scenarios, i.e., ﬁrst pretrain the model on the large-scale\n*Equal contribution, † Dongdong Chen is the corresponding author\nsource data, then ﬁnetune the pretrained model with few la-\nbeled target data, which we call “supervised ﬁnetuning”.\nIn this paper, we study “unsupervised ﬁnetuning”, which\nshares the similar goal as supervised ﬁnetuning that tries to\nadapt the representation pretrained from the source domain\nto the target domain, so that better downstream task perfor-\nmance can be achieved, e.g., retrieval, unsupervised clus-\ntering, and few-shot transfer via supervised ﬁnetuning with\nfew labeled samples. To the best of our knowledge, “un-\nsupervised ﬁnetuning” is not well studied in the computer\nvision ﬁeld and there is almost no prior work that studies\ngeneric unsupervised ﬁnetuning, even though unsupervised\npretraining is actively studied as an extremely hot topic in\nrecent two years.\nAt the ﬁrst glance, implementing unsupervised ﬁnetun-\ning is very intuitive, i.e., continue unsupervised learning\non the unlabeled target data from the pretrained represen-\ntation, in a similar way as supervised ﬁnetuning. However,\nthis naive way does not work that well, because existing\nunsupervised learning schemes (e.g., contrastive learning)\noften require large-scale data to work properly [26]. Oth-\nerwise, they will destroy the original pretrained representa-\ntion structure and struggle in learning a compact represen-\ntation in the target domain. In the following section, we will\nanalyze the mostly used unsupervised contrastive loss, and\nhypothesize that this may be pre-determined by its own for-\nmulation. Inspired by the analysis, we discover two simple\nand effective strategies: sparse source data replaying and\ndata mixing.\nSpeciﬁcally, for data replaying, we add some source data\nback, and involve both these source data and the unlabeled\ntarget data into the contrastive learning process. The main\nmotivation here is using such source data to occupy their\npretrained representation space and help push the target data\nto reside in a smaller compact space. However, to ensure\nthe efﬁciency of unsupervised ﬁnetuning, only a small por-\ntion (e.g., 10%) of source data is involved. To further in-\ncrease the data density and help contrastive learning on a\nsmall data scale, data mixing is used to create extra positive\nsamples. In details, given two random images, we create\narXiv:2110.09510v1  [cs.CV]  18 Oct 2021\na mixed image and regard both the original two images as\npositive samples weighted by the mixing weights. It can\nhelp pull similar images closer and learn more compact rep-\nresentation.\nTo demonstrate the effectiveness of our proposed “un-\nsupervised ﬁnetuning” strategy, we conduct extensive ex-\nperiments on multiple small-scale target datasets with both\nunsupervised pretrained models and supervised pretrained\nmodels. We also evaluate the unsupervised ﬁnetuned model\non different transfer tasks to evaluate its transfer perfor-\nmance, including unsupervised image retrieval, unsuper-\nvised clustering, and few-shot transfer. Experimental re-\nsults show that the proposed unsupervised ﬁnetuning strat-\negy can achieve much better performance than the naive un-\nsupervised ﬁnetuning strategy and the original pretrained\nrepresentation.\nTo summarize, our contributions are in three aspects:\n• To the best of our knowledge, we are the ﬁrst that\nsystematically analyzed the “unsupervised ﬁnetuning”\nproblem by dissecting the widely used contrastive loss.\n• Based on the analysis, we discover two simple and\neffective strategies, i.e., sparse source data replaying\nand data mixing, based on which better unsupervised\nﬁnetuning can be achieved.\n• Extensive experiments are conducted and demonstrate\nthe superiority and generalization ability of our un-\nsupervised ﬁnetuning strategy across different target\ndatasets, pretraining methods and transfer tasks.\n2. Related Works\nSupervised pretraining.\nBeneﬁting from the powerful\nlearning capacity, deep neural networks revolutionized\nmany computer vision tasks in recent years, such as image\nrecognition [19, 30], object detection [27, 16, 3], and se-\nmantic segmentation [6, 23]. However, such deep networks\noften require a large-scale labeled dataset for training, e.g.,\nImageNet[10] and JFT-300M [29]. In order to achieve great\nperformance on small datasets, model pretraining [38, 37]\nhas demonstrated to be an effective way, and most previ-\nous success comes from supervised pretraining. For exam-\nple, the ImageNet pretrained models are widely used in a\nbroad range of downstream tasks [27, 33, 14] and can sig-\nniﬁcantly boost the downstream performance. In the recent\nwork BiT [20], Alexander et al. demonstrate pretraining big\nmodels on big supervised data is extremely powerful and\ncan achieve excellent transfer performance on downstream\nfew-shot tasks.\nUnsupervised Pretraining.\nDespite the great success of\nsupervised pretraining, it is relatively difﬁcult to scale up\nbecause annotating a large scale labeled data is extremely\ncostly and time-consuming. Over the past few years, un-\nsupervised or self-supervised pretraining has attracted more\nand more attentions. Early unsupervised pretraining meth-\nods often design some pretext tasks as the learning sig-\nnal. Examples include recovering the input under corrup-\ntions, e.g., denoise auto-encoders[32], cross-channel auto-\nencoders [36]. Other popular pre-tasks include rotation pre-\ndiction [15] and patch ordering [11]. Recently, contrastive\nlearning [34, 18, 7] has almost dominated this ﬁeld. The\nunderlying motivation is the InfoMax principle [24], which\naims to maximize the mutual information across different\naugmentations of the same images. For detailed implemen-\ntation, it encourages the representation of different augmen-\ntations of the same image to be similar and that of different\nimages to be dissimilar. Many recent unsupervised pretrain-\ning methods [18, 17, 5] have shown unsupervised pretrain-\ning on ImageNet can achieve comparable transfer perfor-\nmance on downstream datasets to the supervised pretraining\ncounterpart. But since unsupervised learning starts tabula\nrasa, it still requires extremely large amounts of unlabeled\ndata to learn a good representation, which is also illustrated\nin the work [22, 26].\nNevertheless, in many application scenarios, it is dif-\nﬁcult or even impossible to collect a large-scale dataset,\nsuch as ﬁnegrained classiﬁcation and some other medical\nrelated tasks, especially when the semantic category num-\nber is small. Inspired by it, we study “unsupervised ﬁnetun-\ning”, which aims to relax this requirement and studies how\nto ﬁnetune the pretrained model with the small-scale target\ndata in an unsupervised way. Our method is orthogonal to\nboth supervised and unsupervised pretraining works. On\nthe one hand, our unsupervised ﬁnetuning can work upon\nboth supervised and unsupervised pretrained models. On\nthe other hand, our unsupervised ﬁnetuning is essentially\ncontrastive learning but with two new strategies, so integrat-\ning better contrastive learning methods can also improve\nour unsupervised ﬁnetuning method.\nTarget Domain Transfer.\nGiven the pretrained model,\nthe most widely used transfer strategy is supervised ﬁne-\ntuning, i.e., collect some labeled samples in the target do-\nmain and ﬁnetune the pretrained model in a supervised way\nwith a proper (often smaller) learning rate. In this paper, we\nstudy how to ﬁnetune the pretrained model with small-scale\nunlabeled target data, which is useful for many real applica-\ntion scenarios. One typical example is that, in some large-\nscale retrieval systems, there is only unlabeled target data.\nAnother typical example is to provide better representation\nto conduct unsupervised clustering. But due to the lack of\nlabel supervision, unsupervised ﬁnetuning is not a trivial\ntask like supervised ﬁnetuning. Using a small number of\nunlabeled target data, naively unsupervised learning from\nthe pretrained model is not satisfactory. Because it will eas-\nily destroy the original representation structure and degrade\nto a trivial solution. In the experiment part, to demonstrate\nthe better transfer performance from unsupervised ﬁnetuned\nrepresentation, we will use several typical transfer strate-\ngies in the target domain for evaluation, including few-shot\ntransfer, unsupervised retrieval and clustering.\n3. Motivation\nGiven a small scale of unlabeled target data T\n=\n{xt\n1, ..., xt\nn}, there are two naive ways to learn represen-\ntation from these data. One is directly conducting unsu-\npervised learning on this data from scratch, and another\nis to conduct unsupervised learning on this data from the\npretrained representation that is trained on other source do-\nmain. To implement the ﬁrst solution, we adopt the existing\nstate-of-the-art unsupervised learning method MoCoV2[9]\non two example target datasets:\nCaltech101 [12] and\nPet [25], which contain a total of 3680, 3060 images for\ntraining respectively. To evaluate the quality of the learned\nrepresentation, we ﬁnetune the pretrained model with a few\nlabeled samples to get the few-shot transfer performance.\nAs the reference, we also show the few-shot transfer perfor-\nmance of the unsupervised pretrained model from the large-\nscale source dataset, i.e., ImageNet.\nAs shown in the second and third row of Table 1, we can\nobserve that conducting unsupervised learning directly on\nthe small-scale target dataset cannot learn a good represen-\ntation, and its few-shot transfer performance is much worse\nthan the unsupervised pretrained model on the large-scale\nsource dataset (ImageNet). But from another perspective,\nit demonstrates the value of the second baseline, i.e., ﬁne-\ntuning upon the pretrained representation from the source\ndomain. However, how to conduct unsupervised ﬁnetun-\ning is still unknown. As the simple baseline, we follow\na similar strategy as the well studied “supervised ﬁnetun-\ning”. In other words, we continue unsupervised learning\non the target data with a smaller learning rate by using the\npretrained model as the initialization. The corresponding\ntransfer performance is shown in the last row of Table 1. It\ncan be seen that, on Caltech101, the transfer performance\nis slightly improved, but on Pet37, the transfer performance\nbecomes even worse. It means unsupervised ﬁnetuning is\nnot as trivial as simple ﬁnetuning.\nTo further understand why unsupervised ﬁnetuning is\nnontrivial, we follow the analysis in the work [8] about\nthe contrastive loss, which represents the generalized con-\ntrastive loss in the below form:\nLcontr = Lalign + λLdistr\n(1)\nwhere Lalign is the alignment term that encourages the\nlearned representation of different augmented views of the\nsame image to be as close as possible. And Ldistr is the dis-\ntribution term that encourages the learned representation to\nModel\nPet37\nCaltech101\n2-Shot 4-Shot 8-Shot 2-Shot 4-Shot 8-Shot\nUnsup-T\n9.08\n10.97\n12.64\n18.69\n27.17\n38.27\nUnsup-S\n56.84\n68.94\n75.10\n63.12\n76.54\n84.45\nUnsupft-T\n55.94\n67.03\n72.92\n68.48\n78.18\n82.03\nTable 1: Few-shot transfer performance on two small-scale\ntarget datasets with different baseline models.\nUnsup-T\nand Unsup-S mean the unsupervised pretrained model on\nthe small target dataset T and the large source dataset Im-\nageNet S from scratch respectively. Unsup-T means the\nnaive unsupervised ﬁnetuned model on T from the pre-\ntrained Unsup-S.\nmatch a prior distribution of high entropy. Following the\ndeﬁnition in [7], the standard contrastive loss is deﬁned as:\nLcontr = −1\nN\nX\ni,j∈MB\nlog\nexp(zi · zj/τ)\nP2N\nk=1 1[k̸=i] exp(zi · zk/τ)\n(2)\nwhere zi, zj are the representations of the two augmented\nviews of the same example. sim(u, v) is the cosine sim-\nilarity between u and v, and τ is the temperature hyper-\nparameter. MB is the mini-batch that consists of random\naugmented pairs of images and N is the mini-batch size. In\nthe detailed implementation, all the learned representation\nzi will be normalized before passing into the contrastive\nloss. The above equation can be further rewritten into:\nLcontr = −1\nNτ\nX\ni,j\nsim(zi, zj)+\n1\nN\nX\ni\nlog\n2N\nX\nk=1\n1[k̸=i] exp(sim(zi, zk)/τ)\n(3)\nThe ﬁrst and second term correspond to the alignment term\nLalign and distribution term in Equation (1) respectively.\nMore interestingly, the second term is related to pairwise\npotential in a Gaussian kernel and can be minimized with a\nuniform encoder. It will encourage the learned representa-\ntion to be uniformly distributed on the hypersphere. Con-\nsidering zi often has a high dimension for larger represen-\ntation capacity, if the training data has a limited scale, the\ndata density is not enough to learn a compact representation\nbecause of the Ldistr term. In this case, contrastive learning\nindeed degrades to a trivial solution, i.e., the learned target\nrepresentation scatters in the whole space sparsely. This ex-\nplains why the ﬁrst naive way does not work very well.\nConversely, if the training data has a very large scale and\ndata density, different samples will squeeze each other, thus\npushing the similar samples to be closer and learned repre-\nsentation more compact. Therefore, the ImageNet unsuper-\nvised pretrained model has a relatively good transfer per-\nformance. As for the naive simple unsupervised ﬁnetuning\n(a) Unsup-𝑇\n(b) Unsup-𝑆\n(c) Naive Unsupft-𝑇\n(d) Unsupft-𝑇with SR\nFigure 1: The feature distribution visualization of the Pet dataset with different models via TSNE. (a) is the unsupervised\npretrained model only on the small-scale Pet dataset from scratch, (b) is the unsupervised pretrained model on the large-scale\nImageNet dataset, (c) is the naive unsupervised ﬁnetuned model on the Pet dataset by initializing with model (b), and (d) is\nthe unsupervised ﬁnetuned model with our source data replaying.\nbaseline, since the source data is not involved in the ﬁne-\ntuning stage, there is much freedom for the sparse target\ndata to wander in the whole space. This will not only eas-\nily destroy the original space structure but also struggle in\nlearning a more compact representation.\nTrying to verify the above analysis, we further visual-\nize the feature distribution on the Pet dataset with different\npretrained models via TSNE in Figure 1. The visualization\nresults roughly align with our above analysis and the few-\nshot transfer performance shown in the Table 1. In details,\nunsupervised learning directly on the small-scale target data\nalmost learns a uniformly distributed representation, and the\nnaive unsupervised ﬁnetuning strategy does not improve the\npretrained representation a lot either. Accompanying the\nvisualization, we also evaluate the feature clustering qual-\nity on the source ImageNet dataset and target Pet dataset.\nBefore and after the unsupervised ﬁnetuning, the cluster-\ning accuracy on ImageNet and Pet are 36.85%, 47.72% and\n16.60%, 47.31% respectively. The signiﬁcant clustering ac-\ncuracy drop on the ImageNet also indicates the naive unsu-\npervised ﬁnetuning will destroy the pretrained representa-\ntion structure on the source dataset.\n4. Method\nBased on the above analysis, we discover two simple and\neffective strategies: sparse source data replaying and data\nmixing. We ﬁnd both of them can signiﬁcantly improve\nunsupervised ﬁnetuning for better representation learning\nby adding more push force from the Ldistr term and more\npull force from the Lalign term respectively. In this paper,\nwe implement unsupervised ﬁnetuning on top of the state-\nof-the-art unsupervised learning framework MoCOV2 [9].\nBefore introducing the above two strategies, we will ﬁrst\nbrieﬂy introduce MoCoV2 for better understanding.\nIn MoCoV2, for each image within the mini-batch, two\ndifferent augmentations will be conducted. One augmented\nsample is regarded as query q and the other is regarded as\nthe positive key k+. All augmented versions of other im-\nages are regarded as negative keys {k−\ni }. Then for each\nimage, the contrastive loss is deﬁned as:\nLq = −log\nexp(zq · z+\nk /τ)\nexp(zq · z+\nk /τ) + PK−1\ni=0 exp(zq · z−\nki/τ)\n,\n(4)\nwhere zq, zk are the L2-normalized feature embedding by\nfeeding q, k into two separated encoders: a query encoder\nand a key encoder. The query encoder and key encoder\nshare the same architecture, i.e., the backbone of the target\nnetwork followed by an extra MLP-based projection head,\nbut have different weights. The key idea of MoCo is making\nthe weights of the key encoder be the momentum moving\naverage of query encoder.\nSparse Source Data Replaying.\nTo alleviate the sparse\ntarget data density issue and avoid destroying the original\npretrained representation, we propose to add some source\ndata back and mix them with the target data together for the\nunsupervised ﬁnetuning. The main motivation is from two\naspects:\n• These source data can help occupy their original fea-\nture space, which leaves a smaller remaining space for\nthe target data to reside and avoids their original repre-\nsentation being destroyed.\n• The training data density is increased, thus more push\nforce from the source data will be imposed onto the\ntarget data via the distribution term Ldistr.\nHowever, since the source data has a very large scale, in-\nvolving all of them into the unsupervised ﬁnetuning pro-\ncess will signiﬁcantly hurt the efﬁciency. Fortunately, as\nthe source data is often well clustered in the pretraining\nrepresentation space, we ﬁnd that only randomly sampling\na small portion (10% by default) of source data as the\nrepresentative samples is good enough, especially when\nequipped with the following the data mixing strategy. Thus\nwe call this strategy “sparse source data replaying”.\nAs\nshown in Figure 1 (d), with this strategy, a more compact\ntarget representation can be learned.\nThe corresponding\nclustering accuracy on the ImageNet and Pet dataset also\nincreases to 28.14%, 56.11% respectively, indicating better\npreserving of the source representation and more compact\ntarget representation.\nContrastive learning with Data Mixing\nTo further in-\ncrease the data density, we propose to use data mixing\nto create extra positive samples, which link different im-\nages with more alignment terms. In our method, we use\ncutmix[35] as the default instantiation of data mixing, and\nwe empirically observed that other mixing strategies like\nMixUP can also work well. In details, given two images,\nwe denote one augmented version of them as the querys qa,\nqb, and another augmented version as the positive keys k+\na ,\nk+\nb , then we create a mixed query sample qm:\nqm ←M ⊙qa + (1 −M) ⊙qb\n(5)\nwhen M ∈[0, 1]W ×H is a binary mixing weight mask, 1 is\na binary mask ﬁlled with ones. ⊙means the element-wise\nmultiplication operation and W, H is the width and height\nof training images. Then the mixed query qm is regarded as\nthe positive sample both for k+\na and k+\nb , thus the contrastive\nloss with respect to qm is changed to:\nLqm = −\n\u0010\nλlog\nexp(zqm · z+\nka/τ)\nexp(zqm · z+\nka/τ) + PK−1\ni=0 exp(zqm · z−\nki/τ)\n+(1 −λ)log\nexp(zqm · z+\nkb/τ)\nexp(zqm · z+\nkb/τ) + PK−1\ni=0 exp(zqm · z−\nki/τ)\n\u0011\n(6)\nwhere λ is the combination ratio between two images and\ndeﬁned as the area ratio of qa in M. During training, λ is\nsampled from the beta distribution Beta(α, α), and α = 1\nby default. At each training iteration, we randomly selected\ntwo training samples from the mini-batch and generate the\nmixed query with a sampled λ.\nTo understand why data mixing can help, we can also\nrewrite the Equation (6) into the format of Equation (1) in a\nsimilar way. Then we will get two alignment terms bridged\nby the common query image qm. If qa and qb are from the\nsource and target domain respectively, the alignment terms\nwill help pull their similar images closer and help encourage\nthe learned representation more compact.\n5. Experiments\nIn this section, we conduct extensive experiments and\nablations to demonstrate the effectiveness of unsupervised\nﬁnetuning in improving the transfer performance. We lever-\nage ImageNet as the large-scale source data for pretrain-\ning and multiple public small datasets as target domain\nfor unsupervised ﬁnetuning and transfer, including Cal-\ntech101 [12], Pet [25], CIFAR100 [21], and Food101 [2].\nFor these target datasets, we use their original data splits,\nwhich have a total of 3060, 3680, 50000, 75750 images for\ntraining respectively. Among them, Food101 (ﬁne-grained\nfood dataset) has a relatively large domain gap with Ima-\ngeNet. To reduce the inﬂuence from randomness, each ex-\nperiment is repeated three trials and reports the averaged\nresult. For comparison, we follow the standard evaluation\nsetting and report the mean class accuracy for Pet, Cal-\ntech101 and top-1 accuracy for other datasets. Regarding\nthe pretrained model for unsupervised ﬁnetuning, we tried\nboth supervised pretraining and unsupervised pretraining to\ndemonstrate the generalization ability.\nEvaluation Metrics.\nTo evaluate the representation qual-\nity, we consider three different evaluation metrics: few-shot\ntransfer, retrieval accuracy, and clustering accuracy. The\nﬁrst metric requires label samples while the latter two met-\nrics are purely unsupervised metrics. More speciﬁcally, for\nfew-shot transfer, we randomly choose K labeled samples\nfor each category and ﬁnetune the learned representation\nwith such labeled samples in a supervised way. For retrieval\naccuracy, we regard each image in the test set as the query,\nand retrieve the top-5 similar images from the training set\nby using the learned representation. Then the ratio of im-\nages that have the same label as the query image is deﬁned\nas the retrieval accuracy. For clustering accuracy, we di-\nrectly run K-Means clustering on the training set and adopt\nthe BCubed Precision [1] as the clustering accuracy. Due to\nthe space limit, we adopt the few-shot transfer performance\nas the main evaluation metric. In the ablation study , we\nwill further try to apply our unsupervised ﬁnetuned repre-\nsentation to help semi-supervised learning.\nImplementation details.\nFor unsupervised pretraining,\nwe use MocoV2[9] and follow its training protocol. The\ninitial learning rate is 0.24 with a cosine scheduler and the\nbatch size is 2,048. All the pretraining models are trained\nwith 800 epochs. For supervised pretraining, we directly\nuse the pretrained models from [20]. For unsupervised ﬁne-\ntuning, the initial learning rate is 0.015 with batch size 256.\nAll the models are ﬁnetuned with 200 epochs with a cosine\nlearning rate scheduler. For transfer, we ﬁnetune the model\nfor 30 epochs and the learning rate for the newly added FC\nlayer and pretrained layers are 3.0 and 0.0001 respectively,\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\n1-Shot Acc\nUnsup-S\n45.49\n41.23\n13.65\n10.09\n27.62\n+ Naive Unsupft-T\n55.65 (+10.16)\n40.01 (−1.22)\n20.83 (+7.18)\n28.36 (+18.27)\n36.21 (+8.59)\n+ Our Unsupft-T\n74.33 (+28.84)\n60.35 (+19.12)\n33.75 (+20.10)\n48.89 (+38.80)\n54.33 (+26.71)\n2-Shot Acc\nUnsup-S\n63.12\n56.84\n22.95\n15.80\n39.68\n+ Naive Unsupft-T\n68.48 (+5.36)\n55.94 (−0.9)\n30.15 (+7.20)\n39.64 (+23.84)\n48.55 (+8.87)\n+ Our Unsupft-T\n81.02 (+17.88)\n70.29 (+13.45)\n44.51 (+21.56)\n61.99 (+46.19)\n64.45 (+24.77)\n4-Shot Acc\nUnsup-S\n76.54\n68.94\n35.08\n25.49\n51.51\n+ Naive Unsupft-T\n78.18 (+1.64)\n67.03 (−1.91)\n39.70 (+4.62)\n50.82 (+25.33)\n58.93 (+7.42)\n+ Our Unsupft-T\n85.35 (+8.81)\n76.89 (+7.95)\n52.60 (+17.52)\n71.14 (+45.65)\n71.50 (+19.99)\n8-Shot Acc\nUnsup-S\n84.45\n75.10\n47.84\n35.50\n60.72\n+ Naive Unsupft-T\n82.03 (−2.42)\n72.92 (−2.18)\n48.59 (+0.75)\n59.58 (+24.08)\n65.78 (+5.06)\n+ Our Unsupft-T\n87.24 (+2.79)\n79.88 (+4.78)\n59.68 (+9.14)\n76.86 (+41.36)\n75.92 (+15.20)\nRetrievalAcc\nUnsup-S\n83.14\n69.70\n53.25\n35.91\n60.50\n+ Naive Unsupft-T\n84.50 (+1.36)\n63.73 (−5.97)\n64.27 (+11.02)\n61.26 (+25.35)\n68.44 (+7.94)\n+ Our Unsupft-T\n85.95 (+2.81)\n71.67 (+1.97)\n68.83 (+15.58)\n75.13 (+39.22)\n75.40 (+14.90)\nClusterAcc\nUnsup-S\n56.16\n47.72\n23.91\n11.85\n34.91\n+ Naive Unsupft-T\n61.33 (+5.17)\n47.31 (−0.41)\n41.72 (+17.81)\n34.18 (+22.33)\n46.14 (+11.23)\n+ Our Unsupft-T\n69.93 (+13.77)\n56.11 (+8.39)\n44.92 (+21.01)\n51.74 (+39.89)\n55.68 (+20.77)\nTable 2: Transfer performance comparison on different target datasets. “Unsup-S” denote the few-shot results by using ImageNet unsu-\npervised pretrained model respectively, “Naive Unsupft-T, and “Our Unsup-T” denote the few-shot results by using the naive unsupervised\nﬁnetuned model and our proposed unsupervised ﬁnetuned model from “Unsup-S”. It shows our unsupervised ﬁnetuning strategy can learn\nbetter representation upon the pretrained models and outperforms the naive baseline strategy.\nboth for few-shot transfer and semi-supervised transfer. The\ndefault backbone is ResNet-50 [19].\n5.1. Main Results\nIn our main experiments, we use the ImageNet unsuper-\nvised pretrained model as the initialization representation,\nand apply the naive unsupervised ﬁnetuning strategy and\nour proposed unsupervised ﬁnetuning strategy on top of it,\nrespectively. To compare the representation quality, few-\nshot transfer accuracy, retrieval accuracy and unsupervised\nclustering accuracy are all evaluated. The detailed compar-\nison results are shown in the Table 2. It can be seen that,\nby unsupervised ﬁnetuning the pretrained representation on\nthe small-scale unlabeled target data, it can signiﬁcantly im-\nprove the transfer performance in the target domain. And\ncompared to the naive unsupervised ﬁnetuning strategy, our\nproposed unsupervised ﬁnetuning strategy consistently out-\nperforms the original pretrained model and brings much\nlarger performance gain, while the naive strategy achieves\nlower transfer performance than the pretrained model on the\nPet dataset. By carefully studying the above results, we can\ndraw more interesting conclusions:\n• In general, the fewer the labeled samples, the bigger\nperformance gain unsupervised ﬁnetuning will bring.\nThis is because ﬁnetuning with fewer labeled samples\nis much easier to overﬁt, thus having a higher require-\nment of the initialization representation.\n• If the target domain has more unlabeled data (e.g.,\nFood101), unsupervised ﬁnetuning will bring better\nperformance and even the naive unsupervised ﬁnetun-\ning strategy can get very plausible results.\n• When the domain gap between the source dataset and\ntarget dataset is small, the performance gain from un-\nsupervised ﬁnetuning is relatively smaller. For exam-\nple, in the ImageNet dataset, there are a lot of animal\nimages (especially “dog”), which potentially have a lot\nof overlapped categories with the Pet dataset. There-\nfore, on the Pet dataset, the pretrained representation\nalready has very good transfer performance.\n5.2. Ablation Study\nIn this section, we conduct a series of ablation studies to\ndemonstrate the generalization ability of unsupervised ﬁne-\ntuning and the importance of our discovered strategies.\nGeneralization Ability to Supervised Pretrained Mod-\nels.\nBesides applying to the default unsupervised pre-\ntrained models, our proposed unsupervised ﬁnetuning strat-\negy is also general to supervised pretrained models.\nIn\nthis experiment, we use the supervised pretrained ResNet50\nmodel from BiT [20] as the instantiation and apply unsu-\npervised ﬁnetuning in a similar way. The few-shot trans-\nfer performance before and after unsupervised ﬁnetuning\nK\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\n1-Shot Acc\nSup-S\n45.78\n50.04\n25.73\n16.48\n38.39\n+ Our Unsupft-T\n73.35 (+27.57)\n61.30 (+11.26)\n32.29 (+6.56)\n42.60 (+26.12)\n56.48 (+18.09)\n2-Shot Acc\nSup-S\n61.07\n63.95\n37.40\n24.99\n51.14\n+ Our Unsupft-T\n79.12 (+18.05)\n73.59 (+9.64)\n45.93 (+8.53)\n61.70 (+36.71)\n68.38 (+17.24)\n4-Shot Acc\nSup-S\n73.89\n76.08\n47.79\n35.26\n62.41\n+ Our Unsupft-T\n84.42 (+10.53)\n84.41 (+8.33)\n55.09 (+7.30)\n67.33 (+32.07)\n75.89 (+13.48)\n8-Shot Acc\nSup-S\n83.87\n82.87\n57.46\n45.60\n70.90\n+ Our Unsupft-T\n88.08 (+4.21)\n85.61 (+3.74)\n60.54 (+3.08)\n69.54 (+23.94)\n79.12 (+8.22)\nTable 3: Few-shot transfer performance on different target datasets when applying unsupervised ﬁnetuning upon the supervised pretrained\nmodels, which demonstrates the generalization ability of unsupervised ﬁnetuning. Here, “Sup-S” denote the few-shot results by using the\noriginal ImageNet supervised pretrained model, “Our Unsup-T” denote the few-shot results by our proposed unsupervised ﬁnetuned model\nfrom “Sup-S”.\nK\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\n1-Shot Acc\nSup-S\n53.28\n50.67\n26.76\n17.05\n36.94\n+ Our Unsupft-T\n74.03 (+20.75)\n64.02 (+13.35)\n34.58 (+7.82)\n42.99 (+25.94)\n53.91 (+16.97)\n2-Shot Acc\nSup-S\n66.32\n65.96\n37.20\n26.20\n48.92\n+ Our Unsupft-T\n83.59 (+17.27)\n79.37 (+13.41)\n50.20 (+13.00)\n64.33 (+38.13)\n69.37 (+20.45)\n4-Shot Acc\nSup-S\n78.08\n79.50\n49.20\n36.38\n60.79\n+ Our Unsupft-T\n87.63 (+9.55)\n86.15 (+6.65)\n58.32 (+9.12)\n70.02 (+33.64)\n75.53 (+14.74)\n8-Shot Acc\nSup-S\n86.23\n82.69\n59.36\n45.84\n68.53\n+ Our Unsupft-T\n89.65 (+3.42)\n86.32 (+3.63)\n63.97 (+4.61)\n71.95 (+26.11)\n77.97 (+9.44)\nTable 4: Few-shot transfer performance on the large supervised pretrained model ResNet101. The annotations are the same as Table 3,\nand the only difference is the larger pretrained model. It demonstrates that our unsupervised ﬁnetuning is also general to large pretrained\nmodels and brings better transfer performance.\nis shown in the Table 3. It can be seen that, our unsuper-\nvised ﬁnetuning can also boost the transfer performance of\nsupervised pretrained models. Compared to the few-shot\ntransfer performance shown in the Table 2, we can further\nobserve: 1) the original supervised pretrained model has\nbetter few-shot transfer performance than the original un-\nsupervised pretrained model. This is because supervised\npretrained models are often trained with the cross entropy\nloss by using supervised labels, and there is not a simi-\nlar distribution term in the cross entropy loss to encourage\nuniformity. Therefore, supervised pretrained representation\nis more compact; 2) by equipping with our unsupervised\nﬁnetuning, the performance gap between unsupervised and\nsupervised pretrained model becomes much smaller. Note\nthat, we also try the naive unsupervised ﬁnetuning strategy\nupon the supervised pretrained model, but ﬁnd its transfer\nperformance is very bad, so we do not provide its results in\nthe table.\nGeneralization Ability to Large Models.\nBesides the\npretraining method, our unsupevised ﬁnetuning strategy\nalso generalizes well to large models. In the Table 4, we\ntake the supervised pretrained ResNet101 model as an ex-\nample and evaluate its transfer performance. It shows that,\neven though the large model itself can produce better trans-\nfer performance than the default ResNet50 model, it can\nstill improve its performance in a similar way to the small\nmodel by using the unsupervised ﬁnenetuned model as ini-\ntialization.\nApplication in Semi-supervised Learning.\nThe essence\nof unsupervised ﬁnetuning is utilizing the unlabeled tar-\nget data to get a better representation. Similarly, in semi-\nsupervised learning, it also aims to leverage the unlabeled\ntarget data as the extra learning guidance to achieve bet-\nter performance. In this experiment, we are curious about\nwhether semi-supervised learning can still beneﬁt from un-\nsupervised ﬁnetuned representation under the same data set-\nting. To answer this question, we leverage the state-of-the-\nart semi-supervised learning method FixMatch [28], and\nuse the original unsupervised pretrained representation and\nunsupervised ﬁnetuned representation as the initialization\nrespectively.\nSurprisingly, as shown in the Table 5, semi-supervised\nlearning also beneﬁts from better initialization models from\nunsupervised ﬁnetuning, but the overall performance gain is\nsmaller. In our understanding, many semi-supervised meth-\nods including FixMatch use the pseudo labels of the un-\nK\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\nSemi-2 Acc\nUnsup-S\n79.04\n54.52\n41.43\n18.23\n48.31\n+ Our Unsupft-T\n82.84 (+3.80)\n68.58 (+14.06)\n56.35 (+14.92)\n70.33 (+52.10)\n69.53 (+21.22)\nSemi-4 Acc\nUnsup-S\n86.07\n68.18\n61.54\n34.67\n62.61\n+ Our Unsupft-T\n86.87 (+0.8)\n73.25 (+5.07)\n66.18 (+4.64)\n79.35 (+44.68)\n76.41 (+13.80)\nSemi-8 Acc\nUnsup-S\n90.12\n75.39\n70.59\n59.06\n73.79\n+ Our Unsupft-T\n90.27 (+0.15)\n79.18 (+3.79)\n71.65 (+1.06)\n82.35 (+23.29)\n80.86 (+7.07)\nTable 5: Semi-supervised learning results by using different models as initialization. “Semi-K” means K labeled samples together with\nthe whole unlabeled data are used for semi-supervised learning. Other annotations are similar as Table 2. It shows that, even though\nsemi-supervised learning also leverage the unlabeled data, our unsupervised ﬁnetuned model can provide better initialization for it and\nboost the ﬁnal performance.\nDataset\nUnsupft SDR Mix K = 2 K = 4 K = 8\nCaltech101\n–\n63.12\n76.54\n84.45\n\u0013\n68.48\n78.18\n82.03\n\u0013\n\u0013\n75.28\n82.99\n85.15\n\u0013\n\u0013\n\u0013\n81.02\n85.35\n87.24\nPet\n–\n56.84\n68.94\n75.10\n\u0013\n55.94\n67.03\n72.92\n\u0013\n\u0013\n62.81\n71.35\n75.14\n\u0013\n\u0013\n\u0013\n70.29\n76.89\n79.88\nCIFAR100\n–\n22.95\n35.08\n47.84\n\u0013\n30.15\n39.70\n48.59\n\u0013\n\u0013\n35.60\n45.01\n52.62\n\u0013\n\u0013\n\u0013\n44.51\n52.60\n59.68\nFood101\n–\n15.84\n25.49\n35.50\n\u0013\n39.64\n50.82\n59.58\n\u0013\n\u0013\n38.87\n48.97\n56.85\n\u0013\n\u0013\n\u0013\n61.99\n71.14\n76.86\nTable 6: Ablation study of the two key strategies: sparse source\ndata replaying (SDR) and data mixing (Mix). “–” means base-\nline without unsupervised ﬁnetuning, “\u0013” means having the cor-\nresponding strategy. The second row of each dataset only with\n“Unsupft” checked denotes the naived unsupervised ﬁnetuning. It\nshows that both source data replaying and data mixing can help\nlearn better representations.\nlabeled data as the extra learning constraints, so providing\nbetter initialization will help generate more precise pseudo\nlabels, subsequently improving the ﬁnal performance. Con-\nversely, if the initialization representation is not good, the\npseudo labels will contain more errors and the gain from the\nunlabeled data will be very marginal. For example, com-\npared to the few-shot transfer, when only 2 labeled sam-\nples exist for each category on the Food101 dataset, semi-\nsupervised learning initialized from the ImageNet unsuper-\nvised pretrained model can only bring about 2% perfor-\nmance gain. By contrast, by using our unsupervised ﬁne-\ntuned model as initialization, the extra performance gain\nfrom semi-supervised learning is 8.3%.\nImportance of Source Data Replaying and Data Mixing.\nSparse source data replaying and data mixing are the two\nkeys in our unsupervised ﬁnetuning strategy. To quantita-\ntively demonstrate their importance, we conduct the abla-\ntion studies by gradually removing each of them. As shown\nin the Table 6, on all the datasets except Food101, source\ndata replaying can boost the transfer performance by a large\nmargin. For example, on the Caltech101 and Pet dataset,\nusing source data replaying will boost the 2-shot transfer\nperformance by 6.8%, 6.9% respectively. The reason why\nsource data replaying cannot help Food101 should be be-\ncause Food101 already has a larger number of images for\neach category, i.e., 750 images per category. As for data\nmixing, it consistently brings better performance on all the\ndatasets.\nEspecially on the Food101 dataset, the 2-shot\ntransfer performance is signiﬁcantly boosted by 23.12%.\n6. Conclusion\nUnsupervised learning has achieved great progress in re-\ncent years and become one of the most active and hot re-\nsearch topic. However, almost all existing works focus on\nthe large-scale unsupervised pretraining, i.e., conduct un-\nsupervised learning on a large-scale dataset. But in many\nreal application scenarios, collecting such a large-scale un-\nlabeled dataset is difﬁcult or impossible. In this paper, we\nstudy “unsupervised ﬁnetuning”, which aims to relax this\nrequirement by unsupervised learning on a small-scale un-\nlabel dataset on top of the pretrained representation. To\nthe best of our knowledge, there is no prior work that fo-\ncuses on this problem in the computer vision ﬁeld yet. Ac-\ncording to our analysis, we ﬁnd unsupervised learning on\nthe small-scale unlabeled data either from scratch or from\nthe pretrained representation does not work well. After an-\nalyzing the widely used contrastive loss, we propose two\nsimple and effective strategies, “sparse source data replay-\ning” and “data mixing”. Through extensive experiments, we\ndemonstrate the proposed unsupervised ﬁnetuning strategy\ncan achieve better performance than the naive unsupervised\nﬁnetuning strategy. We hope our study will inspire more\nresearch in this direction.\nReferences\n[1] Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa\nVerdejo.\nA comparison of extrinsic clustering evaluation\nmetrics based on formal constraints. Information retrieval,\n12(4):461–486, 2009. 5\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101–mining discriminative components with random\nforests. In European conference on computer vision, pages\n446–461. Springer, 2014. 5\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020.\n2\n[4] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features\non non-curated data. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2959–2968,\n2019. 1\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. 2020.\n2\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834–848, 2017. 2\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 2, 3\n[8] Ting Chen and Lala Li. Intriguing properties of contrastive\nlosses. arXiv preprint arXiv:2011.02803, 2020. 3\n[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 3, 4, 5\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 2\n[11] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 1422–1430, 2015. 2\n[12] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\native visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories. In\n2004 conference on computer vision and pattern recognition\nworkshop, pages 178–178. IEEE, 2004. 3, 5\n[13] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\nYuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsuper-\nvised pre-training for person re-identiﬁcation. arXiv preprint\narXiv:2012.03753, 2020. 1\n[14] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\nJianmin Bao, Gang Hua, and Houqiang Li. Improving person\nre-identiﬁcation with iterative impression aggregation. IEEE\nTransactions on Image Processing, 2020. 2\n[15] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. arXiv preprint arXiv:1803.07728, 2018. 2\n[16] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1440–1448,\n2015. 2\n[17] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning.\narXiv preprint\narXiv:2006.07733, 2020. 2\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729–9738, 2020. 2\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 1, 2, 6\n[20] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 2, 5, 6\n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 5\n[22] Suichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei\nZhang, Qi Chu, Bin Liu, and Nenghai Yu. Improve unsu-\npervised pretraining for few-label transfer. In ICCV, 2021.\n2\n[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3431–3440, 2015. 2\n[24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 2\n[25] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on\ncomputer vision and pattern recognition, pages 3498–3505.\nIEEE, 2012. 3, 5\n[26] Cheng Perng Phoo and Bharath Hariharan.\nSelf-training\nfor few-shot transfer across extreme task differences. arXiv\npreprint arXiv:2010.07734, 2020. 1, 2\n[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. arXiv preprint arXiv:1506.01497, 2015.\n2\n[28] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao\nZhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han\nZhang, and Colin Raffel.\nFixmatch: Simplifying semi-\nsupervised learning with consistency and conﬁdence. arXiv\npreprint arXiv:2001.07685, 2020. 7\n[29] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international\nconference on computer vision, pages 843–852, 2017. 2\n[30] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1–9, 2015.\n1, 2\n[31] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105–6114. PMLR,\n2019. 1\n[32] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In Proceedings of the\n25th international conference on Machine learning, pages\n1096–1103, 2008. 2\n[33] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. Learning discriminative features with multiple gran-\nularities for person re-identiﬁcation. In Proceedings of the\n26th ACM international conference on Multimedia, pages\n274–282, 2018. 2\n[34] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3733–\n3742, 2018. 2\n[35] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiﬁers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023–6032, 2019. 5\n[36] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain\nautoencoders: Unsupervised learning by cross-channel pre-\ndiction. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1058–1067, 2017. 2\n[37] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson Corso, and Jianfeng Gao. Uniﬁed vision-language pre-\ntraining for image captioning and vqa. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, volume 34,\npages 13041–13049, 2020. 2\n[38] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\nLiu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training\nand self-training. arXiv preprint arXiv:2006.06882, 2020. 2\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2021-10-18",
  "updated": "2021-10-18"
}