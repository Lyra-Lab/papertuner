{
  "id": "http://arxiv.org/abs/2110.09510v1",
  "title": "Unsupervised Finetuning",
  "authors": [
    "Suichan Li",
    "Dongdong Chen",
    "Yinpeng Chen",
    "Lu Yuan",
    "Lei Zhang",
    "Qi Chu",
    "Bin Liu",
    "Nenghai Yu"
  ],
  "abstract": "This paper studies \"unsupervised finetuning\", the symmetrical problem of the\nwell-known \"supervised finetuning\". Given a pretrained model and small-scale\nunlabeled target data, unsupervised finetuning is to adapt the representation\npretrained from the source domain to the target domain so that better transfer\nperformance can be obtained. This problem is more challenging than the\nsupervised counterpart, as the low data density in the small-scale target data\nis not friendly for unsupervised learning, leading to the damage of the\npretrained representation and poor representation in the target domain. In this\npaper, we find the source data is crucial when shifting the finetuning paradigm\nfrom supervise to unsupervise, and propose two simple and effective strategies\nto combine source and target data into unsupervised finetuning: \"sparse source\ndata replaying\", and \"data mixing\". The motivation of the former strategy is to\nadd a small portion of source data back to occupy their pretrained\nrepresentation space and help push the target data to reside in a smaller\ncompact space; and the motivation of the latter strategy is to increase the\ndata density and help learn more compact representation. To demonstrate the\neffectiveness of our proposed ``unsupervised finetuning'' strategy, we conduct\nextensive experiments on multiple different target datasets, which show better\ntransfer performance than the naive strategy.",
  "text": "Unsupervised Finetuning\nSuichan Li1,*, Dongdong Chen2,‚àó,‚Ä†, Yinpeng Chen2, Lu Yuan2, Lei Zhang2, Qi Chu1, Bin Liu1, Nenghai Yu1,\n1University of Science and Technology of China\n2Microsoft Cloud & AI\n{lsc1230@mail., qchu@, Ô¨Çowice@, ynh@}ustc.edu.cn, cddlyf@gmail.com,\n{ yiche, luyuan, leizhang}@microsoft.com\nAbstract\nThis paper studies ‚Äúunsupervised Ô¨Ånetuning‚Äù, the sym-\nmetrical problem of the well-known ‚Äúsupervised Ô¨Ånetun-\ning‚Äù. Given a pretrained model and small-scale unlabeled\ntarget data, unsupervised Ô¨Ånetuning is to adapt the repre-\nsentation pretrained from the source domain to the target\ndomain so that better transfer performance can be obtained.\nThis problem is more challenging than the supervised coun-\nterpart, as the low data density in the small-scale target\ndata is not friendly for unsupervised learning, leading to\nthe damage of the pretrained representation and poor rep-\nresentation in the target domain. In this paper, we Ô¨Ånd the\nsource data is crucial when shifting the Ô¨Ånetuning paradigm\nfrom supervise to unsupervise, and propose two simple and\neffective strategies to combine source and target data into\nunsupervised Ô¨Ånetuning: ‚Äúsparse source data replaying‚Äù,\nand ‚Äúdata mixing‚Äù. The motivation of the former strategy\nis to add a small portion of source data back to occupy their\npretrained representation space and help push the target\ndata to reside in a smaller compact space; and the moti-\nvation of the latter strategy is to increase the data density\nand help learn more compact representation. To demon-\nstrate the effectiveness of our proposed ‚Äúunsupervised Ô¨Åne-\ntuning‚Äù strategy, we conduct extensive experiments on mul-\ntiple different target datasets, which show better transfer\nperformance than the naive strategy.\n1. Introduction\nIn recent years, visual recognition has achieved tremen-\ndous success [30, 19, 31] from the development of deep\nneural networks and large-scale labeled data. However, in\nsome real applications, acquiring such large-scale data is\nso difÔ¨Åcult or even possible, let alone the intensive annota-\ntion. Therefore, Ô¨Ånetuning from pretraining [20, 4, 13] has\nbecome the dominant training paradigm for such applica-\ntion scenarios, i.e., Ô¨Årst pretrain the model on the large-scale\n*Equal contribution, ‚Ä† Dongdong Chen is the corresponding author\nsource data, then Ô¨Ånetune the pretrained model with few la-\nbeled target data, which we call ‚Äúsupervised Ô¨Ånetuning‚Äù.\nIn this paper, we study ‚Äúunsupervised Ô¨Ånetuning‚Äù, which\nshares the similar goal as supervised Ô¨Ånetuning that tries to\nadapt the representation pretrained from the source domain\nto the target domain, so that better downstream task perfor-\nmance can be achieved, e.g., retrieval, unsupervised clus-\ntering, and few-shot transfer via supervised Ô¨Ånetuning with\nfew labeled samples. To the best of our knowledge, ‚Äúun-\nsupervised Ô¨Ånetuning‚Äù is not well studied in the computer\nvision Ô¨Åeld and there is almost no prior work that studies\ngeneric unsupervised Ô¨Ånetuning, even though unsupervised\npretraining is actively studied as an extremely hot topic in\nrecent two years.\nAt the Ô¨Årst glance, implementing unsupervised Ô¨Ånetun-\ning is very intuitive, i.e., continue unsupervised learning\non the unlabeled target data from the pretrained represen-\ntation, in a similar way as supervised Ô¨Ånetuning. However,\nthis naive way does not work that well, because existing\nunsupervised learning schemes (e.g., contrastive learning)\noften require large-scale data to work properly [26]. Oth-\nerwise, they will destroy the original pretrained representa-\ntion structure and struggle in learning a compact represen-\ntation in the target domain. In the following section, we will\nanalyze the mostly used unsupervised contrastive loss, and\nhypothesize that this may be pre-determined by its own for-\nmulation. Inspired by the analysis, we discover two simple\nand effective strategies: sparse source data replaying and\ndata mixing.\nSpeciÔ¨Åcally, for data replaying, we add some source data\nback, and involve both these source data and the unlabeled\ntarget data into the contrastive learning process. The main\nmotivation here is using such source data to occupy their\npretrained representation space and help push the target data\nto reside in a smaller compact space. However, to ensure\nthe efÔ¨Åciency of unsupervised Ô¨Ånetuning, only a small por-\ntion (e.g., 10%) of source data is involved. To further in-\ncrease the data density and help contrastive learning on a\nsmall data scale, data mixing is used to create extra positive\nsamples. In details, given two random images, we create\narXiv:2110.09510v1  [cs.CV]  18 Oct 2021\na mixed image and regard both the original two images as\npositive samples weighted by the mixing weights. It can\nhelp pull similar images closer and learn more compact rep-\nresentation.\nTo demonstrate the effectiveness of our proposed ‚Äúun-\nsupervised Ô¨Ånetuning‚Äù strategy, we conduct extensive ex-\nperiments on multiple small-scale target datasets with both\nunsupervised pretrained models and supervised pretrained\nmodels. We also evaluate the unsupervised Ô¨Ånetuned model\non different transfer tasks to evaluate its transfer perfor-\nmance, including unsupervised image retrieval, unsuper-\nvised clustering, and few-shot transfer. Experimental re-\nsults show that the proposed unsupervised Ô¨Ånetuning strat-\negy can achieve much better performance than the naive un-\nsupervised Ô¨Ånetuning strategy and the original pretrained\nrepresentation.\nTo summarize, our contributions are in three aspects:\n‚Ä¢ To the best of our knowledge, we are the Ô¨Årst that\nsystematically analyzed the ‚Äúunsupervised Ô¨Ånetuning‚Äù\nproblem by dissecting the widely used contrastive loss.\n‚Ä¢ Based on the analysis, we discover two simple and\neffective strategies, i.e., sparse source data replaying\nand data mixing, based on which better unsupervised\nÔ¨Ånetuning can be achieved.\n‚Ä¢ Extensive experiments are conducted and demonstrate\nthe superiority and generalization ability of our un-\nsupervised Ô¨Ånetuning strategy across different target\ndatasets, pretraining methods and transfer tasks.\n2. Related Works\nSupervised pretraining.\nBeneÔ¨Åting from the powerful\nlearning capacity, deep neural networks revolutionized\nmany computer vision tasks in recent years, such as image\nrecognition [19, 30], object detection [27, 16, 3], and se-\nmantic segmentation [6, 23]. However, such deep networks\noften require a large-scale labeled dataset for training, e.g.,\nImageNet[10] and JFT-300M [29]. In order to achieve great\nperformance on small datasets, model pretraining [38, 37]\nhas demonstrated to be an effective way, and most previ-\nous success comes from supervised pretraining. For exam-\nple, the ImageNet pretrained models are widely used in a\nbroad range of downstream tasks [27, 33, 14] and can sig-\nniÔ¨Åcantly boost the downstream performance. In the recent\nwork BiT [20], Alexander et al. demonstrate pretraining big\nmodels on big supervised data is extremely powerful and\ncan achieve excellent transfer performance on downstream\nfew-shot tasks.\nUnsupervised Pretraining.\nDespite the great success of\nsupervised pretraining, it is relatively difÔ¨Åcult to scale up\nbecause annotating a large scale labeled data is extremely\ncostly and time-consuming. Over the past few years, un-\nsupervised or self-supervised pretraining has attracted more\nand more attentions. Early unsupervised pretraining meth-\nods often design some pretext tasks as the learning sig-\nnal. Examples include recovering the input under corrup-\ntions, e.g., denoise auto-encoders[32], cross-channel auto-\nencoders [36]. Other popular pre-tasks include rotation pre-\ndiction [15] and patch ordering [11]. Recently, contrastive\nlearning [34, 18, 7] has almost dominated this Ô¨Åeld. The\nunderlying motivation is the InfoMax principle [24], which\naims to maximize the mutual information across different\naugmentations of the same images. For detailed implemen-\ntation, it encourages the representation of different augmen-\ntations of the same image to be similar and that of different\nimages to be dissimilar. Many recent unsupervised pretrain-\ning methods [18, 17, 5] have shown unsupervised pretrain-\ning on ImageNet can achieve comparable transfer perfor-\nmance on downstream datasets to the supervised pretraining\ncounterpart. But since unsupervised learning starts tabula\nrasa, it still requires extremely large amounts of unlabeled\ndata to learn a good representation, which is also illustrated\nin the work [22, 26].\nNevertheless, in many application scenarios, it is dif-\nÔ¨Åcult or even impossible to collect a large-scale dataset,\nsuch as Ô¨Ånegrained classiÔ¨Åcation and some other medical\nrelated tasks, especially when the semantic category num-\nber is small. Inspired by it, we study ‚Äúunsupervised Ô¨Ånetun-\ning‚Äù, which aims to relax this requirement and studies how\nto Ô¨Ånetune the pretrained model with the small-scale target\ndata in an unsupervised way. Our method is orthogonal to\nboth supervised and unsupervised pretraining works. On\nthe one hand, our unsupervised Ô¨Ånetuning can work upon\nboth supervised and unsupervised pretrained models. On\nthe other hand, our unsupervised Ô¨Ånetuning is essentially\ncontrastive learning but with two new strategies, so integrat-\ning better contrastive learning methods can also improve\nour unsupervised Ô¨Ånetuning method.\nTarget Domain Transfer.\nGiven the pretrained model,\nthe most widely used transfer strategy is supervised Ô¨Åne-\ntuning, i.e., collect some labeled samples in the target do-\nmain and Ô¨Ånetune the pretrained model in a supervised way\nwith a proper (often smaller) learning rate. In this paper, we\nstudy how to Ô¨Ånetune the pretrained model with small-scale\nunlabeled target data, which is useful for many real applica-\ntion scenarios. One typical example is that, in some large-\nscale retrieval systems, there is only unlabeled target data.\nAnother typical example is to provide better representation\nto conduct unsupervised clustering. But due to the lack of\nlabel supervision, unsupervised Ô¨Ånetuning is not a trivial\ntask like supervised Ô¨Ånetuning. Using a small number of\nunlabeled target data, naively unsupervised learning from\nthe pretrained model is not satisfactory. Because it will eas-\nily destroy the original representation structure and degrade\nto a trivial solution. In the experiment part, to demonstrate\nthe better transfer performance from unsupervised Ô¨Ånetuned\nrepresentation, we will use several typical transfer strate-\ngies in the target domain for evaluation, including few-shot\ntransfer, unsupervised retrieval and clustering.\n3. Motivation\nGiven a small scale of unlabeled target data T\n=\n{xt\n1, ..., xt\nn}, there are two naive ways to learn represen-\ntation from these data. One is directly conducting unsu-\npervised learning on this data from scratch, and another\nis to conduct unsupervised learning on this data from the\npretrained representation that is trained on other source do-\nmain. To implement the Ô¨Årst solution, we adopt the existing\nstate-of-the-art unsupervised learning method MoCoV2[9]\non two example target datasets:\nCaltech101 [12] and\nPet [25], which contain a total of 3680, 3060 images for\ntraining respectively. To evaluate the quality of the learned\nrepresentation, we Ô¨Ånetune the pretrained model with a few\nlabeled samples to get the few-shot transfer performance.\nAs the reference, we also show the few-shot transfer perfor-\nmance of the unsupervised pretrained model from the large-\nscale source dataset, i.e., ImageNet.\nAs shown in the second and third row of Table 1, we can\nobserve that conducting unsupervised learning directly on\nthe small-scale target dataset cannot learn a good represen-\ntation, and its few-shot transfer performance is much worse\nthan the unsupervised pretrained model on the large-scale\nsource dataset (ImageNet). But from another perspective,\nit demonstrates the value of the second baseline, i.e., Ô¨Åne-\ntuning upon the pretrained representation from the source\ndomain. However, how to conduct unsupervised Ô¨Ånetun-\ning is still unknown. As the simple baseline, we follow\na similar strategy as the well studied ‚Äúsupervised Ô¨Ånetun-\ning‚Äù. In other words, we continue unsupervised learning\non the target data with a smaller learning rate by using the\npretrained model as the initialization. The corresponding\ntransfer performance is shown in the last row of Table 1. It\ncan be seen that, on Caltech101, the transfer performance\nis slightly improved, but on Pet37, the transfer performance\nbecomes even worse. It means unsupervised Ô¨Ånetuning is\nnot as trivial as simple Ô¨Ånetuning.\nTo further understand why unsupervised Ô¨Ånetuning is\nnontrivial, we follow the analysis in the work [8] about\nthe contrastive loss, which represents the generalized con-\ntrastive loss in the below form:\nLcontr = Lalign + ŒªLdistr\n(1)\nwhere Lalign is the alignment term that encourages the\nlearned representation of different augmented views of the\nsame image to be as close as possible. And Ldistr is the dis-\ntribution term that encourages the learned representation to\nModel\nPet37\nCaltech101\n2-Shot 4-Shot 8-Shot 2-Shot 4-Shot 8-Shot\nUnsup-T\n9.08\n10.97\n12.64\n18.69\n27.17\n38.27\nUnsup-S\n56.84\n68.94\n75.10\n63.12\n76.54\n84.45\nUnsupft-T\n55.94\n67.03\n72.92\n68.48\n78.18\n82.03\nTable 1: Few-shot transfer performance on two small-scale\ntarget datasets with different baseline models.\nUnsup-T\nand Unsup-S mean the unsupervised pretrained model on\nthe small target dataset T and the large source dataset Im-\nageNet S from scratch respectively. Unsup-T means the\nnaive unsupervised Ô¨Ånetuned model on T from the pre-\ntrained Unsup-S.\nmatch a prior distribution of high entropy. Following the\ndeÔ¨Ånition in [7], the standard contrastive loss is deÔ¨Åned as:\nLcontr = ‚àí1\nN\nX\ni,j‚ààMB\nlog\nexp(zi ¬∑ zj/œÑ)\nP2N\nk=1 1[kÃ∏=i] exp(zi ¬∑ zk/œÑ)\n(2)\nwhere zi, zj are the representations of the two augmented\nviews of the same example. sim(u, v) is the cosine sim-\nilarity between u and v, and œÑ is the temperature hyper-\nparameter. MB is the mini-batch that consists of random\naugmented pairs of images and N is the mini-batch size. In\nthe detailed implementation, all the learned representation\nzi will be normalized before passing into the contrastive\nloss. The above equation can be further rewritten into:\nLcontr = ‚àí1\nNœÑ\nX\ni,j\nsim(zi, zj)+\n1\nN\nX\ni\nlog\n2N\nX\nk=1\n1[kÃ∏=i] exp(sim(zi, zk)/œÑ)\n(3)\nThe Ô¨Årst and second term correspond to the alignment term\nLalign and distribution term in Equation (1) respectively.\nMore interestingly, the second term is related to pairwise\npotential in a Gaussian kernel and can be minimized with a\nuniform encoder. It will encourage the learned representa-\ntion to be uniformly distributed on the hypersphere. Con-\nsidering zi often has a high dimension for larger represen-\ntation capacity, if the training data has a limited scale, the\ndata density is not enough to learn a compact representation\nbecause of the Ldistr term. In this case, contrastive learning\nindeed degrades to a trivial solution, i.e., the learned target\nrepresentation scatters in the whole space sparsely. This ex-\nplains why the Ô¨Årst naive way does not work very well.\nConversely, if the training data has a very large scale and\ndata density, different samples will squeeze each other, thus\npushing the similar samples to be closer and learned repre-\nsentation more compact. Therefore, the ImageNet unsuper-\nvised pretrained model has a relatively good transfer per-\nformance. As for the naive simple unsupervised Ô¨Ånetuning\n(a) Unsup-ùëá\n(b) Unsup-ùëÜ\n(c) Naive Unsupft-ùëá\n(d) Unsupft-ùëáwith SR\nFigure 1: The feature distribution visualization of the Pet dataset with different models via TSNE. (a) is the unsupervised\npretrained model only on the small-scale Pet dataset from scratch, (b) is the unsupervised pretrained model on the large-scale\nImageNet dataset, (c) is the naive unsupervised Ô¨Ånetuned model on the Pet dataset by initializing with model (b), and (d) is\nthe unsupervised Ô¨Ånetuned model with our source data replaying.\nbaseline, since the source data is not involved in the Ô¨Åne-\ntuning stage, there is much freedom for the sparse target\ndata to wander in the whole space. This will not only eas-\nily destroy the original space structure but also struggle in\nlearning a more compact representation.\nTrying to verify the above analysis, we further visual-\nize the feature distribution on the Pet dataset with different\npretrained models via TSNE in Figure 1. The visualization\nresults roughly align with our above analysis and the few-\nshot transfer performance shown in the Table 1. In details,\nunsupervised learning directly on the small-scale target data\nalmost learns a uniformly distributed representation, and the\nnaive unsupervised Ô¨Ånetuning strategy does not improve the\npretrained representation a lot either. Accompanying the\nvisualization, we also evaluate the feature clustering qual-\nity on the source ImageNet dataset and target Pet dataset.\nBefore and after the unsupervised Ô¨Ånetuning, the cluster-\ning accuracy on ImageNet and Pet are 36.85%, 47.72% and\n16.60%, 47.31% respectively. The signiÔ¨Åcant clustering ac-\ncuracy drop on the ImageNet also indicates the naive unsu-\npervised Ô¨Ånetuning will destroy the pretrained representa-\ntion structure on the source dataset.\n4. Method\nBased on the above analysis, we discover two simple and\neffective strategies: sparse source data replaying and data\nmixing. We Ô¨Ånd both of them can signiÔ¨Åcantly improve\nunsupervised Ô¨Ånetuning for better representation learning\nby adding more push force from the Ldistr term and more\npull force from the Lalign term respectively. In this paper,\nwe implement unsupervised Ô¨Ånetuning on top of the state-\nof-the-art unsupervised learning framework MoCOV2 [9].\nBefore introducing the above two strategies, we will Ô¨Årst\nbrieÔ¨Çy introduce MoCoV2 for better understanding.\nIn MoCoV2, for each image within the mini-batch, two\ndifferent augmentations will be conducted. One augmented\nsample is regarded as query q and the other is regarded as\nthe positive key k+. All augmented versions of other im-\nages are regarded as negative keys {k‚àí\ni }. Then for each\nimage, the contrastive loss is deÔ¨Åned as:\nLq = ‚àílog\nexp(zq ¬∑ z+\nk /œÑ)\nexp(zq ¬∑ z+\nk /œÑ) + PK‚àí1\ni=0 exp(zq ¬∑ z‚àí\nki/œÑ)\n,\n(4)\nwhere zq, zk are the L2-normalized feature embedding by\nfeeding q, k into two separated encoders: a query encoder\nand a key encoder. The query encoder and key encoder\nshare the same architecture, i.e., the backbone of the target\nnetwork followed by an extra MLP-based projection head,\nbut have different weights. The key idea of MoCo is making\nthe weights of the key encoder be the momentum moving\naverage of query encoder.\nSparse Source Data Replaying.\nTo alleviate the sparse\ntarget data density issue and avoid destroying the original\npretrained representation, we propose to add some source\ndata back and mix them with the target data together for the\nunsupervised Ô¨Ånetuning. The main motivation is from two\naspects:\n‚Ä¢ These source data can help occupy their original fea-\nture space, which leaves a smaller remaining space for\nthe target data to reside and avoids their original repre-\nsentation being destroyed.\n‚Ä¢ The training data density is increased, thus more push\nforce from the source data will be imposed onto the\ntarget data via the distribution term Ldistr.\nHowever, since the source data has a very large scale, in-\nvolving all of them into the unsupervised Ô¨Ånetuning pro-\ncess will signiÔ¨Åcantly hurt the efÔ¨Åciency. Fortunately, as\nthe source data is often well clustered in the pretraining\nrepresentation space, we Ô¨Ånd that only randomly sampling\na small portion (10% by default) of source data as the\nrepresentative samples is good enough, especially when\nequipped with the following the data mixing strategy. Thus\nwe call this strategy ‚Äúsparse source data replaying‚Äù.\nAs\nshown in Figure 1 (d), with this strategy, a more compact\ntarget representation can be learned.\nThe corresponding\nclustering accuracy on the ImageNet and Pet dataset also\nincreases to 28.14%, 56.11% respectively, indicating better\npreserving of the source representation and more compact\ntarget representation.\nContrastive learning with Data Mixing\nTo further in-\ncrease the data density, we propose to use data mixing\nto create extra positive samples, which link different im-\nages with more alignment terms. In our method, we use\ncutmix[35] as the default instantiation of data mixing, and\nwe empirically observed that other mixing strategies like\nMixUP can also work well. In details, given two images,\nwe denote one augmented version of them as the querys qa,\nqb, and another augmented version as the positive keys k+\na ,\nk+\nb , then we create a mixed query sample qm:\nqm ‚ÜêM ‚äôqa + (1 ‚àíM) ‚äôqb\n(5)\nwhen M ‚àà[0, 1]W √óH is a binary mixing weight mask, 1 is\na binary mask Ô¨Ålled with ones. ‚äômeans the element-wise\nmultiplication operation and W, H is the width and height\nof training images. Then the mixed query qm is regarded as\nthe positive sample both for k+\na and k+\nb , thus the contrastive\nloss with respect to qm is changed to:\nLqm = ‚àí\n\u0010\nŒªlog\nexp(zqm ¬∑ z+\nka/œÑ)\nexp(zqm ¬∑ z+\nka/œÑ) + PK‚àí1\ni=0 exp(zqm ¬∑ z‚àí\nki/œÑ)\n+(1 ‚àíŒª)log\nexp(zqm ¬∑ z+\nkb/œÑ)\nexp(zqm ¬∑ z+\nkb/œÑ) + PK‚àí1\ni=0 exp(zqm ¬∑ z‚àí\nki/œÑ)\n\u0011\n(6)\nwhere Œª is the combination ratio between two images and\ndeÔ¨Åned as the area ratio of qa in M. During training, Œª is\nsampled from the beta distribution Beta(Œ±, Œ±), and Œ± = 1\nby default. At each training iteration, we randomly selected\ntwo training samples from the mini-batch and generate the\nmixed query with a sampled Œª.\nTo understand why data mixing can help, we can also\nrewrite the Equation (6) into the format of Equation (1) in a\nsimilar way. Then we will get two alignment terms bridged\nby the common query image qm. If qa and qb are from the\nsource and target domain respectively, the alignment terms\nwill help pull their similar images closer and help encourage\nthe learned representation more compact.\n5. Experiments\nIn this section, we conduct extensive experiments and\nablations to demonstrate the effectiveness of unsupervised\nÔ¨Ånetuning in improving the transfer performance. We lever-\nage ImageNet as the large-scale source data for pretrain-\ning and multiple public small datasets as target domain\nfor unsupervised Ô¨Ånetuning and transfer, including Cal-\ntech101 [12], Pet [25], CIFAR100 [21], and Food101 [2].\nFor these target datasets, we use their original data splits,\nwhich have a total of 3060, 3680, 50000, 75750 images for\ntraining respectively. Among them, Food101 (Ô¨Åne-grained\nfood dataset) has a relatively large domain gap with Ima-\ngeNet. To reduce the inÔ¨Çuence from randomness, each ex-\nperiment is repeated three trials and reports the averaged\nresult. For comparison, we follow the standard evaluation\nsetting and report the mean class accuracy for Pet, Cal-\ntech101 and top-1 accuracy for other datasets. Regarding\nthe pretrained model for unsupervised Ô¨Ånetuning, we tried\nboth supervised pretraining and unsupervised pretraining to\ndemonstrate the generalization ability.\nEvaluation Metrics.\nTo evaluate the representation qual-\nity, we consider three different evaluation metrics: few-shot\ntransfer, retrieval accuracy, and clustering accuracy. The\nÔ¨Årst metric requires label samples while the latter two met-\nrics are purely unsupervised metrics. More speciÔ¨Åcally, for\nfew-shot transfer, we randomly choose K labeled samples\nfor each category and Ô¨Ånetune the learned representation\nwith such labeled samples in a supervised way. For retrieval\naccuracy, we regard each image in the test set as the query,\nand retrieve the top-5 similar images from the training set\nby using the learned representation. Then the ratio of im-\nages that have the same label as the query image is deÔ¨Åned\nas the retrieval accuracy. For clustering accuracy, we di-\nrectly run K-Means clustering on the training set and adopt\nthe BCubed Precision [1] as the clustering accuracy. Due to\nthe space limit, we adopt the few-shot transfer performance\nas the main evaluation metric. In the ablation study , we\nwill further try to apply our unsupervised Ô¨Ånetuned repre-\nsentation to help semi-supervised learning.\nImplementation details.\nFor unsupervised pretraining,\nwe use MocoV2[9] and follow its training protocol. The\ninitial learning rate is 0.24 with a cosine scheduler and the\nbatch size is 2,048. All the pretraining models are trained\nwith 800 epochs. For supervised pretraining, we directly\nuse the pretrained models from [20]. For unsupervised Ô¨Åne-\ntuning, the initial learning rate is 0.015 with batch size 256.\nAll the models are Ô¨Ånetuned with 200 epochs with a cosine\nlearning rate scheduler. For transfer, we Ô¨Ånetune the model\nfor 30 epochs and the learning rate for the newly added FC\nlayer and pretrained layers are 3.0 and 0.0001 respectively,\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\n1-Shot Acc\nUnsup-S\n45.49\n41.23\n13.65\n10.09\n27.62\n+ Naive Unsupft-T\n55.65 (+10.16)\n40.01 (‚àí1.22)\n20.83 (+7.18)\n28.36 (+18.27)\n36.21 (+8.59)\n+ Our Unsupft-T\n74.33 (+28.84)\n60.35 (+19.12)\n33.75 (+20.10)\n48.89 (+38.80)\n54.33 (+26.71)\n2-Shot Acc\nUnsup-S\n63.12\n56.84\n22.95\n15.80\n39.68\n+ Naive Unsupft-T\n68.48 (+5.36)\n55.94 (‚àí0.9)\n30.15 (+7.20)\n39.64 (+23.84)\n48.55 (+8.87)\n+ Our Unsupft-T\n81.02 (+17.88)\n70.29 (+13.45)\n44.51 (+21.56)\n61.99 (+46.19)\n64.45 (+24.77)\n4-Shot Acc\nUnsup-S\n76.54\n68.94\n35.08\n25.49\n51.51\n+ Naive Unsupft-T\n78.18 (+1.64)\n67.03 (‚àí1.91)\n39.70 (+4.62)\n50.82 (+25.33)\n58.93 (+7.42)\n+ Our Unsupft-T\n85.35 (+8.81)\n76.89 (+7.95)\n52.60 (+17.52)\n71.14 (+45.65)\n71.50 (+19.99)\n8-Shot Acc\nUnsup-S\n84.45\n75.10\n47.84\n35.50\n60.72\n+ Naive Unsupft-T\n82.03 (‚àí2.42)\n72.92 (‚àí2.18)\n48.59 (+0.75)\n59.58 (+24.08)\n65.78 (+5.06)\n+ Our Unsupft-T\n87.24 (+2.79)\n79.88 (+4.78)\n59.68 (+9.14)\n76.86 (+41.36)\n75.92 (+15.20)\nRetrievalAcc\nUnsup-S\n83.14\n69.70\n53.25\n35.91\n60.50\n+ Naive Unsupft-T\n84.50 (+1.36)\n63.73 (‚àí5.97)\n64.27 (+11.02)\n61.26 (+25.35)\n68.44 (+7.94)\n+ Our Unsupft-T\n85.95 (+2.81)\n71.67 (+1.97)\n68.83 (+15.58)\n75.13 (+39.22)\n75.40 (+14.90)\nClusterAcc\nUnsup-S\n56.16\n47.72\n23.91\n11.85\n34.91\n+ Naive Unsupft-T\n61.33 (+5.17)\n47.31 (‚àí0.41)\n41.72 (+17.81)\n34.18 (+22.33)\n46.14 (+11.23)\n+ Our Unsupft-T\n69.93 (+13.77)\n56.11 (+8.39)\n44.92 (+21.01)\n51.74 (+39.89)\n55.68 (+20.77)\nTable 2: Transfer performance comparison on different target datasets. ‚ÄúUnsup-S‚Äù denote the few-shot results by using ImageNet unsu-\npervised pretrained model respectively, ‚ÄúNaive Unsupft-T, and ‚ÄúOur Unsup-T‚Äù denote the few-shot results by using the naive unsupervised\nÔ¨Ånetuned model and our proposed unsupervised Ô¨Ånetuned model from ‚ÄúUnsup-S‚Äù. It shows our unsupervised Ô¨Ånetuning strategy can learn\nbetter representation upon the pretrained models and outperforms the naive baseline strategy.\nboth for few-shot transfer and semi-supervised transfer. The\ndefault backbone is ResNet-50 [19].\n5.1. Main Results\nIn our main experiments, we use the ImageNet unsuper-\nvised pretrained model as the initialization representation,\nand apply the naive unsupervised Ô¨Ånetuning strategy and\nour proposed unsupervised Ô¨Ånetuning strategy on top of it,\nrespectively. To compare the representation quality, few-\nshot transfer accuracy, retrieval accuracy and unsupervised\nclustering accuracy are all evaluated. The detailed compar-\nison results are shown in the Table 2. It can be seen that,\nby unsupervised Ô¨Ånetuning the pretrained representation on\nthe small-scale unlabeled target data, it can signiÔ¨Åcantly im-\nprove the transfer performance in the target domain. And\ncompared to the naive unsupervised Ô¨Ånetuning strategy, our\nproposed unsupervised Ô¨Ånetuning strategy consistently out-\nperforms the original pretrained model and brings much\nlarger performance gain, while the naive strategy achieves\nlower transfer performance than the pretrained model on the\nPet dataset. By carefully studying the above results, we can\ndraw more interesting conclusions:\n‚Ä¢ In general, the fewer the labeled samples, the bigger\nperformance gain unsupervised Ô¨Ånetuning will bring.\nThis is because Ô¨Ånetuning with fewer labeled samples\nis much easier to overÔ¨Åt, thus having a higher require-\nment of the initialization representation.\n‚Ä¢ If the target domain has more unlabeled data (e.g.,\nFood101), unsupervised Ô¨Ånetuning will bring better\nperformance and even the naive unsupervised Ô¨Ånetun-\ning strategy can get very plausible results.\n‚Ä¢ When the domain gap between the source dataset and\ntarget dataset is small, the performance gain from un-\nsupervised Ô¨Ånetuning is relatively smaller. For exam-\nple, in the ImageNet dataset, there are a lot of animal\nimages (especially ‚Äúdog‚Äù), which potentially have a lot\nof overlapped categories with the Pet dataset. There-\nfore, on the Pet dataset, the pretrained representation\nalready has very good transfer performance.\n5.2. Ablation Study\nIn this section, we conduct a series of ablation studies to\ndemonstrate the generalization ability of unsupervised Ô¨Åne-\ntuning and the importance of our discovered strategies.\nGeneralization Ability to Supervised Pretrained Mod-\nels.\nBesides applying to the default unsupervised pre-\ntrained models, our proposed unsupervised Ô¨Ånetuning strat-\negy is also general to supervised pretrained models.\nIn\nthis experiment, we use the supervised pretrained ResNet50\nmodel from BiT [20] as the instantiation and apply unsu-\npervised Ô¨Ånetuning in a similar way. The few-shot trans-\nfer performance before and after unsupervised Ô¨Ånetuning\nK\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\n1-Shot Acc\nSup-S\n45.78\n50.04\n25.73\n16.48\n38.39\n+ Our Unsupft-T\n73.35 (+27.57)\n61.30 (+11.26)\n32.29 (+6.56)\n42.60 (+26.12)\n56.48 (+18.09)\n2-Shot Acc\nSup-S\n61.07\n63.95\n37.40\n24.99\n51.14\n+ Our Unsupft-T\n79.12 (+18.05)\n73.59 (+9.64)\n45.93 (+8.53)\n61.70 (+36.71)\n68.38 (+17.24)\n4-Shot Acc\nSup-S\n73.89\n76.08\n47.79\n35.26\n62.41\n+ Our Unsupft-T\n84.42 (+10.53)\n84.41 (+8.33)\n55.09 (+7.30)\n67.33 (+32.07)\n75.89 (+13.48)\n8-Shot Acc\nSup-S\n83.87\n82.87\n57.46\n45.60\n70.90\n+ Our Unsupft-T\n88.08 (+4.21)\n85.61 (+3.74)\n60.54 (+3.08)\n69.54 (+23.94)\n79.12 (+8.22)\nTable 3: Few-shot transfer performance on different target datasets when applying unsupervised Ô¨Ånetuning upon the supervised pretrained\nmodels, which demonstrates the generalization ability of unsupervised Ô¨Ånetuning. Here, ‚ÄúSup-S‚Äù denote the few-shot results by using the\noriginal ImageNet supervised pretrained model, ‚ÄúOur Unsup-T‚Äù denote the few-shot results by our proposed unsupervised Ô¨Ånetuned model\nfrom ‚ÄúSup-S‚Äù.\nK\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\n1-Shot Acc\nSup-S\n53.28\n50.67\n26.76\n17.05\n36.94\n+ Our Unsupft-T\n74.03 (+20.75)\n64.02 (+13.35)\n34.58 (+7.82)\n42.99 (+25.94)\n53.91 (+16.97)\n2-Shot Acc\nSup-S\n66.32\n65.96\n37.20\n26.20\n48.92\n+ Our Unsupft-T\n83.59 (+17.27)\n79.37 (+13.41)\n50.20 (+13.00)\n64.33 (+38.13)\n69.37 (+20.45)\n4-Shot Acc\nSup-S\n78.08\n79.50\n49.20\n36.38\n60.79\n+ Our Unsupft-T\n87.63 (+9.55)\n86.15 (+6.65)\n58.32 (+9.12)\n70.02 (+33.64)\n75.53 (+14.74)\n8-Shot Acc\nSup-S\n86.23\n82.69\n59.36\n45.84\n68.53\n+ Our Unsupft-T\n89.65 (+3.42)\n86.32 (+3.63)\n63.97 (+4.61)\n71.95 (+26.11)\n77.97 (+9.44)\nTable 4: Few-shot transfer performance on the large supervised pretrained model ResNet101. The annotations are the same as Table 3,\nand the only difference is the larger pretrained model. It demonstrates that our unsupervised Ô¨Ånetuning is also general to large pretrained\nmodels and brings better transfer performance.\nis shown in the Table 3. It can be seen that, our unsuper-\nvised Ô¨Ånetuning can also boost the transfer performance of\nsupervised pretrained models. Compared to the few-shot\ntransfer performance shown in the Table 2, we can further\nobserve: 1) the original supervised pretrained model has\nbetter few-shot transfer performance than the original un-\nsupervised pretrained model. This is because supervised\npretrained models are often trained with the cross entropy\nloss by using supervised labels, and there is not a simi-\nlar distribution term in the cross entropy loss to encourage\nuniformity. Therefore, supervised pretrained representation\nis more compact; 2) by equipping with our unsupervised\nÔ¨Ånetuning, the performance gap between unsupervised and\nsupervised pretrained model becomes much smaller. Note\nthat, we also try the naive unsupervised Ô¨Ånetuning strategy\nupon the supervised pretrained model, but Ô¨Ånd its transfer\nperformance is very bad, so we do not provide its results in\nthe table.\nGeneralization Ability to Large Models.\nBesides the\npretraining method, our unsupevised Ô¨Ånetuning strategy\nalso generalizes well to large models. In the Table 4, we\ntake the supervised pretrained ResNet101 model as an ex-\nample and evaluate its transfer performance. It shows that,\neven though the large model itself can produce better trans-\nfer performance than the default ResNet50 model, it can\nstill improve its performance in a similar way to the small\nmodel by using the unsupervised Ô¨Ånenetuned model as ini-\ntialization.\nApplication in Semi-supervised Learning.\nThe essence\nof unsupervised Ô¨Ånetuning is utilizing the unlabeled tar-\nget data to get a better representation. Similarly, in semi-\nsupervised learning, it also aims to leverage the unlabeled\ntarget data as the extra learning guidance to achieve bet-\nter performance. In this experiment, we are curious about\nwhether semi-supervised learning can still beneÔ¨Åt from un-\nsupervised Ô¨Ånetuned representation under the same data set-\nting. To answer this question, we leverage the state-of-the-\nart semi-supervised learning method FixMatch [28], and\nuse the original unsupervised pretrained representation and\nunsupervised Ô¨Ånetuned representation as the initialization\nrespectively.\nSurprisingly, as shown in the Table 5, semi-supervised\nlearning also beneÔ¨Åts from better initialization models from\nunsupervised Ô¨Ånetuning, but the overall performance gain is\nsmaller. In our understanding, many semi-supervised meth-\nods including FixMatch use the pseudo labels of the un-\nK\nMethod\nCaltech101\nPet\nCIFAR100\nFood101\nAvg\nSemi-2 Acc\nUnsup-S\n79.04\n54.52\n41.43\n18.23\n48.31\n+ Our Unsupft-T\n82.84 (+3.80)\n68.58 (+14.06)\n56.35 (+14.92)\n70.33 (+52.10)\n69.53 (+21.22)\nSemi-4 Acc\nUnsup-S\n86.07\n68.18\n61.54\n34.67\n62.61\n+ Our Unsupft-T\n86.87 (+0.8)\n73.25 (+5.07)\n66.18 (+4.64)\n79.35 (+44.68)\n76.41 (+13.80)\nSemi-8 Acc\nUnsup-S\n90.12\n75.39\n70.59\n59.06\n73.79\n+ Our Unsupft-T\n90.27 (+0.15)\n79.18 (+3.79)\n71.65 (+1.06)\n82.35 (+23.29)\n80.86 (+7.07)\nTable 5: Semi-supervised learning results by using different models as initialization. ‚ÄúSemi-K‚Äù means K labeled samples together with\nthe whole unlabeled data are used for semi-supervised learning. Other annotations are similar as Table 2. It shows that, even though\nsemi-supervised learning also leverage the unlabeled data, our unsupervised Ô¨Ånetuned model can provide better initialization for it and\nboost the Ô¨Ånal performance.\nDataset\nUnsupft SDR Mix K = 2 K = 4 K = 8\nCaltech101\n‚Äì\n63.12\n76.54\n84.45\n\u0013\n68.48\n78.18\n82.03\n\u0013\n\u0013\n75.28\n82.99\n85.15\n\u0013\n\u0013\n\u0013\n81.02\n85.35\n87.24\nPet\n‚Äì\n56.84\n68.94\n75.10\n\u0013\n55.94\n67.03\n72.92\n\u0013\n\u0013\n62.81\n71.35\n75.14\n\u0013\n\u0013\n\u0013\n70.29\n76.89\n79.88\nCIFAR100\n‚Äì\n22.95\n35.08\n47.84\n\u0013\n30.15\n39.70\n48.59\n\u0013\n\u0013\n35.60\n45.01\n52.62\n\u0013\n\u0013\n\u0013\n44.51\n52.60\n59.68\nFood101\n‚Äì\n15.84\n25.49\n35.50\n\u0013\n39.64\n50.82\n59.58\n\u0013\n\u0013\n38.87\n48.97\n56.85\n\u0013\n\u0013\n\u0013\n61.99\n71.14\n76.86\nTable 6: Ablation study of the two key strategies: sparse source\ndata replaying (SDR) and data mixing (Mix). ‚Äú‚Äì‚Äù means base-\nline without unsupervised Ô¨Ånetuning, ‚Äú\u0013‚Äù means having the cor-\nresponding strategy. The second row of each dataset only with\n‚ÄúUnsupft‚Äù checked denotes the naived unsupervised Ô¨Ånetuning. It\nshows that both source data replaying and data mixing can help\nlearn better representations.\nlabeled data as the extra learning constraints, so providing\nbetter initialization will help generate more precise pseudo\nlabels, subsequently improving the Ô¨Ånal performance. Con-\nversely, if the initialization representation is not good, the\npseudo labels will contain more errors and the gain from the\nunlabeled data will be very marginal. For example, com-\npared to the few-shot transfer, when only 2 labeled sam-\nples exist for each category on the Food101 dataset, semi-\nsupervised learning initialized from the ImageNet unsuper-\nvised pretrained model can only bring about 2% perfor-\nmance gain. By contrast, by using our unsupervised Ô¨Åne-\ntuned model as initialization, the extra performance gain\nfrom semi-supervised learning is 8.3%.\nImportance of Source Data Replaying and Data Mixing.\nSparse source data replaying and data mixing are the two\nkeys in our unsupervised Ô¨Ånetuning strategy. To quantita-\ntively demonstrate their importance, we conduct the abla-\ntion studies by gradually removing each of them. As shown\nin the Table 6, on all the datasets except Food101, source\ndata replaying can boost the transfer performance by a large\nmargin. For example, on the Caltech101 and Pet dataset,\nusing source data replaying will boost the 2-shot transfer\nperformance by 6.8%, 6.9% respectively. The reason why\nsource data replaying cannot help Food101 should be be-\ncause Food101 already has a larger number of images for\neach category, i.e., 750 images per category. As for data\nmixing, it consistently brings better performance on all the\ndatasets.\nEspecially on the Food101 dataset, the 2-shot\ntransfer performance is signiÔ¨Åcantly boosted by 23.12%.\n6. Conclusion\nUnsupervised learning has achieved great progress in re-\ncent years and become one of the most active and hot re-\nsearch topic. However, almost all existing works focus on\nthe large-scale unsupervised pretraining, i.e., conduct un-\nsupervised learning on a large-scale dataset. But in many\nreal application scenarios, collecting such a large-scale un-\nlabeled dataset is difÔ¨Åcult or impossible. In this paper, we\nstudy ‚Äúunsupervised Ô¨Ånetuning‚Äù, which aims to relax this\nrequirement by unsupervised learning on a small-scale un-\nlabel dataset on top of the pretrained representation. To\nthe best of our knowledge, there is no prior work that fo-\ncuses on this problem in the computer vision Ô¨Åeld yet. Ac-\ncording to our analysis, we Ô¨Ånd unsupervised learning on\nthe small-scale unlabeled data either from scratch or from\nthe pretrained representation does not work well. After an-\nalyzing the widely used contrastive loss, we propose two\nsimple and effective strategies, ‚Äúsparse source data replay-\ning‚Äù and ‚Äúdata mixing‚Äù. Through extensive experiments, we\ndemonstrate the proposed unsupervised Ô¨Ånetuning strategy\ncan achieve better performance than the naive unsupervised\nÔ¨Ånetuning strategy. We hope our study will inspire more\nresearch in this direction.\nReferences\n[1] Enrique Amig¬¥o, Julio Gonzalo, Javier Artiles, and Felisa\nVerdejo.\nA comparison of extrinsic clustering evaluation\nmetrics based on formal constraints. Information retrieval,\n12(4):461‚Äì486, 2009. 5\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101‚Äìmining discriminative components with random\nforests. In European conference on computer vision, pages\n446‚Äì461. Springer, 2014. 5\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213‚Äì229. Springer, 2020.\n2\n[4] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features\non non-curated data. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 2959‚Äì2968,\n2019. 1\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning\nof visual features by contrasting cluster assignments. 2020.\n2\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834‚Äì848, 2017. 2\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597‚Äì1607. PMLR, 2020. 2, 3\n[8] Ting Chen and Lala Li. Intriguing properties of contrastive\nlosses. arXiv preprint arXiv:2011.02803, 2020. 3\n[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv preprint arXiv:2003.04297, 2020. 3, 4, 5\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248‚Äì255. Ieee, 2009. 2\n[11] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 1422‚Äì1430, 2015. 2\n[12] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\native visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories. In\n2004 conference on computer vision and pattern recognition\nworkshop, pages 178‚Äì178. IEEE, 2004. 3, 5\n[13] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu\nYuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsuper-\nvised pre-training for person re-identiÔ¨Åcation. arXiv preprint\narXiv:2012.03753, 2020. 1\n[14] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen,\nJianmin Bao, Gang Hua, and Houqiang Li. Improving person\nre-identiÔ¨Åcation with iterative impression aggregation. IEEE\nTransactions on Image Processing, 2020. 2\n[15] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. arXiv preprint arXiv:1803.07728, 2018. 2\n[16] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1440‚Äì1448,\n2015. 2\n[17] Jean-Bastien Grill, Florian Strub, Florent Altch¬¥e, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning.\narXiv preprint\narXiv:2006.07733, 2020. 2\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729‚Äì9738, 2020. 2\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 1, 2, 6\n[20] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 2, 5, 6\n[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 5\n[22] Suichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei\nZhang, Qi Chu, Bin Liu, and Nenghai Yu. Improve unsu-\npervised pretraining for few-label transfer. In ICCV, 2021.\n2\n[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3431‚Äì3440, 2015. 2\n[24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 2\n[25] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on\ncomputer vision and pattern recognition, pages 3498‚Äì3505.\nIEEE, 2012. 3, 5\n[26] Cheng Perng Phoo and Bharath Hariharan.\nSelf-training\nfor few-shot transfer across extreme task differences. arXiv\npreprint arXiv:2010.07734, 2020. 1, 2\n[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. arXiv preprint arXiv:1506.01497, 2015.\n2\n[28] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao\nZhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han\nZhang, and Colin Raffel.\nFixmatch: Simplifying semi-\nsupervised learning with consistency and conÔ¨Ådence. arXiv\npreprint arXiv:2001.07685, 2020. 7\n[29] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international\nconference on computer vision, pages 843‚Äì852, 2017. 2\n[30] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1‚Äì9, 2015.\n1, 2\n[31] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105‚Äì6114. PMLR,\n2019. 1\n[32] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In Proceedings of the\n25th international conference on Machine learning, pages\n1096‚Äì1103, 2008. 2\n[33] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. Learning discriminative features with multiple gran-\nularities for person re-identiÔ¨Åcation. In Proceedings of the\n26th ACM international conference on Multimedia, pages\n274‚Äì282, 2018. 2\n[34] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3733‚Äì\n3742, 2018. 2\n[35] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiÔ¨Åers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023‚Äì6032, 2019. 5\n[36] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain\nautoencoders: Unsupervised learning by cross-channel pre-\ndiction. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1058‚Äì1067, 2017. 2\n[37] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson Corso, and Jianfeng Gao. UniÔ¨Åed vision-language pre-\ntraining for image captioning and vqa. In Proceedings of\nthe AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34,\npages 13041‚Äì13049, 2020. 2\n[38] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\nLiu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training\nand self-training. arXiv preprint arXiv:2006.06882, 2020. 2\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2021-10-18",
  "updated": "2021-10-18"
}