{
  "id": "http://arxiv.org/abs/2110.11872v1",
  "title": "Patient level simulation and reinforcement learning to discover novel strategies for treating ovarian cancer",
  "authors": [
    "Brian Murphy",
    "Mustafa Nasir-Moin",
    "Grace von Oiste",
    "Viola Chen",
    "Howard A Riina",
    "Douglas Kondziolka",
    "Eric K Oermann"
  ],
  "abstract": "The prognosis for patients with epithelial ovarian cancer remains dismal\ndespite improvements in survival for other cancers. Treatment involves multiple\nlines of chemotherapy and becomes increasingly heterogeneous after first-line\ntherapy. Reinforcement learning with real-world outcomes data has the potential\nto identify novel treatment strategies to improve overall survival. We design a\nreinforcement learning environment to model epithelial ovarian cancer treatment\ntrajectories and use model free reinforcement learning to investigate\ntherapeutic regimens for simulated patients.",
  "text": "Patient level simulation and reinforcement learning to discover novel strategies for treating\novarian cancer\nBrian Murphy, M.S.1\nMustafa Nasir-Moin, A.B.2\nGrace von Oiste2\nViola Chen, M.D.3\nHoward A Riina, M.D. 2,4,5\nDouglas Kondziolka, M.D.2,6\nEric K Oermann, M.D.2,4,7*\n1 Vilcek Institute of Graduate Biomedical Sciences, NYU Grossman School of Medicine, New York, NY 10016\n2 Department of Neurosurgery, NYU Langone Health System, New York, NY 10016\n3 Oncology Early development, Merck & Co., Inc, Kenilworth, NJ 07033, USA.\n4 Department of Radiology, NYU Langone Health System, New York, NY 10016\n5 Department of Neurology, NYU Langone Health System, New York NY 10016\n6 Department of Radiation Oncology, NYU Langone Health System, New York NY 10016\n7 Center for Data Science, New York University, New York, NY 10016\n*Corresponding Author\nCorresponding Author:\nEric Oermann,MD\nAssistant Professor of Neurosurgery, Radiology, and Data Science\nNYU Langone Health\n530 First Ave\nSkirball, 8R\nNew York, NY 10016\neric.oermann@nyulangone.org\nKeywords: Reinforcement learning, artificial intelligence, ovarian cancer, survival analysis, precision medicine\nData Availability Statement: Code and data available at: https://github.com/bmurphy1993/Cancer_Reinforcement_Learning\nConflict of Interest Statement: The authors declare no potential conflicts of interest\nOther information:\n●\nWord count: abstract 249; statement of translational relevance 126; main text 2,439; supplemental methods 1,694\n●\nFigures: eight including one table and two supplemental figures\n1\nABSTRACT\nPurpose: The prognosis for patients with epithelial ovarian cancer remains dismal despite improvements in survival for other\ncancers. Treatment involves multiple lines of chemotherapy and becomes increasingly heterogeneous after first-line therapy.\nReinforcement learning with real-world outcomes data has the potential to identify novel treatment strategies to improve\noverall survival. We design a reinforcement learning environment to model epithelial ovarian cancer treatment trajectories\nand deploy a reinforcement learning agent to suggest therapeutic regimens for simulated patients.\nExperimental Design: We construct a Markov decision process that serves as a reinforcement learning environment using\nCox proportional hazard survival analysis and patient-level data from The Cancer Genome Atlas. We use a temporal\ndifference learning approach to solve for optimal treatment strategies based on simulated patient trajectories from the\nenvironment. We test average survival time of patients treated by the agent at the start and end of training and against real\npatients treated by clinicians. We also measure the frequency and timing of the agent’s treatments and compare them with\nclinician decisions.\nResults: The agent identifies treatment strategies that improve average survival within the conditions of the environment.\nHowever, it prefers experimental drugs and is not able to find a stable policy when restricted to more common actions.\nConclusions: This experiment demonstrates proof-of-concept for a reinforcement learning approach to identifying epithelial\novarian cancer treatment strategies and can serve as a prototype for future efforts. The  work would benefit from a larger\nsample size of real-world cancer patients and refinement of the Cox survival models.\nStatement of Translational Relevance: Evolutionary dynamics and evolved treatment resistance is a hallmark of\ndisseminated cancer and is often responsible for eventual cancer progression and patient death. Typically multiple lines of\nsingle or multi-agent therapies are administered over time in an attempt to counter these dynamics of resistance while\nminimizing toxicity and maximizing overall survival. The advances in knowledge of this study include: (1) the construction\nof a patient-level simulation of response to individual lines of therapy for metastatic ovarian cancer using real-world\noutcomes data; (2) the creation of a game whereby an agent selects lines of therapy for simulated patients in order to\nmaximize a reward signal (overall survival); (3) a model-free reinforcement learning agent that learns to find an optimal\nsolution for this game.\n2\nINTRODUCTION\nWhile research has led to marked increases in survival rates for many cancers, the dismal prognosis for epithelial\novarian cancer patients (approximately 47% five-year survival) has not improved substantially over the years.(1–3)\nTreatment regimens for epithelial ovarian cancer typically involve surgical debulking followed by adjuvant chemotherapy,\nwhich often proceeds over several lines of therapy with each line using multiple agents in combination.(1,2,4–8) Typical\nfirst-line treatment for epithelial ovarian cancer involves combination of a platinum based compound such as carboplatin and\na taxane based compound such as paclitaxel.(2,6,9) Physicians may further optimize these lines of treatment by combining\nchemotherapy with immunotherapy or radiation, adjusting dosing schedules, and using different drug administration routes.\n(6,10) As patients progress into additional lines of therapy, overall treatment plans become increasingly heterogeneous with\nlittle guidance for optimizing towards overall survival.(1,2,6,9) These complex and heterogeneous regimens used in practice\npresent a challenge for attempts to develop personalized optimal care pathways based on published literature.(11,12) An\nalternative approach could be to learn optimal treatment plans directly from trial and registry data conditioned on individual\npatient characteristics.\nReinforcement learning is a branch of machine learning that uses artificial intelligence (AI) algorithms, “agents,” to\ndiscover optimal strategies for achieving a goal. Reinforcement learning agents function within an environment which can\nconsist of discrete states that describe the environment to the agent, and administer numerical rewards which the agent then\ntries to maximize via the actions it takes. (13,14) Given its ability to learn and optimize algorithms for sequential decision\nmaking, reinforcement learning offers potential benefits in clinical decision making for diseases that involve multiple\ntreatment stages. (15–22) In this study, we reformulate real world cancer trial data as a patient-level simulated environment\nof epithelial ovarian cancer and train a reinforcement learning agent to predict optimal treatment decisions for patients.\nMETHODS\nData Source and Preprocessing\n609 patients with epithelial ovarian cancer were retrieved from The Cancer Genome Atlas (“TCGA”). Comprehensive\ntreatment plans and responses to therapy for these patients were obtained from Villalobos et al., 2018.(7) We employed a\npre-processing pipeline (see link to code) to prepare the dataset prior to conversion into a reinforcement learning\nenvironment. We converted all drug names to their generic equivalent names using a drug standardization index based on the\nNCI Drug Dictionary and Broad GDAC Firehose (see, https://gdisc.bme.gatech.edu/cgi-bin/gdisc/tap5.cgi).(23) We removed\nsamples from the treatment regimens data that did not include the names of the drugs in the treatment line, and where the\ntreatment start and end days were equivalent, making the timing of the treatment line unclear. We additionally excluded\npatients that did not achieve their overall survival endpoint leaving us with 225 out of 460 patients in the final dataset. (Table\n1).\nNext, we reorganized the data into 30-day treatment periods. The full reorganized dataset consists of 9,296 one-month\ntreatment period samples, each containing the patient ID, the number of months since the start of treatment, and the current\ncombination of therapeutic agents, including periods where the patient was not prescribed any treatment. These data contain\n127 unique drug combinations and a “no active treatment” option. We used the subset of patients whose final survival metric\nwas a death event to construct a reinforcement learning environment. This subset consists of 5,931 one-month treatment\nperiod samples, with 107 unique drug combinations and the “no active treatment” option.\nEnvironment: State\nThese data were then used to construct a Markov decision process (MDP) to simulate the treatment decisions and\nsurvival trajectories of epithelial ovarian cancer patients. This served as the reinforcement learning environment in the model,\nwith each state constituting the overall status of the patient, their response to the current line of therapy, as well as time since\nthe start of treatment, total time on therapy, age, race, and tumor specific information (tumor grade and stage). The actions\navailable to the agents consisted of all unique treatment combinations (see Supplemental Methods for detailed environmental\ndescription). We additionally excluded drug combinations not present in the TCGA ovarian cancer dataset due to insufficient\ninformation.\nEnvironment: Survival modelling\nThe critical element of our environment is modeling a set of transition probabilities of each patient from one state to the\nnext given a particular treatment at a particular time. Each state transition involves two sets of probabilities that stochastically\ndetermine the subsequent state. The first determines whether a patient dies with probability P(D) or survives with probability\nP(S) = 1 - P(D). If the patient dies, the subsequent state is death, the terminal state, and simulation proceeds to the next\nround. If the patient survives, the next set of probabilities determines whether she goes into remission, P(R), or needs further\n3\ntreatment in the next state, P(T) = 1 – P(R) (Supplemental Figure S1). To calculate these probabilities, we used a survival\nanalysis based on two multivariate Cox proportional hazard regressions. The first regression calculates baseline hazard using\na terminal death event and months since the start of treatment, and the second calculates baseline hazard using the recurrent\nremission event and months on the current treatment regimen (Supplemental Figure S2). Each regression calculates\ntreatment-specific hazard with the feature set that describes the patient state and the actions from each state. We then sample\neach regression’s survival function according to the patient’s current state-action pair to obtain P(D), P(S), P(R) and P(T)\n(see Supplemental Methods for detailed description of survival modeling). The reward is the total number of months of\npatient survival where the action\ndid not result in death.\n𝑎\nReinforcement learning and statistics\nWe opted for a model free reinforcement learning approach and implemented a deep Q-network (DQN) (see\nSupplemental Methods for details). The agent chooses actions (drug combinations) based on observed state transitions, and\nthe state-action pairs feed into the MDP which stochastically determines subsequent states (Figure 1A, Supplemental Figure\nS1).\nThe DQN agent was trained for 200,000 rounds where one round = one simulated patient, and previous patient\ntrajectories served as the training dataset for the DQN (Figure 1B). During training, we calculated the cumulative average\nsurvival and the moving average survival of the previous 1,000 simulated patients. Final performance for the DQN agent was\nevaluated based on the average simulated patient survival of the last 1,000 patients of training compared to a baseline average\nsurvival calculated from the first 1,000 patients. We also compared the average survival of the last 1,000 simulated patients\nwith the average survival of patients treated by clinicians (i.e., patients with overall survival in the TCGA dataset). We used a\ntwo-sided t-test to test the significance of the average survival at the end of training compared to each baseline average at an\nalpha of 0.05. We characterized treatment decisions for clinicians and the AI policy by calculating the relative frequency that\neach drug combination occured at fixed time intervals from the start of treatment. For heatmaps displaying these patient\ntreatment characteristics, chemotherapy administration frequencies were normalized from time of initial treatment and\nconverted to z-scores across all drug combinations. Z-scores were subsequently bounded between 0-3 to generate smooth\nvisualizations of drug selection and timing in courses of care by both the DQN and human oncologists. We also calculated\nthe percentage of patients receiving each drug combination in each line of treatment, displaying values for the first six\ntreatment lines from the TCGA data, the DQN agent, and the restricted DQN.\nWe separately ran the experiment restricting the actions available to the DQN agent to the drug combinations that\noccured at least five times in the dataset used to construct the MDP. This “restricted” model was intended as a control on the\nagent to prevent it from learning actions drawn from rare drug combinations given in unique circumstances that may not be\nreflective of general oncological care. Lastly, we designed a separate, rules-based agent based on the National\nComprehensive Cancer Network (NCCN) guidelines for ovarian cancer treatment and deployed it in the same environment to\nsimulate performance based on available medical guidance.(9)\nRESULTS\nAfter running the reinforcement learning simulation with the full set of actions for 200,000 simulated cases, we noted a\nmarked shift in policy after approximately 50,000 simulated patients (Figure 2A), whereby the agent learned to use aldesleukin\nresulting in an increase in mean survival from 32.3 months for the first 1,000 patients to 42.9 months for the last 1,000 patients\n(p<0.000) (Figure 2A-B). The AI treatment strategy also exceeded the baseline oncologist driven treatment strategy, which had\na mean survival of 26.4 months, for both the first 1,000 (p=0.003) and last 1,000 patients (p<0.000, Figure 2B). Notably,\nhuman oncologists most commonly prescribed carboplatin and paclitaxel as first-line treatment and over time switched to\ntopotecan, doxorubicin, carboplatin, or paclitaxel monotherapy in later months (Figure 4A, Figure 5A), while the DQN favored\nan almost continuous administration of aldesleukin (Figure 4B, Figure 5B).\nWe also ran the simulation with the restricted action set to see whether the DQN agent would develop an alternative\nstrategy when limited to only more commonly used treatments. After a million simulated cases, the DQN developed a policy\nthat favored gemcitabine and tamoxifen combination therapy and shifted to other regimens such as cisplatin and tamoxifen\ncombination therapy in later rounds (Figure 4C , Figure 5C). After training, the restricted DQN was able to significantly\nimprove average survival, with a mean survival of 45.5 months for the last 1,000 simulated patients compared to the restricted\nbaseline (first 1,000 patients) of 43.4 months (Figure 3A-B, p=0.099). This also represented a statistically significant increase\nover the average clinician, with p-values less than 0.000 for the two-sided t-test between the clinicians and both the baseline\nand trained agent (Figure 3B). However, though the restricted DQN seemed to slowly improve its average performance through\napproximately 450,000 rounds of training, it is not clear that it settled on a stable policy after a million rounds, as did the\nunrestricted DQN (Figure 3A).\n4\nFor an additional comparison, we ran the simulation with the NCCN agent for 200,000 rounds and calculated the average\nsurvival across all simulated patients. The NCCN agent outperformed clinicians (p<0.000) and the last 1,000 patients for both\nthe DQN (p<0.000) and restricted DQN (p<0.000) with average survival of 49.5 months.\nDISCUSSION\nWe present the first use of a reinforcement learning agent to create novel, individualized treatment plans for cancer\npatients after training on a patient level simulator of cancer outcomes based on real-world data. Previous studies have\nexplored the use of reinforcement learning to select specific dosing regimens for single agents (20,21,24), to optimize\nradiotherapy plans (25), and to optimize artificial therapies in simulated clinical  trials (26). However, prior works fail to\naccount for the wide variety of available treatments, and their potential for impacting outcomes when administered in a\nsequential manner as part of a comprehensive oncological treatment plan. In order to accomplish this, we pioneer the use of\nreal-world data to develop a patient level simulation of epithelial ovarian cancer treatment and outcomes to serve as a\nsimulation environment for training a DQN agent to the task of optimizing treatment plans. Previous approaches towards\ntreatment plan discovery and optimization have focused on searching the existing clinical trial literature, an approach\npopularized by IBM Watson (11,12), and from genomic and molecular biomarker data (27–30). These static approaches,\napplied at a single time point, fail to account for the evolution of cancer in response to treatments (1) and the goal of\nmaximizing overall survival across an entire course of care.\nInterestingly, our NCCN rules based agent obtained the overall best results as measured by overall survival outcomes.\nThis is an encouraging validation of our simulation and the NCCN guidelines demonstrating that overall adhering to the\nguidelines lead to the best overall survival for our simulated cohort. Validating guidelines using data-driven simulations of\ncancer is itself an interesting implication of this work. Guidelines have evolved over the past several decades with\nincreasingly sophisticated treatments and a corresponding improvement in overall survival (31,32). An interesting future\ndirection and further validation of this cancer simulation technology could be to see if the simulated patients experience\nsimilar survival trends when treated under the same set of evolving guidelines over time.\nA further sensible future direction would be to test our models on patients that were not used to create the training\nenvironment that a pre-trained agent would “treat” by observing their characteristics as it did with simulated patients and\nrecommending treatments. The reinforcement learning agent would need to prove successful in this task to be considered for\nclinical use. In a clinical setting, the agent could inform oncologists’ decisions for real patients using data from the patient’s\nmedical records and test results as inputs for treatment suggestions at each phase of their cancer and treatment trajectory.\nThis paper has several key limitations worth noting. First and foremost, the quality of our trained agent is fundamentally\nlimited by the fidelity of the underlying simulator. We constructed our patient-level cancer simulation on the TCGA dataset\nannotated by Villalobos et al. (7), and only included patients who achieved their overall survival endpoint limiting our overall\nsample size. This high dimensional dataset with a relatively small sample size makes for a particularly challenging\nreinforcement learning problem, and a clear future direction is to improve upon the existing simulation with a larger amount\nof real world data curated from one or more comprehensive cancer centers or clinical trial databases. A sufficiently massive\ndataset would be at least an order of magnitude larger in terms of the number of patients (225 in this study), and ideally two\norders of magnitude larger, on par with the MIMIC-III dataset that Komorowski et al. used for their model.(17) A further,\ncritical limitation of our model is the mathematical model of survival. Results from the simulation, specifically the fact that\nthe untrained DQN agents outperformed clinicians in average survival (Figure 2B, Figure 3B), show that the survival model\nrequires further improvements to more accurately represent the cancer treatment environment. We employed multivariable\nCox proportional hazard regression (see Supplemental Methods for details), but further refinements of our survival model\nand a comprehensive model of toxicity would likely lead to marked improvements in overall performance.\nIn conclusion, we present the first use of real world data to create a patient level simulation of cancer for the training of\nan AI agent to optimize individual care pathways for patients suffering from epithelial ovarian cancer. Although primarily a\nproof of concept, we expect that the combination of cutting edge AI technologies such as reinforcement learning with\nreal-world data has the potential to significantly contribute to basic oncological decision making at the point of care in\nclinical scenarios that extend beyond current guidelines.\n5\nREFERENCES\n1.\nLheureux S, Braunstein M, Oza AM. Epithelial ovarian cancer: Evolution of management in the era of precision medicine.\nCA Cancer J Clin. 2019;69:280–304.\n2.\nStewart C, Ralyea C, Lockwood S. Ovarian Cancer: An Integrated Review. Semin Oncol Nurs. 2019;35:151–6.\n3.\nWebb PM, Jordan SJ. Epidemiology of epithelial ovarian cancer. Best Pract Res Clin Obstet Gynaecol. 2017;41:3–14.\n4.\nEisenhauer EA. Real-world evidence in the treatment of ovarian cancer. Ann Oncol. 2017;28:viii61–5.\n5.\nMakar AP, Tropé CG, Tummers P, Denys H, Vandecasteele K. Advanced ovarian cancer: Primary or interval debulking?\nFive categories of patients in view of the results of randomized trials and tumor biology: Primary debulking surgery and\ninterval debulking surgery for advanced ovarian cancer. Oncologist. 2016;21:745–54.\n6.\nMonk BJ, Randall LM, Grisham RN. The Evolving Landscape of Chemotherapy in Newly Diagnosed Advanced\nEpithelial Ovarian Cancer. Am Soc Clin Oncol Educ Book. 2019;39:e141–51.\n7.\nVillalobos VM, Wang YC, Sikic BI. Reannotation and Analysis of Clinical and Chemotherapy Outcomes in the Ovarian\nData Set From The Cancer Genome Atlas. JCO Clin Cancer Inform. 2018;2:1–16.\n8.\nChen VJ, Hernandez-Meza G, Agrawal P, Zhang CA, Xie L, Gong CL, et al. Time on Therapy for at Least Three Months\nCorrelates with Overall Survival in Metastatic Renal Cell Carcinoma. Cancers [Internet]. 2019;11. Available from:\nhttp://dx.doi.org/10.3390/cancers11071000\n9.\nNational Comprehensive Cancer Network® (NCCN®). NCCN Guidelines for Patients® Ovarian Cancer. National\nComprehensive Cancer Network® (NCCN®); 2019.\n10. Herrera FG, Irving M, Kandalaft LE, Coukos G. Rational combinations of immunotherapy with radiotherapy in ovarian\ncancer. Lancet Oncol. 2019;20:e417–33.\n11. Strickland E. IBM Watson, heal thyself: How IBM overpromised and underdelivered on AI health care. IEEE Spectrum.\n2019;56:24–31.\n12. Zhang R, Cairelli MJ, Fiszman M, Kilicoglu H, Rindflesch TC, Pakhomov SV, et al. Exploiting Literature-derived\nKnowledge and Semantics to Identify Potential Prostate Cancer Drugs. Cancer Inform. journals.sagepub.com;\n2014;13:103–11.\n13. Osinski B, Jakubowski A, Ziecina P, Milos P, Galias C, Homoceanu S, et al. Simulation-Based Reinforcement Learning\nfor Real-World Autonomous Driving [Internet]. 2020 IEEE International Conference on Robotics and Automation\n(ICRA). 2020. Available from: http://dx.doi.org/10.1109/icra40945.2020.9196730\n14. Sutton RS, Barto AG. Reinforcement Learning, second edition: An Introduction. MIT Press; 2018.\n15. Abdollahian M, Das TK. A MDP model for breast and ovarian cancer intervention strategies for BRCA1/2 mutation\ncarriers. IEEE J Biomed Health Inform. Institute of Electrical and Electronics Engineers (IEEE); 2015;19:720–7.\n16. Imani F, Qiu Z, Yang H. Markov decision process modeling for multi-stage optimization of intervention and treatment\nstrategies in breast cancer. Annu Int Conf IEEE Eng Med Biol Soc. 2020;2020:5394–7.\n17. Komorowski M, Celi LA, Badawi O, Gordon AC, Faisal AA. The Artificial Intelligence Clinician learns optimal\ntreatment strategies for sepsis in intensive care. Nat Med [Internet]. 2018; Available from:\nhttps://doi.org/10.1038/s41591-018-0213-5\n18. Maass K, Kim M. A Markov decision process approach to optimizing cancer therapy using multiple modalities. Math Med\nBiol. Oxford University Press (OUP); 2020;37:22–39.\n6\n19. Tseng H-H, Luo Y, Cui S, Chien J-T, Ten Haken RK, Naqa IE. Deep reinforcement learning for automated radiation\nadaptation in lung cancer. Med Phys. 2017;44:6690–705.\n20. Yazdjerdi P, Meskin N, Al-Naemi M, Al Moustafa A-E, Kovács L. Reinforcement learning-based control of tumor growth\nunder anti-angiogenic therapy. Comput Methods Programs Biomed. Elsevier BV; 2019;173:15–26.\n21. Padmanabhan R, Meskin N, Haddad WM. Reinforcement learning-based control of drug dosing for cancer chemotherapy\ntreatment [Internet]. Mathematical Biosciences. 2017. page 11–20. Available from:\nhttp://dx.doi.org/10.1016/j.mbs.2017.08.004\n22. Shen C, Gonzalez Y, Klages P, Qin N, Jung H, Chen L, et al. Intelligent inverse treatment planning via deep reinforcement\nlearning, a proof-of-principle study in high dose-rate brachytherapy for cervical cancer. Phys Med Biol. 2019;64:115013.\n23. Spainhour JCG, Qiu P. Identification of gene-drug interactions that impact patient survival in TCGA. BMC\nBioinformatics. 2016;17:409.\n24. Yauney G, Shah P. Reinforcement Learning with Action-Derived Rewards for Chemotherapy and Clinical Trial Dosing\nRegimen Selection. In: Doshi-Velez F, Fackler J, Jung K, Kale D, Ranganath R, Wallace B, et al., editors. Proceedings of\nthe 3rd Machine Learning for Healthcare Conference. Palo Alto, California: PMLR; 2018. page 161–226.\n25. Jalalimanesh A, Shahabi Haghighi H, Ahmadi A, Soltani M. Simulation-based optimization of radiotherapy: Agent-based\nmodeling and reinforcement learning. Math Comput Simul. 2017;133:235–48.\n26. Zhao Y, Kosorok MR, Zeng D. Reinforcement learning design for cancer clinical trials. Stat Med. 2009;28:3294–315.\n27. Huang C, Mezencev R, McDonald JF, Vannberg F. Open source machine-learning algorithms for the prediction of optimal\ncancer drug therapies. PLoS One. journals.plos.org; 2017;12:e0186906.\n28. Chi DS, Venkatraman ES, Masson V, Hoskins WJ. The ability of preoperative serum CA-125 to predict optimal primary\ntumor cytoreduction in stage III epithelial ovarian carcinoma. Gynecol Oncol. Elsevier; 2000;77:227–31.\n29. Zhang W, Ota T, Shridhar V, Chien J, Wu B, Kuang R. Network-based survival analysis reveals subnetwork signatures for\npredicting outcomes of ovarian cancer treatment. PLoS Comput Biol. journals.plos.org; 2013;9:e1002975.\n30. Shapira I, Oswald M, Lovecchio J, Khalili H, Menzin A, Whyte J, et al. Circulating biomarkers for detection of ovarian\ncancer and predicting cancer outcomes. Br J Cancer. nature.com; 2014;110:976–83.\n31. Barnholtz-Sloan JS, Schwartz AG, Qureshi F, Jacques S, Malone J, Munkarah AR. Ovarian cancer: changes in patterns at\ndiagnosis and relative survival over the last three decades. Am J Obstet Gynecol. Elsevier; 2003;189:1120–7.\n32. Lin JJ, Egorova N, Franco R, Prasad-Hayes M, Bickell NA. Ovarian Cancer Treatment and Survival Trends Among\nWomen Older Than 65 Years of Age in the United States, 1995-2008. Obstet Gynecol. ncbi.nlm.nih.gov; 2016;127:81–9.\n33. Cox DR. Regression Models and Life-Tables. J R Stat Soc Series B Stat Methodol. 1972;34:187–202.\n34. Ozga A-K, Kieser M, Rauch G. A systematic comparison of recurrent event models for application to composite\nendpoints. BMC Med Res Methodol. 2018;18:2.\n35. Jiang H, Fine JP. Survival Analysis. In: Ambrosius WT, editor. Topics in Biostatistics. Totowa, NJ: Humana Press; 2007.\npage 303–18.\n36. Prentice RL, Williams BJ, Peterson AV. On the regression analysis of multivariate failure time data. Biometrika.\n1981;68:373–9.\n37. Mazroui Y, Mathoulin-Pélissier S, MacGrogan G, Brouste V, Rondeau V. Multivariate frailty models for two types of\nrecurrent events with a dependent terminal event: Application to breast cancer data. Biom J. 2013;55:866–84.\n7\n38. Girshick RB. Fast R-CNN [Internet]. CoRR. 2015. Available from: http://arxiv.org/abs/1504.08083\n39. Graves A. Generating Sequences With Recurrent Neural Networks [Internet]. CoRR. 2013. Available from:\nhttp://arxiv.org/abs/1308.0850\n8\nTABLES\nTable 1:\nTable 1: Overall survival is the number of days to the later of death, last follow up, last recorded tumor progression or\nrecurrence, and last recorded chemotherapy. Only patients with a final survival endpoint (‘Deceased Only’) were included in\nthe analyses in this study.\n9\nFIGURES\nA\nB\nFigure 1:(A) Overall training schema describing reinforcement learning simulation. TCGA data is used to calculate MDP\ntransition probabilities and sample simulated patients. The DQN agent takes simulated patients (current state) as inputs and\nproduces a drug combination (action) for each patient. The MDP stochastically determines each subsequent state based on\nstate-action pairs from the agent. Patient trajectories are stored in replay memory when the patient reaches a terminal state.\nTrajectories from previous rounds are used to train the agent. (B) Conceptual figure showing the underlying premise of a\npatient being on a trajectory of care and being treated at each point by a machine.\n10\nA\nB\nFigure 2: (A) Training run demonstrating agent learning over 200,000 simulated cases. There is a marked shift in policy after\napproximately 50,000 patients, whereby the agent learned to use aldesleukin resulting in an increase in mean survival.\nSMA1000 (blue line) is a simple moving average survival for the previous 1,000 patients, and CMA (orange line) is the\ncumulative average for all previous patients. (B) A box plot demonstrating a comparison between the AI strategies after\n200,000 rounds of training. The DQN was able to significantly improve average survival, with a mean survival of 42.9 months\nfor the last 1,000 simulated patients compared to the first 1,000 baseline of 32.3 months (p<0.000). The DQN policy also\nexceeded average survival of patients treated by clinicians (26.4 months, p<0.000). The NCCN policy achieved the highest\naverage survival at 49.5 months (p<0.000 for clinicians, DQN first 1,000 and DQN last 1,000). Orange lines indicate the\nmedian of the series; green triangles indicate the mean; bottom and top box edges indicate the first and third quartiles,\nrespectively; whiskers indicate 1.5 times the interquartile range; red plusses indicate points outside the whisker range.\n11\nA\nB\nFigure 3: We restricted the set of actions of the DQN to exclude uncommon drugs (see Methods) and noted that (A) there was\na slow but steady increase in the performance of the DQN over the course of training. (B) After 1,000,000 rounds of training,\nthe restricted DQN was able to improve average survival, with a mean survival of 45.5 months for the last 1,000 simulated\npatients compared to the first 1,000 restricted baseline of 43.4 months (p=0.099), and 26.4 months for clinicians (p<0.000). The\nNCCN policy achieved the highest average survival at 49.5 months (p<0.000 for clinicians, restricted DQN first 1,000 and\nrestricted DQN last 1,000)\n12\nA\nB\n13\nC\nFigure 4: Heatmaps of chemotherapy administration frequencies were normalized from time of initial treatment and converted\nto z-scores across all drug combinations. Z-scores were subsequently bounded between 0-3 to generate smooth visualizations\nof drug selection and timing in courses of care by the (A) human oncologists, (B) DQN, and (C) Restricted DQN. The y-axes\nare sorted by overall frequency per treatment across all time intervals.\n14\nA\nB\nC\nFigure  5: Frequency of therapies chosen for each line of treatment by (A) Clinicians, (B) DQN, (C) Restricted DQN. Clinician\ntreatment regimens mirror NCCN guidelines for ovarian cancer. Without restrictions, the DQN favors the use of Aldesleukin\nalmost exclusively, while when limited to common agents, the Restricted DQN tends to favor up-front gemcitabine/tamoxifen,\nfollowed by carboplatin/docetaxel/paclitaxel, but doesn’t learn a single dominant strategy in later rounds.\n15\nSUPPLEMENTAL FIGURES\nSupplemental Figure S1: Markov decision process states and transition probabilities. All patients are assumed to be alive and\nin need of treatment (“Chemo”) in the initial state. Two Cox regressions stochastically determine each subsequent state. The\nfirst regression produces a probability of death P(D) from its survival function, and the second, if the patient survives, produces\nthe probability P(R) that the patient will no longer need treatment (“Remission”).  Solid arrows denote transition probabilities\nfrom the death event regression. Dashed arrows denote probabilities from the recurrence/remission regression. Squares\nrepresent states that are stored in memory during the simulation and shown to the agent. “Survive” circles are intermediate\nstates that are not separately recorded, but determine whether the recurrence/remission regression probabilities are used.\nA\nB\nSupplemental Figure S2: Baseline survival curves from Cox-proportional hazard modeling of (A) death and (B) recurrence.\n16\nSUPPLEMENTAL METHODS\nReinforcement Learning Model\nEnvironment\nUsing the reorganized TCGA data, we constructed a Markov decision process (MDP) to simulate the treatment\ndecisions and survival trajectories of epithelial ovarian cancer patients.(7,14,17) This served as the reinforcement learning\nenvironment in the model. The environment consists of four main components:\n, and\n.\n𝑆,   𝐴,   𝑇𝑠\n', 𝑠, 𝑎\n(\n)\n𝑅𝑠\n'\n( )\nis a finite set of states that describe the health status of the patient. Each state\nin\nincludes whether the patient is\n𝑆\n𝑠\n𝑆\nalive and needs treatment, is alive with cancer in remission, or is deceased, (health state); months since the start of treatment,\ndefined as the number of previous states the patient has survived; the number of consecutive previous states that include the\ncurrent treatment; age at the start of treatment; race; and tumor stage and grade at the start of treatment. Age, race, tumor\nstage, and tumor grade are assigned to simulated patients by sampling values based on their frequency in the TCGA data.\nis a finite set of actions that can be chosen by a reinforcement learning agent.\nconsists of all the unique treatment\n𝐴\n𝐴\ncombinations in the reorganized TCGA drug lines data, and each action\nin\nis a drug or drug combination.\nalso includes\n𝑎\n𝐴\n𝐴\na no-treatment option, but the agent must choose this action if the current state includes a no-treatment-needed health state,\nand the agent cannot choose it if the current state includes a treatment-needed health state. This mechanic is designed to\navoid conflating the state of not needing cancer treatment with the action of taking the patient off all treatments, which by\ndefinition would lead to a no-treatment health state. Thus, the agent is not tasked with diagnosing the status of the patient’s\ncancer, but only with selecting a drug regimen when the environment determines that the patient requires treatment.\nTheoretically, a clinician or agent could select any therapeutic combination that could be constructed from all reasonable\nchoices of chemotherapeutic agents. However, we limited\nto include only those combinations present in the TCGA ovarian\n𝐴\ncancer dataset. We did this to prevent the agent from selecting drug combinations that are not in the data because our model\ncannot calculate transition probabilities for these treatments.\nis a set of transition probabilities giving the likelihood that an action\nchosen in state\nat time\nwill result in\n𝑇(𝑠', 𝑠, 𝑎)\n𝑎\n𝑠\n𝑡\nthe next state\nat time\n.(14,17) Each state transition involves two sets of probabilities that stochastically determine the\n𝑠'\n𝑡+ 1\nsubsequent state. The first determines whether patients die with probability P(D) or survive with probability P(S) = 1 - P(D).\nIf patients die, the subsequent state is death, the terminal state, and simulation proceeds to the next round (i.e., a new\nsimulated patient). If patients survive, the next set of probabilities determines whether their cancer enters remission, P(R), or\nrequires further treatment in the next state, P(T) = 1 – P(R). Supplemental Figure S1 illustrates the interaction of the\ntransition probabilities in\nwith the health state for each state .\n𝑇(𝑠', 𝑠, 𝑎)\n𝑠\nTo calculate these probabilities, we used a survival analysis based on the Cox proportional hazard regression model.(33)\nSurvival analyses often use Cox models to investigate the time to the occurrence of events and the probability of an event’s\noccurrence at a given time interval in the form of a hazard statistic,\n.(34) Using the lifelines python package\nλ\n(https://lifelines.readthedocs.io/en/latest/index.html), we implemented two separate Cox regressions to calculate the transition\nprobabilities for\n. The first, the death event regression, is a common Cox model, and estimates the likelihood of a\n𝑇(𝑠', 𝑠, 𝑎)\npatient dying (a terminal event) at one-month time intervals from the start of treatment.(33,34) The following equation gives\nthe death event regression hazard for a patient :𝑖\nλ𝑖(𝑡) =  λ0(𝑡)𝑒𝑥𝑝(β𝑋𝑖),  𝑖 =  1,...,  𝑛,\nwhere\nis the baseline hazard, accounting for the likelihood of an event with respect to time\nacross all , and\nλ0(𝑡)\n𝑡\n𝑖\n𝑒𝑥𝑝(β𝑋𝑖)\nis the partial or treatment hazard, which is constant over\nand accounts for all other features specific to each (33,34)\nis a\n𝑡\n𝑖.\nβ\nvector of coefficients corresponding to\n, which includes features for each patient’s current health state, current treatment,\n𝑋𝑖\nnumber of previous lines of treatment received, age, race, tumor stage, and tumor grade.(29,33,34) P(D) and P(S) are\ndetermined by sampling from the survival function of\naccording to the patient’s current state-action pair. The survival\nλ𝑖(𝑡)\nfunction gives the probability that a patient has not experienced an event at time , and is defined as (35):\n𝑡\n𝑆(𝑡) = 𝑒𝑥𝑝(−\n0\n𝑡\n∫λ(𝑥)𝑑𝑥)\nThe second model, the recurrence/remission regression, is based on the Prentice-Williams-Peterson approach to the Cox\nmodel, which assumes that the event in question is recurrent and considers the number of past occurrences of the\n17\nevent.(34,36) We use the gap-time version of the Prentice-Williams-Peterson model, which uses time since the previous event\nto calculate the baseline hazard.(34,36,37) The following equation gives the recurrence/remission regression hazard for a\npatient\nand recurrent event (34,36):\n𝑖\n𝑗\nλ𝑖𝑗(𝑡) =  λ0𝑗(𝑡−𝑡𝑗−1)𝑒𝑥𝑝(β𝑗𝑋𝑖𝑗),  𝑖 =  1,...,  𝑛;  𝑗= 1,..., 𝑘𝑖,  𝑘≤𝑘\nincludes features for each patient’s current health state, current treatment, months since start of treatment, number of\n𝑋𝑖𝑗\nprevious lines of treatment received, age, race, tumor stage, and tumor grade. P(R) and P(T) are determined by sampling\nfrom the survival function of\naccording to the patient’s current state-action pair. We used the lifelines CoxPHFitter class\nλ𝑖𝑗(𝑡)\nto implement both regressions with a penalizer term of 0.1 to control for high correlation between covariates (see,\nhttps://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html#lifelines.fitters.coxph_fitter.SemiParametricPHFi\ntter.predict_cumulative_hazard). Supplemental Figures S2A and S2B show the baseline survival curves for the death event\nand recurrence/remission regressions, respectively.\nis the immediate reward administered to the agent by the environment when it transitions to the subsequent state\n𝑅(𝑠')\n𝑠'\n. In this study,\nis simply +1 if\nis not death and -1 if\nis death, and is administered to the agent after each state\n𝑅(𝑠')\n𝑠'\n𝑠'\ntransition. Thus, the total reward after one round of the simulation is the total number of months of patient survival where the\naction\ndid not result in death in the next state.\n𝑎\nThe environment starts by initializing a “patient” in the needs-treatment health state at time\nwith all other state\n𝑡= 0\nfeatures randomly generated based on the TCGA data and held fixed throughout the round of the simulation. The agent then\nselects an action, which along with the state, determines the probabilities of the treatment status of the patient in the\nsubsequent state (needs treatment, does not need treatment, deceased). The next state is then determined stochastically from\nthese probabilities, and a reward signal is administered and added to the cumulative reward. Additionally, the state transition\nis stored for use by a policy in selecting actions in subsequent rounds. This process is repeated until the environment\ntransitions to the “death” state, and the total reward for the round and the trajectory of treatments and transitions (patient\ntrajectory) is stored. The environment runs for a specified number of rounds, each with one patient being treated by the agent,\nand produces the total reward and full treatment trajectory for each round.\nPolicy\nWhen in a needs-treatment health state, the agent selects actions by applying a policy\nto the stored state transitions,\nπ\nwhich compose the replay memory. During each state, the policy trains a model on this replay memory to predict the optimal\naction to maximize total reward. The policy includes an exploration rate\n, which is the probability that the agent will\nϵ∈[0,  1]\nchoose an action randomly. Thus, the policy is an\n-greedy policy.(14)\nstarts at 0.9 and decays after each round until it\nϵ\nϵ\nreaches 0.05. The policy is a deep Q-network (DQN) in the form of a multi-layer perceptron neural network with six fully\nconnected hidden layers, ReLU activation functions, and a fully connected output layer that generates a probability that each\naction is the optimal action. The DQN policy is optimized after each patient trajectory using off-policy temporal difference\n(TD) learning. This optimization calculates loss based on the expected state-action values\nand the actual state-action\n𝑄\nπ(𝑠,  𝑎)\nvalues from saved trajectories.\nis updated using the following equation (14,17):\n𝑄\nπ(𝑠,  𝑎)\n𝑄\nπ 𝑠, 𝑎\n(\n) ←𝑄\nπ 𝑠, 𝑎\n(\n) + α⋅ [𝑟 + γ⋅ 𝑄\nπ 𝑠\n',  𝑎\n'\n(\n) −𝑄\nπ(𝑠, 𝑎)],   𝑓𝑜𝑟 𝑎𝑙𝑙 𝑠∈𝑆,  𝑎∈𝐴(𝑠)\nis determined by a target policy model that uses the DQN weights from a previous round to calculate the action\n𝑄\nπ 𝑠\n', 𝑎\n'\n(\n)\nvalues.\nis the discount rate, which determines how much future rewards are valued relative to immediate rewards.\nγ∈[0,  1]\nThis tells the agent how much it should value future survival relative to survival in the immediate next state.(14) We used a\ndiscount rate\nin this analysis.\nis a constant step-size parameter (14) set at 0.01 in this analysis. We use the\nγ = 0. 99\nα\nRMSprop algorithm and smooth L1 loss to optimize the DQN policy, implemented with pytorch.(38,39)\nIn addition to the DQN agent, we developed an agent based on the National Comprehensive Cancer Network (NCCN)\nguidelines for ovarian cancer treatment to choose the drug combinations for the simulated patient at each time step.(9)\nSpecifically, the NCCN agent is a rules-based policy where we mapped the “preferred regimens” and “other recommended\nregimens” noted in the NCCN guidelines’ Principles of Systemic Therapy of Ovarian Cancer to the drug combinations present\nin our dataset.(9) In treating a simulated patient, the NCCN agent randomly selects one of the “preferred regimens” for the\ngiven disease stage. If after using the “preferred regimens,” the cancer for the simulated patient recurs, then the NCCN agent\nrandomly selects one of the “other recommended regimens” for the given disease stage. We only allowed the NCCN agent to\nchoose actions that are listed in the NCCN guidelines and that occur at least once in the TCGA data.\n18\nFigure 1 summarizes the structure of the entire reinforcement learning simulation. This begins with the reorganized\nTCGA dataset, which we use to create the MDP components: the state (simulated patients and their health state), actions\n(unique drug combinations), and transition probabilities. The environment shows simulated patients to the DQN agent, which\nselects an action in each state. Each state-action pair determines the transition probabilities for survival and subsequent health\nstate until the simulated patient reaches the terminal state. The entire patient trajectory is saved into replay memory, and the\nDQN optimizes based on the replay memory. This process repeats for a specified number of rounds.\n19\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-10-22",
  "updated": "2021-10-22"
}