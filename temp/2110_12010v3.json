{
  "id": "http://arxiv.org/abs/2110.12010v3",
  "title": "ClimateBert: A Pretrained Language Model for Climate-Related Text",
  "authors": [
    "Nicolas Webersinke",
    "Mathias Kraus",
    "Julia Anna Bingler",
    "Markus Leippold"
  ],
  "abstract": "Over the recent years, large pretrained language models (LM) have\nrevolutionized the field of natural language processing (NLP). However, while\npretraining on general language has been shown to work very well for common\nlanguage, it has been observed that niche language poses problems. In\nparticular, climate-related texts include specific language that common LMs can\nnot represent accurately. We argue that this shortcoming of today's LMs limits\nthe applicability of modern NLP to the broad field of text processing of\nclimate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based\nlanguage model that is further pretrained on over 2 million paragraphs of\nclimate-related texts, crawled from various sources such as common news,\nresearch articles, and climate reporting of companies. We find that CLIMATEBERT\nleads to a 48% improvement on a masked language model objective which, in turn,\nleads to lowering error rates by 3.57% to 35.71% for various climate-related\ndownstream tasks like text classification, sentiment analysis, and\nfact-checking.",
  "text": "CLIMATEBERT: A Pretrained Language Model for Climate-Related Text\nNicolas Webersinke,1 Mathias Kraus,1 Julia Anna Bingler,2 Markus Leippold3\n1FAU Erlangen-Nuremberg, Germany\n2ETH Zurich, Switzerland\n3University of Zurich, Switzerland\nnicolas.webersinke@fau.de, mathias.kraus@fau.de, binglerj@ethz.ch, markus.leippold@bf.uzh.ch\nAbstract\nOver the recent years, large pretrained language models (LM)\nhave revolutionized the ﬁeld of natural language processing\n(NLP). However, while pretraining on general language has\nbeen shown to work very well for common language, it has\nbeen observed that niche language poses problems. In par-\nticular, climate-related texts include speciﬁc language that\ncommon LMs can not represent accurately. We argue that\nthis shortcoming of today’s LMs limits the applicability of\nmodern NLP to the broad ﬁeld of text processing of climate-\nrelated texts. As a remedy, we propose CLIMATEBERT, a\ntransformer-based language model that is further pretrained\non over 2 million paragraphs of climate-related texts, crawled\nfrom various sources such as common news, research arti-\ncles, and climate reporting of companies. We ﬁnd that CLI-\nMATEBERT leads to a 48% improvement on a masked lan-\nguage model objective which, in turn, leads to lowering error\nrates by 3.57% to 35.71% for various climate-related down-\nstream tasks like text classiﬁcation, sentiment analysis, and\nfact-checking.\n1\nIntroduction\nResearchers working on climate change-related topics in-\ncreasingly use natural language processing (NLP) to auto-\nmatically extract relevant information from textual data. Ex-\namples include the sentiment or speciﬁcity of language used\nby companies when discussing climate risks and measuring\ncorporate climate change exposure, which increases trans-\nparency to help the public know where we stand on climate\nchange (e.g., Callaghan et al. 2021; Bingler et al. 2022b).\nMany studies in this domain apply traditional NLP meth-\nods, such as dictionaries, bag-of-words approaches or sim-\nple extensions thereof (e.g., Gr¨uning 2011; Sautner et al.\n2022). However, such analyses face considerable limita-\ntions, since climate-related wording could vary substan-\ntially by source (Kim and Kang 2018). Deep learning tech-\nniques that promise higher accuracy are gradually replacing\nthese approaches (e.g., K¨olbel et al. 2020; Luccioni, Baylor,\nand Duchene 2020; Bingler et al. 2022a; Callaghan et al.\n2021; Wang, Chillrud, and McKeown 2021; Friederich et al.\n2021). Indeed, it has been shown in related domains that\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ndeep learning in NLP allows for impressive results, outper-\nforming traditional methods by large margins (Varini et al.\n2020).\nThese deep learning-based approaches make use of lan-\nguage models (LMs), which are trained on large amounts\nof textual and unlabelled data. This training on unlabelled\ndata is called pretraining and leads to the model learning\nrepresentations of words and patterns of common language.\nOne of the most prominent language models is called BERT\n(Bidirectional Encoder Representations from Transformers)\n(Devlin et al. 2018) with its successors ROBERTA (Liu et al.\n2019), Transformer-XL (Dai et al. 2019) and ELECTRA\n(Clark et al. 2020). These models have been trained on huge\namounts of text which was crawled from an unprecedented\namount of online resources.\nAfter the pretraining phase, most LMs are trained on addi-\ntional tasks, the downstream task. For the downstream tasks,\nthe LM builds on and beneﬁts from the word representations\nand language patterns learned in the pretraining phase. The\npre-training beneﬁt is especially large on downstream tasks\nfor which the collection of samples is difﬁcult and, thus, the\nresulting training datasets are small (hundreds or few thou-\nsands of samples). Furthermore, it has been shown that a\nmodel that was pretrained on the downstream task-speciﬁc\ntext exhibits better performance, compared to a model that\nhas been pretrained solely on general text (Araci 2019; Lee\net al. 2020).\nHence, a straightforward extension to the standard com-\nbination of pretraining is the so-called domain-adaptive pre-\ntraining (Gururangan et al. 2020). This approach has re-\ncently been studied for various tasks and basically comes in\nthe form of pretraining multiple times — in particular pre-\ntraining in the language domain of the downstream task, i.e.,\npretraining (general domain)\n+ domain-adaptive\npretraining (downstream domain)\n+ training (downstream task).\nTo date, regardless of the increase in using NLP for cli-\nmate change related research, a model with climate domain-\nadaptive pretraining has not been publicly available, yet.\nResearch so far rather relied on models pretrained on gen-\neral language, and ﬁne-tuned on the downstream task. To\narXiv:2110.12010v3  [cs.CL]  17 Dec 2022\nﬁll this gap, our contribution is threefold. First, we in-\ntroduce CLIMATEBERT, a state-of-the-art language model\nthat is speciﬁcally pretrained on climate-related text cor-\npora of various sources, namely news, corporate disclosures,\nand scientiﬁc articles. This language model is designed to\nsupport researchers of various disciplines in obtaining bet-\nter performing NLP models for a manifold of downstream\ntasks in the climate change domain. Second, to illustrate\nthe strength of CLIMATEBERT, we highlight the perfor-\nmance improvements using CLIMATEBERT on three stan-\ndard climate-related NLP downstream tasks. Third, to fur-\nther promote research at the intersection of climate change\nand NLP, we make the training code and weights of all lan-\nguage models publicly available at GitHub and Hugging\nFace.12\n2\nBackground\nAs illustrated in Figure 1, our LM training approach for CLI-\nMATEBERT comprises all three phases — using an LM pre-\ntrained on a general domain, the domain-adaptive pretrain-\ning on the climate domain, and the training phase on climate-\nrelated downstream tasks.\nPretraining on General Domain\nAs of 2018, pretraining became the quasi-standard for learn-\ning NLP models. First, a neural language model, often with\nmillions of parameters, is trained on large unlabeled corpora\nin a semi-supervised fashion. By learning on multiple levels\nwhich words/word-sequences/sentences appear in the same\ncontext, an LM can represent a semantically similar text by\nsimilar vectors. Typical objectives for training LMs are the\nprediction of masked words or the prediction of a label indi-\ncating whether two sentences occurred consecutively in the\ncorpora (Devlin et al. 2018).\nIn the earlier NLP pretraining days, LMs tradition-\nally used regular or convolutional neural networks (Col-\nlobert and Weston 2008), or later Long-Short-Term-Memory\n(LSTM) networks to process text (Howard and Ruder 2018).\nTodays LMs mostly build on transformer models (e.g., De-\nvlin et al. 2018; Dai et al. 2019; Liu et al. 2019). One of\nthe latter is named ROBERTA (Liu et al. 2019) which was\ntrained on 160GB of various English-language corpora -\ndata from BOOKCORPUS (Zhu et al. 2015), WIKIPEDIA,\na portion of the CCNEWS dataset (Nagel 2016), OPEN-\nWEBTEXT corpus of web content extracted from URLs\nshared on Reddit (Gokaslan and Cohen 2019), and a sub-\nset of CommonCrawl that is said to resemble the story-like\nstyle of WINOGRAD schemas (Trinh and Le 2019). While\nthese sources are valuable to build a model working on gen-\neral language, it has been shown that domain-speciﬁc, niche\nlanguage (such as climate-related text) poses a problem to\ncurrent state-of-the-art language models (Araci 2019).\nDomain-Speciﬁc Pretraining\nAs a remedy to inferior performance of general language\nmodels when applied to niche topics, multiple language\n1www.github.com/climatebert/language-model\n2www.huggingface.co/climatebert\nmodels have been proposed which build on the pretrained\nmodels but continue pretraining on their respective domains.\nFinBERT, LegalBert, MedBert are just a few language mod-\nels that have been further pretrained on the ﬁnance, legal, or\nmedical domain (Araci 2019; Chalkidis et al. 2020; Rasmy\net al. 2021). In general, this domain-adaptive pretraining\nyields more accurate models on downstream tasks (Guru-\nrangan et al. 2020).\nDomain-speciﬁc pretraining requires a decision about\nwhich samples to include in the training process. It is still an\nopen debate which sample strategy improves performance\nbest. Various strategies can be applied to extract the text\nsamples on which the LM is further pretrained. For exam-\nple, while traditional pretraining uses all samples from the\npretraining corpus, similar sample selection (SIM-SELECT)\nuses only a subset of the corpus, in which the samples are\nsimilar to the samples in the downstream task (Ruder and\nPlank 2017). In contrast, diverse sample selection (DIV-\nSELECT) uses a subset of the corpus, which includes dissim-\nilar samples compared to the downstream dataset (Ruder and\nPlank 2017). Previous research has investigated the beneﬁt\nof these approaches, yet no ﬁnal conclusion about the efﬁ-\nciency has been obtained. Consequently, we compare these\napproaches in our experiments.\nNLP on Climate-Related Text\nIn the past, climate-related textual analysis often used pre-\ndeﬁned dictionaries of presumably relevant words and then\nsimply searched for these words within the documents.\nFor example, Cody et al. (2015) use such an approach\nfor climate-related tweets. Similarly, Sautner et al. (2022)\nuse a keyword-based approach to capture ﬁrm-level climate\nchange exposure. However, these methods do not account\nfor context. The lack of context is a signiﬁcant drawback,\ngiven the ambiguity of many climate-related words such\nas ”environment,” ”sustainable,” or ”climate” itself (Varini\net al. 2020).\nOnly recently, BERT has been used for NLP in climate-\nrelated text. The transformers-based BERT models are ca-\npable of accounting for the context of words and have out-\nperformed traditional approaches by large margins across\nvarious climate-related datasets (K¨olbel et al. 2020; Luc-\ncioni, Baylor, and Duchene 2020; Varini et al. 2020; Bin-\ngler et al. 2022a; Callaghan et al. 2021; Wang, Chillrud, and\nMcKeown 2021; Friederich et al. 2021; Stammbach et al.\n2022). However, this research has also shown that extracting\nclimate-related information from textual sources is a chal-\nlenge, as climate change is a complex, fast-moving, and of-\nten ambiguous topic with scarce resources for popular text-\nbased AI tasks.\nWhile context-based algorithms like BERT can detect\na variety of complex and implicit topic patterns in addi-\ntion to many trivial cases, there remains great potential\nfor improvement in several directions. To our knowledge,\nnone of the above cited work has examined the effects of\ndomain-adaptive pretraining on their speciﬁc downstream\ntasks. Therefore, we investigate whether domain-adaptive\npretraining will improve performance for climate change-\nrelated downstream tasks such as text classiﬁcation, senti-\nNews\nAbstracts\nReports\nCommon\ncrawl\nPretraining (general domain)\nDomain-adaptive pretraining (climate\ndomain)\nTraining (downstream tasks)\n+\n+\n- Text classification\n- Sentiment analysis\n- Fact-checking\nFigure 1: Sequence of training phases. Our main contribution is the continued pretraining of language models on the climate\ndomain. In addition, we evaluate the obtained climate domain-speciﬁc language models on various downstream tasks.\nment analysis, and fact-checking.\n3\nCLIMATEBERT\nIn the following, we describe our approach to train CLI-\nMATEBERT. We ﬁrst list the underlying data sources before\ndescribing our sample selection techniques and, ﬁnally, the\nvocabulary augmentation we used for training the language\nmodel.\nText Corpus\nOur goal was to collect a large corpus of text, CORP, that\nincluded general and domain-speciﬁc climate-related lan-\nguage. We decided to include the following three sources:\nnews articles, research abstracts, and corporate climate re-\nports. We decided not to include full research articles be-\ncause this language is likely too speciﬁc and does not rep-\nresent general climate language. We also did not include\nTwitter data, as we assume that these texts are too noisy. In\ntotal, we collected 2,046,523 paragraphs of climate-related\ntext (see Table 1).\nThe NEWS dataset is mainly retrieved from Reﬁnitiv\nWorkspace and includes 135,391 articles tagged with cli-\nmate change topics such as climate politics, climate actions,\nand ﬂoods and droughts. In addition, we crawled climate-\nrelated news articles from the web.\nThe ABSTRACTS dataset includes abstracts of climate-\nrelated research articles crawled from the Web of Science,\nprimarily published between 2000 and 2019.\nThe REPORTS dataset comprises corporate climate and\nsustainability reports of more than 600 companies from the\nyears 2015-2020 retrieved from Reﬁnitiv Workspace and the\nrespective company websites.\nGiven the nature of the datasets, we ﬁnd a large het-\nerogeneity between the paragraphs in terms of number\nof words. Unsurprisingly, on average, the paragraphs with\nthe least words come from the NEWS and the REPORTS\ndatasets. In contrast, ABSTRACTS includes paragraphs with\nthe most words. Table 1 lists these descriptives.\nTo estimate the beneﬁt from domain-adaptive pretrain-\ning, we compare the similarity of our text corpus with the\none used for pretraining ROBERTA. Following Gururangan\net al. (2020), we consider the vocabulary overlap between\nboth corpora. The resulting overlap of 57.05% highlights the\ndissimilarity between the two domains and the need to add\nspeciﬁc vocabularies. Therefore, we expect to see consid-\nerable performance improvements of domain-adaptive pre-\ntraining.\nDataset\nNum. of\nAvg. num. of words\nparagraphs\nQ1\nMean\nQ3\nNews\n1,025,412\n34\n56\n65\nAbstracts\n530,819\n165\n218\n260\nReports\n490,292\n34\n65\n79\nTotal\n2,046,523\n36\n107\n168\nTable 1: Corpus CORP used for pretraining CLIMATEBERT.\nQ1 and Q3 stand for the 0.25 and 0.75 quantiles, respec-\ntively.\nSample Selection\nPrior work has shown that speciﬁc selections of the samples\nused for pretraining can foster the performance of the LM. In\nparticular, incorporating information from the downstream\ntask by selecting similar or diverse samples has been shown\nto yield favorable results compared to using all samples from\nthe dataset. We follow both approaches and select samples\nthat are similar or diverse to climate-text using our text clas-\nsiﬁcation task (see 5). We experiment with three different\nstrategies from Ruder and Plank (2017) for the selection of\nsamples from our corpus:\n• In the most traditional sample selection strategy, FULL-\nSELECT, we use all paragraphs from CORP to train\nCLIMATEBERTF .\n• In SIM-SELECT, we select the 70% of samples from\nCORP, which are most similar to the samples of our text\nclassiﬁcation task. We use a Euclidean similarity met-\nric for this sample selection strategy. We call this LM\nCLIMATEBERTS.\n• In DIV-SELECT, we select the 70% of samples from\nCORP, which are most diverse compared to the samples\nfrom our text classiﬁcation task. We use the sum be-\ntween the type-token-ratio and the Shannon-entropy for\nmeasuring diversity (Ruder and Plank 2017). This LM is\nnamed CLIMATEBERTD.\n• In DIV-SELECT + SIM-SELECT, we use the same diver-\nsity and similarity metrics as before. We then compute\na composite score by summing over their scaled values.\nWe keep the 70% of the samples with the highest com-\nposite score to train CLIMATEBERTD+S.\nDownstream domain-\nDownstream tasks\nadaptive pretraining\ntraining\nHyperparameter\nValue\nBatch size\n2016\n32\nLearning rate\n5e-4\n5e-5\nNumber of epochs\n12\n1000\nPatience\n—\n4\nClass weight\n—\nBalanced\nFeedforward nonlinearity\n—\ntanh\nFeedforward layer\n—\n1\nOutput neurons\n—\nTask dependent\nOptimizer\nAdam\nAdam epsilon\n1e-6\nAdam beta weights\n(0.9, 0.999)\nLearning rate scheduler\nWarmup linear\nWeight decay\n0.01\nTable 2: Hyperparameters used for the downstream domain-\nadaptive pretraining and the downstream tasks training of\nCLIMATEBERT.\nVocabulary Augmentation\nWe extend the existing vocabulary of the original model\nto include domain-speciﬁc terminology. This allows CLI-\nMATEBERT to explicitly learn representations of terminol-\nogy that frequently occur in a climate-related text but not in\nthe general domain. In particular, we add the 235 most com-\nmon tokens as new tokens to the tokenizer, thereby extend-\ning the size of the vocabulary for our basis language model\n(DistilROBERTA) from 50,265 to 50,500. See Appendix C\nfor a list of all added tokens. We also experimented with\nlanguage models that do not use vocabulary augmentation\nor add more tokens. However, overall we ﬁnd improvements\nusing this technique and, thus, apply it to all language mod-\nels which we pretrain on the climate domain.\nModel Selection\nFor all our experiments, we use DistilROBERTA, a distilled\nversion of ROBERTA from Huggingface,3 as our starting\npoint for training (Sanh et al. 2019). All our language mod-\nels are trained with a masked language modeling objective\n(i.e., cross-entropy loss on predicting randomly masked to-\nkens). We report all hyperparameters in Table 2. The large\nbatch size of 2016 for training the LM is achieved using gra-\ndient accumulation.\nTraining on Downstream Task\nAfter pretraining DistilROBERTA on CORP, we follow\nstandard practice (Devlin et al. 2018) and pass the ﬁnal layer\n[CLS] token representation to a task-speciﬁc feedforward\nlayer for prediction. We report all hyperparameters of this\nfeedforward layer in Table 2.\n4\nPerformance Analysis of Language Model\nTable 3 lists the results after pretraining DistilROBERTA on\nCORP with various sample selection strategies. For evalu-\nation, we split CORP randomly into 80% training data and\n20% validation data. The reported loss is the cross-entropy\n3www.huggingface.co/distilroberta-base\nloss on predicting randomly masked tokens from the valida-\ntion data. We ﬁnd that CLIMATEBERTF leads to the lowest\nvalidation loss. This performance is followed by the other\nCLIMATEBERT LMs, which all show similar results. Over-\nall, we ﬁnd that our domain-adaptive pretraining decreases\nthe cross-entropy loss by 46–48% compared to the basis Dis-\ntilROBERTA, cutting the loss almost in half.\nModel\nVal. loss\nDistilROBERTA\n2.238\nCLIMATEBERTF\n1.157\nCLIMATEBERTS\n1.205\nCLIMATEBERTD\n1.204\nCLIMATEBERTD+S\n1.203\nTable 3: Loss of our language models on a validation set\nfrom our text corpus CORP.\n5\nPerformance Analysis for Climate-Related\nDownstream Tasks\nFor our experiments, we used the following downstream\ntasks: text classiﬁcation, sentiment analysis, and fact-\nchecking. Table 4 lists basic statistics about the downstream\ntasks. We repeated the training and evaluation phase 60\ntimes for each experiment, each time with a random 90%\nset of samples for training and the remaining 10% set for\nvalidation.\nDownstream\nNum. of\nLabels\nLabel\ntask\nsamples\ndistribution\nText classiﬁcation\n1220\nclimate-related: yes/no\n1000/220\nSentiment analysis\n1000\nopportunity/neutral/risk\n250/408/342\nFact-checking\n2745\nclaim: support/refute\n1943/802\nTable 4: Overview of our downstream tasks used for evalu-\nating CLIMATEBERT.\nText Classiﬁcation\nFor our text classiﬁcation experiment, we use a dataset con-\nsisting of hand-selected paragraphs from companies’ annual\nreports or sustainability reports. All paragraphs were anno-\ntated as yes (climate-related) or no (not climate-related) by at\nleast four experts from the ﬁeld using the software prodigy.4\nSee Appendix B for our annotation guidelines. In case of a\nclose verdict or a tie between the annotators, the authors of\nthis paper discussed the paragraph in depth before reaching\nan agreement.\nIn the following, Table 5 reports the results of the lan-\nguage models when trained on our text classiﬁcation task,\ni.e., whether the text is climate-related or not. Overall,\nwe ﬁnd that all CLIMATEBERT LMs outperform a non-\npre-trained DistilROBERTA across both metrics for the\ntext classiﬁcation task. Most notably, domain-adaptive pre-\ntraining with similar samples to our downstream tasks\n4www.prodi.gy\n(CLIMATEBERTS) leads to improvements of 32.64% in\nterms of cross-entropy loss and a reduction in the error rate\nof the F1 score by 35.71%.\nText classiﬁcation\nModel\nLoss\nF1\nDistilROBERTA\n0.2420.171\n0.9860.010\nCLIMATEBERTF\n0.1910.136\n0.9890.010\nCLIMATEBERTS\n0.1630.132\n0.9910.008\nCLIMATEBERTD\n0.1970.153\n0.9880.009\nCLIMATEBERTD+S\n0.2170.153\n0.9880.009\nTable 5: Results on our text classiﬁcation task. Reported are\nthe average cross-entropy loss and the average weighted F1\nscore on the validation sets across 60 evaluation runs. Value\nsubscripts report the standard deviations.\nSentiment Analysis\nOur next task studies the sentiment behind the climate-\nrelated paragraphs, using the same dataset as in the previ-\nous section. In our context, we use the term ‘sentiment’ to\ndistinguish whether an entity reports on climate-related de-\nvelopments as negative risk, as positive opportunity, or as\nneutral.\nTherefore, we created a second labeled dataset on climate-\nrelated sentiment, for which we asked the annotators to label\nthe paragraphs by one of the three categories — risk, neutral,\nor opportunity. See Appendix B for our annotation guide-\nlines. Similarly, as before, in case of a close verdict or a tie\nbetween the annotators, the authors of this paper discussed\nthe paragraph in depth before reaching an agreement.\nTable 6 shows the performance of our models in senti-\nment prediction. Again, all CLIMATEBERT LMs outperform\nthe DistilROBERTA baseline model in terms of F1 score and\naverage cross-entropy loss. The largest improvements can be\nobserved with CLIMATEBERTF , which amount to a 7.33%\nlower cross-entropy loss and a 7.42% lower error rate in\nterms of average F1 score compared to the DistilROBERTA\nbaseline LM.\nSentiment analysis\nModel\nLoss\nF1\nDistilROBERTA\n0.1500.069\n0.8250.046\nCLIMATEBERTF\n0.1390.042\n0.8380.036\nCLIMATEBERTS\n0.1400.057\n0.8360.033\nCLIMATEBERTD\n0.1380.043\n0.8350.040\nCLIMATEBERTD+S\n0.1390.043\n0.8340.036\nTable 6: Results on our sentiment analysis task in terms\nof average validation loss and average weighted F1 score\nacross 60 evaluation runs. Subscripts report the standard de-\nviations.\nFact-Checking\nWe now turn to the fact-checking downstream task. We ap-\nply our model to a dataset that was proposed by Diggelmann\net al. (2020) and comprises 1.5k sentences that make a claim\nabout climate-related topics. This CLIMATE-FEVER dataset\nis to the best of our knowledge to date the only dataset\nthat focuses on climate change fact-checking. CLIMATE-\nFEVER adapts the methodology of FEVER, the largest dataset\nof artiﬁcially designed claims, to real-life claims on cli-\nmate change collected online. The authors of CLIMATE-\nFEVER ﬁnd that the surprising, subtle complexity of mod-\neling real-world climate-related claims provides a valuable\nchallenge for general natural language understanding. Work-\ning with this dataset, Wang, Chillrud, and McKeown (2021)\nrecently introduced a novel semi-supervised training method\nto achieve a state-of-the-art (SotA) F1 score of 0.7182 on the\nfact-checking dataset CLIMATE-FEVER.\nClaim:\n97% consensus on human-caused\nglobal warming has been disproven.\nEvidence\nREFUTE\n:\nIn a 2019 CBS poll, 64% of the US\npopulation said that climate change\nis a ””crisis”” or a ””serious prob-\nlem””, with 44% saying human ac-\ntivity was a signiﬁcant contributor.\nClaim:\nThe melting Greenland ice sheet is\nalready a major contributor to ris-\ning sea level and if it was eventu-\nally lost entirely, the oceans would\nrise by six metres around the world,\nﬂooding many of the world’s largest\ncities.\nEvidence\nSUPPORT\n:\nThe Greenland ice sheet occupies\nabout 82% of the surface of Green-\nland, and if melted would cause sea\nlevels to rise by 7.2 metres.\nTable 7: Examples taken from CLIMATE-FEVER.\nEach claim in CLIMATE-FEVER is supported or refuted by\nevidence sentences (see Table 7), and an evidence sentence\ncan also be classiﬁed as giving not enough information. The\nobjective of the model is to classify an evidence sentence to\nsupport or refute a claim. To feed this combination of claim\nand evidence into the model, we concatenate the claims with\nthe related evidence sentences, with a [SEP] token sepa-\nrating them. As in Wang, Chillrud, and McKeown (2021),\nand for comparison with their results, we ﬁlter out all evi-\ndence sentences with the label NOT ENOUGH INFO in the\nCLIMATE-FEVER dataset.\nTable 8 lists the results of our experiments on the\nCLIMATE-FEVER dataset. In line with our previous exper-\niments, we ﬁnd similar or better results for all CLIMATE-\nBERT LMs across all metrics. Our CLIMATEBERTD+S LM\nachieves similar cross-entropy loss compared to the basis\nDistilROBERTA model, yet pushes the average F1 score\nfrom 0.748 to 0.757, which outperforms Wang, Chillrud, and\nMcKeown (2021)’s previous SotA F1 score of 0.7182, and\nis hence, to the best of our knowledge, the new SotA on this\ndataset.\nFact-checking\nModel\nLoss\nF1\nDistilROBERTA\n0.1350.017\n0.7480.036\nCLIMATEBERTF\n0.1340.020\n0.7550.037\nCLIMATEBERTS\n0.1330.017\n0.7530.042\nCLIMATEBERTD\n0.1350.016\n0.7520.042\nCLIMATEBERTD+S\n0.1350.018\n0.7570.044\nTable 8: Results on our fact-checking task on CLIMATE-\nFEVER in terms of average validation loss and average\nweighted F1 score across 60 evaluation runs. Subscripts re-\nport the standard deviations.\n6\nCarbon Footprint\nTraining deep neural networks in general and large lan-\nguage models in particular, has a signiﬁcant carbon footprint\nalready today. If the LM research trends continue, this detri-\nmental climate impact will increase considerably. The topic\nof efﬁcient NLP was also discussed by a working group\nappointed by the ACL Executive Committee to promote\nways that the ACL community can reduce the computational\ncosts of model training (https://public.ukp.informatik.tu-\ndarmstadt.de/enlp/Efﬁcient-NLP-policy-document.pdf).\nWe acknowledge that our work is part of this trend. In\ntotal, training CLIMATEBERT caused 115.15 kg CO2 emis-\nsions. We use two energy efﬁcient NVIDIA RTX A5000\nGPUs: 0.7 kW (power consumption of GPU server) x 350\nhours (combined training time of all experiments) x 470\ngCO2e/kWh (emission factor in Germany in 2018 according\nto\nwww.umweltbundesamt.de/publikationen/entwicklung-\nder-speziﬁschen-kohlendioxid-7) = 115,149 gCO2e. We\nlist all details about our climate impact in Table 9 in\nAppendix A. Nevertheless, we decided to carry out this\nproject, as we see the high potential of NLP to support\naction against climate change. Given our awareness of the\ncarbon footprint of our research, we address this sensitive\ntopic as follows:\n1. We speciﬁcally decided to focus on DistilROBERTA,\nwhich is a considerably smaller model in terms of num-\nber of parameters compared to the non-distilled version\nand, thus, requires less energy to train. Moreover, we do\nnot crawl huge amounts of data without considering the\nquality. This way, we try to take into account the issues\nmentioned by Bender et al. (2021).\n2. Hyperparameter tuning yields considerably higher CO2\nemissions in the training stage due to tens or hundreds\nof different training runs. Note that our multiple train-\ning runs on the downstream task are not causing long\ntraining times as the downstream datasets are very small\ncompared to the dataset used for training the language\nmodel. We therefore refrain from exhaustive hyperpa-\nrameter tuning. Rather, we build on previous ﬁndings.\nWe systematically experimented with a few hyperparam-\neter combinations and found that the hyperparameters\nproposed by Gururangan et al. (2020) lead to the best\nresults.\n3. We would have liked to train and run our model on\nservers powered by renewable energy. This ﬁrst best op-\ntion was unfortunately not available. In order to speed\nup the energy system transformation required to achieve\nthe global climate targets, we contribute our part by do-\nnating Euro 100 to atmosfair. atmosfair was founded in\n2005 and is supported by the German Federal Environ-\nment Agency. atmosfair offsets carbon dioxide in more\nthan 20 locations: from efﬁcient cookstoves in Nigeria,\nEthiopia and India to biogas plants in Nepal and Thai-\nland to solar energy in Senegal and Brazil and renewable\nenergies in Tansania and Indonesia. See www.atmosfair.\nde/en/offset/ﬁx/. We explicitly refrain from calling this\ndonation a CO2 compensation, and we refrain from a so-\nlution that is based on afforestation.\n7\nConclusion\nWe propose CLIMATEBERT, the ﬁrst language model that\nwas pretrained on a large scale dataset of over 2 mil-\nlion climate-related paragraphs. We study various selec-\ntion strategies to ﬁnd samples from our corpus which are\nmost helpful for later tasks. Our experiments reveal that\nour domain-adaptive pretraining leads to considerably lower\nmasked language modeling loss on our climate corpus. We\nfurther ﬁnd that this improvement is also reﬂected in predic-\ntive performance across three essential downstream climate-\nrelated NLP tasks: text classiﬁcation, the analysis of risk and\nopportunity statements by corporations, and fact-checking\nclimate-related claims.\nAcknowledgments\nWe are very thankful to Jan Minx and Max Callaghan from\nthe Mercator Research Institute on Global Commons and\nClimate Change (MCC) Berlin for providing us with the\ndata, which is a subset of the data they used in Berrang-Ford\net al. (2021) and Callaghan et al. (2021).\nReferences\nAraci, D. 2019.\nFinbert: Financial sentiment analy-\nsis with pre-trained language models.\narXiv preprint\narXiv:1908.10063.\nBender, E. M.; Gebru, T.; McMillan-Major, A.; and\nShmitchell, S. 2021.\nOn the Dangers of Stochastic Par-\nrots: Can Language Models Be Too Big? In Proceedings\nof the 2021 ACM Conference on Fairness, Accountability,\nand Transparency, 610–623.\nBerrang-Ford, L.; Sietsma, A. J.; Callaghan, M.; Minx, J. C.;\nScheelbeek, P. F.; Haddaway, N. R.; Haines, A.; and Dan-\ngour, A. D. 2021. Systematic mapping of global research on\nclimate and health: a machine learning review. The Lancet\nPlanetary Health, 5(8): e514–e525.\nBingler, J. A.; Kraus, M.; Leippold, M.; and Webersinke,\nN. 2022a. Cheap talk and cherry-picking: What CLIMATE-\nBERT has to say on corporate climate risk disclosures. Fi-\nnance Research Letters, 102776.\nBingler, J. A.; Kraus, M.; Leippold, M.; and Webersinke, N.\n2022b. Cheap talk in corporate climate commitments: The\nrole of active institutional ownership, signaling, materiality,\nand sentiment. Swiss Finance Institute Research Paper.\nCallaghan, M.; Schleussner, C.-F.; Nath, S.; Lejeune, Q.;\nKnutson, T. R.; Reichstein, M.; Hansen, G.; Theokritoff, E.;\nAndrijevic, M.; Brecha, R. J.; et al. 2021. Machine-learning-\nbased evidence and attribution mapping of 100,000 climate\nimpact studies. Nature climate change, 11(11): 966–972.\nChalkidis, I.; Fergadiotis, M.; Malakasiotis, P.; Aletras, N.;\nand Androutsopoulos, I. 2020. LEGAL-BERT: The muppets\nstraight out of law school. arXiv preprint arXiv:2010.02559.\nClark, K.; Luong, M.-T.; Le, Q. V.; and Manning, C. D.\n2020. ELECTRA: Pre-training Text Encoders as Discrimi-\nnators Rather Than Generators. In International Conference\non Learning Representations.\nCody, E. M.; Reagan, A. J.; Mitchell, L.; Dodds, P. S.; and\nDanforth, C. M. 2015. Climate Change Sentiment on Twit-\nter: An Unsolicited Public Opinion Poll. PLOS ONE, 10(8):\ne0136092.\nCollobert, R.; and Weston, J. 2008. A Uniﬁed Architecture\nfor Natural Language Processing: Deep Neural Networks\nwith Multitask Learning. In Proceedings of the 25th Inter-\nnational Conference on Machine Learning, 160–167.\nDai, Z.; Yang, Z.; Yang, Y.; Carbonell, J. G.; Le, Q.; and\nSalakhutdinov, R. 2019.\nTransformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978–2988.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDiggelmann, T.; Boyd-Graber, J.; Bulian, J.; Ciaramita, M.;\nand Leippold, M. 2020. CLIMATE-FEVER: A Dataset for\nVeriﬁcation of Real-World Climate Claims. arXiv preprint\narXiv:2012.00614.\nFriederich, D.; Kaack, L. H.; Luccioni, A.; and Steffen,\nB. 2021.\nAutomated Identiﬁcation of Climate Risk Dis-\nclosures in Annual Corporate Reports.\narXiv preprint\narXiv:2108.01415.\nGokaslan, A.; and Cohen, V. 2019. OpenWebText Corpus.\nGr¨uning, M. 2011. Artiﬁcial intelligence measurement of\ndisclosure (AIMD). European Accounting Review, 20(3):\n485–519.\nGururangan, S.; Marasovi´c, A.; Swayamdipta, S.; Lo, K.;\nBeltagy, I.; Downey, D.; and Smith, N. A. 2020. Don’t Stop\nPretraining: Adapt Language Models to Domains and Tasks.\nIn Proceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, 8342–8360.\nHershcovich, D.; Webersinke, N.; Kraus, M.; Bingler, J. A.;\nand Leippold, M. 2022. Towards Climate Awareness in NLP\nResearch. arXiv preprint arXiv:2205.05071.\nHoward, J.; and Ruder, S. 2018. Universal Language Model\nFine-tuning for Text Classiﬁcation. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 328–339.\nKim, D.-Y.; and Kang, S.-W. 2018. Analysis of Recognition\nof Climate Changes using Word2Vec. International Journal\nof Pure and Applied Mathematics, 120(6): 5793–5807.\nK¨olbel, J. F.; Leippold, M.; Rillaerts, J.; and Wang, Q. 2020.\nAsk BERT: How regulatory disclosure of transition and\nphysical climate risks affects the CDS term structure. Avail-\nable at SSRN 3616324.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and\nKang, J. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinfor-\nmatics, 36(4): 1234–1240.\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019.\nROBERTA: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692.\nLuccioni, A.; Baylor, E.; and Duchene, N. 2020. Analyzing\nSustainability Reports Using Natural Language Processing.\narXiv preprint arXiv:2011.08073.\nNagel, S. 2016. CC-NEWS.\nRasmy, L.; Xiang, Y.; Xie, Z.; Tao, C.; and Zhi, D. 2021.\nMed-BERT: pretrained contextualized embeddings on large-\nscale structured electronic health records for disease predic-\ntion. NPJ digital medicine, 4(1): 1–13.\nRuder, S.; and Plank, B. 2017. Learning to select data for\ntransfer learning with Bayesian Optimization. In Proceed-\nings of the 2017 Conference on Empirical Methods in Natu-\nral Language Processing, 372–382.\nSanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108.\nSautner, Z.; van Lent, L.; Vilkov, G.; and Zhang, R. 2022.\nFirm-level Climate Change Exposure. Journal of Finance,\nforthcoming.\nStammbach, D.; Webersinke, N.; Bingler, J. A.; Kraus, M.;\nand Leippold, M. 2022. A Dataset for Detecting Real-World\nEnvironmental Claims. arXiv preprint arXiv:2209.00507.\nTrinh, T. H.; and Le, Q. V. 2019. A Simple Method for Com-\nmonsense Reasoning. arXiv preprint arXiv:1806.02847.\nVarini, F. S.; Boyd-Graber, J.; Ciaramita, M.; and Leippold,\nM. 2020. ClimaText: A dataset for climate change topic de-\ntection. In Tackling Climate Change with Machine Learning\n(Climate Change AI) workshop at NeurIPS.\nWang, G.; Chillrud, L.; and McKeown, K. 2021. Evidence\nbased Automatic Fact-Checking for Climate Change Misin-\nformation. International Workshop on Social Sensing on The\nInternational AAAI Conference on Web and Social Media.\nZhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,\nR.; Torralba, A.; and Fidler, S. 2015. Aligning books and\nmovies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE in-\nternational conference on computer vision, 19–27.\nAppendix\nA\nClimate Performance Model Card\nTable 9 shows our climate performance model card, follow-\ning Hershcovich et al. (2022).\nClimateBert\n1. Model publicly available?\nYes\n2. Time to train ﬁnal model\n48 hours\n3. Time for all experiments\n350 hours\n4. Power of GPU and CPU\n0.7 kW\n5. Location for computations\nGermany\n6. Energy mix at location\n470 gCO2eq/kWh\n7. CO2eq for ﬁnal model\n15.79 kg\n8. CO2eq for all experiments\n115.15 kg\n9. Average CO2eq for inference per sample\n0.62 mg\nTable 9: Climate performance model card for ClimateBert.\nB\nAnnotation Guidelines\nFor our annotation procedure, we implemented the fol-\nlowing general rules. The annotators had to label climate-\nrelevant paragraphs. If the paragraph was climate-relevant,\nthen they had to attach a sentiment to a paragraph. Annota-\ntors were asked to apply common sense, e.g., when a given\nparagraph might not provide all the context, but the context\nmight seem obvious. Moreover, annotators were informed\nthat each annotation should be a 0-1 decision. Hence, if\nan annotator was 70% certain, then this was rounded up to\n100%. We asked, on average, ﬁve researchers to annotate the\nsame tasks to obtain some measure of dispersion. In case of\na close verdict or a tie between the annotators, the authors of\nthis paper discussed the paragraph in depth before reaching\nan agreement.\nText classiﬁcation\nThe ﬁrst task was to label climate-relevant paragraphs. The\nlabels are Yes or No. As a general rule, we determined that\njust discussing nature/environment can be sufﬁcient, and\nmentioning clean energy, emissions, fossil fuels, etc., can\nalso be sufﬁcient. It is a Yes, if the paragraph includes some\nwording on a climate change or environment related topic\n(including transition and litigation risks, i.e., emission mit-\nigation measures, energy consumption and energy sources\netc.; and physical risks, i.e., increase in risk of ﬂoods, coastal\narea exposure, storms etc.). It is a No, if the paragraph is not\nrelated to climate policy, climate change or an environmen-\ntal topic at all. For some examples, see Table 10.\nSentiment Analysis\nFor the sentiment analysis, annotators had to provide la-\nbels as to whether a (climate change-related) paragraph talks\nabout a Risk or threat that negatively impacts an entity of in-\nterest, i.e. a company (negative sentiment), or whether an en-\ntity is referring to some Opportunity arising due to climate\nchange (positive sentiment). The paragraph can also make\njust a Neutral statement.\nLabel\nExamples\nYes\nSustainability: The Group is subject\nto stringent and evolving laws, reg-\nulations, standards and best prac-\ntices in the area of sustainabil-\nity (comprising corporate gover-\nnance, environmental management\nand climate change (speciﬁcally\ncapping of emissions), health and\nsafety management and social per-\nformance) which may give rise\nto increased ongoing remediation\nand/or other compliance costs and\nmay adversely affect the Group’s\nbusiness, results of operations, ﬁ-\nnancial condition and/or prospects.\nYes\nScope 3: Optional scope that in-\ncludes indirect emissions associ-\nated with the goods and services\nsupply chain produced outside the\norganization. Included are emis-\nsions from the transport of products\nfrom our logistics centres to stores\n(downstream) performed by exter-\nnal logistics operators (air, land\nand sea transport) as well as the\nemissions associated with electric-\nity consumption in franchise stores.\nNo\nRisk and risk management Opera-\ntional risk and compliance risk Op-\nerational risk is the risk of loss re-\nsulting from inadequate or failed\ninternal processes, people and sys-\ntems, or from external events in-\ncluding legal risk but excluding\nstrategic and reputation risk. It also\nincludes, among other things, tech-\nnology risk, model risk and out-\nsourcing risk.\nTable 10: Examples for the annotation task climate\n(Yes/No).\nTo be more precise, we consider a paragraph relating to\nrisk, if the paragraph mainly talks about 1) business down-\nside risks, potential losses and adverse developments detri-\nmental to the entity 2) and/or about negative impact of an\nentity’s activities on the society/environment 3) and/or asso-\nciates speciﬁc negative adjectives to the anticipated, past or\npresent developments and topics covered.\nWe consider a paragraph relating to opportunities, if the\nparagraph mainly talks about 1) business opportunities aris-\ning from mitigating climate change, from adapting to cli-\nmate change etc. which might be beneﬁcial for a speciﬁc\nentity 2) and/or about positive impact of an entity’s activi-\nties on the society/environment 3) and/or associates speciﬁc\npositive adjectives to the anticipated, past or present devel-\nopments and topics covered.\nLastly, we consider a paragraph as neutral if it mainly\nstates facts and developments 1) without putting them into\npositive or negative perspective for a speciﬁc entity and/or\nthe society and/or the environment, 2) and/or does not as-\nsociate speciﬁc positive or negative adjectives to the antic-\nipated, past or present facts stated and topics covered. For\nsome examples, see Table 11.\nC\nAdded Tokens\n’CO2’, ’emissions’, ”’, ’temperature’, ’environmental’,\n’soil’, ’increase’, ’conditions’, ’potential’, ’increased’, ’ar-\neas’, ’degrees’, ’across’, ’systems’, ’emission’, ’precipi-\ntation’, ’impacts’, ’compared’, ’countries’, ’sustainable’,\n’provide’, ’reduction’, ’annual’, ’reduce’, ’greenhouse’,\n’approach’, ’processes’, ’factors’, ’observed’, ’renewable’,\n’temperatures’, ’distribution’, ’studies’, ’variability’, ’sig-\nniﬁcantly’, ’–’, ’further’, ’regions’, ’addition’, ’showed’,\n’“’, ’industry’, ’consumption’, ’regional’, ’risks’, ’atmo-\nspheric’, ’supply’, ’companies’, ’plants’, ’biomass’, ’elec-\ntricity’, ’respectively’, ’activities’, ’communities’, ’cli-\nmatic’, ’solar’, ’investment’, ’spatial’, ’rainfall’, ’•’, ’sus-\ntainability’, ’costs’, ’reduced’, ’2021’, ’inﬂuence’, ’vegeta-\ntion’, ’sources’, ’possible’, ’ecosystem’, ’scenarios’, ’sum-\nmer’, ’drought’, ’structure’, ’economy’, ’considered’, ’var-\nious’, ’atmosphere’, ’several’, ’technologies’, ’transition’,\n’assessment’, ’dioxide’, ’ocean’, ’fossil’, ’patterns’, ’waste’,\n’solutions’, ’transport’, ’strategy’, ’CH4’, ’policies’, ’un-\nderstanding’, ’concentration’, ’customers’, ’methane’, ’ap-\nplied’, ’increases’, ’estimated’, ’ﬂood’, ’measured’, ’ther-\nmal’, ’concentrations’, ’decrease’, ’greater’, ’following’,\n’proposed’, ’trends’, ’basis’, ’provides’, ’operations’, ’dif-\nferences’, ’hydrogen’, ’adaptation’, ’methods’, ’capture’,\n’variation’, ’reducing’, ’N2O’, ’parameters’, ’ecosystems’,\n’investigated’, ’yield’, ’strategies’, ’indicate’, ’caused’, ’dy-\nnamics’, ’obtained’, ’efforts’, ’coastal’, ’become’, ’agri-\ncultural’, ’decreased’, ’GHG’, ’materials’, ’mainly’, ’rela-\ntionship’, ’ecological’, ’beneﬁts’, ’+/-’, ’challenges’, ’nitro-\ngen’, ’forests’, ’trend’, ’estimates’, ’towards’, ’Committee’,\n’seasonal’, ’developing’, ’particular’, ’importance’, ’tropi-\ncal’, ’ratio’, ’2030’, ’composition’, ’employees’, ’charac-\nteristics’, ’scenario’, ’measurements’, ’plans’, ’fuels’, ’in-\nfrastructure’, ’overall’, ’responses’, ’presented’, ’least’, ’as-\nsess’, ’diversity’, ’periods’, ’delta’, ’included’, ’already’,\n’targets’, ’achieve’, ’affect’, ’conducted’, ’operating’, ’pop-\nulations’, ’variations’, ’studied’, ’additional’, ’construction’,\n’northern’, ’variables’, ’soils’, ’ensure’, ’recovery’, ’com-\nbined’, ’decision’, ’practices’, ’however’, ’determined’, ’re-\nsulting’, ’mitigation’, ’conservation’, ’estimate’, ’identify’,\n’observations’, ’losses’, ’productivity’, ’agreement’, ’mon-\nitoring’, ’investments’, ’pollution’, ’contribution’, ’oppor-\ntunities’, ’simulations’, ’gases’, ’statements’, ’planning’,\n’shares’, ’sediment’, ’ﬂux’, ’requirements’, ’trees’, ’tempo-\nral’, ’determine’, ’southern’, ’previous’, ’integrated’, ’rel-\natively’, ’analyses’, ’means’, ’2050’, ’”’, ’uncertainty’,\n’pandemic’, ’ﬂuxes’, ’ﬁndings’, ’moisture’, ’consistent’,\n’decades’, ’snow’, ’performed’, ’contribute’, ’crisis’\nLabel\nExamples\nOpportunity\nGrid & Infrastructure and Retail – today represent\nthe energy world of tomorrow. We rank among Eu-\nrope‘s market leaders in the grid and retail busi-\nness and have leading positions in renewables. We\nintend to spend a total of between Euro 6.5 bil-\nlion and Euro 7.0 billion in capital throughout the\nGroup from 2017 to 2019.\nOpportunity\nWe want to contribute to the transition to a circu-\nlar economy. The linear economy is not sustain-\nable. We discard a great deal (waste and there-\nfore raw materials, experience, social capital and\nknowledge) and are squandering value as a result.\nThis is not tenable from an economic and ecolog-\nical perspective. As investor we can ‘direct’ com-\npanies and with our network, our scale and our in-\nﬂuence we can help the movement towards a cir-\ncular future (creating a sustainable society) further\nalong.\nNeutral\nA similar approach could be used for allocating\nemissions in the fossil fuel electricity supply chain\nbetween coal miners, transporters and generators.\nWe don’t invest in fossil fuel companies, but those\ninvestors who do should account properly for their\nrole in the production of dangerous emissions from\nburning fossil fuels.\nNeutral\nOmissions: Emissions associated with joint ven-\ntures and investments are not included in the emis-\nsions disclosure as they fall outside the scope of our\noperational boundary. We do not have any emis-\nsions associated with heat, steam or cooling. We\nare not aware of any other material sources of omis-\nsions from our emissions reporting.\nRisk\nWe estimated that between 36.5 and 52.9 per cent\nof loans granted to our clients are exposed to tran-\nsition risks. If the regulator decides to pass am-\nbitious laws to accelerate the transition towards a\nlow-carbon economy, carbon-intensive companies\nwould incur in higher costs, which may prevent\nthem from repaying their debt. In turn, this would\nweaken our bank’s balance sheets. .\nRisk\nAmerican National Insurance Company recognizes\nthat increased claims activity resulting from catas-\ntrophic events, whether natural or man-made, may\nresult in signiﬁcant losses, and that climate change\nmay also affect the affordability and availability of\nproperty and casualty insurance and the pricing for\nsuch products.\nTable 11: Examples for the annotation task sentiment (Op-\nportunity/Neutral/Risk).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-10-22",
  "updated": "2022-12-17"
}