{
  "id": "http://arxiv.org/abs/2110.14020v1",
  "title": "The Difficulty of Passive Learning in Deep Reinforcement Learning",
  "authors": [
    "Georg Ostrovski",
    "Pablo Samuel Castro",
    "Will Dabney"
  ],
  "abstract": "Learning to act from observational data without active environmental\ninteraction is a well-known challenge in Reinforcement Learning (RL). Recent\napproaches involve constraints on the learned policy or conservative updates,\npreventing strong deviations from the state-action distribution of the dataset.\nAlthough these methods are evaluated using non-linear function approximation,\ntheoretical justifications are mostly limited to the tabular or linear cases.\nGiven the impressive results of deep reinforcement learning, we argue for a\nneed to more clearly understand the challenges in this setting.\n  In the vein of Held & Hein's classic 1963 experiment, we propose the \"tandem\nlearning\" experimental paradigm which facilitates our empirical analysis of the\ndifficulties in offline reinforcement learning. We identify function\napproximation in conjunction with fixed data distributions as the strongest\nfactors, thereby extending but also challenging hypotheses stated in past work.\nOur results provide relevant insights for offline deep reinforcement learning,\nwhile also shedding new light on phenomena observed in the online case of\nlearning control.",
  "text": "The Difﬁculty of Passive Learning\nin Deep Reinforcement Learning\nGeorg Ostrovski\nDeepMind\nostrovski@deepmind.com\nPablo Samuel Castro\nGoogle Research, Brain Team\npsc@google.com\nWill Dabney\nDeepMind\nwdabney@deepmind.com\nAbstract\nLearning to act from observational data without active environmental interaction\nis a well-known challenge in Reinforcement Learning (RL). Recent approaches\ninvolve constraints on the learned policy or conservative updates, preventing strong\ndeviations from the state-action distribution of the dataset. Although these methods\nare evaluated using non-linear function approximation, theoretical justiﬁcations\nare mostly limited to the tabular or linear cases. Given the impressive results of\ndeep reinforcement learning, we argue for a need to more clearly understand the\nchallenges in this setting. In the vein of Held & Hein’s classic 1963 experiment,\nwe propose the “tandem learning” experimental paradigm which facilitates our\nempirical analysis of the difﬁculties in ofﬂine reinforcement learning. We identify\nfunction approximation in conjunction with ﬁxed data distributions as the strongest\nfactors, thereby extending but also challenging hypotheses stated in past work. Our\nresults provide relevant insights for ofﬂine deep reinforcement learning, while also\nshedding new light on phenomena observed in the online case of learning control.\n1\nIntroduction\nLearning to act in an environment purely from observational data (i.e. with no environment interac-\ntion), usually referred to as ofﬂine reinforcement learning, has great practical as well as theoretical\nimportance (see [Levine et al., 2020] for a recent survey). In real-world settings like robotics and\nhealthcare, it is motivated by the ambition to learn from existing datasets and the high cost of en-\nvironment interaction. Its theoretical appeal is that stationarity of the data distribution allows for\nmore straightforward convergence analysis of learning algorithms. Moreover, decoupling learning\nfrom data generation alleviates one of the major difﬁculties in the empirical analysis of common\nreinforcement learning agents, allowing the targeted study of learning dynamics in isolation from\ntheir effects on behavior.\nRecent work has identiﬁed extrapolation error as a major challenge for ofﬂine (deep) reinforcement\nlearning [Achiam et al., 2019, Buckman et al., 2021, Fujimoto et al., 2019b, Fakoor et al., 2021,\nLiu et al., 2020, Nair et al., 2020], with bootstrapping often highlighted as either the cause or an\nampliﬁer of the effect: The value of missing or under-represented state-action pairs in the dataset can\nbe over-estimated, either transiently (due to insufﬁcient training or data) or even asymptotically (due\nto modelling or dataset bias), resulting in a potentially severely under-performing acquired policy.\nThe corrective feedback-loop [Kumar et al., 2020b], whereby value over-estimation is self-correcting\nvia exploitation during interaction with the environment (while under-estimation is corrected by\nexploration), is critically missing in the ofﬂine setting.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2110.14020v1  [cs.LG]  26 Oct 2021\nObservation\nAction\nTandem\nForked Tandem\nEnvironment\nActive\nPassive\nPassive\nActive\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\nSpace invaders\nactive\npassive\nFigure 1: (top-left) Held and Hein [1963] experiment setup. (top-right) Illustrations of the Tandem\nand Forked Tandem experiment setups. (bottom) Tandem (active and passive) performance on 4\nselected Atari domains. In all ﬁgures, active agent performance is shown in gray.\nTo mitigate this, typically one of a few related strategies are proposed: policy or learning update\nconstraints preventing deviations from states and actions well-covered by the dataset or satisfying\ncertain uncertainty bounds [Fujimoto et al., 2019a,b, Kumar et al., 2019, 2020c, Achiam et al., 2019,\nWang et al., 2020b, Wu et al., 2021, Nair et al., 2020, Wu et al., 2019, Yu et al., 2020], pessimism\nbias to battle value over-estimation [Buckman et al., 2021, Kidambi et al., 2020], large and diverse\ndatasets to improve state space coverage [Agarwal et al., 2020], or learned models to ﬁll in gaps\nwith synthesized data [Schrittwieser et al., 2021, Matsushima et al., 2020]. While many of these\nenjoy theoretical justiﬁcation in the tabular or linear cases [Thomas et al., 2015], guarantees for the\npractically relevant non-linear case are mostly lacking.\nIn this paper we draw inspiration from the experimental paradigm introduced in the classic Held and\nHein [1963] experiment in psychology. The experiment involved coupling two young animal subjects’\nmovements and visual perceptions to ensure that both receive the same stream of visual inputs,\nwhile only one can actively shape that stream by directing the pair’s movements (Fig. 1, top-left).\nBy showing that, despite identical visual experiences, only the actively moving subject acquired\nadequate visual acuity, the experiment established the importance of active locomotion in learning\nvision. Analogously, we introduce the ‘Tandem RL’ setup, pairing an ‘active’ and a ‘passive’ agent\nin a training loop where only the active agent drives data generation, while both perform identical\nlearning updates from the generated data1. By decoupling learning dynamics from its impact on data\ngeneration, while preserving the non-stationarity of the online learning setting, this experimental\nparadigm promises to be a valuable analytic tool for the precise empirical study of RL algorithms.\nHolding architectures, losses, and crucially data distribution equal across the active and passive agents,\nor varying them in a controlled manner, we perform a detailed empirical analysis of the failure modes\nof passive (i.e. non-interactive, ofﬂine) learning, and pinpoint the contributing factors in properties of\nthe data distribution, function approximation and learning algorithm. Our study conﬁrms some past\nintuitions for the failure modes of ofﬂine learning, while reﬁning and extending the ﬁndings in the\ndeep RL case. In particular, our results indicate an empirically less critical role for bootstrapping\nthan previously hypothesized, while foregrounding erroneous extrapolation or over-generalization\nby a function approximator trained on an inadequate data distribution as the crucial challenge.\nAmong other things, our experiments draw a sharp boundary between the mostly well-behaved (and\nanalytically well-studied) case of linear function approximation, and the non-linear case for which\ntheoretical guarantees are lacking. Moreover, we delineate different, more and less effective, ways of\nenhancing the training data distribution to support successful ofﬂine learning, e.g. by analysing the\nimpact of dataset size and diversity, the stochasticity of the data generating policy, or small amounts of\nself-generated data. Our results provide hints towards a hypothesis relevant in both ofﬂine and online\nRL: robust learning of control with function approximation may require interactivity not merely as\na data gathering mechanism, but as a counterbalance to a (sufﬁciently expressive) approximator’s\ntendency to ‘exploit gaps’ in an arbitrary ﬁxed data distribution by excessive extrapolation.\n1The ‘tandem’ analogy is of course that of two riders, both of whom experience the same route, while only\nthe front rider gets to decide on direction.\n2\n2\nThe Experimental Paradigm of Tandem Reinforcement Learning\nThe Tandem RL setting, extending a similar analytic setup in [Fujimoto et al., 2019b], consists of two\nlearning agents, one of which (the ‘active agent’) performs the usual online training loop of interacting\nwith an environment and learning from the generated data, while the other (the ‘passive agent’) learns\nsolely from data generated by the active agent, while only interacting with the environment for\nevaluation. We distinguish two experimental paradigms (see Fig. 1, top-right):\nTandem: Active and passive agents start with independently initialized networks, and train on an\nidentical sequence of training batches in the exact same order.\nForked Tandem: An agent is trained for a fraction of its total training budget. It is then ‘forked’ into\nactive and passive agents, which start out with identical network weights. The active agent is ‘frozen’,\ni.e. receives no further training, but continues to generate data from its policy. The passive agent is\ntrained on this generated data for the remainder of the training budget.\n2.1\nImplementation\nOur basic experimental agent is ‘Tandem DQN’, an active/passive pair of Double-DQN agents2 [van\nHasselt et al., 2016]. Following the usual training protocol [Mnih et al., 2015], the total training\nbudget is 200 iterations, each of which consists of 1M steps taken on the environment by the active\nagent, interspersed with regular learning updates (on one, or concurrently on both agents, depending\non the paradigm), on batches of transitions sampled from the active agent’s replay buffer. Both agents\nare independently evaluated on the environment for 500K steps after each training iteration.\nMost of our experiments are performed on the Atari domain [Bellemare et al., 2013], using the\nexact algorithm and hyperparameters from [van Hasselt et al., 2016]. We use a ﬁxed set of four\nrepresentative games to demonstrate most of our empirical results, two of which (BREAKOUT, PONG)\ncan be thought of as easy and largely solved by baseline agents, while the others (SEAQUEST, SPACE\nINVADERS) have non-trivial learning curves and remain challenging. Unless stated otherwise, all\nresults show averages over at least 5 seeds, with conﬁdence intervals indicating variation over seeds.\nIn comparative plots, boldface entries indicate the default Tandem DQN conﬁguration, and gray\nlines always correspond to the active agent’s performance.\n2.2\nThe Tandem Effect\nWe begin by reproducing the striking observation in [Fujimoto et al., 2019b] that the passive learner\ngenerally fails to adequately learn from the very data stream that is demonstrably sufﬁcient for its\narchitecturally identical active counterpart; we refer to this phenomenon as the ‘tandem effect’ (Fig. 1,\nbottom). We ascertain the generality of this ﬁnding by replicating it across a broad suite of environ-\nments and agent architectures: Double-DQN on 57 Atari environments (Appendix Figs. 10 & 11),\nadapted agent variants on four Classic Control domains from the OpenAI Gym library [Brockman\net al., 2016] and the MinAtar domain [Young and Tian, 2019] (Appendix Figs. 12 & 15), and\nthe distributed R2D2 agent [Kapturowski et al., 2019] (Appendix Fig. 14). Details on agents and\nenvironments are provided in the Appendix3.\nEmpirically, we make the informal observation that while active and passive Q-networks tend to\nproduce similar values for typical state-action pairs under the active policy (where the action is\nthe active Q-value function’s argmax for a given state), their values are less correlated for other\n(non-argmax) actions, and in fact the active and passive greedy policies of a Tandem DQN tend to\ndisagree in a large fraction of states under the behavior distribution (on average > 75% of states,\nafter 100M steps of training, across 57 Atari games; see Appendix Fig. 13). Moreover, in a fraction\n(≈12/57) of Atari games, we observe the passive agent’s network to strongly over-estimate a fraction\nof state-action values, with the over-estimation growing as training progresses.\n2Our choice of Double-DQN as a baseline is motivated by its relatively strong performance and robustness\ncompared to vanilla DQN [Mnih et al., 2015], paired with its simplicity compared to later variants like Rainbow\n[Hessel et al., 2018], which allows for more easily controlled experiments with fewer moving parts.\n3We provide two Tandem RL implementations: https://github.com/deepmind/deepmind-research/tree/\nmaster/tandem_dqn based on the DQN Zoo [Quan and Ostrovski, 2020], and https://github.com/google/\ndopamine/tree/master/dopamine/labs/tandem_dqn based on the Dopamine library [Castro et al., 2018].\n3\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\nVanilla Tandem DQN\nSame target Q\nSame target \nSame target  & Q\nFigure 2: Active vs. passive performance when using the active agent’s target policy and/or value\nfunction for constructing passive bootstrapping targets.\n3\nAnalysis of the Tandem Effect\nIn line with existing explanations [Levine et al., 2020], we propose that the tandem effect is primarily\ncaused by extrapolation error when certain state-action pairs are under-represented in the active\nagent’s behavior data. Speciﬁcally with ε-greedy policies, even small over-estimation of the values of\nrarely seen actions can lead to sufﬁcient behavior deviations to cause catastrophic under-performance.\nWe further extend this hypothesis: in the context of deep reinforcement learning (i.e. with non-linear\nfunction approximation), an inadequate data distribution can drive over-generalization [Bengio et al.,\n2020], making such erroneous extrapolation likely. While the tandem effect can show up as learning\ninefﬁciency even in the tabular case [Xiao et al., 2021], it proves especially pernicious in the case\nof non-linear function approximation, where erroneous extrapolation can lead to errors not just on\ncompletely unseen, but also rarely seen data, and can persist in the inﬁnite-sample limit.\nCoalescing this view and past analyses of challenges in ofﬂine RL (e.g. [Levine et al., 2020, Fujimoto\net al., 2019b, Liu et al., 2020]) into the following three potential contributing factors in the tandem\neffect provides a natural structure to our analysis:\nBootstrapping (B) The passive agent’s bootstrapping from poorly estimated (in particular, over-\nestimated) values causes any initially small mis-estimation to get ampliﬁed.\nData Distribution (D) Insufﬁcient coverage of sub-optimal actions under the active agent’s policy\nmay lead to their mis-estimation by the passive agent. In the case of over-estimation, this may lead to\nthe passive agent’s under-performance.\nFunction Approximation (F) A non-linear function approximator used as a Q-value function may\ntend to wrongly extrapolate the values of state-action pairs underrepresented in the active agent’s\nbehavior distribution. This tendency can be inherent and persistent, in the sense of being independent\nof initialization and not being reduced with increased training on the same data distribution.\nThese proposed contributing factors are not at all mutually exclusive; they may interact in causing\nor exacerbating the tandem effect. Insufﬁcient coverage of sub-optimal actions under the active\nagent’s behavior distribution (D) may lead to insufﬁcient constraint on the respective values, which\nallows for effects of erroneous extrapolation by a function approximator (F). Where this results in\nover-estimation, the use of bootstrapping (B) carries the potential to ‘pollute’ even well-covered\nstate-action pairs by propagating over-estimated values (especially via the max operator in the case\nof Q-learning). In the next sections we empirically study these three factors in isolation, to establish\ntheir actual roles and relative contributions to the overall difﬁculty of passive learning.\n3.1\nThe Role of Bootstrapping\nOne distinguishing feature of reinforcement learning as opposed to supervised learning is its frequent\nuse of learned quantities as preliminary optimization targets, most prominently in what is referred to\nas bootstrapping in the widely used TD algorithms [Sutton, 1988], where preliminary estimates of\nthe value function are used as update targets. In the Double-DQN algorithm these updates take the\nform Q(s, a) ←r + γ ¯Q(s′, arg maxa′ Q(s′, a′)), where Q denotes the parametric Q-value function,\nand ¯Q is the target network Q-value function, i.e. a time-delayed copy of Q.\nFour value functions are involved in the active and passive updates of Tandem DQN: QA, ¯QA, QP\nand ¯QP , where the A/P subscripts refer to the Q-value functions of the active and passive agents,\n4\nrespectively. The use of its own target network by the passive agent makes bootstrapping a plausible\nstrong contributor to the tandem effect. To test this, we replace the target values and/or policies in the\nupdate equation for the passive agent, with the values provided by the active agent’s value functions:\nQP (s, a) ←\n\n\n\n\n\n\n\nr + γ ¯QP (s′, arg maxa′ QP (s′, a′))\nVanilla Tandem DQN\nr + γ ¯QA(s′, arg maxa′ QP (s′, a′))\nSame Target Q\nr + γ ¯QP (s′, arg maxa′ QA(s′, a′))\nSame Target π\nr + γ ¯QA(s′, arg maxa′ QA(s′, a′))\nSame Target π&Q\nAs shown in Fig. 2, the use of the active value functions as targets reduces the active-passive gap by\nonly a small amount. Note that when both active target values and policy are used, both networks are\nreceiving an identical sequence of targets for their update computations, a sequence that sufﬁces for\nthe active agent to learn a successful policy. Strikingly, despite this the tandem effect appears largely\npreserved: in all but the easiest games (e.g. PONG4) the passive agent fails to learn effectively.\nTo more precisely understand the effect of bootstrapping with respect to a potential value over-\nestimation by the passive agent, in Appendix Fig. 16 we also show the values of the passive networks\nin the above experiment compared to those of the respective active networks. As hypothesised, we\nobserve that the vanilla tandem setting leads to signiﬁcant value over-estimation, and that indeed\nbootstrapping plays a substantial role in amplifying the effect: passive networks trained using the\nactive network’s bootstrap targets do not over-estimate compared to the active network at all.\nThese ﬁndings indicate that a simple notion of value over-estimation itself is not the fundamental\ncause of the tandem effect, and that (B) plays an amplifying, rather than causal role. Additional\nevidence for this is provided below, where the tandem effect occurs in a purely supervised setting\nwithout bootstrapping.\n3.2\nThe Role of the Data Distribution\nThe critical role of the data distribution for ofﬂine learning is well established [Fujimoto et al., 2019b,\nJacq et al., 2019, Liu et al., 2020, Wang et al., 2021]. In particular, Wang et al. [2020a] showed that\nsimpler notions of state-space coverage may not sufﬁce for efﬁcient ofﬂine learning with function\napproximation (even in the linear case and under a strong realizability assumption); much stronger\nassumptions on the data distribution, typically not satisﬁed in practical scenarios, may actually be\nrequired. Here we extend past analysis empirically, by investigating how properties of the data\ndistribution (e.g. stochasticity, stationarity, the size and diversity of the dataset, and its proximity to\nthe passive agent’s own behavior distribution) affect its suitability for passive learning.\nThe exploration parameter ε\nA simple way to affect the data distribution’s state-action coverage\n(albeit in a blunt and uniform way) is by varying the exploration parameter ε of the active agent’s\nε-greedy behavior policy (for training, not for evaluation). Note that a higher ε parameter affects the\nactive agent’s own training performance, as its ability to navigate environments requiring precise\ncontrol is reduced. In Fig. 3 (top) we therefore report the relative passive performance (i.e. as a\nfraction of the active agent’s performance, which itself also varies across parameters), with absolute\nperformance plots included in the Appendix for completeness (Fig. 17). We observe that the relative\npassive performance is indeed substantially improved when the active behavior policy’s stochasticity\n(and as a consequence its coverage of non-argmax actions along trajectories) is increased, and\nconversely it reduces with a greedier behavior policy, providing evidence for the role of (D).\nSticky actions\nAn alternative potential source of stochasticity is the environment itself, e.g. the use\nof ‘sticky actions’ in Atari [Machado et al., 2018]: with ﬁxed probability, an agent action is ignored\n(and the previous action repeated instead). This type of environment-side stochasticity should not be\nexpected to cause new actions to appear in the behavior data, and indeed Fig. 3 (bottom) shows no\nsubstantial impact on the tandem effect.\nReplay size\nOur results contrast with the strong ofﬂine RL results in [Agarwal et al., 2020]. We\nhypothesize that the difference is due to the vastly different dataset size (full training of 200M\n4PONG is an outlier in that it only has 3 actions, and in a large fraction of states actions have no (irreversible)\nconsequences, making greedy policies somewhat robust to errors in the underlying value function.\n5\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpace invaders\n= 0.001\n= 0.003\n= 0.01\n= 0.03\n= 0.1\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpace invaders\nRegular Atari\nSticky Actions\nFigure 3: Passive as fraction of active performance for varying active ε-greedy behavior policies (top);\nregular Atari vs sticky-actions Atari (bottom). We report relative passive performance, as active\nperformance varies across conﬁgurations. See Appendix Figs. 17 & 18 for absolute performance.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\nreplay size=1e6\nreplay size=2e6\nreplay size=4e6\nFigure 4: Active vs. passive performance for different replay sizes (for passive agent).\ntransitions vs. replay buffer of 1M). Interpolating between the tandem and the ofﬂine RL setting\nfrom [Agarwal et al., 2020], we increase the replay buffer size, thereby giving the passive agent\naccess to somewhat larger data diversity and state-action coverage (this does not affect the active\nagent’s training as the active agent is constrained to only sample from the most recent 1M replay\nsamples, as in the baseline variant). A larger replay buffer somewhat mitigates the passive agent’s\nunder-performance (Fig. 4), though it appears to mostly slow down rather than prevent the passive\nagent from eventually under-performing its active counterpart substantially. As we suspect that a\nsufﬁcient replay buffer size may depend on the effective state-space size of an environment, we also\nperform analogous experiments on the (much smaller) classic control domains; results (Appendix\nFig. 22) remain qualitatively the same.\nNote that for a ﬁxed dataset size, sample diversity can take different forms. Many samples from a\nsingle policy may provide better coverage of states on, or near, policy-typical trajectories. Meanwhile,\na larger collection of policies, with fewer samples per policy, provides better coverage of many\ntrajectories at the expense of lesser coverage of small deviations from each. To disentangle the impact\nof these modalities, while also shedding light on the role of stationarity of the distribution, we next\nswitch to the ‘Forked Tandem’ variation of the experimental paradigm.\nFixed policy\nUpon forking, the frozen active policy is executed to produce training data for the\npassive agent, which begins its training initialized with the active network’s weights. Note that this\nconstitutes a stronger variant of the tandem experiment. At the time of forking, the agents do not\nmerely share analogous architectures and equal ‘data history’, but also identical network weights\n(whereas in the simple tandem setting, the agents were distinguished by independently initialized\nnetworks). Moreover, the data used for passive training can be thought of as a ‘best-case scenario’:\ngenerated by a single ﬁxed policy, identical to the passive policy at the beginning of passive training.\nStrikingly, the tandem effect is not only preserved but even exacerbated in this setting (Fig. 5, top):\nafter forking, passive performance decays rapidly in all but the easiest games, despite starting from a\nnear-optimally performing initialization. This re-frames the tandem effect as not merely the difﬁculty\nof passively learning to act, but even to passively maintain performance. Instability appears to\nbe inherent in the very learning process itself, providing strong support to the hypothesis that an\ninterplay between (D) and (F) is critical to the tandem effect.\n6\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n0\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\nForked @25\nForked @50\nForked @100\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n0\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\nForked @25\nForked @50\nForked @100\nFigure 5: Performance of a Forked Tandem DQN, training passively after freezing its data generating\npolicy (top) or its replay buffer (bottom). Vertical lines indicate forking time points.\nIn Appendix Fig. 23 we additionally show that similarly to the regular tandem setting, stochasticity\nof the active policy after forking inﬂuences the passive agent’s ability to maintain performance.\nFixed replay\nA variation on the above experiments is to freeze the replay buffer while continuing\nto train the passive policy from this ﬁxed dataset. Instead of a stream of samples from a single\npolicy, this ﬁxed data distribution now contains a ﬁxed number of samples from a training process\nof the length of the replay buffer, i.e. from a number of different policies. The collapse of passive\nperformance here (Fig. 5, bottom) is less rapid, yet qualitatively similar. In Appendix Fig. 24 we\npresent yet another variant of this experiment with similar results, showing that the effect is robust to\nminor variations in the exact way of ﬁxing the data distribution of a learning agent.\nThese experiments provide strong evidence for the importance of (D): a larger replay buffer,\ncontaining samples from more diverse policies, can be expected to provide an improved coverage of\n(currently) non-greedy actions, reducing the tandem effect. While the forked tandem begins passive\nlearning with the seemingly advantageous high-performing initialization, state-action coverage is\ncritically limited in this case. In the frozen-policy case, a large number of samples from the very\nsame ε-greedy policy can be expected to provide very little coverage of non-greedy actions, while in\nthe frozen-replay case, a smaller number of samples from multiple policies can be expected to only\ndo somewhat better in this regard. In both cases the tandem effect is highly pronounced.\nOn-policy evaluation\nThe strength of the last two experiments lies in the observation that, since\nactive and passive networks have identical parameter values at the beginning of passive training,\ntheir divergence cannot be attributed to small initial differences getting ampliﬁed by training on an\ninadequate data distribution. With so many factors held ﬁxed, the collapse of passive performance\nwhen trained on the very data distribution produced by its own initial policy begs the question whether\noff-policy Q-learning itself is to blame for this failure mode, e.g. via statistical over-estimation\nbias introduced by the max operator [van Hasselt, 2010]. Here we provide a negative answer, by\nperforming on-policy evaluation with SARSA [Rummery and Niranjan, 1994] (Fig. 6), and even\npurely supervised regression on the Monte-Carlo returns (Appendix Fig. 25), in the forked tandem\nsetup. While evaluation succeeds, in the sense of minimizing evaluation error on the given behavior\ndistribution, atypical action values under the behavior policy suffer substantial estimation error,\nresulting in occasional over-estimation. The resulting ε-greedy control policy under-performs the\ninitial policy at forking time as catastrophically as in the other forked tandem experiments (more\ndetails in Appendix A.3). Strengthening the roles of (D) and (F) while further weakening that of\n(B), these observations point to an inherent instability of ofﬂine learning, different from that of Baird’s\nfamous example [Baird, 1995] or the ‘Deadly Triad’ [Sutton and Barto, 2018, van Hasselt et al.,\n2018]; an instability that results purely from erroneous extrapolation by the function approximator,\nwhen the utilized data distribution does not provide adequate coverage of relevant state-action pairs.\nSelf-generated data\nOur ﬁnal empirical question in this section is ‘How much data generated\nby the passive agent is needed to correct for the tandem effect?’. While a full investigation of this\nquestion exceeds the scope of this paper and is left for future work, the tandem setup lends itself to a\n7\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n0\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\nSpace invaders\nForked @25\nForked @50\nForked @100\nFigure 6: Passive performance in Forked Tandem DQN after policy evaluation with SARSA.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\nSpace invaders\n0%\n10%\n20%\n50%\nFigure 7: Passive performance for different amounts of self-generated data in the passive agent’s\nreplay batches.\nsimple experiment: both agents interact with the environment and ﬁll individual replay buffers, one\nof them (for simplicity still referred to as ‘passive’) however learns from data stochastically mixed\nfrom both replay buffers. Fig. 7 shows that even a moderate amount (10%-20%) of ‘own’ data yields\na substantial reduction of the tandem effect, while a 50/50 mixture completely eliminates it.\n3.3\nThe Role of Function Approximation\nWe structure our investigation of the role of function approximation in the tandem effect into two\ncategories: the optimization process and the function class used.\nOptimization\nAgarwal et al. [2020] and Obando-Ceron and Castro [2021] demonstrated that the\nAdam optimization algorithm [Kingma and Ba, 2015] outperforms RMSProp [Tieleman and Hinton,\n2012] used in our experiments. In Appendix Fig. 20 we show that while both active and passive\nagents perform better with Adam, the tandem effect itself is unaffected by the choice of optimizer.\nAnother plausible hypothesis is that the passive network suffers from under-ﬁtting and requires\nmore updates on the same data to attain comparable performance to the active learner. Varying the\nnumber of passive agent updates per active agent update step, we ﬁnd that more updates worsen\nthe performance of the passive agent (Appendix Fig. 21). This rejects insufﬁcient training as a\npossible cause, and further supports the role of (D). We also note that, together with the forked\ntandem experiments in the previous section, this ﬁnding distinguishes the tandem effect from the\nissue of estimation error in the ofﬂine learning setting [Xiao et al., 2021]: while in the tabular setting\nestimation error dominates the learning challenge and a sufﬁcient training duration (assuming full\nstate-space coverage) guarantees convergence to a good solution, this is not necessarily the case with\nfunction approximation trained on a given data distribution.\nFunction class\nGiven that the active and passive agents share an identical network architecture,\nthe passive agent’s under-performance cannot be explained by an insufﬁciently expressive function\napproximator. Performing the tandem experiment with pure regression of the passive network’s\noutputs towards the active network’s (a variant of network distillation [Hinton et al., 2015]), instead\nof TD based training, we observe that the performance gap is indeed vastly reduced and in some\ngames closed entirely (see Appendix Fig. 19); however, strikingly, it remains in some.\nNext, we vary the function class of both networks by varying the depth and width of the utilized\nQ-networks on a set of Classic Control tasks. As can be seen in Fig. 8 (and Appendix Fig. 28),\nthe magnitude of the active-passive performance gap appears negatively correlated with network\nwidth, which is in line with (F): an increase in network capacity results in less pressure towards\nover-generalizing to infrequently seen action values and an ultimately smaller tandem effect. On the\nother hand, the gap seems to correlate positively with network depth. We speculate that this may\n8\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nCartPole - 0 hidden layers\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartPole - 1 hidden layers\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartPole - 2 hidden layers\n4 hidden units\n128 hidden units\n512 hidden units\nFigure 8: Passive performance as a fraction of active performance in CartPole: varying number of\nhidden layers and units.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\n4000\nSpace invaders\ntied layers=0\ntied layers=1\ntied layers=2\ntied layers=3\ntied layers=4\nFigure 9: Active vs. passive performance, with ﬁrst k of 5 layers of active/passive networks shared.\nrelate to the ﬁnding that deeper networks tend to be biased towards simpler (e.g. lower rank) solutions,\nwhich may suffer from increased over-generalization [Huh et al., 2021, Kumar et al., 2020a].\nFinally, we investigate varying the function class of only the passive network by sharing the weights\nof the ﬁrst k (out of 5) layers of active and passive networks, while constraining the passive network\nto only update the remaining top 5 −k layers, and using the ‘representation’ at layer k acquired\nthrough active learning only. This reduces the ‘degrees of freedom’ of the passive agent, which we\nhypothesize reduces its potential for divergence. Indeed, Fig. 9 illustrates that passive performance\ncorrelates strongly with the number of tied layers, with the variant for which only the linear output\nlayer is trained passively performing on par with the active agent. A similar result is obtained in the\nforked tandem setting, see Appendix Fig. 27. This ﬁnding provides a strong indirect hint towards (F):\nwith only part of the network’s layers being trained passively, much of its ‘generalization capacity’ is\nshared between active and passive agents. States that are not aggregated by the shared bottom layers\n(only affected by active training) have to be ‘erroneously’ aggregated by the remaining top layers of\nthe network for over-generalization to occur. A more thorough investigation of this, exceeding the\nscope of this paper, may involve attempting to measure (over-)generalization more directly, e.g. via\ngradient interference [Bengio et al., 2020].\n4\nApplications of the Tandem Setting\nIn addition to being valuable for studying the challenges in ofﬂine RL, we propose that the Tandem\nRL setting provides analytic capabilities that make it a useful tool in the empirical analysis of general\n(online) reinforcement learning algorithms. At its core, the tandem setting aims to decouple learning\ndynamics from its impact on behavior and the data distribution, which are inseparably intertwined in\nthe online setting. While classic ofﬂine RL achieves a similar effect, as an analytic tool it has the\npotential downside of typically using a stationary distribution. Tandem RL, on the other hand, presents\nthe passive agent with a data distribution which realistically represents the type of non-stationarity\nencountered in an online learning process, while still holding that distribution independent from\nthe learning dynamics being studied. This allows Tandem RL to be used to study, e.g., the impact\nof variations in the learning algorithm on the quality of a learned representation, without having to\ncontrol for the indirect confounding effect of a different behavior causing a different data distribution.\nWhile extensive examples of this exceed the scope of this paper, Appendix A.5 contains a single such\nexperiment, testing QR-DQN [Dabney et al., 2018] as a passive learning algorithm (the active agent\nbeing a Double-DQN). This is motivated by the observation of Agarwal et al. [2020], that QR-DQN\noutperforms DQN in the ofﬂine setting. QR-DQN indeed appears to be a nontrivially different passive\nlearning algorithm, signiﬁcantly better in some games, while curiously worse in others (Fig. 29).\n9\n5\nDiscussion and Conclusion\nAt a high level, our work can be viewed as investigating the issue of (in)compatibility between\nthe data distribution used to train a function approximator and the data distribution relevant in its\nevaluation. While in supervised learning, generalization can be viewed as the problem of transfer\nfrom a training to a (given) test distribution, the fundamental challenge for control in reinforcement\nlearning is that the test distribution is created by the very outcome of learning itself, the learned\npolicy. The various difﬁculties of learning to act from ofﬂine data alone throw into focus the role of\ninteractivity in the learning process: only by continuously interacting with the environment does an\nagent gradually ‘unroll’ the very data on which its performance will be evaluated.\nThis need not be an obstacle in the case of exact (i.e. tabular) functions: with sufﬁcient data,\nextrapolation error can be avoided entirely. In the case of function approximation however, as\nsmall errors compound rapidly into a difference in the underlying state distribution, signiﬁcant\ndivergence and, as this and past work demonstrates, ultimately catastrophic under-performance can\noccur. Function approximation plays a two-fold role here: (1) being an approximation, it allows\ndeviations in the outputs; (2) as the learned quantity, it is (especially in the non-linear case) highly\nsensitive to variations in the input distribution. When evaluated for control after ofﬂine training, these\ntwo roles combine in a way that is ‘unexplored’ by the training process: minor output errors cause a\ndrift in behavior, and thereby a drift in the test distribution.\nWhile related, this challenge is subtly different from the well-known divergence issues of off-policy\nlearning with function approximation, demonstrated by Baird’s famous counterexample [Baird,\n1995] (see also [Tsitsiklis and Van Roy, 1996]) and conceptualized as the Deadly Triad [Sutton\nand Barto, 2018, van Hasselt et al., 2018]. While these depend on bootstrapping as a mechanism\nto cause a feedback-loop resulting in value divergence, our results show that the ofﬂine learning\nchallenge persists even without bootstrapping, as small differences in behavior cause a drift in the ‘test\ndistribution’ itself. Instead of a training-time output drift caused by bootstrapping, the central role is\ntaken by a test-time drift of the state distribution caused by the interplay of function approximation\nand a ﬁxed data distribution (as opposed to dynamically self-generated data).\nOur empirical work highlights the importance of interactivity and ‘learning from your own mistakes’\nin learning control. Starting out as an investigation of the challenges in ofﬂine reinforcement learning,\nit also provides a particular viewpoint on the classical online reinforcement learning case. Heuristic\nexplanations for highly successful deep RL algorithms like DQN, based on intuitions from (e.g.)\napproximate policy iteration, need to be viewed with caution in light of the apparent hardness of a\npolicy improvement step based on approximate policy evaluation with a function approximator.\nFinally, the forked tandem experiments show that even high-performing initializations are not robust\nto a collapse of control performance, when trained under their own (but ﬁxed!) behavior distribution.\nNot just learning to act, but even maintaining performance appears hard in this setting. This provides\nan intuition that we distill into the following working conjecture: The dynamics of deep reinforcement\nlearning for control are unstable on (almost) any ﬁxed data distribution.\nExpanding on the classical on- vs. off-policy dichotomy, we propose that indeﬁnitely training on any\nﬁxed data distribution, without strong explicit regularization or additional inductive bias, gives rise\nto ‘exploitation of gaps in the data’ by a function approximator, akin to the over-ﬁtting occurring\nwhen over-training on a ﬁxed dataset in supervised learning. Interaction, i.e. generating at least\nmoderate amounts of one’s own experience, appears to be a powerful, and for the most part necessary,\nregularizer and stabilizer for learning to act, by creating a dynamic equilibrium between optimization\nof a function approximator and its own data-generation process.\nBroader impact statement\nThis work lies in the realm of foundational RL, contributing to the\nfundamental understanding and development of RL algorithms, and as such is far removed from\nethical issues and direct societal consequences. On the other hand, it highlights the empirical difﬁculty\nand limitations of ofﬂine deep RL for control - increasingly important for practical applications,\ne.g. robotics, where interactive data is costly, and learning from ofﬂine datasets is desirable. In this\nway it complements existing theoretical hardness results in this area and provides additional context\nto existing empirical techniques which aim to overcome or circumvent those limitations. We believe\nthat an improved understanding of these challenges can play an important role in creating robust and\nstable ofﬂine learning algorithms whose outputs can be more safely deployed in the real world.\n10\nAcknowledgements\nWe would like to thank Hado van Hasselt and Joshua Greaves for feedback on an early draft of this\npaper, and Zhongwen Xu for an unpublished related piece of work at DeepMind that inspired some\nof our experiments. We also thank Clare Lyle, David Abel, Diana Borsa, Doina Precup, John Quan,\nMarc G. Bellemare, Mark Rowland, Michal Valko, Remi Munos, Rishabh Agarwal, Tom Schaul\nand Yaroslav Ganin, and many other colleagues at DeepMind and Google Brain for the numerous\ndiscussions that helped shape this research.\nReferences\nJoshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep\nQ-learning. arXiv preprint arXiv:1903.08894, 2019.\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofﬂine\nreinforcement learning. In Proceedings of the 37th International Conference on Machine Learning,\nvolume 119, pages 104–114. PMLR, 2020.\nLeemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\nMachine Learning Proceedings 1995, pages 30–37. Elsevier, 1995.\nMarc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning\nEnvironment: an evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,\n47:253–279, 2013.\nEmmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal\ndifference learning. In International Conference on Machine Learning, pages 767–777, 2020.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and\nQiao Zhang.\nJAX: composable transformations of Python+NumPy programs, 2018.\nURL\nhttp://github.com/google/jax.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\nJacob Buckman, Carles Gelada, and Marc G. Bellemare. The importance of pessimism in ﬁxed-dataset\npolicy optimization. In International Conference on Learning Representations, 2021.\nDavid Budden, Matteo Hessel, John Quan, Steven Kapturowski, Kate Baumli, Surya Bhupatiraju,\nAurelia Guy, and Michael King. RLax: Reinforcement Learning in JAX, 2020. URL http:\n//github.com/deepmind/rlax.\nPablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Belle-\nmare.\nDopamine: A research framework for deep reinforcement learning.\narXiv preprint\narXiv:1812.06110, 2018.\nWill Dabney, Mark Rowland, Marc G. Bellemare, and Rémi Munos. Distributional reinforcement\nlearning with quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 32, 2018.\nRasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander J. Smola. Continuous doubly\nconstrained batch reinforcement learning. arXiv preprint arXiv:2102.09225, 2021.\nScott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch\ndeep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019a.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. In Proceedings of the 36th International Conference on Machine Learning, volume 97,\npages 2052–2062. PMLR, 2019b.\nRichard Held and Alan Hein. Movement-produced stimulation in the development of visually guided\nbehavior. Journal of Comparative and Physiological Psychology, 56(5):872–876, 1963.\n11\nTom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.\nURL http://github.com/deepmind/dm-haiku.\nMatteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan\nHorgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: combining improvements in\ndeep reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\n2018.\nMatteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax:\nComposable gradient transformation and optimisation, in JAX!, 2020. URL http://github.\ncom/deepmind/optax.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning Workshop, 2015.\nMinyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola.\nThe low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021.\nAlexis Jacq, Matthieu Geist, Ana Paiva, and Olivier Pietquin. Learning from a learner. In Proceedings\nof the 36th International Conference on Machine Learning, volume 97, pages 2990–2999, 2019.\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent\nexperience replay in distributed reinforcement learning. In International Conference on Learning\nRepresentations, 2019.\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Model-\nbased ofﬂine reinforcement learning. In Advances in Neural Information Processing Systems,\nvolume 33, pages 21810–21823, 2020.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the\nInternational Conference on Learning Representations, 2015.\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\nQ-learning via bootstrapping error reduction. In Advances in Neural Information Processing\nSystems, volume 32, 2019.\nAviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization\ninhibits data-efﬁcient deep reinforcement learning. arXiv preprint arXiv:2010.14498, 2020a.\nAviral Kumar, Abhishek Gupta, and Sergey Levine. DisCor: Corrective feedback in reinforcement\nlearning via distribution correction. In Advances in Neural Information Processing Systems,\nvolume 33, pages 18560–18572, 2020b.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for ofﬂine\nreinforcement learning. In Advances in Neural Information Processing Systems, volume 33, pages\n1179–1191, 2020c.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nYao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforce-\nment learning without great exploration. arXiv preprint arXiv:2007.08202, 2020.\nMarlos C Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael\nBowling. Revisiting the Arcade Learning Environment: Evaluation protocols and open problems\nfor general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018.\nTatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Oﬁr Nachum, and Shixiang Gu. Deployment-\nefﬁcient reinforcement learning via model-based ofﬂine optimization.\narXiv preprint\narXiv:2006.03647, 2020.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,\nShane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, 2015.\n12\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement\nlearning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020.\nJohan S. Obando-Ceron and Pablo Samuel Castro. Revisiting Rainbow: Promoting more insightful\nand inclusive deep reinforcement learning research. In Proceedings of the 38th International\nConference on Machine Learning. PMLR, 2021.\nJohn Quan and Georg Ostrovski. DQN Zoo: Reference implementations of DQN-based agents, 2020.\nURL http://github.com/deepmind/dqn_zoo.\nGavin A. Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems. Techni-\ncal report, University of Cambridge, Department of Engineering Cambridge, UK, 1994.\nJulian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis\nAntonoglou, and David Silver. Online and ofﬂine reinforcement learning by planning with a\nlearned model. arXiv preprint arXiv:2104.06294, 2021.\nRichard S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3\n(1):9–44, 1988.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\nPhilip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High conﬁdence policy\nimprovement. In International Conference on Machine Learning, pages 2380–2388. PMLR, 2015.\nTijmen Tieleman and Geoffrey Hinton. Lecture 6.5: RMSProp. COURSERA: Neural Networks for\nMachine Learning, 2012.\nJohn N. Tsitsiklis and Benjamin Van Roy. Analysis of temporal-difference learning with function ap-\nproximation. In Proceedings of the 9th International Conference on Neural Information Processing\nSystems, page 1075–1081, Cambridge, MA, USA, 1996. MIT Press.\nHado van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems,\nvolume 23, 2010.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-\nlearning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2016.\nHado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil.\nDeep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.\nRuosong Wang, Dean P. Foster, and Sham M. Kakade. What are the statistical limits of ofﬂine RL\nwith linear function approximation? arXiv preprint arXiv:2010.11895, 2020a.\nRuosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M. Kakade. Instabilities of ofﬂine RL\nwith pre-trained neural representation. arXiv preprint arXiv:2103.04947, 2021.\nZiyu Wang, Alexander Novikov, Konrad ˙Zołna, Jost Tobias Springenberg, Scott Reed, Bobak\nShahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized\nregression. arXiv preprint arXiv:2006.15134, 2020b.\nYifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning.\narXiv preprint arXiv:1911.11361, 2019.\nYue Wu, Shuangfei Zhou, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and\nHanlin Goh. Uncertainty weighted actor-critic for ofﬂine reinforcement learning. In Proceedings\nof the 38th International Conference on Machine Learning. PMLR, 2021.\nChenjun Xiao, Ilbin Lee, Bo Dai, Dale Schuurmans, and Csaba Szepesvari. On the sample complexity\nof batch reinforcement learning with policy-induced data. arXiv preprint arXiv:2106.09973, 2021.\nKenny Young and Tian Tian. MinAtar: An Atari-inspired testbed for thorough and reproducible\nreinforcement learning experiments. arXiv preprint arXiv:1903.03176, 2019.\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and\nTengyu Ma. MOPO: Model-based ofﬂine policy optimization. arXiv preprint arXiv:2005.13239,\n2020.\n13\n0\n1000\n2000\n3000\nEpisode return\nAlien\n0\n500\n1000\n1500\n2000\nAmidar\n0\n500\n1000\n1500\n2000\n2500\nAssault\n0\n5000\n10000\n15000\nAsterix\n250\n500\n750\n1000\n1250\n1500\nAsteroids\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 1e6\nAtlantis\n0\n200\n400\n600\n800\nEpisode return\nBank heist\n0\n10000\n20000\n30000\nBattle zone\n0\n2500\n5000\n7500\n10000\n12500\n15000\nBeam rider\n0\n250\n500\n750\n1000\n1250\n1500\nBerzerk\n0\n10\n20\n30\n40\n50\n60\nBowling\n50\n25\n0\n25\n50\n75\nBoxing\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n2000\n4000\n6000\n8000\nCentipede\n0\n1000\n2000\n3000\n4000\nChopper command\n0\n20000\n40000\n60000\n80000\n100000\n120000\nCrazy climber\n0\n5000\n10000\n15000\n20000\n25000\nDefender\n0\n10000\n20000\n30000\n40000\n50000\n60000\nDemon attack\n25\n20\n15\n10\n5\nEpisode return\nDouble dunk\n0\n200\n400\n600\n800\n1000\nEnduro\n100\n80\n60\n40\n20\n0\nFishing derby\n0\n10\n20\n30\nFreeway\n0\n100\n200\n300\n400\n500\n600\nFrostbite\n0\n2000\n4000\n6000\n8000\n10000\nGopher\n0\n100\n200\n300\n400\nEpisode return\nGravitar\n0\n5000\n10000\n15000\n20000\nHero\n20\n15\n10\n5\n0\nIce hockey\n0\n250\n500\n750\n1000\n1250\nJamesbond\n0\n2500\n5000\n7500\n10000\n12500\nKangaroo\n0\n2000\n4000\n6000\n8000\nKrull\n0\n5000\n10000\n15000\n20000\n25000\n30000\nEpisode return\nKung fu master\n0.0\n0.1\n0.2\n0.3\nMontezuma revenge\n0\n500\n1000\n1500\n2000\n2500\nMs pacman\n0\n2000\n4000\n6000\n8000\n10000\nName this game\n0\n2000\n4000\n6000\n8000\n10000\n12000\nPhoenix\n1000\n800\n600\n400\n200\n0\nPitfall\n20\n10\n0\n10\n20\nEpisode return\nPong\n500\n0\n500\nPrivate eye\n0\n2500\n5000\n7500\n10000\n12500\n15000\nQbert\n0\n2500\n5000\n7500\n10000\n12500\nRiverraid\n0\n10000\n20000\n30000\n40000\nRoad runner\n0\n10\n20\n30\n40\n50\n60\nRobotank\n0\n5000\n10000\n15000\nEpisode return\nSeaquest\n30000\n25000\n20000\n15000\nSkiing\n0\n500\n1000\n1500\n2000\nSolaris\n0\n500\n1000\n1500\n2000\n2500\n3000\nSpace invaders\n0\n10000\n20000\n30000\n40000\n50000\n60000\nStar gunner\n10\n9\n8\n7\n6\n5\n4\nSurround\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nEpisode return\nTennis\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n2000\n4000\n6000\n8000\nTime pilot\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n25\n50\n75\n100\n125\n150\nTutankham\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n2500\n5000\n7500\n10000\n12500\nUp n down\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n50\n100\n150\n200\nVenture\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100000\n200000\n300000\nVideo pinball\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n2000\n4000\n6000\n8000\nEpisode return\nWizard of wor\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nYars revenge\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n2500\n5000\n7500\n10000\nZaxxon\nactive\npassive\nFigure 10: Tandem DQN: active vs. passive performance across Atari 57 (3 seeds per game).\nA\nAppendix\nA.1\nImplementation, Hyperparameters and Evaluation Details\nThe implementation of our main agent, Tandem DQN, is based on the Double-DQN [van Hasselt\net al., 2016] agent provided in the DQN Zoo open-source agent collection [Quan and Ostrovski, 2020].\nThe code uses JAX [Bradbury et al., 2018], and the Rlax, Haiku and Optax libraries [Budden et al.,\n2020, Hennigan et al., 2020, Hessel et al., 2020] for RL losses, neural networks and optimization\n14\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMedian human-normalized performance\nactive\npassive\nFigure 11: Tandem DQN: Median human-normalized scores over 57 Atari games (3 seeds per game).\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n500\n400\n300\n200\n100\nEpisode return\nAcrobot\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0\n50\n100\n150\n200\nCartpole\n0\n25\n50\n75\n100\nEnvironment frames (thousands)\n1000\n500\n0\nLunarlander\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n600\n500\n400\n300\n200\n100\nMountaincar\nActive\nPassive\nFigure 12: Tandem DQN: Active vs. passive performance on four selected Classic Control domains.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEnvironment frames (millions)\n0.4\n0.6\n0.8\nMean/Median fraction of disagreement states\nmedian\nmean\nFigure 13: Fraction of states (uniformly sampled from replay buffer) on which active and passive\npolicies disagree, i.e. where arg maxa QA(s, a) ̸= arg maxa QP (s, a), mean and median across 57\nAtari games (3 seeds per game).\n0.0\n0.5\n1.0\n1.5\nUpdate (learner) steps\n1e5\n0\n5000\n10000\nEpisode return\nMs Pacman\n0.0\n0.5\n1.0\n1.5\nUpdate (learner) steps\n1e5\n0\n2000\n4000\n6000\n8000\nSeaquest\n0.0\n0.5\n1.0\n1.5\nUpdate (learner) steps\n1e5\n0\n10000\n20000\nSpace invaders\nR2D2 active\nR2D2 passive\nFigure 14: Tandem R2D2: active vs. passive performance on three Atari domains (3 seeds per game).\nNote: because of the use of an untuned implementation of R2D2, active results are not directly\ncomparable to those of the published agent [Kapturowski et al., 2019].\n15\n0\n2\n4\n6\n8\nEnvironment frames (millions)\n10\n15\n20\n25\n30\n35\n40\n45\nEpisode return\nAsterix\n0\n2\n4\n6\n8\nEnvironment frames (millions)\n4\n6\n8\n10\n12\n14\nBreakout\n0\n2\n4\n6\n8\nEnvironment frames (millions)\n25\n30\n35\n40\n45\nFreeway\n0\n2\n4\n6\n8\nEnvironment frames (millions)\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nSeaquest\n0\n2\n4\n6\n8\nEnvironment frames (millions)\n10\n20\n30\n40\n50\n60\n70\nSpace invaders\nActive\nPassive\nFigure 15: Tandem DQN evaluated on ﬁve MinAtar domains.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n1.0\n0.5\n0.0\n0.5\n1.0\nmax\na\nQP(s, a)\nmax\na\nQA(s, a)\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.1\n0.0\n0.1\n0.2\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n1\n0\n1\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n1.0\n0.5\n0.0\n0.5\n1.0\nSpace invaders\nVanilla Tandem DQN\nSame target \nSame target  & Q\nSame target Q\nFigure 16: Q-value over-estimation by the passive network compared to the active one in Tandem\nDQN with varying bootstrap targets. It can be seen that the passive network in the vanilla Tandem\nDQN setting tends to over-estimate values (compared to the active one), which is almost perfectly\nmitigated by using the same bootstrap target values as the active network, and in fact reversed\nwhen using the same target policy (but not the same target values). Note that in all four conﬁgura-\ntions the passive agents substantially under-perform their active counterparts (Fig. 2), showing that\nbootstrapping-ampliﬁed over-estimation is only part, not the main cause of the tandem effect.\nalgorithms, respectively. All algorithmic hyperparameters correspond to those in the DQN Zoo\nimplementation of Double-DQN.\nIn the ‘Tandem’ setting, active and passive agents’ networks weights are initialized independently.\nAgents in this setting are trained in lockstep, i.e. active and passive agents are updated simultaneously\nfrom the same batch of sampled replay transitions, with the exception of one experiment in Section\n3.3, where we study the effect of the number of passive updates relative to active agent updates.\nIn the ‘Forked Tandem’ setting, only one of the agents is trained at any one time. The active agent\ntrains (as a regular Double-DQN) up to the time of forking, at which point the passive agent is\ncreated as a ‘fork’ (i.e., with identical network weights) of the active agent. After forking, only the\npassive agent is trained. The active agent is used for data generation, either by executing its policy\nand continuing to ﬁll the replay buffer (‘Fixed Policy’ experiment), or by sampling batches from its\n‘frozen’ last replay buffer obtained in the active phase of training (‘Fixed Replay’ experiment).\nThe total training budget is kept ﬁxed at 200 iterations in both settings, split across ‘active’ and\n‘passive’ training phases in the forked tandem setting. In all cases, both active and passive agents are\nevaluated after each training iteration for 500K environment steps. Executed with an NVidia P100\nGPU accelerator, each Atari training run takes approximately 4.5 days of wall-clock time.\nThe majority of our Atari experiments use the regular ALE Atari environment [Bellemare et al.,\n2013], using DQN’s default preprocessing, random noop-starts and action-repeats [Mnih et al., 2015],\nas well as using the reduced action-set (i.e. each game exposing the subset of Atari’s total 18 actions\nwhich are relevant to this game). For the ‘Sticky actions’ experiment, we use the OpenAI Gym variant\nof Atari [Brockman et al., 2016] enhanced with sticky actions [Machado et al., 2018].\nUnless stated explicitly, all our results are reported as mean episode returns averaged across 5 seeds,\nwith light and dark shading indicating 0.5 standard deviation conﬁdence bounds and min/max bounds\n(across seeds), respectively. Gray curves always indicate active performance.\nThe ‘relative passive performance’ (or ‘passive performance as fraction of active performance’)\ncurves are meant to illustrate the relative (under-)performance of the passive agent compared to its\nactive counterpart in cases where the active agent’s performance varies strongly across conﬁgurations.\nDenoting Ra(t), Rp(t) the active and passive (undiscounted) episodic returns at iteration t ∈\n16\n0\n100\n200\n300\n400\nEpisode return\n= 0.001, Breakout\n20\n10\n0\n10\n20\n= 0.001, Pong\n0\n5000\n10000\n15000\n= 0.001, Seaquest\n0\n1000\n2000\n3000\n= 0.001,\nSpace invaders\nactive\npassive\n0\n100\n200\n300\n400\nEpisode return\n= 0.003, Breakout\n20\n10\n0\n10\n20\n= 0.003, Pong\n0\n5000\n10000\n15000\n= 0.003, Seaquest\n0\n1000\n2000\n3000\n= 0.003,\nSpace invaders\nactive\npassive\n0\n100\n200\n300\n400\nEpisode return\n= 0.01, Breakout\n20\n10\n0\n10\n20\n= 0.01, Pong\n0\n5000\n10000\n15000\n= 0.01, Seaquest\n0\n1000\n2000\n3000\n= 0.01,\nSpace invaders\nactive\npassive\n0\n100\n200\n300\n400\nEpisode return\n= 0.03, Breakout\n20\n10\n0\n10\n20\n= 0.03, Pong\n0\n5000\n10000\n15000\n= 0.03, Seaquest\n0\n1000\n2000\n3000\n= 0.03,\nSpace invaders\nactive\npassive\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\n= 0.1, Breakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\n= 0.1, Pong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\n= 0.1, Seaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\n= 0.1, Space invaders\nactive\npassive\nFigure 17: Active vs. passive performance for varying active ε-greedy behavior policies. Note that\nhere active performance varies across settings.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n50\n100\n150\n200\n250\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n2500\n5000\n7500\n10000\n12500\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n500\n1000\n1500\n2000\nSpace invaders\nactive\npassive\nFigure 18: Active vs. passive performance on Atari with sticky actions [Machado et al., 2018].\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\nSpace invaders\nPassive Regression\nPassiveTD\nFigure 19: Active vs. passive performance with regular (TD-based) and regression-based Tandem\nDQN. The latter regresses all the passive agent’s action-values towards the respective outputs of the\nactive agent’s network, which can be viewed as network distillation.\n17\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n10000\n20000\n30000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\nSpace invaders\nactive\npassive\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSpace invaders\nAdam\nRMSProp\nFigure 20: Tandem DQN with the Adam optimizer (instead of RMSProp) for both active and passive\nnetwork optimization. (top) Adam: Active vs. passive performance. (bottom) Passive as fraction of\nactive performance Adam vs. RMSProp. While the Adam optimizer improves both the active and\npassive performance of the Tandem DQN, the relative active-passive gap is not affected strongly.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n500\n1000\n1500\n2000\n2500\nSpace invaders\npassive updates=1\npassive updates=2\npassive updates=4\nFigure 21: Active vs. passive performance for varying number of passive updates per active update.\n{0, . . . , 200}, and setting m = mint min(Ra(t), Rp(t)), the relative performance is computed as\nRp(t) −m\nRa(t) −m\nwith the value being clipped to lie in [0, 1] and set to 1.0 whenever Ra(t) = m.\nFor the classic control [Brockman et al., 2016] and MinAtar [Young and Tian, 2019] experiments\nwe used a modiﬁed version of the DQN agent from the Dopamine library [Castro et al., 2018]. The\nmodiﬁcations made were:\n• Double-DQN [van Hasselt et al., 2016] learning updates instead of vanilla DQN\n• MSE loss instead of Huber loss (as suggested in [Obando-Ceron and Castro, 2021])\n• Networks and wrappers for running MinAtar with the Dopamine agents\n• Tandem training regime (regular and/or forked) instead of regular single-agent training.\nUnless stated explicitly, all hyperparameters follow the respective default conﬁgurations in the\nDopamine library. Our network architecture for the classic control environments are two fully\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nAcrobot\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartpole\n0\n25\n50\n75\n100\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLunarlander\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMountaincar\nbuffer size=500\nbuffer size=50000\nbuffer size=500000\nFigure 22: Passive performance as a fraction of active performance when varying the size of the\nreplay buffer used by the passive agent on Classic Control domains.\n18\nconnected layers of 512 units (each with ReLu activations), followed by a ﬁnal fully connected layer\nthat yields the Q-values for each action. In Figs. 8 and 28 we varied the number of hidden layers and\nunits, where the variation of number of units is applied uniformly across all layers.\nThe default network architecture for the MinAtar environments is one convolutional layer with 16\nfeatures of 3 × 3 × 3 and stride of 1 followed by a ReLu activation, whose output is mapped via a\nfully connected layer to the network’s Q-value outputs.\nThe classic control environments were all run on CPUs; each run took between 20 minutes\n(CARTPOLE) and 2 hours (MOUNTAINCAR). The MinAtar environments were all run on NVidia\nP100 GPUs, each run taking approximately 12 hours to complete. Results for all classic control and\nMinAtar environments are reported as mean episode returns averaged across 10 seeds, with light\nand dark shading indicating 0.5 standard deviation conﬁdence bounds and min/max bounds (across\nseeds), respectively.\nFor the R2D2 experiment (Fig. 14), an untuned variant of the distributed R2D2 algorithm [Kaptur-\nowski et al., 2019] was used. Each run used 4 TPUv3 chips for learning and inference, together\nwith a ﬂeet of approximately 500 CPU-based actor threads for distributed environment interaction,\ncompleting a training run of approximately 150K batch updates in about 7 hours wall-clock time.\nA.2\nForked Tandem: Variants\nHere we present additional experimental variants performed within the Forked Tandem setup.\nVarying exploration parameter ε with ﬁxed policy:\nThis experiment is an extension of the ‘Fixed\nPolicy’ (Fig. 5) and ‘The exploration parameter ε’ (Fig. 3 (top)) experiments. After freezing the\nactive agent’s policy for further data generation, its ε parameter is set to a different value, to explore\nthe impact of the resulting policy stochasticity on the ability of the passive learning process to\nmaintain the initial performance level. We note that because of the ﬁxed active policy, in this case\nactive training performance does not depend on the chosen conﬁguration, and so absolute passive\nperformance curves are more directly comparable.\nSimilar to the results in the regular tandem setup, we observe (in Fig. 17) that the ability of the passive\nagent to maintain the initial performance level is substantially aided by the stochasticity resulting\nfrom a higher value of ε, providing further support for the importance of (D).\nTraining process samples (‘Groundhog day’):\nThe forked tandem experiments in Section 3.2\nindicate that data distributions represented by a ﬁxed replay buffer or a stream of data generated by\na single ﬁxed policy both show a lack of diversity leading to a catastrophic collapse of an (initially\nhigh-performing) agent when trained passively. The naive expectation that the (unbounded) stream of\ndata generated by a ﬁxed policy may provide a better state-action coverage than the ﬁxed-size dataset\nof a single replay buffer (1M transitions) is invalidated by the observation of the ﬁxed-replay training\nleading to somewhat slower degradation of passive performance. Unsurprisingly in hindsight, the\ndiversity given by the samples stemming from many different policies along a learning trajectory of\n1M training steps appears to be signiﬁcantly higher than that generated by a single ε-greedy policy.\nTo probe this further, we devise an experiment attempting to combine both: instead of freezing the\nactive policy after forking, we continue training it (and ﬁlling the replay buffer), however after each\niteration of 1M steps, the active network is reset to its parameter values at forking time. Effectively\nthis produces a data stream that can be viewed as producing samples of the training process of a\nsingle iteration, a variant that we refer to as the ‘Groundhog day’ experiment. This setting combines\nthe multiplicity of data-generating policies with the property of an unbounded dataset being presented\nto the passive agent. The results are shown in Fig. 24 - indeed we observe that the groundhog day\nsetting improves passive performance over the ﬁxed-policy setting, while not clearly changing the\noutcome in comparison to the ﬁxed-replay setting.\nOverall we observe a general robustness of the tandem effect with respect to minor experimental\nvariations in the forked tandem settings.\n19\n0\n50\n100\n150\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\nEnvironment frames (millions)\n0\n2500\n5000\n7500\n10000\n12500\nSeaquest\n0\n50\n100\n150\nEnvironment frames (millions)\n0\n500\n1000\n1500\n2000\nSpace invaders\n= 0.003\n= 0.01\n= 0.03\n= 0.1\n= 0.3\nFigure 23: Forked Tandem DQN: After 50 iterations of regular active training, the active value\nfunction is frozen and used to continuously generate data for the passive agents’ training by executing\nan ε-greedy policy with a given value of ε.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n0\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\nSpace invaders\nForked @25\nForked @50\nForked @100\nFigure 24: Forked Tandem DQN: ‘groundhog day’ variation, active vs. passive performance. After\nforking, the active agent trains for a full iteration (1M environment steps), and is then reset to its\ninitial network parameters at the time of forking, repeatedly for the remaining number of iterations.\nA.3\nForked Tandem: Policy Evaluation\nAmong the most striking ﬁndings in our work are the forked tandem ﬁndings in Sections 3.2 and\nA.2, demonstrating a catastrophic collapse of performance when passively training from a ﬁxed data\ndistribution, even when the starting point for the passive policy is the very same high-performing\npolicy generating the data. This leads to the question whether the process of Q-learning itself is to\nblame for this failure mode, e.g. via the well-known statistical over-estimation bias introduced by\nthe max operator [van Hasselt, 2010]. To test this, we perform two variants of the forked tandem\nexperiment with SARSA [Rummery and Niranjan, 1994] and (purely supervised) Monte-Carlo return\nregression based policy evaluation instead of Q-learning as the passive learning algorithm. (We note\nthat while SARSA evaluation of an ε-greedy policy can still exhibit over-estimation bias, this is not\nthe case for Monte-Carlo return regression.)\nAs can be seen in Figs. 6 and 25(top), even in this on-policy policy evaluation setting, the resulting\ncontrol performance catastrophically collapses after a short length of training. We also observe that\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n0\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\nForked @25\nForked @50\nForked @100\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.5\n1.0\n1.5\n2.0\nMC error\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.5\n1.0\n1.5\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n1\n2\n3\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.5\n1.0\n1.5\nSpace invaders\nForked @25\nForked @50\nForked @100\nFigure 25: Forked Tandem DQN: Policy evaluation with Monte-Carlo return regression. (top) Active\nvs. passive control performance. (bottom) Average absolute Monte-Carlo error. The Monte-Carlo\nerror is minimized effectively by the passive training, while the control performance of the resulting\nε-greedy policy collapses completely.\n20\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n0\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\nForked @25\nForked @50\nForked @100\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1\n2\n3\n4\nMC error\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n0.5\n1.0\n1.5\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0.0\n2.5\n5.0\n7.5\n10.0\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n2\n4\nSpace invaders\nForked @25\nForked @50\nForked @100\nFigure 26: Forked Tandem DQN: Policy evaluation with Monte-Carlo return regression, passive\nnetwork initialized independently of the active network at forking time. Top: Active vs. passive control\nperformance. Bottom: Average absolute Monte-Carlo error. While Monte-Carlo error is minimized\neffectively by the passive training, this does not result in above-random control performance of the\nresulting ε-greedy policy.\nthis is not due to a failing of the evaluation: Fig. 25(bottom) shows effective minimization of the\nMonte-Carlo error, indicating that the control failure is due to extrapolation error (and in particular,\nover-estimation) on infrequent (under the active policy) state-action pairs.\nThese ﬁndings provide strong support for the role of (D) and (F) while weakening that of (B): In\ncontrast to the well-known ‘Deadly Triad’ phenomenon [Sutton and Barto, 2018, van Hasselt et al.,\n2018], the tandem effect occurs without the amplifying mechanism of bootstrapping or the statistical\nover-estimation caused by the max operator, solely due to erroneous extrapolation by the function\napproximator to state-action pairs which are under-represented in the given training data distribution.\nWe have so far presented the equal network weights of active and passive networks at forking time\nas a strength of the forked tandem setting, following the intuition that an initialization by the high-\nperforming policy should be advantageous for maintaining performance in these experiments. A\nplausible counter-argument could be that the representation (i.e., the network weights) learned by the\nactive agent in service of control could be a poor, over-specialized starting point for policy evaluation.\nTo verify that this is not a major factor, we also perform the above Monte-Carlo evaluation experiment\nwith the passive network freshly re-initialized at the beginning of passive training. As shown in\nFig. 26, while a fresh initialization of the passive network indeed allows it to similarly effectively\nminimize Monte-Carlo error, its control performance here never exceeds random performance levels,\nfurther connecting the tandem effect to (D) and (F).\nWe remark that the demonstrated control performance failure of approximate policy evaluation casts a\nshadow over the concept of approximate policy iteration, or the application of this concept in heuristic\nexplanations of the function of empirically successful algorithms like DQN [Mnih et al., 2015].\nA successful greedy improvement step on an approximately evaluated policy appears implausible\ngiven the brittleness of approximate policy evaluation even in the nominally best-case scenario of an\non-policy data distribution.\nAnother view point emerging from these results is that the classic category of ‘on-policy data’ appears\nless relevant in this context: an appropriate data distribution for robust approximate evaluation\ntargeting control seems to require a data distribution sufﬁciently overlapping with the (hypothetical,\nin practice unavailable) behavior distribution of the resulting policy rather than the original evaluated\npolicy.\nA.4\nAdditional Experimental Results on the Role of Function Approximation\nHere we present several extra experiments, complementing the results from the end of Section 3.3.\nThe ﬁrst set of results, shown in Fig. 27, concerns the passive performance in the forked tandem\nsetup, when the ﬁrst (bottom) neural network layers are shared between active and passive agents.\n21\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n50\n100\n150\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n200\n400\n600\n800\n1000\nSpace invaders\ntied layers=0\ntied layers=1\ntied layers=2\ntied layers=3\ntied layers=4\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n500\n1000\n1500\nSpace invaders\ntied layers=0\ntied layers=1\ntied layers=2\ntied layers=3\ntied layers=4\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\nSpace invaders\ntied layers=0\ntied layers=1\ntied layers=2\ntied layers=3\ntied layers=4\nFigure 27: Forked Tandem DQN: passive performance after forking, with the ﬁrst k (out of 5) layers\nof active and passive agent networks shared, i.e. only trained by the active agent, and ﬁxed during the\npassive training phase.\nThese bottom layers are only trained during the active training phase before forking, and are frozen\nafter that. Similar to the corresponding experiment in the regular tandem setting (Fig. 9), we observe\nthat the passive agent’s ability to maintain its initially high performance strongly correlates with the\nnumber of shared, i.e. not passively trained layers. The difference between the passive agent training\na ‘deep’ vs. a ‘linear’ network (the latter corresponds to all but one of the network layers being frozen)\nappears stark: the tandem effect is almost equally catastrophic in all conﬁgurations except for the\nlinear one, where it appears to be strongly reduced. While a more thorough investigation remains to\nfuture work, we remark that overall this ﬁnding appears to supports the importance of (F), in that\nintuitively over-extrapolation can be expected to become more problematic when passive training is\napplied to a larger function class (deeper part of the network).\nThe next experiment, shown in Fig. 28 and extending the results from Fig. 8, investigates the impact\nof network architecture more generally, by varying width and depth of both active and passive agents’\nnetworks. Since changes in Atari-DQN network architecture tend to require expensive re-tuning\nof various hyperparameters, we chose to perform these experiments on the smaller Classic Control\ndomains, where such changes tend to be more straightforward. Nevertheless active performance\nin these domains does depend on the chosen network conﬁguration, so that we report relative\nperformance as the more informative quantity. The ﬁndings across four domains mostly appear\nto echo those on CARTPOLE described in Section 3.3, showing a positive correlation of (relative)\npassive performance with network depth, and a negative correlation with its width. Again, a more\ndetailed investigation of the causes for this exceeds the scope of this paper and is left to future work.\nA.5\nApplications of Tandem RL: Passive QR-DQN\nHere we provide an example application for the Tandem RL setting as an analytic tool for the study\nof learning algorithms in isolation from the confounding factor of behavior. As observed in [Agarwal\n22\n0\n100\n200\n300\n400\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nAcrobot - 0 hidden layers\n0\n100\n200\n300\n400\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartPole - 0 hidden layers\n0\n25\n50\n75\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0LunarLander - 0 hidden layers\n0\n100\n200\n300\n400\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MountainCar - 0 hidden layers\n4 hidden units\n128 hidden units\n512 hidden units\n0\n100\n200\n300\n400\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nAcrobot - 1 hidden layers\n0\n100\n200\n300\n400\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartPole - 1 hidden layers\n0\n25\n50\n75\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0LunarLander - 1 hidden layers\n0\n100\n200\n300\n400\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MountainCar - 1 hidden layers\n4 hidden units\n128 hidden units\n512 hidden units\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative passive performance\nAcrobot - 2 hidden layers\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCartPole - 2 hidden layers\n0\n25\n50\n75\n100\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0LunarLander - 2 hidden layers\n0\n100\n200\n300\n400\nEnvironment frames (thousands)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MountainCar - 2 hidden layers\n4 hidden units\n128 hidden units\n512 hidden units\nFigure 28: Tandem DQN: Passive performance as a fraction of active performance when varying\nnetwork architecture in classic control games. Here network architecture varies for both active and\npassive agent, so active performance is also affected by the conﬁguration.\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n100\n200\n300\n400\n500\nEpisode return\nBreakout\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n20\n10\n0\n10\n20\nPong\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n5000\n10000\n15000\nSeaquest\n0\n50\n100\n150\n200\nEnvironment frames (millions)\n0\n1000\n2000\n3000\nSpace invaders\nactive\npassive\nFigure 29: QR-DQN as a passive learning algorithm, in tandem with a Double-DQN active agent:\nactive vs. passive performance.\net al., 2020], the QR-DQN algorithm [Dabney et al., 2018] can be preferable to DQN in the ofﬂine\nsetting, motivating our attempt to use it as a passive agent, coupled with a regular Double-DQN active\nagent. As shown in Fig. 29, QR-DQN indeed provides a somewhat different passive performance\nproﬁle when compared to the regular Double-DQN tandem, albeit not a clearly better one. While\nperfectly matching active performance in one game (SPACE INVADERS) and even out-performing the\nactive agent in another (BREAKOUT), it also shows exacerbated under-performance or instability in\nthe other two domains. A ﬁne-grained diagnosis of the causes of this are left to future work.\nWe note that any difference in performance between the DQN and QR-DQN algorithms as passive\nagents reﬂects directly on their properties as learning algorithms, i.e. their respective abilities to\nextract information about an appropriate control policy from observational data, while separating\nout any inﬂuence their learning dynamics may have on (transient) behavior and data generation. We\nbelieve that Tandem RL can become a valuable analytic tool for targeted empirical studies of such\nproperties of learning algorithms.\n23\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-10-26",
  "updated": "2021-10-26"
}