{
  "id": "http://arxiv.org/abs/2110.14524v1",
  "title": "Model based Multi-agent Reinforcement Learning with Tensor Decompositions",
  "authors": [
    "Pascal Van Der Vaart",
    "Anuj Mahajan",
    "Shimon Whiteson"
  ],
  "abstract": "A challenge in multi-agent reinforcement learning is to be able to generalize\nover intractable state-action spaces. Inspired from Tesseract [Mahajan et al.,\n2021], this position paper investigates generalisation in state-action space\nover unexplored state-action pairs by modelling the transition and reward\nfunctions as tensors of low CP-rank. Initial experiments on synthetic MDPs show\nthat using tensor decompositions in a model-based reinforcement learning\nalgorithm can lead to much faster convergence if the true transition and reward\nfunctions are indeed of low rank.",
  "text": "Model based Multi-agent Reinforcement Learning\nwith Tensor Decompositions\nPascal Van Der Vaart\nTU Delft\np.r.vandervaart-1@tudelft.nl\nAnuj Mahajan\nUniversity of Oxford\nanuj.mahajan@cs.ox.ac.uk\nShimon Whiteson\nUniversity of Oxford\nshimon.whiteson@cs.ox.ac.uk\nAbstract\nA challenge in multi-agent reinforcement learning is to be able to generalize over\nintractable state-action spaces. Inspired from Tesseract [Mahajan et al., 2021], this\nposition paper investigates generalisation in state-action space over unexplored\nstate-action pairs by modelling the transition and reward functions as tensors\nof low CP-rank. Initial experiments on synthetic MDPs show that using tensor\ndecompositions in a model-based reinforcement learning algorithm can lead to\nmuch faster convergence if the true transition and reward functions are indeed of\nlow rank.\n1\nIntroduction\nRecent progress in multi-agent RL has been promising towards creating agents which are capable\nof generalising over multiple tasks [DeepMind-OEL et al., 2021], they have also demonstrated\neffectiveness in dealing with the problem of exploration in a large action space [Mahajan et al.,\n2019, Gupta et al., 2020] and overcoming intractability arising from its exponential growth in the\nnumber of agents[Mahajan et al., 2021, Wang et al., 2020a,b] when learning under constraints like\ndecentralisation. Inspired from Tesseract [Mahajan et al., 2021], which utilises tensor decomposition\nstructure in factored action spaces, we investigate whether tensor decompositions can be used to\nattain generalisation across the state-action space in cooperative multi-agent setting towards obtaining\nbetter sample efﬁciency. In this position paper, we focus on the model based setting. Initial empirical\nresults on randomly generated MDPs provide promising evidence for state-action generalisation and\nsample efﬁciency using tensor decompositions over baseline model based algorithms which do not\nuse the tensor approximation.\nIn multi-agent reinforcement learning, the goal is to ﬁnd a policy for multiple agents that performs\nwell on a given task. Tasks are formalized by Markov decision processes (MDP), which are described\nby a transition function and a reward function. In the RL setting, the transition and reward functions\nare unknown, and ﬁnding a policy that achieves high reward requires exploration to gather data from\nthe MDP and learn its dynamics. Because the size of the action space grows exponentially with the\namount of agents, it is especially important in multi-agent reinforcement learning to learn with high\nsample efﬁciency as coverage over all state action pairs is not feasible. This work showcases how the\nuse of low rank CP-decompositions can drastically improve sample efﬁciency of classic model-based\nreinforcement learning algorithms and provide better generalisation when the transition and reward\ntensors are of low CP-rank.\n2nd Workshop on Quantum Tensor Networks in Machine Learning (NeurIPS 2021), Sydney, Australia.\narXiv:2110.14524v1  [cs.LG]  27 Oct 2021\n2\nBackground\n2.1\nTensor decompositions\nDeﬁnition 1 An order n tensor over a ﬁeld F with dimensions d1, d2, . . . , dn is a multilinear map\nT : Fd1 × Fd2 × · · · × Fdn →F, and can be represented by a n-dimensional d1 × d2 × · · · × dn\narray Ti1i2...in such that the mapping is deﬁned as\nT(u1, . . . , un) =\nd1\nX\ni1=1\nd2\nX\ni2=1\n· · ·\ndn\nX\nin=1\nTi1i2...inu1\ni1u2\ni2 . . . un\nin\nfor ui ∈Fdi. For a set of indices i1, . . . ik ∈{1, . . . , n} the expression\nT(u1, . . . , ui1−1, I, ui1+1, . . . ui2−1, I, ui2+1, . . . , uik−1, I, uik+1, . . . , un)\ndenotes a di1 × · · · × dik tensor deﬁned by the mapping (ui1, ui2, . . . , uik) 7→T(u1, . . . , un).\nThe set of order n tensors with dimensions d1, d2, . . . dn over F is denoted by Fd1×d2×···×dn.\nThe CANDECOMP/PARAFAC (CP) decomposition for tensors can be thought of as a generaliza-\ntion of the singular value decomposition for matrices. Related work involving tensors and tensor\ndecompositions can be found in appendix A.\nDeﬁnition 2 A rank r CP-decomposition of a tensor T ∈Fd1×d2×···×dn is a set of vectors\n(ui\nl)i=1,...n\nl=1,...r , ul\ni ∈Fdi and scalars (wl)l=1,...,r ∈F such that\nT =\nr\nX\nl=1\nwlu1\nl ⊗u2\nl ⊗· · · ⊗un\nl ,\nwhere ∥ui\nl∥= 1 for all l ∈{1, . . . , r} and i ∈{1, . . . , n}.\nThe tensor T is said to be of CP-rank r if r is the smallest number for which a rank r CP-\ndecomposition for T exists.\nIt is clear that if a large n × n × n tensor T has low rank, the search space of an application which\nrequires an estimate of T can be greatly reduced by incorporating the low rank information. Instead\nof estimating n3 parameters, the problem can be described by 3rn parameters in decomposed form\ninstead. The main idea of this work is to use this fact to efﬁciently estimate the transition and reward\ntensors in discrete multi-agent reinforcement learning problems.\n2.2\nReinforcement learning\nIn reinforcement learning, the goal is to compute a strategy to perform a certain task. Tasks are\nformalized as Markov decision processes (MDPs) (S, A, T, R, γ), where S and A are the state and\naction spaces, T and R are the transition and reward function and γ is the discount factor. At each time\nstep t ∈N an agent chooses an action at ∈A based on the state st ∈S. The environment then returns\na reward rt = R(st, at) and the next state st+1 ∼T(·|st, at). The strategy to choose actions is called\nthe policy π : S × A →[0, 1] which deﬁnes a probability distribution over the actions given the\ncurrent state. The goal of the agent is to maximize the expected discounted reward ET,π\nhPH\nt=1 γtrt\ni\n,\nwhere the expectation is over the states and actions, whose probability distributions are implied by\nthe transition function T and policy π.\nIn multi-agent reinforcement learning (MARL), there are multiple agents that interact with the\nenvironment as opposed to only one agent. Each agent has its own action space Ai, which means that\nthe transition function is now a function S × A1 × · · · × An × S →[0, 1] and the reward function is\nS × A1 × A2 × · · · × An →[0, 1].\nClearly this can be cast as a single agent reinforcement learning problem by setting A = A1×· · ·×An.\nA result of this is that the action space grows exponentially large with the number of agents, further\nincreasing the requirement of efﬁcient exploration. Instead of casting it as a single agent reinforcement\nlearning problem, explicitly incorporating the multi-agent paradigm allows to exploit more structure\nin the MDP.\n2\nIn this work, this is done by considering the transition function to be a tensor T ∈RS×A1×···×An×S\nsuch that Tsa1...ans′ = T(s, a1, . . . , an, s′). The reward function is analogously written as a tensor\nR ∈Rs×a1×···×an. If the tensors T and R are of low rank, models formed by an agent during\ntraining can be expected to generalize across unseen state-action pairs.\n3\nMethods\n3.1\nTensor decomposition algorithms\nWhile computing tensor decompositions is NP-hard in general Hillar and Lim [2013], there exist\nalgorithms such as Harshman [1970] and Anandkumar et al. [2015] which are proven to converge\nin special cases. The algorithm used in this work is an ablation of the alternating rank 1 updates\nalgorithm presented in Anandkumar et al. [2015]. The restarts, clustering and clipping procedures are\nleft out to form a shorter and simpler algorithm which still performs well in practice. The algorithm\nas used is presented in algorithms 3, 4, and 1. The main idea of the algorithm is to run asymmetric\npower updates to compute a good starting value for alternating minimization, which further improves\nthe accuracy of the decomposition.\n3.2\nTensor completion\nIn the tensor completion problem, the goal is to recover a tensor with only partially observed entries.\nFor a tensor T ∈Rd1×···×dn, let Ω∈{0, 1}d1×···×dn denote a mask such that Ωi1...in = 1 if and\nonly if entry Ti1...in has been observed. A method proposed in Jain and Oh [2014] and Liu and\nMoitra [2020] involves solving the minimization problem\nargmin\n{uj\nk}j=1...n\nk=1...r∈Rdj ,{wk}k=1...r∈R\n∥Ω· T −Ω·\nr\nX\nk=1\nwku1\nk ⊗u2\nk ⊗· · · ⊗un\nk∥F ,\nwhere · denotes an entrywise multiplication. Algorithm 1 can be used to solve this problem with a\nslight modiﬁcation to the alternating minimization step as showcased in appendix D.\n3.3\nModel based reinforcement learning\nIn model based reinforcement learning agents make models of the environment to plan ahead, instead\nof attempting to maximize reward directly. Under the assumption that the transition and reward\ntensors are of low rank, using tensor decomposition allows for sample efﬁcient models that generalize\nover unseen state-action pairs.\nThe deterministic reward tensor is estimated using tensor completion, where the unobserved entries\nare simply the state-action pairs the agents have never experienced. After enough exploration, enough\nentries of the reward tensor will be revealed to reconstruct the entire tensor.\nThe algorithm presented in this paper follows a very standard model-based reinforcement learning\napproach and is presented in algorithm 3.4. The NORMALIZE function is an entry wise division so\nthat the resulting tensor is a transition tensor, that is the sum over the resulting states is 1. The\nPOLICYIMPROVEMENT function is clariﬁed in appendix C.\n3.4\nRelationship to Tesseract\nThis method differs from Mahajan et al. [2021] because in this work, decompositions of the entire\ntensor T ∈RS×A1×···×An×S and R ∈RS×A1×···×An are computed. Model based Tesseract instead\nconsiders for each s, s′ ∈S the A1 × · · · × A2 tensor ˜Tss′ = T(es, I, . . . , I, e′\ns), and computes an\nindividual tensor decomposition for each state and next state pair. Analogously, it considers for each\nstate s ∈S the reward tensor Rs = R(es, I, . . . , I) and computes a decomposition for every state.\nAlgorithm 1: Alternating rank 1 updates\nInput: A tensor T ∈Rm×n×p and decomposition rank r\nResult: {uj\nk}j=1...n\nk=1...r, {wk}k=1...r such that T ≈Pr\nk=1 wku1\nk ⊗u2\nk ⊗· · · ⊗un\nk\n({uj\nk}j=1...n\nk=1...r, {wk}k=1...r) = PowerIteration(T, r) (algorithm 3);\n({uj\nk}j=1...n\nk=1...r, {wk}k=1...r) = AlternatingMinimize(T, {uj\nk}j=1...n\nk=1...r, {wk}k=1...r) (algorithm 4);\n3\nIn theory, both methods can represent the same transition and reward functions. To see this,\nconsider for example an MDP with 2 agents, such that the reward tensor is of order 3.\nLet\nR = Pr\ni=1 wixi ⊗yi ⊗zi be the true reward tensor. This can represented by Tesseract by set-\nting Rs = Pr\ni=1 wi⟨es, xi⟩yi ⊗zi, where es is the s-th standard basis vector. This results in the\ncombined reward tensor\nX\ns∈S\nes ⊗Rs =\nX\ns∈S\nes ⊗\nr\nX\ni=1\nwi⟨es, xi⟩yi ⊗zi =\nr\nX\ni=1\nwi\nX\ns∈S\nes ⊗⟨es, xi⟩yi ⊗zi = R\nConversely, if R is of the form P\ns∈S es ⊗Rs where each Rs is of rank r, then the rank of R is\nbounded by |S|r so it can be represented in our framework. Thus, when low rank structure spans\nacross states, out method would ensure better sample efﬁciency as it would require fewer number of\nparameters.\nAlgorithm 2: CP-Decomposed state-action space reinforcement learning\nInput: An MDP, state space size S, action space sizes A1, . . . An, Hyperparameters:\nnepisodes, ntrain, ϵ, nimprovement iter\nResult: Policy π with good performance\nD = 0 ∈NS×A1×···×An×S;\nR = 0 ∈RS×A1×···×An;\nInitialize random π;\nfor episode = 1, 2, . . . , nepisodes do\ns1 ∼MDP;\na1 ∼\n\u001aπ(s0)\nwith probability 1 −ϵ(episode)\nUnif(A)\nwith probability ϵ(episode)\n;\nfor t = 1, . . . , episode length do\n(st+1, rt) ∼MDP(.|st, at);\nDstatst+1 = Dstatst+1 + 1;\nRstat = rt;\nat ∼\n\u001aπ(st)\nwith probability 1 −ϵ(episode)\nUnif(A)\nwith probability ϵ(episode)\n;\nif nepisodes % ntrain = 0 then\nˆT = DECOMP(NORMALIZE(D), rT );\nΩ= D > 0;\nˆR = TENSORCOMPLETION(R, Ω, rR);\nπ = POLICYIMPROVEMENT(π, ˆT, ˆR, nimprovement iter);\n4\nExperiments\n4.1\nRandom transition and reward functions of predeﬁned rank\nThis experiment involves algorithm 3.4 applied to an MDP described by a randomly generated\ntransition tensor T and reward tensor R. The MDP has 20 states, and 3 agents with 10 actions each,\nleading to 20000 state-action pairs. Both the transition and reward tensors are of rank 5. More\ninformation on how they are generated can be found in appendix E.\nWe tested three different agents for experiments. The ﬁrst agent is a baseline agent which uses\nno decompositions. It uses the maximum likelihood estimator for T and ﬁlls in missing rewards\nfor unvisited state-action pairs with the mean of the visited rewards. Secondly, an agent using\ndecomposition across entire state action space for for T and R with three settings of approximate\nrank 5 (exact), 3 (insufﬁcient), 10 (overparametrised). The ﬁnal agent is model based Tesseract with\nrank 5 and 1 decompositions. Note that the rank 5 case can represent the correct transition and reward\ntensors, but is overparametrised for the task (60000 parameters versus 350 for our rank 5 agent for\nthe transition function). Similarly, the rank 1 agent will be insufﬁcient for representing the actual\ndynamics but will provide faster learning.\n4\nFor each agent, if the slice through the estimated transition tensor T corresponding to a speciﬁc state-\naction pair contains only zeros, all entries are set to\n1\n|S|. This means that if there is no estimate for\nT(·|s, a1, a2, a3), a uniform distribution is assumed instead. This happens for the no decompositions\nagent exactly when a state-action pair has never been visited before.\nEach agent is trained for 200 episodes, recomputing their models and applying policy improvement\nevery 10 episodes. The agents use ϵ greedy exploration with epsilon decaying from 0.9 to 0.1. During\ntraining, the total episodic rewards, errors in the transition tensor and errors in the reward tensors\nare tracked. The entire experiment is ran 20 times, with newly generated T and R for each run.\nThe optimal reward in each experiment is computed beforehand via policy improvement on the true\nfunctions T and R, and then for each experiment the optimal reward is subtracted from the episodic\nrewards so that optimal performance is a reward of 0 for each experiment. Finally, the number of\nunique visited state-action pairs is also tracked. The results of the experiment are shown in ﬁgures\n1, 2 and 5a. Figure 1 shows that algorithm 3.4 signiﬁcantly outperforms a standard model-based\napproach without tensor decompositions in the setting where T and R are of low rank. While the\nagent with rank 3 decompositions achieves a slightly sub-optimal policy, the performance seems to\nbe quite robust against incorrectly guessing the correct rank for the problem.\nInterestingly, agents without tensor decompositions outperform the agents that use tensor decomposi-\ntions during the ﬁrst few episodes. This can be attributed to the unrobustness of tensor completion.\nFigure 1 shows that for our algorithm, during the ﬁrst 20 episodes the error in the reward tensor\ncan be of order 105 and higher, because the optimization problem is very ill-conditioned when little\nentries are revealed. Tesseract suffers even more from this problem, as each individual state now\nrequires sufﬁciently many revealed entries. A way to overcome these problems could be for example\nto take the naive estimate without tensor completion when attempting tensor completion results in\nvery extreme values, or adding regularization to the optimization problem. Figure 1 also shows that\nwith sufﬁciently many revealed entries, our method achieves very good approximates of the reward\ntensor. If the approximate reward tensor rank is set correctly (rank 5), the reward tensor is recovered\nalmost exactly after visiting only 4000 (see ﬁgure 5a) or 20% of the state-action pairs. Setting the\nrank results in slower convergence, but still yields a reasonably good estimate. Finally, setting the\nrank too low causes the agent to be incapable of representing the true reward tensor, but on limited\nrevealed entries this estimate still outperforms the estimate without decompositions. Tesseract with\nrank 5 decompositions takes a long time to get a good estimate, but eventually outperforms our\nmethod with rank 3 decompositions. This is explained by the analysis in 3.4, which showed that\nrank 5 Tesseract is in theory capable to represent the true reward tensor, albeit requiring many more\nsamples in comparison as conﬁrmed by this experiment.\nFigure 2 shows the error in transition tensors. Note that the error of Tesseract and the agent without\ndecompositions increases over time. This is due to the fact that for many states-action pairs, the\ndefault uniform distribution assigning probability\n1\n|S| to each state is a better estimate than an extreme\ndistribution resulting from only one observation of that state-action pair. Figure 5a shows that even\nafter 200 episodes, only around 11000 state action pairs out of 20000 total are visited, meaning\nthat many state-action pairs are likely to have been visited only once. This means that unless an\nagent can combine information from different state-action pairs, it is unfeasible to make a good\ntransition function estimate. Since using no decompositions assumes every state-action pair to be\nindependent, there is no generalization across states-action pairs. Tesseract does slightly better as\nit attempts to generalize the action space for each state independently, but ﬁgure 2 shows that our\nmethod produces signiﬁcantly better transition tensor estimates by attempting to generalize over the\ncombined state-action space.\n4.2\nMDP with degenerate states\nIn this experiment we test algorithm 3.4 on state degeneracy, a situation where our method can\nprovide further sample efﬁciency. State degeneracy can occur when observations are noisy. We\nuse an MDP with 3 agents, this time with 16 states and each agent has an action space of size 20.\nThe 16 states are split into 4 groups, where each group has the same transition function of rank\n1, and a linearly dependent reward tensor of rank 1. This means that rank 1 Tesseract is expected\nto be able to recover exact models after enough iterations. Furthermore, similar to the analysis in\n3.4, writing R = P\ni=1,5,9,13(ei + ei+1 + ei+2 + ei+3) ⊗Ri where Ri denotes a reward tensor\nfor each group, reveals that the entire reward tensor is of rank at most 4. Similar to the previous\nexperiment, we consider the following agents: A baseline agent using no decomposition, Agents\n5\n0\n50\n100\n150\n200\nepisodes\n4\n3\n2\n1\n0\ntotal reward\n0\n50\n100\n150\n200\nepisodes\n20\n15\n10\n5\n0\n5\n10\nlog reward SSE\nrank_5_decomp\nrank_3_decomp\nrank_10_decomp\nno_decomp\ntesseract_rank_5\ntesseract_rank_1\noptimal\nFigure 1: Total reward per episode at test time (left) and sum of squared errors of the reward tensor\nestimate (right) after a number of episodes of training.\n0\n50\n100\n150\n200\nepisodes\n0.000\n0.005\n0.010\n0.015\n0.020\ntransition MSE\n0\n50\n100\n150\n200\nepisodes\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\ntransition MSE\nrank_5_decomp\nrank_3_decomp\nrank_10_decomp\nno_decomp\ntesseract_rank_5\ntesseract_rank_1\nFigure 2: Mean squared error of the estimate of the transition tensor after training a number of\nepisodes in the ﬁrst experiment. The right plot is a zoomed in version of the left plot.\nusing decomposition across state action space with ranks 4 and 8, Tesseract with rank 4 and 1. The\nentire experiment is repeated 20 times. The results are shown in ﬁgures 3, 4 and 5b. Like in the results\nof the ﬁrst experiment, ﬁgure 3 shows that the agents that use decompositions accross state-action\nspace outperform the other agents in terms of total reward obtained. This is mostly attributable to\nthe performance on the reward tensor error. We also observe that Tesseract is unable to recover a\ngood reward tensor estimate in the given sample budget. In contrast to the previous experiment, the\ntransition tensor estimates of the agents that use state-action decompositions do not differ signiﬁcantly\nfrom the estimates made by Tesseract. This can be explained by the fact that in this experiment,\nthe groups themselves have entirely independent transition and reward functions, which means that\ngeneralization is only possible within groups. In contrast, the low rank structure imposed on the\nentire transition tensor in the ﬁrst experiment allowed our method to generalize over all states.\n5\nConclusion\nIn this position paper we investigated whether tensor decompositions can be used across state actions\nspace for better sample efﬁciency in RL for the model based setting. Our experiments show that an\nalgorithm which computes CP-decompositions of the environment models has signiﬁcant advantages\nwhen the MDP is described by low rank transition and reward functions.\n0\n50\n100\n150\n200\nepisodes\n50\n40\n30\n20\n10\n0\n10\ntotal reward\n0\n50\n100\n150\n200\nepisodes\n10\n5\n0\n5\n10\n15\n20\nlog reward SSE\nrank_8_decomp\nrank_4_decomp\nno_decomp\ntesseract_rank_1\ntesseract_rank_4\noptimal\nFigure 3: Total reward per episode at test time (left) and sum of squared errors of the reward tensor\nestimate (right) after a number of episodes of training in the state degeneracy MDP.\n6\nReferences\nAnuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean Kossaiﬁ,\nShimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for\nmulti-agent reinforcement learning. In Proceedings of the 38th International Conference on\nMachine Learning, volume 139, pages 7301–7312. PMLR, 2021. URL https://proceedings.\nmlr.press/v139/mahajan21a.html.\nDeepMind-OEL, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub\nSygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-\nSchmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard,\nand Wojciech Marian Czarnecki. Open-ended learning leads to generally capable agents. arXiv\npreprint arXiv:2107.12808, 2021.\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent\nvariational exploration. In Advances in Neural Information Processing Systems, pages 7611–7622,\n2019.\nTarun Gupta, Anuj Mahajan, Bei Peng, Wendelin Böhmer, and Shimon Whiteson. Uneven: Universal\nvalue exploration for multi-agent reinforcement learning. arXiv preprint arXiv:2010.02974, 2020.\nTonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang.\nRode: Learning roles to decompose multi-agent tasks. arXiv preprint arXiv:2010.01523, 2020a.\nJianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling\nmulti-agent q-learning. arXiv preprint arXiv:2008.01062, 2020b.\nChristopher J. Hillar and Lek-Heng Lim. Most tensor problems are np-hard. J. ACM, 60(6), November\n2013. ISSN 0004-5411. doi: 10.1145/2512329. URL https://doi.org/10.1145/2512329.\nRichard Harshman. Foundations of the parafac procedure: Models and conditions for an \"explanatory\"\nmulti-modal factor analysis. UCLA Working Papers in Phonetics, 16, 1970.\nAnimashree Anandkumar, Rong Ge, and Majid Janzamin. Guaranteed non-orthogonal tensor decom-\nposition via alternating rank-1 updates, 2015.\nPrateek Jain and Sewoong Oh. Provable tensor factorization with missing data. In Z. Ghahramani,\nM. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Informa-\ntion Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.\nneurips.cc/paper/2014/file/c15da1f2b5e5ed6e6837a3802f0d1593-Paper.pdf.\nAllen Liu and Ankur Moitra. Tensor completion made practical. In H. Larochelle, M. Ranzato,\nR. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 18905–18916. Curran Associates, Inc., 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/dab1263d1e6a88c9ba5e7e294def5e8b-Paper.pdf.\nAnimashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments for mixture\nmodels and hidden markov models. In Shie Mannor, Nathan Srebro, and Robert C. Williamson,\neditors, Proceedings of the 25th Annual Conference on Learning Theory, volume 23 of Proceedings\nof Machine Learning Research, pages 33.1–33.34, Edinburgh, Scotland, 25–27 Jun 2012. PMLR.\nURL https://proceedings.mlr.press/v23/anandkumar12.html.\nAnimashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor\ndecompositions for learning latent variable models. J. Mach. Learn. Res., 15(1):2773–2832,\nJanuary 2014. ISSN 1532-4435.\nAndrzej Cichocki, A. Phan, Qibin Zhao, Namgil Lee, I. Oseledets, Masashi Sugiyama, and Danilo P.\nMandic. Tensor networks for dimensionality reduction and large-scale optimization: Part 2\napplications and future perspectives. Found. Trends Mach. Learn., 9:431–673, 2017.\nYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration\nfor deep neural networks. CoRR, abs/1710.09282, 2017. URL http://arxiv.org/abs/1710.\n09282.\n7\nJean Kossaiﬁ, Adrian Bulat, Georgios Tzimiropoulos, and Maja Pantic. T-net: Parametrizing fully\nconvolutional nets with a single high-order tensor. 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 7814–7823, 2019.\nJean Kossaiﬁ, Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy M. Hospedales, and Maja\nPantic. Factorized higher-order cnns with an application to spatio-temporal emotion estimation.\n2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6059–\n6068, 2020.\nAdrian Bulat, Jean Kossaiﬁ, Georgios Tzimiropoulos, and Maja Pantic. Incremental multi-domain\nlearning with network latent tensor factorization. In AAAI, 2020.\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,\nMax Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.\nValue-decomposition networks for cooperative multi-agent learning based on team reward. In\nProceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems,\nAAMAS ’18, page 2085–2087, Richland, SC, 2018. International Foundation for Autonomous\nAgents and Multiagent Systems.\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and\nShimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforce-\nment learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International\nConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,\npages 4295–4304. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/\nrashid18a.html.\nStefano Bromuri. A tensor factorization approach to generalization in multi-agent reinforcement\nlearning. In 2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent\nAgent Technology, volume 2, pages 274–281, 2012. doi: 10.1109/WI-IAT.2012.21.\nKamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning\nof pomdps using spectral methods. 06 2016.\n8\nA\nRelated work\nWork on tensor decompositions in general machine learning include Anandkumar et al. [2012], which\nuses the CP-decomposition to learn mixture models and hidden Markov models, and Anandkumar\net al. [2014] learns latent variable models. Another application of tensor methods is to compress\nneural networks in Cichocki et al. [2017] and Cheng et al. [2017]. Kossaiﬁet al. [2019] parametrizes\nconvolutional nets with a high-order tensor of low rank to reduce over-parameterization, with applica-\ntions to spatio-temporal tasks in Kossaiﬁet al. [2020]. In Bulat et al. [2020] this parametrization is\nused for multi-domain image classiﬁcation.\nPrevious reinforcement learning methods that attempt to exploit structure in the multi-agent setting\ninclude VDN Sunehag et al. [2018], which models the joint Q-function (see appendix C) as a sum of\nthe agents individual Q-functions. This is generalised by QMIX Rashid et al. [2018], which learns a\nmonotonic function of the individual Q-functions instead of taking a sum.\nMethods for generalization in multi agent reinforcement learning using speciﬁcally tensor decompo-\nsitions include Bromuri [2012], where tensor decompositions are used to factorize the Q-function in\nmodel free Q−learning algorithms. Our method instead factorizes the estimated transition and reward\nfunctions of the MDP in a model based algorithm. This idea was initially proposed in Mahajan et al.\n[2021], which contains a model free algorithm and a model based algorithm. The difference between\nthe model based algorithm in Mahajan et al. [2021] and our method, is that we factorize over the\nstate-action space, while Mahajan et al. [2021] factorizes only over the action space. This allows our\nwork to potentially generalize over unseen states instead of only over unseen actions.\nWork in tensor decompositions for partially observable MDPs (POMPDs) in a single agent setting\ninclude Azizzadenesheli et al. [2016]. Adapting our method for generalisation in multi agent MDPs\nfor POMPDs is an interesting future research direction\nB\nDecomposition algorithms\nAlgorithm 3: Tensor power iteration with deﬂation\nInput: A tensor T ∈Rd1×···×dn and decomposition rank r, tolerance ϵ\nResult: Vectors {uj\nk}j=1...n\nk=1...r and scalars {wk}k=1...r such that\nT ≈Pr\nk=1 wku1\nk ⊗u2\nk ⊗· · · ⊗un\nk\nfor k = 1, ...r do\nfor j = 1, ..., n do\nuj\nk,0 ∼N(0, 1);\nuj\nk,0 =\nuj\nk\n∥uj\nk∥;\nend\nwhile Pn\nj=1 ∥uj\nk,n+1 −uj\nk,n∥2 > ϵ do\nfor j=1, ..., n do\nuj\nk,m+1 =\nT (u1\nk,m,...,uj−1\nk,m,I,uj+1\nk,m,...,un\nk,m)\n∥T (u1\nk,m,...,uj−1\nk,m,I,uj+1\nk,m,...,un\nk,m)∥;\nend\nend\nwk = T(u1\nk, . . . , un\nk);\nwN\nk = ckwN\nk ;\nT = T −wku1\nk ⊗u2\nk ⊗· · · ⊗un\nk;\nend\n9\nAlgorithm 4: Alternating minimization\nInput: A tensor T, starting values for {uj\nk}j=1...n\nk=1...r, {wk}k=1...r\nResult: {uj\nk}j=1...n\nk=1...r, {wk}k=1...r such that T ≈Pr\nk=1 wku1\nk ⊗u2\nk ⊗· · · ⊗un\nk\nwhile stopping criterion do\nfor l = 1, . . . , r do\nfor j = 1, . . . , n do\nuj\nl = (T −Pr\nk̸=l u1\nk ⊗u2\nk ⊗· · · ⊗un\nk)(u1\nl , . . . , uj−1\nl\n, I, uj+1\nl\n, . . . , un\nl );\nuj\nl =\nuj\nl\n∥uj\nl ∥;\nend\nwl = (T −Pr\nk̸=l u1\nk ⊗u2\nk ⊗· · · ⊗un\nk)(u1\nl , . . . , un\nl );\nend\nend\nC\nPolicy improvement\nPolicy improvement is a well known method in reinforcement learning to compute optimal policies\nwith respect to the MDP parameters T and R. In model based reinforcement learning algorithms, this\nis used to compute a good policy after estimating T and R with estimates ˆT and ˆR. If the estimates\nare close enough, the optimal policy with respect to ˆT, ˆR will also perform well on the actual MDP\ndescribed by T, R.\nComputing the optimal policy uses the Q-function, which maps each state s and each action a to\nthe expected reward of executing action a in state s and following policy π afterwards. This can\nrecursively be written as\nQπ(s, a) = R(s, a) + γ\nX\ns′∈S\nT(s′|a, s)\nX\na′∈A\nπ(a′|s′)Qπ(s′, a′)\nThe value function maps each state to the expected future reward in the state when following policy\nπ. This can be computed from the Q-function via\nV π(s) =\nX\na∈A\nQ(s, a)π(a|s)\nThe main idea of policy improvement is to iteratively select states for which V π(s)\n<\nmaxa∈A Qπ(s, a). This means that there exists an action which achieves better reward than the\ncurrent policy, so the policy π is updated to use the better action instead. After this, since the policy\nhas changed, Qπ and V π need to be computed again to repeat this process. This is guaranteed to\nconverge to an optimal policy eventually on ﬁnite state and action spaces.\nAlgorithm 5: Policy improvement\nInput: Starting policy π0\nResult: Improved policy π\nSet i = 0;\nCompute Qπi and V πi;\nwhile there exist s ∈S such that V πi(s) < maxa Qπ(s, a) do\nPick s such that V πi(s) < maxa Qπi(s, a);\nLet A∗\ns,πi = argmaxa Qπ(s, a);\nDeﬁne a new policy: πi+1 = πi;\nSet πi+1(A∗\ns,πi|s) = 1;\ni = i + 1;\nCompute Qπi and V πi;\nend\n10\nD\nTensor completion\nTo modify algorithm 1 for tensor completion, alternating minimization (algorithm 4) can be modiﬁed\nto solve\nargmin\nuj\nl ∈Rdj\n∥Ω· T −Ω·\nr\nX\nk=1\nwku1\nk ⊗u2\nk ⊗· · · ⊗un\nk∥F\nat each iteration, instead of the usual problem\nargmin\nuj\nl ∈Rdj\n∥T −\nr\nX\nk=1\nwku1\nk ⊗u2\nk ⊗· · · ⊗un\nk∥F .\nThis leads to the update\nuj\nl =\n(Ω· T −Ω· Pr\nk̸=l wku1\nk ⊗u2\nk ⊗· · · ⊗un\nk)(u1\nl , . . . , uj−1\nl\n, I, uj+1\nl\n, . . . , un\nl )\nΩ(u1\nl · u1\nl , . . . , uj−1\nl\n· uj−1\nl\n, I, uj+1\nl\n· uj+1\nl\n, . . . , un\nl · un\nl )\ninstead of the usual update displayed in algorithm 4. Similarly, the update for the weights is given by\nwl =\n(Ω· T −Ω· Pr\nk̸=l wku1\nk ⊗u2\nk ⊗· · · ⊗un\nk)(u1\nl , . . . , un\nl )\nΩ(u1\nl · u1\nl , . . . , un\nl · un\nl )\n.\nThis modiﬁcation is inspired by the tensor completion method in Jain and Oh [2014], which is proven\nto work for symmetric orthogonal tensors.\nE\nTensor generation details\nThe target reward and transition tensors in the experiments are generated by algorithm 6. The weights\nare chosen to be w = linspace(0.1, 1) for the reward tensor in the ﬁrst experiment.\nAlgorithm 6: Tensor generation\nInput: Desired shape (d1, . . . , dn) and rank r, weights w ∈Rr\nResult: Rank r tensor with dimensions d1, . . . , dn\nfor i = 1, . . . , n do\nfor l = 1, . . . , r do\nul\ni ∼N(0, Idi);\nul\ni =\nul\ni\n∥ul\ni∥;\nT = Pr\nl=1 wlul\n1 ⊗· · · ⊗ul\nn;\nA complication is that the transition tensor must satisfy P\ns′ Tsa1a2a3s′ = 1. It is difﬁcult to directly\ngenerate a tensor of ﬁxed rank with this property, and normalizing a tensor by setting\nT ′\ns1a1a2a3s2 =\nTs1a1a2a3s2\nP\ns′ Ts1a1a2a3s′\nchanges the rank of the tensor. This is overcome by iteratively normalizing, computing a new\ndecomposition of higher than the desired rank, and then truncating it to the desired rank. Repeating\nthis as shown in algorithm 7 appears to converge in practice, enabling the generation of a valid\ntransition tensor that is arbitrarily close to to a tensor of desired rank.\n11\nAlgorithm 7: Transition tensor generation\nInput: Desired shape (d1, . . . , dn) and rank r, tolerance ϵ\nResult: Approximately rank r transition tensor with dimensions d1, . . . , dn\nT = GenerateTensor((d1, . . . , dn), r, w);\nwhile ∥Normalize(T) −T∥F > ϵ do\nT = Normalize(T);\nT = TensorDecomp(T, 2r);\nT = Truncate(T, r);\nT = Normalize(T);\n0\n50\n100\n150\n200\nepisodes\n0.001\n0.002\n0.003\n0.004\n0.005\n0.006\ntransition MSE\n0\n50\n100\n150\n200\nepisodes\n0.0008\n0.0010\n0.0012\n0.0014\n0.0016\n0.0018\ntransition MSE\nrank_8_decomp\nrank_4_decomp\nno_decomp\ntesseract_rank_1\ntesseract_rank_4\nFigure 4: Mean squared error of the estimate of the transition tensor after training a number of\nepisodes in the state degeneracy MDP. The right plot is a zoomed in version of the left plot.\nF\nExtra experiment ﬁgures\n0\n50\n100\n150\n200\nepisodes\n0\n2000\n4000\n6000\n8000\n10000\nunique visited state-action pairs\nrank_5_decomp\nrank_3_decomp\nrank_10_decomp\nno_decomp\ntesseract_rank_5\ntesseract_rank_1\n(a) Number of unique states visited after training\nfor a number of episodes in the ﬁrst experiment.\nThe MDP has 20000 unique states in total\n0\n50\n100\n150\n200\nepisodes\n0\n2000\n4000\n6000\n8000\n10000\nunique visited state-action pairs\nrank_8_decomp\nrank_4_decomp\nno_decomp\ntesseract_rank_1\ntesseract_rank_4\n(b) Number of unique states visited after training\nfor a number of episodes in the state degeneracy\nexperiment. The total number of state-action pairs\nin the MDP is 128000\n12\n",
  "categories": [
    "cs.LG",
    "cs.MA"
  ],
  "published": "2021-10-27",
  "updated": "2021-10-27"
}