{
  "id": "http://arxiv.org/abs/2110.14597v1",
  "title": "Evaluating Deep Learning Models and Adversarial Attacks on Accelerometer-Based Gesture Authentication",
  "authors": [
    "Elliu Huang",
    "Fabio Di Troia",
    "Mark Stamp"
  ],
  "abstract": "Gesture-based authentication has emerged as a non-intrusive, effective means\nof authenticating users on mobile devices. Typically, such authentication\ntechniques have relied on classical machine learning techniques, but recently,\ndeep learning techniques have been applied this problem. Although prior\nresearch has shown that deep learning models are vulnerable to adversarial\nattacks, relatively little research has been done in the adversarial domain for\nbehavioral biometrics. In this research, we collect tri-axial accelerometer\ngesture data (TAGD) from 46 users and perform classification experiments with\nboth classical machine learning and deep learning models. Specifically, we\ntrain and test support vector machines (SVM) and convolutional neural networks\n(CNN). We then consider a realistic adversarial attack, where we assume the\nattacker has access to real users' TAGD data, but not the authentication model.\nWe use a deep convolutional generative adversarial network (DC-GAN) to create\nadversarial samples, and we show that our deep learning model is surprisingly\nrobust to such an attack scenario.",
  "text": "Evaluating Deep Learning Models and\nAdversarial Attacks on Accelerometer-Based\nGesture Authentication\nElliu Huangâˆ—Fabio Di Troiaâˆ—Mark Stampâˆ—â€ \nOctober 28, 2021\nAbstract\nGesture-based authentication has emerged as a non-intrusive, ef-\nfective means of authenticating users on mobile devices. Typically,\nsuch authentication techniques have relied on classical machine learn-\ning techniques, but recently, deep learning techniques have been ap-\nplied this problem.\nAlthough prior research has shown that deep\nlearning models are vulnerable to adversarial attacks, relatively lit-\ntle research has been done in the adversarial domain for behavioral\nbiometrics. In this research, we collect tri-axial accelerometer gesture\ndata (TAGD) from 46 users and perform classification experiments\nwith both classical machine learning and deep learning models. Specif-\nically, we train and test support vector machines (SVM) and convolu-\ntional neural networks (CNN). We then consider a realistic adversarial\nattack, where we assume the attacker has access to real usersâ€™ TAGD\ndata, but not the authentication model. We use a deep convolutional\ngenerative adversarial network (DC-GAN) to create adversarial sam-\nples, and we show that our deep learning model is surprisingly robust\nto such an attack scenario.\nâˆ—Department of Computer Science, San Jose State University\nâ€ mark.stamp@sjsu.edu\n1\narXiv:2110.14597v1  [cs.CR]  3 Oct 2021\n1\nIntroduction\nWith the ubiquity of technology, authentication has become an essen-\ntial part of everyday life. Passwords and PINs are the most common\nforms of authentication, but biometrics are also popular.\nBiomet-\nric authentication includes physiological (e.g., facial recognition and\nfingerprint) and behavioral (e.g., gait and keystroke dynamics) ap-\nproaches [5].\nWhile physiological biometric authentication has proven to be highly\neffective, sensors and equipment required for such approaches are usu-\nally costly. Additionally, attackers can sometimes bypass such a sys-\ntem if they have access to a copy of the required features [31]. On the\nother hand, behavioral biometrics not only have the potential to be\ncost effective, they may also be more secure, at least in cases where\nattackers have difficulty imitating the relevant features. Furthermore,\nthe non-intrusive nature of behavioral biometrics may be considered\ndesirable, in comparison to physiological biometric authentication.\nGesture-based authentication is a relatively recent behavioral bio-\nmetric that has achieved promising results. There are various tech-\nniques for analyzing gestures, including acceleration, angular motion,\n3D motion, and a mix of the three. Several machine learning tech-\nniques, including those we discuss in Section 2, have been applied to\nthe gesture-based authentication problem.\nIn this research, we explore the effectiveness of deep learning tech-\nniques on gesture-based authentication. Our research is based on a\nnew dataset that we have collected. Given that our tri-axial accelerom-\neter gesture data (TAGD) are time series, we consider two time se-\nries classification (TSC) techniques: support vector machines (SVM)\nand one-dimensional convolutional neural networks (1D-CNN). When\ncombined with feature extraction techniques, our SVM model pro-\nvides for rudimentary analysis of our TAGD data, as well as a basis\nfor comparison to our 1D-CNN model. We also generate adversarial\nsamples using generative adversarial networks (GAN) and use these\nsamples to explore the robustness of our 1D-CNN model against a\nrealistic adversarial attack.\nThe remainder of this paper is structured as follows. Section 2 dis-\ncusses relevant work in the field of gesture-based authentication and\nadversarial attacks on biometric authentication. Section 3 provides\nan overview of the dataset, including specific steps of the data col-\nlection process and data preprocessing techniques. Machine learning\n2\ntechniques, including generating adversarial samples, and adversarial\nstrategies are introduced in Section 4.\nWe present our experimen-\ntal results for our classification models and an adversarial attack in\nSection 5. Our conclusion and a brief discussion of future research\ndirections are provided in Section 6.\n2\nRelated Work\nRelative to the vast research literature on behavioral biometrics, there\nis comparatively little work on gesture-based authentication. In this\nsection, we provide an overview of research on gesture-based authen-\ntication and adversarial attacks on such security systems.\nMost gesture-based authentication techniques can be categorized\ninto two main methods, namely, touchscreen and motion gestures [11].\nThere are, however, studies that combine both into a single authenti-\ncation system [8].\nTouchscreen-based gesture authentication methods typically ana-\nlyze touch dynamics, including various inputs recorded from a touch-\nscreen interface such as finger size and pressure. One approach con-\nsisted of collecting finger behavior and position data and authenti-\ncated users via SVMs [3]. Another study employed particle swarm\noptimization to find patterns in touchscreen dynamics [24].\nMotion gestures generally rely on accelerometer and gyroscope\ndata to analyze the acceleration and angular motion of the mobile\ndevice. Prior research in this domain has applied dynamic time warp-\ning (DTW) [22], SVMs [23], and hidden Markov models (HMM) [15]\nto authenticate users. One gesture-based approach employed a more\nsophisticated method that involved the â€œleap motionâ€ controller that\ncollects 3D motion data and applied similarity thresholds to authen-\nticate users [19].\nAnother approach analyzed full-body and hand-\ngestures in 3D space using two-stream CNNs [32].\nAdversarial attacks on gesture-based authentication is not a well-\nresearched area, but there are a handful of relevant studies in the\ngeneral field of behavioral biometrics. One study analyzed behavioral\nmouse dynamics and found that deep learning authentication mod-\nels were susceptible to adversarial attacks [28]. Adversarial samples\nhave been generated using a fast gradient sign method (FGSM) to\ncreate perturbations in the data, with a gated recurrent unit (GRU)\n3\nthen used to generate adversarial samples.\nAnother study [1] ana-\nlyzed the resilience of continuous touch-based authentication systems\n(TCAS) to adversarial attacks.\nThese researchers found that their\nTCAS trained with the help of generative adversarial networks (GAN)\nhad a lower false acceptance rate than that of vanilla TCAS. The pa-\nper [2] reports on experiments with randomization attacks on gesture-\nbased security systems that use SVMs, and finds that their models\nare highly vulnerable to adversarial attacks.\nSimilarly, adversarial learning on time series classification has not\nseen much research. Most adversarial attacks involve small perturba-\ntions of the original data using state-of-the-art FGSM or basic iterative\nmethod (BIM) in order to â€œtrickâ€ a classification or regression model.\nA study of adversarial attacks on multivariate time series regression\nfound that three of the most popular deep learning modelsâ€”CNNs,\nGRUs, and long short-term memory (LSTM)â€”were highly suscepti-\nble to such attacks [25]. Another study also used FGSM and BIM\nto create small perturbations in time series data, which significantly\nlowered the classification accuracy of deep learning models for vehicle\nsensors and electricity consumption data [13].\n3\nDataset\nIn this section, we give an overview of the data collected and specify\nthe steps in the data collection process. We also include a discussion\nof data preprocessing and the feature engineering techniques that we\nhave employed.\n3.1\nData Collection\nIn this research, we collect usersâ€™ tri-axial accelerometer gesture data\n(TAGD) while the user holds a smartphone and writes their â€œsigna-\ntureâ€ in the air, similar to the process in [18].\nThe accelerometer\nsensor of the Physics Toolbox Sensor Suite [10] was used to collect\ndata.\nA screenshot of the application is shown in Figure 1.\nThe\ntri-axial acceleration is represented as separate curvesâ€”the red curve\nrepresents acceleration along the x-axis, the green curve represents\nacceleration along the y-axis, the blue curve represents acceleration\nalong the z-axis, and the white curve represents the total magnitude\nof acceleration. Data is collected at a frequency of 100 Hz, i.e., 100\n4\ndata points per second. We performed data collection solely on the\nApple iOS platforms in order to reduce the possibility of smartphone\ntype being a confounding variable. We note that iPhone models varied\nfrom iPhone 8 to iPhone X.\n(a) Accelerometer sensor\n(b) App settings\nFigure 1: Screenshots of Physics Toolbox Sensor Suite app\nAfter installing the app, he user performs the following steps to\ncollect signature data.\n1. Tap the red button to start recording accelerometer data\n2. Move the smartphone in the air to draw a signature\n3. Tap the red button again to stop recording data\n4. Upload data in the form of a CSV file into Google Drive\nThese steps were repeated 50 times to collect 50 signatures for each\nuser.\n5\nIn total, we collected 50 signatures from each of 46 different users.\nUsers typically chose their initials as their signature, but they were\nfree to create their own unique signature. Generally, the time to write\neach signature varied between 3 and 7 seconds, and the entire data\ncollection process for one individual required about 20 minutes. Our\ndataset is freely available for use by other researchers at [17]. A sample\nof our TAGD data is shown in Figure 2.\nFigure 2: Sample tri-axial acceleration time sequence\n3.2\nData Preprocesssing\nThe raw tri-axial data is of variable length. As discussed below, we\nresize all signatures to the same size, as most traditional classification\ntechniques require input data of a fixed size. We have applied several\nfeature engineering techniques to the resulting time series.\n3.2.1\nFeature Engineering\nAs noted above, each signature is a temporal sequence of tri-axial\naccelerometer data.\nFirst, we extract statistical features based on\nthe the acceleration for each axis, ignoring the sequential nature of\nthe data. The resulting distributions varyâ€”we compute the following\nstatistical measures of shape, center, and spread for each of the three\naxes.\n6\nLength (ğ¿)â€” The number of data points in the signature.\nMean (ğœ‡)â€” The center of the distribution is the mean\nğœ‡= 1\nğ‘›\nğ‘›\nâˆ‘ï¸\nğ‘–=1\nğ‘¥ğ‘–= 1\nğ‘›(ğ‘¥1 + ğ‘¥2 + Â· Â· Â· + ğ‘¥ğ‘–)\nMedian (ğ‘š)â€” The median is another measure of the center of the\ndistribution.\nStandard deviation (ğœ)â€” The standard deviation\nğœ=\nâ¯\nâ¸\nâ¸\nâ¸\nâ¸\nâ·\nğ‘›\nâˆ‘ï¸\nğ‘–=1\n(ğ‘¥ğ‘–âˆ’ğœ‡)\nğ‘›âˆ’1\nmeasures the variability in the signature data.\nKurtosis (ğ‘˜)â€” The kurtosis is computed as\nğ‘˜=\nğ‘›\nâˆ‘ï¸\nğ‘–=1\n(ğ‘¥ğ‘–âˆ’ğœ‡)4\nğ‘›ğœ4\nand it measures the weight of the tails relative to the center of\nthe distribution and provides additional information related to\nthe signature motion.\nSkewness (ğ‘ )â€” The symmetry of the distribution, which is computed\nas\nğ‘ =\nğ‘›\nâˆ‘ï¸\nğ‘–=1\n(ğ‘¥ğ‘–âˆ’ğœ‡)3\nğ‘›ğœ3\ncan help us understand the â€œsmoothnessâ€ of motion in a signa-\nture.\nAll of these features provide some information about underlying pat-\nterns in usersâ€™ signatures.\nFor each signature, we calculate a feature vector of the form\n(ğ¿, ğœ‡ğ‘¥, ğœ‡ğ‘¦, ğœ‡ğ‘§, ğ‘šğ‘¥, ğ‘šğ‘¦, ğ‘šğ‘§, ğœğ‘¥, ğœğ‘¦, ğœğ‘§, ğ‘˜ğ‘¥, ğ‘˜ğ‘¦, ğ‘˜ğ‘§, ğ‘ ğ‘¥, ğ‘ ğ‘¦, ğ‘ ğ‘§)\nconsisting of the measures of shape, center, and spread of the distribu-\ntion, as discussed above. This feature vector of 16 elements is utilized\nonly in our SVM experiments, below.\n7\n3.2.2\nTime Series Resampling\nSince 1D-CNNs and GANs require feature vectors of fixed length, we\nuse tslearn [30], we resize all the TAGDs to length 400. The resizing\nfunction in tslearn interpolates for arrays less than the target size.\nWe chose to resample all the time series to length 400 since the median\nlength of the sequences is 380, while the mean length is 400.\n4\nImplementation\nIn this section, we discuss the machine learning techniques that form\nthe basis of our experiments. These techniques, namely, support vec-\ntor machines, convolutional neural networks, and generative adversar-\nial networks, will serve as the basis for our classification experiments\nin Section 5. We also introduce our strategies for adversarial attacks.\n4.1\nSupport Vector Machines\nSupport vector machines (SVMs) are one of the most popular super-\nvised learning techniques for classification and regression. SVMs at-\ntempt to find the optimal separating hyperplane between two labeled\nsets of training data [27]. However, a dataset need not be linearly\nseparable, in which case we can employ the â€œkernel trick.â€ As de-\npicted in Figure 3, the kernel trick maps the input data into a higher-\ndimensional space where it is more likely to be linearly separable. The\nkernel trick, together with â€œsoft marginâ€ calculations that allow for\nclassification errors, makes an SVM an extremely powerful and flexible\ntool in the field of machine learning.\nA classification problem with a small training sample size and high\ndimensionality is prone to overfitting [9]. Feature selection techniques\ncan help to prevent this problem by discarding features, with minimal\nlossâ€”or even improvementsâ€”in performance.\nIn our SVM experi-\nments, we used support vector machine recursive feature elimination\n(SVM-RFE) for feature selection. SVM-RFE consists of eliminating\nthe least significant feature (based on linear SVM weights), then train-\ning a model on the reduced feature set. This process is repeated until\nthe desired number of features is reached, or the performance degrades\nbeyond acceptable limits.\n8\nğœ‘\n=â‡’\nFigure 3: The kernel trick\n4.2\n1D Convolutional Neural Networks\nTypically, convolutional neural networks (CNN) are associated with\nfeature extraction and classification for images, which generally in-\nvolves two-dimensional convolutional neural networks (2D-CNN). In\nsuch a model, 2D data (e.g., images) are fed into a CNN and classified\nvia a final fully-connected layer.\nIn this paper, we do not consider images; instead, we have temporal\nsequences of fixed length. Such data is suitable for one-dimensional\nconvolutional neural networks (1D-CNN). While not as common as\n2D-CNNs, 1D-CNNs are used for signal processing and sequence clas-\nsification, with numerous applications in biomedical and civil engi-\nneering [21].\nThe architecture of a 1D-CNN is analogous to that of a 2D-CNN,\nwith the key differences being the dimensionality of the input data\nand the convolution operation. 2D-CNNs typically use a rectangular\nkernel that slides from left to right, top to bottom. In contrast, 1D-\nCNNs employ a kernel that spans some number of variables and slides\nalong a vector.\n4.3\nAdversarial Strategy\nWe also consider how our deep learning models perform under ad-\nversarial attacks.\nWhile several studies analyze adversarial attacks\ninvolving small perturbations of the original data [20], we explore a\n9\nscenario where we assume the intruder has access to a real usersâ€™ ges-\nture data. We test both poisoning and evasion attacks using learning\nmodels to generative adversarial samples; specifically, we use a type of\ngenerative adversarial network (GAN) to produce adversarial samples.\n4.3.1\nDeep Convolutional Generative Adversarial Net-\nworks\nTwo competing neural networks are trained in a GANâ€”a generative\nnetwork and a discriminative networkâ€”with the generative network\ncreating fake data that is designed to defeat the discriminative net-\nwork. The two networks are trained simultaneously following a game-\ntheoretic approach.\nIn this way, both networks improve, with the\nultimate objective being a model (discriminative, generative, or both)\nthat is stronger than it would have been if it was trained only on the\nreal training data; see [12] for additional details.\nAn overview of GAN structure is depicted in Figure 4. Among\nmany other uses, GANs have been used to generate realistic adversar-\nial samples.\nFigure 4: Overview of GAN structure\nWe use deep convolutional GANs (DC-GAN) to replicate our time\nseries data in a form that will serve as adversarial samples [4]. The\ngenerator and discriminator models are both based on 1D-CNN mod-\nels. The generator essentially performs the functions of a convolutional\nlayer in reverseâ€”the input is an arbitrary sequence of values and it\nuses transposed convolution layers to shape the data into a desired\nform. The discriminator can be based on a traditional convolutional\nneural network. The fully-connected layer outputs a value between\n10\nâˆ’1 and + 1, with a negative output indicating a fake sample and\npositive output indicating a real sample. The generator and discrim-\ninator are connected by a loss function, which provides feedback to\nboth models. Over several epochs of training, the generator should\nbecomes better at generating adversarial samples while the discrim-\ninator should become better at distinguishing between real and fake\nsamples.\nIn our GAN model, the generator has an input consisting of a\nsequence of 100 values sampled from a normal distribution. After the\ndata passes through three transposed convolution layers, the sequence\nof 100 values are transformed into a sequence of the same size as the\nTAGD, that is, 400 Ã— 3. The discriminator model closely follows the\narchitecture of the 1D-CNN classification model outlined above, with\nthe major difference being the binary output of the fully-connected\nlayer.\nThe generator and discriminator are connected by a binary\ncross entropy loss function. In our experiments, we vary the number\nof training epochs to see how effective the adversarial samples are in\nbreaking down our model.\nIn Figure 5, we have examples of TAGD generated using DC-GANs\nwith different training epochs. The number of training epochs is di-\nrectly related to how well the the DC-GAN model can replicate data.\nAs we can see in Figure 5 (a) and (b), the data is quite random doesnâ€™t\nresemble the real TAGD in 2, whereas Figures 5 (c) and (d) appear\ncloser to the real TAGD. As we train the DC-GAN more epochs, the\nadversarial samples resemble the real data more.\n4.3.2\nAdversarial Attack\nSimilar to [26], we â€œpoisonâ€ our training dataset with adversarial sam-\nples generated from DC-GANs, meaning that we mix in adversarial\nsamples with our real training dataset. Then, we train our 1D-CNN\non the poisoned dataset and try to classify real data. The accuracy of\nclassifying with the poisoned training dataset suggests how well our\n1D-CNN can survive a poisoning attack while simultaneously indicat-\ning how well our DC-GAN can generate adversarial samples.\n5\nExperiments and Results\nIn this section, we present and analyze the results of the experiments\noutlined in the previous section. For our first experiment, we provide\n11\n(a) Acceleration sequence after 10 epochs\n(b) Acceleration sequence after 25 epochs\n(c) Acceleration sequence after 50 epochs\n(d) Acceleration sequence after 100 epochs\nFigure 5: DC-GAN generated acceleration sequences\nthe results of a multiclass classification problem using SVMs, based\non the statistical features discussed in Section 3.2.1. Then we apply\na deep learning technique, 1D-CNNs, in the multiclass classification\nproblem.\nBoth of these techniques, SVMs and 1D-CNNs, produce\nstrong results. Then we move on to adversarial learning where we\nuse GANs to generate adversarial samples. With the GAN-generated\nadversarial samples, we show that our deep learning model is robust\nunder a poisoning type of adversarial attack.\nWe use several metrics in our multiclass classification problem.\nThe most basic metric we consider is the accuracy, which is calculated\nfrom a 46 by 46 confusion matrix. Of course, higher accuracy indi-\ncates a more successful model. We also measure the false acceptance\nrate (FAR), which is the rate at which a different user is classified as\nthe actual user, and the false reject rate (FRR), which is the rate at\nwhich real users are mis-classified as other users. Since we are deal-\ning with multiclass classification, we calculate FAR and FRR for each\nindividual user and report the average for the 46 different users [14].\n12\nIntuitively, lower FAR and FRR indicates greater success in the clas-\nsification model.\nIn our SVM experiments, we use SVM and RFE libraries from\nscikit-learn.\nIn our 1D-CNN and GAN experiments, we imple-\nmented Keras libraries [6] to develop our models. For all the experi-\nments, we use an 80-20 train-test split, i.e. 80% of the data is used to\ntrain the model, while the remaining 20% is used for testing.\n5.1\nSVM Results\nWe first considered rudimentary experiments with various kernels and\nvalues of parameters and found that a linear kernel with regularization\nparameter ğ¶= 1000 worked the best for multiclass classification. As\na result, for all experiments in this section, we use an SVM with linear\nkernel and ğ¶= 1000.\nHere, we train our SVM model on the feature vector described in\nSection 3.2.1 and use SVM-RFE to select the strongest features. We\nanalyze the relationship between the number of features and the FAR,\nFPR, and accuracy.\nUsing all 16 features discussed in Section 3.2.1, the SVM achieved\na 95% classification accuracy and 0.0014 and 0.057 FAR and FRR,\nrespectively. These results (and more) are summarized in Figure 6.\nAs we eliminate features based on the rankings determined by\nSVM-RFE, the classification accuracy in Figure 6 (c) generally de-\ncreases, dropping to about 89% with 9 of the 16 features.\nThis is\nstill quite strong, considering the number of classes. As we can see\nfrom Figure 6, the accuracy generally decreases slightly as we elimi-\nnate more features, although the accuracy does not drop below 90%\nuntil we have eliminated 7 features. Similarly, the FAR is exception-\nally low even as features are eliminated, staying below 1% for every\ncombination of features. However, the FRR starts at around 5% and\nincreases to more than 20% once we have eliminated 10 features.\n5.2\n1D-CNN Results\nWe also experiment with a 1D-CNN as our classification model, where\nthe model follows the architecture in [7].\nThe 1D-CNN model is\ntrained on temporal sequences of fixed length, as described in Section 4.\nThe model performs one-dimensional convolutions along the time axis\nwith RELU activation functions after each convolution layer.\nThe\n13\nğ¿, ğœ‡, ğ‘š, ğœ, ğ‘˜, ğ‘ \nğœ‡, ğ‘š, ğœ, ğ‘˜, ğ‘ \nğœ‡, ğ‘š, ğœ, ğ‘ \nğœ‡, ğ‘š, ğœ(ğ‘¦, ğ‘§), ğ‘ (ğ‘¦)\nğœ‡(ğ‘¥, ğ‘§), ğ‘š, ğ‘ (ğ‘¦)\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n0.95\n0.94\n0.92\n0.89\n0.76\nFeatures\nAccuracy\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.0014\n0.0013\n0.0018\n0.0024\n0.0053\nFAR and FRR\n0.057\n0.059\n0.083\n0.110\n0.220\nAccuracy\nFAR\nFRR\nFigure 6: Linear SVM results for selected combinations of features\nfirst convolution layer produces 128 filters, while the second convolu-\ntion layer produces 256 filters. A dropout layer follows the convolution\nlayers to prevent overfitting [16]. Then we have a 1D max-pooling layer\nto downsample the data and highlight the key features. The output of\nthe max-pooling layer is passed through a flatten layer, which is then\npassed through two fully-connected layers. The last fully-connected\nlayer produces the value that corresponds to the classification. This\nmodel is illustrated in Figure 7.\nAfter fine-tuning hyper-parameters (dropout rate and number of\nfilters), we moved on to experiment with kernel size and stride length.\nWe found the accuracy hovered around 89% to 91%. According to [29],\nthe performance of our 1D-CNN should be more receptive to changes\nin hyper-parameters involving the convolution layers.\nWe tested different combinations of kernels and stride lengths in\nthe convolution layers. These results are summarized in Table 1. We\n14\nFigure 7: 1D-CNN architecture\nsee that most of the results are fairly strong, ranging from a low accu-\nracy of 90% to a high of 94%. Generally, as the kernel size increases,\nthe accuracy increases. Similarly, as the stride length increases, the\naccuracy tends to increase. This is most likely due to the fact that\nlarger kernels and stride lengths produce more refined features for the\nfully-connected layers.\nTable 1: Stride length versus kernel size\nstride\nkernel\n3\n5\n10\n25\n1\n0.90457\n0.89391\n0.91276\n0.90870\n3\n0.92696\n0.93348\n0.93630\n0.93587\n6\n0.93087\n0.94000\n0.94109\n0.94022\n5.3\nAdversarial Results\nWe trained a GAN to generate adversarial samples, and using these\nsamples to determine whether our deep learning model is robust under\nadversarial attack. First, we determine how close our GAN-generated\ndata is to real data through a simulated poisoning attack. Then we test\nhow well the adversarial samples can evade an authenticator model.\nAs outlined in Section 4.3.2, we poison our training dataset with\nan increasing percentage of fake data.\nAll real samples are always\nincluded, so any changes in classification accuracy should be caused\nby our adversarial (fake) samples. We trained our DC-GAN for differ-\nent numbers of epochs and different numbers of adversarial samples.\nGenerally, a higher number of epochs results in better imitations of\n15\nthe original data. Increasing the number of adversarial samples in the\ntraining dataset gauges how well our 1D-CNN can resist large-scale\npoisoning.\nThe results of our poisoning attacks are given in Table 2. The accu-\nracy remains relatively high at more than 90%, even for high training\nepochs and up to a 1:1 ratio of real to fake data. Classification accu-\nracy does decrease slightly when there are more adversarial samples\nin the training dataset, but the loss in accuracy is not large. These\nresults show that our 1D-CNN is highly resistant to poisoning attacks.\nTable 2: Adversarial attack results (1840 real samples)\nadversarial\nsamples\nepochs\n10\n25\n50\n100\n100\n0.93609\n0.93130\n0.94761\n0.94565\n250\n0.94587\n0.94848\n0.93783\n0.94196\n600\n0.94087\n0.93978\n0.94002\n0.94152\n1840\n0.94000\n0.94152\n0.92239\n0.93435\nWe conjecture that the reason for the limited success of our ad-\nversarial attack is that it is exceedingly difficult to generate realistic\nsignatures of the type considered in this paper. While the real signa-\ntures in our dataset vary wildly, those that we generated using DC-\nGAN appear to exhibit more homogeneity. We plan to investigate this\nissue further in future work.\n6\nConclusion and Future Work\nPrevious research has shown that SVMs are a viable techniques for\naccelerometer-based gesture authentication [18]. In this paper, we ex-\npanded on and improved upon previous work. First, we refined the\nfeature selection process with SVM-RFE to select the best features,\nwhile maintaining a high classification accuracy. Then, we used deep\nlearning models, specifically 1D-CNNs, for classification. We obtained\nstrong results, with greater than 90% classification accuracy, slightly\nsurpassing the accuracy of our SVM model. Lastly, we experimented\nwith adversarial attacks on our 1D-CNN model, namely, poisoning and\n16\nevasion attacks. These simulate realistic attacks, assuming an intruder\nhas access to the real data, but not the model itself. Our results indi-\ncate that our 1D-CNN is robust under such attacks, achieving greater\nthan 90% accuracy for poisoning attacks and near perfect accuracy\nfor evasion attacks.\nFor future work, additional machine learning techniques could be\nconsidered. For example, long-short term memory (LSTM) models\ncould be used instead of 1D-CNNs, since LSTMs generally perform\nwell on sequential data. Additionally, as mentioned in Section 5.3,\nwe would like to explore alternative methods of generating adversarial\nsamples to determine whether we can improve on the limited adver-\nsarial attack results obtained with DC-GANs.\nReferences\n[1] Mohit Agrawal, Pragyan Mehrotra, Rajesh Kumar, and Ra-\njiv Ratn Shah.\nDefending touch-based continuous authentica-\ntion systems from active adversaries using generative adversarial\nnetworks. arXiv preprint arXiv:2106.07867, 2021.\n[2] Mohammad Al-Rubaie and J Morris Chang. Reconstruction at-\ntacks against mobile-based continuous authentication systems in\nthe cloud. IEEE Transactions on Information Forensics and Se-\ncurity, 11(12):2648â€“2663, 2016.\n[3] Ala Abdulhakim Alariki and Azizah Abdul Manaf. Touch ges-\nture authentication framework for touch screen mobile devices.\nJournal of Theoretical & Applied Information Technology, 62(2),\n2014.\n[4] Sukarna Barua, Sarah Monazam Erfani, and James Bailey. Fcc-\ngan: A fully connected and convolutional net architecture for\ngans. arXiv preprint arXiv:1905.02417, 2019.\n[5] Debnath Bhattacharyya, Rahul Ranjan, Farkhod Alisherov,\nMinkyu Choi, et al.\nBiometric authentication: A review. In-\nternational Journal of u-and e-Service, Science and Technology,\n2(3):13â€“28, 2009.\n[6] Jason Brownlee. Deep learning with Python: develop deep learn-\ning models on Theano and TensorFlow using Keras.\nMachine\nLearning Mastery, 2016.\n17\n[7] Jason Brownlee. 1d convolutional neural network models for hu-\nman activity recognition, Aug 2020.\n[8] Attaullah Buriro, Bruno Crispo, Filippo Delfrari, and Konrad\nWrona. Hold and sign: A novel behavioral biometrics for smart-\nphone user authentication. In 2016 IEEE security and privacy\nworkshops (SPW), pages 276â€“285. IEEE, 2016.\n[9] Xue-wen Chen and Jong Cheol Jeong. Enhanced recursive fea-\nture elimination. In Sixth International Conference on Machine\nLearning and Applications (ICMLA 2007), pages 429â€“435. IEEE,\n2007.\n[10] Chrystian Vieyra. Physics toolbox sensor suite, 2016.\n[11] Gradeigh D Clark and Janne Lindqvist.\nEngineering gesture-\nbased authentication systems.\nIEEE Pervasive Computing,\n14(1):18â€“25, 2015.\n[12] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulku-\nmaran, Biswa Sengupta, and Anil A Bharath. Generative adver-\nsarial networks: An overview. IEEE Signal Processing Magazine,\n35(1):53â€“65, 2018.\n[13] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhas-\nsane Idoumghar, and Pierre-Alain Muller. Adversarial attacks on\ndeep neural networks for time series classification. In 2019 Inter-\nnational Joint Conference on Neural Networks (IJCNN), pages\n1â€“8. IEEE, 2019.\n[14] Margherita Grandini, Enrico Bagli, and Giorgio Visani.\nMet-\nrics for multi-class classification: an overview.\narXiv preprint\narXiv:2008.05756, 2020.\n[15] Dennis Guse. Gesture-based user authentication on mobile de-\nvices using accelerometer and gyroscope. Masterâ€™s thesis, Tech-\nnische UniversitÂ¨at Berlin, 2017.\n[16] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\nSutskever, and Ruslan R Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580, 2012.\n[17] Elliu Huang. Gesture dataset, 2021. Available from the authors\nupon request.\n[18] Elliu Huang, Fabio Di Troia, Mark Stamp, and Preethi Sundar-\navaradhan. A new dataset for smartphone gesture-based authen-\ntication. In ICISSP, pages 771â€“780, 2021.\n18\n[19] Satoru Imura and Hiroshi Hosobe. A hand gesture-based method\nfor biometric authentication.\nIn International Conference on\nHuman-Computer Interaction, pages 554â€“566. Springer, 2018.\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhas-\nsane Idoumghar, and Pierre-Alain Muller. Adversarial attacks on\ndeep neural networks for time series classification. arXiv e-prints,\npages arXivâ€“1903, 2019.\n[21] Serkan Kiranyaz, Onur Avci, Osama Abdeljaber, Turker Ince,\nMoncef Gabbouj, and Daniel J Inman. 1d convolutional neural\nnetworks and applications: A survey. Mechanical systems and\nsignal processing, 151:107398, 2021.\n[22] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Va-\nsudevan. uwave: Accelerometer-based personalized gesture recog-\nnition and its applications.\nPervasive and Mobile Computing,\n5(6):657â€“675, 2009.\n[23] Duo Lu, Kai Xu, and Dijiang Huang.\nA data driven in-air-\nhandwriting biometric authentication system. In 2017 IEEE In-\nternational Joint Conference on Biometrics (IJCB), pages 531â€“\n537. IEEE, 2017.\n[24] Yuxin Meng, Duncan S Wong, Roman Schlegel, et al.\nTouch\ngestures based biometric authentication scheme for touchscreen\nmobile phones. In International conference on information secu-\nrity and cryptology, pages 331â€“350. Springer, 2012.\n[25] Gautam Raj Mode and Khaza Anuarul Hoque. Adversarial ex-\namples in deep learning for multivariate time series regression.\nIn 2020 IEEE Applied Imagery Pattern Recognition Workshop\n(AIPR), pages 1â€“10. IEEE, 2020.\n[26] Luis MuËœnoz-GonzÂ´alez, Bjarne Pfitzner, Matteo Russo, Javier\nCarnerero-Cano, and Emil C Lupu. Poisoning attacks with gen-\nerative adversarial nets. arXiv preprint arXiv:1906.07773, 2019.\n[27] Mark Stamp. Introduction to machine learning with applications\nin information security. CRC Press, Taylor & Francis Group,\nBoca Raton, FL, 2018.\n[28] Yi Xiang Marcus Tan, Alfonso Iacovazzi, Ivan Homoliak, Yuval\nElovici, and Alexander Binder. Adversarial attacks on remote\nuser authentication using behavioural mouse dynamics. In 2019\nInternational Joint Conference on Neural Networks (IJCNN),\npages 1â€“10. IEEE, 2019.\n19\n[29] Wensi Tang, Guodong Long, Lu Liu, Tianyi Zhou, Jing Jiang,\nand Michael Blumenstein. Rethinking 1d-cnn for time series clas-\nsification: A stronger baseline. arXiv preprint arXiv:2002.10061,\n2020.\n[30] Romain Tavenard. tslearn documentation, 2021.\n[31] Cong Wu, Kun He, Jing Chen, Ziming Zhao, and Ruiying Du.\nLiveness is not enough:\nEnhancing fingerprint authentication\nwith behavioral biometrics to defeat puppet attacks.\nIn 29th\n{USENIX} Security Symposium ({USENIX} Security 20), pages\n2219â€“2236, 2020.\n[32] Jonathan Wu, Prakash Ishwar, and Janusz Konrad. Two-stream\ncnns for gesture-based verification and identification: Learning\nuser style. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops, pages 42â€“50, 2016.\n20\n",
  "categories": [
    "cs.CR",
    "cs.LG"
  ],
  "published": "2021-10-03",
  "updated": "2021-10-03"
}