{
  "id": "http://arxiv.org/abs/2110.14800v1",
  "title": "Convolutional Deep Exponential Families",
  "authors": [
    "Chengkuan Hong",
    "Christian R. Shelton"
  ],
  "abstract": "We describe convolutional deep exponential families (CDEFs) in this paper.\nCDEFs are built based on deep exponential families, deep probabilistic models\nthat capture the hierarchical dependence between latent variables. CDEFs\ngreatly reduce the number of free parameters by tying the weights of DEFs. Our\nexperiments show that CDEFs are able to uncover time correlations with a small\namount of data.",
  "text": "arXiv:2110.14800v1  [stat.ML]  27 Oct 2021\nConvolutional Deep Exponential Families\nChengkuan Hong, Christian R. Shelton\nDepartment of Computer Science and Engineering\nUniversity of California, Riverside\nchong009@ucr.edu, cshelton@cs.ucr.edu\nAbstract\nWe describe convolutional deep exponential families (CDEFs) in this paper.\nCDEFs are built based on deep exponential families, deep probabilistic models\nthat capture the hierarchical dependence between latent variables. CDEFs greatly\nreduce the number of free parameters by tying the weights of DEFs. Our experi-\nments show that CDEFs are able to uncover time correlations with a small amount\nof data.\n1\nIntroduction\nDeep neural networks (DNNs) [12] have achieved great success [9] in many areas [3], including\ncomputer vision, natural language processing and time series analysis. Nevertheless, neural net-\nworks have limitations. As a supervised learning method, the testing data should be in the same\nform as the training data, while it is often not the case in practice, and does not allow for missing\ndata. Additionally, a deep neural network needs a large amount of training data.\nDeep exponential families (DEFs) [11], an unsupervised probabilistic graphical models [7], are a\ngood supplement to deep learning. DEFs have the deep structure to learn the hierarchical information\nof the data. DEFs can also represent the data utilizing the power of exponential families. Unlike\nDNNs, they are able to predict any variable set based on any other, and are not limited to the input-\noutput pattern in the training data. So a DEF is able to predict the missing information with any\nsmall part of the testing data.\nIn this paper, we develop convolutional deep exponential families (CDEFs), a modiﬁed model based\non DEFs. A DEF, like a neural network, is composed of fully connected layers of latent variables.\nBoth the variables in each layer and the variables in the connections are from exponential families.\nIn CDEFs, we tie the weights, like in a convolutional neural network [8], which dramatically reduces\nthe number of parameters. Thereby, CDEFs are able to capture time correlations with less data.\nExponential families\nAn exponential family [2] is a set of probability distributions that satisfy a\nspeciﬁc form:\np(x) = h(x) exp(ηT T (x) −a(η)),\nwhere h is the base measure, η are the natural parameters, T are the sufﬁcient statistics, and a is the\nlog-normalizer.\nDeep exponential families\nIn order to construct deep exponential families, the latent layers of\nvariables are stacked hierarchically. The parameters of each variable are controlled by the variables\nof the layer above and the connection weights.\nFor a deep exponential family model, we have L hidden layers {z1, · · · , zL} for each data point x.\nEach of the layers contains Kℓlatent variables zℓ= {zℓ,1, · · · , zℓ,Kℓ}, where zℓ,k is assumed to be\na scalar. This model contains L −1 layers of weights {W1, · · · , WL−1}, which are shared across\nPreprint.\ntraining and testing data. Each Wℓis a Kℓ× Kℓ+1 matrix. We assume there are prior distributions\np(Wℓ) for the weights.\nzℓ+1,k\nwℓ+1,k\n...\nzL,k\nη\nzℓ,k\nwℓ,k\n...\nz1,k\nw1,k\nxi\nw0,i\nKL\nKℓ+1\nKℓ\nK1\nV\nFigure 1: The structure of deep expo-\nnential families with V observations\nfor data point x. xi represents the ith\nobservation. Reproduced based on Fig-\nure 2 of [11].\nAs shown in Figure 1, the top layer of latent variables are\ndrawn from an exponential family:\np(zL,k) = EXPFAML(zL,k, η),\nwhere η is a given parameter, and EXPFAM(x, η) denotes\nx is drawn from an exponential family with natural param-\neter η.\nNext, each latent variable is drawn conditional on the pre-\nvious layer:\np(zℓ,k|zℓ+1, wℓ,k) = EXPFAMℓ(zℓ,k, gℓ(zT\nℓ+1wℓ,k)),\nwhere gℓ, called the link function, maps the inner product\nto the natural parameter, zℓ,k is a scalar, zℓ+1 is a Kℓ+1\nvector and wℓ,k is a row vector from the Kℓ×Kℓ+1 matrix\nWℓ.\nConvolutional Deep Exponential Families\nLike the\nconvolutional neural network, the weights of the convolu-\ntional deep exponential families are also tied such that the\nweight matrix only has small number of free parameters.\nFor example, for a 5 × 3 matrix W, in the original deep\nexponential families model, Woriginal has 15 free param-\neters. But, for a convolutional deep exponential families\nmodel, Wconv only has 3 free parameters when the ﬁlter\nsize is 3 and the stride is 1:\nWoriginal =\n\n\nw11\nw12\nw13\n...\n...\n...\nw51\nw52\nw53\n\n, Wconv =\n\n\nw11\n0\n0\nw21\nw11\n0\nw31\nw21\nw11\n0\nw31\nw21\n0\n0\nw31\n\n\n(1)\nFigure 2 shows the connections for DEFs and CDEFs. Figure 2a shows 15 different weights while\nFigure 2b only has 3 different weights, where the same color represents the same weights.\n(a) DEFs\nstride= 1\nﬁlter size= 3\n(b) CDEFs\nFigure 2: DEFs vs. CDEFs\nLikelihood\nThe observation data is assumed to be drawn conditioned on the lowest hidden layer\nof the DEF, p(xn,i|zn,1). Since we focus on count data, we use the Poisson distribution as the\ndistribution for the observation data.\nIf we let xn,i be the count of event i in sample n and zn,1 is the corresponding hidden variable in\nthe ﬁrst layer, then the likelihood of xn,i would be\np(x = xn,i|zn,1, W0) = Poisson(xn,i, λ = zn,1\nT w0,i) = e−λ λx\nx! .\n2\nThe elements of the observation matrix W0 are from gamma distributions and also tied to be a\nconvolutional matrix.\n2\nConvolutional Sparse Gamma DEF\nIn this paper, we implemented a convolutional sparse gamma DEF.\nThe sparse gamma DEF is a DEF with gamma-distributed latent layers. The probability density of\nthe gamma distribution is\np(z) = z−1 exp(α log(z) −βz −log Γ(α) + α log(β)),\nwhere α and β are natural parameters and Γ is the gamma function.\nThe parameters of a layer are controlled by its immediately higher layer and the weights through the\nlink function, which maps the inner product of the hidden layer and the weights to the parameter of\nthe layer. The link function is given as\ngα = αℓ, gβ =\nαℓ\nzT\nℓ+1wℓ,k\nFrom the link function, we can see the shape is ﬁxed for all the layers while the scale is modiﬁed to\ncontrol the expectation, E(z) = αβ−1.\nThe shape of the weights and hidden layers are set to be less than 1. This kind of gamma distribution\nis called a soft gamma. Most data points drawn from this type of distribution are near 0. It has shown\ngreat performance on feature selection and unsupervised feature discovery [4, 5].\n3\nInference\nTo update the parameters of a CDEF, we need to solve the posterior inference problem. Here, we\nused black box variational inference [10] for the posterior inference.\nVariational inference [6] seeks to solve an optimization problem. It aims to minimize the KL diver-\ngence from an approximating distribution to the posterior, which is equivalent to maximizing the\nEvidence Lower Bound(ELBO)[1]:\nL(q) = Eq(z,W)[log p(x, z, W) −log q(z, W)],\nwhere z denotes all the latent variables and W denotes the weights. This function is the lower bound\non log p(x), which we will maximize by gradient ascent.\nThe approximating distribution q is assumed to be in the mean ﬁeld variational family. Under the\nmean ﬁeld assumption,\nq(z, W) = q(W0)\nL\nY\nℓ=1\nq(Wℓ)\nN\nY\nn=1\nq(zn,ℓ),\nwhere q(zn,ℓ) and q(Wℓ) are fully factorized, n is the sample index and ℓis the layer index. We\nhave a different hidden variable zn,ℓfor a different sample xn.\nEach component in q(zn,ℓ) is\nq(zn,ℓ,k) = EXPFAMℓ(zn,ℓ,k, λn,ℓ,k),\nwhere q(z) and p(z) are in the same exponential family, zn,ℓ,k is the kth hidden variable in layer ℓ\nfor sample n, and λn,ℓ,k is the corresponding parameter.\nq(W) and p(W) are also from the same exponential family, with parameter ξ.\nLet pn,ℓ,k(x, z, W) be the probability of the Markov blanket that contains zn,ℓ,k. Then, the gradient\nfor the approximation of zn,ℓ,k is\n∇λn,ℓ,kL = Ez∼q(z)[[∇λn,ℓ,k log q(zn,ℓ,k)](log pn,ℓ,k(x, z, W) −log q(zn,ℓ,k)].\nFor the original DEFs, the probability of the Markov blanket for a latent variable in the ﬁrst layer is\nlog pn,1,k(x, z, W) = log p(zn,1,k|zn,2, w1,k) + log p(xn|zn,1, W0).\n3\nFor CDEFs, it becomes\nlog pn,1,k(x, z, W) = log p(zn,1,k|zn,2, w1,k) + log p(xnk|zn,1,k, W0),\nwhere xnk denotes the observations connected to the hidden node zn,1,k.\nIn DEFs, the probability of the Markov blanket for a latent variable in the intermediate layer is\nlog pn,ℓ,k(x, z, W) = log p(zn,ℓ,k|zn,ℓ+1, wℓ,k) + log p(zn,ℓ−1|zn,ℓ, Wℓ−1),\nwhile for CDEFs, it becomes\nlog pn,ℓ,k(x, z, W) = log p(zn,ℓ,k|zn,ℓ+1, wℓ,k) + log p(znk,ℓ−1|zn,ℓ,k, Wℓ−1),\nwhere znk,ℓ−1 denotes the hidden variables in the layer ℓ−1 connected to the hidden node zn,ℓ,k.\nIn DEFs, the probability of the Markov blanket for the hidden variable in the top layer is\nlog pn,L,k(x, z, W) = log p(zn,L,k) + log p(zn,L−1|zn,L, WL−1).\nFor CDEFs, it becomes\nlog pn,L,k(x, z, W) = log p(zn,L,k) + log p(znk,L−1|zn,L,k, WL−1),\nwhere znk,L−1 denotes the hidden variables in the layer L −1 connected to the hidden node zn,L,k\nand log p(zn,L,k) is from a given prior distribution.\nNot only did we optimize the ELBO with respect to the hidden variables z, we also optimized the\nELBO with respect to the weights W in the training process. The weights W were ﬁxed in the\ntesting process. We only updated the hidden variables z in the testing process.\nSimilarly, the gradient for W is\n∇ξℓ,i,jL = EW∼q(W)[[∇ξℓ,i,j log q(Wℓ,i,j)](log pℓ,i,j(x, z, W) −log q(Wℓ,i,j))],\nwhere Wℓ,i,j denotes the (i, j)th element of Wℓand pℓ,i,j(x, z, W) is the probability of the\nMarkov blanket that contains Wℓ,i,j. The only difference between DEFs and CDEFs is the term\nlog pℓ,i,j(x, z, W).\nIn DEFs, every Wℓ,i,j corresponds to different ξℓ,i,j. While in CDEFs, several entries of Wℓshare\na same parameter ξ.\nIn DEFs, the probability of the Markov blanket for W0 is\nlog p0,i,j(x, z, W) = log p(W0)(i,j) +\nN\nX\nn=1\nlog p(xn,i|zn,1, W0).\nIn CDEFs, suppose W0(i1,j1), W0(i2,j2), · · · , W0(it,jt) share the same parameter ξ0,i,j, then the\nprobability of the Markov blanket becomes\nlog p0,i,j(x, z, W) = log p(W0)(i1,j1) +\nt\nX\nk=1\nN\nX\nn=1\nlog p(xn,ik|zn,1, W0).\nIn DEFs, the probability of the Markov blanket for Wℓ, where ℓ= {1, · · · , L −1} is\nlog pℓ,i,j(x, z, W) = log p(Wℓ)(i,j) +\nN\nX\nn=1\nlog p(zn,ℓ,i|zn,ℓ+1, Wℓ).\nIn CDEFs, suppose Wℓ(i1,j1), Wℓ(i2,j2), · · · , Wℓ(it,jt) share the same parameter ξℓ,i,j, then the\nprobability of the Markov blanket becomes\nlog pℓ,i,j(x, z, W) = log p(Wℓ)(i1,j1) +\nt\nX\nk=1\nN\nX\nn=1\nlog p(zn,ℓ,ik|zn,ℓ+1, Wℓ).\n4\n4\nExperiments\nWe collected the crime data for Chicago from 2003 to 20161. The days of a year are truncated to\n357, i.e. 51 weeks. The days in a week start on Sunday.\nThe data is arranged in the order: the number of thefts for each location for each day. There are\n77 locations in Chicago. So we have 14 samples (14 years), each of which has 27489 numbers,\nrepresenting the numbers of thefts for each location for each day in that year. As in Figure 3, each\nnode represents the number of crimes for that location.\n· · ·\n· · ·\n· · ·\n77 locations for 1 day\n77 locations for 1 day\n357 days\nFigure 3: The representation of the data.\nThe models we used are all convolutional sparse gamma DEF.\n4.1\nThe Beneﬁt of The Second Layer\nWe constructed 5 models, drawn in Figure 4, to compare the results of test log likelihood:\n• Homogeneous Poisson process (HP): Use the maximum likelihood estimation to estimate\nthe rates of thefts for each location.\n• CDEFs 1-51: 1 layer CDEF with 51 hidden variables in the hidden layer. There is no\noverlap between ﬁlters. Each hidden node is connected with all the observed data in 1\nweek. The ﬁlter size is 539 ( 7 days × 77 locations) and the stride is also 539.\n• CDEFs 1-51, 2-17: 2 layers CDEF with 51 hidden variables in the ﬁrst hidden layer and 17\nhidden nodes in the second hidden layer. The ﬁrst hidden layer is the same as CDEFs 1-51.\nEach node of the second layer is connected with 3 hidden nodes in the ﬁrst hidden layer.\nThe ﬁlter size for the second layer is 3 and the stride is 3.\n• CDEFs 1-51, 2-25: 2 layers CDEFs with 51 hidden nodes in the ﬁrst hidden layer, the\nsame as CDEFs 1-51, and 25 hidden nodes in the second hidden layer. The ﬁlter size for\nthe second hidden layer is 3 and stride is 2.\n• CDEFs 1-51, 2-49: 2 layers CDEFs with 51 hidden nodes in the ﬁrst hidden layer, the\nsame as CDEFs 1-51, and 49 hidden nodes in the second hidden layer. The ﬁlter size for\nthe second hidden layer is 3 and stride is 1.\n1https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2/data\n5\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n7 days for 1 week\n7 days for 1 week\n7 days for 1 week\n51 hidden nodes\n51 weeks\n(a) CDEFs 1-51\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n51 weeks\n(b) CDEFs 1-51, 2-17\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n51 weeks\n(c) CDEFs 1-51, 2-25\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n51 weeks\n(d) CDEFs 1-51, 2-49\nFigure 4: Model Structures\nWe ran the experiments 14 times. For each time, we chose a different year as the testing data. The\nother 13 years were the training data. We hid the data, as in Figure 5, for every other week in the\ntesting year, i.e., we used the data in the odd number of weeks to estimate the number of thefts in\nthe even number of weeks.\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\nvisible 7 days\nhidden 7 days\nvisible 7 days\nvisible 7 days\n357 days\nFigure 5: The representation of the data. Each node represents the 77 numbers of thefts at 77\nlocations for 1 day.\nAs shown in Figure 6, CDEFs with 2 layers have larger test log likelihoods than 1-layer CDEFs.\nCDEFs 1-51, 2-17 and CDEFs 1-51, 2-49 perform better than homogeneous Poisson model.\n6\nCDEFs 1-51\nCDEFs 1-51,2-17\nCDEFs 1-51,2-25\nCDEFs 1-51,2-49\nhomogeneous poisson\nCDEFs 1-51 - HP\nCDEFs 1-51,2-17 - HP\nCDEFs 1-51,2-25 - HP\nCDEFs 1-51,2-49 - HP\n-3.5\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n10 4\ntest log likelihood\n log likelihood\nFigure 6: The left 5 bars are the errorbars of test log likelihood for the above 5 models. The right\n4 bars are the errorbars for the difference between the test log likelihood of CDEFs and HP at each\nrun of the experiments.\n4.2\nThe Beneﬁts of The Overlap\nThe overlap of the ﬁlter can give some clues for all the hidden nodes connected to it. In this way, we\ncould use the hidden nodes to capture the time dependence with less data.\nWe constructed 3 CDEFs models as in Figure 7:\n• CDEFs 1-17: 1 layer CDEFs with 17 hidden nodes. Each hidden node is connected with 3\nweeks of data. The ﬁlter size is 1617 and the stride is 1617 (1617 = 3 weeks × 7 days ×\n77 locations ). There is no overlap.\n• CDEFs 1-25: 1 layer CDEFs with 25 hidden nodes. Each hidden node is connected with 3\nweeks of data. The ﬁlter size is 1617 and the stride is 1078 ( the number of data points for\n2 weeks ).\n• CDEFs 1-49: 1 layer CDEFs with 49 hidden nodes. Each hidden node is connected with 3\nweeks of data. The ﬁlter size is 1617 and the stride is 539 ( the number of data points for 1\nweek ).\n7\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n21 days for 3 weeks\n21 days for 3 weeks\n21 days for 3 weeks\n17 hidden nodes\n51 weeks\n(a) CDEFs 1-17\n· · ·\n· · ·\n25 hidden nodes\n51 weeks\n(b) CDEFs 1-25\n· · ·\n· · ·\n51 weeks\n49 hidden nodes\n(c) CDEFs 1-49\nFigure 7: Model Structures\nFirst, we hid every other 3 weeks of data as in Figure 8. Then, we kept increasing the number of\ndata points visible in every hidden 3 weeks. The visible points in the hidden 3 weeks were chosen\nrandomly. We also ran the experiments 14 times, the same as the previous one.\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\nvisible 21 days\nhidden 21 days\nvisible 21 days\nvisible 21 days\n357 days\nFigure 8: The representation of the data.\nAs shown in Figure 9, when there is no or very small number of data points visible in the hidden\n3 weeks, the overlapping CDEFs (CDEFs 1-25, CDEFs 1-49) behave better than non-overlapping\nCDEFs (CDEFs 1-17) and homogeneous Poisson process. It shows that the CDEFs with overlap can\nutilize the data better since the overlap can help update the parameters of the hidden nodes connected\nto it. As the number of visible data points increases, the performance of CDEFs 1-17 becomes better\nat ﬁrst and then has almost the same behavior as the other CDEFs models, and there is no signiﬁcant\nimprovement for Homogeneous Poisson process or the overlapping CDEFs. The reason is that the\nnumber of thefts is very similar for each location at different days. Even the number of visible points\nincreases, the estimation of the rates of Poisson distribution remains almost the same.\n8\n   0\n   1\n   5\n  10\n  20\n  30\n  50\n 100\n 500\n1000\n1500\nthe number of data points visible in every hidden 3 week\n-3\n-2.8\n-2.6\n-2.4\n-2.2\n-2\n-1.8\n-1.6\ntest log likelihood per point\nCDEFs 1-17\nCDEFs 1-25\nCDEFs 1-49\nHP\nFigure 9: Results\n5\nConclusion\nIn this paper, we developed convolutional deep exponential families as an efﬁcient way to capture\ntime correlation. We have also designed some experiments to show how CDEFs with deep structure\nor overlap behave well with small amount of data.\nReferences\n[1] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n[2] Lawrence D Brown. Fundamentals of statistical exponential families: with applications in\nstatistical decision theory. Ims, 1986.\n[3] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, vol-\nume 1. MIT press Cambridge, 2016.\n[4] Ian\nJ.\nGoodfellow,\nAaron\nCourville,\nand\nYoshua\nBengio.\nLarge-scale\nfeature\nlearning with\nspike-and-slab sparse\ncoding.\nIn\nProceedings of the\nTwenty-nine\nInternational\nConference\non\nMachine\nLearning\n(ICML’12).\nACM,\n2012.\nURL\nhttp://icml.cc/discuss/2012/590.html.\n[5] Daniel Hernández-Lobato, José Miguel Hernández-Lobato, and Pierre Dupont. Generalized\nspike-and-slab priors for bayesian group feature selection using expectation propagation. The\nJournal of Machine Learning Research, 14(1):1891–1945, 2013.\n[6] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduc-\ntion to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.\n[7] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.\nMIT press, 2009.\n9\n[8] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.\nThe handbook of brain theory and neural networks, 3361(10):1995, 1995.\n[9] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436,\n2015.\n[10] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artiﬁcial\nIntelligence and Statistics, pages 814–822, 2014.\n[11] Rajesh Ranganath, Linpeng Tang, Laurent Charlin, and David Blei. Deep exponential families.\nIn Artiﬁcial Intelligence and Statistics, pages 762–771, 2015.\n[12] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:\n85–117, 2015.\n10\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2021-10-27",
  "updated": "2021-10-27"
}