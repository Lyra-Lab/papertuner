{
  "id": "http://arxiv.org/abs/2110.15191v1",
  "title": "URLB: Unsupervised Reinforcement Learning Benchmark",
  "authors": [
    "Michael Laskin",
    "Denis Yarats",
    "Hao Liu",
    "Kimin Lee",
    "Albert Zhan",
    "Kevin Lu",
    "Catherine Cang",
    "Lerrel Pinto",
    "Pieter Abbeel"
  ],
  "abstract": "Deep Reinforcement Learning (RL) has emerged as a powerful paradigm to solve\na range of complex yet specific control tasks. Yet training generalist agents\nthat can quickly adapt to new tasks remains an outstanding challenge. Recent\nadvances in unsupervised RL have shown that pre-training RL agents with\nself-supervised intrinsic rewards can result in efficient adaptation. However,\nthese algorithms have been hard to compare and develop due to the lack of a\nunified benchmark. To this end, we introduce the Unsupervised Reinforcement\nLearning Benchmark (URLB). URLB consists of two phases: reward-free\npre-training and downstream task adaptation with extrinsic rewards. Building on\nthe DeepMind Control Suite, we provide twelve continuous control tasks from\nthree domains for evaluation and open-source code for eight leading\nunsupervised RL methods. We find that the implemented baselines make progress\nbut are not able to solve URLB and propose directions for future research.",
  "text": "URLB: Unsupervised Reinforcement Learning\nBenchmark\nMichael Laskin∗\nUC Berkeley\nmlaskin@berkeley.edu\nDenis Yarats*\nNYU, FAIR\ndenisyarats@cs.nyu.edu\nHao Liu\nUC Berkeley\nKimin Lee\nUC Berkeley\nAlbert Zhan\nUC Berkeley\nKevin Lu\nUC Berkeley\nCatherine Cang\nUC Berkeley\nLerrel Pinto\nNYU\nPieter Abbeel\nUC Berkeley, Covariant\nAbstract\nDeep Reinforcement Learning (RL) has emerged as a powerful paradigm to solve\na range of complex yet speciﬁc control tasks. Yet training generalist agents that\ncan quickly adapt to new tasks remains an outstanding challenge. Recent advances\nin unsupervised RL have shown that pre-training RL agents with self-supervised\nintrinsic rewards can result in efﬁcient adaptation. However, these algorithms\nhave been hard to compare and develop due to the lack of a uniﬁed benchmark.\nTo this end, we introduce the Unsupervised Reinforcement Learning Benchmark\n(URLB). URLB consists of two phases: reward-free pre-training and downstream\ntask adaptation with extrinsic rewards. Building on the DeepMind Control Suite,\nwe provide twelve continuous control tasks from three domains for evaluation and\nopen-source code for eight leading unsupervised RL methods. We ﬁnd that the\nimplemented baselines make progress but are not able to solve URLB and propose\ndirections for future research. Code for the benchmark and implemented baselines\ncan be accessed at https://github.com/rll-research/url_benchmark.\n1\nIntroduction\nDeep Reinforcement Learning (RL) has been at the source of a number of breakthroughs in au-\ntonomous control over the last ﬁve years. RL algorithms have been used to train agents to play Atari\nvideo games directly from pixels [44, 45], learn robotic locomotion [52–54] and manipulation [2]\npolicies from raw sensory input, master the game of Go [58, 59], and play large-scale multiplayer\nvideo games [6, 65]. While these results were signiﬁcant advances in autonomous decision making, a\ndeeper look reveals a fundamental limitation. The above algorithms produced agents capable of only\nsolving the single task they were trained to solve. As a result, current RL approaches produce brittle\npolicies with poor generalization capabilities [16], which limits their applicability to many problems\nof interest [23]. It is therefore important to move beyond today’s powerful but narrow RL systems\ntoward generalist systems capable of quickly adapting to new downstream tasks.\nIn contrast, in the ﬁelds of Computer Vision (CV) and Natural Language Processing (NLP), large-scale\nunsupervised pre-training has enabled sample-efﬁcient few-shot adaptation. In NLP, unsupervised\nsequential modeling has produced powerful few-shot learners [8, 17, 50]. In CV, unsupervised\nrepresentation learning techniques such as contrastive learning have produced algorithms that are\ndramatically more label-efﬁcient than their supervised counterparts [14, 31, 32, 25] and more capable\nof adapting to a host of downstream supervised tasks such as classiﬁcation, segmentation, and object\ndetection. While these advances in unsupervised learning have also beneﬁted RL in terms of learning\n∗equal contribution, order determined by coin ﬂip.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\narXiv:2110.15191v1  [cs.LG]  28 Oct 2021\naction\nPart 1: Unsupervised Pre-training \nwith Intrinsic Rewards\nPart 2: Supervised Finetuning  \nto Downstream Tasks\nobservation \nextrinsic reward\naction\nintrinsic \nreward\nobservation \nintrinsic reward\nself-supervised  \ntask\nwalk\nflip\ndownstream tasks\nrun\nFigure 1: Unlike supervised RL which requires reward interaction at every step, unsupervised RL\nhas two phases: (i) reward-free pre-training and (ii) ﬁne-tuning to an extrinsic reward. During phase\n(i) an agent explores the environment through reward-free interaction with the environment. The\nquality of exploration depends on the intrinsic reward that the agent sets for itself. During phase (ii)\nthe quality of pre-training is evaluated by its adaptation efﬁciency to a downstream task.\nefﬁciently from images [37, 38, 55, 62, 69] as well as introducing new architectures for RL [13, 35],\nthe resulting agents have remained narrow since they still optimize a single extrinsic reward as before.\nFully unsupervised training of RL algorithms requires not only learning self-supervised represen-\ntations but also learning policies without access to extrinsic rewards. Recently, unsupervised RL\nalgorithms have begun to show progress toward more generalist systems by training policies without\nextrinsic rewards. Exploration with self-supervised prediction has enabled agents to explore video\ngames from pixels [48, 49], mutual information-based approaches have demonstrated self-supervised\nskill discovery and generalization to downstream tasks in continuous control domains [21, 30, 43, 57],\nand maximal entropy RL has yielded policies capable of diverse exploration [42, 56, 68]. However,\ncomparing and developing new algorithms has been challenging due to a lack of a uniﬁed evaluation\nbenchmark. Reward-free RL algorithms often use different optimization schemes, different tasks for\nevaluation, and have different evaluation procedures. Additionally, unlike more mature supervised\nRL algorithms [27, 33, 54], there does not exist a uniﬁed codebase for unsupervised RL that can be\nused to develop new methods quickly.\nTo make benchmarking and developing new unsupervised RL approaches easier, we introduce the\nUnsupervised Reinforcement Learning Benchmark (URLB). Built on top of the widely adopted\nDeepMind Control Suite [64], URLB provides a suite of domains of varying difﬁculty for unsu-\npervised pre-training with diverse downstream evaluation tasks. URLB standardizes evaluation of\nunsupervised RL algorithms by deﬁning ﬁxed pre-training and ﬁne-tuning procedures across all\nbaselines. Perhaps most importantly, we open-source code for URLB environments as well as 8\nleading baselines that represent the main approaches taken towards unsupervised pre-training in RL\nto date. Unlike prior code releases for unsupervised RL, URLB uses the same exact optimization\nalgorithm for each baseline which enables transparent benchmarking and lowers the barrier to entry\nfor developing new algorithms. We summarize the main contributions of this paper below:\n1. We introduce URLB, a new benchmark for evaluating unsupervised RL algorithms, which\nconsists of three domains and twelve continuous control tasks of varying difﬁculty to\nevaluate the adaptation efﬁciency of unsupervised RL algorithms.\n2. We open-source a uniﬁed codebase for eight leading unsupervised RL algorithms. Each\nalgorithm is trained with the same optimization backbone for fairness of comparison.\n3. We ﬁnd that while the implemented baselines make progress on the proposed benchmark, no\nexisting unsupervised RL algorithm can solve URLB, and consequently identify promising\nresearch directions to progress unsupervised RL.\nThe benchmark environments, algorithmic baselines, and pre-training and evaluation scripts are\navailable at https://github.com/rll-research/url_benchmark. We believe that URLB will\nmake the development of unsupervised RL agents easier and more transparent by providing a uniﬁed\n2\nWalk\nRun\nFlip\nWalk\nRun\nTop right\nBottom left\nBottom right\nTop left\nWalker\nQuadruped\nJaco Arm\nStand\nJump\nStand\nFigure 2: The three domains (walker, quadruped, jaco arm) and twelve downstream tasks considered\nin URLB. The environments include tasks of varying complexity and require an agent pre-trained on\na given domain to adapt efﬁciently to the downstream tasks within that domain.\nset of evaluation environments, systematic procedures for pre-training and evaluation, and algorithmic\nbaselines that share the same optimization backbone.\n2\nPreliminaries and Notation\nMarkov Decision Process: We consider the typical Reinforcement Learning setting where an agent’s\ninteraction with the environment is modeled through a Markov Decision Process (MDP) [63]. In this\nwork, we benchmark unsupervised RL algorithms in both fully observable MDPs where the agent\nlearns from coordinate state as well as partially observable MDPs (POMDPs) where the agent learns\nfrom partially observable image observations. For simplicity we refer to both image and state-based\nobservations as o. At every timestep t, the agent sees an observation ot and selects an action at\nbased on its policy at ∼πθ(·|ot). The agent then sees the next observation ot+1 and an extrinsic\nreward rext\nt\nprovided by the environment (supervised RL) or an intrinsic reward rint\nt deﬁned through a\nself-supervised objective (unsupervised RL). In this work, we pre-train agents with intrinsic rewards\nrext\nt\nand ﬁne-tune them to downstream tasks with extrinsic rewards rext\nt . Some algorithms considered\nin this work condition the agent on a learned task vector which we denote as w.\nLearning from pixels vs states: We benchmark unsupervised RL where environment observations\not can be either proprioceptive states or RGB images. When learning from pixels, rather than\ndeﬁning the self-supervised task directly as a function of image observations, it is usually more\nconvenient to ﬁrst embed the image and compute the intrinsic reward as a function of these lower\ndimensional features [10, 42, 43, 48]. We therefore deﬁne an embedding as zt = fξ(ot) where\nfξ(ot) is an encoder function. We employ different encoder fξ architectures depending on whether\nthe algorithm receives pixel or state-based input. For pixel-based inputs we use the convolutional\nencoder architecture from SAC-AE [66], while for state-based inputs we use the identity function by\ndefault unless the unsupervised RL algorithm explicitly speciﬁes a different encoding. The intrinsic\nreward rint\nt can be a function of any and all (zt, at, wt) depending on the algorithm. Finally, note\nthat the encoder fξ may or may not be shared with components of the base RL algorithm such as the\nactor and critic.\n3\nURLB: Evaluation and Environments\n3.1\nStandardized of Pre-training and Fine-tuning Procedures\nOne reason why unsupervised RL has been hard to benchmark to date is that there is no agreed\nupon procedure for training and evaluating unsupervised RL agents. To this end, we standardize pre-\ntraining, ﬁne-tuning, and evaluation in URLB. We split pre-training and ﬁne-tuning into two phases\nconsisting of NP T and NF T environment steps respectively. During pre-training, we checkpoint\nagents at 100k, 500k, 1M, 2M steps in order to evaluate downstream performance as a function\nof pre-training steps. For adapting the pre-trained policy to downstream tasks, we evaluate in the\ndata-efﬁcient regime where NF T is 100k, since we are interested in agents that are quick to adapt.\n3\n3.2\nEvaluation\nWe evaluate the performance of an unsupervised RL algorithm by measuring how quickly it adapts to\na downstream task. For each ﬁne-tuning task, we initialize the agent with the pre-trained network\nparameters, ﬁne-tune the agent for 100k steps and measure its performance on the downstream task.\nThis evaluation procedure is similar to how pre-trained networks in CV and NLP are ﬁne-tuned\nto downstream tasks such as classiﬁcation, object detection, and summarization. There exist other\nmeans of evaluating the quality of pre-trained RL agents such as measuring the diversity of data\ncollected during exploration or zero-shot generalization of goal-conditioned agents. However, it is\nchallenging to produce a general method to measure data diversity, and while zero-shot generalization\nwith goal-conditioned agents can be powerful such a benchmark would be limited to goal-conditioned\nRL. For these reasons, data diversity and goal-conditioned zero-shot generalization are less common\nevaluation metrics. In an effort to provide a general benchmark, we focus on the ﬁne-tuning efﬁciency\nof the agent after pre-training which allows us to evaluate a diverse set of baselines.\nUnlike unsupervised methods in CV and NLP which focus solely on representation learning, unsu-\npervised pre-training in RL requires both representation learning and behavior learning. For this\nreason, URLB benchmarks performance for both state-based and pixel-based agents. Benchmarking\nboth state and pixel-based RL separately is important because it allows us to decouple unsuper-\nvised behavior learning from unsupervised representation learning. In state-based RL, the agent\nreceives a near-optimal representation of the world through coordinate states. Evaluating state-based\nunsupervised RL agents allows us to isolate unsupervised behavior discovery without worrying\nabout representation learning as confounding factor. Evaluating pixel-based unsupervised RL agents\nprovides insight into how representations and behaviors can be learned jointly.\n3.3\nURLB Environments\nWe release a set of domains and downstream tasks for URLB that are based on the DeepMind\nControl Suite (DMC) [64]. The three reasons for building URLB on top of DMC are (i) DMC is\nalready widely adopted and familiar to RL practitioners; (ii) DMC environments can be used with\nboth state and pixel-based inputs; (iii) DMC features environments of varying difﬁculty which is\nuseful for designing a benchmark that contains both challenging and feasible tasks. URLB evaluates\nperformance on 12 continuous control tasks (3 domains with 4 downstream tasks per domain). From\neasiest to hardest, the URLB domains and tasks are:\nWalker (Stand, Walk, Flip, Run): A biped constrained to a 2D vertical plane. Walker is a challenging\nintroduction domain for unsupervised RL because it requires the unsupervised agent to learn balancing\nand locomotion skills in order to ﬁne-tune efﬁciently. Quadruped (Stand, Walk, Jump, Run): A\nquadruped within a a 3D space. Like walker, quadruped requires the agent to learn to balance and\nmove but is harder due to a high-dimensional state and action spaces and 3D environment. Jaco\nArm (Reach top left, Reach top right, Reach bottom left, Reach bottom right): Jaco Arm is a 6-DOF\nrobotic arm with a three-ﬁnger gripper. This environment tests the unsupervised RL agent’s ability to\ncontrol the robot arm without locking and perform simple manipulation tasks. It was recently shown\nthat this environment is particularly challenging for unsupervised RL [68].\n4\nURLB: Algorithmic Baselines for Unsupervised RL\nIn addition to introducing URLB, the other primary contribution of this work is open-sourcing a\nuniﬁed codebase for eight leading unsupervised RL algorithms. To date, unsupervised RL algorithms\nhave been hard to compare due to confounding factors such as different evaluation procedures and\noptimization schemes. While URLB provides standardized pre-training, ﬁne-tuning, and evaluation\nprocedures, current algorithms are hard to compare since they rely on different optimization algo-\nrithms. For instance, Curiosity [48] utilizes PPO [54] while APT [42] uses SAC [27] for optimization.\nMoreover, even if two unsupervised RL methods use the same optimization algorithm, small dif-\nferences in implementation can result in large performance differences that are independent of the\npre-training algorithm. For this reason, it is important to provide a uniﬁed codebase with identical\nimplementations of the optimization algorithm for each baseline. Providing such a uniﬁed codebase\nis one of the main contributions of this benchmark.\n4\nAlgorithm 1 Unsupervised RL: Unsupervised Pre-training and Supervised Fine-tuning\nRequire: Randomly initialized actor πθ, critic Qφ, and encoder fξ networks, replay buffer D.\nRequire: Intrinsic rint and extrinsic rext reward functions, discount factor γ.\nRequire: Environment (env), M downstream tasks Tk, k ∈[1, . . . , M].\nRequire: pre-train NPT and ﬁne-tune NFT steps.\n1: for t = 1..NPT do\n▷Part 1: Unsupervised Pre-training\n2:\nat ←πθ(fξ(ot)) + ϵ and ϵ ∼N(0, σ2)\n3:\not+1 ∼P(·|ot, at)\n4:\nD ←D ∪(ot, at, ot+1)\n5:\nUpdate πθ, Qφ, and fξ using minibatches from D and intrinsic reward rint according to Eqs. 1 and 2.\n6: end for\n7: Outputs pre-trained parameters θPT, φPT, and ξPT\n8: for Tk ∈[T1, . . . , TM] do\n▷Part 2: Supervised Fine-tuning\n9:\ninitialize θ ←θPT,φ ←φPT, ξ ←ξPT, reset D\n10:\nfor t = 1..NFT do\n11:\nat ←πθ(fξ(ot)) + ϵ and ϵ ∼N(0, σ2)\n12:\not+1, rext\nt\n∼P(·|ot, at)\n13:\nD ←D ∪(ot, at, rext\nt\n, ot+1)\n14:\nUpdate πθ, Qφ, and fξ using minibatches from D according to Eqs. 1 and 2.\n15:\nend for\n16:\nEvaluate performance of RL agent on task Tk\n17: end for\n4.1\nBackbone RL Algorithm\nSince most of the above algorithms rely on off-policy optimization (and some cannot be optimized\non-policy at all), we opt for a state-of-the-art off-policy optimization algorithm. While SAC [27] has\nbeen the de facto off-policy RL algorithm for many RL methods in the last few years, it is prone to\nsuffering from policy entropy collapse. DrQ-v2 [67] recently showed that using DDPG [41] instead\nof SAC as a learning algorithm leads to a more robust performance on tasks from DMC. For this\nreason, we opt for DrQ-v2 [67] as our base optimization algorithm to learn from images, and DDPG,\nas implemented in DrQ-v2, to learn from states. DDPG is an actor-critic off-policy algorithm for\ncontinuous control tasks. The critic Qφ minimizes the Bellman error\nLQ(φ, D) = E(ot,at,rt,ot+1)∼D\n\u0014\u0010\nQφ(ot, at) −rt −γQ¯φ(ot+1, πθ(ot+1)\n\u00112\u0015\n,\n(1)\nwhere ¯φ is an exponential moving average of the critic weights. The deterministic actor πθ is learned\nby maximizing the expected returns\nLπ(θ, D) = Eot∼D [Qφ(ot, πθ(ot))] .\n(2)\n4.2\nUnsupervised RL Algorithms\nAs part of URLB, we open-source code for eight leading or well-known algorithms across all three of\nthese categories all of which utilize the same optimization backbone. All algorithms provided with\nURLB differ only in their intrinsic reward while keeping all other parts of the RL architecture the\nsame. We list all implemented baselines in Table 1 and provide a brief overview of the algorithms\nconsidered, which are binned into three categories – knowledge-based, data-based, and competence-\nbased algorithms.2 For detailed descriptions of each method we refer the reader to Appendix A.\nKnowledge-based Baselines: Knowledge-based methods aim to increase knowledge about the world\nby maximizing prediction error. As part of the knowledge-based suite, we implement the Intrinsic\nCuriosity Module (ICM) [48], Disagreement [49], and Random Network Distillation (RND) [10]. All\nthree methods utilize a function g to either predict the dynamics g(zt+1|zt, at) (ICM, Disagreement)\nor predict the output of a random network g(zt, at) (RND), where z is the encoding of o. ICM and\nRND maximize prediction error while Disagreement maximizes prediction uncertainty.\nData-based Baselines: Data-based methods aim to achieve data diversity by maximizing entropy.\nWe implement APT [42] and ProtoRL [68] both of which maximize entropy H(z) in different ways.\n2We borrow this terminology from the following unsupervised RL tutorial [61].\n5\nTable 1: Unsupervised RL Algorithms implemented in URLB.\nName\nAlgo. Type\nIntrinsic Reward\nICM [48]\nKnowledge\n∥g(zt+1|zt, at) −zt+1∥2\nDisagreement [49]\nKnowledge\nVar{gi(zt+1|zt, at)}\ni = 1, . . . , N\nRND [10]\nKnowledge\n∥g(zt, at) −˜g(zt, at)∥2\n2\nAPT [42]\nData\nP\nj∈random log ∥zt −zj∥\nj = 1, . . . , K\nProtoRL [68]\nData\nP\nj∈prototypes log ∥zt −zj∥\nj = 1, . . . , K\nSMM [40]\nCompetence\nlog p∗(z) −log qw(z) −log p(w) + log d(w|z)\nDIAYN [21]\nCompetence\nlog q(w|z) + const.\nAPS [43]\nCompetence\nrAPT\nt\n(z) + log q(z|w)\nWalker\nQuadruped\nJaco\nStates (pre-training for 2 × 106)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nRandom Init\nKnowledge-Based\nData-Based\nCompetence-Based\nWalker\nQuadruped\nJaco\nPixels (pre-training for 2 × 106)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nFigure 3: Aggregate results for each algorithm category after pre-training the agent with intrinsic\nrewards for 2M environment steps and ﬁnetuning with extrinisc rewards for 100k steps as described\nin Sec. 3.2. Scores are normalized by the asymptotic performance on each task (i.e., DrQ-v2 and\nDDPG performance after training from 2M steps on pixels and states correspondingly) and we show\nthe mean and standard error of each category. Each algorithm is evaluated across ten random seeds.\nTo provide an aggregate view of each algorithm category, the scores are averaged over individual\ntasks and methods (see Appendix C for detailed results for each algorithm and downstream task).\nThe Random Init baseline represents DrQ-v2 and DDPG trained from a random initialization for\n100k steps. Full results can be found in Section C.\nBoth methods utilize a particle estimator [60] to maximize the entropy by maximizing the distance\nbetween k-nearest neighbors (kNN) for each state or observation embedding z. Since computing kNN\nover the entire replay buffer is expensive, APT estimates entropy across transitions in a randomly\nsampled minibatch. ProtoRL improves on APT by clustering the replay buffer with a contrastive deep\nclustering algorithm SWaV [12]. The centroids of the clusters are called prototypes, which are used\nby ProtoRL to estimate entropy.\nCompetence-based Baselines: Competence-based algorithms, learn an explicit skill vector w by\nmaximizing the mutual information between the encoded observation and skill I(z; w). This mutual\ninformation can be decomposed in two ways, I(z; w) = H(z) −H(z|w) = H(w) −H(w|z). We\nprovide baselines for both decompositions. The former decomposition is utilized in skill discovery\nalgorithms such as DIAYN [21], VIC [24], VALOR [1], which are conceptually similar. For URLB,\nwe implement DIAYN. The latter decomposition, though less common, is implemented in the\nAPS [43], which uses a particle estimator for the entropy term and successor features to represent the\nconditional entropy [30]. Lastly, we implement SMM [40] which combines both decompositions into\none objective. Note that the SMM paper describes both skill-based and skill-free variants, so it can\nbe categorized as both competence and data-based.\n5\nExperiments\nWe evaluate the algorithms listed in Table 1 by pre-training with the intrinsic reward objective and\nﬁne-tuning on the downstream task as described in Section 3.2. For DrQ-v2 optimization we ﬁx the\nhyper-parameters from [67] and for algorithm-speciﬁc hyper-parameters we perform a grid sweep\n6\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nRandom Init\nKnowledge-Based\nData-Based\nCompetence-Based\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco\n(a) State-based learning.\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nRandom Init\nKnowledge-Based\nData-Based\nCompetence-Based\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco\n(b) Pixel-based learning.\nFigure 4: We display the ﬁne-tuning efﬁciency as a function of pre-training steps. As in Fig. 3 scores\nare asymptotically normalized, averaged across tasks and algorithms on a per-category basis, and\nevaluated over ten seeds. Our expectation is that a longer pre-training phase should lead to more\nefﬁcient ﬁne-tuning. However, in several cases the empirical evidence goes against our intuition\ndemonstrating that longer pre-training is not always beneﬁcial. Understanding this shortcoming\nof current methods is an important direction for future research. Detailed results can be found in\nFigures 6 and 7.\nand pick the best performing parameters. We benchmark both state and pixel-based experiments\nand keep all non-algorithm-speciﬁc architectural details the same with a full description available in\nAppendix B. Performance on each downstream task is evaluated over ten random seeds and we display\nthe mean scores and standard errors. We summarize the main results of our evaluation in Figures 3\nand 4, which show evaluation scores grouped by algorithm category, described in Section 4.2, and\nenvironment, described in Section 3.3. An extensive list of results across all algorithms considered in\nthis work can be found in Appendix C.\nBy benchmarking a wide array of exploration algorithms on both state and pixel-based tasks we\nare able to get perspective on the current state of unsupervised RL. Overall, we ﬁnd that while\nunsupervised RL shows promise, it is still far from solving the proposed benchmark and many open\nquestions need to be addressed to make progress toward unsupervised pre-training for RL. We note\nthat solving the benchmark means matching the asymptotic DrQ-v2 (for pixels) and DDPG (for states)\nperformance within 100k steps of ﬁne-tuning. The motivation for this deﬁnition is that unsupervised\nRL agents get access to unlimited reward-free environment interactions. After pre-training, we seek\nto develop agents that adapt quickly to the desired downstream task. We list our observations below:\nO1: None of the implemented unsupervised RL algorithms solve the benchmark. Despite access to up\nto 2M pre-training steps, after 100k steps of ﬁne-tuning no method matches asymptotic performance\non most tasks. The best-performing benchmarked algorithms achieve 40 −70% normalized return\nwhereas the benchmark is considered solved when the agent achieves near 100% normalized returns.\nThis suggests that we are still far as a community from efﬁcient generalization in deep RL.\nO2: Unsupervised RL is not universally better than random initialization. We also observe that\nﬁne-tuning an unsupervised RL baseline is not always preferable to ﬁne-tuning from a random\ninitialization. In particular when learning from states, a random initialization is competitive with\nmost baselines. However, when learning from pixels ﬁne-tuning from random initialization degrades\nsuggesting that representation learning is an important component of unsupervised pre-training.\n7\nO3: There exists a large gap in performance between exploring from states and exploring from pixels.\nAnother observation that supports representation learning as an important aspect of exploration is\nthat exploration algorithms degrade substantially when learning from pixels compared to learning\nfrom state. Shown in Figure 3, most algorithms lose 20 −50% when learning from pixels compared\nto state and especially so on the harder environments (Quadruped, Jaco Arm). These results suggest\nthat better representation learning during pre-training is an important research direction.\nO4: In aggregate, competence-based approaches underperform knowledge-based and data-based\napproaches. While knowledge-based and data-based approaches both perform competitively across\nURLB, we ﬁnd that competence-based approaches are lagging behind. Speciﬁcally, there is no\ncompetence-based approach that achieves state-of-the-art mean performance on any of the URLB\ntasks, which points to competence-based unsupervised RL as an impactful research direction with\nsigniﬁcant room for improvement.\nO5: There is not a single leading unsupervised RL algorithm for both states and pixels. We observe\nthat there is no single state-of-the-art algorithm for unsupervised RL. At 2M pre-training steps,\nAPT [42] and ProtoRL [68] are the leading algorithms for state-based URLB while ICM [48]\nachieves leading performance on pixel-based URLB despite the existence of more sophisticated\nknowledge-based methods [49, 10] (see Figure 5).\nO6: For many unsupervised RL algorithms, rather than monotonically improving performance decays\nas a function of pre-training steps. We desire and would expect that the ﬁne-tuning efﬁciency of\nunsupervised RL algorithms would improve as a function of pre-training steps. Surprisingly, we\nﬁnd that for 9 out of 18 experiments shown in Figure 4, performance either does not improve or\neven degrades as a function of pre-training steps. We see this as potentially the biggest drawback of\ncurrent unsupervised RL approaches – they do not scale with the number of environment interactions.\nDeveloping algorithms that improve monotonically as a function of pre-training steps is an open and\nimpactful line of research.\nO7: New ﬁne-tuning strategies will likely be needed for fast adaptation.\nWhile not investigated\nin depth in this benchmark, new ﬁne-tuning strategies could play a large role in the adoption of\nunsupervised RL. Perhaps part of the issue raised in O6 could be addressed with better ﬁne-tuning.\nThe algorithms in URLB are all ﬁne-tuned by initializing the actor-critic with the pre-trained weights\nand ﬁne-tuning with an extrinsic reward. There are likely other better strategies for ﬁne-tuning,\nparticularly for competence based approaches that are conditioned on the skill w.\n6\nRelated work\nDeep Reinforcement Learning Benchmarks. Part of the accelerated progress in deep RL over the\nlast few years has been due to the existence of stable benchmarks. Speciﬁcally, the Atari Arcade\nLearning Environment [5], the OpenAI gym [7], and more recently the DeepMind Control (DMC)\nSuite [64] have become standard benchmarks for evaluating supervised RL agents in both state\nand pixel-based observation spaces and discrete and continuous action spaces. Open-sourcing code\nfor algorithms has been another aspect that accelerated progress in deep RL. For instance, Duan\net al. [19] not only presented a benchmark for continuous control but also provided baselines for\ncommon supervised RL algorithms, which led to the development of the widely used OpenAI\ngym benchmark [7] and baselines [18]. The combination of challenging yet feasible benchmarks\nand open-sourced code were important components in the discovery of many widely adopted RL\nalgorithms [27, 44, 52–54].\nIn addition to Atari, OpenAi gym, and DeepMind control, there have been many other benchmarks\ndesigned to study different aspects of supervised RL. DeepMind lab [4] benchmarks 3D navigation\nfrom pixels, ProcGen [15, 16] measures generalization of supervised agents in procedurally generated\nenvironments, D4RL [22] and RL unplugged [26] benchmark performance of ofﬂine RL methods,\nB-Pref [39] benchmarks performance of preference-based RL methods, Metaworld [70] measures\nthe performance of multi-task and meta-RL algorithms, and SafetyGym [51] measures how RL\nagents can achieve tasks with safety constraints. However, while the existing benchmarks are\nsuitable for supervised RL algorithms, there is no such benchmark and collections of easy-to-use\nbaseline algorithms for unsupervised RL, which is our primary motivation for accelerating progress\nin unsupervised RL through URLB.\n8\nUnsupervised Reinforcement Learning. While investigations into unsupervised deep RL appeared\nshortly after the landmark DQN [44], the ﬁeld has experienced accelerated progress over the last year,\nwhich has been in part due to advents in unsupervised representation learning in CV [14, 31, 32]\nand NLP [8, 17, 50] as well as the development for stable RL optimization algorithms [27, 33, 41,\n54]. However, unlike CV and NLP which focus solely on unsupervised representation learning,\nunsupervised RL has required both unsupervised representation and behavioral learning.\nUnsupervised Representation Learning for Deep RL: In order for an RL algorithm to learn a policy\nπ(a|s) it must ﬁrst have a good representation for the state s. When working with coordinate\nstate, the representation is supplied by a the human task designer but when operating from image\nobservations o, we must ﬁrst transform the observations into latent vectors z. This transformation\ncomprises the study of representation learning for RL. One of the ﬁrst seminal works on unsupervised\nrepresentation learning for RL showed that unsupervised auxiliary tasks improve performance of\nsupervised RL [34]. Over the last two years, a series of works in unsupervised representation learning\nfor RL with world models [28, 29] contrastive learning [38, 62, 68], autoencoders [66], and data\naugmentation [37, 67, 69] have dramaticaly improved learning efﬁciency from pixels. On many tasks\nfrom the DMC suite, learning from pixels is now as data-efﬁcient as learning from state [38].\nUnsupervised Behavioral Learning for Deep RL: One caveat is that the above algorithms are not fully\nunsupervised since they still optimize for an extrinsic reward but with an auxiliary unsupervised loss.\nFully unsupervised RL also requires unsupervised learning of behaviors, which is typically achieved\nby optimizing for an intrinsic reward [47]. Given that representation learning is already heavily\nbenchmarked for RL [28, 38, 69], URLB focuses mostly on unsupervised behavior learning. Many\nrecent algorithms have been proposed for intrinsic behavioral learning, which include prediction\nmethods [9, 48, 49], maximal entropy-based methods [11, 42, 43, 46, 56, 68], and maximal mutual\ninformation-based methods [21, 30, 43, 57]. However, these methods use different pre-training and\nevaluation procedures, different optimization algorithms, and different environments. To make fully\nunsupervised RL algorithm comparisons transparent and easier to develop, we introduce URLB.\n7\nConclusion\nWe presented URLB, a benchmark designed to measure the performance of unsupervised RL algo-\nrithms. URLB consists of a suite of twelve evaluation tasks of varying difﬁculty from three domains\nand standardized procedures for pre-training and evaluation. We’ve open-sourced implementations\nand evaluation scores for eight leading unsupervised RL algorithms from all major algorithm cat-\negories. To minimize confounding factors, we utilized the same optimization method across all\nbaselines. While none of the implemented baselines solve URLB, many make substantial progress\nsuggesting a number of fruitful directions for unsupervised RL research. We hope that this benchmark\nmakes the development and comparison of unsupervised RL algorithms easier and clearer.\nLimitations. There are a number of limitations for both URLB and unsupervised RL methods\nin general. While URLB tasks are designed to be challenging, they are far from the visual and\ncombinatorial complexity of real-world robotics. However, existing algorithms are unable to solve the\nbenchmark meaning there is substantial room for improvement on the URLB tasks before moving on\nto even more challenging ones. While we present standardized pre-training and evaluation procedures,\nthere can be many other ways of measuring the quality of the exploration algorithm. For instance,\nthe quality of pre-training can be evaluated not only through policy adaptation but also through\ndataset diversity which we do not consider in this paper. In this work, similar to the Atari [5] and\nDMC [64] benchmarks for supervised RL we do not consider goal-conditioned RL which can be\nquite powerful for exploration [20]. For generality, we chose the currently most commonly used\nevaluation procedure that allowed us to benchmark a diverse set of leading exploration algorithms\nbut, of course, other choices are available and would be interesting to investigate in future work.\nPotential negative impacts. Unsupervised RL has the beneﬁts of requiring zero extrinsic reward\ninteractions during pre-training, and due to this the resulting agents may develop policies that are not\naligned with human intent. This could be problematic in the long-term if not addressed early and\ncarefully because as unsupervised robotics get more capable they can inadvertently inﬂict harm on\nthemselves or the environment. Methods for constraining exploration within a broad set of human\npreferences (e.g. explore without harming the environment) is an interesting and important direction\nfor future research in order to produced safe agents.\n9\nAcknowledgements\nThis work was partially supported by Berkeley DeepDrive, BAIR, the Berkeley Center for Human-\nCompatible AI, the Ofﬁce of Naval Research grant N00014-21-1-2769, and DARPA through the\nMachine Common Sense Program.\nReferences\n[1] Achiam, Joshua, Edwards, Harrison, Amodei, Dario, and Abbeel, Pieter. Variational option\ndiscovery algorithms. arXiv preprint arXiv:1807.10299, 2018.\n[2] Akkaya, Ilge, Andrychowicz, Marcin, Chociej, Maciek, Litwin, Mateusz, McGrew, Bob, Petron,\nArthur, Paino, Alex, Plappert, Matthias, Powell, Glenn, Ribas, Raphael, et al. Solving rubik’s\ncube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.\n[3] Barreto, Andre, Borsa, Diana, Quan, John, Schaul, Tom, Silver, David, Hessel, Matteo,\nMankowitz, Daniel, Zidek, Augustin, and Munos, Remi. Transfer in deep reinforcement\nlearning using successor features and generalised policy improvement. In International Confer-\nence on Machine Learning, pp. 501–510. PMLR, 2018.\n[4] Beattie, Charles, Leibo, Joel Z, Teplyashin, Denis, Ward, Tom, Wainwright, Marcus, Küttler,\nHeinrich, Lefrancq, Andrew, Green, Simon, Valdés, Víctor, Sadik, Amir, et al. Deepmind lab.\narXiv preprint arXiv:1612.03801, 2016.\n[5] Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\n[6] Berner, Christopher, Brockman, Greg, Chan, Brooke, Cheung, Vicki, D˛ebiak, Przemysław,\nDennison, Christy, Farhi, David, Fischer, Quirin, Hashme, Shariq, Hesse, Chris, et al. Dota 2\nwith large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[7] Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig, Schneider, Jonas, Schulman, John, Tang,\nJie, and Zaremba, Wojciech. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[8] Brown, Tom B, Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared, Dhariwal,\nPrafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda, et al. Language\nmodels are few-shot learners. In Advances in Neural Information Processing Systems, 2020.\n[9] Burda, Yuri, Edwards, Harri, Pathak, Deepak, Storkey, Amos, Darrell, Trevor, and Efros,\nAlexei A. Large-scale study of curiosity-driven learning. In International Conference on\nLearning Representations, 2019. URL https://openreview.net/forum?id=rJNwDjAqYX.\n[10] Burda, Yuri, Edwards, Harrison, Storkey, Amos, and Klimov, Oleg. Exploration by random\nnetwork distillation. In International Conference on Learning Representations, 2019.\n[11] Campos, Víctor, Sprechmann, Pablo, Hansen, Steven Stenberg, Barreto, Andre, Kapturowski,\nSteven, Vitvitskyi, Alex, Badia, Adria Puigdomenech, and Blundell, Charles. Beyond ﬁne-\ntuning: Transferring behavior in reinforcement learning. In ICML 2021 Workshop on Unsuper-\nvised Reinforcement Learning, 2021.\n[12] Caron, Mathilde, Misra, Ishan, Mairal, Julien, Goyal, Priya, Bojanowski, Piotr, and Joulin,\nArmand. Unsupervised learning of visual features by contrasting cluster assignments. In\nAdvances in Neural Information Processing Systems, 2020.\n[13] Chen, Lili, Lu, Kevin, Rajeswaran, Aravind, Lee, Kimin, Grover, Aditya, Laskin, Michael,\nAbbeel, Pieter, Srinivas, Aravind, and Mordatch, Igor. Decision transformer: Reinforcement\nlearning via sequence modeling, 2021.\n[14] Chen, Ting, Kornblith, Simon, Norouzi, Mohammad, and Hinton, Geoffrey E. A simple\nframework for contrastive learning of visual representations. In International conference on\nmachine learning, 2020.\n10\n[15] Cobbe, Karl, Klimov, Oleg, Hesse, Chris, Kim, Taehoon, and Schulman, John. Quantifying\ngeneralization in reinforcement learning. In International Conference on Machine Learning,\n2019.\n[16] Cobbe, Karl, Hesse, Chris, Hilton, Jacob, and Schulman, John. Leveraging procedural genera-\ntion to benchmark reinforcement learning. In International conference on machine learning,\n2020.\n[17] Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, and Toutanova, Kristina. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies,\n2019.\n[18] Dhariwal, Prafulla, Hesse, Christopher, Klimov, Oleg, Nichol, Alex, Plappert, Matthias, Radford,\nAlec, Schulman, John, Sidor, Szymon, Wu, Yuhuai, and Zhokhov, Peter. Openai baselines.\nhttps://github.com/openai/baselines, 2017.\n[19] Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John, and Abbeel, Pieter. Benchmarking\ndeep reinforcement learning for continuous control. In International conference on machine\nlearning, 2016.\n[20] Ecoffet, Adrien, Huizinga, Joost, Lehman, Joel, Stanley, Kenneth O., and Clune, Jeff. First re-\nturn, then explore. arXiv preprint arXiv:2004.12919, 2020. doi: 10.1038/s41586-020-03157-9.\n[21] Eysenbach, Benjamin, Gupta, Abhishek, Ibarz, Julian, and Levine, Sergey. Diversity is all\nyou need: Learning skills without a reward function. In International Conference on Learning\nRepresentations, 2019.\n[22] Fu, Justin, Kumar, Aviral, Nachum, Oﬁr, Tucker, George, and Levine, Sergey. D4rl: Datasets\nfor deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[23] Gleave, Adam, Dennis, Michael, Wild, Cody, Kant, Neel, Levine, Sergey, and Russell, Stuart.\nAdversarial policies: Attacking deep reinforcement learning. In International Conference on\nLearning Representations, 2020.\n[24] Gregor, Karol, Rezende, Danilo Jimenez, and Wierstra, Daan. Variational intrinsic control. In\nInternational Conference on Learning Representations, 2017.\n[25] Grill, Jean-Bastien, Strub, Florian, Altché, Florent, Tallec, Corentin, Richemond, Pierre H,\nBuchatskaya, Elena, Doersch, Carl, Pires, Bernardo Avila, Guo, Zhaohan Daniel, Azar, Moham-\nmad Gheshlaghi, et al. Bootstrap your own latent: A new approach to self-supervised learning.\nIn Advances in Neural Information Processing Systems, 2020.\n[26] Gulcehre, Caglar, Wang, Ziyu, Novikov, Alexander, Paine, Tom Le, Colmenarejo, Sergio Gomez,\nZolna, Konrad, Agarwal, Rishabh, Merel, Josh, Mankowitz, Daniel, Paduraru, Cosmin, et al. Rl\nunplugged: A suite of benchmarks for ofﬂine reinforcement learning. In Advances in Neural\nInformation Processing Systems, 2020.\n[27] Haarnoja, Tuomas, Zhou, Aurick, Abbeel, Pieter, and Levine, Sergey. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In International\nConference on Machine Learning, 2018.\n[28] Hafner, Danijar, Lillicrap, Timothy, Fischer, Ian, Villegas, Ruben, Ha, David, Lee, Honglak,\nand Davidson, James. Learning latent dynamics for planning from pixels. In International\nConference on Machine Learning, 2019.\n[29] Hafner, Danijar, Lillicrap, Timothy, Ba, Jimmy, and Norouzi, Mohammad. Dream to con-\ntrol: Learning behaviors by latent imagination. In International Conference on Learning\nRepresentations, 2020.\n[30] Hansen, Steven, Dabney, Will, Barreto, André, Warde-Farley, David, de Wiele, Tom Van,\nand Mnih, Volodymyr. Fast task inference with variational intrinsic successor features. In\nInternational Conference on Learning Representations, 2020.\n11\n[31] He, Kaiming, Fan, Haoqi, Wu, Yuxin, Xie, Saining, and Girshick, Ross B. Momentum contrast\nfor unsupervised visual representation learning. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020.\n[32] Hénaff, Olivier J., Srinivas, Aravind, Fauw, Jeffrey De, Razavi, Ali, Doersch, Carl, Eslami, S.\nM. Ali, and van den Oord, Aäron. Data-efﬁcient image recognition with contrastive predictive\ncoding. In International Conference on Machine Learning, 2020.\n[33] Hessel, Matteo, Modayil, Joseph, van Hasselt, Hado, Schaul, Tom, Ostrovski, Georg, Dabney,\nWill, Horgan, Dan, Piot, Bilal, Azar, Mohammad Gheshlaghi, and Silver, David. Rainbow: Com-\nbining improvements in deep reinforcement learning. In Conference on Artiﬁcial Intelligence,\n2018.\n[34] Jaderberg, Max, Mnih, Volodymyr, Czarnecki, Wojciech Marian, Schaul, Tom, Leibo, Joel Z.,\nSilver, David, and Kavukcuoglu, Koray. Reinforcement learning with unsupervised auxiliary\ntasks. In International Conference on Learning Representations, 2017.\n[35] Janner, Michael, Li, Qiyang, and Levine, Sergey. Reinforcement learning as one big sequence\nmodeling problem. CoRR, abs/2106.02039, 2021.\n[36] Kingma, Diederik P and Welling, Max. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n[37] Laskin, Michael, Lee, Kimin, Stooke, Adam, Pinto, Lerrel, Abbeel, Pieter, and Srinivas, Aravind.\nReinforcement learning with augmented data. In Advances in Neural Information Processing\nSystems, 2020.\n[38] Laskin, Michael, Srinivas, Aravind, and Abbeel, Pieter.\nCurl: Contrastive unsupervised\nrepresentations for reinforcement learning. In International Conference on Machine Learning,\n2020.\n[39] Lee, Kimin, Smith, Laura, Dragan, Anca, and Abbeel, Pieter. B-pref: Benchmarking preference-\nbased reinforcement learning. In Thirty-ﬁfth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 1), 2021.\n[40] Lee, Lisa, Eysenbach, Benjamin, Parisotto, Emilio, Xing, Eric P., Levine, Sergey, and Salakhut-\ndinov, Ruslan. Efﬁcient exploration via state marginal matching. CoRR, abs/1906.05274,\n2019.\n[41] Lillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa,\nYuval, Silver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning.\nIn International Conference on Learning Representations, 2016.\n[42] Liu, Hao and Abbeel, Pieter. Behavior from the void: Unsupervised active pre-training. arXiv\npreprint arXiv:2103.04551, 2021.\n[43] Liu, Hao and Abbeel, Pieter. APS: active pretraining with successor features. In International\nConference on Machine Learning, 2021.\n[44] Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare,\nMarc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[45] Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Tim-\nothy, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep\nreinforcement learning. In International Conference on Machine Learning, 2016.\n[46] Mutti, Mirco, Pratissoli, Lorenzo, and Restelli, Marcello. A policy gradient method for task-\nagnostic exploration. In Conference on Artiﬁcial Intelligence, 2021.\n[47] Oudeyer, Pierre-Yves, Kaplan, Frdric, and Hafner, Verena V. Intrinsic motivation systems\nfor autonomous mental development. IEEE transactions on evolutionary computation, 11(2):\n265–286, 2007.\n12\n[48] Pathak, Deepak, Agrawal, Pulkit, Efros, Alexei A, and Darrell, Trevor. Curiosity-driven\nexploration by self-supervised prediction. In International Conference on Machine Learning,\n2017.\n[49] Pathak, Deepak, Gandhi, Dhiraj, and Gupta, Abhinav. Self-supervised exploration via disagree-\nment. In International Conference on Machine Learning, 2019.\n[50] Radford, Alec, Wu, Jeffrey, Child, Rewon, Luan, David, Amodei, Dario, and Sutskever, Ilya.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[51] Ray, Alex, Achiam, Joshua, and Amodei, Dario. Benchmarking safe exploration in deep\nreinforcement learning. arXiv preprint arXiv:1910.01708, 2019.\n[52] Schulman, John, Levine, Sergey, Abbeel, Pieter, Jordan, Michael, and Moritz, Philipp. Trust\nregion policy optimization. In International Conference on Machine Learning, 2015.\n[53] Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan, Michael, and Abbeel, Pieter. High-\ndimensional continuous control using generalized advantage estimation. In International\nConference on Learning Representations, 2016.\n[54] Schulman, John, Wolski, Filip, Dhariwal, Prafulla, Radford, Alec, and Klimov, Oleg. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[55] Schwarzer, Max, Anand, Ankesh, Goel, Rishab, Hjelm, R Devon, Courville, Aaron, and\nBachman, Philip. Data-efﬁcient reinforcement learning with self-predictive representations. In\nInternational Conference on Learning Representations, 2021.\n[56] Seo, Younggyo, Chen, Lili, Shin, Jinwoo, Lee, Honglak, Abbeel, Pieter, and Lee, Kimin.\nState entropy maximization with random encoders for efﬁcient exploration. In International\nConference on Machine Learning, 2021.\n[57] Sharma, Archit, Gu, Shixiang, Levine, Sergey, Kumar, Vikash, and Hausman, Karol. Dynamics-\naware unsupervised discovery of skills. In International Conference on Learning Representa-\ntions, 2020.\n[58] Silver, David, Schrittwieser, Julian, Simonyan, Karen, Antonoglou, Ioannis, Huang, Aja, Guez,\nArthur, Hubert, Thomas, Baker, Lucas, Lai, Matthew, Bolton, Adrian, et al. Mastering the game\nof go without human knowledge. Nature, 550(7676):354, 2017.\n[59] Silver, David, Hubert, Thomas, Schrittwieser, Julian, Antonoglou, Ioannis, Lai, Matthew, Guez,\nArtfhur, Lanctot, Marc, Sifre, Laurent, Kumaran, Dharshan, Graepel, Thore, et al. A general\nreinforcement learning algorithm that masters chess, shogi, and go through self-play. Science,\n362(6419):1140–1144, 2018.\n[60] Singh, Harshinder, Misra, Neeraj, Hnizdo, Vladimir, Fedorowicz, Adam, and Demchuk, Eugene.\nNearest neighbor estimates of entropy. American Journal of Mathematical and Management\nSciences, 23(3-4):301–321, 2003.\n[61] Srinivas, Aravind and Abbeel, Pieter. Unsupervised learning for reinforcement learning, 2021.\nURL https://icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf.\n[62] Stooke, Adam, Lee, Kimin, Abbeel, Pieter, and Laskin, Michael. Decoupling representation\nlearning from reinforcement learning. In International Conference on Machine Learning, 2021.\n[63] Sutton, Richard S and Barto, Andrew G. Reinforcement learning: An introduction. MIT Press,\n2018.\n[64] Tassa, Yuval, Doron, Yotam, Muldal, Alistair, Erez, Tom, Li, Yazhe, Casas, Diego de Las,\nBudden, David, Abdolmaleki, Abbas, Merel, Josh, Lefrancq, Andrew, et al. Deepmind control\nsuite. arXiv preprint arXiv:1801.00690, 2018.\n[65] Vinyals, Oriol, Babuschkin, Igor, Czarnecki, Wojciech M, Mathieu, Michael, Dudzik, Andrew,\nChung, Junyoung, Choi, David H, Powell, Richard, Ewalds, Timo, Georgiev, Petko, et al.\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):\n350–354, 2019.\n13\n[66] Yarats, Denis, Zhang, Amy, Kostrikov, Ilya, Amos, Brandon, Pineau, Joelle, and Fergus, Rob.\nImproving sample efﬁciency in model-free reinforcement learning from images. arXiv preprint\narXiv:1910.01741, 2019.\n[67] Yarats, Denis, Fergus, Rob, Lazaric, Alessandro, and Pinto, Lerrel. Mastering visual continuous\ncontrol: Improved data-augmented reinforcement learning, 2021.\n[68] Yarats, Denis, Fergus, Rob, Lazaric, Alessandro, and Pinto, Lerrel. Reinforcement learning\nwith prototypical representations. In International Conference on Machine Learning, 2021.\n[69] Yarats, Denis, Kostrikov, Ilya, and Fergus, Rob. Image augmentation is all you need: Regu-\nlarizing deep reinforcement learning from pixels. In International Conference on Learning\nRepresentations, 2021.\n[70] Yu, Tianhe, Quillen, Deirdre, He, Zhanpeng, Julian, Ryan, Hausman, Karol, Finn, Chelsea, and\nLevine, Sergey. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement\nlearning. In Conference on Robot Learning, 2020.\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Section 7.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See\nSection 7.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See Section 5,\nthe appendix for hyperparameters. You can access the code with full instructions in the\nsupplementary materials or using this link https://github.com/rll-research/\nurl_benchmark.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Section 5 and 3.2 and the supplementary material.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] See Section 5 and the supplementary material.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See Section F.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [Yes]\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nSee custom_dmc_tasks folder in supplementary materials codebase.\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [N/A]\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n14\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n15\nAppendix:\nUnsupervised Reinforcement Learning Benchmark\nA\nUnsupervised Reinforcement Learning Baselines\nA.1\nKnowledge-based Baselines\nPrediction methods train a forward dynamics model f(ot+1|ot, at) and deﬁne a self-supervised task\nbased on the outputs of the model prediction.\nCuriosity [48]: The Intrinsic Curiosity Module (ICM) deﬁnes the self-supervised task as the error\nbetween the state prediction of a learned dynamics model ˆz′ ∼g(z′|z, a) and the observation. The\nintuition is that parts of the state space that are hard to predict are good to explore because they were\nlikely to be unseen before. An issue with Curiosity is that it is susceptible to the noisy TV problem\nwherein stochastic elements of the environment will always cause high prediction error while not\nbeing informative for exploration:\nrICM\nt\n∝∥g(zt+1|zt, at) −zt+1∥2.\nDisagreement [49]: Disagreement is similar to ICM but instead trains an ensemble of forward models\nand deﬁnes the intrinsic reward as the variance (or disagreement) among the models. Disagreement\nhas the favorable property of not being susceptible to the noisy TV problem, since high stochasticity in\nthe environment will result high prediction error but low variance if it has been thoroughly explored:\nrDisagreement\nt\n∝Var{gi(zt+1|zt, at)}\ni = 1, . . . , N.\nRND [10]: Random Network Distillation (RND) deﬁnes the self-supervised task by predicting the\noutput of a frozen randomly initialized neural network ˜f. This differs from ICM only in that instead\nof predicting the next state, which is effectively an environment-deﬁned function, it tries to predict\nthe vector output of a randomly deﬁned function. Similar to ICM, RND can suffer from the noisy TV\nproblem:\nrRND\nt\n∝∥g(zt, at) −˜g(zt, at)∥2\n2.\nA.2\nData-based Baselines\nRecently, exploration through state entropy maximization has resulted in simple yet effective al-\ngorithms for unsupervised pre-training. We implement two leading variants of this approach for\nURLB.\nAPT [42]: Active Pre-training (APT) utilizes a particle-based estimator [60] that uses K nearest-\nneighbors to estimate entropy for a given state or image embedding. Since APT does not itself perform\nrepresentation learning, it requires an auxiliary representation learning loss to provide latent vectors\nfor entropy estimation, although it is also possible to use random network embeddings [56]. We\nprovide implementations of APT with the forward g(zt+1|zt, at) and inverse dynamics h(at|zt+1, zt)\nrepresentation learning losses:\nrAPT\nt\n∝\nX\nj∈random\nlog ∥zt −zj∥\nj = 1, . . . , K.\nProtoRL [68]: ProtoRL devises a self-supervised pre-training scheme that allows to decouple\nrepresentation learning and exploration to enable efﬁcient downstream generalization to previously\nunseen tasks. For this, ProtoRL uses the contrastive clustering assignment loss from SWaV [12]\nand learns latent representations and a set of prototypes to form the basis of the latent space. The\nprototypes are then used for more accurate estimation of entropy of the state-visitation distribution\nvia KNN particle-based estimator:\nrProto\nt\n∝\nX\nj∈prototypes\nlog ∥zt −zj∥\nj = 1, . . . , K.\n16\nA.3\nCompetence-based Baselines\nCompetence-based approaches learn skills w that maximize the mutual information between encoded\nobservations (or states) and skills I(z; w). The mutual information has two decompositions I(z; w) =\nH(w) −H(w|z) = H(z) −H(z|w). We provide baselines for both decompositions.\nSMM [40]: SMM minimizes DKL(pπ(z) ∥p∗(z)), which maximizes the state entropy, while\nminimizing the cross entropy from the state to the target state distribution. When using skills,\nH(z) can be rewritten as H(z|w) + I(z; w). H(z|w) can maximized by optimizing the reward\nr = log qw(z), which is estimated using a VAE [36] that models the density of z while executing\nskill w. Similar to other mutual information methods that decompose I(z; w) = H(w) −H(w|z),\nSMM learns a discriminator d(w|z) over a set of discrete skills with a uniform prior that maximizes\nH(w):\nrSMM\nt\n=\n∆log p∗(z) −log qw(z) −log p(w) + log d(w|z).\nDIAYN [21]: DIAYN and similar algorithms such as VIC [24] and VALOR [1] are perhaps the best\ncompetence-based exploration algorithms. These methods estimate the mutual information through\nthe ﬁrst decomposition I(z; w) = H(w) −H(w|z). H(w) is kept maximal by drawing w ∼p(w)\nfrom a discrete uniform prior distribution and the density −H(w|z) is estimated with a discriminator\nlog q(w|z).\nrDIAYN\nt\n∝log q(w|z) + const.\nAPS [43]: APS is a recent leading mutual information exploration method that uses the second\ndecomposition I(z; w) = H(z) −H(z|w). H(z) is estimated with a particle estimator as in\nAPT [42] while H(z|w) is estimated with successor features as in VISR [30].3\nrAPS\nt\n∝rAPT\nt\n(z) + log q(z|w)\nB\nHyper-parameters\nIn Table 2 we present a common set of hyper-parameters used in our experiments, while in table 3 we\nlist individual hyper-parameters for each method.\nTable 2: A common set of hyper-parameters used in our experiments.\nCommon hyper-parameter\nValue\nReplay buffer capacity\n106\nAction repeat\n1 states-based and 2 for pixels-based\nSeed frames\n4000\nn-step returns\n3\nMini-batch size\n1024 states-based and 256 for pixels-based\nSeed frames\n4000\nDiscount (γ)\n0.99\nOptimizer\nAdam\nLearning rate\n10−4\nAgent update frequency\n2\nCritic target EMA rate (τQ)\n0.01\nFeatures dim.\n1024 states-based and 50 for pixels-based\nHidden dim.\n1024\nExploration stddev clip\n0.3\nExploration stddev value\n0.2\nNumber pre-training frames\nup to 2 × 106\nNumber ﬁne-turning frames\n1 × 105\n3In this benchmark, the generalized policy improvement(GPI) [3] that is used in Atari games for APS and\nVISR is not implemented for continuous control experiments.\n17\nTable 3: Per algorithm sets of hyper-parameters used in our experiments.\nICM hyper-parameter\nValue\nRepresentation dim.\n512\nReward transformation\nlog(r + 1.0)\nForward net arch.\n(|O| + |A|) →1024 →1024 →|O| ReLU MLP\nInverse net arch.\n(2 × |O|) →1024 →1024 →|A| ReLU MLP\nDisagreement hyper-parameter\nValue\nEnsemble size\n5\nForward net arch:\n(|O| + |A|) →1024 →1024 →|O| ReLU MLP\nRND hyper-parameter\nValue\nRepresentation dim.\n512\nPredictor & target net arch.\n|O| →1024 →1024 →512 ReLU MLP\nNormalized observation clipping\n5\nAPT hyper-parameter\nValue\nRepresentation dim.\n512\nReward transformation\nlog(r + 1.0)\nForward net arch.\n(512 + |A|) →1024 →512 ReLU MLP\nInverse net arch.\n(2 × 512) →1024 →|A| ReLU MLP\nk in NN\n12\nAvg top k in NN\nTrue\nProtoRL hyper-parameter\nValue\nPredictor dim.\n128\nProjector dim.\n512\nNumber of prototypes\n512\nSoftmax temperature\n0.1\nk in NN\n3\nNumber of candidates per prototype\n4\nEncoder target EMA rate (τenc)\n0.05\nSMM hyper-parameter\nValue\nSkill dim.\n4\nSkill discrim lr\n10−3\nVAE lr\n10−2\nDIAYN hyper-parameter\nValue\nSkill dim\n16\nSkill sampling frequency (steps)\n50\nDiscriminator net arch.\n512 →1024 →1024 →16 ReLU MLP\nAPS hyper-parameter\nValue\nRepresentation dim.\n512\nReward transformation\nlog(r + 1.0)\nSuccessor feature dim.\n10\nSuccessor feature net arch.\n|O| →1024 →1024 →10 ReLU MLP\nk in NN\n12\nAvg top k in NN\nTrue\nLeast square batch size\n4096\n18\nC\nPer-domain Individual Results\nIndividual ﬁne-tuning results for each methods are shown in Figure 5. Furthermore, Figures 6 and 7\ndemonstrate individual results of states and pixels based ﬁne-tuning performance as a function of\npre-training steps for each considered method and task.\nWalker\nQuadruped\nJaco\nStates (pre-training for 1 × 105)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nDrQ-v2@100k\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nQuadruped\nJaco\nPixels (pre-training for 1 × 105)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nQuadruped\nJaco\nStates (pre-training for 5 × 105)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nQuadruped\nJaco\nPixels (pre-training for 5 × 105)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nQuadruped\nJaco\nStates (pre-training for 1 × 106)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nQuadruped\nJaco\nPixels (pre-training for 1 × 106)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nQuadruped\nJaco\nStates (pre-training for 2 × 106)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker\nQuadruped\nJaco\nPixels (pre-training for 2 × 106)\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nFigure 5: Individual results of ﬁne-tuning for 100k steps after different degrees of pre-training for\neach considered method. The performance is aggregated across all the tasks within a domain and\nnormalized with respect to the optimal performance.\n19\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Flip\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Run\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Stand\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Walk\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Jump\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Run\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Stand\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Walk\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Bottom Left\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Bottom Right\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Top Left\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Top Right\nFigure 6: Individual results of ﬁne-tuning efﬁciency as a function of pre-training steps for states-based\nlearning.\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Flip\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Run\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Stand\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nWalker: Walk\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Jump\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Run\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Stand\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nQuadruped: Walk\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Bottom Left\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Bottom Right\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Top Left\n1 × 105\n5 × 105\n1 × 106\n2 × 106\nPre-training Frames\n0\n20\n40\n60\n80\n100\nNormalized Return (%)\nJaco: Reach Top Right\nFigure 7: Individual results of ﬁne-tuning efﬁciency as a function of pre-training steps for pixels-based\nlearning.\n20\nD\nFinetuning Learning Curves\nWe provide ﬁnetuning learning curves for agents pre-trained for 2M steps with intrinsic rewards.\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nNormalized Score\nwalker_stand\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.0\n0.2\n0.4\n0.6\n0.8\nNormalized Score\nwalker_walk\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nNormalized Score\nwalker_run\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nNormalized Score\nwalker_flip\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score\nquadruped_walk\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.1\n0.2\n0.3\n0.4\n0.5\nNormalized Score\nquadruped_run\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.2\n0.4\n0.6\n0.8\nNormalized Score\nquadruped_stand\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Score\nquadruped_jump\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Score\njaco_reach_top_left\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nNormalized Score\njaco_reach_top_right\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Score\njaco_reach_bottom_left\n0.2\n0.4\n0.6\n0.8\n1.0\nEnv Steps\n1e5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nNormalized Score\njaco_reach_bottom_right\nICM\nDisagreement\nRND\nAPT\nProtoRL\nDIAYN\nAPS\nSMM\nFinetuning Learning Curves\nFigure 8: Finetuning curves for each evaluated unsupervised algorithm for each task considered in\nthis benchmark after the agent has been pre-trained with intrinsic rewards.\n21\nE\nIndividual Numerical Results\nThe individual numerical results of ﬁne-tuning for each task and each method are presented in Table 4\nfor states-based learning, and in Table 5 for pixels-based learning.\nPre-trainining for 1 × 105 frames\nDomain\nTask\nDDPG (DrQ-v2)\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n538±27\n535±19\n559±20\n581±21\n580±28\n523±16\n388±36\n352±22\n638±33\nRun\n325±25\n384±24\n437±24\n437±33\n424±28\n250±34\n244±28\n259±26\n428±26\nStand\n899±23\n944±5\n937±6\n947±6\n925±9\n926±24\n738±61\n784±68\n872±34\nWalk\n748±47\n805±46\n911±14\n857±32\n888±19\n831±31\n592±53\n584±25\n731±70\nQuadruped\nJump\n236±48\n291±35\n261±45\n383±57\n334±48\n220±33\n386±56\n267±34\n589±57\nRun\n157±31\n195±31\n198±40\n203±20\n161±27\n138±21\n224±33\n179±26\n420±49\nStand\n392±73\n390±59\n420±76\n446±17\n559±57\n425±82\n430±86\n350±55\n662±64\nWalk\n229±57\n185±20\n265±45\n229±26\n173±29\n141±23\n227±47\n193±29\n664±56\nJaco\nReach bottom left\n72±22\n117±24\n100±20\n121±19\n124±19\n86±20\n64±13\n64±15\n156±9\nReach bottom right\n117±18\n155±10\n179±6\n161±8\n141±14\n82±21\n68±17\n44±8\n164±10\nReach top left\n116±22\n152±24\n143±14\n141±15\n136±23\n110±19\n33±6\n26±6\n153±13\nReach top right\n94±18\n159±15\n159±15\n168±9\n175±6\n116±22\n47±12\n59±12\n186±7\nPre-trainining for 5 × 105 frames\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n538±27\n554±27\n568±42\n685±46\n594±29\n501±19\n472±30\n380±24\n637±36\nRun\n325±25\n416±18\n485±25\n499±21\n410±27\n228±18\n328±19\n241±19\n337±25\nStand\n899±23\n930±7\n940±8\n946±5\n930±5\n925±17\n906±18\n762±44\n869±27\nWalk\n748±47\n846±30\n923±5\n869±23\n826±40\n865±16\n791±43\n632±34\n778±58\nQuadruped\nJump\n236±48\n252±41\n452±45\n542±51\n282±48\n225±32\n387±56\n350±59\n493±55\nRun\n157±31\n184±42\n368±28\n377±28\n182±22\n153±29\n205±26\n258±24\n347±39\nStand\n392±73\n422±49\n649±53\n722±49\n470±80\n433±63\n499±78\n459±54\n743±46\nWalk\n229±57\n237±43\n412±67\n498±68\n217±29\n209±47\n238±27\n218±24\n553±74\nJaco\nReach bottom left\n72±22\n94±16\n145±13\n113±11\n123±15\n106±18\n61±10\n38±9\n134±6\nReach bottom right\n117±18\n119±15\n136±15\n144±6\n136±13\n115±19\n82±10\n63±11\n131±8\nReach top left\n116±22\n125±18\n165±9\n121±16\n118±18\n122±23\n57±9\n29±10\n124±11\nReach top right\n94±18\n151±8\n181±7\n150±8\n170±8\n120±22\n60±10\n43±6\n106±10\nPre-trainining for 1 × 106 frames\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n538±27\n524±20\n586±36\n610±27\n505±22\n480±17\n486±17\n338±23\n531±24\nRun\n325±25\n344±30\n488±23\n482±23\n373±22\n254±29\n332±40\n249±24\n352±31\nStand\n899±23\n922±15\n919±11\n946±5\n916±20\n905±25\n903±18\n870±34\n846±28\nWalk\n748±47\n845±27\n880±19\n873±24\n821±35\n848±29\n746±51\n553±33\n808±61\nQuadruped\nJump\n236±48\n306±42\n595±39\n615±53\n400±53\n287±52\n349±63\n365±15\n415±46\nRun\n157±31\n157±24\n444±39\n444±42\n237±35\n206±32\n280±40\n343±25\n400±48\nStand\n392±73\n428±79\n736±51\n763±79\n526±58\n436±62\n391±36\n529±52\n712±57\nWalk\n229±57\n140±24\n729±46\n644±70\n246±33\n266±64\n312±63\n525±76\n505±84\nJaco\nReach bottom left\n72±22\n114±9\n144±9\n114±10\n125±10\n122±22\n58±8\n43±13\n87±8\nReach bottom right\n117±18\n126±10\n129±10\n106±13\n128±12\n113±20\n62±9\n34±6\n109±9\nReach top left\n116±22\n146±11\n156±10\n136±13\n110±5\n114±19\n61±7\n12±2\n108±13\nReach top right\n94±18\n143±10\n159±10\n132±10\n149±11\n123±21\n61±9\n31±9\n101±9\nPre-trainining for 2 × 106 frames\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n538±27\n514±25\n491±21\n515±17\n477±16\n480±23\n505±26\n381±17\n461±24\nRun\n325±25\n388±30\n444±21\n439±34\n344±28\n200±15\n430±26\n242±11\n257±27\nStand\n899±23\n913±12\n907±15\n923±9\n914±8\n870±23\n877±34\n860±26\n835±54\nWalk\n748±47\n713±31\n782±33\n828±29\n759±35\n777±33\n821±36\n661±26\n711±68\nQuadruped\nJump\n236±48\n205±33\n668±24\n590±33\n462±48\n425±63\n298±39\n578±46\n538±42\nRun\n157±31\n133±20\n461±12\n462±23\n339±40\n316±36\n220±37\n415±28\n465±37\nStand\n392±73\n329±58\n840±33\n804±50\n622±57\n560±71\n367±42\n706±48\n714±50\nWalk\n229±57\n143±31\n721±56\n826±19\n434±64\n403±91\n184±26\n406±64\n602±86\nJaco\nReach bottom left\n72±22\n106±8\n134±8\n101±12\n88±12\n121±22\n40±9\n17±5\n96±13\nReach bottom right\n117±18\n119±9\n122±4\n100±10\n115±12\n113±16\n50±9\n31±4\n93±9\nReach top left\n116±22\n119±12\n117±14\n111±10\n112±11\n124±20\n50±7\n11±3\n65±10\nReach top right\n94±18\n137±9\n140±7\n140±10\n136±5\n135±19\n37±8\n19±4\n81±11\nTable 4: Individual results of ﬁne-tuning for 1 × 105 frames after different levels of pre-training in\nthe states-based settings.\n22\nPre-trainining for 1 × 105 frames\nDomain\nTask\nDrQ-v2\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n81±23\n252±54\n80±33\n214±38\n25±1\n293±33\n24±1\n132±24\n38±7\nRun\n41±11\n110±21\n57±14\n78±13\n25±1\n135±13\n22±1\n50±6\n26±1\nStand\n212±28\n315±67\n250±62\n261±34\n162±9\n353±67\n133±8\n233±22\n162±9\nWalk\n141±53\n302±45\n192±68\n263±43\n43±16\n320±52\n23±1\n138±25\n29±2\nQuadruped\nJump\n278±35\n226±40\n173±15\n223±30\n160±24\n246±33\n211±25\n204±24\n182±32\nRun\n156±21\n156±13\n112±12\n145±17\n134±21\n156±27\n148±18\n173±23\n133±24\nStand\n309±47\n329±49\n259±31\n350±43\n266±37\n342±35\n297±36\n350±48\n265±48\nWalk\n151±31\n160±10\n134±24\n154±16\n119±17\n168±24\n149±18\n157±23\n161±27\nJaco\nReach bottom left\n23±10\n18±7\n12±4\n41±7\n0±0\n38±11\n1±1\n12±4\n0±0\nReach bottom right\n23±8\n30±12\n23±8\n57±8\n0±0\n37±9\n1±0\n10±3\n0±0\nReach top left\n40±9\n31±11\n30±9\n66±9\n0±0\n59±14\n2±1\n19±4\n2±1\nReach top right\n37±9\n37±13\n22±8\n48±7\n3±2\n45±16\n4±3\n24±8\n4±2\nPre-trainining for 5 × 105 frames\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n81±23\n260±39\n360±16\n222±37\n28±2\n210±44\n25±1\n117±18\n32±2\nRun\n41±11\n110±15\n131±19\n103±13\n26±1\n85±16\n23±1\n47±4\n27±1\nStand\n212±28\n499±62\n398±65\n289±26\n155±11\n355±61\n139±9\n243±16\n161±9\nWalk\n141±53\n305±51\n348±46\n258±33\n37±10\n250±54\n23±1\n125±19\n48±19\nQuadruped\nJump\n278±35\n286±50\n214±24\n366±44\n147±26\n229±42\n201±20\n248±28\n212±27\nRun\n156±21\n198±29\n153±19\n261±38\n112±20\n144±27\n138±16\n197±24\n178±23\nStand\n309±47\n398±69\n298±35\n453±47\n229±42\n355±67\n279±26\n313±31\n281±51\nWalk\n151±31\n193±31\n129±20\n206±30\n111±20\n157±25\n139±13\n140±19\n141±24\nJaco\nReach bottom left\n23±10\n65±20\n30±8\n47±8\n0±0\n31±14\n1±1\n12±3\n0±0\nReach bottom right\n23±8\n56±16\n34±10\n52±6\n0±0\n35±11\n1±0\n7±2\n0±0\nReach top left\n40±9\n87±22\n47±11\n55±8\n1±1\n35±15\n2±1\n20±4\n2±1\nReach top right\n37±9\n68±17\n33±5\n61±8\n2±1\n42±14\n4±3\n21±5\n1±1\nPre-trainining for 1 × 106 frames\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n81±23\n256±49\n305±34\n250±29\n46±17\n244±37\n26±1\n126±26\n42±12\nRun\n41±11\n116±16\n155±12\n96±13\n26±1\n84±11\n24±1\n47±4\n30±3\nStand\n212±28\n534±73\n565±37\n374±37\n150±13\n480±63\n145±6\n251±19\n170±10\nWalk\n141±53\n285±45\n433±29\n316±41\n36±9\n258±39\n25±1\n137±25\n54±25\nQuadruped\nJump\n278±35\n345±47\n199±31\n368±38\n156±27\n237±50\n201±20\n319±38\n184±29\nRun\n156±21\n179±11\n140±24\n297±36\n115±21\n108±16\n139±13\n165±17\n155±22\nStand\n309±47\n430±44\n257±35\n559±51\n229±42\n338±71\n279±26\n319±27\n275±48\nWalk\n151±31\n251±25\n104±19\n274±19\n115±21\n152±28\n139±13\n213±20\n146±23\nJaco\nReach bottom left\n23±10\n65±19\n42±10\n46±9\n0±0\n36±13\n1±1\n8±3\n0±0\nReach bottom right\n23±8\n88±23\n58±11\n44±9\n0±0\n43±10\n1±0\n4±1\n0±0\nReach top left\n40±9\n76±19\n89±19\n59±7\n6±4\n41±10\n2±1\n20±4\n2±0\nReach top right\n37±9\n87±24\n49±12\n47±7\n2±1\n47±12\n4±3\n22±6\n5±1\nPre-trainining for 2 × 106 frames\nDomain\nTask\nDDPG\nICM\nDisagreement\nRND\nAPT\nProtoRL\nSMM\nDIAYN\nAPS\nWalker\nFlip\n81±23\n231±34\n339±16\n280±31\n28±2\n223±27\n26±1\n114±21\n38±9\nRun\n41±11\n98±11\n154±9\n133±15\n25±2\n87±18\n24±1\n45±3\n30±3\nStand\n212±28\n401±40\n552±92\n389±49\n155±11\n467±69\n145±6\n298±71\n172±10\nWalk\n141±53\n274±44\n424±36\n321±43\n35±8\n297±48\n25±1\n132±19\n37±5\nQuadruped\nJump\n278±35\n312±18\n194±21\n383±28\n164±27\n197±35\n201±20\n262±22\n199±29\nRun\n156±21\n249±21\n143±25\n284±18\n121±20\n137±35\n139±13\n190±18\n156±24\nStand\n309±47\n506±40\n305±35\n561±43\n243±41\n290±56\n279±26\n426±40\n331±43\nWalk\n151±31\n231±15\n145±10\n294±20\n122±21\n138±35\n139±13\n184±23\n146±24\nJaco\nReach bottom left\n23±10\n72±20\n106±18\n39±3\n0±0\n21±5\n1±1\n7±2\n1±0\nReach bottom right\n23±8\n58±19\n90±15\n47±9\n0±0\n28±7\n1±0\n9±3\n1±1\nReach top left\n40±9\n89±22\n127±21\n60±6\n0±0\n47±16\n2±1\n11±2\n2±1\nReach top right\n37±9\n69±18\n118±23\n76±11\n1±1\n52±12\n4±3\n16±3\n10±3\nTable 5: Individual results of ﬁne-tuning for 1 × 105 frames after different levels of pre-training in\nthe pixels-based settings.\nF\nCompute Resources\nURLB is designed to be accessible to the RL research community. Both state and pixel-based\nalgorithms are implemented such that each algorithm requires a single GPU. For local debugging\nexperiments we used NVIDIA RTX GPUs. For large-scale runs used to generate all results in this\nmanuscripts, we used NVIDIA Tesla V100 GPU instances. All experiments were run on internal\nclusters. Each algorithm trains in roughly 30 mins - 12 hours depending on the snapshot (100k, 500k,\n1M, 2M) and input (states, pixels). Since this benchmark required roughly 8k experiments (2 states /\npixels, 12 tasks, 8 algorithms, 10 seeds, 4 snapshots) a total of 100 V100 GPUs were used to produce\nthe results in this benchmark. Researchers who wish to build on URLB will, of course, not need to\nrun this many experiments since they can utilize the results presented in this benchmark.\n23\nG\nIntuition on Competence-based Approaches Underperform on URLB\nAcross the three methods - data-based, knowledge-based, and competence-based - the best data-based\nand knowledge-based methods are competitive with one another. For instance, RND (a leading\nknowledge-based methods) and ProtoRL (a leading data-based method) achieve similar ﬁnetuning\nscores. Both are maximizing data diversity in two different ways - one through maximizing prediction\nerror and the other through entropy maximization.\nOn the other hand, competence-based methods as a whole do much worse than data-based and\nknowledge-based ones. We hypothesize that this is due to current competence-based methods only\nsupporting small skill spaces. Competence-based methods maximize a variational lower bound to the\nmutual information of the form:\nI(τ; z) = H(z) −H(z|τ) = H(z) + E[log p(z|τ)] ≥H(z) + E[log q(z|τ)]\nwhere q(z|s) is called the discriminator. The discriminator can be interpreted as a classiﬁer from\ns →z (or vice versa depending on how you decompose I(s; z) ). In order to have an accurate\ndiscriminator, z is chosen to be small in practice (DIAYN - z is a 16 dim one-hot, SMM - z is 4 dim\ncontinuous, APS - z is 10 dim continuous).\nOpenAI gym environments for continuous control mask this limitation because they terminate if\nthe agent falls over and hence leak extrinsic signal about the downstream task into the environment.\nThis means that the agent learns only useful behaviors that keep it balanced and therefore a small\nskill vector is sufﬁcient for classifying these behaviors. However, in DeepMind control (and hence\nURLB) the episodes have ﬁxed length and therefore the set of possible behaviors is much larger.\nIf the skill space is too small, the most likely skills to be classiﬁed are different conﬁgurations of\nthe agent lying on the ground. We hypothesize that building more powerful discriminators would\nimprove competence-based exploration.\n24\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2021-10-28",
  "updated": "2021-10-28"
}