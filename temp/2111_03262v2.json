{
  "id": "http://arxiv.org/abs/2111.03262v2",
  "title": "CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations",
  "authors": [
    "Tianyu Zhang",
    "Yuxiang Ren",
    "Wenzheng Feng",
    "Weitao Du",
    "Xuecang Zhang"
  ],
  "abstract": "Unsupervised graph representation learning is a non-trivial topic. The\nsuccess of contrastive methods in the unsupervised representation learning on\nstructured data inspires similar attempts on the graph. Existing graph\ncontrastive learning (GCL) aims to learn the invariance across multiple\naugmentation views, which renders it heavily reliant on the handcrafted graph\naugmentations. However, inappropriate graph data augmentations can potentially\njeopardize such invariance. In this paper, we show the potential hazards of\ninappropriate augmentations and then propose a novel Collaborative Graph\nContrastive Learning framework (CGCL). This framework harnesses multiple graph\nencoders to observe the graph. Features observed from different encoders serve\nas the contrastive views in contrastive learning, which avoids inducing\nunstable perturbation and guarantees the invariance. To ensure the\ncollaboration among diverse graph encoders, we propose the concepts of\nasymmetric architecture and complementary encoders as the design principle. To\nfurther prove the rationality, we utilize two quantitative metrics to measure\nthe assembly of CGCL respectively. Extensive experiments demonstrate the\nadvantages of CGCL in unsupervised graph-level representation learning and the\npotential of collaborative framework. The source code for reproducibility is\navailable at https://github.com/zhangtia16/CGCL",
  "text": "CGCL: Collaborative Graph Contrastive\nLearning without Handcrafted Graph Data\nAugmentations\nTianyu Zhang1, Yuxiang Ren2(\u0000) ⋆, Wenzheng Feng2, Weitao Du2,3, and\nXuecang Zhang2\n1 Department of Automation, Tsinghua University, Beijing, China\nzhangty2016@gmail.com\n2 Huawei Technologies, China\n{renyuxiang1, fengwenzheng, zhangxuecang}@huawei.com\n3 Chinese Academy of Sciences\nduweitao@amss.ac.cn\nAbstract. Unsupervised graph representation learning is a non-trivial\ntopic. The success of contrastive methods in the unsupervised repre-\nsentation learning on structured data inspires similar attempts on the\ngraph. Existing graph contrastive learning (GCL) aims to learn the in-\nvariance across multiple augmentation views, which renders it heavily\nreliant on the handcrafted graph augmentations. However, inappropri-\nate graph data augmentations can potentially jeopardize such invariance.\nIn this paper, we show the potential hazards of inappropriate augmenta-\ntions and then propose a novel Collaborative Graph Contrastive Learn-\ning framework (CGCL). This framework harnesses multiple graph en-\ncoders to observe the graph. Features observed from different encoders\nserve as the contrastive views in contrastive learning, which avoids in-\nducing unstable perturbation and guarantees the invariance. To ensure\nthe collaboration among diverse graph encoders, we propose the con-\ncepts of asymmetric architecture and complementary encoders as the\ndesign principle. To further prove the rationality, we utilize two quan-\ntitative metrics to measure the assembly of CGCL respectively. Ex-\ntensive experiments demonstrate the advantages of CGCL in unsuper-\nvised graph-level representation learning and the potential of collabo-\nrative framework. The source code for reproducibility is available at\nhttps://github.com/zhangtia16/CGCL\nKeywords: Graph Representation Learning · Contrastive Learning ·\nCollaborative Framework\n1\nIntroduction\nGraph-structured data contain not only the attribute information of individual\nunits (i.e., nodes) but also the connection information (i.e., edges) between these\n⋆T. Zhang and Y. Ren—Both authors contribute equally to this paper.\narXiv:2111.03262v2  [cs.LG]  1 Apr 2024\n2\nT. Zhang et al.\nNode Dropping\nEdge Perturbation\nSubgraph\nAdd & Delete Edge\nDrop Node\nHigh invariance\nLow invariance\n0.7973\n0.4536\n0.9706\n0.4289\n0.5021\n0.3769\nFig. 1. An illustration of the unstable invariance of three graph augmentation strate-\ngies. The value of each augmented graph is its similarity to the original graph. The\nupper part of each augmentation strategy shows augmented graphs preserving high\ninvariance, while the lower part’s augmentations bring low invariance.\nunits. Due to this ability, many applications exhibit the favorable property of\ngraph-structured data, which makes learning effective graph representations for\ndownstream tasks a non-trivial problem. In recent years, graph neural networks\n(GNNs) [28,12,26,34,22] have demonstrated excellent performance in graph rep-\nresentation learning. Usually, GNNs learn the graph representation in supervised\nor semi-supervised scenarios. However, task-specific labels are scarce or unevenly\ndistributed [38], which make it time-consuming and labor-intensive to obtain su-\npervised signals. For example, labeling protein data necessitates a large workforce\nand material resources. For the obstacle of scarce labels, unsupervised graph\nrepresentation learning has emerged as the critical technology to achieve break-\nthroughs. Recently, contrastive learning has shown attractive potential in unsu-\npervised representation learning, including natural language processing [19] and\ncomputer vision [3,9,6,4]. In the graph domain, some works [35,29,21,27,14,20]\nalso have explored the mechanism, which tends to maximize feature consistency\nunder differently augmented views to learn the desired invariance from these\ntransformations. As a result, data augmentation strategies have a strong influ-\nence on contrastive learning performance.\nData augmentation has achieved great success in image data where the in-\nvariance of various views (e.g., color-invariant, rotation-invariant, and resizing-\ninvariant) are well-understood [3,31]. However, due to the complex structural\ninformation and the coupling between nodes in the graph, the changes induced\nby the data augmentation to graphs are not easy to measure. For example, in\nimage data, it is natural to evaluate what kind of data augmentation is more\npowerful (e.g., color distortion strength for color permuting [3]), but the sit-\nuation of the graph data is much more complicated. Modifying the attributes\nof a node is not only related to the target node but also affects its neighbour-\ning nodes. Furthermore, the importance of each node and edge in the graph\nis far from equivalent, which differs from the importance of pixels in image\ndata. Specifically, removing a critical edge is enough to change a graph from a\nCGCL: Collaborative Graph Contrastive Learning\n3\nconnected graph to a disconnected one, making the augmented graph and the\noriginal graph have little learnable invariance. In Figure 1, we provide an illustra-\ntion to show the unstable invariance between the original graph and augmented\ngraphs under three different data augmentation strategies. To measure the de-\nstruction of invariance brought by those strategies quantitatively, we annotate\nthe cosine similarity between embeddings of original graph and augmented ones.\nThe graph embeddings are extracted by Node2vec [7]. The upper part of each\naugmentation strategy shows an augmented graph preserving high invariance,\nwhile the lower part’s augmentations bring low invariance. Figure 1 shows that\nthe impact of handcrafted augmentations on invariance is uncontrollable, even\nunder the same augmentation strategy. Therefore, commonly used graph aug-\nmentation strategies (e.g., Node dropping, Edge perturbation, and Subgraph\nsampling, etc.) are still not well-explored and lack generalization across datasets\nfrom diverse fields. You et al. [35] reach similar conclusions after testing various\naugmentation strategies. For instance, edge perturbation is more suitable for so-\ncial networks but hurts biochemical molecules. Facing the aforementioned issue,\nmany researchers turn to explore the possibility of discarding data augmentation\nfrom contrastive framework recently [13,36,23].\nTo remedy the issue of unstable invariance from inappropriate data augmen-\ntations, we propose a novel graph-level contrastive learning framework named\nCGCL, where no handcrafted graph augmentation is needed. CGCL uses multi-\nple GNN-based graph encoders to enforce contrastive learning in a collaborative\nway, remedying the limitation of unstable invariance between the original graph\nand augmented graphs. Recalling that contrastive learning aims at learning the\ninvariance across multiple contrastive views, existing GCL methods with data\naugmentations generate multiple contrastive views from the data augmentations.\nBy contrast, CGCL generates contrastive views from the encoder perspective.\nBecause no handcrafted structural disturbance is injected, contrastive learning\non the collaboration of multiple graph encoders can ensure invariance. To cope\nwith the problem of model collapse, we devise the asymmetric structure for\nCGCL. The asymmetry lies in the differences of GNN-based encoders’ message-\npassing schemes. Besides, graph encoders in CGCL are supposed to be comple-\nmentary for a stronger fitting ability. Specifically, high complementarity indicate\nthat encoders together carry less redundant parameters. For a further theoretical\nanalysis, we propose two metrics: Asymmetry Coefficient (AC) and Complemen-\ntarity Coefficient (CC). Those two metrics are to measure the asymmetry and\ncomplementarity of the collaborative framework quantitatively. We validate the\nperformance of CGCL on graph classification task over 9 datasets. Compared\nwith the state-of-the-art methods, CGCL demonstrates better generalization on\nvarious datasets and achieves better results without using extra handcrafted data\naugmentations. In addition, we implement experiments with the two quantita-\ntive metrics. The experiments show that the assembly with high asymmetry and\ncomplementarity has a better performance, which is in accord with our motiva-\ntion of designing CGCL’s asymmetric architecture and complementary encoders.\nThe contributions of our work are summarized as follows:\n4\nT. Zhang et al.\n– We propose a novel Collaborative Graph Contrastive Learning (CGCL) to\nreinforce unsupervised graph-level representation learning, which requires\nno handcrafted data augmentations. We explain the essence of collabora-\ntive framework as generating multiple contrastive views from the encoder\nperspective.\n– We propose the concepts of asymmetric structure and complementary en-\ncoders as foundational principles for the collaborative learning paradigm. To\nprovide a more comprehensive theoretical analysis, we put forth two quanti-\ntative metrics to assess both the asymmetry and complementarity inherent\nin the collaborative framework.\n– Extensive experiments on nine datasets show that CGCL has advantages\nin graph classification task compared with the state-of-the-art methods. Be-\nsides, empirical evidence validates that the architecture of CGCL, character-\nized by its inherent asymmetry and complementarity, indeed yields enhanced\nperformance outcomes.\n2\nRelated Work\n2.1\nGraph-level Representation Learning\nGraph-level representation learning is a critical topic [2], aiming at learning a\nlow-dimensional vector for an entire graph by utilizing its topology structure and\nnodes feature. Early researches could track back to kernel-based methods, includ-\ning GK [25], WL [24], and DGK [33]. Kernel-based methods learn representa-\ntions with a kernel function measuring the similarity between graph structures.\nLater, the idea of neural networks enables researchers to develop some graph\nembedding-based methods. Node2vec [7] use random walk and skip-gram tech-\nniques to capture structural information. Graph2vec [16] learns the graph-level\nembedding directly. Sub2vec [1] utilizes subgraphs for capturing more global\ninformation. In the recent past, GNNs have shown awe-inspiring capabilities.\nGCN [12] extends convolution to graphs by a novel Fourier transformation.\nGAT [28] first imports the attention mechanism into graphs. GIN [32] devel-\nops a general framework to analyze GNN’s learning ability. It is worth not-\ning that a pooling function is needed for graph-level representation learning\nwith GNNs [11]. Recently, due to the insufficient labeled data, unsupervised\nsetting attracts researchers’ attention. Some representative methods include In-\nfoGraph [26], GraphCL [35], AD-GCL [27] and RGCL [14]. We will introduce\nthose GCL methods more detailed in the next subsection.\n2.2\nContrastive Learning\nContrastive learning has been used for unsupervised learning in natural lan-\nguage processing [19] and computer vision [3,9,6,4]. Inspired by those success,\nresearchers try to generalize contrastive learning to graph data. DGI [29] in-\ntroduces the idea of mutual information maximization [10] to learn node-level\nCGCL: Collaborative Graph Contrastive Learning\n5\nrepresentation. InfoGraph [26] further extends the mutual information maxi-\nmization to graph-level representations. More recently, GCL methods with data\naugmentations have achieved the state-of-the-art performance. GraphCL [35]\nfirstly introduces data augmentation strategies for GCL. AD-GCL [27] applies\nan adversarial graph augmentation strategy to avoid capturing redundant in-\nformation. Noticing the importance of augmentation views, RGCL [14] creates\nrationale-aware views for graph-level contrastive learning. As can be seen, exist-\ning graph data augmentation methods heavily rely on the choices of handcrafted\naugmentations, which may bring unstable performance or clumsy pipelines. Some\nlatest researches have been dedicating to explore the the non-necessity of graph\ndata augmentation. AF-GRL [13] develops an augmentation-free framework,\ngenerating views by exploiting nodes sharing the local structural information\nand global semantics. SimGCL [36] discards graph augmentations and creates\ncontrastive views by adding noises to embedding space. NCLA [23] uses aug-\nmented views automatically learned by graph attention mechanism. However,\nthose augmentation-free methods are designed for node-level task, while our\nproposed CGCL learns graph-level representations through a novel collabora-\ntive contrastive framework. Noting that NCLA, which uses multiple differently\nparameterized GATs to generate contrastive views, is actually a special case\nof CGCL. Since we consider using GNN-based graph encoders with different\nschemes rather than the same type of GNNs with different parameters.\n3\nMethodology\nIn this section, we elaborate on the proposed CGCL framework. CGCL employs\nmultiple graph encoders to embed the input graphs into various contrastive\nviews. These graph encoders are dynamically updated in a collaborative manner,\npredicated upon their respective contrastive losses.\n3.1\nFramework Overview\nIn Figure 2, we illustrate the overview of CGCL. Multiple graph encoders process\ninput graphs, yielding embeddings for each graph. Every graph encoder updates\nits parameters by contrasting its learned embeddings to the outputs from the\nother graph encoders. Specifically, the graph embeddings learned by Graph En-\ncoder i are utilized by Graph Encoder 1, 2, . . . , i −1, i + 1, k as contrastive views.\nAs shown in Figure 2, for example, Graph Encoder 1 uses embeddings learned\nby itself as query graphs and embeddings learned by all others as key graphs\nto compute Encoder 1 contrastive loss. After learning, each trained graph en-\ncoder can be utilized to extract graph latent representations from a graph when\npresented as an input.\n3.2\nGNN-based Graph Encoders\nGiven a set of graphs, CGCL needs to encode them into vectorized representa-\ntions. GNNs [28,12,8] have demonstrated their outstanding ability in encoding\n6\nT. Zhang et al.\nGraph\nEncoder 1\n…\nInput Graphs\nMinibatch\nEncoder 1 Graph\nRepresentations\nEncoder 1 Query Graphs\nGraph\nEncoder 2\nEncoder 2 Graph\nRepresentations\nGraph\nEncoder k\n…\nEncoder k Query Graphs\nEncoder k \nContrastive Loss\nEncoder 1 \nContrastive Loss\nEncoder k Graph\nRepresentations\n…\n…\n…\nAssembly\nFig. 2. Framework overview of CGCL. Graph Encoder 1, 2, · · · , k embedded the mini-\nbatch graphs into low-dimensional vectors. To optimize the framework collaboratively,\neach graph encoder calculates its own contrastive loss with the help of others.\ngraphs. In CGCL, we mainly employ GNNs as graph encoders. GNNs follow the\nrecursive neighborhood aggregation and certain message-passing scheme [32] to\nencode graphs. The aggregation process at the l-th layer of a GNN can be rep-\nresented as:\na(l)\nn = AGGREGATION (l) \u0010n\nh(l−1)\nn′\n: n′ ∈N(n)\no\u0011\n,\nh(l)\nn = COMBINE(l) \u0010\nh(l−1)\nn\n, a(l)\nn\n\u0011\n,\nhG = READOUT\n\u0010n\nh(l)\nn : vn ∈V, l ∈L\no\u0011\n(1)\nHere, h(l−1)\nn\nis the representation of node n at the (l −1)-th layer and h(0)\nn\nis the\ninitial feature. N(n) is the set of neighboring nodes of node n. The difference\nbetween each GNN is mainly reflected in the use of unique AGGREGATION(·)\nand COMBINE(·) functions, which define the message-passing schemes. For the\ngraph-level representation learning tasks, GNNs need an extra READOUT(·) to\nsummarize the representation of G from node representations.\nThe graph encoder candidates include GIN [32], GCN [12], GAT [28], DGCNN [37],\nand Set2Set [30] in this paper. Technically, any GNN working on graph-level\ntasks is viable as a graph encoder candidate. Graph encoders can be combined\nCGCL: Collaborative Graph Contrastive Learning\n7\ninto what we term the CGCL assembly, and subsequently, they can be trained\nby the collaborative contrastive learning framework\n3.3\nCollaborative Contrastive Learning\nAs described in Section 3.2, the proposed CGCL consists of multiple graph en-\ncoders. Naturally, the assembly of different types of graph encoders have a sig-\nnificant impact on the performance of CGCL. Therefore, in this section we first\nexplain the essence of collaborative framework. Subsequently, we delve into the\ntwo characteristics inherent to CGCL: an asymmetric structure and the com-\nplementary encoders. To augment our theoretical exposition, we introduce two\nquantitative metrics tailored to assess both the asymmetry and complementarity\nof the collaborative framework.\nEncoder Perspective for Generating Contrastive Views. In contrast to\nexisting GCL methods which use the same encoder to observe multiple aug-\nmented graphs, CGCL uses multiple encoders to observe the same graph and\ngenerate contrastive views. It is pivotal to recognize that the quintessence of con-\ntrastive learning is to learn invariance between different contrastive views incon-\nsistent to the original graph.This underscores that CGCL innovatively generates\ndifferent contrastive views from the encoder perspective, eschewing traditional\ndata augmentations. Given that variations in either encoder configurations or\ndata invariably manifest in the resultant embeddings, collaborative contrastive\nlearning parallels the efficacy of augmentation-based methods but is free from\nthe often cumbersome or resource-intensive augmentation phase. With the multi\ncontrastive views, encoders collaboratively interact. Each encoder’s output, serv-\ning as both query and key graphs, influences its inherent loss and those of its\ncounterparts. This process fosters a mutual distillation of knowledge on graph\nlatent representations.\nAsymmetric Architecture. In contrastive learning, model collapse presents a\nsignificant challenge. Existing methods, such as BYOL [6] and SimSiam [4], em-\nploy techniques like the Exponential Moving Average and Stop Gradient respec-\ntively to avoid model collapse. Those techniques can be concluded as introducing\nasymmetry into model architecture. Reflecting on CGCL, diverse GNN-based\ngraph encoders with distinct message-passing schemes are employed to ensure\nan asymmetric architecture. Essentially, the variance in these schemes introduces\nthe desired asymmetry. Thus, CGCL’s assembly necessitates multiple encoder\ntypes. It is noted that NCLA [23], by employing differently parameterized GATs\nas encoders, essentially adheres to a symmetric architecture, potentially predis-\nposing it to model collapse. Conversely, CGCL’s asymmetric design inherently\nmitigates such risks. As discussed in the above analysis, diverse encoders sustain\nthe asymmetry within CGCL’s assembly. Naturally, the extent of asymmetry\ncan be reflected on the differences between encoders. Thus, we introduce the\n8\nT. Zhang et al.\n…\n…\n…\n…\nEncoder 1\nEncoder 2\n1-cos𝑠𝑖𝑚𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(ℎ1, ℎ𝑁)\nℎ1\nℎ2\nℎ𝑁\n…\n…\nRDM1\n𝑟1\n𝑟2\n𝑟𝑁\n𝑐1 𝑐2\n𝑐𝑁\nRDM2\n…\nCorrelationSpearman(RDM1,RDM2)\nFig. 3. Calculation of correlation between RDMs. With k encoders, Asymmetry Coef-\nficient is calculated by averaging the correlation between any pair of encoders.\nRepresentational Dissimilarity Matrix (RDM), which is a prevalent tool in cog-\nnitive neuroscience [18]. Model discrepancies can be measured by calculating the\ncorrelation between two RDMs. For clarification, we illustrate the process of cal-\nculating the correlation between RDMs in Figure 3. Firstly, with k trained graph\nencoders and graph data with batch size N, we encode the data into k groups\nof batch embeddings. Then, for each batch, we calculate the cosine distance be-\ntween the pairwise embeddings. We can construct an N×N RDM based on the\npairwise distances. Finally, we calculate the correlation between those RDMs\npairwise. In this work, we use Spearman correlation to avoid the assumption of\na linear match between the RDMs. Based on the concept of RDM, we define the\nAsymmetry Coefficient of a certain CGCL’s assembly as follows:\nDefinition 1. Let k denote the number of trained encoders, and RDMD\ni\nbe the\nRepresentational Dissimilarity Matrix defined by encoder i and data D. Then\nthe Asymmetry Coefficient (AC) is formulated as:\nAC = 1 −1\nC2\nk\nX\n(i,j)∈Combination(k)\nCorrelation(RDMD\ni , RDMD\nj )\n(2)\nA high AC indicates that CGCL’s assembly holds high asymmetry and vice\nversa. As an illustrative extreme, if GNN-based encoders were homogeneous,\ntheir parameters would converge due to identical message-passing schemes and\nnetwork structures. With congruent trained parameters, RDMD\ni\nand RDMjD\nwould be roughly the same, resulting in a correlation nearing 1. Given this con-\ntext, the eventual AC would approach 0, underscoring the absence of asymmetry\nin an assembly comprising a singular encoder type.\nComplementary Encoders. Within CGCL, multiple graph encoders observe\ninput graphs to yield contrastive views. Ideally, these encoders should exhibit\nCGCL: Collaborative Graph Contrastive Learning\n9\ncomplementarity to enhance fitting capability. Specifically, an assembly with en-\ncoders possessing non-redundant observation angles demonstrates high comple-\nmentarity. Redundancies in observation angles can be inferred from overlapping\nencoder parameters. This notion of complementarity in CGCL mirrors the di-\nversity imperative of base learners in ensemble learning, where distinct learners\nbetter capture varied information. As discussed in Section 3.3, CGCL gener-\nates contrastive views from encoder perspective, distinguishing it from the data\naugmentations in traditional GCL methods. Inspired by [5], we introduce a loss-\ncentric metric to measure the complementarity of CGCL’s encoders. For clari-\nfication, we refer the training loss upon completion as the stopping loss. Given\na consistent dataset, a smaller stopping loss signifies enhanced assembly fitting\nability. This capability is directly proportional to the non-redundant parame-\nters across all encoders. As previously highlighted, complementarity is evident\nthrough non-redundant observation angles. With the above intuition, we define\nthe Complementarity Coefficient of a certain CGCL’s assembly as follows:\nDefinition 2. Let k denote the number of encoders, and LStop\ni\nbe the loss of\nencoder i when collaborative training is stopped. The Complementarity Coeffi-\ncient (CC) is given by:\nCC = −1\nk\nX\ni∈1,2,...,k\nLStop\ni\n(3)\nWith the above definition, CGCL’s encoders are more complementary with a\nhigher CC. In Equation (3), we introduce normalization to get rid of the effect\nof k.\nDiscussion. The assembly of CGCL is designed to exhibit both asymmetry\nand complementarity. For a more precise quantitative evaluation, we introduce\ntwo metrics tailored to assess these properties of the collaborative framework.\nImportantly, the computation of these metrics is solely based on pre-training\nstage and remains uninfluenced by downstream classifiers, rendering it a valuable\nguidance for assembly selection.\n3.4\nBatch-wise Contrastive Loss\nWe utilize the training batch to conduct contrastive learning. Given a minibatch\nof N graphs {G1, G2, . . . , GN} as the input of k graph encoders {M1, M2, . . . , Mk}.\nContrastive learning can be considered as learning an encoder for a dictionary\nlook-up task [9]. We take M1 as an example. The representations {hM1\nG1 , hM1\nG2 , . . . , hM1\nGN }\nlearned by M1 act as query graphs, while the representations learned by other\nencoders are key graphs. Between query and key graphs, the positive graph rep-\nresentation pairs are:\n{(hM1\nGi , hMj\nGi ) | i ∈[1, N], j ∈[2, k]}\n(4)\n10\nT. Zhang et al.\nAlgorithm 1 Collaborative Graph Contrastive Learning\nInput: The set of GNN-based graph encoders {Mi}k\ni=1; Batch size N; Temperature\nparameter τ\nOutput: Trained graph neural networks {Mi}k\ni=1;\n1: for all sampled graph minibatch {Gj}N\nj=1 do\n2:\nfor j ∈{1, . . . , N} do\n3:\nfor all Mi ∈{Mi}k\ni=1 do\n4:\nhMi\nGj = Mi(Gj)\n5:\nend for\n6:\nend for\n7:\nfor all Mp ∈{Mi}k\ni=1 do\n8:\nfor all Mq ∈{Mi}k\ni=1 \\ Mp do\n9:\nLMp(Mq) = −PN\nj=1 log\nexp\n\u0012\nh\nMp\nGj ·h\nMq\nGj /τ\n\u0013\nPN\nn=1 exp\n\u0012\nh\nMp\nGj ·h\nMq\nGn /τ\n\u0013\n10:\nend for\n11:\nLMp =\nX\nMq∈{Mi}k\ni=1\\Mp\nLMp(Mq)\n12:\nupdate graph encoder Mp to minimize LMp\n13:\nend for\n14: end for\n15:\n16: return graph neural networks {Mi}k\ni=1;\nTherefore, each graph encoded by M1 has k −1 positive samples. In essence, the\npositive graph pair indicates the representations of the same graph but learned\nby different graph encoders. Meanwhile, the other pairs are negative. Here, we\nutilize the InfoNCE [17] to calculate the contrastive loss for each graph encoder.\nWe define the contrastive loss between two graph encoders Mp and Mq first, so\nthe contrastive loss LMp(Mq) can be represented as:\nLMp(Mq) = −\nN\nX\nj=1\nlog\nexp\n\u0010\nhMp\nGj · hMq\nGj /τ\n\u0011\nPN\nn=1 exp\n\u0010\nhMp\nGj · hMq\nGn /τ\n\u0011\n(5)\nHere, τ is the temperature hyper-parameter. In this contrastive loss, Mp acts as\nthe query encoder, while Mq provides key graphs for contrast. When considering\nall collaborative graph encoders, the contrastive loss of Mp can be denoted as:\nLMp =\nX\nMq∈{Mi}k\ni=1\\Mp\nLMp(Mq)\n(6)\nMp can be updated based on the contrastive loss LMp through back-propagation.\nOther graph encoders follow the same process to compute their own contrastive\nloss and get updated by minibatch training iteratively. The detailed collaborative\nlearning process of CGCL is in Algorithm 1.\nCGCL: Collaborative Graph Contrastive Learning\n11\n4\nExperiments\nIn this section, we first introduce the datasets and baselines. To verify the ef-\nfectiveness of proposed CGCL, we conduct experiments to investigate following\nresearch questions:\n– RQ1: How does CGCL perform on unsupervised graph representation learn-\ning as compared to other state-of-art methods?\n– RQ2: How do the assembly of graph encoders impact the performance of\nCGCL concerning the aspects of asymmetry and complementarity\n– RQ3: How do the multiple graph encoders in CGCL converge in the collab-\norative learning process?\n4.1\nDatasets\nTo evaluate CGCL for graph-level representation learning, we conduct experi-\nments on 9 widely used benchmark datasets: NCI1, PROTEINS, D&D, MUTAG,\nCOLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI-\n5K. All of them come from the benchmark TUDataset [15].\n4.2\nBaselines\nFollowing [35], we select three categories of state-of-the-art methods as our\nbaselines: 3 kernel-based methods including Weisfeiler-Lehman subtree kernel\n(WL) [24], Graphlet Kernel (GK), Deep Graph Kernel (DGK) [33], 3 graph\nembedding-based methods including Sub2vec [1], Node2vec [7], Graph2vec [16]\nand 4 GCL methods including InfoGraph [26], GarphCL [35], AD-GCL [27],\nRGCL [14].\n4.3\nUnsupervised Graph Representation Learning (RQ1)\nWe first evaluate the representations learned by CGCL in the setting of unsu-\npervised learning. We follow the same process of InfoGraph [26], where repre-\nsentations are learned by models without any labels and then fed into a SVM\nto evaluate the graph classification performance. CGCLGIN denotes GIN [32]\ntrained under the proposed framework CGCL. The graph encoder candidates\ninclude GAT, GCN, and DGCNN, which are combined with GIN to conduct\ncollaborative contrastive learning. The representations learned by CGCLGIN\nare extracted for the downstream graph classification. We test the task 10 times\nand report the average accuracy and the standard deviation. In our experiments,\nonly two graph encoders or three graph encoders collaborative learning settings\nare used. The result of CGCLGIN we report in Table 1 is the best. From the\nresults in Table 1, CGCL achieves the best result on seven out of nine datasets.\nNoting that GraphCL and RGCL have better result on RDT-M, where average\nnode degree is extremely large compared to other datasets. On the contrary,\nthose two methods with data augmentations perform worse then the proposed\n12\nT. Zhang et al.\nTable 1. Performance on graph classification accuracy in the setting of unsupervised\nlearning. The best result is bolded.\nMethods\nNCI1\nPROTEINS\nD&D\nMUTAG\nCOLLAB\nWL\n76.65±1.99\n72.92±0.56\n76.44±2.35\n80.72±3.00\n-\nGK\n62.48±2.11\n72.23±4.49\n72.54±3.83\n81.66±2.11\n72.84±0.28\nDGK\n80.31±0.46 73.30±0.82\n71.12±0.21\n87.44±2.72\n73.09±0.25\nNode2vec\n54.89±1.61\n57.49±3.57\n67.12±4.32 72.63±10.20\n-\nSub2vec\n52.84±1.47\n53.03±5.55\n59.34±8.01 61.05±15.80\n-\nGraph2vec 73.22±1.81\n73.30±2.05\n71.98±3.54\n83.15±9.25\n-\nInfoGraph 76.20±1.06\n74.44±0.31\n72.85±1.78\n89.01±1.13\n70.65±1.13\nGraphCL\n77.87±0.41\n74.39±0.45\n78.62±0.40\n86.80±1.34\n71.36±1.15\nAD-GCL\n73.91±0.77\n73.28±0.46\n75.79±0.87\n88.74±1.85\n72.02±0.56\nRGCL\n78.14±1.08\n75.03±0.43\n78.86±0.48\n87.66±1.01\n70.92±0.65\nCGCLGIN\n77.89±0.54 76.28±0.31 79.37±0.47 89.05±1.42 73.28±2.12\nMethods\nIMDB-B\nIMDB-M\nRDT-B\nRDT-M\nWL\n72.30±3.44\n46.95±0.46\n68.82±0.41\n46.06±0.21\nGK\n65.87±0.98\n-\n77.34±0.18\n41.01±0.17\nDGK\n66.96±0.56\n44.55±0.52\n78.04±0.39 41.27± 0.18\nNode2vec\n61.03±7.13\n-\n-\n-\nSub2vec\n55.26±1.54\n36.67±0.83\n71.48±0.41\n36.68±0.42\nGraph2vec 71.10±0.54\n50.44±0.87\n75.78±1.03\n47.86±0.26\nInfoGraph 73.03±0.87\n49.69±0.53\n82.50±1.42\n53.46±1.03\nGraphCL\n71.14±0.44\n-\n89.53±0.84\n55.99±0.28\nAD-GCL\n70.21±0.68\n46.60±0.01\n90.07±0.85\n54.33±0.32\nRGCL\n71.85±0.84\n49.27±0.00\n90.34±0.58 56.38±0.40\nCGCLGIN 73.11±0.74 51.73±1.37 91.31±1.22 54.47±1.08\nCGCL on other datasets with a low average node degree. This observation is in\nline with our illustration in Figure 1, since when the graph is small and not dense\nenough, the perturbation from the data augmentation is more likely to damage\nthe invariance, e.g., a greater probability of affecting the graph connectivity by\ndropping a single node.\n4.4\nAsymmetry and Complementarity of CGCL’s Assembly (RQ2)\nFor a further exploration of CGCL’s working mechanism, we test various as-\nsembly with respect to the two quantitative metrics AC and CC. The candidate\nencoders include GIN, GCN, GAT and Set2Set. Specifically, we combine two\nor three of graph encoders on six datasets and use the best result of multiple\nencoders as this assembly’s result. Each point represents a certain assembly and\nits color indicates the performance. This experiment is implemented in the set-\nting of unsupervised graph representation learning. As illustrated in Figure 4,\nthe best assembly generally appears in the top right-hand corner, for example\npoint A. This indicates that the assembly with both high AC and CC generally\nperforms better, which further justifies the rationality of the design of asymmet-\nric architecture and complementary encoders for CGCL. Besides, the assembly\nCGCL: Collaborative Graph Contrastive Learning\n13\nA\nB\nE\nD\nC\nFig. 4. The performance of CGCL’s assembly with respect to asymmetry and comple-\nmentarity over multiple datasets. Point A indicates an example of the assembly with\na high AC and CC. While point B has a high AC and a low CC, point C is with a low\nAC and a high CC high. Point D an E refer to the examples whose AC and CC are\nboth low.\nholding only one property may fail the graph representation learning and result\nin the bad performance of downstream classification. For instance, the assembly\nin the bottom right-hand corner (point B) on PROTEINS has high asymmetry,\nhowever performs bad because of its very low complementarity. Another bad\nexample is the assembly in the top left-hand corner of IMDB-BINARY (point\nC). Though it has a high CC, the AC of this assembly is fairly low. In addition,\nthere also exist assembly, whose AC and CC are both at low levels, such as point\nD on NCI1 and point E on IMDB-MULTI. With the above analysis, we further\nconfirm the effectiveness of our proposed collaborative framework.\n4.5\nConvergence Analysis (RQ3)\nIn CGCL, multiple graph encoders compute their own contrastive losses based on\nrepresentations learned by others, and optimize their losses collaboratively. To\ncheck the reliability of collaborative mechanism, we empirically analyze the con-\nvergence in the optimization process of each individual encoder on PROTEINS\nand IMDB-BINARY. The assembly we use includes GIN, GCN and GAT. In\nFigure 5, we notice that each graph encoder converges synchronously on the two\ndatasets, which justifies our proposed collaborative learning framework. For a\nfurther analysis, we list the RDMs correlation between pairs of GIN, GCN and\nGAT in Table 2 for reference. According to the figure and the table, the trend\nof three encoders’ contrastive losses is in accord with their RDMs’ correlation of\neach other. In terms of PROTEINS, for example, the correlation between GIN\nand GCN is relatively high (0.8620), while the correlations between GAT and\nthose two are low and close (0.5732/0.6308). We observe that those relations\n14\nT. Zhang et al.\nTable 2. The RDMs Correlation between graph encoders in CGCLGIN/GCN/GAT on\nPROTEINS and IMDB-BINARY.\nRDMs Correlation PROTEINS IMDB-B\nGIN-GCN\n0.8620\n0.7164\nGIN-GAT\n0.5732\n0.6891\nGCN-GAT\n0.6308\n0.7038\nFig. 5. Empirical convergence study of different graph encoders in CGCL on PRO-\nTEINS and IMDB-BINARY.\ncorrespond well with the change of losses, which also exhibits the distinction of\nGAT. Such accordance further proves the rationality of Asymmetry Coefficient\nproposed in Section 3.3.\n5\nConclusion\nIn this study, we introduce CGCL, a novel collaborative graph contrastive learn-\ning framework, designed to address the invariance challenge encountered in cur-\nrent GCL methods. Unlike the conventional practice of constructing augmented\ngraphs by hand, CGCL employs multiple GNN-based encoders to generate mul-\ntiple contrastive views. This obviates the need for explicit structural augmenta-\ntion perturbations, thus ensuring invariance. Graph encoders of CGCL learn the\ngraph representations collaboratively, and enhance each other’s learning abil-\nity in an unsupervised manner. We then propose the concepts of asymmetric\nstructure and complementary encoders as the design principle for the collabora-\ntive framework. For a further theoretical analysis, we propose two quantitative\nmetrics to measure the asymmetry and complementarity of the collaborative\nframework. Extensive experiments substantiate the advantages of CGCL and\nunderscores the potential of collaborative framework in the field of GCL.\nCGCL: Collaborative Graph Contrastive Learning\n15\nReferences\n1. Adhikari, B., Zhang, Y., Ramakrishnan, N., Prakash, B.A.: Sub2vec: Feature learn-\ning for subgraphs. In: Pacific-Asia Conference on Knowledge Discovery and Data\nMining. pp. 170–182. Springer (2018)\n2. Chen, F., Wang, Y.C., Wang, B., Kuo, C.C.J.: Graph representation learning: a\nsurvey. APSIPA Transactions on Signal and Information Processing 9 (2020)\n3. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. In: International conference on machine\nlearning. pp. 1597–1607. PMLR (2020)\n4. Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition. pp.\n15750–15758 (2021)\n5. Gontijo-Lopes, R., Smullin, S.J., Cubuk, E.D., Dyer, E.: Affinity and diversity:\nQuantifying mechanisms of data augmentation. arXiv preprint arXiv:2002.08973\n(2020)\n6. Grill, J.B., Strub, F., Altch´e, F., et al.: Bootstrap your own latent-a new approach\nto self-supervised learning. Advances in neural information processing systems 33,\n21271–21284 (2020)\n7. Grover, A., Leskovec, J.: node2vec: Scalable feature learning for networks. In: Pro-\nceedings of the 22nd ACM SIGKDD international conference on Knowledge dis-\ncovery and data mining. pp. 855–864 (2016)\n8. Hamilton, W.L., Ying, R., Leskovec, J.: Inductive representation learning on large\ngraphs. arXiv preprint arXiv:1706.02216 (2017)\n9. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 9729–9738 (2020)\n10. Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,\nTrischler, A., Bengio, Y.: Learning deep representations by mutual information\nestimation and maximization. In: International Conference on Learning Represen-\ntations (2018)\n11. Ju, W., Fang, Z., Gu, Y., Liu, Z., Long, Q., Qiao, Z., Qin, Y., Shen, J., Sun, F.,\nXiao, Z., et al.: A comprehensive survey on deep graph representation learning.\narXiv preprint arXiv:2304.05055 (2023)\n12. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907 (2016)\n13. Lee, N., Lee, J., Park, C.: Augmentation-free self-supervised learning on graphs.\nIn: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp.\n7372–7380 (2022)\n14. Li, S., Wang, X., Zhang, A., Wu, Y., He, X., Chua, T.S.: Let invariant rationale dis-\ncovery inspire graph contrastive learning. In: International conference on machine\nlearning. pp. 13052–13065. PMLR (2022)\n15. Morris, C., Kriege, N.M., Bause, F., Kersting, K., Mutzel, P., Neumann, M.:\nTudataset: A collection of benchmark datasets for learning with graphs. arXiv\npreprint arXiv:2007.08663 (2020)\n16. Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y., Jaiswal,\nS.: graph2vec: Learning distributed representations of graphs. arXiv preprint\narXiv:1707.05005 (2017)\n17. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748 (2018)\n16\nT. Zhang et al.\n18. Popal, H., Wang, Y., Olson, I.R.: A guide to representational similarity analysis for\nsocial neuroscience. Social Cognitive and Affective Neuroscience 14(11), 1243–1253\n(2019)\n19. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n20. Ren, Y., Bai, J., Zhang, J.: Label contrastive coding based graph neural network\nfor graph classification. In: Database Systems for Advanced Applications. pp. 123–\n140. Springer International Publishing (2021)\n21. Ren, Y., Liu, B., Huang, C., Dai, P., Bo, L., Zhang, J.: Heterogeneous deep graph\ninfomax. arXiv preprint arXiv:1911.08538 (2019)\n22. Ren, Y., Wang, B., Zhang, J., Chang, Y.: Adversarial active learning based hetero-\ngeneous graph neural network for fake news detection. In: 2020 IEEE International\nConference on Data Mining (ICDM). pp. 452–461. IEEE (2020)\n23. Shen, X., Sun, D., Pan, S., Zhou, X., Yang, L.T.: Neighbor contrastive learning\non learnable graph augmentation. In: Proceedings of the AAAI Conference on\nArtificial Intelligence. vol. 37, pp. 9782–9791 (2023)\n24. Shervashidze, N., Schweitzer, P., Van Leeuwen, E.J., Mehlhorn, K., Borgwardt,\nK.M.: Weisfeiler-lehman graph kernels. Journal of Machine Learning Research\n12(9) (2011)\n25. Shervashidze, N., Vishwanathan, S., Petri, T., Mehlhorn, K., Borgwardt, K.: Ef-\nficient graphlet kernels for large graph comparison. In: Artificial intelligence and\nstatistics. pp. 488–495. PMLR (2009)\n26. Sun, F.Y., Hoffmann, J., Verma, V., Tang, J.: Infograph: Unsupervised and semi-\nsupervised graph-level representation learning via mutual information maximiza-\ntion. arXiv preprint arXiv:1908.01000 (2019)\n27. Suresh, S., Li, P., Hao, C., Neville, J.: Adversarial graph augmentation to improve\ngraph contrastive learning. Advances in Neural Information Processing Systems\n34, 15920–15933 (2021)\n28. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph\nattention networks. arXiv preprint arXiv:1710.10903 (2017)\n29. Veliˇckovi´c, P., Fedus, W., Hamilton, W.L., Li`o, P., Bengio, Y., Hjelm, R.D.: Deep\ngraph infomax. arXiv preprint arXiv:1809.10341 (2018)\n30. Vinyals, O., Bengio, S., Kudlur, M.: Order matters: Sequence to sequence for sets.\narXiv preprint arXiv:1511.06391 (2015)\n31. Xiao, T., Wang, X., Efros, A.A., Darrell, T.: What should not be contrastive in\ncontrastive learning. arXiv preprint arXiv:2008.05659 (2020)\n32. Xu, K., Hu, W., Leskovec, J., Jegelka, S.: How powerful are graph neural networks?\narXiv preprint arXiv:1810.00826 (2018)\n33. Yanardag, P., Vishwanathan, S.: Deep graph kernels. In: Proceedings of the 21th\nACM SIGKDD international conference on knowledge discovery and data mining.\npp. 1365–1374 (2015)\n34. Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., Leskovec, J.: Hierarchical\ngraph representation learning with differentiable pooling. In: Advances in neural\ninformation processing systems. pp. 4800–4810 (2018)\n35. You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., Shen, Y.: Graph contrastive learn-\ning with augmentations. Advances in Neural Information Processing Systems 33\n(2020)\n36. Yu, J., Yin, H., Xia, X., Chen, T., Cui, L., Nguyen, Q.V.H.: Are graph augmen-\ntations necessary? simple graph contrastive learning for recommendation. In: Pro-\nceedings of the 45th international ACM SIGIR conference on research and devel-\nopment in information retrieval. pp. 1294–1303 (2022)\nCGCL: Collaborative Graph Contrastive Learning\n17\n37. Zhang, M., Cui, Z., Neumann, M., Chen, Y.: An end-to-end deep learning architec-\nture for graph classification. In: Proceedings of the AAAI Conference on Artificial\nIntelligence. vol. 32 (2018)\n38. Zhou, Y., Zheng, H., Huang, X., Hao, S., Li, D., Zhao, J.: Graph neural networks:\nTaxonomy, advances, and trends. ACM Transactions on Intelligent Systems and\nTechnology (TIST) 13(1), 1–54 (2022)\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-11-05",
  "updated": "2024-04-01"
}