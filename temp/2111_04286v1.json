{
  "id": "http://arxiv.org/abs/2111.04286v1",
  "title": "Deep Unsupervised Active Learning on Learnable Graphs",
  "authors": [
    "Handong Ma",
    "Changsheng Li",
    "Xinchu Shi",
    "Ye Yuan",
    "Guoren Wang"
  ],
  "abstract": "Recently deep learning has been successfully applied to unsupervised active\nlearning. However, current method attempts to learn a nonlinear transformation\nvia an auto-encoder while ignoring the sample relation, leaving huge room to\ndesign more effective representation learning mechanisms for unsupervised\nactive learning. In this paper, we propose a novel deep unsupervised Active\nLearning model via Learnable Graphs, named ALLG. ALLG benefits from learning\noptimal graph structures to acquire better sample representation and select\nrepresentative samples. To make the learnt graph structure more stable and\neffective, we take into account $k$-nearest neighbor graph as a priori, and\nlearn a relation propagation graph structure. We also incorporate shortcut\nconnections among different layers, which can alleviate the well-known\nover-smoothing problem to some extent. To the best of our knowledge, this is\nthe first attempt to leverage graph structure learning for unsupervised active\nlearning. Extensive experiments performed on six datasets demonstrate the\nefficacy of our method.",
  "text": "Deep Unsupervised Active Learning on Learnable Graphs\nHandong Ma1, Changsheng Li2, Xinchu Shi3, Ye Yuan2, Guoren Wang2\n1 SCSE, University of Electronic Science and Technology of China, Chengdu, China\n2 School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China\n3 Meituan\nhandongma@std.uestc.edu.cn, {lcs, yuan-ye}@bit.edu.cn, shixinchu@meituan.com, wanggrbit@126.com\nAbstract\nRecently deep learning has been successfully applied to un-\nsupervised active learning. However, current method attempts\nto learn a nonlinear transformation via an auto-encoder while\nignoring the sample relation, leaving huge room to design\nmore effective representation learning mechanisms for unsu-\npervised active learning. In this paper, we propose a novel\ndeep unsupervised Active Learning model via Learnable\nGraphs, named ALLG. ALLG beneﬁts from learning opti-\nmal graph structures to acquire better sample representation\nand select representative samples. To make the learnt graph\nstructure more stable and effective, we take into account k-\nnearest neighbor graph as a priori, and learn a relation propa-\ngation graph structure. We also incorporate shortcut connec-\ntions among different layers, which can alleviate the well-\nknown over-smoothing problem to some extent. To the best\nof our knowledge, this is the ﬁrst attempt to leverage graph\nstructure learning for unsupervised active learning. Extensive\nexperiments performed on six datasets demonstrate the efﬁ-\ncacy of our method.\nIntroduction\nActive learning is an active research topic in machine learn-\ning and computer vision communities. Its goal is to choose\ninformative or representative samples to be labeled, so as\nto reduce the costs of annotating but guarantee the perfor-\nmance of the model trained on these labeled samples. Due to\nits huge potentiality, active learning has been widely applied\nto various tasks, such as image classiﬁcation [Wang et al.\n2016], recommendation systems [Elahi, Ricci, and Rubens\n2016], object detection [Aghdam et al. 2019], semantic seg-\nmentation [Siddiqui, Valentin, and Nießner 2020] and so on.\nUnsupervised active learning targets at selecting repre-\nsentative samples through taking advantage of structure in-\nformation of data. Currently, most unsupervised approaches\n[Yu, Bi, and Tresp 2006; Zhang et al. 2011; Zhu et al. 2015;\nShi and Shen 2016; Li et al. 2018a] intend to minimize the\ndata reconstruction loss with different structure regulariza-\ntion terms for selecting representative samples. These meth-\nods assume that each data point can be represented by a\nlinear combination of a selected sample subset, thus fail-\ning in modeling data with nonlinear structures. To remedy\nthis issue, a kernel based method [Cai and He 2011] incor-\nporates the manifold structure of data into the reproducing\nkernel Hilbert space (RKHS). More recently, deep learning\nhas been applied to solve the unsupervised active learning\nproblem [Li et al. 2020], named DUAL. DUAL attempts to\nnonlinearly map data into a latent space, and then performs\nsample selection in the learnt space.\nThe key to success for DUAL stems from learning a\nnonlinear transformation to obtain new feature representa-\ntions by leveraging deep learning. However, it does not ex-\nplicitly take advantage of the relation among samples dur-\ning representation learning. Recently, graph neural networks\n(GNN) have attracted much attention [Kipf and Welling\n2016; Veliˇckovi´c et al. 2017; Xu et al. 2018], where the\nsample relation has been proved to be helpful for learning\ngood sample representations. Thus, it ought to be beneﬁcial\nto sample selection, if we can leverage graph structure in-\nformation of data, and aggregate neighbor information of\nsamples to learn better representations. But another ques-\ntion arises: how to construct an optimal graph structure for\nnon-graph data still remains an open problem.\nBased on the above considerations, we propose a novel\ndeep unsupervised Active Learning model based on Learn-\nable Graphs, called ALLG. Speciﬁcally, ALLG ﬁrst uti-\nlizes an auto-encoder framework to map samples into a la-\ntent space. Without pre-deﬁning graph structure of samples,\nALLG devises a novel adjacent matrices learning module to\nautomatically learn an optimal graph structure among sam-\nples for jointly reﬁning sample representations and sam-\nple selection. In this module, we incorporate the k-nearest\nneighbor graph as a priori to learn a stable graph structure.\nConsidering that the relation among samples may happen\nto evolve as the sample representations change in differ-\nent network layers, we attempt to learn a series of relation\npropagated adjacent matrices, in the hope of capturing more\nprecise graph structures. Moreover, we add shortcut con-\nnections among different adjacent matrices learning layers,\nwhich can alleviate the well-known over-smoothing problem\nto some extent. Finally, a self-selection layer is employed to\nselect representative samples based on the learnt sample rep-\nresentation.\nThe contributions of this paper are summarized as:\n• ALLG builds a connection between unsupervised active\nlearning and graph structure learning. To the best of our\nknowledge, this is the ﬁrst attempt to leverage graph\nstructure learning for unsupervised active learning.\n• ALLG attempts to learn more precise sample representa-\narXiv:2111.04286v1  [cs.LG]  8 Nov 2021\nEncoder\nAdjacency Matrices \nLearning Layer\n⋮\nSelection Layer\n⋮\n⋯\nAdjacent Matrices Learning Module\nDecoder\nshortcut connection\n⋮\nk-nearest neighbor graph A!\nAdjacency Matrices \nLearning Layer\nrelation propagation\nFigure 1: The overall framework of ALLG. It mainly consists of four modules: An encoder and a decoder are used to learn\na nonlinear mapping. An adjacent matrix learning module aims to learn a relation propagation regularized graph structure for\nprecise sample representation. A shortcut connection is introduced to alleviate the over-smoothing problem. A self-selection\nlayer is used to select representative samples.\ntion by leveraging the graph, and devise a novel mecha-\nnism to dynamically learn a series of graph adjacent ma-\ntrices.\n• Inspired by the idea of residual learning, ALLG adds\nshortcut connections among different adjacent matrices\nlearning layers to alleviate the over-smoothing problem.\nExtensive experiments are performed on six publicly\navailable datasets, and experimental results demonstrate the\neffectiveness of ALLG, compared with the state-of-the-arts.\nRelated Work\nIn this section, we will brieﬂy review some work on unsu-\npervised active leaning and graph structure learning.\nUnsupervised Active Learning\nUnsupervised active learning has attracted much attention in\nrecent years. At the earlier stage, [Yu, Bi, and Tresp 2006]\npropose to utilize transductive experimental design (TED) to\nselect a sample subset, and obtain a greedy solution. ALNR\n[Hu et al. 2013] performs sample selection by consider-\ning the neighborhood relation of samples. RRSS [Nie et al.\n2013] proposes a convex formulation by introducing a struc-\ntured sparsity-inducing norm, and a robust sparse represen-\ntation loss. To select complementary samples, [Shi and Shen\n2016] proposes a diverse loss function that is an extension of\nTED. LSR [Li et al. 2017] was proposed via local structure\nreconstruction to select representative data points. ALFS [Li\net al. 2018a] builds a connection between unsupervised ac-\ntive learning and feature selection, and proposes a convex\nformulation to select samples and features simultaneously.\nMost recently, owing to the powerful representation ca-\npability and great success of deep learning, deep model\nhas been explored to solve the unsupervised active learn-\ning problem [Li et al. 2020]. In [Li et al. 2020], the authors\nutilize deep auto-encoders to embed data to a latent space,\nand then select the most representative samples to best re-\nconstruct both the whole dataset and clustering centroids.\nAlthough deep learning based methods have achieved im-\npressive results, they ignore the relation among data points\nduring sample representation learning and thus have the in-\nferior results.\nGraph Structure Learning\nFor the sake of applying GNNs to non-graph structured data,\nmany graph structure learning methods have been proposed\nin recent years. [Dong et al. 2016; Egilmez, Pavez, and Or-\ntega 2017] explore to learn the graphs from data without as-\nsociating it with the downstream tasks. More recently, [Li\net al. 2018b; Choi et al. 2019; Liu et al. 2019; Chen, Wu,\nand Zaki 2019] aim to dynamic construct graphs towards the\ndownstream tasks. However, these methods are task-speciﬁc\nones which depend on the supervised information.\nDifferent from these models, we propose to optimize the\nlearning of graphs and active learning simultaneously in an\nunsupervised manner.\nMethod\nThe overall architecture of our method is illustrated in Fig-\nure 1. Our network mainly consists of the following compo-\nnents: an encoder and decoder module is used to learn a non-\nlinear transformation. An adjacent matrix learning module\naims to learn multiple optimal adjacent matrices and lever-\nage them for learning compact sample representation, which\nis the core module of our method. A self-selection module\nattempts to select representative samples. Before introduc-\ning these modules in detail, we ﬁrst give some notations.\nLet X = [x1, x2, · · · , xn] ∈Rd×n denote a data matrix,\nwhere xi(1<i<n) is the data point. d and n are the dimen-\nsion and number of data points respectively. Our goal is to\nlearn a nonlinear transformation by considering the sample\nrelation to learn better sample representations. To this end,\nwe ﬁrst learn a latent space with a deep auto-encoder. We\nthen attempt to learn the graph structure of the data in the\nlatent space, and leverage it for learning a good representa-\ntion. Based on the learnt representation, we can select the\nmost representative samples via a self-selection layer. We\nattempt to optimize them in a joint framework.\nEncoder and Decoder\nIn order to learn a nonlinear transformation Θ, we utilize an\nauto-encoder architecture to map the data into a latent space,\nbecause of its effectiveness in unsupervised learning.\nIn our framework, the encoder consists of L fully con-\nnected layers. The output of the l-th layer in the encoder is\ndeﬁned as:\nz(l)\ni\n= σ(W(l)z(l−1)\ni\n+ b(l)),\ni = 1, ..., n\n(1)\nwhere z(0)\ni\n= xi denotes the i-th original training data in X,\nwhich is used as the input of the encoder block. W(l) and\nb(l) are the weights and bias associated with the l-th hid-\nden layer respectively. σ(·) is a nonlinear activation func-\ntion. Then, we can deﬁne the latent representation as:\nZL = Θ(X) = [Θ(x1), Θ(x2), · · · , Θ(xn)] ∈Rd′×n (2)\nwhere d′ denotes the dimension of the latent representation.\nAs for the decoder, it learns another nonlinear mapping\nto reconstruct the original data, which guides the training of\nthe encoder. The decoder has a symmetric structure with the\nencoder, which consists of L fully connected layers as well.\nThen, the reconstruction loss of auto-encoder is deﬁned\nas:\nLr =\nn\nX\ni=1\n||xi −¯xi||2\n2 = ||X −¯X||2\nF ,\n(3)\nwhere ¯X denotes the reconstruction of X. Actually, the input\nX plays a role of self-supervisor to guide the learning of\nauto-encoder.\nGraph Structure Learning\nAfter obtaining the latent representation ZL of the input X,\nwe intend to learn the graph structure of the input X for\ngenerating more precise sample representation.\nAdjacent Matrix Learning: As aforementioned, taking ad-\nvantage of the relation among data points can have a posi-\ntive effect on sample representation learning, which has been\nveriﬁed in graph neural networks (GNNs) [Wu et al. 2020].\nHowever, many GNN algorithms are developed to deal with\ngraph data, and assume that the adjacent relationship is\ngiven, which can not be directly applied to non-graph data.\nTo deal with it, there are some algorithms which attempt to\nconstruct a human estimated graph structure, e.g., k-nearest\nneighbor graph. However, such methods can not guarantee\nthe graph structure is optimal. Based on these considera-\ntions, we aim to learn a data-driven optimal graph structure.\nConsidering that some human estimated graph structure can\nstill reveal some priori information, we can integrate such in-\nformation into our framework to regularize the graph struc-\nture learning. In all, we propose the following regularization\nterms to learn the graph structure:\nLa = α||A1||2\nF + β||A1 −A0||2\nF ,\n(4)\nwhere A0 denotes the k-nearest neighbor graph, and A1 de-\nnotes the learnt graph, i.e., the adjacent matrix, α and β are\npositive trade-off parameters.\nIn Eq.(4), the ﬁrst regularization term aims to reduce the\ncomplexity of the learnt adjacent matrix. The second term\nimposes a constraint on A1, making it not deviate from the\nk-nearest neighbor graph A0, such that the prior information\ncan be integrated. One can control how close the learnt adja-\ncent matrix to the priori by modifying the parameter β. Ac-\ntually, the graph adjacent matrix A1 can be regarded as the\nparameters of a fully connected layer without bias. There-\nfore, it can be updated through a standard back-propagation\nprocedure during training.\nAfter obtaining the adjacent matrix A1, we can obtain the\nnew sample representations S1 based on ZL as:\nS1 = σ(ZLA1),\n(5)\nwhere σ(·) is a nonlinear activation function. Based on the\nabove equation, we can see that the new sample representa-\ntion S1 can be obtained based upon a linear combination of\nZL with the weight matrix A1, thereby the relation among\nsamples can be incorporated into the process of representa-\ntion learning.\nRelation Propagation: It is worth noting that only learn-\ning one adjacent matrix may be sub-optimal for learning the\ngraph structure of data, since the representations of samples\nare from different layers during the process of training. In\norder to learn more stable and effective optimal graph struc-\nture, we intend to learn multi-level adjacent matrices with a\nrelation regularized term. In order to enable the learning of\nmultiple adjacent matrices, we propose another regulariza-\ntion terms as:\nLp =\nN\nX\nl=2\n(α′||Al||2\nF + β′||Al −Al−1||2\nF ),\n(6)\nwhere Al denotes the learnt adjacent matrix of the l-th ma-\ntrix learning layer, α′ and β′ are positive trade-off parame-\nters.\nIn Eq.(6), the ﬁrst term aims to lower the complexities of\nthe learnt adjacent matrices, while the second term utilizes\nthe former learnt adjacent matrix to regularize the latter ad-\njacent matrix. By this means, the relation among samples\ncan be smoothly propagated.\nAfter learning multiple adjacent matrices, we can obtain\nanother sample representation for sample selection as:\nSi+1 = σ(SiAi), i = 1, 2, · · · , N −1\n(7)\nwhere Si denotes the latent sample representations in the i-\nth graph Laplacian layer.\nShortcut Connection: In GNNs, there is a well known\nproblem, i.e., over-smoothing. The over-smoothing problem\nmeans that repeated graph Laplacian eventually make node\nembeddings indistinguishable. Empirically, a shortcut con-\nnection would bring more discriminative features from the\nformer layers to alleviate this problem. The shortcut con-\nnection is illustrated in Figure 2. Mathematically, it can be\ndescribed as:\nSout = rSk + (1 −r)SN,\n(8)\nwhere r is a trade-off parameter to control the contribution\nof the k-th graph structure learning layer to the ﬁnal sample\nrepresentation.\nAdjacent Matrices Learning Layers\n⋯\nshortcut connection\nFigure 2: The short connection in ALLG\nSelf-Selection Layer\nAfter obtaining the ﬁnal latent sample representations, we\nperform sample selection by introducing a self-selection\nlayer, as shown in Figure 1.\nSelf-selection layer takes the output of the adjacent matrix\nlearning layers as the input. In order to select samples to best\nreconstruct all ones, we use the loss function presented as:\nLs =\nn\nX\nl=1\n||Sout −SoutQ||2\n2 + λ||Q||∞,1\n(9)\nwhere Q ∈Rn×n is the reconstruction coefﬁcients for the\nsamples. ||Q||∞,1 = Pn\ni=1 ||qi||∞, where qi is the i-th row\nvector of Q and || · ||∞denotes the sup-norm of a vector,\ndeﬁned as ||a||∞= max\n1≤i≤n |ai|.\nThe ﬁrst term in Eq. (9) aims to pick out m samples to\nreconstruct the whole dataset in the latent space, while the\nsecond term is a regularization term to enforce the coefﬁ-\ncient matrix Q row-sparse. To minimize Eq. (9), similarly, Q\ncan be regarded as the parameters of a fully connected layer\nwithout bias and nonlinear activations, and solved jointly\nthrough a standard back-propagation procedure.\nAfter the inputs passing by the self-selection layers, we\nthen feed them into the decoder as its inputs as:\n¯X0 = SoutQ ∈Rd′×n\n(10)\nAlgorithm 1: Optimization Procedure for ALLG\nInput: The matrix X ∈Rd×n\nParameter: trade-off parameters α, β, λ\nOutput: Q\n1: Calculate the k-nearest neighbor graph A0 with X.\n2: Pre-train an auto-encoder network which has the same\narchitecture with the encoder and decoder block of the\nproposed framework via a standard back-propagate al-\ngorithm on X.\n3: Initialize the encoder and decoder block of the proposed\nframework using the pre-trained network parameters.\n4: while not converged do\n5:\nUpdate the whole network by minimizing the loss\nfunction in Eq.(11).\n6: end while\nOverall Model and Training\nBased on Eq. (3), (4), (6), and (9), the ﬁnal loss function is\nLtotal = Lr + La + Lp + Ls\n(11)\nTo jointly optimize Eq.(11), we use a two-stage training\nstrategy following [Li et al. 2020]: Firstly, we only pre-train\nthe encoder-decoder without considering the matrices learn-\ning module and self-selection layer, minimizing the loss in\nEq. (3). By this means, we obtain good initial parameters for\nﬁne-tuning the whole model. Speciﬁcally, three fully con-\nnection layers are used in the encoder, and the decoder has\na symmetric structure with it. After that, we update all the\nparameters by minimizing Eq.(11) through a standard back-\npropagation procedure. The ReLU is used in our method as\nthe activations. And we optimize Eq. (11) with the Adam\noptimizer where we set the learning rate to 1.0 × 10−3. In\nthe ﬁnal model, ALLG utilizes 2 adjacent matrices learning\nlayers. As for the shortcut connection, we set the k = 1 and\nr = 0.3 in Eq.(8) during training. The hyperparameters α\nand α′ are set to be the same so as the β and β′ during train-\ning. The optimization procedure for ALLG can also be seen\nin Algorithm.1\nPost Processing\nOnce the model converges, we can get the parameter Q\nbased on the self-selection layer. As Q is row sparse after\ntraining, the row values of Q can be regarded as the contri-\nbutions of each data point to reconstruct other data points.\nThus, we calculate the ℓ2-norm of the rows and sort them in\ndescending order to get a rank of samples. Then we can se-\nlect the top-m data points as the most representative samples\nto be labeled.\nExperiments\nIn this section, we will introduce the experimental results to\ndemonstrate the effectiveness of the proposed method. We\nalso conduct some ablation study and analysis on the pro-\nposed ALLG.\n0\n50\n100\n150\n200\n250\n#Query\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nAccuracy\nDCS\nKMeans\nRRSS\nMAED\nALNR\nALFS\nDUAL\nALLG\n(a) Splice-junction\n0\n50\n100\n150\n200\n250\n#Query\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy\nDCS\nKMeans\nRRSS\nMAED\nALNR\nALFS\nDUAL\nALLG\n(b) Plant Species Leaves\n0\n100\n200\n300\n400\n500\n#Query\n0.65\n0.7\n0.75\n0.8\n0.85\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nALFS\nDUAL\nALLG\n(c) Waveform\n0\n200\n400\n600\n800\n1000\n#Query\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nALFS\nDUAL\nALLG\n(d) ESR\n0\n200\n400\n600\n800\n1000\n#Query\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nDUAL\nALLG\n(e) GSAD\n0\n500\n1000\n1500\n2000\n#Query\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nDUAL\nALLG\n(f) Letter Recognition\nFigure 3: The performance comparisons of different active learning methods combined with SVM on six benchmark.\nDatasets\nSize\nDimension\nClass\nSplice-junction\n1000\n60\n2\nPlant Species Leaves\n1600\n64\n100\nWaveform\n5000\n40\n3\nESR\n11500\n178\n5\nGSAD\n13910\n128\n6\nLetter Recognition\n20000\n16\n10\nTable 1: Details of experimental datasets.\nExperimental Setting\nDataset: To demonstrate that datasets from different do-\nmains can beneﬁt from ALLG, we conduct experiments on\nsix publicly available datasets from different domains, and\nthe details of the datasets are summarized in Table 11. In\nthe datasets, ”Splice-junction” and ”ESR” are from biol-\nogy, while ”ESR” is present as time-series. ”Plant Species\nLeaves” and ”Letter Recognition” are generated from im-\nages. ”Waveform” is a physical dataset, while ”GSAD” is\nfrom sensors utilized in simulations.\nBaseline2: We compare our method with several typical un-\nsupervised active learning algorithms, including RRSS [Nie\net al. 2013], ALNR [Hu et al. 2013], MAED [Cai and He\n1These datasets are all downloaded from the UCI Machine\nLearning Repository: https://archive.ics.uci.edu/ml/datasets.php.\n2All source codes are obtained from the authors of the corre-\nsponding papers, except K-means and ALNR.\n2011], ALFS [Li et al. 2018a], DUAL [Li et al. 2020]. We\nalso compare with a matrix column subset selection algo-\nrithm, deterministic column sampling (DCS) [Papailiopou-\nlos, Kyrillidis, and Boutsidis 2014], which can be used for\nunsupervised active learning. In addition, we take K-Means\nas another baseline, in which we choose the samples which\nare closest to the cluster centers as the most representative\nsamples, and we set K = 5 in experiments.\nExperimental protocol: Following [Li et al. 2018a] and\n[Li et al. 2020], we randomly select 50% of the samples as\nthe candidate set and the rest is testing set. Different active\nlearning algorithms are performed on the same candidate set\nto query the most representative m samples. To verify the\nquality of samples selected by these methods, we train two\nclassiﬁers by using these selected samples as the training\ndata: a SVM classiﬁer with a linear kernel and C = 100, as\nwell as a Logistic Regression (LR) classiﬁer. We search the\ntrade-off parameters in our algorithm from {0.1, 1, 10}. For\na fair comparison, the parameters of RRSS, ALNR, MAED,\nALFS, DUAL are all searched from the same space. Each\nexperiment is run ﬁve times, and the result is reported in\nterms of the average accuracy.\nExperimental Result\nGeneral Performance: Figure 3 and Figure 4 show the re-\nsults of different methods combined with a SVM classiﬁer\nand a LR classiﬁer respectively. We can observe that ALLG\noutperforms all other baselines in almost all queries. We use\n0\n50\n100\n150\n200\n250\n#Query\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nAccuracy\nDCS\nKMeans\nRRSS\nMAED\nALNR\nALFS\nDUAL\nALLG\n(a) Splice-junction\n0\n50\n100\n150\n200\n250\n#Query\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy\nDCS\nKMeans\nRRSS\nMAED\nALNR\nALFS\nDUAL\nALLG\n(b) Plant Species Leaves\n0\n100\n200\n300\n400\n500\n#Query\n0.7\n0.75\n0.8\n0.85\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nALFS\nDUAL\nALLG\n(c) Waveform\n0\n200\n400\n600\n800\n1000\n#Query\n0.14\n0.16\n0.18\n0.2\n0.22\n0.24\n0.26\n0.28\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nALFS\nDUAL\nALLG\n(d) ESR\n0\n200\n400\n600\n800\n1000\n#Query\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nDUAL\nALLG\n(e) GSAD\n0\n500\n1000\n1500\n2000\n#Query\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\nAccuracy\nDCS\nKMeans\nMAED\nALNR\nDUAL\nALLG\n(f) Letter Recognition\nFigure 4: The performance comparisons of different active learning methods combined with Logistic Regression on six bench-\nmark.\n# of Query\n25\n50\n75\n100\n125\n150\n175\n200\n225\nAverage\nKmeans\n0.6616\n0.6856\n0.6692\n0.6688\n0.6820\n0.6884\n0.6960\n0.6876\n0.7080\n0.6830\nDCS\n0.6360\n0.6720\n0.6632\n0.6808\n0.6884\n0.6920\n0.6916\n0.7096\n0.7384\n0.6857\nRRSS\n0.5844\n0.6516\n0.6712\n0.6564\n0.6680\n0.6832\n0.6952\n0.7052\n0.7124\n0.6697\nMAED\n0.6544\n0.6720\n0.6709\n0.6725\n0.6641\n0.6632\n0.6689\n0.7116\n0.7180\n0.6772\nALNR\n0.6472\n0.6552\n0.6396\n0.6368\n0.6744\n0.6824\n0.6992\n0.7272\n0.7336\n0.6772\nALFS\n0.6248\n0.6744\n0.6804\n0.6840\n0.7072\n0.7104\n0.7136\n0.7284\n0.7416\n0.6960\nDUAL O\n0.6552\n0.6740\n0.6976\n0.7088\n0.7012\n0.7168\n0.7212\n0.7184\n0.7380\n0.7034\nDUAL\n0.6524\n0.6688\n0.7064\n0.7148\n0.7200\n0.7456\n0.7504\n0.7600\n0.7620\n0.7200\nALLG O\n0.7212\n0.7328\n0.7484\n0.7540\n0.7612\n0.7680\n0.7720\n0.7744\n0.7772\n0.7565\nALLG\n0.7132\n0.7484\n0.7640\n0.7660\n0.7756\n0.7848\n0.7892\n0.7948\n0.7968\n0.7703\nTable 2: Ablation study on sample representation. The bold text indicates the best results and the underlined text indicates the\nsecond best results.\ntwo different classiﬁers to illustrate that the quality of se-\nlected samples by ALLG is agnostic to classiﬁers. It is worth\nnoting that ALLG achieves marked improvement compared\nto deep learning based DUAL, about 3% average improve-\nment on different numbers of query. It’s veriﬁed that learn-\ning graphs of data can really be positive to sample selec-\ntion. Note that we do not perform ALFS and RRSS on larger\ndatasets, because of their unaffordable computational com-\nplexities.\nAblation Study: We perform ablation study on the Splice-\njunction dataset to gain further understanding of the pro-\nposed method. The experimental setting is as:\n• Training Classiﬁers Using the Original Features: For\nALLG and DUAL, they embed samples to a latent space,\nand use the new representation to train classiﬁers. To\neliminate the inﬂuence of new representations, we use\noriginal features to train classiﬁers after obtaining the se-\nlected samples by ALLG and DUAL. We denote them\nas ALLG O and DUAL O respectively. The results are\nshown in Table 2. In general, ALLG O and DUAL O still\nachieve better performance than other methods. Mean-\nwhile, ALLG O has a better result than DUAL O, which\nveriﬁes the effectiveness of our method once again. In\naddition, ALLG is better than ALLG O, and DUAL\nachieves better performance than DUAL O. This illus-\n# of Query\n25\n50\n75\n100\n125\n150\n175\n200\n225\nAverage\nw/o smoothing\n0.6486\n0.7126\n0.7313\n0.7320\n0.7540\n0.7540\n0.7526\n0.7606\n0.7686\n0.7349\nALLG knn\n0.6800\n0.7160\n0.7055\n0.7225\n0.7415\n0.7535\n0.7725\n0.7695\n0.7855\n0.7384\nALLG one\n0.6900\n0.7195\n0.7385\n0.7540\n0.7730\n0.7805\n0.7780\n0.7820\n0.7840\n0.7555\nALLG ts\n0.6820\n0.7430\n0.7380\n0.7430\n0.7540\n0.7820\n0.7890\n0.7840\n0.7850\n0.7555\nALLG td\n0.7145\n0.7430\n0.7505\n0.7630\n0.7735\n0.7715\n0.7855\n0.7970\n0.7940\n0.7658\nALLG\n0.7132\n0.7484\n0.7640\n0.7660\n0.7756\n0.7848\n0.7892\n0.7948\n0.7968\n0.7703\nTable 3: Ablation study on graph structure learning. The bold text indicates the best results.\ntrates that learning a nonlinear representation can be\nhelpful for active learning.\n• Adjacent Matrices Learning Module: We also verify\nthe effectiveness of the graph structure learning module,\ni.e., adjacent matrix learning. We denote: 1) w/o smooth-\ning: ALLG trained without graph structure learning; 2)\nALLG knn: ALLG using k-nearest neighbor graph as the\nonly adjacent matrix; 3) ALLG one: ALLG trained with\none learnt adjacent matrix; 4) ALLG ts: ALLG trained\nwith two learnt adjacent matrices but forcing them to be\nthe same; 5) ALLG td: ALLG trained with two differ-\nent learnt adjacent matrices (i.e., it will become ALLG\nwhen shortcut connections are added). The results are re-\nported in Table 3. We ﬁnd that ALLG knn achieves better\nresults than w/o smoothing, illustrating that leveraging\ngraph structure of data is good for learning a better sam-\nple representation. ALLG one is better than ALLG knn,\ndemonstrating that learning a graph structure can achieve\nsuperior results, compared to a human estimated one.\nALLG td outperforms ALLG one and ALLG ts. This il-\nlustrates that learning multiple different adjacent matri-\nces can be beneﬁcial to representation learning. Finally,\nALLG beats ALLG td, showing that shortcut connection\nis effective for unsupervised active learning.\nParameter Study: We study the sensitivity of our algorithm\nin terms of the trade-off parameters λ, α, β on the Splice-\njunction dataset. We ﬁx the number of queries to 125. The\nresults are shown in Figure 5. Our method is insensitive to\nthe parameters with a relatively wide range.\n0,1\n1\n10\n0.6\n0.8\n1\nAccuracy\n0,1\n1\n10\n0.6\n0.8\n1\nAccuracy\n0,1\n1\n10\n0.6\n0.8\n1\nAccuracy\nFigure 5: Parameter study on Splice-junction dataset.\nConvergence Analysis: We further show the convergence\ncurves of ALLG on the Splice-junction dataset. The results\nare shown in Figure 6. As the number of epoch increases, the\ntotal loss and the supnorm loss of Q are gradually decreased\nuntil convergent. From the ﬁgure, the algorithm converges\nwhen the epoch reaches to around 2000.\n0\n250\n500\n750 1000 1250 1500 1750 2000\nEpoch\n(a) Total loss\n0\n250\n500\n750 1000 1250 1500 1750 2000\nEpoch\n(b) Supnorm loss of Q\nFigure 6: Convergence analysis of ALLG\nFigure 7: Visualization by t-SNE on Splice-junction dataset.\nThe red circles denote the selected samples, and other color\nsolid circles denote different classes.\nVisualization: We use t-SNE [Maaten and Hinton 2008]\nto visualize sample selection on the Splice-junction dataset\nshown in Figure7, the selected samples are distributed uni-\nformly, and can better represent the whole dataset.\nConclusion\nIn this paper, we proposed a novel unsupervised active learn-\ning model by learning optimal graph structures of data. A\nnovel adjacent matrices learning module was devised to\nlearn a series of optimal graph structures of data through\nrelation propagation regularization without pre-deﬁning the\ngraph structures. In the meantime, we took k-nearest neigh-\nbor graph prior to assist in graph structure learning. Fi-\nnally, we utilized a shortcut connection to alleviate the over-\nsmoothing problem. Experimental results demonstrated the\neffectiveness of our method.\nReferences\nAghdam, H. H.; Gonzalez-Garcia, A.; Weijer, J. v. d.; and\nLopez, A. M. 2019. Active Learning for Deep Detection\nNeural Networks. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV).\nCai, D.; and He, X. 2011. Manifold adaptive experimental\ndesign for text categorization. TKDE, 24(4): 707–719.\nChen, Y.; Wu, L.; and Zaki, M. J. 2019. Deep iterative and\nadaptive learning for graph neural networks. arXiv preprint\narXiv:1912.07832.\nChoi, E.; Xu, Z.; Li, Y.; Dusenberry, M. W.; Flores, G.; Xue,\nY.; and Dai, A. M. 2019. Graph convolutional transformer:\nLearning the graphical structure of electronic health records.\narXiv preprint arXiv:1906.04716.\nDong, X.; Thanou, D.; Frossard, P.; and Vandergheynst, P.\n2016.\nLearning Laplacian matrix in smooth graph signal\nrepresentations. IEEE Transactions on Signal Processing,\n64(23): 6160–6173.\nEgilmez, H. E.; Pavez, E.; and Ortega, A. 2017. Graph learn-\ning from data under Laplacian and structural constraints.\nIEEE Journal of Selected Topics in Signal Processing, 11(6):\n825–841.\nElahi, M.; Ricci, F.; and Rubens, N. 2016. A survey of ac-\ntive learning in collaborative ﬁltering recommender systems.\nComputer Science Review, 20: 29–50.\nHu, Y.; Zhang, D.; Jin, Z.; Cai, D.; and He, X. 2013. Active\nlearning via neighborhood reconstruction. In IJCAI, 1415–\n1421.\nKipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nﬁcation with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nLi, C.; Ma, H.; Kang, Z.; Yuan, Y.; Zhang, X.-Y.; and Wang,\nG. 2020. On Deep Unsupervised Active Learning. In Pro-\nceedings of the Twenty-Ninth International Joint Conference\non Artiﬁcial Intelligence, IJCAI-20, 2626–2632.\nLi, C.; Wang, X.; Dong, W.; Yan, J.; Liu, Q.; and Zha, H.\n2018a. Joint active learning with feature selection via cur\nmatrix decomposition. IEEE transactions on pattern analy-\nsis and machine intelligence, 41(6): 1382–1396.\nLi, Q.; Shi, X.; Zhou, L.; Bao, Z.; and Guo, Z. 2017. Active\nlearning via local structure reconstruction. Pattern Recogni-\ntion Letters, 92: 81–88.\nLi, R.; Wang, S.; Zhu, F.; and Huang, J. 2018b.\nAdap-\ntive graph convolutional neural networks.\narXiv preprint\narXiv:1801.03226.\nLiu, P.; Chang, S.; Huang, X.; Tang, J.; and Cheung, J.\nC. K. 2019. Contextualized non-local neural networks for\nsequence learning. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 33, 6762–6769.\nMaaten, L. v. d.; and Hinton, G. 2008. Visualizing data us-\ning t-SNE. Journal of machine learning research, 9(Nov):\n2579–2605.\nNie, F.; Hua, W.; Huang, H.; and Ding, C. 2013. Early active\nlearning via robust representation and structured sparsity. In\nIJCAI, 1572–1578.\nPapailiopoulos, D.; Kyrillidis, A.; and Boutsidis, C. 2014.\nProvable deterministic leverage score sampling. In Proceed-\nings of the 20th ACM SIGKDD international conference on\nKnowledge discovery and data mining, 997–1006.\nShi, L.; and Shen, Y.-D. 2016. Diversifying convex trans-\nductive experimental design for active learning. In IJCAI,\n1997–2003. AAAI Press.\nSiddiqui, Y.; Valentin, J.; and Nießner, M. 2020. ViewAL:\nActive Learning with Viewpoint Entropy for Semantic Seg-\nmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9433–9443.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y. 2017. Graph attention networks. arXiv\npreprint arXiv:1710.10903.\nWang, K.; Zhang, D.; Li, Y.; Zhang, R.; and Lin, L. 2016.\nCost-effective active learning for deep image classiﬁcation.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 27(12): 2591–2600.\nWu, Z.; Pan, S.; Chen, F.; Long, G.; Zhang, C.; and Philip,\nS. Y. 2020. A comprehensive survey on graph neural net-\nworks. IEEE Transactions on Neural Networks and Learn-\ning Systems.\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2018.\nHow powerful are graph neural networks? arXiv preprint\narXiv:1810.00826.\nYu, K.; Bi, J.; and Tresp, V. 2006. Active learning via trans-\nductive experimental design. In Proceedings of the 23rd in-\nternational conference on Machine learning, 1081–1088.\nZhang, L.; Chen, C.; Bu, J.; Cai, D.; He, X.; and Huang, T. S.\n2011. Active learning based on locally linear reconstruction.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 33(10): 2026–2038.\nZhu, F.; Fan, B.; Zhu, X.; Wang, Y.; Xiang, S.; and Pan,\nC. 2015. 10,000+ times accelerated robust subset selection.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 29.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-11-08",
  "updated": "2021-11-08"
}