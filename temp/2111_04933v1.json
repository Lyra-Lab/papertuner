{
  "id": "http://arxiv.org/abs/2111.04933v1",
  "title": "DSBERT:Unsupervised Dialogue Structure learning with BERT",
  "authors": [
    "Bingkun Chen",
    "Shaobing Dai",
    "Shenghua Zheng",
    "Lei Liao",
    "Yang Li"
  ],
  "abstract": "Unsupervised dialogue structure learning is an important and meaningful task\nin natural language processing. The extracted dialogue structure and process\ncan help analyze human dialogue, and play a vital role in the design and\nevaluation of dialogue systems. The traditional dialogue system requires\nexperts to manually design the dialogue structure, which is very costly. But\nthrough unsupervised dialogue structure learning, dialogue structure can be\nautomatically obtained, reducing the cost of developers constructing dialogue\nprocess. The learned dialogue structure can be used to promote the dialogue\ngeneration of the downstream task system, and improve the logic and consistency\nof the dialogue robot's reply.In this paper, we propose a Bert-based\nunsupervised dialogue structure learning algorithm DSBERT (Dialogue Structure\nBERT). Different from the previous SOTA models VRNN and SVRNN, we combine BERT\nand AutoEncoder, which can effectively combine context information. In order to\nbetter prevent the model from falling into the local optimal solution and make\nthe dialogue state distribution more uniform and reasonable, we also propose\nthree balanced loss functions that can be used for dialogue structure learning.\nExperimental results show that DSBERT can generate a dialogue structure closer\nto the real structure, can distinguish sentences with different semantics and\nmap them to different hidden states.",
  "text": "DSBERT:Unsupervised Dialogue Structure learning\nwith BERT\nBingkun Chen†, Shaobing Dai†∗, Yang Li†, Shenghua Zheng†, and Lei Liao †\n†QingNiuZhiSheng Technology Company, China, {chenbk, daisb, liyang, zhengsh, liaolei}@qnzsai.com\nAbstract—Unsupervised dialogue structure learning is an im-\nportant and meaningful task in natural language processing. The\nextracted dialogue structure and process can help analyze human\ndialogue, and play a vital role in the design and evaluation\nof dialogue systems. The traditional dialogue system requires\nexperts to manually design the dialogue structure, which is very\ncostly. But through unsupervised dialogue structure learning,\ndialogue structure can be automatically obtained, reducing the\ncost of developers constructing dialogue process. The learned\ndialogue structure can be used to promote the dialogue generation\nof the downstream task system, and improve the logic and\nconsistency of the dialogue robot’s reply.In this paper, we propose\na Bert-based unsupervised dialogue structure learning algorithm\nDSBERT (Dialogue Structure BERT). Different from the previous\nSOTA models VRNN and SVRNN, we combine BERT and\nAutoEncoder, which can effectively combine context information.\nIn order to better prevent the model from falling into the\nlocal optimal solution and make the dialogue state distribution\nmore uniform and reasonable, we also propose three balanced\nloss functions that can be used for dialogue structure learning.\nExperimental results show that DSBERT can generate a dialogue\nstructure closer to the real structure, can distinguish sentences\nwith different semantics and map them to different hidden states.\nIndex Terms—dialogue structure, bert, autoencoder, balance\nloss\nI. INTRODUCTION\nD\nIalogue structure is very helpful for downstream tasks\nsuch as dialogue system design [1], dialogue analysis [2]\nand dialogue summary [3] [4]. At present, the dialog structure\nin dialog system often requires language experts to manually\ndesign based on relevant professional knowledge, which takes\na lot of time and manpower. Automatic learning dialogue\nstructure by unsupervised learning can reduce the cost of\ndesign systems and support a wide variety of dialogue. The\ndialog structure is typically composed of a probability transfer\nmatrix of latent states, and each utterance pairs of dialogue\n(including user utterances and system utterances) belongs to\na latent state, which has an impact on future status sequences\nand conversations. Fig 1 shows the Original dialogue structure\nof dialogue from SimDial dataset [5] and Fig 2 shows a\ndialogue example in the SimDial dataset.\nA large number of previous research has studied unsu-\npervised methods to model the latent dialog structure. Early\nstudy based on hidden markov model (HMM) algorithm [7]\n[8] [9], usually combined HMM with language model and\ntopic model. Such algorithms can capture the dependencies\nin the conversation through HMM, but lack the ability to\n* Shaobing Dai is the corresponding author.\ngreeting\nrequest bus\nrequest#from_loc\ninform#from_loc\nrequest#to_loc\ninform#to_loc\nrequest#datetime\ninform#datetime\n0.174\n0.174\n0.331\n0.152\n0.152\nquery\nkb_return\ninform#default\nmore_request#duration\ninform#duration\nsatisfy\ngoodbye\nsilence\ninform#default\nmore_request#arrive_in\ninform#arrive_in\nsatisfy\ninform#default\nsatisfy\n(a) Bus information request.\ngreeting\nrequest weather\nrequest#loc\ninform#loc\nrequest#datetime\ninform#datetime\n0.167\n0.167\n0.319\n0.168\n0.168\nquery\nkb_return\ninform#default\nmore_request#temperature\ninform#temperature\nsatisfy\ngoodbye\nsilence\ninform#default\nmore_request#weather_type\ninform#weather_type\nsatisfy\ninform#default\nsatisfy\n(b) Weather information request.\nFigure 1: Original dialogue structure of SimDial dataset [6]\nand the user’s actions are marked in bold.Figure (a) is the\nbus information request data and Figure (b) is the weather\ninformation request data. query and kb return indicate the\nsystem API call and the return result in dialog.\ndiscriminate semantic complex sentences. The model based\non variational recurrent neural network is the most commonly\nused unsupervised dialogue structure learning algorithm, such\nas VRNN [10], SVRNN [11]. VRNN uses VAE as a cell of\nrecurrent neural network (RNN) and each VAE cell includes\nencoder and decoder. VRNN maps each utterance pair into\nlatent state via encoder and restores the original sentence\naccording to the latent state by decoder. SVRNN incorporates\nstructured attention layers into VRNN network to enable the\narXiv:2111.04933v1  [cs.CL]  9 Nov 2021\nmodel to focus on different portions of the conversation,\nresulting in a more reasonable dialogue structure than VRNN.\nsys: \tAsk me about bus information. How can I help?\nusr: \tHi. I need a bus.\nsys: \tWhat's the departure place?\nusr: \tAt Craig.\nsys: \tWhere do you want to take off?\nusr: \tGoing to Lawrance.\nsys: \tWhat time do you need the bus?\nusr: \tDeparture time is 9.\nsys: \t{\"QUERY\": {\"#to_loc\": \"Lawrance\", \"#from_loc\":  \n         \"Craig\", \"#datetime\": \"9\"}, \"GOALS\": [\"#default\"]}\nusr: \t{\\RET\\: {\"#default\": \"137\"}} \nsys: \tBus 137 can take you there. What else can I do\nusr: \tNot done yet. How long will it take?\nsys: \t{\"QUERY\": {\"#to_loc\": \"Lawrance\", \"#from_loc\": \n         \"Craig\", \"#datetime\": \"9\"}, \"GOALS\": [\"#duration\"]}\nusr: \t{\"RET\": {\"#duration\": \"45\"}} \nsys: \tThe ride is 45 minutes long. What else can I do?\nusr: \tNo more questions. Thank you.\nsys: \tGoodbye.\nusr: \t<SILENCE>\nutterance pair x1\nFigure 2: A dialog from the bus information request dataset in\nSimDial. QUERY and RET are the system API call and result\nreturn respectively\n.\nAlthough VRNN and SVRNN have a good ability to learn\ndialogue structure, there are still some problems. First, the\nloss function is not very targeted. The loss function of VRNN\nand SVRNN includes three parts, KL-divergence loss between\ntransition probabilities, loss function of language model and\nloss function of bag-of-word model. The kl divergence loss\nfunction measures the difference between the prior state dis-\ntribution and the predicted state distribution, but both of these\nprobabilities are generated by the model, which has weak\nconstraints on the model. When training language model, the\nlatent state is used as the initial state of RNN, and then next\nword is predicted based on previous word. This process model\ncan easily predict the next word directly from previous word,\nwhich weakens the ability to learn latent state. Second, VRNN\nand SVRNN lack necessary constraints and are easy to fall\ninto local optimum. During our experiment, we found that\nVRNN and SVRNN sometimes fall into local optimum. Many\nutterance with completely different semantics are assigned\nto same latent state. In serious cases, all utterance are only\nassigned to 2 3 latent states.\nIn this paper, we propose DSBERT (Dialogue Structure\nBERT) model, using pre-trained language model BERT [12] as\nthe Encoder and Decoder. Bert has strong language representa-\ntion and feature extraction ability, which can make our model\nlearn semantic information effectively and make our model\nstructure more concise and clear. In order that the latent state\ngenerated by DSBERT is highly discriminative and contains\nsufﬁcient semantic information, we directly use the latent state\nto reconstruct each round of utterance. At the same time, in\norder to prevent the unsupervised algorithm from falling into\nlocal optimum and to enhance the robustness of the model, we\npropose three balanced loss functions, so that the utterance can\nbe reasonably distributed in different latent states. We also use\nTF-IDF to extract the keywords of each utterance and concat\nthe keywords with utterance to make the model more stable\nduring the initial training.\nOur contributions include: (1) We propose a BERT-based\nunsupervised dialogue structure learning framework. The\nmodel structure is more concise and effective than VRNN and\ncan better integrate contextual information to learn dialogue\nsemantics. (2) We propose three balanced loss functions that\ncan be used for dialogue structure learning, which can effec-\ntively prevent the model from falling into local optimal so-\nlution and make utterances reasonably distributed in different\nstates. (3) We propose a scheme of adding keywords to make\nthe preliminary process of unsupervised dialogue structure\nlearning more stable.\nII. RELATED WORK\nIn the early research work of dialogue structure learning, a\ncommon way is to learn dialogue structure based on artiﬁcially\nlabeled data [13]. On the one hand, this method requires a lot\nof person and time to annotate data. On the other hand, the\nrequirements of users are very diverse and manually labeled\ndata cannot satisfy all users. Therefore, it is meaningful to\nstudy unsupervised dialogue structure learning and automat-\nically extract semantic structure from massive unsupervised\ndialogue data. Most of the early researches on unsupervised\ndialogue structure learning were based on hidden markov\nmodel (HMM). Ritter et al. Proposed the ﬁrst unsupervised\ndialogue action learning method for open domain data [8].\nThe model adds additional word information to the interactive\ndata and combines the conversation model and topic model\non HMM. Zhai and Williams proposed three unsupervised\nmethods for learning the dialogue structure of task-oriented\ndialogue [9]. The HMM model is used to model hidden state\nand the topic model is used to connect words and states.\nHowever, these models are based on simple HMM and lack\nthe ability to model highly non-linear, while highly non-linear\nneural network models can capture more semantic information\nand more dynamics in dialogue.\nNowadays, in the ﬁeld of unsupervised dialogue structure\nlearning, neural network-based models are more advanced.\nVariational autoencoders (VAE) [14] [15] [16] is widely used\nin various natural language processing tasks because of its in-\nterpretable model structure and good generation ability. Chung\net al. combined VAE with RNN and proposed a variational\nrecurrent neural network (VRNN) [17]. VRNN can connect\nneural networks with traditional bayesian methods and has a\nnonlinear output at every moment. Shi et al. proposed using a\nmodiﬁed VRNN model with discrete latent vectors to extract\nthe semantic structure of task-oriented dialogue [10]. Assum-\ning that the dialogue structure is composed of a series of latent\nstates. Each utterance of dialogue belongs to a latent state,\nand latent state will affect state and content of the utterance\n2\nBERT Encoder\n\nSoftmax\n\n\n\n\n......\n......\n......\nBERT Decoder\n......\n[CLS]\nsys0\n[SEP]\nusr0\n[unused0]\n[SEP]\nsys0\n[SEP]\nusr0\n[SEP]\n[unused1]\n......\n[SEP]\n[CLS]\nsys0\n[SEP]\nusr0\n[unused0]\n[SEP]\nsys0\n[SEP]\nusr0\n[SEP]\n[unused1]\n......\n[SEP]\n\nSoftmax\np0\nz0\nargmax\nSoftmax\np1\nz1\nargmax\np2\nz2\nargmax\nFigure 3: The structure of DSBERT\nat the next moment. Qiu et al. combined structured attention\non VRNN and proposed SVRNN [11]. Structured attention\nenables model to focus on utterance representations at different\nmoments. SVRNN can learn not only semantic structures but\nalso interactive structures by choosing appropriate structural\nbiases or constraints.\nRecently, models based on transformer [18] have achieved\nstate-of-the-art (SOTA) results in various natural language\nprocessing tasks. Unlike RNN, transformer processes context\ninformation in parallel through the self-attention structure.\nThe pre-trained language model BERT [12] is based on the\ntransformer’s encoder, and the model is pre-trained through\ntwo self-supervised training tasks, Masked LM and Next\nSentence Prediction (NSP). Then the pre-trained model can be\nﬁne-tuned in downstream tasks and achieve impressive results.\nIn order to allow BERT to handle longer sequences, many\nBERT variants for long sequences have also been proposed,\nsuch as Longformer [19], Big Bird [20]. These methods use\nthe Sparse attention mechanism to reduce the time complexity\nof self-attention to linear.\nThe DSBERT framework we propose is based on BERT\nmodel. A BERT encoder is used to convert the dialogue into\nlatent states, and then a BERT decoder is used to restore the\nentire dialogue according to latent states. DSBERT can be\ncombined with different BERT variant models. For example,\nDSBERT can use Longformer as Encoder and Decoder when\nfacing long text. We compared DSBERT and other unsuper-\nvised dialogue structure extraction algorithms and found that\nDSBERT can generate a more reasonable dialogue structure\nand is more stable. It shows that DSBERT has a strong ability\nto learn the dependency relationship between utterances, and\nhas a strong ability to distinguish different semantic sentences.\nIII. DSBERT\nThe structure of DSBERT is shown in Fig 3. It is an\nEncoder-Decoder structure and consists of two BERT models.\nEncoder predicts the latent state of each utterance in dialogue,\nand then decoder uses the latent states to restore the original\ndialogue. Through DSBERT, the state sequence z1, z2, ..., zn\nof the dialog can be obtained, and the state transition proba-\nbility matrix can be estimated through the state sequences. We\nwill describe more details about the key components of our\nmodel in the following subsections.\nA. Problem Deﬁnition\nGiven a corpus D = {X1, X2, ..., X|D|} that containing |D|\ntask-oriented dialogues, each dialogue X = [x1, x2, ..., xt] is\ncomposed of a sequence of utterance pairs, the utterance pair\nat moment i includes system utterance si and user utterance ui,\nxi = [si, ui]. Fig 2 shows a dialog from the bus information\nrequest dataset in SimDial [6]. Our goal is to predict the latent\nstate zi for each utterance pair in the dialog. and then based\non the status of all dialogs The state sequence [z1, z2, ..., zt]\nconstructs the ﬂow of the entire task-based dialogue. We\ncan also construct the entire task-oriented dialogue structure\naccording to the state sequence of all dialogues.\nB. Encoder\nSuppose we have a dialogue X = [x1, x2, ..., xt], xi =\n[si, ui], DSBERT needs to predict latent state of each utterance\npair and obtain the state sequence Z = [z1, z2, ..., zt] of the\nentire dialogue Z = [z1, z2, ..., zt]. DSBERT uses BERT as\nencoder to obtain latent state distribution. We concatenate\nall utterances of the dialog session in order, and use the\nconcatenated token sequence as the input of BERT. There\nis a special token used to predict latent state before each\nutterance pair. For the ﬁrst utterance special token is CLS, and\n3\nthe remaining utterances use unused0, unused1,... as special\ntokens. As shown in the Equation 1.\ninput = [CLS, s1, u1, SEP, unused0, s2, u2,\nSEP, ..., unused(t −1), st, ut, SEP]\n(1)\nBERT can get the embedding matrix E ∈RL×d of the input\nsequence, where L is the total length of the input sequence\nand d is the BERT output feature dimension. Then we use\nthe fully connected neural network to project the embedding\nmatrix E into the latent state space to obtain the feature matrix\nΦ ∈RL×nstate. We take out the feature [φ0, φ1, ..., φt−1]\ncorresponding to [CLS, unused0, unused1, ..., unused(t −2)]\nfrom Φ, and get the probability distribution [p0, p1, ..., pt−1]\nfor each utterance pair by the softmax function. Therefore, the\nstate zi corresponding to utterance pair xi is the state with the\nhighest probability in pi, as shown in the Equation 2.\nΦ = Feed −Forward(E)\npi = softmax(φi)\nzi = argmax\nn\n[pi(n)]\n(2)\nC. Decoder\nIn the previous section, we obtained the feature vectors\nφ0, φ1, ..., φt−1 of special token before each utterance. To\nensure that these feature vectors can retain the information\nof dialogue, we use another BERT model acts as decoder to\nrestore the entire dialogue based from φ0, φ1, ..., φt−1. The\nfeature vector φi corresponding to utterance xi is copied li\ntimes, and li is the number of tokens in utterance xi. After\ncopying, we can get a new feature matrix ˜Φ ∈RL×nstate. Then\nwe apply Gumbel-Softmax [21] on ˜Φ to obtain discrete latent\nstate matrix ˜P ∈RL×nstate. Finally, we use fully connected\nnetwork to obtain the input embedding matrix ˜E ∈RL×h\nof decoder and generate the original dialogue through the\ndecoder. h is the dimension of the BERT token embedding.\nAs shown in the Equation 3.\n˜Φ = [φ0; ...; φ0; ...; φt−1; ...; φt−1]\n˜P = gumbel −softmax(˜Φ)\n˜E = Feed −Forward( ˜P)\n(3)\nThe encoder of DSBERT only uses the features of special\ntoken to predict the latent states, so the decoder also uses the\nfeatures corresponding to special token to restore the dialogue\ninformation. The encoder of DSBERT only uses the features\nof special token to predict the latent states. In the decode stage,\nwe also only use the features of special token to restore the\ndialogue. We only use special token to restore dialogue mainly\nfor the following reasons: First, this can strengthen the ability\nof the special token to store dialogue information. Second,\nwhen training with language model as VRNN, the model can\ndirectly predict the next word based on the previous word\nwithout using the latent state, which weakens the training of\nthe latent state.\nD. Loss\nThe loss function of DSBERT is composed of two parts:\nMask Language Model Loss and a Balance Loss. Balance\nLoss is used to ensure the latent state distribution balance.\nWe ﬁrst introduce Mask Language Model Loss. DSBERT uses\nthe decoder to predict each token of the original dialogue, and\nthen calculates the cross entropy, denoted as LMLM, as shown\nin the equation 4, and θ means The parameter of decoder, ˜E\nis the input of Decoder, and |V | represents the dictionary size.\nLMLM = −\nL\nX\ni=1\nlog p(m = wj|θ, ˜E)\nwj ∈[1, 2, ..., |V |]\n(4)\nAssuming that the batch of data contains M dialogues,\nthe number of utterance pairs contained in these dialogues\nis {u0, u1, ..., uM−1}, the total number of utterance pairs\ncontained in this batch Is U = u0 + u1 + ... + uM−1. We can\nget the probability that utterance pair belongs to a latent state\nthrough encoder, the probability matrix is P ∈RU×nstate. In\nactual use, a model trained only through the language model\nloss is easy to fall into local optimal value. For example, all\ndialogues are distributed in a few latent states, and it is even\npossible that all dialogues are allocated to one latent state. This\nphenomenon also occurs in VRNN and SVRNN. We hope that\nthe probability matrix P should be as balanced as possible\nin the latent state. Therefore, in addition to Mask Language\nModel Loss, a Balance Loss must be used to avoid the situation\nwhere all dialogues are marked as a few latent states at the\nsame time. We propose Three balanced loss functions.\n(1) Balance & KL loss. We use Balance Regularization\nfunction mentioned in [22] to make P more balanced, the\nequation is ||P||b = Pnstate\nj=1\n(PU\ni=1 pij)\n2. But we found\nin experiment that simply using Balance Regularization will\neasily make P become a uniform distribution, so we also\nneed to combine Balance Regularization and KL divergence.P\nis the predicted probability distribution, and a target matrix\nT\n∈RU×nstate needs to be constructed to calculate the\nKL divergence. We select the latent state with the highest\nprobability for each row of P, then change the position with\nthe highest probability in T to 1, and the remaining positions\nto 0. Balance & KL Loss is shown in the equation 5.\nLbalance kl = ||P||b + KL(T||P)\n(5)\n(2) Greedy Balance Loss. We use greedy algorithm to\nconstruct the target matrix Tgreedy ∈RU×nstate according to\nthe probability distribution matrix P ∈RU×nstate. For each\ncolumn j of matrix P, ﬁnd the row i with the highest probabil-\nity, set Tgreedy(i, j) to 1, and then remove the corresponding\nrow from P. Repeat the above process until each line of P\nhas been processed. Pseudo code. After obtaining the target\nmatrix Tgreedy, calculate the KL divergence of Tgreedy and P,\nas shown in equation 6. We only use Greedy Balance Loss in\nthe ﬁrst few epochs to make the model obtain an ideal initial\ndistribution.\n4\nLgreedy balance = KL(Tgreedy||P)\n(6)\n(3) Top Balance Loss. For each column of the probability\nmatrix P, ﬁnd the row with highest probability. The number\nof columns in P is nstate, so nstate row will be found. Then\ntake out these rows to form a new probability matrix P ′ ∈\nRnstate×nstate, and construct the corresponding target matrix\nT ′ ∈Rnstate×nstate. Finally, calculate the KL divergence of\nT ′ and P ′, as shown in equation 7.\nLtop balance = KL(T ′||P ′)\n(7)\nThe loss function of DSBERT is combination of Mask\nLanguage Model Loss and any Balance Loss. As shown in\nequation 8, λ is the coefﬁcient that controls the strength of\nbalance loss.\nL = LMLM + λLbalance\n(8)\nAlgorithm 1: Target matrix T of Balance & KL loss\nData: Probability matrix P ∈RU×nstate\nResult: Target matrix T ∈RU×nstate\nr ←t;\n∆B∗←−∞;\nwhile ∆B ≤∆B∗and r ≤T do\nQ ←arg maxQ≥0 ∆BQ\nt,r(It−1, Bt−1);\n∆B ←∆BQ\nt,r(It−1, Bt−1)/(r −t + 1);\nif ∆B ≥∆B∗then\nQ∗←Q;\n∆B∗←∆B;\nr ←r + 1;\nE. Extract Keywords\nIn the case of unsupervised, sometimes it is not easy for\nthe model to distinguish sentences with different semantics.\nIn order to allow the model to better capture the differences\nof each utterance at the beginning of training, we propose to\nextract k keywords from utterance and concatenate them in\nfront of the utterance. We use TF-IDF to extract keywords,\ntreat each utterance as a corpus, calculate the corresponding\nTF-IDF vector, and then take the k words with the highest\nscores in TF-IDF vector as keywords.\nIV. EXPERIMENTS\nA. Datasets\nWe use SimDial English task-oriented dialogue dataset [6]\nand RiSAWOZ Chinese task-oriented dialogue dataset [23]\nto verify the performance of DSBERT. The SimDial dataset\ncontains four task: bus, weather, movie, restaurant. For each\nutterance pair in SimDial data (including system utterance\nand user utterance), we use the system action and user action\nprovided by the dataset as true labels of the corresponding\nutterance pair, as shown in Figure 2.\nthe table I shows the statistical information of the SimDial\ndata set. We also manually annotated the RiSAWOZ Chinese\ntask-based dialogue dataset. The statistics of the RiSAWOZ\ndataset are shown in the table II. Table I shows the information\nof SimDial dataset. We also manually annotated the RiSAWOZ\nChinese task-oriented dialogue dataset. The information of the\nRiSAWOZ dataset is shown in table II.\nB. Baselines\nWe choose the following ﬁve unsupervised dialogue struc-\nture learning methods as Baselines.\n(1) HMM. HMM is a traditional algorithm for modeling\nhidden states and uses topic models to connect words and\nstates.\n(2) K-Means. We use K-Means to cluster utterance pairs,\nand the cluster id obtained is the latent state of corresponding\nutterance pair.\n(3) D-VRNN [10]. VRNN obtains other utterance infor-\nmation through the RNN structure and maps the utterance to\nlatent state through VAE.\n(4) DD-VRNN [10]. DD-VRNN is similar to D-VRNN, but\nthe method to calculate the state transition prior is different.\n(5) SVRNN [11]. SVRNN is the current SOTA for unsu-\npervised task-oriented dialogue structure learning. It adds a\nstructured attention mechanism to VRNN.\nC. Evaluation Metrics\nWe use the two evaluation metrics [11]: Structure Euclidean\nDistance (SED) and Structure Cross-Entropy (SCE) to mea-\nsure the performance of the model. It should be noted that\nthese two evaluation metrics require the true dialogue structure\nlabels of data. For SimDial and RiSAWOZ, we have already\nmarked their true labels.\nSuppose there are M latent states learned by the model,\nwhich are {s′\ni, i = 1, 2, ..., M}, and there are N true latent\nstates from the dataset, which are {si, i = 1, 2, ..., N}. The\nstate sequence of all dialogues can be learned through the\nmodel, and then the predicted state transition probability\nmatrix T ′ ∈RM×M can be calculated by counting the\noccurrence probability of bi-grams in the state sequence, where\nT ′\nij =\n(s′\ni,s′\nj)\n(s′\ni) . According to the true state sequence, the real\nstate transition probability matrix T ∈RN×N can also be\ncalculated.\nThen we need to calculate the mapping probability matrix\nPsi,s′\ni ∈RN×M between the real state and the predicted state.\nIt can be calculated through dividing the number of utterances\nthat have the ground truth state si and learned state s′i by\nnumber of utterances with the ground truth state si. A similar\nmethod can also be used to calculate the mapping probability\nmatrix Ps′\ni,si ∈RM×N between the predicted state and the\nreal state.\nTo calculate SED and SCE, the state transition probability\nmatrix T ′ needs to be mapped to the real state and obtain the\ntransition probability matrix T ′′ ∈RN×N, see the equation 9.\n5\nData\nTurns(min)\nTurns(max)\nTurns(mean)\nTokens(min)\nTokens(max)\nTokens(mean)\nLabels\nBus\n7\n13\n8.04\n59\n150\n87.96\n14\nWeather\n6\n10\n7.05\n45\n131\n75.46\n12\nMovie\n7\n13\n9.01\n57\n156\n95.24\n22\nRestaurant\n6\n12\n7.99\n48\n146\n89.67\n20\nTABLE I: SimDial Dataset\nData\nTurns(min)\nTurns(max)\nTurns(mean)\nTokens(min)\nTokens(max)\nTokens(mean)\nLabels\nTourist attractions\n3\n11\n5.3\n77\n437\n187.53\n-\nhospitals\n4\n11\n5.79\n101\n474\n212.71\n-\ncomputers\n4\n13\n7.87\n136\n508\n256.32\n-\ncounseling classes\n4\n11\n6.29\n116\n448\n220.22\n-\nTABLE II: RiSAWOZ Dataset\nT ′′\nsa,sb =\nX\ni,j∈{1,2,...,M}\nPsa,s′\ni · T ′\ns′\ni,s′\nj · P ′\ns′\nj,sb\n(9)\nSED and SCE are calculated by the equation 10:\nSED = 1\nN\ns\nX\na,b∈{1,2,...,N}\n(T ′′\nsa,sb −Tsa,sb)2\nSCE = 1\nN\nX\na,b∈{1,2,...,N}\n−log(T ′′\nsa,sb)Tsa,sb\n(10)\nD. Extracted Structure\nExtracted structure.\nE. Result\nWe compare the performance of DSBERT and various base-\nlines on the SimDial English dataset and RiSAWOZ Chinese\ndataset. The experimental results of SimDial are shown in\nTable III.\nV. CONCLUSION\nIn this paper, we analyze the performance differences\nbetween various pre-trained language models ﬁne-tuned on\nquestion answering datasets of varying levels of difﬁculty.\nFurther, we propose an ensemble model and compare its\nperformance to other models. Experimental results show the\neffectiveness of our methods and shows that RoBERTa and an\nauxiliary BiLSTM layer both improve model performance in\nquestion answering. We see the highest F1-score on RoBERTa\nand BART model across all datasets. We also observe at least\na 1% increase in F1-score over the BERT base model when\na BiLSTM layer is added on. Future work includes extending\nour model to incorporating additional attention mechanisms\nand potentially utilizing the MatchLSTM architecture to create\na better performing ensemble model.\nVI. ACKNOWLEDGEMENTS\nThis work was supported in part by the Department of\nDefense and the Army Educational Outreach Program.\nREFERENCES\n[1] S. Young, “Using pomdps for dialog management,” in Spoken Language\nTechnology Workshop, 2007.\n[2] B. Grosz and C. L. Sidner, “Attention, intentions, and the structure of\ndiscourse,” Computational linguistics, 1986.\n[3] G. Murray, S. Renals, and J. Carletta, “Extractive summarization of\nmeeting recordings.” 2005.\n[4] J. Liu, S. Seneff, and V. Zue, “Dialogue-oriented review summary\ngeneration for spoken dialogue recommendation systems,” in Human\nLanguage Technologies: The 2010 Annual Conference of the North\nAmerican Chapter of the Association for Computational Linguistics,\n2010, pp. 64–72.\n[5] T. Zhao and M. Eskenazi, “Zero-shot dialog generation with cross-\ndomain latent actions,” arXiv preprint arXiv:1805.04803, 2018.\n[6] ——, “Zero-shot dialog generation with cross-domain latent actions,”\n2018.\n[7] A. Chotimongkol, “Learning the structure of task-oriented conversations\nfrom the corpus of in-domain dialogs,” Ph.D. dissertation, Carnegie\nMellon University, Language Technologies Institute, School of ..., 2008.\n[8] A. Ritter, C. Cherry, and W. B. Dolan, “Unsupervised modeling of\ntwitter conversations,” in Human Language Technologies: The 2010\nAnnual Conference of the North American Chapter of the Association\nfor Computational Linguistics, 2010, pp. 172–180.\n[9] K. Zhai and J. D. Williams, “Discovering latent structure in task-\noriented dialogues,” in Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\n2014, pp. 36–46.\n[10] W. Shi, T. Zhao, and Z. Yu, “Unsupervised dialog structure learning,”\narXiv preprint arXiv:1904.03736, 2019.\n[11] L. Qiu, Y. Zhao, W. Shi, Y. Liang, F. Shi, T. Yuan, Z. Yu, and S.-C. Zhu,\n“Structured attention for unsupervised dialogue structure induction,”\narXiv preprint arXiv:2009.08552, 2020.\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[13] D. Jurafsky, “Switchboard swbd-damsl shallow-discourse-function anno-\ntation coders manual,” Institute of Cognitive Science Technical Report,\n1997.\n[14] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[15] C. Doersch, “Tutorial on variational autoencoders,” 2016.\n[16] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-\nsupervised learning with deep generative models,” in Advances in neural\ninformation processing systems, 2014, pp. 3581–3589.\n6\nModel\nSED\nSCE\nbus\nweather\nmovie\nrestaurant\nbus\nweather\nmovie\nrestaurant\nHMM\n-\n-\n-\n-\n-\n-\n-\n-\nKMeans\n-\n-\n-\n-\n-\n-\n-\n-\nVRNN\n0.217\n0.233\n0.165\n0.166\n1.661\n1.606\n1.488\n1.391\nSVRNN\n0.213\n0.228\n0.162\n0.176\n1.600\n1.548\n1.449\n1.529\nDSBERT+Balance KL\n0.160\n0.133\n0.133\n0.116\n0.583\n0.430\n0.628\n0.526\nDSBERT+Greedy Balance\n0.198\n0.218\n0.164\n0.139\n1.349\n1.358\n1.448\n1.005\nDSBERT+Top Balance\n0.199\n0.196\n0.163\n0.152\n1.326\n1.150\n1.391\n1.079\nTABLE III: Experimental results of SimDial\n[17] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio,\n“A recurrent latent variable model for sequential data,” Advances in\nneural information processing systems, vol. 28, pp. 2980–2988, 2015.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems, 2017, pp. 5998–6008.\n[19] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv preprint arXiv:2004.05150, 2020.\n[20] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,\nS. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al., “Big bird:\nTransformers for longer sequences.” in NeurIPS, 2020.\n[21] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with\ngumbel-softmax,” arXiv e-prints, 2016.\n[22] X. Chen, W. Hong, F. Nie, J. Z. Huang, and L. Shen, “Enhanced\nbalanced min cut,” International Journal of Computer Vision, vol. 128,\nno. 4, 2020.\n[23] J. Quan, S. Zhang, Q. Cao, Z. Li, and D. Xiong, “RiSAWOZ:\nA large-scale multi-domain Wizard-of-Oz dataset with rich semantic\nannotations for task-oriented dialogue modeling,” in Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP).\nOnline: Association for Computational Linguistics, Nov.\n2020,\npp.\n930–940.\n[Online].\nAvailable:\nhttps://www.aclweb.org/\nanthology/2020.emnlp-main.67\n7\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-11-09",
  "updated": "2021-11-09"
}