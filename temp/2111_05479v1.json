{
  "id": "http://arxiv.org/abs/2111.05479v1",
  "title": "Spatially and Seamlessly Hierarchical Reinforcement Learning for State Space and Policy space in Autonomous Driving",
  "authors": [
    "Jaehyun Kim",
    "Jaeseung Jeong"
  ],
  "abstract": "Despite advances in hierarchical reinforcement learning, its applications to\npath planning in autonomous driving on highways are challenging. One reason is\nthat conventional hierarchical reinforcement learning approaches are not\namenable to autonomous driving due to its riskiness: the agent must move\navoiding multiple obstacles such as other agents that are highly unpredictable,\nthus safe regions are small, scattered, and changeable over time. To overcome\nthis challenge, we propose a spatially hierarchical reinforcement learning\nmethod for state space and policy space. The high-level policy selects not only\nbehavioral sub-policy but also regions to pay mind to in state space and for\noutline in policy space. Subsequently, the low-level policy elaborates the\nshort-term goal position of the agent within the outline of the region selected\nby the high-level command. The network structure and optimization suggested in\nour method are as concise as those of single-level methods. Experiments on the\nenvironment with various shapes of roads showed that our method finds the\nnearly optimal policies from early episodes, outperforming a baseline\nhierarchical reinforcement learning method, especially in narrow and complex\nroads. The resulting trajectories on the roads were similar to those of human\nstrategies on the behavioral planning level.",
  "text": "SPATIALLY AND SEAMLESSLY HIERARCHICAL\nREINFORCEMENT LEARNING FOR STATE SPACE AND POLICY\nSPACE IN AUTONOMOUS DRIVING\nA PREPRINT\nJaehyun Kim\nJaeseung Jeong\nABSTRACT\nDespite advances in hierarchical reinforcement learning, its applications to path planning in au-\ntonomous driving on highways are challenging. One reason is that conventional hierarchical rein-\nforcement learning approaches are not amenable to autonomous driving due to its riskiness: the agent\nmust move avoiding multiple obstacles such as other agents that are highly unpredictable, thus safe\nregions are small, scattered, and changeable over time. To overcome this challenge, we propose a\nspatially hierarchical reinforcement learning method for state space and policy space. The high-level\npolicy selects not only behavioral sub-policy but also regions to pay mind to in state space and for\noutline in policy space. Subsequently, the low-level policy elaborates the short-term goal position\nof the agent within the outline of the region selected by the high-level command. The network\nstructure and optimization suggested in our method are as concise as those of single-level methods.\nExperiments on the environment with various shapes of roads showed that our method ﬁnds the nearly\noptimal policies from early episodes, outperforming a baseline hierarchical reinforcement learning\nmethod, especially in narrow and complex roads. The resulting trajectories on the roads were similar\nto those of human strategies on the behavioral planning level.\nKeywords Hierarchical Reinforcement Learning · Spatial Hierarchy · Autonomous Driving · Path Planning\n1\nIntroduction\nThe essential idea of hierarchical reinforcement learning (HRL) is to ﬁnd a proper hierarchy of abstractions for tasks on\nloosely coupled Markov decision process (MDP) [Parr and Russell, 1998]. One predominant technique is temporal\nabstraction of policies, in which high-level command operates on Semi-MDP that has lower temporal resolution than\nMDP, and the low-level policy speciﬁes primitive actions more frequently under the command [Sutton et al., 1999a].\nAnother technique is state abstraction, for which similar concepts are suggested in Dayan and Hinton [1992], Singh\net al. [1994], and Dietterich [2000]. Regarding the two types of abstraction, Sutton et al. [1999a] proposed option\nframework, where high-level policy selects an option that consists of an initiation set, a sub-policy, and a termination\ncondition. In respect of rewarding, two major directions exist: reward hiding [Dayan and Hinton, 1992] empowers the\nmanager to independently reward sub-managers according to their compliance with the commands, while MAXQ value\ndecomposition [Dietterich, 2000] assigns credit for task rewards by temporally decomposing a state-action value in the\nparent task into expected total rewards during the execution of sub-policy and after the execution of sub-policy.\nAdvances in hierarchical approaches with deep neural networks, rooted in the aforementioned techniques, engage with\nmultiple challenges. Kulkarni et al. [2016] presented hierarchical deep Q networks where every policy is learned as a\nseparate DQN and intrinsic rewards encourage sufﬁcient exploration in the subtask. For tasks on MDPs that are densely\ncoupled in temporal dimension, high-level goals are modiﬁed by a network module [Nachum et al., 2018]. For discrete\nsub-policies, option-critic [Bacon et al., 2017] learns the options end-to-end by policy gradient, without having to\nmanually deﬁne options. More diverse strategies became available through the use of a maximum entropy objective that\nenables a latent layer of high-level policy networks to directly control the sub-policy [Haarnoja et al., 2018]. HiPPO\n[Li et al., 2020] adopted the control, formulated an unbiased latent dependent baseline, and derived a new hierarchical\npolicy gradient that allows joint training of all levels with proximal policy optimization (PPO).\narXiv:2111.05479v1  [cs.LG]  10 Nov 2021\narXiv Template\nA PREPRINT\nRecent approaches of HRL for autonomous driving (AD) [Paxton et al., 2017, Chen et al., 2019, Rezaee et al., 2019]\nkept up with some of these advances but did not fully reckon with following task-speciﬁc attributes. In AD, vehicle\nstates can change unpredictably, rapidly, and frequently due to behaviors of ego agent and others. This does not\nsatisfy the prerequisite of canonical HRL, a loosely coupled MDP. Additionally, the agent must drive through obstacles\nincluding other agents and avoid collision. Otherwise, it will be given a huge penalty and encounter the end of an\nepisode without reaching the goal that accompanies delayed positive rewards. Therefore, the agent is demanded to\nkeep an appropriate distance from obstacles concerning expected returns. In this respect, one way to efﬁciently learn\nnavigation in AD is to sample the short-term goal only in collision-free regions which are relatively safe in near future\nand modify or re-sample them at subsequent time steps.\nEven when given the knowledge about collision-free regions, the optimization is still tricky with gradient-based methods\nwhen the structure of regions is complex. In dense trafﬁc, locally optimal intermediate goal position may exist for\nevery inter-vehicle region (IVR) on a lane due to the contradiction of policy gradient directions between huge and\nhard-constrained objectives about collision avoidance and small and soft-constrained objectives about travel time\nreduction. On this ground, choosing one region among reachable regions in near future can be an effective task\ndecomposition where the subtask is to ﬁnd the local optimum in the given IVR.\nThus, we propose a reinforcement learning method with a two-level hierarchy that accommodates those attributes of\nAD on highways. Our agent can estimate the value of multi-lateral strategies including short-term goals that can be\nlocal optima in IVRs and select the best one given the circumstance at every time step. Our contribution to HRL and\nAD is three-fold:\nWe rethink the role of hierarchical policies, and propose a spatially hierarchical reinforcement learning (SHRL) method\nusing deep neural networks. Our high-level policy selects a combination of behavioral sub-policy and its components,\nthe IVRs to be used as a part of state for the two levels and as the outline of the sub-policy space. Our low-level\npolicy generates an action in continuous space by elaborating within the outline, using a unique network structure and\nalgorithm that we name neural elaboration within reinforced outline (NEWTRO).\nWe suggest a seamless HRL method for smooth state transition in short-term goal planning. The high-level policy\nselects one of either the current subspace or a candidate goal subspace from their subspace features, and the low-level\npolicy can bridge the two subspaces. By sharing the value network for the two-level policies, our network structure and\nmemory are designed to be as concise as those of a single-level actor critic, and learned without temporal abstraction.\nOur methods showed drastic improvement of performance with fast optimization speed. In experiments, the agent\nusing our method received higher rewards from early episodes than a baseline HRL, especially on narrow and complex\nroads. The traveling trajectory of the agent using our method followed general human tactics, while the agent using the\nexisting HRL did not.\nTwo substructures of our methods and a conventional HRL method are compared on various types of roads and trafﬁc\nvolumes to examine how much effective to use collision-free regions that are the decomposition of space depending on\nobstacles.\n2\nBackground and related work\nIn this section, we introduce task deﬁnition in several branches of reinforcement learning (RL) and advantage actor-critic\nunderlying our method to help understanding it. Then we compare our method in comparison with other HRL methods\nfor autonomous driving on highways.\n2.1\nTask deﬁnition in branches of reinforcement learning\nCanonical RL consists of the agent making action a ∈A and the environment of the state s ∈S, interacting with each\nother. The environment follows MDP: it assumes the situation that given the current state st at time t, the next state st+1\ndoes not depend on the past states and actions. When the state is fully observable, the policy π can be established from\nthe s to make a. In multi-agent RL, MDP is expanded to the actions of multi-agents a, where the next state is determined\nby s and a. In certain RLs with feature set or hierarchical RL, s can be abstracted to parts of s, or abstracted states can be\ndeﬁned in different space from S. For goal-based RL methods of Universal Value Function Approximators [Schaul et al.,\n2015] or the policies given goal from the high level in hierarchy, the state-goal value V (s, g) or the state-action-goal\nvalue Q(s, a, g) can be estimated and learned. In single-agent RL or multi-agent RL with self-interested agents, a typical\nobjective is the expected long-term discounted rewards ρ(π) = E{P∞\nt=1 γt−1rt|s0, π}. In RL with multi-objectives,\nthe reward can be multi-dimensional, expressible as rt = {r1,t, r2,t, · · · , rN−1,t, rN,t} for N number of objectives. In\n2\narXiv Template\nA PREPRINT\nthis paper, we design a spatially hierarchical RL agent receiving a long-term goal and multi-objectives who operates on\nthe environment with self-interested multi-agents.\n2.2\nPolicy gradient and advantage actor-critic\nPolicy gradient methods aim to ﬁnd optimal parametrized policies by performing gradient descent to optimize an\nobjective. The policy gradient theorem of Sutton et al. [1999b] derives the gradient with respect to the parameters of\nstochastic policy θ as\n∂ρ(πθ)\n∂θ\n=\nX\ns\ndπθ(s)\nX\na\n∂πθ(s, a)\n∂θ\nQπ(s, a),\n(1)\nwhere dπθ(s) is the stationary distribution following the parameterized policy πθ, and Qπ(s, a) is the value of a\nstate-action pair given a policy P∞\nt=1 E{γt−1rt −ρ(π)|s, a, π}.\nFor a designated start state s0, only the long-term rewards are cared for Qπ(s, a). In Konda [2002], Qπ(s, a) is\ndesigned as a feature vector φθ, which is critic. Advantage actor-critic methods employ the advantage function\nAπ(s, a) = Qπ(s, a) −V π(s), where V π(s) is the state value used as the base line. The underlying reason is that the\npolicy gradient suffers from its high variance because of the dependency on the state-action values during a trajectory.\nIn our method we adopt n-step advantage estimate [Schulman et al., 2015] because it ﬁts our hierarchical actor-critic\ndesign aiming at efﬁcient policy decomposition for domain-speciﬁc subspaces.\n2.3\nHierarchical policies for autonomous driving on highways\nReinforcement learning approaches have been adopted for autonomous driving on highways to teach a vehicle agent\nseveral behaviors, such as cruise control [Chen et al., 2017, Zhao et al., 2017], lane keeping [Kendall et al., 2019], lane\nchanging [Wang et al., 2018], and trafﬁc merging [Wang and Chan, 2017]. Recently, Paxton et al. [2017], Rezaee et al.\n[2019], Chen et al. [2019] suggested hierarchical methods where the high-level policies commonly function as a type of\nbehavior planner, while the low-level policies take different roles: primitive control [Paxton et al., 2017, Chen et al.,\n2019] and motion planning [Rezaee et al., 2019].\nThe method of Paxton et al. [2017] generates motion plans by using tree search, where the high-level options and the\nlow-level actions are selected in turn to extract the best option sequence. For the low-level policy, world states are\nabstracted to the set of continuous, logical, and agent-relative features. Rezaee et al. [2019] designed a cruising method\non multi-lanes, where the high-level behavioral planner makes a decision on a discretized state-action space. They\nregarded path planning as a sequence of symbolically punctuated behaviors, and handed over authority of reactive\nplanning to the low-level policy. They were concerned about specifying deadlines of a ﬁxed expiration time of the\nhigh-level command in AD, so designed the low-level policy as a motion planner that prioritizes safety over the\nhigh-level decisions. Chen et al. [2019] designed networks with spatial and temporal attention mechanism for input\nimages of the front view. While a CNN encoder is shared for actor and critic, the spatial and temporal attention is\napplied only for hierarchical policies. On top of the encoder, salient regions are chosen by spatial attention networks,\nand the sequence of the regions runs through LSTM, whose outputs are applied with temporal attention.\nOur HRL method does not take advantage of temporal abstraction, and is limited to short-term goal planning. Putting\naside the concerns on the temporal aspect in Rezaee et al. [2019] and Chen et al. [2019], our method focuses on\nhierarchical policies with spatial decomposition because we consider that for driving on highways with dense trafﬁc,\nspatially decomposing states can be much easier than temporally decomposing a sequence of multi-dimensional states\nthat are entangled with hierarchical actions of multi-agents. For state abstraction, rather than using popular approaches\nof abstraction for sub-policy or spatial attention networks, we aimed at the high-level policy that can choose its own\nfeatures resembling our eyes looking at several focus points and deciding to return to the salient one. While the\nhierarchical policies of previous research, the hierarchical policies have respective roles in different levels, such as\nabstracted behavior, motion planning, and control. our two-level policies work in the same policy space, which is the\nshort-term goal generation.\n3\nAlgorithm\nIn the temporal respect, if a short-term goal is reachable without the episode ending midway, the expected returns of\nchoosing a short-term goal position are the sum of expected returns during a behavioral motion before reaching the goal\n3\narXiv Template\nA PREPRINT\nand expected returns at the future state when ego vehicle reaches the goal position. Although other agents are highly\nunpredictable, learning through multiple explorations enables estimating the expected returns of the goal selection given\na state st, a long-term goal region lt, and a behavioral motion to reach its short-term goal gt at time t. The details of a\nlong-term goal region are in Section A.5 of Appendix, and we ﬁx the region as l for each episode. In this setting, our\nHRL algorithm additionally employs the features of inter-vehicle regions (IVRs) that are subspaces of the action space\nacquired from our method using domain knowledge. The details of IVRs are in Section A.4 of Appendix.\nThe subspaces are used in three ways, 1) as additional state features of the IVR that ego-vehicle currently belongs to\nct, 2) as selectable state features to pay mind to, and 3) as the outline that conﬁnes the low-level policy space. The\nhigh-level policy π chooses one of the candidates h ∈H whose behavioral mode b ∈B speciﬁes a set of features of\navailable subspaces to pay mind to, Mb, and the subspace for the outline, ob. Thus, a candidate command is deﬁned as\nh = {b, mb, ob} where features of subspace to pay mind to is following mb ∈Mb. Table 1 in Appendix lists a set of\ncandidates determined by the condition of current state.\nThe high-level policy π at time step t is to select the candidate of maximum state-candidate value Qh:\nπ = arg max\nht∈Ht Q(lt, st, ct, ht)\n≃arg max\n˜ht∈˜Ht\nQ(lt, st, ct, ˜ht)\n= arg max\n˜ht∈˜Ht\nQ(lt, st, ct, mb,t, bt),\n(2)\nwhere ˜h is a partial candidate {b, mb}, and the command is chosen by exhaustive search for all possible partial\ncandidates ˜Ht at time step t. The features of outline region ob are not explicitly indicated in inputs for the value\nestimation, since ob is either the current IVR or an IVR in mind, and each behavioral mode includes the decision\nwhether to stay in the current IVR or to move on to IVR in mind given the features of both IVRs as inputs ct and mb,t.\nThe information on encoding process is described in Section 4 in detail.\nIn accordance with the high-level policy, our low-level stochastic policy search φ at time step t generates a two-\ndimensional goal position at ∈N2, where N = (0, 1), given a high-level command ht, such that\nat ∼φ(lt, st, ct, ht)\n= φ(lt, st, ct, bt, mb,t, ob,t).\n(3)\nThe normalized goal position at is interpreted as a position in the local coordinate of the outline region ob,t, and our\nmanually designed function T : N2 →R2 transforms the local position at to the corresponding position in the global\ncoordinate gt given ob,t:\ngt = T (at, ob,t).\n(4)\nThe details of the transformation function T are in Subsection A.2 and A.4 of Appendix. For control of vehicle in\nreaching the goal gt, the outline region ob,t also determines the target heading direction ψ.\nGiven high-level command at every time step, the actual goal position of the agent is determined by φ, thus\nthe high-level state-command value is equal to the state value of low-level policy working in accordance with the\ncommand,\nQh(st, ht) = Vl(st, ht).\n(5)\nBy designing φ to be learned through actor-critic with the state value estimation, all state-candidate values for our\nhigh-level policy can be estimated. When any low-level state value is well estimated, and the low-level policy is locally\noptimal in a given inter-vehicle region, we can naturally assume that the high-level policy that selects the candidate of\nmaximum state-candidate value as the command is also optimal.\nTo achieve this global optimum requires nothing but quality learning through actor-critic for action in continuous\nspace, and we adopted proximal policy optimization (PPO) [Schulman et al., 2017] to prevent an abrupt decrease in the\noptimality of learning. The surrogate objective of PPO for our advantage actor-critic given high-level command is\nLCLIP\nNEW T RO(θ) = Eτ\nT −1\nX\nt=t0\nmin{wt(θ)Al(lt, st, ct, ˜ht, at), wclip\nt\n(θ)Al(lt, st, ct, ˜ht, at)},\n(6)\nwhere the clipped ratio is deﬁned as wclip\nt\n= clip(\nπθ(at|lt,st,ct,˜ht)\npiθold(at|lt,st,ct,˜ht), 1 −ϵ, 1 + ϵ) and the trajectory is given for time\nt0 to T.\n4\narXiv Template\nA PREPRINT\nThe advantage is equal to the difference between the expected return and the state value given high-level\ncommand,\nAl(lt, st, ct, ˜ht, at) = Gt −Vl(lt, st, ct, ˜ht).\n(7)\nCanonical policy gradient methods [Sutton et al., 1999b, Konda, 2002] learn from recent trajectories for smooth update\nof policy parameters, and PPO employs a type of estimator for expected returns G introduced in past work [Williams,\n1992, Mnih et al., 2016a]. We expand the estimator as\nˆ\nGt = rt + γrt+1 + · · · + γT −t+1rT −1rt+1 + γT −tVmax(lT , sT , cT , ˜HT ).\n(8)\nIn the last term in the right side of Equation 8, Vmax(lT , sT , cT , ˜HT ) is the maximum state-candidate value at time step\nT acquired from Equation 2 if T is a non-terminal time step, otherwise 0. Fortunately, this type of expected returns are\nnot tricky to save and batch, since the returns do not require the set of ˜H from t to T −1 that could include varying\nnumber of candidate regions to pay mind to for each behavior through time. In practice, Gt is easily implemented by\nadding ˜HT to the batched trajectory at the training time step. In brief, the main computation ﬂow of our method is\nAlgorithm 1: SHRL with proximal policy optimization\nInitialize the parameters of a type of Actor-Critic networks, Θ = {θ, ϑ, ...}\nfor iteration=1,2,... do\nChoose h = {b, mb, ob} ∈H given l, s, c, b, and mb by max-Q policy of parameters ϑ\nChoose a given l, s, c, b, mb, and ob, by policy search of parameters θ\nTransform a to the goal position g by T (a, ob), where ob deﬁnes the local coordinate for a\nCalculate the target heading direction ψ by using ob\nApply g and ψ to the controller of the agent\nSave the transition < l, s, c, b, mb, a, r > to the memory\nif memory size is equal to T −t0 then\nBatch transitions, and put the batch and ˜HT as training data\nOptimize for policy surrogate objective and critic loss L w.r.t. Θ\nClear memory\nend if\nend for\nsummarized in Algorithm 1, where θ and ϑ are parameters of the low-level actor and critic shared in the hierarchy,\nrespectively.\n4\nDeep neural network structure and processing\nThe general learning model for AD deals with multi-modal features. In our environment, the model receives three\ntypes of inputs: information on vehicles, obstacle positions, and inter-vehicle regions (IVRs). The detailed features are\ndescribed in Subsection A.5 in Appendix. All types of information include two-dimensional global positions such that\nthe relative information of all feature types is expected to be effectively encoded in our model. Our model consists\nof four distinctive modules: inter-vehicle relation encoder, multi-modal information encoder, policy networks, and\nvalue networks. The overall structure of the modules is described in Figure 1. Actor and critic are implemented with\nfully-connected layers, and other modules are explained in Subsections below. We practically employed double-critics\nfor stable value learning, which is proposed for deep Q networks [van Hasselt et al., 2016].\n4.1\nInter-vehicle relation encoder\nInter-vehicle relation encoder deals with feature information on ego vehicle and surrounding vehicles. To integrate\nthe features of a varying number of surrounding vehicles in respect of ego vehicle, an architecture of graph attention\nnetworks, Transformer [Mnih et al., 2016b] is used for the encoder. Transformer was originally proposed for natural\nlanguage processing, and Leurent and Mercat [2019] used it in AD to encode the state of vehicles to make high-level\ndecisions for longitudinal control. Transformer consists of encoder layers and decoder layers, and other operations.\nWe used one layer of decoder without encoding layers, where the source inputs receive the features of surrounding\nvehicles and the target inputs receive the features of ego vehicle. The outputs of inter-vehicle relation encoder are used\nas one type of the inputs for multi-modal information encoder.\n5\narXiv Template\nA PREPRINT\nFigure 1: Model architecture for our method in autonomous driving\n4.2\nMulti-modal information encoder\nAs seen in Figure 1, multi-modal information encoder integrates the long-term goal l, the vehicle encoding v, range\ninformation r, the IVR that the ego belongs to c, and the IVR in mind m, depending on the behavior mode b. The\noutputs are the encoding of goal, state, and the high-level command used for actor and critic. Each behavioral mode\nrelates these features while interpreting them on the corresponding outline region whose features are received as c\nor m. For behavioral mode, we will present the performance two types of designs: 1) indexed selection for tabular\nencoding outputs, 2) network attention from the one-hot vector b on hidden layers, a = f attend1(b), h = f abstract1(m),\nand e = f abstract2(h ⊙a), which is similarly used in HRL [Vezhnevets et al., 2017, Earle et al., 2018].The two\ndesigns allow that common features and skills are shared across behavioral modes. The designs were compared in our\nexperiment to ﬁnd a better structure.\n4.3\nNeural elaboration within reinforced outline (NEWTRO)\nThe actor plays the role of the low-level policy, generating a goal for the two-dimensional center position, which\nshould be conﬁned to the inside of the outline of the given IVR. For the conﬁnement, we adopt a simple normalization,\napplying a sigmoid function with the input coefﬁcient sigmoid(cx) as the activation function for the outputs of the\nactor network. This compares favorably with constraint optimization methods in that it provides faster optimization\nspeed, and that the corners of the ego vehicle mostly stay inside the outline since the goal does not shift closer to the\nborder of the outline unless it is inevitable and values of huge magnitudes are output before the activation. One problem\nof simply using the output normalization is that both the positions of multi-modal input features that construct the\nencoding along with behavior and the featuring positions of the outline IVR are speciﬁed with respect to the global\ncoordinate frame, while the outputs of the actor are normalized between 0 and 1. Thus, we devise an additional\nnormalization technique for the multi-modal information encoding e to train the actor to be invariant of the varying size\nof the outline. Before being used as inputs for the actor network, e is simply copied and divided by the length and\nsampled widths of the outline. An IVR is given as the positions of the left and right sides sampled for N times along\nthe progressing direction of vehicles. The length of the outline along the progressing direction, lo, and the widths of the\nregion, {wo,1, wo,2, ·, wo,N−1, wo,N}, which are the distances between the two sides of each sample, are deﬁned by the\nmethod in Subsection A.1 and A.4 of Appendix. Thus, e of d dimensions is copied N + 1 times, divided by these\nlength and widths, and concatenated so that e turns to {e/lo, e/wo,1, e/wo,2, ·, e/wo,N−1, e/wo,N}. Using the copied\nencodings of N + 1 dimension normalized by the outline that is selected by the reinforced high-level policy, the local\nsub-policy network elaborates the outline by generating a goal position.\n6\narXiv Template\nA PREPRINT\n5\nExperimental results\n5.1\nExperiment setting\nIn our environment, agents appear at random lanes at the left end of the road at random time steps, and an agent ends\nthe episode when it reaches the goal region, which is the right end of the road, or collides with either other vehicles or\nlane shoulders. When a collision occurs, vehicles stay stuck on the road for a ﬁxed delay time and then disappear from\nthe road. The reward given to the agent differs depending on the signiﬁcance of objectives. Big reward or penalty is\ngiven at the end of the episode while small rewards or penalties is given during the episode. The details of rewards are\ndescribed in the table in Section A.5 of Appendix. Vehicle agents are equipped with Stanley controller [Hoffmann et al.,\n2007] for lateral control and hierarchical PID controllers for longitudinal control, and simulated by a kinematic bicycle\nmodel [Kong et al., 2015]. We prepared environments with various types of roads, which are in Figure 4 of Appendix.\nTo average the optimization of multi-agents and simplify the comparison of performance for different methods in Figure\n2, we adopted a joint training method of Mnih et al. [2016b], which is originally for single-agent in multi-environments.\n5.2\nEvaluation\nFigure 2: Training evolution of our methods and a baseline on three types of roads\nWe compare SHRL of two state-command encoding structures to a baseline HRL method with domain knowledge\nabout the deﬁnition of sub-goals. Figure 2 shows the performance of agents on the roads of four straight lanes (left),\ntwo curved lanes (center), and two lanes merging into one (right). During the joint training of all vehicle agents using\neach method, we plotted moving average of the sum of returns for each episode with initiation to 0 and alpha = 0.1.\nThe total episode rewards are close to 1 when an agent successfully learns the task including goal reaching, and when\nan agent fails and have an accident, −1. In comparison with the baseline HRL, the attention-based methods for the\nbehavior mode showed decent performance from early episodes for all types of roads taking advantage of the outline.\nWhen the agent collides with surrounding vehicles, the episodic rewards drops for a moment, and steadily increase\nthrough learning multi-agent interactions. We also tested the structural design using separate modules for each mode,\nbut it was not as effective as the two methods. The baseline method showed ﬂuctuating performance for straight lanes,\nlow performance for the curved lanes, and failed to learn in the merging lanes since safe short-term goals were hardly\nconstructed as the chance of collision increases.\n5.3\nVisualization\nFigure 3: Trajectories of vehicles on two curved lanes\nWe qualitatively examined the traveling trajectory of vehicle agents on curved two lanes in our method against that of\nthe baseline HRL method. General human tactics for the agent is to take the shortest possible path, which could be\na straight line in the optimal environment. The agent should consider collision avoidance, travel time reduction, and\n7\narXiv Template\nA PREPRINT\ncompliance with speed limits. In our experiment, two vehicle agents start to drive from the left end of each lane almost\nsimultaneously with slight randomness. When an inter-vehicle region on the shortest possible path is available, then the\nagent will move on to occupy the region. Figure 3 shows agents using our method. The agent on two curved lane tried\nto take the inner lane at ﬁrst, and then the outer lane in the end. The red agent on merging lane wait until the green\nagents pass the lane in the middle, and follow the green agent. Agents using the baseline HRL, with frequent accidents,\nand seemingly not following the optimal path even when they complete the journey.\n6\nDiscussion and future work\nWe presented hierarchical policies that can utilize space decomposition of state and policy by selecting a subspace at the\nhigh level and learning low-level policy with the given subspace. Although the hierarchical policies are from different\nbase value deﬁnitions that are high-level state-action value and low-level state value given command, we integrated\nthem as the same network structure learned by single-level training. Thus, our method can be utilized as a part of\na nested spatio-temporal hierarchy of reinforcement learning where inner spatial hierarchy is implemented with our\nmethod and outer temporal hierarchy is designed with canonical HRL methods.\nFor autonomous driving, our experiments showed that learning the short-term planning using domain knowledge about\ncollision-free space and its decomposition is extremely efﬁcient. We expect that the use of domain knowledge in\ndeep reinforcement learning with integrated value in the hierarchy, which is seemingly close to the basis of human\ncognitive thinking and proven to be efﬁcient can be applied to complex levels of reinforcement learning: model-based\nreinforcement learning with predictive control or self-interested multi-agent reinforcement learning in game-theoretic\nsituations.\nThe limitation of our method is that a separate process for spatial decomposition is required, and that the performance\nhighly depends on the decomposition ability. Effective automatic spatial decomposition methods through unsupervised\nlearning or reinforcement learning will be able to generalize our HRL algorithm to different tasks.\nReferences\nRonald Parr and Stuart Russell. Reinforcement learning with hierarchies of machines. Advances in neural information\nprocessing systems, pages 1043–1049, 1998.\nRichard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal\nabstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999a.\nPeter Dayan and Geoffrey E. Hinton. Feudal reinforcement learning. In Stephen Jose Hanson, Jack D. Cowan,\nand C. Lee Giles, editors, Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver,\nColorado, USA, November 30 - December 3, 1992], pages 271–278. Morgan Kaufmann, 1992. URL http:\n//papers.nips.cc/paper/714-feudal-reinforcement-learning.\nSatinder P. Singh, Tommi S. Jaakkola, and Michael I. Jordan. Reinforcement learning with soft state aggregation.\nIn Gerald Tesauro, David S. Touretzky, and Todd K. Leen, editors, Advances in Neural Information Processing\nSystems 7, [NIPS Conference, Denver, Colorado, USA, 1994], pages 361–368. MIT Press, 1994. URL http:\n//papers.nips.cc/paper/981-reinforcement-learning-with-soft-state-aggregation.\nThomas G Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition. Journal of\nartiﬁcial intelligence research, 13:227–303, 2000.\nTejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.\nHierarchical deep rein-\nforcement learning:\nIntegrating temporal abstraction and intrinsic motivation.\nIn D. Lee, M. Sugiyama,\nU. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-\nume 29. Curran Associates, Inc., 2016.\nURL https://proceedings.neurips.cc/paper/2016/file/\nf442d33fa06832082290ad8544a8da27-Paper.pdf.\nOﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efﬁcient hierarchical reinforcement learning. In Samy\nBengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 3307–3317, 2018. URL https:\n//proceedings.neurips.cc/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html.\nPierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Satinder P. Singh and Shaul\nMarkovitch, editors, Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017,\nSan Francisco, California, USA, pages 1726–1734. AAAI Press, 2017. URL http://aaai.org/ocs/index.php/\nAAAI/AAAI17/paper/view/14858.\n8\narXiv Template\nA PREPRINT\nTuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical\nreinforcement learning. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80\nof Proceedings of Machine Learning Research, pages 1846–1855. PMLR, 2018. URL http://proceedings.mlr.\npress/v80/haarnoja18a.html.\nAlexander C. Li, Carlos Florensa, Ignasi Clavera, and Pieter Abbeel. Sub-policy adaptation for hierarchical reinforce-\nment learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=ByeWogStDS.\nChris Paxton, Vasumathi Raman, Gregory D. Hager, and Marin Kobilarov. Combining neural networks and tree search\nfor task and motion planning in challenging environments. In 2017 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, IROS 2017, Vancouver, BC, Canada, September 24-28, 2017, pages 6059–6066. IEEE, 2017.\ndoi:10.1109/IROS.2017.8206505. URL https://doi.org/10.1109/IROS.2017.8206505.\nYilun Chen, Chiyu Dong, Praveen Palanisamy, Priyantha Mudalige, Katharina Muelling, and John M. Dolan. Attention-\nbased hierarchical deep reinforcement learning for lane change behaviors in autonomous driving. In 2019 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems, IROS 2019, Macau, SAR, China, November 3-8,\n2019, pages 3697–3703. IEEE, 2019. doi:10.1109/IROS40897.2019.8968565. URL https://doi.org/10.1109/\nIROS40897.2019.8968565.\nKasra Rezaee, Peyman Yadmellat, Masoud S. Nosrati, Elmira Amirloo Abolfathi, Mohammed Elmahgiubi, and Jun Luo.\nMulti-lane cruising using hierarchical planning and reinforcement learning. In 2019 IEEE Intelligent Transportation\nSystems Conference, ITSC 2019, Auckland, New Zealand, October 27-30, 2019, pages 1800–1806. IEEE, 2019.\ndoi:10.1109/ITSC.2019.8916928. URL https://doi.org/10.1109/ITSC.2019.8916928.\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In Francis R.\nBach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML\n2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 1312–1320.\nJMLR.org, 2015. URL http://proceedings.mlr.press/v37/schaul15.html.\nRichard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for\nreinforcement learning with function approximation. In Sara A. Solla, Todd K. Leen, and Klaus-Robert Müller, editors,\nAdvances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November\n29 - December 4, 1999], pages 1057–1063. The MIT Press, 1999b. URL http://papers.nips.cc/paper/\n1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.\nVijaymohan Konda. Actor-critic algorithms. PhD thesis, Massachusetts Institute of Technology, Cambridge, MA, USA,\n2002. URL http://hdl.handle.net/1721.1/8120.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\nXin Chen, Yong Zhai, Chao Lu, Jianwei Gong, and Gang Wang. A learning model for personalized adaptive cruise\ncontrol. In IEEE Intelligent Vehicles Symposium, IV 2017, Los Angeles, CA, USA, June 11-14, 2017, pages 379–384.\nIEEE, 2017. doi:10.1109/IVS.2017.7995748. URL https://doi.org/10.1109/IVS.2017.7995748.\nDongbin Zhao, Zhongpu Xia, and Qichao Zhang.\nModel-free optimal control based intelligent cruise control\nwith hardware-in-the-loop demonstration [research frontier].\nIEEE Comput. Intell. Mag., 12(2):56–69, 2017.\ndoi:10.1109/MCI.2017.2670463. URL https://doi.org/10.1109/MCI.2017.2670463.\nAlex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex\nBewley, and Amar Shah. Learning to drive in a day. In International Conference on Robotics and Automation, ICRA\n2019, Montreal, QC, Canada, May 20-24, 2019, pages 8248–8254. IEEE, 2019. doi:10.1109/ICRA.2019.8793742.\nURL https://doi.org/10.1109/ICRA.2019.8793742.\nPin Wang, Ching-Yao Chan, and Arnaud de La Fortelle. A reinforcement learning based approach for automated lane\nchange maneuvers. In 2018 IEEE Intelligent Vehicles Symposium, IV 2018, Changshu, Suzhou, China, June 26-30,\n2018, pages 1379–1384. IEEE, 2018. doi:10.1109/IVS.2018.8500556. URL https://doi.org/10.1109/IVS.\n2018.8500556.\nPin Wang and Ching-Yao Chan. Formulation of deep reinforcement learning architecture toward autonomous driving\nfor on-ramp merge. In 20th IEEE International Conference on Intelligent Transportation Systems, ITSC 2017,\nYokohama, Japan, October 16-19, 2017, pages 1–6. IEEE, 2017. doi:10.1109/ITSC.2017.8317735. URL https:\n//doi.org/10.1109/ITSC.2017.8317735.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.\n9\narXiv Template\nA PREPRINT\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach.\nLearn., 8:229–256, 1992. doi:10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.\nVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David\nSilver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria-Florina Balcan\nand Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML\n2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages\n1928–1937. JMLR.org, 2016a. URL http://proceedings.mlr.press/v48/mniha16.html.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Dale\nSchuurmans and Michael P. Wellman, editors, Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence,\nFebruary 12-17, 2016, Phoenix, Arizona, USA, pages 2094–2100. AAAI Press, 2016. URL http://www.aaai.\norg/ocs/index.php/AAAI/AAAI16/paper/view/12389.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on\nmachine learning, pages 1928–1937. PMLR, 2016b.\nEdouard Leurent and Jean Mercat.\nSocial attention for autonomous decision-making in dense trafﬁc.\nCoRR,\nabs/1911.12250, 2019. URL http://arxiv.org/abs/1911.12250.\nAlexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray\nKavukcuoglu. Feudal networks for hierarchical reinforcement learning. In Doina Precup and Yee Whye Teh, editors,\nProceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, volume 70 of Proceedings of Machine Learning Research, pages 3540–3549. PMLR, 2017. URL\nhttp://proceedings.mlr.press/v70/vezhnevets17a.html.\nAdam C Earle, Andrew M Saxe, and Benjamin Rosman. Incremental hierarchical reinforcement learning with multitask\nlmdps. 2018.\nGabriel M Hoffmann, Claire J Tomlin, Michael Montemerlo, and Sebastian Thrun. Autonomous automobile trajectory\ntracking for off-road driving: Controller design, experimental validation and racing. In 2007 American Control\nConference, pages 2296–2301. IEEE, 2007.\nJason Kong, Mark Pfeiffer, Georg Schildbach, and Francesco Borrelli. Kinematic and dynamic vehicle models\nfor autonomous driving control design. In 2015 IEEE Intelligent Vehicles Symposium, IV 2015, Seoul, South\nKorea, June 28 - July 1, 2015, pages 1094–1099. IEEE, 2015. doi:10.1109/IVS.2015.7225830. URL https:\n//doi.org/10.1109/IVS.2015.7225830.\nA\nAppendix\nA.1\nRoads and lane approximation\nOur highway driving environments have one of three types of roads: four straight lanes, two curved lanes, and two\nlanes merging into one, which are in Figure 4. To handle the information on these various shapes of lanes, the\nagent uses the sampled points from each lane. For each lane, positions of the left and right sides of the lane are\nsampled along the lane such that a concatenation of convex quadrilaterals can be created. For each quadrilateral,\ninformation on adjacency to another quadrilateral in bordering lanes is given as well.The quadrilaterals approximating\na lane are used to deﬁne inter-vehicle regions (IVRs). When two positions at the rear end (qr, vr) and the front end\n(qf, vf) are given for a lane l, the distance between the two D(l, (qr, vr), (qf, vf)) is deﬁned by the sum of distances,\n(1 −vr)d(Q(l, qr)) + Pqf −1\ni=qr+1 d(Q(l, i)) + vfd(Q(l, qf)), where Q(l, q) is the quadrilateral search function and d(Q)\nis the Euclidean distance between the center positions of the rear end (0.5, 0) and the front end (0.5, 1) of the given\nquadrilateral.\nA.2\nNormalized quadrilateral coordinate\nWe are given four positions of a quadrilateral in a lane with respect to global coordinate or ego-centric coordinate,\n(xfl, yfl), (xfr, yfr), (xrl, yrl), (xrr, yrr), which are positions of front-left, front-right, rear-left, and rear-right. The\ngoal of our transformation is two-fold: First, a position inside the quadrilateral (x, y) should be transformed to the\nnormalized position (u, v) of the given quadrilateral so that (xfl, yfl), (xfr, yfr), (xrl, yrl), and (xrr, yrr) correspond\nto (1, 0), (1, 1), (0, 0), and (0, 1). Second, (x, y) should be the interpolation of positions on both left and right sides\nthat divide the length of each side in the same proportion v along the progression. Thus, we designed the transformation\nso that (x, y) is represented as the interpolation of (xvl, yvl) and (xvr, yvr) with ratio u, which are the interpolation\n10\narXiv Template\nA PREPRINT\nFigure 4: Three types of roads for our highway driving environment\nFigure 5: A position in a quadrilateral represented in normalized quadrilateral coordinate frame\nof (xfl, yfl) and (xrl, yrl) with ratio v and the interpolation of (xfr, yfr) and (xrr, yrr) with ratio v, respectively.\nFigure 5 shows the positions on an arbitrary quadrilateral represented in both global coordinate frame and normalized\nquadrilateral coordinate frame. The transformation from normalized position (u, v) to global position (x, y) is easily\ndone by the two steps of interpolation, while transformation from (x, y) to (u, v) is performed by solving the following\nequations. For x axis,\nxvl = xrl + v(xfl −xrl)\n= xrl + vdl,\n(9)\nxvr = xrr + v(xfr −xrr)\n= xrr + vdr,\n(10)\n11\narXiv Template\nA PREPRINT\nu =\n(x −xvl)\n(xvr −xvl)\n=\nx −(xrl + vdl)\n(xrr + vdr) −(xrl + vdl)\n=\n(x −xrl) −vdl\nv(dr −dl) + (xrr −xrl)\n= e −vdl\nf + vg ,\n(11)\nwhere dl = xfl −xrl, e = x −xrl, f = xrr −xrl, and g = (xfr −xrr) −(xfl −xrl).\nIn the same way for y axis,\nu = i −vhl\nj + vk ,\n(12)\nwhere hl = yfl −yrl, i = y −yrl, j = yrr −yrl, and k = (yfr −yrr) −(yfl −yrl).\nNow a quadratic equation about v is formed by combining the equations about u on both axes.\ne −vdl\nf + vg = i −vhl\nj + vk\n(13)\nThe Equation 13 can be developed to\n(dlk −hlg)v2 + (gi + dlj −hlf −ek)v + (ej −fi) = 0,\n(14)\nwhich is a quadratic equation for v, and this can be substituted to\nav2 + bv + c = 0,\n(15)\nwhere a = dlk −hlg, b = (gi + dlj −hlf −ek), and c = ej −fi.\nSince (x, y) is inside the quadrilateral, u and v are between 0 and 1. Thus, the solution to Equation 15 satisfying\nthe condition is v = −b+\n√\nb2−4ac\n2ac\nif a > 0, v = −b−\n√\nb2−4ac\n2ac\nif a < 0, otherwise, v = −c/b. Then, u = e−vdl\nf+vg if\nf + vg ̸= 0, otherwise, i−vhl\nj+vk .\nA.3\nHashing quadrilaterals\nFigure 6: An exampling for understanding the hashing algorithm of the quadrilaterals of lanes\nLanes are static features with respect to a global reference point. Thus, if all lanes can be approximated before driving,\nor parts of lanes ahead of ego vehicle can be approximated before they come in the view range, the approximated parts\ncan be used repeatedly during driving. In addition, if the quadrilaterals that approximate the lanes can be hashed, the\ntime to search for the quadrilateral inside which the center or a corner position of a vehicle exists can be reduced from\n12\narXiv Template\nA PREPRINT\nTable 1: Candidate behaviors with their available IVRs to pay mind to and the outline IVR\nBehavioral mode\nSet of available IVRs to pay mind to\nOutline IVR\nStay in current IVR\n{Current IVR}, or {Front IVR} or {Rear IVR}\nCurrent IVR\nif a vehicle invades the lane of ego\nManeuver to the other in lane\n{Front IVR} if ego is in rear IVR\nIVR in mind\n(optionally selectable)\n{Rear IVR} if ego is in front IVR\nPay mind to the left\n{IVRs on left lane to pay mind to\nCurrent IVR\nif the lane exists, otherwise ∅\nManeuver to the left\n{IVRs on left lane ahead to pay mind to}\nIVR in mind\nif the lane exists, otherwise ∅\nPay mind to the right\n{IVRs on right lane to pay mind to}\nCurrent IVR\nif the lane exists, otherwise ∅\nManeuver to the right\n{IVRs on right lane ahead to pay mind to}\nIVR in mind\nif the lane exists, otherwise ∅\nO(n) to O(1) where n is the number of quadrilaterals to be searched for. Thus, we devise a hashing algorithm that can\nquickly search for quadrilaterals to which a given position can belong. Figure 6 is an example for understanding the\nhashing algorithm. Our hash function can hash a position in two-dimension to a bin, which can be represented as a blue\nrectangle of the smallest possible unit size. To store the information of quadrilaterals in bins, we hash the vertices\nof the rectangular bounding box that covers a quadrilateral and put the quadrilateral in the bin of each vertex so that\nthe quadrilateral is stored in one to four bins. The grey boxes in the dashed line and the red dots are examples of the\nbounding boxes for quadrilaterals and the vertices of a bounding box, respectively. Practically, the duplicate storing of a\nquadrilateral in one bin is avoided for fast search. The width and height of any bin are kept longer than those of the\nbounding box that covers any quadrilateral, ensuring that vertices of any bounding box are hashed to the same bin or\nbins that are adjacent to each other. This also guarantees that for a position p, the hashing function b = H(p) can give\nthe bin that contains the quadrilateral to which p belongs because p is inside the bounding box that aligns with the bins.\nThe blue dot in Figure 6 is the center position of the blue vehicle, inside the bounding box with the red dots as its vertices.\nA.4\nInter-vehicle regions\nFigure 7: An example of inter-vehicle regions for the purple-colored vehicle\nAn inter-vehicle region (IVR) in a lane is deﬁned as the region between the rear end NQC, nr = (qr, vr), and the\nfront end NQC, nf = (qf, vf), bordered by surrounding vehicles or any end of the view range. Figure 7 shows an\nexample of IVRs for the purple-colored vehicle. Given normalized positions of both ends, the featuring global positions,\n(p0,l, p0,r), (p1,l, p1,r), ·, (pN−1,l, pN−1,r), (pN,l, pN,r), are acquired by sampling N times with uniform distance\ninterval on the left and right sides along the progressing direction, where (p0,l, p0,r) and (pN,l, pN,r) are deﬁned by nr\nand nf, respectively. By using the distance function A.2, the distance between the rear and front Dtotal = D(l, nr, nf)\nis acquired, and from the rear, we can get the next sample at the distance of Ds = Dtotal/(N −1) in turn. Table 1\nspeciﬁes candidate high-level commands of behaviors, the corresponding set of available IVRs to pay mind to, and the\noutline IVR.\nA.5\nOther features, goal, and rewards\nIn our autonomous driving environment, on top of inter-vehicle regions, two additional types of state features exist:\nstate of vehicles and range-sensing.\nThe feature of a vehicle j is\nsj = [pfl,j, pfr,j, prl,j, prr,j, vj, ψj]T ,\n(16)\n13\narXiv Template\nA PREPRINT\nTable 2: Reward types and their deﬁnition given in our environment\nReward type\nDeﬁnition\nGoal reward\n1 if the ego center position gets into the long-term goal\nCollision penalty\n−1 if the vehicle image overlays on\nthe lane shoulders or other vehicles\nProgression reward\ncprovquad where 0 < cpro < 1\nMax speed penalty\ncmaxmin(vquad −lmax, 0)\nfor speed limit lmax where −1 < cmax < −cpro\nMin speed penalty\ncminmax(lmin −vquad, 0)\nfor speed limit lmin where −1 < cmin < 0\nwhere pfl,j = (xfl,j, yfl,j), pfr,j = (xfr,j, yfr,j), prl,j = (xrl,j, yrl,j), and prr,j = (xrr,j, yrr,j). are the positions of\nthe front-left, front-right, rear-left, and rear-right corners, respectively, vj = vx,j, vy,j is the velocity, and psij is the\norientation in radians. State of vehicles consists of features of ego vehicle and arbitrary number of surrounding vehicles\non the road in the view range.\nRange-sensing features are acquired by simulating range sensing of ego vehicle and processing the information about\nglobal or ego-centered positions. An odd number of rays are shot at uniform angular intervals, θ, forming bilateral\nsymmetry where the center ray is directed to the front of the vehicle. Figure 8 shows an example of rays shot from\neach vehicle. Total features of range-sensing include the positions of N number of rays: the start, r0 and the ends,\nr1, r2, ·, rN−1, rN. Rays end where they hit obstacles such as other vehicles or lane shoulders, or at the maximum\ndistance, dmax. We used rays with N = 25, Nθ = 120, and dmax = 1000. Inter-vehicle regions can be applicable as\nFigure 8: Range sensing of vehicles\nthe long-term goal. For experiments, we simply deﬁne a long-term goal region as the rectangular box (l, r, t, b) to deal\nwith two types of goal regions, the region reached after the right end of a random lane and all lanes. Including the goal\nreaching reward, four types of rewards and their deﬁnitions are as in Table 2, where vquad is the velocity along the\nprogressing direction of the quadrilateral that the agent belongs to, lmax and vmin are maximum and minimum speed\nlimits, respectively, and cpro, cmax, and cmin are constants.\n14\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-11-10",
  "updated": "2021-11-10"
}