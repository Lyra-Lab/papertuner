{
  "id": "http://arxiv.org/abs/2111.05805v1",
  "title": "Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language Understanding",
  "authors": [
    "Qianying Liu",
    "Fei Cheng",
    "Sadao Kurohashi"
  ],
  "abstract": "Meta learning with auxiliary languages has demonstrated promising\nimprovements for cross-lingual natural language processing. However, previous\nstudies sample the meta-training and meta-testing data from the same language,\nwhich limits the ability of the model for cross-lingual transfer. In this\npaper, we propose XLA-MAML, which performs direct cross-lingual adaption in the\nmeta-learning stage. We conduct zero-shot and few-shot experiments on Natural\nLanguage Inference and Question Answering. The experimental results demonstrate\nthe effectiveness of our method across different languages, tasks, and\npretrained models. We also give analysis on various cross-lingual specific\nsettings for meta-learning including sampling strategy and parallelism.",
  "text": "Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural\nLanguage Understanding\nQianying Liu\nFei Cheng\nSadao Kurohashi\nGraduate School of Informatics, Kyoto University\nying@nlp.ist.i.kyoto-u.ac.jp, {feicheng,kuro}@i.kyoto-u.ac.jp;\nAbstract\nMeta learning with auxiliary languages has\ndemonstrated promising improvements for\ncross-lingual\nnatural\nlanguage\nprocessing.\nHowever, previous studies sample the meta-\ntraining and meta-testing data from the same\nlanguage, which limits the ability of the model\nfor cross-lingual transfer.\nIn this paper, we\npropose XLA-MAML, which performs direct\ncross-lingual adaption in the meta-learning\nstage.\nWe conduct zero-shot and few-shot\nexperiments on Natural Language Inference\nand Question Answering. The experimental\nresults demonstrate the effectiveness of our\nmethod across different languages, tasks, and\npretrained models. We also give analysis on\nvarious cross-lingual speciﬁc settings for meta-\nlearning including sampling strategy and paral-\nlelism.\n1\nIntroduction\nThere are around 7,000 languages spoken around\nthe world, while most natural language processing\n(NLP) studies only consider English with large-\nscale training data. Such setting limits the ability of\nthese NLP techniques when sufﬁcient labeled data\nis lacking for effective ﬁnetuning. To address this\nproblem and extend the global reach of NLP, recent\nresearch of Cross-lingual Natural Language Under-\nstanding (XL-NLU) focuses on multi-lingual pre-\ntrained representations such as multi-lingual BERT\n(mBERT), which cover more than 100 languages.\nThese cross-lingual pretrained models allow zero-\nshot or few-shot transfer among languages for spe-\nciﬁc tasks, where a model for a task is trained in\nmonolingual English data and directly applied to\na target low resource language. With the support\nof strong multi-lingual representations, the further\ntransfer among languages of XL-NLU could be\nformalized similar to the few-shot learning task.\nMeta learning, or learning to learn, aims to\ncreate models that can learn new skills or adapt\nFigure 1: Illustration of cross-lingual meta learning.\nto new tasks rapidly from few training examples,\nwhich has demonstrated promising result for few-\nshot learning.\nModel-Agnostic Meta-Learning\n(MAML) (Finn et al., 2017) has been recently\nshown beneﬁcial for various few-shot learning and\ncross-domain learning NLP tasks. The algorithm\nallows fast adaptation to a new task using only a\nfew data points.\nFrom MAML perspective, we can re-formulate\ncross-lingual transfer task as shown in Figure 1.\nFor model fθ with parameters θ, we can consider\nthe task distribution P(τ) formed by drawing from\ndifferent auxiliary language bags of data samples\n{(x, y)l}. For meta training, we simulate the cross-\nlingual transfer process and sample a meta dataset\n{S, Q} from different bags {(x, y)l}. The support\nset S simulates the transfer source training data and\nupdates the model parameters to θ′\ni, which is called\nthe inner step. Then the query set Q simulates the\ntransfer target testing data, model parameters are\ntrained by optimizing for the performance of f(θ′)\nwith respect to θ across tasks sampled from the\ntarget bags {(x, y)l}, which is referred as the meta\nstep. The algorithm minimize the prediction error\non target language given a small support set for\nlearning.\nNooralahzadeh et al. (2020) proposed a meta-\nlearning based cross-lingual transfer model, which\nuses data of non-target low-resource languages\nas auxiliary data. However, they sample the in-\nner tasks and meta-update tasks from the same\nauxiliary language, that the meta learning cannot\narXiv:2111.05805v1  [cs.CL]  10 Nov 2021\nperform transfer for cross-lingual adaption.\nIn\nthis paper, to address this problem, we propose\nCross-lingual Adaption Model-Agnostic Meta-\nLearning (XLA-MAML), which performs cross-\nlingual adaption in the meta learning stage to trans-\nfer knowledge across different languages.\nIn conclusion, our contribution is 3-fold:\n• We propose a cross-lingual adaption meta-\nlearning approach, which directly performs\nmeta learning adaption across different lan-\nguages, while previous studies sample the sup-\nport and query set from the same language.\n• We conduct zero-shot and few-shot exper-\niments on two novel benchmarks of XL-\nNLU, XNLI and MLQA. Experimental results\ndemonstrate the effectiveness of our model\ncompared with previous MAML studies, es-\npecially under zero-shot settings.\n• We analyze the XLA-MAML performance un-\nder a number of cross-lingual meta learning\nsettings, including sampling strategy and par-\nallelism. The additional results on XLM-R\nsuggest that XLA-MAML beneﬁts even with\na stronger encoder.\n2\nMeta Learning\nMeta-learning algorithms train over a variety of\nlearning tasks τ including potentially unseen tasks\non a distribution of tasks P(τ) and optimized for\nthe best performance of a base-learner model fθ\nwith parameters θ. Each task τ is associated with a\ndataset sampleD, containing both feature vectors\n{x} and labels {y}. The optimal model parameters\nare:\nθ∗= arg min\nθ\nED∼p(D)[Lθ(D)]\n(1)\nWhere one dataset D is considered as one data\nsample. The dataset sample D consists of a support\nset S and a query set Q, where the support set\nS is used for meta-training which simulates fast\nadaptation and the query set Q is used for meta-\ntesting that evaluates performance and compute a\nloss with respect to model parameter initialization.\nA loop of meta-training to update the parameter\ninitialization and meta-testing to update the loss\nis called an episode. Meta learning consists of\nupdating the parameters of the base-learner fθ by\nepisode loops until it reaches stop criterion. The\noptimal model parameters in Equation 1 could be\nrewritten as:\nθ = arg max\nθ\nES,Q⊂D[\nX\n(x,y)∈Q\nPθ(x, y, S)]\n(2)\nThere\nare\nthree\ncommon\napproaches\nof\nmeta-learning: metric-based, model-based, and\noptimization-based. Each approach uses differ-\nent ways to model Pθ(y|x). Metric based meta\nlearning methods predict probability over a set\nof known labels y is a weighted sum of labels\nof support set samples.\nThe weight is gener-\nated by a kernel function kθ, measuring the sim-\nilarity between two data samples, by modeling\nPθ(y|x, S) = P\n(xi,yi)∈S kθ(x, xi)yi (Koch et al.,\n2015; Sung et al., 2018; Vinyals et al., 2016; Snell\net al., 2017). Model based meta learning meth-\nods depend on a model designed speciﬁcally for\nfast learning, which updates its parameters rapidly\nwith a few training steps. This rapid parameter\nupdate can be achieved by its internal architec-\nture or controlled by another meta-learner model.\nMemory-Augmented Neural Networks (Santoro\net al., 2016) uses external memory storage to fa-\ncilitate the learning process of neural networks.\nMeta Network (Munkhdalai and Yu, 2017) uses\nfast weights, which predicts the parameters of an-\nother neural network and the generated weights.\nIn this work we focus on optimisation-based\nmethod MAML that optimisation-based methods\nare compatible with any model that learns through\ngradient descent. Optimized based adjust the opti-\nmization algorithm so that the model can perform\nfast adaption (Ravi and Larochelle, 2017; Finn\net al., 2017; Nichol et al., 2018).\nFormally, given a task τi associated with a\ndataset sample D = {S, Q}, MAML optimizes\nthe parameters towards the optimal target in Equa-\ntion 2 via two steps. In the meta-training step, or\ninner step for short, the algorithm updates the pa-\nrameters to θ\n′ on the support set S. The updated\nparameters θ\n′ is computed by loss L(0) and gra-\ndient descent steps on S with an update step size\nα,\nθ′\ni = θ −α∇θL(0)\nτi (fθ)\n(3)\nThe meta-testing step, or meta step for short,\ntakes in the updated model f(θ\n′) and optimizes the\nparameters θ based on the loss L(1) of f(θ\n′) across\nQ. While the objective is computed using the up-\ndated model parameters θ\n′, the meta-optimization\nAlgorithm 1: Cross-lingual Adaption Model-Agnostic Meta-Learning\nInput: high-resource language dataset Dsrc,\nset of low-resource language auxiliary datasets {Daux} with languages L\nResult: Predictions {y′\ntgt} = {fθ(xtgt)} for target language test set {xtgt}\nInitialization model f(θ) with pretrained mMLM model;\nFinetune f(θ) with high-resource language dataset Dsrc;\nwhile not stop criterion do\nsample {li}S ⊂L, {li}Q ⊂L;\nsample S = {(xk, yk)}K\nk=1 ⊂S\n{li}S Dli ⊂D, Q = {(xn, yn)}N\nn=1 ⊂S\n{li}Q Dli ⊂D;\nτi : D = (S, Q);\nCompute ∇θL(0)\nτi (fθ) on S;\nCompute adapted parameters with gradient descent: θ′\ni = θ −α∇θL(0)\nτi (fθ);\ncompute ∇θ\nP\nτi∼p(τ) L(1)\nτi (fθ′) on Q;\nCompute meta update for parameters: θ ←θ −β∇θ\nP\nτi∼p(τ) L(1)\nτi (fθ′\ni);\nend\nPerform zero-shot or few-shot evaluation on target language test set {xtgt};\nis performed over the model parameters θ, that the\noptimal model parameters are,\nθ∗= arg min\nθ\nX\nτi∼p(τi)\nL(1)\nτi (fθ′\ni)\n(4)\n= arg min\nθ\nX\nτi∼p(τi)\nL(1)\nτi (fθ−α∇θL(0)\nτi (fθ))\n(5)\nThe model parameters θ are updated with a meta-\nupdate step size β,\nθ ←θ −β∇θ\nX\nτi∼p(τ)\nL(1)\nτi (fθ−α∇θL(0)\nτi (fθ))\n(6)\n3\nCross-lingual Adaption\nModel-Agnostic Meta-Learning\n3.1\nMeta Task Formulation\nCross-lingual Natural Language Understanding\naims to transfer knowledge from high resource lan-\nguages to target low resource languages, that the\nmeta task formulation of MAML needs to adapt to\nsuch changes.\nTo perform cross-lingual adaption, we no longer\nrandomly form meta learning tasks τi from one\ndistribution D = {(x, y)}, where D stands for the\navailable feature vectors x and ground truth labels\ny pairs. We split D into {Dli}li∈L by language l of\nlanguage set L, and form the associated dataset D\nof meta learning tasks τi as D = (S, Q) with the\nfollowing two steps:\n1. Sample two subset of languages for support\nset S and query set Q: {li}S ⊂L, {li}Q ⊂L.\n2. Sample support set S with K data-points and\nquery set Q with N data points from language\nsubsets of D:\nS = {(xk, yk)}K\nk=1 ⊂S\n{li}S Dli ⊂D,\nQ = {(xn, yn)}N\nn=1 ⊂S\n{li}Q Dli ⊂D.\nWhere the language subset size is ﬁxed as a hy-\nperparameter. Thus the sampled tasks τi can simu-\nlate the process of cross-lingual transfer and allow\nthe meta learning perform cross-lingual fast adap-\ntion.\nDue to the characteristics of low-resource cross-\nlingual transfer, we further explore two settings:\n• The available resource amount for the cross-\nlingual transfer is relatively low. To take ad-\nvantage of all the data, we change the sam-\npling strategy from random sampling to cov-\nering all the given data.\n• Parallel data is often beneﬁcial for cross-\nlingual learning, we explore the effectiveness\nof parallelism in the inner-step and meta-step.\n3.2\nTraining Procedure\nWe show the pipeline of training procedure in Al-\ngorithm 1. We ﬁrst initialize our model f(θ) with\nmultilingual pretrained Masked Language Mod-\nels (mMLM) such as mBERT(Devlin et al., 2019),\nwhich jointly trains monolingual masked language\nmodels across multiple languages. These models\nhave been proven strong performance on cross-\nlingual transfer and serves as effective model initial-\nization. In the second step we ﬁnetune our model\non English monolingual data as a second pretrain-\ning step. This step allows the model to take beneﬁts\nDataset\n#Train\n#Dev\n#Test\nMetric\nMNLI\n392,702\n20,000\n20,000\nAcc.\nSQuAD\n87,599\n34,726\n-\nEM/F1\nTable 1: The data statistics of the high resource English\ndatasets. Dev is short for development set. The test set\nof SQuAD is not public available.\nof the high resource data and serves as a baseline\nmodel.\nWe formulate meta-learning tasks as formerly\nstated. The meta dataset sample D aims to perform\nfast adaption under cross-lingual transfer setting.\nWe perform the inner step and update the model\nto f(θ′\ni) on the support set S loss L(0) for hyper-\nparameter set inner steps by gradient descent. We\nperform the meta step and optimize the parameters\nθ based on the loss L(1) of f(θ\n′) on the query set\nQ.\nWe perform either zero-shot evaluation or few-\nshot learning on the target languages. For zero-shot\nevaluation, we directly evaluate the performance of\nthe models on unseen test set {xtgt} of the target\nlanguages. For few-shot learning, we use a small\ndevelopment set of the target language to ﬁnetune\nthe model as the standard supervised learning, and\nthen evaluate the performance on the test set.\n4\nExperiments\n4.1\nDatasets\nWe show the dataset details in Table 1 and Table 2.\n4.1.1\nNatural Language Inference\nNatural Language Inference (NLI) is the task of\ndetermining whether a given pair of hypothesis and\npremise is true (entailment), false (contradiction),\nor undetermined (neutral). It can be considered as\na sentence pair classiﬁcation task.\nMNLI\nThe Multi-Genre Natural Language Infer-\nence (MNLI) (Williams et al., 2018) dataset has\n433k sentence pairs annotated with textual entail-\nment information. The dataset covers a range of\ngenres of spoken and written text and supports\ncross-genre evaluation.\nIn our experiment MNLI is applied as the high\nresource English dataset in the second pretraining\nstep.\nXNLI\nThe Cross-Lingual Natural Language In-\nference (XNLI) (Conneau et al., 2018) dataset\nis build for evaluation of cross-lingual sentence\nunderstanding methods. It consists of a crowd-\nsourced collection of 2500 development and 5000\ntest hypothesis-premise pairs for the MultiNLI cor-\npus with their textual entailment labels. The pairs\ntranslated into 14 languages: French (fr), Spanish\n(es), German (de), Greek (el), Bulgarian (bg), Rus-\nsian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi),\nThai (th), Chinese (zh), Hindi (hi), Swahili (sw)\nand Urdu (ur), which results in 112.5k annotated\npairs.\nThe dataset supports both zero-shot evaluation\nand few-shot training for cross-lingual transfer. Un-\nder the zero-shot evaluation setting, the model is\ndirectly tested on unseen target language with only\nthe high-resource language English data available\nfrom MNLI and low-resource auxiliary languages\ndata from the development set of XNLI. Under few-\nshot learning setting, the model is further ﬁnetuned\nby the development set of target language data of\nXNLI, and then tested on the target test data.\n4.1.2\nQuestion Answering\nQuestion Answering (QA) is the task of answer-\ning a question based on a given support document.\nWhile there are various QA task settings including\nmulti-choice QA, cloze style QA and so on, in this\npaper we focus on reading comprehension dataset,\nwhere the answer to every question is a segment\nof text (a span) from the corresponding reading\npassage.\nSQuAD\nThe Stanford Question Answering\nDataset (SQuAD) (Rajpurkar et al., 2016, 2018)\nconsisting of 100,000+ questions posed by crowd-\nworkers on a set of Wikipedia articles. For each\nquestion, three candidate answers spans are given,\nwhich are segment slices of the paragraph. The\nevaluation of model prediction is based on Exact\nMatch (EM), which measures the percentage of\npredictions that match any one of the ground truth\nanswers exactly, and F1 score, which measures the\naverage overlap between the prediction and ground\ntruth answer.\nIn our experiment, SQuAD 1.1 (Rajpurkar et al.,\n2016) is applied as the high resource English\ndataset in the second pretraining step. There are no\nun-answerable questions in the training data.\nMLQA\nMultilingual\nQuestion\nAnswering\n(MLQA) (Lewis et al., 2020) is a benchmark\ndataset for evaluating cross-lingual question\nanswering performance. MLQA consists of over\n#Examples\nDataset Split\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\nth\ntr\nur\nvi\nzh\nXNLI\nDev\n2,500 2,500 2,500 2,500\n2,500\n2,500 2,500 2,500 2,500 2,500 2,500 2,500 2,500 2,500 2,500\nTest\n5,000 5,000 5,000 5,000\n5,000\n5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000\nMLQA Dev\n517\n-\n512\n-\n1148\n500\n-\n507\n-\n-\n-\n-\n-\n511\n504\nTest\n5,335\n-\n4,517\n-\n11,590 5,254\n-\n4,918\n-\n-\n-\n-\n-\n5,495 5,137\nTable 2: The statistics of the multilingual datasets by languages. Dev is short for development set. The data of\nXNLI is parallel across languages generated by human translation from the English Data. The data of MLQA is\npartially parallel generated by automatic matching of target language Wikipedia articles to English Wikipedia, and\nthen veriﬁed by crowd-sourced human annotation.\n5K extractive QA instances (12K in English) in\nSQuAD format in seven languages - English(en),\nArabic(ar), German(de), Spanish(es), Hindi(hi),\nVietnamese(vi) and Simpliﬁed Chinese(zh). The\ndevelopment set of languages are highly but not\ncompletely parallel, detailed statistics are shown in\nTable 2.\nThere are other cross-lingual question answering\ndatasets such as Cross-lingual Question Answer-\ning Dataset (XQuAD) (Artetxe et al., 2020) and\nInformation-Seeking Question Answering in Ty-\npologically Diverse Languages (TyDiQA) (Clark\net al., 2020). XQuAD does not have a develop-\nment set that it does not support few-shot training\nscenario. TyDiQA has a relatively small size of En-\nglish training data, which has a mis-match with our\nexperimental settings of English as a high resource\nlanguage.\nFor both NLI and QA, we train with the same\nmodel described in Devlin et al. (2019), except for\nthe pretrained layers are changed to mMLMs.\n4.2\nTraining setup and hyper-parameters\nWe implement our algorithm with the libraries Py-\ntorch (Paszke et al., 2019), Transformers (Wolf\net al., 2020a) and higher (Grefenstette et al., 2019).\nAll datasets are loaded with the Datasets (Wolf\net al., 2020b) library.\nWe use the cache of pretrained models of Trans-\nformers to initialize the model. For MNLI ﬁne-\ntuning, we train the model with 32 batch size and\n128 max sequence length for 3 epochs. We use\nAdamW (Loshchilov and Hutter, 2019) optimizer\nwith 2e-5 learning rate. For SQuAD ﬁnetuning,\nwe train the model with 12 batch size, 384 max\nsequence length and 128 document stride for 2\nepochs. We use AdamW optimizer with 3e-5 learn-\ning rate. For both datasets we only use the training\ndata to ﬁnetune the model.\nFor XLA-MAML, for both datasets we use the\nFigure 2: Differences in performance in terms of accu-\nracy scores on the test set for zero-shot XLA-MAML\non XNLI using the mBERT model. Rows correspond\nto auxiliary and columns to target languages. Numbers\non the off-diagonal indicate performance differences\nbetween XLA-MAML and the baseline model. The col-\noring scheme indicates the differences in performance\n(e.g., blue for large improvement).\nsame data preprocessing parameters as the English\nmodel. We use batch size 8 for both inner-step\nupdate and meta-step update. We use learning rate\nof 1e-5 with SGD optimizer for the inner-step up-\ndate, and use learning rate of 1e-5, weight decay of\n0.01 with Adam optimizer (Kingma and Ba, 2015)\nand a linear learning rate scheduler for the meta-\nstep update. NLI models with memory usage of\naround 10G use 1 minute for each 100 meta steps\ntrained on GeForce GTX 1080Ti and QA models\nwith memory usage of around 22G use 2 minutes\nfor each 100 meta steps trained on TITAN RTX.\nSince the low resource languages only have de-\nvelopment set, to avoid ﬁtting to the test set and\nselect a stopping criterion of the algorithm, we\nobserve the performance of a few runs on unseen\nlanguage development set to ﬁx a stopping iteration\nMethod\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\nth\ntr\nur\nvi\nzh\navg\nZero-shot Cross-lingual Transfer\nmBERT (Ours)\n65.65\n68.92\n70.82\n67.09\n81.32\n74.65\n73.15\n60.30\n68.64\n51.32\n55.15\n62.81\n58.32\n70.10\n69.04\n66.48\nOne aux. lang.\nX-MAML hi →X\n66.35\n70.84\n73.39\n69.38\n82.33\n76.71\n75.63\n-\n70.54\n49.94\n56.01\n63.79\n62.16\n72.69\n72.18\n-\nXLA-MAML hi →X\n67.78\n72.36\n74.23\n70.90\n82.04\n76.95\n75.77\n-\n71.98\n50.36\n60.02\n64.87\n64.11\n73.53\n73.43\n-\nTwo aux. lang.\nX-MAML best pair\n67.35\n70.80\n73.71\n69.80\n82.37\n76.85\n75.93\n64.49\n71.10\n51.84\n57.45\n64.17\n62.81\n72.95\n72.91\n68.96\nXLA-MAML same pair\n67.82\n72.04\n72.67\n69.42\n81.44\n76.17\n74.61\n64.85\n71.64\n53.71\n60.02\n64.59\n64.07\n72.71\n73.65\n69.29\nXLA-MAML best pair\n67.91\n72.77\n74.00\n70.63\n82.06\n77.03\n75.96\n65.03\n71.66\n53.82\n60.34\n66.29\n64.31\n73.73\n74.01\n69.97\nFew-shot Cross-lingual Transfer\nmBERT (Ours)\n67.98\n72.59\n73.79\n69.88\n81.2\n75.15\n74.33\n65.53\n71.48\n57.88\n63.17\n65.83\n63.41\n71.54\n73.09\n69.79\nOne aux. lang.\nX-MAML sw →X\n68.04\n71.70\n73.93\n70.06\n82.57\n77.72\n75.93\n64.91\n72.16\n-\n60.94\n65.25\n63.57\n73.05\n74.29\nXLA-MAML sw→X\n69.32\n71.78\n73.47\n70.37\n81.90\n76.40\n75.65\n65.64\n71.62\n-\n63.51\n66.16\n63.73\n71.80\n74.65\nTwo aux. lang.\nX-MAML best pair\n68.09\n72.02\n73.93\n69.48\n82.79\n77.99\n76.69\n65.49\n72.28\n58.59\n61.24\n66.12\n63.21\n73.07\n73.43\n70.30\nXLA-MAML same pair\n68.66\n72.75\n73.57\n70.35\n81.99\n77.23\n76.91\n65.82\n71.92\n58.71\n63.62\n65.73\n63.70\n72.06\n74.11\n70.47\nMachine translate\nDevlin et al. (2019) test\n70.4\n-\n74.4\n-\n81.4\n74.9\n-\n-\n-\n-\n-\n-\n62.1\n-\n70.1\n-\nWu and Dredze (2019) train\n70.8\n75.4\n74.8\n72.1\n82.1\n78.5\n76.9\n65.3\n74.3\n65.3\n63.2\n70.6\n60.6\n67.8\n76.2\n71.6\nTable 3: Accuracy results on the XNLI test set for zero-shot and few-shot XLA-MAML. Each columns indicate\nthe target language. ‘*’ denotes the average accuracy excluding the auxiliary languages, which are not comparable\nto the averages over all languages.\nnumber. Empirically we use 500 meta iterations\nper meta-step language for NLI and 100 meta it-\neration per meta-step language for QA. Similarly,\nwe ﬁx the few-shot ﬁnetuning epoch to 1 epoch for\nboth NLI and QA for fair comparison.\n4.3\nResults\nNLI\nWe employ zero-shot and few-shot learn-\ning for XNLI dataset with XLA-MAML and re-\nport the results in Table 3 and Figure 2. In Ta-\nble 3, the scores are reported by 3 run average\non XNLI test set.\nFor fair comparison to X-\nMAML Nooralahzadeh et al. (2020), that they rely\non a weaker MNLI pretrained model, we use their\npublic ﬁnetuning code and load a MNLI pretrained\nmodel trained with same hyperparameters with our\nbaseline, which achieved higher performance than\nthe scores reported in Nooralahzadeh et al. (2020).\nWe compare results of XLA-MAML and X-\nMAML only using one auxiliary language develop-\nment set as auxiliary data. To simulate fast adaption\nfrom high resource en to low resource language,\nwe use en for the inner-step update and aux. lang\nfor the meta-step update. Here the we use the de-\nvelopment set of XNLI en for inner step which is a\nsubset of MNLI development set. The experimen-\ntal results show no statistical signiﬁcant difference\nin drawing samples from XNLI or MNLI English\ndevelopment set. The results show that our method\ncan achieve comprehensive improvement over the\nlanguages. Nooralahzadeh et al. (2020) reported\nhi to be the most beneﬁcial auxiliary language for\ntheir algorithm, XLA-MAML achieves further im-\nprovement compared to X-MAML with the sample\ncross-lingual data resource. To be noted, under our\nsetting hi is not the most beneﬁcial auxiliary lan-\nguage. We show the zero-shot evaluation results\nof all auxiliary language and target language pairs\nin Figure 2. We observe the auxiliary languages\nbeneﬁcial for zero-shot scores for up to 5.4% ac-\ncuracy for some language pairs such as ur →hi.\nSigniﬁcant improvement was achieved for all aux-\niliary languages except for en, where is reasonable\nthat no cross-lingual adaption is performed and\nalso sw, which matches with the results reported\nby Nooralahzadeh et al. (2020), where X-MAML\nlowered the scores with sw as auxiliary language.\nWe consider this caused by the typological com-\nmonalities of sw.\nWe also report their result of two auxiliary lan-\nguage pairs. We reproduce the best auxiliary lan-\nguage pairs results reported in Nooralahzadeh et al.\n(2020). We show the corresponding results, which\nthe two languages either as inner-step update or\nmeta-step update. We also show the best language\npairs results for XLA-MAML.\nFor few-shot learning learning, we show results\nof two settings. In the one auxiliary and two aux-\niliary setting, since we have a development set on\nthe target language for ﬁnetuning, we use tgt. lang\nas the inner-step and aux. lang as the meta-step.\nUnder few-shot setting, with the supervision of\ntarget language, the effect of auxiliary languages\nMethod\nar\nde\nen\nes\nhi\nvi\nzh\nZero-shot Cross-lingual Transfer\nmBERT (Our baseline)\n48.88/34.73\n66.69/52.42\n80.41/67.11\n69.13/54.69\n42.63/29.85\n55.64/42.04\n56.04/42.46\nOne aux. lang.\nX-MAML hi →X\n53.99/41.09\n69.15/ 55.52\n79.32/ 65.97\n70.15/ 56.08\n-\n59.68/ 46.84\n61.87/ 48.16\nXLA-MAML hi →X\n55.05/42.81\n69.87/57.10\n79.69/66.63\n70.72/57.53\n-\n62.28/50.28\n62.82/49.93\nFew-shot Cross-lingual Transfer\nmBERT (Ours)\n56.28/44.48\n72.28/60.02\n81.08/68.01\n72.34/58.98\n51.98/40.08\n62.47/50.06\n63.80/50.67\nOne aux. lang.\nXLA-MAML hi→X\n57.96/46.29\n72.39/59.57\n80.86/67.64\n72.70/59.34\n58.60/47.05\n64.29/52.16\n65.49/52.77\nMachine translate\nLewis et al. (2020) test\n33.6 / 20.4\n57.9 / 41.8\n80.2 / 67.4\n65.4 / 44.0\n23.8 / 18.9\n58.2 / 33.2\n44.2 / 20.3\nLewis et al. (2020) train\n51.8 / 33.2\n62.0 / 47.5\n77.7 / 65.2\n53.9 / 37.4\n55.0 / 40.0\n62.0 / 43.1\n61.4 / 39.5\nTable 4: Accuracy results on the MLQA test set for zero-shot and few-shot XLA-MAML. Each columns indicate\nthe target language.\nare limited for all MAML methods. The search\nof best language for few-shot learning setting is\nenormous. We compare with the best auxiliary lan-\nguage setting of X-MAML and achieve higher or\ncomparable results. Under few-shot setting, with\nthe supervision of target language, the effect of aux-\niliary languages are limited for all MAML methods.\nIn addition, we report the results from Devlin et al.\n(2019) that use machine translation at test time\n(TRANSLATE-TEST) and results from Wu and\nDredze (2019) that use machine translation at train-\ning time (TRANSLATE-TRAIN) for reference. In\nTRANSLATE-TRAIN, 433k translated sentences\nare used for ﬁne-tuning.\nQA\nWe employ zero-shot and few-shot learning\nfor MLQA dataset with XLA-MAML and report\nresults in Table 4.We use similar settings as NLI for\nQA results. The code is not available for X-MAML\nQA and the paper did not report results on mBERT,\nthat we reproduce their results with our own imple-\nmentation. We report results for one-auxiliary lan-\nguage hi . Few-shot learning was not investigated\nby X-MAML in Nooralahzadeh et al. (2020), that\nwe show our results comparing to few-shot baseline.\nSimilar to NLI, we report TRANSLATE-TEST and\nTRANSLATE-TRAIN Lewis et al. (2020) which\nare strong baselines, where TRANSLATE-TRAIN\nis ﬁnetuned with 100k translated examples.\n5\nAnalysis\n5.1\nResults on XLM-R\nConneau et al. (2020) introduced XLM-R, which\nuses larger models and more data than mBERT to\npretrain an mMLM. We report zero-shot evalua-\ntion results on XLM-Rbase to show that our algo-\nrithm comprehensively achieves improvement on\nstronger baselines. We show the results on XNLI\nModel\naux.\nar\nes\nsw\nvi\nzh\navg.\nXLM-Rbase\n-\n71.71 78.56 65.53 74.41 72.08 73.96\nXLA-MAML hi\n73.63 79.54 67.15 77.36 76.24 75.81\nX-MAML\nhi\n72.87 79.04 65.07 76.49 76.41 75.34\nXLA-MAML bg+de 73.21 79.50 66.23 76.68 76.68 75.65\nTable 5: Results on XLM-Rbase baseline. aux. stands\nfor the auxiliary languages used for training. The avg.\nscore is reported on 15 languages average.\nTask Method\nar\nde\nen\nes\nhi\nvi\nzh\nNLI\nhi r.\n66.17 73.11 81.64 76.03\n-\n71.87 72.91\nhi c.\n67.78 74.23 82.04 76.95\n-\n73.53 73.43\nbg+de r. 66.90\n-\n81.26 75.45 62.83 72.43 72.28\nbg+de c. 67.60\n-\n81.52 76.20 62.95 72.79 72.51\nQA\nhi r.\n53.96 68.78 80.17 70.41\n-\n60.66 62.69\nhi c.\n53.99 69.87 79.69 70.72\n-\n62.28 62.82\nar+hi r.\n-\n71.33 80.18 71.87\n-\n62.77 63.57\nar+hi c.\n-\n71.37 80.27 71.94\n-\n63.45 63.72\nTable 6: Results of different sampling strategies for\nmeta learning.\nThe language codes in ‘Method’ de-\nnotes the languages used for XLA-MAML. ‘r.’ denotes\nrandom sampling. ‘c.’ denotes covering all examples.\nin Table 5. As we can see our model out-performs\nthe zero-shot baseline for around 2% acc. in av-\nerage, and also exceeds XMAML baseline with\nhi, demonstrating the generalization ability of our\nmodel on stronger mMLM baselines.\n5.2\nEffects of Sampling Strategy\nWe report the results for sampling strategy in Ta-\nble 6. Since the development set for XLA-MAML\nis relatively small, unlike in few-shot learning the\ndata for MAML is larger, we consider it impor-\ntant to cover all of the development data during the\nmeta learning update. We use compare two sam-\npling strategies, the ﬁrst strategy randomly samples\na batch of examples at each meta learning episode,\nthe second strategy preprocesses the batches that\nMethod\nar\nde\nen\nes\nhi\nvi\nzh\nhi non.\n67.22 72.91 82.00 76.67\n-\n72.79 72.08\nhi p.\n67.78 74.23 82.04 76.95\n-\n73.53 73.43\nbg+de non. 66.97\n-\n81.14 75.95 63.15 72.63 73.21\nbg+de p.\n67.60\n-\n81.52 76.21 62.95 72.79 72.51\nTable 7: Results of parallelism for meta learning. The\nlanguage codes in ‘Method’ stands for the languages\nused for XLA-MAML. ‘non.’ stands for non-parallel\ntraining. ‘p.’ stands for parallel training.\nthey cover all of the examples. We show results\non one auxiliary language and one auxiliary lan-\nguage bi-pair. There is a signiﬁcant performance\ndrop when the samples are randomly drawn at each\nepisode, reﬂects the importance of making full use\nof all data for low-resource transfer learning.\n5.3\nEffects of Parallel Data\nParallel data is reported useful for cross-lingual\nrepresentation alignment (Lample et al., 2018) and\nmMLM pretraining (Lample and Conneau, 2019).\nThe XNLI dataset is completely parallel, that it is\npossible for us to investigate the effectiveness of\nparallelism of inner-step and meta-step. The re-\nsults on one auxiliary language and one auxiliary\nlanguage bi-pair are shown in Table 7. We can\nsee that parallelism gains all-round but margin im-\nprovement. Taking account that the parallel data is\nrelatively tiny, we consider parallelism useful for\nXLA-MAML training.\n6\nRelated Work\n6.1\nCross-lingual Natural Language\nUnderstanding\nResearch of cross-lingual natural language under-\nstanding has recently been investigated with the\nrelease of datasets and benchmarks (Liang et al.,\n2020; Hu et al., 2020) across a wide range of\ntasks. Alignment by translating the training data to\nthe target language (Wu and Dredze, 2019) or the\ntesting data to the source language (Devlin et al.,\n2019) stands for strong baselines. Cross-lingual\napproaches rely on sharing a encoder layer for mul-\ntilingual languages, by aligning representations to a\nshared vector space (Lample et al., 2018; Xie et al.,\n2018; Artetxe and Schwenk, 2019) or implicitly\njointly multiple language training of monolingual\nMasked Language Models (Devlin et al., 2019;\nLample and Conneau, 2019; Conneau et al., 2020),\nwith or without multilingual parallel data.\n6.2\nMeta Learning for Natural Language\nProcessing\nMeta Learning has demonstrated effectiveness for\nboth monolingual NLP and cross-lingual NLP. Pre-\nvious studies applied meta Learning to few-shot\nlearning and cross-domain transfer learning. Han\net al. (2018) investigated meta Learning on the task\nof few shot relation extraction, Xie et al. (2018)\nstudied meta learning for few-shot intent classiﬁ-\ncation. Dou et al. (2019) validated Meta Learning\nmethods on the GLUE benchmark. Larson et al.\n(2019) and Yu et al. (2018) applied cross-domain\ntransfer learning for classiﬁcation with meta learn-\ning, where each domain is considered as a task.\nFor cross-lingual NLP, Gu et al. (2018) demon-\nstrated the effectiveness of Meta Learning for\nNeural Machine Translation over strong baselines\nof cross-lingual transfer learning. By consider-\ning language pairs as tasks, MAML has shown\nstrong results with as few as 600 parallel sen-\ntences. Nooralahzadeh et al. (2020) proposed X-\nMAML, which is an MAML based method with\nauxiliary languages data as tasks, which uses the\ndata of auxiliary languages in both the inner-step\nand meta-step. While they achieved improvement\nover mMLM baselines, their meta learning proce-\ndure cannot perform cross-lingual adaption. Our\nexperimental results show how our method gains\nsuperiority over X-MAML.\n7\nConclusion\nIn this paper, we propose XLA-MAML, a cross-\nlingual adaption-based meta learning method using\nauxiliary languages to improve cross-lingual trans-\nfer. The algorithm directly performs cross-lingual\nadaption by using different languages in the sup-\nport set and the query set during the meta learning\nprocedure, which encourages the meta-learning al-\ngorithm to capture cross-lingual shared knowledge\namong the auxiliary language and source language,\nand further improve target language performance.\nThe experiments on cross-lingual NLI and QA on\nmBERT and XLM-R show the effectiveness of our\nmethod, especially under zero-shot setting. Under\nfew-shot setting, while the supervision of the target\nlanguage weakens the effect of auxiliary languages,\nour method still achieves comparable or better re-\nsults. We also investigate various XL-speciﬁc set-\ntings for meta learning, including the sampling\nstrategy and the usage of parallelism.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019.\nMas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Trans. Assoc.\nComput. Linguistics, 7:597–610.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nIn\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nZi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.\n2019.\nInvestigating meta-learning algorithms for\nlow-resource natural language understanding tasks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1192–\n1197, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In International Conference on Ma-\nchine Learning, pages 1126–1135. PMLR.\nEdward Grefenstette, Brandon Amos, Denis Yarats,\nPhu Mon Htut, Artem Molchanov, Franziska Meier,\nDouwe Kiela, Kyunghyun Cho, and Soumith Chin-\ntala. 2019.\nGeneralized inner loop meta-learning.\narXiv preprint arXiv:1910.01727.\nJiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,\nand Kyunghyun Cho. 2018. Meta-learning for low-\nresource neural machine translation.\nIn Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 3622–3631,\nBrussels, Belgium. Association for Computational\nLinguistics.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel:\nA large-scale supervised few-shot relation classiﬁca-\ntion dataset with state-of-the-art evaluation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4803–\n4809, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020.\nXtreme:\nA massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In International Conference on Machine\nLearning, pages 4411–4421. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization.\nIn 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nGregory Koch, Richard Zemel, and Ruslan Salakhutdi-\nnov. 2015. Siamese neural networks for one-shot im-\nage recognition. In ICML deep learning workshop,\nvolume 2. Lille.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nAdvances in\nNeural Information Processing Systems (NeurIPS).\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018. Unsupervised ma-\nchine translation using monolingual corpora only.\nIn 6th International Conference on Learning Rep-\nresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceed-\nings. OpenReview.net.\nStefan Larson, Anish Mahendran, Joseph J Peper,\nChristopher Clarke,\nAndrew Lee,\nParker Hill,\nJonathan K Kummerfeld, Kevin Leach, Michael A\nLaurenzano, Lingjia Tang, et al. 2019.\nAn eval-\nuation dataset for intent classiﬁcation and out-of-\nscope prediction. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1311–1316.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. Mlqa: Evaluat-\ning cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7315–\n7330.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Daniel Campos, Ran-\ngan Majumder, and Ming Zhou. 2020. XGLUE: A\nnew benchmark datasetfor cross-lingual pre-training,\nunderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019.\nDecou-\npled weight decay regularization.\nIn 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nTsendsuren Munkhdalai and Hong Yu. 2017.\nMeta\nnetworks. In International Conference on Machine\nLearning, pages 2554–2563. PMLR.\nAlex Nichol, Joshua Achiam, and John Schulman.\n2018.\nOn ﬁrst-order meta-learning algorithms.\nCoRR, abs/1803.02999.\nFarhad Nooralahzadeh, Giannis Bekoulis, Johannes\nBjerva, and Isabelle Augenstein. 2020.\nZero-shot\ncross-lingual transfer with meta learning.\nIn Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP),\npages 4547–4562.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019.\nPy-\ntorch: An imperative style, high-performance deep\nlearning library.\nIn H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nSachin Ravi and Hugo Larochelle. 2017.\nOptimiza-\ntion as a model for few-shot learning. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick,\nDaan Wierstra, and Timothy Lillicrap. 2016. Meta-\nlearning with memory-augmented neural networks.\nIn International conference on machine learning,\npages 1842–1850. PMLR.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017.\nPrototypical networks for few-shot learning.\nIn\nProceedings of the 31st International Conference\non Neural Information Processing Systems, pages\n4080–4090.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang,\nPhilip HS Torr, and Timothy M Hospedales. 2018.\nLearning to compare: Relation network for few-shot\nlearning.\nIn Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n1199–1208.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Ko-\nray Kavukcuoglu, and Daan Wierstra. 2016. Match-\ning networks for one shot learning. In Proceedings\nof the 30th International Conference on Neural In-\nformation Processing Systems, pages 3637–3645.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020a. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nThomas Wolf, Quentin Lhoest, Patrick von Platen,\nYacine Jernite, Mariama Drame, Julien Plu, Julien\nChaumond, Clement Delangue, Clara Ma, Abhishek\nThakur, Suraj Patil, Joe Davison, Teven Le Scao,\nVictor Sanh, Canwen Xu, Nicolas Patry, Angie\nMcMillan-Major, Simon Brandeis, Sylvain Gugger,\nFrançois Lagunas, Lysandre Debut, Morgan Funtow-\nicz, Anthony Moi, Sasha Rush, Philipp Schmidd,\nPierric Cistac, Victor Muštar, Jeff Boudier, and\nAnna Tordjmann. 2020b. Datasets. GitHub. Note:\nhttps://github.com/huggingface/datasets, 1.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nJiateng Xie, Zhilin Yang, Graham Neubig, Noah A.\nSmith, and Jaime Carbonell. 2018.\nNeural cross-\nlingual named entity recognition with minimal re-\nsources.\nIn Conference on Empirical Methods in\nNatural Language Processing (EMNLP), Brussels,\nBelgium.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni\nPotdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text clas-\nsiﬁcation with multiple metrics. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1206–1215.\nA\nExperimental Details\nFor NLI, we follow the standard sentence pair clas-\nsiﬁcation input method for pretrained MLMs (De-\nvlin et al., 2019). We add a special separator token\n[SEP] to the end of each sentence sequence and\nconcatenate them together. A special classiﬁcation\ntoken [CLS] is added in front of every input ex-\nample. The token sequence is then encoded by the\nmMLM. At the output, the [CLS] representation\nis fed into an output layer for classiﬁcation. inputs\nand outputs into the pretrained model and ﬁnetune\nall the parameters end-to-end.\nFor QA, we formulate the task into a sequence\ntagging task (Devlin et al., 2019). For encoding,\nwe represent the input question and passage as a\nsingle packed sequence. Similar to NLI, we add\na special separator token [SEP] to the end of the\nquestion and the support passage and then concate-\nnate them together. A unused special classiﬁcation\ntoken [CLS] is added in front of every input ex-\nample to align the input with the MLM training\nphrase. For ﬁnetuning, we introduce a start vector\nS min RH and an end vector E min RH. The prob-\nability of a token to be the start token is calculated\nby the softmax of a dot product between S and the\ntoken representation Ti: P(i = start) =\neS∗Ti\nP eS∗Ti\nand the end token probability P(j = end) vice\nversa. The ﬁnal score of a candidate span (i, j) is\ndeﬁned as Ti∗S+Tj∗E, where j ≥i. The training\nobjective is the sum of the log-likelihoods of the\ncorrect start and end positions. During evaluation,\nif the best prediction is an invalid answer where\nj < i, we search for the k-best prediction until the\nprediction is valid.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-11-10",
  "updated": "2021-11-10"
}