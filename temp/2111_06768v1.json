{
  "id": "http://arxiv.org/abs/2111.06768v1",
  "title": "A Spiking Neuron Synaptic Plasticity Model Optimized for Unsupervised Learning",
  "authors": [
    "Mikhail Kiselev"
  ],
  "abstract": "Spiking neural networks (SNN) are considered as a perspective basis for\nperforming all kinds of learning tasks - unsupervised, supervised and\nreinforcement learning. Learning in SNN is implemented through synaptic\nplasticity - the rules which determine dynamics of synaptic weights depending\nusually on activity of the pre- and post-synaptic neurons. Diversity of various\nlearning regimes assumes that different forms of synaptic plasticity may be\nmost efficient for, for example, unsupervised and supervised learning, as it is\nobserved in living neurons demonstrating many kinds of deviations from the\nbasic spike timing dependent plasticity (STDP) model. In the present paper, we\nformulate specific requirements to plasticity rules imposed by unsupervised\nlearning problems and construct a novel plasticity model generalizing STDP and\nsatisfying these requirements. This plasticity model serves as main logical\ncomponent of the novel supervised learning algorithm called SCoBUL (Spike\nCorrelation Based Unsupervised Learning) proposed in this work. We also present\nthe results of computer simulation experiments confirming efficiency of these\nsynaptic plasticity rules and the algorithm SCoBUL.",
  "text": "A Spiking Neuron Synaptic Plasticity \nModel Optimized for Unsupervised \nLearning \nA Preprint \nMikhail Kiselev \nChuvash State University \nCheboxary, Russia \nmkiselev@chuvsu.ru \nNovember 2, 2021 \nAbstract \nSpiking neural networks (SNN) are considered as a perspective basis for performing all kinds of learning \ntasks – unsupervised, supervised and reinforcement learning. Learning in SNN is implemented through \nsynaptic plasticity – the rules which determine dynamics of synaptic weights depending usually on \nactivity of the pre- and post-synaptic neurons. Diversity of various learning regimes assumes that \ndifferent forms of synaptic plasticity may be most efficient for, for example, unsupervised and supervised \nlearning, as it is observed in living neurons demonstrating many kinds of deviations from the basic spike \ntiming dependent plasticity (STDP) model. In the present paper, we formulate specific requirements to \nplasticity rules imposed by unsupervised learning problems and construct a novel plasticity model \ngeneralizing STDP and satisfying these requirements. This plasticity model serves as main logical \ncomponent of the novel supervised learning algorithm called SCoBUL (Spike Correlation Based \nUnsupervised Learning) proposed in this work. We also present the results of computer simulation \nexperiments confirming efficiency of these synaptic plasticity rules and the algorithm SCoBUL. \nKeywords: spike timing dependent plasticity, unsupervised learning, winner-takes-all network \n1 \nIntroduction and motivation \nAt present, the sound expectations exist that spiking neural networks (SNN) used as a basis for creation of \nnew generation of artificial intelligence systems may extend significantly their functionality and \napplication area comparatively to intelligent systems based on traditional neural networks. It has been \nproven experimentally that hardware implementations of SNN consume several orders of magnitude less \nenergy than traditional neural network on comparable tasks [1]. Asynchronous nature of SNN operation \nleads to potentially unlimited scalability of SNN-based systems [2] as it is demonstrated on the modern \nneurocomputers like SpiNNaker [3]. Non-trivial involvement of signal transmission delays in information \nprocessing by SNN makes this type of neural networks suitable for processing dynamic data streams [4]. \nHowever, three is still a number of unsolved scientific problems in the field of SNN which hinder wide \napplication of SNN to practical problems. Probably, the hardest of them is the problem of SNN learning.  \nLearning algorithms of traditional neural networks are based on the fact that they can be represented as \nsmooth multi-dimensional functions. Their output values depend smoothly on the input values as well as \non their synaptic weights. It makes possible to use gradient descent methods for their training treated as \noptimization of their synaptic weights. The error backpropagation algorithm based on gradient descent is \na well-known platform for majority of approaches to learning of traditional neural networks. In contrast, \nSNNs are discrete systems by their nature. The “all-or-nothing” behavior of spiking neurons makes direct \napplication of gradient methods impossible. This problem can be overcome to some extent with help of \nthe various “surrogate gradient” methods [5] but this solution is considered by many researchers as partial \nand not exploiting the natural advantages of SNNs. Seemingly, the more adequate approach to SNN \nlearning is based on reproduction of the synaptic plasticity principles observed in living neuronal \nensembles, the principles that utilize the basic concepts of SNN – asynchronous operation of neurons and \nspiking nature of information exchange between them. These principles include the locality principle \nstipulating that rules for synaptic weight adjustment can include only parameters of activity of the pre- \nand post-synaptic neurons. A particular case of this general principle is the well-known Hebbian plasticity \nrule in accordance with which the synapses frequently receiving spikes short time before postsynaptic \nspike generation are potentiated while the synapses with uncorrelated pre/post-synaptic activity are \nsuppressed or are not modified. \nThe locality principle is very general and does not fix exact relationship between weight dynamics and \npre/post-synaptic activity characteristics. For this reason, a plenty of very different SNN synaptic \nplasticity rules have been proposed, that makes the situation with SNN learning strikingly different from \nthe uniform approach to learning in traditional neural networks. The majority of these rules can be \nconsidered as generalizations of spike timing dependent plasticity (STDP) – the synaptic plasticity rule, \nexperimentally observed in living neurons [6]. The pure “classic” STDP can hardly be used as a basis of \nimplementation of learning in SNN, especially in recurrent SNN, due to its inherent instability – in \naccordance with STDP, a potentiated synapse automatically gets more chances to be potentiated further \ncausing the “runaway” network behavior. It made researchers to invent STDP generalizations [7 - 9] \nleading to more balanced network dynamics. Further neurophysiological studies also showed that \nsynaptic plasticity in different kinds of neurons in different brain regions are described by various \nplasticity models sometimes greatly declining from the classic STDP [10]. \nThis fact enables us to think that different plasticity models are adequate for different tasks. In fact, even \nin the realm of traditional neural networks, network architectures and synaptic weight tuning algorithms \nare different for, say, supervised and unsupervised learning. While the layered feed-forward networks are \ncommonly used for supervised learning, for unsupervised learning, the flat networks with (implicit) \nlateral inhibition like Kohonen SOM [11] proofed their efficiency. \nThis thesis determined the motivation for this study. We would like to find SNN plasticity rules satisfying \nthe locality principle and optimized for solution of one class of learning problem, namely, unsupervised \nlearning. In the next Section, I formalize SNN unsupervised learning as a problem of finding spike \nfrequency correlations. Further, I describe the novel synaptic plasticity model and the unsupervised \nlearning algorithm SCoBUL based on it and show that it fits the specific requirements imposed by this \nrepresentation of the unsupervised learning problem. In Section 4, a proof-of-concept level experimental \nevidence of SCoBUL efficiency using emulated DVS camera signal is presented. \n2 \nUnsupervised learning in spiking domain \nThe problem of supervised learning in the most general case can be formulated as a search for certain \nfeatures in the given dataset which distinguish it from the dataset with the identical statistical parameters \n(such as mean or standard deviation) but where each value is generated by the respective random number \ngenerator independently of other values. Nothing can be learnt from the data where each individual value \nis generated by independently working random number generators. Presence of some hidden structure, \npatterns which can be recognized by supervised learning algorithms is indicated, in the general case, by \nincreased (or decreased) probability that certain values appear in certain places in the dataset \ncomparatively to the situation when all data are completely random. These probability deviations can be \nexpressed in terms of correlations (or anti-correlations) between certain variables included in the dataset \nor calculated as derivative variables. This view on unsupervised learning covers temporal patterns, as well \n– in the form of the correlations between a variable and the explicit time coordinate considered as an \nadditional variable of the dataset or the correlations between the current value of a variable and the values \nof some variables (including this variable itself – in case of autocorrelations) in the past.  \nAll this remains valid for data represented as spike sequences, but in this case the data values are \nextremely simple – they are in fact Boolean (spike / no spike). Thus, we can say that unsupervised \nlearning problem for SNN can be formulated without loss of generality as a problem of detecting \ncorrelations between spike frequencies in input spike trains. Detecting of anti-correlations is also covered \nby this approach due to use of inhibitory neurons capable of implementing the logical operation NOT. If \nnetwork includes the excitatory neuron A that would be constantly active unless it is not suppressed by \nthe inhibitory neuron B then the spike sequence generated by the neuron A can be considered as a result \nof logical NOT applied to the input signal activating the neuron B. Then, the signals anti-correlated with \nB’s input signal are correlated with A’s output signal. Temporal (auto)correlations and correlations with \ntime lag can be detected in the same way as for correlations between current spike frequencies if a \nnetwork with some memory mechanism is used. For example, the well-known technique called liquid \nstate machine (LSM) [12] utilizes this idea. LSM includes a large chaotic recurrent SNN whose role is to \nconvert temporal dynamics of input signal into a very high dimensional representation in the form of \ncurrent firing frequencies of its neurons. It is possible because reaction of SNN to an external stimulus \nmanifested in terms of neuron firing frequencies may be observed during long time after the stimulus \npresentation. \nIt follows from the discussion above that, in case of SNN, the problem of unsupervised learning can be \nrepresented as a problem of detection of spike frequency correlations for spikes emitted by input nodes or \nneurons of the network. Let us note that time scales of these correlations may be different – from exact \ncoincidence of firing times to cases of concurrent elevation of mean spike frequency measured for long \ntime intervals. \nIt should be noted that such an understanding of unsupervised learning is very natural for spiking neurons. \nIndeed, the most basic operation characteristic for all spiking neuron models is detection of coinciding \narrival of spikes to its synapses. Only when several synapses with sufficiently great weights receive spike \ninside more or less narrow time window, the neuron fires indicating this fact. \nIn order to define the solved problem formally, we consider the following simplified but still quite general \ninput signal model. We assume that the input signal is generated by N + 1 Poissonian processes. One of \nthem works always and plays the role of background noise. Let us denote its intensity as p0. The other \nprocesses numbered by the index i are switched on randomly with the probability Pi and operate during \nthe time interval ti. During this time interval a certain set of input nodes (we will call it cluster) Ci emit \nspikes with the probability p0 + pi. This elevated activity of cluster’s nodes will be called pattern. \nEvidently, the activity of input nodes inside every cluster i is correlated and statistical significance of this \ncorrelation is determined by pi and the number of activations of this cluster in the whole observed input \nsignal ni. The goal of unsupervised learning is to teach SNN to react specifically to these patterns in the \ninput spike stream. Namely, due to the appropriate synaptic plasticity rules, for each cluster, a recognizing \nneuron should appear in the network. This neuron should fire when and only when the respective cluster \nis active. Thus, our problem is parametrized by the value of p0 and N corteges <Ci, ni, ti, pi>. \n3 \nThe algorithm SCoBUL - network, neuron, synaptic plasticity. \nIn this work, I describe a novel SNN unsupervised learning algorithm approaching the unsupervised \nlearning problem from the positions of spike frequency correlations. The algorithm is called SCoBUL \n(Spike Correlation Based Unsupervised Learning). An application of SNN to any problem related to \nlearning includes three major logical components: network architecture, neuron model and plasticity rule. \nThe novelty of the present work and the algorithm SCoBUL belongs mainly to the third component; \nwhile the network structure and the neuron model used here are quite common. \nSimilar to the majority of studies devoted to unsupervised learning, I utilize the so-called winner-takes-all \n(WTA) network architecture [13]. It is a one-layer SNN where every neuron is connected to a certain \nsubset of input nodes (possibly, to all of them) thru excitatory links and has strong lateral inhibitory \nprojections to the other neurons. This structure can be considered as a spiking analogue of Kohonen’s \nself-organizing map [11], a very efficient architecture of traditional neural networks used in unsupervised \nlearning tasks. The general idea of WTA is the following. Every neuron due to the respectively selected \nplasticity model tries to detect sets of input nodes with coinciding activity periods. At the same time, a \nneuron having learnt successfully to recognize such a group of correlated input nodes inhibits recognition \nof the same group by the other neurons blocking their activity by its inhibitory spikes emitted during \nactivation of this group. Many extensions of this simple architecture have been proposed (for example, 2-\nlayer WTA networks [14]) but, as it was said above, the direction of our movement is enhancement of the \nsynaptic plasticity model. \nOnly one important novelty related to network structure is introduced in this work – the network structure \nis variable – the neurons may die and be born again (or migrate, if you like…). Neuron may die if it is \nconstantly inhibited and cannot fire for a long time. In this case, it is destroyed and re-created by the same \nprocedure which was used for construction of the original neuron population at the beginning of the \nsimulation. Due to this feature, a neuron, inhibited by its happier neighbors having managed to recognize \nthe most significant correlations in the input signal, gets a chance to be resurrected with a new \ncombination of synaptic weights, which could help it to recognize some still “unoccupied” weakly \ncorrelated input node set  \nThe neuron model utilized is also very simple, probably, the simplest spiking neuron model used in \nresearch and applications. It is leaky integrate-and-fire (LIF) neuron [15]. Its simplicity makes it \nefficiently implementable on the modern digital (TrueNorth [16], Loihi [17]) and even analog \n(BrainScaleS[18], NeuroGrid [19]) neurochips. I also use the simplest synapse model – current based \ndelta-synapse. When such a synapse receives spike, it immediately increases (or decreases – if it is \ninhibitory) neuron’s membrane potential by the value equal to its weight. \nThe SCoBUL synaptic plasticity model can be called a generalization of STDP but it is generalized and \nmodified in several directions. Below, we will consider them and discuss how they help solve the \nunsupervised learning problem formulated in the previous section. Similar to the classic STDP, weight \nmodifications in SCoBUL also depend on relative timing of pre- and post-synaptic spikes and this \ndependence includes a temporal parameter τP determining length of time interval inside which the pairs of \nspikes are considered as interrelated and can change synaptic weight. Describing the SCoBUL plasticity \nmodel below, I use the notion of plasticity period. Plasticity period is a time interval of length 2τP \ncentered at the moment of the postsynaptic spike but only if this postsynaptic spike is emitted after τP or \nmore time since the center of the previous plasticity period. It is important to note also that inhibitory \nconnections are not plastic in this model. \n3.1 Synaptic resource \nThe classic form of STDP has additive character. In accordance with STDP, synaptic weight is increased \nor decreased by a certain value depending on relative position of the pre- and post-synaptic spikes on the \ntime axis. If this rule is applied without any restrictions or corrections then it can easily lead to senseless \nvery high positive (or negative) weights due to STDP’s inherent positive feedback. To prevent this, the \nvalues of synaptic weights are bounded artificially by a certain value from above and by zero from below. \nIt solves the problem of unlimited synaptic weights but causes another problem – of catastrophic \nforgetting. Indeed, let us imagine that network was being trained to recognize something for a long time. \nAs a result, the majority of synaptic weights became either saturated (equal to the maximum possible \nvalue) or suppressed (equal to 0). However, presentation of even few wrong training examples or \nexamples containing other patters or simply noise is sufficient to destroy the weight configuration learnt \nand nothing can prevent it. The network will forget everything it has learnt. In order to fight this problem, \nit was proposed in my several earlier works [20] to apply additive plasticity rules to the so-called synaptic \nresource instead of the synaptic weight. The value of synaptic resource W depends monotonously on the \nsynaptic weight w in accordance with the formula \n\n)\n0,\nmax(\n)\n0,\nmax(\n)\n(\nmin\nmax\nmin\nmax\nmin\nW\nw\nw\nW\nw\nw\nw\nw\n\n\n\n\n\n\n\n\n\n\nIn this model, the weight values lay inside the range [wmin, wmax) - while W runs from -∞ to +∞, w runs \nfrom wmin to wmax. When W is either negative or highly positive, synaptic plasticity does not affect a \nsynapse’s strength. Instead, it affects its stability – how many times the synapse should be potentiated or \ndepressed to move it from the saturated state. Thus, to destroy the trained network state, it is necessary to \npresent the number of “bad” examples close to the number of “good” examples used to train it. It should \nbe noted that this feature was found to be useful not only for unsupervised learning – we use it in all our \nSNN studies. Let us add that in the present research wmin is set equal to 0 everywhere. \n3.2 Unconditional synapse depression \nWhen a synapse receives spike, its synaptic resource is decreased by the constant value d- but this \ndecrease can happen at most once inside any time window of length 2τP. Why this simple rule is useful, \nwe will see later – when other features of the SCoBUL model will be discussed. \n3.3 Constant symmetric STDP \nIn my model, all presynaptic spikes arriving inside a plasticity period strengthen the synapse. However, a \nsynapse can be potentiated at most once inside one plasticity period – by the spike coming first. It should \nbe stressed that the relative order of pre- and post-synaptic spikes is not important. When presynaptic \nspike comes just after postsynaptic spike, it potentiates the synapse as well. Thus, this rule can be called \nsymmetric STDP. Besides that, the value of the synaptic resource increment is the constant D+, it does not \ndepend on the exact time difference between pre- and post-synaptic spikes. \n3.4 Suppression of strong inactive synapses \nThis rule is a conceptually new addition to the classic STDP. It states that if a synapse with positive \nresource has not received a spike during current plasticity period it is depressed at its end by the constant \nD-. \n3.5 Constant total synaptic resource \nThe last important logical component of SCoBUL is constancy of neuron’s total synaptic resource. Every \ntime some synapse is potentiated or depressed, the resources of all other synapses are modified in the \nopposite direction and by the same value calculated from the condition that the total synaptic resource of \nthe neuron should remain the same. \n \nNow, having described all logical components of the SCoBUL plasticity model, let us analyze and \nexplain them from point of view of unsupervised learning problem formulated at the end of the previous \nsection. Let us begin from point 3.2. In conjunction with point 3.5, it gives the following very useful \neffect. The classic STDP has many drawbacks, and one of them is uselessness of silent neurons. Indeed, \nin the classic STDP model, the process of weight modification is bound to firing. The neurons which do \nnot fire are not plastic. Therefore, if some neuron is silent because it is constantly inhibited by other \nneurons, it will stay in this state forever, and, therefore, will be just useless burden consuming \ncomputational resources but producing nothing. In my model, combination of rules 3.2 and 3.5 gives the \nfollowing effect. Activity of certain sets of input nodes makes to fire some neurons. Inhibition from these \nactive neurons forces the synapses of silent inhibited neurons connected to the active input nodes \nredistribute their synaptic resource to the other synapses, connected with less active input node groups. \nEven if these weak input node groups could not force to fire any neuron in the initial network \nconfiguration, after this resource redistribution, some silent neurons may accumulate in the respective \nsynapses the amount of synaptic resource sufficient to fire. Thus, this process of “squeezing” synaptic \nresource out of active synapses to less active synapses helps the network recognize all correlations in the \ninput spike streams – not only the most significant ones. \nThe fact that symmetric variant of STDP is more suitable for unsupervised learning than its classic \nasymmetric form is obvious. Indeed, activity period of correlated input node groups may be long. In case \nof possible random delays of spikes inside this activity time window, some of them may appear earlier, \nsome – later. When neuron learns to recognize this group, its synapses connected to these nodes are \nstrengthened. Therefore, it begins to fire earlier when this input node group gets activated. But it means \nthat more of its synapses connected to these nodes will experience depression instead of facilitation. \nThen, further recognition improvement will be impossible. It is evident, that the symmetric STDP does \nnot face this difficulty. \nAt last, point 3.4 solves another hard problem of unsupervised learning. It would be desirable that one \nneuron recognized one cluster, and one cluster were recognized by one neuron. The situation when two \nneurons recognize the same cluster is cured by introduction of stronger mutual inhibition – if the lateral \ninhibition is sufficiently strong, the state when several neurons recognize the same pattern is evidently \nunstable. The problem of recognition of several clusters by one neuron is much harder. Strong lateral \ninhibition cannot help here – instead it can make this undesirable state more probable. Other rules \nconsidered above cannot help as well. The rule 3.4 was designed especially to fight this unpleasant \nscenario. Indeed, if a neuron recognizes the clusters A and B, it means that its synapses connected to A \nand B are strong. Assume that A is active. The neuron fires and therefore, accordingly to rule 3.3, the \nconnections leading to A are potentiated. However, the synapses connected to B have not received spikes \nduring the respective plasticity period and are suppressed by rule 3.4. Thus, rule 3.4 makes the state when \none neuron recognizes several independent correlated input node groups unstable. \nThis discussion demonstrated how rules 3.1 – 3.5 help address various aspects and complications of the \ngeneral problem of unsupervised learning, namely: \n \nto recognize strong and weak correlations by the same network (to prevent situations when there are \nno recognizing neurons for some clusters); \n \nto make recognizing neurons sufficiently specific (to prevent situations when some clusters are \nrecognized by several neurons); \n \nto make recognition unambiguous (to prevent situations when several clusters are recognized by the \nsame neuron). \nIn the next section, we will consider the experimental confirmation of these theses on artificially \ngenerated data imitating a signal from DVS camera. \n4 \nExperimental comparison of STDP and SCoBUL plasticity rules \non an imitated DVS signal. \nIn order to evaluate the benefits of the SCoBUL plasticity comparatively to the standard STDP I decided \nto select a task close to real application of SNN, namely, processing of spiking video signal sent from a \nDVS camera. For this purpose, a program emulator of DVS camera has been created. To simplify and \nspeed up the simulation experiments, I emulated small camera view field of size 20×20 pixels. 3 spike \nstreams (channels) correspond to each pixel. Spike frequency in the first channel is proportional to the \npixel brightness. The other two channels send spikes every time the pixel brightness increases or \ndecreases by a certain value. Thus, the whole input signal includes 1200 spiking channels. The thresholds \nused to convert brightness and brightness changes into spikes are selected so that the mean spike \nfrequency in all channels is close to 30Hz. \nIn these tests, I selected a very simple picture – a light spot moving in the view field of this imaginary \nDVS camera in various directions and with various speed. At every moment, coordinates and speed of \nlight spot (i.e. the point in the 4-dimensional phase space occupied by it) are known. The task is to \ndetermine them from the current activity of the WTA network neurons. \nMore precisely, the procedure is the following. The whole emulation takes 3000000 time steps (we \nassume that 1 time step = 1 msec). The time necessary for the light step to cross the DVS view field is \nseveral hundred milliseconds. During first 2000 sec the network is trained. Next 600 sec are used to \ndetermine the centers of receptive field of every neuron in the phase space. Last 400 sec are broken to 40 \nmsec intervals. For each interval, the real central position of the light spot in the phase space and the \npredicted value of this position are determined. The predicted position is the weighted mean of neuron \nreceptive field centers where the weight is amount of spikes emitted by the given neuron in this time \ninterval. The value of mean squared distance between the real and predicted light spot positions in each \ntime interval serves as a measure of unsupervised learning success. Small value of this difference would \nbe an evidence that network’s neurons learnt to recognize specific positions of the light spot in the phase \nspace. Euclidean metrics in the phase space was selected so that the standard deviation of light spot \ncoordinate during the whole simulation period would be the same for all coordinates.  \nI performed this test with the networks with the standard STDP and the SCoBUL plasticity. In order to \nmake this competition fair I used in both cases the same network parameter optimization procedure based \non genetic algorithm. The parameter variation ranges were the same or equivalent for both plasticity \nmodels; I also made sure that the optimum parameter values found were not close to the boundaries of the \nsearch space. To diminish the probability of accidental bad or good result, I took the criterion values \naveraged for 3 tests with the same hyperparameters but with different sets of initial synaptic weight \nvalues. The population size was 300, mutation probability per individual equaled to 0.5, elitism level was \n0.1. Genetic algorithm terminated when new generation did not show criterion improvement. The \noptimized (minimized) parameter was the mean squared distance between the real and predicted light spot \nposition divided by the mean squared distance between spot position and the centrum of all spot positions \nduring the entire emulation. It is called “Normalized mean squared distance” on Figure 1 showing the \nresults obtained by genetic algorithm. \n \n \nFigure 1. The course of minimization of the light spot position determination error in sequential \ngenerations of genetic algorithm for STDP and SCoBUL synaptic plasticity rules. \n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n1\n2\n3\n4\n5\n6\nNormalized mean squared distance \nGeneration No \nSTDP\nSCoBUL\nWe see that 6 generations were required for SCoBUL networks to reach criterion value stabilization, \nwhile in case STDP the stabilization was reached earlier – after 4 generations; and we see that SCoBUL \nnetworks give much more accurate light spot position/speed determination than STDP networks. While \nthese results should be considered as preliminary and they should be verified in other unsupervised \nlearning tasks, the supremacy of SCoBUL over the classic STDP in this case is undoubted. \n5 \nConclusion. \nIn this paper, the problem of unsupervised learning of SNN was re-formulated as a problem of finding \nspike frequency correlations in input spike streams. Using this approach and remaining inside the \nboundaries of the synaptic plasticity rule locality principle, I propose a modification of the classic STDP \nmodel, which optimizes it for unsupervised learning. Since our research project is oriented primarily to \napplication of SNN to processing of sensory data represented in the spiking form and, as its most \nimportant particular case, to processing of DVS-generated signal, I used an artificially generated “DVS \nsignal” as a benchmark to compare the standard STDP-based WTA network and the SCoBUL network. It \nwas found that the SCoBUL model gives significantly better results. This result can be evaluated as \npromising and opening the way to further perfection of our model while more exact and complete \nmeasurements of its properties and possible limitations are obviously needed. \nThe other goal of this research is creation of hardware-friendly version of STDP-like synaptic plasticity \nmodel. In SCoBUL, this goal is achieved due to the special scenario of application of rules 3.1 – 3.5. \nSome of these rules (3.2 – 3.4) are bound to pre- and post-synaptic spikes and therefore are applied very \noften. However, these rules are very simple – they have the form of addition or subtraction of certain \nconstant values. Rules 3.1 and 3.5 (the synaptic resource renormalization and the calculation of synaptic \nweight from synaptic resource) include much more expensive operations like multiplication and division. \nHowever, it is admissible to apply them periodically with sufficiently long period (say, once per second). \nThus, in general, SCoBUL may have more economic and/or fast implementation than the standard STDP \n– it depends on the concrete processor architecture used. \nAcknowledgements. \nThe present work is a part of the research project in the field of SNN carried out by Chuvash State \nUniversity in cooperation with Kaspersky and the private company Cifrum. \nPreliminary computations resulting in creation of the SCoBUL algorithm were performed on the \ncomputers belonging to Kaspersky, Cifrum and me. Cifrum’s GPU cluster was used for running the \noptimization procedure reported in Section 4.  \nReferences. \n[1] M. Davies et al., \"Advancing Neuromorphic Computing With Loihi: A Survey of Results and \nOutlook,\" in Proceedings of the IEEE, vol. 109, no. 5, pp. 911-934, 2021, doi: \n10.1109/JPROC.2021.3067593. \n[2] S. Carrillo et al., \"Scalable Hierarchical Network-on-Chip Architecture for Spiking Neural Network \nHardware Implementations\", IEEE Transactions on Parallel and Distributed Systems, vol. 24, no. 12, \npp. 2451-2461, 2013, doi: 10.1109/TPDS.2012.289. \n[3] S. B. Furber et al., \"The SpiNNaker Project\". in Proceedings of the IEEE, vol. 102, no 5, pp. 652–\n665, 2014, doi:10.1109/JPROC.2014.2304638 \n[4] J. Pérez et al., “Bio-inspired spiking neural network for nonlinear systems control”, Neural \nNetworks, vol. 104, 2018, pp. 15-25, ISSN 0893-6080, https://doi.org/10.1016/j.neunet.2018.04.002. \n[5] E. O. Neftci, H. Mostafa and F. Zenke, \"Surrogate Gradient Learning in Spiking Neural Networks: \nBringing the Power of Gradient-Based Optimization to Spiking Neural Networks,\" IEEE Signal \nProcessing Magazine, vol. 36, no. 6, pp. 51-63, 2019, doi: 10.1109/MSP.2019.2931595. \n[6] G. Q. Bi, M. M. Poo, “Synaptic modifications in cultured hippocampal neurons: dependence on \nspike timing, synaptic strength, and postsynaptic cell type”, Journal of Neuroscience, vol. 18, no. 24, \n1998, pp. 10464–10472. \n[7] J. Y. Chen et al., “Heterosynaptic plasticity prevents runaway synaptic dynamics”, Journal of \nNeuroscience, vol. 33, no. 40, pp. 15915–15929, 2013 \n[8] F. Zenke, G. Hennequin, and W. Gerstner. “Synaptic plasticity in neural networks needs homeostasis \nwith a fast rate detector”, PLoS Comput. Biol., vol. 9,  no. 11, e1003330, 2013. \n[9] G. G. Turrigiano, “The self-tuning neuron: synaptic scaling of excitatory synapses”, Cell, vol. 135, \nno. 3, pp. 422–435, 2008. \n[10] L.F. Abbott and S.B. Nelson, “Synaptic plasticity: taming the beast”, Nature Neuroscience, vol. 3, \npp. 1178-1183, 2000. \n[11] T. Kohonen, \"Self-Organized Formation of Topologically Correct Feature Maps\". Biological \nCybernetics, vol. 43, no. 1, pp. 59–69, 1982. doi:10.1007/bf00337288. S2CID 206775459 \n[12] W. Maass, “Liquid state machines: Motivation, theory and applications”, World Scientific Review, \nvol., 189, pp. 1-21, 2010. \n[13] W. Maass, “On the computational power of winner-take-all”, Neural Computation, vol. 12, no. 11, \npp. 2519–2535, 2000. \n[14] M. Kiselev and A. Lavrentyev, “A Preprocessing Layer in Spiking Neural Networks – Structure, \nParameters, Performance Criteria”. in Proceedings of IJCNN-2019, Budapest, 2019, paper N-19450. \n[15] W. Gerstner and W. Kistler, “Spiking Neuron Models. Single Neurons, Populations, Plasticity”, \nCambridge University Press, 2002. \n[16] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, “A million spiking-neuron integrated circuit with a \nscalable communication network and interface”, Science, vol. 345, no. 6187, pp. 668–673, 2014. \n[17] M. Davies et al. “Loihi: A Neuromorphic Manycore Processor with On-Chip Learning”, IEEE \nMicro, vol. 38, no. 1, pp. 82-99, 2018. \n[18] https://brainscales.kip.uni-heidelberg.de/ \n[19] B. V. Benjamin et al., \"Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale \nNeural Simulations\", Proceedings of the IEEE, vol. 102, no. 5, pp. 699-716, 2014, doi: \n10.1109/JPROC.2014.2313565. \n[20] M. Kiselev, “Rate Coding vs. Temporal Coding – Is Optimum Between?”, in Proceedings of IJCNN-\n2016, pp. 1355-1359, 2016. \n \n",
  "categories": [
    "cs.NE"
  ],
  "published": "2021-11-12",
  "updated": "2021-11-12"
}