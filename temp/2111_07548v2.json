{
  "id": "http://arxiv.org/abs/2111.07548v2",
  "title": "Unsupervised Lightweight Single Object Tracking with UHP-SOT++",
  "authors": [
    "Zhiruo Zhou",
    "Hongyu Fu",
    "Suya You",
    "C. -C. Jay Kuo"
  ],
  "abstract": "An unsupervised, lightweight and high-performance single object tracker,\ncalled UHP-SOT, was proposed by Zhou et al. recently. As an extension, we\npresent an enhanced version and name it UHP-SOT++ in this work. Built upon the\nfoundation of the discriminative-correlation-filters-based (DCF-based) tracker,\ntwo new ingredients are introduced in UHP-SOT and UHP-SOT++: 1) background\nmotion modeling and 2) object box trajectory modeling. The main difference\nbetween UHP-SOT and UHP-SOT++ is the fusion strategy of proposals from three\nmodels (i.e., DCF, background motion and object box trajectory models). An\nimproved fusion strategy is adopted by UHP-SOT++ for more robust tracking\nperformance against large-scale tracking datasets. Our second contribution lies\nin an extensive evaluation of the performance of state-of-the-art supervised\nand unsupervised methods by testing them on four SOT benchmark datasets -\nOTB2015, TC128, UAV123 and LaSOT. Experiments show that UHP-SOT++ outperforms\nall previous unsupervised methods and several deep-learning (DL) methods in\ntracking accuracy. Since UHP-SOT++ has extremely small model size, high\ntracking performance, and low computational complexity (operating at a rate of\n20 FPS on an i5 CPU even without code optimization), it is an ideal solution in\nreal-time object tracking on resource-limited platforms. Based on the\nexperimental results, we compare pros and cons of supervised and unsupervised\ntrackers and provide a new perspective to understand the performance gap\nbetween supervised and unsupervised methods, which is the third contribution of\nthis work.",
  "text": "1\nUnsupervised Lightweight Single Object\nTracking with UHP-SOT++\nZhiruo Zhou, Student member, IEEE, Hongyu Fu, Student member, IEEE, Suya You,\nand C.-C. Jay Kuo, Fellow, IEEE\nAbstract—An\nunsupervised,\nlightweight\nand\nhigh-\nperformance single object tracker, called UHP-SOT, was\nproposed by Zhou et al. recently. As an extension, we present\nan enhanced version and name it UHP-SOT++ in this work.\nBuilt upon the foundation of the discriminative-correlation-\nﬁlters-based (DCF-based) tracker, two new ingredients are\nintroduced in UHP-SOT and UHP-SOT++: 1) background\nmotion modeling and 2) object box trajectory modeling.\nThe main difference between UHP-SOT and UHP-SOT++\nis the fusion strategy of proposals from three models (i.e.,\nDCF, background motion and object box trajectory models).\nAn improved fusion strategy is adopted by UHP-SOT++\nfor more robust tracking performance against large-scale\ntracking\ndatasets.\nOur\nsecond\ncontribution\nlies\nin\nan\nextensive evaluation of the performance of state-of-the-art\nsupervised and unsupervised methods by testing them\non\nfour\nSOT\nbenchmark\ndatasets\n–\nOTB2015,\nTC128,\nUAV123 and LaSOT. Experiments show that UHP-SOT++\noutperforms\nall\nprevious\nunsupervised\nmethods\nand\nseveral deep-learning (DL) methods in tracking accuracy.\nSince UHP-SOT++ has extremely small model size, high\ntracking performance, and low computational complexity\n(operating at a rate of 20 FPS on an i5 CPU even without\ncode optimization), it is an ideal solution in real-time\nobject tracking on resource-limited platforms. Based on\nthe experimental results, we compare pros and cons of\nsupervised and unsupervised trackers and provide a new\nperspective to understand the performance gap between\nsupervised and unsupervised methods, which is the third\ncontribution of this work.\nIndex Terms—Object tracking, online tracking, single ob-\nject tracking, unsupervised tracking.\nI. INTRODUCTION\nV\nIDEO object tracking is one of the fundamental\ncomputer vision problems. It ﬁnds rich applications\nin video surveillance [1], autonomous navigation [2],\nrobotics vision [3], etc. Given a bounding box on the\ntarget object at the ﬁrst frame, a tracker has to predict\nobject box locations and sizes for all remaining frames in\nonline single object tracking (SOT) [4]. The performance\nof a tracker is measured by accuracy (higher success\nrate), robustness (automatic recovery from tracking loss),\ncomputational complexity and speed (a higher number\nof frames per second of FPS).\nZhiruo Zhou, Hongyu Fu and C.-C. Jay Kuo are with the Ming-Hsieh\nDepartment of Electrical and Computer Engineering, University of\nSouthern California, CA, 90089-2564, USA (e-mails: zhiruozh@usc.edu,\nhongyufu@usc.edu and cckuo@sipi.usc.edu).\nSuya You is with Army Research Laboratory, Adelphi, Maryland,\nUSA (e-mail: suya.you.civ@army.mil).\nOnline trackers can be categorized into supervised\nand unsupervised ones [5]. Supervised trackers based\non deep learning (DL) dominate the SOT ﬁeld in recent\nyears. Some of them use a pre-trained network such\nas AlexNet [6] or VGG [7] as the feature extractor and\ndo online tracking with extracted deep features [8]–[12].\nOthers adopt an end-to-end optimized model which is\ntrained by video datasets in an ofﬂine manner [13], [14]\nand could be adapted to video frames in an online\nfashion [15]–[18]. The tracking problem is formulated as\na template matching problem in siamese trackers [13],\n[14], [19]–[23], which is popular because of its simplicity\nand effectiveness. One recent trend is to apply the Vision\nTransformer in visual tracking [24], [25].\nAlthough DL trackers offer state-of-the-art tracking\naccuracy, they do have some limitations. First, a large\nnumber of annotated tracking video clips are needed in\nthe training, which is a laborious and costly task. Second,\nthey demand large memory space to store the parame-\nters of deep networks due to large model sizes. Third,\nthe high computational power requirement hinders their\napplications in resource-limited devices such as drones\nor mobile phones. Fourth, DL trackers need to be trained\nwith video samples of diverse content. Their capability\nin handling unseen objects appears to be limited, which\nwill be illustrated in the experimental section. In contrast\nwith DL trackers, unsupervised trackers are attractive\nsince they do not need annotated boxes to train track-\ners. They are favored in real-time tracking on resource-\nlimited devices because of lower power consumption.\nAdvanced unsupervised SOT methods often use dis-\ncriminative correlation ﬁlters (DCFs). They were investi-\ngated between 2010 and 2018 [9], [26]–[32]. DCF trackers\nconduct dense sampling around the object box and solve\na regression problem to learn a template for similar-\nity matching. Under the periodic sample assumption,\nmatching can be conducted very fast in the Fourier\ndomain. Spatial-temporal regularized correlation ﬁlters\n(STRCF) [32] adds spatial-temporal regularization to\ntemplate update and performs favorably against other\nDCF trackers [8], [33].\nAs deep neural networks (DNNs) get popular in recent\nyears, there is an increasing interest in learning DNN-\nbased object tracking models from ofﬂine videos without\nannotations. For example, UDT+ [34] and LUDT [35]\ninvestigated cycle learning in video, in which networks\nare trained to track forward and backward with con-\narXiv:2111.07548v2  [cs.CV]  7 Apr 2022\n2\nsistent object proposals. ResPUL [36] mined positive\nand negative samples from unlabeled videos and lever-\naged them for supervised learning in building spatial\nand temporal correspondence. These unsupervised deep\ntrackers reveal a promising direction in exploiting ofﬂine\nvideos without annotations. Yet, they are limited in\nperformance. Furthermore, they need the pre-training\neffort. In contrast, no pre-training on ofﬂine datasets is\nneeded in our unsupervised tracker.\nDespite the above-mentioned developments in unsu-\npervised trackers, there is a signiﬁcant performance gap\nbetween unsupervised DCF trackers and supervised DL\ntrackers. It is attributed to the limitations of DCF trackers\nsuch as failure to recover from tracking loss and inﬂexi-\nbility in object box adaptation. An unsupervised tracker,\ncalled UHP-SOT (Unsupervised High-Performance Sin-\ngle Object Tracker), was recently proposed in [37] to\naddress the issues. UHP-SOT used STRCF as the baseline\nand incorporated two new modules – background mo-\ntion modeling and trajectory-based object box prediction.\nA simple fusion rule was adopted by UHP-SOT to\nintegrate proposals from three modules into the ﬁnal\none. UHP-SOT has the potential to recover from tracking\nloss and offer ﬂexibility in object box adaptation. UHP-\nSOT outperforms all previous unsupervised single object\ntrackers and narrows down the gap between unsuper-\nvised and supervised trackers. It achieves comparable\nperformance against DL trackers on small-scale datasets\nsuch as TB-50 and TB-100 (or OTB 2015) [38].\nThis work is an extension of UHP-SOT with new\ncontributions. First, the fusion strategies in UHP-SOT\nand UHP-SOT++ are different. The fusion strategy in\nUHP-SOT was simple and ad hoc. UHP-SOT++ adopts a\nfusion strategy that is more systematic and well justiﬁed.\nIt is applicable to both small- and large-scale datasets\nwith more robust and accurate performance. Second,\nthis work conducts more extensive experiments on\nfour object tracking benchmarks (i.e., OTB2015, TC128,\nUAV123 and LaSOT) while only experimental results\non OTB2015 were reported for UHP-SOT in [37]. New\nexperimental evaluations demonstrate that UHP-SOT++\noutperforms all previous unsupervised SOT methods\n(including UHP-SOT) and achieves comparable results\nwith DL methods on large-scale datasets. Since UHP-\nSOT++ has an extremely small model size, high tracking\nperformance, and low computational complexity (op-\nerating at a rate of 20 FPS on an i5 CPU even with-\nout code optimization), it is ideal for real-time object\ntracking on resource-limited platforms. Third, we make\nthorough discussion on pros and cons of supervised and\nunsupervised trackers in this work. Besides quantitative\nevaluations, we provide a few exemplary sequences\nwith qualitative analysis on strengths and weaknesses\nof UHP-SOT++ and its benchmarking methods.\nThe rest of this paper is organized as follows. Related\nwork is reviewed in Sec. II. The UHP-SOT++ method\nis detailed in Sec. III. Experimental results are shown\nin Sec. IV. Further discussion is provided in Sec. V.\nConcluding remarks are given in Sec. VI.\nII. RELATED WORK\nFig. 1: Comparison of the inference structures of a DCF\ntracker and a siamese network tracker, where the red\nbox denotes the object bounding box, φ is the feature\nextraction module and ∗is the correlation operation.\nA. DCF and Siamese Networks\nTwo representative single object trackers are reviewed\nand compared here. They are the DCF tracker and the\nsiamese network tracker as shown in Fig. 1. The former is\nan unsupervised one while the latter is a supervised one\nbased on DL. Both of them conduct template matching\nwithin a search region to generate the response map for\nobject location. The matched template in the next frame\nis centered at the location that has the highest response.\nIn the DCF, the template size is the same as that of the\nsearch region so that the Fast Fourier Transform (FFT)\ncould be used to speed up the correlation process. To\nlearn the template, a DCF uses the initial object patch\nto obtain a linear template via regression in the Fourier\ndomain:\narg min\nf\n1\n2∥\nD\nX\nd=1\nxd ∗f d −y∥2,\n(1)\nwhere f is the template to be determined, x ∈RNx×Ny×D\nis the spatial map of D features extracted from the\nobject patch, ∗is the feature-wise spatial convolution,\nand y ∈RNx×Ny is a centered Gaussian-shaped map\nthat serves as the regression label. Templates in DCFs\ntend to contain some background information. Further-\nmore, there exists boundary distortion caused by the 2D\nFourier transform. To alleviate these side effects, it is\n3\noften to weigh the template with a window function to\nsuppress background and image discontinuity.\nIn contrast, the template size in a siamese networks\nis more ﬂexible and signiﬁcantly smaller than the search\nregion. Usually, the object box of the ﬁrst frame serves\nas the template for the search in all later frames. The\ncorrelation is typically implemented by the convolution\nwhich runs fast on GPU. The shape and size of the\npredicted object bounding box are determined by the\nregional proposal network inside the siamese network.\nB. Spatial-Temporal Regularized Correlation Filters (STRCF)\nSTRCF is a DCF-based tracker. It has an improved\nregression objective function using spatial-temporal reg-\nularization. The template is initialized at the ﬁrst frame.\nSuppose that the object appearance at frame t is modeled\nby a template, denoted by ft, which will be used for\nsimilarity matching at frame (t + 1). By modifying Eq.\n(1), STRCF updates its template at frame t by solving the\nfollowing regression equation:\narg min\nf\nn\n1\n2∥\nD\nX\nd=1\nxd\nt ∗f d −y∥2 + 1\n2\nD\nX\nd=1\n∥w · f d∥2\n+µ\n2 ∥f −ft−1∥2,\no\n(2)\nwhere w is the spatial weight on the template, ft−1\nis the template obtained from time t −1, and µ is a\nconstant regularization coefﬁcient. We can interpret the\nthree terms in Eq. (2) as follows. The ﬁrst term is the stan-\ndard regression objective function of a DCF. The second\nterm imposes the spatial regularization. It gives more\nweights to features in the center region of a template in\nthe matching process. The third term imposes temporal\nregularization for smooth appearance change.\nTo search for the box in frame (t+1), STRCF correlates\ntemplate ft with the search region and determines the\nnew box location by ﬁnding the location that gives\nthe highest response. Although STRCF can model the\nappearance change for general sequences, it suffers from\noverﬁtting. That is, it is not able to adapt to largely\ndeformed objects quickly. Furthermore, it cannot recover\nfrom tracking loss. The template model, f, is updated at\nevery frame with a ﬁxed regularization coefﬁcient, µ, in\nstandard STRCF.\nOur UHP-SOT++ adopts STRCF as a building module.\nTo address the above-mentioned shortcomings, we have\nsome modiﬁcation in our implementation. First, we skip\nupdating f if no obvious motion is observed. Second, a\nsmaller µ is used when all modules agree with each other\nin prediction so that f can adapt to the new appearance\nof largely deformed objects faster.\nIII. PROPOSED UHP-SOT++ METHOD\nA. System Overview\nThere are three main challenges in SOT:\n1) signiﬁcant change of object appearance,\n2) loss of tracking,\n3) rapid variation of object’s location and/or shape.\nWe propose a new tracker, UHP-SOT++, to address\nthese challenges, As shown in Fig. 2, it consists of three\nmodules:\n1) appearance model update,\n2) background motion modeling,\n3) trajectory-based box prediction.\nUHP-SOT++ follows the classic tracking-by-detection\nparadigm where the object is detected within a region\ncentered at its last predicted location at each frame. The\nhistogram of gradients (HOG) features as well as the\ncolor name (CN) [39] features are extracted to yield the\nfeature map. We choose the STRCF tracker [32] as the\nbaseline because of its efﬁcient and effective appearance\nmodeling and update. Yet, STRCF cannot handle the\nsecond and the third challenges well because it only\nfocuses on the modeling of object appearance which\ncould vary a lot across different frames. Generally, the\nhigh variety of object appearance is difﬁcult to capture\nusing a single model. Thus, we propose the second and\nthe third modules in UHP-SOT++ to enhance its tracking\naccuracy. UHP-SOT++ operates in the following fashion.\nThe baseline tracker gets initialized at the ﬁrst frame.\nFor the following frames, UHP-SOT++ gets proposals\nfrom all three modules and merges them into the ﬁnal\nprediction based on a fusion strategy.\nThe STRCF tracker was already discussed in Sec.\nII-B. For the rest of this section, we will examine the\nbackground motion modeling module and the trajectory-\nbased box prediction module in Secs. III-B and III-C,\nrespectively. Finally, we will elaborate on the fusion\nstrategy in Sec. III-D. Note that the fusion strategies of\nUHP-SOT and UHP-SOT++ are completely different.\nB. Background Motion Modeling\nWe decompose the pixel displacement between adja-\ncent frames (also called optical ﬂow) into two types: ob-\nject motion and background motion. Background motion\nis usually simpler, and it may be ﬁt by a parametric\nmodel. Background motion estimation [40], [41] ﬁnds\napplications in video stabilization, coding and visual\ntracking. Here, we propose a 6-parameter model in form\nof\nxt+1\n=\nα1xt + α2yt + α0,\n(3)\nyt+1\n=\nβ1xt + β2yt + β0,\n(4)\nwhere (xt+1, yt+1) and (xt, yt) are corresponding back-\nground points in frames (t + 1) and t, respectively,\nand αi and βi, i = 0, 1, 2 are model parameters. With\nmore than three pairs of corresponding points, we can\ndetermine the model parameters using the linear least-\nsquares method. Usually, we choose a few salient points\n(e.g., corners) to build the correspondence. We apply\nthe background model to the grayscale image It(x, y) of\n4\nFig. 2: The system diagram of the proposed UHP-SOT++ method. It shows one example where the object was lost\nat time t −1 but gets retrieved at time t because the proposal from background motion modeling is accepted.\nframe t to ﬁnd the estimated ˆIt+1(x, y) of frame (t + 1).\nThen, we can compute the difference map ∆I:\n∆I = ˆIt+1(x, y) −It+1(x, y),\n(5)\nwhich is expected to have small and large absolute val-\nues in the background and foreground regions, respec-\ntively. Thus, we can determine potential object locations.\nWhile DCF trackers exploit foreground correlation to\nlocate the object, background modeling uses background\ncorrelation to eliminate background inﬂuence in object\ntracking. They complement each other. DCF trackers\ncannot recover from tracking loss easily since it does\nnot have a global view of the scene. In contrast, our\nbackground modeling can ﬁnd potential object locations\nby removing the background.\nC. Trajectory-based Box Prediction\nGiven\npredicted\nbox\ncenters\nof\nthe\nobject\nof\nthe\nlast\nN\nframes,\n{(xt−N, yt−N), · · · , (xt−1, yt−1)},\nwe\ncalculate\nN\n−\n1\ndisplacement\nvectors\n{(∆xt−N+1, ∆yt−N+1), · · · , (∆xt−1, ∆yt−1)} and apply\nthe principal component analysis (PCA) to them. To\npredict the displacement at frame t, we ﬁt the ﬁrst\nprincipal component using a line and set the second\nprincipal component to zero to remove noise. Then, the\ncenter location of the box at frame t can be written as\n(ˆxt, ˆyt) = (xt−1, yt−1) + ( ˆ\n∆xt, ˆ\n∆yt).\n(6)\nSimilarly, we can estimate the width and the height of\nthe box at frame t, denoted by ( ˆwt, ˆht). Typically, the\nphysical motion of an object has an inertia in motion\ntrajectory and its size, and the box prediction process\nattempts to maintain the inertia. It contributes to better\ntracking performance in two ways. First, it removes\nsmall ﬂuctuation of the box in its location and size.\nSecond, when there is a rapid deformation of the target\nobject, the appearance model alone cannot capture the\nshape change effectively. In contrast, the combination of\nbackground motion modeling and the trajectory-based\nbox prediction can offer a more satisfactory solution. For\nexample, Fig. 3, shows a frame of the diving sequence\nin the upper-left subﬁgure, where the green and the\nmagenta boxes are the ground truth and the result of\nUHP-SOT++, respectively. Although a DCF tracker can\ndetect the size change by comparing correlation scores at\nﬁve image resolutions, it cannot estimate the aspect ratio\nchange properly. In contrast, as shown in the lower-left\nsubﬁgure, the residual image after background removal\nin UHP-SOT++ reveals the object shape. By summing up\nabsolute pixel values of the residual image horizontally\nand vertically and using a threshold to determine two\nends of the box, we have\nˆw = xmax −xmin, and ˆh = ymax −ymin.\n(7)\nNote that raw estimates may not be stable across dif-\nferent frames. Estimates that deviate much from the\ntrajectory of (∆wt, ∆ht) are rejected to yield a robust and\ndeformable box proposal.\nD. Fusion Strategy\nWe have three box proposals for the target object at\nframe t: 1) Bapp from the baseline STRCF tracker to\ncapture appearance change, 2) Bbgd from the background\nmotion predictor to eliminate unlikely object regions,\nand 3) Btrj from the trajectory predictor to maintain the\ninertia of the box position and size. A fusion strategy\nis needed to yield the ﬁnal box location and size. We\nconsider a couple of factors for its design.\n1) Proposal Quality: There are three box proposals. The\nquality of each box proposal can be measured by: 1)\nobject appearance similarity, and 2) robustness against\nthe trajectory. We use a binary ﬂag to indicate whether\nthe quality of a proposal is good or not. As shown in\nTable I, the ﬂag is set to one if a proposal keeps proper\nappearance similarity and is robust against trajectory.\nOtherwise, it is set to zero.\n5\nTABLE I: All tracking scenarios are classiﬁed into 8 cases in terms of the overall quality of proposals from three\nmodules. The fusion strategy is set up for each scenario. The update rate is related the regularization coefﬁcient,\nµ, that controls to which extent the appearance model should be updated.\nisGoodapp\nisGoodtrj\nisGoodbgd\nProposal to take\nUpdate rate\n1\n1\n1\nBapp or union of three\nnormal\n1\n1\n0\nBapp or Btrj or union of two\nnormal\n1\n0\n1\nBapp or Bbgd or union of two\nnormal\n0\n1\n1\nBtrj or Bbgd or union of two\nnormal or stronger\n1\n0\n0\nBapp\nnormal\n0\n1\n0\nBapp or Btrj\nnormal or stronger\n0\n0\n1\nBapp or Bbgd\nnormal or stronger\n0\n0\n0\nBapp or last prediction in case of occlusion\nnormal or weaker\nFig. 3: Illustration of shape change estimation based\non background motion model and trajectory-based box\nprediction, where the ground truth and our proposal are\nannotated in green and magenta, respectively.\nFor the ﬁrst measure, we store two appearance mod-\nels: the latest model, ft−1, and an older model, fi, i ≤t−1,\nwhere i is the last time instance where all three boxes\nhave the same location. Model fi is less likely to be con-\ntaminated since it needs agreement from all modules. To\ncheck the reliability of the three proposals, we compute\ncorrelation scores for the following six pairs: (ft−1, Bapp),\n(ft−1, Btrj), (ft−1, Bbgd), (fi, Bapp), (fi, Btrj), and (fi, Bbgd).\nThey provide appearance similarity measures of the two\nprevious models against the current three proposals. A\nproposal has good similarity if one of its correlation\nscores is higher than a threshold.\nFor the second measure, if Bapp and Btrj have a small\ndisplacement (say, 30 pixels) from the last prediction, the\nmove is robust. As to Bbgd, it often jumps around and,\nthus, is less reliable. However, if the standard deviations\nof its historical locations along the x-axis and y-axis are\nsmall enough (e.g., 30 pixels over the past 10 frames),\nthen they are reliable.\n2) Occlusion Detection: We propose an occlusion de-\ntection strategy for color images, which is illustrated in\nFig. 4. As occlusion occurs, we often observe a sudden\ndrop in the similarity score and a rapid change on the\naveraged RGB color values inside the box. A drop is\nsudden if the mean over the past several frames is high\nwhile the current value is signiﬁcantly lower. If this is\ndetected, we keep the new prediction the same as the last\npredicted position since the new prediction is unreliable.\nWe do not update the model for this frame either to\navoid drifting and/or contamination of the appearance\nmodel.\n3) Rule-based Fusion: Since each of the three proposals\nhas a binary ﬂag, all tracking scenarios can be catego-\nrized into 8 cases as shown in Fig. 5. We propose a fusion\nscheme for each case below.\n• When all three proposals are good, their boxes are\nmerged together as a minimum covering rectangle\nif they overlap with each other with IoU above a\nthreshold. Otherwise, Bapp is adopted.\n• When two proposals are good, merge them if they\noverlap with each other with IoU above a thresh-\nold. Otherwise, the one with better robustness is\nadopted.\n• When one proposal is good, adopt that one if it\nis Bapp. Otherwise, that proposal is compared with\nBapp to verify its superiority by observing a higher\nsimilarity score or better robustness.\n• When all proposals have poor quality, the occlusion\ndetection process is conducted. The last prediction\nis adopted in case of occlusion. Otherwise, Bapp is\nadopted.\n• When other proposals outperform Bapp, the regu-\nlarization coefﬁcient, µ, is adjusted accordingly for\nstronger update. Because this might reveal that the\nappearance model needs to be updated more to\ncapture the new appearance.\nThe fusion rule is summarized in Table I. In most cases,\n6\nFig. 4: Illustration of occlusion detection, where the green box shows the object location. The color information and\nsimilarity score could change rapidly if occlusion occurs.\nFig. 5: An example of quality assessment of proposals, where the green box is the ground truth, and yellow, blue\nand magenta boxes are proposals from Bapp, Btrj and Bbgd, respectively, and the bright yellow text on the top-left\ncorner denotes the quality of three proposals (isGoodapp, isGoodtrj, isGoodbgd).\nBapp is reliable and it will be chosen or merged with\nother proposals because the change is smooth between\nadjacent frames in the great majority of frames in a video\nclip.\nIV. EXPERIMENTS\nA. Experimental Set-up\nTo show the performance of UHP-SOT++, we compare\nit with several state-of-the-art unsupervised and super-\nvised trackers on four single object tracking datasets.\nThey are OTB2015 [38], TC128 [42], UAV123 [43] and\nLaSOT [44]. OTB2015 (also named OTB in short) and\nTC128, which contain 100 and 128 color or grayscale\nvideo sequences, respectively, are two widely used\nsmall-scale datasets. UAV123 is a larger one, which has\n123 video sequences with more than 110K frames in total.\nVideos in UAV123 are captured by low-altitude drones.\nThey are useful in the tracking test of small objects\nwith a rapid change of viewpoints. LaSOT is the largest\nsingle object tracking dataset that targets at diversiﬁed\nobject classes and ﬂexible motion trajectories in longer\nsequences. It has one training set with dense annotation\nfor supervised trackers to learn and another test set for\nperformance evaluation. The test set contains 280 videos\nof around 685K frames.\nPerformance evaluation is conducted using the “One\nPass Evaluation (OPE)” protocol. The metrics include\nthe precision plot (i.e., the distance of the predicted and\nactual box centers) and the success plot (i.e., overlapping\nratios at various thresholds). The distance precision (DP)\nis measured at the 20-pixel threshold to rank different\nmethods. The overlap precision is measured by the area-\nunder-curve (AUC) score. We use the same hyperpa-\nrameters as those in STRCF except for regularization\ncoefﬁcient, µ. If the appearance box is not chosen, STRCF\nsets µ = 15 while UHP-SOT++ selects µ ∈{15, 10, 5, 0}.\nThe smaller µ is, the stronger the update is. The number\nof previous frames for trajectory prediction is N = 20.\nThe cutting threshold along the horizontal or vertical\ndirection is set 0.1. The threshold for good similarity\nscore is 0.08, and a threshold of 0.5 for IoU is adopted.\nUHP-SOT++ runs at 20 frames per second (FPS) on a PC\nequipped with an Intel(R) Core(TM) i5-9400F CPU. The\nspeed data of other trackers are either from their original\npapers or benchmarks. Since no code optimization is\n7\nconducted, all reported speed data should be viewed as\nlower bounds for the corresponding trackers.\nB. Ablation study\nWe compare different conﬁgurations of UHP-SOT++\non the TC128 dataset to investigate contributions from\neach module in Fig. 6. As compared with UHP-SOT, im-\nprovements on both DP and AUC in UHP-SOT++ come\nfrom the new fusion strategy. Under this strategy, the\nbackground motion modeling plays an more important\nrole and it has comparable performance even without the\ntrajectory prediction. Although the trajectory prediction\nmodule is simple, it contributes a lot to higher tracking\naccuracy and robustness as revealed by the performance\nimprovement over the baseline STRCF.\nMore performance comparison between UHP-SOT++,\nUHP-SOT and STRCF is presented in Table II. As com-\npared with STRCF, UHP-SOT++ achieves 1.8%, 6.2%,\n6.7% and 6.8% gains in the success rate on OTB, TC128,\nUAV123 and LaSOT, respectively. As to the mean pre-\ncision, it has an improvement of 1.2%, 6.9%, 7.2% and\n10.4%, respectively. Except for OTB, UHP-SOT++ outper-\nforms UHP-SOT in both the success rate and the preci-\nsion. This is especially obvious for large-scale datasets.\nGenerally, UHP-SOT++ has better tracking capability\nthan UHP-SOT. Its performance drop in OTB is due\nto the tracking loss in three sequences; namely, Bird2,\nCoupon and Freeman4. They have complicated appear-\nance changes such as severe rotation, background clutter\nand heavy occlusion. As shown in Fig. 7, errors at some\nkey frames lead to total loss of the object, and the lost ob-\nject cannot be easily recovered from motion. The trivial\nfusion strategy based on appearance similarity in UHP-\nSOT seems to work well on their key frames while the\nfusion strategy of UHP-SOT++ does not suppress wrong\nproposals properly since background clutters have stable\nmotion and trajectories as well.\nFig. 6: The precision plot and the success plot of our\nUHP-SOT++ tracker with different conﬁgurations on the\nTC128 dataset, where the numbers inside the parenthe-\nses are the DP values and AUC scores, respectively.\nC. Comparison with State-of-the-art Trackers\nWe compare the performance of UHP-SOT++ and\nseveral unsupervised trackers for the LaSOT dataset in\nFig. 7: Failure cases of UHP-SOT++ (in green) as com-\npared to UHP-SOT (in red) on OTB2015.\nFig. 8: The success plot and the precision plot of nine\nunsupervised tracking methods for the LaSOT dataset,\nwhere the numbers inside the parentheses are the over-\nlap precision and the distance precision values, respec-\ntively.\n8\nTABLE II: Comparison of state-of-the-art supervised and unsupervised trackers on four datasets, where the\nperformance is measure by the distance precision (DP) and the area-under-curve (AUC) score in percentage. The\nmodel size is measured by the memory required to store needed data such as the model parameters of pre-trained\nnetworks. The best unsupervised performance is highlighted. Also, S and P indicate Supervised and Pre-trained,\nrespectively.\nTrackers\nYear\nS\nP\nOTB2015\nTC128\nUAV123\nLaSOT\nFPS\nDevice\nModel size (MB)\nDP\nAUC\nDP\nAUC\nDP\nAUC\nDP\nAUC\nSiamRPN++ [14]\n2019\n✓\n✓\n91.0\n69.2\n-\n-\n84.0\n64.2\n49.3\n49.5\n35\nGPU\n206\nECO [8]\n2017\n✓\n✓\n90.0\n68.6\n80.0\n59.7\n74.1\n52.5\n30.1\n32.4\n10\nGPU\n329\nUDT+ [34]\n2019\n×\n✓\n83.1\n63.2\n71.7\n54.1\n-\n-\n-\n-\n55\nGPU\n< 1\nLUDT [35]\n2020\n×\n✓\n76.9\n60.2\n67.1\n51.5\n-\n-\n-\n26.2\n70\nGPU\n< 1\nResPUL [36]\n2021\n×\n✓\n-\n58.4\n-\n-\n-\n-\n-\n-\n-\nGPU\n> 6\nECO-HC [8]\n2017\n×\n×\n85.0\n63.8\n75.3\n55.1\n72.5\n50.6\n27.9\n30.4\n42\nCPU\n< 1\nSTRCF [32]\n2018\n×\n×\n86.6\n65.8\n73.5\n54.8\n67.8\n47.8\n29.8\n30.8\n24\nCPU\n< 1\nUHP-SOT [37]\n2021\n×\n×\n90.9\n68.9\n77.4\n57.4\n71.0\n50.1\n31.1\n32.0\n23\nCPU\n< 1\nUHP-SOT++\nOurs\n×\n×\n87.6\n66.9\n78.6\n58.2\n72.7\n51.0\n32.9\n32.9\n20\nCPU\n< 1\nFig. 9: Qualitative evaluation of three leading unsuper-\nvised trackers, where UHP-SOT++ offers a robust and\nﬂexible box prediction.\nFig. 8. The list of benchmarking methods includes ECO-\nHC [8], STRCF [32], CSR-DCF [45], SRDCF [33], Staple\n[30], KCF [27], DSST [29]. UHP-SOT++ outperforms\nother unsupervised methods by a large margin, which\nis larger than 0.02 in the mean scores of the success\nrate and the precision. Besides, its running speed is\n20 FPS, which is comparable with that of the second\nrunner STRCF (24 FPS) and the third runner (42 FPS)\nbased on experiments in OTB. With a small increase\nin computational and memory resources, UHP-SOT++\ngains in tracking performance by adding object box\ntrajectory and background motion modeling modules.\nObject boxes of three leading unsupervised trackers are\nvisualized in Fig. 9 for qualitative performance compar-\nison. As compared with other methods, the proposals of\nUHP-SOT++ offer a robust and ﬂexible box prediction.\nThey follow tightly with the object in both location and\nshape even under challenging scenarios such as motion\nblur and rapid shape change.\nWe compare the success rates of UHP-SOT++ and\nseveral supervised and unsupervised trackers against\nall four datasets in Fig. 10. Note that there are more\nbenchmarking methods for OTB but fewer for TC128,\nUA123 and LaSOT since OTB is an earlier dataset. The\nsupervised deep trackers under consideration include\nSiamRPN++ [14], ECO [8], C-COT [9], DeepSRDCF [33],\nHDT [5], SiamFC 3s [19], CFNet [31] and LCT [46]. Other\ndeep trackers that have leading performance but are\nnot likely to be used on resource-limited devices due\nto their extremely high complexity, such as transformer-\nbased trackers [24], [25], are not included here. Although\nthe performance of a tracker may vary from one dataset\nto the other due to different video sequences collected\nby each dataset, UHP-SOT++ is among the top three in\nall four datasets. This demonstrates the generalization\ncapability of UHP-SOT++. Its better performance than\nECO on LaSOT indicates a robust and effective update\nof the object model. Otherwise, it would degrade quickly\nwith worse performance because of longer LaSOT se-\nquences. Besides, its tracking speed of 20 FPS on CPU\nis faster than many deep trackers such as ECO (10 FPS),\nDeepSRDCF (0.2 FPS), C-COT (0.8 FPS) and HDT (2.7\n9\nFig. 10: The success plot comparison of UHP-SOT++ with several supervised and unsupervised tracking methods\non four datasets, where only trackers with raw results published by authors are listed. For the LaSOT dataset, only\nsupervised trackers are included for performance benchmarking in the plot since the success plot of unsupervised\nmethods is already given in Fig. 8.\nFPS).\nIn Table II, we further compare UHP-SOT++ with\nstate-of-the-art unsupervised deep trackers UDT+ [34],\nLUDT [35] and ResPUL [36] in their AUC and DP\nvalues, running speeds and model sizes. Two leading\nsupervised trackers SiamRPN++ and ECO in Fig. 10\nare also included. It shows that UHP-SOT++ outper-\nforms recent unsupervised deep trackers by a large\nmargin. We should emphasize that deep trackers de-\nmand pre-training on ofﬂine datasets while UHP-SOT++\ndoes not. In addition, UHP-SOT++ is attractive because\nof its lower memory requirement and near real-time\nrunning speed on CPUs. Although ECO-HC also pro-\nvides a light-weight solution, there is a performance\ngap between UHP-SOT++ and ECO-HC. SiamRPN++\nhas the best tracking performance among all track-\ners, due to the merit of end-to-end optimized network\nwith auxiliary modules such as classiﬁcation head and\nthe region proposal network. Yet, its large model size\nand GPU hardware requirement limit its applicability\nin resource-limited devices such as mobile phones or\ndrones. In addition, as an end-to-end optimized deep\ntracker, SiamRPN++ has the interpretability issue to be\ndiscussed later.\nD. Attribute-based Study\nTo better understand the capability of different track-\ners, we analyze the performance variation under var-\nious challenging tracking conditions. These conditions\ncan be classiﬁed into the following attributes: aspect\nratio change (ARC), background clutter (BC), camera\nmotion (CM), deformation (DEF), fast motion (FM), full\nocclusion (FOC), in-plane rotation (IPR), illumination\nvariation (IV), low resolution (LR), motion blur (MB), oc-\n10\nFig. 11: The area-under-curve (AUC) scores for two datasets, TC128 and LaSOT, under the attribute-based evaluation,\nwhere attributes of concern include the aspect ratio change (ARC), background clutter (BC), camera motion (CM),\ndeformation (DEF), fast motion (FM), full occlusion (FOC), in-plane rotation (IPR), illumination variation (IV), low\nresolution (LR), motion blur (MB), occlusion (OCC), out-of-plane rotation (OPR), out-of-view (OV), partial occlusion\n(POC), scale variation (SV) and viewpoint change (VC), respectively.\nclusion (OCC), out-of-plane rotation (OPR), out-of-view\n(OV), partial occlusion (POC), scale variation (SV) and\nviewpoint change (VC). We compare the AUC scores of\nsupervised trackers (e.g., SiamRPN++ and ECO) and un-\nsupervised trackers (e.g., UHP-SOT++, ECO-HC, UDT+\nand STRCF) under these attributes in Fig. 11.\nWe have the following observations. First, among\nunsupervised trackers, UHP-SOT++ has leading perfor-\nmance in all attributes, which reveals improved robust-\nness from its basic modules and fusion strategy. Second,\nalthough ECO utilizes deep features, it is weak in ﬂexible\nbox regression and, as a result, it is outperformed by\nUHP-SOT++ in handling such deformation and shape\nchanges against LaSOT. In contrast, SiamRPN++ is better\nthan other trackers especially in DEF (deformation),\nROT (rotation)and VC (viewpoint change). The superior\nperformance of SiamRPN++ demonstrates the power of\nits region proposal network (RPN) in generating tight\nboxes. The RPN inside SiamRPN++ not only improves\nIoU score but also has the long-term beneﬁt by excluding\nnoisy information. Fourth, supervised trackers perform\nbetter in IV (illumination variation) and LR (low resolu-\ntion) than unsupervised trackers in general. This can be\nexplained by the fact that unsupervised trackers adopt\nHOG, CN features or other shallow features which do\nnot work well under these attributes. They focus on local\nstructures of the appearance and tend to fail to capture\nthe object when the local gradient or color information\nis not stable. Finally, even with the feature limitations,\nUHP-SOT++ still runs second in many attributes against\nLaSOT because of the stability offered by trajectory\nprediction and its capability to recover from tracking loss\nvia background motion modeling.\nV. EXEMPLARY SEQUENCES AND QUALITATIVE\nANALYSIS\nAfter providing quantitative results in Sec. IV, we\nconduct error analysis on a couple of representative\nsequences to gain more insights in this section. Several\nexemplary sequences from LaSOT are shown in Fig. 12,\nin which SiamRPN++ performs either very well or quite\npoorly. In the ﬁrst two sequences, we see the power\nof accurate box regression contributed by the RPN. In\nthis type of sequences, good trackers can follow the\nobject well. Yet, their poor bounding boxes lead to a\nlow success score. Furthermore, the appearance model\nwould be contaminated by the background information\nas shown in the second cat example. The appearance\nmodel of DCF-based methods learns background texture\n(rather than follows the cat) gradually. When the box\nonly covers part of the object, it might also miss some ob-\nject features, resulting in a degraded appearance model.\nIn both scenarios, the long-term performance will drop\nrapidly. Although UHP-SOT++ allows the aspect ratio\nchange to some extent as seen in the ﬁrst ﬂag example, its\nresidual map obtained by background motion modeling\nis still not as effective as the RPN due to lack of semantic\nmeaning. Generally speaking, the performance of UHP-\nSOT++ relies on the quality of the appearance model and\nthe residual map.\nOn the other hand, SiamRPN++ is not robust enough\nto handle a wide range of sequences well. The third\nexample sequence is from video games. SiamRPN++\nsomehow includes background objects in its box pro-\nposals and drifts away from its targets in the presented\nframes. Actually, these background objects are different\nfrom their corresponding target objects in either semantic\nmeaning or local information such as color or texture.\nThe performance of the other three trackers is not af-\nfected. We see that they follow the ground truth without\n11\nFig. 12: Qualitative comparison of top runners against the LaSOT dataset, where tracking boxes of SiamRPN++,\nUHP-SOT++, ECO and ECO-HC are shown in red, green, blue and yellow, respectively. The ﬁrst two rows show\nsequences in which SiamRPN++ outperforms others signiﬁcantly while the last row offers the sequence in which\nSiamRPN++ performs poorly.\nFig. 13: Illustration of three sequences in which UHP-SOT++ performs the best. The tracking boxes of SiamRPN++,\nUHP-SOT++, ECO and ECO-HC are shown in red, green, blue and yellow, respectively.\n12\nany problem. One explanation is that these video game\nsequences could be few in the training set and, as a\nresult, SiamRPN++ cannot offer a reliable tracking result\nfor them.\nFinally, several sequences in which UHP-SOT++ has\nthe top performance are shown in Fig. 13. In the ﬁrst\ncup sequence, all other benchmarking methods lose the\ntarget while UHP-SOT++ could go back to the object\nonce the object has obvious motion in the scene. In\nthe second bottle sequence, UHP-SOT++ successfully\ndetects occlusion without making random guesses and\nthe object box trajectory avoids the box to drift away. In\ncontrast, other trackers make ambitious moves without\nconsidering the inertia of motion. The third bus sequence\nis a complicated one that involves several challenges\nsuch as full occlusion, scale change and aspect ratio\nchange. UHP-SOT++ is the only one that can recover\nfrom tracking loss and provide ﬂexible box predic-\ntions. These examples demonstrate the potential of UHP-\nSOT++ that exploits object and background motion clues\nacross frames effectively.\nVI. CONCLUSION AND FUTURE WORK\nAn unsupervised high-performance tracker, UHP-\nSOT++, was proposed in this paper. It incorporated\ntwo new modules in the STRCF tracker module. They\nwere the background motion modeling module and the\nobject box trajectory modeling module. Furthermore, a\nnovel fusion strategy was adopted to combine proposals\nfrom all three modules systematically. It was shown\nby extensive experimental results on large-scale datasets\nthat UHP-SOT++ can generate robust and ﬂexible object\nbounding boxes and offer a real-time high-performance\ntracking solution on resource-limited platforms.\nThe pros and cons of supervised and unsupervised\ntrackers were discussed. Unsupervised trackers such as\nUHP-SOT and UHP-SOT++ have the potential in deliv-\nering an explainable lightweight tracking solution while\nmaintaining good performance in accuracy. Supervised\ntrackers such as SiamRPN++ beneﬁt from ofﬂine end-\nto-end learning and perform well in general. However,\nthey need to run on GPUs, which is too costly for mobile\nand edge devices. They may encounter problems in rare\nsamples. Extensive supervision with annotated object\nboxes is costly. Lack of interpretability could be a barrier\nfor further performance boosting.\nAlthough UHP-SOT++ offers a state-of-the-art unsu-\npervised tracking solution, there is still a performance\ngap between UHP-SOT++ and SiamRPN++. It is worth-\nwhile to ﬁnd innovative ways to narrow down the\nperformance gap while keeping its attractive features\nsuch as interpretability, unsupervised real-time tracking\ncapability on small devices, etc. as future extension.\nOne idea is to ﬁnd salient points in the predicted and\nreference frames individually, develop a way to build\ntheir correspondences, and reconstruct the object box in\nthe predicted frame based on its salient points.\nREFERENCES\n[1] J. Xing, H. Ai, and S. Lao, “Multiple human tracking based on\nmulti-view upper-body detection and discriminative learning,”\nin 2010 20th International Conference on Pattern Recognition.\nIEEE,\n2010, pp. 1698–1701.\n[2] J. Janai, F. G¨uney, A. Behl, A. Geiger et al., “Computer vision for\nautonomous vehicles: Problems, datasets and state of the art,”\nFoundations and Trends® in Computer Graphics and Vision, vol. 12,\nno. 1–3, pp. 1–308, 2020.\n[3] G. Zhang and P. A. Vela, “Good features to track for visual slam,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 1373–1382.\n[4] A. Yilmaz, O. Javed, and M. Shah, “Object tracking: A survey,”\nAcm computing surveys (CSUR), vol. 38, no. 4, pp. 13–es, 2006.\n[5] M. Fiaz, A. Mahmood, S. Javed, and S. K. Jung, “Handcrafted\nand deep trackers: Recent visual object tracking approaches and\ntrends,” ACM Computing Surveys (CSUR), vol. 52, no. 2, pp. 1–44,\n2019.\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-\nﬁcation with deep convolutional neural networks,” Advances in\nneural information processing systems, vol. 25, pp. 1097–1105, 2012.\n[7] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return\nof the devil in the details: Delving deep into convolutional nets,”\narXiv preprint arXiv:1405.3531, 2014.\n[8] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “Eco:\nEfﬁcient convolution operators for tracking,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2017, pp.\n6638–6646.\n[9] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, “Beyond\ncorrelation ﬁlters: Learning continuous convolution operators\nfor visual tracking,” in European conference on computer vision.\nSpringer, 2016, pp. 472–488.\n[10] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang, “Hierarchical\nconvolutional features for visual tracking,” in Proceedings of the\nIEEE international conference on computer vision, 2015, pp. 3074–\n3082.\n[11] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, and M.-H. Yang,\n“Hedged deep tracking,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 4303–4311.\n[12] N. Wang, W. Zhou, Q. Tian, R. Hong, M. Wang, and H. Li, “Multi-\ncue correlation ﬁlters for robust visual tracking,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2018,\npp. 4844–4853.\n[13] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual\ntracking with siamese region proposal network,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2018,\npp. 8971–8980.\n[14] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “Siamrpn++:\nEvolution of siamese visual tracking with very deep networks,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 4282–4291.\n[15] X. Lu, C. Ma, B. Ni, X. Yang, I. Reid, and M.-H. Yang, “Deep\nregression tracking with shrinkage loss,” in Proceedings of the\nEuropean conference on computer vision (ECCV), 2018, pp. 353–369.\n[16] H. Nam and B. Han, “Learning multi-domain convolutional\nneural networks for visual tracking,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp. 4293–\n4302.\n[17] S. Pu, Y. Song, C. Ma, H. Zhang, and M.-H. Yang, “Deep\nattentive tracking via reciprocative learning,” arXiv preprint\narXiv:1810.03851, 2018.\n[18] Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, and M.-H. Yang,\n“Crest: Convolutional residual learning for visual tracking,” in\nProceedings of the IEEE international conference on computer vision,\n2017, pp. 2555–2564.\n[19] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H.\nTorr, “Fully-convolutional siamese networks for object tracking,”\nin European conference on computer vision. Springer, 2016, pp. 850–\n865.\n[20] R. Tao, E. Gavves, and A. W. Smeulders, “Siamese instance search\nfor tracking,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2016, pp. 1420–1429.\n[21] Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu, “Distractor-\naware siamese networks for visual object tracking,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018, pp.\n101–117.\n13\n[22] Q. Wang, Z. Teng, J. Xing, J. Gao, W. Hu, and S. Maybank,\n“Learning attentions: residual attentional siamese network for\nhigh performance online visual tracking,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2018, pp.\n4854–4863.\n[23] A. He, C. Luo, X. Tian, and W. Zeng, “A twofold siamese network\nfor real-time object tracking,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 4834–4843.\n[24] N. Wang, W. Zhou, J. Wang, and H. Li, “Transformer meets\ntracker: Exploiting temporal context for robust visual tracking,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 1571–1580.\n[25] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Trans-\nformer tracking,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 8126–8135.\n[26] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui, “Visual\nobject tracking using adaptive correlation ﬁlters,” in 2010 IEEE\ncomputer society conference on computer vision and pattern recognition.\nIEEE, 2010, pp. 2544–2550.\n[27] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-speed\ntracking with kernelized correlation ﬁlters,” IEEE transactions on\npattern analysis and machine intelligence, vol. 37, no. 3, pp. 583–596,\n2014.\n[28] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg, “Con-\nvolutional features for correlation ﬁlter based visual tracking,” in\nProceedings of the IEEE international conference on computer vision\nworkshops, 2015, pp. 58–66.\n[29] M. Danelljan, G. H¨ager, F. S. Khan, and M. Felsberg, “Discrimi-\nnative scale space tracking,” IEEE transactions on pattern analysis\nand machine intelligence, vol. 39, no. 8, pp. 1561–1575, 2016.\n[30] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H.\nTorr, “Staple: Complementary learners for real-time tracking,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 1401–1409.\n[31] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. Torr,\n“End-to-end representation learning for correlation ﬁlter based\ntracking,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 2805–2813.\n[32] F. Li, C. Tian, W. Zuo, L. Zhang, and M.-H. Yang, “Learning\nspatial-temporal regularized correlation ﬁlters for visual track-\ning,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 4904–4913.\n[33] M. Danelljan, G. Hager, F. Shahbaz Khan, and M. Felsberg,\n“Learning spatially regularized correlation ﬁlters for visual track-\ning,” in Proceedings of the IEEE international conference on computer\nvision, 2015, pp. 4310–4318.\n[34] N. Wang, Y. Song, C. Ma, W. Zhou, W. Liu, and H. Li, “Unsuper-\nvised deep tracking,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 1308–1317.\n[35] N. Wang, W. Zhou, Y. Song, C. Ma, W. Liu, and H. Li, “Un-\nsupervised deep representation learning for real-time tracking,”\nInternational Journal of Computer Vision, vol. 129, no. 2, pp. 400–418,\n2021.\n[36] Q. Wu, J. Wan, and A. B. Chan, “Progressive unsupervised\nlearning for visual object tracking,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2021, pp.\n2993–3002.\n[37] Z. Zhou, H. Fu, S. You, C. C. Borel-Donohue, and C.-C. J.\nKuo, “Uhp-sot: An unsupervised high-performance single object\ntracker,” arXiv preprint arXiv:2110.01812, 2021.\n[38] Y. Wu, J. Lim, and M.-H. Yang, “Object tracking benchmark,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 37,\nno. 9, pp. 1834–1848, 2015.\n[39] M. Danelljan, F. Shahbaz Khan, M. Felsberg, and J. Van de\nWeijer, “Adaptive color attributes for real-time visual tracking,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2014, pp. 1090–1097.\n[40] K. Hariharakrishnan and D. Schonfeld, “Fast object tracking using\nadaptive block matching,” IEEE transactions on multimedia, vol. 7,\nno. 5, pp. 853–859, 2005.\n[41] A. Aggarwal, S. Biswas, S. Singh, S. Sural, and A. K. Majumdar,\n“Object tracking using background subtraction and motion esti-\nmation in mpeg videos,” in Asian Conference on Computer Vision.\nSpringer, 2006, pp. 121–130.\n[42] P. Liang, E. Blasch, and H. Ling, “Encoding color information for\nvisual tracking: Algorithms and benchmark,” IEEE Transactions on\nImage Processing, vol. 24, no. 12, pp. 5630–5644, 2015.\n[43] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simu-\nlator for uav tracking,” in European conference on computer vision.\nSpringer, 2016, pp. 445–461.\n[44] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu,\nC. Liao, and H. Ling, “Lasot: A high-quality benchmark for\nlarge-scale single object tracking,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n5374–5383.\n[45] L. Alan, T. Voj´ıˇr, L. ˇCehovin, J. Matas, and M. Kristan, “Dis-\ncriminative correlation ﬁlter tracker with channel and spatial\nreliability,” International Journal of Computer Vision, vol. 126, no. 7,\npp. 671–688, 2018.\n[46] C. Ma, X. Yang, C. Zhang, and M.-H. Yang, “Long-term correlation\ntracking,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2015, pp. 5388–5396.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2021-11-15",
  "updated": "2022-04-07"
}