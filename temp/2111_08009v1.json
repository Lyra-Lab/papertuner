{
  "id": "http://arxiv.org/abs/2111.08009v1",
  "title": "Piano Fingering with Reinforcement Learning",
  "authors": [
    "Pedro Ramoneda",
    "Marius Miron",
    "Xavier Serra"
  ],
  "abstract": "Hand and finger movements are a mainstay of piano technique. Automatic\nFingering from symbolic music data allows us to simulate finger and hand\nmovements. Previous proposals achieve automatic piano fingering based on\nknowledge-driven or data-driven techniques. We combine both approaches with\ndeep reinforcement learning techniques to derive piano fingering. Finally, we\nexplore how to incorporate past experience into reinforcement learning-based\npiano fingering in further work.",
  "text": "Piano Fingering with Reinforcement Learning\nPedro Ramoneda\nMusic Technology Group\nPompeu Fabra University\nBarcelona, 08018\npedro.ramoneda@upf.edu\nMarius Miron\nMusic Technology Group\nPompeu Fabra University\nBarcelona, 08018\nmarius.miron@upf.edu\nXavier Serra\nMusic Technology Group\nPompeu Fabra University\nBarcelona, 08018\nxavier.serra@upf.edu\nAbstract\nHand and ﬁnger movements are a mainstay of piano technique. Automatic Finger-\ning from symbolic music data allows us to simulate ﬁnger and hand movements.\nPrevious proposals achieve automatic piano ﬁngering based on knowledge-driven\nor data-driven techniques. We combine both approaches with deep reinforcement\nlearning techniques to derive piano ﬁngering. Finally, we explore how to incorpo-\nrate past experience into reinforcement learning-based piano ﬁngering in further\nwork.\n1\nIntroduction\nPiano technique is considered a fundamental performance skill [2, 3, 4, 5, 6]. Through ﬁngering the\nnotes of a score, we can model the technique of hands and ﬁngers piano movements [7, 8]. Rather\nthan a ﬁxed set of rules, piano ﬁngering is a creative and ﬂexible process individualised for each\npianist [3].\nPiano ﬁngering plays an important role in the realization of the music performance [2]. To that extent,\npianists must adapt the ﬁngering at each moment according to the subsequent ﬁngering patterns’\nneeds [5]. Fingering must preserve the musical content of the work in all its facets: articulation,\ntempo, dynamics, rhythm, style and character [5]. On the other hand, it has to be as comfortable as\npossible [4]. Moreover, piano ﬁngering changes individually according to the size of the hand [6].\nIn this paper we model the problem of automatic ﬁngering by using reinforcement learning (RL).\nThe code is available online [1]. The RL policy is deﬁned as such to reduce the hand’s movement.\nTo that extent, the reward is higher if there are fewer hand positions. Besides, the possible ﬁnger\ncombinations are very large when ﬁngering a score. However, the optimal combinations, which are\nthe most comfortable while respecting the musical content, are more limited.\nThe direct application of our proposed method is to give feedback to the piano students to improve\ntheir ﬁngering. We aim at presenting various alternative ﬁnger’s combinations to the music student.\nOur RL method may offer different solutions corresponding to different iterations and to different\nﬁngers combinations. These solutions may help the music students improve their technique.\nSeveral proposals aim at modeling piano ﬁngering with various techniques, from expert systems [9,\n10] through local search algorithms [11, 12] to data-driven methods [13, 14]. In contrast, we aim\nat codifying the expert knowledge on the reward function of a RL algorithm. Moreover, we aim\nto optimising to a broader term, like the local search algorithms[11, 12], thanks to the RL sparsity\nproperty. Besides, our proposed method seeks to optimize from note to note each action, the Markov\ndecision process, as data-driven proposals [13, 14].\nThe remainder of this paper is structured as follows. We present the RL ﬁngering method in Section 2.\nWe expose the preliminary results and the further work in Section 3 and Section 4.\nPreprint. Under review.\narXiv:2111.08009v1  [cs.OH]  15 Nov 2021\n2\nMethodology\nIn the present approach, an agent interacts with a score understood as the environment. Consequently,\neach score is a different environment. The environment is reduced to only the right hand because we\ncan replicate the left hand by symmetry [11, 13]. We deﬁne the state s associated with the ﬁnger a\ntuple comprising (cf, cn, nn) being ch the current ﬁnger, cn the current note and nn the next note,\nand with the ﬁrst ﬁnger of the sheet known. Note, the size of the notes cn, nn encoding space is\ndetermined by the melodic range.\nThe action a is the next ﬁnger according to its policy π(a|s). We deﬁne a set of ﬁngering rules\nencoded in the reward function r(s|a) dependent on the ﬁnger selected as action a. The reward\nfunction r gives a positive reward if no hand position changes and the negative reward is the opposite.\nIn addition, r negatively rewards the anatomically not feasible actions. Finally, the Q function of a\npolicy π is estimated with a Fully Connected-based DQN. The complete RL algorithm scheme can\nbe found in Figure 1.\n3\nEvaluation\nWe conduct ﬁve experiments to test the behaviour of the ﬁngering algorithm EX1, EX2, EX3, EX4\nand EX5. EX1 contains a sequence of notes with the same pitch and rhythmic ﬁgure. This experiment\naims to test whether the RL agent learns to use the same ﬁnger in every note. In EX2, we have a\npartial split scale of ﬁve ascending notes and the same ﬁve descending notes. In this experiment, we\ntest whether the RL agent learns not to change the hand position. The EX3 is a piece of music that\ndoes not change the hand position throughout its length. Similarly to EX2, EX3 aims at keeping the\nsame position but in a complex environment. EX is a C major scale. Therefore, the RL agent should\nlearn to perform only two hand position changes. The ﬁfth test is a piece with the melodic range of\nthe C major scale. In this case, we want to test whether the RL agent learns to keep the same two\nhand position as EX4 but in a complex environment. All these experiments have been carried out\nwith various improvements and with different numbers of episodes.\nFor the ﬁrst two experiments, due to their simplicity, the results are as expected. In EX1, the same\nﬁnger is playing all the notes sequence, and in the second experiment, there is no change of hand\nposition. In the EX1, each note is encoded as 88 grooves, while rest of the experiments, the encoding\nis reduced to the melodic range, improving the convergence time, as is shown in Figure 2. In some\ncases, the reward function oscillates when following a less conservative strategy and trying to explore\nfor too long, as exposed in EX3, Figure 3. The evolution of reinforcement learning can be seen in\nEX4, shown in Figure 4. The trial and error to achieve optimum ﬁngering resemble the process carried\nout by a pianist. In EX5, Figure 5, we can see the reward function evolves little by little, approaching\nthe expected result. Although the preliminary reinforcement results surpass our expectations, the\nsystem requires more time to achieve the desired results compared with ofﬂine methods. In Section 4,\nwe plan strategies to solve the convergence issue.\n4\nConclusions and future work\nAlthough pianists learn to ﬁnger throughout their career by trial and error, they do not start from\nzero in every score/environment. All the works they have played and also ﬁngered before help\nmusicians to ﬁnger a new score. RL previous approaches attempt to address this through multi-agent\nRL [15], bringing ofﬂine and online RL [16] or ﬁne-tuning the models with RL [17]. We have chosen\nPianoplayer [12], a non-data-driven algorithm that summarises the concept of comfortable, to create a\nsynthetic dataset of more than 1500 piano scores. This dataset has been created by cross-referencing\nMusescore public domain works with the most famous classical piano composers. Thereby we want to\ndemonstrate that it is possible to incorporate synthetic knowledge in a supervised way with a shallow\nGRU network architecture. This architecture was previously proposed for time series RL [18]. The\nresults also surpass our expectations, and the implementation is available online [19]. The shallow\narchitecture can imitate expert systems in a 78% balanced accuracy and feasible combinations of\ndata as shown in Figure 6 data. Departing from the presented synthetic mode, in the future, we will\nexplore different ways of incorporating the existing ﬁngering knowledge into reinforcement learning\nmethods.\n2\nAcknowledgement\nI would like to thank my two colleagues Sergio Izquierdo and Julia Guerrero from the University of\nFreiburg/University of Zaragoza, for all the help in understanding the RL paradigm and the discussion\nabout this project.\nReferences\n[1] \"Source code supplementary material of Piano ﬁngering with reinforcement learning\".\nhttps://github.com/PRamoneda/RL_PianoFingering. Accessed: 2021-09-24.\n[2] Neuhaus, Heinrich. The art of piano playing. Kahn and Averil, 1958.\n[3] Chiantore, L. Historia de la técnica pianística. Alianza Madrid, 2001.\n[4] Brée, M. The groundwork of the Leschetizky method. G. Schirmer, 1905.\n[5] Nieto, A. La digitación pianística. Fundación Banco Exterior, 1988.\n[6] Levaillant, D., Poch, C. & Guinovart, C. Le piano, vol. 1. J.C. Lattès, 1986.\n[7] Ramoneda, P., Tamer, N. C., Eremenko, V., Miron, M. & Serra, X. Score difﬁculty analysis for\npiano performance education. Submitted to ICASSP2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) (2022)\n[8] Ramoneda, P. Computational methods to study piano music in education context (Master’s Thesis,\nUniversitat Pompeu Fabra, 2021).\n[9] Gellrich, M. & Parncutt, R. Piano technique and ﬁngering in the eighteenth and nineteenth\ncenturies: Bringing a forgotten method back to life. British Journal of Music Education 15, 5–23\n(1998).\n[10] Parncutt, R., Sloboda, J. A., Clarke, E. F., Raekallio, M. & Desain, P. An Ergonomie Model\nof Keyboard Fingering for Melodic Fragments Sibelius Academy of Music , Helsinski. Music\nperception: An Interdisciplinary Journal 14, 341– 382 (1997).\n[11] Balliauw, M., Herremans, D., Palhazi Cuervo, D. & Sörensen, K. A variable neighborhood\nsearch algorithm to generate piano ﬁngerings for polyphonic sheet music. International Transac-\ntions in Operational Research 24, 509–535 (2017).\n[12] PianoPlayer automatic piano ﬁngering generator. https://github.com/marcomusy/pianoplayer.\nAccessed: 2021-08-18.\n[13] Nakamura, E., Ono, N. & Sagayama, S. Merged-output hmm for piano ﬁngering of both hands.\nIn Proceedings of the 13th International Society for Music Information Retrieval Conference,\nISMIR 2014, 531–536 (Taipei, 2014).\n[14] Nakamura, E., Saito, Y. & Yoshii, K. Statistical learning and estimation of piano ﬁngering.\nInformation Sciences 517, 68–85 (2020).\n[15] Bu¸soniu, L., Babuška, R., & De Schutter, B. (2010). Multi-agent reinforcement learning: An\noverview. Innovations in multi-agent systems and applications-1, 183-221.\n[16] Nair, A., Dalal, M., Gupta, A., & Levine, S. (2020). Accelerating online reinforcement learning\nwith ofﬂine datasets. arXiv preprint arXiv:2006.09359.\n[17] Jaques, N., Gu, S., Turner, R. E., & Eck, D. (2016). Generating music by ﬁne-tuning recurrent\nneural networks with reinforcement learning.\n[18] Gao, Xiang. (2018). Deep reinforcement learning for time series: playing idealized trading\ngames. Arxiv preprint.\n[19] \"Further work: Source code of Piano ﬁngering with reinforcement learning\".\nhttps://github.com/PRamoneda/further_work_rl. Accessed: 2021-09-24.\n3\nAlgorithm 1 Deep Q-Learning with Experience Replay\nInitialize replay memory D to capacity N\nInitialize action-value function Q with two random sets of weights θ, θ′\nfor episode = 1, M do\nfor t = 1, T do\nSelect a random action at with probability ε\nOtherwise, select at = arg maxaQ(st, a; θ)\nExecute action at, collect reward rt+1 and observe next state st+1\nStore the transition (st, at, rt+1, st+1) in D\nSample mini-batch of transitions (sj, aj, rj+1, sj+1) from D\nif sj+1is terminal then\nyj = rj+1\nelse\nyj = rj+1 + γ maxa′ Q(sj+1, a′; θ′)\nend if\nPerform a gradient descent step using targets yj with respect to the online parameters θ\nEvery C steps, set θ′ ←θ\nend for\nend for\nFigure 1: Deep Q neural network diagram.\n(a)\n(b)\nFigure 2: (a) Test 1 trained on 1000 episodes. (b) test 2 trained on 100\n4\n(a)\n(b)\nFigure 3: (a) Test 3 trained on 200 episodes. (b) Episode/reward over time.\n5\n(a)\n(b)\n(c)\n(d)\nFigure 4: (a) Test 4 trained on 500 episodes. (b) Test 4 trained on 2000 episodes. (c) Test 4 trained on\n5000 episodes. (d) Evolution of reward in 5000 episodes\n6\n(a)\n(b)\nFigure 5: (a) Test 5 trained on 500 episodes. (b) Episode/reward over time.\nFigure 6: Test 3 synthetic model results.\n7\n",
  "categories": [
    "cs.OH"
  ],
  "published": "2021-11-15",
  "updated": "2021-11-15"
}