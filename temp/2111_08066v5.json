{
  "id": "http://arxiv.org/abs/2111.08066v5",
  "title": "Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning",
  "authors": [
    "Vincent Liu",
    "James R. Wright",
    "Martha White"
  ],
  "abstract": "Offline reinforcement learning -- learning a policy from a batch of data --\nis known to be hard for general MDPs. These results motivate the need to look\nat specific classes of MDPs where offline reinforcement learning might be\nfeasible. In this work, we explore a restricted class of MDPs to obtain\nguarantees for offline reinforcement learning. The key property, which we call\nAction Impact Regularity (AIR), is that actions primarily impact a part of the\nstate (an endogenous component) and have limited impact on the remaining part\nof the state (an exogenous component). AIR is a strong assumption, but it\nnonetheless holds in a number of real-world domains including financial\nmarkets. We discuss algorithms that exploit the AIR property, and provide a\ntheoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we\ndemonstrate that the algorithm outperforms existing offline reinforcement\nlearning algorithms across different data collection policies in simulated and\nreal world environments where the regularity holds.",
  "text": "Journal of Artiﬁcial Intelligence Research 77 (2023) 71-101\nSubmitted 01/2023; published 05/2023\nExploiting Action Impact Regularity and Exogenous State\nVariables for Oﬄine Reinforcement Learning\nVincent Liu\nvliu1@ualberta.ca\nJames R. Wright\njames.wright@ualberta.ca\nMartha White\nwhitem@ualberta.ca\nUniversity of Alberta and Alberta Machine Intelligence Institute (Amii)\nEdmonton, Alberta, Canada\nAbstract\nOﬄine reinforcement learning—learning a policy from a batch of data—is known to be\nhard for general MDPs. These results motivate the need to look at speciﬁc classes of MDPs\nwhere oﬄine reinforcement learning might be feasible. In this work, we explore a restricted\nclass of MDPs to obtain guarantees for oﬄine reinforcement learning. The key property,\nwhich we call Action Impact Regularity (AIR), is that actions primarily impact a part of\nthe state (an endogenous component) and have limited impact on the remaining part of\nthe state (an exogenous component). AIR is a strong assumption, but it nonetheless holds\nin a number of real-world domains including ﬁnancial markets. We discuss algorithms that\nexploit the AIR property, and provide a theoretical analysis for an algorithm based on\nFitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing oﬄine\nreinforcement learning algorithms across diﬀerent data collection policies in simulated and\nreal world environments where the regularity holds.\n1. Introduction\nOﬄine reinforcement learning (RL) involves using a previously collected static dataset,\nwithout any online interaction, to learn an output policy. This problem setting is important\nfor a variety of real world problems where learning online can be dangerous, such as for self-\ndriving cars, or when building a good simulator is diﬃcult or costly, such as for healthcare.\nIt is also a useful setting for applications where there is a large amount of data available,\nsuch as dynamic search advertising.\nA challenge in oﬄine RL is that the quality of the output policy can be highly depen-\ndent on the data. Most obviously, the data might not cover some parts of the environment,\nresulting in two issues. The ﬁrst is that the learned policy, when executed in the environ-\nment, is likely to deviate from the behavior that generated its training data and reach a\nstate-action pair that was unseen in the dataset. For these unseen state-action pairs, the\nalgorithm has no information about how to choose a good action. The second issue is that if\nthe dataset does not contain transitions in the high-reward regions of the state-action space,\nit may be impossible for any algorithm to return a good policy. One can easily construct a\nfamily of MDPs with missing data such that no algorithm can identify the MDP and suﬀer\na large suboptimality gap (Chen & Jiang, 2019).\nThe works that provide guarantees on the suboptimality of the output policy usually\nrely on strong assumptions about good data coverage and mild distribution shift. The theo-\nretical results are for methods based on approximate value iteration (AVI) and approximate\n©2023 AI Access Foundation. All rights reserved.\narXiv:2111.08066v5  [cs.LG]  3 May 2023\nLiu, Wright, & White\npolicy iteration (API), with results showing the output policy is close to an optimal policy\n(Farahmand, Szepesv´ari, & Munos, 2010; Munos, 2003, 2005, 2007). They assume a small\nconcentration coeﬃcient, which is the ratio between the state-action distribution induced by\nany policy and the data distribution (Munos, 2003). However, the concentration coeﬃcient\ncan be very large or even inﬁnite in practice, so assuming a small concentration coeﬃcient\ncan be unrealistic for many real world problems. Diﬀerent measures of distribution shift\nare also used, for example, Yin, Bai, and Wang (2021) assume the visitation distribution of\nthe least occupied state-action pair is greater than zero.\nTo avoid making strong assumptions on the concentration coeﬃcient, several works\nconsider constraining divergence between the behavior policy and the output policy on\nthe policy improvement step for API algorithms. The constraints can be enforced either\nas direct policy constraints or by a penalty added to the value function (Levine, Kumar,\nTucker, & Fu, 2020; Wu, Tucker, & Nachum, 2019). Another approach is to constrain the\npolicy set such that it only chooses actions or state-action pairs with suﬃcient data coverage\nwhen applying updates for AVI algorithms (Kumar, Fu, Soh, Tucker, & Levine, 2019; Liu,\nSwaminathan, Agarwal, & Brunskill, 2020). However, these methods only work when the\ndata collection policy covers an optimal policy (see our discussion in Section 3), which can\nbe diﬃcult or impossible to guarantee.\nAnother direction has been to assume pessimistic values for unknown state-action pairs,\nto encourage the agent to learn an improved policy that stays within the parts of the space\ncovered by the data. CQL (Kumar, Zhou, Tucker, & Levine, 2020) penalizes the values for\nout-of-distribution actions and learns a lower bound of the value estimates. A related idea\nis to constrain the bootstrap target to avoid out-of-distribution actions, introduced ﬁrst\nin the BCQ algorithm (Fujimoto, Meger, & Precup, 2019) with practical improvements\ngiven by IQL (Kostrikov, Nair, & Levine, 2022). MOReL (Kidambi, Rajeswaran, Netra-\npalli, & Joachims, 2020) learns a model and an unknown state-action detector to partition\nstates similar to the R-Max algorithm (Brafman & Tennenholtz, 2002), but then uses the\nprinciple of pessimism for these unknown states rather than optimism.\nSafe policy im-\nprovement methods (Thomas, Theocharous, & Ghavamzadeh, 2015; Laroche, Trichelair, &\nDes Combes, 2019) rely on a high-conﬁdence lower bound on the output policy performance,\nperforming policy improvement only when the performance is higher than a threshold.\nIn practice, the results of pessimistic approaches are mixed. Some have been shown to be\neﬀective on the D4RL dataset (Fu, Kumar, Nachum, Tucker, & Levine, 2020). Other results,\nhowever, show methods can be too conservative and fail drastically when the behavior policy\nis not near-optimal (Kumar et al., 2020; Liu et al., 2020), as we reaﬃrm in our experiments.\nFurther, actually using these pessimistic methods can be diﬃcult, as their hyper-parameters\nare not easy to tune in the oﬄine setting (Wu et al., 2019) and some methods require an\nestimate of the behavior policy or data distribution (Kumar et al., 2019; Liu et al., 2020).\nIntuitively, however, there are settings where oﬄine RL should be eﬀective. Consider a\ntrading agent in a stock market. A policy that merely observes stock prices and volumes\nwithout buying or selling any shares provides useful information about the environment.\nFor this collected dataset, an oﬄine agent can counter-factually reason about the utility of\nmany diﬀerent actions as demonstrated in Figure 1, because its actions have limited impact\non the prices and volumes. Such MDPs, which are called Exogenous MDPs, have states\nthat separate into exogenous states (stock price), not impacted by actions, and endogenous\n72\nExploiting AIR and Exogenous State Variables for Offline RL\nCounterfactual \nReasoning\nObserved \nExogenous\nTrajectory 1\n1100\n1000\n900\na = 0\na = 1\n1100\n5\n1000\n5\n900\n6\n900\n4\n900\n5\n1100\n6\nExogenous State\nEndogenous State\na = −1\n1100\n7\nFigure 1: In the stock market example, the exogenous state corresponds to the stock price,\nthe endogenous state corresponds to number of shares the agent has, and the\naction corresponds to the number of share to buy or sell at each time step. Given\nan observed exogenous trajectory, the agent can counter-factually reason about\nthe outcomes of diﬀerent actions and endogenous state.\nstates (number of shares owned by the agent). The structure in Exogenous MDPs has been\nused in online RL to learn more eﬃciently (Dietterich, Trimponias, & Chen, 2018).\nThis exogenous structure, however, has yet to be formally investigated for oﬄine RL,\nthough it is likely already being exploited in industry. Exploiting this structure is natural\nin applied ﬁnancial applications, because datasets allow for alternative trajectories to be\nsimulated, as described in the above example.\nOne (unpublished) system uses RL and\ntrajectory simulation for the optimal order execution problem (Burhani, Ding, Hernandez-\nLeal, Prince, Shi, & Szeto, 2020); it seems likely that there are other such systems in use.\nWhat has yet to be done, however, is to understand the theoretical properties of such\nalgorithms, as well as potential algorithmic improvements.\nIn this paper, we ﬁrst generalize the deﬁnition of exogenous MDPs, and formalize the\naction impact regularity (AIR) property. We say an MDP has the AIR property—or is\nε-AIR—if the actions have a limited impact on exogenous dynamics, with the level of\nimpact determined by ε ≥0.\nThis generalizes the previous deﬁnition, which required\nstrict separation, corresponding to ε = 0. We develop both theory and algorithms for this\nmore general setting, assuming access only to an oﬄine dataset, an approximate (learned)\nendogenous model and the reward function.1\nWe design an eﬃcient algorithm, called FQI-AIR, to exploit the AIR property, that\n(1) does not require an estimate of the behavior policy or the data distribution, (2) has\na straightforward approach to select hyperparameters using just the given data and (3) is\nmuch less sensitive to the quality of the data collection policy, if our assumptions hold. This\nalgorithm is a simple extension of FQI, but is signiﬁcantly more computationally eﬃcient\n1. We could assume an approximate instead of exact reward model. However, in most RL analysis, the\nerror on the transition model is of a higher order than the error for reward models (for example, see\nAgarwal, Kakade, and Yang (2020)). For simplicity, it is often assumed the reward model is known.\n73\nLiu, Wright, & White\nthan the trajectory simulation approach mentioned above and allows us to leverage and\nextend the existing theory for FQI. We bound the suboptimality of the output policy from\nFQI-AIR, in terms of ε and other standard terms (model errors and the inherent Bellman\nerror, see Section 5.2). Importantly, in place of the concentration coeﬃcient, we have a\nterm that depends on the size of the endogenous state and number of actions; when the\nconcentration coeﬃcient is bounded and small, it is on the same order as this term.\nWe then conduct a comprehensive empirical study of FQI-AIR. We compare several\nalgorithms in two simulated environments, across three diﬀerent data collection policies,\nwith varying oﬄine dataset sizes, for ε = 0 (assumption perfectly satisﬁed) and a larger\nε (assumption somewhat violated). FQI-AIR signiﬁcantly outperforms the oﬄine RL al-\ngorithms that do not leverage the AIR property—including FQI, MBS-QI, CQL and IQL;\nthis outcome is expected, but nonetheless veriﬁes that exploiting the AIR property, when\nappropriate, can have a big beneﬁt. We show that these conclusions extend to two environ-\nments based on real-world datasets (for bitcoin trading and for controlling battery usage in\na hybrid car). An important detail here is how hyperparameters are chosen. FQI-AIR can\nexploit AIR for policy evaluation, to automatically select hyperparameters. For the other\nalgorithms, we do not have such an approach, and instead report idealized performance by\npicking hyperparameters based on performance in the environment.\nFinally, these results all used the true endogenous model for FQI-AIR. We chose to do\nso partly because the endogenous model is known for certain AIR-MDPs (e.g., trading,\ninventory management) and partly to focus the investigation on the role of ε rather than\nmodel error. However, for certain AIR-MDPs, we will not have access to the true endogenous\nmodel. In our ﬁnal experiment, we investigated the impact of using a learned endogenous\nmodel in the hybrid car environment, and show that FQI-AIR remains eﬀective.\nAll of this is only possible because we make a strong assumption about the environment.\nHowever, given the hardness results in oﬄine RL, we should acknowledge that we likely need\nto restrict the class of MDPs. This work is a step towards understanding for what classes\nof MDPs oﬄine RL is feasible. At the same time, though we consider a restricted setting, it\nis by no means a trivial setting. There are many real-world examples where this regularity\nholds (as we discuss later in this work). This is doubly true given that our generalization\nprovides some ﬂexibility in violating the assumption: the regularity only needs to hold\napproximately rather than exactly. The algorithms and theory developed here can beneﬁt\nthese real-world applications now, by providing an approach that is well-designed and well-\nbehaved with strong theoretical guarantees for their speciﬁc problem setting.\n2. Problem Formulation\nThe agent-environment interaction is formalized as a ﬁnite horizon Markov decision process\n(MDP) M = (S, A, P, r, H, ν). S is a set of states, and A is an set of actions; for simplicity,\nwe assume that both sets are ﬁnite. P : S × A →∆(S) is the transition probability where\n∆(S) is the set of probability distributions on S, and by slightly abusing notation, we will\nwrite P(s, a, s′) as the probability that the process will transition into state s′ when in state\ns it takes action a. The function r : S × A →[0, rmax] gives the reward when taking action\na in state s, where rmax ∈R+. H ∈Z+ is the planning horizon, and ν ∈∆(S) the initial\nstate distribution.\n74\nExploiting AIR and Exogenous State Variables for Offline RL\nIn the ﬁnite horizon setting, the policies are non-stationary. A non-stationary policy is\na sequence of memoryless policies (π0, . . . , πH−1) where πh : S →∆(A). We assume that\nthe set of states reachable at time step h, Sh ⊂S, are disjoint, without loss of generality,\nbecause we can always deﬁne a new state space S′ = S ×[H −1] where [n] := {0, 1, 2, . . . , n}.\nThen, it is suﬃcient to consider stationary policies π : S →∆(A).\nGiven a policy π, h ∈[H −1], and (s, a) ∈S × A, we deﬁne the value function\nand the action-value function as vπ\nh(s) := Eπ[PH−1\nt=h r(St, At)|Sh = s] and qπ\nh(s, a) :=\nEπ[PH−1\nt=h r(St, At)|Sh = s, Ah = a] where the expectation is with respect to Pπ\nM (we may\ndrop the subscript M when it is clear from the context). Pπ is the probability measure\non the random element in (S × A)H induced by the policy π and the MDP such that, for\nthe trajectory of state-action pairs (S0, A0, . . . , SH−1, AH−1), we have Pπ(S0 = s) = ν(s),\nPπ(At = a|S0, A0, . . . , St) = π(a|St), and Pπ(St+1 = s′|S0, A0, . . . , St, At) = P(St, At, s′)\nfor t ≥0 (Lattimore & Szepesv´ari, 2020).\nThe optimal value function is deﬁned by\nv∗\nh(s) := supπ vπ\nh(s), and the Bellman operator is deﬁned by\n(T qh)(s, a) = r(s, a) +\nX\ns′∈S\nP(s, a, s′) max\na′∈A qh(s′, a′).\nIn the batch setting, we are given a ﬁxed set of transitions D with samples drawn from\na data distribution µ over (S × A).\nIn this paper, we consider the setting where the\ndata is collected by a data collection policy πb.\nThat is, D consists of N trajectories\n(S(i)\n0 , A(i)\n0 , . . . , S(i)\nH−1, A(i)\nH−1) induced by the the interaction of the policy πb and MDP M.\nA representative algorithm for the batch setting is Fitted Q Iteration (FQI) (Ernst,\nGeurts, & Wehenkel, 2005). In the ﬁnite horizon setting, FQI learns an action-value function\nfor each time step, q0, . . . , qH−1 ∈F where F ⊆RS×A is the value function class. The\nalgorithm is deﬁned recursively from the end of the episode: for each time step h from\nH −1 to 0, qh = arg minq∈F ∥q −ˆT qh+1∥2\n2 where ˆT is deﬁned by replacing expectations with\nsample averages for the Bellman operator T and qH = 0. The output policy is obtained by\ngreedifying according to these action-value functions.\n3. Common Assumptions for Oﬄine RL\nIn this section we discuss common assumptions used in oﬄine RL. The most common\nassumptions typically concern properties of the data distribution and MDPs together: either\nthat it suﬃciently covers the set of possible transitions, or that it suﬃciently covers a near-\noptimal policy.\nHowever, these assumptions are often impractical for many real world\napplications.\nFurthermore, recent results show that restriction on the data distribution\nalone are insuﬃcient to obtain guarantees. This motivates considering realistic assumptions\non the MDP alone, as we do in this work.\nSuﬃcient coverage of the data distribution has primarily been quantiﬁed by concentra-\ntion coeﬃcients (Munos, 2003). Given a data distribution µ, the concentration coeﬃcient\nC is deﬁned to be the smallest value such that, for any policy π,\nmax\nh∈[H−1]\nmax\ns∈Sh,a∈Ah\nPπ(Sh = s, Ah = a)\nµ(s, a)\n≤C.\n75\nLiu, Wright, & White\nIf µ(s, a) = 0 for some (s, a), then we deﬁne C = ∞by convention. Several results bound\nthe suboptimality of batch API and AVI algorithms in terms of the concentration coeﬃ-\ncient (Chen & Jiang, 2019; Farahmand et al., 2010; Munos, 2003, 2007). For example, it\nhas been shown that FQI outputs a near-optimal policy when C is small and the value\nfunction class is rich enough, where the upper bound on the suboptimality of the output\npolicy scales linearly with\n√\nC.\nHowever, in practice, the concentration coeﬃcient can be very large or even inﬁnite.\nFor example, if the data collection policy is not well-randomized or exploratory—often the\ncase in practice—then the concentration coeﬃcient is inﬁnite due to missing some state-\naction pairs. Munos (2007) provides some intuition about the size of the concentration\ncoeﬃcient. Suppose that the data distribution is uniform (e.g., µ(s, a) = 1/|S||A|) and the\nenvironment transition probability is less uniform, that is, there exists some policies such\nthat the visitation distribution concentrates on a single state-action pair (e.g., Pπ(Sh =\ns, Ah = a) = 1 for some s, a and h), then the concentration coeﬃcient can be as large as\nthe number of state-action pairs\nAnother direction has been to consider approaches where the data covers a near-optimal\npolicy. The key idea behind these methods is to restrict the policy to choose actions that\nhave suﬃcient data coverage, which is eﬀective if the given data has near-optimal action\nselection. For example, BCQ (Fujimoto et al., 2019) and BEAR (Kumar et al., 2019) only\nbootstrap values from actions a if the probability πb(a|s) is above a threshold b. MBS-QI\n(Liu et al., 2020) extends this to consider state-action probabilities, only bootstrapping\nfrom state-action pairs (s, a) when µ(s, a) is above a threshold. The algorithm is modiﬁed\nfrom FQI by replacing the bootstrap value qh(s, a) by ˜qh(s, a) := I{µ(s, a) ≥b}qh(s, a) and\nthe policy is greedy with respect to ˜qh(s, a). If a state-action pair does not have suﬃcient\ndata coverage, its value is zero. They show that MBS-QI outputs a near-optimal policy if\nPπ∗(µ(Sh, Ah)<b) is small for all h∈[H−1]. That is, the data provides suﬃcient coverage\nfor state-action pairs visited under an optimal policy π∗.\nThough potentially less stringent than having a small concentration coeﬃcient, this\nassumption can be impractical. We may be able to satisfy this assumption in simulated\nenvironments, such as those in our experiments; in the real world, though, if we have a\nsimulator we are unlikely to use oﬄine RL. For many real world domains, optimal policies\nare unknown. In fact, one of the primary purposes of using oﬄine RL is to get (signiﬁcantly)\nimproved policies. It is also hard to carefully design a data collection policy to cover an\nunknown optimal policy, making it diﬃcult to even check whether this assumption holds.\nFinally, some recent negative results suggest it is not suﬃcient to have a good data\ndistribution alone, and that it will be necessary to also make assumptions on the MDP.\nIn particular, Chen and Jiang (2019) showed that if we do not make assumptions on the\nMDP dynamics, no algorithm can achieve a polynomial sample complexity to return a\nnear-optimal policy, even when the algorithm can choose any data distribution µ. Wang,\nFoster, and Kakade (2021) provide an exponential lower bound for the sample complexity\nof oﬀ-policy policy evaluation and optimization algorithms with qπ-realizable linear func-\ntion class, even when the data distribution induces a well-conditioned covariance matrix.\nZanette (2021) provide an example where oﬄine RL is exponentially harder than online\nRL, even with the best data distribution, qπ-realizable function class and assuming the\nexact feedback is observed for each sample in the dataset. Xiao, Lee, Dai, Schuurmans,\n76\nExploiting AIR and Exogenous State Variables for Offline RL\nand Szepesvari (2022) provide an exponential lower bound for the sample complexity of\nobtaining nearly-optimal policies when the data is obtained by following a data collection\npolicy. These results are consistent with the above, since achieving a small concentration\ncoeﬃcient implicitly places assumptions on the MDP.\nThe main message from these negative results is that a good data distribution alone is\nnot suﬃcient. We need to investigate realistic problem-dependent assumptions for MDPs.\nIn the remainder of this work, we explore a restricted class of MDPs, for which we can\nobtain much stronger guarantees when learning oﬄine, without stringent requirements on\ndata collection.\n4. Action Impact Regularity\nActions play an important role for the exponential lower bound constructions cited in the\nlast section. They use tree structures where diﬀerent actions lead to diﬀerent subtrees and\nhence diﬀerent sequence of futures states and rewards. A class of MDPs that do not suﬀer\nfrom these lower bounds are those where actions do not have such strong impact on the\nfuture states and rewards. In this section, we introduce the Action Impact Regularity (AIR)\nproperty, a property of the MDP which allows for more eﬀective oﬄine RL. The state is\npartitioned into an exogenous and endogenous component, and the property reﬂects that\nthe agent’s actions primarily impact the endogenous state with limited inﬂuence on the\nexogenous state. We ﬁrst provide the formal deﬁnition and assumptions we leverage to\ndesign a practical oﬄine RL algorithm and then discuss when these assumptions are likely\nto be satisﬁed.\n4.1 Formal Deﬁnition and Assumptions\nWe use the standard state decomposition from Exogenous MDPs (McGregor, Houtman,\nMontgomery, Metoyer, & Dietterich, 2017; Dietterich et al., 2018). We assume the state\nspace is S = Sexo × Send where Sexo is the exogenous variable and Send is the endoge-\nnous variable.\nThe transition dynamics are P exo : Sexo × A →∆(Sexo) and P end :\nS × A →∆(Send) for exogenous and endogenous variable respectively.\nThe transition\nprobability from a state s1 = (sexo\n1\n, send\n1\n) to another state s2 = (sexo\n2\n, send\n2\n) is P(s1, a, s2) =\nP exo(sexo\n1\n, a, sexo\n2\n)P end(s1, a, send\n2\n).\nDeﬁnition 1 (The AIR Property). An MDP is ε-AIR if S = Sexo × Send, and for any\nactions a, a,′ ∈A, the next exogenous variable distribution is similar if either action a or a′\nis taken. That is, for each state s ∈S,\nDTV\n\u0000P exo(sexo, a), P exo(sexo, a′)\n\u0001\n≤ε\nwhere DTV is the total variation distance between two probability distributions on Sexo.\nFor discrete spaces, the total variation distance is DTV (P, P ′) = 1\n2∥P −P ′∥1 (ℓ1 norm).\nWe deﬁne the AIR-MDP such that the property holds for all exogenous state-action\npairs. If the property does not hold for one of the exogenous state-action pairs, then one\ncan design an adversarial MDP that hides all diﬃculties in this single exogenous state-action\npair and assuming the properties hold for all but one pair would be useless (Jiang, 2018).\n77\nLiu, Wright, & White\nAccess to an (approximate) endogenous model is critical to exploit the AIR property,\nand is a fundamental component of our algorithm. To be precise, we make the following\nassumption in this paper.\nAssumption 1 (AIR with an Approximate Endogenous Model). We assume that the MDP\nis εair-AIR and that we have the reward model r : S × A →[0, rmax] and an approximate\nendogenous model ˆP end : S × A →∆(Send) such that DTV (P end(s, a), ˆP end(s, a)) ≤εp for\nany (s, a) ∈S × A.\nAs mentioned in the introduction, it is common to assume that only the transition\ndynamics are approximated. Moreover, similar to Deﬁnition 1, we need the error on the\napproximate model to hold uniformly. Finally, the above assumption implicitly assumes that\nthe separation between exogenous and endogenous state is given to us. More generally, the\nseparation could be identiﬁed or learned by the agent, as has been done for contingency-\naware RL agents (Bellemare, Veness, & Bowling, 2012) and wireless networks (Dietterich\net al., 2018). Because there are many settings where the separation is clear, we focus on\nthis more clear case ﬁrst where the separation is known.\n4.2 When Are These Assumptions Satisﬁed?\nMany real-world problems can be formulated as ε-AIR MDPs. Further, for many of these\nenvironments, the separation between exogenous and endogenous state is clear, and we\neither know or can reasonably approximate the endogenous model. In this section, we go\nthrough several concrete examples.\nWe can ﬁrst return to our stock trading example, from the introduction. The exoge-\nnous component is the market information (stock prices and volumes) and the endogenous\ncomponent is the number of stock shares owned by the agent. The agent’s actions inﬂu-\nence their own number of shares, but as an individual trader, have limited impact on stock\nprices. Using a dataset of stock prices over time allows the agent to reason counterfactu-\nally about the impact of many possible trajectories of actions (buying/selling) on its shares\n(endogenous state) and proﬁts (reward).\nThere are many settings where the agent has a limited impact on a part of the state.\nThe optimal order execution problem is a task to sell M shares of a stock within H steps;\nthe goal is to maximize the proﬁt. The problem can be formulated as an MDP where the\nexogenous variable is the stock price and endogenous variable is the number of shares left to\nsell. It is common to assume inﬁnite market liquidity (Nevmyvaka, Feng, & Kearns, 2006)\nor that actions have a small impact on the stock price (Abernethy & Kale, 2013; Bertsimas\n& Lo, 1998); this corresponds to assuming the AIR property.\nAnother example is the secretary problem (Freeman, 1983), which a family of problems\nthat can often be used to model real-world application (Babaioﬀ, Immorlica, Kempe, &\nKleinberg, 2007; Goldstein, McAfee, Suri, & Wright, 2020). The goal for the agent is to\nhire the best secretary out of H, interviewed in random order. After each interview, they\nhave to decide if they will hire that applicant, or wait to see a potentially better applicant\nin the future. The problem can be formulated as a 0-AIR MDP where the endogenous\nvariable is a binary variable indicating whether we have chosen to stop or not.\nOther examples include those where the agent only inﬂuences energy eﬃciency, such as\nin the hybrid vehicle problem (Shahamiri, 2008; Lian, Peng, Wu, Tan, & Zhang, 2020) and\n78\nExploiting AIR and Exogenous State Variables for Offline RL\nelectric vehicle charging problem (Abdullah, Gastli, & Ben-Brahim, 2021). In the former\nproblem, the agent controls the vehicle to use either the gas engine or the electrical motor\nat each time step, with the goal to minimize gas consumption; its actions do not impact\nthe driver’s behavior. In the latter problem, the agent controls the charging schedule of an\nelectric vehicle to minimize costs; its actions do not impact electricity cost.\nIn some settings we can even restrict the action set or policy set to make the MDP\nε-AIR. For example, if we know that selling M shares hardly impacts the markets, we can\nrestrict the action space to selling less than or equal to M shares. In the hybrid vehicle\nexample, if the driver can see which mode is used, we can restrict the policy set to only\nswitch actions periodically to minimize distractions for the driver.\nIn these problems with AIR, we often know the reward and transitions for the endoge-\nnous variables, or have a good approximation. For the optimal order execution problem, the\nreward is simply the selling price times the number of shares sold minus transaction costs,\nand the transition probability for P end is the inventory level minus the number of shares\nsold. In other applications, we may be able to use domain knowledge to build an accurate\nmodel for the endogenous dynamics. For the hybrid vehicle, we can use domain knowledge\nto calculate how much gas would be used for a given acceleration. Such information about\nthe dynamics of the system can be simpler for engineers to specify, than (unknown) behav-\nior of diﬀerent drivers and environment conditions. Our theoretical results will include a\nterm for the error in the endogenous model, but it is reasonable to assume that for many\nsettings we can get that error to be relatively low, particularly in comparison to the error\nwe might get if trying to model the exogenous state.\n4.3 Connections to the Literature on Exogenous MDPs\nAIR MDPs can be viewed as an extension of Exogenous MDPs. (1) We allow the action\nto have small impact on the environmental state, while the action has no impact on the\nexogenous state in Exogenous MDPs. (2) We do not assume the reward can be decomposed\nadditively to an exogenous reward and an endogenous reward (Dietterich et al., 2018) nor\nfactor into a sum over each exogenous state variable (Chitnis & Lozano-P´erez, 2020). For\nthis previous deﬁnition of Exogenous MDPs, the focus was on identifying and removing\nthe exogenous state/noise so that the learning problem could be solved more eﬃciently\n(Dietterich et al., 2018; Efroni, Misra, Krishnamurthy, Agarwal, & Langford, 2022), thus\nthe focus on reward decomposition. Our focus is oﬄine learning where we want to exploit\nthe known structure to enable counterfactual reasoning and avoid data coverage issues.\n5. Oﬄine Policy Optimization for AIR MDPs\nIn this section, we discuss several oﬄine algorithms that exploit the AIR property for\npolicy optimization. We then theoretically analyze an FQI-based algorithm, characterizing\nthe performance of its outputted policy.\n5.1 Algorithms for AIR MDPs\nTwo standard classes of algorithms in oﬄine RL are model-based algorithms—that learn a\nmodel from the oﬄine dataset and then use dynamic programming—and model-free algo-\n79\nLiu, Wright, & White\nrithms like ﬁtted Q-iteration (FQI). These two approaches can be tailored to our setting\nwith AIR MDPs, as we described below. There is, however, an even more basic approach in\nour oﬄine RL setting using trajectory simulation, that has previously been used (Burhani\net al., 2020). We start by describing this simpler approach, and then the modiﬁed model-\nbased and FQI approaches.\nA natural approach is to reuse trajectories in the dataset to simulate alternative tra-\njectories for an online RL algorithm. For each episode, a random trajectory is selected\nfrom the dataset. The online RL algorithm—such as an actor-critic method or a Q-learning\nagent—takes actions and deterministically transitions to the next exogenous state in the\ntrajectory. The approximate endogenous and reward model are used to sample the next\nendogenous variable and reward. With such a trajectory simulator, we can run any online\nreinforcement learning algorithm to ﬁnd a good policy for the simulator.\nThis approach, however, does not exploit the fact that the agent is actually learning\noﬄine. The online RL algorithm cannot simply query the model for any state and action,\nand needs a good exploration strategy to ﬁnd a reasonable policy. There are fewer theoretical\nguarantees for such online RL algorithms, and arguably more open questions about their\nproperties than DP-based algorithms and ﬁtted value iteration algorithms.\nA more explicit model-based approach is to learn the exogenous model from data, to\nobtain a complete transition and reward model, and use any planning approach.\nThe\ntransition model for exogenous states can be constructed as if the action has no impact.\nWith the model, we can use any query-eﬃcient planning algorithm to ﬁnd a good policy\nfor the model. Because actions have only small impact in the true MDP, we can learn an\naccurate exogenous model even if we do not have full data coverage.\nMore precisely, recall the oﬄine data is randomly generated by running πb on M, that is,\nD = {(S(i)\n0 , A(i)\n0 , . . . , S(i)\nH−1, A(i)\nH−1)}N\ni=1 sampled according to the probability measure Pπb\nM.\nThe pertinent part is the transitions between exogenous variables, so we deﬁne Dexo =\n{(S(i)\n0 , S(i),exo\n1\n, . . . , S(i),exo\nH−1 )}N\ni=1. The model-based approach constructs an empirical MDP\nMD = (S, A, ˆP, r, H, ˆν). For the tabular setting we have ˆν(s) = 1\nN\nPN\ni=1 I(S(i)\n0\n= s), and\nˆP exo(sexo\nh , a, sexo\nh+1)=\nPN\ni=1 I(S(i),exo\nh\n= sexo\nh , S(i),exo\nh+1\n= sexo\nh+1)\nPN\ni=1 I(S(i),exo\nh\n= sexo\nh )\nfor all a ∈A. Exogenous variables not seen in the data are not reachable, and so can\neither be omitted from ˆP exo or set to self-loop.\nFor large or continuous state spaces,\nwe can learn p(sexo\nh+1|sexo\nh ) using any conditional distribution learning algorithm, and set\nˆP exo(sexo\nh , a, sexo\nh+1) = p(sexo\nh+1|sexo\nh ) for all a ∈A.\nFor large or continuous states spaces, however, learning such a model and planning can\nbe impractical. Learning an accurate exogenous model might be diﬃcult if the exogenous\ntransition is complex or the exogenous state is high-dimensional. Further, it is not possible\nto sweep through all states during planning. Smarter approximate dynamic programming\nalgorithms need to be used, but even these can be quite computationally costly.\nA reasonable alternative is FQI, which approximates value iteration without the need\nto learn a model. Our FQI algorithm that exploits the AIR property is described in Al-\ngorithm 1, which we call FQI-AIR. The algorithm simulates all actions from a state, and\nassumes it transitions to the exogenous state observed in the dataset. The reward and\n80\nExploiting AIR and Exogenous State Variables for Offline RL\nAlgorithm 1 FQI-AIR\nInput: dataset D, value function class F, ˆP end, r\nLet qH = 0, DH−1 = ∅, ..., D1 = ∅\nfor h = H −1, . . . , 0 do\nFor all i ∈{1, . . . , N}, all send\nh\n∈Send, all a ∈A\nSample s\n′end ∼ˆP end(s(i),exo\nh\n, send\nh\n, a), compute target\nt = r(s(i),exo\nh\n, send\nh\n, a) + max\na′∈A qh+1(s(i),exo\nh+1\n, s\n′end, a′)\nAdd (synthetic) pair ((s(i),exo\nh\n, send, a), t) to Dh\nAfter generating Dh\nqh = arg min\nf∈F\nX\n(x,y)∈Dh\n(f(x) −y)2\nπh(s) = arg max\na\nqh(s, a) for all s ∈Sh\nOutput: π = (π0, . . . , πH−1)\nendogenous state for each simulated action can be obtained using the reward model and\napproximate endogenous model. Even though the true MDP is not necessarily 0-AIR MDP,\nwe will show in the analysis that as long as εair is small, the algorithm can return a nearly\noptimal policy in the true MDP. This algorithm, although simple, enjoys theoretical guar-\nantees without making assumptions on the concentration coeﬃcient, and can be much more\ncomputationally eﬃcient than trajectory simulation methods.\nNote that the computational cost scales with the size of Send and A.\nWhen |Send|\nor |A| is large, we can modify FQI-AIR to no longer use full sweeps.\nInstead, we can\nrandomly sample from the endogenous state space and action.\nWe include a practical\nimplementation of FQI-AIR in Algorithm 2. For each exogenous state in the dataset, we\nsample an endogenous state and an action, and query the approximate model to obtain\na target for FQI update. As a result, the computation can be independent of the size of\nSend and A. However, for sample complexity, the performance loss of the algorithm would\ndepend on the squared root of the size |Send||A|, as shown in the next section.\n5.2 Theoretical Analysis of FQI-AIR\nFirst we need the following deﬁnitions. For a given MDP M, we deﬁne J(π, M) := Eπ\nM[R(τ)]\nwhere τ = (S0, A0, . . . , SH−1, AH−1) is a random element in (S × A)H, the expectation Eπ\nM\nis with respect to Pπ\nM, and R(τ) = PH−1\nh=0 r(Sh, Ah).\nWe also need the following assumption on the function approximation error. This is a\ncommon assumption to analyze approximate value iteration algorithms (Antos, Szepesv´ari,\n& Munos, 2006; Munos, 2007). Let ˜νh(sexo\nh ) = Pπb\nM(Sexo\nh\n= sexo\nh ) be the data distribution on\n81\nLiu, Wright, & White\nAlgorithm 2 FQI-AIR for large state spaces\nInput: dataset D, an approximate model ˆP end, r, mini-batch size B, number of training\niteration K, number of updates per iteration M\nInitialize a Q function qθ : Sexo × Send × A × [H] →R, parameterized by θ\n¯θ ←θ\nfor k = 1, . . . , K do\nfor m = 1, . . . , M do\nSample a mini-batch of transitions {(sexo\nj\n, send\nj\n, aj, hj, s\n′exo\nj\n, s\n′end\nj\n)}B\nj=1 from D\nFor all j, sample an endogenous state ˜send\nj\n∈Send and an action ˜aj ∈A randomly,\nsample ˜s\n′end\nj\n∼ˆP end(sexo\nj\n, ˜send\nj\n, ˜aj), and compute target\ntj = r(sexo\nj\n, ˜send\nj\n, ˜aj) + max\na′∈A q¯θ(s\n′exo\nj\n, ˜s\n′end\nj\n, a′, hj + 1)\nCompute the mini-batch loss L(θ) = PB\nj=1(qθ(sexo\nj\n, ˜send\nj\n, ˜aj, hj) −tj)2\nUpdate θ to reduce L(θ)\n¯θ ←θ\nOutput: the greedy policy with respect to qθ\nSexo at horizon h. Given a probability measure νh on Sexo and p ∈[1, ∞), deﬁne the norm\n∥q∥p\np,νh =\nX\nsexo∈Sexo\nX\nsend∈Send\nX\na∈A\nνh(sexo)\n|Send||A||q(sexo, send, a)|p.\nAssumption 2. Assume the function class F is ﬁnite and the inherent Bellman error is\nbounded by εapx, that is,\nεapx = max\nh∈[H] max\nf′∈F min\nf∈F ∥T f′ −f∥2\n2,˜νh.\nWe assume the function class is ﬁnite for simplicity, which is common in many oﬄine\nRL papers (Chen & Jiang, 2019). If the function class is not ﬁnite but has a bounded\ncomplexity measure, we can derive similar results by replacing the size of the function class\nwith the complexity measure. For example, Duan, Jin, and Li (2021) analyze FQI with the\nRademacher complexity. Since studying the complexity measure is not a critical point for\nour paper, we decide to make the ﬁnite function class assumption.\nTheorem 1. Under Assumption 1 and 2, let π∗\nM be an optimal policy in M and π the\noutput policy of FQI-AIR, then with probability at least 1 −ζ\nJ(π, M) ≥J(π∗\nM, M) −2vmaxH(εair + εp)−\n(H + 1)H\nq\n|Send||A|\n r\n72vmax2 ln (H|F||Send||A|/ζ)\nN\n+ 2εapx\n!\n.\nThe bound on performance loss has three components: (1) a sampling error term which\ndecreases with more trajectories; (2) the AIR parameter; and (3) an approximation error\n82\nExploiting AIR and Exogenous State Variables for Offline RL\nterm which depends on the function approximation used. The result implies that as long\nas we have a suﬃcient number of episodes, a good function approximation, and small εair,\nthen the algorithm can ﬁnd a nearly-optimal policy with high probability. For example, if\nεair, εp and εapx are small enough, we only need N = ˜O(H4vmax2|Send||A|/δ) trajectories,\nwhich is polynomial in H, to obtain a δ-optimal policy.\nThe proof can be found in the appendix. The key idea is to introduce a baseline MDP\nMb that is 0-AIR, that approximates M which is actually εair-AIR. The baseline MDP\nMb = (S, A, ˜P, r, H, ν) has ˜P exo(sexo\nh , a, sexo\nh+1) = Pπb\nM(Sexo\nh\n= sexo\nh , Sexo\nh+1 = sexo\nh+1)/Pπb\nM(Sexo\nh\n=\nsexo\nh ) and ˜P end(sh, a, send\nh\n) = ˆP end(sh, a, send\nh\n).\nThe transition probability for exogenous\nstate does not depend on the action a taken, so Mb is 0-AIR. We show that FQI returns a\ngood policy in Mb, and that good policies in Mb are also good in the true MDP M.\nWe can contrast this bound to others in oﬄine RL. For FQI results that assume the\nconcentration coeﬃcient is bounded and small, the error bound has a term that scales with\n√\nC, which is on the same order as the term\np\n|Send||A| in our bound. We can get a similar\nbound by considering this restricted class of MDPs that are ε-AIR, without having to make\nany assumptions on the concentration coeﬃcient. For settings where this assumption is\nappropriate—namely when the MDP is ε-AIR—this is a signiﬁcant success, as we need not\nmake these stringent conditions on data distributions.\n6. Policy Evaluation for AIR MDPs\nWe can also exploit the AIR property, and access to the approximate endogenous model\nand reward model, to evaluate the value of a given policy. Given a trajectories of exogenous\nstates (S(i)\n0 , S(i),exo\n1\n, . . . , S(i),exo\nH−1 ), we can rollout a synthetic trajectory under π and ˆP end:\nτ (i)\nD = (S(i)\n0 , A(i)\n0 , S(i)\n1 , A(i)\n1 , . . . ) where A(i)\nt\n∼π(S(i)\nt ), S(i),end\nt+1\n∼ˆP end(S(i)\nt , A(i)\nt ) and S(i)\nt+1 =\n[S(i),exo\nt+1\n, S(i),end\nt+1\n]. For R(τ (i)\nD ) := PH−1\nt=0 r(S(i)\nt , A(i)\nt ), the average return over the N synthetic\ntrajectories ˆJ(π, M) = 1\nN\nPN\ni=1 R(τ (i)\nD ) is an estimator of J(π, M). This method is simple,\nbut very useful because we can do hyperparameter selection with only the oﬄine dataset\nwithout introducing extra hyperparameters.\nWe can bound the policy evaluation error by Hoeﬀding’s inequality. More sophisticated\nbounds for policy evaluation can be found in (Thomas et al., 2015).\nTheorem 2. Under Assumption 1, given a deterministic policy π, we have that with\nprobability at least 1 −ζ\n\f\f\f ˆJ(π, M)−J(π, M)\n\f\f\f≤vmax\n\u0012\nHεair+Hεp+\nq\nln (2/ζ)\n2N\n\u0013\n.\nThe results suggests that if we have a suﬃcient number of trajectories and small εair\nand εp, then ˆJ(π, M) is a good estimator for J(π, M). Even though the estimator is biased\nand not consistent, we ﬁnd it provides suﬃcient information for hyperparameter selection\nin our experiments.\nTheorem 2 only holds for a given policy that is independent of the oﬄine data.\nIf\nwe want to evaluate the output policy, which depends on the data, we need to apply an\nunion bound for all deterministic policies. In that case, the sampling error term becomes\n˜O(\np\n|S|/N). To avoid the dependence on the state space, an alternative is to split the data\n83\nLiu, Wright, & White\ninto two subsets: one subset is used to obtain an output policy and another subset is used\nto evaluate the output policy.\n7. Simulation Experiments\nWe evaluate the performance of the FQI-based algorithm in two simulated environments\nwith the AIR property: an optimal order execution problem and an inventory management\nproblem. We also tested the other algorithms described in Section 5.1, for completeness\nand to contrast to FQI-AIR. We include these results in Appendix C. FQI-AIR is notably\nbetter than these other approaches, and so we focus on it as the representative algorithm\nthat exploits the AIR property.\nThe ﬁrst goal of these experiments is to demonstrate that existing oﬄine RL algorithms\nfail to learn a good policy for some natural data collection policies, while our proposed\nalgorithm returns a near-optimal policy. To demonstrate this, we test three data collection\npolicies: (1) a random policy which is designed to give a small concentration coeﬃcient, (2)\na learned near-optimal policy obtained using DQN with online interaction, which covers an\noptimal policy reasonably well, and (3) a constant policy which, in theory, has an inﬁnite\nconcentration coeﬃcient due to missing state-action pairs and does not cover an optimal\npolicy. The second goal is to validate the policy evaluation analysis with a varying number\nof trajectories N and εair.\n7.1 Environments\nWe investigated the behavior of the algorithms on two simulated environments that mimic\ntwo real-world problems that satisfy the AIR property: optimal order execution and inven-\ntory management. In the optimal order execution problem, the task is to sell M = 10\nshares within H = 100 steps. The stock prices X1, . . . , Xh are generated by an ARMA(2, 2)\nprocess and scaled to the interval [0, 1]. Speciﬁcally, the ARMA(2, 2) process is\nXt = ˜Xt/20 + 1/2\nwhere\n˜Xt = c + εt +\n2\nX\ni=1\nϕi ˜Xt−i +\n2\nX\ni=1\nθiεt−i,\nϕ1 ∼U(−0.9, 0.0), ϕ2 ∼U(0.0, 0.9) and θi ∼U(−0.5, 0.5) for i = 1, 2. The scaling pa-\nrameters are chosen so that the process is stable and the price is in the interval [0, 1].\nThe endogenous variable Ph is the number of shares left to sell. To construct state, we\nuse the most recent K = 3 prices with the most recent endogenous variable, that is,\nSh = (Xh−2, Xh−1, Xh, Ph).\nThe action space is A = [5].\nThe reward function is the\nstock price Xh multiplied by the number of selling shares min{Ah, Ph}.\nWe consider both a setting with εair = 0 and εair > 0, as well as diﬀerent data generating\npolicies. When the number of selling shares is greater than 0, the stock price drops by 10%\nwith probability εair. For εair = 0, this means selling shares has no impact on the stock\nprice. When εair > 0, it does, allowing us to test how robust FQI-AIR is to some violation\nof the AIR property. The random policy used in the environment chooses 0 with probability\n75% and choose 1, . . . , 5 with probability 5%. The constant policy always chooses action 0.\nWe design an inventory management problem based on existing literature (Kun-\nnumkal & Topaloglu, 2008; Van Roy, Bertsekas, Lee, & Tsitsiklis, 1997). The task is to\n84\nExploiting AIR and Exogenous State Variables for Offline RL\ncontrol the inventory of a product over H = 100 stages. At each stage, we observe the\ninventory level Xt (endogenous) and the previous demand Dt−1 (exogenous) and choose to\nplace a order At ∈[10]. The inventory level evolves as: Xt+1 = (Xt + At −Dt)+. The\nreward function is cAt + h(Xt + At −Dt)+ −b(−Xt −At + Dt)+ where c is the order cost,\nh is the holding cost, and b is the cost for lost sale. We use c = 0.1, h = 0.25 and b = 1.0 in\nthe experiment. To make sure the reward is bounded, we clip the reward at a large negative\nnumber −100. The endogenous variable is the inventory level, which can be as large as\n1000, so we restrict FQI-AIR to sweep only for a subset of the endogenous space, that is,\n[15] ⊂Send.\nAs before, we consider both εair = 0 and εair > 0, which in this case impacts the\ndemand. The demand Dt = ( ˜Dt)+ where ˜Dt follows a normal distribution with mean µ\nand σ = µ/3 and (d)+ := max{d, 0}. In the beginning of each episode, µ is sampled from\na uniform distribution in the interval [3, 9]. When the order is greater than 0, the mean of\nthe demand distribution decreases or increases by 10% with probability εair/2 respectively.\nAgain, we consider three diﬀerent data generating policies. The random policy used\nin the environment is a policy which chooses a value ˜At ∈[Dt−1 −3, Dt−1 + 3] uniformly\nand then choose the action At = max{min{ ˜At, 10}, 0}. The constant policy always chooses\naction At = min{Dt−1, 10}. The near-optimal policy is obtained using DQN with online\ninteraction, for both environments.\nThere is an important nuance for the inventory problem.\nIn this problem, the en-\ndogenous transition and reward depends on the next exogenous variable. Fortunately, we\ncan generalize the deﬁnition of exogenous MDPs such that the endogenous transition is\nP end : S × A × Sexo →∆(Send) and the reward is r : S × A × Sexo →[0, rmax]. We assume\nwe have an approximate endogenous model ˆP end : S × A × Sexo →∆(Send) such that\nDTV (P end(s, a, sexo), ˆP end(s, a, sexo)) ≤εp for any (s, a, sexo) ∈S × A × Sexo. With these\nchanges, the algorithms and the theoretical analysis naturally extend to the new deﬁnition\nof exogenous MDPs.\n7.2 Algorithm Details\nWe compare FQI-AIR to FQI, MBS-QI (Liu et al., 2020), CQL (Kumar et al., 2020), IQL\n(Kostrikov et al., 2022). As we discussed in the previous sections, FQI is expected to work\nwell when the concentration coeﬃcient is small. MBS-QI is expected to perform well when\nthe data covers an optimal policy. CQL and IQL are strong baselines which have been\nshown to be eﬀective empirically for discrete-action environments such as Atari games.\nWe had several choices to make for the baseline algorithms. MBS-QI requires density\nestimation for the data distribution µ. For the optimal order execution problem, we use\nstate discretization and empirical counts to estimate the data distribution as used in the\noriginal paper. For the inventory problem, the state space is already discrete so there is\nno need for discretization.\nWe show the results with the best threshold b from the set\n{0.002, 0.001, 0.0001, 0.00005}. Note that it is possible that there is no data for some states\n(or state discretization) visited by the output policy, and for these states, all action values\nare all zero. To break ties, we allow MBS-QI to choose an action uniformly at random. For\nCQL, we add the CQL(H) loss with a weighting α when updating the action values. We\nshow the results with the best α from the set {0.1, 0.5, 1.0, 5.0} as suggested in the original\n85\nLiu, Wright, & White\n1\n50\n100\n200\n#Episodes\n0\n2\n4\n6\nJ( , M)\nConstant Policy\n1\n50\n100\n200\n#Episodes\nRandom Policy\n1\n50\n100\n200\n#Episodes\nLearned Policy\nFQI-AIR\nCQL\nFQI\nIQL\nMBS-QI\n(a) Optimal order execution problem\n1\n50 100\n200\n#Episodes\n600\n400\n200\nJ( , M)\nConstant Policy\n1\n50 100\n200\n#Episodes\nRandom Policy\n1\n50 100\n200\n#Episodes\nLearned Policy\nFQI-AIR\nCQL\nFQI\nIQL\nMBS-QI\n(b) Inventory management problem\nFigure 2: Comparison between algorithms in the optimal order execution problem and in-\nventory management problem, for εair = 0. The gray lines show the performance\nof the data collection policies. Results are averaged over 30 runs with error bars\nfor one standard error.\npaper. For IQL, we show the results with the best τ from the set {0.7, 0.8, 0.9} and β from\nthe set {10.0, 3.0, 1.0}.\nWe use the same value function approximation for all algorithms in our experiments:\ntwo-layers neural networks with hidden size 128. The neural networks are optimized by\nAdam (Kingma & Ba, 2014) or RMSprop with learning rate in the set {0.001, 0.0003, 0.0001}.\nAll algorithms are trained for 100 iterations. We also tried training the comparator algo-\nrithms for longer, but it did not improve their performance.\nThe hyperparameters for FQI-AIR are selected based on ˆJ(π, M), which only depends\non the dataset.\nThe hyperparameters for comparator algorithms are selected based on\nJ(π, M)—which should be a large advantage—estimated by running the policy π on the\ntrue environment M with 100 rollouts.\n7.3 Policy Performance When εair = 0\nFigure 2 shows the performance of our algorithm and the comparator algorithms with a\ndiﬀerent number of trajectories N = {1, 5, 10, 25, 50, 100, 200} and εair = 0. Our algorithm\noutperforms other algorithms for all data collection policies. This result is not too surprising,\n86\nExploiting AIR and Exogenous State Variables for Offline RL\nas FQI-AIR is the only algorithm to exploit this important regularity in the environment;\nbut nonetheless it shows how useful it is to exploit this AIR property when it is possible.\nWe can ﬁrst look more closely at the optimal order execution results. MBS performs\nslightly better than FQI, however, we found it performs better because the tie-breaking is\ndone with a uniform random policy especially under the constant policy dataset.2 CQL and\nIQL fails when the data collection policy is far from optimal (constant policy) and perform\nreasonably with a learned policy. FQI-AIR exploits the AIR property, and so is robust\nto diﬀerent data collection policies. The results show that exploiting the AIR property is\ncritical for the robust performance.\nWe see similar patterns for the inventory management problem. FQI-AIR outperforms\nthe other algorithms for all data collection policies. CQL and IQL perform well in this\nenvironment. MBS outperforms FQI under the learned policy, but FQI outperforms MBS\nunder the random policy. The results match the expectation that FQI performs well with\nan uniform data and MBS-QI performs well with an expert data.\n7.4 Policy Performance with a Large Value of εair\nNow we consider the impact of using these algorithms when εair > 0. We should expect\nFQI-AIR to be most impacted, as the other algorithms do not exploit the AIR property.\nWe vary εair from 0.1 to 0.8 and ﬁnd that the results are similar to those with small εair.\nFQI-AIR still signiﬁcantly outperforms other oﬄine methods.\nFigure 3 shows the result with εair = 0.8 where the performance of all algorithms drop\nsigniﬁcantly. In theory, FQI-AIR can have a large performance loss with large εair, however,\nFQI-AIR still consistently outperforms other baselines in our experiments, except for the\ninventory management problem with the learned policy. This is because the divergence\nbetween the true exogenous transition and the synthetic exogenous transition in FQI-AIR\ndoes not occur at every time step even when εair is large. For example, in the optimal\norder execution problem, the divergence can only happen when we sell a positive number\nof shares. The theoretical result is the worst-case analysis where the divergence can occur\nat every time step and we suﬀer rmax loss every time the divergence occurs. Therefore, the\nexperiment results suggest that these practical problems considered in the paper are not\nthe worst case and FQI-AIR can perform well even with large εair.\n7.5 Results for Policy Evaluation\nTo validate the policy evaluation analysis, we investigate the diﬀerence | ˆJ(π, M)−J(π, M)|\nwith εair ∈{0, 0.05, 0.1, 0.2, 0.4} and N = {1, 5, 25, 100, 200} where π is the output policy\nof FQI-AIR. We show the 90th percentile of the diﬀerence for each combination of εair and\nN over 90 data points (30 runs under each data collection policy) in Figure 4. The 90th\npercentiles scale approximately linearly with εair and inversely proportional to N. The\nresults suggest that the dependence on εair is linear and the policy evaluation error goes to\nzero at a sublinear rate, which reﬂects the bound of Theorem 2.\n2. A uniform policy in this environment can achieve a performance J(π, M) ≈5.\n87\nLiu, Wright, & White\n1\n50\n100\n200\n#Episodes\n0\n2\n4\nJ( , M)\nConstant Policy\n1\n50\n100\n200\n#Episodes\nRandom Policy\n1\n50\n100\n200\n#Episodes\nLearned Policy\nFQI-AIR\nCQL\nFQI\nIQL\nMBS-QI\n(a) Optimal order execution problem\n1\n50 100\n200\n#Episodes\n1500\n1250\n1000\n750\n500\nJ( , M)\nConstant Policy\n1\n50 100\n200\n#Episodes\nRandom Policy\n1\n50 100\n200\n#Episodes\nLearned Policy\nFQI-AIR\nCQL\nFQI\nIQL\nMBS-QI\n(b) Inventory management problem\nFigure 3: Comparison between algorithms in two simulation problems with εair = 0.8. The\ngray lines show the performance of the data collection policies. The results are\naveraged over 30 runs with error bars representing one standard error.\nN=1\nN=5\nN=25\nN=100\nN=200\nAIR=0\nAIR=0.05\nAIR=0.1\nAIR=0.2\nAIR=0.4\n(a) Optimal order execution problem\n(b) Inventory management problem\nFigure 4: The 90th percentile, over 90 runs, of the diﬀerence | ˆJ(π, M) −J(π, M)|, with\nvarying N and εair. The diﬀerence should be lower for a larger N and smaller\nεair.\n8. Real World Experiments\nTo demonstrate the practicality of the proposed algorithm, we evaluate the proposed al-\ngorithm for two real world experiments: (1) Bitcoin: an optimal order execution for the\nbitcoin market, and (2) Prius: a hybrid car control problem. For the Bitcoin experiment,\n88\nExploiting AIR and Exogenous State Variables for Offline RL\n100\n500\n1000\n#Episodes\n20\n0\n20\nprice improvement (USD)\n(a) Bitcoin\n10\n50\n100\n#Episodes\n4\n2\n0\nfuel cost (10,000)\nCQL\nFQI\nFQI-AIR\nIQL\n(b) Prius\nFigure 5: Performance on real world datasets. For (a), the numbers represent the total\nselling price minus the average price. For (b), the numbers represent the fuel cost\nwith a penalty for not maintaining a desired batter level. Results are averaged\nover 30 runs, shown with one standard error.\nwe use historical prices of bitcoin.3 The problem is to sell one bitcoin within 60 steps where\neach step corresponds to 10 minutes in real world. On each step, the agent chooses to sell\nsome numbers of bitcoins in {0, 0.1, 0.2, 0.3, 0.4, 0.5}. Each episode corresponds to 10 hours,\nwith a start state chosen from a random time step in the data (consisting of 300 days).\nThe exogenous state contains the most recent 60 closing prices, and the endogenous state\ncontains the number of shares left to be sold. We collect an oﬄine dataset by running a\ntrained policy by DQN for N episodes, and report performance of the output policy for the\ntesting period (about 41 days).\nFor the Prius experiment, we use the hybrid car environment from Lian et al. (2020).4\nThe agent can switch between using fuel or battery, with the goal to minimize fuel consump-\ntion while maintaining a desired battery level. The exogenous state is the driving patterns\nand the endogenous state contains the state of charge and the previous action. We collect\nthe oﬄine dataset by running a learned policy with 10 diﬀerent driving patterns, and test\non 12 driving patterns. To better mimic the real-world, where we would not have a random\npolicy or constant policy, we use the learned policy from DQN as the data collection policy.\nFurther, now that the state space is larger, we run FQI-AIR where we randomly sample\nendogenous states and actions, rather than sweeping through all endogenous states and\nactions.\nFQI-AIR performs signiﬁcantly better than CQL, IQL and FQI, as shown in Figure 5.\nMBS-QI does not scale to high-dimensional continuous state spaces, and so is excluded.\nThese results highlight that FQI-AIR can scale to high-dimensional continuous state space\nand large endogenous state space.\n3. The\nbitcoin\ndata\nis\ndownloaded\nfrom\nthe\nkaggle\ncompetition\nhttps://www.kaggle.com/c/\ng-research-crypto-forecasting/data.\n4. We used their code at https://github.com/lryz0612/DRL-Energy-Management/blob/master/Prius_\nmodel_new.py.\n89\nLiu, Wright, & White\n9. Learning an Endogenous Model in AIR-MDPs\nIn the previous experiments, we assume we are given the endogenous models.\nIn this\nsection, we investigate the impact of using an approximate endogenous model learned from\noﬄine data. We perform this experiment in the hybrid car environment, which reﬂects a\nsetting where the endogenous dynamics might in fact not be known and would be useful\nto estimate from data. We use a neural network to approximate the endogenous state and\nreward model, and run FQI-AIR with the learned endogenous model.\nLet us ﬁrst reason about when it might be feasible to learn a reasonably accurate en-\ndogenous model. In the worse case, learning an accurate endogenous and reward model\nwould require data coverage for the entire state space. However, in many practical scenar-\nios, the endogenous model can be easy to learn and does not require full data coverage.\nFor example, in the optimal order execution problem, the endogenous dynamics does not\ndepend on the exogenous variables, as a result, we only need coverage for the endogenous\nstate. In the Prius environment, collecting data from just one driving cycle is suﬃcient to\nlearn a good endogenous model, as we will demonstrate in these experiments.\nWe ﬁrst collect a dataset from the hybrid car environment by running a random policy\nin one of the driving cycles and a deterministic policy for the other driving cycles. This\ndata generation approach mimics a scenario we might see in practice. In the factory, we\nmight have a test system for which it is acceptable to try many diﬀerent actions (using gas\nor the battery), and so get a more varied dataset for learning the endogenous model. We\nwould only get this data from one limited course (one driving cycle). The rest of the data\nwould be collected in the wild, where the deployed solution should not be exploring many\nactions and should largely be deterministic.\nWe also test two model-based baselines (MB): (1) The ﬁrst baseline has full knowledge\nof the reward and endogenous models, and learns the exogenous model from oﬄine data\nwithout exploiting the AIR property. The algorithm is similar to Algorithm 1 but sexo\nh+1\nis generated from the learned exogenous model. (2) The second baseline does not have\nknowledge of the reward, endogenous models and exogenous models. It learns a full model\nto from a state-action pair to the next state. For these model-based baselines, the model\nis parameterized by a two-layer neural network and learned by minimizing the ℓ2 distance\nbetween predicted states and next states recorded in the data. The transitions in these\nenvironments are deterministic, so it is appropriate to learn an expectation model.\nIn Figure 6 (a), we perform an ablation study to compare FQI-AIR and MB with the\ntrue endogenous model or a learned endogenous model. The result shows that (1) MB with\nthe true endogenous model performs slightly worse than FQI-AIR with a small data size.\n(2) FQI-AIR with a learned endogenous model perform worse than FQI-AIR, however, it\noutperforms IQL and MB without the true endogenous model.\n(3) MB with a learned\nendogenous model performs worse than FQI-AIR with a learned endogenous model. This\nsuggests that it is useful to separate the exogenous state and endogenous state especially\nwhen we need to learn an endogenous model.\nNext, we test FQI-AIR when learning the endogenous model only from a more limited\ndataset: a dataset based solely on one cycle. We collect a dataset from the hybrid car\nenvironment by running a random policy in one of the driving cycles for 500 episodes. This\nreﬂects a practical scenario that we can just run our vehicle in a closed area and still are\n90\nExploiting AIR and Exogenous State Variables for Offline RL\n10 100\n500\n#Episodes\n4\n2\n0\nfuel cost (10,000)\nFQI-AIR\nMB\nEndogenous Model\nTrue\nLearned\n(a) Comparison to model-based baselines\n10100\n500\n#Episodes\n4\n2\n0\nfuel cost (10,000)\nFQI-AIR\nEndogenous Model\nTrue\nLearned from one cycle\n(b) Learning the endogenous model with data\nfrom one cycle\nFigure 6: Experiment on the Prius dataset. We include IQL with 500 episodes as a baseline\n(purple line). Results are averaged over 30 runs, shown with one standard error.\nable to obtain a good endogenous model for running FQI-AIR. Figure 6 (b) shows that\nFQI-AIR with the learned endogenous model performs well and is close to FQI-AIR with\ntrue endogenous model.\n10. Conclusion\nIn this paper, we aim to understand whether and when oﬄine RL is feasible for real world\nproblems.\nIt is known that, without extra assumptions on MDPs, oﬄine RL requires\nexponential numbers of samples to obtain a nearly optimal policy with high probability\neven if we have a good data distribution. As a result, we must make assumptions on MDPs\nto make learning feasible (learning with polynomial sample complexity). However, common\nassumptions can be impractical as discussed in Section 3. Therefore, our goal in this paper\nis to study an MDP property that (1) is realistic for several important real world problems\nand (2) makes oﬄine RL feasible.\nWe introduced an MDP property, which we call Action Impact Regularity (AIR). We\ndeveloped an algorithm for MDPs satisfying AIR, that (1) has strong theoretical guarantees\non the supoptimality, without making assumptions about concentration coeﬃcients or data\ncoverage, (2) provides a simple way to select hyperparameters oﬄine, without introducing\nextra hyperparameters and (3) is simple to implement and computationally eﬃcient. We\nshowed empirically that the proposed algorithm signiﬁcantly outperforms existing oﬄine\nRL algorithms, across two simulated environments and two real world environments.\nAppendix A. Experiment Details\nIn this section, we provide more experimental details.\nAlgorithm Details\nThe original MBS-QI algorithm does not work with negative rewards.\nThe reward in the inventory problem is in the range [−100, 0], so we modify the pessimistic\nvalue for MBS-QI for the problem:\n˜qh(s, a) :=\n(\nqh(s, a),\nif ˆµ(s, a) ≥b\n−10000,\nif ˆµ(s, a) < b\n91\nLiu, Wright, & White\ninstead of ˜qh(s, a) := I{ˆµ(s, a) ≥b}qh(s, a) where ˆµ is the estimated data distribution.\nData Collection Policies\nTo collect data for the learned policy, we ﬁrst train a DQN\nalgorithm for 1000 episodes with online interaction with the underlying environment. The\nDQN parameters (that is, learning rate and optimizer) are chosen based on estimated\nJ(π, M). After training, we collect data by running the trained policy.\nAppendix B. Theoretical Analysis\nWe ﬁrst provide deﬁnitions and lemmas that would be useful for proving our main theorems.\nGiven q ∈RS×A, βh be a probability measure on S × A and p > 0, deﬁne ∥q∥p\np,βh =\nP\ns∈S,a∈A βh(s, a)|q(s, a)|p. Given q ∈RS×A, we deﬁne the Bellman evaluation operator for\na given policy π:\n(Tπq)(s, a) := r(s, a) +\nX\ns′∈S\nP(s, a, s′)\nX\na′∈A\nπ(a′|s′)q(s′, a′).\nLemma 1. Given a MDP M, suppose the sequence of value function (q0, . . . , qH−1) satisﬁes\nthat ∥T qh+1 −qh∥2,dπ\nh ≤ε for all policy π with qH = 0. Let π be the greedy policy with\nrespect to (q0, . . . , qH−1), then we have that\nJ(π, M) ≥J(π∗\nM, M) −(H + 1)Hε.\nProof. By the performance diﬀerence lemma (Lemma 5.2.1 of Kakade (2003) or Chen and\nJiang (2019)), we get\nJ(π∗, MD) −J(π, MD)\n= Eπ∗\n\"H−1\nX\nh=0\nq∗(Sh, Ah) −q∗(Sh, πh(Sh))\n#\n≤Eπ∗\n\"H−1\nX\nh=0\nq∗(Sh, Ah) −qh(Sh, Ah) + qh(Sh, πh(Sh)) −q∗(Sh, πh(Sh))\n#\n≤Eπ∗\n\"H−1\nX\nh=0\n|q∗(Sh, Ah) −qh(Sh, Ah)| + |qh(Sh, πh(Sh)) −q∗(Sh, πh(Sh))|\n#\n≤\nH−1\nX\nh=0\n∥q∗−qh∥1,dπ∗\nh π∗+ ∥q∗−qh∥1,dπ∗\nh πh\n≤\nH−1\nX\nh=0\n∥q∗−qh∥2,dπ∗\nh π∗+ ∥q∗−qh∥2,dπ∗\nh πh\nwhere dπ\nh is the state-action distribution at horizon h induced by policy π. The ﬁrst inequal-\nity follows because πh is greedy with respect to qh. We consider a state-action distribution\nβ0 that is induced by some policy, then\n∥q∗−q0∥2,β0 ≤∥T q∗−T q1 + T q1 −q0∥2,β0\n≤∥T q∗−T q1∥2,β0 + ∥T q1 −q0∥2,β0\n≤∥q∗−q1∥2,β1 + ε\n92\nExploiting AIR and Exogenous State Variables for Offline RL\nwhere β1(s′, a′) = P\ns,a β0(s, a)P(s, a, s′)I{a′ = arg maxa′′∈A(q∗(s′, a′′) −q1(s′, a′′))2} is also\ninduced by some policy. The ﬁrst inequality follows by the fact that q∗is the ﬁxed point\nof the operator T . We can recursively apply the same process for ∥q∗−qh∥2,βh, h > 0, and\nwe can get\n∥q∗−qh∥2,βh ≤(H −h)ε.\nPlug in the inequality to the performance diﬀerence lemma, we get\nJ(π∗, MD) −J(π, MD) ≤\nH−1\nX\nh=0\n2(H −h)ε = (H + 1)Hε.\nThe simulation lemma was ﬁrst introduced for discounted setting in (Kearns & Singh,\n2002), and here we prove a modiﬁed version of the simulation lemma for the ﬁnite horizon\nMDPs.\nLemma 2 (Finite Horizon Simulation Lemma). Given a policy π, let M = (S, A, P, r, H, µ)\nand ˆ\nM = (S, A, ˆP, r, H, µ) be two ﬁnite horizon MDPs and π be a policy satisfying that for\nall s ∈S and a ∈A, ∥P(s, a) −ˆP(s, a)∥1 ≤ε. Then, |vπ\nM(s) −vπ\nˆ\nM(s)| ≤ε H2rmax\n2\nfor s ∈S0.\nProof. For all s ∈S0, a ∈A,\n|vπ\nM(s) −vπ\nˆ\nM(s)|\n= |rπ(s) +\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nP(s, a, s′)vπ\nM(s′) −rπ(s) −\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nˆP(s, a, s′)vπ\nˆ\nM(s′)|\n= |\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nP(s, a, s′)vπ\nM(s′) −\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nˆP(s, a, s′)vπ\nˆ\nM(s′)|\n= |\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nP(s, a, s′)vπ\nM(s′) −\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nˆP(s, a, s′)vπ\nM(s′)\n+\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nˆP(s, a, s′)vπ\nM(s′) −\nX\na∈A\nπ(a|s)\nX\ns′∈S1\nˆP(s, a, s′)vπ\nˆ\nM(s′)|\n≤\nX\na∈A\nπ(a|s)|\nX\ns′∈S1\n(P(s, a, s′) −ˆP(s, a, s′))vπ\nM(s′)| + max\ns′∈S1 |vπ\nM(s′) −vπ\nˆ\nM(s′)|\n≤ε(H −1)rmax + max\ns′∈S1 |vπ\nM(s′) −vπ\nˆ\nM(s′)|\n≤ε(H −1)rmax + εP (H −2)rmax + · · · + εP 1rmax\n≤εH2rmax\n2\n= εHvmax\n2\nThe ﬁrst equality follows from the Bellman equation. The second and third inequalities\nfollow from that vπ\nˆ\nM(s) is at most (H −h)rmax for s ∈Sh.\nLemma\n3. Let M\n=\n(S, A, P exo, P end, r, H) be an εair-AIR MDP and Mb\n=\n(S, A, ˜P exo, ˜P end, r, H) with DTV (P end(s, a), ˜P end(s, a)) ≤εp, then for any policy π,\n|J(π, M) −J(π, Mb)| ≤vmaxH(εair + εp).\n93\nLiu, Wright, & White\nProof. Since M is εair-AIR, we have\nDTV\n\u0000P exo(sexo, a), P exo(sexo, a′)\n\u0001\n= 1\n2\n\r\rP exo(sexo, a) −P exo(sexo, a′)\n\r\r\n1 ≤εair\nLet e(s, a, s′end) = P end(s, a, s′end)−˜P end(s, a, s′end), we know DTV\n\u0010\nP end(s, a), ˜P end(s, a)\n\u0011\n=\n1\n2 ∥e(s, a, ·)∥1 = 1\n2\nP\ns′end |e(s, a, s′end)| ≤εp. For any s = (sexo, send) ∈Sexo × Send and\na ∈A, we have\n\r\r\rP(s, a) −˜P(s, a)\n\r\r\r\n1\n=\nX\ns′\n|P([sexo, send], a, s′) −˜P([sexo, send], a, s′)|\n=\nX\ns′\n|P exo(sexo, a, s′exo)P end(s, a, s′end) −˜P exo(sexo, a, s′exo) ˜P end(s, a, s′end)|\n=\nX\ns′\n|P exo(sexo, a, s′exo)( ˜P end(s, a, s′end) + e(s, a, s′end)) −˜P exo(sexo, a, s′exo) ˜P end(s, a, s′end)|\n=\nX\ns′\n| ˜P end(s, a, s′end)[P exo(sexo, a, s′exo) −˜P exo(sexo, a, s′exo)] + P exo(sexo, a, s′exo)e(s, a, s′end)|\n≤\nX\ns′\n˜P end(s, a, s′end)|P exo(sexo, a, s′exo) −\nX\na′\nπ′(a′|sexo)P exo(sexo, a′, s′exo)|+\nX\ns′\nP exo(sexo, a, s′exo)|e(s, a, s′end)|\n≤\nX\na′\nπ′(a|sexo)\nX\ns′exo\n|P exo(sexo, a, s′exo) −P exo(sexo, a′, s′exo)| +\nX\ns′end\n|e(s, a, s′end)|\n≤2εair + 2εp.\nThe ﬁrst inequality follows by writing ˜P exo(sexo, a, s′exo) = P\na′∈A π′(a′|sexo)P exo(sexo, a′, s′exo)\nwhere π′(a′|sexo\nh ) = P\nπb\nM (Sexo\nh\n=sexo\nh\n,Ah=a′)\nP\nπb\nM (Sexo\nh\n=sexo\nh\n)\nwhen the denominator is nonzero, and otherwise let\nπ′(·|sexo\nh ) be an arbitrary distribution. Applying Lemma 2, we get\n|J(π, M) −J(π, Mb)| =\nX\ns0\nµ(s0)|vπ\nM(s0) −vπ\nMb(s0)| ≤vmax(εair + εp)H.\nTheorem 1. Under Assumption 1 and 2, let π∗\nM be an optimal policy in M and π the\noutput policy of FQI-AIR, then with probability at least 1 −ζ\nJ(π, M) ≥J(π∗\nM, M) −2vmaxH(εair + εp)−\n(H + 1)H\nq\n|Send||A|\n r\n72vmax2 ln (H|F||Send||A|/ζ)\nN\n+ 2εapx\n!\n.\n94\nExploiting AIR and Exogenous State Variables for Offline RL\nProof. First ﬁx a horizon h ∈[H] and f′ = qh+1 ∈F. Deﬁne\nR(f) = ∥T f′ −f∥2\n2,˜νh\n=\nX\nsexo\nh\n∈Sexo\nh\nX\nsend\nh\n∈Send\nh\nX\na∈A\n˜νh(sexo\nh )\n|Send||A|(f(sexo\nh , send\nh\n, a) −r −E[max\na′\nf′(Sexo\nh+1, Send\nh+1, a′)])2\nand\nRn(f) = 1\nN\nN\nX\ni=1\nX\nsend\nh\n∈Send\nh\nX\na∈A\n1\n|Send||A|(f(s(i),exo\nh\n, send\nh\n, a) −r −max\na′\nf′(s(i),exo\nh+1\n, send\nh+1, a′))2\nwhere send\nh+1 ∼ˆP end(s(i),exo\nh\n, send\nh\n, a). Let\nˆf = arg min\nf∈F Rn(f), ˜f = arg min\nf∈F R(f),\nour goal is to bound R( ˆf) with high probability. This is similar to bounding the general-\nization error in the statistical learning literature. We follow the proof technique of Lemma\nA.11 in Agarwal, Jiang, Kakade, and Sun (2019) to bound the excess risk.\nFix u = (send\nh\n, a) ∈U := Send × A, let xu\ni = (sexo\nh , send\nh\n, a) and yu\ni = r(sexo\nh , send\nh\n, a) +\nmaxa′ f′(sexo\nh+1, send\nh+1, a′) with (xu\ni , yu\ni ) ∼ν, f∗(xu\ni ) = E[yu\ni |xu\ni ]. Note that our goal is to bound\nR( ˆf) =\n1\n|U|\nP\nu∈U E[( ˆf(xu\ni ) −f∗(xu\ni ))2].\nFirst note that\nE[(f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2] = E[(f(xu\ni ) −f∗(xu\ni ))2]\nand\nVu,f := V[(f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2]\n≤E[((f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2)2]\n≤4vmax2E[(f(xu\ni ) −f∗(xu\ni ))2].\nWe can bound the deviation from the mean for all f ∈F with one-sided Bernstein’s\ninequality: the following holds with probability 1 −ζ,\nE[(f(xu\ni ) −f∗(xu\ni ))2] −1\nN\nN\nX\ni=1\n[(f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2]\n≤\nr\n2Vu,f ln (|F|/ζ)\nN\n+ 4vmax2 ln (|F|/ζ)\n3N\n≤\nr\n8vmax2E[(f(xu\ni ) −f∗(xu\ni ))2] ln (|F|/ζ)\nN\n+ 4vmax2 ln (|F|/ζ)\n3N\n.\nWe need this holds for all u ∈U. By the union bound, we have for all u ∈U\nE[(f(xu\ni ) −f∗(xu\ni ))2] −1\nN\nN\nX\ni=1\n[(f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2]\n≤\nr\n8vmax2E[(f(xu\ni ) −f∗(xu\ni ))2] ln (|F||U|/ζ)\nN\n+ 4vmax2 ln (|F||U|/ζ)\n3N\n.\n95\nLiu, Wright, & White\nSince ˆf is the empirical minimizer, we have\n1\n|U|\nX\nu∈U\n1\nN\nN\nX\ni=1\n[( ˆf(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2]\n≤1\n|U|\nX\nu∈U\n1\nN\nN\nX\ni=1\n[( ˜f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2].\nSince ˜f ∈F, we have\n1\nN\nN\nX\ni=1\n[( ˜f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2] −E[( ˜f(xu\ni ) −f∗(xu\ni ))2]\n≤\ns\n8vmax2E[( ˜f(xu\ni ) −f∗(xu\ni ))2] ln (|F||U|/ζ)\nN\n+ 4vmax2 ln (|F||U|/ζ)\n3N\n.\nSince\n1\n|U|\nP\nu∈U E[( ˜f(xu\ni ) −f∗(xu\ni ))2] ≤εapx, we have\n1\n|U|\nX\nu∈U\n1\nN\nN\nX\ni=1\n[( ˜f(xu\ni ) −yu\ni )2 −(f∗(xu\ni ) −yu\ni )2]\n≤εapx +\nr\n8vmax2εapx ln (|F||U|/ζ)\nN\n+ 4vmax2 ln (|F||U|/ζ)\n3N\n≤3\n2εapx + 16vmax2 ln (|F||U|/ζ)\n3N\n.\nThe second inequality follows by the fact that\n√\nab ≤a+b\n2\nfor a, b ≥0. Then,\n1\n|U|\nX\nu∈U\nE[( ˆf(xu\ni ) −f∗(xu\ni ))2]\n≤\ns\n8vmax2 1\n|U|\nP\nu∈U E[( ˆf(xi) −f∗(xi))2] ln (|F||U|/ζ)\nN\n+ 20vmax2 ln (|F||U|/ζ)\n3N\n+ 3\n2εapx.\nSolving for the quadratic formula, we get\n1\n|U|\nX\nu∈U\nE[( ˆf(xu\ni ) −f∗(xu\ni ))2] ≤36vmax2 ln (|F||U|/ζ)\nN\n+ 2εapx.\nNow deﬁne ˜dπ\nh(sexo) = Pπ\nMb(Sexo\nh\n= sexo\nh ) and dπ\nh(sexo, send, a) = Pπ\nMb(Sexo\nh\n= sexo\nh , Send\nh\n=\nsexo\nh , Ah = a). By the construction of Mb, we have\n∀π, sexo\nh\n∈Sexo\nh ,\n˜dπ\nh(sexo)\n˜νh(sexo) ≤1, and\n∀π, sexo\nh\n∈Sexo\nh , send\nh\n∈Send\nh\n, a ∈A,\ndπ\nh(sexo, send, a)\n˜νh(sexo)/|Send||A| ≤|Send||A|.\n96\nExploiting AIR and Exogenous State Variables for Offline RL\nTherefore, for any policy π, we have with probability 1 −ζ\n∥T qh+1 −qh∥2,dπ\nh ≤\nq\n|Send||A|∥T qh+1 −qh∥2,˜νh\n≤\nq\n|Send||A|(\nr\n36vmax2 ln (|F||Send||A|/ζ)\nN\n+ 2εapx).\nNote that this holds for a ﬁxed h ∈[H] and f′ ∈F. Use the union bound, we have that,\nfor any policy π, h ∈[H] and qh+1 ∈F, with probability 1 −ζ\n∥T qh+1 −qh∥2,dπ\nh ≤\nq\n|Send||A|(\nr\n72vmax2 ln (H|F||Send||A|/ζ)\nN\n+ 2εapx).\nBy Lemma 1, the output policy π satisﬁes\nJ(π∗\nMb, Mb) −J(π, Mb)\n≤(H + 1)H\nq\n|Send||A|(\nr\n72vmax2 ln (H|F||Send||A|/ζ)\nN\n+ 2εapx) = ε1.\nBy Lemma 3, the output policy π satisﬁes\nJ(π∗\nM, M) −J(π, M)\n= J(π∗\nM, M) −J(π∗\nM, Mb) + J(π∗\nM, Mb) −J(π, M)\n≤J(π∗\nM, M) −J(π∗\nM, Mb) + J(π∗\nMb, Mb) −J(π, M)\n≤J(π∗\nM, M) −J(π∗\nM, Mb) + J(π, Mb) −J(π, M) + ε1\n≤|J(π∗\nM, M) −J(π∗\nM, Mb)| + |J(π, Mb) −J(π, M)| + ε1\n≤2vmaxH(εair + εP ) + (H + 1)H\nq\n|Send||A|(\nr\n72vmax2 ln (H|F||Send||A|/ζ)\nN\n+ 2εapx).\nNote that\np\n|Send||A| comes from the fact that we run FQI for all endogenous state and\naction uniformly. This term can be viewed as the concentration coeﬃcient in the standard\nFQI analysis and is unavoidable. However, if we can adapt the weight on the endogenous\nstate and action, we might be able to reduce the dependence.\nTheorem 2. Under Assumption 1, given a deterministic policy π, we have that with\nprobability at least 1 −ζ\n\f\f\f ˆJ(π, M)−J(π, M)\n\f\f\f≤vmax\n\u0012\nHεair+Hεp+\nq\nln (2/ζ)\n2N\n\u0013\n.\nProof. We ﬁrst show that R(τ (i)\nD ) := PH−1\nt=0 r(s(i)\nt , a(i)\nt ) for i ∈[N] are i.i.d. samples with\nmean J(π, Mb).\nLet Pπ\nD be the probability measure on trajectories τD.\nNote that the\nrandomness of τD comes from the generation of exogenous variables in by the interaction\nbetween πb and M, and the generation of actions and endogenous variables by π and ˆP end.\nLet Pπ\nMb be the probability measure on the trajectories sampled by running π on Mb. It\nis suﬃcient to show that Pπ\nD = Pπ\nMb then E[R(τD)] =\nR\nR(τ)dPπ\nD(τ) =\nR\nR(τ)dPπ\nMb(τ) =\nJ(π, Mb).\n97\nLiu, Wright, & White\nFor all trajectories τ = (s0, a0, . . . , sH−1, aH−1) ∈(S × A)H, we have\nPπ\nD(τ) = Pπb\nM(s0, sexo\n1\n, . . . , sexo\nH−1)Pπ\nD(a0, send\n1\n, . . . , send\nH−1, aH−1 | s0, sexo\n1\n, . . . , sexo\nH−1)\n= µ(s0)π(a0|s0)Pπb\nM(sexo\n1\n|s0) ˆP end(s0, a0, send\n1\n)π(a1|s1) . . .\n= µ(s0)π(a0|s0) ˜P(s0, a0, s1)π(a1|s1) · · · = Pπ\nMb(τ).\nBy Hoeﬀding’s inequality, with probability at least 1 −ζ,\n\f\f\f ˆJ(π, M) −J(π, Mb)\n\f\f\f =\n\f\f\f\f\f\n1\nN\nN\nX\ni=1\nR(τ (i)\nD ) −E[R(τD)]\n\f\f\f\f\f ≤vmax\nr 1\n2N ln 2\nζ .\nFinally by Lemma 3, the followings hold with probability at least 1 −ζ:\n| ˆJ(π, M) −J(π, M)| ≤| ˆJ(π, M) −J(π, Mb)| + |J(π, Mb) −J(π, M)|\n≤vmaxH(ϵAIR + ϵP ) + vmax\nr\nln (2/ζ)\n2N\n.\nAppendix C. Comparing FQI-AIR to Online RL with Trajectory\nSimulation\n100\n1000\n#Iteration\n3\n4\n5\n6\nJ( , M)\nFQI-AIR\nAC\nAPI\n(a) Optimal order execution\n100\n1000\n#Iteration\n400\n200\n0\nJ( , M)\nFQI-AIR\nAC\nAPI\n(b) Inventory management\nFigure 7: Comparison to simulation-based algorithms. Results are averaged over 30 runs\nwith the shaded region representing plus and minus one standard error.\nWe compare to two online RL algorithms, approximate policy iteration (API) and actor\ncritic (AC) with ε-greedy exploration, with the trajectory simulator discussed in Section\n5.1. We show the learning curves with random data collection policy and N = 100. Each\niteration contains a sweep over the entire dataset and we evaluate the greedy policy with\nrespect to the Q function after each iteration. The results show that FQI-AIR converges\nto a nearly optimal solution within a few iterations while the online RL algorithms require\nmuch more iterations to ﬁnd a good policy. AC in the optimal order execution problem\ndoes not converge to a stable performance. The ﬁnal performance of API and AC are also\nworse than the performance of FQI-AIR. These results show that online RL algorithms\ncould be used for AIR MDPs. However, they are less direct and eﬃcient, and they could\nﬁnd a slightly diﬀerent solution with ﬁnite data.\n98\nExploiting AIR and Exogenous State Variables for Offline RL\nReferences\nAbdullah, H. M., Gastli, A., & Ben-Brahim, L. (2021). Reinforcement learning based EV\ncharging management systems–a review. IEEE Access, 9, 41506–41531.\nAbernethy, J., & Kale, S. (2013). Adaptive market making via online learning. Advances\nin Neural Information Processing Systems.\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2019). Reinforcement learning: Theory\nand algorithms.\nAgarwal, A., Kakade, S., & Yang, L. F. (2020). Model-based reinforcement learning with a\ngenerative model is minimax optimal. In Conference on Learning Theory.\nAntos, A., Szepesv´ari, C., & Munos, R. (2006). Learning near-optimal policies with bellman-\nresidual minimization based ﬁtted policy iteration and a single sample path. In In-\nternational Conference on Computational Learning Theory.\nBabaioﬀ, M., Immorlica, N., Kempe, D., & Kleinberg, R. (2007). A knapsack secretary\nproblem with applications. In Approximation, randomization, and combinatorial op-\ntimization. Algorithms and techniques. Springer.\nBellemare, M. G., Veness, J., & Bowling, M. (2012). Investigating contingency awareness\nusing Atari 2600 games. In Proceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence.\nBertsimas, D., & Lo, A. W. (1998). Optimal control of execution costs. Journal of ﬁnancial\nmarkets, 1(1), 1–50.\nBrafman, R. I., & Tennenholtz, M. (2002). R-Max-a general polynomial time algorithm for\nnear-optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct),\n213–231.\nBurhani, H., Ding, G. W., Hernandez-Leal, P., Prince, S., Shi, D., & Szeto, S. (2020). Aiden\n– reinforcement learning for order execution.\nhttps://www.borealisai.com/en/\nblog/aiden-reinforcement-learning-for-order-execution/. [Online; accessed\n1-June-2022].\nChen, J., & Jiang, N. (2019). Information-theoretic considerations in batch reinforcement\nlearning. In International Conference on Machine Learning.\nChitnis, R., & Lozano-P´erez, T. (2020). Learning compact models for planning with exoge-\nnous processes. In Conference on Robot Learning.\nDietterich, T., Trimponias, G., & Chen, Z. (2018). Discovering and removing exogenous\nstate variables and rewards for reinforcement learning. In International Conference\non Machine Learning.\nDuan, Y., Jin, C., & Li, Z. (2021).\nRisk bounds and Rademacher complexity in batch\nreinforcement learning. In International Conference on Machine Learning.\nEfroni, Y., Misra, D., Krishnamurthy, A., Agarwal, A., & Langford, J. (2022). Provable RL\nwith exogenous distractors via multistep inverse dynamics. In International Confer-\nence on Learning Representations.\n99\nLiu, Wright, & White\nErnst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.\nJournal of Machine Learning Research, 6.\nFarahmand, A.-m., Szepesv´ari, C., & Munos, R. (2010). Error propagation for approximate\npolicy and value iteration. Advances in Neural Information Processing Systems.\nFreeman, P. (1983). The secretary problem and its extensions: A review. International\nStatistical Review/Revue Internationale de Statistique.\nFu, J., Kumar, A., Nachum, O., Tucker, G., & Levine, S. (2020). D4RL: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219.\nFujimoto, S., Meger, D., & Precup, D. (2019). Oﬀ-policy deep reinforcement learning with-\nout exploration. In International Conference on Machine Learning.\nGoldstein, D. G., McAfee, R. P., Suri, S., & Wright, J. R. (2020). Learning when to stop\nsearching. Management Science, 66(3), 1375–1394.\nJiang, N. (2018). PAC reinforcement learning with an imperfect model. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence.\nKakade, S. M. (2003). On the sample complexity of reinforcement learning. Ph.D. thesis,\nUniversity College London.\nKearns, M., & Singh, S. (2002). Near-optimal reinforcement learning in polynomial time.\nMachine learning, 49(2), 209–232.\nKidambi, R., Rajeswaran, A., Netrapalli, P., & Joachims, T. (2020). MOReL: Model-based\noﬄine reinforcement learning. In Advances in Neural Information Processing Systems.\nKingma, D. P., & Ba, J. (2014).\nAdam: A method for stochastic optimization.\narXiv\npreprint arXiv:1412.6980.\nKostrikov, I., Nair, A., & Levine, S. (2022). Oﬄine reinforcement learning with implicit\nq-learning. In International Conference on Learning Representations.\nKumar, A., Fu, J., Soh, M., Tucker, G., & Levine, S. (2019). Stabilizing oﬀ-policy Q-learning\nvia bootstrapping error reduction. Advances in Neural Information Processing Sys-\ntems.\nKumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative Q-learning for oﬄine\nreinforcement learning. Advances in Neural Information Processing Systems.\nKunnumkal, S., & Topaloglu, H. (2008). Using stochastic approximation methods to com-\npute optimal base-stock levels in inventory control problems. Operations Research,\n56(3), 646–664.\nLaroche, R., Trichelair, P., & Des Combes, R. T. (2019). Safe policy improvement with\nbaseline bootstrapping. In International Conference on Machine Learning.\nLattimore, T., & Szepesv´ari, C. (2020). Bandit algorithms. Cambridge University Press.\nLevine, S., Kumar, A., Tucker, G., & Fu, J. (2020). Oﬄine reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643.\nLian, R., Peng, J., Wu, Y., Tan, H., & Zhang, H. (2020). Rule-interposing deep reinforcement\nlearning based energy management strategy for power-split hybrid electric vehicle.\nEnergy, 197, 117297.\n100\nExploiting AIR and Exogenous State Variables for Offline RL\nLiu, Y., Swaminathan, A., Agarwal, A., & Brunskill, E. (2020).\nProvably good batch\nreinforcement learning without great exploration. Advances in Neural Information\nProcessing Systems.\nMcGregor, S., Houtman, R., Montgomery, C., Metoyer, R., & Dietterich, T. G. (2017). Fac-\ntoring exogenous state for model-free Monte Carlo. arXiv preprint arXiv:1703.09390.\nMunos, R. (2003). Error bounds for approximate policy iteration. In International Confer-\nence on Machine Learning.\nMunos, R. (2005). Error bounds for approximate value iteration. In National Conference\non Artiﬁcial Intelligence.\nMunos, R. (2007). Performance bounds in Lp-norm for approximate value iteration. SIAM\njournal on control and optimization, 46(2), 541–561.\nNevmyvaka, Y., Feng, Y., & Kearns, M. (2006). Reinforcement learning for optimized trade\nexecution. In International Conference on Machine Learning.\nShahamiri, M. (2008). Reinforcement learning in environments with independent delayed-\nsense dynamics. Master’s thesis, University of Alberta.\nThomas, P., Theocharous, G., & Ghavamzadeh, M. (2015). High conﬁdence policy improve-\nment. In International Conference on Machine Learning.\nVan Roy, B., Bertsekas, D. P., Lee, Y., & Tsitsiklis, J. N. (1997). A neuro-dynamic program-\nming approach to retailer inventory management. In IEEE Conference on Decision\nand Control.\nWang, R., Foster, D. P., & Kakade, S. M. (2021). What are the statistical limits of oﬄine\nRL with linear function approximation?. In International Conference on Learning\nRepresentations.\nWu, Y., Tucker, G., & Nachum, O. (2019). Behavior regularized oﬄine reinforcement learn-\ning. arXiv preprint arXiv:1911.11361.\nXiao, C., Lee, I., Dai, B., Schuurmans, D., & Szepesvari, C. (2022). The curse of passive data\ncollection in batch reinforcement learning. In International Conference on Artiﬁcial\nIntelligence and Statistics.\nYin, M., Bai, Y., & Wang, Y.-X. (2021). Near-optimal provable uniform convergence in\noﬄine policy evaluation for reinforcement learning. In International Conference on\nArtiﬁcial Intelligence and Statistics.\nZanette, A. (2021). Exponential lower bounds for batch reinforcement learning: Batch RL\ncan be exponentially harder than online RL. In International Conference on Machine\nLearning.\n101\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-11-15",
  "updated": "2023-05-03"
}