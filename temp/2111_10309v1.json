{
  "id": "http://arxiv.org/abs/2111.10309v1",
  "title": "Unsupervised Visual Time-Series Representation Learning and Clustering",
  "authors": [
    "Gaurangi Anand",
    "Richi Nayak"
  ],
  "abstract": "Time-series data is generated ubiquitously from Internet-of-Things (IoT)\ninfrastructure, connected and wearable devices, remote sensing, autonomous\ndriving research and, audio-video communications, in enormous volumes. This\npaper investigates the potential of unsupervised representation learning for\nthese time-series. In this paper, we use a novel data transformation along with\nnovel unsupervised learning regime to transfer the learning from other domains\nto time-series where the former have extensive models heavily trained on very\nlarge labelled datasets. We conduct extensive experiments to demonstrate the\npotential of the proposed approach through time-series clustering.",
  "text": "Unsupervised Visual Time-Series Representation\nLearning and Clustering\nGaurangi Anand1,2 and Richi Nayak1,3\n1 School of Computer Science, Queensland University of Technology\n2 gaurangianand@hdr.qut.ed.au\n3 r.nayak@qut.edu.au\nAbstract. Time-series data is generated ubiquitously from Internet-\nof-Things (IoT) infrastructure, connected and wearable devices, remote\nsensing, autonomous driving research and, audio-video communications,\nin enormous volumes. This paper investigates the potential of unsuper-\nvised representation learning for these time-series. In this paper, we use a\nnovel data transformation along with novel unsupervised learning regime\nto transfer the learning from other domains to time-series where the for-\nmer have extensive models heavily trained on very large labelled datasets.\nWe conduct extensive experiments to demonstrate the potential of the\nproposed approach through time-series clustering.\nKeywords: Unsupervised learning · Time-series clustering\n1\nIntroduction\nTime-series data is generated ubiquitously in enormous amounts from sensors\nand IoT. With the prior knowledge about labels, it can be observed that the\nsamples have minor to major shape variations across pre-deﬁned labels, as seen\nin Figure 1. These variations can be incorporated in feature representation and\nlearning. However, a large-scale time-series labelling is expensive and requires\ndomain expertise, paving way for unsupervised tasks for time-series [12].\nBeetleFly\nCBF\nFISH\nShapeletSim\nTrace\nFig. 1. Five sample datasets from UCR [4] with one sample per cluster.\nTime-series clustering is an unsupervised task with data representation and\na suitable similarity measure as its core components. Some methods propose\nsophisticated similarity measures applied to raw time-series data, while others\npropose eﬀective representations suitable to simpler similarity measures like Eu-\nclidean Distance (ED). Both the approaches deﬁne the means for unsupervised\narXiv:2111.10309v1  [cs.LG]  19 Nov 2021\n2\nGaurangi Anand and Richi Nayak\nestimation of the extent of resemblance between the samples utilizing charac-\nteristics such as their shape [14], shapelets [7], alignment [3] and structure [13].\nThis extent is guided by the information acquired through supervised or unsu-\npervised learning. In the absence of labels, transfer learning (TL) is one of the\nsolutions [2] where the model learned on source task, rich with labelled infor-\nmation, is ﬁne-tuned for a target task. Typically the source and target datasets\nare related with the latter being comparatively small. The recent success of deep\nlearning has brought advancements to TL methods in the ﬁelds of Natural Lan-\nguage Processing and Computer Vision (CV) with large datasets like Wikipedia,\nand ImageNet [10] for ﬁne-tuning various related data-scarce tasks. However, no\nsuch labelled time-series corpus of a similar scale exists.\nIn this paper, we propose a novel approach of leveraging the large-scale train-\ning from a popular CV-based dataset [5] to the time-series data in an unsuper-\nvised manner. The relatedness of ImageNet data and time-series has not been\nexplored earlier, providing an opportunity to use visual recognition for time-\nseries as commonly done for images. It facilitates representing time-series from\na visual perspective inspired by human visual cognition, involving 2-D convolu-\ntions, unlike the 1-D convolutional approaches popular in time-series domain [7].\nWe utilize the shifted variants of the original time-series to train a model to iso-\nlate the distinct local patterns of the time-series irrespective of their location\nwithin the overall layout, and evaluate the approach for time-series clustering.\nMore speciﬁcally, the contributions of this paper are as follows:\n1. We approach the problem of time-series analysis as the human visual cogni-\ntive process by transforming the 1-D time-series data into 2-D images.\n2. We leverage the training of the very large dataset available in the CV ﬁeld\nto the unsupervised representation learning for time-series through a pre-\ntrained 2-D deep Convolutional Neural Network (CNN) model.\n3. We propose a novel unsupervised learning regime using triplet loss for time-\nseries data using 2-D convolutions and shift-invariance.\n4. The resultant time-series representation has ﬁxed length regardless of the\nvariable length time-series that enables pairwise comparison of time-series\ndata in linear time using Euclidean distance.\n2\nRelated Literature\nWith the availability of unlabelled datasets, several unsupervised learning meth-\nods have emerged ranging from conventional algorithms [19,1] to stacked deep\narchitectures [6,12], for obtaining a representation based on the end task. These\nrepresentations can be data adaptive like Symbolic Aggregate Approximation\n(SAX) [13] or non-data adaptive like Discrete Fourier Transform (DFT), Discrete\nWavelet Transform (DWT) and Piecewise Aggregate Approximations (PAA). A\nmodel-based representation is generated by identifying the model parameters\nthrough training based on the relevant properties. Once trained, a model is used\nas a feature extractor [7] for the end task.\nUnsupervised Visual Time-Series Representation Learning and Clustering\n3\nSimilarity PreservIng RepresentAtion Learning (SPIRAL) [11] preserves the\nsimilarity of Dynamic Time Warping (DTW) distance in the approximated sim-\nilarity matrix, to be then used for conventional partitional clustering like k-\nmeans. k-shape [14] adapts the clustering algorithm to the distance measure\nfor cross-correlation while assigning clusters. Unsupervised Salient Subsequence\nLearning (USSL) [19] identiﬁes the salient subsequences from the time-series\nand performs clustering based on shapelet learning and spectral analysis. Deep\nEmbedding for Clustering (DEC) [18] and Improved Deep Embedding Cluster-\ning (IDEC) [8] are deep learning based non-time-series clustering. Unsupervised\ntriplet loss training has been proposed for time-series [7] where representations\nare learned and evaluated for time-series classiﬁcation using 1-D dilated causal\nconvolutions. Deep Temporal Clustering Representation (DTCR) [12] is a deep\nlearning-based end-to-end clustering for time-series clustering that jointly learns\ntime-series representations and assigns cluster labels using k-means loss.\nExisting deep learning-based time-series representation methods do not use\npre-trained networks due to 1) the latter not being tailored to time-series and\n2) the popularity of 1-D convolutions for sequential data. Bridging this gap by\nutilizing a 2-D pre-trained CNN not only helps to leverage large-scale training\nbut also provides a local pattern-based time-series representation. To the best\nof our knowledge, this is the ﬁrst time-series representation approach combining\nvisual perception with unsupervised triplet loss training using 2-D convolutions.\n3\nProposed Approach\nWe propose to ﬁrst transform time-series into images to leverage the large-scale\ntraining from a 2-D deep CNN pre-trained with ImageNet. This CNN is then\nmodiﬁed and re-trained for feature extraction in unsupervised setting utilizing\na novel triplet loss training.\n1-D to 2-D feature transformation (fT ) The 1-D time-series dataset is\ntransformed into 2-D image dataset, achieved by simply plotting a 1D time-\nseries as a time-series image, and used as 2D matrix to take advantage of 2-D\nconvolution operations [10,9] through pre-trained 2-D CNNs. This emulation of\nhuman visual cognition to inspect/cluster time-series data using this transfor-\nmation enables the vision-inspired systems to interpret the time-series visually.\nCNN Architecture We use ResNet50 [9] as our pre-trained CNN, trained on\nthe large-scale ImageNet dataset [5] for the task of object classiﬁcation. It con-\nsists of residual connections that add depth to the vanilla CNN architecture [10]\nuseful for highly complex CV tasks. ImageNet [5] comprises millions of images\nwith 1000 classes. CNN learned on this data provides visual local pattern based\nrepresentations extracted as 3-D feature tensors from each of its composite lay-\ners. These feature tensors, called feature maps, have the local spatial pattern\nmapping with respect to each of the input images. ImageNet allows the net-\nwork to learn lots of variations in local shape patterns. With that as a premise,\n4\nGaurangi Anand and Richi Nayak\nwe argue that when using visual representations of 1-D time-series most of their\nshape patterns would be easily represented through a pre-trained 2-D CNN. The\nsubsequent ﬁne-tuning helps to add the time-series bias to shape patterns. As a\nresult, relevant shape patterns are obtained for 2-D time-series matrices.\nThe modiﬁed ResNet is depicted in Figure 2. ResNet is retained up to its\nlast convolution layer by removing the fully connected layer trained for the ob-\nject classiﬁcation task. The local spatial patterns in the form of activations for\neach of the time-series images are retrieved within the feature maps. A 2-D\nconvolution layer is then appended to it, followed by the Global Max Pooling\n(GMP) layer [17]. This layer leverages the local patterns within diﬀerent feature\nmaps irrespective of their actual activation location in an image for introducing\nshift-invariance. It helps matching time-series images where the observed local\npatterns may not exactly align spatially. We then append a l2-normalization\nlayer to improve the network training by providing stability to it.\nc6\nGMP\nl2 - Norm\nTriplet loss\nc1\nc2\nc4\nc3\nc5\nFig. 2. Overview of the proposed learning framework with c1...6 convolution layers of\nthe CNN followed with GMP and l2-normalization layer.\nLet XT be the set of 1-D univariate time-series that is transformed into an\nequivalent set of 2-D time-series images, XI. XD refers to the representations\nobtained from this deep network. Using the l2-normalized representation for\neach individual time-series, we generate unsupervised triplets as described be-\nlow. Once trained, the l2-normalized output is used as the Learned Time-series\nRepresentation, called as Learned Deep Visual Representation (LDVR).\nTriplet Selection Triplet loss training helps to ensure that similar instances\nobtain similar representations, while the dissimilar ones are pushed apart with a\nmargin, α. Unlike the usual supervision-based triplet selection [16,7], we propose\na novel unsupervised triplet selection for 2-D time-series images. A positive pair\nconsists of an original instance with its shifted variant, and its pairing with any\nother sample forms the negative pair. This training helps identifying the patterns\nthat can be isolated from the layout and modify the representations such that\nthe anchor and positive are brought closer and separated from negative.\nWe address the challenge of unsupervised triplet selection by considering a\npool of time-series across a large number of datasets. From this pool, two time-\nseries samples, XTa and XTn as anchor and negative respectively, are selected\nUnsupervised Visual Time-Series Representation Learning and Clustering\n5\nrandomly from a dataset and a random shift to XTa is added by introducing\na circular shift of randomly chosen ϵ time-steps to obtain XTp, called positive.\nThe feature transformation is applied to the triplet set to obtain XIa, XTp, and\nXIn. Figure 2 depicts the process of triplet selection with XTa, XTp and XTn as\nthe triplets, where XTp is obtained by introducing a shift of ϵ to XTa.\nClustering The modiﬁed ResNet trained with the unsupervised Triplet loss\nproduces a ﬁxed length representation, XD, called LDVR for each time-series im-\nage. Assuming the compact feature representation to be separable in Euclidean\nspace, a distance based clustering algorithm likes k-means can be applied.\n4\nExperiments\nExtensive experiments are conducted to evaluate the accuracy of the LDVR\ngenerated by the proposed approach for the time-series clustering.\nWe use the publicly available time-series UCR repository [4] with 85 datasets.\nThe sequences in each dataset have equal length ranging from 24 to 2709, with\n2 to 60 classes, split into train and test. The already assigned class labels are\ntreated as cluster identiﬁers and only used in evaluation.\nExperimental setup We use the previous version of UCR of 47 datasets for\nunsupervised selection of triplets. To prevent memorization and overﬁtting of the\nnetwork training, we randomly select 100 samples from them4. For each iteration,\nwe perform 10 circular shifts each time with randomly chosen ϵ varying between\n0.6 and 1.0, representing the percentage of time-series length. A total of 3056\ndistinct instances were added to the pool for triplet selection. The inclusion\nof shifted samples forces the network to learn local patterns isolated from the\nlayout, and use them for matching independent of their actual location across\nsamples. All the time-series of variable length were transformed into images of\n640 × 480 resolution. This value was derived from UCR where 80% of datasets\nhave length below 640; to keep aspect ratio as standard 4:3, height set to 480.\nThe batch-size and learning rate for training was 32 and 0.05 respectively,\ntrained for 200 epochs with ﬁlter size of 3×3 with the stride of 1 and margin α =\n1.0 . The pre-trained ResNet50 network was frozen up to c5 layer and randomly\ninitialized for next layers for triplet loss training. The best number of feature\nmaps for c6 layer was estimated to be 4096. Once trained, the representation\nof dimension d = 4096 is obtained on which clustering is performed. Figure 4\n(left) shows the sensitivity of LDVR with varying margin values and number of\nc6 feature maps w.r.t. average NMI scores.\nBenchmarking methods We benchmarked the LDVR for time-series clus-\ntering (i.e. k-means with euclidean distance) with several methods. First, we\ninclude the Pre-trained Deep Visual Representation (PDVR) i.e., the represen-\ntations extracted from the c5 layer before the Triplet loss training. We use deep\n4 Datasets that do not contain enough samples, we chose 60% of the total samples.\n6\nGaurangi Anand and Richi Nayak\nlearning based two non time-series clustering methods, DEC [18] and IDEC [8]\nto cluster 2-D matrix time-series image data. Three traditional distance measure\nbased methods include SPIRAL [11], k-shape [14] and ED with k-means (ED)\napplied on raw time-series. Additionally, we include the two common time series\nrepresentation methods, SAX (1d-SAX) [13] and DWT [15].\nTwo recent time-series clustering approaches: DTCR [12] and USSL [19] have\npartial results published and have not made their source-codes available for them\nto be included in benchmarking. We provide a short analysis in the end.\nAll methods are evaluated with Normalized Mutual Information (NMI) and\nRand Index (RI), the most common metrics to evaluate time-series clustering.\nThe network training is done on SGI Altix XE cluster with an Intel(R)\nXeon(R) 2.50 GHz E5-2680V3 processor running SUSE Linux. We use Keras\n2.2.2 with Tensorﬂow backend and Python 3.5.\n5\nResults: Clustering Accuracy Performance\nWe ﬁrst compute the cumulative ranking for both the NMI and RI scores, with\nthe lower rank indicative of a better score. For each dataset, LDVR, PDVR and\nother methods are ranked based on their performance from 1 to 9, considering\nties. The total rank across all datasets is computed for each method cumulatively\nand then averaged across all datasets, as shown in Table 1.\nTable 1. Average cumulative ranks and average scores for NMI and RI for 85 datasets.\nLDVR PDVR DEC IDEC SPIRAL ED k-shape SAX DWT\nAvg. Rank (NMI)\n2.74\n3.36\n5.25\n5.66\n2.66\n3.08\n3.98\n3.81\n3.12\nAvg. Score (NMI)\n0.36\n0.32\n0.23\n0.19\n0.30\n0.29\n0.25\n0.29\n0.29\nAvg. Rank (RI)\n2.42\n3.02\n4.66\n5.16\n3.09\n3.35\n4.22\n3.51\n3.40\nAvg. Score (RI)\n0.73\n0.72\n0.66\n0.60\n0.69\n0.69\n0.63\n0.70\n0.69\nAs seen in Table 1, the average cumulative NMI ranks for LDVR and SPIRAL\nare lowest, however LDVR gets maximum wins and the maximum average NMI\nscore, by winning in 28 datasets. SPIRAL and PDVR win on 17 and 16 datasets.\nMoreover, the high average RI score and lowest cumulative average RI rank\nindicate the promising performance of LDVR, with winning on 26 datasets for\nRI, followed by PDVR and SPIRAL with 18 and 14 wins, respectively.\nThe poor results of DEC and IDEC indicate that 2-D matrix image data is\nnot suﬃcient for eﬀective clustering. PDVR is the pre-trained representation,\nwhich when ﬁne-tuned to LDVR, helps in achieving a performance boost in\nmore than 50% of the datasets. This validates that the unsupervised ﬁne-tuning\nproposed in this paper adds a bias thereby improving the clustering.\nTable 2 shows the comparative performance of LDVR with DTCR [12] and\nUSSL [19]. In the absence of their source code, we compare ﬁrst on their pub-\nlished 36 UCR datasets in the ﬁrst three columns, the reason for selection of\nUnsupervised Visual Time-Series Representation Learning and Clustering\n7\nTable 2. Performance comparison (Average RI Score) of LDVR, DTCR and USSL\nM1(#36)\nM2(#36)\nM3(#36)\nM3(#Top 36)\nM3(#Top 73)\nM3(#All 85)\n0.77\n0.76\n0.70\n0.90\n0.77\n0.73\nM1: DTCR [12]\nM2: USSL [19]\nM3: LDVR\nthese datasets is undisclosed. It can be observed that LDVR comes very close to\nDTCR and USSL. However, the average RI values on LDVR’s top 36 datasets is\nsigniﬁcantly higher than that published for the two methods. The next columns\nshow it takes as high as 73 UCR datasets to maintain the RI score of 0.77, and\ndrops only to 0.73 when considering all 85 datasets.\nThough many methods have been proposed for time-series analysis, shape-\nbased clustering still appears to be challenging. The combined performance eval-\nuation through NMI and RI establishes LDVR as the leading state-of-the-art\nmethod capable of attaining accurate time-Series representation and clusters\nwhile considering the shape of the time-series.\n5.1\nComparative Performance\nFigure 3 shows the performance of distance-measure and representation-based\nmethods used for benchmarking compared against LDVR using NMI scores on\nall 85 datasets. The larger numbers of points in yellow triangle indicate the good-\nness of LDVR against all of its competitors. The comparison with the leading\nalgorithm, k-shape, ascertains that not many datasets require cross-correlation\ndistance to be used as a shape similarity measure, with LDVR winning on 57 out\nof 85 datasets. Similarly, DTW based SPIRAL is not as eﬀective and accurate\nas LDVR, losing on total 50 datasets.\n1\nLDVR\n0\n1\nSpiral\n1\nLDVR\n0\n1\nED\n1\nLDVR\n0\n1\nK-shape\n1\nLDVR\n0\n1\nSAX\n1\nLDVR\n0\n1\nDWT\nFig. 3. Pairwise performance comparison of LDVR against traditional methods, with\neach dot as a dataset. The dots appearing in yellow triangle indicate LDVR performs\nbetter for that dataset.\nThese methods end up having false matches due to the forced alignment of\ntime-series which is not always needed. On the other hand, k-means provide\nbetter clusters on LDVR versus ED, proving that the learned visual representa-\ntions are stronger and accurate for point-to-point comparison than the raw 1-D\ntime-series values.\nWe also report the performance of all the methods with respect to the vary-\ning length time-series of the UCR repository. We group the UCR datasets into\n8\nGaurangi Anand and Richi Nayak\nFig. 4. Left: Sensitivity analysis with respect to number of c6 feature maps (red) and\nmargin for triplet loss (blue). Middle: Variations in average NMI performance with\nrespect to diﬀerent time-series length bins. Right: Visualization of c6 feature map\nactivations and local feature patterns that led to correct time-series matching.\n3 categories based on the sequence lengths; (1) 31 datasets with length < 200,\n(2) 29 datasets with length 200 to 500, and (3) 25 datasets with length > 500.\nIt can be observed from Figure 4 (middle), that LVDR performs best in all the\ncategories and does not depend on the time-series length. Despite the variable\nlength of input time-series, the feature representation vector size in LDVR re-\nmains constant, depending on the number of feature maps in c6 layer, that is,\n4096.\nQualitative analysis with Visualization Figure 4 (right most) shows a\ntriplet from the Trace dataset from UCR. The ﬁrst 2 instances are anchor and\npositive, with the third one as negative. The discriminant pattern (marked as\nred in top row), observed in anchor, is slightly shifted in positive pair. The\ncorresponding feature maps (in bottom row) show similar activations for the\nanchor-positive pair. However, the discriminant pattern is not seen in the nega-\ntive instance which belongs to a diﬀerent cluster. As a result, the feature maps\ncorresponding to the absent patterns have diﬀerent activation distribution. This\nproves how the local pattern based visual representation is useful and helps in\nidentifying patterns which are similar within instances of the same cluster and\ndiscriminative across instances of diﬀerent clusters.\n6\nConclusion\nWe propose a novel way of generating representations for time-series using visual\nperception and unsupervised triplet loss training. The proposed approach has\nexplored the relatedness of CV datasets to the time-series where the latter is\nexpressed as 2-D image matrices, unlike the usual 1-D manner. Additionally, we\nutilize an existing pre-trained 2-D CNN with certain modiﬁcations to obtain an\neﬀective time-series representation, LDVR. Extensive experiments demonstrated\nthat LDVR outperforms existing clustering methods without requiring dataset-\nspeciﬁc training. This establishes LDVR as the leading state-of-the-art model-\nbased representation learning method, suitable for time-series clustering. Neither\nbeing ﬁxed to any pre-trained network, nor tied to downstream task of clustering\nor classiﬁcation, this makes the proposed approach highly ﬂexible and adaptable\nto any target task.\nUnsupervised Visual Time-Series Representation Learning and Clustering\n9\nReferences\n1. Aghabozorgi, S., Shirkhorshidi, A.S., Wah, T.Y.: Time-series clustering–a decade\nreview. Information Systems 53, 16–38 (2015)\n2. Bengio, Y.: Deep learning of representations for unsupervised and transfer learning.\nIn: Proceedings of ICML Workshop on Unsupervised and Transfer Learning. pp.\n17–36 (2012)\n3. Berndt, D.J., Cliﬀord, J.: Using dynamic time warping to ﬁnd patterns in time\nseries. In: KDD workshop. vol. 10, pp. 359–370. Seattle, WA (1994)\n4. Chen, Y., Keogh, E., Hu, B., Begum, N., Bagnall, A., Mueen, A., Batista, G.: The\nucr time series classiﬁcation archive (2015)\n5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-\nscale hierarchical image database. In: 2009 IEEE conference on computer vision\nand pattern recognition. pp. 248–255. Ieee (2009)\n6. Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.A.: Deep learning for\ntime series classiﬁcation: a review. Data Mining and Knowledge Discovery 33(4),\n917–963 (2019)\n7. Franceschi, J.Y., Dieuleveut, A., Jaggi, M.: Unsupervised scalable representation\nlearning for multivariate time series. In: Advances in Neural Information Processing\nSystems. pp. 4652–4663 (2019)\n8. Guo, X., Gao, L., Liu, X., Yin, J.: Improved deep embedded clustering with local\nstructure preservation. In: IJCAI (2017)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016)\n10. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-\nvolutional neural networks. In: Advances in neural information processing systems.\npp. 1097–1105 (2012)\n11. Lei, Q., Yi, J., Vaculin, R., Wu, L., Dhillon, I.S.: Similarity preserving representa-\ntion learning for time series clustering. IJCAI’19 pp. 2845–2851 (2019)\n12. Ma, Q., Zheng, J., Li, S., Cottrell, G.W.: Learning representations for time series\nclustering. In: Advances in Neural Information Processing Systems (2019)\n13. Malinowski, S., Guyet, T., Quiniou, R.: 1d-sax: A novel symbolic representation\nfor time series. In: International Symposium on Intelligent Data Analysis (2013)\n14. Paparrizos, J., Gravano, L.: Fast and accurate time-series clustering. ACM Trans-\nactions on Database Systems (TODS) 42(2), 8 (2017)\n15. Popivanov, I., Miller, R.J.: Similarity search over time-series data using wavelets.\nIn: IEEE Proceedings 18th international conference on data engineering (2002)\n16. Schroﬀ, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embedding for face\nrecognition and clustering. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 815–823 (2015)\n17. Tolias, G., Sicre, R., J´egou, H.: Particular object retrieval with integral max-\npooling of cnn activations. In: International Conference on Learning Represen-\ntations (2016)\n18. Xie, J., Girshick, R., Farhadi, A.: Unsupervised deep embedding for clustering\nanalysis. In: International conference on machine learning (2016)\n19. Zhang, Q., Wu, J., Zhang, P., Long, G., Zhang, C.: Salient subsequence learn-\ning for time series clustering. IEEE transactions on pattern analysis and machine\nintelligence 41(9), 2193–2207 (2018)\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-11-19",
  "updated": "2021-11-19"
}