{
  "id": "http://arxiv.org/abs/2111.12679v3",
  "title": "On the (In)Tractability of Reinforcement Learning for LTL Objectives",
  "authors": [
    "Cambridge Yang",
    "Michael Littman",
    "Michael Carbin"
  ],
  "abstract": "In recent years, researchers have made significant progress in devising\nreinforcement-learning algorithms for optimizing linear temporal logic (LTL)\nobjectives and LTL-like objectives. Despite these advancements, there are\nfundamental limitations to how well this problem can be solved. Previous\nstudies have alluded to this fact but have not examined it in depth. In this\npaper, we address the tractability of reinforcement learning for general LTL\nobjectives from a theoretical perspective. We formalize the problem under the\nprobably approximately correct learning in Markov decision processes (PAC-MDP)\nframework, a standard framework for measuring sample complexity in\nreinforcement learning. In this formalization, we prove that the optimal policy\nfor any LTL formula is PAC-MDP-learnable if and only if the formula is in the\nmost limited class in the LTL hierarchy, consisting of formulas that are\ndecidable within a finite horizon. Practically, our result implies that it is\nimpossible for a reinforcement-learning algorithm to obtain a PAC-MDP guarantee\non the performance of its learned policy after finitely many interactions with\nan unconstrained environment for LTL objectives that are not decidable within a\nfinite horizon.",
  "text": "On the (In)Tractability of Reinforcement Learning for LTL Objectives\nCambridge Yang1 , Michael L. Littman2 , Michael Carbin1\n1MIT CSAIL\n2Brown University\ncamyang@csail.mit.edu, mlittman@cs.brown.edu, mcarbin@csail.mit.edu\nAbstract\nIn recent years, researchers have made signiﬁcant\nprogress in devising reinforcement-learning algo-\nrithms for optimizing linear temporal logic (LTL)\nobjectives and LTL-like objectives. Despite these\nadvancements, there are fundamental limitations to\nhow well this problem can be solved.\nPrevious\nstudies have alluded to this fact but have not ex-\namined it in depth. In this paper, we address the\ntractability of reinforcement learning for general\nLTL objectives from a theoretical perspective. We\nformalize the problem under the probably approx-\nimately correct learning in Markov decision pro-\ncesses (PAC-MDP) framework, a standard frame-\nwork for measuring sample complexity in rein-\nforcement learning. In this formalization, we prove\nthat the optimal policy for any LTL formula is PAC-\nMDP-learnable if and only if the formula is in the\nmost limited class in the LTL hierarchy, consist-\ning of formulas that are decidable within a ﬁnite\nhorizon. Practically, our result implies that it is im-\npossible for a reinforcement-learning algorithm to\nobtain a PAC-MDP guarantee on the performance\nof its learned policy after ﬁnitely many interactions\nwith an unconstrained environment for LTL objec-\ntives that are not decidable within a ﬁnite horizon.\n1\nIntroduction\nIn reinforcement learning, we situate an autonomous agent\nin an unknown environment and specify an objective. We\nwant the agent to learn the optimal behavior for achieving the\nspeciﬁed objective by interacting with the environment.\nSpecifying an Objective.\nThe objective for the agent\nis a speciﬁcation over possible trajectories of the overall\nsystem—the environment and the agent. Each trajectory is an\ninﬁnite sequence of the states of the system, evolving through\ntime. The objective speciﬁes which trajectories are desirable\nso that the agent can identify optimal or near-optimal behav-\niors with respect to the objective.\nThe Reward Objective.\nOne form of an objective is a re-\nward function. A reward function speciﬁes a scalar value,\na reward, for each state of the system. The desired trajec-\ntories are those with higher cumulative discounted rewards.\nThe reward-function objective is well studied [Sutton and\nBarto, 1998]. It has desirable properties that allow reinforce-\nment-learning algorithms to provide performance guarantees\non learned behavior [Strehl et al., 2006], meaning that algo-\nrithms can guarantee learning behaviors that achieve almost\noptimal cumulative discounted rewards with high probability.\nDue to its versatility, researchers have adopted the reward-\nfunction objective as the de facto standard of behavior speci-\nﬁcation in reinforcement learning.\n1.1\nThe Linear Temporal Logic Objective\nHowever, reward engineering, the practice of encoding desir-\nable behaviors into a reward function, is a difﬁcult challenge\nin applied reinforcement learning [Dewey, 2014; Littman et\nal., 2017]. To reduce the burden of reward engineering, lin-\near temporal logic (LTL) has attracted researchers’ attention\nas an alternative objective.\nLTL is a formal logic used initially to specify behaviors for\nsystem veriﬁcation [Pnueli, 1977]. An LTL formula is built\nfrom a set of propositions about the state of the environment,\nlogical connectives, and temporal operators such as G (al-\nways) and F (eventually). Many reinforcement-learning tasks\nare naturally expressible with LTL [Littman et al., 2017]. For\nsome classic control examples, we can express: 1) Cart-Pole\nas G up (i.e., the pole always stays up), 2) Mountain-Car\nas F goal (i.e., the car eventually reaches the goal), and 3)\nPendulum-Swing-Up as F G up (i.e., the pendulum eventually\nalways stays up).\nResearchers have thus used LTL as an alternative objec-\ntive speciﬁcation for reinforcement learning [Fu and Topcu,\n2014; Sadigh et al., 2014; Li et al., 2017; Hahn et al., 2019;\nHasanbeig et al., 2019; Bozkurt et al., 2020].\nGiven an\nLTL objective speciﬁed by an LTL formula, each trajectory\nof the system either satisﬁes or violates that formula. The\nagent should learn the behavior that maximizes the prob-\nability of satisfying that formula.\nMoreover, research has\nshown that using LTL objectives supports automated reward\nshaping [Jothimurugan et al., 2019; Camacho et al., 2019;\nJiang et al., 2020].\narXiv:2111.12679v3  [cs.AI]  24 Jun 2022\n1.2\nTrouble with Inﬁnite Horizons\nThe general class of LTL objectives consists of inﬁnite-hori-\nzon objectives—objectives that require inspecting inﬁnitely\nmany steps of a trajectory to determine if the trajectory satis-\nﬁes the objective. For example, consider the objective F goal\n(eventually reach the goal). Given an inﬁnite trajectory, the\nobjective requires inspecting the entire trajectory in the worst\ncase to determine that the trajectory violates the objective.\nDespite the above developments on reinforcement learning\nwith LTL objectives, the inﬁnite-horizon nature of these ob-\njectives presents challenges that have been alluded to—but\nnot formally treated—in prior work. [Henriques et al., 2012;\nAshok et al., 2019; Jiang et al., 2020] noted slow learning\ntimes for mastering inﬁnite-horizon properties. [Littman et\nal., 2017] provided a speciﬁc environment that illustrates the\nintractability of learning for a speciﬁc inﬁnite-horizon objec-\ntive, arguing for the use of a discounted variant of LTL.\nA similar issue exists for the inﬁnite-horizon, average-\nreward objectives.\nIn particular, it is understood that re-\ninforcement-learning algorithms do not have guarantees on\nthe learned behavior for inﬁnite-horizon, average-reward\nproblems without additional assumptions on the environ-\nment [Kearns and Singh, 2002].\nHowever, to our knowledge, no prior work has formally\nanalyzed the learnability of LTL objectives.1\nOur\nResults.\nWe\nleverage\nthe\nPAC-MDP\nframe-\nwork [Strehl et al., 2006] to prove that reinforcement\nlearning for inﬁnite-horizon LTL objectives is intractable.\nThe intuition for this intractability is: Any ﬁnite number of\ninteractions with an environment with unknown transition\ndynamics is insufﬁcient to identify the environment dynam-\nics perfectly.\nMoreover, for an inﬁnite-horizon objective,\na behavior’s satisfaction probability under the inaccurate\nenvironment dynamics can be arbitrarily different from the\nbehavior’s satisfaction probability under the true dynamics.\nConsequently, a learner cannot guarantee with any conﬁ-\ndence that it has identiﬁed near-optimal behavior for an\ninﬁnite-horizon objective.\n1.3\nImplications for Relevant and Future Work\nOur results provide a framework to categorize approaches\nthat either focus on tractable LTL objectives or weaken the\nguarantees of an algorithm. As a result, we interpret several\nprevious approaches as instantiations of the following cate-\ngories:\n• Work with ﬁnite-horizon LTL objectives, the complement\nof inﬁnite-horizon objectives, to obtain guarantees on the\nlearned behavior [Henriques et al., 2012]. These objectives,\nlike a ∧Xa (a is true for two steps), are decidable within a\nknown ﬁnite number of steps.\n• Seek a best-effort conﬁdence interval [Ashok et al., 2019].\nSpeciﬁcally, the interval can be trivial in the worst case, de-\n1Concurrent to this work, [Alur et al., 2021] also examine the\nintractability of LTL objectives. They state and prove a theorem that\nis a weaker version of the core theorem of this work. Their work\nwas made public while this work was under conference review. We\ndiscuss their work in Appendix I.\nnoting that learned behavior is a maximally poor approxima-\ntion of the optimal behavior.\n• Make additional assumptions about the environment to ob-\ntain guarantees on the learned behavior [Fu and Topcu, 2014;\nBr´azdil et al., 2014].\n• Change the problem by working with LTL-like objectives\nsuch as: 1. relaxed LTL objectives that become exactly LTL in\nthe (unreachable) limit [Sadigh et al., 2014; Hahn et al., 2019;\nHasanbeig et al., 2019; Bozkurt et al., 2020] and 2. objec-\ntives that use temporal operators but employ a different se-\nmantics [Littman et al., 2017; Li et al., 2017; Giacomo et al.,\n2019; Camacho et al., 2019]. The learnability of these objec-\ntives is a potential future research direction.\n1.4\nContributions\nWe make the following contributions:\n• A formalization of reinforcement learning with LTL ob-\njectives under the probably approximately correct in Markov\ndecision processes (PAC-MDP) framework [Fiechter, 1994;\nKearns and Singh, 2002; Kakade, 2003], a standard frame-\nwork for measuring sample complexity for reinforcement-\nlearning algorithms; and a formal deﬁnition of LTL-PAC-\nlearnable, a learnability criterion for LTL objectives.\n• A statement and proof that: 1. Any inﬁnite-horizon LTL\nformula is not LTL-PAC-learnable. 2. Any ﬁnite-horizon LTL\nformula is LTL-PAC-learnable. To that end, for any inﬁnite-\nhorizon formula, we give a construction of two special fam-\nilies of MDPs as counterexamples with which we prove that\nthe formula is not LTL-PAC-learnable.\n• Experiments with current reinforcement-learning algo-\nrithms for LTL objectives that provide empirical support for\nour theoretical result.\n• A categorization of approaches that focus on tractable ob-\njectives or weaken the guarantees of LTL-PAC-learnable and\na classiﬁcation of previous approaches into these categories.\n2\nPreliminaries: Reinforcement Learning\nThis section provides deﬁnitions for MDPs, planning, rein-\nforcement learning, and PAC-MDP.\n2.1\nMarkov Processes\nWe ﬁrst review some basic notations for Markov processes.\nA Markov decision process (MDP) is a tuple M\n=\n(S, A, P, s0), where S and A are ﬁnite sets of states and ac-\ntions, P : (S × A) →∆(S) is a transition probability func-\ntion that maps a current state and an action to a distribution\nover next states, and s0 ∈S is an initial state. The MDP is\nsometimes referred to as the environment MDP to distinguish\nit from any speciﬁc objective.\nA (stochastic) Markovian policy π for an MDP is a func-\ntion π: S →∆(A) that maps each state of the MDP to a\ndistribution over the actions.\nA (stochastic) non-Markovian policy π for an MDP is a\nfunction π: ((S × A)∗× S) →∆(A) that maps a history of\nstates and actions of the MDP to a distribution over actions.\nAn MDP and a policy on the MDP induce a discrete-time\nMarkov chain (DTMC). A DTMC is a tuple D = (S, P, s0),\nwhere S is a ﬁnite set of states, P : S →∆(S) is a transition-\nprobability function that maps a current state to a distribution\nover next states, and s0 ∈S is an initial state. A sample path\nof D is an inﬁnite sequence of states w ∈Sω. The sample\npaths of a DTMC form a probability space.\n2.2\nObjective\nAn objective for an MDP M = (S, A, P, s0) is a measurable\nfunction κ: Sω →R on the probability space of the DTMC\nD induced by M and a policy π. The value of the objec-\ntive for the MDP M and a policy π is the expectation of the\nobjective under that probability space:\nV π\nM,κ = Ew∼D[κ(w)]\n(D induced by M and π).\nFor example, the cumulative discounted rewards objec-\ntive [Puterman, 1994] with discount γ and a reward function\nR: S →R is: κreward(w) ≜P∞\ni=0 γi · R(w[i]).\nAn optimal policy maximizes the objective’s value: π∗=\narg maxπ V π\nM,κ. The optimal value V π∗\nM,κ is then the objec-\ntive value of the optimal policy. A policy π is ϵ-optimal if its\nvalue is ϵ-close to the optimal value: V π\nM,κ ≥V π∗\nM,κ −ϵ.\n2.3\nPlanning with a Generative Model\nA planning-with-generative-model algorithm [Kearns et al.,\n1999; Grill et al., 2016] has access to a generative model,\na sampler, of an MDP’s transitions but does not have direct\naccess to the underlying probability values. It can take any\nstate and action and sample a next state. It learns a policy\nfrom those sampled transitions.\nFormally, a planning-with-generative-model algorithm A\nis a tuple (AS, AL), where AS is a sampling algorithm that\ndrives how the environment is sampled, and AL is a learning\nalgorithm that learns a policy from the samples obtained by\napplying the sampling algorithm.\nIn particular, the sampling algorithm AS is a function\nthat maps from a history of sampled environment transitions\n((s0, a0, s′\n0) . . . (sk, ak, s′\nk)) to the next state and action to sam-\nple (sk+1, ak+1) , resulting in s′\nk+1 ∼P( · | sk+1, ak+1 ). Iter-\native application of the sampling algorithm AS produces a\nsequence of sampled environment transitions.\nThe learning algorithm is a function that maps that se-\nquence of sampled environment transitions to a non-Marko-\nvian policy of the environment MDP. Note that the sampling\nalgorithm can internally consider alternative policies as part\nof its decision of what to sample. Also, note that we deliber-\nately consider non-Markovian policies since the optimal pol-\nicy for an LTL objective (deﬁned later) is non-Markovian in\ngeneral (unlike a cumulative discounted rewards objective).\n2.4\nReinforcement Learning\nIn reinforcement learning, an agent is situated in an environ-\nment MDP and only observes state transitions. We also allow\nthe agent to reset to the initial state as in [Fiechter, 1994].\nWe can view a reinforcement-learning algorithm as a\nspecial kind of planning-with-generative-model algorithm\n(AS, AL) such that the sampling algorithm always either fol-\nlows the next state sampled from the environment or resets to\nthe initial state of the environment.\n2.5\nProbably Approximately Correct in MDPs\nA successful planning-with-generative-model algorithm (or\nreinforcement-learning algorithm) should learn from the sam-\npled environment transitions and produce an optimal policy\nfor the objective in the environment MDP. However, since the\nenvironment transitions may be stochastic, we cannot expect\nan algorithm to always produce the optimal policy. Instead,\nwe seek an algorithm that, with high probability, produces a\nnearly optimal policy. The PAC-MDP framework [Fiechter,\n1994; Kearns and Singh, 2002; Kakade, 2003], which takes\ninspiration from probably approximately correct (PAC) learn-\ning [Valiant, 1984], formalizes this notion. The PAC-MDP\nframework requires efﬁciency in both sampling and algorith-\nmic complexity. In this work, we only consider sample ef-\nﬁciency and thus omit the requirement on algorithmic com-\nplexity. Next, we generalize the PAC-MDP framework from\nreinforcement-learning with a reward objective to planning-\nwith-generative-model with a generic objective.\nDeﬁnition 1. Given an objective κ, a planning-with-genera-\ntive-model algorithm (AS, AL) is κ-PAC (probably approxi-\nmately correct for objective κ) in an environment MDP M\nif, with the sequence of transitions T of length N sampled\nusing the sampling algorithm AS, the learning algorithm AL\noutputs a non-Markovian ϵ-optimal policy with probability at\nleast 1 −δ for any given ϵ > 0 and 0 < δ < 1. That is:\nPT∼⟨M,AS⟩N\n\u0010\nV AL(T )\nM,κ\n≥V π∗\nM,κ −ϵ\n\u0011\n≥1 −δ.\n(1)\nWe use T∼\n\nM, AS\u000b\nNto denote that the probability space\nis over the set of length-N transition sequences sampled from\nthe environment M using the sampling algorithm AS. For\nbrevity, we will drop\n\nM, AS\u000b\nN when it is clear from context\nand simply write PT(.) to denote that the probability space is\nover the sampled transitions.\nDeﬁnition 2. Given an objective κ, a κ-PAC planning-with-\ngenerative-model algorithm is sample efﬁciently κ-PAC if the\nnumber of sampled transitions N is asymptotically polyno-\nmial in 1\nϵ , 1\nδ , |S|, |A|.\nNote that the deﬁnition allows the polynomial to have con-\nstant coefﬁcients that depends on κ.\n3\nLinear Temporal Logic Objectives\nThis section describes LTL and its use in objectives.\n3.1\nLinear Temporal Logic\nA linear temporal logic (LTL) formula is built from a ﬁnite set\nof atomic propositions Π, logical connectives ¬, ∧, ∨, tem-\nporal next X, and temporal operators G (always), F (even-\ntually), and U (until). Equation (2) gives the grammar of an\nLTL formula φ over the set of atomic propositions Π:\nφ ··= a\n\f\f¬φ\n\f\fφ∧φ\n\f\fφ∨φ\n\f\fXφ\n\f\fG φ\n\f\fF φ\n\f\fφUφ, a ∈Π. (2)\nLTL is a logic over inﬁnite-length words. Informally, these\ntemporal operators have the following meanings: Xφ asserts\nFinitary\nGuarantee\nSafety\nObligation\nPersistence\nRecurrence\nReactivity\nRestricted\nGeneral\nFigure 1: The hierarchy of LTL\nthat φ is true at the next time step; G φ asserts that φ is always\ntrue; F φ asserts that φ is eventually true; ψ U φ asserts that ψ\nneeds to stay true until φ eventually becomes true. We give\nthe formal semantics of each operator in Appendix A.2. We\nwrite w ⊨φ to denote that the inﬁnite word w satisﬁes φ.\n3.2\nMDP with LTL Objectives\nAn LTL objective maximizes the probability of satisfying an\nLTL formula. We formalize this notion below.\nAn LTL speciﬁcation for an MDP is a tuple (L, φ), where\nL: S →2Π is a labeling function, and φ is an LTL formula\nover atomic propositions Π. The labeling function is a classi-\nﬁer mapping each MDP state to a tuple of truth values of the\natomic propositions in φ. For a sample path w, we use L (w)\nto denote the element-wise application of L on w.\nThe LTL objective ξ speciﬁed by the LTL speciﬁcation is\nthe satisfaction of the formula φ of a sample path mapped by\nthe labeling function L, that is: κ(w) ≜1L(w)⊨φ. The value\nof this objective is called the satisfaction probability of ξ:\nV π\nM,ξ = Pw∼D(L (w) ⊨φ)\n(D induced by M and π).\n3.3\nInﬁnite Horizons in LTL Objectives\nAn LTL formula describes either a ﬁnite-horizon or inﬁnite-\nhorizon property. [Manna and Pnueli, 1987] classiﬁed LTL\nformulas into seven classes, as shown in Figure 1.\nEach\nclass includes all the classes to the left of that class (e.g.,\nFinitary ⊂Guarantee, but Safety ̸⊂Guarantee), with the\nFinitary class being the most restricted and the Reactivity\nclass being the most general. Below we brieﬂy describe the\nkey properties of the leftmost three classes relevant to the core\nof this paper. We present a complete description of all the\nclasses in Appendix A.2.\n• φ ∈Finitary iff there exists a horizon H such that inﬁnite-\nlength words sharing the same preﬁx of length H are either\nall accepted or all rejected by φ. E.g., a ∧Xa (i.e., a is true\nfor two steps) is in Finitary.\n• φ ∈Guarantee iff there exists a language of ﬁnite words L\n(i.e., a Boolean function on ﬁnite-length words) such that\nw ⊨φ if L accepts a preﬁx of w. Informally, a formula\nin Guarantee asserts that something eventually happens.\nE.g., F a (i.e., eventually a is true) is in Guarantee.\n• φ ∈Safety iff there exists a language of ﬁnite words L such\nthat w ⊨φ if L accepts all preﬁxes of w. Informally, a\nformula in Safety asserts that something always happens.\nE.g., G a (i.e., a is always true) is in Safety.\nMoreover, the set of ﬁnitary is the intersection of the set\nof guarantee formulas and the set of safety formulas. Any\nφ ∈Finitary, or equivalently φ ∈Guarantee∩Safety, inher-\nently describes ﬁnite-horizon properties. Any φ ̸∈Finitary,\ng\nh\nq\na1, p\na2, p\na1, 1 −p\na2, 1 −p\ng\nh\nq\na2, p\na1, p\na1, 1 −p\na2, 1 −p\nFigure 2: Two MDPs parameterized by p in range 0 < p < 1.\nAction a1 in the MDP on the left and action a2 in the MDP on the\nright have probability p of transitioning to the state h. Conversely,\naction a2 in the MDP on the left and action a1 in the MDP on the\nright have probability p of transitioning to the state q. Both actions\nin both MDPs have probability 1 −p to loop around the state g.\nor equivalently φ ∈Guarantee∁∪Safety∁, inherently de-\nscribes inﬁnite-horizon properties. We will show that rein-\nforcement-learning algorithms cannot provide PAC guaran-\ntees for LTL objectives speciﬁed by formulas that describe\ninﬁnite-horizon properties.\n3.4\nIntuition of the Problem\nSuppose that we send an agent into one of the MDPs in Fig-\nure 2, and want its behavior to satisfy “eventually reach the\nstate h”, expressed as the LTL formula F h.\nThe optimal\nbehavior is to always choose the action along the transition\ng →h for both MDPs (i.e., a1 for the MDP on the left and a2\nfor the MDP on the right). This optimal behavior satisﬁes the\nobjective with probability one. However, the agent does not\nknow which of the two MDPs it is in. The agent must follow\nits sampling algorithm to explore the MDP’s dynamics and\nuse its learning algorithm to learn this optimal behavior.\nIf the agent observes neither transitions going out of g (i.e.,\ng →h or g →q) during sampling, it will not be able to dis-\ntinguish between the two actions. The best it can do is a 50%\nchance guess and cannot provide any non-trivial guarantee on\nthe probability of learning the optimal action.\nOn the other hand, if the agent observes one of the tran-\nsitions going out of g, it will be able to determine which\naction leads to state h, thereby learning always to take that\naction. However, the probability of observing any such tran-\nsition with N interactions is at most 1 −(1 −p)N. This is\nproblematic: with any ﬁnite N, there always exists a value of\np such that this probability is arbitrarily close to 0. In other\nwords, with any ﬁnite number of interactions, without know-\ning the value of p, the agent cannot guarantee (a non-zero\nlower bound on) its chance of learning a policy that satisﬁes\nthe LTL formula F h.\nFurther, the problem is not limited to this formula. For ex-\nample, the objective “never reach the state q”, expressed as\nthe formula G ¬q, has the same problem in these two MDPs.\nMore generally, for any LTL formula describing an inﬁnite-\nhorizon property, we construct two counterexample MDPs\nwith the same nature as the ones in Figure 2, and prove that it\nis impossible to guarantee learning the optimal policy.\n4\nLearnability of LTL Objectives\nThis section states and outlines the proof to the main result.\nBy specializing the κ-PAC deﬁnitions (Deﬁnitions 1 and 2)\nwith the deﬁnition of LTL objectives in Section 3.2, we obtain\nthe following deﬁnitions of LTL-PAC.\ng0\n...\ngk\n...\ngl\na1, 1 −p\na2, 1 −p\nh0\n. ..\nhu\n. ..\nhv\nq0\n. ..\nqm\n. ..\nqn\na1, p\na2, p\na2, p\na1, p\nM1\nM2\nM1 & M2\nFigure 3: Counterexample MDPs M1 and M2, with transitions distinguished by arrow types (see legend). Both MDPs are parameterized\nby the parameter p that is in range 0 < p < 1. Unlabeled edges are deterministic (actions a1 and a2 transition with probability 1). Ellipsis\nindicates a deterministic chain of states.\nDeﬁnition 3. Given an LTL objective ξ, a planning-with-gen-\nerative-model algorithm (AS, AL) is LTL-PAC (probably ap-\nproximated correct for LTL objective ξ) in an environment\nMDP M for the LTL objective ξ if, with the sequence of tran-\nsitions T of length N sampled using the sampling algorithm\nAS, the learning algorithm AL outputs a non-Markovian ϵ-\noptimal policy with a probability of at least 1−δ for all ϵ > 0\nand 0 < δ < 1. That is,\nPT∼⟨M,AS⟩N\n\u0010\nV AL(T )\nM,ξ\n≥V π∗\nM,ξ −ϵ\n\u0011\n≥1 −δ.\n(3)\nWe call the probability on the left of the inequality the LTL-\nPAC probability of the algorithm (AS, AL).\nDeﬁnition 4. Given an LTL objective ξ, an LTL-PAC plan-\nning-with-generative-model algorithm for ξ is sample efﬁ-\nciently LTL-PAC if the number of sampled transitions N is\nasymptotically polynomial to 1\nϵ , 1\nδ , |S|, |A|.\nWith the above deﬁnitions, we can now deﬁne the PAC\nlearnability of an LTL objective and state the main theorem.\nDeﬁnition 5. An LTL formula φ over atomic proposi-\ntions Π is LTL-PAC-learnable by planning-with-generative-\nmodel (reinforcement-learning) if there exists a sample efﬁ-\nciently LTL-PAC planning-with-generative-model (reinforce-\nment-learning) algorithm for all environment MDPs and all\nconsistent labeling functions L (that is, L maps from the\nMDP’s states to 2Π) for the LTL objective speciﬁed by (L, φ).\nTheorem 1. An LTL formula φ is LTL-PAC-learnable by\nreinforcement-learning (planning-with-generative-model) if\n(and only if) φ is ﬁnitary.\nBetween the two directions of Theorem 1, the forward di-\nrection (“only if”) is more important.\nThe forward direc-\ntion states that for any LTL formula not in Finitary (that is,\ninﬁnite-horizon properties), there does not exist a planning-\nwith-generative-model algorithm—which by deﬁnition also\nexcludes any reinforcement-learning algorithm—that is sam-\nple efﬁciently LTL-PAC for all environments. This result is\nthe core contribution of the paper—inﬁnite-horizon LTL for-\nmulas are not sample efﬁciently LTL-PAC-learnable.\nAlternatively, the reverse direction of Theorem 1 states\nthat, for any ﬁnitary formula (ﬁnite-horizon properties), there\nexists a reinforcement-learning algorithm—which by deﬁ-\nnition is also a planning-with-generative-model algorithm—\nthat is sample efﬁciently LTL-PAC for all environments.\n4.1\nProof of Theorem 1: Forward Direction\nThis section proves the forward direction of Theorem 1. First,\nwe construct a family of pairs of MDPs. Then, for the sin-\ngular case of the LTL formula F h0, we derive a sample\ncomplexity lower bound for any LTL-PAC planning-with-\ngenerative-model algorithm applied to our family of MDPs.\nThis lower bound necessarily depends on a speciﬁc transition\nprobability in the MDPs. Finally, we generalize this bound to\nany non-ﬁnitary LTL formula and conclude the proof.\nMDP Family\nWe give two constructions of parameterized counterexample\nMDPs M1 and M2 shown in Figure 3. The key design be-\nhind each pair in the family is that no planning-with-genera-\ntive-model algorithm can learn a policy that is simultaneously\nϵ-optimal on both MDPs without observing a number of sam-\nples that depends on the probability of a speciﬁc transition.\nBoth MDPs are parameterized by the shape parameters k,\nl, u, v, m, n, and an unknown transition probability parameter\np. The actions are {a1, a2}, and the state space is partitioned\ninto three regions (as shown in Figure 3: states g0...l (the grey\nstates), states h0...v (the line-hatched states), and states q0...n\n(the white states). All transitions, except gl →h0 and gl →\nq0, are the same between M1 and M2. The effect of this\ndifference between the two MDPs is that, for Mi, i ∈{1, 2}:\n• Action ai in Mi at the state gl will transition to the state\nh0 with probability p, inducing a run that cycles in the region\nhu...v forever.\n• Action a3−i (the alternative to ai) in Mi at the state gl will\ntransition to the state q0 with probability p, inducing a run\nthat cycles in the region qm...n forever.\nFurther, for any policy, a run of the policy on both MDPs\nmust eventually reach h0 or q0 with probability 1, and ends\nin an inﬁnite cycle in either hu...v or qm...n.\nSample Complexity of F h0\nWe next consider the LTL objective ξh0 speciﬁed by the LTL\nformula F h0 and the labeling function Lh0 that labels only\nthe state h0 as true. A sample path on the MDPs (Figure 3)\nsatisﬁes this objective iff the path reaches the state h0.\nGiven ϵ > 0 and 0 < δ < 1, our goal is to derive a lower\nbound on the number of sampled environment transitions per-\nformed by an algorithm, so that the satisfaction probability of\nπ, the learned policy, is ϵ-optimal (i.e., V π\nM,ξh0 ≥V π∗\nM,ξh0 −ϵ)\nwith a probability of least 1 −δ.\nThe key rationale behind the following lemma is that, if a\nplanning-with-generative-model algorithm has not observed\nany transition to either h0 or q0, the learned policy cannot be\nϵ-optimal in both M1 and M2.\nLemma 2. For any planning-with-generative-model algo-\nrithm (AS, AL), it must be the case that: min (ζ1, ζ2) ≤\n1\n2, where ζi = PT\n\u0010\nV AL(T )\nMi,ξh0 ≥V π∗\nMi,ξh0 −ϵ\n\f\f\f n (T) = 0\n\u0011\nand\nn(T) is the number of transitions in T that start from gl and\nend in either h0 or q0.\nThe value ζi is the LTL-PAC probability of a learned policy\non Mi, given that the planning-with-generative-model algo-\nrithm did not observe any information that allows the algo-\nrithm to distinguish between M1 and M2.\nProof. We present a proof of Lemma 2 in Appendix B.\nA planning-with-generative-model algorithm cannot learn\nan ϵ-optimal policy without observing a transition to either\nh0 or q0. Therefore, we bound the sample complexity of the\nalgorithm from below by the probability that the sampling\nalgorithm does observe such a transition:\nLemma 3. For the LTL objective ξh0, the number of sam-\nples, N, for an LTL-PAC planning-with-generative-model al-\ngorithm for both M1 and M2 (for any instantiation of the pa-\nrameters k, l, u, v, m, n) has a lower bound of N ≥\nlog(2δ)\nlog(1−p).\nBelow we give a proof sketch of Lemma 3; we give the\ncomplete proof in Appendix C.\nProof Sketch of Lemma 3. First, we assert that the two in-\nequalities of Equation (3) for both M1 and M2 holds true\nfor a planning-with-generative-model algorithm.\nNext, by\nconditioning on n(T) = 0, plugging in the notation of ζi,\nand relaxing both inequalities, we get (1 −ζi)PT(n(T) =\n0) ≤δ, for i ∈{1, 2}.\nThen, since n(T) = 0 only\noccurs when all transitions from gl end in gk, we have\nPT(n(T) = 0) ≥(1 −p)N. Combining the inequalities,\nwe get (1 −min(ζ1, ζ2))(1 −p)N ≤δ. Finally, we apply\nLemma 2 to get the desired lower bound of N ≥\nlog(2δ)\nlog(1−p).\nSample Complexity of Non-ﬁnitary Formulas\nThis section generalizes our lower bound on F h0 to all non-\nﬁnitary LTL formulas. The key observation is that for any\nnon-ﬁnitary LTL formula, we can choose a pair of MDPs,\nM1 and M2, from our MDP family. For both MDPs in this\npair, ﬁnding an ϵ-optimal policy for F h0 is reducible to ﬁnd-\ning an ϵ-optimal policy for the given formula. By this reduc-\ntion, the established lower bound for the case of F h0 also\napplies to the case of any non-ﬁnitary formula. Therefore,\nthe sample complexity of learning an ϵ-optimal policy for any\nnon-ﬁnitary formula has a lower bound of\nlog(2δ)\nlog(1−p).\nWe will use [w1; w2; . . . wn] to denote the concatenation of\nthe ﬁnite-length words w1 . . . wn. We will use wi to denote\nthe repetition of the ﬁnite-length word w by i times, and w∞\nto denote the inﬁnite repetition of w.\nDeﬁnition 6. An accepting (resp. rejecting) inﬁnite-length\nword [wa; w∞\nb ] of φ is uncommittable if there exists ﬁnite-\nlength words wc, wd such that φ rejects (resp. accepts)\n[wa; wi\nb; wc; w∞\nd ] for all i ∈N.\nLemma 4. If φ has an uncommittable word w, there is an\ninstantiation of M1 (or M2) in Figure 3 and a labeling func-\ntion L, such that, for any policy, the satisfaction probabilities\nof that policy in M1 (or M2) for the LTL objectives speciﬁed\nby (L, φ) and (Lh0, F h0) are the same.\nProof. For an uncommittable word w, we ﬁrst ﬁnd the ﬁnite-\nlength words wa,wb,wc,wd according to Deﬁnition 6. We\nthen instantiate M1 and M2 in Figure 3 as follows.\n• If w is an uncommittable accepting word, we set k, l, u,\nv, m, n (Figure 3) to |wa|, |wa| + |wb|, 0, |wb|, |wc| and\n|wc|+|wd|, respectively. We then set the labeling function as\nin Equation (4).\n• If w is an uncommittable rejecting word, we set k, l, u, v,\nm, n (Figure 3) to |wa|, |wa| + |wb|, |wc|, |wc| + |wd|, 0\nand |wb|, respectively. We then set the labeling function as in\nEquation (5).\nL(s)=\n\n\n\n[wa; wb][j] if s=gj\nwb[j]\nif s=hj\n[wc; wd][j] if s=qj\nL(s)=\n\n\n\n[wa; wb][j] if s=gj\n[wc; wd][j] if s=hj\nwb[j]\nif s=qj\n(4)\n(5)\nIn words, for an uncommittable accepting word, we label\nthe states g0...l one-by-one by [wa; wb]; we label the states\nh0...v one-by-one by wb (and set u = 0, which eliminates the\nchain of states h0...u); we label the states q0...n one-by-one\nby [wc; wd]. Symmetrically, for an uncommittable rejecting\nword, we label the states g0...l one-by-one by [wa; wb]; we\nlabel the states h0...v one-by-one by [wc; wd]; we label the\nstates q0...n one-by-one by wb (and set m = 0, which elimi-\nnates the chain of states q0...m).\nBy the above instantiation, the two objectives speciﬁed by\n(L, φ) and (Lh0, F h0) are equivalent in M1 and M2. In\nparticular, any path in M1 or M2 satisﬁes the LTL objec-\ntive speciﬁed by (L, φ) if and only if the path visits the state\nh0 and therefore also satisﬁes the LTL objective speciﬁed by\n(Lh0, F h0). Therefore, any policy must have the same satis-\nfaction probability for both objectives.\nLemma 5. For φ ̸∈Finitary, the number of samples for\na planning-with-generative-model algorithm to be LTL-PAC\nhas a lower bound of N ≥\nlog(2δ)\nlog(1−p).\nProof. A corollary of Lemma 4 is: for any φ that has an un-\ncommittable word, we can construct a pair of MDPs M1 and\nM2 in the family of pairs of MDPs in Figure 3, such that, in\nboth MDPs, a policy is sample efﬁciently LTL-PAC for the\nLTL objective speciﬁed by (L, φ) if it is sample efﬁciently\nLTL-PAC for the LTL objective speciﬁed by (Lh0, F h0).\nThis property implies that the lower bound in Lemma 3 for\nthe objective speciﬁed by (Lh0, F h0) also applies to the ob-\njective speciﬁed by (L, φ), provided that any φ ̸∈Finitary\nhas an uncommittable word.\nIn Appendix D, we prove a\nlemma that any formula φ ̸∈Guarantee has an uncommit-\ntable accepting word, and any formula φ ̸∈Safety has an un-\ncommittable rejecting word. Since Finitary is the intersec-\ntion of Guarantee and Safety, this completes the proof.\nConclusion\nNote that the lower bound\nlog(2δ)\nlog(1−p) depends on p, the tran-\nsition probability in the constructed MDPs. Moreover, for\nδ <\n1\n2, as p approaches 0, this lower bound goes to inﬁn-\nity. As a result, the bound does not satisfy the deﬁnition of\nsample efﬁciently LTL-PAC planning-with-generative-model\nalgorithm for the LTL objective (Deﬁnition 2), and thus no al-\ngorithm is sample efﬁciently LTL-PAC. Therefore, LTL for-\nmulas not in Finitary are not LTL-PAC-learnable. This com-\npletes the proof of the forward direction of Theorem 1.\n4.2\nProof Sketch of Theorem 1: Reverse Direction\nThis section gives a proof sketch to the reverse direction of\nTheorem 1. We give a complete proof in Appendix E.\nWe prove the reverse direction of Theorem 1 by reducing\nthe problem of learning a policy for any ﬁnitary formula to\nthe problem of learning a policy for a ﬁnite-horizon cumula-\ntive rewards objective. We conclude the reverse direction of\nthe theorem by invoking a known PAC reinforcement-learn-\ning algorithm on the later problem.\n• Reduction to Inﬁnite-horizon Cumulative Rewards.\nFirst, given an LTL formula in Finitary and an environment\nMDP, we will construct an augmented MDP with rewards\nsimilar to [Giacomo et al., 2019; Camacho et al., 2019]. We\nreduce the problem of ﬁnding the optimal non-Markovian\npolicy for satisfying the formula in the original MDP to the\nproblem of ﬁnding the optimal Markovian policy that maxi-\nmizes the inﬁnite-horizon (undiscounted) cumulative rewards\nin this augmented MDP.\n• Reduction\nto\nFinite-horizon Cumulative\nRewards.\nNext, we reduce the inﬁnite-horizon cumulative rewards to\na ﬁnite-horizon cumulative rewards, using the fact that the\nformula is ﬁnitary.\n• Sample Complexity Upper Bound. Lastly, [Dann et al.,\n2019] have derived an upper bound on the sample complex-\nity for a reinforcement-learning algorithm for ﬁnite-horizon\nMDPs. We thus specialize this known upper bound to our\nproblem setup of the augmented MDP and conclude that any\nﬁnitary formula is PAC-learnable.\n4.3\nConsequence of the Core Theorem\nTheorem 1 implies that: For any non-ﬁnitary LTL objective,\ngiven any arbitrarily large ﬁnite sample of transitions, the\nlearned policy need not perform near-optimally.\nThis im-\nplication is unacceptable in applications that require strong\nguarantees of the overall system’s behavior.\n5\nEmpirical Justiﬁcations\nThis section empirically demonstrates our main result, the\nforward direction of Theorem 1.\nPrevious work has introduced various reinforcement-learn-\ning algorithms for LTL objectives [Sadigh et al., 2014;\nHahn et al., 2019; Hasanbeig et al., 2019; Bozkurt et al.,\n2020]. We ask the research question: Do the sample complex-\nities of these algorithms depend on the transition probabili-\nties of the environment? To answer the question, we evaluate\nvarious algorithms and empirically measure the sample sizes\nfor them to obtain near-optimal policies with high probability.\n5.1\nMethodology\nWe consider various recent reinforcement-learning algo-\nrithms for LTL objectives [Sadigh et al., 2014; Hahn et al.,\n2019; Bozkurt et al., 2020]. We consider two pairs of LTL\nFigure 4: Left: LTL-PAC probabilities vs. number of samples, vary-\ning parameters p. Right: number of samples needed to reach 0.9\nLTL-PAC probability vs. parameter p.\nformulas and environment MDPs (LTL-MDP pair). The ﬁrst\npair is the formula F h and the counterexample MDP as\nshown in Figure 2. The second pair is adapted from a case\nstudy in [Sadigh et al., 2014]. We focus on the ﬁrst pair in\nthis section and defer the complete evaluation to Appendix G.\nWe run the considered algorithms on each chosen LTL-\nMDP pair with a range of values for the parameter p and let\nthe algorithms perform N environment samples. For each al-\ngorithm and each pair of values of p and N, we ﬁx ϵ = 0.1\nand repeatedly run the algorithm to obtain a Monte Carlo esti-\nmation of the LTL-PAC probability (left side of Equation (3))\nfor that setting of p, N and ϵ. We repeat each setting until\nthe estimated standard deviation of the estimated probability\nis within 0.01. In the end, for each algorithm and LTL-MDP\npair we obtain 5×21 = 105 LTL-PAC probabilities and their\nestimated standard deviations.\nFor the ﬁrst LTL-MDP pair, we vary p by a geometric pro-\ngression from 10−1 to 10−3 in 5 steps. We vary N by a geo-\nmetric progression from 101 to 105 in 21 steps. For the second\nLTL-MDP pair, we vary p by a geometric progression from\n0.9 to 0.6 in 5 steps. We vary N by a geometric progression\nfrom 3540 to 9 × 104 in 21 steps. If an algorithm does not\nconverge to the desired LTL-PAC probability within 9 × 104\nsteps, we rerun the experiment with an extended range of N\nfrom 3540 to 1.5 × 105.\n5.2\nResults\nFigure 4 presents the results for the algorithm in [Bozkurt et\nal., 2020] with the setting of Multi-discount, Q-learning, and\nthe ﬁrst LTL-MDP pair. On the left, we plot the LTL-PAC\nprobabilities vs. the number of samples N, one curve for each\np. On the right, we plot the intersections of the curves in the\nleft plot with a horizontal cutoff of 0.9.\nAs we see from the left plot of Figure 4, for each p, the\ncurve starts at 0 and grows to 1 in a sigmoidal shape as the\nnumber of samples increases. However, as p decreases, the\nMDP becomes harder: As shown on the right plot of Figure 4,\nthe number of samples required to reach the particular LTL-\nPAC probability of 0.9 grows exponentially. Results for other\nalgorithms, environments and LTL formulas are similar and\nlead to the same conclusion.\n5.3\nConclusion\nSince the transition probabilities (p in this case) are unknown\nin practice, one can’t know which curve in the left plot a given\nenvironment will follow. Therefore, given any ﬁnite number\nof samples, these reinforcement-learning algorithms cannot\nprovide guarantees on the LTL-PAC probability of the learned\npolicy. This result supports Theorem 1.\n6\nDirections Forward\nWe have established the intractability of reinforcement learn-\ning for inﬁnite-horizon LTL objectives. Speciﬁcally, for any\ninﬁnite-horizon LTL objective, the learned policy need not\nperform near-optimally given any ﬁnite number of environ-\nment interactions. This intractability is undesirable in ap-\nplications that require strong guarantees, such as trafﬁc con-\ntrol, robotics, and autonomous vehicles [Temizer et al., 2010;\nKober et al., 2013; Schwarting et al., 2018].\nGoing forward, we categorize approaches that either focus\non tractable objectives or weaken the guarantees required by\nan LTL-PAC algorithm. We obtain the ﬁrst category from the\nreverse direction of Theorem 1, and each of the other cat-\negories by relaxing a speciﬁc requirement that Theorem 1\nplaces on an algorithm.\nFurther, we classify previous ap-\nproaches into these categories.\n6.1\nUse a Finitary Objective\nResearchers have introduced speciﬁcation languages that\nexpress ﬁnitary properties and have applied reinforcement\nlearning to objectives expressed in these languages [Hen-\nriques et al., 2012; Jothimurugan et al., 2019]. One value\nproposition of these approaches is that they provide succinct\nspeciﬁcations because ﬁnitary properties written in LTL di-\nrectly are verbose. For example, the ﬁnitary property “a holds\nfor 100 steps” is equivalent to an LTL formula with a conjunc-\ntion of 100 terms: a ∧Xa ∧· · · ∧(X . . . X\n| {z }\n99 times\na).\nFor these succinct speciﬁcation languages, by the reduc-\ntion of these languages to ﬁnitary properties and the reverse\ndirection of Theorem 1, there exist reinforcement-learning al-\ngorithms that give LTL-PAC guarantees.\n6.2\nBest-effort Guarantee\nThe deﬁnition of LTL-PAC (Deﬁnition 3) requires a rein-\nforcement-learning algorithm to learn a policy with satisfac-\ntion probability within ϵ of optimal, for all ϵ > 0. However, it\nis possible to relax this quantiﬁcation over ϵ so that an algo-\nrithm only returns a policy with the best-available ϵ it ﬁnds.\nFor example, [Ashok et al., 2019] introduced a reinforce-\nment-learning algorithm for objectives in the Guarantee\nclass. Using a speciﬁed time budget, the algorithm returns\na policy and an ϵ. Notably, it is possible for the returned ϵ to\nbe 1, a vacuous bound on performance.\n6.3\nKnow More About the Environment\nThe deﬁnition of LTL-PAC (Deﬁnition 3) requires a rein-\nforcement-learning algorithm to provide a guarantee for all\nenvironments. However, on occasion, one can have prior in-\nformation on the transition probabilities of the MDP at hand.\nFor example, [Fu and Topcu, 2014] introduced a reinforce-\nment-learning algorithm with a PAC-MDP guarantee that de-\npends on the time horizon until the MDP reaches a steady\nstate. Given an MDP, this time horizon is generally unknown;\nhowever, if one has knowledge of this time horizon a priori, it\nconstrains the set of MDPs and yields an LTL-PAC guarantee\ndependent on this time horizon.\nAs another example, [Br´azdil et al., 2014] introduced a\nreinforcement-learning algorithm that provides an LTL-PAC\nguarantee provided a declaration of the minimum transition\nprobability of the MDP. This constraint, again, bounds the\nspace of considered MDPs.\n6.4\nUse an LTL-like Objective\nTheorem 1 only considers LTL objectives.\nHowever, one\nopportunity for obtaining a PAC guarantee is to change\nthe problem—use a speciﬁcation language that is LTL-like,\ndeﬁning similar temporal operators but also giving those op-\nerators a different, less demanding, semantics.\nLTL-in-the-limit Objectives\nOne line of work [Sadigh et al., 2014; Hahn et al., 2019;\nHasanbeig et al., 2019; Bozkurt et al., 2020] uses LTL for-\nmulas as the objective, but also introduces one or more hyper-\nparameters λ to relax the formula’s semantics.\nThe rein-\nforcement-learning algorithms in these works learn a policy\nfor the environment MDP given ﬁxed values of the hyper-\nparameters. Moreover, as hyper-parameter values approach a\nlimit point, the learned policy becomes optimal for the hyper-\nparameter-free LTL formula.2 The relationship between these\nrelaxed semantics and the original LTL semantics is anal-\nogous to the relationship between discounted and average-\nreward inﬁnite-horizon MDPs. Since discounted MDPs are\nPAC-MDP-learnable [Strehl et al., 2006], we conjecture that\nthese relaxed LTL objectives (at any ﬁxed hyper-parameter\nsetting) are PAC-learnable.\nGeneral LTL-like Objectives\nPrior approaches [Littman et al., 2017; Li et al., 2017;\nGiacomo et al., 2019; Camacho et al., 2019] also use gen-\neral LTL-like speciﬁcations that do not or are not known to\nconverge to LTL in a limit. For example, [Camacho et al.,\n2019] introduced the reward-machine objective that uses a ﬁ-\nnite state automaton to specify a reward function. As another\nexample, [Littman et al., 2017] introduced geometric LTL.\nGeometric LTL attaches a geometrically distributed horizon\nto each temporal operator. The learnability of these general\nLTL-like objectives is a potential future research direction.\n7\nConclusion\nIn this work, we have formally proved that inﬁnite-horizon\nLTL objectives in reinforcement learning cannot be learned in\nunrestricted environments. By inspecting the core result, we\nhave identiﬁed various possible directions forward for future\nresearch. Our work resolves the apparent lack of a formal\ntreatment of this fundamental limitation of inﬁnite-horizon\nobjectives, helps increase the community’s awareness of this\nproblem, and will help organize the community’s efforts in\nreinforcement learning with LTL objectives.\n2[Hahn et al., 2019] and [Bozkurt et al., 2020] showed that there\nexists a critical setting of the parameters λ∗that produces the opti-\nmal policy. However, λ∗depends on the transition probabilities of\nthe MDP and is therefore consistent with our ﬁndings.\nReferences\n[Alur et al., 2021] Rajeev Alur, Suguman Bansal, Osbert\nBastani, and Kishor Jothimurugan. A framework for trans-\nforming speciﬁcations in reinforcement learning.\narXiv\npreprint: 2111.00272, 2021.\n[Ashok et al., 2019] Pranav Ashok, Jan Kˇret´ınsk´y, and Max-\nimilian Weininger.\nPac statistical model checking for\nmarkov decision processes and stochastic games. In CAV,\n2019.\n[Bozkurt et al., 2020] Alper Bozkurt, Yu Wang, Michael Za-\nvlanos, and Miroslav Pajic. Control synthesis from linear\ntemporal logic speciﬁcations using model-free reinforce-\nment learning. In ICRA, 2020.\n[Br´azdil et al., 2014] Tom´aˇs Br´azdil, Krishnendu Chatterjee,\nMartin Chmel´ık, Vojtˇech Forejt, Jan Kˇret´ınsk´y, Marta\nKwiatkowska, David Parker, and Mateusz Ujma. Veriﬁ-\ncation of markov decision processes using learning algo-\nrithms. In ATVA, 2014.\n[Camacho et al., 2019] Alberto\nCamacho,\nRodrigo\nToro Icarte,\nToryn Q. Klassen,\nRichard Valenzano,\nand Sheila A. McIlraith.\nLtl and beyond: Formal lan-\nguages for reward function speciﬁcation in reinforcement\nlearning. In IJCAI, 2019.\n[Dann et al., 2019] Christoph Dann, Lihong Li, Wei Wei,\nand Emma Brunskill.\nPolicy certiﬁcates: Towards ac-\ncountable reinforcement learning. In ICML, 2019.\n[Dewey, 2014] Dan Dewey. Reinforcement learning and the\nreward engineering principle. In AAAI Spring Symposia,\n2014.\n[Fiechter, 1994] Claude-Nicolas Fiechter.\nEfﬁcient rein-\nforcement learning. In COLT, 1994.\n[Fu and Topcu, 2014] Jie Fu and Ufuk Topcu. Probably ap-\nproximately correct MDP learning and control with tem-\nporal logic constraints. In Robotics: Science and Systems\nX, 2014.\n[Giacomo et al., 2019] Giuseppe De Giacomo, L. Iocchi,\nMarco Favorito, and F. Patrizi. Foundations for restrain-\ning bolts: Reinforcement learning with ltlf/ldlf restraining\nspeciﬁcations. In ICAPS, 2019.\n[Grill et al., 2016] Jean-Bastien Grill, Michal Valko, and\nR. Munos.\nBlazing the trails before beating the path:\nSample-efﬁcient monte-carlo planning. In NIPS, 2016.\n[Hahn et al., 2019] Ernst Moritz Hahn, Mateo Perez, Sven\nSchewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik\nWojtczak. Omega-regular objectives in model-free rein-\nforcement learning. In TACAS, 2019.\n[Hasanbeig et al., 2019] M. Hasanbeig, Yiannis Kantaros,\nA. Abate, D. Kroening, George Pappas, and I. Lee. Re-\ninforcement learning for temporal logic control synthesis\nwith probabilistic satisfaction guarantees. In CDC, 2019.\n[Henriques et al., 2012] David Henriques, Jo˜ao G. Martins,\nPaolo Zuliani, Andr´e Platzer, and Edmund M. Clarke. Sta-\ntistical model checking for markov decision processes. In\nQEST, 2012.\n[Jiang et al., 2020] Yuqian Jiang, Sudarshanan Bharadwaj,\nBo Wu, Rishi Shah, Ufuk Topcu, and Peter Stone.\nTemporal-logic-based reward shaping for continuing\nlearning tasks. arXiv preprint: 2007.01498, 2020.\n[Jothimurugan et al., 2019] Kishor Jothimurugan, R. Alur,\nand Osbert Bastani. A composable speciﬁcation language\nfor reinforcement learning tasks. In NeurlPS, 2019.\n[Kakade, 2003] Sham M. Kakade. On the Sample Complex-\nity of Reinforcement Learning. PhD thesis, Gatsby Com-\nputational Neuroscience Unit, UCL, 2003.\n[Kearns and Singh, 2002] Michael\nKearns\nand\nSatinder\nSingh.\nNear-optimal reinforcement learning in polyno-\nmial time. Machine Learning, 49(2), 2002.\n[Kearns et al., 1999] Michael Kearns, Yishay Mansour, and\nAndrew Y. Ng. Approximate planning in large pomdps\nvia reusable trajectories. In NIPS, 1999.\n[Kober et al., 2013] Jens Kober, J. Bagnell, and Jan Peters.\nReinforcement learning in robotics: A survey. The Inter-\nnational Journal of Robotics Research, 32, 2013.\n[Li et al., 2017] Xiao Li, C. Vasile, and C. Belta. Reinforce-\nment learning with temporal logic rewards. IROS, 2017.\n[Littman et al., 2017] Michael L. Littman, Ufuk Topcu, Jie\nFu, Charles Isbell, Min Wen, and James MacGlashan.\nEnvironment-independent task speciﬁcations via gltl.\narXiv preprint: 1704.04341, 2017.\n[Manna and Pnueli, 1987] Zohar Manna and Amir Pnueli. A\nhierarchy of temporal properties. In PODC, 1987.\n[Pnueli, 1977] Amir Pnueli. The temporal logic of programs.\nIn FOCS, 1977.\n[Puterman, 1994] Martin L. Puterman.\nMarkov Decision\nProcesses—Discrete Stochastic Dynamic Programming.\nJohn Wiley & Sons, Inc., 1994.\n[Sadigh et al., 2014] Dorsa Sadigh, Eric S. Kim, Samuel\nCoogan, S. Shankar Sastry, and Sanjit A. Seshia. A learn-\ning based approach to control synthesis of markov decision\nprocesses for linear temporal logic speciﬁcations. In CDC,\n2014.\n[Schwarting et al., 2018] Wilko Schwarting, Javier Alonso-\nMora, and Daniela Rus. Planning and decision-making for\nautonomous vehicles. Annual Review of Control, Robotics,\nand Autonomous Systems, 1, 2018.\n[Strehl et al., 2006] Alexander\nStrehl,\nLihong\nLi,\nEric\nWiewiora, John Langford, and Michael Littman.\nPac\nmodel-free reinforcement learning. In ICML, 2006.\n[Sutton and Barto, 1998] Richard S. Sutton and Andrew G.\nBarto.\nReinforcement Learning: An Introduction.\nThe\nMIT Press, 1998.\n[Temizer et al., 2010] Selim Temizer, Mykel Kochender-\nfer, Leslie Kaelbling, Tomas Lozano-Perez, and James\nKuchar. Collision avoidance for unmanned aircraft using\nmarkov decision processes. In AIAA GNC, 2010.\n[Valiant, 1984] L. G. Valiant.\nA theory of the learnable.\nCommunications of the ACM, 27(11), 1984.\nAppendices\nA\nBackground on Linear Temporal Logic\nA.1\nLTL Semantics\nIn this section, we give the formal semantics of LTL. Recall\nthat the satisfaction relation w ⊨φ denotes that the inﬁnite\nword w satisﬁes φ. Equation (A.1) deﬁnes this relation.\nw ⊨a\niff a ∈w[0]\na ∈Π\nw ⊨¬φ\niff w ⊭φ\nw ⊨φ ∧ψ\niff w ⊨φ ∧w ⊨ψ\nw ⊨Xφ\niff w[1:] ⊨φ\nw ⊨φ U ψ\niff ∃j ≥0.\n\u0000w[j:] ⊨ψ ∧\n∀k ≥0. k < j ⇒w[k:] ⊨φ\n\u0001\n.\n(A.1)\nThe rest of the operators are deﬁned as syntactic sugar in\nterms of operators in Equation (A.1) as: φ∨ψ ≡¬(¬φ∧¬ψ),\nF φ ≡True U φ, G φ ≡¬F ¬φ.\nWe describe the semantics of each operator in words below.\n• Xφ: the sub-formula φ is true in the next time step.\n• G φ: the sub-formula φ is always true in all future time\nsteps.\n• F φ: the sub-formula φ is eventually true in some future\ntime steps.\n• φ U ψ: the sub-formula φ is always true until the sub-\nformula ψ eventually becomes true, after which φ is al-\nlowed to become false.\nA.2\nComplete Description of the LTL Hierarchy\nIn this section, we describe the key properties of all classes in\nthe LTL hierarchy (see Figure 1).\n• φ ∈Finitary iff there exists a horizon H such that inﬁnite-\nlength words sharing the same preﬁx of length H are either\nall accepted or all rejected by φ. E.g., a ∧Xa (i.e., a is true\nfor two steps) is in Finitary.\n• φ ∈Guarantee iff there exists a language of ﬁnite words L\n(i.e., a Boolean function on ﬁnite-length words) such that\nw ⊨φ if L accepts a preﬁx of w. Informally, a formula\nin Guarantee asserts that something eventually happens.\nE.g., F a (i.e., eventually a is true) is in Guarantee.\n• φ ∈Safety iff there exists a language of ﬁnite words L such\nthat w ⊨φ if L accepts all preﬁxes of w. Informally, a\nformula in Safety asserts that something always happens.\nE.g., G a (i.e., a is always true) is in Safety.\n• φ ∈Obligation iff φ is a logical combination of formulas\nin Guarantee and Safety. E.g., F a ∧G b is in Obligation.\n• φ ∈Persistence iff there exists a language of ﬁnite words\nL such that w ⊨φ if L accepts all but ﬁnitely many pre-\nﬁxes of w. Informally, a formula in Persistence asserts\nthat something happens ﬁnitely often. E.g., F G a (i.e., a is\nnot true for only ﬁnitely many times, and eventually a stays\ntrue forever) is in Persistence.\n• φ ∈Recurrence iff there exists a language of ﬁnite words\nL such that w ⊨φ if L accepts inﬁnitely many preﬁxes of\nw. Informally, a formula in Recurrence asserts that some-\nthing happens inﬁnitely often. E.g., G F a (i.e., a is true for\ninﬁnitely many times) is in Recurrence.\n• φ ∈Reactivity iff φ is a logical combination of formulas\nin Recurrence and Persistence. E.g., G F a ∧F G b is in\nReactivity.\nB\nProof of Lemma 2\nTo the end of proving Lemma 2, we ﬁrst observe the follow-\ning proposition:\nProposition B.1. For any non-Markovian policy π, the satis-\nfaction probabilities for M1 and M2 sum to one:\nV π\nM1,ξh0 + V π\nM2,ξh0 = 1.\nWe give a proof of Proposition B.1 in Appendix B.1.\nProof of Lemma 2. Note that the optimal satisfaction proba-\nbilities in both M1 and M2 is one, that is, V π∗\nMi,ξh0 = 1. This\nis because the policy that always chooses ai in Mi guarantees\nvisitation to the state h0. Therefore, a corollary of Proposi-\ntion B.1 is that for any policy π and any ϵ < 1\n2, the policy π\ncan only be ϵ-optimal in one of M1 and M2. Speciﬁcally,\nwe have:\n1\u0012\nV π\nM1,ξh0 ≥V π∗\nM1,ξh0 −ϵ\n\u0013 + 1\u0012\nV π\nM2,ξh0 ≥V π∗\nM2,ξh0 −ϵ\n\u0013 ≤1. (B.1)\nConsider a speciﬁc sequence of transitions T of length N\nsampled from either M1 or M2. If n(T) = 0, the probability\nof observing T in M1 equals to the probability of observing\nT in M2, that is:\nPT∼⟨M1,AS⟩N ( T = T | n(T) = 0 )\n= PT∼⟨M2,AS⟩N ( T = T | n(T) = 0 ).\nThis is because the only differences between M1 and M2\nare the transitions gl →h0 and gl →q0, and conditioning on\nn(T) = 0 effectively eliminates these differences.\nTherefore, we can write the sum of ζ1 and ζ2 as:\nζ1 + ζ2 =\nX\n∀T\nPT( T = T | n(T) = 0 )×\n(\n1\u0012\nV AL(T )\nM1,ξh0 ≥V π∗\nM1,ξh0 −ϵ\n\u0013 + 1\u0012\nV AL(T )\nM2,ξh0 ≥V π∗\nM2,ξh0 −ϵ\n\u0013\n)\n.\nPlugging in Equation (B.1), we get\nζ1 + ζ2 ≤1.\nThis then implies that min(ζ1, ζ2) ≤1\n2.\nB.1\nProof of Proposition B.1\nProof. We ﬁrst focus on M1. Consider an inﬁnite run τ =\n(s0, a0, s1, a1, . . . ) of the policy π on M1. Let τ[:i] denote\nthe partial history up to state si; let w denote all the states\narXiv:2111.12679v3  [cs.AI]  24 Jun 2022\n(s0, s1, . . . ) in τ. Let Ei denote the event that the visited\nstate at step i is either h0 or q0: w[i] ∈{h0, q0}. We have:\nV π\nM1,ξh0 = P(L (w) ⊨F h0)\n=\n∞\nX\ni=1\nP( L (w) ⊨F h0 | Ei ) · P(Ei)\nGiven that Ei happens, the previous state w[i −1] must be\ngl. Then, the probability of satisfying the formula given the\nevent Ei is the probability of the learned policy choosing a1\nfrom the state gl after observing the partial history τ[i −1]:\nV π\nM1,ξh0 =\n∞\nX\ni=1\nP( π (τ[:i −1]) = a1 | Ei ) · P(Ei). (B.2)\nSymmetrically for M2 we then have:\nV π\nM2,ξh0 =\n∞\nX\ni=1\nP( π (τ[:i −1]) = a2 | Ei ) · P(Ei). (B.3)\nFor any policy and any given partial history, the probability\nof choosing a1 or a2 must sum to 1, that is:\nP( π (τ[:i −1]) = a1 |Ei )+P( π (τ[:i −1]) = a2 |Ei ) = 1\nTherefore, we may add Equation (B.2) and Equation (B.3) to\nget:\nV π\nM1,ξh0 + V π\nM2,ξh0 =\n∞\nX\ni=1\n1 · P(Ei).\nFinally, since the event Ei must happen for some ﬁnitary i\nwith probability 1 (i.e., either h0 or q0 must be reached even-\ntually with probability 1), the expression on the right of the\nequation sums to 1.\nC\nComplete Proof of Lemma 3\nProof. First, consider M1. We will derive a lower bound\nfor N. We begin by asserting that the inequality of Equa-\ntion (3) holds true for a reinforcement-learning algorithm\nA = (AS, AL). That is:\nPT\n\u0010\nV AL(T )\nM1,ξh0 ≥V π∗\nM1,ξh0 −ϵ\n\u0011\n≥1 −δ.\nWe expand the left-hand side by conditioning on n(T) = 0:\nPT\n\u0010\nV AL(T )\nM1,ξh0 ≥V π∗\nM1,ξh0 −ϵ\n\f\f\f n (T) = 0\n\u0011\nPT(n (T) = 0)+\nPT\n\u0010\nV AL(T )\nM1,ξh0 ≥V π∗\nM1,ξh0 −ϵ\n\f\f\f n (T) > 0\n\u0011\n(1 −PT(n (T) = 0))\n≥1 −δ.\nSince PT\n\u0010\nV AL(T )\nM1,ξh0 ≥1 −ϵ\n\f\f\f n (T) > 0\n\u0011\n≤1, we may re-\nlax the inequality to:\n(1 −ζ1)PT(n (T) = 0) ≤δ,\nwhere we also plugged in our deﬁnition of ζi (see Lemma 2).\nThis relaxation optimistically assumes that a reinforcement-\nlearning algorithm can learn an ϵ-optimal policy by observing\nat least one transition to h0 or q0.\nSince there are at most N transitions initiating from the\nstate gl, and n(T) = 0 only occurs when all those transitions\nend up in gk, we have PT(n (T) = 0) ≥(1 −p)N. Incorpo-\nrating this into the inequality we have:\n(1 −ζ1) (1 −p)N ≤δ.\nSymmetrically, for M2 we have:\n(1 −ζ2) (1 −p)N ≤δ.\nSince both inequalities need to hold, we may combine them\nby using min to choose the tighter inequality:\n(1 −min (ζ1, ζ2)) (1 −p)N ≤δ.\nBy applying Lemma 2, we remove the inequality’s depen-\ndence on ζi, and get the desired lower bound of\nN ≥\nlog(2δ)\nlog (1 −p),\nwhich completes the proof of Lemma 3.\nD\nUncommittable Words for non-Finitary\nFormulas\nIn this section, we prove the following lemma:\nLemma D.1. Any LTL formula φ ̸∈Guarantee has an un-\ncommittable accepting word. Any LTL formula φ ̸∈Safety\nhas an uncommittable rejecting word.\nD.1\nPreliminaries\nWe will review some preliminaries to prepare for our proof of\nLemma D.1.\nWe will use an automaton-based argument for our proof of\nLemma D.1. To that end, we recall the following deﬁnitions\nfor automatons.\nDeterministic Finite Automaton.\nA deterministic ﬁnite\nautomaton (DFA) is a tuple (S, A, P, s0, sacc),\nwhere\n(S, A, P, s0) is a deterministic MDP (i.e., P degenerated to\na deterministic function (S × A) →S), and sacc ∈S is an\naccepting state.\nDeterministic Rabin Automaton.\nA deterministic Rabin\nautomaton (DRA) is a tuple (S, Π, T, s0, Acc), where\n• S is a ﬁnite set of states.\n• Π is the atomic propositions of φ.\n• T is a transition function (S × 2Π) →S.\n• s0 ∈S is an initial state.\n• Acc is a set of pairs of subsets of states (Bi, Gi) ∈(2S)2.\nAn inﬁnite-length word w over the atomic propositions Π\nis accepted by the DRA, if there exists a run of the DRA such\nthat there exists a (Bi, Gi) ∈Acc where the run visits all\nstates in Bi ﬁnitely many times and visits some state(s) in Gi\ninﬁnitely many times.\nFor any LTL formula φ, one can always construct an equiv-\nalent DRA that accepts the same set of inﬁnite-length words\nas φ [Safra, 1988].\nD.2\nProof of Lemma D.1 for φ ̸∈Guarantee\nGiven an LTL formula φ, we ﬁrst construct its equivalent\nDRA R = (S, Π, T, s0, Acc) [Safra, 1988].\nA path in a DRA is a sequence of transitions in the DRA. A\ncycle in a DRA is a path that starts from some state and then\nreturns to that state. A cycle is accepting if there exists a pair\n(Bi, Gi) ∈Acc, such that the cycle does not visit states in Bi\nand visits some states in Gi. Conversely, a cycle is rejecting\nif it is not accepting. With the above deﬁnitions and to the\nend of proving Lemma D.1 for the case of φ ̸∈Guarantee,\nwe state and prove the following lemma.\nLemma D.2. For any LTL formula φ ̸∈Guarantee and its\nequivalent DRA R, it must be the case that R contains an\naccepting cycle that is reachable from the initial state and\nthere exists a path from a state in the accepting cycle to a\nrejecting cycle.\nProof. Suppose, for the sake of contradiction, there does not\nexist an accepting cycle that 1. is reachable from the initial\nstate and 2. has a path to a rejecting cycle in the equivalent\nDRA R. Then there are two scenarios:\n• R does not have any accepting cycle that is reachable from\nthe initial state.\n• All accepting cycles reachable from the initial state do not\nhave any path to any rejecting cycle.\nFor the ﬁrst scenario, R must not accept any inﬁnite-length\nword. Therefore φ must be equal to F (i.e., the constant fal-\nsum). However, F is in the Finitary LTL class, which is a\nsubset of Guarantee, so this is a contradiction.\nFor the second scenario, consider any inﬁnite-length\nword w.\nConsider the induced inﬁnite path P\n=\n(s0, w[0], s1, w[1], . . . ) by w on the DRA starting from the\ninitial state s0.\nIf φ accepts the word w, the path P must reach some state\nin some accepting cycle.\nConversely, if φ rejects the word w, the path must not visit\nany state in any accepting cycle. This is because otherwise\nthe path can no longer visit a rejecting cycle once it visits the\naccepting cycle, thereby causing the word to be accepted.\nTherefore, φ accepts the word w as soon as the path P vis-\nits some state in some accepting cycle. This degenerates the\nDRA to a DFA, where the accepting states are all the states\nin the accepting cycles of the DRA. Then, an inﬁnite-length\nword w is accepted by φ if and only if there exists a preﬁx of\nw that is accepted by the DFA.\nBy the property of the Guarantee class (see Appendix A),\nfor φ ∈Guarantee, there exists a language of ﬁnite-length\nwords, L, such that w ⊨φ if L accepts a preﬁx of w [Manna\nand Pnueli, 1987]. Since a DFA recognizes a regular lan-\nguage, the formula must be in the Guarantee LTL class. This\nis also a contradiction.\nTherefore, there must exist an accepting cycle that is reach-\nable from the initial state and has a path to a rejecting cy-\ncle in the equivalent DRA. This completes the proof of\nLemma D.2\nWe are now ready to give a construction of wa, wb, wc\nand wd that directly proves Lemma D.1 for φ ̸∈Guarantee.\nConsider the equivalent DRA R of the LTL formula.\nBy\nLemma D.2, R must contain an accepting cycle that is reach-\nable from the initial state and has a path to a rejecting cycle.\nWe can thus deﬁne the following paths and cycles:\n• Let Pa be the path from the initial state to the accepting\ncycle.\n• Let Pb be the accepting cycle.\n• Let Pc be a path from the last state in the accepting cycle\nto the rejecting cycle.\n• Let Pd be the rejecting cycle.\nFor a path P = (si, w[i], . . . sj, w[j], sj+1), let w(P) denote\nthe ﬁnite-length word consisting only of the characters in be-\ntween every other state (i.e., each character is a tuple of truth\nvalues of the atomic propositions): w(P) = w[i] . . . w[j].\nConsider the assignments of wa = w(Pa), wb = w(Pb),\nwc = w(Pc) and wd = w(Pd). Notice that:\n• The formula φ accepts the inﬁnite-length word [wa; w∞\nb ]\nbecause P b is an accepting cycle.\n• The\nformula\nφ\nrejects\nall\ninﬁnite-length\nwords\n[wa; wi\nb; wc; w∞\nd ] for all i\n∈\nN because P d is a re-\njecting cycle.\nBy Deﬁnition 6, the inﬁnite-length word [wa; w∞\nb ] is an\nuncommittable accepting word.\nThis construction proves\nLemma D.1 for φ ̸∈Guarantee.\nD.3\nProof of Lemma D.1 for φ ̸∈Safety\nThe proof for φ ̸∈Safety is symmetrical to φ ̸∈Guarantee.\nFor completeness, we give the proof below.\nGiven an LTL formula φ, we again ﬁrst construct its equiv-\nalent DRA R = (S, Π, T, s0, Acc).\nTo the end of proving Lemma D.1 for the case of\nφ ̸∈Safety, we state and prove the following lemma.\nLemma D.3. For any LTL formula φ ̸∈Safety and its equiv-\nalent DRA R, it must be the case that R contains a rejecting\ncycle that is reachable from the initial state and has a path\nfrom any state in the rejecting cycle to an accepting cycle.\nProof. Suppose, for the sake of contradiction, there does not\nexist a rejecting cycle that 1. is reachable from the initial state\nand 2. has a path to an accepting cycle in the equivalent DRA\nR. Then there are two scenarios:\n• R does not have any rejecting cycle that is reachable from\nthe initial state.\n• All rejecting cycles reachable from the initial state do not\nhave any path to any accepting cycle.\nFor the ﬁrst scenario, R must not reject any inﬁnite-length\nword.\nTherefore φ must be equal to T (i.e., the constant\ntruth). However, T is in the Finitary LTL class, which is\na subset of Safety, so this is a contradiction.\nFor the second scenario, consider any inﬁnite-length\nword w.\nConsider the induced inﬁnite path P\n=\n(s0, w[0], s1, w[1], . . . ) by w on the DRA starting from the\ninitial state s0.\nIf φ rejects the word w, the path P must reach some state\nin some rejecting cycle.\nConversely, if φ accepts w, the path must not visit any state\nin any rejecting cycle. This is because otherwise the path can\nno longer visit a accepting cycle once it visits the rejecting\ncycle, thereby causing the word to be rejected.\nTherefore, φ rejects the word w as soon as the path P visits\nsome state in some rejecting cycle. We can again construct a\nDFA based on the DRA by letting the accepting states be all\nthe states except those in a rejecting cycle of the DRA. By\nthis construction, φ accepts an inﬁnite-length word w if and\nonly if the DFA accepts all ﬁnite-length preﬁxes of w.\nBy the property of the Safety class (see Appendix A), for\nφ ∈Safety, there exists a language of ﬁnite-length words, L,\nsuch that w ⊨φ if L accepts all preﬁxes of w [Manna and\nPnueli, 1987]. Since a DFA recognizes a regular language,\nthe formula must be in the Safety LTL class. This is also a\ncontradiction.\nTherefore, there must exist a rejecting cycle that is reach-\nable from the initial state and has a path to an accepting\ncycle in the equivalent DRA. This completes the proof of\nLemma D.3\nWe are now ready to give a construction of wa, wb, wc and\nwd that directly proves Lemma D.1 for φ ̸∈Safety. Consider\nthe equivalent DRA R of the LTL formula. By Lemma D.3,\nR must contain a rejecting cycle that is reachable from the\ninitial state and has a path to an accepting cycle. We can thus\ndeﬁne the following paths and cycles:\n• Let Pa be the path from the initial state to the rejecting\ncycle.\n• Let Pb be the rejecting cycle.\n• Let Pc be a path from the last state in the rejecting cycle to\nthe accepting cycle.\n• Let Pd be the accepting cycle.\nConsider the assignments of wa = w(Pa), wb = w(Pb),\nwc = w(Pc) and wd = w(Pd). Notice that:\n• The formula φ rejects the inﬁnite-length word [wa; w∞\nb ] be-\ncause P b is a rejecting cycle.\n• The\nformula\nφ\naccepts\nall\ninﬁnite-length\nwords\n[wa; wi\nb; wc; w∞\nd ] for all i ∈N because P d is an ac-\ncepting cycle.\nBy Deﬁnition 6, the inﬁnite-length word [wa; w∞\nb ] is an\nuncommittable rejecting word.\nThis construction proves\nLemma D.1 for φ ̸∈Safety.\nE\nProof of Theorem 1: the Reverse Direction\nIn this section, we give a proof to the reverse direction of\nTheorem 1.\nE.1\nProof\nReduction\nto\nInﬁnite-horizon\nCumulative\nRewards.\nGiven an LTL formula φ in Finitary with atomic propo-\nsitions Π,\none can compile φ into a DFA\n¯\nM\n=\n( ¯S, 2Π, ¯P, ¯s0, ¯\nsacc) that decides the satisfaction of φ [Latvala,\n2003]. In particular, for a given sample path w of DTMC in-\nduced by a policy and the environment MDP, L(w) satisﬁes\nφ if and only if the DFA, upon consuming L(w), eventually\nreaches the accept state sacc. Here, the DFA has a size (in\nthe worst case) doubly exponential to the size of the formula:\n| ¯S| = O(2exp(|φ|)) [Kupferman and Vardi, 1999].\nWe then use the following product construction to form\nan augmented MDP with rewards\nˆ\nM = ( ˆS, ˆA, ˆP, ˆs0, ˆR).\nSpeciﬁcally,\n• The states and actions are: ˆS = S × ¯S and ˆA = A.\n• The transitions follow the transitions in the environment\nMDP and the DFA simultaneously, where the action input\nof the DFA come from labeling the current state of the en-\nvironment MDP. In particular, the transitions in the aug-\nmented MDP follows the equations: ˆP((s, ¯s), a, (s′, ¯s′)) =\nP(s, a, s′) and ¯s′ = ¯P(¯s, L(s)).\n• The reward function assigns a reward of one to any transi-\ntion from a non-accepting state that reaches sacc in the DFA,\nand zero otherwise: R((s, ˆs), a, (s′, ˆs′)) = 1s̸=sacc∧ˆs′=sacc.\nBy construction, each run of the augmented MDP gives a\nreward of 1 iff the run satisﬁes the ﬁnitary formula φ. The\nexpected (undiscounted) inﬁnite-horizon cumulative rewards\nthus equals the satisfaction probability of the formula. There-\nfore, maximizing the inﬁnite-horizon cumulative rewards in\nthe augmented MDP is equivalent to maximizing the satis-\nfaction probability of φ in the environment MDP.\nReduction to Finite-horizon Cumulative Rewards.\nBy\nthe property of LTL hierarchy [Manna and Pnueli, 1987], for\nany LTL formula φ in Finitary and an inﬁnite-length word\nw, one can decide if φ accepts w by inspecting a length-H\npreﬁx of w. Here, H is a constant that is computable from\nφ. In particular, H equals the longest distance from the start\nstate to a terminal state in our constructed DFA. 1 Thus, since\nthe product construction above does not assign any reward af-\nter the horizon H, the inﬁnite-horizon cumulative rewards is\nfurther equivalent to the ﬁnite-horizon (of length H) cumu-\nlative rewards. Therefore, ﬁnding the optimal policy for φ is\nequivalent to ﬁnding the optimal policy that maximizes the\ncumulative rewards for a ﬁnite horizon H in the augmented\nMDP.\nSample Complexity Upper Bound.\nLastly, [Dann et al.,\n2019] gave a reinforcement-learning algorithm for ﬁnite-\nhorizon cumulative rewards called ORLC (optimistic rein-\nforcement learning with certiﬁcates). The ORLC algorithm\nis sample efﬁciently PAC 2 and has a sample complexity\nof ˜O\n\u0010\u0010\n|S||A|H3\nϵ2\n+ |S|2|A|H4\nϵ\n\u0011\nlog 1\nδ\n\u0011\n.\n3 Incorporating the\n1Note that since φ is ﬁnitary, the DFA does not have any cycles\nexcept at the terminal states [Duret-Lutz et al., 2016].\n2The ORLC algorithm provides a guarantee called individual\npolicy certiﬁcates (IPOC) bound. [Dann et al., 2019] showed that\nthis guarantee implies our PAC deﬁnition, which they called a\nsupervised-learning style PAC bound. Therefore, the ORLC algo-\nrithm is a PAC reinforcement-learning algorithm for ﬁnite-horizon\ncumulative rewards by our deﬁnition.\n3The notation\n˜O(.) is the same as O(.), but ignores any\nlog-terms.\nThe bound given by [Dann et al.,\n2019] is\n˜O\n\u0010\n|S|2|A||H2\nϵ2\nlog 1\nδ\n\u0011\n.\nIt is a upper bound on the number of\nepisodes.\nTo make the bound consistent with our lower bound,\ng\nh\nq\na1, p\na2, p\na1, 1 −p\na2, 1 −p\nFigure F.1: One of the two environment MDPs used in the experi-\nments.\nfact that the augmented MDP has | ˆS| = |S| · O(2exp(|φ|))\nnumber of states, we obtain a sample complexity upper\nbound of ˜O\n\u0010\u0010\n|S| 2exp |φ||A|H3\nϵ2\n+ |S|2(2exp |φ|)2|A|H4\nϵ\n\u0011\nlog 1\nδ\n\u0011\nfor the overall reinforcement-learning algorithm.\nSince for any ﬁnitary formula, we have constructed a rein-\nforcement-learning algorithm that is sample efﬁciently LTL-\nPAC for all environment MDPs, this concludes our proof that\nany ﬁnitary formula is LTL-PAC-learnable.\nF\nEmpirical Justiﬁcations\nThis section empirically demonstrates our main result, the\nforward direction of Theorem 1.\nPrevious work has introduced various reinforcement-learn-\ning algorithms for LTL objectives [Sadigh et al., 2014;\nHahn et al., 2019; Hasanbeig et al., 2019; Bozkurt et al.,\n2020]. We therefore ask the research question: Do the sample\ncomplexities for reinforcement-learning algorithms for LTL\nobjectives introduced by previous work depend on the transi-\ntion probabilities of the environment?\nTo answer the above question, we consider a set of rein-\nforcement-learning algorithms for LTL objectives and empir-\nically measure the sample size for each algorithm to obtain a\nnear-optimal policy with high probability.\nF.1\nMethodology\nReinforcement-learning algorithms.\nWe consider a set of\nrecent reinforcement-learning algorithms for LTL objectives\n[Hahn et al., 2019; Bozkurt et al., 2020], about which we\ngive more details in Appendix G. These algorithms are all\nimplemented in the Mungojerrie toolbox [Hahn et al., 2021].\nObjectives and Environment MDPs.\nWe consider two\npairs of LTL formulas and environment MDPs (LTL-MDP\npair). The ﬁrst pair is the formula F h and the counterex-\nample MDP constructed according to Section 4.1, shown in\nFigure F.1. The second pair is the formula F goal and a grid-\nworld environment MDP from the case study by [Sadigh et\nal., 2014] with a customized transition dynamics, shown in\nFigure F.2.\nExperiment Methodology.\nWe ran the considered algo-\nrithms on each chosen LTL-MDP pair with a range of values\nfor the parameter p and let the algorithms perform N envi-\nronment samples.\nFor each algorithm and each pair of values of p and N,\nwe ﬁx ϵ = 0.1 and repeatedly run the algorithm to obtain a\nMonte Carlo estimation of the LTL-PAC probability (left side\nwhich is a bound on the number of sampled transitions, we multiply\nit by an additional H term.\nFigure F.2: Gridworld environment MDP from [Sadigh et al., 2014]\nwith a customized transition dynamics. The agent starts from the\nlower left corner. At each time step, the agent can choose to move\nup, down, left or right. The white cells are sticky: the agent moves\ntowards the intended direction with probability 1 −p (or stays sta-\ntionary if it will move off the grid), and stays stationary with prob-\nability p. The red cells are trapping: once the agent steps on a red\ncell, it stays there forever.\nof Equation (3)) for that setting of p, N and ϵ. We repeat each\nsetting until the estimated standard deviation of the estimated\nprobability is within 0.01. In the end, for each algorithm and\nLTL-MDP pair we obtain 5 × 21 = 105 LTL-PAC probabili-\nties and their estimated standard deviations.\nFor the ﬁrst LTL-MDP pair, we vary p by a geometric pro-\ngression from 10−1 to 10−3 in 5 steps: p(i) = 10−i+1\n2 for\n1 ≤i ≤5. We vary N by a geometric progression from 101\nto 105 in 21 steps.\nFor the second LTL-MDP pair, we vary p by a geometric\nprogression from 0.9 to 0.6 in 5 steps: p(i) = 0.9 × 0.903−i\nfor 1 ≤i ≤5. We vary N by a geometric progression from\n3540 to 9×104 in 21 steps; if an algorithm does not converge\nto the desired LTL-PAC probability within 9 × 104 steps, we\nrerun the experiment with an extended range of N from 3540\nto 1.5 × 105.\nF.2\nResults\nFigure 4 presents the results for the algorithm in [Bozkurt et\nal., 2020] with the setting of Multi-discount, Q-learning, and\nthe ﬁrst LTL-MDP pair.\nOn the left, we plot the LTL-PAC probabilities vs. the num-\nber of samples N, one curve for each p. On the right, we plot\nthe intersections of the curves in the left plot with a horizontal\ncutoff of 0.9.\nAs we see from the left plot of Figure 4, for each p, the\ncurve starts at 0 and grows to 1 in a sigmoidal shape as the\nnumber of samples increases. However, as p decreases, the\nMDP becomes harder: As shown on the right plot of Figure 4,\nthe number of samples required to reach the particular LTL-\nPAC probability of 0.9 grows exponentially.\nFigure F.3 presents the complete results for all settings for\nthe ﬁrst LTL-MDP pair, and Figure F.4 present the complete\nresults for all settings for the second LTL-MDP pair. These\nresults are similar and lead to the same analysis as above.\nF.3\nConclusion\nSince the transition probabilities (p in this case) are unknown\nin practice, one can’t know which curve in the left plot a given\nenvironment will follow. Therefore, given any ﬁnite num-\nber of samples, these reinforcement-algorithms cannot pro-\nvide guarantees on the LTL-PAC probability of the learned\npolicy. This result supports Theorem 1.\nG\nEmpirical Experiment Details\nG.1\nDetails of Methodology\nChosen\nAlgorithms.\nWe\nconsider\na\nset\nof\nrecent\nreinforcement-learning\nalgorithms\nfor\nLTL\nobjectives\nimplemented in the Mongujerrie toolbox [Hahn et al., 2021].\nA common pattern in these previous works [Sadigh et al.,\n2014; Hahn et al., 2019; Bozkurt et al., 2020] is that each\nwork constructs a product MDP with rewards (i.e., an MDP\nwith a reward function on that MDP) from an LTL formula\nand an environment MDP. Moreover, these works permit the\nuse of any standard reinforcement-learning algorithm, such\nas Q-learning or SARSA(λ), to solve the constructed product\nMDP with the speciﬁed reward function to obtain the product\nMDP’s optimal policy. Finally, these works cast the optimal\npolicy back to a non-Markovian policy of the environment\nMDP, which becomes the algorithm’s output policy.\nFollowing [Hahn et al., 2021], we call each speciﬁc con-\nstruction of a product MDP with rewards as a reward-scheme.\nWe then characterize each reinforcement-learning algorithm\nas a “reward-scheme” and “learning-algorithm” pair.\nWe\nconsider a total of ﬁve reward-schemes 4: Reward-on-acc\n[Sadigh et al., 2014], Multi-discount [Bozkurt et al., 2020],\nZeta-reach [Hahn et al., 2019], Zeta-acc [Hahn et al., 2020],\nand Zeta-discount [Hahn et al., 2020]. We consider a total of\nthree learning-algorithms: Q-learning [Watkins and Dayan,\n1992], Double Q-learning [Hasselt, 2010], and SARSA(λ)\n[Sutton, 1988].\nThis yields a total of 15 reinforcement-\nlearning algorithms for LTL objectives.\nAlgorithm Parameters.\nEach reinforcement-learning al-\ngorithm in Mungojerrie accepts a set of hyper-parameters.\nFor the majority of the hyper-parameters, we use their de-\nfault values as in Mungojerrie Version 1.0 [Hahn et al., 2021].\nWe present the hyper-parameters that differ from the default\nvalues in Table G.1. For each of the hyper-parameters in Ta-\nble G.1, we use a different value from the default value be-\ncause it allow all the algorithms that we consider to converge\nwithin 105 steps (i.e., the maximum learning steps that we\nallow). For SARSA(λ), we use λ = 0.\nSoftware and Platform.\nWe use a custom version of\nMungojerrie. Our modiﬁcations are:\n• Modiﬁcation to allow parallel Monte Carlo estimation of\nthe LTL-PAC probability.\n• Modiﬁcation to allow the reinforcement-learning algo-\nrithms to have a non-linear learning rate decay. In par-\nticular, we use a learning rate of\nk\nk+t at every learning\nstep t, where k is a hyper-parameter (see Table G.1 for the\nvalue of k for each algorithm). This modiﬁcation is nec-\nessary for ensuring Q-learning’s convergence [Watkins and\nDayan, 1992].\n4We use the same naming of each reward-scheme as in the\nMungojerrie toolbox [Hahn et al., 2021]\nWe run all experiments on a machine with 2.9 GHz 6-Core\nCPU and 32 GB of RAM.\nH\nClassiﬁcation of Prior Works\nIn Section 6, we discussed several categories of approaches\nthat either focus on tractable objectives or weaken the guar-\nantees required by an LTL-PAC algorithm. We classiﬁed vari-\nous previous approaches into these categories. In this section,\nwe explain the rationale for each classiﬁcation.\nH.1\nUse a Finitary Objective\n[Henriques et al., 2012] introduced a variant of LTL called\nBounded LTL and used Bounded LTL objective for reinforce-\nment learning. Every Bounded LTL formula is decidable by\na bounded length preﬁx of the input word. Moreover, each\nBounded LTL formula is equivalent to an ﬁnitary LTL for-\nmula. Therefore, we classiﬁed this approach as using a Fini-\ntary objective.\n[Jothimurugan et al., 2019] introduced a task speciﬁcation\nlanguage over ﬁnite-length words. Further, their deﬁnition of\nan MDP contains an additional ﬁnite time horizon H. Each\nsample path of the MDP is then a length-H ﬁnite-length word\nand is evaluated by a formula of the task speciﬁcation lan-\nguage.5 Each formula of the task speciﬁcation language with\na ﬁxed ﬁnite horizon H is equivalent to an LTL formula in\nthe Finitary class. Therefore, we classiﬁed this approach as\nusing a Finitary objective.\nH.2\nBest-Effort Guarantee\nWe classiﬁed [Ashok et al., 2019] to this category and ex-\nplained our rationale of this classiﬁcation in Section 6.\nH.3\nKnow More About the Environment\nWe classiﬁed [Fu and Topcu, 2014; Br´azdil et al., 2014] to\nthis category and explained our rationale of this classiﬁcation\nin Section 6.\nH.4\nUse an LTL-like Objective\nLTL-in-the-limit Objectives\nWe classiﬁed [Sadigh et al., 2014; Hahn et al., 2019; Hasan-\nbeig et al., 2019; Bozkurt et al., 2020] as using LTL-like ob-\njectives, and explained our rationale of these classiﬁcations in\nSection 6.\nGeneral LTL-like Objectives\n[Littman et al., 2017] introduced a discounted variant of\nLTL called Geometric LTL (GLTL). A temporal operator in\na GLTL formula expires within a time window whose length\nfollows a geometric distribution. For example, a GLTL for-\nmula F 0.1goal is satisﬁed if the sample path reaches the goal\n5There are two possible interpretations of the ﬁnite horizon in\n[Jothimurugan et al., 2019]. The ﬁrst interpretation is that the en-\nvironment MDP inherently terminates and produces length-H sam-\nple paths. The second interpretation is that the ﬁnite horizon H is\npart of the speciﬁcation given by a user of their approach. We used\nthe second interpretation to classify their approach. The difference\nbetween the two interpretations is only conceptual — if the environ-\nment inherently terminates with a ﬁxed ﬁnite horizon H, it would be\nequivalent to imposing a ﬁnite horizon H in the task speciﬁcation.\nReinforcement-learning-algorithm\nLearning Rate\nExploration\nReset Episode Every Steps\nQ-learning\n10\n10+t\nLinear decay from 1.0 to 10−1\n10\nDouble Q-learning\n30\n30+t\nLinear decay from 1.0 to 10−1\n10\nSARSA(λ)\n10\n10+t\nLinear decay from 1.0 to 10−3\n10\nTable G.1: Non-default hyper-parameters used for each learning-algorithm\nwithin a time horizon H, where H follows Geometric(0.1),\nthe geometric distribution with the success parameter 0.1.\nSince GLTL’s semantics is different from LTL’s semantics,\nwe classiﬁed this approach as using an LTL-like objective.\n[Li et al., 2017] introduced a variant of LTL called\nTruncated-LTL (TLTL). A formula in TLTL, similar to a for-\nmula in Bounded LTL [Henriques et al., 2012], is decidable\nby a bounded length preﬁx of the input word.\nMoreover,\nTLTL has a qualitative semantics, in addition to the standard\nBoolean semantics of LTL. In particular, the qualitative se-\nmantics of a TLTL formula maps a sample path of the envi-\nronment MDP to a real number that indicates the degree of\nsatisfaction for the TLTL formula. Therefore, we classiﬁed\nthis approach as using an LTL-like objective.\n[Giacomo et al., 2019] introduced Restraining Bolts. A\nRestraining Bolts speciﬁcation is a set of pairs (φi, ri), where\neach φi is an LTLf/LDLf formula, and ri is a scalar reward.\nAn LTLf formula is visuaully similar to an LTL formula;\nhowever, it is interpreted over ﬁnite-length words instead of\ninﬁnite-length words. LDLf is an extension of LTLf and is\nalso interpreted over ﬁnite-length words.\n6 Given an envi-\nronment MDP, the approach checks each ﬁnite length preﬁx\nof a sample path of the MDP against each φi, and if a pre-\nﬁx satisﬁes φi, the approach gives the corresponding reward\nri to the agent. The objective in [Giacomo et al., 2019] is to\nmaximize the discounted cumulative rewards produced by the\nRestraining Bolts speciﬁcation. To the best of our knowledge,\nthis objective is not equivalent to maximizing the satisfaction\nof an LTL formula. Nonetheless, a Restraining Bolts speciﬁ-\ncation can be seen as an LTL-like speciﬁcation for its use of\ntemporal operators. Therefore, we classiﬁed this approach as\nusing an LTL-like objective.\n[Camacho et al., 2019] introduced reward machine. A re-\nward machine speciﬁcation is a deterministic ﬁnite automaton\nequipped with a reward for each transition. The objective in\n[Camacho et al., 2019] is to maximize the discounted cumu-\nlative rewards produced by the reward machine speciﬁcation.\n[Camacho et al., 2019] showed that LTL objectives formulas\nin the Guarantee or Safety class are reducible to reward ma-\nchine objectives without discount factors. However, since the\napproach maximizes discounted cumulative rewards in prac-\ntice, it does not directly optimize for the LTL objectives in\nthe Guarantee or Safety classes.7 Therefore, we classiﬁed\n6LDLf is more expressive than LTLf [De Giacomo and Vardi,\n2013]. In particular, LTLf is equally expressive as the star-free sub-\nset of regular languages while LDLf is equally expressive as the full\nset of regular languages.\n7By their reduction, as the discount factor approach 1 in the limit,\nthe learned policy for the reward machine becomes the optimal pol-\nthis approach as using an LTL-like objective.\nI\nConcurrent Work\nConcurrent to this work, [Alur et al., 2021] developed\na framework to study reductions between reinforcement-\nlearning task speciﬁcations. They looked at various task spec-\niﬁcations, including cumulative discounted rewards, inﬁnite-\nhorizon average-rewards, reachability, safety, and LTL. They\nthoroughly review previous work concerning reinforcement\nlearning for LTL objectives, which we also cite. Moreover,\n[Alur et al., 2021, Theorem 8] states a seemingly similar re-\nsult as the forward direction of our Theorem 1:\nThere does not exist a PAC-MDP algorithm for the\nclass of safety speciﬁcations.\nDespite the parallels, we clarify one crucial difference and\ntwo nuances between our work and theirs.\nFirstly and most importantly, their theorem is equivalent to\n“there exists a safety speciﬁcation that is not PAC-learnable.”,\nwhereas our Theorem 1 works pointwise for each LTL for-\nmula, asserting “all non-ﬁnitary speciﬁcations are not PAC-\nlearnable.” The proof of their theorem gives one safety spec-\niﬁcation and shows that it is not PAC-learnable.8\nOn the\nother hand, the proof of the forward direction of our Theo-\nrem 1 constructs a counterexample for each non-ﬁnitary for-\nmula. This point is crucial since it allows us to precisely\ncarve out the PAC-learnable subset, namely the ﬁnitary for-\nmulas, from the LTL hierarchy. Secondly, their notion of\nsample complexity is slightly different from ours. In par-\nticular, they formulated a reinforcement learning algorithm\nas an iterative algorithm. At each step, the iterative algo-\nrithm outputs a policy πn.\nThen, their notion of sample\ncomplexity is the total number of non-ϵ-optimal policies pro-\nduced during an inﬁnitely long run of the learning algorithm:\n\f\f\f{n | V πn\nM,ξ < V π∗\nM,ξ −ϵ}\n\f\f\f . On the other hand, our notion of\nsample complexity is the number of samples required un-\ntil the learning algorithm outputs ϵ-optimal policies. How-\never, this difference is orthogonal to the core issue caused by\ninﬁnite-horizon LTL formulas. In particular, we can adapt our\ntheorem and proof to use their notion of sample complexity.\nicy for given guarantee or safety LTL objective. Therefore, [Ca-\nmacho et al., 2019] can also be classiﬁed as using an LTL-in-the-\nlimit objectives (for the subset of guarantee and safety objectives.\nNonetheless, we classiﬁed this approach to the general LTL-like ob-\njectives category because reward machine objectives are more gen-\neral than LTL objectives.\n8Their result is similar to what we showed in our Section 4.1,\nwhere we consider the particular guarantee formula F h0 and show\nthat it is not PAC-learnable.\nThirdly, their deﬁnition of safety speciﬁcation is equivalent\nto a strict subset of the safety class in the LTL hierarchy that\nwe consider. In particular, their safety speciﬁcation is equiv-\nalent to LTL formulas of the form G (a1 ∨a2 ∨· · · ∨an),\nwhere each ai ∈Π is an atomic proposition, with n = 0\ndegenerating the speciﬁcation to T (the constant true).\nLastly, they consider only reinforcement-learning algo-\nrithms, whereas we consider the slightly more general plan-\nning-with-generative-model algorithms. We believe their the-\norem and proof can be modiﬁed to accommodate our more\ngeneral algorithm deﬁnition.\nReferences\n[Alur et al., 2021] Rajeev Alur, Suguman Bansal, Osbert\nBastani, and Kishor Jothimurugan. A framework for trans-\nforming speciﬁcations in reinforcement learning.\narXiv\npreprint: 2111.00272, 2021.\n[Ashok et al., 2019] Pranav Ashok, Jan Kˇret´ınsk´y, and Max-\nimilian Weininger.\nPac statistical model checking for\nmarkov decision processes and stochastic games. In CAV,\n2019.\n[Bozkurt et al., 2020] Alper Bozkurt, Yu Wang, Michael Za-\nvlanos, and Miroslav Pajic. Control synthesis from linear\ntemporal logic speciﬁcations using model-free reinforce-\nment learning. In ICRA, 2020.\n[Br´azdil et al., 2014] Tom´aˇs Br´azdil, Krishnendu Chatterjee,\nMartin Chmel´ık, Vojtˇech Forejt, Jan Kˇret´ınsk´y, Marta\nKwiatkowska, David Parker, and Mateusz Ujma. Veriﬁ-\ncation of markov decision processes using learning algo-\nrithms. In ATVA, 2014.\n[Camacho et al., 2019] Alberto\nCamacho,\nRodrigo\nToro Icarte,\nToryn Q. Klassen,\nRichard Valenzano,\nand Sheila A. McIlraith.\nLtl and beyond: Formal lan-\nguages for reward function speciﬁcation in reinforcement\nlearning. In IJCAI, 2019.\n[Dann et al., 2019] Christoph Dann, Lihong Li, Wei Wei,\nand Emma Brunskill.\nPolicy certiﬁcates: Towards ac-\ncountable reinforcement learning. In ICML, 2019.\n[De Giacomo and Vardi, 2013] Giuseppe De Giacomo and\nMoshe Y. Vardi. Linear temporal logic and linear dynamic\nlogic on ﬁnite traces. In IJCAI, 2013.\n[Duret-Lutz et al., 2016] Alexandre Duret-Lutz, Alexandre\nLewkowicz, Amaury Fauchille, Thibaud Michaud, Eti-\nenne Renault, and Laurent Xu. Spot 2.0 - a framework\nfor ltl and ω -automata manipulation. In ATVA, 2016.\n[Fu and Topcu, 2014] Jie Fu and Ufuk Topcu. Probably ap-\nproximately correct MDP learning and control with tem-\nporal logic constraints. In Robotics: Science and Systems\nX, 2014.\n[Giacomo et al., 2019] Giuseppe De Giacomo, L. Iocchi,\nMarco Favorito, and F. Patrizi. Foundations for restrain-\ning bolts: Reinforcement learning with ltlf/ldlf restraining\nspeciﬁcations. In ICAPS, 2019.\n[Hahn et al., 2019] Ernst Moritz Hahn, Mateo Perez, Sven\nSchewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik\nWojtczak. Omega-regular objectives in model-free rein-\nforcement learning. In TACAS, 2019.\n[Hahn et al., 2020] Ernst Hahn, Mateo Perez, Sven Schewe,\nFabio Somenzi, Ashutosh Trivedi, and Dominik Wojtczak.\nFaithful and effective reward schemes for model-free rein-\nforcement learning of omega-regular objectives. In Auto-\nmated Technology for Veriﬁcation and Analysis, 2020.\n[Hahn et al., 2021] Ernst Moritz Hahn, Mateo Perez, Sven\nSchewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik\nWojtczak. Mungojerrie: Reinforcement learning of linear-\ntime objectives. arXiv preprint: 2106.09161, 2021.\n[Hasanbeig et al., 2019] M. Hasanbeig, Yiannis Kantaros,\nA. Abate, D. Kroening, George Pappas, and I. Lee. Re-\ninforcement learning for temporal logic control synthesis\nwith probabilistic satisfaction guarantees. In CDC, 2019.\n[Hasselt, 2010] H. V. Hasselt. Double q-learning. In NIPS,\n2010.\n[Henriques et al., 2012] David Henriques, Jo˜ao G. Martins,\nPaolo Zuliani, Andr´e Platzer, and Edmund M. Clarke. Sta-\ntistical model checking for markov decision processes. In\nQEST, 2012.\n[Jothimurugan et al., 2019] Kishor Jothimurugan, R. Alur,\nand Osbert Bastani. A composable speciﬁcation language\nfor reinforcement learning tasks. In NeurlPS, 2019.\n[Kupferman and Vardi, 1999] Orna Kupferman and Moshe\nVardi. Model checking of safety properties. Formal Meth-\nods in System Design, 19, 1999.\n[Latvala, 2003] Timo Latvala. Efﬁcient model checking of\nsafety properties. In Model Checking Software, 2003.\n[Li et al., 2017] Xiao Li, C. Vasile, and C. Belta. Reinforce-\nment learning with temporal logic rewards. IROS, 2017.\n[Littman et al., 2017] Michael L. Littman, Ufuk Topcu, Jie\nFu, Charles Isbell, Min Wen, and James MacGlashan.\nEnvironment-independent task speciﬁcations via gltl.\narXiv preprint: 1704.04341, 2017.\n[Manna and Pnueli, 1987] Zohar Manna and Amir Pnueli. A\nhierarchy of temporal properties. In PODC, 1987.\n[Sadigh et al., 2014] Dorsa Sadigh, Eric S. Kim, Samuel\nCoogan, S. Shankar Sastry, and Sanjit A. Seshia. A learn-\ning based approach to control synthesis of markov decision\nprocesses for linear temporal logic speciﬁcations. In CDC,\n2014.\n[Safra, 1988] S. Safra. On the complexity of ω-automata. In\nFOCS, 1988.\n[Sutton, 1988] Richard S. Sutton. Learning to predict by the\nmethods of temporal differences. Machine Learning, 3(1),\n1988.\n[Watkins and Dayan, 1992] Christopher J. C. H. Watkins and\nPeter Dayan. Q-learning. Machine Learning, 8(3), 1992.\n(a) Reward-on-acc with Q-learning\n(b) Reward-on-acc with Double Q-learning\n(c) Reward-on-acc with SARSA(λ)\n(d) Multi-discount with Q-learning\n(e) Multi-discount with Double Q-learning\n(f) Multi-discount with SARSA(λ)\n(g) Zeta-reach with Q-learning\n(h) Zeta-reach with Double Q-learning\n(i) Zeta-reach with SARSA(λ)\nFigure F.3: Empirical results of the ﬁrst LTL-MDP pair (continued on next page)\n(j) Zeta-acc with Q-learning\n(k) Zeta-acc with Double Q-learning\n(l) Zeta-acc with SARSA(λ)\n(m) Zeta-discount with Q-learning\n(n) Zeta-discount with Double Q-learning\n(o) Zeta-discount with SARSA(λ)\nFigure F.3: Empirical results of the ﬁrst LTL-MDP pair (continued). Each sub-ﬁgure corresponds to a speciﬁc reward-scheme and learning-\nalgorithm pair. For each sub-ﬁgure, on the left: LTL-PAC probabilities vs. number of samples, for varying parameters p; on the right: number\nof samples needed to reach 0.9 LTL-PAC probability vs. parameters p.\n(a) Reward-on-acc with Q-learning\n(b) Reward-on-acc with Double Q-learning\n(c) Reward-on-acc with SARSA(λ)\n(d) Multi-discount with Q-learning\n(e) Multi-discount with Double Q-learning\n(f) Multi-discount with SARSA(λ)\n(g) Zeta-reach with Q-learning\n(h) Zeta-reach with Double Q-learning\n(i) Zeta-reach with SARSA(λ)\nFigure F.4: Empirical results of the second LTL-MDP pair (continued on next page)\n(j) Zeta-acc with Q-learning\n(k) Zeta-acc with Double Q-learning\n(l) Zeta-acc with SARSA(λ)\n(m) Zeta-discount with Q-learning\n(n) Zeta-discount with Double Q-learning\n(o) Zeta-discount with SARSA(λ)\nFigure F.4: Empirical results of the second LTL-MDP pair (continued). Each sub-ﬁgure corresponds to a speciﬁc reward-scheme and learning-\nalgorithm pair. For each sub-ﬁgure, on the left: LTL-PAC probabilities vs. number of samples, for varying parameters p; on the right: number\nof samples needed to reach 0.9 LTL-PAC probability vs. parameters p.\n",
  "categories": [
    "cs.AI",
    "cs.FL",
    "cs.LG"
  ],
  "published": "2021-11-24",
  "updated": "2022-06-24"
}