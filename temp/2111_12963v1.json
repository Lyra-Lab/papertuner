{
  "id": "http://arxiv.org/abs/2111.12963v1",
  "title": "Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU Neural Networks",
  "authors": [
    "Tilahun M. Getu"
  ],
  "abstract": "Among the several paradigms of artificial intelligence (AI) or machine\nlearning (ML), a remarkably successful paradigm is deep learning. Deep\nlearning's phenomenal success has been hoped to be interpreted via fundamental\nresearch on the theory of deep learning. Accordingly, applied research on deep\nlearning has spurred the theory of deep learning-oriented depth and breadth of\ndevelopments. Inspired by such developments, we pose these fundamental\nquestions: can we accurately approximate an arbitrary matrix-vector product\nusing deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If\nso, can we bound the resulting approximation error? In light of these\nquestions, we derive error bounds in Lebesgue and Sobolev norms that comprise\nour developed deep approximation theory. Guided by this theory, we have\nsuccessfully trained deep ReLU FNNs whose test results justify our developed\ntheory. The developed theory is also applicable for guiding and easing the\ntraining of teacher deep ReLU FNNs in view of the emerging teacher-student AI\nor ML paradigms that are essential for solving several AI or ML problems in\nwireless communications and signal processing; network science and graph signal\nprocessing; and network neuroscience and brain physics.",
  "text": "1\nError Bounds for a Matrix-Vector Product\nApproximation with Deep ReLU Neural Networks\nTilahun M. Getu, Member, IEEE\nAbstract—Among the several paradigms of artiﬁcial intel-\nligence (AI)/machine learning (ML), a remarkably successful\nparadigm is deep learning. Deep learning’s phenomenal success\nhas been hoped to be interpreted via fundamental research on the\ntheory of deep learning. Accordingly, applied research on deep\nlearning has spurred the theory of deep learning oriented depth\nand breadth of developments. Inspired by such developments, we\npose these fundamental questions: can we accurately approximate\nan arbitrary matrix-vector product using deep rectiﬁed linear\nunit (ReLU) feedforward neural networks (FNNs)? If so, can\nwe bound the resulting approximation error? In light of these\nquestions, we derive error bounds in Lebesgue and Sobolev\nnorms that comprise our developed deep approximation theory.\nGuided by this theory, we have successfully trained deep ReLU\nFNNs whose test results justify our developed theory. The\ndeveloped theory is also applicable for guiding and easing the\ntraining of teacher deep ReLU FNNs in view of the emerging\nteacher-student AI/ML paradigms that are essential for solving\nseveral AI/ML problems in wireless communications and signal\nprocessing; network science and graph signal processing; and\nnetwork neuroscience and brain physics.\nIndex Terms—AI/ML, deep learning, deep ReLU FNNs, the\ntheory of deep learning, deep approximation theory, teacher-\nstudent AI/ML paradigms.\nI. INTRODUCTION\nA. Related Works\nUnderstanding natural and artiﬁcial intelligence have been\nmankind’s prominent inquiry that has led to compelling\nadvancements toward various promising technologies. The\npromising technologies continually investigated by the neu-\nroscience and artiﬁcial intelligence (AI)/machine learning\n(ML) research communities include brain emulation, human-\nmachine interfaces, and AI [1, Ch. 2]. In the quest for AI\n[2], feedforward neural networks (FNNs) have been workhorse\nmodels since Rosenblatt’s perceptron of the 1950s [3, Ch.\n1]. Rosenblatt’s perceptron was the ﬁrst algorithmically de-\nscribed neural network [3, Ch. 1] and it inspired multilayer\nperceptrons. Multilayer perceptrons are FNNs with one or\nmore hidden layers and trained using the back-propagation\nalgorithm [4]. Following the back-propagation algorithm’s\ndiscovery in around 1986 [4], research on the representation\nand approximation power of (shallow) FNNs were intensiﬁed\nin the late 1980s and 1990s [5]. During this era, several works\nestablished that FNNs with one hidden layer are universal\napproximators [5]–[10].\nPropelled by the advancements of research on FNNs and\nthe availability of enormous computing power as well as a\nT. M. Getu is with the National Institute of Standards and Technology\n(NIST), 100 Bureau Drive, Gaithersburg, MD 20899, USA and also with the\n´Ecole de Technologie Sup´erieure (´ETS), Montr´eal, QC H3C 1K3, Canada\n(e-mail: tilahun.getu@nist.gov).\ndeluge of data, deep learning [11]–[13] has recently emerged\nas a notably successful AI/ML paradigm. In such an AI/ML\nparadigm, the various implicit discriminative features of the\ntraining data are disentangled by deep representation learning\n[14] through non-linear (also linear1) deep transformations.\nDeep transformations are at the heart of the exponential ex-\npressivity power of deep networks [18]–[23] that have become\nthe norm of AI/ML research, especially after the superhuman\nperformance of AlexNet [24]. AlexNet has sparked waves of\nresearch on deep learning (and deep reinforcement learning)\nthat are now applied in numerous research ﬁelds as diverse\nas clustering, information retrieval, dimensionality reduction,\nand natural language processing [12], [13]; computer vision,\nspeech recognition, image processing, and object recognition\n[11], [12], [25]; wireless communications, wireless network-\ning, and signal processing [26]–[29]; and gaming, ﬁnance,\nenergy, and healthcare [30], [31].\nAlong the lines of the state-of-the-art deep learning tech-\nniques’ striking performance demonstrated for various learn-\ning problems, the most important fundamental question is\ninterpreting the “black box of AI” [32]. This undertaking has\nbeen motivated by the theories on (shallow) FNNs, the rapid\nadvancements and broad adoption of deep learning, and the\nneed to devise an interpretable/explainable AI. Toward inter-\npretable/explainable AI, the theory of deep learning (ToDL)\nhas recently emerged as a promising and vibrant research\nﬁeld. This research ﬁeld, speciﬁcally, attempts to demystify\nthe various hidden transformations of deep architectures in\norder to provide a fundamental theoretical guarantee and\nexplanation on the success of the existing deep networks. Deep\nnetworks under ToDL studies include FNNs, convolutional\nneural networks (CNNs) [33], [34], recurrent neural networks\n(RNNs) and their popular variants named long short-term\nmemory (LSTM) [35], [36], autoencoders (AEs) [37]–[39],\ngenerative adversarial networks (GANs) [40]–[42], ResNet\n[43], and DenseNet [44], [45]. These deep networks’ ToDL ex-\nposition has matured into three fundamental research themes:\napproximation (representation)2, optimization, and generaliza-\ntion [46], [47].\nB. Motivation and Context\nTo develop theories on approximation [22], optimization\n[48], [49], and generalization [50], [51] for the state-of-the-\n1Deep FNNs with linear layers can also learn discriminative features of the\ninput data successfully. In this respect, their convergence was characterized\nin [15], [16] and their nonlinear learning dynamics were analyzed in [17].\n2The mathematics and AI/ML research communities pursue research,\nrespectively, on deep approximation and deep representation which are the\nsame AI/ML problems.\narXiv:2111.12963v1  [cs.LG]  25 Nov 2021\n2\nart deep networks solving either classiﬁcation [52], [53] or\nregression AI/ML problems [47], [54], considerable ToDL\nadvancements have been made via numerous frameworks.\nThese frameworks include mean ﬁeld theory [19], [55], [56],\nrandom matrix theory [57], [58], tensor factorization [59],\n[60], optimization theory [61]–[64], kernel learning [65]–[67],\nlinear algebra [68], [69], spline theory [70], [71], theoretical\nneuroscience [72], [73], high-dimensional probability [74],\nhigh-dimensional statistics [75], manifold theory [19], [76],\nFourier analysis [77], and scattering networks [78], [79].\nCompared with shallow FNNs, several ToDL works have\nestablished that deep FNNs have the power of exponential\nexpressivity [18]–[23]. The exponential expressivity of deep\nnetworks has stimulated lines of ToDL research regarding the\nuniversal approximation capability of ResNet [80] and slim\n(and sparse) networks [81], the quasi-equivalence of depth and\nwidth [82], the width-efﬁciency of rectiﬁed linear unit (ReLU)\nFNNs [83], the improving effect of depth and width [84],\nand un-rectifying signal representation [85]. Representation re-\nsearch – in the context of ToDL – overlaps with approximation\nresearch using deep neural networks and, in particular, deep\nReLU FNNs. For deep ReLU FNNs approximating various\nfunctions, the authors of [86]–[90] developed approximation\ntheories. Although these approximation theories are interesting\nin their own rights and have the potential to inspire much more\nresearch, they focus on a scalar input and a scalar output. In the\nage of big data, nevertheless, ToDL3 research needs to account\nfor deep learning in high dimension. Deep learning in high\ndimension through the lenses of high-dimensional probability\n[91], high-dimensional statistics [92], and empirical process\ntheory [93] which are crucial tools of ToDL, high-dimensional\nbounds afford useful insights on approximation with deep\nReLU FNNs fed with input data in high dimension. High\ndimension considered ﬁttingly with its timeliness, the works in\n[94] and [95] presented FNN-based approximations applicable\nfor parametric partial differential equations.\nIn consideration of high dimension, high-dimensional signal\nprocessing based wireless communications technologies such\nas massive multiple-input multiple-output (MIMO) and mas-\nsively parallel IoT (Internet of things) are on the verge of being\npervasive in 5G (ﬁfth generation) and beyond [96]. Toward 5G\nand beyond, the IEEE communications and signal processing\nresearch communities have swiftly adopted deep learning for\nsolving numerous classiﬁcation and regression AI/ML prob-\nlems in wireless communications, wireless networking, and\nsignal processing [26], [27], [29]. These research ﬁelds’ future\nstrides in deep learning will beneﬁt from fundamental ToDL\ninterpretations. ToDL interpretations, speciﬁcally, inspire and\ninform intelligently adaptive AI/ML-based wireless communi-\ncations algorithms that employ efﬁcient architectures predicted\nby ToDL. From ToDL’s fundamental vantage point, an impor-\ntant gap is the performance quantiﬁcation of the proposed deep\nlearning algorithms fed with input signals modeled by matrix-\nvector products. For matrix-vector products’ approximation\nwith deep ReLU FNNs, consequently, we develop a deep\n3ToDL is also being developed under the names learning theory and the\nmathematics of data science. For all these research ﬁelds, no distinction in\nscope is delineated by the AI/ML and mathematics research communities.\napproximation theory useful for guiding the training of teacher\ndeep ReLU FNNs in view of the emerging teacher-student\nAI/ML paradigms [97]–[101] that are essential for solving\nnumerous AI/ML problems.\nC. Contributions\nThe contributions of this paper are threefold:\n1) For a matrix-vector product approximation with a deep\nReLU FNN, we develop a deep approximation theory by\nderiving error bounds in Lebesgue and Sobolev norms.\n2) We present computer experiments whose empirical re-\nsults corroborate our deep approximation theory.\n3) Implying broader relevance, we discuss the applications\nof our developed theory in the context of teacher-student\nAI/ML paradigms solving various AI/ML problems in\nwireless communications and signal processing; network\nscience and graph signal processing; and network neu-\nroscience and brain physics [102], [103].\nThese contributions bear signiﬁcance in the sense that they in-\nspire numerous works on ToDL geared toward deep represen-\ntation and the performance quantiﬁcation of AI/ML algorithms\napplied to the aforementioned research ﬁelds.\nThe remainder of this paper is organized as follows. Sec.\nII highlights the preliminaries. Sec. III details our problem\nmotivation and problem formulation. Sec. IV documents the\ndeveloped deep approximation theory. Sec. V reports our\ncomputer experiments. Sec. VI presents applications. Finally,\nSec. VII draws concluding remarks and research outlook.\nNotation: italic and bold lowercase(uppercase) letters denote\nscalars and vectors(matrices), respectively. Bold calligraphic\nletters such as N N and W indicate FNNs and a Sobolev\nspace, respectively. Cn(Rn) and Cm×n(Rm×n) represent the\nsets of n-dimensional vectors of complex(real) numbers and\nm × n complex(real) matrices, respectively. R+, N, and\nNn\n0(N0) denote the sets of positive real numbers, natural\nnumbers, and (n-dimensional)natural numbers including zero,\nrespectively. N≥k\n:= {k, k + 1, . . .} denotes the sets of\nnatural numbers greater than or equal to k\n∈\nN0. For\nn ≥2, [n] := {1, . . . , n}. ∼, :=, | · |, (·)T , In, 0m×n,\nand 0 stand for distributed as, equal by deﬁnition, cardinality,\ntranspose, an n × n identity matrix, an m × n zero matrix,\nand a zero vector (matrix) whose dimension will be clear\nin context, respectively. max, vec(·), diag(·), Re{·}, Im{·},\nP(·), and N(0, σ2In)\n\u0000CN(0, σ2In)\n\u0001\nexpress maximum, vec-\ntorization, a (block) diagonal matrix, a real part, an imaginary\npart, probability, and a normal (complex normal) distribution\nwith a zero mean and covariance σ2In, respectively. For\nW\n∈Rm×n and x ∈Rn, wi,j and xi represent the\n(i, j)-th element of W and the i-th entry of x, respectively.\nPer the MATLAB syntax, W (i, :)\n\u0000W (:, j)\n\u0001\ndenotes the i-\nth row(j-th column) of W . ∥W ∥ℓ0 := |(i, j) : wi,j ̸= 0|;\n∥x∥∞:= maxi=1,...,n |xi|; and ∥W ∥∞:= maxi,j |wi,j|.\nThe horizontal concatenation of n vectors(matrices) is de-\nnoted as [w1(W1), . . . , wn(Wn)]. A sequence of K matrix-\nvector tuples is expressed as\n\u0002\n[W1, b1], . . . , [WK, bK]\n\u0003\n. For\nA ⊂Rd (the standard topology), A denotes the closure of\nA. If A, B ⊂Rd, we write A ⊂⊂B provided that A is\n3\ncompact in B. For α ∈Nd\n0, we let |α| := Pd\ni=1 αi. With\nrespect to (w.r.t.) Lp (Lebesgue) spaces and f(x) : Rd →R,\n∥f∥L∞(Ω) := inf\n\b\nC ≥0 : |f(x)| ≤C, ∀x ∈Ω\n\f\fΩ⊂Rd\t\n.\nFor a function f : X →R, supp f and ran f stand for the\nsupport of f and the range of f, respectively. For functions\nf : X →Y and g : Y →Z, their composition is denoted as\ng ◦f : X →Z. A function f : Ω→Rm is an L-Lipschitz\ncontinuous whenever |f(x) −f(y)| ≤L|x −y| for a constant\nL > 0 and Ω⊂Rd.\nII. PRELIMINARIES\nA. The Mathematics of FNNs\nBased on [87, Deﬁnition II.1] and [90, Deﬁnition 1.1], we\nstate the following deﬁnition on the mathematics of FNNs.4\nDeﬁnition 1. Suppose K, N0, N1, N2, . . . , NK ∈N, K ≥2,\nand x ∈RN0. A map Φ : RN0 →RNK characterized as\nΦ(x) := AK(ρ(AK−1(ρ(. . . ρ(A1(x)))))),\n(1)\nwith afﬁne linear maps Ak : RNk−1 →RNk and an element-\nwise ReLU activation function ρ(x) := [ρ(x1), . . . , ρ(xN0)]T\n– ρ(a) := max(a, 0) – is called a ReLU FNN. The ReLU\nFNN’s depth is equal to K and denoted as L(Φ) := K,\nN0 is the dimension of the 0-th layer (input layer), Nk is\nthe dimension of the k-th layer, and NK is the dimension\nof the output layer. The afﬁne linear map corresponding to\nthe k-th layer is deﬁned via Ak(x) = Wkx + bk with\nWk ∈RNk×Nk−1 and bk ∈RNk. W.r.t. Wk and bk, wk,i,j\ndenotes the weight associated with the edge between the j-th\nnode of the (k −1)-th layer and the i-th node of the k-th\nlayer and bk,i is the weight associated with the i-th node of\nthe k-th layer. For the ReLU FNN described by (1), the total\nnumber of neurons, the network connectivity, the maximum\nwidth, and the maximum absolute value of the weights are\ndenoted and deﬁned as N(Φ) := PK\nk=0 Nk, M(Φ) :=\nPK\nk=1(∥Wk∥ℓ0 + ∥bk∥ℓ0), W(Φ) := maxk=0,...,K Nk, and\nB(Φ)\n:=\nmaxk∈[K]\n\b\n∥Wk∥∞, ∥bk∥∞\n\t\n, respectively. The\nclass of FNNs Φ : RN0 →RNK with no more than K\nlayers, connectivity no more than M, input dimension N0,\noutput dimension NK, and activation function ρ is denoted as\nN N N0,NK\nK,M,ρ . To this end, N N N0,NK\n∞,M,ρ := S\nK∈N N N N0,NK\nK,M,ρ ,\nN N N0,NK\nK,∞,ρ\n:=\nS\nM∈N N N N0,NK\nK,M,ρ , and N N N0,NK\n∞,∞,ρ\n:=\nS\nK∈N N N N0,NK\nK,∞,ρ .\nStating Deﬁnition 1 differently, the underneath deﬁnition\nfollows from [88, Deﬁnition 2.2] and [89, Deﬁnition 2.1].\nDeﬁnition 2. Suppose K, N0, N1, . . . , NK ∈N and K ≥2.\nA FNN denoted by Φ is a sequence of matrix-vector tuples\nΦ :=\n\u0002\n[W1, b1], . . . , [WK, bK]\n\u0003\n,\n(2)\nwhere Wk ∈RNk×Nk−1 and bk ∈RNk, k ∈[K], N(Φ) =\nPK\nk=0 Nk, and M(Φ) = PK\nk=1(∥Wk∥ℓ0 + ∥bk∥ℓ0). For x ∈\nRN0 and ρ(a) := max(a, 0), a map Φ : RN0 →RNK is\ndeﬁned as Φ(x) = xK, where xK is obtained recursively as\nx0 := x; xk := ρ(Wkxk−1 + bk), k ∈[K −1]\n(3a)\nxK := WKxK−1 + bK,\n(3b)\n4To stick to our notation, we assume a vector of FNN inputs and outputs.\nwhere ρ(y) = [ρ(y1), . . . , ρ(ym)]T for y = [y1, . . . , ym]T .\nExpressed via (2), the concatenation of two FNNs follows.\nDeﬁnition 3 (Concatenation of FNNs [89, Deﬁnition 2.2]).\nLet K1, N 1\n0 , . . . , N 1\nK1 ∈N and K2, N 2\n0 , . . . , N 2\nK2, ∈N. De-\nﬁne two FNNs as Φ1 :=\n\u0002\n[W 1\n1 , b1\n1], . . . , [W 1\nK1, b1\nK1]\n\u0003\nand\nΦ2 :=\n\u0002\n[W 2\n1 , b2\n1], . . . , [W 2\nK2, b2\nK2]\n\u0003\n. For N 1\n0 = N 2\nK2, the\nconcatenation of Φ1 and Φ2 is given by\nΦ1 • Φ2 :=\n\u0002\n[W 2\n1 , b2\n1], . . . , [W 2\nK2−1, b2\nK2−1], [W 1\n1 W 2\nK2,\nW 1\n1 b2\nK2 + b1\n1], [W 1\n2 , b1\n2], . . . , [W 1\nK1, b1\nK1]\n\u0003\n,\n(4)\nwhere (Φ1 • Φ2)(x) = Φ1\u0000Φ2(x)\n\u0001\nand L(Φ1 • Φ2) =\nP2\ni=1 Ki −1.\nFor a ReLU activation ρ(·) and x ∈R, x = ρ(x) −ρ(−x)\n[87]. This identity is exploited to prove the following lemma.\nLemma 1 (ReLU identity FNN (I-FNN) [89, Remark 2.4]).\nFor N0 ≥2, K ∈N≥2, and x ∈RN0, one can construct an I-\nFNN Φ\nIN0\nK\nwith K layers and at most 2N0K {−1, 1}-valued\nweights such that Φ\nIN0\nK (x) = x. Toward this end,\nΦ\nIN0\nK\n:=\n\u0002\n[[IN0, −IN0]T , 0],\n(K−2) times\nz\n}|\n{\n[I2N0, 0], . . . , [I2N0, 0],\n[[IN0, −IN0], 0]\n\u0003\n.\n(5)\nFor K = 1, an I-FNN is constructed as Φ\nIN0\n1\n:=\n\u0002\n[IN0, 0]\n\u0003\n.\nProof. The proof is provided as supplementary material.\nEchoing the fact FNNs are universal approximators, the\nfollowing proposition follows through (4) and (5).\nProposition 1 (Representation by ReLU FNNs). Let W ∈\nRm×n, x ∈Rn, and ρ(x) := max(x, 0), x ∈R. For an afﬁne\ntransformation A : Rn →Rm deﬁned as A = W x, the\nfollowing holds true.\n1) A can be represented by N N n,m\n2,M,ρ with M\n=\n2∥W ∥ℓ0 + 2m.\n2) For K >> 2, A can be represented by N N n,m\nK,M,ρ with\nM = 2m + 2(K −2)n + 4∥W ∥ℓ0.\n3) For K >> 2, A can also be represented by N N n,m\nK,M,ρ\nwith M = 2Km + 2∥W ∥ℓ0.\nProof. The proof is deferred to Appendix B.\nMeanwhile, the parallelization of FNNs is formalized below.\nLemma 2 (Parallelization of FNNs). Consider n FNNs\nΦi :=\n\u0002\n[W i\n1, bi\n1], . . . , [W i\nK, bi\nK]\n\u0003\n, i ∈[n]. Let K, n ∈N and\nK, n ≥2. Let the parallelization of Φ1, . . . , Φn be denoted\nand deﬁned as P(Φ1, . . . , Φn) :=\n\u0002\n[ ˜\nW1, ˜b1], . . . , [ ˜\nWK, ˜bK]\n\u0003\n.\nIf P(Φ1, . . . , Φn)(x) = [(Φ1(x))T , . . . , (Φn(x))T ]T for x ∈\nRN0, then for ˜Φ := [Φ1, . . . , Φn] and 2 ≤k ≤K\n˜\nW1 =[(W 1\n1 )T , . . . , (W n\n1 )T ]T ; ˜b1 = [(b1\n1)T , . . . , (bn\n1)T ]T (6a)\n˜\nWk =diag(W 1\nk , . . . , W n\nk ); ˜bk = [(b1\nk)T , . . . , (bn\nk)T ]T ,\n(6b)\nwhere L(P( ˜Φ)) = K, M\n\u0000P( ˜Φ)\n\u0001\n= Pn\ni=1 M(Φi), and\nN\n\u0000P( ˜Φ)\n\u0001\n= Pn\ni=1 N(Φi) −(n −1)N0.\nProof. The proof is provided as supplementary material.\n4\nMoreover, we adopt the following lemma.\nLemma 3 (Superposition and parallelization of FNNs\n[87, Lemma II.7]). Suppose ρ(x)\n:=\nmax(x, 0), ai\n∈\nR; n, di, Ki, Mi\n∈\nN; Φi\n∈\nN N di,1\nKi,Mi,ρ, i\n∈\n[n];\nand d\n=\nPn\ni=1 di. Let x\n=\n[xT\n1 , . . . , xT\nn]T\n∈\nRd\nwith xi\n∈\nRdi, i\n∈\n[n]. There exist FNNs Φ1\n∈\nN N d,n\nK,M,ρ and Φ2 ∈N N d,1\nK,M+n,ρ with K = maxi Ki,\nW(Φ1) = W(Φ2) ≤Pn\ni=1 max{2, W(Φi)}, and M =\nPn\ni=1\n\u0000Mi + W(Φi) + 2(K −Ki) + 1\n\u0001\nfulﬁlling Φ1(x) =\n\u0002\na1Φ1(x1), . . . , anΦn(xn)\n\u0003T and Φ2(x) = Pn\ni=1 aiΦi(xi).\nFurthermore, the weights of Φ1 and Φ2 comprise the weights\nof Φi, i ∈[n], {a1, . . . , an}, and ±1’s.\nProof. The proof is deferred to Appendix A-A.\nB. Function Norms and Spaces\nWe acquire the following lemma proved in [89, p. 325-326].\nLemma 4 (Lp-norm of a composition function [89, Lemma\n5.2]). Suppose m, n ∈N with n ≤m. Let U ⊂Rm be open,\nt : U →Rn be continuously differentiable, and ∅̸= K ⊂U\nbe compact. Assume that Dt(x) ∈Rn×m has a full rank ∀x ∈\nK. Then, there is a constant C fulﬁlling\nR\nK(f ◦t)(x)dx ≤\nC ·\nR\nt(K) f(y)dy for all Borel measurable functions such that\nf : t(K) →R+. For all 0 < p < ∞and f : t(K) →R being\nmeasurable\n∥f ◦t∥Lp(K) ≤C1/p · ∥f∥Lp(t(K)).\n(7)\nFor an open set Ω∈Rd and n ∈N0, let Cn(Ω) denote\nthe set of n times continuously differentiable functions on Ω.\nThe space of test functions is denoted as C∞\nc (Ω) :=\n\b\nf ∈\nC∞(Ω)|supp f ⊂⊂Ω\n\t\n. For f ∈Cn(Ω) and α ∈Nd\n0 with\n|α| ≤n, Dαf := ∂|α|f\n\u000e\n∂xα1\n1 . . . ∂xαd\nd . Meanwhile, we\nhereby deﬁne a Sobolev space [5], [88].\nDeﬁnition 4 (Sobolev space [88, Deﬁnition 3.1]). Let n, d ∈\nN and Ω⊂Rd denote an open subset of Rd. Let Lp(Ω)\ndenote the Lebesgue spaces on Ωfor 1 ≤p ≤∞. The\nSobolev space Wn,p(Ω) is deﬁned ∀α ∈Nd\n0 as Wn,p(Ω) :=\n\b\nf ∈Lp(Ω) : Dαf ∈Lp(Ω) with |α| ≤n\n\t\nequipped\nwith the norm deﬁned for f ∈Wn,p(Ω) as ∥f∥Wn,p(Ω) :=\n\u0000 P\n0≤|α|≤n ∥Dαf∥p\nLp(Ω)\n\u00011/p, 1 ≤p < ∞, and\n∥f∥Wn,∞(Ω) :=\nmax\n0≤|α|≤n ∥Dαf∥L∞(Ω).\n(8)\nHereinafter, ∥f∥Wn,p(Ω) = ∥f∥Wn,p = ∥f(x)∥Wn,p(Ω;dx).\nIn the meantime, Deﬁnition 4 is generalized below.\nDeﬁnition 5 (Sobolev spaces of m vector-valued functions\n[88, Deﬁnition B.1]). Let n ∈N0 and m ∈N≥2. The\nSobolev spaces of m vector-valued functions are deﬁned as\nWn,p(Ω; Rm) :=\n\b\n(f1, . . . , fm) : fi ∈Wn,p(Ω)\n\t\nwith\n∥f∥Wn,p(Ω;Rm) :=\n\u0000 Pm\ni=1 ∥fi∥p\nWn,p(Ω)\n\u00011/p, 1 ≤p < ∞;\n∥f∥Wn,∞(Ω;Rm) :=\nmax\ni=1,...,m ∥fi∥Wn,∞(Ω).\n(9)\nWe also acquire the following deﬁnition.\nDeﬁnition 6 (Sobolev semi-norm [88, Deﬁnition B.2]).\nFor f\n∈Wn,p(Ω; Rm) with n, k ∈N0, k ≤n, and\nm ∈N, the Sobolev semi-norm is deﬁned as |f|Wk,p(Ω;Rm) :=\n\u0000 Pm\ni=1,|α|=k ∥Dαfi∥p\nLp(Ω)\n\u00011/p, 1 ≤p < ∞;\n|f|Wk,∞(Ω;Rm) :=\nmax\ni=1,...,m:|α|=k ∥Dαfi∥L∞(Ω).\n(10)\nFor m = 1 and k = 0, Deﬁnition 6 corroborates that\n|f|Wk,p(Ω) = ∥f∥Lp(Ω) [88]. Accordingly, we herein deﬁne\nthe Lebesgue spaces of m vector-valued functions.\nDeﬁnition 7 (Lebesgue spaces of vector-valued func-\ntions). Let m\n∈\nN≥2 and 1\n≤\np\n≤\n∞. Then, the\nLebesgue spaces of m vector-valued functions are deﬁned\nas Lp(Ω; Rm)\n:=\n{(f1, . . . , fm)\n:\nfi\n∈\nLp(Ω)} with\n∥f∥Lp(Ω;Rm) :=\n\u0000 Pm\ni=1 ∥fi∥p\nLp(Ω)\n\u00011/p, 1 ≤p < ∞;\n∥f∥L∞(Ω;Rm) :=\nmax\ni=1,...,m ∥fi∥L∞(Ω).\n(11)\nWe also adopt the corollary below proved in [88, p. 23-24].\nCorollary 1 (Sobolev (semi-)norm of a composition func-\ntion [88, Corollary B.5]). For d, m ∈N, suppose Ω1 ⊂Rd\nand Ω2 ⊂Rm are both open, bounded, and convex. Then,\nthere is a constant C = C(d, m) > 0 with the following\nproperty: if f ∈W1,∞(Ω1; Rm) and g ∈W1,∞(Ω2) are\nLipschitz continuous functions such that ran f ⊂Ω2, then\ng ◦f ∈W1,∞(Ω1) and it holds that\n|g ◦f|W1,∞(Ω1) ≤C|g|W1,∞(Ω2)|f|W1,∞(Ω1;Rm)\n(12a)\n∥g ◦f∥W1,∞(Ω1) ≤C max{∥g∥L∞(Ω2), ˜g ˜f},\n(12b)\nwhere ˜g = |g|W1,∞(Ω2) and ˜f = |f|W1,∞(Ω1;Rm).\nIII. MATRIX-VECTOR PRODUCT APPROXIMATION\nA. Problem Motivation\nIt has been corroborated theoretically and empirically that\ndeep networks – in comparison with shallow networks – have\nthe power of exponential expressivity [18]–[23]. Exponential\nexpressivity, however, comes at a price of the considerable\ntraining difﬁculty of deep architectures.5 Deep architectures’\ntraining often associates – due to successive non-linearities\n[98] – mathematically intractable non-convex optimization\nproblems. Such problems’ gradient-based solutions exhibit an\nincrement in the probability of ﬁnding poor local minima with\nthe depth of an architecture [105]. Along with this global\noptimization issue, plateaus and saddle points are common\nin deep architecture learning [48]. Deep architecture learning\nis also hampered by local optimization issues such as gradient\nvanishing/explosion and the possibility of many dead ReLUs\n(in case of ReLU networks) [48], [106], [107].\nBy easing the training difﬁculty of deep networks for AI/ML\nclassiﬁcation problems via a model compression, knowl-\nedge distillation [97] has recently emerged as a promising\ndeep learning paradigm based on a student-teacher AI/ML\nparadigm. In this paradigm, a student network is trained to\n5Regarding AI/ML algorithms proposed for wireless communications,\nsignal processing, and networking [26], [27], [29], [104], we have witnessed\nthat most reported empirical results are based on shallow networks.\n5\npredict the hard classiﬁcation labels as well as the softened\noutput of a teacher network [97]. The teacher network’s extra\ninformation helps the student network to generalize better and\nsuch knowledge distillation often provides an improvement in\ninference speed [108]. To explain these beneﬁts of knowledge\ndistillation, the authors of [101] pursued a theoretical work on\ndeep linear classiﬁers. For these classiﬁers, it was found that\nthree factors explain the success of knowledge distillation: data\ngeometry, optimization bias, and strong monotonicity [101].\nFollowing its initial success in easing the training difﬁculty\nof deep architectures, knowledge distillation has been de-\nployed to solve many AI/ML classiﬁcation problems. Regard-\ning the classiﬁcation problem of learning from noisy labels,\nthe authors of [109] proposed a uniﬁed distillation framework\nthat employs label relations in semantic knowledge graph and\na small clean dataset. From a small clean dataset, the authors\ndistilled the learned knowledge to facilitate model learning\nfrom the entire noisy dataset [109]. Besides learning from\nnoisy dataset, knowledge distillation was also exploited for\nmodel compression that would enable the efﬁcient execution\nof deep learning models in resource-constrained environments\n[108]. For resource-constrained environments such as mobile\nor embedded devices, the authors of [108] proposed two model\ncompression schemes that jointly leverage weight quantization\nand knowledge distillation of larger teacher networks into\ncompressed student networks. Moreover, knowledge distilla-\ntion was also exploited to blend an LSTM teacher with a CNN\nstudent such that more accurate CNNs – more computationally\nefﬁcient than LSTMs – can be trained under the guidance of\nLSTMs [110].\nBy extending knowledge distillation, the authors of [98]\nintroduced hint training which is a training scheme based on a\nhint deﬁned as the output of a teacher’s hidden layer deployed\nto guide a guided layer of a student [98]. Student models\n– for AI/ML classiﬁcation problems – that are thinner and\ndeeper were successfully trained using hint training [98]. By\nexploiting hint training and knowledge distillation, the authors\nof [99] introduced two teacher-student training schemes for\nan AI/ML regression problem called deep pose regressor.\nFor such a regressor, the authors proposed attentive imitation\nloss and attentive hint training [99] which are, respectively,\nmodiﬁcations of knowledge distillation and hint training.\nMoreover, the authors of [100] considered the scenario of an\nintelligent teacher providing a student – only during training –\nwith privileged information and introduced another emerging\nteacher-student AI/ML paradigm dubbed learning using priv-\nileged information [100]. Unifying learning using privileged\ninformation [100] and knowledge distillation [97], the authors\nof [111] proposed generalized distillation which is a teacher-\nstudent AI/ML paradigm broadly applicable in the context of\nmachines-teaching-machines [111].\nOn the other hand, apart from the contaminating Gaussian\nnoise or error vector, matrix-vector products model numerous\nproblems in network science and graph signal processing;\nnetwork neuroscience and brain physics; and wireless com-\nmunications and signal processing. Beginning with wireless\ncommunications and signal processing, matrix-vector products\nmodel – amongst others – the following problems: a MIMO\nreceived signal [112]; a received signal over a doubly se-\nlective orthogonal frequency division multiplexing (OFDM)\nchannel [113] and a doubly selective MIMO-OFDM channel\n[114]; a massively concurrent non-orthogonal multiple-access\n(MC-NOMA) [115] received signal; and a multi-carrier code\ndivision multiple access (MC-CDMA) [116], [117] – also\nknown as OFDM/CDMA – received signal. Continuing to\nnetwork science and graph signal processing, matrix-vector\nproducts model – among others – the following problems: non-\nparametric regression for graph-aware signal reconstruction;\njoint inference of signals and graphs; identiﬁcation of directed\ngraph topologies and tracking of dynamic networks [118],\n[119]; and graph Fourier transforms [120].\nIn light of the highlighted teacher-student AI/ML paradigms\nand the prevalence of modeling using matrix-vector prod-\nucts, teacher deep ReLU FNNs ΦT\nD,ε(W , x) – fed with\n[(vec(W ))T , xT ]T for W ∈Rm×n and x ∈Rn – that can\neffectively approximate W x ∈Rm can be deployed in the\ntraining of lightweight (compressed) student ReLU FNNs that\ncan perform as good as teacher ReLU FNNs. In this vein,\nthree major AI/ML problems arise: modeling W (e.g., MIMO\nchannel modeling), estimating W (e.g., blind estimation of\nan OFDM channel), and inferring x (e.g., MIMO and OFDM\nsignal detection) using ReLU FNNs. Beginning with ReLU\nFNN based inference of x, once a teacher deep ReLU FNN\nΦT\nD,ε( ˜\nW , x) – for ˜\nW = diag(1, . . . , 1) ∈{0, 1}m×n – is suc-\ncessfully trained, a lightweight student ReLU FNN ΦS\nD,ε(x)\ncan be trained using knowledge distillation [97] or hint training\n[98] by exploiting, respectively, the softened output of the\nteacher output (as well as the hard training labels of the\nstudent regarding x) and the hint layer of the teacher. Con-\ntinuing to the estimation of W using teacher-student ReLU\nFNNs, if a teacher deep ReLU FNN ΦT\nD,ε(W x, xT (xxT )−1)\nis successfully trained, a lightweight student ReLU FNN\nΦS\nD,ε(W x) can be trained using attentive imitation loss or\nattentive hint training [99]. On modeling W using teacher-\nstudent ReLU FNNs, furthermore, in case a teacher deep\nReLU FNN ΦT\nD,ε(W , x) is successfully trained, a lightweight\nstudent ReLU FNN ΦS\nD,ε(x) can be trained using attentive\nimitation loss or attentive hint training [99].\nThe success of the aforementioned three solutions – based\non teacher-student AI/ML paradigms – depends on the\nsuccessful training of matrix-vector product approximating\nteacher deep ReLU FNNs. Such a deep ReLU FNN’s training\ncan be guided by an optimal deep ReLU FNN architecture\nwhose depth, connectivity, maximum width, the maximum\nabsolute value of the weights, and the number of neurons\nare all predicted by a theory. Such a theory will help ease\nthe considerable training difﬁculty of teacher deep ReLU\nFNNs. This is in line with the overarching goal of ToDL\nresearch which aims to identify the critical ingredients that\ncontribute to successful training of deep neural networks [46].\nAccordingly, a deep approximation theory on error bounds of a\nmatrix-vector product approximation using deep ReLU FNNs\nis needed. By the same token, facilitating an understanding on\na deep representation which is one of the three fundamental\nproblems on ToDL [46], [47], quantiﬁcation of error bounds on\na ReLU FNN based matrix-vector product approximation in-\n6\nspires much more research toward an interpretable/explainable\nAI/ML.\nB. Problem Formulation\nLet W\n∈\nRm×n and x\n∈\nRn be a matrix and a\nvector whose product is going to be approximated by a\nReLU FNN ΦD,ε fed with [(vec(W ))T , xT ]T – denoted as\nΦD,ε\n\u0000W , x\n\u0001\n. Suppose wT\ni\n:= (W (i, :))T\n∈Rn. Then,\nW = [wT\n1 , . . . , wT\nm]T and\nW x = [w1x, . . . , wmx]T ∈Rm.\n(13)\nRegarding\nthe\nproduct\nin\n(13)\nand\nits\napproximation\nΦD,ε\n\u0000W , x\n\u0001\n, we are going to derive the error bounds for\n∥ΦD,ε(W , x) −W x∥L∞([−D,D]2n;Rm) and ∥ΦD,ε(W , x) −\nW x∥W1,∞((−D,D)2n;Rm).\nFor a complex matrix W ∈Cm×n and a complex vector\nx ∈Cn, we are also going to derive an error bound for\n∥Φ\nc\nD,ε(W1,2, x1,2) −p1,2∥L∞([−D,D]2n;R2m), where W1,2 :=\n[W1, W2] ∈Rm×2n for W1 := Re{W } ∈Rm×n and\nW2 := Im{W } ∈Rm×n; x1,2 := [xT\n1 , xT\n2 ]T\n∈R2n\nfor x1\n:=\nRe{x}\n∈\nRn and x2\n:=\nIm{x}\n∈\nRn;\np1,2 := [pT\n1 , pT\n2 ]T ∈R2m for p1 := Re{W x} ∈Rm and\np2 := Im{W x} ∈Rm; and Φ\nc\nD,ε(W1,2, x1,2) is a ReLU\nFNN fed with [(vec(W1,2))T , xT\n1,2]T .\nIV. THE DEVELOPED DEEP APPROXIMATION THEORY\nA. Error Bounds in Lebesgue Norms\nFirst, we state the following proposition proved using the\n“sawtooth” construction of [121] and [122].\nProposition 2 ( [87, Proposition III.1]). For all ε ∈(0, 1/2)\nand ρ(x) := max(x, 0), there exist a constant C > 0 and a\nFNN Φsq\nε\n∈N N 1,1\n∞,∞,ρ satisfying L(Φsq\nε ) ≤C log2(ε−1),\nW(Φsq\nε ) = 4, B(Φsq\nε ) ≤4, Φsq\nε (0) = 0, and ∥Φsq\nε (x) −\nx2∥L∞([0,1]) ≤ε.\nProof. The proof is provided as supplementary material.\nAn economic implementation of this approximating ReLU\nFNN is also possible as stated in [123, Proposition III.2].\nBuilding from Proposition 2, the following theorem follows.\nTheorem 1. Let D ∈R+, W ∈Rm×n, and x ∈Rn. For all\nε ∈(0, 1/2) and ρ(x) := max(x, 0), there exists a constant\nC > 0 such that there is a FNN ΦD,ε ∈N N n(m+1),m\n∞,∞,ρ\nsatisfying L(ΦD,ε) ≤C log2(nD2ε−1), W(ΦD,ε) ≤12mn,\nB(ΦD,ε) ≤max{4, 2D2}, ΦD,ε(0, x) = ΦD,ε(W , 0) = 0\nand\n∥ΦD,ε(W , x) −W x∥L∞([−D,D]2n;Rm) ≤ε.\n(14)\nProof. The proof is provided in Appendix C.\nBased on Theorem 1, the following corollary follows.\nCorollary 2. Let D\n∈\nR+, W\n∈\nCm×n, x\n∈\nCn,\nW1 := Re{W } ∈Rm×n, W2 := Im{W } ∈Rm×n,\nx1\n:= Re{x} ∈Rn, and x2\n:= Im{x} ∈Rn. Let\np1 := Re{W x} ∈Rm, p2 := Im{W x} ∈Rm, p1,2 :=\n[pT\n1 , pT\n2 ]T\n∈R2m, W1,2 := [W1, W2] ∈Rm×2n, and\nx1,2 := [xT\n1 , xT\n2 ]T ∈R2n. For all ε ∈(0, 1/2) and ρ(x) :=\nmax(x, 0), there exists a constant C > 0 such that there\nis a FNN Φ\nc\nD,ε ∈N N 2n(m+1),2m\n∞,∞,ρ\nsatisfying L(Φ\nc\nD,ε) ≤\nC log2(4nD2ε−1),\nW(Φ\nc\nD,ε)\n≤\n48mn,\nB(Φ\nc\nD,ε)\n≤\nmax{4, 2D2}, Φ\nc\nD,ε(0, x1,2) = Φ\nc\nD,ε(W1,2, 0) = 0 and\n∥Φ\nc\nD,ε\n\u0000W1,2, x1,2\n\u0001\n−p1,2∥L∞([−D,D]2n;R2m) ≤ε.\n(15)\nProof. The proof is relegated to Appendix D.\nRemark 1. Theorem 1 and Corollary 2 afﬁrm that ε →0 at\nan exponential rate provided that L(ΦD,ε), L(Φ\nc\nD,ε) →∞.\nTherefore, as ReLU FNNs get much deeper, they would result\nin the lowest approximation error.\nB. Error Bounds in Sobolev Norms\nFirst, we state the proposition proved in [88, p. 29-30].\nProposition 3 ( [88, Proposition C.1]). For all ε ∈(0, 1/2)\nand ρ(x) := max(x, 0), there exist constants C1, C2, C3, C4 >\n0 such that there is a FNN Φsq\nε\n∈\nN N 1,1\n∞,∞,ρ with\nM(Φsq\nε ) ≤C1 log2(ε−1), L(Φsq\nε ) ≤C2 log2(ε−1), N(Φsq\nε ) ≤\nC3 log2(ε−1), and Φsq\nε (0) = 0 such that\n∥Φsq\nε (x) −x2∥W1,∞((0,1);dx) ≤ε\n(16a)\n|Φsq\nε |W1,∞((0,1)) ≤C4.\n(16b)\nBased on Proposition 3, the following result follows.\nProposition 4 ( [88, Proposition C.2]). For all ε ∈(0, 1/2),\nD ∈R+, and ρ(x) := max(x, 0), there exist constants\n¯C, C1 > 0 and C2 = C2(D) > 0 and Φ\n˜×(w,x)\nD,ε\n∈N N 2,1\n∞,∞,ρ:\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥W1,∞((−D,D)2;dwdx) ≤ε\n(17a)\nΦ\n˜×(w,x)\nD,ε\n(0, x) = Φ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈R\n(17b)\n|Φ\n˜×(w,x)\nD,ε\n|W1,∞((−D,D)2) ≤¯CD\n(17c)\nL(Φ\n˜×(w,x)\nD,ε\n), M(Φ\n˜×(w,x)\nD,ε\n), N\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤C1 log2(ε−1)+C2.\n(18)\nProof. The proof is provided in Appendix A-B.\nBuilding from Proposition 4, the following theorem follows.\nTheorem 2. Let D ∈R+, W ∈Rm×n, and x ∈Rn.\nFor all ε ∈(0, 1/2) and ρ(x) := max(x, 0), there exist\nconstants ¯¯C1, ¯¯C2 > 0 such that there is a FNN ΦD,ε ∈\nN N n(m+1),m\n∞,∞,ρ\nsatisfying L(ΦD,ε) ≤\n¯¯C1 log2(ε−1) + ¯¯C2,\nM(ΦD,ε) ≤¯¯C1 log2(ε−1)+ ¯¯C2, N(ΦD,ε) ≤¯¯C1 log2(ε−1)+\n¯¯C2, ΦD,ε(0, x) = ΦD,ε(W , 0) = 0 and\n∥ΦD,ε(W , x) −W x∥W1,∞((−D,D)2n;Rm) ≤ε.\n(19)\nProof. The proof is deferred to Appendix E.\n7\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n105\n0.5\n1\n1.5\n2\n10-3\nFig. 1: Training loss and testing error of the deep ReLU product FNN characterized by Theorem 1: K = 10 and ε2 = 2−10.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n105\n0.5\n1\n1.5\n2\n10-3\nFig. 2: Training loss and testing error of the deep ReLU product FNN characterized by Theorem 1: K = 10 and ε2 = 2−10.\nV. COMPUTER EXPERIMENTS\nThis section presents the procedures and empirical results\nof our computer experiments conducted to assess the accuracy\nof a matrix-vector product approximation using deep ReLU\nFNNs. The deep ReLU FNNs’ approximations that have been\nexperimented are a real and a complex matrix-vector product\napproximations as quantiﬁed by Theorem 1 and Corollary 2,\nrespectively.\nCarried out on the NIST GPU (graphics processing unit)\ncluster dubbed ENKI, KERAS [124] with TENSORFLOW\n[125] as a backend was used for the ofﬂine training and online\ntesting of deep ReLU FNNs approximating both real and com-\nplex matrix-vector products. Our matrix-vector product ap-\nproximating network training employed the (hyper)parameters\nlisted in Table I. Besides Table I’s (hyper)parameters, we\ndeployed four KERAS callbacks: model checkpointing, Ten-\nsorBoard, early stopping, and learning rate reduction [126,\nCh. 7]. Regarding the latter two, early stopping and learning\nrate reduction callbacks were set to have patience over twenty\nepochs. Per twenty epochs that render performance stagnation,\nlearning rate reduction callback was set to reduce the learning\nrate by 0.1. Moreover, training and testing datasets were\ngenerated in MATLAB and then uploaded into ENKI.\nA. Real Matrix-Vector Product Approximation\nTo verify and interpret the theory predicted by Theo-\nrem 1, we generated real testing and training datasets. The\ntraining dataset D := {(ai, bi)}ˇn\ni=1 – ˇn = 106, ai =\n[(vec(Wi))T , xT\ni ]T ∈Rn(m+1), and bi = Wixi ∈Rm – was\nformed from 210+1 equispaced points. From these equispaced\npoints generated over [−2, 2], speciﬁcally, all elements of Wi\nand xi were randomly drawn. Besides, we formed four testing\nTABLE I: (Hyper)parameters unless otherwise mentioned.\n(Hyper)parameters\nType/Value\nRemark(s)\n(m, n)\n(8, 4)\nChosen dimensions\nLearning rate\n0.001\nInitial value\nEpoch size\n200\nMaximum epoch size.\nBatch size\n200\nVary large and small batch\nsizes have also been tried.\nOptimizer\nAdam [127]\nAdam produced the least\nMSE and converged fast.\nActivation\nReLU\nThe focus of this work has\nfunction\nbeen on ReLU FNNs.\nDepth of FNNs\n10\nChosen in line with the\n(K)\ndeveloped theory\nTraining-validation\n75% to 25%\n75% of the training dataset\nsplit\nis used for training; 25% of\nit is used for validation.\nInitialization for\nHe normal\nWk(i, :) ∼N(0, σ2INk−1),\nall layers\n[128]\nσ2 = 2/Nk−1 and i ∈[Nk].\nKernel\nmax norm(8)\nDeduced from our developed\nconstraint\ntheory (valid for all layers).\ndatasets Tj := {(ai,j, bi,j)}˘nj\ni=1 – ˘nj is the size of the j-\nth testing dataset, ai,j = [(vec(Wi,j))T , xT\ni,j]T ∈Rn(m+1),\nbi,j = Wi,jxi,j ∈Rm, and j ∈[4] – from 29 + 1 equispaced\npoints. From these equispaced points generated within [−2, 2],\nall elements of Wi,j and xi,j were randomly chosen.\nSetting ε = 2−5, Theorem 1 predicts that the mean squared\nerror (MSE) exhibited by an approximating FNN (ΦD,ε) is\nupper bounded by ε2 = 2−10 such that L(ΦD,ε) ≤9C,\nW(ΦD,ε) ≤384, and B(ΦD,ε) ≤8 – w.r.t. (m, n, D) =\n(8, 4, 2). Thus, choosing C = 2 leads to L(ΦD,ε) ≤18.\nToward this end, we trained a deep ReLU FNN ΦD,ε with\nMSE as a loss function, per-layer neurons N0 = 36, N1 =\n. . . = N9 = 384, and N10 = 8, and L(ΦD,ε) = 10 –\na depth-10 deep network. Despite deep networks’ power of\nexponential expressivity, training them is very difﬁcult; cf.\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n105\n10-5\n10-4\n10-3\nFig. 3: Training loss and testing error of the complex product deep ReLU FNN per Corollary 2: K = 10 and ε2 = 2−10.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n105\n10-5\n10-4\n10-3\nFig. 4: Training loss and testing error of the complex product deep ReLU FNN per Corollary 2: K = 10 and ε2 = 2−10.\nSec. III-A. Guided by Theorem 1, however, we succeeded\nat training a deep ReLU FNN, as shown in Figs. 1 and 2.\nFigs. 1 and 2 – on their left sides – show the lowest MSE\ntraining losses observed at the 200-th epoch and, respectively,\nequal to 6.3456×10−4 and 8.2080×10−4. Manifesting these\ntraining losses, the trained deep ReLU FNNs were tested with\nfour different size testing datasets as shown, respectively, on\nthe right sides of Figs. 1 and 2. These plots corroborate that\nthe approximation MSE exhibited by the trained deep ReLU\nFNNs is less than ε2 = 2−10. This validates Theorem 1.\nB. Complex Matrix-Vector Product Approximation\nTo validate and interpret the theory predicted by Corollary\n2, we considered the reception of QPSK (quadrature phase\nshift keying) symbols xi\n:=\n[xi,1, . . . , xi,n]T\n∈\nCn –\nRe{xi,j}, Im{xi,j}\n∈\n{−1/\n√\n2, 1/\n√\n2} – via a MIMO\nchannel Hi ∈Cm×n [112]. Such a channel is modeled\nby a Rayleigh fading, i.e., Hi\n∼\nCN(0, Im). Having\nimplemented\nHi\nand\nxi,\nwe\ngenerated\ntesting\nand\ntraining\ndatasets.\nThe\ntraining\ndataset\nwas\nformed\nas\nDc\n:=\n{(ci, di)}ˇn\ni=1,\nwhere\nˇn\n=\n106,\nci\n=\n[Re{(vec(Hi))T }, Im{(vec(Hi))T }, Re{xT\ni }, Im{xT\ni }]T\n∈\nR2n(m+1), and di\n=\n[Re{(Hixi)T }, Im{(Hixi)T }]T\n∈\nR2m.\nWe\nalso\ngenerated\nfour\ntesting\ndatasets\nT c\nj\n:=\n{(ci,j, di,j)}˘nj\ni=1,\nwhere\n˘nj\nis\nthe\nsize\nof\nthe\nj-th\ntesting\ndataset,\nci,j\n=\n[Re{(vec(Hi,j))T }, Im{(vec(Hi,j))T }, Re{xT\ni,j}, Im{xT\ni,j}]T\n∈\nR2n(m+1)\nfor\nHi,j\n∼\nCN(0, Im)\nand\nxi,j\ncomprising\nn\nQPSK\nsymbols,\ndi,j\n=\n[Re{(Hi,jxi,j)T }, Im{(Hi,jxi,j)T }]T ∈R2m, and j ∈[4].\nSince the realizations of X ∼N(0, 1) fall within [−3, 3]\nwith a probability equals to 0.99736 and a QPSK symbol is\ndrawn from {−1/\n√\n2, 1/\n√\n2}, all elements of ci and ci,j fall –\nwith high probability – within [−3/\n√\n2, 3/\n√\n2]. Consequently,\nCorollary 2 can be exploited to characterize the approximating\nFNN (Φ\nc\nD,ε) – with an MSE bound equals to ε2 – for the afore-\nmentioned complex product Hx over [−D, D] = [−3, 3].\nThus, Corollary 2 predicts w.r.t. (m, n, C, ε) = (8, 4, 1, 2−5)\nthat L(Φ\nc\nD,ε) ≤13, W(Φ\nc\nD,ε) ≤1536, B(Φ\nc\nD,ε) ≤18, and an\napproximation MSE bound equals to 2−10. Guided by these\nparameters, we successfully trained a depth-10 ReLU FNN\nΦ\nc\nD,ε with N0 = 72, N1 = 1536, N2 = . . . = N9 = 384,\nand N10 = 16, as demonstrated on the left sides of Figs. 3\nand 4. Figs. 3 and 4 – on their left sides – show the lowest\nMSE training losses obtained at the 200-th epoch and, respec-\ntively, equal to 8.88643217 × 10−6 and 7.22278731 × 10−6.\nExhibiting these MSE training losses, the trained deep ReLU\nFNNs were tested with four different size testing datasets as\nshown, respectively, on the right sides of Figs. 3 and 4. These\nplots demonstrate that the approximation MSE exhibited by\nthe trained deep ReLU FNNs is less than ε2 = 2−10. This\nsubstantiates Corollary 2.\nVI. APPLICATIONS\nAs validated in Sec. V, our developed theory on approximat-\ning FNNs for real and complex matrix-vector products serves\nto guide and ease the training of teacher deep ReLU FNNs\nwhich can, in turn, be employed to train lightweight student\nFNNs in the context of teacher-student AI/ML paradigms.\nToward this end, the underneath applications follow.\n6If xi is the realization of X, P(xi ∈[−3, 3]) = p = F(3) −F(−3) for\nF(x) being the CDF (cumulative distribution function) of the standard normal\ndistribution. In MATLAB, p = normcdf(3) −normcdf(−3) = 0.9973.\n9\n1) Wireless Communications and Signal Processing Appli-\ncations: Corollary 2 can guide the training of teacher deep\nReLU FNNs Φ\nc\nD,ε which will be deployed to train lightweight\nstudent ReLU FNNs using hint training [98] or attentive\nhint training [99] for blind detection of a MIMO received\nsignal; blind estimation of a doubly selective (MIMO-)OFDM\nchannel; blind detection of a (MIMO-)OFDM signal received\nthrough a doubly selective (MIMO-)OFDM channel; blind\ndetection of a MC-NOMA received signal and a MC-CDMA\nreceived signal; blind estimation of a doubly selective MC-\nCDMA channel; and blind estimation of a triply-selective\nMIMO channel [129].\n2) Network Science and Graph Signal Processing Applica-\ntions: Theorem 1 can guide the training of teacher deep ReLU\nFNNs ΦD,ε which will be used to train lightweight student\nReLU FNNs using hint training [98] or attentive hint training\n[99] for joint inference of signals and graphs; non-parametric\nregression for graph-aware signal reconstruction; identiﬁcation\nof directed graph topologies and tracking of dynamic networks\n[118], [119]; and graph Fourier transforms [120].\n3) Network Neuroscience and Brain Physics Applications:\nbeing one of the various interdisciplinary ﬁelds that constitute\nbrain physics [130], network neuroscience aims to build a\nfundamental mechanistic understanding of how neuron level\nprocesses contribute to the structure and function of large-scale\ncircuits, brain systems, and whole-brain structure and function\n[102]. It also inquires about perception-action coupling, brain-\nbehavior interactions, and social networks [102], [103].\nTo model brain network structure, an adjacency matrix is\nconstructed from experimental data specifying the physical\ninterconnections between neurons or brain regions [103, Fig. 1,\np. 8] [131]. To model brain network function, a similarity ma-\ntrix is made from experimental data on the activity of neurons\nor brain regions; e.g., fMRI (functional magnetic resonance\nimaging) data of blood oxygen level in different parts of the\nbrain [103, Fig. 2, p. 17]. If a similarity or adjacency matrix\nis denoted by W ∈Rm×m, diag(W , . . . , W )vec(Im) – for\ndiag(W , . . . , W ) ∈Rm2×m2 – can be approximated using a\ndeep ReLU FNN ΦD,ε, as asserted by Theorem 1. Theorem\n1 can, thus, guide the training of teacher deep ReLU FNNs\nΦD,ε which will be used to train – using attentive hint training\n[99] – lightweight student ReLU FNNs. The trained student\nReLU FNNs can then be concatenated and trained with AEs\nfor an efﬁcient dimensionality that may offer further insights\ninto the brain’s structural or functional networks.\nVII. CONCLUDING REMARKS AND RESEARCH OUTLOOK\nRegarding a matrix-vector product approximation with deep\nReLU FNNs, we derived error bounds – in Lebesgue and\nSobolev norms – that constitute our developed deep approxi-\nmation theory. Guided by this theory, we successfully trained\ndeep ReLU FNNs whose test outcomes justify the validity\nof our deep approximation theory. This deep approximation\ntheory is also important for guiding and easing the training\nof teacher deep ReLU FNNs in consideration of the emerging\nteacher-student AI/ML paradigms that are essential for solving\nseveral AI/ML problems in wireless communications and sig-\nnal processing; network science and graph signal processing;\nand network neuroscience and brain physics.\nIn the context of ToDL, this paper inspires numerous lines\nof research including high-dimensional signal processing that\nmight be pursued through the lenses of high-dimensional\nprobability [91], high-dimensional statistics [92], and empir-\nical process theory [93]; deep ReLU FNNs based tighter\nerror bounds for a matrix-vector product approximation that\nmight be derived using sparse grids (following the lead of\n[132]); and error bounds for a matrix-matrix, a tensor-matrix,\nand a tensor-tensor product approximation – applicable in\ninterference excision and channel estimation [133], and brain\nimaging [134] – with deep ReLU FNNs. Furthermore, this\npaper inspires much more applied research on the novel\ntraining of lightweight student models – critical for resource-\nconstrained environments – given the emerging teacher-student\nAI/ML paradigms.\nAPPENDIX A\nPROOFS OF KNOWN RESULTS USED\nFor the sake of clarity, completeness, and rigor, we hereby\nprovide our proofs of the known results that informed our\ntheoretical developments in Appendices C, D, and E.\nA. Proof of Lemma 3\nThe proof of Lemma 3 was highlighted in [87, p. 6] through\nthe proof of [87, Lemma II.6]. We herein provide our proof.\nConsidering K = maxi Ki is the depth of the deepest\nFNN,\nwe\nﬁrst\nmake\nall\nFNNs\nΦi\n∈\nN N di,1\nKi,Mi,ρ\nto\nbe\nof\ndepth\nK.\nThis\ncan\nbe\nachieved\nvia\nthe\nconcatenation\nof\nI-FNN\nΦI1\nK−Ki+1\nand\nΦi\nwhen\nKi\n̸=\nK, where it follows through Deﬁnition 2 and\nLemma\n1\nthat\nΦi\n=\n\u0002\n[W i\n1, bi\n1], . . . , [W i\nKi, bi\nKi]\n\u0003\nand\nΦI1\nK−Ki+1\n=\n\u0002\n[[1, −1]T , 0],\n(K−Ki−1) times\nz\n}|\n{\n[I2, 0], . . . , [I2, 0], [[1, −1], 0]\n\u0003\n.\nEmploying these expressions in (4) for K −Ki −1 > 0 gives\nΦ′\ni ≡ΦI1\nK−Ki+1 • Φi =\n\u0002\n[W i\n1, bi\n1], . . . , [W i\nKi−1, bi\nKi−1],\n[f\nW i\nKi,ebi\nKi],\n(K−Ki−1) times\nz\n}|\n{\n[I2, 0], . . . , [I2, 0], [[1, −1], 0]\n\u0003\n,\n(20)\nwhere Φ′\ni(x) = (ΦI1\nK−Ki+1 • Φi)(x) = Φi(x), f\nW i\nKi =\n[(W i\nKi)T , −(W i\nKi)T ]T , and ebi\nKi = [(bi\nKi)T , −(bi\nKi)T ]T . For\nK −Ki −1 = 0, (20) leads to Φ′\ni ≡ΦI1\nK−Ki+1 • Φi:\nΦ′\ni =\n\u0002\n[W i\n1, bi\n1], . . . , [f\nW i\nKi,ebi\nKi], [[1, −1], 0]\n\u0003\n.\n(21)\nMeanwhile, it is deduced via (20) and (21) that Φi(xi) =\nΦ′\ni(xi) ∈N N di,1\nK,Mi+W(Φi)+2(K−Ki)+1,ρ, i ∈[n]. Conse-\nquently, Φ1(x) = [a1Φ′\n1(x1), . . . , anΦ′\nn(xn)]T and Φ2(x) =\nPn\ni=1 aiΦ′\ni(xi) = [1, . . . , 1]Φ1(x) that are the parallelization\nand superposition of the outputs of n Φ′\nis, respectively. There-\nfore, L(Φ1) = L(Φ2) = K; M(Φ1) = M = Pn\ni=1(Mi +\nW(Φi) + 2(K −Ki) + 1); M(Φ2) = M + n; W(Φ1) =\nW(Φ2) ≤Pn\ni=1 max{2, W(Φi)}; Φ1 ∈N N d,n\nK,M,ρ; and\nΦ2 ∈N N d,1\nK,M+n,ρ. Since Φ1 and Φ2 are constructed from\nΦ′\nis deﬁned via (20) or (21), they are made up of the weights\nof Φi, i ∈[n], ±1’s, and {a1, . . . , an}.\n■\n10\nB. Proof of Proposition 4\nA similar proof is provided in [88, p. 31-33]. We present\nour proof which our developments of Appendix E are based\nupon.\nW.r.t. the identity wx = 1\n2\n\u0000(w+x)2−w2−x2\u0001\nfor x, y ∈R,\nwx = 4D2\u0000w/2D\n\u0001\u0000x/2D\n\u0001\nis also equated as\nwx = 2D2\u0010\u0010|w + x|\n2D\n\u00112\n−\n\u0010|w|\n2D\n\u00112\n−\n\u0010 |x|\n2D\n\u00112\u0011\n.\n(22)\nIn line with (22) and the identity |x| = ρ(x) + ρ(−x),\n[121, Proposition 3] asserts that Φ\n˜×(w,x)\nD,ε\ncan be expressed\nin terms of a square ReLU FNN Φsq\nδ which is characterized\nin Proposition 3. Consequently,\nΦ\n˜×(w,x)\nD,ε\n(w, x) = 2D2\u0000Φsq\nδ (γ|w+x|)−Φsq\nδ (γ|w|)−Φsq\nδ (γ|x|)\n\u0001\n,\n(23)\nwhere γ =\n1\n2D and it follows through (16a) that ∥Φsq\nδ (x) −\nx2∥W1,∞((0,1);dx) ≤δ.\nMeanwhile, implementing |w + x|, |w|, and |x| requires\n3 additional layers. Hence, employing this fact and [121,\nProposition 3], L\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤C0 log2(ε−1)+C0 log2(6D2)+\n3 ≤C0 log2(ε−1) + C1, where C0 is a constant that em-\nanates from the depth constraint of Proposition 3. As there\nwould be 9 additional neurons in the implementations of the\nabsolute value expressions of (23), [121, Proposition 3] also\nasserts that N\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤3C0 log2(ε−1)+3C0 log2(6D2)+\n9 ≤C′ log2(ε−1) + C2. Moreover, since there would be\n17 additional non-zero weights to implement |w + x|, |w|,\nand |x| using the aforementioned additional three layers,\nM\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤3C0 log2(ε−1) + 3C0 log2(6D2) + 17 ≤\nC′′ log2(ε−1) + C3. As C′, C′′\n>\n0, choosing C1\n=\nC1(D), C2 = C2(D), C3 = C3(D) > 0 satisfy (18). Em-\nploying (23), Φ\n˜×(w,x)\nD,ε\n(0, x) = Φ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈R.\nThus, (17b) is satisﬁed.\nTo continue, we deﬁne the following transformations:\ntw\n:\n(−D, D)2\n→\n(0, 1), (w, x)\n7→\n|w|/2D; tx\n:\n(−D, D)2 →(0, 1), (w, x) 7→|x|/2D; twx : (−D, D)2 →\n(0, 1), (w, x) 7→|w + x|/2D; and f : (0, 1) →R, x 7→x2.\nUsing these transformations, (22) and (23) can be, respectively,\nexpressed as wx = 2D2\u0000f ◦twx −f ◦tw −f ◦tx\n\u0001\nand\nΦ\n˜×(w,x)\nD,ε\n(w, x) = 2D2\u0000Φsq\nδ ◦twx −Φsq\nδ ◦tw −Φsq\nδ ◦tx\n\u0001\n. Using\nthese transformations, collecting similar terms, and applying\nthe properties – in line with Deﬁnition 4 and (8) – of L∞-\nnorm:\n∥Φ\n˜×(w,x)\nD,ε\n(w, x)−wx∥W1,∞((−D,D)2;dwdx) ≤2D2\nX\nu∈{w,x,wx}\n∥(Φsq\nδ −f) ◦tu∥W1,∞((−D,D)2)\n(a)\n≤2D2C\nX\nu∈{w,x,wx}\nmax\n\b\n∥Φsq\nδ −f∥L∞((0,1)2), |Φsq\nδ −f|W1,∞((0,1)2)|tu|W1,∞((−D,D)2)\n\t\n(b)\n≤2D2C\nX\nu∈{w,x,wx}\nmax\n\b\r\rΦsq\nδ −f\n\r\r\nL∞((0,1)2), (2D)−1×\n|Φsq\nδ −f|W1,∞((0,1)2)\n\t\n≤6D2C∥Φsq\nδ −x2∥L∞((0,1)2).\n(24)\nwhere (a) follows from Corollary 1 via (12b) and (b) is due\nto Deﬁnition 6 – via (10) – that gives |tu|W1,∞((−D,D)2) =\n1/2D. Deploying Proposition 2,\n\r\rΦsq\nδ (x)−x2\r\r\nL∞((0,1)2) ≤δ.\nThus, setting ε = 6D2Cδ, (24) leads to (17a).\nUsing Deﬁnition 6 – via (10) – in the above-mentioned\nexpression of Φ\n˜×(w,x)\nD,ε\n(w, x) and applying (12a),\n|Φ\n˜×(w,x)\nD,ε\n|W1,∞((−D,D)2) ≤2D2C\nX\nu∈{w,x,wx}\n|Φsq\nδ (x)|W1,∞((0,1))|tu|W1,∞((−D,D)2)\n(a)\n≤3CC4D,\n(25)\nwhere (a) follows from (16b) and |tu|W1,∞((−D,D)2)\n=\n1/2D. Choosing\n¯C\n=\n3CC4\n>\n0, (25) asserts that\n|Φ\n˜×(w,x)\nD,ε\n|W1,∞((−D,D)2) ≤¯CD. This proves (17c).\n■\nC. Proof of Proposition 5\nA similar proof is available in [87, p. 8-9]. We henceforth\nprovide our proof which is going to exploit Lemma 4.\nW.r.t. (22) and the identity |x| = ρ(x)+ρ(−x), [121, Propo-\nsition 3] guarantees that Φ\n˜×(w,x)\nD,ε\ncan be expressed via the\nsquare FNN Φsq\nδ characterized in Proposition 2. Consequently,\nΦ\n˜×(w,x)\nD,ε\n(w, x) = 2D2\u0000Φsq\nδ (γ|w + x|) −\nX\nu∈{|w|,|x|}\nΦsq\nδ (γu)\n\u0001\n,\n(26)\nwhere γ = 1/2D. Using (26) and Lemma 3, the parameters of\nΦ\n˜×(w,x)\nD,ε\ncan be deduced from the parameters of Φsq\nδ stated in\nProposition 2. Thus, W\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤P3\ni=1 max\n\b\n2, 4\n\t\n= 12;\nB\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤max{4, 2D2}, as (26) can be implemented by\nmultiplying the square FNNs’ outputs by 2D2 and adding;\nand L\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤C′ log2(δ−1), as asserted by Proposition\n2. It also follows directly from (26) that Φ\n˜×(w,x)\nD,ε\n(0, x) =\nΦ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈R.\nMeanwhile, we deﬁne the following transformations: tw :\n[−D, D]2 →[0, 1], (w, x) 7→|w|/2D; tx : [−D, D]2 →\n[0, 1], (w, x) 7→|x|/2D; twx : [−D, D]2 →[0, 1], (w, x) 7→\n|w + x|/2D; and f : [0, 1] →R, x 7→x2. Deploying these\ntransformations, (22) and (26) can be, respectively, equated as\nwx = 2D2\u0000f ◦twx −f ◦tw −f ◦tx\n\u0001\nand Φ\n˜×(w,x)\nD,ε\n(w, x) =\n2D2\u0000Φsq\nδ ◦twx−Φsq\nδ ◦tw−Φsq\nδ ◦tx\n\u0001\n. Employing these relations\nin the left-hand side of (32), collecting similar terms, and\napplying the properties of L∞-norm produce the expression\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥L∞([−D,D]2) ≤\n2D2\nX\nu∈{w,x,wx}\n∥\n\u0000Φsq\nδ −f\n\u0001\n◦tu∥L∞([−D,D]2)\n(a)\n≤2D2\nX\nu∈{w,x,wx}\n∥Φsq\nδ (x) −x2∥L∞(tu([−D,D]2)),\n(27)\nwhere (a) follows from Lemma 4 through (7). From the above-\ndeﬁned transformations, tu([−D, D]2) = [0, 1]. Consequently,\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥L∞([−D,D]2)\n6D2\n≤∥Φsq\nδ (x)−x2∥L∞([0,1]).\n(28)\nUsing Proposition 2, ∥Φsq\nδ (x)−x2∥L∞([0,1]) ≤δ. Accordingly,\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥L∞([−D,D]2) ≤ε,\n(29)\n11\nwhere we chose ε = 6D2δ. This leads to (32) and the depth\nconstraint L\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤C′ log2(δ−1) ≤C log2(D2ε−1),\nwhere C is chosen as C >> C′ log2 6.\n■\nAPPENDIX B\nPROOF OF PROPOSITION 1\nExpressed through a ReLU activation function applied\ncomponent-wise, x = ρ(x)−ρ(−x) [87]. Exploiting this iden-\ntity for A = W x, A = Imρ(A) −Imρ(−A). Accordingly,\nA = Imρ(W x) −Imρ(−W x).\nRealizing that ρ(W x) is a transformation with x as the\ninput of a ReLU FNN, W\nas the weight matrix of a\nReLU FNN, and ρ(·) as an element-wise ReLU activation,\nρ(W x) is represented by N N n,m\n1,M ′,ρ with M ′ = ∥W ∥ℓ0.\nW.r.t. this 1-layer ReLU FNN, Imρ(W x) can be realized as\nadding a linear operation on every corresponding output, i.e.,\nmultiplying every respective output by 1 w.r.t. an additional\nlayer having no activation function. Hence, Imρ(W x) can\nbe represented by N N n,m\n2,M1,ρ with M1 = ∥W ∥ℓ0 + m\nand no activation at the output layer. By the same token,\n−Imρ(−W x) can also be represented by N N n,m\n2,M2,ρ with\nM2 = ∥−W ∥ℓ0 + m = ∥W ∥ℓ0 + m and also without any\nactivation function at the output layer. Similar to the concepts\nof [135, Lemma D.2], an addition operation using two FNNs\ncan be conceived as putting two ReLU FNNs parallelly and\nadding the outputs of the corresponding output neurons. Doing\nso results in a 2-layer ReLU FNN with a hidden layer double\nthe size of the ReLU FNNs representing the summands. As\na result, A = Imρ(W x) −Imρ(−W x) can be represented\nby N N n,m\n2,M,ρ with M = 2∥W ∥ℓ0 + 2m and without any\nactivation at the output layer. This completes the proof of 1).\nUtilizing\nthe\nresult\nof\n1),\nlet\nΦ1\n:=\n\u0002\n[[W T , −W T ]T , 0], [[Im, −Im], 0]\n\u0003\n∈\nN N n,m\n2,M,ρ\nand\nΦ2 := ΦIn\nK−1, an I-FNN deﬁned via (5). As ΦIn\nK−1(x) = x,\nA\n=\nA(ΦIn\nK−1)\n=\nΦ1(ΦIn\nK−1)\n=\nΦ1 • ΦIn\nK−1, the\nconcatenation of Φ1 and ΦIn\nK−1. To this end, deploying (4)\nand (5) leads to\nΦ1 • ΦIn\nK−1 =\n\u0002\n[[In, −In]T , 0],\n(K−1)−2 times\nz\n}|\n{\n[I2n, 0], . . . , [I2n, 0],\n[[B1, B2], 0], [[Im, −Im], 0]\n\u0003\n,\n(30)\nwhere B1 = [W T , −W T ]T and B2 = [−W T , W T ]T .\nHence, L(Φ1 • ΦIn\nK−1) = K and M(Φ1 • ΦIn\nK−1) = 2m +\n2(K −2)n+4∥W ∥ℓ0. Accordingly, A can also be represented\nby N N n,m\nK,M,ρ with M = 2m+2(K −2)n+4∥W ∥ℓ0 and no\nactivation at the output layer. This completes the proof of 2).\nSimilarly, if we let Φ1 := ΦIm\nK−1 – an I-FNN deﬁned\nvia (5) – and Φ2 :=\n\u0002\n[[W T , −W T ]T , 0], [[Im, −Im], 0]\n\u0003\n∈\nN N n,m\n2,M,ρ, then A = ΦIm\nK−1(A) = ΦIm\nK−1(Φ2) = ΦIm\nK−1 •\nΦ2, the concatenation of ΦIm\nK−1 and Φ2. Consequently, em-\nploying (4) and (5) gives\nΦIm\nK−1 • Φ2 =\n\u0002\n[[W T , −W T ]T , 0], [[I(A1), I(A2)], 0],\n(K−1)−2 times\nz\n}|\n{\n[I2m, 0], . . . , [I2m, 0], [[Im, −Im], 0]\n\u0003\n,\n(31)\nwhere I(A1) = [Im, −Im]T , I(A2) = [−Im, Im]T . To this\nend, L(ΦIm\nK−1 • Φ2) = K and M(ΦIm\nK−1 • Φ2) = 2Km +\n2∥W ∥ℓ0. Therefore, A can also be represented by N N n,m\nK,M,ρ\nwith M = 2Km + 2∥W ∥ℓ0 and no output activation. This\ncompletes the proof of 3).\n■\nAPPENDIX C\nPROOF OF THEOREM 1\nWe start by stating the following proposition.\nProposition 5. For all ε ∈(0, 1/2), D ∈R+, and ρ(x) :=\nmax(x, 0), there exists a constant C > 0 such that there\nis a FNN Φ\n˜×(w,x)\nD,ε\n∈N N 2,1\n∞,∞,ρ satisfying L(Φ\n˜×(w,x)\nD,ε\n) ≤\nC log2(D2ε−1),\nW(Φ\n˜×(w,x)\nD,ε\n)\n≤\n12,\nB(Φ\n˜×(w,x)\nD,ε\n)\n≤\nmax{4, 2D2}, Φ\n˜×(w,x)\nD,ε\n(0, x) = Φ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈R,\nand\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥L∞([−D,D]2) ≤ε.\n(32)\nProof. The proof is relegated to Appendix A-C.\nTo proceed further, we prove the following proposition.\nProposition 6. Let wT = [w1, . . . , wn]T ∈Rn and x =\n[x1, . . . , xn]T\n∈Rn. For all ε ∈(0, 1/2), D\n∈R+,\nand ρ(x) := max(x, 0), there exists a constant C > 0\nsuch that there is a FNN Φ\n˜×(w,x)\nD,ε\n∈\nN N 2n,1\n∞,∞,ρ sat-\nisfying L(Φ\n˜×(w,x)\nD,ε\n) ≤C log2(nD2ε−1), W(Φ\n˜×(w,x)\nD,ε\n) ≤\n12n, B(Φ\n˜×(w,x)\nD,ε\n)\n≤\nmax{4, 2D2}, Φ\n˜×(w,x)\nD,ε\n(0, x)\n=\nΦ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈Rn, and\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥L∞([−D,D]2n) ≤ε.\n(33)\nProof. Because wx = Pn\ni=1 wixi, this sum of products\ncan be realized via a ReLU FNN Φ\n˜×(w,x)\nD,ε\ncharacterized in\nProposition 5. Speciﬁcally, wx can be implemented via the\nsuperposition of n Φ\n˜×(w,x)\nD,ε\ns that are, respectively, fed with\ninput tuples (wi, xi), i ∈[n]. Consequently,\nΦ\n˜×(w,x)\nD,ε\n(w, x) =\nn\nX\ni=1\nΦ\n˜×(w,x)\nD,˜ε\n(wi, xi),\n(34)\nwhere\n∥Φ\n˜×(w,x)\nD,˜ε\n(wi, xi) −wixi∥L∞([−D,D]2)\n≤\n˜ε, ∀i,\nL(Φ\n˜×(wi,xi)\nD,˜ε\n) ≤C log2(D2˜ε−1), W(Φ\n˜×(wi,xi)\nD,˜ε\n) ≤12, and\nB(Φ\n˜×(wi,xi)\nD,˜ε\n) ≤max{4, 2D2}. Moreover, Φ\n˜×(wi,xi)\nD,˜ε\n(0, xi) =\nΦ\n˜×(wi,xi)\nD,˜ε\n(xi, 0) = 0, ∀xi ∈R, and i ∈[n]. Deploying these\nFNN constraints, we can infer from (34) that Φ\n˜×(w,x)\nD,ε\n(0, x) =\nΦ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈Rn. Besides, using Lemma 3 and\nusing the aforementioned constraints, W(Φ\n˜×(w,x)\nD,ε\n) ≤12n =\nPn\ni=1 max{2, 12} and B(Φ\n˜×(wi,xi)\nD,˜ε\n) ≤max{4, 2D2}, as (34)\ncan be implemented as a superposition of n parallelized FNNs\nwith an output weight matrix equated to [1, 1, . . . , 1] ∈R1×n.\nW.r.t. our subsequent analyses, we deﬁne these parameters\nfor D :=\n\b\n(w1, x1), . . . , (wi, xi), . . . , (wn, xn)\n\t\n, i ∈[n]:\ntwixi : [−D, D]2n →[−D, D]2, D 7→(wi, xi)\n(35a)\ng : [−D, D]2 →R, (w, x) 7→wx.\n(35b)\n12\nDeploying (35a)-(35b) in (34) and wx = Pn\ni=1 wixi,\n\u0000Φ\n˜×(w,x)\nD,ε\n(w, x), wx\n\u0001\n=\nn\nX\ni=1\n\u0000Φ\n˜×(w,x)\nD,˜ε\n, g\n\u0001\n◦twixi.\n(36)\nUsing (36) in the left-hand side of (33), collecting similar\nterms, and applying the properties of L∞-norm:\n∥Φ\n˜×(w,x)\nD,ε\n(w, x)−wx∥L∞([−D,D]2n) ≤\nn\nX\ni=1\n∥(Φ\n˜×(w,x)\nD,˜ε\n−g)\n◦twixi∥L∞([−D,D]2n)\n(a)\n≤\nn\nX\ni=1\n∥Φ\n˜×(w,x)\nD,˜ε\n−g∥L∞(twixi([−D,D]2n))\n(b)\n=\nn\nX\ni=1\n∥Φ\n˜×(w,x)\nD,˜ε\n(w, x) −wx∥L∞([−D,D]2)\n(c)\n=\nn\nX\ni=1\n˜ε,\n(37)\nwhere (a), (b), and (c) follow, respectively, from Lemma 4 via\n(7), (35a)-(35b), and (32) – w.r.t. the ˜ε constraint of Φ\n˜×(w,x)\nD,˜ε\n.\nRealizing the superposition of n parallelized FNNs per Lemma\n3 by choosing ε = n˜ε, L(Φ\n˜×(w,x)\nD,ε\n) ≤C log2(D2˜ε−1) =\nC log2(nD2ε−1). This ﬁnishes the proof of Proposition 6. ■\nEmploying Proposition 6 and Lemma 3 on the paralleliza-\ntion of FNNs, the proof of Theorem 1 resumes in the sequel.\nFrom (13), W x = [w1x, . . . , wmx]T ∈Rm. Per Proposi-\ntion 6, m FNNs are approximating the m afﬁne transforma-\ntions w.r.t. x and the weights w1, . . . , wm. Hence, it can be\ninferred from (33) that\n∥Φ\n˜×(w,x)\nD,ε\n(wi, x) −wix∥L∞([−D,D]2n) ≤ε,\n(38)\nwhere L(Φ\n˜×(w,x)\nD,ε\n) ≤C log2(nD2ε−1), W(Φ\n˜×(w,x)\nD,ε\n) ≤\n12n, B(Φ\n˜×(w,x)\nD,ε\n) ≤max{4, 2D2}, and Φ\n˜×(w,x)\nD,ε\n(0, x) =\nΦ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈Rn. Accordingly,\nW x ≈[Φ\n˜×(w,x)\nD,ε\n(w1, x), . . . , Φ\n˜×(w,x)\nD,ε\n(wm, x)]T ,\n(39)\nwhere (Φ\n˜×(w,x)\nD,ε\n(wi, x))T = Φ\n˜×(w,x)\nD,ε\n(wi, x), i ∈[m].\nPer Lemma 3 on the parallelization of FNNs, there exist\na FNN which exactly implements the parallelization of the\nFNNs – fed with xi = [wi, xT ]T , i ∈[m] – that produce the\nright-hand side of (39). Thus, w.r.t. the constraints of Lemma\n3, (39) can also be expressed via the parallelization of ˜Φ :=\n[Φ\n˜×(w,x)\nD,ε\n, . . . , Φ\n˜×(w,x)\nD,ε\n] for (vi) := (wi, x), i ∈[m], as\nP( ˜Φ)(W , x) = [Φ\n˜×(w,x)\nD,ε\n(v1), . . . , Φ\n˜×(w,x)\nD,ε\n(vm)]T ,\n(40)\nwhere – by Lemma 3 – the parallelized FNN denoted by P( ˜Φ)\nis fed with [(vec(W ))T , xT ]T , L(P( ˜Φ)) ≤C log2(nD2ε−1),\nW\n\u0000P( ˜Φ)\n\u0001\n≤\nPm\ni=1 12n\n=\n12mn, and B\n\u0000P( ˜Φ)\n\u0001\n≤\nmax{4, 2D2}.\nBecause ΦD,ε is also fed with [(vec(W ))T , xT ]T to gen-\nerate an approximation to W x which is also realizable via\nthe parallelized FNN of (40), ΦD,ε(W , x) = P( ˜Φ)(W , x).\nHence, (40) corroborates w.r.t. every scalar output that\nΦD,ε(W , x) = [Φ\n˜×(w,x)\nD,ε\n(v1), . . . , Φ\n˜×(w,x)\nD,ε\n(vm)]T ,\n(41)\nwhere ΦD,ε\n\u00000, x\n\u0001\n= ΦD,ε\n\u0000W , 0\n\u0001\n= 0. Meanwhile, employ-\ning Deﬁnition 7 and (11) in the left-hand side of (14)\n∥ΦD,ε(W , x) −W x∥L∞([−D,D]2n;Rm) =\nmax\ni=1,...,m\n∥Φ\n˜×(w,x)\nD,ε\n(wi, x) −wix∥L∞([−D,D]2n)\n(a)\n≤ε,\n(42)\nwhere (a) follows from (38) and the bound of (14) is obtained.\nThis completes the proof of (14). Since ΦD,ε(W , x) =\nP( ˜Φ)(W , x), the aforementioned constraints of P( ˜Φ) are\nalso the constraints of ΦD,ε mentioned in Theorem 1.\n■\nAPPENDIX D\nPROOF OF COROLLARY 2\nWe are going to deploy Theorem 1, Lemma 2, Lemma 3,\nand the following identities:\np1 := Re{W x} = Re{W }Re{x} −Im{W }Im{x} (43a)\np2 := Im{W x} = Re{W }Im{x} + Im{W }Re{x}. (43b)\nSince W1 = Re{W } ∈Rm×n, W2 = Im{W } ∈Rm×n,\nx1 = Re{x} ∈Rn, and x2 = Im{x} ∈Rn, (43a) and\n(43b) can be approximated via the superposition of two FNNs\ncharacterized via Theorem 1. Thus,\np1 ≈ΦD,˜ε(W1, x1) −ΦD,˜ε(W2, x2)\n(44a)\np2 ≈ΦD,˜ε(W1, x2) + ΦD,˜ε(W2, x1).\n(44b)\nPer the superposition of FNNs stated in Lemma 3, (44a) and\n(44b) can be represented via FNNs Φ\ns1\nD,˜ε and Φ\ns2\nD,˜ε such that\n[p1, p2] ≈[Φ\ns1\nD,˜ε(W1,2, x1,2), Φ\ns2\nD,˜ε(W1,2, x1,2)],\n(45)\nwhere W1,2 = [W1, W2], x1,2 = [xT\n1 , xT\n2 ]T , L(Φ\nsi\nD,˜ε) ≤\nC log2(nD2˜ε−1),\nW(Φ\nsi\nD,˜ε)\n≤\n24mn,\nB(Φ\nsi\nD,˜ε)\n≤\nmax{4, 2D2}, and Φ\nsi\nD,˜ε(0, x1,2) = Φ\nsi\nD,˜ε(W1,2, 0) = 0,\ni = 1, 2. Per Lemma 2, the parallelization of the FNNs\nof (45) fed with all elements of W1,2 and x1,2 produces\np1,2 = [pT\n1 , pT\n2 ]T . This is approximately Φ\nc\nD,ε(W1,2, x1,2)\nand hence for (V1,2) := (W1,2, x1,2)\nΦ\nc\nD,ε(W1,2, x1,2) = [(Φ\ns1\nD,˜ε(V1,2))T , (Φ\ns2\nD,˜ε(V1,2))T ]T , (46)\nwhere Φ\nc\nD,ε\n∈\nN N 2n(m+1),2m\n∞,∞,ρ\nsatisfying L(Φ\nc\nD,ε)\n≤\nC log2(nD2˜ε−1),\nW\n\u0000Φ\nc\nD,ε\n\u0001\n≤\n48mn,\nB\n\u0000Φ\nc\nD,ε\n\u0001\n≤\nmax{4, 2D2}, and Φ\nc\nD,ε\n\u00000, x1,2\n\u0001\n= Φ\nc\nD,ε\n\u0000W1,2, 0\n\u0001\n= 0.\nDeploying (46) and (43a)-(43b) in the left-hand side of (15),\n∥Φ\nc\nD,ε(W1,2, x1,2)−p1,2∥L∞([−D,D]2n;Rm)\n(a)\n≤\n2\nX\ni=1\n\u0000∥ΦD,˜ε\n(Wi, xi)−Wixi∥L∞([−D,D]2n;Rm) +∥ΦD,˜ε(Wi, xi+(−1)i+1)\n−Wixi+(−1)i+1∥L∞([−D,D]2n;Rm)\n\u0001 (b)\n≤4˜ε,\n(47)\nwhere (a) follows from (43a)-(43b), (44a)-(45), and the prop-\nerties of L∞-norm; (b) follows from (14). Setting ε = 4˜ε gives\n(15) and L(Φ\nc\nD,ε) ≤C log2(4nD2ε−1).\n■\n13\nAPPENDIX E\nPROOF OF THEOREM 2\nWe begin by proving the following proposition.\nProposition 7. Let wT = [w1, . . . , wn]T ∈Rn and x =\n[x1, . . . , xn]T ∈Rn. For all ε ∈(0, 1/2), D ∈R+, and\nρ(x) := max(x, 0), there exist constants ¯C1, ¯C2, ¯C3 > 0\nsuch that there is a FNN Φ\n˜×(w,x)\nD,ε\n∈N N 2n,1\n∞,∞,ρ satisfying\nΦ\n˜×(w,x)\nD,ε\n(0, x) = Φ\n˜×(w,x)\nD,ε\n(x, 0) = 0, ∀x ∈Rn, and\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥W1,∞((−D,D)2n;dwdx) ≤ε\n(48)\nM(Φ\n˜×(w,x)\nD,ε\n), N(Φ\n˜×(w,x)\nD,ε\n), L(Φ\n˜×(w,x)\nD,ε\n) ≤¯C1 log2(ε−1) + ¯C2\n(49)\n|Φ\n˜×(w,x)\nD,ε\n|W1,∞((−D,D)2n) ≤¯C3D.\n(50)\nProof. Since wx = Pn\ni=1 wixi, it can be implemented via the\nsuperposition of n Φ\n˜×(w,x)\nD,˜ε\ns characterized via Proposition 4\nand fed, respectively, with input tuples (w1, x1), . . . , (wn, xn).\nThus, the output of a ReLU FNN – Φ\n˜×(w,x)\nD,ε\n– that approxi-\nmates wx can be expressed as\nΦ\n˜×(w,x)\nD,ε\n(w, x) =\nn\nX\ni=1\nΦ\n˜×(w,x)\nD,˜ε\n(wi, xi),\n(51)\nwhere Φ\n˜×(w,x)\nD,˜ε\n(0, x) = 0 = Φ\n˜×(w,x)\nD,˜ε\n(x, 0), ∀x ∈R;\n∥Φ\n˜×(w,x)\nD,˜ε\n(w, x) −wx∥W1,∞((−D,D)2;dwdx) ≤˜ε\n(52)\nL(Φ\n˜×(w,x)\nD,˜ε\n), M(Φ\n˜×(w,x)\nD,˜ε\n), N(Φ\n˜×(w,x)\nD,˜ε\n) ≤C1 log2(˜ε−1)+C2.\n(53)\nDeploying (23) in (51),\nΦ\n˜×(w,x)\nD,ε\n(w, x) = 2D2×\nn\nX\ni=1\n\u0010\nΦsq\nδ\n\u0010|wi + xi|\n2D\n\u0011\n−Φsq\nδ\n\u0010|wi|\n2D\n\u0011\n−Φsq\nδ\n\u0010|xi|\n2D\n\u0011\u0011\n.\n(54)\nWith respect to (22),\nwx = 2D2\nn\nX\ni=1\n\u0010\u0010|wi + xi|\n2D\n\u00112\n−\n\u0010|wi|\n2D\n\u00112\n−\n\u0010|xi|\n2D\n\u00112\u0011\n. (55)\nFor our upcoming simpliﬁcations, we deﬁne these parameters\nfor S :=\n\b\n(w1, x1), . . . , (wi, xi), . . . , (wn, xn)\n\t\n, i ∈[n]:\ntwi : (−D, D)2n →(0, 1), S 7→|wi|/2D\n(56a)\ntxi : (−D, D)2n →(0, 1), S 7→|xi|/2D\n(56b)\ntwixi : (−D, D)2n →(0, 1), S 7→|wi + xi|/2D\n(56c)\nf : [0, 1] →R, x 7→x2.\n(56d)\nEmploying (56a)-(56d), (54) and (55) can be expressed as\nΦ\n˜×(w,x)\nD,ε\n(w, x)\n2D2\n=\nn\nX\ni=1\n\u0000Φsq\nδ ◦twixi −\nX\nui∈{wi,xi}\nΦsq\nδ ◦tui\n\u0001\n(57)\nwx = 2D2\nn\nX\ni=1\n\u0000f ◦twixi −f ◦twi −f ◦txi\n\u0001\n.\n(58)\nSubstituting (57) and (58) in the left-hand side of (48),\ncollecting similar terms, and applying the properties of L∞-\nnorm – via Deﬁnition 4 and (8) – result in\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥W1,∞((−D,D)2n;dwdx) ≤2D2×\nn\nX\ni=1\nX\nui∈{wi,xi,wixi}\n∥\n\u0000Φsq\nδ −f\n\u0001\n◦tui∥W1,∞((−D,D)2n).\n(59)\nApplying Corollary 1 – via (12b) – to each summand of (59),\n∥(Φsq\nδ −f)◦tui∥W1,∞((−D,D)2n) ≤C max{∥Φsq\nδ −f∥L∞((0,1)2)\n, |Φsq\nδ −f|W1,∞((0,1)2)|tui|W1,∞((−D,D)2n)}.\n(60)\nExploiting (10) for tui ∈{twi, txi, twixi},\n|tui|W1,∞((−D,D)2n) = 1/2D.\n(61)\nSubstituting (61) into (60) and, in turn, into (59) lead to\n∥Φ\n˜×(w,x)\nD,ε\n(w, x) −wx∥W1,∞((−D,D)2n;dwdx) ≤2D2C\nn\nX\ni=1\nX\nui∈{wi,xi,wixi}\nmax\n\b\n∥Φsq\nδ −f∥L∞((0,1)2), |Φsq\nδ −f|W1,∞((0,1)2)\n2D\n\t\n≤6D2Cn∥Φsq\nδ (x) −x2∥L∞((0,1)2)\n(a)\n≤(6D2Cδ)n,\n(62)\nwhere (a) follows from Proposition 2. W.r.t. (62) and the ˜ε\nconstraint of (52), ˜ε = 6D2Cδ. As a result, setting ε = ˜εn,\n(62) simpliﬁes to (48). This completes the proof of (48).\nRegarding\nthe\nsuperposition\nof\n(51)\nper\nLemma\n3,\nΦ\n˜×(w,x)\nD,ε\n(w, x) is implemented by adding the outputs of n\nΦ\n˜×(w,x)\nD,˜ε\ns fed, respectively, with (wi, xi). Thus,\nM\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤\nn\nX\ni=1\nM\n\u0000Φ\n˜×(w,x)\nD,˜ε\n\u0001\n+n\n(a)\n≤n[C1 log2(nε−1)\n+ (C2 + 1)]\n(b)\n= ¯C1 log2(ε−1) + ¯C2,\n(63)\nwhere (a) follows from (53) and ε = ˜εn; (b) is due to setting\n¯C1 = nC1 and ¯C2 = n[C1 log2 n + C2 + 1]. Similarly,\nN(Φ\n˜×(w,x)\nD,ε\n) =\nn\nX\ni=1\nN(Φ\n˜×(w,x)\nD,˜ε\n)\n(a)\n≤n[C1 log2(nε−1)+C2]\n≤¯C1 log2(ε−1) + ¯C2; L\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n= L\n\u0000Φ\n˜×(w,x)\nD,˜ε\n\u0001\n(a)\n≤C1 log2(nε−1) + C2 ≤¯C1 log2(ε−1) + ¯C2,\n(64)\nwhere (a) follows from (53) and the assignment ε = ˜εn. Thus,\n(63) and (64) lead to (49). Deploying Deﬁnition 6 – via (10)\n– and applying the properties of L∞-norm to (57):\n|Φ\n˜×(w,x)\nD,ε\n|W1,∞((−D,D)2n) ≤2D2\nn\nX\ni=1\nX\nui∈{wi,xi,wixi}\n|Φsq\nδ (xi) ◦tui|W1,∞((−D,D)2n)\n(a)\n≤2D2C\nn\nX\ni=1\nX\nui∈{wi,xi,wixi}\n|Φsq\nδ (x)|W1,∞((0,1))|tui|W1,∞((−D,D)2n)\n(b)\n≤3nCC4D,\n(65)\n14\nwhere (a) follows from (12a); (b) is due to (16b) and\n(61). Setting\n¯C3\n=\n3nCC4\n>\n0, (65) asserts that\n\f\fΦ\n˜×(w,x)\nD,ε\n\f\f\nW1,∞((−D,D)2n) ≤¯C3D. This proves (50).\n■\nUsing Proposition 7 regarding an approximating FNN on a\nvector-vector product and Lemma 2 (parallelization of FNNs),\nwe proceed to the developments detailed in the sequel.\nFrom (13), W x = [w1x, . . . , wmx]T ∈Rm. Meanwhile,\nProposition 7 afﬁrms that wix can be approximated via a\nReLU FNN such that\n∥Φ\n˜×(w,x)\nD,ε\n(wi, x) −wix∥W1,∞((−D,D)2n;dwdx) ≤ε,\n(66)\nwhere the ReLU FNN constraints of (49) are valid. Thus, W x\ncan be approximated via the outputs of m ReLU FNNs as\n[Φ\n˜×(w,x)\nD,ε\n(w1, x), . . . , Φ\n˜×(w,x)\nD,ε\n(wm, x)]T ≈W x,\n(67)\nwhere (Φ\n˜×(w,x)\nD,ε\n(wi, x))T = Φ\n˜×(w,x)\nD,ε\n(wi, x), i ∈[m]. Let\nus now consider a selection matrix πi := {0, 1}2n×n(m+1)\nequated as (wi, x) := [wT\ni , xT ]T = πi[(W (:))T , xT ]T . If\nwe let Φ\n˜×(w,x)\nD,ε\n:=\n\u0002\n[W1, b1], [W2, b2], . . . , [WK, bK]\n\u0003\nand\n˜Φ\n˜×(wi,x)\nD,ε\n:=\n\u0002\n[ ˜\nW i\n1, ˜bi\n1], [ ˜\nW i\n2, ˜bi\n2], . . . , [ ˜\nW i\nK, ˜bi\nK]\n\u0003\n, then\nΦ\n˜×(w,x)\nD,ε\n(wi, x) = ˜Φ\n˜×(wi,x)\nD,ε\n(W , x), i ∈[m]\n(68)\nprovided that\n˜Φ\n˜×(wi,x)\nD,ε\n=\n\u0002\n[W1πi, b1], [W2, b2], . . . , [WK, bK]\n\u0003\n.\n(69)\nChoosing m selection matrices that fulﬁll (68) and (69),\n[ ˜Φ\n˜×(w1,x)\nD,ε\n(W , x), . . . , ˜Φ\n˜×(wm,x)\nD,ε\n(W , x)]T ≈W x.\n(70)\nTherefore, w.r.t. the constraints of Lemma 2 and (68), (70) is\nalso equated for ˜Φ := [ ˜Φ\n˜×(w1,x)\nD,ε\n, . . . , ˜Φ\n˜×(wm,x)\nD,ε\n] as\nP( ˜Φ)(W , x) = [ ˜Φ\n˜×(w1,x)\nD,ε\n(W , x), . . . , ˜Φ\n˜×(wm,x)\nD,ε\n(W , x)]T\n= [Φ\n˜×(w,x)\nD,ε\n(w1, x), . . . , Φ\n˜×(w,x)\nD,ε\n(wm, x)]T ,\n(71)\nwhere\nM\n\u0000P( ˜Φ)\n\u0001\n=\nPm\ni=1 M\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤\nm[ ¯C1 log2(ε−1) + ¯C2]; N\n\u0000P( ˜Φ)\n\u0001\n≤Pm\ni=1 N\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤\nm[ ¯C1 log2(ε−1) + ¯C2]; and L(P( ˜Φ)) = L\n\u0000Φ\n˜×(w,x)\nD,ε\n\u0001\n≤\n[ ¯C1 log2(ε−1) + ¯C2] ≤m[ ¯C1 log2(ε−1) + ¯C2].\nSince ΦD,ε takes inputs that are all the elements of W\nand x to produce their approximated product which is also\nrealizable by the parallelized FNN of (71), ΦD,ε(W , x) =\nP( ˜Φ)(W , x). Thus, (71) corroborates w.r.t. every scalar out-\nput that\nΦD,ε\n\u0000W , x\n\u0001\n=\n\u0002\nΦ\n˜×(w,x)\nD,ε\n(w1, x), . . . , Φ\n˜×(w,x)\nD,ε\n(wm, x)\n\u0003T .\n(72)\nAs a result, ΦD,ε\n\u00000, x\n\u0001\n= ΦD,ε\n\u0000W , 0\n\u0001\n= 0. Therefore,\nsubstituting (72) and (13) into (19), collecting similar terms,\nand exploiting (9):\n∥ΦD,ε(W , x) −W x∥W1,∞((−D,D)2n;Rm) =\nmax\ni=1,...,m\n∥Φ\n˜×(w,x)\nD,ε\n(wi, x) −wix∥W1,∞((−D,D)2n;dwdx)\n(a)\n≤ε,\n(73)\nwhere (a) follows from (66). This completes the proof of (19).\nBecause ΦD,ε(W , x) = P( ˜Φ)(W , x), the aforementioned\nnetwork constraints of P( ˜Φ) become the constraints of ΦD,ε.\nAs a result, letting m ¯C1 = ¯¯C1 and m ¯C2 = ¯¯C2 lead to the\nnetwork constraints mentioned in Theorem 2.\n■\nACKNOWLEDGMENTS\nThe author acknowledges the US Department of Commerce\nand NIST for funding this work; the NIST support staff for\nfacilitating his NIST Guest Researcher-ship; and the Editor and\nanonymous Reviewers for their thoughtful comments that have\nguided the signiﬁcant improvement of his previously submitted\nmanuscript.\nDEDICATION\nAlong with his NIST colleagues, the author mourned the\nsudden loss of Michael Souryal who was his thoughtful NIST\nmentor. Honoring his late NIST mentor, the author dedicates\nthis paper to the memory of Michael Souryal.\nREFERENCES\n[1] N. Bostrom, Superintelligence: Paths, Dangers, Strategies.\nOxford,\nUK: Oxford Univ. Press, 2014.\n[2] N. J. Nilsson, The Quest for Artiﬁcial Intelligence: A History of Ideas\nand Achievements.\nNew York, NY, USA: Cambridge Univ. Press,\n2009.\n[3] S. Haykin, Neural Networks and Learning Machines, 3rd ed.\nUpper\nSaddle River, NJ, USA: Pearson, 2009.\n[4] D. E. Rumelhart et al., “Learning Representations by Back-propagating\nErrors,” Nature, vol. 323, no. 6088, pp. 533–536, 1986.\n[5] A. Pinkus, “Approximation theory of the MLP model in neural net-\nworks,” Acta Numerica, vol. 8, pp. 143–195, 1999.\n[6] G. Cybenko, “Approximation by superpositions of a sigmoidal func-\ntion,” Math. Control. Signals, Syst., 1989.\n[7] K. Hornik et al., “Multilayer feedforward networks are universal\napproximators,” Neural Netw., vol. 2, no. 5, p. 359–366, Jul. 1989.\n[8] K. Funahashi, “On the approximate realization of continuous mappings\nby neural networks,” Neural Netw., vol. 2, no. 3, p. 183–192, May 1989.\n[9] A. R. Barron, “Approximation and estimation bounds for artiﬁcial\nneural networks,” Mach. Learn., vol. 14, no. 1, p. 115–133, Jan 1994.\n[10] H. N. Mhaskar, “Neural networks for optimal approximation of smooth\nand analytic functions,” Neural Comput, vol. 8, pp. 164–177, 1996.\n[11] Y. Lecun et al., “Deep learning,” Nature, vol. 521, pp. 436–444, 2015.\n[12] I. Goodfellow et al., Deep Learning.\nMIT Press, 2016.\n[13] L. Deng and D. Yu, “Deep learning: Methods and applications,” Found.\nTrends Signal Process., vol. 7, no. 3–4, pp. 197–387, 2014.\n[14] Y. Bengio et al., “Representation learning: A review and new perspec-\ntives,” IEEE Trans. Pattern Anal. Mach. Intell., pp. 1798–1828, 2013.\n[15] K. Kawaguchi, “Deep learning without poor local minima,” in Proc.\nNIPS, 2016, pp. 586–594.\n[16] S. Arora et al., “A convergence analysis of gradient descent for deep\nlinear neural networks.” [Online]. Available: https://arxiv.org/pdf/1810.\n02281.pdf\n[17] A. M. Saxe et al., “Exact solutions to the nonlinear dynamics\nof learning in deep linear neural networks.” [Online]. Available:\nhttps://arxiv.org/pdf/1312.6120.pdf\n[18] D. Rolnick and M. Tegmark, “The power of deeper networks for\nexpressing natural functions,” in Proc. ICLR, 2018.\n[19] B. Poole et al., “Exponential expressivity in deep neural networks\nthrough transient chaos,” in Proc. NIPS, 2016, pp. 3360–3368.\n[20] H. W. Lin et al., “Why does deep and cheap learning work so well?”\nJ. Stat. Phys., vol. 168, no. 6, p. 1223–1247, 2017.\n[21] R. Eldan and O. Shamir, “The power of depth for feedforward neural\nnetworks,” in Proc. COLT, 2016, pp. 907–940.\n[22] H.\nMhaskar\nand\nT.\nPoggio,\n“Deep\nvs.\nshallow\nnetworks\n:\nAn approximation theory perspective.” [Online]. Available: https:\n//arxiv.org/pdf/1608.03287.pdf\n[23] T. Poggio et al., “Why and when can deep – but not shallow – networks\navoid the curse of dimensionality: A review,” Int. J. Autom. Comput.,\nvol. 14, pp. 503–519, 2017.\n15\n[24] A. Krizhevsky et al., “ImageNet classiﬁcation with deep convolutional\nneural networks,” in Proc. NIPS, vol. 25, Jan. 2012, pp. 1097–1105.\n[25] L. Liu et al., “Deep learning for generic object detection: A survey.”\n[Online]. Available: https://arxiv.org/pdf/1809.02165.pdf\n[26] Q. Mao, F. Hu, and Q. Hao, “Deep learning for intelligent wireless\nnetworks: A comprehensive survey,” IEEE Commun. Surveys Tuts.,\nvol. 20, no. 4, pp. 2595–2621, 4th Quart. 2018.\n[27] M. Chen et al., “Artiﬁcial neural networks-based machine learning for\nwireless networks: A tutorial,” IEEE Commun. Surveys Tuts., vol. 21,\nno. 4, pp. 3039–3071, 2019.\n[28] N. C. Luong et al., “Applications of deep reinforcement learning in\ncommunications and networking: A survey,” IEEE Commun. Surveys\nTuts., vol. 21, no. 4, pp. 3133–3174, 2019.\n[29] H. Huang et al., “Deep learning for physical-layer 5G wireless\ntechniques: Opportunities, challenges and solutions,” IEEE Wireless\nCommun., vol. 27, no. 1, pp. 214–222, 2020.\n[30] V. Mnih et al., “Human-level control through deep reinforcement\nlearning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.\n[31] Y. Li, “Deep reinforcement learning.” [Online]. Available: https:\n//arxiv.org/pdf/1810.06339.pdf\n[32] D. Castelvecchi, “The Black Box of AI,” Nature, vol. 538, pp. 20–23,\nOct. 2016.\n[33] J. Gu et al., “Recent advances in convolutional neural networks.”\n[Online]. Available: https://arxiv.org/pdf/1512.07108.pdf\n[34] S. Ji et al., “3D convolutional neural networks for human action\nrecognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 1,\npp. 221–231, Jan. 2013.\n[35] R. Jozefowicz et al., “An empirical exploration of recurrent network\narchitectures,” in Proc. ICML, vol. 37, 2015, pp. 2342–2350.\n[36] K. Greff et al., “LSTM: A search space odyssey,” IEEE Trans. Neural\nNetw. Learn. Syst., vol. 28, no. 10, pp. 2222–2232, 2017.\n[37] M.\nTschannen\net\nal.,\n“Recent\nadvances\nin\nautoencoder-based\nrepresentation\nlearning.”\n[Online].\nAvailable:\nhttps://arxiv.org/pdf/\n1812.05069.pdf\n[38] D. Bank, N. Koenigstein, and R. Giryes, “Autoencoders.” [Online].\nAvailable: https://arxiv.org/pdf/2003.05991.pdf\n[39] S.\nYu\nand\nJ.\nC.\nPrincipe,\n“Understanding\nautoencoders\nwith\ninformation theoretic concepts.” [Online]. Available: https://arxiv.org/\npdf/1804.00057.pdf\n[40] Z. Wang et al., “Generative adversarial networks in computer vision:\nA survey and taxonomy.” [Online]. Available: https://arxiv.org/pdf/\n1906.01529.pdf\n[41] A. Creswell et al., “Generative adversarial networks: An overview,”\nIEEE Signal Process. Mag., vol. 35, no. 1, pp. 53–65, 2018.\n[42] Z. Hu et al., “On unifying deep generative models.” [Online].\nAvailable: https://arxiv.org/pdf/1706.00550.pdf\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. CVPR, 2016, pp. 770–778.\n[44] G. Huang et al., “Densely connected convolutional networks,” in Proc.\nCVPR, 2017, pp. 2261–2269.\n[45] G. Huang et al., “Convolutional networks with dense connectivity,”\nIEEE Trans. Pattern Anal. Mach. Intell., 2019.\n[46] R. Sun et al., “The global landscape of neural networks: An overview,”\nIEEE Signal Process. Mag., vol. 37, no. 5, pp. 95–108, 2020.\n[47] T. Poggio et al., “Theoretical issues in deep networks,” Proc. Natl.\nAcad. Sci. U.S.A., vol. 117, no. 48, pp. 30 039–30 045, 2020.\n[48] R. Sun, “Optimization for deep learning: theory and algorithms.”\n[Online]. Available: https://arxiv.org/pdf/1912.08957.pdf\n[49] P. L. Bartlett et al., “Representing smooth functions as compositions\nof\nnear-identity\nfunctions\nwith\nimplications\nfor\ndeep\nnetwork\noptimization.” [Online]. Available: https://arxiv.org/pdf/1804.05012.pdf\n[50] K. Kawaguchi et al., “Generalization in deep learning.” [Online].\nAvailable: https://arxiv.org/pdf/1710.05468.pdf\n[51] D. Jakubovitz et al., “Generalization error in deep learning.” [Online].\nAvailable: https://arxiv.org/pdf/1808.01174.pdf\n[52] R. Giryes et al., “Deep neural networks with random gaussian weights:\nA universal classiﬁcation strategy?” IEEE Trans. Signal Process.,\nvol. 64, no. 13, pp. 3444–3457, 2016.\n[53] R. Giryes et al., “Corrections to “deep neural networks with random\ngaussian weights: A universal classiﬁcation strategy?”,” IEEE Trans.\nSignal Process., vol. 68, pp. 529–531, 2020.\n[54] J. Fan, C. Ma, and Y. Zhong, “A selective overview of deep learning.”\n[Online]. Available: https://arxiv.org/pdf/1904.05526.pdf\n[55] S. Mei et al., “A mean ﬁeld view of the landscape of two-layer neural\nnetworks,” Proc. Natl. Acad. Sci. U.S.A., vol. 115, no. 33, pp. E7665–\nE7671, 2018.\n[56] S. S. Schoenholz et al., “Deep information propagation.” [Online].\nAvailable: https://arxiv.org/pdf/1611.01232.pdf\n[57] J. Pennington and Y. Bahri, “Geometry of neural network loss surfaces\nvia random matrix theory,” in Proc. ICML, 2017, pp. 2798–2806.\n[58] S. Becker et al., “Geometry of energy landscapes and the optimizability\nof deep neural networks,” Phys. Rev. Lett., vol. 124, no. 10, 2020.\n[59] B.\nD.\nHaeffele\nand\nR.\nVidal,\n“Global\noptimality\nin\ntensor\nfactorization,\ndeep\nlearning,\nand\nbeyond.”\n[Online].\nAvailable:\nhttps://arxiv.org/pdf/1506.07540.pdf\n[60] N. Cohen et al., “On the expressive power of deep learning: A tensor\nanalysis,” in Proc. COLT, 2016, pp. 698–728.\n[61] Y. Bengio, N. L. Roux, P. Vincent, O. Delalleau, and P. Marcotte,\n“Convex neural networks,” in Proc. NIPS, 2006, pp. 123–130.\n[62] F. Bach, “Breaking the curse of dimensionality with convex neural\nnetworks.” [Online]. Available: https://arxiv.org/pdf/1412.8690.pdf\n[63] S. Feizi et al., “Porcupine neural networks: (almost) all local optima\nare global.” [Online]. Available: https://arxiv.org/pdf/1710.02196.pdf\n[64] J. Huang, Q. Qiu, R. Calderbank, and G. Sapiro, “Geometry-aware\ndeep transform,” in Proc. ICCV, 2015, pp. 4139–4147.\n[65] A. Jacot et al., “Neural tangent kernel: Convergence and generalization\nin neural networks,” in Proc. NIPS, 2018, pp. 8571–8580.\n[66] S. Arora et al., “On exact computation with an inﬁnitely wide neural\nnet.” [Online]. Available: https://arxiv.org/pdf/1904.11955.pdf\n[67] M. Belkin et al., “To understand deep learning we need to understand\nkernel learning.” [Online]. Available: https://arxiv.org/pdf/1802.01396.\npdf\n[68] G. Georgiev, “Linear algebra and duality of neural networks.”\n[Online]. Available: https://arxiv.org/pdf/1809.04711.pdf\n[69] N. Tsapanos et al., “Neurons with paraboloid decision boundaries\nfor improved neural network classiﬁcation performance,” IEEE Trans.\nNeural Netw. Learn. Syst., vol. 30, no. 1, pp. 284–294, 2019.\n[70] R. Balestriero and richard baraniuk, “A spline theory of deep learning,”\nin Proc. ICML, Stockholm, Sweden, 10–15 Jul 2018, pp. 374–383.\n[71] R. Balestriero and R. Baraniuk, “Mad max: Afﬁne spline insights into\ndeep learning.” [Online]. Available: https://arxiv.org/pdf/1805.06576.\npdf\n[72] B. A. Richards et al., “A deep learning framework for neuroscience,”\nNat. Neurosci., vol. 22, pp. 1761–1770, 2019.\n[73] A. H. Marblestone et al., “Toward an integration of deep learning and\nneuroscience,” Front. Comput. Neurosci., vol. 10, 2016.\n[74] D.\nZou\net\nal.,\n“Stochastic\ngradient\ndescent\noptimizes\nover-\nparameterized deep ReLU networks.” [Online]. Available: https:\n//arxiv.org/pdf/1811.08888.pdf\n[75] Z. Allen-Zhu et al., “A convergence theory for deep learning via\nover-parameterization.” [Online]. Available: https://arxiv.org/pdf/1811.\n03962.pdf\n[76] W. Zhu et al., “LDMNet: Low dimensional manifold regularized neural\nnetworks,” in Proc. CVPR, 2018, pp. 2743–2751.\n[77] Z. J. Xu et al., “Frequency principle: Fourier analysis sheds light on\ndeep neural networks.” [Online]. Available: https://arxiv.org/pdf/1901.\n06523.pdf\n[78] T. Wiatowski et al., “Energy propagation in deep convolutional neural\nnetworks,” IEEE Trans. Inf. Theory, pp. 4819–4842, 2018.\n[79] T. Wiatowski and H. B¨olcskei, “A mathematical theory of deep\nconvolutional neural networks for feature extraction,” IEEE Trans. Inf.\nTheory, vol. 64, no. 3, pp. 1845–1866, 2018.\n[80] H. Lin and S. Jegelka, “ResNet with one-neuron hidden layers is\na universal approximator.” [Online]. Available: https://arxiv.org/pdf/\n1806.10909.pdf\n[81] F.-L. Fan et al., “On a sparse shortcut topology of artiﬁcial neural\nnetworks.” [Online]. Available: https://arxiv.org/pdf/1811.09003.pdf\n[82] F.-L. Fan, R. Lai, and G. Wang, “Quasi-equivalence of width and\ndepth of neural networks.” [Online]. Available: https://arxiv.org/pdf/\n2002.02515.pdf\n[83] Z. Lu et al., “The expressive power of neural networks: A view from\nthe width,” in Proc. NIPS, 2017.\n[84] K. Kawaguchi, J. Huang, and L. P. Kaelbling, “Effect of depth and\nwidth on local minima in deep learning,” Neural Comput, vol. 31,\nno. 7, pp. 1462–1498, 2019.\n[85] W. Hwang and A. Heinecke, “Un-rectifying non-linear networks for\nsignal representation,” IEEE Trans. Signal Process., vol. 68, 2020.\n[86] B. Hanin, “Universal function approximation by deep neural nets with\nbounded width and ReLU activations,” Mathematics, vol. 7, 2019.\n[87] P. Grohs et al., “Deep neural network approximation theory.” [Online].\nAvailable: https://arxiv.org/pdf/1901.02220v1.pdf\n[88] I. G¨uhring et al., “Error bounds for approximations with deep ReLU\nneural networks in W s,p norms,” Anal. Appl., 2019.\n16\n[89] P. Petersen and F. Voigtlaender, “Optimal approximation of piecewise\nsmooth functions using deep ReLU neural networks,” Neural Netw.,\nvol. 108, pp. 296–330, Dec. 2018.\n[90] H. Boelcskei et al., “Optimal approximation with sparsely connected\ndeep neural networks,” SIAM J. Math Data Sci., vol. 1, p. 8–45, 2019.\n[91] R. Vershynin, High-Dimensional Probability: An Introduction with\nApplications in Data Science.\nCambridge Univ. Press, 2018.\n[92] M. J. Wainwright, High-Dimensional Statistics: A Non-Asymptotic\nViewpoint.\nCambridge Univ. Press, 2019.\n[93] B.\nSen,\n“A\ngentle\nintroduction\nto\nempirical\nprocess\ntheory\nand applications.” [Online]. Available: https://www.stat.columbia.edu/\n∼bodhi/Talks/Emp-Proc-Lecture-Notes.pdf\n[94] G. Kutyniok et al., “A theoretical analysis of deep neural networks\nand parametric PDEs.” [Online]. Available: https://arxiv.org/pdf/1904.\n00377.pdf\n[95] C. Schwab and J. Zech, “Deep learning in high dimension: Neural\nnetwork expression rates for generalized polynomial chaos expansions\nin UQ,” Anal. Appl., vol. 17, no. 1, pp. 19–55, 2019.\n[96] E. Bj¨ornson et al., “Massive MIMO is a reality – what is next? ﬁve\npromising research directions for antenna arrays.” [Online]. Available:\nhttps://arxiv.org/pdf/1902.07678.pdf\n[97] G. E. Hinton et al., “Distilling the knowledge in a neural network,” in\nProc. NIPS Wksp. Deep Learning, Montreal, QC, Canada, 2014.\n[98] A. Romero et al., “Fitnets: Hints for thin deep nets,” in Proc. ICLR,\n2015.\n[99] M. R. U. Saputra et al., “Distilling knowledge from a deep pose\nregressor network.” [Online]. Available: https://arxiv.org/pdf/1908.\n00858.pdf\n[100] V. Vapnik and R. Izmailov, “Learning using privileged information:\nSimilarity control and knowledge transfer,” J. Mach. Learn. Res.,\nvol. 16, no. 1, p. 2023–2049, Jan. 2015.\n[101] M. Phuong and C. H. Lampert, “Towards understanding knowledge\ndistillation,” in Proc. ICML, 2019, pp. 5142–5151.\n[102] D. Bassett and O. Sporns, “Network neuroscience,” Nat. Neurosci.,\nvol. 20, p. 353–364, 2017.\n[103] C. W. Lynn and D. S. Bassett, “The physics of brain network\nstructure, function, and control.” [Online]. Available: https://arxiv.org/\npdf/1809.06441.pdf\n[104] T. Wang et al., “Deep learning for wireless physical layer: Opportu-\nnities and challenges,” China Commun., vol. 14, no. 11, pp. 92–111,\n2017.\n[105] D. Erhan et al., “The difﬁculty of training deep architectures and the\neffect of unsupervised pre-training,” in Proc. AISTATS, 2009.\n[106] Y. Bengio, “Practical recommendations for gradient-based training of\ndeep architectures,” in Neural Networks: Tricks of the Trade, 2012.\n[107] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep\nfeedforward neural networks,” in Proc. AISTATS, 2010.\n[108] A. Polino et al., “Model compression via distillation and quantization,”\nin Proc. ICLR, 2018.\n[109] Y. Li et al., “Learning from noisy labels with distillation,” in Proc.\nICCV, 2017, pp. 1928–1936.\n[110] K. J. Geras et al., “Blending LSTMs into CNNs,” in Proc. ICLR Wksp.,\n2016.\n[111] D. Lopez-Paz et al., “Unifying distillation and privileged information,”\nin Proc. ICLR, 2016.\n[112] E. Biglieri et al., MIMO Wireless Communications.\nNew York, NY,\nUSA: Cambridge Univ. Press, 2007.\n[113] P. Schniter, “Low-complexity equalization of OFDM in doubly selec-\ntive channels,” IEEE Trans. Signal Process., vol. 52, no. 4, pp. 1002–\n1011, Apr. 2004.\n[114] Y. Xiaoyan et al., “Doubly selective fading channel estimation in\nMIMO OFDM systems,” Science in China, vol. 48, 2005.\n[115] R.-A. Stoica and G. T. F. de Abreu, “6G: the wireless communications\nnetwork for collaborative and AI applications.” [Online]. Available:\nhttps://arxiv.org/pdf/1904.03413.pdf\n[116] L. Hanzo and T. Keller, OFDM and MC-CDMA: A Primer.\nWest\nSussex, UK: Wiley, 2006.\n[117] J.-F. H´elard et al., “Multicarrier CDMA: A very promissing multiple\naccess scheme for future wideband wireless networks,” in Proc. Euro-\npean Wksp. Integrated Radio Commun. Syst., 2002.\n[118] G. B. Giannakis et al., “Topology identiﬁcation and learning over\ngraphs: Accounting for nonlinearities and dynamics,” Proc. IEEE, vol.\n106, no. 5, pp. 787–807, 2018.\n[119] Y. Shen et al., “Tensor decompositions for identifying directed graph\ntopologies and tracking dynamic networks,” IEEE Trans. Signal Pro-\nces., vol. 65, no. 14, pp. 3675–3687, 2017.\n[120] A. Ortega et al., “Graph signal processing: Overview, challenges, and\napplications,” Proc. IEEE, vol. 106, no. 5, pp. 808–828, 2018.\n[121] D. Yarotsky, “Error bounds for approximations with deep ReLU\nnetworks,” Neural Networks, vol. 94, p. 103–114, 2017.\n[122] M. Telgarsky, “Representation beneﬁts of deep feedforward networks.”\n[Online]. Available: https://arxiv.org/pdf/1509.08101.pdf\n[123] D. Elbr¨achter et al., “Deep neural network approximation theory,” IEEE\nTrans. Inf. Theory, 2021.\n[124] The Keras Community, “Keras,” last accessed: Mar. 2021. [Online].\nAvailable: https://keras.io/.\n[125] The TensorFlow Community, “Tensorﬂow,” last accessed: Mar. 2021.\n[Online]. Available: https://www.tensorﬂow.org/.\n[126] F. Chollet, Deep Learning with Python.\nShelter Island, NY, USA:\nManning, 2018.\n[127] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimiza-\ntion,” in Proc. ICLR, 2015.\n[128] K. He et al., “Delving deep into rectiﬁers: Surpassing human-\nlevel performance on ImageNet classiﬁcation.” [Online]. Available:\nhttps://arxiv.org/pdf/1502.01852.pdf\n[129] C. Xiao et al., “A discrete-time model for triply selective MIMO\nRayleigh fading channels,” IEEE Trans. Wireless Commun., vol. 3,\nno. 5, pp. 1678–1688, Sep. 2004.\n[130] C. W. Lynn and D. S. Bassett, “The physics of brain network structure,\nfunction and control,” Nat. Rev. Phys., vol. 1, p. 318–332, 2019.\n[131] A. Fornito, A. Zalesky, and E. T. Bullmore, Fundamentals of Brain\nNetwork Analysis.\nSan Diego, CA, USA: Academic Press, 2016.\n[132] H. Montanelli and Q. Du, “New error bounds for deep ReLU networks\nusing sparse grids,” SIAM J. Math. Data Sci., vol. 1, pp. 78–92, 2019.\n[133] T. M. Getu, “Advanced RFI detection, RFI excision, and spectrum\nsensing: Algorithms and performance analyses,” Ph.D. dissertation,\n´Ecole de Technologie Sup´erieure (´ETS), Montr´eal, QC, Canada, 2019.\n[134] H. Becker et al., “Brain-source imaging: From sparse to tensor models,”\nIEEE Signal Process. Mag., vol. 32, no. 6, pp. 100–112, Nov. 2015.\n[135] R. Arora et al., “Understanding deep neural networks with rectiﬁed\nlinear units,” in Proc. ICLR, 2018.\n17\nError Bounds for a Matrix-Vector Product\nApproximation with Deep ReLU Neural Networks\nTilahun M. Getu, Member, IEEE\nAbstract—This supplementary material comprises Proof of\nLemma 1, Proof of Lemma 2, and Proof of Proposition 2.\nNotation: scalars, vectors, and matrices are represented by\nitalic letters, bold lowercase letters, and bold uppercase letters,\nrespectively. Rn denotes the sets of n-dimensional vectors of\nreal numbers. N0 denotes the sets of natural numbers including\nzero. A sequence of K matrix-vector tuples is denoted as\n\u0002\n[W1, b1], . . . , [WK, bK]\n\u0003\n. The Python R⃝syntax len(W )\nstands for the number of rows of W . :=, max, (·)T , diag(·),\nIn, 0m×n, and 0 denote equal by deﬁnition, maximum,\ntranspose, a (block) diagonal matrix, an n×n identity matrix,\nan m × n zero matrix, and a zero vector (matrix) whose\ndimension will be clear in context, respectively. The class of\nfeedforward neural networks (FNNs) Φ : RN0 →RNK with\nno more than K layers, connectivity no more than M, input\ndimension N0, output dimension NK, and activation function\nρ is denoted as N N N0,NK\nK,M,ρ . For an FNN Φ, L(Φ), M(Φ),\nW(Φ), and B(Φ) stand for the depth of Φ, the connectivity of\nΦ, the maximum width of Φ, and the maximum absolute value\nof the weights of Φ, respectively. Regarding Lp (Lebesgue)\nspaces and f(x) : Rd →R, ∥f∥L∞(Ω) := inf\n\b\nC ≥0 :\n|f(x)| ≤C, ∀x ∈Ω\n\f\fΩ⊂Rd\t\n.\nI. Proof of Lemma 1\nIf Φ\nIN0\nK (x) = x for x ∈RN0, then (1) leads to\nAK(ρ(AK−1(ρ(. . . ρ(A1(x)))))) = x,\n(74)\nwhere ρ(x) = [max(0, x1), . . . , max(0, xN0)]T ; Ak(xk−1) =\nWkxk−1 + bk for xk−1 = ρ(Ak−1(xk−2)) and x0 = x; and\n[Wk, bk] – comprising Φ\nIN0\nK\n– is to be inferred for k ∈[K].\nIn the sequel, we are going to exploit these identities: x =\nρ(x) −ρ(−x) and ρ(ρ(x)) = ρ(x).\nFor (74) to be valid, here is a rectiﬁed linear unit (ReLU)\nFNN construction: all layers except the last layer would\nhave an output equal to [(ρ(x))T , (ρ(−x))T ]T ; these two\ncomponents would then be added – at the last layer – so that\nwe will obtain ρ(x)−ρ(−x) = x. With respect to (w.r.t.) this\nconstruction, x1 = ρ(W1x + b1) = ρ(W1[ρ(x) −ρ(−x)] +\nb1) = ρ\n\u0000W1ρ(x)−W1ρ(−x)+b1\n\u0001\n= [(ρ(x))T , (ρ(−x))T ]T .\nW.r.t. the last equality, W1 = [IN0, −IN0]T and b1 = 02N0×1.\nThus, [W1, b1] = [[IN0, −IN0]T , 0].\nW.r.t.\nthe\naforementioned\nconstruction,\nxk\n=\n[(ρ(x))T , (ρ(−x))T ]T , k\n=\n2, . . . , K −1. Accordingly,\nT. M. Getu is with the National Institute of Standards and Technology\n(NIST), 100 Bureau Drive, Gaithersburg, MD 20899, USA and also with the\n´Ecole de Technologie Sup´erieure (´ETS), Montr´eal, QC H3C 1K3, Canada\n(e-mail: tilahun.getu@nist.gov).\nx2 = ρ(W2x1 + b2) = ρ(W2[(ρ(x))T , (ρ(−x))T ]T + b2) =\n[(ρ(x))T , (ρ(−x))T ]T .\nConcerning\nthe\nlast\nequality,\nW2 = I2N0, b2 = 02N0×1, and hence [W2, b2] = [I2N0, 0].\nRepeating\nthe\nsame\nprocedure\nfor\nthe\nk-th\nlayer\n–\nk ∈{3, . . . , K −1}, [Wk, bk] = [I2N0, 0]. At the last\nlayer, xK should be x for Φ\nIN0\nK\nto be an I-FNN. Thus, xK =\nWKxK−1 + bK = WK[(ρ(x))T , (ρ(−x))T ]T + bK = x.\nFrom the last equality, WK = [IN0, −IN0], bK = 0N0×1,\nand hence [WK, bK] = [[IN0, −IN0], 0]. Therefore, recalling\nthat an I-FNN is denoted by a sequence of matrix-vector\ntuples as in (5), employing the deduced [Wk, bk] for k ∈[K]\nleads to (5). For K = 1, (74) leads to x1 = W1x + b1 = x.\nConsequently, Φ\nIN0\n1\n=\n\u0002\n[IN0, 0]\n\u0003\n.\n■\nII. Proof of Lemma 2\nFor K, n ≥2, it follows via (3a) and (3b) that the\nﬁrst layer’s output of the parallelized FNN becomes xp\n1 =\nρ( ˜\nW1x + ˜b1). For the n FNNs, the respective ﬁrst layer\noutputs are given by xi\n1\n=\nρ(W i\n1x + bi\n1), i\n∈\n[n].\nIf P(Φ1, . . . , Φn)(x) = [(Φ1(x))T , . . . , (Φn(x))T ]T , this\nequality should regress to all layers’ outputs. Hence, ρ( ˜\nW1x+\n˜b1) = [(ρ(W 1\n1 x + b1\n1))T , . . . , (ρ(W n\n1 x + bn\n1))T ]T . Since\nρ is applied element-wise, the ﬁrst parallelization conditions\nemanate from: ρ( ˜\nW1x+˜b1) = ρ([(W 1\n1 )T , . . . , (W n\n1 )T ]T x+\n[(b1\n1)T , . . . , (bn\n1)T ]T ). Hence, ˜\nW1 = [(W 1\n1 )T , . . . , (W n\n1 )T ]T\nand ˜b1 = [(b1\n1)T , . . . , (bn\n1)T ]T . This proves (6a).\nUnder\nthe\nﬁrst\nconditions,\nit\nfollows\nthat\nρ( ˜\nW2xp\n1\n+\n˜b2)\n=\n[(ρ(W 1\n2 x1\n1\n+\nb1\n2))T , . . . , (ρ(W n\n2 xn\n1\n+\nbn\n2))T ]T .\nThus,\nρ( ˜\nW2xp\n1\n+\n˜b2)\n=\nρ(diag(W 1\n2 , . . . , W n\n2 )[(x1\n1)T , . . . , (xn\n1)T ]T\n+\n[(b1\n2)T , . . . , (bn\n2)T ]T ). From the parallelization of the ﬁrst\nlayers’ outputs of the n FNNs, xp\n1 = [(x1\n1)T , . . . , (xn\n1)T ]T .\nAccordingly,\nthe\nsecond\nparallelization\nconditions\nare:\n˜\nW2 = diag(W 1\n2 , . . . , W n\n2 ) and ˜b2 = [(b1\n2)T , . . . , (bn\n2)T ]T .\nMoreover, employing the same procedure recursively leads to:\n˜\nWk = diag(W 1\nk , . . . , W n\nk ) and ˜bk = [(b1\nk)T , . . . , (bn\nk)T ]T ,\n3 ≤k ≤K. This ends the proof of (6b).\nAs\nwe\nare\nparallelizing\nn\nequal-depth\nFNNs,\nL(P( ˜Φ))\n=\nK\nand M(P( ˜Φ))\n=\nPK\nk=1(∥˜\nWk∥ℓ0 +\n∥˜bk∥ℓ0)\n=\nPK\nk=1\nPn\ni=1(∥W i\nk∥ℓ0\n+\n∥bi\nk∥ℓ0)\n=\nPn\ni=1\nPK\nk=1(∥W i\nk∥ℓ0 + ∥bi\nk∥ℓ0). From Deﬁnition 1 (stated\nin\nSec.\nII-A),\nM(Φi)\n=\nPK\nk=1(∥W i\nk∥ℓ0 + ∥bi\nk∥ℓ0).\nThus, M(P( ˜Φ))\n=\nPn\ni=1 M(Φi). From the conditions\nof\nLemma\n2,\nN(P( ˜Φ))\n=\nN0 + PK\nk=1 len( ˜\nWk)\nand\nN(Φi)\n=\nN0 + PK\nk=1 len(W i\nk).\nNoting\nthat\nlen( ˜\nWk)\n=\nPn\ni=1 len(W i\nk) and PK\nk=1 len(W i\nk)\n=\nN(Φi) −N0, N(P( ˜Φ)) = N0 + Pn\ni=1\nPK\nk=1 len(W i\nk) =\n18\nN0 + Pn\ni=1(N(Φi) −N0) = Pn\ni=1 N(Φi) −nN0 + N0 =\nPn\ni=1 N(Φi) −(n −1)N0.\n■\nIII. Proof of Proposition 2\nA similar proof is provided in [1, p. 6-8]. For the sake of\ncompleteness, clarity, and rigor, we hereby provide a detailed\nproof.\nTo begin with the “sawtooth” construction of [2] and [3], let\nus consider the following sawtooth function g : [0, 0] →[0, 1]:\ng(x) =\n(\n2x\n: if x < 1/2\n2(1 −x)\n: if x ≥1/2.\n(75)\nBased on (75), let us now consider the s-fold composition of\ng for s ≥2 as\ngs := g ◦g ◦. . . ◦g\n|\n{z\n}\ns times\n,\n(76)\nwhere\ng0(x) := x and g1(x) := g(x).\n(77)\nTelgarsky has shown (see [2, p. 4]) that gs – as deﬁned in\n(76) – is a sawtooth function with 2s−1 uniformly distributed\n“teeth” [2, Lemma 2.4], i.e., each application of g doubles\nthe number of teeth [3]. Employing the linear combinations\nof gs, we are going to be show that the function f(x) = x2,\nx ∈[0, 1], can be approximated.\nSuppose fm be the piece-wise linear interpolation of f with\n2m + 1 uniformly distributed breakpoints (“knots”) per\nfm\n\u0012 k\n2m\n\u0013\n=\n\u0012 k\n2m\n\u00132\n, k = 0, . . . , 2m and m ∈N0.\n(78)\nNote that fm approximates f with error εm = 2−2m−2 [3] in\nthe L∞-norm sense such that [1]\n∥fm(x) −x2∥L∞[0,1] ≤2−2m−2.\n(79)\nIt is now worth underscoring that reﬁning the interpolation\nfrom fm−1 to fm boils down to modifying it by a function\nproportional to a sawtooth function. Accordingly [1], [3],\nfm−1(x) −fm(x) = gm(x)\n22m .\n(80)\nEmploying (78), one can infer that\nf0(k) = k2 = k, k ∈{0, 1}.\n(81)\nDeploying a recursive substitution via (80) while using (81),\nfm(x) = x −\nm\nX\ns=1\ngs(x)\n22s .\n(82)\nYarotsky [3] realized (82) via a (maximum) width-3 deep\nReLU network whilst allowing connections between neurons\n(nodes) in non-consecutive layers (a.k.a. skip connections). We\nherein follow the lead of [1] to realize (82) by a (maximum)\nwidth-4 deep ReLU network (deep ReLU FNN) based on\n(75). To this end, note that the superposition of three ReLU –\ndenoted by ρ(·) – functions realizes g(x) deﬁned in (75) as\ng(x) = 2ρ(x) −4ρ(x −1/2) + 2ρ(x −1).\n(83)\nWith regard to (76) and (83), it follows that gm = ρ(gm):\ngm = 2ρ(gm−1) −4ρ(gm−1 −1/2) + 2ρ(gm−1 −1)\n(84)\ngm = 0 × ρ(fm−1) + 2ρ(gm−1) −4ρ(gm−1 −1/2)\n+ 2ρ(gm−1 −1).\n(85)\nIf we have to realize (85) via a 2-layer ReLU FNN repre-\nsenting the composition of afﬁne linear maps, it follows from\nDeﬁnition 1 that\ngm = A(1)\n2 (ρ(A1([gT\nm−1, f T\nm−1]T ))),\n(86)\nwhere\nA1(x) =\n\n\n1\n0\n1\n0\n1\n0\n0\n1\n\n\n\u0014x1\nx2\n\u0015\n−\n\n\n0\n1/2\n1\n0\n\n\n(87)\nand\nA(1)\n2 (x) =\n\u00022\n−4\n2\n0\u0003\n\n\nx1\nx2\nx3\nx4\n\n.\n(88)\nTo continue, because fm – deﬁned in (78) – is a non-negative\nfunction, fm = ρ(fm). Hence, applying ReLU to both sides\nof (80) gives\nfm = ρ(fm−1) −2−2mρ(gm)\n(89)\nfm\n(a)\n= ρ(fm−1) −2−2m\u0002\n2ρ(gm−1) −4ρ(gm−1 −1/2)\n+ 2ρ(gm−1 −1)\n\u0003\n,\n(90)\nwhere (a) is due to (84). Similar to (86), (90) can also be\nimplemented through a 2-layer ReLU FNN as\nfm = A(2)\n2 (ρ(A1([gT\nm−1, f T\nm−1]T ))),\n(91)\nwhere\nA1(x) =\n\n\n1\n0\n1\n0\n1\n0\n0\n1\n\n\n\u0014x1\nx2\n\u0015\n−\n\n\n0\n1/2\n1\n0\n\n\n(92)\nand\nA(2)\n2 (x) =\n\u0002−2−2m+1\n2−2m+2\n−2−2m+1\n1\u0003\n\n\nx1\nx2\nx3\nx4\n\n. (93)\nSince the ReLU FNNs of (86) and (91) are fed with the\nsame inputs and have the same ﬁrst layer weights as well as\nbiases, they can be parallelized. Paralleization of (86) and (91)\nthen leads to\n\u0014gm\nfm\n\u0015\n= A2\n\u0012\nρ\n\u0012\nA1\n\u0012 \u0014gm−1\nfm−1\n\u0015 \u0013\u0013\u0013\n,\n(94)\n19\nΦsq\nε (x) =\n\u0014gm\nfm\n\u0015\n= A2\n\u0012\nρ\n\u0012\nA1\n\u0012\nA2\n\u0012\nρ\n\u0012\n. . . ρ\n\u0012\nA1\n\u0012\nA2\n\u0012\nρ\n\u0012\nA1\n\u0012 \u0014\nx\nx\n\u0015 \u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\n.\n(96)\nwhere\nA2(x) =\n\u0014\n2\n−4\n2\n0\n−2−2m+1\n2−2m+2\n−2−2m+1\n1\n\u0015\n\n\nx1\nx2\nx3\nx4\n\n.\n(95)\nMeanwhile, iterating the ReLU FNN of (94) recursively down\nto (f0, g0) = (f0(x), g0(x)) = (x, x) – as asserted by (77) and\n(81) – produces the ReLU FNN expressed by (96), as shown\nat the top of this page.\nAccording to (96), fm can be realized via Φsq\nε (x) such that\nL(Φsq\nε ) = m + 1; M(Φsq\nε ) = 13m which is due to (92), (95),\nand (96); W(Φsq\nε ) = 4; and B(Φsq\nε ) ≤4 that also follows from\n(92), (95), and (96). Meanwhile, fm can be realized through\nN N 1,1\nm+1,13m,ρ for ρ(x) := max(0, x) and it approximates f\nwith error ε = 2−2m−2; cf. (79). Thus, log2(ε−1) = 2m +\n2 = 2(m + 1) ⇔m + 1 = L(Φsq\nε ) = C log2(ε−1) for C =\n0.5. Since ε →0 as m →∞, L(Φsq\nε ), M(Φsq\nε ) →∞as\nm →∞. Therefore, there exists Φsq\nε ∈N N 1,1\n∞,∞,ρ – with the\naforementioned network constraints – such that\n\r\rΦsq\nε (x) −\nx2\r\r\nL∞([0,1]) ≤ε.\n■\nREFERENCES\n[1] P. Grohs et al., “Deep neural network approximation theory.” [Online].\nAvailable: https://arxiv.org/pdf/1901.02220v1.pdf\n[2] M. Telgarsky, “Representation beneﬁts of deep feedforward networks.”\n[Online]. Available: https://arxiv.org/pdf/1509.08101.pdf\n[3] D. Yarotsky, “Error bounds for approximations with deep ReLU net-\nworks,” Neural Networks, vol. 94, p. 103–114, 2017.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2021-11-25",
  "updated": "2021-11-25"
}