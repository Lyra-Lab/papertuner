{
  "id": "http://arxiv.org/abs/2111.14362v1",
  "title": "Unsupervised Image Denoising with Frequency Domain Knowledge",
  "authors": [
    "Nahyun Kim",
    "Donggon Jang",
    "Sunhyeok Lee",
    "Bomi Kim",
    "Dae-Shik Kim"
  ],
  "abstract": "Supervised learning-based methods yield robust denoising results, yet they\nare inherently limited by the need for large-scale clean/noisy paired datasets.\nThe use of unsupervised denoisers, on the other hand, necessitates a more\ndetailed understanding of the underlying image statistics. In particular, it is\nwell known that apparent differences between clean and noisy images are most\nprominent on high-frequency bands, justifying the use of low-pass filters as\npart of conventional image preprocessing steps. However, most learning-based\ndenoising methods utilize only one-sided information from the spatial domain\nwithout considering frequency domain information. To address this limitation,\nin this study we propose a frequency-sensitive unsupervised denoising method.\nTo this end, a generative adversarial network (GAN) is used as a base\nstructure. Subsequently, we include spectral discriminator and frequency\nreconstruction loss to transfer frequency knowledge into the generator. Results\nusing natural and synthetic datasets indicate that our unsupervised learning\nmethod augmented with frequency information achieves state-of-the-art denoising\nperformance, suggesting that frequency domain information could be a viable\nfactor in improving the overall performance of unsupervised learning-based\nmethods.",
  "text": "N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE 1\nUnsupervised Image Denoising with\nFrequency Domain Knowledge\nNahyun Kim*\nnhkim21@kaist.ac.kr\nDonggon Jang*\njdg900@kaist.ac.kr\nSunhyeok Lee\nsunhyeok.lee@kaist.ac.kr\nBomi Kim\nby5747@kaist.ac.kr\nDae-Shik Kim\ndaeshik@kaist.ac.kr\nKorea Advanced Institute of Science\nand Technology (KAIST),\nDaejeon, Korea\nAbstract\nSupervised learning-based methods yield robust denoising results, yet they are inher-\nently limited by the need for large-scale clean/noisy paired datasets. The use of unsu-\npervised denoisers, on the other hand, necessitates a more detailed understanding of the\nunderlying image statistics. In particular, it is well known that apparent differences be-\ntween clean and noisy images are most prominent on high-frequency bands, justifying the\nuse of low-pass ﬁlters as part of conventional image preprocessing steps. However, most\nlearning-based denoising methods utilize only one-sided information from the spatial\ndomain without considering frequency domain information. To address this limitation,\nin this study we propose a frequency-sensitive unsupervised denoising method. To this\nend, a generative adversarial network (GAN) is used as a base structure. Subsequently,\nwe include spectral discriminator and frequency reconstruction loss to transfer frequency\nknowledge into the generator. Results using natural and synthetic datasets indicate that\nour unsupervised learning method augmented with frequency information achieves state-\nof-the-art denoising performance, suggesting that frequency domain information could\nbe a viable factor in improving the overall performance of unsupervised learning-based\nmethods.\n1\nIntroduction\nBased on clean and noisy image pairs, supervised learning-based image denoisers have\nshown impressive performance compared to prior-based approaches. A large number of\nhigh-quality image pairs play an important role in the performance of supervised learning-\nbased methods. However, constructing large-scale paired datasets may be unavailable or\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\n* The authors contributed equally.\nGithub: https://github.com/jdg900/UID-FDK\narXiv:2111.14362v1  [eess.IV]  29 Nov 2021\n2 N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\nexpensive in real-world situations. For this reason, image denoising methods that do not\nrequire clean and noisy image pairs have recently drawn attention.\nA noisy image x is usually modeled as the sum of clean background y and noise n:\nx = y+n. Subsequently, noise corrupts the benign pixels, which makes it hard to distinguish\nthe pixels of noise and content in the spatial domain. However, in the frequency domain,\nnoise and content can be easily identiﬁed. As shown in Figure 1 (a), we observe that the noise\nlies in the high-frequency bands and semantic information lies in the low-frequency bands.\nFurthermore, in Figure 1 (b), we note that apparent differences between clean and noisy\nimages are most prominent on high-frequency bands. It may indicate that the frequency\ndomain provides useful evidence for noise removal. However, the recent learning-based\ndenoisers overlook the frequency domain information and use only one-sided information\nfrom the spatial domain.\nMotivated by these observations, we propose the unsupervised denoising method that re-\nﬂects frequency domain information. Speciﬁcally, with a generative adversarial network as\na base structure, we introduce the spectral discriminator and frequency reconstruction loss\nto transfer frequency knowledge to the generator. The spectral discriminator distinguishes\nthe differences between denoised and clean images on high-frequency bands. By propagat-\ning this knowledge to the generator for noise removal, the generator considers the frequency\ndomain and thus produces visually more plausible denoised images to fool the spectral dis-\ncriminator. The frequency reconstruction loss, combined with the cycle consistency loss,\nimproves the image quality and preserves the content of images while narrowing the gap\nbetween clean and denoised images in the frequency domain.\nThe main contributions of our method are summarized as follows: 1) We propose the\nGAN-based unsupervised image denoising method that preserves semantic information and\nproduces a high-quality noise-free image. 2) To the best of our knowledge, it is the ﬁrst\napproach to explore the potential of the frequency domain with Fourier transform in the ﬁeld\nof noise removal tasks. The proposed spectral discriminator and frequency reconstruction\nloss make the generator concentrate on the noise and produce satisfying results. Denoised\nimages recovered by our method are close to clean reference images in both spatial and fre-\nquency domain. 3) The proposed method outperforms existing unsupervised image denoisers\nby a considerable margin. Moreover, our performance is even comparable with supervised\nlearning-based approaches trained with paired datasets.\n2\nRelated Work\n2.1\nImage Denoising\nNon-learning based image denoisers [2, 4, 10, 17, 26, 32, 40, 41, 42, 50] have tried to\nreconstruct clean images using pre-deﬁned priors which model the distribution of noise.\nSpeciﬁcally, a widely used prior in image denoising is non-local self-similarity prior [4, 10,\n17, 41]. Assuming that similar patches exist in a single image, the methods based on non-\nlocal self-similarity [4, 10] remove the noise using these patches.\nRecently, with the advent of deep neural networks, supervised learning-based image de-\nnoisers [27, 46, 47] show promising performance on a set of clean and noisy image pairs.\nHowever, it is challenging to construct clean and noisy image pairs in a real-world scenario.\nTo address the above issues, denoisers that do not rely on clean and noisy image pairs have\nbeen proposed [8, 12, 24, 25, 35]. N2N [25] learns reconstruction using only noisy im-\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE 3\nClean\nNoisy\nLow-passed\nHigh-passed\nSpatial\nSpectral\n(a)\n0\n20\n40\n60\n80\nSpatial Frequency\n120\n140\n160\n180\n200\n220\n240\n260\n280\nPower Spectrum\nClean Images\nNoisy Images\n(b)\nFigure 1: The spectrum analysis in the frequency domain. (a) Visualization of images in the\nspatial domain and corresponding spectrum maps in the frequency domain. (b) The statistics\n(mean and variance) after azimuthal integral over the power spectrum on clean and noisy\nimages of CBSD68. We use AWGN with a noise level σ = 50 to yield noisy images.\nage pairs without ground-truth clean images. N2V [24] estimates a corrupted pixel from its\nneighboring pixels based on a blind-spot mechanism. GCBD [8] generates the noisy im-\nages while modeling the real-world noise distribution through the GAN [16] and trains the\ndenoiser with pseudo clean and noisy image pairs. LIR [12] trains an image denoiser by\ndisentangling invariant representations from noisy images with an unpaired dataset.\n2.2\nFrequency Domain in CNNs\nIn traditional image processing, analyzing images in the frequency domain is known to be ef-\nfective by transforming the image from the spatial domain to the frequency domain. Inspired\nby this idea, several works attempt to utilize the information from the frequency domain in\ndeep neural networks. Xu et al. [43] accelerate the training of neural networks utilizing the\ndiscrete cosine transform. Dzanic et al. [14] observe that discrepancy exists between the im-\nages generated by the GAN [16] and the real images through the analysis of high-frequency\nFourier modes. In addition, attempts to utilize the frequency domain information in the var-\nious ﬁelds, including image forensics [13, 15, 48], image generation [5, 9, 20], and domain\nadaptation [44, 45] are gradually increasing. However, image denoising methods combining\nthe frequency domain analysis with DNN remain much less explored.\n3\nMethod\nIn this section, we ﬁrst introduce the spectral discriminator and frequency reconstruction\nloss that use information from the frequency domain. Then, we present an unsupervised\nframework for image denoising, integrating the proposed discriminator and loss with the\nGAN. The proposed framework is illustrated in Figure 2.\n3.1\nFrequency Domain Constraints\nSpectral Discriminator\nThe simple way for the generator to consider the frequency do-\nmain is that the discriminator transfers the frequency domain knowledge to the generator. To\nthis end, we propose the spectral discriminator similar to that introduced by [9] to measure\n4 N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n𝐺𝑐2n 𝐺𝑛2c 𝑥𝑛\nVGG19\nDC\nDT\nDS\nGn2c\n: pre-trained network\n: non-network module\n: loss\n: trainable network\n𝑥𝑛\n𝑦𝑐\n𝐺𝑛2c 𝑥𝑛\n𝑀𝑠ℎ𝑖𝑓𝑡𝑥\n𝐴𝐼𝑟\n𝐿𝐵𝐺\n𝐿𝐶𝐶& 𝐿𝑅𝑒𝑐𝑜𝑛\n𝐿𝑎𝑑𝑣\nC𝑙𝑒𝑎𝑛\n𝐿𝑎𝑑𝑣\n𝑇𝑒𝑥𝑡𝑢𝑟𝑒\n𝐿𝑎𝑑𝑣\n𝑆𝑝𝑒𝑐𝑡𝑟𝑎𝑙\n𝐿𝑉𝐺𝐺\n𝐿𝑇𝑉\nGc2n\nThe texture representation\nThe spectral representation\n𝑀𝑠ℎ𝑖𝑓𝑡\n𝐴𝐼𝑟\nFigure 2: An overview of the proposed framework. Given an unpaired clean yc and noisy\nimage xn, the generator Gn2c for image denoising takes the noisy image xn as an input and\nlearns the mapping for noise removal. Additional network Gc2n is used to impose the cycle\nconsistency. Three discriminators DC, DT, and DS try to distinguish the denoised image\nGn2c(xn) from real clean image xc in terms of both spatial domain and frequency domain.\nThe whole framework is end-to-end trainable.\nspectral realness. We compute the discrete Fourier transform on 2D image data f(w,h) in\nsize W ×H to feed frequency representations to the discriminator.\nF(k,l) =\nW−1\n∑\nw=0\nH−1\n∑\nh=0\nf(w,h)e−2πi kw\nW e−2πi lh\nH\n(1)\nfor spectral coordinates k = 0,...,W −1 and l = 0,...,H −1.\nRecent studies [9, 13] show that the 1D representation of the Fourier power spectrum is\nsufﬁcient to highlight spectral differences. Following their works, we transform the result of\nFourier transform to polar coordinate and compute azimuthal integration over θ.\nF(r,θ) = F(k,l) : r =\np\nk2 +l2,\nθ = arctan l\nk,\nAI(r) = 1\n2π\nZ 2π\n0\n|F(r,θ)|dθ\n(2)\nwhere AI(r) means the average intensity of the image signal about radial distance r.\nWe propose the spectral discriminator that allows the generator to focus on noise using\nhigh-frequency spectral information. To learn the differences on high-frequency bands, we\npass the 1D spectral vector into the high-pass ﬁlter Fhp and input it to the spectral discrimi-\nnator.\nvI = Fhp(AI(r)),\nFhp(x) =\n(\nx,\nr > rτ,\n0,\notherwise\n(3)\nwhere rτ is a threshold radius for high-pass ﬁltering and vI is a high-pass ﬁltered 1D spectral\nvector of an input I.\nGenerally, the most distinct characteristics between clean and noisy images exist on high-\nfrequency bands. Thus, if there is some remained noise on denoised images, the spectral\ndiscriminator easily distinguishes the difference between the clean and denoised images on\nhigh-frequency bands. By transferring this knowledge to the generator, the generator for\nnoise removal learns to yield visually more plausible images to fool the spectral discrimina-\ntor.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE 5\nFrequency Reconstruction Loss\nCai et al. [5] demonstrate the existence of a gap between\nthe real and generated image in the frequency domain, which leads to artifacts in the spatial\ndomain. Motivated by this observation, we propose to use frequency reconstruction loss with\ncycle consistency loss to ameliorate the quality of denoised images while reducing the gap.\nWe aim that the frequency reconstruction loss which is complementary to cycle consistency\nloss enables the generator to consider the frequency domain. Furthermore, we expect that\nit can serve as an assistant in generating high-quality denoised images. To compute the fre-\nquency reconstruction loss, we map an input xn and reconstructed image Gc2n(Gn2c(xn)) to\nthe frequency domain using Fourier transform. Then, we calculate the frequency reconstruc-\ntion loss by measuring the difference between the two results of the Fourier transform and\ntaking a logarithm to normalize it. Finally, we minimize the following objective:\nLFreq = log(1+\n1\nWH\nW−1\n∑\nk=0\nH−1\n∑\nl=0\n|Fxn(k,l)−FGc2n(Gn2c(xn))(k,l)|)\n(4)\n3.2\nUnsupervised Framework for Image Denoising\nOur goal is to learn a mapping from a noise domain XN to a clean domain YC given unpaired\ntraining images xn ∈XN and yc ∈YC. To learn this mapping, we use the CycleGAN-like\nframework consisting of two generators, Gn2c and Gc2n, and three discriminators, DC, DT,\nand DS. Given a noisy image xn, the generator Gn2c learns to generate a denoised image\nGn2c(xn). While distinguishing the denoised image Gn2c(xn) from the real clean image yc,\nthe discriminator DC makes the generator produce the denoised images closer to the real\nclean domain YC. To stablize training, we use the Least Squares GAN (LSGAN) loss [28]\nfor adversarial loss. The LSGAN loss for Gn2c and DC is:\nLClean\nadv\n= Eyc∼Pc[(DC(yc))2]+Exn∼Pn[(1−DC(Gn2c(xn)))2]\n(5)\nwhere Pn and Pc are the data distributions of the domain XN and domain YC, respectively.\nAs introduced in [37], we adopt the texture discriminator DT in order to guide the gen-\nerator to produce clean contour and preserve texture while removing the noise. Following\nthe scheme of [37], a random color shift algorithm Mshi ft is applied to the denoised image\nGn2c(xn). The texture loss for Gn2c and DT is:\nLTexture\nadv\n= Eyc∼Pc[(DT(Mshi ft(yc)))2]+Exn∼Pn[(1−DT(Mshift(Gn2c(xn))))2]\n(6)\nAs discussed in Section 3.1, we use the spectral discriminator DS to guide the generator to\ngenerate more realistic images by reducing the gap between the clean and denoised image in\nthe frequency domain. The spectral loss for Gn2c and DS is:\nLSpectral\nadv\n= Eyc∼Pc[(DS(vyc))2]+Exn∼Pn[(1−DS(vGn2c(xn)))2]\n(7)\nwhere v denotes the high-pass ﬁltered 1D spectral vector in Eq. 3.\nCycleGAN [49] imposes the two-sided cycle consistency constraint to learn the one-\nto-one mappings between two domains. On the other hand, we use only one-sided cycle\nconsistency to maintain the content between noisy and denoised images. By incorporating\na network Gc2n, we let Gc2n(Gn2c(xn)) be identical to the noisy image xn. The cycle consis-\ntency loss is expressed as:\nLCC = ||xn −Gc2n(Gn2c(xn))||1\n(8)\n6 N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\nwhere ||·||1 is the L1 norm.\nFurthermore, we add the reconstruction loss between the Gc2n(Gn2c(xn)) and xn to stabi-\nlize the training. We employ the negative SSIM loss [38] and combine it with the frequency\nreconstruction loss LFreq in Eq. 4. The reconstruction loss is expressed as:\nLRecon = LFreq(xn,Gc2n(Gn2c(xn)))+LSSIM(xn,Gc2n(Gn2c(xn)))\n(9)\nwhere LSSIM(a,b) denotes the negative SSIM loss, −SSIM(a,b).\nTo impose the local smoothness and mitigate the artifacts in the restored image, we adopt\nthe total variation loss [6]. The total variation loss is expressed as:\nLTV = ∑\nw,h\n(||▽wGn2c(xn)||2 +||▽h Gn2c(xn)||2)\n(10)\nwhere ||·||2 denotes the L2 norm, ▽w and ▽h are the operations to compute the gradients in\nterms of horizontal and vertical directions, respectively.\nInspired by [12, 37], we use the perceptual loss [21] to ensure that extracted features\nfrom the noisy and denoised image are semantically invariant. This allows the image to keep\nits semantics even after the noise has been removed. The perceptual loss is expressed as:\nLVGG = ||φl(xn)−φl(Gn2c(xn))||2\n(11)\nwhere φl(·) denotes the pre-trained VGG-19 [34] on ImageNet [11], l denotes lth layer from\nVGG-19, and we use the Conv5-4 layer of VGG-19 model in our experiments.\nMoreover, we employ the background loss to preserve background consistency between\nthe noisy and denoised image. The background loss constrains the L1 norm between blurred\nresults of the noisy and denoised image. As a blur operator, we adopt a guided ﬁlter [39] that\nsmooths the image while preserving the sharpness such as edges and details. The background\nloss is expressed as:\nLBG = ||GF(xn)−GF(Gn2c(xn))||1\n(12)\nwhere GF(·) denotes the guided ﬁlter.\nOur full objective for the two generators and the three discriminators is expressed as:\nmin\nGn2c,Gc2n\nmax\nDC,DT ,DS\nLClean\nadv\n+LTexture\nadv\n+LSpectral\nadv\n+LCC+\nλVGGLVGG +λBGLBG +λTVLTV +λReconLRecon\n(13)\nWe empirically deﬁne the weights in the full objective as: λVGG = 2, λBG = 2 , λTV = 0.2,\nand λRecon = 0.2.\n4\nExperiment\nIn this section, we provide the implementation details of the proposed method. Then, we\npresent extensive experiments on synthetic and real-world noisy images. Lastly, we conduct\nan ablation study to show the effectiveness of the proposed method. For synthetic noise, we\nuse Additive White Gaussian Noise (AWGN) to synthesize the noisy images. We adopt the\nCBSD68 [29] for evaluation. For real noise, we use the Low-Dose Computed Tomography\ndataset [31] and real photographs SIDD [1] to demonstrate the generalization capacity of the\nproposed method. We employ PSNR and SSIM [38] to evaluate the results.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE 7\n(a) Input\n(b) LPF\n(c) CBM3D\n(d) DnCNN\n(e) FFDNet\n(f) RedNet-30\n(g) N2N\n(h) DIP\n(i) N2V\n(j) LIR\n(k) Ours\n(l) GT\nFigure 3: Qualitative results of our method and other baselines on CBSD68 corrupted by\nAWGN with a noise level σ = 25.\nTraditional\nPaired setting\nUnpaired setting\nMethods\nLPF\nCBM3D [10]\nDnCNN [46]\nFFDNet [47]\nRedNet-30 [27]\nN2N [25]\nDIP [35]\nN2V [24]\nLIR [12]\nOurs\nNoise level\nPSNR (dB)\nσ = 15\n25.93\n33.55\n33.72\n29.68\n33.60\n33.92\n28.51\n28.66\n30.44\n32.21\nσ = 25\n24.61\n30.91\n30.85\n28.71\n30.68\n31.31\n27.26\n27.20\n29.08\n29.37\nσ = 50\n21.49\n27.47\n27.19\n26.79\n26.42\n28.10\n23.66\n24.52\n25.69\n26.03\nNoise level\nSSIM\nσ = 15\n0.7079\n0.9619\n0.9254\n0.8616\n0.9620\n0.9301\n0.8851\n0.9024\n0.9414\n0.9502\nσ = 25\n0.6102\n0.9331\n0.8724\n0.8254\n0.9308\n0.8857\n0.8613\n0.8684\n0.9126\n0.9124\nσ = 50\n0.4266\n0.8722\n0.7490\n0.7463\n0.8502\n0.7973\n0.7510\n0.7927\n0.8435\n0.8375\nTable 1: The average PSNR and SSIM results of our method and other baselines on CBSD68\ncorrupted by AWGN with noise levels σ = {15,25,50}. Our results are marked in bold.\n4.1\nImplementation Details\nWe implement our method with Pytorch [33]. The generator and discriminator architectures\nare detailed in the supplementary material. We train our method up to 100 epochs on Nvidia\nTITAN RTX GPU and RTX A6000 in experiments. We adopt ADAM [23] for optimization.\nThe initial learning rate is set to 0.0001, and we keep the same learning rate for the ﬁrst 70\nepochs and linearly decay the rate to zero over the last 30 epochs. We set the batch size to 16\nin all experiments. We randomly crop 128×128 patches for synthetic noise removal and use\ninput patches of size 256×256 for real-world noise removal. We randomly ﬂip the images\nhorizontally for data augmentation. For high-pass ﬁlter on spectral discriminator, rτ is set\nto ⌊H/2\n√\n2⌋where H is the height of an image and ⌊⌋is a ﬂoor operator. Loss weights are\ndescribed in Section 3.2. Our model is evaluated with three random seeds, and we report its\naverage values for rigorous evaluation.\n4.2\nSynthetic Noise Removal\nWe train the model with DIV2K [29] that contains 800 images with 2K resolution. For the\nunpaired training, we randomly divide the dataset into two parts without intersection. To\nconstruct a noise set, we add the AWGN with noise levels σ = {15,25,50} to images in one\npart using the other part as a clean set. For a fair comparison, we use only the noise set and\ntheir corresponding ground-truth when training other supervised learning-based methods.\n8 N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n(a) LDCT\n(b) BM3D\n(c) RED-CNN\n(d) DIP\n(e) LIR\n(f) Ours\n(g) NDCT\nFigure 4: Qualitative results of our method and other baselines on Mayo Clinic Low Dose\nCT dataset. (a) Real low-dose. (b)-(f) Results of each methods. (g) Real normal-dose. As\nshown in the highlighted red box, the reconstructed image by our method has few noise and\nartifacts. The display window is [160,240] HU.\nTraditional\nPaired setting\nUnpaired setting\nMethods\nBM3D [10]\nRED-CNN [7]\nDIP [35]\nLIR [12]\nOurs\nPSNR (dB)\n29.16\n29.39\n26.97\n27.26\n30.11\nSSIM\n0.8514\n0.9078\n0.8267\n0.8452\n0.8728\nTable 2: The average PSNR and SSIM results of different methods on Mayo Clinic Low\nDose CT dataset. Our results are marked in bold.\nWe select unsupervised methods, i.e. DIP [35], N2N [25], N2V [24], and LIR [12], and\nsupervised methods, i.e. DnCNN [46], FFDNet [47], and RedNet-30 [27], to compare the\nperformance. Traditional Low-Pass Filtering (LPF) and BM3D [10] are also evaluated. As\nshown in Figure 3, the unsupervised methods tend to shift the color and leave apparent visual\nartifacts in the sky. Especially, LIR removes the noise but fails to preserve the texture. With\nfrequency domain information, our method successfully eliminates noise and preserves the\ntexture. The classical LPF using Fourier transform alleviates the noise, but our framework\nthat reﬂects not only the frequency domain knowledge but also spatial domain knowledge\nshows superior results. As shown in Table 1, our model outperforms other unsupervised\nmethods, i.e. DIP, N2V, and LIR, by at least +0.29 dB in PSNR. Although our model is\ntrained on unpaired images, it achieves superior performance in the SSIM than DnCNN and\nFFDNet trained on paired datasets. We conjecture that the reason for better noise removal is\nthe use of the extra domain information that other previous methods do not consider.\n4.3\nReal-World Noise Removal\nIn this section, we evaluate the generalization ability of the proposed method on real-world\nnoise, i.e. Low-Dose Computed Tomography (CT) and real photographs. For the comparison\nof the Low-Dose CT, we adopt BM3D [10], DIP [35], RED-CNN [7], and LIR [12] as\nbaselines. For the comparison of the real photographs, BM3D [10], DIP [35], RedNet-30\n[27], and LIR [12] are selected as baselines.\nDenoising on Low-Dose CT\nSince Computed Tomography (CT) helps to diagnose abnor-\nmalities of organs, CT is widely used in medical analysis. Reducing the radiation dose in\norder to decrease health risks causes noise and artifacts in the reconstructed images. Like\nthe real-world noise, the noise distributions of the reconstructed image are difﬁcult to model\nanalytically. Therefore, we adopt a CT dataset authorized by Mayo Clinic [31] to evaluate\nthe generalization ability of our method on real-world noise. Mayo Clinic dataset consists\nof paired normal-dose and lose-dose CT images for each patient. The Normal-Dose CT\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE 9\n(a) Input\n(b) CBM3D\n(c) RedNet-30\n(d) DIP\n(e) LIR\n(f) Ours\n(g) GT\nFigure 5: Qualitative results of our method and other baselines on SIDD.\nTraditional\nPaired setting\nUnpaired setting\nMethods\nCBM3D [10]\nRedNet-30 [27]\nDIP [35]\nLIR [12]\nOurs\nPSNR (dB)\n28.32\n38.02\n24.68\n33.79\n34.30\nSSIM\n0.6784\n0.9619\n0.5901\n0.9466\n0.9334\nTable 3: The average PSNR and SSIM results of different methods on SIDD. Our results are\nmarked in bold.\n(NDCT) and the Low-Dose CT (LDCT) images correspond to clean and noisy images, re-\nspectively. For the training, we obtain 2,850 images in 512×512 resolution from 20 different\npatients. We construct 1,422 LDCT images from randomly selected 10 patients as a noise set\nand 1,428 NDCT images from the remaining patients as a clean set for unpaired training. For\nthe test, we obtain 865 images from 5 different patients. As shown in Table 2, our method\nachieves the best and the second-best performance in PSNR and SSIM, respectively. Note\nthat our model trained on the unpaired dataset outperforms the RED-CNN trained on the\npaired dataset in PSNR. It indicates that our method can be more practical in medical anal-\nysis where obtaining paired datasets is challenging. We also compare the qualitative results\nwith other baselines. As shown in Figure 4, other methods tend to generate artifacts or lose\ndetails. On the other hand, our method shows a reasonable balance between noise removal\nand image quality. More qualitative results are provided in the supplementary material.\nDenoising on Real Photographs\nTo demonstrate the effectiveness of our method on real\nnoisy photographs, we evaluate our method on SIDD [1] which is obtained from smartphone\ncameras. Because the images of the SIDD comprise various noise levels and brightness,\nthis dataset is the best appropriate to validate the generalization capacity of the denoisers.\nThe SIDD includes 320 pairs of noisy images and corresponding clean images with 4K or\n5K resolutions for the training. For the unpaired training, we divide the dataset into 160\nclean and 160 noisy images without intersection. The other training settings are the same\nas implementation details. For evaluation, we use 1280 cropped patches of size 256 × 256\nin the SIDD validation set. As show in Figure 5, other baselines tend to leave the noise or\nfail to preserve the color of images. In contrast, our method removes the intense noise while\nkeeping the color compared to other baselines. We also report the quantitative results in\nTable 3. More qualitative results are provided in the supplementary material.\n4.4\nAblation Study\nWe conduct an ablation study to demonstrate the validity of our key components: the texture\ndiscriminator DT, the spectral discriminator DS, and the frequency reconstruction loss LFreq.\nWe employ an additional evaluation metric LFD [20] to measure the difference between de-\nnoised images and reference images in the frequency domain. The small LFD value indicates\n10N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\nDS\nDT\nLFreq\nPSNR (dB)\nSSIM\nLFD\n\u0017\n\u0017\n\u0017\n25.59\n0.8290\n6.5955\n\u0013\n\u0017\n\u0017\n25.79\n0.8304\n6.5649\n\u0013\n\u0013\n\u0017\n25.82\n0.8334\n6.5874\n\u0013\n\u0013\n\u0013\n26.03\n0.8375\n6.5795\nTable 4: Ablation study. Quantitative results of our method with and without the texture\ndiscriminator DT, spectral discriminator DS, and frequency reconstruction loss LFreq on\nCBSD68 corrupted by AWGN with a noise level σ = 50. we report the PSNR, SSIM (higher\nis better) and LFD (lower is better). The best results are marked in bold.\nthat the denoised images are close to the reference images. First, to verify the effectiveness\nof the DS, we only add the DS to the base structure. As shown in Table 4, when the DS is\nintegrated, both PSNR and SSIM increase by 0.2 dB and 0.0014, respectively. It demon-\nstrates that the spectral discriminator leads the generator to remove high-frequency related\nnoise effectively by transferring the difference between noisy and clean images on the high-\nfrequency bands. Also, we see that the spectral discriminator makes the denoised images\nclose to clean domain images in the frequency domain, resulting in the decrease of LFD.\nNext, to verify the effectiveness of the DT, we integrate it with the DS. Distinguishing the\ntexture representations helps restore clean contours and ﬁne details related to image quality,\nwhich improves the SSIM metric. A curious phenomenon is that the texture discriminator\nincreases the LFD. We conjecture that the introduction of DT causes a bias to the spatial do-\nmain in maintaining the balance between the spatial and frequency domains, thus increasing\nthe distance in the frequency domain. Adding the LFreq shows results validating our hypoth-\nesis that narrowing the gap in the frequency domain is crucial to generate the high-quality\ndenoised image. In addition, through the decrease of LFD, the frequency reconstruction loss\nmay help to maintain the balance between the spatial and frequency domain.\n5\nConclusion\nIn this paper, we propose an unsupervised learning-based image denoiser that enables the\nimage denoising without clean and noisy image pairs. To the best of our knowledge, it\nis the ﬁrst approach that aims to recover a noise-free image from a corrupted image using\nfrequency domain information. To this end, we introduce the spectral discriminator and\nfrequency reconstruction loss that can propagate the frequency knowledge to the generator.\nBy reﬂecting the information from the frequency domain, our method successfully focuses\non high-frequency components to remove noise. Experiments on synthetic and real noise\nremoval show that our method outperforms other unsupervised learning-based denoisers and\ngenerates more visually pleasing images with fewer artifacts. We believe that considering\nthe frequency domain can be advantageous in other low-level vision tasks as well.\n6\nAcknowledgements\nThis work was supported by the Engineering Research Center of Excellence (ERC) Pro-\ngram supported by National Research Foundation (NRF), Korean Ministry of Science &\nICT (MSIT) (Grant No. NRF-2017R1A5A1014708).\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE11\nReferences\n[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality de-\nnoising dataset for smartphone cameras. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 1692–1700, 2018.\n[2] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for design-\ning overcomplete dictionaries for sparse representation. IEEE Transactions on signal\nprocessing, 54(11):4311–4322, 2006.\n[3] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast, accurate, and lightweight\nsuper-resolution with cascading residual network.\nIn Proceedings of the European\nConference on Computer Vision (ECCV), pages 252–268, 2018.\n[4] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image de-\nnoising. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’05), volume 2, pages 60–65. IEEE, 2005.\n[5] Mu Cai, Hong Zhang, Huijuan Huang, Qichuan Geng, and Gao Huang. Frequency do-\nmain image translation: More photo-realistic, better identity-preserving. arXiv preprint\narXiv:2011.13611, 2020.\n[6] Antonin Chambolle. An algorithm for total variation minimization and applications.\nJournal of Mathematical imaging and vision, 20(1):89–97, 2004.\n[7] Hu Chen, Yi Zhang, Mannudeep K Kalra, Feng Lin, Yang Chen, Peixi Liao, Jiliu\nZhou, and Ge Wang. Low-dose ct with a residual encoder-decoder convolutional neural\nnetwork. IEEE transactions on medical imaging, 36(12):2524–2535, 2017.\n[8] Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising\nwith generative adversarial network based noise modeling. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 3155–3164, 2018.\n[9] Yuanqi Chen, Ge Li, Cece Jin, Shan Liu, and Thomas Li. Ssd-gan: Measuring the\nrealness in the spatial and spectral domains. arXiv preprint arXiv:2012.05535, 2020.\n[10] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image\ndenoising by sparse 3-d transform-domain collaborative ﬁltering. IEEE Transactions\non image processing, 16(8):2080–2095, 2007.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248–255. Ieee, 2009.\n[12] Wenchao Du, Hu Chen, and Hongyu Yang. Learning invariant representation for unsu-\npervised image restoration. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14483–14492, 2020.\n[13] Ricard Durall, Margret Keuper, and Janis Keuper. Watch your up-convolution: Cnn\nbased generative deep neural networks are failing to reproduce spectral distributions.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 7890–7899, 2020.\n12N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n[14] Tarik Dzanic, Karan Shah, and Freddie Witherden. Fourier spectrum discrepancies in\ndeep network generated images. arXiv preprint arXiv:1911.06465, 2019.\n[15] Joel Frank, Thorsten Eisenhofer, Lea Schönherr, Asja Fischer, Dorothea Kolossa, and\nThorsten Holz. Leveraging frequency analysis for deep fake image recognition. In\nInternational Conference on Machine Learning, pages 3247–3258. PMLR, 2020.\n[16] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks.\narXiv preprint arXiv:1406.2661, 2014.\n[17] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm\nminimization with application to image denoising. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pages 2862–2869, 2014.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image trans-\nlation with conditional adversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1125–1134, 2017.\n[20] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Focal frequency loss for\ngenerative models. arXiv preprint arXiv:2012.12821, 2020.\n[21] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style\ntransfer and super-resolution. In European conference on computer vision, pages 694–\n711. Springer, 2016.\n[22] Yoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning\nfrom synthetic to real-noise denoising with adaptive instance normalization. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3482–3492, 2020.\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[24] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denois-\ning from single noisy images. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 2129–2137, 2019.\n[25] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika\nAittala, and Timo Aila. Noise2noise: Learning image restoration without clean data.\narXiv preprint arXiv:1803.04189, 2018.\n[26] Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro, and Andrew Zisserman.\nNon-local sparse models for image restoration. In 2009 IEEE 12th international con-\nference on computer vision, pages 2272–2279. IEEE, 2009.\n[27] Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang.\nImage restoration using very\ndeep convolutional encoder-decoder networks with symmetric skip connections. arXiv\npreprint arXiv:1603.09056, 2016.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE13\n[28] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen\nPaul Smolley. Least squares generative adversarial networks. In Proceedings of the\nIEEE international conference on computer vision, pages 2794–2802, 2017.\n[29] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human\nsegmented natural images and its application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In Proceedings Eighth IEEE International Conference\non Computer Vision. ICCV 2001, volume 2, pages 416–423. IEEE, 2001.\n[30] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral\nnormalization for generative adversarial networks. arXiv preprint arXiv:1802.05957,\n2018.\n[31] Taylor R Moen, Baiyu Chen, David R Holmes III, Xinhui Duan, Zhicong Yu, Lifeng\nYu, Shuai Leng, Joel G Fletcher, and Cynthia H McCollough. Low-dose ct image and\nprojection dataset. Medical physics, 48(2):902–911, 2021.\n[32] Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu, and Wotao Yin. An iterative\nregularization method for total variation-based image restoration. Multiscale Modeling\n& Simulation, 4(2):460–489, 2005.\n[33] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. In NIPS-W, 2017.\n[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[35] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Pro-\nceedings of the IEEE conference on computer vision and pattern recognition, pages\n9446–9454, 2018.\n[36] Stefan Van der Walt, Johannes L Schönberger, Juan Nunez-Iglesias, François\nBoulogne, Joshua D Warner, Neil Yager, Emmanuelle Gouillart, and Tony Yu. scikit-\nimage: image processing in python. PeerJ, 2:e453, 2014.\n[37] Xinrui Wang and Jinze Yu. Learning to cartoonize using white-box cartoon represen-\ntations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8090–8099, 2020.\n[38] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality\nassessment: from error visibility to structural similarity. IEEE transactions on image\nprocessing, 13(4):600–612, 2004.\n[39] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Fast end-to-end trainable\nguided ﬁlter. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1838–1847, 2018.\n[40] Jinjun Xu and Stanley Osher. Iterative regularization and nonlinear inverse scale space\napplied to wavelet-based denoising. IEEE Transactions on Image Processing, 16(2):\n534–544, 2007.\n14N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n[41] Jun Xu, Lei Zhang, David Zhang, and Xiangchu Feng. Multi-channel weighted nu-\nclear norm minimization for real color image denoising. In Proceedings of the IEEE\ninternational conference on computer vision, pages 1096–1104, 2017.\n[42] Jun Xu, Lei Zhang, and David Zhang. A trilateral weighted sparse coding scheme for\nreal-world image denoising. In Proceedings of the European conference on computer\nvision (ECCV), pages 20–36, 2018.\n[43] Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, and Fengbo Ren.\nLearning in the frequency domain. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1740–1749, 2020.\n[44] Yanchao Yang and Stefano Soatto. Fda: Fourier domain adaptation for semantic seg-\nmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4085–4095, 2020.\n[45] Yanchao Yang, Dong Lao, Ganesh Sundaramoorthi, and Stefano Soatto. Phase con-\nsistent ecological domain adaptation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 9011–9020, 2020.\n[46] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaus-\nsian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions\non image processing, 26(7):3142–3155, 2017.\n[47] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and ﬂexible solution\nfor cnn-based image denoising. IEEE Transactions on Image Processing, 27(9):4608–\n4622, 2018.\n[48] Xu Zhang, Svebor Karaman, and Shih-Fu Chang. Detecting and simulating artifacts in\ngan fake images. In 2019 IEEE International Workshop on Information Forensics and\nSecurity (WIFS), pages 1–6. IEEE, 2019.\n[49] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-\nimage translation using cycle-consistent adversarial networks. In Proceedings of the\nIEEE international conference on computer vision, pages 2223–2232, 2017.\n[50] Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole\nimage restoration. In 2011 International Conference on Computer Vision, pages 479–\n486. IEEE, 2011.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE15\nA\nSupplementary Material\nIn this supplementary material, we describe the architecture details and show the additional\nexperiments as follows:\n• In Section B, we describe the architectures of two generators, i.e. Gn2c and Gc2n, and\nthree discriminators, i.e. DC, DT, and DS, in our framework.\n• In Section C, we show the additional results on CBSD68 [29] corrupted by AWGN\nwith a noise level σ = 25.\n• In Section D, we show the additional qualitative results on real-world noise, i.e. Low-\nDose CT authorized by Mayo Clinic [31] and SIDD [1].\n• In Section E, we show the results of an additional ablation study to demonstrate the\nvalidity of the perceptual loss LVGG, the cycle consistency loss LCC, and the recon-\nstruction loss LRecon.\n• In Section F, we show the results on several noise types, such as structured noise and\nPoisson noise, to evaluate the generalization ability of our method.\nB\nThe Details of Architectures\nGenerator Gn2c\nFor the noise removal generator Gn2c, we adopt the network introduced\nby [3]. The main idea of this architecture is multiple cascading connections at global and\nlocal levels which help to propagate low-level information to later layers and remove noise.\nThe details of Gn2c are illustrated in Figure 6 and 7.\nGenerator Gc2n\nFor the generator Gc2n, we adopt the U-Net based network that is similar\nto the architecture introduced by [22]. The role of this network is to translate images from\nthe noise domain to the clean domain. The details of Gc2n are illustrated in Figure 8 and 9.\nDiscriminators DC and DT\nFor the discriminators DC and DT, we employ the 70 × 70\nPatchGAN discriminator [19] which classiﬁes whether 70 × 70 image patches are real or\nfake. The details of DC and DT are illustrated in Figure 10.\nDiscriminator DS\nFor the spectral discriminator DS, we employ the single linear unit as\nthe spectral discriminator. The DS takes a high-pass ﬁltered 1D spectral vector and aims to\nclassify whether the spectral vector is real or fake.\nC\nAdditional Results on AWGN\nWe additionally visualize the results for CBSD68 images corrupted by AWGN with a noise\nlevel σ = 25 and show the PSNR and SSIM in Figure 11 and 12. In Figure 11, our method\noutperforms other methods trained with unpaired dataset by at least +3.44dB and +0.08 in\nterms of PSNR and SSIM, respectively. LIR and N2V spoil the color and lights, but our\nmethod preserves both the color and lights and successfully removes the noise. We also\nshow the challenging example that has repetitive high-frequency patterns hard to distinguish\n16N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\nC\nC\nC\nResidual Block\nReLU\n1x1 Convolution\nC\nConcatenation\nFigure 6: The architecture of Cascading Block used as the basic component in the Gn2c. We\nuse the Residual Block proposed by [18] and the ReLU.\n3x3 Convolution\nReLU\nCascading Block\n1x1 Convolution\nC\nC\nC\nC\nConcatenation\nW x H x 64\nW x H x 64\nW x H x 64\nW x H x 64\nW x H x 64\nW x H x 64\nW x H x 3\nFigure 7: The architecture of generator Gn2c for noise removal. We use the convolution with\nkernel size=3, stride=1, and padding=1.\n3x3 Convolution\nInstance Normalization\nLeakyReLU\n+\nElement-Wise Summation\n+\nFigure 8: The architecture of Instance Residual Block used as the basic component in\nthe Gc2n. We use the convolution with kernel size=3, stride=1, and padding=1 and the\nLeakyReLU with a slope of 0.2.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE17\n3x3 Convolution\nGlobal Average Pooling\nInstance Residual Block\nTranspose Convolution\n+\nElement-Wise Summation\n+\n+\nW x H x 32\nW x H x 32 W x H x 3\n+\n𝑾\n𝟐x 𝑯\n𝟐x 64\n𝑾\n𝟒x 𝑯\n𝟒x 128\n𝑾\n𝟐x 𝑯\n𝟐x 64\nFigure 9:\nThe architecture of generator Gc2n.\nWe use the convolution with kernel\nsize=3, stride=1, and padding=1 and transposed convolution with kernel size=3, stride=2,\npadding=1, and output padding=1.\n4x4 Convolution\nSpectral Normalization\nLeakyReLU\n𝑾\n𝟐x \n𝑯\n𝟐x 64\n𝑾\n𝟒x \n𝑯\n𝟒x 128\n(\n𝑾\n𝟖−𝟏) x (\n𝑯\n𝟖−𝟏) x 512\n𝑾\n𝟖x \n𝑯\n𝟖x 256\n(\n𝑾\n𝟖−𝟐) x (\n𝑯\n𝟖−𝟐) x 1\nStride = 2\nStride = 2\nStride = 2\nStride = 1\nStride = 1\nFigure 10: The architecture of discriminators DC and DT. We use the convolution with kernel\nsize=4 and padding=1. Followed by the convolution, we use the spectral normalization [30]\nand the LeakyReLU with a slope of 0.2.\nwith noise in Figure 12. Our approach removes noise without artifact and also preserves\nthe patterns of the zebra. Although our method is trained under unpaired settings, it shows\ncomparable performance in PSNR and SSIM with the supervised models in Figure 12. Fur-\nthermore, compared to methods trained with unpaired dataset, our approach achieves the\nbest performance in both PSNR and SSIM.\nD\nAdditional Qualitative Results on Real-World Noise\nD.1\nLow-Dose CT\nIn this subsection, we show the additional qualitative results on Low-Dose CT dataset au-\nthorized by Mayo Clinic [31] in Figure 13. As shown in Figure 13, previous methods tend\nto lose details and generate blurred results. However, our method removes the noise, while\npreserving the details of organs. It shows that our method is also practical for medical image\ndenoising.\n18N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n(a) Input (21.21/0.55)\n(b) LPF (21.43/0.60)\n(c) CBM3D (28.55/0.91)\n(d) DnCNN (28.64/0.92)\n(e) FFDNet (24.56/0.83)\n(f) RedNet-30 (27.46/0.91)\n(g) N2N (28.92/0.92)\n(h) DIP (22.90/0.74)\n(i) N2V (24.05/0.81)\n(j) LIR (19.48/0.82)\n(k) Ours (27.49/0.90)\n(l) GT (PSNR/SSIM)\nFigure 11: Qualitative results of our method and other baselines on CBSD68 corrupted by\nAWGN with a noise level σ = 25.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE19\n(a) Input (20.27/0.44)\n(b) LPF (21.29/0.58)\n(c) CBM3D (29.99/0.85)\n(d) DnCNN (30.32/0.87)\n(e) FFDNet (26.29/0.81)\n(f) RedNet-30 (30.50/0.88)\n(g) N2N (30.71/0.88)\n(h) DIP (27.40/0.77)\n(i) N2V (26.73/0.75)\n(j) LIR (26.04/0.83)\n(k) Ours (28.35/0.83)\n(l) GT (PSNR/SSIM)\nFigure 12: Qualitative results of our method and other baselines on CBSD68 corrupted by\nAWGN with a noise level σ = 25.\n20N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\nD.2\nReal Photographs\nIn this subsection, we visualize the additional qualitative results on SIDD [1] in Figure 14\nand 15. As shown in Figure 14, previous methods tend to lose the texture and leave the\nnoise. In contrast, our method removes the noise while preserving the texture compared to\nother baselines. In Figure 15, we observe that our method removes the intense noise while\npreserving the color of images compared to other baselines.\nE\nAdditional Ablation Study\nWe conduct an additional ablation study to demonstrate the validity of the perceptual loss\nLVGG, the cycle consistency loss LCC, and the reconstruction loss LRecon. First, to verify the\neffectiveness of the LVGG, we only add the LVGG. As shown in Table 5, when the LVGG\nis used, both PSNR and SSIM increase by 0.07dB and 0.0068. It demonstrates that the\nperceptual loss LVGG helps to improve the performance, preserving the semantics even after\nthe noise has been removed. Next, to verify the contribution of LCC, we integrate it with\nthe LVGG. We observe that the LCC which enables the one-to-one mapping between noisy\nand denoised images improves the PSNR and SSIM by 0.08dB and 0.004. Finally, when we\nintegrate the LRecon with the LVGG and the LCC, both PSNR and SSIM increase by 0.15dB\nand 0.0063, thus showing the best results in PSNR and SSIM. Through this experiment, we\nvalidate that each of the losses contributes to the performance improvement.\nLVGG\nLCC\nLRecon\nPSNR (dB)\nSSIM\n\u0017\n\u0017\n\u0017\n25.67\n0.8204\n\u0013\n\u0017\n\u0017\n25.74\n0.8272\n\u0013\n\u0013\n\u0017\n25.88\n0.8312\n\u0013\n\u0013\n\u0013\n26.03\n0.8375\nTable 5: Ablation study. Quantitative results of our method with and without the perceptual\nloss LVGG, the cycle consistency loss LCC, and the reconstruction loss LRecon on CBSD68\ncorrupted by AWGN with a noise level σ = 50. We report the PSNR and SSIM (higher is\nbetter). The best results are marked in bold.\nF\nEvaluation on Several Noise Types\nF.1\nStructured Noise\nIn this subsection, we show the results on structured noise. To generate the structured noise,\nwe sample the pixel-wise i.i.d white noise, and convolve it with a 2D Gaussian ﬁlter whose\na kernel size is 21 × 21 and σ is 3 pixel. For the train and evaluation, we follow the same\nsetting as the setting for synthetic noise removal in the main paper. As shown in Figure 16,\nour method is able to remove complex noise compared to BM3D [10] and DIP [35]. Fur-\nthermore, while LIR [12] spoil the lights, our method successfully preserves both the color\nand lights. The quantitative results are summarized in Table 6. Our method outperforms\nthe traditional and unsupervised methods, achieving the second-best performance in terms\nof PSNR and SSIM.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE21\n(a) LDCT\n(b) BM3D\n(c) RED-CNN\n(d) DIP\n(e) LIR\n(f) Ours\n(g) NDCT\n(h) LDCT\n(i) BM3D\n(j) RED-CNN\n(k) DIP\n(l) LIR\n(m) Ours\n(n) NDCT\nFigure 13: Qualitative results of our method and other baselines on Mayo Clinic Low Dose\nCT dataset. As shown in the highlighted red box, the reconstructed images by our method\nhave few noise and preserve the details of organs. The display window is [160,240] HU.\nTraditional\nPaired setting\nUnpaired setting\nMethods\nCBM3D [10]\nRedNet-30 [27]\nDIP [35]\nLIR [12]\nOurs\nPSNR (dB)\n20.62\n28.51\n20.70\n16.90\n25.18\nSSIM\n0.5650\n0.9588\n0.7239\n0.3738\n0.9026\nTable 6: The average PSNR and SSIM results of different methods on CBSD68 corrupted by\nstructured noise. Our results are marked in bold.\n22N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n(a) Input\n(b) CBM3D\n(c) RedNet-30\n(d) DIP\n(e) LIR\n(f) Ours\n(g) GT\n(h) Input\n(i) CBM3D\n(j) RedNet-30\n(k) DIP\n(l) LIR\n(m) Ours\n(n) GT\nFigure 14: Qualitative results of our method and other baselines on real noisy data, SIDD.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE23\n(a) Input\n(b) CBM3D\n(c) RedNet-30\n(d) DIP\n(e) LIR\n(f) Ours\n(g) GT\n(h) Input\n(i) CBM3D\n(j) RedNet-30\n(k) DIP\n(l) LIR\n(m) Ours\n(n) GT\nFigure 15: Qualitative results of our method and other baselines on real noisy data, SIDD.\n24N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n(a) Input\n(b) CBM3D\n(c) RedNet-30\n(d) DIP\n(e) LIR\n(f) Ours\n(g) GT\nFigure 16: Qualitative results of our method and other baselines on CBSD68 corrupted by\nstructured noise.\nN. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE25\nF.2\nPoisson Noise\nIn the comparisons of Poisson noisy images, we use Kodak24 as the test dataset. The images\nare corrupted by independent Poisson noise from Scikit-image library [36]. We train the\nmodels following the settings in the main paper. The visualized results of Poisson noise\nremoval are given in Figure 17 and 18. Our approach shows impressive noise removal results.\nWhile LIR and DIP fail to remove the Poisson noise, our method successfully eliminates the\nnoise and preserves the colors. In Table 7, our method achieves the best performance in\nPSNR and the second-best performance in terms of SSIM even when it is trained under the\nunpaired dataset. It demonstrates that our method has robustness and generalization against\nvarious noise types. Note that we do not change any hyper-parameters when trained under\nseveral types of noise.\n(a) Input\n(b) CBM3D\n(c) RedNet-30\n(d) DIP\n(e) LIR\n(f) Ours\n(g) GT\nFigure 17: Qualitative results of our method and other baselines on Kodak24 corrupted by\nPoisson noise.\nTraditional\nPaired setting\nUnpaired setting\nMethods\nCBM3D [10]\nRedNet-30 [27]\nDIP [35]\nLIR [12]\nOurs\nPSNR (dB)\n32.36\n29.59\n29.59\n26.20\n34.93\nSSIM\n0.8694\n0.9778\n0.8774\n0.7741\n0.9691\nTable 7: The average PSNR and SSIM results of different methods on Kodak24 dataset\ncorrupted by Poisson noise. Our results are marked in bold.\n26N. KIM, D. JANG ET AL: IMAGE DENOISING WITH FREQUENCY DOMAIN KNOWLEDGE\n(a) Input\n(b) CBM3D\n(c) RedNet-30\n(d) DIP\n(e) LIR\n(f) Ours\n(g) GT\nFigure 18: Qualitative results of our method and other baselines on Kodak24 corrupted by\nPoisson noise.\n",
  "categories": [
    "eess.IV",
    "cs.CV"
  ],
  "published": "2021-11-29",
  "updated": "2021-11-29"
}