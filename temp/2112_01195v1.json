{
  "id": "http://arxiv.org/abs/2112.01195v1",
  "title": "Maximum Entropy Model-based Reinforcement Learning",
  "authors": [
    "Oleg Svidchenko",
    "Aleksei Shpilman"
  ],
  "abstract": "Recent advances in reinforcement learning have demonstrated its ability to\nsolve hard agent-environment interaction tasks on a super-human level. However,\nthe application of reinforcement learning methods to practical and real-world\ntasks is currently limited due to most RL state-of-art algorithms' sample\ninefficiency, i.e., the need for a vast number of training episodes. For\nexample, OpenAI Five algorithm that has beaten human players in Dota 2 has\ntrained for thousands of years of game time. Several approaches exist that\ntackle the issue of sample inefficiency, that either offers a more efficient\nusage of already gathered experience or aim to gain a more relevant and diverse\nexperience via a better exploration of an environment. However, to our\nknowledge, no such approach exists for model-based algorithms, that showed\ntheir high sample efficiency in solving hard control tasks with\nhigh-dimensional state space. This work connects exploration techniques and\nmodel-based reinforcement learning. We have designed a novel exploration method\nthat takes into account features of the model-based approach. We also\ndemonstrate through experiments that our method significantly improves the\nperformance of the model-based algorithm Dreamer.",
  "text": "Maximum Entropy Model-based Reinforcement\nLearning\nOleg Svidchenko\nJetBrains Research\nHSE University\nSaint Petersburg, Russia\noleg.svidchenko@jetbrains.com\nAleksei Shpilman\nJetBrains Research\nHSE University\nSaint Petersburg, Russia\naleksei@shpilman.com\nAbstract\nRecent advances in reinforcement learning have demonstrated its ability to solve\nhard agent-environment interaction tasks on a super-human level. However, the\napplication of reinforcement learning methods to a practical and real-world tasks\nis currently limited due to most RL state-of-art algorithms’ sample inefﬁciency,\ni.e., the need for a vast number of training episodes. For example, OpenAI Five\nalgorithm that has beaten human players in Dota 2 has trained for thousands of\nyears of game time. Several approaches exist that tackle the issue of sample\ninefﬁciency, that either offer a more efﬁcient usage of already gathered experience\nor aim to gain a more relevant and diverse experience via a better exploration of an\nenvironment. However, to our knowledge, no such approach exist for model-based\nalgorithms, that showed their high sample efﬁciency in solving hard control tasks\nwith high-dimensional state space. This work connects exploration techniques and\nmodel-based reinforcement learning. We have designed a novel exploration method\nthat takes into account features of the model-based approach. We also demonstrate\nthrough experiments that our method signiﬁcantly improves the performance of\nmodel-based algorithm Dreamer.\n1\nIntroduction\nIn recent years deep reinforcement learning have demonstrated the ability to solve complex control\ntasks such as playing strategic games with super-human performance [1, 2], controlling complex\nrobotic systems [3], and others. However, most of popular reinforcement learning algorithms\nrequire a vast amount of environment interaction experience to achieve this result. For example,\nOpenAI Five [2] takes more than ten thousand years of simulated game time to master Dota 2. This\nlimitation prevents reinforcement learning from being widely applied to real-world tasks.\nSeveral methods attempt to solve the problem of sample efﬁciency. Some approach this problem as the\nproblem of efﬁcient use of already collected data and utilize the idea of experience prioritization [4]\nor data augmentation and normalization to prevent overﬁtting to a small data sample [5, 6, 7]. Others\nencourage agents to explore the environment more efﬁciently and collect more diverse experiences\n[8, 9, 10, 11].\nModel-based reinforcement learning algorithms create a representation of the environments to train\nan agent, rather than only interact with the original environment. This also leads to a signiﬁcantly\nmore sample efﬁcient solutions than model-free approach [12, 13]. Recently, some researchers have\nused [9, 11] simple learned world models to assign intrinsic rewards for an agent. However, to our\nknowledge, no research exist, that bind efﬁcient exploration methods with model-based algorithms.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2112.01195v1  [cs.AI]  2 Dec 2021\nThe main contribution of this work is that we create a connection between exploration methods\nand model-based reinforcement learning. We present a method of maximum-entropy exploration\nfor model-based reinforcement learning agents and build a MaxEnt Dreamer that improves the\nperformance to model-based Dreamer [12]. We also introduce some additional modiﬁcations to\nimprove the stability and perform an ablations study to show their effectiveness. Experimental results\nshow that our exploration method and our modiﬁcations signiﬁcantly improve the performance of\nDreamer. We also perform an ablation study to show that it is indeed exploration techniques that\ncontribute the majority of the improvement.\n2\nBackground\n2.1\nReinforcement Learning\nReinforcement Learning is a part of Machine Learning that focuses on solving agent-environment\ninteraction tasks. In such tasks, the environment usually has state s at each time moment, and the\nagent has an observation o that somehow correlates a state s. At each time step, agent commits an\naction a that leads to transition from state s to some state s′. Then, agent receive a reward r for\ntransition (s, a, s′) and observation o′ that corresponds to a state s′.\nMore formally, in our work, we assume partially observable Markov decision process (POMDP) which\nconsists of state space S, action space A, observation space O, transition function T : S × A →S,\nreward function R : S × A × S →R, observation function fO : S →O and episode termination\nfunction D : S →{0, 1}. All this functions may be stochastic. The goal is to ﬁnd such a policy\nπ : O →A that maximizes expected cumulative discounted reward Eτ|π\nPT\ni=0 γiri where τ is a\ntrajectory that consists of transitions (o, s, a, s′, o′, r, d) acquired from an POMDP with respect to\npolicy π, T is a number of transitions in the trajectory τ and γ ∈(0, 1) is a discount factor. To\nestimate an effectiveness of a speciﬁc policy when start acting in a state s there are a value function\nwhich is deﬁned as:\nVπ(s) = Ea∼π(.|fO(s)), s′∼T (.|s,a)[r + (1 −d) · γ · Vπ(s′))]\n(1)\nwhere d = D(s, a, s′) and r = R(s, a, s′). There are also similar function for a speciﬁc state-action\npair that called Q-function:\nQπ(s, a) = Es′∼T (.|s,a)[r + (1 −d) · γ · Ea′∼π(.|fO(s′))Qπ(s′, a′)]\n(2)\nUsually, reinforcement learning algorithms solve this problem in the assumption that environment\ndynamics (T, R, fO and D) are unknown and therefore agents can gain experience only from\ninteractions with an environment. Such algorithms are called model-free algorithms because that they\ndo not depend on information about environment dynamics.\nOne of the classic modern RL algorithms is the Deep Deterministic Policy Gradient algorithm [14].\nIt utilizes actor-critic architecture. Critic C is a neural network that estimates Q-function (eq. 2) of\na speciﬁc observation-action pair. It is trained by gradient descent with a loss function built upon\ntime-difference error (TD-error): L = E(o,a,o′,r,d(C(o, a) −(r + γ · (1 −d) · QA(o′, A(o′)))2. In\npractice, QA is usually replaced by value yield by a target critic network that slowly updates to match\nthe critic network. Actor A is a neural network that maps observations into an action space. It is\ntrained by a stochastic gradient descent algorithm with a loss function deﬁned as L = −EoC(o, A(o))\nin order to maximize an expectation of Q-function produced by the critic.\nAnother important algorithm is the Proximal Policy Optimization method [15]. As in DDPG, it\nalso utilizes actor-critic architecture to solve the task with continuous observation and action spaces.\nIn this algorithm, critic estimates value function (eq. 1) instead of Q-function. In PPO, the actor\nproduces not a single action but a distribution of actions and trains to maximize the probability of\nactions that lead to a better outcome.\nWe can interpret the Soft Actor-Critic algorithm [16] as a combination of PPO and DDPG algorithms.\nLike the PPO algorithm, it uses stochastic policy instead of a deterministic one, but unlike PPO\ntrains it to maximize Q-function directly as in DDPG. Moreover, the authors add additional action\n2\ndistribution entropy loss to an actor to prevent the policy from converging to a single-value distribution\nseen as a deterministic policy.\n2.2\nModel-based Reinforcement Learning\nA model-based reinforcement learning assumes that agents have access to a world model that can\npredict full transitions (s, a, s′, r, d) by state-action pair.\nFor most environments, the true world model is unknown as transition function, episode termination\nfunction or reward function are too complex or hidden from us. To deal with it, modern model-based\nmethods [17, 12, 13] use trainable world models based on neural networks.\nRecently, the model-based approach demonstrated good results in solving complex RL tasks by\nhaving a better sample efﬁciency. Speciﬁcally, the Dreamer algorithm [12] was able to solve MuJoCo\ntasks from pixels signiﬁcantly faster and better than model-free algorithms like D4PG and A3C.\nDreamer algorithm learns the world model with the assumption of POMDP. It builds a hidden state\nrepresentation using recurrent neural networks and then uses it to generate synthetic trajectories. The\nalgorithm uses only synthetic data to train an agent, which allows to compute gradient through it and\nupdate actor by the sum of predicted rewards. During the evaluation, the world model only encodes\nobservation into hidden state representation and then passes it to an actor that yields an action.\nFigure 1: Original architecture of the Dreamer world model. From [12].\nThe Dreamer world model (ﬁg. 1) consists of three main parts:\n1. Processing of deterministic part hi of the state (orange on ﬁg. 1). It consists of dense and\nrecurrent blocks. The dense block takes action and hidden state as input and passes its\noutput to the recurrent block. Recurrent block takes output of dense block as input and hi as\nhidden features and outputs hi+1.\n2. Processing of stochastic part si of the state (blue on ﬁg. 1). It consists of convolutional the\nEncoder block that estimates p(zi|hi, oi), and the inverse convolutional Decoder block that\ntries to reconstruct oi from hi and zi, and a block that estimates prior distribution. p(zi|hi).\n3. A block that tries to predict gained reward ri from achieving state si = (zi, hi).\nWhile interacting with environment, world model takes hi, zi, ai and oi+1 as input and outputs hi+1\nand zi+1 ∼p(zi+1|hi+1, oi+1). While generating synthetic experience, it uses prior distribution\np(zi|hi) instead of p(zi|hi, oi) to produce zi.\nTo train the world model, Dreamer adapts ELBO-loss from variational autoencoders so that it encodes\nsequence of observations {oi} and decodes sequence of observations {oi} and rewards {ri}. It uses\n3\nhi and zi as latent representation with trainable prior distribution p(zi|hi). The general form of loss\nused for training is:\nL = E\nX\ni\n−[ ln p(ot|zi, hi) + ln p(rt|ht, zt)\n−βDKL(p(zi+1|zi, hi, oi+1)∥p(zi+1|hi)]\n(3)\nIn practice, p(ot|zi, hi) and p(rt|ht, zt) are presented as normal distributions with ﬁxed standard\ndeviation. Therefore, eq. 3 may be represented as:\nL = E\nX\ni\n−[MSE(ot, o′\nt) + MSE(rt, r′\nt)\n−βDKL(p(zi+1|zi, hi, oi+1)∥p(zi+1|hi)]\n(4)\nThis world model allows us to train an agent by computing gradients of cumulative discounted\nreward and then applying gradient ascent. Authors of the Dreamer algorithm propose to use a\nλ-returns estimation method to estimate cumulative discounted reward. This method was initially\nproposed in the Proximal Policy Optimization algorithm and works in the assumption of critic ˆV\nthat estimates value function for the current policy. λ-returns combine rollouted cumulative rewards\nGt = Pt\ni=0 γi · ri + γt+1 · Vπ(st+1) as follows:\nGλ\nT = (1 −λ)\nT\nX\nt=0\nλt · Gt + λT · G0\nWhich can be rewritten as:\nGλ\nT = λT γT Vπ(sT ) +\nT −1\nX\nt=0\nγtλt · (rt + γ · (1 −λ)Vπ(st+1))\n(5)\nHere Vπ(s) is a value function of state s with respect to policy π which in practice are replaced by its\nestimation ˆV produced by a trained critic.\nThere are also several modiﬁcations of a Dreamer algorithm that improve its efﬁciency. For instance,\nthe authors of Dreamer-v2 [13] offers multiple changes that allow modiﬁed algorithm produce a\npolicy that can perform as well as state-of-the-art algorithms do on the Atari Games benchmark.\nHowever, most proposed modiﬁcations are not fundamental and mostly tunes algorithms to the set of\nenvironments. Another important modiﬁcation that we should mention is a Dreaming algorithm [18]\nthat replaces observation reconstruction with a contrastive learning algorithm, which makes the world\nmodel learn more informative state representations.\n2.3\nExploration problem\nTraining with no prior knowledge of POMDP dynamics forces an agent to explore new parts to ﬁnd\ninformation about more optimal transitions. For some environments, ﬁnding information that will\nhelp the agent improve its policy is a complex task.\nPopular model-free algorithms often use either random actions (epsilon-greedy approach), additive\nrandom noise (as in DDPG [14]), or regularization of action distribution entropy (as in SAC [16]).\nHowever, all these methods do not aim to maximize the novelty of the agent’s information explicitly.\nTherefore, they do not work for environments where it is necessary to commit a speciﬁc sequence of\nactions to reach speciﬁc parts of state space.\nPlenty of exploration algorithms directly seek novel information about the environment dynamics.\nSome of them [10, 11, 9, 19] motivate an agent to explore via an additional intrinsic reward. Most\nof these algorithms depend on a prediction error of some trainable model to measure how novel a\ntransition is and assign an intrinsic reward depending on the novelty.\n4\nHazan et al. [8] have proposed an alternative approach that maximizes the entropy of the state\ndistribution directly, which allows an agent to cover state space completely. Proposed maximum\nentropy policy optimization has strong theoretical guarantees. Still, it makes some assumptions about\nthe existence of efﬁcient planning oracle, which makes this algorithm hardly applicable for most\nmodern reinforcement learning tasks.\n3\nMaximum Entropy Exploration in Model-based RL\n3.1\nExploration objective\nIn order to improve exploration and make it more targeted, we aim to maximize the entropy of\ndistribution of states visited by an agent: max H(pπ(sτ>t|st)). However, such distribution may have\nan inﬁnite horizon which is hard to estimate. Also, as we want to improve sample efﬁciency by\nmaximizing entropy of this distribution, we want our method to focus on exploring states in short\nhorizon as they may be reached in a reasonable amount of transitions. Therefore, we will maximize\nan entropy of a discounted state distribution:\nqk\nπ(sτ>t+k−1|st) = (1 −γ)pt+k\nπ\n(st+k−1|st) + γqk+1\nπ\n(sτ>t+k|st)\n(6)\nwhere pk\nπ(sτ>t|st) is a conditional distribution of states at step t + k induced by policy π. Note\nthat this deﬁnition is suitable only for inﬁnite episodes but can be rewritten for ﬁnite episodes by\nincluding a probability of an episode termination:\nqk\nπ(sτ>t+k−1|st) = (1 −pπ(dt+k−1|st)) · [(1 −γ)pt+k\nπ\n(st+k−1|st) + γqk+1\nπ\n(sτ>t+k−1|st)]\n+ pπ(dt+k|st) · pt+k\nπ\n(st+k−1|st)\nWhere pπ(dt+k|st) is a probability that episode will end after committing an action at step t + k −1\nconditioned to state st at step t. Below we will omit termination probabilities for simplicity, but all\nthe formulas may be easily extended to consider them. Therefore, the ﬁnal objective for an agent to\noptimize is\narg max\nπ (Vπ(st) + βH(qπ(sτ>t|st))), where qπ(sτ>t|st) := q1\nπ(sτ>t|st)\n(7)\n3.2\nEstimating discounted state distribution\nAs distribution q1\nπ(sτ>t|st) is unknown, and we can not easily compute it from the world model, we\napproximate it using synthetic data generated by the world model. In model-based algorithms, the\nworld model often generates not single synthetic transitions but transition sequences. Therefore, we\nmay extend equation 6 to a sequence form:\nqk\nπ(sτ>t+k−1|st) = (1 −γ)\nn−1\nX\ni=0\nγi · pt+k+i−1\nπ\n(st+k|st) + γnqk+n\nπ\n(sτ>t+k+n|st)\n(8)\nwhich then can be rewritten in a recurrent form using properties of Markov Decision Process:\nqπ(sτ>t|st) = (1 −γ)\nn−1\nX\ni=0\nγi · pt+i+1\nπ\n(st+i|st)\n+ γnEsτ>t+n−1∼pt+n−1\nπ\n(.|st)qπ(sτ>t+n−1|st+n−1)\n(9)\nEquation 9 allows us to use any distribution estimation model that can sample from distribution\nestimation. To do so, we propose to use importance sampling in the assumption that distribution\nestimation P(.|s) is close enough to qπ(.|s):\nEs∼qπ(.|st)L(s, st) = E{si∼pi\nπ(.|si−1)}t+n\ni=t+1Es∼qπ(.|st+n)[γnL(s, st) + (1 −γ) ·\nn−1\nX\ni=0\nγiL(si+t, st)]\n≈E{si∼piπ(.|si−1)}t+n\ni=t+1Es∼P (.|st+n)[γnL(s, st) + (1 −γ) ·\nn−1\nX\ni=0\nγiL(si+t, st)]\n(10)\n5\nHere L(s, st) is a loss function that is optimized by model P. In practice, P may be any model\nthat allows to sample from conditional distribution and estimate its density. Also, as we now\nhave estimation P(s|st) of distribution qπ(s|st), we will replace H(qπ(s|st)) in equation 7 with\nH(P(s|st)) with the assumption that H(qπ(s|st)) ≈H(P(s|st)) if distributions are close enough.\nHowever, even for distribution estimation P we may not be able to compute the entropy analytically\nand, moreover, not able to compute its gradient over actor network parameters. The solution is\nto estimate it by using importance sampling as in equation 10 followed by applying Monte Carlo\nsampling. As in SAC [16] or DDPG [14], such estimation is theoretically consistent for any actor\narchitecture that either deterministic or uses a reparametrization trick.\n4\nMaximum Entropy Dreamer\n4.1\nApplying Maximum Entropy Exploration\nAs the Dreamer model uses learned hidden state representations in order to deal with partially\nobservable MDP, we will maximize the entropy of the distribution of these hidden states. To estimate\nthe discounted distribution of hidden states, we use the Mixture Density Network [20]. It learns\nvia the maximization of log-likelihood and estimates any distribution with a mixture of normal\ndistributions.\nIn order to improve the stability of our method, we use an additional target MDN that updates every\nstep by soft update: θT = (1 −λ)θT + λθ. This network is then used to estimate the entropy while\nupdating an agent.\nWe also found that stochastic policy learned by the agent has a huge variance of total rewards for\nevery iteration and mean total reward changes too much over the training time. Therefore, we add a\ndeterministic agent that trains in parallel with a stochastic agent to make it more stable during training\nand evaluation time. While the goal of a stochastic agent is to explore the environment efﬁciently, the\ndeterministic agent is trained to maximize the total reward.\n4.2\nAdditional modiﬁcations\nFigure 2: Architecture of the world model after all modiﬁcations. Blue part related to the stochastic\npart of hidden state, orange part related to the deterministic part of hidden state, red block is a reward\nprediction model and green block is a episode termination prediction model.\nEpisode termination prediction model.\nEven though many Dreamer implementations already use\nsuch a model, we implicitly add a classiﬁcation model that predicts whether a synthetic episode is\nﬁnished or not. We also adjust the cumulative return formula accordingly to the presence of this\nmodel. Additionally, in the training process, we compute gradients only for the ﬁrst state of synthetic\nsequences. This is necessary to avoid situations when the agent tries to optimize cumulative reward\nfor states that can not be reached because of the end of the episode.\n6\nWorld model architecture rearrangement.\nIn the original Dreamer architecture, an agent gains\ninformation from observation only after committing its ﬁrst action. Therefore, the ﬁrst observations\nfrom an episode remain unobserved, leading to suboptimal performance when cumulative return\ndepends strongly on the beginning of the episode. We rearrange the order of blocks in Dreamer’s\nworld model architecture so that agent gains information about observation before committing the\nﬁrst action.\nFixed prior distribution usage in observation encoder.\nWhile training the world model, the orig-\ninal Dreamer algorithm uses trainable prior distribution p(st|ht) that updates along with distribution\np(st|ht, ot) produced by the encoder. The use of such a trainable prior will cause the stochastic\npart of the state vector to contain more information than intended. We ﬁx that by adding ﬁxed prior\ndistribution N(0, 1) and optimizing DKL(p(st|ht, ot)∥st ∼N(0, 1)) + DJ(p(st|ht, ot)∥p(st|ht))\ninstead of DKL(p(st|ht, ot)∥p(st|ht)). Here DJ(P∥Q) = DKL(P∥Q) + DKL(Q∥P) is a Jeffreys\nDivergence. Note that we keep trainable p(st|ht) and use it to sample st for an agent training.\nAlong with these important modiﬁcations, we also made some minor modiﬁcations:\n1. Critic now estimates Q-function instead of the Value function. This modiﬁcation do not\naffect cumulative return estimation for agent update as Vπ(s) = Ea∼π(.|s)Qπ(s, a) and\nagent can use reparametrization trick and Monte Carlo method to estimate this mathematical\nexpectation.\n2. For critic and reward models, we replace the MSE loss function with Smooth L1 loss\nfunction to improve training stability for higher values of reward and cumulative reward.\n3. Observation decoder now uses only stochastic part of the hidden state in order to prevent it\nto put too much information about observation into deterministic part of the state.\n4. While exploring, agents additionally have a probability to make a random action along with\nadding random noise to a greedy action.\n5\nExperiments\n5.1\nExperimental setup\nWe have conducted our experiments on the set of PyBullet environments. PyBullet is an open-source\nphysics engine which reimplements some of the popular MuJoCo environments. As the original\nDreamer algorithm learns from visual information, we use rendered images of the size of 64 × 64\npixels.\nAs a baseline, we use the original Dreamer algorithm with hyperparameters described by its authors.\nFor the Revised Dreamer described in subsection 4.2 we increase the size of the stochastic part of\nstate representation to 128 to compensate for the lack of information from the deterministic part of a\nstate from the decoder. We also change the KL divergence coefﬁcient to 0.1 and set the same value\nfor Jeffrey Divergence.\nMaximum Entropy Dreamer uses the same hyperparameters and coefﬁcients as a Revised Dreamer\nexcept that it is no longer uses additive random noise for exploration. For MDN, we set the hidden\nsize to 256, a number of heads to 8, the coefﬁcient of soft update of target MDN to 0.1, and the\ndiscount factor for discounted state distribution to 0.9. The MDN were trained with a learning\nrate 0.0002 with Adam optimizer [21]. An entropy coefﬁcient β in actor optimization objective\n(equation 7) is linearly decreasing from 0.2 to 0.0001 during the training process.\n5.2\nExperiment results\nWe show the results on PyBullet environments in Figure 3. We draw a 95% conﬁdence interval of the\nachieved score calculated with 10 evaluations of an agent and a line that shows a mean score.\nNotably, the original Dreamer algorithm is entirely unable to solve Walker2D and Hopper envi-\nronments. We believe that this is caused by the absence of a model that predicts the termination\nof an episode. In other words, it is too easy for an agent to meet termination conditions in these\nenvironments. Just by introducing modiﬁcations described in Section 4.2 to address that problem,\n7\nFigure 3: Comparison to baselines on PyBullet environments. Our MaxEnt Dreamer algorithm\ncombines entropy-based exploration with some additional modiﬁcations. To evaluate the contribution\nof the exploration, we also show the plot for Dreamer with these additional modiﬁcations only. To\nmeasure an impact of our exploration method on original Dreamer without modiﬁcations, we show\nthe plot for Dreamer with exploration only.\nwe produce an agent that is able to receive a non-zero score. Introducing entropy-based exploration\nimproves the score even further.\nIn the Ant environment, aforementioned modiﬁcations do not yield any improvement, since the\ntermination of an episode is less ambiguous. However, entropy-based exploration does signiﬁcantly\nimproves the overall performance both with and without additional modiﬁcations.\nIn the HalfCheetah environment, our additional modiﬁcations signiﬁcantly improve the result on their\nown and perform on par with the full MaxEnt Dreamer and the Dreamer with our exploration method.\nWe believe this is due to both of the reaching the optimal policy with maximal reward.\n6\nConclusion\nIn this paper we present our maximum-entropy exploration method designed for model-based\nreinforcement learning that can be applied to any model-based RL algorithm that trains an agent as it\nonly requires an ability to sample synthetic transitions using current policy and then compute the\ngradient for generated states over actor neural network parameters.\nWe also employed several techniques to improve the stability of training our algorithm. The ﬁnal\nversion, MaxEnt Dreamer, shows signiﬁcant improvement in 4 widely used environments. We\nalso performed an ablation study to see what improvements are due to this stabilization alone, and\ndemonstrate that in 3 out of 4 environments it is the exploration part that bring the majority of\nimprovement.\nAs a direction for future research we suggest further investigations into the cause of instability of an\nagent trained with model-based maximum entropy exploration. Another direction could be to replace\nthe Mixture Density Network that estimates discounted state distribution with more effective models.\n7\nAcknowledgements\nThis research was supported in part through computational resources of HPC facilities at HSE\nUniversity [22].\n8\nReferences\n[1] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-\nyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n[2] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large\nscale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[3] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur\nPetron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube\nwith a robot hand. arXiv preprint arXiv:1910.07113, 2019.\n[4] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\narXiv preprint arXiv:1511.05952, 2015.\n[5] Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised represen-\ntations for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.\n[6] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous\ncontrol: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645,\n2021.\n[7] Florin Gogianu, Tudor Berariu, Mihaela Rosca, Claudia Clopath, Lucian Busoniu, and Razvan\nPascanu. Spectral normalisation for deep reinforcement learning: an optimisation perspective.\narXiv preprint arXiv:2105.05246, 2021.\n[8] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum\nentropy exploration. In International Conference on Machine Learning, pages 2681–2691.\nPMLR, 2019.\n[9] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pages 2778–\n2787. PMLR, 2017.\n[10] Ruo Yu Tao, Vincent François-Lavet, and Joelle Pineau. Novelty search in representational\nspace for sample efﬁcient exploration. arXiv preprint arXiv:2009.13579, 2020.\n[11] Aleksandr Ermolov and Nicu Sebe. Latent world models for intrinsically motivated exploration.\narXiv preprint arXiv:2010.02302, 2020.\n[12] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:\nLearning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\n[13] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\ndiscrete world models. arXiv preprint arXiv:2010.02193, 2020.\n[14] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n[15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[16] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In International\nconference on machine learning, pages 1861–1870. PMLR, 2018.\n[17] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee,\nand James Davidson. Learning latent dynamics for planning from pixels. In International\nConference on Machine Learning, pages 2555–2565. PMLR, 2019.\n[18] Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by\nlatent imagination without reconstruction. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA), pages 4209–4215. IEEE, 2021.\n[19] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random\nnetwork distillation. arXiv preprint arXiv:1810.12894, 2018.\n[20] Christopher M Bishop. Mixture density networks. 1994.\n9\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[22] P. S. Kostenetskiy, R. A. Chulkevich, and V. I. Kozyrev. HPC resources of the higher school of\neconomics. Journal of Physics: Conference Series, 1740:012050, jan 2021.\n10\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-12-02",
  "updated": "2021-12-02"
}