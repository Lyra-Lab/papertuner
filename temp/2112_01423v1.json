{
  "id": "http://arxiv.org/abs/2112.01423v1",
  "title": "Training Efficiency and Robustness in Deep Learning",
  "authors": [
    "Fartash Faghri"
  ],
  "abstract": "Deep Learning has revolutionized machine learning and artificial\nintelligence, achieving superhuman performance in several standard benchmarks.\nIt is well-known that deep learning models are inefficient to train; they learn\nby processing millions of training data multiple times and require powerful\ncomputational resources to process large batches of data in parallel at the\nsame time rather than sequentially. Deep learning models also have unexpected\nfailure modes; they can be fooled into misbehaviour, producing unexpectedly\nincorrect predictions.\n  In this thesis, we study approaches to improve the training efficiency and\nrobustness of deep learning models. In the context of learning visual-semantic\nembeddings, we find that prioritizing learning on more informative training\ndata increases convergence speed and improves generalization performance on\ntest data. We formalize a simple trick called hard negative mining as a\nmodification to the learning objective function with no computational overhead.\nNext, we seek improvements to optimization speed in general-purpose\noptimization methods in deep learning. We show that a redundancy-aware\nmodification to the sampling of training data improves the training speed and\ndevelops an efficient method for detecting the diversity of training signal,\nnamely, gradient clustering. Finally, we study adversarial robustness in deep\nlearning and approaches to achieve maximal adversarial robustness without\ntraining with additional data. For linear models, we prove guaranteed maximal\nrobustness achieved only by appropriate choice of the optimizer,\nregularization, or architecture.",
  "text": "TRAINING EFFICIENCY AND ROBUSTNESS IN DEEP LEARNING\nby\nFartash Faghri\nA thesis submitted in conformity with the requirements\nfor the degree of Doctor of Philosophy\nDepartment of Computer Science\nUniversity of Toronto\n© Copyright 2022 by Fartash Faghri\narXiv:2112.01423v1  [cs.LG]  2 Dec 2021\nTraining Efﬁciency and Robustness in Deep Learning\nFartash Faghri\nDoctor of Philosophy\nDepartment of Computer Science\nUniversity of Toronto\n2022\nAbstract\nDeep Learning has revolutionized machine learning and artiﬁcial intelligence, achieving\nsuperhuman performance in several standard benchmarks. It is well-known that deep\nlearning models are inefﬁcient to train; they learn by processing millions of training data\nmultiple times and require powerful computational resources to process large batches of\ndata in parallel at the same time rather than sequentially. Deep learning models also have\nunexpected failure modes; they can be fooled into misbehaviour, producing unexpectedly\nincorrect predictions.\nIn this thesis, we study approaches to improve the training efﬁciency and robustness\nof deep learning models. In the context of learning visual-semantic embeddings, we ﬁnd\nthat prioritizing learning on more informative training data increases convergence speed\nand improves generalization performance on test data. We formalize a simple trick called\nhard negative mining as a modiﬁcation to the learning objective function with no compu-\ntational overhead. Next, we seek improvements to optimization speed in general-purpose\noptimization methods in deep learning. We show that a redundancy-aware modiﬁcation to\nthe sampling of training data improves the training speed and develops an efﬁcient method\nfor detecting the diversity of training signal, namely, gradient clustering. Finally, we study\nadversarial robustness in deep learning and approaches to achieve maximal adversarial\nrobustness without training with additional data. For linear models, we prove guaranteed\nmaximal robustness achieved only by appropriate choice of the optimizer, regularization, or\narchitecture.\nii\nAcknowledgements\nThroughout the years, David Fleet was the best supervisor I could ever hope to have. He\nprovided a calm environment without typical pressures, encouraged research wanderings,\nand valued my approaches. His responses to strange ideas were either constructive and kind\nfeedback or a surprisingly deep and thought-provoking question. As an absolute ethical role\nmodel, David always promptly responded with intelligent resolutions. His generous support\nwent beyond ﬁnancial to caring for my physical and mental wellness. It has been an honour\nto have David as my sage mentor.\nI have been blessed to have the support and company of my cheerful, considerate,\nkindhearted, encouraging, and simply amazing wife, Sara Sabour.\nI have been fortunate enough to receive lasting inﬂuences from numerous mentors. I\nhighly appreciate the guidance, encouragement, and constructive feedback of my supervisory\ncommittee, David Duvenaud, Roger Grosse, Graham Taylor, and Chris Maddison. I am also\ngrateful for the wisdom, unique perspective, and pivotal advice of Ian Goodfellow, Nicolas\nLe Roux, Fabian Pedregosa, Jimmy Ba, Daniel Roy, Sanja Fidler, Foteini Agraﬁoti, Alexey\nKurakin, Nicholas Carlini, Mohammad Norouzi, Mehrdad Farajtabar, Mohammad Amin\nSadeghi, Ehsan Fazl Ersi, Aideen NasiriShargh, Afshin Nikzad, Kian Mirjalali, and Vahid\nLiaghat.\nI appreciate the enjoyable, fun and fruitful collaborations with my coauthors Yanshuai\nCao, Ali Ramezani-Kebrya, Iman Tabrizian, Avery Ma, Justin Gilmer, Nicolas Papernot,\nJamie Kiros, and Sven Gowal.\nThanks to my friends, lab-mates and coworkers for making my graduate studies a\npleasurable journey. Among many others at the University of Toronto and the Vector Institute,\nI would like to thank Alireza Shekaramiz, Saeed Seddighin, Jake Snell, Nona Naderi, Kevin\nSwersky, Eleni Triantaﬁllou, Micha Livne, Ladislav Rampasek, Alimohammad Rabbani,\nBahareh Mostafazadeh, Sepideh Mahabadi, Kaveh Ghasemloo, Sajad Norouzi, Aryan Arbabi,\nRahmtin Rotabi, Farzaneh Derakhshan, Alireza Makhzani, Farzaneh Mahdisoltani, Taylor\nKillian, Ehsan Amiri, and Sobhan Foroughi.\niii\nI would like to thank the staff at the University of Toronto, the Department of Computer\nScience, and the Vector Institute for their support. A special thanks to Relu Patrascu for\nproviding technical support throughout the years.\nI am grateful to my family Laleh Kordavani, Hassan Faghri, Faraz Faghri, and Ali Faghri,\nfor their foundational role in my growth and development.\nI am grateful to various organizations for ﬁnancial support. These include the Department\nof Computer Science and the University of Toronto, and the Ontario Graduate Scholarship\n(OGS). Resources used in my research were also provided, in part, by the Province of\nOntario, the Government of Canada through CIFAR, and companies sponsoring the Vector\nInstitute.1 In addition, part of the research in this thesis was initiated during an internship\nwith the Brain Team in Google Research.\n1\nwww.vectorinstitute.ai/#partners\niv\nContents\n1\nIntroduction\n1\n1.1\nPublications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2\nBackground\n8\n2.1\nOptimization for Deep Learning . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.1.1\nStochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . .\n10\n2.1.2\nMomentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.1.3\nSecond Order Methods . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.1.4\nAdaptive Gradient Methods\n. . . . . . . . . . . . . . . . . . . . .\n13\n2.1.5\nInitialization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.1.6\nGeneralization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.1.7\nImplicit Bias of Optimization Methods\n. . . . . . . . . . . . . . .\n16\n2.2\nContrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3\nVSE++: Improving Visual-Semantic Embeddings with Hard Negative Mining 20\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3.2\nLearning Visual-Semantic Embeddings\n. . . . . . . . . . . . . . . . . . .\n23\n3.2.1\nVisual-Semantic Embedding . . . . . . . . . . . . . . . . . . . . .\n23\n3.2.2\nEmphasis on Hard Negatives . . . . . . . . . . . . . . . . . . . . .\n25\n3.2.3\nProbability of Sampling the Hardest Negative . . . . . . . . . . . .\n27\n3.3\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nv\n3.3.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.3.2\nDetails of Training . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.3.3\nResults on MS-COCO . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.3.4\nResults on Flickr30K . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.3.5\nImproving Order Embeddings . . . . . . . . . . . . . . . . . . . .\n32\n3.3.6\nBehavior of Loss Functions\n. . . . . . . . . . . . . . . . . . . . .\n33\n3.3.7\nExamples of Hard Negatives . . . . . . . . . . . . . . . . . . . . .\n33\n3.3.8\nDistribution of distances . . . . . . . . . . . . . . . . . . . . . . .\n34\n3.3.9\nEffect of Negative Set Size on MH Loss . . . . . . . . . . . . . . .\n34\n3.4\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4\nGradient Clustering and A Study of Gradient Variance in Deep Learning\n39\n4.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.2\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n4.3\nMini-batch Gradient with Stratiﬁed Sampling . . . . . . . . . . . . . . . .\n45\n4.3.1\nWeighted Gradient Clustering . . . . . . . . . . . . . . . . . . . .\n46\n4.3.2\nEfﬁcient Gradient Clustering (GC) . . . . . . . . . . . . . . . . . .\n47\n4.4\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n4.4.1\nMNIST: Low Variance, CIFAR-10: Noisy Estimates, ImageNet: No\nStructure\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n4.4.2\nRandom Features Models: How Does Overparametrization Affect\nthe Variance? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n4.4.3\nDuplicates: Back to the Motivation for Gradient Clustering . . . . .\n56\n4.5\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n5\nBridging the Gap Between Adversarial Robustness and Optimization Bias\n59\n5.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n5.2\nNo Trade-offs with Maximally Robust Classiﬁers . . . . . . . . . . . . . .\n62\n5.2.1\nMaximally Robust Classiﬁer . . . . . . . . . . . . . . . . . . . . .\n64\nvi\n5.2.2\nLinear Models: Maximally Robust is the Minimum Norm Classiﬁer\n65\n5.3\nImplicit Robustness of Optimizers . . . . . . . . . . . . . . . . . . . . . .\n66\n5.3.1\nSteepest Descent on Fully-Connected Networks . . . . . . . . . . .\n67\n5.3.2\nGradient Descent on Linear Convolutional Networks . . . . . . . .\n68\n5.3.3\nFourier Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n5.4\nExplicit Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n5.5\nExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n5.5.1\nMaximally Robust to ℓ∞, ℓ2, ℓ1, and Fourier-ℓ∞Bounded Attacks .\n74\n5.5.2\nPlotting the Trade-offs . . . . . . . . . . . . . . . . . . . . . . . .\n75\n5.5.3\nCIFAR-10 Fourier-ℓ∞Robustness . . . . . . . . . . . . . . . . . .\n76\n5.6\nRelated work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n5.6.1\nInvestigating the Gap in the Convergence of Linear Convolutional\nNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n5.7\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n6\nConclusion and Future Work\n84\nA Appendices to the Chapter on Gradient Clustering\n87\nA.1\nAdditional Details of Gradient Clustering\n. . . . . . . . . . . . . . . . . .\n87\nA.1.1\nProof of Proposition 4.3.1\n. . . . . . . . . . . . . . . . . . . . . .\n87\nA.2\nAdditional Details of Efﬁcient GC . . . . . . . . . . . . . . . . . . . . . .\n88\nA.2.1\nConvolutional Layers . . . . . . . . . . . . . . . . . . . . . . . . .\n88\nA.2.2\nComplexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . .\n89\nA.3\nAdditional Details for Experiments . . . . . . . . . . . . . . . . . . . . . .\n90\nA.3.1\nExperimental Details for Image Classiﬁcation Models\n. . . . . . .\n90\nA.3.2\nExperimental Details for Random Features Models . . . . . . . . .\n92\nA.4\nNormalized Variance and Convergence Analysis . . . . . . . . . . . . . . .\n92\nvii\nB\nAppendices to the Chapter on Bridging the Gap Chapter\n94\nB.1\nGeneralization of the Maximally Robust Classiﬁer . . . . . . . . . . . . . .\n94\nB.2\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\nB.2.1\nProof of Lemma 5.2.1\n. . . . . . . . . . . . . . . . . . . . . . . .\n95\nB.2.2\nProof of Maximally Robust to Perturbations Bounded in Fourier\nDomain (Corollary 2) . . . . . . . . . . . . . . . . . . . . . . . . .\n96\nB.3\nLinear Operations in Discrete Fourier Domain . . . . . . . . . . . . . . . .\n97\nB.3.1\nℓ∞Complex Projection\n. . . . . . . . . . . . . . . . . . . . . . .\n97\nB.3.2\nSteepest Ascent Direction w.r.t. Fourier-ℓ∞. . . . . . . . . . . . .\n98\nB.4\nNon-linear Maximally Robust Classiﬁers\n. . . . . . . . . . . . . . . . . .\n99\nB.5\nExtended Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nB.5.1\nDetails of Linear Classiﬁcation Experiments\n. . . . . . . . . . . . 101\nB.5.2\nDetails of CIFAR-10 experiments . . . . . . . . . . . . . . . . . . 102\nB.5.3\nMargin Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\nB.5.4\nVisualization of Fourier Adversarial Attacks . . . . . . . . . . . . . 104\nB.6\nVisualization of Norm-balls . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nBibliography\n113\nviii\nChapter 1\nIntroduction\nDeep Learning has revolutionized machine learning and artiﬁcial intelligence, achieving\nsuperhuman performance in several standard benchmarks, including image classiﬁcation [78],\nthe game of Go [185], and natural language translation [157]. Deep neural networks have\nbecome major components of emerging technologies such as autonomous driving [71],\nrobotics [193], and drug discovery [28].\nIt is also well-known that deep learning models are inefﬁcient to train [192]; they learn\nby processing millions of training data multiple times [78, 185, 157]. They require powerful\ncomputational resources to process large batches of data in parallel at the same time rather\nthan sequentially [159, 163, 21]. Vast amounts of image and text data gathered in the past few\ndecades, and technological advancements in processing hardware, allowed breakthroughs in\ndeep learning applications [40, 16, 118]. However, this recipe is challenging to scale. For\nexample, signiﬁcant advances in language modelling require vast amounts of data and have\nyet to achieve superhuman performance on some datasets [21].\nDeep learning models also have unexpected failure modes; they can be fooled into\nmisbehaviour, producing incorrect predictions [196, 67]. Adversarial perturbations are small\nmodiﬁcations to the input that change the output of a model. Adversarial perturbations\nagainst deep learning image models can be designed to be sufﬁciently subtle that they become\nimperceptible to humans. Adversarial samples are particularly troubling in applications with\n1\nCHAPTER 1. INTRODUCTION\n2\nsigniﬁcant security and privacy requirements [107, 101]. Moreover, adversarial samples are\nnot only security ﬂaws but an intriguing example of our limitation in understanding deep\nlearning models [59]. The lack of explanation for adversarial samples limits our trust and\nconﬁdence in the decisions and predictions of machine learning models. Understanding\nand mitigating the impact of adversarial samples provides ways to improve robustness and\nefﬁciency of models [174].\nThis thesis comprises three parts, each of which explores aspects of efﬁciency and\nrobustness in deep learning from different perspectives.\nThe ﬁrst problem concerns training efﬁciency in computer vision. We hypothesize that\ntraining data are not equally important; the available training signal from some inputs is\nstronger than others. Moreover, we hypothesize that prioritizing learning on more informative\ntraining data increases convergence speed and improves generalization performance on test\ndata. In Chapter 3 (originally published in [50]) we illustrate how a simple trick referred to as\nhard negative mining speeds up training and improves test performance. Although previously\nused in ad-hoc ways, we formalize hard negative mining as a modiﬁcation to the learning\nobjective function. We demonstrate these improvements in learning joint embeddings that is\na standard benchmark for representation learning.\nJoint embeddings enable a wide range of tasks in image, video and language understand-\ning. Examples include shape-image embeddings [115] for shape inference, bilingual word\nembeddings [238], human pose-image embeddings for 3D pose inference [113], ﬁne-grained\nrecognition [166], zero-shot learning [56], and modality conversion via synthesis [165,\n166]. Such embeddings entail mappings from two (or more) domains into a common vector\nspace in which semantically associated inputs (e.g., text and images) are mapped to similar\nlocations. The embedding space thus represents the underlying domain structure, where\nlocation and often direction are semantically meaningful.\nVisual-semantic embeddings have been central to image-caption retrieval and genera-\ntion [103, 98], visual question-answering [128], and more recently in large-scale multi-modal\nrepresentation learning such as CLIP [159]. One approach to visual question-answering,\nCHAPTER 1. INTRODUCTION\n3\nfor example, is to ﬁrst describe an image by a set of captions, and then to ﬁnd the nearest\ncaption in response to a question [1, 237]. For image synthesis from text, one could map\nfrom text to the joint embedding space, and then back to image space [165, 166].\nIt is common to use a contrastive loss function to learn representations with a meaningful\ndistance metric that resembles a conceptual contrast in the input space [76]. For example, we\nwould use a contrastive loss to learn representations of images such that the representation\nof a dog is closer to other dogs relative to any cat. In particular, triplet ranking losses consist\nof terms that contrast a positive similar pair with a dissimilar negative sample. A non-zero\nloss is incurred if the representations of the similar pair are farther from each other than they\nare individually from the negative sample. A triplet loss requires a ground-truth collection\nof matching pairs but the negative sample often requires no annotation as it can be any\nnon-matching input from the database. Triplet losses are deﬁned given a distance metric (or\nmore generally a similarity score) and differ in how the loss scales with the magnitude of the\ndistance violation.\nOur main contribution is to incorporate hard negatives in the loss function with no\ncomputational overhead. A hard negative for any positive pair is deﬁned as the training\nsample incurring the maximum triplet loss. Finding the hardest negative is computationally\nexpensive. Instead, we use semi-hard negatives that are the hardest contrastive triplets in\neach mini-batch. This simple change has no additional computational cost while signiﬁcantly\nimproving the training performance.\nVarious works have extended our work and applied semi-hard negative mining in visual-\nsemantic embeddings and related applications. Examples are visual-semantic embedding\nwith generalized pooling operator [29], Consensus-aware visual-semantic embedding [213],\nand top ranking methods in video retrieval competition tasks [8] for ad-hoc video search\nand video-to-text description generation. These examples illustrate the effectiveness of our\nsimple modiﬁcation to the loss function in computer vision applications.\nThe second problem addressed in this thesis is training efﬁciency as a general challenge\nin training deep neural networks. Instead of focusing on a task-speciﬁc loss function, we\nCHAPTER 1. INTRODUCTION\n4\nseek improvements to optimization speed in gradient-based optimization methods for deep\nlearning. We revisit our hypothesis from the ﬁrst problem that training data are not all equally\nimportant. We hypothesize that training deep learning models on diverse and heterogeneous\ndata distributions should be slower than homogeneous data distributions with less diversity.\nIn Chapter 4 and the publication [48], we develop a method for detecting the diversity of\ntraining signal and propose it as a method for improving training speed. Our method can\ngenerally be used with any gradient-based optimization method. In contrast to Chapter 3,\nvery few general-purpose ideas exist that consistently increase training efﬁciency.\nWe motivate our hypothesis with a simple example for which there exist duplicate data\npoints. To decrease the processing time per iteration, it is common to uniformly sample mini-\nbatches of data during training. As illustrated in the following example, uniform sampling\nis not always ideal. Suppose there are duplicate data points in a training set. We can save\ncomputation time by removing all but one of the duplicates. To get the same gradient update,\nin expectation, it is sufﬁcient to rescale the gradient of the remaining samples in proportion to\nthe number of duplicates. In this example, standard optimization methods for deep learning\nwill be inefﬁcient because with uniform sampling there is a chance of sampling mini-batches\ncontaining duplicates of one data type.\nIn general, redundancy can be extended from exact duplicates to points that are very\nsimilar to each other according to a similarity metric. In Chapter 4, we focus on the similarity\nin the gradient space and establish the connection between gradient redundancy and training\nspeed through a connection to gradient variance reduction. Prior works have theoretically\nestablished that smaller gradient variance results in faster training. Through an illustrative\nexample, we show that a redundancy-aware modiﬁcation to the sampling of training data\nreduces the gradient variance. We prove that the gradient variance is minimized if the\nelements are sampled from a weighted clustering in gradient space. Accordingly, we propose\na gradient clustering method as a computationally efﬁcient method for clustering in the\ngradient space.\nAlthough outside the scope of this thesis, we also note that we have used our observations\nCHAPTER 1. INTRODUCTION\n5\nabout the gradient distribution during training in designing gradient quantization methods to\nspeed up data-parallel optimization for deep learning [49, 164].\nThe third and last problem studied in this thesis concerns adversarial robustness in deep\nlearning. We hypothesize that deep learning models are not inherently weak; their robustness\ndepends on the implicit and explicit training mechanisms. In Chapter 5, we show that\nthese mechanisms imply maximal robustness to the types of perturbations not commonly\nconsidered in adversarial attacks. This observation allows us to ﬁnd alternative approaches\nto adversarial robustness that introduce no additional training overhead compared to stan-\ndard robustness methods. Hence, increasing training efﬁciency in the task of adversarial\nrobustness. Next, we introduce adversarial robustness and standard robustness methods in\nmore detail.\nDeep neural networks achieve high accuracy on standard test sets. Nevertheless, [196]\nshowed that any natural input correctly classiﬁed by a neural network can be modiﬁed\nwith adversarial perturbations that fool the network into misclassiﬁcation. They observed\nthat adversarial perturbations exist even when they are constrained to be small enough that\nthey do not signiﬁcantly affect human perception. Adversarially perturbed inputs are also\ncommonly referred to as Adversarial samples or adversarial examples.\nAdversarial training [67] was proposed to improve the robustness of models through\ntraining on adversarial samples and further generalized as a saddle-point optimization\nproblem [126]. In practice, adversarial training refers to methods that solve the saddle-point\nproblem approximately by minimizing the loss on adversarial samples.\nThe state-of-the-art approach to increase adversarial robustness is adversarial training,\ni.e., empirical risk minimization on adversarial samples. Adversarial training is another\nexample of inefﬁcient training; i.e., in addition to training on large datasets, for robustness,\nwe need to create and train on artiﬁcial adversarial samples to improve models. Adversarial\ntraining also exhibits a trade-off between standard generalization and adversarial robustness.\nThat is, it achieves improved robust accuracy, on adversarially perturbed data, at the expense\nof standard accuracy, the probability of correct predictions on natural data [204].\nCHAPTER 1. INTRODUCTION\n6\nIn Chapter 5, we discuss an alternative formulation to adversarial robustness, namely,\nmaximally robust classiﬁcation, with no robustness trade-off. By connecting this problem\nto the literature on the implicit bias of optimization methods[72], we also ﬁnd maximally\nrobust classiﬁers at no additional computational cost. This link allows for the bidirectional\ntransfer of results between the two literatures on adversarial robustness and optimization\nbias.\nFinally, we conclude in Chapter 6 with a summary of the main results, and a discussion\nof future directions.\n1.1\nPublications\nThe contents of this thesis have largely been taken from the following publications and\ntechnical reports:\n• Faghri, Fartash and Fleet, David J. and Kiros, Jamie R. and Fidler, Sanja, “VSE++:\nImproving Visual-Semantic Embeddings with Hard Negatives\", British Machine\nVision Conference (BMVC), 2018. (Chapter 3)\n• Faghri, Fartash and Duvenaud, David and Fleet, David J. and Ba, Jimmy, “A Study\nof Gradient Variance in Deep Learning\", arXiv, 2020. (Chapter 4)\n• Faghri, Fartash and Duvenaud, David and Fleet, David J. and Ba, Jimmy, “Gluster:\nVariance Reduced Mini-Batch SGD with Gradient Clustering\", Conference on Neural\nInformation Processing Systems (NeurIPS), Workshop on Beyond First Order Methods\nin ML, 2019. (Chapter 4)\n• Faghri, Fartash and Gowal, Sven, Vasconcelos, Cristina and Fleet, David J. and\nPedregosa, Fabian and Le Roux, Nicolas, “Bridging the Gap Between Adversarial\nRobustness and Optimization Bias\", Workshop on Security and Safety in Machine\nLearning Systems, International Conference on Learning Representations (ICLR),\n2021. (Chapter 5)\nCHAPTER 1. INTRODUCTION\n7\nDuring my PhD studies, beyond the research in this thesis, I have authored and con-\ntributed to the following papers. We do not discuss the details of the following papers in this\nthesis.\n• Faghri, Fartash∗1, Tabrizian, Iman∗and Markov, Ilia and Alistarh, Dan and Roy,\nDan M. and Ramezani-Kebrya, Ali, “Adaptive Gradient Quantization for Data-Parallel\nSGD\", Conference on Neural Information Processing Systems (NeurIPS), 2020.\n• Ramezani-Kebrya, Ali and Faghri, Fartash and Markov, Ilya and Aksenov, Vitalii and\nAlistarh, Dan and Roy, Dan M. et al., “NUQSGD: Provably Communication-efﬁcient\nData-parallel SGD via Nonuniform Quantization\", Journal of Machine Learning\nResearch 22.114 (2021): 1-43..\n• Ma, Avery and Faghri, Fartash and Papernot, Nicolas and Farahmand, Amir-massoud,\n“SOAR: Second-Order Adversarial Regularization\", ArXiv, 2020.\n• Zhang, Qingru and Wu, Yuhuai and Faghri, Fartash and Zhang, Tianzong and\nBa, Jimmy, “A Non-asymptotic comparison of SVRG and SGD: tradeoffs between\ncompute and speed\", ArXiv, 2020.\n• Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and\nRaghu, Maithra and Wattenberg, Martin and Goodfellow, Ian, “Adversarial Spheres\",\nICLR Workshop, 2018.\n• Sabour, Sara and Cao, Yanshuai and Faghri, Fartash and Fleet David J. “Adversar-\nial Manipulation of Deep Representations\", International Conference on Learning\nRepresentations (ICLR), 2016.\n1\n∗joint ﬁrst co-authors\nChapter 2\nBackground\nIn this section, we will cover the general background for the thesis and review common\nstandard practices for optimization in deep learning. Further related work that is speciﬁc to\nindividual chapters is developed in subsequent chapters. We refer the reader to [65] for a\ngeneral introduction to deep learning.\n2.1\nOptimization for Deep Learning\nMinimizing an objective is at the core of deep learning. In this section, we focus on methods\nand ideas for solving or simplifying optimization problems in deep learning.\nMany machine learning tasks entail the minimization of the risk, E(x,y)∼D[ℓ(x, y; θ)],\nwhere the input x, y are random variables distributed according to the data distribution D, and\nℓis the per-example loss function parametrized by θ, the parameters of the neural network.\nIn supervised learning, y denotes the ground-truth label. Empirical risk approximates\nthe population risk by the risk of i.i.d. samples {(xi, yi)}N\ni=1, the training set, as L(θ) =\nPN\ni=1ℓ(xi, yi; θ)/N. For differentiable loss functions, the gradient of xi is deﬁned as\n∂\n∂θℓ(xi, yi; θ), i.e., the gradient of the loss with respect to the parameters evaluated at a point\nxi [207].\nOptimization algorithms used for deep learning are predominantly iterative [17]. An\n8\nCHAPTER 2. BACKGROUND\n9\niterative optimizer starts with an initial estimate of the parameters and improves the estimate\niteratively [150]. Each iteration is often broken into two parts, i.e., choosing a direction and\nchoosing the step size to move along this direction.\nOptimization methods are mainly evaluated based on their rate of convergence to a\nlocal or global minimum of the loss function. Such evaluation is either by theoretically\nproving convergence rates on speciﬁc families of functions (e.g., convex functions) [17] or\nempirically, by training a model on benchmark datasets (e.g., a convolutional network on\nthe MNIST dataset [111]). The rate of convergence is either measured by the number of\niterations or the total wall-clock time in non-distributed settings. [33] suggests guidelines for\nempirical evaluation of optimization methods in deep learning.\nGeneralization, the ability to perform well on unseen data, is one of the key elements\nof machine learning [15]. Empirically, generalization is measured by evaluating a trained\nmodel on a held out test set. It is known that the choice of optimizer affects generalization\nperformance [72, 209, 4]).\nThe scope of this section is restricted to optimization methods popular in deep learning.\nWe focus on continuous parameters and assume that the gradient of the loss is deﬁned and\nexists almost everywhere. Optimizing neural networks is often an unconstrained problem but\nregularization methods are commonly used. The functions minimized in deep learning are\noften non-convex with multiple local minima. Nevertheless, we focus on local optimization\nmethods as they are more popular. Bayesian learning [147] and evolution strategies [172]\nare examples of non-local methods not covered in this chapter.\nWithin the optimization framework, Chapter 3 can be described as a modiﬁcation to an\noptimization problem to better match the task’s objective, Chapter 4 is an improvement to\nsolving common optimization problems in deep learning, and Chapter 5 characterizes the\nimplicit bias of optimization on adversarial metrics.\nCHAPTER 2. BACKGROUND\n10\n2.1.1\nStochastic Gradient Descent\nGradient Descent (GD) or Steepest Descent [150], is an iterative optimization method that\nassumes access to the ﬁrst-order derivative of the objective or loss function, e.g., ℓ(x, y; θ)\ndeﬁned above. We use GD to optimize a neural network by evaluating the gradient over the\nfull training set. The update rule for GD is\nθt = θt−1 −αt\nX\ni∈S\n∂\n∂θℓ(xi, yi; θ)\n\f\f\f\f\nθ=θt−1 ,\n(2.1)\nwhere ℓis the loss function parameterized at the t-th iteration of optimization by θt on the\ntraining data xi. αt is the optimization step size, or learning rate, at iteration t.\nGD requires the computation of gradients on the entire training set in each step while the\nalternative, Stochastic Gradient Descent (SGD), uses the gradient of a uniformly sampled\ndata point from the training set. The gradient from a single sample is an unbiased estimate\nof the average gradient on the training set such that they are equivalent in expectation.\nNevertheless, there exists a trade-off between the performance of GD versus SGD (discussed\nin Chapter 4). In mini-batch Stochastic Gradient Descent (mini-batch SGD) we can control\nthe trade-off between GD and SGD by instead sampling B training samples, which comprise\na mini-batch in each step. The mini-batch size is a hyper-parameter that provides ﬂexibility\nin trading per-step computation time for potentially fewer total steps. The update rule for\nmini-batch SGD is\nθt = θt−1 −αt\nX\ni∈Bt\n∂\n∂θℓ(xi, yi; θ)\n\f\f\f\f\nθ=θt−1 ,\n(2.2)\nwhere Bt is the mini-batch of training samples at iteration t. In GD the mini-batch is the\nentire training set while in SGD it is a single sample.\nGD and SGD are guaranteed to converge under speciﬁc assumptions [17, 167]. Theoret-\nically the convergence rates for GD/SGD are known under certain conditions [141]. The\neffect of the mini-batch size in deep learning has been studied extensively [181, 229].\nCHAPTER 2. BACKGROUND\n11\nTo use GD/SGD to train deep neural networks, we usually need to compute the gradient\nof the loss function with respect to the network parameters. Back-Propagation [170] is\none way of numerically computing these gradients. In feed-forward networks, layers are\ncomposed such that the inputs of a layer are the outputs of lower layers or the inputs\nof the model. Back-propagation (back-prop) is the application of the chain rule to this\ndecomposition.\nAlthough GD/SGD are popular optimizers in deep learning, they have known limitations.\nIn [195], two problems are identiﬁed, namely, the elongated ravine problem and the unlearn-\ning problem. An elongated ravine refers to a special shape of the loss function in the space of\nthe model parameters. There are two directions in an elongated ravine, the principal direction\nof the ravine and the perpendicular steep direction of the ravine’s wall. The optimization\nstep size has to be small enough to prevent divergence in the steepest direction while the loss\ndecreases along the principal direction. The convergence rate is determined by the ratio of\nthe two slopes.\nThe unlearning problem, more recently known as catastrophic forgetting, is when the\noptimizer ruins well-optimized parameters to learn new concepts, even though the model\nhas underemployed parameters. Catastrophic forgetting is a challenge for training models in\nthe setting of continual learning [120]. In continual learning, there exist multiple tasks and\nwe only get the training data of a task after we are done with learning with previous tasks. A\nmodel should perform well on all tasks seen so far. In this setting, current optimizers quickly\nlose the performance on previous tasks, when they are trained only with the data of a new\ntask.\nCHAPTER 2. BACKGROUND\n12\n2.1.2\nMomentum\nThe most commonly used variant of SGD in deep learning incorporates Polyak’s momen-\ntum [156] (also known as the heavy ball momentum)\nvt = βvt−1 +\nX\ni∈Bt\n∂\n∂θℓ(xi, yi; θ)\n\f\f\f\f\nθ=θt−1\n(2.3)\nθt = θt−1 −αtvt,\n(2.4)\nwhere β ∈[0, 1) is the momentum coefﬁcient and v is a velocity vector. Empirically, SGD\nwith momentum is often signiﬁcantly faster than SGD [181]. Polyak’s momentum has the\nsame convergence rate as GD with an improved constant. [149] proposed an acceleration\nmethod with an improved convergence rate on convex and continuously differentiable\nfunctions with a Lipschitz continuous gradient [17]. This method was later revitalized as\nNesterov’s momentum by [194] in a similar form to Polyak’s momentum, where the gradient\nestimate for the current step is computed not at the current parameter value, but at the\nanticipated next value. In practice as well as certain convex settings, momentum increases\nﬂexibility in the choice of the step size and mini-batch size [62, 229].\nPolyak’s momentum can also be interpreted as a gradient noise reduction method that\nconnects it to the ideas discussed in Chapter 4.\n2.1.3\nSecond Order Methods\nSecond-order methods address the elongated ravine problem by incorporating the curvature\ninformation into the optimization. The Hessian, H(θ) =\n∂2\n∂θ2 ℓ(xi, yi; θ), represents the\ninformation about the curvature. The generalization of Newton’s method is to replace\ng in the GD update step (Eq. (2.1)) with H−1g where g =\n∂\n∂θℓ(xi, yi; θ). Given the\nHessian, Newton’s method normalizes the gradients along the direction of the walls to\nprevent overshooting and allows consistent improvements in all directions.\nComputing the inverse of the Hessian is computationally demanding when the parameter\nCHAPTER 2. BACKGROUND\n13\nspace is high dimensional. As such, approximations have been suggested. A diagonal\napproximation is suggested in [11]. Hessian-Free optimization [130] is a second-order\nmethod that neither computes the Hessian, nor any approximation to it. Instead, Conjugate\nGradient iterations are used to choose the next optimization direction.\nNatural gradient descent, e.g., K-FAC [132], methods can also be viewed as second-order\nmethods [3]. Natural gradient [3] is deﬁned as F −1g, where\nF(θ) = Ex∼p(x)Ey∼p(y|x,θ)[∂log p(y|x, θ)\n∂θ\n∂log p(y|x, θ)\n∂θ\nT\n],\n(2.5)\nis the Fisher information matrix where x is sampled from the underlying data distribu-\ntion and y is sampled from the distribution of model predictions (See [106] for common\nerrors in the deﬁnition). The Fisher information matrix captures the uncertainty due to\nsampling of the training set and the sampling of mini-batches. Under certain conditions,\nthe Fisher information matrix is equivalent to a positive semi-deﬁnite approximation to the\nHessian [134].\nSecond order methods are still not widely used in deep learning, often because of\nimplementation challenges. Recent work have demonstrated wall-clock time improvements\nwith scalable implementations [5]. Besides, second order methods and more generally\npreconditioned optimization exhibit alternative generalization properties [4]. Our results\nin Chapter 5 can be extended to preconditioned optimization methods to imply alternative\nrobustness guarantees.\n2.1.4\nAdaptive Gradient Methods\nIn settings where gradients have known characteristics such as sparsity, specialized methods\nare preferred over SGD with momentum. ADAGRAD [45] proposes an update direction\nthat works well when gradients are sparse. In this method, each gradient dimension is\ndivided by\nqPT\ni=1 g2\ni,d, where gi,d is the dth gradient dimension at the ith training iteration.\nRMSprop [201] has a similar update rule that uses an exponentially-weighted average of\nCHAPTER 2. BACKGROUND\n14\nthe gradients over time instead of equally-weighted averaging gradients of all iterations.\nAdam [102] is a popular adaptive gradient method that combines Polyak’s momentum with\nRMSprop’s adaptive gradient. The update rule for Adam divides the velocity vector from\nPolyak’s momentum by the second moment of the gradients.\nADAGRAD, RMSprop, and Adam propose dividing the gradient vector by a function of\nits second moment per dimension. Under certain conditions, these methods can be interpreted\nas approximate second-order methods that use diagonal approximations to the Hessian or\nthe Fisher information matrix [17].\nNumerous optimization methods have been proposed for deep learning, yet Adam and\nSGD with Nesterov momentum still match or exceed the performance of the most recent\nones with proper hyper-parameter tuning [144, 178]. We use Adam in Chapter 3 as the\noptimization method. Our method in Chapter 4 can be interpreted as an adaptive gradient\nmethod with per-example reweighing. Adaptive gradient methods have unique generalization\nproperties with robustness implications covered in Chapter 5.\n2.1.5\nInitialization\nFor non-convex functions, poor initialization can cause slow convergence or convergence\nto a local minimum signiﬁcantly worse than the global minimum. Practical initializations\nin deep learning are often based on analysis of the distribution of activations and gradients.\nFor example, the initialization should be such that neither gradients nor activations vanish or\nexplode during back-propagation [82, 78].\nCommon initializations in deep learning are often based on sampling from zero-mean\nuniform or Gaussian distributions [61, 79]. The variance of the distribution is determined\nby the type of activation function and usually depends on the number of inputs and outputs\nof a unit. Orthogonal initialization has also been explored for deep linear and non-linear\nnetworks [175, 155].\nInitialization for deep non-linear models affects the entire training trajectory and results\nin varying properties of a converged model [140]. We expect similar results to Chapter 5\nCHAPTER 2. BACKGROUND\n15\nwould show the choice of initialization implicitly affects the robustness.\n2.1.6\nGeneralization\nIn Machine Learning, we seek accurate models that generalize to unseen data that is estimated\nempirically on held out test data. For performance on test data to be an accurate estimate of\ngeneralization, test data should not be reused. Model selection and hyper-parameter tuning\nrequire an estimate of the test performance that is often estimated on a validation set that is\nheld out from the training set. A generalization gap between training and test performance\ncan be due to model misspeciﬁcation, data limitations, or loss function shortcomings [15].\nThe optimizer can also affect the generalization gap if the choice of the model and loss\nfunction form an underspeciﬁed optimization problem [72].\nModiﬁcations to a model, data, or loss function that improve the generalization gap can\nbe problem/domain speciﬁc. For example, we often seek to learn computer vision models\ninvariant or equivariant to afﬁne image transformations. Using convolutional layers instead\nof fully-connected layers, we achieve translation invariance/equivariance at no additional\ncomputational cost but with reduced model ﬂexibility. A more ﬂexible but computationally\nexpensive approach is data augmentation that trains on transformations of the training data.\nAlternatively, the loss function can be changed to penalize disagreements between the model\noutputs on transformations of a single input, providing both ﬂexibility and potentially lower\ncomputational cost. In Chapter 3, we propose a similar loss function alternative that results\nin both faster training and better generalization.\nThe optimization problems in deep learning are often underspeciﬁed, i.e., the model is\ncomplex enough to ﬁt the training data accurately in more than one way. As such, there are\ngeneric modiﬁcations to the model, loss function, and optimization that restrict the model\ncomplexity. Dropout [191] applied to a layer deactivates units randomly and forces the\nmodel to learn a compact representation but with some redundancy. Regularization penalizes\nthe model by adding additional penalty terms to the loss function. Norm penalties such as ℓ0,\nℓ1, and ℓ2 encourage sparsity or parameter shrinkage and can be interpreted as assuming a\nCHAPTER 2. BACKGROUND\n16\nprior distribution over parameters [142]. Early stopping addresses overﬁtting by selecting the\nbest performing model during training according to the validation performance. A model is\noverﬁtting to the training data if the test performance worsens while the training performance\nimproves.\nA trade-off exists between model complexity and generalization that can be characterized\nby a bias-variance decomposition of the risk in some problems [15]. The relation between\ncomplexity and generalization can be a double-descent curve where the generalization perfor-\nmance of a family of models is worst at a critical model size while improves monotonically\nfor more or less complex models [77].\nIn Chapter 5, we discuss how common modiﬁcations for improving generalization\nsuch as model architecture, regularization, and optimization method indirectly affect model\nrobustness.\n2.1.7\nImplicit Bias of Optimization Methods\nMinimizing the empirical risk for an overparametrized model with more parameters than the\ntraining data has multiple solutions. [228] observed that overparametrized deep models can\neven ﬁt to randomly labeled training data, yet given the correct labels they consistently gen-\neralize to test data. This behavior has been explained using the implicit bias of optimization\nmethods towards particular solutions. [72] proved that minimizing the empirical risk using\nsteepest descent and mirror descent have an implicit bias towards minimum norm solutions\nin overparametrized linear classiﬁcation. Characterizing the implicit bias in linear regression\nproved to be more challenging and dependent on the initialization. [92] proved that training\na deep linear classiﬁer using gradient descent not only implicitly converges to the minimum\nnorm classiﬁer in the space of the product of parameters, each layer is also biased towards\nrank-1 matrices aligned with adjacent layers. [73] proved the implicit bias of gradient\ndescent in training linear convolutional classiﬁers is towards minimum norm solutions in the\nFourier domain that depends on the number of layers. [91] has established the directional\nalignment in the training of deep linear networks using gradient ﬂow (gradient descent with\nCHAPTER 2. BACKGROUND\n17\ninﬁnitesimal step size) as well as the implicit bias of training deep 2-homogeneous networks.\nIn the case of gradient ﬂow the implicit bias of training multi-layer linear models is towards\nrank-1 layers that satisfy directional alignment with adjacent layers [91, Proposition 4.4].\nRecently, [226] has proposed a uniﬁed framework for implicit bias of neural networks using\ntensor formulation that includes fully-connected, diagonal, and convolutional networks and\nweakened the convergence assumptions.\nThe recent theory of generalization in deep learning, in particular the double descent\nphenomenon, studies the generalization properties of minimum norm solutions for ﬁnite and\nnoisy training sets [77]. Characterization of the double descent phenomenon relies on the\nimplicit bias of optimization methods while using additional assumptions about the data\ndistribution. In contrast, our results in Chapter 5 only rely on the implicit bias of optimization\nand hence are independent of the data distribution.\n2.2\nContrastive Learning\nContrastive losses [76] have been used widely in computer vision and machine learning.\nTriplet losses are widely used in retrieval problems where the task is to learn a mapping such\nthat similar inputs are mapped close to each other and farther from dissimilar inputs [216].\nExamples are image retrieval [57, 26], learning to rank problems [112], and max-margin\nstructured prediction [25, 108]. Let f(x; θ) denote a mapping of input x ∈X parametrized\nby θ. A triplet loss function ℓ(q, k+, k−) takes a query input, q ∈X, together with a similar\nand dissimilar key inputs, k+, k−∈X. The loss function encourages the mappings of the\nquery and the similar key to be close to each other while discouraging proximity to the\ndissimilar key. A hinge triplet loss is commonly used in image retrieval where the similarity\nis measured by cosine similarity and the model is penalized if the proximity between the\nnegative pair is not more than the positive pair by a preset margin [103, 98, 235, 189]. The\npairwise hinge loss is an alternative in which elements of positive pairs are encouraged to lie\nwithin a hypersphere of radius ρ1, while negative pairs should be no closer than ρ2 > ρ1. A\nCHAPTER 2. BACKGROUND\n18\npairwise loss restricts the mapping more than a triplet loss and requires two hyper-parameters\nrather than one. In Chapter 3, we propose a modiﬁcation to the hinge triplet loss function.\nFocal loss is a similar recent work that modiﬁes the cross-entropy loss to handle imbalance\nin classes and training data when there are many easy samples but few hard samples such as\nwhen there are few foreground samples but numerous background samples [117].\nRecently, contrastive losses have been successfully employed for unsupervised represen-\ntation learning in methods such as MoCo [80, 31] and SimCLR [30]. In contrast to ranking\nand retrieval applications, new applications of contrastive losses are in unsupervised repre-\nsentation learning where the positive pair is usually the input itself with some perturbation.\nAs an alternative to hinge losses and others [80], the InfoNCE [152] loss and its variants are\nbased on the cross-entropy loss,\nℓ(q, k+, K−) = −log\nexp (q · k+/τ)\nexp (q · k+/τ) + P\nk−∈K−exp (q · k−/τ)\n(2.6)\nwhere q is a query input, k+ a similar key and K−a dissimilar set of keys to the query. τ is\nthe temperature hyper-parameter that controls the expected separation between similar and\ndissimilar samples.\nAn interpretation of ℓ(q, k+, K−) is as the cross-entropy loss where the set of positive\nand negative keys are the categories and exp(q · k/τ) is the unnormalized probability of the\nevent q = k. The cross-entropy loss is commonly used with a ﬁxed set of categories while\nwith in Eq. (2.6) the set of keys is speciﬁc to each query.\nThe equivalence to the standard cross-entropy loss is exact if the normalizing term\nconsists of all negative keys. Suppose we use cross-entropy but use a sampled subset of all\ncategories in the denominator that decreases the computation cost. The drawback is some\nqueries will inﬂuence the training more than others because of modiﬁed probabilities. A ﬁx\nis to use importance sampling according to the unnormalized probabilities of the negative\nkeys at the cost of losing the advantage of subsampling. In the literature on triplet losses\nsuch ideas are often referred to as hard negative mining where instead of using any negative,\nCHAPTER 2. BACKGROUND\n19\nwe mine hard negatives. Our proposed method in Chapter 3 is an example of hard negative\nmining in contrastive losses.\nIn recent works on contrastive losses, the query is uniformly selected from the training set\nand k+ is generated by perturbing the query using common input augmentations. For images,\ncommon perturbations are random resize, random crop, random color jitter, and random\nﬂip. The novelty in works such as MoCo is to keep an adaptive queue of keys to be used as\ndissimilar examples. This ensures the cost of training is reduced by not back-propagating\nthrough the network for negative examples while keeping an up-to-date set of negatives with\nenough contrast. Another novelty in works such as SimCLR and MoCo v2 is to use more\ndiverse and aggressive data augmentations to generate a larger set of possible similar and\ndissimilar pairs and hence better contrastive examples.\nChapter 3\nVSE++: Improving Visual-Semantic\nEmbeddings with Hard Negative\nMining\nThe ﬁrst problem studied in this thesis concerns training efﬁciency in computer vision. We\nhypothesize that training data are not equally important; training signal from some inputs is\nstronger than others. Moreover, we hypothesize that prioritizing learning on more informative\ntraining data increases convergence speed and improves generalization performance on test\ndata.\nTo this end, we present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in\nstructured prediction, and ranking loss functions, we introduce a simple change to common\nloss functions used for multi-modal embeddings. That, combined with ﬁne-tuning and use of\naugmented data, yields signiﬁcant gains in retrieval performance. We showcase our approach,\nVSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with\nexisting methods. On MS-COCO our approach outperforms state-of-the-art methods by\n8.8% in caption retrieval and 11.3% in image retrieval (at R@1).\n20\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING21\nThe content of this chapter have appeared in the following publication:\n• Faghri, Fartash and Fleet, David J. and Kiros, Jamie R. and Fidler, Sanja, “VSE++:\nImproving Visual-Semantic Embeddings with Hard Negatives\", British Machine\nVision Conference (BMVC), 2018.\nTo ensure reproducibility, our code is publicly available 1.\n3.1\nIntroduction\nJoint embeddings enable a wide range of tasks in image, video and language understanding.\nExamples include shape-image embeddings [115] for shape inference, bilingual word em-\nbeddings [238], human pose-image embeddings for 3D pose inference [113], ﬁne-grained\nrecognition [166], zero-shot learning [56], and modality conversion via synthesis [165,\n166]. Such embeddings entail mappings from two (or more) domains into a common vector\nspace in which semantically associated inputs (e.g., text and images) are mapped to similar\nlocations. The embedding space thus represents the underlying domain structure, where\nlocation and often direction are semantically meaningful.\nVisual-semantic embeddings have been central to image-caption retrieval and gener-\nation [103, 98], and visual question-answering [128]. One approach to visual question-\nanswering, for example, is to ﬁrst describe an image by a set of captions, and then to ﬁnd the\nnearest caption in response to a question [1, 237]. For image synthesis from text, one could\nmap from text to the joint embedding space, and then back to image space [165, 166].\nHere we focus on visual-semantic embeddings for cross-modal retrieval; i.e., the retrieval\nof images given captions, or of captions for a query image. As is common in retrieval, we\nmeasure performance by R@K, i.e., recall at K – the fraction of queries for which the\ncorrect item is retrieved in the closest K points to the query in the embedding space (K\nis usually a small integer, often 1). More generally, retrieval is a natural way to assess the\nquality of joint embeddings for image and language data [84].\n1\nhttps://github.com/fartashf/vsepp\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING22\nThe basic problem is one of ranking; the correct target(s) should be closer to the query\nthan other items in the corpus, not unlike learning to rank problems [e.g., 112], and max-\nmargin structured prediction [25, 108]. The formulation and model architecture in this paper\nare most closely related to those of [103], learned with a triplet ranking loss. In contrast\nto that work, we advocate a novel loss, the use of augmented data, and ﬁne-tuning, which,\ntogether, produce a signiﬁcant increase in caption retrieval performance over the baseline\nranking loss on well-known benchmark data. We outperform the best reported result on\nMS-COCO by almost 9%. We also show that the beneﬁt of a more powerful image encoder,\nwith ﬁne-tuning, is ampliﬁed with the use of our stronger loss function. We refer to our\nmodel as VSE++.\nOur main contribution is to incorporate hard negatives in the loss function. This was\ninspired by the use of hard negative mining in classiﬁcation tasks [38, 53, 129], and by\nthe use of hard negatives for improving image embeddings for face recognition [179, 219].\nMinimizing a loss function using hard negative mining is equivalent to minimizing a modiﬁed\nnon-transparent loss function with uniform sampling. We extend the idea with the explicit\nintroduction of hard negatives in the loss for multi-modal embeddings, without any additional\ncost of mining.\nWe also note that our formulation complements other recent articles that propose new\narchitectures or similarity functions for this problem. To this end, we demonstrate improve-\nments to [210]. Among other methods that could be improved with a modiﬁed loss, [214]\npropose an embedding network to fully replace the similarity function used for the ranking\nloss. An attention mechanism on both images and captions is used by [146], where the\nauthors sequentially and selectively focus on a subset of words and image regions to compute\nthe similarity. In [86], the authors use a multi-modal context-modulated attention mechanism\nto compute the similarity between images and captions. Our proposed loss function and\ntriplet sampling could be extended and applied to other such problems.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING23\n3.2\nLearning Visual-Semantic Embeddings\nFor image-caption retrieval the query is a caption and the task is to retrieve the most relevant\nimage(s) from a database. Alternatively, the query may be an image, and the task is to\nretrieves relevant captions. The goal is to maximize recall at K (R@K), i.e., the fraction of\nqueries for which the most relevant item is ranked among the top K items returned.\nLet S = {(in, cn)}N\nn=1 be a training set of image-caption pairs. We refer to (in, cn)\nas positive pairs and (in, cm̸=n) as negative pairs; i.e., the most relevant caption to the\nimage in is cn and for caption cn, it is the image in. We deﬁne a similarity function\ns(i, c) ∈R that should, ideally, give higher similarity scores to positive pairs than negatives.\nIn caption retrieval, the query is an image and we rank a database of captions based on the\nsimilarity function; i.e., R@K is the percentage of queries for which the positive caption\nis ranked among the top K captions using s(i, c). Likewise for image retrieval. In what\nfollows the similarity function is deﬁned on the joint embedding space. This differs from\nother formulations, such as [214], which use a similarity network to directly classify an\nimage-caption pair as matching or non-matching.\n3.2.1\nVisual-Semantic Embedding\nLet φ(i; θφ) ∈RDφ be a feature-based representation computed from image i (e.g., the\nrepresentation before logits in VGG19 [187] or ResNet152 [78]). Similarly, let ψ(c; θψ) ∈\nRDψ be a representation of caption c in a caption embedding space (e.g., a GRU-based text\nencoder). Here, θφ and θψ denote model parameters for the respective mappings to these\ninitial image and caption representations.\nThen, let the mappings into the joint embedding space be deﬁned by linear projections:\nf(i; Wf, θφ) = W T\nf φ(i; θφ)\n(3.1)\ng(c; Wg, θψ) = W T\ng ψ(c; θψ)\n(3.2)\nwhere Wf ∈RDφ×D and Wg ∈RDψ×D. We further normalize f(i; Wf, θφ), and g(c; Wg, θψ),\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING24\nto lie on the unit hypersphere. Finally, we deﬁne the similarity function in the joint embed-\nding space to be the usual inner product:\ns(i, c) = f(i; Wf, θφ) · g(c; Wg, θψ) .\n(3.3)\nLet θ = {Wf, Wg, θψ} be the model parameters. If we also ﬁne-tune the image encoder,\nthen we would also include θφ in θ.\nTraining entails the minimization of empirical loss with respect to θ, i.e., the cumulative\nloss over training data S = {(in, cn)}N\nn=1:\ne(θ, S) = 1\nN\nN\nX\nn=1\nℓ(in, cn)\n(3.4)\nwhere ℓ(in, cn) is a suitable loss function for a single training exemplar. Inspired by the use\nof a triplet loss for image retrieval [e.g., 57, 26], recent approaches to joint visual-semantic\nembeddings have used a hinge-based triplet ranking loss [103, 98, 235, 189]:\nℓSH(i, c) =\nX\nˆc\n[α −s(i, c) + s(i, ˆc)]+ +\nX\nˆi\n[α −s(i, c) + s(ˆi, c)]+ ,\n(3.5)\nwhere α serves as a margin parameter, and [x]+ ≡max(x, 0). This hinge loss comprises\ntwo symmetric terms. The ﬁrst sum is taken over all negative captions ˆc, given query i. The\nsecond is taken over all negative images ˆi, given caption c. Each term is proportional to the\nexpected loss (or violation) over sets of negative samples. If i and c are closer to one another\nin the joint embedding space than to any negative, by the margin α, the hinge loss is zero. In\npractice, for computational efﬁciency, rather than summing over all negatives in the training\nset, it is common to only sum over (or randomly sample) the negatives in a mini-batch of\nstochastic gradient descent [103, 189, 98]. The runtime complexity of computing this loss\napproximation is quadratic in the number of image-caption pairs in a mini-batch.\nOf course there are other loss functions that one might consider. One is a pairwise\nhinge loss in which elements of positive pairs are encouraged to lie within a hypersphere\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING25\ni\nc\nc′\nˆc1\nˆc2\nˆc3\n(a)\ni\nc\nc′\nˆc1\nˆc2\nˆc3\nˆc4\nˆc5\nˆc6\n(b)\nFigure 3.1: An illustration of typical positive pairs and the nearest negative samples. Here\nassume similarity score is the negative distance. Filled circles show a positive pair (i, c),\nwhile empty circles are negative samples for the query i. The dashed circles on the two sides\nare drawn at the same radii. Notice that the hardest negative sample c′ is closer to i in a.\nAssuming a zero margin, b has a higher loss with the SH loss compared to a. The MH loss\nassigns a higher loss to a.\nof radius ρ1 in the joint embedding space, while negative pairs should be no closer than\nρ2 > ρ1. This is problematic as it constrains the structure of the latent space more than\ndoes the ranking loss, and it entails the use of two hyper-parameters which can be very\ndifﬁcult to set. Another possible approach is to use Canonical Correlation Analysis to learn\nWf and Wg, thereby trying to preserve correlation between the text and images in the joint\nembedding [e.g., 104, 46]. By comparison, when measuring performance as R@K, for small\nK, a correlation-based loss will not give sufﬁcient inﬂuence to the embedding of negative\nitems in the local vicinity of positive pairs, which is critical for R@K.\n3.2.2\nEmphasis on Hard Negatives\nInspired by common loss functions used in structured prediction [205, 225, 53], we focus\non hard negatives for training, i.e., the negatives closest to each training query. This is\nparticularly relevant for retrieval since it is the hardest negative that determines success or\nfailure as measured by R@1.\nGiven a positive pair (i, c), the hardest negatives are given by i′ = arg maxj̸=i s(j, c)\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING26\nand c′ = arg maxd̸=c s(i, d). To emphasize hard negatives we deﬁne our loss as\nℓMH(i, c) = max\nc′\n\u0002\nα −s(i, c) + s(i, c′)\n\u0003\n+ + max\ni′\n\u0002\nα −s(i, c) + s(i′, c)\n\u0003\n+ .\n(3.6)\nLike Eq. 3.5, this loss comprises two terms, one with i and one with c as queries. Unlike\nEq. 3.5, this loss is speciﬁed in terms of the hardest negatives, c′ and i′. We refer to the loss\nin Eq. 3.6 as Max of Hinges (MH) loss, and the loss in Eq. 3.5 as Sum of Hinges (SH) loss.\nThere is a spectrum of loss functions from the SH loss to the MH loss. In the MH loss, the\nwinner takes all the gradients, where instead we use re-weighted gradients of all the triplets.\nWe only discuss the MH loss as it was empirically found to perform the best.\nOne case in which the MH loss is superior to SH is when multiple negatives with small\nviolations combine to dominate the SH loss. For example, Fig. 3.1 depicts a positive pair\ntogether with two sets of negatives. In Fig. 3.1a, a single negative is too close to the query,\nwhich may require a signiﬁcant change to the mapping. However, any training step that\npushes the hard negative away, might cause a number of small violating negatives, as in\nFig. 3.1b. Using the SH loss, these ‘new’ negatives may dominate the loss, so the model\nis pushed back to the ﬁrst example in Fig. 3.1a. As a result, the optimization may oscillate\nbetween two states with no reduction in the SH loss while in a similar scenario the MH loss\nwould decrease monotonically because it focuses on the hardest negative.\nFor computational efﬁciency, instead of ﬁnding the hardest negatives in the entire training\nset, we ﬁnd them within each mini-batch. This has the same quadratic complexity as the\ncomplexity of the SH loss. With random sampling of the mini-batches, this approximation\nyields other advantages. One is that there is a high probability of getting hard negatives that\nare harder than at least 90% of the entire training set (see Section 3.2.3 for the explanation).\nMoreover, the loss is potentially robust to label errors in the training data because the\nprobability of sampling the hardest negative over the entire training set is somewhat low.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING27\n3.2.3\nProbability of Sampling the Hardest Negative\nLet S = {(in, cn)}N\nn=1 denote a training set of image-caption pairs, and let C = {cn} denote\nthe set of captions. Suppose we draw M samples in a mini-batch, Q = {(im, cm)}M\nm=1,\nfrom S. Let the permutation, πm, on C refer to the rankings of captions according to the\nsimilarity function s(im, cn) for cn ∈S \\ {cm}. We can assume permutations, πm, are\nuncorrelated.\nGiven a query image, im, we are interested in the probability of getting no captions from\nthe 90th percentile of πm in the mini-batch. Assuming i.i.d. samples, this probability is\nsimply .9(M−1), the probability that no sample in the mini-batch is from the 90th percentile.\nThis probability tends to zero exponentially fast, falling below 1% for M ≥44. Hence, for\nlarge enough mini-batches, with high probability we sample negative captions that are harder\nthan 90% of the entire training set. The probability for the 99.9th percentile of πm tends to\nzero more slowly; it falls below 1% for M ≥6905, which is a relatively large mini-batch.\nWhile we get strong signals by randomly sampling negatives within mini-batches, such\nsampling also provides some robustness to outliers, such as negative captions that better\ndescribe an image compared to the ground-truth caption. Mini-batches as small as 128 can\nprovide strong enough training signal and robustness to label errors. Of course by increasing\nthe mini-batch size, we get harder negative examples and possibly a stronger training signal.\nHowever, by increasing the mini-batch size, we lose the beneﬁt of SGD in ﬁnding good\noptima and exploiting the gradient noise. This can lead to getting stuck in local optima or as\nobserved by [179], extremely long training time.\n3.3\nExperiments\nBelow we perform experiments with our approach, VSE++, comparing it to a baseline\nformulation with SH loss, denoted VSE0, and other state-of-the-art approaches. Essentially,\nthe baseline formulation, VSE0, is similar to that in [103], denoted UVS.\nWe experiment with two image encoders: VGG19 by [187] and ResNet152 by [78]. In\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING28\n#\nModel\nTrainset\nCaption Retrieval\nImage Retrieval\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\n1K Test Images\n3.1.1\nUVS ([103], GitHub)\n1C (1 fold)\n43.4\n75.7\n85.8\n2\n31.0\n66.7\n79.9\n3\n3.1.2\nOrder [210]\n10C+rV\n46.7\n-\n88.9\n2.0\n37.9\n-\n85.9\n2.0\n3.1.3\nEmbedding Net [214]\n10C+rV\n50.4\n79.3\n69.4\n-\n39.8\n75.3\n86.6\n-\n3.1.4\nsm-LSTM [86]\n?\n53.2\n83.1\n91.5\n1\n40.7\n75.8\n87.4\n2\n3.1.5\n2WayNet [46]\n10C+rV\n55.8\n75.2\n-\n-\n39.7\n63.3\n-\n-\n3.1.6\nVSE++\n1C (1 fold)\n43.6\n74.8\n84.6\n2.0\n33.7\n68.8\n81.0\n3.0\n3.1.7\nVSE++\nRC\n49.0\n79.8\n88.4\n1.8\n37.1\n72.2\n83.8\n2.0\n3.1.8\nVSE++\nRC+rV\n51.9\n81.5\n90.4\n1.0\n39.5\n74.1\n85.6\n2.0\n3.1.9\nVSE++ (FT)\nRC+rV\n57.2\n86.0\n93.3\n1.0\n45.9\n79.4\n89.1\n2.0\n3.1.10\nVSE++ (ResNet)\nRC+rV\n58.3\n86.1\n93.3\n1.0\n43.6\n77.6\n87.8\n2.0\n3.1.11\nVSE++ (ResNet, FT)\nRC+rV\n64.6\n90.0\n95.7\n1.0\n52.0\n84.3\n92.0\n1.0\n5K Test Images\n3.1.12\nOrder [210]\n10C+rV\n23.3\n-\n65.0\n5.0\n18.0\n-\n57.6\n7.0\n3.1.13\nVSE++ (FT)\nRC+rV\n32.9\n61.7\n74.7\n3.0\n24.1\n52.8\n66.2\n5.0\n3.1.14\nVSE++ (ResNet, FT)\nRC+rV\n41.3\n71.1\n81.2\n2.0\n30.3\n59.4\n72.4\n4.0\nTable 3.1: Results of experiments on MS-COCO.\nwhat follows, we use VGG19 unless speciﬁed otherwise. As in previous work we extract\nimage features directly from FC7, the penultimate fully connected layer. The dimensionality\nof the image embedding, Dφ, is 4096 for VGG19 and 2048 for ResNet152.\nIn more detail, we ﬁrst resize the image to 256 × 256, and then use either a single crop\nof size 224 × 224 or the mean of feature vectors for multiple crops of similar size. We\nrefer to training with one center crop as 1C, and training with 10 crops at ﬁxed locations as\n10C. These image features can be pre-computed once and reused. We also experiment with\nusing a single random crop, denoted by RC. For RC, image features are computed on the ﬂy.\nRecent works have mostly used RC/10C. In our preliminary experiments, we did not observe\nsigniﬁcant differences between RC/10C. As such, we perform most experiments with RC.\nFor the caption encoder, we use a GRU similar to the one used in [103]. We set\nthe dimensionality of the GRU, Dψ, and the joint embedding space, D, to 1024. The\ndimensionality of the word embeddings that are input to the GRU is set to 300.\nWe further note that in [103], the caption embedding is normalized, while the image\nembedding is not. Normalization of both vectors means that the similarity function is cosine\nsimilarity. In VSE++ we normalize both vectors. Not normalizing the image embedding\nchanges the importance of samples. In our experiments, not normalizing the image embed-\nding helped the baseline, VSE0, to ﬁnd a better solution. However, VSE++ is not signiﬁcantly\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING29\naffected by this normalization.\n3.3.1\nDatasets\nWe evaluate our method on the Microsoft COCO dataset [118] and the Flickr30K dataset [224].\nFlickr30K has a standard 30, 000 images for training. Following [98], we use 1000 images\nfor validation and 1000 images for testing. We also use the splits of [98] for MS-COCO.\nIn this split, the training set contains 82, 783 images, 5000 validation and 5000 test images.\nHowever, there are also 30, 504 images that were originally in the validation set of MS-\nCOCO but have been left out in this split. We refer to this set as rV. Some papers use rV for\ntraining (113, 287 training images in total) to further improve accuracy. We report results\nusing both training sets. Each image comes with 5 captions. The results are reported by\neither averaging over 5 folds of 1K test images or testing on the full 5K test images.\n3.3.2\nDetails of Training\nWe use the Adam optimizer [102]. Models are trained for at most 30 epochs. Except for\nﬁne-tuned models, we start training with learning rate 0.0002 for 15 epochs, and then lower\nthe learning rate to 0.00002 for another 15 epochs. The ﬁne-tuned models are trained by\ntaking a model trained for 30 epochs with a ﬁxed image encoder, and then training it for 15\nepochs with a learning rate of 0.00002. We set the margin to 0.2 for most experiments. We\nuse a mini-batch size of 128 in all experiments. Notice that since the size of the training set\nfor different models is different, the actual number of iterations in each epoch can vary. For\nevaluation on the test set, we tackle over-ﬁtting by choosing the snapshot of the model that\nperforms best on the validation set. The best snapshot is selected based on the sum of the\nrecalls on the validation set.\n3.3.3\nResults on MS-COCO\nThe results on the MS-COCO dataset are presented in Table 3.1. To understand the effect\nof training and algorithmic variations we report ablation studies for the baseline VSE0 (see\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING30\n#\nModel\nTrainset\nCaption Retrieval\nImage Retrieval\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\n3.2.1\nVSE0\n1C (1 fold)\n43.2\n73.9\n85.0\n2.0\n33.0\n67.4\n80.7\n3.0\n3.1.6\nVSE++\n1C (1 fold)\n43.6\n74.8\n84.6\n2.0\n33.7\n68.8\n81.0\n3.0\n3.2.2\nVSE0\nRC\n43.1\n77.0\n87.1\n2.0\n32.5\n68.3\n82.1\n3.0\n3.1.7\nVSE++\nRC\n49.0\n79.8\n88.4\n1.8\n37.1\n72.2\n83.8\n2.0\n3.2.3\nVSE0\nRC+rV\n46.8\n78.8\n89.0\n1.8\n34.2\n70.4\n83.6\n2.6\n3.1.8\nVSE++\nRC+rV\n51.9\n81.5\n90.4\n1.0\n39.5\n74.1\n85.6\n2.0\n3.2.4\nVSE0 (FT)\nRC+rV\n50.1\n81.5\n90.5\n1.6\n39.7\n75.4\n87.2\n2.0\n3.1.9\nVSE++ (FT)\nRC+rV\n57.2\n86.0\n93.3\n1.0\n45.9\n79.4\n89.1\n2.0\n3.2.5\nVSE0 (ResNet)\nRC+rV\n52.7\n83.0\n91.8\n1.0\n36.0\n72.6\n85.5\n2.2\n3.1.10\nVSE++ (ResNet)\nRC+rV\n58.3\n86.1\n93.3\n1.0\n43.6\n77.6\n87.8\n2.0\n3.2.6\nVSE0 (ResNet, FT)\nRC+rV\n56.0\n85.8\n93.5\n1.0\n43.7\n79.4\n89.7\n2.0\n3.1.11\nVSE++ (ResNet, FT)\nRC+rV\n64.6\n90.0\n95.7\n1.0\n52.0\n84.3\n92.0\n1.0\nTable 3.2: The effect of data augmentation and ﬁne-tuning. We copy the relevant results for\nVSE++ from Table 3.1 to enable an easier comparison. Notice that after applying all the\nmodiﬁcations, VSE0 model reaches 56.0% for R@1, while VSE++ achieves 64.6%.\nTable 3.2). Our best result with VSE++ is achieved by using ResNet152 and ﬁne-tuning the\nimage encoder (row 3.1.11), where we see 21.2% improvement in R@1 for caption retrieval\nand 21% improvement in R@1 for image retrieval compared to UVS (rows 3.1.1 and 3.1.11).\nEffect of MH loss. Using ResNet152 and ﬁne-tuning can only lead to 12.6% improve-\nment using the VSE0 formulation (rows 3.2.6 and 3.1.1), while our MH loss function brings\na signiﬁcant additional gain of 8.6% (rows 3.1.11 and 3.2.6).\nEffect of the training set. We compare VSE0 and VSE++ by incrementally improving\nthe training data. Comparing the models trained on 1C (rows 3.1.1 and 3.1.6), we only\nsee 2.7% improvement in R@1 for image retrieval but no improvement in caption retrieval\nperformance. However, when we train using RC (rows 3.1.7 and 3.2.2) or RC+rV (rows 3.1.8\nand 3.2.3), we see that VSE++ gains an improvement of 5.9% and 5.1%, respectively, in\nR@1 for caption retrieval compared to VSE0. This shows that VSE++ can better exploit the\nadditional data.\nEffect of a better image encoding. We also investigate the effect of a better image encoder\non the models. Row 3.1.9 and row 3.2.4 show the effect of ﬁne-tuning the VGG19 image\nencoder. We see that the gap between VSE0 and VSE++ increases to 6.1%. If we use\nResNet152 instead of VGG19 (row 3.1.10 and row 3.2.5), the gap is 5.6%. As for our best\nresult, if we use ResNet152 and also ﬁne-tune the image encoder (row 3.1.11 and row 3.2.6)\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING31\n#\nModel\nTrainset\nCaption Retrieval\nImage Retrieval\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\n3.3.1\nUVS [103]\n1C\n23.0\n50.7\n62.9\n5\n16.8\n42.0\n56.5\n8\n3.3.2\nUVS (GitHub)\n1C\n29.8\n58.4\n70.5\n4\n22.0\n47.9\n59.3\n6\n3.3.3\nEmbedding Net [214]\n10C\n40.7\n69.7\n79.2\n-\n29.2\n59.6\n71.7\n-\n3.3.4\nDAN [146]\n?\n41.4\n73.5\n82.5\n2\n31.8\n61.7\n72.5\n3\n3.3.5\nsm-LSTM [86]\n?\n42.5\n71.9\n81.5\n2\n30.2\n60.4\n72.3\n3\n3.3.6\n2WayNet [46]\n10C\n49.8\n67.5\n-\n-\n36.0\n55.6\n-\n-\n3.3.7\nDAN (ResNet) [146]\n?\n55.0\n81.8\n89.0\n1\n39.4\n69.2\n79.1\n2\n3.3.8\nVSE0\n1C\n29.8\n59.8\n71.9\n3.0\n23.0\n48.8\n61.0\n6.0\n3.3.9\nVSE0\nRC\n31.6\n59.3\n71.7\n4.0\n21.6\n50.7\n63.8\n5.0\n3.3.10\nVSE++\n1C\n31.9\n58.4\n68.0\n4.0\n23.1\n49.2\n60.7\n6.0\n3.3.11\nVSE++\nRC\n38.6\n64.6\n74.6\n2.0\n26.8\n54.9\n66.8\n4.0\n3.3.12\nVSE0 (FT)\nRC\n37.4\n65.4\n77.2\n3.0\n26.8\n57.6\n69.5\n4.0\n3.3.13\nVSE++ (FT)\nRC\n41.3\n69.1\n77.9\n2.0\n31.4\n60.0\n71.2\n3.0\n3.3.14\nVSE0 (ResNet)\nRC\n36.6\n67.3\n78.4\n3.0\n23.3\n52.6\n66.0\n5.0\n3.3.15\nVSE++ (ResNet)\nRC\n43.7\n71.9\n82.1\n2.0\n32.3\n60.9\n72.1\n3.0\n3.3.16\nVSE0 (ResNet, FT)\nRC\n42.1\n73.2\n84.0\n2.0\n31.8\n62.6\n74.1\n3.0\n3.3.17\nVSE++ (ResNet, FT)\nRC\n52.9\n80.5\n87.2\n1.0\n39.6\n70.1\n79.5\n2.0\nTable 3.3: Results on the Flickr30K dataset.\nthe gap becomes 8.6%. The increase in the performance gap shows that the improved loss of\nVSE++ can better guide the optimization when a more powerful image encoder is used.\nComparison with state-of-the-art. Comparing VSE++ (ResNet, FT) to the state-of-the-art\non MS-COCO at the time of publication, 2WayNet (row 3.1.11 and row 3.1.5), we see 8.8%\nimprovement in R@1 for caption retrieval and compared to sm-LSTM (row 3.1.11 and\nrow 3.1.4), 11.3% improvement in image retrieval. We also report results on the full 5K test\nset of MS-COCO in rows 3.1.13 and 3.1.14.\n3.3.4\nResults on Flickr30K\nTables 3.3 summarizes the performance on Flickr30K. We obtain 23.1% improvement in\nR@1 for caption retrieval and 17.6% improvement in R@1 for image retrieval (rows 3.3.1\nand 3.3.17). We observed that VSE++ over-ﬁts when trained with the pre-computed features\nof 1C. The reason is potentially the limited size of the Flickr30K training set. As explained in\nSec. 3.3.2, we select a snapshot of the model before over-ﬁtting occurs, based on performance\nwith the validation set. Over-ﬁtting does not occur when the model is trained using the RC\ntraining data. Our results show the improvements incurred by our MH loss persist across\ndatasets, as well as across models.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING32\n3.3.5\nImproving Order Embeddings\nGiven the simplicity of our approach, our proposed loss function can complement the recent\napproaches that use more sophisticated model architectures or similarity functions. Here\nwe demonstrate the beneﬁts of the MH loss by applying it to another approach to joint\nembeddings called order-embeddings [210]. The main difference with the formulation above\nis the use of an asymmetric similarity function, i.e., s(i, c) = −∥max(0, g(c; Wg, θψ) −\nf(i; Wf, θφ))∥2. Again, we simply replace their use of the SH loss by our MH loss.\nLike their experimental setting, we use the training set 10C+rV. For our Order++, we\nuse the same learning schedule and margin as our other experiments. However, we use their\ntraining settings to train Order0. We start training with a learning rate of 0.001 for 15 epochs\nand lower the learning rate to 0.0001 for another 15 epochs. Like [210] we use a margin\nof 0.05. Additionally, [210] takes the absolute value of embeddings before computing the\nsimilarity function which we replicate only for Order0.\nTable 3.4 reports the results when the SH loss is replaced by the MH loss. We replicate\ntheir results using our Order0 formulation and get slightly better results (row 3.4.1 and\nrow 3.4.3). We observe 4.5% improvement from Order0 to Order++ in R@1 for caption\nretrieval (row 3.4.3 and row 3.4.5). Compared to the improvement from VSE0 to VSE++,\nwhere the improvement on the 10C+rV training set is 1.8%, we gain an even higher im-\nprovement here. This shows that the MH loss can potentially improve numerous similar loss\nfunctions used in retrieval and ranking tasks.\n#\nModel\nCaption Retrieval\nImage Retrieval\nR@1\nR@5\nR@10\nMed r\nR@1\nR@5\nR@10\nMed r\n1K Test Images\n3.4.1\nOrder [210]\n46.7\n-\n88.9\n2.0\n37.9\n-\n85.9\n2.0\n3.4.2\nVSE0\n49.5\n81.0\n90.0\n1.8\n38.1\n73.3\n85.1\n2.0\n3.4.3\nOrder0\n48.5\n80.9\n90.3\n1.8\n39.6\n75.3\n86.7\n2.0\n3.4.4\nVSE++\n51.3\n82.2\n91.0\n1.2\n40.1\n75.3\n86.1\n2.0\n3.4.5\nOrder++\n53.0\n83.4\n91.9\n1.0\n42.3\n77.4\n88.1\n2.0\nTable 3.4: Comparison on MS-COCO. Training set for all the rows is 10C+rV.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING33\nFigure 3.2: Analysis of the behavior of the MH loss on the Flickr30K dataset training with\nRC. This ﬁgure compares the SH loss to the MH loss (Table 3.3, row 3.3.9 and row 3.3.11).\nNotice that, in the ﬁrst 5 epochs the SH loss achieves a better performance, however, from\nthere-on the MH loss leads to much higher recall rates.\n3.3.6\nBehavior of Loss Functions\nWe observe that the MH loss can take a few epochs to ‘warm-up’ during training. Fig. 3.2\ndepicts such behavior on the Flickr30K dataset using RC. Notice that the SH loss starts off\nfaster, but after approximately 5 epochs MH loss surpasses SH loss. To explain this, the\nMH loss depends on a smaller set of triplets compared to the SH loss. Early in training the\ngradient of the MH loss is inﬂuenced by a relatively small set of triples. As such, it can take\nmore iterations to train a model with the MH loss. We explored a simple form of curriculum\nlearning [14] to speed-up the training. We start training with the SH loss for a few epochs,\nthen switch to the MH loss for the rest of the training. However, it did not perform much\nbetter than training solely with the MH loss.\nIn [179], it is reported that with a mini-batch size of 1800, training is extremely slow.\nWe experienced similar behavior with large mini-batches up to 512. However, mini-batches\nof size 128 or 256 exceeded the performance of the SH loss within the same training time.\n3.3.7\nExamples of Hard Negatives\nFig. 3.3 shows the hard negatives in a random mini-batch. These examples illustrate that\nhard negatives from a mini-batch can provide useful gradient information.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING34\nFig. 3.4 provides additional examples comparing the outputs of VSE++ and VSE0.\n3.3.8\nDistribution of distances\nFig. 3.5 compares the distribution of distances for hard negatives using MH loss versus the\nSH loss after 10 and 30 epochs. For the MH loss, the distribution of distances is negatively\nskewed after 10 epochs, while the distribution using the SH loss is approximately symmetric.\nThe reason is, the MH loss disproportionately focuses on the hardest negatives that results in\na sharp drop in the number of hard negatives after cosine similarity 0.4 while a large stack\nof hard negatives is created before 0.4. As the training progresses to epoch 30, the stack of\nhard negatives is dispersed but the drop to zero probability is still sharper than the SH loss.\nAt Epoch 30, this margin is reduced to 0.3.\n3.3.9\nEffect of Negative Set Size on MH Loss\nOur proposed hard negatives are found in the mini-batch of triplets in each training step\nthat is a uniformly sampled subset of the training set. Instead, one could ﬁnd the hardest\nnegatives within the training set which is computationally expensive but potentially performs\nbetter. We study the effective sample size over which we searched for negatives (while\nkeeping the mini-batch size ﬁxed at 128). In the extreme case, when the negative set is the\ntraining set, we get the hardest negatives in the entire training set. As discussed in Sec. 3.2.2,\nsampling a negative set smaller than the training set can potentially be more robust to label\nerrors.\nIn Fig. 3.6, we show the effect of the negative sample set size on the performance of\nMH loss. We compare the caption retrieval performance for different negative set sizes\nvaried from 2 to 512. In practice, for negative set sizes smaller than the mini-batch size,\n128, we randomly sample the negative set from the mini-batch, and when the mini-batch\nsize is smaller than the negative set size, we randomly sample the mini-batch from the\nnegative set. We observe that on this dataset, the optimal negative set size is around 128.\nInterestingly, for negative sets as small as 2, R@1 is slightly below VSE0. We observe a drop\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING35\nin performance as the negative set size is increased to 512. We hypothesize that this number\nis dataset dependant and for a small dataset like Flickr30K, the probability of sampling\na noisy example increases signiﬁcantly. Even though the performance drops with larger\nmini-batch sizes, it still performs better than the SH loss.\n3.4\nConclusion\nThis chapter focused on learning visual-semantic embeddings for cross-modal, image-caption\nretrieval. Inspired by structured prediction, we proposed a new loss based on violations\nincurred by relatively hard negatives compared to current methods that used expected\nerrors [103, 210]. We performed experiments on the MS-COCO and Flickr30K datasets and\nshowed that our proposed loss signiﬁcantly improves performance on these datasets. We\nobserved that the improved loss can better guide a more powerful image encoder, ResNet152,\nand also guide better when ﬁne-tuning an image encoder. At the time of publication, with all\nmodiﬁcations, our VSE++ model achieved state-of-the-art performance on the MS-COCO\ndataset, and was slightly below the best model at the time on the Flickr30K dataset. Our\nproposed loss function can be used to train more sophisticated models that have been using a\nsimilar ranking loss for training.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING36\nGT: A little girl wearing\npink pants, pink and white\ntennis shoes and a white\nshirt with a little girl on it\nputs her face in a blue Talk-\ning Tube.\nHN:\n[0.26]\nBlond\nboy\njumping onto deck.\nGT: A teal-haired woman\nin a very short black dress,\npantyhose, and boots stand-\ning with right arm raised\nand left hand obstructing\nher mouth in microphone-\nsinging fashion is standing.\nHN: [0.08] Two dancers in\nazure appear to be perform-\ning in an alleyway.\nGT: Two men, one in a\ndark blue button-down and\nthe other in a light blue tee,\nare chatting as they walk by\na small restaurant.\nHN: [0.41] Two men with\nguitars strapped to their\nback stand on the street cor-\nner with two other people\nbehind them.\nGT: A man wearing a\nblack\njacket\nand\ngray\nslacks, stands on the side-\nwalk holding a sheet with\nsomething printed on it in\nhis hand.\nHN: [0.26] Two men with\nguitars strapped to their\nback stand on the street cor-\nner with two other people\nbehind them.\nGT: There is a wall of a\nbuilding with several differ-\nent colors painted on it and\nin the distance one person\nsitting down and another\nwalking.\nHN: [0.06] A woman with\nluggage\nwalks\nalong\na\nstreet in front of a large\nadvertisement.\nGT: A man is laying on a\ngirl’s lap, she is looking at\nhim, she also has her hand\non her notebook computer.\nHN: [0.18] A woman sits\non a carpeted ﬂoor with a\nbaby.\nGT: A young blond girl in\na pink sweater, blue skirt,\nand brown boots is jumping\nover a puddle on a cloudy\nday.\nHN:\n[0.51]\nAn\nIndian\nwoman is sitting on the\nground, amongst drawings,\nrocks and shrubbery.\nGT: One man dressed in\nblack is stretching his leg\nup in the air, behind him is\na massive cruise ship in the\nwater.\nHN: [0.24] A topless man\nstraps surfboards on top of\nhis blue car.\nFigure 3.3: Examples from the Flickr30K training set along with their hard negatives in a\nrandom mini-batch according to the loss of a trained VSE++ model. The value in brackets\nis the cost of the hard negative and is in the range [0, 2] in our implementation. HN is the\nhardest negative in a random sample of size 128. GT is the positive caption used to compute\nthe cost of NG.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING37\nGT:\nTwo\nelephants\nare\nstanding by the trees in the\nwild.\nVSE0: [9] Three elephants\nkick up dust as they walk\nthrough the ﬂat by the\nbushes.\nVSE++: [1] A couple ele-\nphants walking by a tree af-\nter sunset.\nGT: A large multi layered\ncake with candles sticking\nout of it.\nVSE0:\n[1] A party dec-\noration containing ﬂowers,\nﬂags, and candles.\nVSE++: [1] A party dec-\noration containing ﬂowers,\nﬂags, and candles.\nGT: The man is walking\ndown the street with no\nshirt on.\nVSE0: [24] A person stand-\ning on a skate board in an\nalley.\nVSE++: [10] Two young\nmen are skateboarding on\nthe street.\nGT: A row of motorcycles\nparked in front of a build-\ning.\nVSE0: [2] a parking area\nfor motorcycles and bicy-\ncles along a street\nVSE++: [1] A number of\nmotorbikes parked on an al-\nley\nGT:\nsome skateboarders\ndoing tricks and people\nwatching them\nVSE0: [39] Young skate-\nboarder displaying skills on\nsidewalk near ﬁeld.\nVSE++:\n[3] Two young\nmen are outside skateboard-\ning together.\nGT: a brown cake with\nwhite icing and some wal-\nnut toppings\nVSE0: [6] A large slice of\nangel food cake sitting on\ntop of a plate.\nVSE++: [16] A baked loaf\nof bread is shown still in\nthe pan.\nGT: A woman holding a\nchild and standing near a\nbull.\nVSE0: [1] A woman hold-\ning a child and standing\nnear a bull.\nVSE++: [1] A woman hold-\ning a child looking at a cow.\nGT: A woman in a short\npink skirt holding a tennis\nracquet.\nVSE0: [6] A man playing\ntennis and holding back his\nracket to hit the ball.\nVSE++: [1] A woman is\nstanding while holding a\ntennis racket.\nFigure 3.4: Examples of MS-COCO test images and the top 1 retrieved captions for VSE0\nand VSE++ (ResNet)-ﬁnetune. The value in brackets is the rank of the highest ranked\nground-truth caption. GT is a sample from the ground-truth captions.\nCHAPTER 3. VSE++: IMPROVING VISUAL-SEMANTIC EMBEDDINGS WITH HARD NEGATIVE MINING38\nFigure 3.5: Distribution of distances for Flickr30K training set averaged over 10 ﬁxed\nexamples over the course of training. The distribution for the MH loss and the SH loss is\nshown for snapshots of the model at epoch 10 and 30. For comparison, the distribution for a\nrandomly initialized model is also depicted.\nFigure 3.6: The effect of the negative set size on the R@1 performance. The optimal R@1\nis achieved near mini-batch size 128.\nChapter 4\nGradient Clustering and A Study of\nGradient Variance in Deep Learning\nThe second problem addressed in this thesis is training efﬁciency as a general challenge in\ntraining deep neural networks. Instead of focusing on a particular task-speciﬁc loss function,\nwe seek improvements to optimization speed in gradient-based optimization methods for\ndeep learning. We revisit our hypothesis from the ﬁrst problem that training data are not\nequally important. We hypothesize that training deep learning models on diverse and\nheterogeneous data distributions is slower than homogeneous data distributions with less\ndiversity.\nIn this context, we study the distribution of gradients during training. We introduce a\nmethod, Gradient Clustering, to minimize the variance of average mini-batch gradient with\nstratiﬁed sampling. We prove that the variance of average mini-batch gradient is minimized\nif the elements are sampled from a weighted clustering in the gradient space. We measure\nthe gradient variance on common deep learning benchmarks and observe that, contrary\nto common assumptions, gradient variance increases at the beginning of the training, and\nsmaller learning rates coincide with higher variance. In addition, we introduce normalized\ngradient variance as a statistic that better correlates with the speed of convergence compared\nto gradient variance.\n39\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING40\nThe content of this chapter have appeared in the following publication:\n• Faghri, Fartash and Duvenaud, David and Fleet, David J. and Ba, Jimmy, “Gluster:\nVariance Reduced Mini-Batch SGD with Gradient Clustering\", Conference on Neural\nInformation Processing Systems (NeurIPS), Workshop on Beyond First Order Methods\nin ML, 2019.\nAdditionally, the results have been further developed in the following publications:\n• Faghri, Fartash, Tabrizian, Iman and Markov, Ilia and Alistarh, Dan and Roy, Dan M.\nand Ramezani-Kebrya, Ali, “Adaptive Gradient Quantization for Data-Parallel SGD\",\nConference on Neural Information Processing Systems (NeurIPS), 2020.\n• Ramezani-Kebrya, Ali and Faghri, Fartash and Markov, Ilya and Aksenov, Vitalii and\nAlistarh, Dan and Roy, Dan M. et al., “NUQSGD: Provably Communication-efﬁcient\nData-parallel SGD via Nonuniform Quantization\", Journal of Machine Learning\nResearch 22.114 (2021): 1-43.\nTo ensure reproducibility, our code is publicly available 1 2 3.\n4.1\nIntroduction\nMany machine learning tasks entail the minimization of the risk, Ex[ℓ(x; θ)], where x is\nan i.i.d. sample from a data distribution, and ℓis the per-example loss parametrized by\nθ. In supervised learning, inputs and ground-truth labels comprise x, and θ is a vector\nof model parameters. Empirical risk approximates the population risk by the risk of a\nsample set {xi}N\ni=1, the training set, as L(θ) = PN\ni=1ℓ(xi; θ)/N. Empirical risk is often\nminimized using gradient-based optimization (ﬁrst-order methods). For differentiable loss\nfunctions, the gradient of x is deﬁned as\n∂\n∂θℓ(x; θ), i.e., the gradient of the loss with respect\n1\nhttps://github.com/fartashf/gvar_code\n2\nhttps://github.com/fartashf/foptim\n3\nhttps://github.com/fartashf/nuqsgd\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING41\nto the parameters evaluated at a point x. Popular in deep learning, Mini-batch Stochastic\nGradient Descent (mini-batch SGD) iteratively takes small steps in the opposite direction of\nthe average gradient of B training samples. The mini-batch size is a hyper-parameter that\nprovides ﬂexibility in trading per-step computation time for potentially fewer total steps. In\nGD the mini-batch is the entire training set while in SGD it is a single sample.\nIn general, using any unbiased stochastic estimate of the gradient and sufﬁciently small\nstep sizes, SGD is guaranteed to converge to a minimum for various function classes [167].\nCommon convergence bounds in stochastic optimization improve with smaller gradient\nvariance [18]. Mini-batch SGD is said to converge faster because the variance of the gradient\nestimates is reduced by a rate linear in the mini-batch size. In practice however, we observe\ndiminishing returns in speeding up the training of almost any deep model on deep learning\nbenchmarks [181]. The transition point to diminishing returns is known to depend on\nthe choice of data, model and optimization method. [229] observed that the limitation\nof acceleration in large batches is reduced when momentum or preconditioning is used.\nOther works suggest that very small mini-batch sizes can still converge fast enough using a\ncollection of tricks [63, 135, 116]. One hypothesis is that the stochasticity due to small mini-\nbatches improves generalization by ﬁnding “ﬂat minima” and avoiding “sharp minima” [68,\n100]. But this hypothesis does not explain why diminishing returns also happens in the\ntraining loss.\nMotivated by the diminishing returns phenomena, we study and model the distribution\nof the gradients. Given a data distribution characterized by its probability density function,\nP(x), the gradient distribution is deﬁned as a transformation of the data distribution by\nthe gradient of the loss function,\n∂\n∂θℓ(x; θ). The transformation is a function of the model\nparameters that can be deterministic, e.g., with a linear model or stochastic, for example,\nwhen using random data augmentation or dropout. As such, the gradient distribution varies\nacross model architectures and evolves during the training.\nAn unbiased gradient estimator is an estimate of the mean of the gradient distribution\nthat is commonly evaluated by its variance. In the noisy gradient view, the average mini-\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING42\nbatch gradient (or the mini-batch gradient) is an unbiased estimator of the expected gradient\nwhere increasing the mini-batch size reduces the variance of this estimator. We propose a\ndistributional view and argue that knowledge of the gradient distribution can be exploited to\nanalyze and improve optimization speed as well as generalization to test data. A mean-aware\noptimization method is at best as strong as a distributional-aware optimization method. In our\ndistributional view, the mini-batch gradient is only an estimate of the mean of the gradient\ndistribution.\nQuestions: We identify the following questions about the gradient distribution.\nStructure of gradient distribution. Is there structure in the distribution over gradi-\nents of standard learning problems?\nImpact of gradient distribution on optimization. What characteristics of the gradi-\nent distribution correlate with the convergence speed and the minimum training/test\nloss reached?\nImpact of optimization on gradient distribution. To what extent do the follow-\ning factors affect the gradient distribution: data distribution, learning rate, model\narchitecture, mini-batch size, optimization method, and the distance to local optima?\nAs we review in Section 4.2, recent work have begun to investigate aforementioned questions\nbut we are far from a comprehensive understanding.\nContributions:\nExploiting clustered distributions. We consider gradient distributions with distinct\nmodes, i.e., the gradients can be clustered. We prove that the variance of average mini-\nbatch gradient is minimized if the elements are sampled from a weighted clustering in\ngradient space (Section 4.3).\nEfﬁcient clustering to minimize variance. We propose Gradient Clustering (GC) as\na computationally efﬁcient method for clustering in the gradient space (Section 4.3.2).\nFig. 4.1 shows an example of clusters found by GC.\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING43\n(a) GD step 0\n(b) GD step 1\n(c) GD step 2\nFigure 4.1: Example of clusters found using Gradient Clustering. A linear classiﬁer\nvisualized during training with gradient descent on 2 linearly separable classes (o, x).\nGradients are assigned to 4 clusters (different colors) using Gradient Clustering (GC). Black\nline depicts current decision boundary. Colored dashed lines depict decision boundaries\npredicted from current boundary and each of the 4 individual clusters. Here, blue points\nbelong to both classes; they have similar gradients, but are far apart in input space. By\nexploiting the knowledge of GC we can get low variance average mini-batch gradients.\nRelation between gradient variance and optimization. We study the gradient vari-\nance on common deep learning benchmarks (MNIST, CIFAR-10, and ImageNet) as\nwell as Random Features models recently studied in deep learning theory (Section 4.4).\nWe observe that gradient variance increases for most of the training, and smaller\nlearning rates coincide with higher variance.\nAn alternative statistic. We introduce normalized gradient variance as a statistic\nthat better correlates with the speed of convergence compared to gradient variance\n(Section 4.4).\nWe emphasize that some of our contributions are primarily empirical yet unexpected.\nWe believe our results provide an opportunity for future theoretical and empirical work.\n4.2\nRelated Work\nModeling gradient distribution. Despite various assumptions on the mini-batch gradient\nvariance, only recently these assumptions have been scrutinized for deep learning models. It\nis common to assume bounded variance in convergence analyses [18]. Works on variance\nreduction propose alternative estimates of the gradient mean with low variance [110, 95]\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING44\nbut they do not plot the variance which is the actual quantity they seek to reduce. Their\nineffectiveness in deep learning has been observed but still requires explanation [39]. There\nare works that present gradient variance plots [137, 217] but they are usually for a single\ngradient coordinate and synthetic problems. The Central limit theorem is also used to argue\nthat the distribution of the mini-batch gradient is a Gaussian [236], which has been challenged\nonly recently [188, 220]. The observation that the gradient noise is heavy tailed has been\nused to justify the superiority of the Adam optimizer in training attention models [231].\nThere also exists a link between the Fisher [3], Neural Tangent Kernel [88], and the gradient\ncovariance matrix [131, 106, 200]. As such, any analysis of one [e.g., 97] could potentially\nbe used to understand others.\nVariance reduction as part of optimization methods for deep learning. Variance\nreduction is an important technique in gradient estimation at the core of many machine\nlearning problems [137]. A variance reduced gradient estimator can then be used with a\ngradient-based optimization method. In such approaches, the performance of the optimizer\ndepends on the gradient estimator but there is no feedback from the optimizer to improve\nthe gradient estimator. Our work is related to variance reduction methods that directly\nmodify the optimization method. [109] considered the difference between the covariance\nmatrix of the gradients and the Fisher matrix and proposed incorporating the covariance\nmatrix as a measure of model uncertainty in optimization. It has also been suggested\nthat the division by the second moments of the gradient in Adam can be interpreted as\nvariance adaptation [106]. Although we do not use Gradient Clustering for optimization, the\nformulation can be interpreted as a unifying approach that deﬁnes variance reduction as an\nobjective.\nImportance Sampling for Optimization. Our work is also closely related to impor-\ntance sampling for stochastic optimization where data points are sampled according to a\nmeasure of importance such as the loss or the norm of the gradient [233, 99, 37, 96, 2, 148].\nThere are also myriad papers on ad-hoc sampling and re-weighting methods for reducing\ndataset imbalance and increasing data diversity [13, 94, 212, 93]. Based on empirical results,\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING45\n[221] suggests that several sampling and ordering methods have only marginal beneﬁts\non standard datasets when training time is long enough. Our method can be viewed as an\nimportance sampling method where the relative size of the cluster denotes the importance of\nits data points. Compared with using only gradient norm or the loss for importance sampling,\nwe exploit the entire gradient vector for each data point that makes the method signiﬁcantly\nmore powerful.\nClustering gradients. Methods related to gradient clustering have been proposed in\nlow-variance gradient estimation [85, 232, 27] supported by promising theory. However,\nthese methods have either limited their experiments to linear models or treated a deep model\nas a linear one. Our proposed GC method performs efﬁcient clustering in the gradient space\nwith very few assumptions. GC is also related to works on model visualization where the\nentire training set is used to understand the behaviour of a model [160].\n4.3\nMini-batch Gradient with Stratiﬁed Sampling\nAn important factor affecting optimization trade-offs is the diversity of training data. SGD\nentails a sampling process, often uniformly sampling from the training set. However, as\nillustrated in the following example, uniform sampling is not always ideal. Suppose there\nare duplicate data points in a training set. We can save computation time by removing all\nbut one of the duplicates. To get the same gradient mean in expectation, it is sufﬁcient to\nrescale the gradient of the remaining sample in proportion to the number of duplicates. In\nthis example, mini-batch SGD will be inefﬁcient because duplicates increase the variance of\nthe average gradient mean.\nSuppose we are given i.i.d. training data, {xi}N\ni=1, and a partitioning of their gradients,\ngi =\n∂\n∂θℓ(xi; θ), into K clusters, where Nk is the size of the k-th cluster. We can estimate\nthe gradient mean on the training set, g =\n∂\n∂θL(θ), by averaging K gradients, one from\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING46\neach of K clusters, uniformly sampled:\nˆg(a) = 1\nN\nK\nX\nk=1\nNk g(k) ,\ng(k) ∼U(Sk) ,\n(4.1)\nwhere g(k) is a uniformly sampled gradient from the k-th cluster Sk = {gi|ai = k}, and\na ∈{1, . . . , K}N where ai is the index of the cluster to which i-th data point is assigned, so\nNk = PN\ni=1I(ai = k). Each sample is treated as a representative of its cluster and weighted\nby the size of that cluster. In the limit of K = N, we recover the batch gradient mean used\nin GD and for K = 1 we recover the single-sample stochastic gradient in SGD.\nProposition 4.3.1. (Bias/Variance of Mini-batch Gradient with Stratiﬁed Sampling).\nFor any partitioning of data, the estimator of the gradient mean using stratiﬁed sampling\n(Eq. (4.1)) is unbiased (E[ˆg] = g) and V[ˆg] = N−2PK\nk=1Nk2V[g(k)], where V[·] is deﬁned\nas the trace of the covariance matrix. (Proof in Appendix A.1.1)\nRemark. Under a stratiﬁed sampling scheme, in a dataset with duplicate samples, the\ngradients of duplicates do not contribute to the variance if assigned to the same partition\nwith no other data points.\n4.3.1\nWeighted Gradient Clustering\nSuppose, for a given number of clusters, K, we want to ﬁnd the optimal partitioning, i.e., one\nthat minimizes the variance of the gradient mean estimator, ˆg. For d-dimensional gradient\nvectors, minimizing the variance in Proposition 4.3.1, is equivalent to ﬁnding a weighted\nclustering of the gradients of data points,\nmin\na V[ˆg(ai)] = min\na\nK\nX\nk=1\nNk2V[g(k)] = min\nC,a\nK\nX\nk=1\nN\nX\ni=1\nNk ∥Ck −gi∥2 I(ai = k) ,\n(4.2)\nwhere a cluster center, Ck ∈Rd, is the average of the gradients in the k-th cluster, and\nV[g(k)] =\n1\nNk\nPN\ni=1∥Ck −gi∥2 I(ai = k). If we did not have the factor Nk, this objective\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING47\nwould be equivalent to the K-Means objective. The additional Nk factors encourage larger\nclusters to have lower variance, with smaller clusters comprising scattered data points.\nIf we could store the gradients for the entire training set, the clustering could be per-\nformed iteratively as a form of block coordinate descent, alternating between the following\nAssignment and Update steps, i.e., computing the cluster assignments and then the cluster\ncenters:\nA :\nai = arg min\nk\nNk ∥Ck −gi∥2\n(4.3)\nU :\nCk = 1\nNk\nN\nX\ni=1\ngi I(ai = k)\n(4.4)\nThe A step is still too complex given the Nk multiplier. As such, we ﬁrst solve it for ﬁxed\ncluster sizes then update Nk before another U step. These updates are similar to Lloyd’s\nalgorithm for K-Means, but with the Nk multipliers, and to Expectation-Maximization for\nGaussian Mixture Models, but here we use hard assignments. In contrast, the additional\nNk multiplier makes the objective more complex in that performing AU updates does not\nalways guarantee a decrease in the clustering objective.\n4.3.2\nEfﬁcient Gradient Clustering (GC)\nPerforming exact AU updates (Eqs. (4.3) and (4.4)) is computationally expensive as they\nrequire the gradient of every data point. Deep learning libraries usually provide efﬁcient\nmethods that compute average mini-batch gradients without ever computing full individual\ngradients. We introduce Gradient Clustering (GC) for performing efﬁcient AU updates by\nbreaking them into per-layer operations and introducing a low-rank approximation to cluster\ncenters.\nFor any feed-forward network, we can decompose terms in AU updates into independent\nper-layer operations as shown in Fig. 4.2. The main operations are computing ∥Ckl −gil∥2\nand cluster updates Cai,l += gil/Nai per layer l; henceforth, we drop the layer index for\nsimplicity.\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING48\nAlgorithm 1 A step using\nEq. (4.5)\nfor i = 1 to N do\nfor k = 1 to K do\nfor l = 1 to L do\nDkl = ∥Ckl −gil∥2\nS = P\nl D·l\nai = arg mink NkS\nAlgorithm 2 Nk update\nNk\n=\n0,\n∀k\n=\n1, · · · , K\nfor i = 1 to N do\nNai += 1\nAlgorithm 3 U step using\nEq. (4.6)\nCk = 0,\n∀k = 1, · · · , K\nfor i = 1 to N do\nfor l = 1 to L do\nCai,l += gil/Nai\nFigure 4.2: Gradient Clustering Algorithm steps applied on the gradients of a deep neural\nnetwork with L layers. gil denotes the gradients of the i-th example w.r.t. the parameters of\nthe l-th layer, Ckl denotes k-th cluster center for the l-th layer, ai denotes the assignment\nindex for the i-th data point and Nk denotes the size of the k-th cluster.\nFor a single fully-connected layer, we denote the layer weights by θ ∈RI×O, where\nI and O denote the input and output dimensions for the layer. We denote the gradient\nwith respect to θ for the training set by g = AD⊤, where A ∈RI×N comprises the\ninput activations to the layer, and D ∈RO×N represents the gradients with respect to the\nlayer outputs. The coordinates of cluster centers corresponding to this layer are denoted by\nC ∈RK×I×O. We index the clusters using k and the data by i. The k-th cluster center is\napproximated as Ck = ckdk⊤, using vectors ck ∈RI and dk ∈RO.\nIn the A step we need to compute ∥Ck −gi∥2\nF as part of the assignment cost, where\n∥· ∥F is the Frobenius-norm. We expand this term into three inner-products, and compute\nthem separately. In particular, the term vec{Ck} ⊙vec{gi} can be written as,\nvec{Ck} ⊙vec{AiDi⊤} = (Ai ⊙ck)(Di ⊙dk) ,\n(4.5)\nwhere ⊙denotes inner product, and the RHS is the product of two scalars. Similarly, we\ncompute the other two terms in the expansion of the assignment cost, i.e., vec{Ck} ⊙\nvec{Ck} and vec{gi} ⊙vec{gi} ([64] proposed a similar idea to compute the gradient\nnorm).\nThe U step in Eq. (4.4) is written as, ckdk⊤= Nk−1PN\ni=1AiDi⊤I(ai = k). This\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING49\nequation might have no exact solution for ck and dk because the sum of rank-1 matrices is\nnot necessarily rank-1. One approximation is the min-Frobenius-norm solution to ck, dk\nusing truncated SVD, where we use left and right singular-vectors corresponding to the\nlargest singular-value of the RHS. However, the following updates are exact if activations\nand gradients of the outputs are uncorrelated, i.e., Ei[AiDi] = Ei[Ai]Ei[Di] (similar to\nassumptions in K-FAC [133]),\nck = 1\nNk\nN\nX\ni=1\nAiI(ai = k)\ndk = 1\nNk\nN\nX\ni=1\nDiI(ai = k) .\n(4.6)\nIn Appendix A.2.1, we describe similar update rules for convolutional layers and in\nAppendix A.2.2, we provide complexity analysis of GC. We can make the cost of GC\nnegligible by making sparse incremental updates to cluster centers using mini-batch updates.\nThe assignment step can also be made more efﬁcient by processing only a portion of data\nas is common for training on large datasets. The rank-1 approximation can be extended to\nhigher rank approximations with multiple independent cluster centers though with challenges\nin the implementation.\n4.4\nExperiments\nIn this section, we evaluate the accuracy of estimators of the gradient mean. This is a\nsurrogate task for evaluating the performance of a model of the gradient distribution. We\nencourage the reader to predict the behaviour of gradient estimators before advancing in this\nsection. In particular, does the reader expect the gradient variance to increase or decrease\nduring the training? Surprisingly, we ﬁnd that the gradient variance often increases in the\nmajority of training.\nWe compare our proposed GC estimator to average mini-batch Stochastic Gradient\n(SG-B), and SG-B with double the mini-batch size (SG-2B). SG-2B is an important baseline\nfor two reasons. First, it is a competitive baseline that always reduces the variance by a factor\nof 2 and requires at most twice the memory size and twice the run-time per mini-batch [181].\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING50\nSecond, the extra overhead of GC is approximately the same as keeping an extra mini-batch\nin the memory when the number of clusters is equal to the mini-batch size. We also include\nStochastic Variance Reduced Gradient (SVRG) [95] as a method with the sole objective of\nestimating gradient mean with low variance.\nWe compare methods on a single trajectory of mini-batch SGD to decouple the opti-\nmization from gradient estimation. That is, we do not train with any of the estimators (hence\nno ‘D’ in SG-B and SG-2B). This allows us to continue analyzing a method even after it\nfails in reducing the variance. For training results using SG-B, SG-2B and, SVRG, we refer\nthe reader to [181, 39]. For training with GC, it sufﬁces to say that behaviours observed in\nthis section are directly related to the performance of GC used for optimization.\nAs all estimators in this work are unbiased, the estimator with lowest variance is better\nestimating the gradient mean. We deﬁne Average Variance (variance in short) as the average\nover all coordinates of the variance of the gradient mean estimate for a ﬁxed model snapshot.\nAverage variance is the normalized trace of the covariance matrix and of particular interest\nin random matrix theory [198].\nWe also measure Normalized Variance, deﬁned as V[g]/E[g2] where the variance of\na 1-dimensional random variable is divided by its second non-central moment. In signal\nprocessing, the inverse of this quantity is the signal to noise ratio (SNR). If SNR is less than\none (normalized variance larger than one), the power of the noise is greater than the signal.\nNormalized variance also appears in standard convergence analysis of stochastic gradient\ndescent [55]. As we show in Appendix A.4, it can be shown that for Lipschitz continuous\nfunctions, variance reduction is only useful if the normalized variance is larger than constant\n1. As such, it better correlates with the convergence speed compared with variance.\nAdditional details of the experimental setup can be found in Appendix A.3.\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING51\n0\n1\n2\n3\n4\n5\nTraining Iteration\n1e4\n0\n1\n2\n3\n4\n5\nAvg. Variance\n1e\n8\nSG-B\nSG-2B\nSVRG\nGC\n(a) MLP on MNIST\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0\n1\n2\n3\n4\nAvg. Variance\n1e\n4\n(b) ResNet8 on CIFAR-10\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0\n2\n4\n6\n8\nAvg. Variance\n1e\n7\n(c) ResNet18 on ImageNet\n0\n1\n2\n3\n4\n5\nTraining Iteration\n1e4\n0.6\n0.7\n0.8\n0.9\nAvg. Normalized Variance\n(d) MLP on MNIST\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.6\n0.7\n0.8\n0.9\n1.0\nAvg. Normalized Variance\n(e) ResNet8 on CIFAR-10\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Variance\n(f) ResNet18 on ImageNet\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.0\n0.5\n1.0\n1.5\nAvg. Gradient Norm\n1e\n4\nimagenet\ncifar100\nmnist\ncifar10\n(g) Gradient Norm\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.00\n0.25\n0.50\n0.75\n1.00\nAvg. Variance\n1e\n4\n(h) Gradient Variance\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Variance\n(i) Gradient Normalized Vari-\nance\nFigure 4.3: Image classiﬁcation models. Variance (top) and normalized variance plots\n(middle). Bottom plots compare gradient statistics for SGD across datasets. We observe:\nnormalized variance correlates with optimization difﬁculty, variance is decreasing on MNIST\nbut increasing on CIFAR-10 and ImageNet, and variance ﬂuctuates with GC on CIFAR-10.\n4.4.1\nMNIST: Low Variance, CIFAR-10: Noisy Estimates, ImageNet: No\nStructure\nIn this section, we study the evolution of gradient variance during training of an MLP on\nMNIST [111], ResNet8 [78] on CIFAR-10 [105], and ResNet18 on ImageNet [40]. Curves\nshown are from a single run and statistics are smoothed out over a rolling window. The\nstandard deviation within the window is shown as a shaded area.\nNormalized variance correlates with the time required to improve accuracy. In\nFigs. 4.3a to 4.3c, the variance of SG-2B is always half the variance of SG-B. A drawback of\nthe variance is that it is not comparable across different problems. For example, on CIFAR-\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING52\n10 the variance of all methods reaches 10−4 while on ImageNet where usually 10× more\niterations are needed, the variance is below 10−6. In contrast, as we show by convergence\nanalysis in Appendix A.4, for Lipschitz continuous functions, variance reduction is only\nuseful if the normalized variance is larger than constant 1. In Figs. 4.3d to 4.3f, normalized\nvariance better correlates with the convergence speed. Normalized variance on both MNIST\nand CIFAR-10 is always below 1 while on ImageNet it quickly goes above 1 (noise stronger\nthan gradient). Notice that the denominator in the normalized variance is shared between all\nmethods on the same trajectory of mini-batch SGD. As such, the normalized variance retains\nthe relation of curves and is a scaled version of variance where the scaling varies during\ntraining as the norm of the gradient changes. For clarity, we only show the curve for SG-B.\nHow does the difﬁculty of optimization change during training? The variance on\nMNIST for all methods is constantly decreasing (Fig. 4.3a), i.e., the strength of noise\ndecreases as we get closer to a local optima. These plots suggest that training an MLP on\nMNIST satisﬁes the Strong Growth Condition (SGC) [177] as the variance is numerically\nzero (below 10−8). Normalized variance (Fig. 4.3d) decreases over time and is well below\n1 (gradient mean has larger magnitude than the variance). SVRG performs particularly\nwell by the end of the training because the training loss has converged to near zero (cross-\nentropy less than 0.005). Promising published results with SVRG are usually on datasets\nsimilar to MNIST where the loss reaches relatively small values. In contrast, on both\nCIFAR-10 (Figs. 4.3b and 4.3e) and ImageNet (Figs. 4.3c and 4.3f), the variance and\nnormalized variance of all methods increase from the beginning for almost the entire training\nand especially after the learning rate drops. This means gradient variance depends on the\ndistance to local optima. We hypothesize that the gradient of each training point becomes\nmore unique as training progresses. Models we considered do not reach zero training loss\nwithin the given training time. If we increase the model size and train long enough and\ndecrease the learning rate, eventually the gradient variance decreases to zero.\nVariance can widely change during training but it happens only on particularly\nnoisy data. On CIFAR-10, the variance of GC suddenly goes up but comes back down before\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING53\nany updates to the cluster centers (Fig. 4.3b) while the variance of SVRG monotonically\nincreases between updates. To explain these behaviours, notice that immediately after cluster\nupdates, GC and SVRG should always have at most the same average variance as SG-B. We\nobserved this behaviour consistently across different architectures such as other variations of\nResNet and VGG on CIFAR-10. Fig. 4.6 shows the effect of adding noise on CIFAR-10.\nLabel smoothing [197] reduces ﬂuctuations but not completely. On the other hand, label\ncorruption, where we randomly change the labels for 10% of the training data eliminates\nthe ﬂuctuations. We hypothesize that the model is oscillating between different states with\nsigniﬁcantly different gradient distributions. The experiments with corrupt labels suggest\nthat mislabeled data might be the cause of ﬂuctuations such that having more randomness in\nthe labels forces the model to ignore originally mislabeled data.\nIs the gradient distribution clustered in any dataset? The variance of GC on MNIST\n(Fig. 4.3a) is consistently lower than SG-2B which means it is exploiting clustering in the\ngradient space. On CIFAR-10 (Fig. 4.3b) the variance of GC is lower than SG-B but not\nlower than SG-2B except when ﬂuctuating. The improved variance is more noticeable when\ntraining with corrupt labels. On ImageNet (Figs. 4.3c and 4.3f), the variance of GC is\noverlapping with SG-B. An example of a gradient distribution where GC is overlapping with\nSG-B is a uniform distribution.\nCan GC speed-up training? Although on MNIST the variance is reduced using GC,\nthere improvements on CIFAR-10 and ImageNet are inconsistent. As such, GC does not\nimprove the convergence speed on CIFAR-10 and ImageNet. On CIFAR-10, we considered\nincreasing the frequency of updates to the sampling until the ﬂuctuations disappear. To\nremove all ﬂuctuations, the frequency of updates after the ﬁrst learning rate drop has to be\nless than every 100 optimization steps which increases the overall wall-clock time of the\nmethod unless the assignment step is performed fast in parallel with a distributed system.\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING54\n0\n2\n4\n6\n8\n10\nhs/N\n10\n9\n10\n7\n10\n5\n10\n3\n10\n1\nMax Avg. Variance\nSG,LR=0.01\nSG,LR=0.001\nSG,LR=0.1\n(a) 3 SGD trajectories\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\n10\n1\nMax Avg. Variance\nSG-B\nSG-2B\nSVRG\nGC\n(b) Trajectory of LR=0.001\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\n10\n1\nMax Avg. Variance\n(c) Trajectory of LR=0.01\nFigure 4.4: Random Features models. Variance (log-scale) versus the over-parametrization\ncoefﬁcient (student’s hidden divided by the training set size). We observe: teacher’s hidden\nis not inﬂuential, variance is low in overparametrized regime, and with larger learning rates.\nWe aggregate results from hyper-parameters not shown.\n4.4.2\nRandom Features Models: How Does Overparametrization Affect the\nVariance?\nThe Random Features (RF) model [162] provides an effective way to explore the behaviour\nof optimization methods across a family of learning problems. The RF model facilitates\nthe discovery of optimization behaviours including the double-descent shape of the risk\ncurve [77, 136]. We train a student RF model with hidden dimensions hs on a ﬁxed\ntraining set, (xi, yi) ∈RI × {±1}, i = 1, . . . , N, sampled from a model, xi ∼N(0, I),\nyi = sign(σ(x⊤\ni ˆ\nθ1)⊤ˆ\nθ2 + b) where σ is the ReLU activation function, and the teacher\nhidden features ˆ\nθ1 ∈RI×ht, and second layer weights and bias, ˆ\nθ2 ∈Rht×1, b ∈R, are\nsampled from the standard normal distribution. Each I dimensional random feature of the\nteacher is scaled to ℓ2 norm 1. We train a student RF model with random features θ1 ∈RI×hs\nand second layer weights θ2 ∈Rhs×1 by minimizing the cross-entropy loss. In Fig. 4.4, we\ntrain hundreds of Random Features models and plot the average variance and normalized\nvariance of gradient estimators. We show both maximum and mean of the statistics during\ntraining. The maximum better captures ﬂuctuations of a gradient estimator and allows us to\nlink our observations of variance to generalization using standard convergence bounds that\nrely on bounded noise [18].\nDo models with small generalization gap converge faster? Based on small error bars,\nthe only hyper-parameters that affect the variance are learning rate and the ratio of the size\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING55\n0\n2\n4\n6\n8\n10\nhs/N\n0.5\n1.0\n1.5\n2.0\nMax Avg. Normalized Variance\nSG-B,LR=0.01\nSG-B,LR=0.001\nSG-B,LR=0.1\n(a) SGD max norm. var.\n0\n2\n4\n6\n8\n10\nhs/N\n0.5\n1.0\n1.5\n2.0\nMean Avg. Normalized Variance\n(b) SGD mean norm. var.\nFigure 4.5: Normalized vari-\nance on overparam. RF is\nless than 1.\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n1\n0\n1\n2\n3\n4\nAvg. Variance\n1e\n4\nSG-B\nSG-2B\nSVRG\nGC\n(a) Label smoothing\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0\n2\n4\n6\n8\nAvg. Variance\n1e\n5\n(b) Corrupt labels\nFigure 4.6: CIFAR-10 Fluc-\ntuations disappear with cor-\nrupt labels.\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Variance\n1e\n4\nSG-B\nSG-2B\nSVRG\nGC\n(a) CIFAR-10\n0\n2\n4\n6\n8\nTraining Iteration\n1e4\n0.0\n0.5\n1.0\n1.5\n2.0\nAvg. Variance\n1e\n5\n(b) CIFAR-100\nFigure 4.7: Image classiﬁ-\ncation with duplicates ex-\nploited by GC.\nof the student hidden layer over the training set size. In contrast, in analysis of risk and the\ndouble descent phenomena, we usually observe a dependence on ratio of the student hidden\nlayer size to the teacher hidden layer size [136]. This suggests that models that generalize\nbetter are not necessarily ones that train faster.\nDoes “diminishing returns” happen because of overparametrization? Figs. 4.4b\nand 4.4c show that with the same learning rate, all methods achieve similar variance in\nthe overparametrized regime. Note that due to the normalization of random features, the\ngradients in each coordinate are expected to decrease as overparametrization increases.\nWe conjecture that the diminishing returns in increasing the mini-batch size should also\nbe observed in overparametrized random features models similar to linear and deeper\nmodels [229, 181].\nHow does the variance change as learning rate varies? Fig. 4.4a shows that the\nvariance is smaller for trajectories with larger learning rates and that the gap grows as\noverparametrization grows. This is a direct consequence of the dependence of the noise\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING56\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\nMax Avg. Variance\nSG-B\nSG-2B\nSVRG\nGC\n(a) 10% duplicates\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\nMax Avg. Variance\n(b) 50% duplicates\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\nMax Avg. Variance\n(c) 90% duplicates\nFigure 4.8: Training RF Models with Duplicates. GC identiﬁes and exploits duplicates.\nPlots are similar to Fig. 4.4. Learning rate in all three is 0.01. In each training, there are 5\ndata points that are repeated equally to make up 10% (left), 50% (middle), and 90% (right)\nof the training set.\nin the gradient on current parameters. In Section 4.4.1 we observe the opposite of this\nbehaviour in deep models. In contrast, Fig. 4.5 shows that for overparametrization less than\n5, all trajectories have similar normalized variance that is larger than one (noise is more\npowerful than the gradient).\n4.4.3\nDuplicates: Back to the Motivation for Gradient Clustering\nIn Fig. 4.8, we trained random features models with additional duplicated data points.\nWe observe that as the ratio of duplicates to non-duplicates increases, the gap between\nthe variance of GC and other methods improve. Without duplicate data, GC is always\nbetween SG-B and SG-2B. It is almost never worse than SG-B and never better than\nSG-2B. GC is as good as SG-2B at mild overparametrization (1 −4). We need a degree of\noverparametrization for GC to reduce the variance but too much overparametrization leaves\nno room for improvement. When duplicates exist, GC performs well with a gap that does\nnot decrease by overparametrization.\nSimilarly, experiments on CIFAR-10 and CIFAR-100 (Fig. 4.7) show that GC signiﬁ-\ncantly reduces the variance when duplicate data points exist. In this experiment, 10 training\npoints are selected randomly and duplicated 10000 times that dominate the training. As the\ntraining set size is 50000, the ratio of redundant points to the original data is 20×. Note that\nbecause of common data augmentations, duplicate data points are not exactly duplicate in\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING57\nthe input space and there is no guarantee that their gradients would be similar.\n4.5\nConclusion\nIn this chapter, we introduced an efﬁcient gradient clustering method and showed that\nstratiﬁed sampling based on the gradient clusters induces a low variance gradient estimator.\nWe observed challenges in employing gradient clustering for optimization. To investigate,\nwe studied the variance of the gradients for various optimization trajectories on standard\nbenchmarks.\nIn the following, we summarize the hypotheses, results, and future work:\n• We proved that stratiﬁed sampling based on a weighted clustering in the gradient space\nminimizes the variance of a mini-batch gradient estimator for a constant distribution\nof Gradients (Section 4.3). We designed an efﬁcient gradient clustering method with a\nper-iteration computational cost comparable to a single back-prop.\n• We demonstrated the success of gradient clustering in reducing the gradient variance\non MNIST as well as in the presence of redundancy such as duplicate data (Sec-\ntions 4.4.1 and 4.4.3). Future work could consider detecting and exploiting other types\nof redundancy and within-class imbalances using gradient clustering.\n• We provided preliminary evidence and justiﬁcation for the correlation between the\nnormalized gradient variance and convergence speed of SGD (Section 4.4 and Ap-\npendix A.4). On MNIST, the normalized gradient variance decreases during training\nwhile it is increasing on CIFAR-10 and ImageNet for the models we tested. Fu-\nture work could verify this correlation empirically for more model architectures and\ndatasets. If veriﬁed, future work can study optimization methods such as stratiﬁed\nsampling with gradient clustering to minimize the normalized variance instead of the\nvariance of gradients.\n• We observed that the distribution of gradient changes signiﬁcantly during the training\nCHAPTER 4. GRADIENT CLUSTERING AND A STUDY OF GRADIENT VARIANCE IN DEEP LEARNING58\nof CIFAR-10 models studied in this chapter. This observation does not challenge\nstandard optimization methods but consistently impedes our proposed stratiﬁed sam-\npling method. We hypothesized that a small subset of the training set such as a few\nmislabeled data might be responsible for the sudden changes in gradient clusters.\nIn our follow-up work (not included in this thesis), we observed that the mean and\nvariance of the normalized gradient do not change signiﬁcantly during the training [49].\nThis observation aligns with our hypothesis. Future work can test our hypothesis\nby comparing the gradient cluster centers which can provide a method for detecting\nmislabeled or ambiguous data in the training set.\n• We sought an answer for the question \"Is there structure in the gradient space?\"\nby applying weighted gradient clustering and measuring the objective, i.e., gradient\nvariance. On MNIST and CIFAR-10, we successfully reduced the objective by\nperforming the efﬁcient but approximate clustering algorithm. This suggests that a\nclustered structure exists in the gradient space for the studied models on these datasets.\nIn contrast, on ImageNet the variance is not reduced which implies a lack of clustered\nstructure in the gradient space. This hypothesis is not fully tested and requires future\nwork. For example, clustering with no weighting can be tried as well as whether the\nimpact of the approximation error in efﬁcient gradient clustering operations. It is also\npossible that alternative distance metrics to Euclidean distance would induce clusters\nthat could be found by a modiﬁed gradient clustering method.\nChapter 5\nBridging the Gap Between\nAdversarial Robustness and\nOptimization Bias\nThe third and last problem studied in this thesis concerns adversarial robustness in deep\nlearning. We hypothesize that robustness depends on the explicit training mechanisms and\ntheir implicit biases. The implication is that optimization choices not only impact the speed\nof training, computational resource requirements, and generalization performance but also\nindirectly affect the security and robustness of the models.\nWe demonstrate that the choice of optimizer, neural network architecture, and regularizer\nsigniﬁcantly affect the adversarial robustness of linear neural networks, providing guarantees\nwithout the need for adversarial training. To this end, we revisit a known result linking\nmaximally robust classiﬁers and minimum norm solutions, and combine it with recent results\non the implicit bias of optimizers. First, we show that, under certain conditions, it is possible\nto achieve both perfect standard accuracy and a certain degree of robustness, simply by\ntraining an overparametrized model using the implicit bias of the optimization. In that\nregime, there is a direct relationship between the type of the optimizer and the attack to\n59\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS60\nwhich the model is robust. To the best of our knowledge, this work is the ﬁrst to study\nthe impact of optimization methods such as sign gradient descent and proximal methods\non adversarial robustness. Second, we characterize the robustness of linear convolutional\nmodels, showing that they resist attacks subject to a constraint on the Fourier-ℓ∞norm. To\nillustrate these ﬁndings we design a novel Fourier-ℓ∞attack that ﬁnds adversarial examples\nwith controllable frequencies. We evaluate Fourier-ℓ∞robustness of adversarially-trained\ndeep CIFAR-10 models from the standard RobustBench benchmark and visualize adversarial\nperturbations.\nThe content of this chapter have appeared in the following publication:\n• Faghri, Fartash and Gowal, Sven and Vasconcelos, Cristina and Fleet, David J. and\nPedregosa, Fabian and Le Roux, Nicolas, “Bridging the Gap Between Adversarial\nRobustness and Optimization Bias\", Workshop on Security and Safety in Machine\nLearning Systems, International Conference on Learning Representations (ICLR),\n2021.\nTo ensure reproducibility, our code is publicly available 1.\n5.1\nIntroduction\nDeep neural networks achieve high accuracy on standard test sets, yet [196] showed that\nany natural input correctly classiﬁed by a neural network can be modiﬁed with adversar-\nial perturbations. Such perturbations fool the network into misclassiﬁcation, even when\nthey are constrained to be imperceptible to humans. Adversarial training improves model\nrobustness by augmenting the training set with adversarial perturbations [67] and can be\ninterpreted as approximately solving a saddle-point problem [126]. Adversarial training is\nthe state-of-the-art approach to adversarial robustness [70, 36] and alternative approaches\nare more likely to exhibit spurious robustness [203]. Nevertheless, adversarial training\nis computationally expensive compared to standard training, as it involves an alternating\n1\nhttps://github.com/fartashf/robust_bias\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS61\noptimization. Adversarial training also exhibits a trade-off between standard generalization\nand adversarial robustness. That is, it achieves improved robust accuracy, on adversarially\nperturbed data, at the expense of standard accuracy, the probability of correct predictions on\nnatural data [204]. This adversarial robustness trade-off has been shown to be intrinsic in a\nnumber of toy examples [52], independent of the learning algorithm in some cases [176].\nAlternatives to adversarial training have been proposed to reduce this trade-off, but a gap\nremains in practice [230].\nHere we consider connections between the adversarial robustness trade-off and optimiza-\ntion biases in training overparametrized models. Deep learning models can often achieve\ninterpolation, i.e., they have the capacity to exactly ﬁt the training data [227]. Their abil-\nity to generalize well in such cases has been attributed to an implicit bias toward simple\nsolutions [72, 77].\nOur main contribution is to connect two large bodies of work on adversarial robustness\nand optimization bias. Focusing on models that achieve interpolation, we use the formulation\nof a Maximally Robust Classiﬁer from robust optimization [12]. We theoretically demonstrate\nthat the choice of optimizer (Corollary 1), neural network architecture (Corollary 2), and\nregularizer (Corollary 3), signiﬁcantly affect the adversarial robustness of linear neural\nnetworks. Even for linear models, the impact of these choices had not been characterized\nprecisely prior to our work. We observe that, in contrast to adversarial training, under certain\nconditions we can ﬁnd maximally robust classiﬁers at no additional computational cost.\nBased on our theoretical results on the robustness of linear convolutional models to\nFourier attacks, we introduce a new class of attacks in the Fourier domain. In particular, we\ndesign the Fourier-ℓ∞attack and illustrate our theoretical results. Extending to non-linear\nmodels, we attack adversarially-trained deep models on CIFAR-10 from the RobustBench\nbenchmark [36] and ﬁnd low and high frequency adversarial perturbations by directly\ncontrolling spectral properties through Fourier constraints. This example demonstrates how\nunderstanding maximal robustness of linear models is a stepping stone to understanding and\nguaranteeing robustness of non-linear models.\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS62\n5.2\nNo Trade-offs with Maximally Robust Classiﬁers\nWe start by deﬁning adversarial robustness and the robustness trade-off in adversarial\ntraining. Then, Section 5.2.1 provides an alternative formulation to adversarial robustness\nthat avoids the robustness trade-off. Let D = {(xi, yi)}n\ni=1 denote a training set sampled\ni.i.d. from a distribution, where xi ∈Rd are features and yi ∈{−1, +1} are binary labels. 2\nA binary classiﬁer is a function ϕ : Rd →R, and its prediction on an input x is given by\nsign(ϕ(x)) ∈{−1, +1}. The aim in supervised learning is to ﬁnd a classiﬁer that accurately\nclassiﬁes the training data and generalizes to unseen test data. One standard framework\nfor training a classiﬁer is Empirical Risk Minimization (ERM), arg minϕ∈Φ L(ϕ), where\nL(ϕ) := E(x,y)∼D ζ(yϕ(x)), Φ is a family of classiﬁers, and ζ : R →R+ is a loss function\nthat we assume to be strictly monotonically decreasing to 0, i.e., ζ′ < 0. Examples are the\nexponential loss, exp (−ˆyy), and the logistic loss, log (1 + exp (−ˆyy)), where ˆy, y are the\nmodel prediction and the ground-truth label.\nGiven a classiﬁer, an adversarial perturbation δ ∈Rd is any small perturbation that\nchanges the model prediction, i.e., sign(ϕ(x + δ)) ̸= sign(ϕ(x)), ∥δ∥≤ε, where ∥· ∥\nis a norm on Rd, and ε is an arbitrarily chosen constant. It is common to use norm-\nball constraints to ensure perturbations are small (e.g., imperceptible in images) but other\nconstraints exist [20]. Commonly used are the ℓp norms, ∥v∥p =\n\u0010Pd−1\ni=0 [v]p\ni\n\u00111/p\n, where\n[v]i denotes the i-th element of a vector v, for i = 0, . . . , d −1. In practice, an adversarial\nperturbation, δ, is found as an approximate solution to the following optimization problem,\nmax\nδ:∥δ∥≤ε ζ(yϕ(x + δ)) .\n(5.1)\nUnder certain conditions, closed form solutions exist to the optimization problem in (5.1).\nFor example, [67] observed that the maximal ℓ∞-bounded adversarial perturbation against\na linear model (i.e., one causing the maximum change in the output) is the sign gradient\n2\nWe restrict our theoretical analysis to binary classiﬁcation but we expect direct extensions to multi-class\nclassiﬁcation.\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS63\ndirection scaled by ε.\n[126] deﬁned an adversarially robust classiﬁer as the solution to the saddle-point opti-\nmization problem,\narg min\nϕ∈Φ\nE(x,y)∼D max\nδ:∥δ∥≤ε ζ(yϕ(x + δ)) .\n(5.2)\nThe saddle-point adversarial robustness problem is the robust counter-part to empirical\nrisk minimization where the expected loss is minimized on worst-case adversarial samples\ndeﬁned as solutions to (5.1). Adversarial Training [67] refers to solving (5.2) using an\nalternated optimization. It is computationally expensive because it often requires solving\n(5.1) many times.\nThe main drawback of deﬁning the adversarially robust classiﬁer using (5.2), and a\ndrawback of adversarial training, is that the parameter ε needs to be known or tuned. The\nchoice of ε controls a trade-off between standard accuracy on samples of the dataset D\nversus the robust accuracy, i.e., the accuracy on adversarial samples. At one extreme ε = 0,\nwhere (5.2) reduces to ERM. At the other, as ε →∞, all inputs in Rd are within the ε-ball of\nevery training point and can be an adversarial input. The value of the inner max in (5.2) for\na training point x, y is the loss of the most conﬁdent prediction over Rd that is predicted as\n−y. For large enough ε, the solution to (5.2) is a classiﬁer predicting the most frequent label,\ni.e., ϕ(·) = p∗, where p∗is the solution to arg minp n−1ζ(−p) + n+1ζ(p), and n−1, n+1\nare the number of negative and positive training labels.\nRobust accuracy is often a complementary generalization metric to standard test accuracy.\nIn practice, we prefer a classiﬁer that is accurate on the test set, and that additionally, achieves\nmaximal robustness. The saddle-point formulation makes this challenging without the\nknowledge of the maximal ε. This trade-off has been studied in various works [204, 230, 51,\n52, 176]. Regardless of the trade-off imposed by ε, adversarial training is considered to be\nthe state-of-the-art for adversarial robustness. The evaluation is based on the robust accuracy\nachieved at ﬁxed ε’s even though the standard accuracy is usually lower than a comparable\nnon-robust model [36, 70].\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS64\n5.2.1\nMaximally Robust Classiﬁer\nIn order to avoid the trade-off imposed by ε in adversarial robustness, we revisit a deﬁnition\nfrom robust optimization.\nDeﬁnition 5.2.1. A Maximally Robust Classiﬁer ([12]) is a solution to\narg max\nϕ∈Φ\n{ε | yiϕ(xi + δ) > 0, ∀i, ∥δ∥≤ε} .\n(5.3)\nCompared with the saddle-point formulation (5.2), ε in (5.3) is not an arbitrary constant.\nRather, it is maximized as part of the optimization problem. Moreover, the maximal ε in this\ndeﬁnition does not depend on a particular loss function. Note, a maximally robust classiﬁer\nis not necessarily unique.\nThe downside of (5.3) is that the formulation requires the training data to be separable\nso that (5.3) is non-empty, i.e., there exists ϕ ∈Φ such that ∀i, yiϕ(xi) > 0. In most deep\nlearning settings, this is not a concern as models are large enough that they can interpolate\nthe training data, i.e., for any dataset there exists ϕ such that ϕ(xi) = yi. An alternative\nformulation is to modify the saddle-point problem and include an outer maximization on\nε by allowing a non-zero slack loss. However, the new slack loss reimposes a trade-off\nbetween standard and robust accuracy (See Appendix B.1).\nOne can also show that adversarial training, i.e., solving the saddle-point problem\n(5.2), does not necessarily ﬁnd a maximally robust classiﬁer. To see this, suppose we are\ngiven the maximal ε in (5.3). Further assume the minimum of (5.2) is non-zero. Then the\ncost in the saddle-point problem does not distinguish between the following two models:\n1) a model that makes no misclassiﬁcation errors but has low conﬁdence, i.e., ∀i, 0 <\nmaxδ yiϕ(xi + δ) ≤c1 for some small c1 2) a model that classiﬁes a training point, xj,\nincorrectly but is highly conﬁdent on all other training data and adversarially perturbed\nones, i.e., ∀i ̸= j, 0 < c2 < maxδ yiϕ(xi + δ). The second model can incur a loss\nnζ(c1) −(n −1)ζ(c2) on xj while being no worse than the ﬁrst model according to the cost\nof the saddle-point problem. The reason is another trade-off between standard and robust\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS65\naccuracy caused by taking the expectation over data points.\n5.2.2\nLinear Models: Maximally Robust is the Minimum Norm Classiﬁer\nGiven a dataset and a norm, what is the maximally robust linear classiﬁer with respect to\nthat norm? In this section, we revisit a result from [12] for classiﬁcation.\nDeﬁnition 5.2.2 (Dual norm). Let ∥·∥be a norm on Rn. The associated dual norm, denoted\n∥· ∥∗, is deﬁned as ∥δ∥∗= supx{|⟨δ, x⟩| | ∥x∥≤1} .\nDeﬁnition 5.2.3 (Linear Separability). We say a dataset is linearly separable if there exists\nw, b such that yi(w⊤xi + b) > 0 for all i.\nLemma 5.2.1 (Maximally Robust Linear Classiﬁer ([12], §12)). For linear models and\nlinearly separable data, the following problems are equivalent; i.e., from a solution of one, a\nsolution of the other is readily found.\nMaximally robust classiﬁer:\narg max\nw,b\n{ε | yi(w⊤(xi + δ) + b) > 0, ∀i, ∥δ∥≤ε} ,\n(5.4)\nMaximum margin classiﬁer:\narg max\nw,b:∥w∥∗≤1\n{ε | yi(w⊤xi + b) ≥ε, ∀i} ,\n(5.5)\nMinimum norm classiﬁer:\narg min\nw,b\n{∥w∥∗| yi(w⊤xi + b) ≥1, ∀i} .\n(5.6)\nThe expression mini yi(w⊤xi + b)/∥w∥is the margin of a classiﬁer w that is the distance\nof the nearest training point to the classiﬁcation boundary, i.e., the line {v : w⊤v = −b}.\nWe provide a proof for general norms based on [12] in Appendix B.2.1. Each formu-\nlation in Lemma 5.2.1 is connected to a wide array of results that can be transferred to\nother formulations. Maximally robust classiﬁcation is one example of a problem in robust\noptimization that can be reduced and solved efﬁciently. Other problems such as robust\nregression as well as robustness to correlated input perturbations have been studied prior to\ndeep learning [12].\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS66\nOn the other hand, maximum margin and minimum norm classiﬁcation have long been\npopular because of their generalization guarantees. Recent theories for overparametrized\nmodels link the margin and the norm of a model to generalization [77]. Although the tools\nare different, connecting the margin and the norm of a model has also been the basis of\ngeneralization theories for Support Vector Machines and AdaBoost [184, 199]. Maximum\nmargin classiﬁcation does not require linear separability, because there can exist a classiﬁer\nwith ε < 0 that satisﬁes the margin constraints. Minimum norm classiﬁcation is the easiest\nformulation to work with in practice as it does not rely on ε nor δ and minimizes a function\nof the weights subject to a set of constraints.\nIn what follows, we use Lemma 5.2.1 to transfer recent results about minimum norm\nclassiﬁcation to maximally robust classiﬁcation. These results have been the basis for\nexplaining generalization properties of deep learning models [77, 145].\n5.3\nImplicit Robustness of Optimizers\nThe most common approach to empirical risk minimization (ERM) is through gradient-based\noptimization. As we will review shortly, [72] showed that gradient descent, and more\ngenerally steepest descent methods, have an implicit bias towards minimum norm solutions.\nFrom the inﬁnitely many solutions that minimize the empirical risk, we can characterize the\none found by steepest descent. Using Lemma 5.2.1, we show that such a classiﬁer is also\nmaximally robust w.r.t. a speciﬁc norm.\nRecall that ERM is deﬁned as arg minϕ∈Φ L(ϕ), where L(ϕ) = E(x,y)∼Dζ(yϕ(x)).\nHere we assume D is a ﬁnite dataset of size n. For the linear family of functions, we write\nL(w, b). Hereafter, we rewrite the loss as L(w) and use an augmented representation with\na constant 1 dimension. For linearly separable data and overparametrized models (d > n),\nthere exist inﬁnitely many linear classiﬁers that minimize the empirical risk [72]. We will\nﬁnd it convenient to ignore the scaling and focus on the normalized vector w/∥w∥, i.e., the\ndirection of w. We will say that the sequence w1, w2, . . . converges in direction to a vector\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS67\nv if limt→∞wt/∥wt∥= v.\n5.3.1\nSteepest Descent on Fully-Connected Networks\nDeﬁnition 5.3.1 (Steepest Descent). Let ⟨·⟩denote an inner product and ∥· ∥its associated\nnorm, f a function to be minimized, and γ a step size. The steepest descent method\nassociated with this norm ﬁnds\nwt+1 = wt + γ∆wt,\nwhere ∆wt ∈arg min\nv\n⟨∇f(wt), v⟩+ 1\n2∥v∥2 .\n(5.7)\nThe steepest descent step, ∆wt, can be equivalently written as −∥∇f(wt)∥∗gnst, where\ngnst ∈arg min {⟨∇f(wt), v⟩| ∥v∥= 1}. A proof can be found in [19, §9.4].\nRemark.\nFor some p-norms, steepest descent steps have closed form expressions. Gradient\nDescent (GD) is steepest descent w.r.t. ℓ2 norm where −∇f(wt) is a steepest descent step.\nSign gradient descent is steepest descent w.r.t. ℓ∞norm where −∥∇f(wt)∥1 sign(∇f(wt))\nis a steepest descent step. Coordinate Descent (CD) is steepest descent w.r.t. ℓ1 norm where\n−∇f(wt)iei is a steepest descent step (i is the coordinate for which the gradient has the\nlargest absolute magnitude).\nTheorem 5.3.1 (Implicit Bias of Steepest Descent ([72] (Theorem 5))). For any separable\ndataset {xi, yi} and any norm ∥· ∥, consider the steepest descent updates from (5.7) for\nminimizing the empirical risk L(w) (deﬁned in Section 5.2) with the exponential loss,\nζ(z) = exp (−z). For all initializations w0, and all bounded step-sizes satisfying a known\nupper bound, the iterates wt satisfy\nlim\nt→∞min\ni\nyiw⊤\nt xi\n∥wt∥\n=\nmax\nw:∥w∥≤1 min\ni\nyiw⊤xi .\n(5.8)\nIn particular, if a unique maximum margin classiﬁer w∗\n∥·∥= arg maxw:∥w∥≤1 mini yiw⊤xi\nexists, the limit direction converges to it, i.e., limt→∞\nwt\n∥wt∥= w∗\n∥·∥.\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS68\nIn other words, the margin converges to the maximum margin and if the maximum\nmargin classiﬁer is unique, the iterates converge in direction to w∗\n∥·∥. We use this result to\nderive our Corollary 1.\nCorollary 1 (Implicit Robustness of Steepest Descent). For any linearly separable dataset\nand any norm ∥· ∥, steepest descent iterates minimizing the empirical risk, L(w) , satisfying\nthe conditions of Theorem 5.3.1, converge in direction to a maximally robust classiﬁer,\narg max\nw\n{ε | yiw⊤(xi + δ) > 0, ∀i, ∥δ∥∗≤ε} .\nIn particular, a maximally robust classiﬁer against ℓ1, ℓ2, and ℓ∞is reached, respectively,\nby sign gradient descent, gradient descent, and coordinate descent.\nProof. By Theorem 5.3.1, the margin of the steepest descent iterates, mini\nyiw⊤\nt xi\n∥wt∥\n, con-\nverges as t →∞to the maximum margin, maxw:∥w∥≤1 mini yiw⊤xi. By Lemma 5.2.1,\nany maximum margin classiﬁer w.r.t. ∥·∥gives a maximally robust classiﬁer w.r.t. ∥·∥∗.\nCorollary 1 implies that for overparametrized linear models, we obtain guaranteed\nrobustness by an appropriate choice of optimizer without the additional cost and trade-off\nof adversarial training. We note that Theorem 5.3.1 and Corollary 1, characterize linear\nmodels, but do not account for the bias b. We can close the gap with an augmented input\nrepresentation, to include the bias explicitly. Or one could preprocess the data, removing the\nmean before training.\nTo extend Corollary 1 to deep learning models one can use generalizations of Theo-\nrem 5.3.1. For the special case of gradient descent, Theorem 5.3.1 has been generalized to\nmulti-layer fully-connected linear networks and a larger family of strictly monotonically\ndecreasing loss functions including the logistic loss [143, Theorem 2].\n5.3.2\nGradient Descent on Linear Convolutional Networks\nIn this section, we show that even for linear models, the choice of the architecture affects\nimplicit robustness, which gives another alternative for achieving maximal robustness. We\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS69\nuse a generalization of Theorem 5.3.1 to linear convolutional models.\nDeﬁnition 5.3.2 (Linear convolutional network). An L-layer convolutional network with\n1-D circular convolution is parameterized using weights of L −1 convolution layers,\nw1, . . . , wL−1 ∈Rd, and weights of a ﬁnal linear layer, wL ∈Rd, such that the linear\nmapping of the network is\nϕconv(x; w1, . . . , wL) := w⊤\nL(wL−1 ⋆· · · (w1 ⋆x)) .\nHere, circular convolution is deﬁned as [w ⋆x]i :=\n1\n√\nd\nPd−1\nk=0[w]−k[x]i+k, where [v]i\ndenotes the i-th element of a vector v for i = 0, . . . , d −1, and i = i mod d. 3\nA linear convolutional network is equivalent to a linear model with weights w =\nwL ⋆(· · · ⋆(w2 ⋆w1)) because of the associative property of convolution. In particular, for\ntwo-layer linear convolutional networks w = w2 ⋆w1.\nDeﬁnition 5.3.3 (Discrete Fourier Transform). F(w) ∈Cd denotes the Fourier coefﬁcients\nof w where [F(w)]d =\n1\n√\nd\nPd−1\nk=0[w]k exp(−2πj\nd kd) and j2 = −1.\nTheorem 5.3.2 (Implicit Bias towards Fourier Sparsity ([73], Theorem 2, 2.a)). Consider the\nfamily of L-layer linear convolutional networks and the sequence of gradient descent iterates,\nwt, minimizing the empirical risk, L(w), with the exponential loss, exp (−z). For almost\nall linearly separable datasets under known conditions on the step size and convergence\nof iterates, wt converges in direction to the classiﬁer minimizing the norm of the Fourier\ncoefﬁcients given by\narg min\nw1,...,wL\n{∥F(w)∥2/L | yi⟨w, xi⟩≥1, ∀i}.\n(5.9)\nIn particular, for two-layer linear convolutional networks the implicit bias is towards the\nsolution with minimum ℓ1 norm of the Fourier coefﬁcients, ∥F(w)∥1. For L > 2, the\nconvergence is to a ﬁrst-order stationary point.\n3\nWe use the usual deﬁnition of circular convolution in signal processing, rather than cross-correlation,\nw↓⋆x with [v↓]i = [v]−i, which is used in deep learning literature, but not associative.\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS70\nAlgorithm 4 Fourier-ℓ∞Attack (see Appendix B.3)\nInput: data x, label y, loss function ζ, classiﬁer ϕ, perturbation size ε, number of attack\nsteps m, dimensions d, Fourier transform F\nfor k = 1 to m do\nˆg = F(∇xζ(yϕ(x)))\n[δ]i = ε [ˆg]i\n|[ˆg]i|, ∀i ∈{0, . . . , d −1}\nx = x + F−1(δ)\nWe use this result to derive our Corollary 2.\nCorollary 2 (Maximally Robust to Perturbations with Bounded Fourier Coefﬁcients). Con-\nsider the family of two-layer linear convolutional networks and the gradient descent iterates,\nwt, minimizing the empirical risk. For almost all linearly separable datasets under condi-\ntions of Theorem 5.3.2, wt converges in direction to a maximally robust classiﬁer,\narg max\nw1,...,wL\n{ε | yiϕconv(xi + δ; {wl}L\nl=1) > 0, ∀i, ∥F(δ)∥∞≤ε} .\nProof in Appendix B.2.2. Corollary 2 implies that, at no additional cost, linear convolu-\ntional models are already maximally robust, but w.r.t. perturbations in the Fourier domain.\nWe call attacks with ℓp constraints in the Fourier domain Fourier-ℓp attacks. Appendix B.6\ndepicts various norm-balls in 3D to illustrate the signiﬁcant geometrical difference between\nthe Fourier-ℓ∞and other commonly used norm-balls for adversarial robustness. One way to\nunderstand Corollary 2 is to think of perturbations that succeed in fooling a linear convolu-\ntional network. Any such adversarial perturbation must have at least one frequency beyond\nthe maximal robustness of the model. This condition is satisﬁed for perturbations with small\nℓ0 norm in the spatial domain, i.e., only a few pixels are perturbed and similarly by ℓ1 norm\nperturbations as they are constrained to be more sparse than other ℓp norm perturbations.\nSparse perturbations in the spatial domain can be dense in the Fourier domain.\n5.3.3\nFourier Attacks\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS71\nThe predominant motivation for designing new attacks is to fool existing models. In\ncontrast, our results characterize the attacks that existing models perform best against, as\nmeasured by maximal robustness. Based on Corollary 2 we design the Fourier-ℓp attack to\nverify our results. Some adversarial attacks exist with Fourier constraints [206, 74]. [183]\nproposed a Fourier-ℓp attack that includes Fourier constraints in addition to ℓp constraints in\nthe spatial domain. Our theoretical results suggest a more general class of attacks with only\nFourier constraints.\nThe maximal ℓp-bounded adversarial perturbation against a linear model in (5.1) consists\nof real-valued constraints with a closed form solution. In contrast, maximal Fourier-ℓp has\ncomplex-valued constraints. In Appendix B.3 we derive the Fourier-ℓ∞attack in closed\nform for linear models and provide the pseudo-code in Algorithm 4. To ﬁnd perturbations as\nclose as possible to natural corruptions such as blur, ε can be a matrix of constraints that is\nmultiplied elementwise by δ. As our visualizations in Fig. 5.1 show, adversarial perturbations\nunder bounded Fourier-ℓ∞can be controlled to be high frequency and concentrated on subtle\ndetails of the image, or low frequency and global. We observe that high frequency Fourier-ℓ∞\nattacks succeed more easily with smaller perturbations compared with low frequency attacks.\nThe relative success of our band-limited Fourier attacks matches the empirical observation\nthat the amplitude spectra of common ℓp attacks are largely band-limited as such attacks\nsucceed more easily [223].\n5.4\nExplicit Regularization\nAbove we discussed the impact of optimization method and model architecture on robustness.\nHere, we discuss explicit regularization as another choice that affects robustness.\nDeﬁnition 5.4.1 (Regularized Classiﬁcation). The regularized empirical risk minimization\nproblem for linear classiﬁcation is deﬁned as\nˆw(λ) = arg min\nw\nE(x,y)∼Dζ(yw⊤x) + λ∥w∥,\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS72\nx\nx + δ\nδ\n(a) ℓ∞attack\nx + δ\nδ\n(b) Fourier-ℓ∞attack\nx + δ\nδ\n(c) High freq. F-ℓ∞\nx + δ\nδ\n(d) Low Freq. F-ℓ∞\nFigure 5.1: Adversarial attacks (ℓ∞and Fourier-ℓ∞) against CIFAR-10 classiﬁcation\nmodels. Fourier-ℓ∞perturbations (b) in the spatial domain are concentrated around subtle\ndetails of the object (darker means stronger perturbation). In contrast, ℓ∞perturbations\n(a) are perceived by people as random noise. Fourier-ℓ∞can also be controlled to be\nhigh or low frequency (c, d). It is more difﬁcult to attack a standard model with only\nlow frequency perturbations (for all attacks ε = 8/255 but for low frequency Fourier-ℓ∞\nε = 50/255, otherwise attack fails). Appendix B.5.4 shows visualizations for variety of\nmodels in RobustBench.\nwhere λ denotes a regularization constant, ζ is a monotone loss function, and D is a dataset.\nFor simplicity we assume this problem has a unique solution while the original ERM can\nhave multiple solutions.\nTheorem 5.4.1 (Maximum Margin Classiﬁer using Regularization ([169], Theorem 2.1)).\nConsider linearly separable ﬁnite datasets and monotonically non-increasing loss func-\ntions. Then as λ →0, the sequence of solutions, ˆw(λ), to the regularized problem in\nDeﬁnition 5.4.1, converges in direction to a maximum margin classiﬁer as deﬁned in (5.5).\nMoreover, if the maximum margin classiﬁer is unique,\nlim\nλ→0\nˆw(λ)\n∥ˆw(λ)∥= arg max\nw:∥w∥≤1\nmin\ni\nyiw⊤xi .\n(5.10)\nThe original proof in [169] was given speciﬁcally for ℓp norms, however we observe that\ntheir proof only requires convexity of the norm, so we state it more generally. Quasi-norms\nsuch as ℓp for p < 1 are not covered by this theorem. In addition, the condition on the\nloss function is weaker than our strict monotonic decreasing condition as shown in [143,\nAppendix A].\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS73\nWe use this result to derive our Corollary 3.\nCorollary 3 (Maximally Robust Classiﬁer via Inﬁnitesimal Regularization). For linearly\nseparable data, under conditions of Theorem 5.4.1, the sequence of solutions to regu-\nlarized classiﬁcation problems converges in direction to a maximally robust classiﬁer.\nThat is, limλ→0 ˆw(λ)/∥ˆw(λ)∥converges to a solution of arg maxw{ε | yiw⊤(xi + δ) >\n0, ∀i, ∥δ∥∗≤ε}.\nProof. By Theorem 5.4.1, the margin of the sequence of regularized classiﬁers, mini yi\nˆ\nw(λ)⊤\n∥ˆ\nw(λ)∥xi,\nconverges to the maximum margin, maxw:∥w∥≤1 mini yiw⊤xi. By Lemma 5.2.1, any maxi-\nmum margin classiﬁer w.r.t. ∥· ∥gives a maximally robust classiﬁer w.r.t. ∥· ∥∗.\nAssuming the solution to the regularized problem is unique, the regularization term\nreplaces other implicit biases in minimizing the empirical risk. The regularization coefﬁcient\ncontrols the trade-off between robustness and standard accuracy. The advantage of this\nformulation compared with adversarial training is that we do not need the knowledge of the\nmaximally robust ε to ﬁnd a maximally robust classiﬁer. It sufﬁces to choose an inﬁnitesimal\nregularization coefﬁcient. [215, Theorem 4.1] generalized Theorem 5.4.1 for a family of\nclassiﬁers that includes fully-connected networks with ReLU non-linearities, which allows\nfor potential extension of Corollary 3 to non-linear models. There remain gaps in this\nextension (see Appendix B.4).\nExplicit regularization has been explored as an alternative approach to adversarial\ntraining [81, 190, 230, 158, 75]. To be clear, we do not propose a new regularization method\nbut rather, we provide a framework for deriving and guaranteeing the robustness of existing\nand future regularization methods.\n5.5\nExperiments\nThis section empirically compares approaches to ﬁnding maximally robust classiﬁers. Sec-\ntion 5.5.3 evaluates the robustness of CIFAR-10 [105] image classiﬁers against our Fourier-\nℓ∞attack. We implement our attack in AutoAttack [35] and evaluate the robustness of\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS74\n100\n101\nd/n\n0.0\n0.5\n1.0\n1.5\nEpsilon\nCD\nReg=L1\nCD\nReg=L1\n(a) ℓ∞attack\n100\n101\nd/n\n0\n2\n4\n6\nEpsilon\nGD+LS\nReg=L2\nGD+LS\nReg=L2\n(b) ℓ2 attack\n100\n101\nd/n\n0\n10\n20\n30\n40\n50\nEpsilon\nSignGD\nReg=Linf\nSignGD\nReg=Linf\n(c) ℓ1 attack\nCD\nGD+LS\nSignGD\nReg=L1\nReg=L2\nReg=Linf\nFigure 5.2: Maximally robust perturbation size (ε) for linear models against ℓ∞, ℓ2, and\nℓ1 attacks. For each attack, there exists one optimizer and one regularization method that\nﬁnds a maximally robust classiﬁer (inner legends). We compare Coordinate Descent (CD),\nGradient Descent with Line Search (GD+LS), Sign Gradient Descent (SignGD), and explicit\nℓ1, ℓ2, and ℓ∞regularization. The gap between methods grows with the overparametrization\nratio (d/n). (More ﬁgures in Appendix B.5.3)\n100\n101\nd/n\n0.25\n0.50\n0.75\n1.00\nEpsilon\nConv Linear,GD+LS\nLinear,CVXPY\nLinear,Reg=Fourier-L1\nLinear,Reg=L1\nLinear,Reg=Linf\nFigure 5.3: Maximally robust ε against Fourier-ℓ∞attack. Explicit Fourier-ℓ1 regular-\nization ﬁnds a maximally robust classiﬁer as it achieves similar robustness as CVXPY’s\nsolution. A linear convolutional model converges to a solution but a small gap exists.\nrecent defenses available in RobustBench [36]. Details of the experiments and additional\nvisualizations are in Appendix B.5.\n5.5.1\nMaximally Robust to ℓ∞, ℓ2, ℓ1, and Fourier-ℓ∞Bounded Attacks\nFig. 5.2 and 5.3 plot the maximally robust ϵ as a function of the overparametrization ratio\nd/n, where d is the model dimension and n is the number of data points. Fig. 5.2 shows\nrobustness against ℓ∞, ℓ2, and ℓ1 attacks for linear models. Coordinate descent and explicit ℓ1\nregularization ﬁnd a maximally robust ℓ∞classiﬁer. Gradient descent and ℓ2 regularization\nﬁnd a maximally robust ℓ2 classiﬁer. Sign gradient descent and ℓ∞regularization ﬁnd a\nmaximally robust ℓ1 classiﬁer. The gap between margins grows as d/n increases. Fig. 5.3\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS75\nshows robustness against Fourier-ℓ∞attack; training a 2-layer linear convolutional network\nwith gradient descent converges to a maximally robust classiﬁer. A gap exists between the\nlinear convolutional network and the maximally robust classiﬁer that should theoretically\ndisappear with training budget greater than what we have used.\nFor these plots we synthesized linearly separable data focusing on overparametrized\nclassiﬁcation problems (i.e., d > n). Plotting the overparametrization ratio shows how\nrobustness changes as models become more complex. We compare models by computing\nthe maximal ε against which they are robust, or equivalently, the margin for linear models,\nmini yiw⊤xi/∥w∥. As an alternative to the margin, we estimate the maximal ε for a\nmodel by choosing a range of potential values, generating adversarial samples, and ﬁnding\nthe largest value against which the classiﬁcation error is zero. Generating adversarial\nsamples involves optimization, and requires more implementation detail compared with\ncomputing the margin. Plots in this section are based on generating adversarial samples to\nmatch common practice in the evaluation of non-linear models. Matching margin plots are\npresented in Appendix B.5.3, which compare against the solution found using CVXPY [42]\nand adversarial training given the maximal ε. Our plots depict mean and error bars for 3\nrandom seeds.\n5.5.2\nPlotting the Trade-offs\nFig. 5.4 illustrates the trade-off between standard accuracy and adversarial robustness.\nAdversarial training ﬁnds the maximally robust classiﬁer only if it is trained with the\nknowledge of the maximally robust ε (Fig. 5.4a). Without this knowledge, we have to search\nfor the maximal ε by training multiple models. This adds further computational complexity\nto adversarial training which performs an alternated optimization. In contrast, explicit\nregularization converges to a maximally robust classiﬁer for a small enough regularization\nconstant (Fig. 5.4b).\nOn CIFAR-10, we compare adversarial training with the regularization method TRADEs [230]\nfollowing the state-of-the-art best practices [70]. Both methods depend on a constant ε\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS76\n0.5\n1.0\n1.5\n2.0\n2.5\nAdversarial Training Epsilon\n0.0\n0.5\n1.0\nEpsilon\n(a) Adversarial training\n10\n6\n10\n4\n10\n2\n100\nRegularization Coefficient\n0.5\n1.0\n1.5\nEpsilon\n(b) ℓ1 regularization\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTraining Epsilon (/ 8\n255)\n0\n20\n40\n60\nRobust Accuracy\nTRADES\nAdv. Train.\n(c)\nCIFAR-10\nTrain.\nEps.\n10\n4\n10\n2\n100\n102\n104\nTRADES Regularization Coefficient\n0\n20\n40\n60\n80\n100\nAccuracy\nRobust\nClean\n(d) CIFAR-10 TRADES\nFigure 5.4: Trade-off in robustness against ℓ∞attack in linear models and CIFAR-\n10. We plot the maximally robust ε for adversarial training and explicit regularization.\nRobustness is controlled using ε in adversarial training (a) and regularization coefﬁcient in\nexplicit regularization (b). Using adversarial training we have to search for the maximal ε but\nfor explicit regularization it sufﬁces to choose a small regularization coefﬁcient. Similarly, on\nCIFAR-10, the highest robustness at a ﬁxed test ε is achieved for ε used during training (c). In\ncontrast to optimal linear regularizations, TRADES shows degradation as the regularization\ncoefﬁcient decreases (d). Discussion in Section 5.5.2\nduring training. Fig. 5.4c shows optimal robustness is achieved for a model trained and tested\nwith the same ε. When the test ε is unknown, both methods need to search for the optimal\nε. Given the optimal training ε, Fig. 5.4d investigates whether TRADES performs similar\nto an optimal linear regularization (observed in Fig. 5.4b), that is the optimal robustness is\nachieved with inﬁnitesimal regularization. In contrast to the linear regime, the robustness\ndegrades with smaller regularization. We hypothesize that with enough model capacity,\nusing the optimal ε, and sufﬁcient training iterations, smaller regularization should improve\nrobustness. That suggests that there is potential for improvement in TRADES and better\nunderstanding of robustness in non-linear models.\n5.5.3\nCIFAR-10 Fourier-ℓ∞Robustness\nFig. 5.5 reports the maximally robust ε of image classiﬁcation models on CIFAR-10. We\nevaluate top defenses on the leaderboard of RobustBench [36]. The attack methods are\nAPGD-CE and APGD-DLR with default hyper-parameters in RobustBench and ε = 8/255.\nTheoretical results do not provide guarantees beyond the maximally robust ε. Even robust\nmodels against corruptions with no adversarial training achieve similar robustness to ℓ2/ℓ∞\nmodels. The maximal ε is the largest at which adversarial accuracy is no more than 1%\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS77\n85.0\n87.5\n90.0\n92.5\n95.0\nStandard Accuracy\n0\n2\n4\n6\nEpsilon (x255)\nLinf\nL2\ncorruptions\nStandard\nFigure 5.5: Maximally robust ε against Fourier-ℓ∞for recent defenses. Color and shape\ndenote the type of robust training: adversarial (ℓ2 or ℓ∞), corruptions, or standard training.\nFourier-ℓ∞is a strong attack against current robust models.\nworse than the standard accuracy. All models have almost zero accuracy against larger, but\nstill perceptually small, perturbations (ε = 20/255). Appendix B.5.4 gives more examples\nof Fourier-ℓ∞attacks and band-limited variations similar to Fig. 5.1 for robustly trained\nmodels, showing that perturbations are qualitatively different from those against the standard\nmodel.\n5.6\nRelated work\nThis paper bridges two bodies of work on adversarial robustness and optimization bias.\nAs such there are many related works, the most relevant of which we discuss here. Prior\nworks either did not connect optimization bias to adversarial robustness beyond margin-\nmaximization [124, 43, 47] or only considered adversarial training with a given perturbation\nsize [114].\nRobustness Trade-offs\nMost prior work deﬁnes the metric for robustness and generaliza-\ntion using an expectation over the loss. Instead, we deﬁne robustness as a set of classiﬁcation\nconstraints. Our approach better matches the security perspective that even a single inaccu-\nrate prediction is a vulnerability. The limitation is explicit constraints only ensure perfect\naccuracy near the training set. Standard generalization remains to be studied using other\napproaches such as those with assumptions on the data distribution. Existing work has used\nassumptions about the data distribution to achieve explicit trade-offs between robustness and\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS78\nstandard generalization [44, 89, 90, 161, 204, 230, 176, 51, 52].\nFourier Analysis of Robustness.\nVarious observations have been made about Fourier\nproperties of adversarial perturbations against deep non-linear models [87, 206, 183]. [223]\nshowed that adversarial training increases robustness to perturbations concentrated at high\nfrequencies and reduces robustness to perturbations concentrated at low frequencies. [153]\nalso observed that the measured margin of classiﬁers at high frequencies is larger than the\nmargin at low frequencies. Our Corollary 2 does not distinguish between low and high\nfrequencies but we establish an exact characterization of robustness. [24] hypothesized about\nthe implicit robustness to ℓ1 perturbations in the Fourier domain while we prove maximal\nrobustness to Fourier-ℓ∞perturbations.\nArchitectural Robustness.\nAn implication of our results is that robustness can be achieved\nat a lower computational cost compared with adversarial training by various architectural\nchoices as recently explored [222, 58, 9]. Moreover, for architectural choices that align with\nhuman biases, standard generalization can also improve [208]. Another potential future\ndirection is to rethink ℓp robustness as an architectural bias and ﬁnd inspiration in the human\nvisual system for appropriate architectural choices.\nRobust Optimization\nA robust counterpart to an optimization problem considers uncer-\ntainty in the data and optimizes for the worst-case. [12] provided extensive formulations and\ndiscussions on robust counterparts to various convex optimization problems. Adversarial\nrobustness is one such robust counterpart and many other robust counterparts could also be\nconsidered in deep learning. An example is adversarial perturbations with different norm-ball\nconstraints at different training inputs. [126] observed the link between robust optimization\nand adversarial robustness where the objective is a min-max problem that minimizes the\nworst-case loss. However, they did not consider the more challenging problem of maximally\nrobust optimization that we revisit.\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS79\nImplicit bias of optimization methods.\nIn Section 2.1.7 we discussed prior work on the\nimplicit bias of optimization methods. Here we revisit prominent works and explain the\nconnection to this chapter.\nMinimizing the empirical risk for an overparametrized model with more parameters than\nthe training data has multiple solutions. [228] observed that overparametrized deep models\ncan even ﬁt to randomly labeled training data, yet given correct labels they consistently gen-\neralize to test data. This behavior has been explained using the implicit bias of optimization\nmethods towards particular solutions. [72] proved that minimizing the empirical risk using\nsteepest descent and mirror descent have an implicit bias towards minimum norm solutions\nin overparametrized linear classiﬁcation. [73] proved the implicit bias of gradient descent in\ntraining linear convolutional classiﬁers is towards minimum norm solutions in the Fourier\ndomain that depends on the number of layers.\nRecent theory of generalization in deep learning, in particular the double descent phe-\nnomenon, studies the generalization properties of minimum norm solutions for ﬁnite and\nnoisy training sets [77]. Characterization of the double descent phenomenon relies on the\nimplicit bias of optimization methods while using additional assumptions about the data\ndistribution. In contrast, our results only rely on the implicit bias of optimization and hence\nare independent of the data distribution.\nHypotheses.\n[67] proposed the linearity hypothesis that informally suggests ℓp adversarial\nsamples exist because deep learning models converge to functions similar to linear mod-\nels. To improve robustness, they argued models have to be more non-linear. Based on\nour framework, linear models are not inherently weak. When trained, regularized, and\nparametrized appropriately they can be robust to some degree, the extent of which depends\non the dataset. [60] proposed adversarial spheres as a toy example where a two layer neural\nnetwork exists with perfect standard and robust accuracy for non-zero perturbations. Yet,\ntraining a randomly initialized model with gradient descent and ﬁnite data does not converge\nto a robust model. Based on our framework, we interpret this as an example where the\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS80\nimplicit bias of gradient descent is not towards the ground-truth model, even though there is\nno misalignment in the architecture. It would be interesting to understand this implicit bias\nin future work.\nRobustness to ℓp-bounded attacks.\nRobustness is achieved when any perturbation to\nnatural inputs that changes a classiﬁer’s prediction also confuses a human. ℓp-bounded\nattacks are the ﬁrst step in achieving adversarial robustness. [203] have recently shown\nmany recent robust models only achieve spurious robustness against ℓ∞and ℓ1 attacks.\n[35] showed that on image classiﬁcation datasets there is still a large gap in adversarial\nrobustness to ℓp-bounded attacks and standard accuracy. Robustness to multiple ℓp-bounded\nperturbations through adversarial training and its trade-offs has also been analyzed [202,\n127]. [182, 180] argue that none of ℓ0, ℓ1, ℓ∞, or SSIM are a perfect match for human\nperception of similarity. That is for any such norm, for any ε, there exists a perturbation\nsuch that humans classify it differently. Attacks based on other perceptual similarity metrics\nexist [234, 119]. This shows that the quest for adversarial robustness should also be seen as\na quest for understanding human perception.\nRobustness through Regularization\nVarious regularization methods have been proposed\nfor adversarial robustness that penalize the gradient norm and can be studied using the\nframework of maximally robust classiﬁcation. [121] proposed general ℓp norm regularization\nof gradients. [81] proposed the Cross-Lipschitz penalty by regularizing the norm of the\ndifference between two gradient vectors of the function. [168] proposed ℓ2 regularization of\nthe norm of the gradients. [190] performed regularization of Frobenius norm of the per-layer\nJacobian. [139] proposed penalizing the curvature of the loss function. [158] proposed\nencouraging local linearity by penalizing the error of local linearity. [186] proposed regular-\nization of the gradient norm where the dual norm of the attack norm is used. [123] proposed\nHessian regularization. [75] showed that some regularization methods are equivalent or\nperform similarly in practice. Strong gradient or curvature regularization methods can suffer\nfrom gradient masking [123].\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS81\nCertiﬁed Robustness.\nAdversarially trained models are empirically harder to attack than\nstandard models. But their robustness is not often provable. Certiﬁably robust models seek\nto close this gap [81, 218, 34, 69, 173]. A model is certiﬁably robust if for any input, it also\nprovides an ε-certiﬁcate that guarantees robustness to any perturbation within the ε-ball of\nthe input. In contrast, a maximally robust classiﬁer ﬁnds a classiﬁer that is guaranteed to\nbe robust to maximal ε while classifying all training data correctly. That allows for data\ndependent robustness guarantees at test time. In this work, we have not explored standard\ngeneralization guarantees.\n5.6.1\nInvestigating the Gap in the Convergence of Linear Convolutional Net-\nworks\nWe further investigated the gap between theory and experiment in Fig. 5.3, and our results\nare consistent with the observations of [226, Section 6]. The gap appears to be due to ﬁnite\ntraining time and it depends on the scale of the initialization and the learning rate. We\nwere able to eliminate the gap for low dimensional problems (e.g., d = 2) by tuning the\ninitialization scale. For high-dimensional problems (e.g., d = 100), it is more challenging to\nﬁnd the optimal initialization scale. As suggested by [226], this is an important problem for\nfuture work of implicit optimization bias literature.\nFor completeness, we did test different aspects of our implementation to rule out other\ncauses. Here is a list of potential causes we eliminated:\n• The scaling of the DFT matrix is accurate for comparison with the minimum-norm\nFourier-ℓ1 solution.\n• Numerical error does not seem to contribute to the gap as we do not see an improvement\nby switching from Float32 to Float64.\n• We thoroughly tested our implementation of the convolution operation and lineariza-\ntion operation.\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS82\n5.7\nConclusion\nWe demonstrated that the choice of optimizer, neural network architecture, and regularizer,\nsigniﬁcantly affect the adversarial robustness of linear neural networks. These results lead\nus to a novel Fourier-ℓ∞attack with controllable spectral properties applied against deep\nnon-linear CIFAR-10 models. Our results provide a framework, insights, and directions\nfor improving robustness of non-linear models through approaches other than adversarial\ntraining.\nIn the following, we summarize the hypotheses, results, and future work:\n• We revisited the deﬁnition of Maximally Robust Classiﬁcation and justiﬁed as a\nformulation of the problem of ﬁnding adversarially robust models. We argue that\nMaximally Robust Classiﬁcation is an alternative problem formulation possibly more\nchallenging to solve than standard min-max formulation of adversarial robustness.\nThere is also no ambiguity in the choice of hyper-parameters in the formulation of the\nMaximally Robust Classiﬁer.\n• For linear models, we prove guaranteed maximal robustness achieved only by the\nchoice of the optimizer, regularization, or architecture. We have achieved this through\nour Corollaries. For example, because of our results, it can no longer be claimed that\n“A regularization method alone cannot achieve maximal robustness”. At least in the\nlinear case we showed that such a regularizer exists.\n• We rigorously established the adversarial robustness of linear convolutional neural\nnetworks in the Fourier domain. In that direction our novel Fourier attack creates one\npotential direction for understanding the robustness of non-linear models.\n• We prove that under certain conditions no additional per-iteration cost of solving an\noptimization problem is needed, yet such a solution might take many iterations to ﬁnd.\nWe have not done computational analysis of the methods studied in this chapter which\ncan be done with additional assumptions on the data distribution. Our results only\nCHAPTER 5. BRIDGING THE GAP BETWEEN ADVERSARIAL ROBUSTNESS AND OPTIMIZATION BIAS83\nrequire linear separability of the data.\n• We have not proposed a novel defense for non-linear models. We discuss future\ndirections and challenges in Section 5.6 and Appendix B.4. In particular, there is\na growing literature on the implicit bias of non-linear networks that can be used to\nextend our results [32, 122, 151]. Non-linear solutions might require signiﬁcantly\ndifferent mathematical tools and efforts from a wider community and take years to\nﬁnd.\n• We do not claim that robustness guarantees achieved through the choice of the opti-\nmizer, architecture, or regularization are sufﬁcient for any given model family, task or\ndomain of application. For example, if the data is not linearly separable, additional\nsacriﬁces need to be made (See Appendix B.1). Having said that, efﬁciently ﬁnding\nthe maximally robust classiﬁer for deep non-linear models might be achievable by\nappropriate choices of all these in addition to adversarial training with an appropriate\nepsilon.\n• There is a small gap in Fig. 5.3 between theory and experiment that might be due\nto a missing condition in the original theory of [73]. We have provided additional\ndiscussion in Section 5.6.1.\nChapter 6\nConclusion and Future Work\nIn this thesis we discussed ideas for improving training efﬁciency and connections to ro-\nbustness in deep learning. Chapter 3 showed that hard negatives are better than uniformly\nsampled negatives; they result in faster training and better generalization. Chapter 4 pro-\nposed gradient clustering to reduce the gradient variance by automatically discovering and\nexploiting data diversity. The mixed results revealed gaps in our understanding of training\ndeep learning models. Finally, Chapter 5 establishes a connection between robustness and\noptimization choices and shows that the two challenges are closely related.\nWhat is the most efﬁcient and robust training method in deep learning? This remains an\nopen question for us. Various approaches focus on improving training efﬁciency and standard\ngeneralization but lack attention to adversarial robustness and biased data distributions.\nInteresting and vibrant approaches include few-shot learning, transfer learning, and meta\nlearning [83, 211, 54]. On the other hand, contrastive learning and adversarial learning [76,\n66] are designed with only the objective of robustness to adversarial inputs or preset data\ntransformations.\nOur observations in Chapter 3 are in favor of adversarial and robust representation\nlearning approaches. Hard negatives in triplet losses are examples are adversarial inputs\nwhere only inputs in the training set deﬁne the natural data distribution. Adversarial learning\nand contrastive learning methods might also beneﬁt from a similar observation that semi-\n84\nCHAPTER 6. CONCLUSION AND FUTURE WORK\n85\nhard negatives provide superior generalization than absolute hard negatives. Curriculum\nlearning [14] might provide more adjustable contrastive and adversarial samples. Learning\nmulti-modal embeddings remain particularly propitious for efﬁcient hard negative-based\nmethods with no need for curriculum learning or generative adversarial networks [29, 213,\n8].\nOur results in Chapter 5 suggest an alternative or complementary approach by design.\nInstead of adversarial learning, under certain conditions, similar solutions can be found by\nappropriate architecture, optimizer and regularizer. The right architecture, e.g., convolution\nversus fully-connected neural networks, would signiﬁcantly reduce the amount of adversarial\ntraining required. In some cases, adversarial training may become unnecessary.\nIn conclusion, this thesis raises more questions than it answers. What follows are some\npossible directions for further research on related problems.\nIn Chapter 5 we discuss the problem of maximally robust classiﬁcation through which\nwe transferred results on implicit and explicit bias of optimization methods to adversarial\nrobustness. There is potential in extending this connection to transfer results in both direc-\ntions. One example is to transfer provable guarantees for deep non-linear models to infer the\nimplicit bias of various optimization methods that have not yet been derived. Exploiting the\nconnection in the opposite direction, there have been recent results on the implicit bias of\nReLU networks and the directional convergence of deep linear networks that imply further\nmaximal robustness results.\nIn Chapter 4 we designed an efﬁcient gradient clustering method. An application of\ngradient clustering is in automatically detecting imbalanced data according to gradient\ninformation and modifying the sampling to adjust and calibrate training. For example, by\nchanging the learning paradigm from Empirical Risk Minimization to Distributionally Robust\nOptimization we should be able to formalize the design of distributionally robust sampling\nmethods. Worst-group error is an evaluation metric commonly used in distributionally robust\noptimization that is a robust counterpart to the average error in expected risk minimzation. In\nSGD with gradient clustering we used a reweighting to achieve unbiased gradient estimates\nCHAPTER 6. CONCLUSION AND FUTURE WORK\n86\nthat would guarantee convergence to the same optimum as SGD with uniform sampling.\nRecently it has been shown that upweighting the minority class negatively impacts the\nminority error while subsampling the majority class improves it [171]. Inspired by this\nobservation one might consider gradient clustering for distributionally robust optimization\nwithout reweighting samples. The gradient clustering sampler would then intrinsically ﬁnd\nmajority groups of data and sample the same number for all majority and minority groups.\nAn extension to both Chapters 4 and 5 is to understand the implications of implicit\nbias in continual and lifelong learning. Continual learning is a challenging task in machine\nlearning restricted by catastrophic forgetting in training and lack of backward and forward\ntransfer. In continual learning and for memoryless methods, we can describe the impact of\nprior learning tasks as a change in the initialization of the future learnings. Based on the\nprior work discussed in Chapter 5, we know in certain problems, the initialization changes\nthe solution found. It is possible to connect the implicit bias and initialization for sequential\nlinear regression tasks and design an optimal method of initialization depending on the\nsimilarity of tasks. Extending this idea to continual learning benchmarks is an interesting\nfuture direction.\nAnother extension to Chapter 5 is to characterize the implicit robustness of precondi-\ntioned optimization methods [4]. The implicit bias of preconditioned methods is different\nfrom gradient descent and depends on how the preconditioning is estimated. The alternative\nimplicit bias results in an alternative implicit robustness that may be allow robustness to a\nnew family of practical adversarial perturbations.\nFinally, in adversarial learning and contrastive learning, the performance depends on\nthe strength of adversarial inputs and data augmentations [31]. One possible direction for\nfuture work is the exact characterization of the dependence on adversarial strength in simple\nlearning settings that would allow better justiﬁcations and recipes for curriculum learning.\nGradient-aware methods such as variations of gradient clustering proposed in Chapter 4\nshould be useful in the direction of adversarial and contrastive learning as recent work\ncorroborates [125].\nAppendix A\nAppendices to the Chapter on\nGradient Clustering\nA.1\nAdditional Details of Gradient Clustering\nA.1.1\nProof of Proposition 4.3.1\nThe gradient estimator, ˆg, is unbiased for any partitioning of data, i.e., equal to the average\ngradient of the training set,\nE[ˆg] = 1\nN\nK\nX\nk=1\nNkE[g(k)] = 1\nN\nK\nX\nk=1\nNk\n\n1\nNk\nNk\nX\nj=1\ng(k)\nj\n\n= 1\nN\nK\nX\nk=1\nNk\nX\nj=1\ng(k)\nj\n|\n{z\n}\n(∗)\n= 1\nN\nN\nX\ni=1\ngi = g ,\nwhere we use the fact that the expectation of a random sample drawn uniformly from a\nsubset is equal to the expectation of the average of samples from that subset. Also note that\nthe gradient of every training example appears once in (∗).\n87\nAPPENDIX A. APPENDICES TO THE CHAPTER ON GRADIENT CLUSTERING\n88\nAlthough partitioning does not affect the bias of ˆg, it does affect the variance,\nV[ˆg] =\n1\nN2\n\n\nN\nX\ni=1\nNk2V[g(k)] + 2\nK\nX\nk=1\nNˆk\nX\nˆk=1\nNkNˆkC[g(k), g(ˆk)]\n\n=\n1\nN2\nK\nX\nk=1\nNk2V[g(k)]\n(A.1)\nwhere the variance is deﬁned as the trace of the covariance matrix. Since we assume the\ntraining set is sampled i.i.d., the covariance between gradients of any two samples is zero.\nIn a dataset with duplicate samples, the gradients of duplicates will be clustered into one\ncluster with zero variance if mingled with no other data points.\nA.2\nAdditional Details of Efﬁcient GC\nA.2.1\nConvolutional Layers\nIn neural networks, the convolution operation is performed as an inner product between\na set of weights θ ∈Rh×w×ˆI×O, namely kernels, by patches of size h × w in the input.\nAssuming that we have preprocessed the input by extracting patches, the gradient w.r.t. θ is\ngi = P\nt gb,t, gb,t ∈RI×O is the gradient at the spatial location t ∈T and I = h × w × ˆI\nis the ﬂattened dimension of a patch. The gradient at spatial location t is computed as\ngb,t = Ab,tD⊤\nb,t.\nLike the fully-connected case, we use a rank-1 approximation to the cluster centers in a\nconvolution layer, deﬁning Ck = ckdk⊤. As such, AU steps are performed efﬁciently. For\nthe A step we rewrite vec{Ck} ⊙vec{gi},\nvec{Ck} ⊙vec{\nX\nt\nAb,tD⊤\nb,t} =\nX\nu,v\n(cku dkv (\nX\nt\nAbtu Dbtv))\n(A.2)\n=\nX\nt\n(\nX\nu\ncku Abtu)(\nX\nv\ndkv Dbtv),\n(A.3)\nwhere the input dimension is indexed by u and the output dimension is indexed by v. Eqs. A.2\nand A.3 provide two ways of computing the inner-product, where we ﬁrst compute the inner\nAPPENDIX A. APPENDICES TO THE CHAPTER ON GRADIENT CLUSTERING\n89\nOperation\nFC Complexity\nConv Complexity\nC ⊙g\nKB(I + O)\nEq. A.2: B(T + K)IO\nEq. A.3: BTK(I + O)\nC ⊙C\nK(I + O)\nK(I + O)\ng ⊙g\nB(I + O)\nEq. A.2: BTIO,\nEq. A.3: BT 2(I + O)\nBack-prop\nBIO\nBTIO\nA step\nKB(I + O)\nSee Sec. A.2.2\nU step\nB(I + O)\nB(I + O)\nTable A.1: Complexity of GC compared to the cost of back-prop.\nsums, then the outer sum. The efﬁciency of each formulation depends on the size of the\nkernel and layer’s input and output dimensions.\nA.2.2\nComplexity Analysis\nGC, described in Fig. 4.2, performs two sets of operations, namely, the cluster center updates\n(U step), and the assignment update of data to clusters (A step). A steps instantly affect the\noptimization by changing the sampling process. As such, we perform an A step every few\nepochs and change the sampling right after. In contrast, the U step can be done in parallel\nand more frequently than the A step, or online using mini-batch updates. The cost of both\nsteps is amortized over optimization steps.\nTable A.1 summarizes the run-time complexity of GC compared to the cost of single\nSGD step. The U step is always cheaper than a single back-prop step. The A step is cheaper\nfor fully-connected layers if K < min(I, O).\nFor convolutional layers, we have two ways to compute the terms in the A step (Eqs. A.2\nand A.3). For C ⊙g, if min(T, K) < K < min(I, O), Eq. A.3 is more efﬁcient. For g ⊙g,\nEq. A.3 is more efﬁcient if T < min(I, O). If K < T, both methods have lower complexity\nthan a single back-prop step. If we did not have the Nk multiplier in the A step, we could\nignore the computation of the norm of the gradients, and hence further reduce the cost.\nAPPENDIX A. APPENDICES TO THE CHAPTER ON GRADIENT CLUSTERING\n90\nIn common neural network architectures, the condition K < T is easily satisﬁed as T in\nall layers is almost always more than 10 and usually greater than 100, while 10-100 clusters\nprovides signiﬁcant variance reduction. As such, the total overhead cost with an efﬁcient\nimplementation is at most 2× the cost of a normal back-prop step. We can further reduce\nthis cost by performing GC on a subset of the layers, e.g., one might exclude the lowest\nconvolutional layers.\nThe total memory overhead is equivalent to increasing the mini-batch size by K samples\nas we only need to store rank-1 approximations to the cluster centers.\nA.3\nAdditional Details for Experiments\nThe mini-batch size in GC and SVRG and the number of clusters in GC are the same as the\nmini-batch size in SG-B and the same as the mini-batch size used for training using SGD. To\nmeasure the gradient variance, we take snapshots of the model during training, sample tens\nof mini-batches from the training set (in case of GC, with stratiﬁed sampling), and measure\nthe average variance of the gradients.\nWe measure the performance metrics (e.g., loss, accuracy and variance) as functions\nof the number of training iterations rather than wall-clock time. In other words, we do not\nconsider computational overhead of different methods. In practice, such analysis is valid as\nlong as the additional operations could be parallelized with negligible cost.\nA.3.1\nExperimental Details for Image Classiﬁcation Models\nOn MNIST, our MLP model consists of there fully connected layers: layer1: 28 ∗28 × 1024,\nlayer2: 1024×1024, layer3: 1024, 10. We use ReLU activations and no dropout in this MLP.\nWe train all methods with learning rate 0.02, weight decay 5 × 10−4, and momentum 0.5.\nOn CIFAR-10, we train ResNet8 with no batch normalization layer and learning rate 0.01,\nweight decay 5 × 10−4, and momentum 0.9 for 80000 iterations. We decay the learning rate\nat 40000 and 60000 iterations by a factor of 0.1. On CIFAR-100, we train ResNet32 starting\nAPPENDIX A. APPENDICES TO THE CHAPTER ON GRADIENT CLUSTERING\n91\nDataset\nModel\nB\nT\nLog T\nEstim T\nU\nGC T\nMNIST\nMLP\n128\n50000\n500\n50\n2000\n10\nCIFAR-10\nResNet8\n128\n80000\n500\n50\n20000\n3\nCIFAR-100\nResNet32\n128\n80000\n500\n50\n20000\n3\nImageNet\nResNet18\n128\n80000\n1000\n10\n10000\n3\nTable A.2: Hyperparameters.\nwith learning rate 0.1. Other hyper-parameters are the same as in CIFAR-10. On ImageNet,\nwe train ResNet18 starting with learning rate 0.1, weight decay 1 × 10−4, and momentum\n0.9. We use a similar learning rate schedule to CIFAR-10.\nIn Appendix A.3.1 we list the following hyper-parameters: the interval of measuring\ngradient variance and normalized variance (Log T), number of gradient estimates used on\nmeasuring variance (Estim T), the interval of updating the control variate in SVRG and the\nclustering in GC (U), and the number of GC update iterations (GC T).\nIn plots for random features models, each point is generated by keeping hs ﬁxed at 1000\nand varying N in the range [0.1, 10]. We average over 3 random seeds, 2 teacher hidden\ndimensions and 2 input dimensions (both ×0.1 and ×10 student hidden). We use mini-batch\nsize 10 for SG-B, SVRG, and GC.\nA rough estimate of the overparametrization coefﬁcient (discussed in Section 4.4.2) for\ndeep models is to divide the total number of parameters by the number of training data. On\nMNIST the coefﬁcient is approximately 37 for CNN and 31 for MLP. On CIFAR-10 it is\napproximately 3 for ResNet8 and 9 for ResNet32. Common data augmentations increase\nthe effective training set size by 10×. On the other hand, the depth potentially increases\nthe capacity of models exponentially (cite the paper that theoretically says how many data\npoints a model can memorize). As such, it is difﬁcult to directly relate these numbers to the\nbehaviours observed in RF models.\nAPPENDIX A. APPENDICES TO THE CHAPTER ON GRADIENT CLUSTERING\n92\n0\n2\n4\n6\n8\n10\nhs/N\n10\n9\n10\n7\n10\n5\n10\n3\n10\n1\nMean Avg. Variance\nSG,LR=0.01\nSG,LR=0.001\nSG,LR=0.1\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\n10\n1\nMean Avg. Variance\nSG-B\nSG-2B\nSVRG\nGC\n0\n2\n4\n6\n8\n10\nhs/N\n10\n7\n10\n5\n10\n3\n10\n1\nMean Avg. Variance\nFigure A.1: Mean variance plots for Fig. 4.4\nA.3.2\nExperimental Details for Random Features Models\nThe number of training iterations is chosen such that the training loss has ﬂattened. The\nmaximum is taken over the last 70% of iterations (the variance is usually high for all methods\nin the ﬁrst 30%). Mean variance plots for random features models are similar to max variance\nplots presented in Section 4.4.2.\nWe aggregate results from multiple experiments with the following range of hyper-\nparameters. Each point is generated by keeping hs ﬁxed at 1000 and varying N in the\nrange [0.1, 10]. We average over 3 random seeds, 2 teacher hidden dimensions and 2 input\ndimensions (both ×0.1 and ×10 student hidden).\nA.4\nNormalized Variance and Convergence Analysis\nNormalized variance appears in standard convergence analysis of stochastic gradient de-\nscent [55]. To understand the connection, we brieﬂy review a standard result. Let f : Rn →\nR be a L-Lipschitz continuous function, i.e.,\n∥∇f(x) −∇f(y)∥≤L∥x −y∥, ∀x, y ∈Rn .\n(A.4)\nConsider the stochastic gradient descent method with updates,\nxk+1 = xk −αkgk\n(A.5)\nAPPENDIX A. APPENDICES TO THE CHAPTER ON GRADIENT CLUSTERING\n93\nwhere αk is the step size and\ngk := ∇f(xk) + ek\n(A.6)\nis an estimate of the gradient with the residual ek.\nFor αk = 1/L, the iterates of stochastic gradient descent satisfy the inequality [55, Eq.\n2.3]\nf(xk+1) ≤f(xk) −1\n2L∥∇f(xk)∥2 + 1\n2L∥ek∥2 ,\n(A.7)\nLet E[ek] = 0, by subtracting f(x∗) from both sides and taking expectations we have\nE[f(xk+1) −f(x∗)] ≤E[f(xk) −f(x∗)] −1\n2LE[∥∇f(xk)∥2] + 1\n2LE[∥ek∥2]\n(A.8)\nE[f(xk+1) −f(x∗)] ≤E[f(xk) −f(x∗)] −1\n2LE[∥∇f(xk)∥2](1 −ζ) ,\n(A.9)\nwhere ζ = E[∥ek∥2]/E[∥∇f(xk)∥2] is the normalized variance of the residual and the\ngradient as deﬁned in Section 4.4.\nIf the normalized variance is small, i.e. ζ ≪1, the noise error is dominated by the\nnon-stochastic error. As such, reducing the gradient variance would not speed up the training\nwhile for 1 ≪ζ, reducing the variance improves the convergence speed. This matches the\ndiminishing returns observations in practice as we discuss in Section 4.4.\nAppendix B\nAppendices to the Chapter on\nBridging the Gap Chapter\nB.1\nGeneralization of the Maximally Robust Classiﬁer\nDeﬁnition B.1.1 (Maximally Robust Classiﬁer with Slack Loss). Let ξ ≥0 denote a given\nslack variable. A maximally robust classiﬁer with slack loss is the solution to\narg max\nϕ∈Φ\n\u001a\nε | E (x,y) max\n∥δ∥≤ε ζ(yϕ(x + δ)) ≤ξ\n\u001b\n.\n(B.1)\nThis formulation is similar to the saddle-point problem in that we seek to minimize\nthe expectation of the worst case loss. The difference is that we also seek to maximize ε.\nHowever, we have introduced another arbitrary variable ξ that is not optimized as part of the\nproblem. For linear classiﬁers and the hinge loss, ζ(z) = [1 −z]+, Eq. (B.1) can be written\nas,\narg max\nw\nn\nε | E (x,y)[1 −yw⊤x + ε∥w∥∗]+ ≤ξ\no\n,\n(B.2)\nwhere [·]+ is the hinge loss, and the weight penalty term ∥w∥∗is inside the hinge loss. This\nsubtle difference makes solving the problem more challenging than weight penalty outside\n94\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n95\nthe loss.\nBecause of the two challenges we noted, we do not study the maximal robustness with\nslack loss.\nB.2\nProofs\nB.2.1\nProof of Lemma 5.2.1\nProof. We ﬁrst show that the maximally robust classiﬁer is equivalent to a robust counterpart\nby removing δ from the problem,\narg max\nw,b\n{ε | yi(w⊤(xi + δ) + b) > 0, ∀i, ∥δ∥≤ε}\n(homogeneity of p-norm)\n= arg max\nw,b\n{ε | yi(w⊤(xi + εδ) + b) > 0, ∀i, ∥δ∥≤1}\n(if it is true for all δ it is true for the worst of them)\n= arg max\nw,b\n{ε | inf\n∥δ∥≤1 yi(w⊤(xi + εδ) + b) > 0, ∀i}\n= arg max\nw,b\n{ε | yi(w⊤xi + b) + ε inf\n∥δ∥≤1 w⊤δ > 0, ∀i}\n(deﬁnition of dual norm)\n= arg max\nw,b\n{ε | yi(w⊤xi + b) > ε∥w∥∗, ∀i}\nAssuming w ̸= 0, which is a result of linear separability assumption, we can divide both\nsides by ∥w∥∗and change variables,\n= arg max\nw,b\n{ε | yi(w⊤xi + b) ≥ε, ∀i, ∥w∥∗≤1} ,\nwhere we are also allowed to change > to ≥because any solution to one problem gives an\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n96\nequivalent solution to the other given w ̸= 0.\nNow we show that the robust counterpart is equivalent to the minimum norm classiﬁcation\nproblem by removing ε. When the data is linearly separable there exists a solution with\nε > 0,\narg max\nw,b\n{ε | yi(w⊤xi + b) > ε∥w∥∗, ∀i}\n= arg max\nw,b\n\u001a\nε | yi\n\u0012 w⊤\nε∥w∥∗\nxi +\nb\nε∥w∥∗\n\u0013\n≥1, ∀i\n\u001b\nThis problem is invariant to any non-zero scaling of (w, b), so with no loss of generality\nwe set ∥w∥∗= 1.\n= arg max\nw,b\n\u001a\nε | yi\n\u0012w⊤\nε xi + b\n\u0013\n≥1, ∀i, ∥w∥∗= 1\n\u001b\nLet w′ = w/ϵ, then the solution to the following problem gives a solution for w,\narg max\nw′,b\n\u001a\n1\n∥w′∥∗\n| yi(w′⊤xi + b) ≥1, ∀i\n\u001b\n= arg min\nw′,b\nn\n∥w′∥∗| yi(w′⊤xi + b) ≥1, ∀i\no\n.\nB.2.2\nProof of Maximally Robust to Perturbations Bounded in Fourier Do-\nmain (Corollary 2)\nThe proof mostly follows from the equivalence for linear models in Appendix B.2.1 by\nsubstituting the dual norm of Fourier-ℓ1. Here, A∗denotes the complex conjugate transpose,\n⟨u, v⟩= u⊤v∗is the complex inner product, [F ]ik =\n1\n√\nDωik\nD the DFT matrix where\nωD = e−j2π/D, j = √−1.\nLet ∥· ∥be a norm on Cn and ⟨·, ·⟩be the complex inner product. Similar to Rn, the\nassociated dual norm is deﬁned as ∥δ∥∗= supx{|⟨δ, x⟩| | ∥x∥≤1} .\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n97\n∥F(w)∥1\n=\nsup\n∥δ∥∞≤1\n|⟨F(w), δ⟩|\n(Expressing DFT as a linear transformation.)\n=\nsup\n∥δ∥∞≤1\n|⟨F w, δ⟩|\n=\nsup\n∥δ∥∞≤1\n|⟨w, F ∗δ⟩|\n(Change of variables and F −1 = F ∗.)\n=\nsup\n∥F δ∥∞≤1\n|⟨w, δ⟩|\n=\nsup\n∥F(δ)∥∞≤1\n|⟨w, δ⟩| .\nB.3\nLinear Operations in Discrete Fourier Domain\nFinding an adversarial sample with bounded Fourier-ℓ∞involves ℓ∞complex projection\nto ensure adversarial samples are bounded, as well as the steepest ascent direction w.r.t\nthe Fourier-ℓ∞norm. We also use the complex projection onto ℓ∞simplex for proximal\ngradient method that minimizes the regularized empirical risk.\nB.3.1\nℓ∞Complex Projection\nLet v denote the ℓ2 projection of x ∈Cd onto the ℓ∞unit ball. It can be computed as,\narg min\n∥v∥∞≤1\n1\n2∥v −x∥2\n2\n(B.3)\n= {v : ∀i, vi = arg min\n|vi|≤1\n1\n2|vi −xi|2} ,\n(B.4)\nthat is independent projection per coordinate which can be solved by 2D projections onto ℓ2\nthe unit ball in the complex plane.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n98\nB.3.2\nSteepest Ascent Direction w.r.t. Fourier-ℓ∞\nConsider the following optimization problem,\narg max\nv:∥F v∥∞≤1\nf(v) ,\n(B.5)\nwhere F ∈Cd×d is the Discrete Fourier Transform (DFT) matrix and F ∗= F −1 and F ∗is\nthe conjugate transpose.\nNormalized steepest descent direction is deﬁned as (See [19, Section 9.4]),\narg min\nv\n{∇⟨f(w), v⟩: ∥v∥= 1} .\n(B.6)\nSimilarly, we can deﬁne the steepest ascent direction,\narg max\nv∈Rd\n{|⟨∇f(w), v⟩| : ∥F v∥∞= 1}\n(B.7)\n(Assuming f is linear.)\n(B.8)\narg max\nv∈Rd\n{|⟨g, F ∗F v⟩| : ∥F v∥∞= 1}\n(B.9)\narg max\nv∈Rd\n{|⟨F g, F v⟩| : ∥F v∥∞= 1}\n(B.10)\nwhere g = ∇f(w).\nConsider the change of variable u = F v ∈Cd×d. Since v is a real vector its DFT is\nHermitian, i.e., u∗\ni = [u]−i for all coordinates i where j = j mod d. Similarly, F g is\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n99\nHermitian.\narg max\nu∈Cd:∥u∥∞=1\n{|⟨F g, u⟩| : u∗\ni = [u]−i}\n(B.11)\narg max\nu∈Cd:∀i,|ui|=1\n{|[F g]iui| + |[F g]−i u∗\ni | : u∗\ni = [u]−i}\n(B.12)\narg max\nu∈Cd:∀i,|ui|=1\n{|[F g]iui| + |[F g]∗\ni u∗\ni | : u∗\ni = [u]−i}\n(B.13)\narg max\nu∈Cd:∀i,|ui|=1\n{|[F g]iui| : u∗\ni = [u]−i}\n(B.14)\nui = [F g]i/|[F g]i| .\n(B.15)\nand the steepest ascent direction is vi = F −1ui which is a real vector. In practice, there can\nbe non-zero small imaginary parts as numerical errors which we remove.\nB.4\nNon-linear Maximally Robust Classiﬁers\nRecall that the deﬁnition of a maximally robust classiﬁer (Deﬁnition 5.2.1) handles non-linear\nfamilies of functions, Φ:\narg max\nϕ∈Φ\n{ε | yiϕ(xi + δ) > 0, ∀i, ∥δ∥≤ε} .\nHere we extend the proof in Lemma 5.2.1 that made the maximally robust classiﬁcation\ntractable by removing δ and ε from the problem. In linking a maximally robust classiﬁer to\na minimum norm classiﬁer when there exists a non-linear transformation, the ﬁrst step that\nrequires attention is the following,\narg max\nϕ∈Φ\n{ε : inf\n∥δ∥≤1 yiϕ(xi + εδ) > 0, ∀i}\n̸= arg max\nϕ∈Φ\n{ε : yiϕ(xi) + ε inf\n∥δ∥≤1 ϕ(δ) > 0, ∀i}\nLemma B.4.1 (Gradient Norm Weighted Maximum Margin). Let Φ be a family of locally\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n100\nlinear classiﬁers near training data, i.e.,\nΦ = {ϕ : ∃ξ > 0, ∀i, ∥δ∥≤1, ε ∈[0, ξ),\nϕ(xi + εδ) = ϕ(xi) + εδ⊤∂\n∂xϕ(xi)}.\nThen a maximally robust classiﬁer is a solution to the following problem,\narg max\nϕ∈Φ,ε≤ξ\n{ε : yiϕ(xi) > ε∥∂\n∂xϕ(xi)∥∗, ∀i} .\nProof.\narg max\nϕ\n{ε : inf\n∥δ∥≤1 yiϕ(xi + εδ) ≥0, ∀i}\n(Taylor approx.)\n= arg max\nϕ\n{ε : inf\n∥δ∥≤1 yiϕ(xi) + yiεδ ∂\n∂xϕ(xi) ≥0, ∀i}\n= arg max\nϕ\n{ε : yiϕ(xi) + ε inf\n∥δ∥≤1 δ ∂\n∂xϕ(xi) ≥0, ∀i}\n(Dual to the local derivative.)\n= arg max\nϕ\n{ε : yiϕ(xi) ≥ε∥∂\n∂xϕ(xi)∥∗, ∀i}\n(Assuming constant gradient norm near data.)\n=\narg max\nϕ:∥∂\n∂xϕ(x)∥∗≤1\n{ε : yiϕ(xi) ≥ε, ∀i} .\nThe equivalence in Lemma B.4.1 fails when Φ includes functions with non-zero higher\norder derivatives within the ε of the maximally robust classiﬁer. In practice, this failure\nmanifests itself as various forms of gradient masking or gradient obfuscation where the\nmodel has almost zero gradient near the data but large higher-order derivatives [6].\nVarious regularization methods have been proposed for adversarial robustness that\npenalize the gradient norm and can be studied using the framework of maximally robust\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n101\nclassiﬁcation [168, 186, 123, 139] Strong gradient or curvature regularization methods can\nsuffer from gradient masking [123].\nFor general family of non-linear functions, the interplay with implicit bias of optimization\nand regularization methods remains to be characterized. The solution to the regularized\nproblem in Deﬁnition 5.4.1 is not necessarily unique. In such cases, the implicit bias of the\noptimizer biases the robustness.\nB.5\nExtended Experiments\nB.5.1\nDetails of Linear Classiﬁcation Experiments\nFor experiments with linear classiﬁers, we sample n training data points from the N(0, Id),\nd-dimensional standard normal distribution centered at zero. We label data points y =\nsign(w⊤x), using a ground-truth linear separator sampled from N(0, Id). For n < d,\nthe generated training data is linearly separable. This setting is similar to a number of\nrecent theoretical works on the implicit bias of optimization methods in deep learning\nand speciﬁcally the double descent phenomenon in generalization [138, 41]. We focus on\nrobustness against norm-bounded attacks centered at the training data, in particular, ℓ2, ℓ∞,\nℓ1 and Fourier-ℓ∞bounded attacks.\nBecause the constraints and the objective in the minimum norm linear classiﬁcation\nproblem are convex, we can use off-the-shelf convex optimization toolbox to ﬁnd the\nsolution for small enough d and n. We use the CVXPY library [42]. We evaluate the\nfollowing approaches based on the implicit bias of optimization: Gradient Descent (GD),\nCoordinate Descent (CD), and Sign Gradient Descent (SignGD) on fully-connected networks\nas well as GD on linear two-layer convolutional networks (discussed in Section 5.3). We\nalso compare with explicit regularization methods (discussed in Section 5.4) trained using\nproximal gradient methods [154]. We do not use gradient descent because ℓp norms can\nbe non-differentiable at some points (e.g., ℓ1 and ℓ∞) and we seek a global minima of the\nregularized empirical risk. We also compare with adversarial training. As we discussed in\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n102\nHyperparameter\nValues\nRandom seed\n0,1,2\nd\n100\nd/n\n1, 2, 4, 8, 16, 32\nTraining steps\n10000\nLearning rate\n1e−5, 3e−5, 1e−4, 3e−4, 1e−3, 3e−3,\n1e−2, 3e−2, 1e−1, 3e−1, 1, 2, 3, 6, 9, 10,\n20, 30, 50\nReg. coefﬁcient\n1e−7, 1e−6, 1e−5, 1e−4, 1e−3, 1e−2,\n1e−1, 1, 10, 3e−3, 5e−3, 3e−2, 5e−2, 3e−1,\n5e−1\nLine search max step\n1, 10, 100, 1000\nAdv. Train steps\n10\nAdv. Train learning rate\n0.1\nRuntime (line search/prox. method)\n< 20 minutes\nRuntime (others)\n< 2 minutes\nTable B.1: Range of Hyperparameters. Each run uses 2 CPU cores.\nSection 5.2.1 we need to provide the value of maximally robust ε to adversarial training\nfor ﬁnding a maximally robust classiﬁer. In our experiments, we give an advantage to\nadversarial training by providing it with the maximally robust ε. We also use the steepest\ndescent direction corresponding to the attack norm to solve the inner maximization.\nFor regularization methods a sufﬁciently small regularization coefﬁcient achieves max-\nimal robustness. Adversarial training given the maximal ε also converges to the same\nsolution. We tune all hyper-parameters for all methods including learning rate regularization\ncoefﬁcient and maximum step size in line search. We provide a list of values in Table B.1.\nB.5.2\nDetails of CIFAR-10 experiments\nFor Figs. 5.4c and 5.4d, the model is a WRN-28-10. We use SGD momentum (momentum\nset to 0.9) with a learning rate schedule that warms up from 0 to LR for 10 epochs, then\ndecays slowly using a cosine schedule back to zero over 200 epochs. LR is set to 0.1 *\nBS / 256, where batch size, BS, is set to 1024. Experiments runs on Google Cloud TPUv3\nover 32 cores. All models are trained from scratch and uses the default initialization from\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n103\n100\n101\nd/n\n0.0\n0.5\n1.0\n1.5\nEpsilon\n(a) ℓ∞attack\n100\n101\nd/n\n0\n2\n4\n6\nEpsilon\n(b) ℓ2 attack\n100\n101\nd/n\n0\n10\n20\n30\n40\n50\nEpsilon\n(c) ℓ1 attack\nCD\nGD+LS\nSignGD\nReg=L1\nReg=L2\nReg=Linf\nFigure B.1: Margin of models in Fig. 5.2. Models are trained to be robust against ℓ∞, ℓ2,\nℓ1 attacks. For each attack, there exists one optimizer and one regularization method that\nﬁnds the maximally robust classiﬁer. Adversarial training also ﬁnds the solution given the\nmaximal ε.\nJAX/Haiku. We use the KL loss and typical adversarial loss for adversarial training. The\ninner optimization either maximizes the KL divergence (for TRADES) or the cross-entropy\nloss (for AT) and we use Adam with a step-size of 0.1.\nFor the evaluation, we use 40 PGD steps (with Adam as the underlying optimizer and\nstep-size 0.1). Instead of optimizing the cross-entropy loss, we used the margin-loss [22].\nFor Fig. 5.5, we evaluate the models in Table B.2 against our Fourier-ℓp attack with\nvarying ε in the range [0, 8] × 255 with step size 0.5. We report the largest ε at which the\nrobust test accuracy is at most 1% lower than standard test accuracy of the model. We run\nthe attack for 20 iterations with no restarts and use apgd-ce, and apgd-dlr methods from\nAutoAttack.\nB.5.3\nMargin Figures\nA small gap exists between the solution found using CVXPY compared with coordinate\ndescent. That is because of limited number of training iterations. The convergence of\ncoordinate descent to minimum ℓ1 norm solution is slower than the convergence of gradient\ndescent to minimum ℓ2 norm solution. There is also a small gap between the solution of ℓ1\nregularization and CVXPY. The reason is the regularization coefﬁcient has to be inﬁnitesimal\nbut in practice numerical errors prevent us from training using very small regularization\ncoefﬁcients.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n104\nModel name\nRobust training type\nStandard\n-\nGowal2020Uncovering_70_16_extra\nLinf\nGowal2020Uncovering_28_10_extra\nLinf\nWu2020Adversarial_extra\nLinf\nCarmon2019Unlabeled\nLinf\nSehwag2020Hydra\nLinf\nGowal2020Uncovering_70_16\nLinf\nGowal2020Uncovering_34_20\nLinf\nWang2020Improving\nLinf\nWu2020Adversarial\nLinf\nHendrycks2019Using\nLinf\nGowal2020Uncovering_extra\nL2\nGowal2020Uncovering\nL2\nWu2020Adversarial\nL2\nAugustin2020Adversarial\nL2\nEngstrom2019Robustness\nL2\nRice2020Overfitting\nL2\nRice2020Overfitting\nL2\nRony2019Decoupling\nL2\nDing2020MMA\nL2\nHendrycks2020AugMix_ResNeXt\ncorruptions\nHendrycks2020AugMix_WRN\ncorruptions\nKireev2021Effectiveness_RLATAugMixNoJSDcorruptions\nKireev2021Effectiveness_AugMixNoJSD\ncorruptions\nKireev2021Effectiveness_Gauss50percentcorruptions\nKireev2021Effectiveness_RLAT\ncorruptions\nTable B.2: List of models evaluated in Fig. 5.5.\nB.5.4\nVisualization of Fourier Adversarial Attacks\nIn Figs. B.3, B.5 and B.7 we visualize adversarial samples for models available in Robust-\nBench [36]. Fourier-ℓ∞adversarial samples are qualitatively different from ℓ∞adversarial\nsamples as they concentrate on the object.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n105\n100\n101\nd/n\n0.25\n0.50\n0.75\n1.00\nEpsilon\nConv Linear,GD+LS\nLinear,Adv train,GD+LS\nLinear,CVXPY\nLinear,Reg=Fourier-L1\nLinear,Reg=L1\nLinear,Reg=Linf\nFigure B.2: Fourier-ℓ1 margin of Linear Convolutional Models.\nB.6\nVisualization of Norm-balls\nTo reach an intuition of the norm-ball for Fourier ℓ∞norm, we visualize a number of\ncommon norm-balls in 3D in Fig. B.9. Norm-balls have been visualized in prior work [10]\nbut we are not aware of any visualization of Fourier-ℓ∞.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n106\nx\nx + δ\nδ\n|F(δ)|\n(a) ℓ∞attack\nx\nx + δ\nδ\n|F(δ)|\n(b) Fourier-ℓ∞attack\nFigure B.3: Adversarial attacks (ℓ∞and Fourier-ℓ∞) against CIFAR-10 with standard\ntraining. WideResNet-28-10 model with standard training. The attack methods are APGD-\nCE and APGD-DLR with default hyper-parameters in RobustBench. We use ε = 8/255 for\nboth attacks. Fourier-ℓ∞perturbations are more concentrated on the object. Darker color in\nperturbations means larger magnitude. The optimal Fourier attack step is achieved when the\nmagnitude in the Fourier domain is equal to the constraints.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n107\nx\nx + δ\nδ\n|F(δ)|\n(a) ℓ∞attack\nx\nx + δ\nδ\n|F(δ)|\n(b) Fourier-ℓ∞attack\nFigure B.4: Adversarial attacks (High and low frequency Fourier-ℓ∞) against CIFAR-\n10 with standard training. WideResNet-28-10 model with standard training. The attack\nmethods are APGD-CE and APGD-DLR with default hyper-parameters in RobustBench.\nWe use ε = 15/255, 45/255 respectively for high and low frequency. Darker color in\nperturbations means larger magnitude. The optimal Fourier attack step is achieved when the\nmagnitude in the Fourier domain is equal to the constraints.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n108\nx\nx + δ\nδ\n|F(δ)|\n(a) ℓ∞attack\nx\nx + δ\nδ\n|F(δ)|\n(b) Fourier-ℓ∞attack\nFigure B.5: Adversarial attacks (ℓ∞and Fourier-ℓ∞) against CIFAR-10 ℓ∞model of\n[23]. Adversarially trained model against ℓ∞attacks. The attack methods are APGD-CE\nand APGD-DLR with default hyper-parameters in RobustBench. We use ε = 8/255 for\nboth attacks. Fourier-ℓ∞perturbations are more concentrated on the object. Darker color in\nperturbations means larger magnitude. The optimal Fourier attack step is achieved when the\nmagnitude in the Fourier domain is equal to the constraints.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n109\nx\nx + δ\nδ\n|F(δ)|\n(a) ℓ∞attack\nx\nx + δ\nδ\n|F(δ)|\n(b) Fourier-ℓ∞attack\nFigure B.6: Adversarial attacks (High and low frequency Fourier-ℓ∞) against CIFAR-\n10 ℓ∞model of [23]. WideResNet-28-10 model with standard training. The attack methods\nare APGD-CE and APGD-DLR with default hyper-parameters in RobustBench. We use\nε = 15/255, 45/255 respectively for high and low frequency. Darker color in perturbations\nmeans larger magnitude. The optimal Fourier attack step is achieved when the magnitude in\nthe Fourier domain is equal to the constraints.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n110\nx\nx + δ\nδ\n|F(δ)|\n(a) ℓ∞attack\nx\nx + δ\nδ\n|F(δ)|\n(b) Fourier-ℓ∞attack\nFigure B.7: Adversarial attacks (ℓ∞and Fourier-ℓ∞) against CIFAR-10 ℓ2 model of\n[7]. Adversarially trained model against ℓ2 attacks. The attack methods are APGD-CE and\nAPGD-DLR with default hyper-parameters in RobustBench. We use ε = 8/255 for both\nattacks. Fourier-ℓ∞perturbations are more concentrated on the object. Darker color in\nperturbations means larger magnitude. The optimal Fourier attack step is achieved when the\nmagnitude in the Fourier domain is equal to the constraints.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n111\nx\nx + δ\nδ\n|F(δ)|\n(a) ℓ∞attack\nx\nx + δ\nδ\n|F(δ)|\n(b) Fourier-ℓ∞attack\nFigure B.8: Adversarial attacks (High and low frequency Fourier-ℓ∞) against CIFAR-\n10 ℓ2 model of [7]. WideResNet-28-10 model with standard training. The attack methods\nare APGD-CE and APGD-DLR with default hyper-parameters in RobustBench. We use\nε = 15/255, 45/255 respectively for high and low frequency. Darker color in perturbations\nmeans larger magnitude. The optimal Fourier attack step is achieved when the magnitude in\nthe Fourier domain is equal to the constraints.\nAPPENDIX B. APPENDICES TO THE CHAPTER ON BRIDGING THE GAP CHAPTER\n112\n(a) ∥δ∥2 = 1\n(b) ∥δ∥1 = 1\n(c) ∥δ∥∞= 1\n(d) ∥F(δ)∥∞= 1\nFigure B.9: Unit norm balls in 3-D (red) and their 2-D projections (green). Linear models\ntrained with gradient descent are maximally robust to ℓ2 perturbations. Two-layer linear\nconvolutional networks trained with gradient descent are maximally robust to perturbations\nwith bounded Fourier-ℓ∞.\nBibliography\n[1]\nAishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence\nZitnick, Devi Parikh, and Dhruv Batra. “VQA: Visual question answering”. In:\nInternational Journal of Computer Vision (IJCV) 123.1 (2017), pp. 4–31.\n[2]\nGuillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua\nBengio. “Variance Reduction in SGD by Distributed Importance Sampling”. In: arXiv\ne-prints, arXiv:1511.06481 (Nov. 2015), arXiv:1511.06481. arXiv: 1511.06481\n[stat.ML].\n[3]\nShun-Ichi Amari. “Natural gradient works efﬁciently in learning”. In: Neural com-\nputation 10.2 (1998), pp. 251–276.\n[4]\nShun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji\nSuzuki, Denny Wu, and Ji Xu. “When Does Preconditioning Help or Hurt General-\nization?” In: arXiv preprint arXiv:2006.10732 (2020).\n[5]\nRohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. “Scalable\nsecond order optimization for deep learning”. In: arXiv preprint arXiv:2002.09018 7\n(2021), p. 15.\n[6]\nAnish Athalye, Nicholas Carlini, and David Wagner. “Obfuscated gradients give a\nfalse sense of security: Circumventing defenses to adversarial examples”. In: arXiv\npreprint arXiv:1802.00420 (2018).\n113\nBIBLIOGRAPHY\n114\n[7]\nMaximilian Augustin, Alexander Meinke, and Matthias Hein. “Adversarial robust-\nness on in-and out-distribution improves explainability”. In: European Conference\non Computer Vision. Springer. 2020, pp. 228–245.\n[8]\nGeorge Awad, Asad A. Butt, Keith Curtis, Yooyoung Lee, Jonathan Fiscus, Afzal\nGodil, Andrew Delgado, Jesse Zhang, Eliot Godard, Lukas Diduch, Jeffrey Liu,\nAlan F. Smeaton, Yvette Graham, Gareth J. F. Jones, Wessel Kraaij, and Georges\nQuénot. “TRECVID 2020: comprehensive campaign for evaluating video retrieval\ntasks across multiple application domains”. In: Proceedings of TRECVID 2020.\nNIST, USA. 2020.\n[9]\nMuhammad Awais, Fahad Shamshad, and Sung-Ho Bae. “Towards an Adversarially\nRobust Normalization Approach”. In: CoRR abs/2006.11007 (2020).\n[10]\nFrancis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. “Struc-\ntured sparsity through convex optimization”. In: Statistical Science 27.4 (2012),\npp. 450–468.\n[11]\nSue Becker, Yann Le Cun, et al. “Improving the convergence of back-propagation\nlearning with second order methods”. In: Proceedings of the 1988 connectionist\nmodels summer school. San Matteo, CA: Morgan Kaufmann. 1988, pp. 29–37.\n[12]\nAharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization.\nVol. 28. Princeton University Press, 2009.\n[13]\nYoshua Bengio and Jean-Sébastien Senecal. “Adaptive Importance Sampling to\nAccelerate Training of a Neural Probabilistic Language Model”. In: IEEE Trans.\nNeural Networks 19.4 (2008), pp. 713–722.\n[14]\nYoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. “Curriculum\nlearning”. In: International Conference on Machine Learning (ICML). Vol. 382.\nACM International Conference Proceeding Series. ACM, 2009, pp. 41–48.\n[15]\nChristopher M. Bishop. Pattern recognition and machine learning. Springer, 2007.\nBIBLIOGRAPHY\n115\n[16]\nOndˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn,\nJohannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand,\net al. “Findings of the 2014 workshop on statistical machine translation”. In: Pro-\nceedings of the ninth workshop on statistical machine translation. 2014, pp. 12–\n58.\n[17]\nLéon Bottou, Frank E Curtis, and Jorge Nocedal. “Optimization methods for large-\nscale machine learning”. In: arXiv preprint arXiv:1606.04838 (2016).\n[18]\nLéon Bottou, Frank E. Curtis, and Jorge Nocedal. “Optimization Methods for Large-\nScale Machine Learning”. In: SIAM Review 60.2 (2018), pp. 223–311.\n[19]\nStephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university\npress, 2004.\n[20]\nTom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. “Ad-\nversarial Patch”. In: arXiv e-prints, arXiv:1712.09665 (Dec. 2017), arXiv:1712.09665.\narXiv: 1712.09665 [cs.CV].\n[21]\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. “Language Models are Few-Shot Learners”. In: Neural Information\nProcessing Systems (NeurIPS). 2020.\n[22]\nNicholas Carlini and David Wagner. “Towards evaluating the robustness of neural\nnetworks”. In: 2017 ieee symposium on security and privacy (sp). IEEE. 2017,\npp. 39–57.\nBIBLIOGRAPHY\n116\n[23]\nYair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang.\n“Unlabeled Data Improves Adversarial Robustness”. In: NeurIPS. 2019, pp. 11190–\n11201.\n[24]\nJosue Ortega Caro, Yilong Ju, Ryan Pyle, and Ankit Patel. “Using Learning Dy-\nnamics to Explore the Role of Implicit Regularization in Adversarial Examples”. In:\narXiv preprint arXiv:2006.11440 (2020).\n[25]\nOlivier Chapelle, Quoc Le, and Alex Smola. “Large margin optimization of ranking\nmeasures”. In: NIPS workshop: Machine learning for Web search. 2007.\n[26]\nGal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. “Large scale online\nlearning of image similarity through ranking”. In: Journal of Machine Learning\nResearch (JMLR) 11.Mar (2010), pp. 1109–1135.\n[27]\nBeidi Chen, Yingchen Xu, and Anshumali Shrivastava. “Fast and Accurate Stochastic\nGradient Estimation”. In: Neural Information Processing Systems (NeurIPS). 2019,\npp. 12339–12349.\n[28]\nHongming Chen, Ola Engkvist, Yinhai Wang, Marcus Olivecrona, and Thomas\nBlaschke. “The rise of deep learning in drug discovery”. In: Drug discovery today\n23.6 (2018), pp. 1241–1250.\n[29]\nJiacheng Chen, Hexiang Hu, Hao Wu, Yuning Jiang, and Changhu Wang. “Learn-\ning the best pooling strategy for visual semantic embedding”. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021,\npp. 15789–15798.\n[30]\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. “A simple\nframework for contrastive learning of visual representations”. In: arXiv preprint\narXiv:2002.05709 (2020).\n[31]\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. “Improved baselines with\nmomentum contrastive learning”. In: arXiv preprint arXiv:2003.04297 (2020).\nBIBLIOGRAPHY\n117\n[32]\nLenaic Chizat and Francis Bach. “Implicit bias of gradient descent for wide two-layer\nneural networks trained with the logistic loss”. In: Conference on Learning Theory.\nPMLR. 2020, pp. 1305–1338.\n[33]\nDami Choi, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J Maddison,\nand George E Dahl. “On empirical comparisons of optimizers for deep learning”. In:\narXiv preprint arXiv:1910.05446 (2019).\n[34]\nJeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. “Certiﬁed adversarial robust-\nness via randomized smoothing”. In: arXiv preprint arXiv:1902.02918 (2019).\n[35]\nFrancesco Croce and Matthias Hein. “Reliable evaluation of adversarial robustness\nwith an ensemble of diverse parameter-free attacks”. In: ICML. Vol. 119. Proceedings\nof Machine Learning Research. PMLR, 2020, pp. 2206–2216.\n[36]\nFrancesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion,\nMung Chiang, Prateek Mittal, and Matthias Hein. “RobustBench: a standardized\nadversarial robustness benchmark”. In: arXiv preprint arXiv:2010.09670 (2020).\n[37]\nDominik Csiba and Peter Richtárik. “Importance sampling for minibatches”. In: The\nJournal of Machine Learning Research 19.1 (2018), pp. 962–982.\n[38]\nNavneet Dalal and Bill Triggs. “Histograms of oriented gradients for human detec-\ntion”. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nVol. 1. IEEE. 2005, pp. 886–893.\n[39]\nAaron Defazio and Léon Bottou. “On the Ineffectiveness of Variance Reduced Opti-\nmization for Deep Learning”. In: Neural Information Processing Systems (NeurIPS).\n2019, pp. 1753–1763.\n[40]\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. “ImageNet: A\nlarge-scale hierarchical image database”. In: IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR). IEEE Computer Society, 2009, pp. 248–255.\nBIBLIOGRAPHY\n118\n[41]\nZeyu Deng, Abla Kammoun, and Christos Thrampoulidis. “A Model of Double\nDescent for High-dimensional Binary Linear Classiﬁcation”. In: arXiv e-prints,\narXiv:1911.05822 (Nov. 2019), arXiv:1911.05822. arXiv: 1911.05822 [stat.ML].\n[42]\nSteven Diamond and Stephen Boyd. “CVXPY: A Python-embedded modeling\nlanguage for convex optimization”. In: Journal of Machine Learning Research 17.83\n(2016), pp. 1–5.\n[43]\nGavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. “Max-\nMargin Adversarial (MMA) Training: Direct Input Space Margin Maximization\nthrough Adversarial Training”. In: CoRR abs/1812.02637 (2018). arXiv: 1812.\n02637. URL: http://arxiv.org/abs/1812.02637.\n[44]\nEdgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. “Provable\ntradeoffs in adversarially robust classiﬁcation”. In: arXiv e-prints, arXiv:2006.05161\n(June 2020), arXiv:2006.05161. arXiv: 2006.05161 [cs.LG].\n[45]\nJohn Duchi, Elad Hazan, and Yoram Singer. “Adaptive subgradient methods for\nonline learning and stochastic optimization”. In: Journal of Machine Learning\nResearch 12.Jul (2011), pp. 2121–2159.\n[46]\nAviv Eisenschtat and Lior Wolf. “Linking Image and Text with 2-Way Nets”. In:\n(2017).\n[47]\nGamaleldin F. Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy\nBengio. Large Margin Deep Networks for Classiﬁcation. 2018. arXiv: 1803 .\n05598 [stat.ML].\n[48]\nFartash Faghri, David Duvenaud, David J Fleet, and Jimmy Ba. “A Study of Gradient\nVariance in Deep Learning”. In: arXiv preprint arXiv:2007.04532 (2020).\n[49]\nFartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel M. Roy, and\nAli Ramezani-Kebrya. “Adaptive Gradient Quantization for Data-Parallel SGD”. In:\nNeurIPS. 2020.\nBIBLIOGRAPHY\n119\n[50]\nFartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. “VSE++: Improv-\ning Visual-Semantic Embeddings with Hard Negatives”. In: British Machine Vision\nConference 2018, BMVC 2018, Newcastle, UK, September 3-6, 2018. BMVA Press,\n2018, p. 12. URL: http://bmvc2018.org/contents/papers/0344.\npdf.\n[51]\nAlhussein Fawzi, Hamza Fawzi, and Omar Fawzi. “Adversarial vulnerability for any\nclassiﬁer”. In: Advances in neural information processing systems. 2018, pp. 1178–\n1187.\n[52]\nAlhussein Fawzi, Omar Fawzi, and Pascal Frossard. “Analysis of classiﬁers’ robust-\nness to adversarial perturbations”. In: Machine Learning 107.3 (2018), pp. 481–\n508.\n[53]\nPedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan.\n“Object detection with discriminatively trained part-based models”. In: IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence (PAMI) 32.9 (2010), pp. 1627–\n1645.\n[54]\nChelsea Finn, Pieter Abbeel, and Sergey Levine. “Model-agnostic meta-learning\nfor fast adaptation of deep networks”. In: International Conference on Machine\nLearning. PMLR. 2017, pp. 1126–1135.\n[55]\nMichael P Friedlander and Mark Schmidt. “Hybrid deterministic-stochastic methods\nfor data ﬁtting”. In: SIAM Journal on Scientiﬁc Computing 34.3 (2012), A1380–\nA1405.\n[56]\nAndrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas\nMikolov, et al. “Devise: A deep visual-semantic embedding model”. In: Neural\nInformation Processing Systems (NeurIPS). 2013, pp. 2121–2129.\n[57]\nAndrea Frome, Yoram Singer, Fei Sha, and Jitendra Malik. “Learning globally-\nconsistent local distance functions for shape-based image retrieval and classiﬁcation”.\nIn: International Conference in Computer Vision (ICCV). IEEE. 2007, pp. 1–8.\nBIBLIOGRAPHY\n120\n[58]\nAngus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham W.\nTaylor. “Batch Normalization is a Cause of Adversarial Vulnerability”. In: CoRR\nabs/1905.02161 (2019).\n[59]\nAmirata Ghorbani, Abubakar Abid, and James Zou. “Interpretation of neural net-\nworks is fragile”. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence.\nVol. 33. 01. 2019, pp. 3681–3688.\n[60]\nJustin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu,\nMartin Wattenberg, and Ian Goodfellow. “Adversarial Spheres”. In: arXiv e-prints,\narXiv:1801.02774 (Jan. 2018), arXiv:1801.02774. arXiv: 1801.02774 [cs.CV].\n[61]\nXavier Glorot and Yoshua Bengio. “Understanding the difﬁculty of training deep\nfeedforward neural networks”. In: Proceedings of the thirteenth international confer-\nence on artiﬁcial intelligence and statistics. 2010, pp. 249–256.\n[62]\nGabriel Goh. “Why Momentum Really Works”. In: Distill (2017). DOI: 10.23915/\ndistill.00006. URL: http://distill.pub/2017/momentum.\n[63]\nNoah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gholami,\nKai Rothauge, Michael W. Mahoney, and Joseph Gonzalez. “On the Computational\nInefﬁciency of Large Batch Sizes for Stochastic Gradient Descent”. In: arXiv e-\nprints, arXiv:1811.12941 (Nov. 2018), arXiv:1811.12941. arXiv: 1811.12941\n[cs.LG].\n[64]\nIan Goodfellow. “Efﬁcient Per-Example Gradient Computations”. In: arXiv e-prints,\narXiv:1510.01799 (Oct. 2015), arXiv:1510.01799. arXiv: 1510.01799 [stat.ML].\n[65]\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learn-\ning. Vol. 1. MIT press Cambridge, 2016.\n[66]\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. “Generative Adversarial Nets”.\nIn: Neural Information Processing Systems (NeurIPS). Ed. by Z. Ghahramani, M.\nWelling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger. Curran Associates,\nBIBLIOGRAPHY\n121\nInc., 2014, pp. 2672–2680. URL: http://papers.nips.cc/paper/5423-\ngenerative-adversarial-nets.pdf.\n[67]\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining and harness-\ning adversarial examples”. In: arXiv preprint arXiv:1412.6572 (2014).\n[68]\nIan J. Goodfellow and Oriol Vinyals. “Qualitatively characterizing neural network\noptimization problems”. In: International Conference on Learning Representations\n(ICLR). 2015.\n[69]\nSven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli\nQin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli.\n“On the Effectiveness of Interval Bound Propagation for Training Veriﬁably Robust\nModels”. In: arXiv e-prints, arXiv:1810.12715 (Oct. 2018), arXiv:1810.12715. arXiv:\n1810.12715 [cs.LG].\n[70]\nSven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.\n“Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial\nExamples”. In: arXiv e-prints, arXiv:2010.03593 (Oct. 2020), arXiv:2010.03593.\narXiv: 2010.03593 [stat.ML].\n[71]\nSorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. “A survey\nof deep learning techniques for autonomous driving”. In: Journal of Field Robotics\n37.3 (2020), pp. 362–386.\n[72]\nSuriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. “Characterizing im-\nplicit bias in terms of optimization geometry”. In: arXiv preprint arXiv:1802.08246\n(2018).\n[73]\nSuriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. “Implicit bias\nof gradient descent on linear convolutional networks”. In: Advances in Neural\nInformation Processing Systems. 2018, pp. 9461–9471.\nBIBLIOGRAPHY\n122\n[74]\nChuan Guo, Jared S. Frank, and Kilian Q. Weinberger. “Low Frequency Adversarial\nPerturbation”. In: Proceedings of the Thirty-Fifth Conference on Uncertainty in\nArtiﬁcial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019. Ed. by Amir\nGloberson and Ricardo Silva. Vol. 115. Proceedings of Machine Learning Research.\nAUAI Press, 2019, pp. 1127–1137. URL: http://proceedings.mlr.press/\nv115/guo20a.html.\n[75]\nYiwen Guo, Long Chen, Yurong Chen, and Changshui Zhang. “On connections\nbetween regularizations for improving dnn robustness”. In: IEEE transactions on\npattern analysis and machine intelligence (2020).\n[76]\nRaia Hadsell, Sumit Chopra, and Yann LeCun. “Dimensionality Reduction by Learn-\ning an Invariant Mapping”. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). IEEE Computer Society, 2006, pp. 1735–1742.\n[77]\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. “Sur-\nprises in High-Dimensional Ridgeless Least Squares Interpolation”. In: arXiv e-\nprints, arXiv:1903.08560 (Mar. 2019), arXiv:1903.08560. arXiv: 1903.08560\n[math.ST].\n[78]\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep Residual Learning\nfor Image Recognition”. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). IEEE Computer Society, 2016, pp. 770–778.\n[79]\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet classiﬁcation”. In: Pro-\nceedings of the IEEE international conference on computer vision. 2015, pp. 1026–\n1034.\n[80]\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. “Momentum\ncontrast for unsupervised visual representation learning”. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020, pp. 9729–\n9738.\nBIBLIOGRAPHY\n123\n[81]\nMatthias Hein and Maksym Andriushchenko. “Formal guarantees on the robustness\nof a classiﬁer against adversarial manipulation”. In: Advances in neural information\nprocessing systems. 2017, pp. 2266–2276.\n[82]\nSepp Hochreiter. “The vanishing gradient problem during learning recurrent neural\nnets and problem solutions”. In: International Journal of Uncertainty, Fuzziness and\nKnowledge-Based Systems 6.02 (1998), pp. 107–116.\n[83]\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. “Learning to learn\nusing gradient descent”. In: International Conference on Artiﬁcial Neural Networks.\nSpringer. 2001, pp. 87–94.\n[84]\nMicah Hodosh, Peter Young, and Julia Hockenmaier. “Framing image description\nas a ranking task: Data, models and evaluation metrics”. In: Journal of Artiﬁcial\nIntelligence Research 47 (2013), pp. 853–899.\n[85]\nThomas Hofmann, Aurélien Lucchi, Simon Lacoste-Julien, and Brian McWilliams.\n“Variance Reduced Stochastic Gradient Descent with Neighbors”. In: Neural Infor-\nmation Processing Systems (NeurIPS). 2015, pp. 2305–2313.\n[86]\nYan Huang, Wei Wang, and Liang Wang. “Instance-aware Image and Sentence\nMatching with Selective Multimodal LSTM”. In: (2017).\n[87]\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran,\nand Aleksander Madry. “Adversarial examples are not bugs, they are features”. In:\nAdvances in Neural Information Processing Systems. 2019, pp. 125–136.\n[88]\nArthur Jacot, Clément Hongler, and Franck Gabriel. “Neural Tangent Kernel: Conver-\ngence and Generalization in Neural Networks”. In: Neural Information Processing\nSystems (NeurIPS). 2018, pp. 8580–8589.\n[89]\nAdel Javanmard and Mahdi Soltanolkotabi. “Precise Statistical Analysis of Classiﬁ-\ncation Accuracies for Adversarial Training”. In: arXiv e-prints, arXiv:2010.11213\n(Oct. 2020), arXiv:2010.11213. arXiv: 2010.11213 [stat.ML].\nBIBLIOGRAPHY\n124\n[90]\nAdel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. “Precise Tradeoffs in\nAdversarial Training for Linear Regression”. In: COLT. Vol. 125. Proceedings of\nMachine Learning Research. PMLR, 2020, pp. 2034–2078.\n[91]\nZiwei Ji and Matus Telgarsky. “Directional convergence and alignment in deep\nlearning”. In: NeurIPS. 2020.\n[92]\nZiwei Ji and Matus Telgarsky. “Gradient descent aligns the layers of deep linear\nnetworks”. In: arXiv preprint arXiv:1810.02032 (2018).\n[93]\nAngela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean,\nGregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C\nLipton, et al. “Accelerating Deep Learning by Focusing on the Biggest Losers”. In:\narXiv preprint arXiv:1910.00762 (2019).\n[94]\nLu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. “MentorNet:\nLearning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted\nLabels”. In: arXiv e-prints, arXiv:1712.05055 (Dec. 2017), arXiv:1712.05055. arXiv:\n1712.05055 [cs.CV].\n[95]\nRie Johnson and Tong Zhang. “Accelerating Stochastic Gradient Descent using Pre-\ndictive Variance Reduction”. In: Neural Information Processing Systems (NeurIPS).\n2013, pp. 315–323.\n[96]\nTyler B Johnson and Carlos Guestrin. “Training deep models faster with robust,\napproximate importance sampling”. In: Advances in Neural Information Processing\nSystems 31 (2018), pp. 7265–7275.\n[97]\nRyo Karakida, Shotaro Akaho, and Shun-ichi Amari. “Pathological spectra of the\nFisher information metric and its variants in deep neural networks”. In: arXiv preprint\narXiv:1910.05992 (2019).\n[98]\nAndrej Karpathy and Li Fei-Fei. “Deep visual-semantic alignments for generat-\ning image descriptions”. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR). 2015, pp. 3128–3137.\nBIBLIOGRAPHY\n125\n[99]\nAngelos Katharopoulos and François Fleuret. “Biased Importance Sampling for\nDeep Neural Network Training”. In: arXiv e-prints, arXiv:1706.00043 (May 2017),\narXiv:1706.00043. arXiv: 1706.00043 [cs.LG].\n[100]\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,\nand Ping Tak Peter Tang. “On Large-Batch Training for Deep Learning: Generaliza-\ntion Gap and Sharp Minima”. In: International Conference on Learning Representa-\ntions (ICLR). OpenReview.net, 2017.\n[101]\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof\nT Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. “The (un) reliability of\nsaliency methods”. In: Explainable AI: Interpreting, Explaining and Visualizing\nDeep Learning. Springer, 2019, pp. 267–280.\n[102]\nDiederik Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In:\n(2015).\n[103]\nRyan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. “Unifying visual-semantic\nembeddings with multimodal neural language models”. In: (2014).\n[104]\nBenjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. “Associating neural word em-\nbeddings with deep image representations using ﬁsher vectors”. In: IEEE Conference\non Computer Vision and Pattern Recognition (CVPR). 2015, pp. 4437–4446.\n[105]\nAlex Krizhevsky et al. “Learning multiple layers of features from tiny images”. In:\n(2009).\n[106]\nFrederik Kunstner, Philipp Hennig, and Lukas Balles. “Limitations of the empir-\nical Fisher approximation for natural gradient descent”. In: Neural Information\nProcessing Systems (NeurIPS). 2019, pp. 4158–4169.\n[107]\nAlexey Kurakin, Ian J. Goodfellow, and Samy Bengio. “Adversarial examples in the\nphysical world”. In: ICLR (Workshop). OpenReview.net, 2017.\nBIBLIOGRAPHY\n126\n[108]\nQuoc Le and Alexander Smola. “Direct optimization of ranking measures”. In: arXiv\npreprint arXiv:0704.3359 (2007).\n[109]\nNicolas Le Roux, Yoshua Bengio, and Andrew Fitzgibbon. “Improving ﬁrst and\nsecond-order methods by modeling uncertainty”. In: Optimization for Machine\nLearning (2011), p. 403.\n[110]\nNicolas Le Roux, Mark Schmidt, and Francis R. Bach. “A Stochastic Gradient\nMethod with an Exponential Convergence Rate for Finite Training Sets”. In: Neural\nInformation Processing Systems (NeurIPS). 2012, pp. 2672–2680.\n[111]\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. “Gradient-based\nlearning applied to document recognition”. In: Proceedings of the IEEE 86.11 (1998),\npp. 2278–2324.\n[112]\nHang Li. “Learning to rank for information retrieval and natural language process-\ning”. In: Synthesis Lectures on Human Language Technologies 7.3 (2014), pp. 1–\n121.\n[113]\nSijin Li, Weichen Zhang, and Antoni B Chan. “Maximum-margin structured learning\nwith deep networks for 3d human pose estimation”. In: International Conference in\nComputer Vision (ICCV). 2015, pp. 2848–2856.\n[114]\nYan Li, Ethan X Fang, Huan Xu, and Tuo Zhao. “Implicit bias of gradient de-\nscent based adversarial training on separable data”. In: International Conference on\nLearning Representations. 2019.\n[115]\nYangyan Li, Hao Su, Charles Ruizhongtai Qi, Noa Fish, Daniel Cohen-Or, and\nLeonidas J Guibas. “Joint embeddings of shapes and images via CNN image puriﬁ-\ncation.” In: 34.6 (2015), pp. 234–1.\n[116]\nTao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. “Don’t Use\nLarge Mini-batches, Use Local SGD”. In: International Conference on Learning\nRepresentations (ICLR). OpenReview.net, 2020.\nBIBLIOGRAPHY\n127\n[117]\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. “Focal loss\nfor dense object detection”. In: Proceedings of the IEEE international conference on\ncomputer vision. 2017, pp. 2980–2988.\n[118]\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Dollár, and C Lawrence Zitnick. “Microsoft coco: Common objects\nin context”. In: European Conference on Computer Vision (ECCV). Springer. 2014,\npp. 740–755.\n[119]\nHsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai, and Alec\nJacobson. “Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically\nDifferentiable Renderer”. In: ICLR (Poster). OpenReview.net, 2019.\n[120]\nDavid Lopez-Paz et al. “Gradient Episodic Memory for Continual Learning”. In:\nAdvances in Neural Information Processing Systems. 2017, pp. 6470–6479.\n[121]\nChunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. “A uniﬁed gradient regulariza-\ntion family for adversarial examples”. In: 2015 IEEE International Conference on\nData Mining. IEEE. 2015, pp. 301–309.\n[122]\nKaifeng Lyu and Jian Li. “Gradient Descent Maximizes the Margin of Homogeneous\nNeural Networks”. In: International Conference on Learning Representations (ICLR).\nOpenReview.net, 2020.\n[123]\nAvery Ma, Fartash Faghri, and Amir-massoud Farahmand. “Adversarial Robustness\nthrough Regularization: A Second-Order Approach”. In: arXiv e-prints, arXiv:2004.01832\n(Apr. 2020), arXiv:2004.01832. arXiv: 2004.01832 [cs.LG].\n[124]\nLinhai Ma and Liang Liang. “Increasing-Margin Adversarial (IMA) Training to\nImprove Adversarial Robustness of Neural Networks”. In: CoRR abs/2005.09147\n(2020). arXiv: 2005 . 09147. URL: https : / / arxiv . org / abs / 2005 .\n09147.\nBIBLIOGRAPHY\n128\n[125]\nShuang Ma, Zhaoyang Zeng, Daniel McDuff, and Yale Song. “Active Contrastive\nLearning of Audio-Visual Video Representations”. In: International Conference on\nLearning Representations. 2021. URL: https://openreview.net/forum?\nid=OMizHuea_HB.\n[126]\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. “Towards deep learning models resistant to adversarial attacks”. In:\narXiv preprint arXiv:1706.06083 (2017).\n[127]\nPratyush Maini, Eric Wong, and Zico Kolter. “Adversarial robustness against the\nunion of multiple perturbation models”. In: International Conference on Machine\nLearning. PMLR. 2020, pp. 6640–6650.\n[128]\nMateusz Malinowski, Marcus Rohrbach, and Mario Fritz. “Ask Your Neurons: A\nNeural-based Approach to Answering Questions about Images”. In: International\nConference in Computer Vision (ICCV). 2015.\n[129]\nTomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. “Ensemble of exemplar-\nsvms for object detection and beyond”. In: International Conference in Computer\nVision (ICCV). IEEE. 2011, pp. 89–96.\n[130]\nJames Martens. “Deep learning via Hessian-free optimization.” In: ICML. Vol. 27.\n2010, pp. 735–742.\n[131]\nJames Martens. “New insights and perspectives on the natural gradient method”. In:\narXiv e-prints, arXiv:1412.1193 (Dec. 2014), arXiv:1412.1193. arXiv: 1412.1193\n[cs.LG].\n[132]\nJames Martens and Roger Grosse. “Optimizing neural networks with kronecker-\nfactored approximate curvature”. In: International conference on machine learning.\n2015, pp. 2408–2417.\n[133]\nJames Martens and Roger B. Grosse. “Optimizing Neural Networks with Kronecker-\nfactored Approximate Curvature”. In: International Conference on Machine Learn-\nBIBLIOGRAPHY\n129\ning (ICML). Vol. 37. JMLR Workshop and Conference Proceedings. JMLR.org,\n2015, pp. 2408–2417.\n[134]\nJames Martens and Ilya Sutskever. “Training deep and recurrent networks with\nhessian-free optimization”. In: Neural networks: Tricks of the trade. Springer, 2012,\npp. 479–535.\n[135]\nDominic Masters and Carlo Luschi. “Revisiting Small Batch Training for Deep Neu-\nral Networks”. In: arXiv e-prints, arXiv:1804.07612 (Apr. 2018), arXiv:1804.07612.\narXiv: 1804.07612 [cs.LG].\n[136]\nSong Mei and Andrea Montanari. “The generalization error of random features\nregression: Precise asymptotics and double descent curve”. In: arXiv e-prints,\narXiv:1908.05355 (Aug. 2019), arXiv:1908.05355. arXiv: 1908.05355 [math.ST].\n[137]\nShakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. “Monte\nCarlo Gradient Estimation in Machine Learning.” In: J. Mach. Learn. Res. 21.132\n(2020), pp. 1–62.\n[138]\nAndrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. “The generalization\nerror of max-margin linear classiﬁers: High-dimensional asymptotics in the over-\nparametrized regime”. In: arXiv preprint arXiv:1911.01544 (2019).\n[139]\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal\nFrossard. “Robustness via curvature regularization, and vice versa”. In: Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019,\npp. 9078–9086.\n[140]\nEdward Moroshko, Blake E. Woodworth, Suriya Gunasekar, Jason D. Lee, Nati Sre-\nbro, and Daniel Soudry. “Implicit Bias in Deep Linear Classiﬁcation: Initialization\nScale vs Training Accuracy”. In: (2020). Ed. by Hugo Larochelle, Marc’Aurelio\nRanzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. URL: https://\nproceedings.neurips.cc/paper/2020/hash/fc2022c89b61c76bbef978f1370660bf-\nAbstract.html.\nBIBLIOGRAPHY\n130\n[141]\nEric Moulines and Francis Bach. “Non-asymptotic analysis of stochastic approxima-\ntion algorithms for machine learning”. In: Advances in neural information processing\nsystems 24 (2011), pp. 451–459.\n[142]\nKevin Murphy. Machine learning, a probabilistic perspective. 2014.\n[143]\nMor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona\nSavarese, Nathan Srebro, and Daniel Soudry. “Convergence of gradient descent on\nseparable data”. In: The 22nd International Conference on Artiﬁcial Intelligence and\nStatistics. PMLR. 2019, pp. 3420–3428.\n[144]\nZachary Nado, Justin M Gilmer, Christopher J Shallue, Rohan Anil, and George E\nDahl. “A large batch optimizer reality check: Traditional, generic optimizers sufﬁce\nacross batch sizes”. In: arXiv preprint arXiv:2102.06356 (2021).\n[145]\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya\nSutskever. “Deep Double Descent: Where Bigger Models and More Data Hurt”.\nIn: CoRR abs/1912.02292 (2019). arXiv: 1912.02292. URL: http://arxiv.\norg/abs/1912.02292.\n[146]\nHyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. “Dual attention networks for\nmultimodal reasoning and matching”. In: (2017).\n[147]\nRadford M Neal. “Bayesian Learning for Neural Networks”. PhD thesis. University\nof Toronto, 1995.\n[148]\nDeanna Needell, Rachel Ward, and Nathan Srebro. “Stochastic Gradient Descent,\nWeighted Sampling, and the Randomized Kaczmarz algorithm”. In: Neural Informa-\ntion Processing Systems (NeurIPS). 2014, pp. 1017–1025.\n[149]\nYurii Nesterov. “A method for unconstrained convex minimization problem with the\nrate of convergence O (1/kˆ 2)”. In: Doklady AN USSR. Vol. 269. 1983, pp. 543–547.\n[150]\nJorge Nocedal and Stephen Wright. Numerical optimization. Springer Science &\nBusiness Media, 2006.\nBIBLIOGRAPHY\n131\n[151]\nGreg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. “A Function Space\nView of Bounded Norm Inﬁnite Width ReLU Nets: The Multivariate Case”. In:\nInternational Conference on Learning Representations (ICLR). OpenReview.net,\n2020.\n[152]\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. “Representation learning with\ncontrastive predictive coding”. In: arXiv preprint arXiv:1807.03748 (2018).\n[153]\nGuillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and\nPascal Frossard. “Hold me tight! Inﬂuence of discriminative features on deep network\nboundaries”. In: arXiv preprint arXiv:2002.06349 (2020).\n[154]\nNeal Parikh and Stephen Boyd. “Proximal algorithms”. In: Foundations and Trends\nin Optimization (2013).\n[155]\nJeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. “Resurrecting the\nsigmoid in deep learning through dynamical isometry: theory and practice”. In:\nAdvances in neural information processing systems. 2017, pp. 4785–4795.\n[156]\nBoris T Polyak. “Some methods of speeding up the convergence of iteration meth-\nods”. In: USSR Computational Mathematics and Mathematical Physics 4.5 (1964),\npp. 1–17.\n[157]\nMartin Popel, Marketa Tomkova, Jakub Tomek, Łukasz Kaiser, Jakob Uszkoreit,\nOndˇrej Bojar, and Zdenˇek Žabokrtsk`y. “Transforming machine translation: a deep\nlearning system reaches news translation quality comparable to human professionals”.\nIn: Nature communications 11.1 (2020), pp. 1–15.\n[158]\nChongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvi-\njotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. “Adver-\nsarial robustness through local linearization”. In: Advances in Neural Information\nProcessing Systems. 2019, pp. 13847–13856.\nBIBLIOGRAPHY\n132\n[159]\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n“Learning transferable visual models from natural language supervision”. In: arXiv\npreprint arXiv:2103.00020 (2021).\n[160]\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. “SVCCA:\nSingular Vector Canonical Correlation Analysis for Deep Learning Dynamics\nand Interpretability”. In: Neural Information Processing Systems (NeurIPS). 2017,\npp. 6076–6085.\n[161]\nAditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang.\n“Understanding and mitigating the tradeoff between robustness and accuracy”. In:\narXiv preprint arXiv:2002.10716 (2020).\n[162]\nAli Rahimi and Benjamin Recht. “Random Features for Large-Scale Kernel Ma-\nchines”. In: NIPS. Curran Associates, Inc., 2007, pp. 1177–1184.\n[163]\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\nRadford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. 2021.\narXiv: 2102.12092 [cs.CV].\n[164]\nAli Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan Alistarh,\nand Daniel M Roy. “NUQSGD: Provably Communication-efﬁcient Data-parallel\nSGD via Nonuniform Quantization”. In: Journal of Machine Learning Research\n22.114 (2021), pp. 1–43.\n[165]\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and\nHonglak Lee. “Generative adversarial text to image synthesis”. In: (2016).\n[166]\nScott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. “Learning deep repre-\nsentations of ﬁne-grained visual descriptions”. In: IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). 2016, pp. 49–58.\n[167]\nHerbert Robbins and Sutton Monro. “A stochastic approximation method”. In: The\nannals of mathematical statistics (1951), pp. 400–407.\nBIBLIOGRAPHY\n133\n[168]\nAndrew Ross and Finale Doshi-Velez. “Improving the adversarial robustness and\ninterpretability of deep neural networks by regularizing their input gradients”. In:\nProceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 32. 2018.\n[169]\nSaharon Rosset, Ji Zhu, and Trevor J Hastie. “Margin maximizing loss functions”.\nIn: Advances in neural information processing systems. 2004, pp. 1237–1244.\n[170]\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. “Learning represen-\ntations by back-propagating errors”. In: nature 323.6088 (1986), p. 533.\n[171]\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. “An investi-\ngation of why overparameterization exacerbates spurious correlations”. In: Interna-\ntional Conference on Machine Learning (ICML). PMLR. 2020, pp. 8346–8356.\n[172]\nTim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. “Evolution\nstrategies as a scalable alternative to reinforcement learning”. In: arXiv preprint\narXiv:1703.03864 (2017).\n[173]\nHadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. “A\nconvex relaxation barrier to tight robustness veriﬁcation of neural networks”. In:\nAdvances in Neural Information Processing Systems. 2019, pp. 9835–9846.\n[174]\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander\nMadry. “Do Adversarially Robust ImageNet Models Transfer Better?” In: NeurIPS.\n2020.\n[175]\nAndrew M Saxe, James L McClelland, and Surya Ganguli. “Exact solutions to the\nnonlinear dynamics of learning in deep linear neural networks”. In: arXiv preprint\narXiv:1312.6120 (2013).\n[176]\nLudwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander\nMadry. “Adversarially robust generalization requires more data”. In: Advances in\nNeural Information Processing Systems. 2018, pp. 5014–5026.\nBIBLIOGRAPHY\n134\n[177]\nMark Schmidt and Nicolas Le Roux. “Fast convergence of stochastic gradient descent\nunder a strong growth condition”. In: arXiv preprint arXiv:1308.6370 (2013).\n[178]\nRobin M. Schmidt, Frank Schneider, and Philipp Hennig. “Descending through a\nCrowded Valley - Benchmarking Deep Learning Optimizers”. In: Proceedings of\nthe 38th International Conference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings\nof Machine Learning Research. PMLR, 2021, pp. 9367–9376. URL: http://\nproceedings.mlr.press/v139/schmidt21a.html.\n[179]\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. “Facenet: A uniﬁed\nembedding for face recognition and clustering”. In: IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). 2015, pp. 815–823.\n[180]\nAyon Sen, Xiaojin Zhu, Liam Marshall, and Robert Nowak. “Should Adversarial\nAttacks Use Pixel p-Norm?” In: arXiv e-prints, arXiv:1906.02439 (June 2019),\narXiv:1906.02439. arXiv: 1906.02439 [cs.LG].\n[181]\nChristopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,\nRoy Frostig, and George E. Dahl. “Measuring the Effects of Data Parallelism\non Neural Network Training”. In: arXiv e-prints, arXiv:1811.03600 (Nov. 2018),\narXiv:1811.03600. arXiv: 1811.03600 [cs.LG].\n[182]\nMahmood Sharif, Lujo Bauer, and Michael K. Reiter. “On the Suitability of Lp-\nNorms for Creating and Preventing Adversarial Examples”. In: CVPR Workshops.\nIEEE Computer Society, 2018, pp. 1605–1613.\n[183]\nYash Sharma, Gavin Weiguang Ding, and Marcus Brubaker. “On the effectiveness\nof low frequency perturbations”. In: arXiv preprint arXiv:1903.00073 (2019).\n[184]\nJohn Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony.\n“Structural Risk Minimization Over Data-Dependent Hierarchies”. In: IEEE Trans.\nInf. Theory 44.5 (1998), pp. 1926–1940.\nBIBLIOGRAPHY\n135\n[185]\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,\nArthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al.\n“Mastering the game of go without human knowledge”. In: nature 550.7676 (2017),\npp. 354–359.\n[186]\nCarl-Johann Simon-Gabriel, Yann Ollivier, Leon Bottou, Bernhard Schölkopf, and\nDavid Lopez-Paz. “First-order adversarial vulnerability of neural networks and\ninput dimension”. In: International Conference on Machine Learning. PMLR. 2019,\npp. 5809–5817.\n[187]\nKaren Simonyan and Andrew Zisserman. “Very deep convolutional networks for\nlarge-scale image recognition”. In: (2015).\n[188]\nUmut Simsekli, Levent Sagun, and Mert Gürbüzbalaban. “A Tail-Index Analysis of\nStochastic Gradient Noise in Deep Neural Networks”. In: International Conference\non Machine Learning (ICML). Vol. 97. Proceedings of Machine Learning Research.\nPMLR, 2019, pp. 5827–5837.\n[189]\nRichard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew\nY Ng. “Grounded compositional semantics for ﬁnding and describing images with\nsentences”. In: 2 (2014), pp. 207–218.\n[190]\nJure Sokoli´c, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. “Robust\nlarge margin deep neural networks”. In: IEEE Transactions on Signal Processing\n65.16 (2017), pp. 4265–4280.\n[191]\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. “Dropout: A simple way to prevent neural networks from overﬁtting”.\nIn: The Journal of Machine Learning Research 15.1 (2014), pp. 1929–1958.\n[192]\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. “Energy and Policy Con-\nsiderations for Deep Learning in NLP”. In: ACL (1). Association for Computational\nLinguistics, 2019, pp. 3645–3650.\nBIBLIOGRAPHY\n136\n[193]\nNiko Sünderhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox, Jürgen\nLeitner, Ben Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, et al. “The\nlimits and potentials of deep learning for robotics”. In: The International Journal of\nRobotics Research 37.4-5 (2018), pp. 405–420.\n[194]\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. “On the impor-\ntance of initialization and momentum in deep learning”. In: International conference\non machine learning. 2013, pp. 1139–1147.\n[195]\nRichard S Sutton. “Two problems with backpropagation and other steepest-descent\nlearning procedures for networks”. In: Proceedings of Eightth Annual Conference of\nthe Cognitive Science Society, 1986. 1986.\n[196]\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,\nIan Goodfellow, and Rob Fergus. “Intriguing properties of neural networks”. In:\narXiv preprint arXiv:1312.6199 (2013).\n[197]\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew\nWojna. “Rethinking the Inception Architecture for Computer Vision”. In: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer\nSociety, 2016, pp. 2818–2826.\n[198]\nTerence Tao. Topics in random matrix theory. Vol. 132. American Mathematical\nSoc., 2012.\n[199]\nMatus Telgarsky. “Margins, Shrinkage, and Boosting”. In: ICML (2). Vol. 28. JMLR\nWorkshop and Conference Proceedings. JMLR.org, 2013, pp. 307–315.\n[200]\nValentin Thomas, Fabian Pedregosa, Bart van Merriënboer, Pierre-Antoine Man-\nzagol, Yoshua Bengio, and Nicolas Le Roux. “On the interplay between noise\nand curvature and its effect on optimization and generalization”. In: International\nConference on Artiﬁcial Intelligence and Statistics (AISTATS). 2020.\nBIBLIOGRAPHY\n137\n[201]\nTijmen Tieleman and Geoffrey Hinton. “Lecture 6.5-RMSProp: Divide the gradient\nby a running average of its recent magnitude”. In: COURSERA: Neural networks for\nmachine learning 4.2 (2012), pp. 26–31.\n[202]\nFlorian Tramèr and Dan Boneh. “Adversarial Training and Robustness for Multiple\nPerturbations”. In: NeurIPS. 2019, pp. 5858–5868.\n[203]\nFlorian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. “On\nAdaptive Attacks to Adversarial Example Defenses”. In: arXiv e-prints, arXiv:2002.08347\n(Feb. 2020), arXiv:2002.08347. arXiv: 2002.08347 [cs.LG].\n[204]\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Alek-\nsander Madry. “Robustness may be at odds with accuracy”. In: arXiv preprint\narXiv:1805.12152 (2018).\n[205]\nIoannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.\n“Large margin methods for structured and interdependent output variables”. In:\nJournal of Machine Learning Research (JMLR) 6.Sep (2005), pp. 1453–1484.\n[206]\nYusuke Tsuzuku and Issei Sato. “On the structural sensitivity of deep convolutional\nnetworks to the directions of fourier basis functions”. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 2019, pp. 51–60.\n[207]\nVladimir Naumovich Vapnik. “An overview of statistical learning theory”. In: IEEE\ntransactions on neural networks 10.5 (1999), pp. 988–999.\n[208]\nCristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin, Nicolas Le Roux, and\nRoss Goroshin. “An Effective Anti-Aliasing Approach for Residual Networks”. In:\nCoRR abs/2011.10675 (2020).\n[209]\nSharan Vaswani, Reza Babanezhad, Jose Gallego, Aaron Mishkin, Simon Lacoste-\nJulien, and Nicolas Le Roux. “To Each Optimizer a Norm, To Each Norm its\nGeneralization”. In: CoRR abs/2006.06821 (2020). arXiv: 2006.06821. URL:\nhttps://arxiv.org/abs/2006.06821.\nBIBLIOGRAPHY\n138\n[210]\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. “Order-embeddings of\nimages and language”. In: (2016).\n[211]\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan\nWierstra. “Matching Networks for One Shot Learning”. In: Advances in Neural\nInformation Processing Systems 29: Annual Conference on Neural Information\nProcessing Systems 2016, December 5-10, 2016, Barcelona, Spain. Ed. by Daniel D.\nLee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett.\n2016, pp. 3630–3638. URL: https://proceedings.neurips.cc/paper/\n2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html.\n[212]\nKailas Vodrahalli, Ke Li, and Jitendra Malik. “Are All Training Examples Created\nEqual? An Empirical Study”. In: arXiv e-prints, arXiv:1811.12569 (Nov. 2018),\narXiv:1811.12569. arXiv: 1811.12569 [cs.LG].\n[213]\nHaoran Wang, Ying Zhang, Zhong Ji, Yanwei Pang, and Lin Ma. “Consensus-aware\nvisual-semantic embedding for image-text matching”. In: European Conference on\nComputer Vision (ECCV). Springer. 2020, pp. 18–34.\n[214]\nLiwei Wang, Yin Li, and Svetlana Lazebnik. “Learning Two-Branch Neural Net-\nworks for Image-Text Matching Tasks”. In: IEEE Transactions on Pattern Analysis\nand Machine Intelligence (PAMI) (2018).\n[215]\nColin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. “Regularization matters:\nGeneralization and optimization of neural nets vs their induced kernel”. In: Advances\nin Neural Information Processing Systems. 2019, pp. 9712–9724.\n[216]\nKilian Q Weinberger and Lawrence K Saul. “Distance metric learning for large\nmargin nearest neighbor classiﬁcation.” In: Journal of machine learning research\n10.2 (2009).\n[217]\nYeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and\nJimmy Ba. “An Empirical Study of Large-Batch Stochastic Gradient Descent with\nBIBLIOGRAPHY\n139\nStructured Covariance Noise”. In: International Conference on Artiﬁcial Intelligence\nand Statistics (AISTATS). 2019. arXiv: 1902.08234.\n[218]\nEric Wong and Zico Kolter. “Provable defenses against adversarial examples via\nthe convex outer adversarial polytope”. In: International Conference on Machine\nLearning. PMLR. 2018, pp. 5286–5295.\n[219]\nChao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krähenbühl. “Sam-\npling Matters in Deep Embedding Learning”. In: (2017).\n[220]\nJingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanx-\ning Zhu. “On the noisy gradient descent that generalizes as sgd”. In: International\nConference on Machine Learning. PMLR. 2020, pp. 10367–10376.\n[221]\nXiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. “When Do Curricula Work?” In:\n9th International Conference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net, 2021. URL: https://openreview.\nnet/forum?id=tW4QEInpni.\n[222]\nCihang Xie, Mingxing Tan, Boqing Gong, Alan L. Yuille, and Quoc V. Le. “Smooth\nAdversarial Training”. In: CoRR abs/2006.14536 (2020).\n[223]\nDong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer.\n“A fourier perspective on model robustness in computer vision”. In: Advances in\nNeural Information Processing Systems 32 (2019), pp. 13276–13286.\n[224]\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. “From image\ndescriptions to visual denotations: New similarity metrics for semantic inference\nover event descriptions”. In: 2 (2014), pp. 67–78.\n[225]\nChun-Nam John Yu and Thorsten Joachims. “Learning structural svms with latent\nvariables”. In: International Conference on Machine Learning (ICML). ACM. 2009,\npp. 1169–1176.\nBIBLIOGRAPHY\n140\n[226]\nChulhee Yun, Shankar Krishnan, and Hossein Mobahi. “A Unifying View on Implicit\nBias in Training Linear Neural Networks”. In: arXiv e-prints, arXiv:2010.02501\n(Oct. 2020), arXiv:2010.02501. arXiv: 2010.02501 [cs.LG].\n[227]\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.\n“Understanding deep learning requires rethinking generalization”. In: arXiv preprint\narXiv:1611.03530 (2016).\n[228]\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.\n“Understanding deep learning requires rethinking generalization”. In: International\nConference on Learning Representations (ICLR). OpenReview.net, 2017.\n[229]\nGuodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George\nE. Dahl, Christopher J. Shallue, and Roger B. Grosse. “Which Algorithmic Choices\nMatter at Which Batch Sizes? Insights From a Noisy Quadratic Model”. In: Neural\nInformation Processing Systems (NeurIPS). 2019, pp. 8194–8205.\n[230]\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and\nMichael I. Jordan. “Theoretically Principled Trade-off between Robustness and\nAccuracy”. In: arXiv e-prints, arXiv:1901.08573 (Jan. 2019), arXiv:1901.08573.\narXiv: 1901.08573 [cs.LG].\n[231]\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank\nJ. Reddi, Sanjiv Kumar, and Suvrit Sra. “Why ADAM Beats SGD for Attention\nModels”. In: CoRR abs/1912.03194 (2019). arXiv: 1912.03194. URL: http:\n//arxiv.org/abs/1912.03194.\n[232]\nPeilin Zhao and Tong Zhang. “Accelerating Minibatch Stochastic Gradient De-\nscent using Stratiﬁed Sampling”. In: arXiv e-prints, arXiv:1405.3080 (May 2014),\narXiv:1405.3080. arXiv: 1405.3080 [stat.ML].\n[233]\nPeilin Zhao and Tong Zhang. “Stochastic optimization with importance sampling for\nregularized loss minimization”. In: international conference on machine learning.\nPMLR. 2015, pp. 1–9.\nBIBLIOGRAPHY\n141\n[234]\nZhengyu Zhao, Zhuoran Liu, and Martha Larson. “Adversarial Color Enhancement:\nGenerating Unrestricted Adversarial Images by Optimizing a Color Filter”. In: arXiv\ne-prints, arXiv:2002.01008 (Feb. 2020), arXiv:2002.01008. arXiv: 2002.01008\n[cs.CV].\n[235]\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, An-\ntonio Torralba, and Sanja Fidler. “Aligning Books and Movies: Towards Story-like\nVisual Explanations by Watching Movies and Reading Books”. In: International\nConference in Computer Vision (ICCV). 2015.\n[236]\nZhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. “The Anisotropic\nNoise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima\nand Regularization Effects”. In: International Conference on Machine Learning\n(ICML). Vol. 97. Proceedings of Machine Learning Research. PMLR, 2019, pp. 7654–\n7663.\n[237]\nC Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv\nBatra, and Devi Parikh. “Measuring machine intelligence through visual question\nanswering”. In: AI Magazine (2016).\n[238]\nWill Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. “Bilingual\nword embeddings for phrase-based machine translation”. In: Empirical Methods in\nNatural Language Processing (EMNLP). 2013, pp. 1393–1398.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV"
  ],
  "published": "2021-12-02",
  "updated": "2021-12-02"
}