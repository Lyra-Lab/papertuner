{
  "id": "http://arxiv.org/abs/2112.01660v1",
  "title": "The Influence of Data Pre-processing and Post-processing on Long Document Summarization",
  "authors": [
    "Xinwei Du",
    "Kailun Dong",
    "Yuchen Zhang",
    "Yongsheng Li",
    "Ruei-Yu Tsay"
  ],
  "abstract": "Long document summarization is an important and hard task in the field of\nnatural language processing. A good performance of the long document\nsummarization reveals the model has a decent understanding of the human\nlanguage. Currently, most researches focus on how to modify the attention\nmechanism of the transformer to achieve a higher ROUGE score. The study of data\npre-processing and post-processing are relatively few. In this paper, we use\ntwo pre-processing methods and a post-processing method and analyze the effect\nof these methods on various long document summarization models.",
  "text": "The Inﬂuence of Data Pre-processing and Post-processing on Long\nDocument Summarization\nKailun Dong, Xinwei Du, Yuchen Zhang, Yongsheng Li, Ruei-Yu Tsay\nAbstract\nLong document summarization is an im-\nportant and hard task in the ﬁeld of natural\nlanguage processing. A good performance\nof the long document summarization re-\nveals the model has a decent understand-\ning of the human language.\nCurrently,\nmost researches focus on how to mod-\nify the attention mechanism of the trans-\nformer to achieve a higher ROUGE score.\nThe study of data pre-processing and post-\nprocessing are relatively few. In this pa-\nper, we use two pre-processing methods\nand a post-processing method and analyze\nthe effect of these methods on various long\ndocument summarization models. 1\n1\nIntroduction\nLong document summarization is a hard and im-\nportant task which requires the model to identify\nand extract the important information in the doc-\nument and generate a ﬂuent summary.\nA good\nperformance in the long document summarization\nusually shows the model’s decent understanding\nof natural language.\nLong document summarization is a sequence\nto sequence task, and a general way to handle\nthis task is by using an LSTM encoder and de-\ncoder with the attention mechanism. Discourse-\nAware (Cohan et al., 2018) is a pioneering paper in\nthe long document summarization, they provided\ntwo standard datasets (arXiv and pubMed) and\nproposed a hierarchical encoder and a discourse-\naware decoder to generate the summary.\nThey\nalso adopted copying mechanism to address the\nproblem of unknown tokens and used a decoder\ncoverage vector to avoid repeated phrases in the\n1You can ﬁnd our implementation on Github: https://\ngithub.com/Anthonyive/csci-544-project.\nOur\nvideo\ndemo:https://www.youtube.com/\nwatch?v=oVIVtOPeWEs\nsummary. TLM model (Subramanian et al., 2019)\ncombined both extractive and abstractive meth-\nods and used a transformer model (Vaswani et\nal., 2017) to generate the summary. T5 (Raffel\net al., 2019) is a language model which explored\nthe transfer learning techniques and introduced a\nuniﬁed text-to-text framework. BigBird (Zaheer\net al., 2021) proposed a sparse attention mecha-\nnism and reduced the attention complexity from\nquadratic to linear.\nLongformer and its variant\nLED (Beltagy et al., 2020) further modiﬁed the at-\ntention mechanism and combined local windowed\nattention with global attention. HEPOS (Huang\net al., 2021) proposed a novel efﬁcient encoder-\ndecoder attention and achieved the state-of-the-art\nresults on PubMed dataset.\nNowadays, more and more long document sum-\nmarization researches focus on how to improve\nthe attention mechanism of the transformer. How-\never, other research areas, like image retrieval,\nfor example, have post-processing methods like\nPQ (J´egou et al., 2011), DBA (Philbin et al.,\n2007), and QE (Chum et al., 2011). These post-\nprocessing methods can further improve the per-\nformance of the model and the implementation\nof the above-mentioned post-processing methods\nare independent of the core image retrieval algo-\nrithms. Whereas, decent pre-processing and post-\nprocessing methods are lacking in the area of long\ndocument summarization.\nIn this paper, we use two pre-processing meth-\nods and a post-processing method and analyze the\neffect of these methods.\n2\nMethods\nFigure 1 shows our long document summarization\npipeline. For a given document, we ﬁrst use a pre-\nprocessing method to extract important informa-\ntion from the original document. Later, we feed\nthe extracted text to a long document summariza-\ntion model and generate the summary. Then, we\narXiv:2112.01660v1  [cs.CL]  3 Dec 2021\nFigure 1: Long Document Summarization Pipeline.\nuse a post-processing method to reﬁne the sum-\nmary generated by the model and form our ﬁnal\nabstract.\n2.1\nPre-processing\nWe use extraction-based methods to fulﬁll data\npre-processing.\nThe extraction-based method is\nusing some ranking algorithms to extract impor-\ntant sentences from original documents. Here in\nthis paper, we used two different approaches of ex-\ntractive methods to tackle the summarization prob-\nlem.\n2.1.1\nFrequency Driven Extraction\nWe ﬁrst tried a Frequency Driven Approach. This\nmethod is useful for several reasons: (1) It gives\nus a baseline of what the target rouge-1, rouge-2,\nrouge-L scores are going to be; (2) It could be-\ncome the ﬁrst processing step for later steps; (3)\nIts fast, reliable, and light processing step makes it\neasy to pipe to other methods.\nHere we used a very simple scoring algorithm:\nGiven a document D with N sentences, D =\n{S1, S2, ..., SN}, we ﬁrst remove stopwords and\nassign each remained word wi with equal weight\n1 (we can also give values other than 1 based on\nprior knowledge, we set all weight to 1 for sim-\nplicity). Then, we calculate the value of each word\nvwi by calculating the total weight of word wi in\nthis document (in this case, we try to calculate the\nfrequency of each word in this document since we\nset all weight to 1).\nvwi = count(D, wi)\n(1)\nFor each sentence Si, the value of each sentence\nVSi euqals to the sum of the value of the words in\nthe sentence.\nVSi =\nX\nwi∈Si\nVwi\n(2)\nFinally, we choose the top M sentences to create\nthe summary. M is a hyperparameter. And we set\nM as the nnumber of sentences of the ground true\nsummary.\nResult = top M(D)\n(3)\nWe acknowledge that the top M approach\nmight be limited as M is dependent on the refer-\nence summary data. However, we are interested in\nﬁnding the differences between whether using this\nfrequency driven approach will increase the train\nand dev data accuracies or not.\nFigure 2 show the ﬂowchart of frequency driven\nalgorithm.\n2.1.2\nTextRank Algorithm\nThe choosing of M could be a problem for the al-\ngorithm in section 2.1.1 when we try to summary\nnew data. To resolve ﬁnding M, we choose two\napproaches: One is we treat this M as a ﬁxed hy-\nperparameter, i.e. a ﬁxed number of sentences to\nﬁnd in the predicted summary. Another one is to\nuse another approach without using M.\nWe use a graph-based method, TextRank (Mi-\nhalcea and Tarau, 2004), which was inﬂuenced by\nGoogle’s Page Rank Algorithm (Page et al., 1999),\nto do extractive document summarization.\n2.2\nLong Document Summarization Models\nWe adopt various long document summarization\nmodel to perform the summarization. We use ﬁne-\ntuned BigBird and LED models, and trained a T5\nmodel from scratch.\nA beneﬁt of our summarization pipeline is that\nthe implementation of long document summariza-\ntion won’t affect the data pre-processing step and\npost-processing step.\n2.3\nPost-processing\nWe use GPT-3 model to handle the post-\nprocessing step. GPT-3 model is a transformer-\nbased language model and is trained on large\ndatasets which contain a huge amount of informa-\ntion about the reading text for many kinds of areas.\nLots of experiments show that GPT-3 has a strong\nlanguage generating ability. Therefore, in our ex-\nperiment, we use the GPT-3 model to do the ﬁne\npost-processing step.\nFigure 2: The Flowchart of Frequency Driven Algorithm.\nGiven the summary proposed by a long doc-\nument summarization model, we use prompt as\n”Original [summary], Polished Sentence:”. GPT-\n3 model will complete this prompt and generate\nthe polished sentences. We adopt the curie engine\nof GPT-3 which has a decent computing speed and\nperformance.\n3\nExperiments and Results\n3.1\nDatasets\nWe used two datasets, namely arXiv and PubMed,\nto train our models and evaluate our models’ per-\nformances.\nThe arXiv dataset and the PubMed dataset are\nboth free and open-access resources for research\nuse.\nBecause the original full-size datasets are\ntoo large, the arXiv and PubMed datasets we used\nare subsets of their original data that take stor-\nage space over 1T and 90G. The arXiv dataset\nand PubMed dataset consist of 215913 and 133215\npublished scholarly articles respectively, each hav-\ning been separated into a training set, a validation\nset, and a testing set with similar distributions on\narticles’ sentence counts and token counts. The\ndatasets also contain section information and ci-\ntation information of the articles, while our team\nmainly focuses on the article texts to extract the\narticles’ summarizations.\nTable 1 shows the distribution characteristics of\nthe two datasets.\n3.2\nModel Setting\nIn order to better test the generalization ability of\nour proposed method, we use a variety of long\ndocument models. The following describes how\nwe use these models.\nBigBird: We used two BigBird (Zaheer et al.,\n2021) models ﬁntuned on arXiv and PubMed\ndatasets respectly to obtain the Rouge perfor-\nmance of BigBird model on these two datasets.\nThe maximum number of input tokens for Big-\nBird is 4096. To generate the summary, we set\nthe length penalty as 0.8, number of beam search\nas 5 and the maximum number of output tokens as\n256.\nLED: We used Longformer-Encoder-Decoder\n(LED), a variant of Longformer model (Beltagy\net al., 2020), designed for long document summa-\nrization. We used two ﬁntuned LED models pre-\ntrained on arXiv and PubMed datasets respectly.\nThe maximum number of input tokens of LED\nis 16384, but because of the limitations of GPU\nmemory, we set the maximum number of input to-\nkens as 8192. Longformer proposed a global at-\ntention mechanism, which can help the model to\nunderstand the documents better. We put gloabl\nattention on the start token of each sentence.\nT5: T5 (Raffel et al., 2019) is a uniﬁed Text-to-\nText Transfer Transformer pre-trained on a large\ntext corpus and has been demostrated to achieve\nthe state-of-the-art performance on many NLP\nTable 1: Dataset Distribution Characteristics.\nDataset\nNum of doc\nAvg num of sen\nAvg num of tokens (doc)\nAvg num of tokens (sen)\narXiv\n215k\n206\n6029\n29\nPubMed\n133k\n86\n3048\n33\nTable 2: Summarization ROUGE score for long documents.\narXiv\nPubMed\nModel\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nBigBird\n41.94\n21.59\n36.56\n42.45\n18.18\n37.74\n+ Extraction\n27.54\n7.63\n17.06\n34.21\n10.89\n20.65\n+ TextRank\n35.58\n10.36\n19.59\n42.15\n27.80\n32.95\n+ GPT-3\n42.16\n21.86\n36.75\n34.95\n12.73\n31.07\nLED\n45.96\n26.97\n42.48\n43.10\n17.11\n32.76\n+ Extraction\n37.13\n12.46\n28.01\n41.56\n18.95\n24.02\n+ TextRank\n40.43\n11.23\n25.00\n49.53\n21.45\n29.48\n+ GPT-3\n38.22\n21.46\n35.67\n38.39\n14.40\n27.68\nT5 (Fail)\n12.65\n5.45\n10.82\n14.42\n5.58\n12.99\nExtraction (Only)\n27.05\n7.94\n13.87\n34.65\n12.40\n19.66\nTextRank (Only)\n29.49\n9.58\n14.86\n31.34\n12.25\n17.35\ntasks. We ﬁne-tuned long document summariza-\ntion on the T5 pre-trained model using PubMed\nand Arxiv dataset, the maximum input length is\nset to 4096 and the maximum generation length is\nset to 256 due to GPU memory limits.\n3.3\nResults and Discussion\nTable 2 shows the Rouge score of the BigBird\nmodel and LED model, and the score when we ap-\nply pre-processing and post-processing methods.\n(Due to the limitation of computing resources and\ntime, We only tested the ﬁrst 100 test data in each\ndataset. Therefore, instead of focusing on the spe-\nciﬁc Rouge value, please focus on the difference in\nRouge between different methods.) We also listed\nthe performance of our T5 model, which is trained\nfrom scratch, and the performance when we only\nuse the pre-processing methods.\nAccording to Table 2, pre-processing method\nTextRank may increase the performance some-\ntimes and decreases the performance at other time.\nPre-processing method ﬁnds the important sen-\ntence.\nIt will help long document summariza-\ntion models discard unimportant content and re-\nduce distractions.\nHowever, the pre-processing\nmethods are not useful all the time, when it ex-\ntracts unimportant content and discard important\nsentence by mistake, this method will decrease the\nsummary performance.\nAs for post-processing, when we try to apply\nGPT-3 as a post-processing method, the Rouge\nscore of the model decreases. It makes sense since\nwe only give GPT-3 model the summary generated\nby the long document summarization model. We\ntry to input the original whole paper but it is too\nlong to be handled by GPT-3.\nMoreover, T5 does not perform well on long\ndocument summarizations according to the Rouge\nscore, though it claimed to get good performance\nin summarization in the original T5 paper on short\ndocuments of CNN news. We think this may be\nbecause that the average length of CNN news is\nonly 656 will the average length of arXiv and\nPubmed datasets are more than 3000.\nIn addi-\ntion, the abstractive summary generated by the\nT5 model is very short, the original summary of\nPubMed has a median length of 226 and only has\na median length of 10 for generated summary.\n4\nConclusions and Future Work\nTo explore how to further improve the perfor-\nmance of the long document summarization, we\nused frequency driven extracted algorithm and\nTextRank algorithm for data pre-processing, and\nuse GPT-3 for post-processing. The experiment\nresults show that our proposed method can not im-\nprove the performance of the model.\nIn future work, we need to why study will T5\nmodel generate such short summaries and if there\nare ways to modify it to adapt to long document\nsummarization. We also want to try larger mod-\nels such as T5-base and T5-large to see if we can\nimprove the performance.\nMoreover, we believe a decent post-processing\nmethod is needed not only for long document sum-\nmarization, but also for other language generation\ntasks such as machine translation, dialogue gener-\nation. A model dedicated to polishing articles may\nbe useful for NLP tasks.\nReferences\n[Beltagy et al.2020] Iz Beltagy,\nMatthew E Peters,\nand\nArman\nCohan.\n2020.\nLongformer:\nThe long-document transformer.\narXiv preprint\narXiv:2004.05150.\n[Chum et al.2011] Ondˇrej\nChum,\nAndrej\nMikul´ık,\nMichal Perdoch, and Jiˇr´ı Matas. 2011. Total recall\nii: Query expansion revisited. In CVPR 2011, pages\n889–896.\n[Cohan et al.2018] Arman Cohan, Franck Dernoncourt,\nDoo Soon Kim, Trung Bui, Seokhwan Kim, Wal-\nter Chang, and Nazli Goharian.\n2018.\nA\ndiscourse-aware attention model for abstractive\nsummarization of long documents. arXiv preprint\narXiv:1804.05685.\n[Huang et al.2021] Luyang\nHuang,\nShuyang\nCao,\nNikolaus Parulian, Heng Ji, and Lu Wang.\n2021.\nEfﬁcient attentions for long document summariza-\ntion. arXiv preprint arXiv:2104.02112.\n[J´egou et al.2011] Herv´e\nJ´egou,\nFlorent\nPerronnin,\nMatthijs Douze, Jorge S´anchez, Patrick P´erez, and\nCordelia Schmid.\n2011.\nAggregating local im-\nage descriptors into compact codes. IEEE transac-\ntions on pattern analysis and machine intelligence,\n34(9):1704–1716.\n[Mihalcea and Tarau2004] Rada Mihalcea and Paul Ta-\nrau. 2004. Textrank: Bringing order into text. In\nProceedings of the 2004 conference on empirical\nmethods in natural language processing, pages 404–\n411.\n[Page et al.1999] Lawrence Page, Sergey Brin, Rajeev\nMotwani, and Terry Winograd. 1999. The pagerank\ncitation ranking: Bringing order to the web. Techni-\ncal report, Stanford InfoLab.\n[Philbin et al.2007] James\nPhilbin,\nOndrej\nChum,\nMichael Isard, Josef Sivic, and Andrew Zisserman.\n2007. Object retrieval with large vocabularies and\nfast spatial matching. In 2007 IEEE conference on\ncomputer vision and pattern recognition, pages 1–8.\nIEEE.\n[Raffel et al.2019] Colin Raffel, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu.\n2019. Exploring the limits of transfer learning with\na uniﬁed text-to-text transformer.\narXiv preprint\narXiv:1910.10683.\n[Subramanian et al.2019] Sandeep Subramanian, Ray-\nmond Li, Jonathan Pilault, and Christopher Pal.\n2019.\nOn extractive and abstractive neural doc-\nument summarization with transformer language\nmodels. arXiv preprint arXiv:1909.03186.\n[Vaswani et al.2017] Ashish Vaswani, Noam Shazeer,\nNiki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need.\n[Zaheer et al.2021] Manzil Zaheer, Guru Guruganesh,\nAvinava Dubey, Joshua Ainslie, Chris Alberti, San-\ntiago Ontanon, Philip Pham, Anirudh Ravula, Qifan\nWang, Li Yang, and Amr Ahmed. 2021. Big bird:\nTransformers for longer sequences.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-12-03",
  "updated": "2021-12-03"
}