{
  "id": "http://arxiv.org/abs/2112.01675v1",
  "title": "Challenges and Opportunities in Approximate Bayesian Deep Learning for Intelligent IoT Systems",
  "authors": [
    "Meet P. Vadera",
    "Benjamin M. Marlin"
  ],
  "abstract": "Approximate Bayesian deep learning methods hold significant promise for\naddressing several issues that occur when deploying deep learning components in\nintelligent systems, including mitigating the occurrence of over-confident\nerrors and providing enhanced robustness to out of distribution examples.\nHowever, the computational requirements of existing approximate Bayesian\ninference methods can make them ill-suited for deployment in intelligent IoT\nsystems that include lower-powered edge devices. In this paper, we present a\nrange of approximate Bayesian inference methods for supervised deep learning\nand highlight the challenges and opportunities when applying these methods on\ncurrent edge hardware. We highlight several potential solutions to decreasing\nmodel storage requirements and improving computational scalability, including\nmodel pruning and distillation methods.",
  "text": "arXiv:2112.01675v1  [cs.LG]  3 Dec 2021\nChallenges and Opportunities in Approximate\nBayesian Deep Learning for Intelligent IoT\nSystems\nMeet P. Vadera, and Benjamin M. Marlin\nManning College of Information and Computer Sciences\nUniversity of Massachusetts Amherst\nAmherst, MA, USA\n{mvadera, marlin}@cs.umass.edu\nAbstract\nApproximate Bayesian deep learning methods hold signiﬁcant promise\nfor addressing several issues that occur when deploying deep learning com-\nponents in intelligent systems, including mitigating the occurrence of over-\nconﬁdent errors and providing enhanced robustness to out of distribution\nexamples. However, the computational requirements of existing approxi-\nmate Bayesian inference methods can make them ill-suited for deployment\nin intelligent IoT systems that include lower-powered edge devices. In this\npaper, we present a range of approximate Bayesian inference methods for\nsupervised deep learning and highlight the challenges and opportunities\nwhen applying these methods on current edge hardware. We highlight\nseveral potential solutions to decreasing model storage requirements and\nimproving computational scalability, including model pruning and distil-\nlation methods.\n1\nIntroduction\nDeep learning research has shown promising results in many application areas\nof artiﬁcial intelligence including object detection, language modeling, speech\nrecognition, medical imagining, image segmentation and many more [1–9]. Key\ncomponents driving the overall success of deep learning-based methods include\nadvances in learning algorithms, neural network architectures, computing hard-\nware including graphics processing units (GPUs) and tensor processing units\n(TPUs), and the availability of large labeled data sets.\nHowever, as deep learning models are being deployed in ﬁelded intelligent\nsystems, including intelligent IoT systems, several challenges have become in-\ncreasingly prominent. These challenges include the deployment-time occurrence\nof high conﬁdence errors, the need to be robust to out-of-distribution inputs,\n1\nand the potential for in-domain adversarial inputs. High conﬁdence errors oc-\ncur when a probabilistic machine learning model ascribes high probability to\nan incorrect output (e.g., a class label in a classiﬁcation setting) [10]. Deployed\ndeep learning models can also encounter inputs from data distribution that dif-\nfer systematically from the distribution they were trained on. In such cases,\nmodels need to avoid making high conﬁdence errors. One potential approach\nto dealing with this issue is to explicitly identify inputs as out-of-distribution\nand to decline to make predictions for them [11]. Lastly, intelligent systems can\nalso encounter adversarial inputs of diﬀerent types. Early work in adversarial\nexample generation focused on algorithms for making low-norm changes to in-\nputs that, while being nearly imperceptible to humans, result in models making\nhighly conﬁdent errors [12].\nOne of the important factors contributing to susceptibility to these problems\nis model uncertainty.\nSupervised deep learning models are most commonly\ntrained by optimizing their parameters to minimize a training loss function.\nThis approach yields a single locally optimal setting of the model parameters,\nwhich are then used to make predictions at deployment time. However, given\nthe large number of parameters used in current models, there typically exist\nmultiple qualitatively diﬀerent sets of parameters yielding similar training loss\nfunction values, but making diﬀerent predictions on future inputs. Commmon\ntraining procedures select a single set of such parameters, despite the fact that\nfor a given amount of training data, there may be signiﬁcant uncertainty over\nwhat the best model parameters actually are.\nBayesian inference provides a diﬀerent perspective on the problem of train-\ning deep neural network models that attempts to represent model uncertainty\nand propagate it to the point of issuing predictions and making decisions. In\nBayesian inference, the unknown model parameters are formally treated as ran-\ndom variables. The goal becomes to infer the posterior probability distribution\nover the unknown model parameters given the available data. When the data\nsupport multiple distinct interpretations in terms of settings of the model pa-\nrameters, the model posterior will reﬂect this uncertainty. Incorporating model\nuncertainty into prediction and decision-making typically decreases overcon-\nﬁdence in predictions via the Bayesian model averaging eﬀect, and can also\nincrease robustness to out-of-distribution and adversarial inputs [13,14]. Quan-\ntities computed from the model posterior can also be used as inputs for auxiliary\nproblems, including the detection of out-of-distribution examples.\nBayesian inference has the potential to provide a comprehensive theoretical\nfoundation for constructing uncertainty-aware and robust deep learning-based\nintelligent IoT systems, including systems that must reason robustly over com-\nplex multi-modal inputs in the ﬁeld.\nHowever, the application of Bayesian\ninference methods to deep neural network models is challenging due to the large\nscale of current state-of-the-art prediction models.\nIn this paper, we focus on the challenges and opportunities that arise when\nwe consider deploying Bayesian deep learning approaches on IoT edge devices.\nSpeciﬁcally, traditional approximate Bayesian learning algorithms represent the\nmodel posterior as a large ensemble of models. This can be expensive from\n2\nthe storage perspective, as the cost of storing an ensemble of S models will\nbe S times higher than the cost of storing a single model unless additional\ncompression is used. Further, when making predictions, traditional Bayesian\nensembles require processing each input instance through each element of the\nensemble. Again, this requires S times more compute when using an ensemble\nof S models compared to the use of a single model.\nThe remainder of this paper is organized as follows. In Section 2 we be-\ngin by providing a comprehensive discussion of Bayesian supervised learning,\napproximate Bayesian inference, and the scalability challenges of deploying cur-\nrent Bayesian deep learning model representations on edge hardware. Next, in\nsections 3 and 4 we discuss model compression techniques that can be leveraged\nfor compressing Bayesian posterior distributions. These approaches either look\nat compressing each member of the model ensemble, or compress the entire en-\nsemble into a surrogate model. We point out challenges, opportunities and open\nresearch directions related to both approaches.\n2\nBayesian Deep Learning\nand the Challenge of Scalability\nIn this section we introduce the fundamental concepts of Bayesian inference for\nsupervised deep learning along with foundational approximation methods. We\ndiscuss the scalability challenges when deploying such methods on edge and IoT\nsystems.\n2.1\nBayesian Supervised Learning\nSupervised learning forms the core of machine learning-based prediction sys-\ntems. In a supervised learning problem, we are given a dataset D consisting of\ninput-output pairs {(xi, yi)|1 ≤i ≤N}, where xi ∈RD is the input or feature\nvector and yi ∈Y is the output or prediction target. We let Dx be the dataset of\ninputs and Dy be the dataset of outputs. The nature of Y depends on the task\nat hand. For classiﬁcation tasks, Y is a ﬁnite set, whereas for regression tasks,\nwe usually have Y ∈R. In this paper, we speciﬁcally focus on the classiﬁcation\nsetting in supervised learning, where the goal is to learn a function f : RD →Y\nthat can accurately predict the outputs from the inputs.\nIn probabilistic supervised learning, we construct the prediction function us-\ning a conditional probability model of the form fθ(x) = p(y|x, θ) where θ ∈RK\nare the model parameters. The conditional likelihood of the inputs given the\noutputs and the parameters is denoted by p(Dy|Dx, θ).\nUnder the assump-\ntion that the outputs are independent and identically distributed given their\ncorresponding outputs, we have that p(Dy|Dx, θ) = QN\ni=1 p(yi|xi, θ). A key in-\ngredient in Bayesian inference, as well as in traditional point-estimated neural\nnetworks, is the prior distribution p(θ|λ) over model parameters. As the name\nsuggests, the prior distribution represents our beliefs about the distribution of\n3\nthe model parameters prior to analyzing the data. The prior distribution can\nhave its own hyperparameters, here denoted by λ [15].\nBayesian inference involves the computation of the posterior distribution\nover the unknown model parameters given a training dataset Dtr and the prior.\nThe parameter posterior is obtained using the Bayes theorem, as shown in Equa-\ntion (1).\np(θ|Dtr, λ) =\np(Dy\ntr|Dx\ntr, θ)p(θ|λ)\nR p(Dy\ntr|Dx\ntr, θ)p(θ|λ)dθ\n(1)\nThe denominator in the parameter posterior (referred to as the “evidence”\nterm) is intractable to compute for most ML models, including for deep neural\nnetworks [15]. As a result, the computation of the exact posterior distribution\nis intractable. However, in practice, the quantity of interest is often not the\nparameter posterior distribution itself, but rather low-dimensional expectations\nunder the parameter posterior.\nOne key posterior expectation in the supervised learning setting is the pos-\nterior predictive distribution, which is necessary and suﬃcient for making max-\nimum probability predictions for outputs given inputs while integrating over\nthe uncertainty in the model parameters. The posterior predictive distribution\ncomputation is shown in Equation (2).\np(y|x, Dtr, λ) = Ep(θ|Dtr,λ)[p(y|x, θ)]\n(2)\nAnother posterior expectation that is useful in uncertainty quantiﬁcation\nis the expected posterior predictive entropy. Posterior predictive entropy (also\nreferred to as the total uncertainty of the predictive distribution) can be decom-\nposed into quantities referred to as expected data uncertainty and knowledge\nuncertainty [16]. These three forms of uncertainty are related by the equation\nshown below:\nH\n\u0002\nEp(θ|D) [p (y|x, θ)]\n\u0003\n|\n{z\n}\nTotal Uncertainty\n=\nI [y, θ|x, D]\n|\n{z\n}\nKnowledge Uncertainty\n+ Ep(θ|D) [H [p (y|x, θ)]]\n|\n{z\n}\nExpected Data Uncertainty\n(3)\nKnowledge uncertainty can be eﬃciently computed as the diﬀerence between\ntotal uncertainty and expected data uncertainty, both of which are functions of\nposterior expectations. Recent work has leveraged these uncertainty estimates\nto explore a range of down-stream tasks such out-of-distribution detection, mis-\nclassiﬁcation detection, and active learning that rely on uncertainty quantiﬁca-\ntion and decomposition. [17–23]. However, all of these posterior expectations\nare also intractable to compute exactly for deep learning models. We thus next\nturn to the problem of approximate Bayesian inference methods.\n4\n2.2\nApproximate Bayesian Inference for Supervised Learn-\ning\nAs indicated in the previous subsection, posterior expectations including the\nposterior distribution needed for Bayesian supervised learning is intractable in\nits original form. To tackle this problem there is a signiﬁcant body of work\nin the area of approximate Bayesian inference techniques. The ultimate goal of\nthese approximation methods is to compute approximate posterior expectations\nthat are close to their theoretical counterparts. Approximate Bayesian meth-\nods can be broadly divided into three categories: Markov Chain Monte Carlo\n(MCMC) methods, surrogate density estimation methods, and other approxi-\nmation methods. We describe each category of methods below and discuss their\nedge deployment challenges.\n2.2.1\nMarkov Chain Monte Carlo Methods\nMCMC methods provide an approximation to the intractable parameter poste-\nrior p(θ|D, λ) via a set of samples drawn for this distribution. MCMC methods\nsimulate a Markov chain that converges to the parameter posterior as its steady\nstate distribution. The simulated states of the Markov chain after convergence\ncorrespond to samples from the parameter posterior. Once we collect a set of\nparameter samples of the desired size, we can approximate expectations with\nrespect to the parameter posterior using empirical averages over the sampled\nparameter values [24]. For example, the posterior predictive distribution can be\napproximated as shown in Equation (4). As we can see, computing the Monte\nCarlo approximation to the posterior predictive distribution is very similar to\ncomputing the predictive distribution of a model ensemble.\np(y|x, D, λ) = Ep(θ|D,λ)[p(y|x, θ)]\n≈EpMC(θ|D,λ)[p(y|x, θ)]\n= 1\nS\nS\nX\ns=1\np(y|x, θs)\n(4)\nExamples of classical MCMC methods include the Gibbs sampler [25] and the\nMetropolis-Hastings sampler [26]. The earliest work on MCMC samplers for\nneural networks traces back to the application of Hamiltonian Monte Carlo [27]\nmethods. While a number of MCMC methods have since been developed with\nimproved properties including slice sampling [28], elliptical slice sampling [29],\nand Riemann manifold sampling methods [30], these methods all require using\nall of the available data when computing the likelihood term needed for posterior\ninference. Although only linear in the number of data cases, this can be a highly\nexpensive operation for large data sets and models and can render MCMC\nmethods practically infeasible in scenarios where stochastic gradient descent\n(SGD) [31] can be usefully applied to optimize model parameters.\nHowever, recent advances in MCMC approaches have enabled the use of\nSGD-like mini-batch algorithms, greatly extending the range of applicability\n5\nof MCMC methods. Prominent examples of such approaches include stochas-\ntic gradient Langevin dynamics (SGLD) [32], stochastic gradient Hamiltonian\nMonte Carlo (SGHMC) [33] and their cyclic learning rate versions as presented\nin [34].\nAnother important property that determines the eﬃcacy of MCMC meth-\nods is the degree of mixing. The degree of mixing refers to how eﬃciently the\nMarkov chain traverses the posterior distribution after convergence [35]. Bet-\nter mixing enables faster collection of a more diverse set of parameter samples.\nHowever, the mixing properties of MCMC methods depend on the dimensional-\nity of the parameter space. Modern deep neural networks can have an extremely\nlarge number of parameters (millions or more), potentially leading to inadequate\nmixing.\nAn alternative to sampling in the original parameter space RK is to sample\nin a reduced dimensional space RK′ for K′ ≪K and to project back to the full-\ndimensional space. This process is termed subspace inference [36]. Subspaces\ncan be generated using singular value decomposition (SVD) applied to SGD\niterates, or by generating random projection matrices, among other possible\noptions [36]. These methods introduce bias by restricting the sampler to operate\nin a subspace of RK, but reduce variance by enabling the underlying Markov\nchain to mix faster.\nWhile recent advances in MCMC methods are enabling the application of\nBayesian inference approaches to increasingly large deep learning models, there\nremains a sizeable gap in terms of the practical deployment of Monte Carlo-\nbased posterior approximations on edge hardware to support intelligent IoT\nsystems. As noted previously, MCMC methods generate S samples from the\nmodel posterior in place of the single set of parameters used by optimization-\nbased deep learning. This means that the deployment of Monte Carlo-based\napproximate prediction methods requires S times more storage as well as S\ntimes more computation without further approximations.\nHowever, the actual increase in prediction latency depends on a number of\nfactors. First, in the edge setting, it is typical for an edge device to process\ndata from on-board sensors. In this case, the edge device may only need to\nmake predictions for a single instance at a time. For GPU or TPU accelerated\nedge devices, it may thus be possible to run a single input through multiple\nmodels with identical structure in parallel, eﬀectively batching the models in-\nstead of the inputs. The feasibility of this approach depends on the size of the\nmodel to be deployed. Second, many edge platform toolchains have the ability\nto perform weight quantization and to use mixed precision arithmetic when de-\nploying models. Such transformations can be applied separately to each element\nof a posterior ensemble to improve edge deployment eﬃciency. We note that\nthe eﬀect of such quantization has not been well studied for Bayesian posterior\nensembles, but there is reason to believe that they might better tolerate more\naggressive quantization than single models due to the average that is taken over\nthe outputs of the posterior ensemble when making predictions.\nLastly, we note that the memory constraints of edge devices can also play\na non-trivial role in prediction latency when using ensembles due to the time\n6\nrequired to load models. In some instances, the time needed to load models\nis signiﬁcantly higher than the time needed to use the loaded model to make\npredictions for a single input. When this is the case, it can dramatically increase\nprediction latency. Indeed, the number of models that can be present in memory\nsimultaneously can currently be the eﬀective limiting factor on the size of the\nMCMC posterior ensemble that can usefully be deployed.\nAn interesting opportunity motivated by these observations is the develop-\nment of optimized model loading methods speciﬁcally for ensembles of models\nwith identical architectures. In this case, the structure of the computation graph\nunderlying the architecture should only need to be instantiated once, and it may\nbe possible to more rapidly iterate through diﬀerent sets of parameters for the\nsame architecture than is possible using current libraries that do not include\nsuch optimizations [37].\n2.2.2\nSurrogate Density Methods\nAn alternate approach to MCMC methods is approximating the original pos-\nterior distribution via an analytically tractable parameterized surrogate distri-\nbution.\nThus, given the original posterior distribution p(θ|Dtr, λ), surrogate\ndensity methods aim to approximate the true posterior using an auxiliary dis-\ntribution q(θ|φ), where φ are the auxiliary parameters [38–41].\nA common\napproximation for auxiliary parameters is the use of a multivariate Gaussian\ndistribution with a diagonal covariance matrix N(θ; µ, Σ), also known as mean-\nﬁeld variational inference. Here, the auxiliary parameters are φ = [µ, Σ]. The\nmain reason why mean-ﬁeld variational inference is popular is due to the simple\n“re-parameterization trick” that makes sampling from the auxiliary distribu-\ntion straightforward [42]. With the re-parameterization trick, we can sample\nθk ∼N(µk, Σkk) by ﬁrst drawing η ∼N(0, 1), followed by the linear trans-\nformation θk = µk + η · √Σkk. This allows us to backpropagate through the\nvariational parameters while drawing samples of the model parameters to ap-\nproximate the objective functions used for learning.\nNow, for estimating the auxiliary parameters, we need an objective function\nthat can measure the discrepancy between the surrogate distribution and the\nground truth posterior. Needless to say, this objective function must also be\ncomputationally tractable. The most commonly used discrepancy function is\nthe Kullback-Leibler (KL) divergence as shown below [43].\nKL(p(θ)||q(θ)) = Ep(θ)\n\u0014\nlog\n\u0012p(θ)\nq(θ)\n\u0013\u0015\nThe KL divergence is a directional divergence, and thus is not symmetric\nin its arguments. This can result in two diﬀerent measures depending on the\ndirectionality. When the surrogate posterior is used as the ﬁrst argument, the\nresult is the variational inference (VI) framework [38,39]. When the surrogate\nposterior is used as the second argument, the result is the expectation propaga-\ntion (EP) framework [41]. This often leads to VI methods having mode seeking\n7\nbehavior, as they are not forced to ensure the same support as the original pos-\nterior. EP methods on the other hand are forced to ensure exact support, but\nthis can often lead to incorrect mode estimation as it must cover the support\nof the original posterior. These two extremes can also be interpolated by more\ngeneralized divergence measures, including alpha divergence [44].\nThese methods also suﬀer from scalability issues due to the need to com-\npute the log likelihood and its gradient over the entire dataset.\nSimilar to\nthe stochastic gradient version of MCMC methods introduced earlier, advances\nin the past decade have led to more scalable methods in this family as well,\nincluding stochastic variational inference [45], that are able to accommodate\nlarge-scale datasets using mini-batch gradients.\nIn addition to mean ﬁeld VI, other more advanced approximations are pos-\nsible, such as multiplicative normalizing ﬂows (MNF) [46], Bayesian hypernet-\nworks [47], and Rank-1 factorization [48]. In multiplicative normalizing ﬂows,\nwe choose a simple density function such as the isotropic Gaussian distribution,\nand use a bijective function to transform the samples drawn from the simple den-\nsity function to form more complex distributions. The Bayesian hypernetworks\napproach builds upon the MNF approach and uses a neural network model to\ntransform the samples drawn from a simpler density function to model a more\ncomplex posterior distribution. Finally, the rank-1 factorization approach repre-\nsents the model parameters as a product of two rank-1 factors thereby reducing\nthe dimensionality of the base distribution of the approximate posterior. For\nexample, if we have a parameter matrix W ∈RM×N, this can be re-written as\na matrix product of W1 ∈RM×1 and W2 ∈R1×N. This eﬀectively reduces the\nnumber of parameters from O(M · N) to O(M + N).\nMC Dropout is a particularly interesting approach, which is equivalent to ap-\nproximate variational inference under speciﬁc assumptions [49]. Dropout itself\nwas ﬁrst introduced as a regularization technique where during every training\niteration a pre-determined proportion of activations is randomly set to zero to\nreduce overﬁtting. At test time dropout is switched oﬀand all units partici-\npate in making predictions [50]. In MC Dropout, by contrast, dropout is used\nat prediction time. This leads to a stochastic forward pass through the point-\nestimated model. Multiple forward passes through the model are used and the\npredictive distributions are averaged. This procedure is equivalent to sampling\nfrom a speciﬁc approximate variational posterior, but has the advantage that it\nis very easy to implement.\nThe major drawback of surrogate density methods is that they introduce\nbias into the estimation of the posterior distribution, unless the true posterior\nbelongs to the family of auxiliary distributions. The degree of bias will depend\non the functional form of the auxiliary distribution, and the divergence measure\nused to estimate the parameters of the auxiliary distribution.\nIn addition, under these methods, we still face hurdles in computing pos-\nterior expectations including the approximate posterior predictive distribution.\nIn particular, even if q(θ|φ) is a simple parametric distribution, the expecta-\ntion Eq(θ|φ)[p(y|x, θ)] still usually cannot be computed analytically due to the\nnon-linearity of p(y|x, θ). As a result, we have to again resort to Monte Carlo\n8\napproximation, and draw samples from the approximate variational posterior.\nGenerally, these methods trade-oﬀthe potential bias in the surrogate param-\neter posterior for the ability to draw independent samples once the surrogate\nposterior parameters have been estimated.\nThe deployment considerations for models derived using variational infer-\nence methods are distinct from those of MCMC methods. One of the beneﬁts\nof mean ﬁeld variational inference over MCMC methods is that the representa-\ntion of the posterior approximation only requires twice the storage of a single\nmodel. However, for this to be useful at deployment time, it must be possible\nto eﬃciently sample models from the approximate parameter posterior on the\nﬂy. The same is true of approaches like MC Dropout that require the sampling\nof random masks at inference time.\nIn the case of MC Dropout, for exam-\nple, current model conversion pipelines from PyTorch [51] to TensorRT [37] via\nONNX [52] strip out stochastic dropout layers. It appears that for such ap-\nproaches to have practically useful storage advantages over MCMC methods on\ncurrent edge hardware, additional development may be needed to enable the\nautomated deployment of computation graphs with stochastic elements.\nAn\nalternative for hardware where the computational cost of on-the-ﬂy generation\nof samples from the variational posterior is prohibitive is to pre-generate and\ndeploy a ﬁxed variational approximate posterior ensemble.\nHowever, in this\ncase, all the considerations that apply to the generation of MCMC posterior\nensembles will also apply.\n2.2.3\nAdditional Approximate Bayesian Inference Methods\nWith the emergence of Generative Adversarial Networks (GANs) [53], there has\nbeen work on learning implicit generative model representations of the parame-\nter posterior that can be used at test time to draw an arbitrary but ﬁnite number\nof samples from this posterior approximation. The existing work in this area\ninvolves training GANs that can approximate the posterior distribution asserted\nby SGLD [17,54]. To compute the approximate posterior predictive distribution,\nwe yet again would use Monte Carlo approximation as shown in the previous\nsubsections. An advantage of these methods is that in theory we do not need to\nstore posterior samples for use during inference and we can also determine the\nnumber of posterior samples to use on the ﬂy. However, the same deployment\nissues noted for variational inference apply here as well in terms of the feasibil-\nity of on-the-ﬂy sampling from an auxiliary model. In addition, this approach\nrequires allocating additional memory for the auxiliary GAN model.\nDeep Ensembles [55] have also shown strong performance in terms of repre-\nsenting predictive distributions, and can also be considered as an approximation\nto Bayesian inference [56]. However, deep ensembles can be expensive during\ntraining, as we need to train each model in the ensemble from scratch. To al-\nleviate this issue, snapshot ensembles [57] uses a cyclical learning rate schedule\nto generate more ensembles in less training time. However, these methods learn\na mixture of posterior modes, which is diﬀerent from an approximation to the\nfull posterior. Regardless, Deep Ensembles have been shown to yield better\n9\nperformance relative to using small numbers of samples in a traditional MCMC\napproach. Deployment considerations similar to those of MCMC ensembles also\napply to deep ensembles.\n2.2.4\nDiscussion\nAs described in this section, diﬀerent methods for approximating Bayesian in-\nference have diﬀerent strengths and weaknesses in terms of their theoretical and\npractical properties.\nThe ﬁrst such property is the bias-variance tradeoﬀin\napproximate posterior estimation. MCMC methods provide an unbiased esti-\nmate of the posterior in theory, but this relies on convergence of the underlying\nMarkov chains, which can be time-consuming. Mixing of the Markov chains is\nanother important factor determining the diversity of the approximate poste-\nrior ensemble. On the other hand, we can learn surrogate posterior distributions\nmuch more eﬃciently using stochastic gradient descent, but they introduce bias\nthat depends on the functional choice of the auxiliary distribution and the di-\nvergence measure used. However, while important, these factors largely impact\ntraining time and the quality of approximations, not their subsequent edge de-\nployability.\nIn terms of edge deployability, a common aspect among all the methods\ndiscussed in this section is that they require the application of Monte Carlo-\nbased approximations at test time. When S samples are used, the computational\ncomplexity of making predictions is S times higher than if a single instance of the\nbase model is used. As noted previously, the use of mixed precision arithmetic\ncan help to accelerate these computations, but a reduction from 32 bit to 16\nbit representations will typically at most half the prediction latency of deployed\nmodels. This is likely to be far from suﬃcient when considering closing the\nprediction latency gap between single optimization-based models and model\nensembles.\nIn addition, as noted previously, MCMC methods have O(S ·K) storage cost\nwhere K is the number of parameters in the base model and S is the number of\nsamples. By contrast, mean-ﬁeld variational inference has O(2K) storage cost,\nbut there appear to be practical challenges to actually realizing the beneﬁts of\nthe reduced storage cost of variational methods on current edge platforms. In\nthe next sections, we turn to possible modeling and algorithmic approaches to\nimproving the edge scalability of approximate Bayesian inference methods in\nterms of both computation time and storage costs.\n3\nModel Pruning Approaches to Improving\nthe Scalability of Bayesian Deep Learning\nAs described in the previous section, the storage and computational scalability\nproperties of Bayesian inference for neural network models can be a signiﬁ-\ncant barrier to edge and IoT deployment, despite their potential beneﬁts in the\ncontext of intelligent systems. In this section, we discuss model pruning and\n10\nsparsiﬁcation approaches that aim to reduce the storage and computation re-\nquirement for Bayesian ensembles. These approaches can be broadly divided\ninto unstructured and structured pruning approaches, as we describe below.\n3.1\nUnstructured Pruning\nOptimization-based unstructured pruning methods aim to compress neural net-\nwork models by sparsifying their weight matrices. The earliest work on un-\nstructured neural network pruning dates back to the Optimal Brain Damage\nmethod [58]. In the optimal brain damage approach, the authors presented a\nTaylor series approximation of the objective function, and show that under the\nassumption that the Hessian matrix is diagonal, the weights corresponding to\nthe second order derivatives that go to zero can be removed with little to no loss\nin performance. The follow-up Optimal Brain Surgeon method [59] highlights\nthat the diagonal Hessian assumption can be limiting, leading to the removal of\nconnections that should be retained. It uses second order derivatives to decide\nwhich connections to remove, and obtains better generalization on held-out test\ndata compared to the Optimal Brain Damage approach.\nAnother line of early work in unstructured pruning removes parameters\nbased on their magnitudes. The earliest work in this area dates back to [60].\nSince then, magnitude-based unstructured pruning has been revisited in more\ndetail, and it has been found that iterative pruning with ﬁne-tuning can help\nprune more parameters of a neural network while retaining better predictive\nperformance compared to one-time post-hoc pruning [61].\nIn the iterative pruning with ﬁne-tuning approach, we deﬁne a pruning rate\np (the percentage of weights to prune) for each pruning cycle and remove all\nweights with magnitudes in the bottom p percentile. We then ﬁne tune the\nmodel by optimizing the unpruned weights to a desired level of convergence.\nWe repeat these pruning and ﬁne-tuning cycles until we achieve the desired\noverall sparsity level.\nExperimental results on iterative pruning with ﬁne-tuning have shown that\nfor the ImageNet data set [62], the authors were able to reduce the number of\nactive parameters in the AlexNet model [63] by 9×, and reduce the number of\nactive parameters in VGG16 [64] by 16×. It is important to note that regu-\nlarization methods (such as the application of an ℓ2 or ℓ1 penalty) can help to\nensure that the magnitude of the weights that are not contributing to predic-\ntive performance are driven to zero. Building upon this, there has been work on\nfurther compressing the model parameters using quantization and Huﬀman cod-\ning [65] after pruning. There has also been additional work on weight magnitude\nthreshold-based pruning that allows for restoring connections [66–68].\nThe Lottery Ticket Hypothesis [69] proposed an iterative pruning method\nthat is very similar to the method of [61], which we refer to as Iterative Pruning\nwith Rewinding. This approach diﬀers from basic iterative pruning in that fol-\nlowing each pruning iteration, the weights used to initialize the next iteration\nof the algorithm are formed by combining the pruned weights with the original\nrandom weight vector (generated during initialization), instead of the weight\n11\nvector obtained at the end of the previous pruning iteration. Eﬀectively, the\nactive weights are rewound to their initial values at the start of each round\nof iterative pruning. Through this approach, the authors found that there are\nsparse substructures within deep neural networks that, when initialized ran-\ndomly, achieve very similar performance to the original dense networks with no\npruning.\nWhile unstructured pruning methods are well-studied in the context of optimization-\nbased deep learning, they have not received as much attention in the Bayesian\ndeep learning literature despite their potential to reduce the storage complex-\nity of posterior model ensembles. Several applications of unstructured sparsity\nare possible and deserve future study. First, MCMC methods can be used to\ngenerate a posterior ensemble consisting of a set of models.\nA single round\nof pruning can then be applied to each of the models to remove a speciﬁed\npercentage of the smallest weights, resulting in an ensemble of weight-sparse\nmodels.\nHowever, optimization-based ﬁne-tuning can not be applied in this\nsetting without the potential for signiﬁcantly altering the distribution that the\nensemble represents. Importantly, there is potential for the sparsiﬁed models in\nsuch an ensemble to tolerate much higher levels of sparsity than an individual\nmodel due to the averaging that occurs over the elements of the ensemble when\nmaking predictions.\nAn alternative approach is to use optimization-based iterative pruning and\nﬁne-tuning to derive a sparse network structure and a starting set of param-\neter values.\nMCMC methods can then be initialized to sample within this\nsparse structure, starting from the parameter values found using optimization.\nOptimization-based iterative pruning approaches can also potentially be com-\nbined with variational Bayesian deep learning methods.\nIn the case of the\nclassical Gaussian mean ﬁeld approximation, both weights that are close to zero\nand weights that have high posterior variance could potentially be pruned from\nthe model. Since variational inference is fundamentally an optimization-based\nprocedure, it can also be composed with an iterative pruning and ﬁne-tuning\nprocess to more closely mimic the process that is used in standard optimization-\nbased deep learning, as described above.\nWe note that while unstructured pruning has been proven to preserve predic-\ntion accuracy even at high levels of weight sparsity for large optimization-based\nmodels, whether and how these savings convert into practical savings in de-\nployed systems is fairly complicated. We ﬁrst consider the storage properties of\nweight-sparse models. In particular, the parameter matrices for a weight-sparse\nmodel must be stored in a compressed format to yield any storage beneﬁt.\nSparse matrices are typically stored either in coordinate list (COO) format\nor compressed sparse column/row (CSC/CSR) format. In these formats, at a\nhigh level, we store the indices and values of the non-zero elements of the matrix.\nFor the COO format, for example, the space complexity of the resulting data\nstructure is O(3N), where N is the number of non-zero elements. Importantly,\nhow we compose weight sparsity with Bayesian deep learning can have signiﬁcant\nimpact on the storage complexity of the resulting models.\nIn the ﬁrst approach described above, we considered independently sparsi-\n12\nfying each element of the posterior ensemble. If we retain a total of N non-zero\nweights for each of the S models, the total storage required is O(3NS). In the\nsecond approach, we considered sampling withing a ﬁxed sparse structure. If\nthis sparse structure has N non-zero weights, the total required storage is the-\noretically, O(2N + NS) since we only need to store the indices of the non-zero\nelements once. For S ≫2, this reduces the required storage of the sparse ensem-\nble by 66% over standard COO format. While no current sparse matrix formats\nexist that support this optimization, it would be straightforward to implement\nfrom a storage perspective.\nLastly, we note that realizing the computational savings of weight-sparse net-\nworks with GPUs and TPUs is also currently a challenge. PyTorch, for example,\nhas sparse matrix support that is currently in beta testing. Existing libraries for\nedge GPU/TPU-accelerated architectures such as TensorRT currently only sup-\nport highly restricted sparsity patterns. As a result, fully realizing the theoret-\nical computational beneﬁt of weight-sparse Bayesian deep learning approaches\non edge platforms will require additional support for GPU/TPU-accelerated\nlinear algebra operations over sparse matrices.\n3.2\nStructured Pruning\nOptimization-based structured pruning methods apply similar pruning princi-\nples to those of unstructured pruning, but with the aim of pruning larger struc-\ntural elements such as entire hidden unit or entire convolutional kernels. The\nsimplest structured pruning method extend the iterative pruning approaches\ndescribed above to also include removing hidden units or convolutional kernels\nthat have no incoming connections [61]. However, the induced sparsity pat-\nterns can be such that few units are actually pruned. These basic approaches\ncan be improved by modifying the pruning criteria to prune units where the\nnorm of their incoming weights is in the bottom p percentile across all active\nunits [70]. This approach ensures that a desired number of units is pruned from\nthe network on each round.\nAnother set of approaches leverage group LASSO (least absolute shrinkage\nand selection operator) regularization to encourage a group of incoming weights\nto go to zero simultaneously [71–74]. To apply this approach, we must ﬁrst\nmust partition the model parameters in the parameter vector φ into K groups\nGk. The form of the regularizer is R(φ) = PK\nk=1\n\u0000 P\nj∈Gk φ2\nj\n\u00011/2 when using the\ngroup ℓ1/ℓ2 regularizer. For a feedforward model, we place all the incoming\nweights for each hidden unit into a group. Similarly, we collect all the incoming\nweights for a particular channel in a convolution layer together into a group.\nThe regularizer will then tend to encourage all the weights in a group to go to\nzero or most of the weights in a group to be non-zero. This can make it much\neasier to identify structures for pruning using weight norm-based criteria and\ncan require less ﬁne-tuning as most inputs to a unit will tend to already be close\nto zero prior to pruning.\nStructured pruning can also be composed with approximate Bayesian deep\nlearning methods in multiple ways. As with unstructured pruning, we can also\n13\nseparately apply structured pruning to each element of a posterior ensemble.\nHowever, the chances that all incoming weights for a unit will be close to zero\nis many times smaller than the chance that a single weight will be small. In\nthis case, the use of an explicit sparsity inducing prior, such as a spike-and-\nslab prior, would very likely be necessary to obtain meaningful pruning. The\napproach of using optimization-based structured pruning methods to select a\nstructure and an initial set of weights to run MCMC-based methods from is\nalso applicable and is a potentially promising approach. Again, there are close\nrelationships between structured sparsity methods for optimization-based deep\nlearning and variational Bayesian deep learning. There is also speciﬁc prior work\non sparsity inducing priors for variational Bayesian methods. Previous work in\nthis area includes the use of horseshoe priors [75] for approximate Bayesian\ninference [76,77].\nFinally, we note that there is a signiﬁcant gap between the practical imple-\nmentation of models that result from structured sparsity methods and unstruc-\ntured sparsity methods. This is due to the fact that structured sparsity methods\nlearn more compact dense models that do not need sparse matrix support to\nrealize storage and computational savings. This is a signiﬁcant advantage con-\nsidering the current state of sparse matrix support in software frameworks for\nedge platforms. However, structured sparsity methods have been observed to\nrequire more non-zero weights than weight-sparse models to obtain similar levels\nof predictive performance [78]. On the other hand, the run time of linear alge-\nbra operations for sparse matrices on real hardware can have signiﬁcantly more\noverhead than when operating over dense matrices. The trade-oﬀs between the\ntheoretical and practical properties of these approaches requires further study,\nand the best approach to use is likely to be highly dependent on the hardware\nand software support available on a particular deployment platform.\n4\nModel Distillation Approaches to Improving\nthe Scalability of Bayesian Deep Learning\nIn this section we describe posterior distillation methods, which provide an al-\nternative to pruning methods that can also both decrease the storage cost and\nthe computational cost of applying Bayesian deep learning methods. Examples\nof methods in this area include Bayesian Dark Knowledge (BDK) [79] and Gen-\neralized Posterior Expectation Distillation (GPED) [19]. These methods aim to\ncompress the computation of expectations under the model posterior into neural\nnetworks whose storage and computational complexity can be set to match de-\nployment constraints. As a result, distillation approaches can expose a ﬂexible\ntrade-oﬀbetween resource use and posterior approximation accuracy.\nIn the case of BDK, the selected posterior expectation is the posterior pre-\ndictive distribution. BDK approximates the posterior predictive distribution by\nlearning an auxiliary neural network model to compress the Monte Carlo ap-\nproximation to the posterior predictive distribution EpMC(θ|D,λ)[p(y|x, θ)]. The\n14\nGPED approach extends the BDK approach to the case of distilling arbitrary\nposterior expectations. GPED has been used to directly approximate model\nand data uncertainty via posterior expectation distillation.\nThe major advantage of this family of approaches is that they can drastically\nreduce the deployment time computational complexity of posterior predictive\ninference relative to using a Monte Carlo average computed using many samples.\nHowever, a shortcoming of this family of approaches is that they only capture\nthe target posterior expectations. Thus, they do not have the ability to compute\nother statistics without being re-trained.\nEnsemble distribution distillation (EnD2) is a closely related approach that\naims to distill the collective predictive distribution outputs of the models in an\nensemble into a neural network that predicts the parameters of a Dirichlet distri-\nbution [18]. The goal is to preserve more information about the distribution of\noutputs of the ensemble in such a way that multiple statistics of the ensemble’s\noutputs can be eﬃciently approximated. We note that the EnD2 approach can\nbe applied to any ensemble of models producing categorical output distributions\nand can thus be applied to distill the predictive distributions of the elements of\na posterior ensemble obtained using MCMC methods as well as those obtained\nfrom a variational approximation. We also note that this approach can be ex-\ntended to approximate the distribution of other posterior quantities by distilling\nin to approximating models that output other types of distributions.\nThere is an interesting trade-oﬀbetween distilling the full parameter poste-\nrior distribution into models that predict speciﬁc posterior expectations, such\nas BDK and GPED, and approaches that distill aspects of the posterior into\ndistributions. As noted above, expectation distillation is a less general approach\nand models must be re-trained to extend coverage to additional expectations.\nOn the other hand, EnD2 gains generality by predicting parametric distribu-\ntions, from which a wider range of posterior properties can be computed. The\ndrawback of this approach is that it introduces irreducible bias via the selection\nof a particular approximating family of distributions in a way that is similar to\nvariational inference. As a result, although a wider range of posterior properties\ncan be estimated using EnD2, their accuracy can be varied and all can be biased\nif the approximating family is not a good match to the true posterior.\nIn terms of deployment on edge hardware, one of the distinct advantages of\ndistillation-based approximations over both MCMC and variational methods is\nthat the deployment time computational complexity of approximating posterior\nexpectations can be completely controlled via the selection of the model archi-\ntecture that the posterior is distilled into. Once this architecture is selected\nand the distillation process is carried out, the result is a single model (or one\nmodel per posterior expectation of interest) that can then be deployed. This\ncan be much simpler than deploying a posterior ensemble in the case of MCMC\nmethods.\nHowever, maximizing the performance of distillation-based methods for a\ngiven computational and storage budget requires performing an architecture\nsearch to determine the optimal architecture for the approximating model. Since\ndistillation-based learning is itself an optimization problem, one method to per-\n15\nform this search is to start with a large model architecture and apply iterative\npruning and ﬁne-tuning as described in the previous section. The use of both\nweight-level and the structure-level pruning is possible, with the deployment\ncaveats noted in the previous section.\nThe GPED paper [19] uses such an approach to expose storage-performance\nand computation-performance Pareto frontiers by varying the level of pruning\nused. That work shows that pruning-based selection of model architectures is\nsuperior to more basic approaches like searching the space of layer-width mul-\ntipliers. More generally, it demonstrates the potential for signiﬁcant sensitivity\nto the architecture of the approximating model. The question of how to most\neﬃciently search for an optimal distillation architecture remains an open ques-\ntion.\n5\nConclusions and Discussion\nIn this paper we have provided a comprehensive overview of approaches to ap-\nproximate Bayesian inference from the speciﬁc perspective of the challenges that\nemerge when considering the deployment of Bayesian deep learning models on\nedge hardware. We have presented a number of potential research directions\naimed at improving the deployment scalability of Bayesian deep learning meth-\nods, with a focus on pruning and distillation-based model compression meth-\nods. Realizing such improvements will be crucial to enabling the practical use\nof Bayesian deep learning methods on edge hardware and to the development\nof intelligent IoT systems.\nLastly, we note that the nature of current edge hardware is likely to force\ntrade-oﬀs between multiple facets of the performance of Bayesian deep learn-\ning models deployed at the edge, including predictive performance, uncertainty\nquantiﬁcation ability, robustness, and prediction latency. Methods like distil-\nlation and pruning can provide ﬂexible trade-oﬀs between these facets of per-\nformance, and it is imperative that candidate models are comprehensively and\nsimultaneously evaluated with respect to all facets of performance to ensure that\ngains with respect to one subset of facets does not come at an unacceptable cost\nwith respect to other subsets [20].\nIt is also worth emphasizing that the approximation approaches that yield\noptimal Bayesian deep learning deployments in respect to one edge platform\ncould be far from optimal for another platform (or could even be infeasible)\ndepending on the properties of the platform, including available storage and\nthe speed and level of parallelism of computation. The problem of learning to\npredict optimal deployment conﬁgurations across platforms and tasks is itself a\nvery interesting and important research challenge in this space.\n16\nAcknowledgement\nThis work was partially supported by the US Army Research Laboratory under\ncooperative agreement W911NF-17-2-0196.\nThe views and conclusions con-\ntained in this document are those of the authors and should not be interpreted\nas representing the oﬃcial policies, either expressed or implied, of the Army\nResearch Laboratory or the US government.\nReferences\n[1] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with\ndeep recurrent neural networks,” in Acoustics, speech and signal processing\n(icassp), 2013 ieee international conference on.\nIEEE, 2013, pp. 6645–\n6649.\n[2] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” 2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 2261–2269, 2016.\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT, 2018.\n[4] G. Singh, M. P. Vadera, L. Samavedham, and E. C. H. Lim, “Machine\nlearning-based framework for multi-class diagnosis of neurodegenerative\ndiseases: A study on parkinson’s disease,” IFAC-PapersOnLine, vol. 49,\npp. 990–995, 2016.\n[5] G. Singh,\nM. Vadera,\nL. Samavedham,\nand E. C.-H. Lim,\n“Mul-\nticlass\ndiagnosis\nof\nneurodegenerative\ndiseases:\nA\nneuroimaging\nmachine-learning-based approach,” Industrial & Engineering Chemistry\nResearch, vol. 58, no. 26, pp. 11 498–11505, 2019. [Online]. Available:\nhttps://doi.org/10.1021/acs.iecr.8b06064\n[6] K. Suzuki, “Overview of deep learning in medical imaging,” Radiological\nphysics and technology, vol. 10, no. 3, pp. 257–273, 2017.\n[7] M. P. Vadera and B. M. Marlin,\n“Poster abstract:\nInvestigating\nfusion-based deep learning architectures for smoking puﬀdetection,” in 4th\nIEEE/ACM International Conference on Connected Health: Applications,\nSystems and Engineering Technologies, CHASE 2019, Arlington, VA,\nUSA, Septempter 25-27, 2019.\nIEEE, 2019, pp. 11–12. [Online]. Available:\nhttps://doi.org/10.1109/CHASE48038.2019.00011\n[8] S.\nMinaee,\nY.\nBoykov,\nF.\nPorikli,\nA.\nPlaza,\nN.\nKehtarnavaz,\nand\nD.\nTerzopoulos,\n“Image\nsegmentation\nusing\ndeep\nlearning:\nA\nsurvey,”\nCoRR,\nvol.\nabs/2001.05566,\n2020.\n[Online].\nAvailable:\nhttps://arxiv.org/abs/2001.05566\n17\n[9] M. P. Vadera, S. Ghosh, K. Ng, and B. M. Marlin, “Post-hoc loss-\ncalibration for bayesian neural networks,” in UAI, 2021.\n[10] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of mod-\nern neural networks,” in International Conference on Machine Learning,\n2017, pp. 1321–1330.\n[11] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassiﬁed\nand out-of-distribution examples in neural networks,” in 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nOpenReview.net, 2017.\n[Online]. Available: https://openreview.net/forum?id=Hkg4TI9xl\n[12] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” in ICLR, 2015.\n[13] A. G. Wilson, “The case for bayesian deep learning,” 2020.\n[14] M. P. Vadera, S. N. Shukla, B. Jalaian, and B. M. Marlin, “Assessing the\nadversarial robustness of monte carlo and distillation methods for deep\nbayesian neural network classiﬁcation,” in AAAI SafeAI Workshop, 2020.\n[15] R. M. Neal, Bayesian Learning for Neural Networks.\nBerlin, Heidelberg:\nSpringer-Verlag, 1996.\n[16] S. Depeweg, J. M. Hern´andez-Lobato, F. Doshi-Velez, and S. Udluft, “De-\ncomposition of uncertainty for active learning and reliable reinforcement\nlearning in stochastic systems,” ArXiv, vol. abs/1710.07283, 2017.\n[17] K.-C. Wang, P. Vicol, J. Lucas, L. Gu, R. Grosse, and R. Zemel, “Ad-\nversarial distillation of bayesian neural network posteriors,” arXiv preprint\narXiv:1806.10317, 2018.\n[18] A. Malinin, B. Mlodozeniec, and M. Gales, “Ensemble distribution\ndistillation,” in International Conference on Learning Representations,\n2020. [Online]. Available: https://openreview.net/forum?id=BygSP6Vtvr\n[19] M. P. Vadera, B. Jalaian, and B. M. Marlin, “Generalized bayesian poste-\nrior expectation distillation for deep neural networks,” in UAI, 2020.\n[20] M. P. Vadera, A. D. Cobb, B. Jalaian, and B. M. Marlin, “URSABench:\nComprehensive Benchmarking of Approximate Bayesian Inference Meth-\nods for Deep Neural Networks,” in ICML Workshop on Uncertainty and\nRobustness in Deep Learning, 2020.\n[21] N. Houlsby, F. Husz´ar, Z. Ghahramani, and M. Lengyel, “Bayesian ac-\ntive learning for classiﬁcation and preference learning,” arXiv preprint\narXiv:1112.5745, 2011.\n18\n[22] C. Holtsclaw, M. P. Vadera, and B. M. Marlin, “Towards joint segmenta-\ntion and active learning for block-structured data streams,” in The ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining Workshop\non Data Collection, Curation, and Labeling for Mining and Learning, 2019.\n[23] A. Kirsch, J. van Amersfoort, and Y. Gal, “Batchbald: Eﬃcient and diverse\nbatch acquisition for deep bayesian active learning,” in Advances in Neu-\nral Information Processing Systems 32: Annual Conference on Neural In-\nformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer,\nF. d’Alch´e-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 7024–7035.\n[24] A. F. Smith and G. O. Roberts, “Bayesian computation via the gibbs sam-\npler and related markov chain monte carlo methods,” Journal of the Royal\nStatistical Society: Series B (Methodological), vol. 55, no. 1, pp. 3–23, 1993.\n[25] G. Casella and E. I. George, “Explaining the gibbs sampler,” The American\nStatistician, vol. 46, no. 3, pp. 167–174, 1992.\n[26] S. Chib and E. Greenberg, “Understanding the metropolis-hastings algo-\nrithm,” The american statistician, vol. 49, no. 4, pp. 327–335, 1995.\n[27] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth, “Hybrid monte\ncarlo,” Physics letters B, vol. 195, no. 2, pp. 216–222, 1987.\n[28] R. M. Neal, “Slice sampling,” Annals of statistics, pp. 705–741, 2003.\n[29] I. Murray, R. P. Adams, and D. J. C. MacKay, “Elliptical slice sampling,”\nin AISTATS, 2010.\n[30] M. Girolami and B. Calderhead, “Riemann manifold langevin and hamilto-\nnian monte carlo methods,” Journal of the Royal Statistical Society: Series\nB (Statistical Methodology), vol. 73, no. 2, pp. 123–214, 2011.\n[31] L. Bottou, “Large-scale machine learning with stochastic gradient descent,”\nin Proceedings of COMPSTAT’2010.\nSpringer, 2010, pp. 177–186.\n[32] M. Welling and Y. W. Teh, “Bayesian learning via stochastic gradient\nlangevin dynamics,” in Proceedings of the 28th international conference\non machine learning (ICML-11), 2011, pp. 681–688.\n[33] T. Chen, E. B. Fox, and C. Guestrin, “Stochastic gradient hamiltonian\nmonte carlo,” in International Conference on Machine Learning, 2014.\n[34] R. Zhang, C. Li, J. Zhang, C. Chen, and A. G. Wilson, “Cyclical stochastic\ngradient mcmc for bayesian deep learning,” in ICLR, 2020.\n[35] S. Brooks, A. Gelman, G. Jones, and X.-L. Meng, Handbook of markov\nchain monte carlo.\nCRC press, 2011.\n19\n[36] P. Izmailov, W. J. Maddox, P. Kirichenko, T. Garipov, D. P. Vetrov, and\nA. G. Wilson, “Subspace inference for bayesian deep learning,” in UAI,\n2019.\n[37] NVIDIA Corporation. TensorRT Developer Guide. [Online]. Available:\nhttps://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\n[38] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, “An intro-\nduction to variational methods for graphical models,” Machine learning,\nvol. 37, no. 2, pp. 183–233, 1999.\n[39] T. S. Jaakkola and M. I. Jordan, “Bayesian parameter estimation via varia-\ntional methods,” Statistics and Computing, vol. 10, no. 1, pp. 25–37, 2000.\n[40] S. Ghosh, F. M. Delle Fave, and J. Yedidia, “Assumed density ﬁltering\nmethods for learning bayesian neural networks,” in Thirtieth AAAI Con-\nference on Artiﬁcial Intelligence, 2016.\n[41] T. P. Minka, “Expectation propagation for approximate bayesian infer-\nence,” in Proceedings of the Seventeenth conference on Uncertainty in arti-\nﬁcial intelligence.\nMorgan Kaufmann Publishers Inc., 2001, pp. 362–369.\n[42] C.\nBlundell,\nJ.\nCornebise,\nK.\nKavukcuoglu,\nand\nD.\nWierstra,\n“Weight uncertainty in neural network,” in Proceedings of the 32nd\nInternational Conference on Machine Learning,\nser. Proceedings of\nMachine Learning Research, F. Bach and D. Blei, Eds., vol. 37.\nLille,\nFrance:\nPMLR, 07–09 Jul 2015, pp. 1613–1622. [Online]. Available:\nhttps://proceedings.mlr.press/v37/blundell15.html\n[43] D. J. MacKay, Information theory, inference and learning algorithms.\nCambridge university press, 2003.\n[44] Y. Li and R. E. Turner, “R´enyi divergence variational inference,” in Ad-\nvances in Neural Information Processing Systems 29, 2016, pp. 1073–1081.\n[45] M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley, “Stochastic variational\ninference,” The Journal of Machine Learning Research, vol. 14, no. 1, pp.\n1303–1347, 2013.\n[46] C. Louizos and M. Welling, “Multiplicative normalizing ﬂows for varia-\ntional bayesian neural networks,” in Proceedings of the 34th International\nConference on Machine Learning-Volume 70.\nJMLR. org, 2017, pp. 2218–\n2227.\n[47] D. Krueger, C.-W. Huang, R. Islam, R. Turner, A. Lacoste, and A. C.\nCourville, “Bayesian hypernetworks,” ArXiv, vol. abs/1710.04759, 2017.\n[48] M. Dusenberry, G. Jerfel, Y. Wen, Y. Ma, J. Snoek, K. Heller, B. Lakshmi-\nnarayanan, and D. Tran, “Eﬃcient and scalable bayesian neural nets with\nrank-1 factors,” in International conference on machine learning.\nPMLR,\n2020, pp. 2782–2792.\n20\n[49] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Rep-\nresenting model uncertainty in deep learning,” in international conference\non machine learning, 2016, pp. 1050–1059.\n[50] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdi-\nnov, “Dropout: a simple way to prevent neural networks from overﬁtting,”\nThe Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958,\n2014.\n[51] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-\nperformance deep learning library,” in Advances in Neural Information Pro-\ncessing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-\nBuc, E. Fox, and R. Garnett, Eds.\nCurran Associates, Inc., 2019, pp.\n8024–8035.\n[52] J. Bai, F. Lu, K. Zhang et al., “Onnx: Open neural network exchange,”\nhttps://github.com/onnx/onnx, 2019.\n[53] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in neural information processing systems, 2014, pp. 2672–2680.\n[54] C. Henning, J. von Oswald, J. Sacramento, S. C. Surace, J.-P. Pﬁster, and\nB. F. Grewe, “Approximating the predictive distribution via adversarially-\ntrained hypernetworks,” in Bayesian Deep Learning Workshop, NeurIPS\n(Spotlight) 2018, 2018.\n[55] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable\npredictive uncertainty estimation using deep ensembles,” in Advances in\nNeural Information Processing Systems, 2017, pp. 6402–6413.\n[56] A. G. Wilson and P. Izmailov, “Bayesian deep learning and a probabilis-\ntic perspective of generalization,” in Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020.\n[57] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, and K. Q. Weinberger,\n“Snapshot ensembles:\nTrain 1, get M for free,” in 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nOpenReview.net, 2017.\n[Online]. Available: https://openreview.net/forum?id=BJYwwY9ll\n[58] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in\nAdvances in Neural information processing systems, 1989.\n21\n[59] B. Hassibi, D. G. Stork, and G. J. Wolﬀ, “Optimal brain surgeon and gen-\neral network pruning,” IEEE International Conference on Neural Networks,\npp. 293–299 vol.1, 1993.\n[60] J. Hertz, A. Krogh, and R. G. Palmer, Introduction to the Theory of Neural\nComputation.\nUSA: Addison-Wesley Longman Publishing Co., Inc., 1991.\n[61] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and con-\nnections for eﬃcient neural networks,” in Advances in Neural information\nprocessing systems, 2015.\n[62] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference on\ncomputer vision and pattern recognition.\nIeee, 2009, pp. 248–255.\n[63] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with\ndeep convolutional neural networks,” in Advances in neural information\nprocessing systems, 2012, pp. 1097–1105.\n[64] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[65] S. Han, H. Mao, and W. J. Dally, “Deep compression:\nCompressing\ndeep neural network with pruning, trained quantization and huﬀman\ncoding,” in 4th International Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track\nProceedings, Y. Bengio and Y. LeCun, Eds., 2016. [Online]. Available:\nhttp://arxiv.org/abs/1510.00149\n[66] Y. Guo, A. Yao, and Y. Chen, “Dynamic network surgery for eﬃcient\ndnns,” in Advances in Neural information processing systems, 2016.\n[67] X. Jin, X.-T. Yuan, J. Feng, and S. Yan, “Training skinny deep neu-\nral networks with iterative hard thresholding methods,” ArXiv, vol.\nabs/1607.05423, 2016.\n[68] S. Han, J. Pool, S. Narang, H. Mao, S. Tang, E. Elsen, B. Catanzaro,\nJ. Tran, and W. J. Dally, “Dsd: Regularizing deep neural networks with\ndense-sparse-dense training ﬂow,” ArXiv, vol. abs/1607.04381, 2016.\n[69] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Training pruned\nneural networks,” ArXiv, vol. abs/1803.03635, 2018.\n[70] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning ﬁlters\nfor eﬃcient convnets,” ArXiv, vol. abs/1608.08710, 2016.\n[71] Y. Zhang and Z. Ou, “Learning sparse structured ensembles with stochastic\ngradient mcmc sampling and network pruning,” 2018 IEEE 28th Interna-\ntional Workshop on Machine Learning for Signal Processing (MLSP), pp.\n1–6, 2018.\n22\n[72] J. M. Alvarez and M. Salzmann, “Learning the number of neurons in deep\nnetworks,” in Advances in Neural Information Processing Systems, 2016,\npp. 2270–2278.\n[73] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured spar-\nsity in deep neural networks,” in Advances in neural information processing\nsystems, 2016, pp. 2074–2082.\n[74] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating very deep\nneural networks,” 2017 IEEE International Conference on Computer Vi-\nsion (ICCV), pp. 1398–1406, 2017.\n[75] C. M. Carvalho, N. G. Polson, and J. G. Scott, “Handling sparsity via\nthe horseshoe,” in Artiﬁcial Intelligence and Statistics.\nPMLR, 2009, pp.\n73–80.\n[76] S. Ghosh and F. Doshi-Velez, “Model selection in bayesian neural networks\nvia horseshoe priors,” arXiv preprint arXiv:1705.10388, 2017.\n[77] C. Louizos, K. Ullrich, and M. Welling, “Bayesian compression for deep\nlearning,” ArXiv, vol. abs/1705.08665, 2017.\n[78] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the state of\nneural network pruning?” arXiv preprint arXiv:2003.03033, 2020.\n[79] A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling, “Bayesian dark\nknowledge,” in Advances in Neural Information Processing Systems, 2015,\npp. 3438–3446.\n23\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-12-03",
  "updated": "2021-12-03"
}