{
  "id": "http://arxiv.org/abs/2112.01705v1",
  "title": "Multilingual Text Classification for Dravidian Languages",
  "authors": [
    "Xiaotian Lin",
    "Nankai Lin",
    "Kanoksak Wattanachote",
    "Shengyi Jiang",
    "Lianxi Wang"
  ],
  "abstract": "As the fourth largest language family in the world, the Dravidian languages\nhave become a research hotspot in natural language processing (NLP). Although\nthe Dravidian languages contain a large number of languages, there are\nrelatively few public available resources. Besides, text classification task,\nas a basic task of natural language processing, how to combine it to multiple\nlanguages in the Dravidian languages, is still a major difficulty in Dravidian\nNatural Language Processing. Hence, to address these problems, we proposed a\nmultilingual text classification framework for the Dravidian languages. On the\none hand, the framework used the LaBSE pre-trained model as the base model.\nAiming at the problem of text information bias in multi-task learning, we\npropose to use the MLM strategy to select language-specific words, and used\nadversarial training to perturb them. On the other hand, in view of the problem\nthat the model cannot well recognize and utilize the correlation among\nlanguages, we further proposed a language-specific representation module to\nenrich semantic information for the model. The experimental results\ndemonstrated that the framework we proposed has a significant performance in\nmultilingual text classification tasks with each strategy achieving certain\nimprovements.",
  "text": "Multilingual Text Classification for Dravidian Languages  \n \nXiaotian Lin1, Nankai Lin1, Kanoksak Wattanachote1, Shengyi Jiang1,2(ï€ª), Lianxi Wang1,2 (ï€ª) \n1. School of Information Science and Technology, Guangdong University of Foreign Studies \nGuangzhou \n2. Guangzhou Key Laboratory of Multilingual Intelligent Processing, Guangdong University of \nForeign Studies, Guangzhou \njiangshengyi@163.com, wanglianxi@gdufs.edu.cn \n \n \nAbstract. As the fourth largest language family in the world, the Dravidian languages have become a \nresearch hotspot in natural language processing (NLP). Although the Dravidian languages contain a large \nnumber of languages, there are relatively few public available resources. Besides, text classification task, \nas a basic task of natural language processing, how to combine it to multiple languages in the Dravidian \nlanguages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to address these \nproblems, we proposed a multilingual text classification framework for the Dravidian languages. On the \none hand, the framework used the LaBSE pre-trained model as the base model. Aiming at the problem \nof text information bias in multi-task learning, we propose to use the MLM strategy to select language-\nspecific words, and used adversarial training to perturb them. On the other hand, in view of the problem \nthat the model cannot well recognize and utilize the correlation among languages, we further proposed a \nlanguage-specific representation module to enrich semantic information for the model. The experimental \nresults demonstrated that the framework we proposed has a significant performance in multilingual text \nclassification tasks with each strategy achieving certain improvements. \nKeywords. Dravidian Languages, Multilingual Text Classification, Multi-task learning \n1. Introduction \nDravidian languages are the common terminology used to represent the South Indian languages, \nwhich consist of around 26 languages. Out of these 26 Dravidian languages, Tamil, Malayalam, and \nKannada are regarded as official languages and have been spoken by around 220 million people in the \nIndian subcontinent, Singapore, and Sri Lanka. As the fourth largest languages in the world, although \nscholars have carried out targeted research on it, there are mainly the following problems in it. \n(1) Existing researches on Dravidian languages mainly focus on processing text in one certain language. \nNevertheless, Dravidian languages include multiple languages, and technology for one certain \nlanguage cannot be applied to other languages well, even if these languages are in the same language \nfamily. \n(2) As far as we know, due to the existence of shared tokens among Dravidian languages, there is a \ncertain correlation among Dravidian languages. Nevertheless, the existing models focus on studying \nmonolingual without considering the correlation among the Dravidian languages. These researches \n \nXiaotian Lin, Nankai Lin and Kanoksak Wattanachote are co-first authors of the article. \nShengyi Jiang and Lianxi Wang are co-corresponding authors.    \ncannot effectively promote the multilingual research of the entire Dravidian language. \n(3) Most of the existing works utilize language features to improve the effectiveness of the model, such \nas affixes, syntactic structure, and so on. Because of the differences in grammar among different \nlanguages, these methods are difficult to transfer to other languages. \n(4) Multi-task learning is an important technology in natural language processing. It can solve the \nproblem of scarcity of annotation resources for each task by combining multiple tasks. There are \nsome studies that apply multi-task learning to multi-language research, so that information between \nmultiple languages can be shared. However, there are soly slightly researchers, whose their \nresearches apply multi-task learning to deal with multilingual tasks in the Dravidian language. \nHence, we aimed to apply multi-task learning technology for the Dravidian language to address these \nproblems. Multi-task Learning (MTL) proposed to learn shared information among multiple related tasks \nand obtain better performance than learning each task independently. Since multi-task learning utilizes \npotential correlations among related tasks to extract common features and yield performance gains, it has \nbeen widely used in text classification tasks. For example, Zhao et al. (2020) proposed to utilize multi-\ntask learning for text classification, which is composed of a shared encoder, a multi-label classification \ndecoder, and a hierarchical categorization decoder. However, because multi-task learning usually shares \nthe parameters of the general presentation layer among different task, it will cause the problem of text \ninformation bias when processing the multilingual classification tasks, resulting in the model only \nperforming well in individual languages. Therefore, how to integrate more task learning for multilingual \ntext classification tasks, is still a major difficulty in NLP. \n \nTo address these issues above, based on multi-task learning and adversarial training, we proposed \na multilingual text classification framework for the Dravidian languages. On the one hand, the framework \nuses the LaBSE pre-trained model as the base model. Aiming at the problem of text information bias in \nmulti-task learning, we propose to use the MLM strategy to select language-specific words, and use \nadversarial training to perturb them. On the other hand, in view of the problem that the model cannot \nwell recognize and utilize the correlation among languages, we further propose a language representation \nmodule to enrich semantic information for the model. \nThe main contributions of this paper are as follows: \n(1) We proposed a multilingual text classification framework to better handle multilingual text \nclassification tasks. \n(2) We proposed language-specific word extraction technology based on MLM strategy to extract \nlanguage-specific words. \n(3) We proposed to perturb the language information to solve the problem of language information bias \nin multilingual text classification. \n(4) We proposed an innovative method to extract the knowledge about the correlation among languages \ninto the model. \n(5) The framework we proposed has a significant performance in multilingual text classification tasks \nwith each strategy achieving certain improvements. \n \n2. Related work \n2.1 Multilingual text classification \n  Multilingual text classification is one of the research hotspots in the field of natural language \nprocessing. Compared with single-language text classification, there are two unresolved difficulties: (1) \nIt is difficult to share and uniformly express the semantic space of different languages. (2) The existing \nmethods are mostly for single language texts while it has low adaptability to Multilingual texts. To \naddress these problems, some researchers put forward some valuable approaches. Liu et al. (2018) \nproposed to use automatic associative memory with multilingual data fusion to realize multilingual short \ntext classification tasks. Meng et al. (2019) proposed a model based on LDA and Bi-LSTM-CNN to solve \nthe problem of multilingual short text classification, and used topic vectors and word vectors to extract \ntext information in each language. In this vein, their proposed ideas solved the problem of the scarcity in \nshort text features to a certain extent. Meng et al. (2020) proposed a multilingual text classification model \ncombining Bi-LSTM and CNN to extract text features and obtain deeper text representation in various \nlanguages. Meng (2018) used multilingual text feature conversion and fusion strategies to solve the \ndomain adaptability of the classifier in different languages, and employed deep learning strategies to \nimprove the accuracy of the classifiers. Groenwold et al. (2020) evaluated the effects of multiple pre-\ntrained language models on multilingual classification tasks. Kazhuparambil and Kaushik (2020) \nproposed to use the XLM model to automatically classify YouTube comments mixed in English and \nMalay, which achieves the best results. Mishra et al. (2020) developed to leverage a variety of \ntransformed models for different data sets in the TRAC2020 evaluation task and fine-tuned them. They \nalso proposed joint label classification and multi-language joint training methods to improve \nclassification performance for label marginalization problems. \nAlthough these multilingual text classification methods have attracted increasing research interests in \nrecent years, there are no publicly available corpus and targeted researches for multilingual China-related \nnews classification. \n2.2 Researches for Dravidian languages \n \nSo far, there are relatively few researches on text classification in term of the Dravidian languages \nand the lack of publicly available corpus. As far as we know, the first evaluation task for hate speech/ \noffensive content detection and sentiment analysis for the Dravidian languages were all first proposed in \nFIRE2020. \n \nIn offensive Language identification tasks, Sai and Sharma (2020) proposed a novel method using \ntranslation and transliteration method, which can obtain better results from fine-tuning and integrated \nmulti-language converter networks such as XLM-RoBERTa and mBERT. Hande et al. (2021) proposed \nto generate a pseudo-labels dataset called CMTRA to increase the amount of training data for the \nlanguage models and fine-tune several recent pre-trained language models on this newly constructed \ndataset. In addition, through using TF-IDF vectors and character-level n-grams as features. Veena et al. \n(2020) developed and evaluated four systems for processing Malayalam, including logistic regression, \nXGBoost, long and short-term memory networks, and attention networks. As for the sentiment analysis \ntasks, to overcome the code-mixed and grammatical irregularities problems, Chakravarthi et al. (2020) \ncreated a gold standard Tamil-English code-switched, sentiment-annotated corpus. Sachin Kumar et al. \n(2018) focused on providing a comparative study for identifying sentiment of Malayalam tweets using \ndeep learning methods such as convolutional neural net (CNN), long short-term memory units (LSTM). \nSun and Zhou (2020) used the hidden state of XLM-Roberta to extract semantic information. They \nproposed a new model by extracting the output of the top hidden layer in XLM-Roberta and providing \nthem as input to the convolutional neural network, and finally connecting them to obtain better results. \n2.3 Adversarial training in natural language processing \nAdversarial training refers to the method of constructing adversarial samples and mixing them with \nthe original samples to train the model, which can improve the generalization performance of the original \nexamples. Previous work mainly used adversarial training in the image field. For natural language \nprocessing, there are several targeted researches. \nGao et al. (2018) proposed to generate small adversarial perturbations for the original samples ina \nblack-box setting, making the model misclassify the text sample. In particular, their method includes two \nsteps: (1) Finding the most important words for modification through a scoring strategy, which causes \nthe deep classifier to make an incorrect prediction. (2) By applying a simple character-level conversion \nto word ranked highest to make the edit distance of the disturbance minimized. Miyato et al. (2017) first \nproposed to use adversarial training for word embedding in text input. However, this method is lack of \ninterpretability. In order not to affect the performance of the model, while generating adversarial text for \nperturbation, Sato et al. (2018) proposed to limit the directions of perturbations toward the existing words \nin the word embeddings. \n3. Framework \nIn this paper, we proposed a multilingual text classification framework based on multi-task learning \nand adversarial training. As shown in Figure 1, the framework contains four components: (1) Text general \nrepresentation module; (2) Language-specific words extraction module; (3) Language information \nperturbation module; (4) Language-specific representation module. \nThe framework uses the text general representation module to extract the vector representation of \nthe text. At the same time, in the language-specific words extraction module, the framework utilizes \nMLM strategy to identify important language-specific words in sentences. Next, the language \ninformation perturbation module injects different degrees of noise into the embeddings of language-\nspecific words and other words to perturb the generalization of the model. In order to fully learn the \ngeneral semantic information and the correlation among languages, we further spliced the language \ndescriptor with the sentence vector ğ‘†ğ‘– output by the general representation module. \n \nFigure 1. The framework structure. \n3.1 Text general representation \n  \nLaBSE (Language-agnostic BERT Sentence Encoder)1 presented by Feng et al. (2020) is a BERT-\nbased model trained on 17 billion monolingual sentences and 6 billion bilingual sentence pairs, resulting \nin a model that is effective even on low-resource languages for which there is no data available during \ntraining. Whatâ€™s more, unlike BERT, LaBSE removes the NSP (Next Sentence Prediction) task and MLM \npre-training has been extended to the multilingual setting by modifying MLM training to include \nconcatenated translation pairs, known as translation language modeling (TLM). \n \nFor classification tasks, given an input sentence, its input representation is the sum of the \ncorresponding token, segment and position embeddings. Besides, generally, we use a special token [CLS] \nas its sentence vector representation.  \nTherefore, our proposed technique utilizes the LaBSE model as the base model to encode the \nlanguage features and use the first input token [CLS] to obtain the sentence vector representation. Hence, \nfor the i-th sentence, the sentence vector is expressed as follows. \n ğ‘†ğ‘–= ğ¿ğ‘ğµğ‘†ğ¸(ğ‘ğ‘–, ğ‘ğ‘–, ğ‘ğ‘–) \nWhere ğ‘ğ‘–, ğ‘ğ‘–, ğ‘ğ‘– are the token embeddings, the segmentation embeddings and the position embeddings \nrespectively. \n \n1 https://github.com/bojone/labse \n3.2 Language-specific words extraction \n \nIn multi-task learning, the imbalance of the amount of data among different languages leads to the \ngeneral representation layer focusing more on languages with more training data. To obtain general \nrepresentation, we adopt adversarial training with MLM to prevent the general representation module \nwith a base model called LaBSE from paying too much language-specific information.  \n \nAs we all know, for samples of different languages, there are certain tokens that contain a large \namount of language information. To avoid the general representation layer focusing too much on specific \nlanguage information, the first step is to identify words that contain rich language information, so we \nproposed to build a language recognition model based on the LaBSE model. Guided by supervised fitting \ntask representation, the model can learn and distinguish language information. When the model predicts \na sample, the output prediction probability can be regarded as the proportion of language information \ncontained in the sample estimated by the model. We further use this model and MLM strategy to quantify \nthe amount of language information of each word. \n \nSpecially, let ğ‘†= [ğ‘¥1, ğ‘¥2, ğ‘¥3, â€¦ , ğ‘¥ğ‘›]  denote the input sentence, and  ğ‘‚ğ‘¦(ğ‘†)  refers to the output \nprediction probability by the language recognition model LaBSE for correct label ğ‘¦ . The language \ninformation ğ¼ğ‘¤ğ‘– of word  ğ‘¤ğ‘– id defined as \nğ¼ğ‘¤ğ‘–= ğ‘‚ğ‘¦(ğ‘†) âˆ’ğ‘‚ğ‘¦(ğ‘†\\ğ‘¤ğ‘–), \nwhere  ğ‘†\\ğ‘¤ğ‘–= [ğ‘¤0, ğ‘¤1, â€¦ , [ğ‘€ğ´ğ‘†ğ¾], â€¦ , ğ‘¤ğ‘›] is the sentence after replacing ğ‘¤ğ‘– as [ğ‘€ğ´ğ‘†ğ¾]. \n \nLater, for each sentence, we rank all the words according to the ranking score ğ¼ğ‘¤ğ‘– in descending \norder and only take the score greater than 0 as the language-specific words to form a word list  ğ¼. \n \n3.3 Language information perturbation \n \nAdversarial training presented by Goodfellow et al. (2015) is a novel regularization method that \nimproves the robustness of misclassifying small perturbed inputs (Sato et al., 2018). Following great \nsuccess in the image processing field, Miyato et al. (2017) first proposed to apply this idea to natural \nlanguage processing (NLP) tasks. He pointed out that adding perturbation to the input word embedding \nspace improves the generalization performance of models for NLP tasks. Inspired by his research, in \norder to make the model have greater language generalization, we further increase the adversarial \nperturbations for the words embedding of those language-specific words selected in section 3.2. \n \nSpecially, let  ğ‘Ÿğ´ğ‘‘ğ‘£ğ‘‡\nğ‘¡\n  be adversarial perturbation vector for t-th word ğ‘¥ğ‘¡  in word embedding \nvectors [ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘¡] as ğ‘¥ and ğ‘¦ represent the label for each language. We assume that ğ‘Ÿğ´ğ‘‘ğ‘£ğ‘‡\nğ‘¡\n is a D-\ndimensional vector whose dimension always matches that of word embedding vector  ğ‘¤ğ‘¡.  ğ¿(ğ‘¥, ğ‘¦, ğœƒ) is \nthe loss function of individual training sample  (ğ‘¥, ğ‘¦)  in training dataset where ğœƒ  are the model \nparameters, then the adversarial perturbation ğ‘Ÿğ´ğ‘‘ğ‘£ğ‘‡\nğ‘¡\n is calculated as follow.  \nğ‘Ÿğ´ğ‘‘ğ‘£ğ‘‡\nğ‘¡\n= ğ›¼ğœ–ğ‘”ğ‘¡\nâ€–ğ‘”â€–2\n  \nğ‘”ğ‘¡= âˆ‡ğ‘¤ğ‘¡ ğ¿(ğ‘¥, ğ‘¦, ğœƒ) \n ğ¿(ğ‘¥, ğ‘¦, ğœƒ) = ğ‘™ğ‘œğ‘”ğ‘(ğ‘¦|ğ‘¥; ğœƒ)  \nWhere ğ‘” is a concatenated vector of ğ‘”ğ‘¡ for all ğ‘¡ and ğ›¼ is a weight threshold which represents the \ndegree of perturbation in language-specific words. In this paper, the weight value ğ›¼ of language-specific \nword is set to 1.5 and others are set to 1.0. The optimal value of this value will be proved in Section 5.3. \n \nFigure 2. Adversarial training. \n3.4 Language-specific representation \n \nThere is a certain correlation among different languages of the Dravidian language family. \nTherefore, in order to better recognize the interaction among different languages, we introduced language \ndescriptors based on the self-attention mechanism proposed Vaswani et al. (2017) to simulate the \ninteraction.  \nIn the adversarial training process, we use the disturbing gradient for backpropagation and \nparameter update, then remove the noise of the embedding layer, restore the original gradient, and \nperform the next epoch of iterative training. \n \nFormally, assuming that a language descriptor means one kind of language label and is represented \nas a vector ğ‘ğ‘–âˆˆâ„ğ‘š where m is equal to the general representation dimensionality. Consequently, all \nlanguage descriptors for all languages can compose a matrix ğ‘âˆˆğ‘…ğ‘›Ã—ğ‘š , and n is the number of \nlanguages, and each row is the descriptor for a certain language. Therefore, a language descriptor for a \ncertain language ğ‘– is obtained as follows. \nğ‘ğ‘–\nğ‘›ğ‘’ğ‘¤= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ğ‘–ğ‘ğ‘‡)ğ‘ \nWe first calculate the dot product between the original descriptor ğ‘ğ‘– and the other descriptors, \nwhich is then normalized by the ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ function. The output vector can represent the interactions \namong different languages. Next, the dot product of this output and ğ‘ is calculated to obtain the new \ndescriptor ğ‘ğ‘–\nğ‘›ğ‘’ğ‘¤ , which can be considered as the weighted sum of all language descriptors with \nregarding to the language ğ‘–. \n3.5 Multi-dimension information fusion \nIn order to fully learn the general semantic information and the correlation among languages, we \nfurther spliced the language descriptor with the sentence vector ğ‘†ğ‘– output by the general representation \nmodule to fuse multi-dimensional semantic information and map it to the labels dimensions \ncorresponding to each language by a fully connected layer. \nâ„ğ‘–= [ğ‘†ğ‘–; ğ‘ğ‘–\nğ‘›ğ‘’ğ‘¤] \n ğ‘ƒ= (ğ‘Šâ„ğ‘–+ ğ‘) \nHere ğ‘Š  and ğ‘  are parameters of the fully connected layer, â„ğ‘–  is the spliced vector of ğ‘†ğ‘–  and \nğ‘ğ‘–\nğ‘›ğ‘’ğ‘¤. \n4. Experiment \n4.1 Dataset \n \nFigure 3. Data distribution for task 1. \nIn order to ensure the effectiveness of the framework, it has been tested on two different tasks. Task \n1: Fire2021 message-level polarity classification task1 (Priyadharshini et al., 2021). This task gives a \ncode-mixed dataset of comments/posts in Tamil-English, Malayalam-English, and Kannada-English. \nBased on this dataset, the participators have to classify it into one of the five labels (positive, negative, \nneutral, mixed emotions, or not in the intended languages). The data distribution is shown in Figure 3. \nTask 2: EACL2021 Offensive language identification2 (Chakravarthi et al., 2021). This task is to identify \noffensive language content of the code-mixed dataset of comments/posts in Dravidian Languages, which \nis shown in Figure 4. \n \n1 https://competitions.codalab.org/competitions/30642 \n2 https://competitions.codalab.org/competitions/27654 \n \n \nFigure 4. Data distribution for task 2. \n4.2 Metric \nDifferent from macro-F1, the weighted-F1 calculates metrics for each label, and finds their average \nweighted by the number of true instances for each label, allowing it to take into account the importance \nof different categories and achieving better results in the evaluation for imbalance data. Both evaluations \nuse weighted-F1 as the evaluation index. In order to accurately compare with the evaluation teamsâ€™ \nmodels, we selected weighted-F1 index as the evaluation metric. \n \n4.3 Detail of experiments \nIn this paper, we compare our framework with the commonly used deep pre-trained model (XLM, \nXLM-RoBERTa, Muril, LaBSE and BERT). The relevant parameters are shown in Table 1. Besides, we \nuse pytorch and transformers framework to implement the models. \nTable 1. The parameter of the pre-trained model \nParameter \nvalues \nLearning rate \n5e-5 \nDropout \n0.5 \nWeight decay \n0.001 \nOptimization function \nAdam \nThe largest number of the sentence \n128 \nBatch size \n64 \n \n5.  Results and analysis \n5.1 Comparative experiments \nIn order to verify the effectiveness of our framework, we designed 7 sets of experiments in this paper, \nconsists of 3 sets of model comparison experiments, 3 sets of ablation experiments and 1 set of \nexploration experiment. As for the comparison experiments, to select a high-performance classification \nmodel, we explore the performance among different deep learning models, the performance among pre-\ntrained models and the performance between multilingual training and monolingual training. Besides, \nwe also compare the performance of our framework with the state-of-the-art model. In light of ablation \nexperiments, it is mainly to verify the effectiveness of each module in our framework. Moreover, we \nfurther explore the weight setting of Î± during language information perturbation module. \nComparative experiment 1: Performance comparison among different deep learning models. In \norder to select the base model, we compare the performance during pre-trained models such as XLM-\nRoBERTa-Base (Conneau et al., 2020), XLM (Conneau et al., 2020), LaBSE, Muril (Khanuja et al., 2021) \nand Multilingual-BERT (Devlin et al., 2019) on the FIRE 2021. The results are shown in Figure 5, it is \nobvious to see that the LaBSE model is relatively effective in each task in terms of average scores. \n \nFigure 5. The results of pre-trained models. \nComparative experiment 2: Performance comparison between the model trained with multilingual \ncorpus and one trained with monolingual corpus. After selecting the best base model, we further \ncompared the performance of the model trained with multilingual corpus and one trained with \nmonolingual corpus on the FIRE 2021 dataset and EACL 2021 dataset. As shown in Table 2, the model \ntrained with monolingual corpus outperforms that one trained with multilingual corpus. We explored the \nreason for this result is that the general representation module pays too much attention to the language-\nspecific information, leading to a decline in the generalization capability of the model. \nTable 2. The results of multilingual corpus training. \nTask \nLanguage \nMonolingual \nMultilingual \nFIRE 2021 \nMalayalam \n0.7503 \n0.7388 \nKannada \n0.6566 \n0.6494 \nTamil \n0.6957 \n0.7065 \nEACL 2021 \nMalayalam \n0.9677 \n0.9606 \nKannada \n0.7854 \n0.7813 \nTamil \n0.8120 \n0.8201 \nAverage \n0.7780 \n0.7761 \n \nComparative experiment 3: Performance comparison with the state-of-the-art methods. In this \npaper, the datasets we used are from FIRE 2021 and EACL 2021 evaluation competitions. Therefore, in \norder to verify the effectiveness of the proposed framework, we further compare the results of our \nframework with the top 10 teams in these two evaluation competitions (shown in Figure 6 and Figure 7). \nSince the task in our paper is a multilingual task, in Figure 5 and Figure 6 the ranking is based on the \naverage of each team in the three languages. The experimental results show that our framework achieved \nthe best results in most languages of the task (see the red line).  \n \n \n69.63%\n65.77% 65.53% 64.77% 64.13% 63.60% 63.27% 63.20%\n57.47% 57.40%\n71.70%\n50.00%\n55.00%\n60.00%\n65.00%\n70.00%\n75.00%CIA_NITTMUCICSOA-NLPIIITT-PawanAI MLSSNCSE_NLPIIITT-Karthik PuranikSSN_NLP_MLRGIIIT_DWDAmrita_CEN\nAverage weighted-F1\nof three languages\nThe names of each team.\nThe model in ACL (in order of ranking)\nOur model\nFigure 6. The results on FIRE 2021 dataset. \n \nFigure 7. The results on EACL 2021 dataset. \n \nWe also compared our model with the state-of-the-art language model of each language on the two \ndatasets. The results are shown in Table 3. In the Malayalam task and Tamil task of FIRE 2021 and the \nMalayalam task of EACL 2021, our model did not exceed the monolingual language state-of-the-art \nmodel. But from the average result, our model is better than the combination of three state-of-the-art \nmonolingual languages, which is an increase of 0.2% on FIRE 2021 and an increase of 2.84% on EACL \n2021. \nTable 3. Comparison of our model with the state-of-the-art monolingual language model. \nDataset \nModel \nMalayalam \nKannada \nTamil \nAvgeage \nFIRE \n2021 \nState-of-\nthe-art \n0.8040  \n(ZYBank-AI Team) \n0.6300 \n (SSNCSE_NLP) \n0.7110  \n(CIA_NITT) \n0.7150 \nOurs \n0.7599 \n0.6822 \n0.7083 \n0.7170 \nEACL \n2021 \nState-of-\nthe-art \n0.9700 (hate-alert)  \n(Saha et al., 2021) \n0.7500 (SJ-AJ) \n(Jayanthi and Gupta, 2021) \n0.7800 (hate-alert) \n(Saha et al., 2021) \n0.8333 \nOurs \n0.9637 \n0.7910 \n0.8314 \n0.8617 \n \n5.2 Ablation experiment \nAblation experiment 1: Effectiveness verification of adversarial training. In this paper, on the basis \nof selecting the LaBSE model as the base model to obtain sentence vectors, adversarial training was \nadded to the word embedding layer for joint training. We utilize the FGM algorithm to regularize the \nword embedding layer of the LaBSE model and perturb it to enhance the robustness of the model. On \n83.00%\n82.33% 82.00%\n81.00% 80.67% 80.67% 80.33% 80.33% 80.00% 79.67%\n86.17%\n76.00%\n78.00%\n80.00%\n82.00%\n84.00%\n86.00%\n88.00%hate-alertSJ-AJindicnlp@kgpALI-B2B-AIbitionsIIITKCodewithzichaoMUCSNLP@CUEThypers\nAverage of three languages\nThe names of each team.\nThe models in EACL (in order of ranking)\nOur model\nthe FIRE 2021 dataset, we tried to add adversarial training to the monolingual models and the \nmultilingual model. As the result shown in Table 4. It is obvious to see that adding perturbation to the \nword embedding layer of the model can improve the performance to a certain extent. Because adding \nperturbation to the model is equivalent to generating adversarial samples to the model, which enhances \nthe diversity of training samples. At the same time, the overall performance of the multilingual model \nthat only uses adversarial learning is not as good as training three monolingual models separately. \nTable 4. Comparison of adversarial training. \nModel \nMalayalam \nKannada \nTamil \nAverage \nMonolingual \n0.7503 \n0.6566 \n0.6957 \n0.7009 \nMonolingual + adversarial training \n0.7543 \n0.6771 \n0.7007 \n0.7107 \nMultilingual \n0.7388 \n0.6494 \n0.7065 \n0.6982 \nMultilingual + adversarial training \n0.7473 \n0.6545 \n0.7018 \n0.7012 \n \nAblation experiment 2: Effectiveness verification of language-specific representation module. In \norder to extract language-specific words, on the basis of adversarial training, we further proposed to train \na language recognition model and apply MLM strategy to quantify the amount of language information \nof each word. We verify the performance of this module on the FIRE 2021 dataset. As shown in Table 5, \nthe experimental results demonstrate that this module has a certain impact on classification performance \nwith the average value improving 1.13% in terms of comparing with the model without this module, \nwhich indicates that this module can make the general representation module more unbiased to increases \nthe generalization ability of the model. \nTable 5. Comparison of language-specific representation module. \nModel \nMalayalam \nKannada \nTamil \nAverage \nMultilingual \n0.7388 \n0.6494 \n0.7065 \n0.6982 \nMultilingual + adversarial training \n0.7473 \n0.6545 \n0.7018 \n0.7012 \nMultilingual + adversarial training + \nlanguage-specific words extraction \n0.7567 \n0.6849 \n0.6958 \n0.7125 \n \nAblation experiment 3: Effectiveness verification of language descriptor. Since all the corpus we \nused belongs to the Dravidian language family, there is a certain correlation among them. Therefore, in \norder to integrate language-related information into the model, we proposed to introduce language \ndescriptors for joint training during the language representation module. The results are shown in Table \n6. In terms of average score, the classification result increased by 0.45% compared to the experimental \nresults without this module, verifying the effectiveness and feasibility of this framework. \nTable 6. Comparison of language descriptor. \nModel \nMalayalam \nKannada \nTamil \nAverage \nMultilingual \n0.7388 \n0.6494 \n0.7065 \n0.6982 \nMultilingual + adversarial training \n0.7473 \n0.6545 \n0.7018 \n0.7012 \nMultilingual + adversarial training+ language-\nspecific words extraction \n0.7567 \n0.6849 \n0.6958 \n0.7125 \nMultilingual + adversarial training+ language-\nspecific words extraction + language descriptor \n0.7599 \n0.6822 \n0.7083 \n0.7170 \n \n5.3 Explore experiments \nEventually, the value of weight Î± is also verified in this paper. We explored the influence of this \nparameter on the multilingual model based on adversarial training. The results shown in Table 7 \ndemonstrate that when the value of Î± is 1.5, the model performs the best. This means that the perturbation \ndegree of language-specific words by the model is 1.5 times than the perturbation degree of other words. \nTable 7. The results of threshold. \nThreshold \nMal \nKannada \nTamil \nAverage \n1.1 \n0.7562 \n0.6655 \n0.7099 \n0.7105 \n1.2 \n0.7528 \n0.6668 \n0.6917 \n0.7038 \n1.3 \n0.7526 \n0.6685 \n0.6836 \n0.7016 \n1.4 \n0.7605 \n0.6635 \n0.6953 \n0.7064 \n1.5 \n0.7567 \n0.6849 \n0.6958 \n0.7125 \n \n6. Conclusion \nIn this paper, we proposed a multilingual text classification framework for the Dravidian languages. \nOn the one hand, the framework uses the LaBSE pre-trained model as the base model. Aiming at the \nproblem of text information bias in multi-task learning, we proposed to use the MLM strategy to select \nlanguage-specific words, and implemented adversarial training to perturb them. On the other hand, in \nview of the problem that the model cannot well recognize and utilize the correlation among languages, \nwe further proposed a language representation module to enrich semantic information for the model. In \nthe future, we will expand the number of languages and improve the performance of the multilingual text \nclassification framework. \nReference \nChakravarthi, B.R., Muralidaran, V., Priyadharshini, R., McCrae, J.P., 2020. Corpus Creation for \nSentiment Analysis in Code-Mixed Tamil-English Text. CoRR abs/2006.00206. \nChakravarthi, B.R., Priyadharshini, R., Jose, N., Kumar M, A., Mandl, T., Kumaresan, P.K., Ponnusamy, \nR., R L, H., McCrae, J.P., Sherly, E., 2021. Findings of the Shared Task on Offensive Language \nIdentification in Tamil, Malayalam, and Kannada, in: Proceedings of the First Workshop on Speech \nand Language Technologies for Dravidian Languages. Association for Computational Linguistics, \nKyiv, pp. 133â€“145. \nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., GuzmÃ¡n, F., Grave, E., Ott, M., \nZettlemoyer, L., Stoyanov, V., 2020. Unsupervised Cross-lingual Representation Learning at Scale. \nhttps://doi.org/10.18653/v1/2020.acl-main.747 \nDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of deep bidirectional \ntransformers for language understanding, in: NAACL HLT 2019 - 2019 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: Human Language \nTechnologies - Proceedings of the Conference. \nFeng, F., Yang, Y., Cer, D., Arivazhagan, N., Wang, W., 2020. Language-agnostic BERT Sentence \nEmbedding. CoRR abs/2007.0. \nGao, J., Lanchantin, J., Soffa, M. Lou, Qi, Y., 2018. Black-Box Generation of Adversarial Text Sequences \nto Evade Deep Learning Classifiers, in: 2018 IEEE Security and Privacy Workshops (SPW). pp. \n50â€“56. https://doi.org/10.1109/SPW.2018.00016 \nGoodfellow, I.J., Shlens, J., Szegedy, C., 2015. Explaining and Harnessing Adversarial Examples. \nGroenwold, S., Honnavalli, S., Ou, L., Parekh, A., Levy, S., Mirza, D., Wang, W.Y., 2020. Evaluating the \nRole of Language Typology in Transformer-Based Multilingual Text Classification. CoRR \nabs/2004.13939. \nHande, A., Puranik, K., Yasaswini, K., Priyadharshini, R., Thavareesan, S., Sampath, A., \nShanmugavadivel, K., Thenmozhi, D., Chakravarthi, B.R., 2021. Offensive Language Identification \nin Low-resourced Code-mixed Dravidian languages using Pseudo-labeling. \nJayanthi, S.M., Gupta, A., 2021. SJ_AJ@DravidianLangTech-EACL2021: Task-Adaptive Pre-Training \nof Multilingual BERT models for Offensive Language Identification, in: Proceedings of the First \nWorkshop on Speech and Language Technologies for Dravidian Languages. Association for \nComputational Linguistics, Kyiv, pp. 307â€“312. \nKazhuparambil, S., Kaushik, A., 2020. Classification of Malayalam-English Mix-Code Comments using \nCurrent State of Art, in: 2020 IEEE International Conference for Innovation in Technology \n(INOCON). pp. 1â€“6. https://doi.org/10.1109/INOCON50539.2020.9298382 \nKhanuja, S., Bansal, D., Mehtani, S., Khosla, S., Dey, A., Gopalan, B., Margam, D.K., Aggarwal, P., \nNagipogu, R.T., Dave, S., Gupta, S., Gali, S.C.B., Subramanian, V., Talukdar, P.P., 2021. MuRIL: \nMultilingual Representations for Indian Languages. CoRR abs/2103.10730. \nLiu, J., Cui, R., Zhao, Y., 2018. Multilingual Short Text Classification via Convolutional Neural Network, \nin: Meng, X., Li, R., Wang, K., Niu, B., Wang, X., Zhao, G. (Eds.), Web Information Systems and \nApplications. Springer International Publishing, Cham, pp. 27â€“38. \nMao, Y., Yun, S., Liu, W., Du, B., 2020. Tchebycheff Procedure for Multi-task Text Classification, in: \nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics. \nAssociation for Computational Linguistics, pp. 4217--4226. https://doi.org/10.18653/v1/2020.acl-\nmain.388 \nMeng, X., 2018. Research and Implementation of Multilingual Text Classification System Based on Deep \nLearning. Yanbian University. \nMeng, X., Cui, R., Zhao, Y., Fang, M., 2020. Multilingual text classification method based on bi-\ndirectional long short-term memory and convolutional neural network. Application Research of \nComputers 37, 2669â€“2673. \nMeng, X., Cui, R., Zhao, Y., Zhang, Z., 2019. Multilingual Short Text Classification Based on LDA and \nBiLSTM-CNN Neural Network, in: Ni, W., Wang, X., Song, W., Li, Y. (Eds.), Web Information \nSystems and Applications. Springer International Publishing, Cham, pp. 319â€“323. \nMishra, Sudhanshu, Prasad, S., Mishra, Shubhanshu, 2020. Multilingual Joint Fine-tuning of \nTransformer models for identifying Trolling, Aggression and Cyberbullying at TRAC 2020, in: \nProceedings of the Second Workshop on Trolling, Aggression and Cyberbullying. European \nLanguage Resources Association (ELRA), Marseille, France, pp. 120â€“125. \nMiyato, T., Dai, A.M., Goodfellow, I., 2021. Adversarial Training Methods for Semi-Supervised Text \nClassification. \nPriyadharshini, R., Chakravarthi, B.R., Thavareesan, S., Chinnappa, D., Thenmozhi, D., Ponnusamy, R., \n2021. Overview of the DravidianCodeMix 2021 Shared Task on Sentiment Detection in Tamil, \nMalayalam, and Kannada, in: Forum for Information Retrieval Evaluation, FIRE 2021. Association \nfor Computing Machinery. \nSachin Kumar, S., Anand Kumar, M., Soman, K.P., 2019. Identifying Sentiment of Malayalam Tweets \nUsing Deep Learning, in: Patnaik, S., Yang, X.-S., Tavana, M., Popentiu-Vl\\uadicescu, F., Qiao, F. \n(Eds.), Digital Business: Business Algorithms, Cloud Computing and Data Engineering. Springer \nInternational Publishing, Cham, pp. 391â€“408. https://doi.org/10.1007/978-3-319-93940-7_16 \nSaha, D., Paharia, N., Chakraborty, D., Saha, P., Mukherjee, A., 2021. Hate-Alert@DravidianLangTech-\nEACL2021: Ensembling strategies for Transformer-based Offensive language Detection, in: \nProceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages. \nAssociation for Computational Linguistics, Kyiv, pp. 270â€“276. \nSai, S., Sharma, Y., 2020. Siva@HASOC-Dravidian-CodeMix-FIRE-2020: Multilingual offensive \nspeech detection in code-mixed and romanized text, in: CEUR Workshop Proceedings. \nSato, M., Suzuki, J., Shindo, H., Matsumoto, Y., 2018. Interpretable Adversarial Perturbation in Input \nEmbedding Space for Text, in: Proceedings of the 27th International Joint Conference on Artificial \nIntelligence, IJCAIâ€™18. AAAI Press, pp. 4323â€“4330. \nSun, R., Zhou, X., 2020. SRJ @ Dravidian-CodeMix-FIRE2020: Automatic classification and \nidentification sentiment in code-mixed text, in: CEUR Workshop Proceedings. \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., Polosukhin, I., \n2017. Attention is all you need, in: Advances in Neural Information Processing Systems. \nVeena, P. V., Ramanan, P., Remmiya Devi, G., 2020. CENMates@HASOC-Dravidian-CodeMix-\nFIRE2020: Offensive language identification on code-mixed social media comments, in: CEUR \nWorkshop Proceedings. \nZhao, W., Gao, H., Chen, S., Wang, N., 2020. Generative Multi-Task Learning for Text Classification. \nIEEE Access 8, 86380â€“86387. https://doi.org/10.1109/ACCESS.2020.2991337 \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2021-12-03",
  "updated": "2021-12-03"
}