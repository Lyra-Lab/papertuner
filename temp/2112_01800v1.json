{
  "id": "http://arxiv.org/abs/2112.01800v1",
  "title": "A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled Samples",
  "authors": [
    "Sen Jia",
    "Shuguo Jiang",
    "Zhijie Lin",
    "Nanying Li",
    "Meng Xu",
    "Shiqi Yu"
  ],
  "abstract": "With the rapid development of deep learning technology and improvement in\ncomputing capability, deep learning has been widely used in the field of\nhyperspectral image (HSI) classification. In general, deep learning models\noften contain many trainable parameters and require a massive number of labeled\nsamples to achieve optimal performance. However, in regard to HSI\nclassification, a large number of labeled samples is generally difficult to\nacquire due to the difficulty and time-consuming nature of manual labeling.\nTherefore, many research works focus on building a deep learning model for HSI\nclassification with few labeled samples. In this article, we concentrate on\nthis topic and provide a systematic review of the relevant literature.\nSpecifically, the contributions of this paper are twofold. First, the research\nprogress of related methods is categorized according to the learning paradigm,\nincluding transfer learning, active learning and few-shot learning. Second, a\nnumber of experiments with various state-of-the-art approaches has been carried\nout, and the results are summarized to reveal the potential research\ndirections. More importantly, it is notable that although there is a vast gap\nbetween deep learning models (that usually need sufficient labeled samples) and\nthe HSI scenario with few labeled samples, the issues of small-sample sets can\nbe well characterized by fusion of deep learning methods and related\ntechniques, such as transfer learning and a lightweight model. For\nreproducibility, the source codes of the methods assessed in the paper can be\nfound at https://github.com/ShuGuoJ/HSI-Classification.git.",
  "text": "A Survey: Deep Learning for Hyperspectral Image\nClassiﬁcation with Few Labeled Samples\nSen Jiaa,b, Shuguo Jianga, Zhijie Lina, Nanying Lia, Meng Xua,b, Shiqi Yuc,∗\naCollege of Computer Science and Software Engineering, Shenzhen University, China\nbSZU Branch, Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society, China\ncDepartment of Computer Science and Engineering, Southern University of Science and\nTechnology, China\nAbstract\nWith the rapid development of deep learning technology and improvement in\ncomputing capability, deep learning has been widely used in the ﬁeld of hyper-\nspectral image (HSI) classiﬁcation. In general, deep learning models often con-\ntain many trainable parameters and require a massive number of labeled samples\nto achieve optimal performance. However, in regard to HSI classiﬁcation, a large\nnumber of labeled samples is generally diﬃcult to acquire due to the diﬃculty\nand time-consuming nature of manual labeling. Therefore, many research works\nfocus on building a deep learning model for HSI classiﬁcation with few labeled\nsamples. In this article, we concentrate on this topic and provide a systematic\nreview of the relevant literature. Speciﬁcally, the contributions of this paper are\ntwofold. First, the research progress of related methods is categorized accord-\ning to the learning paradigm, including transfer learning, active learning and\nfew-shot learning. Second, a number of experiments with various state-of-the-\nart approaches has been carried out, and the results are summarized to reveal\nthe potential research directions. More importantly, it is notable that although\nthere is a vast gap between deep learning models (that usually need suﬃcient\nlabeled samples) and the HSI scenario with few labeled samples, the issues of\nsmall-sample sets can be well characterized by fusion of deep learning methods\nand related techniques, such as transfer learning and a lightweight model. For\nreproducibility, the source codes of the methods assessed in the paper can be\nfound at https://github.com/ShuGuoJ/HSI-Classification.git.\nKeywords:\nhyperspectral image classiﬁcation, deep learning, transfer learning,\n⋆The work is supported by the National Natural Science Foundation of China (Grant No.\n41971300, 61901278 and 61976144), the National Key Research and Development Program\nof China (Grant No. 2020AAA0140002), the Program for Young Changjiang Scholars, the\nKey Project of Department of Education of Guangdong Province (Grant No. 2020ZDZX3045)\nand the Shenzhen Scientiﬁc Research and Development Funding Program under (Grant No.\nJCYJ20180305124802421 and JCYJ20180305125902403).\n∗Corresponding author\nEmail address: yusq@sustech.edu.cn (Shiqi Yu)\nPreprint submitted to Neurocomputing\nDecember 6, 2021\narXiv:2112.01800v1  [cs.CV]  3 Dec 2021\nfew-shot learning\n1. Introduction\nHyperspectral remote sensing technology is a method that organically com-\nbines the spectrum of ground objects determined by their unique material com-\nposition with the spatial image reﬂecting the shape, texture and layout of ground\nobjects, to realize the accurate detection, recognition and attribute analysis of\nground objects. The resultant hyperspectral images (HSIs) not only contain\nabundant spectral information reﬂecting the unique physical properties of the\nground features but also provide rich spatial information of the ground fea-\ntures. Therefore, HSIs can be utilized to solve problems that cannot be solved\nwell in multispectral or natural images, such as the precise identiﬁcation of each\npixel. Since diﬀerent materials exhibit speciﬁc spectral characteristics, the clas-\nsiﬁcation performance of HSI can be more accurate. Due to these advantages,\nhyperspectral remote sensing has been widely used in many applications, such\nas precision agriculture [1], crop monitoring [2], and land resources [3, 4]. In\nenvironmental protection, HSI has been employed to detect gas [5], oil spills [6],\nwater quality [7, 8] and vegetation coverage [9, 10], to better protect our living\nenvironment.\nIn the medical ﬁeld, HSI has been utilized for skin testing to\nexamine the health of human skin [11].\nAs a general pattern recognition problem, HSI classiﬁcation has received a\nsubstantial amount of attention, and a large number of research results have\nbeen achieved in the past several decades. According to the previous work [12],\nall researches can be divided into the spectral-feature method, spatial-feature\nmethod, and spectral-spatial-feature method. The spectral feature is the prim-\nitive characteristic of the hyperspectral image, which is also called the spectral\nvector or spectral curve. And the spatial feature [13] means the relationship be-\ntween the central pixel and its context, which can greatly increase the robustness\nof the model. In the early period of the study on HSI classiﬁcation, researchers\nmainly focused on the pure spectral feature-based methods, which simply apply\nclassiﬁers to pixel vectors, such as support vector machines (SVM) [14], neural\nnetworks [15], logistic regression [16], to obtain classiﬁcation results without\nany feature extraction. But raw spectra contain much redundant information\nand the relation between spectra and ground objects is non-linear, which en-\nlarges the diﬃculty of the model classiﬁcation. Therefore, most later methods\ngive more attention to dimension reduction and feature extraction to learn the\nmore discriminative feature. For the approaches based on dimension reduction,\nprinciple component analysis [17], independent component analysis [18], linear\ndiscriminant analysis [19], and low-rank [20] are widely used. Nevertheless, the\nperformance of those models is still unsatisfactory. Because, there is a common\nphenomenon in the hyperspectral image which is that diﬀerent surface objects\nmay have the same spectral characteristic and, otherwise, the same surface ob-\njects may have diﬀerent spectral characteristics. The variability of spectra of\n2\nground objects is caused by illumination, environmental, atmospheric, and tem-\nporal conditions. Those enlarge the probability of misclassiﬁcation. Thus, those\nmethods are only based on spectral information, and ignore spatial information,\nresulting in unsatisfactory classiﬁcation performance. The spatial characteristic\nof ground objects supply abundant information of shape, context, and layout\nabout ground objects, and neighboring pixels belong to the same class with high\nprobability, which is useful for improving classiﬁcation accuracy and robustness\nof methods.\nThen, a large number of feature extraction methods that inte-\ngrate the spatial structural and texture information with the spectral features\nhave been developed, including morphological [21, 22, 23], ﬁltering [24, 25], cod-\ning [26], etc. Since deep learning-based methods are mainly concerned in this\npaper, the readers are referred to [27] for more details on these conventional\ntechniques.\nIn the past decade, deep learning technology has developed rapidly and\nreceived widespread attention.\nCompared with traditional machine learning\nmodel, deep learning technology does not need to artiﬁcially design feature\npatterns and can automatically learn patterns from data.\nTherefore, it has\nbeen successfully applied in the ﬁelds of natural language processing, speech\nrecognition, semantic segmentation, autonomous driving, and object detection,\nand gained excellent performance. Recently, it also has been introduced into\nthe ﬁeld of HSI classiﬁcation.\nResearchers have proposed a number of new\ndeep learning-based HIS classiﬁcation approaches, as shown in the left part of\nFigure 2. Currently, all methods, based on the joint spectral-spatial feature,\ncan be divided into two categories—Two-Stream and Single-Stream, according\nto whether they simultaneously extract the joint spectral-spatial feature. The\narchitecture of two-stream usually includes two branches—spectral branch and\nspatial branch. The former is to extract the spectral feature of the pixel, and\nthe latter is to capture the spatial relation of the central pixel with its neighbor\npixels. And the existing methods have covered all deep learning modules, such\nas fully connected layer, convolutional layer, and recurrent unit.\nIn the general deep learning framework, a large number of training samples\nshould be provided to well train the model and tune the numerous parameters.\nHowever, in practice, manually labeling is often very time-consuming and ex-\npensive due to the need for expert knowledge, and thus, a suﬃcient training\nset is often unavailable. As shown in Figure 1 (here the widely used Kennedy\nSpace Center (KSC) hyperspectral image is utilized for illustration), the left\nﬁgure randomly selects 10 samples per class and contains 130 labeled samples\nin total, which is very scattered and can hardly be seen. Alternatively, the right\nﬁgure in Figure 1 displays 50% of labeled samples, which is more suitable for\ndeep learning-based methods. Hence, there is a vast gap between the training\nsamples required by deep learning models and the labeled samples that can be\ncollected in practice. And there are many learning paradigms proposed for solv-\ning the problem of few label samples, as shown in the right part of Figure 2. In\nsection 2, we will discuss them in detail. And they can be integrated with any\nmodel architecture. Some pioneering works such as [28] started the topic by\ntraining a deep model with good generalization only using few labeled samples.\n3\nHowever, there are still many challenges for this topic.\ngap\nFigure 1: Illustration of the massive gap between practical situations (i.e., few labeled samples)\nand a large number of labeled samples of deep learning-based methods. Here, the widely used\nKennedy Space Center (KSC) hyperspectral image is employed, which contains 13 land covers\nand 5211 labeled samples (detailed information can be found in the experimental section).\nGenerally, suﬃcient samples are required to well train a deep learning model (as illustrated\nin the right ﬁgure), which is hard to be achieved in practice due to the diﬃculty of manually\nlabeling (as shown in the left ﬁgure).\nIn this paper, we hope to provide a comprehensive review of the state-of-the-\nart deep learning-based methods for HSI classiﬁcation with few labeled samples.\nFirst, instead of separating the various methods according to feature fusion\nmanner, such as spectral-based, spatial-based, and joint spectral-spatial-based\nmethods, the research progress of methods related to few training samples is cat-\negorized according to the learning paradigm, including transfer learning, active\nlearning, and few-shot learning. Second, a number of experiments with various\nstate-of-the-art approaches have been carried out, and the results are summa-\nrized to reveal the potential research directions. Further, it should be noted that\ndiﬀerent from the previous review papers [12, 29], this paper mainly focuses on\nthe few labeled sample issue, which is considered as the most challenging prob-\nlem in the HSI classiﬁcation scenario. For reproducibility, the source codes of\nthe methods conducted in the paper can be found at the web site for the paper1.\nThe remainder of this paper is organized as follows. Section 2 introduces the\ndeep models that are popular in recent years. In Section 3, we divide the previ-\nous works into four mainstream learning paradigms, including transfer learning,\nactive learning, and few-shot learning. In Section 4, we performed many ex-\nperiments, and a number of representative deep learning-based classiﬁcation\nmethods are compared on several real hyperspectral image data sets. Finally,\nconclusions and suggestions are provided in Section 5.\n1https://github.com/ShuGuoJ/HSI-Classification.git\n4\nTwo-Stream\nSingle-Stream\nSpectral Branch\nSpatial Branch\nFully Connected \nNetwork\n1D Convolution\nRNN\n2D Convolution\n3D Convolution\nSupervised \nLearning\nUnsupervised \nLearning\nFew-shot \nLearning\nTransfer \nLearning\nActive Learning\nAuto-Encoder\nModel\nFrame\nLearning \nParadigm\nPrototype \nNetwork\nRelation Network\nSiamese Network\nFine-tuning\nDomain Adaption\nBreaking Tie\nStandard Auto-\nencoder\nSparse Auto-\nencoder\n3D Auto-encoder\nFigure 2: The category of deep learning-based methods for hyperspectral image classiﬁcation.\nThe left is from the model architecture point of view, while the right is from the learning\nparadigm point of view. It is worth noting that the both kinds of methods can be combined\narbitrarily.\n2. Deep learning models for HSI classiﬁcation\nIn this section, three classical deep learning models, including the autoen-\ncoder, convolutional neural network (CNN), and recurrent neural network (RNN),\nfor HSI classiﬁcation are respectively described, and the relevant references are\nreviewed.\n2.1. Autoencoder for HSI classiﬁcation\nAn autoencoder [30] is a classic neural network, which consists of two parts:\nan encoder and a decoder. The encoder pencoder(h|x) maps the input x as a\nhidden representation h, and then, the decoder pdecoder(ˆx|h) reconstructs ˆx\nfrom h. It aims to make the input and output as similar as possible. The loss\nfunction can be formulated as follows:\nL(x, ˆx) = min |x −ˆx|\n(1)\nwhere L is the similarity measure. If the dimension of h is smaller than x, the\nautoencoder procedure is undercomplete and can be used to reduce the data\ndimension. Evidently, if there is not any constraint on h, the autoencoder is the\nsimplest identical function. In other words, the network does not learn anything.\nTo avoid such a situation, the usual way is to add the normalization term Ω(h)\nto the loss.\nIn [31?\n], the normalization of the autoencoder, referred as a\nsparse autoencoder, is Ω(h) = λ P\ni hi, which will make most of the parameters\nof the network very close to zero.\nTherefore, it is equipped with a certain\ndegree of noise immunity and can produce the sparsest representation of the\ninput. Another way to avoid the identical mapping is by adding some noise\ninto x to make the damaged input xnoise and then forcing the decoder to\nreconstruct the x. In this situation, it becomes the denoising autoencoder [32],\nwhich can remove the additional noise from xnoise and produce a powerful\nhidden representation of the input. In general, the autoencoder plays the role\nof feature extractor [33] to learn the internal pattern of data without labeled\nsamples. Figure 3 illustrates the basic architecture of the autoencoder model.\n5\nencoder\ndecoder\nX\nX\nTraining\nInference\none-hot \nvector\nclassifier\n··· ···\nFigure 3: The architecture of the autoencoder. The solid line represents training, while the\ndashed line represents inference.\nTherefore, Chen et al. [34] used an autoencoder for the ﬁrst time for feature\nextraction and classiﬁcation of HSIs. First, in the pretraining stage, the spectral\nvector of each pixel directly inputs the encoder module, and then, the decoder\nis used to reconstruct it so that the encoder has the ability to extract spectral\nfeatures. Alternatively, to obtain the spatial features, principal component anal-\nysis (PCA) is utilized to reduce the dimensionality of the hyperspectral image,\nand then, the image patch is ﬂattened into a vector. Another autoencoder is\nemployed to learn the spatial features. Finally, the spatial-spectral joint infor-\nmation obtained above is fused and classiﬁed. Subsequently, a large number of\nhyperspectral image classiﬁcation methods [35, 36] based on autoencoders ap-\npeared. Most of these methods adopt the same training strategy as [34], which\nis divided into two modules: fully training the encoder in an unsupervised man-\nner and ﬁne-tuning the classiﬁer in a supervised manner. Each of these methods\nattempts diﬀerent types of encoders or preprocessing methods to adapt to HSI\nclassiﬁcation under the condition of small samples. For example, Xing et al. [36]\nstack multiple denoising autoencoders to form a feature extractor, which has a\nstronger anti-noise ability to extract more robust representations. Given that\nthe same ground objects may have diﬀerent spectra while diﬀerent ground ob-\njects may exhibit similar spectra, spectral-based classiﬁcation methods often fail\nto achieve satisfactory performance, and spatial structural information of ob-\njects provides an eﬀective supplement. To gain a better spatial description of an\nobject, some autoencoder models combined with convolutional neural networks\n(CNNs) have been developed [37, 38]. Concretely, the autoencoder module is\nable to extract spectral features on large unlabeled samples, while the CNN\nis proven to be able to extract spatial features well. After fusion, the spatial-\nspectral features can be achieved. Further, to reduce the number of trainable\n6\nparameters, some researchers use the lightweight models, such as SVMs [39, 40],\nrandom forests [41, 42] or logistic regression [34, 43], to serve as the classiﬁer.\nDue to the three-dimensional (3D) pattern of hyperspectral images, it is\ndesirable to simultaneously investigate the spectral and spatial information such\nthat the joint spatial-spectral correlation can be better examined. Some three-\ndimensional operators and methods have been proposed. In the preprocessing\nstage, Li et al. [44] utilized the 3D Gabor operator to fuse spatial information\nand spectral information to obtain spatial-spectral joint features, which were\nthen fed into the autoencoder to obtain more abstract features. Mei et al. [40]\nused a 3D convolutional operator to construct an autoencoder to extract spatial-\nspectral features directly. In addition, image segmentation has been introduced\nto characterize the region structure of objects to avoid misclassiﬁcation of pixels\nat the boundary [45]. Therefore, Liu et al. [46] utilized superpixel segmentation\ntechnology as a postprocessing method to perform boundary regularization on\nthe classiﬁcation map.\n2.2. Convolutional Neural Networks (CNNs) for HSI classiﬁcation\nIn theory, the CNN uses a group of parameters that refer to a kernel function\nor kernel to scan the image and produce a speciﬁed feature. It has three main\ncharacteristics that make it very powerful for feature representation, and thus,\nthe CNN has been successfully applied in many research ﬁelds. The ﬁrst one is\nthe local connection that greatly decreases the number of trainable parameters\nand makes itself suitable for processing large images. This is the most obvious\ndiﬀerence from the fully connected network, which has a full connection between\ntwo neighboring neural layers and is unfriendly for large spatial images. To fur-\nther reduce the number of parameters, the same convolutional kernel shares the\nsame parameters, which is the second characteristic of CNNs. In contrast, in the\ntraditional neural network, the parameters of the output are independent from\neach other. However, the CNN applies the same parameters for all of the output\nto cut back the number of parameters, leading to the third characteristic: shift\ninvariance. It means that even if the feature of an object has shifted from one\nposition to another, the CNN model still has the capacity to capture it regard-\nless of where it appears. Speciﬁcally, a common convolutional layer consists of\nthree traditional components: linear mapping, the activation function and the\npooling function. Similar to other modern neural network architectures, activa-\ntion functions are used to bring a nonlinear mapping feature into the network.\nGenerally, the rectiﬁed linear unit (ReLU) is the prior choice. Pooling makes\nuse of the statistical characteristic of the local region to represent the output\nof a speciﬁed position. Taking the max pooling step as an example, it employs\nthe max value to replace the region of input. Clearly, the pooling operation is\nrobust to small changes and noise interfere, which could be smoothed out by\nthe pooling operation in the output, and thus, more abstract features can be\nreserved.\nIn the early works of applying CNNs for HSI classiﬁcation, two-dimensional\nconvolution was the most widely used method, which is mainly employed to ex-\ntract spatial texture information [47, 28, 48], but the redundant bands greatly\n7\nenlarge the size of the convolutional kernel, especially the channel dimension-\nality. Later, a combination of one-dimensional convolution and two-dimensional\nconvolution appeared [49] to solve the above problem. Concretely, one-dimensional\nand two-dimensional convolutions are responsible for extracting spectral and\nspatial features, respectively. The two types of features are then fused before\nbeing input to the classiﬁer. For the small training sample problem, due to\ninsuﬃcient labeled samples, it is diﬃcult for CNNs to learn eﬀective features.\nFor this reason, some researchers usually introduced traditional machine learn-\ning methods, such as attribute proﬁles [50], GLCM [51], hash learning [52], and\nMarkov Random ﬁelds [53], to introduce prior information to the convolutional\nnetwork and improve the performance of the network. Similar to the trend of\nautoencoder-based classiﬁcation methods, three-dimensional CNN models have\nalso been applied to HSI classiﬁcation in recent years and have shown better\nfeature fusion capabilities [54, 55]. However, due to the large number of pa-\nrameters, three-dimensional convolution is not suitable for solving small-sample\nclassiﬁcation problems under supervised learning.\nTo reduce the number of\nparameters of 3D convolution, Fang et al. [56] designed a 3D separable convolu-\ntion. In contrast, Mou et al. [57, 58] introduced an autoencoder scheme into the\nthree-dimensional convolution module to solve this problem. By a combination\nwith the classic autoencoder training method, the three-dimensional convolu-\ntion autoencoder can be trained in an unsupervised learning manner, and then,\nthe decoder is replaced with a classiﬁer, while the parameters of the encoder are\nfrozen. Finally, a small classiﬁer is trained by supervised learning. Moreover,\ndue to the success of ResNet [59], scholars have studied the HSI classiﬁcation\nproblem based on convolutional residuals [57, 58, 60, 61]. These methods try to\nuse jump connections to enable the network to learn complex features with a\nsmall number of labeled samples. Similarly, CNNs with dense connections have\nalso been introduced into this ﬁeld [62, 63]. In addition, the attention mech-\nanism is another hotpot for fully mining sample features.\nConcretely, Haut\nand Xiong et al. [64, 65] incorporated the attention mechanism with CNNs for\nHSI classiﬁcation. Although the above models can work well on HSI, they can-\nnot overcome the disadvantage of the low spatial resolution of HSIs, which may\ncause mixed pixels. To make up for this shortcoming, multimodality CNN mod-\nels have been proposed. These methods [66, 67, 68] combine HSIs and LiDAR\ndata together to increase the discriminability of sample features. Moreover, to\nachieve good performance under the small-sample scenario, Yu et al. [28] en-\nlarged the training set through data augmentation by implementing rotation\nand ﬂipping. On the one hand, this method increases the number of samples\nand improves their diversity. On the other hand, it enhances the model’s ability\nof rotation invariance, which is important in some ﬁelds such as remote sensing.\nSubsequently, Li et al. [69, 70] designed a data augmentation scheme for HSI\nclassiﬁcation. They combined the samples in pairs so that the model no longer\nlearns the characteristics of the samples themselves but learns the diﬀerences\nbetween the samples. Diﬀerent combinations make the scale of the training set\nlarger, which is more conducive for model training.\n8\n2.3. Recurrent neural network (RNN) for HSI classiﬁcation\nCompared with other forms of neural networks, recurrent neural networks\n(RNNs) [71] have memory capabilities and can record the context information\nof sequential data. Because of this memory characteristic, recurrent neural net-\nworks are widely used in tasks such as speech recognition and machine trans-\nlation.\nMore precisely, the input of a recurrent neural network is usually a\nsequence of vectors. At each time step t, the network receives an element xt\nin a sequence and the state ht−1 of the previous time step, and produces an\noutput yt and a state ht representing the context information at the current\nmoment. This process can be formulated as:\nht = f(Whhht−1 + Wxhxt + b)\n(2)\nwhere Wxh represents the weight matrix from the input layer to the hidden\nlayer, Whh denotes the state transition weight in the hidden layer, and b is the\nbias. It can be seen that the current state of the recurrent neural network is\ncontrolled by both the state of the previous time step and the current input.\nThis mechanism allows the recurrent neural network to capture the contextual\nsemantic information implicitly between the input vectors. For example, in the\nmachine translation task, it can enable the network to understand the semantic\nrelationship between words in a sentence.\nHowever, the classic RNN is prone to encounter gradient explosion or gradi-\nent vanishing problems during the training process. When there are too many\ninputs, the derivation chain of the RNN will become too long, making the gradi-\nent value close to inﬁnity or zero. Therefore, the classic RNN model is replaced\nby a long short-term memory (LSTM) network [71] or a gated recurrent unit\n(GRU) [72] in the HSI classiﬁcation task.\nBoth LSTM and GRU use gating technology to ﬁlter the input and the\nprevious state so that the network can forget unnecessary information and retain\nthe most valuable context. LSTM maintains an internal memory state, and\nthere are three gates: input gate it, forget gate ft and output gate ot, which\nare formulated as:\nit = σ(Wi · [xt, ht−1])\n(3)\nft = σ(Wf · [xt, ht−1])\n(4)\not = σ(Wio · [xt, ht−1])\n(5)\nIt can be found that the three gates are generated based on the current input\nand the previous state. First, the current input and the previous state will be\nspliced and mapped to a new input gt according to the following formula:\ngt = tanh(Wg · [xt, ht−1])\n(6)\nSubsequently, the input gate, the forget gate, the new input gt and the\n9\ninternal memory unit ˆht−1 update the internal memory state tegother. In this\nprocess, LSTM discards invalid information and adds new semantic information.\nˆht = ft ⊙ˆht−1 + it ⊙gt\n(7)\nFinally, the new internal memory state is ﬁltered by the output gate to form\nthe output of the current time step\nht = ot ⊙tanh(ˆht)\n(8)\nConcerning HSI processing, each spectral image is a high-dimensional vector\nand can be regarded as a sequence of data. There are many works using LSTM\nfor HSI classiﬁcation tasks. For instance, Mou et al. [73] proposed an LSTM-\nbased HSI classiﬁcation method for the ﬁrst time, and their work only focused\non spectral information. For each sample pixel vector, each band is input into\nthe LSTM step by step. To improve the performance of the model, spatial in-\nformation is considered in subsequent research. For example, Liu et al. fully\nconsidered the spatial neighborhood of the sample and used a multilayer LSTM\nto extract spatial spectrum features [74]. Speciﬁcally, in each time step, the\nsampling points of the neighborhood are sequentially input into the network to\ndeeply mine the context information in the spatial neighborhood. In [75], Zhou\net al. used two LSTMs to extract spectral features and spatial features. In\nparticular, for the extraction of spatial features, PCA is ﬁrst used to extract\nprincipal components from the sample rectangular space neighborhood. Then,\nthe ﬁrst principal component is divided into several lines to form a set of se-\nquence data, and gradually input into the network. In contrast, Ma and Zhang\net al. [76, 77] measures the similarity between the sample point in the spatial\nneighborhood and the center point. The sample points in the neighborhood will\nbe reordered according to the similarity and then input into the network step by\nstep. This approach allows the network to focus on learning sample points that\nare highly similar to the center point, and the memory of the internal hidden\nstate can thus be enhanced. Erting Pan et al. [78] proposed an eﬀective tiny\nmodel for spectral-spatial classiﬁcation on HSIs based on a single gate recurrent\nunit (GRU). In this work, the rectangular space neighborhood is ﬂattened into\na vector, which is used to initialize the hidden vector h0 of GRU, and the center\npoint pixel vector is input into the network to learn features.\nIn addition, Wu and Saurabh argue that it is diﬃcult to dig out the internal\nfeatures of the sample by directly inputting a single original spectral vector\ninto the RNN [79, 80]. The authors use a one-dimensional convolution operator\nto extract multiple feature vectors from the spectrum vector, which form a\nfeature sequence and are then input to the RNN. Finally, the fully connected\nlayer and the softmax function are adopted to obtain the classiﬁcation result.\nIt can be seen that only using recurrent neural networks or one-dimensional\nconvolution to extract the spatial-spectrum joint features is actually not eﬃcient\nbecause this will cause the loss of spatial structure information. Therefore, some\nresearchers combine two-dimensional/three-dimensional CNNs with an RNN\n10\nand use convolution operators to extract spatial-spectral joint features.\nFor\nexample, Hao et al. [81] utilized U-Net to extract features and input them into\nan LSTM or GRU so that the contextual information between features could\nbe explored. Moreover, Shi et al. [82] introduced the concept of the directional\nsequence to fully extract the spatial structure information of HSIs. First, the\nrectangular area of the sampling point is divided into nine overlapping patches.\nSecond, the patch will be mapped to a set of feature vectors through a three-\ndimensional convolutional network, and the relative position of the patch can\ngenerate 8 combinations of directions (for example, top, middle, bottom, left,\ncenter, and right) to form a direction sequence. Finally, the sequence is input\ninto the LSTM or GRU to obtain the classiﬁcation result. In this way, the spatial\ndistribution and structural characteristics of the features can be explored.\n3. Deep learning paradigms for HSI classiﬁcation with few labeled\nsamples\nAlthough diﬀerent HSI classiﬁcation methods have diﬀerent speciﬁc designs,\nthey all follow some learning paradigms. In this section, we mainly introduce\nseveral learning paradigms that are applied to HSI classiﬁcation with few la-\nbeled training samples. These learning paradigms are based on speciﬁc learning\ntheories. We hope to provide a general guide for researchers to design algo-\nrithms.\n3.1. Deep Transfer Learning for HSI classiﬁcation\nTransfer learning [83] is an eﬀective method to deal with the small-sample\nproblem. Transfer learning tries to transfer knowledge learned from one domain\nto another. First, there are two data sets/domains, one is called a source domain\nthat contains abundant labeled samples, and the other is called a target domain\nand only contains few labeled samples. To facilitate the subsequent description,\nwe deﬁne the source domain as Ds, the target domain as Dt, and their label\nspaces as Ys and Yt, respectively. Usually, the data distribution of the source\ndomain and the target domain are inconsistent: P(Xs) ̸= P(Xt). Therefore,\nthe purpose of transfer learning is to use the knowledge learned from Ds to\nidentify the labels of samples in Dt.\nFine-tuning is a general method in transfer learning that uses Ds to train\nthe model and adjust it by Dt. Its original motivation is to reduce the number\nof samples needed during the training process.\nSince deep learning models\ngenerally contain a vast number of parameters and if it is trained on the target\ndomain Dt, it is easy to overﬁt and perform poorly in practice. However, ﬁne-\ntuning allows the model parameters to reach a suboptimal state, and a small\nnumber of training samples of the target domain can tune the model to reach\nthe optimal state. It involves two steps. First, the speciﬁc model will be fully\ntrained on the source domain Ds with abundant labeled samples to make the\nmodel parameters arrive at a good state. Then, the model is transferred to the\ntarget domain Dt, except for some task-related modules, and slightly tuned on\nDt so that the model ﬁts the data distribution of the target domain Dt.\n11\nfω\nSource domain\nTarget domain\none-hot \nvector\nFigure 4: Flowchart of the ﬁne-tuning method. The solid line represents pretraining, and the\ndashed line represents ﬁne-tuning. fω is a learning function.\nBecause the ﬁne-tuning method is relatively simple, it is widely used in the\ntransfer learning method for hyperspectral image classiﬁcation. To our knowl-\nedge, Yang et al. [84] are the ﬁrst to combine deep learning with transfer learning\nto classify hyperspectral images. The model consists of two convolutional neural\nnetworks, which are used to extract spectral features and spatial features. Then,\nthe joint spectral-spatial feature will be input into the fully connected layer to\ngain a ﬁnal result. According to ﬁne-tuning, the model is ﬁrst fully trained\non the hyperspectral image of the source domain. Next, the fully connected\nlayer is replaced and the parameters of the convolutional network are reserved.\nFinally, the transfer model will be trained on the target hyperspectral image to\nadapt to the new data distribution. The later transfer learning models based on\nﬁne-tuning basically follow that architecture [85, 86, 87, 88]. It is worth noting\nthat Deng et al. [89] combined transfer learning with active learning to classify\nHSI.\nData distribution adaptation is another commonly used transfer learning\nmethod. The basic idea of this theory is that in the original feature space, the\ndata probability distributions of the source domain and the target domain are\nusually diﬀerent. However, they can be mapped to a common feature space\ntogether. In this space, their data probability distributions become similar. In\n2014, Ghifary et al. [90] ﬁrst proposed a shadow neural network-based domain\nadaptation model, called DaNN. The innovation of this work is that a maximum\nmean discrepancy (MMD) adaptation layer is added to calculate the distance\nbetween the source domain and the target domain. Moreover, the distance is\nmerged into the loss function to reduce the diﬀerence between the two data\ndistributions. Subsequently, Tzeng et al. [91] extended this work with a deeper\nnetwork and proposed deep domain confusion to solve the adaptive problem of\ndeep networks. Wang et al. [92] introduced the deep domain adaptation model\nto the ﬁeld of hyperspectral image classiﬁcation for the ﬁrst time.\nIn [92],\n12\nRepresentation \nLearning\nClass \nDiscriminator\nDomain \nDiscriminator\nClass \nLabel\nDomain \nLabel\nSamples\nFigure 5: Flowchart of DANN.\ntwo hyperspectral images from diﬀerent scenes will be mapped to two low-\ndimensional subspaces by the deep neural network, in which the samples are\nrepresented as manifolds. MMD is used to measure the distance between two\nlow-dimensional subspaces and is added to the loss function to make two low-\ndimensional subspaces have high similarity. In addition, they still add the sum\nof the distances between samples and their neighbor into the loss function to\nensure that the low-dimensional manifold is discriminative.\nMotivated by the excellent performance of generative adversarial net (GAN),\nYaroslav et al. [93] ﬁrst introduced it into transfer learning.\nThe network\nis named DANN (domain-adversarial neural network), which is diﬀerent from\nDaNN proposed by Ghifary et al. [90]. The generator Gf and the discriminator\nGd compete with each other until they have converged. In transfer learning,\nthe data in one of the domains (usually the target domain) are regarded as the\ngenerated sample. The generator aims to learn the characteristics of the target\ndomain sample so that the discriminator cannot distinguish which domain the\nsample comes from to achieve the purpose of domain adaptation. Therefore,\nGf is used to represent the feature extractor here.\nElshamli et al. [94] ﬁrst introduced the concept of DANN to the task of\nhyperspectral image classiﬁcation. Compared to general GNN, it has two dis-\ncriminators. One is the class discriminator predicting the class labels of samples,\nand the other is the domain discriminator predicting the source of the samples.\nDiﬀerent from the two-stage method, DANN is an end-to-end model that can\nperform representation learning and classiﬁcation tasks simultaneously. More-\nover, it is easy to train. Further, it outperforms two-stage frameworks such as\nthe denoising autoencoder and traditional approaches such as PCA in hyper-\nspectral image classiﬁcation.\n3.2. Deep Active Learning for HSI classiﬁcation\nActive learning [95] in the supervised learning method can eﬃciently deal\nwith small-sample problems.\nIt can eﬀectively learn discriminative features\nby autonomously selecting representative or high-information samples from the\ntraining set, especially when the labeled samples are scarce. Generally speaking,\nactive learning consists of ﬁve components, A = (C, L, U, Q, S). Among them,\nC represents one or a group of classiﬁers. L and U represent the labeled samples\n13\nand unlabeled samples, respectively. Q is the query function, which is used to\nquery the samples with a large amount of information among the unlabeled\nsamples. S is an expert and can label unlabeled samples. In general, active\nlearning has two stages. The ﬁrst stage is the initialization stage. In this stage,\na small number of samples will be randomly selected to form the training set L\nand be labeled by experts to train the classiﬁer. The second stage is the iterative\nquery. Q will select new samples from the unlabeled sample set U for S to mark\nthem based on the results of the previous iteration and add them to the training\nset L. The active learning method applied to hyperspectral image classiﬁcation\nis mainly based on the active learning algorithm of the committee and the\nactive learning algorithm based on the posterior probability. In the committee-\nbased active learning algorithm, the EQB method uses entropy to measure the\namount of information in unlabeled samples. Speciﬁcally, the training set L will\nbe divided into k subsets to train k classiﬁers and then use these k classiﬁers to\nclassify all unlabeled samples. Therefore, each unlabeled sample corresponds to\nk predicted labels. The entropy value is calculated from this:\nxEQB = arg min\nxi∈U\nHEQB(xi)\nlog(Ni)\n(9)\nwhere H represents the entropy value, and Ni represents the number of classes\npredicted by the sample xi. Samples with large entropy will be selected and\nmanually labeled [96]. In [97], the deep belief network is used to generate the\nmapping feature h of the input x in an unsupervised way, and then, h will be used\nto calculate the information entropy. At the same time, sparse representation is\nused to estimate the representations of the sample. In the process of selecting\nsamples for active learning, the information entropy and representations of the\nsamples are comprehensively considered.\nIn contrast, the active learning method based on posterior probability [98, 99,\n100] is more widely used. Breaking ties belongs to the active learning method of\nposterior probability, which is widely used in hyperspectral classiﬁcation tasks.\nThis method ﬁrst uses speciﬁes models, such as convolutional networks, maxi-\nmum likelihood estimation classiﬁers, support vector machines, etc., to estimate\nthe posterior probabilities of all samples in the candidate pool. Then, the ap-\nproach uses the posterior probability to input the following formula to produce\na measure of sample uncertainty:\nxBT = arg min\nxi∈U\n\u001a\nmax\nw∈N p (y∗\ni = w|xi) −\nmax\nw∈N\\w+ p(y∗\ni = w|xi)\n\u001b\n(10)\nIn the above formula, we ﬁrst calculate the diﬀerence between the largest proba-\nbility and the second-largest probability among the posterior probabilities of all\ncandidate samples and select the sample with the minimum diﬀerence to join the\nvaluable data set. The lower xBT is, the more uncertain is the sample. In [98],\nLi et al. ﬁrst used an autoencoder to construct an active learning model for\nhyperspectral image classiﬁcation tasks. At the same time, Sun et al. [99] also\n14\nproposed a similar method. However, this method only uses spectral features.\nBecause of the eﬀectiveness of spatial information, in [101], when generating\nthe posterior probability, the space-spectrum joint features are considered at\nthe same time. In contrast, Cao et al. [100] use convolutional neural networks\nto generate the posterior probability.\nIn general, the active learning method can automatically select eﬀective\nsamples according to certain criteria, reduce ineﬃcient redundant samples, and\nthus well alleviate the problem of missing training samples in the small-sample\nproblem.\nfω\nSample \nselection\nFigure 6: Architecture of active learning.\n3.3. Deep Few-shot Learning for HSI classiﬁcation\nFew-shot learning is among meta-learning approaches and aims to study the\ndiﬀerence between the samples instead of directly learning what the sample is,\ndiﬀerent from most other deep learning methods.\nIt makes the model learn\nto learn. In few-shot classiﬁcation, given a small support set with N labeled\nsamples Sk\nN = {(x1, y1), · · · , (xN, yN)}, which have k categories, the classiﬁer\nwill mask the query sample with the label of the largest similarity sample among\nSk\nN. To achieve this target, many learning frameworks have been proposed and\nthey can be divided into two categories: meta-based model and metric-based\nmodel.\nThe prototype network [102] is one of the metric-based models of few-shot\nlearning.\nIts basic idea is that every class can be depicted by a prototype\nrepresentation, and the samples that belong to the same category should be\naround the class prototype. First, all samples will be transformed into a metric\nspace through an embedding function fφ : RD →RM and represented by the\nembedding vector ck ∈RM. Due to the powerful ability of the convolutional\nnetwork, it is used as the embedding function. Moreover, the prototype vector\nis usually the mean of the embedding vector of the samples in the support set\nfor each class ci.\nci =\n1\n|Si|\nX\n(xj,yj)∈Si\nfφ(xj)\n(11)\nIn [103], Liu et al. simply introduce the prototype network into hyperspectral\nimage classiﬁcation task and use ResNet [59] to serve as a feature extractor\nthat maps the samples into a metric space. Then, the prototype network is\n15\nsigniﬁcantly improved for the hyperspectral image classiﬁcation task by [104].\nIn the paper, the spatial-spectral feature is ﬁrst integrated by the local pattern\ncoding, and the 1D-CNN converts it to an embedding vector. The prototype is\nthe weighted mean of these embedding vectors, which is contrary to the general\nprototype network. In [105] Xi et al. replace the mapping function with hybrid\nresidual attention [106] and introduce a new loss function to force the network\nto increase the interclass distance and decrease the intraclass distance.\nCNN\nTraining samples\nTest sample\nEmbedding features\nFigure 7: Architecture of a prototype network.\nThe relation network [107] is another metric-based model of few-shot learn-\ning. In general, it has two modules: the embedding function fφ : RD →RM\nand relation function fψ : R2M →R. The function of the embedding module is\nthe same as the prototype network, and its key idea is the relation module. The\nrelation module is to calculate the similarity of samples. It is a learnable module\nthat is diﬀerent from the Euclidean distance or cosine distance. In other words,\nthe relation network introduces a learnable metric function based on the proto-\ntype network. The relation module can more precisely describe the diﬀerence\nof samples by the study. During inference, the query embedding fψ(xi) will be\ncombined with the support embedding fψ(xj) as C(fψ(xi), fψ(xj)). Usually,\nC(·, ·) is a concatenation operation. Then, the relation function will transform\nthe splicing vector to a relation score ri,j, which indicates the similarity between\nxi and xj.\nri,j = fψ(C(fψ(xi), fψ(xj)))\n(12)\nSeveral works have introduced the relation network into hyperspectral image\nclassiﬁcation to solve the small sample set problem. Deng et al. [108] ﬁrst in-\ntroduced the relation network into HSI. They use a 2-dimensional convolutional\nneural network to construct both the embedding function and relation function.\nGao et al. [109] and Ma et al. [110] have also proposed a similar architecture.\nIn [111], to extract the joint spatial-spectral feature, Rao et al. implemented\nthe embedding function with a 3-dimensional convolutional neural network.\nThe Siamese network [112, 113, 114] is a typical network in few-shot learning.\nCompared with the above network, its input is a sample pair.\nThus, it is\ncomposed by two parallel subnetworks fφ1 : RD →RM with the same structure\n16\nfϕ\ngφ\nsample pool\na query sample\nfeature space\nrelation\nscore\none-hot\nvector\nembedding \nmodule\nmetric\nmodule\nrelation module\ninput\nFigure 8: Architecture of relation network.\nand sharing parameters. The subnetworks respectively accept an input sample\nand map it to a low-dimensional metric space to generate their own embedding\nfφ1(xi) and fφ1(xj).\nThe Euclidean distances D(xi, xj) is used to measure\ntheir similarity.\nD(xi, xj) = ∥fφ1(xi) −fφ1(xj)∥2\n(13)\nThe higher the similarity between the two samples is, the more likely they are to\nbelong to the same class. Recently, the Siamese network was introduced into HSI\nclassiﬁcation. Usually, a 2-dimensional convolutional neural network [115, 116]\nis used to serve as the embedding function, as in the above two networks. In the\nsame way, several methods combined the 1-dimensional convolution neural net-\nwork with the 2-dimensional one [117, 118] or use a 3-dimensional network [119]\nfor the joint spectral-spatial feature.\nMoreover, Miao et al. [120] have tried\nto use the stack autoencoder to construct the embedding function fφ1. After\ntraining, the model has the ability to identify the diﬀerence between samples.\nTo obtain the ﬁnal classiﬁcation result, we still need a classiﬁer to classify the\nembedding feature of the sample, which is diﬀerent from the prototype network\nand the relation network. To avoid overﬁtting under limited labeled samples,\nan SVM is usually used as a classiﬁer since it is famous for its lightweight.\n4. Experiments\nIn most papers, comprehensive experiments and analysis are introduced to\ndescribe the advantages and disadvantages of the methods in the paper. How-\never, the problem is that diﬀerent papers may choose diﬀerent experimental\nsettings. For example, the same number of samples for training or test is used\nin the experiments, and the chosen samples are normally diﬀerent since they are\nchosen randomly. To evaluate diﬀerent methods fairly, we should use the exact\nsame experimental setting. That is the reason why we design experiments to\nevaluate diﬀerent methods.\n17\nCNN\nLoss function\nPatch1\nPatch2\nFeature maps\nFigure 9: Architecture of the Siamese network.\nAs described above, the main methods of small-sample learning currently in-\nclude the autoencoder, few-shot learning, transfer learning, active learning, and\ndata augmentation. Therefore, some representative networks of the following\nmethods–S-DMM [121], SSDL [37], 3DCAE [40], TwoCnn [122], SSLstm [75]\nand 3DVSCNN [123], which contain convolutional network models and recur-\nrent network models, are selected to conduct experiments on three benchmark\ndata sets–PaviaU, Salinas and KSC. All models are based on deep learning.\nHere, we only focus on the robustness of the model on a small-sample data set,\nso they classify hyperspectral images based on joint spectral-spatial features.\nAccording to the sample size per category in the training data set, the exper-\niment is divided into three groups. The ﬁrst has 10 samples for each category,\nthe second has 50 samples for each category and the third has 100 samples for\neach category. At the same time, to ensure the stability of the model, each\ngroup of experiments is performed ten times, and the training data set is dif-\nferent each time. Finally, models are evaluated by average accuracy (AA) and\noverall accuracy (OA).\n4.1. Introduction of data sets\n• Pavia University (PaviaU): The Pavia University data set consists of\nhyperspectral images, each with 610*340 pixels and a spatial resolution of\n1.3 meters, which was taken by the ROSIS sensor above Pavia University\nin Italy. The spectral imagery continuously images 115 wavelengths in the\nrange of 0.43∼0.86 um. Since 12 of the wavelengths are polluted by noise,\neach pixel in the ﬁnal data set contains 103 bands. It contains 42,776\nlabeled samples in total, covering 9 objects. In addition, its sample size\nof each object is shown in Table 1.\n• Salinas: The Salinas data set consists of hyperspectral images with 512*217\npixels and a spatial resolution of 3.7 meters, taken over the Salinas Valley\nin California by the AVIRIS sensor. The spectral imagery continuously\nimages 224 wavelengths in the range of 0.2∼2.4 um. Since 20 of the bands\n18\ncannot be reﬂected by water, each pixel in the ﬁnal data set contains 204\nbands. It contains 54,129 labeled samples in total, covering 16 objects. In\naddition, its sample size of each object is shown in Table 2.\n• Kennedy Space Center (KSC): The KSC data set was taken at the\nKennedy Space Center (KSC), above Florida, and used the AVIRIS sensor.\nIts hyperspectral images contain 512*641 pixels, and the spatial resolution\nis 18 meters. The spectral imagery continuously images 224 wavelengths\nin the range of 400∼2500 nm. Similarly, after removing 48 bands that are\nabsorbed by water and have a low signal-to-noise ratio, each pixel in the\nﬁnal data set contains 176 bands. It contains 5211 label samples, covering\n13 objects. Moreover, its sample size of each object is shown in Table 3.\nTable 1: Pavia University. It contains 9 objects. The second column and last column represent\nthe name of objects and sample number, respectively.\nNO.\nClass\nTotal\nC1\nAsphalt\n6631\nC2\nMeadows\n18649\nC3\nGravel\n2099\nC4\nTrees\n3064\nC5\nPainted metal sheets\n1345\nC6\nBare Soil\n5029\nC7\nBitumen\n1330\nC8\nSelf-Blocking Bricks\n3682\nC9\nShadows\n947\n4.2. Selected models\nSome state-of-the-art methods are choose to evaluate their performance.\nThey were trained using diﬀerent platforms, including Caﬀe, PyTorch, etc.\nSome platforms such Caﬀe are not well supported by the new development\nenvironments. Most models are our re-implementations and are trained using\nthe exact same setting. Most of the above model settings are based on the orig-\ninal paper, and some are modiﬁed slightly based on the experiment. All models\nare trained and tested on the same training data set that is picked randomly\nbased on pixels and the test data set, and their settings have been optimally\ntuned. The implementation situation of the code is shown in Table 4. The\ndescriptions of the chosen models are provided in the following part.\n• SAE LR [34]. This is the ﬁrst paper to introduce the autoencoder into\nhyperspectral image classiﬁcation, opening a new era of hyperspectral im-\nage processing. It adopts a raw autoencoder composed of linear layers to\nextract the feature. The size of the neighbor region is 5 × 5, and the ﬁrst\n4 components of PCA are chosen. Subsequently, we can gain a spatial\nfeature vector. Before inputting into the model, the raw spatial feature\n19\nTable 2: Salinas. It contains 16 objects. The second column and last column represent the\nname of objects and sample number, respectively.\nNO.\nClass\nTotal\nC1\nBroccoli green weeds 1\n2009\nC2\nBroccoli green weeds 22\n3726\nC3\nFallow\n1976\nC4\nFallow rough plow\n1394\nC5\nFallow smooth\n2678\nC6\nStubble\n3959\nC7\nCelery\n3579\nC8\nGrapes untrained\n11271\nC9\nSoil vineyard develop\n6203\nC10\nCorn senesced green weeds\n3278\nC11\nLettuce romaine 4wk\n1068\nC12\nLettuce romaine 5wk\n1927\nC13\nLettuce romaine 6wk\n916\nC14\nLettuce romaine 7wk\n1070\nC15\nVineyard untrained\n7268\nC16\nVineyard vertical trellis\n1807\nTable 3: KSC. It contains 13 objects. The second column and last column represent the name\nof objects and sample number, respectively.\nNO.\nClass\nTotal\nC1\nScrub\n761\nC2\nWillow swamp\n243\nC3\nCabbage palm hammock\n256\nC4\nCabbage palm/oak hammock\n252\nC5\nSlash pine\n161\nC6\nOak/broadleaf hammock\n229\nC7\nHardwood swamp\n105\nC8\nGraminoid marsh\n431\nC9\nSpartina marsh\n520\nC10\nCattail marsh\n404\nC11\nSalt marsh\n419\nC12\nMud ﬂats\n503\nC13\nWater\n927\nTable 4: Originators of model implementations. F denotes that the code of the model comes\nfrom the original paper. T denotes our implemented model.\nS-DMM\nSSDL\n3DCAE\nTwoCnn\n3DVSCNN\nSSLstm\nCNN HSI\nSAE LR\nF\nT\nF\nT\nT\nT\nT\nT\n20\nand the spatial feature are stacked to form a joint feature. To reduce the\ndiﬃculty of training, it uses a greedy layerwise pretraining method to train\neach layer, and the parameters of the encoder and decoder are symmetric.\nThen, the encoder concatenates a linear classiﬁer for ﬁne tuning. Accord-\ning to [34], the hidden size is set to 60, 20, and 20 for PaviaU, Salinas,\nand KSC, respectively.\n• S-DMM [121]. This is a relation network that contains an embedding\nmodule and relation module implemented by 2D convolutional networks.\nThe model aims to make samples in the feature space have a small intra-\nclass distance and a large interclass distance through a learnable feature\nembedding function and a metric function. After training, all samples will\nbe assigned to the corresponding clusters. Finally, a simple KNN is used\nto classify the query sample. In the experiment, the neighbor region of\nthe pixel is ﬁxed as 5 × 5 and the feature dimension is set to 64.\n• 3DCAE [40]. This is a 3D convolutional autoencoder adopting a 3D con-\nvolution layer to extract the joint spectral-spatial feature. First, 3DCAE is\ntrained by the traditional method, and then, an SVM classiﬁer is adopted\nto classify the hidden features on the top of 3DCAE. In the experiment,\nthe neighbor region of the pixel is set to 5 × 5 and 90% of the samples are\nused to train the 3D autoencoder. There are two diﬀerent hyperparameter\nsettings corresponding to Salinas and PaviaU, and the model has not been\ntested on KSC in [121]. Therefore, on the KSC, the model uses the same\nhyperparameter conﬁguration as on the Salinas because they are collected\nby the same sensor.\n• SSDL [37]. This is a typical two-stream structure extracting the spectral\nand spatial feature separately through two diﬀerent branches and merging\nthem at the end. Inspired by [34], the author adopts a 1D autoencoder to\nextract the spectral feature. In the branch of spatial feature extraction,\nthe model uses a spatial pyramid pooling layer to replace the traditional\npooling layer on the top convolutional layer. The spatial pyramid pooling\nlayer enables the deep convolutional neural network to generate a ﬁxed-\nlength feature. On the one hand, it enables the model to convert the input\nof diﬀerent sizes into a ﬁxed-length, which is good for the module that is\nsensitive to the input size; on the other hand, it is useful for the model\nto better adapt to objects of diﬀerent scales, and the output will include\nfeatures from coarse to ﬁne, achieving multiscale feature fusion. Then,\na simple logistic classiﬁer is used to classify the spectra-spatial feature.\nIn the experiment, 80% of the data are used to train the autoencoder\nthrough the method of greedy layer-wise pretraining. Moreover, in the\nspatial branch, the size of the neighbor region is set to 42*42 and PCA is\nused to extract the ﬁrst component. Then, the overall model is trained\ntogether.\n• TwoCnn [122]. This is a two-stream structure based on ﬁne-tuning. In\nthe spectral branch, it adopts a 1D convolutional layer to capture local\n21\ninformation of spectral features, which is entirely diﬀerent from SSDL. In\nparticular, transfer learning is used to pretrain parameters of the model\nand endow it with good robustness on limited samples.\nThe pairs of\nthe source data set and target data set are Pavia Center–PavaU, Indian\npines-Salinas, and Indian pines-KSC. In [122], they also did not test the\nmodel on KSC. Thus, we regard Indian pines as the source domain for\nKSC, given that both data sets come from the same type of sensor. The\nneighbor region of the pixel is set to 21*21. Additionally, it averages along\nthe spectral channel to reduce the input dimension, instead of PCA. In the\npretraining process, 15% of samples of each category of Pavia and 90%\nof samples of each category of Indian pines are treated as the training\ndata set, and the rest serve as the test data set. To make the number\nof bands in the source data set and target data set the same, we ﬁlter\nout the band that has the smaller variance. According to [122], all other\nlayers are transferred except for the softmax layer. Finally, the model is\nﬁne-tuned on the target data set with the same conﬁguration.\n• 3DVSCNN [123].\nThis is a general CNN-based image classiﬁcation\nmodel, but it uses a 3D convolutional network to extract spectral-spatial\nfeatures simultaneously followed by a fully connected network for classiﬁ-\ncation. The main idea of [123] is the usage of active learning. The process\ncan be divided into two steps: the selection of valuable samples and the\ntraining of the model. In [123], an SVM serves as a selector to iteratively\nselect some of the most valuable samples according to Eq.(10). Then, the\n3DVSCNN is trained on the valuable data set. The size of its neighbor\nregion is set to 13*13. During data preprocessing, it uses PCA to extract\nthe top 10 components for PaviaU and Salinas, and the top 30 components\nfor KSC, which contain more than 99% of the original spectral information\nand still keep a clear spatial geometry. In the experiment, 80% of samples\nwill be picked by the SVM to form a valuable data set for 4 samples in\neach iteration. Then, the model is trained on the valuable data set.\n• CNN HSI [28]. The model combines multilayer 1 × 1 2D convolutions\nfollowed by local response normalization to capture the feature of hyper-\nspectral images. To avoid the loss of information after PCA, it uses 2D\nconvolution to extract spectral and spatial joint features directly, instead\nof 3D convolution. At the same time, it also adopts a dropout layer and\ndata augmentation, including rotation and ﬂipping, to improve the gen-\neralization of the model and reduce overﬁtting. After data augmentation,\nan image can generate eight diﬀerent orientation images. Moreover, the\nmodel removes the linear classiﬁer to decrease the number of trainable\nparameters. According to [28], the dropout rate is set to 0.6, the size of\nthe neighbor region is 5 × 5, and the batch size is 16 in the experiment.\n• SSLstm [75]. Unlike the above methods, SSLstm adopts recurrent net-\nworks to process spectral and spatial features simultaneously. In the spec-\ntral branch, called SeLstm, the spectral vector is seen as a sequence. In\n22\nthe spatial branch, called SaLstm, it treats each line of the image patch\nas a sequence element. Therefore, along the column direction, the im-\nage patch can be well converted into a sequence. In particular, it fuses\nthe predictions of the two branches in the label space to obtain the ﬁnal\nprediction result, which is deﬁned as\nP(y = j|xi) = wspePspe(y = j|xi) + wspaPspa(y = j|xi)\n(14)\nwhere P(y = j|xi) denotes the ﬁnal posterior probability, Pspe(y = j|xi)\nand Pspa(y = j|xi) denote the posterior probabilities from spectral and\nspatial modules, respectively, and wspe and wspa are fusion weights that\nsatisfy the sum of 1. In the experiment, the size of the neighbor region\nis set to 32*32 for PaviaU and Salinas. In addition, for KSC, it is set to\n64*64. Next, the ﬁrst component of PCA is reserved on all data sets. The\nnumber of hidden nodes of the spectral branch and the spatial branch are\n128 and 256, respectively. In addition, wspe and wspa are set to 0.5 and\n0.5 separately.\n4.3. Experimental results and analysis\nThe accuracy of the test data set is shown in Table 5, Table 6, and Table\n7. Corresponding classiﬁcation maps are shown in Figure 11∼19. The ﬁnal\nclassiﬁcation result of the pixel is decided by the voting result of 10 experiments.\nTaking Table 5 as an example, the experiment is divided into three groups,\nand the sample sizes in each group are 10, 50, and 100, respectively. The afore-\nmentioned models are conducted 10 times in every experiment sets. Then, we\ncount the average of their class classiﬁcation accuracy, AA, and OA for compar-\ning their performance. When sample size is 10, S-DMM has the highest AA and\nOA, which are 91.08% and 84.45% respectively, in comparison with the AA and\nOA of 71.58% and 60.00%, 75.34 % and 74.79%, 74.60% and 78.61%, 75.64%\nand 75.17%, 72.77% and 69.59%, 85.12% and 82.13%, 72.40% and 66.05% for\n3DCAE, SSDL, TwoCnn, 3DVSCNN, SSLstm, CNN HSI and SAE LR. Besides,\nS-DMM has the largest number of class classiﬁcation accuracy. When the sam-\nple size is 50, S-DMM and CNN HSI have the highest AA and OA respectively,\nwhich are 96.47% and 95.21%. In the last group, 3DVSCNN and CNN HSI\nhave the highest AA and OA, which are 97.13% and 97.35%. According to the\nother two tables, we can conclude with a similar result.\nAs shown in Table 5, Table 6 and Table 7, we can conclude that most models’\nperformance on KSC, except for 3DCAE, is better than the other two data sets.\nEspecially when the data set contains few samples, the accuracy of S-DMM is\nup to 94%, superior to other data sets. This is because the surface objects on\nthe KSC itself have a discriminating border between each other, regardless of\nits higher spatial resolution than that of the other data sets, as shown in Figure\n17∼19. In the other data sets, models easily misclassify the objects that have a\nsimilar spatial structure, as illustrated in Meadows (class 2) and Bare soil (class\n6) in PaviaU and Fallow rough plow (class 4) and Grapes untrained (class 8) in\nSalinas, as shown in 11∼16. The accuracy of all models on Grapes untrained is\n23\nlower than other classes in Salinas. In Figure 10, on all data sets, as the number\nof samples increases, the accuracy of all models will improve together.\nAs shown in Figure 10, when the sample size of each category is 10, S-DMM\nand CNN HSI have achieved stable and excellent performance on all data sets.\nThey are not sensitive to the size of the data set. In Figure 10(b) and Fig-\nure 10(c), with increasing sample size, the accuracy of S-DMM and CNN HSI\nhave improved slightly, but their increase is lower than that of others. In Fig-\nure 10(a), when the sample size increases from 50 to 100, we can obtain the\nsame conclusion. This result shows that both of them can be applied to solve\nthe small-sample problem in hyperspectral images. Especially for S-DMM, it\nhas gained the best performance on the metric of AA and OA on Salinas and\nKSC in the experiment with a sample size of 10. On PaviaU, it still wins the\nthird place. This result also proves that it can work well on a few samples.\nAlthough TwoCnn, 3DVSCNN, and SSLstm achieve good performance on all\ndata sets, when the data set contains fewer samples, they will not work well.\nIt is worth mentioning that 3DVSNN uses fewer samples to train than other\nmodels for selecting valuable samples. This approach may not be beneﬁcial for\nthose classes with few samples. As shown in 7, 3DVSCNN has a good perfor-\nmance on OA, but a bad performance on AA. For class 7, when its sample size\nincreases from 10 to 50 and 100, its accuracy drops. This is because the total\nsample size of it is the smallest on KSC. Therefore, it contains few valuable\nsamples. Moreover, the step of selecting valuable samples would cause an im-\nbalance between the classes, which leads to the accuracy of class 7 decreasing.\nOn almost all data sets, autoencoder-based models achieve poor performance\ncompared with other models. Although unsupervised learning does not need\nto label samples, if there are no constraints, the autoencoder might actually\nlearn nothing. Moreover, since it has a symmetric architecture, it would result\nin a vast number of parameters and increase the diﬃculty of training. There-\nfore, SSDL and SAE LR use a greedy layerwise pretraining method to solve this\nproblem. However, 3DCAE does not adopt this approach and achieves the worst\nperformance on all data sets. As shown in Figure 10, it still has considerable\nroom for improvement.\nOverall, classiﬁcation results based on few-shot learning, active learning,\ntransfer learning, and data augmentation are better than autoencoder-based\nunsupervised learning methods on the limited sample in all experiments. Few-\nshot learning beneﬁts from the exploration of the relationship between samples\nto ﬁnd a discriminative decision boarder. Active learning beneﬁts from the se-\nlection of valuable samples, which enables the model to focus more attention\nto indistinguishable samples. Transfer learning makes good use of the similar-\nity between diﬀerent data sets, which reduces the quantity of data required for\ntraining and trainable parameters, improving the model’s robustness. Accord-\ning to raw data, the method of data augmentation generates more samples to\nexpand the diversity of samples. Although the autoencoder can learn the inter-\nnal structure of the unlabeled data set, the ﬁnal feature representation might\nnot have task-related characteristics. This is the reason why its performance on\na small-sample data set is inferior to supervised learning.\n24\nTable 5: PaviaU. Classiﬁcation accuracy obtained by S-DMM [121], 3DCAE [40], SSDL [37],\nTwoCnn [122], 3DVSCNN [123], SSLstm [75], CNN HSI [28] and SAE LR [34] on PaviaU.\nThe best accuracies are marked in bold. The ”size” in the ﬁrst line denotes the sample size\nper category.\nsize\nclasses\nS-DMM\n3DCAE\nSSDL\nTwoCnn\n3DVSCNN\nSSLstm\nCNN HSI\nSAE LR\n10\n1\n94.34\n49.41\n68.33\n71.80\n63.03\n72.59\n84.60\n66.67\n2\n73.13\n51.60\n72.94\n88.27\n69.22\n68.86\n67.57\n56.68\n3\n86.85\n54.06\n53.71\n47.58\n71.77\n48.08\n72.80\n46.37\n4\n95.04\n94.81\n88.58\n96.29\n85.10\n79.06\n93.65\n80.10\n5\n99.98\n99.86\n97.21\n94.99\n98.61\n93.80\n99.84\n98.81\n6\n85.58\n57.40\n66.21\n49.75\n75.17\n62.53\n78.35\n55.87\n7\n98.55\n80.34\n68.17\n58.65\n65.61\n65.39\n92.14\n81.42\n8\n86.47\n57.97\n64.07\n66.95\n55.77\n67.60\n78.17\n66.83\n9\n99.81\n98.76\n98.83\n97.15\n96.48\n97.02\n98.92\n98.90\nAA\n91.08\n71.58\n75.34\n74.60\n75.64\n72.77\n85.12\n72.40\nOA\n84.55\n60.00\n74.79\n78.61\n75.17\n69.59\n82.13\n66.05\n50\n1\n97.08\n80.76\n76.11\n88.50\n90.60\n82.96\n93.66\n78.83\n2\n90.09\n63.14\n87.39\n86.43\n93.68\n82.42\n94.82\n65.36\n3\n95.15\n62.57\n70.28\n69.21\n90.64\n81.59\n94.87\n65.50\n4\n97.35\n97.33\n89.27\n98.80\n93.47\n91.31\n94.49\n92.43\n5\n100.00\n100.00\n98.14\n99.81\n99.92\n99.67\n100.00\n99.47\n6\n96.32\n80.15\n75.12\n84.93\n94.15\n82.58\n88.14\n72.30\n7\n99.31\n88.45\n75.80\n83.12\n94.98\n92.34\n97.21\n86.04\n8\n92.97\n75.11\n70.57\n83.57\n91.55\n84.75\n87.52\n79.74\n9\n99.98\n99.69\n99.61\n99.91\n98.72\n99.39\n99.78\n99.29\nAA\n96.47\n83.02\n82.48\n88.25\n94.19\n88.56\n94.50\n82.10\nOA\n94.04\n64.17\n84.92\n90.69\n94.23\n84.50\n95.21\n77.42\n100\n1\n97.11\n83.05\n85.59\n92.21\n94.38\n90.84\n94.44\n78.64\n2\n91.64\n73.45\n86.17\n76.86\n95.90\n83.26\n97.75\n74.28\n3\n94.23\n73.02\n80.29\n72.24\n95.96\n80.66\n95.37\n79.87\n4\n98.70\n97.87\n97.14\n99.28\n97.65\n92.54\n95.88\n93.54\n5\n100.00\n100.00\n99.06\n99.89\n99.95\n99.57\n99.99\n99.24\n6\n93.51\n86.82\n83.16\n95.90\n97.92\n87.61\n91.01\n69.83\n7\n99.21\n90.17\n94.08\n89.88\n98.39\n93.45\n98.37\n89.42\n8\n92.73\n88.31\n88.43\n90.03\n94.21\n90.08\n92.41\n85.05\n9\n99.99\n99.82\n99.65\n99.98\n99.85\n99.80\n99.70\n99.55\nAA\n96.35\n88.06\n90.40\n90.70\n97.13\n90.87\n96.10\n85.49\nOA\n94.65\n70.15\n89.33\n94.76\n97.05\n87.19\n97.35\n81.44\n25\nTable 6: Salinas. Classiﬁcation accuracy obtained by S-DMM [121], 3DCAE [40], SSDL [37],\nTwoCnn [122], 3DVSCNN [123], SSLstm [75], CNN HSI [28] and SAE LR [34] on Salinas.\nThe best accuracies are marked in bold. The ”size” in the ﬁrst line denotes the sample size\nper category.\nsize\nclasses\nS-DMM\n3DCAE\nSSDL\nTwoCnn\n3DVSCNN\nSSLstm\nCNN HSI\nSAE LR\n10\n1\n99.45\n99.28\n76.01\n88.22\n97.92\n79.38\n98.80\n86.01\n2\n99.21\n59.04\n69.24\n78.09\n99.71\n72.49\n98.77\n44.21\n3\n96.70\n66.54\n69.89\n74.80\n95.09\n86.83\n95.48\n44.72\n4\n99.56\n98.65\n94.96\n98.19\n99.28\n99.45\n98.36\n97.40\n5\n97.12\n81.94\n89.43\n96.54\n93.35\n94.95\n92.55\n83.93\n6\n89.64\n98.52\n96.19\n98.96\n99.81\n93.65\n99.96\n87.28\n7\n99.82\n97.31\n76.83\n92.52\n96.73\n87.82\n99.61\n96.94\n8\n70.53\n68.11\n42.58\n54.35\n67.89\n61.64\n77.51\n41.58\n9\n99.02\n95.06\n89.58\n81.22\n99.42\n90.47\n97.19\n78.45\n10\n91.13\n9.43\n76.40\n75.18\n91.75\n86.66\n89.23\n30.75\n11\n97.56\n72.26\n93.04\n92.26\n95.26\n91.37\n95.45\n23.52\n12\n99.87\n72.16\n86.60\n86.40\n96.65\n95.38\n99.96\n82.63\n13\n99.25\n99.78\n95.46\n98.18\n96.64\n96.90\n99.22\n92.88\n14\n96.30\n89.93\n90.50\n96.10\n99.68\n91.68\n96.80\n62.40\n15\n72.28\n56.98\n65.40\n55.60\n83.86\n75.55\n72.03\n57.10\n16\n95.29\n44.35\n75.89\n92.39\n92.03\n88.43\n94.07\n76.75\nAA\n93.92\n75.58\n80.50\n84.94\n94.07\n87.04\n94.06\n67.91\nOA\n89.69\n71.50\n74.29\n77.54\n90.18\n81.20\n91.31\n67.43\n50\n1\n99.97\n98.81\n92.70\n97.99\n99.99\n94.18\n99.20\n85.37\n2\n99.84\n86.97\n88.30\n91.35\n99.94\n92.34\n99.57\n92.51\n3\n99.84\n54.83\n87.50\n94.87\n99.74\n97.02\n99.62\n81.25\n4\n99.93\n98.87\n99.41\n99.96\n99.89\n99.95\n99.63\n98.40\n5\n99.40\n95.62\n95.83\n98.96\n99.38\n98.34\n98.79\n95.12\n6\n99.92\n99.62\n98.95\n99.87\n100.00\n98.78\n99.98\n98.86\n7\n99.92\n98.17\n96.47\n96.60\n99.85\n97.80\n99.78\n98.55\n8\n68.92\n81.74\n62.99\n68.05\n85.35\n77.17\n77.93\n46.04\n9\n99.76\n94.87\n95.34\n86.01\n99.99\n96.15\n99.71\n94.84\n10\n97.18\n12.87\n95.31\n93.94\n98.23\n97.23\n97.33\n77.69\n11\n99.57\n75.82\n97.73\n97.10\n98.59\n97.71\n99.54\n77.14\n12\n99.90\n58.18\n97.51\n97.16\n99.89\n98.88\n99.84\n96.87\n13\n99.84\n99.98\n98.55\n98.60\n100.00\n99.12\n99.87\n97.33\n14\n98.15\n93.80\n97.54\n99.37\n99.91\n99.24\n99.53\n91.49\n15\n76.12\n41.84\n69.04\n67.21\n88.77\n86.24\n83.39\n65.15\n16\n98.87\n69.00\n94.34\n97.78\n98.55\n97.64\n98.15\n91.94\nAA\n96.07\n78.81\n91.72\n92.80\n98.00\n95.49\n96.99\n86.78\nOA\n90.92\n74.73\n85.79\n87.01\n95.30\n91.37\n95.08\n79.49\n100\n1\n99.86\n98.81\n98.22\n98.74\n99.99\n97.86\n99.77\n92.44\n2\n99.74\n91.88\n96.54\n96.70\n99.99\n97.74\n99.86\n89.46\n3\n99.99\n63.20\n95.40\n97.47\n99.16\n98.91\n99.79\n92.05\n4\n99.84\n99.12\n99.29\n99.95\n99.85\n99.78\n99.44\n99.03\n5\n99.58\n98.24\n98.09\n99.61\n99.70\n98.89\n99.54\n96.32\n6\n99.99\n99.95\n99.12\n99.79\n100.00\n99.62\n100.00\n98.96\n7\n99.93\n98.71\n97.14\n97.94\n99.88\n98.97\n99.86\n98.42\n8\n67.88\n71.43\n59.51\n66.83\n90.54\n86.00\n79.90\n39.73\n9\n99.81\n95.51\n94.87\n90.65\n99.98\n98.15\n99.75\n96.34\n10\n96.54\n22.92\n96.97\n96.21\n97.77\n98.55\n97.29\n84.35\n11\n99.75\n76.67\n99.28\n99.25\n99.82\n99.39\n99.70\n92.76\n12\n100.00\n64.12\n99.39\n98.01\n99.99\n99.84\n99.99\n96.97\n13\n99.87\n99.98\n98.74\n99.34\n99.98\n99.38\n99.75\n97.48\n14\n98.66\n94.73\n98.62\n99.72\n99.91\n99.44\n99.67\n93.52\n15\n78.73\n63.65\n83.03\n70.16\n91.31\n86.77\n91.86\n69.09\n16\n99.27\n79.70\n96.65\n99.26\n99.26\n98.69\n99.10\n93.21\nAA\n96.21\n82.41\n94.43\n94.35\n98.57\n97.37\n97.83\n89.38\nOA\n91.56\n76.61\n88.67\n90.25\n96.89\n94.41\n96.28\n81.95\n26\nTable 7: KSC. Classiﬁcation accuracy obtained by S-DMM [121], 3DCAE [40], SSDL [37],\nTwoCnn [122], 3DVSCNN [123], SSLstm [75], CNN HSI [28] and SAE LR [34] on KSC. The\nbest accuracies are marked in bold. The ”size” in the ﬁrst line denotes the sample size per\ncategory.\nsize\nclasses\nS-DMM\n3DCAE\nSSDL\nTwoCnn\n3DVSCNN\nSSLstm\nCNN HSI\nSAE LR\n10\n1\n93.49\n35.46\n79.21\n67.11\n95.33\n73.58\n92.17\n83.95\n2\n89.74\n49.40\n67.68\n58.37\n40.39\n68.45\n81.67\n69.01\n3\n95.16\n40.41\n76.87\n77.20\n75.41\n81.59\n86.91\n50.61\n4\n58.72\n5.54\n70.33\n75.12\n35.87\n76.16\n60.83\n20.21\n5\n87.95\n33.38\n81.26\n88.08\n47.42\n87.22\n64.37\n23.11\n6\n93.42\n51.05\n79.18\n66.44\n64.29\n76.71\n66.16\n45.39\n7\n98.63\n16.32\n95.26\n92.74\n57.79\n96.42\n96.00\n63.58\n8\n97.93\n46.44\n72.42\n61.92\n71.88\n52.95\n85.77\n58.05\n9\n94.88\n86.25\n87.00\n92.31\n79.00\n90.65\n91.06\n76.24\n10\n98.12\n8.76\n72.59\n86.27\n56.57\n89.04\n85.13\n63.12\n11\n97.51\n76.21\n88.68\n78.17\n86.99\n89.32\n95.60\n89.98\n12\n93.69\n8.54\n83.65\n78.09\n60.79\n83.96\n89.66\n69.59\n13\n100.00\n46.95\n99.98\n100.00\n84.92\n100.00\n99.95\n97.90\nAA\n92.25\n38.82\n81.09\n78.60\n65.90\n82.00\n84.25\n62.36\nOA\n94.48\n49.73\n83.71\n82.29\n77.40\n83.07\n91.13\n72.68\n50\n1\n97.99\n22.53\n96.12\n72.95\n98.45\n96.77\n94.40\n88.21\n2\n98.24\n30.98\n94.56\n94.04\n39.90\n98.19\n91.50\n78.50\n3\n98.69\n45.10\n96.55\n90.10\n99.13\n99.47\n94.47\n83.06\n4\n78.22\n3.86\n93.51\n92.33\n74.01\n98.32\n76.49\n43.07\n5\n92.16\n40.54\n96.94\n97.12\n64.32\n99.55\n87.03\n53.33\n6\n98.49\n62.07\n96.70\n93.80\n77.21\n99.72\n70.89\n51.90\n7\n98.36\n18.00\n99.64\n97.82\n20.36\n100.00\n98.00\n84.73\n8\n99.21\n43.04\n91.92\n90.60\n96.25\n97.40\n93.86\n77.77\n9\n99.96\n89.77\n98.57\n89.55\n63.91\n98.83\n98.77\n86.47\n10\n99.92\n12.12\n93.70\n95.56\n54.72\n99.52\n91.67\n85.28\n11\n98.62\n80.38\n97.86\n98.40\n90.95\n99.11\n87.75\n96.56\n12\n99.07\n19.85\n94.99\n95.01\n87.37\n99.67\n89.54\n82.19\n13\n100.00\n91.24\n100.00\n90.00\n96.77\n99.46\n98.95\n99.44\nAA\n96.84\n43.04\n96.24\n92.10\n74.10\n98.92\n90.25\n77.73\nOA\n98.68\n54.01\n96.88\n96.61\n96.03\n98.72\n97.39\n84.93\n100\n1\n98.17\n19.03\n97.41\n96.51\n98.94\n99.74\n93.93\n89.77\n2\n98.74\n34.13\n98.60\n99.58\n56.50\n99.79\n89.93\n80.77\n3\n99.55\n57.18\n96.67\n99.42\n99.81\n99.23\n98.33\n82.88\n4\n88.29\n1.38\n97.96\n98.68\n88.29\n99.14\n85.86\n53.95\n5\n93.11\n52.46\n99.51\n100.00\n76.23\n100.00\n93.77\n58.52\n6\n99.61\n59.77\n98.68\n97.36\n80.62\n99.53\n74.96\n58.22\n7\n100.00\n8.00\n100.00\n100.00\n32.00\n100.00\n98.00\n86.00\n8\n99.79\n51.81\n95.53\n98.07\n98.91\n99.40\n97.37\n83.96\n9\n99.74\n87.40\n98.74\n98.74\n63.93\n99.12\n99.76\n91.95\n10\n100.00\n13.16\n98.22\n99.61\n72.47\n100.00\n97.70\n91.28\n11\n99.91\n83.76\n99.06\n99.97\n94.42\n99.81\n99.84\n97.81\n12\n99.33\n24.94\n97.99\n99.03\n94.32\n99.80\n95.31\n85.73\n13\n100.00\n90.07\n99.96\n99.94\n97.62\n99.94\n99.85\n99.58\nAA\n98.17\n44.85\n98.33\n98.99\n81.08\n99.65\n94.20\n81.57\nOA\n98.96\n59.63\n98.75\n99.15\n98.55\n99.68\n98.05\n89.15\n27\n\u0007\u0005\n\t\u0005\n\u000b\u0005\n\r\u0005\n\u0006\u0005\u0005\n\u001a\u001f$'#!\u0000\u001a\"/!\n\u000b\u0005\n\f\n\u000e\u0005\n\u000f  +(\u001f .\u0002\u0001\u0003\n\u0018\u001f,\"\u001f\u001c\n\u001a\u0004\u0011\u0016\u0016\n\b\u0011\u0010\u000f\u0012\n\u001a\u001a\u0011\u0015\n\u001b-&\u0010%%\n\b\u0011\u001d\u001a\u0010\u0017\u0017\n\u001a\u001a\u0015)*$\n\u0010\u0017\u0017\u001e\u0013\u001a\u0014\n\u001a\u000f\u0012\u001e\u0015\u0019\n(a)\n\u0007\u0005\n\t\u0005\n\u000b\u0005\n\r\u0005\n\u0006\u0005\u0005\n\u0019\u001d\"%!\u001f\u0000\u0019 ,\u001f\n\f\n\u000e\u0005\n\u000f\u001e\u001e)&\u001d\u001e+\u0002\u0001\u0003\n\u0019\u001d! #\u001d'\n\u0019\u0004\u0011\u0016\u0016\n\b\u0011\u0010\u000f\u0012\n\u0019\u0019\u0011\u0015\n\u001a*$\u0010##\n\b\u0011\u001b\u0019\u0010\u0017\u0017\n\u0019\u0019\u0015'(\"\n\u0010\u0017\u0017\u001c\u0013\u0019\u0014\n\u0019\u000f\u0012\u001c\u0015\u0018\n(b)\n\u0007\u0005\n\t\u0005\n\u000b\u0005\n\r\u0005\n\u0006\u0005\u0005\n\u001a\u001e#&\" \u0000\u001a!- \n\u000b\u0005\n\f\n\u000e\u0005\n\u000f\u001f\u001f*'\u001e\u001f,\u0002\u0001\u0003\n\u0015\u001a\u0010\n\u001a\u0004\u0011\u0017\u0017\n\b\u0011\u0010\u000f\u0012\n\u001a\u001a\u0011\u0016\n\u001b+%\u0010$$\n\b\u0011\u001c\u001a\u0010\u0018\u0018\n\u001a\u001a\u0016()#\n\u0010\u0018\u0018\u001d\u0013\u001a\u0014\n\u001a\u000f\u0012\u001d\u0016\u0019\n(c)\nFigure 10: Change in accuracy over the number of samples for each category. (a) PaviaU. (b)\nSalinas. (c) KSC.\n28\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 11: Classiﬁcation maps on the PaviaU data set (10 samples per class). (a) Original.\n(b) S-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI.\n(i) SAE LR.\n29\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 12: Classiﬁcation maps on the PaviaU data set (50 samples per class). (a) Original.\n(b) S-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI.\n(i) SAE LR.\n30\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 13: Classiﬁcation maps on the PaviaU data set (100 samples per class). (a) Original.\n(b) S-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI.\n(i) SAE LR.\n31\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 14: Classiﬁcation maps on the Salinas data set (10 samples per class). (a) Original.\n(b) S-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI.\n(i) SAE LR.\n32\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 15: Classiﬁcation maps on the Salinas (50 samples per class). (a) Original. (b) S-\nDMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI. (i)\nSAE LR.\n33\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 16: Classiﬁcation maps on the Salinas data set (100 samples per class). (a) Original.\n(b) S-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI.\n(i) SAE LR.\n34\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 17: Classiﬁcation maps on the KSC data set (10 samples per class). (a) Original. (b)\nS-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI. (i)\nSAE LR.\n35\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 18: Classiﬁcation maps on the KSC data set (50 samples per class). (a) Original. (b)\nS-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI. (i)\nSAE LR.\n36\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\nFigure 19: Classiﬁcation maps on the KSC data set (100 samples per class). (a) Original. (b)\nS-DMM. (c) 3DCAE. (d) SSDL. (e) TwoCnn. (f) 3DVSCNN. (g) SSLstm. (h) CNN HSI. (i)\nSAE LR.\n37\n4.4. Model parameters\nTo further explore the reasons why the model has achieved diﬀerent results\non the benchmark data set, we also counted the number of trainable parameters\nof each framework (including the decoder module) on diﬀerent data sets, which\nare shown in Table 8. On all data sets, the model with the least number of\ntraining parameters is the SAE LR, the second is the CNN HSI and the most is\nthe TwoCnn. SAE LR is a lightweight architecture in all models for the simple\nlinear layer, but its performance is poor. Diﬀerent from other 2D convolution\napproaches in HSI, CNN HSI solely uses a 1 × 1 kernel to process an image.\nMoreover, it uses a 1 × 1 convolution layer to serve as a classiﬁer instead of the\nlinear layer, which greatly reduces the number of trainable parameters. The next\nis the S-DMM. This also explains why S-DMM and CNN HSI are less aﬀected\nby augmentation in sample size but very eﬀective on few samples. Additionally,\nthe problem of overﬁtting is of little concern in these approaches.\nStacking\nthe spectral and spatial feature to generate the ﬁnal fused feature is the main\nreason for the large number of parameters of TwoCnn.\nHowever, regardless\nof its potentially millions of trainable parameters, it can work well on limited\nsamples, beneﬁting from transfer learning, which decreases trainable parameters\nand achieves good performance on all target data sets. Next, the models with\nthe most parameters are successively 3DCAE and SSLstm. 3DCAE’s trainable\nparameters are at most eight times those of SSDL, which contains not only\na 1D autoencoder in the spectral branch but also a spatial branch based on\na 2D convolutional network, but 3DCAE is still worse than SSDL. Although\n3D convolutional and pooling modules can greatly avoid the problem of data\nstructure information loss caused by the ﬂattening operation, the complexity\nof the 3D structure and the symmetric structure of the autoencoder increase\nthe number of model parameters, which make it easy to overﬁt the model.\n3DVSCNN also uses a 3D convolutional module and is better than 3DCAE,\nwhich ﬁrst reduces the number of redundant bands by PCA. That may also\nbe applied to 3DCAE to decrease the number of model parameters and make\ngood use of characteristics of 3D convolution, extracting spectral and spatial\ninformation simultaneously. The main contribution of the parameter of SSLstm\ncomes from the spatial branch. Although the gate structure of LSTM improves\nthe model’s capabilities of long and short memory, it increases the complexity\nof the model. When the number of hidden layer units increases, the model’s\nparameters will also skyrocket greatly. Perhaps it is the coupling between the\nspectral features and recurrent network that make performance of SSLstm not as\nbad as that of 3DCAE on all data sets, which has a similar number of parameters\nand even achieved superior results on KSC. Moreover, there are no methods that\nwere adopted for solving the problem of few samples. This ﬁnding also shows\nthat supervised learning is better than unsupervised learning in some tasks.\n4.5. The speed of model convergence\nIn addition, we compare the convergence speed of the model according to\nthe changes in training loss of each model in the ﬁrst 200 epochs on each group\n38\nTable 8: The number of trainable parameters\nPaviaU\nSalinas\nKSC\nS-DMM\n33921\n40385\n38593\n3DCAE\n256563\n447315\n425139\nSSDL\n35650\n48718\n44967\nTwoCnn\n1379399\n1542206\n1501003\n3DVSCNN\n42209\n42776\n227613\nSSLstm\n367506\n370208\n401818\nCNN HSI\n22153\n33536\n31753\nSAE LR\n21426\n5969\n5496\nof experiments (see Figure 20∼22). Because the autoencoder and classiﬁer of\n3DCAE are be trained separately, and all data are used during training the\nautoencoder, it is not comparable to other models. Therefore, it is not be listed\nhere. On all data sets, S-DMM has the fastest convergence speed. After ap-\nproximately 3 epochs, the training loss tends to become stable given its fewer\nparameters.\nAlthough CNN HSI has a similar performance to S-DMM and\nfewer parameters, the learning curve of CNN HSI’s convergence rate is slower\nthan that of S-DMM and is sometimes accompanied by turbulence. The second\nplace regarding performance is held by TwoCnn, which is mainly due to transfer\nlearning to better position the initial parameters, and it actually has fewer pa-\nrameters requiring training. Thus, it just needs a few epochs to ﬁne-tune on the\ntarget data set. Moreover, the training curve of most models stabilizes after 100\nepochs. The training loss of the SSLstm has severe oscillations in all data sets.\nThis is especially noted in the SeLstm, where the loss sometimes has diﬃculty\nin decreasing. When the sequence is very long, the challenge might be that the\nrecurrent neural network is more susceptible to a vanishing or exploding gradi-\nent. Moreover, the pixels of the hyperspectral image usually contain hundreds\nof bands, which is the reason why the training loss has diﬃculty decreasing or\noscillations occur in SeLstm. In the spatial branch, it does not have this serious\ncondition because the length of the spatial sequence depending on patch size is\nshorter than spectral sequences. During training, the LSTM-based model spent\na considerable amount of time because it cannot train in parallel.\n5. Conclusions\nIn this paper, we introduce the current research diﬃculties, namely, few\nsamples, in the ﬁeld of hyperspectral image classiﬁcation and discuss popular\nlearning frameworks. Furthermore, we also introduce several popular learning\nalgorithms to solve the small-sample problem, such as autoencoders, few-shot\nlearning, transfer learning, activate learning, and data augmentation. Accord-\ning to the above methods, we select some representative models to conduct\nexperiments on hyperspectral benchmark data sets. We developed three diﬀer-\nent experiments to explore the performance of the models on small-sample data\n39\n\u0002\n\u0004\u0006\n\u0006\u0002\n\u0007\u0006\n\u0003\u0002\u0002\n\u0003\u0004\u0006\n\u0003\u0006\u0002\n\u0003\u0007\u0006\n\u0004\u0002\u0002\n\u000b\u0012\u0011\t\f\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0006\n\u0002\u0001\u0006\u0002\n\u0002\u0001\u0007\u0006\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0006\n\u0003\u0001\u0006\u0002\n\u0003\u0001\u0007\u0006\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0006\n\u0004\u0001\u0006\u0002\n\u0004\u0001\u0007\u0006\n\u0005\u0001\u0002\u0002\n\u0005\u0001\u0004\u0006\n\u0005\u0001\u0006\u0002\n\u0005\u0001\u0007\u0006\n\u000e\u001c\u001d\u001d\n\u0005\n\u0016\u0014\t\u0010\u0010\n\t\u0010\u0010\u0017\f\u0014\r\n\u0014\u0000\n\u000f\u000f\n\u0014\b\u000b\u0017\u000e\u0013\n\u0014\u0018\u000e\u001d\u001e\u001a\n\u0014\u0019\u000e\u001d\u001e\u001a\n\u0014\u0014\n\u000e\n\u0015\u001f\u001c\t\u001b\u001b\n(a)\n\u0002\n\u0004\u0006\n\u0006\u0002\n\u0007\u0006\n\u0003\u0002\u0002\n\u0003\u0004\u0006\n\u0003\u0006\u0002\n\u0003\u0007\u0006\n\u0004\u0002\u0002\n\u000b\u0012\u0011\t\f\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0006\n\u0002\u0001\u0006\u0002\n\u0002\u0001\u0007\u0006\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0006\n\u0003\u0001\u0006\u0002\n\u0003\u0001\u0007\u0006\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0006\n\u0004\u0001\u0006\u0002\n\u0004\u0001\u0007\u0006\n\u000e\u001c\u001d\u001d\n\u0005\n\u0016\u0014\t\u0010\u0010\n\t\u0010\u0010\u0017\f\u0014\r\n\u0014\u0000\n\u000f\u000f\n\u0014\b\u000b\u0017\u000e\u0013\n\u0014\u0018\u000e\u001d\u001e\u001a\n\u0014\u0019\u000e\u001d\u001e\u001a\n\u0014\u0014\n\u000e\n\u0015\u001f\u001c\t\u001b\u001b\n(b)\n\u0002\n\u0004\u0006\n\u0006\u0002\n\u0007\u0006\n\u0003\u0002\u0002\n\u0003\u0004\u0006\n\u0003\u0006\u0002\n\u0003\u0007\u0006\n\u0004\u0002\u0002\n\u000b\u0012\u0011\t\f\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0006\n\u0002\u0001\u0006\u0002\n\u0002\u0001\u0007\u0006\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0006\n\u0003\u0001\u0006\u0002\n\u0003\u0001\u0007\u0006\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0006\n\u000e\u001c\u001d\u001d\n\u0005\n\u0016\u0014\t\u0010\u0010\n\t\u0010\u0010\u0017\f\u0014\r\n\u0014\u0000\n\u000f\u000f\n\u0014\b\u000b\u0017\u000e\u0013\n\u0014\u0018\u000e\u001d\u001e\u001a\n\u0014\u0019\u000e\u001d\u001e\u001a\n\u0014\u0014\n\u000e\n\u0015\u001f\u001c\t\u001b\u001b\n(c)\nFigure 20: Training Loss on the PaviaU data set. (a) 10 samples per class. (b) 50 samples\nper class. (c) 100 samples per class.\n40\n\u0002\n\u0004\u0007\n\u0007\u0002\n\t\u0007\n\u0003\u0002\u0002\n\u0003\u0004\u0007\n\u0003\u0007\u0002\n\u0003\t\u0007\n\u0004\u0002\u0002\n\u000f\u0016\u0015\r\u0010\n$\u0002\u0001\u0007\n\u0002\u0001\u0002\n\u0002\u0001\u0007\n\u0003\u0001\u0002\n\u0003\u0001\u0007\n\u0004\u0001\u0002\n\u0004\u0001\u0007\n\u0005\u0001\u0002\n\u0005\u0001\u0007\n\u0006\u0001\u0002\n\u0006\u0001\u0007\n\u0007\u0001\u0002\n\u0007\u0001\u0007\n\b\u0001\u0002\n\b\u0001\u0007\n\t\u0001\u0002\n\t\u0001\u0007\n\n\u0001\u0002\n\n\u0001\u0007\n\u000b\u0001\u0002\n\u000b\u0001\u0007\n\u0003\u0002\u0001\u0002\n\u0003\u0002\u0001\u0007\n\u0003\u0003\u0001\u0002\n\u0003\u0003\u0001\u0007\n\u0003\u0004\u0001\u0002\n\u0003\u0004\u0001\u0007\n\u0003\u0005\u0001\u0002\n\u0003\u0005\u0001\u0007\n\u0012 !!\n\u0005\u000e\u001a\u0018\r\u0014\u0014\n\r\u0014\u0014\u001b\u0010\u0018\u0011\n\u0018\u0000\u000e\u0013\u0013\n\u0018\f\u000f\u001b\u0012\u0017\n\u0018\u001c\u0012!\"\u001e\n\u0018\u001d\u0012!\"\u001e\n\u0018\u0018\u000e\u0012\n\u0019# \r\u001f\u001f\n(a)\n\u0002\n\u0004\u0007\n\u0007\u0002\n\b\u0007\n\u0003\u0002\u0002\n\u0003\u0004\u0007\n\u0003\u0007\u0002\n\u0003\b\u0007\n\u0004\u0002\u0002\n\f\u0013\u0012\n\r\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0007\n\u0002\u0001\u0007\u0002\n\u0002\u0001\b\u0007\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0007\n\u0003\u0001\u0007\u0002\n\u0003\u0001\b\u0007\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0007\n\u0004\u0001\u0007\u0002\n\u0004\u0001\b\u0007\n\u0005\u0001\u0002\u0002\n\u0005\u0001\u0004\u0007\n\u0005\u0001\u0007\u0002\n\u0005\u0001\b\u0007\n\u0006\u0001\u0002\u0002\n\u000f\u001d\u001e\u001e\n\u0005\u000b\u0017\u0015\n\u0011\u0011\n\n\u0011\u0011\u0018\r\u0015\u000e\n\u0015\u0000\u000b\u0010\u0010\n\u0015\t\f\u0018\u000f\u0014\n\u0015\u0019\u000f\u001e\u001f\u001b\n\u0015\u001a\u000f\u001e\u001f\u001b\n\u0015\u0015\u000b\u000f\n\u0016 \u001d\n\u001c\u001c\n(b)\n\u0002\n\u0004\u0006\n\u0006\u0002\n\u0007\u0006\n\u0003\u0002\u0002\n\u0003\u0004\u0006\n\u0003\u0006\u0002\n\u0003\u0007\u0006\n\u0004\u0002\u0002\n\u000b\u0012\u0011\t\f\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0006\n\u0002\u0001\u0006\u0002\n\u0002\u0001\u0007\u0006\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0006\n\u0003\u0001\u0006\u0002\n\u0003\u0001\u0007\u0006\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0006\n\u0004\u0001\u0006\u0002\n\u0004\u0001\u0007\u0006\n\u0005\u0001\u0002\u0002\n\u000e\u001c\u001d\u001d\n\u0005\n\u0016\u0014\t\u0010\u0010\n\t\u0010\u0010\u0017\f\u0014\r\n\u0014\u0000\n\u000f\u000f\n\u0014\b\u000b\u0017\u000e\u0013\n\u0014\u0018\u000e\u001d\u001e\u001a\n\u0014\u0019\u000e\u001d\u001e\u001a\n\u0014\u0014\n\u000e\n\u0015\u001f\u001c\t\u001b\u001b\n(c)\nFigure 21: Training Loss on the Salinas data set.(a) 10 samples per class. (b) 50 samples per\nclass. (c) 100 samples per class.\n41\n\u0002\n\u0004\u0007\n\u0007\u0002\n\b\u0007\n\u0003\u0002\u0002\n\u0003\u0004\u0007\n\u0003\u0007\u0002\n\u0003\b\u0007\n\u0004\u0002\u0002\n\f\u0013\u0012\n\r\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0007\n\u0002\u0001\u0007\u0002\n\u0002\u0001\b\u0007\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0007\n\u0003\u0001\u0007\u0002\n\u0003\u0001\b\u0007\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0007\n\u0004\u0001\u0007\u0002\n\u0004\u0001\b\u0007\n\u0005\u0001\u0002\u0002\n\u0005\u0001\u0004\u0007\n\u0005\u0001\u0007\u0002\n\u0005\u0001\b\u0007\n\u0006\u0001\u0002\u0002\n\u0006\u0001\u0004\u0007\n\u000f\u001d\u001e\u001e\n\u0005\u000b\u0017\u0015\n\u0011\u0011\n\n\u0011\u0011\u0018\r\u0015\u000e\n\u0015\u0000\u000b\u0010\u0010\n\u0015\t\f\u0018\u000f\u0014\n\u0015\u0019\u000f\u001e\u001f\u001b\n\u0015\u001a\u000f\u001e\u001f\u001b\n\u0015\u0015\u000b\u000f\n\u0016 \u001d\n\u001c\u001c\n(a)\n\u0002\n\u0004\u0007\n\u0007\u0002\n\b\u0007\n\u0003\u0002\u0002\n\u0003\u0004\u0007\n\u0003\u0007\u0002\n\u0003\b\u0007\n\u0004\u0002\u0002\n\f\u0013\u0012\n\r\n!\u0002\u0001\u0004\u0007\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0007\n\u0002\u0001\u0007\u0002\n\u0002\u0001\b\u0007\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0007\n\u0003\u0001\u0007\u0002\n\u0003\u0001\b\u0007\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0007\n\u0004\u0001\u0007\u0002\n\u0004\u0001\b\u0007\n\u0005\u0001\u0002\u0002\n\u0005\u0001\u0004\u0007\n\u0005\u0001\u0007\u0002\n\u0005\u0001\b\u0007\n\u0006\u0001\u0002\u0002\n\u0006\u0001\u0004\u0007\n\u0006\u0001\u0007\u0002\n\u0006\u0001\b\u0007\n\u0007\u0001\u0002\u0002\n\u0007\u0001\u0004\u0007\n\u0007\u0001\u0007\u0002\n\u000f\u001d\u001e\u001e\n\u0005\u000b\u0017\u0015\n\u0011\u0011\n\n\u0011\u0011\u0018\r\u0015\u000e\n\u0015\u0000\u000b\u0010\u0010\n\u0015\t\f\u0018\u000f\u0014\n\u0015\u0019\u000f\u001e\u001f\u001b\n\u0015\u001a\u000f\u001e\u001f\u001b\n\u0015\u0015\u000b\u000f\n\u0016 \u001d\n\u001c\u001c\n(b)\n\u0002\n\u0004\u0007\n\u0007\u0002\n\b\u0007\n\u0003\u0002\u0002\n\u0003\u0004\u0007\n\u0003\u0007\u0002\n\u0003\b\u0007\n\u0004\u0002\u0002\n\f\u0013\u0012\n\r\n!\u0002\u0001\u0004\u0007\n\u0002\u0001\u0002\u0002\n\u0002\u0001\u0004\u0007\n\u0002\u0001\u0007\u0002\n\u0002\u0001\b\u0007\n\u0003\u0001\u0002\u0002\n\u0003\u0001\u0004\u0007\n\u0003\u0001\u0007\u0002\n\u0003\u0001\b\u0007\n\u0004\u0001\u0002\u0002\n\u0004\u0001\u0004\u0007\n\u0004\u0001\u0007\u0002\n\u0004\u0001\b\u0007\n\u0005\u0001\u0002\u0002\n\u0005\u0001\u0004\u0007\n\u0005\u0001\u0007\u0002\n\u0005\u0001\b\u0007\n\u0006\u0001\u0002\u0002\n\u0006\u0001\u0004\u0007\n\u0006\u0001\u0007\u0002\n\u0006\u0001\b\u0007\n\u0007\u0001\u0002\u0002\n\u0007\u0001\u0004\u0007\n\u000f\u001d\u001e\u001e\n\u0005\u000b\u0017\u0015\n\u0011\u0011\n\n\u0011\u0011\u0018\r\u0015\u000e\n\u0015\u0000\u000b\u0010\u0010\n\u0015\t\f\u0018\u000f\u0014\n\u0015\u0019\u000f\u001e\u001f\u001b\n\u0015\u001a\u000f\u001e\u001f\u001b\n\u0015\u0015\u000b\u000f\n\u0016 \u001d\n\u001c\u001c\n(c)\nFigure 22: Training Loss on the KSC data set. (a) 10 samples per class. (b) 50 samples per\nclass. (c) 100 samples per class.\n42\nsets and documented their changes with increasing sample size, ﬁnally evaluating\ntheir eﬀectiveness and robustness through AA and OA. Then, we also compared\nthe number of parameters and convergence speeds of various models to further\nanalyze their diﬀerences. Ultimately, we also highlight several possible future\ndirections of hyperspectral image classiﬁcation on small samples:\n• Autoencoders, including linear autoencoders and 3D convolutional au-\ntoencoders, have been widely explored and applied to solve the sample\nproblem in HSI. Nevertheless, their performance does not approach ex-\ncellence.\nThe future development trend should be focused on few-shot\nlearning, transfer learning, and active learning.\n• We can fuse some learning paradigms to make good use of the advantages\nof each approach. For example, regarding the fusion of transfer learning\nand active learning, such an approach can select the valuable samples on\nthe source data set and transfer the model to the target data set to avoid\nthe imbalance of the class sample size.\n• According to the experimental results, the RNN is also suitable for hy-\nperspectral image classiﬁcation. However, there is little work focused on\ncombining the learning paradigms with RNN. Recently, the transformer,\nas an alternative to the RNN that is capable of processing in parallel, has\nbeen introduced into the computer vision domain and has achieved good\nperformance on some tasks such as object detection. Therefore, we can\nalso employ this method in hyperspectral image classiﬁcation and combine\nit with some learning paradigms.\n• Graph convolution network has been growing more and more interested\nin hyperspectral image classiﬁcation. Fully connected network, convolu-\ntion network, and recurrent network are just suitable for processing the\neuclidean data and do not solve with the non-euclidean data directly. And\nimage can be regarded as a special case of the euclidean-data. Thus, there\nare many researches [124, 125, 126] utilizing graph convolution networks\nto classify HSI.\n• The reason for requiring a large amount of label samples is the tremendous\ntrainable parameters of the deep learning model. There are many methods\nproposed, such as group convolution [127], to light the weight of a deep\nneural network. So, how to construct a light-weight model further is also\na future direction.\nAlthough few label classiﬁcation can save much time and labor force to col-\nlect and label diverse samples, the models are easy to suﬀer from over-ﬁt and\ngaining a weak generalization. Thus, how to avoid the over-ﬁtting and improve\nmodel’s generalization is the huge challenge of HSI few label classiﬁcation in the\napplication potential.\n43\nAcknowledgments\nThe work is partly supported by the National Natural Science Foundation\nof China (Grant No. 61976144).\nReferences\n[1] M. Teke, H. Deveci, O. Halilo˘glu, S. G¨urb¨uz, U. Sakarya, A short sur-\nvey of hyperspectral remote sensing applications in agriculture, in: 2013\n6th International Conference on Recent Advances in Space Technologies\n(RAST), IEEE, 2013, pp. 171–176.\n[2] I. Strachan, E. Pattey, J. Boisvert, Impact of nitrogen and environmental\nconditions on corn as detected by hyperspectral reﬂectance, Remote Sens.\nEnviron. 80 (2) (2002) 213–224.\n[3] A. Bannari, A. Pacheco, K. Staenz, H. McNairn, K. Omari, Estimating\nand mapping crop residues cover on agricultural lands using hyperspectral\nand ikonos data, Remote Sens Environ 104 (4) (2006) 447–459.\n[4] C. Sabine, M. Robert, S. Thomas, R. Manuel, E. Paula, P. Marta, P. Ali-\ncia, Potential of hyperspectral imagery for the spatial assessment of soil\nerosion stages in agricultural semi-arid spain at diﬀerent scales, in: 2014\nIEEE Geoscience and Remote Sensing Symposium, IEEE, 2014, pp. 2918–\n2921.\n[5] P. Kuﬂik, S. Rotman, Band selection for gas detection in hyperspectral\nimages, in: 2012 IEEE 27th Convention of Electrical and Electronics En-\ngineers in Israel, 2012, pp. 1–4. doi:10.1109/EEEI.2012.6376973.\n[6] S. Foudan, K. Menas, E. Tarek, G. Richard, Y. Ruixin, Hyperspectral im-\nage analysis for oil spill detection, in: Summaries of NASA/JPL Airborne\nEarth Science Workshop, Pasadena, CA, 2001, pp. 5–9.\n[7] A. Mohamad, Sea water chlorophyll-a estimation using hyperspectral im-\nages and supervised artiﬁcial neural network, Ecol Inf 24 (2014) 60–68.\ndoi:10.1016/j.ecoinf.2014.07.004.\n[8] J. Sylvain, G. Mireille, A novel maximum likelihood based method for\nmapping depth and water quality from hyperspectral remote-sensing data,\nRemote Sens Environ 147 (2014) 121–132. doi:10.1016/j.rse.2014.01.\n026.\n[9] C. J¨anicke, A. Okujeni, S. Cooper, M. Clark, P. Hostert, S. van der Linden,\nBrightness gradient-corrected hyperspectral image mosaics for fractional\nvegetation cover mapping in northern california, Remote Sensing Letters\n11 (1) (2020) 1–10.\n44\n[10] J. Li, Y. Pang, Z. Li, W. Jia, Tree species classiﬁcation of airborne hy-\nperspectral image in cloud shadow area, in: International Symposium of\nSpace Optical Instrument and Application, Springer, 2018, pp. 389–398.\n[11] Z. Du, M. Jeong, S. Kong, Band selection of hyperspectral images for\nautomatic detection of poultry skin tumors, IEEE Transactions on Au-\ntomation Science and Engineering 4 (3) (2007) 332–339. doi:10.1109/\nTASE.2006.888048.\n[12] S. Li, W. Song, L. Fang, Y. Chen, J. Benediktsson, Deep learning for\nhyperspectral image classiﬁcation: An overview, IEEE Transactions on\nGeoscience and Remote Sensing PP (99) (2019) 1–20.\n[13] A. Plaza, J. Plaza, G. Martin, Incorporation of spatial constraints into\nspectral mixture analysis of remotely sensed hyperspectral data, Ma-\nchine Learning for Signal Processing .mlsp .ieee International Workshop\non (2009) 1 – 6.\n[14] F. Melgani, L. Bruzzone, Classiﬁcation of hyperspectral remote sensing\nimages with support vector machines, IEEE Transactions on Geoence\nand Remote Sensing 42 (8) (2004) 1778–1790. doi:10.1109/TGRS.2004.\n831865.\n[15] Y. Zhong, L. Zhang, An adaptive artiﬁcial immune network for supervised\nclassiﬁcation of multi-/hyperspectral remote sensing imagery, IEEE Trans.\nGeosci. Remote Sens. 50 (3) (2011) 894–909.\n[16] J. Li, J. Bioucas-Dias, A. Plaza, Semisupervised hyperspectral image clas-\nsiﬁcation using soft sparse multinomial logistic regression, IEEE Geosci.\nRemote Sens. Lett. 10 (2) (2012) 318–322.\n[17] G. Licciardi, P. Marpu, J. Chanussot, J. Benediktsson, Linear versus non-\nlinear pca for the classiﬁcation of hyperspectral data based on the ex-\ntended morphological proﬁles, IEEE Geosci. Remote Sens. Lett. 9 (3)\n(2011) 447–451.\n[18] A. Villa, J. Chanussot, C. Jutten, J. Benediktsson, S. Moussaoui, On the\nuse of ICA for hyperspectral image analysis, in: Proc. Geoscience and\nRemote Sensing Symp.,2009 IEEE Int.,IGARSS 2009, Vol. 4, 2009, pp.\nIV–97. doi:10.1109/IGARSS.2009.5417363.\n[19] C. Zhang, Y. Zheng, Hyperspectral remote sensing image classiﬁcation\nbased on combined SVM and LDA, in: SPIE Asia Paciﬁc Remote Sensing,\n2014, p. 92632P.\n[20] L. He, J. Li, A. Plaza, Y. Li, Discriminative low-rank Gabor ﬁltering for\nspectral-spatial hyperspectral image classiﬁcation, IEEE Transactions on\nGeoence and Remote Sensing PP (99) (2016) 1–15. doi:10.1109/TGRS.\n2016.2623742.\n45\n[21] M. D. Mura, J. A. Benediktsson, B. Waske, L. Bruzzone, Extended proﬁles\nwith morphological attribute ﬁlters for the analysis of hyperspectral data,\nInt. J. Remote Sens. 31 (22) (2010) 5975–5991.\n[22] N. Falco, J. Atli Benediktsson, L. Bruzzone, Spectral and spatial classiﬁ-\ncation of hyperspectral images based on ICA and reduced morphological\nattribute proﬁles, IEEE Transactions on Geoscience and Remote Sensing\n53 (11) (2015) 6223–6240.\n[23] M. Dalla Mura, A. Villa, J. Atli Benediktsson, J. Chanussot, L. Bruzzone,\nClassiﬁcation of hyperspectral images by using extended morphological\nattribute proﬁles and independent component analysis, IEEE Geoscience\nand Remote Sensing Letters 8 (3) (2011) 542–546.\n[24] S. Jia, L. Shen, Q. Li, Gabor feature-based collaborative representation\nfor hyperspectral imagery classiﬁcation, IEEE Transactions on Geoscience\nand Remote Sensing 53 (2) (2015) 1118–1129.\n[25] Y. Qian, M. Ye, J. Zhou, Hyperspectral image classiﬁcation based on\nstructured sparse logistic regression and three-dimensional wavelet texture\nfeatures, IEEE Transactions on Geoscience and Remote Sensing 51 (4)\n(2013) 2276–2291.\n[26] W. Li, C. Chen, H. Su, Q. Du, Local binary patterns and extreme learn-\ning machine for hyperspectral imagery classiﬁcation, IEEE Trans Geosci\nRemote Sens 53 (7) (2015) 3681–3693.\n[27] P. Ghamisi, M. Dalla Mura, J. Atli Benediktsson, A survey on spectral–\nspatial classiﬁcation techniques based on attribute proﬁles, IEEE Trans-\nactions on Geoscience and Remote Sensing 53 (5) (2015) 2335–2353.\n[28] S. Yu, S. Jia, C. Xu, Convolutional neural networks for hyperspectral\nimage classiﬁcation, Neurocomputing 219 (2017) 88–98.\n[29] M. Paoletti, J. Haut, J. Plaza, A. Plaza, Deep learning classiﬁers for\nhyperspectral imaging: A review, ISPRS J. Photogramm. Remote Sens.\n158 (Dec.) (2019) 279–317.\n[30] G. Hinton, R. Salakhutdinov, Reducing the dimensionality of data with\nneural networks, science 313 (5786) (2006) 504–507.\n[31] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsuper-\nvised feature learning, Journal of Machine Learning Research 15 (2011)\n215–223.\n[32] P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol, Extracting and com-\nposing robust features with denoising autoencoders, in: International Con-\nference on Machine Learning, 2008, pp. 1096–1103.\n46\n[33] L. Windrim, R. Ramakrishnan, A. Melkumyan, R. Murphy, A. Chlin-\ngaryan, Unsupervised feature-learning for hyperspectral data with autoen-\ncoders, Remote Sensing 11 (7) (2019) 864.\n[34] Y. Chen, Z. Lin, X. Zhao, G. Wang, Y. Gu, Deep learning-based classiﬁca-\ntion of hyperspectral data, IEEE J Sel Topics Appl Earth Observ Remote\nSens 7 (6) (2014) 2094–2107.\n[35] A. Ghasem, S. Farhad, R. Peter, Spectral–spatial feature learning for hy-\nperspectral imagery classiﬁcation using deep stacked sparse autoencoder,\nJ. Appl. Remote Sens. 11 (4) (2017) 042604.\n[36] C. Xing, L. Ma, X. Yang, Stacked denoise autoencoder based feature\nextraction and classiﬁcation for hyperspectral images, Journal of Sensors\n2016 (2016).\n[37] J. Yue, S. Mao, M. Li, A deep learning framework for hyperspectral image\nclassiﬁcation using spatial pyramid pooling, Remote Sensing Letters 7 (9)\n(2016) 875–884.\n[38] S. Hao, W. Wang, Y. Ye, T. Nie, B. Lorenzo, Two-stream deep architec-\nture for hyperspectral image classiﬁcation, IEEE Trans. Geosci. Remote\nSens. 56 (4) (2017) 2349–2361.\n[39] X. Sun, F. Zhou, J. Dong, F. Gao, Q. Mu, X. Wang, Encoding spectral and\nspatial context information for hyperspectral image classiﬁcation, IEEE\nGeosci. Remote Sens. Lett. 14 (12) (2017) 2250–2254.\n[40] S. Mei, J. Ji, Y. Geng, Z. Zhang, X. Li, Q. Du, Unsupervised spatial–\nspectral feature learning by 3d convolutional autoencoder for hyperspec-\ntral classiﬁcation, IEEE Trans. Geosci. Remote Sens. 57 (9) (2019) 6808–\n6820.\n[41] C. Zhao, X. Wan, G. Zhao, B. Cui, W. Liu, B. Qi, Spectral-spatial classiﬁ-\ncation of hyperspectral imagery based on stacked sparse autoencoder and\nrandom forest, European journal of remote sensing 50 (1) (2017) 47–63.\n[42] X. Wan, C. Zhao, Y. Wang, W. Liu, Stacked sparse autoencoder in hy-\nperspectral data classiﬁcation using spectral-spatial, higher order statis-\ntics and multifractal spectrum features, Infrared Physics & Technology 86\n(2017) 77–89.\n[43] C. Wang, P. Zhang, Y. Zhang, L. Zhang, W. Wei, A multi-label hyperspec-\ntral image classiﬁcation method with deep learning features, in: Proceed-\nings of the International Conference on Internet Multimedia Computing\nand Service, 2016, pp. 127–131.\n[44] J. Li, B. Lorenzo, S. Liu, Deep feature representation for hyperspectral\nimage classiﬁcation, in: 2015 IEEE International Geoscience and Remote\nSensing Symposium (IGARSS), IEEE, 2015, pp. 4951–4954.\n47\n[45] M. Atif, L. Tao, Eﬃcient deep auto-encoder learning for the classiﬁcation\nof hyperspectral images, in: 2016 International Conference on Virtual\nReality and Visualization (ICVRV), IEEE, 2016, pp. 44–51.\n[46] Y. Liu, G. Cao, Q. Sun, S. Mel, Hyperspectral classiﬁcation via learnt\nfeatures, in: 2015 IEEE International Conference on Image Processing\n(ICIP), IEEE, 2015, pp. 2591–2595.\n[47] H. Lee, K. Heesung, Going deeper with contextual cnn for hyperspectral\nimage classiﬁcation, IEEE Trans. Image Process. 26 (10) (2017) 4843–\n4855.\n[48] J. Leng, T. Li, G. Bai, Q. Dong, H. Dong, Cube-cnn-svm: a novel hyper-\nspectral image classiﬁcation method, in: 2016 IEEE 28th International\nConference on Tools with Artiﬁcial Intelligence (ICTAI), IEEE, 2016, pp.\n1027–1034.\n[49] H. Zhang, Y. Li, Y. Zhang, Q. Shen, Spectral-spatial classiﬁcation of\nhyperspectral imagery using a dual-channel convolutional neural network,\nRemote Sensing Letters 8 (5) (2017) 438–447.\n[50] E. Aptoula, M. Ozdemir, B. Yanikoglu, Deep learning with attribute pro-\nﬁles for hyperspectral image classiﬁcation, IEEE Geosci. Remote Sens.\nLett. 13 (12) (2016) 1970–1974.\n[51] W. Zhao, S. Li, A. Li, B. Zhang, Y. Li, Hyperspectral images classiﬁca-\ntion with convolutional neural network and textural feature using limited\ntraining samples, Remote Sensing Letters 10 (5) (2019) 449–458.\n[52] C. Yu, M. Zhao, M. Song, Y. Wang, F. Li, R. Han, C. Chang, Hyper-\nspectral image classiﬁcation method based on cnn architecture embedding\nwith hashing semantic feature, IEEE J. Sel. Topics Appl. Earth Observ.\nRemote Sens. 12 (6) (2019) 1866–1881.\n[53] C. Qing, J. Ruan, X. Xu, J. Ren, J. Zabalza, Spatial-spectral classiﬁcation\nof hyperspectral images: a deep learning framework with markov random\nﬁelds based modelling, IET Image Proc. 13 (2) (2018) 235–245.\n[54] Z. Zhong, J. Li, Z. Luo, M. Chapman, Spectral-spatial residual network\nfor hyperspectral image classiﬁcation: A 3-d deep learning framework,\nIEEE Trans. Geosci. Remote Sens. 56 (2) (2017) 847–858.\n[55] B. Liu, X. Yu, P. Zhang, X. Tan, R. Wang, L. Zhi, Spectral–spatial clas-\nsiﬁcation of hyperspectral image using three-dimensional convolution net-\nwork, J. Appl. Remote Sens. 12 (1) (2018) 016005.\n[56] B. Fang, Y. Li, H. Zhang, J. Chan, Collaborative learning of lightweight\nconvolutional neural network and deep clustering for hyperspectral im-\nage semi-supervised classiﬁcation with limited training samples, ISPRS J\nPhotogramm Remote Sens 161 (2020) 164–178.\n48\n[57] L. Mou, P. Ghamisi, X. Zhu, Unsupervised spectral–spatial feature learn-\ning via deep residual conv–deconv network for hyperspectral image clas-\nsiﬁcation, IEEE Trans. Geosci. Remote Sens. 56 (1) (2017) 391–406.\n[58] A. Sellami, M. Farah, I. Farah, B. Solaiman, Hyperspectral imagery clas-\nsiﬁcation based on semi-supervised 3-d deep neural network and adaptive\nband selection, Expert Syst. Appl. 129 (2019) 246–259.\n[59] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-\nnition, in: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp.\n770–778.\n[60] M. Paoletti, J. Haut, R. Fernandez-Beltran, J. Plaza, A. Plaza, F. Pla,\nDeep pyramidal residual networks for spectral–spatial hyperspectral im-\nage classiﬁcation, IEEE Transactions on Geoscience and Remote Sensing\n57 (2) (2018) 740–754.\n[61] X. Ma, A. Fu, J. Wang, H. Wang, B. Yin, Hyperspectral image classiﬁ-\ncation based on deep deconvolution network with skip architecture, IEEE\nTrans. Geosci. Remote Sens. 56 (8) (2018) 4781–4791.\n[62] M. Paoletti, J. Haut, J. Plaza, A. Plaza, Deep&dense convolutional neu-\nral network for hyperspectral image classiﬁcation, Remote Sensing 10 (9)\n(2018) 1454.\n[63] W. Wang, S. Dou, Z. Jiang, L. Sun, A fast dense spectral–spatial convo-\nlution network framework for hyperspectral images classiﬁcation, Remote\nSensing 10 (7) (2018) 1068.\n[64] J. Haut, M. Paoletti, J. Plaza, A. Plaza, J. Li, Visual attention-driven\nhyperspectral image classiﬁcation, IEEE Transactions on Geoscience and\nRemote Sensing 57 (10) (2019) 8065–8080.\n[65] Z. Xiong, Y. Yuan, Q. Wang, Ai-net: attention inception neural networks\nfor hyperspectral image classiﬁcation, in: IGARSS 2018-2018 IEEE In-\nternational Geoscience and Remote Sensing Symposium, IEEE, 2018, pp.\n2647–2650.\n[66] Q. Feng, D. Zhu, J. Yang, B. Li, Multisource hyperspectral and lidar data\nfusion for urban land-use mapping based on a modiﬁed two-branch convo-\nlutional neural network, ISPRS International Journal of Geo-Information\n8 (1) (2019) 28.\n[67] X. Xu, W. Li, Q. Ran, Q. Du, L. Gao, B. Zhang, Multisource remote\nsensing data classiﬁcation based on convolutional neural network, IEEE\nTransactions on Geoscience and Remote Sensing 56 (2) (2017) 937–949.\n[68] H. Li, G. Pedram, S. Uwe, X. Zhu, Hyperspectral and lidar fusion using\ndeep three-stream convolutional neural networks, Remote Sensing 10 (10)\n(2018) 1649.\n49\n[69] W. Li, C. Chen, M. Zhang, H. Li, Q. Du, Data augmentation for hyper-\nspectral image classiﬁcation with deep cnn, IEEE Geoscience and Remote\nSensing Letters 16 (4) (2018) 593–597.\n[70] W. Wei, J. Zhang, L. Zhang, C. Tian, Y. Zhang, Deep cube-pair network\nfor hyperspectral imagery classiﬁcation, Remote Sensing 10 (5) (2018)\n783.\n[71] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural compu-\ntation 9 (8) (1997) 1735–1780.\n[72] C. Kyunghyun, V. Bart, G. Caglar, B. Dzmitry, B. Fethi, S. Holger,\nB. Yoshua, Learning phrase representations using rnn encoder-decoder for\nstatistical machine translation, arXiv preprint arXiv:1406.1078 (2014).\n[73] L. Mou, P. Ghamisi, X. Zhu, Deep recurrent neural networks for hyper-\nspectral image classiﬁcation, IEEE Trans. Geosci. Remote Sens. 55 (7)\n(2017) 3639–3655.\n[74] B. Liu, X. Yu, A. Yu, P. Zhang, G. Wan, Spectral-spatial classiﬁcation of\nhyperspectral imagery based on recurrent neural networks, Remote Sens-\ning Letters 9 (12) (2018) 1118–1127.\n[75] F. Zhou, R. Hang, Q. Liu, X. Yuan, Hyperspectral image classiﬁcation\nusing spectral-spatial lstms, Neurocomputing 328 (2019) 39–47.\n[76] M. Andong, F. A. M, Z. Wang, Z. Yin, Hyperspectral image classiﬁca-\ntion using similarity measurements-based deep recurrent neural networks,\nRemote Sensing 11 (2) (2019) 194.\n[77] X. Zhang, Y. Sun, K. Jiang, C. Li, L. Jiao, H. Zhou, Spatial sequential\nrecurrent neural network for hyperspectral image classiﬁcation, IEEE J.\nSel. Topics Appl. Earth Observ. Remote Sens. 11 (11) (2018) 4141–4155.\n[78] E. Pan, X. Mei, Q. Wang, Y. Ma, J. Ma, Spectral-spatial classiﬁcation for\nhyperspectral image based on a single gru, Neurocomputing 387 (2020)\n150–160.\n[79] H. Wu, P. Saurabh, Semi-supervised deep learning using pseudo labels\nfor hyperspectral image classiﬁcation, IEEE Trans. Image Process. 27 (3)\n(2017) 1259–1270.\n[80] H. Wu, P. Saurabh, Convolutional recurrent neural networks forhyper-\nspectral data classiﬁcation, Remote Sensing 9 (3) (2017) 298.\n[81] S. Hao, W. Wang, S. Mathieu, Geometry-aware deep recurrent neural net-\nworks for hyperspectral image classiﬁcation, IEEE Trans. Geosci. Remote\nSens. (2020).\n[82] C. Shi, P. Chi-Man, Multi-scale hierarchical recurrent neural networks for\nhyperspectral image classiﬁcation, Neurocomputing 294 (2018) 82–93.\n50\n[83] S. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on\nKnowledge and Data Engineering 22 (10) (2010) 1345–1359.\ndoi:10.\n1109/tkde.2009.191.\n[84] J. Yang, Y. Zhao, J. Chan, C. Yi, Hyperspectral image classiﬁcation us-\ning two-channel deep convolutional neural network, in: 2016 IEEE Inter-\nnational Geoscience and Remote Sensing Symposium (IGARSS), IEEE,\n2016, pp. 5079–5082.\n[85] J. Yang, Y. Zhao, J. Chan, Learning and transferring deep joint spectral–\nspatial features for hyperspectral classiﬁcation, IEEE Trans. Geosci. Re-\nmote Sens. 55 (8) (2017) 4729–4742.\n[86] L. Lin, C. Chen, J. Yang, S. Zhang, Deep transfer hsi classiﬁcation method\nbased on information measure and optimal neighborhood noise reduction,\nElectronics 8 (10) (2019) 1112.\n[87] H. Zhang, Y. Li, Y. Jiang, P. Wang, Q. Shen, C. Shen, Hyperspectral\nclassiﬁcation based on lightweight 3-d-cnn with transfer learning, IEEE\nTrans. Geosci. Remote Sens. 57 (8) (2019) 5813–5828.\n[88] Y. Jiang, Y. Li, H. Zhang, Hyperspectral image classiﬁcation based on 3-d\nseparable resnet and transfer learning, IEEE Geosci. Remote Sens. Lett.\n16 (12) (2019) 1949–1953.\n[89] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, Active transfer learning network:\nA uniﬁed deep joint spectral–spatial feature learning model for hyperspec-\ntral image classiﬁcation, IEEE Trans. Geosci. Remote Sens. 57 (3) (2018)\n1741–1754.\n[90] M. Ghifary, W. Kleijn, M. Zhang, Domain adaptive neural networks for\nobject recognition, in: Paciﬁc Rim international conference on artiﬁcial\nintelligence, Springer, 2014, pp. 898–904.\n[91] E. Tzeng, J. Hoﬀman, N. Zhang, K. Saenko, T. Darrell, Deep domain con-\nfusion: Maximizing for domain invariance, arXiv preprint arXiv:1412.3474\n(2014).\n[92] Z. Wang, B. Du, Q. Shi, W. Tu, Domain adaptation with discriminative\ndistribution and manifold embedding for hyperspectral image classiﬁca-\ntion, IEEE Geosci. Remote Sens. Lett. 16 (7) (2019) 1155–1159.\n[93] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-\nlette, M. Marchand, V. Lempitsky, Domain-adversarial training of neural\nnetworks, The Journal of Machine Learning Research 17 (1) (2016) 2096–\n2030.\n[94] A. Elshamli, G. Taylor, A. Berg, S. Areibi, Domain adaptation using\nrepresentation learning for the classiﬁcation of remote sensing images,\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 10 (9) (2017)\n4198–4209.\n51\n[95] B. Settles, Active learning literature survey, Tech. rep., University of\nWisconsin-Madison Department of Computer Sciences (2009).\n[96] J. Haut, M. Paoletti, J. Plaza, J. Li, A. Plaza, Active learning with con-\nvolutional neural networks for hyperspectral image classiﬁcation using a\nnew bayesian approach, IEEE Trans. Geosci. Remote Sens. 56 (11) (2018)\n6440–6461.\n[97] P. Liu, H. Zhang, K. Eom, Active deep learning for classiﬁcation of hyper-\nspectral images, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.\n10 (2) (2016) 712–724.\n[98] J. Li, Active learning for hyperspectral image classiﬁcation with a stacked\nautoencoders based neural network, in: 2015 7th Workshop on Hyperspec-\ntral Image and Signal Processing: Evolution in Remote Sensing (WHIS-\nPERS), IEEE, 2015, pp. 1–4.\n[99] Y. Sun, J. Li, W. Wang, P. Antonio, Z. Chen, Active learning based au-\ntoencoder for hyperspectral imagery classiﬁcation, in: 2016 IEEE Inter-\nnational Geoscience and Remote Sensing Symposium (IGARSS), IEEE,\n2016, pp. 469–472.\n[100] X. Cao, J. Yao, Z. Xu, D. Meng, Hyperspectral image classiﬁcation with\nconvolutional neural network and active learning, IEEE Trans. Geosci.\nRemote Sens. (2020).\n[101] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, Active transfer learning network:\nA uniﬁed deep joint spectral–spatial feature learning model for hyperspec-\ntral image classiﬁcation, IEEE Trans. Geosci. Remote Sens. 57 (3) (2018)\n1741–1754.\n[102] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learn-\ning, in: Advances in neural information processing systems, 2017, pp.\n4077–4087.\n[103] Y. Liu, M. Su, L. Liu, C. Li, Y. Peng, J. Hou, T. Jiang, Deep residual pro-\ntotype learning network for hyperspectral image classiﬁcation, in: Second\nTarget Recognition and Artiﬁcial Intelligence Summit Forum, Vol. 11427,\nInternational Society for Optics and Photonics, 2020, p. 1142705.\n[104] H. Tang, Y. Li, X. Han, Q. Huang, W. Xie, A spatial–spectral prototypical\nnetwork for hyperspectral remote sensing image, IEEE Geosci. Remote\nSens. Lett. 17 (1) (2019) 167–171.\n[105] B. Xi, J. Li, Y. Li, R. Song, Y. Shi, S. Liu, Q. Du, Deep prototypical\nnetworks with hybrid residual attention for hyperspectral image classiﬁ-\ncation, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 13 (2020)\n3683–3700.\n52\n[106] A. Muqeet, M. Iqbal, S. Bae, Hran: Hybrid residual attention network for\nsingle image super-resolution, IEEE Access 7 (2019) 137020–137029.\n[107] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,\nLearning to compare: Relation network for few-shot learning, in: Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 1199–1208.\n[108] B. Deng, D. Shi, Relation network for hyperspectral image classiﬁcation,\nin: 2019 IEEE International Conference on Multimedia & Expo Work-\nshops (ICMEW), IEEE, 2019, pp. 483–488.\n[109] K. Gao, B. Liu, X. Yu, J. Qin, P. Zhang, X. Tan, Deep relation network for\nhyperspectral image few-shot classiﬁcation, Remote Sensing 12 (6) (2020)\n923.\n[110] X. Ma, S. Ji, J. Wang, J. Geng, H. Wang, Hyperspectral image classiﬁca-\ntion based on two-phase relation learning network, IEEE Trans. Geosci.\nRemote Sens. 57 (12) (2019) 10398–10409.\n[111] M. Rao, P. Tang, Z. Zhang, Spatial–spectral relation network for hyper-\nspectral image classiﬁcation with limited training samples, IEEE J. Sel.\nTopics Appl. Earth Observ. Remote Sens. 12 (12) (2019) 5086–5100.\n[112] B. Jane, G. Isabelle, L. Yann, S. Eduard, S. Roopak, Signature veriﬁca-\ntion using a” siamese” time delay neural network, in: Advances in neural\ninformation processing systems, 1994, pp. 737–744.\n[113] C. Sumit, H. Raia, L. Yann, Learning a similarity metric discriminatively,\nwith application to face veriﬁcation, in: Proc.IEEE Conf. Comput. Vis.\nPattern Recognit., Vol. 1, IEEE, 2005, pp. 539–546.\n[114] M. Norouzi, D. Fleet, R. Salakhutdinov, Hamming distance metric learn-\ning, in: Advances in neural information processing systems, 2012, pp.\n1061–1069.\n[115] B. Liu, X. Yu, P. Zhang, A. Yu, Q. Fu, X. Wei, Supervised deep feature\nextraction for hyperspectral image classiﬁcation, IEEE Trans. Geosci. Re-\nmote Sens. 56 (4) (2017) 1909–1921.\n[116] B. Liu, X. Yu, A. Yu, G. Wan, Deep convolutional recurrent neural net-\nwork with transfer learning for hyperspectral image classiﬁcation, J. Appl.\nRemote Sens. 12 (2) (2018) 026028.\n[117] Z. Li, X. Tang, W. Li, C. Wang, C. Liu, J. He, A two-stage deep domain\nadaptation method for hyperspectral image classiﬁcation, Remote Sensing\n12 (7) (2020) 1054.\n[118] L. Huang, Y. Chen, Dual-path siamese cnn for hyperspectral image clas-\nsiﬁcation with limited training samples, IEEE Geosci. Remote Sens. Lett.\n(2020).\n53\n[119] M. Rao, P. Tang, Z. Zhang, A developed siamese cnn with 3d adaptive\nspatial-spectral pyramid pooling for hyperspectral image classiﬁcation,\nRemote Sensing 12 (12) (2020) 1964.\n[120] J. Miao, B. Wang, X. Wu, L. Zhang, B. Hu, J. Zhang, Deep feature extrac-\ntion based on siamese network and auto-encoder for hyperspectral image\nclassiﬁcation, in: IGARSS 2019-2019 IEEE International Geoscience and\nRemote Sensing Symposium, IEEE, 2019, pp. 397–400.\n[121] B. Deng, S. Jia, D. Shi, Deep metric learning-based feature embedding\nfor hyperspectral image classiﬁcation, IEEE Transactions on Geoence and\nRemote Sensing 58 (2) (2020) 1422–1435.\n[122] J. Yang, Y. Zhao, J. Chan, Learning and transferring deep joint spectral-\nspatial features for hyperspectral classiﬁcation, IEEE Trans. Geosci. Re-\nmote Sens. 55 (8) (2017) 4729–4742. doi:10.1109/TGRS.2017.2698503.\n[123] L. Hu, X. Luo, Y. Wei, Hyperspectral image classiﬁcation of convolu-\ntional neural network combined with valuable samples, Journal of Physics\nConference Series 1549 (2020) 052011.\n[124] S. Wan, C. Gong, P. Zhong, B. Du, L. Zhang, J. Yang, Multiscale dynamic\ngraph convolutional network for hyperspectral image classiﬁcation, IEEE\nTransactions on Geoscience and Remote Sensing 58 (5) (2019) 3162–3177.\n[125] B. Liu, K. Gao, A. Yu, W. Guo, R. Wang, X. Zuo, Semisupervised graph\nconvolutional network for hyperspectral image classiﬁcation, J. Appl. Re-\nmote Sens. 14 (2) (2020) 026516.\n[126] S. Wan, C. Gong, P. Zhong, S. Pan, G. Li, J. Yang, Hyperspectral image\nclassiﬁcation with context-aware dynamic graph convolutional network,\nIEEE Transactions on Geoscience and Remote Sensing 59 (1) (2020) 597–\n612.\n[127] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,\nM. Andreetto, H. Adam, Mobilenets: Eﬃcient convolutional neural net-\nworks for mobile vision applications, arXiv preprint arXiv:1704.04861\n(2017).\n54\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "eess.IV"
  ],
  "published": "2021-12-03",
  "updated": "2021-12-03"
}