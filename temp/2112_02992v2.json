{
  "id": "http://arxiv.org/abs/2112.02992v2",
  "title": "Towards More Robust Natural Language Understanding",
  "authors": [
    "Xinliang Frederick Zhang"
  ],
  "abstract": "Natural Language Understanding (NLU) is a branch of Natural Language\nProcessing (NLP) that uses intelligent computer software to understand texts\nthat encode human knowledge. Recent years have witnessed notable progress\nacross various NLU tasks with deep learning techniques, especially with\npretrained language models. Besides proposing more advanced model\narchitectures, constructing more reliable and trustworthy datasets also plays a\nhuge role in improving NLU systems, without which it would be impossible to\ntrain a decent NLU model. It's worth noting that the human ability of\nunderstanding natural language is flexible and robust. On the contrary, most of\nexisting NLU systems fail to achieve desirable performance on out-of-domain\ndata or struggle on handling challenging items (e.g., inherently ambiguous\nitems, adversarial items) in the real world. Therefore, in order to have NLU\nmodels understand human language more effectively, it is expected to prioritize\nthe study on robust natural language understanding. In this thesis, we deem\nthat NLU systems are consisting of two components: NLU models and NLU datasets.\nAs such, we argue that, to achieve robust NLU, the model architecture/training\nand the dataset are equally important. Specifically, we will focus on three NLU\ntasks to illustrate the robustness problem in different NLU tasks and our\ncontributions (i.e., novel models and new datasets) to help achieve more robust\nnatural language understanding. Moving forward, the ultimate goal for robust\nnatural language understanding is to build NLU models which can behave humanly.\nThat is, it's expected that robust NLU systems are capable to transfer the\nknowledge from training corpus to unseen documents more reliably and survive\nwhen encountering challenging items even if the system doesn't know a priori of\nusers' inputs.",
  "text": "Towards More Robust Natural Language Understanding\nThesis\nPresented in Partial Fulﬁllment of the Requirements for the Degree Bachelor\nof Science in Computer Science and Engineering with Honors\nUndergraduate Research Distinction in the College of Engineering of The\nOhio State University\nBy\nXinliang (Frederick) Zhang,\nUndergraduate Program in Computer Science and Engineering\nThe Ohio State University\n2021\nThesis Committee:\nDr. Huan Sun, Advisor\nDr. Marie-Catherine de Marneffe\narXiv:2112.02992v2  [cs.CL]  27 Feb 2022\n© Copyright by\nXinliang (Frederick) Zhang\n2021\nAbstract\nNatural Language Understanding (NLU) is a branch of Natural Language Processing\n(NLP) that uses intelligent computer software to understand texts that encode human knowl-\nedge. Recent years have witnessed notable progress across various NLU tasks with deep\nlearning techniques, especially with pretrained language models. Besides proposing more\nadvanced model architectures, constructing more reliable and trustworthy datasets also\nplays a huge role in improving NLU systems, without which it would be impossible to train\na decent NLU model. It’s worth noting that the human ability of understanding natural\nlanguage is ﬂexible and robust. On the contrary, most of existing NLU systems fail to\nachieve desirable performance on out-of-domain data or struggle on handling challenging\nitems (e.g., inherently ambiguous items, adversarial items) in the real world. Therefore, in\norder to have NLU models understand human language more effectively, it is expected to\nprioritize the study on robust natural language understanding.\nIn this thesis, we deem that NLU systems are consisting of two components: NLU\nmodels and NLU datasets. As such, we argue that, to achieve robust NLU, the model\narchitecture/training and the dataset are equally important. Speciﬁcally, we will focus\non three NLU tasks to illustrate the robustness problem in different NLU tasks and our\ncontributions (i.e., novel models and new datasets) to help achieve more robust natural\nlanguage understanding. The major technical contributions of this thesis are:\nii\n1.\nWe study how to utilize diversity boosters (e.g., beam search & QPP) to help neural\nquestion generator synthesize diverse QA pairs, upon which a Question Answering\n(QA) system is trained to improve the generalization on the unseen target domain. It’s\nworth mentioning that our proposed QPP (question phrase prediction) module, which\npredicts a set of valid question phrases given an answer evidence, plays an important\nrole in improving the cross-domain generalizability for QA systems. Besides, a\ntarget-domain test set is constructed and approved by the community to help evaluate\nthe model robustness under the cross-domain generalization setting.\n2.\nWe investigate inherently ambiguous items in Natural Language Inference, for which\nannotators don’t agree on the label. Ambiguous items are overlooked in the literature\nbut often occurring in the real world. We build an ensemble model, AAs (Artiﬁcial\nAnnotators), that simulates underlying annotation distribution to effectively identify\nsuch inherently ambiguous items. Our AAs are better at handling inherently ambigu-\nous items since the model design captures the essence of the problem better than\nvanilla model architectures.\n3.\nWe follow a standard practice to build a robust dataset for FAQ retrieval task, COUGH.\nIn our dataset analysis, we show how COUGH better reﬂects the challenge of FAQ\nretrieval in the real situation than its counterparts. The imposed challenge will push\nforward the boundary of research on FAQ retrieval in real scenarios.\nMoving forward, the ultimate goal for robust natural language understanding is to build\nNLU models which can behave humanly. That is, it’s expected that robust NLU systems are\ncapable to transfer the knowledge from training corpus to unseen documents more reliably\nand survive when encountering challenging items even if the system doesn’t know a priori\nof users’ inputs.\niii\nDedicated to my parents.\niv\nAcknowledgments\nI feel incredibly fortunate to have Dr. Huan Sun as my advisor, without whom nothing\nin this thesis is possible. I would like to express my sincere gratitude to her for critiquing\nmy work and my ideas in a constructive way. Her vision and rigorous research attitudes\nhave shaped my thoughts. I am always indebted to her for guiding me all the way, for her\ncontagious energy, for being so supportive and caring about students.\nI owe a great debt of gratitude to Dr. Marie-Catherine de Marneffe for her countless\nhelp. I am super grateful for her many invaluable insights and suggestions on my work. She\nhas been so generous with her time, reading, reviewing and commenting on many of my\nwritings. I am always thankful for her positive encouragement and praise.\nIt’s also my great privilege to collaborate with my friends, lab-mates at SunLab and my\npast mentor: Xiang Yue, Ziyu Yao, Heming Sun, Emmett Jesrani and Dr. Chen Chen.\nI appreciate the help from anyone who helped me along my education journey: Hangzhou\nJindu Tianchang Elementary School, Hangzhou Caihe Experimental School, Hangzhou\nXuejun High School, Sichuan University and Ohio State University. I am especially thankful\nto my Chinese friends, who always compliment me and cheer me up no matter what.\nLast but not the least, my deepest gratitude, without any doubt, goes to my parents,\nRongchang and Hangjuan. They gave birth to me, raised me up, set good examples for me,\nand taught me tremendously many invaluable lessons. I wouldn’t become who I am without\ntheir trust and support. All in all, thanks for their unconditional love and upbringing.\nv\nVita\n2021- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ph.D. in Computer Science and Engineer-\ning, University of Michigan.\n2018-2021 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.S. in Computer Science and Engineer-\ning & Industrial and Systems Engineering,\nThe Ohio State University.\n2016-2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .B.E. in Industrial Engineering, Sichuan\nUniversity.\nPublications\nResearch Publications\nXinliang Frederick Zhang and Marie-Catherine de Marneffe. Identifying inherent dis-\nagreement in natural language inference. In NAACL 2021. 2021.\nXinliang Frederick Zhang, Heming Sun, Xiang Yue, Simon Lin, and Huan Sun. COUGH:\nA challenge dataset and models for COVID-19 FAQ retrieval. In EMNLP 2021. 2021.\nXiang Yue*, Xinliang Frederick Zhang*, Ziyu Yao, Simon Lin, and Huan Sun. Clin-\niQG4QA: Generating diverse questions for domain adaptation of clinical question answering.\nIn IEEE BIBM 2021. 2021. (*equal contributions)\nFields of Study\nMajor Field: Computer Science and Engineering\nvi\nTable of Contents\nPage\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nii\nDedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\niv\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nv\nVita\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nvi\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nx\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nxi\n1.\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.1\nNatural Language Understanding (NLU)\n. . . . . . . . . . . . . . . . .\n1\n1.2\nRobustness Problem in NLU . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.\nClinical Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nOut-of-Domain Test Set . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3\nFramework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3.1\nOverview of Our Framework\n. . . . . . . . . . . . . . . . . . .\n9\n2.3.2\nPreliminary Observation . . . . . . . . . . . . . . . . . . . . . .\n10\n2.4\nDiverse Question Generation for QA\n. . . . . . . . . . . . . . . . . . .\n11\n2.4.1\nOverview of Diverse Question Generation\n. . . . . . . . . . . .\n11\n2.4.2\nQuestion Phrase Prediction (QPP) . . . . . . . . . . . . . . . . .\n12\n2.5\nEvaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.5.1\nExperiment Setup\n. . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.5.2\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.6\nAnalysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nvii\n2.6.1\nQuantitative Analysis\n. . . . . . . . . . . . . . . . . . . . . . .\n15\n2.6.2\nQualitative Analysis: Error Analysis\n. . . . . . . . . . . . . . .\n16\n2.7\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.\nNatural Language Inference\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2\nInherently Ambiguous Items in CB\n. . . . . . . . . . . . . . . . . . . .\n20\n3.3\nLinguistic Rules\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.4\nArtiﬁcial Annotators . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.4.1\nArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.4.2\nTraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.5\nEvaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n3.6\nAnalysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n3.6.1\nEmpirical Results Analysis\n. . . . . . . . . . . . . . . . . . . .\n27\n3.6.2\nLinguistic Construction Analysis . . . . . . . . . . . . . . . . .\n29\n3.7\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.\nFAQ Retrieval\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4.2\nStandard FAQ Dataset Construction: COUGH . . . . . . . . . . . . . . .\n32\n4.2.1\nFAQ Bank Construction . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2.2\nUser Query Bank Construction\n. . . . . . . . . . . . . . . . . .\n34\n4.2.3\nAnnotated Relevance Set Construction . . . . . . . . . . . . . .\n34\n4.3\nCOUGH Dataset Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.4\nFAQ Retrieval Methods\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n4.4.1\nFAQ Retrieval Methods Overview . . . . . . . . . . . . . . . . .\n37\n4.4.2\nUnsupervised FAQ Retrieval . . . . . . . . . . . . . . . . . . . .\n38\n4.5\nEvaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n4.6\nAnalysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.7\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n5.\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nAppendices\n47\nA.\nSupplementary Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nA.1\nClinical Question Answering . . . . . . . . . . . . . . . . . . . . . . . .\n47\nA.1.1\nAnswer Evidence Extractor . . . . . . . . . . . . . . . . . . . .\n47\nviii\nA.1.2\nQuestion Phrases Identiﬁcation . . . . . . . . . . . . . . . . . .\n48\nA.1.3\nDev Set Construction\n. . . . . . . . . . . . . . . . . . . . . . .\n49\nix\nList of Tables\nTable\nPage\n2.1\nStatistics of the datasets.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2\nQA performance on MIMIC-III test set. . . . . . . . . . . . . . . . . . . .\n14\n2.3\nAutomatic evaluation of the generated questions on emrQA dataset.\n. . . .\n15\n3.1\nExamples from CommitmentBank. . . . . . . . . . . . . . . . . . . . . . .\n19\n3.2\nNumber of items in each class in train/dev/test.\n. . . . . . . . . . . . . . .\n21\n3.3\nBaselines and AAs overall performance on CB dev and test sets, and F1\nscores of each class on the test set. . . . . . . . . . . . . . . . . . . . . . .\n26\n3.4\nModels’ predictions for CB test items. . . . . . . . . . . . . . . . . . . . .\n28\n3.5\nConfusion matrix for the test set. . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.6\nF1 for CB test set under the embedding environments and “I don’t know/believe/think”\n(“negR”).\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.7\nBERT-based models performance on test items correctly predicted by vs.\nitems missed by linguistic rules.\n. . . . . . . . . . . . . . . . . . . . . . .\n30\n4.1\nComparison of COUGH with representative counterparts.\n. . . . . . . . . .\n33\n4.2\nBasic statistics of FAQ bank in COUGH.\n. . . . . . . . . . . . . . . . . . .\n35\n4.3\nEvaluation on COUGH.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n4.4\nError analysis with ﬁne-tuned BERT (Q-q).\n. . . . . . . . . . . . . . . . .\n41\nx\nList of Figures\nFigure\nPage\n2.1\nIllustration of our framework equipped with QPP. . . . . . . . . . . . . . .\n10\n2.2\nDistributions over types of questions generated by NQG models and the\nground truth.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3\nQA and QG examples.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.1\nArtiﬁcial Annotators (AAs) setup. . . . . . . . . . . . . . . . . . . . . . .\n24\n4.1\nExamples from the COUGH dataset. . . . . . . . . . . . . . . . . . . . . . .\n32\n4.2\nLanguage distribution for non-English FAQ items. . . . . . . . . . . . . . .\n37\nxi\nChapter 1: Introduction\n1.1\nNatural Language Understanding (NLU)\nHave you ever asked: “Siri, how is the weather today?”, “Cortana, what is the best spot\nfor hiking in Columbus?” or ”Xiaoice, could you tell me how’s trafﬁc outside?”. If so, you\nhave experienced receiving a data-supported answer from your personalized AI assistant. A\nnatural question that people would ask is how can the agent understand an utterance and\nintents and generate a relevant response. The answer is Natural Language Understanding.\nNatural Language Understanding (NLU) is a branch of Natural Language Processing\n(NLP) in the area of Artiﬁcial Intelligence (AI) that uses intelligent computer software to\nunderstand texts that encode human knowledge. Some representative NLU applications (and\nthere are way more) are: Automated Reasoning, Question Answering, Text Categorization,\nLarge-scale Content Analysis, Information Retrieval and Textual Entailment. NLU is\ngenerally considered an AI-hard problem (i.e., a problem that is hard to be solved by AI\nsystems) (Yampolskiy, 2013). NLU is an AI-hard problem mainly because the nature of\nhuman language (e.g., ambiguity) makes NLU difﬁcult. For example, given the following\nsentence “when the hammer hit the glass table, it shattered”,1 humans know that it is the\nglass table that shattered but not the hammer. This is because our prior knowledge let us\n1https://www.colorado.edu/earthlab/2020/02/07/what-natural-language-processing-and-why-it-hard.\n1\nknow what glass is and that glass can shatter easily. However, coreference resolution is still\na challenging task for NLU models, and thus, NLU systems still have difﬁculties ﬁguring\nout which one of these two objects shatters.\nRecent years have witnessed notable progress across various Natural Language Un-\nderstanding tasks, especially after entering the deep learning era in 2012. Deep learning\napproaches quickly outperformed statistical learning methods by a large margin on many\nNLU tasks. As today, neural network-based NLP models have reached many new milestones\n(e.g., model performance comes close to or surpasses the level of non-expert humans) and\nhave become the dominating approach for NLP tasks. Typical neural network-based NLP\nmodels/algorithms are RNN (Elman, 1990), LSTM (Hochreiter and Schmidhuber, 1997),\nGRU (Cho et al., 2014), Seq2Seq (Sutskever et al., 2014), attention mechanism (Luong\net al., 2015) and Transformer (Vaswani et al., 2017). Recently, pretrained language models,\nsuch as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019b), have dramatically\naltered the NLP landscape and marked new records on the majority of NLU tasks. However,\nthe neural NLP models work well for supervised tasks in which there is abundant labeled\ndata for learning, but still perform poorly for low-resource and cross-domain tasks where\nthe training data is insufﬁcient and the test data is from different domains, respectively.\nBesides more advanced model architectures, reliable and trustworthy datasets also play\na huge role in improving NLU systems. Without a decent dataset, it would be challenging\nto train a machine learning model, not to mention carrying out a valid evaluation. As such,\ncomprehensive evaluation benchmarks, aggregating datasets of multiple NLU tasks, emerged\nin the past few years such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al.,\n2019a). They are diagnostic datasets designed to evaluate and analyze model performance\nwith respect to a wide range of linguistic phenomena found in human language.\n2\nThe human ability of understanding natural language is ﬂexible and robust. Therefore,\nhuman capability of understanding multiple language tasks simultaneously and transferring\nthe knowledge to unseen documents is mostly reliable. On the contrary, most of existing\nNLU models built on word/character levels are exclusively trained on a restricted dataset.\nThese restricted datasets normally only characterize one particular domain or only include\nsimple examples which might not well reﬂect the task difﬁculties in reality. Consequently,\nsuch models usually fail to achieve desirable performance on out-of-domain data or struggle\non handling challenging items (e.g., inherently ambiguous items, adversarial items) in the\nreal world. Moreover, machine learning algorithms are usually data-hungry and can easily\nmalfunction when there is insufﬁcient amount of training data. Therefore, in order to have\nNLU models understand human language more effectively, it is expected to prioritize the\nstudy on robust natural language understanding.\n1.2\nRobustness Problem in NLU\nIn this thesis, we deem that NLU systems are consisting of two components: NLU\nmodels and NLU datasets. As such, we argue that, to achieve robust NLU, the model\narchitecture/training and the dataset are equally important. If either component is weak,\nit would be hard to achieve full robustness. Therefore, in order to achieve full robustness\nin NLU, researchers are expected to implement robust models which then are trained on\nconstructed robust datasets. In this thesis, we deﬁne robust models and robust datasets as\nfollow:\n1. Robust models are expected to be resistant to domain changes and resilient to\nchallenging items (e.g., inherently ambiguous items, adversarial items).\n3\n2. Robust datasets are expected to reﬂect real-world challenges and encode knowledge\nthat is difﬁcult to be unraveled simply by surface-level2 understanding.\nIn short, a truly robust NLU system is expected to be a robust model trained on robust\ndatasets.\nThree NLU tasks for NLU robustness problem\nIn the context of NLP, robustness is an umbrella term which could be interpreted\ndifferently from different angles. In this thesis, we will focus on three NLU tasks to illustrate\nthe robustness problem in different NLU tasks and our contributions (i.e., novel models and\nnew datasets) to help achieve more robust natural language understanding.\nThe ﬁrst robustness problem that will be studied in this thesis is the cross-domain\ngeneralization.In Question Answering, most past work on open-domain were only testing\nmodels on in-domain data (source domain), despite outperforming human performance.\nHowever, these well-performing models have a relatively weak generalizability, which is the\ncrux of this robustness problem. That is, when such models are deployed on out-of-domain\ndata (target domain), their performances go down drastically, which is way behind human\nperformance. Similar trend is also observed under the clinical setting where a model trained\non one corpus may not generalize well to new clinical texts collected from different medical\ninstitutions (Yue et al., 2020, 2021). In Chapter 2, we will study how to utilize diversity\nboosters to help Question Generator (QG) synthesize diverse3 QA pairs, upon which a\nQuestion Answering system is trained to improve the generalization to the unseen target\ndomain. We also construct a target-domain test set to help evaluate models’ generalizability.\n2For example, the presence of “not” or “bad” doesn’t always indicate a negative sentiment.\n3“Diverse” here means questions with different syntactic structures or different topics.\n4\nThe second robustness problem that will be studied in this thesis is how to better han-\ndle inherently ambiguous items, one type of challenging items in reality. In sentiment\nanalysis and textual entailment tasks, it has been observed that there are inherently ambigu-\nous/disagreement4 items for which annotators have different annotations (Kenyon-Dean\net al., 2018; Pavlick and Kwiatkowski, 2019; Zhang and de Marneffe, 2021). These items\nwere usually treated as noise and removed in the dataset construction phase, which is\nproblematic. In Chapter 3, we will investigate inherently ambiguous items, which are over-\nlooked in the literature but often occurring in the real world, in the NLI (Natural Language\nInference) task. To this end, we build an ensemble model, AAs (Artiﬁcial Annotators),\nwhich simulates underlying annotation distribution by capturing the modes in annotations to\neffectively identify such inherently ambiguous items.\nThe third robustness problem that will be studied in this thesis is how to construct a\nreliable and challenging dataset (i.e., robust dataset). In textual entailment and FAQ retrieval\ntasks, common datasets (e.g., SNLI (Bowman et al., 2015) and MultiNLI (Williams et al.,\n2018) for textual entailment; FAQIR (Karan and ˇSnajder, 2016) and StackFAQ (Karan and\nˇSnajder, 2018) for FAQ retrieval) used for training and testing might not well characterize\nthe real difﬁculties of respective tasks. In the aforementioned datasets, sentence lengths and\nlanguage complexities are generally low, styles are limited and the search space is small. In\nChapter 4, we will follow a standard practice to build a robust dataset for the FAQ Retrieval\ntask. In our dataset analysis, we will also show how this dataset better reﬂects the challenge\nof FAQ Retrieval in the real situation than its counterparts.\nWe will conclude with recommendations for future work about how to better approach\nrobustness problem in NLU in Chapter 5.\n4In this thesis, “ambiguous” and “disagreement” will be used interchangeably.\n5\nChapter 2: Clinical Question Answering\n2.1\nIntroduction\nClinical question answering (QA), which aims to automatically answer natural language\nquestions based on clinical texts in Electronic Medical Records (EMR), has been identiﬁed\nas an important task to assist clinical practitioners (Patrick and Li, 2012; Raghavan et al.,\n2018; Pampari et al., 2018; Fan, 2019; Rawat et al., 2020). Neural QA models in recent\nyears (Chen et al., 2017; Devlin et al., 2019b) show promising results in this research.\nHowever, answering clinical questions still remains challenging in real-world scenarios\nbecause well-trained QA systems may not generalize well to new clinical contexts from\na different institute or patient group. For example, Yue et al. (2020) pointed out when\na clinical QA model trained on the emrQA (Pampari et al., 2018) dataset is deployed to\nanswer questions on MIMIC-III clinical texts (Johnson et al., 2016), its performance drops\nby around 30% even on questions that are similar to those in training.\nMost of the existing clinical QA datasets and setups focus on in-domain testing while\nleaving the generalization challenge under-explored. In this chapter, we propose to evaluate\nthe performance of clinical QA models on target contexts and questions which may have\ndifferent distributions from the training data. Due to the lack of publicly-available clinical\n6\nQA pairs for our proposed evaluation setting, we ask clinical experts to annotate a new test\nset on the sampled MIMIC-III (Johnson et al., 2016) clinical texts.\nInspired by recent work on question generation (QG) for improving QA performance in\nthe open domain (Golub et al., 2017; Wang et al., 2019c; Shakeri et al., 2020), we implement\nan answer evidence extractor and a seq2seq-based QG model to synthesize QA pairs on\ntarget contexts to train a QA model. However, we do not observe that such QA models\nachieve better performance on our curated MIMIC-III QA set, compared with that trained\non emrQA. Our error analysis reveals that the automatic generation technique often falls\nshort of generating questions that are diverse enough to serve as useful training data for\nclinical QA models.\nTo this end, we investigate two kinds of approaches to diversify the generation. Inspired\nby Ippolito et al. (2019) whio study various decoding-based methods, we pick the standard\nbeam search as a representative of the decoding-based approach since it achieves satisfying\nperformance in various generation tasks. On the other hand, another practice (topic-guided\napproach) is to have a diversiﬁcation step followed by a conditional generation. In general,\nsuch techniques ﬁrst decide question topics and then generate questions conditioned on\nselected topics (Kang et al., 2019; Cho et al., 2019; Liu et al., 2020). Following the second\napproach, we propose a simple but effective question phrase prediction (QPP) module to\ndiversify the generation. Speciﬁcally, QPP takes the extracted answer evidence as input\nand sequentially predicts potential question phrases (e.g., “What treatment”, “How often”)\nthat signify what types of questions humans may ask about the answer evidence. Then, by\ndirectly forcing a QG model to produce speciﬁed question phrases at the beginning of the\nquestion generation process (both in training and inference), QPP enables diverse questions\nto be generated.\n7\nThrough comprehensive experiments, we demonstrate that when using QA pairs auto-\nmatically synthesized by diverse QG, especially by the QPP-enhanced QG, we are able to\nboost QA performance by 4.5%-9% in terms of Exact Match (EM), compared with their\ncounterparts directly trained on the source QA dataset (i.e., emrQA).\n2.2\nOut-of-Domain Test Set\nUnlike open domain, there are very few publicly available QA datasets in the clinical\ndomain. EmrQA dataset (Pampari et al., 2018), which was generated based on medical\nexpert-made question templates and existing annotations on n2c2 challenge datasets (n2c2,\n2006), is a commonly adopted dataset for clinical reading comprehension.\nHowever, all the QA pairs in emrQA are based on n2c2 clinical texts and thus not suitable\nfor our generalization setting. Yue et al. (2020) studied a similar problem and annotated\na test set on MIMIC-III clinical texts (Johnson et al., 2016). However, their test set is too\nsmall (only 50 QA pairs) and not publicly available. Given the lack of a reasonably large\nclinical QA test set for studying generalization, with the help of three clinical experts, we\ncreate 1287 QA pairs on a sampled set of MIMIC-III (Johnson et al., 2016) clinical notes,\nwhich have been reviewed and approved by PhysioNet.5\nAnnotation Process. We sample 36 MIMIC-III clinical notes6 as contexts. For each context,\nclinical experts can ask any questions as long as an answer can be extracted from the context.\nTo save annotation effort, QA pairs generated by 9 QG models (i.e., all base QG models and\ntheir diversity-enhanced variants; see Section 2.5.1) are provided as references, and (nearly)\n5https://physionet.org/. PhysioNet is a resource center with missions to conduct and catalyze for biomedical\nresearch, which offers free access to large collections of physiological and clinical data, such as MIMIC-III\n(Johnson et al., 2016).\n6When sampling MIMIC-III notes, we ensure that all the sampled clinical texts do not appear in emrQA,\nacknowledging that there is a small overlap between the two datasets.\n8\n(Question / Context)\nemrQA\nMIMIC-III\n# Train\n781,857 / 337\n- / 337\n# Dev\n86,663 / 41\n8,824 / 40\n# Test\n98,994 / 42\n1,287 / 36\n# Total\n967,514 / 420\n- / 413\nfor purpose of\nQG & QA\n(source domain)\nQA\n(target domain)\nTable 2.1: Statistics of the datasets. We synthesize a machine-generated dev set and ask\nhuman experts to annotate a test set for MIMIC-III. Details of dev set construction can be\nfound in Setion A.1.3.\nduplicates are removed. Meanwhile, clinical experts are highly encouraged to create new\nquestions based on the given clinical text (which are marked as “human-generated”). But if\nthey do ﬁnd that the machine-generated questions sound natural and match the provided\nanswer, they can keep them (which are marked as “human-veriﬁed”). After obtaining the\nannotated questions, we ask another clinical expert to do a ﬁnal pass of the questions in\norder to further ensure the quality of the test set. The ﬁnal test set consists of 1287 questions\n(of which 975 are “human-veriﬁed” and 312 are “human-generated”).\nIn the following sections, we consider emrQA as the source dataset and our annotated\nMIMIC-III QA dataset as the target data. Detailed statistics of the two datasets are given in\nTable 2.1.\n2.3\nFramework\n2.3.1\nOverview of Our Framework\nWe ﬁrst give an overview of our framework without including any diversity booster.\nTo solve the proposed generalization challenge of clinical QA, inspired by recent work\non question generation (QG) for QA in the open domain (Golub et al., 2017; Wang et al.,\n9\nQuestion\nPhrase\nPrediction\nWhat treatment\nHow often\nWhat dosage\nQG \nModel\nQ1\nQ2\nQ3\nAnswer Evidence\nAnnotated\nQA pairs\non Target Contexts\non Source Contexts\nTrain\nTrain\nDiverse \nGenerated Questions\nclonazepam 1 mg tablet\nsig: one tablet once a\nday (at bedtime) as\nneeded for insomnia.\nInput\nWhat treatment did the\npatient use for insomnia?\nHow often does the\npatient take clonazepam?\nWhat dosage of insomnia\ndoes the patient take?\nQA \nModel\nTrain\nFigure 2.1: Illustration of our framework equipped with QPP: A key component is our\nquestion phrase prediction (QPP) module, which aims to generate diverse question phrases\nand can be “plugged-and-played” with most existing QG models to diversify their generation.\n2019c; Shakeri et al., 2020), we implement an answer evidence extractor and a seq2seq-\nbased neural QG model (Du et al., 2017, NQG) to synthesize QA pairs on target contexts.\nSpeciﬁcally, given a document, we deploy a ClinicalBERT (Alsentzer et al., 2019) model\nto extract a long text7 span as an answer evidence. We formulate such span prediction\nproblem as a BIO tagging task. After prediction, we develop some heuristic rules (e.g.,\nremoving/merging very short extracted evidences) to further improve the quality of the\nextracted evidences; more details are listed in Appendix A.1.1. Based on the extracted\nanswer evidences, a seq2seq-based QG model can be used to generate questions. Both\nanswer evidence extractor and QG model are trained on the source data and then used to\nsynthesize QA pairs on target contexts, based on which a QA model can be trained.\n2.3.2\nPreliminary Observation\nTo our surprise, training on the synthesized target-context QA pairs does not yield an\nimprovement of QA on the constructed MIMIC-III QA set. Speciﬁcally, F1 is 79.43 for the\nQA model trained on corpus synthesized by NQG (neural question generation) model, which\n7Following Pampari et al. (2018); Yue et al. (2020), we focus on long text spans instead of short answers\nsince the former often contain richer information, which is more useful to support clinical decision making.\n10\n0\n29.94\n0\n0\n3.95\n5.54\n0\n0\n0\n0\n0\n0\n0\n1.38\n0\n0\n0\n0\n0\n0\n96.05\n63.14\nNQG\nNQG+BeamSearch\nWhat\nWhen\nHas\nWas\nWhy\nHow\nIs\nDid\nCan\nAny\nDoes\n8.39\n13.8\n0.01\n0.01\n25.37\n26.73\n4.44\n1.94\n0.91\n0.63\n0.87\n1.16\n9.09\n13.24\n0.27\n0.14\n4.03\n1.31\n12.72\n4.11\n33.89\n36.93\nNQG+QPP (Ours)\nGround Truth\nNQG: 84.2     NQG+BS:45.2   NQG+QPP (Ours): 11.0 \nKL \nDivergence: \nFigure 2.2: Distributions over types of questions generated by NQG models and the ground\ntruth. BS: Beam Search; QPP: Question Phrase Prediction module.\nis a little inferior to directly training the QA model on emrQA (79.99 F1). An outstanding\ncharacteristic we observe in the generated questions is the large bias of question types\n(e.g., most questions are “Does” while there is few “Why” and no “How” question). The\ndistributions of question types are in Figure 2.2 (see top-left sub-plot).\n2.4\nDiverse Question Generation for QA\nGiven the observation above, we argue that the synthetic questions should be diverse so\nthat they could serve as more useful training corpora.\n2.4.1\nOverview of Diverse Question Generation\nWe investigate two kinds of approaches to diversify the generation. In the ﬁrst decoding-\nbased approach, we select the standard beam search as the representative since it is well\n11\nAlgorithm 1 Training procedure of our framework equipped with QPP.\nInput: labeled source data {(PS, AS, QS)}, unlabeled target data {PT}\nOutput: Generated QA pairs {(A′\nT, Q′\nT)} on target contexts; An optimized QA model for\nanswering questions on target contexts\nPretraining Stage\n1: Train Answer Evidence Extractor based on the source data {(PS, AS)}\n2: Obtain question phrase data YS from QS and train Question Phrase Prediction module\non the source data {(AS, YS)}\n3: Train a QPP-enhanced QG model on the source data {(AS, YS, QS)}\nTraining Stage\n4: Use AEE to extract potential answer evidences {A′\nT} on the target contexts {PS}\n5: Use QPP to predict potential question phrases set {Y ′\nT} on {A′\nT}\n6: Use QPP-enhanced QG to generate diverse questions {Q′\nT} based on {(A′\nT, Y ′\nT)}\n7: Train a QA model on synthetic target data {(PT, A′\nT, Q′\nT)}\nstudied and shows competitive performance in diversifying generations (Ippolito et al.,\n2019). For the other kind (topic-guided approach), we propose a question phrase prediction\n(QPP) module, which predicts a set of valid question phrases given an answer evidence\n(Figure 2.1). Then, conditioned on a question phrase sampled from the set predicted by the\nQPP, a QG model is utilized to complete the rest of the question.\n2.4.2\nQuestion Phrase Prediction (QPP)\nWe formulate the question phrase prediction task as a sequence prediction problem and\nadopt a commonly used seq2seq model (Luong et al., 2015). More formally, given an answer\nevidence a, QPP aims to predict a sequence of question phrases s = (s1, ..., s|s|)(e.g., “What\ntreatment” (s1) →“How often” (s2) →“What dosage” (s3), with |s| = 3).\nDuring training, we assume that the set of question phrases is arranged in a pre-deﬁned\norder. Such orderings can be obtained with some heuristic methods, e.g., using a descending\norder based on question phrase frequency in the corpus8 (more details are in Appendix A.1.2).\nAs such, we aim to minimize:\n8In emrQA, each answer evidence is tied with multiple questions, which allows the training for QPP.\n12\nLQPP = −\nX\nlog P(s|a; θ)\n(2.1)\nwhere s, a, θ denote question phrase sequence, input answer evidence and all the parameters\nin QPP, respectively. Algorithm 1 illustrates the pretraining and training procedure of our\nframework when equipped with our proposed QPP module.\nIn the inference stage, QPP can dynamically decide the number of question phrases for\neach answer evidence by predicting a special [STOP] type. By decomposing QG into two\nsteps (diversiﬁcation followed by generation), the proposed QPP can increase the diversity\nin a more controllable way compared with decoding-based approach.\n2.5\nEvaluation and Results\n2.5.1\nExperiment Setup\nBase QG and QA models: In our experiments, we adopt three base QG models: NQG (Du\net al., 2017), NQG++ (Zhou et al., 2017) and BERT-SQG (Chan and Fan, 2019). For QA,\nwe use two base models, DocReader (Chen et al., 2017) and ClinicalBERT (Alsentzer et al.,\n2019).\nTo investigate the effectiveness of diverse QG for QA, we consider the following variants\nof each base QG model: (1) Base Model: Inference with greedy search; (2) Base Model +\nBeam Search: Inference with Beam Search with the beam size at K and keep top K beams\n(we set K = 3) (3) Base Model + QPP: Inference with greedy search for both QPP module\nand Base model.\nWhen training a QA model, we only use the synthetic data on the target contexts and do\nnot combine the synthetic data with the source data since the combination does not help in\nour preliminary experiments.\n13\nQA Datasets\nDocReader (Chen et al., 2017)\nClinicalBERT (Alsentzer et al., 2019)\nHuman\nVeriﬁed\nHuman\nGenerated\nOverall\nTest\nHuman\nVeriﬁed\nHuman\nGenerated\nOverall\nTest\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nemrQA (Pampari et al., 2018)\n61.44\n78.82\n69.87\n83.66\n63.48\n79.99\n61.23\n78.56\n69.23\n82.83\n63.17\n79.59\nNQG (Du et al., 2017)\n64.71\n79.36\n66.99\n79.67\n65.26\n79.43\n59.49\n76.68\n67.3\n82.59\n61.38\n78.11\n+ BeamSearch\n67.07\n81.21\n71.15\n83.07\n68.07\n81.66\n63.17\n79.17\n68.91\n84.26\n64.56\n80.4\n+ QPP (Ours)\n68.82\n82.89\n74.68\n85.18\n70.09\n83.44\n63.79\n79.56\n69.23\n84.33\n65.11\n80.72\nNQG++ (Zhou et al., 2017)\n65.94\n78.71\n66.34\n81.34\n66.04\n79.35\n59.59\n75.85\n65.06\n80.11\n60.92\n76.88\n+ BeamSearch\n68.10\n80.09\n72.11\n84.56\n69.07\n81.17\n64.61\n80.30\n68.26\n83.70\n65.50\n81.12\n+ QPP (Ours)\n70.05\n83.47\n74.36\n85.92\n71.10\n84.06\n65.33\n80.64\n70.83\n85.76\n66.67\n81.88\nBERT-SQG [Chan et al., 2019]\n66.05\n79.64\n70.19\n81.47\n67.05\n80.08\n59.59\n78.04\n65.06\n82.20\n60.92\n79.05\n+ BeamSearch\n68.71\n81.98\n73.71\n84.44\n69.93\n82.58\n61.94\n79.02\n67.31\n82.54\n63.25\n79.88\n+ QPP (Ours)\n70.77\n83.60\n74.36\n85.53\n71.64\n84.07\n64.21\n80.53\n69.23\n85.38\n65.43\n81.71\nTable 2.2: QA performance on MIMIC-III test set. emrQA is also included as a baseline\ndataset to illustrate that the generated diverse questions on MIMIC-III are useful to improve\nthe QA model performance on new contexts.\nEvaluation Metrics: For QG evaluation, we focus on evaluating both relevance and diver-\nsity. Following previous work (Du et al., 2017; Zhang et al., 2018), we use BLEU (Papineni\net al., 2002), ROUGE-L (Lin, 2004) as well as METEOR (Lavie and Denkowski, 2009) for\nrelevance evaluation. Since the Beam Search and our QPP module enable QG models to\ngenerate multiple questions given an evidence, we report the top-1 relevance among the\ngenerated questions following Cho et al. (2019). For diversity, we report Distinct (Li et al.,\n2016) as well as Entropy (Zhang et al., 2018) scores. We calculate BLEU and the diversity\nmeasures based on 3- and 4-grams.\nFor QA evaluation, we report exact match (EM) (the percentage of predictions that\nmatch the ground truth answers exactly) and F1 (the average overlap between the predictions\nand ground truth answers) as in Rajpurkar et al. (2016).\n2.5.2\nResults\nTable 2.2 summarizes the performance of two widely used QA models, DocReader\n(Chen et al., 2017) and ClinicalBERT (Alsentzer et al., 2019), on the MIMIC-III test set.\nThe QA models are trained on different corpora, including the emrQA dataset as well as QA\npairs generated by different models.\n14\nModels\nRelevance\nDiversity\nBLEU3\nBLEU4\nMR\nRG\nDist3\nDist4\nEnt3\nEnt4\nNQG (Du et al., 2017)\n91.45\n90.11\n60.70\n94.62\n0.233\n0.282\n4.473\n4.738\n+ BeamSearch\n94.33\n93.42\n62.08\n95.56\n0.569\n0.775\n5.406\n5.812\n+ QPP (Ours)\n96.82\n96.33\n64.38\n97.49\n3.177\n5.289\n7.100\n7.777\nNQG++ (Zhou et al., 2017)\n97.11\n96.65\n71.57\n97.86\n0.229\n0.275\n4.419\n4.648\n+ BeamSearch\n98.35\n98.07\n72.98\n98.55\n0.618\n0.848\n5.497\n5.953\n+ QPP (Ours)\n99.15\n99.03\n74.01\n99.11\n3.183\n5.293\n7.111\n7.798\nBERT-SQG (Chan and Fan, 2019)\n89.07\n87.99\n65.25\n94.91\n0.228\n0.276\n4.594\n4.849\n+ BeamSearch\n95.45\n94.84\n66.39\n96.22\n0.510\n0.713\n5.522\n6.015\n+ QPP (Ours)\n96.54\n96.19\n67.51\n97.42\n3.344\n5.332\n7.173\n7.816\nTable 2.3: Automatic evaluation of the generated questions on emrQA dataset. For each\nbase model, the best performing variant is in bold. RG: ROUGE-L, MR: METEOR, Dist:\nDistinct, Ent: Entropy.\nWe also evaluate QG models on the emrQA dataset (i.e., train and test QG solely on\nsource domain). As can be seen from Table 2.3, the three selected base models (NQG,\nNQG++ and BERT-SQG) all achieve very promising relevance scores; however, they do not\nperform well with diversity scores. The diversity of generated questions is boosted to some\nextent when the Beam Search is used since it can offer ﬂexibility for QG models to explore\nmore candidates when decoding. In comparison, the QPP module in our framework leads to\nthe best results under both relevance and diversity evaluation. Particularly, it obtains 5%\nabsolute improvement in terms of Dist4 for each base model.\n2.6\nAnalysis\n2.6.1\nQuantitative Analysis\nAnalysis on QA Generalization: As expected, the corpora generated by diverse QG help\nthe QA model perform consistently better than those generated by their respective base\nQG version as well as emrQA (Table 2.2). Between the two diversity-boosting approaches,\nwe observe that the QA model trained on the corpora by QPP-enhanced QG achieves the\nbest performance. Moreover, results on the human-generated portion are consistently better\nthan those on human-veriﬁed. This is likely due to the fact that human-generated questions\n15\nContext: ... he was guaiac negative on admission.\nhematocrit remained stable overnight. 5. abd pain: suspect\nsecondary to chronic pancreatitis. amylase unchanged\n-emrQA: 5. abd pain\n-NQG Generated: 5. abd pain:\n-NQG+BeamSearch: 5. abd pain:\n-NQG+QPP: 5. abd pain: suspect secondary to chronic\npancreatitis.\nQA Example from MIMIC-III\nQuestion: Why did the patient get abd pain?\nAnswer by QA model trained on\n-NQG: Does the patient have any pain?\n-NQG+BeamSearch: Does the patient have any pain history? Does\nthe patient have pain? Does the patient have any pain?\n-NQG+QPP: Why did the patient have acetaminophen? What\ntreatment has the patient had for his pain? How was pain treated?\nDoes the patient have any pain? ...\nQG Example from MIMIC-III\nContext: ... the patient was taking at home prior to admission were\nnot restarted. 25. acetaminophen 325-650 mg po/ng q6h:prn pain\n26. dabigatran etexilate 150 mg po bid...\nQuestions generated by\nFigure 2.3: QA and QG examples. The red parts in contexts are ground-truth answer\nevidences.\nare more readable and sensible while human-veriﬁed ones are less natural (though the\ncorrectness is ensured). All these results indicate that improving the diversity of generated\nquestions can help better train QA models on the new contexts and better address the\ngeneralization challenge.\nAnalysis on QG diversity: Figure 2.2 shows the distribution over types of questions\ngenerated by NQG-based models (i.e., base model, base + beam search and base + QPP) and\nthe ground truth on emrQA dataset. We observe that the Kullback–Leibler (KL) divergence\nbetween the distributions of generated questions and the ground truth is smaller after enabling\ndiversity booster. The gap reaches the minimum when our QPP module is plugged in. It’s\nworth noting that even some of the least frequent types of questions (e.g., “How”, “Why”)\ncan be generated when our QPP module is turned on. These observations demonstrate\ndiversity booster, especially our QPP module, can help generate diverse questions.\n2.6.2\nQualitative Analysis: Error Analysis\nIn Figure 2.3, we ﬁrst present a QA example and a QG example from MIMIC-III.\nIn the QA example, this “why” question can be correctly answered by the QA model\n(DocReader) trained on the “NQG+QPP” generated corpus while the QA models trained\non other generated corpora fail. This is because the NQG model and “NQG+BeamSearch”\n16\ncannot generate any “why” questions as shown in Figure 2.2. Thus QA models trained on\nsuch corpora cannot answer questions of less frequent types. Though the emrQA dataset\ncontains diverse questions (including “why” questions), its contexts might be different from\nMIMIC-III in terms of topic, note structures, writing styles, etc. So the model trained on\nemrQA struggles to answer some questions. In the QG example, the base model NQG can\nonly generate one question. Though utilizing the Beam Search enables the model to explore\nmultiple candidates, the generated questions are quite similar and are less likely to help\nimprove QA. Enabling our QPP module helps generate diverse questions including “Why”,\n“What”, “How”, etc.\n2.7\nConclusion\nIn this chapter, we systematically investigate the generalization challenge of clinical\nreading comprehension and construct a new test set on MIMIC-III clinical texts. After\nobserving simply using QG for QA does not work, we explore the importance of generating\ndiverse questions. That is, we study two approaches for boosting question diversity, beam\nsearch and QPP. Particularly, our proposed QPP (question phrase prediction) module sig-\nniﬁcantly improves the cross-domain generalizability of QA systems. Our comprehensive\nexperiments allow for a better understanding of why diverse question generation can help\nQA on new clinical documents (i.e., target domain).\n17\nChapter 3: Natural Language Inference\n3.1\nIntroduction\nNatural language inference (NLI)9 is the problem of determining whether a natural\nlanguage hypothesis h can be inferred (or entailed) from a natural language premise p (i.a.,\nDagan et al., 2005; MacCartney and Manning, 2009). Conventionally, people only examine\nitems that are suitable for systematic inferences (i.e., items for which people consistently\nagree on the NLI label).\nHowever, Pavlick and Kwiatkowski (2019) observed inherent disagreements among\nannotators in several NLI datasets (e.g., SNLI (Bowman et al., 2015)), which cannot be\nsmoothed out by hiring more people. They pointed out that to achieve robust NLU, we\nneed to be able to tease apart systematic inferences (i.e., items for which most people agree\non the annotations) from items inherently leading to disagreement. The last example in\nTable 3.1 is a typical disagreement item: some annotators consider it to be an entailment (3\nor 2), while others view it as a contradiction (-3). Clearly, the annotators have two different\ninterpretations on the complement clause “If she’d said Carolyn had borrowed a book from\nClare and wanted to return it”. Moreover, a common practice in the literature to generate\nan inference label from annotations is to take the average (i.a., Pavlick and Callison-Burch,\n2016). In this case, it would be “Neutral”, but such label is not accurately capturing the\n9In this thesis, we use “textual entailment” and “Natural Language Inference” or “NLI” interchangeably.\n18\n1\nPremise: Some of them, like for instance the farm in Connecticut, are quite small. If I like a place I buy it. I guess you could say\nit’s a hobby.\nHypothesis: buying places is a hobby.\nEntailment (Entailment) [3, 3, 2, 2, 2, 2, 1, 1]\n2\nPremise: “I hope you are settling down and the cat is well.” This was a lie. She did not hope the cat was well.\nHypothesis: the cat was well.\nNeutral (Neutral) [0, 0, 0, 0, 0, 0, 0, 0, -3]\n3\nPremise: “All right, so it wasn’t the bottle by the bed. What was it, then?” Cobalt shook his head which might have meant he\ndidn’t know or might have been admonishment for Oliver who was still holding the bottle of wine.\nHypothesis: Cobalt didn’t know.\nNeutral (Disagreement) [1, 0, 0, 0, 0, 0, 0, -2]\n4\nPremise: A: No, it doesn’t. B: And, of course, your court system when you get into the appeals, I don’t believe criminal is in a\ncourt by itself.\nHypothesis: criminal is in a court by itself.\nContradiction (Contradiction) [-1, -1, -2, -2, -2, -2, -2, -3]\n5\nPremise: A: The last one I saw was Dances With The Wolves. B: Yeah, we talked about that one too. And he said he didn’t think\nit should have gotten all those awards.\nHypothesis: Dances with the Wolves should have gotten all those awards.\nContradiction (Disagreement) [0, 0, -1, -1, -2, -2, -2, -3]\n6\nPremise: Meg realized she’d been a complete fool. She could have said it differently. If she’d said Carolyn had borrowed a book\nfrom Clare and wanted to return it they ’d have given her the address.\nHypothesis: Carolyn had borrowed a book from Clare.\nDisagreement (Disagreement) [3, 3, 3, 2, 0, -3, -3, -3]\nTable 3.1: Examples from CommitmentBank, with ﬁner-grained NLI labels. The labels\nin parentheses come from Jiang and de Marneffe (2019a). Scores in brackets are the raw\nhuman annotations.\ndistribution. Alternatively, some work simply ignored the “Disagreement” portion but only\nstudied systematic inferences items (Jiang and de Marneffe, 2019b,a; Raffel et al., 2019).\nKenyon-Dean et al. (2018) also pointed out in sentiment analysis task, when performing\nreal-time sentiment classiﬁcation, an automated system cannot know a priori whether the\ndata sample is inherently non-ambiguous. Here, in line with what Kenyon-Dean et al. (2018)\nsuggested for sentiment analysis, we propose a ﬁner-grained labeling for NLI: teasing\ndisagreement items, labeled “Disagreement”, from systematic inferences, which can be\n“Contradiction”, ”Neutral” or “Entailment”. As such, in order to achieve robust NLU in NLI\ntask, the developed models should be able to identify inherent disagreement items when\npossible and carry out systematic inferences on non-disagreement items.\nTo this end, we propose Artiﬁcial Annotators (AAs), an ensemble of BERT models\n(Devlin et al., 2019a), which simulate the uncertainty in the annotation process by capturing\n19\nmodes in annotations. That is, we expect to utilize simulated modes of annotations to\nenhance ﬁner-grained NLI label prediction. Our results, on the CommitmentBank, show that\nAAs perform statistically signiﬁcantly better than all baselines (including BERT baselines)\nby a large margin in terms of both F1 and accuracy. We also show that AAs manage to learn\nlinguistic patterns and context-dependent reasoning.\n3.2\nInherently Ambiguous Items in CB\nWe start with the introduction to the dataset used in this chapter, CommitmentBank\n(de Marneffe et al., 2019), and then move on to how we determine ambiguous items and\nsystematic inference items.\nThe CommitmentBank (CB) is a corpus of 1,200 naturally occurring discourses origi-\nnally collected from news articles, ﬁction and dialogues. Each discourse consists of up to 2\nprior context sentences and 1 target sentence with a clause-embedding predicate under 4\nembedding environments (negation, modal, question or antecedent of conditional). Annota-\ntors judged the extent to which the speaker/author of the sentences is committed to the truth\nof the content of the embedded clause (CC), responding on a Likert scale from +3 to -3,\nlabeled at 3 points (+3/speaker is certain the CC is true, 0/speaker is not certain whether the\nCC is true or false, -3/speaker is certain the CC is false). Following Jiang and de Marneffe\n(2019a), we recast CB by taking the context and target as the premise and the embedded\nclause in the target as the hypothesis.\nCommon NLI benchmark datasets are SNLI (Bowman et al., 2015) and MultiNLI\n(Williams et al., 2018), but these datasets have only one annotation per item in the training\nset. CB has at least 8 annotations per item, which permits to identify items on which\nannotators disagree. Jiang and de Marneffe (2019a) discarded items if less than 80% of the\n20\nEntailment\nNeutral\nContradiction\nDisagreement\nTotal\nTrain\n177\n57\n196\n410\n840\nDev\n23\n9\n22\n66\n120\nTest\n58\n19\n54\n109\n240\nTotal\n258\n85\n272\n585\n1,200\nTable 3.2: Number of items in each class in train/dev/test.\nannotations are within one of the following three ranges: [1,3] Entailment, 0 Neutral, [-3,-1]\nContradiction. The gold label for example 3 in Table 3.1 would thus be “Disagreement”.\nHowever, this seems a bit too stringent, given that 70% of the annotators all agree on the\n0 label and there is only one annotation towards the extreme. Likewise, for example 5,\nmost annotators chose a negative score and the item might therefore be better labeled as\n“Contradiction” rather than “Disagreement”. To decide on the ﬁner-grained NLI labels, we\ntherefore also took variance and mean into account, as follows:10\n• Entailment: 80% of annotations fall in the range [1,3] OR the annotation variance ≤\n1 and the annotation mean > 1.\n• Neutral: 80% of annotations is 0 OR the annotation variance ≤1 and the absolute\nmean of annotations is bound within 0.5.\n• Contradiction: 80% of annotations fall in the range [-3, -1] OR the annotation\nvariance ≤1 and the annotation mean < -1.\n• Disagreement: Items which do not fall in any of the three categories above.\nWe randomly split CB into train/dev/test sets in a 7:1:2 ratio.11 Table 3.2 gives splits’\nbasic statistics.\n10Compared with the labeling scheme in Jiang and de Marneffe (2019a), our labeling scheme results in 59\nfewer Disagreement items, 48 of which are labeled as Neutral.\n11We don’t follow the SuperGLUE splits (Wang et al., 2019a) as they do not include disagreement items.\n21\n3.3\nLinguistic Rules\nOur developed linguistic rules are inspired by and adapted from Jiang and de Marneffe\n(2019a) to explicitly include the most discriminating expressions for disagreement items. We\nutilize three linguistic features which are provided in CB: entailment-canceling environment\n(negation, modal, question, antecedent of conditional), matrix verb and its subject person.\n1. Items under conditional are disagreement.\n2. Items under question and with second person are neutral.\n3. Items under question and with non-second person are disagreement.\n4. Items of the form ”I don’t know/think/believe” are contradiction (i.e., negRaising\nstructure).\n5. Items with factive verbs are entailment.\n6. Items under negation and with non-factive verbs are disagreement.\n7. Items under modal and with non-third person are entailment.\nWhen this policy is executed, there are two additional auxiliary rules: Items not falling\nin any group above are assigned a disagreement label as it is the dominant class in CB; For\nitems satisfying more than one rule, the label will be determined by the higher-ranked rule\n(a smaller number indicates a higher rank). Note that the rules above also reveal the most\ndiscriminating expressions for each class.\n3.4\nArtiﬁcial Annotators\nWe aim at ﬁnding an effective way to tease items leading to systematic inferences apart\nfrom items leading to disagreement. As pointed out by Calma and Sick (2017), annotated\nlabels are subject to uncertainty. Annotations are indeed inﬂuenced by several factors:\nworkers’ past experience and concentration level, cognition complexities of items, etc. They\n22\nproposed to simulate the annotation process in an active learning paradigm to make use of\nthe annotations that contribute to uncertainty. Likewise, for NLI, Gantt et al. (2020) observed\nthat directly training on raw annotations using annotator identiﬁer improves performance.\nEssentially, Gantt et al. (2020) used a mixed-effect model to learn a mapping from an item\nand the associated annotator identiﬁer to a NLI label. However, annotator identiﬁers are not\nalways accessible, especially in many datasets that have been there for a while. Thus, we\ndecide to simulate the annotation process instead of learning from real identiﬁers.\nAs shown by Pavlick and Kwiatkowski (2019), if annotations of an item follow unimodal\ndistributions, then it is suitable to use aggregation (i.e., take an average) to obtain a inference\nlabel; but such an aggregation is not appropriate when annotations follow multi-modal\ndistributions. Without loss of generality, we assume that items are associated with n-modal\ndistributions, where n ≥1. Usually, systematic inference items are tied to unimodal annota-\ntions while disagreement items are tied to multi-modal annotations. We, thus, introduce the\nnotion of Artiﬁcial Annotators (AAs), where each individual “annotator” learns to model\none mode.\n3.4.1\nArchitecture\nAAs is an ensemble of n BERT models (Devlin et al., 2019a) with a primary goal of\nﬁner-grained NLI label prediction. n is determined to be 3 as there are up to 3 relation-\nships between premise and hypothesis, excluding the disagreement class. Within AAs,\neach BERT is trained for an auxiliary systematic inference task which is to predict en-\ntailment/neutral/contradiction based on a respective subset of annotations. The subsets of\nannotations for the three BERT are mutually exclusive.\n23\nEntailment\n-biased\nContradiction\n-biased\nNeutral\n-biased\nMLP\nPREMISE [SEP] HYPOTHESIS\n𝒍𝒐𝒔𝒔𝒇\n𝒍𝒐𝒔𝒔𝒏\n𝒍𝒐𝒔𝒔𝒄\n𝒍𝒐𝒔𝒔𝒆\nFigure 3.1: Artiﬁcial Annotators (AAs) setup.\nA high-level overview of AAs is shown in Figure 3.1. Intuitively, each BERT separately\npredicts a systematic inference label, each of which represents a mode12 of the annotations.\nThe representations of these three labels are further aggregated as augmented information to\nenhance ﬁnal ﬁne-grained NLI label prediction (see Eq. 3.1).\nIf we view the AAs as a committee of three members, our architecture is reminiscent\nof the Query by Committee (QBC) (Seung et al., 1992), an effective approach for active\nlearning paradigm. The essence of QBC is to select unlabeled data for labeling on which\ndisagreement among committee members (i.e., learners pre-trained on the same labeled\ndata) occurs. The selected data will be labeled by an oracle (e.g., domain experts) and\nthen used to further train the learners. Likewise, in our approach, each AA votes for an\nitem independently. However, the purpose is to detect disagreements instead of using\ndisagreements as a measure to select items for further annotations. Moreover, in our AAs,\nthe three members are trained on three disjoint annotation partitions for each item (see\nSection 3.4.2).\n12It’s possible that three modes collapse to (almost) a point.\n24\n3.4.2\nTraining\nWe ﬁrst sort the annotations in descending order for each item and divide them into three\npartitions.13 For each partition, we generate an auxiliary label derived from the annotation\nmean. If the mean is greater/smaller than +0.5/-0.5, then it’s entailment/contradiction;\notherwise, it’s neutral. The ﬁrst BERT model is always enforced to predict the auxiliary\nlabel of the ﬁrst partition to simulate an entailment-biased annotator. Likewise, the second\nand third BERT models are trained to simulate neutral-biased and contradiction-biased\nannotators.\nEach BERT produces a pooled representation for the [CLS] token. The three represen-\ntations are passed through a multi-layer perceptron (MLP) to obtain the ﬁner-grained NLI\nlabel:\nP(y|x) = softmax(Ws tanh(Wt[e; n; c]))\n(3.1)\nwith [e; n; c] being the concatenation of three learned representations out of entailment-\nbiased, neutral-biased and contradiction-biased BERT models. Ws and Wt are parameters\nto be learned.\nThe overall loss is deﬁned as the weighted sums of four cross-entropy losses:\nloss = r ∗lossf + 1 −r\n3\n(losse + lossn + lossc)\n(3.2)\nwhere r ∈[0, 1] controls the primary ﬁner-grained NLI label prediction task loss ratio.\n3.5\nEvaluation and Results\nEvaluation Setting: We include ﬁve baselines to compare with:\n• “Always 0”: Always predict Disagreement.\n13For example, if there are 8 annotations for a given item, the annotations are divided into partitions of size\n3, 2 and 3.\n25\nDev\nTest\nAcc.\nF1\nAcc.\nF1\nEntail\nNeutral\nContradict\nDisagree\nAlways 0\n55.00\n39.03\n45.42\n28.37\n0.00\n0.00\n0.00\n62.46\nCBOW\n55.25\n40.54\n45.09\n28.37\n0.00\n0.00\n0.69\n62.17\nHeuristic\n65.00\n62.08\n54.17\n50.60\n22.54\n52.94\n64.46\n58.20\nVanilla BERT\n63.71\n63.54\n62.50\n61.93\n59.26\n49.64\n69.09\n61.93\nJoint BERT\n64.47\n64.28\n62.61\n62.07\n59.77\n47.27\n67.36\n63.21\nAAs (ours)\n65.15\n64.41\n65.60*\n64.97*\n61.07\n51.27\n70.89\n66.49*\nTable 3.3: Baselines and AAs overall performance on CB dev and test sets, and F1 scores of\neach class on the test set (average of 10 runs). * indicates statistically signiﬁcant difference\n(t-test, p ≤0.01).\n• CBOW (Continuous Bags of Words): Each item is represented as the average of its\ntokens’ GLOVE vectors (Pennington et al., 2014).\n• Heuristic baseline: Linguistics-driven rules (detailed out in chapter 3.3), adapted from\nJiang and de Marneffe (2019a); e.g., conditional environment discriminates for disagree-\nment items.\n• Vanilla BERT: (Devlin et al., 2019a) Straightforwardly predict among 4 ﬁner-grained\nNLI labels.\n• Joint BERT: Two BERT models are jointly trained, each of which has a different\nspeciality. The ﬁrst one (2-way) identiﬁes whether a sentence pair is a disagreement\nitem. If not, this item is fed into the second BERT (3-way) which carries out systematic\ninference.\nFor all baselines involving BERT, we follow the standard practice of concatenating the\npremise and the hypothesis with [SEP].\nResults: Table 3.3 gives the accuracy and F1 for each baseline and AAs, on the CB dev and\ntest sets. We run each model 10 times, and report the average. Also, Our AAs achieve the\nlowest standard deviations on test set items compared to BERT-based models, indicating\nthat it is more stable and potentially more robust to wild environments.\n26\n3.6\nAnalysis\n3.6.1\nEmpirical Results Analysis\nCBOW is essentially the same as the “Always 0” baseline as it keeps predicting Dis-\nagreement regardless of the input. The Heuristic baseline achieves competitive performance\non the dev set, though it has a signiﬁcantly worse result on the test set. Not surprisingly, both\nBERT-based baselines outperform the Heuristic on the test set: ﬁne-tuning BERT often lead\nto better performance, including for NLI (Peters et al., 2019; McCoy et al., 2019). These\nobservations are consistent with Jiang and de Marneffe (2019a) who observed a similar\ntrend, though only on systematic inferences. Our proposed AAs perform consistently better\nthan all baselines, and statistically signiﬁcantly better on the test set (t-test, p ≤0.01).\nTable 3.3 also gives F1 for each class on the test set. AAs outperform all BERT-based\nmodels under all classes. However, compared with the Heuristic, AAs show an inferior\nresult on “Neutral” items mainly due to the lack of “Neutral” training data. The ﬁrst 4\nexamples in Table 3.4 show examples for which AAs make the correct prediction while other\nbaselines might not. The confusion matrix in Table 3.5 shows that the majority (∼60%) of\nerrors come from wrongly predicting a systematic inference item as a disagreement item.\nIn 91% of such errors, AAs predict that there is more than one mode for the annotation\n(i.e., the three labels predicted by individual “annotators” in AAs are not unanimous), as\nin example 5 in Table 3.4. AAs are thus predicting more modes than necessary when the\nannotation is actually following a uni-modal distribution. On the contrary, when the item\nis supposed to be a disagreement item but is missed by AAs (as in example 6 and 7 in\nTable 3.4), AAs mistakenly predict that there is only one mode in the annotations 78% of\nthe time. It thus seems that a method which captures accurately the number of modes in the\nannotation distribution would lead to a better model.\n27\n1\nPremise: B: Yeah, it is. A: For instance, B: I’m a historian, and my father had kept them, I think, since nineteen twenty-seven uh,\nbut he burned the ones from twenty-seven to ﬁ-, A: My goodness. B: I could not believe he did that,\nHypothesis: his father burned the ones from twenty-seven\nHeuristics: C\nV. BERT: D\nJ. BERT: E\nAAs: E {E, E, E}\nGold: E [3, 3, 3, 3, 3, 2, 2, -1]\n2\nPremise: ‘She was about to tell him that was his own stupid fault and that she wasn’t here to wait on him - particularly since he\nhad proved to be so inhospitable. But she bit back the words. Perhaps if she made herself useful he might decide she could stay -\nfor a while at least just until she got something else sorted out.\nHypothesis: she could stay\nHeuristics: D\nV. BERT: D\nJ. BERT: D\nAAs: N {N, N, N}\nGold: N [3, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n3\nPremise: A: but that is one of my solutions. Uh... B: I know here in Dallas that they have just instituted in the last couple of\nyears, uh, a real long period of time that you can absentee vote before the elections. And I do not think they have seen a really\nhigh improvement.\nHypothesis: they have seen a really high improvement.\nHeuristics: C\nV. BERT: C\nJ. BERT: C\nAAs: C {C, C, C}\nGold: C [-1, -2, -2, -2, -2, -2, -2, -2, -3, -3]\n4\nPremise:B: So did you commute everyday then or, A: No. B: Oh, okay. A: No, no, it was a six hour drive. B: Oh, okay, when\nyou said it was quite a way away, I did not know that meant you had to drive like an hour\nHypothesis: speaker A had to drive like an hour\nHeuristics: C\nV. BERT: D\nJ. BERT: E\nAAs: D {E, C, C}\nGold: D [3, 2, 2, 1, 0, 0, -1, -1, -1, -3]\n5\nPremise: The assassin’s tone and bearing were completely conﬁdent. If he noticed that Zukov was now edging further to the side\nwidening the arc of ﬁre he did not appear to be troubled.\nHypothesis: Zukov was edging further to the side\nHeuristics: D\nV. BERT: D\nJ. BERT: D\nAAs: D {E, E, N}\nGold: E [3, 3, 3, 3, 2, 2, 1, 1]\n6\nPremise: B: Yeah, and EDS is very particular about this, hair cuts, A: Wow. B: I mean it was like you can’t have, you know, such\nand such facial hair, no beards, you know, and just really detailed. A: A: I don’t know that that would be a good environment to\nwork in.\nHypothesis: that would be a good environment to work in\nHeuristics: C\nV. BERT: C\nJ. BERT: D\nAAs: C {C, C, C}\nGold: D [2, 0, 0, 0, 0, -1, -2, -3]\n7\nPremise: “Willy did mention it. I was puzzled, I ’ll admit, but now I understand.” How did you know Heather had been there?\nHypothesis: Heather had been there\nHeuristics: N\nV. BERT: E\nJ. BERT: E\nAAs: E {E, E, E}\nGold: D [3, 3, 3, 2, 1, 1, 0, 0, 0]\nTable 3.4: Models’ predictions for CB test items. Labels in [] are predictions by individual\nAAs.\nPredict\nGold\nE\nN\nC\nD\nTotal\nE\n37\n2\n0\n13\n52\nN\n1\n10\n0\n3\n14\nC\n0\n0\n34\n13\n47\nD\n20\n7\n20\n80\n127\nTotal\n58\n19\n54\n109\n240\nTable 3.5: Confusion matrix for the test set. E: entailment, N: neutral, C: contradiction, D:\ndisagreement.\n28\nnegation\nmodal\nconditional\nquestion\nnegR\nHeuristic\n51.29\n48.02\n37.69\n44.64\n54.16\nV. BERT\n60.91\n73.98\n44.84\n53.02\n61.91\nJ. BERT\n60.94\n73.95\n46.02\n51.68\n63.67\nAAs\n65.96\n80.18\n48.05\n54.95\n68.00\nTable 3.6:\nF1 for CB test set under the embedding environments and “I don’t\nknow/believe/think” (“negR”).\n3.6.2\nLinguistic Construction Analysis\nWe also examine the model performance for different linguistic constructions to in-\nvestigate whether the model learns some of the linguistic patterns present in the Heuristic\nbaseline. The Heuristic rules are strongly tied to the embedding environments. Another\nconstruction used is one which can lead to “neg-raising” reading, where a negation in the\nmatrix clause is interpreted as negating the content of the complement, as in example 3\n(Table 3.4) where I do not think they have seen a really high improvement is interpreted as I\nthink they did not see a really high improvement. “Neg-raising” readings often occur with\nknow, believe or think in the ﬁrst person under negation. There are 85 such items in the test\nset: 41 contradictions (thus neg-raising items), 39 disagreements and 5 entailments. Context\ndetermines whether a neg-raising inference is triggered (An and White, 2019).\nTable 3.6 gives F1 scores for the Heuristic, BERT models and AAs for items under the\ndifferent embedding environments and potential neg-raising items in the test set. Though\nAAs achieve the best overall results, it suffers under conditional and question environments,\nas the corresponding training data is scarce (9.04% and 14.17%, respectively). The Heuristic\nbaseline always assigns contradiction to the “I don’t know/believe/think” items, thus captur-\ning all 41 neg-raising items but missing disagreements and entailments. BERT, a SOTA NLP\nmodel, is not great at capturing such items either: 71.64 F1 on contradiction vs. 52.84 on the\nothers (Vanilla BERT); 71.69 F1 vs. 56.16 (Joint BERT). Our AAs capture neg-raising items\n29\nCorrect inference\nby Heuristic?\nYes (130)\nNo (110)\nAcc.\nF1\nAcc.\nF1\nV. BERT\n80.00\n80.45\n41.51\n42.48\nJ. BERT\n79.74\n80.04\n42.73\n44.15\nAAs\n84.37\n84.85\n46.97\n48.75\nTable 3.7: BERT-based models performance on test items correctly predicted by vs. items\nmissed by linguistic rules. Numbers next to Yes/No denote the size.\nbetter with 77.26 F1 vs. 59.38, showing an ability to carry out context-dependent inference\non top of the learned linguistic patterns. Table 3.7, comparing performance on test items\ncorrectly predicted by the linguistic rules vs. items for which context-dependent reasoning\nis necessary, conﬁrms this: AAs outperform the BERT baselines in both categories.\n3.7\nConclusion\nIn this chapter, we introduced ﬁner-grained natural language inference. This task aims at\nteasing systematic inferences from inherent disagreements. The inherent disagreement items\nare challenging for NLU models to handle, rarely studied in past NLI work. We show that\nour proposed AAs, which simulate the uncertainty in annotation process by capturing the\nmodes in annotations, perform statistically signiﬁcantly better than all baselines. However\nthe performance obtained (∼66%) is still far from achieving truly robust NLU, leaving room\nfor improvement.\n30\nChapter 4: FAQ Retrieval\n4.1\nIntroduction\nFAQ, short for frequently asked questions, is designed for the purpose of providing\ninformation on frequent questions or concerns. The FAQ retrieval task is deﬁned as ranking\nFAQ items {(qi, ai)} from an FAQ Bank given a user query Q. In the FAQ retrieval literature\n(Karan and ˇSnajder, 2016, 2018; Sakata et al., 2019), a user query Q can be learned to match\nwith the question ﬁeld qi, the answer ﬁeld ai or their concatenation (i.e., FAQ tuple) qi + ai.\nTo advance the COVID-19 information search, we present an FAQ dataset, COUGH,\nconsisting of FAQ Bank, Query Bank, and Relevance Set, following the standard of con-\nstructing a robust FAQ dataset (Manning et al., 2008). The FAQ Bank contains 15919 FAQ\nitems scraped from 55 authoritative institutional websites. COUGH covers a wide range of\nperspectives on COVID-19, spanning from general information about the virus to speciﬁc\nCOVID-19-related instructions for a healthy diet. For evaluation, we further construct Query\nBank and Relevance Set, including 1201 crowd-sourced queries and their relevance to a set\nof FAQ items judged by annotators. Examples from COUGH are shown in Figure 4.1.\nOur dataset poses several new challenges (e.g., the answers being long and noisy, and\nhard to match due to larger search space) to existing FAQ retrieval models. The diversity\nof FAQ items, which is reﬂected in their varying query forms and lengths as well as in\nnarrative styles, also contributes to these challenges. Furthermore, these challenges can\n31\nFigure 4.1: Examples from the COUGH dataset.\nreﬂect the characteristics and difﬁculties of FAQ retrieval in real scenarios better than\ncounterparts like FAQIR (Karan and ˇSnajder, 2016) and StackFAQ (Karan and ˇSnajder,\n2018) (Table 4.1). Moreover, in contrast to all prior datasets, COUGH covers multiple query\nforms (e.g., question and query string forms) and has many annotated FAQs for each user\nquery, whereas queries in existing FAQ datasets are limited to the question form and have\nmuch fewer annotations. As such, our COUGH is deemed as a robust dataset, upon which\na robust FAQ retriever could be developed to handle some real challenges (e.g., lengthy\nanswer, enormous search space) better.\nThe contribution in this chapter is two-fold. First, we construct a challenging dataset\nCOUGH to aid the development of COVID-19 FAQ retrieval models. Second, we conduct ex-\ntensive experiments using various SOTA models across different settings, explore limitations\nof current FAQ retrieval models, and discuss future work along this line.\n4.2\nStandard FAQ Dataset Construction: COUGH\nSince the outbreak of COVID-19, the community has witnessed many datasets released\nto advance the research of COVID-19.The most related work to ours are Sun and Sedoc\n(2020) and Poliak et al. (2020), both of which constructed a collection of COVID-19 FAQs\n32\nFAQIR\n(Karan and ˇSnajder)\nStackFAQ\n(Karan and ˇSnajder)\nLocalGov\n(Sakata et al.)\nSun and Sedoc\nPoliak et al.\nCOUGH (ours)\nDomain\nYahoo!\nStackExchange\nGovernment\nCOVID-19\nCOVID-19\nCOVID-19\n# of FAQs\n4,313\n719\n1,786\n690\n2,115\n15,919\n# of Queries (Q)\n1,233\n1,249\n784\n6,495*\n24,240*\n1,201\n# of annotations per Q\n8.22\nNot Applicable\n<10\n5\n5\n32.17\nQuery Length\n7.30\n13.84\n**\n**\n**\n12.97\nFAQ-query Length\n12.30\n10.39\n**\n**\n**\n13.00\nFAQ-answer Length\n33.00\n76.54\n**\n**\n**\n113.58\nLanguage\nEnglish\nEnglish\nJapanese\nEnglish\nMulti-lingual\nMulti-lingual\n# of sources\n1\n1\n1\n12\n34\n55\nTable 4.1: Comparison of COUGH with representative counterparts. *: Extracted from\nexisting resources (e.g., COVID-19 Twitter dataset (Chen et al., 2020)). **: Not Applicable,\neither not in English or not publicly available.\nby scraping authoritative websites (e.g., CDC and WHO). However, the dataset in the former\nwork is not available yet and the latter work does not evaluate models on their dataset,\nand there is still a great need to understand how existing models would perform on the\nCOVID-19 FAQ retrieval task. Moreover, the numbers of FAQs14 in the 5 existing FAQ\ndatasets (Table 4.1) are generally lower than 2000, which renders a small search space and\nthus the ease for FAQ retrievers to ﬁnd the most relevant FAQ given a query.\nA typical research-oriented FAQ dataset (Manning et al., 2008) consists of three parts:\nFAQ Bank, User Query Bank and Annotated Relevance Set. In this section, we will describe\nhow we construct each of the three in detail.\n4.2.1\nFAQ Bank Construction\nWe developed scrapers based on JHU-COVID-QA library Poliak et al. (2020) with\nmodiﬁcations to enable special features for our COUGH dataset.\nWeb scraping: We collect FAQ items from authoritative international organizations, state\ngovernments and some other credible websites including reliable encyclopedias and medical\nforums. Moreover, we scrape three types of FAQs: question form (i.e., an interrogative\nstatement), query string (i.e., a string of words to elicit information) form and forum form\n14In the literature, only 789 FAQ items are used for evaluation on FAQIR (Karan and ˇSnajder, 2018; Mass\net al., 2020).\n33\n(FAQs scrapped from medical forums). Inspired by Manning et al. (2008), we loosen the\nconstraint that queries must be in question form since we want to study a more generic and\nchallenging problem. We also scrape 6,768 non-English FAQs to increase language diversity.\nOverall, we scraped a total of 15,919 FAQ items covering all three types and 19 languages.\nAll FAQ items were collected and ﬁnalized on Aug. 30th, 2020.\n4.2.2\nUser Query Bank Construction\nFollowing Karan and ˇSnajder (2016); Manning et al. (2008), we do not crowdsource\nqueries from scratch, but instead ask annotators to paraphrase our provided query templates\n(See phase 1 below for details). In this way, we can ensure that 1) the collected queries are\npertinent to COVID-19; 2) the collected queries are not too simple; 3) the chance of getting\n(nearly) duplicate user queries is reduced.\nPhase 1: Query Template Creation: We sample 5% of FAQ items from each English\nnon-forum source and use the question part as the template.\nPhase 2: Paraphrasing for Queries: In this phase, each annotator is expected to give three\nparaphrases for each query template. Annotators are encouraged to give deep paraphrases\n(i.e., grammatically different but semantically similar/same) to simulate the noisy and diverse\nenvironment in real scenarios. In the end, we obtain 1236 user queries.\n4.2.3\nAnnotated Relevance Set Construction\nPhase 1: Initial Candidate Pool Construction: For each user query, as suggested by\nprevious work (Manning et al., 2008; Karan and ˇSnajder, 2016; Sakata et al., 2019), we\nrun 4 models15, BM25 (Q-q), BM25 (Q-q+a), BERT (Q-q) and BERT (Q-a) ﬁne-tuned\non COUGH, to instantiate a candidate FAQ pool. Each model complements the others and\n15Explanations of these models are in chapter 4.4.2.\n34\nType\nNumber\nQ-Length\nA-length\n# English\nQuestion\n4978\n14.64\n123.89\nQuery String\n2139\n9.18\n89.60\nForum\n2034\n147.46\n90.49\n# Non-English\nQuestion\n3396\n-\n-\nQuery String\n3372\n-\n-\n# Total\n-\n15919\n-\n-\nTable 4.2: Basic statistics of FAQ bank in COUGH.\ncontributes its top-10 relevant FAQ items. We then take the union to remove duplicates,\ngiving an average pool size of 32.2.\nPhase 2: Human Annotation: Each annotator gives each ⟨Query, FAQ item⟩tuple a score\nbased on the annotation scheme (i.e., Matched (4), Useful (3), Useless (2) and Non-relevant\n(1)) which is adapted from Karan and ˇSnajder (2016); Sakata et al. (2019). In order to\nreduce the variance and bias in annotation, each tuple has at least 3 annotation scores. In\nour ﬁnalized Annotated Relevance Set, we keep all raw scores and include two additional\nlabels: 1) mean of raw annotation scores; 2) binary label (positive/negative). We identify all\ntuples with mean score greater than 3 as positive examples.\nAmong 1236 user queries, we ﬁnd that there are 35 “unanswerable” queries that have no\nassociated positive FAQ item. In the end, there are 1201 user queries involved for evaluation\nafter removing “unanswerable” queries.\n4.3\nCOUGH Dataset Analysis\nBesides the generic goal of large size, diversity, and low noise, COUGH features 4\nadditional aspects:\nVarying Query Forms: As indicated in Table 4.2, there are multiple query forms. In\nevaluation, we include both question and query string forms. These two distinct forms are\ndifferent in terms of query format (interrogative vs. declarative), average answer length\n35\n(123.89 vs. 89.60) and topics. Question form is usually related to general information about\nthe virus while query string form is often searching for more speciﬁc instructions concerning\nCOVID-19 (e.g., healthy diet during pandemic). In Figure 4.1, the ﬁrst FAQ item is in\nquestion form while the second one is in query string form.\nAnswer Nature: Table 4.1 shows the answer ﬁelds in COUGH are much longer than those\nin any prior dataset. We also observe that answers might contain some contents which are\nnot directly pertinent to the query, partially resulting in the long length nature. For example,\nin COUGH, the answer to a query “What is novel coronavirus” contains extra information\nabout comparisons with other viruses. Such lengthy and noisy nature of answers shows the\ndifﬁculty of FAQ retrieval in real scenarios.\nLarge-scale Relevance Annotation: Many existing FAQ datasets overlooked the scale\nof annotations (Table 4.1); yet, that would hurt the evaluation reliability since many true\npositive ⟨Query, FAQ item⟩tuples were omitted. Following Manning et al. (2008), for each\nuser query, we constructed a large-scale candidate pool to reduce the chance of missing\ntrue positive tuples. The annotation procedure yielded 39760 annotated ⟨Query, FAQ item⟩\ntuples, each of which is annotated by at least 3 people to reduce annotation bias. Furthermore,\nwe ﬁnd that there are 7856 (19.76%) positive tuples (i.e., mean score > 3). Besides, from\nthe perspective of FAQ Bank, 6648 of 7117 English non-forum items appear at least once in\nInitial Candidate Pool, and 3790 of them have at least one “matched” user query.\nMultilinguality: COUGH includes 6768 FAQ items covering 18 non-English languages. In\nthis thesis, we do not include FAQ items in languages other than English in the evaluation.16\nHowever, we do encourage investigators who use COUGH to better utilize non-English FAQ\n16No annotation is done on non-Engligh items.\n36\nFigure 4.2: Language distribution for non-English FAQ items.\nitems for other potential tasks, such as multi-lingual FAQ retrieval and transfer learning\nfrom English FAQ items to low-resource non-English FAQ items.\nFigure 4.2 shows the language distribution (excluding English) of FAQ items in COUGH\ndataset. Like English FAQ items, non-English FAQ items are also presented in both question\nand query string forms. Statistics of non-English items can be found in Table 4.2.\n4.4\nFAQ Retrieval Methods\n4.4.1\nFAQ Retrieval Methods Overview\nThe standard practice in FAQ retrieval focuses on retrieving the most-matched FAQ\nitems given a user query (Karan and ˇSnajder, 2018). Many earlier work, such as FAQ\nFINDER (Burke et al., 1997), query expansion (Kim and Seo, 2006) and BM25 (Robertson\nand Zaragoza, 2009), resorted to traditional IR techniques by leveraging lexical mapping\nand/or semantic similarity. In the deep learning era, many studies show that Neural Networks\n37\nare useful for FAQ retrieval as they are good at learning the semantic relevance between\nqueries and FAQ items. Along this line, (Karan and ˇSnajder, 2016) adopted Convolution\nNeural Networks, (Gupta and Carvalho, 2019) utilized LSTM, and (Sakata et al., 2019)\nleveraged an ensemble of TSUBAKI (Shinzato et al., 2012) and BERT (Devlin et al., 2019b).\nRecently, Mass et al. (2020) employed CombSum and PoolRank, ensembles of BM25 and\nBERT models, to learn ranking without requiring manual annotations.\n4.4.2\nUnsupervised FAQ Retrieval\nIn this chapter, we only focus on the unsupervised models since the size of User Query\nBank (1201 items) is not large enough for supervised learning, especially for ﬁne-tuning\ncomplex language models like BERT. We experiment with three commonly-used and SOTA\nunsupervised models to understand their limitations and ﬁgure out the challenge present in\nreal scenarios for FAQ retrieval. Besides, each model has three conﬁgurable modes, Q-q,\nQ-a and Q-q+a, where we match user queries (Q) to the question (q) and answer (a) of an\nFAQ item as well as their concatenation (q+a)17, respectively.\nBaseline Models\n(1) BM25 (Robertson and Zaragoza, 2009), a commonly adopted IR baseline, is a nonlinear\ncombination of term frequency, document frequency and document length.\n(2) BERT (Devlin et al., 2019b) is a pretrained language model. We experiment with\nSentence-BERT (Reimers and Gurevych, 2019), a Siamese network built for comparison\nbetween sentence-pair embeddings, which specializes in generating meaningful sentence\nrepresentations.\n17Q-q+a mode is only used for BM25 and BM25 in CombSum.\n38\nFine-tuning: We use Multiple Negatives Ranking (MNR) loss18 (Henderson et al., 2017) to\nﬁne-tune Sentence-BERT on FAQ bank. For the Q-q mode, similar to Mass et al. (2020),\nwe use GPT2 (Radford et al., 2019) to generate synthetic questions as positive q’s to match\nwith Q and ﬁlter out low-quality ones via Elasticsearch. For the Q-a mode, an FAQ item\nitself is a positive pair. For both modes, negative q’s or a’s are randomly sampled.\n(3) CombSum (Mass et al., 2020) ﬁrst computes three matching scores between the user\nquery and FAQ items via BM25 (Q-q+a), BERT (Q-q) and BERT (Q-a) models, respectively.\nThen, the three scores are normalized and combined by averaging.\n4.5\nEvaluation and Results\nEvaluation Metric: We adopt our binary label (positive/negative) as ground truth labels.\nFollowing previous work (Karan and ˇSnajder, 2016, 2018; Sakata et al., 2019), we adopt\nwidely-used MAP (Mean Average Precision)19, MRR (Mean Reciprocal Rank) and P@5\n(Precision at top 5) metrics.\nEvaluation Settings: For the scope of this chapter, we only evaluate on English non-\nforum FAQ items, and leave the non-English and forum ones for future research as great\nchallenges have already been observed under the current setting. However, we do encourage\ninvestigators who use COUGH to utilize these two categories for other potential applications\n(e.g., multi-lingual IR, transfer learning in IR).\nEvaluation Results: Models’ results are listed in Table 4.3. The current best results (P@5:\n0.31; MAP: 0.42; MRR: 0.64) are not satisfying, showing a large room for improvement.\n18For efﬁciency, MNR loss is computed using answers of other FAQs in the same training batch as negative\nanswers.\n19Evaluated on top-100 retrieved FAQ items.\n39\nP@5\nMAP\nMRR\nBM25 (Q-q)\n0.27\n0.38\n0.56\nBM25 (Q-a)\n0.16\n0.23\n0.34\nBM25 (Q-q+a)\n0.25\n0.34\n0.52\nBERT (Q-q) w/o ﬁnetune\n0.29\n0.42\n0.59\n+ ﬁnetune on pesudo Q-q\n0.26\n0.36\n0.60\nBERT (Q-a) w/o ﬁnetune\n0.06\n0.12\n0.17\n+ ﬁnetune on FAQ Bank\n0.23\n0.30\n0.50\nCombSum w/o ﬁnetune\n0.21\n0.31\n0.49\n+ ﬁntune on pesudo Q-q\n0.23\n0.31\n0.53\n+ ﬁntune on FAQ Bank\n0.31\n0.39\n0.63\n+ ﬁntune on pesudo Q-q and FAQ Bank\n0.31\n0.39\n0.64\nTable 4.3: Evaluation on COUGH. BERT refers to Sentence-BERT (Reimers and Gurevych,\n2019).\nThese results not only conﬁrm that COUGH is challenging but also signify more robust\nmethods and models are needed to handle challenges imposed by COUGH more effectively.\n4.6\nAnalysis\nQuantitative Analysis: It is not surprising to see that the Q-q mode consistently performs\nbetter than the Q-a mode regardless of underlying models. This is mainly caused by the fact\nthat question ﬁelds are more similar to user queries than answer ﬁelds, in terms of syntactic\nstructures and semantic meanings. As discussed in Section 4.3, the answer nature (lengthy\nand noisy) and large search space, albeit well characterize the FAQ retrieval task in real\nscenarios, do bring a great challenge to current FAQ retrieval models.\nWe observe that ﬁne-tuning in the way we experimented with can only help improve the\nperformance of the Q-a mode by a small margin, but might slightly hurt the Q-q mode due\nto the noise introduced in generating synthetic queries. Moreover, ensemble models don’t\nperform as well as expected, since the particular Q-a model involved is weak (even after\nﬁne-tuning), which negatively impacts performance. In consequence, doing straightforward\nﬁne-tuning or ensemble simply by stacking models wouldn’t improve the performance\n40\nQuery: What research is being done on antibody tests and their accuracy?\nFAQ item: Q: What is antibody testing? How do I get a COVID-19 antibody test? A: CDC and partners are\ninvestigating to determine if you can get sick with COVID-19 more than once ...\nGold label: Negative [useful, useless, useless]\nPredicted rank: 3\nQuery: Are COVID-19 antibody tests accurate?\nFAQ item: Q: Should I be tested with an antibody (serology) test for COVID-19? A: ... Antibody tests have\nlimited ability to diagnose COVID-19 and should not be used alone to diagnose COVID-19 ...\nGold label: Positive [useful, useful, matched]\nPredicted rank: 26\nTable 4.4: Error analysis with ﬁne-tuned BERT (Q-q). Human annotations are inside [].\nsigniﬁcantly, which conﬁrms that COUGH is a challenging dataset. Interesting future work\nincludes developing more advanced techniques to handle long and noisy answer ﬁelds.\nQualitative Analysis: To understand ﬁnetuned BERT (Q-q) better, we conduct error anal-\nysis as shown in Table 4.4 to show its major types of errors, hoping to further improve\nit in the future. Currently, ﬁnetuned BERT (Q-q) suffers from the following issues: 1)\nbiased towards responses with similar texts (e.g., “antibody tests” and “antibody testing”);\n2) fails to capture the semantic similarities under complex environments (e.g., pragmatic\nreasoning is required to understand that “limited ability” indicates results are not accurate\nfor diagnosing COVID-19).\n4.7\nConclusion\nIn this chapter, we introduce COUGH, a large challenging dataset for COVID-19 FAQ\nretrieval. COUGH features varying query forms, long and noisy answers, larger search space\nand multilinguality. COUGH also serves as a better evaluation benchmark since it has large-\nscale relevance annotations. Albeit results show the limitations of current FAQ retrieval\nmodels, COUGH is a more robust dataset than its counterparts since it better characterizes\nthe challenges present in real scenarios for FAQ retrieval.\n41\nChapter 5: Conclusion\nIn this thesis, I have embarked on building models and constructing datasets towards\nmore robust natural language understanding. We start with a discussion on what robustness\nproblem is in natural language understanding. That is, fully-trained NLU models are usually\nlacking generalizability and ﬂexibility. In this thesis, we argue that, in order to achieve truly\nrobust natural language understanding, implementing robust models and curating robust\ndatasets are equally important. In this thesis, we investigate the NLU robustness problem in\nthree NLU tasks (i.e., Question Answering, Natural Language Inference and Information\nRetrieval). We then propose novel methods and construct new datasets to advance research\non improving the robustness of NLU systems.\nIn Chapter 2, we study how to utilize diversity boosters (e.g., beam search & QPP) to\nhelp Question Generator synthesize diverse QA pairs, upon which a Question Answering\n(QA) system is trained to improve the generalization onto unseen target domain. It’s worth\nmentioning that our proposed QPP (question phrase prediction) module, which predicts a\nset of valid question phrases given an answer evidence, plays an important role in improving\nthe cross-domain generalizability for QA systems. Besides, a target-domain test set is\nconstructed and approved by the community to help evaluate the model robustness under\ncross-domain generalization setting. In Chapter 3, we investigate inherently ambiguous\nitems in the NLI (Natural Language Inference) task, which are overlooked in the literature\n42\nbut often occurring in the real world, for which annotators don’t agree with the gold label.\nWe build an ensemble model, AAs (Artiﬁcial Annotators), which simulates underlying\nannotation distribution to effectively identify such inherently ambiguous items. Our AAs,\nmotivated by the nature of inherently ambiguous items, are better than vanilla models since\nour model design captures the essence of the problem better. In Chapter 4, we follow a\nstandard practice to build a robust dataset for FAQ retrieval task. In our dataset analysis, we\nshow how COUGH better reﬂects the challenge of FAQ retrieval in the real situation than its\ncounterparts. The imposed challenge (e.g., long and noisy answer, large search space) will\npush forward the boundary of research on FAQ retrieval in real scenarios.\nOverall, the technical contributions of this thesis are as follows:\n1.\nWe investigate the robustness problem in depth, and identify the equal importance of\nmodels implementation and datasets construction towards improving the robustness\nof NLU systems. In this thesis, we speciﬁcally study three concrete NLU tasks.\n2.\nWe propose two novel methods to help improve NLU model robustness. Speciﬁcally,\nwe evaluate the effect of diverse question generation (QG) for clinical QA under\nthe cross-domain evaluation setting, and propose QPP (Question Phrase Prediction)\nmodule as an effective diversity booster for QG (Yue et al., 2021). Moreover, we\npropose AAs (Artiﬁcial Annotators) to simulate underlying annotation distribution to\nhandle a previously-overlooked NLI class better, inherent disagreement items (Zhang\nand de Marneffe, 2021).\n3. We construct two robust datasets, QA test set on MIMIC-III Database (Yue et al.,\n2021) and COUGH (Zhang et al., 2021). They will serve as better evaluation\nbenchmarks to examine designed models’ generalization capabilities and abilities to\nhandle real-scenario challenges (e.g., longer FAQ and larger search space).\n43\nFuture Research: Moving forward, the ultimate goal for robust natural language under-\nstanding is to build NLU models which can behave humanly. That is, it’s expected that\nrobust NLU models are capable to transfer the knowledge from training corpus to unseen\ndocuments more reliably and survive when encountering challenging items even if the model\ndoesn’t know a priori of users’ inputs. Two suggested important research frontiers are:\n1) Improve model generalization under cross-domain setting: In Chapter 2, we dis-\ncussed how we utilized QG model to help alleviate the generalization challenge encountered\nby QA systems. However, the question whether a better QA system could further improve\nthe QG is yet known, which is, however, worth deeper investigation. Ideally, when intro-\nducing an auxiliary module to help the main model, we also expect to see that the auxiliary\nmodule could be beneﬁted by the joint training with the main model. Besides, in Chapter 2,\nthe reason we decided to utilize QG that way is that we observed that the QG system didn’t\nsuffer from severe generalization issues under the clinical setting. However, in open-domain,\nthe aforementioned observation might not hold. In that case, it might be better to enforce\nthe model to learn text representations that are invariant to domain changes. Recent work on\ncross-domain NER (Named Entity Recognition) have shown some progress along this path\n(Jia et al., 2019). I also have a great interest in text generation. Though the majority of work\nthat utilize domain adaptation techniques to tackle the generalization challenge focuses on\nclassiﬁcation tasks (Ganin et al., 2016; Chen and Cardie, 2018; Chen et al., 2018), could\nwe effectively extend the success of domain adaptation to text generation? This might be a\npromising research direction since the text generation can be formulated as a sequence of\nclassiﬁcations.\n2) Embrace more challenges in NLU: In Chapter 3 and 4, we discussed two datasets,\nCommitmentBank & COUGH, on which we could develop methods that target at solving\n44\nNLU challenges under more realistic scenarios. SQuAD 2.0 (Rajpurkar et al., 2018) is\na another great role model for datasets that aim at this goal. To do well on SQuAD 2.0,\nmodels must not only answer questions when possible, but also determine when no answer\nis supported by the paragraph and then say “no”. This is a real challenge for QA system as\nit’s not always the case that an answer could be found in a seemingly relevant document for\na question. Another typical real challenge in NLU is how to solve mathematical problems.\nHendrycks et al. (2021) presents a new math dataset on which a standard CS PhD student\nwho doesn’t especially like Math gets 40% accuracy while a fully-trained GPT-3 (Brown\net al., 2020) models only gets 5%. Pretrained language models like GPT-3 or BERT\nis believed to heavily rely on the context to reason about the given prompt. However,\nmathematical language isn’t necessarily constrained by contexts,20 which imposes a great\nchallenge to NLU systems. Additionally, in order to get full credits for a problem, the\ndeployed system is also required to give correct reasoning steps, which is way more difﬁcult\nthan simply generating an answer. The following is an example from MATH dataset:21\nProblem:\nIf Σ∞\nn=0cos2nθ = 5, what is cos2θ?\nSolution: The geometric series is 1 + cos2θ + cos4θ + ... =\n1\n1−cos2θ = 5. Hence,\ncos2θ = 4\n5. Then, cos2θ = 2cos2θ −1 = 3\n5\nMoreover, linguistic rules or features, without any doubt, deserve more attention even if\nwe are living in the realm of neural computing world. This is because linguistic rules or\nfeatures exhibit great power when tackling challenging NLU problems. In Chapter 3, we\nﬁnd that SOTA NLU models, BERT, obtain inferior results to our linguistics-driven heuristic\nrules on dev set. This shows that giant neural models still fail to capture some necessary\n20Math question could be context-free such as ”let a equal one plus two minus three times four, is a\ncongruent to zero when the modulo is ﬁve?”\n21This example corresponds to the second example in their Figure 1.\n45\nlinguistic phenomena. As such, it’s essential to discover how to effectively incorporate\nlinguistic information into neural models to compensate for what the neural network-model\nis weak at. A simple practice is to embed linguistic features such as NER and POS tags\ninto original texts. Particularly, I observed that a vanilla attention-based Seq2Seq model,\nwhen being equipped with linguistic features, could achieve better performance than BART\n(Lewis et al., 2020),22 a variant of BERT specializing in text generation, on both in-domain\nand cross-domain question generation tasks.\n22In general, BART (∼139M) has 8 times more parameters than vanilla attention-based Seq2Seq (∼17M).\n46\nAppendix A: Supplementary Materials\nA.1\nClinical Question Answering\nA.1.1\nAnswer Evidence Extractor\nFormulation and Implementation Formally, given a document (context) p = {p1, p2, ..., pm},\nwhere pi is the i-th token of the document and m is the total number of tokens, we aim to\nextract potential evidence sequences. Firstly, we adopt the ClinicalBERT model (Alsentzer\net al., 2019) to encode the document:\nU = ClinicalBERT{p1, ..., pm}.\n(A.1)\nwhere U ∈Rm×d, and d is size of the dimension.\nFollowing the same paradigm of the BERT model for the sequence labeling task (Devlin\net al., 2019b), we predict the BIO tag for each aj as follows:\nPr(aj|pi) = softmax(U · W + b), ∀pi ∈p\n(A.2)\nWe train model on source contexts by minimizing the negative log-likelihood loss.\nPost-processing Heuristic Rules We observe that when we directly apply the ClinicalBERT\n(Alsentzer et al., 2019) system described in Section 2.3.1 on clinical texts, the extracted\nanswer evidences sometimes are broken sentences due to the noisy nature and uninformative\nlanguage (e.g., acronyms) of clinical texts. To make sure the extracted evidences are mean-\ningful, we designed a “merge-and-drop” heuristic rule to further improve the extractor’s\n47\naccuracy. Speciﬁcally, for each extracted evidence candidate, we ﬁrst examine the length\n(number of tokens) of the extracted evidence. If the length is larger than the threshold η, we\nkeep this evidence; otherwise, we compute the distance, i.e., the number of tokens between\nthe current candidate span and the closest span. If the distance is smaller than the threshold\nγ, we merge these two “close-sitting” spans; otherwise, we drop this over-short evidence\nspan. In our experiments, we set η and γ to be 3 and 3, respectively, since they help the QA\nsystem achieve the best performance on the dev set.\nA.1.2\nQuestion Phrases Identiﬁcation\nIn order to utilize the Question Phrase Prediction (QPP) module and make the QPP\nmodule generic enough without loss of generality, we identify valid n-gram Question Phrases\nin an automatic way.\nTo prepare an exhaustive list of valid n-gram Question Phrases, we ﬁrst collect all of the\nﬁrst n words appearing in questions in emrQA, forming three (i.e., n=1, 2, 3) raw Question\nPhrases set.\nWe observe that all uni-grams are valid question phrases (e.g., “How”, “When”, “What”),\nso we don’t do any pruning and keep the uni-gram question phrases set as it is.\nAs for n-gram (n ≥2) Question Phrases set, we conduct ﬁne-grained ﬁltering. We only\nconsider n-grams with occurrence frequency greater than the threshold ζ as valid n-gram\nQuestion Phrases. In our experiment, we set ζ as 0.02%. Less frequent n-gram words\n(i.e., frequency < 0.02%) will degrade to unigram Question Phrases in accordance with\ncorresponding question types (e.g., “Has lasix” →“Has”*) so as to maintain lossless. In the\nend, n-gram (n ≥2) Question Phrases sets, without any information loss, consist of both\nn-gram Question Phrases and degraded unigram Question Phrases.\n48\nA.1.3\nDev Set Construction\nThe dev set on MIMIC-III is constructed by sampling generated questions from 9 QG\nmodels and is used to tune the hyper-parameters only. Instead of uniformly sampling from\n9 QG models, we followed the sampling ratio of 1:3:6 (Base model, Base+BeamSearch,\nBase+QPP) for each QG method, which made the dev set cover as many diverse questions\nas possible.\n49\nBibliography\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann,\nand Matthew McDermott. Publicly available clinical BERT embeddings. NAACL Clinical\nNLP Workshop 2019, 2019.\nHannah Youngeun An and Aaron Steven White. The lexical and grammatical sources of\nneg-raising inferences. In Proceedings of the Society for Computation in Linguistics\n(SCiL 2020), pages 220–233, 2019.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In EMNLP’15, pages 632–642,\n2015.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020.\n50\nRobin Burke, Kristian Hammond, Vladimir Kulyukin, Steven Lytinen, Noriko Tomuro, and\nScott Schoenberg. Question answering from frequently asked question ﬁles: Experiences\nwith the FAQ FINDER system. AI Magazine, pages 57–66, 1997.\nAdrian Calma and Bernhard Sick. Simulation of annotators for active learning: Uncertain\nOracles. In Proceedings of the Workshop and Tutorial on Interactive Adaptive Learning\nco-located with European Conference on Machine Learning and Principles and Practice\nof Knowledge Discovery in Databases (ECML PKDD 2017), pages 49–58, 2017.\nYing-Hong Chan and Yao-Chung Fan. A recurrent BERT-based model for question genera-\ntion. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering,\npages 154–162, 2019.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer\nopen-domain questions. In ACL’17, pages 1870–1879, 2017.\nEmily Chen, Kristina Lerman, and Emilio Ferrara. Tracking social media discourse about\nthe COVID-19 pandemic: Development of a public coronavirus twitter data set. JMIR\nPublic Health and Surveillance, 2020.\nXilun Chen and Claire Cardie. Multinomial adversarial networks for multi-domain text\nclassiﬁcation. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages\n1226–1240, 2018.\n51\nXilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Q. Weinberger. Adver-\nsarial deep averaging networks for cross-lingual sentiment classiﬁcation. Trans. Assoc.\nComput. Linguistics, 6:557–570, 2018.\nJaemin Cho, Minjoon Seo, and Hannaneh Hajishirzi. Mixture content selection for diverse\nsequence generation. In EMNLP-IJCNLP’19, pages 3112–3122, 2019.\nKyunghyun Cho, Bart van Merrienboer, C¸ aglar G¨ulc¸ehre, Dzmitry Bahdanau, Fethi\nBougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations us-\ning RNN encoder-decoder for statistical machine translation. In Alessandro Moschitti,\nBo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empiri-\ncal Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha,\nQatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1724–1734,\n2014.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual en-\ntailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty,\nVisual Object Classiﬁcation and Recognizing Textual Entailment, First PASCAL Machine\nLearning Challenges Workshop, MLCW 2005, pages 177–190, 2005.\nMarie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\nInvestigating projection in naturally occurring discourse. In Sinn und Bedeutung 23, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\n52\nLinguistics: Human Language Technologies, NAACL-HLT 2019, pages 4171–4186,\n2019a.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In NAACL-HLT’19, pages\n4171–4186, 2019b.\nXinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for\nreading comprehension. In ACL’17, pages 1342–1352, 2017.\nJeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.\nJungwei Fan. Annotating and characterizing clinical sentences with explicit why-qa cues.\nIn NAACL Clinical NLP Workshop, pages 101–106, 2019.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle,\nFranc¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial\ntraining of neural networks. J. Mach. Learn. Res., 17:59:1–59:35, 2016.\nWilliam Gantt, Benjamin Kane, and Aaron Steven White. Natural language inference with\nmixed effects. In The Ninth Joint Conference on Lexical and Computational Semantics\n(*SEM 2020), 2020.\nDavid Golub, Po-Sen Huang, Xiaodong He, and Li Deng. Two-stage synthesis networks for\ntransfer learning in machine comprehension. In EMNLP’17, pages 835–844, 2017.\nSparsh Gupta and Vitor R. Carvalho. FAQ retrieval using attentive matching. In SIGIR’19,\npage 929–932, 2019.\n53\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan Sung, Laszlo Lukacs, Ruiqi\nGuo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. Efﬁcient natural language response\nsuggestion for smart reply. CoRR, abs/1705.00652, 2017.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the\nMATH dataset. CoRR, abs/2103.03874, 2021.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation,\npages 1735–1780, 1997.\nDaphne Ippolito, Reno Kriz, Jo˜ao Sedoc, Maria Kustikova, and Chris Callison-Burch.\nComparison of diverse decoding methods from conditional language models. In ACL ’19,\npages 3752–3762, 2019.\nChen Jia, Liang Xiao, and Yue Zhang. Cross-domain NER using cross-domain language\nmodeling. In Proceedings of the 57th Conference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\npages 2464–2474, 2019.\nNanjiang Jiang and Marie-Catherine de Marneffe. Evaluating BERT for natural language\ninference: A case study on the Commitmentbank. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2019, pages 6085–6090,\n2019a.\nNanjiang Jiang and Marie-Catherine de Marneffe. Do you know that Florence is packed\nwith visitors? Evaluating state-of-the-art models of speaker commitment. In Proceedings\n54\nof the 57th Conference of the Association for Computational Linguistics, ACL 2019, pages\n4208–4213, 2019b.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.\nMIMIC-III, a freely accessible critical care database. Scientiﬁc data, 3:160035, 2016.\nJunmo Kang, Haritz Puerto San Roman, et al. Let me know what to ask: Interrogative-word-\naware question generation. In Proceedings of the 2nd Workshop on Machine Reading for\nQuestion Answering, pages 163–171, 2019.\nMladen Karan and Jan ˇSnajder. FAQIR - a frequently asked questions retrieval test collection.\nIn Text, Speech, and Dialogue - 19th International Conference, TSD 2016, pages 74–81,\n2016.\nMladen Karan and Jan ˇSnajder. Paraphrase-focused learning to rank for domain-speciﬁc\nfrequently asked questions retrieval. In Expert Systems with Applications, pages 418–433,\n2018.\nKian Kenyon-Dean, Eisha Ahmed, Scott Fujimoto, Jeremy Georges-Filteau, Christopher\nGlasz, Barleen Kaur, Auguste Lalande, Shruti Bhanderi, Robert Belfer, Nirmal Kana-\ngasabai, Roman Sarrazingendron, Rohit Verma, and Derek Ruths. Sentiment analysis:\nIt’s complicated! In Proceedings of the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2018, pages 1886–1895, 2018.\n55\nHarksoo Kim and Jungyun Seo. High-performance FAQ retrieval using an automatic\nclustering method of query logs. Information Processing & Management, pages 650 –\n661, 2006.\nAlon Lavie and Michael J Denkowski. The meteor metric for automatic evaluation of\nmachine translation. Machine translation, 23(2-3):105–115, 2009.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880, 2020.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\nobjective function for neural conversation models. In NAACL-HLT’16, pages 110–119,\n2016.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text\nSummarization Branches Out, pages 74–81, Barcelona, Spain, July 2004.\nBang Liu, Haojie Wei, Di Niu, Haolan Chen, and Yancheng He. Asking questions the\nhuman way: Scalable question-answer generation from text corpus. In WWW’20, pages\n2032–2043, 2020.\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to\nattention-based neural machine translation. In EMNLP’15, pages 1412–1421, 2015.\n56\nBill MacCartney and Christopher D. Manning. An extended model of natural logic. In\nProceedings of the Eight International Conference on Computational Semantics, IWCS\n2009, pages 140–156, 2009.\nChristopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze.\nIntroduction to\nInformation Retrieval. Cambridge University Press, 2008.\nYosi Mass, Boaz Carmeli, Haggai Roitman, and David Konopnicki. Unsupervised FAQ\nretrieval with question generation and BERT. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, ACL 2020, pages 807–812, 2020.\nTom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing\nsyntactic heuristics in natural language inference. In Proceedings of the 57th Conference\nof the Association for Computational Linguistics, ACL 2019, pages 3428–3448, 2019.\nn2c2.\nn2c2 nlp research data sets, 2006.\nURL https://portal.dbmi.hms.\nharvard.edu/projects/n2c2-nlp/.\nAnusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. emrqa: A large corpus\nfor question answering on electronic medical records. In EMNLP’18, pages 2357–2368,\n2018.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for\nautomatic evaluation of machine translation. In ACL’02, pages 311–318, 2002.\nJon Patrick and Min Li. An ontology for clinical questions about the contents of patient\nnotes. Journal of Biomedical Informatics, 45(2):292–306, 2012.\n57\nEllie Pavlick and Chris Callison-Burch. Most “babies” are “little” and most “problems” are\n“huge”: Compositional entailment in adjective-nouns. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics, ACL 2016, 2016.\nEllie Pavlick and Tom Kwiatkowski. Inherent disagreements in human textual inferences.\nTransactions of the Association for Computational Linguistics, pages 677–694, 2019.\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors\nfor word representation. In Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2014, pages 1532–1543, 2014.\nMatthew E. Peters, Sebastian Ruder, and Noah A. Smith. To tune or not to tune? Adapting\npretrained representations to diverse tasks. In Proceedings of the 4th Workshop on\nRepresentation Learning for NLP, RepL4NLP@ACL 2019, pages 7–14, 2019.\nAdam Poliak, Max Fleming, Cash Costello, Kenton W Murray, Mahsa Yarmohammadi,\nShivani Pandya, Darius Irani, Milind Agarwal, Udit Sharma, Shuo Sun, Nicola Ivanov,\nLingxi Shang, Kaushik Srinivasan, Seolhwa Lee, Xu Han, Smisha Agarwal, and Jo˜ao\nSedoc. Collecting veriﬁed COVID-19 question answer pairs, 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. Technical report, OpenAI, 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. CoRR, abs/1910.10683, 2019.\n58\nPreethi Raghavan, Siddharth Patwardhan, Jennifer J Liang, and Murthy V Devarakonda.\nAnnotating electronic medical records for question answering.\narXiv preprint\narXiv:1805.06816, 2018.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. In EMNLP’16, pages 2383–2392, 2016.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable\nquestions for squad. In ACL’18, pages 784–789, 2018.\nBhanu Pratap Singh Rawat, Wei-Hung Weng, Preethi Raghavan, and Peter Szolovits. Entity-\nenriched neural models for clinical question answering. arXiv preprint arXiv:2005.06587,\n2020.\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese\nBERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP 2019, pages 3980–3990, 2019.\nStephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. Foundations and Trends® in Information Retrieval, pages 333–389, 2009.\nWataru Sakata, Tomohide Shibata, Ribeka Tanaka, and Sadao Kurohashi. FAQ retrieval\nusing query question similarity and BERT-based query-answer relevance. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR 2019, pages 1113–1116, 2019.\n59\nH. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In\nProceedings of the Fifth Annual ACM Conference on Computational Learning Theory,\nCOLT 1992, Pittsburgh, PA, USA, July 27-29, 1992, pages 287–294, 1992.\nSiamak Shakeri, C´ıcero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Feng Nan, Zhiguo\nWang, Ramesh Nallapati, and Bing Xiang. End-to-end synthetic data generation for\ndomain adaptation of question answering systems. In EMNLP’20, pages 5445–5460,\n2020.\nKeiji Shinzato, Tomohide Shibata, Daisuke Kawahara, and Sadao Kurohashi. Tsubaki:\nAn open search engine infrastructure for developing information access methodology.\nJournal of Information Processing, pages 216–227, 2012.\nShuo Sun and Jo˜ao Sedoc. An analysis of BERT FAQ retrieval models for COVID-19\ninfobot, 2020.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural\nnetworks. In NIPS ’14, pages 3104–3112, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information\nProcessing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008,\n2017.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. SuperGLUE: A stickier benchmark for\ngeneral-purpose language understanding systems. In Advances in Neural Information\n60\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems\n2019, NeurIPS 2019, pages 3261–3275, 2019a.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. GLUE: A multi-task benchmark and analysis platform for natural language\nunderstanding. In 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019, 2019b.\nHuazheng Wang, Zhe Gan, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, and Hongning\nWang. Adversarial domain adaptation for machine reading comprehension. In EMNLP-\nIJCNLP’19, pages 2510–2520, 2019c.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In NAACL’18, pages 1112–1122, 2018.\nRoman V. Yampolskiy. Turing test as a deﬁning feature of ai-completeness. In Artiﬁcial\nIntelligence, Evolutionary Computing and Metaheuristics - In the Footsteps of Alan\nTuring, volume 427, pages 3–17. Springer, 2013.\nXiang Yue, Bernal Jimenez Gutierrez, and Huan Sun. Clinical reading comprehension: A\nthorough analysis of the emrQA dataset. In ACL’20, 2020.\nXiang Yue, Xinliang Frederick Zhang, Ziyu Yao, Simon Lin, and Huan Sun. CliniQG4QA:\nGenerating diverse questions for domain adaptation of clinical question answering. In\nIEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021, 2021.\nXinliang Frederick Zhang and Marie-Catherine de Marneffe. Identifying inherent disagree-\nment in natural language inference. In Proceedings of the 2021 Conference of the North\n61\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, 2021.\nXinliang Frederick Zhang, Heming Sun, Xiang Yue, Simon Lin, and Huan Sun. COUGH:\nA challenge dataset and models for COVID-19 FAQ retrievall language inference. In Pro-\nceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2021, 2021.\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill\nDolan. Generating informative and diverse conversational responses via adversarial\ninformation maximization. In NeurIPS’18, pages 1810–1820, 2018.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural\nquestion generation from text: A preliminary study. In National CCF Conference on\nNatural Language Processing and Chinese Computing, pages 662–671. Springer, 2017.\n62\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-12-01",
  "updated": "2022-02-27"
}