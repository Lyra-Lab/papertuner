{
  "id": "http://arxiv.org/abs/2112.03100v1",
  "title": "Hierarchical Reinforcement Learning with Timed Subgoals",
  "authors": [
    "Nico Gürtler",
    "Dieter Büchler",
    "Georg Martius"
  ],
  "abstract": "Hierarchical reinforcement learning (HRL) holds great potential for\nsample-efficient learning on challenging long-horizon tasks. In particular,\nletting a higher level assign subgoals to a lower level has been shown to\nenable fast learning on difficult problems. However, such subgoal-based methods\nhave been designed with static reinforcement learning environments in mind and\nconsequently struggle with dynamic elements beyond the immediate control of the\nagent even though they are ubiquitous in real-world problems. In this paper, we\nintroduce Hierarchical reinforcement learning with Timed Subgoals (HiTS), an\nHRL algorithm that enables the agent to adapt its timing to a dynamic\nenvironment by not only specifying what goal state is to be reached but also\nwhen. We discuss how communicating with a lower level in terms of such timed\nsubgoals results in a more stable learning problem for the higher level. Our\nexperiments on a range of standard benchmarks and three new challenging dynamic\nreinforcement learning environments show that our method is capable of\nsample-efficient learning where an existing state-of-the-art subgoal-based HRL\nmethod fails to learn stable solutions.",
  "text": "Hierarchical Reinforcement Learning\nWith Timed Subgoals\nNico Gürtler\nDieter Büchler\nGeorg Martius\nMax Planck Institute for Intelligent Systems\nTübingen, Germany\n{nguertler, dbuechler, gmartius}@tue.mpg.de\nAbstract\nHierarchical reinforcement learning (HRL) holds great potential for sample-\nefﬁcient learning on challenging long-horizon tasks. In particular, letting a higher\nlevel assign subgoals to a lower level has been shown to enable fast learning on\ndifﬁcult problems. However, such subgoal-based methods have been designed with\nstatic reinforcement learning environments in mind and consequently struggle with\ndynamic elements beyond the immediate control of the agent even though they\nare ubiquitous in real-world problems. In this paper, we introduce Hierarchical\nreinforcement learning with Timed Subgoals (HiTS), an HRL algorithm that en-\nables the agent to adapt its timing to a dynamic environment by not only specifying\nwhat goal state is to be reached but also when. We discuss how communicating\nwith a lower level in terms of such timed subgoals results in a more stable learning\nproblem for the higher level. Our experiments on a range of standard benchmarks\nand three new challenging dynamic reinforcement learning environments show that\nour method is capable of sample-efﬁcient learning where an existing state-of-the-art\nsubgoal-based HRL method fails to learn stable solutions.1\n1\nIntroduction\nHierarchical reinforcement learning (HRL) has recently begun to live up to its promise of sample-\nefﬁcient learning on difﬁcult long-horizon tasks. The idea behind HRL is to break down a complex\nproblem into a hierarchy of more tractable subtasks. A particularly successful approach to deﬁning\nsuch a hierarchy is to let a high-level policy choose a subgoal which a low-level policy is then tasked\nwith achieving [8]. Due to the resulting temporal abstraction such subgoal-based HRL methods have\nbeen shown to be able to learn demanding tasks with unprecedented efﬁciency [31, 23, 19].\nIn order to realize the full potential of HRL it is necessary to design algorithms that enable concurrent\nlearning on all levels of the hierarchy. However, the changing behavior of the lower level during\ntraining introduces a major difﬁculty. From the perspective of the higher level, the reinforcement\nlearning environment and the policy on the lower level constitute an effective environment which\ndetermines what consequences its actions will have. During training, the learning progress on the\nlower level renders this effective environment non-stationary. If this issue is not addressed, the higher\nlevel will usually not start to learn efﬁciently before the lower level is fully converged. This situation is\nsimilar to a manager and a worker trying to solve a task together while the meaning of the vocabulary\nthey use for communication is continuously changing. Clearly, a stable solution can then only be\nfound once the worker reacts reliably to instructions. Hence, to enable true concurrent learning, all\n1Videos and code, including our algorithm and the proposed dynamic environments, can be found at\nhttps://github.com/martius-lab/HiTS.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2112.03100v1  [cs.LG]  6 Dec 2021\nlevels in a hierarchy should see transitions that look like they were generated by interacting with a\nstationary effective environment.\nExisting algorithms partially mask the non-stationarity of the effective environment by replacing\nthe subgoal chosen by the higher level appropriately in hindsight. Combined with the subtask of\nachieving or progressing towards the assigned subgoal as fast as possible, this approach was shown\nto enable fast learning on a range of challenging sparse-reward, long-horizon tasks [23, 19]. What\nthese methods do not take into account, however, is that, if adaptive temporal abstraction is used, the\nhigher level in the hierarchy is effectively interacting with a semi-Markov decision process (SMDP),\ni.e., transition times vary. If the objective of the lower level is to reach a subgoal as fast as possible,\nthen the amount of time that elapses until it reaches a given subgoal and returns control to the higher\nlevel will decrease during training. Hence, the distribution of the transition times the higher level\nsees will shift to lower values which introduces an additional source of non-stationarity. When trying\nto quickly traverse a static environment, such as a maze, this shift is in line with the overall task and\nwill contribute to the learning progress.\nYet, as soon as dynamic elements that are beyond the immediate control of the agent are present, the\nsituation changes radically. Consider, for example, the task of returning a tennis ball to a speciﬁed\npoint on the ground by hitting it with a racket. This clearly requires the agent to time its actions so as\nto intercept the ball trajectory with the racket while it has the right orientation and velocity. Even if\nthe higher level found a sequence of subgoals (specifying the state of the racket) that brought about\nthe right timing, this solution would stop working as soon as the lower level learns to reach them\nfaster. This would require the higher level to choose a different and possibly longer sequence of\nsubgoals, a process that would continue until the lower level was fully converged. Hence, exposing\nthe higher level to a non-stationary distribution of transition times will lead to training instability\nand slow learning. As it is the rule rather than the exception for real-world environments to contain\ndynamic elements beyond the immediate control of the agent – think of humans collaborating with a\nrobot or an autonomous car navigating trafﬁc – this problem can be expected to hinder the application\nof HRL to real-world tasks.\nIn order to solve the non-stationarity issue in dynamic environments, we propose to let the higher\nlevel choose not only what subgoal is to be reached but also when. By emitting such timed subgoals,\nconsisting of a desired subgoal and a time interval that is supposed to elapse before it is reached,\nthe higher level has explicit control over the transition times of the SMDP it interacts with. This\ncompletely removes the non-stationarity of transition times and allows for stable concurrent learning in\ndynamic environments when combined with a technique for hiding the non-stationarity of transitions\nin the subgoal space. It furthermore gives the higher level explicit control over the degree of temporal\nabstraction, giving it more direct access to the trade off between a small effective problem horizon\nand exercising tight control over the agent.\nThe main contribution of this work is to distill these insights into the formulation of a sample-efﬁcient\nHRL algorithm based on timed subgoals (Section 3) which we term Hierarchical reinforcement\nlearning with Timed Subgoals (HiTS). We demonstrate that HiTS is competitive on four standard\nbenchmark tasks and propose three novel tasks that exemplify the challenges introduced by dynamic\nelements in the environment. While HiTS succeeds in solving them, prior methods fail to learn\na stable solution (Section 4). In a theoretical analysis, we show that the use of timed subgoals\nin combination with hindsight action relabeling [19] removes the non-stationarity of the SMDP\ngenerating the data the higher level is trained on (Section 3).\n2\nBackground\n2.1\nReinforcement learning and subgoal-based hierarchical reinforcement learning\nWe assume the reinforcement learning problem is formulated as a Markov decision process (MDP)\ndeﬁned by a tuple (S, A, p, r, ρ0, γ) consisting of state space S, action space A, transition probabili-\nties p(s′ | s, a), reward function r(s, a), initial state distribution ρ0(s0) and discount factor γ ∈[0, 1).\nAt each time step, the agent is provided with a state st ∈S and samples an action at ∈A from the\ndistribution π(· | st) deﬁned by its policy π. The environment then provides feedback in the form of\na reward r(st, at) ∈R and transitions to the next state st+1 ∈S. The objective is to ﬁnd an optimal\n2\npolicy π∗that maximizes the expected discounted return\nJ(π) = Eπ,s0∼ρ0\n\" ∞\nX\nt=0\nγtr(st, at)\n#\n.\n(1)\nIn this section, we consider the subgoal-based HRL approach to solving this problem where a lower\nlevel is tasked with achieving a subgoal provided by a higher level [8, 23, 19]. The lower level (index\n0) acts directly on the environment by outputting a primitive action in A, whereas the higher level\n(index 1) proposes subgoals for level 0. Hence, the action of level 1 is the subgoal for level 0, i.e.\ng0 := a1\nt. Since level 0 performs a variable number of steps τ in the environment before “ﬁnishing”\nits subtask, level 1 effectively interacts with a semi-Markov decision process (SMDP) [16, 17]. Its\ntransition probabilities p(st+τ, τ | st, a1\nt) determine the distribution not only of the next state st+τ\nlevel 1 observes but also of the time τ that elapses beforehand.\nExisting subgoal-based algorithms reward the lower level either for being close to the subgoal [23] or\nfor progressing in a given direction in subgoal space [31] or penalize every action that does not lead\nto the immediate achievement of the subgoal [19]. In this section, we consider the latter case of a\nshortest path objective with a reward\nr0(s′, g0) =\n\u001a0\nif φ(s′, g0) = 1,\n−1\notherwise\n(2)\nfor transitioning to a new state s′. The function φ(s′, g0) evaluates to 1 only if s′ is sufﬁciently close\nto the subgoal g0 in some metric (e.g. Euclidean distance). The higher level is queried for a new\nsubgoal either if the lower level achieved a state sufﬁciently close to the subgoal or if the lower level\nhas used up a ﬁxed budget of actions.\n2.2\nNon-stationarity of the induced SMDP and hindsight action relabeling\nThe higher level of the hierarchy interacts with an SMDP that is induced by the environment dynamics\nand the behaviour of the lower level. Changes to the low-level policy during training render this\nSMDP non-stationary which prevents the higher level from learning a stable solution. Applying\nthe recently proposed hindsight action relabeling technique can alleviate this problem for static\nenvironments [19]. We show that for dynamic environments the non-stationarity reappears.\nFor reasons of sample-efﬁciency, data should be reused after a policy update instead of being\ndiscarded. We therefore consider training with an off-policy algorithm that uses a replay buffer. On\nlevel 1 transitions of the form (s, a1, r1, s′) are being stored, containing the state s, the action a1, the\nreward r1 and the state s′ at the end of the subtask execution. For now, we assume that the action\nspace of level 1 (which is equal to the goal space of level 0) is the full state space.\nDue to the changing low-level policy π0, the distribution of the next state s′ and the reward r1 given a\nﬁxed state action pair (s, a1) will change in the course of training. Hence, the SMDP level 1 interacts\nwith is non-stationary and learning a stable high-level policy becomes feasible only after the lower\nlevel is fully converged. Hindsight action relabeling, as proposed in [19], addresses this problem by\n“pretending” that the low-level policy is already optimal, i.e., is able to reach assigned subgoals. This\nis achieved by replacing the high-level action (corresponding to the assigned subgoal) with the state\nthe lower level actually achieved before storing transitions in the replay buffer. If the reward depends\non the action, it has to be adapted as well:\n(s, a1, r1, s′) =⇒(s, ˆa1 := s′, ˆr1 := r1(s, ˆa1), s′)\n(hindsight action relabeling).\n(3)\nIn Fig. 1a we illustrate this relabeling procedure.\nProposition 1 Applying hindsight action relabeling (Eq. 3) at level 1 generates transitions\nthat follow a stationary state transition and reward distribution, i.e. pt\n\u0000s, ˆa1, ˆr1, s′\u0001\n=\npt\n\u0000s, ˆa1\u0001\np\n\u0000s′, ˆr1 | s, ˆa1\u0001\nwhere p\n\u0000s′, ˆr1 | s, ˆa1\u0001\nis time-independent provided that the subgoal\nspace is equal to the full state space.\nThe proof is given in Suppl. A. The remaining caveat is that the higher level should learn to restrict\nitself to subgoals which are feasible for the lower level. By conducting testing transitions [19], which\n3\nunsuccessful original transition\n(a)\nhindsight action \nrelabeling\npretend lower level has achieved \nassigned goal\nenvironment dynamics\n(b)\nslow reaching\nfast reaching\nFigure 1: Relabeling results in a stationary state-transition distribution in static environments, but not\nin dynamic ones. (a) Action relabeling replaces the high-level action a1 with the achieved state s′\nas if the lower level was already optimal. (b) In dynamic environments with a state that factorizes\ninto a directly controllable part sc and a remaining part se with dynamics, action relabeling does not\nprevent different environment outcomes for slow and fast low-level policies if the subgoal determines\nonly sc. In this example, the agent tries to reach a moving platform.\nare exempt from hindsight action relabeling and penalize infeasible subgoals, an appropriate incentive\nfor the higher level can be introduced. Note that this reintroduces a non-stationarity to the data in the\nreplay buffer. For the remaining part of this section we do not consider testing transitions as they are\nnot directly related to the issue we are concerned with.\nDynamic environments.\nWe now consider environments with dynamic elements which are not\ndirectly controllable by the agent, for instance a ﬂying ball or a moving platform. In this case,\nchoosing the full state space as the subgoal space is problematic as it would include parts of the\nstate the agent has no control over. Consequently, the higher level would be forced to learn the full\ndynamics of the environment in order to be able to propose feasible subgoals. In such a setting it is\ntherefore more appropriate to restrict the subgoal space to the directly controllable part of the state.\nFormally, we assume the state space to factorize as S = Sc × Se into a directly controllable part Sc,\ne.g. the agent, and a part Se corresponding to the rest of the environment [18, 15, 11]. For simplicity,\nwe assume that sc does not inﬂuence se in this section, i.e., p\n\u0000s′e | se) = p(s′e | se, sc, a0\u0001\n. A\ntypical choice for a subgoal space would then be Sc, i.e. G0 = Sc, as it allows the lower level to\nfocus on controlling the agent alone and allows for transfer to related tasks. However, due to the\nrestriction of the subgoal space, Prop. 1 does not apply anymore.\nIn particular, hindsight action relabeling does not result in a stationary state-transition distribution\nanymore as illustrated in Fig. 1 (b). Even though relabeling the action of level 1 according to ˆa1 = s′c\nhides the inﬂuence of the lower level on the achieved goal, the state of the environment s′e varies\naccording to how long the lower level was in control. This transition time τ will shrink during the\ncourse of training as the lower level tries to optimize a shortest path objective. As a consequence,\nthe same relabeled state-action pair (st, ˆa1 = s′c) comes with different environmental outcomes\nse\nt+τ and se\nt+˜τ depending on the learning progress of the lower level. This makes it impossible for\nthe agent to adapt its behavior to the environment before the lower level is fully converged. As a\nconsequence, existing subgoal-based HRL methods with adaptive temporal abstraction struggle with\ndynamic environments.\n3\nHierarchical reinforcement learning with timed subgoals\nAn observation that points to a way out of the non-stationarity dilemma in dynamic environments is\nthat the readily controllable part often only sparsely inﬂuences the rest of the environment. In the\nexample from the last section, the dynamics of the moving platform in Fig. 1 (b) is not affected by\nthe agent at all. As a result, knowing the elapsed time alone completely determines the distribution of\nse, no matter how complex its dynamics may be. We therefore propose to let time stand in for se and\nto condition the lower level on a timed subgoal (g0, ∆t0) where the desired time until achievement\n∆t0 ∈N>0 determines when the lower level is supposed to reach the subgoal state g0 ∈Sc.\nHence, after being assigned a timed subgoal at time t, the lower level stays in control until t + ∆t0.\nThis deﬁnition of the hierarchy ﬁxes the transition time the higher level sees to τ = ∆t0. Combined\nwith the assumption that se evolves independently of sc and when replacing the action a1 via hindsight\naction relabeling as discussed in Section 2.2, this completely removes the non-stationarity of the\nSMDP the level is interacting with.\n4\nProposition 2 If the not directly controllable part of the environment evolves completely inde-\npendently of the controllable part, i.e., if p\n\u0000s′e | se) = p(s′e | se, sc, a0\u0001\n, and if hindsight action\nrelabeling is used, the transitions in the replay buffer of a higher level assigning timed subgoals to\na lower level are consistent with a completely stationary SMDP. Thus, they follow a distribution\npt\n\u0000s, ˆa1, τ, s′, ˆr1\u0001\n= pt\n\u0000s, ˆa1\u0001\np\n\u0000s′, τ, ˆr1 | s, ˆa1\u0001\nwhere p\n\u0000s′, τ, ˆr1 | s, ˆa1\u0001\nis time-independent.\nThe proof is given in Suppl. A. Intuitively, the non-stationarity is removed because hindsight action\nrelabeling hides the inﬂuence of the low-level policy on the state s′c the agent transitions to while\nﬁxing the time interval ∆t during which the lower level is in control makes sure that the low-level\npolicy does not affect s′e.\nOf course, the assumption of an environment which is not at all inﬂuenced by the agent is restrictive.\nHowever, the interactions between agent and environment are often sparse in time (e.g. in the case of\na racket hitting a ball). The higher level can then learn to identify the situations in which it has to\nexercise tight control over the agent because it is about to inﬂuence the environment and align its\nchoice of desired subgoal achievement times to them. It then has full control over the interactions\nbetween agent and environment and the episode is divided into a sequence of time intervals during\nwhich the assumption of independent dynamics and consequently also Proposition 2 hold (see Fig. 2\n(a)). From this point on, the transitions added to the replay buffer of the higher level will be consistent\nwith a stationary SMDP again.\n3.1\nThe HiTS algorithm\nIn this section, we present Hierarchical reinforcement learning with Timed Subgoals (HiTS), an HRL\nalgorithm for sample-efﬁcient learning on dynamic environments. We consider a two-level hierarchy\nin which the higher level assigns timed subgoals to the lower level as illustrated in Fig. 2 (b). For\nimplementation details we refer to Suppl. B.\n3.1.1\nThe higher level\nThe high-level policy π1 (· | s, g) is conditioned on the state and optionally on the episode goal and\noutputs a timed subgoal a1 = (g0, ∆t0\nt) ∈G0 ×N>0. The lower level then pursues this timed subgoal\nfor ∆t0\nt time steps after which the higher level is queried again. The objective of the higher level is\nto maximize the expected environment return while maintaining temporal abstraction. It therefore\nreceives the cumulative environment reward plus a penalty −c < 0 for emitting a subgoal,\nr1(st:t+∆t0\nt −1, at:t+∆t0\nt −1) =\n∆t0\nt −1\nX\nn=0\nγnr(st+n, at+n) −c .\n(4)\nNote that for Proposition 2 to hold, the reward r1 should neither depend on the atomic actions at nor\non the state sc except for when a timed subgoal is achieved.\n(a)\nlevel 1\nlevel 0\ntimed\nsubgoal\naction/state\nRL environment\n(b)\nFigure 2: (a) The inﬂuence of sc on se is assumed to be sparse in time so that the higher level can\nalign timed subgoals to it. (b) Execution of the HiTS algorithm over time.\n5\n3.1.2\nThe lower level\nThe subtask assigned to the lower level is to achieve the timed subgoal a1 = (g0, ∆t0\nt). Note that\nthe desired achievement time ∆t0\nt is given relative to the current time t, i.e., the goal state g0 is to\nbe achieved at the time t + ∆t0\nt. It is therefore decremented in each time step, ∆t0\nt+1 = ∆t0\nt −1,\nbefore being passed to the policy. We chose this representation as the dynamics of an agent usually\ndo not have an explicit time dependence and consequently time intervals rather than absolute times\nare relevant to control. In other words, a policy conditioned on this representation of a timed subgoal\nin combination with a time-invariant environment is automatically time-invariant as well.\nA state is translated into an achieved subgoal by a map f G : S →G0. In the setting of a factorized\nstate space S = Sc × Se the mapping may simply project onto the directly controllable part Sc. The\npolicy is conditioned on the observation o0\nt (e.g. sc) and the desired timed subgoal and deﬁnes a\ndensity π0(· | o0\nt, g0, ∆t0\nt) in action space. During execution of the hierarchy, an action a0\nt ∈A is\nsampled from this distribution and passed on to the environment. If the desired time until achievement\nhas run out, i.e., if ∆t0\nt+1 = 0, the higher level is queried for a new subgoal (see Fig. 2 (b)).\nDuring training, the lower level receives a non-zero reward only if it achieves a timed subgoal, i.e., if\nthe achieved subgoal is sufﬁciently close to the desired subgoal at the desired time of achievement.\nHence, the reward in a transition to a new state st+1 reads\nr0\nt =\n\u001a1\nif φ(f G(st+1), g0) = 1 and ∆t0\nt+1 = 0,\n0\notherwise,\n(5)\nwhere φ : G2 →[0, 1] speciﬁes whether the achieved and the desired subgoal are sufﬁciently close\nto each other. Note that when learning the value function of the lower level we consider st+1 as a\nterminal state when ∆t0\nt+1 = 0 is reached, i.e., we do not bootstrap and set γ0 = 0.\n3.1.3\nOff-policy training and use of hindsight\nIn order to realize the potential of HRL for sample-efﬁcient learning, past experience should be reused\ninstead of being discarded after each policy update. Hence, we use the off-policy reinforcement\nlearning algorithm Soft Actor-Critic (SAC) [13] to train the policies on both levels. We address the\nnon-stationarity issue by applying hindsight action relabeling to transitions before storing them in the\nreplay buffer of the higher level,\n(st, a1\nt = (g0\nt , ∆t0\nt), rt, st+∆t0\nt ) =⇒(st, ˆa1\nt := (ˆg0\nt , ∆t0\nt), r1(st, ˆa1\nt), st+∆t0\nt ) ,\n(6)\nwhere ˆg0\nt = f G(st+∆t0\nt ) denotes the subgoal the lower level achieved. Note that the desired time\nuntil achievement ∆t0\nt does not have to be replaced as it is ﬁxed by the higher level by deﬁnition.\nUnder the assumption of a controllable part of the state which does not inﬂuence the rest of the\nenvironment, Proposition 2 ensures that the relabeled transitions in the replay buffer are consistent\nwith a completely stationary SMDP.\nIn practice hindsight goal relabeling according to Eq. 6 is only applied if the desired subgoal was\nnot achieved, i.e., if φ(ˆg0, g0) = 0. This gives the higher level an idea of how precise the lower\nlevel is in achieving assigned subgoals while still hiding its failures. Furthermore, while pursuing a\nﬁxed percentage of timed subgoals, the lower level deterministically outputs the mean of its action\ndistribution. If it still fails to achieve the desired goal, the higher level is penalized. Such testing\ntransitions give the higher level an idea of the current capabilities of the low-level policy and therefore\nensure that it assigns feasible subgoals [19]. While these two exceptions from hindsight action\nrelabeling reintroduce a non-stationarity into the replay buffer of the higher level, they are necessary\nto make it respect the limitations of the lower level.\nAs the reward in Eq. 5 is sparse, we use Hindsight Experience Replay (HER) [2] to generalize from\nachieved to desired timed subgoals on the lower level. The resulting hindsight goal transitions are\nbased on hindsight action transitions so as to conserve their stationarity property. When choosing an\nachieved subgoal at n time steps in the future as hindsight goal, the low-level transition is relabeled as\n(st, a0\nt, rt, st+1, g0\nt , ∆t0\nt) =⇒(st, a0\nt, rt, st+1, ˆg0\nt := f G(st+n), c\n∆t\n0\nt := n) .\n(7)\nTo increase sample-efﬁciency, we also apply conventional HER on the higher level for goal-based\nenvironments.\n6\nFigure 3: Dynamic environments. (a) To reach the goal, the agent has to trigger the movement of a\nplatform at the right time. (b) In order not to collide with the drawbridge before it opens, the agent\nhas to time the unfurling of its sails correctly. (c) A tennis ball is to be returned by a robot arm to a\nvarying goal region.\n4\nExperiments\nThe goal of our experimental evaluation is to compare HiTS with prior subgoal-based methods in\nterms of sample efﬁciency and stability of learning. We evaluate on four standard benchmarks as\nwell as three new reinforcement learning environments2 that exemplify the challenges introduced\nby dynamic elements which are beyond the immediate control of the agent (see Fig. 3). The three\nproposed environments require the agent to ﬁnd the shortest path (in terms of time steps) to the goal,\ni.e., the reward is −1 in every time step unless the goal was achieved (in which case it is 0). If the\nagent did not reach the goal after a predeﬁned number of time steps, the episode is terminated as well.\nPlatforms.\nThe side-scroller-like Platforms environment requires the agent to trigger the movement\nof a platform just at the right time to be able to use it later on to reach the goal. Hence, the timing of the\nagent’s actions has a lasting impact on the dynamic elements in the environment which renders credit\nassignment in terms of primitive actions difﬁcult. The Platforms environment is therefore – despite\nits simplicity – quite challenging for existing state-of-the-art methods both “ﬂat” and hierarchical.\nDrawbridge.\nThe agent in the Drawbridge environment has to unfurl the sails of a sailing ship at\nthe right time to pass a drawbridge immediately after it opened. Note that the agent cannot actively\ndecelerate the ship. It is therefore essential to not accelerate too early in order not to collide with the\ndrawbridge and lose all momentum. While this control problem is quite simple, it requires the agent\nto wait – an ability existing subgoal-based HRL methods do not intrinsically have.\nTennis2D.\nIn the Tennis2D environment a two-dimensional robot arm with a racket as an end\neffector has to return a ball to a speciﬁed area on the ground. As it is only through the contact between\nthe racket and the ball that the agent can inﬂuence the outcome of the episode, it is crucial that the\nagent learns to precisely time the movement of the arm.\n4.1\nEmpirical evaluation and comparison to baselines\nWe compare HiTS with two hierarchical and one “ﬂat” baseline algorithm. As a subgoal-based HRL\nbaseline we consider a two-level Hierarchical Actor-Critic (HAC) [19] hierarchy due to its capacity\nfor concurrent learning and its sample efﬁciency. HAC uses a shortest path objective with the reward\nspeciﬁed in Eq. 2 on both levels as well as hindsight action relabeling and HER. As a second baseline,\nwe consider a two-level HAC hierarchy with an observation and a subgoal space that have been\naugmented with time. The observation is thus given by the state and the time that has passed in the\ncurrent episode, (s, t) ∈S × N≥0. An augmented subgoal (g, ¯t) ∈G × N≥0 is achieved if the state\nsc of the agent is sufﬁciently close to g and the current time t is close to the desired achievement\ntime ¯t. The subgoals used by this baseline hierarchy consequently contain information about when\nthey are to be achieved. The underlying HAC algorithm is left completely unchanged, however. We\nfurthermore consider SAC in combination with HER as a non-hierarchical baseline.\n2The environments comply with OpenAI’s gym interface and are available at https://github.com/martius-\nlab/HiTS.\n7\n0\n1\n2\n3\nsteps\n1e5\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\n0.0\n0.5\n1.0\n1.5\nsteps\n1e5\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n1\n2\n3\nsteps\n1e5\n0\n250\n500\n750\n1000\nreturn\nLevel 1\nLevel 0\nHiTS 1\nHiTS 0\nHAC\nHAC\nSAC+HER\nHAC\nHAC+time\n0\n2\n4\nsteps\n1e5\n1000\n900\n800\n700\nreturn\n0\n2\n4\nsteps\n1e6\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\n0\n1\n2\nsteps\n1e7\n0.0\n0.2\n0.4\n0.6\nsuccess rate\n0\n2\nsteps\n1e7\n0\n25\n50\n75\n100\nmin\ni |tc\nta\ni | [ms]\n0.0\n0.5\n1.0\nsteps\n1e6\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 4: (a)–(g) Results on the environments. We compare HiTS against HAC, HAC with time added\nto the goal space and SAC. Shaded regions indicate one standard deviation over results obtained with\nmultiple random seeds. (h) Absolute discrepancy between the time tc of contact between ball and\nracket and the closest subgoal achievement time ta\ni in an episode of Tennis2D with HiTS. One time\nstep in the environment corresponds to 10 ms.\nIn order not to bias our comparison by the choice of underlying ﬂat RL algorithm, we use our own\nimplementation of HAC which is based on SAC. Hence, all considered hierarchies use the same ﬂat\nRL algorithm. Implementation details for HAC and HiTS can be found in Suppl. B. Details about\ntraining and hyperparameter optimization are given in Suppl. C. Note that all results reported in this\nsection were obtained using deterministic policies that output the mean of the action distribution on\neach level. The performance of the stochastic policies used during training is shown in Suppl. C.\nFig. 4 (a), (b) and (c) summarize the results on three sparse-reward, long-horizon tasks introduced in\n[19]. Note that our implementation of HAC consistently exceeds the original results reported in [19].\nHiTS furthermore outperforms HAC on two of these static environments HAC was designed for. We\nhypothesize that in some environments the lower level might be able to generalize over ∆t instead\nof having to learn how to speed up a movement by trial and error. In the Ball in cup environment\n[30] the agent has to catch a ball attached to a cup via a string. This environment can be considered\ndynamic as the ball is not under the direct control of the agent. Since the inﬂuence of the cup on the\nball is not necessarily sparse in time and the reward depends on the state of the cup, the environment\nviolates the assumptions of Proposition 2. Nevertheless, HiTS is able to solve the task slightly faster\nthan the strong SAC baseline while HAC suffers from a considerable variance (see Fig. 4 (d)). Thus,\nin practice the second-order dynamics of an environment may allow for sufﬁciently tight control over\na continuous interaction between agent and environment with a limited number of timed subgoals.\nIn summary, the performance of HiTS is competitive on standard benchmark tasks, even in static\nenvironments.\nFig. 4 (e) shows the average success rates of all policy hierarchies over the course of training for the\nPlatforms environment. The SAC baseline fails to make any progress due to the lack of structured\nexploration and temporal abstraction. The two-level HAC hierarchy, on the other hand, begins to\nlearn how to solve the task but stagnates at an average success rate of around 40%. Interestingly,\naugmenting the observation and subgoal space with absolute time does not improve the performance\nof the two-level HAC hierarchy but impedes it instead. As HAC is not aware of the signiﬁcance of\nthe time component, it continues to pursue an augmented subgoal even if its time component already\nlies in the past. As a consequence, the agent gets “stuck” on a missed augmented subgoal until the\naction budget of the lower level is exhausted. As this throws timing off completely and reintroduces\na non-stationarity in the induced SMDP, the solutions found by the augmented HAC hierarchy are\nextremely brittle. Moreover, conditioning the low-level policy on absolute time introduces a spurious\nexplicit time dependence. In contrast to this, HiTS is adapted to the use of timed subgoals: It\n8\nconditions the lower level on the time interval that is left until the next timed subgoal is to be achieved.\nAdditionally, it always queries the higher level for a new timed subgoal when this time has run out\nand can therefore recover from missing a timed subgoal. HiTS consequently makes fast progress on\nthe Platforms environment and reaches an asymptotic success rate of around 86%.\nA look at the learning progress of individual runs of the two-level HAC hierarchy (see Suppl. C)\nreveals that it does learn to solve the task in some cases. However, these solutions typically deteriorate\nquickly due to the learning progress on the lower level and the resulting non-stationarity of the SMDP\nas discussed in Section 2. In contrast to this, the performance of runs using the HiTS hierarchy is\nmuch more stable due to the use of timed subgoals as discussed in Section 3.\nThe challenge in the Drawbridge environment is to reach the goal (the end of the river) in the\nshortest possible time, which requires the right timing in order not to collide with the yet unopened\nbridge. Fig. 4 (f) shows the average return of all algorithms on the Drawbridge environment. In\na successful episode it is equal to minus the time needed to reach the goal while an unsuccessful\nepisode corresponds to a value of -1000 (the maximum episode length). While SAC quickly learns\nto immediately unfurl the sails, its local exploration in action space never ﬁnds the optimal timing.\nThe two-level HAC hierarchy cannot reproduce the ideal behavior of waiting before unfurling the\nsails after the lower level has become fast at achieving subgoals and therefore stagnates at an even\nlower return. Increasing the subgoal budget would alleviate this problem but decrease temporal\nabstraction. Hence, a recursively optimal [4] HAC hierarchy may be far from optimal with respect\nto the environment MDP if the subgoal space is not equal to the full state space. HiTS does not\nsuffer from this problem as the environment dynamics are independent of the agent’s state in this\ncase. Conditioning the lower level on time is therefore sufﬁcient for HiTS to learn to time the passage\nthrough the drawbridge correctly.\nIn contrast to the environments discussed so far, controlling the agent in the Tennis2D environment\nis challenging as actions contain raw torques applied to the joints of the robot arm. Moreover, the\ntrajectory and spin of the ball vary considerably from episode to episode. Nevertheless, HAC initially\nmanages to return a small fraction of the balls to the goal area (see Fig. 4 (g)). Its performance\ndecreases, however, after about 1.5 million time steps as the lower level has become too fast and a\nshort sequence of subgoals cannot reproduce precisely timed movements anymore. As assigning\ncredit for returning a ball to torques is challenging, SAC only learns slowly. The HiTS hierarchy\ncan directly assign credit to a timed subgoal, i.e., to what robot state was to be achieved when.\nFig. 4 (h) shows how the discrepancy between the moment of contact between ball and racket and the\nclosest achievement time of a timed subgoal decreases during training. The higher level can therefore\nexercise full control over the robot arm when it matters. Moreover, the alignment of a timed subgoal\nwith the contact between racket and ball implies that the induced SMDP is close to stationary. As a\nconsequence, HiTS achieves a performance of around 40% on average within 20 million time steps.\nAlso note that HiTS uses more timed subgoals around the time of contact between racket and ball as\nshown in Fig. S3. Thus, its temporal abstraction is adapted to the task.\nThe success rates of HiTS on Platforms and Tennis2D depicted in Fig. 4 (e) and (g) exhibit a large\nvariance over random seeds. This can mostly be attributed to the environments themselves in which a\nsmall change in the behavior of the agent can have a big impact on its performance. In contrast to\nthis, the variance of HiTS on the standard benchmarks shown in Fig. 4 (a)–(d) is much smaller. A\nschedule for the entropy coefﬁcient or target entropy and the learning rate of HiTS could help with\nachieving proper convergence on the more challenging dynamic environments.\n5\nRelated work\nThe key role of abstraction in solving long-horizon, sparse-reward tasks has been recognized early\non in the RL community [8, 28, 25, 29, 9]. The options framework [29, 26] formalizes the idea of\naugmenting the action space with closed-loop activities that take over control for an extended period\nof time. The resulting temporal abstraction facilitates credit assignment for a policy selecting among\noptions. While options were originally handcrafted or learned separately [4, 26], the option-critic\narchitecture [3] jointly learns the parameters of the options and a policy over options. However, to\nkeep up stable temporal abstraction in this approach, regularization is necessary [3, 14]. A related\napproach to learning a hierarchy is based on the idea that a high-level controller should have a\nrepertoire of skills to choose from [7] or to modulate [12, 15, 11, 10]. The set of skills is often\n9\npretrained using an auxiliary reward where diversity is encouraged by an information theoretic term\n[11, 10].\nAlternatively, the lower level of the hierarchy may be trained to reach an assigned subgoal [27, 8,\n26, 22]. In this setting, the lower level can be rewarded for moving in a speciﬁed direction in goal\nspace [31], for being close to the goal state [23] or for achieving a goal state up to a tolerance [19].\nIn the latter case it is natural to query the higher level for a new subgoal only when the current one\nhas been achieved or a ﬁxed action budget is exhausted [19]. In contrast to this, using a dense reward\nis usually tied to a ﬁxed temporal abstraction scheme [23, 31]. The goal space can either be learned\n[31, 24] or predeﬁned using domain knowledge [23, 19].\nIt has been observed in the context of options [29, 26] as well as subgoal-based methods [23, 19] that\nthe higher level in a hierarchy effectively interacts with a non-stationary SMDP. In order to enable\noff-policy learning from stored high-level transitions, Nachum et al. [23] approximately determine\nhigh-level actions that are likely to reproduce the stored behavior of the lower level under the current\nlow-level policy. This procedure has to be repeated after each update to the low-level policy and\nintroduces noise due to the use of sampling in the approximation. Levy et al. [19] address the non-\nstationarity issue by hindsight action relabeling as discussed in Section 2. None of these methods take\ninto account, however, that, when using adaptive temporal abstraction as in [19], the transition times\nof the SMDP change over the course of training and consequently introduce an additional source\nof non-stationarity which prevents sample-efﬁcient learning in dynamic environments. Although in\nBlaes et al. [5] the environment contains dynamic objects, they come to rest quickly if not manipulated\nby the agent, such that the non-stationarity issue does not need to be addressed.\nAs hindsight action relabeling hides the limitations of the current low-level policy, the higher level\nmay learn to choose infeasible subgoals. Levy et al. [19] therefore penalize the higher level when the\nlower level fails to reach a subgoal. We adopt this method as it is general and simple. Zhang et al.\n[33] restrict the subgoal to a k-step adjacent region via an adjacency loss. As the loss is based on an\nadjacency network distilled from an adjacency matrix, incorporating this approach into HiTS may\nimprove sample efﬁciency, albeit only on low-dimensional subgoal spaces as it requires discretization.\nIn the context of robotics, via-points are used to specify a sequence of conﬁgurations a robot should\nmove through, often also specifying when these conﬁgurations are to be reached [21]. They therefore\nserve a similar purpose as timed subgoals. The difference lies in how they are used. A classic approach\nin robotics would be to ﬁrst obtain a controller for the robot, to then specify via-points, interpolate\nbetween them, and use the controller to follow the resulting trajectory. Hence, no concurrent learning\nis involved and the non-stationarity problem does not occur.\n6\nConclusion\nWe present an HRL algorithm designed for sample-efﬁcient learning in dynamic environments.\nIn a theoretical analysis, we show how its use of timed subgoals in conjunction with hindsight\naction relabeling attenuates the non-stationarity problem of HRL even when the lower level is not\nconditioned on the full state. Moreover, our experiments demonstrate that our method is competitive\non a range of standard benchmark tasks and outperforms existing state-of-the-art baselines in terms\nof sample complexity and stability on three new challenging dynamic tasks.\nThe effectiveness of HRL generally depends on the structure of the environment and our method\nis no exception in this regard. In particular, we consider environments with a directly controllable\npart which only sparsely inﬂuences the rest. This assumption ensures that temporal abstraction is\ncompatible with stable concurrent learning. Furthermore, our practical algorithm still contains minor\nsources of non-stationarity like testing transitions. Nevertheless, our experiments demonstrate that\nour method performs well even when deviating from the idealized setting.\nAn interesting direction for future work is the detection of interactions between agent and environment\nand the active alignment of timed subgoals to them. Furthermore, two or more levels which pursue\ntimed subgoals may enable efﬁcient learning on more challenging dynamic long-horizon tasks.\n10\nAcknowledgments and Disclosure of Funding\nWe would like to thank Bernhard Schölkopf, Jan Peters, Alexander Neitz, Giambattista Parascandolo,\nDiego Agudelo España, Hsiao-Ru Pan, Simon Guist, Sebastian Gomez-Gonzalez, Sebastian Blaes\nand Pavel Kolev for their valuable feedback.\nGeorg Martius is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1\n– Project number 390727645. We acknowledge the support from the German Federal Ministry of\nEducation and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B).\nReferences\n[1] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation hyper-\nparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining, pages 2623–2631, 2019.\n[2] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nO. P. Abbeel, and W. Zaremba. Hindsight experience replay. In Advances in neural information\nprocessing systems, pages 5048–5058, 2017.\n[3] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 31, 2017.\n[4] A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete\nevent dynamic systems, 13(1):41–77, 2003.\n[5] S. Blaes, M. Vlastelica, J.-J. Zhu, and G. Martius. Control What You Can: Intrinsically\nmotivated task-planning agent. In Advances in Neural Information Processing Systems (NeurIPS\n2019), pages 12520–12531. Curran Associates, Inc., Dec. 2019.\n[6] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.\nOpenai gym, 2016.\n[7] C. Daniel, G. Neumann, and J. Peters. Hierarchical relative entropy policy search. In Artiﬁcial\nIntelligence and Statistics, pages 273–281. PMLR, 2012.\n[8] P. Dayan and G. E. Hinton. Feudal reinforcement learning. In Advances in neural information\nprocessing systems, pages 271–278, 1993.\n[9] T. G. Dietterich. Hierarchical reinforcement learning with the maxq value function decomposi-\ntion. Journal of artiﬁcial intelligence research, 13:227–303, 2000.\n[10] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills\nwithout a reward function. In International Conference on Learning Representations, 2019.\n[11] C. Florensa, Y. Duan, and P. Abbeel. Stochastic neural networks for hierarchical reinforcement\nlearning. arXiv preprint arXiv:1704.03012, 2017.\n[12] T. Haarnoja, K. Hartikainen, P. Abbeel, and S. Levine. Latent space policies for hierarchical\nreinforcement learning. In International Conference on Machine Learning, pages 1851–1860.\nPMLR, 2018.\n[13] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.\n[14] J. Harb, P.-L. Bacon, M. Klissarov, and D. Precup. When waiting is not an option: Learning\noptions with a deliberation cost. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 32, 2018.\n[15] N. Heess, G. Wayne, Y. Tassa, T. Lillicrap, M. Riedmiller, and D. Silver. Learning and transfer\nof modulated locomotor controllers. arXiv preprint arXiv:1610.05182, 2016.\n[16] R. Howard. Semi-markovian decision processes. Proc. Intern. Statist. Inst., page 625–652,\n1963.\n[17] Q. Hu and W. Yue. Semi-Markov Decision Processes, pages 105–120. Springer US, Boston,\nMA, 2008. doi: 10.1007/978-0-387-36951-8_5.\n[18] G. Konidaris and A. Barto. Autonomous shaping: Knowledge transfer in reinforcement learning.\nIn Proceedings of the 23rd international conference on Machine learning, pages 489–496, 2006.\n11\n[19] A. Levy, G. Konidaris, R. Platt, and K. Saenko.\nLearning Multi-Level Hierarchies with\nHindsight. In International Conference on Learning Representations, 2019.\n[20] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.\nContinuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n[21] K. M. Lynch and F. C. Park. Modern Robotics. Cambridge University Press, 2017.\n[22] A. McGovern and A. G. Barto. Automatic discovery of subgoals in reinforcement learning using\ndiverse density. In Proceedings of the 18th International Conference on Machine Learning,\npages 361–368, 2001.\n[23] O. Nachum, S. S. Gu, H. Lee, and S. Levine. Data-efﬁcient hierarchical reinforcement learning.\nIn Advances in Neural Information Processing Systems, pages 3303–3313, 2018.\n[24] O. Nachum, S. Gu, H. Lee, and S. Levine. Near-optimal representation learning for hierarchical\nreinforcement learning. In International Conference on Learning Representations, 2019.\n[25] R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. Advances in\nneural information processing systems, pages 1043–1049, 1998.\n[26] D. Precup. Temporal abstraction in reinforcement learning. University of Massachusetts\nAmherst, 2000.\n[27] J. Schmidhuber. Learning to generate sub-goals for action sequences. In Artiﬁcial neural\nnetworks, pages 967–972, 1991.\n[28] A. Schwartz and S. Thrun. Finding structure in reinforcement learning. Advances in neural\ninformation processing systems, 7:385–392, 1995.\n[29] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal\nabstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.\n[30] Y. Tassa, S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez,\nT. Lillicrap, and N. Heess. dm_control: Software and tasks for continuous control, 2020.\n[31] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu.\nFeUdal networks for hierarchical reinforcement learning. In D. Precup and Y. W. Teh, editors,\nProceedings of the 34th International Conference on Machine Learning, volume 70 of Pro-\nceedings of Machine Learning Research, pages 3540–3549, International Convention Centre,\nSydney, Australia, 06–11 Aug 2017. PMLR.\n[32] J. Weng, H. Chen, A. Duburcq, K. You, M. Zhang, D. Yan, H. Su, and J. Zhu. Tianshou.\nhttps://github.com/thu-ml/tianshou, 2020.\n[33] T. Zhang, S. Guo, T. Tan, X. Hu, and F. Chen. Generating adjacency-constrained subgoals in\nhierarchical reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages\n21579–21590. Curran Associates, Inc., 2020.\n12\nSupplementary Material:\nHierarchical Reinforcement Learning\nWith Timed Subgoals\nA\nProofs\nIn this section we provide proofs for Prop. 1 and 2. For the sake of clarity, we denote random variables\nby capital letters and realizations of random variables by lowercase letters. The probability density of\na random variable X conditioned on another random variable Y is denoted by pX|Y (x | y).\nProp. 1 is concerned with the distribution of high-level transitions generated by hindsight action\nrelabeling [19],\n(st, a1\nt, rt, st+τ) =⇒(st, ˆa1\nt := st+τ, ˆr1\nt := r1(st, ˆa1\nt), st+τ)\n(hindsight action relabeling), (8)\nwhere τ denotes the transition time of the high-level SMDP, i.e., how long the lower level stays in\ncontrol.\nProposition 1 Applying hindsight action relabeling (Eq. 8) at level 1 generates transitions\nthat follow a stationary state transition and reward distribution, i.e. pt\n\u0000s, ˆa1, ˆr1, s′\u0001\n=\npt\n\u0000s, ˆa1\u0001\np\n\u0000s′, ˆr1 | s, ˆa1\u0001\nwhere p\n\u0000s′, ˆr1 | s, ˆa1\u0001\nis time-independent provided that the subgoal\nspace is equal to the full state space.\nProof. For clarity we omit the superscript 1 in the proof as actions and rewards always correspond\nto the higher level. Consider random variables St, St+τ representing the current and next state\nin a transition. Both take values in the state space S and may have an arbitrary time-dependent\njoint density function pSt,St+τ (st, st+τ), i.e., in general pSs,Ss+τ ̸= pSt,St+τ for s ̸= t. Applying\nhindsight action relabeling Eq. 8 then corresponds to deﬁning ˆAt = St+τ and ˆRt = r1(St, ˆAt). The\njoint probability density of the next state St+τ and the relabeled reward ˆRt conditioned on the state\nSt and the relabeled action ˆAt is then given by\npSt+τ , ˆ\nRt|St, ˆ\nAt (st+τ, ˆrt | st, ˆat) = p ˆ\nRt, ˆ\nAt|St,St+τ (ˆrt, ˆat | st, st+τ) pSt,St+τ (st, st+τ)\npSt, ˆ\nAt (st, ˆat)\n(9)\n= δ\n\u0000ˆrt −r1 (st, ˆat)\n\u0001\nδ (ˆat −st+τ) pSt,St+τ (st, st+τ)\npSt, ˆ\nAt (st, ˆat)\n(10)\n= δ\n\u0000ˆrt −r1 (st, ˆat)\n\u0001\nδ (ˆat −st+τ) pSt,St+τ (st, st+τ)\npSt,St+τ (st, ˆat)\n(11)\n= δ\n\u0000ˆrt −r1 (st, ˆat)\n\u0001\nδ (ˆat −st+τ) pSt,St+τ (st, st+τ)\npSt,St+τ (st, st+τ)\n(12)\n= δ\n\u0000ˆrt −r1 (st, ˆat)\n\u0001\nδ (ˆat −st+τ) ,\n(13)\nwhere δ denotes the Dirac delta function. In (11) we have used ˆA = St+τ. Hence, the conditional\ndistribution of the next state and the reward given the current state and the action is stationary. □\nProp. 2 extends this result to the transition time of the SMDP given that timed subgoals and hindsight\naction relabeling are used,\n(st, a1\nt = (g0\nt , ∆t0\nt), rt, st+∆t0\nt ) =⇒(st, ˆa1\nt := (ˆg0\nt , ∆t0\nt), ˆr1 := r1(st, ˆa1\nt), st+∆t0\nt ) .\n(14)\nProposition 2 If the not directly controllable part of the environment evolves completely inde-\npendently of the controllable part, i.e., if p\n\u0000s′e | se) = p(s′e | se, sc, a0\u0001\n, and if hindsight action\nrelabeling is used, the transitions in the replay buffer of a higher level assigning timed subgoals to\na lower level are consistent with a completely stationary SMDP. Thus, they follow a distribution\npt\n\u0000s, ˆa1, τ, s′, ˆr1\u0001\n= pt\n\u0000s, ˆa1\u0001\np\n\u0000s′, τ, ˆr1 | s, ˆa1\u0001\nwhere p\n\u0000s′, τ, ˆr1 | s, ˆa1\u0001\nis time-independent.\nProof. We again omit the superscript 1 in the proof as actions and rewards always correspond to the\nhigher level. In addition to St, St+τ as deﬁned in the proof of Prop. 1, consider random variables\n13\n∆Tt and Gt corresponding to the desired time until achievement of a timed subgoal and the subgoal,\nrespectively. Recall that the state is assumed to consist of a controllable and a non-controllable\npart, St = (Sc\nt , Se\nt ). Note that applying hindsight action relabeling (Eq. 14) then corresponds to\ndeﬁning ˆGt = Sct+τ. For the sake of brevity we drop the speciﬁcation of the random variables in\nthe subscript of the density, i.e., p(x, y) := pX,Y (x, y). Consider the probability density of the next\nstate, the transition time and the reward given the current state, the subgoal and the desired time until\nachievement,\np (st+τ, τ, ˆrt | st, ˆgt, ∆tt) = p (τ, ˆrt, ˆgt | st, st+τ, ∆tt) pt (st, st+τ, ∆tt)\npt (st, ˆgt, ∆tt)\n(15)\n= δ (τ −∆tt) δ\n\u0000ˆrt −r1 (st, ˆgt, ∆tt)\n\u0001\nδ\n\u0000ˆgt −sc\nt+τ\n\u0001\n(16)\n· pt (st, st+τ, ∆tt)\npt (st, ˆgt, ∆tt)\n(17)\n= δ (τ −∆tt) δ\n\u0000ˆrt −r1 \u0000st, sc\nt+τ, ∆tt\n\u0001\u0001\nδ\n\u0000ˆgt −sc\nt+τ\n\u0001\n(18)\n· pt (st, st+τ, ∆tt)\npt \u0000st, sc\nt+τ, ∆tt\n\u0001 ,\n(19)\nwhere we used the superscript t to highlight that a probability density is time-dependent. Since the\ntime evolution of se was assumed to be independent of sc and a, the fraction is time-independent,\npt (st, st+τ, ∆tt)\npt \u0000st, sc\nt+τ, ∆tt\n\u0001 = pt (st+τ | st, ∆tt) pt (st, ∆tt)\npt \u0000sc\nt+τ | st, ∆tt\n\u0001\npt (st, ∆tt)\n(20)\n= p\n\u0000se\nt+τ | st, ∆tt\n\u0001\npt \u0000sc\nt+τ | st, ∆tt\n\u0001\npt (st, ∆tt)\npt \u0000sc\nt+τ | st, ∆tt\n\u0001\npt (st, ∆tt)\n(21)\n= p\n\u0000se\nt+τ | st, ∆tt\n\u0001\n.\n(22)\nHence, the conditional density\np (st+τ, τ, ˆrt | st, ˆgt, ∆tt) = δ (τ −∆tt) δ\n\u0000ˆrt −r1 (st, ˆgt, ∆tt)\n\u0001\nδ\n\u0000ˆgt −sc\nt+τ\n\u0001\np\n\u0000se\nt+τ | st, ∆tt\n\u0001\n(23)\ncorresponds to a completely stationary SMDP. □\nB\nAlgorithm implementation details\nIn this section we discuss our implementations of HAC and HiTS. Both support Deep Deterministic\nPolicy Gradient (DDPG) [20] and Soft Actor-Critic (SAC) [13] as underlying off-policy reinforcement\nlearning algorithms. We used the implementation of SAC and DDPG provided by the reinforcement\nlearning library Tianshou [32]. For our experiments in Section 4 we only used SAC, however, as it\nperformed better in tests on the Platforms environment.\nB.1\nHierarchical Actor-Critic (HAC)\nOur implementation of HAC deviates slightly from the one presented in Levy et al. [19] in order to\nimprove performance and ﬂexibility:\n• We use SAC in instead of DDPG in our experiments.\n• We do not squash the learned Q-functions to the interval [−H, 0] where H > 0 denotes the\nmaximum number of actions on a level until control is returned to the next higher level or\nthe episode ends. Instead, we apply the mapping\nf(x) = log\n\u0012\n1\n1 + exp(−x)\n\u0013\nto the output of a multilayer perceptron to map it to (−∞, 0). This ensures that the learned Q-\nfunction can reach values below −H which occur in the true Q-function due to bootstrapping.\nFurthermore, this formulation allows for adding negative auxiliary rewards like the entropy\nterm of SAC or regularization terms.\n14\n• In contrast to [19] we bootstrap when learning from failed testing transitions, i.e., we do not\nset the discount rate to zero. We do this to prevent failing earlier from being more attractive\nto the agent than failing later. In other words, if no bootstrapping was used, the higher\nlevel could avoid credit assignment for negative rewards occurring later in the episode by\nimmediately choosing an infeasible subgoal. If the current low-level policy is likely to not\nachieve an assigned subgoal at some point during the episode, it would make sense for the\nhigher level to immediately choose an infeasible subgoal.\n• Our implementation is compatible with environments complying with OpenAI’s goal-based\ngym interface [6].\n• We use a more ﬂexible version of HER than Levy et al. [19]. In particular, we not only\nimplement the “episode” hindsight goal sampling strategy but also the “future” variant and\nwe do not always use the last state of the episode as a hindsight goal. Furthermore, we allow\nfor custom ﬁltering of the achieved goals in an episode before sampling from them.\nB.2\nHierarchical reinforcement learning with timed subgoals (HiTS)\nThe implementation details mentioned in Section B.1 also apply to the higher level in the HiTS\nhierarchy which shares code with our HAC implementation.\nMoreover, we restricted the desired time until achievement ∆t0\nt the higher level outputs to an interval\nI = (0, ∆tmax) with ∆tmax > 0 in order to prevent the higher level from ofﬂoading the complete\ntask to the lower level. Furthermore, we sample a ﬁxed fraction (5% in our experiments) of all timed\nsubgoals from a uniform distribution over G0 × I in order to improve exploration and make sure that\nthe replay buffer contains penalties for all infeasible regions of the goal space similar to Levy et al.\n[19].\nIn practice, we let the high-level policy output a real-valued desired time until achievement ∆t0\nt ∈I.\nWe then decrement in each time step, ∆t0\nt+1 = ∆t0\nt −1, and check for ∆t0\nt+1 ≤0 as a condition for\nquerying a new timed subgoal from the higher level. This is equivalent to ﬁrst rounding to the next\ngreater integer and then checking for equality to zero, i.e.,\n\u0006\n∆t0\nt+1\n\u0007\n= 0. The notation ∆t0\nt ∈N>0\nwe chose in Section 3.1 is therefore compatible with our implementation based on real-valued ∆t0\nt\nbut easier to parse.\nNote that the choice of SAC as an underlying off-policy algorithm facilitates the alignment of the\ndesired achievement times t + ∆t0\nt with those points in time when the controllable part interacts with\nthe rest of the environment. See Section C.7 for a discussion of this mechanism.\nC\nExperiments\nC.1\nEnvironments and hierarchies\nIf not stated otherwise, the observation space of the lower level and the subgoal space are the state\nspace of the agent. The architecture of the hidden layers of the policies and Q-functions are the same\nover algorithms.\nPlatforms.\nAs an exception, in the Platforms environment all levels see the full state. Thus, the\nlower level has a fair chance of learning to cope with the dynamic elements of the environment even\nif no timing information is communicated by the higher level. In particular, the lower level of the\nHAC hierarchy can learn not to fall off the lower platform by waiting for the moving platform before\ntrying to roll onto it.\nTennis2D.\nWhen applying HER on the higher level of all hierarchies on the Tennis2D environment,\nonly achieved states which correspond to the second contact between ball and ground are considered.\nThis choice is motivated by the observation that generalization from arbitrary states of the environment\nto goals corresponding to the ball bouncing of the ground might be difﬁcult. As all hierarchies and\nthe non-hierarchical baseline use this version of HER, it does not bias the results in Section 4.\nBall in cup.\nAs this environment is not goal-based, HAC was modiﬁed to use the reward speciﬁed\nin Eq. 4. The higher level of the hierarchy is therefore quite similar to its counterpart in HiTS.\n15\nNevertheless, HAC suffers from a signiﬁcantly higher variance on this environment which can be\nattributed to the use of subgoals as opposed to timed subgoals.\nC.2\nTraining and hyperparameter optimization\nTable S1: Summary of hyperparameter optimization.\nUR5Reacher\nAntFourRooms\nPendulum\nBall in cup\nobjective\nsuccess rate\nsuccess rate\nsuccess rate\nreturn\naverage over steps\n{0, ..., 5e5}\n{0, ..., 1.2e6}\n{0, ..., 5e5}\n{0, ..., 5e5}\ntrials\n40\n100\n60\n60\nparallel trials\n20\n50\n20\n20\nseeds per trial\n5\n10\n10\n10\nPlatforms\nDrawbridge\nTennis2D\nobjective\nsuccess rate\nreturn\nsuccess rate\naverage over steps\n{1e6, ..., 3e6}\n{0, ..., 5e5}\n{5e6, ..., 1e7}\ntrials\n60\n40\n20\nparallel trials\n20\n20\n20\nseeds per trial\n10\n7\n10\nThe hyperparameter optimization was conducted using the Optuna framework [1] with the Tree-\nstructured Parzen Estimator sampler. The objective was to maximize the average success or return\nover a ﬁxed range of time steps. Since performance on the more challenging environments is quite\nstochastic, multiple seeds were ran per trial. To keep the overall runtime low, 20 trials were ran in\nparallel. Table S1 summarizes the hyperparameter optimization on all considered environments.\nThe results of the best trial of an algorithm on an environment were then reproduced with different\nrandom seeds to remove the maximization bias. All results reported in Fig. 4 are obtained from 30\ndifferent seeds, except for HAC and SAC on Tennis2D where only 25 and 5 seeds were considered,\nrespectively. The remaining 5 and 25 seeds led to runs that diverged at some point after 1e7 time\nsteps.\nUnless otherwise speciﬁed, the hyperparameters which were optimized are:\n• learning rate (same for all levels)\n• target smoothing coefﬁcient (same for all levels)\n• entropy coefﬁcient mode (ﬁxed or tuned according to target entropy) (per level)\n• entropy coefﬁcient or target entropy (per level)\n• c from Eq. 4 (only HiTS level 1)\nThe remaining ﬁxed hyperparmeters (like the subgoal budget) were the same across algorithms. In\ncase of the AntFourRooms environment separate learning rates for both levels and the timed subgoal\nbudget were optimized for HAC and HiTS which improved the performance of both algorithms. We\nrefer to the conﬁguration ﬁles for a list of the optimized hyperparameters.\nC.3\nHardware and runtimes\nThe experiments presented in Section 4 and Appendix C were run on a cluster with several different\nCPU models. For concreteness, we give approximate runtimes per random seed on one core of an\nIntel Xeon IceLake-SP 8360Y in Table S2. We only list the runtimes of our HiTS implementation.\nThe runtimes of all baselines were similar, however.\nC.4\nResults for stochastic policies\nFig. S1 shows the performance of the stochastic policies used during training as a function of steps\ntaken (in contrast to Fig. 4 which shows learning curves for deterministic policies outputting the\nmean of the action distribution). Due to the exploration in action space, the success rates and returns\nof the stochastic policies are usually lower than those of their deterministic counterparts.\n16\nTable S2: Runtimes per random seed for HiTS on one core of an Intel Xeon IceLake-SP 8360Y.\nPlatforms\nDrawbridge\nTennis2D\nUR5Reacher\nAntFourRooms\ntime steps\n5e6\n5e5\n2e7\n5e5\n1.2e6\nruntime [h]\n12\n0.5\n28\n0.7\n1.6\nLevel 1\nLevel 0\nHiTS 1\nHiTS 0\nHAC\nHAC\nSAC+HER\nHAC\nHAC+time\n0\n2\n4\nsteps\n1e5\n1000\n900\n800\n700\nreturn\n0\n1\n2\n3\nsteps\n1e5\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\n0.0\n0.5\n1.0\nsteps\n1e6\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.5\n1.0\n1.5\nsteps\n1e5\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n1\n2\n3\nsteps\n1e5\n0\n250\n500\n750\n1000\nreturn\n0\n2\n4\nsteps\n1e6\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\n0\n1\n2\nsteps\n1e7\n0.0\n0.2\n0.4\n0.6\nsuccess rate\nFigure S1: Results on the environments with the stochastic policies used during training. We compare\nHiTS against HAC, HAC with time added to the goal space and SAC. Shaded regions indicate one\nstandard deviation over results obtained with multiple random seeds.\nC.5\nAnalysis of individual runs on the Platforms environment\nFig. S2 shows the success rates of HAC and HiTS for ﬁve different seeds on the Platforms environment.\nWhile HAC manages to ﬁnd a solution for some seeds, it usually deteriorates quickly due to the\nlearning progress on the lower level and the resulting non-stationarity of the SMDP the higher level\ninteracts with (as discussed in Section 4). As a result, the performance of the HAC hierarchies is\nquite unstable. HiTS, on the other hand, quickly ﬁnds a solution for all random seeds. For some\nseeds the performance drops again for a limited amount of time steps. This can be attributed to the\nremaining non-stationarity of the effective SMDP as discussed in Section 3.1 as well as to the task\nitself for which the optimal policy is very close to behavior that leads to complete failure.\n(a) HAC\n0\n2\n4\nsteps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\n(b) HiTS\n0\n2\n4\nsteps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsuccess rate\nFigure S2: Five individual runs with different random seeds on the Platforms environment generated\nwith a two-level HAC hierarchy (a) and a HiTS hierarchy (b).\n17\nC.6\nDistribution of subgoal achievement times on Tennis2D\nWhile the ball is still far away from the racket in the Tennis2D environment, it is not necessary for\nthe agent to be reactive. It can therefore afford temporal abstraction in the beginning of the episode.\nAs soon as the ball gets close to the racket, however, the agent might beneﬁt from being able to adjust\nits actions based on the current position, velocity and spin of the ball. Thus, the sweet spot in the\ntrade-off between temporal abstraction and reactiveness changes during an episode of Tennis2D.\nThis is reﬂected in the way HiTS distributes timed subgoals over the episode. Fig. S3 shows a\nhistogram of the difference between subgoal achievement times and the time of the contact between\nball and racket for HiTS and HAC. While HiTS uses more timed subgoals around the time of the\ncontact, HAC distributes its subgoals more uniformly over the episode. This indicates that the\nhigher level of the HiTS hierarchy has learned to choose small ∆t when it is crucial for performance\nand larger ones when it can afford it. The explicit representation of the time interval between\nconsecutive timed subgoals can be expected to facilitate HiTS’ adaptation of temporal abstraction to\nthe environment.\n2\n0\n2\nta\ntc [s]\nsubgoal density\nHAC\nHiTS\nFigure S3: Histogram of the difference between subgoal achievement times ta and the time tc of\nthe contact between racket and ball in the Tennis2D environment. HiTS uses more timed subgoals\naround the contact whereas HAC distributes its subgoals more uniformly over the episode. Averaged\nover 30 seeds.\nC.7\nStochasticity of the low-level policy while pursuing a timed subgoal\nIn Section 3 we discussed the assumption that the higher level learns to line up timed subgoals with\nthose points in time at which the agent inﬂuences the rest of the environment (see Fig. 2 (a)). Fig. 4 (h)\ndemonstrates that this happens in practice in the Tennis2D environment (at least up to a time scale\non which the inertia of the robot arm renders the inﬂuence of low-level actions insigniﬁcant). This\nalignment is incentivized by the use of SAC: The lower level, which is trained using a maximum\nentropy objective, is particularly stochastic inbetween timed subgoals as it can afford to explore there.\nWhen ∆t is close to 0, on the other hand, the noise on the agent’s state is small since this is necessary\nfor achieving the timed subgoal (see Fig. S4). Hence, the higher level is encouraged to align the\nsubgoal achievement times with those points in time when agent and environment interact so as to\nsolve the task unimpeded by low-level noise. It is an interesting direction for future work that could\nimprove sample efﬁciency to more actively guide the alignment of timed subgoals with interactions\nbetween agent and environment.\nC.8\nInﬂuence of the (timed) subgoal budget on performance\nAs the (timed) subgoal budget was not optimized except in the AntFourRooms environment, we\ninvestigated its inﬂuence on the performance of HiTS and HAC on the Platforms task. Fig. S5 shows\nthe success rate of both algorithms as a function of the (timed) subgoal budget. The maximum number\nof actions on the lower level (HAC) or the maximum ∆t (HiTS) was scaled inversely proportional to\nthe budget, all other hyperparameters were left ﬁxed. The performance of HAC improves slightly\nwith the subgoal budget but stagnates at around 50%. While HiTS proﬁts from a slightly increased\n18\n0\n250\n500\nt [ms]\n0.0\n0.1\n0.2\n0.3\nstandard deviation\n0\n1\n2\nFigure S4: Standard deviation of the joint angles (in radians) of the robot arm from the Tennis2D\nenvironment as a function of ∆t. The data was generated by initializing the robot arm in a ﬁxed\nposition and letting the lower level pursue the same timed subgoal 100 times.\ntimed subgoal budget performance suffers when increasing the budget further. This can be attributed\nto the simultaneous inverse scaling of the maximum ∆t which reduces temporal abstraction. In\nsummary, the (timed) subgoal budget does have an inﬂuence of the performance of both algorithms\nbut even when optimizing over it, HAC still cannot solve the Platforms environment.\n10\n20\n(timed) subgoal budget\n0.00\n0.25\n0.50\n0.75\n1.00\nsuccess rate\nHAC\nHiTS\nFigure S5: (Timed) subgoal budget sweep on the Platforms environment. The (timed) subgoal budget\nused for the experiments in the Platforms environment presented in the main text was 10. Success\nrates averaged over 30 seeds.\n19\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-12-06",
  "updated": "2021-12-06"
}