{
  "id": "http://arxiv.org/abs/2112.05848v3",
  "title": "Faster Deep Reinforcement Learning with Slower Online Network",
  "authors": [
    "Kavosh Asadi",
    "Rasool Fakoor",
    "Omer Gottesman",
    "Taesup Kim",
    "Michael L. Littman",
    "Alexander J. Smola"
  ],
  "abstract": "Deep reinforcement learning algorithms often use two networks for value\nfunction optimization: an online network, and a target network that tracks the\nonline network with some delay. Using two separate networks enables the agent\nto hedge against issues that arise when performing bootstrapping. In this paper\nwe endow two popular deep reinforcement learning algorithms, namely DQN and\nRainbow, with updates that incentivize the online network to remain in the\nproximity of the target network. This improves the robustness of deep\nreinforcement learning in presence of noisy updates. The resultant agents,\ncalled DQN Pro and Rainbow Pro, exhibit significant performance improvements\nover their original counterparts on the Atari benchmark demonstrating the\neffectiveness of this simple idea in deep reinforcement learning. The code for\nour paper is available here:\nGithub.com/amazon-research/fast-rl-with-slow-updates.",
  "text": "Faster Deep Reinforcement Learning with\nSlower Online Network\nKavosh Asadi\nAmazon Web Services\nRasool Fakoor\nAmazon Web Services\nOmer Gottesman\nBrown University\nTaesup Kim\nSeoul National University\nMichael L. Littman\nBrown University\nAlexander J. Smola\nAmazon Web Services\nAbstract\nDeep reinforcement learning algorithms often use two networks for value func-\ntion optimization: an online network, and a target network that tracks the online\nnetwork with some delay. Using two separate networks enables the agent to\nhedge against issues that arise when performing bootstrapping. In this paper we\nendow two popular deep reinforcement learning algorithms, namely DQN and\nRainbow, with updates that incentivize the online network to remain in the prox-\nimity of the target network. This improves the robustness of deep reinforcement\nlearning in presence of noisy updates. The resultant agents, called DQN Pro and\nRainbow Pro, exhibit signiﬁcant performance improvements over their original\ncounterparts on the Atari benchmark demonstrating the effectiveness of this simple\nidea in deep reinforcement learning. The code for our paper is available here:\nGithub.com/amazon-research/fast-rl-with-slow-updates.\n1\nIntroduction\nAn important competency of reinforcement-learning (RL) agents is learning in environments with\nlarge state spaces like those found in robotics (Kober et al., 2013), dialog systems (Williams et al.,\n2017), and games (Tesauro, 1994; Silver et al., 2017). Recent breakthroughs in deep RL have\ndemonstrated that simple approaches such as Q-learning (Watkins & Dayan, 1992) can surpass\nhuman-level performance in challenging environments when equipped with deep neural networks for\nfunction approximation (Mnih et al., 2015).\nTwo components of a gradient-based deep RL agent are its objective function and optimization\nprocedure. The optimization procedure takes estimates of the gradient of the objective with respect to\nnetwork parameters and updates the parameters accordingly. In DQN (Mnih et al., 2015), for example,\nthe objective function is the empirical expectation of the temporal difference (TD) error (Sutton,\n1988) on a buffered set of environmental interactions (Lin, 1992), and variants of stochastic gradient\ndescent are employed to best minimize this objective function.\nA fundamental difﬁculty in this context stems from the use of bootstrapping. Here, bootstrapping\nrefers to the dependence of the target of updates on the parameters of the neural network, which is\nitself continuously updated during training. Employing bootstrapping in RL stands in contrast to\nsupervised-learning techniques and Monte-Carlo RL (Sutton & Barto, 2018), where the target of our\ngradient updates does not depend on the parameters of the neural network.\nMnih et al. (2015) proposed a simple approach to hedging against issues that arise when using\nbootstrapping, namely to use a target network in value-function optimization. The target network\nis updated periodically, and tracks the online network with some delay. While this modiﬁcation\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2112.05848v3  [cs.LG]  17 Apr 2023\nconstituted a major step towards combating misbehavior in Q-learning (Lee & He, 2019; Kim et al.,\n2019; Zhang et al., 2021), optimization instability is still prevalent (van Hasselt et al., 2018).\nOur primary contribution is to endow DQN and Rainbow (Hessel et al., 2018) with a term that ensures\nthe parameters of the online-network component remain in the proximity of the parameters of the\ntarget network. Our theoretical and empirical results show that our simple proximal updates can\nremarkably increase robustness to noise without incurring additional computational or memory costs.\nIn particular, we present comprehensive experiments on the Atari benchmark (Bellemare et al., 2013)\nwhere proximal updates yield signiﬁcant improvements, thus revealing the beneﬁts of using this\nsimple technique for deep RL.\n2\nBackground and Notation\nRL is the study of the interaction between an environment and an agent that learns to maximize\nreward through experience. The Markov Decision Process (Puterman, 1994), or MDP, is used to math-\nematically deﬁne the RL problem. An MDP is speciﬁed by the tuple ⟨S, A, R, P, γ⟩, where S is the\nset of states and A is the set of actions. The functions R : S × A →R and P : S × A × S →[0, 1]\ndenote the reward and transition dynamics of the MDP. Finally, a discounting factor γ is used to\nformalize the intuition that short-term rewards are more valuable than those received later.\nThe goal in the RL problem is to learn a policy, a mapping from states to a probability distribution\nover actions, π : S →P(A), that obtains high sums of future discounted rewards. An important\nconcept in RL is the state value function. Formally, it denotes the expected discounted sum of future\nrewards when committing to a policy π in a state s: vπ(s) := E\n\u0002 P∞\nt=0 γtRt\n\f\fS0 = s, π\n\u0003\n. We deﬁne\nthe Bellman operator T π as follows:\n\u0002\nT πv\n\u0003\n(s) :=\nX\na∈A\nπ(a | s)\n\u0000R(s, a) +\nX\ns′∈S\nγ P(s, a, s′)v(s′)\n\u0001\n,\nwhich we can write compactly as: T πv := Rπ + γP πv , where\n\u0002\nRπ\u0003\n(s) = P\na∈A π(a|s)R(s, a)\nand\n\u0002\nP πv\n\u0003\n(s) = P\na∈A π(a | s) P\ns′∈SP(s, a, s′)v(s′). We also denote: (T π)nv := T π · · · T π\n|\n{z\n}\nn compositions\nv .\nNotice that vπ is the unique ﬁxed-point of (T π)n for all natural numbers n, meaning that vπ =\n(T π)nvπ , for all n. Deﬁne v⋆as the optimal value of a state, namely: v⋆(s) := maxπ vπ(s), and π⋆\nas a policy that achieves v⋆(s) for all states. We deﬁne the Bellman Optimality Operator T ⋆:\n\u0002\nT ⋆v\n\u0003\n(s) :=max\na∈A R(s, a) +\nX\ns′∈S\nγ P(s, a, s′)v(s′) ,\nwhose ﬁxed point is v⋆. These operators are at the heart of many planning and RL algorithms\nincluding Value Iteration (Bellman, 1957) and Policy Iteration (Howard, 1960).\n3\nProximal Bellman Operator\nIn this section, we introduce a new class of Bellman operators that ensure that the next iterate in\nplanning and RL remain in the vicinity of the previous iterate. To this end, we deﬁne the Bregman\nDivergence generated by a convex function f:\nDf(v′, v) := f(v′) −f(v) −⟨∇f(v), v′ −v⟩.\nExamples include the lp norm generated by f(v) = 1\n2 ∥v∥2\np and the Mahalanobis Distance generated\nby f(v) = 1\n2⟨v, Qv⟩for a positive semi-deﬁnite matrix Q.\nWe now deﬁne the Proximal Bellman Operator (T π\nc,f)n:\n(T π\nc,f)nv := arg min\nv′ ||v′ −(T π)nv||2\n2 + 1\nc Df(v′, v) ,\n(1)\nwhere c ∈(0, ∞). Intuitively, this operator encourages the next iterate to be in the proximity of\nthe previous iterate, while also having a small difference relative to the point recommended by the\n2\noriginal Bellman Operator. The parameter c could, therefore, be thought of as a knob that controls\nthe degree of gravitation towards the previous iterate.\nOur goal is to understand the behavior of Proximal Bellman Operator when used in conjunction with\nthe Modiﬁed Policy Iteration (MPI) algorithm (Puterman, 1994; Scherrer et al., 2015). Deﬁne Gv\nas the greedy policy with respect to v. At a certain iteration k, Proximal Modiﬁed Policy Iteration\n(PMPI) proceeds as follows:\nπk\n←\nGvk−1 ,\n(2)\nvk\n←\n(T πk\nc,f )nvk−1 .\n(3)\nThe pair of updates above generalize existing algorithms. Notably, with c →∞and general n we get\nMPI, with c →∞and n = 1 the algorithm reduces to Value Iteration, and with c →∞and n = ∞\nwe have a reduction to Policy Iteration. For ﬁnite c, the two extremes of n, namely n = 1 and n = ∞,\ncould be thought of as the proximal versions of Value Iteration and Policy Iteration, respectively.\nTo analyze this approach, it is ﬁrst natural to ask if each iteration of PMPI could be thought of as\na contraction so we can get sound and convergent behavior in planning and learning. For n > 1,\nScherrer et al. (2015) constructed a contrived MDP demonstrating that one iteration of MPI can\nunfortunately expand. As PMPI is just a generalization of MPI, the same example from Scherrer\net al. (2015) shows that PMPI can expand. In the case of n = 1, we can rewrite the pair of equations\n(2) and (3) in a single update as follows: vk ←T ⋆\nc,fvk−1 . When c →∞, standard proofs can be\nemployed to show that the operator is a contraction (Littman & Szepesvári, 1996). We now show that\nT ⋆\nc,f is a contraction for ﬁnite values of c. See our appendix for proofs.\nTheorem 1. The Proximal Bellman Optimality Operator T ⋆\nc,f is a contraction with ﬁxed point v⋆.\nTherefore, we get convergent behavior when using T ⋆\nc,f in planning and RL. The addition of the\nproximal term is fortunately not changing the ﬁxed point, thus not negatively affecting the ﬁnal\nsolution. This could be thought of as a form of regularization that vanishes in the limit; the algorithm\nconverges to v⋆even without decaying 1/c.\nGoing back to the general n ≥1 case, we cannot show contraction, but following previous work (Bert-\nsekas & Tsitsiklis, 1996; Scherrer et al., 2015), we study error propagation in PMPI in presence of\nadditive noise where we get a noisy sample of the original Bellman Operator (T πk)nvk−1 + ϵk. The\nnoise can stem from a variety of reasons, such as approximation or estimation error. For simplicity,\nwe restrict the analysis to Df(v′, v) = ||v′ −v||2\n2, so we rewrite update (3) as:\nvk ←arg min\nv′ ||v′ −\n\u0000(T πk)nvk−1 + ϵk\n\u0001\n||2\n2 + 1\nc ∥v′ −vk−1∥2\n2\nwhich can further be simpliﬁed to:\nvk ←(1 −β)(T πk)nvk−1 + βvk−1\n|\n{z\n}\n:=(T\nπk\nβ\n)nvk−1\n+(1 −β)ϵk,\nwhere β =\n1\n1+c. This operator is a generalization of the operator proposed by Smirnova & Dohmatob\n(2020) who focused on the case of n = 1. To build some intuition, notice that the update is multiplying\nerror ϵk by a term that is smaller than one, thus better hedging against large noise. While the update\nmay slow progress when there is no noise, it is entirely conceivable that for large enough values of ϵk,\nit is better to use non-zero β values. In the following theorem we formalize this intuition. Our result\nleans on the theory provided by Scherrer et al. (2015) and could be thought of as a generalization of\ntheir theorem for non-zero β values.\nTheorem 2. Consider the PMPI algorithm speciﬁed by:\nπk\n←\nGϵ′\nkvk−1 ,\n(4)\nvk\n←\n(T πk\nβ )nvk−1 + (1 −β)ϵk .\n(5)\nDeﬁne the Bellman residual bk := vk −T πk+1vk, and error terms xk := (I −γP πk)ϵk and\nyk := γP π∗ϵk. After k steps:\nv∗−vπk = vπ∗−(T πk+1\nβ\n)nvk\n|\n{z\n}\ndk\n+ (T πk+1\nβ\n)nvk −vπk\n|\n{z\n}\nsk\n3\n• where dk ≤γP π∗dk−1 −\n\u0000(1 −β)yk−1 + βbk−1\n\u0001\n+ (1 −β) Pn−1\nj=1 (γP πk)jbk−1 + ϵ′\nk\n• sk ≤\n\u0000(1 −β)(γP πk)n + βI\n\u0001\n(I −γP πk)−1bk−1\n• bk ≤\n\u0000(1 −β)(γP πk)n + βI\n\u0001\nbk−1 + (1 −β)xk + ϵ′\nk+1\nThe bound provides intuition as to how the proximal Bellman Operator can accelerate convergence in\nthe presence of high noise. For simplicity, we will only analyze the effect of the ϵ noise term, and\nignore the ϵ′ term. We ﬁrst look at the Bellman residual, bk. Given the Bellman residual in iteration\nk −1, bk−1, the only inﬂuence of the noise term ϵk on bk is through the (1 −β)xk, term, and we see\nthat bk decreases linearly with larger β.\nThe analysis of sk is slightly more involved but follows similar logic. The bound for sk can be\ndecomposed into a term proportional to bk−1 and a term proportional to βbk−1, where both are\nmultiplied with positive semi-deﬁnite matrices. Since bk−1 itself linearly decreases with β, we\nconclude that larger β decreases the bound quadratically.\nThe effect of β on the bound for dk is more complex. The terms βyk−1 and Pn−1\nj=1 (γP πk)jbk−1\nintroduce a linear decrease of the bound on dk with β, while the term β(I −Pn−1\nj=1 (γP πk)j)bk−1\nintroduces a quadratic dependence whose curvature depends on I −Pn−1\nj=1 (γPπk)j. This complex\ndependence on β highlights the trade-off between noise reduction and magnitude of updates. To\nunderstand this trade-off better, we examine two extreme cases for the magnitude on the noise. When\nthe noise is very large, we may set β = 1, equivalent to an inﬁnitely strong proximal term. It is easy\nto see that for β = 1, the values of dk and sk remain unchanged, which is preferable to the increase\nthey would suffer in the presence of very large noise. On the other extreme, when no noise is present,\nthe xk and yk terms in Theorem 2 vanish, and the bounds on dk and sk can be minimized by setting\nβ = 0, i.e. without noise the proximal term should not be used and the original Bellman update\nperformed. Intermediate noise magnitudes thus require a value of β that balances the noise reduction\nand update size.\n4\nDeep Q-Network with Proximal Updates\nWe now endow DQN-style algorithms with proximal updates. Let ⟨s, a, r, s′⟩denote a buffered tuple\nof interaction. Deﬁne the following objective function:\nh(θ, w) := bE⟨s,a,r,s′⟩\nh\u0000r + γ max\na′\nbQ(s′, a′; θ) −bQ(s, a; w)\n\u00012i\n.\n(6)\nOur proximal update is deﬁned as follows:\nwt+1 ←arg min\nw h(wt, w) + 1\n2˜c ∥w −wt∥2\n2 .\n(7)\nThis algorithm closely resembles the standard proximal-point algorithm (Rockafellar, 1976; Parikh &\nBoyd, 2014) with the important caveat that the function h is now taking two vectors as input. At each\niteration, we hold the ﬁrst input constant while optimizing over the second input.\nIn the optimization literature, the proximal-point algorithm is well-studied in contexts where\nan analytical solution to (7) is available.\nWith deep learning no closed-form solution exists,\nso we approximately solve (7) by taking a ﬁxed number of descent steps using stochastic gra-\ndients.\nSpeciﬁcally, starting each iteration with w = wt, we perform multiple w updates\nw ←w −α\n\u0000∇2h(wt, w) + 1\n˜c(w −wt)\n\u0001\n. We end the iteration by setting wt+1 ←w. To make\na connection to standard deep RL, the online weights w could be thought of as the weights we\nmaintain in the interim to solve (7) due to lack of a closed-form solution. Also, what is commonly\nreferred to as the target network could better be thought of as just the previous iterate in the above\nproximal-point algorithm.\nObserve that the update can be written as: w ←\n\u00001 −(α/˜c)\n\u0001\n· w + (α/˜c) · wt −α∇2h(wt, w) .\nNotice the intuitively appealing form: we ﬁrst compute a convex combination of wt and w, based\non the hyper-parameters α and ˜c, then add the gradient term to arrive at the next iterate of w. If wt\nand w are close, the convex combination is close to w itself and so this DQN with proximal update\n4\n(DQN Pro) would behave similarly to the original DQN. However, when w strays too far from wt,\ntaking the convex combination ensures that w gravitates towards the previous iterate wt. The gradient\nsignal from minimizing the squared TD error (6) should then be strong enough to cancel this default\ngravitation towards wt. The update includes standard DQN as a special case when ˜c →∞. The\npseudo-code for DQN is presented in the Appendix. The difference between DQN and DQN Pro is\nminimal (shown in gray), and corresponds with a few lines of code in our implementation.\n5\nExperiments\nIn this section, we empirically investigate the effectiveness of proximal updates in planning and\nreinforcement-learning algorithms. We begin by conducting experiments with PMPI in the context of\napproximate planning, and then move to large-scale RL experiments in Atari.\n5.1\nPMPI Experiments\nWe now focus on understanding the empirical impact of adding the proximal term on the performance\nof approximate PMPI. To this end, we use the pair of update equations:\nπk\n←\nGvk−1 ,\nvk\n←\n(1 −β)\n\u0000(T πk)nvk−1 + ϵk\n\u0001\n+ βvk−1 .\nFor this experiment, we chose the toy 8×8 Frozen Lake environment from Open AI Gym (Brockman\net al., 2016), where the transition and reward model of the environment is available to the planner.\nUsing a small environment allows us to understand the impact of the proximal term in the simplest\nand most clear setting. Note also that we arranged the experiment so that the policy greediﬁcation\nstep Gvk−1 ∀k is error-free, so we can solely focus on the interplay between the proximal term and\nthe error caused by imperfect policy evaluation.\nWe applied 100 iterations of PMPI, then measured the quality of the resultant policy π := π100 as\ndeﬁned by the distance between its true value and that of the optimal policy, namely ∥V ⋆−V π∥∞.\nWe repeated the experiment with different magnitudes of error, as well as different values of the β\nparameter.\n0.00\n0.25\n0.50\n0.70\n0.95\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n||V\nV ||\nn = 1\n= 0\n Uniform(-0.001,0.001)\n Uniform(-0.005,0.005)\n Uniform(-0.01,0.01)\n Uniform(-0.05,0.05)\n Uniform(-0.1,0.1)\n0.00\n0.25\n0.50\n0.70\n0.95\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n||V\nV ||\nn = 3\n= 0\n Uniform(-0.001,0.001)\n Uniform(-0.005,0.005)\n Uniform(-0.01,0.01)\n Uniform(-0.05,0.05)\n Uniform(-0.1,0.1)\nFigure 1: Performance of approximate PMPI has a U-shaped dependence on the parameter β =\n1\n1+c.\nResults are averaged over 30 random seeds with n = 1 (left) and n = 3 (right).\nFrom Figure 1, it is clear that the ﬁnal performance exhibits a U-shape with respect to the parameter\nβ. It is also noticable that the best-performing β is shifting to the right side (larger values) as we\nincrease the magnitude of noise. This trend makes sense, and is consistent with what is predicted by\nTheorem 2: As the noise level rises, we have more incentive to use larger (but not too large) β values\nto hedge against it.\n5.2\nAtari Experiments\nIn this section, we evaluate the proximal (or Pro) agents relative to their original DQN-style counter-\nparts on the Atari benchmark (Bellemare et al., 2013), and show that endowing the agent with the\nproximal term can lead into signiﬁcant improvements in the interim as well as in the ﬁnal performance.\nWe next investigate the utility of our proposed proximal term through further experiments. Please see\nthe Appendix for a complete description of our experimental pipeline.\n5\n5.2.1\nSetup\nWe used 55 Atari games (Bellemare et al., 2013) to conduct our experimental evaluations. Following\nMachado et al. (2018) and Castro et al. (2018), we used sticky actions to inject stochasticity into the\notherwise deterministic Atari emulator.\nOur training and evaluation protocols and the hyper-parameter settings follow those of the Dopamine\nbaseline (Castro et al., 2018). To report performance, we measured the undiscounted sum of\nrewards obtained by the learned policy during evaluation. We further report the learning curve\nfor all experiments averaged across 5 random seeds. We reiterate that we used the exact same\nhyper-parameters for all agents to ensure a sound comparison.\nFigure 2: A minimal hyper-parameter tuning for ˜c in DQN Pro (left) and Rainbow Pro (right).\nOur Pro agents have a single additional hyper-parameter ˜c. We did a minimal random search on 6\ngames to tune ˜c. Figure 2 visualizes the performance of Pro agents as a function of ˜c. In light of this\nresult, we set ˜c = 0.2 for DQN Pro and ˜c = 0.05 for Rainbow Pro. We used these values of ˜c for all\n55 games, and note that we performed no further hyper-parameter tuning at all.\n5.2.2\nResults\nThe ﬁrst question is whether endowing the DQN agent with the proximal term can yield signiﬁcant\nimprovements over the original DQN. Figure 3 (top) shows a comparison between DQN and DQN\nPro in terms of the ﬁnal performance. In particular, following standard practice (Wang et al., 2016;\nDabney et al., 2018; van Seijen et al., 2019), for each game we compute:\nScoreDQN Pro −ScoreDQN\nmax(ScoreDQN, ScoreHuman) −ScoreRandom\n.\nBars shown in red indicate the games in which we observed better ﬁnal performance for DQN Pro\nrelative to DQN, and bars in blue indicate the opposite. The height of a bar denotes the magnitude of\nthis improvement for the corresponding benchmark; notice that the y-axis is scaled logarithmically.\nWe took human and random scores from previous work (Nair et al., 2015; Dabney et al., 2018). It is\nclear that DQN Pro dramatically improves upon DQN. We defer to the Appendix for full learning\ncurves on all games tested.\nCan we fruitfully combine the proximal term with some of the existing algorithmic improvements\nin DQN? To answer this question, we build on the Rainbow algorithm of Hessel et al. (2018) who\nsuccessfully combined numerous important algorithmic ideas in the value-based RL literature. We\npresent this result in Figure 3 (bottom). Observe that the overall trend is for Rainbow Pro to yield\nlarge performance improvements over Rainbow.\nAdditionally, we measured the performance of our agents relative to human players. To this end, and\nagain following previous work (Wang et al., 2016; Dabney et al., 2018; van Seijen et al., 2019), for\neach agent we compute the human-normalized score:\nScoreAgent −ScoreRandom\nScoreHuman −ScoreRandom\n.\nIn Figure 4 (left), we show the median of this score for all agents, which Wang et al. (2016) and Hessel\net al. (2018) argued is a sensible quantity to track. We also show per-game learning curves with\nstandard error in the Appendix.\n6\nHero\nDemonAttack\nJourneyEscape\nAssault\nCarnival\nFreeway\nKrull\nPong\nAlien\nBowling\nRobotank\nYarsRevenge\nChopperCommand\nMontezumaRevenge\nAsteroids\nBerzerk\nAtlantis\nSolaris\nCrazyClimber\nMsPacman\nBattleZone\nPitfall\nFrostbite\nRoadRunner\nTutankham\nPrivateEye\nRiverraid\nQbert\nPhoenix\nVenture\nBankHeist\nAmidar\nGravitar\nKungFuMaster\nFishingDerby\nSeaquest\nGopher\nIceHockey\nAsterix\nBeamRider\nZaxxon\nPooyan\nVideoPinball\nAirRaid\nUpNDown\nJamesbond\nBreakout\nBoxing\nStarGunner\nSpaceInvaders\nKangaroo\nEnduro\nWizardOfWor\nTimePilot\nNameThisGame\n- 50%\n- 50%\n- 10%\n- 10%\n0%\n0%\n+ 10%\n+ 10%\n+ 50%\n+ 50%\n+ 200%\n+ 200%\nGravitar\nBankHeist\nPrivateEye\nMontezumaRevenge\nHero\nPitfall\nFrostbite\nAmidar\nAtlantis\nKangaroo\nBowling\nChopperCommand\nKrull\nRiverraid\nJamesbond\nVenture\nJourneyEscape\nCarnival\nSolaris\nAsteroids\nFishingDerby\nBattleZone\nBoxing\nVideoPinball\nMsPacman\nQbert\nFreeway\nUpNDown\nEnduro\nRobotank\nRoadRunner\nTutankham\nCrazyClimber\nBreakout\nPooyan\nBerzerk\nAlien\nYarsRevenge\nPong\nTimePilot\nAssault\nIceHockey\nKungFuMaster\nZaxxon\nDemonAttack\nNameThisGame\nStarGunner\nBeamRider\nSeaquest\nAirRaid\nGopher\nWizardOfWor\nAsterix\nSpaceInvaders\nPhoenix\n- 50%\n- 50%\n- 10%\n- 10%\n0%\n0%\n+ 10%\n+ 10%\n+ 100%\n+ 100%\n+ 3000%\n+ 3000%\nFigure 3: Final gain for DQN Pro over DQN (top), and Rainbow Pro over Rainbow (bottom),\naveraged over 5 seeds. DQN Pro and Rainbow Pro signiﬁcantly outperform their original counterparts.\nWe make two key observations from this ﬁgure. First, the very basic DQN Pro agent is capable\nof achieving human-level performance (1.0 on the y-axis) after 120 million frames. Second, the\nRainbow Pro agent achieves 220 percent human-normalized score after only 120 million frames.\n5.2.3\nAdditional Experiments\nOur purpose in endowing the agent with the proximal term was to keep the online network in the\nvicinity of the target network, so it would be natural to ask if this desirable property can manifest\nitself in practice when using the proximal term. In Figure 4, we answer this question afﬁrmatively\nby plotting the magnitude of the update to the target network during synchronization. Notice that\nwe periodically synchronize online and target networks, so the proximity of the online and target\nnetwork should manifest itself in a low distance between two consecutive target networks. Indeed,\nthe results demonstrate the success of the proximal term in terms of obtaining the desired proximity\nof online and target networks.\nWhile using the proximal term leads to signiﬁcant improvements, one may still wonder if the advan-\ntage of DQN Pro over DQN is merely stemming from a poorly-chosen period hyper-parameter in the\noriginal DQN, as opposed to a truly more stable optimization in DQN Pro. To refute this hypothesis,\nwe ran DQN with various settings of the period hyper-parameter {2000, 4000, 8000, 12000}. This\nset included the default value of the hyper-parameter (8000) from the original paper (Mnih et al.,\n2015), but also covered a wider set of settings.\nAdditionally, we tried an alternative update strategy for the target network, referred to as Polyak\naveraging, which was popularized in the context of continuous-action RL (Lillicrap et al., 2015):\nθ ←τw + (1 −τ)θ. For this update strategy, too, we tried different settings of the τ hyper-parameter,\nnamely {0.05, 0.005, 0.0005}, which includes the value 0.005 used in numerous papers (Lillicrap\net al., 2015; Fujimoto et al., 2018; Asadi et al., 2021).\nFigure 5 presents a comparison between DQN Pro and DQN with periodic and Polyak target updates\nfor various hyper-parameter settings of period and τ. It is clear that DQN Pro is consistently\n7\n0\n40\n80\n120\n                Training Frames (Million)             \n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n human-normalized median\nRainbow Pro\nRainbow\nDQN Pro\nDQN\nFigure 4: (left): Human-normalized median performance for DQN, Rainbow, DQN Pro, and Rainbow\nPro on 55 Atari games. Results are averaged over 5 independent seeds. Our agents, Rainbow Pro\n(yellow) and DQN Pro (red) outperform their original counterparts Rainbow (green) and DQN (blue).\n(right): Using the proximal term reduces the magnitude of target network updates.\nFigure 5: A comparison between DQN Pro and DQN with periodic (left) and Polyak (right) updates.\noutperforming the two alternatives regardless of the speciﬁc values of period and τ, thus clearly\ndemonstrating that the improvement is stemming from a more stable optimization procedure leading\nto a better interplay between the two networks.\nFinally, an alternative approach to ensuring lower distance between the online and the target network\nis to anneal the step size based on the number of updates performed on the online network since\nthe last online-target synchronization. In this case we performed this experiment in 4 games where\nwe knew proximal updates provide improvements based on our DQN Pro versus DQN resulst in\nFigure 3. In this case we linearly decreased the step size from the original DQN learning rate α\nto α′ ≪α where we tuned α′ using random search. Annealing indeed improves DQN, but DQN\nPro outperforms the improved version of DQN. Our intuition is that Pro agents only perform small\nupdates when the target network is far from the online network, but naively decaying the learning\nrate can harm progress when the two networks are in vicinity of each other.\n6\nDiscussion\nIn our experience using proximal updates in the parameter space were far superior than proximal\nupdates in the value space. We believe this is because the parameter-space deﬁnition can enforce the\nproximity globally, while in the value space one can only hope to obtain proximity locally and on a\nbatch of samples. One may hope to use natural gradients to enforce value-space proximity in a more\nprincipled way, but doing so usually requires signiﬁcantly more computational resources Knight &\n8\nFigure 6: Learning-rate decay does not bridge the gap between DQN and DQN Pro.\nLerner (2018). This is in contrast to our proximal updates which add negligible computational cost in\nthe simple form of taking a dimension-wise weighted average of two weight vectors.\nIn addition, for a smooth (Lipschitz) Q function, performing parameter-space regularization guaran-\ntees function-space regularization. Concretely: ∀s, ∀a |Q(s, a; θ)−Q(s, a; θ′)| ≤L||θ −θ′|| , where\nL is the Lipschitz constant of Q. Moreover, deep networks are Lipschitz (Neyshabur et al., 2015;\nAsadi et al., 2018), because they are constructed using compositions of Lipschitz functions (such as\nReLU, convolutions, etc) and that composition of Lipschitz functions is Lipschitz. So performing\nvalue-space updates may be an overkill. Lipschitz property of deep networks has successfully been\nleveraged in other contexts, such as in generative adversarial training Arjovsky et al. (2017).\nA key selling point of our result is simplicity, because simple results are easy to understand, implement,\nand reproduce. We obtained signiﬁcant performance improvements by adding just a few lines of\ncodes to the publicly available implementations of DQN and Rainbow Castro et al. (2018).\n7\nRelated Work\nThe introduction of proximal operators could be traced back to the seminal work of Moreau (1962,\n1965), Martinet (1970) and Rockafellar (1976), and the use of the proximal operators has since\nexpanded into many areas of science such as signal processing (Combettes & Pesquet, 2009),\nstatistics and machine learning (Beck & Teboulle, 2009; Polson et al., 2015; Reddi et al., 2015), and\nconvex optimization (Parikh & Boyd, 2014; Bertsekas, 2011b,a).\nIn the context of RL, Mahadevan et al. (2014) introduced a proximal theory for deriving convergent\noff-policy algorithms with linear function approximation. One intriguing characteristic of their\nwork is that they perform updates in primal-dual space, a property that was leveraged in sample\ncomplexity analysis (Liu et al., 2020) for the proximal counterparts of the gradient temporal-difference\nalgorithm (Sutton et al., 2008). Proximal operators also have appeared in the deep RL literature.\nFor instance, Fakoor et al. (2020b) used proximal operators for meta learning, and Maggipinto et al.\n(2020) improved TD3 (Fujimoto et al., 2018) by employing a stochastic proximal-point interpretation.\nThe effect of the proximal term in our work is reminiscent of the use of trust regions in policy-gradient\nalgorithms (Schulman et al., 2015, 2017; Wang et al., 2019; Fakoor et al., 2020a; Tomar et al., 2021).\nHowever, three factors differentiate our work: we deﬁne the proximal term using the value function,\nnot the policy, we enforce the proximal term in the parameter space, as opposed to the function space,\nand we use the target network as the previous iterate in our proximal deﬁnition.\n8\nConclusion and Future work\nWe showed a clear advantage for using proximal terms to perform slower but more effective updates\nin approximate planning and reinforcement learning. Our results demonstrated that proximal updates\nlead to more robustness with respect to noise. Several improvements to proximal methods exist,\nsuch as the acceleration algorithm (Nesterov, 1983; Li & Lin, 2015), as well as using other proximal\nterms (Combettes & Pesquet, 2009), which we leave for future work.\n9\nAcknowledgment\nWe thank Lihong Li, Pratik Chaudhari, and Shoham Sabach for their valuable insights in different\nstages of this work.\n9\nReferences\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In ICML,\n2017.\nAsadi et al. Lipschitz continuity in model-based reinforcement learning. In ICML, 2018.\nAsadi, K., Parikh, N., Parr, R. E., Konidaris, G. D., and Littman, M. L. Deep radial-basis value\nfunctions for continuous control. In AAAI Conference on Artiﬁcial Intelligence, 2021.\nBeck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse\nproblems. SIAM Journal on Imaging Sciences, 2009.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 2013.\nBellman, R. E. Dynamic Programming. 1957.\nBertsekas, D. P. Incremental gradient, subgradient, and proximal methods for convex optimization:\nA survey. Optimization for Machine Learning, 2011a.\nBertsekas, D. P. Incremental proximal methods for large scale convex optimization. Mathematical\nProgramming, 2011b.\nBertsekas, D. P. and Tsitsiklis, J. N. Neuro-dynamic programming. Athena Scientiﬁc, 1996.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.\nOpenai gym, 2016.\nCastro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. Dopamine: A Research\nFramework for Deep Reinforcement Learning. 2018.\nCombettes, P. L. and Pesquet, J.-C. Proximal Splitting Methods in Signal Processing. Fixed-point\nalgorithms for inverse problems in science and engineering, 2009.\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. Implicit quantile networks for distributional\nreinforcement learning. In International conference on machine learning, pp. 1096–1105. PMLR,\n2018.\nFakoor, R., Chaudhari, P., and Smola, A. J. P3O: Policy-on policy-off policy optimization. In\nConference on Uncertainty in Artiﬁcial Intelligence, 2020a.\nFakoor, R., Chaudhari, P., Soatto, S., and Smola, A. J. Meta-Q-learning. In International Conference\non Learning Representations, 2020b.\nFujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic\nmethods. In International Conference on Machine Learning, 2018.\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B.,\nAzar, M., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In\nAAAI Conference on Artiﬁcial Intelligence, 2018.\nHoward, R. A. Dynamic programming and markov processes. 1960.\nKim, S., Asadi, K., Littman, M., and Konidaris, G. Deepmellow: removing the need for a target\nnetwork in deep q-learning. In International Joint Conference on Artiﬁcial Intelligence, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference\non Learning Representations, 2015.\nKnight, E. and Lerner, O. Natural gradient deep q-learning. arXiv preprint arXiv:1803.07482, 2018.\nKober, J., Bagnell, J. A., and Peters, J. Reinforcement learning in robotics: A survey. International\nJournal of Robotics Research, 2013.\n10\nLee, D. and He, N. Target-based temporal-difference learning. In International Conference on\nMachine Learning, 2019.\nLi, H. and Lin, Z. Accelerated proximal gradient methods for nonconvex programming. Advances in\nneural information processing systems, 2015.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\nContinuous control with deep reinforcement learning. In International Conference on Learning\nRepresentations, 2015.\nLin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine learning, 1992.\nLittman, M. L. and Szepesvári, C. A generalized reinforcement-learning model: Convergence and\napplications. In ICML, volume 96, pp. 310–318. Citeseer, 1996.\nLiu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M. Finite-sample analysis of proximal\ngradient TD algorithms. In Conference on Uncertainty in Artiﬁcial Intelligence, 2020.\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M.\nRevisiting the arcade learning environment: Evaluation protocols and open problems for general\nagents. Journal of Artiﬁcial Intelligence Research, 2018.\nMaggipinto, M., Susto, G. A., and Chaudhari, P.\nProximal deterministic policy gradient.\nIn\nInternational Conference on Intelligent Robots and Systems, 2020.\nMahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., and Liu, J.\nProximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual\nSpaces. arXiv, 2014.\nMartinet, B. Regularisation, d’inéquations variationelles par approximations succesives. Revue\nFrancaise d’informatique et de Recherche operationelle, 1970.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Ried-\nmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement\nlearning. Nature, 2015.\nMoreau, J. J. Fonctions convexes duales et points proximaux dans un espace hilbertien. Comptes\nrendus hebdomadaires des séances de l’Académie des sciences, 1962.\nMoreau, J. J. Proximité et dualité dans un espace hilbertien. Bulletin de la Société Mathématique de\nFrance, 1965.\nNair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V.,\nSuleyman, M., Beattie, C., Petersen, S., et al. Massively parallel methods for deep reinforcement\nlearning. arXiv preprint arXiv:1507.04296, 2015.\nNesterov, Y. A method for unconstrained convex minimization problem with the rate of convergence\no (1/kˆ 2). In Doklady an USSR, 1983.\nNeyshabur et al. Norm-based capacity control in neural networks. In COLT, 2015.\nParikh, N. and Boyd, S. proximal algorithms. Foundations and Trends in optimization, 2014.\nPolson, N. G., Scott, J. G., and Willard, B. T. Proximal algorithms in statistics and machine learning.\nStatistical Science, 2015.\nPuterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. 1994.\nReddi, S., Poczos, B., and Smola, A. Doubly robust covariate shift correction. In AAAI Conference\non Artiﬁcial Intelligence, 2015.\nRockafellar, R. T. Monotone operators and the proximal point algorithm. SIAM Journal on Control\nand Optimization, 1976.\n11\nScherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M. Approximate modiﬁed policy\niteration and its application to the game of tetris. J. Mach. Learn. Res., 16:1629–1676, 2015.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In\nInternational conference on machine learning, 2015.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization\nalgorithms. arXiv, 2017.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,\nL., Lai, M., Bolton, A., et al. Mastering the game of go without human knowledge. Nature, 2017.\nSmirnova, E. and Dohmatob, E. On the convergence of smooth regularized approximate value\niteration schemes. Advances in Neural Information Processing Systems, 33, 2020.\nSutton, R. S. Learning to predict by the methods of temporal differences. Machine learning, 1988.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. 2018.\nSutton, R. S., Szepesvári, C., and Maei, H. R. A convergent O(n) temporal-difference algorithm\nfor off-policy learning with linear function approximation. In Advances in Neural Information\nProcessing Systems, 2008.\nTesauro, G. TD-gammon, a self-teaching backgammon program, achieves master-level play. Neural\ncomputation, 1994.\nTomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M. Mirror descent policy optimization, 2021.\nvan Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement\nlearning and the deadly triad. arXiv, 2018.\nvan Seijen, H., Fatemi, M., and Tavakoli, A. Using a logarithmic mapping to enable lower discount\nfactors in reinforcement learning. Advances in Neural Information Processing Systems, 2019.\nWang, Y., He, H., Tan, X., and Gan, Y. Trust region-guided proximal policy optimization. In\nAdvances in Neural Information Processing Systems, 2019.\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N. Dueling network\narchitectures for deep reinforcement learning. In International conference on machine learning,\npp. 1995–2003. PMLR, 2016.\nWatkins, C. J. and Dayan, P. Q-learning. Machine learning, 1992.\nWilliams, J. D., Asadi, K., and Zweig, G. Hybrid code networks: practical and efﬁcient end-to-end\ndialog control with supervised and reinforcement learning. In Association for Computational\nLinguistics, 2017.\nZhang, S., Yao, H., and Whiteson, S. Breaking the deadly triad with a target network. arXiv, 2021.\n12\n10\nAppendix\n10.1\nPseudo-code for DQN Pro\nBelow, we present the pseudo-code for DQN Pro. Notice that the difference between DQN and DQN\nPro is minimal (highlighted in gray).\nAlgorithm 1 DQN with Proximal Iteration (DQN Pro)\n1: Initialize θ, N, period, replay buffer D, α, and ˜c\n2: s ←env.reset(), w ←θ, numUpdates ←0\n3: repeat\n4:\na ∼ϵ-greedy\n\u0000Q(s, ·; w)\n\u0001\n5:\ns′, r ←env.step(s, a)\n6:\nadd ⟨s, a, r, s′⟩to D\n7:\nif s′ is terminal then\n8:\ns ←env.reset()\n9:\nend if\n10:\nfor n in {1, ..., N} do\n11:\nsample B = {⟨s, a, r, s′⟩}, compute ∇wh(w)\n12:\nw←\n\u00001 −(α/˜c)\n\u0001\nw + (α/˜c)θ −α∇wh(w)\n13:\nnumUpdates ←numUpdates + 1\n14:\nif numUpdates % period = 0 then\n15:\nθ ←w\n16:\nend if\n17:\nend for\n18: until convergence\n10.2\nImplementation Details\nTable 1 and 2 show hyper-parameters, computing infrastructure, and libraries used for the experiments\nin this paper for all games tested. Our training and evaluation protocols and the hyper-parameter\nsettings closely follow those of the Dopamine baseline. To report performance results, we measured\nthe undiscounted sum of rewards obtained by the learned policy during evaluation.\n13\nDQN hyper-parameters (shared)\nReplay buffer size\n200000\nTarget update period\n8000\nMax steps per episode\n27000\nEvaluation frequency\n10000\nBatch size\n64\nUpdate period\n4\nNumber of frame skip\n4\nNumber of episodes to evaluate\n2\nUpdate horizon\n1\nϵ-greedy (training time)\n0.01\nϵ-greedy (evaluation time)\n0.001\nϵ-greedy decay period\n250000\nBurn-in period / Min replay size\n20000\nLearning rate\n10−4\nDiscount factor (γ)\n0.99\nTotal number of iterations\n3 × 107\nSticky actions\nTrue\nOptimizer\nAdam Kingma & Ba (2015)\nNetwork architecture\nNature DQN network Mnih et al. (2015)\nRandom seeds\n{0, 1, 2, 3, 4}\nRainbow hyper-parameters (shared)\nBatch size\n64\nOther\nConﬁg ﬁle rainbow_aaai.gin from Dopamine\nDQN Pro and Rainbow Pro hyper-parameter\n˜c (DQN Pro)\n0.2\n˜c (Rainbow Pro)\n0.05\nTable 1: Hyper-parameters used for all methods for all 55 games of Atari-2600 benchmarks\n. All results reported in our paper are averages over repeated runs initialized with each of the random\nseeds listed above and run for the listed number of episodes.\nComputing Infrastructure\nMachine Type\nAWS EC2 - p2.16xlarge\nGPU Family\nTesla K80\nCPU Family\nIntel Xeon 2.30GHz\nCUDA Version\n11.0\nNVIDIA-Driver\n450.80.02\nLibrary Version\nPython\n3.8.5\nNumpy\n1.20.1\nGym\n0.18.0\nPytorch\n1.8.0\nTable 2: Computing infrastructure and software libraries used in all experiments in this paper.\n14\n10.3\nProofs\nTheorem 1. The Proximal Bellman Optimality Operator T ⋆\nc,f is a contraction with ﬁxed point v⋆.\nWe make two assumptions:\n1. f is smooth, or more speciﬁcally that its gradient is 1-Lipschitz: ||∇f(v1) −∇f(v2)|| ≤\n||v1 −v2|| ∀v1, ∀v2 .\n2. the value of the parameter c is large, in particular c >\n2\n1−γ .\nProof. Both terms are convex and differentiable, therefore by setting the gradient to zero, we have:\nT ⋆\nc,fv = T ⋆v + 1\nc\n\u0000∇f(v) −∇f(T ⋆\nc,fv)\n\u0001\n,\nWe can then show:\n||T ⋆\nc,fv1 −T ⋆\nc,fv2||\n=\n||T ⋆v1 + 1\nc\n\u0000∇f(v1) −∇f(T ⋆\nc,fv1)\n\u0001\n−T ⋆v2 −1\nc\n\u0000∇f(v2) −∇f(T ⋆\nc,fv2)\n\u0001\n||\n≤\n||T ⋆v1 −T ⋆v2|| + 1\nc ||∇f(v1) −∇f(v2)|| + 1\nc ||∇f(T ⋆\nc,fv1) −∇f(T ⋆\nc,fv2)||\n(ﬁrst assumption)\n≤\n||T ⋆v1 −T ⋆v2|| + 1\nc ||∇f(v1) −∇f(v2)|| + 1\nc ||T ⋆\nc,fv1 −T ⋆\nc,fv2||\nThis implies:\nc −1\nc\n||T ⋆\nc,fv1 −T ⋆\nc,fv2||\n≤\n||T ⋆v1 −T ⋆v2|| + 1\nc ||∇f(v1) −∇f(v2)||\n(ﬁrst assumption)\n≤\n||T ⋆v1 −T ⋆v2|| + 1\nc ||v1 −v2||\n≤\nγc + 1\nc\n||v1 −v2||\nTherefore,\n||T ⋆\nc,fv1 −T ⋆\nc,fv2||\n≤\nγc + 1\nc −1 ||v1 −v2|| ,\nAllowing us to conclude that T ⋆\nc,f is a contraction (second assumption).\nFurther, to show that v⋆is indeed the ﬁxed point of T ⋆\nc,f, notice from the original formulation:\nT ⋆\nc,fv := arg min\nv′ ||v′ −T ⋆v||2\n2 + 1\nc Df(v′, v) ,\nthat, at point v∗setting v′ = v⋆jointly minimizes the ﬁrst term, because v⋆= T ⋆v⋆due to ﬁxed-\npoint deﬁntion, and it also minimizes the second term because Df(v⋆, v⋆) = 0 and that Bregman\ndivergence is non-negative. Therefore, T ⋆\nc,fv⋆= v⋆; v⋆is the ﬁxed-point of T ⋆\nc,fv⋆. Since, T ⋆\nc,f is a\ncontraction, this ﬁxed point is unique.\nTheorem 2. Consider the PMPI algorithm speciﬁed by:\nπk\n←\nGϵ′\nkvk−1 ,\n(4)\nvk\n←\n(T πk\nβ )nvk−1 + (1 −β)ϵk .\n(5)\nDeﬁne the Bellman residual bk := vk −T πk+1vk, and error terms xk := (I −γP πk)ϵk and\nyk := γP π∗ϵk. After k steps:\nv∗−vπk = vπ∗−(T πk+1\nβ\n)nvk\n|\n{z\n}\ndk\n+ (T πk+1\nβ\n)nvk −vπk\n|\n{z\n}\nsk\n15\n• where dk ≤γP π∗dk−1 −\n\u0000(1 −β)yk−1 + βbk−1\n\u0001\n+ (1 −β) Pn−1\nj=1 (γP πk)jbk−1 + ϵ′\nk\n• sk ≤\n\u0000(1 −β)(γP πk)n + βI\n\u0001\n(I −γP πk)−1bk−1\n• bk ≤\n\u0000(1 −β)(γP πk)n + βI\n\u0001\nbk−1 + (1 −β)xk + ϵ′\nk+1\nWe make two assumptions:\n1. we assume ϵ error in policy evaluation step, as already stated in equation (4).\n2. we assume ϵ′ error in policy greediﬁcation step πk ←Gϵ′\nkvk−1∀k. This means ∀π T πvk −\nT πk+1vk ≤ϵ′\nk+1. Note that this assumption is orthogonal to the thesis of our paper, but we\nkept it for generality.\nProof. Step 0: bound the Bellman residual: bk := vk −T πk+1vk .\nbk\n=\nvk −T πk+1vk\n=\nvk −T πkvk + T πkvk −T πk+1vk\n(from our assumption\n∀π T πvk −T πk+1vk ≤ϵ′\nk+1)\n≤\nvk −T πkvk + ϵ′\nk+1\n=\nvk −(1 −β)ϵk −T πkvk + (1 −β)γP πkϵk + (1 −β)ϵk −(1 −β)γP πkϵk + ϵ′\nk+1\n\u0010\nfrom T πkvk + (1 −β)γP πkϵk = T πk(vk −(1 −β)ϵk\n\u0001\u0011\n=\nvk −(1 −β)ϵk −T πk(vk −(1 −β)ϵk) + (1 −β) (I −γPπk)ϵk\n|\n{z\n}\nxk\n+ϵ′\nk+1\n=\nvk −(1 −β)ϵk −T πk(vk −(1 −β)ϵk) + (1 −β)xk + ϵ′\nk+1\n(from vk −(1 −β)ϵk = (T πk\nβ )nvk−1)\n=\n(1 −β)(T πk)nvk−1 + βvk−1 −T πk\u0000(1 −β)(T πk)nvk−1 + βvk−1\n\u0001\n+ (1 −β)xk + ϵ′\nk+1\n(from linearity of T πk)\n=\n(1 −β)(T πk)nvk−1 −T πk\u0000(1 −β)(T πk)nvk−1\n\u0001\n+ β\n\u0000vk−1 −T πkvk−1\n\u0001\n+ (1 −β)xk + ϵ′\nk+1\n=\n(1 −β)\n\u0010\n(T πk)nvk−1 −T πk\u0000(T πk)nvk−1\n\u0001\u0011\n+ β\n\u0000vk−1 −T πkvk−1\n\u0001\n+ (1 −β)xk + ϵ′\nk+1\n=\n(1 −β)\n\u0010\n(T πk)nvk−1 −(T πk)n\u0000T πkvk−1\n\u0001\u0011\n+ β\n\u0000vk−1 −T πkvk−1\n\u0001\n+ (1 −β)xk + ϵ′\nk+1\n=\n(1 −β)(γP πk)n\u0000vk−1 −T πk(vk−1\n|\n{z\n}\n=bk−1\n)\n\u0001\n+ β\n\u0000vk−1 −T πkvk−1\n|\n{z\n}\n=bk−1\n\u0001\n+ (1 −β)xk + ϵ′\nk+1 ,\nallowing us to conclude:\nbk =\n\u0000(1 −β)(γP πk)n + βI\n\u0001\nbk−1 + (1 −β)xk + ϵ′\nk+1 .\nStep 1: bound the distance to the optimal value: dk+1 := v∗−(T πk+1\nβ\n)nvk .\ndk+1\n=\nv∗−(T πk+1\nβ\n)nvk\n=\nT π∗v∗−T π∗vk + T π∗vk −T πk+1vk\n|\n{z\n}\n≤ϵ′\nk+1\n+ T πk+1vk −(T πk+1\nβ\n)nvk\n|\n{z\n}\n=gk+1\n≤\nγP π∗(v∗−vk) + ϵ′\nk+1 + gk+1\n=\nγP π∗(v∗−vk) + (1 −β)γP π∗ϵk −(1 −β)γP π∗ϵk + ϵ′\nk+1 + gk+1\n=\nγP π∗\u0010\nv∗−\n\u0000vk −(1 −β)ϵk\n\u0001\u0011\n−(1 −β) γP π∗ϵk\n| {z }\nyk\n+ϵ′\nk+1 + gk+1\n=\nγP π∗\u0000v∗−(T πk\nβ )nvk−1\n|\n{z\n}\n=dk\n\u0001\n−(1 −β)yk + ϵ′\nk+1 + gk+1\n=\nγP π∗dk −(1 −β)yk + ϵ′\nk+1 + gk+1\n16\nAdditionally we can bound gk+1 as follows:\ngk+1\n=\nT πk+1vk −(T πk+1\nβ\n)nvk\n=\n(1 −β)\n\u0000T πk+1vk −(T πk+1)nvk\n\u0001\n+ β(T πk+1vk −vk)\n=\n(1 −β)\nn−1\nX\nj=1\n(γP πk+1)jbk + β(−bk)\nAllowing us to conclude that:\ndk+1 ≤γP π∗dk −\n\u0000(1 −β)yk + βbk\n\u0001\n+ (1 −β)\nn−1\nX\nj=1\n(γP πk+1)jbk + ϵ′\nk+1\nStep 2: bound the distance between the approximate value and the value of the policy: sk :=\n(T πk\nβ )nvk−1 −vπk .\nsk\n=\n(T πk\nβ )nvk−1 −vπk\n=\n(T πk\nβ )nvk−1 −(T πk)∞vk−1\n=\n(1 −β)(T πk)nvk−1 + βvk−1 −(1 −β)(T πk)∞vk−1 −β(T πk)∞vk−1\n=\n(1 −β)\n\u0000(T πk)nvk−1 −(T πk)∞vk−1\n\u0001\n+ β\n\u0000vk−1 −(T πk)∞vk−1\n\u0001\n=\n(1 −β)(γP πk)n\u0000vk−1 −(T πk)∞vk−1\n\u0001\n+ β\n\u0000vk−1 −(T πk)∞vk−1\n\u0001\n=\n\u0000(1 −β)(γP πk)n + βI\n\u0001\u0000vk−1 −(T πk)∞vk−1\n\u0001\n=\n\u0000(1 −β)(γP πk)n + βI\n\u0001\n(I −γP πk)−1\u0000vk−1 −T πkvk−1\n|\n{z\n}\nbk−1\n\u0001\n.\nAllowing us to conclude that:\nsk =\n\u0000(1 −β)(γP πk)n + βI\n\u0001\n(I −γP πk)−1bk−1 .\n17\nFigure 7: Comparison between DQN Pro (red) and DQN (blue) over 55 Atari games (Part I).\n11\nLearning curves\nWe present full learning curves of DQN, DQN Pro, Rainbow, and Rainbow Pro for the 55 Atari\ngames. All results are averaged over 5 independent seeds.\n18\nFigure 8: Comparison between DQN Pro (red) and DQN (blue) over 55 Atari games (Part II).\n19\nFigure 9: Comparison between Rainbow Pro (yellow) and Rainbow (green) over 55 Atari games\n(Part I).\n20\nFigure 10: Comparison between Rainbow Pro (yellow) and Rainbow (green) over 55 Atari games\n(Part II).\n21\n0\n40\n80\nTraining Frames (Million)\n0\n5000\n10000\n15000\nPhoenix\n0\n40\n80\nTraining Frames (Million)\n500\n1000\n1500\n2000\n2500\nSpaceInvaders\n0\n40\n80\nTraining Frames (Million)\n2000\n4000\n6000\n8000\nAsterix\n0\n40\n80\nTraining Frames (Million)\n0\n2500\n5000\n7500\n10000\n12500\nGopher\nRainbow\nRainbow Pro with original c = 0.05\nRainbow Pro with c = 0.1\n0\n40\n80\nTraining Frames (Million)\n0\n500\n1000\n1500\n2000\n2500\nGravitar\n0\n40\n80\nTraining Frames (Million)\n0\n500\n1000\n1500\n2000\nAmidar\n0\n40\n80\nTraining Frames (Million)\n10000\n20000\n30000\nHero\n0\n40\n80\nTraining Frames (Million)\n2000\n4000\n6000\nFrostbite\nRainbow\nRainbow Pro with original c = 0.05\nRainbow Pro with c = 0.1\nFigure 11: A study on games with the strongest (top) and weakest performance for Rainbow Pro with\nthe original ˜c = 0.05. Using a slightly less powerful proximal term (corresponding to larger ˜c = 0.1)\nenables us to recover the downside (bottom) while still providing beneﬁts on games that are more\nconducive to using the proximal updates (top).\n12\nMotivating Example for Adaptive Updates\nIn this section we speciﬁcally look at 4 domains in which Rainbow Pro did signiﬁcantly better than\nthe original Rainbow, as well 4 domains where Rainbow Pro is underperforming Rainbow. Note,\nagain, that it is uncommon for Rainbow Pro with the original ˜c to underperform, but here we have a\ndeeper dive into these cases for a better understanding.\nFrom Figure 11, we observe that by using a slightly larger value of ˜c, which slightly decreases the\nincentive for online-target proximity, we can recover from the downside, while still maintaining\nsuperior performance on games that are conducive to proximal updates. This suggest that, while\nusing a ﬁxed ˜c value is enough to obtain signiﬁcant performance improvement, adaptively choosing ˜c\nwould provide us with even more reliable improvements when performing proximal updates. In this\ncontext, a promising idea would be to hinge on the variance of our gradient updates when setting ˜c.\nWe leave this promising direction for future work.\n22\nFigure 12: Performing proximal updates in the value space has a limited positive impact.\n13\nProximal Updates in the Value Space\nOur primary contribution was to show the usefulness of performing proximal updates in the parameter\nspace. That said, we also implemented a version of proximal updates that operated in the value space.\nMore speciﬁcally, in this case we updated the parameters of the online network as follows:\nw ←bE⟨s,a,r,s′⟩\nh\u0000r + γ max\na′\nbQ(s′, a′; θ) −bQ(s, a; w)\n\u00012i\n+ 1\n˜c\nbE⟨s,a⟩\nh\u0000 bQ(s, a; w) −bQ(s, a; θ)\n\u00012].\nWe conducted numerous experiments using variants of this idea (such as using separate replay buffer\nfor each term, performing the update for all actions in buffered states, etc) but we generally found the\nvalue-space updates to be ineffective. As mentioned in the main paper, we believe this is because\nthe parameter-space deﬁnition can enforce the proximity globally, while in the value space one can\nonly hope to obtain proximity locally and on a batch of samples. To perform global updates we\nmay need to compute the natural gradient, which typically requires matrix invasion Knight & Lerner\n(2018), and thus adding signiﬁcant computational burden to the original algorithm. In comparison,\nthe parameter-space version is effective, simple to implement, and capable of enforcing proximity\nglobally due to the Lipschitz property of neural networks.\n23\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2021-12-10",
  "updated": "2023-04-17"
}