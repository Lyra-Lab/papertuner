{
  "id": "http://arxiv.org/abs/2112.07055v2",
  "title": "Large Language Models are not Models of Natural Language: they are Corpus Models",
  "authors": [
    "Csaba Veres"
  ],
  "abstract": "Natural Language Processing (NLP) has become one of the leading application\nareas in the current Artificial Intelligence boom. Transfer learning has\nenabled large deep learning neural networks trained on the language modeling\ntask to vastly improve performance in almost all downstream language tasks.\nInterestingly, when the language models are trained with data that includes\nsoftware code, they demonstrate remarkable abilities in generating functioning\ncomputer code from natural language specifications. We argue that this creates\na conundrum for the claim that eliminative neural models are a radical\nrestructuring in our understanding of cognition in that they eliminate the need\nfor symbolic abstractions like generative phrase structure grammars. Because\nthe syntax of programming languages is by design determined by phrase structure\ngrammars, neural models that produce syntactic code are apparently\nuninformative about the theoretical foundations of programming languages. The\ndemonstration that neural models perform well on tasks that involve clearly\nsymbolic systems, proves that they cannot be used as an argument that language\nand other cognitive systems are not symbolic. Finally, we argue as a corollary\nthat the term language model is misleading and propose the adoption of the\nworking term corpus model instead, which better reflects the genesis and\ncontents of the model.",
  "text": "Date of publication xxx, 2022, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nLarge Language Models are not Models\nof Natural Language: they are Corpus\nModels.\nCSABA VERES\nDepartment of Information Science and Media Studies, University of Bergen, Norway (e-mail: csaba.veres@uib.no)\nThis work was supported by the News Angler Project through the Norwegian Research Council under Project 275872.\nABSTRACT\nNatural Language Processing (NLP) has become one of the leading application areas\nin the current Artiﬁcial Intelligence boom. Transfer learning has enabled large deep learning neural\nnetworks trained on the language modeling task to vastly improve performance in almost all downstream\nlanguage tasks. Interestingly, when the language models are trained with data that includes software code,\nthey demonstrate remarkable abilities in generating functioning computer code from natural language\nspeciﬁcations. We argue that this creates a conundrum for the claim that eliminative neural models are\na radical restructuring in our understanding of cognition in that they eliminate the need for symbolic\nabstractions like generative phrase structure grammars. Because the syntax of programming languages is by\ndesign determined by phrase structure grammars, neural models that produce syntactic code are apparently\nuninformative about the theoretical foundations of programming languages. The demonstration that neural\nmodels perform well on tasks that involve clearly symbolic systems, proves that they cannot be used as an\nargument that language and other cognitive systems are not symbolic. Finally, we argue as a corollary that\nthe term language model is misleading and propose the adoption of the working term corpus model instead,\nwhich better reﬂects the genesis and contents of the model.\nINDEX TERMS natural language processing, deep learning, syntax, linguistics, language model, automatic\nprogramming, neural networks\nI. INTRODUCTION\nDeep-learning Artiﬁcial Neural Networks (ANNs) imple-\nment a multi-layered machine learning architecture which\nenables sophisticated representation learning [1]. They have\nsigniﬁcantly changed the technological, societal and com-\nmercial landscape in the past decade [2, 3]. In this article\nwe focus on deep learning models for Natural Language\nProcessing (NLP). Transformer based deep learning models\n[4] have recorded signiﬁcant improvements in various natural\nlanguage tasks, and have entered service in industrial appli-\ncations that have a signiﬁcant linguistic component [5, 6]. As\nan engineering artefact, these models have clearly enjoyed\nsigniﬁcant success. What is less clear is how much they\nhave contributed to our understanding of natural language\nand cognitive architecture. While the most powerful systems\nhave shown remarkable abilities complex language tasks like\nquestion answering and writing prose about arbitrary topics\n[7], there are contrary claims that ANNs are nothing but\ngiant stochastic parrots, with many hidden dangers if mis-\nrepresented as systems that understand language in any non\ntrivial sense [8, 9].\nLanguage models are joint probability distributions over\nsequences of words, or alternatively, functions that return a\nprobability measure over strings drawn from some vocabu-\nlary [10, 11]. Large Neural LMs learn probability functions\nfor sequences of real valued, continuous vector representa-\ntions of words rather than discrete lexical items. Continuous\nrepresentations are effective at generalising across novel con-\ntexts, resulting in better performance across a range of tasks\n[11]. The probability distribution is learned through a form\nof language modeling, where the task is to \"predict the next\nword given the previous words\" [12] (p.191) in word strings\ndrawn from a corpus.\nNeural LMs stipulate a mental architecture that stands in\nstark contrast with a Classical symbolic view [13] dominant\nin Linguistics since at least the beginning of the 20th century,\nwhen language scholars began to study the structural proper-\nties of languages [14, 15]. The rigorous study of language\nVOLUME 4, 2016\n1\narXiv:2112.07055v2  [cs.CL]  15 Jun 2022\nVeres et al.: Language Models are not Models of Language\nas a cognitive faculty was pioneered by Noam Chomsky\nwho introduced Generative Phrase Structure Grammars as\nthe rule structures underlying linguistic competence [16, 17].\nIn subsequent years a number of important challenges and\nmodiﬁcations emerged, both from within and outside the re-\nsearch program1 [18]. Nevertheless, in the midst of technical\ncontroversies and disagreements, researchers have all agreed\nthat \"... linguistic knowledge is couched in the form of rules\nand principles.\" [19] (p.74).\nThe recent success of deep learning ANNs on wide ranging\nlinguistic tasks have given support to the claim that sta-\ntistical models are preferable to generative phrase structure\ngrammars as theories of linguistic competence. Manning and\nSchütze argue in their classic text \"Foundations of Statistical\nNatural Language Processing\" that cognition in general and\nlanguage in particular are \"best formalized as probabilistic\nprocesses\" [12] (p.15). A similar position is more vigorously\nexpressed by Peter Norvig in the context of a debate with\nNoam Chomsky at MIT2: \"Many phenomena in science are\nstochastic, and the simplest model of them is a probabilistic\nmodel; I believe language is such a phenomenon and there-\nfore that probabilistic models are our best tool for represent-\ning facts about language\" [20]. Perhaps most dramatically,\nGeoff Hinton has proclaimed in his Turing Award acceptance\nspeech that the success of \"machine translation was the ﬁnal\nnail in the cofﬁn of symbolic AI\" [21] (32’30”).\nIn this paper we will defend the Classical symbolic view\nwith an argument analogous to an indirect proof in logic,\nwhere the assumed truth of a premise leads to an absurd\nconclusion, thereby proving the falsehood of that premise.\nWe present the argument as follows. In section 2 we provide\nsome background by brieﬂy explaining what is meant by a\nrule based, generative phrase structure grammar in natural\nlanguage and software code. The argument itself begins in\nsection 3 which details why ANN models are an alternative\nto explicit rule based grammar. The assumed premise is that\nthe success of neural models makes them good explanatory\nmodels of natural language without the need for symbols. In\nsection 4 we describe recent successes using ANN models\n- especially language models - to automatically generate\nsoftware code. But this is where the contradiction arises. If\nANN models can be construed as explanatory theories for\nnatural language based on their successes on language tasks\nthen, in the absence of counter arguments, they should be\ngood explanatory theories for computer language as well.\nWe see no such argument and therefore arrive at the absurd\nconclusion that ANNs are good explanatory models of soft-\nware. We know this is absurd because it is just a fact that\nthe acceptable syntax of computer languages is determined\n1Chomsky describes his early work as an attempt to create a theoretical\napparatus which was rich enough to describe the data. But it was always\nunderstood that the initial machinery had to be wrong because such a rich,\ncomplex system couldn’t meet the criterion for biological evolution. The\nsubsequent years were spent by reducing the complexity of the theoretical\nmachinery https://youtu.be/pUWmTXkpHjE?t=3520\n2MIT150: Brains, Minds and Machines Symposium, June 16, 2011.\nTranscript available at https://chomsky.info/20110616/\nby their grammar. Therefore, successful ANN models of nat-\nural language cannot be used as evidence against generative\nphrase structure grammars in natural language. In section 5\nwe show that in fact language models are most accurately\ndescribed as corpus models. The paper then concludes.\nII. GENERATIVE PHRASE STRUCTURE GRAMMAR FOR\nNATURAL LANGUAGE AND SOFTWARE CODE\nLinguistics in the ﬁrst half of the 20th century was mainly\na taxonomic science with researchers pursuing Immediate\nConstituent (IC) analysis inspired by Bloomﬁeld [22], and\nSaussure’s structural linguistics as outlined in his 1916 book,\nCourse in General Linguistics. The goal of early linguists\nwas to develop methods that divide an expression into its\nimmediate constituents, and continue the subdivision until\nsyntactically indivisible parts were obtained. The essential\ninsight contributed by Chomsky was that the constituent\nstructure of language is the product of a system of rewrite\nrules of the form A →ω where A is a class label and\nω is a string that could contain terminal strings as well\nas other class labels [23]. An early example in [17] is the\nfollowing simple grammar. (Note that the early formulations\nof grammar did not yet include the necessary machinery for\nrecursive deﬁnitions [15].)\nSentence →NP + V P\n(1)\nNP →T + N\n(2)\nV P →V erb + NP\n(3)\nT →the\n(4)\nN →man, ball, etc.\n(5)\nV erb →hit, took, etc.\n(6)\nThis grammar can generate sentences of the type shown in\nFigure 1, through a series of derivations.\nFIGURE 1. A tree diagram showing a result of a sequence of derivations\nusing rules 1 - 6. It does not show the order of the derivation.\nSyntactic Structures [17] outlined a new agenda for a\nformal linguistic theory in which language was considered as\n\" ... a set (ﬁnite or inﬁnite) of sentences, each ﬁnite in length\nand constructed out of a ﬁnite set of elements\" [17](p. 13).\nSentences are the output of a generative grammar, and the\nLinguistics project is to explore grammars which generate all\nand only the grammatical sentences of natural languages.\nAn important distinction was drawn between linguistic\ncompetence, the speaker-hearer’s knowledge of language,\n2\nVOLUME 4, 2016\nVeres et al.: Language Models are not Models of Language\nand performance, actual instances of language use in con-\ncrete situations [24]. A grammar on this view is a description\nof competence. It is not only a theory about possible sentence\nstructure but about the intrinsic knowledge that allows the\nspeaker to generate an inﬁnite set of grammatical sentences\nwith a ﬁnite set of rules, and for the hearer to assign lin-\nguistic structure to each of those sentences. Performance, on\nthe other hand, includes the many psychological processes\nunderlying actual linguistic productions, including effects\nof attention, memory, and so on. The multitude of these\npsychological processes are often little understood and can\nresult in productions rife with errors from interference by\na range of unpredictable, non language speciﬁc processes.\nFor this reason Chomsky advocated using intuitions about\ngrammatical acceptability as the primary data for linguistic\ntheories rather than speech or text corpora.\nOne of the pioneers of high level computer programming\nlanguages, John W. Backus who led the Applied Science Di-\nvision of IBM’s Programming Research Group3 took inspi-\nration from Chomsky’s work on PSGs and conceived a meta-\nlanguage that could describe the syntax of languages that was\neasier for programmers to write than the machine languages\nof the time. The meta language later became known as\nBackus-Naur form (BNF), so called partly because it was\noriginally co-developed by Peter Naur in a 1963 IBM report\non the ALGOL 60 programming language\"4. The BNF is a\nnotation for context free grammars consisting of productions\nover terminal and nonterminal symbols, used to provide the\nprecise syntax of programming languages which is required\nfor writing compilers and interpreters for the language [25].\nThe original syntax described in the technical report included\nmeta characters as shown in (7)\n< > ::= |\n(7)\nwhere sequences of characters enclosed in the brackets\nrepresent metalinguistic variables and the ::= and | are\nmetalinguistic connectives. Productions or rewrite rules are\nexpressed using this machinery. For example the productions\nin (8) and (9) generate strings such as (10) and (11)\n<ab> ::= ( | [ | <ab> ( | <ab> <d>\n(8)\n<d> ::= 0 | 1 | 2 | 3 | 4 | 5 | 6\n(9)\n[(((1(36(\n(10)\n(12345(\n(11)\nBNF productions can be used to construct phrase structure\ndescriptions of program expressions. For example given a\nsimple grammar that includes the following production,\n<statement> ::=\nif <expression> then <statement>\n| if <expression> then <statement>\nelse <statement>\n| <other>\n3https://betanews.com/2007/03/20/john-w-backus-1924-2007/\n4https://www.masswerk.at/algol60/report.htm\none can generate statement (12) where Ei are expressions\nand Si are statements\nif E1 then if E2 then S1 else S2\n(12)\nIn turn the statement can be parsed to return the tree in ﬁgure\n2 (example from [25], p. 211)\nFIGURE 2. A parse of the statement if E1 then if E2 then S1 else S2.\nChomsky’s context free grammars were designed to handle\nthe inherent ambiguities in natural languages, and the BNF\ninherited this property, which is undesirable for program-\nming languages. One solution was proposed by Bryan Ford\nwho introduced Parsing Expression Grammars (PEGs) which\neliminated ambiguity with several new devices including\nordered choice [26]. In fact the current speciﬁcation for\nPython 3.9 uses a mixture of BNF and PEG5.\nIII. LANGUAGE WITHOUT RULES\nThe story of modern ANNs can be traced back to the Logical\nCalculus of McCulloch and Pitts [27] who showed that it\nwas possible to model the behaviour of networks of neurons\nwith a logical calculus. This inspired researchers to create\nnetworks of artiﬁcial neurons with complex computational\nproperties, which gave rise to the current generation of neu-\nral networks, the Deep Learning Networks [28], which can\nlearn complex non linear transformations implicated in image\nrecognition, language processing, and other domains. The\npromise is that such models can explain complex cognitive\nphenomena such as language understanding, without the\nneed for abstract symbol manipulating machinery.\nChurchland argued that cognitive behaviour can be re-\nduced to brain states, vis-à-vis parallel neural computations.\nCiting the work of Pellionis et al. on the Tensorial approach\nto the geometry of brain function [29], she asks us to imagine\nthat \"... arrays of neurons are interpretable as executing\nvector-to-vector transformations because that is what they\nreally are doing −the computational problems a nervous\nsystem has to solve are fundamentally geometrical problems\"\n[30](p.418). That is, transformations of real valued tensors is\njust what it is to be a cognitive agent.\nPinker and Prince [19] described this idea as eliminative\nconnectionism which are neural systems where it is impos-\nsible to ﬁnd a principled mapping between \"... the compo-\nnents of a PDP6 model and the steps or memory structures\nimplicated by a symbol-processing theory ...\". Computations\n5https://docs.python.org/3.9/reference/grammar.html\n6Parallel Distributed Processing\nVOLUME 4, 2016\n3\nVeres et al.: Language Models are not Models of Language\nare performed at a non symbolic level, and any correspond-\ning symbolic descriptions would only serve as short-hand\napproximations that could be used, for example, to make\nintuitive predictions [19]. The authors further distinguished\neliminative connectionism from implementational connec-\ntionism which is a class of systems in which the computations\ncarried out by collections of neurons are isomorphic to the\nstructures and symbol manipulations of a symbolic system.\nFor example, recurrent neural networks with long short-term\nmemory have been shown to learn very simple context free\nand context sensitive languages [31]. More speciﬁcally the\nlanguage with sentences of the form anbn is learned through\ngate units acting as counters that can keep track of the\nnumber of terminal symbols in simple sequences [31]. It\nis therefore crucial to establish which kind of system deep\nlearning models are, because eliminative neural systems are\nthe only ones that offer a theoretical alternative to traditional\ngrammar. Implementational systems would be fully compat-\nible with a rules based linguistic theory and would therefore\nbe theoretically uninteresting.\nAccording to Marcus [32], a clear criterion for recog-\nnising a genuinely eliminative connectionist system is how\nit implements compositionality. Compositionality is a char-\nacteristic of Classical symbolic architectures, in which the\nrepresentation-bearing computational units are structured ex-\npressions, and the operations performed on the expressions\ndepends on their structure [13]. For example, P follows\nfrom P&Q because of an operation which applies to the\nconstituent structure of the representation. The P and the Q\nin the formula can be of arbitrary complexity and the opera-\ntion will apply in exactly the same way. Eliminative ANNs\nare not Classical architectures because the operation that\nis responsible for transforming the network state from one\nwhich represents P&Q to one which represents P does not\noperate by virtue of the constituent structure of the formula.\nInstead, the computation involves the network settling on an\nactivation state representing P following the presentation of\nP&Q because of the values of the model parameters which\nwere adjusted to match the statistical association between\nP&Q and P in the training data. In addition, if we substituted\ncomplex novel formulas for P and Q in a Classical system\nthen the inference to P&Q would still hold, but the same\nwould not be true in a network model that did not include\nthe formulas in the training set. Finally if both P and Q were\ntrue in a Classical system then P&Q would also be true. On\nthe other hand an ANN which was trained to output P when\nP&Q was input would not output P&Q when presented with\nP and Q on the input, unless it was speciﬁcally trained to do\nso.\nThere are good reasons to believe that deep learning net-\nworks are to be understood as non compositional, eliminative\nconnectionist models. Bengio et al. [33] argue that it is the\nnon-linear transformations between the vectors in a deep\nlearning architecture that allows the network to perform its\nfunctions, setting them apart from symbolic systems. As an\nexample, \"If Tuesday and Thursday are represented by very\nsimilar vectors, they will have very similar causal effects\non other vectors of neural activity.\" [33] p.59. In a Classi-\ncal system there is no inherent similarity between the two\nsymbols \"Tuesday\" and \"Thursday\". Overlapping patterns of\ninference are only possible by asserting explicit axioms in\nthe system to establish that the concepts are members of a\ncollection that behave equivalently in certain circumstances.\nIn a deep learning network, by contrast, the inferences are\nlicensed by the similarity between the vectors themselves,\nwhich emerges through the learning process. \"Tuesday\" and\n\"Thursday\", on this account, have similar effects because\nthey have overlapping representations learned from the sen-\ntences in the training corpus and not because they share\ncommon relations in an axiomatic system.\nAdditionally, Yun et al. [34] present a proof that deep\nlearning transformers are universal approximators of contin-\nuous sequence-to-sequence functions7 with compact support,\nbut a critical assumption of the proof is contextual map-\nping of words in a sentence, such that the representation\nof individual words depends on the whole sentence. The\nproof requires, for example, that the \"I\" must be mapped\nto different vector embeddings in \"I am happy\" and \"I am\nBob\". The semantic contribution of the constituent to the\nwhole is a function of the whole, a direct contradiction to\ncompositionality.\nA. LARGE NEURAL LANGUAGE MODELS\nThe most successful neural models for NLP are very large\ndeep learning models that are trained on some version of the\nlanguage modeling task, using vast amounts of text [35]. The\nmodels capture intricate statistical generalisations available\nin the training corpus which can subsequently be exploited in\ntask speciﬁc training with much smaller data sets, a paradigm\ncalled transfer learning [35]. Large LMs also appear to\nencode structural relations in language and can be probed\nin various ways to display aspects of their linguistic compe-\ntence. For example Hewitt & Manning [36] argue that vector\nbased representations in deep learning neural networks can\ncapture syntactic tree dependencies between words without\nan explicit mechanism for constructing parse trees. Using\nstructural probes with a learned, linear transformation, they\nargue that syntax trees are embedded \"implicitly\" in the deep\nlearning model’s vector geometry. Furthermore, they suggest\nthat subject-verb agreement can be solved by using L2 dis-\ntance metrics, since the verb that has to agree in number is\ncloser to the subject than it is to any of the irrelevant attractor\nnouns following the transformation.\nGoldberg [37] reports a similar ﬁnding, that the BERT\n[38] model with no additional ﬁne tuning does remarkably\nwell on predicting the subject appropriate number marking\non the focus verb in sentences, even if there are mismatching\ndistractors between the subject and the verb. He concludes\nthat \"exploring the extent to which deep purely attention\n7An additional condition is that the function must be permutation equiv-\nariant, unless positional encodings are used.\n4\nVOLUME 4, 2016\nVeres et al.: Language Models are not Models of Language\nbased architectures such as BERT are capable of capturing\nhierarchy-sensitive and syntactic dependencies — as well as\nthe mechanisms by which this is achieved — is a fascinating\narea for future research\". In fact a sub ﬁeld of machine learn-\ning that has affectionately come to be known as Bertology\n[39], is attempting to do just that.\nIn summary, neural LMs display some of the represen-\ntations that are found in symbolic rule based theories, but\nit is not clear how they emerge or how they participate in\ncomputation. A potential explanation is suggested by Nefdt\n[40] who proposed a reﬁnement to the idea of composi-\ntionality with a distinction between Process-, State-, and\nOutput- Compositionality. A system is process compositional\nif the procedures it computes have meaningful parts, and\nthose parts are in principle knowable. The meaningful parts\nare composed procedurally to give increasingly complex\nmeaningful expressions. This is essentially the sense of com-\npositionality we have ascribed to Classical systems. State\ncompositionality on the other hand describes representations\nin the system that can be decomposed into smaller meaning-\nful units even though they were not generated by a known\ncompositional process. For example there are many analyses\nfor ways in which lexical items might be decomposed into\nmeaningful constituents (e.g. bachelor = unmarried + man),\nbut the decomposition does not exhaust the range of potential\ninferences the lexical item can enter into, nor is there a\ncompositional process to explain how the constituents can\nbe combined in learning the concept [41]. Output compo-\nsitionality takes a functional view in which for any input\ntokens t there is a function f which produces an output o that\nis state compositional. Output compositionality is a special\ncase of state compositionality because it does not imply\nthat the entire analysis needs to be state compositional, it\nis sufﬁcient that local outputs are. Neural language models\nwould on this account exhibit output compositionality, such\nthat various structured states can be observed, but these states\ndo not reveal meaningful constituents which are causally\nresponsible for the evolution of the system. The discovery\nof local compositional structure does not reveal how those\nstructures participate in the evolution of the system ([42, 43,\n33]).\nIV. NEURAL NETWORK MODELS AND SOFTWARE\nCODE\nHindle et al. [44] proposed that the majority of software that\npeople write can be described as natural programs which are\nrelatively simple, repetitive code that is used for communi-\ncation with other programmers as much as it is to provide\nmachine instructions. They successfully trained N-gram lan-\nguage models on software code to show that programming\nlanguages share many of the regularities observed in natural\nlanguages. They then used the trained model as a code\nsuggestion plugin for the Eclipse Interactive Development\nEnvironment, resulting in keystroke savings of up to 61%\nover the built in suggestion engine [44].\nSubsequent research expanded on this work by training\nsigniﬁcantly larger language models for mining source code\nrepositories and providing new complexity metrics [45], and\nusing deep learning networks for code completion [46].\nIn a popular blog post, Andrej Karpathy drew our atten-\ntion to the Unreasonable Effectiveness of Recurrent Neural\nNetworks8 with some illustrations of RNNs learning from\ncharacter level data. He showed that models could learn\nto generate natural language prose, prose which includes\nmarkdown, valid XML, LaTeX, and source code in the C\nprogramming language. The RNN generated both LaTeX and\nC code with remarkable syntactic ﬁdelity including spacing,\nmatching brackets, variable declarations and so on. Observed\nerrors were principally semantic. For example, one example\nof LaTeX code had a 13 line environment beginning with\n\\begin{proof} and ending with \\end{lemma}. That\nis, the coreference dependency is partially broken; while the\n\\begin{} block is closed as required by the syntax, the\nRNN has not correctly remembered the exact identity of\nthe block. Similarly the C code contains very few syntactic\nerrors, but contains semantic errors such as variable names\nnot being used consistently, or using undeﬁned variables.\nA. LARGE NEURAL LANGUAGE MODELS AND CODE\nSYNTHESIS\nHendrycks et al. [47] released APPS, an ambitious dataset\nand benchmark for measuring the effectiveness of machine\nlearning models in a realistic code generation framework,\ninvolving natural language problem speciﬁcations and func-\ntional test cases. The problems have three levels of difﬁculty;\nintroductory, interview, and competitive. The following ex-\nample is the natural language description in an \"interview\"\nlevel problem.\nYou are given two integers n and m. Calculate the\nnumber of pairs of arrays (a, b) such that: the length\nof both arrays is equal to m; each element of each\narray is an integer between 1 and n (inclusive); ai\n≤bi for any index i from 1 to m; array a is sorted\nin non-descending order; array b is sorted in non-\nascending order. As the result can be very large,\nyou should print it modulo 109 + 7.\nInput: The only line contains two integers n and m\n(1 ≤n ≤1000, 1 ≤m ≤10). Output: Print one\ninteger - the number of arrays a and b satisfying\nthe conditions described above modulo 109 + 7.\n—–Examples—–\nInput: 2 2, Output: 5\nInput: 10 1, Output: 55\nInput: 723 9, Output: 157557417\nThe authors gathered 10,000 coding problems with\n131,836 test cases for checking the generated solutions, and\n232,444 gold-standard solutions written by humans. They\ntested four models for their ability to generate code that\n8http://karpathy.github.io/2015/05/21/rnn-effectiveness/ The title is a play\non The Unreasonable Effectiveness of Mathematics in the Natural Sciences,\na 1960 article by the Nobel Prize winning physicist Eugene Wigner.\nVOLUME 4, 2016\n5\nVeres et al.: Language Models are not Models of Language\ncould solve the test cases; code which is both syntactically\nwell formed and semantically correct. The models they tested\nwere GPT-2 [48] (two versions, with 0.1 and 1.5 Billion\nparameters), GPT-Neo (2.7B parameters) [49] and GPT-3 [7]\n(175B parameters). The GPT-2 models received additional\npre-training with 30 GB of Python code from GitHub. GPT-\nNeo was trained on the Pile [50] which already contains code\nfrom GitHub. GPT-3 had no prior training on code. All but\nGPT-3 were then further ﬁne tuned on the APPS dataset,\nwhile GPT-3 was tested in a few-shot scenario.\nThe results showed that in general bigger models were\nbetter than the smaller models, and pre-training on code is\ncritical since the largest GPT-3 model without pre-training\n(or ﬁne tuning) was outperformed by the smallest GPT-2\nmodel.\nIn general this proved to be a difﬁcult task and the best\nmodel passed only 14.68% of test cases on the introductory\nproblems, and 6.54% for the ’competitive’ problems. On a\nstrict criterion where success was deﬁned as passing all of the\ntest cases, these ﬁgures dropped to 3.9% and 0% respectively.\nHowever, the important ﬁnding for our argument was the\nextremely low rate of syntax errors in the code. Remarkably,\nGPT-Neo generated code that failed because of syntax errors\non only 3% of its solutions on the \"introductory\" problems,\nincreasingly to approximately 5% for the \"competition\" prob-\nlems.\nListing 1 shows a machine synthesized solution for the the\nexample problem. The solution attempts to build arrays as\ndescribed, to return a result which depends on those arrays,\nand to provide the answer as modulo 109 + 7. However the\nlogic it uses to build and compare the arrays is faulty and the\nresults incorrect.\n1\n2\n3 n, m = map(int, input().split())\n4 mod = 10**9+7\n5\n6 fact = [1, 1]\n7 factinv = [1, 1]\n8 inv = [0, 1]\n9\n10 for i in range(2, n+1):\n11\nfact.append((fact[-1] * i) % mod)\n12\ninv.append((-inv[mod % i] * (mod//i)) % mod)\n13\nfactinv.append((factinv[-1] * inv[-1]) % mod)\n14\n15 def cmb(n, r, mod):\n16\nif (r < 0) or (n < r):\n17\nreturn 0\n18\nr = min(r, n - r)\n19\nreturn fact[n] * factinv[r] * factinv[n-r] %\nmod\n20\n21 print((cmb(n, m, mod) * cmb(m-1, n-1, mod)) % mod)\nListing 1. Example solution from a model tested on APPS. This solution\npasses 0 test cases but it is a legal Python program.\nAustin et al. [51] released a somewhat easier dataset, the\nMostly Basic Programming Problems (MBPP), comprising\nnatural language descriptions, concrete test cases, and gold\nstandard solutions. They argued that the poor model perfor-\nmance reported by [47] was due to the difﬁcult nature of\nthe problems. In particular the competition set was written in\na manner that purposely obfuscated the connection between\nproblem statement and solution code. The MBPP was there-\nfore designed to maintain the clarity of the natural language\ndescriptions and to be solvable by entry-level programmers.\nThe problems and solutions were provided by crowd-sourced\nworkers and subsequently inspected by the authors. The mod-\nels used for the experiments were BERT-style transformer\nmodels [4] with parameter counts ranging from 244 million\nto 137 billion, and pre-trained on a combination of web doc-\numents, dialog data, and Wikipedia. The pre-training dataset\ncontained 2.97B documents, of which 13.8M were web sites\nthat contained some code and text, although complete source\ncode ﬁles were not included.\nThe experiments showed a log-linear improvement in per-\nformance with model size, and only marginal improvement\nof ﬁne tuning over the pre-trained models. The largest model\nwith 137 billion parameters could solve approximately 60%\nof the problems with at least one generated solution, while the\nsmallest 244 million parameter model could solve fewer than\n5%. The models beneﬁted from ﬁne-tuning by approximately\na constant 10% from the 422 million to the 68 billion param-\neter model. As a result the beneﬁt of ﬁne-tuning decreased\nproportionately as model size increased.\nThe qualitative error analysis revealed that the main reason\nfor failure was not syntactic errors but problems with the code\nsemantics. Even the smallest models produced syntactically\ncorrect Python code about 80% of the time, increasing to over\n90% for the larger models. The most commonly observed\nsemantic errors were in complex problems with multiple\nconstraints or sub-problems, where the generated code only\nsolved one sub problem. For example the problem \"Write a\nfunction to ﬁnd the longest palindromic subsequence in the\ngiven string\" might result in code that found just one of the\npalindromic sequences but not the longest one. Another pos-\nsible mistake was code that solved a problem that was similar,\nbut more common than the provided one. For example the\nquery “Write a python function to ﬁnd the largest number\nthat can be formed with the given list of digits.\" might result\nin a program that calculated the largest digit in the list.\nOne difﬁculty with this ﬁnding is that it is not possible\nto identify the exact locus of the semantic errors. It could,\nfor example be the natural language component which fails\nto extract the correct speciﬁcation. In order to investigate\nthe level of semantic understanding in the code synthesis\ncomponent, the authors tested to see how well the model\ncould predict the output of the ground truth source code. If\nthe model had some understanding of the semantics of code\nthen this should manifest itself in learning code execution.\nPrevious work has found that different architectures were\nin fact capable of learning to execute code [52]. In a few-\nshot setting where only the code was presented as a prompt\nand the model had to predict the output in an input-output\ntest case, the accuracy was 16.4%, and improved to 28.8%\n6\nVOLUME 4, 2016\nVeres et al.: Language Models are not Models of Language\nwhen two input-output example pairs were also presented.\nThese results dropped to 8.6% and 11.6% respectively when\n2 test cases were presented (and the model had to answer\nboth correctly). More interestingly, when only the natural\nlanguage problem description and an example input-output\nprompt was presented the model performed more accurately,\nat 12.8%. Finally, the model performed better when it was\npresented with only the examples as prompt then it did\nwith the code itself, with no example (10.2% example only,\n8.6% code only). Together the results suggest that the model\nwas not considering the semantics of the code at all when\npredicting the results of execution. The model appeared to be\nusing previously observed patterns rather any understanding\nof the code.\nOpenAI Codex9 is an experimental API from the Mi-\ncrosoft owned company which powers GitHub Copilot10.\nCodex is a GPT-3 based model ﬁne tuned on publicly avail-\nable code, which generates Python code from docstrings [53].\nFor testing purposes the authors introduced the HumanEval\nevaluation set which includes 164 manually constructed pro-\ngramming problems consisting of a function signature, doc-\nstring, body, and several unit tests. Listing 2 shows an exam-\nple problem together with a correct and an incorrect solution.\nThe incorrect solution supports Austin et al.’s analysis that\nthe errors are not syntactic but semantic, in this case caused\nby solving only a part of the problem. In this example the\ngenerated algorithm fails to consider the boundary condition\nwhere n = 1.\n1\n2 def is_prime(n):\n3\n\"\"\"Return true if a given number is prime, and\n4\nfalse otherwise.\n5\n>>> is_prime(6) False\n6\n>>> is_prime(101) True\n7\n>>> is_prime(11) True\n8\n>>> is_prime(13441) True\n9\n>>> is_prime(61) True\n10\n>>> is_prime(4) False\n11\n>>> is_prime(1) False\n12\n\"\"\"\n13\n### COMPLETION 6 (CORRECT): ###\n14\nprime = True\n15\nif n == 1:\n16\nreturn False\n17\nfor i in range(2, n):\n18\nif n % i == 0:\n19\nprime = False\n20\nreturn prime\n21\n22\n### COMPLETION 7 (WRONG): ###\n23\nfor i in range(2, n):\n24\nif n % i == 0:\n25\nreturn False\n26\nreturn True\nListing 2. Example HumanEval docstring and a correct and incorrect\ncompletion\nThey evaluated (a) a 12B parameter vanilla GPT-3 model,\n(b) Codex, which is GPT-3 ﬁne-tuned on 159GB of code\n9https://openai.com/blog/openai-codex/\n10https://copilot.github.com/\ncollected from GitHub, and (c) Codex-S which is further\nﬁne-tuned on correctly implemented standalone functions.\nWhen looking at single solutions the vanilla GPT-3 model\nwas unable to solve any HumanEval problems, while the 12B\nparameter Codex solved 28.8%. Fine-tuning this model on\nfunction solutions produced Codex-S which solved 37.7% of\nthe problems, which was a bigger gain from ﬁne-tuning than\nobserved on the other studies. When considering multiple so-\nlutions, Codex-S was able to ﬁnd at least one correct solution\n72.31% of the time from among 100 candidate solutions, and\n46.81% of the time among 10 solutions.\nProbing error points once again provided some interesting\ninsights. The authors created \"building blocks\" which were\nsimple tasks such as “convert the string s to lowercase”,\n“remove all vowels from the string”, and “replace spaces with\ntriple spaces”. When the building blocks were systematically\nconcatenated to form increasingly complex function descrip-\ntions, model performance decreased exponentially with in-\ncreasing problem complexity. This pattern is not likely to be\nobserved with human programmers, as the task is simply to\nlink simple discrete functions together.\nIn summary, deep learning neural network models are\nbeginning to have impressive success in generating syntacti-\ncally correct code from natural language speciﬁcations. The\nvarious models demonstrate not only a knowledge of pro-\ngramming constructs like variable declaration and operation\nsignatures, but also constructs that depend on constituency\nstructure like long distance dependencies (e.g. matching\nbrackets, matching if-then imperatives). On the other hand\nthe knowledge of semantics is either absent or very limited.\nV. LARGE NEURAL LANGUAGE MODELS ARE CORPUS\nMODELS\nThe surprising success of neural LMs to synthesize computer\ncode has resulted in some puzzling claims. For example [51]\nspeculate that for models \"it is not necessary to explicitly\nencode the grammar of the underlying language −they\nlearn it from data.\" Taken literally the claim would be that\nthe neural network is implementational after all, since it\nis learning the grammar of a programming language. We\nhave already pointed out the problems with this position\nwhen applied to models of natural language, and suggest\nthat similar problems exist in the domain of programming\nlanguages.\nThe most parsimonious hypothesis is that language models\nperform natural language and programming tasks through the\nsame mechanisms, which is not that they learn the grammar\nof the language. The evidence that neural network repre-\nsentations can encode hierarchical components in language\n(e.g. [36, 43]) presents the possibility that the same repre-\nsentations are used in encoding structural relationships in\nsoftware code. Some support for this comes from [54] who\ndeveloped a novel method for probing neural LMs for their\nability to encode natural language syntax. In order to validate\ntheir method they used neural models trained on a synthetic\nlanguage for arithmetic expressions with a simple syntax,\nVOLUME 4, 2016\n7\nVeres et al.: Language Models are not Models of Language\nconcluding that the neural network encoding of the structural\npatterns generated by an artiﬁcial grammar are analogous to\ntheir encoding of natural language structures.\nIt is important to point out once again that programming\nlanguages have an exact grammar which is, by hypothesis,\nnot learned by the LM. Therefore the LM’s ability to ab-\nstract hierarchical relationships by some means other than a\ngrammar, is a powerful tool which can result in impressive\nlinguistic performance as well.\nIt is clear that large LMs are able to generate syntactically\nwell formed productions in domains where they receive large\namounts of training data, whether that be natural language\nor software code. Since the data is their only resource for\nlearning, they can be susceptible to the characteristics of the\ndata set. For example, Bender et al. [9] documented many\ndifferent selection biases which can affect which utterances\nare included in corpora. Perhaps even more problematical is\nthat the performance of language models on language tasks\nis strongly inﬂuenced by the diversity of the training corpora\n[50, 55], which shows that what is modeled is not language\nper se, but the characteristics of the training corpora. Simi-\nlarly Brown et al. [7] used a ﬁltered version of the Common\nCrawl dataset11, a collection of text based on 12 years of web\ncrawling, and found that the unﬁltered version gave poorer\nresults on language tasks than the higher quality version\nwhich was ﬁltered in part through a comparison with text in\nwritten books.\nIf the quality of a dataset can inﬂuence the performance\nof the model, then there should be clear guidelines as to\nwhat constitutes a high quality dataset. To the best of our\nknowledge the answer to this in the neural LM literature is\nrather vague. Take for example the Pile, a dataset speciﬁcally\ndesigned to be a large, diverse and high-quality resource for\nmachine learning [50]. Unfortunately the authors are not ex-\nplicit about what they regard as \"high-quality\", even though\nthey use the descriptor 33 times in their paper. Some clues\nemerge through the description of individual data sources\nsuch as Wikipedia which is \"... a source of high quality,\nclean English text ...\" (p.4), suggesting as one criterion a text\nwritten in complete English sentences with some editorial\ncontrol. Also mentioned is the Common Crawl web content\nwhere one challenge is \"... difﬁculty of cleaning and ﬁltering\nthe Common Crawl data ...\" due to low level problems such\nas presence of non text characters, segmentation problems,\ncapitalization, etc. [12]. Common Crawl data also contains\nduplicate lines which often signals highly repetitive \"boiler-\nplate text\", which can reduce performance on downstream\ntasks [56]. Grave et al. [57] tried to ﬁlter Common Crawl by\nusing Wikipedia \"because the articles are curated, the corre-\nsponding text is of high quality\". Again, editorial control is\nan important factor. Another technique for increasing quality\nby comparing against Wikipedia was used by Wenzek et al.\n[58] who measured the perplexity score of target web pages\nagainst a language model trained on Wikipedia, and took the\n11https://commoncrawl.org/the-data/\nperplexity as a measure of quality. Again the strategy is to\nassume Wikipedia is high quality and use it to ﬁlter other\ncontent.\nThe practice is problematic in our view because it is\nimportant to be speciﬁc about what should be accepted as\nappropriate input to a \"language model\", as the data is\nliterally the \"language\" in the model. In linguistic theory, as\nwe saw earlier, Chomsky proposed grammatical acceptability\njudgement as a way to decide which utterances counted as\nvalid data for linguistic enquiry. But this view is criticised\nin statistical NLP. For example Manning [59] argued against\nthe Chomskyan notion of grammaticality and proposed prob-\nabilistic syntax largely on the strength of examples which\npurportedly show the non categorical nature of grammar.\nNorvig [20] argued even more forcefully that \"... people\nhave to continually understand the uncertain, ambiguous,\nnoisy speech of others ... Chomsky for some reason wants\nto avoid this, and therefore he must declare the actual facts of\nlanguage use out of bounds and declare that true linguistics\nonly exists in the mathematical realm ... Chomsky dislikes\nstatistical models in that they tend to make linguistics an\nempirical science (a science about how people actually use\nlanguage) ...\" So for Norvig the data for the study of lnguage\nis the \"actual use\" of language which are the noisy and\nuncertain utterances of the everyday. But this view makes it\ndifﬁcult to justify eliminating any \"low quality\" utterances\nreﬂecting how people \"actually use language\" from the cor-\npus. If we are going to judge dataset quality by comparing\nit against the edited and grammatical text in \"high quality\"\nsources such as Wikipedia, then we need to be careful with\nour theoretical assumptions. The problem is that we might be\nsmuggling grammaticality in by the back door.\nFor the preceding reasons we would suggest a clariﬁcation\nin terminology, and propose a change from the theory-laden\nterm language model to the more objectively accurate term\ncorpus model. Not only does the term corpus model better\nreﬂect the contents of models, it also provides transparency in\ndiscussing issues such as model bias. One might be surprised\nif a language model is biased, or if there is different bias in\ntwo different language models, but a bias in corpus models\nand different biases in different corpus models is almost an\nexpectation. Natural language is not biased. What people say\nor write can be biased.\nVI. CONCLUSION\nWe considered the challenge that deep learning neural models\npresent to traditional generative phrase structure theories of\nnatural language, and showed the challenge to be invalid\nsince the arguments could equally and absurdly be applied\nagainst phrase structure in software code. We claim our\nargument to be more powerful and permanent than more\ntraditional attempts to prove that neural networks cannot\nperform particular tasks. This style of argument has an il-\nlustrious history in the ﬁeld, where research activity was\ndrastically reduced for decades by the publication of Min-\nsky and Pappert’s Perseptrons, which showed fundamental\n8\nVOLUME 4, 2016\nVeres et al.: Language Models are not Models of Language\ncomputational limitations of the contemporary neural models\n[60]. More sophisticated models were eventually devised to\novercome these limitations, and the previously devastating\ncriticisms vanished. Our argument is more powerful because\nit is predicated on the success of neural models. We argue\nthat achieving high performance on arbitrary NLP tasks is\nirrelevant to theories of natural language in general, and\ngenerative grammar in particular, because the same argu-\nments can apply equally to programming languages which\nare clearly the product of a generative grammar.\nAs a corollary we argued that the term \"language model\"\nis misleading. A more accurate and useful term would be\n\"corpus model\". We hope this clariﬁcation is useful for prac-\ntical work as well as scientiﬁc discovery, and look forward\nto theoretical insights that can be gained by exploring the\nsimilarities between the statistical epiphenomena created by\nnatural languages and computer software in shared corpus\nmodels.\nPinker and Prince argued that the connectionist models of\nthe time failed to deliver a \"radical restructuring of cognitive\ntheory\" [19](p.78), because they did not adequately model\nrelevant linguistic phenomena. We argue that modern neural\nmodels similarly fail, but from the opposite perspective. In\nbecoming universal mimics that can imitate the behaviour\nof clearly rule driven processes, they become uninformative\nabout the true nature of the phenomena they are \"parroting\"\n[9]. Enormous amounts of training data and advances in\ncompute power have made the modern incarnation of artiﬁ-\ncial neural networks tremendously capable in solving certain\nproblems that previously required human-like intelligence,\nbut just like their predecessors, they have failed to deliver\na revolution in our understanding of human cognition.\nReferences\n[1]\nYann LeCun, Yoshua Bengio, and Geoffrey Hin-\nton. “Deep learning”. In: Nature 521.7553 (2015),\npp. 436–444.\nISSN: 0028-0836.\nDOI: 10 . 1038 /\nnature14539.\n[2]\nTara Balakrishnan, Michael Chui, Bryce Hall, and\nNicolaus Henke. The state of AI in 2020: November\n17, 2020 Survey. 2020. URL: https://www.mckinsey.\ncom / business - functions / mckinsey - analytics / our -\ninsights / global - survey - the - state - of - ai - in - 2020.\n(accessed: 07.10.2021).\n[3]\nJacques Bughin, Eric Hazan, Sree Ramaswamy,\nMichael Chui, Tera Alla, Peter Dahlström, Nico-\nlaus Henke, and Monica Trench. Artiﬁcial Intelli-\ngence: the Next Digital Frontier? A McKinsey pa-\nper. https : / / www . mckinsey . com / ~ / media /\nmckinsey/industries/advancedelectronics/ourinsights/\nhowartiﬁcialintelligencecandeliverrealvaluetocompanies/\nmgi - artiﬁcial - intelligence - discussion - paper. ashx.\nAccessed: 07.10.2021. 2017.\n[4]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need.\n2017. arXiv: 1706.03762 [cs.CL].\n[5]\nPandu Nayak. Understanding searches better than\never before. URL: https://www.blog.google/products/\nsearch / search - language - understanding - bert/. (ac-\ncessed: 16-11-2021).\n[6]\nKevin Scott. Microsoft teams up with OpenAI to ex-\nclusively license GPT-3 language model. URL: https:\n//blogs.microsoft.com/blog/2020/09/22/microsoft-\nteams-up-with-openai-to-exclusively-license-gpt-3-\nlanguage-model/. (accessed: 16-11-2021).\n[7]\nTom\nB.\nBrown,\nBenjamin\nMann,\nNick\nRyder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Lan-\nguage Models are Few-Shot Learners. 2020. arXiv:\n2005.14165 [cs.CL].\n[8]\nEmily M Bender and Alexander Koller. “Climbing\ntowards NLU: On Meaning, Form, and Understanding\nin the Age of Data”. In: Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (2020), pp. 5185–5198. DOI: 10.18653/v1/\n2020.acl-main.463.\n[9]\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. “On the Dangers\nof Stochastic Parrots: Can Language Models Be Too\nBig?” In: Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency.\nFAccT ’21. Virtual Event, Canada: Association for\nComputing Machinery, 2021, pp. 610–623. ISBN:\n9781450383097. DOI: 10 . 1145 / 3442188 . 3445922.\nURL: https://doi.org/10.1145/3442188.3445922.\n[10]\nChristopher D. Manning, Prabhakar Raghavan, and\nHinrich Schütze. Introduction to Information Re-\ntrieval. Cambridge, UK: Cambridge University Press,\n2008. ISBN: 978-0-521-86571-5. URL: http : / / nlp .\nstanford.edu/IR- book/information- retrieval- book.\nhtml.\n[11]\nYoshua Bengio, Réjean Ducharme, Pascal Vincent,\nand Christian Jauvin. “A Neural Probabilistic Lan-\nguage Model”. In: Journal of Machine Learning Re-\nsearch 3 (2003), pp. 1137–1155.\n[12]\nChristopher Manning and Hinrich Schütze. Foun-\ndations of Statistical Natural Language Processing.\nCambridge, Massachusetts: MIT Press, 1999.\n[13]\nJerry A. Fodor and Zenon W. Pylyshyn. “Connection-\nism and cognitive architecture: A critical analysis”.\nIn: Cognition 28.1 (1988), pp. 3–71. ISSN: 0010-0277.\nDOI: https://doi.org/10.1016/0010-0277(88)90031-5.\nVOLUME 4, 2016\n9\nVeres et al.: Language Models are not Models of Language\nURL: https://www.sciencedirect.com/science/article/\npii/0010027788900315.\n[14]\nFrederick J. Newmeyer. The History of Modern Lin-\nguistics. 2014. URL: https://www.linguisticsociety.\norg/resource/history- modern- linguistics (visited on\n10/07/2021). (accessed: 07.10.2021).\n[15]\nHoward Lasnik and Terje Lohndal. “Brief overview of\nthe history of generative syntax”. In: The Cambridge\nHandbook of Generative Syntax. Ed. by MarcelEdi-\ntor den Dikken. Cambridge Handbooks in Language\nand Linguistics. Cambridge University Press, 2013,\npp. 26–60. DOI: 10.1017/CBO9780511804571.007.\n[16]\nNoam Chomsky. “Three models for the description\nof language”. In: IRE Transactions on Information\nTheory 2.3 (1956), pp. 113–124. DOI: 10.1109/TIT.\n1956.1056813.\n[17]\nNoam Chomsky. Syntactic Structures. Mouton & Co.,\n1957.\n[18]\nEwa D ˛abrowska. “What exactly is Universal Gram-\nmar, and has anyone seen it?” In: Frontiers in Psychol-\nogy 6 (2015), p. 852. ISSN: 1664-1078. DOI: 10.3389/\nfpsyg.2015.00852. URL: https://www.frontiersin.org/\narticle/10.3389/fpsyg.2015.00852.\n[19]\nSteven Pinker and Alan Prince. “On language and\nconnectionism: Analysis of a parallel distributed pro-\ncessing model of language acquisition”. In: Cognition\n28.1-2 (1988), pp. 73–193. ISSN: 0010-0277. DOI: 10.\n1016/0010-0277(88)90032-7.\n[20]\nPeter Norvig. On Chomsky and the Two Cultures of\nStatistical Learning. 2012. URL: http://norvig.com/\nchomsky . html (visited on 10/07/2021). (accessed:\n22.11.2021).\n[21]\nGeoffrey Hinton. Geoffrey Hinton and Yann LeCun,\n2018 ACM A.M. Turing Award Lecture \"The Deep\nLearning Revolution\". 2019. URL: https://youtu.be/\nVsnQf7exv5I?t=1945 (visited on 10/11/2021).\n[22]\nLeonard Bloomﬁeld. Language. New York: Holt,\nRinehart & Winston, 1933.\n[23]\nHoward Lasnik and Terje Lohndal. “Phrase structure\ngrammar”. In: The Cambridge Handbook of Genera-\ntive Syntax. Ed. by MarcelEditor den Dikken. Cam-\nbridge Handbooks in Language and Linguistics. Cam-\nbridge University Press, 2013, pp. 202–225. DOI: 10.\n1017/CBO9780511804571.007.\n[24]\nNoam Chomsky. Aspects of the Theory of Syntax.\nCambridge, Massachusetts: MIT Press, 1965.\n[25]\nAlfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey\nD. Ullman. Compilers: Principles, Techniques, and\nTools (2nd Edition). USA: Addison-Wesley Longman\nPublishing Co., Inc., 2006. ISBN: 0321486811.\n[26]\nBryan Ford. “Parsing Expression Grammars: A\nRecognition-Based Syntactic Foundation”. In: Pro-\nceedings of the 31st ACM SIGPLAN-SIGACT Sym-\nposium on Principles of Programming Languages.\nPOPL ’04. Venice, Italy: Association for Computing\nMachinery, 2004, pp. 111–122. ISBN: 158113729X.\nDOI: 10.1145/964001.964011. URL: https://doi.org/\n10.1145/964001.964011.\n[27]\nWarren S McCulloch and Walter Pitts. “A logical\ncalculus of the ideas immanent in nervous activity”.\nIn: The bulletin of mathematical biophysics 5.4 (1943),\npp. 115–133.\n[28]\nYann LeCun, Yoshua Bengio, and Geoffrey Hin-\nton. “Deep learning”. In: Nature 521.7553 (2015),\npp. 436–444.\nISSN: 0028-0836.\nDOI: 10 . 1038 /\nnature14539.\n[29]\nA. J. Pellionisz and Rodolfo R. Llinás. “Tensorial\napproach to the geometry of brain function: Cerebellar\ncoordination via a metric tensor”. In: Neuroscience 5\n(1980), pp. 1125–1136.\n[30]\nPatricia S. Churchland. Neurophilosophy: Toward a\nUniﬁed Science of the Mind-Brain. Bradford, 1986.\n[31]\nF.A. Gers and E. Schmidhuber. “LSTM recurrent net-\nworks learn simple context-free and context-sensitive\nlanguages”. In: IEEE Transactions on Neural Net-\nworks 12.6 (2001), pp. 1333–1340. DOI: 10.1109/72.\n963769.\n[32]\nGary F. Marcus. “Rethinking Eliminative Connection-\nism”. In: Cognitive Psychology 37.3 (1998), pp. 243–\n282. ISSN: 0010-0285. DOI: 10.1006/cogp.1998.0694.\n[33]\nYoshua Bengio, Yann Lecun, and Geoffrey Hinton.\n“Deep learning for AI”. In: Communications of the\nACM 64.7 (2021), pp. 58–65. ISSN: 0001-0782. DOI:\n10.1145/3448250.\n[34]\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat, Sashank J. Reddi, and Sanjiv Kumar. Are\nTransformers universal approximators of sequence-\nto-sequence functions? 2020. arXiv: 1912 . 10077\n[cs.LG].\n[35]\nRishi Bommasani et al. On the Opportunities and\nRisks of Foundation Models. 2021. arXiv: 2108.07258\n[cs.LG].\n[36]\nJohn Hewitt and Christopher D. Manning. “A Struc-\ntural Probe for Finding Syntax in Word Representa-\ntions”. In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers). Minneapolis,\nMinnesota: Association for Computational Linguis-\ntics, June 2019, pp. 4129–4138. DOI: 10.18653/v1/\nN19-1419. URL: https://aclanthology.org/N19-1419.\n[37]\nYoav Goldberg. Assessing BERT’s Syntactic Abilities.\n2019. arXiv: 1901.05287 [cs.CL].\n[38]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding.\n2019. arXiv: 1810.04805 [cs.CL].\n[39]\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\nA Primer in BERTology: What we know about how\nBERT works. 2020. arXiv: 2002.12327 [cs.CL].\n[40]\nRyan M. Nefdt. “A Puzzle concerning Compositional-\nity in Machines”. In: Minds and Machines 30.1 (2020),\n10\nVOLUME 4, 2016\nVeres et al.: Language Models are not Models of Language\npp. 47–75. ISSN: 0924-6495. DOI: 10.1007/s11023-\n020-09519-6.\n[41]\nJerry A. Fodor. “The Present Status of the Innate-\nness Controversy”. In: RePresentations: Philosophi-\ncal Essays on the Foundations of Cognitive Science.\nEd. by Jerry Fodor. Cambridge, MA: MIT Press, 1981,\npp. 257–316.\n[42]\nYoshua Bengio. “Learning Deep Architectures for AI.\nFoundations Trends Machine Learning, vol. 2 (1)”. In:\n(2009).\n[43]\nIan Tenney, Dipanjan Das, and Ellie Pavlick. “BERT\nRediscovers the Classical NLP Pipeline”. In: arXiv\n(2019). eprint: 1905.05950.\n[44]\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhen-\ndong Su, and Premkumar Devanbu. “On the Natu-\nralness of Software”. In: Commun. ACM 59.5 (Apr.\n2016), pp. 122–131. ISSN: 0001-0782. DOI: 10.1145/\n2902362. URL: https://doi.org/10.1145/2902362.\n[45]\nMiltiadis Allamanis and Charles Sutton. “Mining\nSource Code Repositories at Massive Scale Using\nLanguage Modeling”. In: Proceedings of the 10th\nWorking Conference on Mining Software Repositories.\nMSR ’13. San Francisco, CA, USA: IEEE Press, 2013,\npp. 207–216. ISBN: 9781467329361.\n[46]\nMartin White, Christopher Vendome, Mario Linares-\nVásquez, and Denys Poshyvanyk. “Toward Deep\nLearning Software Repositories”. In: Proceedings of\nthe 12th Working Conference on Mining Software\nRepositories. MSR ’15. Florence, Italy: IEEE Press,\n2015, pp. 334–345. ISBN: 9780769555942.\n[47]\nDan Hendrycks, Steven Basart, Saurav Kadavath,\nMantas Mazeika, Akul Arora, Ethan Guo, Collin\nBurns, Samir Puranik, Horace He, Dawn Song,\nand Jacob Steinhardt. “Measuring Coding Chal-\nlenge Competence With APPS”. In: arXiv preprint\narXiv:2105.09938 (2021).\n[48]\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. “Language Models\nare Unsupervised Multitask Learners”. In: 2019.\n[49]\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. GPT-Neo: Large Scale Autoregres-\nsive Language Modeling with Mesh-Tensorﬂow. Ver-\nsion 1.0. 2021. URL: http://github.com/eleutherai/gpt-\nneo.\n[50]\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, Shawn Presser,\nand Connor Leahy. The Pile: An 800GB Dataset of\nDiverse Text for Language Modeling. 2020. arXiv:\n2101.00027 [cs.CL].\n[51]\nJacob\nAustin,\nAugustus\nOdena,\nMaxwell\nNye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and\nCharles Sutton. “Program Synthesis with Large Lan-\nguage Models”. In: arXiv (2021). eprint: 2108.07732.\n[52]\nDavid Bieber, Charles Sutton, Hugo Larochelle,\nand Daniel Tarlow. “Learning to Execute Programs\nwith Instruction Pointer Attention Graph Neural Net-\nworks”. In: Advances in Neural Information Process-\ning Systems. Ed. by H. Larochelle, M. Ranzato, R.\nHadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran\nAssociates, Inc., 2020, pp. 8626–8637. URL: https :\n/ / proceedings . neurips . cc / paper / 2020 / ﬁle /\n62326dc7c4f7b849d6f013ba46489d6c-Paper.pdf.\n[53]\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela\nMishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet,\nFelipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-Voss, William Hebgen Guss, Alex Nichol,\nAlex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,\nSuchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Rad-\nford, Matthew Knight, Miles Brundage, Mira Murati,\nKatie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Woj-\nciech Zaremba. “Evaluating Large Language Models\nTrained on Code”. In: (2021). arXiv: 2107 . 03374\n[cs.LG].\n[54]\nGrzegorz Chrupała and Afra Alishahi. “Correlating\nNeural and Symbolic Representations of Language”.\nIn: Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics (2019),\npp. 2952–2962. DOI: 10.18653/v1/p19-1283.\n[55]\nTuring-NLG language model. Turing-NLG: A 17-\nbillion-parameter language model by Microsoft. 2020.\nURL: https://www.microsoft.com/en- us/research/\nblog/turing- nlg- a- 17- billion- parameter- language-\nmodel - by - microsoft/ (visited on 02/13/2020). (ac-\ncessed: 22.11.2021).\n[56]\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. “Deduplicating Training Data\nMakes Language Models Better”. In: arXiv (2021).\neprint: 2107.06499.\n[57]\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. “Learning Word\nVectors for 157 Languages”. In: arXiv (2018). eprint:\n1802.06893.\n[58]\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis\nConneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. “CCNet: Extracting\nHigh Quality Monolingual Datasets from Web Crawl\nData”. In: arXiv (2019). eprint: 1911.00359.\nVOLUME 4, 2016\n11\nVeres et al.: Language Models are not Models of Language\n[59]\nChristopher Manning. “Probabilistic Syntax”. In:\nProbabilistic Linguistics. Ed. by Rens Bod, Jen-\nnifer Hay, and Stefanie Jannedy. Cambridge, Mas-\nsachusetts: MIT Press, 2003. Chap. 8, pp. 289–342.\n[60]\nMarvin Minsky and Seymour Papert. Perceptrons; an\nintroduction to computational geometry. Cambridge,\nMassachusetts: MIT Press, 1969.\nCSABA VERES was born in Budapest in 1964.\nCsaba received the Ph.D. degree in cognitive sci-\nence from the University of Arizona, Tucson. His\nthesis was in the ﬁeld of Psycholinguistics, where\nhe studied the role of meaning on sentence parsing\nand representation. He subsequently began work\nas a computer and information scientist at Mel-\nbourne University in Australia.\nHe is currently Full Professor at the Department\nof Information Science and Media Studies at the\nUniversity of Bergen, Norway. His areas of expertise include NLP, machine\nlearning, and semantic web technologies. He has experience as an Academic\nand Practitioner. He founded a Norwegian company called LexiTags, and\nconsulted as Head of AI with the London based educational technology\nstartup, Flooved. He has published a wide assortment of original research\narticles, and has had popular linked data apps on the Apple store, called\nMapXplore and AuotoMind. He has also held positions as a Research Sci-\nentist at the Australian Defence Science and Technology Organisation, and\nas a Senior Lecturer in the Department of Information Systems, Melbourne\nUniversity.\n12\nVOLUME 4, 2016\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-12-13",
  "updated": "2022-06-15"
}