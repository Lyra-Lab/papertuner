{
  "id": "http://arxiv.org/abs/2112.07082v2",
  "title": "DeepDiffusion: Unsupervised Learning of Retrieval-adapted Representations via Diffusion-based Ranking on Latent Feature Manifold",
  "authors": [
    "Takahiko Furuya",
    "Ryutarou Ohbuchi"
  ],
  "abstract": "Unsupervised learning of feature representations is a challenging yet\nimportant problem for analyzing a large collection of multimedia data that do\nnot have semantic labels. Recently proposed neural network-based unsupervised\nlearning approaches have succeeded in obtaining features appropriate for\nclassification of multimedia data. However, unsupervised learning of feature\nrepresentations adapted to content-based matching, comparison, or retrieval of\nmultimedia data has not been explored well. To obtain such retrieval-adapted\nfeatures, we introduce the idea of combining diffusion distance on a feature\nmanifold with neural network-based unsupervised feature learning. This idea is\nrealized as a novel algorithm called DeepDiffusion (DD). DD simultaneously\noptimizes two components, a feature embedding by a deep neural network and a\ndistance metric that leverages diffusion on a latent feature manifold,\ntogether. DD relies on its loss function but not encoder architecture. It can\nthus be applied to diverse multimedia data types with their respective encoder\narchitectures. Experimental evaluation using 3D shapes and 2D images\ndemonstrates versatility as well as high accuracy of the DD algorithm. Code is\navailable at https://github.com/takahikof/DeepDiffusion",
  "text": " \n \n1 \nAccepted to the IEEE Access journal. \nDate of publication November 1, 2022,  Digital Object Identifier: 10.1109/access.2022.3218909,  Pages 116287-116301. \nDeepDiffusion: Unsupervised Learning of  \nRetrieval-adapted Representations via  \nDiffusion-based Ranking on Latent Feature \nManifold  \nTakahiko Furuya1 and Ryutarou Ohbuchi1 \n1Department of Computer Science and Engineering, University of Yamanashi, 4-3-11 Takeda, Kofu-shi, Yamanashi-ken, 400-8511, Japan  \nCorresponding author: Takahiko Furuya (e-mail: takahikof@ yamanashi.ac.jp). \nThis work was supported by JSPS KAKENHI (Grant No. 21K17763). \nABSTRACT Unsupervised learning of feature representations is a challenging yet important problem for \nanalyzing a large collection of multimedia data that do not have semantic labels. Recently proposed neural \nnetwork-based unsupervised learning approaches have succeeded in obtaining features appropriate for \nclassification of multimedia data. However, unsupervised learning of feature representations adapted to \ncontent-based matching, comparison, or retrieval of multimedia data has not been explored well. To obtain \nsuch retrieval-adapted features, we introduce the idea of combining diffusion distance on a feature manifold \nwith neural network-based unsupervised feature learning. This idea is realized as a novel algorithm called \nDeepDiffusion (DD). DD simultaneously optimizes two components, a feature embedding by a deep neural \nnetwork and a distance metric that leverages diffusion on a latent feature manifold, together. DD relies on \nits loss function but not encoder architecture. It can thus be applied to diverse multimedia data types with \ntheir respective encoder architectures. Experimental evaluation using 3D shapes and 2D images \ndemonstrates versatility as well as high accuracy of the DD algorithm. Code is available at \nhttps://github.com/takahikof/DeepDiffusion \nINDEX TERMS unsupervised representation learning, multimedia information retrieval, deep learning \nI. INTRODUCTION \nTechnology for content-based multimedia information \nretrieval is essential to manage a variety of data, such as \ntext documents, two-dimensional (2D) images, and three-\ndimensional (3D) shapes. Over the past decade, accuracy of \nmultimedia information retrieval has improved drastically \ndue in large part to the development of distance metric \nlearning techniques, especially those using Deep Neural \nNetworks (DNNs). Previous studies (e.g., [1], [2]) trained a \nDNN by using a large amount of labeled data to acquire a \ndistance metric useful for matching, comparing, or ranking \nof the data. Feature representations extracted from the \nsupervisedly trained DNNs yield retrieval accuracy \nsignificantly higher than traditional handcrafted features. \nHowever, collecting sufficient number of labeled data is \noften impractical since annotation by humans is quite \nlaborious. Insufficiency in number of labeled data hampers \nthe supervised learning of feature representations and their \ndistance metric. \nRecently, unsupervised learning of feature representations, \nwhich tries to obtain expressive features from a collection of \nunlabeled data, has received much attention. In general, \nunsupervised learning is more challenging than supervised \nlearning since direct supervision by labels is not available. To \ntrain feature extraction, or encoder, DNNs without using \nlabels, “pretext” tasks are usually employed. In the field of \n2D image analysis, such pretext tasks as self-reconstruction \n[3], context prediction [4], pseudo label classification [5], and \nfeature contrast [6] have been proposed. Training using these \npretext tasks achieved a certain level of success in learning \nfeature representations in an unsupervised manner. \nWe argue that these previous pretext tasks for \nunsupervised representation learning have two shortcomings. \n(i) First, features learned by most of the previous pretext \ntasks for classification [4]–[16] are not optimal for retrieval. \n \n \n2 \nIdentification \nnumber ID of x \nFeature embedding \nby encoder DNN \nLatent feature space \nLatent Manifold  \nRanking (LMR) loss \nUnsupervised feature learning by DeepDiffusion (DD) algorithm \nFIGURE 1. The proposed DeepDiffusion (DD) algorithm learns retrieval-adapted feature representations via ranking on a latent feature manifold. By \nminimizing Latent Manifold Ranking loss, the encoder DNN and the latent feature manifold (i.e., intrinsic features) are optimized for comparison of \ndata samples. DD is applicable to a wide range of multimedia data including 3D shape and 2D image. \nExtrinsic feature f \nSmoothing  \nterm Lsmooth \nFitting  \nterm Lfit \nAugmented training \nsample x̂  \nTraining sample x \n(e.g., 3D shape) \nData \naugmentation \nθ \nIntrinsic feature m \n7,843 \nThat is, DNN training using these pretext tasks attempts to \nform the latent feature space where similar data samples are \nembedded close to each other. However, none of the existing \nmethods [4]–[16] impose any constraints on the structure of \nfeature distribution in the learned latent feature space. \nFeatures learned without such constraints are expected to be \nnonlinearly distributed, possibly with gaps, in the latent \nfeature space. Accurate classification can be achieved if \ndecision boundaries among such nonlinearly distributed set \nof features are properly established by a subsequent \nsupervised nonlinear classifier. On the other hand, however, \nretrieval accuracy suffers if the latent feature space is highly \nnonlinear, for example with bunching and/or gaps in the \nlatent feature distribution. Feature comparison for retrieval is \ntypically done by using a fixed distance metric (e.g., the \nEuclidean distance). The fixed distance metric has difficulty \nin finding all the data samples relevant to a query in the latent \nfeature space having nonlinear distribution. To achieve \naccurate retrieval, a latent feature space having a continuous \nand smooth distance metric adapted to the nonlinear \ndistribution of the input data is essential. \n(ii) Second, some of the previous pretext tasks (e.g., [3], \n[4]) lack versatility, as they can only be applied to a specific \ndata type. For example, the context prediction task [4] works \nonly for 2D images since it heavily depends on the structure \nof 2D image. Or, the self-reconstruction [3] requires a loss \nfunction in addition to a pair of an encoder DNN and a \ndecoder DNN, all of which must be exclusively designed for \neach data type. \nOur goal in this paper is to develop a datatype agnostic \nunsupervised learning framework that learns retrieval-\nadapted feature representation. To this end, we introduce \ndiffusion distance on latent feature manifold.  \nWe first motivate the use of diffusion distance on latent \nfeature manifold. Manifold hypothesis [61] assumes that \nhigh-dimensional data samples or their feature descriptors \ntend to lie in the vicinity of a nonlinear, low-dimensional \nmanifold. Before the DNN era, manifold learning techniques \n[21] have empirically shown their effectiveness in tasks such \nas dimensionality reduction, clustering, and retrieval of \ndiverse data types. We believe that the manifold hypothesis is \nalso applicable to latent feature spaces formed by randomly \ninitialized DNNs designed for encoding various data types \nsuch as 2D image and 3D shape. We thus hypothesize that \nincorporating the idea of manifold learning into the \nunsupervised deep learning could achieve our goal, i.e., \nlearning retrieval-adapted features of diverse data types. \nFollowing our hypothesis, we attempt to optimize an encoder \nDNN and its latent feature space by using a pretext task that \nrespects the structure of nonlinear manifold lying in the latent \nfeature space. Our pretext task leverages diffusion distance \non feature manifold [45] as a distance metric. Diffusion \ndistance on a feature manifold is a powerful metric for \ninformation retrieval [45]. Given a set of discrete data points, \nor nodes, diffusion distance is defined as an average length of \nmultiple paths between two nodes on the feature manifold \ngraph. Averaging distances through multiple paths produces \na smoother distance metric among a set of nodes. As \ndescribed in [45], diffusion distances are typically computed \nby iteratively propagating similarities on the manifold graph. \nMany studies on multi-media information retrieval (e.g., [17], \n[18], [19], [45]) demonstrated that the data-adaptive diffusion \ndistance can retrieve semantically similar data more \naccurately than a non-data-adaptive, or fixed, distance such \nas Euclidean distance. These successes motivate us to use the \ndiffusion distance on a feature manifold, which tends to be \nsmooth, for learning retrieval-adapted features. Also, to \nachieve applicability to a wide range of multimedia data, our \npretext task is formulated as a loss function so that it works \nin conjunction with various data types, datasets, and encoder \nDNN architectures.  \nOur motivation described above is realized as a novel \nalgorithm called DeepDiffusion (DD). Fig. 1 illustrates the \nlearning framework of the DD algorithm. The crux of the DD \nalgorithm is its loss function, which is designed to learn \nretrieval-adapted features. Our loss function, named Latent \nManifold Ranking (LMR) loss, is built by reformulating the \nloss function of the Manifold Ranking algorithm [17], [19]. \n \n \n3 \nAs shown in Fig. 1, the LMR loss is computed based on \ndiffusion-based ranking in the latent feature space. The LMR \nloss is used to jointly optimize two sets of parameters, that are, \nconnection weights within the encoder DNN and the \nmanifold formed by latent features of all training samples. \nThis optimization aims to transform the initial latent feature \nspace so that a submanifold consisting of mutually similar \nfeatures to contract while mutually distinct submanifolds to \nmove farther away from each other. Diffusion distance-based \nranking is achieved by comparing the latent features using a \nfixed (e.g., Euclidean) distance in the transformed latent \nfeature space. After a training, feature vectors extracted by \nthe trained encoder DNN are used for retrieval. In addition to \nthis simple feature extraction, we also propose another \nfeature extraction method that exploits both the trained \nencoder and the optimized latent feature manifold to boost \nretrieval accuracy. \nStrengths of the DD algorithm are threefold. First, it learns \nfeatures appropriate for content-based matching, comparison, \nand ranking of multimedia data. Second, our method learns \nsuch retrieval-adapted features in a fully unsupervised \nmanner. The DD algorithm requires no semantic labels for its \ntraining. Third, our algorithm is widely applicable to any \ntype of multimedia data as long as encoder DNNs that \nprocess the multimedia data type are available. This is \nbecause the DD relies only on its loss function.  \nWe evaluate the effectiveness of the DD algorithm by \nusing two types of multimedia data, that are, 3D shape \nrepresented by point set and 2D image represented by pixels. \nThe experiments demonstrate that feature representations \nlearned by the DD algorithm yield retrieval accuracy \nsignificantly higher than those learned by the existing \nmethods for unsupervised feature learning.  \nContributions of this paper can be summarized as follows. \n Proposing a novel unsupervised representation \nlearning algorithm named DeepDiffusion (DD). \nTraining framework of the DD algorithm is suitable \nto learn retrieval-adapted features and is applicable \nto diverse multimedia data types. \n Empirically evaluating accuracy and versatility of \nthe DD algorithm by using scenarios of 3D shape \nretrieval and 2D image retrieval. \nThe rest of the paper is organized as follows. Related work \nis reviewed in Section II and our proposed algorithm is \ndescribed in Section III. Section IV reports experimental \nresults. Finally, conclusion and future work are discussed in \nSection V.  \n \n \n \n \n \n \nII. RELATED WORK \nA. DNN-BASED DISTANCE METRIC LEARNING \nSupervised deep metric learning is a well-studied approach \nto improving accuracy of information retrieval [48]. Majority \nof the approach creates supervisory signals by forming pairs \n[1], triplets [2], or higher order tuples [54], of labeled training \nsamples. A latent feature space is optimized so that intra-\nclass distances become smaller than inter-class distances. \nSome studies [55], [56] exploit user-generated tags as weak \nsupervisory signal for deep metric learning. Meanwhile, a \ngroup of studies [49], [50], [57] try to directly optimize \nretrieval accuracy indices such as Average Precision and \nRecall via DNN training. They adopt ranked lists as \nsupervisory signals and train the DNN with a dedicated \nobjective function to maximize the retrieval accuracy. \nAlthough these approaches ([48]–[50], [57]) are effective in \nimproving retrieval accuracy, they require a large number of \nlabeled data for training.  \nUnsupervised fine-tuning of a DNN is one of the \nalternatives for deep metric learning. [51], [52], [58], [59] \nmine pseudo supervisory signals from latent features of \nunlabeled data to fine-tune the DNN. Although the mining \nprocedures are unsupervised, they heavily depend on latent \nfeatures extracted by the DNN pre-trained with labeled data. \nIn contrast, our approach is purely unsupervised. That is, a \nDNN is trained from scratch by using only unlabeled data to \nobtain latent features suitable for retrieval. \nB. DNN-BASED UNSUPERVISED REPRESENTATION \nLEARNING \nUnsupervised learning can potentially leverage a large \namount of unlabeled data as training samples. To effectively \ntrain DNNs without relying on labels, pretext tasks are \nusually employed. Various pretext tasks for unsupervised \nrepresentation learning have been proposed mainly in the \nfield of 2D image analysis.  \nAutoencoder (AE) [3] learns features via a self-\nreconstruction task. An AE is formed by paring an encoder \nDNN with a decoder DNN. The encoder embeds an input \ndata into the latent feature space while the decoder \nreconstructs the input from the embedded feature. Generative \nAdversarial Network (GAN) [20] learns a sample distribution \nof training dataset through adversarial training of a generator \nDNN and a discriminator DNN. Although GAN was \noriginally proposed to generate realistic 2D images, it has \nalso proven useful for unsupervised visual feature learning \n[7]. The other pretext tasks for unsupervised visual feature \nlearning include, for example, predicting different channels \n(e.g., [8]), predicting spatial context (e.g., [4]), and predicting \ngeometric transformation (e.g., [9]) of 2D images. Though \nthese pretext tasks perform reasonably well in learning \nfeature representation, they lack versatility. Designing the \npretext task requires deep understanding of the data type. In \n \n \n4 \naddition, designing a DNN architecture and/or a loss function \nfor the data type also requires knowledge of the data type. \nCompared to the abovementioned pretext tasks, pseudo-\nlabel classification (e.g., [5]) and feature contrast (e.g., [6]) \nare more versatile and flexible. These tasks do not require the \ndecoder DNN, and the encoder DNN is trained with a loss \nfunction that can be used independently of data type. The \npseudo-label classification approach automatically assigns a \npseudo category label to each unlabeled training sample. \nInstance Discrimination [5], [10], [60] assigns a unique \npseudo-label per training sample. The pseudo-labels of \nDeepCluster [11] are generated by using k-means clustering \nof a set of latent features extracted by an encoder DNN. \nSwapping Assignments between multiple Views (SwAV) \n[12] and Local Aggregation [13] incorporate a clustering \nprocedure as a part of a DNN to generate pseudo-labels \nadapted to the training data. On the other hand, feature \ncontrast, also known as contrastive learning, trains the \nencoder DNN by comparing latent features. The feature \ncontrast approach tries to form a latent feature space where \nfeatures of positive sample pairs are embedded closer while \nfeatures of negative sample pairs are embedded further apart \nfrom each other. Majority of the feature contrast methods, \ne.g., [6], [14] and [15] creates a positive pair by coupling two \ndifferent “views” of a training sample. These two views are \ngenerated by applying data augmentation with different \naugmentation parameters to the training sample. A negative \npair is formed between the pair of views derived from \nmutually different training samples. Features learned by \npseudo-label \nclassification \nor \nfeature \ncontrast \nwere \nexperimentally proven to be effective for classification and \nsegmentation of multimedia data, especially 2D images. \nNevertheless, the existing pretext tasks mentioned in this \nsection do not necessarily learn features that are suitable for \ninformation retrieval. Training by these pretext tasks do not \npay much attention to the structure of the initial latent feature \nmanifold formed by a randomly initialized encoder DNN. In \naddition, these pretext tasks do not impose any constraints on \nthe learned latent feature distribution. Therefore, the training \nwould strongly distort the initial latent feature manifold and \nconverges to an unconstrained latent feature space having \nnonlinear distribution. Such nonlinear latent feature \ndistribution, possibly with folds, bunches and gaps, is \nunsuitable for information retrieval ranked by using a fixed \ndistance metric. For example, pseudo-labels of DeepCluster \n[11] ignore the nonlinearity of latent feature manifold since \nthey are created by the k-means clustering. If one of the \nestimated centroids lies among different submanifolds in the \nlatent feature space, DeepCluster overly deforms these \nsubmanifolds so that their latent features agglomerate as one \ncluster. Also, positive/negative pairs used by the feature \ncontrast approach [6], [14], [15] may distort the latent feature \nmanifold since negative samples are chosen randomly from a \ntraining data set. If both positive and negative samples lie on \nthe same submanifold, it would be torn apart to converge to a \nlatent feature space that is not optimal for retrieval. Our DD, \non the other hand, learns retrieval-adapted features based on \nthe criterion entirely different from the existing methods, that \nis, diffusion distance on the latent feature manifold. \nC. GRAPH-BASED MANIFOLD LEARNING \nManifold learning is a powerful technique to discover a \nnonlinear low-dimensional subspace inherent in a high-\ndimensional feature space. Various manifold learning \nalgorithms were proposed primarily before the DNN era [21]. \nTo unveil an intrinsic geometry of the high-dimensional \nspace, manifold learning algorithms analyze a manifold \ngraph. In this graph, each node represents a sample (i.e., a \nraw datum or its feature vector) and a set of such nodes are \nconnected by edges whose weights are calculated based on \nproximity of the samples. The representative manifold \nlearning algorithms (e.g., [22], [23]) use eigenanalysis of the \nmanifold graph to nonlinearly project the samples to the low-\ndimensional subspace. Manifold Ranking (MR) [17][19] \nlearns a distance metric among the samples by using \nsimilarity diffusion on the manifold graph. We later review \nthe MR algorithm.  \nThese classical manifold learning algorithms described \nabove have been widely used for compression, clustering, \ncomparison, and visualization, of high-dimensional data. \nHowever, accuracy of the distance metric learned by these \nclassical algorithms highly depends on quality of data \nsamples associated with nodes. Analyzing the manifold \ngraph formed by raw data or their handcrafted features often \nproduces a sub-optimal distance metric. \nSome studies attempt to combine the idea of manifold \nlearning with deep learning [24]–[27] for “manifold-aware” \nDNNs. Each of these studies proposes either a loss function \nor a DNN architecture that considers the manifold structure \nof input features. [24]–[26] propose loss functions that learn a \nlatent feature space whose distance metric is constrained by \nproximity, in an input feature space, of training samples. [27] \nproposes to use a graph convolution layer as a building block \nof a DNN to diffuse similarity over a manifold formed by \ninput features. \nThese manifold-aware deep learning methods [24]–[27] \nsucceeded in finding feature embedding better than the \nclassical manifold learning methods [21]. However, most of \nthe existing manifold-aware DNNs do not optimize feature \nextraction since they process handcrafted features extracted \nprior to training the embedding DNN. In contrast, we take a \nholistic approach: our proposed DD optimizes both \nextraction of features from raw input data and embedding of \nthe extracted features to the latent feature space. \n1) Manifold Ranking algorithm \nHere, we briefly review the Manifold Ranking algorithm [17], \n[19], whose loss function serves as the basis of our LMR loss. \nMR learns a distance metric for retrieval of diverse \nmultimedia data (e.g., [18]). MR works either in supervised, \n \n \n5 \nsemi-supervised, or unsupervised mode. We describe the \nunsupervised MR below.  \nGiven a set of N unlabeled feature vectors, MR first \nconstructs a manifold graph W by connecting neighboring \nsamples in the feature space. W ∈ ℝN×N is an adjacency \nmatrix whose element wij represents a similarity between two \nsamples i and j. Usually, wij is calculated from the Euclidean \ndistance between the input feature vectors. The objective of \nMR is to find a set of ranking score vectors {ri}i=1, …, N \naccording to the diffusion distances on the manifold. The \nranking vector ri contains similarity values between query i \nthat serves as the diffusion source and all the other features \nlying on the manifold. The loss function associated with \n{ri}i=1, …, N is formulated in (1). For simplicity, we omit some \ncoefficients contained in the original loss function [19].  \n\n\n1\n2\n2\n,...,\n1\n1\n1\nargmin\n \nN\nN\nN\nN\ni\ni\nij\ni\nj\ni\ni\nj\nw\n\n\n\n\n\n\n\n\n\nr\nr\nr\ny\nr\nr\n \n(1) \nIn (1), the first term is called fitting constraint, which fits \nthe ranking score vector ri of the query i to the one-hot vector \nyi representing the diffusion source. The second term is \ncalled smoothing constraint, which makes the query and its \nneighbors in the input feature space to have similar ranking \nscores. Eq. 1 can be solved either by an iterative form or a \nclosed form solutions [17] [19]. Both solutions, however, \nrequire high spatial and temporal costs to analyze N×N \nadjacency matrix when the number of samples N is large \n(e.g., N > 10k). Our proposed loss function is more efficient \nand is compatible with DNN training on a larger dataset (e.g., \nN~100k).  \nIII. PROPOSED METHOD \nA. OVERVIEW OF DEEPDIFFUSION ALGORITHM \nFig. 1 illustrates our DeepDiffusion (DD) algorithm. The \nconcept of the DD algorithm is to incorporate the idea of the \nMR algorithm, which leverages smooth diffusion distance on \nfeature manifold for retrieval of diverse data types, into the \nproblem of unsupervised deep representation learning. DD \ndiffers significantly from the existing pretext tasks for \nunsupervised deep representation learning in that DD \nrespects the structure of a latent feature manifold formed by a \nrandomly initialized encoder DNN. During a training by DD, \nboth feature extraction and embedding by the encoder DNN \nare optimized so that the features are salient and the metric \nspace is continuous and smooth. In the optimized latent \nfeature space, the fixed distance (e.g., Euclidean distance) \namong the latent features approximates the diffusion distance \non the latent feature manifold formed by the initial latent \nfeatures.  \nLearning such retrieval-adapted latent feature space is \nachieved primarily by a novel loss function called Latent \nManifold Ranking (LMR) loss. The LMR loss is built upon \nthe loss function of the MR algorithm shown in (1). As \ndepicted in Fig. 1, the LMR loss is computed by using two \nkinds of features, namely, extrinsic features and intrinsic \nfeatures. An extrinsic feature is a feature of the input sample \nembedded by using the encoder DNN. An intrinsic feature is \na feature lying on the manifold formed by latent features of \nall the training samples. The intrinsic features are not \ncomputed by the encoder, but updated via training.  \nOne can use the MR loss shown in (1) as-is as a loss \nfunction for DNN training. However, directly applying the \nMR loss to DNN training confronts two issues. First, \nconstructing a full manifold graph having N2 connections \nfrom N training samples suffers from high temporal and \nspatial computation costs. Our LMR loss mitigates this issue \nby constructing an ad-hoc, small manifold graph constructed \nfrom a subset of data samples for each training iteration. That \nis, at every training iteration, a bipartite manifold graph is \nformed between B (B ≪ N) extrinsic features extracted from \nan input minibatch and N intrinsic features of all training \nsamples. The use of the small manifold graph significantly \nreduces both temporal and spatial complexities compared to \nthe full manifold graph. Second, the (squared) Euclidean \ndistance used in the MR loss is not necessarily the best \ncriterion for deep learning of distance metric. Recent studies \non supervised deep metric learning (e.g., [28]) have \ndemonstrated that comparing latent features by using \ndivergence leads to a latent feature space better than that \nlearned by using the Euclidean distance (e.g., [1], [2], [54]). \nFollowing the success of supervised deep metric learning, we \nemploy divergence between probability distributions, instead \nof the Euclidean distance used in (1), to compare the ranking \nscore vectors. Detail of the LMR loss is described in Section \nIII.B. \nAfter training, the optimized encoder DNN and the \noptimized latent feature manifold (i.e., intrinsic features) are \nused to extract features from novel data unseen during the \ntraining. We propose two feature extraction methods. The \nfirst method simply uses the trained encoder only. The \nsecond method exploits both the encoder and the latent \nfeature manifold for feature extraction. Details of the feature \nextraction are described in Section III.C. \nSince the crux of the DD algorithm is the LMR loss \nfunction, DD can potentially work in conjunction with any \ndata type and encoder DNN architecture. As mentioned in \nSection I, we assume that the manifold hypothesis is valid in \nthe latent feature space of an encoder DNN. If the initial \nlatent features lie on a nonlinear manifold, DD is expected to \nlearn retrieval-adapted latent features regardless of input data \ntype. In the experiments, we demonstrate the versatility of \nDD by using multiple data types, datasets, and encoder DNN \narchitectures. \nB. LATENT MANIFOLD RANKING LOSS \nThis subsection elaborates on the LMR loss, which is the \ncore of the DD algorithm. We first define the symbols \nnecessary to formulate our loss. Let {(xn, IDn)}n=1, …, N be a \ntraining dataset containing N unlabeled samples x. Each \n \n \n6 \ntraining sample x is paired with its unique identification \nnumber ID, which is used to specify a diffusion source. ID \nfor the n-th training sample is n. In other words, the value of \nID differs per training sample and does not change \nthroughout training. The DD algorithm at the training stage \ntakes as its input a mini-batch containing B training samples, \ni.e., {(xb, IDb)}b=1, …, B. To diversify the training samples, we \napply data augmentation to each sample x in the mini-batch \nto obtain x̂ . The augmented sample x̂  is then embedded to the \nP-dimensional (e.g., P=256) latent feature space by using the \nencoder DNN parameterized by θ. The embedded feature, or \nextrinsic feature, is denoted by f, whose L2 norm is \nnormalized to 1. Let M ∈ ℝN×P be a matrix representing the \nlatent feature manifold formed by all the training samples. M \nis constructed by stacking a set of N row vectors, i.e., \n{mn}n=1, …, N. θ and M are optimization targets of the DD \nalgorithm.  \nOur objective function using the LMR loss is defined as \n(2), (3), and (4). Similar to the original MR loss (1), our \nLMR loss (2) comprises the fitting term Lfit and the \nsmoothing term Lsmooth. The coefficient λ balances these two \nterms. \nfit\nsmooth\n, \nargmin L\nL\n\n\n\nM\n \n(2) \n\n\nfit\n1\n, \nB\nb\nb\nb\nL\nH\n\n\nr\ny\n \n(3) \n\n\nsmooth\n1\n1\n||\nB\nN\nbn\nb\nn\nb\nn\nL\nw D\n\n\n\nr\nr\n \n(4) \nFitting term: Lfit constrains the ranking vector rb of the \ntraining sample in the mini-batch to be close to the diffusion \nsource vector yb. rb is computed as rb = softmax(fb ∙ MT). The \nN-dimensional vector rb holds probabilistic similarities \namong the feature fb and all the intrinsic features contained in \nM. yb is an N-dimensional one-hot vector whose IDb-th \nelement is 1 while the other elements are 0. We use cross-\nentropy H(rb, yb) to compare the ranking score vector and the \ndiffusion source vector.  \nThe effect of the fitting term is as follows. By minimizing \nLfit, all the extrinsic features are embedded farther from each \nother since their ranking vectors are pulled toward different \ndiffusion source vectors. Therefore, the fitting term is \neffective in learning salient features that can distinguish each \ntraining sample from other samples contained in the training \ndataset. \nSmoothing term: Lsmooth constrains the extrinsic features \nand their neighboring intrinsic features to have similar \nranking score vectors. Identical to the fitting term, rb of \nLsmooth is computed as rb = softmax(fb ∙ MT). rn is defined as \nrn = softmax(mn ∙ MT) where mn is n-th row of M. Thus, rn \ncontains ranking scores from the intrinsic feature mn to all \nthe intrinsic features including mn itself. wbn indicates \nsimilarity between the extrinsic feature fb and the intrinsic \nfeature mn. wbn can be interpreted as one of the connection \nweights of the bipartite manifold graph, whose matrix size is \nB×N, formed between the B extrinsic features and the N \nintrinsic features. The value for wbn is computed by using (5), \nwhere kNN(fb) is a set of k nearest intrinsic features of fb. We \nuse Cosine distance in the latent feature space to compute \nkNN(fb). The neighborhood size k is a hyper-parameter to be \ndetermined later. D(rb || rn) is a dissimilarity between the two \nranking score vectors. We use the Jensen-Shannon \ndivergence, i.e., a symmetric version of the Kullback-Leibler \ndivergence, to measure the dissimilarity between the two \nprobability distributions rb and rn. \n\n     if  \notherwise\n0\nT\nn\nb\nb\nn\nbn\nkNN\nw\n\n\n\n\n\nm\nf\nf\nm\n \n(5) \nBy reducing Lsmooth, the extrinsic features and their \nneighboring intrinsic features are attracted to each other in \nthe latent feature space. In other words, when Lsmooth is small, \nthe extrinsic features are likely to be projected onto the \nsurface of the latent feature manifold formed by the intrinsic \nfeatures. Therefore, concurrently minimizing Lfit and Lsmooth \nhelps the encoder DNN embed each training sample in the \nproximity of its corresponding intrinsic feature point on the \nlatent manifold. In such a latent feature space, the distances \nbetween the extrinsic features computed as diffusion \ndistances approximate the distances along the surface of the \nlatent feature manifold.  \nChoosing hyper-parameters: The LMR loss has two \nhyper-parameters, that are, the balancing parameter λ and the \nnumber of nearest intrinsic features k. Both hyper-parameters \ncontrol smoothness of the feature distribution in the latent \nfeature space. Optimal values for λ and k would depend on \nvarious conditions including data samples used for training \nand the architecture of an encoder DNN. To validate \ngeneralization ability of the DD algorithm, we fix λ at 1 and k \nat 20 when we empirically compare DD against the existing \nunsupervised \nfeature \nlearning \nalgorithms. \nWe \nalso \nexperimentally investigate the influences that these hyper-\nparameters have on retrieval accuracy. \nInitializing \nlearnable \nparameters: \nWe \nrandomly \ninitialize the set of connection weights θ of the encoder DNN \nby using the algorithm proposed by He et al. [29]. \nSubsequently, each of the training samples is projected into a \nfeature in the latent space by using the randomly initialized \nencoder. These randomly projected feature vectors are used \nas the initial values of the latent feature manifold matrix M. \nWe employ such initialization of M since several studies [30] \n[31] have shown that randomly initialized DNNs extract \nfeatures having some degree of accuracy. We expect that M \ninitialized with the randomly projected features represents \nmore accurate initial manifold structure than M initialized \nwith random values sampled from, for example, a normal \ndistribution. The effectiveness of initializing M with \n \n \n7 \nrandomly projected features will be shown in the \nexperiments. \nEffect of data augmentation: Data augmentation is \napplied to each training sample prior to passing it to the DNN. \nWe expect that the training with data augmentation yields \nlatent features robust against perturbation of training samples \n(e.g., affine transformation of 3D shapes). The fitting term \nLfit facilitates learning such robust features. That is, ranking \nvectors of multiple augmented data samples derived from an \noriginal training sample are pulled toward the same diffusion \nsource vector as their original. As a result, latent features \nbecome less sensitive to the perturbation caused by the data \naugmentation. \nThe \ndetailed \nprocedure \nof \nthe \ndata \naugmentation is described in Section IV.A. \nAlgorithm 1 summarizes the procedures of unsupervised \nDNN training by the DD algorithm. \nC. FEATURE EXTRACTION \nAfter the training, the encoder DNN and the latent manifold \nare used for feature extraction from unseen data samples. We \npropose two features, that are, an embedded feature DD (E) \nand a diffused feature DD (D). Computation of the embedded \nfeature is straightforward. An input sample is projected to the \nlatent feature space by using the encoder DNN. The output \nfrom the encoder, i.e., f, is the DD (E) feature of the input. \nThe Euclidean distance between the pair of DD (E) features \nis expected to approximate the diffusion distance on the \nlatent feature manifold. Algorithm 2 summarizes the \nprocedures for extracting the DD (E) feature. \nWe observe that the iterative diffusion process adopted by \nthe traditional diffusion-based ranking algorithms [45] would \nmake the DD (E) feature better fit for retrieval. We thus \npropose the DD (D) feature, which is a smoothed version of \nthe DD (E) feature. The DD (D) feature is computed by \nsimilarity diffusion on the latent feature manifold. After \ncomputing f, we find its k nearest intrinsic features kNN(f) \nfrom the optimized M, or {mn}n=1, …, N, to determine multiple \ndiffusion sources. kNN(f) is a set of k intrinsic features \nclosest to f found by using the Euclidean distance. The \ndiffusion sources are represented by an N-dimensional vector \ng0. The n-th element of g0, i.e., g0(n), is computed by using \n(6).  \n\n0\n1      if  \n( )\n0\notherwise\nn\nkNN\nn\n\n\n\n\nm\nf\ng\n \n(6) \n We use the recursive formula gr = gr˗1 ∙ S, which is one of \nAlgorithm 1 Training by using the DD algorithm. \n1: Inputs: The training dataset {(xn, IDn)}n=1, …, N \n2: \nThe encoder DNN parameterized by θ \n3: \nThe balancing coefficient λ (e.g., λ=1) \n4: \nThe number of neighbors k (e.g., k=20) \n5: \nThe optimization algorithm (e.g., Adam [43] with \ninitial learning rate η=10−4 ) \n6: Outputs: Optimized encoder parameters θ  \n7: \nOptimized intrinsic features M, or {mn}n=1, …, N , of \nthe training samples \n8: Initialization:  \n9: \nRandomly initialize θ. \n10: \nInitialize intrinsic features M by feeding all the training \nsamples into the encoder DNN. \n11: for each training epoch do \n12: \nfor each minibatch {(xb, IDb)}b=1, …, B sampled from the \ntraining dataset do \n13: \nAugment the samples {xb}b=1, …, B to obtain {x̂ b}b=1, …, B. \n14: \nFeed the augmented samples {x̂ b}b=1, …, B into the \nencoder DNN to extract their latent \nfeatures \n{fb}b=1, …, B  . \n15: \nCreate one-hot vectors {yb}b=1, …, B  from {IDb}b=1, …, B . \n16: \nCompute Lfit by using {fb} and {yb} as inputs to Eq. (3). \n17: \nCompute Lsmooth by using {fb}, M, and k as inputs to Eq. \n(4) and (5). \n18: \nCompute the overall loss value: L ← Lfit + λ Lsmooth . \n19: \nUpdate θ and M by using the optimization algorithm: \n20: \nL\n\n\n\n\n\n\n\n \n21: \nL\n\n\n\n\nM\nM\nM\n \n22: \nend for \n23: end for \n24: return θ and M \nAlgorithm 3 Extracting DD (D) feature. \n1: Inputs: The data sample x \n2: \nThe encoder DNN optimized by Algorithm 1 \n3: \nThe intrinsic features M optimized by Algorithm 1 \n4: \nThe number of neighbors k (e.g., k=20) \n5: \nThe number of iterations R for similarity diffusion \n(e.g., R=20). \n6: Output: The DD (D) feature gR of x \n7: Construct the sparse similarity matrix S: (S can be \nprecomputed prior to feature extraction.) \n8: \nS ← M ∙ MT \n9: \nfor each row si of S do \n10: \nSparsify si by replacing all the elements except for the k \nlargest elements with 0.  \n11: \nend for \n12: Feed the input sample x into the encoder DNN to obtain the \nlatent feature f. \n13: Find a set of k nearest intrinsic features of f, i.e., kNN(f), from \nthe intrinsic features M. \n14: Create the diffusion source vector g0 by using Eq. (6). \n15: for r = 1, ..., R do \n16: \nDiffuse similarity: gr ← gr˗1 ∙ S \n17: end for \n18: Normalize the scale of gR: gR ← gR / ||gR|| \n19: return gR  \nAlgorithm 2 Extracting DD (E) feature. \n1: Inputs: The data sample x \n2: \nThe encoder DNN optimized by Algorithm 1 \n3: Output: The DD (E) feature f of x \n4: Feed the input sample x into the encoder DNN to obtain the \nlatent feature f.  \n5: return f \n \n \n8 \nthe simplest forms of diffusion computation on a feature \nmanifold [45], to derive similarity. S ∈ ℝN×N is a sparse \nsimilarity graph whose (i, j) element is mi ∙ m T \nj  if mj is \nincluded in a set of k nearest neighbors of mi, and 0 \notherwise. By iterating the recursive formula, the diffusion \nsource vector gr becomes smooth according to the pairwise \nsimilarities of the intrinsic features optimized during the \ntraining. We iterate the recursion for R=20 times, whose \nvalidity is evaluated in the experiments. After the iteration, gR \nis scaled to unit length to obtain the DD (D) feature. k for \ncomputing the diffusion feature is identical to k used in the \nLMR loss. Algorithm 3 summarizes the procedures for \nextracting the DD (D) feature. \n Retrieval rankings are generated by using the Euclidean \ndistances among either the DD (E) features, the DD (D) \nfeatures, or the “fused” features. Intending to improve \nretrieval accuracy, the fused feature DD (E+D) is formed by \nconcatenating the DD (E) vector (i.e., f) and the DD (D) \nvector (i.e., the scaled gR). \nIV. EXPERIMENTS AND RESULTS \nA. EXPERIMENTAL SETUP \nWe comprehensively evaluate the effectiveness of the \nproposed DD algorithm through the experiments using \nmultiple data types (i.e., 3D shape and 2D image), multiple \ndatasets, and multiple encoder architectures.  \nDatasets: We use three 3D shape datasets and three 2D \nimage datasets. Table 1 summarizes the statistics for each \ndataset. ModelNet10 (MN10) [32], ModelNet40 (MN40) \n[32], and ShapeNetCore55 (SN55) [33] contain 3D \npolygonal models of rigid 3D objects such as furniture, \nvehicles, and household appliances. We convert each 3D \nshape to a set of 1,024 3D points by using the algorithm by \nOhbuchi [34]. Each 3D point set is normalized so that its \ngravity center coincides the origin of the 3D space and its \nwhole shape is enclosed by a unit sphere. Fashion MNIST \n(FMNIST) [35] includes 2D images of clothes, while STL10 \n[37] consists of 2D natural images. COIL100 [36] contains \n2D images of 100 different objects placed on a turntable. For \neach object, a set of 72 images was taken at intervals of 5 \ndegrees while rotating the turntable. We use the images taken \nbetween 0 to 175 degrees for training, and the rest for \nevaluation.  \nRetrieval accuracy of the testing data is measured in Mean \nAverage Precision (MAP) [%], which is the de facto standard \naccuracy index for information retrieval systems. \nCompetitors: The DD algorithm is compared against 11 \nexisting unsupervised feature learning algorithms. They are \nAE [3], GAN [7], Noise As Targets (NAT) [16], Parametric \nInstance Discrimination (PID) [5], Non-Parametric Instance \nDiscrimination (NPID) [10], DeepCluster (DC) [11], SwAV \n[12], LocalAggregation (LA) [13], Invariant and Spreading \nInstance Feature (ISIF) [6], Simple framework for \nContrastive Learning of Representations (SimCLR) [14], and \nMomentum Contrastive learning (MoCo) [15]. These \ncompetitors train their encoder DNNs by using different \ntraining objectives, or loss functions. AE uses a self-\nreconstruction loss. We use the Chamfer distance loss [62] \nfor the 3D point set AE and the mean squared error loss for \n2D image AE. GAN uses the adversarial loss [7] for training. \nNAT, PID, NPID, DC, SwAV, and LA employ pseudo-label \nclassification loss, while ISIF, SimCLR, and MoCo employ \nfeature contrast loss. Pseudo-label classification and feature \ncontrast are training objectives widely used for unsupervised \nfeature learning. We fix the number of clusters for DC, \nSwAV, and LA at roughly 10 times the number of object \ncategories, as suggested in [11]. \nImplementation details: For a fair comparison, we use \nthe same data augmentation, DNN architecture, and \noptimization method among all the algorithms used in the \nexperiments. \nWe employ online data augmentation. Each training \nsample in the mini-batch is augmented with a probability of \n0.8. A 3D point set is augmented by random affine \ntransformation. Specifically, the point set is first randomly \nrotated about the x-, y-, and z-axes in this order. The rotation \nangle for each axis is randomly and independently chosen \nfrom the uniform distribution U(−5°, 5°). After the rotation, \nthe point set is anisotropically scaled. Directions of scaling \nare the x-, y-, and z-axes, and scaling factor for each axis is \nrandomly sampled from U(0.8, 1.2). The 3D shape is then \nsheared successively in the x-, y- and z-axes. Shearing factor \nfor each axis is sampled from U(−0.2, 0.2). Finally, the 3D \nshape is randomly translated. Displacement along each axis \nis chosen from U(−0.2, 0.2). A 2D image is augmented by \nrandom cropping and horizontal flipping. The 2D image is \nisotropically scaled by the factor 1.2, and a rectangle having \nthe original image size is cropped at random position. The \ncropped image is then horizontally flipped with the \nprobability of 0.5. \nWe use four popular DNN architectures as encoders. To \nencode 3D point sets, we adopt PointNet [38] and Dynamic \nGraph CNN (DGCNN) [39]. To process 2D images, we \nemploy Inception V1 (also known as GoogLeNet) [40] and \nResNet18 [41]. The number of dimensions P of the latent \nspace is fixed at 256 throughout the experiments. In addition \nto the encoder DNN, AE and GAN require a decoder DNN. \nWe employ the 3D point set decoder composed of five fully-\nconnected layers [42] and the 2D image decoder consisting \nTable 1 3D shape/2D image datasets used in our experiments. \nDataset \nData format for \neach sample \nNumber of  \ncategories \nNumber of  \ntraining data \nNumber of \ntesting data \nMN10 [32] \na set of 1,024  \n3D points \n10 \n3,991 \n908 \nMN40 [32] \n40 \n9,843 \n2,468 \nSN55 [33] \n55 \n35,764 \n10,265 \nFMNIST [35] \n28×28 pix., 1 ch. \n10 \n60,000 \n10,000 \nCOIL100 [36] 128×128 pix., 3 ch. \n100 \n3,600 \n3,600 \nSTL10 [37] \n96×96 pix., 3 ch. \n10 \n100,500 \n800 \n \n \n9 \nof five deconvolution layers [7]. \nThe DNNs are trained by using mini-batch gradient \ndescent. We use Adam [43] with the initial learning rate of \n10−4. Each mini-batch contains 16 3D shapes or 64 2D \nimages. We iterate the training for 300 epochs and report the \nhighest MAP scores obtained during the training.  \nThe DD algorithm is implemented in Python using \nTensorFlow library [44]. Most of the experiments were done \non a PC having an Intel Core i9-7900X CPU and an Nvidia \nGeForce RTX 2080 Ti with 11 GBytes of GPU memory.  \nB. EXPERIMENTAL RESULTS AND DISCUSSION \n1) Comparison with existing algorithms \nTable 2 compares retrieval accuracies of the 12 unsupervised \nfeature learning algorithms including the proposed DD. In \nthe tables, “DD (E)” and “DD (D)” indicate the embedded \nfeature and the diffused feature, respectively, learned by \nusing the DD algorithm. “DD (E+D)” denotes concatenation \nof the two feature vectors above. The hyper-parameters of \nDD, i.e., k and λ, are fixed at 20 and 1, respectively. As the \nreference, Table 2 includes “untrained” that shows accuracy \nof the features extracted by using the randomly initialized \nencoder DNN.  \nAs shown in Table 2, our DD algorithm yields MAP \nscores higher than the existing methods for almost all the \ncombinations of the encoder DNNs and the datasets. These \nresults verify that optimization using the diffusion distance \non the latent feature manifold is effective for learning feature \nrepresentation adapted to multimedia information retrieval.  \nIn addition, Table 2 compares efficacy of the two proposed \nfeatures, i.e., DD (E) and DD (D). The winner among DD (E) \nand DD (D) depends on dataset and encoder used. But the \nfused feature DD (E+D) slightly but consistently outperforms \nthe two. We suspect that distances among the embedded \nfeatures of the testing data slightly differ from the diffusion \ndistances on the latent manifold since the testing data are not \nused in training. Thus, the diffused feature, which is \ncomputed by using the latent feature manifold, would assist \nthe embedded feature to find larger number of true neighbors \nin the latent feature space. \nFig. 2 visualizes the latent feature spaces learned by using \nthe three unsupervised feature learning algorithms, i.e., AE, \nSwAV, and our DD. We use PointNet and Inception V1 as \nencoder DNNs and use DD (E) as the latent features of DD. \nThe t-SNE algorithm [53] is used to visualize the latent \nfeatures extracted from the testing samples of the MN10 and \nFMNIST datasets. As can be observed in Fig. 2, the latent \nfeatures learned by the DD algorithms are well-separated. \nThat is, each cluster of the latent features consists of one or \ntwo semantic categories in most cases and these clusters are \nreasonably far from each other. Such feature embeddings \ncontribute to the high retrieval accuracy of the DD algorithm \nshown in Table 2. On the other hand, AE and SwAV tend to \nlearn feature embeddings that are not suitable for information \nretrieval. In the latent space of AE, the feature clusters \noverlap each other. SwAV learns favorable feature \nembeddings for the MN10 dataset, but the latent features of \nthe FMNIST dataset are separated into small clusters. Since \nthe training frameworks of AE and SwAV are not intended to \nacquire retrieval-adapted features, they do not necessarily \nform latent feature spaces suitable for retrieval. \n2) In-depth evaluation of DD algorithm \nThis subsection investigates the influences that various \ndesign parameters of the DD algorithm have on retrieval \naccuracy. We use the MN10, MN40, FMNIST, and \nCOIL100 datasets for evaluation. PointNet and Inception V1 \nare employed as encoder DNNs.  \nHyper-parameters: Fig. 3 plots retrieval accuracies of the \nDD (E) feature against the number of nearest intrinsic \nfeatures k used for computing the smoothing term. We can \nTable 2 Comparison of retrieval accuracies (MAP [%]) using multiple data types, datasets, and encoder DNN architectures. \nTraining \nobjective \nUnsupervised \nfeature learning \nalgorithm \n3D point set data \n2D image data \nPointNet [38] encoder \nDGCNN [39] encoder \nInception V1 [40] encoder \nResNet18 [41] encoder \nMN10 \ndataset \nMN40 \ndataset \nSN55 \ndataset \nMN10 \ndataset \nMN40 \ndataset \nSN55 \ndataset \nFMNIST \ndataset \nCOIL100 \ndataset \nSTL10 \ndataset \nFMNIST \ndataset \nCOIL100 \ndataset \nSTL10 \ndataset \n− \nuntrained \n53.3 \n35.9 \n39.8 \n37.0 \n24.8 \n32.0 \n23.4 \n57.4 \n12.4 \n24.7 \n53.8 \n11.2 \nself-reconstruction \nAE [3] \n67.3 \n46.4 \n51.2 \n68.0 \n47.2 \n51.6 \n44.2 \n69.1 \n14.8 \n42.8 \n68.1 \n14.8 \ndata generation \nvs. discrimination \nGAN [7] \n67.0 \n48.8 \n52.9 \n58.1 \n49.4 \n50.9 \n28.9 \n69.3 \n13.7 \n36.2 \n63.8 \n13.1 \nPseudo-label \nclassification \nNAT [16] \n68.5 \n47.8 \n50.1 \n59.0 \n47.2 \n48.9 \n37.9 \n80.7 \n16.1 \n46.8 \n83.0 \n16.8 \nPID [5] \n70.9 \n49.4 \n53.9 \n66.2 \n50.7 \n52.9 \n35.3 \n81.4 \n15.2 \n44.9 \n83.7 \n15.3 \nNPID [10] \n71.5 \n45.3 \n57.0 \n72.9 \n47.7 \n58.9 \n39.0 \n76.1 \n16.6 \n37.1 \n79.8 \n16.0 \nDC [11] \n70.4 \n46.1 \n54.9 \n71.6 \n48.1 \n53.6 \n43.5 \n84.3 \n18.6 \n39.5 \n86.3 \n16.2 \nSwAV [12] \n73.9 \n48.4 \n56.2 \n73.7 \n53.5 \n57.7 \n35.2 \n78.3 \n16.3 \n31.9 \n80.7 \n17.1 \nLA [13] \n76.2 \n54.0 \n44.4 \n74.9 \n57.2 \n43.9 \n31.9 \n80.7 \n17.1 \n34.3 \n81.2 \n16.5 \nLatent feature \ncontrast \nISIF [6] \n67.0 \n51.1 \n51.6 \n64.5 \n52.1 \n51.3 \n31.1 \n77.2 \n14.2 \n32.9 \n72.4 \n15.0 \nSimCLR [14] \n70.1 \n54.2 \n55.2 \n70.5 \n56.4 \n53.9 \n29.6 \n76.6 \n17.0 \n36.4 \n74.2 \n16.0 \nMoCo [15] \n70.3 \n52.8 \n52.2 \n67.8 \n54.3 \n51.9 \n35.2 \n81.4 \n16.7 \n37.3 \n82.5 \n17.2 \nSimilarity \ndiffusion on latent \nfeature manifold \nDD (E)  (ours) \n80.4 \n58.0 \n58.6 \n80.6 \n61.2 \n59.3 \n49.8 \n89.2 \n20.1 \n49.8 \n85.8 \n17.4 \nDD (D)  (ours) \n76.8 \n56.2 \n61.4 \n76.5 \n58.2 \n61.7 \n50.0 \n89.3 \n19.7 \n50.9 \n84.1 \n17.3 \nDD (E+D)  (ours) \n80.6 \n59.9 \n61.7 \n80.9 \n62.6 \n62.2 \n50.2 \n90.4 \n20.6 \n51.7 \n86.0 \n17.6 \n \n \n10 \nobserve that, for all the four datasets, MAP scores gradually \nimprove with increasing k from 1 to 20. Optimal k depends \non dataset (and probably also on encoder DNN). The \nFMNIST dataset requires larger k than the other datasets to \nattain peak accuracy. This is probably because the category \nsize of FMNIST (6,000 samples per category) is larger than \nthose of the other datasets (tens to hundreds samples per \ncategory). When the category size is large, computing the \nsmoothing term with large k would facilitate capturing the \nmanifold structure composed of many samples of the same \ncategory. \n Fig. 4 shows the influence the balancing parameter λ of \nthe LMR loss has on the retrieval accuracy of the DD (E) \nfeature. In the graph, λ=0 means the DNN is trained by using \nthe fitting term only. Fig. 4 indicates that the smoothing term \nhas positive impact on retrieval accuracy. However, \nexcessively large λ leads to low retrieval accuracy since \ngradients derived from the loss function is dominated by the \nsmoothing term. \nFig. 5 shows retrieval accuracy of the DD (E+D) feature \nplotted against the number of iterations R for calculating the \ndiffused feature. Although the influence by R on the accuracy \nis small, the diffusion with sufficient iterations slightly \nimproves the MAP score. The accuracies saturate at about \nabout R=20 for the MN10, MN40, and COIL100 datasets. In \nthe FMNIST, the accuracy continues to improve at R > 20. \n \n \n \n \nAE \nSwAV \nDD (ours) \n \n(a) MN10 dataset \n \n \n \nAE \nSwAV \nDD (ours) \n \n(b) FMNIST dataset \nFIGURE 2 Visualization of the latent feature space learned by AE [3], SwAV [12], and our DD algorithm.  \n \n \n \nFIGURE 3 Retrieval accuracy of the DD (E) \nfeature plotted against the number k of \nneighboring intrinsic features. \nFIGURE 4 Retrieval accuracy of the DD (E) \nfeature \nplotted \nagainst \nthe \nbalancing \nparameter λ for the LMR loss. \nFIGURE 5 Retrieval accuracy of the DD (E+D) \nfeature \nplotted \nagainst \nthe \nnumber \nof \niterations R for the diffused feature. \n \n \n \n30\n40\n50\n60\n70\n80\n90\n100\n0\n20\n40\n60\n80\n100\nMAP [%]\nNumber of nearest neighbors k\nMN10\nMN40\nFMNIST\nCOIL100\n30\n40\n50\n60\n70\n80\n90\n100\n0\n1/8\n1/4\n1/2\n1\n2\n4\n8\nMAP [%]\nCoefficient λ for smoothing term\nMN10\nMN40\nFMNIST\nCOIL100\n45\n55\n65\n75\n85\n95\n0\n10\n20\n30\n40\nMAP [%]\nNumber of iterations R\nMN10\nMN40\nFMNIST\nCOIL100\n \n \n11 \nSince FMNIST has more training data than the other three \ndatasets, more iterations would be required for convergence \nof the similarity diffusion. \nAblation study: We evaluate contributions from the \ncomponents of the DD algorithm, that are, the loss function, \nthe data augmentation of training samples, and the \ninitialization of intrinsic features. Table 3 demonstrates that \nall the four components positively impact retrieval accuracy. \nThe two terms of the LMR loss, i.e., Lfit and Lsmooth, are \nindispensable to learn retrieval-adapted features. The data \naugmentation \nclearly \nimproves \nretrieval \naccuracy. \nDiversifying training samples would help the DD algorithm \nobtain robustness against perturbation of the samples and \nimprove generalization ability. \nTable 3 also shows that the initialization method for \nintrinsic features has a significant impact on accuracy. In the \ncolumn “RPF init.”, “Yes” indicates that the intrinsic features \nare initialized with the features of training data extracted by \nthe encoder having random parameters. “No” means that the \nintrinsic features are initialized with randomly sampled \nvalues [29]. Evidently, initialization with random projection \nfeatures improves accuracy. Fig. 6 compares training \nbehavior \nbetween \nthe \ntwo \ninitialization \nmethods. \nInitialization with random values appears to cause \noverfitting; MAP of the testing data saturates early (around \n120 epochs) while loss value of the training data continues to \ndecrease. In contrast, the training started with randomly \nprojected features is more regularized and yields higher MAP \nscore (nearly 80%) on the MN10 dataset. \nComputational cost for training: We evaluate training \nscalability of the DD algorithm. To do so, we create pseudo \nlarge-scale training datasets by duplicating the 3D shape data \ncontained in the MN10 dataset. During training, we measure \nGPU memory footprint and time per epoch. Fig. 7 shows that \nthe training by DD is reasonably efficient when the number \nof training samples N is less than 100K. However, both \nspatial and temporal costs grow significantly as N increases. \nUsing the GPU having 11 GBytes of memory, training is not \nexecutable at N > 330K due to an out-of-GPU-memory error. \nTraining by DD requires O(N) spatial complexity since \nmultiple N-dimensional ranking score vectors as well as N \nintrinsic features need to be stored to compute the LMR loss. \nThe increase in GPU memory usage against N, however, is \nnot linear in Fig. 7. This is probably due to the behavior \nspecific to the TensorFlow library. In terms of training time, \nthe DD algorithm requires O(N2) temporal complexity since \nthe similarities (i.e., wbn in (4)) of all pairs of the N extrinsic \nfeatures and the N intrinsic features must be calculated \nduring training of one epoch.  \nThe high training costs of the DD algorithm might be \nmitigated by using subsampling methods (e.g., [46]) to \nreduce N for loss computation, or approximate nearest \nneighbor search methods (e.g., [47]) to avoid the all-pair \ncomparison among the extrinsic/intrinsic features. We leave \nfor future work applications of these techniques to improve \nefficiency of the DD algorithm. \nV. DISCUSSION \nWhile the quantitative experiments in Section IV show the \neffectiveness of the proposed DD algorithm, it is still unclear \nhow DD behaves differently from the existing unsupervised \ndeep feature learning algorithms. As we mentioned in \nSection I, DD aims to transform the initial latent feature \nspace so that a submanifold consisting of mutually similar \nfeatures to contract while mutually distinct submanifolds to \nmove farther away from each other. To prove that DD works \nas intended, this section compares training behaviors of the \nmultiple feature learning algorithms including DD. We \nvisualize and observe changes in the manifold structure in the \nlatent feature space during training. However, the \nvisualization methods for high-dimensional space such as t-\nSNE used in Fig. 2 are not suitable for observing the true \nTable 3 Ablation study of the DD algorithm. Accuracy of DD (E) feature \nis measured in MAP [%]. (DA: data augmentation, RPF init.: initializing \nintrinsic features with randomly projected features.) \nLfit \nLsmooth \nDA \nRPF init. \nMN10 \nMN40 FMNIST COIL100 \nYes \nYes \nYes \nYes \n80.4 \n58.0 \n49.8 \n89.2 \nNo \nYes \nYes \nYes \n56.4 \n35.7 \n20.9 \n63.7 \nYes \nNo \nYes \nYes \n70.0 \n48.9 \n45.0 \n84.1 \nYes \nYes \nNo \nYes \n71.2 \n49.2 \n47.2 \n81.7 \nYes \nYes \nYes \nNo \n75.8 \n53.3 \n32.5 \n87.8 \n \n \nFIGURE 6 Training behavior on the MN10 dataset using different\ninitialization methods for the intrinsic features M. \n \nFIGURE 7 Computation costs for DNN training as a function of the\nnumber of training samples N. \n7.0\n7.5\n8.0\n8.5\n9.0\n45\n50\n55\n60\n65\n70\n75\n80\n85\n0\n50\n100\n150\n200\n250\n300\nLoss (training data)\nMAP [%] (testing data)\nTraining epochs\nInit. with randomly projected features\nInit. with randomly sampled values\n0\n20\n40\n60\n80\n100\n120\n140\n0\n2\n4\n6\n8\n10\n12\n0\n50K 100K 150K 200K 250K 300K 350K 400K\ntraining time per epoch \n[minutes]\nGPU memory footprint \n[GBytes]\nNumber of training samples N\nOut-of-\nmemory\n \n \n12 \nstructure of feature manifold. This is because these \nvisualization methods by themselves use some form of \ndimension reduction that could obscure the observation. We \nthus use a far simpler problem, i.e., a 2D toy dataset, to \nobserve manifold structures. Our toy dataset consists of 1,000 \n2D points lying on the three-arm spiral distribution (see “0th \nepoch” in Fig. 8a).  \nWe first describe the setup of the experiment. We use an \nencoder DNN consisting of four fully-connected layers. The \ninput and output layers of the encoder have two neurons, \n30\n40\n50\n60\n70\n80\n90\n100\n0\n1,000\n2,000\n3,000\n4,000\n5,000\n6,000\nMAP [%] (training data)\nTraining epochs\nSimCLR \nDD with k=200 \nDC with 30 clusters \nPID \nDD with k=20 \nDD with k=5 \nPID \n0th epoch \n500th epoch \n2,000th epoch \n6,000th epoch \nFIGURE 8. Comparison of training behaviors of the unsupervised feature learning algorithms using the 2D toy dataset. (a) While the existing\nalgorithms deform the latent feature space ignoring the structure of the initial latent feature manifold, our DD with the proper number of neighbors\n(k=20) succeeds in disentangling the nonlinearity of the initial latent manifold. (b) As the DNN training progresses, retrieval accuracy of DD with k=20\nimproves. That is, the latent features of DD with k=20 adapted to retrieval during training. \nSimCLR \n0th epoch \n500th epoch \n2,000th epoch \n6,000th epoch \nDD with k=20 \n0th epoch \n500th epoch \n2,000th epoch \n6,000th epoch \nDC with 30 clusters \n0th epoch \n500th epoch \n2,000th epoch \n6,000th epoch \nDD with k=5 \n0th epoch \n500th epoch \n2,000th epoch \n6,000th epoch \nDD with k=200 \n0th epoch \n500th epoch \n2,000th epoch \n6,000th epoch \n(b) Retrieval accuracy of 2D toy data plotted against training epoch. \n(a) Visualization of 2D latent feature space formed by 1,000 features extracted from the 2D toy data. Color of each latent feature point indicates its \npseudo semantic category. \n \n \n13 \nwhile the hidden layers have 1,000 neurons. Following the \nmanifold hypothesis, we want to create a nonlinear feature \nmanifold (e.g., the three-arm spiral) in the latent space of the \nencoder DNN. To do so, we pretrain the encoder DNN so \nthat it learns an identity mapping. That is, we sample 10,000 \n2D points from the uniform distribution as training data for \npretraining. The encoder DNN is pretrained so that the input \n2D points are embedded in the same coordinates as the inputs \nin the 2D latent space. After the pretraining, the encoder \nDNN is further trained by using either PID, DC, SimCLR, or \nour DD. For DD, we use the different values for the number \nof neighbors k, i.e., k=5, 20, and 200. The 1,000 2D points \nlying on the three-arm spiral is used as a training dataset. The \ntraining is iterated for 6,000 epochs. In addition to observing \nthe latent feature space, we compute retrieval accuracy of the \nlatent features. To measure retrieval accuracy, we embed all \nthe 1,000 2D toy data points in the training dataset and \ncompare these latent features by using the Euclidean distance. \nA MAP score is computed assuming that the features lying \non the same submanifold, i.e., the same arm of the spiral, \nbelong to the same semantic category. \nFig. 8a visualizes changes in the 2D latent feature space \nduring training. Fig. 8b shows retrieval accuracies, measured \nin MAP, during training. Note that the latent feature \ndistribution and the MAP score (77%) at the beginning of \ntraining (0th epoch) are the same across all the methods in \nFig. 8. Evidently, the six cases in Fig. 8 show different \ntraining behaviors. PID and DC destroy the structure of the \ninitial latent feature manifold. That is, the three submanifolds \nduring training overlap with each other, resulting in decrease \nin retrieval accuracy. PID in this experiment tries to embed \nthe latent features in a straight line so that each training \nsample can be discriminated. However, PID fails to separate \nthe green submanifold from the other two. DC in this \nexperiment mixes up the latent features after 2,000 epochs. \nThis is because pseudo labels, i.e., cluster centers, are \ngenerated among different submanifolds. Latent features are \nmixed up since those belonging to the different submanifolds \nare attracted to the same cluster center. For SimCLR, the \nlatent feature space is hardly distorted and retrieval accuracy \nis almost unchanged throughout training. This is probably \nbecause the training objective of SimCLR, i.e., making \ndistances among positive pairs smaller than distances among \nnegative pairs, is achieved to some extent in the initial latent \nspace. Although SimCLR does not show high retrieval \naccuracy, it is likely to yield high classification accuracy if a \nnonlinear classifier is trained in the learned latent feature \nspace.  \nIn contrast, DD behaves differently from PID, DC, and \nSimCLR. When DD uses an appropriate value for k (i.e., DD \nwith k=20), the nonlinearly distributed initial latent features \nare successfully disentangled. Although continuity of the \ninitial feature distribution is lost to some degree, the three \nsubmanifolds at the 6,000th epoch are mutually separated \nand the latent features on the same arm are embedded close \nto each other. Retrieval accuracy of DD with k=20 improves \nas the training progresses, and MAP score reaches nearly \n97%. These results indicate that the initial latent features \nproperly adapt to retrieval via the training by DD.  \nThe success of DD stems from the design of its loss \nfunction consisting of both the fitting term and the smoothing \nterm. As we mentioned in Section III.B, the fitting term acts \nto move latent features away from each other. Therefore, \nusing only the fitting term ignores the manifold structure and \nwould result in a latent feature space similar to PID. The \nsmoothing term helps DD consider the manifold structure by \nconstraining neighboring latent features to have similar \nranking scores. Combining the fitting term and the smoothing \nterm (with proper k) enables DD to transform the latent \nfeatures while preserving the proximity of data points in \nneighborhoods on the same submanifold. \nFig. 8 also shows that using an inappropriate value for k \nleads DD to training failure. When k is small, i.e., k=5, the \nlatent features after the training of 6,000 epochs are split into \nnumerous small clusters and retrieval accuracy scarcely \nchanges from the initial MAP score. Or, if k is too large, i.e., \nk=200, a collapse of the latent feature space occurs. The \ncollapse occurs since the smoothing term acts strongly at a \nlarge scale. Each latent feature attracts 200 of its neighbors \nout of 1,000 training samples. Such a global influence force \nall the features to converge at the same point in the latent \nspace. As also demonstrated in Fig. 3, the smoothing term \nand its hyperparameter k have a significant impact on the \nretrieval accuracy of DD. \nVI. CONCLUSION AND FUTURE WORK \nThis paper tackled the problem of unsupervised learning of \nfeature representations suitable for multimedia information \nretrieval. To obtain retrieval-adapted features without relying \non semantic labels, we proposed the novel algorithm called \nDeepDiffusion (DD). The DD exploits diffusion distances on \na latent feature manifold to optimize a feature extraction and \nembedding as well as a distance metric among embedded \nfeatures. The optimization is achieved by using the carefully \ndesigned loss function, named Latent Manifold Ranking \n(LMR) loss, which encourages to form the latent feature \nspace suitable for computing similarities among data samples. \nThe comprehensive evaluation showed the following \nadvantages and limitation of the DD algorithm. \n DD is capable of learning features more adapted to \ninformation retrieval than the existing deep learning-\nbased unsupervised feature learning algorithms. \n DD is versatile as it learns retrieval-adapted features \nregardless of the data types, the datasets, and the \nencoder DNN architectures we have tried. \n DD is reasonably efficient on datasets having less \nthan 100K training samples. However, for larger \n \n \n14 \ndatasets, DD suffers from high spatial and temporal \ncomputational costs. \nWe also evaluated a difference in training behavior among \nthe DD algorithm and the existing unsupervised feature \nlearning algorithms. The visualization of the latent feature \nspace \ndemonstrated \nthat \nDD \nwith \nan \nappropriate \nhyperparameter learns a distance metric that reflects the \nmanifold structure in an initial latent feature space.  \nFuture work includes evaluating the DD algorithm on \ndatasets having more samples (e.g., 1M) and datasets \nconsisting of multimedia data other than 3D shape and 2D \nimage (e.g., text document). As we discussed in the section \non experiments, the current DD algorithm has difficulty in \ntraining using very large datasets. We will thus consider \nimproving computational efficiency of the DD. Also, we \nintend to extend the DD algorithm to a scenario of semi-\nsupervised or supervised learning of retrieval-adapted feature \nrepresentations. \n REFERENCES \n[1] R. Hadsell, S. Chopra, Y. LeCun, Dimensionality Reduction by \nLearning an Invariant Mapping, Proc. CVPR 2006, pp. 1735–1742, \n2006. \n[2] F. Schroff, D. Kalenichenko, J. Philbin, FaceNet: A Unified \nEmbedding for Face Recognition and Clustering, Proc. CVPR 2015, \npp. 815–823, 2015. \n[3] G. E. Hinton, R. R. Salakhutdinov, Reducing the Dimensionality of \nData with Neural Networks, Science, 313(5786), pp. 504–507, 2006. \n[4] C. Doersch, A. Gupta, A. A. Efros, Unsupervised Visual \nRepresentation Learning by Context Prediction, Proc. ICCV 2015, pp. \n1422–1430, 2015. \n[5] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, T. Brox, \nDiscriminative Unsupervised Feature Learning with Convolutional \nNeural Networks, Proc. NIPS 2014, pp. 766–774, 2014. \n[6] M. Ye, X. Zhang, P. C. Yuen, S.-F. Chang, Unsupervised Embedding \nLearning via Invariant and Spreading Instance Feature, Proc. CVPR \n2019, pp. 6210–6219, 2019. \n[7] A. Radford, L. Metz, S. Chintala, Unsupervised Representation \nLearning with Deep Convolutional Generative Adversarial Networks, \nProc. ICLR 2016, 2016. \n[8] G. Larsson, M. Maire, G. Shakhnarovich, Learning Representations \nfor Automatic Colorization, Proc. ECCV 2016, pp. 577–593, 2016. \n[9] S. Gidaris, P. Singh, N. Komodakis, Unsupervised Representation \nLearning by Predicting Image Rotations, Proc. ICLR 2018, 2018. \n[10]  Z. Wu, Y. Xiong, S. X. Yu, D. Lin, Unsupervised Feature Learning \nvia Non-parametric Instance Discrimination, Proc. CVPR 2018, pp. \n733–3742, 2018. \n[11] M. Caron, P. Bojanowski, A. Joulin, M. Douze, Deep Clustering for \nUnsupervised Learning of Visual Features, Proc. ECCV 2018, pp. \n132–149, 2018. \n[12] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, A. Joulin, \nUnsupervised Learning of Visual Features by Contrasting Cluster \nAssignments, Proc. NeurIPS 2020, 2020. \n[13] C. Zhuang, A. L. Zhai, D. Yamins, Local Aggregation for \nUnsupervised Learning of Visual Embeddings, Proc. ICCV 2019, pp. \n6002–6012, 2019. \n[14] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A Simple Framework \nfor Contrastive Learning of Visual Representations, Proc. ICMR 2020, \npp. 1597–1607, 2020. \n[15] K. He, H. Fan, Y. Wu, S. Xie, R. Girshick, Momentum Contrast for \nUnsupervised Visual Representation Learning, Proc. CVPR 2020, pp. \n9729–9738, 2020. \n[16] P. Bojanowski, A. Joulin, Unsupervised Learning by Predicting Noise, \nProc. ICML 2017, pp. 517–526, 2017. \n[17] D. Zhou, J. Weston, A. Gretton, O. Bousquet, B. Schölkopf, Ranking \non data manifolds, Proc. NIPS 2004, pp. 169–176, 2004. \n[18] J. He, M. Li, H.-J. Zhang, H. Tong, C. Zhang, Manifold-Ranking \nBased Image Retrieval, Proc. MM 2004, pp. 9–16, 2004. \n[19] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, B. Schölkopf, Learning \nwith Local and Global Consistency, Proc. NIPS 2003, pp. 321–328, \n2003. \n[20] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial \nNetworks, Proc. NIPS 2014, 2014. \n[21] X. Huo, X. Ni, A. K. Smith, A Survey of Manifold-Based Learning \nMethods, In Recent Advances in Data Mining of Enterprise Data, pp. \n691–745, 2007. \n[22] M. Belkin, P. Niyogi, Laplacian Eigenmaps and Spectral Techniques \nfor Embedding and Clustering, Proc. NIPS 2001, pp. 586–691, 2001. \n[23] J. B. Tenenbaum, V. Silva, J. C. Langford, A Global Geometric \nFramework for Nonlinear Dimensionality Reduction, Science, \n290(5500), pp. 2319–2323, 2000. \n[24] K. Jia, L. Sun, S. Gao, Z. Song, B. E. Shi, Laplacian Auto-Encoders: \nAn explicit learning of nonlinear data manifold, Neurocomputing, 160, \npp. 250–260, 2015. \n[25] G. Mishne, U. Shaham, A. Cloninger, I. Cohen, Diffusion Nets, \nApplied and Computational Harmonic Analysis, 47(2), pp. 259–285, \n2019. \n[26] U. Shaham, K. Stanton, H. Li, B. Nadler, R. Basri, Y. Kluger, \nSpectralNet: Spectral Clustering using Deep Neural Networks, Proc. \nICLR 2018, 2018. \n[27] Z. Dou, H. Cui, L. Zhang, B. Wang, Learning Global and Local \nConsistent Representations for Unsupervised Image Retrieval via \nDeep Graph Diffusion Networks, arXiv preprint, arXiv:2001.01284, \n2020. \n[28] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, S. Singh, No \nFuss Distance Metric Learning using Proxies, Proc. ICCV 2017, pp. \n360–368, 2017. \n[29] K. He, X. Zhang, S. Ren, J. Sun, Delving Deep into Rectifiers: \nSurpassing Human-Level Performance on ImageNet Classification, \nProc. ICCV 2015, pp. 1026–1034, 2015. \n[30] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best \nmulti-stage architecture for object recognition?, Proc. ICCV 2009, pp. \n2146–2153, 2009. \n[31] A. M. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. Suresh, A. Y. Ng, On \nRandom Weights and Unsupervised Feature Learning, Proc. ICML \n2011, pp. 1089–1096, 2011. \n[32] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, J. Xiao, 3D \nShapeNets: A deep representation for volumetric shapes, Proc. CVPR \n2015, pp. 1912–1920, 2015. \n[33] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. \nLi, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, F. Yu, \nShapeNet: An Information-Rich 3D Model Repository, arXiv preprint, \narXiv:1512.03012, 2015. \n[34] R. Ohbuchi, T. Minamitani, T. Takei, Shape-Similarity Search of 3D \nModels by using Enhanced Shape Functions, IJCAT, 23, No. 2/3/4, \npp. 70–85, 2005. \n[35] H. Xiao, K. Rasul, R. Vollgraf, Fashion-MNIST: a Novel Image \nDataset for Benchmarking Machine Learning Algorithms, arXiv \npreprint, arXiv:1708.07747, 2017. \n[36] S. A Nene, S. K. Nayar, H. Murase, Columbia Object Image Library \n(COIL-100), Technical Report No. CUCS-006-96, 1996. \n[37] A. Coates, H. Lee, A. Y. Ng, An Analysis of Single Layer Networks \nin Unsupervised Feature Learning, Proc. AISTATS 2011, pp. 215–223, \n2011. \n[38] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: Deep learning on point \nsets for 3d classification and segmentation, Proc. CVPR 2017, pp. \n652–660, 2017. \n[39] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, J. M. \nSolomon, Dynamic Graph CNN for Learning on Point Clouds, ACM \nTransactions on Graphics, 38(5), Article No. 146, 2019. \n[40] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. \nErhan, V. Vanhoucke, A. Rabinovich, Going deeper with \nconvolutions, Proc. CVPR 2015, pp. 1–9, 2015. \n[41] K. He, X. Zhang, S. Ren, J. Sun, Deep Residual Learning for Image \nRecognition, Proc. CVPR 2016, pp. 770–778, 2016. \n \n \n15 \n[42] P. Achlioptas, O. Diamanti, I. Mitliagkas, L. Guibas, Learning \nRepresentations and Generative Models for 3D Point Clouds, Proc. \nICML 2018, pp. 40–49, 2018. \n[43] D. P. Kingma, J. L. Ba, Adam: A method for stochastic optimization, \nProc. ICLR 2015, 2015. \n[44] M. Abadi et al., TensorFlow: Large-scale machine learning on \nheterogeneous systems, Proc. OSDI 2016, pp. 265–283, 2016. \n[45] M. Donoser, H. Bischof, Diffusion Processes for Retrieval Revisited, \nProc. CVPR 2013, pp. 1320–1327, 2013. \n[46] T. Mikolov et al., Distributed representations of words and phrases \nand their compositionality, Proc. NIPS 2013, pp. 3111–3119, 2013. \n[47] J. Johnson et al., Billion-Scale Similarity Search with GPUs, IEEE \nTransactions on Big Data, 7(3), pp. 535–547, 2021. \n[48] M. Kaya, H. S. Bilge, Deep Metric Learning: A Survey, Symmetry, , \n11(9), 1066, 2019. \n[49] F. Cakir et al., Deep Metric Learning to Rank, Proc. CVPR 2019, pp. \n1861–1870, 2019. \n[50] J. Revaud et al., Learning with Average Precision: Training Image \nRetrieval with a Listwise Loss, Proc. ICCV 2019, pp. 5107–5116, \n2019. \n[51] F. Radenović et al., Fine-tuning CNN Image Retrieval with No \nHuman Annotation, TPAMI, 41(7), pp. 1655–1668, 2019. \n[52] A. Iscen et al., Mining on Manifolds: Metric Learning without Labels, \nProc. CVPR 2018, pp. 7642–7651, 2018. \n[53] L. Maaten, G. Hinton, Visualizing data using t-SNE, Journal of \nMachine Learning Research, 9(11), 2008. \n[54] H. O. Song, Y. Xiang, S. Jegelka, S. Savarese, Deep Metric Learning \nvia Lifted Structured Feature Embedding, Proc. CVPR 2016, pp. \n4004-4012, 2016. \n[55] Z. Li, J. Tang, Weakly Supervised Deep Metric Learning for \nCommunity-Contributed Image Retrieval, Transactions on \nMultimedia, 17(11), pp. 1989–1999, 2015. \n[56] Z. Li, J. Tang, Weakly Supervised Deep Matrix Factorization for \nSocial Image Understanding, Transactions on Image Processing, \n26(1), pp. 276–288, 2017. \n[57] Y. Patel, G. Tolias, J. Matas, Recall@k Surrogate Loss with Large \nBatches and Similarity Mixup, Proc. CVPR 2022. \n[58] Y. Li, S. Kan, Z. He, Unsupervised Deep Metric Learning with \nTransformed Attention Consistency and Contrastive Clustering Loss, \nProc. ECCV 2020, pp. 141–157, 2020. \n[59] S. Kim, D. Kim, M. Cho, S, Kwak, Self-Taught Metric Learning \nwithout Labels, Proc. CVPR 2022. \n[60] Y. Tao, K. Takagi, K. Nakata, Clustering-friendly Representation \nLearning via Instance Discrimination and Feature Decorrelation, Proc. \nICLR 2021, 2021. \n[61] C. Fefferman, S. Mitter, H. Narayanan, Testing the Manifold \nHypothesis, Journal of the American Mathematical Society, 29(4), pp. \n983–1049, 2016. \n[62] H. Fan, H. Su, L. J. Guibas, A Point Set Generation Network for 3D \nObject Reconstruction from a Single Image, Proc. CVPR 2017. pp. \n605–613, 2017. \n \n \n",
  "categories": [
    "cs.CV",
    "cs.IR"
  ],
  "published": "2021-12-14",
  "updated": "2022-11-14"
}