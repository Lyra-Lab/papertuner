{
  "id": "http://arxiv.org/abs/2112.07356v2",
  "title": "Technical Language Supervision for Intelligent Fault Diagnosis in Process Industry",
  "authors": [
    "Karl Löwenmark",
    "Cees Taal",
    "Stephan Schnabel",
    "Marcus Liwicki",
    "Fredrik Sandin"
  ],
  "abstract": "In the process industry, condition monitoring systems with automated fault\ndiagnosis methods assist human experts and thereby improve maintenance\nefficiency, process sustainability, and workplace safety. Improving the\nautomated fault diagnosis methods using data and machine learning-based models\nis a central aspect of intelligent fault diagnosis (IFD). A major challenge in\nIFD is to develop realistic datasets with accurate labels needed to train and\nvalidate models, and to transfer models trained with labeled lab data to\nheterogeneous process industry environments. However, fault descriptions and\nwork-orders written by domain experts are increasingly digitised in modern\ncondition monitoring systems, for example in the context of rotating equipment\nmonitoring. Thus, domain-specific knowledge about fault characteristics and\nseverities exists as technical language annotations in industrial datasets.\nFurthermore, recent advances in natural language processing enable weakly\nsupervised model optimisation using natural language annotations, most notably\nin the form of natural language supervision (NLS). This creates a timely\nopportunity to develop technical language supervision (TLS) solutions for IFD\nsystems grounded in industrial data, for example as a complement to\npre-training with lab data to address problems like overfitting and inaccurate\nout-of-sample generalisation. We surveyed the literature and identify a\nconsiderable improvement in the maturity of NLS over the last two years,\nfacilitating applications beyond natural language; a rapid development of weak\nsupervision methods; and transfer learning as a current trend in IFD which can\nbenefit from these developments. Finally we describe a general framework for\nTLS and implement a TLS case study based on SentenceBERT and contrastive\nlearning based zero-shot inference on annotated industry data.",
  "text": "Technical Language Supervision for\nIntelligent Fault Diagnosis in Process Industry\nKarl L¨owenmark1, Cees Taal2, Stephan Schnabel3, Marcus Liwicki4and Fredrik Sandin5\n1,4,5 Embedded Intelligent Systems Laboratory (EISLAB), Lule˚a University of Technology,\n971 87 Lule˚a, Sweden\nkarl.ekstrom@ltu.se marcus.liwicki@ltu.se fredrik.sandin@ltu.se\n2 SKF Research & Technology Development, Meidoornkade 14, 3992 AE Houten,\nP.O. Box 2350, 3430 DT Nieuwegein, The Netherlands\ncees.taal@skf.com\n3 SKF Condition Monitoring Center Lule˚a AB, 977 75 Lule˚a, Sweden\nstephan.schnabel@haw-landshut.de\nABSTRACT\nIn the process industry, condition monitoring systems with\nautomated fault diagnosis methods assist human experts and\nthereby improve maintenance efﬁciency, process sustainabil-\nity, and workplace safety. Improving the automated fault di-\nagnosis methods using data and machine learning-based mod-\nels is a central aspect of intelligent fault diagnosis (IFD). A\nmajor challenge in IFD is to develop realistic datasets with ac-\ncurate labels needed to train and validate models, and to trans-\nfer models trained with labeled lab data to heterogeneous pro-\ncess industry environments. However, fault descriptions and\nwork-orders written by domain experts are increasingly digi-\ntised in modern condition monitoring systems, for example in\nthe context of rotating equipment monitoring. Thus, domain-\nspeciﬁc knowledge about fault characteristics and severities\nexists as technical language annotations in industrial datasets.\nFurthermore, recent advances in natural language processing\nenable weakly supervised model optimisation using natural\nlanguage annotations, most notably in the form of natural\nlanguage supervision (NLS). This creates a timely opportu-\nnity to develop technical language supervision (TLS) solu-\ntions for IFD systems grounded in industrial data, for exam-\nple as a complement to pre-training with lab data to address\nproblems like overﬁtting and inaccurate out-of-sample gen-\neralisation. We surveyed the literature and identify a con-\nsiderable improvement in the maturity of NLS over the last\ntwo years, facilitating applications beyond natural language;\nKarl L¨owenmark et al. This is an open-access article distributed under the\nterms of the Creative Commons Attribution 3.0 United States License, which\npermits unrestricted use, distribution, and reproduction in any medium, pro-\nvided the original author and source are credited.\na rapid development of weak supervision methods; and trans-\nfer learning as a current trend in IFD which can beneﬁt from\nthese developments. Finally we describe a general framework\nfor TLS and implement a TLS case study based on Sentence-\nBERT and contrastive learning based zero-shot inference on\nannotated industry data.\n1. INTRODUCTION\nCondition-monitoring (CM) based fault diagnosis of rotat-\ning machinery (Carden & Fanning, 2004; A. K. Jardine et\nal., 2006) is widely used in industry to optimise equipment\navailability, uniformity of product characteristics and safety\nin the work environment, and to minimise production losses\nand material waste.\nIn process industry, this typically re-\nquires human expert analysts with years of training and de-\ntailed knowledge about the operational states, functional roles\nand contexts of the machines being monitored. Due to grow-\ning demands on production efﬁciency and the vast amounts\nof data consequently generated in modern CM systems, au-\ntomated fault diagnosis systems (Kothamasu et al., 2006) are\nrequired to assist human analysis through alarms and policy\nrecommendation. Important tasks for the automated system\nare fault detection and classiﬁcation to generate alarms and\nﬁlter data, and fault severity estimation to predict remain-\ning useful life and recommend policy options. Existing auto-\nmated systems are mainly based on expert systems (Nan et al.,\n2008), with a knowledge-base derived from physical proper-\nties of analysed components, and a rule-based inference en-\ngine with local thresholds set by experts (SKF, 2022). In the\ncase of vibration measurements of rotating machinery, sig-\nnal processing and kinematics based condition indicators are\ncommonly used as knowledge-bases (A. Jardine et al., 2006;\nInternational Journal of Prognostics and Health Management, ISSN2153-2648, 2022 000\n1\narXiv:2112.07356v2  [cs.AI]  20 Oct 2022\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nTable 1. Fault Diagnosis Tasks\nTask\nQuestion addressed\nOutput\nAdded value\nAutomated?\nDetection\nIs there a fault present?\nYes/No\nAlert human analysis\nYes\nClassiﬁcation\nWhat type of fault?\nClass\nGuide human analysis\nPartially\nSeverity\nHow severe is the fault?\nMagnitude\nMotivate maintenance\nNo\nRUL\nTime until maintenance is needed?\nRisk vs Time\nMaintenance planning\nNo\nRoot Cause\nWhat caused the fault?\nDescription\nPreventive policies\nNo\nRai & Upadhyay, 2016; Randall & Antoni, 2011). Intelli-\ngent fault diagnosis (IFD) (Lei et al., 2020) has been proposed\nto enhance the automated systems by inferring fault charac-\nteristics directly from process or lab data through learning\nbased methods. Improving existing models is vital to meet\nthe increasing demands on CM systems to improve produc-\ntion and equipment life cycle efﬁciency in process industry\n(ProcessIT, 2018; Shin & Jun, 2015), and the machine CM\nmarket is estimated at $2.6 billion with a compound annual\ngrowth rate estimation of 7.1%1. For example, improved IFD\nalgorithms can contribute to: reducing the number of unnec-\nessary interventions; facilitating remanufacturing of compo-\nnents (A. SKF & Kommunikation, 2020); optimising mainte-\nnance schedules; and enabling analysts to focus on qualiﬁed\npreventive tasks.\nHowever, it is difﬁcult to develop realistic datasets with ac-\ncurate labels needed to train and validate IFD models, and\nsuch data are expected to generalise poorly between process-\nindustry plants due to their heterogeneous nature. Recent in-\nnovations in natural language processing offer a timely oppor-\ntunity to address this challenge with methods used in natural\nlanguage supervision (NLS) (F. Chen et al., 2022a) using dig-\nitalised technical language fault descriptions and work-orders\navailable in many process industry datasets. Processing tech-\nnical language poses unique challenges different from natual\nlanguage, promoting the need for Techincal Language Pro-\ncessing (TLP) and a technical version of NLS in Techincal\nLanguage Supervision (TLS). Therefore we survey the state\nof the art in IFD, NLS and TLP, and discuss how TLS can be\napplied to IFD in a process industry context.\n1.1. Background\nFault Diagnosis (FD) deals with the mapping of measured\nsignal features to component conditions. The most basic con-\ndition is whether a fault is present or not, but more complex\nestimations such as fault class, fault severity, remaining use-\nful life (RUL) and root cause analysis (RCA) can also be re-\nquired. Table 1 describes these ﬁve major subtasks of FD, or-\ndered in rising complexity based on interviews with condition\nmonitoring experts from process industry.\nFault detection\nand classiﬁcation are tasks that are frequently automated in\nprocess industry through signal processing (Kothamasu et al.,\n1https://www.marketsandmarkets.com/Market-Reports/\nmachine-health-monitoring-market-29627363.html\n2006; Nan et al., 2008; SKF, 2022), and for example model-\nbased thresholding. Fault severity estimation, a vital tool in\nmaintenance decisions, is next in line to be automated, but\nis challenging due to nonlinear relationships between signal\nfeatures and fault evolution (Cerrada et al., 2018). RUL de-\npends on the evolution of estimated fault severity over time,\nand predicts the remaining time until a fault is so severe that\na component is no longer useful (Lei et al., 2018; D. Wang\net al., 2017). RCA is a complex task that may be challeng-\ning to automate, but will indirectly be improved if simpler\ntasks are automated and human experts can invest more time\nin preventive policies.\nThe upper part of Figure 1 illustrates an example of a typi-\ncal FD system (labeled Pipeline 1) implemented in process\nindustry, see for instance (SKF, 2022; PdM, 2021; Cahill,\n2021).\nThe system requires no fault history data to learn\nfrom, but requires process information and kinematic models\nfor the extraction of condition indicators (Sharma & Parey,\n2016). Faults are detected and classiﬁed using signal pro-\ncessing (A. K. Jardine et al., 2006), for instance root mean\nsquare, peak-to-peak and time synchronous average in the\ntime domain (chung Fu, 2011); spectral density, enveloping\nand Hilbert transform in the frequency domain; and dictio-\nnaries, wavelets and the Wigner-Ville distribution in the time-\nfrequency domain; as well as kinematics based condition in-\ndicators, for instance the frequency intensity in the ball pass\nfrequency of the outer race in ballpoint bearings. The de-\ncomposed signal is then analysed with typically simple rules\nbased on indicator magnitude deﬁned by experienced ana-\nlysts. Once a fault is detected by the model, a human analyst\nis alerted for in-depth diagnosis. The analyst decides whether\nto further investigate alarms or not, describes eventual faults\nin the form of natural-language annotations and makes work\norders. Thus, the automated FD model acts like a ﬁlter be-\ntween the massive amount of sensor data that is constantly\ngenerated, and the accurate but resource-constrained analysis\nof human experts. Based on cases from two industry collab-\norations with major process industry actors in Northern Swe-\nden2, analysts monitor around 5000 alarms per analyst per\nyear, after ﬁltering, where at most 20% of generated alarms\npoint to component faults and the rest are due to temporary\nor constant signal malfunctions.\n2Smurﬁt Kappa, 700 000 tonnes of Kraftliner per year, and SCA Munksund,\n400 000 tonnes of Kraftliner per year\n2\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nLabelled Lab Data with Artificial\nor Accelerated Faults\nUnlabelled CM Sensor Data + \nProcess Data\nData\nModelling\nDecision\nProcess Industry Fault Diagnosis Pipeline (1)\nTransfer Learning Fault Diagnosis Pipeline (2)\nTechnical Language Supervision Pipeline(3)\nUnlabelled CM Sensor Data + \nProcess Data + Annotations\nSignal Processing + \nCondition Indicators\nHuman Analyst -> Work Orders \nand Annotations\nAlarms, Decision Support \nand Work Orders\nTechnical Language Processing \n+ Fault Diagnosis mapping\nTransfer Learning from \nLab to Industry Data\nAlarms, Decision Support, \nWork Orders and Annotations\nFigure 1. An overview of a typical process industry fault diagnosis pipeline (1), possible transfer learning IFD pipeline additions\n(2), and our suggested natural language supervision pipeline (3). Both (2) and (3) can provide considerable contributions to (1),\nwith the strongest contributions coming from both pipelines implemented in symbiosis.\nWith improved automated FD, analysts could focus on more\nadvanced fault diagnosis tasks beyond the current capabilities\nof IFD. Considerable research has been invested in automated\nFD, and many learning-based methods have shown promising\nresults on test datasets (R. Liu et al., 2018; Stetco et al., 2019;\nHoang & Kang, 2019). However, the accurate deep learning\nmodels used in many IFD publications require vast amounts\nof training data in the form of labelled datasets, sets that typ-\nically do not exist in process industry cases (Khan & Yairi,\n2018). Instead, training and test datasets are created in lab\nenvironments with artiﬁcial or accelerated fault development,\nsuch as the Case Western Reserve University bearing dataset\n(Case Western Reserve University Bearing Data Center Web-\nsite, n.d.), the Intelligent Maintenance System (IMS) by the\nUniversity of Cincinnati dataset (NASA prognostic data repos-\nitory, n.d.), and the Machinery Failure Prevention Technology\n(MFPT) dataset (Condition Based Maintenance Fault Database\nfor Testing of Diagnostic and Prognostics Algorithms, n.d.),\nbut typically generalise poorly to heterogeneous environments\n(Smith & Randall, 2015; S. Zhang et al., 2019) such as pro-\ncess industries. Thus, despite the maturity of IFD methods in\nterms of literature, supervised IFD lacks wide-spread imple-\nmentation in industry.\nIndustry datasets suitable for IFD can in some cases poten-\ntially be created, but it is difﬁcult and costly to deﬁne high-\nquality labels that are accurately connected to relevant data.\nTherefore, transfer learning (Schwendemann et al., 2021), il-\nlustrated in Pipeline 2 in Figure 1, has become an increasingly\npopular approach to develop IFD methods without requiring\na large labelled dataset in the target domain (Lei et al., 2020).\nIdeally, a model could be developed/trained with data from a\nlab environment, then transferred to similar components in an\nindustrial environment. However, this remains a challenging\ngoal due to differences between developing faults, heteroge-\nneous environments, varying sensor and signal-to-noise con-\nditions, and complex coupling of signal components. Thus,\na method for the extraction of labels for industry data would\nbe valuable and can facilitate implementations of current IFD\n3\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nmodels, as well as transfer learning by providing access to\nlabels in the target domain.\nWhile labels are lacking in realistic CM datasets, technical\nlanguage fault descriptions are written by analysts when doc-\numenting and monitoring the development of for example\nbearing faults over long periods of time (several months).\nThus, the text-annotations produced as outputs of Pipeline 1\nin Figure 1 contain valuable albeit noisy information about\nfault development characteristics and severities. This moti-\nvates the question, can such domain-speciﬁc annotations and\nrelated knowledge be used for training and ﬁne-tuning of IFD\nmethods as a substitute for regular labels?\nLanguage has been used to train machine learning models for\nimage recognition and object detection through recent break-\nthroughs in natural language supervision (Radford et al., 2021a;\nRamesh et al., 2021). Can a similar approach be used to train\nIFD models on industry data using annotations and work or-\nders as zero-shot labels?\n1.2. Contribution\nWe propose the usage of TLS on technical language fault\ndescriptions to overcome the lack of labels in industry CM\ndatasets. TLS is grounded in three ﬁelds, IFD, TLP and NLS,\nand we brieﬂy survey all three to motivate the purpose and\nbeneﬁts of TLS. Potential TLS contributions supervision are\nimproved support for human analysts and automation of sim-\npler tasks by augmenting the label domain for transfer learn-\ning or zero-shot learning.\nPipeline 3 in Figure 1 illustrates the concept of a technical\nlanguage supervision framework for process industry data.\nUnlabelled CM sensor data and process data are used to ex-\ntract features through methods already used in IFD models,\nand the features are mapped to annotation embeddings. In\nthe implementation stage, an unannotated signal is thereby\nmapped to the closest language fault queires in the joint em-\nbedding space, and with a sufﬁciently good model and well\nchosen queries, the fault class and severity can be estimated\nand described. Besides alarms and work orders, a language\nbased model could also retrieve spectra from queries and gen-\nerate new annotations and descriptions of detected faults.We\nimplement a TLS model based on process industry signals\nand annotations, and show an example of spectrum retrieval\nfrom free form queries, as well as zero-shot fault classiﬁca-\ntion of spectra.\n1.3. Research Trends\nWe also surveyed the fault diagnosis literature and recent pub-\nlications on language-based learning in the context of natu-\nral language supervision and image captioning to identify the\ntrends of publications that combine these concepts. Figure 2\nshows the number of published articles per year according\n1950\n1960\n1970\n1980\n1990\n2000\n2010\n2020\nPublication year\n0\n1\n2\n3\n4\nLog of number of publications\nNLP\nFD\nFD + ML\nImage Captioning\nFD + Transfer Learning\nWO + NLP\nFD + Weak Supervision\nFigure 2. Trends of publications between 1967 and 2020, ob-\ntained through Scopus queries looking for publications with\nthe targeted keywords in the article title, the abstract or the\nkeywords. For instance, a query for fault diagnosis related\nkeywords and transfer learning is designed as follows: (”con-\ndition monitoring” OR ”fault diagnosis” OR ”fault classiﬁ-\ncation” OR ”fault detection” ) AND ”transfer learning” The\nannual number of articles about the application of machine\nlearning (ML) to condition monitoring (CM) and fault di-\nagnosis (FD) increases exponentially. That is also the case\nfor the annual number of natural language processing (NLP)\narticles, which now equates the total annual number of FD-\nrelated articles. A total of 15 articles that use NLP on work\norders (WO) were found, but no implementations of natural\nlanguage supervision on fault diagnosis problems were iden-\ntiﬁed. Weak supervision, or weakly supervised learning, is\nalso not yet commonly used, with 4 articles in 2020 and 5\narticles so far in 2021.\nto Scopus for search queries containing keywords related to\nfault diagnosis and machine learning. We present the publi-\ncation trends of natural language processing (NLP), fault di-\nagnosis (FD), fault diagnosis with machine learning (FD +\nML), fault diagnosis with transfer learning (FD + Transfer\nLearning), image captioning, work orders with natural lan-\nguage processing (WO + NLP), and ﬁnally fault diagnosis\nwith weak supervision. For fault diagnosis, a query includ-\ning ”condition monitoring” OR ”fault diagnosis” OR ”fault\ndetection” OR ”fault classiﬁcation” was used. For machine\nlearning, ”machine learning” OR ”data driven” OR ”deep learn-\ning” OR ”artiﬁcial intelligence” were used. The queries ”trans-\nfer learning”, ”work order”, ”natural language processing”\nand ”image captioning” were used explicitly as is. Weak su-\npervision was queried as ”weak supervision” OR ”weakly su-\npervised”.\nThe trends show that machine learning is increasingly applied\nin the FD literature, and that transfer learning has become\nincreasingly popular, going from 2 publications in 2016 to\n178 publications in 2020. NLP is a rapidly evolving ﬁeld of\nresearch, with signiﬁcant practical advancements in the last\ndecade. This is also reﬂected in the swift growth of image\ncaptioning publications starting in 2015, increasing from 11\n4\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nTable 2. Fault Diagnosis Model Frameworks\nFramework\nData requirements\nChallenges\nUnsupervised Learning\nCM data\nApplications beyond fault detection\nSupervised Learning\nLabelled CM dataset\nLack of labelled industry data\nTransfer Learning\nLabelled lab dataset, CM data\nLab features different from industry features\nWeak Supervision\nWeakly Labelled CM dataset\nStill requires labels\nLanguage Based\nCM dataset, CM annotations\nNot yet applied in IFD\nto 322 publications in four years. 15 publications that use nat-\nural language processing with work orders were found, but\nNLP was employed for information retrieval, and no publica-\ntions combining natural language supervision with IFD were\nfound. Weak supervision only appeared nine times (with one\nvalid article scheduled for 2022 not counted) in our queries,\nbut notably three articles were cited more than ten times;\nX. Li, Zhang, et al. (2020) with 64 and 30 (X. Li, Li, & Ma,\n2020) citations, and Yu, Fu, et al. (2021) with 12 , showing\nthat the interest far outweighs the current publication number.\nArticles citing weak supervision articles were mainly focused\non transfer learning, but we predict an increase in direct men-\ntions of weak supervision methods.\n1.4. Outline of article\nIn Section II, we describe the application of FD in process in-\ndustry, which is subject to constraints related to the high cost\nof unplanned stops that can affect the whole production pro-\ncess. Five principal FD tasks are described, and the related\nmethods and algorithms used for automated FD are also pre-\nsented. In section III we brieﬂy review natural language su-\npervision and related ﬁelds such as image captioning, and dis-\ncuss how natural language can be integrated in an IFD frame-\nwork. Section IV describes a case study implementation of a\nTLS solution for IFD based on theories from section III, using\nprocess industry data for training and illustrations of model\nperformance. We focus on rotating machinery in process in-\ndustry, but in principle the framework of technical language\nsupervision is expected to generalise to other fault diagnosis\napplications where fault descriptions are also present.\n2. DEEP LEARNING IN INTELLIGENT FAULT DIAGNO-\nSIS\nTable 2 summarises different data-driven methods used for\nIFD, besides the kinematic rule-based method already dis-\ncussed in the background. The methods are ordered roughly\nby maturity and data requirements. Unsupervised learning\napplies directly to unlabelled CM data, and it is partially im-\nplemented in process industries (SKF, n.d.; Monitron, n.d.;\nSimon, n.d.; Emerson, 2021). Supervised learning requires a\nlabelled dataset in the application environment, and is widely\ninvestigated in the literature (Yin et al., 2014; Khan & Yairi,\n2018; R. Liu et al., 2018; Helbing & Ritter, 2018; Stetco et\nal., 2019; Zhang et al., 2020), but not in process industry.\nTransfer learning requires a labelled dataset for pre-training,\nand data from the application environment, ideally labelled,\nfor ﬁne-tuning. The number of articles on transfer learning\nhas increased rapidly in the last decade, but although transfer\nbetween lab environments show great results, we ﬁnd no arti-\ncles that apply transfer learning methods directly on process\nindustry data. Finally, natural language supervision based\nlearning only requires unlabelled CM data with associated\nannotations, but this method remains to be adapted and in-\nvestigated for fault diagnosis tasks. The ﬁrst mentions of nat-\nural language processing for in an IFD context was 2020 in\nthe name of ”technical language processing”, though natural\nlanguage supervision is yet to be introduced to IFD.\n2.1. Unsupervised Learning\nUnsupervised learning, i.e learning patterns without labels, is\nconnected to the modelling module of Figure 1 and is pri-\nmarily used for clustering, encoding, feature extraction and\nanomaly detection fault detection (Lei et al., 2016). Models\ncommonly used for clustering are k-means, Principal Com-\nponent Analysis (PCA) and t-distributed Stochastic Neigh-\nbor Embedding (t-SNE). Auto-Encoders and variational auto-\nencoders (Jiang et al., 2018; Haidong et al., 2018) and Dic-\ntionary Learning (Papyan et al., 2018; H. Liu et al., 2011)\nare commonly used for Encodings and Anomaly Detection.\nVirtually all models can be used to reduce dimensionality\nand extract features depending on the data, with PCA and\nt-SNE being more direct dimensionality reductions and auto-\nencoders serving as a more complex reconstruction model,\noften with encoders/decoders based on convolutions, recur-\nrence or transformers.\nClustering, encodings and feature extraction can be valuable\nways of understanding, simplifying or visualising data. A\nCM dataset with healthy and unhealthy data can with the right\nmethods and data be divisible in to two clusters, which can\nthen be manually labelled healthy and unhealthy, thus detect-\ning faults (Yiakopoulos et al., 2011). Likewise, encodings or\nextracted features can serve as values in a simple rule-based\nsystem for fault detection or classiﬁcation, and extracted fea-\ntures in particular can give valuable insight in feature impor-\ntance. Regardless, the lack of a supervision signal necessi-\ntates a human in the last step to validate or assign meaning\nto clusters, encodings or features, before the model is ready\nto automatically detect faults. Anomaly detection can work\n5\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nmore autonomously by learning the healthy state of a signal,\nthen classifying deviations from this state as detected faults\n(del Campo & Sandin, 2017) or into fault classes (C. Lu et\nal., 2017).\nHowever, healthy states that lack sufﬁcient presence in the\ntraining set has a risk of being classiﬁed as unhealthy at de-\nployment, and unhealthy states that are present during train-\ning might be considered healthy, which is difﬁcult to detect\ndue to the lack of labels in the dataset. Furthermore, devia-\ntions might occur due to healthy states, or in directions rel-\natively orthogonal to previous deviations. Such issues fall\nwithin the scope of zero-shot learning, wherein a model is\nrequired to observe and predict samples from a previously\nunseen class or distribution.(T. Zhang et al., 2021). For zero-\nshot learning to work, there has to be a distinct characteris-\ntic of faults and healthy states that is true for previously un-\nseen faults or healthy states, which can be leveraged to assign\nthese distributions to the correct class. NLS is sometimes dis-\ncussed in the scope of zero-shot learning, and zero-shot learn-\ning techniques are often used in NLS. Likewise, zero-shot\nlearning can be used to augment supervised learning methods\nbeyond classes present in the supervision signal, but it is best\ndescribed under the umbrella term of unsupervised learning\nor through the lens of weak supervision, as discussed in Sec-\ntion 2.4.\n2.2. Supervised Learning\nSupervised Learning can be employed for any FD task, as\nlong as sufﬁcient data and good labels are present. Transfer\nLearning, Weak Supervision and Language Supervision are\nall arguably subgroups of supervised learning explicitly de-\nsigned to circumvent the limitation of requiring good labels.\nArchitectures used in supervised learning are thus also em-\nployed in its derivatives, though with different learning pro-\ncedures, just as how for instance auto-encoders from unsu-\npervised learning can be used together with an output layer in\na supervised paradigm.\nSupervised learning architectures used in IFD range from shal-\nlow models such as tree-based models, e.g. random forest\n(D. Zhang et al., 2018); support vector machines (Yin et al.,\n2014; Qin, 2012); probabilistic models such as Bayesian statis-\ntics (Stief et al., 2019; H. Zhang et al., 2018); and deep ar-\nchitectures such as fully connected feed forward deep neural\nnetworks (F. Jia et al., 2016); (variational) auto-encoders with\nclassiﬁcation layers (Yan et al., 2021; Haidong et al., 2018);\nconvolutional neural networks (F. Jia et al., 2018; Pan et al.,\n2018), commonly used in image analysis; recurrent neural\nnetworks (H. Liu et al., 2018; Qiao et al., 2020; X. Chen et al.,\n2021), commonly used in language analysis but applicable on\nsequential data in general. Importantly, supervised learning\nhas been employed for fault severity estimation (Cerrada et\nal., 2018) and RUL prediction (Babu et al., 2016; X. Li et al.,\n2018; Ben Ali et al., 2015; Guo et al., 2017; Lei et al., 2018;\nD. Wang et al., 2017).\nLabelling industry datasets for supervised learning can facili-\ntate implementations in that industry environment, but the la-\nbelling process is costly, and requires analyst efforts. Further-\nmore, some faults have stochastic features, for example due\nto the varying nature of the source geometry or signal transfer\nfunction, and are thus difﬁcult to generalise with supervised\nclassiﬁers. In general, faults are undesirable and therefore rel-\natively scarce in industrial datasets, but are required in train-\ning datasets for supervised learning. Consequently, produc-\ning a labelled industry dataset for supervised learning would\nrequire considerable resources and potentially occupy ana-\nlyst time necessary for condition monitoring. Therefore, fault\nclassiﬁcation models described in the literature are typically\ntrained on labelled data from lab environments, where faults\nare generally either artiﬁcially induced or provoked through\nintense loads, as it might take several years until faults de-\nvelop naturally.\nThe development of the fault is then ac-\ncelerated by e.g high loads or starved lubrication, which in-\ncrease fault development per revolution, and high speeds to\nincrease revolutions per minute (RPM). High RPM also pro-\nduce higher signal-to-noise ratios as some noise is stationary\nand fault features increase more in magnitude than noise fea-\ntures.\nIdeally, a model supervised on a component in a lab environ-\nment would then be deployable in an industry environment,\nbut there are two issues that makes this difﬁcult. Firstly, arti-\nﬁcial or accelerated fault developments result in fault charac-\nteristics that are different compared to faults in industry envi-\nronments. Therefore, the decision boundaries do not neces-\nsarily generalise well from lab to industry environments, and\nthe feature space can differ due to different fault development\nprocesses. Secondly, signals generated in a lab setting differ\ngreatly from signals in an industry environment where a com-\nponent is connected to several other components in a larger\nsystem, and signal components are combined and masked by\nnoise. The signal to noise-ratio will consequently be lower in\nthe industry environment, and the coupling with surrounding\ncomponents can shift the true feature space as well. Thus, di-\nrect supervised learning works best in the environment where\nit has been trained, and generalisation can be difﬁcult unless\nlabels are preserved in the target space.\n2.3. Transfer Learning\nRecently, the research focus in IFD has shifted to include\nmethods to overcome the lack of labels in industry datasets\nsuch as transer learning and weak supervision.\nTransfer learning seeks to develop methods for training of a\nmodel in one environment, then ﬁne-tuning the feature space\nand decision boundaries to suit implementation in another en-\nvironment (C. Li et al., 2020). In situations with sparse data\n6\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\noptimisation limits, transfer learning can use domains with\nrich data, such as lab datasets, to infer necessary knowledge\n(Q. Zhang et al., 2021). The research on transfer learning in\nfault diagnosis applications has increased rapidly over the last\nfew years, with many successful transfers between different\nlab datasets (Lei et al., 2020). As models improve, transfer\nlearning can enable broader implementation of these models\nin process industry with a lower demand for labeled instances\ncompared to supervised learning (Cao et al., 2018a), while\nsolving the same tasks.\nMethods used in transfer learning vary; many publications\nuse transferrable convolutional neural networks (Cao et al.,\n2018b; Shao et al., 2019; B. Yang et al., 2019; Guo et al.,\n2019; Zhong et al., 2019; Xu et al., 2020; T. Han et al., 2020;\nZ. He et al., 2020; Wen et al., 2020; Z. Chen et al., 2020;\nShao et al., 2021), occasionally employed with adversarial\nnetworks (Q. Wang et al., 2019; T. Han et al., 2019; X. Li et\nal., 2020); some use recurrent neural networks (A. Zhang et\nal., 2018; An et al., 2019; Zhao et al., 2020); auto-encoders\nare also used (Wen et al., 2019), and recently weak supervi-\nsion (Li et al., 2020) and digital twin-based transfer learning\n(Xu et al., 2019) have been successfully implemented.\nTransferring knowledge from one environment to another adds\nan additional beneﬁt to symbol-feature relation graphs be-\nsides illustrating the process of the reasoning module. Hu-\nmans learn concepts in a highly transferable manner, and it is\nfor instance highly feasible that an experienced analyst could\ndiagnose faults in a previously unseen environment with good\naccuracy, while a learning based model would certainly fail at\nadapting unless optimised through transfer learning. The un-\nderlying concepts of fault developments are likely the same\nin both environments, which is what humans use to gener-\nalise knowledge. Optimizing not only direct mappings, but\nsymbol-feature relation graphs as well, can thus create mod-\nels with stronger generalisability by mimicking human knowl-\nedge (Y. Li et al., 2020).\n2.4. Weak Supervision\nWeak supervision is an umbrella term for a set of methods\ndeveloped to perform supervised tasks on data where labels\nare insufﬁcient for regular supervised learning (Z.-H. Zhou,\n2017). It can work in conjunction with transfer learning to\nenhance ﬁne-tuning on the target dataset, or stand-alone to\nfacilitate direct optimisation in the target environment. Table\n3 illustrates three major ways in which labels can be insufﬁ-\ncient, the cause, and proposed methods to amend the issue.\n2.4.1. Incomplete supervision\nIncomplete labels are characterised by a dataset where most\ndata points are unlabelled. In a CM dataset, faults that have\nnot been discovered yet are a cause for incompleteness, as\nthis prevents the assumption that all unlabelled data is healthy\ndata.\nThe main strategy for dealing with incomplete datasets is called\nsemi-supervised learning (van Engelen & Hoos, 2020; Zhai\net al., 2019; Jian et al., 2021), which aims to create clusters\nof features that correspond to the available labels, and to es-\ntimate the probability that an unseen feature belongs to one\nof the identiﬁed clusters. Semi-supervised learning has been\nemployed in IFD settings on lab datasets with partial (Razavi-\nFar et al., 2019) or limited labels (Yu, Lin, et al., 2021). By\nimplementing semi-supervised learning on a CM dataset with\nnatural language supervision, it is possible to include all time\nseries data for a prediction, where particularly noisy samples\nwould be less likely to affect the model optimisation process,\nas they are likely distributed far away from the cluster cen-\ntres. The diagnosis of faults in unlabelled samples also be-\nlong to the domain of semi-supervised learning, albeit with\nthe additional challenge associated with many unique com-\nponents and features. This challenge can necessitate active\nlearning (Aghdam et al., 2019; Jian et al., 2021), in which\na model identiﬁes selected unlabelled datapoints and alerts a\nhuman expert to label them. Active learning requires human\nintervention, but aims to make use of human efforts as efﬁ-\nciently as possible to improve the model accuracy (Q. Zhang\net al., 2021). Another scheme used to overcome incomplete\nlabels is few-shot learning (Y. Wang et al., 2020), where a\nmodel is optimised to perform supervision tasks with insuf-\nﬁcient data for normal supervision training (D. Zhou et al.,\n2018; A. Zhang et al., 2019; Ren et al., 2020). Few-shot learn-\ning provides an interesting opportunity to learn fault features\nwith only a few instances in a training datasets, as can be the\ncase for many rare faults or components. In the case where no\nlabels exist, supervision algorithms might still be applicable\nthrough zero-shot learning (T. Zhang et al., 2021). In zero-\nshot learning, the model seeks to generalise knowledge from\nseen classes to unseen classes with similar behaviour, much\nlike how humans can see images of house-cats and dogs and\nthen correctly categorise lions to felines and wolves to ca-\nnines (Gao et al., 2020; Feng & Zhao, 2021).\n2.4.2. Inexact supervision\nInexact labels coarsely describe some aspects of the ground\ntruth for a set of features, but do not accurately deﬁne it. In\ngeneral, symbols like labels can not fully represent physical\nprocesses of unknown dimensions. Instead, labels deﬁne se-\nmantics at a certain level of approximation and scale. Thus,\nlabels of physical processes are by nature incomplete seman-\ntical descriptions of reality. CM annotations do not describe\nthe properties of each recording in a faulty component, only\nthat from a large bag of recording a fault has been diagnosed.\nThe fault features from each recording were likely not equally\nimportant for the diagnosis however, and learning which fea-\ntures imply which diagnoses would be easier if each record-\ning had its own label.\n7\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nTable 3. Different weak supervision challenges, causes and solutions\nWeak supervision group\nCause\nSolution\nIncomplete labels\nMissing labels from datapoints\nActive learning\nSemi-supervised learning\nFew-shot learning\nZero-shot learning\nInexact labels\nMultiple datapoints per label\nMulti-instance learning\nContrastive learning\nInaccurate labels\nLabel is wrong\nRegularisation\nRe-labelling\nThe challenges of inexact labels have been proposed to be\novercome through multi-instance learning (Dietterich et al.,\n1997; Hoffmann et al., 2011; Zeng et al., 2015) and con-\ntrastive learning. In multi-instance learning, the optimisation\nalgorithms seeks to ﬁnd the common denominators in the la-\nbel ”bags” that are present for learning. By learning from\nwhich components were replaced and which were not, corre-\nlations in underlying features such as fault severity or deteri-\noration speed can be associated as parts of the bag and used\nfor predictions.\n2.4.3. Inaccurate supervision\nInaccurate labels occur when analysts make fault diagnosis\nmistakes. This is unlikely to occur with fault classiﬁcation,\nbut possible with fault severity due to the higher complexity\nof that task. An analyst can for example assume that a fault\nmay be severe and order a replacement of the component to\navoid failure, while the fault actually is minor.\nInaccurate labels are characterised by not conforming to the\nground truth, in other words being wrong. To learn with noisy\nor inaccurate labels, a model seeks to identify and potentially\ncorrect incorrect labels (Tanaka et al., 2018). Thus, the model\nmaintains some trust in its predictions, capable of deeming\nthe label inaccurate when conﬁdence in prediction is high and\nlabel features deviate from similar labels (J. Li et al., 2019).\nThis trust can be reinforced with physics induced machine\nlearning to maintain a baseline estimate of how labels and\nsignals should correlate, based on physical knowledge of the\nproblem.\n3. TECHNICAL LANGUAGE SUPERVISION\nThe direction of research in IFD points towards ﬁnding ways\nto transfer the success on lab datasets to successful applica-\ntions on industry datasets (Lei et al., 2020; Fink et al., 2020).\nBoth transfer learning and weak supervision can create the\nopportunity to implement successful algorithms on new data-\nsets without requiring an expensive labelling process.\nIn-\nspired by recent innovations in TLP and NLS, TLS present a\nthird, yet unused direction to integrate the annotations present\nin CM datasets as labels, learning directly from technical lan-\nguage.\nThe potential effects of TLS can be summarised as\n•\nOpportunities\n–\nFacilitates direct optimisation on heterogeneous in-\ndustry data\n–\nMethods are available and developed in other re-\nsearch areas\n–\nLanguage data is commonly associated with condi-\ntion monitoring data-bases\n•\nChallenges\n–\nLanguage annotations are uncertain, and require tech-\nnical language processing and weak supervision tech-\nniques to use\n–\nProcessing of technical language jointly with indus-\ntry signals is a novel area of research yet to be de-\nveloped\n–\nRapid progress requires open industry datasets con-\ntaining potentially sensitive information\nIn this section, we brieﬂy describe the state of TLP and NLS,\nthen combine these into an outline of how TLP can be imple-\nmented.\n3.1. Natural Language Supervision\nNatural Language Supervision (F. Chen et al., 2022b) is a re-\ncent term introduced to describe machine learning optimisa-\ntion based on free-form text descriptions rather than prede-\nﬁned labels, though language has been used in a similar fash-\nion to labels before. Labutov et al. (2019) trained semantic\nparsers that interpret questions and feedback from user natu-\nral language responses. Hancock et al. (2018), used natural\nlanguage explanations of human labelling decision to create\nBabbleLabble, which converts explanations to noisy labels\nthrough a semantic parser. Murty et al. (2020) introduced\nExpBERT, which is a BERT variation that forms representa-\ntions using BERT with natural language explanations of the\ninputs.\nText-encoding is a crucial part of NLP and has seen rapid\ndevelopment recent years. Language models (Peters et al.,\n2018) based on the transformer have increased the represen-\ntational powers of text encoders drastically (Radford, 2018a;\n8\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nDevlin et al., 2019; Radford et al., 2019a; Z. Yang et al., 2020;\nMicrosoft, 2020; Brown et al., 2020a). Early examples of\ntext-image pairings used simpler encoding methods, such as\nBag of Words(BoW) and TF-IDF, or recursive encodings de-\nrived from the word2vec model (Mikolov et al., 2013), the\npredecessor of current transformer-based language models.\nThe choice of text-encoder depends on data size and com-\nputational power; a larger model can produce better repre-\nsentations, but requires more data and computational power\nto train. Pre-trained language models with general natural\nlanguage representational capacity, such as BERT (Devlin et\nal., 2019), have successfully been ﬁne-tuned on speciﬁc tasks\nwith signiﬁcantly smaller datasets, based on the assumption\nthat the target language and source language has similar un-\nderlying distributions.\nOptimizing mappings between natural language and images\nhas been done before natural language supervision was intro-\nduced; for example, image captioning (Zakir Hossain et al.,\n2019; X. Lu et al., 2018; S. He et al., 2020) and visual ques-\ntion answering (Antol et al., 2015) have both trained map-\npings between images and text through top-down or bottom-\nup mappings (Anderson et al., 2018) and semantic attention\n(Zhang et al., 2019; Ding et al., 2020). Knowledge and con-\ncepts can also be integrated using language as a supervision\ntool through neuro-symbolic concept learning (Mao et al.,\n2019), where visual concepts, word representations, and se-\nmantic parsing of sentences are jointly learned.\nImage recognition generally uses image-text pairs available\nfrom online data crawling to train mappings between text and\nimages. Learning directly from the text can also facilitate\nzero-shot classiﬁers from language descriptions. Elhoseiny et\nal. (2013) used text-based descriptions to create a zero-shot\nimage classiﬁer, with text features extracted through Term\nfrequency–Inverse document frequency (Tf-Idf) followed by\nClustered Latent Semantic Indexing. J. Lu et al. (2019) intro-\nduced ViLBERT, a Vision-and-Language version of BERT,\nthat learns image recognition and language understanding in\na two-stream model with interactions between image and text\nto improve performance compared to single-stream models.\nY. Zhang et al. (2020) classiﬁed medical images by utiliz-\ning text-image pairs through contrastive visual representa-\ntion learning (ConVIRT) to learn pairings between images\nand texts. Desai & Johnson (2020) introduced Virtex, which\nuses captions to enhance pre-training of an image recognition\nCNN. Sariyildiz et al. (2020) mask words in image-annotation\npairs to create image-conditioned masked language modelling\n(ICMLM) for image classiﬁcation.\nIn a recent publication, Radford et al. (2021a) at OpenAI\npresented CLIP, Contrastive Language–Image Pre-training,\nwhich popularised the term natural language supervision and\nshowed its efﬁcacy for zero-shot classiﬁcation. They used\ntransformers (Vaswani et al., 2017) for both text and image\nencodings (Dosovitskiy et al., 2020), and a contrastive (Tian\net al., 2020) BoW prediction objective to connect text la-\nbel to image features in a vector quantised encoding space\n(van den Oord et al., 2018; Razavi et al., 2019). FILIP by\nYao et al. (2021) uses a ﬁne-grained word-patch image align-\nment to detect and classify objects based on text descriptions,\nobtaining ﬁner level-alignment in image-text comprehension\nthrough unsupervised natural language supervision. C. Jia\net al. (2021) scaled natural language supervision further by\ntraining directly on un-ﬁltered images and annotaions with\nover one billion image-text pairs. Z. Wang, Yu, Firat, & Cao\n(2021) introduced unsupervised data generation to synthesise\nlabels for downstream tasks and thus achieve SOTA results\non SuperGLUE (A. Wang et al., 2020).\nIn earlier models, Ramanathan et al. (2013) used natural lan-\nguage supervision to train a video event understanding model\nin 2013 through a rule-based BoW-like model, and Williams\net al. (2018) used language as reward functions for training\nrobots.\n3.2. Technical Language Processing\nThe term Technical language processing was introduced in\nDec 2020 by Brundage, Sexton, et al. (2021) in collabora-\ntion with the American National Institute of Standards and\nTechnology, and concerns the application of NLP techniques\nand pipelines on technical language. The processing of tech-\nnical language requires natural language processing methods\nwith additional considerations related to the characteristics of\ntechnical language, which is characterised by a higher fre-\nquency of information-rich key-words, more abbreviations,\nand considerably less data than natural language. TLP can be\nused as a basis for TLS, but can also directly enhance CM\npractices by offering insights into key performance indicators\nfrom work order features (Sharp et al., 2021).\nThe challenges inherent in using free form text data from in-\ndustrial contexts - namely data scarcity, a high density of im-\nportant but (to the model) undeﬁned abbreviations, and tech-\nnical terms and concepts critical for maintenance context but\nnot inherently deﬁned by their context - are different enough\nfrom current NLP research to warrant its own key word in\nTLP. An ideal TLP model which performs as well as mod-\nern language models do on natural language would be able to\nanswer free-form questions on the text dataset, understand-\ning what parts of MWOs and annotations indicate fault class,\nseverity or maintenance actions, and similar tasks currently\nonly possible with human analysis. However, the aforemen-\ntioned challenges make direct implementation of pre-trained\nlanguage models difﬁcult; Dima et al. (2021) describe the\nchallenges in adapting natural language processing for tech-\nnical text in detail. and warn against possible shortcomings\nof implementing SOTA NLP models without considering the\nspeciﬁc needs of the process or the people involved. A large\n9\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nTable 4. Annotations associated with data from Figure 3.\nCase\nMonths after fault detection\nAnnotation (translated from Swedish)\nBPFO indication\n4\nBPFO Env low\nBPFO\n10\nBPFO visible in mm/s as overtones\nhigh up in the spectrum between\n1000 and 2000 Hz. WO written on BPFO\nFeedback\n12\nBearing replaced YYYYMMDD\nlevels of BPFO low again\ngE PtP\na)\ngE PtP\nFrequency [order]\nc)\ngE PtP\nb)\nFigure 3. Order analysis results for a vibration signal at a) 4\nmonths; b) 10 months; and c) 12 months after the ﬁrst in-\ndication of a fault in a drying cylinder bearing of a paper\nmachine. Included are also the corresponding text annota-\ntions written by experienced condition monitoring analysts\nemployed at the factory. The annotations have been translated\nfrom Swedish to English to improve clarity. BPFO peaks are\nclearly visible in panel a) four months after the ﬁrst indica-\ntion of the bearing fault. After ten months, the amplitude of\nthe BPFO peaks in panel b) have increased and a work order\n(WO) has been written by the analysts. Two months later the\nbearing has been replaced and no BPFO signature can be seen\nin panel c).\nblack box model can lead to issues with model justiﬁability,\nscrutiny and bias, undermining conﬁdence in the system.\n3.2.1. Technical Language Processing Implementations\nImplementations of word embedding models, among those\nlanguage models, have seen some testing.\nNandyala et al. (2021), implemented ﬁve models for vector\nrepresentation of technical text using an open source dataset\ndescribing 5,485 work orders for 5 excavators Hodkiewicz et\nal. (2017). To evaluate their results they relied on qualitative\nhuman evaluation in word and sentence similarities, as well\nas word cluster projections, as no obvious extrinsic evaluation\ntasks are available in the model. The authors also survey the\nliterature on ﬁelds with challenges similar to those faces in\ntechnical language, and discovered similar problem formula-\ntions in ﬁnance, law, medicine and bio-medicine. In particu-\nlar, the bio-medical community has developed public datasets\nfor training and benchmarking of domain-speciﬁc NLP mod-\nels.\nCadavid et al. (2020) used a French version of RoBERTa\n(Y. Liu et al., 2019) called CamemBERT to estimate language\nfeatures such as duration and criticality of maintenance prob-\nlems based on operator descriptions. They used equipment\ndescriptions, importance and symptoms as input, and type of\ndisturbance as criticality output (dominant or recessive) and\nmaintenance workload (hours) as outputs for duration. Such\ninput-output pairs allow for extrinsic evaluation, but also ﬁne-\ntuning of model parameters. The results indicate that Tf-IDf\nconsiderably outperforms the base CamemBERT and almost\nas well as ﬁne-tuned CamemBERT, which implies that the\ntask, data or evaluation are insufﬁcient to fully beneﬁt from\nthe representational capacities of large language models.\nBrundage, Sharp, & Pavel (2021) show an association be-\ntween signal values and expert annotations by generating a\ntechnical language dataset with the help of two technicians.\nOne technician generated and monitored faults, followed by\nanother technician writing annotations. The authors ﬁnd a\nclear correlation between annotation contents and expert con-\ndition monitoring, which presents a strong case for language\nsupervision. Lowenmark et al. (2022) investigate the effect\nof out-of-vocabulary technical terms on BERT and Sentence-\nBERT performance annotation representations by substitut-\ning key terms with in-vocabulary natural language terms. The\nchallenges of evaluation without labels or benchmark datasets\nwere also discussed, and two methods to simulate extrinsic\nmetrics were suggested. The authors found that the cluster-\nability as measured by k-means score, and the predictability\nof automatically assigned fault class labels, both improved\nwith only a few key words substituted.\n10\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\n3.2.2. Technical Language Processing Embeddings\nLanguage-based models require a mathematical representa-\ntion of language. This is achieved through pre-processing\nand an embedding algorithm. The pre-processing step in-\nvolves tokenisation, cleaning and spell-checking, stop-words\nremoval, stemming/lemmatisation, and fundamental language\nanalysis such as part of speech tagging and named entity recog-\nnition. The embedding algorithm can be as simple as one-hot\nencoding or a complex massive transformers based architec-\nture.\nFigure 3 and Table 4 illustrate an example of technical lan-\nguage annotations and condition monitoring signals from a\ncraft liner production plant in northern Sweden. The Fig-\nure shows three different envelope-ﬁltered measurements as-\nsociated with the annotations shown in the Table. The ﬁrst\nannotation indicates that there is a fault of class Ball-Pass\nFrequency Outer ring (BPFO) with a low severity, which is\nrelated to the low-intensity peaks at characteristic kinemati-\ncally based order frequencies in the spectrum. The second\nannotation describes that the corresponding overtones have\nincreased in magnitude and that a work order has been writ-\nten. At that point the fault is estimated to be more severe and\nat the end of its RUL, so the component (bearing) has to be\nreplaced. Finally, the third annotation informs that a bearing\nhas been replaced and that the vibration levels are low, indi-\ncating a healthy component.\n3.2.3. Challenges and Solutions\nPre-processing of technical language faces several difﬁcul-\nties, as use of technical language can vary even in the same\nﬁeld, and there is no uniformly deﬁned list of stems/lemmas,\nstop-words or correct spellings. For instance, if a CM dataset\ncontains faults of class ”Ball-Pass Frequency Outer” (BPFO)\nand ”Ball-Pass Frequency Inner” (BPFI), but one is consider-\nably more common than the other, an automated spell-checker\nmight assume that one is a spelling error. Likewise, there is\nno deﬁned dictionary for stemming of technical words such\nas BPFO or BPFI, and reducing both words to ”BPF” nat-\nurally loses critical information. Therefore it is necessary\nwith a ”human-in-the-loop” system until a level of language\nprocessing maturity which accurately covers the heteroge-\nneous ﬁeld of technical language is achieved. One dictionary\nof technical stop words has been produced (Sarica & Luo,\n2021), though it is not necessarily the case that this list is ac-\ncurate for industries besides those covered in the article.\nEncoding technical language to vectors faces a major chal-\nlenge in that many technical words speciﬁc to industries are\nnot in the vocabulary of NLP models trained on natural lan-\nguage. Addressing this directly with NLP methods is thus\nrelated to handling out of vocabulary (OOV) words. A com-\nmon method to deal with OOV words, used in for instance\nBERT (Devlin et al., 2019) and GPT (Radford, 2018b; Rad-\nford et al., 2019b; Brown et al., 2020b), is to input subword\nencodings such as byte-pair encodings (BPE) (Gage, 1994;\nSennrich et al., 2015) or WordPieces (Schuster & Nakajima,\n2012; Y. Wu et al., 2016), rather than the words themselves\nas inputs to the model. Both models work by learning to\nmaximise the coverage of words in the corpus using a typi-\ncally ﬁxed amount of subwords. Thus, common words are\nassigned one whole token, while uncommon words or word\nendings, such as the ”ing”-sufﬁx in for instance ”running”,\nmight be assigned multiple tokens. The difference between\nBPE and WordPiece comes mainly from how the subwords\nare assigned, where BPE chooses the most frequent byte pair\nand WordPiece chooses the the pair which maximises the\nlikelihood of the training data. Other models try to learn to\npredict the meaning of an unknown word based on surround-\ning words, individual characters, or a combination of both\n(Lochter et al., 2020). Implementing an OOV solution which\nallows transfer learning of a pre-trained deep learning NLP\nencoder could potentiate more semantically accurate repre-\nsentations of technical language word embeddings, which in\nturn would improve the potential for TLS.\nAnother method to encode technical language is through hu-\nman designed expert systems - essentially a set of rules de-\nscribing the keywords for faults, actions, severities etc (Sex-\nton et al., 2018). The annotation\n”High BPFO in env3. WO on bearing replacement”\nwould thus be decomposed into\nclass −BPFO; severity −high; detected in −env3;\naction −WO replacement; action target −bearing.\nThese keywords can then serve as targets for annotation pre-\ndiction or language based supervision, acting as less noisy\nlabels than learned embeddings for language representations.\nHowever, such a system is difﬁcult to scale and vulnerable to\nnew keywords being introduced, essentially requiring tailored\nengineering and maintenance for each unique industry. It is\nalso vulnerable to oversights from the engineers of the expert\nsystem, for instance missing negations in statements, unfore-\nseen keyword usage or a lack of context due to the removal\nof semantics.\n3.3. Outline of Technical Language Supervision concepts\nand model\nIn the infant stage of TLP, classical NLP methods such as\nstop-word removal, lemmatisation, stemming and BoW anal-\nysis have been used. A potential improvement is to apply\nmore recent innovations in pre-processing and analysis, such\nas word embedding algorithms coupled with manual tagging\nof industry-speciﬁc technical language.\nFigures 4 and 5 show a TLS model inspired by the CLIP\n11\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nModerate levels\nof BPFO\nAnnotations\nTime & frequency\ndomain data\n(Trainable)\nProjection\nheads\nNx3200\n->Nx64 \nNx768\n->Nx64 \n(Frozen)\nEncoders\nSpectra\nNx3200\n(identity)\nText ->\nNx768\n(BERT)\nInput\nbatch\n(N) \nAnnotation projection features\nA1\nA2\nA3\nA…\nAN\nS1A1 S1A2 S1A3\nS1AN\nS1\nS2\nS3\nS…\nS5\nS2A1 S2A2 S2A3\nS2AN\nS3A1 S3A2 S3A3\nS3AN\nSNA1 SNA2 SNA3\nSNAN\n…\n…\n…\n…\n…\n…\n…\n…\nFeature similarity NxN\nSignal projection features\nFigure 4.\nExample illustrating the pre-training step of a\nnatural language supervision model. Annotations and time-\nfrequency domain signal features are encoded, and the model\nis optimised to connect the correct text-feature pair in the\nbatch of training examples, here marked with dark green\ncolour, through contrastive learning.\nBPFO\nLow\nZero-shot\nprediction\nAnnotation projection features\nA1\nA2\nA3\nA…\nAM\nSxA1\nSxA2\nSxA3\nSxA…\nSxAM\nFeature Similarity (MxN)\nSx\nFault\nclass\nFault \nSeverity\nBPFI\nHigh\nBPFO\nLow\n…\n…\nInput spectrum Sx\nMx768\n->Mx64 \nText ->\nMx768\n(BERT)\nSignal projection\nfeatures\nInput for \nzero-shot\nclassification\nNx3200\n->Nx64 \n(Frozen)\nEncoders\n(Frozen)\nProjection\nheads\n”Severity\nlevels of\nclass”\nQuery generator\nSpectra\nNx3200\n(identity)\nargmax\nFigure 5. Example illustrating how inference can be gener-\nated with the natural-language supervised model outlined in\nFigure 4. Signal and language inputs are compared in the pro-\njection space learned during pre-training, and pairs with the\nhighest feature similarity are used as outputs in either spec-\ntrum retrieval or zero-shot classiﬁcation.\nmodel Radford et al. (2021a) describing natural language su-\npervision. In the pre-training step, a technical language en-\ncoder and a fault diagnosis encoder are used to produce fault\nand text features. A mapping between fault and text encod-\nings is learned through contrastive learning (Tian et al., 2020;\nY. Zhang et al., 2020). In the inference phase, the same en-\ncoders are used, but additionally there exists a label query\nmechanism that maps an input signal to the annotation-based\nlabel that is closest to the query in the joint data and language\nembedding space.\nIn the case of IFD of rotating machinery, the input is typically\nsensor data in time-, frequency- and time-frequency-domains.\nIFD data encoding methods are described in section II, and\ntypically consist of variations of CNNs. Recently, the Trans-\nformer (Vaswani et al., 2017), an architecture introduced to\nmodel long-range dependencies and training inefﬁciencies in\nNLP, has been successfully used for image recognition with-\nout any convolutions in the model (Dosovitskiy et al., 2020;\nB. Wu et al., 2020; K. Han et al., 2021).\nIn order to train classiﬁcation or regression models using lan-\nguage, and not just an annotation generator, a language based\nlabelling method is required. Based on current state-of-the-\nart methods, some human intervention is required in this step\nto pre-deﬁne the label-space, so that annotations can be mat-\nched to the closest label semantically.\nIn (Radford et al.,\n2021a), a BoW method is implemented to complete pre-deﬁned\nsentence structures by inserting the correct term chosen from\nthe bag. A similar model could be used in IFD, with more\nthan one degree of freedom in the query to label both fault\nclass and severity Potentially, further degrees of freedom also\nenables labelling time-aspects of fault evolution. With a large\ntext dataset and access to well deﬁned labels in parallel with\nthe annotations, a mapping between a more feature-rich en-\ncoding and the label space can be learned and implemented\nto produce labels in a weakly supervised manner for data-\nannotation pairs where labelled data are not available.\nIn the case of CM data, the volume and density of text data is\nlow compared to web-crawl results for captioned images on\nthe Internet, or extensively annotated datasets such as COCO\n(Lin et al., 2015). The language is also domain speciﬁc, and\nannotations are connected to time-frequency data recordings\nin the dataset, while the semantics of an annotation can be\nbased on analysis of trends over many measurement record-\nings. This motivates the use of pre-trained models, in combi-\nnation with feature-engineering and ﬁne-tuning to adapt the\nmodel to the domain-speciﬁc terms used in process industry.\nWeak supervision will also be required to deal with unanno-\ntated faults, time-delays, a lack of annotations in healthy data,\nand noise in the annotations resulting from domain-speciﬁc\nlanguage, spelling errors, and grounding noise due to subjec-\ntive interpretations.\n4. CASE STUDY\nWe implement a version of the architecture presented in Fig-\nures 4 and 5 using data from a craft paper production plant in\nnorthern Sweden, with spectrum and annotation embedding\nprojection heads as trainable parameters through contrastive\nlearning.\n4.1. Data\nThe data used comes from six months of recorded data in\ntwo large paper mills producing Kraftliner in northern Swe-\nden, and consists of annotated condition monitoring signals\nfrom assets, such as dryers, rollers and gearboxes etc. Figure\n6 shows a schema of the data structure for each paper mill.\nEach paper mill forms a database. The database consists of\nmultiple machine parts called assets, which occasionally have\nassociated annotations when faults have been detected and di-\nagnosed. Each asset has multiple subassets consisting of dif-\nferent types of signals, from one or more sensors. Subassets\ncan be two sensors mounted on the same asset but at opposite\n12\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nends, or signals from the same sensor that have been trans-\nformed using ﬁlters such as enveloping. Subassets consist\nof multiple recordings in the form of time series and spectra\nmeasurements, which are the data used in this case study. A\nrecording is one series of data, typically 6400 vibration mea-\nsurement samples taken over 6.4 seconds, from which spectra\nwith 3200 samples up to 500Hz are computed. Thus, for each\nannotation there is one associated asset, with multiple subas-\nsets, with multiple recordings.\nIn our dataset, we have 109 annotations with a total of 21090\nassociated recordings present in a span of ten days before\nand after the annotation date. Many annotations are identical,\nand of the 109 annotations there are 43 unique fault descrip-\ntions. As data scales up, the number of unique annotations\nwill also increase, which is why a pre-trained language model\nis needed to ensure system scalability.\n4.2. Text Encoder\nThe text encoder part of out case study TLS model is seen in\nFigure 4, shown in orange at the top of the ﬁgure.\nThe annotations are embedded using a pretrained and frozen\nSentenceBERT (Reimers & Gurevych, 2019) model trained\non Swedish corpora (Rekathati, 2021), which transforms ev-\nery annotation to a 768-dimensional embedding vector. as\nshown in the ﬁrst two boxes.\nSentenceBERT is based on\nBERT and RoBERTa, but is trained to speciﬁcally produce\ngood sentence embeddings through siamese and triplet net-\nworks (Schroff et al., 2015). In the normal BERT model,\neach word is projected to a 768-dimensional embedding vec-\ntor. For example, an annotation with ten words is embed-\nded with dimensions 10x768. To use these embeddings for\ndownstream tasks, it is common to pool them to 1x768 then\nuse a feed-forward neural network (FFN). Pooling can be ac-\ncomplished by averaging each embedding, taking weighted\nmax values, or by using the classiﬁcation (CLS)-token, which\nis a ﬁnal token added to the BERT model that effectively\nbecomes a learned pooling of the self-attention. Sentence-\nBERT is a BERT-based model ﬁne-tuned on the task of pool-\ning word embeddings to sentence-embeddings, using corpora\nwith similar and dissimlar sentences, and an objective func-\nCondition\nMonitoring\nDatabase\nAssets\nSubassets\n(sensors)\nRecordings\n(measurements)\nRecordings\n(measurements)\nRecording \n(measurement)\nSubassets\n(sensors)\nSubasset\n(sensors, filters)\nAssets\nAsset (machine\ncomponent)\nAnno-\ntation\nFigure 6. Schema of data structure in a condition monitoring\ndatabase. The database consists of multiple machine parts\ncalled assets, which sometimes have associated annotations.\nEach asset has multiple subassets consisting of different types\nof signals, from one or more sensors. Each subasset consists\nof multiple recordings in the form of time series and spectra\nmeasurements.\ntion deﬁned to minimise some distance measure, either soft-\nmax, cosine or euclidean between triplets, between similar\nsentence embeddings. Thus, annotations, which typically con-\nsist of one sentence, can be transformed directly to 1x768\nwith a model speciﬁcally optimised for this task.\nAn FFN is then used to reduce the dimensions down to 64\nto introduce trainable parameters and reduce the complexity\nof the dot-product in the contrastive learning step. The FFN\nis a simple two-layer network with one skip-connection go-\ning from 768 to 64 to 64, with a Gaussian error linear unit\n(GELU) activation function, a 10% dropout, and layer norm.\nThe output of the FFN is then used as input for the contrastive\nlearning step, seen in the rightmost two boxes of the ﬁgure\n4.3. Signal Encoder\nAs there are only 109 annotations it is challenging to opti-\nmise a network at the asset or subasset level. Therefore we\npropagate the labels down to the recordings level, where we\nhave 21090 spectrum-annotation-pairs with 43 unique anno-\ntations. Thus, the same annotation at an asset will describe\nevery spectra related to that asset, even if some spectra are\nvoid of fault features. However, as shown by C. Jia et al.\n(2021) and Z. Wang, Yu, Yu, et al. (2021), noisy text-image\npairs can still converge to a general understanding through the\nweak supervision that is still present, and it is likely that the\nsame will hold true when replacing images with sensor data.\nWe directly use the spectra as the fault features, which can be\ninterpreted as the pre-trained model being a FFT and envelope\nﬁlter of the raw time series. The spectra are projected from\n3200 to 64 dimensions with the same reasoning and the same\nmodel setup as the annotation embeddings. This is shown in\nFigure 4 as the spectra encoder being empty, going from 3200\nto 3200. As with the annotation embeddings, the resulting 64-\ndimensional vectors are then sent to the contrastive learning\nstep, seen in the next blue box.\n4.4. Contrastive Learning\nWe train the data using contrastive loss to project positive\npairs of signals close and negative pairs further away in a\nprojection space, inspired by the methodology presented in\n(Radford et al., 2021b). Logits are computed through the dot\nproduct of the text and spectrum embeddings in a batch. The\nself-similarities of spectrum and text embeddings are then\ncomputed through the dot product with themselves. The tar-\ngets, the ”labels” for the constrastive loss, are then computed\nas the softmax of the averages of the self-similarities. The\nloss for the text-encoder and the spectrum-encoder are then\ncomputed separately through cross entropy loss of the logits\nand the targets. Finally, the model batch loss is deﬁned as the\nmean of the spectrum and text losses. We trained the model\nfor only three epochs, as the loss on the validation set quickly\nstarted deviating from the train loss, as seen in Figure 7.\n13\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\n0\n5\n10\n15\n20\n25\n30\nEpochs\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nTraining loss\n2.05\n2.10\n2.15\n2.20\n2.25\n2.30\nValidation loss\nFigure 7. Training and validation contrastive loss for case\nstudy model.\n4.5. Zero-shot analysis\nFinally, the pre-trained model is used to show which spectra\nin the dataset best correspond to fault queries through spec-\ntrum retrieval, and to predict fault classes based on an unla-\nbelled spectra with a label query through zero-shot classiﬁ-\ncation. More speciﬁcally, unlabelled spectra chosen from the\ndataset, and manually chosen label queries, are both used as\ninputs, while the highest dot product of the embeddings gen-\nerates a prediction output.\nFigure 8 shows spectrum retrieval using queries, described in\nTable 5, as inputs and receiving matching spectra as outputs.\nThe queries are embedded using the pre-trained technical lan-\nguage supervision model, alongside all spectra in the training\nand validation set. The output spectra are those with the high-\nest embedding dot product.\nFigure 9 illustrates a zero-shot classiﬁcation implementation\nof the technical language supervision framework, with queries\nalso described in Table 5. Four examples of spectrum in-\nputs are shown in overlapping pairs in the upper two parts of\nthe ﬁgure. The corresponding annotations and axes for these\nspectra are colour coded and marked as S1-S4. The lower\npart of the ﬁgure illustrates zero-shot classiﬁcation with ﬁve\nqueries, where the inner product between the queries and the\nspectra was computed. The inner product between a spec-\ntra and a query is represented directly over the query, with\ncolours indicating which spectra the bar is related to.\nTable 5. Query inputs for spectrum retrieval and zero-shot\nclassiﬁcation\nQuery ID\nQuery\nQ1\n”BPFO low levels”\nQ2\n”WO cable replacement”\nQ3\n”Replace sensor”\nQ4\n”DC FS”\nQ5\n”Breakdown”\n0\n100\n200\n300\n400\n500\n0.0\n0.2\nQ1 (0.6): \n\"bpfo low levels\"\n0\n100\n200\n300\n400\n500\n0\n50\nQ2 (0.49): \n\"WO cable\nreplacement\"\n0\n100\n200\n300\n400\n500\n0\n50\nIntensity\nQ3 (0.5): \n\"replace sensor\"\n0\n100\n200\n300\n400\n500\n0.0\n0.2\nQ4 (0.5): \n\"dc fs\"\n0\n100\n200\n300\n400\n500\nFrequency (Hz)\n0\n1\nQ5 (0.58): \n\"breakdown\"\nFigure 8. Spectrum retrieval using text queries to sample the\ntop three spectra with the highest embedding dot products\n4.6. Results\n4.6.1. Spectrum retrieval\nThe results of the case study indicate that even with a lim-\nited amount of data and a relatively simple model with few\nhyperparameters, there are aspects of fault diagnosis learned\nwithout any labels. For instance, the top four spectra cho-\nsen in the spectrum retrieval task shown in Figure 8 are all\nexamples of signals that correspond to their respective query;\nqueries 1 and 4 retrieve spectra indicating bearing faults, with\nhigh frequency peaks likely corresponding to characteristic\nfrequencies of bearings, while queries 2 and 3 both indicate\ncable or sensor faults, seen in the unnaturally high intensity\nclose to zero, indicating a bias in the time series. Query 4 il-\nlustrates one interesting property of the correlations between\nlanguage and signals, where ”DC FS” means ”drying group\nfree side”, which is a phrase commonly seen in conjunction\nwith bearing fault detection or bearing replacement work or-\nders. Query 5 was chosen to test the model where no clear\ncorrelations were to be expected, as there were very few oc-\ncurrences of breakdown in the dataset and breakdown does\nnot have one clear signal representation. However, upon con-\nsultation with an expert analyst, we learned that the model\nhad picked up a correlation between spectra indicating loose-\n14\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\n0\n1000\n2000\n3000\nFrequency (Hz)\n0.0\n0.2\nIntensity S1\nS1: bpfo in env low\nlevels keep watch\nS2: WO written bearing\nreplacement dc fs\n0\n1000\n2000\n3000\nFrequency (Hz)\n0.0\n6.1\nIntensity S3\nS3: WO written\ncable replacement\nS4: the sensor shall be\nreplaced next stop\nQ1:\nbpfo\nlow levels\nQ2:\nWO\ncable\nreplacement\nQ3:\nreplace\nsensor\nQ4:\ndc fs\nQ5:\nbreak\ndown\nQueries\n0.0\n0.1\n0.2\n0.3\nEmbedding dot product\n0.0\n0.1\nIntensity S2\n0.0\n4.1\nIntensity S4\nFigure 9. Zero-shot implementation on the case study data.\nInput spectra are shown in the upper two graphs, and input\nqueries with matching inner products are shown in the lower\ngraph.\nness, which apparently was the reason behind this breakdown.\nThus, the analyst found this spectrum retrieval valuable and\nsuccessful, indicating the need for close collaboration with\nindustries even when developing self-supervised data-driven\nmodels.\n4.6.2. Zero-shot predictions\nThe zero-shot predictions shown in Figure 9 produce good\nresults, where Q1 and Q4 correctly have a higher values for\nspectra 1 and 2 than for spectra 3 and 4, while Q2 and Q3\ncorrectly correlate more with spectra 3 and 4. In particu-\nlar, Q1, ”BPFO low levels”, correctly correlates signiﬁcantly\nmore with the spectra whose annotation reads ”BPFO in env\nlow levels keep watch”, and Q4 likewise correctly correlates\nmuch more with S2. Furthermore, both queries correlate more\nwith S1 and S2 which are spectra that indicate bearing faults,\ndespite the individual characteristics of each spectra being\ndifferent. Q2 and Q3 both correlate strongly to the similar\nspectra S3 and S4, with Q3 correlating stronger with both\nspectra, and S3 stronger with each query. However, since ca-\nble and sensor faults in general show similar feature spaces,\nboth queries are accurately mapped to similar spectra. Q5,\n”breakdown”, correlates poorly with all chosen spectra, which\nis an accurate classiﬁcation as none of the input spectra should\nindicate a breakdown.\nIn both the spectrum retrieval and the zero-shot prediction we\nused normalised text embedding and unnormalised spectrum\nembeddings before normalising the dot product, as opposed\nto the normalised spectrum embeddings used during training.\nNormalising the text embeddings had little impact on either\ntask, but the spectrum retrieval was affected considerably by\nnormalisiation of the spectrum embeddings, producing better\nretrievals with higher values for BPFO-related annotations,\nbut lower values and worse retrievals for cable and sensor\nfaults, while zero-shot predictions were relatively unaffected.\n4.7. Discussion\nThe technical language supervision model outlined and im-\nplemented in this case study is a basic adaptation of the model\nused in Radford et al. (2021b). It faces several challenges re-\nlated to the application to technical language and condition\nmonitoring signals, which are discussed in the following sub-\nsections. Table 6 summarises the tasks, challenges and pro-\nposed approaches for text encodings, signal encodings, con-\ntrastive learning and zero-shot classiﬁcation.\n4.7.1. Text encoder\nThe main challenge for the text encoder is to create good\nembeddings of technical language, as they are the basis for\nthe potential of the contrastive learning step. As discussed\nin Section 3.2, this challenge is due to technical language\nbeing different from the natural language normally used to\ntrain language models, and technical language data scarcity.\nIn this case study, we opted to use a pre-trained natural lan-\nguage model without any ﬁne-tuning. Three approaches for\nimprovements of technical language encodings are shown in\nthe table, which can be summarised as using small-data in-\ndustry speciﬁc solutions through technical language process-\ning, discussed in Section 3.2; large data self-supervised pre-\ntraining solutions; and supervised ﬁne-tuning, both discussed\nin Section 3.1.\n4.7.2. Signal encoder\nFor the signal encoder, the main task is to produce good fault\nfeature representations prior to the projection head, compara-\nble to the language model step of the language encoder. The\nlack of labelled industry data sets, the difﬁculty of feature\ntransfer and the non-linear and industry-speciﬁc properties of\nfault severity, are all challenges for this task. Approaches\nto overcome these challenges are discussed in Section two,\n15\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nTable 6. Different TLS tasks, challenges and approaches\nTask\nChallenges\nApproaches\nEncoding\ntechnical\nlanguage.\nTechnical language different\nfrom natural language.\nLimited data availability.\nTechnical language processing, see 3.2.\nSelf-supervised pre-training, see 3.1.\nSupervised ﬁne-tuning, see 3.1.\nEncoding\nfault\nfeatures\nLabelled industry data scarce.\nLab features difﬁcult to transfer.\nNon-linear evolution of fault severity.\nFault severity levels industry speciﬁc.\nTransfer Learning, see 2.3.\nWeak Supervision, see 2.4.\nContrastive learning for ﬁne-tuning,\nsee 2.4.2 and 3.1.\nContrastive learning\noptimisation.\nFaults appear and evolve over\nmultiple recordings and signal types.\nSequential model projection\nheads, see 4.7.3.\nData augmentation, see 4.7.3.\nEvaluating zero-shot.\nperformance and implementation.\nNovel task.\nNo benchmark test set.\nIndustry expert analysis &\nIndustry test deployment,\nsee 4.7.4.\nbut more speciﬁcally transfer learning, weak supervision and\ncontrastive learning are viable approaches, with speciﬁc sec-\ntions shown in Table 6\n4.7.3. Contrastive learning\nThe task of the contrastive learning part of the model is to\nforce positive pairs to a similar projection space, while nega-\ntive pairs are pushed away. The main challenge in this step is\nrelated to data properties, where annotations are too scarce\nto fully leverage the utility of scale that is shown in NLS\n(F. Chen et al., 2022a), and fault evolution too nonlinear for\nannotation propagation to accurately work as data augmenta-\ntion. Furthermore, unlike in NLS prediction of image classes,\nTLS individual recordings are insufﬁcient information to fully\nassess fault characteristics, akin to describing a movie from\njust one frame. Thus, multiple recordings must be consid-\nered to mimic human analysis in the contrastive learning step,\nwhich requires methods able to attend to sequential data such\nas recurrent neural networks or transformers, either as projec-\ntion heads or integrated in the text and signal encoders.\nPropagating annotation embeddings to each corresponding\nrecording increases the size of the dataset, but also leads to in-\naccurate supervision from annotations on the recordings level,\narising due to the inexactness between recordings level and\nasset level. For example, if a sensor is faulty at half of its\nmeasurements, but works for the other half, the model should\nideally be trained only on the faulty signals. Likewise, BPFO\nis typically detected ﬁrst in envelope spectra, thus resulting\nin BPFO annotations being associated with normal spectra\nwhere BPFO features have likely not appeared yet. The va-\nriety of input types in the spectra inputs is in itself an issue\nfor optimisation, as the network will have to learn to project\ntwo very different signals to the same projection in the joint\nembedding space.\nHowever, knowledge of expected fault\nbehaviour with regards to annotation types could be lever-\naged to perform improved data augmentation and more accu-\nrately propagate annotations in time with annotation contents\nchanging depending on fault type and time distance from true\nannotation.\n4.7.4. Zero-shot predictions\nThe main challenges with zero-shot classiﬁcation in an indus-\ntry environment is that it is a novel ﬁeld and hard to evaluate\nwithout labelled test sets. The contrastive loss or accuracy\nduring optimisation is relative to the model, and offers little\ninsight into model performance at implementation. There-\nfore we use Figures 8 and 9 to illustrate model performance\nfor two test scenarios. This evaluation requires prior fault di-\nagnosis knowledge however, compared to the much simpler\ntask of evaluating natural language supervision classiﬁcation\nfor image captioning. However, the efﬁcacy of the model\ncan also be evaluated by test deployment in industry, where\nfeedback from industry experts evaluates whether the model\nworks to improve current fault diagnosis practices or not.\nInvestigating the zero-shot predictions in Figure 9 showed\nthat a spectrum containing BPFO features gave high inner\nproducts also for cable and sensor queries, and we specu-\nlated that this might be due to latent BPFO features occasion-\nally seen in the cable and sensor-associated spectrum training\ndata. This is an issue of incomplete supervision, which is\nfurther exacerbated if unannotated data is used during train-\ning, as the absence of fault annotations does not necessar-\nily guarantee the absence of fault features, given that early\nfaults might go undetected by current analysis. The issues of\nweak supervision can be addressed by adding data-speciﬁc\nsolutions, by for instance limiting extraction dates to after\nthe annotation, or adding pre-processing of annotations to\nmanually handle ”replaced”-like annotations as a different\nclass. This issue might also be solveable by simply scaling\nup data, which has worked in natural language supervision as\ndiscussed in Section 4.3.\n5. CONCLUSION\nThe fault descriptions and maintenance records commonly\nstored in modern process industry CM systems are unexploited\n16\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nsources of information for training IFD systems. Language\npresent in CM datasets can be used for technical language\nbased supervision of IFD models to facilitate automation of\nroutine FD tasks and develop more accurate decision sup-\nport for complex tasks (Ekstr¨om & Sandin, 2020).\nSince\nlanguage-based labels are intrinsically uncertain, weakly su-\npervised learning methods need to be developed, which can\nalso support transfer learning of pretrained IFD models with\nlabels extracted from language in industry datasets. Our ex-\nperiments show that even with a basic TLS implementation,\nwithout custom signal processing or pre-trained fault diag-\nnosis encoders, a joint embedding space for annotations and\nfault features can be learned and used for zero-shot classiﬁ-\ncation.\nImprovements in TLS can occur both through an enhance-\nment of the TLP pipeline for technical language representa-\ntions, or through augmented integration of IFD-based signal\nencoders.However, a major challenge for TLP and TLS re-\nsearch is the lack of realistic and open annotated industry\ndata, which can be used for comparative studies and bench-\nmarks. Furthermore, the assistance of industry experts was\nsometimes required to understand the annotation language\nand how annotations were motivated by signal features and\nthe context. Thus, in this work the collaboration between in-\ndustry and academia was key. Open access annotated datasets\nwith clearly described features and valid benchmark tasks\nare needed to make this important direction of research more\nreadily accessible.\nACKNOWLEDGEMENT\nThis work is supported by the Strategic innovation program\nProcess industrial IT and Automation (PiIA), a joint invest-\nment of Vinnova, Formas and the Swedish Energy Agency,\nreference number 2019-02533.\nThe analysis of the results was done with help from H˚akan\nSirkka, a condition monitoring analyst with experience of the\nindustry data used.\nWe thank the members of the project reference group includ-\ning Per-Erik Larsson, Kjell Lundberg, H˚akan Sirkka and Pe-\nter Wikstr¨om, for valuable inputs.\nKL thanks Prakash Chandra Chhipa for helpful discussions\non contrastive learning and joint embedding spaces.\nREFERENCES\nAghdam, H. H., Gonzalez-Garcia, A., van de Weijer, J., &\nL´opez, A. M. (2019). Active learning for deep detection\nneural networks.\nAn, Z., Li, S., Xin, Y., Xu, K., & Ma, H. (2019). An in-\ntelligent fault diagnosis framework dealing with arbitrary\nlength inputs under different working conditions. Measure-\nment Science and Technology, 30(12).\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., & Zhang, L. (2018). Bottom-up and top-down\nattention for image captioning and visual question answer-\ning.\nAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zit-\nnick, C. L., & Parikh, D. (2015, December). Vqa: Visual\nquestion answering. In Proceedings of the ieee interna-\ntional conference on computer vision (iccv).\nBabu, G., Zhao, P., & Li, X.-L. (2016). Deep convolutional\nneural network based regression approach for estimation\nof remaining useful life. Lecture notespams in Computer\nScience (including subseries Lecture notespams in Artiﬁ-\ncial Intelligence and Lecture notespams in Bioinformat-\nics), 9642, 214-228.\nBen Ali, J., Chebel-Morello, B., Saidi, L., Malinowski, S., &\nFnaiech, F. (2015). Accurate bearing remaining useful life\nprediction based on weibull distribution and artiﬁcial neu-\nral network. Mechanical Systems and Signal Processing,\n56, 150-172.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,\nDhariwal, P., ... Amodei, D. (2020a). Language models\nare few-shot learners.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,\nDhariwal, P., ... Amodei, D. (2020b). Language models\nare few-shot learners.\nBrundage, M. P., Sexton, T., Hodkiewicz, M., Dima, A., &\nLukens, S. (2021). Technical language processing: Un-\nlocking maintenance knowledge. Manufacturing Letters,\n27, 42-46.\nBrundage, M. P., Sharp, M., & Pavel, R. (2021, Jun). Qualify-\ning evaluations from human operators: Integrating sensor\ndata with natural language logs. PHM Society European\nConference. 6.\nCadavid, J. P. U., Grabot, B., Lamouri, S., Pellerin, R., &\nFortin, A. (2020). Valuing free-form text data from mainte-\nnance logs through transfer learning with camembert. En-\nterprise Information Systems, 0(0), 1-29.\nCahill, J. (2021). Improving subsurface models to reduce\ndrilling uncertainty.\nCao, P., Zhang, S., & Tang, J. (2018a). Preprocessing-free\ngear fault diagnosis using small datasets with deep con-\nvolutional neural network-based transfer learning. IEEE\nAccess, 6, 26241-26253.\nCao, P., Zhang, S., & Tang, J. (2018b). Preprocessing-free\ngear fault diagnosis using small datasets with deep con-\nvolutional neural network-based transfer learning. IEEE\nAccess, 6, 26241-26253.\nCarden, E. P., & Fanning, P. (2004). Vibration based condi-\ntion monitoring: A review. Structural Health Monitoring,\n3(4), 355-377.\n17\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nCase western reserve university bearing data center web-\nsite.\n(n.d.).\nhttps://csegroups.case.edu/\nbearingdatacenter/pages/welcome-case\n-western-reserve-university-bearing\n-data-center-website.\nCerrada, M., S´anchez, R.-V., Li, C., Pacheco, F., Cabrera,\nD., de Oliveira], J. V., & V´asquez, R. E. (2018). A review\non data-driven fault severity assessment in rolling bearings.\nMechanical Systems and Signal Processing, 99, 169 - 196.\nChen, F., Zhang, D., Han, M., Chen, X., Shi, J., Xu, S., & Xu,\nB. (2022a). Vlp: A survey on vision-language pre-training.\narXiv.\nChen, F., Zhang, D., Han, M., Chen, X., Shi, J., Xu, S., & Xu,\nB. (2022b). Vlp: A survey on vision-language pre-training.\narXiv.\nChen, X., Zhang, B., & Gao, D. (2021). Bearing fault diag-\nnosis base on multi-scale cnn and lstm model. Journal of\nIntelligent Manufacturing, 32(4), 971-987.\nChen, Z., Gryllias, K., & Li, W. (2020). Intelligent fault\ndiagnosis for rotary machinery using transferable convo-\nlutional neural network. IEEE Transactions on Industrial\nInformatics, 16(1), 339-349.\nchung Fu, T. (2011). A review on time series data mining.\nEngineering Applications of Artiﬁcial Intelligence, 24(1),\n164-181.\nCondition based maintenance fault database for testing of di-\nagnostic and prognostics algorithms. (n.d.). https://\nwww.mfpt.org/fault-data-sets/.\ndel Campo, S. M., & Sandin, F. (2017). Online feature learn-\ning for condition monitoring of rotating machinery. En-\ngineering Applications of Artiﬁcial Intelligence, 64, 187 -\n196.\nDesai, K., & Johnson, J. (2020). Virtex: Learning visual\nrepresentations from textual annotations.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019).\nBert: Pre-training of deep bidirectional transformers for\nlanguage understanding.\nDietterich, T. G., Lathrop, R. H., & Lozano-P ˜A©rez, T.\n(1997). Solving the multiple instance problem with axis-\nparallel rectangles. Artiﬁcial Intelligence, 89(1), 31-71.\nDima, A., Lukens, S., Hodkiewicz, M., Sexton, T., &\nBrundage, M. P. (2021). Adapting natural language pro-\ncessing for technical text. Applied AI Letters, 2(3), e33.\nDing, S., Qu, S., Xi, Y., & Wan, S. (2020). Stimulus-driven\nand concept-driven analysis for image caption generation.\nNeurocomputing, 398, 520-530.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D.,\nZhai, X., Unterthiner, T., ... Houlsby, N. (2020). An image\nis worth 16x16 words: Transformers for image recognition\nat scale.\nEkstr¨om, K., & Sandin, F. (2020). Fault severity estima-\ntion using weak supervision with language based labels\nand condition monitoring data.\nElhoseiny, M., Saleh, B., & Elgammal, A. (2013). Write a\nclassiﬁer: Zero-shot learning using purely textual descrip-\ntions. In 2013 ieee international conference on computer\nvision (p. 2584-2591).\nEmerson. (2021). Featured technologies/machine learning.\nFeng, L., & Zhao, C. (2021). Fault description based attribute\ntransfer for zero-sample industrial fault diagnosis. IEEE\nTransactions on Industrial Informatics, 17(3), 1852-1862.\nFink, O., Wang, Q., Svens´en, M., Dersin, P., Lee, W.-J., &\nDucoffe, M. (2020). Potential, challenges and future direc-\ntions for deep learning in prognostics and health manage-\nment applications. Engineering Applications of Artiﬁcial\nIntelligence, 92, 103678.\nGage, P. (1994). A new algorithm for data compression. The\nC Users Journal archive, 12, 23-38.\nGao, Y., Gao, L., Li, X., & Zheng, Y. (2020). A zero-shot\nlearning method for fault diagnosis under unknown work-\ning loads.\nJournal of Intelligent Manufacturing, 31(4),\n899-909.\nGuo, L., Lei, Y., Xing, S., Yan, T., & Li, N. (2019). Deep\nconvolutional transfer learning network: A new method for\nintelligent fault diagnosis of machines with unlabeled data.\nIEEE Transactions on Industrial Electronics, 66(9), 7316-\n7325.\nGuo, L., Li, N., Jia, F., Lei, Y., & Lin, J. (2017). A recurrent\nneural network based health indicator for remaining useful\nlife prediction of bearings. Neurocomputing, 240, 98-109.\nHaidong, S., Hongkai, J., Xingqiu, L., & Shuaipeng, W.\n(2018). Intelligent fault diagnosis of rolling bearing using\ndeep wavelet auto-encoder with extreme learning machine.\nKnowledge-Based Systems, 140, 1-14.\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., ...\nTao, D. (2021). A survey on visual transformer.\nHan, T., Liu, C., Yang, W., & Jiang, D. (2019). A novel\nadversarial learning framework in deep convolutional neu-\nral network for intelligent diagnosis of mechanical faults.\nKnowledge-Based Systems, 165, 474-487.\nHan, T., Liu, C., Yang, W., & Jiang, D. (2020). Deep transfer\nnetwork with joint distribution adaptation: A new intel-\nligent fault diagnosis framework for industry application.\nISA Transactions, 97, 269 - 281.\nHancock, B., Varma, P., Wang, S., Bringmann, M., Liang, P.,\n& R´e, C. (2018). Training classiﬁers with natural language\nexplanations.\nHe, S., Liao, W., Tavakoli, H. R., Yang, M., Rosenhahn, B.,\n& Pugeault, N. (2020). Image captioning through image\ntransformer.\n18\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nHe, Z., Shao, H., Zhong, X., & Zhao, X. (2020). Ensem-\nble transfer cnns driven by multi-channel signals for fault\ndiagnosis of rotating machinery cross working conditions.\nKnowledge-Based Systems, 207.\nHelbing, G., & Ritter, M. (2018). Deep learning for fault de-\ntection in wind turbines. Renewable and Sustainable En-\nergy Reviews, 98, 189 - 198.\nHoang, D.-T., & Kang, H.-J. (2019). A survey on deep learn-\ning based bearing fault diagnosis. Neurocomputing, 335,\n327-335.\nHodkiewicz, M. R., Batsioudis, Z., Radomiljac, T., & Ho,\nM. T. (2017). Why autonomous assets are good for relia-\nbility – the impact of ‘operator-related component’ failures\non heavy mobile equipment reliability..\nHoffmann, R., Zhang, C., Ling, X., Zettlemoyer, L., & Weld,\nD. S. (2011, June). Knowledge-based weak supervision\nfor information extraction of overlapping relations. In Pro-\nceedings of the 49th annual meeting of the association for\ncomputational linguistics: Human language technologies\n(pp. 541–550). Portland, Oregon, USA: Association for\nComputational Linguistics.\nJardine, A., Lin, D., & Banjevic, D.\n(2006).\nA review\non machinery diagnostics and prognostics implementing\ncondition-based maintenance.\nMechanical Systems and\nSignal Processing, 20(7), 1483-1510.\nJardine, A. K., Lin, D., & Banjevic, D. (2006). A review\non machinery diagnostics and prognostics implementing\ncondition-based maintenance.\nMechanical Systems and\nSignal Processing, 20(7), 1483 - 1510.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham,\nH., . . . Duerig, T. (2021). Scaling up visual and vision-\nlanguage representation learning with noisy text supervi-\nsion.\nJia, F., Lei, Y., Lin, J., Zhou, X., & Lu, N. (2016). Deep\nneural networks: A promising tool for fault characteristic\nmining and intelligent diagnosis of rotating machinery with\nmassive data. Mechanical Systems and Signal Processing,\n72-73, 303-315.\nJia, F., Lei, Y., Lu, N., & Xing, S. (2018). Deep normalized\nconvolutional neural network for imbalanced fault classi-\nﬁcation of machinery and its understanding via visualiza-\ntion. Mechanical Systems and Signal Processing, 110, 349-\n367.\nJian, C., Yang, K., & Ao, Y. (2021). Industrial fault diagno-\nsis based on active learning and semi-supervised learning\nusing small training set. Engineering Applications of Arti-\nﬁcial Intelligence, 104, 104365.\nJiang, G., Xie, P., He, H., & Yan, J. (2018). Wind turbine\nfault detection using a denoising autoencoder with tempo-\nral information. IEEE/ASME Transactions on Mechatron-\nics, 23(1), 89-100.\nKhan, S., & Yairi, T. (2018). A review on the application of\ndeep learning in system health management. Mechanical\nSystems and Signal Processing, 107, 241-265.\nKothamasu, R., Huang, S. H., & VerDuin, W. H.\n(2006,\nJul 01).\nSystem health monitoring and prognostics —\na review of current paradigms and practices. The Inter-\nnational Journal of Advanced Manufacturing Technology,\n28(9), 1012-1024.\nLabutov, I., Yang, B., & Mitchell, T. (2019). Learning to\nlearn semantic parsers from natural language supervision.\nLei, Y., Jia, F., Lin, J., Xing, S., & Ding, S. (2016). An in-\ntelligent fault diagnosis method using unsupervised feature\nlearning towards mechanical big data. IEEE Transactions\non Industrial Electronics, 63(5), 3137-3147.\nLei, Y., Li, N., Guo, L., Li, N., Yan, T., & Lin, J. (2018).\nMachinery health prognostics: A systematic review from\ndata acquisition to rul prediction. Mechanical Systems and\nSignal Processing, 104, 799-834.\nLei, Y., Yang, B., Jiang, X., Jia, F., Li, N., & Nandi, A. K.\n(2020). Applications of machine learning to machine fault\ndiagnosis: A review and roadmap. Mechanical Systems\nand Signal Processing, 138, 106587.\nLi, C., Zhang, S., Qin, Y., & Estupinan, E. (2020). A system-\natic review of deep transfer learning for machinery fault\ndiagnosis. Neurocomputing, 407, 121 - 135.\nLi, J., Wong, Y., Zhao, Q., & Kankanhalli, M. (2019). Learn-\ning to learn from noisy labeled data. In (Vol. 2019-June,\np. 5046-5054).\nLi, X., Ding, Q., & Sun, J.-Q. (2018). Remaining useful\nlife estimation in prognostics using deep convolution neu-\nral networks. Reliability Engineering and System Safety,\n172, 1-11.\nLi, X., Li, X., & Ma, H.\n(2020).\nDeep representation\nclustering-based fault diagnosis method with unsupervised\ndata applied to rotating machinery. Mechanical Systems\nand Signal Processing, 143, 106825.\nLi, X., Zhang, W., Ding, Q., & Li, X. (2020). Diagnosing\nrotating machines with weakly supervised data using deep\ntransfer learning. IEEE Transactions on Industrial Infor-\nmatics, 16(3), 1688-1697.\nLi, X., Zhang, W., Ding, Q., & Li, X. (2020). Diagnosing\nrotating machines with weakly supervised data using deep\ntransfer learning. IEEE Transactions on Industrial Infor-\nmatics, 16(3), 1688-1697.\nLi, X., Zhang, W., Xu, N.-X., & Ding, Q. (2020). Deep\nlearning-based machinery fault diagnostics with domain\nadaptation across sensors at different places. IEEE Trans-\nactions on Industrial Electronics, 67(8), 6785-6794.\nLi, Y., Lin, T., Yi, K., Bear, D. M., Yamins, D. L. K., Wu,\nJ., ... Torralba, A. (2020). Visual grounding of learned\nphysical models.\n19\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nLin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R.,\nHays, J., . . . Doll´ar, P. (2015). Microsoft coco: Common\nobjects in context.\nLiu, H., Liu, C., & Huang, Y. (2011). Adaptive feature ex-\ntraction using sparse coding for machinery fault diagnosis.\nMechanical Systems and Signal Processing, 25(2), 558 -\n574.\nLiu, H., Zhou, J., Zheng, Y., Jiang, W., & Zhang, Y. (2018).\nFault diagnosis of rolling bearings with recurrent neural\nnetwork-based autoencoders. ISA Transactions, 77, 167-\n178.\nLiu, R., Yang, B., Zio, E., & Chen, X. (2018). Artiﬁcial\nintelligence for fault diagnosis of rotating machinery: A\nreview. Mechanical Systems and Signal Processing, 108,\n33-47.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ...\nStoyanov, V. (2019). Roberta: A robustly optimized bert\npretraining approach.\nLochter, J. V., Silva, R. M., & Almeida, T. A. (2020). Deep\nlearning models for representing out-of-vocabulary words.\nLowenmark, K., Taal, C., Nivre, J., Liwicki, M., & Sandin,\nF. (2022). Processing of condition monitoring annotations\nwith bert and technical language substitution: A case study.\nProceedings of the 7th European Conference of the Prog-\nnostics and Health Management Society 2022, 306-314.\nLu, C., Wang, Z.-Y., Qin, W.-L., & Ma, J. (2017). Fault diag-\nnosis of rotary machinery components using a stacked de-\nnoising autoencoder-based health state identiﬁcation. Sig-\nnal Processing, 130, 377-388.\nLu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: Pre-\ntraining task-agnostic visiolinguistic representations for\nvision-and-language tasks.\nLu, X., Wang, B., Zheng, X., & Li, X. (2018). Exploring\nmodels and data for remote sensing image caption genera-\ntion. IEEE Transactions on Geoscience and Remote Sens-\ning, 56(4), 2183-2195.\nMao, J., Gan, C., Kohli, P., Tenenbaum, J. B., & Wu, J.\n(2019). The neuro-symbolic concept learner: Interpreting\nscenes, words, and sentences from natural supervision.\nMicrosoft. (2020). Turing-nlg: A 17-biliion paramater lan-\nguage model by microsoft.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J.\n(2013). Distributed representations of words and phrases\nand their compositionality.\nMonitron, A. (n.d.). Detect abnormal machine behavior and\nenable predictive maintenance.\nMurty, S., Koh, P. W., & Liang, P. (2020). Expbert: Repre-\nsentation engineering with natural language explanations.\nNan, C., Khan, F., & Iqbal, M. T. (2008). Real-time fault\ndiagnosis using knowledge-based expert system. Process\nSafety and Environmental Protection, 86(1), 55-71.\nNandyala, A., Lukens, S., Rathod, S., & Agarwal. (2021,\nJun). Evaluating word representations in a technical lan-\nguage processing pipeline. PHM Society European Con-\nference. 6.\nNasa prognostic data repository.\n(n.d.).\nhttps://\nti.arc.nasa.gov/tech/dash/groups/pcoe/\nprognostic-data-repository/.\nPan, J., Zi, Y., Chen, J., Zhou, Z., & Wang, B. (2018). Lift-\ningnet: A novel deep learning network with layerwise fea-\nture learning from noisy mechanical data for fault classiﬁ-\ncation. IEEE Transactions on Industrial Electronics, 65(6),\n4973-4982.\nPapyan, V., Romano, Y., Sulam, J., & Elad, M. (2018). The-\noretical foundations of deep learning via sparse represen-\ntations: A multilayer sparse model and its connection to\nconvolutional neural networks.\nIEEE Signal Processing\nMagazine, 35(4), 72-89.\nPdM. (2021). Pdm services vibration analysis monitoring.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized\nword representations.\nProcessIT. (2018). Processit.eu european roadmap for pro-\ncess industrial automation. second version. , 3(3).\nQiao, M., Yan, S., Tang, X., & Xu, C. (2020). Deep convolu-\ntional and lstm recurrent neural networks for rolling bear-\ning fault diagnosis under strong noises and variable loads.\nIEEE Access, 8, 66257-66269.\nQin, S. (2012). Survey on data-driven industrial process mon-\nitoring and diagnosis. Annual Reviews in Control, 36(2),\n220-234.\nRadford, A. (2018a). Improving language understanding by\ngenerative pre-training..\nRadford, A. (2018b). Improving language understanding by\ngenerative pre-training..\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., ... Sutskever, I. (2021a). Learning transfer-\nable visual models from natural language supervision.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., ... Sutskever, I. (2021b). Learning transfer-\nable visual models from natural language supervision.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., &\nSutskever, I. (2019a). Language models are unsupervised\nmultitask learners..\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., &\nSutskever, I. (2019b). Language models are unsupervised\nmultitask learners..\nRai, A., & Upadhyay, S. (2016). A review on signal pro-\ncessing techniques utilized in the fault diagnosis of rolling\nelement bearings. Tribology International, 96, 289-306.\n20\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nRamanathan, V., Liang, P., & Fei-Fei, L. (2013). Video event\nunderstanding using natural language descriptions. In 2013\nieee international conference on computer vision (p. 905-\n912).\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-\nford, A., . . . Sutskever, I. (2021). Zero-shot text-to-image\ngeneration.\nRandall, R. B., & Antoni, J. (2011). Rolling element bearing\ndiagnostics—a tutorial.\nMechanical Systems and Signal\nProcessing, 25(2), 485 - 520.\nRazavi, A., van den Oord, A., & Vinyals, O. (2019). Gener-\nating diverse high-ﬁdelity images with vq-vae-2.\nRazavi-Far, R., Hallaji, E., Farajzadeh-Zanjani, M., & Saif,\nM. (2019). A semi-supervised diagnostic framework based\non the surface estimation of faulty distributions.\nIEEE\nTransactions on Industrial Informatics, 15(3), 1277-1286.\nReimers, N., & Gurevych, I. (2019, November). Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks.\nIn EMNLP-IJCNLP 2019 (pp. 3982–3992).\nHong Kong, China: Association for Computational Lin-\nguistics.\nRekathati, F.\n(2021).\nThe KBLab blog: Introducing a\nSwedish sentence transformer.\nRen, Z., Zhu, Y., Yan, K., Chen, K., Kang, W., Yue, Y., &\nGao, D. (2020). A novel model with the ability of few-shot\nlearning and quick updating for intelligent fault diagnosis.\nMechanical Systems and Signal Processing, 138.\nSarica, S., & Luo, J. (2021, Aug). Stopwords in technical\nlanguage processing. PLOS ONE, 16(8), e0254937.\nSariyildiz, M. B., Perez, J., & Larlus, D. (2020). Learning\nvisual representations with caption annotations.\nSchroff, F., Kalenichenko, D., & Philbin, J.\n(2015, Jun).\nFacenet: A uniﬁed embedding for face recognition and\nclustering. CVPR 2015.\nSchuster, M., & Nakajima, K. (2012). Japanese and korean\nvoice search. In ICASSP 2012 (p. 5149-5152).\nSchwendemann, S., Amjad, Z., & Sikora, A. (2021). Bear-\ning fault diagnosis with intermediate domain based layered\nmaximum mean discrepancy: A new transfer learning ap-\nproach. Engineering Applications of Artiﬁcial Intelligence,\n105, 104415.\nSennrich, R., Haddow, B., & Birch, A. (2015). Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\nSexton, T., Brundage, M., Hodkiewicz, M., & Smoker, T.\n(2018, 2018-09-24). Benchmarking for keyword extraction\nmethodologies in maintenance work orders. 2018 Annual\nConference of the Prognostics and Health Management So-\nciety, Philadelphia, PA.\nShao, H., Xia, M., Han, G., Zhang, Y., & Wan, J. (2021).\nIntelligent fault diagnosis of rotor-bearing system under\nvarying working conditions with modiﬁed transfer convo-\nlutional neural network and thermal images. IEEE Trans-\nactions on Industrial Informatics, 17(5), 3488-3496.\nShao, S., McAleer, S., Yan, R., & Baldi, P. (2019). Highly\naccurate machine fault diagnosis using deep transfer learn-\ning. IEEE Transactions on Industrial Informatics, 15(4),\n2446-2455.\nSharma, V., & Parey, A. (2016). A review of gear fault di-\nagnosis using various condition indicators. In (Vol. 144,\np. 253-263).\nSharp, M., Brundage, M., Sexton, T., & Madhusudanan, F.\n(2021, 2021-04-22 04:04:00). Discovering critical KPI fac-\ntors from natural language in maintenance work orders. ,\n3(3).\nShin, J.-H., & Jun, H.-B. (2015). On condition based main-\ntenance policy. Journal of Computational Design and En-\ngineering, 2(2), 119 - 127.\nSimon, J.\n(n.d.).\nAmazon monitron, a simple and cost-\neffective service enabling predictive maintenance.\nSKF. (n.d.). Skf enlight ai.\nSKF. (2022). Skf @ptitude observer user manual.\nSKF, A., & Kommunikation, S. (2020, March). Skf annual\nreport\n2020.\nhttps://investors.skf.com/\nsites/default/files/pr/202103032688-1\n.pdf.\nSmith, W., & Randall, R. (2015). Rolling element bearing\ndiagnostics using the case western reserve university data:\nA benchmark study. Mechanical Systems and Signal Pro-\ncessing, 64-65, 100-131.\nStetco, A., Dinmohammadi, F., Zhao, X., Robu, V., Flynn,\nD., Barnes, M., ... Nenadic, G. (2019). Machine learning\nmethods for wind turbine condition monitoring: A review.\nRenewable Energy, 133, 620-635.\nStief, A., Ottewill, J., Baranowski, J., & Orkisz, M. (2019). A\npca and two-stage bayesian sensor fusion approach for di-\nagnosing electrical and mechanical faults in induction mo-\ntors. IEEE Transactions on Industrial Electronics, 66(12),\n9510-9520.\nTanaka, D., Ikami, D., Yamasaki, T., & Aizawa, K. (2018).\nJoint optimization framework for learning with noisy la-\nbels. In (p. 5552-5560).\nTian, Y., Krishnan, D., & Isola, P. (2020). Contrastive multi-\nview coding.\nvan den Oord, A., Vinyals, O., & Kavukcuoglu, K. (2018).\nNeural discrete representation learning.\nvan Engelen, J. E., & Hoos, H. H. (2020, Feb 01). A survey\non semi-supervised learning. Machine Learning, 109(2),\n373-440.\n21\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., ... Polosukhin, I. (2017). Attention is all\nyou need.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael,\nJ., Hill, F., . . . Bowman, S. R. (2020). Superglue: A stick-\nier benchmark for general-purpose language understand-\ning systems.\nWang, D., Tsui, K.-L., & Miao, Q. (2017). Prognostics and\nhealth management: A review of vibration based bearing\nand gear health indicators. IEEE Access, 6, 665-676.\nWang, Q., Michau, G., & Fink, O. (2019). Domain adaptive\ntransfer learning for fault diagnosis. In (p. 279-285).\nWang, Y., Yao, Q., Kwok, J., & Ni, L. M. (2020). Generaliz-\ning from a few examples: A survey on few-shot learning.\nWang, Z., Yu, A. W., Firat, O., & Cao, Y. (2021). Towards\nzero-label language learning.\nWang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y.\n(2021). Simvlm: Simple visual language model pretraining\nwith weak supervision. arXiv.\nWen, L., Gao, L., & Li, X. (2019). A new deep transfer learn-\ning based on sparse auto-encoder for fault diagnosis. IEEE\nTransactions on Systems, Man, and Cybernetics: Systems,\n49(1), 136-144.\nWen, L., Li, X., & Gao, L. (2020). A transfer convolutional\nneural network for fault diagnosis based on resnet-50. Neu-\nral Computing and Applications, 32(10), 6111-6124.\nWilliams, E. C., Gopalan, N., Rhee, M., & Tellex, S. (2018).\nLearning to parse natural language to grounded reward\nfunctions with weak supervision.\nIn 2018 ieee inter-\nnational conference on robotics and automation (icra)\n(p. 4430-4436).\nWu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z., ...\nVajda, P. (2020). Visual transformers: Token-based image\nrepresentation and processing for computer vision.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M.,\nMacherey, W., ... Dean, J. (2016). Google’s neural ma-\nchine translation system: Bridging the gap between human\nand machine translation. ArXiv, abs/1609.08144.\nXu, G., Liu, M., Jiang, Z., Shen, W., & Huang, C. (2020).\nOnline fault diagnosis method based on transfer convolu-\ntional neural networks. IEEE Transactions on Instrumen-\ntation and Measurement, 69(2), 509-520.\nXu, Y., Sun, Y., Liu, X., & Zheng, Y. (2019). A digital-twin-\nassisted fault diagnosis using deep transfer learning. IEEE\nAccess, 7, 19990-19999.\nYan, X., She, D., Xu, Y., & Jia, M.\n(2021).\nDeep regu-\nlarized variational autoencoder for intelligent fault diagno-\nsis of rotor bearing system within entire life-cycle process.\nKnowledge-Based Systems, 226, 107142.\nYang, B., Lei, Y., Jia, F., & Xing, S. (2019). An intelligent\nfault diagnosis approach based on transfer learning from\nlaboratory bearings to locomotive bearings. Mechanical\nSystems and Signal Processing, 122, 692 - 706.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.,\n& Le, Q. V. (2020). Xlnet: Generalized autoregressive\npretraining for language understanding.\nYao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., ... Xu,\nC. (2021). Filip: Fine-grained interactive language-image\npre-training.\nYiakopoulos, C., Gryllias, K., & Antoniadis, I.\n(2011).\nRolling element bearing fault detection in industrial envi-\nronments based on a k-means clustering approach. Expert\nSystems with Applications, 38(3), 2888 - 2911.\nYin, S., Ding, S., Xie, X., & Luo, H. (2014). A review on ba-\nsic data-driven approaches for industrial process monitor-\ning. IEEE Transactions on Industrial Electronics, 61(11),\n6418-6428.\nYu, K., Fu, Q., Ma, H., Lin, T., & Li, X. (2021, 07). Sim-\nulation data driven weakly supervised adversarial domain\nadaptation approach for intelligent cross-machine fault di-\nagnosis. Structural Health Monitoring, 20.\nYu, K., Lin, T. R., Ma, H., Li, X., & Li, X.\n(2021).\nA\nmulti-stage semi-supervised learning approach for intelli-\ngent fault diagnosis of rolling bearing using data augmen-\ntation and metric learning. Mechanical Systems and Signal\nProcessing, 146, 107043.\nZakir Hossain, M., Sohel, F., Shiratuddin, M., & Laga, H.\n(2019). A comprehensive survey of deep learning for im-\nage captioning. ACM Computing Surveys, 51(6).\nZeng, D., Liu, K., Chen, Y., & Zhao, J. (2015, September).\nDistant supervision for relation extraction via piecewise\nconvolutional neural networks. In Proceedings of the 2015\nconference on empirical methods in natural language pro-\ncessing (pp. 1753–1762). Lisbon, Portugal: Association\nfor Computational Linguistics.\nZhai, X., Oliver, A., Kolesnikov, A., & Beyer, L. (2019). S4l:\nSelf-supervised semi-supervised learning.\nZhang, A., Li, S., Cui, Y., Yang, W., Dong, R., & Hu, J.\n(2019). Limited data rolling bearing fault diagnosis with\nfew-shot learning. IEEE Access, 7, 110895-110904.\nZhang, A., Wang, H., Li, S., Cui, Y., Liu, Z., Yang, G., &\nHu, J. (2018). Transfer learning with deep recurrent neu-\nral networks for remaining useful life estimation. Applied\nSciences (Switzerland), 8(12).\nZhang, D., Qian, L., Mao, B., Huang, C., Huang, B., & Si,\nY. (2018). A data-driven design for fault detection of wind\nturbines using random forests and xgboost. IEEE Access,\n6, 21020-21031.\nZhang, H., Zhang, Q., Liu, J., & Guo, H.\n(2018).\nFault\ndetection and repairing for intelligent connected vehicles\n22\nINTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT\nbased on dynamic bayesian network model. IEEE Internet\nof Things Journal, 5(4), 2431-2440.\nZhang, Q., Lu, J., & Jin, Y. (2021, Feb 01). Artiﬁcial intel-\nligence in recommender systems. Complex & Intelligent\nSystems, 7(1), 439-457.\nZhang, S., Ye, F., Wang, B., & Habetler, T. G. (2019). Semi-\nsupervised learning of bearing anomaly detection via deep\nvariational autoencoders.\nZhang, S., Zhang, S., Wang, B., & Habetler, T. G. (2020).\nDeep learning algorithms for bearing fault diagnostics—a\ncomprehensive review. IEEE Access, 8, 29857-29881.\nZhang, T., Chen, J., Li, F., Zhang, K., Lv, H., He, S., & Xu, E.\n(2021). Intelligent fault diagnosis of machines with small\n& imbalanced data: A state-of-the-art review and possible\nextensions. ISA Transactions.\nZhang, Y., Jiang, H., Miura, Y., Manning, C. D., & Langlotz,\nC. P. (2020). Contrastive learning of medical visual repre-\nsentations from paired images and text.\nZhang, Z., Wu, Q., Wang, Y., & Chen, F. (2019). High-\nquality image captioning with ﬁne-grained and semantic-\nguided visual attention. IEEE Transactions on Multimedia,\n21(7), 1681-1693.\nZhao, K., Jiang, H., Wu, Z., & Lu, T. (2020). A novel transfer\nlearning fault diagnosis method based on manifold embed-\nded distribution alignment with a little labeled data. Jour-\nnal of Intelligent Manufacturing.\nZhong, S.-S., Fu, S., & Lin, L. (2019). A novel gas turbine\nfault diagnosis method based on transfer learning with cnn.\nMeasurement: Journal of the International Measurement\nConfederation, 137, 435-453.\nZhou, D., He, J., Yang, H., & Fan, W. (2018). Sparc: Self-\npaced network representation for few-shot rare category\ncharacterization. In (p. 2807-2816).\nZhou, Z.-H. (2017, 08). A brief introduction to weakly su-\npervised learning. National Science Review, 5(1), 44-53.\n23\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-12-11",
  "updated": "2022-10-20"
}