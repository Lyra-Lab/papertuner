{
  "id": "http://arxiv.org/abs/2112.09025v1",
  "title": "Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs",
  "authors": [
    "Ezgi Korkmaz"
  ],
  "abstract": "The use of deep neural networks as function approximators has led to striking\nprogress for reinforcement learning algorithms and applications. Yet the\nknowledge we have on decision boundary geometry and the loss landscape of\nneural policies is still quite limited. In this paper we propose a framework to\ninvestigate the decision boundary and loss landscape similarities across states\nand across MDPs. We conduct experiments in various games from Arcade Learning\nEnvironment, and discover that high sensitivity directions for neural policies\nare correlated across MDPs. We argue that these high sensitivity directions\nsupport the hypothesis that non-robust features are shared across training\nenvironments of reinforcement learning agents. We believe our results reveal\nfundamental properties of the environments used in deep reinforcement learning\ntraining, and represent a tangible step towards building robust and reliable\ndeep reinforcement learning agents.",
  "text": "Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across\nMDPs\nEzgi Korkmaz\nezgikorkmazk@gmail.com\nAbstract\nThe use of deep neural networks as function approximators\nhas led to striking progress for reinforcement learning algo-\nrithms and applications. Yet the knowledge we have on deci-\nsion boundary geometry and the loss landscape of neural poli-\ncies is still quite limited. In this paper we propose a frame-\nwork to investigate the decision boundary and loss landscape\nsimilarities across states and across MDPs. We conduct ex-\nperiments in various games from Arcade Learning Environ-\nment, and discover that high sensitivity directions for neu-\nral policies are correlated across MDPs. We argue that these\nhigh sensitivity directions support the hypothesis that non-\nrobust features are shared across training environments of\nreinforcement learning agents. We believe our results reveal\nfundamental properties of the environments used in deep rein-\nforcement learning training, and represent a tangible step to-\nwards building robust and reliable deep reinforcement learn-\ning agents.\n1\nIntroduction\nBuilding on the success of DNNs for image classiﬁcation,\ndeep reinforcement learning has seen remarkable advances\nin various complex environments Mnih et al. (2015); Schul-\nman et al. (2017); Lillicrap et al. (2015). Along with these\nsuccesses come new challenges stemming from the current\nlack of understanding of the structure of the decision bound-\nary and loss landscape of neural network policies. Notably, it\nhas been shown that the high sensitivity of DNN image clas-\nsiﬁers to imperceptible perturbations to inputs also occurs\nfor neural policies Huang et al. (2017); Lin et al. (2017);\nKorkmaz (2020). This lack of robustness is especially criti-\ncal for deep reinforcement learning, where the actions taken\nby the agent can have serious real-life consequences Levin\nand Carrie (2018).\nRecent work has shown that adversarial examples are a\nconsequence of the existence of non-robust (i.e. adversarial)\nfeatures of datasets used in image classiﬁer training Ilyas\net al. (2019). That is, there are certain features which are ac-\ntually useful in classifying the data, but are extremely sen-\nsitive to small perturbations, and thus incomprehensible to\nhumans.\nPublished in Association for the Advancement of Artiﬁcial Intelli-\ngence (AAAI) Conference on Artiﬁcial Intelligence 2022.\nThe existence in standard datasets of non-robust features,\nwhich both generalize well but are simultaneously highly\nsensitive to small perturbations, raise serious concerns about\nthe way that DNN models are currently trained and tested.\nOn the one hand, since non-robust features are actually use-\nful in classiﬁcation, current training methods which opti-\nmize for classiﬁcation accuracy have no reason to ignore\nthem. On the other hand, since these features can be al-\ntered with visually imperceptible perturbations they present\na formidable obstacle to constructing models that behave\nanything like humans. Instead, models trained in the pres-\nence of non-robust features are likely to have directions\nof high-sensitivity (or equivalently small margin) correlated\nwith these features.\nIn the reinforcement learning setting, where policies are\ngenerally trained in simulated environments, the presence\nof non-robust features leading to high-sensitivity directions\nfor neural network policies raises serious concerns about the\nability of these policies to generalize beyond their training\nsimulations. Consequently, identifying the presence of non-\nrobust features in simulated training environments and un-\nderstanding their effects on neural-network policies is an\nimportant ﬁrst step in designing algorithms for training re-\ninforcement learning agents which can perform well in real-\nworld settings.\nIn this paper we study the decision boundaries of neural\nnetwork policies in deep reinforcement learning. In particu-\nlar, we ask: how are high-sensitivity directions related to the\nactions taken across states for a neural network policy? Are\nhigh-sensitivity directions correlated between neural poli-\ncies trained in different MDPs? Do state-of-the-art adver-\nsarially trained deep reinforcement learning policies inherit\nsimilar high-sensitivity directions to vanilla trained deep re-\ninforcement learning policies? Do non-robust features exist\nin the deep reinforcement learning training environments?\nTo answer these questions, we propose a framework based\non identifying directions of low distance to the neural policy\ndecision boundary, and investigating how these directions\naffect the decisions of the agents across states and MDPs.\nOur main contributions are as follows:\n• We introduce a framework based on computing a high-\nsensitivity direction in one state, and probing the decision\nboundary of the neural policy along this direction as the\nneural policy is executed in a set of carefully controlled\narXiv:2112.09025v1  [cs.LG]  16 Dec 2021\nscenarios.\n• We examine the change in the action distribution of an\nagent whose input is shifted along a high-sensitivity di-\nrection, and show that in several cases these directions\ncorrespond to shifts towards the same action across dif-\nferent states. These results lend credence to the hypothe-\nsis that high-sensitivity directions for neural policies cor-\nrespond to non-robust discriminative features.\n• We investigate the state-of-the-art adversarially trained\ndeep reinforcement learning policies and show that ad-\nversarially trained deep reinforcement learning policies\nshare high sensitivity directions with vanilla trained deep\nreinforcement learning policies.\n• Via experiments in the Arcade Learning Environment we\nrigorously show that the high-sensitivity directions com-\nputed in our framework correlate strongly across states\nand in several cases across MDPs. This suggests that dis-\ntinct MDPs from standard baseline environments contain\ncorrelated non-robust features that are utilized by deep\nneural policies.\n2\nRelated Work and Background\n2.1\nDeep Reinforcement Learning\nIn this paper we examine discrete action space MDPs that\nare represented by a tuple: M = (S, A, P, r, γ, s0) where\nS is a set of states, A is a set of discrete actions, P\n:\nS ×A×S →R is the transition probability, r : S ×A →R\nis the reward function, γ is the discount factor, and s0 is the\ninitial state distribution. The agent interacts with the envi-\nronment by observing s ∈S, taking actions a ∈A, and\nreceiving rewards r : S × A →R. A policy π : S × A →R\nassigns a probability distribution over actions π(s, ·) to each\nstate s. The goal in reinforcement learning is to learn a pol-\nicy π that maximizes the expected cumulative discounted re-\nward R = E[PT −1\nt=0 γtr(st, at)] where at ∼π(st, ·). For an\nMDP M and policy π we call a sequence of state, action,\nreward, next state tuples, (si, ai, ri, s′\ni), that occurs when\nutilizing π in M an episode. We use pM,π to denote the\nprobability distribution over the episodes generated by the\nrandomness in M and the policy π. In Q-learning the goal of\nmaximizing the expected discounted cumulative rewards is\nachieved by building a state-action value function Q(s, a) =\nEs∼π(s,·) [P∞\nt=0 γtr(st, at)|s0 = s, a0 = a]. The function\nQ(s, a) is intended to represent the expected cumulative re-\nwards obtained by taking action a in state s, and in all future\nstates s′ taking the action a′ which maximizes Q(s′, a′).\n2.2\nAdversarial Perturbation Methods\nIn order to identify high-sensitivity directions for neural\npolicies we use methods designed to compute adversarial\nperturbations of minimal ℓp-norm. The ﬁrst paper to discuss\nadversarial examples for DNNs was Szegedy et al. (2014).\nSubsequently Goodfellow, Shelens, and Szegedy (2015) in-\ntroduced the fast gradient sign method (FGSM) which com-\nputes adversarial perturbations by maximizing the lineariza-\ntion of the cost function in the ℓ∞-ball of radius ϵ.\nxadv = x + ϵ ·\n∇xJ(x, y)\n∥∇xJ(x, y)∥p\n,\n(1)\nHere x is the clean input, y is the correct label for x, and J is\nthe cost function used to train the neural network classiﬁer.\nFurther improvements were obtained by Kurakin, Goodfel-\nlow, and Bengio (2016) by using FGSM gradients to itera-\ntively search within the ℓp-norm ball of radius ϵ.\nCurrently, the state-of-the-art method for computing min-\nimal adversarial perturbations is the formulation proposed\nby Carlini and Wagner (2017). In particular, Athalye, Car-\nlini, and Wagner (2018) and Tramer et al. (2020) showed\nthat the C&W formulation can overcome the majority of the\nproposed defense and detection algorithms. For this reason,\nin this work we create the adversarial perturbations mostly\nby utilizing the C&W formulation and its variants. In par-\nticular, in the deep reinforcement learning setup the C&W\nformulation is,\nmin\nsadv∈S c · J(sadv) + ∥sadv −s∥\n\n(2)\nwhere s is the unperturbed input, sadv is the adversarially\nperturbed input, and J(s) is the augmented cost function\nused to train the network. Note that in the deep reinforce-\nment learning setup the C&W formulation ﬁnds the mini-\nmum distance to a decision boundary in which the action\ntaken by the deep reinforcement learning policy is non-\noptimal. The second method we use to produce adversarial\nexamples is the ENR method Chen et al. (2018),\nmin\nsadv∈S c · J(sadv) + λ∥sadv −s∥+ λ∥sadv −s∥\n\n(3)\nBy adding ENR this method produces sparser perturbations\ncompared to the C&W formulation with similar ℓ2-norm.\n2.3\nDeep Reinforcement Learning and\nAdversarial Perspective\nThe ﬁrst work on adversarial examples in deep reinforce-\nment learning appeared in Huang et al. (2017) and Kos and\nSong (2017). These two concurrent papers demonstrated\nthat FGSM adversarial examples could signiﬁcantly de-\ncrease the performance of deep reinforcement learning poli-\ncies in standard baselines. Follow up work by Pattanaik et al.\n(2018) introduced an alternative to the standard FGSM by\nutilizing an objective function which attempts to increase\nthe probability of the worst possible action (i.e. the action\naw minimizing Q(s, a)).\nThe work of Lin et al. (2017) and Sun et al. (2020) at-\ntempts to exploit the difference in the importance of states\nacross time by only introducing adversarial perturbations at\nstrategically chosen moments. In these papers the perturba-\ntions are computed via the C&W formulation. Due to the\nvulnerabilities outlined in the papers mentioned above, there\nhas been another line of work on attempting to train agents\nthat are robust to adversarial perturbations. Initially Man-\ndlekar et al. (2017) introduced adversarial examples during\nreinforcement learning training to improve robustness. An\nalternate approach of Pinto et al. (2017) involves modelling\nthe agent and the adversary as playing a zero-sum game, and\ntraining both agent and adversary simultaneously. A simi-\nlar idea of modelling the agent vs the adversary appears in\nGleave et al. (2020), but with an alternative constraint that\nthe adversary must take natural actions in the simulated envi-\nronment rather than introducing ℓp-norm bounded perturba-\ntions to the agent’s observations. Most recently, Zhang et al.\n(2020) introduced the notion of a State-Adversarial MDP,\nand used this new deﬁnition to design an adversarial training\nalgorithm for deep reinforcement learning with more solid\ntheoretical motivation. Subsequently, several concerns have\nbeen raised regarding the problems introduced by adversar-\nial training (Korkmaz 2021a,b,c).\n3\nHigh-Sensitivity Directions\nThe approach in our paper is based on identifying directions\nof high sensitivity for neural policies. A ﬁrst attempt to de-\nﬁne the notion of high-sensitivity direction might be to say\nthat v is a high sensitivity direction for π at state s if for\nsome small ϵ > 0\narg max\na\nπ(s + ϵv, a) ̸= arg max\na\nπ(s, a)\nbut for a random direction r = ∥v∥\n∥g∥g with g ∼N (0, I)\narg max\na\nπ(s + ϵr, a) = arg max\na\nπ(s, a)\nwith high probability over the random vector r. In other\nwords, v is a high sensitivity direction if small perturbations\nalong v change the action taken by policy π in state s, but\nperturbations of the same magnitude in a random direction\ndo not. There is a subtle issue with this deﬁnition however.\nIn reinforcement learning there are no ground truth la-\nbels for the correct action for an agent to take in a given\nstate. While the above deﬁnition requires that the policy\nswitches which action is taken when the input is shifted a\nsmall amount in the v direction, there is no a priori reason\nthat this shift will be bad for the agent.\nInstead, the only objective metric we have of the perfor-\nmance of a policy is the ﬁnal reward obtained by the agent.\nTherefore we deﬁne high-sensitivity direction as follows:\nDeﬁnition 3.1. Recall that R = PT −1\nt=0 γtr(st, at) is the\ncumulative reward. A vector v is a high-sensitivity direction\nfor a policy π if there is an ϵ > 0 such that\nEat∼π(st+ϵv,·)[R] ≪Eat∼π(st,·)[R]\nbut for a random direction r = ∥v∥\n∥g∥g with g ∼N (0, I)\nEat∼π(st+ϵr,·)[R] ≈Eat∼π(st,·)[R].\nIn short, v is a high-sensitivity direction if small pertur-\nbations along v signiﬁcantly reduce the expected cumulative\nrewards of the agent, but the same magnitude perturbations\nalong random directions do not. This deﬁnition ensures not\nonly that small perturbations in the direction v cross the de-\ncision boundary of the neural policy in many states s, but\nalso that the change in policy induced by these perturbations\nhas a semantically meaningful impact on the agent.\nTo see how high-sensitivity directions arise naturally con-\nsider the linear setting where we think of s as a vector of\nfeatures in Rn, and for each action a we associate a weight\nvector wa ∈Rn. The policy in this setting is given by de-\nterministically taking the action arg maxa∈A⟨wa, s⟩in state\ns. We assume that the weight vectors are not too correlated\nwith each other: ⟨wa, wa′⟩< α · min{∥wa∥2 , ∥wa′∥2}\nfor some constant α < 1. We also assume that the lengths\nof the weight vectors are not too different: for all a, a′\n∥wa∥≤β ∥wa′∥for some constant β > 1.\nSuppose that there is a set of states S1 accounting for a\nsigniﬁcant fraction of the total rewards such that: (1) by tak-\ning the optimal action a∗(s) for s ∈S1 the agent receives\nreward 1, and (2) there is one action b, such that taking ac-\ntion b gives reward 0 for a signiﬁcant fraction of states in\nS1. We claim that, assuming that there is a constant gap be-\ntween ⟨wa∗(s), s⟩and ⟨wa, s⟩for a ̸= a∗(s) then v = wb is\na high-sensitivity direction.\nProposition 3.2. Assume that there exist constants c, d > 0\nsuch that c < ⟨wa∗(s), s⟩−⟨wa, s⟩< d for all a ̸= a∗(s)\nand s ∈S1. Then wb is a high-sensitivity direction.\nProof. See Appendix A.\nThe empirical results in the rest of the paper conﬁrm that\nhigh-sensitivity directions do occur in deep neural policies,\nand further explore the correlations between these directions\nacross states and across MDPs.\n4\nFramework for Investigating\nHigh-Sensitivity Directions\nIn this section we introduce a framework to seek answers for\nthe following questions:\n• Are high sensitivity directions shared amongst states in\nthe same MDP?\n• Is there a correlation between high sensitivity directions\nacross MDPs and across algorithms?\n• Do non-robust features exist in the deep reinforcement\nlearning training environments?\nIt is important to note that the goal of this framework\nis not to demonstrate the already well-known fact that ad-\nversarial perturbations are a problem for deep reinforce-\nment learning. Rather, we are interested in determining\nwhether high-sensitivity directions are correlated across\nstates, across MDPs and across training algorithms. The\npresence of such correlation would indicate that non-robust\nfeatures are an intrinsic property of the training environ-\nments themselves. Thus, understanding the extent to which\nnon-robust features correlate in these ways can serve as a\nguide for how we should design algorithms and training en-\nvironments to improve robustness.\nGiven a policy π and state s we compute a direction\nv(s, π(s, ·)) of minimal margin to the decision boundary\nπ(s, ·) in state s. We ﬁx a bound κ such that perturbations\nof norm κ in a random direction r with ∥r∥2 = 1 have in-\nsigniﬁcant impact on the rewards of the agent.\nEat∼π(st+κr,·)[R] ≈Eat∼π(st,·)[R].\nOur framework is based on evaluating the sensitivity of a\nneural policy along the direction v in a sequence of increas-\ningly general settings. Each setting is deﬁned by (1) the state\ns chosen to compute v(s, π(s, ·)), and (2) the states s′ where\na small perturbation along v(s, π(s, ·)) is applied when de-\ntermining the action taken.\nDeﬁnition 4.1. Individual state setting, Aindividual, is the set-\nting where in each state s a new direction is computed and a\nperturbation along that direction is applied. In each state si\nwe compute\ns∗\ni = si + κ ·\nv(si, π(si, ·))\n∥v(si, π(si, ·))∥2\n(4)\nand then take an action determined by π(s∗\ni , ·).\nThe individual state setting acts as a baseline to which we\ncan compare the expected rewards of an agent with inputs\nperturbed along a single direction v. If the decline in re-\nwards for an agent whose inputs are perturbed along a single\ndirection is close to the decline in rewards for the individual\nstate setting, this can be seen as evidence that the direction\nv satisﬁes the ﬁrst condition of Deﬁnition 3.1.\nDeﬁnition 4.2. Episode independent random state setting,\nArandom\ne\n, is the setting where a random state s is sampled\nfrom a random episode e, and the perturbation v(s, π(s, ·))\nis applied to all the states visited in another episode e′. Sam-\nple e ∼pM and s ∼e. Given an episode e′ ∼pM, in each\nstate s′\ni of e′ we compute\ns∗\ni = s′\ni + κ ·\nv(s, π(s, ·))\n∥v(s, π(s, ·))∥2\n(5)\nand then take an action determined by π(s∗\ni , ·).\nThe episode independent random state setting is designed\nto identify high-sensitivity directions v. By comparing the\nreturn in this setting with the case of a random direction r\nas described in Deﬁnition 3.1 we can decide whether v is a\nhigh-sensitivity direction.\nDeﬁnition 4.3. Environment independent random state set-\nting, Arandom\nM\n, is the setting where a random state s(M) is\nsampled from a random episode of the MDP M, and the\nperturbation v(s(M), π(s(M), ·)) is applied to all the states\nvisited in an episode of a different MDP M′. Sample e ∼M\nand s(M) ∼e. Given an episode e′ ∼pM′, in each state s′\ni\nwe compute\ns∗\ni = s′\ni + κ ·\nv(s(M), π(s(M), ·))\n∥v(s(M), π(s(M), ·)))∥2\n(6)\nand then take an action determined by π(s∗\ni , ·).\nThe environment independent random state setting is de-\nsigned to test whether high-sensitivity directions are shared\nacross MDPs. As with the episode independent setting, com-\nparing with perturbations in a random direction allows us to\nconclude whether a high-sensitivity direction for M is also\na high-sensitivity direction for M′.\nDeﬁnition 4.4. Algorithm independent random state setting,\nArandom\nalg\n, is the setting where a random state s(M) is sampled\nfrom a random episode of the MDP M, and the perturbation\nv(s(M), π(s(M), ·)) is applied to all the states visited in\nan episode for a policy π′ trained with a different algorithm.\nSample e ∼M and s(M) ∼e. Given an episode e′ ∼pM′,\nin each state s′\ni we compute\ns∗\ni = s′\ni + κ ·\nv(s(M), π(s(M), ·))\n∥v(s(M), π(s(M), ·)))∥2\n(7)\nand then take an action determined by π′(s∗\ni , ·).\nNote that in the above deﬁnition we allow M = M′,\nwhich corresponds to transferring perturbations between\ntraining algorithms in the same MDP. We will refer to the\nsetting where M ̸= M′ (i.e. transferring between algo-\nrithms and between MDPs at the same time) as algorithm\nand environment independent, and denote this setting by\nArandom\nalg+M. Algorithm 1 gives the implementation for Deﬁni-\ntion 4.4.\nAlgorithm 1: High-sensitivity directions with Arandom\nalg\nInput: Episode e of MDP M, state s(M) ∼e, policy\nπ(s, ·), policy π′(s, ·), episode e′ of MDP M′, perturbation\nbound κ\nfor s′\ni in e′ do\ns∗\ni ←s′\ni + κ ·\nv(s(M), π(s(M), ·))\n∥v(s(M), π(s(M), ·))∥2\na∗(s∗\ni ) = arg maxa π′(s∗\ni , a)\nend for\nreturn Ea∗(s∗\ni )[R]\n5\nExperiments\nThe Arcade Learning Environment (ALE) is used as a stan-\ndard baseline to compare and evaluate new deep reinforce-\nment learning algorithms as they are developed (Hasselt,\nGuez, and Silver 2016; Mnih et al.; Wang et al. 2016; Fe-\ndus et al. 2020; Rowland et al. 2019; Mnih et al. 2015;\nHessel et al. 2018; Kapturowski et al. 2019; Dabney et al.\n2018; Dabney, Ostrovski, and Barreto 2021; Xu et al. 2020;\nSchmitt, Hessel, and Simonyan 2020). As a result, any sys-\ntematic issues with these environments are of critical im-\nportance. Any bias within the environment that favors some\nalgorithms over others risks inﬂuencing the direction of re-\nsearch in deep reinforcement learning for the next several\nyears. This inﬂuence could take the form of diverting re-\nsearch effort away from promising algorithms, or giving a\nfalse sense of security that certain algorithms will perform\nwell under different conditions. Thus, for these reasons it\nis essential to investigate the existence of non-robust fea-\ntures and the correlations between high-sensitivity direc-\ntions within the ALE.\nIn our experiments agents are trained with Double Deep\nQ-Network (DDQN) proposed by Wang et al. (2016) with\nprioritized experience replay Schaul et al. (2016) in the\nALE introduced by Bellemare et al. (2013) with the OpenAI\nbaselines version Brockman et al. (2016). The state-of-the-\nart adversarially trained deep reinforcement learning polices\nTable 1: Impacts of C&W and ENR formulation for the proposed framework for investigating high sensitivity directions cross-\nstates and cross-MDPs.\nSettings [Adversarial Technique]\nBankHeist\nRoadRunner\nJamesBond\nCrazyClimber\nTimePilot\nPong\nAindividual [ENR]\n0.646±0.018\n0.821± 0.046\n0.098±0.047\n0.750±0.030\n0.815±0.049\n0.995±0.003\nAindividual [C&W]\n0.694±0.045\n0.876±0.017\n0.038±0.050\n0.646±0.056\n0.334±0.107\n1.0±0.000\nArandom\ne\n[ENR]\n0.764±0.022\n0.961±0.008\n0.612±0.040\n0.980± 0.002\n0.517±0.085\n1.0±0.000\nArandom\ne\n[C&W]\n0.118±0.041\n0.649±0.041\n0.019±0.058\n0.956±0.003\n0.228±0.097\n1.0±0.000\nArandom\nM\n[ENR]\n0.496±0.055\n0.816±0.034\n0.910±0.051\n0.993±0.002\n0.312±0.118\n0.946±0.017\nArandom\nM\n[C&W]\n0.022±0.0349\n0.919±0.018\n0.304±0.038\n0.036±0.028\n0.017±0.073\n0.966±0.008\nGaussian\n0.078±0.037\n0.027±0.025\n0.038±0.063\n0.054±0.025\n0.031±0.063\n0.045±0.018\nutilize the State-Adversarial DDQN (SA-DDQN) algorithm\nproposed by Zhang et al. (2020) with prioritized experience\nreplay Schaul et al. (2016). We average over 10 episodes in\nour experiments. We report the results with standard error of\nthe mean throughout the paper. The impact on an agent is\ndeﬁned by normalizing the performance drop as follows\nImpact = Scoremax −Scoreset\nScoremax −Scoremin\n.\n(8)\nHere Scoremax is the score of the baseline trained agent fol-\nlowing the learned policy in a clean run of the agent in the\nenvironment, Scoreset is the score of the agent with settings\nintroduced in Section 4, and Scoremin is the score the agent\nreceives when choosing the worst possible action in each\nstate. All scores are recorded at the end of an episode.\n5.1\nInvestigating High-Sensitivity Directions\nTable 1 shows impact values in games from the Atari Base-\nlines for the different settings from our framework utiliz-\ning the C&W formulation, ENR formulation and Gaussian\nnoise respectively to compute the directions v. In all the ex-\nperiments we set the ℓ2-norm bound κ in our framework to\na level so that Gaussian noise with ℓ2-norm κ has insignif-\nicant impact. In Table 1 we show the Gaussian noise im-\npacts on the environments of interest with the same ℓ2-norm\nbound κ used in the ENR formulation and C&W formula-\ntions. Therefore, high impact for the Arandom\ne\nand Arandom\nM\nset-\nting in these experiments indicates that we have identiﬁed a\nhigh-sensitivity direction. The results indicate that it is gen-\nerally true that a direction of small margin corresponds to a\nhigh-sensitivity direction. However, the ENR formulation is\nmore consistent in identifying high-sensitivity directions.\nIt is extremely surprising to notice that the impact of\nAindividual in JamesBond is distinctly lower than Arandom\ne\n. The\nreason for this in JamesBond is that Arandom\ne\nconsistently\nshifts all actions towards action 12 while Aindividual causes\nthe agent to choose different actions in every state. See sec-\ntion 5.2 for more details. We observe that this consistent shift\ntowards one particular action results in a larger impact on the\nagent’s performance in certain environments. In JamesBond,\nthere are obstacles that the agent must jump over in order to\navoid death, and consistently taking action 12 prevents the\nagent from jumping far enough. In CrazyClimber, the con-\nsistent shift towards one action results in the agent getting\nstuck in one state where choosing any other action would\nlikely free it.1\nWe further investigated the state-of-the-art adversarially\ntrained deep reinforcement learning policies as well as archi-\ntectural differences in variants of DQN with our framework\nin Section 5.4 and in Appendix C. We have found consis-\ntent results on identifying high-sensitivity directions inde-\npendent from the training algorithms and the architectural\ndifferences.\n5.2\nShifts in Actions Taken\nIn this subsection we investigate more closely exactly how\nperturbations along high-sensitivity directions affect the ac-\ntions taken by the agent. We then argue that the results of this\nsection suggest that high-sensitivity directions correspond to\nmeaningful non-robust features used by the agent to make\ndecisions. Note that in the reinforcement learning setting it\nis somewhat subtle to argue about the relationship between\nperturbations, non-robust features and actions taken. For ex-\nample, suppose that the direction v does indeed correspond\nto a non-robust feature which the neural policy uses to de-\ncide which action to take in several states. The presence\nof the feature may indicate that different actions should be\ntaken in state s versus state s′, even though the feature it-\nself is the same. Thus, even if v corresponds to a non-robust\nfeature, this may not be detectable by looking at which ac-\ntions the agent takes in state s + ϵv versus state s. However,\nthe converse still holds: if there is a noticeable shift across\nmultiple states towards one particular action a under the per-\nturbation, then this constitutes evidence that the direction v\ncorresponds to a non-robust feature that the neural policy\nuses to decide to take action a.\nIn the OpenAI Atari baseline version of the ALE, the dis-\ncrete action set is numbered from 0 to |A|. For each episode\nwe collected statistics on the probability P(a) of taking ac-\ntion a in the following scenarios:\n• Pbase(a) - the fraction of states in which action a ∈A is\ntaken in an episode with no perturbation added.\n• Pshift(a) - the fraction of states in which action a ∈A is\ntaken in an episode with the perturbation added.\n1See Appendix B for more detailed information on this issue,\nand the visualizations of the state representations of the deep rein-\nforcement learning policies for these cases.\n• Pshift(a, b) - the fraction of states in which action a ∈\nA would have been taken by the agent if there were no\nperturbation, but action b ∈A was taken due to the added\nperturbation.\n• Pcontrol(a) = P\nb Pshift(a, b) - the fraction of states in\nwhich action a ∈A would have been taken by the agent\nif there were no perturbation, in an episode with the per-\nturbation added.\nPong\nBankHeist\nCrazyClimber\nRoadRunner\nTimePilot\nJamesBond\nFigure 1: Action statistics for episode independent random\nstate setting Arandom\ne\nand environment independent random\nstate setting Arandom\nM\ndeﬁned in Section 4 with ENR formu-\nlation for Pcontrol(a) , Pbase(a) and Pshift(a).\nIn Figure 1 we observe that in Roadrunner, JamesBond,\nand TimePilot Pcontrol(a) and Pbase(a) are similar. That is, for\nthese environments the distribution on which action would\nbe taken without perturbation does not change much. How-\never, signiﬁcant changes do occur in which action actually\nis taken due to the perturbation along the high-sensitivity\ndirection. In Pong, BankHeist, and CrazyClimber the pres-\nence of the perturbation additionally causes a large change\nin which action would be taken.\nIn Figure 2 we show the heatmaps corresponding to\nPshift(a, b). In these heatmaps we show the fraction of states\nin which the agent would have taken action a, but instead\ntook action b due to the perturbation. In some environ-\nments there is a dominant action shift towards one par-\nticular action b from one particular control action a as in\nBankHeist, CrazyClimber and TimePilot. The implication\nfor these games is that the high-sensitivity direction v cor-\nresponds to a non-robust feature that consistently shifts the\nneural policy’s decision from action a to action b.\nIn some games there are one or more clear bright columns\nwhere all the control actions are shifted towards the same\nparticular action, for instance in JamesBond. The presence\nof a bright column indicates that, for many different input\nstates (where the agent would have taken a variety of dif-\nferent actions), the high-sensitivity direction points across a\ndecision boundary toward one particular action. As argued\nin the beginning of the section, these results suggest that\nthe high-sensitivity direction found for JamesBond actually\ncorresponds to a meaningful non-robust feature used by the\nneural policy. Closer examination of the semantics of Road-\nRunner reveals a similar shift towards a single type of action.\nIn Roadrunner, the actions 4 and 12 correspond to left\nJamesBond\nBankHeist\nRoadRunner\nTimePilot\nCrazyClimber\nPong\nFigure 2: Heat map of Pshift(a, b) deﬁned in Section 5.2 for\nepisode independent random state setting Arandom\ne\nand envi-\nronment independent random state setting Arandom\nM\nwith ENR\nand leftﬁre, both of which move the player directly to the\nleft2. These actions correspond to two bright columns in\nthe heatmap indicating signiﬁcant shift towards the actions\nmoving the player directly left from several other actions.\nAs with the case of JamesBond, this suggests that the high-\nsensitivity direction v found in each of these games corre-\nsponds to a non-robust feature that the neural policy uses\nto decide on movement direction. To make the results de-\nscribed above more quantitative we report the percentage\nshift towards a particular action type in Table 2. Speciﬁcally,\nwe compute the total probability mass τ on actions which\nhave been shifted from what they would have been, the total\nprobability mass ρ shifted towards the particular action type,\nand report ρ\nτ .\nTable 2: Percentage of total action shift.\nMDP\nAction Number\nPercentage Shift\n\u0010ρ\nτ\n\u0011\nBankHeist\nAction 16\n0.380\nRoadRunner\nActions 4 and 12\n0.279\nJamesBond\nAction 12\n0.345\nCrazyClimber\nAction 1\n0.708\nTimePilot\nAction 1\n0.185\n5.3\nCross-MDP Correlations in High-sensitivity\nDirections\nIn this section we investigate more closely the correlation\nof high-sensitivity directions between MDPs. In these ex-\nperiments we utilize environment independent random state\nsetting Arandom\nM\nwhere the perturbation is computed from a\nrandom state of a random episode of one MDP, and then\nadded to a completely different MDP. In Figure 3 we show\nthe impacts for the Arandom\nM\nsetting in six different environ-\nments. Each row shows the environment where the perturba-\ntion is added, and each column shows the environment from\nwhich the perturbation is computed. Note that the diagonal\nof the matrix corresponds to Arandom\ne\n, and thus provides a\n2See Appendix B for more details on action names and mean-\nings for each game.\nbaseline for the impact value of the high-sensitivity direction\nin the MDP from which it was computed. The off diagonal\nelements represent the degree to which the direction com-\nputed in one MDP remains a high-sensitivity direction in\nanother MDP. Perceptual similarity distances are computed\nvia LPIPS Zhang et al. (2018).\nFigure 3: Cross-MDP high sensitivity similarities and per-\nceptual similarities for Arandom\nM\nwith ENR formulation.\nOne can observe intriguing properties of the Atari base-\nlines where certain environments are very likely to share\nhigh-sensitivity directions. For example, the high-sensitivity\ndirections computed from RoadRunner are high-sensitivity\ndirections in other MDPs and the high-sensitivity directions\ncomputed from other MDPs are high-sensitivity directions\nfor RoadRunner. Additionally, in half of the environments\nhigher or similar impact is achieved when using a direc-\ntion from a different environment rather than using one\ncomputed in the environment itself. Recall that the results\nof Section 5.2 imply that high-sensitivity directions corre-\nspond to non-robust features within a single MDP. There-\nfore the high level of correlation in high-sensitivity direc-\ntions across MDPs is an indication that deep reinforcement\nlearning agents are learning representations that have corre-\nlated non-robust features across different environments.\n5.4\nShared Non-Robust Features and Adversarial\nTraining\nIn this section we investigate the correlations of the high-\nsensitivity directions between state-of-the-art adversarially\ntrained deep reinforcement learning policies and vanilla\ntrained deep reinforcement learning policies within the same\nMDP and across MDPs. In particular, Table 3 demonstrates\nthe performance drop of the policies with settings AGaussian,\nArandom\nalg\nand Arandom\nalg+M deﬁned in Section 4. In more detail,\nTable 3 shows that a perturbation computed from a vanilla\ntrained deep reinforcement learning policy trained in the\nCrazyClimber MDP decreases the performance by 54.6%\nwhen it is introduced to the observation system of the state-\nof-the-art adversarially trained deep reinforcement learning\npolicy trained in the RoadRunner MDP. Similarly, a per-\nturbation computed from a vanilla trained deep reinforce-\nment learning policy trained in the CrazyClimber MDP de-\ncreases the performance by 65.9% when it is introduced\nto the observation system of the state-of-the-art adversar-\nially trained deep reinforcement learning policy trained in\nthe Pong MDP. This shows that non-robust features are\nnot only shared across states and across MDPs, but also\nshared across different training algorithms. The state-of-the-\nart adversarially trained deep reinforcement learning poli-\ncies learn similar non-robust features which carry high sen-\nsitivity towards certain directions. It is quite concerning that\nalgorithms speciﬁcally focused on solving adversarial vul-\nnerability problems are still learning similar non-robust fea-\ntures as vanilla deep reinforcement learning training algo-\nrithms. This fact not only presents serious security concerns\nfor adversarially trained models, but it posits a new research\nproblem on the environments in which we train.\nTable 3: Impacts of AGaussian, Arandom\nalg\nand Arandom\nalg+M where the\nperturbation is computed from a policy trained with DDQN\nand introduced to the observation system of the state-of-the-\nart adversarially trained deep reinforcement learning policy\nMDPs\nAGaussian\nArandom\nalg\nArandom\nalg+M\nRoadRunner\n0.023±0.058\n0.397±0.024\n0.546±0.014\nPong\n0.019±0.007\n1.0±0.000\n0.659±0.069\nBankHeist\n0.061±0.012\n0.758±0.042\n0.241±0.009\n6\nConclusion\nIn this paper we focus on several questions: (i) Do neu-\nral policies share high sensitivity directions amongst differ-\nent states? (ii) Are the high sensitivity directions correlated\nacross MDPs and across algorithms? (iii) Do deep reinforce-\nment learning agents learn non-robust features from the deep\nreinforcement learning training environments? To be able to\ninvestigate these questions we introduce a framework con-\ntaining various settings. Using this framework we show that\na direction of small margin to the decision boundary in a\nsingle state is often a high-sensitivity direction for the deep\nneural policy. We then investigate more closely how pertur-\nbations along high-sensitivity directions change the actions\ntaken by the agent, and ﬁnd that in some cases they shift\nthe decisions of the policy towards one particular action. We\nargue that this suggests that high sensitivity directions cor-\nrespond to non-robust features used by the policy to make\ndecisions. Furthermore, we show that a high-sensitivity di-\nrection for one MDP is likely to be a high-sensitivity direc-\ntion for another MDP in the Arcade Learning Environment.\nMoreover, we show that the state-of-the-art adversarially\ntrained deep reinforcement learning policies share the exact\nsame high-sensitivity directions with vanilla trained deep re-\ninforcement learning policies. We systematically show that\nthe non-robust features learnt by deep reinforcement learn-\ning policies are decoupled from states, MDPs, training algo-\nrithms and the architectural differences. Rather these high-\nsensitivity directions are a part of the learning environment\nresulting from learning non-robust features. We believe that\nthis cross-MDP correlation of high-sensitivity directions is\nimportant for understanding the decision boundaries of neu-\nral policies. Furthermore, the fact that neural policies learn\nnon-robust features that are shared across baseline deep rein-\nforcement learning training environments is crucial for im-\nproving and investigating the robustness and generalization\nof deep reinforcement learning agents.\nReferences\nAthalye, A.; Carlini, N.; and Wagner, D. A. 2018.\nOb-\nfuscated Gradients Give a False Sense of Security: Cir-\ncumventing Defenses to Adversarial Examples.\nIn Dy,\nJ. G.; and Krause, A., eds., Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML 2018,\nStockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018,\nvolume 80 of Proceedings of Machine Learning Research,\n274–283. PMLR.\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The arcade learning environment: An evaluation plat-\nform for general agents. Journal of Artiﬁcial Intelligence\nResearch., 253–279.\nBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;\nSchulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.\narXiv:1606.01540.\nCarlini, N.; and Wagner, D. 2017. Towards Evaluating the\nrobustness of neural networks. In 2017 IEEE Symposium on\nSecurity and Privacy (SP), 39–57.\nChen, P.; Sharma, Y.; Zhang, H.; Yi, J.; and Hsieh, C. 2018.\nEAD: Elastic-Net Attacks to Deep Neural Networks via Ad-\nversarial Examples. 10–17.\nDabney,\nW.;\nOstrovski,\nG.;\nand\nBarreto,\nA.\n2021.\nTemporally-Extended ϵ-Greedy Exploration. In 9th Interna-\ntional Conference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021. OpenReview.net.\nDabney, W.; Rowland, M.; Bellemare, M. G.; and Munos, R.\n2018. Distributional Reinforcement Learning With Quantile\nRegression. In McIlraith, S. A.; and Weinberger, K. Q., eds.,\nProceedings of the Thirty-Second AAAI Conference on Ar-\ntiﬁcial Intelligence, (AAAI-18), the 30th innovative Applica-\ntions of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI\nSymposium on Educational Advances in Artiﬁcial Intelli-\ngence (EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, 2892–2901. AAAI Press.\nFedus, W.; Ramachandran, P.; Agarwal, R.; Bengio, Y.;\nLarochelle, H.; Rowland, M.; and Dabney, W. 2020. Revis-\niting Fundamentals of Experience Replay. In Proceedings\nof the 37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume 119\nof Proceedings of Machine Learning Research, 3061–3071.\nPMLR.\nGleave, A.; Dennis, M.; Wild, C.; Neel, K.; Levine, S.; and\nRussell, S. 2020. Adversarial Policies: Attacking Deep Re-\ninforcement Learning. International Conference on Learn-\ning Representations ICLR.\nGoodfellow, I.; Shelens, J.; and Szegedy, C. 2015. Explaning\nand Harnessing Adversarial Examples. International Con-\nference on Learning Representations.\nHasselt, H. v.; Guez, A.; and Silver, D. 2016. Deep Rein-\nforcement Learning with Double Q-learning. In Thirtieth\nAAAI conference on artiﬁcial intelligence.\nHessel, M.; Modayil, J.; van Hasselt, H.; Schaul, T.; Ostro-\nvski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M. G.;\nand Silver, D. 2018. Rainbow: Combining Improvements\nin Deep Reinforcement Learning.\nIn McIlraith, S. A.;\nand Weinberger, K. Q., eds., Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), 3215–3222. AAAI Press.\nHuang, S.; Papernot, N.; Goodfellow, Y., Ian an Duan; and\nAbbeel, P. 2017. Adversarial Attacks on Neural Network\nPolicies. Workshop Track of the 5th International Confer-\nence on Learning Representations.\nIlyas, A.; Santurkar, S.; Tsipras, D.; Engstrom, L.; Tran,\nB.; and Madry, A. 2019.\nAdversarial Examples Are Not\nBugs, They Are Features. In Wallach, H. M.; Larochelle,\nH.; Beygelzimer, A.; d’Alch´e-Buc, F.; Fox, E. B.; and Gar-\nnett, R., eds., Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, 125–136.\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2019.\nRecurrent Experience Replay in Dis-\ntributed Reinforcement Learning. In 7th International Con-\nference on Learning Representations, ICLR 2019, New Or-\nleans, LA, USA, May 6-9, 2019. OpenReview.net.\nKorkmaz, E. 2020. Nesterov Momentum Adversarial Per-\nturbations in the Deep Reinforcement Learning Domain. In-\nternational Conference on Machine Learning, ICML 2020,\nInductive Biases, Invariances and Generalization in Rein-\nforcement Learning Workshop.\nKorkmaz, E. 2021a.\nAdversarial Training Blocks Gener-\nalization in Neural Policies. International Conference on\nLearning Representation (ICLR) Robust and Reliable Ma-\nchine Learning in the Real World Workshop.\nKorkmaz, E. 2021b. Inaccuracy of State-Action Value Func-\ntion for Non-Optimal Actions in Adversarially Trained Deep\nNeural Policies. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR)\nWorkshops, 2323–2327.\nKorkmaz, E. 2021c. Investigating Vulnerabilities of Deep\nNeural Policies. Conference on Uncertainty in Artiﬁcial In-\ntelligence (UAI).\nKos, J.; and Song, D. 2017. Delving Into Adversarial At-\ntacks on Deep Policies. International Conference on Learn-\ning Representations.\nKurakin, A.; Goodfellow, I.; and Bengio, S. 2016.\nAd-\nversarial examples in the physical world.\narXiv preprint\narXiv:1607.02533.\nLevin, S.; and Carrie, J. 2018. Self-driving Uber kills Ari-\nzona woman in ﬁrst fatal crash involving pedestrian. The\nGuardian.\nLillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;\nTassa, Y.; Silver, D.; and Wierstra, D. 2015.\nContinuous\ncontrol with deep reinforcement learning.\narXivpreprint\narXiv:1509.02971.\nLin, Y.-C.; Zhang-Wei, H.; Liao, Y.-H.; Shih, M.-L.; Liu,\ni.-Y.; and Sun, M. 2017. Tactics of Adversarial Attack on\nDeep Reinforcement Learning Agents. Proceedings of the\nTwenty-Sixth International Joint Conference on Artiﬁcial In-\ntelligence, 3756–3762.\nMandlekar, A.; Zhu, Y.; Garg, A.; Fei-Fei, L.; and Savarese,\nS. 2017. Adversarially Robust Policy Learning: Active Con-\nstruction of Physically-Plausible Perturbations. In Proceed-\nings of the IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS), 3932–3939.\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap,\nT. P.; Harley, T.; Silver, D.; and Kavukcuoglu, K. ???? Asyn-\nchronous Methods for Deep Reinforcement Learning.\nIn\nBalcan, M.; and Weinberger, K. Q., eds., Proceedings of the\n33nd International Conference on Machine Learning, ICML\n2016, New York City, NY, USA, June 19-24, 2016.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness, J.; Bellemare, a. G.; Graves, A.; Riedmiller, M.; Fidje-\nland, A.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.;\nand Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. Nature, 518: 529–533.\nPattanaik, A.; Tang, Z.; Liu, S.; and Gautham, B. 2018.\nRobust Deep Reinforcement Learning with Adversarial At-\ntacks.\nIn Proceedings of the 17th International Confer-\nence on Autonomous Agents and MultiAgent Systems, 2040–\n2042.\nPinto, L.; Davidson, J.; Sukthankar, R.; and Gupta, A. 2017.\nRobust Adversarial Reinforcement Learning. International\nConference on Learning Representations ICLR.\nRowland, M.; Dadashi, R.; Kumar, S.; Munos, R.; Belle-\nmare, M. G.; and Dabney, W. 2019. Statistics and Samples\nin Distributional Reinforcement Learning.\nIn Chaudhuri,\nK.; and Salakhutdinov, R., eds., Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, volume 97\nof Proceedings of Machine Learning Research, 5528–5536.\nPMLR.\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay.\nIn 4th International Con-\nference on Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016, Conference Track Proceedings.\nSchmitt, S.; Hessel, M.; and Simonyan, K. 2020. Off-Policy\nActor-Critic with Shared Experience Replay. In Proceedings\nof the 37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume 119\nof Proceedings of Machine Learning Research, 8545–8554.\nPMLR.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv:1707.06347v2 [cs.LG].\nSun, J.; Zhang, T.; Xie, X.; Ma, L.; Zheng, Y.; Chen, K.;\nand Liu, Y. 2020. Stealthy and Efﬁcient Adversarial Attacks\nagainst Deep Reinforcement Learning. Association for the\nAdvancement of Artiﬁcal intelligence (AAAI).\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\nD.; Goodfellow, I.; and Fergus, R. 2014. Intriguing proper-\nties of neural networks. In Proceedings of the International\nConference on Learning Representations (ICLR).\nTramer, F.; Carlini, N.; Brendel, W.; and Madry, A. 2020.\nOn Adaptive Attacks to Adversarial Example Defenses.\nNeurIPS.\nWang, Z.; Schaul, T.; Hessel, M.; Van Hasselt, H.; Lanctot,\nM.; and De Freitas, N. 2016. Dueling network architectures\nfor deep reinforcement learning. Internation Conference on\nMachine Learning ICML., 1995–2003.\nXu, Z.; van Hasselt, H. P.; Hessel, M.; Oh, J.; Singh, S.;\nand Silver, D. 2020. Meta-Gradient Reinforcement Learn-\ning with an Objective Discovered Online.\nIn Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds.,\nAdvances in Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nZhang, H.; Chen, H.; Xiao, C.; Li, B.; Liu, M.; Boning,\nD. S.; and Hsieh, C. 2020.\nRobust Deep Reinforcement\nLearning against Adversarial Perturbations on State Obser-\nvations. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Bal-\ncan, M.; and Lin, H., eds., Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Infor-\nmation Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nZhang, R.; Isola, P.; Efros, A.; Shechtman, E.; and Wang, O.\n2018. The Unreasonable Effectiveness of Deep Features as\na Perceptual Metric. Conference on Computer Vision and\nPattern Recognition (CVPR).\nA\nHigh Sensitivity Directions\nProposition A.1. Assume that there exist constants c, d > 0\nsuch that c < ⟨wa∗(s), s⟩−⟨wa, s⟩< d for all a ̸= a∗(s)\nand s ∈S1. Then wb is a high-sensitivity direction.\nProof. By assumption for all a ∈A and s ∈S1 we have\n⟨wa −wb, s⟩≤⟨wa∗(s) −wb, s⟩< d\nTherefore setting ϵ =\n2d\n(1−α)∥wb∥2 we have\n⟨s + ϵwb, wa −wb⟩< d + ϵα ∥wb∥2 −ϵ ∥wb∥2\n= d −(1 −α)ϵ ∥wb∥2 < 0\nThus the when an ϵ perturbation is added in the wb direction\nthe agent always takes action b for states s ∈S1 and receives\nzero reward. On the other hand let g ∼N (0, I) and set\nr =\n∥wb∥\n∥g∥g. For each action a, by standard concentration\nof measure results for uniform random vectors in the unit\nsphere we have that with probability 1 −exp(O(−t2))\n|⟨r, wa⟩| < t ∥wa∥∥wb∥\n√n\n< tβ ∥wa∥2\n√n\nTherefore, for each action a ̸= a∗(s)\n⟨s + ϵr, wa∗(s)−wa⟩\n> c −ϵtβ\n\r\rwa∗(s)\n\r\r2\n√n\n−ϵtβ ∥wa∥2\n√n\n> c −\n4tdβ3\n(1 −α)√n.\nThe above lower bound is positive for t = c(1−α)√n\n8dβ3\n. Since\nα, β, c and d are constants independent of n, we con-\nclude that ⟨s + ϵr, wa∗(s) −wa⟩> 0 with probability\n1 −exp(O(−n)). Therefore, taking a union bound over the\nset of actions a ̸= a∗(s), we have that the agent always takes\naction a∗(s) in states s ∈S1 with high probability, receiving\nthe same rewards as when there is no perturbation.\nThe proof of Proposition A.1 relies primarily on the fact\nthat, when the dimension n is large, for any ﬁxed direction\nv, a randomly chosen direction will be nearly orthogonal to\nv. This means that if there is one single direction v along\nwhich small perturbations can affect the expected cumula-\ntive rewards obtained by the policy π, then random direc-\ntions are very likely to be orthogonal to v and thus very\nunlikely to have signiﬁcant impact on rewards. In the lin-\near setting considered in Proposition A.1, one can formally\nprove this fact. However, it seems plausible that the general\nphenomenon will carry over to the more complicated setting\nof deep neural policies, again based on the fact that if there\nare only a few special high-sensitivity directions v, then ran-\ndom perturbations are likely to be nearly orthogonal to these\ndirections when the state space is high-dimensional.\nB\nAction Mapping and Action Names\nTable 4 shows action names and the corresponding action\nnumbers for RoadRunner. The action names and the action\nnumbers will be used frequently in detail in Section 5.2 in\nFigure 1 and Figure 2.\nTable 4: Action names and corresponding action numbers\nfor RoadRunner from Arcade Learning Environment Belle-\nmare et al. (2013). The action names and corresponding\nnumbers vary from game to game.\n’NOOP’:0\n’UPRIGHT’: 6\n’LEFTFIRE’:12\n’FIRE’:1\n’UPLEFT’: 7\n’DOWNFIRE’:13\n’UP’: 2\n’DOWNRIGHT’: 8\n’UPRIGHTFIRE’:14\n’RIGHT ’: 3\n’DOWNLEFT’: 9\n’UPLEFTFIRE’:15\n’LEFT’: 4\n’UPFIRE’: 10\n’DOWNRIGHTFIRE’:16\n’DOWN’: 5\n’RIGHTFIRE’: 11\n’DOWNLEFTFIRE’:17\nFigure 4: Markov Decision Processes from Arcade Learning\nEnvironment Bellemare et al. (2013). Columns represent the\nrollout of states. Left: CrazyClimber. Middle Left: TimePi-\nlot. Middle Right: RoadRunner. Right: BankHeist.\nC\nArchitectural Differences\nIn this section we provide results for different DDQN ar-\nchitectures for the episode independent random state setting\nArandom\ne\nand the environment independent random state set-\nting Arandom\nM\n. Our aim is to investigate if architectural differ-\nences between different Double Deep Q-Networks yields to\nthe creation of a different set of non-robust features. We have\nfound that different architectures (e.g. Prior DDQN, Duel\nDDQN, Prior Duel DDQN) share the same non-robust fea-\ntures. In the experiments in this section the perturbation is\ncomputed in the DDQN Prior Duel architecture, and then\nadded to the deep reinforcement learning policy’s observa-\ntion system trained with DDQN, DDQN Prior, DDQN Duel\nand DDQN Prior Duel.\nTable 5: Impacts of the different settings within the proposed\nframework where the perturbation is computed in the DDQN\nPrior Duel architecture and added to the agent’s observation\nsystem trained with DDQN, DDQN Prior, DDQN Duel and\nDDQN Prior Duel for the Pong environment.\nArchitectures\nAGaussian\nArandom\ne\nArandom\nM\nDDQN\n0.002\n1.0\n0.995\nDDQN Prior\n0.075\n1.0\n0.985\nDDQN Duel\n0.036\n1.0\n0.966\nDDQN Prior-Duel\n0.041\n1.0\n0.981\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2021-12-16",
  "updated": "2021-12-16"
}