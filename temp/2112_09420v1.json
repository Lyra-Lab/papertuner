{
  "id": "http://arxiv.org/abs/2112.09420v1",
  "title": "A random energy approach to deep learning",
  "authors": [
    "Rongrong Xie",
    "Matteo Marsili"
  ],
  "abstract": "We study a generic ensemble of deep belief networks which is parametrized by\nthe distribution of energy levels of the hidden states of each layer. We show\nthat, within a random energy approach, statistical dependence can propagate\nfrom the visible to deep layers only if each layer is tuned close to the\ncritical point during learning. As a consequence, efficiently trained learning\nmachines are characterised by a broad distribution of energy levels. The\nanalysis of Deep Belief Networks and Restricted Boltzmann Machines on different\ndatasets confirms these conclusions.",
  "text": "A random energy approach to deep learning\nRongrong Xie\nKey Laboratory of Quark and Lepton Physics (MOE) and Institute of Particle Physics,\nCentral China Normal University (CCNU), Wuhan, China\nand\nMatteo Marsili∗\nQuantitative Life Sciences Section\nThe Abdus Salam International Centre for Theoretical Physics, 34151 Trieste, Italy\nDecember 20, 2021\nAbstract\nWe study a generic ensemble of deep belief networks which is parametrized\nby the distribution of energy levels of the hidden states of each layer. We\nshow that, within a random energy approach, statistical dependence can\npropagate from the visible to deep layers only if each layer is tuned close\nto the critical point during learning. As a consequence, eﬃciently trained\nlearning machines are characterised by a broad distribution of energy levels.\nThe analysis of Deep Belief Networks and Restricted Boltzmann Machines\non diﬀerent datasets conﬁrms these conclusions.\n1\nIntroduction\nThe study of ensembles of random systems can provide several insights on the\nproperties of complex systems, such as heavy ions [29], ecologies [15], disordered\nmaterials [17], satisﬁability in computer science [18] and machine learning [30].\nIndeed the collective behaviour of a system composed of many interacting degrees\nof freedom often does not depend on the speciﬁc realisation of the wiring of\nthe interactions, but only on the statistical properties of the resulting energy\nlandscape. In these circumstances, any realisation of a random system that shares\nthe same statistical properties enjoys the same “typical\" collective behaviour.\nThe Random Energy Model (REM) [6] is probably the simplest exemplar of\nthis approach. It makes minimal assumptions on the network of interactions,\nbecause interactions of any order can occur among the variables [6]. It features\na phase transition between a random (high temperature) phase and a low\ntemperature frozen phase, which reproduces the gross features of more complex\nsystems such as spin glasses.\n∗marsili@ictp.it\n1\narXiv:2112.09420v1  [cond-mat.dis-nn]  17 Dec 2021\nHere we adopt the same approach to study systems that form an internal\nrepresentation of a complex environment, such as deep belief networks (DBN).\nA DBN is composed of a stack of layers of variables, which interact only with\nvariables in neighbouring layers, as shown in Fig. 1 a).\nThe bottom layer\nis in contact with the data that the network seeks to learn, that is akin to\nthe environment for a physical system. Typically the data has a non-trivial\nhidden structure of statistical dependencies, which the learning machine aims at\nextracting.\nDuring training, the interaction strengths are adjusted in such a way that\nthe distribution of internal states of the machine form a representation of the\ndata, in the sense that when a state is drawn from this distribution and it is\npropagated to the bottom layer it reproduces the distribution of the data with\nwhich the machine had been trained. The distribution of internal states after\ntraining depends on the structure of the data, on the machine’s architecture and\non the initial condition for the weights (which is generally assumed random, see\ne.g. [11]), and is generally characterised by a complex energy landscape, with\neﬀective interactions between variables of arbitrary order. In this paper, we’re\ngoing to consider this system as a draw from an ensemble of random systems\nin such a way that each layer can be thought of as a REM. This allows us to\ncharacterise the properties of learned representations within a rather general\nframework. Our main conclusion is that, in order for the information on the\ndata’s statistical dependencies to propagate to the deep layers, each layer has to\nbe tuned to the critical point.\nThere is extensive evidence that learning eﬃciency and criticality are re-\nlated [12, 3, 21]. Critical models that are close to a phase transition or the edge\nof chaos are much better learners and generalisers than non-critical ones. The\nbrain itself is conjectured to operate close to a critical state [2, 20]. Criticality\naﬀords several beneﬁts to information processing [25] but there is no agreed\ngeneral rationale that explains the ubiquitously observed relation between criti-\ncality and eﬃciency in learning. For deep learning, in particular, Schoenholtz\net al. [24] reach conclusions similar to ours. The study deep (deterministic)\nneural networks with random Gaussian weights and argue that “only when the\nstatistics of weights and variances is such that the network is close to the edge\nof chaos the signal propagates from the bottom to deep layers”. Yet their results\napply to untrained networks with random weights. Our more general approach\nreaches the same conclusion for well trained deep (probabilistic) neural networks,\nirrespective of the statistics of the weights, of the data and of the architecture.\nA similar REM approach can be extended to generic systems who form internal\nrepresentations of their ﬂuctuating environment, thereby oﬀering a rationale for\nthe observation of criticality in living systems [19].\nIn what follows we shall ﬁrst set the stage in Section 2 by introducing the main\nproblem and the notations. Next we shall focus on a random energy description\nof learning machines in Section 3. We shall end with a ﬁnal discussion.\n2\n2\nThe general framework\nIn order to set the stage, let us discuss the generic properties of a systems\nwith an architecture similar to that of deep belief networks (DBN) [22], such\nas that shown in Fig. 1 a). We denote by . . . , sℓ+1, sℓ, sℓ−1, . . . , s1 the internal\nstate of the diﬀerent layers, each of which is deﬁned by a set of nℓbinary\nvariables, e.g.\nsℓ∈{±1}nℓ.\nHence the number of states in layer ℓis 2nℓ,\nℓ= 1, 2, . . .. Subsequent layers are connected by interaction parameters, and the\nﬁrst layer is connected to the \"visible\" layer s0 ∈Rn0, which is composed of a\nhigh-dimensional vector (n0 ≫1).\nThis architecture corresponds to a joint probability distribution, for the\nvisible (s0) and the hidden (sℓ) units, that reﬂects the Markov property of\nstatistical dependencies between layers:\np(sℓ, sℓ−1, . . . , s1, s0) = p(s0|s1) · · · p(sℓ−1|sℓ)p(sℓ).\n(1)\nIn Eq. (1)\np(sℓ) =\nX\nsℓ+1,...\np(sℓ|sℓ+1)p(sℓ+1| . . .) · · ·\n(2)\nis the marginal distribution of layer ℓ, with respect to deeper layers ℓ+1, ℓ+2, . . ..\nThe interaction parameters between each layer are initialised to random\nvalues, and they are adjusted during the training phase in such a way as to\nmaximise the likelihood of a dataset ˆs0 = (s(1)\n0 , . . . , s(N)\n0\n) of a sample of N data\npoints.\nIn an eﬃciently trained learning machine the statistical dependence needs\nto extend across diﬀerent layers, in such a way that the state sℓof a deep layer\nshould signiﬁcantly depend on s0 (see Fig. 1 a). In addition, the learning machine\nshould work also as a generative model. This means that the vector s0 which can\nbe generated propagating a state sℓrandomly drawn from p(sℓ) to the visible\nlayer, should be statistically indistinguishable from the data points with which\nthe machine was trained. In other words, the distribution p(s0) obtained from\nEq. (1) marginalising over all hidden states should approximate the distribution\nof the data.\nTypical cases of interest are those where the data has a non-trivial struc-\nture. This is the case when the variation of s0 across the samples spans a low\ndimensional manifold of intrinsic dimension d which is much smaller than the\nnumber n0 of components of s0. Ansuini et al. [1], for example, estimate that\nthe MNIST dataset (n0 = 784) spans a space with intrinsic dimension d ≃13.\nIn loose terms, learning entails extracting a compressed representation sℓof the\nstructure of statistical dependencies of the data.\n2.1\nCompression levels and the clamped distribution\nA natural measure for the level of compression is the entropy of the internal\nrepresentation of the data. A proxy for this quantity is obtained considering the\n3\nb)\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 2\n 4\n 6\n 8\n 10\nMNIST\nFashionMNIST\nRandomised MNIST\n-3\n-2.5\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n`\n64 \n32 \n16 \n8 \n4 \n2 \n1 \n1/\n2 \nepoch\na)\n…\ns1\ns`−1\ns`+1\ns`\nb\n/cOfOG2z0OqBgcM593LPnCBhVCrb/jIKC4tLyvF1dLa+sbmlrm905JxKjBp4pjFohMgSRjlpKmoYqSTCIKigJF2MLyc+O17IiSN+Y0aJcSLUJ/TkGKktOSbe3cVN0JqEISZHPsuYez2+Mg3y3bVnsL6S5yclCFHwzc/3V6M04hwhRmSsuvYifIyJBTFjIxLbipJgvAQ9UlXU4iIr1smn5sHWqlZ4Wx0I8ra6r+3MhQJOUoCvTkJKmc9yb\nif143VeG5l1GepIpwPDsUpsxSsTWpwupRQbBiI0QFlRntfACYSVLqykS3Dmv/yXtGpV57Rauz4p1y/yOoqwDwdQAQfOoA5X0IAmYHiAJ3iBV+PReDbejPfZaMHId3bhF4yPbwo/lPU=</latexit>q(s⇤\n`)\np(s`)\ns0\nˆH[s`]/ log N\nFigure 1:\na) typical architecture of a deep belief network. The statistical\nproperties of an internal layer (shaded blue upper part) depend on the shallower\nlayers and with the data (shaded green lower part) which is the analogue of\nthe environment in which the internal layer interacts with. The two arrows\nsymbolically indicate the fact that i) the most likely states s∗\nℓ(s0) for each\ndata point s0 in the dataset induce a distribution q(s∗\nℓ) in the hidden states\nof the DBN, and that ii) the learned representation p(sℓ) can be used as a\ngenerative process.\nb) Entropy ˆH[sℓ] of the internal layer ℓ, as a function\nof the depth ℓof the layer for a DBN trained on the MNIST dataset with\nnℓ= 500, 250, 120, 60, 30, 25, 20, 15, 10 and 5, for ℓ= 1, 2, . . . , 9. The entropies of\nthe representations at diﬀerent stages of training are shown in diﬀerent colours\n(see colour bar on the right) for the MNIST dataset. Data for a DBN with the\nsame architecture trained on the Fashion MNIST and on randomised MNIST\ndatasets are shown with open circles (◦) and asterisks (∗), respectively (after 64\nepochs of training). The randomised MNIST dataset is obtained randomising\nthe positions of pixels within each image.\n4\ndistribution of clamped states q(sℓ). This is obtained presenting each data point\ns(i)\n0\non the visible layer and ﬁnding the most likely corresponding clamped state\ns∗\nℓ(s(i)\n0 ) = arg max\nsℓp(sℓ|s(i)\n0 )\n(3)\nof layer ℓ. q(sℓ) is the fraction of data points s(i)\n0\nin the sample for which the\nclamped state coincides with sℓ. The entropy\nˆH[sℓ] = −\nX\nsℓ\nq(sℓ) log q(sℓ)\n(4)\nof the distribution of clamped states provides a measure of the compression\nlevel achieved in layer ℓ. When p(sℓ|s(i)\n0 ) is a sharply peaked distribution, as\nis the case for well trained learning machines [26, 27], q(sℓ) is representative of\nthe distribution obtained sampling the distribution p(sℓ|s0) of internal states,\nwhen s0 is a sampled datapoint. In addition, a well trained machine is expected\nto reproduce the (unknown) distribution of the data by the model p(s0) that\nis obtained from Eq. (1) by marginalisation over the hidden layers. In these\ncases, we expect that q(sℓ) ≈p(sℓ) approximates well the marginal distribution\nof sℓ, and that the entropy ˆH[sℓ] provides an estimate of the mutual information\nI(sℓ, s0) = H[sℓ] −H[sℓ|s0] between the data and the internal representation,\nbecause ˆH[sℓ] ≈H[sℓ] and H[sℓ|s0] ≈0.\nA plot of ˆH[sℓ] as a function of layer depth ℓis shown in Fig. 1 b) for a DBN\nat diﬀerent stages of training on the MNIST, for a DBN trained on the Fashion\nMNIST datasets and on randomised MNIST data1. As Fig. 1 b) shows, the\ndiﬀerent layers of a well trained machine cover uniformly the range of entropies\nH[s] ≤log N, whereas untrained DBNs generate representations whose entropy\nis either close to the entropy of the data (log N) or to zero. A DBN trained\non a randomised dataset exhibits a similar behaviour, which is consistent with\nthe fact that the only structure left in the dataset is the variation in grey level\nacross the sample.\n3\nA random energy theory of deep learning\nIn order to discuss the statistical mechanics properties of the DBN, we consider\nit as a system in thermal equilibrium with a heat bath at inverse temperature\nβ = 1. Hence the Hamiltonian of the system is obtained by taking the logarithm\nof the joint distribution in Eq. (1)\nH(s0, . . . , sℓ)\n=\n−log p(s0, . . . , sℓ)\n(5)\n=\nus0|s1 + . . . + usℓ−1|sℓ+ Esℓ−E0.\n(6)\nApart for a constant E0, which plays no major role in what follows, the Hamilto-\nnian is a sum of a term Esℓ= E0 −log p(sℓ) for layer ℓand of interaction terms\nusk−1|sk = −log p(sk−1|sk) between layers, for k = 1, . . . , ℓ.\n1In randomised datasets pixels are reshuﬄed within each image.\n5\nFor a particular task, each of these terms takes a value which depends on the\nvalues of the weights with which the network is initialised and – for a trained\nnetwork – on the dataset ˆs0 with which the DBN is trained. In order to shed light\non the generic nature of the energy landscape, we will compare the behaviour of\nlearning machines with that of models where the terms in Eq. (6) are taken at\nrandom from a distribution, as in random energy models [6]. As for models of\nspin glasses, the random energy assumption is questionable if taken too seriously.\nYet, most of the results only depend on the degeneracy of energy levels (i.e. the\nnumber of states in a given energy range) so we believe that the conclusions that\nwe shall derive provide a rather robust qualitative description of the statistical\nmechanics of probabilistic (deep) learning machines.\nIn what follows, we shall ﬁrst discuss the properties of the internal layer p(sℓ)\nof a learning machine and then discuss the interaction with its environment,\nwhich is formed of the data and of the shallower layers.\n3.1\nRandom energy description of a single layer\nLet us ﬁrst focus on the statistics of a generic hidden layer sℓ. We shall drop\nthe index ℓin this subsection. In analogy with the REM, we assume that the\nenergies Es are drawn from the distribution\nP{Es ≤−∆z} = e−zγ ,\nz ≥0\n(7)\nindependently for each state s. The statistical properties of the learned model\nare parametrised by ∆> 0 and γ, and they have been discussed in Ref. [6, 16]\nfor γ = 2 and in Ref. [14] for γ > 0, to which we refer for technical details. The\nderivation of the main results is discussed in the Appendix. We shall conﬁne our\ndiscussion to the case γ ≥1, since it can be argued [14] that the properties of\nthe REM with γ < 1 are not consistent with those of a system that learns.\nThe generic behaviour of the system in the thermodynamic limit, i.e. for\nn ≫1, is characterised by a phase transition as ∆→∆∗, where\n∆∗= γ (n log 2)1−1/γ .\n(8)\nFor ∆< ∆∗the system exhibits a \"high temperature\" phase where the entropy\nH[s] ≃\n\"\n1 −\n\u0012 ∆\n∆∗\n\u0013\nγ\nγ−1 #\nn log 2\n(9)\nis proportional to the number n = nℓof nodes (see [14] for details).\nThe\nentropy H[s] instead remains ﬁnite as n →∞for ∆> ∆∗. The transition is\ncontinuous but it becomes sharper and sharper as γ →1+. For ﬁnite systems,\nH[s] exhibits a variation as a function of ∆that become sharper and sharper in\nthe neighbourhood of ∆∗as n increases. The speciﬁc heat C, which is given by\nthe variance of Es, reaches a maximum at a value ∆m which approaches ∆∗as n\ngrows large. The behaviour of H[s] and C as a function of ∆for 20 realisations\nof the REM with γ = 1 and γ = 2 are shown in Fig. 2 (top) for n = 25.\n6\n 0.8\n 0.9\n 1\n 1.1\n 1.2\n 1.3\n 1.4\n 1.5\n 0.8\n 0.9\n 1\n 1.1\n 1.2\n 1.3\n 1.4\n 1.5\nn=15\nn=20\nn=25\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 1.1\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 1.1\nn=15\nn=20\nn=25\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 0.1\n 1\n 10\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 0.1\n 1\n 10\nn=15\nn=20\nn=25\nW\nZIMkId+iVuXCji1k9x59+YtrPQ1gOXezjnXnJzwoQzbVz321lZXVvf2CxsFbd3dvdK5f2Dlo5TRaFJYx6rTkg0cCahaZjh0EkUEBFyaIejm6nfgSlWSwfzDgBX5CBZBGjxFgpKJd6t8ANOZu3QATlilt1Z8DLxMtJBeVoBOWvXj+mqQBpKCdadz03MX5GlGUw6TYSzUkhI7IALqWSiJA+9ns8\nAk+sUofR7GyJQ2eqb83MiK0HovQTgpihnrRm4r/ed3URFd+xmSGpB0/lCUcmxiPE0B95kCavjYEkIVs7diOiSKUGOzKtoQvMUvL5NWrepdVGv35X6dR5HAR2hY3SKPHSJ6ugONVATUZSiZ/SK3pwn58V5dz7moytOvnOI/sD5/AE2TpLP</latexit>∆/∆m\nt (epoch)\nW\nZIMkId+iVuXCji1k9x59+YtrPQ1gOXezjnXnJzwoQzbVz321lZXVvf2CxsFbd3dvdK5f2Dlo5TRaFJYx6rTkg0cCahaZjh0EkUEBFyaIejm6nfgSlWSwfzDgBX5CBZBGjxFgpKJd6t8ANOZu3QATlilt1Z8DLxMtJBeVoBOWvXj+mqQBpKCdadz03MX5GlGUw6TYSzUkhI7IALqWSiJA+9ns8\nAk+sUofR7GyJQ2eqb83MiK0HovQTgpihnrRm4r/ed3URFd+xmSGpB0/lCUcmxiPE0B95kCavjYEkIVs7diOiSKUGOzKtoQvMUvL5NWrepdVGv35X6dR5HAR2hY3SKPHSJ6ugONVATUZSiZ/SK3pwn58V5dz7moytOvnOI/sD5/AE2TpLP</latexit>∆/∆m\nk\n2WZFaoS/0jXjwo4tWf4s1/Y9ruQVsfDzem2FmXhgLbsDzvp3c2vrG5lZ+u7Czu7dfdA8Om0YlmrIGVULpdkgME1yBnAQrB1rRqJQsFY4vpn5rQemDVfyHiYxCyIylHzAKQEr9dwiPJXTro4wixUdTc96bsmreHPgVeJnpIQy1HvuV7evaBIxCVQYzq+F0OQEg2cCjYtdBPDYkLHZMg6lkoSM\nROk8On+NQqfTxQ2pYEPFd/T6QkMmYShbYzIjAy95M/M/rJDC4ClIu4wSYpItFg0RgUHiWAu5zSiIiSWEam5vxXRENKFgsyrYEPzl1dJs1rxLyrVu/NS7TqLI4+O0QkqIx9dohq6RXUQBQl6Bm9ojfn0Xlx3p2PRWvOyWaO0B84nz9myJLt</latexit>t (epoch)\nn=15\nn=20\nn=25\n∆/∆⇤\n∆m/∆⇤\n∆/∆⇤\n∆m/∆⇤\n∆/∆⇤\nγ = 1\nγ = 2\n∆/∆⇤\nH[s]\nA\nB9HicdVDLSsNAFL3xWeur6tLNYBFchaT4WhbdFnBPiANZTKdtEMnkzgzKZTQ73DjQhG3fow7/8ZJm4KHhg4nHMv98wJEs6UdpxPa2V1bX1js\n7RV3t7Z3duvHBy2VZxKQlsk5rHsBlhRzgRtaY57SaS4ijgtBOMb3O/M6FSsVjc62lC/QgPBQsZwdpIfsPrRViPgjBTM79fqTr2hZMDObazJIX\niFkoVCjT7lY/eICZpRIUmHCvluU6i/QxLzQins3IvVTBZIyH1DNU4IgqP5uHnqFTowxQGEvzhEZz9ftGhiOlplFgJvOI6reXi395XqrDaz9j\nIk1FWRxKEw50jHKG0ADJinRfGoIJpKZrIiMsMREm57KpoTlT9H/pF2z3Uu7dnderd8UdZTgGE7gDFy4gjo0oAktIPAj/AML9bEerJerbfF6I\npV7BzBD1jvXwUTkY=</latexit>H[s]\nCv\nCv\nFigure 2:\nTop row: Entropy H[s] (purple lines) and speciﬁc heat C (green lines)\nfor 20 realisations of the REM with n = 25 for γ = 1 (left) and γ = 2 (right).\nThe dotted lines denote the values of ˆH[s] for layer ℓ= 6 of the DBN of Fig. 1\ntrained on the MNIST dataset (n = nℓ= 25 nodes). Bottom row: Value of ∆\nobtained matching the observed value of ˆH[sℓ] for layers ℓ= 6, 7 and 8 of a DBN\ntrained on MNIST (nℓ= 25, 20 and 15, respectively).\nIn order to relate the behaviour of learning machines with REMs, we seek for\nthe value of ∆for which the entropy H[s] of a REM with n variables equals the\nentropy ˆH[sℓ] of the layer of the DBN with the same number n = nℓof variables\n(see Fig. 2 top). Fig. 2 (bottom) shows the values of ∆obtained in this way\nfor the layer with nℓ= 25 units of the DBN used in Fig. 1 (see caption) during\ntraining on the MNIST database. As this ﬁgure shows, the parameter ∆which\nbest describes the distribution of energy levels of the internal representation\napproaches rapidly a value very close to ∆m, irrespective of the value of γ.\nFor larger values of n, we resort to the annealed approximation of Ref. [14] to\nestimate H[s] for the model, and compute the value of ∆, matching the observed\nentropy ˆH[sℓ] for each layer, as explained above. The results are shown in Fig. 3\nfor both DBMs and RBMs, for diﬀerent dataset (see caption for details). We see\nthat internal representations of learning machines trained on highly structured\ndatasets are best described by REMs close to the critical point, i.e. ∆≃∆∗.\nUntrained learning machines and machines trained on structureless datasets are\ninstead described by oﬀcritical REMs with ∆that diﬀers substantially from\n∆∗. For the shallower layers (n ≥102) the variation of the REM’s entropy\nis concentrated in a narrow interval around ∆∗while the entropy of the layer\napproaches the upper bound ˆH[sℓ] ≤log N. These two eﬀects conspire to give\n∆≃∆∗even for untrained networks.\nThe critical point in the REM is located in the vicinity of the value ∆m where\nthe speciﬁc heat C attains its maximum (see Fig. 2). Tkačik et al. [28] suggest that\n7\nFigure 3:\nEstimate of the parameter ∆for γ = 1 (top) and γ = 2 (bottom)\nfor a DBN with 10 layers with n = 500, 250, 120, 60, 30, 25, 20, 15, 10 and 5 units.\nEach panel shows the results for an untrained DBN, for DBN trained on the\nMNIST (left) and on Fashion MNIST (right) datasets, and for a DBN trained\non shuﬄed datasets. Data for RBMs with n ∈[10, 70] units trained on MNIST\nare also shown (see Appendix for more details).\nthis criterium can be advocated to detect criticality in general, given the estimate\np(s) of the probability distribution over the states of a system. This construction\n(see also [13]) is based on identifying the energy levels Es = −log p(s) from the\nprobability distribution, and analysing the statistical mechanical properties of\na system whose partition function Z(β) = P\ns e−βEs is derived introducing a\nﬁctitious temperature β. In analogy with systems undergoing second order phase\ntransitions, Tkačik et al. [28] identify the occurrence of a peak in the speciﬁc heat\nat β = 1 as a signature of criticality of the system described by p(s) (the activity\nof assembles of neurons in [28]). We observe that this construction explores the\nlarge deviation properties of the system, where the expected value of Es, which\nis the entropy H[s], attains atypically small (β > 1) or large (β < 1) values2.\nHence this construction singles out the compression level H[s] as the order\nparameter of the transition. This is a natural choice in learning systems, that are\nbuilt in order to extract the most compressed representation of high dimensional\ndatasets. Cubero et al. [4] show that this same construction implies that eﬃcient\ncodes in Minimum Description Length (MDL) [10] are critical, thereby clarifying\nthe nature of the phase transition. As in the REM, the critical state in MDL\ncodes separates a \"high temperature\" phase of noisy representations from a \"low\ntemperature\" one, which is dominated by just one state [4]. This is reminiscent\nof the mode collapse phenomenon observed in generative adversarial networks [9],\n2This is not true when the construction is based on a distribution p(s) of sampled states,\nas in Refs. [28, 13]. This is because p(s) only contains information on the typical behaviour at\nβ = 1 and it cannot describe large deviations.\n8\nβ\nEs = −log(ps)\n=\n\">AB63icbVBNSwMxEJ2tX7V+VT16CRahXspuEfVYFMFjBfsB7VKyabYNTbJLkhXK0r/gxYMiXv1D3vw3Zts9aOuDgcd7M8zM\nC2LOtHdb6ewtr6xuVXcLu3s7u0flA+P2jpKFKEtEvFIdQOsKWeStgwznHZjRbEIO0Ek9vM7zxRpVkH80pr7AI8lCRrDJpLh\n6dz4oV9yaOwdaJV5OKpCjOSh/9YcRSQSVhnCsdc9zY+OnWBlGOJ2V+omMSYTPKI9SyUWVPvp/NYZOrPKEIWRsiUNmqu/J1Ist\nJ6KwHYKbMZ62cvE/7xeYsJrP2UyTgyVZLEoTDgyEcoeR0OmKDF8agkmitlbERljhYmx8ZRsCN7y6ukXa95l7X6w0WlcZPHUYQ\nTOIUqeHAFDbiHJrSAwBie4RXeHOG8O/Ox6K14OQzx/AHzucPMHCNrw=</latexit>p(E)\nC\nH[E]\nFigure 4:\nSpeciﬁc heat C and relevance H[E] as a function of the (ﬁctitious)\ninverse temperature of the construction of Refs. [28, 13], for a RBM with n = 22\nnodes trained on the MNIST database. The original distribution of energy levels\np(E) (i.e. for β = 1) is compared to the one for β = 1.2 in the inset. This shows\nthat already close to the phase transition p(β) develops a bimodal character,\nwhich is enhanced as β increases. Close to the maximum of C, the gap between\nthe ground state and the ﬁrst excited state is of order ∆E ≈10.\nwhich refers to the situation where the learned model “specialises” to generate\nonly a limited variety of the inputs with which it has been trained. In this\nperspective, criticality of the internal representation arises as a consequence\nof learning machines striking the optimal trade-oﬀbetween the accuracy with\nwhich it reproduces data points and its ability to generate the full variability of\nthe data.\nWe observe, however, that the location of the phase transition does not\nalways coincide with a peak in the speciﬁc heat. In order to show this, Fig. 4\nreports the values of the speciﬁc heat C as a function of β obtained with this\nconstruction, using the energy levels obtained from the internal representation\np(s) of a RBM with n = 22 hidden units trained on the MNIST dataset. As\nFig. 4 shows, C exhibits a maximum at β ≈2.23 rather than at β = 1. This\nis a consequence of the fact that for β > 1 the distribution p(E) of energies\ndevelops a gap between the ground state and the bulk of excited states (see\ninset of Fig. 4). This is fully consistent with the mode collapse phase transition\ndiscussed above. Fig. 4 also shows that the (diﬀerential) entropy H[E] of the\npdf of energies shows instead a maximum close to β = 1. This suggests that\nH[E] – which has been called the relevance in [5, 7] – is a better predictor of\nthe location of the phase transition in learning systems. Both C and H[E] are\nmeasures of the width of the distribution of energy levels, but C is maximal\nfor bimodal energy distributions, whereas H[E] is maximal when p(E) covers\ndensely a wide energy spectrum.\n9\n3.2\nThe relation between layers\nThe relation between internal states sℓand inputs s0 can be studied by considering\nthe most likely clamped states s∗\nℓ, s∗\nℓ−1, . . . s∗\n1 that the network associates to an\ninput s0. These are the solution of the maximisation problem\ns∗\nℓ(s0) = arg max\nsℓ\n\u001a\nlog p(sℓ) + max\nsℓ−1 vsℓ−1|sℓ(s0)\n\u001b\n(10)\nwhere we deﬁned\nvsℓ−1|sℓ(s0) =\nmax\ns1...,sℓ−2 log p(sℓ−1, . . . , s1, s0|sℓ).\n(11)\nThe term vsℓ−1|sℓ(s0) characterises the “environment” with which the ℓth internal\nlayer interacts with, and it satisﬁes a recursion relation\nvsℓ−1|sℓ(s0)\n=\nlog p(sℓ−1|sℓ) + max\nsℓ−2\n\u0014\nmax\nsℓ−3,...,s1 log p(sℓ−2, sℓ−3, . . . , s0|sℓ−1, sℓ)\n\u0015\n=\nlog p(sℓ−1|sℓ) + max\nsℓ−2\n\u0002\nlog p(sℓ−2|sℓ−1) + vsℓ−2|sℓ−1(s0)\n\u0003\n(12)\nwhere we invoked the Markov property, i.e. the fact that p(sℓ−2, . . . , s0|sℓ−1, sℓ)\ndoes not depend on sℓ. Eq. (12) describes how the statistical dependencies\npropagate across layers and Eq. (10) how it determines the distribution of\nclamped states\nq(sℓ) = P{s∗\nℓ= sℓ}.\n(13)\nNote that, if in some intermediate layer the optimisation on sℓ−2 in Eq. (12)\nis dominated by the ﬁrst term log p(sℓ−2|sℓ−1), then the dependence on s0\nof vsℓ−1|sℓ(s0) is lost. This means that the solution of Eq. (10) will also be\nindependent of s0. In this case, the clamped distribution concentrates on just one\nstate, which is what we observe in deep layers of untrained DBNs. On the other\nhand, if the optimisation in Eq. (10) is dominated by the term vsℓ−1|sℓ(s0) each\ndiﬀerent input will likely result in a diﬀerent internal layer s∗\nℓand ˆH[sℓ] ≃log N,\nas we observe in shallow layers of DBNs. These two extreme situations well\ncharacterise the behaviour of H[s∗\nℓ] observed in Fig. 1 for untrained learning\nmachines.\nConsidering the input data s0 as a random vector drawn from some (unknown)\ndistribution implies that for all sℓand sℓ−1, vsℓ−1|sℓis a random variable that\nwe assume is drawn independently from a distribution\nP{vsℓ−1|sℓ> Θℓx −¯vℓ} = e−xθℓ,\n(x ≥0) ,\n(14)\nwhere θℓ≥1, Θℓsets the scale of the ﬂuctuations of vsℓ−1|sℓand ¯vℓ> 0 is\na constant that plays no role in what follows3.\nEq. (14) approximates the\nenvironment with which the ℓth layer interacts with a REM.\n3¯vℓshould be large enough so that vsℓ−1|sℓ≤0 for all sℓ−1 ∈{±1}nℓ−1 and sℓ∈{±1}nℓ.\n10\nThe distribution of clamped states can be computed as in Ref. [14] using\nresults from extreme value theory [8]. Speciﬁcally, the intermediate maximisation\nof vsℓ−1|sℓover sℓ−1 (see e.g. Eq. 10) can be carried out explicitly, with the result\nmax\nsℓ−1 vsℓ−1|sℓ≃aℓ+ Θℓ\nΘ∗\nℓ\nηsℓ\nwhere aℓ= Θℓ(nℓ−1 log 2)1/θℓ−¯vℓis a constant, ηsℓis a random variable that\nfollows a Gumbel distribution\nP{ηℓ< x} = e−e−x.\n(15)\nand\nΘ∗\nℓ= θℓ(nℓ−1 log 2)1−1/θℓ\n(16)\ncoincides with the critical value of ∆∗for n = nℓ−1 and θℓ= γℓ(see Eq. 8).\nSince the maximum is taken over 2nℓ−1 random variables, the characterisation\nprovided by extreme value theory is accurate even for moderate values of nℓ−1.\nThis result can be used to compute the distribution of clamped states s∗\nℓ, as\nshown in Ref. [14], i.e.\nq(sℓ)\n=\nP\n\u001a\nlog p(sℓ) + max\nsℓ−1 vsℓ−1|sℓ≥log p(s′\nℓ) + max\nsℓ−1 vsℓ−1|s′\nℓ, ∀s′\nℓ\n\u001b\n=\nP\n\u001a\nηs′\nℓ≤ηsℓ+ β log p(sℓ)\np(s′\nℓ), ∀s′\nℓ\n\u001b\n(17)\n=\n1\nZ(β)p(sℓ)β,\nZ(β) =\nX\nsℓ\np(sℓ)β ,\n(18)\nwhere we used Eq. (15) to compute Eq. (17) (see Appendix for more details).\nThe exponent β is given by\nβ = Θ∗\nℓ\nΘℓ\n= θℓ\nΘℓ\n(nℓ−1 log 2)1−1/θℓ\n(19)\nthat depends on the size nℓ−1 of layer ℓ−1 and on the parameter Θℓ−1. Eq. (18)\nshows that the distribution of clamped states q(sℓ) coincides with the distribution\np(sℓ) only when β = 1, i.e. when the parameter Θℓis tuned to the parameter\nΘ∗\nℓfor which a REM with energies drawn from Eq. (14) is critical. In other\nwords, statistical dependence on the data s0 can propagate to deep layers of a\nneural network only if the environment with which layer ℓinteracts is tuned at\nthe critical point, i.e. Θℓ≈Θ∗\nℓ, irrespective of the value of θℓ. If Θℓ> Θ∗\nℓthe\nmaximisation in Eq. (10) is dominated by the term vsℓ−1|sℓwhich results in a\nnoisy clamped distribution q(sℓ) which is broader than p(sℓ). If instead Θℓ≤Θ∗\nℓ\nthe maximisation is dominated by the ﬁrst term and q(sℓ) concentrates on the\nmost probable values of sℓ, i.e. those with higher p(sℓ).\nLet us discuss the implications of these ﬁndings on the performance of the\nlearning machines as a generative model. Taking q(sℓ) as a projection of the data\n11\ndistribution in the ℓth layer, the case β < 1 corresponds to the situation where\np(sℓ) is more sharply peaked than q(sℓ). Hence the DBN will not reproduce\nthe full variability of the data with which it has been trained. Rather it will\npredominantly generate the most likely patterns. This is what the authors of\nRef. [26] observed when generating data points from deep layers of a DBN (see\nFig. 1 b). Conversely, for β < 1 the distribution p(sℓ) is ﬂatter that q(sℓ). Noisy\npatters, which are generally unlikely in the data, are generated more frequently\nin this case. This behaviour is characteristic of shallow layers of DBN, as shown\nin Ref. [26] (see also Fig. 1 b).\n4\nConclusion\nSummarising, we study deep belief networks within a random energy ensemble\napproach. Each layer is described as a random energy model with a stretched\nexponential distribution of energies with parameter γ, as in Ref. [14]. Each layer\nfeatures a phase transition to a “frozen” state for a particular value ∆∗of the\ninteraction strength, which depends on the number of hidden units and on γ\n(Eq. 16). We show that i) the internal representation of hidden layers of well\ntrained learning machines (DBN and RBM) on structured datasets (MNIST and\nFashion MNIST) are best described by REMs with a parameter that is close to\nthe critical point ∆∗, irrespective of γ in a wide range of n. Untrained learning\nmachines or machines trained on structureless data are instead best ﬁtted by\noﬀ-critical REMs. Furthermore, ii) in order to propagate the dependence on\nthe data to the deep layers, each layer should interact with the data through\na system (the environment) which should be tuned at the critical point of the\ncorresponding REM.\nNotice that close to the critical point ∆∗, the distribution of energies of a\nREM is approximately ﬂat, irrespective of the value of γ (see the Appendix). This\nprediction can be tested empirically, because, together with the relation between\nthe clamped distribution and the distribution p(s) of Eq. (18), it implies that the\ndistribution of clamped states should exhibit statistical criticality. Refs. [23, 26]\nprovide ample evidence that support this claim.\n5\nAcknowledgments\nRongrong Xie acknowledges a fellowship from the China Scholarship Council\n(CSC) under Grant CSC No. 202006770018.\nA\nThe generalised REM\nThe thermodynamic properties of the REM are derived following the same\narguments as in Ref. [6, 16, 14]. The behaviour in the random phase (∆< ∆∗)\ncan be discussed within the annealed approximation. The number of states at\n12\nenergy E is asymptotically given by eS(E) where the thermodynamic entropy is\nS(E) = n log 2 −(−E/∆)γ .\n(20)\nThe ground state energy is given by the value for which S(EGS) = 0. This yields\nEGS = ∆(n log 2)1/γ. For small values of ∆, the partition function\nZ(∆) =\nX\ns\ne−Es ≃\nZ\ndEeS(E)−E\n(21)\nis dominated by the saddle point value E(∆) for which dS\ndE = 1. This gives\nE(∆) = −∆(∆/γ)1/(γ−1) .\n(22)\nThis expression is valid as long as E(∆) > EGS. The critical point is given by\nthe point where the thermodynamics becomes dominated by few states with\nenergy close to the ground state, i.e. when E(∆∗) = EGS. This condition yields\nEq. (8). In order to obtain Eq. (9) we observe that Eq. (22) can be written as\nE(∆) = EGS(∆/∆∗)1/(γ−1). This readily gives the expression Eq. (9) for the\nentropy H[s] = S (E(∆)). At ∆∗, the expansion of the entropy around EGS\nyields\nS(E) ≃E −EGS + γ −1\n2\n(E −EGS)2\nEGS\n+ O((E −EGS)3)\nwhich is approximately linear on an energy range δE ≪√−EGS = √γn log 2,\nfor γ > 1. Hence the distribution of energies p(E) ≃eS(E)−E extends over a\nrange of order δE ∼√n and the speciﬁc heat C ∼δE2 is proportional to n.\nFor γ = 1 instead the entropy is linear in E for all values of ∆and at ∆∗the\ndistribution of energies extends over a range proportional to n, so that C ∼n2.\nIn order to derive Eq. (18), we observe [14] that the probability in Eq. (17)\ncan be evaluated explicitly using Gumbel distribution for ηs. Indeed\nP\nn\nηs′\nℓ≤ηsℓ+ zs|s′, ∀s′o\n=\nZ ∞\n−∞\ndηse−ηs−e−ηs Y\ns′̸=s\nP{ηs′ ≤ηs + zs|s′}\n=\nZ ∞\n0\ndye−y−y P\ns′̸=s e\n−zs|s′\n(23)\nwhere we used the fact that P{ηs′ ≤ηs + zs|s′} = e−e\n−ηs−zs|s′ and changed\nvariables to y = e−ηs.\nB\nData and network architectures\nThis study employs the handwritten digits dataset MNIST, and the article\nimages dataset Fashion-MNIST. Both databases, consisting of 28 × 28 grayscale\nimages with 10 diﬀerent classes. Both are split into a training sets with 60000\nimages and a test sets with 10000 images.\n13\nWe adopted the Persistent Contrastive Divergence algorithm to train DBN\nand RBM. The training process runs for 64 epochs with a mini-batch size of\n100 and a learning rate of 0.1. We set the persistent chains to be 100 steps\nlong, and the initial weights to be 4\nq\n6\nnV is+nHid × N(nV is, nHid), where nV is\nand nHid are the number of visible units and hidden units, respectively, and\nN(nV is, nHid) is a sample generated from the standard normal distribution\nand with a dimension of nV is × nHid. The initial visible and hidden biases are\nassigned to be zero with a dimension of 1 × nV is and 1 × nHid, respectively.\nIn the γ = 1 case, as shown in the upper subplots in Figure 3, we use a\ndiﬀerent number of hidden units in RMB when training with these two datasets.\nFor MINIST, we set it to be 10, 20, 25, 30, 35, 40, 45, 50, 60 and 70, respectively,\nwhile for Fashion MNIST, we ﬁx it to be 70, 80, 90, 100, 150, 200, 250, 300, 400\nand 500, respectively. In the γ = 2 case, as shown in the lower subplots in Figure\n3, the number of hidden units in RBM is the same when training with MINIST\nand Fashion MNIST. Both are set to be 70, 80, 90, 100, 130, 160, 200, 230, 260\nand 300, respectively. For the untrained DBN, we use the initial parameters\n(weight, visible bias and hidden bias) instead of using the well-trained parameters\nto compute in order to ˆH[s] and the corresponding value of ∆. For the DBN\nrandom data, we purposely randomise the positions of pixels within each image\nin both MNIST and Fashion MNIST.\nReferences\n[1] Alessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan.\nIntrinsic dimension of data representations in deep neural networks. In\nAdvances in Neural Information Processing Systems, pages 6111–6122, 2019.\n[2] John M Beggs. The criticality hypothesis: how local cortical networks might\noptimize information processing. Philosophical Transactions of the Royal\nSociety A: Mathematical, Physical and Engineering Sciences, 366(1864):329–\n343, 2008.\n[3] Nils Bertschinger and Thomas Natschläger. Real-time computation at the\nedge of chaos in recurrent neural networks. Neural computation, 16(7):1413–\n1436, 2004.\n[4] R Cubero, M Marsili, and Y Roudi. Minimum description length codes are\ncritical. Entropy, 20(10):755, Oct 2018.\n[5] Ryan John Cubero, Junghyo Jo, Matteo Marsili, Yasser Roudi, and Juyong\nSong.\nStatistical criticality arises in most informative representations.\nJournal of Statistical Mechanics: Theory and Experiment, 2019(6):063402,\njun 2019.\n[6] Bernard Derrida. Random-energy model: An exactly solvable model of\ndisordered systems. Physical Review B, 24(5):2613, 1981.\n14\n[7] O Duranthon, M Marsili, and R Xie.\nMaximal relevance and optimal\nlearning machines. Journal of Statistical Mechanics: Theory and Experiment,\n2021(3):033409, 2021.\n[8] Janos Galambos. The asymptotic theory of extreme order statistics. John\nWiley and Sons, New york, 1978.\n[9] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative\nadversarial nets. In Proceedings of the 27th International Conference on\nNeural Information Processing Systems-Volume 2, pages 2672–2680, 2014.\n[10] Peter D Grünwald and Abhijit Grunwald. The minimum description length\nprinciple. MIT press, 2007.\n[11] Geoﬀrey E Hinton. A practical guide to training restricted boltzmann\nmachines. In Neural networks: Tricks of the trade, pages 599–619. Springer,\n2012.\n[12] Chris G Langton. Computation at the edge of chaos: Phase transitions and\nemergent computation. Physica D: Nonlinear Phenomena, 42(1-3):12–37,\n1990.\n[13] E D Lee, C P Broedersz, and W Bialek. Statistical Mechanics of the US\nSupreme Court. Journal of Statistical Physics, 160:275–301, July 2015.\n[14] Matteo Marsili. The peculiar statistical mechanics of optimal learning\nmachines.\nJournal of Statistical Mechanics:\nTheory and Experiment,\n2019(10):103401, 2019.\n[15] Robert M May.\nWill a large complex system be stable?\nNature,\n238(5364):413–414, 1972.\n[16] Marc Mezard and Andrea Montanari. Information, physics, and computation.\nOxford University Press, 2009.\n[17] Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory\nand beyond: An Introduction to the Replica Method and Its Applications,\nvolume 9. World Scientiﬁc Publishing Company, 1987.\n[18] Rémi Monasson, Riccardo Zecchina, Scott Kirkpatrick, Bart Selman, and\nLidror Troyansky. Determining computational complexity from characteris-\ntic ‘phase transitions’. Nature, 400(6740):133–137, 1999.\n[19] T Mora and W Bialek. Are biological systems poised at criticality? Journal\nof Statistical Physics, 144(2):268–302, 2011.\n[20] Dietmar Plenz, Tiago L Ribeiro, Stephanie R Miller, Patrick A Kells, Ali\nVakili, and Elliott L Capek. Self-organized criticality in the brain. arXiv\npreprint arXiv:2102.09124, 2021.\n15\n[21] Andrea Roli, Marco Villani, Alessandro Filisetti, and Roberto Serra. Dy-\nnamical criticality: overview and open questions. Journal of Systems Science\nand Complexity, 31(3):647–663, 2018.\n[22] Yasser Roudi and Graham Taylor. Learning with hidden variables. Current\nopinion in neurobiology, 35:110–118, 2015.\n[23] M. E. Rule, M. Sorbaro, and M. H. Hennig. Optimal encoding in stochastic\nlatent-variable Models. ArXiv e-prints, page arXiv:1802.10361, February\n2018.\n[24] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-\nDickstein. Deep information propagation. arXiv preprint arXiv:1611.01232,\n2016.\n[25] Woodrow L Shew and Dietmar Plenz. The functional beneﬁts of criticality\nin the cortex. The neuroscientist, 19(1):88–100, 2013.\n[26] J Song, M Marsili, and J Jo.\nResolution and relevance trade-oﬀs in\ndeep learning. Journal of Statistical Mechanics: Theory and Experiment,\n2018(12):123406, dec 2018.\n[27] Juyong Song. Eﬃcient data representation of deep neural networks, 2018.\n[28] G Tkačik, T Mora, O Marre, D Amodei, S E Palmer, M J Berry, and\nW Bialek. Thermodynamics and signatures of criticality in a network of\nneurons. Proceedings of the National Academy of Sciences, 112(37):11508–\n11513, 2015.\n[29] Eugene P Wigner. Characteristic vectors of bordered matrices with inﬁnite\ndimensions i. In The Collected Works of Eugene Paul Wigner, pages 524–540.\nSpringer, 1993.\n[30] Lenka Zdeborová and Florent Krzakala. Statistical physics of inference:\nThresholds and algorithms. Advances in Physics, 65(5):453–552, 2016.\n16\n",
  "categories": [
    "cond-mat.dis-nn",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2021-12-17",
  "updated": "2021-12-17"
}