{
  "id": "http://arxiv.org/abs/2112.09741v2",
  "title": "Envisioning Future Deep Learning Theories: Some Basic Concepts and Characteristics",
  "authors": [
    "Weijie J. Su"
  ],
  "abstract": "To advance deep learning methodologies in the next decade, a theoretical\nframework for reasoning about modern neural networks is needed. While efforts\nare increasing toward demystifying why deep learning is so effective, a\ncomprehensive picture remains lacking, suggesting that a better theory is\npossible. We argue that a future deep learning theory should inherit three\ncharacteristics: a \\textit{hierarchically} structured network architecture,\nparameters \\textit{iteratively} optimized using stochastic gradient-based\nmethods, and information from the data that evolves \\textit{compressively}. As\nan instantiation, we integrate these characteristics into a graphical model\ncalled \\textit{neurashed}. This model effectively explains some common\nempirical patterns in deep learning. In particular, neurashed enables insights\ninto implicit regularization, information bottleneck, and local elasticity.\nFinally, we discuss how neurashed can guide the development of deep learning\ntheories.",
  "text": "Envisioning Future Deep Learning Theories: Some Basic\nConcepts and Characteristics\nWeijie J. Su∗\nAugust 8, 2024\nAbstract\nTo advance deep learning methodologies in the next decade, a theoretical framework for\nreasoning about modern neural networks is needed. While efforts are increasing toward demys-\ntifying why deep learning is so effective, a comprehensive picture remains lacking, suggesting\nthat a better theory is possible. We argue that a future deep learning theory should inherit\nthree characteristics: a hierarchically structured network architecture, parameters iteratively\noptimized using stochastic gradient-based methods, and information from the data that evolves\ncompressively. As an instantiation, we integrate these characteristics into a graphical model\ncalled neurashed.\nThis model effectively explains some common empirical patterns in deep\nlearning. In particular, neurashed enables insights into implicit regularization, information bot-\ntleneck, and local elasticity. Finally, we discuss how neurashed can guide the development of\ndeep learning theories.\n1\nIntroduction\nDeep learning is recognized as a monumentally successful approach to many data-extensive appli-\ncations in image recognition, natural language processing, and board game programs (Krizhevsky\net al., 2017; LeCun et al., 2015; Silver et al., 2016). Despite extensive efforts (Jacot et al., 2018;\nBartlett et al., 2017; Berner et al., 2021), however, our theoretical understanding of how this increas-\ningly popular machinery works and why it is so effective remains incomplete. This is exemplified\nby the substantial vacuum between the highly sophisticated training paradigm of modern neural\nnetworks and the capabilities of existing theories. For instance, the optimal architectures for certain\nspecific tasks in computer vision remain unclear (Tolstikhin et al., 2021).\nTo better fulfill the potential of deep learning methodologies in increasingly diverse domains,\nheuristics and computation are unlikely to be adequate—a comprehensive theoretical foundation for\ndeep learning is needed. Ideally, this theory would demystify these black-box models, visualize the\nessential elements, and enable principled model design and training. A useful theory would, at a\nminimum, reduce unnecessary computational burden and human costs in present-day deep-learning\nresearch, even if it could not make all complex training details transparent.\nUnfortunately, it is unclear how to develop a deep learning theory from first principles. Instead,\nin this paper we take a phenomenological approach that captures some important characteristics\nof deep learning. Roughly speaking, a phenomenological model provides an overall picture rather\n∗University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu.\n1\narXiv:2112.09741v2  [cs.LG]  9 Aug 2024\nthan focusing on details, and allows for useful intuition and guidelines so that a more complete\ntheoretical foundation can be developed.\nTo address what characteristics of deep learning should be considered in a phenomenological\nmodel, we recall the three key components in deep learning: architecture, algorithm, and data (Zde-\nborová, 2020). The most pronounced characteristic of modern network architectures is their hier-\narchical composition of simple functions. Indeed, overwhelming evidence shows that multiple-layer\narchitectures are superior to their shallow counterparts (Eldan and Shamir, 2016), reflecting the\nfact that high-level features are hierarchically represented through low-level features (Hinton, 2021;\nBagrov et al., 2020). The optimization workhorse for training neural networks is stochastic gradient\ndescent or Adam (Kingma and Ba, 2015), which iteratively updates the network weights using noisy\ngradients evaluated from small batches of training samples. Overwhelming evidence shows that\nthe solution trajectories of iterative optimization are crucial to generalization performance (Soudry\net al., 2018). It is also known that the effectiveness of deep learning relies heavily on the structure\nof the data (Blum and Rivest, 1992; Goldt et al., 2020), which enables the compression of data\ninformation in the late stages of deep learning training (Tishby and Zaslavsky, 2015; Shwartz-Ziv\nand Tishby, 2017).\n2\nNeurashed\nWe introduce a simple, interpretable, white-box model that simultaneously possesses the hierarchi-\ncal, iterative, and compressive characteristics to guide the development of a future deep learning\ntheory. This model, called neurashed, is a phenomenological model that mimics neural networks\nthrough the training process. Depending on the neural network that it imitates, a neurashed model\nis represented as a graph with nodes partitioned into different levels (Figure 1). For example, the\nnumber of levels is the same as the number of layers of the corresponding neural network. In-\nstead of corresponding with a single neuron in the corresponding neural network, an l-level node\nin neurashed represents a feature that the neural network can learn in its l-layer. For example, the\nnodes in the first/bottom level denote lowest-level features, whereas the nodes in the last/top level\ncorrespond to the class membership in the classification problem. To describe the dependence of\nhigh-level features on low-level features, neurashed includes edges between a node and its dependent\nnodes in the preceding level. This reflects the hierarchical nature of features in neural networks.\nLayer one\nLayer two\nLayer three\nLayer four\nFigure 1:\nA neurashed model that imitates a four-layer neural network for a three-class\nclassification problem. For instance, the feature represented by the leftmost node in the second\nlevel is formed by the features represented by the three leftmost nodes in the first level.\nGiven any input sample, a node in neurashed is in one of two states: firing or not firing. The\n2\nfirst-layer nodes take the pixels as input when the input is an image.1 The unique last-level node\nthat fires for an input corresponds with the label of the input. Whether a node in the first level\nfires or not is determined by the input. For a middle-level node, its state is determined by the\nfiring pattern of its dependent nodes in the preceding levels through its firing rule. For example,\nlet a node represent cat and its dependent nodes be cat head and cat tail. We can activate cat\nwhen either or both of the two dependent nodes are firing. Alternatively, let a node represent panda\nhead and consider its dependent nodes dark circle, black ear, and white face. We can let the\npanda head node be firing only if all three dependent nodes are firing. In general, the firing rule is\na Boolean function with arity being the number of dependent nodes.\nWe call the subgraph induced by the firing nodes the feature pathway of a given input. A feature\npathway should be consistent with the firing rules. In general, samples from different classes have\nrelatively distinctive feature pathways, commonly shared at lower levels but more distinct at higher\nlevels. By contrast, feature pathways of same-class samples are identical or similar. An illustration\nis given in Figure 2.\nLayer one\nLayer two\nLayer three\nLayer four\n(a) Class 1a\nLayer one\nLayer two\nLayer three\nLayer four\n(b) Class 1b\nLayer one\nLayer two\nLayer three\nLayer four\n(c) Class 2\nLayer one\nLayer two\nLayer three\nLayer four\n(d) Class 3\nFigure 2: Feature pathways of the neurashed model in Figure 1. Firing nodes are marked in\nred. Class 1 includes two types of samples with slightly different feature pathways, which is a\nreflection of heterogeneity in real-life data (Feldman, 2020).\nTo mimic the prediction of neural network, all nodes F in neurashed except for the last-level\nnodes are assigned a nonnegative value λF as a measure of each node’s ability to sense the corre-\nsponding feature. A large value of λF means that when this node fires it can send out strong signals\nto connected nodes in the next level. Hence, λF is the amplification factor of F. Moreover, let ηfF\ndenote the weight of a connected second-last-level node f and last-level node F. Given an input,\nwe define the score of each node, which is sent to its connected nodes on the next level: For any\nfirst-level node F, let score SF = λF if F is firing and SF = 0 otherwise; for any firing middle-level\n1For simplicity, we assume that a node is firing if the corresponding pixel value is above a certain threshold.\n3\nnode F, its score depends on its amplification factor as well as the scores of its dependent nodes:\nSF = λF\nX\nf→F\nSf,\nwhere the sum is over all dependent nodes f of F in the lower level. Likewise, let SF = 0 for any\nnon-firing middle-level node F. For the last-level nodes F1, . . . , FK corresponding to the K classes,\nlet\nZj =\nX\nf→Fj\nηfFjSf\n(2.1)\nbe the logit for the jth class, where the sum is over all second-last-level dependent nodes f of Fj.\nFinally, we predict the probability that this input is in the jth class as\npj(x) =\nexp(Zj)\nPK\ni=1 exp(Zi)\n.\nTo mimic the iterative characteristic of neural network training, we must be able to update the\namplification factors for neurashed in correspondence with backpropagation in the neural network.\nAt initialization, because there is no predictive ability as such for neurashed, we set λF and ηfF\nto zero, other constants, or random numbers. We assume that a node is firing if it is in the union\nof the feature pathways of all training samples in the mini-batch for computing the gradient. In\neach backpropagation, the neural network can improve its ability to sense features of images in\nthe mini-batch. In light of this, we increase the amplification ability of any firing node, thereby\nfacilitating the predictions for samples with similar feature pathways. This recognizes the famous\nadage “cells that fire together wire together” from Hebbian learning (Hebb, 2005). Specifically, if a\nnode F is firing in the backpropagation, we update its amplification factor λF by letting\nλF ←g+(λF ),\n(2.2)\nwhere g+ is an increasing function satisfying g+(x) > x for all x ≥0. The simplest choices include\ng+(x) = ax for a > 1 and g+(x) = x + c for c > 0. The strengthening of firing feature pathways is\nconsistent with a recent analysis of simple hierarchical models (Poggio et al., 2020; Allen-Zhu and\nLi, 2020). By contrast, for any node F that is not firing in the backpropagation, we decrease its\namplification factor by setting\nλF ←g−(λF )\n(2.3)\nfor an increasing function g−satisfying 0 ≤g−(x) ≤x; for example, g−(x) = bx for some 0 < b ≤\n1. This recognizes regularization techniques such as weight decay, batch normalization (Ioffe and\nSzegedy, 2015), layer normalization (Ba et al., 2016), and dropout (Srivastava et al., 2014) in deep-\nlearning training, which effectively impose certain constraints on the weight parameters (Fang et al.,\n2021). Update rules g+, g−generally vary with respect to nodes and iteration number. Likewise,\nwe apply rule g+ to ηfF when the connected second-last-level node f and last-level node F both\nfire; otherwise, g−is applied. An illustration is given in Algorithm 1.\nThe training dynamics above could improve neurashed’s predictive ability. In particular, the\nupdate rules allow nodes appearing frequently in feature pathways to quickly grow their amplification\nfactors. Consequently, updating on an image would improve the prediction on images with similar\nfeature pathways, which is consistent with the local elasticity phenomenon in real-world neural\n4\nnetworks (He and Su, 2020). Moreover, for an input x belonging to the jth class, the amplification\nfactors of most nodes in its feature become relatively large during training, and the true-class logit\nZj also becomes much larger than the other logits Zi for i ̸= j. This shows that the probability of\npredicting the correct class pj(x) →1 as the number of iterations tends to infinity.\nWhile neurashed may appear to have all features predetermined before training and not capture\nthe extraction of features, particularly for nodes in the bottom layer, this limitation is less critical\nthan it might seem.\nIn the neurashed model, one can envision the graph as containing a vast\narray of potential feature pathways, with the training dynamics described by the update rules (2.2)\nand (2.3) serving to identify the relevant features among a plethora of potentially irrelevant ones.\nThis perspective is consistent with the random feature view of neural networks (Rahimi and Recht,\n2007; Yehudai and Shamir, 2019), in which features remain fixed during training, and only the\nfeature weights are updated. Meanwhile, it is worth mentioning that a crucial distinction between\nour neurashed and the random feature model is that the former allows for hierarchy in feature\nformation.\nThe modeling strategy of neurashed is similar to a watershed, where tributaries meet to form\na larger stream (hence “neurashed”). This modeling strategy gives neurashed the innate charac-\nteristics of a hierarchical structure and iterative optimization. As a caveat, we do not regard the\nfeature representation of neurashed as fixed. Although the graph is fixed, the evolving amplifica-\ntion factors represent features in a dynamic manner. Note that neurashed is different from capsule\nnetworks (Sabour et al., 2017) and GLOM (Hinton, 2021) in that our model is meant to shed light\non the black box of deep learning, not serve as a working system.\nAlgorithm 1 Training dynamics of neurashed\ninput: neurashed graph, update rule g+ and g−, input data with feature pathways\nwhile training continues do\nSample a mini-batch of size B\nLet G be the union of all feature pathways of the mini-batch sample\nfor each node F in neurashed do\nif F ∈G then\nλF ←g+(λF )\nelse\nλF ←g−(λF )\nend if\nend for\nend while\noutput: amplification factors of all nodes\n3\nInsights into Puzzles\nImplicit regularization. Conventional wisdom from statistical learning theory suggests that a\nmodel may not perform well on test data if its parameters outnumber the training samples; to\navoid overfitting, explicit regularization is needed to constrain the search space of the unknown\nparameters (Friedman et al., 2001). In contrast to other machine learning approaches, modern neu-\nral networks—where the number of learnable parameters is often orders of magnitude larger than\n5\nthat of the training samples—enjoy surprisingly good generalization even without explicit regular-\nization (Zhang et al., 2021a). From an optimization viewpoint, this shows that simple stochastic\ngradient-based optimization for training neural networks implicitly induces a form of regularization\nbiased toward local minima of low “complexity” (Soudry et al., 2018; Bartlett et al., 2020). However,\nit remains unclear how implicit regularization occurs from a geometric perspective (Nagarajan and\nKolter, 2019; Razin and Cohen, 2020; Zhou, 2021).\nTo gain geometric insights into implicit regularization using our conceptual model, recall that\nonly firing features grow during neurashed training, whereas the remaining features become weaker\nduring backpropagation.\nFor simplicity, consider stochastic gradient descent with a mini-batch\nsize of 1. Here, only common features shared by samples from different classes constantly fire in\nneurashed, whereas features peculiar to some samples or certain classes fire less frequently. As a\nconsequence, these common features become stronger more quickly, whereas the other features grow\nless rapidly or even diminish.\nLayer one\nLayer two\nLayer three\n(a)\nLayer one\nLayer two\nLayer three\n(b)\nLayer one\nLayer two\nLayer three\n(c)\nFigure 3: Part of neurashed that corresponds to a single class. Plots (a) and (b) show two\nfeature pathways using small-batch training (the last-level (layer three) node is firing but is not\nmarked in red for simplicity). Plot (c) represents the learned neurashed models, where larger\nnodes indicate larger amplification factors. The three nodes in the middle indicate a sparse\nlearned feature pathway.\nWhen gradient descent or large-batch stochastic gradient descent are used, many features fire\nin each update of neurashed, thereby increasing their amplification factors simultaneously.\nBy\ncontrast, a small-batch method constructs the feature pathways in a sparing way. Consequently,\nthe feature pathways learned using small batches are sparser, suggesting a form of compression. This\ncomparison is illustrated in Figures 3 and 4, which imply that different samples from the same class\ntend to exhibit vanishing variability in their high-level features during later training, and is consistent\nwith the recently observed phenomenon of neural collapse (Papyan et al., 2020). Intuitively, this\nconnection is indicative of neurashed’s compressive nature.\nThe law of data separation further\nextends this connection across layers (He and Su, 2023).\n6\nLayer one\nLayer two\nLayer three\n(a)\nLayer one\nLayer two\nLayer three\n(b)\nFigure 4: Plot (a) denotes the firing pattern when both feature pathways are included in\nthe case of large-batch training (as in Figure 3, the last-level node is not marked in red for\nsimplicity). Plot (b) shows that a dense feature pathway is learned using large-batch training\ncompared to Figure 3.\nAlthough the concept that small-batch training can lead to implicit regularization remains a\nhypothesis, there is considerable supporting evidence, both empirical and theoretical. Empirical\nstudies in Keskar et al. (2016) and Smith et al. (2020) showed that neural networks trained by\nsmall-batch methods generalize better than when trained by large-batch methods. Moreover, Ilyas\net al. (2019) and Xiao et al. (2021) showed that neural networks tend to be more accurate on test\ndata if these models leverage less information of the images. From a theoretical angle, HaoChen\net al. (2020) related generalization performance to a solution’s sparsity level when a simple nonlinear\nmodel is trained using stochastic gradient descent.\nInformation bottleneck. In Tishby and Zaslavsky (2015); Shwartz-Ziv and Tishby (2017), the\ninformation bottleneck theory of deep learning was introduced, based on the observation that neural\nnetworks undergo an initial fitting phase followed by a compression phase. In the initial phase, neural\nnetworks seek to both memorize the input data and fit the labels, as manifested by the increase in\nmutual information—a measure quantifying the reduction in uncertainty about one random variable\nby observing another—between a hidden level and both the input and labels. In the second phase,\nthe networks compress all irrelevant information from the input, as demonstrated by the decrease\nin mutual information between the hidden level and input.\nInstead of explaining how this mysterious phenomenon emerges in deep learning, which is be-\nyond our scope, we shed some light on information bottleneck by producing the same phenomenon\nusing neurashed. As with implicit regularization, we observe that neurashed usually contains many\nredundant feature pathways when learning class labels. Initially, many nodes grow and thus en-\ncode more information regarding both the input and class labels. Subsequently, more frequently\nfiring nodes become more dominant than less frequently firing ones. Because nodes compete to\ngrow their amplification factors, dominant nodes tend to dwarf their weaker counterparts after a\nsufficient amount of training. Hence, neurashed starts to “forget” the information encoded by the\nweaker nodes, thereby sharing less mutual information with the input samples (see an illustration\nin Figure 5). The compressive characteristic of neurashed arises, loosely speaking, from the internal\ncompetition among nodes. This interpretation of the information bottleneck via neurashed is rem-\niniscent of the human brain, which has many neuron synapses during childhood that are pruned to\nleave fewer firing connections in adulthood (Feinberg, 1982).\nLocal elasticity. Last, we consider a recently observed phenomenon termed local elasticity (He\n7\nLayer one\nLayer two\nLayer three\n0\n0.5\n1\n1.5\n2\n2.5\n3\nMI with input\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nMI with label\nFirst level\nSecond level\nFigure 5:\nA neurashed model for a binary classification problem.\nIn the current plot,\nthe 1st, 2nd, and 7th nodes (from left to right) on the first level are firing, which are\ndenoted as (1, 2, 7).\nIn total, there are four firing patterns of Class 1 on the first level:\n(1, 2, 7), (2, 3, 7), (4, 5, 7), (5, 6, 7).\nIn the second level, the first and third nodes fire if one\nor more dependent nodes fire, and the second (dominant) node fires if two or more dependent\nnodes fire. The left panel displays a feature pathway of Class 1. Class 2 has four feature path-\nways that are symmetric to those of Class 1. The right panel shows the information bottleneck\nphenomenon for this neurashed model. As with Shwartz-Ziv and Tishby (2017), noise is added\nin calculating the mutual information (MI) between the first/second level and the input (8\ntypes)/labels (2 types). More details are given in the appendix.\nand Su, 2020) in deep learning training, which asks how the update of neural networks via backprop-\nagation at a base input changes the prediction at a test sample. Formally, for K-class classification,\nlet z1(x, w), . . . , zK(x, w) be the logits prior to the softmax operation with input x and network\nweights w. Writing w+ for the updated weights using the base input x, we define\nLE(x, x′) :=\nqPK\ni=1(zi(x′, w+) −zi(x′, w))2\nqPK\ni=1(zi(x, w+) −zi(x, w))2\n(3.1)\nas a measure of the impact of base x on test x′. A large value of this measure indicates that the\nbase has a significant impact on the test input. Through extensive experiments, He and Su (2020)\ndemonstrated that well-trained neural networks are locally elastic in the sense that the value of this\nmeasure depends on the semantic similarity between two samples x and x′. If they are similar—say,\nimages of a cat and tiger—the impact is significant, and if they are dissimilar—say, images of a\ncat and turtle—the impact is low. Experimental results are shown in Figure 6. For comparison,\nlocal elasticity does not appear in linear classifiers because of the leverage effect, which refers to the\ndisproportionate influence of an observation on the fitted value of a least-squares model when that\nobservation lies far from the bulk of the explanatory variables. More recently, Chen et al. (2020);\nDeng et al. (2021); Zhang et al. (2021b) showed that local elasticity implies good generalization\nability.\nWe now show that neurashed exhibits the phenomenon of local elasticity, which yields insights\ninto how local elasticity emerges in deep learning. To see this, note that similar training samples\nshare more of their feature pathways. For example, the two types of samples in Class 1 in Figure 2 are\npresumably very similar and indeed have about the same feature pathways; Class 1 and Class 2 are\nmore similar to each other than Class 1 and Class 3 in terms of feature pathways. Metaphorically\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\n600\n700\nUpdated with Class brown bear\nbrown bear\nleopard\ntiger\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\n600\n700\nUpdated with Class leopard\nbrown bear\nleopard\ntiger\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n100\n200\n300\n400\n500\n600\n700\nUpdated with Class tiger\nbrown bear\nleopard\ntiger\nFrequency\nLE\nLE\nLE\nFigure 6: Histograms of LE(x, x′) defined in (3.1), evaluated on the pre-trained VGG-19\nnetwork (Simonyan and Zisserman, 2014). For example, in the left panel the base input x are\nimages of brown bears. Each class contains 120 images sampled from ImageNet (Deng et al.,\n2009). Tigers and leopards are both felines and are therefore similar. The histograms show\nthat local elasticity appears to be larger when the two classes are the same or similar.\nspeaking, applying backpropagation at an image of a leopard, the feature pathway for leopard\nstrengthens as the associated amplification factors increase. While this update also strengthens\nthe feature pathway for tiger, it does not impact the brown bear feature pathway as much, which\npresumably overlaps less with the leopard feature pathway. This update in turn leads to a more\nsignificant change in the logits (2.1) of an image of a tiger than those of a brown bear. Returning\nto Figure 2 for an illustration of this interpretation, the impact of updating at a sample in Class 1a\nis most significant on Class 1b, less significant on Class 2, and unnoticeable on Class 3.\n4\nOutlook\nIn addition to shedding new light on implicit regularization, information bottleneck, and local elas-\nticity, neurashed is likely to facilitate insights into other common empirical patterns of deep learning.\nFirst, a byproduct of our interpretation of implicit regularization might evidence a subnetwork with\ncomparable performance to the original, which could have implications on the lottery ticket hypoth-\nesis of neural networks (Frankle and Carbin, 2018). Second, while a significant fraction of classes in\nImageNet (Deng et al., 2009) have fewer than 500 training samples, deep neural networks perform\nwell on these classes in tests. Neurashed could offer a new perspective on these seemingly conflicting\nobservations—many classes are basically the same (for example, ImageNet contains 120 dog-breed\nclasses), so the effective sample size for learning the common features is much larger than the size\nof an individual class. Last, neurashed might help reveal the benefit of data augmentation tech-\nniques such as cropping. In the language of neurashed, cat head and cat tail each are sufficient\nto identify cat. If both concepts appear in the image, cropping reinforces the neurashed model\nby impelling it to learn these concepts separately. Nevertheless, these views are preliminary and\nrequire future consolidation.\nWhile closely resembling neural networks in many aspects, neurashed is not merely intended to\nbetter explain some phenomena in deep learning. Instead, our main goal is to offer insights into\n9\nthe development of a comprehensive theoretical foundation for deep learning in future research. In\nparticular, neurashed’s efficacy in interpreting many puzzles in deep learning could imply that neural\nnetworks and neurashed evolve similarly during training. We therefore believe that a comprehensive\ndeep learning theory is unlikely without incorporating the hierarchical, iterative, and compressive\ncharacteristics.\nThat said, useful insights can be derived from analyzing models without these\ncharacteristics in some specific settings (Jacot et al., 2018; Chizat et al., 2019; Wu et al., 2018; Mei\net al., 2018; Chizat and Bach, 2018; Belkin et al., 2019; Lee et al., 2019; Xu et al., 2019; Oymak\nand Soltanolkotabi, 2020; Chan et al., 2021).\nIntegrating the three characteristics in a principled manner might necessitate a novel mathe-\nmatical framework for reasoning about the composition of nonlinear functions. Because it could\ntake years before such mathematical tools become available, a practical approach for the present,\ngiven that such theoretical guidelines are urgently needed (E, 2021), is to better relate neurashed\nto neural networks and develop finer-grained models. For example, an important question is to un-\nderstand the correspondence between a neural network and a neurashed model and, in particular,\nhow they speak for each other. Specifically, we need to determine the unit in neural networks that\ncorresponds with a feature node in neurashed. For example, a feature node might correspond to\na few neurons in the same layer of the neural network that are often activated at the same time.\nIndeed, recent studies showed that activation maps are sparse in modern architectures in the sense\nthat only a very small portion of neurons are activated in prediction (Li et al., 2022; Zhang et al.,\n2022). This can serve as a good starting point for investigating the relationship between neuron\nactivations in neural networks and feature pathways in neurashed, and more specifically, to explore\nthe possibility of using neurashed to improve the interpretability of neural networks in practice.\nTo generalize neurashed, edges could be fired instead of nodes. Another potential extension is to\nintroduce stochasticity to rules g+ and g−for updating amplification factors and rendering feature\npathways random or adaptive to learned amplification factors. Relaxing the constraint that the\nconnection of the neurashed graph is fixed could also be an interesting avenue to explore. Owing to\nthe flexibility of neurashed as a graphical model, such possible extensions are endless.\nAcknowledgments\nWe would like to thank Patrick Chao, Zhun Deng, Cong Fang, Hangfeng He, Qingxuan Jiang,\nKonrad Kording, Yi Ma, and Jiayao Zhang for helpful discussions and comments. We are grateful\nto two anonymous reviewers for their constructive comments that helped improve the presentation\nof the paper. This work was supported in part by an Alfred Sloan Research Fellowship and the\nWharton Dean’s Research Fund.\nReferences\nZ. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning. arXiv\npreprint arXiv:2001.04413, 2020.\nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\nA. A. Bagrov, I. A. Iakovlev, A. A. Iliasov, M. I. Katsnelson, and V. V. Mazurenko. Multiscale structural\ncomplexity of natural patterns. Proceedings of the National Academy of Sciences, 117(48):30241–30251,\n2020.\n10\nP. L. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks. In\nAdvances in Neural Information Processing Systems, pages 6241–6250, 2017.\nP. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression. Proceedings of\nthe National Academy of Sciences, 117(48):30063–30070, 2020.\nM. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the classical\nbias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854, 2019.\nJ. Berner, P. Grohs, G. Kutyniok, and P. Petersen. The modern mathematics of deep learning. arXiv preprint\narXiv:2105.04026, 2021.\nA. L. Blum and R. L. Rivest. Training a 3-node neural network is NP-complete. Neural Networks, 5(1):\n117–127, 1992.\nK. H. R. Chan, Y. Yu, C. You, H. Qi, J. Wright, and Y. Ma. ReduNet: A white-box deep network from the\nprinciple of maximizing rate reduction. arXiv preprint arXiv:2105.10446, 2021.\nS. Chen, H. He, and W. J. Su. Label-aware neural tangent kernel: Toward better generalization and local\nelasticity. In Advances in Neural Information Processing Systems, volume 33, pages 15847–15858, 2020.\nL. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using\noptimal transport. In Advances in Neural Information Processing Systems, pages 3040–3050, 2018.\nL. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. In Advances in Neural\nInformation Processing Systems, 2019.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255. IEEE\nComputer Society, 2009.\nZ. Deng, H. He, and W. J. Su.\nToward better generalization bounds with locally elastic stability.\nIn\nInternational Conference on Machine Learning, 2021.\nW. E. The dawning of a new era in applied mathematics. Notices of the American Mathematical Society,\n68(4):565–571, 2021.\nR. Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference on Learning\nTheory, pages 907–940. PMLR, 2016.\nC. Fang, H. He, Q. Long, and W. J. Su. Exploring deep neural networks via layer-peeled model: Minority\ncollapse in imbalanced training. Proceedings of the National Academy of Sciences, 2021.\nI. Feinberg.\nSchizophrenia: caused by a fault in programmed synaptic elimination during adolescence?\nJournal of Psychiatric Research, 17(4):319–334, 1982.\nV. Feldman. Does learning require memorization? a short tale about a long tail. In Symposium on Theory\nof Computing, pages 954–959, 2020.\nJ. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In\nInternational Conference on Learning Representations, 2018.\nJ. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning, volume 1. Springer Series in\nStatistics New York, 2001.\nS. Goldt, M. Mézard, F. Krzakala, and L. Zdeborová. Modeling the influence of data structure on learning\nin neural networks: The hidden manifold model. Physical Review X, 10(4):041044, 2020.\n11\nJ. Z. HaoChen, C. Wei, J. D. Lee, and T. Ma. Shape matters: Understanding the implicit bias of the noise\ncovariance. arXiv preprint arXiv:2006.08680, 2020.\nH. He and W. J. Su.\nThe local elasticity of neural networks.\nIn International Conference on Learning\nRepresentations, 2020.\nH. He and W. J. Su. A law of data separation in deep learning. Proceedings of the National Academy of\nSciences, 120(36):e2221704120, 2023.\nD. O. Hebb. The organization of behavior: A neuropsychological theory. Psychology Press, 2005.\nG. Hinton. How to represent part-whole hierarchies in a neural network. arXiv preprint arXiv:2102.12627,\n2021.\nA. Ilyas, S. Santurkar, L. Engstrom, B. Tran, and A. Madry. Adversarial examples are not bugs, they are\nfeatures. Advances in Neural Information Processing Systems, 32, 2019.\nS. Ioffe and C. Szegedy.\nBatch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In International Conference on Machine Learning, pages 448–456. PMLR, 2015.\nA. Jacot, F. Gabriel, and C. Hongler.\nNeural tangent kernel: convergence and generalization in neural\nnetworks. In Advances in Neural Information Processing Systems, pages 8580–8589, 2018.\nN. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep\nlearning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on\nLearning Representations, 2015.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet classification with deep convolutional neural\nnetworks. Communications of the ACM, 60(6):84–90, 2017.\nY. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.\nJ. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington. Wide neural\nnetworks of any depth evolve as linear models under gradient descent. arXiv preprint arXiv:1902.06720,\n2019.\nZ. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi, K. Ye, F. Chern, F. Yu, R. Guo, and\nS. Kumar. Large models are parsimonious learners: Activation sparsity in trained transformers. arXiv\npreprint arXiv:2210.06313, 2022.\nS. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks.\nProceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.\nV. Nagarajan and J. Z. Kolter. Uniform convergence may be unable to explain generalization in deep learning.\nIn Advances in Neural Information Processing Systems, volume 32, 2019.\nS. Oymak and M. Soltanolkotabi. Towards moderate overparameterization: global convergence guarantees\nfor training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 2020.\nV. Papyan, X. Han, and D. L. Donoho. Prevalence of neural collapse during the terminal phase of deep\nlearning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020.\nT. Poggio, A. Banburski, and Q. Liao. Theoretical issues in deep networks. Proceedings of the National\nAcademy of Sciences, 117(48):30039–30045, 2020.\n12\nA. Rahimi and B. Recht. Random features for large-scale kernel machines. Advances in Neural Information\nProcessing Systems, 20, 2007.\nN. Razin and N. Cohen. Implicit regularization in deep learning may not be explainable by norms. Advances\nin Neural Information Processing Systems, 33, 2020.\nS. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. In Advances in Neural Infor-\nmation Processing Systems, volume 31, pages 3859–3869, 2017.\nR. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information. arXiv\npreprint arXiv:1703.00810, 2017.\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search.\nNature, 529(7587):484–489, 2016.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nS. Smith, E. Elsen, and S. De. On the generalization benefit of noise in stochastic gradient descent. In\nInternational Conference on Machine Learning, pages 9058–9067. PMLR, 2020.\nD. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on\nseparable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018.\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to\nprevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929–1958,\n2014.\nN. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Infor-\nmation Theory Workshop (ITW), pages 1–5. IEEE, 2015.\nI. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, D. Keysers, J. Uszko-\nreit, M. Lucic, and A. Dosovitskiy.\nMlP-mixer: An all-MLP architecture for vision.\narXiv preprint\narXiv:2105.01601, 2021.\nL. Wu, C. Ma, and W. E. How SGD selects the global minima in over-parameterized learning: A dynamical\nstability perspective. In Advances in Neural Information Processing Systems, pages 8289–8298, 2018.\nK. Xiao, L. Engstrom, A. Ilyas, and A. Madry. Noise or signal: The role of image backgrounds in object\nrecognition. In International Conference on Learning Representations, 2021.\nZ.-Q. J. Xu, Y. Zhang, and Y. Xiao. Training behavior of deep neural network in frequency domain. In\nInternational Conference on Neural Information Processing, pages 264–274. Springer, 2019.\nG. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural\nnetworks. Advances in Neural Information Processing Systems, 32, 2019.\nL. Zdeborová. Understanding deep learning is also a job for physicists. Nature Physics, 16(6):602–604, 2020.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires\nrethinking generalization. Communications of the ACM, 64(3):107–115, 2021a.\nJ. Zhang, H. Wang, and W. J. Su. Imitating deep learning dynamics via locally elastic stochastic differential\nequations. In Advances in Neural Information Processing Systems, volume 34, 2021b.\n13\nZ. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou. Moefication: Transformer feed-forward layers are\nmixtures of experts. In Findings of the Association for Computational Linguistics: ACL 2022, pages\n877–890, 2022.\nZ.-H. Zhou. Why over-parameterization of deep neural networks does not overfit? Science China Information\nSciences, 64(1):1–3, 2021.\n14\nAppendix\nFeature pathways in Figure 5. All eight feature pathways of the neurashed model in Figure 5.\nThe left column and right column correspond to Class 1 and Class 2, respectively.\nExperimental details. In the experimental setup of the right panel of Figure 5, all amplifica-\ntion factors at initialization are set to independent uniform random variables on (0, 0.01). We use\ng−(λF ) = 1.022−1\n4 λF and g+(λF ) = 1.022\n11\n4 λF for all hidden nodes except for the 7th (from left to\nright) node, which uses g+(λF ) = 1.022\n3\n4 λF . In the early phase of training, the firing pattern on the\nsecond level improves at distinguishing the two types of samples in Class 1, depending on whether\nthe 1st or 3rd node fires. This also applies to Class 2. Hence, the mutual information between the\nsecond level and the input tends to log2 4 = 2. By contrast, in the late stages, the amplification\nfactors of the 1st and 3rd nodes become negligible compared with that of the 2nd node, leading to\nindistinguishability between the two types in Class 1. As a consequence, the mutual information\ntends to log2 2 = 1. The discussion on the first level is similar and thus is omitted.\nAnalyses for local elasticity. A common question from the reviews is whether neurashed can\noffer more quantitative results and insights into other phenomena. I’ve obtained the following new\nresults.\n15\nTheorem 1. For a neurashed with m nodes in each level, select b1 nodes uniformly at random at\neach level except for the last level; next, for each class, select b2 nodes uniformly at random at each\nlevel except for the last level; last, for each class, draw edges from the b1 + b2 selected nodes to those\nin the consecutive levels. Let g+(x) = c + x and g−(x) = x for some c > 1. Suppose that each class\nhas the same number of training examples. In the regime where the number of iterations tends to\n∞and m is sufficiently large, a backpropagation at Class j gives\n∆Zj\n∆Zj′ →b1+b2\nb1\n> 1.\nAbove, ∆Zj denotes the change of the logit associated with Class j.\nThe above inequality\nimplies that an image has a more significant impact on images from the same class compared with\nthose from other classes. This is a quantitative characterization of local elasticity. Next, regarding\nthe effect of the number of layers, we have the following theorem.\nTheorem 2. Under the same assumptions as the above theorem, if the number L of layers grow\nsufficiently quickly, then we have\n∆Zj\n∆Zj′ →∞.\nHere we provide some intuition for the proof.\nThe starting point is to use the asymptotic\nexpansion ∆Zj = (1+oP(1))[(c(t+1)b2 +c(Kt+1)b1)L −(ctb2 +cKtb1)L]. Besides, I now can show\nthat neurashed can also shed light on self-supervised/contrastive learning. Roughly speaking, related\ninputs have similar feature pathways and so tend to be grouped together in new representations.\nMore details to follow in the final revision.\nAfter N iterations, suppose there are N1, N2, and N3 times from the three classes, respectively.\nThen\nfj(x) =\n\n\n\n\n\n\n\n\n\n\n\na2N1(7N1 + 5N2 + 2N3)\ny = Cj, j = 1\na2N2(5N1 + 7N2 + 2N3)\ny = Cj, j = 2\n2a2N3(N1 + N2 + N3) + aN3f′\ny = Cj, j = 2\n0\ny ̸= Cj,\nwhere f′ denotes the contribution from F.\nNow, suppose we update the neural networks using an example from C1. That is, replace N1 by\nN+\n1 := N1 + 1. Let us see how the prediction on C2 and C3 changes. Note that\na2N+\n1 (7N+\n1 + 5N2 + 2N3) −a2N1(7N1 + 5N2 + 2N3)\n= a2(N1 + 1)(7N1 + 5N2 + 2N3 + 7) −a2N1(7N1 + 5N2 + 2N3)\n= 7a2N1 + a2(7N1 + 5N2 + 2N3 + 7)\nand for C2 we see\na2N2(5N+\n1 + 7N2 + 2N3) −a2N2(5N1 + 7N2 + 2N3)\n= 5a2N2.\nfor C3 we have\n2a2N3(N+\n1 + N2 + N3) + aN3f′ −2a2N3(N1 + N2 + N3) −aN3f′\n= 2a2N3\nNote that in generally 5a2N2 > 2a2N3 unless N3 is really large. However, to overcome this\ndifficulty, we can say that for nodes in F L, their amplifying factors are constant, meaning that they\nare all always on or never get updated. For example, we can let the F L nodes be on if any of its\nchildren node is on.\n16\n",
  "categories": [
    "cs.LG",
    "cond-mat.dis-nn",
    "cond-mat.stat-mech",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2021-12-17",
  "updated": "2024-08-09"
}