{
  "id": "http://arxiv.org/abs/2112.10459v1",
  "title": "Safe multi-agent deep reinforcement learning for joint bidding and maintenance scheduling of generation units",
  "authors": [
    "Pegah Rokhforoz",
    "Olga Fink"
  ],
  "abstract": "This paper proposes a safe reinforcement learning algorithm for generation\nbidding decisions and unit maintenance scheduling in a competitive electricity\nmarket environment. In this problem, each unit aims to find a bidding strategy\nthat maximizes its revenue while concurrently retaining its reliability by\nscheduling preventive maintenance. The maintenance scheduling provides some\nsafety constraints which should be satisfied at all times. Satisfying the\ncritical safety and reliability constraints while the generation units have an\nincomplete information of each others' bidding strategy is a challenging\nproblem. Bi-level optimization and reinforcement learning are state of the art\napproaches for solving this type of problems. However, neither bi-level\noptimization nor reinforcement learning can handle the challenges of incomplete\ninformation and critical safety constraints. To tackle these challenges, we\npropose the safe deep deterministic policy gradient reinforcement learning\nalgorithm which is based on a combination of reinforcement learning and a\npredicted safety filter. The case study demonstrates that the proposed approach\ncan achieve a higher profit compared to other state of the art methods while\nconcurrently satisfying the system safety constraints.",
  "text": "arXiv:2112.10459v1  [eess.SY]  20 Dec 2021\nSafe multi-agent deep reinforcement learning for joint bidding and\nmaintenance scheduling of generation units\nPEGAH ROKHFOROZ1 AND OLGA FINK1\nAbstract. This paper proposes a safe reinforcement learning algorithm for generation bidding decisions\nand unit maintenance scheduling in a competitive electricity market environment.\nIn this problem, each\nunit aims to ﬁnd a bidding strategy that maximizes its revenue while concurrently retaining its reliability\nby scheduling preventive maintenance. The maintenance scheduling provides some safety constraints which\nshould be satisﬁed at all times. Satisfying the critical safety and reliability constraints while the generation\nunits have an incomplete information of each others’ bidding strategy is a challenging problem.\nBi-level\noptimization and reinforcement learning are state of the art approaches for solving this type of problems.\nHowever, neither bi-level optimization nor reinforcement learning can handle the challenges of incomplete\ninformation and critical safety constraints. To tackle these challenges, we propose the safe deep deterministic\npolicy gradient reinforcement learning algorithm which is based on a combination of reinforcement learning\nand a predicted safety ﬁlter. The case study demonstrates that the proposed approach can achieve a higher\nproﬁt compared to other state of the art methods while concurrently satisfying the system safety constraints.\n1. Introduction\nFinding an optimal bidding strategy and maintenance scheduling of generation units in an electricity\nmarket would lead to a higher proﬁt and an improved reliability of the entire system. The electricity market\ncreates competition among the units since the proﬁt of each unit depends on the market clearing price which\nis determined by the system operator (ISO), while the market clearing price is impacted by the bidding\nstrategies of all units [1]. This problem can be categorized as a multi-agent bi-level decision-making problem\n[2]. The units, which are referred to as agents, are strategic and price-takers. In the ﬁrst level, the agents\ndecide about their strategies individually. In the second level, the ISO clears the market and obtains the\npower generation amount for each unit such that the demand of the system can be satisﬁed in all periods of\ntime [3, 4].\nIn this competition, the main aim of the unit is to maximize its proﬁt by choosing the best bidding strategy.\nIn addition to this goal, increasing the reliability and safety is another important objective that should be\nconsidered by the unit [5]. In order to retain the reliability, the units need to perform preventive maintenance\nactions that take a deﬁned period of time enabling, thereby, to prolong the lifetime of the units. However,\nperforming maintenance imposes maintenance costs on the units. In this case, the units need to obtain their\noptimal maintenance scheduling which makes a trade-oﬀbetween increasing reliability and decreasing the\nmaintenance cost of the system [6, 7]. Hence, in summary, the goal of the unit in the electricity market\ncan be considered as ﬁnding the optimal bidding strategies and maintenance schedule which maximizes the\nsystem’s proﬁts as well as reliability and minimizes the maintenance cost.\nNash equilibrium (NE) is one typical solution to the competition problem where no unit can increase its\nproﬁt by changing its strategies while the other units are not changing their strategies [8]. Many papers\naddress the optimal bidding of the electricity market using the game theory approach [9, 10, 1, 11, 12]. In\nthese research studies, increasing reliability and maintenance schedules are, typically, not considered in the\nobjective functions of the units. In fact, ﬁnding the NE of this problem is a challenging problem since the\n(1) Chair of Intelligent Maintenance Systems, ETH Zurich, Switzerland.\nCorresponding author, email\naddress:ofink@ethz.ch.\nDate: December 21, 2021.\n1\n2\nunits are neither aware of the bidding of others nor of the market-clearing price of the system. In addition,\nanother challenge is that the electricity demand should be satisﬁed at all time even when some units are out\nof operation and are performing maintenance. This imposes some constraints on the maintenance scheduling\nof units and the NE of the game should satisfy these critical constraints. Hence, the units face the incomplete\ninformation game associated with some uncertainties and constraints [2]. This is a challenging problem and\nneeds some coordination among the units [13, 14]\nBesides the works that address the bidding strategies of units, many papers address the distributed main-\ntenance scheduling problem [15, 16].\nThese papers proposed optimization approaches in which the units\ndecide about their maintenance scheduling. To handle the challenges of electricity demand satisfaction, an\nalgorithm based on the coordination and negotiation between ISO and units is proposed in [17, 18]. These\npapers assume that units send their marginal cost as the bidding price to the ISO. Therefore, obtaining the\noptimal bidding strategies is not their goal.\nAll of the above works assume that the units have complete information of the market-clearing price as\nwell as the bidding of their competitors. Thereby, they can solve the optimization problem and reach their\nNE. It is worth noting that this is a limiting assumption in the real market system.\nReinforcement learning (RL) is a promising tool to handle challenges of incomplete information and system\nuncertainty [19]. In the case of an electricity market, this incomplete information and uncertainty are due to\nthe unknown information about the actions of other units as well as the market clearing price.\nQ-learning is one of the typical RL algorithms which applies look-up tables to ﬁnd the optimal strategy\nin each state of the system[20] . However, since a bidding strategy in electricity markets is a continuous\naction, this method has a high computational time [21]. Deep Q-learning can tackle this challenge by using\nneural networks as function approximators for obtaining the strategies of the units [22, 23]. Some research\nstudies address the bidding strategies using deep RL algorithm [2, 24, 25]. However, they do not consider\nmaintenance scheduling in addition to the bidding strategies.\nAs mentioned above, applying maintenance scheduling as one of the aims of the generation units imposes\nsome critical constraints on the system such as electricity demand satisfaction and allocating the time for\nperforming preventive maintenance.\nThese constraints related to system safety and reliability should be\nsatisﬁed at all time periods. Unfortunately, deep RL cannot ensure that all the system constraints can be\nsatisﬁed during the training of the units.\nOne way to solve this issue is to combine a predicted safety ﬁlter with a RL algorithm [26]. In the proposed\napproach, a safety ﬁlter is responsible for satisfying system safety constraints and ﬁnding strategies which are\nas close as possible to the RL strategies while also satisfying all the system constraints. In other words, if RL\nstrategies violate some constraints of the system, the predicted safety ﬁlter adjusts the decisions of the units\nto ensure that the constraints are satisﬁed.\nIn this paper, we propose to deﬁne the joint bidding and maintenance scheduling of generation units in\nthe electricity market system as a safe deep RL problem. The proposed approach is able to satisfy the safety\nconstraints and also handle the challenges of incomplete information due to the unknown bidding strategies\nof other units. In this approach, the units do not need to know the model and the bidding strategies of other\nunits. They can update their strategies by getting feedback and reward from the electricity market system in\neach time step of the learning algorithm. In other words, at each time step, the units obtain their strategies\nand send them to the ISO. Then, they obtain their proﬁt based on the information that they get from ISO,\nand update their policy. The main contributions of this paper are as follows:\n1) We propose to formulate the maintenance scheduling and electricity bidding process of multiple gen-\neration units as a bi-level optimization problem. In the ﬁrst level, the units aim to obtain the strategies\nwhich maximize their reward function while satisfying the system safety constraints related to the mainte-\nnance scheduling of agents. In the second level, the ISO clears the market and obtains the electricity price\n3\nand power generation of each unit such that the total cost of the system is minimized and all the network\nconstraints are satisﬁed.\n2) We develop a multi-agent deep RL using the deterministic policy gradient (DDPG) algorithm to obtain\nthe maintenance scheduling and bidding price such that the reward function of the agents is maximized\nwithout considering the safety constraints. This algorithm can handle the high-dimensional continuous action\nand state space. Moreover, it can tackle the non-stationary and uncertainties of the environment where each\nunit is not aware of the decisions of other units.\n3) We design a predicted safety ﬁlter to guarantee that all the safety constraints are satisﬁed. This ﬁlter\nensures that during the training as well as testing of the RL algorithm the units can perform maintenance\nfor some speciﬁed time intervals. Moreover, this ﬁlter provides the conditions which ensure that the load\nof the system can be satisﬁed at all times by limiting the number of units that can perform maintenance\nsimultaneously.\n4) The proposed methodology can be applied online to the real system since the safety ﬁlter makes sure\nthat even during the training of the learning algorithm all the safety and demand constraints can be satisﬁed.\nIn other words, the agent does not need to learn its strategies oﬄine and we do not need to build a virtual\nelectrical market environment which is associated with many uncertainties.\nThe rest of the paper is organized as follows. The preliminaries on RL methodology and algorithm are\nintroduced in Section 2. The units’ and ISO’s objective functions are formulated in Section 3. The solution\nmethod based on a combination of RL and predicted safety ﬁlter is proposed in Section 4. Simulation results\nof the case study are presented in Section 5. Concluding remarks are made in Section 6.\n1.1. Indices and sets\nN: set of units {1, · · · , N} indexed by i.\nNs: set of samples {1, · · · , Ns} indexed by j.\nT : set of operational intervals {1, · · · , T} indexed by t.\n1.2. Parameters\nt: Number of time period\ngmax\ni\n: maximum power generation of generation unit i (MW)\ngmin\ni\n: minimum power generation of generation unit i (MW)\nλi,m: marginal cost of generation unit i (\n$\nMW )\nkmax\ni\n: maximum bidding of generation unit i\nRU\ni , RD\ni : ramp up/down of generation unit i\nM: Maximum number of generation units that can perform maintenance simultaneously\nHi: Required duration of preventive maintenance of generation unit i\nDi: Minimum duration of a single maintenance action of generation unit i\nci: Maintenance cost of unit i\n1.3. Variables\ngi(t): power generation of generation unit i at time t (MW)\nλ(t): electrical market price at time t (\n$\nMW )\nui(t): maintenance decision of generation unit i at time t\nki(t): bidding decision of generation unit i\n4\n2. Preliminary on RL algorithm\nThe aim of RL is to maximize the total discounted reward function where the environment can be modeled\nas a Markov decision process (MDP) [19]. An action-value function is deﬁned as follows:\nQπ(s(t), a(t)) = Eπ\nh NT\nX\nk=0\nγkr(t + k + 1)|s(t), a(t), π\ni\n,\nwhere s(t) ∈S and a(t) ∈A are the state and the action at time t. The policy is the probability of choosing\naction a given state s. The reward and discount factor are expressed by r and γ where γ ∈[0, 1). The\nexpected value of the reward over policy π is given by Eπ[ · ]. The aim of the RL algorithm is to ﬁnd the\npolicy which maximizes the action-value function as follows:\nQ∗(st, at) = max\nπ\nQπ(st, at).\n(1)\nThe conventional RL methods such as Q-learning can solve the action-value function (2) using a look-up\ntable. In this case, a table is established to store the action-value function values for all possible state-action\npairs. One typical way to update a table is to implement the temporal diﬀerence method [27] in which the\nvalue-action function is updated as follows:\nQl+1\nπ\n\u0000s(t), a(t)\n\u0001\n= r(t) + γ max\na(t+1) Ql\nπ\n\u0000s(t + 1), a(t + 1)\n\u0001\n,\nwhere l is the iteration index.\nHowever, in cases where either the state or action space is continuous, Q-learning faces the “curse of\ndimensionality” problem. Deep RL is one way to tackle this challenge. In this method, instead of a look-up\ntable, a neural network is trained to estimate the action-value function. This network can create a continuous\nmapping from the state and action pairs to the action-value function.\n3. Problem formulation\nIn this section, we formulate the optimization problem of generation units and ISO for the electrical market\nwith a uniform price. In the proposed model, the generation units decide about their maintenance scheduling\nui and bidding strategy ki while the ISO obtains the amount of power generation for each unit gi and the\nelectrical market price λi through the following optimization problems.\n3.1. Maintenance model of generation units\nIn this part, inspired by [18], we model the unit maintenance cost and its corresponding constraints. In\nthe proposed model we assume that unit i is charged the maintenance cost ci at each time while performing\nmaintenance and seeks to ﬁnd the strategy that minimizes the total maintenance cost. In addition, we assume\nthat during the decision horizon T unit i needs to perform maintenance at least for the total duration of\nHi. This preventive maintenance activity ensures that the generation unit can retain the system reliability.\nWe assume that Hi is determined by the safety regulation for each unit which is set based on the generation\nunit’s data and model. Based on these preventive maintenance activities, we impose the following constraint:\nX\nt∈T\nui(t) ≥Hi,\n∀i ∈N .\n(2)\nIn addition, there is another constraint which represents the condition that if the maintenance of each unit\nstarts at time t, it should be completed, which takes Di periods of time. We formulate this constraint as\nfollows:\nui(t) −ui(t −1) ≤ui(t + Di −1),\n∀t ∈T , ∀i ∈N.\n(3)\n5\nIt is worth to mention that these two constraints, (3.1) and (3.1), imply that for the whole decision horizon\nof time t ∈T , unit i can perform maintenance several times with an interruption between them, provided\nthat each maintenance action takes Di periods of time and the total period of maintenance is larger than the\nrequired duration Hi of preventive maintenance of generation unit i.\nIn the following, we propose the optimization model of generation units using the above discussion.\n3.2. Optimization model of generation units\nIn this section, we propose the optimization problem in which the goal of the units is to obtain their\noptimal bidding and maintenance scheduling strategies which maximize their revenue and also satisfy the\nmaintenance constraints (Equations (3.1) and (3.1)). In this optimization problem formulation for clarity\nreasons and without loss of generality, we assume that each producer has only one generation unit.\nIn\naddition, we assume that the start up and shut down costs are zero and each unit has a linear marginal cost.\nThe optimization problem is formulated as follows:\nmax\nki,ui\nX\nt∈T\n\u0010\nλ(t)−λi,m−c\niui(t)\n\u0011\nsubject to :\nA1 : 1 ≤ki(t) ≤kmax\ni\n,\n∀t ∈T ,\nA2 :\nX\nt∈T\nui(t) ≥Hi,\nA3 : ui(t) −ui(t −1) ≤ui(t + Di −1),\n∀t ∈T ,\nA4 : ui(t) = {0, 1},\n∀t ∈T ,\n(4)\nwhere the ﬁrst term in the objective function λ(t)gi(t) expresses the revenue that unit i obtains by producing\nand selling the power.\nThe second term λi,mgi(t) and the third term ciui(t) are the marginal and the\nmaintenance costs of unit i. Constraint A1 expresses the limit of the strategic bidding variable. In fact,\neach unit chooses a bidding strategy ki(t) and oﬀers ki(t)λi,m where ki(t) = 1 means that unit i oﬀers its\nactual marginal cost and behaves competitively, and 1 < ki(t) ≤kmax\ni\nmeans that unit i behaves strategically.\nConstraints A2 and A3 are the maintenance constraints which have been explained in Section 3.1. Constraint\nA4 denotes that the maintenance decision is a binary variable where ui(t) = 1 indicates that the unit performs\nmaintenance at time t.\nIn problem (3.2), the price of the market λ(t) and the power generation of units gi(t) are obtained through\nthe ISO optimization problem which is described in the following section.\n3.3. ISO optimization model\nIn this section, we derive the optimization formulation of ISO where the maintenance decision ui and\nbidding strategy ki of the units are obtained through Eq.(3.2). ISO optimization aims to minimize the cost\nof the market while satisfying the load demand of the system. To achieve this aim, we formulate the problem\nas follows:\n6\nmin\ng\nX\nt∈T\nX\ni∈N\nki(t)λi,mgi(t)\nsubject to :\nC1 :\nX\ni∈N\n=d(t) :\nλ(t),\n∀t ∈T ,\nC2 : (1 −ui(t))gmin\ni\n≤gi(t) ≤(1 −ui(t))gmax\ni\n,\n∀i ∈N , ∀t ∈T ,\nC3 : −gi(t−1) ≤RU\ni ,\n∀i ∈N , ∀t ∈T ,\nC4 : gi(t −1) −≤RD\ni ,\n∀i ∈N, ∀t ∈T ,\n(5)\nwhere the objective function is to minimize the total costs of the units. The clearing price of the system\nλ is the dual multiplier of Constraint C1. Constraint C1 expresses the demand-supply balance constraint.\nConstraint C2 denotes the maximum and minimum power that unit i can produce. In fact, this constraint\nimplies that when the unit is performing maintenance, its production output is zero. Constraints C3 and C4\nexpress the ramp up and ramp down constraints of unit i.\nIn the following section, we explain our proposed methodology to solve problems (3.2) and (3.3).\n4. Solution methodology: safe multi-agent deep RL\nAs we can see from the problem formulation in Section 3, we face a bi-level optimization problem. In\nthis problem, in the upper level, the units aim to maximize their reward by obtaining their optimal bidding\nstrategies and maintenance schedule. Also, the objective function of problem (3.2) depends on the price λ(t)\nof the market which can be obtained through (3.3). Since the optimization of (3.3) depends on the decision\nvariables of all units, λ(t) is the function of all other units’ decisions. The dependency of λ(t) on all the\nunits’ decision variables makes problem (3.2) a challenging problem. On the one hand, each unit cannot\nobtain its decision variables without considering the decisions of other units. On the other hand, each unit\nis not aware of the decisions of other units. Hence, unit i faces some uncertainty when solving problem (3.2).\nTo solve these challenges, we propose a algorithm which is a combination of RL and a safety ﬁlter. The\nproposed algorithm comprises two parts. In the ﬁrst part, the units learn the strategies of other units using\nRL algorithm. In the second part, all the critical safety constraints (A2, A3) are satisﬁed using the safety\nﬁlter. In other words, the RL algorithms is able to ﬁnd the strategies of the units without considering the\nsafety constraints. Then, the units send their maintenance decisions to the safety ﬁlter and the ﬁlter obtains\nthe decisions which are close to the decisions of the units while satisfying the safety constraints as well. In\nthe next step, the bidding and maintenance decisions are sent to the ISO and it obtains the price and power\ngeneration of the units by clearing the market. Based on this information, the units update their strategies\nin order to gain more proﬁt. The schematic of this approach is shown in Figure 1.\nThe formulation of the RL algorithm and the predicted safety ﬁlter are explained in the following sections.\n4.1. Deep RL using DDPG algorithm\nAs mentioned above, the RL algorithm aims to obtain the strategies of the agents which consist of mainte-\nnance scheduling and bidding algorithm without considering the safety constraints A2-A3 of (3.2). Since the\nbidding strategy is a continuous action, we apply deep RL using deep deterministic policy gradient (DDPG)\nalgorithm in which the two neural networks are used to obtain on the one hand the value function and on\nthe other hand the policy for each of the units. To implement this algorithm, we need to deﬁne the state, the\naction, and the reward of each agent. Let us deﬁne the state of the system at time t as follows:\ns(t) = (λ(t −1), d(t −1)),\n(6)\n7\nFigure 1. Schematic of the proposed solution methodology.\nwhich consists of the electrical price and the demand of the system at the previous time step. This state can\nbe provided by ISO and describes the system environment. The action of unit i can be deﬁned as follows:\nai(t) = (ui(t), ki(t)),\n(7)\nwhere ui(t) = {0, 1} and 1 ≤ki(t) ≤kmax\ni\nare the maintenance and bidding strategies of generating unit i.\nLet us consider the objective function of (3.2), the reward function of unit i at time t can be formulated\nas:\nri(t) = λ(t)−λi,m−c\niui(t).\n(8)\nEquation (4.1) implies that the reward of unit i is the price that it obtains by selling its power to the market\nsubtracting its generation and maintenance cost.\nIn the following, we explain the details of the DDPG algorithm and how the neural network weights\nare updated. As mentioned before, this algorithm comprises two neural networks which are estimating the\naction-value function and the policy of the units. They are also known as actor-critic networks. It should\nbe mentioned that since we have N units and each unit decides about its own strategy individually, we have\nan individual actor-critic network for each unit. In other words, we have N actor-critic networks in which\neach unit updates the weights of the corresponding network using the DDPG algorithm. In this algorithm,\neach agent ﬁrst obtains its action ai(t) using the actor-network. In the next step, it executes the action and\ngets the reward ri(t) and the new state s(t + 1). Then, it stores the tuple (s(t), s(t + 1), ai(t), ri(t)) and uses\na mini-batch (sj(t), sj(t + 1), aj\ni(t), rj\ni (t)) to update the actor-critic network weights. In the following, we\nexplain the details of the DDPG algorithm.\n8\nCritic network: In the DDPG algorithm, for each unit, two critic networks are considered to estimate\nthe action-value functions which are called target and behaviour critic networks. The inputs of the networks\nare the state s(t) and the action of the units ai(t) which are deﬁned as (4.1) and (4.1), respectively. The\noutput is the associated action-value function Qi(s(t), ai(t)). Let us consider θ′\ni and θi as the weights of the\ntarget and the behaviour critic networks of unit i, respectively. The loss function for updating the weights of\nthe behaviour critic networks is deﬁned as follows:\nLi(θi) = 1\nNs\nX\nj∈Ns\n\u0000Qj\ni,target(t) −Qi(sj(t), aj\ni(t); θi)\n\u00012,\n∀i ∈N,\nwhere the target network Qi,target is calculated as follows:\nQj\ni,target(t) = rj\ni (t) + γ max\naj\ni (t+1)\nQi(sj(t + 1), aj\ni(t + 1), θ′\ni),\n∀j ∈Ns,\n(9)\nwhich in fact is the summation of the current reward ri(t) and the discounted value of the maximum action-\nvalue at the next time step t + 1 which is obtained by the target network.\nDuring training and updating of the weights of the networks, the target network is updated at a slower\nspeed than the behaviour critic network. This condition helps to stabilize the learning process. The weights\nof the behaviour network for unit i are updated as follows:\nθi ←θi −ηi∇θiLi(θi),\n(10)\nwhere ηi is the learning rate of the behavior network.\nThe weights of the target critic network for unit i are updated as follows:\nθ′\ni ←(1 −τi)θi + τiθ′\ni,\n(11)\nwhere τi is the learning rate of the target network of unit i.\nActor network: The actor network of unit i is designed to obtain the optimal policy by considering the\nestimated action-value function, which is deﬁned as:\nπi(s(t)) = arg max\nai(t) Qi(s(t), ai(t)).\nThe input of the actor network is the current state s(t) and the output is the action ai(t) which maximizes\nQi(s(t), ai(t)). Let us deﬁne the loss function of the actor network as follows:\nLi(θµ\ni ) = −1\nNs\nX\n∀j∈Ns\n\u0000Qi(sj(t), aj\ni (t); θi)\n\u0001\n|aj\ni (t)=µi(sj(t),θµ\ni )\n\u0001\n,\n(12)\nwhere θµ\ni is the weight of actor network and µi(sj(t), θµ\ni ) is the policy generated by the network. In fact, the\ngoal of unit i is to ﬁnd the policy which maximizes Qi(s(t), ai(t)). Hence, by minimizing the loss function as\n(4.1) we can obtain the policy that maximizes Qi(s(t), ai(t)).\nThe weights of the actor network are updated as follows:\nθµ\ni ←θµ\ni −ηµ\ni ∇θµ\ni L(θµ\ni ),\n(13)\nwhere ηµ\ni is the learning rate of the actor network of unit i.\nIn the same way as for critic network, for the sake of stability, there is also a target actor network whose\nweights are updated as:\nθ\n′µ\ni\n←(1 −τ µ\ni )θi + τµ\ni θµ\ni ,\n(14)\nThe schematic of the DDPG algorithm for unit i is depicted in Figure 2.\n9\nFigure 2. Schematic of the DDPG algorithm for unit i. Dash lines show the information\nthat is used for updating the weights of networks.\n4.2. Predicted safety ﬁlter model\nAs mentioned above, we aim to formulate the safety ﬁlter which obtains a maintenance decision close to the\nunits’ original decision while making sure that the updated decision guarantees that all the safety constraints\nof problem (3.2) are satisﬁed for the predicted horizon time as well. To achieve this aim, at each instant t,\nthe units send their decisions ui(t) to the predicted safety ﬁlter. Then, the predicted safety ﬁlter obtains\nmaintenance decisions uf,i(t) which satisfy constraints A2, A3 for all periods of t′ = {t, · · · , T}. Moreover,\none additional aim is to ensure that while several units perform maintenance simultaneously and are not able\nto produce any power in that period of time, the load at the network level can still be satisﬁed. To achieve\nthis goal, we restrict the number of units that can perform maintenance simultaneously. By considering all\nof these restrictions, we propose to formulate the safety ﬁlter optimization as follows:\n10\nmin\nuf,i\nX\ni∈N\n∥uf,i(t) −ui(t)∥2\nS1 : xf,i(t′ + 1) = (1 −uf,i(t′))xf,i(t′) + (1 −uf,i(t′)),\nt′ = {t, · · · , T}, ∀i ∈N\nS2 : xf,i(T ) ≥Hi, ∀i ∈N\nS3 :\nX\ni∈N\nuf,i(t′) ≤M,\nt′ = {t, · · · , T},\nS4 : uf,i(t′) −uf,i(t′ −1) ≤uf,i(t′ + Di −1),\nt′ = {t, · · · , T}, ∀i ∈N\nS5 : uf,i(t′) = {0, 1},\nt′ = {t, · · · , T}, ∀i ∈N\n(15)\nwhere xf,i(0) = 0. The objective function implies that if the maintenance decisions of the units ui(t) can\nsatisfy all the constraints, the output of the safety ﬁlter is the same as the units’ decisions. Constraints S1\nand S2 are equivalent to constraint A2 of problem (3.2). In fact, xf,i(t′) measures the duration that the unit\nperforms maintenance which at the end of the decision horizon T should be longer than Hi. Constraint S3\nensures that maximum M units can perform maintenance simultaneously, in which choosing the appropriate\nvalue of M guarantees that at each time instant there is a suﬃcient number of units in the power grid system\nthat can satisfy the electrical demand of the system. Constraints S4 and S5 are the same as A3 and A4 which\nimply that the units need Di time for performing a maintenance action and the decision of the predicted\nsafety ﬁlter is a binary variable.\nDue to the nonlinear constraint S1, Problem (4.2) is a nonlinear programming problem. Using big-M\nmethod, we convert (4.2) to a mixed integer linear programming (MILP) problem. Let us consider Zi(t′) =\nuf,i(t′)xf,i(t′) and add the following constraints to (4.2) as an equivalent of the nonlinear term:\n0 ≤Zi(t′) ≤uf,i(t′)M\nZi(t′) ≥xf,i(t′) −(1 −uf,i(t′))M\nZi(t′) ≤xf,i(t′) + (1 −uf,i(t′))M,\n(16)\nwhere M is a large positive constant [28].\nThen, using (4.2), we have the following MILP optimization:\nmin\nuf,i,Zi\nX\ni∈N\n∥uf,i(t) −ui(t)∥2\nS1 : xf,i(t′ + 1) = xf,i(t′) −Zi(t′) + (1 −uf,i(t′)),\nt′ = {t, · · · , T},\nS2 −S5, (4.2).\n4.3. Learning algorithm for electricity market bidding and maintenance scheduling\nIn this section, we summarize the proposed formulation of safe RL in Algorithm 1. This algorithm shows\nhow all units can ﬁnd their maintenance scheduling and electricity market bidding.\n5. Case study evaluation results\nIn this section, we evaluate the performance of the proposed algorithm on IEEE 30-bus system with\n6 generation units [29]. We consider a linear marginal cost for each generation and neglect the ramp up\nand down of the generating units. The maximum bidding kmax\ni\nis considered as being 2 (Constraint A1 in\n3.2). Moreover, to increase the safety of the units, we impose the constraint that each unit should perform\nmaintenance for at least 1 day during 100 days (Constraint A2 in (3.2)). In addition, the repair duration\ntime of a single maintenance action for all of generating units Di is considered to be one day. It means that\nonce they decide to perform maintenance, it should be taken at least one day (Constraint A3 in (3.2)). We\n11\nAlgorithm 1 Safe RL algorithm for bidding and maintenance scheduling\n1: Input: γ, kmax\ni\n, Hi, Di, gmin\ni\n, gmax\ni\n, RU\ni , RD\ni , θi, θ′\ni, ηi, τi, θµ\ni , θ\n′µ\ni , ηµ\ni , τ µ\ni , M, Ns, ∀i ∈N\n2: Initialize the system safety state xf,i(1) = 0, ∀i ∈N\n3: for episode = 1, . . . , M do\n4:\nInitialize the system state s(t) using (4.1)\n5:\nT = {1 + (episode −1)T, · · · , (episode)T}\n6:\nfor t ∈T do\n7:\nObtain the action from the actor network ai(t) = max(min(µi(s(t), θµ\ni ), kmax\ni\n), 1),\n∀i ∈N\n8:\nObtain the maintenance scheduling which satisﬁes the safety ﬁlter uf,i(t) using (4.2)\n9:\nui(t) = uf,i(t),\n∀i ∈N\n10:\nObtain the market clearing price λ(t) and units’ power generation gi(t), ∀i ∈N , using (3.3)\n11:\nObserve the next state s(t + 1) = (λ(t), d(t))\n12:\nCalculate the reward function ri(t), ∀i ∈N , using (4.1)\n13:\nStore the transition (s(t), s(t + 1), ai(t), ri(t)), ∀i ∈N\n14:\nRandomly sample (sj(t), sj(t + 1), aj\ni(t), rj\ni (t)), ∀i ∈N, ∀j ∈Ns\n15:\nObtain the target network Qj\ni,target(t), ∀i ∈N, ∀j ∈Ns, using (4.1)\n16:\nUpdate the critic networks’ weights θi, ∀i ∈N , using (4.1)\n17:\nUpdate the actor networks’ weights θµ\ni , ∀i ∈N, using (4.1)\n18:\nUpdate the target critic networks’ weights θ′\ni, ∀i ∈N, using (4.1)\n19:\nUpdate the target actor networks’ weights θ\n′µ\ni , ∀i ∈N, using (4.1)\n20:\nend for\n21: end for\nTable 1. Generation units’ parameters\nGeneration unit\nλi,m\ngmax\ni\ngmin\ni\nci\n[\n$\nMWh]\n[MW]\n[MW]\n[$]\n1\n2\n80\n5\n120\n2\n1.75\n80\n5\n135\n3\n1\n50\n5\n142\n4\n3.25\n55\n5\n125\n5\n3\n30\n5\n175\n6\n3\n40\n5\n165\nassume that the maximum number of generating units that can perform maintenance simultaneously M is 2\n(Constraint S3 in (4.2)). This limitation makes sure that the load of the system can be satisﬁed at all periods\nof time. The other parameters of the generating units are shown in Table 1. The parameters of the network\nare obtained from [29].\nIn this case study to implement Algorithm 1, we consider 100 episodes in which each episode consists of\n30 days. We set two hidden layers for the actor-critic networks of each generating unit. In each layer, we use\nthe Rectiﬁed Linear Unit (ReLU ) activation function. The initial value for the weights of neural networks is\nchosen randomly between 1 and 3. The number of samples Ns = 100 is chosen for the DDPG algorithm. In\nthe following, we evaluate the performance of the safe RL Algorithm 1.\n5.1. Generation unit reward\nWe obtain the average reward over each episode for all units (¯ri(episode) =\n(episode)T\nP\n1+(episode−1)T\nri(t)\nT\n, ∀i ∈N,\nepisode = 1, · · · , M). Then We calculate the summation of this average reward over all units ( P\ni∈N\n¯ri(episode))\nand depict it for all episode (episode = 1, · · · , M) in Figure 3. As we can see in this ﬁgure, at the beginning,\nthe average reward is comparably low since the units are not aware of the market price and they are in the\n12\n20\n40\n60\n80\n100\nEpisode\n-2000\n-1000\n0\n1000\n2000\n3000\n4000\nAaverage profit [$]\nFigure 3. Sum of the average reward over each episode for all units using the learning\nalgorithm.\ntraining stage. This ﬁgure shows that the reward is increasing during training which means that the units\ncan increase their proﬁt by changing their bidding and maintenance strategies. The algorithm converges after\nabout 40 episodes.\nThe bidding strategies of units ki(t), ∀i ∈N, are shown in Figure 4. This ﬁgure shows that the bidding of\nunits converges after about 40 episodes of the algorithm and that the units learn how to change their bidding\nto improve their proﬁt.\n5.2. Maintenance scheduling\nThe maintenance scheduling of generation units between day 700 until 900 is shown in Figure 6 (we only\nshow a limited time interval to visualize the frequency of maintenance scheduling more clearly). This ﬁgure\nshows that at each time step, the maximum of two generating units can perform maintenance simultaneously.\nMoreover, each generation performs maintenance at least once during 100 days. This increases the system\nreliability and safety.\nThe average maintenance cost per unit for each episode and during the learning algorithm is demonstrated\nin Figure 6. According to this ﬁgure, the maintenance cost is decreasing during the learning algorithm which\nmeans that units learn how to ﬁnd the sub-optimal maintenance scheduling and do not perform maintenance\nfrequently.\n13\n0\n20\n40\n60\n80\n100\n2\n4\n6\n8\nG1\n0\n20\n40\n60\n80\n100\n2\n4\n6\n8\nG2\n0\n20\n40\n60\n80\n100\n2\n4\n6\n8\nBidding strategies \nG3\n0\n20\n40\n60\n80\n100\n2\n4\n6\n8\nG4\n0\n20\n40\n60\n80\n100\n2\n4\n6\n8\nG5\n0\n20\n40\n60\n80\n100\nEpisode\n2\n4\n6\n8\nG6\nFigure 4. Average biding of all units during each episode using the learning algorithm.\n5.3. Eﬀect of the safety ﬁlter\nAs mentioned in Section 4, the RL algorithm obtains the units’ strategies without considering the safety\nconstraints. The predicted safety ﬁlter is then responsible to satisfy the safety constraints. In the case that\nwe do not consider the safety ﬁlter, there is not any guarantee that the units perform maintenance before they\nfail. For-example, during the interval between iteration 700 and 900, the units do not perform maintenance\nwhich decreases the system safety. In addition, during training, when units are not aware of the other units’\nmaintenance decision, did not learn their best strategies, and explore diﬀerent strategies to learn the behaviour\nof system, using only RL without the safety ﬁlter might result in the case that most of the units may schedule\ntheir maintenance simultaneously. In this case, the demand of the system would not be able to be satisﬁed.\nFigure 7 shows the time interval where most of the units perform maintenance simultaneously and there is\nnot any guarantee for satisfying the load of the system.\n5.4. Performance comparison of diﬀerent RL algorithms\nIn this section, we compare the results of Algorithm 1 with the Q-learning algorithm. In Q-learning, the\nRL algorithm applies a look-up table to update its policy. To obtain the look-up table, the states and actions\n14\n700\n750\n800\n850\n900\n0\n1\nG1\n700\n750\n800\n850\n900\n0\n1\nG2\n700\n750\n800\n850\n900\n0\n1\nMaintenance decision\nG3\n700\n750\n800\n850\n900\n0\n1\nG4\n700\n750\n800\n850\n900\n0\n1\nG5\n700\n750\n800\n850\n900\nDay\n0\n1\nG6\nFigure 5. Maintenance scheduling of generation units during the learning algorithm be-\ntween day 700 until 900.\nof the system should be discretized. Hence, we consider discrete points for price and load as the state and\nbidding strategy as the action. Figure 8 shows the average proﬁt of the safe DDPG algorithm and the safe\nQ-learning algorithm. As we can see, the DDPG algorithm converges in fewer episodes than the Q-learning.\nMoreover, the DDPG algorithm can achieve a higher proﬁt than the Q-learning algorithm. These results can\nbe explained with the fact that in this problem, the state and the actions are continuous. Hence, using a\nlook-up table which approximates the continuous space with some discrete values decreases the performance\nand accuracy of the algorithm.\n6. Conclusions\nIn this paper, we address the problem of joint bidding and maintenance schedule in the electricity market\nenvironment. We propose the safe deep RL algorithm to solve the problem. In the ﬁrst step of this algorithm,\neach unit obtains its strategies using deep RL methodology without considering the safety constraints. Then\nin the second step, the safety ﬁlter modiﬁes the units’ decisions to ensure that all the system constraints\ncan be satisﬁed. The proposed algorithm can handle the challenges of uncertainty and safety constraints.\nThe results on the case study show that during the training, the proﬁt of the units is increasing, while the\ngeneration units also perform preventive maintenance. In addition, the results demonstrate that the proposed\nmethodology can achieve a higher proﬁt compared to the Q-learning algorithm.\n15\n20\n40\n60\n80\n100\n0\n2000\nG1\n20\n40\n60\n80\n100\n0\n2000\nG2\n20\n40\n60\n80\n100\n0\n2000\nMaintenance cost [$]\nG3\n20\n40\n60\n80\n100\n0\n2000\nG4\n20\n40\n60\n80\n100\n0\n2000\nG5\n20\n40\n60\n80\n100\nEpisode\n0\n2000\nG6\nFigure 6. Average maintenance cost during each episode for all units using Algorithm 1.\nAs future work, one can consider some other sources of uncertainties in the electricity market such as re-\nnewable energies. In addition, the proposed algorithm can be applied to a larger system with more generation\nunits by considering additional constraints such as contingency of the network.\nReferences\n[1] C. Wang, W. Wei, J. Wang, F. Liu, and S. Mei, “Strategic oﬀering and equilibrium in coupled gas and\nelectricity markets,” IEEE Transactions on Power Systems, vol. 33, no. 1, pp. 290–306, 2017.\n[2] Y. Du, F. Li, H. Zandi, and Y. Xue, “Approximating nash equilibrium in day-ahead electricity market\nbidding with multi-agent deep reinforcement learning,” Journal of Modern Power Systems and Clean\nEnergy, no. 99, pp. 1–11, 2021.\n[3] M. Kohansal, A. Sadeghi-Mobarakeh, S. D. Manshadi, and H. Mohsenian-Rad, “Strategic convergence\nbidding in nodal electricity markets: optimal bid selection and market implications,” IEEE Transactions\non Power Systems, vol. 36, no. 2, pp. 891–901, 2020.\n[4] G. Zhang, G. Zhang, Y. Gao, and J. Lu, “Competitive strategic bidding optimization in electricity\nmarkets using bilevel programming and swarm technique,” IEEE Transactions on Industrial Electronics,\nvol. 58, no. 6, pp. 2138–2146, 2010.\n[5] M. Yildirim, X. A. Sun, and N. Z. Gebraeel, “Sensor-driven condition-based generator maintenance sched-\nuling—part i: Maintenance problem,” IEEE Transactions on Power Systems, vol. 31, no. 6, pp. 4253–\n4262, 2016.\n16\n100\n150\n200\n250\n300\n0\n1\nG1\n100\n150\n200\n250\n300\n0\n1\nG2\n100\n150\n200\n250\n300\n0\n1\nMaintenance decision\nG3\n100\n150\n200\n250\n300\n0\n1\nG4\n100\n150\n200\n250\n300\n0\n1\nG5\n100\n150\n200\n250\n300\nIteration\n0\n1\nG6\nFigure 7. Maintenance scheduling of units without presence of safety ﬁlter during the day\n100 until 300.\n[6] L. Chen and J. Toyoda, “Optimal generating unit maintenance scheduling for multi-area system with\nnetwork constraints,” IEEE Transactions on Power Systems, vol. 6, no. 3, pp. 1168–1174, 1991.\n[7] A. Volkanovski, B. Mavko, T. Boˇsevski, A. ˇCauˇsevski, and M. ˇCepin, “Genetic algorithm optimisation\nof the maintenance scheduling of generating units in a power system,” Reliability Engineering & System\nSafety, vol. 93, no. 6, pp. 779–789, 2008.\n[8] H. Song, C.-C. Liu, and J. Lawarr´ee, “Nash equilibrium bidding strategies in a bilateral electricity\nmarket,” IEEE transactions on Power Systems, vol. 17, no. 1, pp. 73–79, 2002.\n[9] Y. Ye, D. Papadaskalopoulos, J. Kazempour, and G. Strbac, “Incorporating non-convex operating char-\nacteristics into bi-level optimization electricity market models,” IEEE Transactions on Power Systems,\nvol. 35, no. 1, pp. 163–176, 2019.\n[10] T. Dai and W. Qiao, “Finding equilibria in the pool-based electricity market with strategic wind power\nproducers and network constraints,” IEEE Transactions on Power Systems, vol. 32, no. 1, pp. 389–399,\n2016.\n[11] X.-P. Zhang, Restructured electric power systems: analysis of electricity markets with equilibrium models,\nvol. 71. John Wiley & Sons, 2010.\n[12] D. Pozo and J. Contreras, “Finding multiple nash equilibria in pool-based markets: A stochastic epec\napproach,” IEEE Transactions on Power Systems, vol. 26, no. 3, pp. 1744–1752, 2011.\n17\n0\n20\n40\n60\n80\n100\nEpisod\n-2000\n-1000\n0\n1000\n2000\n3000\n4000\nAaverage profit [$]\nSafe Q-learning algorithm\nSafe DDPG-algorithm\nFigure 8. Comparison of the average proﬁt over all units of safe Q-learning with safe DDPG\nalgorithm.\n[13] M. Marwali and S. Shahidehpour, “Long-term transmission and generation maintenance scheduling with\nnetwork, fuel and emission constraints,” IEEE Transactions on power systems, vol. 14, no. 3, pp. 1160–\n1165, 1999.\n[14] H. Pandzic, A. J. Conejo, I. Kuzle, and E. Caro, “Yearly maintenance scheduling of transmission lines\nwithin a market environment,” IEEE Transactions on Power Systems, vol. 27, no. 1, pp. 407–415, 2011.\n[15] C. Feng and X. Wang, “A competitive mechanism of unit maintenance scheduling in a deregulated\nenvironment,” IEEE transactions on power systems, vol. 25, no. 1, pp. 351–359, 2009.\n[16] C. Min, M. Kim, J. Park, and Y. Yoon, “Game-theory-based generation maintenance scheduling in\nelectricity markets,” Energy, vol. 55, pp. 310–318, 2013.\n[17] P. Rokhforoz, B. Gjorgiev, G. Sansavini, and O. Fink, “Multi-agent maintenance scheduling based on the\ncoordination between central operator and decentralized producers in an electricity market,” Reliability\nEngineering & System Safety, vol. 210, p. 107495, 2021.\n[18] A. J. Conejo, R. Garc´ıa-Bertrand, and M. D´ıaz-Salazar, “Generation maintenance scheduling in restruc-\ntured power systems,” IEEE Transactions on Power Systems, vol. 20, no. 2, pp. 984–992, 2005.\n[19] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.\n[20] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no. 3-4, pp. 279–292, 1992.\n[21] J. D. R. Mill´an, D. Posenato, and E. Dedieu, “Continuous-action q-learning,” Machine Learning, vol. 49,\nno. 2, pp. 247–265, 2002.\n18\n[22] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep reinforcement learning: A\nbrief survey,” IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 26–38, 2017.\n[23] H. Xu, H. Sun, D. Nikovski, S. Kitamura, K. Mori, and H. Hashimoto, “Deep reinforcement learning\nfor joint bidding and pricing of load serving entity,” IEEE Transactions on Smart Grid, vol. 10, no. 6,\npp. 6366–6375, 2019.\n[24] Y. Liang, C. Guo, Z. Ding, and H. Hua, “Agent-based modeling in electricity market using deep deter-\nministic policy gradient algorithm,” IEEE Transactions on Power Systems, vol. 35, no. 6, pp. 4180–4192,\n2020.\n[25] Y. Ye, D. Qiu, M. Sun, D. Papadaskalopoulos, and G. Strbac, “Deep reinforcement learning for strategic\nbidding in electricity markets,” IEEE Transactions on Smart Grid, vol. 11, no. 2, pp. 1343–1355, 2019.\n[26] K. P. Wabersich and M. N. Zeilinger, “A predictive safety ﬁlter for learning-based control of constrained\nnonlinear dynamical systems,” Automatica, vol. 129, p. 109597, 2021.\n[27] I. Menache, S. Mannor, and N. Shimkin, “Basis function adaptation in temporal diﬀerence reinforcement\nlearning,” Annals of Operations Research, vol. 134, no. 1, pp. 215–238, 2005.\n[28] A. Fortuny and B. McCarl, “A representation and economic interpretation of a two-level programming\nproblem,” Journal of the operational Research Society, vol. 32, no. 9, pp. 783–792, 1981.\n[29] E. Bompard, W. Lu, and R. Napoli, “Network constraint impacts on the competitive electricity markets\nunder supply-side strategic bidding,” IEEE Transactions on Power Systems, vol. 21, no. 1, pp. 160–170,\n2006.\n",
  "categories": [
    "eess.SY",
    "cs.AI",
    "cs.LG",
    "cs.SY"
  ],
  "published": "2021-12-20",
  "updated": "2021-12-20"
}