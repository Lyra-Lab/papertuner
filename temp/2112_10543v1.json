{
  "id": "http://arxiv.org/abs/2112.10543v1",
  "title": "Spiral Language Modeling",
  "authors": [
    "Yong Cao",
    "Yukun Feng",
    "Shaohui Kuang",
    "Gu Xu"
  ],
  "abstract": "In almost all text generation applications, word sequences are constructed in\na left-to-right (L2R) or right-to-left (R2L) manner, as natural language\nsentences are written either L2R or R2L. However, we find that the natural\nlanguage written order is not essential for text generation. In this paper, we\npropose Spiral Language Modeling (SLM), a general approach that enables one to\nconstruct natural language sentences beyond the L2R and R2L order. SLM allows\none to form natural language text by starting from an arbitrary token inside\nthe result text and expanding the rest tokens around the selected ones. It\nmakes the decoding order a new optimization objective besides the language\nmodel perplexity, which further improves the diversity and quality of the\ngenerated text. Furthermore, SLM makes it possible to manipulate the text\nconstruction process by selecting a proper starting token. SLM also introduces\ngeneration orderings as additional regularization to improve model robustness\nin low-resource scenarios. Experiments on 8 widely studied Neural Machine\nTranslation (NMT) tasks show that SLM is constantly effective with up to 4.7\nBLEU increase comparing to the conventional L2R decoding approach.",
  "text": "Spiral Language Modeling\nYong Cao, Yukun Feng, Shaohui Kuang, Gu Xu\nByteDance\n{yongc, yukunfeng, kuangshaohui, guxu}@bytedance.com\nAbstract\nIn almost all text generation applications, word sequences\nare constructed in a left-to-right (L2R) or right-to-left (R2L)\nmanner, as natural language sentences are written either L2R\nor R2L. However, we ﬁnd that the natural language written\norder is not essential for text generation. In this paper, we pro-\npose Spiral Language Modeling (SLM), a general approach\nthat enables one to construct natural language sentences be-\nyond the L2R and R2L order. SLM allows one to form natural\nlanguage text by starting from an arbitrary token inside the\nresult text and expanding the rest tokens around the selected\nones. It makes the decoding order a new optimization objec-\ntive besides the language model perplexity, which further im-\nproves the diversity and quality of the generated text. Fur-\nthermore, SLM makes it possible to manipulate the text con-\nstruction process by selecting a proper starting token. SLM\nalso introduces generation orderings as additional regulariza-\ntion to improve model robustness in low-resource scenarios.\nExperiments on 8 widely studied Neural Machine Transla-\ntion (NMT) tasks show that SLM is constantly effective with\nup to 4.7 BLEU increase comparing to the conventional L2R\ndecoding approach.\nIntroduction\nNatural language sentences are formed with tokens written\neither L2R or R2L. As a result, natural language sentences\nare represented by token sequences in their written order in\nmost NLP applications. In conventional language modeling,\nthe joint probability of a token sequence P(x) is usually fac-\ntorized along the time axis P(x) = Q\nt P(xt|x<t). The fac-\ntorization implies a default ordering of L2R. Accordingly,\nwhen applying the corresponding language model to text\ngeneration tasks, the generation ordering is naturally L2R.\nHowever, constructing a natural language sentence is not\nnecessarily an L2R approach in people’s minds. The con-\nstruction process is more like starting from a critical cogni-\ntive element and ﬁlling in auxiliary words around it until a\ncomplete sentence with correct grammar. Researchers have\nalready conducted empirical studies on the sequence order-\ning for density estimation, and auto-regressive pre-training\ntasks (Uria, Murray, and Larochelle 2014; Germain et al.\n2015; Uria et al. 2016; Yang et al. 2020).\nPermutation language modeling (Yang et al. 2020) is car-\nried out to model sequence orderings in auto-regressive pre-\ntraining. However, it may not be directly applied to text gen-\n.\nA\nlanguage\nis\na\nflash\nof\nthe\nhuman\nspirit\n[EOL]\n[EOR]\n\"+\"\n\"- \" left\nright\nflash\nflash\na\nflash\na\nis\nflash\na\nis\nof\n……\nflash\na\nis\nof\nlanguage\nthe\nA\nhuman\n[EOL]\n.\nspirit\n[EOR]\n-\n-\n+\n-\n+\n-\n+\n-\n+\n+\n+\n+\nflash\na\nis\nlanguage\nof\nFigure 1: An example of generation ordering in SLM\n.\neration. In-text generation, the result sequence length is un-\nknown until a special end token is generated. So a generation\nmodel may not foresee a token in the far end as we don’t\nknow whether the generation process will stop at the very\nnext position. Both L2R and R2L ordering holds the above\nproperty so that they are widely used in different text gen-\neration tasks. Permutation language modeling does not hold\nthe property as it allows to predict tokens in the far end.\nIn this paper, we propose a general approach called Spiral\nLanguage Modeling that allows one to construct text sen-\ntences with any valid generation ordering. Figure 1 shows\nan example of generation ordering. The generation process\nstarts from an arbitrary token inside the result sentence. The\nselected token forms an initial segment. The initial segment\nin the example is “ﬂash”. The rest generation process con-\nstructs around the segment “ﬂash” by arranging tokens on\nits right or its left following the arrows. The high-lighted to-\nkens in the middle of the ﬁgure are the appended ones at\neach step. “a” is appended to the left, then “is” is also ap-\npended to the left, while “of” is appended to the right, and\nso on so force. We introduce two special tokens, “[EOL]”\nand “[EOR]”, which stand for the left end and the right end,\nrespectively. The generation process stops when both end to-\nkens are generated. The sequence at the bottom of the ﬁgure\nis the actual generation ordering. Speciﬁcally, if one starts\nfrom “[EOL]” and constantly appends tokens to its right un-\ntil “[EOR]”, the generation ordering is exactly L2R. That is,\nL2R is just a special case of SLM generation ordering. Sim-\nilarly, we may get R2L ordering by starting from “[EOR]”.\nWhen we draw the construction arrows as shown in Figure\n1, these arrows form a set of spirals over the text. This is\nwhy we call our approach Spiral Language Modeling.\narXiv:2112.10543v1  [cs.CL]  20 Dec 2021\nWhen applying to text generation, SLM provides a family\nof valid generation orderings for one single text sentence.\nIn training, the SLM objective is the sequence probability\nexpectation over all generation orderings. During inference,\ndifferent generation ordering may lead to the different result\ntext. Therefore, SLM actually provides a family of decoding\norderings so that the text generation model may pick an opti-\nmal one from them. As SLM may start from any token inside\nthe result sentence, one can easily manipulate the decoding\nprocess to ensure the occurrence of any token or phrase. The\nonly thing one needs to do is to ﬁx the token or phrase as the\ninitial segment and let SLM infer the rest of the sentence.\nComparing to the conventional text generation approaches,\nSLM has the following distinctive advantages.\n• SLM provides an additional ordering dimension for the\nlanguage models to optimize.\n• SLM introduces generation orderings as additional regu-\nlarization to mitigate over-ﬁtting, as it prevents the lan-\nguage models from memorizing training data.\n• SLM allows one to easily manipulate the decoding pro-\ncess.\nWe conduct empirical studies on Machine Translation\n(MT) tasks where the target language sentence decoding\nis a typical text generation problem. Most of the conven-\ntional MT approaches follow the L2R decoding ordering.\nWe employ the same encoder and decoder network struc-\nture when comparing SLM to L2R ordering baselines. The\nempirical study shows that SLM is constantly effective on\n2 IWSLT’14 tasks and 6 IWSLT’16 tasks with an average\n2.6 BLEU increase. Speciﬁcally, SLM achieves signiﬁcant\nimprovement on IWSLT’16 CS→EN and EN→CS with 4.7\nand 4.2 BLEU increases, respectively. Here we summarize\nour contributions as follows.\n• We propose a novel language modeling approach, called\nSpiral Language Modeling.\n• SLM has distinct advantages for text generation, includ-\ning generation ordering optimization, training data regu-\nlarization, and decoding process manipulation.\n• We apply SLM to MT tasks and conduct detailed experi-\nments to show its effectiveness.\nRelated Work\nMost of the Neural Machine Translation (NMT) approaches\ndecode the target language sentences through an L2R order\n(Vaswani et al. 2017; Zhang et al. 2018; Zhou, Zhang, and\nZong 2019; Wang et al. 2020). Researchers are also inter-\nested in the research topics on sequence ordering and text\ndecoding manipulation.\nSequence Ordering Sequence ordering is studied in\nthe density estimation tasks ever since Uria, Murray, and\nLarochelle (2014) propose to optimize the mean auto-\nregressive cost over all orderings of a given variable se-\nquence. Germain et al. (2015) further implement their order-\nagnostic training by permuting the input variable sequence\nwith observed variables all before the masked ones. Uria\net al. (2016) treat the orderings of the sequence variables\nas a stochastic variable with a uniform distribution and opti-\nmize the expected likelihood over the ordering of variables.\nIntroducing ordering generally improves the density estima-\ntion effectiveness. Recently, permutation language modeling\nis borrowed in the large-scale unsupervised auto-regressive\nmodel pre-training task XLNet in Yang et al. (2020). The\nXLNet design a partial prediction task where normally the\nlast 1/6 of the permuted sequence is masked for reconstruc-\ntion. To the best of our knowledge, there is no existing work\ndiscussing sequence ordering in text generation tasks. Bao\net al. (2020) also employed a pseudo-mask method to sup-\nport different factorization orders in the Partially AutoRe-\ngressive model, in which the randomly sampled factoriza-\ntion orders are similar to permutation-based language mod-\neling used by XLNet.\nDecoding Manipulation Decoding text during gener-\nation naturally follows the text written order. However,\nemploying additional decoding order generally introduces\ntext generation diversity which leads to better performance.\nFinch and Sumita (2009) use an R2L trained language model\ntogether with an L2R trained one and achieve BLEU gains\nconstantly on all MT tasks. Zhang et al. (2013) expand the\nbidirectional decoding idea to a mixture of the L2R and\nR2L ordering with a directed graphical model factorization.\nNMT approaches such as (Liu et al. 2016; Sennrich, Had-\ndow, and Birch 2016; Hoang, Haffari, and Cohn 2017; Sen-\nnrich et al. 2017) run beam search for L2R and R2L models\nindependently to get two n-best lists and re-score the union\nof the two lists to obtain the best one. They all train separate\nforward and reverse decoders and require multiple rounds\nof decoding during inference. Comparing to the above bi-\ndirectional decoding approaches, SLM is much neater as it\nexplores all possible decoding directions that could maxi-\nmize the result sequence scores in a single round of beam\nsearch.\nBesides re-scoring, researchers are also trying to ﬁnd\na uniﬁed bi-directional searching approach. Zhang et al.\n(2018) employ the reverse decoder as an auxiliary module\nfor the forward decoder to foresee a part of the target text.\nZhou, Zhang, and Zong (2019) propose a synchronous bidi-\nrectional neural machine translation (SB-NMT) approach\nthat generates an L2R sequence and an R2L sequence si-\nmultaneously and interactively. Zhou et al. (2019) carry out\na decoding strategy that starts from both ends of a target\nsequence and generates two subsequences separately. More-\nover, Yang et al. (2021) proposes a reversed strategy of the\nabove one, which predicts a starting token ﬁrst and gener-\nates the two subsequences on both sides separately. All the\nabove approaches simply try to ﬁnd a better way to com-\nbine an L2R and an R2L language model. Besides MT tasks,\n(Mou et al. 2015, 2016; Li and Sun 2018; Liu et al. 2019)\nalso employ bi-directional language models with well cho-\nsen decoding strategy in dialog generation tasks. Our pro-\nposed SLM breaks the constraints that language modeling\nmay only follow the L2R or R2L order. Therefore, SLM\nprovides a wide number of decoding strategies that the re-\nsult decoding model can choose from.\nApproach\nSLM is a general approach for text generation. In this sec-\ntion, we ﬁrst present the deﬁnition of SLM and the under-\nlying theory. We further introduce the SLM applications for\nthe Neural Machine Translation (NMT) tasks.\nSpiral Language Modeling\nSLM is carried out speciﬁcally for text generation scenarios.\nWe borrow the permutation language modeling from Yang\net al. (2020) and change it to ﬁt text generation applications.\nLanguage Modeling\nConsider a sequence of random vari-\nables, X1, X2, ..., XT , each of which can take any value in\na ﬁnite set V. The Language Modeling goal is to estimate\nthe probability of any sequence x = [x1, x2, ..., xT ], where\nT ≥1 and xt ∈V for t = 1...T, that is, to estimate the joint\nprobability\nP(x = x1, x2, ..., xT )\n(1)\nThe above joint probability can be factorized along the time\naxis according to the law of total probability\nP(x)& =\nY\nt\nP(xt|x<t)\n(2)\nLanguage models introduce a set of parameters θ to approx-\nimate the conditional probability p(xt|x<t, θ) by maximiz-\ning p(x) = Q\nt p(xt|x<t, θ).\nEquation 2 implies a default language construction or-\nder which is from left to right. Let z denote the ordering\nvector which depicts the construction order of a given lan-\nguage sentence. The default ordering vector in Equation 2\nis z = [1, 2, ..., T]. The factorization of the joint probability\nP(x) explicitly depends on the ordering z as follows.\nP(x) =\nY\nt\nP(xzt|xz<t)\n(3)\nThe ordering z ranges from a full set of the permutation or-\ndering ZT . However, not all ordering is suitable for text gen-\neration.\nGeneration Ordering\nWhen applying language models\nfor text generation, the sequence length T is unknown. In the\nL2R and R2L generation approaches, the sequence length\nis predicted by introducing an explicit end token, such as\n“[EOS]”. The generation ordering z should hold the property\nthat the selection of zt given z<t must be valid for any possi-\nble length T. For example, the ordering z<t = [1, 2, 3], zt =\n5 will result in an invalid factorization when the sequence\nlength is actually 4. That is, we cannot foresee a token in\nthe far end during sequence generation as we don’t know\nwhether the generation process will stop at the next posi-\ntion. Both L2R and R2L ordering holds the above property.\nThe text decoding alternatives mentioned in the related work\nsection all follow the orderings that hold the above property.\nSLM introduces a set of permutations that hold the above\nproperty. Speciﬁcally, a sequence is re-ordered by ﬁrstly\npicking an arbitrary token or a sequence of continue tokens\ninside the original one and keeps expanding the existing seg-\nment by appending the next left or right token until the seg-\nment meets the end tokens for both the left and the right\ndirection. We introduce two special tokens, “[EOL]” and\n“[EOR]”, representing the end-of-left and the end-of-right,\nrespectively, when both tokens are decoded in xz<t, the gen-\neration process stops. Figure 1 shows an example of how a\ntext sequence is generated in a valid generation ordering in\nSLM. In fact, L2R and R2L orderings are two special cases\nof SLM. L2R starts from the very left token and keeps ap-\npending the next right token while R2L does in a reversed\nmanner. Furthermore, the alternatives, which combine L2R\nand R2L language models in different ways, are all special\ncases of SLM.\nObjective\nInstead of maximizing a single joint probabil-\nity, the objective of SLM is to maximize the expectation over\nall generation orderings.\nmax\nθ\nEz∈Zg\n\"Y\nt\npθ(xzt|xz<t)\n#\n(4)\nwhere Zg is the set of all generation orderings. For a given\ntext sentence x with T tokens, an estimated upper bound of\n|Zg| is T · 2T −2. Therefore, it’s intractable to go through Zg\nto estimate equation (4). A straightforward approach is to\napply a random sampling strategy.\nA certain generation ordering actually deﬁnes a decoding\npolicy which leads to an individual joint probability factor-\nization over the learned parameters. During inference, the\nordering z and the next token xzt are evaluated together with\na beam search approach to maximize pθ(x). A proper decod-\ning policy corresponding to the ordering z will result in a\nbetter perplexity of the result sequence. Therefore, we could\nemploy proper sampling strategies during training to rein-\nforce the selection of generation order. We apply a two-stage\nsampling strategy in the NMT applications. We ﬁrst apply a\nrandom sampling strategy during the ﬁrst half of the training\nsteps. We then sample the generation orderings that result in\nhigher perplexities for the rest training steps. The sampling\nstrategies are further discussed in the empirical study ses-\nsion.\nMachine Translation\nNMT has attracted much attention ever since the seq-to-\nseq architecture (Sutskever, Vinyals, and Le 2014) is carried\nout. Most of the popular NMT solutions employ encoder-\ndecoder network architectures with different variations. The\ntranslation objective is the target language sentence proba-\nbility conditioned on the source language sentence.\nmax\nθ\nY\nt\npθ(xt|x<t, s)\n(5)\nwhere s is the source language sentence. Machine transla-\ntion with encoder-decoder architecture is a special text gen-\neration scenario where we can naturally apply SLM. By in-\ntroducing the generation ordering, equation 5 is changed to\nmax\nθ\nEz∈Zg\n\"Y\nt\npθ(xzt|xz<t, s)\n#\n(6)\nwhich is to maximize the expectation over all generation or-\nderings of the target sentence given the source sentence.\nEncoder\nDecoder\ns0\ns1\ns2\n…\n…\nx0\nx1\nx2\n…\n…\nx1\nx2\n…\n…\n…\nh0\nh1\nh2\n…\n…\nEncoder\nDecoder\ns0\ns1\ns2\n…\n…\nxt\nxt+1\n…\nxt+1\nxt-1\n…\n…\n…\nh0\nh1\nh2\n…\n…\n+\n-\n..\n…\n..\n…\n..\nStarting Token Prediction\n(a)\n(b)\nFigure 2: (a) Conventional encoder-decoder network, (b) SLM encoder-decoder network\n.\nSLM Architecture\nAlmost all decoder networks are im-\nplemented to follow a L2R decoding procedure. RNN-based\ndecoders recursively append RNN cells to the right of the\nexisting RNN chain to generate the next token, while the\ntransformer-based (Vaswani et al. 2017) decoders employ\na special attention mask to prevent positions from attend-\ning to subsequent positions on their right. Manipulating the\ntext generation order is done by changing the sequences that\nfed into the decoder networks. For example, feeding in a\nreversed target language text will result in an R2L text gen-\neration order.\nWe also want to implement SLM without changing the\nexisting decoding procedures. Given a text sentence x =\n[x1, x2, ..., xT ] and a speciﬁc generation ordering z\n=\n[t, t + 1, t −1, ...], we may form a unique sequence ˆx =\n[⟨xt, +⟩, ⟨xt+1, −⟩, ⟨xt−1, +⟩...] which encodes both tokens\nand their ordering. Each element of the sequence is a tuple\nof the current token and the next construction direction. “+”\nrefers to append the next token to the right while “-” refers\nto append to the left. Decoding the original text sequence x\nwith a given ordering z is then equivalent to decoding the\nreformed ˆx from left to right.\nAlthough each element of ˆx is a tuple, we only need to\npredict the token part and treat the direction part simply as\ninput. Firstly, there is no ground truth for the direction part as\nthere are no preferred generation orderings. Secondly, the di-\nrection part is only an auxiliary input to determine which to-\nken (on the left or the right) to predict. For instance, the sub\nsequence [⟨xt, −⟩, ⟨xt−1, −⟩] deﬁnes a segment [xt−1, xt].\nThe last tuple of the sequence holds a “-” direction. It means\nthat the next decoded token will be appended to the left\nof the current sequence and the next ground-truth token is\nxt−2. If we change the sub sequence to [⟨xt, −⟩, ⟨xt−1, +⟩],\nthe segment that it deﬁnes is still [xt−1, xt]. However, the\nnext decoded token will be appended to the right of the se-\nquence, and the corresponding ground-truth token is xt+1\nthen. To incorporate the direction information as part of the\ninput of the model, We introduce the direction embedding\nEdirt. Then the distributed representation of a tuple in ˆx is\na summation of the token embedding Ext and the direction\nembedding Edirt.\nE ˆxt = Ext + Edirt\n(7)\nStarting Token Prediction\nFigure 2-b shows an illustra-\ntion of the SLM architecture. Besides the modiﬁcation of\nthe decoder input, there is an additional structure appended\nto the encoder. Unlike the ﬁxed L2R generation ordering,\nwhere the starting token is always the very left one, SLM\nmay start decoding from any token in the result sequence.\nWe need an independent network to estimate the very ﬁrst\nfactor pθ(xz1|s).\n𝛴\nEncoder\ns0\ns1\ns2\n…\n…\nh0\nh1\nh2\n…\n…\nFull Connected\n𝑣!\n\"\n…\n𝛼!\n\"\n…\nsoftmax\n𝑇\n𝒱\nSigmoid\n1\n0\n1\n0\n0\n…\n…\n𝑣&\n\"\n𝛼&\n\"\nFigure 3: Auto-alignment network\n.\nEstimating pθ(xz1|s) is actually to determine which to-\nkens will show up in the target language sentence given the\nsource language sentence. A straightforward solution is to\nbuild |V| binary classiﬁers for each is to estimate the possi-\nbility of the corresponding token occurring in the target sen-\ntence. As the tokens inside the target sentence usually have a\nstrong translation correlation to the corresponding tokens in\nthe source sentence, we carry out an auto-alignment network\nto enhance the prediction performance. Figure 3 shows the\ndetails of the starting token prediction network.\nWe put full connected networks right after the encoder\noutput to compute the potentials over the target language vo-\ncabulary V.\nv = W(H×|V|)(W(H×H)h + b(H))\n(8)\nwhere h is the encoder output and v is the calculated poten-\nModel\nDE-EN’14\nEN-DE’14\nCS-EN’16\nEN-CS‘16\nFR-EN’16\nEN-FR’16\nDE-EN‘16\nEN-DE’16\nL2R Baseline\n34.56\n28.34\n23.58\n16.54\n37.29\n38.12\n31.58\n26.24\nSLM+Random\n35.23\n29.24\n28.08\n20.68\n37.91\n40.12\n32.61\n28.79\nSLM+Sampling\n36.18\n29.52\n28.26\n20.73\n39.14\n40.33\n33.87\n28.83\n(+1.62)\n(+1.18)\n(+4.68)\n(+4.19)\n(+1.85)\n(+2.21)\n(+2.29)\n(+2.59)\nTable 1: SLM results comparing to L2R baselines\ntial. The ﬁnal potential of each vocabulary token ev(k) is a\nweighted sum of v along the time axis.\nα(k)\nt\n=\nexp v(k)\nt\nP\nt exp v(k)\nt\n(9)\nev(k) =\nX\nt\nα(k)\nt\nv(k)\nt\n(10)\nwhere v(k)\nt\nis the potential of the k-th token at time t, α(k)\nt\nis the normalized weight calculated from a softmax layer.\nev(k) is further fed to a Sigmoid layer where a binary clas-\nsiﬁcation cross entropy loss is applied afterward. If the k-th\nvocabulary token occurs in the target sentence, it receives a\nlabel “1” and “0” otherwise. The ultimate loss function is\nthe summation of the starting token prediction loss and the\ntranslation loss.\nInference\nWe employ beam search during the translation\ninference. The probability of the initial token pθ(xzt|s) is\nestimated with the starting token prediction network shown\nin Figure 3. One needs to try both directions when decoding\nthe rest tokens. Let’s take the sentence in Figure 1 as an ex-\nample. Assuming that the beam size is 2, we now have top-2\ncandidates [“ﬂash”, “language”] after the ﬁrst step of infer-\nence. We need to try both left and right searching directions\nby appending direction input manually. So we get 4 tuples,\n[⟨flash, +⟩, ⟨flash, −⟩, ⟨language, +⟩, ⟨language, −⟩],\nas input for the next round of beam searching.\nExperiments\nWe evaluate the effectiveness of SLM on the well-studied\nNMT tasks where the generation of the target language text\nis a typical text generation sub-task. Comparing SLM to the\nconventional L2R decoding ordering, we ﬁnd that SLM is\nconsistently effective.\nDataset\nLanguage\nTrain\nValid\nTest\nIWSLT’14\nEN↔DE\n170\n7\n7\nIWSLT’16\nEN↔CS\n114\n2\n2\nIWSLT’16\nEN↔DE\n196\n2\n2\nIWSLT’16\nEN↔FR\n220\n2\n2\nTable 2: NMT datasets, numbers in thousand(k)\nDatasets and Settings\nThe NMT datasets are listed in table 2, including\nIWSLT’14 English↔German (EN↔DE) and IWSLT’16\nEnglish↔{Czech, French, German} (EN↔{CS, FR, DE}).\nAll data are pre-processed with the open-source tool Fairseq.\nWe employ Byte-Pair-Encoding (BPE) as the tokenization\nmethod with a vocab size of 10k for all datasets.\nIn the empirical studies, we focus on the comparison be-\ntween SLM and the conventional L2R decoding baselines.\nWe choose the widely used Transformer (Vaswani et al.\n2017) model with the transformer-base conﬁgurations for\nboth SLM and L2R baselines. We adopt the layer normaliza-\ntion and positional embedding as illustrated in Raffel et al.\n(2020), in which the layer normalization in each block is\nset out of the residual path with the bias removed. In train-\ning, we use Adam optimizer with an initial learning rate of\n9×10−5. The ﬁrst 1000 update steps are warm-up steps. The\ndrop-out rate is set to 0.3. During inference, we use a beam\nsize of 5. We introduce length penalty(Wu et al. 2016) when\ncalculating the scores of each beam search candidate. We\ncalculate the ﬁnal BLEU score in a max order of 4 after re-\nmoving BPE-tokens. With the above settings, we reproduce\nvery similar results on IWSLT’14 EN↔DE as reported in\n(Pham and Le 2021; Wu et al. 2020). The above settings are\napplied to both L2R baselines and SLM models, only that\nSLM introduces an additional starting token prediction part\nto the encoder as depicted in the last section.\nResult Analysis\nTable 1 shows the full experiment results on 8 transla-\ntion tasks. L2R baselines and SLM reported results are\nadopted until full convergence or maximum update steps of\n800k. In the “SLM+Random” row, a uniform random sam-\npling strategy is employed during the training stage. In the\n“SLM+Sampling” row, we apply a two-stage sampling strat-\negy. During the ﬁrst 90% of the training process, we still em-\nploy the random sampling strategy, while in the rest 10% up-\ndate steps, we search for the best generation orderings based\non the learned model. The detailed sampling strategy is dis-\ncussed in the rest of the section.\nAs shown in the table, SLM signiﬁcantly improves the\nNMT performance on all tasks. We achieve an average in-\ncrease of 2.6 BLEU scores. Speciﬁcally, on the IWSLT’16\nEN↔CS task, which has the smallest training dataset with\nonly 114K sentence pairs, SLM outperforms the transformer\nbaseline model with a 4.2 BLEU score. We further study the\nproperties of SLM from a different perspective in the rest of\nthe section.\nConvergence\nAs we try to maximize the expectation of\nthe joint probability over all valid generation orderings in\nSLM, the objective is more complex than the conventional\nL2R decoder objective. For one single sentence pair, the tar-\nget ground-truth text ordering is sampled on demand during\nFigure 4: Attention maps of start token prediction\n.\ntraining. Therefore, the SLM model gets a different target\nground truth every time it meets the same sentence pair. As\na result, the SLM model converges slightly slower than the\nbaseline L2R transformer model in the early training stage.\nWe compare the convergence of the baseline L2R trans-\nformer model and the SLM over 2 million update steps.\nThe experiments are conducted on the IWSLT’14 DE→EN,\nIWSLT’16 (DE, FR)→EN datasets, respectively. The three\nconvergence curves are shown in Figure 5. The baseline L2R\nmodels converge a little bit faster at the early training stage\nand stop growing at around 800k steps. However, the SLM\nmodels keep growing better after outperforming the L2R\nbaselines after around 400k steps. The BLEU scores on the\ndevelopment set are still growing even when the update steps\nmove close to 2 million.\nFigure 5: Convergence curves\n.\nLow-Resource Scenario\nWe notice that SLM achieves\nover 4 BLEU increases on the IWSLT’16 EN↔CS task,\nwith the smallest training set with only 114K sentence pairs.\nWe further testify to the assumption that SLM may work\nbetter in low-resource scenarios. We evaluate the conven-\ntional L2R model and SLM on IWSLT’14 DE→EN and\nIWSLT’16 FR→EN, with 1/4, 1/2, 3/4 of training data, re-\nspectively. The results are shown in Figure 6. The perfor-\nmance gaps between the SLM models and the L2R base-\nlines go bigger as there are less training data. Speciﬁcally,\nthe SLM model outperforms its L2R baseline on IWSLT’14\nDE→EN with an increase of close to 8 BLEU scores with\nonly 1/4 of the full dataset (about 42K sentence pairs). Com-\nparing to the full set, SLM only has a 7 BLEU score de-\ncrease while the L2R baseline losses 15 BLEU scores with\n1/4 training data. We get similar observations on IWSLT’16\nFR→EN. SLM introduces randomness to the training data\nby employing different generation orderings. SLM prevents\nthe model from memorizing the training data and mitigates\nover-ﬁtting problems, especially in low-resource scenarios.\n10\n15\n20\n25\n30\n35\n40\n25%\n50%\n75%\n100%\nIWSLT’14 DE→EN\nL2R\nSLM\n10\n15\n20\n25\n30\n35\n40\n25%\n50%\n75%\n100%\nIWSLT’16 FR→EN\nL2R\nSLM\nFigure 6: Results on different training data portions\n.\nStarting Token Prediction\nAs SLM can generate result\ntext starting from any token inside it, the model needs to de-\ncide which starting token may lead to a better result. We em-\nploy an auto-alignment network structure as shown in Fig-\nure 3 to predict starting tokens. We directly predict which\ntokens will occur in the target language text given a source\nlanguage sentence. There is a tricky problem when predict-\ning which tokens will occur in the target sentence. The stop\nwords usually have larger term frequencies which will result\nin a higher occurrence probability. However, starting from a\nstop word does not provide much useful information. So we\nexclude stop words when constructing ground-truth for the\nstarting token prediction networks. We ﬁlter stop words with\nthe NLTK library. The Sigmoid output value at each vocab-\nulary index is an approximation of pθ(xz1|s), the probability\nof the corresponding token occurring in the target sentence.\nThe estimated probability is further used in the downstream\ndecoding beam search as part of the joint probability factor-\nization.\nStarting token prediction is a multi-task learning ap-\nproach. The auto-alignment network is expected to gener-\nate a proper source sentence representation for reasoning\nwhether a speciﬁc target token may be involved in the trans-\nlated sentence. The attention mechanism depicted in Equa-\ntion 9 and 10 is designed for this purpose. The source sen-\ntence representation for different target tokens is different.\nFigure 4 shows an example of the attention map. We can\nDE→EN Example\nSource\nwir werden f¨ahig sein, unsere ideen direkt auf die digitalen medien abzuladen.\nGround-truth\nwe‘re going to be able to dump our ideas directly to digital media.\nL2R baseline\nwe’re going to be able to download our ideas directly to digital media.\nSLM\nwe’ll be able to transfer our ideas directly to the digital media.\ngeneration order\ndigital/+ media/- the/- to/- directly/- ideas/+ ./+ [EOR]/- our/- transfer/- to/- able/- be/- ’ll/- we/- [EOL]\nSLM with ﬁxed start\nwe’ll be able to dump our ideas directly to digital media.\ngeneration order\ndump/- to/+ our/- able/- be/+ ideas/+ directly/+ to/- ’ll/- we/+ digital/+ media/+ ./+ [EOR]/- [EOL]\nTable 3: Translation Examples from IWSLT’16 DE→EN (the bold font tokens are the starting tokens)\nsee that the proposed attention mechanism can automatically\nalign the translation token pairs among the translation sen-\ntence pairs. For example, the English token “complexity” in\nthe left attention map has a very big attention score at the\nGerman token “komplexit¨at”.\nDecoding Manipulation\nSLM allows the decoder to start\nfrom any token or any set of continuous tokens. The decod-\ning process can be manipulated by selecting a proper starting\ntoken manually during the inference stage. This feature al-\nlows one to preserve any token that s/he would like to put in\nthe result sentence. We put a DE→EN example in Table 3\nto show how we can manipulate the translation results. The\nSLM model selects “digital” as the starting token after eval-\nuating with a beam search strategy. Suppose we’ve already\nhad a candidate word, for example, ”dump” to represent how\nthe idea transferred to the media. In that case, we can manu-\nally start from ”dump” and get a result sentence much closer\nto the ground-truth sentence with the SLM model.\nSampling Strategy\nSLM requires a sampling strategy\nduring the training stage so that we may approximate the\njoint probability expectation. A straightforward strategy is\nto sample generation orderings randomly. However, each\ngeneration ordering deﬁnes a decoding policy, leading to\nan individual sequence joint probability factorization over\nthe learned parameters. Good decoding policies result in the\nbetter perplexity of the result sequence compared to the fair\nones. In the training stage, we generally want a sampling\nstrategy to reinforce the selection of the better generation\nordering that corresponds to better decoding policies.\nStrategy\nSamples\nBLEU\nL2R\n1\n34.56\nRandom\n1\n35.23\nSampling by ﬁx top-1 starting token\n1\n35.89\nSampling by ﬁx top-3 starting token\n3\n36.18\nTable 4: Sampling strategies comparison\nWe apply a two-stage sampling strategy. In the ﬁrst 90%\nof the training process, we still employ the random sampling\nstrategy. In the rest 10% of the training steps, we ﬁx the top-k\nranked start tokens returned by the starting token prediction\nnetworks. For each top-k token, we randomly select the rest\norderings to speed up the sampling process. Table 4 shows\nthe comparison among different strategies on the IWSLT’14\nDE→EN task. The best BLEU score is achieved by sam-\npling over top-3 starting tokens. Keeping more top-k to-\nkens does not improve the ﬁnal performance anymore. When\nkeep top-3 starting tokens, one sentence pair is replicated\n3 times with different decoding ordering. The rest training\nsteps should at least go through all training data once.\nWe also summarize the reported start-of-the-art (SOTA)\nresults on the IWSLT’14 DE→EN task in table 5. We em-\nploy the two-stage sampling strategy to get the reported\nSLM results. The SLM result in the last row is very much\nclose to the result reported by Wu et al. (2020). As the main\npurpose of this paper is to discuss how orderings affect text\ngeneration effectiveness in SLM, we do not put effort into\nintegrating existing independent approaches to outperform\nthe SOTA. The below table provides a reference of how\nmuch SLM advances the L2R baseline on the IWSLT’14\nDE→EN task.\nModel\nDE→EN\nTransformer Base(Vaswani et al. 2017)\n34.56\nAdversarial training(Wang, Gong, and Liu 2019)\n35.2\nMixed Representations(Wu et al. 2020)\n36.4\nAutoDropout(Pham and Le 2021)\n35.8\nSmart-Start Decoding(Yang et al. 2021)\n35.61\nSLM+Sampling 800k-steps\n36.18\nSLM+Sampling 2 million-steps\n36.31\nTable 5: Comparison to SOTA on IWSLT’14 DE→EN.\nConclusion\nIn this paper, we present a general approach called SLM that\nenables one to generate natural language sentences beyond\nthe L2R and R2L orderings. The generation ordering em-\nployed by SLM constructs natural language text by start-\ning from an arbitrary token inside the result text and then\nappending tokens around the constructed segment. By in-\ntroducing generation ordering, SLM includes a new opti-\nmization objective besides the language model perplexity,\nwhich further improves the diversity and quality of the gen-\nerated text. As SLM samples the generation order during the\ntraining stage, SLM introduces additional regularization to\nimprove model robustness. The experiments also show that\nSLM can work well under extreme low resource scenarios\nin the NMT tasks. Furthermore, one may easily manipulate\nthe text construction process by manually choosing the ini-\ntial decoding order. We conduct experiments on 8 widely\nstudied NMT tasks. The results show that SLM is constantly\neffective comparing to the conventional L2R decoding ap-\nproach.\nReferences\nBao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu,\nX.; Wang, Y.; Gao, J.; Piao, S.; Zhou, M.; et al. 2020.\nUnilmv2: Pseudo-masked language models for uniﬁed lan-\nguage model pre-training. In International Conference on\nMachine Learning, 642–652. PMLR.\nFinch, A.; and Sumita, E. 2009. Bidirectional phrase-based\nstatistical machine translation. In Proceedings of the 2009\nConference on Empirical Methods in Natural Language\nProcessing, 1124–1132.\nGermain, M.; Gregor, K.; Murray, I.; and Larochelle, H.\n2015. Made: Masked autoencoder for distribution estima-\ntion.\nIn International Conference on Machine Learning,\n881–889. PMLR.\nHoang, C. D. V.; Haffari, G.; and Cohn, T. 2017. Towards\nDecoding as Continuous Optimisation in Neural Machine\nTranslation. In Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing, 146–156.\nLi, J.; and Sun, X. 2018.\nA Syntactically Constrained\nBidirectional-Asynchronous Approach for Emotional Con-\nversation Generation. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 678–683.\nLiu, D.; Fu, J.; Qu, Q.; and Lv, J. 2019. BFGAN: back-\nward and forward generative adversarial networks for lexi-\ncally constrained sentence generation. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing, 27(12):\n2350–2361.\nLiu, L.; Utiyama, M.; Finch, A.; and Sumita, E. 2016.\nAgreement on Target-bidirectional Neural Machine Trans-\nlation. In Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 411–416.\nMou, L.; Song, Y.; Yan, R.; Li, G.; Zhang, L.; and Jin,\nZ. 2016. Sequence to Backward and Forward Sequences:\nA Content-Introducing Approach to Generative Short-Text\nConversation. In Proceedings of COLING 2016, the 26th In-\nternational Conference on Computational Linguistics: Tech-\nnical Papers, 3349–3358.\nMou, L.; Yan, R.; Li, G.; Zhang, L.; and Jin, Z. 2015. Back-\nward and forward language modeling for constrained sen-\ntence generation. arXiv preprint arXiv:1512.06612.\nPham, H.; and Le, Q. V. 2021.\nAutodropout: Learning\ndropout patterns to regularize deep networks. arXiv preprint\narXiv:2101.01761, 1(2): 3.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. arXiv:1910.10683.\nSennrich, R.; Birch, A.; Currey, A.; Germann, U.; Haddow,\nB.; Heaﬁeld, K.; Miceli-Barone, A. V.; and Williams, P.\n2017. The University of Edinburgh’s Neural MT Systems\nfor WMT17. In Proceedings of the Second Conference on\nMachine Translation, 389–399.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Edinburgh\nNeural Machine Translation Systems for WMT 16. In Pro-\nceedings of the First Conference on Machine Translation:\nVolume 2, Shared Task Papers, 371–376.\nSutskever, I.; Vinyals, O.; and Le, Q. V. 2014. Sequence\nto sequence learning with neural networks. In Advances in\nneural information processing systems, 3104–3112.\nUria, B.; Cˆot´e, M.-A.; Gregor, K.; Murray, I.; and\nLarochelle, H. 2016. Neural autoregressive distribution esti-\nmation. The Journal of Machine Learning Research, 17(1):\n7184–7220.\nUria, B.; Murray, I.; and Larochelle, H. 2014. A deep and\ntractable density estimator. In International Conference on\nMachine Learning, 467–475. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, B.; Zhao, D.; Lioma, C.; Li, Q.; Zhang, P.; and Si-\nmonsen, J. G. 2020. Encoding word order in complex em-\nbeddings. In International Conference on Learning Repre-\nsentations.\nWang, D.; Gong, C.; and Liu, Q. 2019. Improving neural\nlanguage modeling via adversarial training. In International\nConference on Machine Learning, 6555–6565. PMLR.\nWu, L.; Xie, S.; Xia, Y.; Fan, Y.; Lai, J.-H.; Qin, T.; and Liu,\nT. 2020. Sequence generation with mixed representations.\nIn International Conference on Machine Learning, 10388–\n10398. PMLR.\nWu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.;\net al. 2016.\nGoogle’s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144.\nYang, J.; Ma, S.; Zhang, D.; Wan, J.; Li, Z.; and Zhou, M.\n2021. Smart-Start Decoding for Neural Machine Transla-\ntion. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 3982–3988.\nYang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,\nR.; and Le, Q. V. 2020. XLNet: Generalized Autoregressive\nPretraining for Language Understanding.\nZhang, H.; Toutanova, K.; Quirk, C.; and Gao, J. 2013. Be-\nyond Left-to-right: Multiple Decomposition Structures for\nSMT. In Proceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 12–21.\nZhang, X.; Su, J.; Qin, Y.; Liu, Y.; Ji, R.; and Wang, H.\n2018. Asynchronous bidirectional decoding for neural ma-\nchine translation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 32.\nZhou, L.; Zhang, J.; and Zong, C. 2019. Synchronous Bidi-\nrectional Neural Machine Translation. Transactions of the\nAssociation for Computational Linguistics, 7: 91–105.\nZhou, L.; Zhang, J.; Zong, C.; and Yu, H. 2019. Sequence\nGeneration: From Both Sides to the Middle. 5471—-5477.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2021-12-20",
  "updated": "2021-12-20"
}