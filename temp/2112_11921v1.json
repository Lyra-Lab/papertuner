{
  "id": "http://arxiv.org/abs/2112.11921v1",
  "title": "Variational Quantum Soft Actor-Critic",
  "authors": [
    "Qingfeng Lan"
  ],
  "abstract": "Quantum computing has a superior advantage in tackling specific problems,\nsuch as integer factorization and Simon's problem. For more general tasks in\nmachine learning, by applying variational quantum circuits, more and more\nquantum algorithms have been proposed recently, especially in supervised\nlearning and unsupervised learning. However, little work has been done in\nreinforcement learning, arguably more important and challenging. Previous work\nin quantum reinforcement learning mainly focuses on discrete control tasks\nwhere the action space is discrete. In this work, we develop a quantum\nreinforcement learning algorithm based on soft actor-critic -- one of the\nstate-of-the-art methods for continuous control. Specifically, we use a hybrid\nquantum-classical policy network consisting of a variational quantum circuit\nand a classical artificial neural network. Tested in a standard reinforcement\nlearning benchmark, we show that this quantum version of soft actor-critic is\ncomparable with the original soft actor-critic, using much less adjustable\nparameters. Furthermore, we analyze the effect of different hyper-parameters\nand policy network architectures, pointing out the importance of architecture\ndesign for quantum reinforcement learning.",
  "text": "Variational Quantum Soft Actor-Critic\nQingfeng Lan\nDepartment of Computing Science\nUniversity of Alberta\nEdmonton, Canada\nqlan3@ualberta.ca\nAbstract—Quantum computing has a superior advantage in\ntackling speciﬁc problems, such as integer factorization and\nSimon’s problem. For more general tasks in machine learning, by\napplying variational quantum circuits, more and more quantum\nalgorithms have been proposed recently, especially in supervised\nlearning and unsupervised learning. However, little work has been\ndone in reinforcement learning, arguably more important and\nchallenging. Previous work in quantum reinforcement learning\nmainly focuses on discrete control tasks where the action space\nis discrete. In this work, we develop a quantum reinforcement\nlearning algorithm based on soft actor-critic — one of the state-\nof-the-art methods for continuous control. Speciﬁcally, we use a\nhybrid quantum-classical policy network consisting of a varia-\ntional quantum circuit and a classical artiﬁcial neural network.\nTested in a standard reinforcement learning benchmark, we show\nthat this quantum version of soft actor-critic is comparable\nwith the original soft actor-critic, using much less adjustable\nparameters. Furthermore, we analyze the effect of different\nhyper-parameters and policy network architectures, pointing out\nthe importance of architecture design for quantum reinforcement\nlearning.\nIndex Terms—deep reinforcement learning, quantum rein-\nforcement learning, quantum computation, variational quantum\ncircuits\nI. INTRODUCTION\nA classical bit can only represent 0 or 1. However, a\nquantum bit or qubit can be in a superposition of both 0\nand 1 simultaneously in quantum computing. Furthermore,\na multiple-qubit system can exhibit quantum entanglement,\nallowing a set of qubits to show a much more complex\ncorrelation than a classical system with the same number of\nbits. These unique properties greatly enhance the power of\nquantum computation. Many quantum algorithms have been\nproposed to beneﬁt from these properties, such as Grover’s\nalgorithm (Grover, 1997), Simon’s algorithm (Simon, 1997),\nand Shor’s algorithm (Shor, 1994). These quantum algorithms\nare more efﬁcient than classical algorithms for speciﬁc prob-\nlems.\nRecently, researchers in machine learning began to take\nadvantage of quantum computing and invented many quantum\nmachine learning algorithms, such as classiﬁcation (Farhi\net al., 2020; Schuld et al., 2020; Havlíˇcek et al., 2019),\ntransfer learning (Mari et al., 2020), generative adversarial\nnetworks (Lloyd and Weedbrook, 2018; Dallaire-Demers and\nKilloran, 2018), and clustering problems (Otterbach et al.,\nThe source code is available at https://github.com/qlan3/QuantumExplorer.\n2017). These algorithms utilize variational quantum circuits\n(VQCs), which are computation models with a sequence of\nquantum gates, wires, and measurements. The quantum gates\nare usually “variational” with learnable parameters. So VQCs\nare often viewed as the quantum version of artiﬁcial neural\nnetworks (ANNs), known as quantum neural networks or\nparameterized quantum circuits (Beer et al., 2020; Broughton\net al., 2020).\nAs another important area of machine learning, reinforce-\nment learning (RL) has already shown its great success in\nGo (Silver et al., 2017), video games (Vinyals et al., 2019;\nBerner et al., 2019; Zha et al., 2021; Badia et al., 2020),\nsimulated robotic tasks (Haarnoja et al., 2018; Lillicrap et al.,\n2016), and recommendation systems (Zheng et al., 2018; Zhao\net al., 2018; Afsar et al., 2021). Nevertheless, quantum RL\nremains to be an underexplored area. Most of the previous\nwork focus on discrete control and these quantum algorithms\nare primarily based on Q-learing Williams (1992). Moreover,\nnot all the proposed quantum RL algorithms are properly\ntested in standard RL benchmarks. Instead, some of them\nare tested in sequential decision-making tasks transformed\nfrom classical problems, such as the eigenvalue problem and\nquantum state generation in quantum physics.\nTo ﬁll the gaps mentioned previously, we propose varia-\ntional quantum soft actor-critic — a new quantum RL algo-\nrithm for continuous control tasks. It is based on soft actor-\ncritic (SAC) (Haarnoja et al., 2018), one of the state-of-the-\nart RL algorithms. Speciﬁcally, we design a hybrid quantum-\nclassical policy network consisting of a VQC and a classical\nANN. We test this algorithm in one of the standard RL tasks\nand show that it is able to achieve a similar performance\ncompared to the classical SAC with much less learnable\nparameters.\nII. BACKGROUND\nIn this section, we ﬁrst introduce some background knowl-\nedge of RL for continuous control. We then present a short\nintroduction of soft actor-critic and a brief description of\nVQCs.\nA. Reinforcement Learning for Continuous Control\nConsider a Markov decision process (MDP), M\n=\n(S, A, p, p0, r, γ). Both the state space S and the action space\nA are assumed to be continuous. p : S×A×S →[0, ∞) is the\nstate transition probability. p0 : S →[0, ∞) is the initial state\narXiv:2112.11921v1  [quant-ph]  20 Dec 2021\nFig. 1. The interaction circle between the agent and the environment in RL\nsetting. This ﬁgure is taken from Chapter 3 of Sutton and Barto (2018).\nprobability. r : S × A →R is the reward function. Finally,\nγ ∈[0, 1) is the discount factor.\nGiven a MDP M, an agent generates a trajectory based\non a policy distribution π : S × A →[0, ∞) by interacting\nwith the environment. Speciﬁcally, starting from a state S0 ∼\np0(·), the agent samples an action At ∈A according to the\npolicy π (i.e. At ∼π(·|St)) at each time-step t = 0, 1, 2, . . . ,\nreceives a reward signal Rt = r(St, At), and observes the next\nstate St+1 which is sampled from the transition function (i.e.\nSt+1 ∼p(·|St, At)). The overall interaction circle between the\nagent and the environment is shown in Fig. 1.\nFor\ninﬁnite\nhorizon\ntasks,\nwe\ncan\ndeﬁne\nreturn\nas\nthe\ntotal\ndiscounted\nreward\nfrom\ntime-step\nt:\nGt\n=\nP∞\nk=t γk−tr(Sk, Ak). Value functions can be deﬁned as the\nexpected return under policy π, vπ(s) = Eπ[Gt|St = s];\nsimilarly, action-value functions are deﬁned as qπ(s, a) =\nEπ[Gt|St = s, At = a]. The goal of the agent is to obtain\na policy π that maximizes the expected return starting from\ninitial states. Formally, let a policy πθ be a differentiable\nfunction of a weight vector θ. Our goal is to ﬁnd θ that\nmaximizes the objective funtion\nJ(θ) =\nZ\np0(s)vπθ(s)ds.\n(1)\nB. Soft Actor-Critic\nSAC has shown its excellent performance and high sample\nefﬁciency in many continuous control tasks (Haarnoja et al.,\n2018). As one of the algorithm based on the maximum entropy\nframework (Ziebart et al., 2008; Toussaint, 2009; Haarnoja\net al., 2017), SAC aims to maximize the expected return and\npolicy entropy simultaneously. Different from the standard RL\nsetting, for maximum entropy RL, the return includes entropy\nterms, deﬁned as\nGt =\n∞\nX\nk=t\nγk−t(r(Sk, Ak) + αH(πθ(·|Sk)),\n(2)\nwhere\nα\nis\na\npositive\nconstant\nand\nH(p)\n=\n−\nR\nx p(x) log p(x)dx\nis\nthe\ndifferential\nentropy\nfor\nprobability\ndensity\nfunction\np(x).\nThe\nvalue\nfunctions\nand\naction-value\nfunctions\nare\ndeﬁned\nas\nvπθ(s0)\n=\nEπ [P∞\nt=0 γt(r(st, at) + αH(πθ(·|st)))] and qπθ(s0, a0)\n=\nEπ [P∞\nt=0 γtr(st, at) + α P∞\nt=1 γtH(πθ(·|st))].\nThe\nopti-\nmization objective is still deﬁned as J(θ) =\nR\np0(s)vπθ(s)ds.\nThe policy entropy regularization is believed to improve\nexploration by encouraging more stochastic policies (Haarnoja\net al., 2017; Ziebart, 2010). There is also evidence showing\nthat the optimization landscape gets smoother by introducing\nentropy regularization, which allows a larger learning rate\nand makes it easier to optimize (Ahmed et al., 2019).\nIn practice, we usually parameterize the action-value func-\ntion with a neural network as Qφ(s, a), where φ is the weight\nvector of the neural network. As shown in Haarnoja et al.\n(2018), the objective can then be further reduced to\nJ(θ) = ES∼dπθ ,A∼πθ[Qφ(S, A) −α log(πθ(A|S))],\n(3)\nwhere dπθ is the state distribution given policy πθ. Further-\nmore, to reduce the overestimation bias, two action-value\nfunctions (Qφ1 and Qφ2) are introduced and the minimum\naction-value is used (Fujimoto et al., 2018). The reparameter-\nization technique is applied to reduce the gradient estimation\nvariance. To be speciﬁc, the action A is sampled from the\npolicy π parameterized with θ given the current state S:\nA ∼πθ(·|S). We reparameterize the action with a function f,\nA = fθ(ϵ; S), ϵ ∼p(·) where p(·) is some ﬁxed distribution\nsuch as Gaussian. For simplicity, let ˜Aθ = fθ(ϵ; S) when ϵ and\nS can be easily deduced given the context. Moreover, since\ndπθ is hard to compute in practice, we use a replay buffer D to\napproximate it. Combining the above modiﬁcations, we then\nrewrite the objective as:\nJ(θ) =\nE\nS∼D,ϵ∼N(0,1)[min\ni=1,2 Qφi(S, ˜Aθ) −α log(πθ( ˜Aθ|S))].\n(4)\nC. Variational Quantum Circuits\nA VQC consists of various quantum gates and wires, with\nlearnable parameters (Benedetti et al., 2019) to adjust these\ngates (e.g. rotation gates). Similar to parameters in ANNs,\nthese learnable parameters in VQCs can be optimized to\napproximate complex continuous functions. So VQCs are also\nknown as quantum neural networks or parameterized quantum\ncircuits (Beer et al., 2020; Broughton et al., 2020).\nA typical VQC mainly has three components — an encoder\ncircuit Uθenc, a variational circuit Uθvar, and measurement.\nThe encoder circuit is usually the ﬁrst operation of a quantum\ncircuit. Given a classical input x, Uθenc encodes it into a\nquantum state: x →Uθenc(x)|0⟩\nN n, where n is the number\nof qubits. Here, we introduce three encoder circuits — basis\nembedding, angle embedding, and amplitude embedding. The\nbasis embedding encodes n binary features into a basis state of\nn qubits. The angle embedding encodes n features into rotation\nangles of n qubits with the help of rotation gates. Finally, the\namplitude embedding encodes 2n features into the amplitude\nvector of n qubits, which provides an exponential reduction\nin terms of the number of qubits (Benedetti et al., 2019).\nA variational circuit often follows after an encoder circuit.\nUsually, it consists of alternate layers of rotation gates and\nentanglement gates (e.g., a closed chain or ring of CNOT\ngates) (Kandala et al., 2017; Schuld et al., 2020). After a\nvariational circuit, we measure the states of qubits in the end.\nencoder layer\n1st variational layer\n2nd variational layer\nRx(s1)\nR(α1\n1, β1\n1, γ1\n1)\nR(α2\n1, β2\n1, γ2\n1)\nRx(s2)\nR(α1\n2, β1\n2, γ1\n2)\nR(α2\n2, β2\n2, γ2\n2)\nRx(s3)\nR(α1\n3, β1\n3, γ1\n3)\nR(α2\n3, β2\n3, γ2\n3)\nFig. 2. The architecture of a vanilla VQC with 3 qubits.\n1st variational data re-uploading layer\nlast variational layer\n|0⟩\nR(α1\n1, β1\n1, γ1\n1)\nRx(λ1\n1s1)\n2nd variational\ndata re-uploading\nlayer\nR(αn+1\n1\n, βn+1\n1\n, γn+1\n1\n)\n|0⟩\nR(α1\n2, β1\n2, γ1\n2)\nRx(λ1\n2s2)\nR(αn+1\n2\n, βn+1\n2\n, γn+1\n2\n)\n|0⟩\nR(α1\n3, β1\n3, γ1\n3)\nRx(λ1\n3s3)\nR(αn+1\n3\n, βn+1\n3\n, γn+1\n3\n)\nFig. 3. The architecture of a data re-uploading VQC with 3 qubits.\nSimilar to an ANN, we can also train a VQC with gradient\ndescent. Given a quantum simulator, we could apply back-\npropagation to compute gradient analytically and optimize the\nVQC parameters. However, backpropagation is not applicable\nfor a physical quantum computer since it is impossible to mea-\nsure and store intermediate quantum states during computation\nwithout impacting the whole computation process. Instead,\nwe apply (stochastic) parameter-shift (Li et al., 2017; Mitarai\net al., 2018; Schuld et al., 2019; Banchi and Crooks, 2021)\nto compute gradients for any multi-qubit VQCs. After getting\ngradients, we can then optimize the parameters with traditional\noptimizers used for ANNs, such as RMSprop (Tieleman and\nHinton, 2012) and Adam (Kingma and Ba, 2015).\nIII. RELATED WORK\nThere are already many applications of VQCs and hybrid\nquantum-classical systems in quantum machine learning (Bi-\namonte et al., 2017), such as supervised learning (Reben-\ntrost et al., 2014; Amin et al., 2018), unsupervised learn-\ning (Basheer et al., 2020; Winci et al., 2020), and RL (Dunjko\net al., 2017; Kwak et al., 2021). Since this work is mainly\nabout applying a VQC to RL, we will mainly introduce some\nrelated work in quantum RL in this section.\nDong et al. (2008) suggested to represent states and actions\nas eigenvectors of a Hilbert space for a quantum system and\nproposed a quantum RL algorithm based on Q-learning (Dayan\nand Watkins, 1992). This algorithm applies Grover itera-\ntion (Grover, 1997) to update action selection probabilities. It\nis shown to have an excellent exploration-exploitation balance\nin a gridworld environment. However, it is only applicable\nto tasks with discrete states and actions, due to the discrete\nrepresentation scheme. Wang et al. (2020) designed a quantum\nversion of the classical cartpole balancing problem (Barto\net al., 1983) and showed that deep Q-learning (Volodymyr\net al., 2013; Mnih et al., 2015) was able to solve this task.\nThe results suggested that general quantum control tasks can\nalso serve as RL benchmarks.\nChen et al. (2020) and Skolik et al. (2021) showed that vari-\national quantum deep Q-learning worked with the experience\nreplay and the target network. Kwak et al. (2021) proposed\na quantum RL algorithm based on deep Q-learning (Mnih\net al., 2015) and proximal policy optimization (Schulman\net al., 2017). The implementation used the PennyLane li-\nbrary (Bergholm et al., 2018) and was tested in the cart-\npole environment (Barto et al., 1983). Later, softmax-PQC\nis developed by Jerbi et al. (2021) which applied a softmax\npolicy to successfully solve several standard RL tasks, based\non REINFORCE (Williams, 1992). Wu et al. (2020) extended\nquantum RL algorithm to tasks with continuous action spaces\nbased on deep deterministic policy gradient (DDPG) (Silver\net al., 2014). Finally, Wei et al. (2021) proposed a deep RL\nalgorithm with a quantum-inspired experience replay which\nhelps to achieve a better balance between exploration and\nexploitation.\nIV. VARIATIONAL QUANTUM SOFT ACTOR-CRITIC\nAlthough Wu et al. (2020) proposed the quantum DDPG\nalgorithm to solve continuous tasks, it is not tested in any\nstandard RL benchmarks. To ﬁll this gap, we propose vari-\national quantum SAC (QuantumSAC) — a quantum version\nof SAC with a hybrid quantum-classical policy network. Note\nthat for both SAC and QuantumSAC, a policy network has\nstate s as input and outputs the parameters of a policy\ndistribution. For example, we output the mean µ and the\nUθenc\nencoder \ncircuit\nUθvar\nvariational \ncircuit\nmeasurement\ninput s\nμ\nσ\n3-qubit \nquantum layer \n(VQC)\n4-neuron \nclassical layer \n(ANN)\nFig. 4.\nThe architecture of a hybrid quantum-classical policy network for\nQuantumSAC.\nFig. 5. Pendulum-v0 from OpenAI gym. The goal of this task is to swing\na frictionless pendulum up and keep it upright. The state is a 3d continuous\nvector, and the action is a real number chosen from [−2.0, 2.0]. For each\nepisode, the pendulum starts at a random angle from −π to π with a random\nvelocity between −1 and 1. After 200 steps, an episode terminates.\nstandard deviation σ for a Gaussian policy distribution. For\nQuantumSAC, following Jerbi et al. (2021), we use a VQC to\nencode input state s by angle embedding where each qubit is\nused to encode one element in s. Thus, the number of qubits\nequals the dimension of state s. Since the input (s) dimension\nand the output (µ and σ) dimension are usually different, we\nmay encounter a dimension mismatch if we measure the state\nof each qubit and output the results directly. To solve this\nproblem, after measuring the states of qubits, we apply a linear\nANN to transform the measurement results to parameters of a\npolicy distribution. The overall structure of an examplar hybrid\nquantum-classical neural network (with three qubits and four\nneurons) is shown in Fig. 4.\nWe present a complete description of QuantumSAC in\nAlgorithm 1. The critical difference between SAC and Quan-\ntumSAC is that we replace the classical policy network in\nSAC with a hybrid quantum-classical policy network in Quan-\ntumSAC. In general, hybrid quantum-classical networks can\nbe used to estimate action-value functions as well. However,\nusing a hybrid quantum-classical policy network is enough\nto serve our purpose, and we still use ANNs as action-value\nnetworks for simplicity.\nV. EXPERIMENTS\nIn this section, we demonstrate the potential of Quantum-\nSAC in a standard RL benchmark from OpenAI gym (Brock-\nman et al., 2016) — Pendulum-v0 (Fig. 5). Both the state space\nand action space for Pendulum-v0 are continuous. The state\nis a 3d vector, and the action is a real number chosen from\nAlgorithm 1 QuantumSAC\nInput: initial policy parameters θ, initial action-value es-\ntimate parameters φ1 and φ2, γ, α, ρ, empty experience\nreplay D.\nInitialize the hybrid quantum-classical policy network with\nθ.\nInitialize two action-value networks with φ1 and φ2 respec-\ntively.\nSet target action-value networks parameters: φtarg,1 ←φ1\nand φtarg,2 ←φ2.\nfor each time-step do\nObserve state S, select action A ∼πθ(·|S), and excute\nA in the environment.\nObserve next state S′, reward R, and binary done signal\nd to indicate whether S′ is a terminal state or not.\nStore (S, A, R, S′, d) in D.\nReset the environment state if d = 1 (i.e. S′ is terminal).\nSample a batch of transitions B = {(S, A, R, S′, d)}\nfrom D randomly.\nCompute target values y(R, S′, d)\n=\nR + γ(1 −\nd)\n\u0000mini=1,2 Qφtarg,i(S′, A′) −α log πθ(A′|S′)\n\u0001\nwhere\nA′ ∼πθ(·|S′).\nUpdate φi by minimizing: EB[(Qφi(S, A)−y(R, S′, d))2]\nfor i = 1, 2.\nUpdate θ by maximizing: EB[mini=1,2 Qφi(S, ˜Aθ) −\nα log πθ( ˜Aθ|S)].\nDo a soft update for target action-value networks:\nφtarg,i ←ρφtarg,i + (1 −ρ)φi for i = 1, 2.\nend for\n[−2.0, 2.0]. So we use a linear layer with input dimension 3\nand output dimension 2 (µ and σ) for the ANN part in the\nhybrid quantum-classical policy network. For each episode,\nthe pendulum starts at a random angle from −π to π with\na random velocity between −1 and 1. After 200 steps, an\nepisode terminates. The goal of this task is to swing a\nfrictionless pendulum up and keep it upright.\nFor the VQC part, we investigate two options, a vanilla\nVQC and a data re-uploading VQC (Pérez-Salinas et al.,\n2020), as shown in Fig. 2 and Fig. 3 respectively. A vanilla\nVQC has an encoder circuit (an angle embedding layer) and\nseveral variational layers without any other fancy structures. A\ndata re-uploading VQC consists of several variational data re-\nuploading layers and one last variational layer. Compared to a\nvanilla VQC, the input data is encoded many times in a data re-\nuploading VQC, which signiﬁcantly improves the expressivity\nof VQCs (Pérez-Salinas et al., 2020; Skolik et al., 2021; Jerbi\net al., 2021). We will verify this claim in our settings.\nThe experimental setup is as follows. Our implementations\nof SAC and QuantumSAC were based on PyTorch (Paszke\net al., 2019) and PennyLane (Bergholm et al., 2018), running\nin simulation. We trained each algorithm with 50, 000 steps.\nFor both algorithms, the action-value networks were multi-\nlayer perceptrons (MLPs) with hidden layers [32, 32]. The\n0\n10000\n20000\n30000\n40000\n50000\nStep\n1400\n1200\n1000\n800\n600\n400\n200\nAverage Return\nSAC\nQuantumSAC (re-uploading VQC)\nQuantumSAC (vanilla VQC)\nFig. 6.\nLearning curves of QuantumSAC and SAC with the best hyper-\nparameter setting in Pendulum-v0. The depicted return in all learning curves\nwas averaged over the last 10 episodes, and the curves were smoothed using\nan exponential average. All experimental results were averaged over 10 runs,\nwith the shaded area representing one standard error. In terms of the overall\nperformance, SAC ≈QuantumSAC (re-uploading VQC) > QuantumSAC\n(vanilla VQC).\npolicy network for SAC was also an MLP with hidden layers\n[32, 32]. For QuantumSAC, the policy network was a hybrid\nquantum-classical network where the number of VQC layers 1\n(denoted as n) was chosen from {1, 2, 4, 8}. The size of the\nexperience replay was 10, 000. The batch size was 32. The\ndiscount factor γ was 0.99. α was set to 0.2 and ρ was set to\n0.995. All networks were optimized by Adam (Kingma and\nBa, 2015). The step-size of action-value networks were set to\n3e−3. We did a grid search for the step-size of policy networks\nwith scope {1e −1, 3e −2, 1e −2, 3e −3, 1e −3, 3e −4}.\nWe compared the performance of each algorithm by the mean\nreturn of the last 10 training episodes. The depicted returns in\nall learning curves were averaged over the last 10 episodes,\nand the curves were smoothed using an exponential average.\nAll experimental results were averaged over 10 runs, with the\nshaded area representing one standard error.\na) QuantumSAC v.s. SAC: We ﬁrst show learning curves\nof QuantumSAC and SAC with their best hyper-parameter\nsettings in Fig. 6. Clearly, we see that with a data re-uploading\nVQC policy network, QuantumSAC almost performs as well\nas SAC. Note that the number of VQC layers n of Quan-\ntumSAC (re-uploading VQC) shown in this ﬁgure is 2. So\nthe total number of learnable parameters in this data re-\nuploading VQC policy network is just 41, while the total\nnumber of learnable parameters in SAC policy network is\n1, 250! This result shows the signiﬁcant advantage of VQCs\nin reducing model parameters while achieving a comparable\nsample efﬁciency.\nHowever, the performance of QuantumSAC is greatly in-\nﬂuenced by the architecture of the VQC — QuantumSAC\n1For a vanilla VQC, the number of VQC layers refers to the number of\nvariational layers. For a data re-uploading VQC, the number of VQC layers\nrefers to the number of variational data re-uploading layers.\n0\n10000\n20000\n30000\n40000\n50000\nStep\n1300\n1200\n1100\n1000\n900\n800\n700\n600\n500\nAverage Return\nn=1\nn=2\nn=4\nn=8\n(a) QuantumSAC with a vanilla VQC\n0\n10000\n20000\n30000\n40000\n50000\nStep\n1600\n1400\n1200\n1000\n800\n600\n400\n200\nAverage Return\nn=1\nn=2\nn=4\nn=8\n(b) QuantumSAC with a data re-uploading VQC\nFig. 7. Learning curves of QuantumSAC with various numbers of VQC layers\nin Pendulum-v0. The depicted return in all learning curves was averaged over\nthe last 10 episodes, and the curves were smoothed using an exponential\naverage. All experimental results were averaged over 10 runs, with the shaded\narea representing one standard error. In general, more VQC layers lead to\nbetter performance.\n(vanilla VQC) is outperformed by both SAC and QuantumSAC\n(re-uploading VQC). This result reveals the importance of\narchitecture design for VQCs in quantum RL. The design of\nVQCs may even beneﬁt from the evolution road of ANNs. To\nconclude, we believe that more empirical effort and theoretical\nunderstanding are needed in designing more powerful and\nefﬁcient VQCs for quantum RL.\nb) The inﬂuence of the number of VQC layers: We\nvaried the number of VQC layers n in QuantumSAC for\nboth vanilla VQCs and data re-uploading VQCs to study its\neffect on the learning process. To be speciﬁc, n was chosen\nfrom {1, 2, 4, 8}. For each n, we swept the step-size for the\npolicy network and presented the results with the best hyper-\nparameter conﬁgurations. As shown in Fig. 7, the performance\nis generally improved as n increases. However, the detailed\neffect is different between vanilla VQCs and data re-uploading\nVQCs. For vanilla VQCs, there is a consistent performance\nimprovement as n increases, although the learning stability has\nno apparent change. For data re-uploading VQCs, there is a\nclear performance improvement when n is increased from 1 to\n2. The variance of the average return is also reduced obviously.\nHowever, after adding more layers, both the performance\nimprovement and variance reduction are not evident. This\nresult suggests that a 2-layer data re-uploading VQC already\nhas enough capacity to model the optimal policy, while the\nmodelling capacity of an 8-layer vanilla VQC is much lower.\nVI. CONCLUSION AND FUTURE WORK\nIn this work, we presented the variational quantum soft\nactor-critic algorithm and tested it in a pendulum balanc-\ning task. We showed the quantum advantage in reducing\nmodel parameters while achieving similar performance to the\nclassical algorithm. To fully exploit the power of quantum\ncomputation in RL, designing more expressive and efﬁcient\nVQCs is essential. As for future work, we would like to test\nour algorithm further in a physical quantum computer.\nREFERENCES\nAfsar, M. M., Crump, T., and Far, B. (2021). Reinforcement\nlearning based recommender systems: A survey.\narXiv\npreprint arXiv:2101.06286.\nAhmed, Z., Le Roux, N., Norouzi, M., and Schuurmans, D.\n(2019).\nUnderstanding the impact of entropy on policy\noptimization.\nIn International Conference on Machine\nLearning.\nAmin, M. H., Andriyash, E., Rolfe, J., Kulchytskyy, B., and\nMelko, R. (2018). Quantum boltzmann machine. Physical\nReview X.\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvit-\nskyi, A., Guo, Z. D., and Blundell, C. (2020). Agent57:\nOutperforming the atari human benchmark. In International\nConference on Machine Learning.\nBanchi, L. and Crooks, G. E. (2021).\nMeasuring analytic\ngradients of general quantum evolution with the stochastic\nparameter shift rule. Quantum.\nBarto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neu-\nronlike adaptive elements that can solve difﬁcult learning\ncontrol problems. IEEE transactions on systems, man, and\ncybernetics.\nBasheer, A., Afham, A., and Goyal, S. K. (2020).\nQuan-\ntum\nk-nearest\nneighbors\nalgorithm.\narXiv\npreprint\narXiv:2003.09187.\nBeer, K., Bondarenko, D., Farrelly, T., Osborne, T. J., Salz-\nmann, R., Scheiermann, D., and Wolf, R. (2020). Training\ndeep quantum neural networks. Nature communications.\nBenedetti, M., Lloyd, E., Sack, S., and Fiorentini, M. (2019).\nParameterized quantum circuits as machine learning models.\nQuantum Science and Technology.\nBergholm, V., Izaac, J., Schuld, M., Gogolin, C., Alam,\nM. S., Ahmed, S., Arrazola, J. M., Blank, C., Delgado,\nA., Jahangiri, S., et al. (2018).\nPennyLane: Automatic\ndifferentiation of hybrid quantum-classical computations.\narXiv preprint arXiv:1811.04968.\nBerner, C., Brockman, G., Chan, B., Cheung, V., D˛ebiak, P.,\nDennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C.,\net al. (2019). Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680.\nBiamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe,\nN., and Lloyd, S. (2017).\nQuantum machine learning.\nNature.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schul-\nman, J., Tang, J., and Zaremba, W. (2016). Openai gym.\narXiv preprint arXiv:1606.01540.\nBroughton, M., Verdon, G., McCourt, T., Martinez, A. J., Yoo,\nJ. H., Isakov, S. V., Massey, P., Halavati, R., Niu, M. Y.,\nZlokapa, A., et al. (2020). Tensorﬂow quantum: A software\nframework for quantum machine learning. arXiv preprint\narXiv:2003.02989.\nChen, S. Y.-C., Yang, C.-H. H., Qi, J., Chen, P.-Y., Ma, X.,\nand Goan, H.-S. (2020). Variational quantum circuits for\ndeep reinforcement learning. IEEE Access.\nDallaire-Demers, P.-L. and Killoran, N. (2018).\nQuantum\ngenerative adversarial networks. Physical Review A.\nDayan, P. and Watkins, C. (1992).\nQ-learning.\nMachine\nlearning.\nDong, D., Chen, C., Li, H., and Tarn, T.-J. (2008). Quantum\nreinforcement learning.\nIEEE Transactions on Systems,\nMan, and Cybernetics, Part B (Cybernetics).\nDunjko, V., Taylor, J. M., and Briegel, H. J. (2017).\nAd-\nvances in quantum reinforcement learning. In 2017 IEEE\nInternational Conference on Systems, Man, and Cybernetics\n(SMC).\nFarhi, E., Neven, H., et al. (2020). Classiﬁcation with quantum\nneural networks on near term processors. Quantum Review\nLetters.\nFujimoto, S., Hoof, H., and Meger, D. (2018).\nAddressing\nfunction approximation error in actor-critic methods.\nIn\nInternational Conference on Machine Learning.\nGrover, L. K. (1997). Quantum mechanics helps in searching\nfor a needle in a haystack. Physical Review Letters.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017).\nReinforcement learning with deep energy-based policies. In\nInternational Conference on Machine Learning.\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S.,\nTan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al.\n(2018). Soft actor-critic algorithms and applications. arXiv\npreprint arXiv:1812.05905.\nHavlíˇcek, V., Córcoles, A. D., Temme, K., Harrow, A. W.,\nKandala, A., Chow, J. M., and Gambetta, J. M. (2019).\nSupervised learning with quantum-enhanced feature spaces.\nNature.\nJerbi, S., Gyurik, C., Marshall, S., Briegel, H., and Dunjko,\nV. (2021). Parametrized quantum policies for reinforcement\nlearning. Advances in Neural Information Processing Sys-\ntems.\nKandala,\nA.,\nMezzacapo,\nA.,\nTemme,\nK.,\nTakita,\nM.,\nBrink, M., Chow, J. M., and Gambetta, J. M. (2017).\nHardware-efﬁcient variational quantum eigensolver for\nsmall molecules and quantum magnets. Nature.\nKingma, D. P. and Ba, J. (2015).\nAdam: A method for\nstochastic optimization.\nIn International Conference on\nLearning Representations.\nKwak, Y., Yun, W. J., Jung, S., Kim, J.-K., and Kim, J. (2021).\nIntroduction to quantum reinforcement learning: Theory and\npennylane-based implementation.\nIn 2021 International\nConference on Information and Communication Technology\nConvergence (ICTC).\nLi, J., Yang, X., Peng, X., and Sun, C.-P. (2017).\nHybrid\nquantum-classical approach to quantum optimal control.\nPhysical Review Letters.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,\nTassa, Y., Silver, D., and Wierstra, D. (2016). Continuous\ncontrol with deep reinforcement learning. In International\nConference on Learning Representations.\nLloyd, S. and Weedbrook, C. (2018).\nQuantum generative\nadversarial learning. Physical review letters.\nMari, A., Bromley, T. R., Izaac, J., Schuld, M., and Killoran,\nN. (2020). Transfer learning in hybrid classical-quantum\nneural networks. Quantum.\nMitarai, K., Negoro, M., Kitagawa, M., and Fujii, K. (2018).\nQuantum circuit learning. Physical Review A.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland,\nA. K., Ostrovski, G., et al. (2015).\nHuman-level control\nthrough deep reinforcement learning. Nature.\nOtterbach, J., Manenti, R., Alidoust, N., Bestwick, A., Block,\nM., Bloom, B., Caldwell, S., Didier, N., Fried, E. S., Hong,\nS., et al. (2017). Unsupervised machine learning on a hybrid\nquantum computer. arXiv preprint arXiv:1712.05771.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. (2019).\nPyTorch: An imperative style, high-\nperformance deep learning library.\nAdvances in neural\ninformation processing systems.\nPérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E., and La-\ntorre, J. I. (2020). Data re-uploading for a universal quantum\nclassiﬁer. Quantum.\nRebentrost, P., Mohseni, M., and Lloyd, S. (2014). Quantum\nsupport vector machine for big data classiﬁcation. Physical\nReview Letters.\nSchuld, M., Bergholm, V., Gogolin, C., Izaac, J., and Killo-\nran, N. (2019). Evaluating analytic gradients on quantum\nhardware. Physical Review A.\nSchuld, M., Bocharov, A., Svore, K. M., and Wiebe, N. (2020).\nCircuit-centric quantum classiﬁers. Physical Review A.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. (2017).\nProximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347.\nShor, P. W. (1994).\nAlgorithms for quantum computation:\ndiscrete logarithms and factoring.\nIn Proceedings 35th\nannual symposium on foundations of computer science.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and\nRiedmiller, M. (2014). Deterministic policy gradient algo-\nrithms. In Proceedings of the 31st International Conference\non International Conference on Machine Learning.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I.,\nHuang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton,\nA., et al. (2017). Mastering the game of Go without human\nknowledge. Nature.\nSimon, D. R. (1997). On the power of quantum computation.\nSIAM journal on computing.\nSkolik, A., Jerbi, S., and Dunjko, V. (2021). Quantum agents\nin the gym: a variational quantum algorithm for deep q-\nlearning. arXiv preprint arXiv:2103.15084.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learn-\ning: An Introduction. MIT press, second edition.\nTieleman, T. and Hinton, G. (2012). Lecture 6.5—RMSprop:\nDivide the gradient by a running average of its recent\nmagnitude.\nCOURSERA: Neural Networks for Machine\nLearning.\nToussaint, M. (2009).\nRobot trajectory optimization using\napproximate inference. In Proceedings of the 26th annual\ninternational conference on machine learning.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,\nDudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T.,\nGeorgiev, P., et al. (2019). Grandmaster level in StarCraft\nII using multi-agent reinforcement learning. Nature.\nVolodymyr, M., Kavukcuoglu, K., Silver, D., Graves, A., and\nAntonoglou, I. (2013). Playing Atari with Deep Reinforce-\nment Learning. In NIPS Deep Learning Workshop.\nWang, Z. T., Ashida, Y., and Ueda, M. (2020). Deep rein-\nforcement learning control of quantum cartpoles. Physical\nReview Letters.\nWei, Q., Ma, H., Chen, C., and Dong, D. (2021).\nDeep\nreinforcement learning with quantum-inspired experience\nreplay. IEEE Transactions on Cybernetics.\nWilliams, R. J. (1992). Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. Ma-\nchine learning.\nWinci, W., Buffoni, L., Sadeghi, H., Khoshaman, A., An-\ndriyash, E., and Amin, M. H. (2020).\nA path towards\nquantum advantage in training deep generative models with\nquantum annealers. Machine Learning: Science and Tech-\nnology.\nWu, S., Jin, S., Wen, D., and Wang, X. (2020).\nQuantum\nreinforcement learning in continuous action space. arXiv\npreprint arXiv:2012.10711.\nZha, D., Xie, J., Ma, W., Zhang, S., Lian, X., Hu, X.,\nand Liu, J. (2021).\nDouZero: Mastering DouDizhu with\nself-play deep reinforcement learning.\narXiv preprint\narXiv:2106.06135.\nZhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., and Tang, J.\n(2018). Deep reinforcement learning for page-wise recom-\nmendations. In Proceedings of the 12th ACM Conference\non Recommender Systems.\nZheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J., Xie,\nX., and Li, Z. (2018). DRN: A deep reinforcement learning\nframework for news recommendation. In Proceedings of the\n2018 World Wide Web Conference.\nZiebart, B. D. (2010). Modeling purposeful adaptive behavior\nwith the principle of maximum causal entropy. Carnegie\nMellon University.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al.\n(2008). Maximum entropy inverse reinforcement learning.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intel-\nligence.\n",
  "categories": [
    "quant-ph",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2021-12-20",
  "updated": "2021-12-20"
}