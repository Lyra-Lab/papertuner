{
  "id": "http://arxiv.org/abs/2112.11947v3",
  "title": "Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment",
  "authors": [
    "Aizaz Sharif",
    "Dusica Marijan"
  ],
  "abstract": "Deep reinforcement learning is actively used for training autonomous car\npolicies in a simulated driving environment. Due to the large availability of\nvarious reinforcement learning algorithms and the lack of their systematic\ncomparison across different driving scenarios, we are unsure of which ones are\nmore effective for training autonomous car software in single-agent as well as\nmulti-agent driving environments. A benchmarking framework for the comparison\nof deep reinforcement learning in a vision-based autonomous driving will open\nup the possibilities for training better autonomous car driving policies. To\naddress these challenges, we provide an open and reusable benchmarking\nframework for systematic evaluation and comparative analysis of deep\nreinforcement learning algorithms for autonomous driving in a single- and\nmulti-agent environment. Using the framework, we perform a comparative study of\ndiscrete and continuous action space deep reinforcement learning algorithms. We\nalso propose a comprehensive multi-objective reward function designed for the\nevaluation of deep reinforcement learning-based autonomous driving agents. We\nrun the experiments in a vision-only high-fidelity urban driving simulated\nenvironments. The results indicate that only some of the deep reinforcement\nlearning algorithms perform consistently better across single and multi-agent\nscenarios when trained in various multi-agent-only environment settings. For\nexample, A3C- and TD3-based autonomous cars perform comparatively better in\nterms of more robust actions and minimal driving errors in both single and\nmulti-agent scenarios. We conclude that different deep reinforcement learning\nalgorithms exhibit different driving and testing performance in different\nscenarios, which underlines the need for their systematic comparative analysis.\nThe benchmarking framework proposed in this paper facilitates such a\ncomparison.",
  "text": "Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies\nin a Multi-agent Urban Driving Environment\nAizaz Sharif\nSimula Research Laboratory\nOslo, Norway\naizaz@simula.no\nDusica Marijan\nSimula Research Laboratory\nOslo, Norway\ndusica@simula.no\nAbstract—Background: Deep reinforcement learning is actively\nused for training autonomous car policies in a simulated driving\nenvironment. Due to the large availability of various reinforce-\nment learning algorithms and the lack of their systematic com-\nparison across different driving scenarios, we are unsure of which\nones are more effective for training autonomous car software\nin single-agent as well as multi-agent driving environments.\nAims: A benchmarking framework for the comparison of deep\nreinforcement learning in a vision-based autonomous driving\nwill open up the possibilities for training better autonomous\ncar driving policies. Method: To address these challenges, we\nprovide an open and reusable benchmarking framework for\nsystematic evaluation and comparative analysis of deep rein-\nforcement learning algorithms for autonomous driving in a\nsingle- and multi-agent environment. Using the framework, we\nperform a comparative study of four discrete and two contin-\nuous action space deep reinforcement learning algorithms. We\nalso propose a comprehensive multi-objective reward function\ndesigned for the evaluation of deep reinforcement learning-based\nautonomous driving agents. We run the experiments in a vision-\nonly high-ﬁdelity urban driving simulated environments. Results:\nThe results indicate that only some of the deep reinforcement\nlearning algorithms perform consistently better across single\nand multi-agent scenarios when trained in various multi-agent-\nonly environment settings. For example, A3C- and TD3-based\nautonomous cars perform comparatively better in terms of more\nrobust actions and minimal driving errors in both single and\nmulti-agent scenarios. Conclusions: We conclude that different\ndeep reinforcement learning algorithms exhibit different driving\nand testing performance in different scenarios, which underlines\nthe need for their systematic comparative analysis. The bench-\nmarking framework proposed in this paper facilitates such a\ncomparison.\nKeywords—deep reinforcement learning; multi-agent systems;\nautonomous cars; autonomous driving; testing autonomous driving\nI. INTRODUCTION\nAutonomous cars (ACs) are complex decision-making sys-\ntems that are unfortunately prone to errors [1]. They commonly\nuse machine/deep learning algorithms as part of decision-\nmaking software, which is known to be difﬁcult to validate\n[2], [3]. Therefore, ACs need comprehensive training and\nevaluation in order to minimize risk to the public. For the\nsame reason, autonomous driving (AD) research is nowadays\nperformed within simulated driving environments (also used\nby the state-of-the-art industrial solutions like Tesla [4] and\nComma ai [5]), as they provide ﬂexibility for testing and\nvalidating AD without posing any danger to the real world.\nHowever, we observe three speciﬁc challenges in training and\nvalidating ACs in the existing simulation environments.\nFirst, while the majority of AD research is focused on using\ndeep reinforcement learning (DRL) for training ACs in a simu-\nlated urban driving scenario [6] [7] [8] [9] [10], there is a lack\nof comparison among DRL algorithms for vision-based urban\ndriving scenarios. Having such a benchmark of commonly\nused DRL algorithms can be useful for understanding why\nsome algorithms perform worse than others in speciﬁc driving\nscenarios, which can lead to the improvements of the state-of-\nart DRL algorithms for AD.\nSecond, the majority of existing research trains ACs as non-\ncommunicating and independent single intelligent agents [11]\n[12], therefore treating the ACs as a single-agent driving\nproblem. However, in the near future AD will be a complex\nmulti-agent problem [13]. By bringing more than one AC\ninto a multi-agent environment we can evaluate how a non-\nstationary driving scenario affects AC’s control decisions when\ninteracting with other ACs. Existing comparative analyses of\nRL algorithms for AD are still limited to single-agent driving\nenvironments [14] [15], and there is no systematic study\nperformed yet on which DRL models work best for AD in\na multi-agent environment.\nThird, a vast portion of existing research is actively focused\non testing ACs trained on vision-based end-to-end systems.\nWhile designing a reward function is one of the key require-\nments in learning an efﬁcient and robust DRL driving agent,\nwe lack evidence of a detailed multi-objective-based reward\nfunction that is utilized within independent multi-agent DRL\nalgorithms. There is ongoing research on designing a better\nreward function [16] [17] which is highly required for better\nautonomous driving, but so far they are still focused on single-\nagent driving scenarios. Furthermore, in order to compare\nwhich DRL works the best in single and multi-agent-focused\nscenarios, their reward functions must also be deﬁned in a\ncomprehensive way in order to perform a fair comparison\nin terms of robustness. As DRL heavily relies on the design\nof the reward function for active exploration and learning, it\nis necessary to consider various objectives within a reward\nfunction in order to observe and evaluate which objectives are\nsuccessfully achieved by each DRL-based AC agent within\ndifferent driving settings. We, therefore, also lack studies on\nthe performance of DRL algorithms that are trying to achieve\nmultiple goals using the reward function.\nTo address these three challenges, in this paper we provide\narXiv:2112.11947v3  [cs.AI]  23 Mar 2023\nan open and reusable end-to-end benchmarking framework\nfor DRL algorithms in a complex urban multi-agent AD\nenvironment. The framework enables us to bring competitive\nand independent driving agents in both discrete and continuous\naction space environments. We also propose a comprehensive\nmulti-objective reward function designed for the evaluation of\nsimulation-based learning of DRL AC agents. We validate the\nframework by performing a comprehensive comparative study\nof the robustness of DRL algorithms for urban driving policies\nin both single-agent and multi-agent AD scenarios.\nThe key contributions in this paper are:\n1) We provide an end-to-end benchmarking framework for\nthe systematic evaluation of DRL-based AC driving\npolicies in complex vision-based urban driving environ-\nments.\n2) We propose a multi-objective reward function designed\nfor the evaluation of simulation-based learning of DRL\nAC agents.\n3) The framework supports training and validating AC’s\ndriving policies in both single- and multi-agent driving\nscenarios.\n4) The framework enables evaluating the effectiveness of\nAC driving agents in different environment conﬁgura-\ntions.\n5) Using the framework, we perform a comprehensive\ncomparative study of four discrete and two continuous\naction space DRL algorithms for AD in vision-only high\nﬁdelity urban driving simulated environments.\n6) Drawing from the study results, we suggest some re-\nsearch directions on improving the robustness of DRL\nalgorithms for AD.\n7) The implementation of our benchmarking framework, as\nwell as all experimental results, are open and reusable,\nwhich supports the reproducibility of research in the AD\ndomain.\nII. RELATED WORK\nIn AD research, there are only a few benchmarks for evalu-\nating the performance of RL-based AD models. Vinitsky [15]\nproposes a benchmark for DRL in mixed-autonomy trafﬁc.\nWhile the benchmark involves four scenarios: the ﬁgure eight\nnetwork, the merge network, the grid, and the bottleneck, it\nevaluates a limited number of reinforcement learning (RL)\nalgorithms (two gradient-based and two gradient-free). In\naddition, the proposed benchmark is speciﬁc to connected\nAD research. Stang [14] proposes another benchmark for RL\nalgorithms in a simulated AD environment. As a limitation,\nthis benchmark focuses on a simple lane-tracking task, and\nfurthermore, evaluates only off-policy RL algorithms. In con-\ntrast, our work evaluates both on-policy and off-policy algo-\nrithms and further allows for comparing the performance of\nDRL algorithms in a complex urban environment. As another\nlimitation, both [15] and [14] only support DRL benchmarking\nfor single-agent AD environments.\nLi [18] introduces a driving simulation framework called\nMetaDrive and performs a benchmarking of RL algorithms for\nAD. While the authors use ﬁve different driving scenarios, they\nonly evaluate two RL algorithms (PPO and SAC). The work\ncould also beneﬁt from using realistic visual rendering as pro-\nvided by the CARLA framework in our work. Palanisamy [19]\nproposes a multi-agent urban driving framework in which one\ncan train more than one AC. Using IMPALA, a connected AC\npolicy is trained within the CARLA simulator. However, as a\nlimitation, the work is restricted to connected AD problems\nonly.\nFurthermore, there are frameworks proposed for training\nand testing autonomous vehicles. For example, F1TENTH\nframework [20] with three racing scenarios and baselines\nfor testing and evaluating autonomous vehicles. However, the\nframework does not support dealing with ARL. Han [21]\nproposes an off-road simulated environment for AD with\nrealistic off-road scenes, such as mountains, deserts, snowy\nﬁelds, and highlands. While realistic environments are useful\nfor evaluating the generalization abilities of AD models, the\nwork is limited to single-agent AD environments.\nIII. DEEP REINFORCEMENT LEARNING FOR\nAUTONOMOUS DRIVING\nReinforcement learning is mainly modeled as a formulation\nof the Markov Decision Process (MDP), where the desired\ngoal of the agents in a certain environment is to learn an op-\ntimal policy. This goal is achieved by maximizing cumulative\nreward after interacting with an environment. The MDP model\nconsists of M(S, A, P, R, γ), where S is a set of agent’s state\nand A is a set of discrete or continuous actions. R : S×A 7→R\nis a reward function value returned against every action A,\nwhereas γ is the discount rate applied to the future reward\nvalues. Lastly, the MDP model consists of a P : S × A 7→S\nas the transition probability which describes the stochastic\nprobability distribution of the next state s′ ϵ S given actions.\nFollowing the basics of the MDP model, the agent is dependent\non the previous state only to make the next decision. Such a\nsystem obeys Markov property during RL control decisions.\nReinforcement learning has achieved great results over the\npast few years due to the advancements in DRL. In DRL,\nthe MDP model is solved by using deep neural networks to\nlearn weight parameters θ over the time span of environment\nexploration. DRL models consist of a policy π which is\nresponsible for taking an action given a state π(a|s) and\na value function vπs for estimating maximum reward given\nthe current policy s ϵ S. Traditional RL fails at solving\nhigh-dimensional state space problems and therefore deep\nneural networks enable the possibility of learning a function\napproximation over large input and action state spaces to solve\ncomplex problems. The policy and value functions in DRL\nare therefore learned using the deﬁned deep net models to\nestimate future actions. In our work, the DRL algorithms we\nchoose to benchmark are based on a model-free approach.\nIn model-free RL, the policy π and value function vπs are\nlearned directly by interacting with the environment without\ntaking model dynamics and the transition probability function\nof every state space into consideration.\nNext, we provide a brief description of the DRL models\nwe use for the training and validation of AD in a simulated\nurban environment. These models are chosen on the basis of\ni) popularity in the DRL-based AD research community and\nii) coverage of discrete as well as continuous action space (for\nmulti-agent testing purposes).\nA. DRL Algorithms for Autonomous Driving\n1) Discrete Action Space:\nProximal Policy Optimization (PPO):\nPPO [22] is a DRL algorithm, which also serves as one of\nthe extensions to the policy gradient (PG) algorithms. Vanilla\nPG faces the problem of high gradient variance, and therefore\nPPO solves it by adding constraints like clipped surrogate\nobjective and the KL penalty coefﬁcient. Such improvements\nmade PPO a very easy choice in the domain of DRL over the\npast few years.\nIn terms of vision-based AD, the authors in [6] use PPO\nas an RL algorithm to train a driving policy using synthetic\nsimulated RGB images. The trained policy is transferred to\nreal-world testing experiments for analyzing the perception\nand control perspective of the AC. Another authors in [23]\nuses PPO for proposing a road detecting algorithm in an urban\ndriving environment. They carry out experiments in a Udacity\nracing game simulator [24] as well as in a small OpenAI gym\ncarracing-v0 environment [25].\nAsynchronous Advantage Actor-Critic (A3C):\nA3C [26] is a well-known gradient descent-based RL algo-\nrithm. Following the on-policy technique, it focuses on using\ntwo neural networks - actor and critic. An actor is respon-\nsible for making actions while the critic aims for learning a\nvalue function. A3C takes this approach by keeping a global\ncritic model while making multiple actor models for parallel\ntraining.\nA3C has been used in [27] for training an AC policy in\na virtual environment that can work in a real-world situation\nas well. By using synthetic images as an input, the policy\nlearns to drive in an urban scenario using the proposed DRL\nmethod. Another work by the authors in [7] also uses A3C by\ncombining RL with image semantic segmentation to train an\nAC driving policy. Using the TORCS [28] racing simulator,\ntheir goal is to lower the gap between virtual and real models\nwhile training an RL agent. A3C is also used in [29] where\nthe authors created an end-to-end driving approach without\ndetailed perception tasks on input images.\nImportance Weighted Actor-Learner Architecture (IM-\nPALA):\nIMPALA [30] uses an actor-critic technique, but with the\ntwist of decoupling actors. Following a V-trace off-policy\napproach, IMPALA’s main objective is to scale up the DRL\ntraining capability by adding multiple independent actors.\nThe actors, in this case, are meant to generate experience\ntrajectories for the policy to learn, while the learner tries to\noptimize not only the policy but also the value function.\nAn author in [19] proposes a multi-agent urban driving\nframework in which one can train more than one AC. Using\nIMPALA, the author performs training on connected AC\npolicy within the CARLA simulator.\nDeep Q Networks (DQN):\nDQN [31] falls into the category of value-based methods,\nwhere the goal is to learn the policy function by going through\nthe optimal action-value function. DQN uses off-policy and\ntemporal differences (TD) to learn state-action pairs by col-\nlecting episodes as experience replay. Using the episodic data,\nsamples from the replay memory are used in order to learn\nQ-values, as a neural network function approximation. DQN\nis one of the foundation models in the upbringing of DRL\nand there are many improved versions of DQN implemented\nby overcoming the existing ﬂaws.\nDQN is used in [32] for training an AC in a Unity-based\nurban driving simulator. The authors use cameras and lasers as\ntheir input sensors for training the driving policies. Another\nwork in [8] uses DQN for ﬁrst training a driving agent in\nsimulation to test its navigation capabilities in both simulated\nand real-world driving situations. DQN is also utilized in [33]\nfor learning to steer a vehicle within a realistic physics\nsimulator. Given the camera input feeds, their DQN-based\nagent is aiming for following lanes with minimal offroad\nsteering errors.\n2) Continuous Action Space:\nDeep Deterministic Policy Gradient (DDPG):\nWhen it comes to continuous action space, DDPG [34] is\nthe most widely used algorithm in DRL research. DDPG is\na model-free and off-policy approach falling under the actor-\ncritic algorithms. It extends DQN by learning a deterministic\npolicy in a continuous action space using actor-critic frame-\nwork.\nDPG is extensively used in the ﬁeld of training autonomous\nvehicles within simulated driving scenarions [9]. Authors\nin [10] uses DDPG to construct a DRL model for learning\nto avoid collisions and steering. DDPG is also used in [35]\nfor learning a lane following policy within driving simulation\nusing continuous action space.\nTwin Delayed DDPG (TD3):\nTD3 [36] extends the idea of DDPG in continuous action\nspace algorithms by tackling issues regarding overestimation\nof the Q-learning function found in the value-based and actor-\ncritic methods. This results in both improving the learning\nspeed and model performance of DDPG within a continuous\naction setting.\nTD3 is extensively used in urban driving scenarios for\ntraining AC agents. As a model-free approach, TD3 is used\nin [37] [38] for learning an AD policy within an urban\nsimulated environment. The authors proposed a framework for\nlearning complex driving scenarios using visual encoding to\ncapture low-level latent variables. Another work in [39] also\nuses TD3 in to overcome the challenges of driving in an urban\nsimulated environment.\nB. Multi-agent Autonomous Driving\nWhile introducing multi-agent AD agents, we need to\nconsider an environment where agents do not have access to\nall the states at each time step. Such types of environments are\nfound in the ﬁeld of robotics and ACs where an agent is limited\nto the sensory information gathered by its hardware. Therefore,\nthe existing MDP can be termed as a Partially Observable\nMarkov decision process (POMDP) [40]. Furthermore, the cur-\nrent formulation of POMDP can be reformulated as Partially\nObservable Stochastic Games (POSG) [41] by deﬁning a DRL\ncontrol problem as a tuple (I, S, A, O, P, R). In POSG, we can\nincorporate multi-agent scenarios using Markov Games [42]\nwhere multiple agents are interacting with the environment.\nAn actor i ϵ I receives its partial observations from a joint\nobservation state oi ϵ Oi at each time step t. Following the\ntraditional MDP approach, each actor uses its learned policy\nfunction πi : Oi 7→Ai to perform actions ai ϵ Ai. As a return,\neach actor gets a desired reward value ri ϵ Ri.\nIV. END-TO-END BENCHMARKING FRAMEWORK\nIn this section, we provide the details of the DRL bench-\nmarking framework architecture as well as an explanation of\nthe reward function, hyperparameters, and driving policies.\nThe implementation of the framework is available at 1.\nA. Driving Policies\nWe divide our driving policies into πAC, where πAC rep-\nresents the policy of AC driving agents that are trained using\none of the six selected DRL algorithms. Policies and their\nassociated algorithm symbols are mentioned in Table I. Their\nusage is thoroughly explained in Section V-B.\nTABLE I: Driving Policies and their associated symbols for\nACs.\nDRL Algorithm\nNotation for AC Policy\nPPO\nπP P O\nA3C\nπA3C\nIMPALA\nπIMP ALA\nDQN\nπDQN\nDDPG\nπDDP G\nTD3\nπT D3\nB. Deep Neural Network Model\nA pictorial description of the DRL benchmarking frame-\nwork can be seen in Figure 1. Each driving agent as AC\nreceives partial input state observation of 84x84x3 dimen-\nsion images through the front camera sensors. Cameras are\nmounted as part of the driving agents, and during each time\nstep of the simulation environment, cameras capture the input\nstate observations which serve as an input layer to the DRL\nmodel. The input layer is then passed to the convolutions and\nconnected to hidden layers for extracting important features\nbefore they are passed to the output layer of the architecture.\nACs driving policies predict the control actions at the output\nlayer based on the 3-dimensional input images at each time\nstep.\n1https://github.com/AizazSharif/Benchmarking-QRS-2022\nSince our six selected DRL models work on a discrete as\nwell as a continuous action space, the output layer of the\narchitecture predicts nine distinct action values for the discrete\naction space and two ﬂoat values for the continuous action\nspace policies. These output actions can be summed into three\nvehicle control commands: Steer, Throttle, and Brake. Reverse\nis disabled for our experiments.\nC. Reward functions\nAs described in Section III-B, each agent in a POMDP\nsetting is following MDP. Thus, the driving policies receive\nreward R at each time step of the simulated environment while\ncollecting trajectories of the tuple (S, R, A). R is the reward\nvalue returned while performing action A on the current state\nobservations S which helps in improving the driving policies\nπAC.\nOne of the contributions of this paper is the design of a\ndetailed reward function with multiple objectives for learning\nACs in a closed-loop multi-agent urban driving environments.\nFor our experiments, we deﬁne the reward function RAC used\nby the DRL AC policies πAC during the training phase.\nRAC can be formulated as:\nRAC =\n\n\n\n\n\n\n\n\n\n−50.0(CVt ∧COt ∧CPt)\nSafety\n+10(∆D + Ft)\nEfﬁciency\n−0.5(OSt)\nLane Keeping\n+φ\nPenalty Constant\nRAC aims to maximize the driving performance of an AC\nagent by keeping the following objectives in check:\nSafety: The ﬁrst objective of RAC includes CVt, COt, and\nCPt that represent a boolean value {0,1} for collision with\nanother vehicle, road objects, and pedestrian respectively. To\nensure safety, AC tries to avoid any type of collision based on\npast rewards.\nEfﬁciency: If driving efﬁciently, an AC agent gets positive\nrewards on minimizing the distance to goal state compared to\nthe previous timestep i.e. ∆D = Dt−1 −Dt and speed of the\ndriving agent as as Ft.\nLane Keeping: For ensuring lane keeping, OSt refers to\nthe boolean value represented by a boolean value {0,1} for\nminimizing offroad steering errors of the AC model.\nPenalty Constant: φ at the end of the reward function is a\npenalty constant used to encourage AC models to explore the\nenvironment and making decisions while driving. This helps in\nsituations where DRL agents get stuck and remain indecisive\nin making decisions, and therefore not able to move and learn.\nD. Hyperparameters\nThere are some common hyperparameter conﬁgurations\ncalled model hyperparameters used for deﬁning the training\nand testing of the driving DRL policies as described in\nSection IV-B. As for the algorithmic hyperparamters, we\nﬁrst select hyperparameters by going through the literature\non the available implementations for DRL in AD. Since\nwe are using six different DRL implementations for training\ntheir driving policies, each of the algorithms requires separate\nAutonomous Driving \nDeep RL Agents\nObservations\n1\nAction\n2\nSteer\nThrottle\nReward\n3\nMulti-agent Autonomous \nDriving Environment\nDiscrete Action\nSpace\nContinuous Action\nSpace\nDiscrete (9)\nBox (2)\nBrake\nFigure 1: End-to-end DRL benchmarking framework for AC agents. An agent receives an input observation image of 84x84x3\nwhich is passed to a DRL model. Actions are selected at the output layer of every agent and are performed in the next time\nstep of the simulation in order to obtain a reward and a new observation state.\nhyperparameter tuning. Therefore we take advantage of a\nhybrid hyperparameter tuning algorithm known as Population\nBased Training (PBT) [43] during the training of each DRL\nAC agent. Each DRL is started with an initial conﬁguration\nset of hyperparameters and with the help of PBT each DRL\nagent explores the possible conﬁgurations, leading to different\ntraining and episodic runs. The details of the hyperparameters\nfor all the AC agents are provided in the GitHub repository 2.\nThe extensive details of the key hyperparameters used in\nthe training phase of the ACs are shown in Table II. Based on\na DRL algorithm and its interaction with the environment, the\nnumber of episodes varies between 650 and 800 episodes, and\nsteps per training iteration also vary accordingly as shown in\nthe Table.\nV. COMPARATIVE STUDY\nIn this section, we present a comparative study of the\nperformance of six DRL algorithms for AD, including the\ndescription of the simulation frameworks for the training and\ntesting of AC agents.\nThe research questions in our work evaluate:\nRQ1: Which DRL-based ACs act better (or worse) in a\nsingle-agent as well as a multi-agent competitive scenario\nwhen trained in a multi-agent-only scenario?\nRQ2: Which DRL-based ACs are more successful in ful-\nﬁlling multiple driving objectives throughout different envi-\nronments?\nIn RQ1, we evaluate the driving performance of victim ACs\nusing six Driving Performance Metrics:\n• CV: the amount of collision with another driving vehicle\n• CO: the amount of collision with road objects\n• CP: the amount of collision with pedestrians\n• OS: offroad steering percentage from its driving lane\n• TTFC: the time it takes to have the ﬁrst collision\n• SPEED: the forward speed of the AC agent\n2https://github.com/AizazSharif/Benchmarking-QRS-2022\nFor CV , CO, CP, and OS we calculate the percentage of\nerror within each episode (as a value between 0 and 1). In\neach testing episode per environment, we run 5000 simulation\nsteps and at the end of 50 episodic test runs, we compute\nthe average error rate for each metric across all the episodes.\nTTFC on the other hand shows the time in seconds it takes\nto detect the ﬁrst collision in a testing episode. SPEED is\nvisualized to plot the average forward speed (in km/h) by each\nDRL-based ACs.\nWhereas in RQ2, we use the same Driving Performance\nMetrics to measure the success rate of DRL ACs with respect\nto Safety, Efﬁciency, and Lane Keeping across all environ-\nments.\n• Safety is calculated as an average among all collision\nbased metrics such as CV , CO, and CP.\n• Efﬁciency is computed by using SPEED and a seventh\nmetric DISTANCE.\n• Lane Keeping is measured as an average among offroad\nsteering metric values taken as OS.\nFor calculating DISTANCE, we deﬁne a goal state for\neach driving condition covered by each DRL policy during\ntesting. This is required since we can only rely on current\nepisodic results during testing. DISTANCE can be formu-\nlated as:\nDISTANCEt =\nq\n(xgoal −xspawn)2 + (ygoal −yspawn)2−\nq\n(xgoal −xcurrent)2 + (ygoal −ycurrent)2\nwhere it measures the difference between euclidean dis-\ntances from the goal to the spawn state versus the ending\nstates of the episode.\nWe analyze which DRL-based AC agent achieves each\nlearning objective as described in Section IV-C.\nTABLE II: Hyperparameters for the training of AC driving\nagents.\nAC Policy\nHyperparameters\nvalue\nTotal Training Episodes\n650\nEpochs per minibatch\n8\nValue Loss Coefﬁcient\n1.0\nEntropy Coefﬁcient\n0.01\nPPO\nTotal Training steps\n1331200\nLearning Rate\n0.001\nClipping\n0.3\nKL Initialization\n0.3\nLambda\n0.99\nKL Target\n0.03\nTotal Training Episodes\n600\nValue Loss Coefﬁcient\n1.0\nEntropy Coefﬁcient\n0.01\nA3C\nTotal Training steps\n1228800\nLearning Rate\n0.005\nLambda\n1.0\nGradient Clipping\n30.0\nSample Async\nTrue\nTotal Training Episodes\n800\nValue Loss Coefﬁcient\n0.5\nEntropy Coefﬁcient\n0.01\nIMPALA\nVtrace\nTrue\nVtrace Threshold\n1.0\nTotal Training steps\n1638400\nLearning Rate\n0.0001\nGradient Clipping\n30.0\nTotal Training Episodes\n800\nNo. of Atoms\n1\nNoisy Network\nFalse\nDueling DQN\nTrue\nN-step Q-learning\n1\nDQN\nInitial ϵ\n1.0\nFinal ϵ\n0.02\nReplay Buffer Size\n400\nEntropy Coefﬁcient\n0.01\nTotal Training steps\n1638400\nLearning Rate\n0.0001\nGradient Clipping\n40.0\nTotal Training Episodes\n800\nPolicy Delay\n1\nTarget Noise\n0.2\nSigma (Σ)\n0.2\nTau (τ)\n0.002\nN-step Q-learning\n1\nDDPG\nInitial Exploration Scale\n1.0\nReplay Buffer Size\n500\nCritics Learning Rate\n0.001\nActor Learning Rate\n0.001\nEntropy Coefﬁcient\n0.01\nTotal Training steps\n1638400\nLearning Rate\n0.0001\nTotal Training Episodes\n790\nPolicy Delay\n1\nTarget Noise\n0.2\nTau (τ)\n0.0002\nSigma (Σ)\n0.2\nN-step Q-learning\n1\nInitial Exploration Scale\n1.0\nTD3\nReplay Buffer Size\n500\nCritics Learning Rate\n0.001\nActor Learning Rate\n0.001\nEntropy Coefﬁcient\n0.01\nTotal Training steps\n1617920\nLearning Rate\n0.0001\nLambda\n0.005\nBatch Size\n128\nBatch Mode\nComplete Episodes\nEpsilon (ϵ)\n0.1\nCommon Parameters\nTraining Steps per episode\n2048\nOptimizer\nAdam\nDiscount Factor (γ)\n0.99\nA. Driving Environment\nTo train our AC agents in a partially observable urban\nenvironment we use Town03 map from Carla with Python\nAPI [44]. This environment is conﬁgured for training multi-\nagent ACs and testing the same ACs in a single and multi-\nagent scenario. The details of the training and testing con-\nﬁgurations for our experiments are mentioned in Section V-B.\nWe ﬁrst provide brief descriptions of the driving environments\ndisplayed in the Figure 2.\n1) Driving Environment 1 (env_1): We use Straight road\nfrom the Town03 map. The driving setting is simple yet\nsuitable especially for validating multi-agent AC policies in\norder to see their lane-keeping capability.\n2) Driving Environment 2 (env_2): We use Three Way\nintersection (also known as T-intersection) from the Town03\nmap. The driving setting is perfect for validating multi-agent\nAC policies in a scenario where multiple driving cars, as well\nas pedestrians, are faced.\n3) Driving Environment 3 (env_3): The environment has\nindependent non-communicating agents spawned close to the\nfour-way intersection throughout the testing scenarios. The\nchoice of a four-way intersection as a driving scenario is based\non its higher complexity for an AC agent. Similar to env_2,\nthis driving environment also contains pedestrians.\n4) Driving Environment 4 (env_4): The environment has\nindependent non-communicating agents spawned close to the\nRoundabout throughout the training and testing scenarios. The\nchoice of the roundabout as a driving scenario is based on its\nhigher complexity for an AC agent.\n5) Driving Environment 5 (env_5): The environment is\ncalled Merge where two roads merge to a single point. The\nchoice of merge as a driving scenario is also based on its\nhigher complexity for an AC agent.\nFigure 2: Illustration of Town03 Carla urban driving environ-\nment. Bottom middle highlighted area on the map represents\nenv_1 (Straight), top right highlighted area on the map rep-\nresents env_2 (Three Way intersection) and top middle high-\nlighted area displays env_3 (Four-way intersection).\nMiddle\nhighlighted area on the same map shows env_4 (Roundabout)\nwhile bottom left highlighted area represents env_5 (Merge).\nB. Experimental Setup\n1) Scenario 1: Training and testing of multi-agent ACs:\nFirst, we train AC policies πAC in a multi-agent driving\nenvironment. There are DRL-based, as well as a few auto-\ncontrolled cars driving around each of the AC agents rep-\nresenting human drivers from real-life scenarios. We train\nπAC policies using the reward function RAC described in\nSection IV-C. The performance of AC policies that are trained\nin the ﬁrst scenario is shown in Figure 3.\n10\n20\n30\n40\n50\n0\n2k\n4k\n6k\n8k\n10k\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nTraining Iterations\nMean Episodic Reward\nPPO\n10\n20\n30\n40\n50\n8k\n8.5k\n9k\n9.5k\n10k\n10.5k\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nTraining Iterations\nA3C\n10\n20\n30\n40\n50\n4k\n6k\n8k\n10k\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nTraining Iterations\nMean Episodic Reward\nIMPALA\n10\n20\n30\n40\n50\n0\n2k\n4k\n6k\n8k\n10k\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nTraining Iterations\nDQN\n10\n20\n30\n40\n50\n5k\n6k\n7k\n8k\n9k\n10k\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nTraining Iterations\nMean Episodic Reward\nDDPG\n10\n20\n30\n40\n50\n0\n5k\n10k\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nTraining Iterations\nTD3\nFigure 3: Convergence of DRL AC policies in Scenario 1.\nEach plot represents DRL algorithm trained in all 5 driving\nenvironments. The ﬁrst four plots shows mean episodic reward\nfor PPO, A3C, IMPALA, and DQN from a discrete action\nspace. The last two plots illustrate mean episodic reward for\nDDPG and TD3 algorithms from a continuous action space.\nNext, for testing and validating ACs in a multi-agent Sce-\nnario 1 all six DRL-based ACs drive next to each other, as well\nas around auto-controlled cars, therefore acting as independent\nnon-communicating competitive driving agents, illustrated in\nFigure 4 (a). The driving performance of each AC is evaluated\nagainst the six Driving Performance Metrics.\n2) Scenario 2: Testing of single-agent ACs: After training\nall six DRL-based ACs for a number of episodes in a multi-\nagent Scenario 1, we validate the driving performance of each\nAC policy in a single-agent driving environment, illustrated in\nFigure 4 (b). Testing in a single-agent scenario is important\nin order to validate the driving capability of AC policies in\nsimilar driving situations but with no cars around. Just like in\nScenario 1, ACs’ driving performance is validated based on\nthe six Driving Performance Metrics.\nBoth scenarios are illustrated in Figure 4.\n(a)\n(b)\nFigure 4: Illustration of two different scenarios for the testing\nphase. The left ﬁgure (a) shows a competitive multi-agent\ntesting environment for all of the ﬁve driving environments.\nenv_2 and env_3 additionally consists of pedestrians as well.\nRight ﬁgure (b) displays a single-agent driving scenario for\ntesting every DRL-based AC agent individually.\nC. Simulation Setup\nThe proposed benchmarking framework uses the following\nopen libraries/frameworks:\nCARLA [45] is an urban driving simulation framework\ndesigned for training and validating autonomous vehicles.\nCARLA is famous for its highly integrated Python API and\naccess to high-ﬁdelity urban driving environments. We use the\n0.9.4 version.\nRLlib [46] is a very ﬁne-tuned and scalable DRL framework\nlibrary. RLlib gives the opportunity to access more than one\nDRL policy graph and its hyperparameters for creating a non-\nshared multi-agent system. We use versions 0.8.0 for IMPALA\nand 0.8.5 for the rest of the 5 DRL algorithms.\nMacad-gym [47] is an open-source framework that connects\nCARLA, RLlib, and Open AI’s Gym toolkit [25]. We have\nmodiﬁed the framework by adding competitive multi-agent\ndriving functionalities required for our experiments. We use\nthe 0.1.4 version.\nTensorﬂow [48] is one of the leading frameworks used to\ncreate deep learning-based algorithms. We use version 2.2.0\nwithin the RLlib library.\nVI. RESULTS & ANALYSIS\nIn this section, we discuss the experimental results for\ntesting DRL AC policies. For collecting the results, we run\n50 testing episodes in both single and multi-agent scenarios\nto take an average among the Driving Performance Metrics\nexplained in Section V. In each episode, we use 5000 simula-\ntion steps in env_1 to env_5 in order to test the performance\nof every AC driving policy.\nA. RQ1: Testing AC policies in a single and multi-agent\nscenario\nWe ﬁrst look into the performance comparison among the\n4 discrete and 2 continuous action space DRL AC policies.\nEach of the AC agents is trained with a various number of\nepisodes and their model convergence performance has been\ndiscussed in Section V-B1. Now we use the same training\nenvironment and analyze their driving behavior in both single\nand multi-agent scenarios.\nThe evaluation results of the driving performance of DRL-\nbased AC agents are presented in Table III using ﬁve Driving\nPerformance Metrics: CV, CO, CP, OS, and TTFC. AC poli-\ncies having values closer to 0 are driving error-free, while\nthose near to 1 have a higher failure state. The results of\nthese ﬁve metrics should be analyzed jointly with the sixth\nmetric: SPEED. This is because a car could have no collision\nand offroad steering errors due to being stationary. Therefore,\nwe provide Figure 5, which shows an AC’s driving speed per\ntimestep in a testing simulation. For displaying all the episodic\nresults, we took an average of both Scenario 1 and 2 for every\nDRL AC policy across each driving environment setting. The\ndash sign (-) for the CV metric represents Scenario 2 where\nthe evaluation contains only single-agent driving scenarios.\nSimilarly, dash sign (-) for the CP metric also represents\ndriving scenarios and environments where pedestrians were\nnot involved. For TTFC, we use double dash sign (- -) that\nrepresents cases when there is no collision detected by the\ndriving policy.\n1) πP P O:\nπP P O based AC policy was able to avoid\ncollision with other vehicles except for some minimal collision\nstates in env_3. On the other hand, πP P O performed poorly\nin respect of road and pedestrian collisions throughout the\nmulti-agent driving scenarios. From simpler environments as\nenv_1 to much more complex driving conditions as env_4\nand env_5, we see many offroad steering errors made by the\nπP P O algorithm. We see similar behavior in the single-agent\nscenario, where it performs no offroad steering errors in only\nenv_3, while the agent collides with other road objects and\ndrives off-lane throughout the driving settings. PPO-based AC\npolicy also ends up having its ﬁrst collisions in early timesteps\nof the episodes, whether tested in multi-agent or single-agent\nscenarios. πP P O performance with respect to SPEED has\nbeen mostly consistent regardless of the collisions and offroad\nsteering states as stated above.\n2) πA3C: From Table III, it is clear that A3C algorithm-\nbased policy πA3C performs best with no collision with other\nvehicles in four out of ﬁve environments. It also performs with\nminimal road collisions and offroad steering errors across all\ndriving environments. The driving behavior remains similar in\nboth multi and single-agent testing scenarios. While testing\nagainst pedestrians within env_2 and env_3, it successfully\navoids any collision. πA3C only gets very minor states of\ncollision with other vehicles in env_3 multi-agent scenarios.\nπA3C policy while facing these minor collisions was detected\nin the ﬁrst half of the episodes, but the driving agent was\nable to recover from the failure state. The time to detect the\nﬁrst collision in such scenarios is also comparatively longer\nthan other driving policies as shown in the Table. In terms\nof SPEED, the A3C algorithm-based policy πA3C keeps\na decent driving speed throughout episodic timesteps. A3C\noverall performs consistently throughout the testing episodes\nwith both high and low-speed values.\n3) πIMP ALA: πIMP ALA can be observed in our exper-\niments as the weakest discrete action space algorithm. In a\nmulti-agent scenario, it sometimes avoids collisions with other\ndriving vehicles, but most of the time ends up colliding with\nother road objects and pedestrians with a large percentage\nof offroad steering errors. The driving behavior is slightly\nimproved when driving alone within env_4 and env_5. Other\nthan single-agent testing, πIMP ALA-based AC policy in env_4\nis also found in its ﬁrst collision state in the earlier timesteps of\nthe episodes. This can also be visualized in terms of SPEED,\nwhere the AC policy performs its best in env_4, while in the\nrest it drastically decreases its speed after early collision states.\n4) πDQN: πDQN overall performs second best within dis-\ncrete action space algorithms. In 4 out of 5 environments, the\nπDQN AC policy avoids vehicle collisions in the multi-agent\nscenario. However, it constantly drives offroad in multi-agent\nconditions and also collides a few times with pedestrians while\nfacing them in env_2 and env_3. The driving performance\nof πDQN improves a lot when driving alone in single-agent\nscenarios across all environments. It only faces road object\ncollision states in a difﬁcult and complex env_4 environment.\nThe driving policy even after getting its ﬁrst collision in the\nearly stage of the episodes recovers to a better driving state.\nThe SPEED factor is maintained quite well by πDQN AC\npolicy throughout the testing phase.\n5) πDDP G: In continuous action space algorithms, we ﬁrst\nevaluate the robustness of πDDP G across various driving\nconditions. The πDDP G policy performs the worst in overall\ndriving while following the road lane and therefore gets into\nthe majority of the offroad driving states whether it is a single\nor multi-agent scenario. It also collides with pedestrians while\ntesting in env_2 and env_3. The time to detect the ﬁrst collision\nis also earlier in four out of 5 driving environments whether it\nis a ﬁrst or second scenario. The driving SPEED of πDDP G\ncan be interpreted as similar to πIMP ALA where it slows down\nin early timesteps of the episodes due to collisions and offroad\nsteering errors.\n6) πT D3: In continuous action space algorithms, πT D3\non the other hand makes a lot better driving decisions as\ncompared to πDDP G. It successfully avoids collision with\nother vehicles within multi-agent scenario settings and also\ngets in no contact with pedestrians across env_2 and env_3\ndriving conditions. The πT D3 AC policy faces some minor\nroad collision and offroad steering states in multi-agent sce-\nnario testing. The driving behavior slightly drops when πT D3\nis tested in single-agent scenarios, especially in env_4 where\nboth offroad steering and road collision percentages increases.\nIn the case of environments where it had any collision, the time\nto detect the ﬁrst one is far greater than the ones detected in\nthe DDPG AC policy. The forward SPEED conditions of\nπT D3 is in general slower than πDDP G, and therefore able\nto drive mostly safer in comparison to πDDP G from start to\nend.\nIn summary, the experimental results of testing AC policies\nin a single and multi-agent scenario indicate that the A3C and\nTABLE III: Comparison of the behavior of AC driving agents in terms of CV, CO, CP, OS error percentages, and TTFC when\ntested both in a multi (Scenario 1) and single-agent (Scenario 2) environment.\nScenario 1\nScenario 2\nEnvironment\nMetric\nπP P O\nπA3C\nπIMP ALA\nπDQN\nπDDP G\nπT D3\nπP P O\nπA3C\nπIMP ALA\nπDQN\nπDDP G\nπT D3\nenv_1 (Straight)\nCV\n0.0\n0.0\n0.0\n0.0\n0.19\n0.0\n-\n-\n-\n-\n-\n-\nCO\n0.76\n0.0\n0.97\n0.0\n0.19\n0.1\n0.2\n0.0\n0.9\n0.0\n0.05\n0.05\nCP\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOS\n0.9\n0.0\n0.96\n0.7\n0.9\n0.2\n0.2\n0.1\n0.95\n0.1\n0.5\n0.0\nTTFC (seconds)\n268\n- -\n53.6\n603\n67\n335\n402\n- -\n120.6\n- -\n60.3\n616.4\nenv_2 (Three Way)\nCV\n0.0\n0.0\n0.0\n0.17\n0.3\n0.0\n-\n-\n-\n-\n-\n-\nCO\n0.9\n0.1\n0.0\n0.0\n0.18\n0.05\n0.2\n0.006\n0.19\n0.0\n0.0\n0.16\nCP\n0.0\n0.0\n0.2\n0.05\n0.2\n0. 0\n-\n-\n-\n-\n-\n-\nOS\n0.2\n0.1\n0.6\n0.3\n0.9\n0.0\n0.0\n0.0\n0.2\n0.0\n0.19\n0.39\nTTFC (seconds)\n241.2\n636.6\n268\n314.9\n53.6\n284.08\n281.4\n670\n281.4\n- -\n62\n263\nenv_3 (Four Way)\nCV\n0.1\n0.1\n0.1\n0.0\n0.0\n0.0\n-\n-\n-\n-\n-\n-\nCO\n0.0\n0.0\n0.7\n0.26\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.0\n0.1\nCP\n0.1\n0.0\n0.1\n0.05\n0.1\n0.0\n-\n-\n-\n-\n-\n-\nOS\n0.8\n0.1\n0.1\n0.3\n0.95\n0.0\n0.21\n0.0\n0.19\n0.49\n0.1\n0.17\nTTFC (seconds)\n564.14\n611.04\n42.88\n335\n248.302\n- -\n- -\n- -\n49\n- -\n- -\n308.2\nenv_4 (Roundabout)\nCV\n0.0\n0.0\n0.5\n0.0\n0.0\n0.0\n-\n-\n-\n-\n-\n-\nCO\n0.8\n0.2\n0.8\n0.0\n0.9\n0.0\n0.4\n0.0\n0.05\n0.5\n0.7\n0.59\nCP\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOS\n0.6\n0.3\n0.3\n0.75\n0.89\n0.1\n0.8\n0.08\n0.6\n0.64\n0.94\n0.55\nTTFC (seconds)\n214.4\n663.7\n268.89\n- -\n53.6\n- -\n335.1\n- -\n660\n26.8\n51\n120.6\nenv_5 (Merge)\nCV\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-\n-\n-\n-\n-\n-\nCO\n0.9\n0.0\n0.0\n0.0\n0.0\n0.05\n0.2\n0.0\n0.0\n0.0\n0.34\n0.05\nCP\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOS\n0.5\n0.1\n0.4\n0.8\n0.98\n0.6\n0.8\n0.0\n0.06\n0.1\n0.96\n0.21\nTTFC (seconds)\n323.6\n- -\n- -\n- -\n- -\n634.22\n536\n- -\n- -\n- -\n33.5\n650\n0\n2000\n4000\nSteps\n0\n10\n20\nSpeed (km/h)\nPPO env_1\n0\n2000\n4000\nSteps\n0\n5\n10\nA3C env_1\n0\n2000\n4000\nSteps\n0\n10\n20\n30\nIMPALA env_1\n0\n2000\n4000\nSteps\n0\n5\n10\nDQN env_1\n0\n2000\n4000\nSteps\n0\n10\n20\nDDPG env_1\n0\n2000\n4000\nSteps\n0\n2\nTD3 env_1\n0\n2000\n4000\nSteps\n0\n5\n10\nSpeed (km/h)\nPPO env_2\n0\n2000\n4000\nSteps\n0\n5\n10\nA3C env_2\n0\n2000\n4000\nSteps\n0\n2\n4\nIMPALA env_2\n0\n2000\n4000\nSteps\n0\n2\nDQN env_2\n0\n2000\n4000\nSteps\n0\n10\nDDPG env_2\n0\n2000\n4000\nSteps\n0\n2\n4\nTD3 env_2\n0\n2000\n4000\nSteps\n0\n5\n10\nSpeed (km/h)\nPPO env_3\n0\n2000\n4000\nSteps\n0\n5\n10\n15\nA3C env_3\n0\n2000\n4000\nSteps\n0\n2\n4\n6\nIMPALA env_3\n0\n2000\n4000\nSteps\n0\n5\nDQN env_3\n0\n2000\n4000\nSteps\n0\n5\n10\n15\nDDPG env_3\n0\n2000\n4000\nSteps\n0.0\n2.5\n5.0\n7.5\nTD3 env_3\n0\n2000\n4000\nSteps\n0\n5\n10\nSpeed (km/h)\nPPO env_4\n0\n2000\n4000\nSteps\n0\n5\n10\nA3C env_4\n0\n2000\n4000\nSteps\n0\n2\n4\nIMPALA env_4\n0\n2000\n4000\nSteps\n0\n10\nDQN env_4\n0\n2000\n4000\nSteps\n0\n10\n20\nDDPG env_4\n0\n2000\n4000\nSteps\n0\n1\n2\n3\nTD3 env_4\n0\n2000\n4000\nSteps\n0\n5\n10\nSpeed (km/h)\nPPO env_5\n0\n2000\n4000\nSteps\n0\n2\n4\nA3C env_5\n0\n2000\n4000\nSteps\n0\n5\n10\n15\nIMPALA env_5\n0\n2000\n4000\nSteps\n0\n5\n10\nDQN env_5\n0\n2000\n4000\nSteps\n0\n2\n4\n6\nDDPG env_5\n0\n2000\n4000\nSteps\n0\n2\n4\nTD3 env_5\nFigure 5: Comparison of the behavior of AC driving agents in terms of forward speed when tested both in a single and\nmulti-agent scenario. Each row represents an average driving speed of DRL AC policies in the driving environments labeled\nfrom env_1 to env_5.\nTD3-based driving agents perform better than the rest of the\nDRL agents in both single and multi-agent scenarios within\nﬁve driving environment settings, answering RQ1.\nB. RQ2: Success rate of AC driving policies in achieving\nmultiple driving objectives\n1) Safety: By visualizing the results in Figure 6, we can see\nthe average success rate of each DRL algorithm in achieving\nsafety across all driving environments. A value closer to zero is\nconsidered much safer than the ones closer to one displayed in\nthe ﬁgure. As thoroughly discussed in Section VI-A we can\nclearly see that πA3C overall performs the best in terms of\navoiding collisions with drivers, pedestrians, and other road\nobjects. πDQN performs similarly to πA3C within env_1 and\nenv_5 but performs less safely in the rest of the 3 environ-\nments. πP P O other than env_3 has a mediocre success rate in\nterms of safety, whereas πIMP ALA other than env_5 performs\nthe worst overall. In continuous action space, πT D3 shows\nthe overall safest driving behavior across all environmental\nconditions as compared to πDDP G.\nPPO\nA3C\nIMPALA\nDQN\nDDPG\nTD3\n0\n0.2\n0.4\n0.6\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nSafety\nFigure 6: Illustration of the success rate within DRL AC\npolicies with respect to safety. Each algorithm in discrete\nand continuous action space is evaluated across all driving\nenvironments.\n2) Efﬁciency: When it comes to the success rate for mea-\nsuring efﬁciency, we need to analyze the results with respect to\nboth the driving SPEED and DISTANCE covered for each\nDRL AC policy. Figure 7 displays the box plot of minimum,\nmaximum, and median speed values throughout the testing\nphase. In discrete action space, πA3C maintains consistent\nspeed across all environments, whereas in continuous space\nalgorithms, πT D3 is much slower than πDDP G.\nTo fully understand the efﬁciency, we also visualize Figure 8\nto measure the average distance covered by each DRL AC\npolicy, since speed does not alone guarantee the efﬁciency of\nAC policies. πA3C overall covers the majority of the distance\nacross driving conditions. A similar pattern can be observed\nfor πT D3 in terms of driving more distance than πDDP G by\nalso maintaining a slower speed as discussed above.\n3) Lane Keeping: As described in Section IV-C, one of\nthe objectives of DRL AC policies in this experiment is to\nalso focus on lane-keeping behavior. A safer car is much\nmore likely to drive within a lane with minimum offroad\nsteering errors, but these are two different goals that separately\nneed to be achieved. In terms of evaluating success rate,\nπA3C once again performs the best. Other than env_4 it\nlearns to fully drive in its lane throughout the experiments.\nπIMP ALA remains inconsistent with its lane-keeping decisions\nPPO\nA3C\nIMPALA\nDQN\nDDPG\nTD3\n0\n10\n20\n30\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nEfficiency (Speed)\nFigure 7: Illustration of the success rate within DRL AC\npolicies with respect to efﬁciency in speed. Each algorithm\nin discrete and continuous action space is evaluated across all\ndriving environments.\nPPO\nA3C\nIMPALA\nDQN\nDDPG\nTD3\n0\n20\n40\n60\n80\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nEfficiency (Distance)\nFigure 8: Illustration of the success rate within DRL AC\npolicies with respect to efﬁciency in covering distance. Each\nalgorithm in discrete and continuous action space is evaluated\nacross all driving environments.\nsince it performs the worst in simplistic environments such\nas env_1 but performs better in complex conditions such\nas env_4. πP P O and πDQN are somewhat similar in lane-\nkeeping decision-making where both do not perform well in\nenv_4. When coming towards continuous action space, πT D3\nperforms better in terms of driving within its lane in all of the\nenvironments compared to πDDP G by a large margin.\nPPO\nA3C\nIMPALA\nDQN\nDDPG\nTD3\n0\n0.2\n0.4\n0.6\n0.8\n1\nStraight\nThree Way\nFour Way\nRoundabout\nMerge\nLane Keeping\nFigure 9: Illustration of the success rate within DRL AC\npolicies with respect to lane keeping. Each algorithm in\ndiscrete and continuous action space is evaluated across all\ndriving environments\nIn summary, the analysis of the success rate of testing\nAC policies in RQ2 in all of the environments indicates that\nthe A3C and TD3-based driving agents perform safe driving\ndecisions, cover the majority of the distance, and perform\nminimal offroad steering errors. The analysis also shows that\nπA3C in discrete action space learns to drive with consistent\nspeed while πT D3 tends to keep lower speed as a trade-off in\norder to make better driving decisions.\nVII. DISCUSSION AND OPEN RESEARCH DIRECTIONS\nThis section provides our observations and thoughts on\nthe effectiveness and robustness of the six evaluated DRL\nalgorithms for their use in AD research, speciﬁcally, as AC\nagents used for testing the driving agents.\nPPO as AC, in general, did not perform better than the\nrest of the algorithms while driving in a single or multi-\nagent scenario. Standard PPO policies are often stuck at non-\noptimal actions while learning since they are very sensitive\nto sparse high rewards. However, the PPO-based AC policy\ndrove efﬁciently with a good driving speed and covered a lot\nof distance. This shows that PPO-based policies need to focus\nmore on safety and lane-keeping driving behaviors as per our\nexperiments.\nA3C as AC performs the best in both single-agent and\nmulti-agent environments. With its default implementation, the\ncritic model seems to learn the value function well while actors\nare independently learning their driving policies in parallel.\nA3C seems to work well where training is performed on\ninput camera images [27] [7] [29] as in our case. One of the\nfuture directions is to investigate how A3C-based policy would\nperform if it tries to drive with more speed than the rest of\ndiscrete action space policies.\nIMPALA has shown great results when used before in the\nsame simulation environment [47], but the main difference\nin that work is that the authors implemented the IMPALA\nalgorithm for training multi-agent ACs using shared connected\nAC weights. In our work, we train a non-shared multi-agent\nAC which shows to not only fail in single-agent testing but also\nin a multi-agent competitive scenario. The algorithm is unable\nto effectively use the trajectories gathered from actors that\nare decoupled from the learner. IMPALA algorithm has only\nbeen successful in driving in complex driving conditions such\nas roundabouts while also performing fewer offroad steering\nmistakes. This shows that the algorithm has the potential to\nfurther tune the driving capability in complex urban driving\nscenarios.\nDefault DQN is the most used algorithm within the DRL\ncommunity and even in AD research, even though training\nACs using DQN in our case did not achieve good results as\ncompared to A3C. A lot of extensions have been proposed\nsince the popularity of DQN in atari games. Therefore, one\nprosperous research direction is to explore the continuous\naction space as in DDPG [49].\nDDPG is widely used for training AC policies [9] [10] [35],\nand we show that the improved DRL model TD3 performs\nbetter during single and multi-agent testing scenarios. The\nalgorithmic advantages of the TD3 policy over DDPG really\nhelp in learning a multi-agent driving policy. At the same\ntime, TD3 maintained a very low speed in our experiments\nto perform safe and efﬁcient driving. Just like A3C in discrete\naction space, TD3 in continuous action space needs to be\nfurther looked into with respect to higher speed and analyze\nthe safety and lane keeping performing in parallel.\nVIII. THREATS TO VALIDITY\nNon stationary multi-agent driving environments. In\nmulti-agent non-stationary environments, each agent’s transi-\ntion probability and reward function depends on the actions\nof all the agents since they change every time with the\nactions performed by the agents. DRL research for AD is\nmainly focused on driving in a single-agent stationary MDP\nenvironment. Driving behavior is affected a lot when tested\nin a multi-agent scenario due to the non-stationary driving\nenvironment [50]. This is one of the key threats to the existing\nDRL-based AD research that is performed only in a single-\nagent scenario. As explained in Section VI-A, the driving\nperformance of AC agents is affected signiﬁcantly when they\nare exposed to more AC agents in a multi-agent setting.\nChoice of DRL algorithms. We have selected a total of six\nDRL algorithms to test the robustness of AC driving agents as\ndescribed in Section III-A. While there could be other suitable\nDRL algorithms, we have selected ours to cover a range\nof DRL categories, including value-based, policy-based, and\nactor-critic-based. Furthermore, our benchmarking framework\nrelies on the Ray RLlib framework [46] which makes it\npossible to implement a competitive DRL-based multi-agent\ndriving environment. At the moment, the choice of DRL\nalgorithms also partly depends on which DRL algorithms work\nsmoothly while integrated with Ray RLlib, Tensorﬂow [48],\nand CARLA-speciﬁc versions [45].\nHyperparameters tuning. Hyperparameter tuning is con-\nsidered a very sensitive part of training DRL policies, and to\nmitigate this threat, we have used the best hyperparameters\nreported in the past implementations in literature as a starting\npoint, while also utilizing a hybrid hyperparameter tuning\nalgorithm to search for the most optimum hyperparameters.\nExperimenting with hyperparameter tuning can be tested fur-\nther in future AD research.\nIX. CONCLUSION\nIn this work, we compare the robustness of DRL-based\npolicies while training ACs agents. By ﬁrst training DRL AC\npolicies in a multi-agent environment, we test their driving\nperformance in both single and multi-agent scenarios. We\nanalyze the robustness of AC policies using six evaluation\nmetrics within both multi and single-agent scenarios. We\nalso propose a comprehensive multi-objective reward function\ndesigned for the evaluation of DRL AC driving agents. Based\non our proposed reward function, we analyze the overall\nsuccess rate in achieving safety, efﬁciency, and lane keeping\nfor each DRL policy as described in Section VI.\nREFERENCES\n[1] J. Garcia, Y. Feng, J. Shen, S. Almanee, Y. Xia, Chen, and Q. Alfred,\n“A comprehensive study of autonomous vehicle bugs,” in Proceedings of\nthe ACM/IEEE 42nd International Conference on Software Engineering,\n2020.\n[2] V. Riccio, G. Jahangirova, and A. e. a. Stocco, “Testing machine learning\nbased systems: a systematic mapping,” Empir Software Eng, p. 5193,\n2020.\n[3] D. Marijan and A. Gotlieb, “Software testing for machine learning,”\nProceedings\nof\nthe\nAAAI\nConference\non\nArtiﬁcial\nIntelligence,\nvol. 34, no. 09, pp. 13 576–13 582, Apr. 2020. [Online]. Available:\nhttps://ojs.aaai.org/index.php/AAAI/article/view/7084\n[4] D. Silver, “Ai day showcases the breadth of tesla’s ambition,” in Forbes,\n2021.\n[Online].\nAvailable:\nhttps://www.forbes.com/sites/davidsilver/\n2021/08/20/ai-day-showcases-the-breadth-of-teslas-ambition/\n[5] H. Schäfer, “End-to-end lateral planning,” in comma.ai, 2021.\n[6] B. Osi´nski, A. Jakubowski, P. Zi˛ecina, P. Miło´s, C. Galias, S. Homo-\nceanu, and H. Michalewski, “Simulation-based reinforcement learning\nfor real-world autonomous driving,” in IEEE International Conference\non Robotics and Automation (ICRA), 2020.\n[7] N. Xu, B. Tan, and B. Kong, “Autonomous driving in reality\nwith reinforcement learning and image translation,” arXiv preprint\narXiv:1801.05299, 2018.\n[8] P. Almási, R. Moni, and B. Gyires-Tóth, “Robust reinforcement\nlearning-based autonomous driving agent for simulation and real world,”\nin 2020 International Joint Conference on Neural Networks (IJCNN).\nIEEE, 2020, pp. 1–8.\n[9] S. Wang, D. Jia, and X. Weng, “Deep reinforcement learning for\nautonomous driving,” arXiv preprint arXiv:1811.11329, 2018.\n[10] H. Porav and P. Newman, “Imminent collision mitigation with rein-\nforcement learning and vision,” in 21st International Conference on\nIntelligent Transportation Systems (ITSC), 2018.\n[11] M. e. a. Zhou, “Smarts: An open-source scalable multi-agent rl training\nschool for autonomous driving.”\nPMLR, 2021.\n[12] W. Schwarting, A. Pierson, J. Alonso-Mora, S. Karaman, and D. Rus,\n“Social behavior for autonomous vehicles,” Proceedings of the National\nAcademy of Sciences, 2019.\n[13] K. Jang, E. Vinitsky, B. Chalaki, B. Remer, L. Beaver, A. A. Ma-\nlikopoulos, and A. Bayen, “Simulation to scaled city: Zero-shot policy\ntransfer for trafﬁc control via autonomous vehicles,” in 10th ACM/IEEE\nInternational Conference on Cyber-Physical Systems (ICCPS), 2019.\n[14] M. Stang, D. Grimm, M. Gaiser, and E. Sax, “Evaluation of deep\nreinforcement learning algorithms for autonomous driving,” in 2020\nIEEE Intelligent Vehicles Symposium (IV), 2020, pp. 1576–1582.\n[15] E. Vinitsky, A. Kreidieh, L. L. Flem, N. Kheterpal, K. Jang, F. Wu,\nR. Liaw, E. Liang, and A. M. Bayen, “Benchmarks for reinforcement\nlearning in mixed-autonomy trafﬁc,” in CoRL, 2018.\n[16] Z. Huang, J. Wu, and C. Lv, “Efﬁcient deep reinforcement learning with\nimitative expert priors for autonomous driving,” IEEE Transactions on\nNeural Networks and Learning Systems, 2022.\n[17] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement\nlearning for urban autonomous driving,” in 2019 IEEE intelligent\ntransportation systems conference (ITSC). IEEE, 2019, pp. 2765–2771.\n[18] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive:\nComposing diverse driving scenarios for generalizable reinforcement\nlearning,” 2021. [Online]. Available: https://arxiv.org/abs/2109.12674\n[19] P. Palanisamy, “Multi-agent connected autonomous driving using deep\nreinforcement learning,” in 2020 International Joint Conference on\nNeural Networks (IJCNN), 2020.\n[20] M. O’Kelly, H. Zheng, D. Karthik, and R. Mangharam, “F1tenth:\nAn open-source evaluation environment for continuous control and\nreinforcement learning,” in Proceedings of the NeurIPS 2019 Compe-\ntition and Demonstration Track, ser. Proceedings of Machine Learning\nResearch, H. J. Escalante and R. Hadsell, Eds., vol. 123. PMLR, 08–14\nDec 2020, pp. 77–89.\n[21] I. Han, D.-H. Park, and K.-J. Kim, “A new open-source off-road en-\nvironment for benchmark generalization of autonomous driving,” IEEE\nAccess, vol. 9, pp. 136 071–136 082, 2021.\n[22] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n“Proximal policy optimization algorithms,” 2017.\n[23] M. Holen, R. Saha, M. Goodwin, C. W. Omlin, and K. E. Sandsmark,\n“Road detection for reinforcement learning based autonomous car,” in\nProceedings of the The 3rd International Conference on Information\nScience and System (ICISS).\nAssociation for Computing Machinery,\n2020.\n[24] “A self-driving car simulator built with unity,” in Udacity, 2017.\n[Online]. Available: https://github.com/udacity/self-driving-car-sim\n[25] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in Proceedings of The 33rd International Con-\nference on Machine Learning (PMLR), 2016.\n[27] Y. You, X. Pan, Z. Wang, and C. Lu, “Virtual to real reinforcement\nlearning for autonomous driving,” 2017.\n[28] “Torcs, the open racing car simulator.” 2007. [Online]. Available:\nhttps://sourceforge.net/projects/torcs/\n[29] M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, and F. Nashashibi,\n“End-to-end race driving with deep reinforcement learning,” in IEEE\nInternational Conference on Robotics and Automation (ICRA), 2018.\n[30] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward,\nY. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu,\n“IMPALA: Scalable distributed deep-RL with importance weighted\nactor-learner architectures,” in Proceedings of the 35th International\nConference on Machine Learning (PMLR), 2018.\n[31] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. A. Riedmiller, “Playing atari with deep reinforcement\nlearning,” 2013.\n[32] A. R. Fayjie, S. Hossain, D. Oualid, and D.-J. Lee, “Driverless car:\nAutonomous driving using deep reinforcement learning in urban envi-\nronment,” in 15th International Conference on Ubiquitous Robots (UR),\n2018.\n[33] P. Wolf, C. Hubschneider, M. Weber, A. Bauer, J. Härtl, F. Dürr, and\nJ. M. Zöllner, “Learning how to drive in a real world simulation with\ndeep q-networks,” in IEEE Intelligent Vehicles Symposium (IV), 2017.\n[34] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” 2015.\n[35] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-\nD. Lam, A. Bewley, and A. Shah, “Learning to drive in a day,” in\nInternational Conference on Robotics and Automation (ICRA), 2019.\n[36] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approx-\nimation error in actor-critic methods,” 2018.\n[37] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement\nlearning for urban autonomous driving,” in 2019 IEEE Intelligent\nTransportation Systems Conference (ITSC), 2019.\n[38] J. Chen, S. E. Li, and M. Tomizuka, “Interpretable end-to-end urban\nautonomous driving with latent deep reinforcement learning,” IEEE\nTransactions on Intelligent Transportation Systems, 2021.\n[39] H. Liu, Z. Huang, and C. Lv, “Improved deep reinforcement learning\nwith expert demonstrations for urban autonomous driving,” 2021.\n[40] F. A. Oliehoek, “Decentralized pomdps,” in Springer Berlin Heidelberg,\n2012.\n[41] R. Emery-Montemerlo, S. Thrun, G. Gordon, and J. Schneider, “Approx-\nimate solutions for partially observable stochastic games with common\npayoffs,” in International Joint Conference on Autonomous Agents and\nMultiagent Systems.\nIEEE Computer Society, 2004.\n[42] M. L. Littman, “Markov games as a framework for multi-agent rein-\nforcement learning,” in 11th International Conference on International\nConference on Machine Learning, 1994.\n[43] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue,\nA. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fernando,\nand K. Kavukcuoglu, “Population based training of neural networks,”\n2017. [Online]. Available: https://arxiv.org/abs/1711.09846\n[44] “3rd maps and navigation,” 2021. [Online]. Available: https://carla.\nreadthedocs.io/en/latest/core_map/\n[45] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,\n“CARLA: An open urban driving simulator,” in 1st Annual Conference\non Robot Learning, 2017.\n[46] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang,\nW. Paul, M. I. Jordan, and I. Stoica, “Ray: A distributed framework\nfor emerging AI applications,” 2017.\n[47] P. Palanisamy, “Multi-agent connected autonomous driving using deep\nreinforcement learning,” in International Joint Conference on Neural\nNetworks (IJCNN), 2020.\n[48] “TensorFlow: Large-scale machine learning on heterogeneous systems,”\n2015. [Online]. Available: https://www.tensorﬂow.org/\n[49] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. M. O. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” CoRR, vol. abs/1509.02971, 2016.\n[50] G. Papoudakis, F. Christianos, A. Rahman, and S. V. Albrecht, “Dealing\nwith non-stationarity in multi-agent deep reinforcement learning,” 2019.\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.MA"
  ],
  "published": "2021-12-22",
  "updated": "2023-03-23"
}