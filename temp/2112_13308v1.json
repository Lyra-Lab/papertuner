{
  "id": "http://arxiv.org/abs/2112.13308v1",
  "title": "Unsupervised Clustering Active Learning for Person Re-identification",
  "authors": [
    "Wenjing Gao",
    "Minxian Li"
  ],
  "abstract": "Supervised person re-identification (re-id) approaches require a large amount\nof pairwise manual labeled data, which is not applicable in most real-world\nscenarios for re-id deployment. On the other hand, unsupervised re-id methods\nrely on unlabeled data to train models but performs poorly compared with\nsupervised re-id methods. In this work, we aim to combine unsupervised re-id\nlearning with a small number of human annotations to achieve a competitive\nperformance. Towards this goal, we present a Unsupervised Clustering Active\nLearning (UCAL) re-id deep learning approach. It is capable of incrementally\ndiscovering the representative centroid-pairs and requiring human annotate\nthem. These few labeled representative pairwise data can improve the\nunsupervised representation learning model with other large amounts of\nunlabeled data. More importantly, because the representative centroid-pairs are\nselected for annotation, UCAL can work with very low-cost human effort.\nExtensive experiments demonstrate the superiority of the proposed model over\nstate-of-the-art active learning methods on three re-id benchmark datasets.",
  "text": "STUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n1\nUnsupervised Clustering Active Learning for\nPerson Re-identiﬁcation\nWenjing Gao\nwenjinggao@njust.edu.cn\nMinxian Li*\nminxianli@njust.edu.cn\nSchool of Computer Science and\nEngineering\nNanjing University of Science and Tech-\nnology\nAbstract\nSupervised person re-identiﬁcation (re-id) approaches require a large amount of pair-\nwise manual labeled data, which is not applicable in most real-world scenarios for re-id\ndeployment. On the other hand, unsupervised re-id methods rely on unlabeled data to\ntrain models but performs poorly compared with supervised re-id methods. In this work,\nwe aim to combine unsupervised re-id learning with a small number of human annota-\ntions to achieve a competitive performance. Towards this goal, we present a Unsuper-\nvised Clustering Active Learning (UCAL) re-id deep learning approach. It is capable of\nincrementally discovering the representative centroid-pairs and requiring human anno-\ntate them. These few labeled representative pairwise data can improve the unsupervised\nrepresentation learning model with other large amounts of unlabeled data. More impor-\ntantly, because the representative centroid-pairs are selected for annotation, UCAL can\nwork with very low-cost human effort. Extensive experiments demonstrate the superi-\nority of the proposed model over state-of-the-art active learning methods on three re-id\nbenchmark datasets.\n1\nIntroduction\nIn recent years, person re-identiﬁcation (re-id) attracted lots of research attentions because\nof its practical applications on public security and smart city [11, 19, 29, 45]. Person re-\nidentiﬁcation aims to recognize the same identity of person across non-overlapped cameras,\nwhich is a challenging task in computer vision. Because of the non-overlap of ID labels\nbetween training and test set, re-id methods aims to learn a discriminative feature represen-\ntation model for each person image. Despite promising results reported in previous works,\nre-id relies heavily the acquisition of ID labels for each person image. Unlike the labelling\nprocess for general categories which only requires each image to be labeled, the acquisition\nof ID labels need to annotate all pairs of person images, which costs huge human effort.\nSupervised re-id methods can achieve encouraging performances but require a large num-\nber of manually labeled identity matching image pairs. However, the manual labeling for all\nperson image pairs is a tedious and expensive process in re-id task. The cost of human label-\ning effort increases tremendously with the size of camera network and the number of persons\nin each camera. One the other hand, unsupervised re-id methods can be trained by abundant\n∗Corresponding author, Email: minxianli@njust.edu.cn\nThis work is supported by National Natural Science Foundation of China (Project No. 62076132), and Natural\nScience Foundation of Jiangsu (Project No. BK20211194).\narXiv:2112.13308v1  [cs.CV]  26 Dec 2021\n2\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\nunlabeled person images but signiﬁcantly inferior in re-id accuracy. Without cross-view pair-\nwise ID labels, the re-id model is not able to learn the discriminative feature representation\nfor the signiﬁcant appearance change across cameras.\nTo save human labeling effort, active learning aims to select a subset of samples that\nare the most representative for labeling, and then use them to train the model in order to\nachieve a competitive performance compared with fully supervised models. Specially, un-\nsupervised active learning (UAL) [16] aims to select representative samples from totally no\nlabeled samples. It is also called early active learning (EAL) [27]. UAL/EAL re-id meth-\nods [22, 23, 25, 31, 34] assume that no labeled image pairs are available in training set in\nthe beginning. Pairwise constraint minimization [22], triangle-free subgraph maximization\n[31], and reinforcement learning [25] techniques are adopted to select a subset of pairwise\nsamples for querying labeling. After selecting a small number of representative data for la-\nbeling, existing unsupervised active learning use these pair annotation to train re-id model\nwith reliable supervised models. However, all the above active learning re-id methods train\nthe re-id model only from the selected image pairs, losing sight of the rest unlabeled sam-\nples. Actually, the unlabeled data also beneﬁt to re-id model to learn discriminative feature\nrepresentation by exploring the association from the similar samples.\nTo overcome the above limitations, we consider a unsupervised clustering based active\nlearning re-id framework. Unsupervised clustering based deep model can offer a reliable\ndata structure by generating pseudo labels without any human annotation. Based on un-\nsupervised clustering model, unsupervised active learning aims to select the representative\nsample pairs to reorganize the cluster structure. The advantages are two-fold: (1) Based on\nthe clustering algorithm, unsupervised active learning method can easily and efﬁciently ﬁnd\nthe most representative sample pairs in the global space. (2) Depending on the representative\npairs selection, the reorganization of the cluster structure can help the feature representation\nlearning in a more effective way.\nIn this work, the main contributions are as follows: (1) We formulated an Unsupervised\nClustering Active Learning (UCAL) model for person re-identiﬁcation. This model com-\nbines jointly both unsupervised learning and active learning principles in an integrated learn-\ning framework. To the best of our knowledge, this is the ﬁrst attempt at active learning with\nunsupervised learning person re-id model. (2) We propose an effective active learning strat-\negy by the means of selecting the representative centroid-pairs from unsupervised clustering\nstructure. (3) We present a clustering reorganization method (i.e. splitting/merging) to max-\nimize the effect of active learning, so it takes the low-cost human labeling labor.\nExtensive comparative experiments demonstrate the advantages of UCAL over the state-\nof-the-art active learning re-id approaches on three popular benchmarks: Market-1501[45],\nDukeMTMC-ReID[29, 46], and MSMT17[36].\nEspecially, the proposed UCAL model\nachieves the best performance with the lowest annotation cost.\n2\nRelated Work\nUnsupervised Learning in Re-ID. According to whether auxiliary data is used in training\nstage, unsupervised learning methods can be devided into two groups: (1) Pure unsuper-\nvsed learning. Most existing unsupervised learning re-id methods [2, 20, 21, 33, 42] adopt\nclustering-based approaches to produce pseudo labels, and then update the feature repre-\nsentation model by these pseudo labels. The challenge is how to obtain the precise cluster\nstructure and how to alleviate the negative impact by noisy pseudo labels. (2) Unsupervised\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n3\ndomain adaptation (UDA). UDA approaches aim to transfer the learned knowledge from a\nlabeled source domain to an unlabeled target domain. The methods can be classiﬁed into\nthree groups: Source domain pre-trained methods [7, 8, 40, 43, 44], Image-synthesis based\nmethods [3, 5, 14, 32, 36], and Joint-learning based methods [9, 10, 28, 47, 48]. Generally,\nthe performance of unsupervised learning approaches is limited, because of the lack of the\npair relation supervision.\nSemi-Supervised Learning in Re-ID. Semi-supervised learning methods [1, 8, 17, 18, 24,\n37, 41] train the re-id model both on pre-labeled data and unlabeled data. Liu et al.[24] pro-\nposed a coupled dictionary learning by randomly pre-labeling one-third training data. Most\nof these methods [1, 17, 18, 37, 41] work on one-shot learning setting. One-shot learning\nrequires selecting all or most person identities, and then labeling one image or tracklet for\neach identity. Bak et al. [1] learned a texture metric and a color metric on each camera pair\nby a one-shot metric learning approach. Ye et al. [41] proposed a label estimation approach\nto learn feature representations by using the pre-labeled data to formulate an anchor graph.\nWu et al. [37] initialized a CNN model using pre-labeled data per ID, and then adopted a\nstep-wise learning approach to update the CNN model. Li et al. [17, 18] used pre-labeled\nwithin-camera tracklet per ID to initialize a deep model, and then incrementally discover\ncross-camera tracklet association to improve the representation capability of deep model.\nAlthough semi-supervised learning methods improve re-id performance by one-shot la-\nbeling strategy, this labeling setting is actually not practical for re-id task. In practice, the\ntotal number of individual ID is hard to know, not mention to label one instance for each ID.\nActive Learning and Human-in-the-loop in Re-ID. For reducing the annotation cost in\na more practical way, pair-wise labeling re-id approaches are proposed. According to the\ndifferent applied stages, these methods can be divided into two categories, including active\nlearning methods [22, 23, 25, 26, 31, 34] and human-in-the-loop methods [4, 27, 35]: (1)\nActive learning re-id methods aim at selecting a small number of image pairs to query human\nlabeling in the training stage. Liu et al. [22] proposed an early active learning algorithm\n(EALPC) with a pairwise constraint to select the most representative samples for labeling.\nRoy et al. [31] presented a pairwise training subset selection framework to minimize human\nannotation effort. Liu et al. [25] designed a deep reinforcement active learning (DRAL)\nmodel to minimize human effort in the training stage. (2) Human-in-the-loop re-id methods\nfocus on optimising the ranking list of every probe by human feedback directly in the test\nstage. Liu et al. [27] learned a post-rank function for re-ordering the rank list during a\nre-identiﬁcation process. Wang et al. [35] proposed a distance metric learning method to\nincrementally optimise each new probe by human annotation in the deployment stage.\nCompared with unsupervised learning and semi-supervised re-id methods,active learn-\ning and human-in-the-loop re-id methods reasonably add a handful of pair-wise labels to\nimprove the performance. This paper follows active learning scheme, and our objective is to\nminimize human labeling effort to improve the performance by jointly unsupervised learning\nand active learning.\n3\nMethodology\nIn this section, we introduce the overall framework of our Unsupervised Clustering Active\nLearning (UCAL) method for person re-identiﬁcation. An overview of the proposed UCAL\nmodel is depicted in Fig. 1. Given N unlabeled training person images I = {III1,III2,··· ,IIIN}\nextracted from multiple camera views. The training objective is to select M pairs of unlabeled\n4\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n…\nunlabeled images\nCNN\nexact \nfeatures\noriginal clusters\nhard negative centroid-pair\nyes\nno\nupdate label memory\nsplit cluster\nhard positive centroid-pair\nmerge cluster\nyes\nno\nupdate label memory\n√\n×\n...\n(a)\n(b)\nrefined clusters\nFigure 1: An overview of the proposed Unsupervised Clustering Active Learning (UCAL)\nmethod for re-id model learning. The UCAL takes as unlabeled person images from all\nthe camera views. The objective is to derive a person discriminative feature representative\nfeature representation module by unsupervised active learning. To this end, we formulate\nthe UCAL model (see Sec. 3.2) with (a) Split by Negative centroid-Pair Selection (SNPS)\nmodule and (b) Merge by Positive centroid-Pair Selection (MPPS) module in an unsuper-\nvised clustering framework. Different colours correspond to different clusters. Best viewed\nin colour.\ndata to ask for human’s labelling and learn a discriminative person re-id model. The proposed\nUCAL framework is driven by both of unlablled data and labeled data. In particular, we ﬁrst\nadopt an unsupervised clustering method to discover the data distribution structure. And\nthen, we select the key centroid-pairs from the clustering structure by Negative centroid-Pair\nSelection (SNPS) module and Merge by Positive centroid-Pair Selection (MPPS) module,\nand annotate the positive/negative relationships for these pairs. At last, the model is updated\nby the re-organized clustering structure in an iterative way to learn a discriminative feature\nrepresentation.\n3.1\nBase Unsupervised Clustering Model\nWe adopt a DBSCAN based unsupervised clustering method [6, 10] as the base clustering\nframework. In this framework, a CNN-based [12] encoder fθ is used to represent person\nimage feature. Given the unlabeled training data I, we adopt a self-paced clustering strategy\n[10] to group the data into clusters. According to the results of clustering, we update the\nparameters θ of the feature encoder fθ. We enforce ||xxx|| = 1 via a L2-normalization layer.\nThe loss function is an unsupervised contrastive learning which is similar with [10, 38], but\ncomputed only by the unlabeled data.\nLoss(xxx) = −log\nexp(xxxTccck/τ)\n∑n\ni=1 exp(xxxTccci/τ)\n(1)\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n5\nwhere xxxTccck expresses the similarity between the sample xxx and the k-th cluster centroid ccck, n\nis the number of clusters, and τ is a temperature parameter that controls the concentration of\nthe distribution [13].\n3.2\nUnsupervised Active Learning Framework\nThe base unsupervised clustering algorithm inevitably generates the incorrect pseudo labels,\nwhich will be harmful to the CNN model. For reﬁning the clustering results, we aim to\nexplore two kinds of representative sample pairs from the clustering structure: hard negative\ncentroid-pair and hard positive centroid-pair. The hard negative centroid-pair is the pair\nbetween two group centroids which have two IDs but wrongly grouped into the same cluster.\nThe hard positive centroid-pair is the pair between two cluster centroids which have the\nsame ID but wrongly grouped into two different clusters. After obtaining these two kinds\nof sample pairs candidates, positive/negative relationships are required to label by human\nexperts. After that, two categories of manipulations: split and merge are adopted to reﬁne\nthe current clustering structure. The CNN model is then updated by the reﬁned clustering\nresult.\n3.2.1\nSplit by Hard Negative Centroid-Pair Selection\nA single cluster generated the base clustering algorithm may contain some small groups\nbelonged to different ground truth IDs. This kind of relationship between these groups is\ncalled hard negative. This result gives incorrectly the same pseudo label to these individual\nsamples grouped into one cluster, which be harmful to update the feature representation\nCNN model. To alleviate this problem, we propose a Split by hard Negative centroid-Pair\nSelection (SNPS) method to mine the hard negative centroid-pairs within the same cluster.\nGiven nt clusters C = {C1,C2,··· ,Cnt} at epoch t, we use k-medoids [15] algorithm to\ngenerate k group candidates for each cluster. For j-th group Gj, gggj represents the centroid of\nG j (Fig.1(a)). However, the real number of groups which have negative relations depends on\nthe different clusters. That is, k is not ﬁxed for each cluster. Therefore, we design a reliability\ncriterion to decide k for each cluster. As discussed in [10], a reliable cluster should have two\nproperties: high independence and high compactness. Thus, we aim to ﬁnd k groups for each\ncluster with high independence and high compactness as splitting candidates. According to\nthis criterion, an algorithm is proposed to decide k as follow:\ncompj = minSimintra(G j) ∈[0,1]\n(2)\nindep j = 1−maxSiminter(G j,G) ∈[0,1]\n(3)\nk∗= argmax\nk∈[2,kmax]\nk\n∑\nj=1\ncompj ×indepj\n(4)\nwhere Simintra represents the similarities between samples within group G j, Siminter repre-\nsents the similarities between samples from group Gj and samples from other groups, kmax\nis set to\n√\n|Ci|/2\n2\n.\nDiscussion. Although the similar concept of independence and compactness is proposed\nin [10], the objective is quite different in this paper. The objective of independence and\n6\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\ncompactness in [10] is to make each cluster more reliable. In SNPS, we aim to obtain\na reliable splitting structure by measuring the independence and compactness of several\ngroups controlled by k. Moreover, the criterion of good independence and compactness in\n[10] is decided by the density threshold, which is difﬁcult to choose appropriately. In this\npaper, we adopt the maximum product of independence and compactness to select the most\nreliable splitting structure.\n3.2.2\nMerge by Hard Positive Centroid-Pair Selection\nThe individual samples which have the same person ID are very likely separated into dif-\nferent clusters by the clustering algorithm. This improper clustering structure generates the\ndifferent pseudo labels to those clusters which should be grouped into the same cluster. This\nkind of relationship between these clusters is called hard positive. The existence of hard\npositive pairs harms the representation ability of CNN model. To mine the hard positive\nrelationship between clusters, we propose a Merge by hard Positive centroid-Piar Selection\n(MPPS) method.\nGiven nt clusters C = {C1,C2,··· ,Cnt} at epoch t, the cluster centroid of cluster Ci is\ndeﬁned as ccci, and s(ccci,cccj) ∈[0,1] represents the similarity between ccci and cccj. For each\ncentroid ccci, a rank list [s1,s2,··· ,sl,··· ,slmax] is computed by the similarity between ccci and\nother centroids from high to low. lmax is the maximum number of candidate clusters. If\nall centroid-pairs are provided to human experts, the cost will be very high. To lower the\nlabeling cost, we design a measure function to decide how many the similar centroid-pairs\nare selected to labeling. We take the similarity difference between the adjacent centroids in\nthe rank list as the measure value.\nddd = {dl = sl −sl+1|l ∈[1,lmax −1]}\n(5)\nd∗\nl =\ndl −min(ddd)\nmax(ddd)−min(ddd) ∈[0,1]\n(6)\nwhere d∗\nl is the min-max normalization value.\nInspired by [30], the high value of d∗\nl indicates a large margin of similarity difference\nbetween sl and sl+1. It means the positive probabilities of pair ci-cl and pair ci-cl+1 are\nuncertainty. So, pair ci-cl and pair ci-cl+1 need to be labeled by users. We select the ﬁrst\nl centroids with cccj as the labeling pairs when d∗\nl > δ. For the centroid-pairs labelled as\npositive, the corresponding clusters will be merged to one cluster, which is given the same\npseudo label during the current training epoch.\n3.3\nOverall Model Training\nWe adopt a self-paced clustering strategy [10] as the base clustering algorithm, and the loss\nfunction (Eq. (1)) for the CNN parameters updating. During the ﬁrst 15 training epochs, the\nabove pure unsupervised method works as the initialized model. After the ﬁrst 15 epochs of\nthe training process, we deploy SNPS and MPPS modules to minimise the negative effect\nof unstable clustering structure. Speciﬁcally, the UCAL model ﬁrst deploy SNPS module\n(Sec. 3.2.1) to ﬁnd the hard negative centroid-pairs and split the original cluster into some\nsmaller clusters. And then, we deploy MPPS module (Sec. 3.2.2) to ﬁnd the hard positive\ncentroid-pairs and merge these clusters to a bigger one. To lower the labeling cost, we also\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n7\ndesign a label memory (Fig. 1) to record the positive/negative relationships between cen-\ntroids labeled by human expert. If the relationship between two centroids has been labeled\nin the label memory, the labeling request of this relationship is no need in the subsequent\nlabeling process.\n4\nExperiments\n4.1\nDatasets and Evaluation Protocol\nDatasets. To evaluate the proposed UCAL model, we reports results on three large person\nre-identiﬁcation datasets: Market-1501[45], DukeMTMC-ReID[46], MSMT17[36].\n(1) Market-1501[45]: Market-1501 is widely used large-scale re-id dataset. It contains\n32,668 images of 1,501 person identities from 6 camera views. There are 12,936 images of\n751 identities in training set, and 3,368 queries of 750 identities are used as the query set to\nsearch the true match among the remained 19,732 images.\n(2) DukeMTMC-ReID[46]: DukeMTMC-ReID is another one of the most popular large\nscale re-id dataset which consists 36,411 pedestrian images from 1,812 person identities cap-\ntured from 8 different cameras. Speciﬁcally, 16,522 images (702 identities) are adopted for\ntraining, 2,228 (702 identities) images are used as query to be retrieved from the remaining\n17,661 images (1,110 identities).\n(3) MSMT17[36]: MSMT17 is a larger and more challenging dataset, which contains\n4,101 identities and 126,441 pedestrian images. There are 32,621 images of 1,041 identities\nin training set, and 93,820 images of 3,060 identities in test set. In the test set, 11,659\npedestrian images are randomly selected as probe images, and the other 82,161 images are\ntreated as gallery images.\nEvaluation Protocol. In the experiments, we used the Cumulative Matching Characteristic\n(CMC) and mean Average Precision (mAP) metrics to measure the methods’ performance.\nThe cost of human labeling effort is computed as follow:\ncost =\nM\nN ∗(N −1)/2 ∗100%\n(7)\nwhere N represents the number of unlabeled training samples, and M represents the labeled\npairs from human experts.\n4.2\nImplementation Details\nWe adopt an ImageNet pre-trained ResNet-50 [12] as the backbone for our UCAL model.\nAfter pooling-5 layer,we remove subsequent layers and add a 1D BatchNorm layer and an\nL2-normalization layer to derive the feature representations. Person bounding box images\nare resized to 256 × 128 as input. To ensure each training mini-batch has person images\nfrom all cameras, we set the batch size to 64 for all datasets mentioned in this paper. We\nadopted Adam optimiser with a weight decay of 0.0005, the learning rate is initialized to\n3.5 × 10−4. We train the model for 50 epochs, and the learning rate is divided by 10 after\nevery 20 epochs. From the 15th epoch and every subsequent epoch,we use SNPS and MPPS\nmodules to generate labeled pairs. By default, we set the maximum number of clusters that\ncan be merged in each epoch to 20% of the total number of clusters and δ = 0.3. All the\nexperiments on three datasets follow the same settings as above.\n8\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\nTable 1: Comparison of proposed UCAL approach with state-of-the-art unsupervised,\ndomain adaptation, semi-supervised and active learning approaches on Market-1501,\nDukeMTMC-ReID, and MSMT17.\nMethods\nMarket-1501 [45]\nDukeMTMC-ReID [46]\nMSMT17 [36]\nR1\nR5\nR10\nmAP\ncost(%)\nR1\nR5\nR10\nmAP\ncost(%)\nR1\nR5\nR10\nmAP\ncost(%)\nunsup\nOIM[39]\nCVPR’17\n38.0\n58.0\n66.3\n14.0\n0\n24.5\n38.8\n46.0\n11.3\n0\n-\n-\n-\n-\n-\nBUC[20]\nAAAI’19\n66.2\n79.6\n84.5\n38.3\n0\n47.4\n62.6\n68.4\n27.5\n0\n-\n-\n-\n-\n-\nSSL[21]\nCVPR’20\n71.7\n83.8\n87.4\n37.8\n0\n52.5\n63.5\n68.9\n28.6\n0\n-\n-\n-\n-\n-\nMMCL[33]\nCVPR’20\n80.3\n89.4\n92.3\n45.5\n0\n65.2\n75.9\n80.0\n40.2\n0\n35.4\n44.8\n49.8\n11.2\n0\nHCT[42]\nCVPR’20\n80.0\n91.6\n95.2\n56.4\n0\n69.6\n83.4\n87.4\n50.7\n0\n-\n-\n-\n-\n-\ndomain\nPTGAN[36]\nCVPR’18\n38.6\n-\n66.1\n-\n0\n27.4\n-\n50.7\n-\n0\n10.2\n-\n24.4\n2.9\n0\nECN[48]\nCVPR’19\n75.1\n87.6\n91.6\n43.0\n0\n63.3\n75.8\n80.4\n40.4\n0\n25.3\n36.3\n42.1\n8.5\n0\nSSG[8]\nICCV’19\n80.0\n90.0\n92.4\n58.3\n0\n73.0\n80.6\n83.2\n53.4\n0\n32.2\n-\n51.2\n13.3\n0\nMMCLtrans[33]\nCVPR’20\n84.4\n92.8\n95.0\n60.4\n0\n72.4\n82.9\n85.0\n51.4\n0\n43.6\n54.3\n58.9\n16.2\n0\nSPCL[10]\nNIPS’20\n89.7\n96.1\n97.6\n77.5\n0\n-\n-\n-\n-\n-\n53.7\n65.0\n69.8\n26.8\n0\nsemi\nEUG[37]\nCVPR’18\n49.8\n66.4\n72.7\n22.5\n-\n45.2\n59.2\n63.4\n24.5\n-\n-\n-\n-\n-\n-\nTAUDL[17]\nECCV’18\n63.7\n-\n-\n41.2\n-\n61.7\n-\n-\n43.5\n-\n28.4\n-\n-\n12.5\n-\nUTAL[18]\nPAMI’19\n69.2\n-\n-\n46.2\n-\n62.3\n-\n-\n44.6\n-\n31.4\n-\n-\n13.1\n-\nSSG++[8]\nICCV’19\n86.2\n94.6\n96.5\n68.7\n-\n76.0\n85.8\n89.3\n60.3\n-\n41.6\n-\n62.2\n18.3\n-\nactive\nTMA[26]\nECCV’16\n47.9\n-\n-\n22.3\n13.58\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDRAL[25]\nICCV’19\n84.2\n94.3\n96.6\n66.3\n0.15\n74.3\n84.8\n88.4\n56.0\n0.12\n-\n-\n-\n-\n-\nDRALupper[25]\nICCV’19\n88.0\n95.3\n96.8\n73.3\n100\n78.0\n88.7\n91.6\n60.9\n100\n-\n-\n-\n-\n-\nours\nBaseline\nthis paper\n87.1\n95.0\n96.5\n69.9\n0\n77.4\n87.1\n90.7\n60.5\n0\n44.9\n57.9\n63.1\n20.3\n0\nSupervised\nthis paper\n94.2\n98.2\n98.9\n83.4\n100\n84.6\n92.2\n94.1\n71.1\n100\n71.3\n84.2\n88.0\n45.5\n100\nUCAL\nthis paper\n91.8\n96.8\n97.9\n78.2\n0.08\n81.2\n89.7\n92.5\n66.3\n0.06\n63.2\n75.1\n79.4\n35.7\n0.03\n4.3\nComparisons with State-Of-The-Art Methods\nIn this section, we compare the proposed UCAL model with sixteen state-of-the-art re-\nid methods, including unsupervised learning (OIM[39], BUC[20], SSL[21], MMCL[33],\nHCT[42]), unsupervised domain adaptation (PTGAN[36], ECN[48], SSG[8], MMCLtrans[33],\nSPCL[10]), semi-supervised learning (EUG[37], TAUDL[17], UTAL[18], SSG++[8]), and\nactive learning (TMA[26], DRAL[25]) models. The rank-(1,5,10) matching accuracy(%)\nand mAP(%) performance evaluated on Market-1501 [45], DukeMTMC-ReID [46], MSMT17\n[36] are showed in Table 1. Moreover, the cost of human labeling effort computed by Eq.(7)\nis also given. The baseline model is trained by the unsupervised clustering method (Sec. 3.1)\nwithout any labeling cost. The supervised model adopt the same loss function (Eq. (1)) for\ntraining but using ground truth label instead of pseudo label. The experimental results show\nthree observations as follows.\n(1) The proposed UCAL model outperforms all competitors of active learning models\nwith signiﬁcant margins both on performance and cost. For example, the mAP margin is\n11.9%(78.2-66.3) on Market1501 and 10.3%(66.3-56.0) on DukeMTMC-ReID. More im-\nportantly, the labeling cost of UCAL model is only 0.08% on Market1501 while TMA[26]’s\ncost is 13.58% and DRAL[25]’s cost is 0.15%, and 0.06% on DukeMTMC-ReID while\nDRAL[25]’s cost is 0.12%.\n(2) Compared with state-of-the-art unsupervised/domain adaptation/semi-supervised learn-\ning models, the performance of our model is superior but with very low cost. Although there\nis no labeling cost under the above setting, the performance is limited especially on the\nlarge scale benchmark such as MSMT17. As discussed in Section 2, the labeling cost of\nsemi-supervised learning is not able to estimated. Compared with the state-of-the-art unsu-\npervised learning model, our UCAL model improves the performance of rank1 and mAP on\nMSMT17 by 9.5%(63.2-53.7) and 8.9%(35.7-26.8) with 0.03% cost.\n(3) The proposed UCAL model narrows the gap between active learning and supervised\nlearning models. Speciﬁcally, compared with the full labeling cost (100%) requirement\nof supervised learning model, by our UCAL model, the gap of rank1/mAP is narrowed\nto -2.4%(91.8-94.2) and -5.2%(78.2-83.4) with 0.08% cost on Market1501, -3.4%(81.2-\n84.6) and -4.8%(66.3-71.1) with 0.06% cost on DukeMTMC-ReID, -8.1%(63.2-71.3) and\n-9.8%(35.7-45.5) with 0.03% cost on MSMT17.\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n9\n4.4\nComponent Analysis and Discussion\nWe conducted detailed UCAL model component analysis on two large person re-id datasets,\nMarket1501 and MSMT17.\nEffect of SNPS and MPPS Modules. We started by testing the performance impact of\nSNPS module and MPPS module. Based on the baseline model (Section 3.1), we ﬁrstly\ntested our SNPS component and MPPS component separately. Then, we combined these\ntwo components and tested the ﬁnal performance. Table 2 shows that, the proposed SNPS\nand MPPS are both superior over the baseline model. After combining these two modules\ntogether to reﬁne the clustering result, the UCAL model achieves mAP gain of 7.2 percent\n(78.2-71.0) and 13.6 percent (34.8-21.2) on Market1501 and MSMT17 respectively. This\nvalidates the proposed idea of split and merge modules, which reﬁnes the clusering structure\nby mining hard negative centroid-pairs and hard positive centroid-pairs.\nTable 2: Effect of SNPS component and MPPS component.\nMethods\nMarket-1501 [45]\nMSMT17 [36]\nR1\nR5\nR10\nmAP\ncost(%)\nR1\nR5\nR10\nmAP\ncost(%)\nBaseline\n87.1\n94.6\n96.4\n71.0\n0\n46.3\n60.0\n65.6\n21.2\n0\nSNPS\n90.5\n96.4\n97.7\n76.8\n0.027\n53.4\n66.1\n71.0\n26.7\n0.010\nMPPS\n88.9\n96.1\n97.6\n72.2\n0.046\n56.0\n68.7\n73.9\n30.2\n0.019\nSNPS+MPPS\n91.8\n96.8\n97.9\n78.2\n0.075\n63.2\n75.1\n79.4\n35.7\n0.033\nEffect with Labeling Cost. To further examine how well the proposed labeling strategy\nenables more discriminative re-id model learning, we tracked the improvement of UCAL\ncompared with the base clustering model throughout the training. Fig. 2 shows that, along\nwith the increase of the labeling cost, the improvement of UCAL becomes more and more\nobvious versus the base clustering model.\n(a) Market1501\n(b) MSMT17\nFigure 2: The improvement of ucal compared with the base clustering model during training\non (a) Market1501 and (b) MSMT17 benchmarks. The labeling cost of UCAL is depicted in\nthe in brackets. Best viewed in colour.\n5\nConclusion\nWe presented a novel Unsupervised Clustering Active Learning (UCAL) model for active\nlearning based person re-identiﬁcation. The model improves the feature representation abil-\nity of deep model dramatically with low labeling cost. This is achieved optimising jointly\n10\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\nboth Split by hard Negative centroid-Pair Selection (SNPS) module and Merge by hard Pos-\nitive centroid-Pair Selection (MPPS) module by in a uniﬁed architecture. Extensive evalua-\ntions were conducted on three large-scale person re-id benchmarks to validate the advantages\nof the proposed UCAL model over state-of-the-art active learning re-id methods.\nReferences\n[1] Slawomir Bak and Peter Carr. One-shot metric learning for person re-identiﬁcation. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages\n2990–2999, 2017.\n[2] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Deep association learning for unsu-\npervised video person re-identiﬁcation. Proc. Bri. Mach. Vis. Conf., 2018.\n[3] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Instance-guided context rendering for\ncross-domain person re-identiﬁcation. In Proc. IEEE Int. Conf. Comput. Vis., pages\n232–242, 2019.\n[4] Abir Das, Rameswar Panda, and Amit Roy-Chowdhury. Active image pair selection\nfor continuous person re-identiﬁcation. In IEEE Int. Conf. on Img. Proc., pages 4263–\n4267, 2015.\n[5] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, and Jianbin\nJiao.\nImage-image domain adaptation with preserved self-similarity and domain-\ndissimilarity for person re-identiﬁcation. In Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., pages 994–1003, 2018.\n[6] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. A density-based\nalgorithm for discovering clusters in large spatial databases with noise. In Kdd, vol-\nume 96, pages 226–231, 1996.\n[7] Hehe Fan, Liang Zheng, Chenggang Yan, and Yi Yang.\nUnsupervised person re-\nidentiﬁcation: Clustering and ﬁne-tuning. ACM Transactions on Multimedia Com-\nputing, Communications, and Applications, 14(4):1–18, 2018.\n[8] Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, and Thomas S\nHuang. Self-similarity grouping: A simple unsupervised cross domain adaptation ap-\nproach for person re-identiﬁcation. In Proc. IEEE Int. Conf. Comput. Vis., pages 6112–\n6121, 2019.\n[9] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label\nreﬁnery for unsupervised domain adaptation on person re-identiﬁcation.\nProc. Int.\nConf. on Learn. Rep., 2020.\n[10] Yixiao Ge, Dapeng Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Self-paced con-\ntrastive learning with hybrid memory for domain adaptive object re-id. Proc. Neur.\nInfo. Proc. Sys., 2020.\n[11] Shaogang Gong, Marco Cristani, Shuicheng Yan, and Chen Change Loy. Person re-\nidentiﬁcation. Springer, 2014.\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n11\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages 770–\n778, 2016.\n[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531, 2015.\n[14] Yan Huang, Qiang Wu, JingSong Xu, and Yi Zhong. Sbsgan: Suppression of inter-\ndomain background shift for person re-identiﬁcation. In Proc. IEEE Int. Conf. Comput.\nVis., pages 9527–9536, 2019.\n[15] Leonard Kaufman and Peter J Rousseeuw. Finding groups in data: an introduction to\ncluster analysis, volume 344. John Wiley & Sons, 2009.\n[16] Changsheng Li, Handong Ma, Zhao Kang, Ye Yuan, Xiao-Yu Zhang, and Guoren\nWang. On deep unsupervised active learning. Proc. Int. Jo. Conf. of Artif. Intell.,\n2020.\n[17] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised person re-identiﬁcation\nby deep learning tracklet association. In Proc. Eur. Conf. Comput. Vis., pages 737–753,\n2018.\n[18] Minxian Li, Xiatian Zhu, and Shaogang Gong.\nUnsupervised tracklet person re-\nidentiﬁcation. IEEE Trans. Pattern Anal. Mach. Intell., 42(7):1770–1782, 2019.\n[19] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep ﬁlter pairing\nneural network for person re-identiﬁcation. In Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., pages 152–159, 2014.\n[20] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi Yang. A bottom-up clustering\napproach to unsupervised person re-identiﬁcation. In AAAI Conf. on Art. Intel., 2019.\n[21] Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, and Qi Tian. Unsupervised person\nre-identiﬁcation via softened similarity learning. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 3390–3399, 2020.\n[22] Wenhe Liu, Xiaojun Chang, Ling Chen, and Yi Yang. Early active learning with pair-\nwise constraint for person re-identiﬁcation. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases, pages 103–118, 2017.\n[23] Wenhe Liu, Xiaojun Chang, Ling Chen, Dinh Phung, Xiaoqin Zhang, Yi Yang, and\nAlexander G Hauptmann. Pair-based uncertainty and diversity promoting early active\nlearning for person re-identiﬁcation. ACM Transactions on Intelligent Systems and\nTechnology, 11(2):1–15, 2020.\n[24] Xiao Liu, Mingli Song, Dacheng Tao, Xingchen Zhou, Chun Chen, and Jiajun Bu.\nSemi-supervised coupled dictionary learning for person re-identiﬁcation. In Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., pages 3550–3557, 2014.\n[25] Zimo Liu, Jingya Wang, Shaogang Gong, Huchuan Lu, and Dacheng Tao. Deep re-\ninforcement active learning for human-in-the-loop person re-identiﬁcation. In Proc.\nIEEE Int. Conf. Comput. Vis., pages 6122–6131, 2019.\n12\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n[26] Niki Martinel, Abir Das, Christian Micheloni, and Amit K Roy-Chowdhury. Temporal\nmodel adaptation for person re-identiﬁcation. In Proc. Eur. Conf. Comput. Vis., pages\n858–877, 2016.\n[27] Feiping Nie, Hua Wang, Heng Huang, and Chris Ding. Early active learning via robust\nrepresentation and structured sparsity. In Proc. Int. Jo. Conf. of Artif. Intell., pages\n1572–1578, 2013.\n[28] Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pontil, Shaogang Gong, Tiejun\nHuang, and Yonghong Tian. Unsupervised cross-dataset transfer learning for person\nre-identiﬁcation. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages 1306–\n1315, 2016.\n[29] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Per-\nformance measures and a data set for multi-target, multi-camera tracking. In Workshop\nof Eur. Conf. Comput. Vis., pages 17–35, 2016.\n[30] Alex Rodriguez and Alessandro Laio. Clustering by fast search and ﬁnd of density\npeaks. science, 344(6191):1492–1496, 2014.\n[31] Sourya Roy, Sujoy Paul, Neal E Young, and Amit K Roy-Chowdhury.\nExploiting\ntransitivity for learning person re-identiﬁcation models on a budget. In Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., pages 7064–7072, 2018.\n[32] Yingzhi Tang, Yang Xi, Nannan Wang, Bin Song, and Xinbo Gao. Cgan-tm: A novel\ndomain-to-domain transferring method for person re-identiﬁcation. IEEE Trans. Img.\nProc., 29:5641–5651, 2020.\n[33] Dongkai Wang and Shiliang Zhang. Unsupervised person re-identiﬁcation via multi-\nlabel classiﬁcation. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages 10981–\n10990, 2020.\n[34] Hanxiao Wang, Shaogang Gong, and Tao Xiang. Highly efﬁcient regression for scal-\nable person re-identiﬁcation. In Proc. Bri. Mach. Vis. Conf., pages 1–8, 2016.\n[35] Hanxiao Wang, Shaogang Gong, Xiatian Zhu, and Tao Xiang.\nHuman-in-the-loop\nperson re-identiﬁcation. In Proc. Eur. Conf. Comput. Vis., pages 405–422, 2016.\n[36] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge\ndomain gap for person re-identiﬁcation. In Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., pages 79–88, 2018.\n[37] Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, and Yi Yang. Exploit the\nunknown gradually: One-shot video-based person re-identiﬁcation by stepwise learn-\ning. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages 5177–5186, 2018.\n[38] Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin. Unsupervised feature learn-\ning via non-parametric instance discrimination. In Proc. IEEE Conf. Comput. Vis. Pat-\ntern Recognit., pages 3733–3742, 2018.\n[39] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang. Joint detection\nand identiﬁcation feature learning for person search. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 3376–3385. IEEE, 2017.\nSTUDENT, PROF, COLLABORATOR: BMVC AUTHOR GUIDELINES\n13\n[40] Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo, Xing Sun, Hao Cheng, Xiaowei\nGuo, Feiyue Huang, Rongrong Ji, and Shaozi Li. Asymmetric co-teaching for unsuper-\nvised cross-domain person re-identiﬁcation. In AAAI Conf. on Art. Intel., volume 34,\npages 12597–12604, 2020.\n[41] Mang Ye, Xiangyuan Lan, and Pong C Yuen. Robust anchor embedding for unsuper-\nvised video person re-identiﬁcation in the wild. In Proc. Eur. Conf. Comput. Vis., pages\n170–186, 2018.\n[42] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo. Hierarchical clustering with\nhard-batch triplet loss for person re-identiﬁcation. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 13657–13665, 2020.\n[43] Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, and\nYonghong Tian. Ad-cluster: Augmented discriminative clustering for domain adaptive\nperson re-identiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9021–9030, 2020.\n[44] Xinyu Zhang, Jiewei Cao, Chunhua Shen, and Mingyu You. Self-training with pro-\ngressive augmentation for unsupervised cross-domain person re-identiﬁcation. In Proc.\nIEEE Int. Conf. Comput. Vis., pages 8222–8231, 2019.\n[45] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.\nScalable person re-identiﬁcation: A benchmark. In Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., pages 1116–1124, 2015.\n[46] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled samples generated by gan\nimprove the person re-identiﬁcation baseline in vitro. In Proc. IEEE Int. Conf. Comput.\nVis., pages 3754–3762, 2017.\n[47] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Generalizing a person retrieval\nmodel hetero-and homogeneously. In Proc. Eur. Conf. Comput. Vis., pages 172–188,\n2018.\n[48] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi Yang. Invariance matters:\nExemplar memory for domain adaptive person re-identiﬁcation. In Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., pages 598–607, 2019.\n",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "published": "2021-12-26",
  "updated": "2021-12-26"
}