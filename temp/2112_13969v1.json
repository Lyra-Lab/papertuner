{
  "id": "http://arxiv.org/abs/2112.13969v1",
  "title": "LINDA: Unsupervised Learning to Interpolate in Natural Language Processing",
  "authors": [
    "Yekyung Kim",
    "Seohyeong Jeong",
    "Kyunghyun Cho"
  ],
  "abstract": "Despite the success of mixup in data augmentation, its applicability to\nnatural language processing (NLP) tasks has been limited due to the discrete\nand variable-length nature of natural languages. Recent studies have thus\nrelied on domain-specific heuristics and manually crafted resources, such as\ndictionaries, in order to apply mixup in NLP. In this paper, we instead propose\nan unsupervised learning approach to text interpolation for the purpose of data\naugmentation, to which we refer as \"Learning to INterpolate for Data\nAugmentation\" (LINDA), that does not require any heuristics nor manually\ncrafted resources but learns to interpolate between any pair of natural\nlanguage sentences over a natural language manifold. After empirically\ndemonstrating the LINDA's interpolation capability, we show that LINDA indeed\nallows us to seamlessly apply mixup in NLP and leads to better generalization\nin text classification both in-domain and out-of-domain.",
  "text": "LINDA: Unsupervised Learning to Interpolate\nin Natural Language Processing\nYekyung Kim\nHyundai Motor Company\nyekyung.kim@hyundai.com\nSeohyeong Jeong\nHyundai Motor Company\nKyunghyun Cho\nNew York University\nGenentech\nCIFAR Fellow\nAbstract\nDespite the success of mixup in data augmen-\ntation, its applicability to natural language pro-\ncessing (NLP) tasks has been limited due to\nthe discrete and variable-length nature of nat-\nural languages. Recent studies have thus re-\nlied on domain-speciﬁc heuristics and man-\nually crafted resources, such as dictionaries,\nin order to apply mixup in NLP. In this pa-\nper, we instead propose an unsupervised learn-\ning approach to text interpolation for the pur-\npose of data augmentation, to which we re-\nfer as “Learning to INterpolate for Data Aug-\nmentation” (LINDA), that does not require\nany heuristics nor manually crafted resources\nbut learns to interpolate between any pair of\nnatural language sentences over a natural lan-\nguage manifold. After empirically demonstrat-\ning the LINDA’s interpolation capability, we\nshow that LINDA indeed allows us to seam-\nlessly apply mixup in NLP and leads to bet-\nter generalization in text classiﬁcation both in-\ndomain and out-of-domain.\n1\nIntroduction\nData augmentation has become one of the key tools\nin modern deep learning especially when dealing\nwith low-resource tasks or when confronted with\ndomain shift in the test time (Lim et al., 2019).\nNatural language processing (NLP) is not an excep-\ntion to this trend, and since 2016 (Sennrich et al.,\n2015), a number of studies have demonstrated its\neffectiveness in a wide variety of problems, includ-\ning machine translation and text classiﬁcation (Ng\net al., 2020a; Kumar et al., 2020a).\nWe can categorize data augmentation algorithms\ninto two categories. An algorithm in the ﬁrst cate-\ngory takes as input a single training example and\nproduces (often stochastically) various perturbed\nversions of it. These perturbed examples are used\ntogether with the original target, to augment the\ntraining set. In the case of back-translation (Edunov\net al., 2018), the target side of an individual train-\ning example is translated into the source language\nby a reverse translation model, to form a new train-\ning example with a back-translated source and the\noriginal target translation. A few approaches in\nthis category have been proposed and studied in the\ncontext of NLP, including rule-based approaches\n(Wei and Zou, 2019a) and language model (LM)\nbased approaches (Ng et al., 2020a; Yi et al., 2021),\nwith varying degrees of success due to the chal-\nlenges arising from the lack of known metric in the\ntext space.\nAn augmentation algorithm in the second cat-\negory is often a variant of mixup (Zhang et al.,\n2017; Verma et al., 2019; Zhao and Cho, 2019)\nwhich takes as input a pair of training examples\nand (stochastically) produces an interpolated ex-\nample. Such an algorithm often produces an inter-\npolated target accordingly as well, encouraging a\nmodel that behaves linearly between any pair of\ntraining examples. Such an algorithm can only be\napplied when interpolation between two data points\nis well-deﬁned, often implying that the input space\nis continuous and ﬁxed-dimensional. Both of these\nproperties are not satisﬁed in the case of NLP. In\norder to overcome this issue of the lack of inter-\npolation in text, earlier studies have investigated\nrather ad-hoc ways to address them. For instance,\nGuo et al. (2019) pad one of two input sentences to\nmake them of equal length before mixing a random\nsubset of words from them to form an augmented\nsentence. Yoon et al. (2021) goes one step beyond\nby using saliency information from a classiﬁer to\ndetermine which words to mix in, although this\nrelies on a strong assumption that saliency infor-\nmation from a classiﬁer is already meaningful for\nout-of-domain (OOD) robustness or generalization.\nIn this paper, we take a step back and ask what in-\nterpolation means in the space of text and whether\nwe can train a neural net to produce an interpo-\narXiv:2112.13969v1  [cs.CL]  28 Dec 2021\nlated text given an arbitrary pair of text. We start\nby deﬁning text interpolation as sampling from a\nconditional language model given two distinct text\nsequences. Under this conditional language model,\nboth of the original sequences must be highly likely\nwith the interpolated text, but the degree to which\neach is more likely than the other is determined by\na given mixing ratio. This leads us naturally to a\nlearning objective as well as a model to implement\ntext interpolation, which we describe in detail in\nthe main section. We refer to this approach of text\ninterpolation as Learning to INterpolate for Data\nAugmentation (LINDA).\nWe demonstrate the LINDA’s capability of text\ninterpolation by modifying and ﬁnetuning BART\n(Lewis et al., 2020) with the proposed learning\nobjective on a moderate-sized corpus (Wikipedia).\nWe manually inspect interpolated text and observe\nthat LINDA is able to interpolate between any\ngiven pair of text with its strength controlled by\nthe mixup ratio. We further use an automated met-\nric, such as unigram precision, and demonstrate\nthat there is a monotonic trend between the mixup\nratio and the similarity of generated interpolation\nto one of the provided text snippets. Based on these\nobservations, we test LINDA in the context of data\naugmentation by using it as a drop-in interpolation\nengine in mixup for text classiﬁcation. LINDA\noutperforms existing data augmentation methods\nin the in-domain and achieves a competitive per-\nformance in OOD settings. LINDA also shows its\nstrength in low-resource training settings.\n2\nBackground and Related Work\nBefore we describe our proposal on learning to\ninterpolate, we explain and discuss some of the\nbackground materials, including mixup, its use in\nNLP and more broadly data augmentation in NLP.\n2.1\nMixup\nMixup is an algorithm proposed by Zhang et al.\n(2017) to improve both generalization and robust-\nness of a deep neural net. The main principle be-\nhind mixup is that a robust classiﬁer must behave\nsmoothly between any two training examples. This\nis implemented by replacing the usual empirical\nrisk, Remp = 1\nN\nPN\nn=1 l(xn, yn; θ), with\nRmixup =\n1\nN2\nN\nX\nn,n′=1\nZ 1\n0\nl(fx\nint(xn, xn′; α),\n(1)\nfy\nint(yn, yn′; α); θ) dα,\nwhere x is a training example, y is the correspond-\ning label, N is the total number of training ex-\namples, α is the mixup ratio, and l(x, y; θ) is a\nper-example loss. Unlike the usual empirical risk,\nmixup considers every pair of training examples,\n(xn, yn) and (xn′, yn′), and their α-interpolation\nusing a pair of interpolation functions fx\nint and fy\nint\nfor all possible α ∈[0, 1].\nWhen the dimensions of all input examples\nmatch and each dimension is continuous, it is a\ncommon practice to use linear interpolation:\nfx\nint(x, x′; α) = αx + (1 −α)x′.\n2.2\nMixup in NLP\nAs discussed earlier, there are two challenges in\napplying mixup to NLP. First, data points, i.e. text,\nin NLP have varying lengths. Some sentences are\nlonger than other sentences, and it is unclear how to\ninterpolate two data points of different length. Guo\net al. (2019) avoids this issue by simply padding\na shorter sequence to match the length of a longer\none before applying mixup.\nWe ﬁnd this sub-\noptimal and hard-to-justify as it is unclear what a\npadding token means, both linguistically and com-\nputationally. Second, even if we are given a pair of\nsame-length sequences, each token within these se-\nquences is drawn from a ﬁnite set of discrete tokens\nwithout any intrinsic metric on them. Chen et al.\n(2020) instead interpolates the contextual word em-\nbedding vectors to circumvent this issue, although\nit is unclear whether token-level interpolation im-\nplies sentence-level interpolation. Sun et al. (2020)\non the other hand interpolates two examples at\nthe sentence embedding space of a classiﬁer being\ntrained. This is more geared toward classiﬁcation,\nbut it is unclear what it means to interpolate two ex-\namples in a space that is not stationary (i.e., evolves\nas training continues.) Yoon et al. (2021) proposes\na more elaborate mixing strategy, but this strategy\nis heavily engineered manually, which makes it\ndifﬁcult to grasp why such a strategy is a good\ninterpolation scheme.\n2.3\nNon-Mixup Data Augmentation in NLP\nAlthough we focus on mixup in this paper, data\naugmentation has been studied before however fo-\ncusing on using a single training example at a time.\nOne family of such approaches is a token-level re-\nplacement. Wei and Zou (2019a) suggests four sim-\nple data augmentation techniques, such as synonym\nreplacement, random insertion, random swap, and\nFigure 1: The overall architecture of our framework. Input texts xa and xb of which have possibly varying lengths\nare encoded and passed to the length converter to be transformed into representations with the interpolated match-\ning length, ˜L. Ha\nL and Hb\nL, are interpolated into ˜HL with the mixup ratio α. The decoder is trained to reconstruct\n˜HL into xa and xb with the risk, RLINDA. We show an example where xa and xb has length 5 and 3, respectively\nand the hidden dimension size is 3.\nrandom deletion, to boost text classiﬁcation per-\nformance. Xie et al. (2019) substitutes the exist-\ning noise injection methods with data augmenta-\ntion methods such as RandAugment (Cubuk et al.,\n2019) or back-translation. All these methods are\nlimited in that ﬁrst, they cannot consider larger con-\ntext when replacing an individual word, and second,\nreplacement rules must be manually devised.\nAnother family consists of algorithms that rely\non using a language model to produce augmenta-\ntion. Wu et al. (2019) proposes conditional BERT\ncontextual augmentation for labeled sentences us-\ning replacement-based methods. Anaby-Tavor et al.\n(2020) use GPT2 (Radford et al., 2019) instead to\nsynthesize examples for a given class. More re-\ncently, Kumar et al. (2020b) uses any pre-trained\ntransformer (Vaswani et al., 2017) based models for\ndata augmentation. These augmentation strategies\nare highly specialized for a target classiﬁcation task\nand often require ﬁnetuning with labeled examples.\nThis makes them less suitable for low-resource set-\ntings in which there are by deﬁnition not enough\nlabeled examples, to start with.\nAn existing approach that is perhaps closest\nto our proposal is self-supervised manifold-based\ndata augmentation (SSMBA) by Ng et al. (2020a).\nSSMBA uses any masked language model as a de-\nnoising autoencoder which learns to perturb any\ngiven sequence along the data manifold. This en-\nables SSMBA to produce realistic-looking aug-\nmented examples without resorting to any hand-\ncrafted rules and to make highly non-trivial pertur-\nbation, beyond simple token replacement. It also\ndoes not require any labeled example.\nNg et al. (2020a) notice themselves such aug-\nmentation is nevertheless highly local. This implies\nthat an alternative approach that makes non-local\naugmentation may result in a classiﬁer that behaves\nbetter over the data manifold and achieves superior\ngeneralization and robustness. This is the goal of\nour proposal which we describe in the next section.\n3\nLINDA: Learning to Interpolate for\nData Augmentation\nIn this section, we ﬁrst deﬁne text interpolation (dis-\ncrete sequence interpolation) and discuss desider-\nata that should be met. We then describe how we\nimplement this notion of text interpolation using\nneural sequence modeling. We present a neural net\narchitecture and a learning objective function used\nto train this neural network and show how to use a\ntrained neural network for data augmentation.\n3.1\nText Interpolation\nWe are given two sequences (text), xa\n=\n(xa\n1, . . . , xa\nLa) and xb = (xb\n1, . . . , xb\nLb).1 They are\npotentially of two different lengths, i.e., La ̸= Lb,\nand each token is from a ﬁnite vocabulary, i.e.,\nxa\ni ∈Σ and xb\nj ∈Σ. With these two sequences, we\ndeﬁne text interpolation as a procedure of drawing\nanother sequence from the following conditional\ndistribution:\np(y|xa, xb, α) =\nLy\nY\nt=1\np(yt|y<t, xa, xb, α),\n(2)\nwhere α ∈[0, 1] is a mixup ratio, and Ly is the\nlength of an interpolated text y.\nIn order for sampling from this distribution to be\ntext interpolation, four conditions must be met. Let\nus describe them informally ﬁrst. First, when α is\ncloser to 0, a sample drawn from this distribution\nshould be more similar to xa. When α is closer to\n1, it should be more similar to xb. It is however\n1We use text and sequence interchangeably throughout the\npaper, as both of them refer to the same thing in our context.\nimportant that any sample must be similar to both\nxa and xb. Finally, this distribution must be smooth\nin that we should be able to draw samples that are\nneither xa and xb. If the ﬁnal two conditions were\nnot met, we would not call it “inter”polation.\nSlightly more formally, these conditions trans-\nlate to the following statements. First, (Condition\n1) p(xa|xa, xb, α) > p(xb|xa, xb, α) when α <\n0.5. Similarly, (Condition 2) p(xa|xa, xb, α) <\np(xb|xa, xb, α) when α > 0.5. The third condition\ncan be stated as (Condition 3) p(xa|xa, xb, α) ≫0\nand p(xb|xa, xb, α) ≫0, although this does not\nfully capture the notion of similarity between an\ninterpolated sample and either of xa or xb. We\nrely on parametrization and regularization to cap-\nture this aspect of similarity.\nThe ﬁnal condi-\ntion translates to (Condition 4) p(xa|xa, xb, α) +\np(xb|xa, xb, α) ≪1.\nIn the rest of this section, we describe how to\nparametrize this interpolation distribution with a\ndeep neural network and train it to satisfy these\nconditions, for us to build the ﬁrst learning-based\ntext interpolation approach.\n3.2\nParametrization\nAs shown in Figure 1, LINDA uses an encoder-\ndecoder architecture to parametrize the interpola-\ntion distribution in Eq. (2), resembling sequence-\nto-sequence learning (Sutskever et al., 2014; Cho\net al., 2014). The encoder takes the inputs respec-\ntively and delivers the encoded examples to the\nlength converter, where each encoded example is\nconverted to the same length. Matching length rep-\nresentations of the inputs are then interpolated with\nα. The decoder takes the interpolated representa-\ntion and reconstructs the original inputs xa and xb\nproportionally according to the given α value.\nEncoder\nWe assume the input is a sequence of\ndiscrete tokens, x = (x1, ...xL) ∈ΣL, where Σ\nis a ﬁnite set of unique tokens and L may vary\nfrom one example to another. The encoder, which\ncan be implemented as a bidirectional recurrent\nnetwork (Bahdanau et al., 2014) or as a trans-\nformer (Vaswani et al., 2017), reads each of the in-\nput pairs and outputs a set of vector representations;\nHa = {ha\n1, . . . , ha\nLa} and Hb = {hb\n1, . . . , hb\nLb}.\nLength Converter\nUnlike in images where in-\nputs are downsampled to a matching dimension,\nthe original lengths of the input sequences are\npreserved throughout the computation in NLP. To\nmatch the varying lengths, preserving the original\nlength information, we adapt a location-based at-\ntention method proposed by Shu et al. (2020). First,\nwe set the target length, ˜L, as the interpolated value\nof La and Lb:\n˜L = ⌈αLa + (1 −α)Lb⌉.\n(3)\nEach vector set is either down- or up-sampled\nto match the interpolated length ˜L which results\nin ˜Ha = {˜ha\n1, . . . , ˜ha\n˜L} and ˜Hb = {˜hb\n1, . . . , ˜hb\n˜L}.\nEach ˜h is a weighted sum of the hidden vectors:\n˜hj = PL\nk=1 wj\nkhk, where wj\nk = eaj\nk\u000e PL\nk′=1 eaj\nk′\nand aj\nk = −1\n2σ2 (k −L\n˜Lj)2 with trainable σ. We ﬁ-\nnally compute the interpolated hidden vector set\n˜H = {˜h1, . . . , ˜h˜L} by ˜hi = α˜ha\ni + (1 −α)˜hb\ni.\nDecoder\nThe decoder, which can be also imple-\nmented as a recurrent network or a transformer\nwith causal attention, takes ˜H as an input and mod-\nels the conditional probability distribution of the\noriginal input sequences xa and xb accordingly to\nthe mixup ratio α. The decoder’s output is then the\ninterpolation distribution p(y|xa, xb, α) in Eq. (2).\n3.3\nA Training Objective Function\nWe design a training objective and regularization\nstrategy so that a trained model is encouraged to\nsatisfy the four conditions laid out above and pro-\nduces a text interpolation distribution.\nA main objective.\nWe design a training objective\nfunction to impose the conditions laid out earlier in\n§3.1. The training objective function for LINDA is\nRLINDA = −1\nN2\nN\nX\nn,n′=1\nZ 1\n0\nα log(p(xn|xn, xn′, α)\n+ (1 −α) log(p(xn′|xn, xn′, α))dα,\n(4)\nwhere we assume there are N training exam-\nples and consider all possible α values equally\nlikely. Minimizing this objective function encour-\nages LINDA to satisfy the ﬁrst three conditions; a\ntrained model will put a higher probability to one of\nthe text pairs according to α and will refrain from\nputting any high probability to other sequences.\nBecause RLINDA is computationally expensive,\nwe resort to stochastic optimization in practice us-\ning a small minibatch of M randomly-paired ex-\namples at a time with α ∼U(0, 1):\n˜RLINDA = −1\nM\nM\nX\nm=1\nα log p(xa,m|xa,m, xb,m, αm)\n+ (1 −α) log p(xb,m|xa,m, xb,m, αm).\n(5)\nIt is however not enough to minimize this objec-\ntive function alone, as it does not prevent some of\nthe degenerate solutions that violate the remaining\nconditions. In particular, the model may encode\neach possible input as a set of unique vectors such\nthat the decoder is able to decode out both inputs xa\nand xb perfectly from the sum of these unique vec-\ntor sets. When this happens, there is no meaningful\ninterpolation between xa and xb learned by the\nmodel. That is, the model will put the entire proba-\nbility mass divided into xa and xb but nothing else,\nviolating the fourth condition. When this happens,\nlinear interpolation of hidden states is meaningless\nand will not lead to meaningful interpolation in the\ntext space, violating the third condition.\nRegularization\nIn order to avoid the degenerate\nsolution, we introduce three regularization tech-\nniques. First, inspired by the masked language\nmodeling suggested in Devlin et al. (2019), we\nrandomly apply the word-level masking to both\ninputs, xa and xb. The goal is to force the model to\npreserve the contextual information from the origi-\nnal sentences. Each word of the input sentence is\nrandomly masked using the masking probability,\npmask. The second one is to encourage each hidden\nvector out of the encoder to be close to the origin:\nRreg\nLINDA = RLINDA + λ\nM\nM\nX\nm=1\nLm\nX\ni=1\n∥hm\ni ∥2,\n(6)\nwhere λ ≥0 is a regularization strength. Finally,\nwe add small, zero-mean Gaussian noise to each\nvector hm\ni before interpolation during training. The\ncombination of these two has an effect of tightly\npacking all training inputs, or their hidden vectors\ninduced by the encoder, in a small ball centered\nat the origin. In doing so, interpolation between\ntwo points passes by similar, interpolatable inputs\nplaced in the hidden space. This is similar to how\nvariational autoencoders form their latent variable\nspace (Kingma and Welling, 2013).\n3.4\nAugmentation\nOnce trained, the model can be used to interpolate\nany pair of sentences with an arbitrary choice of\ndecoding strategy and distribution of α. When we\nare dealing with a single-sentence classiﬁcation,\nwe draw random pairs of training examples with\ntheir labels as a minibatch, ((xa\nm, ya\nm), (xb\nm, yb\nm))\nalong with a randomly drawn mixup ratio value\nαm.\nThen, based on the learned distribution,\nLINDA generates an interpolated version\n˜\nxm\nwhere we set the corresponding interpolated la-\nbel to ˜\nym = (1 −αm)ya\nm + αmyb\nm. In the case\nwhere the input consists of two sentences, we pro-\nduce an interpolated sentence for each of these sen-\ntences separately. For instance, in the case of nat-\nural language inference (NLI) (Rockt¨aschel et al.,\n2015), we interpolate the premise and hypothesis\nsentences separately and independently from each\nother and concatenate the interpolated premise and\nhypothesis to form an augmented training example.\n4\nLINDA: Training Details\nWe train LINDA once on a million sentence pairs\nrandomly drawn from English Wikipedia and use\nit for all the experiments. We detail how we train\nthis interpolation model in this section.\nWe base our interpolation model on a pre-trained\nBART released by Lewis et al. (2020). More specif-\nically, we use BART-large. BART is a Transformer-\nbased encoder-decoder model, pre-trained as a de-\nnoising autoencoder. We modify BART-large so\nthat the encoder computes the hidden representa-\ntions of two input sentences individually. These\nrepresentations are up/downsampled to match the\ntarget interpolated length, after which they are lin-\nearly combined according to the mixing ratio. The\ndecoder computes the interpolation distribution as\nin Eq. (2), in an autoregressive manner.\nWe train this model using the regularized recon-\nstruction loss deﬁned in Eq. (6). We uniformly\nsample the mixup ratio α ∈[0, 1] at random for\neach minibatch during training as well as for evalu-\nation. We set the batch size to 8 and use Adam op-\ntimizer (Kingma and Ba, 2015) with a ﬁxed learn-\ning rate of 1e−5. We use 8 GPUs (Tesla T4) for\ntraining. A word-level masking is adapt to input\nsentence with pmask = 0.1. We set λ = 0.001 and\nthe standard deviation of Gaussian noise to 0.001.\nOnce trained, we can produce an interpolated\nsentence from the model using any decoding strat-\negy. We use beam search with beam size set to 4 in\nall the experiments.\n5\nDoes LINDA Learn to Interpolate?\nWe ﬁrst check whether LINDA preserves the con-\ntextual information of reference sentences propor-\ntionally to the mixup ratio α, which is the key\nproperty of interpolation we have designed LINDA\nto exhibit. We investigate how much information\nfrom each of two original sentences is retained\nFigure 2: Average unigram precision of the interpo-\nlated sentences while varying the mixup ratio α in (top)\nWikipedia and (bottom) TREC.\n(measured in terms of unigram precision)2 while\nvarying the mixup ratio α. We expect a series of\ninterpolated sentences to show a monotonically\ndecreasing trend in unigram precision when com-\nputed against the ﬁrst original sentence and the\nopposite trend against the second original sentence,\nas the mixup ratio moves from 1 to 0.\nFigure 2 conﬁrms our expectation and shows\nthe monotonic trend. LINDA successfully recon-\nstructs one of the original sentences with an ex-\ntreme mixup ratio, with unigram precision scores\nclose to 1. As α nears 0.5, the interpolated sen-\ntence deviates signiﬁcantly from both of the input\nsentences, as anticipated. These interpolated sen-\ntences however continue to be well-formed, as we\nwill show later for instance in Table 6 and Table 7\nfrom Appendix. This implies that LINDA indeed\ninterpolates sentences over a natural language man-\nifold composed mostly of well-formed sentences\nonly, as we expected and designed it for.\nWe observe a similar trend even when LINDA\nwas used with non-Wikipedia sentences, as shown\nat the bottom of Figure 2 although overall unigram\n2Unigram precision is certainly not the perfect metric, but\nit at least allows us to see how many unigrams are maintained\nas a proxy to the amount of retrained information.\nprecision scores across the mixing ratios are gener-\nally a little bit lower than they were with Wikipedia.\nIn Table 1, we present four sample interpolations\nusing four random pairs of validation examples\nfrom SST-2 (Socher et al., 2013) and Yelp3 (see\n§6.1). As anticipated from the unigram precision\nscore based analysis above, most of the interpolated\nsentences exhibit some mix of semantic and syntac-\ntic structures from both of the original sentences,\naccording to the mixing ratio, as well-demonstrated\nin the ﬁrst and third examples. The second example\nshows the LINDA’s capability of paraphrasing as\nwell as using synonyms. In the ﬁnal example, both\nof the inputs are not well-formed, unlike those sen-\ntences in Wikipedia, and LINDA fails to interpolate\nthese two ill-formed sentences and resorts to pro-\nducing a Wikipedia-style, well-formed sentence,\nwhich clearly lies on a natural language manifold,\nthat has only a vague overlap with either of the\ninput sentences. These examples provide us with\nevidence that LINDA has successfully learned to\nmix contents from two inputs, i.e., interpolate them,\nover a natural language manifold of well-formed\nsentences.\nOverall, we ﬁnd the observations in this section\nto be strong evidence supporting that the proposed\napproach indeed learns to interpolate between two\nsentences over a natural language manifold com-\nposed of well-formed sentences only. This is unlike\nexisting interpolation approaches used for apply-\ning mixup to NLP, where interpolated sentences\nare almost never well-formed and cannot mix the\noriginal sentences in non-trivial ways.\n6\nLINDA for Data Augmentation\nEncouraged by the promising interpolation results\nfrom the previous section, we conduct an extensive\nset of experiments to verify the effectiveness of\nLINDA for data augmentation in this section.\nWe run all experiments in this section by ﬁne-\ntuning a pre-trained BERT-base (Devlin et al.,\n2019) using Adam (Kingma and Ba, 2015) with\na ﬁxed learning rate of 3e−5. We use the batch\nsize of 32 and train a model for 5 epochs when\nwe use a full task dataset, and the batch size of 8\nfor 30 epochs in the case of low-resource settings.\nWe report the average accuracy and standard devia-\ntion over ﬁve runs. For each training example, we\nproduce one perturbed example from a data aug-\nmentation method so that the size of the augmented\n3https://www.yelp.com/dataset\nMixup\nRatio\nExamples\n0.50\nInput 1\nRelaxed in its perfect quiet pace and proud in its message\nInput 2\nThe plot is both contrived and cliched\nLINDA\nThe plot is both perfect and cliched in its message.\n0.60\nInput 1\nRichly entertaining and suggestive of any number of metaphorical readings\nInput 2\nThe ﬁlm is superﬁcial and will probably be of interest primarily to its target audience\nLINDA\nThe ﬁlm is very entertaining and will probably be of interest to many of its target audience regardless\nof its genre.\n0.50\nInput 1\nLoved the Vegan menu! Glad to have so many options! The free margarita was really good also!\nInput 2\nService was awfully slow... food was just meh.... eventhough we get comp for drinks.... the whole\nexperienced didnt make up for it..\nLINDA\nLincoln was very pleased with the menu... Glad to have so many options!...eventhough we got the free\ndrinks for the whole team...it was really good..\n0.54\nInput 1\nengaging , imaginative ﬁlmmaking in its nearly 2 1/2\nInput 2\nthe ﬁlm has ( its ) moments , but\nLINDA\nThe ﬁlm was released in the United States on October 2, 2019, and in Canada on October 3, 2019.\nTable 1: Examples of interpolated sentences, generated by LINDA, and their original inputs on SST-2 and Yelp.\nData Proﬁle\nMethod\nTREC-Coarse\nTREC-Fine\nSST-2\nIMDB\nLow\nVanilla\n61.2±7.63 / 81.0±1.99\n53.7±2.46 / 66.8±2.33\n59.8±6.75 / 63.1±4.24\n61.8±2.41 / 67.8±2.94\nEDA\n57.0±6.54 / 72.0±4.64\n54.7±1.73 / 66.5±4.19\n62.3±3.18 / 60.5±2.81\n62.5±3.00 / 69.0±3.04\nSSMBA\n60.0±5.38 / 80.6±1.53\n53.9±1.42 / 66.5±4.66\n60.3±6.26 / 64.8±5.78\n64.1±2.10 / 69.9±2.63\nMixup\n59.9±4.94 / 81.6±2.57\n52.3±2.12 / 67.9±3.03\n59.7±4.64 / 62.1±4.38\n62.2±1.86 / 67.4±2.37\nLINDA\n62.2±3.09 / 83.4±2.30\n54.2±1.50 / 69.2±2.60\n63.5±4.44 / 66.5±3.84\n67.3±2.60 / 71.0±1.61\nFull\nVanilla\n97.2±0.36\n87.9±1.05\n92.3±0.06\n89.2±0.59\nEDA\n96.4±0.77\n90.6±0.84\n92.2±0.52\n89.5±0.18\nSSMBA\n96.9±0.41\n90.2±0.71\n92.3±0.38\n89.2±0.17\nSSMBAsoft\n97.3±0.23\n88.3±0.54\n92.3±0.35\n89.4±0.13\nMixup\n97.5±0.30\n86.4±1.23\n92.4±0.41\n89.1±0.18\nLINDA\n97.6±0.36\n91.6 ±0.75\n92.8±0.34\n89.3 ±0.18\nTable 2: Comparison of classiﬁcation accuracy (%) of BERTBASE after ﬁne-tuning with each data augmentation\nmethod on four different datasets under full and low resource (5-shot/10-shot) settings. All the results are reported\nas “mean(±std)” across 5 random runs. The best results are indicated in bold.\ndataset matches that of the original training set. We\ntest three different labeling strategies for LINDA\nand report the best accuracy. For further details,\nrefer to Appendix C.\n6.1\nDownstream Tasks\nWe conduct experiments on seven datasets: TREC\n(coarse,\nﬁne) (Li and Roth, 2002),\nSST-2,\nIMDB(Maas et al., 2011), RTE (Wang et al.,\n2018a), MNLI (Williams et al., 2018) and QNLI\n(Rajpurkar et al., 2016). For each dataset, we cre-\nate two versions of training scenarios; a full set-\nting where the original training set is used, and a\nlow-resource setting where only 5 or 10 randomly\nselected training examples per class are used. We\nalso test LINDA on OOD generalization, by closely\nfollowing the protocol from (Ng et al., 2020a). We\nuse the Amazon Review (Ni et al., 2019) and Yelp\nReview datasets as well as the Movies dataset cre-\nated by combining SST-2 and IMDB. We train a\nmodel separately on each domain and evaluate it\non all the domains. We report the in-domain and\nOOD accuracies. See Appendix B for more details.\n6.2\nBaselines\nWe compare LINDA against three existing aug-\nmentation methods. First, EDA4(Wei and Zou,\n2019b) is a heuristic method that randomly per-\nforms synonym replacement, random insertion, ran-\n4https://github.com/jasonwei20/eda_nlp\nMethod\nRTE\nQNLI\nMNLI-m/m\nARclothing\nMovie\nYelp\nID\nOOD\nID\nOOD\nID\nOOD\nVanilla\n68.2±1.78\n88.0±0.77\n77.0±0.50 / 77.6 ±0.46\n73.1±2.85\n73.0±2.86\n90.8±0.33\n84.8±0.74\n66.0±0.88\n65.4±1.49\nEDA\n65.6±2.22\n86.6±0.70\n76.4±0.27 / 77.1±0.39\n72.6±2.86\n72.6±2.79\n90.8±0.35\n84.8±0.88\n64.9±0.97\n64.8±1.26\nSSMBA\n66.8±2.18\n87.6±0.58\n76.4±1.18 / 77.3±0.51\n72.6±2.85\n72.8±2.70\n90.8±0.27\n84.7±0.79\n65.7±1.13\n65.3±1.00\nSSMBAsoft\n69.4±2.36\n88.5±1.08\n77.0±0.10 / 77.7±0.29\n73.2±2.85\n73.6±2.92\n90.8±0.24\n85.5±0.86\n66.5±1.03\n65.8±1.31\nMixup\n70.9±0.94\n87.6±1.13\n76.8±0.22 / 77.6±0.31\n72.9±2.77\n72.9±2.80\n90.7±0.30\n83.9±1.71\n65.6±0.88\n65.2±1.20\nLINDA\n71.9±1.87\n88.6±0.25\n77.4±0.27 / 78.0±0.29\n73.5±2.87\n73.2±2.82\n91.0±0.25\n85.7±0.86\n66.2±1.19\n65.6±1.20\nTable 3: Average evaluation accuracy (%) of BERTBASE across 5 runs. First three columns show accuracy in NLI\ndatasets and the following three columns show accuracy in ID and OOD settings with the ARclothing, Movie, and\nYelp datasets. See Appendix D for accuracy in each category. All the results are reported as “mean(±std)” across\n5 random runs. The best results are indicated in bold.\ndom swap, and random deletion. Second, SSMBA5\n(Ng et al., 2020a) perturbs an input sentence by\nletting a masked language model reconstruct a ran-\ndomly corrupted input sentence. For SSMBA, we\nreport two different performances; one (SSMBA)\nwhere the label of an original input is used as it\nis, and the other (SSMBAsoft) where a predicted\ncategorical distribution (soft label) by another clas-\nsiﬁer trained on an original, unaugmented dataset\nis used. Third, mixup (Guo et al., 2019) mixes a\npair of sentence at the sentence embedding level.\n6.3\nResult\nIn Table 2 we present the results on four text classi-\nﬁcation datasets for both full and low-resource set-\ntings. In the case of low-resource settings, LINDA\noutperforms all the other augmentation methods\nexcept for the 5-shot setting with the TREC-ﬁne\ntask, although the accuracies are all within standard\ndeviations in this particular case. Because Ng et al.\n(2020a) reported low accuracies when predicted\nlabels were used with a small number of training\nexamples already, we do not test SSMBAsoft in the\nlow-resource settings. Similarly in the full setting,\nLINDA outperforms all the other methods on all\ndatasets except for IMDB, although the accuracies\nare all within two standard deviations. This demon-\nstrates the effectiveness of the proposed approach\nregardless of the availability of labeled examples.\nIn the ﬁrst three columns of Table 3, we report\naccuracies on the NLI datasets; RTE, QNLI and\nMNLI ans show that LINDA outperforms other\ndata augmentation methods on all of the three\ndatasets. LINDA improves the evaluation accu-\nracy by 5.4% on RTE, 0.7% on QNLI, and 0.5%\non MNLI when all the other augmentation meth-\n5https://github.com/nng555\nods, except for SSMBAsoft, degrade the baseline\n(Vanilla) performance.\nWe report the result from the OOD generaliza-\ntion experiments in the last three columns of Table\n3. Although SSMBAsoft achieves the greatest im-\nprovement in OOD generalization over the vanilla\napproach in ARclothing and Yelp, LINDA does not\nfail to show improvement over the baseline across\nall datasets in both in-domain and OOD settings.\nIn agreement with Ng et al. (2020a), we could not\nobserve any improvement with the other augmenta-\ntion methods, EDA and Mixup, in this experiment.\n7\nConclusion\nMixup, which was originally proposed for com-\nputer vision applications, has been adapted in re-\ncent years for the problems in NLP. To address the\nissues of arising from applying mixup to variable-\nlength discrete sequences, existing efforts have\nlargely relied on heuristics-based interpolation. In\nthis paper, we take a step back and started by con-\ntemplating what it means to interpolate two se-\nquences of different lengths and discrete tokens.\nWe then come up with a set of conditions that\nshould be satisﬁed by an interpolation operator on\nnatural language sentences and proposed a neural\nnet based interpolation scheme, called LINDA, that\nstochastically produces an interpolated sentence\ngiven a pair of input sentences and a mixing ratio.\nWe empirically show that LINDA is indeed able\nto interpolate two natural language sentences over\na natural language manifold of well-formed sen-\ntences both quantitatively and qualitatively. We\nthen test it for the original purpose of data augmen-\ntation by plugging it into mixup and testing it on\nnine different datasets. LINDA outperforms all the\nother data augmentation methods, including EDA\nand SSMBA, on most datasets and settings, and\nconsistently outperforms non-augmentation base-\nlines, which is not the case with the other augmen-\ntation methods.\nAcknowledgement\nKC was supported by NSF Award 1922658 NRT-\nHDR: FUTURE Foundations, Translation, and Re-\nsponsibility for Data Science.\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\ntext: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classiﬁcation.\narXiv preprint arXiv:2004.12239.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation.\narXiv preprint\narXiv:1406.1078.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and\nQuoc V Le. 2019. Randaugment: Practical data aug-\nmentation with no separate search. arXiv preprint\narXiv:1909.13719, 2(4):7.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 489–500, Brussels, Belgium. Association for\nComputational Linguistics.\nHongyu Guo, Yongyi Mao, and Richong Zhang. 2019.\nAugmenting data with mixup for sentence clas-\nsiﬁcation:\nAn empirical study.\narXiv preprint\narXiv:1905.08941.\nJeff Heaton. 2017. Ian goodfellow, yoshua bengio, and\naaron courville: Deep learning. Genetic Program-\nming and Evolvable Machines, 19:305–307.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015.\nAdam:\nA method for stochastic optimization.\nCoRR,\nabs/1412.6980.\nDiederik P Kingma and Max Welling. 2013.\nAuto-\nencoding\nvariational\nbayes.\narXiv\npreprint\narXiv:1312.6114.\nAmit\nKumar,\nRupjyoti\nBaruah,\nRajesh\nKumar\nMundotiya,\nand\nAnil\nKumar\nSingh.\n2020a.\nTransformer-based\nneural\nmachine\ntranslation\nsystem for Hindi – Marathi: WMT20 shared task.\nIn Proceedings of the Fifth Conference on Machine\nTranslation, pages 393–395, Online. Association for\nComputational Linguistics.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020b. Data augmentation using pre-trained trans-\nformer models. arXiv preprint arXiv:2003.02245.\nMike\nLewis,\nYinhan\nLiu,\nNaman\nGoyal,\nMar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nXin Li and Dan Roth. 2002. Learning question clas-\nsiﬁers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nSungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim,\nand Sungwoon Kim. 2019. Fast autoaugment. In\nNeurIPS.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, A. Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis.\nIn\nACL.\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\n2020a. Ssmba: Self-supervised manifold based data\naugmentation for improving out-of-domain robust-\nness. arXiv preprint arXiv:2009.10195.\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\n2020b.\nSSMBA: Self-supervised manifold based\ndata augmentation for improving out-of-domain ro-\nbustness.\nIn Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1268–1283, Online. Associa-\ntion for Computational Linguistics.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019.\nJustifying recommendations using distantly-labeled\nreviews and ﬁne-grained aspects. In EMNLP.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019.\nLan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nTim Rockt¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2015.\nReasoning about entailment with neural attention.\narXiv preprint arXiv:1509.06664.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015.\nImproving neural machine translation\nmodels with monolingual data.\narXiv preprint\narXiv:1511.06709.\nRaphael Shu, Jason Lee, Hideki Nakayama, and\nKyunghyun Cho. 2020.\nLatent-variable non-\nautoregressive neural machine translation with de-\nterministic inference using a delta posterior.\nPro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 34(05):8846–8853.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, A. Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In EMNLP.\nLichao Sun, Congying Xia, Wenpeng Yin, Tingting\nLiang, Philip S. Yu, and Lifang He. 2020. Mixup-\ntransformer: Dynamic data augmentation for nlp\ntasks. In COLING.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27:3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nVikas Verma, Alex Lamb, Christopher Beckham, Amir\nNajaﬁ, Ioannis Mitliagkas, David Lopez-Paz, and\nYoshua Bengio. 2019. Manifold mixup: Better rep-\nresentations by interpolating hidden states.\nIn In-\nternational Conference on Machine Learning, pages\n6438–6447. PMLR.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018a.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding.\nArXiv,\nabs/1804.07461.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018b.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJason Wei and Kai Zou. 2019a.\nEda:\nEasy\ndata augmentation techniques for boosting perfor-\nmance on text classiﬁcation tasks.\narXiv preprint\narXiv:1901.11196.\nJason Wei and Kai Zou. 2019b.\nEDA: Easy data\naugmentation techniques for boosting performance\non text classiﬁcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\nChina. Association for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019. Conditional bert contextual\naugmentation. In International Conference on Com-\nputational Science, pages 84–95. Springer.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V Le. 2019. Unsupervised data aug-\nmentation for consistency training. arXiv preprint\narXiv:1904.12848.\nMingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun\nLiu, and Zhi-Ming Ma. 2021.\nReweighting aug-\nmented samples by minimizing the maximal ex-\npected loss. ArXiv, abs/2103.08933.\nSoyoung Yoon, Gyuwan Kim, and Kyumin Park. 2021.\nSsmix: Saliency-based span mixup for text classiﬁ-\ncation. ArXiv, abs/2106.08062.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin,\nand David Lopez-Paz. 2017.\nmixup:\nBeyond\nempirical risk minimization.\narXiv preprint\narXiv:1710.09412.\nJake Zhao and Kyunghyun Cho. 2019.\nRetrieval-\naugmented convolutional neural networks against\nadversarial examples. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 11563–11571.\nA\nInterpolation on Different Domain\nFigure 3: Average unigram precision score of the in-\nterpolated sentences with varying mixup ratio values in\nthe SST-2 dataset.\nFigure 3 shows the average unigram precision\nscore of the interpolated sentences with respect to\nthe mixup ratio, α. We observe that the general\ntrend is identical to the average unigram precision\nscore measured in the Wikipedia corpus and TREC,\nas presented in Figure 2.\nB\nDetails of Downstream Datasets\nDataset\nTask\nLabel\nTrain\nTRECcoarse\nClassiﬁcation\n6\n5.5k\nTRECfine\nClassiﬁcation\n47\n5.5k\nSST-2\nSentiment\n2\n67k\nIMDB\nSentiment\n2\n46k\nRTE\nNLI\n2\n2.5k\nMNLI\nNLI\n3\n25k†\nQNLI\nNLI\n2\n25k†\nARclothing\nRatings\n5\n25k†\nYelp\nRatings\n5\n25k†\nTable 4: Dataset summary statistics. C, S, OOD, and,\nNLI in the task column represents classiﬁcation, senti-\nment analysis, out-of-distribution, and natural language\ninference respectively. Training sets marked with a †\nare sampled randomly from a larger dataset.\nWe present the details of downstream datasets\nin Table 4. For experiments, we report accuracy\nusing the ofﬁcial test set on TREC, IMDB, AR and\nYELP. For those where the label for the ofﬁcial test\nset is not available, SST-2, RTE, MNLI and QNLI,\nwe use the validation set as the test set. Further\ndetails are shown in Table 5. SST-2 and RTE are\nfrom the GLUE dataset (Wang et al., 2018b). We\nreport the number of the samples in each split in\nthe (train/validation) format. Rows for TREC and\nIMDB are reported in the (train/test) format.\nDataset\n#\nTRECcoarse\n5.5k / 500\nTRECfine\n5.5k / 500\nSST-2\n67k / 872\nIMDB\n25k / 25k\nRTE\n2.5k / 3k\nMNLI\n2.5k / 9.7k\nQNLI\n2.5k / 5.5k\nARclothing\nClothing\n25k / 2k\nWomen\n25k / 2k\nMen\n25k/ 2k\nBaby\n25k/ 2k\nShoes\n25k / 2k\nYELP\nAmerican\n25k / 2k\nChinese\n25k / 2k\nItalian\n25k / 2k\nJapanese\n25k / 2k\nTable 5: Summary of which split of dataset is used to\nreport the performance on downstream datasets. Col-\numn # indicates the number of samples in the data split\nas either in (train/validation) or in (train/test) based on\nthe split used to report the performance with.\nFor ID and OOD experiments, we use three\ndifferent datasets;\nARclothing,\nYelp,\nMovies.\nARclothing consists with ﬁve clothing categories:\nClothes, Women clothing, Men Clothing, Baby\nClothing, Shoes and Yelp, following the preprocess-\ning in Hendrycks et al. (2020), with four groups\nof food types: American, Chinese, Italian, and\nJapanese. For both datasets, we sample 25,000\nreviews for each category and the model predicts\na review’s 1 to 5 star rating. Movie dataset is con-\nsists with SST-2 and IMDB. We train the model\nseparately on each domain, then evaluate on all\ndomains to report the average in-domain and OOD\nperformance across 5 runs.\nC\nLabel Generation for LINDA\nC.1\nChoice of the Label Generation\nFor the full dataset setting, shown in Table 2, evalu-\nation accuracy of LINDA is reported with different\nlabeling method for each dataset. Note that we\nonly report accuracy with the interpolated labels in\nthe low-resources settings since (Ng et al., 2020b)\nhas already reported that with a small amount of\ntraining examples, using the psuedo labeling show\nlow accuracies. Using the interpolated labels for\nTREC-coarse and TREC-Fine shows the best per-\nformances in the full dataset setting. However, for\nIMDB and SST-2, utilizing the predicted soft la-\nbel shows the best performance for the full setting\nexperiment.\nFor NLI experiments, reported in Table 3, best\nLabel\nT\nTREC-Coarse\nTREC-Fine\nSST-2\nIMDB\nRTE\nQNLI\nMNLI-(m/mm)\nInterpolated\n1\n97.6±0.36\n91.4±0.26\n92.3±0.43\n89.2±0.15\n66.1±3.8\n88.0±0.4\n77.0±0.3/77.1±0.2\nInterpolated\n0.5\n97.4±0.21\n91.6±0.75\n92.2±0.37\n89.3±0.22\n66.8±1.4\n87.4±0.7\n76.2±0.4/77.1±0.4\npredicted\n-\n97.4±0.09\n90.1±0.39\n92.8±0.34\n89.3±0.18\n71.9±1.9\n88.6±0.2\n77.4±0.3/78.0±0.3\nTable 6: Effect of using the different label generation\nperformances for all three datasets are also with\nthe predicted soft label. Lastly, for the ID and\nOOD experiments in Table 3, using the predicted\nsoft label shows the best performance in ARclothing\nand Yelp datasets across all domains (ﬁve different\ndomains in ARclothing and four different domains\nin Yelp). In Movie, using the interpolated labels\nexplained in §3.4 shows the best result when train-\ning the model on IMDB and evaluating on SST-2.\nHowever, when training the model on SST-2 and\nevaluating on IMDB, using the predicted soft label\nshows the best result.\nC.2\nEffect of Label Sharpening\nAlthough we apply linear interpolation in LINDA,\nwe observed that unigram precision score of inter-\npolated sentence is on the side of the larger α value\nwell Figure 2 in §5. Based on this observation,\nwe investigate the effect of sharpening our label\n˜y by applying the sharpening with temperature T\n(Heaton, 2017). Denote ˜y is soft label of a class\ndistributions ˜yi and sharpening operation is deﬁned\nas operation\nSharpen( ˜\nym, T) = ˜yi1/T /\nL\nX\ni=1\nexp( ˜yi1/T ), (7)\nWhen T equals 1, the label does not change and as\nT gets closer to 0, Sharpen(˜y, T) approaches to a\nhard label. We show the effect of using sharpened\nlabels in Table 6.\nC.3\nEffect of the predicted Label\nAs our approach is under the unsupervised learning\nscheme, where only contextual information is con-\nsidered, interpolated labels suggested in 3.4 have\nrisk of being mismatched with the correct label.\nTherefore, we investigate using the predicted label\nwhich is evaluated with the model taught with the\noriginal training data, which we call teacher, to\nmakes the model to be susceptible to the damaged\nlabel issue. Once the teacher model is trained on\nthe original set of unaugmented data, we evalu-\nate augmented samples and replace the label infor-\nmation ˜y with the output probability of a trained\nteacher model. In Table 6, we show the difference\nin accuracy when the interpolated label is used ver-\nsus when the predicted label is used. It shows that\nusing the predicted label signiﬁcantly improves the\nperformance in SST-2 and NLI datasets, despite of\nusing the interpolated label even hurting the perfor-\nmance.\nD\nID and OOD Results over All Domains\nTable 3 in §6.3 shows the average ID and OOD\naccuracy (%) over all categories. However, unlike\nthe Movie dataset, where it consists of only two\ndomains; SST-2 and IMDB, ARclothing and Yelp\ndatasets consist of multiple domains. ARclothing\nconsists with ﬁve clothing categories: Clothes,\nWomen clothing, Men Clothing, Baby Clothing,\nShoes and Yelp, following the preprocessing in\nHendrycks et al. (2020), with four groups of food\ntypes: American, Chinese, Italian, and Japanese.\nWe report accuracy (%) over all categories along\nwith standard deviation values over 5 random runs\nin the ARclothing and Yelp datasets in Table 8 and\nTable 9.\nE\nEffect of the Mixup Ratio with\nWikipedia Sentences\nWe present an example of the interpolated sen-\ntences from Wikipedia sentences with the varying\nvalues of alpha in Table 7. Two reference sen-\ntences are randomly drawn from English Wikipedia\ncorpus. As alpha gets closer to 1, the interpolated\nsentence resembles the sentence 1 more and as\nalpha gets closer to 0, it resembles the sentence 2\nmore. Moreover, we see that LINDA is capable of\ngenerating sentences using tokens that have never\nappeared in the reference sentences as well. This\ncapability of LINDA comes from leveraging the\npower of pre-trained generation model, BART, on\nthe vast amount of English Wikipedia sentences\nwith the reconstruction loss.\nSentence 1\nSentence 2\nHenry tried, with little success, to reacquire the property\nRichard had sold, and had to live modestly as a gentle\nman, never formally taking title to the earldom\nKobilje Creek, a left tributary of the Ledava River, ﬂows\nthrough it.\nα\nGenerated Sentences\n0.1\nKobilje Creek, a left tributary of the Ledava River, ﬂows through it.\n0.2\nKobilje Creek, a left tributary of the Ledava River, ﬂows through it to the north.\n0.3\nKobilje Creek, a left tributary of the River Ledava, ﬂows through it, as does a small river.\n0.4\nKobilje, a small creek, left the tributary of the Ledava River, and it ﬂows through the town.\n0.5\nKenny, with a little help from his father, was able to take over the family business, which had been run by his\nmother.\n0.6\nHenry tried, with little success, to reacquaint himself with the property, and had to live as a modest earl.\n0.7\nHenry tried, with little success, to reacquaint himself wit h the property, and had to live as a modest gentleman,\nnever formally taking part in the title.\n0.8\nHenry tried, with little success, to reacquaint himself with the property, and had to live as a modest gentleman,\nnever formally taking title to the earldom\n0.9\nHenry tried, with little success, to reacquire the property Richard had sold, and had to live modestly as a\ngentleman, never formally taking title to the title.\nTable 7: Example of a series of interpolated sentences by adjusting the value of alpha given two input sentences.\nVanilla\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n72.1±0.48\n-\n-\n-\n-\nWomen\n69.2±0.73\n69.6±0.40\n-\n-\n-\nMen\n72.4±0.64\n72.0±1.15\n72.7±0.50\n-\n-\nBaby\n77.1±0.35\n76.4±0.92\n77.5±1.00\n78.0±0.44\n-\nShoes\n71.6±0.75\n71.3±0.44\n72.4±0.82\n70.5±0.51\n73.2±0.61\nEDA\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n71.9±0.30\n-\n-\n-\n-\nWomen\n68.1±0.33\n69.5±0.58\n-\n-\n-\nMen\n71.6±0.37\n71.5±1.04\n71.8±0.29\n-\n-\nBaby\n76.8±0.42\n76.2±0.28\n76.1±0.30\n77.9±0.44\n-\nShoes\n71.6±0.72\n72.0±0.69\n71.3±0.57\n70.4±0.82\n71.9±0.23\nSSMBA\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n71.7±0.24\n-\n-\n-\n-\nWomen\n68.9±0.65\n69.2±0.22\n-\n-\n-\nMen\n72.3±0.65\n71.1±0.73\n71.8±0.26\n-\n-\nBaby\n76.7±0.75\n76.3±0.56\n76.5±0.64\n77.6±0.26\n-\nShoes\n72.1±0.75\n72.1±0.62\n71.8±0.75\n70.4±0.58\n72.8±0.34\nSSMBAsoft\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n72.9±0.41\n-\n-\n-\n-\nWomen\n69.3±0.19\n70.0±0.35\n-\n-\n-\nMen\n72.6±0.66\n72.6±0.58\n72.8±0.40\n-\n-\nBaby\n77.9±0.30\n77.6±0.66\n77.4±0.71\n78.4±0.75\n-\nShoes\n72.5±0.52\n72.6±0.54\n72.2±0.79\n70.9±0.64\n72.7±0.58\nMixup\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n72.0±0.42\n-\n-\n-\n-\nWomen\n68.3±0.51\n69.6±0.25\n-\n-\n-\nMen\n71.7±0.41\n72.3±0.50\n72.2±0.30\n-\n-\nBaby\n76.3±1.09\n76.7±0.65\n76.7±0.74\n77.8±0.25\n-\nShoes\n71.6±0.72\n72.1±0.50\n72.0±0.66\n70.3±0.63\n73.0±0.46\nLINDA\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n71.4±0.34\n-\n-\n-\n-\nWomen\n68.4±0.58\n69.2±0.46\n-\n-\n-\nMen\n72.4±0.48\n71.6±0.65\n72.4±0.33\n-\n-\nBaby\n77.6±0.26\n77.0±1.16\n76.4±0.92\n77.7±0.50\n-\nShoes\n72.0 ±0.49\n71.9±0.67\n71.6±0.69\n70.4±0.82\n73.1±0.19\nLINDAsoft\nClothing\nWomen\nMen\nBaby\nShoes\nClothing\n72.8±0.27\n-\n-\n-\n-\nWomen\n69.4±0.59\n69.9±0.43\n-\n-\n-\nMen\n71.7±0.31\n72.6±0.39\n72.7±0.44\n-\n-\nBaby\n77.3±0.39\n77.2±0.18\n77.0±0.12\n78.5±0.43\n-\nShoes\n72.5±0.46\n71.7±0.64\n72.0±0.26\n70.4±0.82\n73.4±0.61\nTable 8: Accuracy (%) of BERTBASE in the ARclothing dataset. Each column represents the category where a\nmodel is trained with and each row represents the category where a model is evaluated on. All the results are\nreported as “mean(±std)” across 5 random runs.\nVanilla\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n66.2 ±0.49\n-\n-\n-\nChinese\n63.0±1.18\n64.7±0.58\n-\n-\nItalian\n66.8±0.52\n65.7±0.40\n66.4±0.34\n-\nJapanese\n65.5±1.34\n66.4±0.99\n64.9±0.48\n66.6±0.52\nEDA\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n65.3±0.20\n-\n-\n-\nChinese\n63.1±072\n63.4±0.41\n-\n-\nItalian\n65.5±0.59\n64.6±1.48\n65.2±0.75\n-\nJapanese\n65.3±0.71\n65.8±0.82\n64.3±1.63\n65.6 ±0.38\nSSMBA\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n65.8±0.93\n-\n-\n-\nChinese\n64.4±0.80\n64.1±0.16\n-\n-\nItalian\n65.5±1.20\n65.2±0.45\n66.4±0.50\n-\nJapanese\n65.3±0.39\n66.3±0.82\n65.2±1.31\n66.6±0.29\nSSMBAsoft\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n66.6±0.69\n-\n-\n-\nChinese\n63.9±1.02\n65.1±0.53\n-\n-\nItalian\n66.5±0.46\n65.6±0.58\n67.4±0.58\n-\nJapanese\n66.1±0.69\n67.5±0.98\n65.4±0.62\n67.1±0.18\nMixup\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n65.6±0.59\n-\n-\n-\nChinese\n63.5±1.05\n64.5±0.58\n-\n-\nItalian\n66.0±1.17\n65.1±0.63\n66.3±0.52\n-\nJapanese\n65.9±0.72\n66.0±0.67\n65.3±0.93\n66.2±0.38\nLINDA\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n65.5±0.75\n-\n-\n-\nChinese\n63.7±0.76\n64.0±0.37\n-\n-\nItalian\n66.3±0.69\n65.3±0.98\n66.1±0.21\n-\nJapanese\n65.8±0.61\n65.5±1.02\n64.3±1.07\n66.7±0.21\nLINDAsoft\nAmerican\nChinese\nItalian\nJapanese\nAmerican\n66.2±0.72\n-\n-\n-\nChinese\n64.2±0.42\n64.7±0.51\n-\n-\nItalian\n66.2±0.86\n65.9±0.44\n67.3±0.98\n-\nJapanese\n65.7±0.56\n67.0±0.75\n64.6±1.35\n66.8±0.45\nTable 9: Accuracy (%) of BERTBASE in the Yelp dataset. Each column represents the category where a model is\ntrained with and each row represents the category where a model is evaluated on. All the results are reported as\n“mean(±std)” across 5 random runs.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2021-12-28",
  "updated": "2021-12-28"
}