{
  "id": "http://arxiv.org/abs/2112.15131v1",
  "title": "Resource-Efficient Deep Learning: A Survey on Model-, Arithmetic-, and Implementation-Level Techniques",
  "authors": [
    "JunKyu Lee",
    "Lev Mukhanov",
    "Amir Sabbagh Molahosseini",
    "Umar Minhas",
    "Yang Hua",
    "Jesus Martinez del Rincon",
    "Kiril Dichev",
    "Cheol-Ho Hong",
    "Hans Vandierendonck"
  ],
  "abstract": "Deep learning is pervasive in our daily life, including self-driving cars,\nvirtual assistants, social network services, healthcare services, face\nrecognition, etc. However, deep neural networks demand substantial compute\nresources during training and inference. The machine learning community has\nmainly focused on model-level optimizations such as architectural compression\nof deep learning models, while the system community has focused on\nimplementation-level optimization. In between, various arithmetic-level\noptimization techniques have been proposed in the arithmetic community. This\narticle provides a survey on resource-efficient deep learning techniques in\nterms of model-, arithmetic-, and implementation-level techniques and\nidentifies the research gaps for resource-efficient deep learning techniques\nacross the three different level techniques. Our survey clarifies the influence\nfrom higher to lower-level techniques based on our resource-efficiency metric\ndefinition and discusses the future trend for resource-efficient deep learning\nresearch.",
  "text": "RESOURCE-EFFICIENT DEEP LEARNING: A SURVEY ON\nMODEL-, ARITHMETIC-, AND IMPLEMENTATION-LEVEL\nTECHNIQUES\nA PREPRINT\nJunKyu Lee\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\njunkyu.lee@qub.ac.uk\nLev Mukhanov\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\nl.mukhanov@qub.ac.uk\nAmir Sabbagh Molahosseini\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\na.sabbaghmolahosseini@qub.ac.uk\nUmar Minhas\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\nu.minhas@qub.ac.uk\nYang Hua\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\ny.hua@qub.ac.uk\nJesus Martinez del Rincon\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\nj.martinez-del-rincon@qub.ac.uk\nKiril Dichev\nUniversity of Cambridge\nCambridge, England, UK\nkiril.dichev@gmail.com\nCheol-Ho Hong ∗\nChung-Ang University\nSeoul, South Korea\ncheolhohong@cau.ac.kr\nHans Vandierendonck\nQueen’s University Belfast\nBelfast, Northern Ireland, UK\nh.vandierendonck@qub.ac.uk\nABSTRACT\nDeep learning is pervasive in our daily life, including self-driving cars, virtual assistants, social\nnetwork services, healthcare services, face recognition, etc. However, deep neural networks demand\nsubstantial compute resources during training and inference. The machine learning community has\nmainly focused on model-level optimizations such as architectural compression of deep learning\nmodels, while the system community has focused on implementation-level optimization. In between,\nvarious arithmetic-level optimization techniques have been proposed in the arithmetic community.\nThis article provides a survey on resource-efﬁcient deep learning techniques in terms of model-,\narithmetic-, and implementation-level techniques and identiﬁes the research gaps for resource-efﬁcient\ndeep learning techniques across the three different level techniques. Our survey clariﬁes the inﬂuence\nfrom higher to lower-level techniques based on our resource-efﬁciency metric deﬁnition and discusses\nthe future trend for resource-efﬁcient deep learning research.\nKeywords deep learning, neural networks, resource efﬁciency, arithmetic utilization\n1\nIntroduction\nRecent improvements in network and storage devices have provided the machine learning community with the\nopportunity to utilize immense data sources, leading to the golden age of AI and deep learning [22]. Since modern deep\nneural networks (DNNs) require considerable computing resources and are deployed in a variety of compute devices,\nranging from high-end servers to mobile devices with limited computational resources, there is a strong need to realize\neconomical DNNs that ﬁt within the resource constraints [118, 150, 151]. Resource-efﬁcient deep learning research\nhas vividly been carried out independently in various research communities including the machine learning, computer\narithmetic, and computing system communities. Recently, DeepMind proposed the resource-efﬁcient deep learning\nbenchmark metric which is the accuracy along with the required memory footprint and number of operations [78].\n∗Corresponding Author\narXiv:2112.15131v1  [cs.LG]  30 Dec 2021\narXiv Template\nA PREPRINT\nNeural Network\nCNN\nModel-Level \nResource-Efficient \nTechniques\nArithmetic-Level\nResource-Efficient \nTechniques\nImplementation-Level\nResource-Efficient  \nTechniques\nAbstract Resource Efficiency\nPhysical Resource Efficiency\nAccuracy/Parameter\nAccuracy/Operation\nAccuracy/(Memory \nFootprint)\nAccuracy/(Core Utilization) \nAccuracy/(Memory Access) \nAccuracy/Joule \nResource-Efficient \nDeep Learning Techniques Hierarchy \nMultiple Resource Efficiency Metrics\nResource Efficiency Metric Hierarchy \nFigure 1: Survey on resource-efﬁcient deep learning techniques based on resource efﬁciency metrics.\nWith this regard, this article surveys resource-efﬁcient techniques for deep learning based on the three-level categoriza-\ntion: the model-, arithmetic-, and implementation-level techniques along with various resource efﬁciency metrics as\nshown in Fig. 1. Our resource efﬁciency metrics include the accuracy per parameter, operation, memory footprint, core\nutilization, memory access, and Joule. For the resource-efﬁciency comparison between the baseline DNN and a DNN\nutilizing resource-efﬁcient techniques, the accuracy should be equivalent between the two DNNs. In other words, it is\nnot fair to compare the resource efﬁciency between a DNN producing a high accuracy and a DNN producing a low\naccuracy since the resource efﬁciency is signiﬁcantly higher in a low performing DNN based on our resource metrics.\nWe categorize the resource-efﬁcient techniques into the model-level resource-efﬁcient techniques if they compress\nthe DNN model sizes; the arithmetic-level resource-efﬁcient techniques if they utilize reduced precision arithmetic\nand/or customized arithmetic rules; the implementation-level resource-efﬁcient techniques if they apply hardware\noptimization techniques to the DNNs (e.g., locating local memory near to processing elements) to improve physical\nresource efﬁciency such as the accuracy per compute resource and per Joule.\nIn Fig. 1, Convolutional Neural Networks (CNNs) can be considered as a resource-efﬁcient technique since they improve\nthe accuracy per parameter, per operation, and per memory footprint, compared to fully connected neural networks. The\nresource-efﬁciency from CNNs can be further improved by applying the model-, arithmetic-, and implementation-level\ntechniques. The model- and arithmetic-level techniques can affect the accuracy since they affect either the DNN model\nstructure or the arithmetic rule, while the implementation-level techniques generally do not affect the accuracy. The\nmodel-level techniques mostly contribute to improving abstract resource efﬁciency, while the implementation-level\ntechniques contribute to improve physical resource efﬁciency. Without careful consideration at the intersection between\nthe model- and the implementation-level techniques, a DNN model compressed by the model-level techniques might\nrequire signiﬁcant runtime compute resources, incurring longer training time and inference latency than the original\nmodel [108, 31]. Thus, to optimize the performance and energy efﬁciency on a particular hardware, it is essential to\nconsider the joint effect of the model, arithmetic and implementation-level optimizations. Our survey focuses on the\nthree different level resource-efﬁcient techniques for CNN architectures, since CNN is one of the most widely used\ndeep learning architectures [91].\nRelated survey works are as follows. Sze et al. [145] provided a comprehensive tutorial and survey towards efﬁcient\nprocessing of DNNs, discussing DNN architectures, software frameworks (e.g., PyTorch, TensorFlow, Keras, etc.), and\nthe implementation methods optimizing Multiply-and-Accumulate Computations (MACs) of DNNs on given compute\nplatforms. Cheng et al. [35] conducted a survey on the model compression techniques including pruning, low-rank\nfactorization, compact convolution, and knowledge distillation. Deng et al. [42] discussed joint model-compression\nmethods which combined multiple model-level compression techniques, and their efﬁcient implementation on particular\ncomputing platforms. Wang et al. [157] provided a survey on custom hardware implementations of DNNs and evaluated\ntheir performances using the Rooﬂine model of [162].\nUnlike the previous survey works, we conduct a comprehensive survey on resource-efﬁcient deep learning techniques\nin terms of the model-, arithmetic-, and implementation-level techniques by clarifying which resource-efﬁciency can\nbe improved with particular techniques according to our resource-efﬁciency metrics as deﬁned in Section 2.2. Such\nclariﬁcation would provide machine learning engineers, computer arithmetic designers, software developers, and\nhardware manufacturers with useful information to improve particular resource efﬁciency for their DNN applications.\nBesides, since we notice that fast wireless communication and edge computing development affects deep learning\napplications [180], our survey also includes cutting-edge resource-efﬁcient techniques for distributed AI such as early\n2\narXiv Template\nA PREPRINT\nx1\nx2\nx3\nyout\nA1,(4) \n(a) Single Artificial Neuron \nor Single Perceptron\n(b) Multi-Layer Perceptron or Neural Network\nw1\nw2\nw3\nw1,1,(2)\nA1,(2)\nx1,(1)\nx2,(1)\nx3,(1)\nw1,2,(2)\nw1,3,(2)\nw1,4,(2)\nA2,(2)\nA3,(2)\nA4,(2)\nA1,(3)\nA2,(3)\ns1,(2)\ns2,(2)\ns3,(2)\ns4,(2)\ns1,(3)\ns2,(3)\ns1,(4)\n∑\n∑\n∑\n∑\n∑\n∑\n∑\n∑\nA\nyout\nFigure 2: Perceptron and neural network model.\nexiting techniques [150, 151]. The holistic and multi-facet view for resource-efﬁcient techniques for deep learning\nfrom our survey would allow for a better understanding of the available techniques and, as consequence, a better global\noptimization, compared to previous survey works. The main contributions of our paper include:\n• This paper ﬁrst provides a comprehensive survey coverage of the recent resource-efﬁcient techniques for\nDNNs in terms of the model-, arithmetic-, and implementation-level techniques.\n• To the best of our knowledge, our paper is the ﬁrst to provide comprehensive survey on arithmetic-level\nutilization techniques for deep learning.\n• This paper utilizes multiple resource efﬁciency metrics to clarify which resource efﬁciency metrics each\ntechnique improves.\n• This paper provides the inﬂuence of resource-efﬁcient deep learning techniques from higher to lower level\ntechniques (refer to Fig. 1).\n• We discuss the future trend for the resource-efﬁcient deep learning techniques.\nWe discuss our resource efﬁciency metrics for deep learning in Section 2, the model-level resource-efﬁcient techniques in\nSection 3, the arithmetic-level techniques in Section 4, the implementation-level techniques in Section 5, the inﬂuences\nbetween different-level techniques in Section 6, the future trend in Section 7, and conclusion in Section 8. Our paper\nexcludes higher-level training procedure manipulation techniques such as one-pass ImageNet [78], bag of freebies [20],\ndata augmentation, etc.\n2\nBackground on Deep Learning and Resource-Efﬁciency\nThis section describes deep learning overview and resource efﬁciency metric, as preparatory to the description of\nresource-efﬁcient techniques via the three different levels.\n2.1\nDeep Learning Overview\nDeep learning is deﬁned as “learning multiple levels of representation” [17] and often utilizes DNNs to learn the\nmultiple levels of representation. DNNs are trained using the training data set, and their prediction accuracy is evaluated\nusing the test dataset [5]. In this section, we describe the perceptron model (i.e., artiﬁcial neuron) ﬁrst and then DNNs\nlater.\n2.1.1\nPerceptron Model:\nThe McCulloch and Pitts’s neuron (a.k.a. M-P neuron) [112], proposed in 1943, was a system mimicking the neuron in\nthe nervous system, receiving multiple binary inputs and producing one binary output based on a threshold. Inspired by\nthe work of [112], Rosenblatt [128] proposed the “perceptron” model consisting of multiple weights, a summation, and\nan activation function as shown in Fig. 2.(a). Eq. (1) describes a perceptron’s ﬁring activity yout using the inputs xi\nassociated with their weights wi, where the i represents an index to indicate one of multiple inputs.\nyout =\n\u001a1,\nif (Σnin\ni=1wi × xi > threshold) or (Σnin\ni=1wi × xi + bias > 0)\n0,\nif (Σnin\ni=1wi × xi ≤threshold) or (Σnin\ni=1wi × xi + bias ≤0),\n(1)\nwhere nin is the number of the inputs. The function that determines the ﬁring activity is referred to as the activation\nfunction, and the bias is in proportion to the probability of the ﬁring activation [116]. Since single perceptron model\n3\narXiv Template\nA PREPRINT\nis suitable only for linearly separable problems, a Multi-Layer Perceptron (MLP) model can be used for non-linearly\nseparable problems as shown in Fig. 2.(b), where wj,k,(i) represents a weight linking the jth neuron in the (i −1)th\nlayer to the kth neuron in the ith layer. The signal sj,(i) in Fig. 2 follows Eq. (2):\nsj,(l) = Σ\nn(l−1)\nin\ni=1\n(wi,j,(l) × xi,(l−1)) = (WT\n(l)x(l−1))j,\n(2)\nand xj,(l) = θP (sj,(l)), where θP (s) is a perceptron’s activation function that follows Eq. (1) (i.e., step function), and\nW(l) consists of the matrix elements, wi,j,(l)s, for the ith row and the jth column.\n2.1.2\nDeep Neural Network:\nSince it requires tremendous efforts for human to optimize MLPs manually, neural networks that adopts a soft threshold\nactivation function θN (e.g., sigmoid, ReLU, etc.) were proposed to train the weights according to the training data\n[163, 160]. [116] notice that neural network is sometimes interchangeably used with MLP. For clarity, we name an\nalgorithm as an MLP if it utilizes a step function for its activation functions and as a neural network if it utilizes a soft\nthreshold function. In Fig. 2.(b), the output from the ith neuron at the lth layer in a neural network employing a soft\nthreshold activation function, θN(·), can be represented as Eq. (3):\nxi,(l) = θN(si,(l)).\n(3)\nA neural network allows the weights and the biases to be trained using the backpropagation [5]. A neural network\nmodel is often referred to as a feed-forward model in that the weights always link the neurons in the current layer to the\nneurons in the very next layer. In a neural network, the middle layers located between the input and output layer, are\noften referred to as hidden layers (e.g., two hidden layers in Fig. 2.(b)). A neural network with multiple hidden layers is\nreferred to as a DNN [145].\n2.1.3\nTraining - Backpropagation:\nIn the forward pass, the neuron outputs are propagated in forward direction based on the matrix-vector multiplications\nas shown in Eq. (2). Likewise, the weights and the biases can be trained in backward direction using matrix-vector\nmultiplications. This method is called as the backpropagation. The backpropagation method consists of the three steps,\nallowing a gradient descent algorithm to be implemented efﬁciently on computers. It ﬁnds the activation gradients,\nδj,(l)s (i.e., the gradients with respect to all the signals, sj,(l)s, in Eq (2)), in step 1, ﬁnds the weight gradients (i.e., the\ngradients with respect to all the weights) using the activation gradients in step 2, and ﬁnally updates the weights using\nthe weight gradients in step 3. All δj,(l−1)s are found in backward direction using the matrix-vector multiplications\nby replacing WT\n(l) to W(l) and xj,(l−1) to δj,(l) in Eq. (3). After all activation gradients have been found, the weight\ngradients can be found. Finally, the weights are updated using the weight gradients. The backpropagation requires\nadditional storage to store all the weights and activation values. Once a DNN is trained, the DNN is used for the\ninference task using the trained weights. Please refer to [5] for the further details for the backprogation method. After a\nDNN being trained, the DNN’s accuracy is evaluated using the validation dataset which is unseen from the training.\n2.1.4\nConvolutional Neural Network:\nSince CNN is one of the most successful and widely used deep learning architectures [91], we exemplify CNN as a\nrepresentative deep learning architecture. CNN employs multiple convolutional layers, and each convolutional layer\nutilizes multiple ﬁlters to perform convolutions independently with respect to each ﬁlter as shown in Fig. 3, where\na ﬁlter at a convolutional layer consists of as many kernels as the number of the channels at the input layer (e.g., 3\nkernels per ﬁlter in Fig. 3). For example, each 3 × 3 ﬁlter has 9 weight parameters and slides from the top-left to the\nbottom-right position, generating 4 output values with respect to each position (e.g., top-left, top-right, bottom-left,\nand bottom-right position) in Fig. 3. The outputs from the convolutions are often called feature maps and are fed into\nactivation functions. Modern CNNs such as ResNet [67] often employ a batch normalization layer [83] between the\nconvolutional layer and the ReLU layer to improve the accuracy.\nThe CNN is a resource-efﬁcient deep learning architecture in terms of the accuracy per parameter and per operation by\nleveraging the two properties, local receptive ﬁeld and shared weights [93]. For example, performing convolutions\nusing multiple small kernels extracts multiple local receptive features from the input image during training, and each\nkernel contains some meaningful pattern from the input image after being trained [179]. Thus, CNN utilizes much\nfewer weights than fully connected DNN, since the kernel’s height and weight are generally much smaller than the\nheight and the width at the input layer, leading to the improved resource efﬁciency. Notice that a convolutional layer\nbecomes a fully connected layer if the height and the width at the input layer are matched with each kernel’s height and\n4\narXiv Template\nA PREPRINT\nk1,1,1\na1,1,1\na4,4,1\nk3,3,1\n.\n.\n.\nk1,1,1\na1,1,1\na4,4,1\nk3,3,1\nChannel #1\nChannel #M\n.\n.\n.\nFilter #1\nCovolution\nFilter #M\nCovolution\n.\n.\n.\nFeature Maps \nConvolutions by Multiple Filters\nInput\nLayer\nE.g., \nPrevious\nReLU\nLayer\nBatch Normalization \nLayer (Optional)\nReLU\nLayer\nConvolutional Layer\nFigure 3: Convolution operations in a CNN.\nwidth. The number of total weights in a layer in a CNN is much less than used in a fully connected neural network,\nsince the local receptive weights are shared over the entire feature on a layer.\nTraining CNN also utilizes the backprogation using the transpose of kernel matrices in a ﬁlter to update the weights\nin the ﬁlter. The mini-batch gradient descent algorithm is widely used to train CNNs, which utilizes part of training\ndata to update the weights per iteration. The number of data used per iteration is often referred to as the batch size B\n(e.g., B = 64 or 128). Each epoch consumes the entire training data, consisting of N/B iterations, where N is the\nnumber of the entire training data. The mini-batch gradient descent method is a resource-efﬁcient training algorithm in\nterms of the accuracy per operation, compared to the batch gradient descent method that utilizes entire training dataset\nper iteration (i.e., the batch gradient descent method updates the weights per epoch). For parallel backpropagation\nimplementation with respect to B data samples in one mini-batch, all the weights and all the activation values using B\ntraining samples should be stored to update the weights per the mini-batch iteration, requiring B× additional storage,\ncompared to the backpropagation using a stochastic gradient descent algorithm which updates the weights per training\nsample. Our paper refers to the term, DNN, as any neural network with several hidden layers, including CNN.\n2.2\nResource Efﬁciency Metrics for Deep Learning\nRecently, researchers from DeepMind [78] proposed the metrics for resource-efﬁcient deep learning benchmarks,\nincluding the top-1 accuracy, the required memory footprint for training, and the required number of ﬂoating operations\nfor training, and evaluated the resource-efﬁciency for deep learning applications with jointly considering the three\nmetrics. The Rooﬂine model [162] discussed attainable performance in terms of the operational intensity deﬁned as the\nnumber of ﬂoating point operations per DRAM access. Motivated by [78, 162], our resource efﬁciency metrics include\nthe accuracy per parameter, per operation, per memory footprint, per core utilization, per memory access, and per Joule\nas shown in Fig. 1.\n2.2.1\nAccuracy per Parameter:\nWe consider the accuracy per parameter (i.e., weight) for a resource-efﬁciency metric. The accuracy per parameter is an\nabstract resource efﬁciency metric since higher accuracy per parameter does not always imply higher physical resource\nefﬁciency after its implementation [78, 1].\n2.2.2\nAccuracy per Operation:\nWe consider the accuracy per arithmetic operation for a resource-efﬁciency metric. This is also an abstract metric, since\nit can be evaluated prior to the implementation.\n2.2.3\nAccuracy per Compute Resource:\nThe instruction-driven architecture such as CPU or GPU requires substantial memory accesses due to instruction fetch\nand decode operations, while the data-driven architecture such as ASIC or FPGA can minimize the number of memory\naccesses, resulting in energy efﬁciency. We further categorize such compute resource into core utilization, memory\nfootprint, and memory access, required to operate a DNN on given computing platforms. For example, the memory\naccess can be interpreted as GPU DRAM access (or off-chip memory) for a GPU and as FPGA on-chip memory access\n(or off-chip memory) for a FPGA.\n5\narXiv Template\nA PREPRINT\nFigure 4: Categorization for model-level resource-efﬁcient techniques.\na. Accuracy per Core Utilization: The core utilization in this paper represents the utilization percentage of the processing\ncores or processing elements.\nb. Accuracy per Memory Footprint: The accuracy per memory footprint is related to both physical and abstract resource\nefﬁciency as shown in Fig. 1. The memory footprint is in proportion to the number of the parameters, but it can be\nvaried according to a precision-level applied for arithmetic. For example, if a half precision arithmetic is applied for a\ndeep learning, the memory footprint can be saved by 2×, compared to a single precision arithmetic deep learning.\nc. Accuracy per Memory Access: A computing kernel having a low operational intensity cannot approach a peak\nperformance deﬁned by hardware speciﬁcation since the data supply rate from DRAM to CPU cannot catch up with the\ndata consumption rate by arithmetic operations. Such kernels are called “memory bound kernels” in [162]. Other type\nkernels are named “compute bound kernels” that can approach a peak performance deﬁned by hardware speciﬁcation.\nUtilizing reduced precision arithmetic can improve the performance for both memory bound kernels by improving\nthe data supply rate from DRAM to CPU and compute bound kernels by increasing word-level parallelism on SIMD\narchitectures [95].\n2.2.4\nAccuracy per Joule:\nThe dynamic power consumption is the main factor to determine energy consumption required for computationally\nintensive tasks (e.g., DNN training/inference tasks). The dynamic power consumption, PD, follows:\nPD = #T T R × CCP × V 2\nCP × fCP ,\n(4)\nwhere #T T R is the number of toggled transistors, CCP is an effective capacitance, VCP is an operational voltage, and\nfCP is an operational frequency for a given computing platform CP. Generally, the required minimum operational\nvoltage is in proportion to the operational frequency. Therefore, adapting the frequency to the voltage scaling can save\npower cubically (a.k.a. Dynamic Voltage Frequency Scaling [137]). For example, minimizing the operations required\nto operate DNN during runtime contributes to minimizing #T T R, resulting in power reduction and energy saving; we\ndiscuss further the resource-efﬁcient techniques leveraging this in Section 5.2.1.\n3\nModel-Level Resource-Efﬁcient Techniques\nThe model-level resource-efﬁcient techniques, mostly developed from machine learning community, aim at reducing\nthe DNN model size to ﬁt the models to resource-constrained systems such as mobile devices, IoTs, etc. We categorize\nthe model-level resource-efﬁcient techniques as shown in Fig. 4.\n3.1\nWeight Quantization\nThe weight quantization techniques quantize the weights with a smaller number of bits, improving the accuracy per\nmemory footprint. The training procedure should be amended according to the weight quantization schemes.\n3.1.1\nBinary Weight Quantization:\nBinaryConnect training scheme [38] allowed a DNN to represent the weights using one bit. In step 1, the weights are\nencoded to {−1, 1} using a stochastic clipping function. In step 2, the forward pass is performed using the encoded\nbinary weights. In step 3, backpropagation seeks all activation gradients using full precision. In step 4, the weights are\n6\narXiv Template\nA PREPRINT\nupdated using full precision, and the training procedure goes back to step 1 for the training using the next mini-batch.\nThis method required only one bit to represent the weights, thus improving the accuracy per memory footprint. In\naddition, the binary weight quantization also removed the need for multiplication arithmetic operations for MAC\noperations, improving the accuracy per operation. Moreover, if the activations are also quantized to the binary value, all\nMAC operations in DNN can be implemented only with XNOR gates and a counter [39, 124].\n3.1.2\nTernary Weight Quantization:\nLi et al. [98] proposed ternary weight networks that utilized ternary weights, improving accuracy, compared to the\nbinary weight networks. All the weights on each layer were quantized into three values, requiring only two bits to\nrepresent the quantized weights. Overall training procedure was similar to [38], but with ternary valued weights instead\nof the binary weights. The ternary weight network showed equivalent accuracy to various single precision networks\nwith MNIST, CIFAR-10 and ImageNet, while the binary weight quantization [38] showed minor accuracy loss. Zhu et\nal. [185] scaled the ternary weights independently for each layer with a layer-wise scaling approach, improving the\naccuracy further, compared to [98].\n3.1.3\nMixed Quantization:\n[81] proposed “Quantized Neural Network” that quantizes the activations and the weights to arbitrary lower precision\nformat. For example, quantizing the weights to 1 bit and the activations to 2 bits improved the accuracy, compared to\nthe binarized DNN of [39].\n3.2\nPruning\nPruning unimportant neurons, ﬁlters, and channels can save computational resources for deep learning applications\nwithout sacriﬁcing accuracy, improving the accuracy per parameter and per operation. Coarse-grained pruning methods\nsuch as pruning ﬁlters or channels are not ﬂexible to achieve a prescribed accuracy, but can be implemented efﬁciently\non hardware [104], implying higher physical resource efﬁciency than ﬁne-grained pruning such as pruning weights.\nNotice that such pruning methods can degrade conﬁdence scores without careful re-training, even though they did not\naffect top-1 accuracy [174].\n3.2.1\nPruning Weights:\nIn 1990, LeCun et al. proposed a weight pruning method to generate sparse DNNs with fewer weights without losing\naccuracy [94]. In 2015, the weight pruning approach was revisited [66], and the weights were pruned based on their\nmagnitudes after training – the pruned DNNs were retrained to regain the lost accuracy. The pruning and re-training\nprocedures could be performed iteratively to prune the weights further. This method reduced the number of weights\nof AlexNet by 9× without losing accuracy. In 2016, Guo et al. [58] noticed that pruning wrong weights could not be\nrevived, and proposed to prune and splice the weights per mini-batch training to minimize the risk from pruning wrong\nweights from previous mini-batch training. For example, the pruned weights were also participated in the weight update\nprocedure during the backpropagation and were restored when they were re-considered as the important weights. In\n2017, Yang et. al [170] proposed an energy-aware weight pruning method in which the energy consumption of a CNN\nwas directly measured to guide the pruning process. In 2019, Frankly et al. [50] demonstrated that some of pruned\nmodels outperformed the original model by retraining the pruned models with replacing the survived weights with the\ninitial random weights used for training the original model.\n3.2.2\nPruning Neurons:\nInstead of pruning individual weights, pruning a neuron can remove a group of the weights belonging to the neuron\n[143, 110, 77, 178]. In 2015, [143] pruned the redundant neurons having similar weight values in a trained DNN model.\nFor example, the weights in a baseline neuron were compared to the weights in other neurons at the same layer, and\nthe neurons having similar weights to the baseline neuron were fused to the baseline neuron based on a Euclidean\ndistance metric in the weight values between the two neurons. In 2016, [110] pruned the redundant neurons based on\nthe “determinantal point process” metric. Hu et. al [77] measured the average percentage of zero activations per neuron\nand pruned the neurons having a high percentage of zero activations according to a given compression rate. Yu et al.\n[178] pruned unimportant neurons based on the effect of the pruning error propagation on the ﬁnal response layer (e.g.,\nthe neurons were pruned backward from the ﬁnal layer to the ﬁrst layer). The methods of pruning neurons improved the\nresource efﬁciency such as the accuracy per parameter and per operation.\n7\narXiv Template\nA PREPRINT\n3.2.3\nPruning Filters:\nPruning insigniﬁcant ﬁlters after training can improve the accuracy per parameter and per operation. The feature maps\nassociated with the pruned ﬁlters and the next kernels associated with the pruned feature maps should be also pruned.\nPruning ﬁlters can maintain the dense structure of DNN unlike pruning weights, implying that it is highly probable to\nimprove physical resource efﬁciency further, compared to pruning weights. Li et al. [99] pruned unimportant ﬁlters\nbased on the summation of absolute weight values in the ﬁlter. The pruned DNNs were retrained with the survived\nﬁlters to regain the lost accuracy. Yang et al. [171] pruned ﬁlters based on a platform-aware magnitude-based metric\ndepending on the resource-constrained devices. ThiNet [107] calculated the signiﬁcance of the ﬁlters using the outputs\nof the next layer and pruned the insigniﬁcant ﬁlters based on this signiﬁcance measurement.\n3.2.4\nPruning Channels:\nUnlike pruning ﬁlters, pruning channels removes the ﬁlters at the current layer and the kernels at the next layer\nassociated with the pruned channels. The network slimming approach [104] pruned insigniﬁcant channels, producing\ncompact models while keeping equivalent accuracy, compared to the models prior to pruning. For example, insigniﬁcant\nchannels were identiﬁed based on scaling factors generated from the batch normalization of [83], and the channels\nassociated with lower scaling factors were pruned. After the initial training, the channels associated with relatively\nlow scaling factors were ﬁrst pruned, and retraining was then performed to reﬁne the network. He et al. [69] identiﬁed\nunimportant channels using LASSO regression from a pre-trained CNN model and pruned them. The channel pruning\nbrought 5 × speed-up on VGG-16 with minor accuracy loss. Lin et al. [102] pruned unimportant channels during\nruntime based on a decision maker trained by reinforcement learning. Gao et al. [53] proposed another dynamic channel\npruning method that dynamically skipped the convolution operations associated with unimportant channels.\n3.3\nCompact Convolution\nTo improve resource-efﬁciency such as the accuracy per operation and per parameter from computationally intensive\nconvolution operations, many compact convolution methods were proposed.\n3.3.1\nSqueezing Channel:\nIn 2016, Iandola et al. [82] proposed SqueezeNet in which each network block utilized the number of 1×1 ﬁlters less\nthan the number of the input channels to reduce the network width in the squeezing stage and then utilized multiple\n1×1 and 3×3 kernels in the expansion stage. The computational complexity was signiﬁcantly reduced by squeezing\nthe width, while compensating the accuracy in the expansion stage. SqueezeNet reduced the number of parameters\nby 50×, compared to AlexNet on ImageNet without losing accuracy, improving accuracy per parameter. Gholami et\nal. [55] proposed SqueezeNext that utilized separable convolutions in the expansion stage; a k × k ﬁlter was divided\ninto a k × 1 and a 1 × k ﬁlter. Such separable convolutions reduced the number of parameters further, compared to\nSqueezeNet while maintaining AlexNet’s accuracy on ImageNet, improving accuracy per parameter further, compared\nto SqueezeNet.\n3.3.2\nDepth-Wise Separable Convolution:\nXception [36] utilized the depth-wise separable convolutions, that replace 3D convolutions with 2D separable con-\nvolutions followed by 1D convolutions (i.e., point-wise convolutions) as shown in Fig. 5, to reduce computational\ncomplexity. The 2D separable convolutions are performed separately with respect to different channels. Howard et al.\n[76] proposed MobileNet v1 that utilizes the depth-wise separable convolutions with the two hyperparameters, “width\nmultiplier and resolution multiplier\", to ﬁt DNNs to resource-constrained devices by fully leveraging the accuracy and\nresource trade-off in the DNNs. MobileNet v1 showed equivalent accuracy to GoogleNet and VGG16 on ImageNet\ndataset with less computational complexity, improving the accuracy per parameter and per operation.\n3.3.3\nLinear Bottleneck Layer:\nIn general, the manifold of interest (i.e., the subspace formed by the set of activations at each layer) could be embedded\nin low-dimensional subspaces in deep learning. Inspired by this, Sandler et al. [132] proposed MobileNet v2 consisting\nof a series of bottleneck layer blocks. Each bottleneck layer block as shown in Fig. 6 received lower dimensional input,\nexpanded the input to high dimensional intermediate feature maps, and projected the high dimensional intermediate\nfeatures onto low dimensional features. Keeping linearity for the output feature maps was crucial to avoid destroying\ninformation from non-linear activations, so linear activation functions were used at the end of each bottleneck block.\n8\narXiv Template\nA PREPRINT\n3 channels\n2D \nconvolutions\npoint-wise \nconvolutions\n…\n…\n…\n…\nFigure 5: Depth-wise convolution used in [76].\nchannels\nFigure 6: Bottleneck layer block used in [132].\n3.3.4\nGroup Convolution:\nIn a group convolution method, the input channels are divided into several groups, and the channels in each group\nare separately participated in convolution with other groups. For example, the input channels with three groups\nrequired three separate convolutions. Since group convolution does not communicate with the channels in other groups,\ncommunication between different groups is performed after the separate convolutions. Group convolution methods of\n[181, 108, 80, 79] reduced the number of MAC operations, improving the accuracy per operation, compared to DNNs\nusing regular convolution. In 2012, AlexNet utilized group convolution to train the DNNs effectively using the two\nNVIDIA GTX580 GPUs [91]. Surprisingly, an AlexNet using the group convolution showed superior accuracy to\nan AlexNet using regular convolution, improving the accuracy per operation. In 2017, ResNext [167] utilized group\nconvolution based on ResNet [67] using a cardinality parameter (i.e., the number of groups). In 2018, Zhang et al. [181]\nnoticed that the point-wise convolutions were computationally intensive in practice in the depth-wise convolutions and\nproposed ShufﬂeNet that applied group convolution to every point-wise convolution to reduce compute complexity\nfurther, compared to MobileNet v1. ShufﬂeNet shufﬂed the output channels from the grouped point-wise convolution to\ncommunicate with different grouped convolutions, demonstrating superior accuracy to MobileNet v1 on ImageNet and\nCOCO datasets, given the same arithmetic operation cost budget. Ma et al. [108] proposed ShufﬂeNet v2 that improved\nphysical resource efﬁciency further, compared to ShufﬂeNet [181] by employing equal channel width for input and\noutput channels where applicable and minimizing the number of operations required for 1 × 1 convolutions. Rather\nthan choosing each group randomly and shufﬂing them, Huang et al. [80] proposed to learn each group for a group\nconvolution during training. The “learned group convolution” was applied in Densenet [79], and Densenet improved\nthe accuracy per parameter and per operation, compared to ShufﬂeNet, given a prescribed accuracy.\n3.3.5\nOctave Convolution:\nChen et al. [34] decomposed feature maps into a higher and a lower frequency part to save the feature maps’ memory\nfootprint and reduce the computational cost. The decomposed feature maps were used by speciﬁc convolution called\n“Octave Convolution” that performs a convolution between the higher and lower frequency part. The application of\nthe octave convolution to ResNet-152 architecture achieved higher accuracy using ImageNet dataset than the regular\nconvolution, improving the accuracy per operation and per memory footprint.\n3.3.6\nDownsampling:\nQin et al. [122] applied a downsampling approach (e.g., a larger stride size for a convolution) to MobileNet v1,\nimproving the top-1 accuracy by 5.5% over MobileNet v1 on the ILSVRC 2012 dataset, given a 12M arithmetic\noperations budget.\n3.3.7\nLow Rank Approximation:\nDenton et al. [44] proposed a low rank approximation that compresses the kernel tensors in the convolutional layers\nand the weight matrices in the fully connected layers by using singular value decomposition. Another low rank\napproximation [89] used Tucker decomposition to compress the feature maps, resulting in signiﬁcant reductions in the\n9\narXiv Template\nA PREPRINT\nmodel size, the inference latency, and the energy consumption. Such low rank approximation methods improve the\naccuracy per parameter, per operation, and per memory footprint.\n3.4\nKnowledge Distillation\nThe knowledge from a large-scale high performing model (teacher network) could be transferred to a compact neural\nnetwork (student network) to improve resource efﬁciency such as accuracy per parameter and per operation for inference\ntasks [23, 127, 29]. Bucilu˘a et al. [23] utilized data with the labels generated from the teacher model (i.e., a large scale\nensemble model) to train a compact neural network. The compact model was trained with the pseudo training data\ngenerated from the teacher model, demonstrating equivalent accuracy to the teacher model. Ba and Caruana [14] noticed\nthat the softmax outputs often resulted in the student network ignoring the information of the other categorizations than\nthe one with the highest probability, and utilized the values prior to the softmax layer, from the teacher network, for\nthe training labels to allow the student network to learn the teacher network more efﬁciently. Hinton et al. [72] added\na “temperature” term for the labels to enrich the information from the teacher network and train the student network\nmore efﬁciently, compared to [14]. Romeo et. al [127] utilized both labels and intermediate representations from a\nwider teacher network to compress it to a thinner and deeper student network. The “hint layer” was chosen from the\nteacher network and the “guided layer” was chosen from the student network. The student network was then trained\nso that the intermediate representation deviation between the outputs from the hint layers and guided layers could be\nminimized. A thinner student network employed 10.4× less weight parameters, compared to a wider teacher network,\nwhile improving accuracy. This technique is also known as “hint learning”. The hint learning was applied to both the\nregion proposal and classiﬁcation components for object detection applications [29].\n3.5\nNeural Architecture Search for Compressed Models\nZoph et al. [187] proposed Neural Architectural Search (NAS) technique to seek optimal DNN models in the space of\nhyperparameters of network width, depth, and resolution. In case that compute resource budget was limited (e.g., mobile\ndevices), many NAS variants exploited the trade-off between accuracy and latency to maximize resource efﬁciency\ngiven compute resource budget [70, 147, 148, 149, 164, 13]. He et al. [70] proposed a NAS employing reinforcement\nlearning, AutoML, that sampled the least sufﬁcient candidate design space to compress the DNN models. MnasNet [147]\nutilized reinforcement learning with a balanced reward function between the accuracy and the latency to seek a compact\nneural network model. Wu et. al [164] proposed a gradient-based NAS that produced a DNN model with 2.4× model\nsize reduction, compared to a MobileNet v2 without losing accuracy on ImageNet dataset. Florian et. al [133] proposed\na narrow-space NAS to generate low-resource DNNs satisfying strict memory budget and inference time requirement\nfor IoT applications. [13] noticed that conventional NAS might improve abstract resource efﬁciency rather than physical\nresource-efﬁciency, and utilized the hardware information including the inference latency for a NAS to ensure that\nthe candidate models could improve the physical resource-efﬁciency in practice. Efﬁcientnet [148] utilized a NAS\nwith compound scaling of depth, width, and resolution to seek optimal DNN models given ﬁxed compute resource\nbudgets. Another NAS utilizing compound scaling, Efﬁcientdet, was proposed for object detection applications [149].\nEfﬁcientdet improved the accuracy using COCO dataset with 4 −9× model size reduction, compared to state-of-the-art\nobject detectors, improving the accuracy per parameters. Recently, [25] proposed a feed-forward NAS approach that\nproduced a customized DNN, given compute resource and latency constraint.\n4\nArithmetic-Level resource-efﬁcient Techniques\nUtilizing lower precision arithmetic reduces the memory footprint and the time spent transferring data across buses\nand interconnections [49, 114, 186, 172]. Employing least sufﬁcient arithmetic precision for DNN applications can\nimprove the accuracy per memory footprint and the accuracy per memory access. We categorize the arithmetic-level\nresource-efﬁcient techniques into the two categories as shown in Fig. 7, Arithmetic-Level Techniques for Inference\nand Arithmetic-Level Techniques for Training. We discuss different number formats ﬁrst and the deployment of such\nnumber formats on DNNs later.\n4.1\nNumber Formats for Deep Learning\nThis subsection describes various number formats for deep learning applications, as preparatory to explaining the\narithmetic-level resource-efﬁcient techniques. Fixed-Point (FiP) number format utilizes a binary ﬁxed point between\nfraction and integer part. For example, an 8-bit FiP format, “01.100000”, represents 1.5 (i.e., ...0 × 21 + 1 × 20 +\n1 × 2−1 + 0 × 2−2...) for the decimal representation, and the point between integer part and fraction part is ﬁxed for\n10\narXiv Template\nA PREPRINT\nFigure 7: Categorization of arithmetic-level resource-efﬁcient techniques.\narithmetic operations. Therefore, it could be implemented with simple circuits, but the available data range is very\nlimited [120].\nWe exemplify the IEEE-754 general-purpose ﬂoating-point standard [4] to explain Floating-Point (FP) format and its\narithmetic, since this standard is used for most commercially available CPUs and GPUs. The IEEE 754 Floating-Point\n(IFP) data format [4] consists of sign, exponent, and signiﬁcand as shown in Eq. (5). For example, a ﬂoating point\nnumber has a (p + 1)-bit signiﬁcand (including the hidden one), an e-bit exponent, and an 1 sign bit. The machine\nepsilon ϵmach is deﬁned as 2−(p+1). The value represented by FP is as follows:\nyout =\n\u001anormal mode:\n(−1)sign × (1 × 20 + d1 × 2−1 + ... + dp × 2−p) × 2exponent−bias\nsubnormal mode:\n(−1)sign × (d1 × 2−1 + ... + dp × 2−p) × 21−bias,\n(5)\nwhere d1, ... , dp represent binary digits, the ‘1’ associated with the coefﬁcient 20 is referred to as the hidden ‘1’, the\nexponent is stored in offset notation, and the bias is a positive constant. If the absolute value of exponent is zero, the\nﬂoating-point value is represented by the subnormal mode. IEEE 754 standard requires exact rounding for addition,\nsubtraction, multiplication, and division; the ﬂoating point arithmetic result should be identical to the one obtained\nfrom the ﬁnal rounding after exact calculation. For example, based on the IEEE 754 rounding to nearest mode standard,\nﬂoating point arithmetic should follow Eq. (6):\nfl(x1 ⊙x2) = (x1 ⊙x2)(1 + ϵr),\n(6)\nwhere |ϵr| ≤ϵmach, ⊙is one of the four arithmetic operations, and fl(·) represents the result from the ﬂoating point\narithmetic. Notice that quantization quantizes data to lower precision, while arithmetic is a rule applied to arithmetic\noperations between the two operands. For example, the quantization affects the values for the two operands, x1 and x2\nin Eq. (6), while arithmetic affects the rounding error, ϵr.\n4.1.1\nHalf, Single, and Double Precision:\nThe IEEE Floating-Point 32- (IFP32 or single precision) and 64-bit (IFP64 or double precision) versions are available\non most of off-the-shelf conventional processors. Besides, IEEE-754 standard includes a 16-bit FP format (IFP16 or\nhalf precision) [4]. p = 52, e = 11, and bias = 1023 for IFP64, p = 23, e = 8, and bias = 127 for IFP32, and p = 10,\ne = 5, and bias = 15 for IFP16. IFP16 is currently supported in hardware on some of modern GPUs to accelerate\nDNN applications [73, 37].\n4.1.2\nBrain Float-Point Format using 16 Bits (BFloat16):\nIn 2018, a 16-bit Brain Floating-Point format [176, 24] was proposed that was tailored to deep learning applications.\nThe BFloat16 consists of an 8-bit exponent and a 7-bit signiﬁcand, supporting a wider dynamic data range than IFP16.\nBFloat16 is currently supported in hardware in the Intel Cooper Lake Xeon processors, the NVIDIA A100 GPUs, and\nthe Google TPUs.\n4.1.3\nDLFloat:\nIn the race of designing speciﬁc FP formats for DNNs, [6, 158] proposed another 16-bit precision format, DLFloat,\nconsisting of 6-bit exponent and 9-bit signiﬁcand to provide better balance between dynamic data range and precision\nthan IFP16 and BFloat16 formats for some of deep learning applications.\n11\narXiv Template\nA PREPRINT\n4.1.4\nTensorFloat32 (TF32):\nNVIDIA proposed a 19-bit data format, TF32, consisting of 1-bit sign, 8-bit exponent and 10-bit signiﬁcand to accelerate\ndeep learning applications on A100 GPUs with the same dynamic range support as IFP32 [48]. TF32 Tensor cores\nin an A100 truncates IFP32 operands to 19-bit TF32 format but accumulates them using IFP32 arithmetic for MAC\noperations.\n4.2\nArithmetic-Level Techniques for Inference\nThis subsection discusses various resource-efﬁcient arithmetic-level techniques based on the pre-trained DNNs for the\ninference tasks.\n4.2.1\nLower Precision Arithmetic:\nLower precision FiP arithmetic has been widely used to deploy DNNs on edge devices [169]. [165, 159] analyzed the\neffect of deploying various lower precision arithmetic on the DNN inference tasks in terms of accuracy and latency.\nThe BitFusion method accelerated DNN inference tasks by employing variable bit-width FiP formats dynamically\ndepending on the different layers [138]. Similarly, Tambe et al. [146] proposed AdaptiveFloat that adjusted dynamic\nranges of FP numbers depending on the different layers, resulting in higher energy efﬁciency than FiP-based methods,\ngiven the same accuracy requirement.\n4.2.2\nEncoding Weights and Using Lookup Table:\n[21] leveraged the fact that the exponent values of most of the weights were located within a narrow range and encoded\nthe frequent exponent values of the weights with fewer bits using Huffman coding scheme, improving the accuracy per\nmemory footprint for natural language processing applications. A lookup table, located between the memory and FP\narithmetic units, is used to convert the encoded exponent values into FP exponent values.\n4.2.3\nApplying Various Number Format Quantizations to DNNs:\nThe Residue Number System (RNS) is a parallel and carry-limited number system that transforms a big natural number\nto several smaller residues. Therefore, RNS was often used to perform parallel and independent calculations on\nresidues without carry-propagation. [28] exploited such parallelism to accelerate DNN computation. In a RNS-based\nDNN, the weights of a pre-trained model were transformed to RNS presentation. Recently, RNS was used to replace\ncostly multiplication operations with simple logical operations such as multiplexing and shifting, accelerating DNN\napplications [131, 105, 130]. The Logarithmic Number System (LNS) applies the logarithm to the absolute values of\nthe real numbers [120]. The main advantage of LNS is in the capability of transforming multiplications into additions\nand divisions into subtractions. In 2018, [156] utilized a 5-bit logarithmic format using arbitrary log bases to improve\nthe resource efﬁciency such as accuracy per memory footprint and per operations by replacing costly multiplication\narithmetic operations to simple bit-shift operations [156]. The Posit number format [61] employs multiple separate\nexponent ﬁelds to represent dynamic range effectively. Recently, DNNs utilizing Posit showed higher accuracy than\nvarious FP8 formats using Mushroom and Iris datasets [26, 27].\n4.3\nArithmetic-Level Techniques for Training\nThis subsection discusses arithmetic-level resource-efﬁcient techniques used for DNN training tasks. Training DNNs\ngenerally requires a higher precision arithmetic due to extremely small weight gradient values [38, 185]. Adjusting\narithmetic precision according to different training procedures such as forward propagation, activation gradient updates,\nand weight updates can accelerate DNN training [59]. Training quantized DNNs often required stochastic rounding\nschemes [60, 166, 172, 184].\n4.3.1\nMixed-Precision Training:\nA conventional mixed precision training applied lower precision arithmetic to the multiplications in MACs, including\nboth forward and backward path, and higher precision arithmetic to the accumulations in the MACs using the lower\nprecision quantized operands [86, 114]. The higher precision outcomes from MACs were quantized to a lower precision\nformat to be used for consequent operations. In the following (X + Y) formats, X represents the data format used for\nMAC operations, and Y represents the arithmetic applied for the accumulations in MAC operations (refer to [59] for the\ndetails for the lower and higher precision arithmetic usage.).\n12\narXiv Template\nA PREPRINT\na. IFP16 + IFP32: In 2018, [114] noticed that the weights were updated using very small weight gradient values, and\napplied a lower precision arithmetic IFP16 to the multiplications and a higher precision IFP32 to the accumulations for\nthe weight updates. For example, in the mixed-precision training approach in [114], IFP16 was used to store weights,\nactivations, activation gradients and weight gradients, while IFP32 was used to keep the weight copies for their updates.\nAlong with accumulating IFP16 operands using IFP32 arithmetic, the use of loss scaling allowed the mixed precision\ntraining to achieve equivalent accuracy to the IFP32 training while reducing the memory footprint.\nb. BFloat16 + IFP32: In 2018, the mixed-precision DNN training using (BFloat16 + IFP32) was explored in [176]. In\n2019, [86] studied the BFloat16’s feasibility for mixed precision training for various DNNs including AlexNet, ResNet,\nGAN, etc., and concluded that (BFloat16 + IFP32) scheme outperformed the (IFP16 + IFP32) scheme since BFloat16\ncould represent the same dynamic range of data as IFP32 while using fewer bits.\ne. FP8 + DLFloat In 2018, [158] proposed a mixed-precision training method that applies the 5eFP8 format (1 sign-bit,\n5-bit exponent, and 2-bit for signiﬁcand) to the multiplications and DLFloat to the accumulations in MAC operations.\nThe mixed precision method improved resource efﬁciency such as accuracy per memory footprint and accuracy per\nmemory access, compared to various (FP16 + IFP32) schemes with respect to different FP16 formats. Compared to the\nprevious (FP16 + IFP32) methods, the chunk-accumulation and stochastic rounding schemes were additionally used to\nminimize the accuracy loss in [158], The chunk-based accumulation utilized 64 data per chunk instead of one long\nsequential accumulation to reduce rounding errors. Utilizing stochastic rounding scheme with limited precision format\nfor deep learning was proposed earlier in [60]. [144] noted that (5eFP8 + DLFloat) training degraded accuracy for\nDNNs utilizing depth-wise convolutions such as MobileNets. To overcome this issue, [144] proposed to employ two\ndifferent 8 bit ﬂoating-point formats each for forward and backward propagation to minimize the accuracy degradation\nfor compressed DNNs. The mixed-precision training utilized 5eFP8 for backpropagation and another 8-bit ﬂoating-point\nformat with (Sign, Exponent, signiﬁcand) = (1, 4, 3), 4eFP8, for forward propagation.\nc. DLFloat only: In 2019, [6] employed the DLFloat for entire training procedure, removing the necessity of data\nconversions between the multiplications and the accumulations and found that DLFloat could provide better balance\nbetween dynamic range and precision than IFP16 and BFloat16 for LSTM networks [74] using Penn Treebank dataset.\nThe DLFloat arithmetic units removed subnormal mode and supported the round-to-nearest up mode to minimize\ncomputational complexity. In [6], the DLFloat arithmetic showed equivalent performance to IFP32 on ResNet-32 using\nCIFAR10 and ResNet-50 using ImageNet, while using a half of IFP32 bit width.\nf. INT8-based: Yang et al. [172] noticed that previously proposed mixed-precision training schemes did not quantize\nthe data in the batch normalization layer, requiring high ﬂoating-point arithmetic in some parts of the data paths. To\novercome this issue, [172] proposed a uniﬁed INT8-based quantization framework that quantizes all data paths in DNN\nincluding weights, activation, gradient, batch normalization, weight update, etc. into INT8-based data. However, this\ntraining method degraded the accuracy to some extent. In 2020, Zhu et al. [186] improved the accuracy, compared to\nthe work of [172] while keeping uniﬁed INT8-based quantization framework. [186] minimized the deviation of the\nactivation gradient direction between before and after quantization by measuring the distance during runtime based on\nthe inner product between the two normalized gradient vectors generated before and after quantization.\ng. Layer-Wise Adaptive Fixed-Point Training: In 2020, Zhang et al. [182] proposed a layer-wise adaptive quantization\nscheme. For example, activation gradient distributions at fully connected layers followed a narrower distribution,\nrequiring more bit-width for the quantizations. [182] quantized AlexNet using INT8 for all the weights and activations\nand both INT8 (22%) and INT16 (78%) for the activation gradients. The quantized AlexNet achieved equivalent\naccuracy to the one using IFP32 for entire training on ImageNet dataset.\n4.3.2\nBlock Floating-Point Training\nBlock Floating-Point (BFP) format utilizes a shared exponent for a series of numbers in a data block in order to reduce\ndata-size [161]. Applying BFP to DNNs can improve the resource efﬁciency in terms of accuracy per memory footprint\nand per memory access. In addition, BFP utilizes less transistors for simpler adders and multipliers than FP adders and\nmultipliers, resulting in improving accuracy per Joule. Various versions of DNN training methods using BFP were\nproposed to improve resource efﬁciency.\na. Flexpoint: A DNN-optimized BFP format, Flexpoint [90], was proposed by Intel, and it was used with the Nervana\nneural processors. The BFP format used 5 bits for a shared exponent and 16 bits for the signiﬁcand for the data in\na data block. Flexpoint utilized the format of (Flex N)+M, where Flex N represents variable number of bits for the\nshared exponent according to the different epochs, and M represents the number of bits for the separated signiﬁcand.\nFor example, the number of exponent bits is adapted based on the dynamic range of the weight values depending on\nthe number of iterations; the dynamic range of the weight values at the current iteration was predicted at the previous\niteration. The (Flex N + 16) format produced equivalent accuracy to IFP32 in AlexNet using ImageNet dataset and\n13\narXiv Template\nA PREPRINT\nFigure 8: Implementation-level resource-efﬁcient techniques.\na ResNet using CIFAR-10 dataset, resulting in signiﬁcant resource efﬁciency improvement in terms of accuracy per\nmemory footprint and accuracy per memory access.\nb. BFP + FP training: Drumond et al. [45] proposed a hybrid use of BFP and FP for DNN training that uses BFP only\nfor MAC operations and FP for the other operations. Such hybrid training method brought 8.5 × potential throughput\nimprovement with minor accuracy loss in WideResNet28-10 using CIFAR-100 dataset on a Stratix V FPGA.\nc. Block MiniFloat: [49] noticed that ordinary BFP formats were limited in minimizing original data loss with fewer\nbits and improving arithmetic density per memory access for deep learning applications. To address the two issues, Fox\net al. [49] proposed the Block Miniﬂoat (BM) along with customized hardware circuit design. The BM<e,m> format\nfollows:\nyout =\n\u001anormal mode:\n(−1)sign × (1 × 20 + d1 × 2−1 + ... + dm × 2−m) × 2exponent−bias−BIASSE\nsubnormal mode:\n(−1)sign × (d1 × 2−1 + ... + dm × 2−m) × 21−bias−BIASSE,\n(7)\nwhere bias = 2e−1 −1 and BIASSE is a shared exponent value. BIASSE is scaled according to the maximum value\nof the data for dot-product operations. For example, BM<2,3> represents a 6-bit data format having 1 sign bit, 2-bit\nexponent, and 3-bit signiﬁcand. Such BM variant formats were applied for training. Utilizing these 6-bit BM formats\nproduced equivalent accuracy to IFP32 formats but with fewer bits using CIFAR 10 and 100 dataset with ResNet-18,\nresulting in reduced memory-trafﬁc and low energy consumption. Therefore, BM improved the resource efﬁciency in\nterm of accuracy per memory access.\n5\nImplementation-Level Resource-Efﬁcient Techniques\nFig. 8 classiﬁes the implementation-level resource-efﬁcient techniques. Most implementation-level techniques have\nfocused on improving energy efﬁciency and computational speed for MAC operations, since MACs generally occupy\nmore than 90% of computational workload for both training and inference tasks in DNNs [145]. The implementation-\nlevel resource-efﬁcient techniques exploited the characteristics of MACs in DNN including data reuse, sparsity of\nweights and activations, and weight repetition from quantized DNNs.\n5.1\nLeveraging Data Reuse from Convolution\nThe weights and the activations are heavily reused in convolution operations. For example, the weights of a ﬁlter are\nreused ((H −kH + 1) × (W −kW + 1))/stride times, where H = W = 4 (height and width at input channel) and\nkH = kW = 3 (height and width for a kernel) in Fig. 3. Generally, H and W are three orders of magnitude (e.g.,\n128, 256, etc), kH and kW are one order of magnitude (e.g., 3, 5, etc), and stride is either 1 or 2. For example, if\nH = W = 128, kH = kW = 3, and stride = 1, each ﬁlter is reused 16129 times for convolutional operations. Each\ninput element at a covolutional layer is also reused approximately M × kH × kW times, where M is the number of the\ntotal kernels used in the layer. Fig. 9 describes the data access patterns for MAC operations used for convolutional\nlayers. In each MAC computation in Fig. 9.(a), the data, a, b, and c, are read from the memory for multiply and add\ncomputation, and the result d is written back to the memory, where c contains a partial sum for the MAC. To save energy\nconsumption, highly reused data for MAC computations can be stored in small local memory as shown in Fig. 9.(b).\n14\narXiv Template\nA PREPRINT\n×\n+\na\nb\nc\nSlow\nLarge\nMem\nd\nFast\nSmall\nLocal\nMem\n×\n+\na\nb\nc\nSlow\nLarge\nMem\nd\nProcessing Element (PE)\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\nPE\n(a) MAC without optimization\n(b) Data reuse using local memory\n(c) Spatial Architecture\nFigure 9: Multiply-and-accumulate dataﬂow.\nFor example, the power consumption required to access data depends on where the data are located – accessing data\nfrom off-chip memory, DRAM, generally requires two orders of magnitude more than from on-chip memory [31]. For\ncommercially available CPUs or GPUs, transforming convolutional operations into matrix multiplications can leverage\nsuch data reuse properties to accelerate the convolution operations by utilizing highly optimized BLAS libraries [155].\nMany research works presented how to leverage such data reuse properties to improve the resource efﬁciency.\n5.1.1\nEmploying SRAM Local Memory near to Processing Elements:\nThe use of SRAM buffers reduces the energy consumed by DNNs by up to two orders of magnitude, compared to\nDRAM. Similar to Fig. 9.(b), Dianao architecture [30] employed one Neural Functional Unit (NFU) integrated with\nthree separated local buffers, each for holding 16 input neurons, 16 × 16 weights, and 16 output neurons, in order\nto optimize circuitry for MAC operations. The weights and the activations stored to the local memories were reused\nefﬁciently by additionally using internal registers to store the partial sums and the circular buffer. The NFU is a\nthree-stage pipelined architecture consisting of the multiplication, the adder-tree, and the activation stage. In the\nmultiplication stage, 256 multipliers support the multiplications based on the weight connections between 16 input and\n16 output neurons. In the adder-tree stage, 16 feature maps are generated from the multiplications based on adder-tree\nstructure. In the activation stage, the 16 feature maps are approximated for the 16 activations by using piece-wise\nlinear function approximation. DianNao with 65nm ASIC layout brought up to 118× speedup and reduced the energy\nconsumption by 21×, compared to a 128-bit 2GHz SIMD processor over the benchmarks used in [30]. One of the\nfollowing studies adapted DianNao to deploy it on a supercomputer and named it as DaDianNao [33]. Since the number\nof weights is generally larger than the number of input activations for convolution operations, DaDianNao stored a big\nchunk of weights and shared them to multiple NFUs by using a central embedded DRAM to reduce the data movement\ncost in delivering the weights associated to each NFU.\n5.1.2\nLeveraging Spatial Architecture:\nDesigning PEs and their local memory according to data reuse properties of MAC operations improved energy efﬁciency\non FPGAs and ASIC [31, 32]. For example, Google TPU employs a systolic array architecture to send the data directly\nto an adjacent Processing Element (PE) as shown in Fig. 9.(c) [3]. Chen et al. [31] noticed that the computational\nthroughput and energy consumption of CNNs mainly depended on data movement rather than computation and proposed\na “row-stationary” spatial architecture (a variant of Fig. 9.(c)), Eyeriss, to support parallel processing with minimal data\nmovement energy cost by fully exploiting the data reuse property. For example, the three PEs in the ﬁrst column in\nFig 9.(c) can be assigned to compute the ﬁrst row of the convolution output using a 3 × 3 ﬁlter - the three elements on\neach row of the kernel are stored to the local memory on each PE (i.e., “row-stationary” structure in [145]), and all the\nelements in the kernel are reused during convolution, generating the ﬁrst row of the output. In this case, the partial\nsummation values are stored back to the local memory on each PE.\n5.1.3\nCircuit Optimization:\nExploring binary weights [38] with binary inputs offered the opportunity to explore XNOR gates for the efﬁcient\nimplementation of CNN [123], improving the accuracy per memory foot print and per Joule. In 2021, [183] proposed\nhardware-friendly statistic-based quantization units and near data processing engines to accelerate mixed precision\ntraining schemes by minimizing the number of accesses to higher precision data.\n15\narXiv Template\nA PREPRINT\n5.2\nLeveraging Sparsity of Weights and Activations\nIn the forward pass, negative feature map values are converted to zeros after ReLU activation functions, making the\nactivation data structure sparse. In addition, the trained weight values follow a sharp Gaussian distribution centered at\nzero, locating most of the weights near to zero. Quantizing such weights makes the weight data structure sparse, so the\nsparse weights can be fully exploited on the quantized networks such as binarized DNNs [39, 38] and ternary weight\nDNNs [98, 185].\n5.2.1\nSkipping Operations during Runtime:\nIn 2016, several methods to conditionally skip MAC operations were proposed simultaneously [31, 10, 103]. Eyeriss [31]\nemployed clock gating to block the convolution operations during runtime when either the weight or the activation\nwas detected as zero in order to save computational power. Cnvlutin [10] skipped MAC operations associated with\nzero activations by employing separated “neuron lanes” according to different channels. Similarly, [103] proposed\nCambricon-X that fetches the activations associated with any non-zero weights for convolutions by using the “step\nindexing” in order to skip the MAC operations associated with the zero weights. Cambricon-X brought 6 × resource-\nefﬁciency improvement in terms of accuracy per Joule, compared to the original DianNao architecture. In 2017, Kim et\nal. [87] proposed ZeNa that performs MAC operations only if both the weights and the activations are non-zero values.\nIn 2018, Akhlaghi et al. [8] proposed a runtime technique, SnaPEA, that performs MAC operations associated with\npositive weights ﬁrst and then negative weights later while monitoring the sign of the partial sum value. Since the\nactivation values from ReLU are always greater or equal to zero, the convolution operation can be terminated once\nthe partial sum value becomes negative. Notice that such decision should be performed during runtime, since the zero\nvalued activation patterns depend on the test images. In 2021, another method skipping zero operations, GoSPA [41],\nwas proposed, which is similar to ZeNa in that MAC operations were performed only when both input activations and\nweights were non-zero values. [41] constructed “Static Sparsity Filter\" module by leveraging the property that the\nweight values are static while the activation values are dynamic to ﬁlter out zero activations associated with non-zero\nweights on the ﬂy before MAC operations. Such skipping operation optimization techniques improved the accuracy per\nJoule, since the transistors associated with skipped operations were not toggled during runtime, saving dynamic power\nconsumption.\n5.2.2\nEncoding Sparse Weights/Activations/Feature Maps:\nSince memory access operations dominate the power consumption in deep learning applications, fetching the weights\nless frequently from memory by encoding and compressing the weights and the activations can improve the resource\nefﬁciency such as the accuracy per memory footprint, per memory access, and per Joule. Han et al. [65, 64] utilized the\nHuffman encoding scheme to compress the weights. The quantized DNN reduced the memory footprint of AlexNet on\nImageNet dataset by 35 times without losing accuracy. In [65, 64], a three stage pipelined operation was performed\nin order to reduce the memory footprint of DNNs as follows. The pruned sparse quantized weights were stored with\nCompressed Sparse Row (CSR) format and then divided into several groups. The weights in the same group were\nshared with the average value over the weights in the group, and they were re-trained thereafter. Huffman coding was\nused to compress the weights further. Parashar et al. [119] employed an encoding scheme to compress sparse weights\nand activations and designed an associated dataﬂow, \"Compressed-Sparse Convolutional Neural Networks (SCNN)\",\nto minimise data transfer and reduce memory footprint. Aimar et al. [7] proposed NullHop that encodes the sparse\nfeature maps by using two sequentially ordered (i.e., internally indexed) additional storage, one for a 3D mask to\nindicate the positions of non-zero values and the other for storing the non-zero data sequentially in the feature map. For\nexample, ‘0’s are marked at the position of zero values in the 3D mask, otherwise ‘1’s are marked. Decoding refers\nto both the 3D mask and the non-zero value list. Rhu et al. [125] presented HashedNet that utilizes a low cost hash\nfunction to compress sparse activations. The virtualized DNN (vDNN) [126] compressed sparse activation units using\nthe “zero-value compression” technique to minimize the data transfer cost between GPU and CPU. The vDNN allowed\nusers to utilize both GPU and CPU memory for DNN training.\n5.2.3\nDecomposing Kernel Matrix:\nLi et al. [100] proposed SqueezeFlow that reduces the number of operations for convolutions by decomposing the\nkernel matrix into non-zero valued sub-matrices and zero-valued sub-matrices. This method can improve the accuracy\nper Joule.\n16\narXiv Template\nA PREPRINT\n5.3\nLeveraging Weight Repetition in Quantized DNNs\nHedge et al. [71] noticed that the identical weight values were often repeated in quantized DNNs such as binary weight\nDNNs [39, 38] and ternary weight DNNs [98, 185] and proposed the Unique Weight CNN Accelerator (UCNN) that\nreduces the number of memory accesses and the number of operations by leveraging the repeated weight values in the\nquantized DNNs. For example, if a 2 × 2 kernel consisting of {k1,1, k1,2, k2,1, k2,2} performs a convolution with the\nactivation maps, {a1,1, a1,2, a2,1, a2,2}. The conventional covolutional operation, Σi=2,j=2\ni=1,j=1ki,j × ai,j, requires 8 read\nmemory accesses, 4 multiplications, and 3 additions. If two of the weights in the kernel are identical (e.g., k1,1 = k2,2\nand k1,2 = k2,1), the covolutional operation can be performed using: k1,1(a1,1 + a2,2) + k1,2(a1,2 + a2,1), requiring 6\nread memory accesses, 2 multiplications, and 3 additions. The UCNN improved the accuracy per Joule by 1.2 −4× in\nAlexNet and LeNet on Eyeriss architecture using ImageNet dataset.\n5.4\nLeveraging Innovative Technology\nMany research attempts have leveraged innovative computing architecture technologies such as neuromorphic computing\nand in-memory processing as follows.\n5.4.1\nNeuromorphic Computing:\nNeuromorphic computing mimics the brain, including brain components such as neurons and synapses; furthermore,\nbiological neural systems include axons, dendrites, glial cells, and spiking signal transferring mechanisms [84]. The\nmemristor, ‘memory resistor’, is one of the most widely used devices in neuromorphic computing. ISAAC [136]\nreplaced MAC operation units with the memristor crossbars based on DaDianNao architecture. In the crossbars,\nevery wire in horizontal wire array was connected to every wire in vertical wire array with a resistor. Different level\nvoltages, V = [v1, v2, ..., vm], were applied to the horizontal wires connected to a vertical wire by the different resistors,\nR = [1/g1, 1/g2, ..., 1/gm]. With mapping vi to input elements and gi to weights, where i = 1, ..., m, the output current,\nI, from the vertical wire can be represented as MAC operations in a layer, I = Σm\ni (vi×gi), based on the Kirchhoff’s law.\nMultiple MAC operations can be performed by collecting the currents from multiple vertical wires. ISAAC employed\ndigital-to-analog converters to receive the input elements and covert them into the appropriate voltages and analog-\nto-digital converters to convert the current values into digitized feature map values. Due to lack of re-programability\nof resistors in the crossbars, ISAAC architecture was only available for the inference tasks. ISAAC improved 5.5×\naccuracy per Joule, compared to full-ﬂedged DaDianNao. As another neuromorphic computing approaches, many\nresearch attempts implemented Hodgkin-Huxley and Morris Lecar models [75] that describes the activity of neurons\nusing nonlinear differential equations in hardware simulators [109, 92, 129, 139, 142, 140, 141, 56, 16, 15, 46]. Several\nstudies implemented neuromorphic architectures in ASIC, including TrueNorth [9], SpiNNaker [117], Neurogrid [18],\nBrainScaleS [134], and IFAT [121]. Please refer to [135] for a comprehensive survey of neuromorphic computing.\n5.4.2\nIn-Memory Processing:\nScaling down the size of transistors enables energy efﬁciency and high performance for Von-Neumann computing\nsystems. However, it became very challenging in the era of sub-10nm technologies due to physical limitations [51,\n113, 11]. To address this challenge, researchers proposed a paradigm of in-memory processing to improve performance\nand energy efﬁciency by integrating computations units into memory devices [63, 168, 101]. Several studies proposed\nto enable in-memory processing to accelerate DNNs [12, 43, 68, 88, 52, 175]. For example, XNOR-SRAM [175]\nintegrated the XNOR gates and accumulation logic into SRAM to fetch data from SRAM and perform MAC operation\nin one cycle. Notice that this approach was applicable for binarized DNNs such as [39].\n5.5\nAdaptive Compute Resource Assignment\nThis subsection comprises the methods assigning runtime compute resources adaptively to the DNN inference workload\nto improve resource efﬁciency. The implementation of the DNNs can be adapted to the accuracy requirements of the\napplications by using various runtime implementation techniques as follows.\n5.5.1\nEarly Exiting:\nThe required depth of DNN depends on the problem complexity. The “early exiting” technique allows a DNN to\nclassify an object as early as possible by having multiple exit classiﬁer points in a single DNN [118, 150, 151]. The\nearly exiting technique was applied to distributed computing systems, addressing concern about privacy, response time,\nand higher quality of experience [151]. Such early exiting methods minimized the compute resources and the inference\n17\narXiv Template\nA PREPRINT\nlatency, improving the accuracy per Joule, per operation, and per core utilization. Please refer to [111] for the details on\nthe early exiting techniques.\n5.5.2\nRuntime Channel Width Adaptation:\nThe runtime channel width adaptation pruned unimportant ﬁlters during runtime. In 2018, Fang et. al [47] presented a\nsingle DNN model, NestDNN, being able to switch between multiple capacities of the DNN during runtime according\nto the accuracy and inference latency requirement. During training, unimportant ﬁlters from the original model were\npruned to generate the smallest possible model, “seed model”. Each re-training, some of pruned ﬁlters were added to\nthe seed model while ﬁxing the ﬁlter parameters from the previous training. Since the seed model was descended from\nthe original model, the accuracy for each capacity in NestDNN was higher than the model having the identical capacity\ntrained from the scratch. Similarly, Yu et. al [177] proposed another runtime switchable DNN model, Slimmable Neural\nNetwork, in which a larger capacity model shared the ﬁlter parameters from a smaller capacity model.\n5.5.3\nRuntime Model Switching:\nLee et al. [96] selected the best performing object detectors between multiple DNN detectors during runtime according\nto dynamic video content to improve the accuracy per core utilization and per Joule. Lou et al. [106] switched between\nmultiple DNNs, generated from the Once-For-All NAS of [25], during runtime according to dynamic workload. For\nexample, when the inference latency of a DNN was increased due to a newly assigned workload, a runtime decision\nmaker downgraded the current DNN during runtime to meet a latency constraint. Such runtime model switching\napproaches were appropriate when memory resources were sufﬁcient, since the multiple DNNs should be pre-loaded in\nDRAM.\n6\nInterrelated Inﬂuences\nThis section discusses the inﬂuence from higher- to lower-level techniques as shown in Fig. 1.\n6.1\nInﬂuences of Model-Level Techniques on Arithmetic-Level Techniques\nWeight quantization [38, 98, 185] in model-level techniques inﬂuenced arithmetic-level techniques as shown in Fig. 7.\nThe multiplications using the quantized binary weights can be replaced with multiplexers, removing multiplication\narithmetic operations. The resource efﬁciency from the model-level techniques can be further improved by utilizing the\narithmetic-level techniques. For example, quantized DNNs such as ternary weight and binarized DNNs allowed INT8\narithmetic to be used in training [166, 172]. When reduced precision DNNs suffered from zero gradients, the reduced\nprecision arithmetic was replaced with a hybrid version arithmetic using both BFP and FP [45] or the Block MiniFloat\nformat [49].\n6.2\nInﬂuences of Model-Level Techniques on Implementation-Level Techniques\nWeight quantization in model-level techniques inﬂuenced the implementation-level techniques as shown in Fig. 8.\nPruning weights can bring sparsity in the hardware architecture while pruning ﬁlters [107, 99] maintains dense structure.\nWeight quantization in the model-level techniques allows a DNN to utilize fewer bits for weights in order to save\nmemory resource usage, requiring customized hardware. For example, EIE [64] is an inference accelerator with weights\nquantized by 4 bits. To implement the weight quantization method effectively, EIE utilized weight sharing to reduce the\nmodel size further and ﬁt the compressed DNN into the on-chip SRAM. Exploring binary weights [39] with binary\ninputs offered the opportunity to explore XNOR gates for the efﬁcient implementation of CNNs [123], improving\nthe accuracy per memory footprint and per Joule. In [153], ternary neural networks [98, 185] were implemented on\nFPGAs by unrolling convolution operations. Since quantized DNNs [98, 185] increased the number of repeated weights\nin DNNs, UCNN [71] leveraged the property of the repeated weight values in quantized DNNs to improve resource\nefﬁciency such as accuracy per memory access and per operation. As main limitation, the weight quantization methods\nsuch as [38, 185, 98] were not suitable for commercially available CPUs and GPUs, since such computing platforms do\nnot support binary and ternary weights in hardware. Therefore, the implementation of weight quantization methods\non CPUs or GPUs might not improve accuracy per Joule as higher precision arithmetic still was required in part of\ndata path in training and inference. The bottleneck structures generated by compact convolutions in [132, 82] can be\nused to reduce the data size transferred between a local device and an edge server for the efﬁcient implementation of\nedge-based AI [111].\n18\narXiv Template\nA PREPRINT\n6.3\nInﬂuences of Arithmetic-Level Techniques on Implementation-Level Techniques\nThe arithmetic-level techniques inﬂuenced the implementation-level techniques as follows.\nFirst, the research in arithmetic utilization acted as a catalyst for commodity CPUs and GPUs. For example, the\nmixed precision research [114] laid a foundation for tensor cores in latest NVIDIA GPUs, which can accelerate the\nperformance of deep learning workloads by supporting a fused multiply–add operation and the mixed precision training\ncapability in hardware [19]. The BFloat16 format [24] designed by Google overcomes the limited accuracy issue of\nthe IFP16 format by providing the same dynamic range as IFP32, and it is supported in hardware in Intel Cooper\nLake Xeon processors, NVIDIA A100 GPUs, and Google TPUs. In 2016, NVIDIA Pascal GPUs supported IFP16\narithmetic in hardware to accelerate DNN applications. In 2017, NVIDIA Volta GPUs supported IFP16 tensor cores. In\n2020, the NVIDIA Ampere architecture supported tensor cores, TF32, BFloat16, and sparsity acceleration in hardware\nto accelerate MACs [2]. The Graphcore company developed the Intelligent Processing Unit (IPU), which employs\nlocal memory assigned to each processing unit with support for a large number of independently operating hardware\nthreads [85]. The IPU is an efﬁcient computing architecture customized to “ﬁne-grained, irregular computation that\nexhibits irregular data access”.\nSecondly, the arithmetic-level techniques led to specialized custom accelerators for deep learning. There is ample\nevidence in the arithmetic-level literature such as [49, 45, 21, 158, 170] that even smaller operators (e.g., 16 bits\nor even less) have almost no impact on the accuracy of DNNs. For example, DianNao [30] and DaDianNao [33]\nwere customized to 16-bit ﬁxed-point arithmetic operators instead of word-size (e.g., 32-bit) ﬂoating-point operators.\nISAAC [136] is a fully-ﬂedged crossbar-based CNN accelerator architecture, which implemented a memristor-based\nlogic based on resistive memory, suitable for 16-bit arithmetic for DNN workloads. Wang et al. [158] designed their\ncustomized 8-bit ﬂoating point arithmetic multiplications with 16-bit accumulations on an ASIC-based hardware\nplatform with a 14nm silicon technology to support energy-efﬁcient deep learning training. The Eyeriss [31] and\nSnaPEA [8] accelerators were customized to 16-bit arithmetic. UCNN [71] utilized 8-bit and 16-bit ﬁxed point\nconﬁgurations. SCNN [119] utilized 16-bit multiplication and 24-bit accumulation.\nLastly, the mixed precision training schemes were accelerated in hardware by minimizing the data conversion overhead\nbetween lower and higher precision formats in updating weights and activations [183]. Also, the stochastic rounding\nscheme was supported in hardware in Intel Loihi processor [40] and Graphcore IPU [85], since it was often required for\nquantizing weights and activations during training [166, 60, 172].\n7\nFuture Trend for Resource-Efﬁcient Deep Learning\nOpen research issues in resource-efﬁcient deep learning emerge in an attempt to improve the resource efﬁciency further,\ncompared to the state-of-the-art resource-efﬁcient techniques discussed in this paper.\n7.1\nFuture Trend for Model-Level Resource-Efﬁcient Techniques\nRecently, edge-based computing has become pervasive, and ﬁtting DNN models into such resource-constrained devices\nfor inference tasks has become extremely challenging.\n7.1.1\nImproving Physical Resource Efﬁciency under Very Low Compute Resource Budget:\nMany researchers considered keeping dense network structures after pruning parameters, including pruning channels\n[104, 53], ﬁlters [99, 107], etc., to implement the pruned networks efﬁciently on commercially available CPUs and\nGPUs. Since then, various budget-aware network pruning methods were proposed, given a resource budget such as the\nnumber of ﬂoating point operations [57] and the number of neurons [97] for the inference task. NetAdapt [171] pruned\nthe ﬁlters as it measured physical resources such as latency, energy, memory footprint, etc. to improve the physical\nresource efﬁciency directly rather than abstract resource efﬁciency. Along with the fast technology development\nin computer networks and wireless communications, research attempts to improve physical resource efﬁciency are\nexpected to continue to deploy appropriate DNN models on extremely low resource devices such as mobile, IoT, and\nedge devices.\n7.1.2\nNeural Network Search Methods Combined with Domain Speciﬁc Knowledge:\nIn 2016 and 2017, handcrafted compressed DNNs were presented such as SqueezeNet [82], MobileNet [76], ShufﬂeNet\n[181], and DenseNet [79], and they improved both abstract and physical resource efﬁciency. Various NAS methods\n[148, 149, 147, 70] assisted to seek the optimized DNN models (e.g., least sufﬁcient models) by searching candidate\n19\narXiv Template\nA PREPRINT\nspaces according to the training dataset, and the compressed models found by the NAS methods generally showed\nsuperior physical resource efﬁciency to the handcrafted compressed DNNs. As mobile and edge devices become\nprevalent, we expect that automatic search methods integrating with domain speciﬁc model compression methods are\nexpected to be paid attention in the future. For example, performance-aware NAS methods for resource-constrained\ndevices have been vividly paid attention [13, 147, 149, 148, 171] since 2019. Such performance-aware NAS methods can\nbe enhanced by adopting recent domain speciﬁc model-level resource-efﬁcient techniques such as [65, 82, 76, 99, 50].\n7.1.3\nTheoretical Studies Behind Model-Level Resource-Efﬁcient Techniques:\nThe bias-variance trade-off [54] is behind the model-level resource-efﬁcient techniques. For example, compressed\nmodels having fewer parameters increase the regularization effect on the accuracy, minimizing overﬁtting issues [65, 54].\nFor example, [50] proposed the lottery ticket hypothesis in that better (or equivalent) performing sub-DNNs using fewer\nweights exist inside a dense, randomly-initialized, feed-forward DNN. In order to seek the better performing sparse\nsub-DNNs, “winning tickets”, the survived weights from weight pruning were re-trained by replacing the survived\nweights with the random weights initially used to train the original dense DNN. The lottery ticket hypothesis implies\nthat such sparse DNNs could be found in even compressed dense DNNs. We expect that such theoretical studies\nsupporting model-level resource-efﬁcient techniques can be paid attention in the future.\n7.2\nFuture Trend for Arithmetic-Level Resource-Efﬁcient Techniques\nAs edge- and mobile-based devices becomes pervasive for AI applications, open research issues emerge in the attempts\nto improve further physical resource efﬁciency on such resource-constrained devices, compared to state-of-the-art\narithmetic-level techniques.\n7.2.1\nAdapting Arithmetic Precision Level to Numerical Properties of DNNs\nIn 2011, Vanhoucke et al. [155] demonstrated the feasibility of INT8 arithmetic for inference tasks using a shallow\ndepth neural network on an Intel x86 architecture. In 2015, Gupta et al. [60] demonstrated that employing FiP16 with\na stochastic rounding scheme for a shallow depth neural network produced equivalent accuracy using MNIST and\nCIFAR10 to that using IFP32. In 2018, [114] presented the guidelines for mixed precision training. The guidelines\ncontained the information on how to deploy different-level arithmetic precision on different computing components\nin MAC operations. The guidelines led to further research attempts in hardware optimization for mixed precision\ntraining [183]. We expect that the research attempts in adapting an arithmetic precision to DNN computing components\naccording to their numerical stability characteristics will continue in the future.\n7.2.2\nAdapting Arithmetic Format to Problem Complexity:\nSince ﬂoating point arithmetic is computationally intensive, several studies have removed ﬂoating point arithmetic in\ntraining tasks. For example, Wu et al. [166] demonstrated that quantized networks such as binary or ternary weight\nnetworks can be trained using INT8 arithmetic along with a scaling and a stochastic rounding scheme. In 2020, Yang\net al. [172] demonstrated that quantizing weights and activations with INT8 format while applying INT24 arithmetic\nto the weight updates could accelerate training and inference tasks for various ResNet models using ImageNet with\nminor accuracy loss, compared to those using IFP32. Recently, RNS-based quantization was applied to various DNNs\n[131, 130]. It will continue in the future to explore how to adapt a number format to given DNN structures for inference\nand training tasks in order to improve resource efﬁciency further on resource-constrained devices.\n7.3\nFuture Trend for Implementation-Level Resource-Efﬁcient Techniques\nIn general, there are two ways to accelerate DNN computations. One is to optimize DNN computations on given\ncompute architecture such as CPUs and GPUs. The other is to customize dataﬂow on FPGAs and ASIC.\n7.3.1\nLeveraging Spatial and Temporal Data Access Pattern with Lower Precision Arithmetic on CPUs and\nGPUs:\nA decade ago, [154] accelerated DNN computations on a SIMD CPU by leveraging the data reuse property from MAC\noperations using SSE instructions and fast ﬁxed point arithmetic. NVIDIA Tensor cores and Google TPUs support a\ncustomized arithmetic precision format such as IFP16, BFloat16, etc. and customized datapath in hardware for deep\nlearning applications [2, 3]. The research attempts to leverage spatial and temporal data access patterns with lower\nprecision arithmetic in commercially available CPUs and GPUs will continue in the future.\n20\narXiv Template\nA PREPRINT\n7.3.2\nLeveraging Spatial and Temporal Data Access Pattern with Lower Precision Arithmetic on FPGAs and\nASIC:\nIn 2014, [30] stressed the limitation of commercially available GPUs and CPUs for DNN applications: “While a cache\nis an excellent storage structure for a general-purpose processor, it is a sub-optimal way to exploit reuse because of the\ncache access overhead (tag check, associativity, line size, speculative read, etc.) and cache conﬂicts.” To overcome\nthis limitation, [30] proposed a SIMD style hardware accelerator, DianNao, that employs three separate local on-chip\nmemories (SRAMs) to maximize the performance by fully leveraging the data reuse property. [33] pointed out that\nDianNao was still limited in the memory bandwidth to access massive weights in covolutional layers and proposed\nDaDianNao architecture employing large eDRAMs with four banks to store and share the weights in the eDRAMs\nefﬁciently. In 2016, [31] pointed out that the data movement cost is still dominant, compared to the computation cost\nfor DNN applications on the SIMD/SIMT architectures of [33, 30] and proposed a dataﬂow architecture to minimize\nenergy consumption caused by data movement in DNN applications. Since 2016, most implementation-level techniques\nleveraged the sparsity of weights and activations in DNNs to minimize the number of arithmetic operations during\nruntime [10, 103, 87, 8, 41] and the data transfer cost required to store and transfer the sparse weights and activations\n[64, 7, 119, 125, 126]. The research efforts to customize DNN dataﬂow by leveraging spatial and temporal data access\npatterns with lower precision arithmetic are expected to continue in the future.\n7.3.3\nResource-Efﬁcient Implementation on Distributed AI Compute Platforms:\nResource-efﬁcient techniques on distributed AI such as split federated learning [62, 152] and early exiting [150, 151]\nhave recently attracted a great deal of attention thanks to fast wireless network technology development. The main open\nresearch issues include data communication overhead between an edge device and the cloud and energy consumption,\nrequired to run DNNs on a lower power (or battery) edge device. For example, many research attempts leveraged the\nbottleneck structure of a DNN to save the data communication bandwidth, but such attempts could degrade the accuracy\nsigniﬁcantly in the DNNs employing compact convolutions [111]. Thus, adapting such model compression techniques\nto distributed AI environments can be paid attention in the future in order to save energy consumption on edge devices\nand the bandwidth required for communication between an edge device and a cloud. For example, encoding and\ndecoding ofﬂoading data from the cloud to edge devices or vice versa can minimize the data communication overhead\n[173]. Such resource-efﬁcient encoding/decoding schemes for split learning (or inference) tasks can draw attention in\nthe future.\n7.3.4\nNeuromorphic Computing for Deep Learning:\nNeuromorphic computing can lead to dramatic changes in energy efﬁciency for deep learning [135]. This expectation\nis based on the fact that neuromorphic computing is not an incremental improvement of existing von Neumann\narchitectures requiring considerable energy due to substantial instruction fetch/decode operations, but an fully optimized\ndataﬂow optimization customized to the activity of a neural network. Therefore, neuromorphic computing research can\nbe paid attention in the future to maximize the accuracy per Joule.\n8\nConclusion\nOur survey is the ﬁrst to provide a comprehensive survey coverage of the recent resource-efﬁcient deep learning\ntechniques based on the three-level hierarchy including model-, arithmetic-, and implementation-level techniques. Our\nsurvey also utilizes multiple resource efﬁciency metrics to clarify which resource efﬁciency metrics each technique\ncan improve. For example, most model-level resource-efﬁcient techniques contribute to improving abstract resource\nefﬁciency, while the arithmetic- and the implementation-level techniques directly contribute to improving physical\nresource efﬁciency by employing reduced precision arithmetic and/or optimizing the dataﬂow of DNN architectures.\nTherefore, the efﬁcient implementation of the model-level techniques on given compute platforms is essential to improve\nphysical resource efﬁciency [145].\nIn the future, we expect that the three-level resource-efﬁcient deep learning techniques can be adapted to distributed\nAI applications, along with fast wireless communication technology development. Since edge or mobile devices\nare subjected to physical resource constraints such as power, memory, and inference speed, the implementation\nshould consider such constraints for the distributed AI applications. The state-of-the-art works include the NAS\nvariants of [147, 133, 115] that seek the optimal performing DNN models ﬁtted to the resource-constrained edge-\ndevices. Improving such NAS variants by combining them with various model-, arithmetic-, and implementation-level\nresource-efﬁcient techniques can be paid attention in the future.\n21\narXiv Template\nA PREPRINT\nFinally, our survey suggests that the bias-variance trade-off [54] is behind the model-level resource-efﬁcient techniques.\nAccording to the trade-off, a DNN having fewer parameters increases the regularization effect on the accuracy,\nminimizing overﬁtting issues. Therefore, there exists the least sufﬁcient model size that produces the best accuracy on\ntest dataset according to the problem complexity and the training data quantity and quality. Similarly, [50] claimed\nthe lottery ticket hypothesis in that better (or equivalent) performing sub-DNNs using fewer weights exist inside a\ndense feed-forward DNN. Einstein quoted “Everything should be made as simple as possible, but not simpler.” We\nhope that our survey will contribute to machine learning, arithmetic, and system community by providing them with a\ncomprehensive survey for various resource-efﬁcient deep learning techniques as guidelines to seek DNN structures\nusing least sufﬁcient parameters and least sufﬁcient precision arithmetic on particular compute platforms, customized to\nthe problem complexity and the training data quantity and quality.\nAcknowledgments\nThis project has received funding by the Engineering and Physical Sciences Research Council under the grant agreement\nNo. EP/T022345/1 and by CHIST-ERA under the grant agreement No. CHIST-ERA-18-SDCDN-002 (DiPET). This\nresearch was also partially supported by National R&D Program through the National Research Foundation of Korea\n(NRF) funded by Ministry of Science and ICT (2021M3H2A1038042)\nReferences\n[1] Imagenet benchmark (image classiﬁcation on imagenet).\nhttps://paperswithcode.com/sota/\nimage-classification-on-imagenet.\n[2] Nvidia ampere architecture white paper.\nhttps://images.nvidia.com/aem-dam/en-zz/Solutions/\ndata-center/nvidia-ampere-architecture-whitepaper.pdf.\n[3] Quantifying the performance of the TPU, our ﬁrst machine learning chip. https://cloud.google.com/blog/\nproducts/gcp/quantifying-the-performance-of-the-tpu-our-first-machine-learning-chip.\n[4] Ieee standard for ﬂoating-point arithmetic. IEEE Std 754-2019 (Revision of IEEE 754-2008), pages 1–84, 2019.\n[5] Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning From Data. AMLBook, 2012.\n[6] A. Agrawal, S. M. Mueller, B. M. Fleischer, X. Sun, N. Wang, J. Choi, and K. Gopalakrishnan. Dlﬂoat: A\n16-b ﬂoating point format designed for deep learning training and inference. In 2019 IEEE 26th Symposium on\nComputer Arithmetic (ARITH), pages 92–95, 2019.\n[7] Alessandro Aimar, Hesham Mostafa, Enrico Calabrese, Antonio Rios-Navarro, Ricardo Tapiador-Morales,\nIulia-Alexandra Lungu, Moritz B. Milde, Federico Corradi, Alejandro Linares-Barranco, Shih-Chii Liu, and\nTobi Delbruck. Nullhop: A ﬂexible convolutional neural network accelerator based on sparse representations of\nfeature maps. IEEE Transactions on Neural Networks and Learning Systems, 30(3):644–656, 2019.\n[8] V. Akhlaghi, A. Yazdanbakhsh, K. Samadi, R. K. Gupta, and H. Esmaeilzadeh. Snapea: Predictive early\nactivation for reducing computation in deep convolutional neural networks. In 2018 ACM/IEEE 45th Annual\nInternational Symposium on Computer Architecture (ISCA), pages 662–673, Los Alamitos, CA, USA, jun 2018.\nIEEE Computer Society.\n[9] Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John Arthur, Paul Merolla, Nabil Imam,\nYutaka Nakamura, Pallab Datta, Gi-Joon Nam, Brian Taba, Michael Beakes, Bernard Brezzo, Jente B. Kuang,\nRajit Manohar, William P. Risk, Bryan Jackson, and Dharmendra S. Modha. Truenorth: Design and tool ﬂow of\na 65 mw 1 million neuron programmable neurosynaptic chip. IEEE Transactions on Computer-Aided Design of\nIntegrated Circuits and Systems, 34(10):1537–1557, 2015.\n[10] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright Jerger, and Andreas Moshovos.\nCnvlutin: Ineffectual-neuron-free deep neural network computing. In 2016 ACM/IEEE 43rd Annual International\nSymposium on Computer Architecture (ISCA), pages 1–13, 2016.\n[11] Mustafa F. Ali, Robert Andrawis, and Kaushik Roy. Dynamic read current sensing with ampliﬁed bit-line voltage\nfor stt-mrams. IEEE Transactions on Circuits and Systems II: Express Briefs, 67(3):551–555, 2020.\n[12] Mustafa F. Ali, Akhilesh Jaiswal, and Kaushik Roy. In-memory low-cost bit-serial addition using commodity\ndram technology. IEEE Transactions on Circuits and Systems I: Regular Papers, 67(1):155–165, 2020.\n[13] Andrew Anderson, Jing Su, Rozenn Dahyot, and David Gregg. Performance-oriented neural architecture search,\n2020.\n[14] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in Neural Information Processing\nSystems (NeurIPS), 2014.\n22\narXiv Template\nA PREPRINT\n[15] Arindam Basu. Small-signal neural models and their applications. IEEE Transactions on Biomedical Circuits\nand Systems, 6(1):64–75, 2012.\n[16] Arindam Basu, Csaba Petre, and Paul Hasler. Bifurcations in a silicon neuron. In 2008 IEEE International\nSymposium on Circuits and Systems, pages 428–431, 2008.\n[17] Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Isabelle Guyon,\nGideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver, editors, Proceedings of ICML Workshop on\nUnsupervised and Transfer Learning, volume 27 of Proceedings of Machine Learning Research, pages 17–36,\nBellevue, Washington, USA, 02 Jul 2012. PMLR.\n[18] Ben Varkey Benjamin, Peiran Gao, Emmett McQuinn, Swadesh Choudhary, Anand R. Chandrasekaran, Jean-\nMarie Bussat, Rodrigo Alvarez-Icaza, John V. Arthur, Paul A. Merolla, and Kwabena Boahen. Neurogrid: A\nmixed-analog-digital multichip system for large-scale neural simulations. Proceedings of the IEEE, 102(5):699–\n716, 2014.\n[19] Pierre Blanchard, Nicholas J Higham, Florent Lopez, Theo Mary, and Srikara Pranesh. Mixed precision block\nfused multiply-add: Error analysis and application to gpu tensor cores. SIAM Journal on Scientiﬁc Computing,\n42(3):C124–C141, 2020.\n[20] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection, 2020.\n[21] R. Bordawekar, B. Abali, and M.H. Chen. Eﬂoat: Entropy-coded ﬂoating point format for deep learning.\narXiv:2102.02705, 2021.\n[22] Léon Bottou and Yann Le Cun. Large scale online learning. In Advances in Neural Information Processing\nSystems 16. MIT Press, 2004.\n[23] Cristian Bucilu˘a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06, page 535–541,\nNew York, NY, USA, 2006. Association for Computing Machinery.\n[24] N. Burgess, J. Milanovic, N. Stephens, K. Monachopoulos, and D. Mansell. Bﬂoat16 processing for neural\nnetworks. In 2019 IEEE 26th Symposium on Computer Arithmetic (ARITH), pages 88–91, 2019.\n[25] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and\nspecialize it for efﬁcient deployment. In ICLR ’20: International Conference on Learning Representations, 2020.\n[26] Z. Carmichael, H. F. Langroudi, C. Khazanov, J. Lillie, J. L. Gustafson, and D. Kudithipudi. Deep positron: A\ndeep neural network using the posit number system. In 2019 Design, Automation Test in Europe Conference\nExhibition (DATE), pages 1421–1426, 2019.\n[27] Zachariah Carmichael, Hamed F. Langroudi, Char Khazanov, Jeffrey Lillie, John L. Gustafson, and Dhireesha\nKudithipudi. Performance-efﬁciency trade-off of low-precision numerical formats in deep neural networks. In\nProceedings of the Conference for Next Generation Arithmetic 2019, CoNGA’19, New York, NY, USA, 2019.\nAssociation for Computing Machinery.\n[28] Chip-Hong Chang, Amir Sabbagh Molahosseini, Azadeh Alsadat Emrani Zarandi, and Tian Fatt Tay. Residue\nnumber systems: A new paradigm to datapath optimization for low-power and high-performance digital signal\nprocessing applications. IEEE Circuits and Systems Magazine, 15(4):26–44, 2015.\n[29] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efﬁcient object\ndetection models with knowledge distillation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 742–751.\nCurran Associates, Inc., 2017.\n[30] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. Diannao: A\nsmall-footprint high-throughput accelerator for ubiquitous machine-learning. SIGPLAN Not., 49(4):269–284,\nFebruary 2014.\n[31] Y. Chen, J. Emer, and V. Sze. Eyeriss: A spatial architecture for energy-efﬁcient dataﬂow for convolutional\nneural networks. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA),\npages 367–379, 2016.\n[32] Yu-Hsin Chen, Tushar Krishna, Joel S. Emer, and Vivienne Sze. Eyeriss: An energy-efﬁcient reconﬁgurable\naccelerator for deep convolutional neural networks. IEEE Journal of Solid-State Circuits, 52(1):127–138, 2017.\n[33] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu,\nNinghui Sun, and Olivier Temam. Dadiannao: A machine-learning supercomputer. In Proceedings of the 47th\nAnnual IEEE/ACM International Symposium on Microarchitecture, MICRO-47, pages 609–622, Washington,\nDC, USA, 2014. IEEE Computer Society.\n23\narXiv Template\nA PREPRINT\n[34] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan,\nand Jiashi Feng. Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave\nconvolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October\n2019.\n[35] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and acceleration for deep neural networks: The\nprinciples, progress, and challenges. IEEE Signal Processing Magazine, 35(1):126–136, 2018.\n[36] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n[37] NVIDIA Corporation. NVIDIA Tesla V100 GPU architecture, 8 2017. WP-08608-001v1.1.\n[38] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\neditors, Advances in Neural Information Processing Systems 28, pages 3123–3131. Curran Associates, Inc.,\n2015.\n[39] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks:\nTraining deep neural networks with weights and activations constrained to +1 or -1, 2016.\n[40] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday,\nGeorgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, Yuyun Liao, Chit-Kwan Lin, Andrew Lines, Ruokun\nLiu, Deepak Mathaikutty, Steven McCoy, Arnab Paul, Jonathan Tse, Guruguhanathan Venkataramanan, Yi-Hsin\nWeng, Andreas Wild, Yoonseok Yang, and Hong Wang. Loihi: A neuromorphic manycore processor with\non-chip learning. IEEE Micro, 38(1):82–99, 2018.\n[41] Chunhua Deng, Yang Sui, Siyu Liao, Xuehai Qian, and Bo Yuan. Gospa: An energy-efﬁcient high-performance\nglobally optimized sparse convolutional neural network accelerator. In 2021 ACM/IEEE 48th Annual Interna-\ntional Symposium on Computer Architecture (ISCA), pages 1110–1123, 2021.\n[42] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration for\nneural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):485–532, 2020.\n[43] Quan Deng, Lei Jiang, Youtao Zhang, Minxuan Zhang, and Jun Yang. Dracc: a dram based accelerator for\naccurate cnn inference. In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), pages 1–6, 2018.\n[44] Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within\nconvolutional networks for efﬁcient evaluation. In Proceedings of the 27th International Conference on Neural\nInformation Processing Systems - Volume 1, NeurIPS’14, page 1269–1277, Cambridge, MA, USA, 2014. MIT\nPress.\n[45] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsaﬁ. Training dnns with hybrid block ﬂoating point. In\n32nd Conference on Neural Information Processing Systems, 2018.\n[46] D. Dupeyron, S. Le Masson, Y. Deval, G. Le Masson, and J.-P. Dom. A bicmos implementation of the hodgkin-\nhuxley formalism. In Proceedings of Fifth International Conference on Microelectronics for Neural Networks,\npages 311–316, 1996.\n[47] Biyi Fang, Xiao Zeng, and Mi Zhang. Nestdnn: Resource-aware multi-tenant on-device deep learning for\ncontinuous mobile vision. MobiCom ’18, New York, NY, USA, 2018. Association for Computing Machinery.\n[48] M. Fasi, N.J. Higham, M. Mikaitis, and S. Pranesh. Numerical behavior of nvidia tensor cores. PeerJ Computer\nScience, 7(e330):1–19, 2021.\n[49] Sean Fox, Seyedramin Rasoulinezhad, Julian Faraone, David Boland, and Philip Leong. A block miniﬂoat repre-\nsentation for training deep neural networks. In ICLR ’21: International Conference on Learning Representations,\n2021.\n[50] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.\nIn ICLR ’19: International Conference on Learning Representations, 2019.\n[51] Adi Fuchs and David Wentzlaff. The accelerator wall: Limits of chip specialization. In 2019 IEEE International\nSymposium on High Performance Computer Architecture (HPCA), pages 1–14, 2019.\n[52] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, and Christos Kozyrakis. Tetris: Scalable and efﬁcient neural\nnetwork acceleration with 3d memory. 45(1):751–764, April 2017.\n[53] Xitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, and Cheng zhong Xu. Dynamic channel pruning:\nFeature boosting and suppression. In ICLR ’19: International Conference on Learning Representations, 2019.\n[54] Stuart Geman, Elie Bienenstock, and René Doursat. Neural networks and the bias/variance dilemma. Neural\nComputation, 4(1):1–58, 1992.\n24\narXiv Template\nA PREPRINT\n[55] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and Kurt Keutzer.\nSqueezenext: Hardware-aware neural network design. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, June 2018.\n[56] Meisam Gholami and Saeed Saeedi. Digital cellular implementation of morris-lecar neuron model. In 2015 23rd\nIranian Conference on Electrical Engineering, pages 1235–1239, 2015.\n[57] Ariel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. Morphnet: Fast\namp; simple resource-constrained structure learning of deep networks. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1586–1595, 2018.\n[58] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efﬁcient dnns. In Proceedings of\nthe 30th International Conference on Neural Information Processing Systems, NIPS’16, page 1387–1395, Red\nHook, NY, USA, 2016. Curran Associates Inc.\n[59] Rishi Raj Gupta and Virender Ranga. Comparative study of different reduced precision techniques in deep\nneural network. In Shailesh Tiwari, Erma Suryani, Andrew Keong Ng, K. K. Mishra, and Nitin Singh, editors,\nProceedings of International Conference on Big Data, Machine Learning and their Applications, pages 123–136,\nSingapore, 2021. Springer Singapore.\n[60] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited\nnumerical precision. In Proceedings of the 32nd International Conference on International Conference on\nMachine Learning - Volume 37, ICML’15, page 1737–1746. JMLR.org, 2015.\n[61] Gustafson and Yonemoto. Beating ﬂoating point at its own game: Posit arithmetic. Supercomput. Front. Innov.:\nInt. J., 4(2):71–86, June 2017.\n[62] Yoo Jeong Ha, Minjae Yoo, Gusang Lee, Soyi Jung, Sae Won Choi, Joongheon Kim, and Seehwan Yoo. Spatio-\ntemporal split learning for privacy-preserving medical platforms: Case studies with covid-19 ct, x-ray, and\ncholesterol data. IEEE Access, 9:121046–121059, 2021.\n[63] Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, João Dinis Ferreira, Nika Mansouri Ghiasi, Minesh Patel,\nMohammed Alser, Saugata Ghose, Juan Gómez-Luna, and Onur Mutlu. Simdram: A framework for bit-serial\nsimd processing using dram. In Proceedings of the 26th ACM International Conference on Architectural Support\nfor Programming Languages and Operating Systems, ASPLOS 2021, page 329–345, New York, NY, USA, 2021.\nAssociation for Computing Machinery.\n[64] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. Eie:\nEfﬁcient inference engine on compressed deep neural network. In Proceedings of the 43rd International\nSymposium on Computer Architecture, ISCA ’16, page 243–254. IEEE Press, 2016.\n[65] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. In ICLR ’16: International Conference on Learning Representations,\n2016.\n[66] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efﬁcient neural\nnetwork. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 28, pages 1135–1143. Curran Associates, Inc., 2015.\n[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[68] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N.\nVijaykumar. Newton: A dram-maker’s accelerator-in-memory (aim) architecture for machine learning. In 2020\n53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 372–385, 2020.\n[69] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In 2017 IEEE\nInternational Conference on Computer Vision (ICCV), pages 1398–1406, 2017.\n[70] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression\nand acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV),\nSeptember 2018.\n[71] Kartik Hegde, Jiyong Yu, Rohit Agrawal, Mengjia Yan, Michael Pellauer, and Christopher W. Fletcher. Ucnn:\nExploiting computational reuse in deep neural networks via weight repetition. In Proceedings of the 45th Annual\nInternational Symposium on Computer Architecture, ISCA ’18, page 674–687. IEEE Press, 2018.\n[72] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.\n[73] Nhut-Minh Ho and Weng-Fai Wong. Exploiting half precision arithmetic in nvidia gpus. In 2017 IEEE High\nPerformance Extreme Computing Conference (HPEC), pages 1–7, 2017.\n25\narXiv Template\nA PREPRINT\n[74] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, nov\n1997.\n[75] A. L. Hodgkin and A. F. Huxley. A quantitative description of membrane current and its application to conduction\nand excitation in nerve. The Journal of Physiology, 117(4):500–544, 1952.\n[76] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications,\n2017.\n[77] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning\napproach towards efﬁcient deep architectures, 2016.\n[78] Huiyi Hu, Ang Li, Daniele Calandriello, , and Dilan Gorur. One pass imagenet. In NeurIPS 2021 Workshop on\nImagenet: past, present and future, 2021.\n[79] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, 2017.\n[80] Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Condensenet: An efﬁcient densenet\nusing learned group convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018.\n[81] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.\nQuantized neu-\nral networks: Training neural networks with low precision weights and activations. J. Mach. Learn. Res.,\n18(1):6869–6898, January 2017.\n[82] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer.\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size, 2016.\n[83] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on\nMachine Learning, volume 37 of Proceedings of Machine Learning Research, pages 448–456, Lille, France,\n07–09 Jul 2015. PMLR.\n[84] E.M. Izhikevich. Which model to use for cortical spiking neurons? IEEE Transactions on Neural Networks,\n15(5):1063–1070, 2004.\n[85] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting the graphcore ipu architecture\nvia microbenchmarking. arXiv preprint arXiv:1912.03413, 2019.\n[86] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha,\nDharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bﬂoat16 for deep\nlearning training. arXiv preprint arXiv:1905.12322, 2019.\n[87] Dongyoung Kim, Junwhan Ahn, and Sungjoo Yoo. Zena: Zero-aware neural network accelerator. IEEE Design\nTest, 35(1):39–46, 2018.\n[88] Duckhwan Kim, Jaeha Kung, Sek Chai, Sudhakar Yalamanchili, and Saibal Mukhopadhyay. Neurocube: A\nprogrammable digital neuromorphic architecture with high-density 3d memory. In 2016 ACM/IEEE 43rd Annual\nInternational Symposium on Computer Architecture (ISCA), pages 380–392, 2016.\n[89] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compression of\ndeep convolutional neural networks for fast and low power mobile applications. In ICLR ’16: International\nConference on Learning Representations, 2016.\n[90] Urs Köster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K. Bansal, William H. Constable, O˘guz H.\nElibol, Scott Gray, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, and Naveen Rao.\nFlexpoint: An adaptive numerical format for efﬁcient training of deep neural networks. In Proceedings of the\n31st International Conference on Neural Information Processing Systems, NeurIPS’17, page 1740–1750, Red\nHook, NY, USA, 2017. Curran Associates Inc.\n[91] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. Commun. ACM, 60(6):84–90, May 2017.\n[92] S. Le Masson, A. Laﬂaquiere, T. Bal, and G. Le Masson. Analog circuits for modeling biological neural networks:\ndesign and applications. IEEE Transactions on Biomedical Engineering, 46(6):638–645, 1999.\n[93] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.\n[94] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. In D. Touretzky, editor, Advances in Neural\nInformation Processing Systems 2, volume 2. Morgan-Kaufmann, 1990.\n26\narXiv Template\nA PREPRINT\n[95] JunKyu Lee, Gregory D. Peterson, Dimitrios S. Nikolopoulos, and Hans Vandierendonck. Air: Iterative\nreﬁnement acceleration using arbitrary dynamic precision. Parallel Computing, 97:102663, 2020.\n[96] JunKyu Lee, Blesson Varghese, Roger Woods, and Hans Vandierendonck. Tod: Transprecise object detection to\nmaximise real-time accuracy on the edge. In IEEE International Conference on Fog and Edge Computing, pages\n53–60, 2021.\n[97] Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning of neural networks with budget-aware\nregularization. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n9100–9108, 2019.\n[98] Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. In Workshop on Efﬁcient Methods for Deep Neural\nNetworks in the 30th International Conference on Neural Information Processing Systems, NeurIPS’16, 2016.\n[99] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets.\nIn ICLR ’17: International Conference on Learning Representations, 2017.\n[100] Jiajun Li, Shuhao Jiang, Shijun Gong, Jingya Wu, Junchao Yan, Guihai Yan, and Xiaowei Li. Squeezeﬂow: A\nsparse cnn accelerator exploiting concise convolution rules. IEEE Transactions on Computers, 68(11):1663–1677,\n2019.\n[101] Shuangchen Li, Dimin Niu, Krishna T. Malladi, Hongzhong Zheng, Bob Brennan, and Yuan Xie. Drisa: A\ndram-based reconﬁgurable in-situ accelerator. In 2017 50th Annual IEEE/ACM International Symposium on\nMicroarchitecture (MICRO), pages 288–301, 2017.\n[102] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In Proceedings of the 31st International\nConference on Neural Information Processing Systems, NeurIPS’17, page 2178–2188, Red Hook, NY, USA,\n2017. Curran Associates Inc.\n[103] S. Liu, Z. Du, J. Tao, D. Han, T. Luo, Y. Xie, Y. Chen, and T. Chen. Cambricon: An instruction set architecture\nfor neural networks. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture\n(ISCA), pages 393–405, 2016.\n[104] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efﬁcient convolutional networks through\nnetwork slimming. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2755–2763, 2017.\n[105] Zhi-Gang Liu and Matthew Mattina. Efﬁcient residue number system based winograd convolution. In European\nConference on Computer Vision, pages 53–68. Springer, 2020.\n[106] Wei Lou, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, and Geoff V. Merrett. Dynamic-ofa: Runtime dnn\narchitecture switching for performance scaling on heterogeneous embedded platforms. In 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3104–3112, 2021.\n[107] J. Luo, J. Wu, and W. Lin. Thinet: A ﬁlter level pruning method for deep neural network compression. In 2017\nIEEE International Conference on Computer Vision (ICCV), pages 5068–5076, 2017.\n[108] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for efﬁcient cnn\narchitecture design. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\n[109] Qingyun Ma, Mohammad Raﬁqul Haider, Vinaya Lal Shrestha, and Yehia Massoud. Bursting hodgkin—\nhuxley model-based ultra-low-power neuromimetic silicon neuron. Analog Integr. Circuits Signal Process.,\n73(1):329–337, October 2012.\n[110] Zelda Mariet and Suvrit Sra. Diversity networks: Neural network compression using determinantal point\nprocesses. In ICLR ’16: International Conference on Learning Representations, 2016.\n[111] Yoshitomo Matsubara, Marco Levorato, and Francesco Restuccia. Split computing and early exiting for deep\nlearning applications: Survey and research challenges, 2021.\n[112] Warren Mcculloch and Walter Pitts. A logical calculus of ideas immanent in nervous activity. Bulletin of\nMathematical Biophysics, 5:127–147, 1943.\n[113] Sally A. McKee. Reﬂections on the memory wall. In Proceedings of the 1st Conference on Computing Frontiers,\nCF ’04, page 162, New York, NY, USA, 2004. Association for Computing Machinery.\n[114] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich K Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In\nICLR ’18: International Conference on Learning Representations, 2018.\n[115] Umar Ibrahim Minhas, Lev Mukhanov, Georgios Karakonstantis, Hans Vandierendonck, and Roger Woods.\nLeveraging transprecision computing for machine vision applications at the edge. In 2021 IEEE Workshop on\nSignal Processing Systems (SiPS), pages 205–210, 2021.\n27\narXiv Template\nA PREPRINT\n[116] Michael A. Nielsen. Neural networks and deep learning, 2018.\n[117] Eustace Painkras, Luis A. Plana, Jim Garside, Steve Temple, Francesco Galluppi, Cameron Patterson, David R.\nLester, Andrew D. Brown, and Steve B. Furber. Spinnaker: A 1-w 18-core system-on-chip for massively-parallel\nneural network simulation. IEEE Journal of Solid-State Circuits, 48(8):1943–1953, 2013.\n[118] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for energy-efﬁcient and\nenhanced pattern recognition. In 2016 Design, Automation Test in Europe Conference Exhibition (DATE), pages\n475–480, 2016.\n[119] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan Venkatesan, Brucek\nKhailany, Joel Emer, Stephen W. Keckler, and William J. Dally. Scnn: An accelerator for compressed-sparse\nconvolutional neural networks. In Proceedings of the 44th Annual International Symposium on Computer\nArchitecture, ISCA ’17, page 27–40, New York, NY, USA, 2017. Association for Computing Machinery.\n[120] Behrooz Parhami. Computer Arithmetic: Algorithms and Hardware Designs. Oxford University Press, New\nYork, 2010.\n[121] Jongkil Park, Sohmyung Ha, Theodore Yu, Emre Neftci, and Gert Cauwenberghs. A 65k-neuron 73-mevents/s\n22-pj/event asynchronous micro-pipelined integrate-and-ﬁre array transceiver. In 2014 IEEE Biomedical Circuits\nand Systems Conference (BioCAS) Proceedings, pages 675–678, 2014.\n[122] Z. Qin, Z. Zhang, X. Chen, C. Wang, and Y. Peng. Fd-mobilenet: Improved mobilenet with a fast downsampling\nstrategy. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 1363–1367, 2018.\n[123] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation\nusing binary convolutional neural networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,\nComputer Vision – ECCV 2016, pages 525–542, Cham, 2016. Springer International Publishing.\n[124] Arthur J Redfern, Lijun Zhu, and Molly K Newquist. Bcnn: A binary cnn with all matrix ops quantized to 1 bit\nprecision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n4604–4612, 2021.\n[125] M. Rhu, M. O’Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W. Keckler.\nCompressing dma engine:\nLeveraging activation sparsity for training deep neural networks. In 2018 IEEE International Symposium on\nHigh Performance Computer Architecture (HPCA), pages 78–91, 2018.\n[126] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulﬁqar, and Stephen W. Keckler. Vdnn: Virtualized\ndeep neural networks for scalable, memory-efﬁcient neural network design. In The 49th Annual IEEE/ACM\nInternational Symposium on Microarchitecture, MICRO-49. IEEE Press, 2016.\n[127] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.\nFitnets: Hints for thin deep nets. In ICLR ’15: International Conference on Learning Representations, 2015.\n[128] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain.\nPsychological Review, 65(6):386–408, 1958.\n[129] Sylvain Saighi, Laure Buhry, Yannick Bornat, Gilles N’Kaoua, Jean Tomas, and Sylvie Renaud. Adjusting the\nneurons models in neuromimetic ics using the voltage-clamp technique. In 2008 IEEE International Symposium\non Circuits and Systems, pages 1564–1567, 2008.\n[130] S. Salamat, M. Imani, S. Gupta, and T. Rosing. Rnsnet: In-memory neural network acceleration using residue\nnumber system. In 2018 IEEE International Conference on Rebooting Computing (ICRC), pages 1–12, 2018.\n[131] N. Samimi, M. Kamal, A. Afzali-Kusha, and M. Pedram. Res-dnn: A residue number system-based dnn\naccelerator unit. IEEE Transactions on Circuits and Systems I: Regular Papers, 67(2):658–671, 2020.\n[132] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:\nInverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), June 2018.\n[133] Florian Scheidegger, Luca Benini, Costas Bekas, and A. Cristiano I. Malossi. Constrained deep neural network\narchitecture search for iot devices accounting for hardware calibration. In Advances in Neural Information\nProcessing Systems 32, pages 6056–6066, 2019.\n[134] Johannes Schemmel, Andreas Grübl, Stephan Hartmann, Alexander Kononov, Christian Mayr, Karlheinz Meier,\nSebastian Millner, Johannes Partzsch, Stefan Schiefer, Stefan Scholze, Rene Schüffny, and Marc-Olivier Schwartz.\nLive demonstration: A scaled-down version of the brainscales wafer-scale neuromorphic system. In 2012 IEEE\nInternational Symposium on Circuits and Systems (ISCAS), pages 702–702, 2012.\n28\narXiv Template\nA PREPRINT\n[135] Catherine D Schuman, Thomas E Potok, Robert M Patton, J Douglas Birdwell, Mark E Dean, Garrett S Rose,\nand James S Plank. A survey of neuromorphic computing and neural networks in hardware. arXiv preprint\narXiv:1705.06963, 2017.\n[136] A. Shaﬁee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Strachan, M. Hu, R. S. Williams, and\nV. Srikumar. Isaac: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. In\n2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), pages 14–26, 2016.\n[137] Li Shang, Alireza S. Kaviani, and Kusuma Bathala. Dynamic power consumption in virtex™-ii fpga family.\nIn Proceedings of the 2002 ACM/SIGDA Tenth International Symposium on Field-programmable Gate Arrays,\nFPGA ’02, pages 157–164, New York, NY, USA, 2002. ACM.\n[138] Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, and Hadi Esmaeilzadeh.\nBit fusion: Bit-level dynamically composable architecture for accelerating deep neural networks. In Proceedings\nof the 45th Annual International Symposium on Computer Architecture, ISCA ’18, page 764–775. IEEE Press,\n2018.\n[139] Jonghan Shin and C. Koch. Dynamic range and sensitivity adaptation in a silicon spiking neuron. IEEE\nTransactions on Neural Networks, 10(5):1232–1238, 1999.\n[140] Mario F. Simoni, Gennady S. Cymbalyuk, Michael Elliott Sorensen, Ronald L. Calabrese, and Stephen P.\nDeWeerth. Development of hybrid systems: Interfacing a silicon neuron to a leech heart interneuron. In Todd K.\nLeen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13,\nPapers from Neural Information Processing Systems (NeurIPS) 2000, Denver, CO, USA, pages 173–179. MIT\nPress, 2000.\n[141] M.F. Simoni, G.S. Cymbalyuk, M.E. Sorensen, R.L. Calabrese, and S.P. DeWeerth. A multiconductance silicon\nneuron with biologically matched dynamics. IEEE Transactions on Biomedical Engineering, 51(2):342–354,\n2004.\n[142] M.F. Simoni and S.P. DeWeerth. Adaptation in a vlsi model of a neuron. IEEE Transactions on Circuits and\nSystems II: Analog and Digital Signal Processing, 46(7):967–970, 1999.\n[143] Suraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for deep neural networks. In Proceedings of\nthe British Machine Vision Conference (BMVC), pages 31.1–31.12. BMVA Press, September 2015.\n[144] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi (Viji)\nSrinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit ﬂoating point (hfp8) training\nand inference for deep neural networks. In Advances in Neural Information Processing Systems 32, pages\n4900–4909. Curran Associates, Inc., 2019.\n[145] V. Sze, Y. Chen, T. Yang, and J. S. Emer. Efﬁcient processing of deep neural networks: A tutorial and survey.\nProceedings of the IEEE, 105(12):2295–2329, 2017.\n[146] Thierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks,\nand Gu-Yeon Wei. Algorithm-hardware co-design of adaptive ﬂoating-point encodings for resilient deep learning\ninference. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pages 1–6, 2020.\n[147] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le.\nMnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2019.\n[148] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking model scaling for convolutional neural networks. volume 97\nof Proceedings of Machine Learning Research, pages 6105–6114, Long Beach, California, USA, 09–15 Jun\n2019. PMLR.\n[149] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efﬁcientdet: Scalable and efﬁcient object detection. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n[150] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. Branchynet: Fast inference via early exiting from\ndeep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464–2469,\n2016.\n[151] Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. Distributed deep neural networks over the cloud, the\nedge and end devices. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS),\npages 328–339, 2017.\n[152] Chandra Thapa, M. A. P. Chamikara, Seyit Camtepe, and Lichao Sun. Splitfed: When federated learning meets\nsplit learning, 2021.\n29\narXiv Template\nA PREPRINT\n[153] Stephen Tridgell, Martin Kumm, Martin Hardieck, David Boland, Duncan Moss, Peter Zipf, and Philip H. W.\nLeong. Unrolling ternary neural networks. ACM Trans. Reconﬁgurable Technol. Syst., 12(4), October 2019.\n[154] Robert van de Geijn and Kazushige Goto. BLAS (Basic Linear Algebra Subprograms), pages 157–164. Springer\nUS, Boston, MA, 2011.\n[155] Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed of neural networks on cpus. In\nDeep Learning and Unsupervised Feature Learning Workshop, NeurIPS 2011, 2011.\n[156] S. Vogel, M. Liang, A. Guntoro, W. Stechele, and G. Ascheid. Efﬁcient hardware acceleration of cnns using\nlogarithmic data representation with arbitrary log-base. In 2018 IEEE/ACM International Conference on\nComputer-Aided Design (ICCAD), pages 1–8, 2018.\n[157] Erwei Wang, James J. Davis, Ruizhe Zhao, Ho-Cheung Ng, Xinyu Niu, Wayne Luk, Peter Y. K. Cheung, and\nGeorge A. Constantinides. Deep neural network approximation for custom hardware: Where we’ve been, where\nwe’re going. ACM Comput. Surv., 52(2), May 2019.\n[158] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural\nnetworks with 8-bit ﬂoating point numbers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 7675–7684.\nCurran Associates, Inc., 2018.\n[159] Peisong Wang, Xiangyu He, Qiang Chen, Anda Cheng, Qingshan Liu, and Jian Cheng. Unsupervised network\nquantization via ﬁxed-point factorization. IEEE Transactions on Neural Networks and Learning Systems, 2020.\n[160] P. J. Werbos.\nBackpropagation through time: what it does and how to do it.\nProceedings of the IEEE,\n78(10):1550–1560, 1990.\n[161] J. H. Wilkinson, editor. The Algebraic Eigenvalue Problem. Oxford University Press, Inc., New York, NY, USA,\n1988.\n[162] Samuel Williams, Andrew Waterman, and David Patterson. Rooﬂine: An insightful visual performance model\nfor multicore architectures. Commun. ACM, 52(4):65–76, April 2009.\n[163] H. R. Wilson and Jack D. Cowan. Excitatory and inhibitory interactions in localized populations of model\nneurons. Biophysical Journal, 12:1–24, 1972.\n[164] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda,\nYangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural\narchitecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019.\n[165] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep\nlearning inference: Principles and empirical evaluation, 2020.\n[166] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks.\narXiv preprint arXiv:1802.04680, 2018.\n[167] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations\nfor deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages\n5987–5995, 2017.\n[168] Xin Xin, Youtao Zhang, and Jun Yang. Elp2im: Efﬁcient and low power bitwise operation processing in dram.\nIn 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 303–314,\n2020.\n[169] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and Xian-sheng\nHua. Quantization networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7308–7316, 2019.\n[170] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efﬁcient convolutional neural networks using\nenergy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), July 2017.\n[171] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig\nAdam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Vittorio Ferrari, Martial\nHebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision – ECCV 2018, pages 289–304, Cham,\n2018. Springer International Publishing.\n[172] Yukuan Yang, Lei Deng, Shuang Wu, Tianyi Yan, Yuan Xie, and Guoqi Li. Training high-performance and\nlarge-scale deep neural networks with full 8-bit integers. Neural Networks, 125:70 – 82, 2020.\n30\narXiv Template\nA PREPRINT\n[173] Shuochao Yao, Jinyang Li, Dongxin Liu, Tianshi Wang, Shengzhong Liu, Huajie Shao, and Tarek Abdelzaher.\nDeep compressive ofﬂoading: Speeding up neural network inference by trading edge computation for network\nlatency. In Proceedings of the 18th Conference on Embedded Networked Sensor Systems, SenSys ’20, page\n476–488, New York, NY, USA, 2020. Association for Computing Machinery.\n[174] Reza Yazdani, Marc Riera, Jose-Maria Arnau, and Antonio González. The dark side of dnn pruning. In\nProceedings of the 45th Annual International Symposium on Computer Architecture, ISCA ’18, page 790–801.\nIEEE Press, 2018.\n[175] S. Yin, Z. Jiang, J. Seo, and M. Seok. Xnor-sram: In-memory computing sram macro for binary/ternary deep\nneural networks. IEEE Journal of Solid-State Circuits, 55(6):1733–1743, 2020.\n[176] Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng. Image classiﬁcation at supercomputer\nscale. arXiv preprint arXiv:1811.06992, 2018.\n[177] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. In ICLR ’19:\nInternational Conference on Learning Representations, 2019.\n[178] R. Yu, A. Li, C. Chen, J. Lai, V. I. Morariu, X. Han, M. Gao, C. Lin, and L. S. Davis. Nisp: Pruning networks\nusing neuron importance score propagation. In 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9194–9203, 2018.\n[179] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David Fleet,\nTomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, pages 818–833,\nCham, 2014. Springer International Publishing.\n[180] C. Zhang, P. Patras, and H. Haddadi. Deep learning in mobile and wireless networking: A survey. IEEE\nCommunications Surveys Tutorials, 21(3):2224–2287, 2019.\n[181] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An extremely efﬁcient convolutional neural network for\nmobile devices. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6848–6856,\n2018.\n[182] Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du,\nTian Zhi, and Yunji Chen. Fixed-point back-propagation training. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), June 2020.\n[183] Yongwei Zhao, Chang Liu, Zidong Du, Qi Guo, Xing Hu, Yimin Zhuang, Zhenxing Zhang, Xinkai Song, Wei Li,\nXishan Zhang, Ling Li, Zhiwei Xu, and Tianshi Chen. Cambricon-q: A hybrid architecture for efﬁcient training.\nIn 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 706–719,\n2021.\n[184] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth\nconvolutional neural networks with low bitwidth gradients, 2018.\n[185] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. In ICLR ’17:\nInternational Conference on Learning Representations, 2017.\n[186] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan.\nTowards uniﬁed int8 training for convolutional neural network. In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1966–1976, 2020.\n[187] Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In ICLR ’17: International\nConference on Learning Representations, 2017.\n31\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2021-12-30",
  "updated": "2021-12-30"
}