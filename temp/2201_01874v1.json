{
  "id": "http://arxiv.org/abs/2201.01874v1",
  "title": "Combining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations",
  "authors": [
    "Igor Halperin",
    "Jiayu Liu",
    "Xiao Zhang"
  ],
  "abstract": "We suggest a simple practical method to combine the human and artificial\nintelligence to both learn best investment practices of fund managers, and\nprovide recommendations to improve them. Our approach is based on a combination\nof Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns\nthe intent of fund managers as suggested by their trading history, and recovers\ntheir implied reward function. At the second step, this reward function is used\nby a direct RL algorithm to optimize asset allocation decisions. We show that\nour method is able to improve over the performance of individual fund managers.",
  "text": "Combining Reinforcement Learning and Inverse Reinforcement Learning for\nAsset Allocation Recommendations\nIgor Halperin 1 Jiayu Liu 1 Xiao Zhang 1\nAbstract\nWe suggest a simple practical method to combine\nthe human and artiﬁcial intelligence to both learn\nbest investment practices of fund managers, and\nprovide recommendations to improve them. Our\napproach is based on a combination of Inverse\nReinforcement Learning (IRL) and RL. First, the\nIRL component learns the intent of fund managers\nas suggested by their trading history, and recov-\ners their implied reward function. At the second\nstep, this reward function is used by a direct RL\nalgorithm to optimize asset allocation decisions.\nWe show that our method is able to improve over\nthe performance of individual fund managers.\n1. Introduction\nPortfolio management is a quintessential example of stochas-\ntic multi-period (dynamic) optimization, also known as\nstochastic optimal control. Reinforcement Learning (RL)\nprovides data-driven methods for problems of optimal con-\ntrol, that are able to work with high dimensional data.\nSuch applications are beyond the reach of classical meth-\nods of optimal control, which typically only work for low-\ndimensional data. Many of the most classical problems of\nquantitative ﬁnance such as portfolio optimization, wealth\nmanagement and option pricing can be very naturally for-\nmulated and solved as RL problems, see e.g. (Dixon et al.,\n2020).\nA critically important input to any RL method is a reward\nfunction. The reward function provides a condensed formu-\nlation of the agent’s intent, and respectively it informs the\noptimization process of RL. The latter amounts to ﬁnding\npolicies that maximize the expected total reward obtained\nin a course of actions by the agent. One simple example\nof a reward function for RL of portfolio management is\n1AI Center of Excellence for Asset Management, Fi-\ndelity\nInvestments.\nEmails:\nIgor\nHalperin,\nJiayuLiu,\nXiao\nZhang\n<igor.halperin@fmr.com,\njiayu.liu@fmr.com,\nxiao.zhang2@fmr.com>.\nUnder Review\nprovided by the classical Markowitz mean-variance objec-\ntive function applied in a multi-period setting ((Dixon et al.,\n2020)). However, there might exist other speciﬁcations of\nthe reward function for portfolio management that might be\nmore aligned with the actual practices of active portfolio\nmanagers.\nInverse Reinforcement Learning (IRL) is a sub-ﬁeld of RL\nthat addresses the inverse problem of inferring the reward\nfunction from a demonstrated behavior. The behavioral data\nare given in the form of sequences of state-action pairs per-\nformed either by a human or AI agent. IRL is popular in\nrobotics and video games for cases where engineering a\ngood reward function (one that promotes a desired behavior)\nmay be a challenging task on its own. The problem of dy-\nnamic portfolio optimization arguably belongs in such class\nof problems. Indeed, while the ’best’ objective function to\nbe used for portfolio optimization is unknown, we can try to\nuse IRL to learn it from the investment behavior of active\nportfolio managers (PMs).\nIn this paper, we propose a combined use of IRL and RL\n(in that sequence) to both learn the investment strategies of\nportfolio managers, and reﬁne them, by suggesting tweaks\nthat improve the overall portfolio performance. Our ap-\nproach can be viewed as a way to develop a ‘collective\nintelligence’ for a group of fund managers with similar in-\nvestment philosophies. By using trading histories of such\nfunds, our algorithm learns the reward function from all\nof them jointly in the IRL step, and then uses this reward\nfunction to optimize the investment policy in the second, RL\nstep.\nWe emphasize that our approach is not intented to replace\nactive portfolio managers, but rather to assist them in their\ndecision making. In particular, we do not re-optimize the\nstock selection made by PMs, but simply change allocations\nto stocks that were already selected. As will be explained\nin more details below, data limitations force us to work\nwith a dimensionally reduced portfolio optimization prob-\nlem. In this work, we choose to aggregate all stocks in a\ngiven portfolio by their industry sector. This provides a\nlow-dimensional view of portfolios held by active managers.\nRespectively, the optimal policy is given in terms of alloca-\ntions to industrial sectors, and not in terms of allocations to\narXiv:2201.01874v1  [cs.LG]  6 Jan 2022\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n2\nindividual stocks. However, given any portfoﬂio-speciﬁc se-\nlection of stocks for a given sector, a recommended optimal\nsector-level allocation can be achieved simply by changing\npositions in stocks already selected.1\nOur paper presents a ﬁrst practical example of a joined ap-\nplication of IRL and RL for asset allocation and portfolio\nmanagement that leverages human intelligence and transfers\nit, in a digestible form, to an AI system (our RL agent).\nWhile we use speciﬁc algorithms for the IRL and RL steps\n(respectively, the T-REX and G-learner algorithms, see be-\nlow), our framework is general and modular, and enables\nusing of other IRL and RL algorithms, if needed. We show\nthat a combined use of IRL and RL enables to both learn\nbest investment practices of fund managers, and provide\nrecommendations to improve them.\nThe paper is organized as follows. In Sect. 1.1 we overview\nprevious related work. Sect. 2 outlines the theoretical frame-\nwork of our approach, presenting the parametric T-REX\nalgorithm for the IRL part, and the G-learner algorithm for\nthe RL part. Sect. 3 shows the results of our analysis. The\nﬁnal Sect. 4 provides a summary and conclusions.\n1.1. Related Work\nOur approach combines approaches developed separately\nin the research community focused on RL and IRL prob-\nlems. For a general review of RL and IRL along with their\napplications to ﬁnance, see (Dixon et al., 2020).\nIn its reliance on a combination of IRL and RL for optimiza-\ntion of a ﬁnancial portfolio, this paper follows the approach\nof (Dixon & Halperin, 2021). The latter uses a continuous\nstate-action version of G-learning, a probabilistic extension\nof Q-learning (Fox et al., 2016), for the RL step, and an\nIRL version of G-learning called GIRL for the IRL step. In\nthis work, we retain the G-learner algorithm from (Dixon\n& Halperin, 2021) for the RL step, but replace the IRL step\nwith a parametric version of a recent algorithm called T-\nREX (Brown et al., 2019) which will be presented in more\ndetails in Sect. 2.1.\n2. The method\nIn this section, we describe our proposed framework for\nasset allocation recommendations as a sequence that com-\nbines IRL and RL in a two-step procedure. The framework\nis designed to be generic enough to accommodate various\nIRL and RL methods. In this work, we focus on an ex-\ntension of the approach of (Dixon & Halperin, 2021) that\nincorporates portfolio performance ranking information into\n1This assumes that stocks from all sectors are already present\nin the portfolio, and that changes of positions in individual stocks\ndo not produce a considerable price impact.\nits policy optimization algorithm. This extension is inspired\nby a recently proposed IRL method called T-REX (Brown\net al., 2019). Here we ﬁrst provide a short overview of\nT-REX and G-learner methods, and then introduce our IRL-\nRL framework. In its IRL step, our proposed parametric\nT-REX algorithm infers PMs’ asset allocation decision rules\nfrom portfolio historical data, and provides a high quality\nreward function to evaluate the performance. The learned\nreward function is then utilized by G-learner in the RL step\nto learn the optimal policy for asset allocation recommenda-\ntions. Policy optimization amounts to a recursive procedure\ninvolving only linear algebra operations, which can be com-\nputed within seconds.\n2.1. The IRL step with parametric T-REX\nIRL deals with recovering reward functions from the ob-\nserved agents’ behavior as a way of rationalizing their intent.\nThe learned reward function can then be used for evalua-\ntion and optimization of action policies. The IRL approach\nreduces the labor of deﬁning proper reward functions in\nRL for applications where their choice is not immediately\nobvious.\nMany traditional IRL methods try to learn the reward func-\ntion from demonstrations under the assumption that the\ndemonstrated history corresponds to an optimal or near-\noptimal policy employed by an agent that produced the data,\nsee e.g. (Pomerleau, 1991; Ross et al., 2011; Torabi et al.,\n2018). These IRL methods therefore simply try to mimic\nor justify the observed behavior by ﬁnding a proper reward\nfunction that explains this behavior. This implies that by\nfollowing their approach, the best one can hope for IRL\nfor investment decisions is that it would be able to imitate\ninvestment policies of asset managers.\nHowever, the assumption that the demonstrated behavior is\nalready optimal might not be realistic, or even veriﬁable,\nfor applications in quantitative ﬁnance. This is because,\nunlike robotics or video games, the real market environment\ncannot be reproduced on demand. Furthermore, relying on\nthis assumption for inferring rewards may also have a side\neffect of transferring various psychological biases of asset\nmanagers into an inferred ‘optimal’ policy.\nInstead of simply trying to mimic the observed behavior\nof asset managers, it would be highly desirable to try to\nimprove over their investment decisions, and potentially\ncorrect for some biases implicit in their decision-making. A\nprincipled way towards implementing such a program would\nbe to infer the intent of asset managers from observing\ntheir trading decisions without the assumption they their\nstrategies were fully or partially successful. With such an\napproach, demonstrated trajectories would be scored by how\nwell they succeed in achieving a certain goal. This is clearly\nvery different from seeking a reward function that would\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n3\nsimply rationalize the observed behavior of asset managers.\nSuch inference of agents’ intent is made possible using a re-\ncently proposed IRL algorithm called T-REX, for Trajectory-\nbased Reward Extrapolation (Brown et al., 2019). The T-\nREX method ﬁnds the reward function that captures the\nintent of the agent without assuming a near-optimality of\nthe demonstrated behavior. Once the reward function that\ncaptures agent’s intent is found, it can be used to further\nimprove the performance by optimizing this reward. This\ncould be viewed as extrapolation of the reward beyond the\ndemonstrated behavior, thus explaining the ‘EX’ part in the\nname T-REX.\nTo overcome the issue of potential sub-optimality of demon-\nstrated behavior, T-REX relies on certain extra information\nthat is not normally used in other IRL methods. More specif-\nically, T-REX uses rankings of all demonstrated trajectories\naccording to a certain ﬁnal score. The ﬁnal score is assigned\nto the whole trajectory rather than to a single time-step. The\nlocal reward function is then learned from the condition that\nit should promote the most desirable trajectories according\nto the score. In this work, we score different portfolio tra-\njectories by their total realized returns, however, we could\nalso use risk-adjusted metrics such as the Sharpe or Sortino\nratios.\nLet O : {S, A} be a state-action space of an MDP envi-\nronment, and ˆrθ(·) with parameters θ be a target reward\nfunction to be optimized in the IRL problem. Given M\nranked observed sequences {om}M\nm=1 (oi ≺oj if i < j,\nwhere “≺” indicates the preferences, expressed e.g. via a\nranking order, between pairwise sequences), T-REX con-\nducts reward inference by solving the following optimiza-\ntion problem:\nmax\nθ\nX\noi≺oj\nlog\ne\nP\n{s,a}∈oj ˆrθ(s,a)\ne\nP\n{s,a}∈oi ˆrθ(s,a) + e\nP\n{s,a}∈oj ˆrθ(s,a)\n(1)\nThis objective function is equivalent to the softmax nor-\nmalized cross-entropy loss for a binary classiﬁer, and can\nbe easily trained using common machine learning libraries\nsuch as PyTorch or TensorFlow. As a result, the learned opti-\nmal reward function can preserve the ranking order between\npairs of sequences.\nThe original T-REX algorithm is essentially a non-\nparametric model that encodes the reward function into\na deep neural network (DNN). This might be a reasonable\napproach for robotics or video games where there is a plenty\nof data, while the reward function might be highly non-\nlinear and complex. Such setting is however ill-suited for\napplications to portfolio management where the amount of\navailable data is typically quite small. Therefore, in this\nwork we pursue a parametric version of T-REX, where the\nreward function is encoded into a function with a small\nnumber of parameters, which is then optimized using Eq.(1).\nA particular model of the reward function that will be pre-\nsented in the next section has only 4 tunable parameters,\nwhich appears to be about the right order of model complex-\nity given available data. The added beneﬁt in comparison to\nthe original DNN-based implementation of T-REX is that a\nparametric T-REX is much faster to train.\n2.2. The RL step with G-learner\nG-learner is an algorithm for the direct RL problem of port-\nfolio optimizaiton that was proposed in (Dixon & Halperin,\n2021). It relies on a continuous state-action version of G-\nlearning, a probabilistic extension of Q-learning designed\nfor noisy environments (Fox et al., 2016). G-learner solves\na ﬁnite-horizon direct RL problem of portfolio optimiza-\ntion in a high-dimensional continuous state-action space\nusing a sample-based approach that operates with available\nhistorical data of portfolio trading. The algorithm uses a\nclosed-form state transition model to capture the stochastic\ndynamics in a noisy environment, and therefore belongs in\nthe class of model-based RL approaches. In this work, we\nemploy G-learner as a RL solver for the sequential portfo-\nlio optimization, while deﬁning a new reward function to\nevaluate portfolio managers’ performance.\nUnlike (Dixon & Halperin, 2021) that considered G-learning\nfor a portfolio of individual stocks held by a retail investor,\nhere we consider portfolios typically held by professional\nasset managers. Due to data limitations, it is unfeasible to\nestimate models that track the whole investment universe\ncounting thousands of stocks, and maintain individual stocks\nholdings and their changes as, respectively, state and action\nvariables. A viable alternative is is to aggregate individ-\nual stock holdings into buckets constructed according to\na particular dimension reduction principle, and deﬁne the\nstate and action variables directly in such dimensionally\nreduced space. In this paper, we choose to aggregate all\nstocks in the PM’s portfolio into N = 11 sectors of the\nstandard GICS industry classiﬁcation, effectively mapping\nthe portfolio into portfolio of sector exposures. The state\nvector xt ∈RN is respectively deﬁned as a vector of dollar\nvalues of stock positions in each sector at time t. The action\nvariable ut ∈RN is given by the vector of changes in these\npositions as a result of trading at time step t. Furthermore,\nlet rt ∈RN represent the asset returns as a random vari-\nable with the mean ¯rt and covariance matrix Σr. As in our\napproach we identify assets with sector exposures, ¯rt and\nΣr will be interpreted as expected sector returns and sector\nreturn covariance, respectively.\nThe state transition model is deﬁned as follows:\nxt+1 = At(xt + ut), At = diag(1 + rt)\n(2)\nIn this work we use a simpliﬁed form of the reward from\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n4\n(Dixon & Halperin, 2021), which we write as follows:\nRt(xt, ut|θ) = −Et\n\u0014\u0010\nˆPt −Vt\n\u00112\u0015\n−λ ·\n\u00001T ut −Ct\n\u00012 −ω · uT\nt ut\n(3)\nwhere Ct is a money ﬂow to the fund at time step t minus the\nchange of a cash position of the fund at the same time step,\nλ and ω are parameters, and Vt and ˆPt are, respectively, the\nvalues of asset manager’s and reference portfolios, deﬁned\nas follows:\nVt = (1 + rt)T (xt + ut)\nˆPt = ρ · Bt + (1 −ρ) · η · 1T xt\n(4)\nwhere η and ρ are additional parameters, and Bt is a bench-\nmark portfolio, such as e.g. the SPX index portfolio, prop-\nerly re-scaled to match the size of the PM’s portfolio at the\nstart of the investment period.\nThe reward function (3) consists of three terms, each en-\ncoding one of the portfolio managers’ trading insights. In\nthe ﬁrst term, ˆPt deﬁnes the target portfolio market value\nat time t. It is speciﬁed as a linear combination of a refer-\nence benchmark portfolio value Bt and the current portfolio\ngrowing with rate η according to Eq.(4), where ρ ∈[0, 1]\nis a parameter deﬁning the relative weight between the two\nterms. Vt gives the portfolio value at time t + ∆t, after the\ntrade ut is made at time t. The ﬁrst term in (3) imposes a\npenalty for under-performance of the traded portfolio rel-\native to its moving target. The second term enforces the\nconstraint that the total amount of trades in the portfolio\nshould match the inﬂow Ct to the portfolio at each time\nstep, with λ being a parameter penalizing violations of the\nequality constraint. The third term approximates transaction\ncosts by a quadratic function with parameter ω, thus serving\nas a L2 regularization. The vector θ of model parameters\nthus contains four reward parameters {ρ, η, λ, ω}.\nImportantly, the reward function (3) is a quadratic function\nof the state and action. Such a choice of the reward func-\ntion implies that the value- and action-value functions are\nsimilarly quadratic functions of the state and action, with\ntime-dependent coefﬁcients. Respectively, with a quadratic\nreward functions such as (3), a G-learning algorithm should\nnot engage neural networks or other function approxima-\ntions, which would invariably bring their own parameters\nto be optimized. In contrast, with the quadratic reward\n(3), learning the optimal value and action-value functions\namounts to learning the coefﬁcients of quadratic functions,\nwhich amounts to a small number of linear algebra oper-\nations (Dixon et al., 2020). As a result, an adaptation of\nthe general G-learning method for the quadratic reward (3)\nproduces a very fast policy optimization algorithm called\nG-learner in (Dixon & Halperin, 2021).\nG-learner is applied to solve the proposed RL problem with\nour deﬁned MDP model in Eq.(2) and reward function in\nEq.(3). Algorithm 1 demonstrates the entire optimization\nsolving process. It takes the model parameters as inputs,\nalong with discount factor γ. In addition, it uses a prior\npolicy π(0) which usually encodes domain knowledge of\nreal world problems. G-learner (and G-learning in general)\ncontrol the deviation of the optimal policy πt from the\nprior π(0) by incorporating the KL divergence of πt and\nπ(0) into a modiﬁed, regularized reward function, with a\nhyperparameter β that controls the magnitude of the KL\nregularizer. When β is large, the deviation can be arbitrarily\nlarge, while in the limit β →0, πt is forced to be equal\nto π(0), so there is no learning in this limit. Furthermore,\nFt(xt) is the value function, and Gt(xt, ut) is the action-\nvalue function. The algorithm is initialized by computing\nthe optimal terminal value and action-value functions F ∗\nT\nand G∗\nT . They are computed by ﬁnding the optimal last-step\naction aT that achieves the highest one-step terminal reward,\ni.e. solves the equation ∂Rt(xt,ut)\n∂ut\n|t=T = 0. Thereafter, the\npolicy associated with earlier time steps can be derived\nrecursively backward in time as shown in the for loop of\nAlgorithm 1. For a detailed derivation of V alue Update\nand ActionV alue Update steps, we refer to Section 4 in\n(Dixon & Halperin, 2021).\nAlgorithm 1 G-learner policy optimization\nInput\n:λ, ω, η, ρ, β, γ, {¯rt, xt, ut, Bt, Ct}T\nt=0, Σr, π(0)\nOutput :π∗\nt = π(0) · eβ(G∗\nt −F ∗\nt ), t = 0, · · · , T\nInitialize: F ∗\nT , G∗\nT\nwhile not converge do\nfor t ∈[T-1, -1, 0] do\nFt ←Value Update(Ft+1, Gt+1)\nGt ←ActionValue Update(Ft, Gt+1)\nend\nend\nreturn {F ∗\nt , G∗\nt, π∗\nt }T\nt=0\nOnce the optimal policy π∗\nt = π∗\nt (ut|xt) is learned, the\nrecommended action at time t is given by the mode of the\naction policy for the given state xt.\n2.3. A uniﬁed IRL-RL framework\nFigure 1 illustrates the overall workﬂow of our proposed\nIRL-RL framework. We apply this framework to provide\nportfolio allocation recommendations to improve fund re-\nturns.\nIt takes the observed M state-action sequences\n(om : {xt, ut}T\nt=0) from multiple funds as input training\ndata. As cumulative funds’ returns can be used to measure\nthe overall PM’s performance over a given observation pe-\nriod, we use the realized total returns in the train dataset\nas a ranking criterion for the IRL step with the parametric\nT-REX. As an alternative, we could rely on risk-adjusted\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n5\nmetrics such as e.g. the Sharpe or Sortino ratios.\nWith the chosen ranking criterion, the T-REX IRL module\nﬁnds the optimal parameters of the reward function deﬁned\nin Eq.(3). The optimal reward function parameters are then\npassed to the RL module which optimizes the action pol-\nicy. Both modules operate in the ofﬂine learning mode,\nand do not require an online integration with the trading\nenvironment.\nFigure 1. The ﬂowchat of our IRL-RL framework\nThe IRL module can also work individually to provide be-\nhavioral insights via analysis of inferred model parameters.\nIt is designed to be generalizable to different reward func-\ntions, if needed. Ranking criteria need to be rationally\ndetermined to align with the logic of rewards and depict\nthe intrinsic behavioral goal of demonstrations. The human-\nprovided ranking criterion based on the performance on\ndifferent portfolio paths thus informs a pure AI system (our\nG-learner agent). As a result, our IRL/RL pipeline im-\nplements a simple human-machine interaction loop in the\ncontext of portfolio management and asset allocation.\nWe note that by using T-REX with the return-based scoring,\nour model already has a great starting point to hope for\nsuccess. Indeed, as our reward function and ranking are\naligned with PMs’ own assessment of their success, this\nalready deﬁnes a ‘ﬂoor’ for the model performance. This\nis because our approach is guaranteed to at least match\nindividual funds’ performance by taking the exact replica\nof their actions. Therefore, our method cannot do worse,\nat least in-sample, than PMs but it can do better - which is\nindeed the case as will be shown below.\nAnother comment due here is that while in this work we\nuse the aggregation to the sectors as a particular scheme\nfor dimension reduction, our combined IRL-RL framework\ncould also be used with other ways of dimension reduction,\nfor example we could aggregate all stocks by their exposure\nto a set of factors.\n3. Experiments\nThis section describes our experiments. First, we explain\ndata pre-processing steps. Once the state and action vari-\nables are computed, we sequentially apply the parametric\nT-REX and G-Learning algorithms according to Eq.(1) and\nAlgorithm 1, respectively. We then show how portfolio\nmanagers’ insights can be inferred using T-REX, and then\ndemonstrate that G-learner is able to outperform active man-\nagers by ﬁnding optimal policies for rewards inferred at the\nﬁrst stage.\n3.1. Data Preparation\nIn our experiments, two sets of mutual funds are chosen\nbased on their benchmark indexes. To anonymize our data,\nwe replace actual funds’ names by single-letter names. The\nﬁrst set includes six funds ({Si}6\ni=1) that all have the S&P\n500 index as a benchmark. The second set includes twelve\nfunds that have the the Russell 3000 index as a benchmark.\nThe second group is further divided into growth and value\nfund groups ({RGi}7\ni=1 and {RVi}5\ni=1). Each fund’s trad-\ning trajectory covers the period from January 2017 to De-\ncember 2019, and contains its monthly holdings and trades\nfor eleven sectors (i.e., xt, ut) as well as monthly cashﬂows\nCt. The ﬁrst two years of data are used for training (so that\nwe choose T=24 months), and the last year starting from\nJanuary 2019 is used for testing. At the starting time step we\nassign each fund’s total net asset value to its corresponding\nbenchmark value (i.e., Bt=0) in order to align their size and\nuse the actual benchmark return at each time step to calcu-\nlate their values afterwards (i.e., t > 0). The resulting time\nseries {xt, ut, Bt, Ct}T\nt=0 are further normalized by divid-\ning by their initial values at t = 0. In the training phase, the\ncanonical ARMA model (Hannan, 2009) is applied to fore-\ncast expected sector returns rt, and the regression residue\nis then used to estimate the sector return covariance matrix\nΣr. The prior policy π(0) is ﬁtted to a multivariate Gaussian\ndistribution with a constant mean and variance calculated\nfrom sector trades in the training set.\n3.2. Parametric T-REX with portfolio return rankings\nThe pre-processed fund trading data is ranked based on\nfunds’ overall performance realized over the training pe-\nriod. T-REX is then trained to learn the four parameters\n{ρ, η, λ, ω} that enter Eq.(3). The training/test experiments\nare executed separately for all three fund sets (i.e. funds\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n6\nwith the S&P 500 benchmark, Russell 3000 Growth funds,\nand Russell 3000 Value funds).\nRecall that the loss function for T-REX is given by the cross-\nentropy loss for a binary classiﬁer, see Eq.(1). Figure 2\nshows the classiﬁcation accuracy in the training (91.1%)\nand test phases (83.2%) for the S&P 500 fund set. The plots\nin the two sub-ﬁgures show that the learned reward function\npreserves the fund ranking order. Correlation scores are\nused to measure their level of association, producing the\nvalues of 0.988 and 0.954 for the training and test sets,\nrespectively. The excellent out-of-sample correlation scores\ngives a strong support to our chosen method of the reward\nfunction design.\nFigure 2. T-REX: classiﬁcation accuracy and ranking order preser-\nvation measured by correlation scores\nFund managers’ investment strategies can be analyzed using\nthe inferred reward parameters. Figure 3 shows the parame-\nter convergence curve using the S&P 500 fund group. The\nvalues converge after about thirty iterations, which suggests\nadequacy of our chosen model of the reward to the actual\ntrading data. The optimized benchmark weight ρ∗= 0.951\ncaptures the correlation between funds’ performance and\ntheir benchmark index. Its high value implies that portfolio\nmanagers’ trading strategy is to target an accurate bench-\nmark tracking. The value η∗= 1.247 implies funds are\nin the growing trend with prospective rate at 24.7% from\nfund managers’ view. Since the sum of trades are close\nenough to the fund cashﬂows in our dataset, the introduced\npenalty parameter λ converges to a very small value at 0.081.\nThe estimated average trades’ volatility ω∗is around 10%,\nwhich measures the level of consistency across trades in the\nS&P 500 fund group.\nThe experimental results of our proposed parametric T-REX\nare summarized in Table 1. Our model achieved high clas-\nsiﬁcation accuracy (acc) and correlation scores (cor) on\nall three fund groups in both training and test phases. The\noptimal values of reward function parameters are listed in\nthe table as well. Small values of λ∗verify the holding of\nequality constraint over the sum of trades in our dataset.\nComparing across different fund groups, we can identify dif-\nferent trading strategies of portfolio managers. Funds in the\nS&P500 group are managed to track their benchmark very\nclosely with correlation ρ∗= 0.951. On the other hand,\nFigure 3. T-REX: reward parameter learning curve\ncorrelation ρ∗reduces signiﬁcantly to 0.584 and then to\n0.186 for fund groups {RGi}7\ni=1 and {RVi}5\ni=1 which have\nthe same benchmark index Russell 3000, while being seg-\nmented based on their investment approaches (i.e., growth\nvs. value). For the bucket denoted {Si}6\ni=1, growth and\nblend funds are combined. As shown in the table, funds in\nthe segmented groups show very limited variations of their\ntrading amounts compared to those combined in a single\ngroup. This can be seen as an evidence of fund managers’\nconsistency in their investment decisions/policies. Growth\nfunds in {RG}7\ni=1 with η∗= 1.532 produce the highest\nexpected growth rates as implied by their trading activities.\nThis result is in line with the fact that growth investors se-\nlect companies that offer strong earnings growth while value\ninvestors choose stocks that appear to be undervalued in the\nmarketplace.\nTable 1. Inferred reward parameter values and T-REX model accu-\nracy from all three fund groups\nFund Group\n{Si}6\ni=1\n{RGi}7\ni=1\n{RVi}5\ni=1\nρ∗\n0.951\n0.186\n0.584\nη∗\n1.247\n1.532\n1.210\nλ∗\n0.081\n0.009\n0.009\nω∗\n0.100\n0.012\n0.009\nacc (train/test)\n0.911/0.832\n0.878/0.796\n0.906/0.832\ncor (train/test)\n0.988/0.954\n0.925/0.884\n0.759/0.733\nThe four reward parameters can be used to group different\nfund managers or different trading strategies into distinct\nclusters in the space of reward parameters. While the present\nanalysis in this paper suggests pronounced patterns based\non the comparison across three fund groups, including more\nfund trading data from many portfolio managers might be\nable to bring further insights into fund managers’ behavior.\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n7\nThis is left here for a future work.\n3.3. G-learner for allocation recommendation\nOnce the reward parameters are learned for all groups of\nfunds, they are passed to G-learner for the policy optimiza-\ntion step. The pre-processed fund trading data in year 2017\nand 2018 is used as inputs for policy optimization (see Fig-\nure 1). Trading trajectories were truncated to twelve months’\nlong in order to align with the length of the test period (year\n2019). The optimal policy π∗\nt = π∗\nt (ut|xt) is learned using\nthe train dataset, and then applied to the test set throughout\nthe entire test period. Differently from the training phase,\nwhere we use expected sector returns in the reward function,\nin the test phase we use realized monthly returns to derive\nthe next month’s holdings xt+1 after trades ut was made\nfor the current month with xt. At the start time of the test\nperiod, the holdings xt=0 coincide with actual historical\nholdings. We use this procedure along with the learned\npolicy π∗\nt (ut|xt) onward from February 2019 to create\ncounterfactual portfolio trajectories for the test period. We\nwill refer to these RL-recommended trajectories as the AI\nAlter Ego’s (AE) trajectories.\nFigures 4, 5, 6 show the outperformance of our RL policy\nfrom PM’s trading strategies (i.e, AE −PM) for all three\nfund groups for both the training and test sets for forward\nperiod of 12 months, plotted as functions of their time ar-\nguments. We note that both curves for the training and test\nsets follow similar paths for at least seven months before\nthey started to diverge (the curves for Russell 3000 value\ngroup diverged signiﬁcantly after the eighth months’ trades\nand thus were truncated for a better visualization effect).\nIn practice, we recommend to update the policy through a\nre-training once the training and test results start to diverge.\nIn general, our results suggest that our G-learner is able to\ngeneralize (i.e. perform well out-of-sample) up to 6 months\ninto the future.\nAnalysis at the individual fund level for all three groups is\npresented in Figures 7, 9, 11 for the training set, and Figures\n8, 10, 12 for the test set. Our model outperformed most of\nPMs throughout the test period except funds RG5 and RV5.\nIt is interesting to note that the test results outperformed the\ntraining ones for the S&P 500 fund group. This may be due\nto a potential market regime drift (which is favorable to us,\nin the present case). More generally, a detailed study would\nbe desired to address the impact of potential market drift or\na regime change on optimal asset allocation policies. Such\ntopics are left here for a future research.\n4. Conclusion and Future Work\nIn this work, we presented a ﬁrst practical two-step pro-\ncedure that combines the human and artiﬁcial intelligence\nFigure 4. G-learner: overall trading performance with funds bench-\nmarked by S&P500\nFigure 5. G-learner: overall trading performance with growth\nfunds benchmarked by Russell 3000\nFigure 6. G-learner: overall trading performance with value funds\nbenchmarked by Russell 3000\nfor optimization of asset allocation and investment portfo-\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n8\nFigure 7. G-learner:\ntrading performance of individual funds\nbenchmarked by S&P500 on the training set\nFigure 8. G-learner:\ntrading performance of individual funds\nbenchmarked by S&P500 on the test set\nFigure 9. G-learner: trading performance of individual growth\nfunds benchmarked by Russell 3000 on the training set\nlios. At the ﬁrst step, we use inverse reinforcement learning\n(IRL), and more speciﬁcally the parametric T-REX algo-\nFigure 10. G-learner: trading performance of individual growth\nfunds benchmarked by Russell 3000 on the test set\nFigure 11. G-learner: trading performance of individual value\nfunds benchmarked by Russell 3000 on the training set\nFigure 12. G-learner: trading performance of individual value\nfunds benchmarked by Russell 3000 on the test set\nrithm, to infer the reward function that captures fund man-\nagers’ intent of achieving higher returns. At the second step,\nCombining Reinforcement Learning and Inverse Reinforcement Learning for Asset Allocation Recommendations\n9\nwe use the direct reinforcement learning with the inferred\nreward function to improve the investment policy. Our ap-\nproach is modular, and allows one to use different IRL or\nRL models, if needed, instead of the particular combination\nof the parametric T-REX and G-learner that we used in this\nwork. It can also use other ranking criteria, e.g. sorting\nby the Sharpe ratio or the Sortino ratio, instead of ranking\ntrajectories by their total return as was done above.\nUsing aggregation of equity positions at the sector level,\nwe showed that our approach is able to outperform most\nof fund managers. Outputs of the model can be used by\nindividual fund managers by keeping their stock selection\nand re-weighting their portfolios across industrial sectors\naccording to the recommendation from G-learner. A practi-\ncal implementation of our method should involve checking\nthe feasibility of recommended allocations and controlling\nfor potential market impact effects and/or transaction costs\nresulting from such rebalancing.\nFinally, we note that while in this work we choose a par-\nticular method of dimensional reduction that aggregates all\nstocks at the industry level, this is not the only available\nchoice. An alternative approach could be considered, where\nall stocks are aggregated based on their factor exposure.\nThis is left here for a future work.\nReferences\nBrown, D., Goo, W., Nagarajan, P., and Niekum, S. Extrap-\nolating Beyond Suboptimal Demonstrations via Inverse\nReinforcement Learning from Observations. In Interna-\ntional Conference on Machine Learning, pp. 783–792,\n2019.\nDixon, M. and Halperin, I. Goal-Based Wealth Management\nwith Generative Reinforcement Learning. RISK, July\n2021, p.66; arXiv preprint arXiv:2002.10990, 2021.\nDixon, M., Halperin, I., and Bilokon, P. Machine Learning\nin Finance: from Theory to Practice. Springer, 2020.\nFox, R., Pakman, A., and Tishby, N. Taming the Noise in\nReinforcement Learning via Soft Updates. In Proceed-\nings of the Thirty-Second Conference on Uncertainty in\nArtiﬁcial Intelligence, 2016.\nHannan, E. J. Multiple Time Series, volume 38. John Wiley\n& Sons, 2009.\nPomerleau, D. A. Efﬁcient Training of Artiﬁcial Neural Net-\nworks for Autonomous Navigation. Neural computation,\n3(1):88–97, 1991.\nRoss, S., Gordon, G., and Bagnell, D. A Reduction of Imi-\ntation Learning and Structured Prediction to No-Regret\nOnline Learning. In International conference on artiﬁcial\nintelligence and statistics, pp. 627–635, 2011.\nTorabi, F., Warnell, G., and Stone, P. Behavioral Cloning\nfrom Observation. In International Joint Conference on\nArtiﬁcial Intelligence, pp. 4950–4957, 2018.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "q-fin.CP",
    "q-fin.PM"
  ],
  "published": "2022-01-06",
  "updated": "2022-01-06"
}